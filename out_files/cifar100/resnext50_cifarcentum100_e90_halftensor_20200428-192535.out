# Model: resnext50
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.resnext
<function resnext50 at 0x7f75625a4f28>
# model requested: 'resnext50'
# printing out the model
ResNext(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (conv2): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv3): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (3): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv4): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (3): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (4): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (5): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv5): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (avg): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=100, bias=True)
)
# model is low precision
# Model: resnext50
# Dataset: cifarcentum
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  4.152 ( 4.152)	Data  0.115 ( 0.115)	Loss 4.7383e+00 (4.7383e+00)	Acc@1   0.00 (  0.00)	Acc@5   3.91 (  3.91)
Epoch: [0][ 10/391]	Time  0.109 ( 0.478)	Data  0.001 ( 0.011)	Loss 1.3195e+01 (1.2132e+01)	Acc@1   0.78 (  0.85)	Acc@5   4.69 (  5.75)
Epoch: [0][ 20/391]	Time  0.113 ( 0.304)	Data  0.001 ( 0.008)	Loss 5.4844e+00 (9.9801e+00)	Acc@1   0.00 (  1.00)	Acc@5   5.47 (  5.73)
Epoch: [0][ 30/391]	Time  0.112 ( 0.242)	Data  0.001 ( 0.006)	Loss 4.7734e+00 (8.3668e+00)	Acc@1   0.00 (  1.03)	Acc@5   2.34 (  5.24)
Epoch: [0][ 40/391]	Time  0.111 ( 0.211)	Data  0.001 ( 0.006)	Loss 4.6172e+00 (7.4733e+00)	Acc@1   2.34 (  1.18)	Acc@5   9.38 (  5.49)
Epoch: [0][ 50/391]	Time  0.109 ( 0.191)	Data  0.001 ( 0.005)	Loss 4.5742e+00 (6.9193e+00)	Acc@1   0.78 (  1.16)	Acc@5   6.25 (  5.39)
Epoch: [0][ 60/391]	Time  0.112 ( 0.178)	Data  0.001 ( 0.005)	Loss 4.6367e+00 (6.5434e+00)	Acc@1   0.78 (  1.26)	Acc@5   6.25 (  5.55)
Epoch: [0][ 70/391]	Time  0.111 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.5977e+00 (6.2709e+00)	Acc@1   0.78 (  1.24)	Acc@5   4.69 (  5.48)
Epoch: [0][ 80/391]	Time  0.109 ( 0.162)	Data  0.001 ( 0.005)	Loss 4.5820e+00 (6.0646e+00)	Acc@1   1.56 (  1.27)	Acc@5   5.47 (  5.58)
Epoch: [0][ 90/391]	Time  0.112 ( 0.156)	Data  0.001 ( 0.005)	Loss 4.5977e+00 (5.9020e+00)	Acc@1   0.00 (  1.35)	Acc@5   3.91 (  5.67)
Epoch: [0][100/391]	Time  0.112 ( 0.152)	Data  0.001 ( 0.005)	Loss 4.6172e+00 (5.7732e+00)	Acc@1   1.56 (  1.34)	Acc@5   3.12 (  5.69)
Epoch: [0][110/391]	Time  0.112 ( 0.148)	Data  0.001 ( 0.004)	Loss 4.5859e+00 (5.6673e+00)	Acc@1   2.34 (  1.37)	Acc@5   3.91 (  5.75)
Epoch: [0][120/391]	Time  0.111 ( 0.145)	Data  0.001 ( 0.004)	Loss 4.5977e+00 (5.5787e+00)	Acc@1   0.78 (  1.36)	Acc@5   2.34 (  5.76)
Epoch: [0][130/391]	Time  0.112 ( 0.143)	Data  0.001 ( 0.004)	Loss 4.5742e+00 (5.5028e+00)	Acc@1   0.00 (  1.38)	Acc@5   3.91 (  5.77)
Epoch: [0][140/391]	Time  0.112 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.5859e+00 (5.4375e+00)	Acc@1   2.34 (  1.44)	Acc@5  11.72 (  5.88)
Epoch: [0][150/391]	Time  0.111 ( 0.139)	Data  0.001 ( 0.004)	Loss 4.5742e+00 (5.3812e+00)	Acc@1   2.34 (  1.46)	Acc@5   7.81 (  5.98)
Epoch: [0][160/391]	Time  0.111 ( 0.137)	Data  0.001 ( 0.004)	Loss 4.5508e+00 (5.3310e+00)	Acc@1   1.56 (  1.50)	Acc@5   7.03 (  6.05)
Epoch: [0][170/391]	Time  0.113 ( 0.136)	Data  0.001 ( 0.004)	Loss 4.5508e+00 (5.2862e+00)	Acc@1   1.56 (  1.52)	Acc@5   8.59 (  6.15)
Epoch: [0][180/391]	Time  0.112 ( 0.135)	Data  0.001 ( 0.004)	Loss 4.5938e+00 (5.2459e+00)	Acc@1   1.56 (  1.58)	Acc@5   7.03 (  6.30)
Epoch: [0][190/391]	Time  0.111 ( 0.134)	Data  0.001 ( 0.004)	Loss 4.5859e+00 (5.2094e+00)	Acc@1   1.56 (  1.66)	Acc@5   5.47 (  6.44)
Epoch: [0][200/391]	Time  0.110 ( 0.133)	Data  0.001 ( 0.004)	Loss 4.5859e+00 (5.1774e+00)	Acc@1   2.34 (  1.66)	Acc@5   7.03 (  6.49)
Epoch: [0][210/391]	Time  0.111 ( 0.132)	Data  0.001 ( 0.004)	Loss 4.5039e+00 (5.1466e+00)	Acc@1   1.56 (  1.68)	Acc@5   9.38 (  6.66)
Epoch: [0][220/391]	Time  0.112 ( 0.131)	Data  0.001 ( 0.004)	Loss 4.5508e+00 (5.1192e+00)	Acc@1   1.56 (  1.69)	Acc@5  10.94 (  6.77)
Epoch: [0][230/391]	Time  0.114 ( 0.130)	Data  0.001 ( 0.004)	Loss 4.5195e+00 (5.0934e+00)	Acc@1   2.34 (  1.72)	Acc@5   7.03 (  6.85)
Epoch: [0][240/391]	Time  0.110 ( 0.129)	Data  0.001 ( 0.004)	Loss 4.5039e+00 (5.0689e+00)	Acc@1   2.34 (  1.76)	Acc@5   9.38 (  6.99)
Epoch: [0][250/391]	Time  0.111 ( 0.129)	Data  0.001 ( 0.004)	Loss 4.4609e+00 (5.0465e+00)	Acc@1   3.91 (  1.82)	Acc@5   8.59 (  7.07)
Epoch: [0][260/391]	Time  0.111 ( 0.128)	Data  0.001 ( 0.004)	Loss 4.5312e+00 (5.0248e+00)	Acc@1   2.34 (  1.85)	Acc@5   9.38 (  7.19)
Epoch: [0][270/391]	Time  0.109 ( 0.127)	Data  0.001 ( 0.004)	Loss 4.3867e+00 (5.0039e+00)	Acc@1   3.91 (  1.89)	Acc@5  11.72 (  7.33)
Epoch: [0][280/391]	Time  0.110 ( 0.127)	Data  0.001 ( 0.004)	Loss 4.4297e+00 (4.9841e+00)	Acc@1   3.12 (  1.95)	Acc@5  11.72 (  7.50)
Epoch: [0][290/391]	Time  0.114 ( 0.126)	Data  0.001 ( 0.004)	Loss 4.4141e+00 (4.9652e+00)	Acc@1   3.12 (  1.99)	Acc@5  11.72 (  7.67)
Epoch: [0][300/391]	Time  0.111 ( 0.126)	Data  0.001 ( 0.004)	Loss 4.4492e+00 (4.9474e+00)	Acc@1   1.56 (  2.01)	Acc@5  12.50 (  7.85)
Epoch: [0][310/391]	Time  0.109 ( 0.126)	Data  0.001 ( 0.004)	Loss 4.3398e+00 (4.9294e+00)	Acc@1   3.12 (  2.06)	Acc@5  10.94 (  8.05)
Epoch: [0][320/391]	Time  0.110 ( 0.125)	Data  0.001 ( 0.004)	Loss 4.5195e+00 (4.9121e+00)	Acc@1   3.91 (  2.09)	Acc@5  11.72 (  8.26)
Epoch: [0][330/391]	Time  0.111 ( 0.125)	Data  0.001 ( 0.004)	Loss 4.3906e+00 (4.8959e+00)	Acc@1   6.25 (  2.16)	Acc@5  17.97 (  8.48)
Epoch: [0][340/391]	Time  0.109 ( 0.125)	Data  0.001 ( 0.004)	Loss 4.3008e+00 (4.8789e+00)	Acc@1   2.34 (  2.24)	Acc@5  14.84 (  8.71)
Epoch: [0][350/391]	Time  0.111 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.4062e+00 (4.8649e+00)	Acc@1   1.56 (  2.25)	Acc@5  13.28 (  8.82)
Epoch: [0][360/391]	Time  0.112 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.3867e+00 (4.8509e+00)	Acc@1   6.25 (  2.27)	Acc@5  17.97 (  8.99)
Epoch: [0][370/391]	Time  0.112 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.3750e+00 (4.8372e+00)	Acc@1   2.34 (  2.31)	Acc@5  15.62 (  9.15)
Epoch: [0][380/391]	Time  0.112 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.3984e+00 (4.8245e+00)	Acc@1   4.69 (  2.33)	Acc@5  14.06 (  9.30)
Epoch: [0][390/391]	Time  1.289 ( 0.126)	Data  0.001 ( 0.004)	Loss 4.2773e+00 (4.8114e+00)	Acc@1   1.25 (  2.37)	Acc@5  16.25 (  9.49)
## e[0] optimizer.zero_grad (sum) time: 0.2986428737640381
## e[0]       loss.backward (sum) time: 15.493740797042847
## e[0]      optimizer.step (sum) time: 2.6765129566192627
## epoch[0] training(only) time: 49.375025510787964
# Switched to evaluate mode...
Test: [  0/100]	Time  0.456 ( 0.456)	Loss 4.3320e+00 (4.3320e+00)	Acc@1   4.00 (  4.00)	Acc@5  20.00 ( 20.00)
Test: [ 10/100]	Time  0.038 ( 0.078)	Loss 4.2227e+00 (4.2777e+00)	Acc@1   3.00 (  3.91)	Acc@5  15.00 ( 15.82)
Test: [ 20/100]	Time  0.041 ( 0.060)	Loss 4.2461e+00 (4.2874e+00)	Acc@1   5.00 (  4.43)	Acc@5  16.00 ( 16.76)
Test: [ 30/100]	Time  0.039 ( 0.053)	Loss 4.3398e+00 (4.2996e+00)	Acc@1   7.00 (  4.48)	Acc@5  18.00 ( 16.87)
Test: [ 40/100]	Time  0.040 ( 0.050)	Loss 4.3242e+00 (4.2994e+00)	Acc@1   3.00 (  4.27)	Acc@5  17.00 ( 17.07)
Test: [ 50/100]	Time  0.037 ( 0.048)	Loss 4.2695e+00 (4.3058e+00)	Acc@1   8.00 (  4.14)	Acc@5  21.00 ( 16.94)
Test: [ 60/100]	Time  0.039 ( 0.046)	Loss 4.4336e+00 (4.3176e+00)	Acc@1   6.00 (  4.08)	Acc@5  14.00 ( 16.82)
Test: [ 70/100]	Time  0.040 ( 0.045)	Loss 4.4688e+00 (4.3143e+00)	Acc@1   2.00 (  3.90)	Acc@5  13.00 ( 16.48)
Test: [ 80/100]	Time  0.041 ( 0.044)	Loss 4.3398e+00 (4.3234e+00)	Acc@1   3.00 (  3.83)	Acc@5  14.00 ( 16.27)
Test: [ 90/100]	Time  0.039 ( 0.044)	Loss 4.1992e+00 (4.3184e+00)	Acc@1   8.00 (  3.96)	Acc@5  21.00 ( 16.42)
 * Acc@1 3.900 Acc@5 16.270
### epoch[0] execution time: 53.80622053146362
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.298 ( 0.298)	Data  0.171 ( 0.171)	Loss 4.2656e+00 (4.2656e+00)	Acc@1   5.47 (  5.47)	Acc@5  12.50 ( 12.50)
Epoch: [1][ 10/391]	Time  0.111 ( 0.130)	Data  0.001 ( 0.019)	Loss 4.2070e+00 (4.2983e+00)	Acc@1   5.47 (  4.47)	Acc@5  23.44 ( 16.12)
Epoch: [1][ 20/391]	Time  0.114 ( 0.122)	Data  0.001 ( 0.012)	Loss 4.1758e+00 (4.2736e+00)	Acc@1   6.25 (  4.54)	Acc@5  23.44 ( 17.22)
Epoch: [1][ 30/391]	Time  0.110 ( 0.119)	Data  0.001 ( 0.009)	Loss 4.2617e+00 (4.2752e+00)	Acc@1   3.91 (  4.26)	Acc@5  17.19 ( 17.04)
Epoch: [1][ 40/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.008)	Loss 4.4023e+00 (4.2683e+00)	Acc@1   0.78 (  4.21)	Acc@5  14.84 ( 17.05)
Epoch: [1][ 50/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.007)	Loss 4.3633e+00 (4.2741e+00)	Acc@1   3.12 (  4.12)	Acc@5  11.72 ( 16.82)
Epoch: [1][ 60/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.007)	Loss 4.2461e+00 (4.2712e+00)	Acc@1   3.91 (  4.28)	Acc@5  18.75 ( 17.14)
Epoch: [1][ 70/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.0977e+00 (4.2620e+00)	Acc@1   5.47 (  4.30)	Acc@5  22.66 ( 17.57)
Epoch: [1][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.1641e+00 (4.2551e+00)	Acc@1   4.69 (  4.32)	Acc@5  20.31 ( 17.51)
Epoch: [1][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.2266e+00 (4.2455e+00)	Acc@1   5.47 (  4.50)	Acc@5  14.06 ( 17.75)
Epoch: [1][100/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.2031e+00 (4.2401e+00)	Acc@1   3.91 (  4.65)	Acc@5  13.28 ( 17.88)
Epoch: [1][110/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.0859e+00 (4.2349e+00)	Acc@1   2.34 (  4.73)	Acc@5  21.09 ( 18.00)
Epoch: [1][120/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.1836e+00 (4.2287e+00)	Acc@1   4.69 (  4.74)	Acc@5  19.53 ( 18.27)
Epoch: [1][130/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.1094e+00 (4.2221e+00)	Acc@1   6.25 (  4.80)	Acc@5  21.88 ( 18.48)
Epoch: [1][140/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.0625e+00 (4.2131e+00)	Acc@1   7.03 (  4.87)	Acc@5  23.44 ( 18.70)
Epoch: [1][150/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.0820e+00 (4.2066e+00)	Acc@1   3.12 (  4.88)	Acc@5  19.53 ( 18.86)
Epoch: [1][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.1133e+00 (4.2028e+00)	Acc@1   6.25 (  4.94)	Acc@5  16.41 ( 19.07)
Epoch: [1][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.0898e+00 (4.1978e+00)	Acc@1   6.25 (  4.93)	Acc@5  21.88 ( 19.14)
Epoch: [1][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.0742e+00 (4.1944e+00)	Acc@1   5.47 (  4.91)	Acc@5  22.66 ( 19.16)
Epoch: [1][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.0117e+00 (4.1902e+00)	Acc@1   9.38 (  4.99)	Acc@5  23.44 ( 19.33)
Epoch: [1][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.0000e+00 (4.1833e+00)	Acc@1   4.69 (  5.00)	Acc@5  23.44 ( 19.48)
Epoch: [1][210/391]	Time  0.116 ( 0.114)	Data  0.001 ( 0.005)	Loss 4.0078e+00 (4.1786e+00)	Acc@1   3.12 (  5.03)	Acc@5  28.12 ( 19.68)
Epoch: [1][220/391]	Time  0.111 ( 0.114)	Data  0.001 ( 0.005)	Loss 4.1523e+00 (4.1734e+00)	Acc@1   7.03 (  5.15)	Acc@5  22.66 ( 19.84)
Epoch: [1][230/391]	Time  0.110 ( 0.114)	Data  0.001 ( 0.005)	Loss 4.2930e+00 (4.1688e+00)	Acc@1   3.12 (  5.14)	Acc@5  14.06 ( 19.99)
Epoch: [1][240/391]	Time  0.110 ( 0.114)	Data  0.001 ( 0.005)	Loss 4.1094e+00 (4.1658e+00)	Acc@1   3.12 (  5.13)	Acc@5  18.75 ( 20.14)
Epoch: [1][250/391]	Time  0.109 ( 0.114)	Data  0.001 ( 0.004)	Loss 4.0039e+00 (4.1612e+00)	Acc@1  12.50 (  5.20)	Acc@5  25.78 ( 20.30)
Epoch: [1][260/391]	Time  0.111 ( 0.114)	Data  0.001 ( 0.004)	Loss 4.0469e+00 (4.1544e+00)	Acc@1   7.03 (  5.25)	Acc@5  24.22 ( 20.47)
Epoch: [1][270/391]	Time  0.110 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.8633e+00 (4.1470e+00)	Acc@1   7.81 (  5.30)	Acc@5  32.03 ( 20.70)
Epoch: [1][280/391]	Time  0.112 ( 0.114)	Data  0.001 ( 0.004)	Loss 4.1719e+00 (4.1432e+00)	Acc@1   4.69 (  5.35)	Acc@5  21.88 ( 20.85)
Epoch: [1][290/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.8457e+00 (4.1382e+00)	Acc@1  10.16 (  5.41)	Acc@5  25.78 ( 21.03)
Epoch: [1][300/391]	Time  0.115 ( 0.114)	Data  0.001 ( 0.004)	Loss 4.0352e+00 (4.1334e+00)	Acc@1   7.81 (  5.45)	Acc@5  26.56 ( 21.20)
Epoch: [1][310/391]	Time  0.114 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.9805e+00 (4.1290e+00)	Acc@1   7.03 (  5.48)	Acc@5  25.78 ( 21.33)
Epoch: [1][320/391]	Time  0.110 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.8711e+00 (4.1239e+00)	Acc@1   6.25 (  5.52)	Acc@5  28.91 ( 21.51)
Epoch: [1][330/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.9785e+00 (4.1200e+00)	Acc@1   4.69 (  5.56)	Acc@5  27.34 ( 21.67)
Epoch: [1][340/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 4.0938e+00 (4.1154e+00)	Acc@1   9.38 (  5.62)	Acc@5  27.34 ( 21.81)
Epoch: [1][350/391]	Time  0.112 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.9375e+00 (4.1101e+00)	Acc@1   6.25 (  5.70)	Acc@5  23.44 ( 21.97)
Epoch: [1][360/391]	Time  0.111 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.9688e+00 (4.1056e+00)	Acc@1   7.03 (  5.74)	Acc@5  28.12 ( 22.14)
Epoch: [1][370/391]	Time  0.111 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.9727e+00 (4.1014e+00)	Acc@1   6.25 (  5.77)	Acc@5  26.56 ( 22.29)
Epoch: [1][380/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.9512e+00 (4.0978e+00)	Acc@1  10.94 (  5.83)	Acc@5  32.03 ( 22.41)
Epoch: [1][390/391]	Time  0.109 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.7656e+00 (4.0925e+00)	Acc@1   7.50 (  5.89)	Acc@5  31.25 ( 22.60)
## e[1] optimizer.zero_grad (sum) time: 0.28891682624816895
## e[1]       loss.backward (sum) time: 13.710294008255005
## e[1]      optimizer.step (sum) time: 2.703386068344116
## epoch[1] training(only) time: 44.75811433792114
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.9570e+00 (3.9570e+00)	Acc@1   8.00 (  8.00)	Acc@5  28.00 ( 28.00)
Test: [ 10/100]	Time  0.038 ( 0.053)	Loss 4.0469e+00 (3.9114e+00)	Acc@1  12.00 (  9.18)	Acc@5  26.00 ( 28.18)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 3.8789e+00 (3.9100e+00)	Acc@1   8.00 (  8.81)	Acc@5  26.00 ( 28.43)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 4.0586e+00 (3.9157e+00)	Acc@1  10.00 (  8.61)	Acc@5  25.00 ( 28.32)
Test: [ 40/100]	Time  0.038 ( 0.042)	Loss 3.9766e+00 (3.9099e+00)	Acc@1   7.00 (  8.34)	Acc@5  26.00 ( 28.39)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 3.7930e+00 (3.9059e+00)	Acc@1   9.00 (  8.20)	Acc@5  33.00 ( 28.57)
Test: [ 60/100]	Time  0.041 ( 0.041)	Loss 3.7129e+00 (3.9083e+00)	Acc@1   8.00 (  8.34)	Acc@5  34.00 ( 28.54)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 4.0625e+00 (3.9072e+00)	Acc@1   3.00 (  8.23)	Acc@5  29.00 ( 28.76)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 4.1055e+00 (3.9204e+00)	Acc@1   7.00 (  8.01)	Acc@5  18.00 ( 28.54)
Test: [ 90/100]	Time  0.040 ( 0.040)	Loss 3.6836e+00 (3.9116e+00)	Acc@1  11.00 (  7.97)	Acc@5  36.00 ( 28.73)
 * Acc@1 7.920 Acc@5 28.730
### epoch[1] execution time: 48.89493417739868
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.270 ( 0.270)	Data  0.162 ( 0.162)	Loss 3.9609e+00 (3.9609e+00)	Acc@1  10.16 ( 10.16)	Acc@5  27.34 ( 27.34)
Epoch: [2][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.018)	Loss 3.8438e+00 (3.8551e+00)	Acc@1  10.16 (  9.02)	Acc@5  31.25 ( 30.68)
Epoch: [2][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.011)	Loss 3.8203e+00 (3.8446e+00)	Acc@1  10.94 (  8.89)	Acc@5  32.03 ( 29.35)
Epoch: [2][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 3.7559e+00 (3.8492e+00)	Acc@1  10.94 (  8.64)	Acc@5  32.81 ( 29.94)
Epoch: [2][ 40/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.008)	Loss 4.0312e+00 (3.8632e+00)	Acc@1   7.03 (  8.19)	Acc@5  22.66 ( 29.17)
Epoch: [2][ 50/391]	Time  0.118 ( 0.117)	Data  0.001 ( 0.007)	Loss 3.7520e+00 (3.8545e+00)	Acc@1  10.16 (  8.52)	Acc@5  32.81 ( 29.26)
Epoch: [2][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.8711e+00 (3.8520e+00)	Acc@1   7.81 (  8.85)	Acc@5  29.69 ( 29.55)
Epoch: [2][ 70/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.7715e+00 (3.8569e+00)	Acc@1   9.38 (  8.77)	Acc@5  34.38 ( 29.74)
Epoch: [2][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.8184e+00 (3.8551e+00)	Acc@1   7.81 (  8.65)	Acc@5  22.66 ( 29.68)
Epoch: [2][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.7891e+00 (3.8540e+00)	Acc@1  11.72 (  8.63)	Acc@5  32.03 ( 29.88)
Epoch: [2][100/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.5781e+00 (3.8554e+00)	Acc@1  14.06 (  8.64)	Acc@5  40.62 ( 29.81)
Epoch: [2][110/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8223e+00 (3.8482e+00)	Acc@1  11.72 (  8.76)	Acc@5  37.50 ( 30.17)
Epoch: [2][120/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8730e+00 (3.8449e+00)	Acc@1  10.16 (  8.90)	Acc@5  28.12 ( 30.22)
Epoch: [2][130/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.9180e+00 (3.8422e+00)	Acc@1   6.25 (  8.87)	Acc@5  30.47 ( 30.42)
Epoch: [2][140/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8770e+00 (3.8381e+00)	Acc@1   8.59 (  8.94)	Acc@5  29.69 ( 30.57)
Epoch: [2][150/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8926e+00 (3.8378e+00)	Acc@1   9.38 (  8.98)	Acc@5  36.72 ( 30.60)
Epoch: [2][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.7441e+00 (3.8347e+00)	Acc@1  11.72 (  9.10)	Acc@5  36.72 ( 30.78)
Epoch: [2][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.7656e+00 (3.8306e+00)	Acc@1  10.94 (  9.23)	Acc@5  34.38 ( 30.99)
Epoch: [2][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.7793e+00 (3.8278e+00)	Acc@1  12.50 (  9.26)	Acc@5  34.38 ( 31.03)
Epoch: [2][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8145e+00 (3.8249e+00)	Acc@1  11.72 (  9.31)	Acc@5  38.28 ( 31.23)
Epoch: [2][200/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8906e+00 (3.8252e+00)	Acc@1   9.38 (  9.34)	Acc@5  29.69 ( 31.24)
Epoch: [2][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.9551e+00 (3.8198e+00)	Acc@1   8.59 (  9.45)	Acc@5  27.34 ( 31.42)
Epoch: [2][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.5801e+00 (3.8162e+00)	Acc@1  10.94 (  9.50)	Acc@5  35.94 ( 31.52)
Epoch: [2][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.6504e+00 (3.8112e+00)	Acc@1  13.28 (  9.56)	Acc@5  39.06 ( 31.72)
Epoch: [2][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7617e+00 (3.8050e+00)	Acc@1  11.72 (  9.66)	Acc@5  28.91 ( 31.90)
Epoch: [2][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.6992e+00 (3.7999e+00)	Acc@1   8.59 (  9.75)	Acc@5  36.72 ( 32.07)
Epoch: [2][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7637e+00 (3.7956e+00)	Acc@1  10.16 (  9.83)	Acc@5  33.59 ( 32.16)
Epoch: [2][270/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.5508e+00 (3.7921e+00)	Acc@1  14.84 (  9.89)	Acc@5  40.62 ( 32.28)
Epoch: [2][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.6641e+00 (3.7879e+00)	Acc@1  11.72 (  9.95)	Acc@5  37.50 ( 32.46)
Epoch: [2][290/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.5020e+00 (3.7855e+00)	Acc@1  19.53 (  9.99)	Acc@5  44.53 ( 32.55)
Epoch: [2][300/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.8184e+00 (3.7809e+00)	Acc@1   9.38 ( 10.07)	Acc@5  33.59 ( 32.70)
Epoch: [2][310/391]	Time  0.109 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.6934e+00 (3.7767e+00)	Acc@1   9.38 ( 10.13)	Acc@5  38.28 ( 32.81)
Epoch: [2][320/391]	Time  0.112 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.7324e+00 (3.7740e+00)	Acc@1   6.25 ( 10.14)	Acc@5  32.81 ( 32.89)
Epoch: [2][330/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.7246e+00 (3.7700e+00)	Acc@1   8.59 ( 10.20)	Acc@5  32.03 ( 32.99)
Epoch: [2][340/391]	Time  0.116 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.7090e+00 (3.7661e+00)	Acc@1   8.59 ( 10.27)	Acc@5  29.69 ( 33.09)
Epoch: [2][350/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.7207e+00 (3.7604e+00)	Acc@1  11.72 ( 10.36)	Acc@5  35.16 ( 33.25)
Epoch: [2][360/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.5820e+00 (3.7581e+00)	Acc@1   9.38 ( 10.42)	Acc@5  38.28 ( 33.29)
Epoch: [2][370/391]	Time  0.111 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.8301e+00 (3.7547e+00)	Acc@1  14.06 ( 10.46)	Acc@5  33.59 ( 33.39)
Epoch: [2][380/391]	Time  0.113 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.3965e+00 (3.7500e+00)	Acc@1  17.97 ( 10.52)	Acc@5  46.09 ( 33.53)
Epoch: [2][390/391]	Time  0.106 ( 0.114)	Data  0.001 ( 0.004)	Loss 3.5645e+00 (3.7458e+00)	Acc@1  10.00 ( 10.62)	Acc@5  37.50 ( 33.70)
## e[2] optimizer.zero_grad (sum) time: 0.2888665199279785
## e[2]       loss.backward (sum) time: 13.714353084564209
## e[2]      optimizer.step (sum) time: 2.7071211338043213
## epoch[2] training(only) time: 44.850895166397095
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 3.5078e+00 (3.5078e+00)	Acc@1  15.00 ( 15.00)	Acc@5  39.00 ( 39.00)
Test: [ 10/100]	Time  0.040 ( 0.054)	Loss 3.7012e+00 (3.5684e+00)	Acc@1  13.00 ( 14.82)	Acc@5  38.00 ( 40.91)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 3.3828e+00 (3.5674e+00)	Acc@1  19.00 ( 15.29)	Acc@5  47.00 ( 41.95)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 3.8887e+00 (3.5779e+00)	Acc@1  13.00 ( 14.61)	Acc@5  30.00 ( 41.10)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 3.7480e+00 (3.5729e+00)	Acc@1  20.00 ( 15.37)	Acc@5  43.00 ( 41.05)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 3.2969e+00 (3.5690e+00)	Acc@1  23.00 ( 15.25)	Acc@5  47.00 ( 41.47)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 3.4785e+00 (3.5681e+00)	Acc@1  19.00 ( 15.05)	Acc@5  41.00 ( 41.16)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 3.8711e+00 (3.5684e+00)	Acc@1  17.00 ( 14.96)	Acc@5  40.00 ( 41.08)
Test: [ 80/100]	Time  0.041 ( 0.041)	Loss 3.6172e+00 (3.5767e+00)	Acc@1  12.00 ( 14.83)	Acc@5  39.00 ( 40.96)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 3.3262e+00 (3.5730e+00)	Acc@1  17.00 ( 14.68)	Acc@5  50.00 ( 41.22)
 * Acc@1 14.610 Acc@5 40.950
### epoch[2] execution time: 48.9434380531311
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.276 ( 0.276)	Data  0.160 ( 0.160)	Loss 3.6230e+00 (3.6230e+00)	Acc@1  13.28 ( 13.28)	Acc@5  41.41 ( 41.41)
Epoch: [3][ 10/391]	Time  0.115 ( 0.129)	Data  0.001 ( 0.018)	Loss 3.5352e+00 (3.5678e+00)	Acc@1  16.41 ( 14.42)	Acc@5  37.50 ( 40.34)
Epoch: [3][ 20/391]	Time  0.112 ( 0.122)	Data  0.001 ( 0.011)	Loss 3.4199e+00 (3.5671e+00)	Acc@1  11.72 ( 14.21)	Acc@5  42.97 ( 39.58)
Epoch: [3][ 30/391]	Time  0.111 ( 0.119)	Data  0.001 ( 0.009)	Loss 3.5430e+00 (3.5752e+00)	Acc@1  13.28 ( 14.14)	Acc@5  33.59 ( 39.52)
Epoch: [3][ 40/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.008)	Loss 3.7441e+00 (3.5736e+00)	Acc@1  13.28 ( 13.68)	Acc@5  35.94 ( 39.46)
Epoch: [3][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 3.4316e+00 (3.5690e+00)	Acc@1  13.28 ( 13.59)	Acc@5  40.62 ( 39.77)
Epoch: [3][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.5859e+00 (3.5561e+00)	Acc@1  14.06 ( 13.87)	Acc@5  37.50 ( 40.06)
Epoch: [3][ 70/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.2754e+00 (3.5411e+00)	Acc@1  18.75 ( 14.24)	Acc@5  49.22 ( 40.61)
Epoch: [3][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.5020e+00 (3.5385e+00)	Acc@1  11.72 ( 14.45)	Acc@5  39.84 ( 40.59)
Epoch: [3][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.3340e+00 (3.5339e+00)	Acc@1   9.38 ( 14.47)	Acc@5  45.31 ( 40.77)
Epoch: [3][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.3828e+00 (3.5285e+00)	Acc@1  21.88 ( 14.64)	Acc@5  48.44 ( 40.80)
Epoch: [3][110/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4844e+00 (3.5235e+00)	Acc@1   9.38 ( 14.67)	Acc@5  39.84 ( 40.87)
Epoch: [3][120/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4316e+00 (3.5195e+00)	Acc@1  21.09 ( 14.80)	Acc@5  42.19 ( 40.98)
Epoch: [3][130/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4043e+00 (3.5163e+00)	Acc@1  18.75 ( 14.87)	Acc@5  43.75 ( 41.06)
Epoch: [3][140/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.5000e+00 (3.5096e+00)	Acc@1  16.41 ( 14.97)	Acc@5  44.53 ( 41.34)
Epoch: [3][150/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2539e+00 (3.5050e+00)	Acc@1  21.09 ( 15.08)	Acc@5  50.78 ( 41.50)
Epoch: [3][160/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4453e+00 (3.5037e+00)	Acc@1  15.62 ( 15.16)	Acc@5  43.75 ( 41.58)
Epoch: [3][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4629e+00 (3.4971e+00)	Acc@1  13.28 ( 15.21)	Acc@5  42.97 ( 41.70)
Epoch: [3][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2715e+00 (3.4928e+00)	Acc@1  23.44 ( 15.18)	Acc@5  48.44 ( 41.83)
Epoch: [3][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.3320e+00 (3.4893e+00)	Acc@1  16.41 ( 15.20)	Acc@5  44.53 ( 41.95)
Epoch: [3][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.3320e+00 (3.4823e+00)	Acc@1  19.53 ( 15.36)	Acc@5  48.44 ( 42.23)
Epoch: [3][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2812e+00 (3.4740e+00)	Acc@1  20.31 ( 15.55)	Acc@5  47.66 ( 42.51)
Epoch: [3][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2129e+00 (3.4663e+00)	Acc@1  17.19 ( 15.62)	Acc@5  48.44 ( 42.67)
Epoch: [3][230/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4121e+00 (3.4617e+00)	Acc@1  17.97 ( 15.72)	Acc@5  43.75 ( 42.83)
Epoch: [3][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2246e+00 (3.4584e+00)	Acc@1  18.75 ( 15.74)	Acc@5  55.47 ( 42.91)
Epoch: [3][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3594e+00 (3.4557e+00)	Acc@1  13.28 ( 15.79)	Acc@5  42.97 ( 42.97)
Epoch: [3][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1895e+00 (3.4526e+00)	Acc@1  25.00 ( 15.86)	Acc@5  50.00 ( 43.01)
Epoch: [3][270/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0879e+00 (3.4490e+00)	Acc@1  18.75 ( 15.89)	Acc@5  57.03 ( 43.12)
Epoch: [3][280/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1602e+00 (3.4425e+00)	Acc@1  21.88 ( 16.02)	Acc@5  53.12 ( 43.36)
Epoch: [3][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2871e+00 (3.4369e+00)	Acc@1  18.75 ( 16.14)	Acc@5  46.88 ( 43.45)
Epoch: [3][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3379e+00 (3.4324e+00)	Acc@1  16.41 ( 16.21)	Acc@5  49.22 ( 43.57)
Epoch: [3][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4160e+00 (3.4292e+00)	Acc@1  16.41 ( 16.26)	Acc@5  46.09 ( 43.66)
Epoch: [3][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3066e+00 (3.4227e+00)	Acc@1  19.53 ( 16.34)	Acc@5  47.66 ( 43.89)
Epoch: [3][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3887e+00 (3.4166e+00)	Acc@1  18.75 ( 16.42)	Acc@5  47.66 ( 44.03)
Epoch: [3][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1406e+00 (3.4117e+00)	Acc@1  21.88 ( 16.57)	Acc@5  47.66 ( 44.15)
Epoch: [3][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9141e+00 (3.4059e+00)	Acc@1  30.47 ( 16.70)	Acc@5  60.94 ( 44.33)
Epoch: [3][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3633e+00 (3.4022e+00)	Acc@1  18.75 ( 16.79)	Acc@5  46.88 ( 44.44)
Epoch: [3][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2207e+00 (3.3978e+00)	Acc@1  15.62 ( 16.85)	Acc@5  50.78 ( 44.61)
Epoch: [3][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9648e+00 (3.3923e+00)	Acc@1  25.00 ( 16.94)	Acc@5  57.81 ( 44.73)
Epoch: [3][390/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2695e+00 (3.3869e+00)	Acc@1  16.25 ( 17.06)	Acc@5  51.25 ( 44.86)
## e[3] optimizer.zero_grad (sum) time: 0.29399800300598145
## e[3]       loss.backward (sum) time: 13.752305746078491
## e[3]      optimizer.step (sum) time: 2.7368547916412354
## epoch[3] training(only) time: 44.87804365158081
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 3.2695e+00 (3.2695e+00)	Acc@1  19.00 ( 19.00)	Acc@5  49.00 ( 49.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 3.1289e+00 (3.0923e+00)	Acc@1  20.00 ( 22.45)	Acc@5  51.00 ( 53.00)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 3.0977e+00 (3.0894e+00)	Acc@1  15.00 ( 22.19)	Acc@5  53.00 ( 53.43)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 3.3262e+00 (3.0882e+00)	Acc@1  20.00 ( 22.58)	Acc@5  50.00 ( 53.39)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 3.2246e+00 (3.0943e+00)	Acc@1  20.00 ( 22.17)	Acc@5  56.00 ( 52.98)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 2.9062e+00 (3.1011e+00)	Acc@1  28.00 ( 22.29)	Acc@5  56.00 ( 52.78)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 2.8945e+00 (3.0994e+00)	Acc@1  23.00 ( 22.21)	Acc@5  58.00 ( 52.89)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 3.3184e+00 (3.1019e+00)	Acc@1  19.00 ( 22.39)	Acc@5  45.00 ( 52.72)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 3.2676e+00 (3.1130e+00)	Acc@1  15.00 ( 22.19)	Acc@5  49.00 ( 52.33)
Test: [ 90/100]	Time  0.041 ( 0.041)	Loss 3.0664e+00 (3.1083e+00)	Acc@1  29.00 ( 22.29)	Acc@5  51.00 ( 52.52)
 * Acc@1 22.280 Acc@5 52.440
### epoch[3] execution time: 48.986753702163696
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.277 ( 0.277)	Data  0.164 ( 0.164)	Loss 3.1465e+00 (3.1465e+00)	Acc@1  15.62 ( 15.62)	Acc@5  52.34 ( 52.34)
Epoch: [4][ 10/391]	Time  0.115 ( 0.129)	Data  0.001 ( 0.018)	Loss 3.1543e+00 (3.1388e+00)	Acc@1  25.78 ( 20.31)	Acc@5  53.91 ( 52.56)
Epoch: [4][ 20/391]	Time  0.106 ( 0.122)	Data  0.001 ( 0.011)	Loss 3.0352e+00 (3.1432e+00)	Acc@1  21.09 ( 20.80)	Acc@5  52.34 ( 52.27)
Epoch: [4][ 30/391]	Time  0.111 ( 0.119)	Data  0.001 ( 0.009)	Loss 3.1719e+00 (3.1350e+00)	Acc@1  25.00 ( 21.35)	Acc@5  50.78 ( 52.24)
Epoch: [4][ 40/391]	Time  0.110 ( 0.118)	Data  0.001 ( 0.008)	Loss 3.3008e+00 (3.1420e+00)	Acc@1  16.41 ( 21.65)	Acc@5  46.09 ( 51.92)
Epoch: [4][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.9922e+00 (3.1320e+00)	Acc@1  23.44 ( 21.77)	Acc@5  50.00 ( 51.88)
Epoch: [4][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.1113e+00 (3.1251e+00)	Acc@1  27.34 ( 21.82)	Acc@5  56.25 ( 51.82)
Epoch: [4][ 70/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.3496e+00 (3.1232e+00)	Acc@1  18.75 ( 21.89)	Acc@5  44.53 ( 51.76)
Epoch: [4][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.1074e+00 (3.1289e+00)	Acc@1  21.09 ( 21.98)	Acc@5  53.12 ( 51.65)
Epoch: [4][ 90/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.1602e+00 (3.1347e+00)	Acc@1  24.22 ( 21.75)	Acc@5  50.78 ( 51.60)
Epoch: [4][100/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.1738e+00 (3.1316e+00)	Acc@1  23.44 ( 21.79)	Acc@5  50.78 ( 51.75)
Epoch: [4][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0234e+00 (3.1337e+00)	Acc@1  22.66 ( 21.83)	Acc@5  53.12 ( 51.65)
Epoch: [4][120/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9531e+00 (3.1263e+00)	Acc@1  25.00 ( 21.96)	Acc@5  62.50 ( 51.93)
Epoch: [4][130/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8066e+00 (3.1164e+00)	Acc@1  24.22 ( 22.11)	Acc@5  58.59 ( 52.22)
Epoch: [4][140/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9590e+00 (3.1155e+00)	Acc@1  25.00 ( 22.13)	Acc@5  56.25 ( 52.23)
Epoch: [4][150/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8926e+00 (3.1080e+00)	Acc@1  29.69 ( 22.31)	Acc@5  59.38 ( 52.46)
Epoch: [4][160/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2012e+00 (3.1014e+00)	Acc@1  26.56 ( 22.50)	Acc@5  52.34 ( 52.69)
Epoch: [4][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2188e+00 (3.0995e+00)	Acc@1  21.88 ( 22.48)	Acc@5  46.88 ( 52.67)
Epoch: [4][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.0215e+00 (3.0966e+00)	Acc@1  25.00 ( 22.57)	Acc@5  52.34 ( 52.74)
Epoch: [4][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9551e+00 (3.0906e+00)	Acc@1  22.66 ( 22.71)	Acc@5  57.03 ( 52.90)
Epoch: [4][200/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6738e+00 (3.0851e+00)	Acc@1  27.34 ( 22.77)	Acc@5  61.72 ( 53.00)
Epoch: [4][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9414e+00 (3.0777e+00)	Acc@1  29.69 ( 22.94)	Acc@5  49.22 ( 53.08)
Epoch: [4][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8086e+00 (3.0717e+00)	Acc@1  28.12 ( 23.06)	Acc@5  57.03 ( 53.23)
Epoch: [4][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6992e+00 (3.0673e+00)	Acc@1  25.78 ( 23.11)	Acc@5  64.84 ( 53.37)
Epoch: [4][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9668e+00 (3.0611e+00)	Acc@1  26.56 ( 23.13)	Acc@5  57.03 ( 53.53)
Epoch: [4][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0234e+00 (3.0567e+00)	Acc@1  22.66 ( 23.19)	Acc@5  56.25 ( 53.68)
Epoch: [4][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9824e+00 (3.0544e+00)	Acc@1  28.12 ( 23.23)	Acc@5  52.34 ( 53.78)
Epoch: [4][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0078e+00 (3.0498e+00)	Acc@1  20.31 ( 23.32)	Acc@5  57.81 ( 53.92)
Epoch: [4][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6211e+00 (3.0452e+00)	Acc@1  29.69 ( 23.41)	Acc@5  67.97 ( 54.03)
Epoch: [4][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9258e+00 (3.0418e+00)	Acc@1  25.00 ( 23.45)	Acc@5  54.69 ( 54.13)
Epoch: [4][300/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8262e+00 (3.0378e+00)	Acc@1  26.56 ( 23.53)	Acc@5  56.25 ( 54.26)
Epoch: [4][310/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7754e+00 (3.0321e+00)	Acc@1  26.56 ( 23.63)	Acc@5  60.16 ( 54.41)
Epoch: [4][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9258e+00 (3.0269e+00)	Acc@1  21.88 ( 23.71)	Acc@5  57.81 ( 54.55)
Epoch: [4][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7070e+00 (3.0210e+00)	Acc@1  30.47 ( 23.85)	Acc@5  59.38 ( 54.63)
Epoch: [4][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0137e+00 (3.0177e+00)	Acc@1  19.53 ( 23.85)	Acc@5  52.34 ( 54.70)
Epoch: [4][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7148e+00 (3.0144e+00)	Acc@1  28.91 ( 23.93)	Acc@5  64.06 ( 54.78)
Epoch: [4][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6680e+00 (3.0100e+00)	Acc@1  27.34 ( 24.01)	Acc@5  61.72 ( 54.90)
Epoch: [4][370/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9180e+00 (3.0057e+00)	Acc@1  27.34 ( 24.05)	Acc@5  58.59 ( 55.03)
Epoch: [4][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6367e+00 (3.0007e+00)	Acc@1  33.59 ( 24.14)	Acc@5  64.06 ( 55.12)
Epoch: [4][390/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9941e+00 (2.9978e+00)	Acc@1  30.00 ( 24.24)	Acc@5  52.50 ( 55.22)
## e[4] optimizer.zero_grad (sum) time: 0.29224681854248047
## e[4]       loss.backward (sum) time: 13.719858646392822
## e[4]      optimizer.step (sum) time: 2.6949357986450195
## epoch[4] training(only) time: 44.96217465400696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 2.9629e+00 (2.9629e+00)	Acc@1  22.00 ( 22.00)	Acc@5  60.00 ( 60.00)
Test: [ 10/100]	Time  0.041 ( 0.054)	Loss 2.8613e+00 (2.7907e+00)	Acc@1  19.00 ( 28.09)	Acc@5  55.00 ( 60.27)
Test: [ 20/100]	Time  0.037 ( 0.047)	Loss 2.5957e+00 (2.8025e+00)	Acc@1  33.00 ( 28.19)	Acc@5  64.00 ( 61.05)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.9238e+00 (2.7940e+00)	Acc@1  24.00 ( 27.71)	Acc@5  58.00 ( 61.10)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 2.7070e+00 (2.7739e+00)	Acc@1  35.00 ( 28.46)	Acc@5  65.00 ( 61.54)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 2.6641e+00 (2.7829e+00)	Acc@1  39.00 ( 28.75)	Acc@5  58.00 ( 60.80)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 2.6836e+00 (2.7803e+00)	Acc@1  32.00 ( 28.70)	Acc@5  66.00 ( 61.03)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 3.2148e+00 (2.7898e+00)	Acc@1  21.00 ( 28.66)	Acc@5  50.00 ( 60.93)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 3.0918e+00 (2.8002e+00)	Acc@1  24.00 ( 28.31)	Acc@5  55.00 ( 60.79)
Test: [ 90/100]	Time  0.041 ( 0.041)	Loss 2.7070e+00 (2.7951e+00)	Acc@1  35.00 ( 28.40)	Acc@5  64.00 ( 61.03)
 * Acc@1 28.090 Acc@5 61.010
### epoch[4] execution time: 49.0979278087616
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.271 ( 0.271)	Data  0.163 ( 0.163)	Loss 2.8535e+00 (2.8535e+00)	Acc@1  27.34 ( 27.34)	Acc@5  62.50 ( 62.50)
Epoch: [5][ 10/391]	Time  0.115 ( 0.128)	Data  0.001 ( 0.018)	Loss 2.8516e+00 (2.8029e+00)	Acc@1  25.78 ( 28.55)	Acc@5  60.94 ( 61.65)
Epoch: [5][ 20/391]	Time  0.111 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.5840e+00 (2.8146e+00)	Acc@1  32.03 ( 27.38)	Acc@5  67.19 ( 61.09)
Epoch: [5][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.9160e+00 (2.7910e+00)	Acc@1  29.69 ( 28.53)	Acc@5  54.69 ( 61.44)
Epoch: [5][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 2.7168e+00 (2.7692e+00)	Acc@1  25.78 ( 29.27)	Acc@5  58.59 ( 61.74)
Epoch: [5][ 50/391]	Time  0.110 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.5371e+00 (2.7642e+00)	Acc@1  39.06 ( 29.53)	Acc@5  68.75 ( 61.60)
Epoch: [5][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.7656e+00 (2.7520e+00)	Acc@1  25.78 ( 29.53)	Acc@5  64.84 ( 62.05)
Epoch: [5][ 70/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.5391e+00 (2.7470e+00)	Acc@1  36.72 ( 29.52)	Acc@5  64.06 ( 62.10)
Epoch: [5][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.7734e+00 (2.7442e+00)	Acc@1  30.47 ( 29.46)	Acc@5  63.28 ( 62.26)
Epoch: [5][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.7246e+00 (2.7429e+00)	Acc@1  27.34 ( 29.51)	Acc@5  61.72 ( 62.20)
Epoch: [5][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.3945e+00 (2.7395e+00)	Acc@1  35.16 ( 29.50)	Acc@5  71.09 ( 62.38)
Epoch: [5][110/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.0215e+00 (2.7404e+00)	Acc@1  21.09 ( 29.41)	Acc@5  58.59 ( 62.35)
Epoch: [5][120/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6426e+00 (2.7438e+00)	Acc@1  30.47 ( 29.38)	Acc@5  60.94 ( 62.15)
Epoch: [5][130/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6426e+00 (2.7417e+00)	Acc@1  30.47 ( 29.28)	Acc@5  64.06 ( 62.20)
Epoch: [5][140/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6465e+00 (2.7407e+00)	Acc@1  31.25 ( 29.36)	Acc@5  65.62 ( 62.15)
Epoch: [5][150/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.7598e+00 (2.7399e+00)	Acc@1  32.03 ( 29.40)	Acc@5  59.38 ( 62.17)
Epoch: [5][160/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5703e+00 (2.7396e+00)	Acc@1  31.25 ( 29.36)	Acc@5  67.19 ( 62.06)
Epoch: [5][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5840e+00 (2.7384e+00)	Acc@1  28.12 ( 29.34)	Acc@5  66.41 ( 62.07)
Epoch: [5][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5215e+00 (2.7374e+00)	Acc@1  35.16 ( 29.40)	Acc@5  64.06 ( 62.05)
Epoch: [5][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5195e+00 (2.7342e+00)	Acc@1  30.47 ( 29.50)	Acc@5  73.44 ( 62.14)
Epoch: [5][200/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5703e+00 (2.7314e+00)	Acc@1  33.59 ( 29.51)	Acc@5  67.19 ( 62.21)
Epoch: [5][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.7051e+00 (2.7276e+00)	Acc@1  29.69 ( 29.59)	Acc@5  61.72 ( 62.24)
Epoch: [5][220/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9590e+00 (2.7267e+00)	Acc@1  25.00 ( 29.61)	Acc@5  56.25 ( 62.29)
Epoch: [5][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.7148e+00 (2.7254e+00)	Acc@1  28.12 ( 29.64)	Acc@5  58.59 ( 62.28)
Epoch: [5][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5059e+00 (2.7198e+00)	Acc@1  36.72 ( 29.75)	Acc@5  67.19 ( 62.38)
Epoch: [5][250/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6367e+00 (2.7171e+00)	Acc@1  28.12 ( 29.77)	Acc@5  63.28 ( 62.44)
Epoch: [5][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5293e+00 (2.7147e+00)	Acc@1  31.25 ( 29.77)	Acc@5  64.84 ( 62.51)
Epoch: [5][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6582e+00 (2.7115e+00)	Acc@1  29.69 ( 29.85)	Acc@5  61.72 ( 62.57)
Epoch: [5][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7500e+00 (2.7083e+00)	Acc@1  27.34 ( 29.90)	Acc@5  63.28 ( 62.69)
Epoch: [5][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4043e+00 (2.6990e+00)	Acc@1  39.84 ( 30.13)	Acc@5  62.50 ( 62.86)
Epoch: [5][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5508e+00 (2.6944e+00)	Acc@1  34.38 ( 30.25)	Acc@5  64.84 ( 62.92)
Epoch: [5][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7891e+00 (2.6913e+00)	Acc@1  26.56 ( 30.33)	Acc@5  60.16 ( 62.96)
Epoch: [5][320/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3262e+00 (2.6852e+00)	Acc@1  43.75 ( 30.48)	Acc@5  72.66 ( 63.06)
Epoch: [5][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5547e+00 (2.6799e+00)	Acc@1  34.38 ( 30.60)	Acc@5  68.75 ( 63.18)
Epoch: [5][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6855e+00 (2.6755e+00)	Acc@1  26.56 ( 30.65)	Acc@5  61.72 ( 63.27)
Epoch: [5][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4688e+00 (2.6685e+00)	Acc@1  33.59 ( 30.81)	Acc@5  67.97 ( 63.41)
Epoch: [5][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7930e+00 (2.6639e+00)	Acc@1  32.81 ( 30.94)	Acc@5  60.94 ( 63.50)
Epoch: [5][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3535e+00 (2.6598e+00)	Acc@1  34.38 ( 31.06)	Acc@5  70.31 ( 63.64)
Epoch: [5][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5996e+00 (2.6573e+00)	Acc@1  35.16 ( 31.05)	Acc@5  62.50 ( 63.73)
Epoch: [5][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2910e+00 (2.6525e+00)	Acc@1  37.50 ( 31.14)	Acc@5  71.25 ( 63.86)
## e[5] optimizer.zero_grad (sum) time: 0.2928469181060791
## e[5]       loss.backward (sum) time: 13.722979307174683
## e[5]      optimizer.step (sum) time: 2.6993916034698486
## epoch[5] training(only) time: 44.95544075965881
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 2.8164e+00 (2.8164e+00)	Acc@1  33.00 ( 33.00)	Acc@5  61.00 ( 61.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 2.7246e+00 (2.6765e+00)	Acc@1  24.00 ( 32.09)	Acc@5  58.00 ( 63.64)
Test: [ 20/100]	Time  0.040 ( 0.046)	Loss 2.5820e+00 (2.6721e+00)	Acc@1  36.00 ( 32.48)	Acc@5  66.00 ( 63.62)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.6816e+00 (2.6680e+00)	Acc@1  28.00 ( 32.16)	Acc@5  65.00 ( 64.10)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 2.6035e+00 (2.6770e+00)	Acc@1  40.00 ( 32.37)	Acc@5  65.00 ( 64.12)
Test: [ 50/100]	Time  0.041 ( 0.042)	Loss 2.6250e+00 (2.6879e+00)	Acc@1  40.00 ( 32.51)	Acc@5  60.00 ( 63.71)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 2.7402e+00 (2.6873e+00)	Acc@1  32.00 ( 32.59)	Acc@5  62.00 ( 63.59)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 3.2402e+00 (2.6983e+00)	Acc@1  29.00 ( 32.32)	Acc@5  60.00 ( 63.41)
Test: [ 80/100]	Time  0.038 ( 0.040)	Loss 2.7969e+00 (2.7081e+00)	Acc@1  29.00 ( 32.15)	Acc@5  63.00 ( 63.27)
Test: [ 90/100]	Time  0.036 ( 0.040)	Loss 2.7227e+00 (2.6996e+00)	Acc@1  29.00 ( 32.18)	Acc@5  60.00 ( 63.40)
 * Acc@1 32.100 Acc@5 63.440
### epoch[5] execution time: 49.01766872406006
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.269 ( 0.269)	Data  0.155 ( 0.155)	Loss 2.2773e+00 (2.2773e+00)	Acc@1  38.28 ( 38.28)	Acc@5  71.09 ( 71.09)
Epoch: [6][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.017)	Loss 2.1836e+00 (2.3693e+00)	Acc@1  40.62 ( 37.14)	Acc@5  75.78 ( 71.09)
Epoch: [6][ 20/391]	Time  0.104 ( 0.121)	Data  0.001 ( 0.011)	Loss 2.4609e+00 (2.3716e+00)	Acc@1  31.25 ( 37.24)	Acc@5  67.19 ( 70.31)
Epoch: [6][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.8320e+00 (2.4080e+00)	Acc@1  27.34 ( 36.72)	Acc@5  59.38 ( 69.30)
Epoch: [6][ 40/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.008)	Loss 2.5234e+00 (2.4097e+00)	Acc@1  31.25 ( 36.36)	Acc@5  61.72 ( 68.96)
Epoch: [6][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.3926e+00 (2.4086e+00)	Acc@1  35.94 ( 36.32)	Acc@5  73.44 ( 68.75)
Epoch: [6][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.5586e+00 (2.4020e+00)	Acc@1  32.81 ( 36.40)	Acc@5  67.19 ( 69.19)
Epoch: [6][ 70/391]	Time  0.118 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.5762e+00 (2.4029e+00)	Acc@1  39.06 ( 36.52)	Acc@5  62.50 ( 69.23)
Epoch: [6][ 80/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.3828e+00 (2.3985e+00)	Acc@1  37.50 ( 36.43)	Acc@5  69.53 ( 69.36)
Epoch: [6][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.5898e+00 (2.3996e+00)	Acc@1  33.59 ( 36.42)	Acc@5  64.84 ( 69.49)
Epoch: [6][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.5566e+00 (2.3934e+00)	Acc@1  29.69 ( 36.47)	Acc@5  67.19 ( 69.59)
Epoch: [6][110/391]	Time  0.109 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2656e+00 (2.3987e+00)	Acc@1  39.06 ( 36.21)	Acc@5  75.78 ( 69.52)
Epoch: [6][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4238e+00 (2.3926e+00)	Acc@1  32.81 ( 36.22)	Acc@5  72.66 ( 69.75)
Epoch: [6][130/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0957e+00 (2.3862e+00)	Acc@1  44.53 ( 36.37)	Acc@5  80.47 ( 69.88)
Epoch: [6][140/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0625e+00 (2.3821e+00)	Acc@1  43.75 ( 36.55)	Acc@5  72.66 ( 69.95)
Epoch: [6][150/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2773e+00 (2.3838e+00)	Acc@1  39.84 ( 36.55)	Acc@5  71.09 ( 69.95)
Epoch: [6][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2090e+00 (2.3805e+00)	Acc@1  37.50 ( 36.66)	Acc@5  77.34 ( 70.09)
Epoch: [6][170/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4316e+00 (2.3743e+00)	Acc@1  35.94 ( 36.88)	Acc@5  73.44 ( 70.18)
Epoch: [6][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5391e+00 (2.3733e+00)	Acc@1  33.59 ( 36.96)	Acc@5  71.09 ( 70.19)
Epoch: [6][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1758e+00 (2.3683e+00)	Acc@1  42.19 ( 37.03)	Acc@5  67.97 ( 70.25)
Epoch: [6][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1875e+00 (2.3632e+00)	Acc@1  42.97 ( 37.12)	Acc@5  72.66 ( 70.36)
Epoch: [6][210/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2305e+00 (2.3599e+00)	Acc@1  32.03 ( 37.16)	Acc@5  72.66 ( 70.46)
Epoch: [6][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1504e+00 (2.3561e+00)	Acc@1  40.62 ( 37.28)	Acc@5  70.31 ( 70.60)
Epoch: [6][230/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0957e+00 (2.3511e+00)	Acc@1  39.84 ( 37.41)	Acc@5  81.25 ( 70.71)
Epoch: [6][240/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3379e+00 (2.3479e+00)	Acc@1  42.19 ( 37.50)	Acc@5  75.00 ( 70.81)
Epoch: [6][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5312e+00 (2.3519e+00)	Acc@1  39.84 ( 37.42)	Acc@5  66.41 ( 70.70)
Epoch: [6][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3301e+00 (2.3513e+00)	Acc@1  36.72 ( 37.48)	Acc@5  72.66 ( 70.72)
Epoch: [6][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3477e+00 (2.3473e+00)	Acc@1  34.38 ( 37.51)	Acc@5  74.22 ( 70.85)
Epoch: [6][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2031e+00 (2.3443e+00)	Acc@1  44.53 ( 37.61)	Acc@5  73.44 ( 70.94)
Epoch: [6][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3555e+00 (2.3422e+00)	Acc@1  38.28 ( 37.65)	Acc@5  65.62 ( 71.02)
Epoch: [6][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4297e+00 (2.3395e+00)	Acc@1  32.81 ( 37.68)	Acc@5  67.19 ( 71.09)
Epoch: [6][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9502e+00 (2.3375e+00)	Acc@1  46.88 ( 37.76)	Acc@5  78.91 ( 71.12)
Epoch: [6][320/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1895e+00 (2.3323e+00)	Acc@1  35.94 ( 37.85)	Acc@5  75.00 ( 71.24)
Epoch: [6][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1504e+00 (2.3328e+00)	Acc@1  43.75 ( 37.85)	Acc@5  75.78 ( 71.23)
Epoch: [6][340/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1113e+00 (2.3284e+00)	Acc@1  50.00 ( 37.95)	Acc@5  77.34 ( 71.30)
Epoch: [6][350/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9512e+00 (2.3240e+00)	Acc@1  45.31 ( 38.05)	Acc@5  82.81 ( 71.39)
Epoch: [6][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1934e+00 (2.3229e+00)	Acc@1  43.75 ( 38.11)	Acc@5  74.22 ( 71.41)
Epoch: [6][370/391]	Time  0.104 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0371e+00 (2.3225e+00)	Acc@1  47.66 ( 38.14)	Acc@5  78.12 ( 71.44)
Epoch: [6][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0781e+00 (2.3191e+00)	Acc@1  43.75 ( 38.23)	Acc@5  74.22 ( 71.49)
Epoch: [6][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4219e+00 (2.3176e+00)	Acc@1  37.50 ( 38.22)	Acc@5  71.25 ( 71.54)
## e[6] optimizer.zero_grad (sum) time: 0.29360151290893555
## e[6]       loss.backward (sum) time: 13.746336936950684
## e[6]      optimizer.step (sum) time: 2.7146363258361816
## epoch[6] training(only) time: 44.95405888557434
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.3145e+00 (2.3145e+00)	Acc@1  44.00 ( 44.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 2.1797e+00 (2.3540e+00)	Acc@1  40.00 ( 39.82)	Acc@5  80.00 ( 72.00)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 2.1602e+00 (2.3351e+00)	Acc@1  39.00 ( 39.62)	Acc@5  77.00 ( 71.71)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 2.2969e+00 (2.3357e+00)	Acc@1  39.00 ( 39.26)	Acc@5  71.00 ( 71.74)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 2.4043e+00 (2.3199e+00)	Acc@1  41.00 ( 39.66)	Acc@5  69.00 ( 71.98)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 2.3340e+00 (2.3313e+00)	Acc@1  42.00 ( 39.67)	Acc@5  64.00 ( 71.67)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 2.3066e+00 (2.3217e+00)	Acc@1  43.00 ( 39.79)	Acc@5  71.00 ( 71.64)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 2.6816e+00 (2.3370e+00)	Acc@1  29.00 ( 39.58)	Acc@5  68.00 ( 71.15)
Test: [ 80/100]	Time  0.037 ( 0.041)	Loss 2.5117e+00 (2.3478e+00)	Acc@1  33.00 ( 39.10)	Acc@5  63.00 ( 70.73)
Test: [ 90/100]	Time  0.041 ( 0.041)	Loss 2.5000e+00 (2.3454e+00)	Acc@1  35.00 ( 39.05)	Acc@5  71.00 ( 71.04)
 * Acc@1 38.960 Acc@5 70.860
### epoch[6] execution time: 49.08390235900879
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.273 ( 0.273)	Data  0.163 ( 0.163)	Loss 2.1816e+00 (2.1816e+00)	Acc@1  42.97 ( 42.97)	Acc@5  75.00 ( 75.00)
Epoch: [7][ 10/391]	Time  0.111 ( 0.128)	Data  0.001 ( 0.018)	Loss 2.3438e+00 (2.1028e+00)	Acc@1  40.62 ( 42.40)	Acc@5  68.75 ( 75.36)
Epoch: [7][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.0176e+00 (2.1206e+00)	Acc@1  46.88 ( 41.78)	Acc@5  79.69 ( 75.63)
Epoch: [7][ 30/391]	Time  0.117 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.1211e+00 (2.1301e+00)	Acc@1  42.97 ( 42.29)	Acc@5  73.44 ( 75.35)
Epoch: [7][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.9746e+00 (2.1375e+00)	Acc@1  46.09 ( 42.11)	Acc@5  75.78 ( 75.21)
Epoch: [7][ 50/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.1992e+00 (2.1408e+00)	Acc@1  42.19 ( 42.23)	Acc@5  75.00 ( 75.06)
Epoch: [7][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.2383e+00 (2.1343e+00)	Acc@1  35.16 ( 41.96)	Acc@5  74.22 ( 75.42)
Epoch: [7][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.0469e+00 (2.1146e+00)	Acc@1  44.53 ( 42.29)	Acc@5  77.34 ( 75.78)
Epoch: [7][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.1172e+00 (2.1184e+00)	Acc@1  43.75 ( 42.29)	Acc@5  80.47 ( 75.90)
Epoch: [7][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.0469e+00 (2.1143e+00)	Acc@1  49.22 ( 42.49)	Acc@5  77.34 ( 75.95)
Epoch: [7][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9883e+00 (2.1107e+00)	Acc@1  40.62 ( 42.45)	Acc@5  75.78 ( 75.95)
Epoch: [7][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0312e+00 (2.1120e+00)	Acc@1  44.53 ( 42.41)	Acc@5  75.78 ( 75.86)
Epoch: [7][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9180e+00 (2.1022e+00)	Acc@1  46.09 ( 42.55)	Acc@5  78.91 ( 76.08)
Epoch: [7][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.1562e+00 (2.1029e+00)	Acc@1  42.19 ( 42.44)	Acc@5  73.44 ( 76.07)
Epoch: [7][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0488e+00 (2.1022e+00)	Acc@1  43.75 ( 42.53)	Acc@5  74.22 ( 76.01)
Epoch: [7][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.1348e+00 (2.1000e+00)	Acc@1  41.41 ( 42.66)	Acc@5  75.78 ( 76.00)
Epoch: [7][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1016e+00 (2.0969e+00)	Acc@1  42.97 ( 42.75)	Acc@5  68.75 ( 75.99)
Epoch: [7][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2363e+00 (2.0952e+00)	Acc@1  42.97 ( 42.82)	Acc@5  69.53 ( 76.02)
Epoch: [7][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1992e+00 (2.0935e+00)	Acc@1  40.62 ( 42.81)	Acc@5  75.00 ( 76.01)
Epoch: [7][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1484e+00 (2.0952e+00)	Acc@1  42.19 ( 42.74)	Acc@5  77.34 ( 76.00)
Epoch: [7][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1035e+00 (2.0945e+00)	Acc@1  43.75 ( 42.79)	Acc@5  73.44 ( 76.04)
Epoch: [7][210/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9502e+00 (2.0923e+00)	Acc@1  47.66 ( 42.78)	Acc@5  79.69 ( 76.03)
Epoch: [7][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9248e+00 (2.0911e+00)	Acc@1  50.78 ( 42.86)	Acc@5  77.34 ( 76.03)
Epoch: [7][230/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7725e+00 (2.0899e+00)	Acc@1  50.00 ( 42.95)	Acc@5  81.25 ( 76.06)
Epoch: [7][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0410e+00 (2.0873e+00)	Acc@1  46.88 ( 43.07)	Acc@5  74.22 ( 76.07)
Epoch: [7][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9180e+00 (2.0822e+00)	Acc@1  42.19 ( 43.17)	Acc@5  80.47 ( 76.16)
Epoch: [7][260/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0488e+00 (2.0806e+00)	Acc@1  45.31 ( 43.15)	Acc@5  74.22 ( 76.19)
Epoch: [7][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0195e+00 (2.0786e+00)	Acc@1  48.44 ( 43.24)	Acc@5  76.56 ( 76.29)
Epoch: [7][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1055e+00 (2.0770e+00)	Acc@1  39.06 ( 43.27)	Acc@5  78.91 ( 76.30)
Epoch: [7][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7988e+00 (2.0727e+00)	Acc@1  53.91 ( 43.35)	Acc@5  78.91 ( 76.39)
Epoch: [7][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1270e+00 (2.0691e+00)	Acc@1  49.22 ( 43.47)	Acc@5  75.78 ( 76.46)
Epoch: [7][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9961e+00 (2.0644e+00)	Acc@1  44.53 ( 43.60)	Acc@5  82.03 ( 76.54)
Epoch: [7][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9395e+00 (2.0625e+00)	Acc@1  42.19 ( 43.64)	Acc@5  79.69 ( 76.54)
Epoch: [7][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3164e+00 (2.0616e+00)	Acc@1  34.38 ( 43.61)	Acc@5  75.78 ( 76.55)
Epoch: [7][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9805e+00 (2.0597e+00)	Acc@1  46.09 ( 43.60)	Acc@5  77.34 ( 76.58)
Epoch: [7][350/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9092e+00 (2.0561e+00)	Acc@1  50.00 ( 43.73)	Acc@5  82.81 ( 76.69)
Epoch: [7][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0410e+00 (2.0538e+00)	Acc@1  40.62 ( 43.77)	Acc@5  78.12 ( 76.77)
Epoch: [7][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0820e+00 (2.0532e+00)	Acc@1  46.88 ( 43.79)	Acc@5  78.91 ( 76.78)
Epoch: [7][380/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0586e+00 (2.0529e+00)	Acc@1  47.66 ( 43.80)	Acc@5  75.00 ( 76.72)
Epoch: [7][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0898e+00 (2.0518e+00)	Acc@1  48.75 ( 43.84)	Acc@5  75.00 ( 76.75)
## e[7] optimizer.zero_grad (sum) time: 0.2921712398529053
## e[7]       loss.backward (sum) time: 13.747859001159668
## e[7]      optimizer.step (sum) time: 2.701374053955078
## epoch[7] training(only) time: 45.038883447647095
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.0527e+00 (2.0527e+00)	Acc@1  42.00 ( 42.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.042 ( 0.053)	Loss 2.0488e+00 (2.0106e+00)	Acc@1  44.00 ( 46.18)	Acc@5  79.00 ( 78.73)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8184e+00 (1.9945e+00)	Acc@1  51.00 ( 46.38)	Acc@5  78.00 ( 78.33)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 2.2109e+00 (1.9980e+00)	Acc@1  40.00 ( 46.10)	Acc@5  72.00 ( 78.19)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.8965e+00 (1.9932e+00)	Acc@1  56.00 ( 46.27)	Acc@5  79.00 ( 77.98)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.7480e+00 (2.0075e+00)	Acc@1  54.00 ( 45.88)	Acc@5  78.00 ( 77.45)
Test: [ 60/100]	Time  0.037 ( 0.042)	Loss 1.9961e+00 (1.9939e+00)	Acc@1  45.00 ( 46.05)	Acc@5  82.00 ( 77.43)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 2.1152e+00 (1.9993e+00)	Acc@1  47.00 ( 46.04)	Acc@5  74.00 ( 77.42)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.2109e+00 (2.0051e+00)	Acc@1  44.00 ( 45.91)	Acc@5  72.00 ( 77.30)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.0645e+00 (1.9969e+00)	Acc@1  50.00 ( 46.40)	Acc@5  72.00 ( 77.40)
 * Acc@1 46.490 Acc@5 77.460
### epoch[7] execution time: 49.16085934638977
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.276 ( 0.276)	Data  0.150 ( 0.150)	Loss 1.6602e+00 (1.6602e+00)	Acc@1  53.91 ( 53.91)	Acc@5  85.94 ( 85.94)
Epoch: [8][ 10/391]	Time  0.114 ( 0.129)	Data  0.001 ( 0.017)	Loss 1.8730e+00 (1.8736e+00)	Acc@1  50.00 ( 48.30)	Acc@5  78.12 ( 79.40)
Epoch: [8][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.6611e+00 (1.8186e+00)	Acc@1  53.91 ( 49.48)	Acc@5  84.38 ( 80.58)
Epoch: [8][ 30/391]	Time  0.113 ( 0.120)	Data  0.001 ( 0.008)	Loss 1.9961e+00 (1.8278e+00)	Acc@1  40.62 ( 49.34)	Acc@5  78.91 ( 80.44)
Epoch: [8][ 40/391]	Time  0.110 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.8271e+00 (1.8429e+00)	Acc@1  48.44 ( 48.55)	Acc@5  80.47 ( 80.37)
Epoch: [8][ 50/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.9434e+00 (1.8660e+00)	Acc@1  42.97 ( 48.02)	Acc@5  75.78 ( 79.81)
Epoch: [8][ 60/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.9990e+00 (1.8703e+00)	Acc@1  42.19 ( 47.91)	Acc@5  75.78 ( 79.78)
Epoch: [8][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.8564e+00 (1.8719e+00)	Acc@1  43.75 ( 47.78)	Acc@5  83.59 ( 79.93)
Epoch: [8][ 80/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.9150e+00 (1.8723e+00)	Acc@1  52.34 ( 47.83)	Acc@5  77.34 ( 79.84)
Epoch: [8][ 90/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5566e+00 (1.8565e+00)	Acc@1  53.12 ( 48.09)	Acc@5  83.59 ( 80.28)
Epoch: [8][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0762e+00 (1.8555e+00)	Acc@1  45.31 ( 48.27)	Acc@5  74.22 ( 80.43)
Epoch: [8][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0098e+00 (1.8581e+00)	Acc@1  49.22 ( 48.28)	Acc@5  76.56 ( 80.33)
Epoch: [8][120/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9219e+00 (1.8634e+00)	Acc@1  49.22 ( 48.13)	Acc@5  81.25 ( 80.29)
Epoch: [8][130/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7734e+00 (1.8598e+00)	Acc@1  52.34 ( 48.23)	Acc@5  81.25 ( 80.43)
Epoch: [8][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8115e+00 (1.8607e+00)	Acc@1  49.22 ( 48.22)	Acc@5  78.12 ( 80.43)
Epoch: [8][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9912e+00 (1.8628e+00)	Acc@1  50.78 ( 48.31)	Acc@5  77.34 ( 80.35)
Epoch: [8][160/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9053e+00 (1.8644e+00)	Acc@1  49.22 ( 48.19)	Acc@5  80.47 ( 80.28)
Epoch: [8][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0723e+00 (1.8639e+00)	Acc@1  47.66 ( 48.23)	Acc@5  77.34 ( 80.31)
Epoch: [8][180/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8223e+00 (1.8622e+00)	Acc@1  46.09 ( 48.23)	Acc@5  81.25 ( 80.43)
Epoch: [8][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0488e+00 (1.8674e+00)	Acc@1  43.75 ( 48.15)	Acc@5  73.44 ( 80.29)
Epoch: [8][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0352e+00 (1.8652e+00)	Acc@1  42.97 ( 48.30)	Acc@5  78.91 ( 80.31)
Epoch: [8][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8926e+00 (1.8614e+00)	Acc@1  47.66 ( 48.44)	Acc@5  80.47 ( 80.31)
Epoch: [8][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5918e+00 (1.8594e+00)	Acc@1  53.91 ( 48.48)	Acc@5  88.28 ( 80.40)
Epoch: [8][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6152e+00 (1.8581e+00)	Acc@1  53.12 ( 48.46)	Acc@5  82.81 ( 80.42)
Epoch: [8][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9268e+00 (1.8578e+00)	Acc@1  48.44 ( 48.45)	Acc@5  78.91 ( 80.39)
Epoch: [8][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8887e+00 (1.8571e+00)	Acc@1  44.53 ( 48.48)	Acc@5  79.69 ( 80.36)
Epoch: [8][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9395e+00 (1.8576e+00)	Acc@1  47.66 ( 48.44)	Acc@5  78.12 ( 80.32)
Epoch: [8][270/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7627e+00 (1.8558e+00)	Acc@1  45.31 ( 48.44)	Acc@5  78.12 ( 80.37)
Epoch: [8][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6865e+00 (1.8545e+00)	Acc@1  52.34 ( 48.45)	Acc@5  83.59 ( 80.38)
Epoch: [8][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8799e+00 (1.8526e+00)	Acc@1  46.88 ( 48.46)	Acc@5  81.25 ( 80.43)
Epoch: [8][300/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7080e+00 (1.8506e+00)	Acc@1  49.22 ( 48.46)	Acc@5  82.81 ( 80.47)
Epoch: [8][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1309e+00 (1.8519e+00)	Acc@1  42.19 ( 48.41)	Acc@5  75.00 ( 80.47)
Epoch: [8][320/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7529e+00 (1.8488e+00)	Acc@1  47.66 ( 48.47)	Acc@5  84.38 ( 80.55)
Epoch: [8][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8447e+00 (1.8488e+00)	Acc@1  46.88 ( 48.50)	Acc@5  77.34 ( 80.52)
Epoch: [8][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0469e+00 (1.8474e+00)	Acc@1  46.09 ( 48.51)	Acc@5  76.56 ( 80.63)
Epoch: [8][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5947e+00 (1.8467e+00)	Acc@1  54.69 ( 48.52)	Acc@5  84.38 ( 80.67)
Epoch: [8][360/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7598e+00 (1.8474e+00)	Acc@1  57.81 ( 48.55)	Acc@5  80.47 ( 80.63)
Epoch: [8][370/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6963e+00 (1.8459e+00)	Acc@1  55.47 ( 48.59)	Acc@5  81.25 ( 80.66)
Epoch: [8][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9102e+00 (1.8464e+00)	Acc@1  47.66 ( 48.57)	Acc@5  79.69 ( 80.67)
Epoch: [8][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8516e+00 (1.8463e+00)	Acc@1  46.25 ( 48.61)	Acc@5  81.25 ( 80.66)
## e[8] optimizer.zero_grad (sum) time: 0.29194092750549316
## e[8]       loss.backward (sum) time: 13.720069169998169
## e[8]      optimizer.step (sum) time: 2.721524953842163
## epoch[8] training(only) time: 45.09022879600525
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.9160e+00 (1.9160e+00)	Acc@1  49.00 ( 49.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.8408e+00 (1.8061e+00)	Acc@1  54.00 ( 52.73)	Acc@5  83.00 ( 80.55)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.5977e+00 (1.7970e+00)	Acc@1  53.00 ( 51.52)	Acc@5  86.00 ( 81.43)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 1.9053e+00 (1.8059e+00)	Acc@1  51.00 ( 50.45)	Acc@5  79.00 ( 81.45)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.9443e+00 (1.8237e+00)	Acc@1  53.00 ( 50.12)	Acc@5  80.00 ( 81.39)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.6982e+00 (1.8499e+00)	Acc@1  52.00 ( 49.49)	Acc@5  81.00 ( 80.80)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 1.8652e+00 (1.8361e+00)	Acc@1  50.00 ( 49.90)	Acc@5  81.00 ( 80.75)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.9521e+00 (1.8402e+00)	Acc@1  44.00 ( 49.80)	Acc@5  78.00 ( 80.73)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.9229e+00 (1.8439e+00)	Acc@1  43.00 ( 49.73)	Acc@5  80.00 ( 80.52)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.0547e+00 (1.8405e+00)	Acc@1  43.00 ( 49.62)	Acc@5  79.00 ( 80.58)
 * Acc@1 49.730 Acc@5 80.730
### epoch[8] execution time: 49.19921684265137
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.270 ( 0.270)	Data  0.163 ( 0.163)	Loss 1.6182e+00 (1.6182e+00)	Acc@1  53.91 ( 53.91)	Acc@5  83.59 ( 83.59)
Epoch: [9][ 10/391]	Time  0.112 ( 0.128)	Data  0.001 ( 0.018)	Loss 1.7832e+00 (1.6644e+00)	Acc@1  48.44 ( 52.77)	Acc@5  84.38 ( 84.45)
Epoch: [9][ 20/391]	Time  0.113 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.7148e+00 (1.6797e+00)	Acc@1  50.78 ( 53.12)	Acc@5  82.03 ( 84.30)
Epoch: [9][ 30/391]	Time  0.115 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.7793e+00 (1.6636e+00)	Acc@1  52.34 ( 53.65)	Acc@5  79.69 ( 84.15)
Epoch: [9][ 40/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.5547e+00 (1.6709e+00)	Acc@1  60.16 ( 53.33)	Acc@5  86.72 ( 83.86)
Epoch: [9][ 50/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.6680e+00 (1.6745e+00)	Acc@1  56.25 ( 53.11)	Acc@5  86.72 ( 83.82)
Epoch: [9][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.8193e+00 (1.6759e+00)	Acc@1  42.97 ( 52.96)	Acc@5  82.03 ( 83.75)
Epoch: [9][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.7139e+00 (1.6840e+00)	Acc@1  57.81 ( 52.77)	Acc@5  78.12 ( 83.58)
Epoch: [9][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.6074e+00 (1.6837e+00)	Acc@1  48.44 ( 52.47)	Acc@5  85.16 ( 83.62)
Epoch: [9][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.9072e+00 (1.6866e+00)	Acc@1  43.75 ( 52.34)	Acc@5  81.25 ( 83.61)
Epoch: [9][100/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7666e+00 (1.6899e+00)	Acc@1  52.34 ( 52.34)	Acc@5  83.59 ( 83.58)
Epoch: [9][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7344e+00 (1.6864e+00)	Acc@1  52.34 ( 52.55)	Acc@5  83.59 ( 83.56)
Epoch: [9][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5703e+00 (1.6907e+00)	Acc@1  53.12 ( 52.42)	Acc@5  86.72 ( 83.56)
Epoch: [9][130/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9609e+00 (1.6883e+00)	Acc@1  46.88 ( 52.50)	Acc@5  79.69 ( 83.64)
Epoch: [9][140/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5801e+00 (1.6868e+00)	Acc@1  51.56 ( 52.65)	Acc@5  84.38 ( 83.64)
Epoch: [9][150/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7920e+00 (1.6865e+00)	Acc@1  53.12 ( 52.66)	Acc@5  78.12 ( 83.68)
Epoch: [9][160/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6631e+00 (1.6858e+00)	Acc@1  53.91 ( 52.58)	Acc@5  85.16 ( 83.71)
Epoch: [9][170/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5498e+00 (1.6849e+00)	Acc@1  57.03 ( 52.69)	Acc@5  87.50 ( 83.70)
Epoch: [9][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7705e+00 (1.6836e+00)	Acc@1  45.31 ( 52.72)	Acc@5  80.47 ( 83.64)
Epoch: [9][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4824e+00 (1.6837e+00)	Acc@1  60.16 ( 52.74)	Acc@5  86.72 ( 83.62)
Epoch: [9][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6982e+00 (1.6819e+00)	Acc@1  50.78 ( 52.85)	Acc@5  86.72 ( 83.66)
Epoch: [9][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7969e+00 (1.6819e+00)	Acc@1  48.44 ( 52.81)	Acc@5  80.47 ( 83.65)
Epoch: [9][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6416e+00 (1.6845e+00)	Acc@1  52.34 ( 52.72)	Acc@5  85.16 ( 83.61)
Epoch: [9][230/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6963e+00 (1.6873e+00)	Acc@1  52.34 ( 52.61)	Acc@5  84.38 ( 83.56)
Epoch: [9][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6230e+00 (1.6887e+00)	Acc@1  55.47 ( 52.58)	Acc@5  85.16 ( 83.55)
Epoch: [9][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5771e+00 (1.6857e+00)	Acc@1  53.91 ( 52.61)	Acc@5  88.28 ( 83.62)
Epoch: [9][260/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0312e+00 (1.6870e+00)	Acc@1  43.75 ( 52.62)	Acc@5  72.66 ( 83.56)
Epoch: [9][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6104e+00 (1.6884e+00)	Acc@1  52.34 ( 52.61)	Acc@5  85.16 ( 83.55)
Epoch: [9][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5850e+00 (1.6872e+00)	Acc@1  51.56 ( 52.61)	Acc@5  85.94 ( 83.59)
Epoch: [9][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8750e+00 (1.6884e+00)	Acc@1  44.53 ( 52.54)	Acc@5  78.91 ( 83.54)
Epoch: [9][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6562e+00 (1.6866e+00)	Acc@1  55.47 ( 52.55)	Acc@5  84.38 ( 83.60)
Epoch: [9][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6699e+00 (1.6859e+00)	Acc@1  53.12 ( 52.57)	Acc@5  85.16 ( 83.61)
Epoch: [9][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7441e+00 (1.6856e+00)	Acc@1  57.03 ( 52.58)	Acc@5  84.38 ( 83.60)
Epoch: [9][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5137e+00 (1.6845e+00)	Acc@1  51.56 ( 52.59)	Acc@5  89.84 ( 83.64)
Epoch: [9][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6396e+00 (1.6856e+00)	Acc@1  48.44 ( 52.56)	Acc@5  85.16 ( 83.63)
Epoch: [9][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4111e+00 (1.6837e+00)	Acc@1  61.72 ( 52.60)	Acc@5  89.84 ( 83.67)
Epoch: [9][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5273e+00 (1.6824e+00)	Acc@1  56.25 ( 52.61)	Acc@5  84.38 ( 83.68)
Epoch: [9][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7090e+00 (1.6829e+00)	Acc@1  54.69 ( 52.63)	Acc@5  86.72 ( 83.66)
Epoch: [9][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6162e+00 (1.6816e+00)	Acc@1  53.12 ( 52.69)	Acc@5  86.72 ( 83.68)
Epoch: [9][390/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7051e+00 (1.6822e+00)	Acc@1  55.00 ( 52.72)	Acc@5  80.00 ( 83.64)
## e[9] optimizer.zero_grad (sum) time: 0.2935974597930908
## e[9]       loss.backward (sum) time: 13.732284307479858
## e[9]      optimizer.step (sum) time: 2.7128844261169434
## epoch[9] training(only) time: 45.075552225112915
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.8193e+00 (1.8193e+00)	Acc@1  52.00 ( 52.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.038 ( 0.054)	Loss 1.9482e+00 (1.7831e+00)	Acc@1  47.00 ( 52.64)	Acc@5  81.00 ( 81.91)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.6924e+00 (1.7691e+00)	Acc@1  54.00 ( 52.62)	Acc@5  86.00 ( 83.19)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 1.7617e+00 (1.7548e+00)	Acc@1  48.00 ( 52.35)	Acc@5  79.00 ( 82.94)
Test: [ 40/100]	Time  0.040 ( 0.043)	Loss 1.8477e+00 (1.7611e+00)	Acc@1  50.00 ( 51.54)	Acc@5  85.00 ( 82.93)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.7461e+00 (1.7718e+00)	Acc@1  55.00 ( 51.71)	Acc@5  80.00 ( 82.33)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.7705e+00 (1.7568e+00)	Acc@1  52.00 ( 51.59)	Acc@5  84.00 ( 82.56)
Test: [ 70/100]	Time  0.037 ( 0.041)	Loss 1.8379e+00 (1.7682e+00)	Acc@1  47.00 ( 51.37)	Acc@5  82.00 ( 82.37)
Test: [ 80/100]	Time  0.037 ( 0.041)	Loss 1.7373e+00 (1.7714e+00)	Acc@1  42.00 ( 51.26)	Acc@5  82.00 ( 82.38)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 1.9092e+00 (1.7633e+00)	Acc@1  48.00 ( 51.57)	Acc@5  79.00 ( 82.46)
 * Acc@1 51.510 Acc@5 82.510
### epoch[9] execution time: 49.20035171508789
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.261 ( 0.261)	Data  0.155 ( 0.155)	Loss 1.5020e+00 (1.5020e+00)	Acc@1  54.69 ( 54.69)	Acc@5  86.72 ( 86.72)
Epoch: [10][ 10/391]	Time  0.115 ( 0.127)	Data  0.001 ( 0.017)	Loss 1.3721e+00 (1.5475e+00)	Acc@1  60.94 ( 55.68)	Acc@5  87.50 ( 85.87)
Epoch: [10][ 20/391]	Time  0.114 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.6309e+00 (1.5431e+00)	Acc@1  57.81 ( 56.47)	Acc@5  82.81 ( 85.90)
Epoch: [10][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.7041e+00 (1.5306e+00)	Acc@1  50.78 ( 56.05)	Acc@5  82.81 ( 86.32)
Epoch: [10][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.2314e+00 (1.5176e+00)	Acc@1  66.41 ( 56.78)	Acc@5  89.06 ( 86.30)
Epoch: [10][ 50/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.6934e+00 (1.5167e+00)	Acc@1  51.56 ( 56.83)	Acc@5  84.38 ( 86.47)
Epoch: [10][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.5508e+00 (1.5165e+00)	Acc@1  57.81 ( 56.88)	Acc@5  89.06 ( 86.44)
Epoch: [10][ 70/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.6260e+00 (1.5230e+00)	Acc@1  52.34 ( 56.77)	Acc@5  82.81 ( 86.09)
Epoch: [10][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.7217e+00 (1.5358e+00)	Acc@1  54.69 ( 56.37)	Acc@5  83.59 ( 86.10)
Epoch: [10][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4326e+00 (1.5404e+00)	Acc@1  59.38 ( 56.28)	Acc@5  88.28 ( 85.99)
Epoch: [10][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6797e+00 (1.5360e+00)	Acc@1  51.56 ( 56.27)	Acc@5  85.94 ( 86.08)
Epoch: [10][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6133e+00 (1.5422e+00)	Acc@1  50.78 ( 56.19)	Acc@5  87.50 ( 85.98)
Epoch: [10][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4941e+00 (1.5389e+00)	Acc@1  60.94 ( 56.31)	Acc@5  85.94 ( 85.95)
Epoch: [10][130/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3936e+00 (1.5375e+00)	Acc@1  57.03 ( 56.30)	Acc@5  88.28 ( 85.96)
Epoch: [10][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6738e+00 (1.5463e+00)	Acc@1  56.25 ( 56.06)	Acc@5  78.91 ( 85.80)
Epoch: [10][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5107e+00 (1.5479e+00)	Acc@1  63.28 ( 55.99)	Acc@5  83.59 ( 85.70)
Epoch: [10][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5947e+00 (1.5479e+00)	Acc@1  53.12 ( 56.01)	Acc@5  83.59 ( 85.73)
Epoch: [10][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7393e+00 (1.5467e+00)	Acc@1  49.22 ( 56.08)	Acc@5  85.16 ( 85.75)
Epoch: [10][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4248e+00 (1.5486e+00)	Acc@1  57.81 ( 56.02)	Acc@5  88.28 ( 85.71)
Epoch: [10][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4658e+00 (1.5499e+00)	Acc@1  57.03 ( 56.05)	Acc@5  88.28 ( 85.72)
Epoch: [10][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6250e+00 (1.5502e+00)	Acc@1  53.91 ( 56.04)	Acc@5  82.03 ( 85.71)
Epoch: [10][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5752e+00 (1.5491e+00)	Acc@1  60.94 ( 56.09)	Acc@5  89.06 ( 85.77)
Epoch: [10][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7568e+00 (1.5505e+00)	Acc@1  47.66 ( 56.06)	Acc@5  84.38 ( 85.74)
Epoch: [10][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5557e+00 (1.5493e+00)	Acc@1  53.12 ( 56.11)	Acc@5  82.81 ( 85.78)
Epoch: [10][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4736e+00 (1.5528e+00)	Acc@1  57.03 ( 56.04)	Acc@5  87.50 ( 85.75)
Epoch: [10][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4961e+00 (1.5557e+00)	Acc@1  52.34 ( 55.94)	Acc@5  84.38 ( 85.74)
Epoch: [10][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3027e+00 (1.5531e+00)	Acc@1  59.38 ( 55.94)	Acc@5  90.62 ( 85.79)
Epoch: [10][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6611e+00 (1.5540e+00)	Acc@1  51.56 ( 55.86)	Acc@5  84.38 ( 85.79)
Epoch: [10][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6494e+00 (1.5564e+00)	Acc@1  51.56 ( 55.83)	Acc@5  83.59 ( 85.73)
Epoch: [10][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6689e+00 (1.5575e+00)	Acc@1  54.69 ( 55.76)	Acc@5  83.59 ( 85.74)
Epoch: [10][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6621e+00 (1.5583e+00)	Acc@1  56.25 ( 55.74)	Acc@5  83.59 ( 85.72)
Epoch: [10][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4199e+00 (1.5601e+00)	Acc@1  57.03 ( 55.68)	Acc@5  86.72 ( 85.70)
Epoch: [10][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4561e+00 (1.5595e+00)	Acc@1  55.47 ( 55.67)	Acc@5  87.50 ( 85.69)
Epoch: [10][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5889e+00 (1.5607e+00)	Acc@1  56.25 ( 55.65)	Acc@5  86.72 ( 85.68)
Epoch: [10][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4541e+00 (1.5593e+00)	Acc@1  56.25 ( 55.72)	Acc@5  85.16 ( 85.70)
Epoch: [10][350/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4854e+00 (1.5591e+00)	Acc@1  58.59 ( 55.69)	Acc@5  88.28 ( 85.70)
Epoch: [10][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6357e+00 (1.5584e+00)	Acc@1  56.25 ( 55.68)	Acc@5  83.59 ( 85.70)
Epoch: [10][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2627e+00 (1.5566e+00)	Acc@1  64.06 ( 55.75)	Acc@5  87.50 ( 85.70)
Epoch: [10][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5791e+00 (1.5569e+00)	Acc@1  57.03 ( 55.79)	Acc@5  84.38 ( 85.67)
Epoch: [10][390/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6133e+00 (1.5580e+00)	Acc@1  55.00 ( 55.81)	Acc@5  86.25 ( 85.64)
## e[10] optimizer.zero_grad (sum) time: 0.29297399520874023
## e[10]       loss.backward (sum) time: 13.734558820724487
## e[10]      optimizer.step (sum) time: 2.7124733924865723
## epoch[10] training(only) time: 45.08243131637573
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.8584e+00 (1.8584e+00)	Acc@1  56.00 ( 56.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.042 ( 0.055)	Loss 1.7188e+00 (1.7160e+00)	Acc@1  56.00 ( 55.27)	Acc@5  86.00 ( 82.73)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.4678e+00 (1.6550e+00)	Acc@1  61.00 ( 55.52)	Acc@5  87.00 ( 83.67)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 1.7197e+00 (1.6516e+00)	Acc@1  52.00 ( 54.74)	Acc@5  83.00 ( 84.00)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.7275e+00 (1.6403e+00)	Acc@1  55.00 ( 54.98)	Acc@5  80.00 ( 84.34)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.5088e+00 (1.6612e+00)	Acc@1  53.00 ( 54.29)	Acc@5  88.00 ( 83.86)
Test: [ 60/100]	Time  0.040 ( 0.041)	Loss 1.4805e+00 (1.6434e+00)	Acc@1  58.00 ( 54.61)	Acc@5  86.00 ( 84.02)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.6045e+00 (1.6435e+00)	Acc@1  54.00 ( 54.79)	Acc@5  81.00 ( 84.14)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.8359e+00 (1.6589e+00)	Acc@1  54.00 ( 54.42)	Acc@5  81.00 ( 83.98)
Test: [ 90/100]	Time  0.037 ( 0.040)	Loss 1.7920e+00 (1.6488e+00)	Acc@1  53.00 ( 54.66)	Acc@5  82.00 ( 84.09)
 * Acc@1 54.500 Acc@5 84.220
### epoch[10] execution time: 49.1924946308136
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.275 ( 0.275)	Data  0.167 ( 0.167)	Loss 1.4619e+00 (1.4619e+00)	Acc@1  56.25 ( 56.25)	Acc@5  88.28 ( 88.28)
Epoch: [11][ 10/391]	Time  0.108 ( 0.128)	Data  0.001 ( 0.018)	Loss 1.4756e+00 (1.3999e+00)	Acc@1  61.72 ( 60.58)	Acc@5  89.06 ( 88.78)
Epoch: [11][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.2666e+00 (1.3699e+00)	Acc@1  64.06 ( 61.05)	Acc@5  92.97 ( 89.10)
Epoch: [11][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.0557e+00 (1.3684e+00)	Acc@1  76.56 ( 61.16)	Acc@5  90.62 ( 88.94)
Epoch: [11][ 40/391]	Time  0.110 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.4648e+00 (1.3651e+00)	Acc@1  56.25 ( 61.11)	Acc@5  90.62 ( 89.16)
Epoch: [11][ 50/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.4219e+00 (1.3611e+00)	Acc@1  58.59 ( 60.85)	Acc@5  89.84 ( 89.12)
Epoch: [11][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.3848e+00 (1.3714e+00)	Acc@1  58.59 ( 60.62)	Acc@5  89.06 ( 89.01)
Epoch: [11][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.5645e+00 (1.3793e+00)	Acc@1  55.47 ( 60.45)	Acc@5  85.94 ( 88.82)
Epoch: [11][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.2930e+00 (1.3918e+00)	Acc@1  61.72 ( 60.03)	Acc@5  93.75 ( 88.58)
Epoch: [11][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.6172e+00 (1.4013e+00)	Acc@1  57.81 ( 60.04)	Acc@5  82.81 ( 88.42)
Epoch: [11][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4326e+00 (1.4054e+00)	Acc@1  61.72 ( 59.92)	Acc@5  89.06 ( 88.34)
Epoch: [11][110/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2803e+00 (1.4018e+00)	Acc@1  57.81 ( 59.94)	Acc@5  90.62 ( 88.35)
Epoch: [11][120/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3574e+00 (1.3987e+00)	Acc@1  64.84 ( 60.12)	Acc@5  87.50 ( 88.36)
Epoch: [11][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4756e+00 (1.4044e+00)	Acc@1  57.03 ( 59.92)	Acc@5  91.41 ( 88.32)
Epoch: [11][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3418e+00 (1.4073e+00)	Acc@1  64.84 ( 59.96)	Acc@5  88.28 ( 88.24)
Epoch: [11][150/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3027e+00 (1.4049e+00)	Acc@1  65.62 ( 60.00)	Acc@5  86.72 ( 88.29)
Epoch: [11][160/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4707e+00 (1.4099e+00)	Acc@1  58.59 ( 59.80)	Acc@5  84.38 ( 88.19)
Epoch: [11][170/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4668e+00 (1.4113e+00)	Acc@1  60.16 ( 59.63)	Acc@5  86.72 ( 88.18)
Epoch: [11][180/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6191e+00 (1.4144e+00)	Acc@1  53.12 ( 59.44)	Acc@5  82.81 ( 88.08)
Epoch: [11][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1182e+00 (1.4174e+00)	Acc@1  66.41 ( 59.32)	Acc@5  90.62 ( 88.04)
Epoch: [11][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3965e+00 (1.4204e+00)	Acc@1  60.94 ( 59.27)	Acc@5  86.72 ( 88.00)
Epoch: [11][210/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3975e+00 (1.4180e+00)	Acc@1  62.50 ( 59.26)	Acc@5  86.72 ( 88.00)
Epoch: [11][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3779e+00 (1.4196e+00)	Acc@1  65.62 ( 59.27)	Acc@5  89.84 ( 87.95)
Epoch: [11][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6064e+00 (1.4217e+00)	Acc@1  53.91 ( 59.16)	Acc@5  85.94 ( 87.89)
Epoch: [11][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6191e+00 (1.4231e+00)	Acc@1  53.12 ( 59.12)	Acc@5  86.72 ( 87.82)
Epoch: [11][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1094e+00 (1.4222e+00)	Acc@1  63.28 ( 59.09)	Acc@5  92.97 ( 87.85)
Epoch: [11][260/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6016e+00 (1.4226e+00)	Acc@1  57.81 ( 59.10)	Acc@5  84.38 ( 87.82)
Epoch: [11][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4814e+00 (1.4244e+00)	Acc@1  57.03 ( 59.08)	Acc@5  88.28 ( 87.80)
Epoch: [11][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2969e+00 (1.4255e+00)	Acc@1  57.81 ( 59.07)	Acc@5  89.84 ( 87.83)
Epoch: [11][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4316e+00 (1.4271e+00)	Acc@1  59.38 ( 59.00)	Acc@5  89.06 ( 87.80)
Epoch: [11][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6133e+00 (1.4291e+00)	Acc@1  53.12 ( 58.98)	Acc@5  84.38 ( 87.77)
Epoch: [11][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5088e+00 (1.4287e+00)	Acc@1  60.16 ( 58.99)	Acc@5  85.16 ( 87.74)
Epoch: [11][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3965e+00 (1.4300e+00)	Acc@1  64.06 ( 59.01)	Acc@5  87.50 ( 87.67)
Epoch: [11][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3848e+00 (1.4308e+00)	Acc@1  58.59 ( 58.99)	Acc@5  89.06 ( 87.67)
Epoch: [11][340/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4932e+00 (1.4301e+00)	Acc@1  58.59 ( 59.01)	Acc@5  89.84 ( 87.70)
Epoch: [11][350/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3525e+00 (1.4279e+00)	Acc@1  63.28 ( 59.09)	Acc@5  86.72 ( 87.70)
Epoch: [11][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3965e+00 (1.4265e+00)	Acc@1  58.59 ( 59.14)	Acc@5  90.62 ( 87.73)
Epoch: [11][370/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4277e+00 (1.4263e+00)	Acc@1  61.72 ( 59.12)	Acc@5  89.06 ( 87.75)
Epoch: [11][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5342e+00 (1.4271e+00)	Acc@1  50.78 ( 59.06)	Acc@5  86.72 ( 87.71)
Epoch: [11][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4072e+00 (1.4289e+00)	Acc@1  58.75 ( 59.02)	Acc@5  90.00 ( 87.69)
## e[11] optimizer.zero_grad (sum) time: 0.29566359519958496
## e[11]       loss.backward (sum) time: 13.782376766204834
## e[11]      optimizer.step (sum) time: 2.70382022857666
## epoch[11] training(only) time: 45.04905319213867
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.7285e+00 (1.7285e+00)	Acc@1  57.00 ( 57.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.040 ( 0.053)	Loss 1.7490e+00 (1.7377e+00)	Acc@1  54.00 ( 54.18)	Acc@5  85.00 ( 82.64)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.5781e+00 (1.7492e+00)	Acc@1  60.00 ( 54.19)	Acc@5  85.00 ( 82.67)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 1.9600e+00 (1.7743e+00)	Acc@1  42.00 ( 53.10)	Acc@5  83.00 ( 82.23)
Test: [ 40/100]	Time  0.040 ( 0.043)	Loss 1.7432e+00 (1.7825e+00)	Acc@1  55.00 ( 52.66)	Acc@5  78.00 ( 82.22)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.5537e+00 (1.7968e+00)	Acc@1  54.00 ( 52.43)	Acc@5  86.00 ( 81.61)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 1.4814e+00 (1.7850e+00)	Acc@1  56.00 ( 52.26)	Acc@5  85.00 ( 81.90)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.6650e+00 (1.7779e+00)	Acc@1  58.00 ( 52.54)	Acc@5  80.00 ( 81.96)
Test: [ 80/100]	Time  0.037 ( 0.041)	Loss 1.8584e+00 (1.7819e+00)	Acc@1  55.00 ( 52.59)	Acc@5  77.00 ( 81.90)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 1.9307e+00 (1.7712e+00)	Acc@1  49.00 ( 52.89)	Acc@5  77.00 ( 82.09)
 * Acc@1 52.950 Acc@5 82.150
### epoch[11] execution time: 49.185667514801025
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.267 ( 0.267)	Data  0.156 ( 0.156)	Loss 1.5947e+00 (1.5947e+00)	Acc@1  53.91 ( 53.91)	Acc@5  85.94 ( 85.94)
Epoch: [12][ 10/391]	Time  0.112 ( 0.128)	Data  0.001 ( 0.017)	Loss 1.0762e+00 (1.2979e+00)	Acc@1  70.31 ( 63.42)	Acc@5  91.41 ( 89.35)
Epoch: [12][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.1709e+00 (1.2822e+00)	Acc@1  60.94 ( 63.73)	Acc@5  92.97 ( 89.43)
Epoch: [12][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.3477e+00 (1.2744e+00)	Acc@1  57.81 ( 63.89)	Acc@5  92.97 ( 89.82)
Epoch: [12][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.2656e+00 (1.2797e+00)	Acc@1  68.75 ( 63.68)	Acc@5  89.06 ( 89.86)
Epoch: [12][ 50/391]	Time  0.109 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.2803e+00 (1.2829e+00)	Acc@1  58.59 ( 63.42)	Acc@5  92.19 ( 89.89)
Epoch: [12][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.2871e+00 (1.2951e+00)	Acc@1  57.81 ( 62.74)	Acc@5  88.28 ( 89.60)
Epoch: [12][ 70/391]	Time  0.109 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.2646e+00 (1.3019e+00)	Acc@1  58.59 ( 62.57)	Acc@5  88.28 ( 89.40)
Epoch: [12][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.6738e+00 (1.3165e+00)	Acc@1  51.56 ( 62.08)	Acc@5  86.72 ( 89.28)
Epoch: [12][ 90/391]	Time  0.112 ( 0.116)	Data  0.002 ( 0.006)	Loss 1.4600e+00 (1.3260e+00)	Acc@1  58.59 ( 61.96)	Acc@5  85.94 ( 89.07)
Epoch: [12][100/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.9561e-01 (1.3210e+00)	Acc@1  70.31 ( 62.20)	Acc@5  93.75 ( 89.22)
Epoch: [12][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4668e+00 (1.3246e+00)	Acc@1  57.81 ( 62.09)	Acc@5  86.72 ( 89.15)
Epoch: [12][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1973e+00 (1.3243e+00)	Acc@1  62.50 ( 62.00)	Acc@5  88.28 ( 89.13)
Epoch: [12][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2393e+00 (1.3225e+00)	Acc@1  67.19 ( 62.02)	Acc@5  87.50 ( 89.15)
Epoch: [12][140/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2656e+00 (1.3238e+00)	Acc@1  57.81 ( 61.93)	Acc@5  92.97 ( 89.12)
Epoch: [12][150/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3721e+00 (1.3298e+00)	Acc@1  61.72 ( 61.71)	Acc@5  92.19 ( 89.06)
Epoch: [12][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3369e+00 (1.3362e+00)	Acc@1  52.34 ( 61.42)	Acc@5  93.75 ( 89.02)
Epoch: [12][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5000e+00 (1.3387e+00)	Acc@1  56.25 ( 61.27)	Acc@5  88.28 ( 89.00)
Epoch: [12][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2324e+00 (1.3394e+00)	Acc@1  61.72 ( 61.21)	Acc@5  95.31 ( 89.07)
Epoch: [12][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4688e+00 (1.3415e+00)	Acc@1  57.81 ( 61.19)	Acc@5  85.94 ( 89.05)
Epoch: [12][200/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1660e+00 (1.3413e+00)	Acc@1  68.75 ( 61.21)	Acc@5  89.06 ( 89.00)
Epoch: [12][210/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3789e+00 (1.3383e+00)	Acc@1  60.16 ( 61.32)	Acc@5  90.62 ( 89.04)
Epoch: [12][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1855e+00 (1.3394e+00)	Acc@1  65.62 ( 61.27)	Acc@5  91.41 ( 89.02)
Epoch: [12][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3447e+00 (1.3393e+00)	Acc@1  57.03 ( 61.28)	Acc@5  89.06 ( 88.98)
Epoch: [12][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1572e+00 (1.3402e+00)	Acc@1  67.19 ( 61.26)	Acc@5  93.75 ( 89.00)
Epoch: [12][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3555e+00 (1.3418e+00)	Acc@1  62.50 ( 61.24)	Acc@5  86.72 ( 88.99)
Epoch: [12][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1533e+00 (1.3409e+00)	Acc@1  62.50 ( 61.21)	Acc@5  92.97 ( 88.98)
Epoch: [12][270/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4941e+00 (1.3428e+00)	Acc@1  53.12 ( 61.13)	Acc@5  92.19 ( 89.00)
Epoch: [12][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2715e+00 (1.3418e+00)	Acc@1  57.81 ( 61.12)	Acc@5  89.06 ( 88.97)
Epoch: [12][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2812e+00 (1.3404e+00)	Acc@1  60.94 ( 61.12)	Acc@5  87.50 ( 88.97)
Epoch: [12][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4727e+00 (1.3421e+00)	Acc@1  58.59 ( 61.08)	Acc@5  84.38 ( 88.91)
Epoch: [12][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1729e+00 (1.3405e+00)	Acc@1  62.50 ( 61.10)	Acc@5  90.62 ( 88.97)
Epoch: [12][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4365e+00 (1.3406e+00)	Acc@1  63.28 ( 61.13)	Acc@5  90.62 ( 88.98)
Epoch: [12][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3057e+00 (1.3397e+00)	Acc@1  66.41 ( 61.15)	Acc@5  87.50 ( 89.02)
Epoch: [12][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6328e+00 (1.3411e+00)	Acc@1  53.91 ( 61.13)	Acc@5  84.38 ( 88.98)
Epoch: [12][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2773e+00 (1.3412e+00)	Acc@1  64.84 ( 61.14)	Acc@5  87.50 ( 88.96)
Epoch: [12][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4326e+00 (1.3435e+00)	Acc@1  60.94 ( 61.10)	Acc@5  88.28 ( 88.92)
Epoch: [12][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1650e+00 (1.3435e+00)	Acc@1  67.19 ( 61.10)	Acc@5  92.97 ( 88.92)
Epoch: [12][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1484e+00 (1.3437e+00)	Acc@1  64.06 ( 61.07)	Acc@5  91.41 ( 88.93)
Epoch: [12][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3164e+00 (1.3436e+00)	Acc@1  65.00 ( 61.08)	Acc@5  92.50 ( 88.94)
## e[12] optimizer.zero_grad (sum) time: 0.2911553382873535
## e[12]       loss.backward (sum) time: 13.790392875671387
## e[12]      optimizer.step (sum) time: 2.712998867034912
## epoch[12] training(only) time: 45.08834958076477
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.6768e+00 (1.6768e+00)	Acc@1  58.00 ( 58.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 1.6602e+00 (1.5683e+00)	Acc@1  54.00 ( 58.91)	Acc@5  89.00 ( 85.45)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.4883e+00 (1.5661e+00)	Acc@1  61.00 ( 58.43)	Acc@5  89.00 ( 85.57)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 1.5479e+00 (1.5678e+00)	Acc@1  57.00 ( 58.00)	Acc@5  87.00 ( 85.58)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.6318e+00 (1.5915e+00)	Acc@1  59.00 ( 56.95)	Acc@5  83.00 ( 85.02)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.4414e+00 (1.6312e+00)	Acc@1  58.00 ( 56.35)	Acc@5  86.00 ( 84.33)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.5547e+00 (1.6300e+00)	Acc@1  54.00 ( 56.02)	Acc@5  86.00 ( 84.31)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.6094e+00 (1.6387e+00)	Acc@1  61.00 ( 56.07)	Acc@5  82.00 ( 84.18)
Test: [ 80/100]	Time  0.041 ( 0.041)	Loss 1.6064e+00 (1.6443e+00)	Acc@1  59.00 ( 56.00)	Acc@5  85.00 ( 84.15)
Test: [ 90/100]	Time  0.038 ( 0.040)	Loss 1.9883e+00 (1.6386e+00)	Acc@1  48.00 ( 56.15)	Acc@5  79.00 ( 84.33)
 * Acc@1 56.140 Acc@5 84.490
### epoch[12] execution time: 49.195088624954224
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.277 ( 0.277)	Data  0.168 ( 0.168)	Loss 1.1475e+00 (1.1475e+00)	Acc@1  64.06 ( 64.06)	Acc@5  89.84 ( 89.84)
Epoch: [13][ 10/391]	Time  0.114 ( 0.129)	Data  0.001 ( 0.018)	Loss 1.2109e+00 (1.2064e+00)	Acc@1  67.97 ( 65.20)	Acc@5  88.28 ( 90.84)
Epoch: [13][ 20/391]	Time  0.115 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.2646e+00 (1.1848e+00)	Acc@1  63.28 ( 65.81)	Acc@5  89.06 ( 91.15)
Epoch: [13][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.2119e+00 (1.2011e+00)	Acc@1  68.75 ( 65.20)	Acc@5  91.41 ( 90.78)
Epoch: [13][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.0049e+00 (1.1970e+00)	Acc@1  72.66 ( 65.28)	Acc@5  92.19 ( 91.03)
Epoch: [13][ 50/391]	Time  0.110 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.3086e+00 (1.1992e+00)	Acc@1  62.50 ( 65.20)	Acc@5  88.28 ( 90.78)
Epoch: [13][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.3330e+00 (1.2092e+00)	Acc@1  64.84 ( 65.02)	Acc@5  91.41 ( 90.74)
Epoch: [13][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.3740e+00 (1.2112e+00)	Acc@1  57.03 ( 64.92)	Acc@5  87.50 ( 90.89)
Epoch: [13][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.5625e+00 (1.2238e+00)	Acc@1  56.25 ( 64.59)	Acc@5  86.72 ( 90.69)
Epoch: [13][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.4287e+00 (1.2287e+00)	Acc@1  57.03 ( 64.39)	Acc@5  89.06 ( 90.68)
Epoch: [13][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2197e+00 (1.2268e+00)	Acc@1  64.06 ( 64.46)	Acc@5  87.50 ( 90.68)
Epoch: [13][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3809e+00 (1.2225e+00)	Acc@1  67.19 ( 64.63)	Acc@5  88.28 ( 90.65)
Epoch: [13][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1992e+00 (1.2280e+00)	Acc@1  62.50 ( 64.42)	Acc@5  89.84 ( 90.48)
Epoch: [13][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1191e+00 (1.2257e+00)	Acc@1  64.84 ( 64.49)	Acc@5  92.97 ( 90.59)
Epoch: [13][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2090e+00 (1.2256e+00)	Acc@1  71.09 ( 64.44)	Acc@5  90.62 ( 90.64)
Epoch: [13][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3027e+00 (1.2260e+00)	Acc@1  63.28 ( 64.44)	Acc@5  87.50 ( 90.62)
Epoch: [13][160/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1416e+00 (1.2224e+00)	Acc@1  67.97 ( 64.56)	Acc@5  90.62 ( 90.69)
Epoch: [13][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3408e+00 (1.2239e+00)	Acc@1  64.06 ( 64.45)	Acc@5  89.06 ( 90.67)
Epoch: [13][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2744e+00 (1.2258e+00)	Acc@1  56.25 ( 64.35)	Acc@5  89.84 ( 90.62)
Epoch: [13][190/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2822e+00 (1.2283e+00)	Acc@1  66.41 ( 64.26)	Acc@5  85.94 ( 90.54)
Epoch: [13][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2275e+00 (1.2308e+00)	Acc@1  64.06 ( 64.18)	Acc@5  92.97 ( 90.50)
Epoch: [13][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1387e+00 (1.2339e+00)	Acc@1  63.28 ( 64.02)	Acc@5  93.75 ( 90.44)
Epoch: [13][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2432e+00 (1.2363e+00)	Acc@1  65.62 ( 64.03)	Acc@5  89.84 ( 90.37)
Epoch: [13][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3574e+00 (1.2389e+00)	Acc@1  60.94 ( 63.95)	Acc@5  86.72 ( 90.38)
Epoch: [13][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1133e+00 (1.2398e+00)	Acc@1  65.62 ( 63.92)	Acc@5  93.75 ( 90.38)
Epoch: [13][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5234e+00 (1.2426e+00)	Acc@1  52.34 ( 63.83)	Acc@5  87.50 ( 90.33)
Epoch: [13][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2324e+00 (1.2435e+00)	Acc@1  71.88 ( 63.81)	Acc@5  85.94 ( 90.28)
Epoch: [13][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4541e+00 (1.2462e+00)	Acc@1  62.50 ( 63.78)	Acc@5  82.03 ( 90.22)
Epoch: [13][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5264e+00 (1.2485e+00)	Acc@1  60.16 ( 63.67)	Acc@5  85.16 ( 90.19)
Epoch: [13][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2148e+00 (1.2497e+00)	Acc@1  62.50 ( 63.65)	Acc@5  92.19 ( 90.16)
Epoch: [13][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2539e+00 (1.2511e+00)	Acc@1  63.28 ( 63.58)	Acc@5  95.31 ( 90.14)
Epoch: [13][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2109e+00 (1.2531e+00)	Acc@1  61.72 ( 63.55)	Acc@5  91.41 ( 90.11)
Epoch: [13][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3223e+00 (1.2535e+00)	Acc@1  67.19 ( 63.56)	Acc@5  86.72 ( 90.10)
Epoch: [13][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2930e+00 (1.2568e+00)	Acc@1  64.84 ( 63.48)	Acc@5  92.97 ( 90.07)
Epoch: [13][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2871e+00 (1.2561e+00)	Acc@1  61.72 ( 63.48)	Acc@5  92.19 ( 90.09)
Epoch: [13][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1406e+00 (1.2566e+00)	Acc@1  67.97 ( 63.44)	Acc@5  90.62 ( 90.09)
Epoch: [13][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1523e+00 (1.2569e+00)	Acc@1  68.75 ( 63.47)	Acc@5  89.06 ( 90.08)
Epoch: [13][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0137e+00 (1.2554e+00)	Acc@1  67.19 ( 63.51)	Acc@5  93.75 ( 90.12)
Epoch: [13][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3809e+00 (1.2553e+00)	Acc@1  57.03 ( 63.50)	Acc@5  87.50 ( 90.13)
Epoch: [13][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1670e+00 (1.2557e+00)	Acc@1  56.25 ( 63.50)	Acc@5  93.75 ( 90.13)
## e[13] optimizer.zero_grad (sum) time: 0.2923624515533447
## e[13]       loss.backward (sum) time: 13.751909255981445
## e[13]      optimizer.step (sum) time: 2.7320892810821533
## epoch[13] training(only) time: 45.07132935523987
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.6758e+00 (1.6758e+00)	Acc@1  58.00 ( 58.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 1.7881e+00 (1.5599e+00)	Acc@1  53.00 ( 58.18)	Acc@5  84.00 ( 86.27)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.1289e+00 (1.4843e+00)	Acc@1  68.00 ( 59.19)	Acc@5  89.00 ( 86.90)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 1.5557e+00 (1.5091e+00)	Acc@1  59.00 ( 58.55)	Acc@5  85.00 ( 86.84)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.4111e+00 (1.5193e+00)	Acc@1  62.00 ( 57.95)	Acc@5  85.00 ( 86.56)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.3447e+00 (1.5491e+00)	Acc@1  55.00 ( 57.29)	Acc@5  88.00 ( 86.00)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.3643e+00 (1.5294e+00)	Acc@1  54.00 ( 57.77)	Acc@5  88.00 ( 86.03)
Test: [ 70/100]	Time  0.037 ( 0.041)	Loss 1.3760e+00 (1.5371e+00)	Acc@1  64.00 ( 57.96)	Acc@5  90.00 ( 86.13)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.6084e+00 (1.5469e+00)	Acc@1  57.00 ( 57.64)	Acc@5  89.00 ( 85.96)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.7100e+00 (1.5392e+00)	Acc@1  55.00 ( 57.95)	Acc@5  86.00 ( 86.18)
 * Acc@1 58.080 Acc@5 86.270
### epoch[13] execution time: 49.1951220035553
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.279 ( 0.279)	Data  0.168 ( 0.168)	Loss 1.1016e+00 (1.1016e+00)	Acc@1  67.97 ( 67.97)	Acc@5  93.75 ( 93.75)
Epoch: [14][ 10/391]	Time  0.114 ( 0.129)	Data  0.001 ( 0.018)	Loss 9.8779e-01 (1.1159e+00)	Acc@1  71.88 ( 65.98)	Acc@5  92.19 ( 91.97)
Epoch: [14][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.012)	Loss 1.0068e+00 (1.1016e+00)	Acc@1  72.66 ( 67.37)	Acc@5  93.75 ( 92.19)
Epoch: [14][ 30/391]	Time  0.114 ( 0.120)	Data  0.001 ( 0.009)	Loss 9.7363e-01 (1.1009e+00)	Acc@1  69.53 ( 67.14)	Acc@5  95.31 ( 92.44)
Epoch: [14][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.1572e+00 (1.1193e+00)	Acc@1  65.62 ( 66.81)	Acc@5  96.09 ( 92.15)
Epoch: [14][ 50/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.1885e+00 (1.1322e+00)	Acc@1  67.97 ( 66.59)	Acc@5  89.84 ( 92.06)
Epoch: [14][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.1387e+00 (1.1415e+00)	Acc@1  64.84 ( 66.05)	Acc@5  91.41 ( 91.89)
Epoch: [14][ 70/391]	Time  0.109 ( 0.117)	Data  0.001 ( 0.006)	Loss 9.8193e-01 (1.1403e+00)	Acc@1  67.97 ( 66.16)	Acc@5  96.88 ( 91.97)
Epoch: [14][ 80/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.3955e+00 (1.1379e+00)	Acc@1  64.84 ( 66.36)	Acc@5  88.28 ( 91.99)
Epoch: [14][ 90/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.2412e+00 (1.1407e+00)	Acc@1  63.28 ( 66.41)	Acc@5  87.50 ( 92.02)
Epoch: [14][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.8672e-01 (1.1376e+00)	Acc@1  71.88 ( 66.48)	Acc@5  95.31 ( 92.03)
Epoch: [14][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.5947e-01 (1.1347e+00)	Acc@1  76.56 ( 66.72)	Acc@5  93.75 ( 92.02)
Epoch: [14][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1260e+00 (1.1366e+00)	Acc@1  62.50 ( 66.59)	Acc@5  91.41 ( 91.94)
Epoch: [14][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0020e+00 (1.1418e+00)	Acc@1  69.53 ( 66.42)	Acc@5  93.75 ( 91.73)
Epoch: [14][140/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0225e+00 (1.1395e+00)	Acc@1  71.09 ( 66.49)	Acc@5  94.53 ( 91.82)
Epoch: [14][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1406e+00 (1.1451e+00)	Acc@1  64.06 ( 66.33)	Acc@5  92.97 ( 91.73)
Epoch: [14][160/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1738e+00 (1.1499e+00)	Acc@1  68.75 ( 66.28)	Acc@5  91.41 ( 91.67)
Epoch: [14][170/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1846e+00 (1.1547e+00)	Acc@1  68.75 ( 66.14)	Acc@5  92.19 ( 91.64)
Epoch: [14][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3457e+00 (1.1554e+00)	Acc@1  57.03 ( 66.07)	Acc@5  92.19 ( 91.64)
Epoch: [14][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1494e+00 (1.1564e+00)	Acc@1  67.97 ( 66.01)	Acc@5  92.19 ( 91.61)
Epoch: [14][200/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1611e+00 (1.1581e+00)	Acc@1  65.62 ( 65.97)	Acc@5  91.41 ( 91.59)
Epoch: [14][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1113e+00 (1.1586e+00)	Acc@1  69.53 ( 65.95)	Acc@5  93.75 ( 91.59)
Epoch: [14][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3721e+00 (1.1594e+00)	Acc@1  64.84 ( 65.96)	Acc@5  85.94 ( 91.51)
Epoch: [14][230/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0791e+00 (1.1613e+00)	Acc@1  70.31 ( 65.98)	Acc@5  92.19 ( 91.47)
Epoch: [14][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2422e+00 (1.1643e+00)	Acc@1  64.84 ( 65.94)	Acc@5  89.84 ( 91.40)
Epoch: [14][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1289e+00 (1.1635e+00)	Acc@1  64.06 ( 65.96)	Acc@5  92.19 ( 91.39)
Epoch: [14][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0254e+00 (1.1616e+00)	Acc@1  71.09 ( 66.02)	Acc@5  94.53 ( 91.40)
Epoch: [14][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3408e+00 (1.1640e+00)	Acc@1  60.94 ( 65.93)	Acc@5  92.19 ( 91.41)
Epoch: [14][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2773e+00 (1.1647e+00)	Acc@1  63.28 ( 65.92)	Acc@5  87.50 ( 91.36)
Epoch: [14][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0938e+00 (1.1660e+00)	Acc@1  69.53 ( 65.89)	Acc@5  89.06 ( 91.34)
Epoch: [14][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0977e+00 (1.1638e+00)	Acc@1  71.09 ( 65.97)	Acc@5  92.97 ( 91.36)
Epoch: [14][310/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0273e+00 (1.1644e+00)	Acc@1  71.09 ( 65.95)	Acc@5  91.41 ( 91.37)
Epoch: [14][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2646e+00 (1.1655e+00)	Acc@1  62.50 ( 65.94)	Acc@5  92.97 ( 91.36)
Epoch: [14][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2256e+00 (1.1679e+00)	Acc@1  61.72 ( 65.85)	Acc@5  91.41 ( 91.33)
Epoch: [14][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1514e+00 (1.1710e+00)	Acc@1  71.09 ( 65.80)	Acc@5  92.97 ( 91.30)
Epoch: [14][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1230e+00 (1.1701e+00)	Acc@1  67.97 ( 65.80)	Acc@5  90.62 ( 91.31)
Epoch: [14][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2695e+00 (1.1702e+00)	Acc@1  58.59 ( 65.79)	Acc@5  89.84 ( 91.32)
Epoch: [14][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4434e+00 (1.1717e+00)	Acc@1  57.81 ( 65.78)	Acc@5  85.94 ( 91.29)
Epoch: [14][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1748e+00 (1.1705e+00)	Acc@1  67.97 ( 65.83)	Acc@5  89.06 ( 91.29)
Epoch: [14][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4072e+00 (1.1709e+00)	Acc@1  60.00 ( 65.81)	Acc@5  87.50 ( 91.30)
## e[14] optimizer.zero_grad (sum) time: 0.29354071617126465
## e[14]       loss.backward (sum) time: 13.773926496505737
## e[14]      optimizer.step (sum) time: 2.710975170135498
## epoch[14] training(only) time: 45.10636377334595
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.5361e+00 (1.5361e+00)	Acc@1  61.00 ( 61.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 1.3125e+00 (1.4554e+00)	Acc@1  62.00 ( 61.18)	Acc@5  92.00 ( 87.36)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.1611e+00 (1.4070e+00)	Acc@1  65.00 ( 61.81)	Acc@5  89.00 ( 87.90)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 1.5791e+00 (1.4395e+00)	Acc@1  53.00 ( 60.61)	Acc@5  87.00 ( 87.13)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.3936e+00 (1.4366e+00)	Acc@1  57.00 ( 60.15)	Acc@5  89.00 ( 87.46)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.5430e+00 (1.4711e+00)	Acc@1  61.00 ( 59.33)	Acc@5  83.00 ( 86.88)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.2725e+00 (1.4577e+00)	Acc@1  63.00 ( 59.33)	Acc@5  91.00 ( 87.36)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.5381e+00 (1.4560e+00)	Acc@1  54.00 ( 59.37)	Acc@5  89.00 ( 87.44)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.5732e+00 (1.4588e+00)	Acc@1  57.00 ( 59.38)	Acc@5  83.00 ( 87.35)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.8613e+00 (1.4536e+00)	Acc@1  54.00 ( 59.71)	Acc@5  82.00 ( 87.38)
 * Acc@1 59.790 Acc@5 87.340
### epoch[14] execution time: 49.22578048706055
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.278 ( 0.278)	Data  0.157 ( 0.157)	Loss 1.2051e+00 (1.2051e+00)	Acc@1  67.97 ( 67.97)	Acc@5  90.62 ( 90.62)
Epoch: [15][ 10/391]	Time  0.113 ( 0.129)	Data  0.001 ( 0.017)	Loss 1.0479e+00 (1.0939e+00)	Acc@1  67.97 ( 68.82)	Acc@5  90.62 ( 92.40)
Epoch: [15][ 20/391]	Time  0.114 ( 0.122)	Data  0.001 ( 0.011)	Loss 9.8438e-01 (1.0686e+00)	Acc@1  70.31 ( 68.45)	Acc@5  92.97 ( 93.04)
Epoch: [15][ 30/391]	Time  0.111 ( 0.120)	Data  0.001 ( 0.009)	Loss 9.0186e-01 (1.0574e+00)	Acc@1  71.88 ( 68.40)	Acc@5  96.09 ( 93.20)
Epoch: [15][ 40/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.007)	Loss 8.4912e-01 (1.0463e+00)	Acc@1  75.78 ( 68.83)	Acc@5  95.31 ( 93.31)
Epoch: [15][ 50/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.007)	Loss 9.4141e-01 (1.0420e+00)	Acc@1  72.66 ( 68.90)	Acc@5  93.75 ( 93.15)
Epoch: [15][ 60/391]	Time  0.116 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.1143e+00 (1.0480e+00)	Acc@1  67.97 ( 68.78)	Acc@5  90.62 ( 92.93)
Epoch: [15][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 9.8779e-01 (1.0515e+00)	Acc@1  73.44 ( 68.67)	Acc@5  91.41 ( 92.91)
Epoch: [15][ 80/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 9.2920e-01 (1.0554e+00)	Acc@1  74.22 ( 68.54)	Acc@5  95.31 ( 92.88)
Epoch: [15][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.1943e+00 (1.0545e+00)	Acc@1  66.41 ( 68.66)	Acc@5  90.62 ( 92.88)
Epoch: [15][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.8516e-01 (1.0586e+00)	Acc@1  77.34 ( 68.68)	Acc@5  97.66 ( 92.75)
Epoch: [15][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.2959e-01 (1.0598e+00)	Acc@1  75.78 ( 68.71)	Acc@5  95.31 ( 92.73)
Epoch: [15][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1963e+00 (1.0645e+00)	Acc@1  67.19 ( 68.54)	Acc@5  87.50 ( 92.72)
Epoch: [15][130/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1709e+00 (1.0654e+00)	Acc@1  62.50 ( 68.40)	Acc@5  91.41 ( 92.72)
Epoch: [15][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1680e+00 (1.0678e+00)	Acc@1  69.53 ( 68.36)	Acc@5  91.41 ( 92.69)
Epoch: [15][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.3164e-01 (1.0718e+00)	Acc@1  73.44 ( 68.25)	Acc@5  91.41 ( 92.63)
Epoch: [15][160/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3857e+00 (1.0800e+00)	Acc@1  64.84 ( 68.20)	Acc@5  87.50 ( 92.48)
Epoch: [15][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.1797e-01 (1.0819e+00)	Acc@1  74.22 ( 68.17)	Acc@5  94.53 ( 92.45)
Epoch: [15][180/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1846e+00 (1.0862e+00)	Acc@1  64.84 ( 67.98)	Acc@5  91.41 ( 92.40)
Epoch: [15][190/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3525e+00 (1.0893e+00)	Acc@1  59.38 ( 67.83)	Acc@5  89.84 ( 92.38)
Epoch: [15][200/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2646e+00 (1.0902e+00)	Acc@1  62.50 ( 67.75)	Acc@5  89.84 ( 92.34)
Epoch: [15][210/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1348e+00 (1.0916e+00)	Acc@1  66.41 ( 67.68)	Acc@5  90.62 ( 92.31)
Epoch: [15][220/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1514e+00 (1.0950e+00)	Acc@1  69.53 ( 67.58)	Acc@5  90.62 ( 92.27)
Epoch: [15][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3906e+00 (1.0965e+00)	Acc@1  57.81 ( 67.51)	Acc@5  85.94 ( 92.23)
Epoch: [15][240/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3271e+00 (1.1013e+00)	Acc@1  60.94 ( 67.39)	Acc@5  89.06 ( 92.20)
Epoch: [15][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0508e+00 (1.1011e+00)	Acc@1  72.66 ( 67.41)	Acc@5  94.53 ( 92.24)
Epoch: [15][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0547e+00 (1.1025e+00)	Acc@1  68.75 ( 67.39)	Acc@5  91.41 ( 92.24)
Epoch: [15][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0596e+00 (1.1048e+00)	Acc@1  69.53 ( 67.32)	Acc@5  93.75 ( 92.19)
Epoch: [15][280/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1904e+00 (1.1074e+00)	Acc@1  63.28 ( 67.27)	Acc@5  92.19 ( 92.16)
Epoch: [15][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1553e+00 (1.1089e+00)	Acc@1  63.28 ( 67.26)	Acc@5  89.84 ( 92.14)
Epoch: [15][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1436e+00 (1.1098e+00)	Acc@1  69.53 ( 67.24)	Acc@5  92.19 ( 92.12)
Epoch: [15][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1709e+00 (1.1107e+00)	Acc@1  64.06 ( 67.23)	Acc@5  89.06 ( 92.09)
Epoch: [15][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4258e+00 (1.1091e+00)	Acc@1  58.59 ( 67.30)	Acc@5  86.72 ( 92.08)
Epoch: [15][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0303e+00 (1.1092e+00)	Acc@1  68.75 ( 67.28)	Acc@5  95.31 ( 92.08)
Epoch: [15][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4316e+00 (1.1112e+00)	Acc@1  57.81 ( 67.24)	Acc@5  87.50 ( 92.07)
Epoch: [15][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1270e+00 (1.1127e+00)	Acc@1  66.41 ( 67.22)	Acc@5  92.19 ( 92.03)
Epoch: [15][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2939e+00 (1.1142e+00)	Acc@1  64.06 ( 67.17)	Acc@5  89.06 ( 91.98)
Epoch: [15][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4033e-01 (1.1128e+00)	Acc@1  74.22 ( 67.21)	Acc@5  95.31 ( 91.99)
Epoch: [15][380/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3467e+00 (1.1145e+00)	Acc@1  64.84 ( 67.20)	Acc@5  88.28 ( 91.96)
Epoch: [15][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1484e+00 (1.1156e+00)	Acc@1  68.75 ( 67.19)	Acc@5  91.25 ( 91.96)
## e[15] optimizer.zero_grad (sum) time: 0.2956521511077881
## e[15]       loss.backward (sum) time: 13.76615595817566
## e[15]      optimizer.step (sum) time: 2.7068402767181396
## epoch[15] training(only) time: 45.09646725654602
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.3623e+00 (1.3623e+00)	Acc@1  65.00 ( 65.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.041 ( 0.052)	Loss 1.5391e+00 (1.4231e+00)	Acc@1  55.00 ( 62.64)	Acc@5  89.00 ( 87.09)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.1201e+00 (1.3561e+00)	Acc@1  69.00 ( 63.38)	Acc@5  91.00 ( 88.48)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 1.3535e+00 (1.3738e+00)	Acc@1  58.00 ( 62.26)	Acc@5  92.00 ( 88.52)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 1.3223e+00 (1.3663e+00)	Acc@1  65.00 ( 61.93)	Acc@5  89.00 ( 88.68)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.3818e+00 (1.3888e+00)	Acc@1  60.00 ( 61.57)	Acc@5  86.00 ( 87.98)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.4600e+00 (1.3771e+00)	Acc@1  60.00 ( 61.72)	Acc@5  88.00 ( 88.23)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.4023e+00 (1.3848e+00)	Acc@1  61.00 ( 61.54)	Acc@5  86.00 ( 88.18)
Test: [ 80/100]	Time  0.038 ( 0.040)	Loss 1.3789e+00 (1.3849e+00)	Acc@1  67.00 ( 61.42)	Acc@5  85.00 ( 88.11)
Test: [ 90/100]	Time  0.038 ( 0.040)	Loss 1.6436e+00 (1.3764e+00)	Acc@1  57.00 ( 61.53)	Acc@5  84.00 ( 88.22)
 * Acc@1 61.470 Acc@5 88.200
### epoch[15] execution time: 49.17714285850525
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.270 ( 0.270)	Data  0.158 ( 0.158)	Loss 1.0508e+00 (1.0508e+00)	Acc@1  67.97 ( 67.97)	Acc@5  92.97 ( 92.97)
Epoch: [16][ 10/391]	Time  0.114 ( 0.128)	Data  0.001 ( 0.017)	Loss 9.3457e-01 (9.5574e-01)	Acc@1  72.66 ( 71.66)	Acc@5  92.97 ( 93.61)
Epoch: [16][ 20/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.011)	Loss 9.2871e-01 (9.8575e-01)	Acc@1  71.88 ( 69.83)	Acc@5  93.75 ( 93.53)
Epoch: [16][ 30/391]	Time  0.111 ( 0.119)	Data  0.001 ( 0.009)	Loss 7.4463e-01 (9.8519e-01)	Acc@1  76.56 ( 69.93)	Acc@5  96.09 ( 93.65)
Epoch: [16][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 9.1309e-01 (9.8633e-01)	Acc@1  73.44 ( 70.20)	Acc@5  96.09 ( 93.69)
Epoch: [16][ 50/391]	Time  0.106 ( 0.117)	Data  0.001 ( 0.007)	Loss 7.4561e-01 (9.8309e-01)	Acc@1  76.56 ( 70.47)	Acc@5  95.31 ( 93.73)
Epoch: [16][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.1172e+00 (9.9128e-01)	Acc@1  66.41 ( 69.98)	Acc@5  92.97 ( 93.70)
Epoch: [16][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.5156e-01 (9.9050e-01)	Acc@1  70.31 ( 70.05)	Acc@5  96.88 ( 93.77)
Epoch: [16][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.1836e+00 (9.9585e-01)	Acc@1  63.28 ( 69.91)	Acc@5  92.19 ( 93.74)
Epoch: [16][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.0547e+00 (9.9406e-01)	Acc@1  72.66 ( 70.23)	Acc@5  93.75 ( 93.66)
Epoch: [16][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.3154e-01 (1.0014e+00)	Acc@1  76.56 ( 70.04)	Acc@5  92.97 ( 93.52)
Epoch: [16][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.4131e-01 (1.0002e+00)	Acc@1  71.09 ( 69.99)	Acc@5  96.09 ( 93.54)
Epoch: [16][120/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.4336e-01 (1.0086e+00)	Acc@1  67.97 ( 69.69)	Acc@5  94.53 ( 93.46)
Epoch: [16][130/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1455e+00 (1.0093e+00)	Acc@1  66.41 ( 69.79)	Acc@5  90.62 ( 93.37)
Epoch: [16][140/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0684e+00 (1.0156e+00)	Acc@1  64.06 ( 69.59)	Acc@5  90.62 ( 93.31)
Epoch: [16][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.1895e-01 (1.0187e+00)	Acc@1  68.75 ( 69.51)	Acc@5  95.31 ( 93.27)
Epoch: [16][160/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.7168e-01 (1.0217e+00)	Acc@1  75.00 ( 69.47)	Acc@5  93.75 ( 93.21)
Epoch: [16][170/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1172e+00 (1.0287e+00)	Acc@1  64.84 ( 69.27)	Acc@5  94.53 ( 93.16)
Epoch: [16][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.6777e-01 (1.0307e+00)	Acc@1  72.66 ( 69.26)	Acc@5  92.97 ( 93.15)
Epoch: [16][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0146e+00 (1.0309e+00)	Acc@1  70.31 ( 69.20)	Acc@5  92.19 ( 93.14)
Epoch: [16][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1787e+00 (1.0352e+00)	Acc@1  64.06 ( 69.06)	Acc@5  92.97 ( 93.09)
Epoch: [16][210/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0166e+00 (1.0353e+00)	Acc@1  70.31 ( 69.08)	Acc@5  94.53 ( 93.12)
Epoch: [16][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.4043e-01 (1.0349e+00)	Acc@1  72.66 ( 69.11)	Acc@5  96.88 ( 93.18)
Epoch: [16][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4043e+00 (1.0373e+00)	Acc@1  57.81 ( 68.99)	Acc@5  89.06 ( 93.18)
Epoch: [16][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0664e+00 (1.0348e+00)	Acc@1  66.41 ( 69.10)	Acc@5  89.84 ( 93.16)
Epoch: [16][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.5898e-01 (1.0359e+00)	Acc@1  73.44 ( 69.11)	Acc@5  93.75 ( 93.12)
Epoch: [16][260/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0908e+00 (1.0354e+00)	Acc@1  71.88 ( 69.11)	Acc@5  96.09 ( 93.17)
Epoch: [16][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2061e+00 (1.0353e+00)	Acc@1  63.28 ( 69.17)	Acc@5  90.62 ( 93.14)
Epoch: [16][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.9111e-01 (1.0355e+00)	Acc@1  68.75 ( 69.16)	Acc@5  96.09 ( 93.12)
Epoch: [16][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1201e+00 (1.0372e+00)	Acc@1  65.62 ( 69.15)	Acc@5  94.53 ( 93.12)
Epoch: [16][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0420e+00 (1.0387e+00)	Acc@1  63.28 ( 69.11)	Acc@5  94.53 ( 93.10)
Epoch: [16][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0811e+00 (1.0408e+00)	Acc@1  67.97 ( 69.03)	Acc@5  93.75 ( 93.04)
Epoch: [16][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0361e+00 (1.0411e+00)	Acc@1  68.75 ( 69.03)	Acc@5  92.19 ( 93.02)
Epoch: [16][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1377e+00 (1.0447e+00)	Acc@1  63.28 ( 68.94)	Acc@5  89.06 ( 92.95)
Epoch: [16][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0010e+00 (1.0436e+00)	Acc@1  75.00 ( 69.01)	Acc@5  92.19 ( 92.96)
Epoch: [16][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1113e+00 (1.0451e+00)	Acc@1  66.41 ( 68.95)	Acc@5  92.97 ( 92.95)
Epoch: [16][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3535e+00 (1.0472e+00)	Acc@1  60.16 ( 68.91)	Acc@5  88.28 ( 92.92)
Epoch: [16][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2354e+00 (1.0505e+00)	Acc@1  63.28 ( 68.82)	Acc@5  89.84 ( 92.87)
Epoch: [16][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2275e+00 (1.0516e+00)	Acc@1  64.84 ( 68.77)	Acc@5  92.19 ( 92.87)
Epoch: [16][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.6289e-01 (1.0545e+00)	Acc@1  72.50 ( 68.70)	Acc@5  92.50 ( 92.81)
## e[16] optimizer.zero_grad (sum) time: 0.29689955711364746
## e[16]       loss.backward (sum) time: 13.80608320236206
## e[16]      optimizer.step (sum) time: 2.7373735904693604
## epoch[16] training(only) time: 45.075623750686646
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 1.5957e+00 (1.5957e+00)	Acc@1  67.00 ( 67.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.047 ( 0.055)	Loss 1.5020e+00 (1.5149e+00)	Acc@1  58.00 ( 63.18)	Acc@5  88.00 ( 86.09)
Test: [ 20/100]	Time  0.039 ( 0.048)	Loss 1.3340e+00 (1.4730e+00)	Acc@1  64.00 ( 61.90)	Acc@5  91.00 ( 87.33)
Test: [ 30/100]	Time  0.039 ( 0.045)	Loss 1.4561e+00 (1.4823e+00)	Acc@1  56.00 ( 61.19)	Acc@5  87.00 ( 87.06)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.3232e+00 (1.4742e+00)	Acc@1  63.00 ( 60.66)	Acc@5  88.00 ( 87.41)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.4629e+00 (1.4809e+00)	Acc@1  59.00 ( 60.51)	Acc@5  88.00 ( 87.04)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.5156e+00 (1.4594e+00)	Acc@1  56.00 ( 60.75)	Acc@5  85.00 ( 87.13)
Test: [ 70/100]	Time  0.041 ( 0.041)	Loss 1.3906e+00 (1.4625e+00)	Acc@1  59.00 ( 60.73)	Acc@5  85.00 ( 87.10)
Test: [ 80/100]	Time  0.037 ( 0.041)	Loss 1.4707e+00 (1.4624e+00)	Acc@1  66.00 ( 60.74)	Acc@5  84.00 ( 87.02)
Test: [ 90/100]	Time  0.037 ( 0.041)	Loss 1.7646e+00 (1.4517e+00)	Acc@1  54.00 ( 60.91)	Acc@5  82.00 ( 87.13)
 * Acc@1 61.000 Acc@5 87.070
### epoch[16] execution time: 49.223249673843384
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.271 ( 0.271)	Data  0.162 ( 0.162)	Loss 1.0801e+00 (1.0801e+00)	Acc@1  67.19 ( 67.19)	Acc@5  92.97 ( 92.97)
Epoch: [17][ 10/391]	Time  0.112 ( 0.128)	Data  0.001 ( 0.018)	Loss 8.3887e-01 (9.7266e-01)	Acc@1  73.44 ( 70.81)	Acc@5  95.31 ( 93.82)
Epoch: [17][ 20/391]	Time  0.114 ( 0.122)	Data  0.001 ( 0.011)	Loss 8.1006e-01 (9.4817e-01)	Acc@1  78.12 ( 71.88)	Acc@5  96.09 ( 94.05)
Epoch: [17][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.0234e+00 (9.3317e-01)	Acc@1  69.53 ( 72.48)	Acc@5  94.53 ( 94.43)
Epoch: [17][ 40/391]	Time  0.117 ( 0.118)	Data  0.001 ( 0.008)	Loss 8.5205e-01 (9.2327e-01)	Acc@1  76.56 ( 72.41)	Acc@5  93.75 ( 94.46)
Epoch: [17][ 50/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.007)	Loss 8.4180e-01 (9.1855e-01)	Acc@1  77.34 ( 72.33)	Acc@5  94.53 ( 94.36)
Epoch: [17][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 9.5850e-01 (9.2175e-01)	Acc@1  71.88 ( 72.44)	Acc@5  94.53 ( 94.39)
Epoch: [17][ 70/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.0957e+00 (9.2747e-01)	Acc@1  65.62 ( 72.12)	Acc@5  93.75 ( 94.37)
Epoch: [17][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.1406e+00 (9.4074e-01)	Acc@1  63.28 ( 71.73)	Acc@5  92.19 ( 94.06)
Epoch: [17][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.0020e+00 (9.4759e-01)	Acc@1  71.09 ( 71.51)	Acc@5  94.53 ( 93.95)
Epoch: [17][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0205e+00 (9.4889e-01)	Acc@1  69.53 ( 71.57)	Acc@5  89.06 ( 93.88)
Epoch: [17][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.8145e-01 (9.5469e-01)	Acc@1  71.88 ( 71.47)	Acc@5  90.62 ( 93.79)
Epoch: [17][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0430e+00 (9.6032e-01)	Acc@1  71.88 ( 71.30)	Acc@5  91.41 ( 93.67)
Epoch: [17][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.3408e-01 (9.6808e-01)	Acc@1  68.75 ( 71.17)	Acc@5  94.53 ( 93.58)
Epoch: [17][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.7754e-01 (9.6564e-01)	Acc@1  70.31 ( 71.22)	Acc@5  96.88 ( 93.65)
Epoch: [17][150/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0127e+00 (9.6677e-01)	Acc@1  68.75 ( 71.30)	Acc@5  92.97 ( 93.68)
Epoch: [17][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.9170e-01 (9.6890e-01)	Acc@1  71.88 ( 71.27)	Acc@5  94.53 ( 93.64)
Epoch: [17][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.7461e-01 (9.7221e-01)	Acc@1  65.62 ( 71.14)	Acc@5  93.75 ( 93.61)
Epoch: [17][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.7354e-01 (9.7197e-01)	Acc@1  71.88 ( 71.19)	Acc@5  94.53 ( 93.63)
Epoch: [17][190/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.2969e-01 (9.7443e-01)	Acc@1  75.00 ( 71.20)	Acc@5  90.62 ( 93.59)
Epoch: [17][200/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0273e+00 (9.7595e-01)	Acc@1  68.75 ( 71.08)	Acc@5  93.75 ( 93.59)
Epoch: [17][210/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.8623e-01 (9.7763e-01)	Acc@1  75.00 ( 71.06)	Acc@5  93.75 ( 93.57)
Epoch: [17][220/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1475e+00 (9.8246e-01)	Acc@1  67.97 ( 70.92)	Acc@5  89.84 ( 93.53)
Epoch: [17][230/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1836e+00 (9.8304e-01)	Acc@1  65.62 ( 70.89)	Acc@5  92.19 ( 93.54)
Epoch: [17][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1260e+00 (9.8373e-01)	Acc@1  64.84 ( 70.83)	Acc@5  90.62 ( 93.54)
Epoch: [17][250/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4570e-01 (9.8358e-01)	Acc@1  72.66 ( 70.81)	Acc@5  97.66 ( 93.55)
Epoch: [17][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1592e+00 (9.8444e-01)	Acc@1  67.19 ( 70.74)	Acc@5  89.06 ( 93.56)
Epoch: [17][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1592e+00 (9.8565e-01)	Acc@1  64.84 ( 70.71)	Acc@5  91.41 ( 93.55)
Epoch: [17][280/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.7939e-01 (9.8538e-01)	Acc@1  78.12 ( 70.76)	Acc@5  92.97 ( 93.54)
Epoch: [17][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9248e-01 (9.8388e-01)	Acc@1  78.91 ( 70.78)	Acc@5  94.53 ( 93.56)
Epoch: [17][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4531e-01 (9.8391e-01)	Acc@1  75.78 ( 70.75)	Acc@5  91.41 ( 93.56)
Epoch: [17][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.8096e-01 (9.8437e-01)	Acc@1  67.19 ( 70.73)	Acc@5  95.31 ( 93.55)
Epoch: [17][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1611e+00 (9.8725e-01)	Acc@1  69.53 ( 70.67)	Acc@5  91.41 ( 93.52)
Epoch: [17][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0371e+00 (9.9062e-01)	Acc@1  69.53 ( 70.61)	Acc@5  93.75 ( 93.50)
Epoch: [17][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0850e+00 (9.9138e-01)	Acc@1  71.88 ( 70.58)	Acc@5  89.84 ( 93.49)
Epoch: [17][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0068e+00 (9.9066e-01)	Acc@1  70.31 ( 70.61)	Acc@5  91.41 ( 93.49)
Epoch: [17][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0605e+00 (9.9229e-01)	Acc@1  70.31 ( 70.55)	Acc@5  92.97 ( 93.48)
Epoch: [17][370/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.7500e-01 (9.9184e-01)	Acc@1  72.66 ( 70.55)	Acc@5  91.41 ( 93.47)
Epoch: [17][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9980e-01 (9.9390e-01)	Acc@1  78.91 ( 70.49)	Acc@5  96.88 ( 93.43)
Epoch: [17][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0566e+00 (9.9476e-01)	Acc@1  68.75 ( 70.43)	Acc@5  92.50 ( 93.41)
## e[17] optimizer.zero_grad (sum) time: 0.2910759449005127
## e[17]       loss.backward (sum) time: 13.791318416595459
## e[17]      optimizer.step (sum) time: 2.698585271835327
## epoch[17] training(only) time: 45.06670808792114
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 1.6045e+00 (1.6045e+00)	Acc@1  58.00 ( 58.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.7793e+00 (1.6093e+00)	Acc@1  51.00 ( 59.82)	Acc@5  88.00 ( 85.64)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.4648e+00 (1.5737e+00)	Acc@1  62.00 ( 59.57)	Acc@5  88.00 ( 86.33)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 1.7549e+00 (1.5916e+00)	Acc@1  58.00 ( 59.03)	Acc@5  82.00 ( 86.00)
Test: [ 40/100]	Time  0.041 ( 0.043)	Loss 1.4990e+00 (1.5876e+00)	Acc@1  56.00 ( 58.93)	Acc@5  90.00 ( 86.32)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.3564e+00 (1.5924e+00)	Acc@1  62.00 ( 58.73)	Acc@5  89.00 ( 86.08)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.6240e+00 (1.5738e+00)	Acc@1  53.00 ( 59.07)	Acc@5  84.00 ( 86.16)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 1.6484e+00 (1.5691e+00)	Acc@1  54.00 ( 59.00)	Acc@5  87.00 ( 86.20)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.6221e+00 (1.5713e+00)	Acc@1  65.00 ( 59.01)	Acc@5  83.00 ( 86.06)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.9736e+00 (1.5679e+00)	Acc@1  49.00 ( 58.88)	Acc@5  80.00 ( 86.05)
 * Acc@1 58.940 Acc@5 86.070
### epoch[17] execution time: 49.19565558433533
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.272 ( 0.272)	Data  0.164 ( 0.164)	Loss 1.1309e+00 (1.1309e+00)	Acc@1  67.97 ( 67.97)	Acc@5  90.62 ( 90.62)
Epoch: [18][ 10/391]	Time  0.114 ( 0.128)	Data  0.001 ( 0.018)	Loss 9.5752e-01 (8.7291e-01)	Acc@1  72.66 ( 73.51)	Acc@5  93.75 ( 94.89)
Epoch: [18][ 20/391]	Time  0.111 ( 0.122)	Data  0.001 ( 0.011)	Loss 8.2324e-01 (8.6849e-01)	Acc@1  75.78 ( 73.77)	Acc@5  96.09 ( 95.05)
Epoch: [18][ 30/391]	Time  0.115 ( 0.119)	Data  0.001 ( 0.009)	Loss 9.1211e-01 (8.8451e-01)	Acc@1  67.19 ( 72.86)	Acc@5  92.97 ( 94.91)
Epoch: [18][ 40/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.008)	Loss 8.9746e-01 (8.8762e-01)	Acc@1  67.97 ( 72.85)	Acc@5  95.31 ( 94.97)
Epoch: [18][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 8.2568e-01 (8.8274e-01)	Acc@1  76.56 ( 72.98)	Acc@5  92.97 ( 95.01)
Epoch: [18][ 60/391]	Time  0.110 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.0215e-01 (8.7638e-01)	Acc@1  82.03 ( 73.32)	Acc@5  97.66 ( 94.99)
Epoch: [18][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.4375e-01 (8.7810e-01)	Acc@1  78.91 ( 73.48)	Acc@5  95.31 ( 94.92)
Epoch: [18][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.3252e-01 (8.7996e-01)	Acc@1  72.66 ( 73.36)	Acc@5  96.09 ( 94.90)
Epoch: [18][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.8271e-01 (8.8369e-01)	Acc@1  77.34 ( 73.35)	Acc@5  95.31 ( 94.88)
Epoch: [18][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.4873e-01 (8.8020e-01)	Acc@1  70.31 ( 73.48)	Acc@5  91.41 ( 94.86)
Epoch: [18][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.6240e-01 (8.8244e-01)	Acc@1  69.53 ( 73.36)	Acc@5  94.53 ( 94.77)
Epoch: [18][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.9990e-01 (8.8964e-01)	Acc@1  76.56 ( 73.19)	Acc@5  92.19 ( 94.63)
Epoch: [18][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.0479e-01 (8.9383e-01)	Acc@1  70.31 ( 73.07)	Acc@5  96.88 ( 94.59)
Epoch: [18][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.7822e-01 (8.9098e-01)	Acc@1  78.91 ( 73.17)	Acc@5  97.66 ( 94.61)
Epoch: [18][150/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1299e+00 (8.9873e-01)	Acc@1  65.62 ( 72.97)	Acc@5  92.19 ( 94.50)
Epoch: [18][160/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.9170e-01 (9.0421e-01)	Acc@1  74.22 ( 72.80)	Acc@5  94.53 ( 94.44)
Epoch: [18][170/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.6084e-01 (9.0556e-01)	Acc@1  73.44 ( 72.83)	Acc@5  97.66 ( 94.44)
Epoch: [18][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.5557e-01 (9.0764e-01)	Acc@1  70.31 ( 72.79)	Acc@5  94.53 ( 94.42)
Epoch: [18][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.8916e-01 (9.0780e-01)	Acc@1  69.53 ( 72.75)	Acc@5  92.97 ( 94.42)
Epoch: [18][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.6143e-01 (9.0661e-01)	Acc@1  71.88 ( 72.77)	Acc@5  94.53 ( 94.41)
Epoch: [18][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2305e+00 (9.1066e-01)	Acc@1  63.28 ( 72.66)	Acc@5  92.97 ( 94.36)
Epoch: [18][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.0762e-01 (9.1318e-01)	Acc@1  77.34 ( 72.53)	Acc@5  92.97 ( 94.34)
Epoch: [18][230/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.3164e-01 (9.1657e-01)	Acc@1  67.19 ( 72.40)	Acc@5  93.75 ( 94.27)
Epoch: [18][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1885e+00 (9.2162e-01)	Acc@1  66.41 ( 72.31)	Acc@5  91.41 ( 94.18)
Epoch: [18][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0947e+00 (9.2562e-01)	Acc@1  65.62 ( 72.24)	Acc@5  92.19 ( 94.14)
Epoch: [18][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0811e+00 (9.2904e-01)	Acc@1  64.06 ( 72.14)	Acc@5  93.75 ( 94.10)
Epoch: [18][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0537e+00 (9.3023e-01)	Acc@1  71.09 ( 72.09)	Acc@5  94.53 ( 94.12)
Epoch: [18][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4043e-01 (9.3072e-01)	Acc@1  68.75 ( 72.05)	Acc@5  93.75 ( 94.14)
Epoch: [18][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.8242e-01 (9.3271e-01)	Acc@1  70.31 ( 72.00)	Acc@5  90.62 ( 94.13)
Epoch: [18][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.1787e-01 (9.3450e-01)	Acc@1  78.91 ( 71.96)	Acc@5  94.53 ( 94.09)
Epoch: [18][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0439e+00 (9.3440e-01)	Acc@1  64.84 ( 71.93)	Acc@5  95.31 ( 94.11)
Epoch: [18][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4922e-01 (9.3432e-01)	Acc@1  71.88 ( 71.93)	Acc@5  92.97 ( 94.10)
Epoch: [18][330/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.5312e-01 (9.3679e-01)	Acc@1  70.31 ( 71.88)	Acc@5  96.88 ( 94.08)
Epoch: [18][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0674e+00 (9.3844e-01)	Acc@1  64.84 ( 71.85)	Acc@5  94.53 ( 94.05)
Epoch: [18][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.9795e-01 (9.3981e-01)	Acc@1  72.66 ( 71.83)	Acc@5  97.66 ( 94.03)
Epoch: [18][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.2471e-01 (9.4121e-01)	Acc@1  73.44 ( 71.81)	Acc@5  98.44 ( 94.01)
Epoch: [18][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.0430e-01 (9.4442e-01)	Acc@1  71.09 ( 71.72)	Acc@5  92.19 ( 93.95)
Epoch: [18][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.2910e-01 (9.4428e-01)	Acc@1  73.44 ( 71.71)	Acc@5  96.09 ( 93.92)
Epoch: [18][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3701e+00 (9.4559e-01)	Acc@1  58.75 ( 71.66)	Acc@5  91.25 ( 93.92)
## e[18] optimizer.zero_grad (sum) time: 0.28981924057006836
## e[18]       loss.backward (sum) time: 13.769148826599121
## e[18]      optimizer.step (sum) time: 2.7163567543029785
## epoch[18] training(only) time: 45.09033536911011
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 1.5000e+00 (1.5000e+00)	Acc@1  63.00 ( 63.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.046 ( 0.055)	Loss 1.5381e+00 (1.3683e+00)	Acc@1  60.00 ( 64.73)	Acc@5  91.00 ( 88.91)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.2568e+00 (1.3129e+00)	Acc@1  67.00 ( 65.24)	Acc@5  91.00 ( 89.38)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 1.6113e+00 (1.3263e+00)	Acc@1  55.00 ( 64.00)	Acc@5  87.00 ( 89.06)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.1797e+00 (1.3315e+00)	Acc@1  70.00 ( 63.85)	Acc@5  91.00 ( 89.37)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.2695e+00 (1.3440e+00)	Acc@1  68.00 ( 63.82)	Acc@5  91.00 ( 88.98)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.4043e+00 (1.3335e+00)	Acc@1  59.00 ( 63.92)	Acc@5  88.00 ( 89.34)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.2676e+00 (1.3372e+00)	Acc@1  69.00 ( 63.89)	Acc@5  90.00 ( 89.37)
Test: [ 80/100]	Time  0.037 ( 0.041)	Loss 1.4219e+00 (1.3390e+00)	Acc@1  63.00 ( 63.81)	Acc@5  88.00 ( 89.27)
Test: [ 90/100]	Time  0.038 ( 0.040)	Loss 1.6562e+00 (1.3331e+00)	Acc@1  53.00 ( 63.98)	Acc@5  88.00 ( 89.33)
 * Acc@1 63.820 Acc@5 89.250
### epoch[18] execution time: 49.19458746910095
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.274 ( 0.274)	Data  0.163 ( 0.163)	Loss 7.6562e-01 (7.6562e-01)	Acc@1  78.91 ( 78.91)	Acc@5  96.09 ( 96.09)
Epoch: [19][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.018)	Loss 6.7432e-01 (7.7271e-01)	Acc@1  77.34 ( 76.92)	Acc@5  96.88 ( 95.74)
Epoch: [19][ 20/391]	Time  0.112 ( 0.122)	Data  0.001 ( 0.011)	Loss 8.0225e-01 (7.9354e-01)	Acc@1  74.22 ( 75.74)	Acc@5  98.44 ( 95.50)
Epoch: [19][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 7.0068e-01 (7.9096e-01)	Acc@1  77.34 ( 75.53)	Acc@5  97.66 ( 95.67)
Epoch: [19][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 9.5898e-01 (8.0691e-01)	Acc@1  68.75 ( 75.30)	Acc@5  95.31 ( 95.29)
Epoch: [19][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 6.5625e-01 (8.1273e-01)	Acc@1  79.69 ( 75.34)	Acc@5  96.88 ( 95.21)
Epoch: [19][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.8955e-01 (8.0596e-01)	Acc@1  76.56 ( 75.69)	Acc@5  96.88 ( 95.43)
Epoch: [19][ 70/391]	Time  0.110 ( 0.117)	Data  0.001 ( 0.006)	Loss 9.5215e-01 (8.0657e-01)	Acc@1  66.41 ( 75.57)	Acc@5  96.09 ( 95.44)
Epoch: [19][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.4121e-01 (8.0620e-01)	Acc@1  75.78 ( 75.56)	Acc@5  93.75 ( 95.42)
Epoch: [19][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.9834e-01 (8.1218e-01)	Acc@1  73.44 ( 75.37)	Acc@5  94.53 ( 95.30)
Epoch: [19][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.1055e-01 (8.1941e-01)	Acc@1  72.66 ( 75.10)	Acc@5  97.66 ( 95.27)
Epoch: [19][110/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.7109e-01 (8.2558e-01)	Acc@1  75.00 ( 74.89)	Acc@5  95.31 ( 95.19)
Epoch: [19][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.4473e-01 (8.2974e-01)	Acc@1  80.47 ( 74.88)	Acc@5  96.88 ( 95.19)
Epoch: [19][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.1299e-01 (8.3629e-01)	Acc@1  74.22 ( 74.70)	Acc@5  96.09 ( 95.16)
Epoch: [19][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.7539e-01 (8.4074e-01)	Acc@1  76.56 ( 74.58)	Acc@5  96.88 ( 95.10)
Epoch: [19][150/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.7979e-01 (8.4961e-01)	Acc@1  73.44 ( 74.32)	Acc@5  95.31 ( 95.02)
Epoch: [19][160/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.5205e-01 (8.5185e-01)	Acc@1  75.00 ( 74.19)	Acc@5  93.75 ( 95.03)
Epoch: [19][170/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.0859e-01 (8.5081e-01)	Acc@1  72.66 ( 74.16)	Acc@5  96.09 ( 95.07)
Epoch: [19][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.6182e-01 (8.5238e-01)	Acc@1  72.66 ( 74.13)	Acc@5  95.31 ( 95.06)
Epoch: [19][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.5752e-01 (8.5720e-01)	Acc@1  67.97 ( 73.95)	Acc@5  93.75 ( 95.02)
Epoch: [19][200/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.9795e-01 (8.6299e-01)	Acc@1  70.31 ( 73.69)	Acc@5  98.44 ( 94.99)
Epoch: [19][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.1895e-01 (8.6537e-01)	Acc@1  68.75 ( 73.58)	Acc@5  93.75 ( 94.99)
Epoch: [19][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.6035e-01 (8.6637e-01)	Acc@1  70.31 ( 73.56)	Acc@5  94.53 ( 94.95)
Epoch: [19][230/391]	Time  0.118 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.6621e-01 (8.6854e-01)	Acc@1  73.44 ( 73.47)	Acc@5  94.53 ( 94.94)
Epoch: [19][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.4678e-01 (8.7086e-01)	Acc@1  75.00 ( 73.44)	Acc@5  91.41 ( 94.90)
Epoch: [19][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0381e+00 (8.7471e-01)	Acc@1  69.53 ( 73.39)	Acc@5  93.75 ( 94.84)
Epoch: [19][260/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0654e+00 (8.7848e-01)	Acc@1  70.31 ( 73.25)	Acc@5  92.19 ( 94.80)
Epoch: [19][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.2773e-01 (8.7964e-01)	Acc@1  71.09 ( 73.24)	Acc@5  96.09 ( 94.79)
Epoch: [19][280/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0215e-01 (8.8106e-01)	Acc@1  80.47 ( 73.21)	Acc@5  96.88 ( 94.80)
Epoch: [19][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0869e+00 (8.8271e-01)	Acc@1  67.19 ( 73.13)	Acc@5  92.19 ( 94.78)
Epoch: [19][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.8330e-01 (8.8435e-01)	Acc@1  71.88 ( 73.08)	Acc@5  96.09 ( 94.75)
Epoch: [19][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.2871e-01 (8.8580e-01)	Acc@1  71.88 ( 73.08)	Acc@5  94.53 ( 94.73)
Epoch: [19][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2412e+00 (8.8690e-01)	Acc@1  60.94 ( 73.04)	Acc@5  93.75 ( 94.74)
Epoch: [19][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.1943e-01 (8.9015e-01)	Acc@1  74.22 ( 72.96)	Acc@5  96.88 ( 94.71)
Epoch: [19][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0771e+00 (8.9387e-01)	Acc@1  64.06 ( 72.87)	Acc@5  93.75 ( 94.65)
Epoch: [19][350/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0732e+00 (8.9616e-01)	Acc@1  66.41 ( 72.82)	Acc@5  94.53 ( 94.62)
Epoch: [19][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0479e+00 (8.9797e-01)	Acc@1  71.88 ( 72.80)	Acc@5  92.19 ( 94.59)
Epoch: [19][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0205e+00 (8.9886e-01)	Acc@1  67.97 ( 72.78)	Acc@5  92.97 ( 94.56)
Epoch: [19][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0645e+00 (8.9762e-01)	Acc@1  67.97 ( 72.82)	Acc@5  92.19 ( 94.59)
Epoch: [19][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.0566e-01 (8.9777e-01)	Acc@1  78.75 ( 72.82)	Acc@5 100.00 ( 94.60)
## e[19] optimizer.zero_grad (sum) time: 0.29037928581237793
## e[19]       loss.backward (sum) time: 13.75912094116211
## e[19]      optimizer.step (sum) time: 2.7386841773986816
## epoch[19] training(only) time: 45.074578523635864
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.2510e+00 (1.2510e+00)	Acc@1  65.00 ( 65.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.040 ( 0.052)	Loss 1.4297e+00 (1.3672e+00)	Acc@1  55.00 ( 63.36)	Acc@5  91.00 ( 88.91)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.4014e+00 (1.3401e+00)	Acc@1  62.00 ( 64.29)	Acc@5  89.00 ( 89.62)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 1.4668e+00 (1.3412e+00)	Acc@1  62.00 ( 64.61)	Acc@5  87.00 ( 89.26)
Test: [ 40/100]	Time  0.040 ( 0.043)	Loss 1.0908e+00 (1.3334e+00)	Acc@1  71.00 ( 64.37)	Acc@5  94.00 ( 89.41)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.2529e+00 (1.3464e+00)	Acc@1  63.00 ( 63.94)	Acc@5  88.00 ( 89.00)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 1.3848e+00 (1.3303e+00)	Acc@1  60.00 ( 63.84)	Acc@5  89.00 ( 89.36)
Test: [ 70/100]	Time  0.037 ( 0.041)	Loss 1.2754e+00 (1.3361e+00)	Acc@1  59.00 ( 63.69)	Acc@5  90.00 ( 89.34)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.5654e+00 (1.3448e+00)	Acc@1  59.00 ( 63.60)	Acc@5  88.00 ( 89.15)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.6826e+00 (1.3359e+00)	Acc@1  64.00 ( 63.80)	Acc@5  86.00 ( 89.13)
 * Acc@1 63.790 Acc@5 89.150
### epoch[19] execution time: 49.2041015625
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.263 ( 0.263)	Data  0.158 ( 0.158)	Loss 7.5244e-01 (7.5244e-01)	Acc@1  76.56 ( 76.56)	Acc@5  95.31 ( 95.31)
Epoch: [20][ 10/391]	Time  0.111 ( 0.128)	Data  0.001 ( 0.018)	Loss 7.6855e-01 (7.0794e-01)	Acc@1  74.22 ( 77.49)	Acc@5  97.66 ( 96.73)
Epoch: [20][ 20/391]	Time  0.113 ( 0.121)	Data  0.001 ( 0.011)	Loss 8.2715e-01 (7.3753e-01)	Acc@1  70.31 ( 76.93)	Acc@5  96.88 ( 96.21)
Epoch: [20][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 9.4092e-01 (7.4694e-01)	Acc@1  74.22 ( 76.84)	Acc@5  94.53 ( 96.35)
Epoch: [20][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 9.3994e-01 (7.6005e-01)	Acc@1  71.88 ( 76.54)	Acc@5  95.31 ( 96.19)
Epoch: [20][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 6.2402e-01 (7.7358e-01)	Acc@1  83.59 ( 76.16)	Acc@5  98.44 ( 96.09)
Epoch: [20][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 5.9619e-01 (7.8649e-01)	Acc@1  84.38 ( 75.63)	Acc@5  97.66 ( 95.89)
Epoch: [20][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.2939e-01 (7.8994e-01)	Acc@1  76.56 ( 75.54)	Acc@5  96.88 ( 95.93)
Epoch: [20][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.9346e-01 (7.9168e-01)	Acc@1  72.66 ( 75.39)	Acc@5  96.09 ( 95.94)
Epoch: [20][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.8135e-01 (7.9772e-01)	Acc@1  73.44 ( 75.21)	Acc@5  92.97 ( 95.86)
Epoch: [20][100/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.2178e-01 (8.0221e-01)	Acc@1  69.53 ( 75.20)	Acc@5  96.09 ( 95.80)
Epoch: [20][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.8076e-01 (7.9970e-01)	Acc@1  78.12 ( 75.27)	Acc@5  96.09 ( 95.81)
Epoch: [20][120/391]	Time  0.107 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.7100e-01 (8.0234e-01)	Acc@1  78.12 ( 75.19)	Acc@5  96.09 ( 95.71)
Epoch: [20][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0459e+00 (8.1130e-01)	Acc@1  67.19 ( 74.98)	Acc@5  92.97 ( 95.62)
Epoch: [20][140/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.8037e-01 (8.1493e-01)	Acc@1  77.34 ( 74.87)	Acc@5  93.75 ( 95.56)
Epoch: [20][150/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0439e+00 (8.1827e-01)	Acc@1  67.97 ( 74.74)	Acc@5  95.31 ( 95.50)
Epoch: [20][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.0811e-01 (8.1959e-01)	Acc@1  73.44 ( 74.71)	Acc@5  95.31 ( 95.50)
Epoch: [20][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.2920e-01 (8.2172e-01)	Acc@1  67.97 ( 74.65)	Acc@5  96.09 ( 95.49)
Epoch: [20][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.5449e-01 (8.2624e-01)	Acc@1  76.56 ( 74.62)	Acc@5  93.75 ( 95.46)
Epoch: [20][190/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.3643e-01 (8.2561e-01)	Acc@1  71.09 ( 74.65)	Acc@5  94.53 ( 95.46)
Epoch: [20][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.0273e-01 (8.3157e-01)	Acc@1  77.34 ( 74.47)	Acc@5  96.09 ( 95.41)
Epoch: [20][210/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.3066e-01 (8.3228e-01)	Acc@1  72.66 ( 74.46)	Acc@5  94.53 ( 95.43)
Epoch: [20][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.3740e-01 (8.3396e-01)	Acc@1  74.22 ( 74.40)	Acc@5  96.09 ( 95.44)
Epoch: [20][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0322e+00 (8.3542e-01)	Acc@1  64.06 ( 74.37)	Acc@5  97.66 ( 95.46)
Epoch: [20][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.9785e-01 (8.3780e-01)	Acc@1  77.34 ( 74.27)	Acc@5  96.09 ( 95.45)
Epoch: [20][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.5801e-01 (8.4021e-01)	Acc@1  72.66 ( 74.23)	Acc@5  95.31 ( 95.41)
Epoch: [20][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.8076e-01 (8.4279e-01)	Acc@1  75.78 ( 74.17)	Acc@5  95.31 ( 95.38)
Epoch: [20][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.0576e-01 (8.4223e-01)	Acc@1  75.78 ( 74.19)	Acc@5  94.53 ( 95.42)
Epoch: [20][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.6670e-01 (8.4080e-01)	Acc@1  71.88 ( 74.18)	Acc@5  94.53 ( 95.42)
Epoch: [20][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.7598e-01 (8.4230e-01)	Acc@1  71.88 ( 74.14)	Acc@5  94.53 ( 95.43)
Epoch: [20][300/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1016e+00 (8.4600e-01)	Acc@1  64.06 ( 74.03)	Acc@5  93.75 ( 95.40)
Epoch: [20][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.7812e-01 (8.4666e-01)	Acc@1  82.81 ( 74.03)	Acc@5  98.44 ( 95.37)
Epoch: [20][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.8965e-01 (8.4701e-01)	Acc@1  72.66 ( 74.02)	Acc@5  95.31 ( 95.37)
Epoch: [20][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.2178e-01 (8.4941e-01)	Acc@1  78.12 ( 73.93)	Acc@5  93.75 ( 95.34)
Epoch: [20][340/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.6621e-01 (8.5162e-01)	Acc@1  71.88 ( 73.87)	Acc@5  94.53 ( 95.30)
Epoch: [20][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9980e-01 (8.5329e-01)	Acc@1  77.34 ( 73.81)	Acc@5  92.97 ( 95.27)
Epoch: [20][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.9258e-01 (8.5481e-01)	Acc@1  75.00 ( 73.81)	Acc@5  96.09 ( 95.27)
Epoch: [20][370/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4775e-01 (8.5465e-01)	Acc@1  71.09 ( 73.86)	Acc@5  93.75 ( 95.25)
Epoch: [20][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.2871e-01 (8.5526e-01)	Acc@1  75.78 ( 73.86)	Acc@5  96.88 ( 95.23)
Epoch: [20][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4619e-01 (8.5513e-01)	Acc@1  77.50 ( 73.84)	Acc@5  97.50 ( 95.23)
## e[20] optimizer.zero_grad (sum) time: 0.29084181785583496
## e[20]       loss.backward (sum) time: 13.77931523323059
## e[20]      optimizer.step (sum) time: 2.7170398235321045
## epoch[20] training(only) time: 45.10274052619934
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.5078e+00 (1.5078e+00)	Acc@1  62.00 ( 62.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.2510e+00 (1.3569e+00)	Acc@1  61.00 ( 64.36)	Acc@5  93.00 ( 88.55)
Test: [ 20/100]	Time  0.041 ( 0.046)	Loss 1.1104e+00 (1.2992e+00)	Acc@1  72.00 ( 65.62)	Acc@5  90.00 ( 90.14)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 1.6611e+00 (1.3394e+00)	Acc@1  54.00 ( 64.45)	Acc@5  86.00 ( 89.29)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.2158e+00 (1.3305e+00)	Acc@1  67.00 ( 64.29)	Acc@5  93.00 ( 89.66)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.3467e+00 (1.3507e+00)	Acc@1  62.00 ( 63.80)	Acc@5  91.00 ( 89.24)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.3730e+00 (1.3373e+00)	Acc@1  54.00 ( 63.82)	Acc@5  92.00 ( 89.39)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 1.4570e+00 (1.3399e+00)	Acc@1  60.00 ( 64.00)	Acc@5  89.00 ( 89.35)
Test: [ 80/100]	Time  0.041 ( 0.041)	Loss 1.5762e+00 (1.3474e+00)	Acc@1  63.00 ( 63.86)	Acc@5  84.00 ( 89.27)
Test: [ 90/100]	Time  0.041 ( 0.040)	Loss 1.4873e+00 (1.3316e+00)	Acc@1  59.00 ( 64.11)	Acc@5  89.00 ( 89.43)
 * Acc@1 64.220 Acc@5 89.530
### epoch[20] execution time: 49.22763395309448
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.270 ( 0.270)	Data  0.153 ( 0.153)	Loss 6.6797e-01 (6.6797e-01)	Acc@1  83.59 ( 83.59)	Acc@5  98.44 ( 98.44)
Epoch: [21][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.017)	Loss 8.7842e-01 (7.3211e-01)	Acc@1  75.78 ( 78.84)	Acc@5  96.09 ( 96.24)
Epoch: [21][ 20/391]	Time  0.115 ( 0.121)	Data  0.001 ( 0.011)	Loss 5.9473e-01 (7.1729e-01)	Acc@1  82.03 ( 78.42)	Acc@5  97.66 ( 96.73)
Epoch: [21][ 30/391]	Time  0.115 ( 0.119)	Data  0.001 ( 0.008)	Loss 7.9346e-01 (7.1946e-01)	Acc@1  74.22 ( 78.00)	Acc@5  95.31 ( 96.67)
Epoch: [21][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 7.1484e-01 (7.2815e-01)	Acc@1  78.12 ( 77.57)	Acc@5  96.88 ( 96.55)
Epoch: [21][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 8.2178e-01 (7.4030e-01)	Acc@1  74.22 ( 77.07)	Acc@5  93.75 ( 96.34)
Epoch: [21][ 60/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.6182e-01 (7.4649e-01)	Acc@1  75.00 ( 77.00)	Acc@5  95.31 ( 96.29)
Epoch: [21][ 70/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.006)	Loss 9.3896e-01 (7.5323e-01)	Acc@1  70.31 ( 76.55)	Acc@5  92.97 ( 96.24)
Epoch: [21][ 80/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.3984e-01 (7.5577e-01)	Acc@1  76.56 ( 76.42)	Acc@5  94.53 ( 96.21)
Epoch: [21][ 90/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.3447e-01 (7.5803e-01)	Acc@1  71.09 ( 76.33)	Acc@5  96.09 ( 96.21)
Epoch: [21][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.1211e-01 (7.6199e-01)	Acc@1  69.53 ( 76.14)	Acc@5  94.53 ( 96.17)
Epoch: [21][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.9775e-01 (7.6005e-01)	Acc@1  79.69 ( 76.30)	Acc@5  96.88 ( 96.20)
Epoch: [21][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.0605e-01 (7.5906e-01)	Acc@1  76.56 ( 76.33)	Acc@5  94.53 ( 96.15)
Epoch: [21][130/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.5996e-01 (7.6268e-01)	Acc@1  72.66 ( 76.19)	Acc@5  93.75 ( 96.14)
Epoch: [21][140/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.4580e-01 (7.6948e-01)	Acc@1  71.09 ( 76.07)	Acc@5  93.75 ( 96.02)
Epoch: [21][150/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5195e-01 (7.7298e-01)	Acc@1  78.91 ( 76.08)	Acc@5  96.88 ( 96.01)
Epoch: [21][160/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.3584e-01 (7.7562e-01)	Acc@1  79.69 ( 76.01)	Acc@5  93.75 ( 95.97)
Epoch: [21][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.6719e-01 (7.7777e-01)	Acc@1  75.00 ( 76.00)	Acc@5  95.31 ( 95.94)
Epoch: [21][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.3994e-01 (7.7730e-01)	Acc@1  74.22 ( 76.08)	Acc@5  92.19 ( 95.94)
Epoch: [21][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.0898e-01 (7.7734e-01)	Acc@1  81.25 ( 76.19)	Acc@5  96.09 ( 95.93)
Epoch: [21][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.3994e-01 (7.8069e-01)	Acc@1  73.44 ( 76.09)	Acc@5  93.75 ( 95.91)
Epoch: [21][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.3193e-01 (7.8417e-01)	Acc@1  78.12 ( 76.01)	Acc@5  96.09 ( 95.86)
Epoch: [21][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.1289e-01 (7.8357e-01)	Acc@1  75.00 ( 76.00)	Acc@5  94.53 ( 95.87)
Epoch: [21][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9346e-01 (7.8582e-01)	Acc@1  78.12 ( 75.98)	Acc@5  96.88 ( 95.81)
Epoch: [21][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.2812e-01 (7.8741e-01)	Acc@1  74.22 ( 75.89)	Acc@5  95.31 ( 95.79)
Epoch: [21][250/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.9463e-01 (7.9077e-01)	Acc@1  71.09 ( 75.80)	Acc@5  95.31 ( 95.77)
Epoch: [21][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.6133e-01 (7.9157e-01)	Acc@1  76.56 ( 75.80)	Acc@5  92.97 ( 95.75)
Epoch: [21][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4131e-01 (7.9599e-01)	Acc@1  72.66 ( 75.65)	Acc@5  95.31 ( 95.72)
Epoch: [21][280/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4727e-01 (7.9963e-01)	Acc@1  72.66 ( 75.56)	Acc@5  91.41 ( 95.67)
Epoch: [21][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.7012e-01 (8.0081e-01)	Acc@1  75.00 ( 75.52)	Acc@5  96.09 ( 95.66)
Epoch: [21][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9395e-01 (8.0255e-01)	Acc@1  77.34 ( 75.47)	Acc@5  92.97 ( 95.61)
Epoch: [21][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.5205e-01 (8.0331e-01)	Acc@1  73.44 ( 75.43)	Acc@5  96.88 ( 95.60)
Epoch: [21][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.2227e-01 (8.0316e-01)	Acc@1  73.44 ( 75.43)	Acc@5  96.09 ( 95.62)
Epoch: [21][330/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.8916e-01 (8.0524e-01)	Acc@1  72.66 ( 75.39)	Acc@5  94.53 ( 95.61)
Epoch: [21][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0547e+00 (8.0826e-01)	Acc@1  67.97 ( 75.34)	Acc@5  92.19 ( 95.56)
Epoch: [21][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.3447e-01 (8.0883e-01)	Acc@1  74.22 ( 75.32)	Acc@5  94.53 ( 95.56)
Epoch: [21][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.8584e-01 (8.0989e-01)	Acc@1  64.84 ( 75.29)	Acc@5  93.75 ( 95.53)
Epoch: [21][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.5645e-01 (8.1087e-01)	Acc@1  73.44 ( 75.27)	Acc@5  95.31 ( 95.53)
Epoch: [21][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.1201e-01 (8.1191e-01)	Acc@1  74.22 ( 75.23)	Acc@5  95.31 ( 95.51)
Epoch: [21][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2266e+00 (8.1343e-01)	Acc@1  65.00 ( 75.16)	Acc@5  92.50 ( 95.50)
## e[21] optimizer.zero_grad (sum) time: 0.294295072555542
## e[21]       loss.backward (sum) time: 13.825971841812134
## e[21]      optimizer.step (sum) time: 2.7542951107025146
## epoch[21] training(only) time: 45.06105422973633
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.4688e+00 (1.4688e+00)	Acc@1  63.00 ( 63.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.043 ( 0.054)	Loss 1.4482e+00 (1.3801e+00)	Acc@1  64.00 ( 64.45)	Acc@5  90.00 ( 89.09)
Test: [ 20/100]	Time  0.037 ( 0.047)	Loss 1.3154e+00 (1.3487e+00)	Acc@1  65.00 ( 64.71)	Acc@5  89.00 ( 89.71)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 1.2891e+00 (1.3569e+00)	Acc@1  64.00 ( 64.48)	Acc@5  91.00 ( 89.58)
Test: [ 40/100]	Time  0.040 ( 0.043)	Loss 1.2227e+00 (1.3361e+00)	Acc@1  68.00 ( 64.76)	Acc@5  93.00 ( 89.71)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.2598e+00 (1.3422e+00)	Acc@1  60.00 ( 64.20)	Acc@5  93.00 ( 89.35)
Test: [ 60/100]	Time  0.040 ( 0.041)	Loss 1.2539e+00 (1.3240e+00)	Acc@1  70.00 ( 64.08)	Acc@5  90.00 ( 89.64)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.3271e+00 (1.3305e+00)	Acc@1  64.00 ( 64.03)	Acc@5  87.00 ( 89.48)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.6201e+00 (1.3366e+00)	Acc@1  58.00 ( 63.86)	Acc@5  87.00 ( 89.38)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.4375e+00 (1.3238e+00)	Acc@1  59.00 ( 63.95)	Acc@5  91.00 ( 89.49)
 * Acc@1 64.010 Acc@5 89.540
### epoch[21] execution time: 49.20118498802185
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.261 ( 0.261)	Data  0.154 ( 0.154)	Loss 7.4463e-01 (7.4463e-01)	Acc@1  78.12 ( 78.12)	Acc@5  96.88 ( 96.88)
Epoch: [22][ 10/391]	Time  0.114 ( 0.127)	Data  0.001 ( 0.017)	Loss 5.7324e-01 (6.7054e-01)	Acc@1  78.91 ( 79.55)	Acc@5  97.66 ( 96.66)
Epoch: [22][ 20/391]	Time  0.110 ( 0.121)	Data  0.001 ( 0.011)	Loss 7.9932e-01 (7.0782e-01)	Acc@1  71.88 ( 78.01)	Acc@5  95.31 ( 96.84)
Epoch: [22][ 30/391]	Time  0.115 ( 0.119)	Data  0.001 ( 0.009)	Loss 6.2646e-01 (7.1157e-01)	Acc@1  80.47 ( 78.35)	Acc@5  98.44 ( 96.85)
Epoch: [22][ 40/391]	Time  0.110 ( 0.118)	Data  0.001 ( 0.007)	Loss 8.5596e-01 (7.1696e-01)	Acc@1  70.31 ( 77.78)	Acc@5  96.09 ( 96.76)
Epoch: [22][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 7.9736e-01 (7.2008e-01)	Acc@1  75.78 ( 77.77)	Acc@5  94.53 ( 96.66)
Epoch: [22][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.9775e-01 (7.2515e-01)	Acc@1  78.91 ( 77.68)	Acc@5  96.09 ( 96.49)
Epoch: [22][ 70/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.1035e-01 (7.1886e-01)	Acc@1  75.78 ( 77.67)	Acc@5  99.22 ( 96.61)
Epoch: [22][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.8809e-01 (7.2995e-01)	Acc@1  77.34 ( 77.25)	Acc@5  96.88 ( 96.58)
Epoch: [22][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.7402e-01 (7.3481e-01)	Acc@1  73.44 ( 77.09)	Acc@5  92.97 ( 96.51)
Epoch: [22][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.4658e-01 (7.3608e-01)	Acc@1  73.44 ( 77.09)	Acc@5  96.88 ( 96.47)
Epoch: [22][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.3594e-01 (7.3825e-01)	Acc@1  76.56 ( 77.02)	Acc@5  93.75 ( 96.35)
Epoch: [22][120/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.4414e-01 (7.4496e-01)	Acc@1  78.12 ( 76.92)	Acc@5  96.09 ( 96.29)
Epoch: [22][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.2227e-01 (7.4457e-01)	Acc@1  75.78 ( 76.94)	Acc@5  97.66 ( 96.28)
Epoch: [22][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.6826e-01 (7.4556e-01)	Acc@1  71.09 ( 76.89)	Acc@5  96.88 ( 96.37)
Epoch: [22][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.1240e-01 (7.4771e-01)	Acc@1  81.25 ( 76.85)	Acc@5  93.75 ( 96.33)
Epoch: [22][160/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.0615e-01 (7.4997e-01)	Acc@1  77.34 ( 76.82)	Acc@5  97.66 ( 96.31)
Epoch: [22][170/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.4229e-01 (7.5310e-01)	Acc@1  71.09 ( 76.75)	Acc@5  93.75 ( 96.26)
Epoch: [22][180/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.1445e-01 (7.5239e-01)	Acc@1  78.12 ( 76.83)	Acc@5  97.66 ( 96.26)
Epoch: [22][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.7041e-01 (7.5488e-01)	Acc@1  78.12 ( 76.79)	Acc@5  99.22 ( 96.29)
Epoch: [22][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.1162e-01 (7.5944e-01)	Acc@1  72.66 ( 76.69)	Acc@5  93.75 ( 96.25)
Epoch: [22][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.1436e-01 (7.6124e-01)	Acc@1  78.12 ( 76.70)	Acc@5  96.88 ( 96.22)
Epoch: [22][220/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.1748e-01 (7.6380e-01)	Acc@1  68.75 ( 76.58)	Acc@5  96.09 ( 96.17)
Epoch: [22][230/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5439e-01 (7.6311e-01)	Acc@1  76.56 ( 76.63)	Acc@5  95.31 ( 96.16)
Epoch: [22][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.6191e-01 (7.6554e-01)	Acc@1  71.09 ( 76.51)	Acc@5  93.75 ( 96.15)
Epoch: [22][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4619e-01 (7.6688e-01)	Acc@1  72.66 ( 76.46)	Acc@5  94.53 ( 96.12)
Epoch: [22][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.5146e-01 (7.6497e-01)	Acc@1  77.34 ( 76.51)	Acc@5  96.09 ( 96.14)
Epoch: [22][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.2207e-01 (7.6621e-01)	Acc@1  81.25 ( 76.49)	Acc@5  97.66 ( 96.11)
Epoch: [22][280/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.8281e-01 (7.6959e-01)	Acc@1  72.66 ( 76.38)	Acc@5  95.31 ( 96.09)
Epoch: [22][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0303e+00 (7.6981e-01)	Acc@1  68.75 ( 76.38)	Acc@5  91.41 ( 96.08)
Epoch: [22][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.5850e-01 (7.7159e-01)	Acc@1  68.75 ( 76.36)	Acc@5  92.19 ( 96.03)
Epoch: [22][310/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2217e-01 (7.7239e-01)	Acc@1  77.34 ( 76.33)	Acc@5  96.09 ( 96.02)
Epoch: [22][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4814e-01 (7.7512e-01)	Acc@1  75.00 ( 76.26)	Acc@5  97.66 ( 96.00)
Epoch: [22][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.4463e-01 (7.7606e-01)	Acc@1  78.91 ( 76.26)	Acc@5  96.88 ( 95.97)
Epoch: [22][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7051e-01 (7.7520e-01)	Acc@1  78.12 ( 76.27)	Acc@5  96.88 ( 95.99)
Epoch: [22][350/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.0371e-01 (7.7566e-01)	Acc@1  78.91 ( 76.26)	Acc@5  94.53 ( 96.01)
Epoch: [22][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.6221e-01 (7.7585e-01)	Acc@1  75.78 ( 76.24)	Acc@5  96.09 ( 96.00)
Epoch: [22][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1494e+00 (7.7577e-01)	Acc@1  63.28 ( 76.23)	Acc@5  94.53 ( 96.02)
Epoch: [22][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7490e-01 (7.7718e-01)	Acc@1  75.78 ( 76.20)	Acc@5  96.09 ( 96.01)
Epoch: [22][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0166e+00 (7.7968e-01)	Acc@1  67.50 ( 76.11)	Acc@5  90.00 ( 95.98)
## e[22] optimizer.zero_grad (sum) time: 0.2930567264556885
## e[22]       loss.backward (sum) time: 13.784872055053711
## e[22]      optimizer.step (sum) time: 2.7419700622558594
## epoch[22] training(only) time: 45.06068420410156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.7314e+00 (1.7314e+00)	Acc@1  64.00 ( 64.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.038 ( 0.054)	Loss 1.3174e+00 (1.3943e+00)	Acc@1  67.00 ( 65.45)	Acc@5  89.00 ( 88.73)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.2383e+00 (1.3165e+00)	Acc@1  70.00 ( 65.90)	Acc@5  89.00 ( 89.24)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 1.3516e+00 (1.3338e+00)	Acc@1  64.00 ( 65.61)	Acc@5  89.00 ( 88.87)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.3857e+00 (1.3302e+00)	Acc@1  64.00 ( 65.56)	Acc@5  92.00 ( 89.20)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.3652e+00 (1.3372e+00)	Acc@1  62.00 ( 65.24)	Acc@5  90.00 ( 89.12)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.3096e+00 (1.3151e+00)	Acc@1  64.00 ( 65.62)	Acc@5  86.00 ( 89.52)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.3604e+00 (1.3215e+00)	Acc@1  65.00 ( 65.39)	Acc@5  87.00 ( 89.56)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.5420e+00 (1.3252e+00)	Acc@1  65.00 ( 65.44)	Acc@5  88.00 ( 89.37)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 1.7178e+00 (1.3128e+00)	Acc@1  59.00 ( 65.65)	Acc@5  86.00 ( 89.52)
 * Acc@1 65.440 Acc@5 89.540
### epoch[22] execution time: 49.16165828704834
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.273 ( 0.273)	Data  0.164 ( 0.164)	Loss 6.6357e-01 (6.6357e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.88 ( 96.88)
Epoch: [23][ 10/391]	Time  0.114 ( 0.129)	Data  0.001 ( 0.018)	Loss 8.1348e-01 (6.9114e-01)	Acc@1  76.56 ( 78.12)	Acc@5  96.88 ( 96.66)
Epoch: [23][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 5.8154e-01 (6.7418e-01)	Acc@1  79.69 ( 78.65)	Acc@5  98.44 ( 97.21)
Epoch: [23][ 30/391]	Time  0.110 ( 0.119)	Data  0.001 ( 0.009)	Loss 5.7520e-01 (6.6089e-01)	Acc@1  84.38 ( 79.26)	Acc@5  98.44 ( 97.10)
Epoch: [23][ 40/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.008)	Loss 7.9883e-01 (6.6599e-01)	Acc@1  75.00 ( 79.17)	Acc@5  95.31 ( 97.03)
Epoch: [23][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 5.9375e-01 (6.6671e-01)	Acc@1  82.81 ( 79.21)	Acc@5  96.88 ( 96.91)
Epoch: [23][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.2217e-01 (6.6577e-01)	Acc@1  75.00 ( 79.19)	Acc@5  99.22 ( 96.96)
Epoch: [23][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.0459e-01 (6.6441e-01)	Acc@1  78.12 ( 79.23)	Acc@5  96.09 ( 96.99)
Epoch: [23][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.5869e-01 (6.7483e-01)	Acc@1  77.34 ( 78.89)	Acc@5  96.88 ( 96.90)
Epoch: [23][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.5830e-01 (6.8498e-01)	Acc@1  75.00 ( 78.50)	Acc@5  94.53 ( 96.71)
Epoch: [23][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.3643e-01 (6.8854e-01)	Acc@1  74.22 ( 78.36)	Acc@5  94.53 ( 96.76)
Epoch: [23][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.8604e-01 (6.9095e-01)	Acc@1  82.81 ( 78.39)	Acc@5  96.09 ( 96.71)
Epoch: [23][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.7578e-01 (6.9495e-01)	Acc@1  82.03 ( 78.38)	Acc@5  94.53 ( 96.63)
Epoch: [23][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.2861e-01 (6.9869e-01)	Acc@1  74.22 ( 78.20)	Acc@5  97.66 ( 96.62)
Epoch: [23][140/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.5195e-01 (6.9929e-01)	Acc@1  75.00 ( 78.21)	Acc@5  94.53 ( 96.63)
Epoch: [23][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.1289e-01 (7.0140e-01)	Acc@1  75.78 ( 78.14)	Acc@5  95.31 ( 96.66)
Epoch: [23][160/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.9053e-01 (7.0528e-01)	Acc@1  76.56 ( 78.05)	Acc@5  95.31 ( 96.65)
Epoch: [23][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.5430e-01 (7.0709e-01)	Acc@1  81.25 ( 77.99)	Acc@5  98.44 ( 96.61)
Epoch: [23][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.2275e-01 (7.1005e-01)	Acc@1  77.34 ( 77.92)	Acc@5  96.09 ( 96.62)
Epoch: [23][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.6025e-01 (7.1140e-01)	Acc@1  78.12 ( 77.90)	Acc@5  94.53 ( 96.60)
Epoch: [23][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.4004e-01 (7.1267e-01)	Acc@1  82.81 ( 77.92)	Acc@5  98.44 ( 96.58)
Epoch: [23][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.2227e-01 (7.1714e-01)	Acc@1  75.00 ( 77.77)	Acc@5  96.88 ( 96.56)
Epoch: [23][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.9395e-01 (7.1741e-01)	Acc@1  76.56 ( 77.82)	Acc@5  98.44 ( 96.55)
Epoch: [23][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.9697e-01 (7.1830e-01)	Acc@1  71.09 ( 77.81)	Acc@5  93.75 ( 96.55)
Epoch: [23][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.0566e-01 (7.1761e-01)	Acc@1  75.00 ( 77.87)	Acc@5  96.09 ( 96.56)
Epoch: [23][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.7529e-01 (7.1600e-01)	Acc@1  80.47 ( 77.94)	Acc@5  96.88 ( 96.56)
Epoch: [23][260/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.1514e-01 (7.1689e-01)	Acc@1  85.94 ( 77.92)	Acc@5  98.44 ( 96.56)
Epoch: [23][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7539e-01 (7.1821e-01)	Acc@1  77.34 ( 77.87)	Acc@5  94.53 ( 96.53)
Epoch: [23][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.8418e-01 (7.2043e-01)	Acc@1  75.78 ( 77.81)	Acc@5  93.75 ( 96.51)
Epoch: [23][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.7275e-01 (7.1997e-01)	Acc@1  82.81 ( 77.85)	Acc@5  96.88 ( 96.51)
Epoch: [23][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.1699e-01 (7.2276e-01)	Acc@1  75.00 ( 77.77)	Acc@5  95.31 ( 96.49)
Epoch: [23][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0068e-01 (7.2423e-01)	Acc@1  78.12 ( 77.72)	Acc@5  97.66 ( 96.47)
Epoch: [23][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.8770e-01 (7.2607e-01)	Acc@1  71.88 ( 77.64)	Acc@5  93.75 ( 96.45)
Epoch: [23][330/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.1572e-01 (7.2716e-01)	Acc@1  79.69 ( 77.61)	Acc@5  96.09 ( 96.45)
Epoch: [23][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.8730e-01 (7.2943e-01)	Acc@1  71.09 ( 77.56)	Acc@5  95.31 ( 96.42)
Epoch: [23][350/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.6084e-01 (7.3216e-01)	Acc@1  72.66 ( 77.49)	Acc@5  95.31 ( 96.38)
Epoch: [23][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0215e-01 (7.3272e-01)	Acc@1  78.91 ( 77.47)	Acc@5  95.31 ( 96.37)
Epoch: [23][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2461e-01 (7.3410e-01)	Acc@1  78.91 ( 77.42)	Acc@5  96.09 ( 96.35)
Epoch: [23][380/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.2275e-01 (7.3519e-01)	Acc@1  74.22 ( 77.39)	Acc@5  96.88 ( 96.34)
Epoch: [23][390/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6396e-01 (7.3781e-01)	Acc@1  81.25 ( 77.31)	Acc@5  98.75 ( 96.32)
## e[23] optimizer.zero_grad (sum) time: 0.2921276092529297
## e[23]       loss.backward (sum) time: 13.80435824394226
## e[23]      optimizer.step (sum) time: 2.739182472229004
## epoch[23] training(only) time: 45.078694105148315
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.3467e+00 (1.3467e+00)	Acc@1  64.00 ( 64.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.040 ( 0.054)	Loss 1.6963e+00 (1.3997e+00)	Acc@1  58.00 ( 64.36)	Acc@5  91.00 ( 88.64)
Test: [ 20/100]	Time  0.040 ( 0.047)	Loss 1.3682e+00 (1.3419e+00)	Acc@1  69.00 ( 65.86)	Acc@5  88.00 ( 89.43)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 1.6660e+00 (1.3632e+00)	Acc@1  54.00 ( 64.87)	Acc@5  91.00 ( 88.90)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.1221e+00 (1.3390e+00)	Acc@1  71.00 ( 65.56)	Acc@5  92.00 ( 89.29)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.2354e+00 (1.3514e+00)	Acc@1  66.00 ( 65.25)	Acc@5  88.00 ( 89.02)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.2637e+00 (1.3244e+00)	Acc@1  65.00 ( 65.41)	Acc@5  92.00 ( 89.41)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.5293e+00 (1.3230e+00)	Acc@1  60.00 ( 65.24)	Acc@5  87.00 ( 89.59)
Test: [ 80/100]	Time  0.037 ( 0.041)	Loss 1.3779e+00 (1.3231e+00)	Acc@1  65.00 ( 65.14)	Acc@5  90.00 ( 89.70)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 1.5830e+00 (1.3187e+00)	Acc@1  62.00 ( 65.23)	Acc@5  88.00 ( 89.76)
 * Acc@1 65.240 Acc@5 89.770
### epoch[23] execution time: 49.19955062866211
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.271 ( 0.271)	Data  0.163 ( 0.163)	Loss 8.3838e-01 (8.3838e-01)	Acc@1  75.78 ( 75.78)	Acc@5  91.41 ( 91.41)
Epoch: [24][ 10/391]	Time  0.113 ( 0.129)	Data  0.001 ( 0.018)	Loss 6.6846e-01 (6.8643e-01)	Acc@1  81.25 ( 79.83)	Acc@5  96.09 ( 96.31)
Epoch: [24][ 20/391]	Time  0.111 ( 0.122)	Data  0.001 ( 0.011)	Loss 7.5000e-01 (6.4974e-01)	Acc@1  71.88 ( 79.43)	Acc@5  96.09 ( 97.25)
Epoch: [24][ 30/391]	Time  0.112 ( 0.120)	Data  0.001 ( 0.009)	Loss 5.7129e-01 (6.4357e-01)	Acc@1  83.59 ( 79.44)	Acc@5  98.44 ( 97.15)
Epoch: [24][ 40/391]	Time  0.116 ( 0.119)	Data  0.001 ( 0.008)	Loss 5.5176e-01 (6.4273e-01)	Acc@1  81.25 ( 79.52)	Acc@5  96.88 ( 97.18)
Epoch: [24][ 50/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.007)	Loss 7.6709e-01 (6.4725e-01)	Acc@1  77.34 ( 79.32)	Acc@5  93.75 ( 97.18)
Epoch: [24][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.4307e-01 (6.4995e-01)	Acc@1  74.22 ( 79.23)	Acc@5  99.22 ( 97.17)
Epoch: [24][ 70/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.3486e-01 (6.5232e-01)	Acc@1  76.56 ( 79.30)	Acc@5  97.66 ( 97.14)
Epoch: [24][ 80/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.7334e-01 (6.5637e-01)	Acc@1  75.00 ( 79.16)	Acc@5  99.22 ( 97.15)
Epoch: [24][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.5576e-01 (6.6180e-01)	Acc@1  78.12 ( 79.00)	Acc@5  96.88 ( 97.06)
Epoch: [24][100/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.2803e-01 (6.6220e-01)	Acc@1  76.56 ( 78.90)	Acc@5  97.66 ( 97.09)
Epoch: [24][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.3232e-01 (6.5771e-01)	Acc@1  81.25 ( 79.08)	Acc@5  96.09 ( 97.09)
Epoch: [24][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.7832e-01 (6.6030e-01)	Acc@1  74.22 ( 78.99)	Acc@5  96.88 ( 97.08)
Epoch: [24][130/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.3486e-01 (6.6098e-01)	Acc@1  73.44 ( 78.96)	Acc@5  95.31 ( 97.05)
Epoch: [24][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.7012e-01 (6.6623e-01)	Acc@1  75.00 ( 78.91)	Acc@5  97.66 ( 97.02)
Epoch: [24][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.5293e-01 (6.6962e-01)	Acc@1  78.91 ( 78.85)	Acc@5  95.31 ( 96.98)
Epoch: [24][160/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.2754e-01 (6.7427e-01)	Acc@1  76.56 ( 78.70)	Acc@5  96.09 ( 96.96)
Epoch: [24][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.8555e-01 (6.7648e-01)	Acc@1  76.56 ( 78.56)	Acc@5  99.22 ( 96.94)
Epoch: [24][180/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.0146e-01 (6.7648e-01)	Acc@1  89.06 ( 78.57)	Acc@5  97.66 ( 96.97)
Epoch: [24][190/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.2070e-01 (6.7968e-01)	Acc@1  76.56 ( 78.46)	Acc@5  96.88 ( 96.95)
Epoch: [24][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.2393e-01 (6.8324e-01)	Acc@1  83.59 ( 78.40)	Acc@5  99.22 ( 96.91)
Epoch: [24][210/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.4570e-01 (6.8528e-01)	Acc@1  74.22 ( 78.37)	Acc@5  94.53 ( 96.90)
Epoch: [24][220/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.2793e-01 (6.8664e-01)	Acc@1  84.38 ( 78.38)	Acc@5  95.31 ( 96.86)
Epoch: [24][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.3242e-01 (6.8712e-01)	Acc@1  82.03 ( 78.37)	Acc@5  96.88 ( 96.87)
Epoch: [24][240/391]	Time  0.118 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.6270e-01 (6.8820e-01)	Acc@1  75.78 ( 78.34)	Acc@5  96.09 ( 96.86)
Epoch: [24][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.1621e-01 (6.8906e-01)	Acc@1  82.03 ( 78.33)	Acc@5  97.66 ( 96.87)
Epoch: [24][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.1191e-01 (6.9153e-01)	Acc@1  78.12 ( 78.28)	Acc@5  96.88 ( 96.85)
Epoch: [24][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.5830e-01 (6.9216e-01)	Acc@1  80.47 ( 78.28)	Acc@5  94.53 ( 96.85)
Epoch: [24][280/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4238e-01 (6.9339e-01)	Acc@1  70.31 ( 78.24)	Acc@5  92.97 ( 96.81)
Epoch: [24][290/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9443e-01 (6.9559e-01)	Acc@1  76.56 ( 78.20)	Acc@5  96.09 ( 96.80)
Epoch: [24][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.6240e-01 (6.9665e-01)	Acc@1  71.09 ( 78.17)	Acc@5  92.19 ( 96.77)
Epoch: [24][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6396e-01 (6.9769e-01)	Acc@1  81.25 ( 78.14)	Acc@5  98.44 ( 96.76)
Epoch: [24][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0996e-01 (6.9978e-01)	Acc@1  78.91 ( 78.09)	Acc@5  97.66 ( 96.74)
Epoch: [24][330/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.6855e-01 (7.0155e-01)	Acc@1  76.56 ( 78.00)	Acc@5  96.88 ( 96.74)
Epoch: [24][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.5195e-01 (7.0220e-01)	Acc@1  71.88 ( 77.94)	Acc@5  97.66 ( 96.74)
Epoch: [24][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.2812e-01 (7.0248e-01)	Acc@1  75.00 ( 77.92)	Acc@5  96.09 ( 96.74)
Epoch: [24][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6406e-01 (7.0238e-01)	Acc@1  81.25 ( 77.94)	Acc@5  97.66 ( 96.74)
Epoch: [24][370/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2168e-01 (7.0112e-01)	Acc@1  78.91 ( 77.99)	Acc@5  95.31 ( 96.73)
Epoch: [24][380/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.9014e-01 (7.0331e-01)	Acc@1  72.66 ( 77.93)	Acc@5  94.53 ( 96.72)
Epoch: [24][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.7119e-01 (7.0364e-01)	Acc@1  70.00 ( 77.94)	Acc@5  95.00 ( 96.71)
## e[24] optimizer.zero_grad (sum) time: 0.2929565906524658
## e[24]       loss.backward (sum) time: 13.758525609970093
## e[24]      optimizer.step (sum) time: 2.7295753955841064
## epoch[24] training(only) time: 45.09631419181824
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.3604e+00 (1.3604e+00)	Acc@1  64.00 ( 64.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.5176e+00 (1.4321e+00)	Acc@1  59.00 ( 64.18)	Acc@5  90.00 ( 88.73)
Test: [ 20/100]	Time  0.037 ( 0.047)	Loss 1.3730e+00 (1.3773e+00)	Acc@1  66.00 ( 64.90)	Acc@5  86.00 ( 89.24)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 1.6445e+00 (1.3889e+00)	Acc@1  58.00 ( 63.94)	Acc@5  88.00 ( 88.97)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.0674e+00 (1.3723e+00)	Acc@1  69.00 ( 64.17)	Acc@5  92.00 ( 89.32)
Test: [ 50/100]	Time  0.042 ( 0.042)	Loss 1.2217e+00 (1.3820e+00)	Acc@1  65.00 ( 64.06)	Acc@5  87.00 ( 89.06)
Test: [ 60/100]	Time  0.045 ( 0.041)	Loss 1.5156e+00 (1.3570e+00)	Acc@1  57.00 ( 64.33)	Acc@5  84.00 ( 89.34)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.4248e+00 (1.3607e+00)	Acc@1  61.00 ( 64.11)	Acc@5  86.00 ( 89.35)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.6270e+00 (1.3720e+00)	Acc@1  61.00 ( 63.90)	Acc@5  86.00 ( 89.20)
Test: [ 90/100]	Time  0.037 ( 0.040)	Loss 1.6318e+00 (1.3628e+00)	Acc@1  61.00 ( 64.19)	Acc@5  87.00 ( 89.26)
 * Acc@1 64.290 Acc@5 89.330
### epoch[24] execution time: 49.18517255783081
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.279 ( 0.279)	Data  0.165 ( 0.165)	Loss 6.8945e-01 (6.8945e-01)	Acc@1  78.91 ( 78.91)	Acc@5  97.66 ( 97.66)
Epoch: [25][ 10/391]	Time  0.112 ( 0.129)	Data  0.001 ( 0.018)	Loss 5.6738e-01 (6.0281e-01)	Acc@1  83.59 ( 82.46)	Acc@5  97.66 ( 97.37)
Epoch: [25][ 20/391]	Time  0.112 ( 0.122)	Data  0.001 ( 0.011)	Loss 6.9678e-01 (5.9299e-01)	Acc@1  71.88 ( 81.55)	Acc@5  97.66 ( 97.84)
Epoch: [25][ 30/391]	Time  0.112 ( 0.120)	Data  0.001 ( 0.009)	Loss 5.2686e-01 (6.0362e-01)	Acc@1  85.16 ( 81.15)	Acc@5  97.66 ( 97.53)
Epoch: [25][ 40/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.008)	Loss 4.6826e-01 (5.9190e-01)	Acc@1  83.59 ( 81.52)	Acc@5  98.44 ( 97.69)
Epoch: [25][ 50/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.007)	Loss 4.9292e-01 (5.9456e-01)	Acc@1  83.59 ( 81.37)	Acc@5 100.00 ( 97.82)
Epoch: [25][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.4990e-01 (5.9080e-01)	Acc@1  79.69 ( 81.37)	Acc@5  96.09 ( 97.87)
Epoch: [25][ 70/391]	Time  0.117 ( 0.117)	Data  0.001 ( 0.006)	Loss 5.8154e-01 (5.9567e-01)	Acc@1  81.25 ( 81.13)	Acc@5  99.22 ( 97.83)
Epoch: [25][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.9668e-01 (6.0119e-01)	Acc@1  78.91 ( 81.02)	Acc@5  99.22 ( 97.80)
Epoch: [25][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.3037e-01 (6.0560e-01)	Acc@1  82.03 ( 80.92)	Acc@5  96.09 ( 97.79)
Epoch: [25][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.5879e-01 (6.0966e-01)	Acc@1  74.22 ( 80.60)	Acc@5  96.09 ( 97.73)
Epoch: [25][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.6006e-01 (6.1580e-01)	Acc@1  81.25 ( 80.39)	Acc@5  96.88 ( 97.64)
Epoch: [25][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.4561e-01 (6.1970e-01)	Acc@1  75.00 ( 80.35)	Acc@5  95.31 ( 97.59)
Epoch: [25][130/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.5811e-01 (6.2354e-01)	Acc@1  75.00 ( 80.23)	Acc@5  96.88 ( 97.54)
Epoch: [25][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.4150e-01 (6.2492e-01)	Acc@1  84.38 ( 80.20)	Acc@5  97.66 ( 97.49)
Epoch: [25][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.3711e-01 (6.2739e-01)	Acc@1  82.03 ( 80.15)	Acc@5  98.44 ( 97.46)
Epoch: [25][160/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.9561e-01 (6.2767e-01)	Acc@1  82.81 ( 80.17)	Acc@5  97.66 ( 97.44)
Epoch: [25][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.9536e-01 (6.3143e-01)	Acc@1  84.38 ( 80.08)	Acc@5  98.44 ( 97.41)
Epoch: [25][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.2607e-01 (6.3371e-01)	Acc@1  71.09 ( 80.04)	Acc@5  98.44 ( 97.40)
Epoch: [25][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5684e-01 (6.3638e-01)	Acc@1  79.69 ( 79.99)	Acc@5  95.31 ( 97.36)
Epoch: [25][200/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.1875e-01 (6.3813e-01)	Acc@1  77.34 ( 79.92)	Acc@5  95.31 ( 97.36)
Epoch: [25][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.2021e-01 (6.3979e-01)	Acc@1  78.91 ( 79.89)	Acc@5  96.88 ( 97.36)
Epoch: [25][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.9482e-01 (6.4423e-01)	Acc@1  82.81 ( 79.85)	Acc@5  94.53 ( 97.32)
Epoch: [25][230/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.9346e-01 (6.4815e-01)	Acc@1  77.34 ( 79.76)	Acc@5  96.09 ( 97.26)
Epoch: [25][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.4209e-01 (6.5232e-01)	Acc@1  77.34 ( 79.58)	Acc@5  96.88 ( 97.23)
Epoch: [25][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.4512e-01 (6.5412e-01)	Acc@1  75.78 ( 79.48)	Acc@5  96.09 ( 97.22)
Epoch: [25][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5576e-01 (6.5749e-01)	Acc@1  78.12 ( 79.41)	Acc@5  98.44 ( 97.18)
Epoch: [25][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.1230e-01 (6.6002e-01)	Acc@1  85.16 ( 79.37)	Acc@5  97.66 ( 97.15)
Epoch: [25][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.4268e-01 (6.6194e-01)	Acc@1  77.34 ( 79.28)	Acc@5  95.31 ( 97.14)
Epoch: [25][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9541e-01 (6.6650e-01)	Acc@1  75.00 ( 79.12)	Acc@5  96.09 ( 97.12)
Epoch: [25][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6553e-01 (6.6864e-01)	Acc@1  77.34 ( 79.04)	Acc@5  98.44 ( 97.12)
Epoch: [25][310/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.5352e-01 (6.7028e-01)	Acc@1  72.66 ( 78.99)	Acc@5  92.97 ( 97.09)
Epoch: [25][320/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.2744e-01 (6.7104e-01)	Acc@1  82.81 ( 79.00)	Acc@5  97.66 ( 97.07)
Epoch: [25][330/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.0039e-01 (6.6919e-01)	Acc@1  89.84 ( 79.04)	Acc@5  99.22 ( 97.10)
Epoch: [25][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.7480e-01 (6.6879e-01)	Acc@1  73.44 ( 79.03)	Acc@5  97.66 ( 97.12)
Epoch: [25][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.9746e-01 (6.7036e-01)	Acc@1  73.44 ( 78.97)	Acc@5  93.75 ( 97.09)
Epoch: [25][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.4785e-01 (6.7232e-01)	Acc@1  82.03 ( 78.90)	Acc@5  99.22 ( 97.08)
Epoch: [25][370/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.1045e-01 (6.7271e-01)	Acc@1  78.91 ( 78.91)	Acc@5  96.88 ( 97.08)
Epoch: [25][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0449e-01 (6.7285e-01)	Acc@1  80.47 ( 78.91)	Acc@5  97.66 ( 97.08)
Epoch: [25][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.8076e-01 (6.7294e-01)	Acc@1  77.50 ( 78.90)	Acc@5  96.25 ( 97.08)
## e[25] optimizer.zero_grad (sum) time: 0.29331231117248535
## e[25]       loss.backward (sum) time: 13.802321672439575
## e[25]      optimizer.step (sum) time: 2.7154057025909424
## epoch[25] training(only) time: 45.09371614456177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.3994e+00 (1.3994e+00)	Acc@1  67.00 ( 67.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.3750e+00 (1.3935e+00)	Acc@1  62.00 ( 65.27)	Acc@5  91.00 ( 88.55)
Test: [ 20/100]	Time  0.040 ( 0.046)	Loss 1.4404e+00 (1.3716e+00)	Acc@1  68.00 ( 65.48)	Acc@5  90.00 ( 88.81)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 1.5381e+00 (1.3932e+00)	Acc@1  60.00 ( 64.48)	Acc@5  88.00 ( 88.55)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.4209e+00 (1.3824e+00)	Acc@1  65.00 ( 64.51)	Acc@5  90.00 ( 88.88)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.4326e+00 (1.4050e+00)	Acc@1  64.00 ( 64.25)	Acc@5  91.00 ( 88.69)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 1.3848e+00 (1.3870e+00)	Acc@1  67.00 ( 64.28)	Acc@5  91.00 ( 89.16)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.6035e+00 (1.3801e+00)	Acc@1  60.00 ( 64.34)	Acc@5  87.00 ( 89.23)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.4746e+00 (1.3807e+00)	Acc@1  61.00 ( 64.27)	Acc@5  90.00 ( 89.30)
Test: [ 90/100]	Time  0.038 ( 0.040)	Loss 2.0098e+00 (1.3766e+00)	Acc@1  55.00 ( 64.51)	Acc@5  79.00 ( 89.37)
 * Acc@1 64.600 Acc@5 89.420
### epoch[25] execution time: 49.22222638130188
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.260 ( 0.260)	Data  0.155 ( 0.155)	Loss 5.2490e-01 (5.2490e-01)	Acc@1  82.81 ( 82.81)	Acc@5  97.66 ( 97.66)
Epoch: [26][ 10/391]	Time  0.113 ( 0.127)	Data  0.001 ( 0.017)	Loss 6.0596e-01 (5.5975e-01)	Acc@1  78.91 ( 82.60)	Acc@5  99.22 ( 98.22)
Epoch: [26][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.011)	Loss 7.3486e-01 (5.8434e-01)	Acc@1  79.69 ( 82.18)	Acc@5  96.09 ( 97.73)
Epoch: [26][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 4.3945e-01 (5.6953e-01)	Acc@1  86.72 ( 82.36)	Acc@5  98.44 ( 98.01)
Epoch: [26][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 7.6709e-01 (5.8272e-01)	Acc@1  80.47 ( 82.20)	Acc@5  98.44 ( 97.79)
Epoch: [26][ 50/391]	Time  0.117 ( 0.117)	Data  0.001 ( 0.007)	Loss 5.2246e-01 (5.8251e-01)	Acc@1  81.25 ( 82.25)	Acc@5  98.44 ( 97.72)
Epoch: [26][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.7041e-01 (5.9101e-01)	Acc@1  74.22 ( 81.75)	Acc@5  98.44 ( 97.67)
Epoch: [26][ 70/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.4883e-01 (5.9440e-01)	Acc@1  82.81 ( 81.53)	Acc@5  97.66 ( 97.65)
Epoch: [26][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.0547e-01 (5.9517e-01)	Acc@1  81.25 ( 81.46)	Acc@5  98.44 ( 97.73)
Epoch: [26][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.4258e-01 (5.9826e-01)	Acc@1  80.47 ( 81.45)	Acc@5  95.31 ( 97.73)
Epoch: [26][100/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.9834e-01 (5.9877e-01)	Acc@1  74.22 ( 81.51)	Acc@5  96.88 ( 97.74)
Epoch: [26][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.0498e-01 (6.0338e-01)	Acc@1  78.91 ( 81.25)	Acc@5  97.66 ( 97.74)
Epoch: [26][120/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.8262e-01 (6.0481e-01)	Acc@1  81.25 ( 81.28)	Acc@5  95.31 ( 97.73)
Epoch: [26][130/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.2842e-01 (6.0533e-01)	Acc@1  78.12 ( 81.29)	Acc@5  98.44 ( 97.76)
Epoch: [26][140/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.0410e-01 (6.1041e-01)	Acc@1  78.12 ( 81.12)	Acc@5  97.66 ( 97.71)
Epoch: [26][150/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.4375e-01 (6.1529e-01)	Acc@1  73.44 ( 80.97)	Acc@5  93.75 ( 97.67)
Epoch: [26][160/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.8223e-01 (6.1593e-01)	Acc@1  71.09 ( 80.89)	Acc@5  96.09 ( 97.67)
Epoch: [26][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.6367e-01 (6.2002e-01)	Acc@1  74.22 ( 80.71)	Acc@5  96.88 ( 97.61)
Epoch: [26][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5732e-01 (6.1981e-01)	Acc@1  78.91 ( 80.64)	Acc@5  94.53 ( 97.62)
Epoch: [26][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.7275e-01 (6.1778e-01)	Acc@1  82.03 ( 80.68)	Acc@5  97.66 ( 97.61)
Epoch: [26][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.9570e-01 (6.1914e-01)	Acc@1  80.47 ( 80.66)	Acc@5  97.66 ( 97.57)
Epoch: [26][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.8359e-01 (6.2179e-01)	Acc@1  78.91 ( 80.57)	Acc@5  94.53 ( 97.53)
Epoch: [26][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.8301e-01 (6.2337e-01)	Acc@1  81.25 ( 80.51)	Acc@5  96.88 ( 97.50)
Epoch: [26][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.1143e-01 (6.2502e-01)	Acc@1  79.69 ( 80.48)	Acc@5  97.66 ( 97.49)
Epoch: [26][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.4795e-01 (6.2626e-01)	Acc@1  81.25 ( 80.43)	Acc@5  94.53 ( 97.46)
Epoch: [26][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.6855e-01 (6.2977e-01)	Acc@1  76.56 ( 80.34)	Acc@5  93.75 ( 97.44)
Epoch: [26][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.4312e-01 (6.2825e-01)	Acc@1  88.28 ( 80.36)	Acc@5 100.00 ( 97.46)
Epoch: [26][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0107e-01 (6.2803e-01)	Acc@1  82.81 ( 80.35)	Acc@5  96.88 ( 97.45)
Epoch: [26][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0020e-01 (6.3007e-01)	Acc@1  80.47 ( 80.31)	Acc@5  96.88 ( 97.44)
Epoch: [26][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9561e-01 (6.3122e-01)	Acc@1  81.25 ( 80.27)	Acc@5 100.00 ( 97.44)
Epoch: [26][300/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.0566e-01 (6.3303e-01)	Acc@1  74.22 ( 80.18)	Acc@5  96.09 ( 97.43)
Epoch: [26][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.0186e-01 (6.3244e-01)	Acc@1  89.84 ( 80.19)	Acc@5  99.22 ( 97.45)
Epoch: [26][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.7471e-01 (6.3219e-01)	Acc@1  82.03 ( 80.19)	Acc@5  96.88 ( 97.45)
Epoch: [26][330/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.3691e-01 (6.3426e-01)	Acc@1  71.09 ( 80.11)	Acc@5  96.09 ( 97.43)
Epoch: [26][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7881e-01 (6.3580e-01)	Acc@1  75.00 ( 80.07)	Acc@5  95.31 ( 97.40)
Epoch: [26][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.7627e-01 (6.3548e-01)	Acc@1  79.69 ( 80.11)	Acc@5  97.66 ( 97.38)
Epoch: [26][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.9209e-01 (6.3947e-01)	Acc@1  71.88 ( 79.98)	Acc@5  94.53 ( 97.36)
Epoch: [26][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.5645e-01 (6.4379e-01)	Acc@1  73.44 ( 79.83)	Acc@5  95.31 ( 97.33)
Epoch: [26][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.1211e-01 (6.4582e-01)	Acc@1  67.19 ( 79.79)	Acc@5  94.53 ( 97.30)
Epoch: [26][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.7451e-01 (6.4934e-01)	Acc@1  76.25 ( 79.69)	Acc@5  93.75 ( 97.27)
## e[26] optimizer.zero_grad (sum) time: 0.2908957004547119
## e[26]       loss.backward (sum) time: 13.791453838348389
## e[26]      optimizer.step (sum) time: 2.7325801849365234
## epoch[26] training(only) time: 45.070876359939575
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 1.4092e+00 (1.4092e+00)	Acc@1  66.00 ( 66.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 1.6201e+00 (1.3817e+00)	Acc@1  56.00 ( 64.82)	Acc@5  89.00 ( 88.27)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.2031e+00 (1.3244e+00)	Acc@1  72.00 ( 65.86)	Acc@5  92.00 ( 89.57)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 1.6094e+00 (1.3422e+00)	Acc@1  59.00 ( 65.29)	Acc@5  94.00 ( 89.71)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 1.0664e+00 (1.3385e+00)	Acc@1  70.00 ( 65.56)	Acc@5  95.00 ( 90.05)
Test: [ 50/100]	Time  0.040 ( 0.042)	Loss 1.4746e+00 (1.3549e+00)	Acc@1  66.00 ( 65.29)	Acc@5  85.00 ( 89.53)
Test: [ 60/100]	Time  0.046 ( 0.041)	Loss 1.4141e+00 (1.3363e+00)	Acc@1  63.00 ( 65.11)	Acc@5  88.00 ( 89.75)
Test: [ 70/100]	Time  0.037 ( 0.041)	Loss 1.4922e+00 (1.3479e+00)	Acc@1  62.00 ( 64.89)	Acc@5  91.00 ( 89.80)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.5293e+00 (1.3644e+00)	Acc@1  61.00 ( 64.54)	Acc@5  87.00 ( 89.60)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 1.7246e+00 (1.3558e+00)	Acc@1  55.00 ( 64.73)	Acc@5  82.00 ( 89.60)
 * Acc@1 64.810 Acc@5 89.640
### epoch[26] execution time: 49.18601655960083
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.280 ( 0.280)	Data  0.162 ( 0.162)	Loss 6.5967e-01 (6.5967e-01)	Acc@1  78.12 ( 78.12)	Acc@5  96.09 ( 96.09)
Epoch: [27][ 10/391]	Time  0.114 ( 0.128)	Data  0.001 ( 0.018)	Loss 5.8643e-01 (5.8505e-01)	Acc@1  84.38 ( 81.53)	Acc@5  96.88 ( 97.80)
Epoch: [27][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 5.7227e-01 (5.6822e-01)	Acc@1  82.03 ( 81.70)	Acc@5 100.00 ( 97.92)
Epoch: [27][ 30/391]	Time  0.111 ( 0.119)	Data  0.001 ( 0.009)	Loss 4.8291e-01 (5.7926e-01)	Acc@1  85.16 ( 81.40)	Acc@5 100.00 ( 97.91)
Epoch: [27][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 5.0537e-01 (5.7200e-01)	Acc@1  85.16 ( 81.82)	Acc@5  99.22 ( 97.94)
Epoch: [27][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 5.6543e-01 (5.5982e-01)	Acc@1  82.03 ( 82.17)	Acc@5  98.44 ( 98.02)
Epoch: [27][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 4.2749e-01 (5.5283e-01)	Acc@1  86.72 ( 82.47)	Acc@5  97.66 ( 97.98)
Epoch: [27][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.6768e-01 (5.5801e-01)	Acc@1  72.66 ( 82.26)	Acc@5  95.31 ( 97.87)
Epoch: [27][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.1465e-01 (5.6054e-01)	Acc@1  85.94 ( 82.29)	Acc@5  98.44 ( 97.85)
Epoch: [27][ 90/391]	Time  0.109 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.3516e-01 (5.6545e-01)	Acc@1  81.25 ( 82.15)	Acc@5  97.66 ( 97.85)
Epoch: [27][100/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.3525e-01 (5.6855e-01)	Acc@1  84.38 ( 82.14)	Acc@5  97.66 ( 97.80)
Epoch: [27][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.3477e-01 (5.7379e-01)	Acc@1  79.69 ( 81.89)	Acc@5  98.44 ( 97.83)
Epoch: [27][120/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.8467e-01 (5.7533e-01)	Acc@1  75.78 ( 81.82)	Acc@5  96.09 ( 97.82)
Epoch: [27][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.1377e-01 (5.8059e-01)	Acc@1  78.12 ( 81.67)	Acc@5  96.88 ( 97.80)
Epoch: [27][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.4443e-01 (5.8183e-01)	Acc@1  80.47 ( 81.65)	Acc@5  97.66 ( 97.81)
Epoch: [27][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.4014e-01 (5.8655e-01)	Acc@1  79.69 ( 81.44)	Acc@5  99.22 ( 97.79)
Epoch: [27][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.9678e-01 (5.8741e-01)	Acc@1  77.34 ( 81.34)	Acc@5  98.44 ( 97.80)
Epoch: [27][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.3984e-01 (5.9153e-01)	Acc@1  71.88 ( 81.22)	Acc@5  95.31 ( 97.78)
Epoch: [27][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.6748e-01 (5.9311e-01)	Acc@1  78.91 ( 81.19)	Acc@5  96.09 ( 97.77)
Epoch: [27][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.1084e-01 (5.9711e-01)	Acc@1  80.47 ( 81.04)	Acc@5  96.09 ( 97.75)
Epoch: [27][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.2705e-01 (5.9994e-01)	Acc@1  77.34 ( 80.94)	Acc@5  96.09 ( 97.71)
Epoch: [27][210/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.3184e-01 (5.9962e-01)	Acc@1  80.47 ( 80.93)	Acc@5 100.00 ( 97.73)
Epoch: [27][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.4004e-01 (5.9966e-01)	Acc@1  85.94 ( 80.91)	Acc@5  97.66 ( 97.73)
Epoch: [27][230/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.0635e-01 (5.9871e-01)	Acc@1  82.81 ( 80.94)	Acc@5  96.09 ( 97.71)
Epoch: [27][240/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5000e-01 (5.9899e-01)	Acc@1  72.66 ( 80.93)	Acc@5  96.09 ( 97.71)
Epoch: [27][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.3564e-01 (6.0082e-01)	Acc@1  85.94 ( 80.97)	Acc@5  97.66 ( 97.70)
Epoch: [27][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0264e-01 (6.0323e-01)	Acc@1  75.78 ( 80.86)	Acc@5  96.09 ( 97.70)
Epoch: [27][270/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.6465e-01 (6.0458e-01)	Acc@1  77.34 ( 80.83)	Acc@5  96.09 ( 97.70)
Epoch: [27][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.7871e-01 (6.0621e-01)	Acc@1  75.00 ( 80.74)	Acc@5  97.66 ( 97.67)
Epoch: [27][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.4453e-01 (6.0832e-01)	Acc@1  78.12 ( 80.68)	Acc@5  98.44 ( 97.68)
Epoch: [27][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.5312e-01 (6.0972e-01)	Acc@1  72.66 ( 80.68)	Acc@5  94.53 ( 97.66)
Epoch: [27][310/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.7236e-01 (6.1026e-01)	Acc@1  77.34 ( 80.65)	Acc@5  97.66 ( 97.67)
Epoch: [27][320/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4863e-01 (6.1320e-01)	Acc@1  81.25 ( 80.59)	Acc@5  94.53 ( 97.63)
Epoch: [27][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0205e-01 (6.1367e-01)	Acc@1  82.81 ( 80.58)	Acc@5  97.66 ( 97.62)
Epoch: [27][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6602e-01 (6.1489e-01)	Acc@1  78.91 ( 80.51)	Acc@5  98.44 ( 97.62)
Epoch: [27][350/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.5615e-01 (6.1680e-01)	Acc@1  84.38 ( 80.47)	Acc@5  97.66 ( 97.59)
Epoch: [27][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6641e-01 (6.1713e-01)	Acc@1  83.59 ( 80.45)	Acc@5  99.22 ( 97.58)
Epoch: [27][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.3096e-01 (6.1873e-01)	Acc@1  75.00 ( 80.41)	Acc@5  96.88 ( 97.57)
Epoch: [27][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8262e-01 (6.2036e-01)	Acc@1  75.78 ( 80.37)	Acc@5  98.44 ( 97.55)
Epoch: [27][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7979e-01 (6.2223e-01)	Acc@1  77.50 ( 80.29)	Acc@5  96.25 ( 97.54)
## e[27] optimizer.zero_grad (sum) time: 0.29265856742858887
## e[27]       loss.backward (sum) time: 13.831790685653687
## e[27]      optimizer.step (sum) time: 2.7206149101257324
## epoch[27] training(only) time: 45.06557607650757
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.4180e+00 (1.4180e+00)	Acc@1  62.00 ( 62.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 1.5586e+00 (1.4542e+00)	Acc@1  57.00 ( 64.45)	Acc@5  93.00 ( 89.27)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.2158e+00 (1.4034e+00)	Acc@1  68.00 ( 65.10)	Acc@5  92.00 ( 90.38)
Test: [ 30/100]	Time  0.041 ( 0.044)	Loss 1.6074e+00 (1.4292e+00)	Acc@1  60.00 ( 64.52)	Acc@5  84.00 ( 89.42)
Test: [ 40/100]	Time  0.040 ( 0.043)	Loss 1.3760e+00 (1.4118e+00)	Acc@1  66.00 ( 64.66)	Acc@5  92.00 ( 89.80)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.3252e+00 (1.4353e+00)	Acc@1  67.00 ( 64.18)	Acc@5  90.00 ( 89.25)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.2324e+00 (1.4114e+00)	Acc@1  63.00 ( 64.69)	Acc@5  92.00 ( 89.51)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.3555e+00 (1.4095e+00)	Acc@1  64.00 ( 64.89)	Acc@5  88.00 ( 89.68)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.5840e+00 (1.4106e+00)	Acc@1  61.00 ( 64.80)	Acc@5  89.00 ( 89.72)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.8252e+00 (1.4091e+00)	Acc@1  58.00 ( 64.95)	Acc@5  85.00 ( 89.85)
 * Acc@1 64.970 Acc@5 89.900
### epoch[27] execution time: 49.19406032562256
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.273 ( 0.273)	Data  0.157 ( 0.157)	Loss 5.9570e-01 (5.9570e-01)	Acc@1  84.38 ( 84.38)	Acc@5  98.44 ( 98.44)
Epoch: [28][ 10/391]	Time  0.113 ( 0.129)	Data  0.001 ( 0.018)	Loss 5.3223e-01 (5.1691e-01)	Acc@1  82.03 ( 83.03)	Acc@5  99.22 ( 98.65)
Epoch: [28][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 4.4727e-01 (5.0786e-01)	Acc@1  85.16 ( 83.59)	Acc@5  98.44 ( 98.47)
Epoch: [28][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 4.4849e-01 (5.1659e-01)	Acc@1  87.50 ( 83.72)	Acc@5  98.44 ( 98.19)
Epoch: [28][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 4.9805e-01 (5.1872e-01)	Acc@1  83.59 ( 83.63)	Acc@5  99.22 ( 98.17)
Epoch: [28][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 5.4541e-01 (5.2429e-01)	Acc@1  79.69 ( 83.49)	Acc@5  97.66 ( 98.09)
Epoch: [28][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 4.1553e-01 (5.1434e-01)	Acc@1  85.94 ( 83.76)	Acc@5  99.22 ( 98.19)
Epoch: [28][ 70/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.5723e-01 (5.1883e-01)	Acc@1  78.91 ( 83.54)	Acc@5  96.88 ( 98.21)
Epoch: [28][ 80/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.8022e-01 (5.1890e-01)	Acc@1  85.16 ( 83.47)	Acc@5  99.22 ( 98.22)
Epoch: [28][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.1465e-01 (5.2482e-01)	Acc@1  83.59 ( 83.23)	Acc@5  97.66 ( 98.23)
Epoch: [28][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.1333e-01 (5.3248e-01)	Acc@1  85.16 ( 83.03)	Acc@5  98.44 ( 98.22)
Epoch: [28][110/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.4756e-01 (5.4255e-01)	Acc@1  78.12 ( 82.67)	Acc@5  95.31 ( 98.16)
Epoch: [28][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.8496e-01 (5.4480e-01)	Acc@1  79.69 ( 82.63)	Acc@5  97.66 ( 98.17)
Epoch: [28][130/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.0195e-01 (5.5171e-01)	Acc@1  82.81 ( 82.40)	Acc@5  98.44 ( 98.13)
Epoch: [28][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.4590e-01 (5.5153e-01)	Acc@1  82.03 ( 82.42)	Acc@5  99.22 ( 98.13)
Epoch: [28][150/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.4746e-01 (5.5352e-01)	Acc@1  78.12 ( 82.35)	Acc@5  98.44 ( 98.14)
Epoch: [28][160/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.0146e-01 (5.5426e-01)	Acc@1  81.25 ( 82.28)	Acc@5  98.44 ( 98.13)
Epoch: [28][170/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.4258e-01 (5.5318e-01)	Acc@1  80.47 ( 82.32)	Acc@5  98.44 ( 98.14)
Epoch: [28][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.6533e-01 (5.5598e-01)	Acc@1  84.38 ( 82.26)	Acc@5  98.44 ( 98.13)
Epoch: [28][190/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.2324e-01 (5.5666e-01)	Acc@1  74.22 ( 82.27)	Acc@5  95.31 ( 98.12)
Epoch: [28][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.2207e-01 (5.5789e-01)	Acc@1  81.25 ( 82.25)	Acc@5  97.66 ( 98.10)
Epoch: [28][210/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.1162e-01 (5.6077e-01)	Acc@1  89.06 ( 82.21)	Acc@5  99.22 ( 98.07)
Epoch: [28][220/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.2451e-01 (5.6342e-01)	Acc@1  79.69 ( 82.11)	Acc@5  96.88 ( 98.01)
Epoch: [28][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.7959e-01 (5.6524e-01)	Acc@1  85.94 ( 82.10)	Acc@5  99.22 ( 97.99)
Epoch: [28][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.1826e-01 (5.6641e-01)	Acc@1  80.47 ( 82.10)	Acc@5  96.88 ( 97.97)
Epoch: [28][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6299e-01 (5.6570e-01)	Acc@1  79.69 ( 82.08)	Acc@5  99.22 ( 97.99)
Epoch: [28][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.7876e-01 (5.6651e-01)	Acc@1  85.16 ( 82.06)	Acc@5  98.44 ( 97.99)
Epoch: [28][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9878e-01 (5.6856e-01)	Acc@1  85.16 ( 82.00)	Acc@5  96.09 ( 97.96)
Epoch: [28][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.8613e-01 (5.7072e-01)	Acc@1  75.00 ( 81.93)	Acc@5  97.66 ( 97.95)
Epoch: [28][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6836e-01 (5.7224e-01)	Acc@1  83.59 ( 81.89)	Acc@5  99.22 ( 97.92)
Epoch: [28][300/391]	Time  0.114 ( 0.115)	Data  0.003 ( 0.004)	Loss 6.4209e-01 (5.7293e-01)	Acc@1  80.47 ( 81.88)	Acc@5  97.66 ( 97.92)
Epoch: [28][310/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.1006e-01 (5.7505e-01)	Acc@1  74.22 ( 81.78)	Acc@5  96.88 ( 97.91)
Epoch: [28][320/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.4443e-01 (5.7695e-01)	Acc@1  81.25 ( 81.70)	Acc@5  97.66 ( 97.88)
Epoch: [28][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.3428e-01 (5.7840e-01)	Acc@1  81.25 ( 81.66)	Acc@5  98.44 ( 97.87)
Epoch: [28][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.7793e-01 (5.7992e-01)	Acc@1  74.22 ( 81.61)	Acc@5  92.97 ( 97.86)
Epoch: [28][350/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.7236e-01 (5.8155e-01)	Acc@1  77.34 ( 81.56)	Acc@5  96.88 ( 97.84)
Epoch: [28][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8506e-01 (5.8398e-01)	Acc@1  79.69 ( 81.49)	Acc@5  96.88 ( 97.82)
Epoch: [28][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.7485e-01 (5.8446e-01)	Acc@1  85.16 ( 81.49)	Acc@5  97.66 ( 97.82)
Epoch: [28][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6357e-01 (5.8714e-01)	Acc@1  75.78 ( 81.40)	Acc@5  96.09 ( 97.80)
Epoch: [28][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9102e-01 (5.8797e-01)	Acc@1  72.50 ( 81.38)	Acc@5  97.50 ( 97.81)
## e[28] optimizer.zero_grad (sum) time: 0.2924337387084961
## e[28]       loss.backward (sum) time: 13.763401508331299
## e[28]      optimizer.step (sum) time: 2.7110707759857178
## epoch[28] training(only) time: 45.09425449371338
# Switched to evaluate mode...
Test: [  0/100]	Time  0.176 ( 0.176)	Loss 1.5117e+00 (1.5117e+00)	Acc@1  59.00 ( 59.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.5566e+00 (1.5194e+00)	Acc@1  57.00 ( 63.09)	Acc@5  89.00 ( 88.09)
Test: [ 20/100]	Time  0.040 ( 0.046)	Loss 1.3730e+00 (1.4743e+00)	Acc@1  62.00 ( 63.05)	Acc@5  90.00 ( 88.52)
Test: [ 30/100]	Time  0.038 ( 0.043)	Loss 1.6553e+00 (1.4856e+00)	Acc@1  56.00 ( 62.42)	Acc@5  92.00 ( 88.39)
Test: [ 40/100]	Time  0.047 ( 0.042)	Loss 1.4600e+00 (1.4659e+00)	Acc@1  65.00 ( 62.85)	Acc@5  88.00 ( 88.80)
Test: [ 50/100]	Time  0.039 ( 0.041)	Loss 1.2842e+00 (1.4611e+00)	Acc@1  67.00 ( 62.84)	Acc@5  89.00 ( 88.45)
Test: [ 60/100]	Time  0.040 ( 0.041)	Loss 1.5762e+00 (1.4525e+00)	Acc@1  63.00 ( 63.15)	Acc@5  86.00 ( 88.57)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.3779e+00 (1.4481e+00)	Acc@1  65.00 ( 63.30)	Acc@5  87.00 ( 88.65)
Test: [ 80/100]	Time  0.039 ( 0.040)	Loss 1.7500e+00 (1.4513e+00)	Acc@1  67.00 ( 63.41)	Acc@5  86.00 ( 88.65)
Test: [ 90/100]	Time  0.038 ( 0.040)	Loss 1.7246e+00 (1.4400e+00)	Acc@1  60.00 ( 63.77)	Acc@5  86.00 ( 88.74)
 * Acc@1 64.050 Acc@5 88.750
### epoch[28] execution time: 49.18683862686157
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.267 ( 0.267)	Data  0.153 ( 0.153)	Loss 3.8647e-01 (3.8647e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [29][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.017)	Loss 4.4189e-01 (4.8779e-01)	Acc@1  85.16 ( 84.02)	Acc@5  99.22 ( 98.58)
Epoch: [29][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.011)	Loss 5.9766e-01 (4.9866e-01)	Acc@1  78.91 ( 83.52)	Acc@5  99.22 ( 98.44)
Epoch: [29][ 30/391]	Time  0.109 ( 0.119)	Data  0.001 ( 0.009)	Loss 4.7705e-01 (4.9102e-01)	Acc@1  87.50 ( 84.27)	Acc@5  97.66 ( 98.41)
Epoch: [29][ 40/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.007)	Loss 5.6250e-01 (4.8376e-01)	Acc@1  77.34 ( 84.36)	Acc@5  99.22 ( 98.57)
Epoch: [29][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 6.4795e-01 (4.7790e-01)	Acc@1  78.91 ( 84.56)	Acc@5  97.66 ( 98.67)
Epoch: [29][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 4.5776e-01 (4.8164e-01)	Acc@1  82.03 ( 84.37)	Acc@5  99.22 ( 98.60)
Epoch: [29][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 5.1953e-01 (4.8667e-01)	Acc@1  85.16 ( 84.19)	Acc@5  96.88 ( 98.54)
Epoch: [29][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.4199e-01 (4.9451e-01)	Acc@1  81.25 ( 83.94)	Acc@5 100.00 ( 98.46)
Epoch: [29][ 90/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.1426e-01 (5.0571e-01)	Acc@1  82.81 ( 83.65)	Acc@5  98.44 ( 98.33)
Epoch: [29][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.9609e-01 (5.0722e-01)	Acc@1  80.47 ( 83.61)	Acc@5  99.22 ( 98.37)
Epoch: [29][110/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.0068e-01 (5.0885e-01)	Acc@1  78.12 ( 83.54)	Acc@5  96.09 ( 98.37)
Epoch: [29][120/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.8936e-01 (5.1391e-01)	Acc@1  78.91 ( 83.45)	Acc@5  99.22 ( 98.38)
Epoch: [29][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.6357e-01 (5.1667e-01)	Acc@1  78.91 ( 83.39)	Acc@5  96.09 ( 98.32)
Epoch: [29][140/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.5840e-01 (5.1760e-01)	Acc@1  72.66 ( 83.33)	Acc@5  94.53 ( 98.32)
Epoch: [29][150/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.1475e-01 (5.1906e-01)	Acc@1  81.25 ( 83.31)	Acc@5  99.22 ( 98.32)
Epoch: [29][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.5029e-01 (5.2212e-01)	Acc@1  83.59 ( 83.18)	Acc@5  96.88 ( 98.28)
Epoch: [29][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.9268e-01 (5.2480e-01)	Acc@1  84.38 ( 83.06)	Acc@5 100.00 ( 98.27)
Epoch: [29][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.6309e-01 (5.3201e-01)	Acc@1  78.91 ( 82.86)	Acc@5  98.44 ( 98.23)
Epoch: [29][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.3379e-01 (5.3222e-01)	Acc@1  77.34 ( 82.86)	Acc@5  98.44 ( 98.25)
Epoch: [29][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.0215e-01 (5.3871e-01)	Acc@1  79.69 ( 82.70)	Acc@5  96.88 ( 98.20)
Epoch: [29][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.5127e-01 (5.4248e-01)	Acc@1  80.47 ( 82.58)	Acc@5  99.22 ( 98.21)
Epoch: [29][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.8105e-01 (5.4405e-01)	Acc@1  82.81 ( 82.56)	Acc@5  98.44 ( 98.20)
Epoch: [29][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.3320e-01 (5.4465e-01)	Acc@1  82.81 ( 82.47)	Acc@5  96.09 ( 98.18)
Epoch: [29][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.0361e-01 (5.4639e-01)	Acc@1  77.34 ( 82.43)	Acc@5  98.44 ( 98.18)
Epoch: [29][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.3682e-01 (5.4905e-01)	Acc@1  77.34 ( 82.32)	Acc@5  94.53 ( 98.14)
Epoch: [29][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.3525e-01 (5.5222e-01)	Acc@1  80.47 ( 82.20)	Acc@5  95.31 ( 98.12)
Epoch: [29][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0205e-01 (5.5400e-01)	Acc@1  77.34 ( 82.16)	Acc@5  97.66 ( 98.07)
Epoch: [29][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5283e-01 (5.5602e-01)	Acc@1  81.25 ( 82.13)	Acc@5  98.44 ( 98.05)
Epoch: [29][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.5225e-01 (5.5696e-01)	Acc@1  82.81 ( 82.12)	Acc@5  98.44 ( 98.03)
Epoch: [29][300/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.0293e-01 (5.5717e-01)	Acc@1  82.03 ( 82.14)	Acc@5  99.22 ( 98.04)
Epoch: [29][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8311e-01 (5.5804e-01)	Acc@1  79.69 ( 82.11)	Acc@5  97.66 ( 98.05)
Epoch: [29][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.1943e-01 (5.5868e-01)	Acc@1  89.84 ( 82.10)	Acc@5  98.44 ( 98.05)
Epoch: [29][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.2207e-01 (5.5987e-01)	Acc@1  80.47 ( 82.11)	Acc@5  96.88 ( 98.04)
Epoch: [29][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.2930e-01 (5.6070e-01)	Acc@1  84.38 ( 82.08)	Acc@5  97.66 ( 98.03)
Epoch: [29][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.8037e-01 (5.6148e-01)	Acc@1  75.00 ( 82.09)	Acc@5  96.09 ( 98.02)
Epoch: [29][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.2637e-01 (5.6116e-01)	Acc@1  85.94 ( 82.08)	Acc@5  97.66 ( 98.02)
Epoch: [29][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2754e-01 (5.6243e-01)	Acc@1  75.78 ( 82.06)	Acc@5  96.09 ( 98.00)
Epoch: [29][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.7227e-01 (5.6446e-01)	Acc@1  81.25 ( 82.03)	Acc@5  98.44 ( 97.98)
Epoch: [29][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.5811e-01 (5.6543e-01)	Acc@1  86.25 ( 82.00)	Acc@5  97.50 ( 97.99)
## e[29] optimizer.zero_grad (sum) time: 0.29535722732543945
## e[29]       loss.backward (sum) time: 13.794523000717163
## e[29]      optimizer.step (sum) time: 2.731461763381958
## epoch[29] training(only) time: 45.10693120956421
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.4883e+00 (1.4883e+00)	Acc@1  66.00 ( 66.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.040 ( 0.053)	Loss 1.6270e+00 (1.4463e+00)	Acc@1  60.00 ( 66.73)	Acc@5  91.00 ( 89.45)
Test: [ 20/100]	Time  0.040 ( 0.047)	Loss 1.0576e+00 (1.3645e+00)	Acc@1  71.00 ( 67.90)	Acc@5  93.00 ( 89.90)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 1.6924e+00 (1.3853e+00)	Acc@1  59.00 ( 67.32)	Acc@5  87.00 ( 89.45)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.2100e+00 (1.3746e+00)	Acc@1  78.00 ( 66.88)	Acc@5  93.00 ( 89.83)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.6465e+00 (1.3926e+00)	Acc@1  64.00 ( 66.47)	Acc@5  87.00 ( 89.75)
Test: [ 60/100]	Time  0.040 ( 0.042)	Loss 1.3574e+00 (1.3696e+00)	Acc@1  61.00 ( 66.51)	Acc@5  92.00 ( 90.13)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.4219e+00 (1.3812e+00)	Acc@1  63.00 ( 66.11)	Acc@5  89.00 ( 90.03)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.2969e+00 (1.3762e+00)	Acc@1  67.00 ( 66.10)	Acc@5  95.00 ( 90.11)
Test: [ 90/100]	Time  0.037 ( 0.041)	Loss 1.4756e+00 (1.3630e+00)	Acc@1  60.00 ( 66.14)	Acc@5  92.00 ( 90.15)
 * Acc@1 66.250 Acc@5 90.190
### epoch[29] execution time: 49.23883295059204
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.269 ( 0.269)	Data  0.154 ( 0.154)	Loss 4.7803e-01 (4.7803e-01)	Acc@1  84.38 ( 84.38)	Acc@5  97.66 ( 97.66)
Epoch: [30][ 10/391]	Time  0.114 ( 0.128)	Data  0.001 ( 0.017)	Loss 4.7803e-01 (4.6600e-01)	Acc@1  86.72 ( 85.58)	Acc@5  98.44 ( 98.72)
Epoch: [30][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.011)	Loss 5.7178e-01 (4.4999e-01)	Acc@1  82.81 ( 86.27)	Acc@5  96.09 ( 98.51)
Epoch: [30][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 4.2700e-01 (4.3225e-01)	Acc@1  89.84 ( 86.74)	Acc@5  98.44 ( 98.64)
Epoch: [30][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 4.5532e-01 (4.1541e-01)	Acc@1  83.59 ( 87.08)	Acc@5  98.44 ( 98.74)
Epoch: [30][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 3.1274e-01 (4.0698e-01)	Acc@1  91.41 ( 87.55)	Acc@5  99.22 ( 98.81)
Epoch: [30][ 60/391]	Time  0.117 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.6060e-01 (4.0186e-01)	Acc@1  92.19 ( 87.73)	Acc@5 100.00 ( 98.82)
Epoch: [30][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.8931e-01 (3.9387e-01)	Acc@1  94.53 ( 88.15)	Acc@5  98.44 ( 98.88)
Epoch: [30][ 80/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.0288e-01 (3.8978e-01)	Acc@1  95.31 ( 88.33)	Acc@5 100.00 ( 98.92)
Epoch: [30][ 90/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.3254e-01 (3.8344e-01)	Acc@1  94.53 ( 88.52)	Acc@5 100.00 ( 98.99)
Epoch: [30][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4231e-01 (3.7733e-01)	Acc@1  92.19 ( 88.67)	Acc@5 100.00 ( 99.03)
Epoch: [30][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.3789e-01 (3.7250e-01)	Acc@1  89.06 ( 88.79)	Acc@5  99.22 ( 99.04)
Epoch: [30][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.1431e-01 (3.6809e-01)	Acc@1  89.06 ( 88.90)	Acc@5  99.22 ( 99.08)
Epoch: [30][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.2324e-01 (3.6431e-01)	Acc@1  90.62 ( 88.98)	Acc@5  98.44 ( 99.09)
Epoch: [30][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.6523e-01 (3.5934e-01)	Acc@1  89.06 ( 89.15)	Acc@5  99.22 ( 99.12)
Epoch: [30][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4500e-01 (3.5539e-01)	Acc@1  92.19 ( 89.27)	Acc@5 100.00 ( 99.14)
Epoch: [30][160/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.6074e-01 (3.5265e-01)	Acc@1  92.97 ( 89.36)	Acc@5  97.66 ( 99.14)
Epoch: [30][170/391]	Time  0.107 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.7402e-01 (3.4820e-01)	Acc@1  90.62 ( 89.56)	Acc@5  99.22 ( 99.15)
Epoch: [30][180/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.6646e-01 (3.4614e-01)	Acc@1  89.06 ( 89.59)	Acc@5  98.44 ( 99.16)
Epoch: [30][190/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.8306e-01 (3.4264e-01)	Acc@1  86.72 ( 89.68)	Acc@5  97.66 ( 99.18)
Epoch: [30][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9617e-01 (3.3904e-01)	Acc@1  94.53 ( 89.77)	Acc@5  99.22 ( 99.18)
Epoch: [30][210/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8955e-01 (3.3644e-01)	Acc@1  92.19 ( 89.83)	Acc@5 100.00 ( 99.20)
Epoch: [30][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2876e-01 (3.3355e-01)	Acc@1  94.53 ( 89.89)	Acc@5  99.22 ( 99.22)
Epoch: [30][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4976e-01 (3.3044e-01)	Acc@1  93.75 ( 90.00)	Acc@5 100.00 ( 99.22)
Epoch: [30][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0203e-01 (3.2757e-01)	Acc@1  93.75 ( 90.07)	Acc@5  99.22 ( 99.22)
Epoch: [30][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7979e-01 (3.2564e-01)	Acc@1  90.62 ( 90.13)	Acc@5 100.00 ( 99.25)
Epoch: [30][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3035e-01 (3.2444e-01)	Acc@1  92.97 ( 90.14)	Acc@5  99.22 ( 99.24)
Epoch: [30][270/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9150e-01 (3.2316e-01)	Acc@1  91.41 ( 90.16)	Acc@5  99.22 ( 99.25)
Epoch: [30][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7148e-01 (3.2223e-01)	Acc@1  90.62 ( 90.18)	Acc@5 100.00 ( 99.26)
Epoch: [30][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4841e-01 (3.1995e-01)	Acc@1  91.41 ( 90.27)	Acc@5  99.22 ( 99.26)
Epoch: [30][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7417e-01 (3.1824e-01)	Acc@1  92.19 ( 90.31)	Acc@5  99.22 ( 99.28)
Epoch: [30][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5269e-01 (3.1615e-01)	Acc@1  92.97 ( 90.37)	Acc@5  99.22 ( 99.28)
Epoch: [30][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6663e-01 (3.1383e-01)	Acc@1  95.31 ( 90.44)	Acc@5 100.00 ( 99.29)
Epoch: [30][330/391]	Time  0.111 ( 0.115)	Data  0.002 ( 0.004)	Loss 3.4204e-01 (3.1249e-01)	Acc@1  89.84 ( 90.47)	Acc@5  97.66 ( 99.30)
Epoch: [30][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4854e-01 (3.1005e-01)	Acc@1  92.19 ( 90.53)	Acc@5 100.00 ( 99.31)
Epoch: [30][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9736e-01 (3.0891e-01)	Acc@1  89.84 ( 90.55)	Acc@5 100.00 ( 99.32)
Epoch: [30][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9517e-01 (3.0687e-01)	Acc@1  86.72 ( 90.61)	Acc@5  99.22 ( 99.33)
Epoch: [30][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6782e-01 (3.0572e-01)	Acc@1  89.06 ( 90.64)	Acc@5 100.00 ( 99.34)
Epoch: [30][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2009e-01 (3.0350e-01)	Acc@1  91.41 ( 90.71)	Acc@5 100.00 ( 99.34)
Epoch: [30][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2764e-01 (3.0225e-01)	Acc@1  87.50 ( 90.75)	Acc@5 100.00 ( 99.34)
## e[30] optimizer.zero_grad (sum) time: 0.29459047317504883
## e[30]       loss.backward (sum) time: 13.77150559425354
## e[30]      optimizer.step (sum) time: 2.7216386795043945
## epoch[30] training(only) time: 45.10237431526184
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1406e+00 (1.1406e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.3594e+00 (1.1861e+00)	Acc@1  65.00 ( 72.45)	Acc@5  95.00 ( 92.64)
Test: [ 20/100]	Time  0.040 ( 0.047)	Loss 1.1963e+00 (1.1507e+00)	Acc@1  71.00 ( 72.81)	Acc@5  92.00 ( 93.00)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 1.6221e+00 (1.1966e+00)	Acc@1  66.00 ( 71.87)	Acc@5  88.00 ( 92.32)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.1914e+00 (1.1787e+00)	Acc@1  73.00 ( 71.90)	Acc@5  93.00 ( 92.68)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.3447e+00 (1.1920e+00)	Acc@1  69.00 ( 71.63)	Acc@5  92.00 ( 92.49)
Test: [ 60/100]	Time  0.041 ( 0.041)	Loss 1.3018e+00 (1.1560e+00)	Acc@1  66.00 ( 72.00)	Acc@5  93.00 ( 92.75)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.2168e+00 (1.1631e+00)	Acc@1  68.00 ( 71.87)	Acc@5  93.00 ( 92.73)
Test: [ 80/100]	Time  0.040 ( 0.041)	Loss 1.2480e+00 (1.1656e+00)	Acc@1  70.00 ( 71.78)	Acc@5  93.00 ( 92.77)
Test: [ 90/100]	Time  0.037 ( 0.041)	Loss 1.5664e+00 (1.1533e+00)	Acc@1  65.00 ( 71.93)	Acc@5  92.00 ( 92.79)
 * Acc@1 72.040 Acc@5 92.820
### epoch[30] execution time: 49.23568272590637
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.278 ( 0.278)	Data  0.160 ( 0.160)	Loss 2.8638e-01 (2.8638e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [31][ 10/391]	Time  0.113 ( 0.129)	Data  0.001 ( 0.018)	Loss 2.5537e-01 (2.1648e-01)	Acc@1  91.41 ( 93.18)	Acc@5 100.00 ( 99.72)
Epoch: [31][ 20/391]	Time  0.115 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.4426e-01 (2.0923e-01)	Acc@1  91.41 ( 93.53)	Acc@5 100.00 ( 99.81)
Epoch: [31][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.0081e-01 (2.1671e-01)	Acc@1  92.19 ( 93.37)	Acc@5 100.00 ( 99.67)
Epoch: [31][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 2.2058e-01 (2.1342e-01)	Acc@1  92.97 ( 93.48)	Acc@5  99.22 ( 99.71)
Epoch: [31][ 50/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.8115e-01 (2.1100e-01)	Acc@1  93.75 ( 93.55)	Acc@5 100.00 ( 99.74)
Epoch: [31][ 60/391]	Time  0.116 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.1155e-01 (2.0902e-01)	Acc@1  92.97 ( 93.62)	Acc@5 100.00 ( 99.74)
Epoch: [31][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.7664e-01 (2.0992e-01)	Acc@1  96.09 ( 93.46)	Acc@5 100.00 ( 99.76)
Epoch: [31][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.3293e-01 (2.0792e-01)	Acc@1  96.09 ( 93.61)	Acc@5 100.00 ( 99.75)
Epoch: [31][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7749e-01 (2.0607e-01)	Acc@1  95.31 ( 93.77)	Acc@5  99.22 ( 99.71)
Epoch: [31][100/391]	Time  0.117 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5503e-01 (2.0783e-01)	Acc@1  96.09 ( 93.72)	Acc@5 100.00 ( 99.70)
Epoch: [31][110/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.4656e-02 (2.0739e-01)	Acc@1  99.22 ( 93.74)	Acc@5 100.00 ( 99.71)
Epoch: [31][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0225e-01 (2.0907e-01)	Acc@1  92.19 ( 93.74)	Acc@5  97.66 ( 99.66)
Epoch: [31][130/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2522e-01 (2.0821e-01)	Acc@1  92.97 ( 93.76)	Acc@5  98.44 ( 99.66)
Epoch: [31][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.7368e-01 (2.0824e-01)	Acc@1  89.84 ( 93.74)	Acc@5 100.00 ( 99.68)
Epoch: [31][150/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0715e-01 (2.0914e-01)	Acc@1  93.75 ( 93.69)	Acc@5 100.00 ( 99.67)
Epoch: [31][160/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1704e-01 (2.0822e-01)	Acc@1  92.97 ( 93.71)	Acc@5 100.00 ( 99.68)
Epoch: [31][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8140e-01 (2.0871e-01)	Acc@1  93.75 ( 93.72)	Acc@5 100.00 ( 99.68)
Epoch: [31][180/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1240e-01 (2.0951e-01)	Acc@1  91.41 ( 93.66)	Acc@5 100.00 ( 99.68)
Epoch: [31][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7102e-01 (2.0832e-01)	Acc@1  91.41 ( 93.70)	Acc@5 100.00 ( 99.67)
Epoch: [31][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5195e-01 (2.0705e-01)	Acc@1  91.41 ( 93.72)	Acc@5 100.00 ( 99.69)
Epoch: [31][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7603e-01 (2.0636e-01)	Acc@1  92.97 ( 93.74)	Acc@5 100.00 ( 99.69)
Epoch: [31][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4954e-01 (2.0664e-01)	Acc@1  93.75 ( 93.70)	Acc@5 100.00 ( 99.69)
Epoch: [31][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2412e-01 (2.0671e-01)	Acc@1  92.97 ( 93.72)	Acc@5  98.44 ( 99.69)
Epoch: [31][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2079e-01 (2.0568e-01)	Acc@1  96.09 ( 93.75)	Acc@5 100.00 ( 99.69)
Epoch: [31][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3340e-01 (2.0530e-01)	Acc@1  95.31 ( 93.77)	Acc@5  98.44 ( 99.68)
Epoch: [31][260/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3171e-01 (2.0471e-01)	Acc@1  96.88 ( 93.79)	Acc@5 100.00 ( 99.69)
Epoch: [31][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5757e-01 (2.0554e-01)	Acc@1  92.19 ( 93.76)	Acc@5  99.22 ( 99.68)
Epoch: [31][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4414e-01 (2.0450e-01)	Acc@1  92.19 ( 93.78)	Acc@5 100.00 ( 99.69)
Epoch: [31][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1252e-01 (2.0408e-01)	Acc@1  92.19 ( 93.79)	Acc@5  99.22 ( 99.68)
Epoch: [31][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6221e-01 (2.0362e-01)	Acc@1  92.97 ( 93.79)	Acc@5 100.00 ( 99.69)
Epoch: [31][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8345e-01 (2.0394e-01)	Acc@1  92.97 ( 93.78)	Acc@5 100.00 ( 99.70)
Epoch: [31][320/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8738e-01 (2.0288e-01)	Acc@1  93.75 ( 93.81)	Acc@5 100.00 ( 99.70)
Epoch: [31][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5854e-01 (2.0321e-01)	Acc@1  91.41 ( 93.79)	Acc@5 100.00 ( 99.70)
Epoch: [31][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3306e-01 (2.0161e-01)	Acc@1  96.09 ( 93.86)	Acc@5 100.00 ( 99.71)
Epoch: [31][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0764e-01 (2.0131e-01)	Acc@1  92.19 ( 93.85)	Acc@5 100.00 ( 99.71)
Epoch: [31][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4121e-01 (2.0188e-01)	Acc@1  91.41 ( 93.81)	Acc@5  99.22 ( 99.71)
Epoch: [31][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7004e-01 (2.0121e-01)	Acc@1  96.88 ( 93.84)	Acc@5  99.22 ( 99.71)
Epoch: [31][380/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6467e-01 (2.0076e-01)	Acc@1  94.53 ( 93.84)	Acc@5  99.22 ( 99.71)
Epoch: [31][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0337e-01 (2.0111e-01)	Acc@1  91.25 ( 93.81)	Acc@5  98.75 ( 99.71)
## e[31] optimizer.zero_grad (sum) time: 0.28998732566833496
## e[31]       loss.backward (sum) time: 13.75447940826416
## e[31]      optimizer.step (sum) time: 2.745037794113159
## epoch[31] training(only) time: 45.06433320045471
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.1533e+00 (1.1533e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.4102e+00 (1.2191e+00)	Acc@1  65.00 ( 73.09)	Acc@5  93.00 ( 92.45)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.2930e+00 (1.1780e+00)	Acc@1  71.00 ( 73.10)	Acc@5  93.00 ( 92.62)
Test: [ 30/100]	Time  0.037 ( 0.043)	Loss 1.7207e+00 (1.2327e+00)	Acc@1  65.00 ( 72.29)	Acc@5  88.00 ( 92.10)
Test: [ 40/100]	Time  0.037 ( 0.042)	Loss 1.2129e+00 (1.2098e+00)	Acc@1  76.00 ( 72.34)	Acc@5  93.00 ( 92.51)
Test: [ 50/100]	Time  0.039 ( 0.041)	Loss 1.4463e+00 (1.2265e+00)	Acc@1  70.00 ( 72.00)	Acc@5  93.00 ( 92.37)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.2871e+00 (1.1879e+00)	Acc@1  69.00 ( 72.48)	Acc@5  93.00 ( 92.62)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.2891e+00 (1.1947e+00)	Acc@1  67.00 ( 72.30)	Acc@5  93.00 ( 92.68)
Test: [ 80/100]	Time  0.039 ( 0.040)	Loss 1.3291e+00 (1.1953e+00)	Acc@1  70.00 ( 72.37)	Acc@5  93.00 ( 92.74)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 1.5732e+00 (1.1803e+00)	Acc@1  61.00 ( 72.48)	Acc@5  94.00 ( 92.80)
 * Acc@1 72.580 Acc@5 92.840
### epoch[31] execution time: 49.17606806755066
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.264 ( 0.264)	Data  0.154 ( 0.154)	Loss 2.1240e-01 (2.1240e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.112 ( 0.127)	Data  0.001 ( 0.017)	Loss 9.5581e-02 (1.5436e-01)	Acc@1  97.66 ( 95.53)	Acc@5 100.00 ( 99.86)
Epoch: [32][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.7615e-01 (1.5796e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.81)
Epoch: [32][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.008)	Loss 2.1338e-01 (1.6968e-01)	Acc@1  92.97 ( 95.04)	Acc@5 100.00 ( 99.72)
Epoch: [32][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.4963e-01 (1.7699e-01)	Acc@1  94.53 ( 94.86)	Acc@5  98.44 ( 99.70)
Epoch: [32][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.0345e-01 (1.7567e-01)	Acc@1  96.88 ( 94.90)	Acc@5 100.00 ( 99.71)
Epoch: [32][ 60/391]	Time  0.110 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.3879e-01 (1.7225e-01)	Acc@1  97.66 ( 95.09)	Acc@5 100.00 ( 99.72)
Epoch: [32][ 70/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.4502e-01 (1.7259e-01)	Acc@1  95.31 ( 94.95)	Acc@5 100.00 ( 99.76)
Epoch: [32][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.0459e-01 (1.7185e-01)	Acc@1  94.53 ( 94.97)	Acc@5  98.44 ( 99.75)
Epoch: [32][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0974e-01 (1.6921e-01)	Acc@1  97.66 ( 95.04)	Acc@5 100.00 ( 99.74)
Epoch: [32][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4426e-01 (1.6940e-01)	Acc@1  92.97 ( 94.98)	Acc@5 100.00 ( 99.77)
Epoch: [32][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9202e-01 (1.7021e-01)	Acc@1  93.75 ( 94.93)	Acc@5 100.00 ( 99.79)
Epoch: [32][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6663e-01 (1.6950e-01)	Acc@1  93.75 ( 94.98)	Acc@5 100.00 ( 99.79)
Epoch: [32][130/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2329e-01 (1.7029e-01)	Acc@1  96.88 ( 94.95)	Acc@5 100.00 ( 99.80)
Epoch: [32][140/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3477e-01 (1.7205e-01)	Acc@1  95.31 ( 94.90)	Acc@5 100.00 ( 99.79)
Epoch: [32][150/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9849e-01 (1.7228e-01)	Acc@1  94.53 ( 94.91)	Acc@5  98.44 ( 99.79)
Epoch: [32][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2217e-01 (1.7203e-01)	Acc@1  92.97 ( 94.87)	Acc@5 100.00 ( 99.79)
Epoch: [32][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8127e-01 (1.7075e-01)	Acc@1  95.31 ( 94.92)	Acc@5 100.00 ( 99.79)
Epoch: [32][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5820e-01 (1.7095e-01)	Acc@1  94.53 ( 94.91)	Acc@5 100.00 ( 99.79)
Epoch: [32][190/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8958e-01 (1.7037e-01)	Acc@1  95.31 ( 94.92)	Acc@5  99.22 ( 99.79)
Epoch: [32][200/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8115e-01 (1.7038e-01)	Acc@1  92.19 ( 94.89)	Acc@5 100.00 ( 99.79)
Epoch: [32][210/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3450e-01 (1.6928e-01)	Acc@1  93.75 ( 94.91)	Acc@5 100.00 ( 99.79)
Epoch: [32][220/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4294e-01 (1.6879e-01)	Acc@1  96.88 ( 94.93)	Acc@5 100.00 ( 99.79)
Epoch: [32][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1143e-01 (1.6908e-01)	Acc@1  91.41 ( 94.91)	Acc@5  99.22 ( 99.78)
Epoch: [32][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2000e-01 (1.6831e-01)	Acc@1  95.31 ( 94.92)	Acc@5 100.00 ( 99.79)
Epoch: [32][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7041e-01 (1.6769e-01)	Acc@1  93.75 ( 94.92)	Acc@5 100.00 ( 99.79)
Epoch: [32][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0693e-01 (1.6796e-01)	Acc@1  97.66 ( 94.91)	Acc@5  99.22 ( 99.79)
Epoch: [32][270/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4854e-01 (1.6858e-01)	Acc@1  93.75 ( 94.90)	Acc@5  99.22 ( 99.79)
Epoch: [32][280/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7676e-01 (1.6790e-01)	Acc@1  94.53 ( 94.92)	Acc@5 100.00 ( 99.79)
Epoch: [32][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4404e-01 (1.6724e-01)	Acc@1  96.09 ( 94.94)	Acc@5 100.00 ( 99.79)
Epoch: [32][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4050e-01 (1.6677e-01)	Acc@1  95.31 ( 94.95)	Acc@5 100.00 ( 99.79)
Epoch: [32][310/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9983e-01 (1.6696e-01)	Acc@1  94.53 ( 94.95)	Acc@5  99.22 ( 99.79)
Epoch: [32][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4148e-01 (1.6698e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.79)
Epoch: [32][330/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4539e-01 (1.6662e-01)	Acc@1  95.31 ( 94.96)	Acc@5 100.00 ( 99.78)
Epoch: [32][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2131e-01 (1.6668e-01)	Acc@1  91.41 ( 94.96)	Acc@5 100.00 ( 99.78)
Epoch: [32][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3452e-01 (1.6594e-01)	Acc@1  96.09 ( 94.98)	Acc@5 100.00 ( 99.78)
Epoch: [32][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6187e-01 (1.6545e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.78)
Epoch: [32][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1639e-01 (1.6602e-01)	Acc@1  96.88 ( 95.01)	Acc@5 100.00 ( 99.78)
Epoch: [32][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2034e-01 (1.6618e-01)	Acc@1  92.19 ( 95.01)	Acc@5  99.22 ( 99.78)
Epoch: [32][390/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5015e-01 (1.6553e-01)	Acc@1  96.25 ( 95.03)	Acc@5 100.00 ( 99.78)
## e[32] optimizer.zero_grad (sum) time: 0.2912900447845459
## e[32]       loss.backward (sum) time: 13.756256103515625
## e[32]      optimizer.step (sum) time: 2.739595890045166
## epoch[32] training(only) time: 45.01208472251892
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.0820e+00 (1.0820e+00)	Acc@1  76.00 ( 76.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.040 ( 0.053)	Loss 1.4893e+00 (1.2577e+00)	Acc@1  67.00 ( 74.09)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.4004e+00 (1.2099e+00)	Acc@1  70.00 ( 73.81)	Acc@5  91.00 ( 92.48)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 1.7363e+00 (1.2632e+00)	Acc@1  66.00 ( 72.94)	Acc@5  90.00 ( 92.06)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.2451e+00 (1.2432e+00)	Acc@1  75.00 ( 72.80)	Acc@5  95.00 ( 92.56)
Test: [ 50/100]	Time  0.040 ( 0.042)	Loss 1.5518e+00 (1.2577e+00)	Acc@1  71.00 ( 72.59)	Acc@5  92.00 ( 92.37)
Test: [ 60/100]	Time  0.037 ( 0.041)	Loss 1.4492e+00 (1.2217e+00)	Acc@1  68.00 ( 72.93)	Acc@5  93.00 ( 92.64)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 1.2998e+00 (1.2300e+00)	Acc@1  68.00 ( 72.79)	Acc@5  93.00 ( 92.66)
Test: [ 80/100]	Time  0.041 ( 0.041)	Loss 1.3672e+00 (1.2312e+00)	Acc@1  70.00 ( 72.77)	Acc@5  92.00 ( 92.73)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.7012e+00 (1.2142e+00)	Acc@1  59.00 ( 72.80)	Acc@5  93.00 ( 92.78)
 * Acc@1 72.910 Acc@5 92.840
### epoch[32] execution time: 49.13283324241638
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.269 ( 0.269)	Data  0.162 ( 0.162)	Loss 2.0618e-01 (2.0618e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.110 ( 0.128)	Data  0.001 ( 0.018)	Loss 1.6162e-01 (1.3764e-01)	Acc@1  93.75 ( 95.74)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.111 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.2512e-01 (1.3698e-01)	Acc@1  96.88 ( 96.06)	Acc@5 100.00 ( 99.89)
Epoch: [33][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.0679e-01 (1.3235e-01)	Acc@1  92.19 ( 96.14)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 40/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.4783e-01 (1.3044e-01)	Acc@1  95.31 ( 96.13)	Acc@5  98.44 ( 99.89)
Epoch: [33][ 50/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.0358e-01 (1.2873e-01)	Acc@1  96.88 ( 96.14)	Acc@5 100.00 ( 99.89)
Epoch: [33][ 60/391]	Time  0.116 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.4294e-01 (1.3176e-01)	Acc@1  94.53 ( 95.95)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.7993e-01 (1.3230e-01)	Acc@1  92.19 ( 95.91)	Acc@5 100.00 ( 99.89)
Epoch: [33][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.3496e-02 (1.3003e-01)	Acc@1  98.44 ( 95.97)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 90/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.8857e-02 (1.2904e-01)	Acc@1 100.00 ( 96.03)	Acc@5 100.00 ( 99.91)
Epoch: [33][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6089e-01 (1.3007e-01)	Acc@1  96.88 ( 96.06)	Acc@5 100.00 ( 99.91)
Epoch: [33][110/391]	Time  0.117 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3428e-01 (1.2926e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 ( 99.92)
Epoch: [33][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3477e-01 (1.2935e-01)	Acc@1  96.88 ( 96.06)	Acc@5  99.22 ( 99.90)
Epoch: [33][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7603e-01 (1.2981e-01)	Acc@1  92.97 ( 96.07)	Acc@5 100.00 ( 99.91)
Epoch: [33][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0400e-01 (1.3102e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 ( 99.91)
Epoch: [33][150/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2927e-01 (1.3144e-01)	Acc@1  97.66 ( 96.00)	Acc@5 100.00 ( 99.90)
Epoch: [33][160/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1694e-01 (1.3171e-01)	Acc@1  96.88 ( 95.99)	Acc@5 100.00 ( 99.90)
Epoch: [33][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6309e-01 (1.3247e-01)	Acc@1  95.31 ( 95.94)	Acc@5 100.00 ( 99.90)
Epoch: [33][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6846e-01 (1.3272e-01)	Acc@1  96.09 ( 95.93)	Acc@5 100.00 ( 99.90)
Epoch: [33][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9238e-01 (1.3336e-01)	Acc@1  95.31 ( 95.92)	Acc@5  99.22 ( 99.90)
Epoch: [33][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7163e-01 (1.3426e-01)	Acc@1  97.66 ( 95.90)	Acc@5 100.00 ( 99.90)
Epoch: [33][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4282e-01 (1.3390e-01)	Acc@1  95.31 ( 95.88)	Acc@5 100.00 ( 99.90)
Epoch: [33][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4246e-01 (1.3453e-01)	Acc@1  94.53 ( 95.84)	Acc@5 100.00 ( 99.90)
Epoch: [33][230/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4233e-01 (1.3458e-01)	Acc@1  94.53 ( 95.84)	Acc@5 100.00 ( 99.89)
Epoch: [33][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1829e-01 (1.3460e-01)	Acc@1  96.88 ( 95.86)	Acc@5 100.00 ( 99.88)
Epoch: [33][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.3567e-02 (1.3472e-01)	Acc@1  97.66 ( 95.84)	Acc@5 100.00 ( 99.89)
Epoch: [33][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0419e-01 (1.3461e-01)	Acc@1  97.66 ( 95.84)	Acc@5 100.00 ( 99.89)
Epoch: [33][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3623e-01 (1.3456e-01)	Acc@1  96.88 ( 95.83)	Acc@5 100.00 ( 99.89)
Epoch: [33][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4392e-01 (1.3519e-01)	Acc@1  92.97 ( 95.82)	Acc@5 100.00 ( 99.89)
Epoch: [33][290/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0638e-01 (1.3464e-01)	Acc@1  97.66 ( 95.84)	Acc@5 100.00 ( 99.89)
Epoch: [33][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7273e-01 (1.3538e-01)	Acc@1  95.31 ( 95.81)	Acc@5  99.22 ( 99.88)
Epoch: [33][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.8613e-02 (1.3535e-01)	Acc@1  98.44 ( 95.82)	Acc@5 100.00 ( 99.88)
Epoch: [33][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3025e-01 (1.3571e-01)	Acc@1  96.09 ( 95.81)	Acc@5 100.00 ( 99.88)
Epoch: [33][330/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2720e-01 (1.3637e-01)	Acc@1  96.09 ( 95.79)	Acc@5 100.00 ( 99.88)
Epoch: [33][340/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0691e-01 (1.3645e-01)	Acc@1  93.75 ( 95.79)	Acc@5  99.22 ( 99.87)
Epoch: [33][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7148e-02 (1.3594e-01)	Acc@1  98.44 ( 95.82)	Acc@5 100.00 ( 99.88)
Epoch: [33][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0376e-01 (1.3631e-01)	Acc@1  96.88 ( 95.80)	Acc@5 100.00 ( 99.87)
Epoch: [33][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4709e-01 (1.3643e-01)	Acc@1  96.09 ( 95.79)	Acc@5 100.00 ( 99.87)
Epoch: [33][380/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0876e-01 (1.3626e-01)	Acc@1  96.88 ( 95.79)	Acc@5  99.22 ( 99.87)
Epoch: [33][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5610e-01 (1.3649e-01)	Acc@1  91.25 ( 95.78)	Acc@5 100.00 ( 99.88)
## e[33] optimizer.zero_grad (sum) time: 0.2946176528930664
## e[33]       loss.backward (sum) time: 13.783196687698364
## e[33]      optimizer.step (sum) time: 2.7287726402282715
## epoch[33] training(only) time: 45.045257329940796
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.1348e+00 (1.1348e+00)	Acc@1  77.00 ( 77.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.5234e+00 (1.3098e+00)	Acc@1  65.00 ( 74.00)	Acc@5  94.00 ( 92.82)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.3574e+00 (1.2516e+00)	Acc@1  72.00 ( 74.19)	Acc@5  91.00 ( 92.81)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 1.8750e+00 (1.3174e+00)	Acc@1  70.00 ( 73.03)	Acc@5  87.00 ( 92.16)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.2979e+00 (1.2931e+00)	Acc@1  78.00 ( 73.32)	Acc@5  95.00 ( 92.78)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.5840e+00 (1.3057e+00)	Acc@1  69.00 ( 72.86)	Acc@5  91.00 ( 92.45)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.4551e+00 (1.2639e+00)	Acc@1  69.00 ( 73.25)	Acc@5  93.00 ( 92.79)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.3115e+00 (1.2722e+00)	Acc@1  67.00 ( 73.11)	Acc@5  94.00 ( 92.79)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.4717e+00 (1.2724e+00)	Acc@1  71.00 ( 73.01)	Acc@5  92.00 ( 92.84)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 1.6953e+00 (1.2545e+00)	Acc@1  62.00 ( 73.00)	Acc@5  94.00 ( 92.96)
 * Acc@1 73.130 Acc@5 92.980
### epoch[33] execution time: 49.12859010696411
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.268 ( 0.268)	Data  0.162 ( 0.162)	Loss 9.7900e-02 (9.7900e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.111 ( 0.128)	Data  0.001 ( 0.018)	Loss 1.1609e-01 (1.1526e-01)	Acc@1  98.44 ( 96.66)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.114 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.5271e-01 (1.2064e-01)	Acc@1  96.09 ( 96.47)	Acc@5 100.00 (100.00)
Epoch: [34][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.1938e-01 (1.1960e-01)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 (100.00)
Epoch: [34][ 40/391]	Time  0.110 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.0162e-01 (1.1759e-01)	Acc@1  98.44 ( 96.59)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 50/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.5283e-01 (1.1920e-01)	Acc@1  94.53 ( 96.49)	Acc@5 100.00 ( 99.95)
Epoch: [34][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.2952e-01 (1.2181e-01)	Acc@1  96.09 ( 96.27)	Acc@5 100.00 ( 99.95)
Epoch: [34][ 70/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.0618e-02 (1.2115e-01)	Acc@1  98.44 ( 96.31)	Acc@5 100.00 ( 99.94)
Epoch: [34][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.9031e-02 (1.2021e-01)	Acc@1  98.44 ( 96.34)	Acc@5 100.00 ( 99.94)
Epoch: [34][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.3069e-02 (1.1903e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.95)
Epoch: [34][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6467e-01 (1.1797e-01)	Acc@1  95.31 ( 96.45)	Acc@5  99.22 ( 99.94)
Epoch: [34][110/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7261e-01 (1.1694e-01)	Acc@1  94.53 ( 96.50)	Acc@5  99.22 ( 99.93)
Epoch: [34][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3464e-01 (1.1849e-01)	Acc@1  96.09 ( 96.45)	Acc@5 100.00 ( 99.94)
Epoch: [34][130/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2305e-01 (1.1886e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.93)
Epoch: [34][140/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0547e-01 (1.1905e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.94)
Epoch: [34][150/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5918e-01 (1.1866e-01)	Acc@1  92.97 ( 96.39)	Acc@5 100.00 ( 99.94)
Epoch: [34][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.5337e-02 (1.1863e-01)	Acc@1  98.44 ( 96.39)	Acc@5 100.00 ( 99.93)
Epoch: [34][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4087e-01 (1.1865e-01)	Acc@1  92.97 ( 96.37)	Acc@5 100.00 ( 99.93)
Epoch: [34][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.6426e-02 (1.1804e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.93)
Epoch: [34][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3391e-01 (1.1812e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.93)
Epoch: [34][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4795e-01 (1.1866e-01)	Acc@1  94.53 ( 96.41)	Acc@5 100.00 ( 99.93)
Epoch: [34][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.9143e-02 (1.1783e-01)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7017e-01 (1.1863e-01)	Acc@1  96.09 ( 96.44)	Acc@5  99.22 ( 99.93)
Epoch: [34][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1157e-01 (1.1855e-01)	Acc@1  98.44 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4795e-01 (1.1896e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.93)
Epoch: [34][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1420e-01 (1.1891e-01)	Acc@1  96.09 ( 96.46)	Acc@5 100.00 ( 99.93)
Epoch: [34][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3672e-01 (1.1966e-01)	Acc@1  96.09 ( 96.44)	Acc@5 100.00 ( 99.92)
Epoch: [34][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.1177e-02 (1.1941e-01)	Acc@1  97.66 ( 96.43)	Acc@5 100.00 ( 99.92)
Epoch: [34][280/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7566e-01 (1.2015e-01)	Acc@1  94.53 ( 96.40)	Acc@5 100.00 ( 99.92)
Epoch: [34][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9651e-02 (1.1962e-01)	Acc@1  99.22 ( 96.42)	Acc@5 100.00 ( 99.92)
Epoch: [34][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2268e-01 (1.2048e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.92)
Epoch: [34][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3989e-01 (1.2012e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.92)
Epoch: [34][320/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.4861e-02 (1.2040e-01)	Acc@1  99.22 ( 96.39)	Acc@5 100.00 ( 99.92)
Epoch: [34][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1572e-01 (1.2072e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.92)
Epoch: [34][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.1248e-02 (1.2053e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.91)
Epoch: [34][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0168e-01 (1.2093e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.91)
Epoch: [34][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8628e-01 (1.2126e-01)	Acc@1  92.97 ( 96.38)	Acc@5  98.44 ( 99.91)
Epoch: [34][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7554e-01 (1.2126e-01)	Acc@1  94.53 ( 96.37)	Acc@5 100.00 ( 99.91)
Epoch: [34][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.2214e-02 (1.2112e-01)	Acc@1  98.44 ( 96.39)	Acc@5 100.00 ( 99.91)
Epoch: [34][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1194e-01 (1.2147e-01)	Acc@1  96.25 ( 96.38)	Acc@5 100.00 ( 99.91)
## e[34] optimizer.zero_grad (sum) time: 0.2902648448944092
## e[34]       loss.backward (sum) time: 13.77562928199768
## e[34]      optimizer.step (sum) time: 2.7284936904907227
## epoch[34] training(only) time: 45.026249408721924
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.1816e+00 (1.1816e+00)	Acc@1  74.00 ( 74.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.043 ( 0.054)	Loss 1.6357e+00 (1.3394e+00)	Acc@1  65.00 ( 73.82)	Acc@5  92.00 ( 92.64)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.4629e+00 (1.2863e+00)	Acc@1  73.00 ( 73.90)	Acc@5  91.00 ( 92.81)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.0195e+00 (1.3561e+00)	Acc@1  64.00 ( 72.68)	Acc@5  86.00 ( 92.19)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.3486e+00 (1.3267e+00)	Acc@1  73.00 ( 72.90)	Acc@5  94.00 ( 92.68)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.5967e+00 (1.3421e+00)	Acc@1  69.00 ( 72.45)	Acc@5  90.00 ( 92.29)
Test: [ 60/100]	Time  0.040 ( 0.042)	Loss 1.5303e+00 (1.3047e+00)	Acc@1  68.00 ( 72.75)	Acc@5  91.00 ( 92.51)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.3281e+00 (1.3145e+00)	Acc@1  71.00 ( 72.62)	Acc@5  93.00 ( 92.44)
Test: [ 80/100]	Time  0.040 ( 0.041)	Loss 1.5127e+00 (1.3139e+00)	Acc@1  71.00 ( 72.53)	Acc@5  93.00 ( 92.54)
Test: [ 90/100]	Time  0.037 ( 0.041)	Loss 1.7363e+00 (1.2934e+00)	Acc@1  63.00 ( 72.65)	Acc@5  93.00 ( 92.65)
 * Acc@1 72.750 Acc@5 92.700
### epoch[34] execution time: 49.13577365875244
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.275 ( 0.275)	Data  0.161 ( 0.161)	Loss 8.9966e-02 (8.9966e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.105 ( 0.127)	Data  0.001 ( 0.018)	Loss 1.0364e-01 (9.7906e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.113 ( 0.121)	Data  0.001 ( 0.011)	Loss 6.0516e-02 (9.7509e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 ( 99.96)
Epoch: [35][ 30/391]	Time  0.111 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.4038e-01 (1.0272e-01)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 7.4768e-02 (1.0485e-01)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.96)
Epoch: [35][ 50/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.007)	Loss 7.6599e-02 (1.0459e-01)	Acc@1  97.66 ( 96.69)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 60/391]	Time  0.110 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.7341e-02 (1.0393e-01)	Acc@1  98.44 ( 96.71)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 70/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.3477e-01 (1.0350e-01)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.9172e-02 (1.0238e-01)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.7383e-02 (1.0171e-01)	Acc@1  99.22 ( 96.84)	Acc@5 100.00 ( 99.98)
Epoch: [35][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.8145e-02 (1.0160e-01)	Acc@1  97.66 ( 96.83)	Acc@5 100.00 ( 99.98)
Epoch: [35][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0120e-01 (1.0200e-01)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.98)
Epoch: [35][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2695e-01 (1.0143e-01)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.98)
Epoch: [35][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0931e-01 (1.0239e-01)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.97)
Epoch: [35][140/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5256e-02 (1.0194e-01)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.97)
Epoch: [35][150/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2048e-01 (1.0381e-01)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 ( 99.96)
Epoch: [35][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1023e-01 (1.0429e-01)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.96)
Epoch: [35][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4526e-01 (1.0519e-01)	Acc@1  96.09 ( 96.70)	Acc@5  99.22 ( 99.95)
Epoch: [35][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0266e-01 (1.0491e-01)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.95)
Epoch: [35][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2793e-01 (1.0494e-01)	Acc@1  95.31 ( 96.74)	Acc@5 100.00 ( 99.95)
Epoch: [35][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0370e-01 (1.0430e-01)	Acc@1  95.31 ( 96.75)	Acc@5 100.00 ( 99.95)
Epoch: [35][210/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.6436e-02 (1.0470e-01)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.95)
Epoch: [35][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.7952e-02 (1.0473e-01)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 ( 99.95)
Epoch: [35][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.9651e-02 (1.0516e-01)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.95)
Epoch: [35][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1505e-01 (1.0492e-01)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.95)
Epoch: [35][250/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4648e-01 (1.0517e-01)	Acc@1  95.31 ( 96.72)	Acc@5 100.00 ( 99.96)
Epoch: [35][260/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5735e-01 (1.0574e-01)	Acc@1  94.53 ( 96.71)	Acc@5 100.00 ( 99.96)
Epoch: [35][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5796e-02 (1.0566e-01)	Acc@1  99.22 ( 96.71)	Acc@5 100.00 ( 99.95)
Epoch: [35][280/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.1064e-02 (1.0567e-01)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.95)
Epoch: [35][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4893e-01 (1.0645e-01)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.95)
Epoch: [35][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4856e-01 (1.0679e-01)	Acc@1  97.66 ( 96.69)	Acc@5  99.22 ( 99.95)
Epoch: [35][310/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.5144e-02 (1.0650e-01)	Acc@1  97.66 ( 96.71)	Acc@5 100.00 ( 99.95)
Epoch: [35][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0007e-02 (1.0658e-01)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.94)
Epoch: [35][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0370e-01 (1.0608e-01)	Acc@1  94.53 ( 96.73)	Acc@5 100.00 ( 99.94)
Epoch: [35][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1261e-01 (1.0664e-01)	Acc@1  94.53 ( 96.71)	Acc@5 100.00 ( 99.94)
Epoch: [35][350/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0022e-01 (1.0704e-01)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.94)
Epoch: [35][360/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4319e-01 (1.0674e-01)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.94)
Epoch: [35][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0394e-01 (1.0641e-01)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.94)
Epoch: [35][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.9875e-02 (1.0676e-01)	Acc@1  99.22 ( 96.71)	Acc@5 100.00 ( 99.94)
Epoch: [35][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0950e-01 (1.0701e-01)	Acc@1  98.75 ( 96.70)	Acc@5 100.00 ( 99.94)
## e[35] optimizer.zero_grad (sum) time: 0.29435205459594727
## e[35]       loss.backward (sum) time: 13.763988971710205
## e[35]      optimizer.step (sum) time: 2.749680995941162
## epoch[35] training(only) time: 45.014873027801514
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1943e+00 (1.1943e+00)	Acc@1  70.00 ( 70.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.5420e+00 (1.4110e+00)	Acc@1  64.00 ( 72.73)	Acc@5  93.00 ( 92.64)
Test: [ 20/100]	Time  0.041 ( 0.047)	Loss 1.5146e+00 (1.3475e+00)	Acc@1  72.00 ( 73.52)	Acc@5  91.00 ( 92.86)
Test: [ 30/100]	Time  0.038 ( 0.045)	Loss 2.1133e+00 (1.4170e+00)	Acc@1  64.00 ( 72.39)	Acc@5  86.00 ( 92.16)
Test: [ 40/100]	Time  0.042 ( 0.043)	Loss 1.4219e+00 (1.3907e+00)	Acc@1  72.00 ( 72.56)	Acc@5  96.00 ( 92.78)
Test: [ 50/100]	Time  0.040 ( 0.043)	Loss 1.6260e+00 (1.3977e+00)	Acc@1  68.00 ( 72.16)	Acc@5  91.00 ( 92.39)
Test: [ 60/100]	Time  0.040 ( 0.042)	Loss 1.5547e+00 (1.3573e+00)	Acc@1  69.00 ( 72.54)	Acc@5  91.00 ( 92.62)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.4053e+00 (1.3601e+00)	Acc@1  69.00 ( 72.55)	Acc@5  94.00 ( 92.70)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.5625e+00 (1.3602e+00)	Acc@1  70.00 ( 72.38)	Acc@5  93.00 ( 92.75)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 1.8477e+00 (1.3403e+00)	Acc@1  61.00 ( 72.45)	Acc@5  94.00 ( 92.84)
 * Acc@1 72.470 Acc@5 92.900
### epoch[35] execution time: 49.1572585105896
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.274 ( 0.274)	Data  0.170 ( 0.170)	Loss 7.4036e-02 (7.4036e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.112 ( 0.129)	Data  0.001 ( 0.019)	Loss 1.1456e-01 (8.5241e-02)	Acc@1  96.09 ( 97.59)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.012)	Loss 8.6731e-02 (8.9278e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 30/391]	Time  0.115 ( 0.119)	Data  0.001 ( 0.009)	Loss 6.9946e-02 (9.4434e-02)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.92)
Epoch: [36][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 8.1726e-02 (9.3236e-02)	Acc@1  97.66 ( 97.22)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 9.9548e-02 (9.5855e-02)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 60/391]	Time  0.110 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.1511e-01 (9.5007e-02)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 70/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.3314e-02 (9.2684e-02)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [36][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.0382e-01 (9.2426e-02)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [36][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 9.7595e-02 (9.0737e-02)	Acc@1  94.53 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [36][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.6853e-02 (9.1738e-02)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [36][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0553e-01 (9.2182e-02)	Acc@1  96.09 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [36][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.9670e-02 (9.2681e-02)	Acc@1  96.88 ( 97.21)	Acc@5  99.22 ( 99.94)
Epoch: [36][130/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0077e-01 (9.1814e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.94)
Epoch: [36][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.1543e-02 (9.2607e-02)	Acc@1  96.88 ( 97.26)	Acc@5 100.00 ( 99.94)
Epoch: [36][150/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.8838e-02 (9.2013e-02)	Acc@1  99.22 ( 97.31)	Acc@5 100.00 ( 99.95)
Epoch: [36][160/391]	Time  0.110 ( 0.115)	Data  0.002 ( 0.005)	Loss 1.4844e-01 (9.3395e-02)	Acc@1  96.09 ( 97.27)	Acc@5  99.22 ( 99.94)
Epoch: [36][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.8379e-02 (9.3044e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [36][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.6956e-02 (9.3585e-02)	Acc@1  99.22 ( 97.26)	Acc@5 100.00 ( 99.94)
Epoch: [36][190/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3611e-01 (9.3618e-02)	Acc@1  96.09 ( 97.27)	Acc@5  99.22 ( 99.94)
Epoch: [36][200/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5195e-02 (9.3133e-02)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.93)
Epoch: [36][210/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.5815e-02 (9.2550e-02)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.94)
Epoch: [36][220/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3196e-01 (9.1840e-02)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.94)
Epoch: [36][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3501e-01 (9.1784e-02)	Acc@1  93.75 ( 97.33)	Acc@5 100.00 ( 99.94)
Epoch: [36][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.6650e-02 (9.1732e-02)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.94)
Epoch: [36][250/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.2397e-02 (9.1690e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.94)
Epoch: [36][260/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.1829e-02 (9.1870e-02)	Acc@1  99.22 ( 97.32)	Acc@5 100.00 ( 99.94)
Epoch: [36][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1591e-01 (9.2185e-02)	Acc@1  96.88 ( 97.31)	Acc@5 100.00 ( 99.94)
Epoch: [36][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0339e-01 (9.2108e-02)	Acc@1  95.31 ( 97.31)	Acc@5 100.00 ( 99.94)
Epoch: [36][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4849e-02 (9.2212e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.94)
Epoch: [36][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.5876e-02 (9.2090e-02)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.94)
Epoch: [36][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4294e-01 (9.2176e-02)	Acc@1  96.88 ( 97.29)	Acc@5  99.22 ( 99.94)
Epoch: [36][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.8624e-02 (9.2485e-02)	Acc@1  98.44 ( 97.27)	Acc@5 100.00 ( 99.94)
Epoch: [36][330/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0557e-02 (9.2848e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.93)
Epoch: [36][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2854e-01 (9.3119e-02)	Acc@1  96.09 ( 97.26)	Acc@5 100.00 ( 99.94)
Epoch: [36][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.1492e-02 (9.2994e-02)	Acc@1  96.88 ( 97.26)	Acc@5 100.00 ( 99.93)
Epoch: [36][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.6619e-02 (9.3403e-02)	Acc@1  96.09 ( 97.24)	Acc@5 100.00 ( 99.93)
Epoch: [36][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.8511e-02 (9.3635e-02)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.93)
Epoch: [36][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9041e-02 (9.3529e-02)	Acc@1  96.09 ( 97.21)	Acc@5 100.00 ( 99.93)
Epoch: [36][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4233e-01 (9.3527e-02)	Acc@1  95.00 ( 97.21)	Acc@5 100.00 ( 99.93)
## e[36] optimizer.zero_grad (sum) time: 0.29151248931884766
## e[36]       loss.backward (sum) time: 13.755142211914062
## e[36]      optimizer.step (sum) time: 2.6957836151123047
## epoch[36] training(only) time: 45.055105209350586
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.2031e+00 (1.2031e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.6787e+00 (1.4445e+00)	Acc@1  63.00 ( 72.09)	Acc@5  94.00 ( 92.18)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.5156e+00 (1.3815e+00)	Acc@1  70.00 ( 72.95)	Acc@5  92.00 ( 92.33)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.0645e+00 (1.4433e+00)	Acc@1  60.00 ( 72.19)	Acc@5  88.00 ( 91.97)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.4229e+00 (1.4082e+00)	Acc@1  75.00 ( 72.54)	Acc@5  96.00 ( 92.61)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.7070e+00 (1.4146e+00)	Acc@1  68.00 ( 72.47)	Acc@5  91.00 ( 92.31)
Test: [ 60/100]	Time  0.040 ( 0.041)	Loss 1.6084e+00 (1.3715e+00)	Acc@1  70.00 ( 72.89)	Acc@5  93.00 ( 92.59)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.4678e+00 (1.3765e+00)	Acc@1  69.00 ( 72.92)	Acc@5  92.00 ( 92.54)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.5420e+00 (1.3763e+00)	Acc@1  71.00 ( 72.89)	Acc@5  93.00 ( 92.65)
Test: [ 90/100]	Time  0.046 ( 0.041)	Loss 1.9756e+00 (1.3555e+00)	Acc@1  59.00 ( 72.95)	Acc@5  93.00 ( 92.74)
 * Acc@1 72.960 Acc@5 92.750
### epoch[36] execution time: 49.17822861671448
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.265 ( 0.265)	Data  0.163 ( 0.163)	Loss 1.2646e-01 (1.2646e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.111 ( 0.128)	Data  0.001 ( 0.018)	Loss 7.3059e-02 (7.7667e-02)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 ( 99.93)
Epoch: [37][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.0504e-01 (7.9356e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 8.8623e-02 (7.7447e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 40/391]	Time  0.116 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.0339e-01 (7.6840e-02)	Acc@1  96.88 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.2756e-01 (7.9121e-02)	Acc@1  97.66 ( 97.89)	Acc@5  99.22 ( 99.97)
Epoch: [37][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 5.6458e-02 (7.8124e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 70/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.4961e-02 (7.6937e-02)	Acc@1  96.09 ( 97.85)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.3303e-02 (7.7322e-02)	Acc@1  97.66 ( 97.83)	Acc@5  99.22 ( 99.97)
Epoch: [37][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.2213e-01 (7.7799e-02)	Acc@1  96.09 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [37][100/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.8075e-02 (7.8036e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.98)
Epoch: [37][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.1003e-02 (7.8446e-02)	Acc@1  96.09 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [37][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.7913e-02 (7.8458e-02)	Acc@1  99.22 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [37][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.2468e-02 (7.9089e-02)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 ( 99.95)
Epoch: [37][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.1310e-02 (7.9747e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 ( 99.96)
Epoch: [37][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7737e-01 (8.0883e-02)	Acc@1  93.75 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [37][160/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.9854e-02 (8.1474e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.95)
Epoch: [37][170/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.3875e-02 (8.0836e-02)	Acc@1 100.00 ( 97.63)	Acc@5 100.00 ( 99.95)
Epoch: [37][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.3101e-02 (8.1011e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [37][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.8542e-02 (8.0661e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.96)
Epoch: [37][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.7708e-02 (8.1281e-02)	Acc@1  96.09 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [37][210/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.5298e-02 (8.1294e-02)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.95)
Epoch: [37][220/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.1849e-02 (8.1759e-02)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.95)
Epoch: [37][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4453e-01 (8.2548e-02)	Acc@1  94.53 ( 97.52)	Acc@5 100.00 ( 99.95)
Epoch: [37][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.1960e-02 (8.2753e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 ( 99.95)
Epoch: [37][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.7351e-02 (8.3077e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 ( 99.95)
Epoch: [37][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.5449e-02 (8.2582e-02)	Acc@1  96.09 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [37][270/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.1239e-02 (8.2366e-02)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [37][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.5500e-02 (8.2020e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [37][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.3008e-02 (8.2301e-02)	Acc@1  96.09 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [37][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1237e-01 (8.2267e-02)	Acc@1  95.31 ( 97.52)	Acc@5  99.22 ( 99.95)
Epoch: [37][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2659e-01 (8.2526e-02)	Acc@1  95.31 ( 97.51)	Acc@5  99.22 ( 99.95)
Epoch: [37][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8115e-02 (8.2724e-02)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.95)
Epoch: [37][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.2469e-02 (8.3047e-02)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.95)
Epoch: [37][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6956e-02 (8.3136e-02)	Acc@1  98.44 ( 97.47)	Acc@5 100.00 ( 99.95)
Epoch: [37][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3013e-01 (8.3896e-02)	Acc@1  93.75 ( 97.43)	Acc@5 100.00 ( 99.94)
Epoch: [37][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5674e-02 (8.3818e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.95)
Epoch: [37][370/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4360e-02 (8.4257e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.94)
Epoch: [37][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0374e-02 (8.4423e-02)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.94)
Epoch: [37][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8713e-01 (8.4978e-02)	Acc@1  92.50 ( 97.38)	Acc@5 100.00 ( 99.95)
## e[37] optimizer.zero_grad (sum) time: 0.28650736808776855
## e[37]       loss.backward (sum) time: 13.764800310134888
## e[37]      optimizer.step (sum) time: 2.719978094100952
## epoch[37] training(only) time: 45.05054998397827
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.3232e+00 (1.3232e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.041 ( 0.054)	Loss 1.7158e+00 (1.4468e+00)	Acc@1  63.00 ( 72.36)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.5557e+00 (1.3745e+00)	Acc@1  73.00 ( 73.86)	Acc@5  92.00 ( 93.05)
Test: [ 30/100]	Time  0.038 ( 0.045)	Loss 2.1016e+00 (1.4405e+00)	Acc@1  66.00 ( 73.19)	Acc@5  88.00 ( 92.42)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.3809e+00 (1.4129e+00)	Acc@1  76.00 ( 73.37)	Acc@5  95.00 ( 92.93)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.7812e+00 (1.4236e+00)	Acc@1  69.00 ( 73.06)	Acc@5  89.00 ( 92.59)
Test: [ 60/100]	Time  0.042 ( 0.042)	Loss 1.5146e+00 (1.3812e+00)	Acc@1  69.00 ( 73.21)	Acc@5  90.00 ( 92.70)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.5146e+00 (1.3900e+00)	Acc@1  66.00 ( 73.10)	Acc@5  93.00 ( 92.70)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.6504e+00 (1.3915e+00)	Acc@1  71.00 ( 73.07)	Acc@5  93.00 ( 92.73)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.9424e+00 (1.3694e+00)	Acc@1  62.00 ( 73.23)	Acc@5  93.00 ( 92.80)
 * Acc@1 73.270 Acc@5 92.850
### epoch[37] execution time: 49.1776282787323
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.279 ( 0.279)	Data  0.159 ( 0.159)	Loss 1.0529e-01 (1.0529e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.112 ( 0.128)	Data  0.001 ( 0.017)	Loss 7.7759e-02 (7.6427e-02)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 6.4087e-02 (8.0023e-02)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 30/391]	Time  0.113 ( 0.120)	Data  0.001 ( 0.009)	Loss 5.6854e-02 (7.3798e-02)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 40/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.008)	Loss 6.7444e-02 (6.9791e-02)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.2683e-01 (7.0815e-02)	Acc@1  97.66 ( 97.79)	Acc@5  99.22 ( 99.94)
Epoch: [38][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.2153e-02 (7.2538e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 70/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.5430e-02 (7.1645e-02)	Acc@1  99.22 ( 97.73)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 80/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.0933e-02 (7.1492e-02)	Acc@1  96.88 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 90/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.8379e-02 (7.0877e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.97)
Epoch: [38][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1542e-01 (7.2555e-02)	Acc@1  96.88 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [38][110/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.0131e-02 (7.2927e-02)	Acc@1  98.44 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [38][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.2990e-02 (7.1812e-02)	Acc@1 100.00 ( 97.80)	Acc@5 100.00 ( 99.96)
Epoch: [38][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.0933e-02 (7.1529e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.96)
Epoch: [38][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0685e-02 (7.1730e-02)	Acc@1 100.00 ( 97.78)	Acc@5 100.00 ( 99.97)
Epoch: [38][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.7026e-02 (7.1138e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [38][160/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.1716e-02 (7.1103e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [38][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2781e-01 (7.1075e-02)	Acc@1  96.09 ( 97.84)	Acc@5  99.22 ( 99.96)
Epoch: [38][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.5154e-02 (7.0841e-02)	Acc@1  96.88 ( 97.85)	Acc@5 100.00 ( 99.97)
Epoch: [38][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.2307e-02 (7.0758e-02)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.97)
Epoch: [38][200/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4027e-02 (7.0620e-02)	Acc@1 100.00 ( 97.87)	Acc@5 100.00 ( 99.97)
Epoch: [38][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.2205e-02 (7.0434e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.97)
Epoch: [38][220/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.2734e-02 (7.0795e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.97)
Epoch: [38][230/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.1188e-02 (7.0785e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.97)
Epoch: [38][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.3549e-02 (7.0772e-02)	Acc@1  99.22 ( 97.89)	Acc@5 100.00 ( 99.97)
Epoch: [38][250/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0791e-02 (7.0907e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 ( 99.98)
Epoch: [38][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.9276e-02 (7.1067e-02)	Acc@1 100.00 ( 97.87)	Acc@5 100.00 ( 99.98)
Epoch: [38][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.9153e-02 (7.1223e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 ( 99.97)
Epoch: [38][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.2673e-02 (7.1456e-02)	Acc@1  99.22 ( 97.86)	Acc@5 100.00 ( 99.97)
Epoch: [38][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.9326e-02 (7.1903e-02)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 ( 99.98)
Epoch: [38][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0618e-02 (7.2317e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 ( 99.98)
Epoch: [38][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.8389e-02 (7.2699e-02)	Acc@1  96.88 ( 97.85)	Acc@5 100.00 ( 99.98)
Epoch: [38][320/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.7566e-02 (7.3185e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.98)
Epoch: [38][330/391]	Time  0.118 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.3242e-02 (7.3245e-02)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 ( 99.98)
Epoch: [38][340/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.0261e-02 (7.3630e-02)	Acc@1  97.66 ( 97.84)	Acc@5 100.00 ( 99.98)
Epoch: [38][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.7463e-02 (7.4064e-02)	Acc@1  97.66 ( 97.83)	Acc@5 100.00 ( 99.98)
Epoch: [38][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8298e-02 (7.3828e-02)	Acc@1  96.88 ( 97.83)	Acc@5 100.00 ( 99.98)
Epoch: [38][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.5012e-02 (7.3841e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.98)
Epoch: [38][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0248e-01 (7.3719e-02)	Acc@1  96.88 ( 97.83)	Acc@5 100.00 ( 99.98)
Epoch: [38][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2937e-01 (7.4368e-02)	Acc@1  93.75 ( 97.83)	Acc@5 100.00 ( 99.98)
## e[38] optimizer.zero_grad (sum) time: 0.2887904644012451
## e[38]       loss.backward (sum) time: 13.769803762435913
## e[38]      optimizer.step (sum) time: 2.71596097946167
## epoch[38] training(only) time: 45.0809600353241
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.2432e+00 (1.2432e+00)	Acc@1  76.00 ( 76.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.040 ( 0.053)	Loss 1.5967e+00 (1.5009e+00)	Acc@1  65.00 ( 72.36)	Acc@5  93.00 ( 92.09)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.7051e+00 (1.4194e+00)	Acc@1  72.00 ( 73.10)	Acc@5  92.00 ( 92.76)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.1387e+00 (1.4826e+00)	Acc@1  66.00 ( 72.74)	Acc@5  87.00 ( 92.42)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.4004e+00 (1.4497e+00)	Acc@1  74.00 ( 73.00)	Acc@5  95.00 ( 92.76)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.8389e+00 (1.4650e+00)	Acc@1  68.00 ( 72.92)	Acc@5  89.00 ( 92.43)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.6299e+00 (1.4229e+00)	Acc@1  69.00 ( 73.18)	Acc@5  92.00 ( 92.66)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 1.4473e+00 (1.4308e+00)	Acc@1  66.00 ( 72.93)	Acc@5  94.00 ( 92.66)
Test: [ 80/100]	Time  0.040 ( 0.041)	Loss 1.6348e+00 (1.4341e+00)	Acc@1  70.00 ( 72.86)	Acc@5  94.00 ( 92.67)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.0254e+00 (1.4085e+00)	Acc@1  62.00 ( 72.99)	Acc@5  93.00 ( 92.82)
 * Acc@1 73.030 Acc@5 92.860
### epoch[38] execution time: 49.219571113586426
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.277 ( 0.277)	Data  0.161 ( 0.161)	Loss 8.1604e-02 (8.1604e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.110 ( 0.128)	Data  0.001 ( 0.018)	Loss 4.4891e-02 (6.9584e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.114 ( 0.121)	Data  0.001 ( 0.011)	Loss 5.4169e-02 (6.9261e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 7.8857e-02 (6.2033e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 8.9844e-02 (6.4552e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 6.2866e-02 (6.5204e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 (100.00)
Epoch: [39][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.9885e-02 (6.2936e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [39][ 70/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.5431e-02 (6.2318e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 (100.00)
Epoch: [39][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.1726e-02 (6.2147e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 (100.00)
Epoch: [39][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.3792e-02 (6.3026e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [39][100/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.9102e-02 (6.2809e-02)	Acc@1  97.66 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [39][110/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.1370e-02 (6.3609e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [39][120/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.4951e-02 (6.4367e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [39][130/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.5674e-02 (6.4538e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [39][140/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.6793e-02 (6.4969e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [39][150/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.2744e-02 (6.4627e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [39][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.1188e-02 (6.5525e-02)	Acc@1  99.22 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [39][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.0576e-02 (6.5741e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [39][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.5573e-02 (6.6012e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [39][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.7627e-02 (6.5978e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [39][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.5227e-02 (6.6251e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [39][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5378e-02 (6.6149e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [39][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.7434e-02 (6.5760e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [39][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.4291e-02 (6.5449e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [39][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.6284e-02 (6.5330e-02)	Acc@1  96.09 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [39][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2476e-01 (6.6147e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [39][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9316e-02 (6.6323e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [39][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2524e-01 (6.6506e-02)	Acc@1  96.88 ( 98.13)	Acc@5  99.22 ( 99.98)
Epoch: [39][280/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.6621e-02 (6.6280e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [39][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.5205e-02 (6.6699e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [39][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.1462e-02 (6.6839e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [39][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7515e-02 (6.6791e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [39][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2815e-02 (6.6901e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [39][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.1736e-02 (6.7395e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [39][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.5797e-02 (6.7439e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.97)
Epoch: [39][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.3232e-02 (6.7092e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [39][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.1797e-02 (6.6970e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [39][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.7656e-02 (6.7250e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [39][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.5938e-02 (6.7808e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [39][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.7769e-02 (6.8266e-02)	Acc@1  97.50 ( 98.04)	Acc@5 100.00 ( 99.97)
## e[39] optimizer.zero_grad (sum) time: 0.29581117630004883
## e[39]       loss.backward (sum) time: 13.766985654830933
## e[39]      optimizer.step (sum) time: 2.7211339473724365
## epoch[39] training(only) time: 44.947669982910156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 1.1641e+00 (1.1641e+00)	Acc@1  76.00 ( 76.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.038 ( 0.051)	Loss 1.6904e+00 (1.4614e+00)	Acc@1  67.00 ( 73.09)	Acc@5  92.00 ( 92.82)
Test: [ 20/100]	Time  0.037 ( 0.045)	Loss 1.7891e+00 (1.4192e+00)	Acc@1  72.00 ( 73.71)	Acc@5  91.00 ( 92.86)
Test: [ 30/100]	Time  0.038 ( 0.043)	Loss 2.3262e+00 (1.4914e+00)	Acc@1  60.00 ( 72.81)	Acc@5  87.00 ( 92.39)
Test: [ 40/100]	Time  0.038 ( 0.042)	Loss 1.5420e+00 (1.4605e+00)	Acc@1  73.00 ( 73.02)	Acc@5  94.00 ( 92.78)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 1.8398e+00 (1.4741e+00)	Acc@1  67.00 ( 72.88)	Acc@5  91.00 ( 92.49)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.6025e+00 (1.4279e+00)	Acc@1  69.00 ( 73.16)	Acc@5  93.00 ( 92.77)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 1.5117e+00 (1.4422e+00)	Acc@1  69.00 ( 73.06)	Acc@5  93.00 ( 92.80)
Test: [ 80/100]	Time  0.039 ( 0.040)	Loss 1.7539e+00 (1.4436e+00)	Acc@1  68.00 ( 72.98)	Acc@5  94.00 ( 92.84)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 1.9775e+00 (1.4259e+00)	Acc@1  63.00 ( 73.11)	Acc@5  92.00 ( 92.85)
 * Acc@1 73.090 Acc@5 92.880
### epoch[39] execution time: 49.03227639198303
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.264 ( 0.264)	Data  0.159 ( 0.159)	Loss 7.8735e-02 (7.8735e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.114 ( 0.127)	Data  0.001 ( 0.018)	Loss 5.9143e-02 (5.7681e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.93)
Epoch: [40][ 20/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.011)	Loss 8.1909e-02 (5.9292e-02)	Acc@1  97.66 ( 98.55)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.111 ( 0.119)	Data  0.001 ( 0.009)	Loss 6.5002e-02 (5.9312e-02)	Acc@1  96.88 ( 98.39)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 7.8064e-02 (6.0704e-02)	Acc@1  96.09 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 8.6182e-02 (6.0711e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 5.1514e-02 (6.2361e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 70/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.0750e-02 (6.3423e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.2572e-02 (6.3176e-02)	Acc@1 100.00 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.0114e-01 (6.2995e-02)	Acc@1  96.09 ( 98.21)	Acc@5 100.00 ( 99.97)
Epoch: [40][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.6406e-02 (6.3660e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [40][110/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.0942e-02 (6.4250e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [40][120/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.0679e-02 (6.3193e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [40][130/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.0720e-02 (6.3059e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [40][140/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.1544e-02 (6.3336e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [40][150/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.3984e-02 (6.3861e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [40][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.7272e-02 (6.3315e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [40][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.0385e-02 (6.3219e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.97)
Epoch: [40][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.0659e-02 (6.2850e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [40][190/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.1768e-02 (6.2754e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [40][200/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.0161e-02 (6.2217e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [40][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.6101e-02 (6.1869e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [40][220/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.8918e-02 (6.1507e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [40][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.6060e-02 (6.2092e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [40][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.2998e-02 (6.2178e-02)	Acc@1  96.88 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [40][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2665e-02 (6.2166e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [40][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5063e-02 (6.2523e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [40][270/391]	Time  0.119 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1755e-01 (6.3324e-02)	Acc@1  95.31 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [40][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.1788e-02 (6.3690e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [40][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0913e-02 (6.3276e-02)	Acc@1  96.88 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [40][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7130e-02 (6.3668e-02)	Acc@1 100.00 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [40][310/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.3741e-02 (6.3297e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [40][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3098e-01 (6.3468e-02)	Acc@1  95.31 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [40][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.7261e-02 (6.3416e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [40][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8625e-02 (6.3103e-02)	Acc@1 100.00 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [40][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.2795e-02 (6.3000e-02)	Acc@1 100.00 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [40][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6915e-02 (6.2931e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [40][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.1716e-02 (6.3070e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [40][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.7119e-02 (6.3556e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [40][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.5237e-02 (6.3515e-02)	Acc@1  98.75 ( 98.22)	Acc@5 100.00 ( 99.98)
## e[40] optimizer.zero_grad (sum) time: 0.28867673873901367
## e[40]       loss.backward (sum) time: 13.774804592132568
## e[40]      optimizer.step (sum) time: 2.6719045639038086
## epoch[40] training(only) time: 45.040074825286865
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.1602e+00 (1.1602e+00)	Acc@1  78.00 ( 78.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.6514e+00 (1.4568e+00)	Acc@1  68.00 ( 73.64)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.6260e+00 (1.4138e+00)	Acc@1  72.00 ( 73.71)	Acc@5  92.00 ( 92.71)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.0684e+00 (1.4715e+00)	Acc@1  63.00 ( 72.84)	Acc@5  88.00 ( 92.26)
Test: [ 40/100]	Time  0.039 ( 0.042)	Loss 1.5498e+00 (1.4402e+00)	Acc@1  75.00 ( 73.20)	Acc@5  93.00 ( 92.76)
Test: [ 50/100]	Time  0.037 ( 0.041)	Loss 1.7480e+00 (1.4629e+00)	Acc@1  70.00 ( 72.90)	Acc@5  90.00 ( 92.45)
Test: [ 60/100]	Time  0.040 ( 0.041)	Loss 1.7383e+00 (1.4311e+00)	Acc@1  70.00 ( 73.11)	Acc@5  91.00 ( 92.67)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.5459e+00 (1.4441e+00)	Acc@1  70.00 ( 72.89)	Acc@5  91.00 ( 92.62)
Test: [ 80/100]	Time  0.040 ( 0.041)	Loss 1.7783e+00 (1.4389e+00)	Acc@1  67.00 ( 73.00)	Acc@5  93.00 ( 92.69)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 2.0703e+00 (1.4160e+00)	Acc@1  62.00 ( 73.08)	Acc@5  94.00 ( 92.79)
 * Acc@1 73.100 Acc@5 92.830
### epoch[40] execution time: 49.1541543006897
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.268 ( 0.268)	Data  0.161 ( 0.161)	Loss 4.4678e-02 (4.4678e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.114 ( 0.128)	Data  0.001 ( 0.018)	Loss 4.6051e-02 (5.8541e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 4.8218e-02 (5.9333e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 9.4910e-02 (6.1747e-02)	Acc@1  96.09 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 8.7646e-02 (5.9803e-02)	Acc@1  96.09 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 3.7567e-02 (5.9345e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 60/391]	Time  0.107 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.7271e-02 (5.9153e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 ( 99.99)
Epoch: [41][ 70/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 5.0812e-02 (5.7298e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [41][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.8676e-02 (5.6369e-02)	Acc@1 100.00 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [41][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.8046e-02 (5.5282e-02)	Acc@1 100.00 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [41][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.2349e-02 (5.6094e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [41][110/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.6865e-02 (5.5947e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [41][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.4424e-02 (5.6202e-02)	Acc@1 100.00 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [41][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.3955e-02 (5.5298e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [41][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.7617e-02 (5.5487e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [41][150/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.7679e-02 (5.5582e-02)	Acc@1 100.00 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [41][160/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0651e-01 (5.6127e-02)	Acc@1  96.88 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [41][170/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.2957e-02 (5.6821e-02)	Acc@1  95.31 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [41][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.9307e-02 (5.6491e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [41][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.5767e-02 (5.7667e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [41][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.8604e-02 (5.7478e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [41][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.3854e-02 (5.7918e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [41][220/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.3140e-02 (5.7824e-02)	Acc@1  96.88 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [41][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.3945e-02 (5.7722e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [41][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.5522e-02 (5.7807e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [41][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.4504e-02 (5.7550e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [41][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4882e-02 (5.7560e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [41][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3026e-02 (5.7966e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [41][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0187e-01 (5.8090e-02)	Acc@1  97.66 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [41][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4966e-01 (5.8719e-02)	Acc@1  96.09 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [41][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.1859e-02 (5.8789e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [41][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0577e-02 (5.8952e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [41][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6367e-02 (5.8609e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [41][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.2664e-02 (5.8387e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [41][340/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.8086e-02 (5.8272e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [41][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5979e-02 (5.8392e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [41][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.7831e-02 (5.9105e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [41][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.6417e-02 (5.9076e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [41][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.0232e-02 (5.9161e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [41][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.9854e-02 (5.9444e-02)	Acc@1  96.25 ( 98.38)	Acc@5 100.00 ( 99.98)
## e[41] optimizer.zero_grad (sum) time: 0.2903883457183838
## e[41]       loss.backward (sum) time: 13.765821933746338
## e[41]      optimizer.step (sum) time: 2.704092025756836
## epoch[41] training(only) time: 45.081175804138184
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.2021e+00 (1.2021e+00)	Acc@1  73.00 ( 73.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.054)	Loss 1.6221e+00 (1.4925e+00)	Acc@1  68.00 ( 72.82)	Acc@5  92.00 ( 92.55)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.6279e+00 (1.4235e+00)	Acc@1  72.00 ( 73.57)	Acc@5  93.00 ( 92.76)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.1250e+00 (1.5008e+00)	Acc@1  67.00 ( 72.81)	Acc@5  90.00 ( 92.52)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.6582e+00 (1.4762e+00)	Acc@1  72.00 ( 72.95)	Acc@5  94.00 ( 93.00)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.7949e+00 (1.4938e+00)	Acc@1  69.00 ( 72.73)	Acc@5  90.00 ( 92.57)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7822e+00 (1.4578e+00)	Acc@1  67.00 ( 73.00)	Acc@5  90.00 ( 92.72)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.5488e+00 (1.4665e+00)	Acc@1  67.00 ( 72.97)	Acc@5  92.00 ( 92.70)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.7080e+00 (1.4607e+00)	Acc@1  69.00 ( 73.00)	Acc@5  95.00 ( 92.74)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.9824e+00 (1.4392e+00)	Acc@1  61.00 ( 73.03)	Acc@5  94.00 ( 92.79)
 * Acc@1 73.120 Acc@5 92.860
### epoch[41] execution time: 49.19938611984253
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.266 ( 0.266)	Data  0.151 ( 0.151)	Loss 6.3110e-02 (6.3110e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.115 ( 0.128)	Data  0.001 ( 0.017)	Loss 4.4403e-02 (5.1630e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.010)	Loss 3.2623e-02 (4.9345e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.008)	Loss 3.8422e-02 (4.8034e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.007)	Loss 4.1718e-02 (5.1283e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 5.9509e-02 (5.2079e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.4412e-02 (5.1308e-02)	Acc@1  97.66 ( 98.73)	Acc@5  99.22 ( 99.97)
Epoch: [42][ 70/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.3884e-02 (5.1764e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.2999e-02 (5.0792e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.9734e-02 (5.1469e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.98)
Epoch: [42][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.3203e-02 (5.2145e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 ( 99.98)
Epoch: [42][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5076e-02 (5.1750e-02)	Acc@1 100.00 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [42][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.0680e-02 (5.1449e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [42][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.1005e-02 (5.1745e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [42][140/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.7281e-02 (5.2373e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [42][150/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1149e-02 (5.2326e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [42][160/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.1370e-02 (5.2424e-02)	Acc@1  96.88 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [42][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8961e-02 (5.2184e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [42][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.5918e-02 (5.2514e-02)	Acc@1  96.09 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [42][190/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.5034e-02 (5.2424e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [42][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4241e-02 (5.2363e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [42][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.3721e-02 (5.2254e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [42][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.4097e-02 (5.2209e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [42][230/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.8553e-02 (5.2633e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [42][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9957e-02 (5.3117e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [42][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.3253e-02 (5.3201e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [42][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.1544e-02 (5.3192e-02)	Acc@1  97.66 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [42][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.3232e-02 (5.3255e-02)	Acc@1  96.88 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [42][280/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.1168e-02 (5.3594e-02)	Acc@1  99.22 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [42][290/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.5073e-02 (5.3748e-02)	Acc@1  96.88 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [42][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.1412e-02 (5.3379e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [42][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.8116e-02 (5.3317e-02)	Acc@1  99.22 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [42][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.6375e-02 (5.3981e-02)	Acc@1  97.66 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [42][330/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3708e-01 (5.4224e-02)	Acc@1  97.66 ( 98.53)	Acc@5  99.22 ( 99.99)
Epoch: [42][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.8145e-02 (5.4346e-02)	Acc@1  96.09 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [42][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.5389e-02 (5.4076e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [42][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.9906e-02 (5.4392e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [42][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4008e-02 (5.4635e-02)	Acc@1 100.00 ( 98.53)	Acc@5 100.00 ( 99.99)
Epoch: [42][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8351e-02 (5.4504e-02)	Acc@1 100.00 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [42][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.3608e-02 (5.4379e-02)	Acc@1  97.50 ( 98.53)	Acc@5 100.00 ( 99.99)
## e[42] optimizer.zero_grad (sum) time: 0.2892582416534424
## e[42]       loss.backward (sum) time: 13.741986751556396
## e[42]      optimizer.step (sum) time: 2.67313814163208
## epoch[42] training(only) time: 45.06875991821289
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.1855e+00 (1.1855e+00)	Acc@1  76.00 ( 76.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.6289e+00 (1.5013e+00)	Acc@1  66.00 ( 72.64)	Acc@5  93.00 ( 92.91)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.5820e+00 (1.4344e+00)	Acc@1  72.00 ( 73.48)	Acc@5  93.00 ( 92.95)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.0938e+00 (1.4956e+00)	Acc@1  66.00 ( 73.13)	Acc@5  88.00 ( 92.42)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.6543e+00 (1.4706e+00)	Acc@1  71.00 ( 73.37)	Acc@5  94.00 ( 92.83)
Test: [ 50/100]	Time  0.040 ( 0.042)	Loss 1.8271e+00 (1.4851e+00)	Acc@1  70.00 ( 73.20)	Acc@5  91.00 ( 92.65)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.6436e+00 (1.4518e+00)	Acc@1  68.00 ( 73.48)	Acc@5  93.00 ( 92.79)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 1.5000e+00 (1.4640e+00)	Acc@1  65.00 ( 73.38)	Acc@5  91.00 ( 92.69)
Test: [ 80/100]	Time  0.037 ( 0.041)	Loss 1.7861e+00 (1.4659e+00)	Acc@1  70.00 ( 73.36)	Acc@5  91.00 ( 92.69)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.0176e+00 (1.4471e+00)	Acc@1  63.00 ( 73.46)	Acc@5  94.00 ( 92.75)
 * Acc@1 73.620 Acc@5 92.800
### epoch[42] execution time: 49.19061088562012
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.266 ( 0.266)	Data  0.161 ( 0.161)	Loss 3.8147e-02 (3.8147e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.111 ( 0.128)	Data  0.001 ( 0.018)	Loss 2.6901e-02 (5.5282e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 3.0167e-02 (4.7657e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 30/391]	Time  0.113 ( 0.120)	Data  0.001 ( 0.009)	Loss 6.8420e-02 (4.7641e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 6.9031e-02 (4.9397e-02)	Acc@1  96.88 ( 98.61)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 50/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.007)	Loss 4.3732e-02 (4.8723e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 60/391]	Time  0.106 ( 0.117)	Data  0.001 ( 0.006)	Loss 4.6783e-02 (4.9211e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 4.5044e-02 (4.8458e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.1290e-02 (4.9136e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.7302e-02 (4.8188e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [43][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.7831e-02 (4.8935e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [43][110/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.6051e-02 (4.9675e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][120/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.8818e-02 (4.8763e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][130/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.1350e-02 (4.8668e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.2712e-02 (4.8829e-02)	Acc@1  96.88 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [43][150/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.3232e-02 (4.9218e-02)	Acc@1  97.66 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][160/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.5695e-02 (4.9207e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.3051e-02 (4.9437e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.6509e-02 (4.9739e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [43][190/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9730e-02 (5.0111e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [43][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.8645e-02 (5.0246e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [43][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.3884e-02 (5.0516e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [43][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.2206e-02 (5.0191e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [43][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.6896e-02 (5.0012e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [43][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4536e-02 (5.0012e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [43][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9882e-02 (4.9851e-02)	Acc@1 100.00 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.9590e-02 (4.9875e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [43][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0211e-02 (4.9862e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.4270e-02 (5.0072e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8237e-02 (5.0075e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9591e-02 (5.0000e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2720e-02 (4.9886e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [43][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3325e-02 (4.9895e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [43][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.2317e-02 (5.0289e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [43][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.6082e-02 (5.0680e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [43][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.2816e-02 (5.0896e-02)	Acc@1 100.00 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.4474e-02 (5.0795e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [43][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7685e-02 (5.0813e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [43][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4149e-02 (5.0835e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [43][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6028e-01 (5.1180e-02)	Acc@1  93.75 ( 98.58)	Acc@5 100.00 ( 99.99)
## e[43] optimizer.zero_grad (sum) time: 0.29247522354125977
## e[43]       loss.backward (sum) time: 13.783267974853516
## e[43]      optimizer.step (sum) time: 2.7370753288269043
## epoch[43] training(only) time: 45.04959034919739
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.2686e+00 (1.2686e+00)	Acc@1  78.00 ( 78.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.040 ( 0.052)	Loss 1.6426e+00 (1.5409e+00)	Acc@1  65.00 ( 72.82)	Acc@5  93.00 ( 92.91)
Test: [ 20/100]	Time  0.037 ( 0.046)	Loss 1.6455e+00 (1.4914e+00)	Acc@1  72.00 ( 73.29)	Acc@5  94.00 ( 93.00)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.4570e+00 (1.5569e+00)	Acc@1  65.00 ( 72.61)	Acc@5  87.00 ( 92.42)
Test: [ 40/100]	Time  0.039 ( 0.042)	Loss 1.5879e+00 (1.5297e+00)	Acc@1  74.00 ( 73.02)	Acc@5  95.00 ( 92.83)
Test: [ 50/100]	Time  0.039 ( 0.041)	Loss 1.8828e+00 (1.5356e+00)	Acc@1  68.00 ( 72.84)	Acc@5  90.00 ( 92.57)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.6699e+00 (1.4980e+00)	Acc@1  68.00 ( 73.11)	Acc@5  93.00 ( 92.70)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.6152e+00 (1.5080e+00)	Acc@1  66.00 ( 73.08)	Acc@5  93.00 ( 92.70)
Test: [ 80/100]	Time  0.039 ( 0.040)	Loss 1.8105e+00 (1.5110e+00)	Acc@1  68.00 ( 73.04)	Acc@5  94.00 ( 92.78)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 2.0371e+00 (1.4860e+00)	Acc@1  65.00 ( 73.14)	Acc@5  93.00 ( 92.84)
 * Acc@1 73.230 Acc@5 92.870
### epoch[43] execution time: 49.124411821365356
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.269 ( 0.269)	Data  0.160 ( 0.160)	Loss 6.5918e-02 (6.5918e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.109 ( 0.128)	Data  0.001 ( 0.018)	Loss 7.2632e-02 (4.2970e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.011)	Loss 3.7109e-02 (3.9868e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.116 ( 0.119)	Data  0.001 ( 0.009)	Loss 3.5156e-02 (4.2033e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 2.8992e-02 (4.4447e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 50/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.007)	Loss 3.6102e-02 (4.4776e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.0242e-02 (4.4558e-02)	Acc@1  96.88 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 70/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.4796e-02 (4.4298e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 80/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.0813e-02 (4.3429e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9297e-02 (4.4067e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][100/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.2837e-02 (4.4332e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [44][110/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.9956e-02 (4.4150e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][120/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.9011e-02 (4.4316e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [44][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4839e-02 (4.3998e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][140/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.8248e-02 (4.4474e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [44][150/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.8258e-02 (4.5083e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [44][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5650e-02 (4.5607e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.1453e-02 (4.5587e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.7678e-02 (4.6033e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [44][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.9011e-02 (4.6094e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [44][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.6335e-02 (4.5649e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.0079e-02 (4.5492e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [44][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.6223e-02 (4.5196e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [44][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.7290e-02 (4.5502e-02)	Acc@1  96.88 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [44][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4307e-02 (4.5472e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [44][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0029e-02 (4.5535e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4338e-02 (4.5500e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][270/391]	Time  0.105 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2532e-02 (4.5771e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.3549e-02 (4.5707e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.6143e-02 (4.5892e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [44][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.0659e-02 (4.6085e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [44][310/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6885e-02 (4.6322e-02)	Acc@1  96.88 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [44][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.1584e-02 (4.6466e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0991e-02 (4.6300e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.8960e-02 (4.6374e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [44][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.9854e-02 (4.6496e-02)	Acc@1  96.88 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [44][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.3914e-02 (4.6712e-02)	Acc@1  96.88 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [44][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5335e-02 (4.6658e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [44][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6550e-02 (4.6690e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [44][390/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3783e-02 (4.6923e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 ( 99.99)
## e[44] optimizer.zero_grad (sum) time: 0.2918119430541992
## e[44]       loss.backward (sum) time: 13.75295901298523
## e[44]      optimizer.step (sum) time: 2.7042243480682373
## epoch[44] training(only) time: 45.057427167892456
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.2139e+00 (1.2139e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 1.7041e+00 (1.5451e+00)	Acc@1  69.00 ( 73.00)	Acc@5  94.00 ( 93.27)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.6387e+00 (1.4828e+00)	Acc@1  74.00 ( 73.76)	Acc@5  91.00 ( 93.05)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 2.3594e+00 (1.5437e+00)	Acc@1  63.00 ( 73.13)	Acc@5  88.00 ( 92.58)
Test: [ 40/100]	Time  0.040 ( 0.043)	Loss 1.5254e+00 (1.5089e+00)	Acc@1  71.00 ( 73.24)	Acc@5  95.00 ( 93.02)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.9102e+00 (1.5280e+00)	Acc@1  69.00 ( 72.88)	Acc@5  89.00 ( 92.71)
Test: [ 60/100]	Time  0.041 ( 0.041)	Loss 1.6494e+00 (1.4909e+00)	Acc@1  69.00 ( 73.15)	Acc@5  92.00 ( 92.85)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.6484e+00 (1.5034e+00)	Acc@1  66.00 ( 73.00)	Acc@5  91.00 ( 92.77)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.7949e+00 (1.5018e+00)	Acc@1  66.00 ( 72.91)	Acc@5  94.00 ( 92.83)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.1250e+00 (1.4748e+00)	Acc@1  61.00 ( 73.04)	Acc@5  93.00 ( 92.96)
 * Acc@1 73.170 Acc@5 92.970
### epoch[44] execution time: 49.16434669494629
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.277 ( 0.277)	Data  0.147 ( 0.147)	Loss 4.5105e-02 (4.5105e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.016)	Loss 4.6539e-02 (4.0905e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.010)	Loss 4.0924e-02 (5.4381e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 30/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.008)	Loss 2.9556e-02 (5.0976e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 40/391]	Time  0.107 ( 0.118)	Data  0.001 ( 0.007)	Loss 4.9744e-02 (5.1156e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 50/391]	Time  0.116 ( 0.118)	Data  0.001 ( 0.007)	Loss 7.5562e-02 (4.8101e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.3112e-02 (4.8250e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.3346e-02 (4.7154e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.1177e-02 (4.6215e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.6443e-02 (4.5138e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 ( 99.98)
Epoch: [45][100/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.6570e-02 (4.4746e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.98)
Epoch: [45][110/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.1361e-02 (4.4363e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [45][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.4851e-02 (4.3550e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [45][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.4220e-02 (4.3476e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [45][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.6743e-02 (4.2998e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [45][150/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.0680e-02 (4.3076e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.98)
Epoch: [45][160/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.3608e-02 (4.3135e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [45][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.7129e-02 (4.3572e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [45][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9388e-02 (4.4071e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [45][190/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7868e-02 (4.3888e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [45][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.4626e-02 (4.3571e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 ( 99.98)
Epoch: [45][210/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.4220e-02 (4.3114e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [45][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4368e-02 (4.3021e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [45][230/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2003e-02 (4.2679e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [45][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.9111e-02 (4.2834e-02)	Acc@1  96.88 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [45][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0782e-02 (4.3146e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [45][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5857e-02 (4.3447e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [45][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.8615e-02 (4.3521e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [45][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.1289e-02 (4.3895e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [45][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9800e-02 (4.3823e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [45][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9651e-02 (4.4276e-02)	Acc@1  98.44 ( 98.86)	Acc@5  99.22 ( 99.99)
Epoch: [45][310/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.2053e-02 (4.4124e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [45][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4307e-02 (4.4237e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [45][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.5156e-02 (4.4214e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [45][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.6713e-02 (4.4352e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [45][350/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.8807e-02 (4.4502e-02)	Acc@1  97.66 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [45][360/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.0688e-02 (4.4469e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [45][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7130e-02 (4.4770e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [45][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.6072e-02 (4.5038e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [45][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.2224e-02 (4.5024e-02)	Acc@1  96.25 ( 98.83)	Acc@5 100.00 ( 99.99)
## e[45] optimizer.zero_grad (sum) time: 0.29112839698791504
## e[45]       loss.backward (sum) time: 13.76451587677002
## e[45]      optimizer.step (sum) time: 2.693965196609497
## epoch[45] training(only) time: 45.066404819488525
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 1.2930e+00 (1.2930e+00)	Acc@1  75.00 ( 75.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.7354e+00 (1.5810e+00)	Acc@1  67.00 ( 73.36)	Acc@5  93.00 ( 92.36)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.7578e+00 (1.5149e+00)	Acc@1  72.00 ( 73.43)	Acc@5  91.00 ( 92.52)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 2.3848e+00 (1.5803e+00)	Acc@1  63.00 ( 72.65)	Acc@5  87.00 ( 92.10)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.6465e+00 (1.5379e+00)	Acc@1  70.00 ( 73.00)	Acc@5  95.00 ( 92.71)
Test: [ 50/100]	Time  0.040 ( 0.042)	Loss 1.8936e+00 (1.5461e+00)	Acc@1  69.00 ( 73.02)	Acc@5  89.00 ( 92.31)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.6963e+00 (1.5071e+00)	Acc@1  68.00 ( 73.43)	Acc@5  92.00 ( 92.51)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 1.6816e+00 (1.5184e+00)	Acc@1  65.00 ( 73.23)	Acc@5  91.00 ( 92.56)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.8291e+00 (1.5197e+00)	Acc@1  67.00 ( 73.11)	Acc@5  94.00 ( 92.58)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.1328e+00 (1.4955e+00)	Acc@1  63.00 ( 73.21)	Acc@5  90.00 ( 92.64)
 * Acc@1 73.290 Acc@5 92.660
### epoch[45] execution time: 49.19763493537903
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.257 ( 0.257)	Data  0.154 ( 0.154)	Loss 3.3630e-02 (3.3630e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.113 ( 0.127)	Data  0.001 ( 0.017)	Loss 4.8553e-02 (3.7254e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.011)	Loss 2.4796e-02 (3.9771e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.96)
Epoch: [46][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.7161e-02 (3.8962e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 9.0942e-02 (4.2722e-02)	Acc@1  96.88 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.6693e-02 (4.4275e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 5.5206e-02 (4.3513e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 70/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.2257e-02 (4.2923e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.7445e-02 (4.2160e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 90/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.0558e-02 (4.2119e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [46][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.0995e-02 (4.1882e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [46][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.5004e-02 (4.1702e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [46][120/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1393e-02 (4.1257e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][130/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.9895e-02 (4.1303e-02)	Acc@1  98.44 ( 98.88)	Acc@5  99.22 ( 99.98)
Epoch: [46][140/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9236e-02 (4.1357e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.98)
Epoch: [46][150/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.6204e-02 (4.1473e-02)	Acc@1  96.88 ( 98.89)	Acc@5 100.00 ( 99.98)
Epoch: [46][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.7903e-02 (4.0837e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [46][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.1342e-02 (4.0930e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [46][180/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.7007e-02 (4.0868e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [46][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4576e-02 (4.1016e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [46][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0035e-02 (4.1078e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [46][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.6793e-02 (4.1189e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [46][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1133e-02 (4.1111e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [46][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.6621e-02 (4.0859e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [46][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.3955e-02 (4.1384e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [46][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2998e-02 (4.1238e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [46][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8909e-02 (4.1359e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [46][270/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7506e-02 (4.1687e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.4402e-02 (4.2012e-02)	Acc@1  96.88 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [46][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3788e-02 (4.1671e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.4697e-02 (4.1489e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.2102e-02 (4.1626e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.3864e-02 (4.1597e-02)	Acc@1  98.44 ( 98.90)	Acc@5  99.22 ( 99.99)
Epoch: [46][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6467e-02 (4.1696e-02)	Acc@1  96.88 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.2694e-02 (4.1700e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2654e-02 (4.1982e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.98)
Epoch: [46][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.5267e-02 (4.2472e-02)	Acc@1  97.66 ( 98.88)	Acc@5 100.00 ( 99.98)
Epoch: [46][370/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2379e-02 (4.2259e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.7424e-02 (4.2195e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2552e-02 (4.1960e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.98)
## e[46] optimizer.zero_grad (sum) time: 0.2869586944580078
## e[46]       loss.backward (sum) time: 13.749091386795044
## e[46]      optimizer.step (sum) time: 2.7167201042175293
## epoch[46] training(only) time: 45.02580761909485
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.3096e+00 (1.3096e+00)	Acc@1  74.00 ( 74.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.041 ( 0.054)	Loss 1.7217e+00 (1.5808e+00)	Acc@1  67.00 ( 72.45)	Acc@5  94.00 ( 93.45)
Test: [ 20/100]	Time  0.037 ( 0.047)	Loss 1.7588e+00 (1.5282e+00)	Acc@1  73.00 ( 73.24)	Acc@5  90.00 ( 93.14)
Test: [ 30/100]	Time  0.042 ( 0.044)	Loss 2.2363e+00 (1.5817e+00)	Acc@1  64.00 ( 72.81)	Acc@5  88.00 ( 92.39)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.6182e+00 (1.5377e+00)	Acc@1  74.00 ( 73.27)	Acc@5  94.00 ( 92.85)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9092e+00 (1.5524e+00)	Acc@1  71.00 ( 73.00)	Acc@5  89.00 ( 92.47)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7354e+00 (1.5151e+00)	Acc@1  69.00 ( 73.21)	Acc@5  93.00 ( 92.66)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.5889e+00 (1.5225e+00)	Acc@1  67.00 ( 73.17)	Acc@5  91.00 ( 92.66)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.7275e+00 (1.5227e+00)	Acc@1  69.00 ( 73.06)	Acc@5  94.00 ( 92.67)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.1152e+00 (1.4987e+00)	Acc@1  60.00 ( 73.13)	Acc@5  92.00 ( 92.74)
 * Acc@1 73.290 Acc@5 92.780
### epoch[46] execution time: 49.15643000602722
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.278 ( 0.278)	Data  0.168 ( 0.168)	Loss 4.0710e-02 (4.0710e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.113 ( 0.129)	Data  0.001 ( 0.018)	Loss 2.5787e-02 (3.5370e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.7120e-02 (3.7191e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.112 ( 0.120)	Data  0.001 ( 0.009)	Loss 1.5671e-02 (3.6934e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 4.9622e-02 (3.9670e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 6.1768e-02 (4.1136e-02)	Acc@1  96.88 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.109 ( 0.117)	Data  0.001 ( 0.006)	Loss 4.0436e-02 (3.9311e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 9.3384e-02 (4.0791e-02)	Acc@1  97.66 ( 98.97)	Acc@5  99.22 ( 99.99)
Epoch: [47][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.2542e-02 (4.0341e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 7.9651e-02 (4.1030e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [47][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.0791e-02 (4.1525e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [47][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.0383e-02 (4.2088e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [47][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.5227e-02 (4.1786e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [47][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.7770e-02 (4.1370e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][140/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0945e-02 (4.1413e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.5786e-02 (4.1136e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [47][160/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0706e-01 (4.1374e-02)	Acc@1  95.31 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.7659e-02 (4.1210e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.0258e-02 (4.1300e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.7272e-02 (4.1534e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.3242e-02 (4.1520e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.9723e-02 (4.1266e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8513e-02 (4.1278e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.1586e-02 (4.1374e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.5248e-02 (4.1243e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.2917e-02 (4.1512e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2614e-02 (4.1605e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.5807e-02 (4.1537e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5803e-02 (4.1365e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7863e-02 (4.1129e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.3579e-02 (4.1095e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0569e-02 (4.0895e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5857e-02 (4.1333e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.2429e-02 (4.1394e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][340/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2421e-02 (4.1560e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][350/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.3242e-02 (4.1452e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9099e-02 (4.1392e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.9143e-02 (4.1399e-02)	Acc@1  96.09 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.6255e-02 (4.1536e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][390/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0175e-01 (4.1928e-02)	Acc@1  96.25 ( 98.92)	Acc@5 100.00 ( 99.99)
## e[47] optimizer.zero_grad (sum) time: 0.2899281978607178
## e[47]       loss.backward (sum) time: 13.717665195465088
## e[47]      optimizer.step (sum) time: 2.728688955307007
## epoch[47] training(only) time: 45.08108353614807
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 1.3809e+00 (1.3809e+00)	Acc@1  73.00 ( 73.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.040 ( 0.054)	Loss 1.7891e+00 (1.6058e+00)	Acc@1  66.00 ( 71.91)	Acc@5  95.00 ( 93.00)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.8184e+00 (1.5590e+00)	Acc@1  75.00 ( 73.19)	Acc@5  89.00 ( 92.81)
Test: [ 30/100]	Time  0.037 ( 0.044)	Loss 2.4238e+00 (1.6139e+00)	Acc@1  63.00 ( 72.77)	Acc@5  86.00 ( 92.19)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.7373e+00 (1.5772e+00)	Acc@1  73.00 ( 72.98)	Acc@5  95.00 ( 92.61)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.9434e+00 (1.5937e+00)	Acc@1  71.00 ( 72.76)	Acc@5  90.00 ( 92.31)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.6982e+00 (1.5555e+00)	Acc@1  71.00 ( 72.98)	Acc@5  94.00 ( 92.59)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.5439e+00 (1.5636e+00)	Acc@1  66.00 ( 72.92)	Acc@5  93.00 ( 92.55)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.8174e+00 (1.5647e+00)	Acc@1  68.00 ( 72.86)	Acc@5  93.00 ( 92.56)
Test: [ 90/100]	Time  0.037 ( 0.040)	Loss 2.0801e+00 (1.5391e+00)	Acc@1  64.00 ( 72.98)	Acc@5  92.00 ( 92.59)
 * Acc@1 73.120 Acc@5 92.650
### epoch[47] execution time: 49.179301023483276
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.259 ( 0.259)	Data  0.154 ( 0.154)	Loss 2.7252e-02 (2.7252e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.112 ( 0.127)	Data  0.001 ( 0.017)	Loss 2.3697e-02 (3.9992e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.111 ( 0.121)	Data  0.001 ( 0.011)	Loss 5.7617e-02 (4.2714e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 6.6528e-02 (4.3119e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 3.7354e-02 (4.0996e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.007)	Loss 4.2725e-02 (3.9984e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.6234e-02 (3.9495e-02)	Acc@1  98.44 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.2794e-02 (3.9265e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.5297e-02 (3.8877e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [48][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.0619e-02 (3.7803e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.5349e-02 (3.7747e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [48][110/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.1738e-02 (3.7087e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][120/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4180e-02 (3.6800e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [48][130/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.7750e-02 (3.6728e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [48][140/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4943e-02 (3.6964e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][150/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3285e-02 (3.7009e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][160/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.4006e-02 (3.7181e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4515e-02 (3.6642e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.7506e-02 (3.6596e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5961e-02 (3.6484e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8091e-02 (3.6175e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.1819e-02 (3.6801e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [48][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.4586e-02 (3.7196e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [48][230/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9867e-02 (3.7318e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.3619e-02 (3.7450e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][250/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.5867e-02 (3.7488e-02)	Acc@1  96.88 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7573e-02 (3.7555e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1311e-02 (3.7924e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.99)
Epoch: [48][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6245e-02 (3.8107e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [48][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5543e-02 (3.7813e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.0710e-02 (3.7852e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [48][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.9093e-02 (3.7694e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.9154e-02 (3.7693e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6983e-02 (3.7567e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.8186e-02 (3.7912e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3849e-02 (3.7609e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [48][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.4707e-02 (3.7946e-02)	Acc@1  96.88 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5253e-02 (3.7682e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2186e-02 (3.7848e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2937e-02 (3.8058e-02)	Acc@1  97.50 ( 99.00)	Acc@5 100.00 ( 99.99)
## e[48] optimizer.zero_grad (sum) time: 0.2929501533508301
## e[48]       loss.backward (sum) time: 13.780661582946777
## e[48]      optimizer.step (sum) time: 2.710249185562134
## epoch[48] training(only) time: 44.95099663734436
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.3945e+00 (1.3945e+00)	Acc@1  75.00 ( 75.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.040 ( 0.052)	Loss 1.7803e+00 (1.6413e+00)	Acc@1  68.00 ( 72.27)	Acc@5  94.00 ( 92.82)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.9473e+00 (1.5491e+00)	Acc@1  70.00 ( 73.24)	Acc@5  90.00 ( 93.10)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.4961e+00 (1.6198e+00)	Acc@1  63.00 ( 72.55)	Acc@5  88.00 ( 92.39)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.7725e+00 (1.5877e+00)	Acc@1  71.00 ( 72.68)	Acc@5  93.00 ( 92.80)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.9121e+00 (1.5969e+00)	Acc@1  69.00 ( 72.47)	Acc@5  90.00 ( 92.51)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.8652e+00 (1.5540e+00)	Acc@1  71.00 ( 72.84)	Acc@5  90.00 ( 92.66)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.6094e+00 (1.5645e+00)	Acc@1  69.00 ( 72.75)	Acc@5  92.00 ( 92.68)
Test: [ 80/100]	Time  0.041 ( 0.041)	Loss 1.8291e+00 (1.5710e+00)	Acc@1  68.00 ( 72.84)	Acc@5  93.00 ( 92.65)
Test: [ 90/100]	Time  0.038 ( 0.040)	Loss 2.2305e+00 (1.5506e+00)	Acc@1  65.00 ( 72.95)	Acc@5  92.00 ( 92.71)
 * Acc@1 73.000 Acc@5 92.740
### epoch[48] execution time: 49.06957793235779
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.272 ( 0.272)	Data  0.163 ( 0.163)	Loss 1.5442e-02 (1.5442e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.018)	Loss 1.6220e-02 (2.9203e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.114 ( 0.121)	Data  0.001 ( 0.011)	Loss 5.8167e-02 (3.2140e-02)	Acc@1  99.22 ( 99.44)	Acc@5  99.22 ( 99.96)
Epoch: [49][ 30/391]	Time  0.111 ( 0.119)	Data  0.001 ( 0.009)	Loss 3.6316e-02 (3.3793e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.97)
Epoch: [49][ 40/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.008)	Loss 8.2947e-02 (3.4919e-02)	Acc@1  96.88 ( 99.24)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 50/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.007)	Loss 3.3478e-02 (3.5331e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 ( 99.98)
Epoch: [49][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.6596e-02 (3.5487e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [49][ 70/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.3497e-02 (3.5134e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [49][ 80/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.7852e-02 (3.5278e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [49][ 90/391]	Time  0.117 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.7222e-02 (3.5580e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.98)
Epoch: [49][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1497e-02 (3.4764e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 ( 99.98)
Epoch: [49][110/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.8046e-02 (3.4529e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [49][120/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6840e-02 (3.4527e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [49][130/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.5206e-02 (3.4478e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [49][140/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9419e-02 (3.5311e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [49][150/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6367e-02 (3.5137e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [49][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.5044e-02 (3.5259e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [49][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3514e-02 (3.5617e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [49][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6510e-02 (3.5364e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [49][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.1626e-02 (3.5646e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [49][200/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.3203e-02 (3.5509e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [49][210/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.9368e-02 (3.5803e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [49][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.4463e-02 (3.6107e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [49][230/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5986e-02 (3.6162e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [49][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.6509e-02 (3.6431e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [49][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.6560e-02 (3.6181e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [49][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.9520e-02 (3.5978e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [49][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.8920e-02 (3.5983e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [49][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7140e-02 (3.6010e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [49][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.3396e-02 (3.6520e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [49][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6479e-02 (3.6188e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [49][310/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.3436e-02 (3.6417e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [49][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0857e-02 (3.6334e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [49][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.2866e-02 (3.6647e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [49][340/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7994e-02 (3.6736e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [49][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.3628e-02 (3.7007e-02)	Acc@1  95.31 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [49][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7715e-02 (3.7159e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [49][370/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0669e-02 (3.7421e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [49][380/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2104e-02 (3.7373e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [49][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9957e-02 (3.7305e-02)	Acc@1  98.75 ( 99.05)	Acc@5 100.00 ( 99.99)
## e[49] optimizer.zero_grad (sum) time: 0.29238009452819824
## e[49]       loss.backward (sum) time: 13.812093496322632
## e[49]      optimizer.step (sum) time: 2.7000436782836914
## epoch[49] training(only) time: 44.94469213485718
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.3418e+00 (1.3418e+00)	Acc@1  77.00 ( 77.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.040 ( 0.054)	Loss 1.9473e+00 (1.6565e+00)	Acc@1  62.00 ( 71.91)	Acc@5  96.00 ( 92.82)
Test: [ 20/100]	Time  0.040 ( 0.047)	Loss 1.8281e+00 (1.5649e+00)	Acc@1  73.00 ( 73.48)	Acc@5  90.00 ( 93.14)
Test: [ 30/100]	Time  0.039 ( 0.045)	Loss 2.3047e+00 (1.6285e+00)	Acc@1  62.00 ( 72.65)	Acc@5  88.00 ( 92.32)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.7354e+00 (1.5887e+00)	Acc@1  71.00 ( 73.05)	Acc@5  93.00 ( 92.83)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 2.0566e+00 (1.5972e+00)	Acc@1  69.00 ( 72.98)	Acc@5  89.00 ( 92.37)
Test: [ 60/100]	Time  0.043 ( 0.042)	Loss 1.7832e+00 (1.5603e+00)	Acc@1  69.00 ( 73.13)	Acc@5  91.00 ( 92.59)
Test: [ 70/100]	Time  0.037 ( 0.041)	Loss 1.6133e+00 (1.5704e+00)	Acc@1  67.00 ( 73.08)	Acc@5  90.00 ( 92.48)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.6875e+00 (1.5720e+00)	Acc@1  68.00 ( 73.09)	Acc@5  93.00 ( 92.52)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.2656e+00 (1.5512e+00)	Acc@1  60.00 ( 73.13)	Acc@5  93.00 ( 92.58)
 * Acc@1 73.100 Acc@5 92.670
### epoch[49] execution time: 49.0860652923584
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.288 ( 0.288)	Data  0.174 ( 0.174)	Loss 2.8915e-02 (2.8915e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.112 ( 0.130)	Data  0.001 ( 0.019)	Loss 2.4399e-02 (2.4384e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.111 ( 0.123)	Data  0.001 ( 0.012)	Loss 5.2094e-02 (2.8984e-02)	Acc@1  97.66 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.114 ( 0.120)	Data  0.001 ( 0.009)	Loss 3.3264e-02 (3.2416e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.008)	Loss 3.0624e-02 (3.2036e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.109 ( 0.118)	Data  0.001 ( 0.007)	Loss 3.9978e-02 (3.2368e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.116 ( 0.117)	Data  0.001 ( 0.007)	Loss 3.0518e-02 (3.3204e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 4.2816e-02 (3.3791e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.9114e-02 (3.3996e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.4697e-02 (3.3943e-02)	Acc@1  96.88 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.6743e-02 (3.4843e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9678e-02 (3.5336e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.6443e-02 (3.5666e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [50][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.1077e-02 (3.5292e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [50][140/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.8014e-02 (3.5523e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [50][150/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.5767e-02 (3.4996e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [50][160/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.1523e-02 (3.4839e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.3793e-02 (3.4677e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.5757e-02 (3.4531e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7807e-02 (3.4076e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.6509e-02 (3.3843e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2797e-02 (3.3677e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4241e-02 (3.3368e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.2755e-02 (3.2961e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.1738e-02 (3.2942e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1713e-02 (3.2770e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.8909e-02 (3.3488e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.9316e-02 (3.3521e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0975e-02 (3.3339e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.9265e-02 (3.3345e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4986e-03 (3.3143e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9816e-02 (3.3018e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7466e-02 (3.3005e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0068e-02 (3.3292e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1942e-02 (3.3343e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5299e-02 (3.3584e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2095e-02 (3.3535e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4948e-02 (3.4073e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.3284e-02 (3.4010e-02)	Acc@1  96.88 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.3945e-02 (3.4060e-02)	Acc@1  98.75 ( 99.14)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.29109907150268555
## e[50]       loss.backward (sum) time: 13.78525161743164
## e[50]      optimizer.step (sum) time: 2.7274744510650635
## epoch[50] training(only) time: 45.124194860458374
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.3545e+00 (1.3545e+00)	Acc@1  76.00 ( 76.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.042 ( 0.053)	Loss 1.8701e+00 (1.6507e+00)	Acc@1  67.00 ( 72.45)	Acc@5  93.00 ( 92.64)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8799e+00 (1.5776e+00)	Acc@1  73.00 ( 73.48)	Acc@5  91.00 ( 93.00)
Test: [ 30/100]	Time  0.041 ( 0.045)	Loss 2.2324e+00 (1.6310e+00)	Acc@1  64.00 ( 72.87)	Acc@5  88.00 ( 92.35)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.7920e+00 (1.5963e+00)	Acc@1  70.00 ( 73.05)	Acc@5  94.00 ( 92.88)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 2.0430e+00 (1.6041e+00)	Acc@1  67.00 ( 72.75)	Acc@5  89.00 ( 92.65)
Test: [ 60/100]	Time  0.037 ( 0.042)	Loss 1.7773e+00 (1.5709e+00)	Acc@1  71.00 ( 72.93)	Acc@5  92.00 ( 92.77)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 1.4980e+00 (1.5794e+00)	Acc@1  71.00 ( 72.97)	Acc@5  92.00 ( 92.69)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.9180e+00 (1.5848e+00)	Acc@1  69.00 ( 73.01)	Acc@5  94.00 ( 92.65)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.2559e+00 (1.5621e+00)	Acc@1  63.00 ( 73.07)	Acc@5  91.00 ( 92.75)
 * Acc@1 73.140 Acc@5 92.830
### epoch[50] execution time: 49.26018714904785
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.272 ( 0.272)	Data  0.166 ( 0.166)	Loss 2.9816e-02 (2.9816e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.018)	Loss 1.6388e-02 (3.3678e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.0676e-02 (3.2035e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 5.7709e-02 (3.4461e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 4.3701e-02 (3.3989e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 3.5248e-02 (3.3151e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.8732e-02 (3.2666e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.0518e-02 (3.2464e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.9861e-02 (3.2643e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [51][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.9185e-02 (3.1996e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [51][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.6865e-02 (3.2129e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [51][110/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.8953e-03 (3.1742e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [51][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.8177e-02 (3.1814e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [51][130/391]	Time  0.119 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.3087e-02 (3.1211e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [51][140/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4429e-02 (3.0911e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [51][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.6041e-02 (3.1184e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [51][160/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.7627e-02 (3.0843e-02)	Acc@1  96.88 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [51][170/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2888e-02 (3.1357e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [51][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3580e-02 (3.1318e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [51][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.7485e-02 (3.2108e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [51][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8177e-02 (3.1955e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [51][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4063e-02 (3.2571e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.98)
Epoch: [51][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0996e-02 (3.2411e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.2053e-02 (3.2626e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.98)
Epoch: [51][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8183e-02 (3.2449e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.98)
Epoch: [51][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6627e-02 (3.2595e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.98)
Epoch: [51][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3783e-02 (3.2538e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.98)
Epoch: [51][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3895e-02 (3.2584e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][280/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2125e-02 (3.2408e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7237e-02 (3.2462e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.5431e-02 (3.2705e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3087e-02 (3.2804e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0242e-02 (3.2812e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8046e-02 (3.2771e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.98)
Epoch: [51][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9144e-02 (3.2979e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2949e-02 (3.2882e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3237e-02 (3.2979e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7695e-02 (3.3106e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.98)
Epoch: [51][380/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.7852e-02 (3.2942e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.98)
Epoch: [51][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6162e-02 (3.2901e-02)	Acc@1  97.50 ( 99.16)	Acc@5 100.00 ( 99.98)
## e[51] optimizer.zero_grad (sum) time: 0.29355835914611816
## e[51]       loss.backward (sum) time: 13.75919508934021
## e[51]      optimizer.step (sum) time: 2.722160816192627
## epoch[51] training(only) time: 45.07397246360779
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.3232e+00 (1.3232e+00)	Acc@1  76.00 ( 76.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.8857e+00 (1.6776e+00)	Acc@1  67.00 ( 73.18)	Acc@5  95.00 ( 93.09)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.9619e+00 (1.5854e+00)	Acc@1  71.00 ( 73.76)	Acc@5  90.00 ( 93.14)
Test: [ 30/100]	Time  0.039 ( 0.045)	Loss 2.3262e+00 (1.6416e+00)	Acc@1  65.00 ( 73.39)	Acc@5  87.00 ( 92.23)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.8008e+00 (1.6077e+00)	Acc@1  70.00 ( 73.46)	Acc@5  92.00 ( 92.66)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 2.0684e+00 (1.6194e+00)	Acc@1  68.00 ( 73.04)	Acc@5  88.00 ( 92.22)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.7520e+00 (1.5789e+00)	Acc@1  68.00 ( 73.15)	Acc@5  93.00 ( 92.49)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.6016e+00 (1.5877e+00)	Acc@1  67.00 ( 73.06)	Acc@5  91.00 ( 92.44)
Test: [ 80/100]	Time  0.037 ( 0.041)	Loss 2.0059e+00 (1.5929e+00)	Acc@1  69.00 ( 72.91)	Acc@5  93.00 ( 92.46)
Test: [ 90/100]	Time  0.037 ( 0.041)	Loss 2.3066e+00 (1.5698e+00)	Acc@1  64.00 ( 73.03)	Acc@5  91.00 ( 92.53)
 * Acc@1 73.030 Acc@5 92.600
### epoch[51] execution time: 49.18909025192261
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.282 ( 0.282)	Data  0.166 ( 0.166)	Loss 2.0477e-02 (2.0477e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.110 ( 0.129)	Data  0.001 ( 0.018)	Loss 4.6906e-02 (3.0289e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.011)	Loss 5.6946e-02 (2.6033e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 4.0741e-02 (2.7424e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 5.1056e-02 (2.8757e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.110 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.3193e-02 (2.9518e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.2873e-02 (2.8321e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.5457e-02 (2.9917e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.6326e-02 (3.0450e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.5620e-02 (3.0241e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9617e-02 (3.0375e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.2664e-02 (3.0653e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.6093e-02 (2.9924e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4587e-02 (3.0388e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [52][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.3270e-02 (3.0308e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [52][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5762e-02 (2.9992e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 ( 99.99)
Epoch: [52][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.3325e-02 (3.0012e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.0110e-02 (2.9851e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4088e-02 (2.9511e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8570e-02 (2.9161e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1866e-02 (2.9270e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.7130e-02 (2.9166e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3895e-02 (2.9157e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.8114e-03 (2.8617e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2260e-02 (2.8854e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.6926e-02 (2.9216e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7258e-02 (2.9373e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8768e-02 (2.9388e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3804e-02 (2.9639e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.3232e-02 (2.9897e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6840e-02 (2.9760e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3081e-02 (2.9555e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6749e-02 (2.9542e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2024e-02 (2.9658e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.8239e-02 (2.9676e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1973e-02 (2.9973e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.5858e-02 (3.0196e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.8818e-02 (3.0279e-02)	Acc@1  97.66 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.3121e-02 (3.0293e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7445e-02 (3.0531e-02)	Acc@1  98.75 ( 99.22)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.28998517990112305
## e[52]       loss.backward (sum) time: 13.750571012496948
## e[52]      optimizer.step (sum) time: 2.69433856010437
## epoch[52] training(only) time: 45.05758476257324
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.3535e+00 (1.3535e+00)	Acc@1  76.00 ( 76.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.040 ( 0.053)	Loss 1.9609e+00 (1.6480e+00)	Acc@1  66.00 ( 73.09)	Acc@5  94.00 ( 92.64)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.7266e+00 (1.5542e+00)	Acc@1  73.00 ( 74.43)	Acc@5  92.00 ( 92.67)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.3359e+00 (1.6144e+00)	Acc@1  63.00 ( 73.55)	Acc@5  89.00 ( 92.16)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.6533e+00 (1.5792e+00)	Acc@1  74.00 ( 73.68)	Acc@5  95.00 ( 92.66)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.9746e+00 (1.5962e+00)	Acc@1  68.00 ( 73.29)	Acc@5  89.00 ( 92.20)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7139e+00 (1.5551e+00)	Acc@1  69.00 ( 73.44)	Acc@5  93.00 ( 92.43)
Test: [ 70/100]	Time  0.041 ( 0.041)	Loss 1.5537e+00 (1.5609e+00)	Acc@1  69.00 ( 73.48)	Acc@5  91.00 ( 92.44)
Test: [ 80/100]	Time  0.040 ( 0.041)	Loss 1.9580e+00 (1.5645e+00)	Acc@1  67.00 ( 73.40)	Acc@5  93.00 ( 92.49)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 1.9629e+00 (1.5390e+00)	Acc@1  68.00 ( 73.49)	Acc@5  94.00 ( 92.56)
 * Acc@1 73.530 Acc@5 92.650
### epoch[52] execution time: 49.198529958724976
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.267 ( 0.267)	Data  0.154 ( 0.154)	Loss 2.6932e-02 (2.6932e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.017)	Loss 2.8305e-02 (3.0061e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.115 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.1423e-02 (2.8434e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.111 ( 0.120)	Data  0.001 ( 0.009)	Loss 4.8462e-02 (2.9740e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 3.7445e-02 (2.8304e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 6.2469e-02 (2.8122e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.9953e-02 (2.8618e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.8356e-02 (2.8621e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.0751e-02 (2.9184e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.6783e-02 (2.9747e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3489e-02 (2.9439e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.3997e-02 (2.9987e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [53][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8127e-02 (2.9570e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [53][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.6072e-02 (2.9901e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 ( 99.99)
Epoch: [53][140/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2797e-02 (2.9725e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [53][150/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9922e-02 (2.9708e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [53][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4030e-02 (2.9537e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6962e-02 (2.9362e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3819e-02 (2.9182e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2711e-02 (2.8882e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3840e-02 (2.8612e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4450e-02 (2.8353e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.9948e-02 (2.8418e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [53][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.2205e-02 (2.8694e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [53][240/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.3539e-02 (2.8648e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [53][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3295e-02 (2.8819e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [53][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.5410e-02 (2.8770e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [53][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2720e-02 (2.8661e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [53][280/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3046e-02 (2.8539e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [53][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3947e-02 (2.8455e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [53][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.1229e-02 (2.8396e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [53][310/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1210e-02 (2.8362e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [53][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7603e-02 (2.8508e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [53][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6535e-02 (2.8430e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [53][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.3110e-02 (2.8531e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [53][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6605e-03 (2.8354e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [53][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.8971e-02 (2.8541e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [53][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3224e-02 (2.8706e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [53][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0889e-02 (2.8588e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [53][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0090e-02 (2.8505e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 ( 99.99)
## e[53] optimizer.zero_grad (sum) time: 0.29163575172424316
## e[53]       loss.backward (sum) time: 13.770063400268555
## e[53]      optimizer.step (sum) time: 2.7052314281463623
## epoch[53] training(only) time: 45.06572699546814
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 1.3594e+00 (1.3594e+00)	Acc@1  74.00 ( 74.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.056)	Loss 1.8818e+00 (1.6353e+00)	Acc@1  64.00 ( 72.36)	Acc@5  94.00 ( 92.73)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.7842e+00 (1.5657e+00)	Acc@1  73.00 ( 73.71)	Acc@5  92.00 ( 92.90)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.3086e+00 (1.6257e+00)	Acc@1  64.00 ( 73.10)	Acc@5  90.00 ( 92.26)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.8135e+00 (1.6013e+00)	Acc@1  72.00 ( 73.12)	Acc@5  94.00 ( 92.71)
Test: [ 50/100]	Time  0.044 ( 0.042)	Loss 1.9424e+00 (1.6222e+00)	Acc@1  70.00 ( 72.94)	Acc@5  90.00 ( 92.25)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7031e+00 (1.5834e+00)	Acc@1  70.00 ( 73.21)	Acc@5  93.00 ( 92.41)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.6279e+00 (1.5854e+00)	Acc@1  69.00 ( 73.21)	Acc@5  90.00 ( 92.46)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 2.0898e+00 (1.5862e+00)	Acc@1  70.00 ( 73.17)	Acc@5  93.00 ( 92.46)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.0801e+00 (1.5608e+00)	Acc@1  64.00 ( 73.37)	Acc@5  94.00 ( 92.58)
 * Acc@1 73.400 Acc@5 92.680
### epoch[53] execution time: 49.2024347782135
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.257 ( 0.257)	Data  0.152 ( 0.152)	Loss 3.8879e-02 (3.8879e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.112 ( 0.127)	Data  0.001 ( 0.017)	Loss 4.6265e-02 (3.1167e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.114 ( 0.121)	Data  0.001 ( 0.011)	Loss 5.1025e-02 (3.2959e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.6052e-02 (3.2354e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.110 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.2634e-02 (3.1228e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.0309e-02 (3.0091e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.1708e-02 (3.0647e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.2077e-02 (2.9267e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.7374e-02 (2.8695e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.6152e-02 (2.9135e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.9429e-02 (2.9318e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5701e-02 (2.8874e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0899e-02 (2.8816e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8671e-02 (2.8653e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0857e-02 (2.8282e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4826e-02 (2.8010e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9562e-02 (2.8010e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.118 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4000e-02 (2.7861e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.3051e-02 (2.8113e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5226e-03 (2.8230e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4427e-02 (2.7863e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.7760e-02 (2.7982e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8829e-02 (2.8074e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.7699e-02 (2.8375e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1042e-02 (2.8411e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7863e-02 (2.8395e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7410e-02 (2.8239e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6335e-02 (2.8285e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5406e-02 (2.8638e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9104e-02 (2.8713e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2690e-02 (2.8622e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3783e-02 (2.8586e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3966e-02 (2.8706e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.7312e-02 (2.8989e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9943e-02 (2.9224e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1591e-02 (2.9276e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5864e-02 (2.9111e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7542e-02 (2.9131e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7130e-02 (2.9220e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7145e-02 (2.9257e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.29091548919677734
## e[54]       loss.backward (sum) time: 13.78508996963501
## e[54]      optimizer.step (sum) time: 2.691274881362915
## epoch[54] training(only) time: 45.02684998512268
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 1.3711e+00 (1.3711e+00)	Acc@1  75.00 ( 75.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.052)	Loss 1.9268e+00 (1.6577e+00)	Acc@1  65.00 ( 71.73)	Acc@5  94.00 ( 92.91)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.8604e+00 (1.5817e+00)	Acc@1  71.00 ( 73.19)	Acc@5  93.00 ( 93.10)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5000e+00 (1.6522e+00)	Acc@1  63.00 ( 72.39)	Acc@5  84.00 ( 92.19)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8428e+00 (1.6291e+00)	Acc@1  71.00 ( 72.73)	Acc@5  94.00 ( 92.68)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.9199e+00 (1.6465e+00)	Acc@1  70.00 ( 72.63)	Acc@5  90.00 ( 92.35)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.6738e+00 (1.6046e+00)	Acc@1  69.00 ( 73.03)	Acc@5  93.00 ( 92.64)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.7256e+00 (1.6124e+00)	Acc@1  70.00 ( 73.03)	Acc@5  90.00 ( 92.62)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.9824e+00 (1.6167e+00)	Acc@1  69.00 ( 73.01)	Acc@5  93.00 ( 92.54)
Test: [ 90/100]	Time  0.040 ( 0.041)	Loss 2.2852e+00 (1.5902e+00)	Acc@1  60.00 ( 73.11)	Acc@5  91.00 ( 92.65)
 * Acc@1 73.140 Acc@5 92.680
### epoch[54] execution time: 49.15679883956909
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.273 ( 0.273)	Data  0.166 ( 0.166)	Loss 1.8768e-02 (1.8768e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.111 ( 0.129)	Data  0.001 ( 0.018)	Loss 2.8641e-02 (1.8444e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.112 ( 0.122)	Data  0.001 ( 0.012)	Loss 1.5839e-02 (2.0613e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.9913e-02 (2.2296e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.9165e-02 (2.4663e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.6138e-02 (2.4247e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.4109e-02 (2.4826e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 5.2094e-02 (2.6514e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.5711e-02 (2.7097e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.3722e-02 (2.6559e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [55][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.3046e-02 (2.6787e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [55][110/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.1260e-02 (2.7196e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [55][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9485e-02 (2.6892e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [55][130/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9135e-02 (2.6549e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [55][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1261e-02 (2.6337e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [55][150/391]	Time  0.117 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.2746e-03 (2.5954e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [55][160/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0197e-02 (2.5697e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2430e-02 (2.6399e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8580e-02 (2.6384e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8524e-02 (2.6431e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4048e-02 (2.6320e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4046e-02 (2.6344e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.119 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2985e-02 (2.6166e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1851e-02 (2.6288e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2352e-02 (2.6228e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2532e-02 (2.6328e-02)	Acc@1  97.66 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2959e-02 (2.6274e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8809e-02 (2.6292e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1393e-02 (2.6252e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7944e-02 (2.6421e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0813e-02 (2.6262e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9408e-02 (2.6295e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0630e-02 (2.6206e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5574e-02 (2.6382e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5991e-02 (2.6276e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1860e-02 (2.6243e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.4952e-02 (2.6400e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3178e-02 (2.6199e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.6774e-02 (2.6157e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.7852e-02 (2.6396e-02)	Acc@1  98.75 ( 99.40)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.2873692512512207
## e[55]       loss.backward (sum) time: 13.774218082427979
## e[55]      optimizer.step (sum) time: 2.7213025093078613
## epoch[55] training(only) time: 45.07784867286682
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.3975e+00 (1.3975e+00)	Acc@1  75.00 ( 75.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.040 ( 0.053)	Loss 1.9443e+00 (1.6647e+00)	Acc@1  66.00 ( 73.00)	Acc@5  93.00 ( 92.45)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.8496e+00 (1.5791e+00)	Acc@1  74.00 ( 74.14)	Acc@5  92.00 ( 92.81)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 2.5645e+00 (1.6475e+00)	Acc@1  65.00 ( 73.16)	Acc@5  84.00 ( 92.19)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.8154e+00 (1.6261e+00)	Acc@1  71.00 ( 73.29)	Acc@5  94.00 ( 92.73)
Test: [ 50/100]	Time  0.046 ( 0.042)	Loss 2.0820e+00 (1.6425e+00)	Acc@1  67.00 ( 73.14)	Acc@5  89.00 ( 92.43)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.7939e+00 (1.6028e+00)	Acc@1  68.00 ( 73.33)	Acc@5  91.00 ( 92.62)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.6494e+00 (1.6110e+00)	Acc@1  67.00 ( 73.31)	Acc@5  91.00 ( 92.56)
Test: [ 80/100]	Time  0.039 ( 0.040)	Loss 2.0098e+00 (1.6172e+00)	Acc@1  69.00 ( 73.23)	Acc@5  93.00 ( 92.58)
Test: [ 90/100]	Time  0.040 ( 0.040)	Loss 2.1875e+00 (1.5927e+00)	Acc@1  66.00 ( 73.40)	Acc@5  92.00 ( 92.58)
 * Acc@1 73.480 Acc@5 92.600
### epoch[55] execution time: 49.18583297729492
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.273 ( 0.273)	Data  0.162 ( 0.162)	Loss 9.9869e-03 (9.9869e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.116 ( 0.129)	Data  0.001 ( 0.018)	Loss 3.0197e-02 (2.1410e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 6.4583e-03 (2.1298e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 5.3444e-03 (2.1063e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.4091e-02 (2.1718e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.2428e-02 (2.3301e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.7319e-02 (2.3027e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 4.3335e-02 (2.4021e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.0538e-02 (2.4261e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.5482e-02 (2.3698e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2156e-02 (2.3723e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.5833e-02 (2.4205e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4572e-02 (2.4225e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.0680e-02 (2.4258e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9562e-02 (2.4023e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8971e-02 (2.4261e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2848e-02 (2.4550e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2324e-02 (2.4308e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7685e-02 (2.4016e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.5645e-02 (2.4416e-02)	Acc@1  97.66 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2385e-02 (2.4578e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5696e-02 (2.4795e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7334e-02 (2.4831e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9257e-02 (2.4748e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4648e-02 (2.4643e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4094e-02 (2.4970e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.6804e-02 (2.4997e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3199e-02 (2.5069e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6891e-02 (2.5096e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6810e-02 (2.5216e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8061e-02 (2.5180e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3788e-02 (2.5369e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0437e-02 (2.5278e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6113e-02 (2.5984e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1342e-02 (2.5877e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8169e-03 (2.5984e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4023e-02 (2.6097e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1284e-02 (2.6372e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.9276e-02 (2.6636e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.0812e-02 (2.6574e-02)	Acc@1  98.75 ( 99.36)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.29262852668762207
## e[56]       loss.backward (sum) time: 13.797111511230469
## e[56]      optimizer.step (sum) time: 2.7235782146453857
## epoch[56] training(only) time: 45.059988260269165
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.4805e+00 (1.4805e+00)	Acc@1  75.00 ( 75.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.054)	Loss 1.9941e+00 (1.6728e+00)	Acc@1  68.00 ( 73.00)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.040 ( 0.047)	Loss 1.8115e+00 (1.5966e+00)	Acc@1  75.00 ( 73.62)	Acc@5  92.00 ( 92.71)
Test: [ 30/100]	Time  0.039 ( 0.045)	Loss 2.4570e+00 (1.6554e+00)	Acc@1  62.00 ( 73.06)	Acc@5  89.00 ( 92.32)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.9375e+00 (1.6301e+00)	Acc@1  70.00 ( 73.22)	Acc@5  94.00 ( 92.78)
Test: [ 50/100]	Time  0.040 ( 0.042)	Loss 2.0098e+00 (1.6522e+00)	Acc@1  69.00 ( 73.08)	Acc@5  88.00 ( 92.37)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7754e+00 (1.6073e+00)	Acc@1  74.00 ( 73.44)	Acc@5  92.00 ( 92.62)
Test: [ 70/100]	Time  0.037 ( 0.041)	Loss 1.6055e+00 (1.6197e+00)	Acc@1  67.00 ( 73.25)	Acc@5  90.00 ( 92.62)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0918e+00 (1.6249e+00)	Acc@1  70.00 ( 73.26)	Acc@5  94.00 ( 92.56)
Test: [ 90/100]	Time  0.037 ( 0.041)	Loss 2.1797e+00 (1.5988e+00)	Acc@1  62.00 ( 73.42)	Acc@5  93.00 ( 92.63)
 * Acc@1 73.460 Acc@5 92.670
### epoch[56] execution time: 49.19154095649719
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.274 ( 0.274)	Data  0.159 ( 0.159)	Loss 2.0813e-02 (2.0813e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.112 ( 0.128)	Data  0.001 ( 0.018)	Loss 4.1870e-02 (2.3940e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 9.9304e-02 (2.6948e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.1757e-02 (2.4950e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 2.4536e-02 (2.5269e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.1820e-02 (2.4401e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.8387e-02 (2.4314e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.0534e-03 (2.3997e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.0460e-02 (2.3594e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.3569e-02 (2.4356e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [57][100/391]	Time  0.117 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3992e-02 (2.4322e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [57][110/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0746e-02 (2.4508e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [57][120/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.7363e-02 (2.4289e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [57][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.5436e-02 (2.4163e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [57][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.5607e-03 (2.3462e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [57][150/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.6485e-03 (2.3638e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [57][160/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5375e-02 (2.3476e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.7802e-02 (2.3507e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.2358e-02 (2.4162e-02)	Acc@1  97.66 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [57][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8656e-02 (2.4312e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [57][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3977e-02 (2.4073e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][210/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6586e-02 (2.3904e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4534e-02 (2.3623e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [57][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4170e-02 (2.3587e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [57][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.4528e-03 (2.3739e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7670e-02 (2.3701e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [57][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0742e-02 (2.4096e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][270/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2169e-02 (2.4129e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2909e-02 (2.3979e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9806e-02 (2.3994e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9958e-02 (2.4021e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8320e-02 (2.4116e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7609e-02 (2.4291e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4282e-02 (2.4205e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0136e-02 (2.4138e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0447e-02 (2.4063e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2125e-02 (2.4035e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.3389e-03 (2.3990e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2459e-02 (2.4114e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.4697e-02 (2.4158e-02)	Acc@1  97.50 ( 99.44)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.29166173934936523
## e[57]       loss.backward (sum) time: 13.773290395736694
## e[57]      optimizer.step (sum) time: 2.732522487640381
## epoch[57] training(only) time: 45.081039905548096
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 1.4502e+00 (1.4502e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.051)	Loss 1.9111e+00 (1.6873e+00)	Acc@1  66.00 ( 73.55)	Acc@5  95.00 ( 92.73)
Test: [ 20/100]	Time  0.038 ( 0.045)	Loss 1.7090e+00 (1.5924e+00)	Acc@1  71.00 ( 74.00)	Acc@5  93.00 ( 92.95)
Test: [ 30/100]	Time  0.046 ( 0.043)	Loss 2.5137e+00 (1.6525e+00)	Acc@1  63.00 ( 73.23)	Acc@5  87.00 ( 92.32)
Test: [ 40/100]	Time  0.040 ( 0.042)	Loss 1.8311e+00 (1.6213e+00)	Acc@1  70.00 ( 73.24)	Acc@5  94.00 ( 92.78)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.8350e+00 (1.6398e+00)	Acc@1  68.00 ( 72.92)	Acc@5  90.00 ( 92.33)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.6934e+00 (1.5988e+00)	Acc@1  70.00 ( 73.16)	Acc@5  95.00 ( 92.56)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7646e+00 (1.6102e+00)	Acc@1  67.00 ( 73.08)	Acc@5  91.00 ( 92.58)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0957e+00 (1.6161e+00)	Acc@1  71.00 ( 73.16)	Acc@5  92.00 ( 92.53)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 2.1855e+00 (1.5903e+00)	Acc@1  64.00 ( 73.31)	Acc@5  91.00 ( 92.62)
 * Acc@1 73.400 Acc@5 92.660
### epoch[57] execution time: 49.19808745384216
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.267 ( 0.267)	Data  0.161 ( 0.161)	Loss 2.7832e-02 (2.7832e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.112 ( 0.128)	Data  0.001 ( 0.018)	Loss 1.4091e-02 (1.7902e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.115 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.5327e-02 (1.9680e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.3636e-02 (2.0307e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 2.3087e-02 (2.0505e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.116 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.4582e-02 (2.0683e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.4114e-02 (2.1050e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.9337e-02 (2.1819e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.2683e-02 (2.2250e-02)	Acc@1  96.88 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.4740e-02 (2.1770e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2466e-02 (2.1927e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9226e-02 (2.2375e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7181e-02 (2.2897e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.6082e-02 (2.3008e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0233e-02 (2.2809e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.8544e-02 (2.2943e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.5955e-02 (2.3357e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7151e-02 (2.3010e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7014e-02 (2.2926e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0933e-02 (2.2652e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.1708e-02 (2.2630e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.9716e-03 (2.2559e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1566e-02 (2.2608e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0246e-02 (2.2531e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.9307e-02 (2.2670e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2013e-02 (2.2868e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.4201e-03 (2.2871e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7756e-02 (2.2878e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2596e-02 (2.2743e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0475e-02 (2.2826e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 ( 99.99)
Epoch: [58][300/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7023e-02 (2.2895e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [58][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5574e-02 (2.3395e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [58][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4673e-02 (2.3243e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5686e-02 (2.3234e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3557e-02 (2.3151e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6724e-02 (2.3304e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3020e-02 (2.3230e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.3955e-02 (2.3475e-02)	Acc@1  99.22 ( 99.49)	Acc@5  99.22 ( 99.99)
Epoch: [58][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1790e-02 (2.3640e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [58][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0923e-02 (2.3999e-02)	Acc@1  97.50 ( 99.47)	Acc@5 100.00 ( 99.99)
## e[58] optimizer.zero_grad (sum) time: 0.29210352897644043
## e[58]       loss.backward (sum) time: 13.813752174377441
## e[58]      optimizer.step (sum) time: 2.722982406616211
## epoch[58] training(only) time: 45.06976842880249
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 1.4795e+00 (1.4795e+00)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 2.0059e+00 (1.7119e+00)	Acc@1  65.00 ( 72.82)	Acc@5  93.00 ( 92.27)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.7598e+00 (1.5989e+00)	Acc@1  72.00 ( 73.76)	Acc@5  92.00 ( 92.71)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.4004e+00 (1.6648e+00)	Acc@1  65.00 ( 73.42)	Acc@5  87.00 ( 92.03)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.8525e+00 (1.6344e+00)	Acc@1  74.00 ( 73.68)	Acc@5  92.00 ( 92.56)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9795e+00 (1.6525e+00)	Acc@1  69.00 ( 73.39)	Acc@5  90.00 ( 92.16)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.8018e+00 (1.6075e+00)	Acc@1  70.00 ( 73.57)	Acc@5  91.00 ( 92.43)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7354e+00 (1.6161e+00)	Acc@1  64.00 ( 73.42)	Acc@5  90.00 ( 92.41)
Test: [ 80/100]	Time  0.040 ( 0.041)	Loss 2.0703e+00 (1.6185e+00)	Acc@1  69.00 ( 73.47)	Acc@5  93.00 ( 92.43)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.1895e+00 (1.5951e+00)	Acc@1  63.00 ( 73.62)	Acc@5  92.00 ( 92.49)
 * Acc@1 73.580 Acc@5 92.590
### epoch[58] execution time: 49.21108937263489
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.267 ( 0.267)	Data  0.161 ( 0.161)	Loss 6.0616e-03 (6.0616e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.110 ( 0.128)	Data  0.001 ( 0.018)	Loss 3.8879e-02 (2.1890e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.111 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.7914e-02 (2.8314e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.115 ( 0.119)	Data  0.001 ( 0.009)	Loss 7.8003e-02 (3.0959e-02)	Acc@1  97.66 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 7.3090e-03 (2.8585e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.1576e-02 (2.9114e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.116 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.0889e-02 (2.7891e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.3417e-02 (2.8050e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.4702e-02 (2.7092e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.4267e-02 (2.6773e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.3923e-03 (2.6570e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8066e-02 (2.6229e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4597e-02 (2.6206e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3023e-02 (2.5739e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.5040e-02 (2.5853e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8951e-02 (2.5557e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4979e-02 (2.5351e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7670e-02 (2.5123e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5717e-02 (2.5051e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3512e-02 (2.4958e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4155e-02 (2.4900e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5076e-02 (2.4565e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1177e-02 (2.4553e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2776e-02 (2.4916e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9577e-02 (2.4680e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9022e-02 (2.4975e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.5767e-02 (2.4824e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.2877e-02 (2.4725e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.7885e-03 (2.4494e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7908e-02 (2.4731e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5091e-02 (2.4731e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9852e-02 (2.4567e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.0985e-02 (2.4825e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7151e-02 (2.4735e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1658e-02 (2.4757e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4912e-02 (2.4776e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4839e-02 (2.4567e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8970e-02 (2.4936e-02)	Acc@1  96.88 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8417e-02 (2.4992e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6183e-02 (2.4977e-02)	Acc@1  98.75 ( 99.39)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.2902848720550537
## e[59]       loss.backward (sum) time: 13.75817584991455
## e[59]      optimizer.step (sum) time: 2.7106471061706543
## epoch[59] training(only) time: 45.04937934875488
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.4160e+00 (1.4160e+00)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.8945e+00 (1.6882e+00)	Acc@1  65.00 ( 72.55)	Acc@5  94.00 ( 92.36)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8945e+00 (1.6177e+00)	Acc@1  71.00 ( 73.19)	Acc@5  92.00 ( 92.86)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5566e+00 (1.6769e+00)	Acc@1  62.00 ( 73.06)	Acc@5  86.00 ( 92.13)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.9062e+00 (1.6489e+00)	Acc@1  71.00 ( 73.39)	Acc@5  94.00 ( 92.51)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.9805e+00 (1.6605e+00)	Acc@1  70.00 ( 73.02)	Acc@5  90.00 ( 92.24)
Test: [ 60/100]	Time  0.040 ( 0.042)	Loss 1.7246e+00 (1.6159e+00)	Acc@1  71.00 ( 73.20)	Acc@5  92.00 ( 92.54)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7637e+00 (1.6291e+00)	Acc@1  66.00 ( 73.08)	Acc@5  92.00 ( 92.51)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 2.0078e+00 (1.6337e+00)	Acc@1  71.00 ( 73.14)	Acc@5  93.00 ( 92.49)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2402e+00 (1.6117e+00)	Acc@1  66.00 ( 73.30)	Acc@5  91.00 ( 92.57)
 * Acc@1 73.350 Acc@5 92.600
### epoch[59] execution time: 49.18258595466614
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.283 ( 0.283)	Data  0.168 ( 0.168)	Loss 1.4641e-02 (1.4641e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.114 ( 0.130)	Data  0.001 ( 0.018)	Loss 4.0169e-03 (2.5454e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.114 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.6541e-02 (2.1970e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.114 ( 0.120)	Data  0.001 ( 0.009)	Loss 2.2644e-02 (2.1359e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.008)	Loss 2.3560e-02 (2.3386e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.3148e-02 (2.4327e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.4612e-02 (2.4225e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.2232e-02 (2.3745e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.8483e-02 (2.4061e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.2476e-02 (2.3444e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5373e-02 (2.2313e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.6591e-02 (2.2395e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.3560e-02 (2.2715e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6479e-02 (2.2515e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9938e-02 (2.2719e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [60][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0121e-02 (2.2991e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [60][160/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0182e-02 (2.2707e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4763e-02 (2.2541e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2802e-02 (2.2047e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4435e-02 (2.2485e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8585e-02 (2.2398e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.123 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5131e-02 (2.2102e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2812e-02 (2.2149e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0864e-02 (2.2013e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5526e-02 (2.1807e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4473e-02 (2.1587e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0691e-02 (2.1582e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9028e-02 (2.1510e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4648e-02 (2.1449e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6083e-02 (2.1369e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9409e-02 (2.1240e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3376e-02 (2.1054e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3600e-02 (2.1009e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.7596e-03 (2.1001e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5040e-02 (2.0998e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.1449e-03 (2.1011e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5955e-02 (2.1074e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8143e-02 (2.1097e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5116e-02 (2.1081e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2324e-02 (2.0988e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.2972395420074463
## e[60]       loss.backward (sum) time: 13.819098949432373
## e[60]      optimizer.step (sum) time: 2.7478995323181152
## epoch[60] training(only) time: 45.117050886154175
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.4502e+00 (1.4502e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.9160e+00 (1.6884e+00)	Acc@1  63.00 ( 73.18)	Acc@5  93.00 ( 92.64)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.8555e+00 (1.6180e+00)	Acc@1  71.00 ( 73.76)	Acc@5  92.00 ( 92.90)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5547e+00 (1.6809e+00)	Acc@1  63.00 ( 73.23)	Acc@5  86.00 ( 92.16)
Test: [ 40/100]	Time  0.038 ( 0.042)	Loss 1.8643e+00 (1.6534e+00)	Acc@1  70.00 ( 73.54)	Acc@5  94.00 ( 92.61)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9043e+00 (1.6645e+00)	Acc@1  70.00 ( 73.25)	Acc@5  90.00 ( 92.29)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7393e+00 (1.6185e+00)	Acc@1  70.00 ( 73.51)	Acc@5  92.00 ( 92.59)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.7393e+00 (1.6318e+00)	Acc@1  65.00 ( 73.35)	Acc@5  93.00 ( 92.55)
Test: [ 80/100]	Time  0.040 ( 0.041)	Loss 1.9932e+00 (1.6372e+00)	Acc@1  71.00 ( 73.36)	Acc@5  94.00 ( 92.54)
Test: [ 90/100]	Time  0.040 ( 0.041)	Loss 2.2656e+00 (1.6146e+00)	Acc@1  64.00 ( 73.45)	Acc@5  91.00 ( 92.58)
 * Acc@1 73.520 Acc@5 92.660
### epoch[60] execution time: 49.21902871131897
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.264 ( 0.264)	Data  0.152 ( 0.152)	Loss 2.0248e-02 (2.0248e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.017)	Loss 9.4833e-03 (1.7179e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.111 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.1164e-02 (2.1844e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 3.0182e-02 (2.0928e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.3214e-02 (2.0978e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 7.6828e-03 (2.0431e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.116 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.3743e-02 (2.0388e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.3264e-02 (2.0885e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 4.8523e-03 (2.1326e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6800e-02 (2.1552e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5129e-02 (2.1090e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9266e-02 (2.0825e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.9917e-02 (2.0841e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5076e-02 (2.0693e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.108 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1215e-02 (2.0516e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2400e-02 (2.0546e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2324e-02 (2.0281e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6663e-02 (2.0184e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4063e-02 (2.0358e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0309e-02 (2.0234e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1688e-02 (2.0258e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6205e-02 (2.0182e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0981e-02 (2.0062e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5167e-02 (2.0201e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.2786e-02 (2.0251e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5575e-03 (2.0262e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8855e-03 (2.0302e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5772e-02 (2.0282e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.8430e-03 (2.0097e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3191e-02 (2.0066e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.7809e-03 (2.0027e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2806e-02 (2.0258e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5129e-02 (2.0288e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0017e-02 (2.0360e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5192e-02 (2.0304e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9287e-02 (2.0375e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.9874e-03 (2.0397e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5787e-02 (2.0398e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1820e-02 (2.0349e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.1758e-02 (2.0432e-02)	Acc@1  98.75 ( 99.55)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.2932109832763672
## e[61]       loss.backward (sum) time: 13.80442214012146
## e[61]      optimizer.step (sum) time: 2.727224588394165
## epoch[61] training(only) time: 45.0894091129303
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.3906e+00 (1.3906e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.038 ( 0.052)	Loss 1.8428e+00 (1.6617e+00)	Acc@1  68.00 ( 73.64)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.8711e+00 (1.6065e+00)	Acc@1  72.00 ( 73.81)	Acc@5  93.00 ( 92.90)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.4707e+00 (1.6699e+00)	Acc@1  65.00 ( 73.23)	Acc@5  84.00 ( 92.19)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.9619e+00 (1.6479e+00)	Acc@1  69.00 ( 73.51)	Acc@5  94.00 ( 92.51)
Test: [ 50/100]	Time  0.040 ( 0.042)	Loss 1.9316e+00 (1.6562e+00)	Acc@1  69.00 ( 73.10)	Acc@5  91.00 ( 92.24)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7510e+00 (1.6104e+00)	Acc@1  70.00 ( 73.28)	Acc@5  93.00 ( 92.56)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7812e+00 (1.6229e+00)	Acc@1  66.00 ( 73.14)	Acc@5  92.00 ( 92.52)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0371e+00 (1.6284e+00)	Acc@1  71.00 ( 73.15)	Acc@5  93.00 ( 92.52)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.2441e+00 (1.6046e+00)	Acc@1  66.00 ( 73.26)	Acc@5  92.00 ( 92.60)
 * Acc@1 73.360 Acc@5 92.670
### epoch[61] execution time: 49.19581627845764
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.278 ( 0.278)	Data  0.160 ( 0.160)	Loss 2.3102e-02 (2.3102e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.115 ( 0.129)	Data  0.001 ( 0.018)	Loss 4.0680e-02 (2.6604e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.115 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.0584e-02 (2.2632e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.110 ( 0.120)	Data  0.001 ( 0.009)	Loss 1.6235e-02 (2.1234e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.111 ( 0.119)	Data  0.001 ( 0.008)	Loss 2.0981e-02 (2.1112e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 4.9225e-02 (2.1885e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.2253e-02 (2.1110e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.1921e-02 (2.0915e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.3295e-02 (2.1659e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.2094e-02 (2.1397e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1032e-02 (2.1346e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4526e-02 (2.1023e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0178e-02 (2.1109e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8997e-02 (2.0786e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1932e-02 (2.0792e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.7313e-02 (2.1460e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6373e-02 (2.1409e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9852e-02 (2.1446e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1650e-02 (2.1038e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0551e-02 (2.1026e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6205e-02 (2.0957e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.1870e-02 (2.0953e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.8888e-03 (2.0683e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4175e-02 (2.0409e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.1400e-03 (2.0278e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.6054e-03 (2.0166e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3947e-02 (2.0030e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.8937e-03 (2.0242e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0147e-02 (2.0291e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3245e-02 (2.0085e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4452e-03 (1.9997e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5360e-02 (2.0184e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.5532e-02 (2.0190e-02)	Acc@1  99.22 ( 99.53)	Acc@5  99.22 (100.00)
Epoch: [62][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4076e-02 (2.0261e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4580e-02 (2.0167e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.9885e-03 (2.0108e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4320e-02 (2.0090e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.6936e-02 (2.0161e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6159e-02 (2.0065e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7811e-02 (2.0166e-02)	Acc@1  98.75 ( 99.52)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.2959585189819336
## e[62]       loss.backward (sum) time: 13.808213233947754
## e[62]      optimizer.step (sum) time: 2.715643882751465
## epoch[62] training(only) time: 45.11123442649841
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.4326e+00 (1.4326e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.8740e+00 (1.6809e+00)	Acc@1  65.00 ( 72.91)	Acc@5  93.00 ( 92.45)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.8457e+00 (1.6079e+00)	Acc@1  71.00 ( 73.57)	Acc@5  91.00 ( 92.71)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5312e+00 (1.6667e+00)	Acc@1  62.00 ( 72.87)	Acc@5  85.00 ( 92.06)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8848e+00 (1.6410e+00)	Acc@1  70.00 ( 73.34)	Acc@5  94.00 ( 92.49)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.8926e+00 (1.6497e+00)	Acc@1  70.00 ( 73.08)	Acc@5  91.00 ( 92.22)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7939e+00 (1.6038e+00)	Acc@1  68.00 ( 73.28)	Acc@5  91.00 ( 92.51)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7383e+00 (1.6171e+00)	Acc@1  67.00 ( 73.18)	Acc@5  92.00 ( 92.51)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0098e+00 (1.6223e+00)	Acc@1  71.00 ( 73.21)	Acc@5  93.00 ( 92.46)
Test: [ 90/100]	Time  0.040 ( 0.041)	Loss 2.2676e+00 (1.5992e+00)	Acc@1  66.00 ( 73.31)	Acc@5  91.00 ( 92.55)
 * Acc@1 73.380 Acc@5 92.630
### epoch[62] execution time: 49.25069451332092
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.278 ( 0.278)	Data  0.167 ( 0.167)	Loss 2.0157e-02 (2.0157e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.116 ( 0.129)	Data  0.001 ( 0.018)	Loss 4.5441e-02 (2.0247e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.115 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.9012e-02 (1.9678e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.113 ( 0.120)	Data  0.001 ( 0.009)	Loss 1.3336e-02 (1.8661e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.2985e-02 (1.8104e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 7.2403e-03 (1.8964e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.1871e-02 (1.9730e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 9.7809e-03 (2.0855e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.5797e-02 (2.0960e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.6602e-02 (2.0579e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.4986e-03 (2.0335e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2527e-02 (1.9927e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.0720e-02 (2.0529e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.5116e-02 (2.0201e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.1393e-02 (1.9932e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7105e-02 (1.9960e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3519e-02 (1.9721e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6678e-02 (1.9927e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.1113e-02 (1.9996e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1421e-02 (1.9740e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.0109e-03 (1.9588e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.1759e-02 (1.9577e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4877e-02 (1.9672e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0689e-02 (1.9558e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8412e-02 (1.9667e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1833e-02 (1.9795e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1879e-02 (2.0005e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4610e-02 (1.9870e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2136e-03 (1.9610e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9927e-02 (1.9648e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5472e-02 (1.9693e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2784e-03 (1.9699e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7868e-02 (1.9730e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2627e-02 (1.9820e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8588e-03 (1.9688e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3954e-02 (1.9614e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0004e-02 (1.9647e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4857e-02 (1.9750e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5114e-02 (1.9637e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.4596e-02 (1.9666e-02)	Acc@1  97.50 ( 99.56)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.2977895736694336
## e[63]       loss.backward (sum) time: 13.855311632156372
## e[63]      optimizer.step (sum) time: 2.7408881187438965
## epoch[63] training(only) time: 45.12202286720276
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.4307e+00 (1.4307e+00)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 1.9062e+00 (1.6883e+00)	Acc@1  64.00 ( 72.73)	Acc@5  93.00 ( 92.36)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8838e+00 (1.6127e+00)	Acc@1  71.00 ( 73.43)	Acc@5  93.00 ( 92.81)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 2.5508e+00 (1.6790e+00)	Acc@1  63.00 ( 72.87)	Acc@5  83.00 ( 92.00)
Test: [ 40/100]	Time  0.041 ( 0.043)	Loss 1.9316e+00 (1.6540e+00)	Acc@1  69.00 ( 73.24)	Acc@5  93.00 ( 92.41)
Test: [ 50/100]	Time  0.040 ( 0.042)	Loss 1.9033e+00 (1.6630e+00)	Acc@1  70.00 ( 73.04)	Acc@5  91.00 ( 92.24)
Test: [ 60/100]	Time  0.037 ( 0.042)	Loss 1.7969e+00 (1.6167e+00)	Acc@1  69.00 ( 73.26)	Acc@5  92.00 ( 92.59)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7764e+00 (1.6294e+00)	Acc@1  66.00 ( 73.20)	Acc@5  92.00 ( 92.56)
Test: [ 80/100]	Time  0.040 ( 0.041)	Loss 2.0312e+00 (1.6342e+00)	Acc@1  70.00 ( 73.17)	Acc@5  92.00 ( 92.56)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.2852e+00 (1.6117e+00)	Acc@1  67.00 ( 73.36)	Acc@5  91.00 ( 92.62)
 * Acc@1 73.540 Acc@5 92.680
### epoch[63] execution time: 49.25457787513733
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.270 ( 0.270)	Data  0.154 ( 0.154)	Loss 6.5956e-03 (6.5956e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.112 ( 0.128)	Data  0.001 ( 0.017)	Loss 1.9775e-02 (1.6252e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.011)	Loss 3.0640e-02 (1.8945e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 3.8666e-02 (1.9189e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.116 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.9409e-02 (2.0424e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 4.5715e-02 (2.0216e-02)	Acc@1  97.66 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.0368e-02 (2.0517e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.0567e-02 (2.0523e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.2980e-02 (2.0404e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.2589e-02 (1.9934e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1635e-02 (1.9425e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7456e-02 (1.8997e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7868e-02 (1.9097e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4420e-02 (1.8522e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.8190e-03 (1.8443e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.9622e-02 (1.8617e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7426e-02 (1.8816e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1551e-02 (1.8863e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6001e-02 (1.8915e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9297e-02 (1.8989e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4229e-02 (1.9004e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8046e-02 (1.9315e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9938e-02 (1.9364e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.0643e-03 (1.9419e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.0716e-02 (1.9527e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4801e-02 (1.9333e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.8990e-02 (1.9442e-02)	Acc@1  97.66 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.3765e-03 (1.9237e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6031e-02 (1.9173e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7883e-02 (1.9161e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6342e-02 (1.9134e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.6594e-03 (1.9055e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2501e-02 (1.8984e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0142e-02 (1.9130e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0548e-02 (1.9036e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1586e-02 (1.9201e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9547e-02 (1.9061e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5495e-02 (1.9039e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.3854e-02 (1.9263e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.7861e-02 (1.9371e-02)	Acc@1  98.75 ( 99.55)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.2899763584136963
## e[64]       loss.backward (sum) time: 13.834893703460693
## e[64]      optimizer.step (sum) time: 2.741939067840576
## epoch[64] training(only) time: 45.05973243713379
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.4287e+00 (1.4287e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.054)	Loss 1.9141e+00 (1.7085e+00)	Acc@1  65.00 ( 72.55)	Acc@5  95.00 ( 92.64)
Test: [ 20/100]	Time  0.040 ( 0.047)	Loss 1.8545e+00 (1.6251e+00)	Acc@1  71.00 ( 73.52)	Acc@5  93.00 ( 92.90)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5234e+00 (1.6821e+00)	Acc@1  63.00 ( 72.97)	Acc@5  85.00 ( 92.26)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8389e+00 (1.6523e+00)	Acc@1  71.00 ( 73.39)	Acc@5  94.00 ( 92.66)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9482e+00 (1.6637e+00)	Acc@1  70.00 ( 73.06)	Acc@5  91.00 ( 92.25)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.7256e+00 (1.6182e+00)	Acc@1  70.00 ( 73.25)	Acc@5  92.00 ( 92.56)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7715e+00 (1.6296e+00)	Acc@1  65.00 ( 73.17)	Acc@5  93.00 ( 92.56)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0117e+00 (1.6344e+00)	Acc@1  70.00 ( 73.26)	Acc@5  92.00 ( 92.53)
Test: [ 90/100]	Time  0.041 ( 0.041)	Loss 2.2930e+00 (1.6099e+00)	Acc@1  67.00 ( 73.45)	Acc@5  92.00 ( 92.60)
 * Acc@1 73.590 Acc@5 92.670
### epoch[64] execution time: 49.1957528591156
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.276 ( 0.276)	Data  0.158 ( 0.158)	Loss 2.0996e-02 (2.0996e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.114 ( 0.128)	Data  0.001 ( 0.017)	Loss 2.0752e-02 (1.9404e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.2283e-02 (2.1220e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.7593e-02 (1.8817e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 9.1019e-03 (1.7844e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.7151e-02 (1.8555e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.9215e-02 (1.9076e-02)	Acc@1  97.66 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.2339e-02 (1.8711e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.1475e-02 (1.8919e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.1780e-02 (1.8803e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2919e-02 (1.8517e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9388e-02 (1.8533e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5732e-02 (1.8651e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.5436e-02 (1.8856e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1932e-02 (1.8661e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.112 ( 0.116)	Data  0.003 ( 0.005)	Loss 1.1917e-02 (1.8728e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.9117e-03 (1.8587e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3438e-02 (1.8688e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1551e-02 (1.8638e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7502e-02 (1.8943e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2278e-02 (1.8917e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.6346e-02 (1.8973e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7273e-02 (1.8946e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3672e-02 (1.9242e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0294e-02 (1.9152e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.6981e-03 (1.9142e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.1310e-02 (1.9284e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1708e-02 (1.9328e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2657e-02 (1.9383e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4244e-02 (1.9283e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6215e-02 (1.9425e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2746e-03 (1.9522e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.120 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.0741e-02 (1.9673e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5192e-02 (1.9496e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0597e-02 (1.9357e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0544e-02 (1.9605e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3039e-02 (1.9485e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8967e-02 (1.9544e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2505e-02 (1.9493e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0930e-02 (1.9493e-02)	Acc@1  98.75 ( 99.61)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.2915477752685547
## e[65]       loss.backward (sum) time: 13.802756071090698
## e[65]      optimizer.step (sum) time: 2.743293285369873
## epoch[65] training(only) time: 45.065723180770874
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.4414e+00 (1.4414e+00)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.8906e+00 (1.7077e+00)	Acc@1  64.00 ( 73.36)	Acc@5  95.00 ( 92.55)
Test: [ 20/100]	Time  0.040 ( 0.047)	Loss 1.8389e+00 (1.6215e+00)	Acc@1  72.00 ( 74.05)	Acc@5  92.00 ( 92.86)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 2.6328e+00 (1.6911e+00)	Acc@1  60.00 ( 73.16)	Acc@5  84.00 ( 92.06)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.9121e+00 (1.6628e+00)	Acc@1  70.00 ( 73.49)	Acc@5  94.00 ( 92.49)
Test: [ 50/100]	Time  0.038 ( 0.043)	Loss 1.9551e+00 (1.6727e+00)	Acc@1  70.00 ( 73.20)	Acc@5  91.00 ( 92.24)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7725e+00 (1.6283e+00)	Acc@1  69.00 ( 73.31)	Acc@5  93.00 ( 92.51)
Test: [ 70/100]	Time  0.037 ( 0.042)	Loss 1.7559e+00 (1.6385e+00)	Acc@1  64.00 ( 73.27)	Acc@5  93.00 ( 92.48)
Test: [ 80/100]	Time  0.045 ( 0.041)	Loss 2.0117e+00 (1.6413e+00)	Acc@1  71.00 ( 73.31)	Acc@5  93.00 ( 92.46)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2578e+00 (1.6172e+00)	Acc@1  67.00 ( 73.47)	Acc@5  91.00 ( 92.55)
 * Acc@1 73.570 Acc@5 92.610
### epoch[65] execution time: 49.223817586898804
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.281 ( 0.281)	Data  0.165 ( 0.165)	Loss 2.4338e-02 (2.4338e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.110 ( 0.129)	Data  0.001 ( 0.018)	Loss 4.9866e-02 (1.9813e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 5.8746e-03 (1.8576e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.110 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.5350e-02 (1.9833e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.110 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.7303e-02 (2.0178e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.1301e-02 (2.0377e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.1955e-02 (2.0787e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.7242e-02 (2.0419e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.0685e-02 (1.9974e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.4725e-02 (1.9286e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.1907e-03 (1.9052e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.9498e-03 (1.8637e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6739e-02 (1.8744e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1513e-02 (1.8836e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0933e-02 (1.9045e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.9133e-02 (1.9133e-02)	Acc@1  97.66 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0635e-02 (1.9088e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7548e-02 (1.8911e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8330e-02 (1.8966e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3756e-02 (1.9217e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6602e-02 (1.9202e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.7133e-03 (1.9071e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4603e-02 (1.9234e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.9395e-03 (1.9081e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6337e-02 (1.9009e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.9798e-03 (1.9055e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.8065e-02 (1.9172e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3016e-02 (1.9035e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3819e-02 (1.9061e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3725e-02 (1.9269e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.8419e-03 (1.9557e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.6970e-03 (1.9550e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3203e-02 (1.9554e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.1321e-02 (1.9639e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1002e-02 (1.9610e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6719e-03 (1.9535e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9287e-02 (1.9680e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0582e-02 (1.9546e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3939e-02 (1.9398e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0872e-02 (1.9325e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.29636311531066895
## e[66]       loss.backward (sum) time: 13.812767028808594
## e[66]      optimizer.step (sum) time: 2.756904125213623
## epoch[66] training(only) time: 45.04429316520691
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.4170e+00 (1.4170e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.055)	Loss 1.9199e+00 (1.6936e+00)	Acc@1  64.00 ( 72.91)	Acc@5  94.00 ( 92.27)
Test: [ 20/100]	Time  0.039 ( 0.048)	Loss 1.8818e+00 (1.6236e+00)	Acc@1  72.00 ( 73.57)	Acc@5  92.00 ( 92.67)
Test: [ 30/100]	Time  0.038 ( 0.045)	Loss 2.4941e+00 (1.6866e+00)	Acc@1  65.00 ( 73.13)	Acc@5  85.00 ( 92.03)
Test: [ 40/100]	Time  0.040 ( 0.043)	Loss 1.9219e+00 (1.6572e+00)	Acc@1  71.00 ( 73.46)	Acc@5  93.00 ( 92.39)
Test: [ 50/100]	Time  0.040 ( 0.043)	Loss 1.8994e+00 (1.6643e+00)	Acc@1  68.00 ( 73.18)	Acc@5  91.00 ( 92.18)
Test: [ 60/100]	Time  0.041 ( 0.042)	Loss 1.7646e+00 (1.6186e+00)	Acc@1  69.00 ( 73.34)	Acc@5  93.00 ( 92.49)
Test: [ 70/100]	Time  0.039 ( 0.042)	Loss 1.7402e+00 (1.6303e+00)	Acc@1  66.00 ( 73.31)	Acc@5  93.00 ( 92.45)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0312e+00 (1.6346e+00)	Acc@1  70.00 ( 73.37)	Acc@5  93.00 ( 92.37)
Test: [ 90/100]	Time  0.040 ( 0.041)	Loss 2.2715e+00 (1.6096e+00)	Acc@1  68.00 ( 73.56)	Acc@5  91.00 ( 92.46)
 * Acc@1 73.600 Acc@5 92.530
### epoch[66] execution time: 49.21877455711365
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.271 ( 0.271)	Data  0.164 ( 0.164)	Loss 8.6365e-03 (8.6365e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.116 ( 0.129)	Data  0.001 ( 0.018)	Loss 2.1973e-02 (2.3571e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.110 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.4277e-02 (2.2393e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.114 ( 0.120)	Data  0.001 ( 0.009)	Loss 2.0782e-02 (2.0412e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 8.5983e-03 (2.0207e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.3481e-02 (2.0283e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.0959e-03 (2.0095e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.0584e-02 (1.9343e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 9.3765e-03 (1.9605e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.7258e-02 (1.9709e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3474e-02 (1.9782e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.5825e-03 (1.9735e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2146e-02 (1.9769e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0096e-02 (1.9853e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4168e-02 (1.9971e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.4686e-03 (1.9445e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0017e-02 (1.9352e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3924e-02 (1.9807e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1650e-02 (1.9729e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8890e-02 (1.9753e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.8201e-03 (1.9669e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5330e-02 (1.9721e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.9469e-02 (1.9982e-02)	Acc@1  97.66 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.7506e-02 (1.9899e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6510e-02 (2.0003e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7792e-02 (1.9973e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4412e-02 (1.9949e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1597e-02 (1.9749e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6718e-02 (1.9874e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0605e-02 (1.9676e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8143e-02 (1.9666e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.9062e-02 (1.9739e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4610e-02 (1.9609e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9012e-02 (1.9699e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.4067e-02 (1.9691e-02)	Acc@1  97.66 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.0915e-03 (1.9571e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1513e-02 (1.9625e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4404e-02 (1.9459e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3374e-02 (1.9425e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.6792e-02 (1.9489e-02)	Acc@1  97.50 ( 99.56)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.29534173011779785
## e[67]       loss.backward (sum) time: 13.811840295791626
## e[67]      optimizer.step (sum) time: 2.744393825531006
## epoch[67] training(only) time: 45.11716914176941
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.4209e+00 (1.4209e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 1.9189e+00 (1.6941e+00)	Acc@1  69.00 ( 73.64)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.8545e+00 (1.6160e+00)	Acc@1  72.00 ( 73.86)	Acc@5  91.00 ( 92.86)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.5176e+00 (1.6743e+00)	Acc@1  64.00 ( 73.19)	Acc@5  85.00 ( 92.06)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.9082e+00 (1.6435e+00)	Acc@1  70.00 ( 73.61)	Acc@5  94.00 ( 92.46)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.9365e+00 (1.6530e+00)	Acc@1  69.00 ( 73.27)	Acc@5  91.00 ( 92.24)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7451e+00 (1.6101e+00)	Acc@1  70.00 ( 73.36)	Acc@5  93.00 ( 92.59)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.7451e+00 (1.6209e+00)	Acc@1  65.00 ( 73.38)	Acc@5  93.00 ( 92.56)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0000e+00 (1.6236e+00)	Acc@1  71.00 ( 73.44)	Acc@5  93.00 ( 92.53)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 2.2363e+00 (1.5980e+00)	Acc@1  67.00 ( 73.58)	Acc@5  92.00 ( 92.65)
 * Acc@1 73.730 Acc@5 92.710
### epoch[67] execution time: 49.23846745491028
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.275 ( 0.275)	Data  0.162 ( 0.162)	Loss 2.2308e-02 (2.2308e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.111 ( 0.128)	Data  0.001 ( 0.018)	Loss 1.7227e-02 (2.0397e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.115 ( 0.122)	Data  0.001 ( 0.011)	Loss 8.2016e-03 (1.7123e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.115 ( 0.120)	Data  0.001 ( 0.009)	Loss 9.7198e-03 (1.8591e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.8845e-02 (2.0217e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 4.5349e-02 (1.9595e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.5228e-02 (1.9434e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.119 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.0727e-02 (1.9645e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.0902e-02 (1.8741e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.5291e-03 (1.8767e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [68][100/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2466e-02 (1.8475e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [68][110/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9485e-02 (1.8628e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [68][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9846e-02 (1.8846e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [68][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.7130e-02 (1.9085e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [68][140/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9562e-02 (1.9290e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [68][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9653e-02 (1.9447e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [68][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8417e-02 (1.9049e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4275e-02 (1.8843e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1240e-02 (1.8745e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8692e-02 (1.8797e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7609e-02 (1.9282e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0918e-02 (1.9511e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3071e-02 (1.9506e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5297e-02 (1.9204e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4277e-02 (1.9151e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2568e-02 (1.9272e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0203e-02 (1.9151e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1408e-02 (1.9558e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3565e-02 (1.9547e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2314e-02 (1.9565e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6872e-03 (1.9554e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [68][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4191e-02 (1.9410e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [68][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5101e-02 (1.9337e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7792e-02 (1.9204e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3535e-02 (1.9344e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.4376e-03 (1.9245e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3329e-02 (1.9199e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2863e-02 (1.9237e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.1208e-02 (1.9352e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9485e-02 (1.9196e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.2958691120147705
## e[68]       loss.backward (sum) time: 13.852849006652832
## e[68]      optimizer.step (sum) time: 2.7447285652160645
## epoch[68] training(only) time: 45.07162046432495
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.4023e+00 (1.4023e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.042 ( 0.053)	Loss 1.9551e+00 (1.6784e+00)	Acc@1  66.00 ( 72.91)	Acc@5  92.00 ( 92.18)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.8525e+00 (1.6052e+00)	Acc@1  73.00 ( 73.81)	Acc@5  92.00 ( 92.62)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5293e+00 (1.6727e+00)	Acc@1  64.00 ( 73.13)	Acc@5  85.00 ( 91.90)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8828e+00 (1.6454e+00)	Acc@1  71.00 ( 73.46)	Acc@5  94.00 ( 92.37)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.9014e+00 (1.6564e+00)	Acc@1  70.00 ( 73.24)	Acc@5  91.00 ( 92.14)
Test: [ 60/100]	Time  0.038 ( 0.041)	Loss 1.7354e+00 (1.6087e+00)	Acc@1  71.00 ( 73.46)	Acc@5  93.00 ( 92.46)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7598e+00 (1.6190e+00)	Acc@1  67.00 ( 73.44)	Acc@5  92.00 ( 92.44)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0449e+00 (1.6231e+00)	Acc@1  70.00 ( 73.48)	Acc@5  93.00 ( 92.42)
Test: [ 90/100]	Time  0.038 ( 0.040)	Loss 2.2305e+00 (1.5978e+00)	Acc@1  66.00 ( 73.63)	Acc@5  92.00 ( 92.55)
 * Acc@1 73.670 Acc@5 92.650
### epoch[68] execution time: 49.167076587677
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.275 ( 0.275)	Data  0.165 ( 0.165)	Loss 9.7504e-03 (9.7504e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.018)	Loss 2.4521e-02 (2.1328e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.011)	Loss 3.4149e-02 (2.1913e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.009)	Loss 2.2507e-02 (2.0027e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 2.1545e-02 (2.0567e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.2537e-02 (2.0229e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.5950e-02 (2.0167e-02)	Acc@1  97.66 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.9379e-02 (1.9431e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.2562e-02 (1.9710e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.5360e-02 (1.9495e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2568e-02 (1.9777e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.8169e-03 (1.9161e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2980e-02 (1.9139e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.3193e-02 (1.9465e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3077e-02 (1.9232e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.0323e-02 (1.9496e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3895e-02 (1.9260e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4124e-02 (1.9457e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0157e-02 (1.9665e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.3010e-02 (1.9613e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.5898e-02 (1.9677e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2003e-02 (1.9730e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6373e-02 (1.9826e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4328e-02 (1.9809e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2171e-02 (1.9777e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.3357e-03 (1.9842e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1589e-02 (1.9714e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5717e-02 (1.9564e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3100e-02 (1.9338e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3356e-02 (1.9196e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6000e-03 (1.9079e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.4403e-03 (1.8963e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3092e-02 (1.8856e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7842e-02 (1.8936e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7858e-03 (1.9000e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0065e-02 (1.8873e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.1068e-03 (1.8745e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9297e-02 (1.8721e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6418e-02 (1.8588e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.6580e-02 (1.8527e-02)	Acc@1  97.50 ( 99.65)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.2920877933502197
## e[69]       loss.backward (sum) time: 13.827225923538208
## e[69]      optimizer.step (sum) time: 2.7218174934387207
## epoch[69] training(only) time: 45.05887413024902
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.4316e+00 (1.4316e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.8877e+00 (1.6881e+00)	Acc@1  65.00 ( 72.36)	Acc@5  92.00 ( 92.09)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.8330e+00 (1.6113e+00)	Acc@1  72.00 ( 73.33)	Acc@5  91.00 ( 92.52)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.4961e+00 (1.6700e+00)	Acc@1  62.00 ( 72.71)	Acc@5  86.00 ( 92.00)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.9209e+00 (1.6444e+00)	Acc@1  72.00 ( 73.29)	Acc@5  94.00 ( 92.44)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9814e+00 (1.6554e+00)	Acc@1  69.00 ( 73.12)	Acc@5  92.00 ( 92.22)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7422e+00 (1.6128e+00)	Acc@1  70.00 ( 73.30)	Acc@5  93.00 ( 92.48)
Test: [ 70/100]	Time  0.037 ( 0.041)	Loss 1.7881e+00 (1.6244e+00)	Acc@1  66.00 ( 73.31)	Acc@5  93.00 ( 92.48)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 2.0195e+00 (1.6287e+00)	Acc@1  71.00 ( 73.44)	Acc@5  93.00 ( 92.43)
Test: [ 90/100]	Time  0.037 ( 0.041)	Loss 2.2520e+00 (1.6038e+00)	Acc@1  67.00 ( 73.55)	Acc@5  93.00 ( 92.55)
 * Acc@1 73.650 Acc@5 92.630
### epoch[69] execution time: 49.18425464630127
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.270 ( 0.270)	Data  0.158 ( 0.158)	Loss 1.6342e-02 (1.6342e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.017)	Loss 3.4332e-02 (1.8096e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.114 ( 0.121)	Data  0.001 ( 0.011)	Loss 2.3026e-02 (2.1251e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.6062e-02 (2.0813e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.8595e-02 (1.9884e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.106 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.2293e-02 (2.0316e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.4175e-02 (2.0116e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.6276e-02 (2.0507e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.3557e-02 (1.9539e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3496e-02 (1.9629e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0727e-02 (1.9808e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5732e-02 (1.9535e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1909e-02 (1.9358e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.3384e-03 (1.9109e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.0490e-03 (1.8980e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4475e-02 (1.8726e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4702e-02 (1.8624e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5930e-02 (1.8993e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.8343e-03 (1.9168e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6098e-02 (1.8974e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9608e-02 (1.8908e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.1449e-03 (1.8885e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.9128e-03 (1.8839e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8692e-02 (1.8740e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5976e-02 (1.8958e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7750e-02 (1.8827e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8433e-02 (1.8696e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6846e-02 (1.8688e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0691e-02 (1.8571e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4053e-02 (1.8554e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1459e-02 (1.8335e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9694e-02 (1.8254e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.5444e-03 (1.8371e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4796e-02 (1.8377e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0981e-02 (1.8301e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.1382e-02 (1.8512e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2064e-02 (1.8594e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3428e-02 (1.8477e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5518e-02 (1.8552e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4763e-03 (1.8461e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.2965574264526367
## e[70]       loss.backward (sum) time: 13.78690791130066
## e[70]      optimizer.step (sum) time: 2.736611843109131
## epoch[70] training(only) time: 45.07985329627991
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.4375e+00 (1.4375e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.042 ( 0.054)	Loss 1.9111e+00 (1.6873e+00)	Acc@1  66.00 ( 73.27)	Acc@5  94.00 ( 92.55)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8398e+00 (1.6103e+00)	Acc@1  71.00 ( 73.86)	Acc@5  91.00 ( 92.76)
Test: [ 30/100]	Time  0.037 ( 0.045)	Loss 2.4570e+00 (1.6699e+00)	Acc@1  65.00 ( 73.35)	Acc@5  87.00 ( 92.19)
Test: [ 40/100]	Time  0.040 ( 0.043)	Loss 1.8857e+00 (1.6405e+00)	Acc@1  72.00 ( 73.66)	Acc@5  93.00 ( 92.61)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9707e+00 (1.6525e+00)	Acc@1  69.00 ( 73.43)	Acc@5  91.00 ( 92.27)
Test: [ 60/100]	Time  0.041 ( 0.042)	Loss 1.7490e+00 (1.6067e+00)	Acc@1  67.00 ( 73.62)	Acc@5  92.00 ( 92.56)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.6992e+00 (1.6190e+00)	Acc@1  65.00 ( 73.51)	Acc@5  93.00 ( 92.51)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0039e+00 (1.6201e+00)	Acc@1  71.00 ( 73.59)	Acc@5  92.00 ( 92.44)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.2559e+00 (1.5941e+00)	Acc@1  68.00 ( 73.73)	Acc@5  92.00 ( 92.55)
 * Acc@1 73.790 Acc@5 92.600
### epoch[70] execution time: 49.19278264045715
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.269 ( 0.269)	Data  0.156 ( 0.156)	Loss 1.6174e-02 (1.6174e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.115 ( 0.128)	Data  0.001 ( 0.017)	Loss 1.5839e-02 (2.3077e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.7319e-02 (1.9641e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.009)	Loss 7.0419e-03 (1.7905e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.6983e-02 (1.7738e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 8.6487e-02 (2.0339e-02)	Acc@1  97.66 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.1118e-02 (2.0613e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 5.6076e-03 (1.9606e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.7090e-02 (1.8879e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.0121e-02 (1.8666e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8158e-02 (1.8686e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.0029e-02 (1.9081e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1787e-02 (1.9135e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0231e-02 (1.9299e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3252e-02 (1.9325e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.118 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1253e-02 (1.9693e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9806e-02 (1.9652e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0432e-02 (1.9558e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.9490e-02 (1.9449e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6052e-02 (1.9772e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.110 ( 0.116)	Data  0.002 ( 0.005)	Loss 1.7120e-02 (1.9473e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4168e-02 (1.9262e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.7252e-02 (1.9295e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.7252e-02 (1.9198e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3588e-02 (1.8972e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4915e-03 (1.8906e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5604e-02 (1.8896e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2772e-02 (1.9004e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0704e-02 (1.9024e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.5444e-03 (1.8856e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.9237e-03 (1.8849e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.5062e-03 (1.8866e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8677e-02 (1.8904e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0081e-02 (1.8824e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1088e-02 (1.8782e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4290e-02 (1.8774e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2138e-02 (1.8733e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8021e-02 (1.8859e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0239e-02 (1.9016e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.8971e-02 (1.9102e-02)	Acc@1  98.75 ( 99.60)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.2968881130218506
## e[71]       loss.backward (sum) time: 13.829823732376099
## e[71]      optimizer.step (sum) time: 2.766010046005249
## epoch[71] training(only) time: 45.1365909576416
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.4229e+00 (1.4229e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.9180e+00 (1.6878e+00)	Acc@1  66.00 ( 73.09)	Acc@5  93.00 ( 92.36)
Test: [ 20/100]	Time  0.041 ( 0.046)	Loss 1.8672e+00 (1.6151e+00)	Acc@1  71.00 ( 73.81)	Acc@5  92.00 ( 92.71)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5332e+00 (1.6809e+00)	Acc@1  64.00 ( 73.23)	Acc@5  85.00 ( 92.06)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.8887e+00 (1.6564e+00)	Acc@1  70.00 ( 73.63)	Acc@5  94.00 ( 92.59)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9121e+00 (1.6626e+00)	Acc@1  69.00 ( 73.33)	Acc@5  90.00 ( 92.29)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7959e+00 (1.6186e+00)	Acc@1  69.00 ( 73.43)	Acc@5  92.00 ( 92.57)
Test: [ 70/100]	Time  0.041 ( 0.041)	Loss 1.7773e+00 (1.6324e+00)	Acc@1  66.00 ( 73.39)	Acc@5  92.00 ( 92.52)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0195e+00 (1.6364e+00)	Acc@1  71.00 ( 73.47)	Acc@5  93.00 ( 92.52)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.3027e+00 (1.6128e+00)	Acc@1  67.00 ( 73.55)	Acc@5  91.00 ( 92.60)
 * Acc@1 73.620 Acc@5 92.690
### epoch[71] execution time: 49.28726887702942
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.270 ( 0.270)	Data  0.149 ( 0.149)	Loss 3.8544e-02 (3.8544e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.114 ( 0.129)	Data  0.001 ( 0.017)	Loss 2.1362e-02 (2.3046e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.2863e-02 (2.0005e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.115 ( 0.120)	Data  0.001 ( 0.008)	Loss 1.5381e-02 (1.9173e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.4717e-02 (1.9910e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.105 ( 0.117)	Data  0.001 ( 0.007)	Loss 4.2053e-02 (2.0236e-02)	Acc@1  97.66 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 3.7354e-02 (2.0553e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.3972e-02 (2.0161e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.6357e-02 (1.9591e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4145e-02 (1.9674e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8753e-02 (1.9342e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8021e-02 (1.8717e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5594e-02 (1.8575e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3741e-02 (1.8337e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0124e-02 (1.8365e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1208e-02 (1.8119e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.7509e-03 (1.8230e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.5117e-03 (1.7989e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1116e-02 (1.8126e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0628e-02 (1.7946e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5253e-02 (1.8031e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2079e-02 (1.7931e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.5962e-03 (1.8098e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4275e-02 (1.8227e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.0710e-02 (1.8437e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0483e-02 (1.8406e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4686e-03 (1.8481e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1317e-02 (1.8419e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0056e-02 (1.8328e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0065e-02 (1.8415e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4307e-02 (1.8453e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7761e-02 (1.8363e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0254e-02 (1.8190e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1515e-02 (1.8136e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2842e-02 (1.8020e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1317e-02 (1.8098e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.3515e-03 (1.7958e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0223e-02 (1.7878e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4420e-02 (1.7936e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.0222e-02 (1.8016e-02)	Acc@1  98.75 ( 99.66)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.29343366622924805
## e[72]       loss.backward (sum) time: 13.835628747940063
## e[72]      optimizer.step (sum) time: 2.757967233657837
## epoch[72] training(only) time: 45.06440854072571
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.4316e+00 (1.4316e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.8701e+00 (1.6661e+00)	Acc@1  64.00 ( 72.73)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.8262e+00 (1.5915e+00)	Acc@1  73.00 ( 73.71)	Acc@5  92.00 ( 92.86)
Test: [ 30/100]	Time  0.039 ( 0.045)	Loss 2.4941e+00 (1.6499e+00)	Acc@1  65.00 ( 73.23)	Acc@5  86.00 ( 92.35)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.8691e+00 (1.6255e+00)	Acc@1  71.00 ( 73.59)	Acc@5  93.00 ( 92.68)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9346e+00 (1.6387e+00)	Acc@1  70.00 ( 73.35)	Acc@5  91.00 ( 92.43)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7676e+00 (1.5942e+00)	Acc@1  71.00 ( 73.52)	Acc@5  93.00 ( 92.74)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.8027e+00 (1.6068e+00)	Acc@1  65.00 ( 73.42)	Acc@5  92.00 ( 92.70)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.9922e+00 (1.6104e+00)	Acc@1  71.00 ( 73.44)	Acc@5  93.00 ( 92.72)
Test: [ 90/100]	Time  0.042 ( 0.041)	Loss 2.2617e+00 (1.5869e+00)	Acc@1  65.00 ( 73.57)	Acc@5  91.00 ( 92.76)
 * Acc@1 73.640 Acc@5 92.850
### epoch[72] execution time: 49.22468972206116
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.261 ( 0.261)	Data  0.155 ( 0.155)	Loss 9.6512e-03 (9.6512e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.110 ( 0.127)	Data  0.001 ( 0.017)	Loss 2.2095e-02 (1.5920e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.114 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.9211e-02 (1.6617e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.0513e-02 (1.9534e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.5436e-02 (1.9409e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 4.1687e-02 (1.9455e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.0475e-02 (1.9632e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.5345e-02 (1.9323e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.1482e-03 (1.9771e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0620e-02 (1.9754e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.7933e-02 (1.9443e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9236e-02 (1.9312e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1032e-02 (1.9290e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6327e-02 (1.9774e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5635e-02 (1.9773e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4954e-02 (1.9518e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.6043e-03 (1.9357e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4445e-02 (1.9239e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2623e-02 (1.9251e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.1281e-02 (1.9287e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2125e-02 (1.9217e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2733e-02 (1.9187e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4576e-02 (1.9353e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3458e-02 (1.9336e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2115e-02 (1.9246e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3275e-02 (1.9411e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5541e-02 (1.9351e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2932e-02 (1.9182e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3718e-02 (1.9037e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4992e-02 (1.9091e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0228e-02 (1.8972e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.8472e-02 (1.8968e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3695e-02 (1.8888e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1314e-02 (1.8697e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.6387e-02 (1.8634e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2959e-02 (1.8710e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7573e-02 (1.8821e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0368e-02 (1.8792e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7479e-03 (1.8725e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.3374e-02 (1.8756e-02)	Acc@1  97.50 ( 99.60)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.29190874099731445
## e[73]       loss.backward (sum) time: 13.830209255218506
## e[73]      optimizer.step (sum) time: 2.7415692806243896
## epoch[73] training(only) time: 45.0871639251709
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.4229e+00 (1.4229e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.053)	Loss 1.8965e+00 (1.6858e+00)	Acc@1  68.00 ( 73.36)	Acc@5  94.00 ( 92.73)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.8750e+00 (1.6067e+00)	Acc@1  71.00 ( 73.76)	Acc@5  92.00 ( 92.86)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.5488e+00 (1.6734e+00)	Acc@1  63.00 ( 73.16)	Acc@5  85.00 ( 91.94)
Test: [ 40/100]	Time  0.039 ( 0.042)	Loss 1.8984e+00 (1.6482e+00)	Acc@1  72.00 ( 73.49)	Acc@5  94.00 ( 92.44)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9502e+00 (1.6555e+00)	Acc@1  70.00 ( 73.22)	Acc@5  90.00 ( 92.18)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7783e+00 (1.6100e+00)	Acc@1  69.00 ( 73.39)	Acc@5  92.00 ( 92.52)
Test: [ 70/100]	Time  0.041 ( 0.041)	Loss 1.7334e+00 (1.6228e+00)	Acc@1  65.00 ( 73.37)	Acc@5  92.00 ( 92.51)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0508e+00 (1.6263e+00)	Acc@1  72.00 ( 73.42)	Acc@5  93.00 ( 92.51)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 2.2383e+00 (1.6018e+00)	Acc@1  66.00 ( 73.49)	Acc@5  92.00 ( 92.63)
 * Acc@1 73.630 Acc@5 92.700
### epoch[73] execution time: 49.182501554489136
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.270 ( 0.270)	Data  0.159 ( 0.159)	Loss 9.8495e-03 (9.8495e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.112 ( 0.129)	Data  0.001 ( 0.018)	Loss 1.2230e-02 (1.9086e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.112 ( 0.122)	Data  0.001 ( 0.011)	Loss 9.2468e-03 (1.8881e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.009)	Loss 5.7030e-03 (1.8574e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.3023e-02 (1.8912e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.0536e-02 (1.8502e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.3252e-02 (1.7534e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.7863e-03 (1.7324e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.2125e-02 (1.8092e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.9791e-02 (1.8593e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9196e-02 (1.8854e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0035e-02 (1.9587e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7654e-02 (1.9554e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4404e-02 (1.9391e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.5345e-02 (1.9383e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7990e-02 (1.9189e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1292e-02 (1.8825e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0447e-02 (1.8623e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1536e-02 (1.8274e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.2010e-03 (1.8209e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2321e-02 (1.8428e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.8730e-03 (1.8376e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.0359e-03 (1.8739e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6113e-02 (1.8753e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0101e-02 (1.8672e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.1498e-03 (1.8806e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4053e-02 (1.8733e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0477e-02 (1.8652e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8625e-02 (1.8588e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6245e-02 (1.8489e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5007e-02 (1.8497e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.5624e-02 (1.8572e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8021e-02 (1.8510e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5396e-02 (1.8364e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6230e-02 (1.8447e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5373e-02 (1.8290e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3649e-02 (1.8216e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8753e-02 (1.8261e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0432e-02 (1.8256e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9058e-02 (1.8409e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.2927515506744385
## e[74]       loss.backward (sum) time: 13.828678607940674
## e[74]      optimizer.step (sum) time: 2.7401652336120605
## epoch[74] training(only) time: 45.05547046661377
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.4229e+00 (1.4229e+00)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.040 ( 0.053)	Loss 1.8408e+00 (1.6752e+00)	Acc@1  65.00 ( 73.36)	Acc@5  94.00 ( 92.45)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8223e+00 (1.6014e+00)	Acc@1  72.00 ( 74.19)	Acc@5  93.00 ( 92.81)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.6543e+00 (1.6708e+00)	Acc@1  61.00 ( 73.45)	Acc@5  84.00 ( 92.06)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.9141e+00 (1.6484e+00)	Acc@1  72.00 ( 73.80)	Acc@5  94.00 ( 92.49)
Test: [ 50/100]	Time  0.037 ( 0.042)	Loss 1.9473e+00 (1.6565e+00)	Acc@1  69.00 ( 73.39)	Acc@5  91.00 ( 92.31)
Test: [ 60/100]	Time  0.040 ( 0.042)	Loss 1.7607e+00 (1.6123e+00)	Acc@1  69.00 ( 73.56)	Acc@5  93.00 ( 92.59)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7725e+00 (1.6242e+00)	Acc@1  65.00 ( 73.54)	Acc@5  92.00 ( 92.54)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0137e+00 (1.6289e+00)	Acc@1  71.00 ( 73.53)	Acc@5  94.00 ( 92.54)
Test: [ 90/100]	Time  0.037 ( 0.040)	Loss 2.2480e+00 (1.6058e+00)	Acc@1  67.00 ( 73.64)	Acc@5  92.00 ( 92.65)
 * Acc@1 73.750 Acc@5 92.720
### epoch[74] execution time: 49.162067890167236
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.284 ( 0.284)	Data  0.166 ( 0.166)	Loss 6.3667e-03 (6.3667e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.113 ( 0.130)	Data  0.001 ( 0.018)	Loss 1.5442e-02 (1.7062e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.3130e-02 (1.9106e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.113 ( 0.120)	Data  0.001 ( 0.009)	Loss 1.3702e-02 (1.9435e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.4091e-02 (1.9061e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.117 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.0803e-02 (1.8060e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.0572e-03 (1.8242e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.3611e-02 (1.7623e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.2711e-02 (1.7799e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.8372e-02 (1.7767e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.5596e-03 (1.7655e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4130e-02 (1.7784e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.7384e-02 (1.8164e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3443e-02 (1.8393e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.3070e-02 (1.8757e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.1286e-02 (1.8564e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4490e-02 (1.8505e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2398e-02 (1.8155e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1269e-02 (1.8157e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2085e-02 (1.8078e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6541e-02 (1.8083e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5442e-02 (1.7899e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.5603e-02 (1.8125e-02)	Acc@1  99.22 ( 99.62)	Acc@5  99.22 (100.00)
Epoch: [75][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.6419e-03 (1.7917e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3672e-02 (1.7834e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6037e-02 (1.7910e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7960e-02 (1.7914e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9867e-02 (1.8036e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.2370e-03 (1.7976e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3565e-02 (1.7899e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9434e-02 (1.8004e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.6866e-03 (1.8082e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.1100e-03 (1.7991e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.7504e-03 (1.7873e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5213e-02 (1.7817e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2797e-02 (1.7877e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6276e-02 (1.7840e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3712e-02 (1.7998e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4252e-02 (1.8042e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.0894e-02 (1.7978e-02)	Acc@1  97.50 ( 99.62)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.2951195240020752
## e[75]       loss.backward (sum) time: 13.827642440795898
## e[75]      optimizer.step (sum) time: 2.7675528526306152
## epoch[75] training(only) time: 45.10723090171814
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.4570e+00 (1.4570e+00)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.040 ( 0.052)	Loss 1.9082e+00 (1.7043e+00)	Acc@1  66.00 ( 72.82)	Acc@5  93.00 ( 92.64)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.8174e+00 (1.6200e+00)	Acc@1  71.00 ( 73.38)	Acc@5  92.00 ( 92.76)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.5254e+00 (1.6808e+00)	Acc@1  66.00 ( 72.97)	Acc@5  85.00 ( 92.06)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8223e+00 (1.6511e+00)	Acc@1  72.00 ( 73.34)	Acc@5  93.00 ( 92.46)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9570e+00 (1.6588e+00)	Acc@1  69.00 ( 73.14)	Acc@5  90.00 ( 92.22)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7061e+00 (1.6128e+00)	Acc@1  72.00 ( 73.31)	Acc@5  93.00 ( 92.57)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.7256e+00 (1.6236e+00)	Acc@1  64.00 ( 73.30)	Acc@5  93.00 ( 92.58)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.9834e+00 (1.6247e+00)	Acc@1  71.00 ( 73.35)	Acc@5  94.00 ( 92.56)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.2402e+00 (1.5993e+00)	Acc@1  68.00 ( 73.46)	Acc@5  92.00 ( 92.63)
 * Acc@1 73.560 Acc@5 92.670
### epoch[75] execution time: 49.2111439704895
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.260 ( 0.260)	Data  0.149 ( 0.149)	Loss 1.5854e-02 (1.5854e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.113 ( 0.127)	Data  0.001 ( 0.017)	Loss 3.2196e-02 (2.0926e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.113 ( 0.121)	Data  0.001 ( 0.010)	Loss 2.5833e-02 (1.8764e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.114 ( 0.119)	Data  0.001 ( 0.008)	Loss 4.6997e-03 (1.6821e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.8030e-02 (1.7855e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 3.0441e-02 (1.8936e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.6663e-02 (1.9238e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 5.7373e-03 (1.8720e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.117 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.2990e-02 (1.8777e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.3858e-03 (1.8297e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1284e-02 (1.8181e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [76][110/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3458e-02 (1.8444e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [76][120/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0294e-02 (1.8836e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [76][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.6743e-02 (1.8695e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [76][140/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0857e-02 (1.8558e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [76][150/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.5215e-03 (1.8460e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [76][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.7084e-02 (1.8617e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1429e-02 (1.8317e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5793e-02 (1.8126e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9012e-02 (1.8107e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5686e-02 (1.8184e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5381e-02 (1.8106e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0462e-02 (1.7961e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.3842e-03 (1.7888e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2535e-02 (1.8015e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6129e-02 (1.8182e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.3624e-03 (1.8264e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8219e-02 (1.8086e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3542e-02 (1.7980e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3474e-02 (1.7918e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0457e-02 (1.8083e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6657e-02 (1.8257e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0020e-02 (1.8370e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3687e-02 (1.8460e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6235e-02 (1.8284e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2024e-02 (1.8073e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5701e-02 (1.8007e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.8247e-02 (1.8287e-02)	Acc@1  96.88 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1757e-02 (1.8318e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7026e-02 (1.8466e-02)	Acc@1  97.50 ( 99.60)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.2949516773223877
## e[76]       loss.backward (sum) time: 13.845004081726074
## e[76]      optimizer.step (sum) time: 2.7453153133392334
## epoch[76] training(only) time: 45.0634343624115
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.4141e+00 (1.4141e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.8652e+00 (1.6611e+00)	Acc@1  66.00 ( 73.45)	Acc@5  94.00 ( 92.64)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8018e+00 (1.5887e+00)	Acc@1  72.00 ( 74.14)	Acc@5  93.00 ( 92.95)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.5234e+00 (1.6522e+00)	Acc@1  61.00 ( 73.45)	Acc@5  86.00 ( 92.23)
Test: [ 40/100]	Time  0.041 ( 0.043)	Loss 1.8877e+00 (1.6307e+00)	Acc@1  70.00 ( 73.66)	Acc@5  94.00 ( 92.63)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9639e+00 (1.6427e+00)	Acc@1  70.00 ( 73.35)	Acc@5  90.00 ( 92.31)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7324e+00 (1.5981e+00)	Acc@1  69.00 ( 73.52)	Acc@5  93.00 ( 92.61)
Test: [ 70/100]	Time  0.040 ( 0.041)	Loss 1.7666e+00 (1.6097e+00)	Acc@1  66.00 ( 73.54)	Acc@5  92.00 ( 92.62)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 1.9795e+00 (1.6144e+00)	Acc@1  72.00 ( 73.65)	Acc@5  93.00 ( 92.56)
Test: [ 90/100]	Time  0.041 ( 0.041)	Loss 2.2109e+00 (1.5906e+00)	Acc@1  68.00 ( 73.75)	Acc@5  92.00 ( 92.65)
 * Acc@1 73.810 Acc@5 92.720
### epoch[76] execution time: 49.233333349227905
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.263 ( 0.263)	Data  0.153 ( 0.153)	Loss 1.1047e-02 (1.1047e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.114 ( 0.128)	Data  0.001 ( 0.017)	Loss 9.0561e-03 (1.9290e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.114 ( 0.122)	Data  0.001 ( 0.011)	Loss 9.8877e-03 (1.8164e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.008)	Loss 1.2482e-02 (1.7899e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.4292e-02 (1.6964e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 1.5503e-02 (1.6782e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 ( 99.98)
Epoch: [77][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.7128e-03 (1.6944e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.6022e-02 (1.7010e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 80/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.6479e-02 (1.6913e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 90/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.4436e-03 (1.6626e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 ( 99.99)
Epoch: [77][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9836e-02 (1.6665e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 ( 99.99)
Epoch: [77][110/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2047e-02 (1.6967e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [77][120/391]	Time  0.117 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.0436e-03 (1.7613e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [77][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.7716e-03 (1.7596e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [77][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5198e-02 (1.7778e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [77][150/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3702e-02 (1.7803e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [77][160/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4027e-02 (1.7888e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.8049e-03 (1.8033e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.9182e-03 (1.8002e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5327e-02 (1.8008e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2684e-02 (1.8160e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4374e-02 (1.8046e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8372e-02 (1.7935e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.4026e-02 (1.8340e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8580e-02 (1.8244e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8473e-02 (1.8410e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [77][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7410e-02 (1.8330e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [77][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.7656e-03 (1.8294e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [77][280/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8687e-02 (1.8333e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [77][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6337e-02 (1.8292e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [77][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.7585e-03 (1.8025e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [77][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3539e-02 (1.8048e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [77][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.4535e-02 (1.8316e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0460e-02 (1.8361e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2718e-02 (1.8383e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9694e-02 (1.8245e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3621e-02 (1.8151e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4429e-02 (1.8103e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0615e-02 (1.8117e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4149e-02 (1.8237e-02)	Acc@1  98.75 ( 99.62)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.29428601264953613
## e[77]       loss.backward (sum) time: 13.822299242019653
## e[77]      optimizer.step (sum) time: 2.7537453174591064
## epoch[77] training(only) time: 45.071845054626465
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 1.4053e+00 (1.4053e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.055)	Loss 1.8760e+00 (1.6450e+00)	Acc@1  66.00 ( 73.45)	Acc@5  93.00 ( 92.64)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.7744e+00 (1.5741e+00)	Acc@1  73.00 ( 74.10)	Acc@5  93.00 ( 92.86)
Test: [ 30/100]	Time  0.039 ( 0.045)	Loss 2.4727e+00 (1.6393e+00)	Acc@1  65.00 ( 73.65)	Acc@5  85.00 ( 92.19)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8643e+00 (1.6141e+00)	Acc@1  71.00 ( 73.95)	Acc@5  94.00 ( 92.61)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.9482e+00 (1.6288e+00)	Acc@1  70.00 ( 73.61)	Acc@5  90.00 ( 92.35)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.7041e+00 (1.5833e+00)	Acc@1  69.00 ( 73.80)	Acc@5  93.00 ( 92.62)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7676e+00 (1.5943e+00)	Acc@1  68.00 ( 73.80)	Acc@5  93.00 ( 92.62)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.9521e+00 (1.5960e+00)	Acc@1  72.00 ( 73.85)	Acc@5  93.00 ( 92.58)
Test: [ 90/100]	Time  0.040 ( 0.041)	Loss 2.1875e+00 (1.5708e+00)	Acc@1  66.00 ( 73.91)	Acc@5  90.00 ( 92.67)
 * Acc@1 73.950 Acc@5 92.740
### epoch[77] execution time: 49.220603466033936
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.272 ( 0.272)	Data  0.154 ( 0.154)	Loss 1.0010e-02 (1.0010e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.111 ( 0.128)	Data  0.001 ( 0.017)	Loss 8.4763e-03 (1.8158e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.6367e-02 (1.7333e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.113 ( 0.120)	Data  0.001 ( 0.009)	Loss 2.2995e-02 (1.7630e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.1133e-02 (1.7979e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.1971e-02 (1.8177e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.117 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.6948e-03 (1.7672e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.3283e-02 (1.7370e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.4168e-02 (1.7454e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2352e-02 (1.7858e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8951e-02 (1.7289e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.2697e-03 (1.7666e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5671e-02 (1.7650e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.0720e-02 (1.7631e-02)	Acc@1  99.22 ( 99.61)	Acc@5  99.22 ( 99.99)
Epoch: [78][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7319e-02 (1.7687e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [78][150/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.5004e-02 (1.7874e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 ( 99.99)
Epoch: [78][160/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0925e-02 (1.7659e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5259e-02 (1.7663e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.6276e-02 (1.7712e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.1705e-03 (1.7502e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.1281e-02 (1.7445e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0132e-02 (1.7268e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.2239e-03 (1.7093e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.3923e-03 (1.7204e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.8370e-03 (1.7261e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.6103e-03 (1.7248e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0920e-02 (1.7216e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8250e-02 (1.7185e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1164e-02 (1.7086e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3458e-02 (1.7022e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1658e-02 (1.6964e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4546e-02 (1.6975e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9577e-02 (1.7064e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0033e-02 (1.7237e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.6436e-03 (1.7215e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5717e-02 (1.7265e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2024e-02 (1.7452e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.8201e-03 (1.7373e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.3934e-03 (1.7402e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6123e-02 (1.7325e-02)	Acc@1  98.75 ( 99.64)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.2919118404388428
## e[78]       loss.backward (sum) time: 13.835175275802612
## e[78]      optimizer.step (sum) time: 2.734483003616333
## epoch[78] training(only) time: 45.047011613845825
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.4346e+00 (1.4346e+00)	Acc@1  76.00 ( 76.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.9111e+00 (1.6896e+00)	Acc@1  66.00 ( 73.09)	Acc@5  93.00 ( 92.27)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8691e+00 (1.6233e+00)	Acc@1  71.00 ( 73.67)	Acc@5  92.00 ( 92.62)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5176e+00 (1.6855e+00)	Acc@1  64.00 ( 72.77)	Acc@5  86.00 ( 91.90)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8867e+00 (1.6546e+00)	Acc@1  69.00 ( 73.20)	Acc@5  94.00 ( 92.39)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9775e+00 (1.6630e+00)	Acc@1  69.00 ( 72.94)	Acc@5  90.00 ( 92.16)
Test: [ 60/100]	Time  0.039 ( 0.042)	Loss 1.7812e+00 (1.6190e+00)	Acc@1  69.00 ( 73.15)	Acc@5  91.00 ( 92.44)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.6914e+00 (1.6301e+00)	Acc@1  66.00 ( 73.17)	Acc@5  92.00 ( 92.41)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 2.0176e+00 (1.6333e+00)	Acc@1  71.00 ( 73.22)	Acc@5  94.00 ( 92.43)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2656e+00 (1.6075e+00)	Acc@1  67.00 ( 73.40)	Acc@5  92.00 ( 92.53)
 * Acc@1 73.470 Acc@5 92.590
### epoch[78] execution time: 49.186814308166504
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.272 ( 0.272)	Data  0.157 ( 0.157)	Loss 8.4763e-03 (8.4763e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.115 ( 0.129)	Data  0.001 ( 0.017)	Loss 1.7227e-02 (2.3725e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.93)
Epoch: [79][ 20/391]	Time  0.114 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.4994e-02 (1.9724e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 30/391]	Time  0.115 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.8061e-02 (1.9321e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 ( 99.97)
Epoch: [79][ 40/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.008)	Loss 4.8370e-02 (1.9317e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 ( 99.98)
Epoch: [79][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.7145e-02 (1.8557e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.98)
Epoch: [79][ 60/391]	Time  0.116 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.0948e-03 (1.8120e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [79][ 70/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.4033e-02 (1.9135e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 ( 99.99)
Epoch: [79][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.5259e-02 (1.8969e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 ( 99.99)
Epoch: [79][ 90/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.6169e-02 (1.8271e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 ( 99.99)
Epoch: [79][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2871e-02 (1.8182e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [79][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.6062e-02 (1.8332e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [79][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.4833e-03 (1.8337e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [79][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.1891e-02 (1.8345e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [79][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 3.1128e-02 (1.8270e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [79][150/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.7435e-02 (1.8277e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [79][160/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6068e-02 (1.7924e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3321e-02 (1.7871e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.8767e-02 (1.7888e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.5754e-03 (1.7976e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.3482e-03 (1.7794e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.5951e-03 (1.7670e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4671e-02 (1.7658e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.7357e-03 (1.7581e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.8190e-03 (1.7517e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6052e-02 (1.7578e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9287e-02 (1.7491e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9477e-03 (1.7422e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2680e-02 (1.7451e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1566e-02 (1.7373e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0597e-02 (1.7270e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.2572e-02 (1.7340e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1742e-02 (1.7292e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5842e-03 (1.7326e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4058e-02 (1.7447e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7700e-02 (1.7469e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3924e-02 (1.7607e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3138e-02 (1.7574e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.5349e-02 (1.7719e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1281e-02 (1.7776e-02)	Acc@1  98.75 ( 99.63)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.2940390110015869
## e[79]       loss.backward (sum) time: 13.8142569065094
## e[79]      optimizer.step (sum) time: 2.7575504779815674
## epoch[79] training(only) time: 45.07618474960327
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.4580e+00 (1.4580e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.042 ( 0.054)	Loss 1.9443e+00 (1.7263e+00)	Acc@1  64.00 ( 72.82)	Acc@5  94.00 ( 92.18)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8643e+00 (1.6349e+00)	Acc@1  70.00 ( 73.71)	Acc@5  92.00 ( 92.71)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.5215e+00 (1.6925e+00)	Acc@1  65.00 ( 73.29)	Acc@5  86.00 ( 92.10)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8320e+00 (1.6600e+00)	Acc@1  71.00 ( 73.49)	Acc@5  93.00 ( 92.54)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.9512e+00 (1.6702e+00)	Acc@1  70.00 ( 73.24)	Acc@5  90.00 ( 92.24)
Test: [ 60/100]	Time  0.041 ( 0.042)	Loss 1.7266e+00 (1.6246e+00)	Acc@1  71.00 ( 73.41)	Acc@5  92.00 ( 92.52)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7461e+00 (1.6371e+00)	Acc@1  66.00 ( 73.35)	Acc@5  93.00 ( 92.51)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.9688e+00 (1.6412e+00)	Acc@1  70.00 ( 73.40)	Acc@5  94.00 ( 92.51)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.3828e+00 (1.6158e+00)	Acc@1  67.00 ( 73.57)	Acc@5  92.00 ( 92.60)
 * Acc@1 73.650 Acc@5 92.630
### epoch[79] execution time: 49.20949959754944
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.272 ( 0.272)	Data  0.159 ( 0.159)	Loss 1.1238e-02 (1.1238e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.113 ( 0.129)	Data  0.001 ( 0.018)	Loss 8.0566e-03 (1.2941e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.111 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.6998e-02 (1.5161e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.114 ( 0.120)	Data  0.001 ( 0.009)	Loss 9.0256e-03 (1.5880e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.6708e-02 (1.5758e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.2354e-02 (1.5185e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.0203e-02 (1.5283e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.1940e-03 (1.5805e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.4353e-02 (1.5858e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.8870e-02 (1.6377e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.106 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.4806e-03 (1.6205e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.1744e-02 (1.6503e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 6.8626e-03 (1.6739e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.9836e-02 (1.6557e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8921e-02 (1.6583e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.6810e-02 (1.6687e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.3630e-02 (1.6884e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.5651e-03 (1.6940e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.4098e-02 (1.7239e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2978e-02 (1.7203e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.4119e-02 (1.7302e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.6987e-02 (1.7614e-02)	Acc@1  97.66 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0468e-02 (1.7265e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1551e-02 (1.7180e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4236e-02 (1.7198e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9907e-02 (1.7188e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.2480e-02 (1.7242e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8616e-02 (1.7151e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.9962e-03 (1.7011e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.4507e-03 (1.6946e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.2528e-03 (1.6807e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3519e-02 (1.6728e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8976e-02 (1.6737e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2156e-02 (1.6666e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2675e-02 (1.6604e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.121 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.8910e-02 (1.6718e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2708e-03 (1.6769e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1154e-02 (1.6715e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4915e-02 (1.6831e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.5959e-02 (1.6888e-02)	Acc@1  97.50 ( 99.69)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.2954421043395996
## e[80]       loss.backward (sum) time: 13.877311706542969
## e[80]      optimizer.step (sum) time: 2.751856803894043
## epoch[80] training(only) time: 45.09044408798218
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.4209e+00 (1.4209e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.040 ( 0.053)	Loss 1.9180e+00 (1.7006e+00)	Acc@1  64.00 ( 72.55)	Acc@5  93.00 ( 92.91)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.8311e+00 (1.6093e+00)	Acc@1  71.00 ( 73.38)	Acc@5  92.00 ( 93.14)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.5098e+00 (1.6741e+00)	Acc@1  65.00 ( 73.00)	Acc@5  84.00 ( 92.26)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.9023e+00 (1.6469e+00)	Acc@1  70.00 ( 73.37)	Acc@5  93.00 ( 92.66)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9443e+00 (1.6573e+00)	Acc@1  69.00 ( 73.18)	Acc@5  91.00 ( 92.33)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.7705e+00 (1.6125e+00)	Acc@1  70.00 ( 73.28)	Acc@5  93.00 ( 92.67)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7744e+00 (1.6229e+00)	Acc@1  66.00 ( 73.24)	Acc@5  93.00 ( 92.65)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 2.0371e+00 (1.6277e+00)	Acc@1  72.00 ( 73.36)	Acc@5  93.00 ( 92.62)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2715e+00 (1.6018e+00)	Acc@1  65.00 ( 73.43)	Acc@5  91.00 ( 92.68)
 * Acc@1 73.520 Acc@5 92.760
### epoch[80] execution time: 49.238537073135376
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.267 ( 0.267)	Data  0.156 ( 0.156)	Loss 1.5541e-02 (1.5541e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.112 ( 0.127)	Data  0.001 ( 0.017)	Loss 1.7868e-02 (1.9648e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.111 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.6342e-02 (1.6741e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.0056e-02 (1.7198e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 1.1337e-02 (1.6843e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.007)	Loss 3.0365e-02 (1.7508e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.8890e-02 (1.7373e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.5671e-02 (1.7285e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.1722e-03 (1.8301e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2772e-02 (1.8065e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3924e-02 (1.7504e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.5671e-02 (1.7260e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.6459e-02 (1.7605e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0355e-02 (1.7328e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 4.9057e-03 (1.7059e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4221e-02 (1.7055e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1314e-02 (1.6853e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0468e-02 (1.6970e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4389e-02 (1.7026e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.7206e-02 (1.7230e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7258e-02 (1.7230e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9587e-02 (1.7238e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2476e-02 (1.7454e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.5602e-03 (1.7452e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.9204e-03 (1.7452e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.8348e-03 (1.7414e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.5400e-02 (1.7514e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.4234e-03 (1.7452e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1667e-02 (1.7339e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3924e-02 (1.7249e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.5918e-03 (1.7169e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.106 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.3155e-03 (1.7137e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3550e-02 (1.7065e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9699e-02 (1.6949e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9968e-02 (1.6988e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4399e-02 (1.7015e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2405e-02 (1.6931e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5411e-02 (1.6836e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.117 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1131e-02 (1.6756e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.6774e-02 (1.6796e-02)	Acc@1  98.75 ( 99.68)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.30018067359924316
## e[81]       loss.backward (sum) time: 13.864971399307251
## e[81]      optimizer.step (sum) time: 2.768799066543579
## epoch[81] training(only) time: 45.11203622817993
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.4307e+00 (1.4307e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.054)	Loss 1.9609e+00 (1.7055e+00)	Acc@1  65.00 ( 72.82)	Acc@5  94.00 ( 92.45)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8281e+00 (1.6191e+00)	Acc@1  72.00 ( 73.86)	Acc@5  92.00 ( 92.76)
Test: [ 30/100]	Time  0.039 ( 0.045)	Loss 2.5527e+00 (1.6786e+00)	Acc@1  64.00 ( 73.26)	Acc@5  86.00 ( 91.94)
Test: [ 40/100]	Time  0.040 ( 0.043)	Loss 1.9014e+00 (1.6499e+00)	Acc@1  70.00 ( 73.54)	Acc@5  94.00 ( 92.39)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9238e+00 (1.6590e+00)	Acc@1  69.00 ( 73.24)	Acc@5  90.00 ( 92.22)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.7520e+00 (1.6144e+00)	Acc@1  70.00 ( 73.41)	Acc@5  92.00 ( 92.52)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.6934e+00 (1.6269e+00)	Acc@1  67.00 ( 73.41)	Acc@5  92.00 ( 92.48)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0391e+00 (1.6308e+00)	Acc@1  70.00 ( 73.44)	Acc@5  93.00 ( 92.47)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2852e+00 (1.6068e+00)	Acc@1  65.00 ( 73.55)	Acc@5  91.00 ( 92.55)
 * Acc@1 73.620 Acc@5 92.620
### epoch[81] execution time: 49.23252511024475
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.272 ( 0.272)	Data  0.160 ( 0.160)	Loss 2.8854e-02 (2.8854e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.113 ( 0.129)	Data  0.001 ( 0.017)	Loss 1.0696e-02 (1.7422e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.111 ( 0.122)	Data  0.001 ( 0.011)	Loss 3.4752e-03 (1.6553e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.114 ( 0.120)	Data  0.001 ( 0.009)	Loss 8.4457e-03 (1.5078e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.007)	Loss 9.1934e-03 (1.6080e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.007)	Loss 8.2855e-03 (1.7065e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.8583e-03 (1.6260e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.4229e-02 (1.6120e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.109 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.4748e-02 (1.6331e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.006)	Loss 4.4327e-03 (1.6402e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.4043e-03 (1.6413e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.110 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.5531e-03 (1.6589e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.7433e-03 (1.6774e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3229e-02 (1.6624e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.3270e-02 (1.6891e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4923e-02 (1.6625e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.4000e-03 (1.6692e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2154e-02 (1.6667e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5757e-02 (1.6796e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0193e-02 (1.6765e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2990e-02 (1.6824e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.4604e-03 (1.6893e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.0190e-03 (1.6794e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.6664e-03 (1.6952e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4496e-02 (1.7115e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4717e-02 (1.7214e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2227e-02 (1.7375e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2589e-02 (1.7257e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5327e-02 (1.7192e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3849e-02 (1.7119e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.5596e-03 (1.7225e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9012e-02 (1.7281e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6098e-02 (1.7281e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.9275e-03 (1.7184e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7842e-02 (1.7192e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.2327e-03 (1.7302e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4435e-02 (1.7460e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5854e-02 (1.7464e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.2392e-03 (1.7396e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.9154e-02 (1.7489e-02)	Acc@1  98.75 ( 99.62)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.2934551239013672
## e[82]       loss.backward (sum) time: 13.878623962402344
## e[82]      optimizer.step (sum) time: 2.77111554145813
## epoch[82] training(only) time: 45.07522392272949
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.3906e+00 (1.3906e+00)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.9043e+00 (1.6753e+00)	Acc@1  66.00 ( 73.00)	Acc@5  94.00 ( 92.45)
Test: [ 20/100]	Time  0.038 ( 0.047)	Loss 1.8652e+00 (1.5959e+00)	Acc@1  71.00 ( 73.95)	Acc@5  91.00 ( 92.86)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.5195e+00 (1.6567e+00)	Acc@1  62.00 ( 73.29)	Acc@5  85.00 ( 92.23)
Test: [ 40/100]	Time  0.045 ( 0.043)	Loss 1.9141e+00 (1.6326e+00)	Acc@1  70.00 ( 73.71)	Acc@5  94.00 ( 92.63)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9580e+00 (1.6423e+00)	Acc@1  68.00 ( 73.49)	Acc@5  91.00 ( 92.33)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.7490e+00 (1.5981e+00)	Acc@1  70.00 ( 73.62)	Acc@5  93.00 ( 92.64)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.7275e+00 (1.6089e+00)	Acc@1  66.00 ( 73.58)	Acc@5  93.00 ( 92.66)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 2.0449e+00 (1.6129e+00)	Acc@1  69.00 ( 73.57)	Acc@5  92.00 ( 92.60)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2598e+00 (1.5893e+00)	Acc@1  66.00 ( 73.69)	Acc@5  92.00 ( 92.69)
 * Acc@1 73.790 Acc@5 92.760
### epoch[82] execution time: 49.18774628639221
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.271 ( 0.271)	Data  0.159 ( 0.159)	Loss 1.4633e-02 (1.4633e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.113 ( 0.127)	Data  0.001 ( 0.017)	Loss 9.5291e-03 (1.2992e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.113 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.0429e-02 (1.4872e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.4114e-02 (1.5263e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.112 ( 0.118)	Data  0.001 ( 0.008)	Loss 2.2232e-02 (1.5789e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.007)	Loss 5.8594e-03 (1.5407e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.6632e-02 (1.5852e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.8534e-02 (1.6069e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.6957e-02 (1.6408e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.5828e-02 (1.6960e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0269e-02 (1.7023e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0126e-02 (1.7039e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 7.9422e-03 (1.7370e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.5291e-03 (1.6874e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1101e-02 (1.6697e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.1429e-02 (1.6535e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2135e-02 (1.6702e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.7891e-03 (1.6483e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1713e-02 (1.6356e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2095e-02 (1.6357e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.5757e-02 (1.6495e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 6.3324e-03 (1.6526e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7548e-02 (1.6539e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3672e-02 (1.6626e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4587e-02 (1.6691e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1052e-03 (1.6663e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.4207e-03 (1.6621e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.3177e-03 (1.6625e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.4425e-03 (1.6644e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5299e-02 (1.6791e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6169e-02 (1.6794e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5375e-02 (1.6734e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.6027e-03 (1.6744e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 5.3902e-03 (1.6779e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.6664e-03 (1.6702e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2077e-02 (1.6599e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2894e-02 (1.6584e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.8180e-03 (1.6567e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1774e-02 (1.6562e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2741e-02 (1.6462e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.293931245803833
## e[83]       loss.backward (sum) time: 13.856398105621338
## e[83]      optimizer.step (sum) time: 2.7528769969940186
## epoch[83] training(only) time: 45.11469340324402
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.4043e+00 (1.4043e+00)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.038 ( 0.054)	Loss 1.8799e+00 (1.6832e+00)	Acc@1  69.00 ( 73.73)	Acc@5  94.00 ( 92.82)
Test: [ 20/100]	Time  0.041 ( 0.047)	Loss 1.8379e+00 (1.6069e+00)	Acc@1  72.00 ( 74.14)	Acc@5  91.00 ( 92.90)
Test: [ 30/100]	Time  0.040 ( 0.044)	Loss 2.5508e+00 (1.6705e+00)	Acc@1  61.00 ( 73.35)	Acc@5  84.00 ( 92.13)
Test: [ 40/100]	Time  0.037 ( 0.043)	Loss 1.9150e+00 (1.6429e+00)	Acc@1  71.00 ( 73.68)	Acc@5  94.00 ( 92.54)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9814e+00 (1.6555e+00)	Acc@1  68.00 ( 73.43)	Acc@5  91.00 ( 92.31)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.7861e+00 (1.6088e+00)	Acc@1  69.00 ( 73.61)	Acc@5  93.00 ( 92.61)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7510e+00 (1.6198e+00)	Acc@1  66.00 ( 73.59)	Acc@5  92.00 ( 92.59)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 2.0078e+00 (1.6245e+00)	Acc@1  70.00 ( 73.65)	Acc@5  92.00 ( 92.51)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.2227e+00 (1.6000e+00)	Acc@1  66.00 ( 73.75)	Acc@5  91.00 ( 92.59)
 * Acc@1 73.790 Acc@5 92.670
### epoch[83] execution time: 49.26396059989929
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.273 ( 0.273)	Data  0.160 ( 0.160)	Loss 1.4130e-02 (1.4130e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.113 ( 0.128)	Data  0.001 ( 0.017)	Loss 2.7161e-02 (1.9690e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.114 ( 0.121)	Data  0.001 ( 0.011)	Loss 2.4811e-02 (1.9071e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.112 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.2215e-02 (1.8423e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.114 ( 0.118)	Data  0.001 ( 0.007)	Loss 8.4152e-03 (1.8819e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.007)	Loss 6.4240e-03 (1.7871e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.115 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.0981e-02 (1.7774e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.0948e-02 (1.7259e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.5274e-02 (1.6876e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 8.9340e-03 (1.6876e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.4765e-02 (1.6924e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.116 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2840e-02 (1.6967e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0172e-02 (1.6812e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0239e-02 (1.6919e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4198e-02 (1.7080e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.4854e-02 (1.7226e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.3237e-03 (1.7354e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.1514e-02 (1.7437e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.9547e-02 (1.7424e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3107e-02 (1.7216e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0172e-02 (1.7006e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.9601e-02 (1.7147e-02)	Acc@1  98.44 ( 99.64)	Acc@5  99.22 (100.00)
Epoch: [84][220/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1820e-02 (1.7056e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3908e-02 (1.7389e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5465e-02 (1.7234e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3611e-02 (1.6984e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.5879e-02 (1.6942e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.1412e-02 (1.6929e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0399e-02 (1.7142e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4002e-02 (1.7204e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4521e-02 (1.7190e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7222e-02 (1.7288e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1475e-02 (1.7364e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.1561e-02 (1.7382e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.6872e-03 (1.7391e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.7084e-02 (1.7401e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2238e-02 (1.7332e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.3357e-03 (1.7307e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2169e-02 (1.7358e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4323e-02 (1.7257e-02)	Acc@1  98.75 ( 99.63)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.30208492279052734
## e[84]       loss.backward (sum) time: 13.897166013717651
## e[84]      optimizer.step (sum) time: 2.744105815887451
## epoch[84] training(only) time: 45.04249048233032
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.4062e+00 (1.4062e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.042 ( 0.053)	Loss 1.8936e+00 (1.6678e+00)	Acc@1  68.00 ( 73.45)	Acc@5  93.00 ( 92.64)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.8281e+00 (1.5961e+00)	Acc@1  72.00 ( 73.90)	Acc@5  94.00 ( 92.95)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5098e+00 (1.6561e+00)	Acc@1  64.00 ( 73.23)	Acc@5  85.00 ( 92.19)
Test: [ 40/100]	Time  0.047 ( 0.043)	Loss 1.8965e+00 (1.6300e+00)	Acc@1  69.00 ( 73.46)	Acc@5  94.00 ( 92.61)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.9756e+00 (1.6395e+00)	Acc@1  70.00 ( 73.31)	Acc@5  91.00 ( 92.31)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7715e+00 (1.5941e+00)	Acc@1  68.00 ( 73.48)	Acc@5  92.00 ( 92.59)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7363e+00 (1.6053e+00)	Acc@1  68.00 ( 73.51)	Acc@5  92.00 ( 92.58)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 1.9824e+00 (1.6101e+00)	Acc@1  72.00 ( 73.59)	Acc@5  93.00 ( 92.53)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2461e+00 (1.5850e+00)	Acc@1  67.00 ( 73.68)	Acc@5  91.00 ( 92.58)
 * Acc@1 73.730 Acc@5 92.670
### epoch[84] execution time: 49.15594816207886
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.278 ( 0.278)	Data  0.169 ( 0.169)	Loss 2.0142e-02 (2.0142e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.114 ( 0.129)	Data  0.001 ( 0.019)	Loss 3.3478e-02 (2.2156e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.011)	Loss 2.2446e-02 (1.9990e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.114 ( 0.120)	Data  0.001 ( 0.009)	Loss 6.8321e-03 (1.9285e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.111 ( 0.119)	Data  0.001 ( 0.008)	Loss 3.9825e-02 (1.9188e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.111 ( 0.118)	Data  0.001 ( 0.007)	Loss 2.4139e-02 (1.8481e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.110 ( 0.117)	Data  0.001 ( 0.006)	Loss 7.3509e-03 (1.7430e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 2.5757e-02 (1.8018e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.4328e-02 (1.8137e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.4854e-02 (1.7764e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.3787e-03 (1.7479e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6922e-02 (1.7484e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2459e-02 (1.7921e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2924e-02 (1.7654e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2451e-02 (1.7403e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9694e-02 (1.7551e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.0719e-03 (1.7203e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.8267e-03 (1.7250e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.2337e-02 (1.7184e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1154e-02 (1.7222e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.0106e-02 (1.7115e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7975e-02 (1.7255e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.8244e-02 (1.7280e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.9434e-02 (1.7223e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.1250e-02 (1.7252e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0706e-02 (1.7256e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.3844e-02 (1.7263e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.3488e-02 (1.7480e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3941e-02 (1.7426e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1742e-02 (1.7443e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7685e-02 (1.7394e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.8677e-02 (1.7232e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3046e-02 (1.7128e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7670e-02 (1.7173e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9211e-02 (1.7085e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2093e-02 (1.7098e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.7477e-03 (1.7087e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.6823e-03 (1.7111e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.5569e-03 (1.7032e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4912e-02 (1.7018e-02)	Acc@1  98.75 ( 99.66)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.29270243644714355
## e[85]       loss.backward (sum) time: 13.867348670959473
## e[85]      optimizer.step (sum) time: 2.750678539276123
## epoch[85] training(only) time: 45.14302396774292
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.4463e+00 (1.4463e+00)	Acc@1  76.00 ( 76.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.041 ( 0.053)	Loss 1.8662e+00 (1.6701e+00)	Acc@1  68.00 ( 73.36)	Acc@5  93.00 ( 92.82)
Test: [ 20/100]	Time  0.038 ( 0.046)	Loss 1.8145e+00 (1.5969e+00)	Acc@1  71.00 ( 73.90)	Acc@5  92.00 ( 93.00)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.4922e+00 (1.6610e+00)	Acc@1  63.00 ( 73.32)	Acc@5  85.00 ( 92.23)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8779e+00 (1.6353e+00)	Acc@1  71.00 ( 73.66)	Acc@5  94.00 ( 92.61)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9160e+00 (1.6468e+00)	Acc@1  68.00 ( 73.27)	Acc@5  90.00 ( 92.29)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7959e+00 (1.6035e+00)	Acc@1  71.00 ( 73.48)	Acc@5  92.00 ( 92.56)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.7061e+00 (1.6149e+00)	Acc@1  66.00 ( 73.48)	Acc@5  93.00 ( 92.56)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0195e+00 (1.6174e+00)	Acc@1  69.00 ( 73.48)	Acc@5  93.00 ( 92.58)
Test: [ 90/100]	Time  0.039 ( 0.040)	Loss 2.2090e+00 (1.5910e+00)	Acc@1  66.00 ( 73.60)	Acc@5  93.00 ( 92.66)
 * Acc@1 73.680 Acc@5 92.740
### epoch[85] execution time: 49.273672342300415
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.271 ( 0.271)	Data  0.161 ( 0.161)	Loss 1.1322e-02 (1.1322e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.112 ( 0.128)	Data  0.001 ( 0.018)	Loss 2.6611e-02 (1.4912e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.011)	Loss 7.1182e-03 (1.4440e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 1.8646e-02 (1.3937e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.3985e-02 (1.5661e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 8.4000e-03 (1.5376e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.7441e-02 (1.5080e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 8.5449e-03 (1.5750e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 9.9335e-03 (1.5490e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.006)	Loss 6.2599e-03 (1.4872e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0279e-02 (1.5360e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.7145e-02 (1.5722e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6266e-02 (1.6103e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 5.3558e-03 (1.5999e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0849e-02 (1.5780e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.8757e-02 (1.5833e-02)	Acc@1  97.66 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.7914e-02 (1.5994e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8646e-02 (1.5996e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4412e-02 (1.6164e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2421e-02 (1.6227e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4216e-02 (1.6673e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0020e-02 (1.6719e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.2192e-02 (1.7006e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0218e-02 (1.7067e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.8779e-03 (1.7121e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.7022e-03 (1.7048e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.1095e-03 (1.7130e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4290e-02 (1.7046e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7807e-02 (1.6998e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0735e-02 (1.7124e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2125e-02 (1.7170e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3145e-02 (1.7182e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.9144e-02 (1.7515e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.3435e-02 (1.7510e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5152e-02 (1.7414e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4778e-02 (1.7319e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.2141e-02 (1.7203e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0126e-02 (1.7212e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5373e-02 (1.7026e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4657e-03 (1.7080e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.29485368728637695
## e[86]       loss.backward (sum) time: 13.822654008865356
## e[86]      optimizer.step (sum) time: 2.751513957977295
## epoch[86] training(only) time: 45.011351346969604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.4473e+00 (1.4473e+00)	Acc@1  75.00 ( 75.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.040 ( 0.054)	Loss 1.8896e+00 (1.6889e+00)	Acc@1  66.00 ( 72.82)	Acc@5  94.00 ( 93.00)
Test: [ 20/100]	Time  0.046 ( 0.047)	Loss 1.7910e+00 (1.6059e+00)	Acc@1  73.00 ( 73.67)	Acc@5  92.00 ( 93.10)
Test: [ 30/100]	Time  0.039 ( 0.045)	Loss 2.5371e+00 (1.6659e+00)	Acc@1  62.00 ( 73.06)	Acc@5  85.00 ( 92.26)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.8594e+00 (1.6399e+00)	Acc@1  71.00 ( 73.44)	Acc@5  94.00 ( 92.63)
Test: [ 50/100]	Time  0.040 ( 0.043)	Loss 1.9678e+00 (1.6511e+00)	Acc@1  68.00 ( 73.18)	Acc@5  91.00 ( 92.37)
Test: [ 60/100]	Time  0.037 ( 0.042)	Loss 1.7793e+00 (1.6073e+00)	Acc@1  69.00 ( 73.38)	Acc@5  92.00 ( 92.61)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.6875e+00 (1.6180e+00)	Acc@1  67.00 ( 73.32)	Acc@5  92.00 ( 92.62)
Test: [ 80/100]	Time  0.037 ( 0.041)	Loss 2.0352e+00 (1.6222e+00)	Acc@1  70.00 ( 73.37)	Acc@5  94.00 ( 92.63)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2656e+00 (1.5966e+00)	Acc@1  67.00 ( 73.47)	Acc@5  92.00 ( 92.75)
 * Acc@1 73.580 Acc@5 92.810
### epoch[86] execution time: 49.13477182388306
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.264 ( 0.264)	Data  0.159 ( 0.159)	Loss 1.2009e-02 (1.2009e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.112 ( 0.128)	Data  0.001 ( 0.018)	Loss 4.1992e-02 (1.6052e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.112 ( 0.122)	Data  0.001 ( 0.011)	Loss 7.2937e-03 (1.5894e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.115 ( 0.119)	Data  0.001 ( 0.009)	Loss 9.2468e-03 (1.5546e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.113 ( 0.118)	Data  0.001 ( 0.008)	Loss 5.8403e-03 (1.6847e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.116 ( 0.117)	Data  0.001 ( 0.007)	Loss 6.9580e-03 (1.6319e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.1497e-02 (1.5760e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.116 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.9821e-02 (1.5487e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.2444e-02 (1.5665e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.006)	Loss 3.2928e-02 (1.5672e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.1881e-02 (1.6030e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.6739e-02 (1.6218e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 ( 99.99)
Epoch: [87][120/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.9572e-02 (1.6397e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 ( 99.99)
Epoch: [87][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.1271e-02 (1.6737e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [87][140/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3824e-02 (1.6440e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 ( 99.99)
Epoch: [87][150/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0139e-02 (1.6297e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 ( 99.99)
Epoch: [87][160/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8158e-02 (1.6041e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6876e-02 (1.6008e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 8.8272e-03 (1.6206e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1719e-02 (1.6444e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.4826e-02 (1.6460e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.8060e-03 (1.6376e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.4159e-02 (1.6441e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 7.9651e-03 (1.6246e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.1255e-02 (1.6464e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.6144e-02 (1.6757e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4091e-02 (1.7083e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7181e-02 (1.6915e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9577e-02 (1.6916e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2169e-02 (1.7010e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.4460e-02 (1.7350e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4686e-03 (1.7227e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.6062e-02 (1.7250e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2471e-02 (1.7358e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7212e-02 (1.7275e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.6272e-03 (1.7102e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.2135e-02 (1.7114e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.7482e-03 (1.7005e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.4740e-02 (1.6937e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.8425e-03 (1.6969e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.30198192596435547
## e[87]       loss.backward (sum) time: 13.838993787765503
## e[87]      optimizer.step (sum) time: 2.736802816390991
## epoch[87] training(only) time: 45.12864422798157
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.3945e+00 (1.3945e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.041 ( 0.054)	Loss 1.9160e+00 (1.7073e+00)	Acc@1  67.00 ( 73.18)	Acc@5  95.00 ( 92.73)
Test: [ 20/100]	Time  0.039 ( 0.047)	Loss 1.8477e+00 (1.6263e+00)	Acc@1  73.00 ( 73.76)	Acc@5  93.00 ( 92.90)
Test: [ 30/100]	Time  0.038 ( 0.044)	Loss 2.5312e+00 (1.6898e+00)	Acc@1  62.00 ( 73.13)	Acc@5  85.00 ( 92.06)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.8535e+00 (1.6544e+00)	Acc@1  72.00 ( 73.44)	Acc@5  94.00 ( 92.59)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.9404e+00 (1.6629e+00)	Acc@1  68.00 ( 73.14)	Acc@5  90.00 ( 92.31)
Test: [ 60/100]	Time  0.038 ( 0.042)	Loss 1.7705e+00 (1.6177e+00)	Acc@1  70.00 ( 73.34)	Acc@5  92.00 ( 92.61)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.6445e+00 (1.6293e+00)	Acc@1  67.00 ( 73.31)	Acc@5  94.00 ( 92.63)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 2.0312e+00 (1.6330e+00)	Acc@1  70.00 ( 73.32)	Acc@5  93.00 ( 92.57)
Test: [ 90/100]	Time  0.038 ( 0.041)	Loss 2.3125e+00 (1.6090e+00)	Acc@1  66.00 ( 73.52)	Acc@5  91.00 ( 92.64)
 * Acc@1 73.660 Acc@5 92.680
### epoch[87] execution time: 49.27380299568176
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.270 ( 0.270)	Data  0.166 ( 0.166)	Loss 7.8812e-03 (7.8812e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.114 ( 0.128)	Data  0.001 ( 0.018)	Loss 1.1963e-02 (1.3765e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.111 ( 0.122)	Data  0.001 ( 0.011)	Loss 1.5701e-02 (1.3858e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.113 ( 0.119)	Data  0.001 ( 0.009)	Loss 3.7964e-02 (1.4442e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.115 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.5091e-02 (1.4279e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.007)	Loss 2.8961e-02 (1.4924e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.111 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.2871e-02 (1.5075e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.9180e-02 (1.5255e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.006)	Loss 1.5404e-02 (1.5273e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 9.1934e-03 (1.5334e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.7578e-02 (1.5669e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.114 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.0416e-02 (1.5966e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.8158e-02 (1.5918e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.3231e-03 (1.6129e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.0719e-02 (1.6344e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4183e-02 (1.6142e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0017e-02 (1.5944e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3794e-02 (1.5954e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.1978e-02 (1.5780e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.0948e-02 (1.5670e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0599e-02 (1.5825e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.8823e-03 (1.6058e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.2490e-03 (1.5928e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.5502e-02 (1.6142e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.5778e-02 (1.6156e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.0706e-02 (1.6191e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3972e-02 (1.6234e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5701e-02 (1.6198e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.7349e-02 (1.6333e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.6670e-03 (1.6261e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0243e-02 (1.6383e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.0043e-03 (1.6284e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.9551e-02 (1.6360e-02)	Acc@1  97.66 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2657e-02 (1.6276e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.0426e-02 (1.6403e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.5961e-02 (1.6300e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.9133e-03 (1.6350e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9880e-03 (1.6300e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.8849e-02 (1.6373e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 7.9346e-03 (1.6342e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.29599523544311523
## e[88]       loss.backward (sum) time: 13.8318510055542
## e[88]      optimizer.step (sum) time: 2.7577617168426514
## epoch[88] training(only) time: 45.02461051940918
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 1.4209e+00 (1.4209e+00)	Acc@1  74.00 ( 74.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.042 ( 0.052)	Loss 1.8652e+00 (1.6805e+00)	Acc@1  66.00 ( 72.91)	Acc@5  93.00 ( 92.64)
Test: [ 20/100]	Time  0.041 ( 0.046)	Loss 1.7871e+00 (1.6005e+00)	Acc@1  72.00 ( 73.76)	Acc@5  93.00 ( 92.90)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.4844e+00 (1.6626e+00)	Acc@1  62.00 ( 73.10)	Acc@5  86.00 ( 92.19)
Test: [ 40/100]	Time  0.039 ( 0.043)	Loss 1.8682e+00 (1.6363e+00)	Acc@1  70.00 ( 73.29)	Acc@5  94.00 ( 92.63)
Test: [ 50/100]	Time  0.038 ( 0.042)	Loss 1.9629e+00 (1.6473e+00)	Acc@1  70.00 ( 73.10)	Acc@5  90.00 ( 92.37)
Test: [ 60/100]	Time  0.040 ( 0.041)	Loss 1.7354e+00 (1.6034e+00)	Acc@1  71.00 ( 73.33)	Acc@5  93.00 ( 92.69)
Test: [ 70/100]	Time  0.039 ( 0.041)	Loss 1.6836e+00 (1.6134e+00)	Acc@1  64.00 ( 73.23)	Acc@5  92.00 ( 92.68)
Test: [ 80/100]	Time  0.039 ( 0.041)	Loss 2.0039e+00 (1.6170e+00)	Acc@1  71.00 ( 73.27)	Acc@5  93.00 ( 92.65)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2227e+00 (1.5926e+00)	Acc@1  66.00 ( 73.42)	Acc@5  92.00 ( 92.75)
 * Acc@1 73.520 Acc@5 92.810
### epoch[88] execution time: 49.150176763534546
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.274 ( 0.274)	Data  0.165 ( 0.165)	Loss 1.0635e-02 (1.0635e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.114 ( 0.128)	Data  0.001 ( 0.018)	Loss 9.8648e-03 (1.6819e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.113 ( 0.121)	Data  0.001 ( 0.011)	Loss 1.8219e-02 (1.8422e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.115 ( 0.119)	Data  0.001 ( 0.009)	Loss 2.2903e-02 (1.8435e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.116 ( 0.118)	Data  0.001 ( 0.008)	Loss 1.7410e-02 (1.9682e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.113 ( 0.117)	Data  0.001 ( 0.007)	Loss 4.6082e-02 (1.9943e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.114 ( 0.117)	Data  0.001 ( 0.006)	Loss 1.0353e-02 (2.0194e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.112 ( 0.117)	Data  0.001 ( 0.006)	Loss 6.6071e-03 (1.9029e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.115 ( 0.116)	Data  0.001 ( 0.006)	Loss 9.6359e-03 (1.9097e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.006)	Loss 2.5131e-02 (1.9125e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.3689e-03 (1.8927e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.111 ( 0.116)	Data  0.001 ( 0.005)	Loss 2.2858e-02 (1.8767e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 9.2621e-03 (1.8543e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.113 ( 0.116)	Data  0.001 ( 0.005)	Loss 8.7814e-03 (1.8004e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.112 ( 0.116)	Data  0.001 ( 0.005)	Loss 1.3962e-02 (1.7646e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2217e-02 (1.7305e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.111 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4015e-02 (1.6900e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 3.2837e-02 (1.6969e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.3969e-02 (1.6723e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.115 ( 0.115)	Data  0.001 ( 0.005)	Loss 4.8187e-02 (1.6606e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.8921e-02 (1.6554e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 2.2293e-02 (1.6600e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.005)	Loss 5.7907e-03 (1.6632e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.116 ( 0.115)	Data  0.001 ( 0.005)	Loss 1.4763e-02 (1.6728e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.005)	Loss 9.8190e-03 (1.6578e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 6.1951e-03 (1.6616e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.8000e-02 (1.6555e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.1688e-02 (1.6598e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9333e-02 (1.6521e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.2711e-02 (1.6552e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.110 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.9775e-02 (1.6512e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.1036e-02 (1.6545e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.109 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.0933e-02 (1.6537e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 8.4076e-03 (1.6411e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 1.6022e-02 (1.6524e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.113 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.3193e-02 (1.6578e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.107 ( 0.115)	Data  0.001 ( 0.004)	Loss 4.8561e-03 (1.6484e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.112 ( 0.115)	Data  0.001 ( 0.004)	Loss 9.3689e-03 (1.6447e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.114 ( 0.115)	Data  0.001 ( 0.004)	Loss 2.0630e-02 (1.6415e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.108 ( 0.115)	Data  0.001 ( 0.004)	Loss 3.4119e-02 (1.6401e-02)	Acc@1  98.75 ( 99.67)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.2959592342376709
## e[89]       loss.backward (sum) time: 13.838842630386353
## e[89]      optimizer.step (sum) time: 2.746500015258789
## epoch[89] training(only) time: 45.113361120224
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.4238e+00 (1.4238e+00)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.039 ( 0.052)	Loss 1.8945e+00 (1.6922e+00)	Acc@1  67.00 ( 73.09)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.039 ( 0.046)	Loss 1.8389e+00 (1.6153e+00)	Acc@1  71.00 ( 73.76)	Acc@5  92.00 ( 92.86)
Test: [ 30/100]	Time  0.039 ( 0.044)	Loss 2.5625e+00 (1.6791e+00)	Acc@1  62.00 ( 73.10)	Acc@5  83.00 ( 92.19)
Test: [ 40/100]	Time  0.038 ( 0.043)	Loss 1.8486e+00 (1.6472e+00)	Acc@1  71.00 ( 73.41)	Acc@5  94.00 ( 92.61)
Test: [ 50/100]	Time  0.039 ( 0.042)	Loss 1.8965e+00 (1.6532e+00)	Acc@1  68.00 ( 73.22)	Acc@5  91.00 ( 92.41)
Test: [ 60/100]	Time  0.039 ( 0.041)	Loss 1.7568e+00 (1.6084e+00)	Acc@1  71.00 ( 73.38)	Acc@5  93.00 ( 92.74)
Test: [ 70/100]	Time  0.038 ( 0.041)	Loss 1.7002e+00 (1.6194e+00)	Acc@1  66.00 ( 73.38)	Acc@5  92.00 ( 92.69)
Test: [ 80/100]	Time  0.038 ( 0.041)	Loss 2.0098e+00 (1.6224e+00)	Acc@1  72.00 ( 73.56)	Acc@5  93.00 ( 92.62)
Test: [ 90/100]	Time  0.039 ( 0.041)	Loss 2.2500e+00 (1.5967e+00)	Acc@1  67.00 ( 73.69)	Acc@5  92.00 ( 92.70)
 * Acc@1 73.790 Acc@5 92.750
### epoch[89] execution time: 49.22661590576172
### Training complete:
#### total training(only) time: 4059.5245258808136
##### Total run time: 4434.7559452056885
