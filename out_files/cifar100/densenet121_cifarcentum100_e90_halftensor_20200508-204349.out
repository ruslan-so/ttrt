# Model: densenet121
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.densenet
<function densenet121 at 0x7f381244cf28>
# model requested: 'densenet121'
# printing out the model
DenseNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (features): Sequential(
    (dense_block_layer_0): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_0): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block_layer_1): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_1): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block_layer_2): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_12): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_13): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_14): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_15): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_16): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_17): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_18): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_19): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_20): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_21): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_22): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_23): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_2): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block3): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_12): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_13): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_14): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_15): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (linear): Linear(in_features=1024, out_features=100, bias=True)
)
# model is low precision
# Model: densenet121
# Dataset: cifarcentum
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  4.637 ( 4.637)	Data  0.112 ( 0.112)	Loss 4.6641e+00 (4.6641e+00)	Acc@1   0.00 (  0.00)	Acc@5   4.69 (  4.69)
Epoch: [0][ 10/391]	Time  0.119 ( 0.533)	Data  0.001 ( 0.011)	Loss 4.5664e+00 (4.5969e+00)	Acc@1   0.78 (  3.20)	Acc@5  13.28 ( 11.65)
Epoch: [0][ 20/391]	Time  0.118 ( 0.337)	Data  0.001 ( 0.006)	Loss 4.5273e+00 (4.6044e+00)	Acc@1   6.25 (  3.76)	Acc@5  17.97 ( 12.91)
Epoch: [0][ 30/391]	Time  0.121 ( 0.268)	Data  0.001 ( 0.005)	Loss 4.4922e+00 (4.5790e+00)	Acc@1   4.69 (  3.88)	Acc@5  14.84 ( 13.94)
Epoch: [0][ 40/391]	Time  0.127 ( 0.232)	Data  0.001 ( 0.004)	Loss 4.4883e+00 (4.5394e+00)	Acc@1   2.34 (  4.04)	Acc@5  17.97 ( 14.58)
Epoch: [0][ 50/391]	Time  0.119 ( 0.210)	Data  0.001 ( 0.003)	Loss 4.1602e+00 (4.4782e+00)	Acc@1   5.47 (  4.49)	Acc@5  24.22 ( 15.98)
Epoch: [0][ 60/391]	Time  0.119 ( 0.195)	Data  0.001 ( 0.003)	Loss 4.0508e+00 (4.4339e+00)	Acc@1   7.81 (  4.67)	Acc@5  24.22 ( 17.02)
Epoch: [0][ 70/391]	Time  0.120 ( 0.185)	Data  0.001 ( 0.003)	Loss 4.0156e+00 (4.3909e+00)	Acc@1   7.03 (  5.08)	Acc@5  24.22 ( 18.06)
Epoch: [0][ 80/391]	Time  0.121 ( 0.177)	Data  0.001 ( 0.002)	Loss 3.9316e+00 (4.3586e+00)	Acc@1  10.16 (  5.28)	Acc@5  32.81 ( 18.55)
Epoch: [0][ 90/391]	Time  0.120 ( 0.171)	Data  0.001 ( 0.002)	Loss 3.9727e+00 (4.3302e+00)	Acc@1   9.38 (  5.45)	Acc@5  28.91 ( 19.11)
Epoch: [0][100/391]	Time  0.121 ( 0.166)	Data  0.001 ( 0.002)	Loss 4.1094e+00 (4.2995e+00)	Acc@1  11.72 (  5.65)	Acc@5  25.78 ( 19.86)
Epoch: [0][110/391]	Time  0.121 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1172e+00 (4.2760e+00)	Acc@1  10.16 (  5.91)	Acc@5  22.66 ( 20.41)
Epoch: [0][120/391]	Time  0.119 ( 0.159)	Data  0.001 ( 0.002)	Loss 4.2031e+00 (4.2438e+00)	Acc@1   3.12 (  6.27)	Acc@5  25.00 ( 21.24)
Epoch: [0][130/391]	Time  0.121 ( 0.156)	Data  0.001 ( 0.002)	Loss 3.9570e+00 (4.2228e+00)	Acc@1   7.81 (  6.45)	Acc@5  31.25 ( 21.85)
Epoch: [0][140/391]	Time  0.121 ( 0.153)	Data  0.001 ( 0.002)	Loss 3.9355e+00 (4.2007e+00)	Acc@1   6.25 (  6.60)	Acc@5  34.38 ( 22.41)
Epoch: [0][150/391]	Time  0.119 ( 0.151)	Data  0.001 ( 0.002)	Loss 3.7539e+00 (4.1795e+00)	Acc@1   7.81 (  6.80)	Acc@5  32.03 ( 22.89)
Epoch: [0][160/391]	Time  0.116 ( 0.149)	Data  0.001 ( 0.002)	Loss 3.9043e+00 (4.1607e+00)	Acc@1  10.16 (  6.99)	Acc@5  28.12 ( 23.37)
Epoch: [0][170/391]	Time  0.121 ( 0.148)	Data  0.001 ( 0.002)	Loss 3.7578e+00 (4.1449e+00)	Acc@1  11.72 (  7.13)	Acc@5  38.28 ( 23.86)
Epoch: [0][180/391]	Time  0.119 ( 0.146)	Data  0.001 ( 0.002)	Loss 3.9883e+00 (4.1301e+00)	Acc@1  10.16 (  7.25)	Acc@5  24.22 ( 24.27)
Epoch: [0][190/391]	Time  0.120 ( 0.145)	Data  0.001 ( 0.002)	Loss 3.7363e+00 (4.1156e+00)	Acc@1  11.72 (  7.40)	Acc@5  32.03 ( 24.66)
Epoch: [0][200/391]	Time  0.121 ( 0.144)	Data  0.001 ( 0.002)	Loss 4.0469e+00 (4.0971e+00)	Acc@1   6.25 (  7.56)	Acc@5  22.66 ( 25.16)
Epoch: [0][210/391]	Time  0.118 ( 0.143)	Data  0.001 ( 0.002)	Loss 3.8281e+00 (4.0842e+00)	Acc@1  11.72 (  7.69)	Acc@5  34.38 ( 25.54)
Epoch: [0][220/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.002)	Loss 3.5645e+00 (4.0714e+00)	Acc@1  14.84 (  7.81)	Acc@5  39.84 ( 25.90)
Epoch: [0][230/391]	Time  0.123 ( 0.141)	Data  0.001 ( 0.002)	Loss 3.6152e+00 (4.0562e+00)	Acc@1  14.84 (  7.98)	Acc@5  39.84 ( 26.31)
Epoch: [0][240/391]	Time  0.119 ( 0.140)	Data  0.001 ( 0.002)	Loss 3.7051e+00 (4.0447e+00)	Acc@1  10.94 (  8.13)	Acc@5  32.81 ( 26.65)
Epoch: [0][250/391]	Time  0.127 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8203e+00 (4.0357e+00)	Acc@1   8.59 (  8.17)	Acc@5  31.25 ( 26.88)
Epoch: [0][260/391]	Time  0.121 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7090e+00 (4.0240e+00)	Acc@1  14.84 (  8.35)	Acc@5  37.50 ( 27.27)
Epoch: [0][270/391]	Time  0.122 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.6484e+00 (4.0115e+00)	Acc@1  14.84 (  8.53)	Acc@5  35.94 ( 27.60)
Epoch: [0][280/391]	Time  0.120 ( 0.137)	Data  0.001 ( 0.001)	Loss 3.5781e+00 (3.9981e+00)	Acc@1  15.62 (  8.72)	Acc@5  39.84 ( 27.96)
Epoch: [0][290/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.001)	Loss 3.4766e+00 (3.9844e+00)	Acc@1  16.41 (  8.89)	Acc@5  43.75 ( 28.37)
Epoch: [0][300/391]	Time  0.120 ( 0.136)	Data  0.001 ( 0.001)	Loss 3.5879e+00 (3.9732e+00)	Acc@1  13.28 (  9.06)	Acc@5  37.50 ( 28.66)
Epoch: [0][310/391]	Time  0.121 ( 0.136)	Data  0.001 ( 0.001)	Loss 3.8750e+00 (3.9631e+00)	Acc@1   7.03 (  9.14)	Acc@5  32.03 ( 28.94)
Epoch: [0][320/391]	Time  0.121 ( 0.135)	Data  0.001 ( 0.001)	Loss 3.8105e+00 (3.9522e+00)	Acc@1  13.28 (  9.31)	Acc@5  31.25 ( 29.27)
Epoch: [0][330/391]	Time  0.125 ( 0.135)	Data  0.001 ( 0.001)	Loss 3.4258e+00 (3.9405e+00)	Acc@1  17.19 (  9.44)	Acc@5  43.75 ( 29.59)
Epoch: [0][340/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.001)	Loss 3.6660e+00 (3.9307e+00)	Acc@1   9.38 (  9.57)	Acc@5  33.59 ( 29.88)
Epoch: [0][350/391]	Time  0.122 ( 0.134)	Data  0.001 ( 0.001)	Loss 3.4609e+00 (3.9189e+00)	Acc@1  14.84 (  9.74)	Acc@5  42.19 ( 30.24)
Epoch: [0][360/391]	Time  0.121 ( 0.134)	Data  0.001 ( 0.001)	Loss 3.4863e+00 (3.9085e+00)	Acc@1  15.62 (  9.89)	Acc@5  39.84 ( 30.46)
Epoch: [0][370/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.001)	Loss 3.6094e+00 (3.8986e+00)	Acc@1  15.62 ( 10.04)	Acc@5  38.28 ( 30.73)
Epoch: [0][380/391]	Time  0.123 ( 0.133)	Data  0.001 ( 0.001)	Loss 3.4707e+00 (3.8887e+00)	Acc@1  17.19 ( 10.19)	Acc@5  40.62 ( 31.02)
Epoch: [0][390/391]	Time  1.487 ( 0.136)	Data  0.001 ( 0.001)	Loss 3.6406e+00 (3.8786e+00)	Acc@1  17.50 ( 10.35)	Acc@5  32.50 ( 31.30)
## e[0] optimizer.zero_grad (sum) time: 0.6934280395507812
## e[0]       loss.backward (sum) time: 16.190452337265015
## e[0]      optimizer.step (sum) time: 8.117634773254395
## epoch[0] training(only) time: 53.37933039665222
# Switched to evaluate mode...
Test: [  0/100]	Time  0.671 ( 0.671)	Loss 3.5332e+00 (3.5332e+00)	Acc@1  13.00 ( 13.00)	Acc@5  39.00 ( 39.00)
Test: [ 10/100]	Time  0.046 ( 0.105)	Loss 3.6973e+00 (3.5018e+00)	Acc@1   8.00 ( 15.55)	Acc@5  44.00 ( 43.09)
Test: [ 20/100]	Time  0.046 ( 0.077)	Loss 3.5059e+00 (3.4899e+00)	Acc@1  13.00 ( 16.00)	Acc@5  43.00 ( 43.14)
Test: [ 30/100]	Time  0.046 ( 0.068)	Loss 3.7324e+00 (3.4900e+00)	Acc@1  11.00 ( 16.39)	Acc@5  40.00 ( 43.48)
Test: [ 40/100]	Time  0.046 ( 0.062)	Loss 3.5801e+00 (3.4868e+00)	Acc@1  17.00 ( 16.10)	Acc@5  43.00 ( 43.32)
Test: [ 50/100]	Time  0.047 ( 0.060)	Loss 3.3848e+00 (3.4883e+00)	Acc@1  20.00 ( 16.39)	Acc@5  42.00 ( 43.04)
Test: [ 60/100]	Time  0.047 ( 0.058)	Loss 3.4316e+00 (3.4853e+00)	Acc@1  16.00 ( 16.34)	Acc@5  47.00 ( 43.08)
Test: [ 70/100]	Time  0.050 ( 0.056)	Loss 3.5098e+00 (3.4850e+00)	Acc@1  16.00 ( 16.28)	Acc@5  43.00 ( 43.08)
Test: [ 80/100]	Time  0.046 ( 0.055)	Loss 3.6602e+00 (3.4977e+00)	Acc@1  13.00 ( 16.19)	Acc@5  35.00 ( 42.64)
Test: [ 90/100]	Time  0.047 ( 0.054)	Loss 3.4395e+00 (3.4957e+00)	Acc@1  20.00 ( 16.32)	Acc@5  48.00 ( 42.74)
 * Acc@1 16.420 Acc@5 42.850
### epoch[0] execution time: 58.85153269767761
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.331 ( 0.331)	Data  0.151 ( 0.151)	Loss 3.4023e+00 (3.4023e+00)	Acc@1  16.41 ( 16.41)	Acc@5  47.66 ( 47.66)
Epoch: [1][ 10/391]	Time  0.120 ( 0.141)	Data  0.001 ( 0.015)	Loss 3.5508e+00 (3.4645e+00)	Acc@1  11.72 ( 16.55)	Acc@5  45.31 ( 43.11)
Epoch: [1][ 20/391]	Time  0.120 ( 0.131)	Data  0.001 ( 0.008)	Loss 3.4648e+00 (3.4616e+00)	Acc@1  17.97 ( 16.37)	Acc@5  42.19 ( 43.60)
Epoch: [1][ 30/391]	Time  0.122 ( 0.128)	Data  0.001 ( 0.006)	Loss 3.2520e+00 (3.4512e+00)	Acc@1  24.22 ( 16.61)	Acc@5  46.88 ( 43.70)
Epoch: [1][ 40/391]	Time  0.122 ( 0.127)	Data  0.001 ( 0.005)	Loss 3.5898e+00 (3.4486e+00)	Acc@1  14.84 ( 17.04)	Acc@5  38.28 ( 43.86)
Epoch: [1][ 50/391]	Time  0.122 ( 0.126)	Data  0.001 ( 0.004)	Loss 3.3516e+00 (3.4218e+00)	Acc@1  18.75 ( 17.49)	Acc@5  46.88 ( 44.47)
Epoch: [1][ 60/391]	Time  0.122 ( 0.125)	Data  0.001 ( 0.003)	Loss 3.3828e+00 (3.4218e+00)	Acc@1  17.97 ( 17.52)	Acc@5  46.09 ( 44.47)
Epoch: [1][ 70/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.003)	Loss 3.4727e+00 (3.4209e+00)	Acc@1  13.28 ( 17.54)	Acc@5  39.84 ( 44.55)
Epoch: [1][ 80/391]	Time  0.127 ( 0.124)	Data  0.001 ( 0.003)	Loss 3.1953e+00 (3.4064e+00)	Acc@1  16.41 ( 17.54)	Acc@5  47.66 ( 44.97)
Epoch: [1][ 90/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.003)	Loss 3.4395e+00 (3.3986e+00)	Acc@1  22.66 ( 17.74)	Acc@5  47.66 ( 45.24)
Epoch: [1][100/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.2793e+00 (3.3957e+00)	Acc@1  21.09 ( 17.74)	Acc@5  50.00 ( 45.22)
Epoch: [1][110/391]	Time  0.131 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.1777e+00 (3.3799e+00)	Acc@1  21.88 ( 17.99)	Acc@5  50.00 ( 45.59)
Epoch: [1][120/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.5254e+00 (3.3834e+00)	Acc@1  20.31 ( 18.03)	Acc@5  42.97 ( 45.60)
Epoch: [1][130/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.4238e+00 (3.3833e+00)	Acc@1  19.53 ( 18.02)	Acc@5  53.12 ( 45.57)
Epoch: [1][140/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.2227e+00 (3.3841e+00)	Acc@1  19.53 ( 17.99)	Acc@5  45.31 ( 45.52)
Epoch: [1][150/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.0859e+00 (3.3810e+00)	Acc@1  19.53 ( 17.97)	Acc@5  55.47 ( 45.58)
Epoch: [1][160/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.2832e+00 (3.3800e+00)	Acc@1  16.41 ( 17.93)	Acc@5  52.34 ( 45.70)
Epoch: [1][170/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.2988e+00 (3.3754e+00)	Acc@1  24.22 ( 18.04)	Acc@5  50.00 ( 45.83)
Epoch: [1][180/391]	Time  0.126 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.6797e+00 (3.3710e+00)	Acc@1  15.62 ( 18.10)	Acc@5  45.31 ( 45.90)
Epoch: [1][190/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.2500e+00 (3.3649e+00)	Acc@1  22.66 ( 18.19)	Acc@5  49.22 ( 46.05)
Epoch: [1][200/391]	Time  0.124 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.1738e+00 (3.3611e+00)	Acc@1  21.09 ( 18.24)	Acc@5  51.56 ( 46.13)
Epoch: [1][210/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2793e+00 (3.3562e+00)	Acc@1  20.31 ( 18.30)	Acc@5  48.44 ( 46.28)
Epoch: [1][220/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0625e+00 (3.3494e+00)	Acc@1  26.56 ( 18.40)	Acc@5  56.25 ( 46.44)
Epoch: [1][230/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.3281e+00 (3.3443e+00)	Acc@1  21.09 ( 18.47)	Acc@5  46.09 ( 46.56)
Epoch: [1][240/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0449e+00 (3.3352e+00)	Acc@1  25.00 ( 18.61)	Acc@5  55.47 ( 46.78)
Epoch: [1][250/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1250e+00 (3.3276e+00)	Acc@1  26.56 ( 18.76)	Acc@5  53.12 ( 46.96)
Epoch: [1][260/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1660e+00 (3.3226e+00)	Acc@1  21.88 ( 18.83)	Acc@5  50.78 ( 47.07)
Epoch: [1][270/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1562e+00 (3.3168e+00)	Acc@1  26.56 ( 19.03)	Acc@5  54.69 ( 47.28)
Epoch: [1][280/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1465e+00 (3.3119e+00)	Acc@1  25.00 ( 19.14)	Acc@5  49.22 ( 47.39)
Epoch: [1][290/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1660e+00 (3.3036e+00)	Acc@1  22.66 ( 19.31)	Acc@5  50.78 ( 47.65)
Epoch: [1][300/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9883e+00 (3.2962e+00)	Acc@1  28.91 ( 19.45)	Acc@5  56.25 ( 47.81)
Epoch: [1][310/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9824e+00 (3.2901e+00)	Acc@1  30.47 ( 19.57)	Acc@5  57.03 ( 48.00)
Epoch: [1][320/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0273e+00 (3.2815e+00)	Acc@1  26.56 ( 19.72)	Acc@5  53.12 ( 48.21)
Epoch: [1][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.1191e+00 (3.2751e+00)	Acc@1  19.53 ( 19.82)	Acc@5  53.91 ( 48.38)
Epoch: [1][340/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.2539e+00 (3.2701e+00)	Acc@1  17.97 ( 19.88)	Acc@5  51.56 ( 48.47)
Epoch: [1][350/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.9961e+00 (3.2645e+00)	Acc@1  25.78 ( 19.91)	Acc@5  55.47 ( 48.64)
Epoch: [1][360/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.1680e+00 (3.2588e+00)	Acc@1  17.97 ( 20.01)	Acc@5  53.12 ( 48.81)
Epoch: [1][370/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.9980e+00 (3.2510e+00)	Acc@1  23.44 ( 20.11)	Acc@5  57.81 ( 49.00)
Epoch: [1][380/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.8516e+00 (3.2432e+00)	Acc@1  23.44 ( 20.28)	Acc@5  61.72 ( 49.21)
Epoch: [1][390/391]	Time  0.115 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.1172e+00 (3.2336e+00)	Acc@1  23.75 ( 20.43)	Acc@5  48.75 ( 49.43)
## e[1] optimizer.zero_grad (sum) time: 0.6848812103271484
## e[1]       loss.backward (sum) time: 14.312591791152954
## e[1]      optimizer.step (sum) time: 8.132852554321289
## epoch[1] training(only) time: 47.800498485565186
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.9336e+00 (2.9336e+00)	Acc@1  24.00 ( 24.00)	Acc@5  59.00 ( 59.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.1641e+00 (3.0245e+00)	Acc@1  20.00 ( 24.18)	Acc@5  49.00 ( 58.00)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 3.0820e+00 (3.0670e+00)	Acc@1  21.00 ( 24.71)	Acc@5  53.00 ( 56.62)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 3.3555e+00 (3.0568e+00)	Acc@1  18.00 ( 24.90)	Acc@5  48.00 ( 56.26)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 3.0605e+00 (3.0527e+00)	Acc@1  27.00 ( 24.93)	Acc@5  58.00 ( 56.15)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 3.0449e+00 (3.0615e+00)	Acc@1  27.00 ( 24.90)	Acc@5  47.00 ( 55.71)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 2.9180e+00 (3.0635e+00)	Acc@1  28.00 ( 24.84)	Acc@5  57.00 ( 55.41)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 3.0254e+00 (3.0642e+00)	Acc@1  32.00 ( 24.79)	Acc@5  54.00 ( 55.38)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 3.1777e+00 (3.0770e+00)	Acc@1  21.00 ( 24.51)	Acc@5  52.00 ( 55.19)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 3.1270e+00 (3.0705e+00)	Acc@1  23.00 ( 24.77)	Acc@5  57.00 ( 55.47)
 * Acc@1 24.810 Acc@5 55.590
### epoch[1] execution time: 52.82614254951477
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.282 ( 0.282)	Data  0.144 ( 0.144)	Loss 2.8574e+00 (2.8574e+00)	Acc@1  28.12 ( 28.12)	Acc@5  59.38 ( 59.38)
Epoch: [2][ 10/391]	Time  0.119 ( 0.135)	Data  0.001 ( 0.014)	Loss 3.0820e+00 (2.9993e+00)	Acc@1  24.22 ( 23.93)	Acc@5  58.59 ( 55.68)
Epoch: [2][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.008)	Loss 2.9062e+00 (3.0007e+00)	Acc@1  32.03 ( 24.70)	Acc@5  60.94 ( 56.06)
Epoch: [2][ 30/391]	Time  0.122 ( 0.126)	Data  0.001 ( 0.006)	Loss 2.8301e+00 (2.9790e+00)	Acc@1  32.03 ( 25.00)	Acc@5  59.38 ( 56.75)
Epoch: [2][ 40/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.005)	Loss 3.0293e+00 (2.9447e+00)	Acc@1  27.34 ( 25.76)	Acc@5  53.91 ( 57.49)
Epoch: [2][ 50/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 2.7188e+00 (2.9462e+00)	Acc@1  25.78 ( 25.72)	Acc@5  61.72 ( 57.52)
Epoch: [2][ 60/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.003)	Loss 3.0371e+00 (2.9421e+00)	Acc@1  25.78 ( 25.61)	Acc@5  51.56 ( 57.48)
Epoch: [2][ 70/391]	Time  0.124 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.0156e+00 (2.9278e+00)	Acc@1  21.09 ( 25.83)	Acc@5  59.38 ( 57.72)
Epoch: [2][ 80/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.0156e+00 (2.9177e+00)	Acc@1  24.22 ( 26.18)	Acc@5  53.91 ( 58.03)
Epoch: [2][ 90/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.1992e+00 (2.9121e+00)	Acc@1  25.00 ( 26.37)	Acc@5  48.44 ( 58.04)
Epoch: [2][100/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.8203e+00 (2.8914e+00)	Acc@1  31.25 ( 26.86)	Acc@5  60.94 ( 58.53)
Epoch: [2][110/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.7422e+00 (2.8835e+00)	Acc@1  31.25 ( 27.14)	Acc@5  67.19 ( 58.73)
Epoch: [2][120/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.8672e+00 (2.8741e+00)	Acc@1  31.25 ( 27.46)	Acc@5  57.03 ( 58.84)
Epoch: [2][130/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.0078e+00 (2.8674e+00)	Acc@1  25.00 ( 27.49)	Acc@5  53.91 ( 59.00)
Epoch: [2][140/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.8281e+00 (2.8655e+00)	Acc@1  24.22 ( 27.58)	Acc@5  64.06 ( 58.98)
Epoch: [2][150/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.7617e+00 (2.8606e+00)	Acc@1  32.81 ( 27.64)	Acc@5  62.50 ( 59.07)
Epoch: [2][160/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.4727e+00 (2.8571e+00)	Acc@1  33.59 ( 27.57)	Acc@5  64.84 ( 59.12)
Epoch: [2][170/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.6035e+00 (2.8513e+00)	Acc@1  40.62 ( 27.74)	Acc@5  66.41 ( 59.35)
Epoch: [2][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.8887e+00 (2.8516e+00)	Acc@1  24.22 ( 27.67)	Acc@5  58.59 ( 59.33)
Epoch: [2][190/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.6484e+00 (2.8437e+00)	Acc@1  33.59 ( 27.80)	Acc@5  65.62 ( 59.56)
Epoch: [2][200/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.7461e+00 (2.8387e+00)	Acc@1  35.16 ( 28.01)	Acc@5  64.06 ( 59.71)
Epoch: [2][210/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.4863e+00 (2.8316e+00)	Acc@1  35.16 ( 28.17)	Acc@5  71.09 ( 59.94)
Epoch: [2][220/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.3516e+00 (2.8265e+00)	Acc@1  43.75 ( 28.22)	Acc@5  71.88 ( 60.03)
Epoch: [2][230/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.4961e+00 (2.8161e+00)	Acc@1  33.59 ( 28.50)	Acc@5  61.72 ( 60.22)
Epoch: [2][240/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.7266e+00 (2.8070e+00)	Acc@1  25.78 ( 28.64)	Acc@5  63.28 ( 60.42)
Epoch: [2][250/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.6992e+00 (2.8025e+00)	Acc@1  32.03 ( 28.73)	Acc@5  64.84 ( 60.46)
Epoch: [2][260/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.4199e+00 (2.7962e+00)	Acc@1  39.06 ( 28.85)	Acc@5  69.53 ( 60.61)
Epoch: [2][270/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.5332e+00 (2.7901e+00)	Acc@1  32.81 ( 28.98)	Acc@5  70.31 ( 60.78)
Epoch: [2][280/391]	Time  0.132 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.7676e+00 (2.7865e+00)	Acc@1  29.69 ( 29.01)	Acc@5  59.38 ( 60.86)
Epoch: [2][290/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.5312e+00 (2.7803e+00)	Acc@1  32.81 ( 29.14)	Acc@5  66.41 ( 61.03)
Epoch: [2][300/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.7754e+00 (2.7762e+00)	Acc@1  29.69 ( 29.23)	Acc@5  59.38 ( 61.11)
Epoch: [2][310/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.6816e+00 (2.7713e+00)	Acc@1  33.59 ( 29.33)	Acc@5  64.06 ( 61.25)
Epoch: [2][320/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.5879e+00 (2.7662e+00)	Acc@1  29.69 ( 29.45)	Acc@5  67.97 ( 61.30)
Epoch: [2][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.7402e+00 (2.7619e+00)	Acc@1  25.00 ( 29.54)	Acc@5  65.62 ( 61.43)
Epoch: [2][340/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.5527e+00 (2.7557e+00)	Acc@1  31.25 ( 29.69)	Acc@5  66.41 ( 61.60)
Epoch: [2][350/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.5117e+00 (2.7518e+00)	Acc@1  35.94 ( 29.75)	Acc@5  66.41 ( 61.69)
Epoch: [2][360/391]	Time  0.132 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.5508e+00 (2.7479e+00)	Acc@1  31.25 ( 29.79)	Acc@5  65.62 ( 61.79)
Epoch: [2][370/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.5430e+00 (2.7433e+00)	Acc@1  37.50 ( 29.87)	Acc@5  67.97 ( 61.91)
Epoch: [2][380/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.6035e+00 (2.7398e+00)	Acc@1  35.94 ( 29.94)	Acc@5  66.41 ( 62.03)
Epoch: [2][390/391]	Time  0.112 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.2812e+00 (2.7342e+00)	Acc@1  33.75 ( 30.04)	Acc@5  75.00 ( 62.16)
## e[2] optimizer.zero_grad (sum) time: 0.6863458156585693
## e[2]       loss.backward (sum) time: 14.360210180282593
## e[2]      optimizer.step (sum) time: 8.148500919342041
## epoch[2] training(only) time: 47.77148485183716
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 2.5996e+00 (2.5996e+00)	Acc@1  36.00 ( 36.00)	Acc@5  67.00 ( 67.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 2.5703e+00 (2.5719e+00)	Acc@1  28.00 ( 34.18)	Acc@5  68.00 ( 66.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 2.4121e+00 (2.5849e+00)	Acc@1  40.00 ( 34.48)	Acc@5  69.00 ( 66.33)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 2.7578e+00 (2.5718e+00)	Acc@1  30.00 ( 33.74)	Acc@5  61.00 ( 66.35)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 2.5215e+00 (2.5669e+00)	Acc@1  33.00 ( 33.85)	Acc@5  71.00 ( 66.63)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 2.5020e+00 (2.5725e+00)	Acc@1  41.00 ( 34.25)	Acc@5  62.00 ( 66.29)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.4609e+00 (2.5625e+00)	Acc@1  27.00 ( 33.98)	Acc@5  71.00 ( 66.39)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 2.5352e+00 (2.5637e+00)	Acc@1  32.00 ( 34.10)	Acc@5  67.00 ( 66.34)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 2.6719e+00 (2.5757e+00)	Acc@1  34.00 ( 33.83)	Acc@5  61.00 ( 66.15)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 2.5723e+00 (2.5663e+00)	Acc@1  39.00 ( 34.07)	Acc@5  64.00 ( 66.42)
 * Acc@1 34.060 Acc@5 66.630
### epoch[2] execution time: 52.784881591796875
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.284 ( 0.284)	Data  0.149 ( 0.149)	Loss 2.2617e+00 (2.2617e+00)	Acc@1  38.28 ( 38.28)	Acc@5  72.66 ( 72.66)
Epoch: [3][ 10/391]	Time  0.120 ( 0.137)	Data  0.001 ( 0.014)	Loss 2.4023e+00 (2.4499e+00)	Acc@1  37.50 ( 35.44)	Acc@5  68.75 ( 68.68)
Epoch: [3][ 20/391]	Time  0.119 ( 0.131)	Data  0.001 ( 0.008)	Loss 2.4609e+00 (2.4306e+00)	Acc@1  32.81 ( 35.94)	Acc@5  68.75 ( 68.75)
Epoch: [3][ 30/391]	Time  0.120 ( 0.128)	Data  0.001 ( 0.006)	Loss 2.3164e+00 (2.3993e+00)	Acc@1  27.34 ( 35.58)	Acc@5  75.00 ( 69.93)
Epoch: [3][ 40/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.005)	Loss 2.1934e+00 (2.3949e+00)	Acc@1  41.41 ( 36.01)	Acc@5  72.66 ( 69.70)
Epoch: [3][ 50/391]	Time  0.130 ( 0.125)	Data  0.001 ( 0.004)	Loss 2.4590e+00 (2.3952e+00)	Acc@1  35.94 ( 36.17)	Acc@5  67.19 ( 69.62)
Epoch: [3][ 60/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.003)	Loss 2.5059e+00 (2.3969e+00)	Acc@1  36.72 ( 36.26)	Acc@5  65.62 ( 69.53)
Epoch: [3][ 70/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.003)	Loss 2.2910e+00 (2.3894e+00)	Acc@1  32.81 ( 36.26)	Acc@5  71.88 ( 69.80)
Epoch: [3][ 80/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.003)	Loss 2.3672e+00 (2.3862e+00)	Acc@1  38.28 ( 36.55)	Acc@5  71.88 ( 69.93)
Epoch: [3][ 90/391]	Time  0.124 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.3184e+00 (2.3862e+00)	Acc@1  40.62 ( 36.73)	Acc@5  73.44 ( 69.85)
Epoch: [3][100/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.3516e+00 (2.3893e+00)	Acc@1  39.84 ( 36.64)	Acc@5  67.97 ( 69.76)
Epoch: [3][110/391]	Time  0.127 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.4570e+00 (2.3882e+00)	Acc@1  35.16 ( 36.65)	Acc@5  67.97 ( 69.74)
Epoch: [3][120/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.2832e+00 (2.3885e+00)	Acc@1  43.75 ( 36.57)	Acc@5  75.78 ( 69.76)
Epoch: [3][130/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.3223e+00 (2.3918e+00)	Acc@1  41.41 ( 36.56)	Acc@5  71.09 ( 69.55)
Epoch: [3][140/391]	Time  0.134 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.3594e+00 (2.3874e+00)	Acc@1  34.38 ( 36.76)	Acc@5  70.31 ( 69.70)
Epoch: [3][150/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.2129e+00 (2.3896e+00)	Acc@1  35.94 ( 36.70)	Acc@5  73.44 ( 69.67)
Epoch: [3][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.4727e+00 (2.3824e+00)	Acc@1  35.16 ( 36.93)	Acc@5  73.44 ( 69.75)
Epoch: [3][170/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2402e+00 (2.3809e+00)	Acc@1  35.94 ( 36.97)	Acc@5  72.66 ( 69.86)
Epoch: [3][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.3379e+00 (2.3762e+00)	Acc@1  32.03 ( 37.02)	Acc@5  69.53 ( 70.04)
Epoch: [3][190/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2969e+00 (2.3716e+00)	Acc@1  39.84 ( 37.18)	Acc@5  73.44 ( 70.19)
Epoch: [3][200/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.5332e+00 (2.3745e+00)	Acc@1  26.56 ( 37.17)	Acc@5  68.75 ( 70.06)
Epoch: [3][210/391]	Time  0.119 ( 0.122)	Data  0.002 ( 0.002)	Loss 2.0293e+00 (2.3726e+00)	Acc@1  42.19 ( 37.25)	Acc@5  81.25 ( 70.12)
Epoch: [3][220/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.7344e+00 (2.3724e+00)	Acc@1  28.91 ( 37.24)	Acc@5  61.72 ( 70.13)
Epoch: [3][230/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1621e+00 (2.3664e+00)	Acc@1  42.97 ( 37.35)	Acc@5  74.22 ( 70.32)
Epoch: [3][240/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2988e+00 (2.3640e+00)	Acc@1  43.75 ( 37.45)	Acc@5  70.31 ( 70.43)
Epoch: [3][250/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2188e+00 (2.3599e+00)	Acc@1  42.19 ( 37.55)	Acc@5  71.09 ( 70.47)
Epoch: [3][260/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8965e+00 (2.3569e+00)	Acc@1  50.00 ( 37.61)	Acc@5  79.69 ( 70.52)
Epoch: [3][270/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.3789e+00 (2.3531e+00)	Acc@1  35.16 ( 37.68)	Acc@5  67.97 ( 70.62)
Epoch: [3][280/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2539e+00 (2.3508e+00)	Acc@1  42.97 ( 37.73)	Acc@5  69.53 ( 70.67)
Epoch: [3][290/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2285e+00 (2.3498e+00)	Acc@1  42.97 ( 37.80)	Acc@5  71.09 ( 70.68)
Epoch: [3][300/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1328e+00 (2.3494e+00)	Acc@1  45.31 ( 37.79)	Acc@5  75.78 ( 70.72)
Epoch: [3][310/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2969e+00 (2.3436e+00)	Acc@1  32.81 ( 37.90)	Acc@5  75.78 ( 70.86)
Epoch: [3][320/391]	Time  0.134 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2012e+00 (2.3385e+00)	Acc@1  39.06 ( 38.01)	Acc@5  78.12 ( 70.96)
Epoch: [3][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.9600e+00 (2.3350e+00)	Acc@1  48.44 ( 38.07)	Acc@5  75.78 ( 71.02)
Epoch: [3][340/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.3125e+00 (2.3291e+00)	Acc@1  41.41 ( 38.21)	Acc@5  67.97 ( 71.12)
Epoch: [3][350/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.3711e+00 (2.3275e+00)	Acc@1  41.41 ( 38.21)	Acc@5  71.88 ( 71.20)
Epoch: [3][360/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.3613e+00 (2.3225e+00)	Acc@1  42.19 ( 38.29)	Acc@5  72.66 ( 71.32)
Epoch: [3][370/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.4883e+00 (2.3231e+00)	Acc@1  32.03 ( 38.23)	Acc@5  64.06 ( 71.30)
Epoch: [3][380/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.1875e+00 (2.3189e+00)	Acc@1  43.75 ( 38.35)	Acc@5  72.66 ( 71.35)
Epoch: [3][390/391]	Time  0.105 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.1270e+00 (2.3174e+00)	Acc@1  51.25 ( 38.41)	Acc@5  72.50 ( 71.36)
## e[3] optimizer.zero_grad (sum) time: 0.6923003196716309
## e[3]       loss.backward (sum) time: 14.32286024093628
## e[3]      optimizer.step (sum) time: 8.167096138000488
## epoch[3] training(only) time: 47.69018507003784
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 2.3730e+00 (2.3730e+00)	Acc@1  40.00 ( 40.00)	Acc@5  70.00 ( 70.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.2578e+00 (2.2711e+00)	Acc@1  43.00 ( 41.18)	Acc@5  76.00 ( 73.55)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 2.0215e+00 (2.2418e+00)	Acc@1  45.00 ( 40.71)	Acc@5  74.00 ( 74.24)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 2.4902e+00 (2.2499e+00)	Acc@1  36.00 ( 40.39)	Acc@5  72.00 ( 74.23)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 2.2168e+00 (2.2419e+00)	Acc@1  40.00 ( 40.20)	Acc@5  76.00 ( 73.90)
Test: [ 50/100]	Time  0.058 ( 0.051)	Loss 2.0273e+00 (2.2476e+00)	Acc@1  47.00 ( 40.22)	Acc@5  70.00 ( 73.76)
Test: [ 60/100]	Time  0.047 ( 0.051)	Loss 2.1445e+00 (2.2389e+00)	Acc@1  41.00 ( 40.18)	Acc@5  75.00 ( 73.70)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 2.2676e+00 (2.2374e+00)	Acc@1  39.00 ( 40.35)	Acc@5  68.00 ( 73.56)
Test: [ 80/100]	Time  0.055 ( 0.050)	Loss 2.4277e+00 (2.2479e+00)	Acc@1  37.00 ( 40.04)	Acc@5  71.00 ( 73.38)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 2.3711e+00 (2.2404e+00)	Acc@1  43.00 ( 40.11)	Acc@5  68.00 ( 73.37)
 * Acc@1 40.140 Acc@5 73.450
### epoch[3] execution time: 52.74375081062317
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.312 ( 0.312)	Data  0.152 ( 0.152)	Loss 2.1367e+00 (2.1367e+00)	Acc@1  40.62 ( 40.62)	Acc@5  76.56 ( 76.56)
Epoch: [4][ 10/391]	Time  0.120 ( 0.138)	Data  0.001 ( 0.015)	Loss 2.1660e+00 (2.0568e+00)	Acc@1  39.84 ( 44.39)	Acc@5  74.22 ( 77.13)
Epoch: [4][ 20/391]	Time  0.121 ( 0.130)	Data  0.001 ( 0.008)	Loss 2.1738e+00 (2.0447e+00)	Acc@1  36.72 ( 44.20)	Acc@5  72.66 ( 76.49)
Epoch: [4][ 30/391]	Time  0.121 ( 0.127)	Data  0.001 ( 0.006)	Loss 2.2305e+00 (2.0693e+00)	Acc@1  37.50 ( 42.94)	Acc@5  75.00 ( 76.71)
Epoch: [4][ 40/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.005)	Loss 1.7705e+00 (2.0519e+00)	Acc@1  51.56 ( 43.45)	Acc@5  87.50 ( 76.87)
Epoch: [4][ 50/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.004)	Loss 2.1836e+00 (2.0486e+00)	Acc@1  44.53 ( 43.63)	Acc@5  74.22 ( 76.75)
Epoch: [4][ 60/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 2.0781e+00 (2.0446e+00)	Acc@1  49.22 ( 43.81)	Acc@5  72.66 ( 76.86)
Epoch: [4][ 70/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.003)	Loss 2.2520e+00 (2.0501e+00)	Acc@1  39.06 ( 43.68)	Acc@5  71.88 ( 76.91)
Epoch: [4][ 80/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.0352e+00 (2.0506e+00)	Acc@1  46.09 ( 43.72)	Acc@5  80.47 ( 76.89)
Epoch: [4][ 90/391]	Time  0.129 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.8486e+00 (2.0452e+00)	Acc@1  52.34 ( 43.97)	Acc@5  79.69 ( 76.97)
Epoch: [4][100/391]	Time  0.125 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.1680e+00 (2.0457e+00)	Acc@1  43.75 ( 43.87)	Acc@5  75.00 ( 76.93)
Epoch: [4][110/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.0566e+00 (2.0567e+00)	Acc@1  44.53 ( 43.67)	Acc@5  73.44 ( 76.63)
Epoch: [4][120/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.8604e+00 (2.0578e+00)	Acc@1  50.00 ( 43.71)	Acc@5  82.81 ( 76.59)
Epoch: [4][130/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.2949e+00 (2.0606e+00)	Acc@1  39.84 ( 43.71)	Acc@5  75.00 ( 76.66)
Epoch: [4][140/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.9795e+00 (2.0627e+00)	Acc@1  46.09 ( 43.78)	Acc@5  77.34 ( 76.65)
Epoch: [4][150/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8867e+00 (2.0637e+00)	Acc@1  44.53 ( 43.71)	Acc@5  82.03 ( 76.63)
Epoch: [4][160/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0957e+00 (2.0643e+00)	Acc@1  38.28 ( 43.58)	Acc@5  75.00 ( 76.65)
Epoch: [4][170/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2031e+00 (2.0614e+00)	Acc@1  39.84 ( 43.69)	Acc@5  73.44 ( 76.69)
Epoch: [4][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1133e+00 (2.0626e+00)	Acc@1  39.06 ( 43.71)	Acc@5  74.22 ( 76.69)
Epoch: [4][190/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1348e+00 (2.0632e+00)	Acc@1  43.75 ( 43.81)	Acc@5  70.31 ( 76.57)
Epoch: [4][200/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0840e+00 (2.0620e+00)	Acc@1  43.75 ( 43.82)	Acc@5  78.12 ( 76.62)
Epoch: [4][210/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0293e+00 (2.0614e+00)	Acc@1  46.09 ( 43.87)	Acc@5  78.91 ( 76.59)
Epoch: [4][220/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0879e+00 (2.0610e+00)	Acc@1  42.19 ( 43.85)	Acc@5  78.12 ( 76.65)
Epoch: [4][230/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9385e+00 (2.0585e+00)	Acc@1  46.09 ( 43.90)	Acc@5  83.59 ( 76.73)
Epoch: [4][240/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0840e+00 (2.0539e+00)	Acc@1  42.19 ( 44.00)	Acc@5  77.34 ( 76.80)
Epoch: [4][250/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8545e+00 (2.0495e+00)	Acc@1  49.22 ( 44.11)	Acc@5  81.25 ( 76.86)
Epoch: [4][260/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1133e+00 (2.0488e+00)	Acc@1  41.41 ( 44.15)	Acc@5  75.00 ( 76.89)
Epoch: [4][270/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2305e+00 (2.0462e+00)	Acc@1  37.50 ( 44.17)	Acc@5  74.22 ( 76.94)
Epoch: [4][280/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9814e+00 (2.0448e+00)	Acc@1  47.66 ( 44.23)	Acc@5  79.69 ( 76.96)
Epoch: [4][290/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1934e+00 (2.0448e+00)	Acc@1  39.84 ( 44.24)	Acc@5  75.78 ( 76.98)
Epoch: [4][300/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8809e+00 (2.0443e+00)	Acc@1  49.22 ( 44.23)	Acc@5  80.47 ( 77.00)
Epoch: [4][310/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9434e+00 (2.0425e+00)	Acc@1  41.41 ( 44.29)	Acc@5  78.91 ( 77.02)
Epoch: [4][320/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1543e+00 (2.0417e+00)	Acc@1  37.50 ( 44.28)	Acc@5  73.44 ( 77.02)
Epoch: [4][330/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9795e+00 (2.0412e+00)	Acc@1  45.31 ( 44.28)	Acc@5  78.91 ( 77.08)
Epoch: [4][340/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1504e+00 (2.0372e+00)	Acc@1  35.16 ( 44.34)	Acc@5  81.25 ( 77.14)
Epoch: [4][350/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0254e+00 (2.0357e+00)	Acc@1  40.62 ( 44.32)	Acc@5  78.12 ( 77.20)
Epoch: [4][360/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9395e+00 (2.0376e+00)	Acc@1  46.09 ( 44.31)	Acc@5  82.03 ( 77.17)
Epoch: [4][370/391]	Time  0.135 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.8057e+00 (2.0362e+00)	Acc@1  47.66 ( 44.36)	Acc@5  80.47 ( 77.16)
Epoch: [4][380/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.8564e+00 (2.0351e+00)	Acc@1  47.66 ( 44.37)	Acc@5  81.25 ( 77.16)
Epoch: [4][390/391]	Time  0.106 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.8506e+00 (2.0323e+00)	Acc@1  55.00 ( 44.39)	Acc@5  81.25 ( 77.21)
## e[4] optimizer.zero_grad (sum) time: 0.7015423774719238
## e[4]       loss.backward (sum) time: 14.255484342575073
## e[4]      optimizer.step (sum) time: 8.131795406341553
## epoch[4] training(only) time: 47.65990662574768
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.1484e+00 (2.1484e+00)	Acc@1  42.00 ( 42.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 2.1758e+00 (2.1150e+00)	Acc@1  43.00 ( 45.27)	Acc@5  79.00 ( 77.09)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.7686e+00 (2.1002e+00)	Acc@1  47.00 ( 44.90)	Acc@5  84.00 ( 76.38)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 2.1465e+00 (2.0917e+00)	Acc@1  43.00 ( 44.90)	Acc@5  74.00 ( 76.32)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 2.1992e+00 (2.0833e+00)	Acc@1  46.00 ( 44.73)	Acc@5  74.00 ( 76.29)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 2.0273e+00 (2.1024e+00)	Acc@1  48.00 ( 44.49)	Acc@5  73.00 ( 75.94)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.8564e+00 (2.0791e+00)	Acc@1  49.00 ( 44.85)	Acc@5  81.00 ( 76.44)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 2.2090e+00 (2.0812e+00)	Acc@1  41.00 ( 45.06)	Acc@5  74.00 ( 76.32)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 2.1543e+00 (2.0884e+00)	Acc@1  46.00 ( 44.77)	Acc@5  71.00 ( 76.25)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 2.2773e+00 (2.0794e+00)	Acc@1  49.00 ( 45.02)	Acc@5  73.00 ( 76.33)
 * Acc@1 44.980 Acc@5 76.460
### epoch[4] execution time: 52.70202970504761
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.276 ( 0.276)	Data  0.137 ( 0.137)	Loss 1.8779e+00 (1.8779e+00)	Acc@1  46.09 ( 46.09)	Acc@5  75.78 ( 75.78)
Epoch: [5][ 10/391]	Time  0.121 ( 0.136)	Data  0.001 ( 0.013)	Loss 1.8770e+00 (1.8070e+00)	Acc@1  50.78 ( 51.42)	Acc@5  78.12 ( 81.39)
Epoch: [5][ 20/391]	Time  0.122 ( 0.129)	Data  0.001 ( 0.008)	Loss 1.8887e+00 (1.8332e+00)	Acc@1  47.66 ( 50.37)	Acc@5  82.03 ( 80.92)
Epoch: [5][ 30/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.005)	Loss 1.9453e+00 (1.8480e+00)	Acc@1  49.22 ( 49.80)	Acc@5  76.56 ( 80.85)
Epoch: [5][ 40/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.004)	Loss 1.7314e+00 (1.8598e+00)	Acc@1  50.78 ( 49.50)	Acc@5  82.81 ( 80.74)
Epoch: [5][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.6904e+00 (1.8427e+00)	Acc@1  49.22 ( 49.66)	Acc@5  82.81 ( 81.27)
Epoch: [5][ 60/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.9541e+00 (1.8485e+00)	Acc@1  44.53 ( 49.28)	Acc@5  79.69 ( 80.97)
Epoch: [5][ 70/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.9414e+00 (1.8504e+00)	Acc@1  43.75 ( 48.93)	Acc@5  78.12 ( 80.94)
Epoch: [5][ 80/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.9785e+00 (1.8577e+00)	Acc@1  45.31 ( 48.75)	Acc@5  76.56 ( 80.67)
Epoch: [5][ 90/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.0664e+00 (1.8598e+00)	Acc@1  40.62 ( 48.51)	Acc@5  77.34 ( 80.56)
Epoch: [5][100/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.9814e+00 (1.8603e+00)	Acc@1  44.53 ( 48.54)	Acc@5  77.34 ( 80.49)
Epoch: [5][110/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.7021e+00 (1.8615e+00)	Acc@1  53.91 ( 48.55)	Acc@5  82.03 ( 80.46)
Epoch: [5][120/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.8057e+00 (1.8551e+00)	Acc@1  52.34 ( 48.62)	Acc@5  78.91 ( 80.49)
Epoch: [5][130/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6250e+00 (1.8476e+00)	Acc@1  51.56 ( 48.77)	Acc@5  85.94 ( 80.71)
Epoch: [5][140/391]	Time  0.133 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7344e+00 (1.8454e+00)	Acc@1  52.34 ( 48.84)	Acc@5  82.81 ( 80.77)
Epoch: [5][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7510e+00 (1.8436e+00)	Acc@1  49.22 ( 48.83)	Acc@5  85.94 ( 80.82)
Epoch: [5][160/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9844e+00 (1.8419e+00)	Acc@1  43.75 ( 48.95)	Acc@5  75.78 ( 80.81)
Epoch: [5][170/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9521e+00 (1.8359e+00)	Acc@1  46.88 ( 49.13)	Acc@5  75.78 ( 80.94)
Epoch: [5][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8545e+00 (1.8378e+00)	Acc@1  52.34 ( 49.21)	Acc@5  80.47 ( 80.87)
Epoch: [5][190/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5244e+00 (1.8351e+00)	Acc@1  57.81 ( 49.37)	Acc@5  88.28 ( 80.92)
Epoch: [5][200/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8955e+00 (1.8369e+00)	Acc@1  44.53 ( 49.30)	Acc@5  79.69 ( 80.86)
Epoch: [5][210/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6377e+00 (1.8375e+00)	Acc@1  53.12 ( 49.29)	Acc@5  82.81 ( 80.86)
Epoch: [5][220/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5664e+00 (1.8368e+00)	Acc@1  55.47 ( 49.24)	Acc@5  86.72 ( 80.85)
Epoch: [5][230/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9395e+00 (1.8356e+00)	Acc@1  51.56 ( 49.27)	Acc@5  76.56 ( 80.87)
Epoch: [5][240/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9092e+00 (1.8334e+00)	Acc@1  43.75 ( 49.36)	Acc@5  81.25 ( 80.90)
Epoch: [5][250/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6416e+00 (1.8324e+00)	Acc@1  51.56 ( 49.39)	Acc@5  86.72 ( 80.91)
Epoch: [5][260/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9482e+00 (1.8316e+00)	Acc@1  49.22 ( 49.40)	Acc@5  76.56 ( 80.91)
Epoch: [5][270/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9072e+00 (1.8326e+00)	Acc@1  42.19 ( 49.31)	Acc@5  77.34 ( 80.86)
Epoch: [5][280/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6553e+00 (1.8320e+00)	Acc@1  54.69 ( 49.34)	Acc@5  84.38 ( 80.87)
Epoch: [5][290/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7861e+00 (1.8314e+00)	Acc@1  53.12 ( 49.37)	Acc@5  80.47 ( 80.89)
Epoch: [5][300/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8965e+00 (1.8298e+00)	Acc@1  52.34 ( 49.45)	Acc@5  79.69 ( 80.88)
Epoch: [5][310/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.8740e+00 (1.8319e+00)	Acc@1  48.44 ( 49.41)	Acc@5  78.91 ( 80.86)
Epoch: [5][320/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.8770e+00 (1.8300e+00)	Acc@1  50.00 ( 49.42)	Acc@5  75.00 ( 80.87)
Epoch: [5][330/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.7354e+00 (1.8291e+00)	Acc@1  50.78 ( 49.42)	Acc@5  85.16 ( 80.90)
Epoch: [5][340/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6270e+00 (1.8280e+00)	Acc@1  51.56 ( 49.44)	Acc@5  86.72 ( 80.92)
Epoch: [5][350/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6260e+00 (1.8268e+00)	Acc@1  54.69 ( 49.49)	Acc@5  86.72 ( 80.96)
Epoch: [5][360/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6406e+00 (1.8233e+00)	Acc@1  50.78 ( 49.59)	Acc@5  83.59 ( 81.03)
Epoch: [5][370/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6436e+00 (1.8214e+00)	Acc@1  49.22 ( 49.61)	Acc@5  84.38 ( 81.02)
Epoch: [5][380/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6641e+00 (1.8204e+00)	Acc@1  56.25 ( 49.61)	Acc@5  83.59 ( 81.04)
Epoch: [5][390/391]	Time  0.108 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.9512e+00 (1.8214e+00)	Acc@1  45.00 ( 49.60)	Acc@5  78.75 ( 81.00)
## e[5] optimizer.zero_grad (sum) time: 0.6929800510406494
## e[5]       loss.backward (sum) time: 14.31409478187561
## e[5]      optimizer.step (sum) time: 8.187443971633911
## epoch[5] training(only) time: 47.70597314834595
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.2070e+00 (2.2070e+00)	Acc@1  47.00 ( 47.00)	Acc@5  70.00 ( 70.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.9365e+00 (1.9841e+00)	Acc@1  45.00 ( 47.91)	Acc@5  80.00 ( 78.09)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.5547e+00 (1.9504e+00)	Acc@1  49.00 ( 47.81)	Acc@5  85.00 ( 78.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 2.1680e+00 (1.9453e+00)	Acc@1  45.00 ( 47.74)	Acc@5  74.00 ( 78.16)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 2.0469e+00 (1.9318e+00)	Acc@1  52.00 ( 47.85)	Acc@5  80.00 ( 78.59)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.9316e+00 (1.9529e+00)	Acc@1  53.00 ( 47.57)	Acc@5  76.00 ( 78.29)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.6104e+00 (1.9344e+00)	Acc@1  52.00 ( 47.85)	Acc@5  84.00 ( 78.44)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 2.1602e+00 (1.9436e+00)	Acc@1  44.00 ( 48.06)	Acc@5  74.00 ( 78.28)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.1270e+00 (1.9568e+00)	Acc@1  44.00 ( 47.62)	Acc@5  71.00 ( 78.04)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.9346e+00 (1.9432e+00)	Acc@1  50.00 ( 47.98)	Acc@5  80.00 ( 78.30)
 * Acc@1 47.920 Acc@5 78.310
### epoch[5] execution time: 52.70932388305664
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.287 ( 0.287)	Data  0.136 ( 0.136)	Loss 1.5459e+00 (1.5459e+00)	Acc@1  57.03 ( 57.03)	Acc@5  85.94 ( 85.94)
Epoch: [6][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.013)	Loss 1.9131e+00 (1.6871e+00)	Acc@1  41.41 ( 52.77)	Acc@5  78.91 ( 83.95)
Epoch: [6][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.007)	Loss 1.6406e+00 (1.6869e+00)	Acc@1  50.78 ( 52.27)	Acc@5  85.16 ( 84.15)
Epoch: [6][ 30/391]	Time  0.131 ( 0.126)	Data  0.001 ( 0.005)	Loss 1.7588e+00 (1.6848e+00)	Acc@1  50.00 ( 52.49)	Acc@5  82.81 ( 83.59)
Epoch: [6][ 40/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.004)	Loss 1.6367e+00 (1.6905e+00)	Acc@1  57.81 ( 52.72)	Acc@5  85.16 ( 83.35)
Epoch: [6][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.7715e+00 (1.6910e+00)	Acc@1  57.81 ( 52.91)	Acc@5  82.81 ( 83.46)
Epoch: [6][ 60/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.5879e+00 (1.6716e+00)	Acc@1  50.00 ( 53.15)	Acc@5  88.28 ( 83.81)
Epoch: [6][ 70/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.5488e+00 (1.6710e+00)	Acc@1  57.81 ( 52.98)	Acc@5  85.16 ( 83.79)
Epoch: [6][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.5820e+00 (1.6690e+00)	Acc@1  55.47 ( 53.18)	Acc@5  87.50 ( 83.75)
Epoch: [6][ 90/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.7363e+00 (1.6712e+00)	Acc@1  52.34 ( 53.17)	Acc@5  82.03 ( 83.71)
Epoch: [6][100/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.8398e+00 (1.6723e+00)	Acc@1  51.56 ( 53.01)	Acc@5  81.25 ( 83.69)
Epoch: [6][110/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.7500e+00 (1.6797e+00)	Acc@1  52.34 ( 52.72)	Acc@5  80.47 ( 83.59)
Epoch: [6][120/391]	Time  0.133 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.6943e+00 (1.6821e+00)	Acc@1  54.69 ( 52.75)	Acc@5  84.38 ( 83.52)
Epoch: [6][130/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5107e+00 (1.6782e+00)	Acc@1  56.25 ( 52.83)	Acc@5  89.06 ( 83.64)
Epoch: [6][140/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6143e+00 (1.6758e+00)	Acc@1  58.59 ( 52.95)	Acc@5  83.59 ( 83.58)
Epoch: [6][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5752e+00 (1.6748e+00)	Acc@1  54.69 ( 52.95)	Acc@5  85.16 ( 83.59)
Epoch: [6][160/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5000e+00 (1.6715e+00)	Acc@1  54.69 ( 53.03)	Acc@5  85.94 ( 83.69)
Epoch: [6][170/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4893e+00 (1.6642e+00)	Acc@1  55.47 ( 53.29)	Acc@5  88.28 ( 83.78)
Epoch: [6][180/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6406e+00 (1.6655e+00)	Acc@1  48.44 ( 53.19)	Acc@5  84.38 ( 83.74)
Epoch: [6][190/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5752e+00 (1.6674e+00)	Acc@1  52.34 ( 53.14)	Acc@5  86.72 ( 83.77)
Epoch: [6][200/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7637e+00 (1.6686e+00)	Acc@1  49.22 ( 53.13)	Acc@5  87.50 ( 83.74)
Epoch: [6][210/391]	Time  0.132 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7490e+00 (1.6663e+00)	Acc@1  50.78 ( 53.22)	Acc@5  82.03 ( 83.75)
Epoch: [6][220/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6494e+00 (1.6671e+00)	Acc@1  49.22 ( 53.19)	Acc@5  82.03 ( 83.72)
Epoch: [6][230/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5986e+00 (1.6656e+00)	Acc@1  50.78 ( 53.19)	Acc@5  84.38 ( 83.80)
Epoch: [6][240/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6191e+00 (1.6652e+00)	Acc@1  55.47 ( 53.17)	Acc@5  86.72 ( 83.81)
Epoch: [6][250/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8877e+00 (1.6688e+00)	Acc@1  47.66 ( 53.13)	Acc@5  80.47 ( 83.74)
Epoch: [6][260/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5840e+00 (1.6685e+00)	Acc@1  53.12 ( 53.15)	Acc@5  88.28 ( 83.75)
Epoch: [6][270/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5020e+00 (1.6696e+00)	Acc@1  59.38 ( 53.19)	Acc@5  85.16 ( 83.71)
Epoch: [6][280/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6475e+00 (1.6721e+00)	Acc@1  55.47 ( 53.17)	Acc@5  85.94 ( 83.65)
Epoch: [6][290/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8369e+00 (1.6735e+00)	Acc@1  49.22 ( 53.08)	Acc@5  78.12 ( 83.64)
Epoch: [6][300/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9297e+00 (1.6748e+00)	Acc@1  53.91 ( 53.11)	Acc@5  76.56 ( 83.62)
Epoch: [6][310/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6279e+00 (1.6729e+00)	Acc@1  50.00 ( 53.18)	Acc@5  80.47 ( 83.63)
Epoch: [6][320/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6035e+00 (1.6723e+00)	Acc@1  51.56 ( 53.20)	Acc@5  86.72 ( 83.64)
Epoch: [6][330/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.5283e+00 (1.6718e+00)	Acc@1  53.12 ( 53.19)	Acc@5  90.62 ( 83.68)
Epoch: [6][340/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.5938e+00 (1.6718e+00)	Acc@1  52.34 ( 53.11)	Acc@5  83.59 ( 83.68)
Epoch: [6][350/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.7832e+00 (1.6734e+00)	Acc@1  46.88 ( 53.07)	Acc@5  78.91 ( 83.64)
Epoch: [6][360/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.7920e+00 (1.6734e+00)	Acc@1  46.88 ( 53.02)	Acc@5  77.34 ( 83.63)
Epoch: [6][370/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3564e+00 (1.6716e+00)	Acc@1  60.16 ( 53.11)	Acc@5  90.62 ( 83.67)
Epoch: [6][380/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4922e+00 (1.6700e+00)	Acc@1  53.91 ( 53.18)	Acc@5  89.84 ( 83.69)
Epoch: [6][390/391]	Time  0.109 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.9629e+00 (1.6701e+00)	Acc@1  46.25 ( 53.18)	Acc@5  81.25 ( 83.70)
## e[6] optimizer.zero_grad (sum) time: 0.690100908279419
## e[6]       loss.backward (sum) time: 14.290958881378174
## e[6]      optimizer.step (sum) time: 8.116259336471558
## epoch[6] training(only) time: 47.65720176696777
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.1426e+00 (2.1426e+00)	Acc@1  44.00 ( 44.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 1.8760e+00 (1.9334e+00)	Acc@1  47.00 ( 48.18)	Acc@5  80.00 ( 80.00)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.7119e+00 (1.8676e+00)	Acc@1  50.00 ( 49.48)	Acc@5  84.00 ( 80.48)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 2.2754e+00 (1.8867e+00)	Acc@1  43.00 ( 49.52)	Acc@5  76.00 ( 80.35)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 1.8867e+00 (1.8831e+00)	Acc@1  48.00 ( 49.63)	Acc@5  85.00 ( 80.56)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.9404e+00 (1.9114e+00)	Acc@1  48.00 ( 49.10)	Acc@5  77.00 ( 80.06)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.6445e+00 (1.8947e+00)	Acc@1  55.00 ( 49.39)	Acc@5  82.00 ( 80.15)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 2.0156e+00 (1.9008e+00)	Acc@1  49.00 ( 49.39)	Acc@5  78.00 ( 79.94)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.1914e+00 (1.9136e+00)	Acc@1  46.00 ( 49.22)	Acc@5  73.00 ( 79.70)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 2.0254e+00 (1.9109e+00)	Acc@1  50.00 ( 49.22)	Acc@5  77.00 ( 79.78)
 * Acc@1 49.190 Acc@5 79.730
### epoch[6] execution time: 52.65997505187988
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.276 ( 0.276)	Data  0.137 ( 0.137)	Loss 1.5049e+00 (1.5049e+00)	Acc@1  65.62 ( 65.62)	Acc@5  85.94 ( 85.94)
Epoch: [7][ 10/391]	Time  0.122 ( 0.136)	Data  0.001 ( 0.013)	Loss 1.2246e+00 (1.5403e+00)	Acc@1  63.28 ( 57.60)	Acc@5  89.84 ( 86.36)
Epoch: [7][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.007)	Loss 1.5791e+00 (1.5214e+00)	Acc@1  61.72 ( 57.51)	Acc@5  84.38 ( 86.79)
Epoch: [7][ 30/391]	Time  0.121 ( 0.126)	Data  0.001 ( 0.005)	Loss 1.4219e+00 (1.5221e+00)	Acc@1  57.03 ( 56.93)	Acc@5  85.16 ( 86.77)
Epoch: [7][ 40/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.004)	Loss 1.8057e+00 (1.5326e+00)	Acc@1  50.78 ( 56.59)	Acc@5  82.81 ( 86.26)
Epoch: [7][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.3574e+00 (1.5366e+00)	Acc@1  53.91 ( 56.13)	Acc@5  89.06 ( 86.09)
Epoch: [7][ 60/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.4561e+00 (1.5352e+00)	Acc@1  55.47 ( 56.05)	Acc@5  86.72 ( 86.18)
Epoch: [7][ 70/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.5342e+00 (1.5388e+00)	Acc@1  53.12 ( 55.97)	Acc@5  87.50 ( 86.19)
Epoch: [7][ 80/391]	Time  0.125 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.6436e+00 (1.5413e+00)	Acc@1  53.91 ( 56.13)	Acc@5  85.94 ( 86.10)
Epoch: [7][ 90/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.6250e+00 (1.5395e+00)	Acc@1  56.25 ( 56.30)	Acc@5  83.59 ( 86.16)
Epoch: [7][100/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.7227e+00 (1.5439e+00)	Acc@1  53.12 ( 56.13)	Acc@5  80.47 ( 86.05)
Epoch: [7][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5957e+00 (1.5436e+00)	Acc@1  58.59 ( 56.25)	Acc@5  84.38 ( 86.01)
Epoch: [7][120/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5293e+00 (1.5435e+00)	Acc@1  59.38 ( 56.28)	Acc@5  85.94 ( 85.99)
Epoch: [7][130/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4863e+00 (1.5452e+00)	Acc@1  60.16 ( 56.24)	Acc@5  85.94 ( 85.91)
Epoch: [7][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5977e+00 (1.5444e+00)	Acc@1  53.12 ( 56.22)	Acc@5  85.94 ( 85.93)
Epoch: [7][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3018e+00 (1.5454e+00)	Acc@1  64.06 ( 56.17)	Acc@5  86.72 ( 85.93)
Epoch: [7][160/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5273e+00 (1.5427e+00)	Acc@1  57.81 ( 56.28)	Acc@5  86.72 ( 85.91)
Epoch: [7][170/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7129e+00 (1.5407e+00)	Acc@1  53.12 ( 56.35)	Acc@5  85.16 ( 85.95)
Epoch: [7][180/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5137e+00 (1.5459e+00)	Acc@1  50.00 ( 56.11)	Acc@5  89.06 ( 85.92)
Epoch: [7][190/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6348e+00 (1.5463e+00)	Acc@1  50.78 ( 56.19)	Acc@5  85.94 ( 85.89)
Epoch: [7][200/391]	Time  0.133 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4287e+00 (1.5429e+00)	Acc@1  57.81 ( 56.30)	Acc@5  85.94 ( 85.91)
Epoch: [7][210/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5371e+00 (1.5441e+00)	Acc@1  54.69 ( 56.28)	Acc@5  86.72 ( 85.84)
Epoch: [7][220/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5996e+00 (1.5415e+00)	Acc@1  54.69 ( 56.38)	Acc@5  85.94 ( 85.87)
Epoch: [7][230/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4658e+00 (1.5436e+00)	Acc@1  61.72 ( 56.33)	Acc@5  89.06 ( 85.82)
Epoch: [7][240/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7314e+00 (1.5402e+00)	Acc@1  57.81 ( 56.43)	Acc@5  83.59 ( 85.89)
Epoch: [7][250/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4717e+00 (1.5393e+00)	Acc@1  57.03 ( 56.41)	Acc@5  89.06 ( 85.94)
Epoch: [7][260/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4580e+00 (1.5388e+00)	Acc@1  57.03 ( 56.40)	Acc@5  89.06 ( 85.92)
Epoch: [7][270/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.5820e+00 (1.5386e+00)	Acc@1  50.78 ( 56.36)	Acc@5  86.72 ( 85.94)
Epoch: [7][280/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.5557e+00 (1.5413e+00)	Acc@1  55.47 ( 56.31)	Acc@5  85.94 ( 85.90)
Epoch: [7][290/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6650e+00 (1.5397e+00)	Acc@1  51.56 ( 56.30)	Acc@5  82.03 ( 85.93)
Epoch: [7][300/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.8555e+00 (1.5435e+00)	Acc@1  51.56 ( 56.27)	Acc@5  83.59 ( 85.84)
Epoch: [7][310/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3164e+00 (1.5434e+00)	Acc@1  60.16 ( 56.24)	Acc@5  88.28 ( 85.83)
Epoch: [7][320/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4932e+00 (1.5406e+00)	Acc@1  60.16 ( 56.33)	Acc@5  86.72 ( 85.90)
Epoch: [7][330/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3672e+00 (1.5406e+00)	Acc@1  60.16 ( 56.29)	Acc@5  90.62 ( 85.93)
Epoch: [7][340/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4580e+00 (1.5411e+00)	Acc@1  56.25 ( 56.27)	Acc@5  89.06 ( 85.91)
Epoch: [7][350/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4668e+00 (1.5421e+00)	Acc@1  59.38 ( 56.23)	Acc@5  87.50 ( 85.86)
Epoch: [7][360/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6582e+00 (1.5417e+00)	Acc@1  55.47 ( 56.28)	Acc@5  84.38 ( 85.89)
Epoch: [7][370/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6445e+00 (1.5405e+00)	Acc@1  51.56 ( 56.34)	Acc@5  82.03 ( 85.91)
Epoch: [7][380/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6172e+00 (1.5413e+00)	Acc@1  55.47 ( 56.32)	Acc@5  82.81 ( 85.89)
Epoch: [7][390/391]	Time  0.106 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3174e+00 (1.5408e+00)	Acc@1  61.25 ( 56.30)	Acc@5  86.25 ( 85.91)
## e[7] optimizer.zero_grad (sum) time: 0.6914856433868408
## e[7]       loss.backward (sum) time: 14.306010246276855
## e[7]      optimizer.step (sum) time: 8.129698753356934
## epoch[7] training(only) time: 47.64361810684204
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.8164e+00 (1.8164e+00)	Acc@1  55.00 ( 55.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.6709e+00 (1.7849e+00)	Acc@1  50.00 ( 51.45)	Acc@5  87.00 ( 82.36)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.6230e+00 (1.7449e+00)	Acc@1  56.00 ( 52.52)	Acc@5  83.00 ( 82.90)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.8799e+00 (1.7423e+00)	Acc@1  48.00 ( 53.06)	Acc@5  80.00 ( 83.00)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.7568e+00 (1.7414e+00)	Acc@1  56.00 ( 52.78)	Acc@5  85.00 ( 83.39)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.6309e+00 (1.7458e+00)	Acc@1  53.00 ( 52.92)	Acc@5  83.00 ( 82.92)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.6494e+00 (1.7393e+00)	Acc@1  57.00 ( 53.07)	Acc@5  90.00 ( 83.07)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.7471e+00 (1.7438e+00)	Acc@1  56.00 ( 53.14)	Acc@5  85.00 ( 82.93)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.9609e+00 (1.7546e+00)	Acc@1  50.00 ( 52.68)	Acc@5  80.00 ( 82.77)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 1.7129e+00 (1.7429e+00)	Acc@1  57.00 ( 52.95)	Acc@5  84.00 ( 82.91)
 * Acc@1 52.860 Acc@5 82.830
### epoch[7] execution time: 52.66078495979309
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.278 ( 0.278)	Data  0.146 ( 0.146)	Loss 1.6611e+00 (1.6611e+00)	Acc@1  52.34 ( 52.34)	Acc@5  81.25 ( 81.25)
Epoch: [8][ 10/391]	Time  0.120 ( 0.136)	Data  0.001 ( 0.014)	Loss 1.7002e+00 (1.4595e+00)	Acc@1  57.03 ( 57.60)	Acc@5  85.16 ( 87.43)
Epoch: [8][ 20/391]	Time  0.122 ( 0.129)	Data  0.001 ( 0.008)	Loss 1.4443e+00 (1.4214e+00)	Acc@1  60.16 ( 59.26)	Acc@5  89.06 ( 88.13)
Epoch: [8][ 30/391]	Time  0.128 ( 0.127)	Data  0.001 ( 0.006)	Loss 1.2061e+00 (1.4060e+00)	Acc@1  64.06 ( 59.68)	Acc@5  89.84 ( 88.38)
Epoch: [8][ 40/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.005)	Loss 1.2578e+00 (1.4047e+00)	Acc@1  59.38 ( 60.08)	Acc@5  92.19 ( 88.19)
Epoch: [8][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.4023e+00 (1.4126e+00)	Acc@1  63.28 ( 59.83)	Acc@5  86.72 ( 87.84)
Epoch: [8][ 60/391]	Time  0.131 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.5234e+00 (1.4265e+00)	Acc@1  58.59 ( 59.35)	Acc@5  89.06 ( 87.79)
Epoch: [8][ 70/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.3174e+00 (1.4311e+00)	Acc@1  60.94 ( 59.32)	Acc@5  93.75 ( 87.80)
Epoch: [8][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.5273e+00 (1.4287e+00)	Acc@1  55.47 ( 59.03)	Acc@5  86.72 ( 87.99)
Epoch: [8][ 90/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.6143e+00 (1.4336e+00)	Acc@1  56.25 ( 58.85)	Acc@5  82.03 ( 87.81)
Epoch: [8][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3369e+00 (1.4323e+00)	Acc@1  64.06 ( 58.88)	Acc@5  88.28 ( 87.77)
Epoch: [8][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5508e+00 (1.4331e+00)	Acc@1  55.47 ( 58.76)	Acc@5  86.72 ( 87.80)
Epoch: [8][120/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4932e+00 (1.4359e+00)	Acc@1  55.47 ( 58.66)	Acc@5  82.81 ( 87.69)
Epoch: [8][130/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5361e+00 (1.4390e+00)	Acc@1  57.03 ( 58.46)	Acc@5  86.72 ( 87.63)
Epoch: [8][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3496e+00 (1.4385e+00)	Acc@1  57.81 ( 58.39)	Acc@5  90.62 ( 87.65)
Epoch: [8][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4580e+00 (1.4413e+00)	Acc@1  59.38 ( 58.38)	Acc@5  86.72 ( 87.56)
Epoch: [8][160/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1357e+00 (1.4445e+00)	Acc@1  65.62 ( 58.35)	Acc@5  89.84 ( 87.47)
Epoch: [8][170/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4658e+00 (1.4459e+00)	Acc@1  58.59 ( 58.37)	Acc@5  86.72 ( 87.44)
Epoch: [8][180/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3975e+00 (1.4429e+00)	Acc@1  54.69 ( 58.40)	Acc@5  89.84 ( 87.47)
Epoch: [8][190/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5527e+00 (1.4405e+00)	Acc@1  53.12 ( 58.51)	Acc@5  86.72 ( 87.53)
Epoch: [8][200/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1592e+00 (1.4402e+00)	Acc@1  66.41 ( 58.56)	Acc@5  92.19 ( 87.54)
Epoch: [8][210/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3125e+00 (1.4413e+00)	Acc@1  60.94 ( 58.60)	Acc@5  89.06 ( 87.47)
Epoch: [8][220/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6689e+00 (1.4418e+00)	Acc@1  55.47 ( 58.58)	Acc@5  84.38 ( 87.47)
Epoch: [8][230/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5342e+00 (1.4444e+00)	Acc@1  60.16 ( 58.52)	Acc@5  83.59 ( 87.41)
Epoch: [8][240/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6689e+00 (1.4442e+00)	Acc@1  51.56 ( 58.52)	Acc@5  78.91 ( 87.36)
Epoch: [8][250/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6533e+00 (1.4446e+00)	Acc@1  57.03 ( 58.50)	Acc@5  83.59 ( 87.36)
Epoch: [8][260/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5889e+00 (1.4442e+00)	Acc@1  55.47 ( 58.52)	Acc@5  85.16 ( 87.38)
Epoch: [8][270/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4189e+00 (1.4431e+00)	Acc@1  65.62 ( 58.60)	Acc@5  85.94 ( 87.40)
Epoch: [8][280/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3535e+00 (1.4445e+00)	Acc@1  67.97 ( 58.67)	Acc@5  87.50 ( 87.36)
Epoch: [8][290/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4736e+00 (1.4456e+00)	Acc@1  56.25 ( 58.63)	Acc@5  86.72 ( 87.30)
Epoch: [8][300/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4443e+00 (1.4458e+00)	Acc@1  58.59 ( 58.63)	Acc@5  88.28 ( 87.31)
Epoch: [8][310/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4062e+00 (1.4490e+00)	Acc@1  56.25 ( 58.55)	Acc@5  87.50 ( 87.25)
Epoch: [8][320/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6738e+00 (1.4471e+00)	Acc@1  49.22 ( 58.58)	Acc@5  83.59 ( 87.27)
Epoch: [8][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.5967e+00 (1.4483e+00)	Acc@1  53.12 ( 58.52)	Acc@5  86.72 ( 87.20)
Epoch: [8][340/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1660e+00 (1.4456e+00)	Acc@1  60.16 ( 58.58)	Acc@5  93.75 ( 87.24)
Epoch: [8][350/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2793e+00 (1.4456e+00)	Acc@1  59.38 ( 58.54)	Acc@5  88.28 ( 87.29)
Epoch: [8][360/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4688e+00 (1.4456e+00)	Acc@1  58.59 ( 58.51)	Acc@5  86.72 ( 87.31)
Epoch: [8][370/391]	Time  0.137 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.5322e+00 (1.4449e+00)	Acc@1  54.69 ( 58.50)	Acc@5  85.94 ( 87.31)
Epoch: [8][380/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4414e+00 (1.4439e+00)	Acc@1  57.03 ( 58.48)	Acc@5  89.84 ( 87.35)
Epoch: [8][390/391]	Time  0.106 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4395e+00 (1.4432e+00)	Acc@1  63.75 ( 58.53)	Acc@5  86.25 ( 87.37)
## e[8] optimizer.zero_grad (sum) time: 0.6892893314361572
## e[8]       loss.backward (sum) time: 14.28316879272461
## e[8]      optimizer.step (sum) time: 8.140952825546265
## epoch[8] training(only) time: 47.64379954338074
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.7100e+00 (1.7100e+00)	Acc@1  57.00 ( 57.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.061 ( 0.061)	Loss 1.5303e+00 (1.6677e+00)	Acc@1  57.00 ( 55.73)	Acc@5  83.00 ( 82.45)
Test: [ 20/100]	Time  0.051 ( 0.055)	Loss 1.5098e+00 (1.6530e+00)	Acc@1  59.00 ( 55.76)	Acc@5  81.00 ( 83.05)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.8311e+00 (1.6517e+00)	Acc@1  44.00 ( 55.00)	Acc@5  85.00 ( 83.32)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.7461e+00 (1.6564e+00)	Acc@1  55.00 ( 54.85)	Acc@5  85.00 ( 83.54)
Test: [ 50/100]	Time  0.050 ( 0.051)	Loss 1.7568e+00 (1.6665e+00)	Acc@1  57.00 ( 54.71)	Acc@5  82.00 ( 83.41)
Test: [ 60/100]	Time  0.057 ( 0.051)	Loss 1.4219e+00 (1.6488e+00)	Acc@1  59.00 ( 54.82)	Acc@5  87.00 ( 83.67)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.6396e+00 (1.6479e+00)	Acc@1  54.00 ( 54.73)	Acc@5  85.00 ( 83.65)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.7754e+00 (1.6557e+00)	Acc@1  55.00 ( 54.70)	Acc@5  79.00 ( 83.43)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.7393e+00 (1.6510e+00)	Acc@1  53.00 ( 54.70)	Acc@5  88.00 ( 83.64)
 * Acc@1 54.900 Acc@5 83.710
### epoch[8] execution time: 52.69728326797485
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.289 ( 0.289)	Data  0.144 ( 0.144)	Loss 1.2109e+00 (1.2109e+00)	Acc@1  61.72 ( 61.72)	Acc@5  93.75 ( 93.75)
Epoch: [9][ 10/391]	Time  0.120 ( 0.137)	Data  0.001 ( 0.014)	Loss 1.1973e+00 (1.3493e+00)	Acc@1  67.97 ( 61.36)	Acc@5  89.06 ( 88.64)
Epoch: [9][ 20/391]	Time  0.119 ( 0.129)	Data  0.001 ( 0.008)	Loss 1.5869e+00 (1.3370e+00)	Acc@1  53.91 ( 61.72)	Acc@5  85.16 ( 88.80)
Epoch: [9][ 30/391]	Time  0.123 ( 0.127)	Data  0.001 ( 0.006)	Loss 1.6045e+00 (1.3437e+00)	Acc@1  58.59 ( 61.84)	Acc@5  86.72 ( 89.09)
Epoch: [9][ 40/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.004)	Loss 1.0928e+00 (1.3322e+00)	Acc@1  68.75 ( 62.12)	Acc@5  92.97 ( 89.20)
Epoch: [9][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.2861e+00 (1.3241e+00)	Acc@1  59.38 ( 62.04)	Acc@5  93.75 ( 89.38)
Epoch: [9][ 60/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.2773e+00 (1.3076e+00)	Acc@1  56.25 ( 62.18)	Acc@5  91.41 ( 89.50)
Epoch: [9][ 70/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.3057e+00 (1.3097e+00)	Acc@1  64.84 ( 61.92)	Acc@5  87.50 ( 89.59)
Epoch: [9][ 80/391]	Time  0.124 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1553e+00 (1.2996e+00)	Acc@1  69.53 ( 62.36)	Acc@5  93.75 ( 89.74)
Epoch: [9][ 90/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.3174e+00 (1.3005e+00)	Acc@1  63.28 ( 62.34)	Acc@5  89.06 ( 89.56)
Epoch: [9][100/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.1904e+00 (1.3018e+00)	Acc@1  62.50 ( 62.25)	Acc@5  91.41 ( 89.44)
Epoch: [9][110/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.3848e+00 (1.3078e+00)	Acc@1  56.25 ( 62.00)	Acc@5  89.06 ( 89.46)
Epoch: [9][120/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.2666e+00 (1.3177e+00)	Acc@1  63.28 ( 61.87)	Acc@5  88.28 ( 89.23)
Epoch: [9][130/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.3955e+00 (1.3196e+00)	Acc@1  61.72 ( 61.83)	Acc@5  91.41 ( 89.15)
Epoch: [9][140/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.4766e+00 (1.3194e+00)	Acc@1  57.81 ( 61.84)	Acc@5  90.62 ( 89.18)
Epoch: [9][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5371e+00 (1.3234e+00)	Acc@1  57.81 ( 61.76)	Acc@5  84.38 ( 89.14)
Epoch: [9][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3457e+00 (1.3239e+00)	Acc@1  60.94 ( 61.75)	Acc@5  88.28 ( 89.12)
Epoch: [9][170/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3789e+00 (1.3275e+00)	Acc@1  58.59 ( 61.66)	Acc@5  90.62 ( 89.11)
Epoch: [9][180/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3330e+00 (1.3278e+00)	Acc@1  62.50 ( 61.58)	Acc@5  85.94 ( 89.14)
Epoch: [9][190/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5020e+00 (1.3323e+00)	Acc@1  61.72 ( 61.50)	Acc@5  83.59 ( 89.08)
Epoch: [9][200/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3418e+00 (1.3319e+00)	Acc@1  62.50 ( 61.48)	Acc@5  91.41 ( 89.13)
Epoch: [9][210/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1914e+00 (1.3316e+00)	Acc@1  63.28 ( 61.49)	Acc@5  92.19 ( 89.15)
Epoch: [9][220/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1855e+00 (1.3319e+00)	Acc@1  67.19 ( 61.56)	Acc@5  94.53 ( 89.15)
Epoch: [9][230/391]	Time  0.133 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4619e+00 (1.3356e+00)	Acc@1  60.16 ( 61.53)	Acc@5  86.72 ( 89.14)
Epoch: [9][240/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4473e+00 (1.3365e+00)	Acc@1  57.81 ( 61.47)	Acc@5  89.06 ( 89.14)
Epoch: [9][250/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1191e+00 (1.3360e+00)	Acc@1  66.41 ( 61.45)	Acc@5  92.19 ( 89.13)
Epoch: [9][260/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3760e+00 (1.3389e+00)	Acc@1  57.81 ( 61.33)	Acc@5  85.94 ( 89.06)
Epoch: [9][270/391]	Time  0.135 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3799e+00 (1.3420e+00)	Acc@1  62.50 ( 61.29)	Acc@5  89.06 ( 89.00)
Epoch: [9][280/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4004e+00 (1.3438e+00)	Acc@1  64.84 ( 61.29)	Acc@5  85.94 ( 88.97)
Epoch: [9][290/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2725e+00 (1.3432e+00)	Acc@1  68.75 ( 61.33)	Acc@5  90.62 ( 88.98)
Epoch: [9][300/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2656e+00 (1.3424e+00)	Acc@1  60.94 ( 61.36)	Acc@5  92.19 ( 89.01)
Epoch: [9][310/391]	Time  0.134 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2852e+00 (1.3423e+00)	Acc@1  62.50 ( 61.35)	Acc@5  90.62 ( 89.03)
Epoch: [9][320/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.5869e+00 (1.3427e+00)	Acc@1  59.38 ( 61.35)	Acc@5  83.59 ( 89.01)
Epoch: [9][330/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3145e+00 (1.3427e+00)	Acc@1  57.03 ( 61.34)	Acc@5  89.06 ( 88.98)
Epoch: [9][340/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0068e+00 (1.3416e+00)	Acc@1  67.97 ( 61.34)	Acc@5  92.19 ( 88.99)
Epoch: [9][350/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4482e+00 (1.3418e+00)	Acc@1  58.59 ( 61.38)	Acc@5  86.72 ( 88.97)
Epoch: [9][360/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2803e+00 (1.3414e+00)	Acc@1  67.97 ( 61.40)	Acc@5  89.84 ( 88.97)
Epoch: [9][370/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3809e+00 (1.3412e+00)	Acc@1  58.59 ( 61.38)	Acc@5  89.06 ( 88.98)
Epoch: [9][380/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.6416e+00 (1.3420e+00)	Acc@1  57.03 ( 61.37)	Acc@5  82.81 ( 88.96)
Epoch: [9][390/391]	Time  0.113 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3516e+00 (1.3440e+00)	Acc@1  58.75 ( 61.33)	Acc@5  86.25 ( 88.90)
## e[9] optimizer.zero_grad (sum) time: 0.6867434978485107
## e[9]       loss.backward (sum) time: 14.357912540435791
## e[9]      optimizer.step (sum) time: 8.168314218521118
## epoch[9] training(only) time: 47.7340772151947
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.5488e+00 (1.5488e+00)	Acc@1  58.00 ( 58.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.6562e+00 (1.6866e+00)	Acc@1  60.00 ( 56.00)	Acc@5  88.00 ( 85.18)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.3408e+00 (1.6007e+00)	Acc@1  62.00 ( 57.10)	Acc@5  90.00 ( 85.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.9443e+00 (1.5923e+00)	Acc@1  44.00 ( 56.87)	Acc@5  81.00 ( 85.55)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.6670e+00 (1.5915e+00)	Acc@1  58.00 ( 56.39)	Acc@5  87.00 ( 85.68)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.6191e+00 (1.6014e+00)	Acc@1  56.00 ( 55.88)	Acc@5  82.00 ( 85.14)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.3164e+00 (1.5806e+00)	Acc@1  64.00 ( 56.21)	Acc@5  90.00 ( 85.38)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.5938e+00 (1.5852e+00)	Acc@1  55.00 ( 56.32)	Acc@5  87.00 ( 85.31)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.8750e+00 (1.5932e+00)	Acc@1  55.00 ( 56.38)	Acc@5  80.00 ( 85.19)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.6367e+00 (1.5871e+00)	Acc@1  56.00 ( 56.63)	Acc@5  85.00 ( 85.29)
 * Acc@1 56.860 Acc@5 85.290
### epoch[9] execution time: 52.754727363586426
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.283 ( 0.283)	Data  0.141 ( 0.141)	Loss 1.3281e+00 (1.3281e+00)	Acc@1  62.50 ( 62.50)	Acc@5  91.41 ( 91.41)
Epoch: [10][ 10/391]	Time  0.120 ( 0.136)	Data  0.001 ( 0.014)	Loss 1.1396e+00 (1.2461e+00)	Acc@1  68.75 ( 63.49)	Acc@5  89.84 ( 90.77)
Epoch: [10][ 20/391]	Time  0.119 ( 0.129)	Data  0.001 ( 0.008)	Loss 1.0811e+00 (1.1981e+00)	Acc@1  66.41 ( 65.10)	Acc@5  94.53 ( 91.52)
Epoch: [10][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 1.2422e+00 (1.1955e+00)	Acc@1  62.50 ( 65.12)	Acc@5  89.84 ( 90.90)
Epoch: [10][ 40/391]	Time  0.122 ( 0.125)	Data  0.001 ( 0.004)	Loss 1.2285e+00 (1.2015e+00)	Acc@1  66.41 ( 65.03)	Acc@5  92.19 ( 90.97)
Epoch: [10][ 50/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.2939e+00 (1.2171e+00)	Acc@1  63.28 ( 64.28)	Acc@5  91.41 ( 90.93)
Epoch: [10][ 60/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.2305e+00 (1.2244e+00)	Acc@1  66.41 ( 64.28)	Acc@5  90.62 ( 90.89)
Epoch: [10][ 70/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.2344e+00 (1.2197e+00)	Acc@1  66.41 ( 64.22)	Acc@5  92.97 ( 91.09)
Epoch: [10][ 80/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.4092e+00 (1.2241e+00)	Acc@1  57.03 ( 64.16)	Acc@5  87.50 ( 90.98)
Epoch: [10][ 90/391]	Time  0.130 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1533e+00 (1.2244e+00)	Acc@1  65.62 ( 64.20)	Acc@5  89.06 ( 90.86)
Epoch: [10][100/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2744e+00 (1.2264e+00)	Acc@1  60.16 ( 64.14)	Acc@5  88.28 ( 90.79)
Epoch: [10][110/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2012e+00 (1.2297e+00)	Acc@1  64.06 ( 64.01)	Acc@5  91.41 ( 90.62)
Epoch: [10][120/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2051e+00 (1.2351e+00)	Acc@1  66.41 ( 63.92)	Acc@5  93.75 ( 90.53)
Epoch: [10][130/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2471e+00 (1.2374e+00)	Acc@1  64.06 ( 63.92)	Acc@5  90.62 ( 90.54)
Epoch: [10][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2480e+00 (1.2390e+00)	Acc@1  64.84 ( 63.84)	Acc@5  90.62 ( 90.55)
Epoch: [10][150/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3545e+00 (1.2445e+00)	Acc@1  61.72 ( 63.63)	Acc@5  90.62 ( 90.53)
Epoch: [10][160/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.6875e-01 (1.2403e+00)	Acc@1  75.00 ( 63.84)	Acc@5  92.97 ( 90.59)
Epoch: [10][170/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1836e+00 (1.2419e+00)	Acc@1  67.19 ( 63.84)	Acc@5  92.97 ( 90.56)
Epoch: [10][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2588e+00 (1.2469e+00)	Acc@1  57.03 ( 63.61)	Acc@5  91.41 ( 90.50)
Epoch: [10][190/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5332e+00 (1.2476e+00)	Acc@1  50.78 ( 63.54)	Acc@5  87.50 ( 90.43)
Epoch: [10][200/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3105e+00 (1.2524e+00)	Acc@1  63.28 ( 63.46)	Acc@5  89.06 ( 90.30)
Epoch: [10][210/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4043e+00 (1.2533e+00)	Acc@1  59.38 ( 63.49)	Acc@5  89.84 ( 90.30)
Epoch: [10][220/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0430e+00 (1.2519e+00)	Acc@1  66.41 ( 63.52)	Acc@5  93.75 ( 90.36)
Epoch: [10][230/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2363e+00 (1.2559e+00)	Acc@1  65.62 ( 63.47)	Acc@5  91.41 ( 90.29)
Epoch: [10][240/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2100e+00 (1.2568e+00)	Acc@1  65.62 ( 63.43)	Acc@5  91.41 ( 90.28)
Epoch: [10][250/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1504e+00 (1.2579e+00)	Acc@1  67.19 ( 63.40)	Acc@5  92.19 ( 90.26)
Epoch: [10][260/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2275e+00 (1.2590e+00)	Acc@1  61.72 ( 63.34)	Acc@5  90.62 ( 90.26)
Epoch: [10][270/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1650e+00 (1.2588e+00)	Acc@1  66.41 ( 63.30)	Acc@5  89.84 ( 90.23)
Epoch: [10][280/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1865e+00 (1.2582e+00)	Acc@1  66.41 ( 63.33)	Acc@5  91.41 ( 90.24)
Epoch: [10][290/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3047e+00 (1.2554e+00)	Acc@1  65.62 ( 63.40)	Acc@5  89.06 ( 90.30)
Epoch: [10][300/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0938e+00 (1.2556e+00)	Acc@1  67.19 ( 63.39)	Acc@5  95.31 ( 90.30)
Epoch: [10][310/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4023e+00 (1.2607e+00)	Acc@1  60.16 ( 63.28)	Acc@5  88.28 ( 90.22)
Epoch: [10][320/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2275e+00 (1.2625e+00)	Acc@1  62.50 ( 63.26)	Acc@5  88.28 ( 90.18)
Epoch: [10][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2715e+00 (1.2622e+00)	Acc@1  67.97 ( 63.29)	Acc@5  88.28 ( 90.16)
Epoch: [10][340/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3330e+00 (1.2633e+00)	Acc@1  60.16 ( 63.24)	Acc@5  90.62 ( 90.10)
Epoch: [10][350/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.5820e+00 (1.2667e+00)	Acc@1  57.03 ( 63.16)	Acc@5  82.81 ( 90.06)
Epoch: [10][360/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2549e+00 (1.2668e+00)	Acc@1  67.19 ( 63.16)	Acc@5  88.28 ( 90.01)
Epoch: [10][370/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1709e+00 (1.2647e+00)	Acc@1  68.75 ( 63.25)	Acc@5  89.84 ( 90.05)
Epoch: [10][380/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2803e+00 (1.2640e+00)	Acc@1  62.50 ( 63.30)	Acc@5  88.28 ( 90.05)
Epoch: [10][390/391]	Time  0.111 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3730e+00 (1.2639e+00)	Acc@1  60.00 ( 63.29)	Acc@5  88.75 ( 90.04)
## e[10] optimizer.zero_grad (sum) time: 0.6909623146057129
## e[10]       loss.backward (sum) time: 14.275158882141113
## e[10]      optimizer.step (sum) time: 8.114665269851685
## epoch[10] training(only) time: 47.57813501358032
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.4580e+00 (1.4580e+00)	Acc@1  61.00 ( 61.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.6475e+00 (1.6140e+00)	Acc@1  52.00 ( 58.18)	Acc@5  87.00 ( 84.82)
Test: [ 20/100]	Time  0.052 ( 0.055)	Loss 1.3906e+00 (1.5755e+00)	Acc@1  68.00 ( 58.81)	Acc@5  88.00 ( 85.24)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.8242e+00 (1.5921e+00)	Acc@1  51.00 ( 58.29)	Acc@5  84.00 ( 85.16)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.4609e+00 (1.5933e+00)	Acc@1  59.00 ( 58.17)	Acc@5  87.00 ( 85.34)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.5381e+00 (1.6036e+00)	Acc@1  59.00 ( 57.78)	Acc@5  85.00 ( 85.00)
Test: [ 60/100]	Time  0.055 ( 0.050)	Loss 1.4150e+00 (1.5840e+00)	Acc@1  63.00 ( 57.84)	Acc@5  88.00 ( 85.23)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.5342e+00 (1.5859e+00)	Acc@1  58.00 ( 57.94)	Acc@5  90.00 ( 85.23)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6621e+00 (1.5840e+00)	Acc@1  56.00 ( 57.74)	Acc@5  80.00 ( 85.28)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7354e+00 (1.5773e+00)	Acc@1  50.00 ( 57.73)	Acc@5  84.00 ( 85.34)
 * Acc@1 57.890 Acc@5 85.510
### epoch[10] execution time: 52.60430145263672
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.283 ( 0.283)	Data  0.149 ( 0.149)	Loss 1.3574e+00 (1.3574e+00)	Acc@1  62.50 ( 62.50)	Acc@5  89.06 ( 89.06)
Epoch: [11][ 10/391]	Time  0.119 ( 0.136)	Data  0.001 ( 0.014)	Loss 1.1465e+00 (1.1464e+00)	Acc@1  66.41 ( 66.48)	Acc@5  92.19 ( 91.05)
Epoch: [11][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.008)	Loss 1.1494e+00 (1.1382e+00)	Acc@1  65.62 ( 66.56)	Acc@5  89.84 ( 91.33)
Epoch: [11][ 30/391]	Time  0.121 ( 0.126)	Data  0.001 ( 0.006)	Loss 1.2305e+00 (1.1226e+00)	Acc@1  66.41 ( 67.14)	Acc@5  91.41 ( 91.63)
Epoch: [11][ 40/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.005)	Loss 1.1758e+00 (1.1249e+00)	Acc@1  63.28 ( 66.98)	Acc@5  92.97 ( 91.56)
Epoch: [11][ 50/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.1602e+00 (1.1295e+00)	Acc@1  67.97 ( 66.96)	Acc@5  92.19 ( 91.45)
Epoch: [11][ 60/391]	Time  0.124 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.1592e+00 (1.1447e+00)	Acc@1  65.62 ( 66.61)	Acc@5  92.97 ( 91.34)
Epoch: [11][ 70/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.4121e+00 (1.1524e+00)	Acc@1  60.16 ( 66.35)	Acc@5  84.38 ( 91.26)
Epoch: [11][ 80/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.0820e+00 (1.1517e+00)	Acc@1  67.19 ( 66.26)	Acc@5  90.62 ( 91.27)
Epoch: [11][ 90/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1689e+00 (1.1523e+00)	Acc@1  65.62 ( 66.35)	Acc@5  92.19 ( 91.17)
Epoch: [11][100/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.0967e+00 (1.1551e+00)	Acc@1  67.97 ( 66.27)	Acc@5  93.75 ( 91.13)
Epoch: [11][110/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.1875e+00 (1.1579e+00)	Acc@1  64.84 ( 66.26)	Acc@5  92.97 ( 91.16)
Epoch: [11][120/391]	Time  0.127 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.2109e+00 (1.1628e+00)	Acc@1  65.62 ( 66.14)	Acc@5  89.84 ( 91.11)
Epoch: [11][130/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3418e+00 (1.1654e+00)	Acc@1  60.94 ( 65.96)	Acc@5  90.62 ( 91.14)
Epoch: [11][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3145e+00 (1.1721e+00)	Acc@1  64.06 ( 65.79)	Acc@5  85.94 ( 91.06)
Epoch: [11][150/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3994e+00 (1.1750e+00)	Acc@1  62.50 ( 65.81)	Acc@5  89.84 ( 91.00)
Epoch: [11][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3672e+00 (1.1793e+00)	Acc@1  63.28 ( 65.72)	Acc@5  89.06 ( 90.97)
Epoch: [11][170/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2168e+00 (1.1831e+00)	Acc@1  60.16 ( 65.62)	Acc@5  89.06 ( 90.94)
Epoch: [11][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1914e+00 (1.1828e+00)	Acc@1  67.97 ( 65.63)	Acc@5  88.28 ( 90.92)
Epoch: [11][190/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2393e+00 (1.1837e+00)	Acc@1  65.62 ( 65.62)	Acc@5  91.41 ( 90.92)
Epoch: [11][200/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.9014e-01 (1.1833e+00)	Acc@1  75.00 ( 65.63)	Acc@5  94.53 ( 90.90)
Epoch: [11][210/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1680e+00 (1.1866e+00)	Acc@1  66.41 ( 65.53)	Acc@5  92.97 ( 90.91)
Epoch: [11][220/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2715e+00 (1.1863e+00)	Acc@1  63.28 ( 65.55)	Acc@5  91.41 ( 90.89)
Epoch: [11][230/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1592e+00 (1.1873e+00)	Acc@1  69.53 ( 65.52)	Acc@5  93.75 ( 90.89)
Epoch: [11][240/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1377e+00 (1.1871e+00)	Acc@1  68.75 ( 65.54)	Acc@5  92.19 ( 90.91)
Epoch: [11][250/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0371e+00 (1.1858e+00)	Acc@1  68.75 ( 65.51)	Acc@5  94.53 ( 90.93)
Epoch: [11][260/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1377e+00 (1.1869e+00)	Acc@1  64.84 ( 65.42)	Acc@5  93.75 ( 90.97)
Epoch: [11][270/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3623e+00 (1.1858e+00)	Acc@1  57.03 ( 65.47)	Acc@5  91.41 ( 90.99)
Epoch: [11][280/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1963e+00 (1.1854e+00)	Acc@1  60.94 ( 65.49)	Acc@5  91.41 ( 90.96)
Epoch: [11][290/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0488e+00 (1.1855e+00)	Acc@1  67.19 ( 65.51)	Acc@5  92.97 ( 90.95)
Epoch: [11][300/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3096e+00 (1.1825e+00)	Acc@1  57.03 ( 65.58)	Acc@5  91.41 ( 90.98)
Epoch: [11][310/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1973e+00 (1.1830e+00)	Acc@1  64.06 ( 65.54)	Acc@5  92.97 ( 91.00)
Epoch: [11][320/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.9902e-01 (1.1826e+00)	Acc@1  68.75 ( 65.53)	Acc@5  91.41 ( 90.99)
Epoch: [11][330/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1172e+00 (1.1842e+00)	Acc@1  68.75 ( 65.49)	Acc@5  90.62 ( 90.99)
Epoch: [11][340/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3027e+00 (1.1874e+00)	Acc@1  65.62 ( 65.48)	Acc@5  92.19 ( 90.95)
Epoch: [11][350/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1523e+00 (1.1885e+00)	Acc@1  64.06 ( 65.44)	Acc@5  92.97 ( 90.97)
Epoch: [11][360/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.3584e+00 (1.1891e+00)	Acc@1  60.16 ( 65.47)	Acc@5  86.72 ( 90.97)
Epoch: [11][370/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1865e+00 (1.1919e+00)	Acc@1  64.06 ( 65.36)	Acc@5  91.41 ( 90.95)
Epoch: [11][380/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.4326e+00 (1.1919e+00)	Acc@1  60.94 ( 65.39)	Acc@5  87.50 ( 90.94)
Epoch: [11][390/391]	Time  0.112 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2949e+00 (1.1927e+00)	Acc@1  68.75 ( 65.36)	Acc@5  91.25 ( 90.92)
## e[11] optimizer.zero_grad (sum) time: 0.6810624599456787
## e[11]       loss.backward (sum) time: 14.348809242248535
## e[11]      optimizer.step (sum) time: 8.142003774642944
## epoch[11] training(only) time: 47.653197050094604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.5410e+00 (1.5410e+00)	Acc@1  63.00 ( 63.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.6494e+00 (1.5265e+00)	Acc@1  53.00 ( 59.73)	Acc@5  84.00 ( 86.09)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0703e+00 (1.4629e+00)	Acc@1  67.00 ( 59.86)	Acc@5  91.00 ( 87.48)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.7471e+00 (1.4961e+00)	Acc@1  52.00 ( 59.58)	Acc@5  80.00 ( 86.00)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5869e+00 (1.4976e+00)	Acc@1  61.00 ( 59.02)	Acc@5  88.00 ( 86.39)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.5752e+00 (1.5111e+00)	Acc@1  58.00 ( 58.98)	Acc@5  83.00 ( 86.25)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2354e+00 (1.4918e+00)	Acc@1  65.00 ( 59.33)	Acc@5  92.00 ( 86.52)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.8086e+00 (1.4912e+00)	Acc@1  56.00 ( 59.48)	Acc@5  83.00 ( 86.59)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.5352e+00 (1.5017e+00)	Acc@1  60.00 ( 59.25)	Acc@5  84.00 ( 86.22)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 1.6260e+00 (1.4942e+00)	Acc@1  56.00 ( 59.40)	Acc@5  89.00 ( 86.38)
 * Acc@1 59.380 Acc@5 86.580
### epoch[11] execution time: 52.68838119506836
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.291 ( 0.291)	Data  0.152 ( 0.152)	Loss 1.0146e+00 (1.0146e+00)	Acc@1  68.75 ( 68.75)	Acc@5  92.97 ( 92.97)
Epoch: [12][ 10/391]	Time  0.120 ( 0.137)	Data  0.001 ( 0.015)	Loss 1.1387e+00 (1.0315e+00)	Acc@1  60.16 ( 68.25)	Acc@5  92.97 ( 93.39)
Epoch: [12][ 20/391]	Time  0.118 ( 0.130)	Data  0.001 ( 0.008)	Loss 8.1982e-01 (1.0612e+00)	Acc@1  77.34 ( 68.97)	Acc@5  92.19 ( 92.56)
Epoch: [12][ 30/391]	Time  0.121 ( 0.127)	Data  0.001 ( 0.006)	Loss 1.2168e+00 (1.0766e+00)	Acc@1  64.06 ( 68.32)	Acc@5  90.62 ( 92.49)
Epoch: [12][ 40/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.005)	Loss 1.0713e+00 (1.0821e+00)	Acc@1  71.88 ( 68.27)	Acc@5  90.62 ( 92.51)
Epoch: [12][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.2373e+00 (1.0736e+00)	Acc@1  57.81 ( 68.55)	Acc@5  91.41 ( 92.63)
Epoch: [12][ 60/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 9.5117e-01 (1.0726e+00)	Acc@1  73.44 ( 68.47)	Acc@5  93.75 ( 92.75)
Epoch: [12][ 70/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.2842e+00 (1.0689e+00)	Acc@1  65.62 ( 68.55)	Acc@5  89.84 ( 92.70)
Epoch: [12][ 80/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1074e+00 (1.0675e+00)	Acc@1  72.66 ( 68.59)	Acc@5  89.06 ( 92.67)
Epoch: [12][ 90/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1885e+00 (1.0740e+00)	Acc@1  64.06 ( 68.36)	Acc@5  89.06 ( 92.58)
Epoch: [12][100/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.0537e+00 (1.0736e+00)	Acc@1  64.84 ( 68.33)	Acc@5  91.41 ( 92.54)
Epoch: [12][110/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.2480e+00 (1.0724e+00)	Acc@1  63.28 ( 68.38)	Acc@5  89.06 ( 92.50)
Epoch: [12][120/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0068e+00 (1.0751e+00)	Acc@1  71.09 ( 68.31)	Acc@5  96.09 ( 92.55)
Epoch: [12][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1816e+00 (1.0801e+00)	Acc@1  67.97 ( 68.24)	Acc@5  92.19 ( 92.49)
Epoch: [12][140/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0586e+00 (1.0804e+00)	Acc@1  67.97 ( 68.20)	Acc@5  90.62 ( 92.48)
Epoch: [12][150/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.7510e-01 (1.0854e+00)	Acc@1  70.31 ( 68.13)	Acc@5  95.31 ( 92.43)
Epoch: [12][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1768e+00 (1.0900e+00)	Acc@1  61.72 ( 67.98)	Acc@5  93.75 ( 92.36)
Epoch: [12][170/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2852e+00 (1.0939e+00)	Acc@1  64.06 ( 67.86)	Acc@5  89.84 ( 92.36)
Epoch: [12][180/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0928e+00 (1.0964e+00)	Acc@1  67.19 ( 67.83)	Acc@5  93.75 ( 92.35)
Epoch: [12][190/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0049e+00 (1.0940e+00)	Acc@1  68.75 ( 67.83)	Acc@5  95.31 ( 92.36)
Epoch: [12][200/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.9414e-01 (1.0961e+00)	Acc@1  71.88 ( 67.82)	Acc@5  91.41 ( 92.30)
Epoch: [12][210/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0098e+00 (1.0954e+00)	Acc@1  64.84 ( 67.76)	Acc@5  96.09 ( 92.33)
Epoch: [12][220/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.6875e-01 (1.0951e+00)	Acc@1  73.44 ( 67.72)	Acc@5  93.75 ( 92.33)
Epoch: [12][230/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1963e+00 (1.0967e+00)	Acc@1  65.62 ( 67.70)	Acc@5  90.62 ( 92.29)
Epoch: [12][240/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2012e+00 (1.0992e+00)	Acc@1  61.72 ( 67.62)	Acc@5  92.19 ( 92.29)
Epoch: [12][250/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.9316e-01 (1.1016e+00)	Acc@1  67.97 ( 67.51)	Acc@5  94.53 ( 92.28)
Epoch: [12][260/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1172e+00 (1.1035e+00)	Acc@1  71.09 ( 67.47)	Acc@5  91.41 ( 92.23)
Epoch: [12][270/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1406e+00 (1.1064e+00)	Acc@1  68.75 ( 67.42)	Acc@5  89.06 ( 92.17)
Epoch: [12][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.9902e-01 (1.1058e+00)	Acc@1  75.00 ( 67.45)	Acc@5  92.19 ( 92.17)
Epoch: [12][290/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1465e+00 (1.1077e+00)	Acc@1  63.28 ( 67.34)	Acc@5  92.97 ( 92.16)
Epoch: [12][300/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4473e+00 (1.1096e+00)	Acc@1  57.03 ( 67.27)	Acc@5  85.16 ( 92.09)
Epoch: [12][310/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2979e+00 (1.1103e+00)	Acc@1  57.81 ( 67.22)	Acc@5  90.62 ( 92.09)
Epoch: [12][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2646e+00 (1.1109e+00)	Acc@1  62.50 ( 67.25)	Acc@5  89.06 ( 92.10)
Epoch: [12][330/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2822e+00 (1.1147e+00)	Acc@1  64.84 ( 67.16)	Acc@5  88.28 ( 92.05)
Epoch: [12][340/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2197e+00 (1.1170e+00)	Acc@1  67.97 ( 67.12)	Acc@5  92.19 ( 92.01)
Epoch: [12][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3066e+00 (1.1182e+00)	Acc@1  64.06 ( 67.10)	Acc@5  86.72 ( 91.98)
Epoch: [12][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0664e+00 (1.1191e+00)	Acc@1  67.19 ( 67.03)	Acc@5  90.62 ( 91.97)
Epoch: [12][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.0527e-01 (1.1194e+00)	Acc@1  75.78 ( 67.05)	Acc@5  93.75 ( 91.94)
Epoch: [12][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.1992e-01 (1.1209e+00)	Acc@1  71.09 ( 67.03)	Acc@5  95.31 ( 91.94)
Epoch: [12][390/391]	Time  0.109 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1074e+00 (1.1205e+00)	Acc@1  65.00 ( 67.05)	Acc@5  90.00 ( 91.95)
## e[12] optimizer.zero_grad (sum) time: 0.6897637844085693
## e[12]       loss.backward (sum) time: 14.24140739440918
## e[12]      optimizer.step (sum) time: 8.099913597106934
## epoch[12] training(only) time: 47.560450315475464
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.4639e+00 (1.4639e+00)	Acc@1  61.00 ( 61.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.5645e+00 (1.5724e+00)	Acc@1  64.00 ( 59.64)	Acc@5  88.00 ( 86.18)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.1240e+00 (1.5309e+00)	Acc@1  67.00 ( 59.52)	Acc@5  95.00 ( 87.05)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.7832e+00 (1.5571e+00)	Acc@1  52.00 ( 58.19)	Acc@5  83.00 ( 86.32)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.4922e+00 (1.5437e+00)	Acc@1  65.00 ( 58.56)	Acc@5  90.00 ( 86.56)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.7266e+00 (1.5519e+00)	Acc@1  60.00 ( 58.55)	Acc@5  79.00 ( 86.08)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2686e+00 (1.5385e+00)	Acc@1  58.00 ( 58.52)	Acc@5  93.00 ( 86.31)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.7109e+00 (1.5463e+00)	Acc@1  54.00 ( 58.45)	Acc@5  86.00 ( 86.34)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.7471e+00 (1.5505e+00)	Acc@1  54.00 ( 58.47)	Acc@5  83.00 ( 86.28)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.6963e+00 (1.5348e+00)	Acc@1  54.00 ( 58.77)	Acc@5  87.00 ( 86.51)
 * Acc@1 59.010 Acc@5 86.640
### epoch[12] execution time: 52.54797911643982
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.284 ( 0.284)	Data  0.144 ( 0.144)	Loss 1.0762e+00 (1.0762e+00)	Acc@1  66.41 ( 66.41)	Acc@5  94.53 ( 94.53)
Epoch: [13][ 10/391]	Time  0.128 ( 0.136)	Data  0.001 ( 0.014)	Loss 1.0781e+00 (9.7408e-01)	Acc@1  66.41 ( 69.74)	Acc@5  91.41 ( 94.60)
Epoch: [13][ 20/391]	Time  0.121 ( 0.129)	Data  0.001 ( 0.008)	Loss 9.1406e-01 (9.7417e-01)	Acc@1  71.09 ( 70.31)	Acc@5  92.97 ( 94.31)
Epoch: [13][ 30/391]	Time  0.121 ( 0.126)	Data  0.001 ( 0.006)	Loss 8.3252e-01 (9.7359e-01)	Acc@1  73.44 ( 70.34)	Acc@5  96.88 ( 94.15)
Epoch: [13][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 1.0908e+00 (9.9471e-01)	Acc@1  67.19 ( 69.91)	Acc@5  89.06 ( 93.81)
Epoch: [13][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.1357e+00 (1.0061e+00)	Acc@1  67.97 ( 69.64)	Acc@5  87.50 ( 93.54)
Epoch: [13][ 60/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.003)	Loss 9.4678e-01 (1.0039e+00)	Acc@1  71.09 ( 69.75)	Acc@5  92.97 ( 93.43)
Epoch: [13][ 70/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.2002e+00 (1.0107e+00)	Acc@1  65.62 ( 69.58)	Acc@5  91.41 ( 93.41)
Epoch: [13][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 9.9512e-01 (1.0193e+00)	Acc@1  70.31 ( 69.44)	Acc@5  90.62 ( 93.31)
Epoch: [13][ 90/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.0244e+00 (1.0175e+00)	Acc@1  71.88 ( 69.49)	Acc@5  93.75 ( 93.37)
Epoch: [13][100/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.002)	Loss 9.9316e-01 (1.0199e+00)	Acc@1  69.53 ( 69.45)	Acc@5  93.75 ( 93.32)
Epoch: [13][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0879e+00 (1.0201e+00)	Acc@1  69.53 ( 69.50)	Acc@5  92.19 ( 93.32)
Epoch: [13][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2617e+00 (1.0273e+00)	Acc@1  58.59 ( 69.37)	Acc@5  90.62 ( 93.23)
Epoch: [13][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3213e-01 (1.0259e+00)	Acc@1  71.09 ( 69.39)	Acc@5  93.75 ( 93.19)
Epoch: [13][140/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2549e+00 (1.0313e+00)	Acc@1  64.84 ( 69.24)	Acc@5  87.50 ( 93.14)
Epoch: [13][150/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.7217e-01 (1.0343e+00)	Acc@1  73.44 ( 69.16)	Acc@5  92.19 ( 93.07)
Epoch: [13][160/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0020e+00 (1.0319e+00)	Acc@1  68.75 ( 69.18)	Acc@5  96.09 ( 93.09)
Epoch: [13][170/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2031e+00 (1.0351e+00)	Acc@1  65.62 ( 69.06)	Acc@5  89.84 ( 93.06)
Epoch: [13][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1250e+00 (1.0381e+00)	Acc@1  65.62 ( 69.04)	Acc@5  89.84 ( 93.02)
Epoch: [13][190/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0039e+00 (1.0404e+00)	Acc@1  64.06 ( 68.94)	Acc@5  92.19 ( 92.98)
Epoch: [13][200/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1836e+00 (1.0464e+00)	Acc@1  65.62 ( 68.78)	Acc@5  90.62 ( 92.86)
Epoch: [13][210/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3115e-01 (1.0464e+00)	Acc@1  74.22 ( 68.85)	Acc@5  94.53 ( 92.85)
Epoch: [13][220/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0977e+00 (1.0514e+00)	Acc@1  67.19 ( 68.72)	Acc@5  92.97 ( 92.80)
Epoch: [13][230/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0156e+00 (1.0552e+00)	Acc@1  71.09 ( 68.68)	Acc@5  92.19 ( 92.76)
Epoch: [13][240/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0127e+00 (1.0521e+00)	Acc@1  68.75 ( 68.73)	Acc@5  95.31 ( 92.83)
Epoch: [13][250/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.6924e-01 (1.0507e+00)	Acc@1  68.75 ( 68.78)	Acc@5  94.53 ( 92.83)
Epoch: [13][260/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0820e+00 (1.0521e+00)	Acc@1  64.06 ( 68.69)	Acc@5  96.09 ( 92.84)
Epoch: [13][270/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0410e+00 (1.0527e+00)	Acc@1  71.09 ( 68.70)	Acc@5  90.62 ( 92.84)
Epoch: [13][280/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.8926e-01 (1.0541e+00)	Acc@1  70.31 ( 68.67)	Acc@5  91.41 ( 92.81)
Epoch: [13][290/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2402e+00 (1.0544e+00)	Acc@1  65.62 ( 68.66)	Acc@5  92.19 ( 92.83)
Epoch: [13][300/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0264e+00 (1.0553e+00)	Acc@1  68.75 ( 68.64)	Acc@5  92.97 ( 92.82)
Epoch: [13][310/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0957e+00 (1.0576e+00)	Acc@1  69.53 ( 68.58)	Acc@5  89.84 ( 92.80)
Epoch: [13][320/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2227e+00 (1.0587e+00)	Acc@1  60.16 ( 68.54)	Acc@5  89.84 ( 92.77)
Epoch: [13][330/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2266e+00 (1.0588e+00)	Acc@1  62.50 ( 68.53)	Acc@5  89.84 ( 92.78)
Epoch: [13][340/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.2080e+00 (1.0601e+00)	Acc@1  63.28 ( 68.50)	Acc@5  89.84 ( 92.75)
Epoch: [13][350/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0469e+00 (1.0599e+00)	Acc@1  71.09 ( 68.50)	Acc@5  92.19 ( 92.75)
Epoch: [13][360/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.6914e-01 (1.0616e+00)	Acc@1  72.66 ( 68.47)	Acc@5  95.31 ( 92.72)
Epoch: [13][370/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1182e+00 (1.0628e+00)	Acc@1  65.62 ( 68.38)	Acc@5  92.97 ( 92.71)
Epoch: [13][380/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.6826e-01 (1.0622e+00)	Acc@1  70.31 ( 68.43)	Acc@5  95.31 ( 92.71)
Epoch: [13][390/391]	Time  0.108 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0127e+00 (1.0631e+00)	Acc@1  71.25 ( 68.41)	Acc@5  95.00 ( 92.70)
## e[13] optimizer.zero_grad (sum) time: 0.6846308708190918
## e[13]       loss.backward (sum) time: 14.31795859336853
## e[13]      optimizer.step (sum) time: 8.14308476448059
## epoch[13] training(only) time: 47.603607177734375
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.4170e+00 (1.4170e+00)	Acc@1  62.00 ( 62.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.046 ( 0.063)	Loss 1.5039e+00 (1.4596e+00)	Acc@1  60.00 ( 61.73)	Acc@5  92.00 ( 88.27)
Test: [ 20/100]	Time  0.060 ( 0.056)	Loss 1.2227e+00 (1.3845e+00)	Acc@1  64.00 ( 62.76)	Acc@5  92.00 ( 89.05)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.7266e+00 (1.3942e+00)	Acc@1  51.00 ( 62.35)	Acc@5  89.00 ( 88.87)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.4600e+00 (1.4026e+00)	Acc@1  63.00 ( 61.90)	Acc@5  89.00 ( 88.51)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.4473e+00 (1.4224e+00)	Acc@1  57.00 ( 61.45)	Acc@5  88.00 ( 88.08)
Test: [ 60/100]	Time  0.051 ( 0.051)	Loss 1.1055e+00 (1.4052e+00)	Acc@1  68.00 ( 61.70)	Acc@5  95.00 ( 88.31)
Test: [ 70/100]	Time  0.055 ( 0.050)	Loss 1.6123e+00 (1.4112e+00)	Acc@1  56.00 ( 61.62)	Acc@5  88.00 ( 88.27)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.4141e+00 (1.4149e+00)	Acc@1  69.00 ( 61.69)	Acc@5  84.00 ( 88.05)
Test: [ 90/100]	Time  0.049 ( 0.050)	Loss 1.6572e+00 (1.4076e+00)	Acc@1  58.00 ( 61.74)	Acc@5  83.00 ( 88.16)
 * Acc@1 61.770 Acc@5 88.200
### epoch[13] execution time: 52.654820680618286
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.290 ( 0.290)	Data  0.156 ( 0.156)	Loss 8.5498e-01 (8.5498e-01)	Acc@1  72.66 ( 72.66)	Acc@5  93.75 ( 93.75)
Epoch: [14][ 10/391]	Time  0.119 ( 0.136)	Data  0.001 ( 0.015)	Loss 8.9307e-01 (9.3288e-01)	Acc@1  70.31 ( 71.66)	Acc@5  93.75 ( 93.89)
Epoch: [14][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.008)	Loss 1.0400e+00 (9.4717e-01)	Acc@1  67.97 ( 71.35)	Acc@5  95.31 ( 94.23)
Epoch: [14][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.006)	Loss 7.6807e-01 (9.4330e-01)	Acc@1  76.56 ( 71.67)	Acc@5  96.88 ( 94.25)
Epoch: [14][ 40/391]	Time  0.122 ( 0.125)	Data  0.001 ( 0.005)	Loss 9.1455e-01 (9.4584e-01)	Acc@1  74.22 ( 71.46)	Acc@5  93.75 ( 94.38)
Epoch: [14][ 50/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.1279e+00 (9.3154e-01)	Acc@1  66.41 ( 71.80)	Acc@5  92.19 ( 94.59)
Epoch: [14][ 60/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.004)	Loss 8.4229e-01 (9.3273e-01)	Acc@1  73.44 ( 71.72)	Acc@5  95.31 ( 94.51)
Epoch: [14][ 70/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.003)	Loss 8.9746e-01 (9.2556e-01)	Acc@1  73.44 ( 72.03)	Acc@5  95.31 ( 94.53)
Epoch: [14][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 9.1113e-01 (9.2755e-01)	Acc@1  71.88 ( 72.06)	Acc@5  95.31 ( 94.58)
Epoch: [14][ 90/391]	Time  0.125 ( 0.123)	Data  0.001 ( 0.003)	Loss 9.1650e-01 (9.2766e-01)	Acc@1  75.00 ( 72.15)	Acc@5  92.97 ( 94.52)
Epoch: [14][100/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.0205e+00 (9.3174e-01)	Acc@1  70.31 ( 71.94)	Acc@5  95.31 ( 94.55)
Epoch: [14][110/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.002)	Loss 9.8340e-01 (9.4353e-01)	Acc@1  68.75 ( 71.61)	Acc@5  91.41 ( 94.32)
Epoch: [14][120/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.1152e+00 (9.4907e-01)	Acc@1  71.09 ( 71.56)	Acc@5  88.28 ( 94.16)
Epoch: [14][130/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.0283e+00 (9.5039e-01)	Acc@1  67.19 ( 71.48)	Acc@5  92.97 ( 94.20)
Epoch: [14][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.8428e-01 (9.5145e-01)	Acc@1  71.88 ( 71.48)	Acc@5  94.53 ( 94.18)
Epoch: [14][150/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.7354e-01 (9.5445e-01)	Acc@1  71.09 ( 71.36)	Acc@5  96.88 ( 94.18)
Epoch: [14][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.5215e-01 (9.5800e-01)	Acc@1  67.19 ( 71.36)	Acc@5  93.75 ( 94.13)
Epoch: [14][170/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3359e-01 (9.5954e-01)	Acc@1  70.31 ( 71.36)	Acc@5  94.53 ( 94.08)
Epoch: [14][180/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0283e+00 (9.6543e-01)	Acc@1  65.62 ( 71.13)	Acc@5  95.31 ( 93.98)
Epoch: [14][190/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1084e+00 (9.6811e-01)	Acc@1  68.75 ( 71.09)	Acc@5  93.75 ( 93.93)
Epoch: [14][200/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.6387e-01 (9.6747e-01)	Acc@1  67.97 ( 71.07)	Acc@5  94.53 ( 93.89)
Epoch: [14][210/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0361e+00 (9.7105e-01)	Acc@1  67.19 ( 70.95)	Acc@5  96.09 ( 93.88)
Epoch: [14][220/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.2861e-01 (9.7301e-01)	Acc@1  75.00 ( 70.84)	Acc@5  94.53 ( 93.86)
Epoch: [14][230/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2793e+00 (9.7760e-01)	Acc@1  64.06 ( 70.79)	Acc@5  88.28 ( 93.79)
Epoch: [14][240/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0352e+00 (9.8009e-01)	Acc@1  74.22 ( 70.77)	Acc@5  91.41 ( 93.73)
Epoch: [14][250/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1572e+00 (9.8267e-01)	Acc@1  64.06 ( 70.70)	Acc@5  93.75 ( 93.71)
Epoch: [14][260/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0596e+00 (9.8550e-01)	Acc@1  64.84 ( 70.61)	Acc@5  92.19 ( 93.68)
Epoch: [14][270/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0781e+00 (9.8854e-01)	Acc@1  71.88 ( 70.55)	Acc@5  92.97 ( 93.64)
Epoch: [14][280/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1094e+00 (9.9173e-01)	Acc@1  67.19 ( 70.50)	Acc@5  95.31 ( 93.61)
Epoch: [14][290/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0391e+00 (9.9369e-01)	Acc@1  70.31 ( 70.45)	Acc@5  92.19 ( 93.59)
Epoch: [14][300/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0264e+00 (9.9291e-01)	Acc@1  66.41 ( 70.48)	Acc@5  92.19 ( 93.58)
Epoch: [14][310/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 7.6514e-01 (9.9523e-01)	Acc@1  77.34 ( 70.42)	Acc@5  96.88 ( 93.56)
Epoch: [14][320/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0166e+00 (9.9628e-01)	Acc@1  75.00 ( 70.41)	Acc@5  92.19 ( 93.54)
Epoch: [14][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0176e+00 (9.9758e-01)	Acc@1  71.88 ( 70.38)	Acc@5  93.75 ( 93.52)
Epoch: [14][340/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0186e+00 (9.9939e-01)	Acc@1  76.56 ( 70.32)	Acc@5  93.75 ( 93.50)
Epoch: [14][350/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0918e+00 (9.9982e-01)	Acc@1  66.41 ( 70.29)	Acc@5  91.41 ( 93.52)
Epoch: [14][360/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1436e+00 (1.0016e+00)	Acc@1  64.84 ( 70.24)	Acc@5  90.62 ( 93.49)
Epoch: [14][370/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.6279e-01 (1.0039e+00)	Acc@1  73.44 ( 70.17)	Acc@5  95.31 ( 93.47)
Epoch: [14][380/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.0078e-01 (1.0048e+00)	Acc@1  78.12 ( 70.18)	Acc@5  96.09 ( 93.46)
Epoch: [14][390/391]	Time  0.108 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1641e+00 (1.0046e+00)	Acc@1  65.00 ( 70.19)	Acc@5  91.25 ( 93.45)
## e[14] optimizer.zero_grad (sum) time: 0.6923110485076904
## e[14]       loss.backward (sum) time: 14.322929620742798
## e[14]      optimizer.step (sum) time: 8.192709922790527
## epoch[14] training(only) time: 47.686593532562256
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.4004e+00 (1.4004e+00)	Acc@1  65.00 ( 65.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.3525e+00 (1.4432e+00)	Acc@1  63.00 ( 61.91)	Acc@5  92.00 ( 87.73)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.2979e+00 (1.3821e+00)	Acc@1  66.00 ( 62.57)	Acc@5  90.00 ( 88.81)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6270e+00 (1.3882e+00)	Acc@1  53.00 ( 62.03)	Acc@5  86.00 ( 88.61)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.3574e+00 (1.3791e+00)	Acc@1  65.00 ( 62.12)	Acc@5  90.00 ( 88.61)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.3545e+00 (1.3998e+00)	Acc@1  60.00 ( 61.78)	Acc@5  87.00 ( 88.29)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2148e+00 (1.3822e+00)	Acc@1  63.00 ( 62.02)	Acc@5  91.00 ( 88.41)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.6211e+00 (1.3941e+00)	Acc@1  58.00 ( 61.86)	Acc@5  91.00 ( 88.39)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3818e+00 (1.3960e+00)	Acc@1  64.00 ( 61.94)	Acc@5  87.00 ( 88.46)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.6377e+00 (1.3862e+00)	Acc@1  55.00 ( 62.18)	Acc@5  85.00 ( 88.55)
 * Acc@1 62.350 Acc@5 88.770
### epoch[14] execution time: 52.70454120635986
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.289 ( 0.289)	Data  0.152 ( 0.152)	Loss 9.1602e-01 (9.1602e-01)	Acc@1  74.22 ( 74.22)	Acc@5  95.31 ( 95.31)
Epoch: [15][ 10/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.015)	Loss 8.1738e-01 (8.5778e-01)	Acc@1  74.22 ( 74.22)	Acc@5  97.66 ( 95.31)
Epoch: [15][ 20/391]	Time  0.119 ( 0.129)	Data  0.001 ( 0.008)	Loss 9.1309e-01 (8.5300e-01)	Acc@1  67.97 ( 74.59)	Acc@5  97.66 ( 95.72)
Epoch: [15][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 8.8574e-01 (8.6749e-01)	Acc@1  69.53 ( 73.79)	Acc@5  96.88 ( 95.46)
Epoch: [15][ 40/391]	Time  0.129 ( 0.125)	Data  0.001 ( 0.005)	Loss 1.0762e+00 (8.8849e-01)	Acc@1  64.06 ( 73.13)	Acc@5  94.53 ( 95.26)
Epoch: [15][ 50/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.004)	Loss 9.2432e-01 (8.9284e-01)	Acc@1  68.75 ( 72.90)	Acc@5  93.75 ( 95.02)
Epoch: [15][ 60/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.1523e+00 (8.9259e-01)	Acc@1  69.53 ( 73.09)	Acc@5  91.41 ( 94.97)
Epoch: [15][ 70/391]	Time  0.124 ( 0.123)	Data  0.001 ( 0.003)	Loss 8.1299e-01 (8.9572e-01)	Acc@1  76.56 ( 73.17)	Acc@5  94.53 ( 94.81)
Epoch: [15][ 80/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 8.3545e-01 (8.9653e-01)	Acc@1  75.78 ( 73.12)	Acc@5  94.53 ( 94.70)
Epoch: [15][ 90/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.2803e-01 (9.0090e-01)	Acc@1  78.91 ( 73.07)	Acc@5  94.53 ( 94.57)
Epoch: [15][100/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 9.7900e-01 (9.0640e-01)	Acc@1  67.97 ( 72.89)	Acc@5  92.19 ( 94.48)
Epoch: [15][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.7510e-01 (9.0498e-01)	Acc@1  70.31 ( 72.95)	Acc@5  91.41 ( 94.43)
Epoch: [15][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.8525e-01 (9.1060e-01)	Acc@1  71.09 ( 72.77)	Acc@5  94.53 ( 94.31)
Epoch: [15][130/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.9541e-01 (9.0869e-01)	Acc@1  78.91 ( 72.73)	Acc@5  93.75 ( 94.33)
Epoch: [15][140/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.5879e-01 (9.1009e-01)	Acc@1  80.47 ( 72.74)	Acc@5  96.09 ( 94.29)
Epoch: [15][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.2529e-01 (9.1045e-01)	Acc@1  67.19 ( 72.71)	Acc@5  94.53 ( 94.32)
Epoch: [15][160/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.0361e-01 (9.1521e-01)	Acc@1  82.03 ( 72.58)	Acc@5  97.66 ( 94.34)
Epoch: [15][170/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.4775e-01 (9.1787e-01)	Acc@1  67.19 ( 72.51)	Acc@5  92.97 ( 94.33)
Epoch: [15][180/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.5596e-01 (9.2120e-01)	Acc@1  72.66 ( 72.41)	Acc@5  95.31 ( 94.31)
Epoch: [15][190/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.8975e-01 (9.2360e-01)	Acc@1  70.31 ( 72.30)	Acc@5  92.97 ( 94.27)
Epoch: [15][200/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0576e+00 (9.2771e-01)	Acc@1  68.75 ( 72.16)	Acc@5  92.97 ( 94.19)
Epoch: [15][210/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.5303e-01 (9.2998e-01)	Acc@1  71.09 ( 72.10)	Acc@5  95.31 ( 94.20)
Epoch: [15][220/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2305e+00 (9.3390e-01)	Acc@1  65.62 ( 71.98)	Acc@5  87.50 ( 94.14)
Epoch: [15][230/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.1787e-01 (9.3431e-01)	Acc@1  72.66 ( 71.94)	Acc@5  94.53 ( 94.12)
Epoch: [15][240/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0469e+00 (9.3416e-01)	Acc@1  69.53 ( 71.97)	Acc@5  95.31 ( 94.14)
Epoch: [15][250/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.5977e-01 (9.3756e-01)	Acc@1  76.56 ( 71.87)	Acc@5  95.31 ( 94.11)
Epoch: [15][260/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1650e+00 (9.4116e-01)	Acc@1  67.97 ( 71.82)	Acc@5  89.84 ( 94.05)
Epoch: [15][270/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.9258e-01 (9.4268e-01)	Acc@1  68.75 ( 71.77)	Acc@5  94.53 ( 94.02)
Epoch: [15][280/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.1309e-01 (9.4291e-01)	Acc@1  76.56 ( 71.75)	Acc@5  96.09 ( 94.02)
Epoch: [15][290/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.2773e-01 (9.4149e-01)	Acc@1  74.22 ( 71.83)	Acc@5  92.97 ( 94.03)
Epoch: [15][300/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.1348e-01 (9.4304e-01)	Acc@1  75.00 ( 71.78)	Acc@5  94.53 ( 94.01)
Epoch: [15][310/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.8633e-01 (9.4539e-01)	Acc@1  71.09 ( 71.72)	Acc@5  92.19 ( 93.98)
Epoch: [15][320/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.5752e-01 (9.4578e-01)	Acc@1  69.53 ( 71.71)	Acc@5  95.31 ( 93.96)
Epoch: [15][330/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0273e+00 (9.4732e-01)	Acc@1  67.19 ( 71.66)	Acc@5  94.53 ( 93.96)
Epoch: [15][340/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.1895e-01 (9.4833e-01)	Acc@1  73.44 ( 71.62)	Acc@5  90.62 ( 93.94)
Epoch: [15][350/391]	Time  0.141 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1670e+00 (9.4911e-01)	Acc@1  65.62 ( 71.61)	Acc@5  91.41 ( 93.95)
Epoch: [15][360/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0234e+00 (9.4969e-01)	Acc@1  69.53 ( 71.59)	Acc@5  92.97 ( 93.95)
Epoch: [15][370/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.9854e-01 (9.5183e-01)	Acc@1  68.75 ( 71.48)	Acc@5  95.31 ( 93.93)
Epoch: [15][380/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.3740e-01 (9.5268e-01)	Acc@1  69.53 ( 71.44)	Acc@5  97.66 ( 93.93)
Epoch: [15][390/391]	Time  0.102 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1182e+00 (9.5271e-01)	Acc@1  66.25 ( 71.42)	Acc@5  93.75 ( 93.94)
## e[15] optimizer.zero_grad (sum) time: 0.6862137317657471
## e[15]       loss.backward (sum) time: 14.335480451583862
## e[15]      optimizer.step (sum) time: 8.081048011779785
## epoch[15] training(only) time: 47.58375406265259
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.5654e+00 (1.5654e+00)	Acc@1  60.00 ( 60.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 1.4180e+00 (1.3709e+00)	Acc@1  65.00 ( 64.00)	Acc@5  90.00 ( 88.27)
Test: [ 20/100]	Time  0.058 ( 0.055)	Loss 1.1475e+00 (1.3211e+00)	Acc@1  65.00 ( 64.81)	Acc@5  92.00 ( 89.14)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6416e+00 (1.3268e+00)	Acc@1  56.00 ( 64.29)	Acc@5  85.00 ( 88.97)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.4502e+00 (1.3402e+00)	Acc@1  63.00 ( 63.73)	Acc@5  87.00 ( 88.83)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.5215e+00 (1.3487e+00)	Acc@1  59.00 ( 63.57)	Acc@5  89.00 ( 88.80)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.1240e+00 (1.3300e+00)	Acc@1  66.00 ( 63.67)	Acc@5  92.00 ( 89.13)
Test: [ 70/100]	Time  0.056 ( 0.050)	Loss 1.5977e+00 (1.3346e+00)	Acc@1  62.00 ( 63.70)	Acc@5  90.00 ( 89.08)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.3545e+00 (1.3467e+00)	Acc@1  67.00 ( 63.62)	Acc@5  85.00 ( 88.84)
Test: [ 90/100]	Time  0.050 ( 0.050)	Loss 1.6865e+00 (1.3427e+00)	Acc@1  57.00 ( 63.69)	Acc@5  83.00 ( 88.90)
 * Acc@1 63.770 Acc@5 88.960
### epoch[15] execution time: 52.60350036621094
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.284 ( 0.284)	Data  0.145 ( 0.145)	Loss 7.4854e-01 (7.4854e-01)	Acc@1  76.56 ( 76.56)	Acc@5  95.31 ( 95.31)
Epoch: [16][ 10/391]	Time  0.120 ( 0.138)	Data  0.001 ( 0.014)	Loss 6.8213e-01 (7.7472e-01)	Acc@1  75.00 ( 75.50)	Acc@5  96.88 ( 95.95)
Epoch: [16][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.008)	Loss 6.0693e-01 (7.7455e-01)	Acc@1  78.91 ( 75.45)	Acc@5  97.66 ( 96.13)
Epoch: [16][ 30/391]	Time  0.122 ( 0.126)	Data  0.001 ( 0.006)	Loss 6.8652e-01 (7.9881e-01)	Acc@1  80.47 ( 75.38)	Acc@5  96.09 ( 95.89)
Epoch: [16][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.004)	Loss 9.3164e-01 (8.0734e-01)	Acc@1  76.56 ( 75.30)	Acc@5  94.53 ( 95.92)
Epoch: [16][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 7.9639e-01 (8.1347e-01)	Acc@1  75.78 ( 75.08)	Acc@5  92.97 ( 95.66)
Epoch: [16][ 60/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.003)	Loss 7.3096e-01 (8.1839e-01)	Acc@1  74.22 ( 74.85)	Acc@5  98.44 ( 95.67)
Epoch: [16][ 70/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.003)	Loss 8.0078e-01 (8.2090e-01)	Acc@1  71.09 ( 74.72)	Acc@5  94.53 ( 95.54)
Epoch: [16][ 80/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.9541e-01 (8.2495e-01)	Acc@1  75.00 ( 74.51)	Acc@5  93.75 ( 95.51)
Epoch: [16][ 90/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 8.1494e-01 (8.3123e-01)	Acc@1  72.66 ( 74.25)	Acc@5  97.66 ( 95.36)
Epoch: [16][100/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 8.0029e-01 (8.3220e-01)	Acc@1  74.22 ( 74.27)	Acc@5  96.09 ( 95.35)
Epoch: [16][110/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.0020e+00 (8.3494e-01)	Acc@1  67.97 ( 74.18)	Acc@5  93.75 ( 95.33)
Epoch: [16][120/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 8.3691e-01 (8.4237e-01)	Acc@1  76.56 ( 73.95)	Acc@5  94.53 ( 95.23)
Epoch: [16][130/391]	Time  0.127 ( 0.123)	Data  0.001 ( 0.002)	Loss 7.8174e-01 (8.4223e-01)	Acc@1  75.00 ( 74.02)	Acc@5  96.09 ( 95.21)
Epoch: [16][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.8770e-01 (8.4545e-01)	Acc@1  75.00 ( 73.86)	Acc@5  95.31 ( 95.20)
Epoch: [16][150/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3750e-01 (8.4894e-01)	Acc@1  74.22 ( 73.89)	Acc@5  91.41 ( 95.11)
Epoch: [16][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0049e+00 (8.5056e-01)	Acc@1  67.97 ( 73.98)	Acc@5  92.19 ( 95.07)
Epoch: [16][170/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1748e+00 (8.5259e-01)	Acc@1  70.31 ( 73.96)	Acc@5  88.28 ( 95.03)
Epoch: [16][180/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.9805e-01 (8.5937e-01)	Acc@1  71.88 ( 73.79)	Acc@5  90.62 ( 94.92)
Epoch: [16][190/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.7402e-01 (8.6230e-01)	Acc@1  74.22 ( 73.75)	Acc@5  97.66 ( 94.90)
Epoch: [16][200/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.3203e-01 (8.6189e-01)	Acc@1  72.66 ( 73.71)	Acc@5  97.66 ( 94.90)
Epoch: [16][210/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0107e+00 (8.6489e-01)	Acc@1  74.22 ( 73.65)	Acc@5  92.19 ( 94.86)
Epoch: [16][220/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.0908e-01 (8.6599e-01)	Acc@1  73.44 ( 73.65)	Acc@5  96.88 ( 94.88)
Epoch: [16][230/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.5879e-01 (8.7027e-01)	Acc@1  75.78 ( 73.55)	Acc@5  96.88 ( 94.84)
Epoch: [16][240/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.9121e-01 (8.7341e-01)	Acc@1  72.66 ( 73.47)	Acc@5  93.75 ( 94.80)
Epoch: [16][250/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.4238e-01 (8.7461e-01)	Acc@1  71.88 ( 73.42)	Acc@5  93.75 ( 94.81)
Epoch: [16][260/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1602e+00 (8.7844e-01)	Acc@1  61.72 ( 73.28)	Acc@5  92.19 ( 94.77)
Epoch: [16][270/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.4912e-01 (8.8006e-01)	Acc@1  73.44 ( 73.22)	Acc@5  96.09 ( 94.78)
Epoch: [16][280/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.3164e-01 (8.8270e-01)	Acc@1  71.09 ( 73.17)	Acc@5  93.75 ( 94.75)
Epoch: [16][290/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 7.9053e-01 (8.8500e-01)	Acc@1  74.22 ( 73.11)	Acc@5  96.88 ( 94.70)
Epoch: [16][300/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.6084e-01 (8.8572e-01)	Acc@1  75.78 ( 73.13)	Acc@5  95.31 ( 94.68)
Epoch: [16][310/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.6338e-01 (8.8958e-01)	Acc@1  72.66 ( 72.99)	Acc@5  94.53 ( 94.64)
Epoch: [16][320/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1221e+00 (8.9142e-01)	Acc@1  67.19 ( 72.94)	Acc@5  90.62 ( 94.61)
Epoch: [16][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.9014e-01 (8.9339e-01)	Acc@1  78.91 ( 72.92)	Acc@5  98.44 ( 94.59)
Epoch: [16][340/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0752e+00 (8.9562e-01)	Acc@1  66.41 ( 72.84)	Acc@5  94.53 ( 94.57)
Epoch: [16][350/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1084e+00 (8.9806e-01)	Acc@1  67.97 ( 72.81)	Acc@5  90.62 ( 94.54)
Epoch: [16][360/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.1768e+00 (9.0163e-01)	Acc@1  61.72 ( 72.68)	Acc@5  92.97 ( 94.49)
Epoch: [16][370/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0186e+00 (9.0242e-01)	Acc@1  64.06 ( 72.64)	Acc@5  92.97 ( 94.49)
Epoch: [16][380/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.1992e-01 (9.0458e-01)	Acc@1  71.88 ( 72.55)	Acc@5  93.75 ( 94.46)
Epoch: [16][390/391]	Time  0.108 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0488e+00 (9.0705e-01)	Acc@1  67.50 ( 72.51)	Acc@5  93.75 ( 94.46)
## e[16] optimizer.zero_grad (sum) time: 0.6848571300506592
## e[16]       loss.backward (sum) time: 14.34892725944519
## e[16]      optimizer.step (sum) time: 8.07992959022522
## epoch[16] training(only) time: 47.60643124580383
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.5947e+00 (1.5947e+00)	Acc@1  57.00 ( 57.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.4873e+00 (1.5336e+00)	Acc@1  65.00 ( 62.18)	Acc@5  89.00 ( 86.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1768e+00 (1.4629e+00)	Acc@1  71.00 ( 63.00)	Acc@5  92.00 ( 87.86)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.7461e+00 (1.4704e+00)	Acc@1  55.00 ( 62.58)	Acc@5  89.00 ( 88.06)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.5293e+00 (1.4614e+00)	Acc@1  67.00 ( 62.61)	Acc@5  88.00 ( 88.37)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.4580e+00 (1.4521e+00)	Acc@1  65.00 ( 62.47)	Acc@5  88.00 ( 88.45)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.3262e+00 (1.4369e+00)	Acc@1  65.00 ( 62.56)	Acc@5  90.00 ( 88.69)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4961e+00 (1.4386e+00)	Acc@1  67.00 ( 62.49)	Acc@5  87.00 ( 88.51)
Test: [ 80/100]	Time  0.051 ( 0.050)	Loss 1.5508e+00 (1.4357e+00)	Acc@1  64.00 ( 62.62)	Acc@5  82.00 ( 88.42)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.7012e+00 (1.4214e+00)	Acc@1  58.00 ( 62.85)	Acc@5  79.00 ( 88.51)
 * Acc@1 62.880 Acc@5 88.520
### epoch[16] execution time: 52.61414980888367
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.291 ( 0.291)	Data  0.152 ( 0.152)	Loss 7.3340e-01 (7.3340e-01)	Acc@1  77.34 ( 77.34)	Acc@5  95.31 ( 95.31)
Epoch: [17][ 10/391]	Time  0.119 ( 0.137)	Data  0.001 ( 0.015)	Loss 7.2607e-01 (7.8050e-01)	Acc@1  73.44 ( 75.36)	Acc@5  99.22 ( 95.67)
Epoch: [17][ 20/391]	Time  0.119 ( 0.130)	Data  0.001 ( 0.008)	Loss 8.1543e-01 (8.1355e-01)	Acc@1  73.44 ( 75.07)	Acc@5  96.88 ( 95.35)
Epoch: [17][ 30/391]	Time  0.122 ( 0.127)	Data  0.001 ( 0.006)	Loss 8.5156e-01 (8.0929e-01)	Acc@1  68.75 ( 75.18)	Acc@5  97.66 ( 95.51)
Epoch: [17][ 40/391]	Time  0.130 ( 0.126)	Data  0.001 ( 0.005)	Loss 7.4805e-01 (8.2294e-01)	Acc@1  72.66 ( 74.77)	Acc@5  96.09 ( 95.48)
Epoch: [17][ 50/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.004)	Loss 8.8916e-01 (8.3446e-01)	Acc@1  74.22 ( 74.37)	Acc@5  96.09 ( 95.44)
Epoch: [17][ 60/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 7.7246e-01 (8.2216e-01)	Acc@1  73.44 ( 74.51)	Acc@5  96.09 ( 95.63)
Epoch: [17][ 70/391]	Time  0.127 ( 0.124)	Data  0.001 ( 0.003)	Loss 8.2031e-01 (8.1423e-01)	Acc@1  73.44 ( 74.78)	Acc@5  94.53 ( 95.68)
Epoch: [17][ 80/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 8.1250e-01 (8.1133e-01)	Acc@1  77.34 ( 75.04)	Acc@5  93.75 ( 95.56)
Epoch: [17][ 90/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 9.6045e-01 (8.1402e-01)	Acc@1  71.88 ( 74.95)	Acc@5  93.75 ( 95.53)
Epoch: [17][100/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 9.6680e-01 (8.1726e-01)	Acc@1  70.31 ( 74.92)	Acc@5  95.31 ( 95.47)
Epoch: [17][110/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.002)	Loss 8.2422e-01 (8.1725e-01)	Acc@1  73.44 ( 74.85)	Acc@5  95.31 ( 95.50)
Epoch: [17][120/391]	Time  0.130 ( 0.123)	Data  0.001 ( 0.002)	Loss 1.0869e+00 (8.2346e-01)	Acc@1  62.50 ( 74.66)	Acc@5  92.19 ( 95.42)
Epoch: [17][130/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 6.9189e-01 (8.2382e-01)	Acc@1  79.69 ( 74.67)	Acc@5  96.09 ( 95.47)
Epoch: [17][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.7383e-01 (8.2665e-01)	Acc@1  77.34 ( 74.60)	Acc@5  98.44 ( 95.50)
Epoch: [17][150/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.1406e-01 (8.2980e-01)	Acc@1  71.09 ( 74.45)	Acc@5  95.31 ( 95.48)
Epoch: [17][160/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.0674e-01 (8.3123e-01)	Acc@1  68.75 ( 74.31)	Acc@5  95.31 ( 95.53)
Epoch: [17][170/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.2236e-01 (8.3443e-01)	Acc@1  72.66 ( 74.15)	Acc@5  96.88 ( 95.55)
Epoch: [17][180/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.3242e-01 (8.3625e-01)	Acc@1  79.69 ( 74.12)	Acc@5  97.66 ( 95.52)
Epoch: [17][190/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.1045e-01 (8.3645e-01)	Acc@1  78.12 ( 74.15)	Acc@5  96.09 ( 95.54)
Epoch: [17][200/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0566e+00 (8.3877e-01)	Acc@1  68.75 ( 74.09)	Acc@5  92.19 ( 95.50)
Epoch: [17][210/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.6533e-01 (8.4196e-01)	Acc@1  67.97 ( 73.99)	Acc@5  92.97 ( 95.46)
Epoch: [17][220/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.7939e-01 (8.4213e-01)	Acc@1  73.44 ( 73.97)	Acc@5  93.75 ( 95.48)
Epoch: [17][230/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.0039e-01 (8.4460e-01)	Acc@1  71.09 ( 73.93)	Acc@5  96.09 ( 95.47)
Epoch: [17][240/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3018e-01 (8.4542e-01)	Acc@1  75.00 ( 73.94)	Acc@5  93.75 ( 95.45)
Epoch: [17][250/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.7637e-01 (8.4556e-01)	Acc@1  76.56 ( 73.96)	Acc@5  95.31 ( 95.42)
Epoch: [17][260/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.7100e-01 (8.4406e-01)	Acc@1  75.00 ( 73.96)	Acc@5  97.66 ( 95.41)
Epoch: [17][270/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3359e-01 (8.4533e-01)	Acc@1  71.88 ( 73.89)	Acc@5  95.31 ( 95.42)
Epoch: [17][280/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.4219e-01 (8.4561e-01)	Acc@1  77.34 ( 73.89)	Acc@5  98.44 ( 95.42)
Epoch: [17][290/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.3643e-01 (8.4552e-01)	Acc@1  71.09 ( 73.92)	Acc@5  96.09 ( 95.41)
Epoch: [17][300/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0205e+00 (8.4669e-01)	Acc@1  69.53 ( 73.91)	Acc@5  89.84 ( 95.37)
Epoch: [17][310/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.4199e-01 (8.4825e-01)	Acc@1  83.59 ( 73.90)	Acc@5  98.44 ( 95.34)
Epoch: [17][320/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.7646e-01 (8.5021e-01)	Acc@1  76.56 ( 73.90)	Acc@5  92.97 ( 95.30)
Epoch: [17][330/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.0420e-01 (8.4996e-01)	Acc@1  77.34 ( 73.87)	Acc@5  93.75 ( 95.30)
Epoch: [17][340/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.5703e-01 (8.5265e-01)	Acc@1  71.09 ( 73.81)	Acc@5  96.09 ( 95.28)
Epoch: [17][350/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.9844e-01 (8.5534e-01)	Acc@1  73.44 ( 73.74)	Acc@5  93.75 ( 95.25)
Epoch: [17][360/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.1113e-01 (8.5851e-01)	Acc@1  78.12 ( 73.67)	Acc@5  93.75 ( 95.18)
Epoch: [17][370/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.0361e-01 (8.5856e-01)	Acc@1  75.78 ( 73.67)	Acc@5  97.66 ( 95.20)
Epoch: [17][380/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.2031e-01 (8.6017e-01)	Acc@1  78.91 ( 73.67)	Acc@5  93.75 ( 95.15)
Epoch: [17][390/391]	Time  0.104 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.9307e-01 (8.6166e-01)	Acc@1  73.75 ( 73.63)	Acc@5  95.00 ( 95.13)
## e[17] optimizer.zero_grad (sum) time: 0.6877446174621582
## e[17]       loss.backward (sum) time: 14.327274322509766
## e[17]      optimizer.step (sum) time: 8.205421447753906
## epoch[17] training(only) time: 47.64458656311035
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.5537e+00 (1.5537e+00)	Acc@1  58.00 ( 58.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.5547e+00 (1.5040e+00)	Acc@1  65.00 ( 61.45)	Acc@5  88.00 ( 87.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.2793e+00 (1.4438e+00)	Acc@1  66.00 ( 62.33)	Acc@5  89.00 ( 88.14)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6357e+00 (1.4577e+00)	Acc@1  56.00 ( 61.68)	Acc@5  86.00 ( 88.10)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.2520e+00 (1.4442e+00)	Acc@1  69.00 ( 61.85)	Acc@5  91.00 ( 88.41)
Test: [ 50/100]	Time  0.050 ( 0.051)	Loss 1.8467e+00 (1.4553e+00)	Acc@1  57.00 ( 61.94)	Acc@5  82.00 ( 88.10)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.4678e+00 (1.4459e+00)	Acc@1  60.00 ( 61.85)	Acc@5  93.00 ( 88.44)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.5850e+00 (1.4530e+00)	Acc@1  60.00 ( 61.62)	Acc@5  88.00 ( 88.42)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.7637e+00 (1.4618e+00)	Acc@1  55.00 ( 61.46)	Acc@5  83.00 ( 88.19)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.7070e+00 (1.4519e+00)	Acc@1  57.00 ( 61.66)	Acc@5  85.00 ( 88.22)
 * Acc@1 61.820 Acc@5 88.300
### epoch[17] execution time: 52.6407253742218
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.280 ( 0.280)	Data  0.141 ( 0.141)	Loss 8.0859e-01 (8.0859e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.09 ( 96.09)
Epoch: [18][ 10/391]	Time  0.124 ( 0.137)	Data  0.001 ( 0.014)	Loss 8.0322e-01 (7.7202e-01)	Acc@1  75.00 ( 75.78)	Acc@5  96.09 ( 96.38)
Epoch: [18][ 20/391]	Time  0.120 ( 0.130)	Data  0.001 ( 0.008)	Loss 7.2607e-01 (7.7139e-01)	Acc@1  81.25 ( 76.00)	Acc@5  95.31 ( 96.35)
Epoch: [18][ 30/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.005)	Loss 7.3486e-01 (7.8525e-01)	Acc@1  75.00 ( 75.73)	Acc@5  97.66 ( 96.14)
Epoch: [18][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.004)	Loss 8.8672e-01 (7.6991e-01)	Acc@1  71.09 ( 75.90)	Acc@5  93.75 ( 96.25)
Epoch: [18][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 7.7979e-01 (7.5866e-01)	Acc@1  74.22 ( 76.16)	Acc@5  96.88 ( 96.51)
Epoch: [18][ 60/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.003)	Loss 7.5537e-01 (7.5827e-01)	Acc@1  77.34 ( 76.33)	Acc@5  97.66 ( 96.41)
Epoch: [18][ 70/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.9834e-01 (7.6378e-01)	Acc@1  81.25 ( 76.41)	Acc@5  92.19 ( 96.37)
Epoch: [18][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.7578e-01 (7.6247e-01)	Acc@1  74.22 ( 76.49)	Acc@5  98.44 ( 96.32)
Epoch: [18][ 90/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.3770e-01 (7.6566e-01)	Acc@1  78.12 ( 76.36)	Acc@5  97.66 ( 96.23)
Epoch: [18][100/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.002)	Loss 7.3633e-01 (7.6554e-01)	Acc@1  78.91 ( 76.42)	Acc@5  95.31 ( 96.27)
Epoch: [18][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.5488e-01 (7.6868e-01)	Acc@1  74.22 ( 76.34)	Acc@5  96.88 ( 96.21)
Epoch: [18][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.9590e-01 (7.7443e-01)	Acc@1  77.34 ( 76.15)	Acc@5  96.09 ( 96.15)
Epoch: [18][130/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.5693e-01 (7.8021e-01)	Acc@1  72.66 ( 75.94)	Acc@5  96.09 ( 96.05)
Epoch: [18][140/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.9141e-01 (7.7990e-01)	Acc@1  79.69 ( 75.91)	Acc@5  96.09 ( 96.09)
Epoch: [18][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.5684e-01 (7.8295e-01)	Acc@1  78.91 ( 75.81)	Acc@5  94.53 ( 96.01)
Epoch: [18][160/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3750e-01 (7.8541e-01)	Acc@1  73.44 ( 75.78)	Acc@5  92.19 ( 95.99)
Epoch: [18][170/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0039e+00 (7.9257e-01)	Acc@1  71.09 ( 75.62)	Acc@5  94.53 ( 95.94)
Epoch: [18][180/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.7734e-01 (7.9219e-01)	Acc@1  70.31 ( 75.65)	Acc@5  97.66 ( 95.93)
Epoch: [18][190/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.6182e-01 (7.9336e-01)	Acc@1  73.44 ( 75.61)	Acc@5  93.75 ( 95.92)
Epoch: [18][200/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.7354e-01 (7.9865e-01)	Acc@1  75.78 ( 75.49)	Acc@5  95.31 ( 95.85)
Epoch: [18][210/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.0957e-01 (8.0012e-01)	Acc@1  76.56 ( 75.49)	Acc@5  93.75 ( 95.80)
Epoch: [18][220/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.2920e-01 (8.0205e-01)	Acc@1  70.31 ( 75.40)	Acc@5  92.97 ( 95.79)
Epoch: [18][230/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.6865e-01 (8.0329e-01)	Acc@1  71.09 ( 75.34)	Acc@5  97.66 ( 95.78)
Epoch: [18][240/391]	Time  0.135 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.6748e-01 (8.0540e-01)	Acc@1  75.78 ( 75.24)	Acc@5  99.22 ( 95.78)
Epoch: [18][250/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.4375e-01 (8.0843e-01)	Acc@1  75.00 ( 75.18)	Acc@5  96.88 ( 95.75)
Epoch: [18][260/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3555e-01 (8.1130e-01)	Acc@1  70.31 ( 75.11)	Acc@5  96.09 ( 95.74)
Epoch: [18][270/391]	Time  0.134 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.0068e-01 (8.1090e-01)	Acc@1  78.12 ( 75.15)	Acc@5  96.09 ( 95.75)
Epoch: [18][280/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.6670e-01 (8.0809e-01)	Acc@1  73.44 ( 75.26)	Acc@5  93.75 ( 95.76)
Epoch: [18][290/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.6357e-01 (8.0772e-01)	Acc@1  80.47 ( 75.32)	Acc@5  95.31 ( 95.75)
Epoch: [18][300/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 7.7295e-01 (8.1155e-01)	Acc@1  75.78 ( 75.18)	Acc@5  95.31 ( 95.71)
Epoch: [18][310/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0303e+00 (8.1355e-01)	Acc@1  71.09 ( 75.15)	Acc@5  94.53 ( 95.68)
Epoch: [18][320/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.0664e-01 (8.1389e-01)	Acc@1  75.00 ( 75.17)	Acc@5  95.31 ( 95.68)
Epoch: [18][330/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.0088e-01 (8.1602e-01)	Acc@1  75.00 ( 75.12)	Acc@5  94.53 ( 95.65)
Epoch: [18][340/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 1.0029e+00 (8.1658e-01)	Acc@1  68.75 ( 75.11)	Acc@5  94.53 ( 95.63)
Epoch: [18][350/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.7861e-01 (8.1584e-01)	Acc@1  77.34 ( 75.12)	Acc@5  97.66 ( 95.66)
Epoch: [18][360/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.6924e-01 (8.1565e-01)	Acc@1  71.09 ( 75.06)	Acc@5  92.97 ( 95.67)
Epoch: [18][370/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.001)	Loss 6.6553e-01 (8.1738e-01)	Acc@1  79.69 ( 75.01)	Acc@5  96.88 ( 95.64)
Epoch: [18][380/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 9.9219e-01 (8.1922e-01)	Acc@1  69.53 ( 74.94)	Acc@5  92.19 ( 95.61)
Epoch: [18][390/391]	Time  0.109 ( 0.122)	Data  0.001 ( 0.001)	Loss 8.3545e-01 (8.2067e-01)	Acc@1  75.00 ( 74.92)	Acc@5  95.00 ( 95.60)
## e[18] optimizer.zero_grad (sum) time: 0.6935181617736816
## e[18]       loss.backward (sum) time: 14.287440299987793
## e[18]      optimizer.step (sum) time: 8.245242834091187
## epoch[18] training(only) time: 47.64351797103882
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.4014e+00 (1.4014e+00)	Acc@1  62.00 ( 62.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.6709e+00 (1.4998e+00)	Acc@1  57.00 ( 63.55)	Acc@5  90.00 ( 87.73)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.2529e+00 (1.4024e+00)	Acc@1  69.00 ( 63.71)	Acc@5  90.00 ( 89.14)
Test: [ 30/100]	Time  0.055 ( 0.053)	Loss 1.6709e+00 (1.4210e+00)	Acc@1  61.00 ( 63.29)	Acc@5  88.00 ( 88.94)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.4746e+00 (1.4161e+00)	Acc@1  59.00 ( 62.73)	Acc@5  92.00 ( 89.17)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 1.5928e+00 (1.4255e+00)	Acc@1  60.00 ( 62.75)	Acc@5  88.00 ( 88.78)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.1777e+00 (1.4204e+00)	Acc@1  66.00 ( 62.72)	Acc@5  91.00 ( 89.02)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 1.6318e+00 (1.4155e+00)	Acc@1  68.00 ( 62.86)	Acc@5  89.00 ( 89.10)
Test: [ 80/100]	Time  0.058 ( 0.050)	Loss 1.4951e+00 (1.4229e+00)	Acc@1  57.00 ( 62.64)	Acc@5  88.00 ( 89.11)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.8623e+00 (1.4174e+00)	Acc@1  53.00 ( 62.78)	Acc@5  85.00 ( 89.11)
 * Acc@1 63.020 Acc@5 89.140
### epoch[18] execution time: 52.66093897819519
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.286 ( 0.286)	Data  0.124 ( 0.124)	Loss 9.3066e-01 (9.3066e-01)	Acc@1  76.56 ( 76.56)	Acc@5  94.53 ( 94.53)
Epoch: [19][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.012)	Loss 8.8623e-01 (7.7739e-01)	Acc@1  71.88 ( 77.20)	Acc@5  96.09 ( 96.02)
Epoch: [19][ 20/391]	Time  0.124 ( 0.129)	Data  0.001 ( 0.007)	Loss 7.8223e-01 (7.4960e-01)	Acc@1  72.66 ( 77.12)	Acc@5  97.66 ( 96.58)
Epoch: [19][ 30/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.005)	Loss 8.2227e-01 (7.1853e-01)	Acc@1  74.22 ( 78.02)	Acc@5  96.88 ( 96.95)
Epoch: [19][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 7.7588e-01 (7.1433e-01)	Acc@1  78.12 ( 78.33)	Acc@5  98.44 ( 96.95)
Epoch: [19][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.003)	Loss 9.1064e-01 (7.2227e-01)	Acc@1  72.66 ( 78.20)	Acc@5  95.31 ( 96.78)
Epoch: [19][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 8.3789e-01 (7.2953e-01)	Acc@1  74.22 ( 77.80)	Acc@5  95.31 ( 96.76)
Epoch: [19][ 70/391]	Time  0.132 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.2305e-01 (7.3431e-01)	Acc@1  80.47 ( 77.63)	Acc@5  98.44 ( 96.68)
Epoch: [19][ 80/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.4111e-01 (7.3416e-01)	Acc@1  79.69 ( 77.59)	Acc@5  97.66 ( 96.70)
Epoch: [19][ 90/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 5.9326e-01 (7.3243e-01)	Acc@1  81.25 ( 77.57)	Acc@5  97.66 ( 96.69)
Epoch: [19][100/391]	Time  0.129 ( 0.123)	Data  0.001 ( 0.002)	Loss 6.2354e-01 (7.3181e-01)	Acc@1  84.38 ( 77.65)	Acc@5  97.66 ( 96.68)
Epoch: [19][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.1973e-01 (7.3626e-01)	Acc@1  77.34 ( 77.47)	Acc@5  96.88 ( 96.68)
Epoch: [19][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.9189e-01 (7.3653e-01)	Acc@1  78.91 ( 77.46)	Acc@5  96.09 ( 96.59)
Epoch: [19][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.1504e-01 (7.4093e-01)	Acc@1  72.66 ( 77.34)	Acc@5  94.53 ( 96.54)
Epoch: [19][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.0088e-01 (7.4221e-01)	Acc@1  75.00 ( 77.29)	Acc@5  95.31 ( 96.48)
Epoch: [19][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.3887e-01 (7.4838e-01)	Acc@1  72.66 ( 77.11)	Acc@5  96.09 ( 96.37)
Epoch: [19][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.3672e-01 (7.4989e-01)	Acc@1  79.69 ( 77.04)	Acc@5  97.66 ( 96.36)
Epoch: [19][170/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.8848e-01 (7.4897e-01)	Acc@1  82.03 ( 77.06)	Acc@5  96.88 ( 96.34)
Epoch: [19][180/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.7197e-01 (7.5418e-01)	Acc@1  73.44 ( 76.84)	Acc@5  96.88 ( 96.33)
Epoch: [19][190/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.2959e-01 (7.5853e-01)	Acc@1  71.09 ( 76.70)	Acc@5  95.31 ( 96.26)
Epoch: [19][200/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.7256e-01 (7.6131e-01)	Acc@1  75.00 ( 76.60)	Acc@5  92.97 ( 96.23)
Epoch: [19][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0254e+00 (7.6473e-01)	Acc@1  72.66 ( 76.54)	Acc@5  92.97 ( 96.21)
Epoch: [19][220/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.0752e-01 (7.6559e-01)	Acc@1  78.12 ( 76.56)	Acc@5  97.66 ( 96.20)
Epoch: [19][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9697e-01 (7.6791e-01)	Acc@1  69.53 ( 76.46)	Acc@5  92.97 ( 96.17)
Epoch: [19][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9014e-01 (7.6877e-01)	Acc@1  69.53 ( 76.39)	Acc@5  94.53 ( 96.15)
Epoch: [19][250/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6670e-01 (7.6968e-01)	Acc@1  70.31 ( 76.38)	Acc@5  94.53 ( 96.12)
Epoch: [19][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4707e-01 (7.7004e-01)	Acc@1  78.12 ( 76.40)	Acc@5  96.88 ( 96.11)
Epoch: [19][270/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.5156e-01 (7.7152e-01)	Acc@1  75.78 ( 76.37)	Acc@5  92.97 ( 96.07)
Epoch: [19][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8125e-01 (7.7340e-01)	Acc@1  77.34 ( 76.32)	Acc@5  93.75 ( 96.05)
Epoch: [19][290/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.7793e-01 (7.7436e-01)	Acc@1  75.00 ( 76.30)	Acc@5  96.09 ( 96.03)
Epoch: [19][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4316e-01 (7.7469e-01)	Acc@1  75.00 ( 76.28)	Acc@5  96.09 ( 96.03)
Epoch: [19][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2432e-01 (7.7487e-01)	Acc@1  71.88 ( 76.29)	Acc@5  93.75 ( 96.01)
Epoch: [19][320/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.9658e-01 (7.7627e-01)	Acc@1  67.97 ( 76.21)	Acc@5  92.97 ( 96.00)
Epoch: [19][330/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.7344e-01 (7.7764e-01)	Acc@1  75.78 ( 76.17)	Acc@5  96.09 ( 95.97)
Epoch: [19][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.0811e-01 (7.8033e-01)	Acc@1  76.56 ( 76.08)	Acc@5  96.09 ( 95.94)
Epoch: [19][350/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0879e+00 (7.8160e-01)	Acc@1  68.75 ( 76.06)	Acc@5  95.31 ( 95.93)
Epoch: [19][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.8125e-01 (7.8090e-01)	Acc@1  74.22 ( 76.07)	Acc@5  94.53 ( 95.94)
Epoch: [19][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.9629e-01 (7.8182e-01)	Acc@1  75.78 ( 76.00)	Acc@5  96.88 ( 95.93)
Epoch: [19][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0869e+00 (7.8307e-01)	Acc@1  68.75 ( 75.93)	Acc@5  92.97 ( 95.92)
Epoch: [19][390/391]	Time  0.103 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0908e+00 (7.8405e-01)	Acc@1  67.50 ( 75.90)	Acc@5  95.00 ( 95.91)
## e[19] optimizer.zero_grad (sum) time: 0.6961817741394043
## e[19]       loss.backward (sum) time: 14.227084875106812
## e[19]      optimizer.step (sum) time: 8.187317609786987
## epoch[19] training(only) time: 47.487810373306274
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.4893e+00 (1.4893e+00)	Acc@1  61.00 ( 61.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.047 ( 0.063)	Loss 1.3262e+00 (1.4956e+00)	Acc@1  66.00 ( 63.00)	Acc@5  92.00 ( 87.27)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.2871e+00 (1.4447e+00)	Acc@1  66.00 ( 63.86)	Acc@5  87.00 ( 88.24)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.9258e+00 (1.4595e+00)	Acc@1  53.00 ( 62.87)	Acc@5  83.00 ( 88.06)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.4004e+00 (1.4423e+00)	Acc@1  65.00 ( 62.83)	Acc@5  91.00 ( 88.46)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.5830e+00 (1.4582e+00)	Acc@1  61.00 ( 62.75)	Acc@5  83.00 ( 88.22)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.2432e+00 (1.4347e+00)	Acc@1  66.00 ( 62.98)	Acc@5  93.00 ( 88.52)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 1.7744e+00 (1.4378e+00)	Acc@1  59.00 ( 62.94)	Acc@5  88.00 ( 88.54)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 1.6855e+00 (1.4368e+00)	Acc@1  60.00 ( 62.95)	Acc@5  80.00 ( 88.43)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.6104e+00 (1.4264e+00)	Acc@1  61.00 ( 63.22)	Acc@5  91.00 ( 88.64)
 * Acc@1 63.530 Acc@5 88.850
### epoch[19] execution time: 52.530577182769775
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.278 ( 0.278)	Data  0.136 ( 0.136)	Loss 7.9932e-01 (7.9932e-01)	Acc@1  75.00 ( 75.00)	Acc@5  98.44 ( 98.44)
Epoch: [20][ 10/391]	Time  0.122 ( 0.136)	Data  0.001 ( 0.013)	Loss 7.0508e-01 (7.0488e-01)	Acc@1  75.78 ( 78.05)	Acc@5  95.31 ( 96.88)
Epoch: [20][ 20/391]	Time  0.121 ( 0.129)	Data  0.001 ( 0.007)	Loss 7.5684e-01 (6.7243e-01)	Acc@1  75.78 ( 78.87)	Acc@5  96.09 ( 97.51)
Epoch: [20][ 30/391]	Time  0.121 ( 0.126)	Data  0.001 ( 0.005)	Loss 6.0742e-01 (6.7951e-01)	Acc@1  79.69 ( 78.98)	Acc@5  97.66 ( 97.20)
Epoch: [20][ 40/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.004)	Loss 6.3428e-01 (6.8112e-01)	Acc@1  78.91 ( 78.81)	Acc@5  99.22 ( 97.05)
Epoch: [20][ 50/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 7.3096e-01 (6.7311e-01)	Acc@1  74.22 ( 78.92)	Acc@5  98.44 ( 97.15)
Epoch: [20][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.8174e-01 (6.7829e-01)	Acc@1  75.00 ( 78.80)	Acc@5  97.66 ( 97.07)
Epoch: [20][ 70/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.3135e-01 (6.7442e-01)	Acc@1  80.47 ( 78.93)	Acc@5  98.44 ( 97.11)
Epoch: [20][ 80/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.6748e-01 (6.7119e-01)	Acc@1  78.91 ( 78.87)	Acc@5  96.09 ( 97.11)
Epoch: [20][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.3135e-01 (6.6977e-01)	Acc@1  82.03 ( 79.12)	Acc@5  97.66 ( 97.05)
Epoch: [20][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.2334e-01 (6.7321e-01)	Acc@1  75.78 ( 79.19)	Acc@5  96.09 ( 97.03)
Epoch: [20][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3750e-01 (6.7409e-01)	Acc@1  71.88 ( 79.10)	Acc@5  94.53 ( 97.04)
Epoch: [20][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.2754e-01 (6.7689e-01)	Acc@1  75.78 ( 79.05)	Acc@5 100.00 ( 97.05)
Epoch: [20][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.5684e-01 (6.7970e-01)	Acc@1  73.44 ( 78.88)	Acc@5  96.09 ( 96.98)
Epoch: [20][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.1621e-01 (6.8584e-01)	Acc@1  83.59 ( 78.71)	Acc@5  97.66 ( 96.93)
Epoch: [20][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.4336e-01 (6.9095e-01)	Acc@1  74.22 ( 78.53)	Acc@5  95.31 ( 96.89)
Epoch: [20][160/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.3936e-01 (6.9374e-01)	Acc@1  74.22 ( 78.49)	Acc@5  96.09 ( 96.85)
Epoch: [20][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9336e-01 (6.9432e-01)	Acc@1  78.91 ( 78.45)	Acc@5  95.31 ( 96.86)
Epoch: [20][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0039e-01 (6.9739e-01)	Acc@1  75.00 ( 78.29)	Acc@5  94.53 ( 96.84)
Epoch: [20][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8252e-01 (6.9726e-01)	Acc@1  81.25 ( 78.35)	Acc@5  99.22 ( 96.83)
Epoch: [20][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6260e-01 (6.9799e-01)	Acc@1  81.25 ( 78.36)	Acc@5  95.31 ( 96.81)
Epoch: [20][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9053e-01 (7.0184e-01)	Acc@1  75.00 ( 78.21)	Acc@5  93.75 ( 96.76)
Epoch: [20][220/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6855e-01 (7.0408e-01)	Acc@1  78.12 ( 78.13)	Acc@5  98.44 ( 96.74)
Epoch: [20][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9688e-01 (7.0565e-01)	Acc@1  78.12 ( 78.11)	Acc@5  94.53 ( 96.73)
Epoch: [20][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5781e-01 (7.0810e-01)	Acc@1  78.91 ( 78.04)	Acc@5  95.31 ( 96.67)
Epoch: [20][250/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9053e-01 (7.1085e-01)	Acc@1  77.34 ( 77.96)	Acc@5  94.53 ( 96.62)
Epoch: [20][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.1504e-01 (7.1530e-01)	Acc@1  75.78 ( 77.82)	Acc@5  93.75 ( 96.58)
Epoch: [20][270/391]	Time  0.133 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5488e-01 (7.1910e-01)	Acc@1  78.91 ( 77.71)	Acc@5  97.66 ( 96.54)
Epoch: [20][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8135e-01 (7.2268e-01)	Acc@1  75.00 ( 77.59)	Acc@5  96.09 ( 96.52)
Epoch: [20][290/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0996e-01 (7.2406e-01)	Acc@1  78.12 ( 77.57)	Acc@5  96.09 ( 96.51)
Epoch: [20][300/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0986e-01 (7.2650e-01)	Acc@1  83.59 ( 77.53)	Acc@5  96.88 ( 96.45)
Epoch: [20][310/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5293e-01 (7.2830e-01)	Acc@1  74.22 ( 77.46)	Acc@5  96.09 ( 96.43)
Epoch: [20][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0264e-01 (7.2948e-01)	Acc@1  78.91 ( 77.40)	Acc@5  96.88 ( 96.41)
Epoch: [20][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.4951e-01 (7.3043e-01)	Acc@1  75.78 ( 77.36)	Acc@5  96.09 ( 96.41)
Epoch: [20][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5879e-01 (7.3211e-01)	Acc@1  75.78 ( 77.28)	Acc@5  97.66 ( 96.40)
Epoch: [20][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.5254e-01 (7.3433e-01)	Acc@1  73.44 ( 77.22)	Acc@5  94.53 ( 96.40)
Epoch: [20][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.7598e-01 (7.3559e-01)	Acc@1  75.78 ( 77.19)	Acc@5  95.31 ( 96.38)
Epoch: [20][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.5156e-01 (7.3717e-01)	Acc@1  71.88 ( 77.18)	Acc@5  95.31 ( 96.36)
Epoch: [20][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5088e-01 (7.4127e-01)	Acc@1  82.03 ( 77.06)	Acc@5  98.44 ( 96.32)
Epoch: [20][390/391]	Time  0.110 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.6973e-01 (7.4307e-01)	Acc@1  65.00 ( 76.99)	Acc@5  97.50 ( 96.31)
## e[20] optimizer.zero_grad (sum) time: 0.6910562515258789
## e[20]       loss.backward (sum) time: 14.273510217666626
## e[20]      optimizer.step (sum) time: 8.077782154083252
## epoch[20] training(only) time: 47.435088872909546
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.2148e+00 (1.2148e+00)	Acc@1  64.00 ( 64.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.6074e+00 (1.4729e+00)	Acc@1  64.00 ( 64.45)	Acc@5  87.00 ( 87.91)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.3682e+00 (1.3885e+00)	Acc@1  67.00 ( 64.57)	Acc@5  89.00 ( 89.48)
Test: [ 30/100]	Time  0.058 ( 0.053)	Loss 1.9023e+00 (1.4146e+00)	Acc@1  56.00 ( 63.87)	Acc@5  83.00 ( 89.16)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.5518e+00 (1.4289e+00)	Acc@1  64.00 ( 63.68)	Acc@5  92.00 ( 89.15)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4307e+00 (1.4314e+00)	Acc@1  66.00 ( 63.71)	Acc@5  90.00 ( 89.12)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1934e+00 (1.4134e+00)	Acc@1  65.00 ( 63.98)	Acc@5  93.00 ( 89.23)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.6904e+00 (1.4231e+00)	Acc@1  62.00 ( 63.86)	Acc@5  85.00 ( 89.04)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 1.4121e+00 (1.4342e+00)	Acc@1  63.00 ( 63.65)	Acc@5  84.00 ( 88.88)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.7949e+00 (1.4350e+00)	Acc@1  58.00 ( 63.70)	Acc@5  84.00 ( 88.86)
 * Acc@1 63.540 Acc@5 88.840
### epoch[20] execution time: 52.43368077278137
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.285 ( 0.285)	Data  0.146 ( 0.146)	Loss 6.0352e-01 (6.0352e-01)	Acc@1  82.03 ( 82.03)	Acc@5  96.09 ( 96.09)
Epoch: [21][ 10/391]	Time  0.121 ( 0.135)	Data  0.001 ( 0.014)	Loss 6.1914e-01 (6.6513e-01)	Acc@1  79.69 ( 79.47)	Acc@5  99.22 ( 97.16)
Epoch: [21][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.008)	Loss 7.8809e-01 (6.7290e-01)	Acc@1  74.22 ( 79.17)	Acc@5  96.88 ( 96.99)
Epoch: [21][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 6.9092e-01 (6.5873e-01)	Acc@1  77.34 ( 79.49)	Acc@5 100.00 ( 97.18)
Epoch: [21][ 40/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.005)	Loss 6.5820e-01 (6.4284e-01)	Acc@1  78.91 ( 79.74)	Acc@5  96.09 ( 97.33)
Epoch: [21][ 50/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 8.2617e-01 (6.4307e-01)	Acc@1  75.00 ( 79.95)	Acc@5  96.09 ( 97.30)
Epoch: [21][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.5996e-01 (6.3098e-01)	Acc@1  85.16 ( 80.34)	Acc@5  99.22 ( 97.37)
Epoch: [21][ 70/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.5664e-01 (6.2965e-01)	Acc@1  84.38 ( 80.40)	Acc@5  97.66 ( 97.29)
Epoch: [21][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.6602e-01 (6.2695e-01)	Acc@1  79.69 ( 80.56)	Acc@5  97.66 ( 97.34)
Epoch: [21][ 90/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.2021e-01 (6.3019e-01)	Acc@1  79.69 ( 80.40)	Acc@5  95.31 ( 97.30)
Epoch: [21][100/391]	Time  0.125 ( 0.123)	Data  0.001 ( 0.002)	Loss 5.9033e-01 (6.3071e-01)	Acc@1  78.91 ( 80.34)	Acc@5  98.44 ( 97.31)
Epoch: [21][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.7725e-01 (6.3436e-01)	Acc@1  78.91 ( 80.25)	Acc@5  97.66 ( 97.31)
Epoch: [21][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.4307e-01 (6.4002e-01)	Acc@1  80.47 ( 79.98)	Acc@5  97.66 ( 97.31)
Epoch: [21][130/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.3135e-01 (6.4653e-01)	Acc@1  78.91 ( 79.81)	Acc@5  97.66 ( 97.27)
Epoch: [21][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.2275e-01 (6.5386e-01)	Acc@1  75.78 ( 79.70)	Acc@5  96.09 ( 97.21)
Epoch: [21][150/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.9824e-01 (6.5869e-01)	Acc@1  78.91 ( 79.50)	Acc@5  95.31 ( 97.16)
Epoch: [21][160/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.1787e-01 (6.6315e-01)	Acc@1  74.22 ( 79.40)	Acc@5  93.75 ( 97.06)
Epoch: [21][170/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.6895e-01 (6.6507e-01)	Acc@1  78.12 ( 79.30)	Acc@5  94.53 ( 96.99)
Epoch: [21][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.1982e-01 (6.6976e-01)	Acc@1  75.78 ( 79.20)	Acc@5  95.31 ( 96.95)
Epoch: [21][190/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.1836e-01 (6.7208e-01)	Acc@1  76.56 ( 79.11)	Acc@5  95.31 ( 96.94)
Epoch: [21][200/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.3105e-01 (6.7453e-01)	Acc@1  74.22 ( 79.04)	Acc@5  95.31 ( 96.91)
Epoch: [21][210/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.1826e-01 (6.7523e-01)	Acc@1  79.69 ( 79.05)	Acc@5  96.88 ( 96.91)
Epoch: [21][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2754e-01 (6.7595e-01)	Acc@1  75.78 ( 79.02)	Acc@5  96.88 ( 96.92)
Epoch: [21][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4668e-01 (6.7830e-01)	Acc@1  71.88 ( 78.96)	Acc@5  96.09 ( 96.91)
Epoch: [21][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6670e-01 (6.8027e-01)	Acc@1  71.88 ( 78.86)	Acc@5  95.31 ( 96.88)
Epoch: [21][250/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0703e-01 (6.8352e-01)	Acc@1  75.78 ( 78.73)	Acc@5  97.66 ( 96.85)
Epoch: [21][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9160e-01 (6.8663e-01)	Acc@1  71.88 ( 78.64)	Acc@5  97.66 ( 96.83)
Epoch: [21][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2705e-01 (6.9091e-01)	Acc@1  81.25 ( 78.55)	Acc@5  94.53 ( 96.79)
Epoch: [21][280/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2217e-01 (6.9337e-01)	Acc@1  76.56 ( 78.48)	Acc@5  97.66 ( 96.80)
Epoch: [21][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2383e-01 (6.9776e-01)	Acc@1  69.53 ( 78.35)	Acc@5  96.88 ( 96.78)
Epoch: [21][300/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9727e-01 (6.9887e-01)	Acc@1  79.69 ( 78.29)	Acc@5  96.88 ( 96.78)
Epoch: [21][310/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8213e-01 (6.9989e-01)	Acc@1  78.12 ( 78.27)	Acc@5  98.44 ( 96.77)
Epoch: [21][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9541e-01 (7.0178e-01)	Acc@1  71.88 ( 78.21)	Acc@5  94.53 ( 96.75)
Epoch: [21][330/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.4375e-01 (7.0438e-01)	Acc@1  75.78 ( 78.11)	Acc@5  95.31 ( 96.73)
Epoch: [21][340/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0898e-01 (7.0697e-01)	Acc@1  79.69 ( 78.06)	Acc@5  97.66 ( 96.71)
Epoch: [21][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.4023e-01 (7.0859e-01)	Acc@1  78.12 ( 78.02)	Acc@5  94.53 ( 96.70)
Epoch: [21][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0654e-01 (7.0989e-01)	Acc@1  79.69 ( 77.97)	Acc@5  96.88 ( 96.70)
Epoch: [21][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2119e-01 (7.1234e-01)	Acc@1  77.34 ( 77.85)	Acc@5  96.88 ( 96.68)
Epoch: [21][380/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1670e-01 (7.1276e-01)	Acc@1  81.25 ( 77.83)	Acc@5  96.88 ( 96.68)
Epoch: [21][390/391]	Time  0.109 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2510e-01 (7.1418e-01)	Acc@1  75.00 ( 77.80)	Acc@5  98.75 ( 96.66)
## e[21] optimizer.zero_grad (sum) time: 0.6963930130004883
## e[21]       loss.backward (sum) time: 14.283767700195312
## e[21]      optimizer.step (sum) time: 8.127696990966797
## epoch[21] training(only) time: 47.424317836761475
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.2744e+00 (1.2744e+00)	Acc@1  57.00 ( 57.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.050 ( 0.062)	Loss 1.4619e+00 (1.5029e+00)	Acc@1  65.00 ( 63.45)	Acc@5  90.00 ( 86.73)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.2314e+00 (1.4352e+00)	Acc@1  68.00 ( 64.33)	Acc@5  89.00 ( 87.81)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.7881e+00 (1.4573e+00)	Acc@1  52.00 ( 63.52)	Acc@5  86.00 ( 87.77)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5498e+00 (1.4381e+00)	Acc@1  66.00 ( 63.63)	Acc@5  92.00 ( 88.51)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.5850e+00 (1.4517e+00)	Acc@1  62.00 ( 63.20)	Acc@5  88.00 ( 88.35)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2363e+00 (1.4295e+00)	Acc@1  66.00 ( 63.26)	Acc@5  90.00 ( 88.72)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.8545e+00 (1.4362e+00)	Acc@1  60.00 ( 62.85)	Acc@5  84.00 ( 88.58)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.5166e+00 (1.4410e+00)	Acc@1  60.00 ( 62.80)	Acc@5  84.00 ( 88.47)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.7969e+00 (1.4214e+00)	Acc@1  59.00 ( 63.18)	Acc@5  84.00 ( 88.59)
 * Acc@1 63.400 Acc@5 88.640
### epoch[21] execution time: 52.43885016441345
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.282 ( 0.282)	Data  0.153 ( 0.153)	Loss 7.3242e-01 (7.3242e-01)	Acc@1  76.56 ( 76.56)	Acc@5  94.53 ( 94.53)
Epoch: [22][ 10/391]	Time  0.120 ( 0.136)	Data  0.001 ( 0.015)	Loss 6.4502e-01 (6.0010e-01)	Acc@1  83.59 ( 82.03)	Acc@5  98.44 ( 97.80)
Epoch: [22][ 20/391]	Time  0.120 ( 0.128)	Data  0.001 ( 0.008)	Loss 6.5527e-01 (6.0610e-01)	Acc@1  82.03 ( 81.14)	Acc@5  97.66 ( 97.51)
Epoch: [22][ 30/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.006)	Loss 5.7568e-01 (5.8758e-01)	Acc@1  82.81 ( 81.78)	Acc@5  98.44 ( 97.86)
Epoch: [22][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 5.3711e-01 (5.9280e-01)	Acc@1  83.59 ( 81.57)	Acc@5  99.22 ( 98.00)
Epoch: [22][ 50/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.3564e-01 (5.9082e-01)	Acc@1  82.03 ( 81.65)	Acc@5  98.44 ( 98.02)
Epoch: [22][ 60/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 6.9922e-01 (5.8797e-01)	Acc@1  75.78 ( 81.63)	Acc@5  96.88 ( 98.03)
Epoch: [22][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.1631e-01 (5.9082e-01)	Acc@1  74.22 ( 81.58)	Acc@5  98.44 ( 97.90)
Epoch: [22][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.8789e-01 (5.9622e-01)	Acc@1  78.91 ( 81.47)	Acc@5  98.44 ( 97.91)
Epoch: [22][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.8887e-01 (5.9644e-01)	Acc@1  79.69 ( 81.43)	Acc@5  98.44 ( 97.91)
Epoch: [22][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.7031e-01 (5.9961e-01)	Acc@1  82.03 ( 81.29)	Acc@5  97.66 ( 97.90)
Epoch: [22][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (6.0412e-01)	Acc@1  83.59 ( 81.07)	Acc@5  97.66 ( 97.84)
Epoch: [22][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.5469e-01 (6.0396e-01)	Acc@1  85.94 ( 81.01)	Acc@5  98.44 ( 97.81)
Epoch: [22][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.0498e-01 (6.0555e-01)	Acc@1  82.03 ( 80.98)	Acc@5  96.88 ( 97.82)
Epoch: [22][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.8643e-01 (6.1252e-01)	Acc@1  82.03 ( 80.81)	Acc@5  96.88 ( 97.72)
Epoch: [22][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.2305e-01 (6.1062e-01)	Acc@1  81.25 ( 80.89)	Acc@5  96.88 ( 97.75)
Epoch: [22][160/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.9355e-01 (6.1581e-01)	Acc@1  73.44 ( 80.69)	Acc@5  96.09 ( 97.71)
Epoch: [22][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7148e-01 (6.1858e-01)	Acc@1  75.78 ( 80.59)	Acc@5  98.44 ( 97.71)
Epoch: [22][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5625e-01 (6.2325e-01)	Acc@1  79.69 ( 80.43)	Acc@5  97.66 ( 97.66)
Epoch: [22][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5996e-01 (6.2856e-01)	Acc@1  74.22 ( 80.36)	Acc@5  92.19 ( 97.57)
Epoch: [22][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1865e-01 (6.2862e-01)	Acc@1  84.38 ( 80.31)	Acc@5  97.66 ( 97.57)
Epoch: [22][210/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3516e-01 (6.3189e-01)	Acc@1  82.81 ( 80.26)	Acc@5  96.09 ( 97.51)
Epoch: [22][220/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0947e-01 (6.3545e-01)	Acc@1  76.56 ( 80.16)	Acc@5  96.88 ( 97.47)
Epoch: [22][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2793e-01 (6.3839e-01)	Acc@1  79.69 ( 80.10)	Acc@5  96.88 ( 97.43)
Epoch: [22][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2900e-01 (6.3948e-01)	Acc@1  74.22 ( 80.04)	Acc@5  97.66 ( 97.43)
Epoch: [22][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5215e-01 (6.4088e-01)	Acc@1  74.22 ( 80.02)	Acc@5  92.97 ( 97.42)
Epoch: [22][260/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1484e-01 (6.4451e-01)	Acc@1  78.91 ( 79.90)	Acc@5  96.09 ( 97.37)
Epoch: [22][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3184e-01 (6.4829e-01)	Acc@1  81.25 ( 79.75)	Acc@5  96.88 ( 97.34)
Epoch: [22][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6670e-01 (6.5221e-01)	Acc@1  73.44 ( 79.60)	Acc@5  93.75 ( 97.29)
Epoch: [22][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9453e-01 (6.5496e-01)	Acc@1  74.22 ( 79.54)	Acc@5  94.53 ( 97.24)
Epoch: [22][300/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0693e-01 (6.5595e-01)	Acc@1  83.59 ( 79.53)	Acc@5  98.44 ( 97.24)
Epoch: [22][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9678e-01 (6.5534e-01)	Acc@1  80.47 ( 79.56)	Acc@5  94.53 ( 97.22)
Epoch: [22][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9541e-01 (6.5688e-01)	Acc@1  76.56 ( 79.51)	Acc@5  93.75 ( 97.20)
Epoch: [22][330/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5293e-01 (6.5846e-01)	Acc@1  77.34 ( 79.49)	Acc@5  94.53 ( 97.17)
Epoch: [22][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2217e-01 (6.5938e-01)	Acc@1  78.12 ( 79.49)	Acc@5  97.66 ( 97.17)
Epoch: [22][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3232e-01 (6.6222e-01)	Acc@1  83.59 ( 79.41)	Acc@5  96.09 ( 97.14)
Epoch: [22][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5625e-01 (6.6273e-01)	Acc@1  80.47 ( 79.40)	Acc@5  96.88 ( 97.15)
Epoch: [22][370/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.4863e-01 (6.6518e-01)	Acc@1  72.66 ( 79.31)	Acc@5  95.31 ( 97.12)
Epoch: [22][380/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5586e-01 (6.6574e-01)	Acc@1  80.47 ( 79.32)	Acc@5  96.88 ( 97.10)
Epoch: [22][390/391]	Time  0.106 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.9648e-01 (6.6682e-01)	Acc@1  68.75 ( 79.26)	Acc@5  95.00 ( 97.09)
## e[22] optimizer.zero_grad (sum) time: 0.6854796409606934
## e[22]       loss.backward (sum) time: 14.304930686950684
## e[22]      optimizer.step (sum) time: 8.109020471572876
## epoch[22] training(only) time: 47.40050649642944
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.4844e+00 (1.4844e+00)	Acc@1  58.00 ( 58.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.5322e+00 (1.4735e+00)	Acc@1  65.00 ( 63.55)	Acc@5  90.00 ( 88.91)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.1699e+00 (1.3918e+00)	Acc@1  69.00 ( 65.19)	Acc@5  93.00 ( 90.19)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5703e+00 (1.4073e+00)	Acc@1  53.00 ( 64.26)	Acc@5  89.00 ( 89.97)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.5078e+00 (1.3949e+00)	Acc@1  64.00 ( 64.51)	Acc@5  90.00 ( 90.15)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.5264e+00 (1.4032e+00)	Acc@1  62.00 ( 64.57)	Acc@5  91.00 ( 89.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0957e+00 (1.3796e+00)	Acc@1  68.00 ( 64.67)	Acc@5  93.00 ( 89.98)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.7197e+00 (1.3870e+00)	Acc@1  61.00 ( 64.59)	Acc@5  85.00 ( 89.87)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6250e+00 (1.3977e+00)	Acc@1  60.00 ( 64.48)	Acc@5  85.00 ( 89.62)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7715e+00 (1.3855e+00)	Acc@1  61.00 ( 64.90)	Acc@5  83.00 ( 89.66)
 * Acc@1 65.150 Acc@5 89.770
### epoch[22] execution time: 52.3879771232605
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.279 ( 0.279)	Data  0.143 ( 0.143)	Loss 4.9634e-01 (4.9634e-01)	Acc@1  82.03 ( 82.03)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.118 ( 0.135)	Data  0.001 ( 0.014)	Loss 5.3906e-01 (5.6212e-01)	Acc@1  82.81 ( 81.96)	Acc@5  98.44 ( 98.01)
Epoch: [23][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.008)	Loss 5.5420e-01 (5.6173e-01)	Acc@1  83.59 ( 82.55)	Acc@5  99.22 ( 97.92)
Epoch: [23][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 5.7959e-01 (5.6543e-01)	Acc@1  77.34 ( 82.21)	Acc@5  98.44 ( 97.93)
Epoch: [23][ 40/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 6.2256e-01 (5.6969e-01)	Acc@1  78.12 ( 81.99)	Acc@5  97.66 ( 97.88)
Epoch: [23][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.9277e-01 (5.7164e-01)	Acc@1  81.25 ( 81.79)	Acc@5  97.66 ( 97.90)
Epoch: [23][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.4453e-01 (5.7381e-01)	Acc@1  80.47 ( 81.70)	Acc@5  97.66 ( 97.87)
Epoch: [23][ 70/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.8496e-01 (5.6812e-01)	Acc@1  78.91 ( 81.91)	Acc@5  98.44 ( 98.03)
Epoch: [23][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.7725e-01 (5.7334e-01)	Acc@1  78.12 ( 81.83)	Acc@5  97.66 ( 98.00)
Epoch: [23][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.2793e-01 (5.7690e-01)	Acc@1  79.69 ( 81.76)	Acc@5  97.66 ( 97.99)
Epoch: [23][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.6934e-01 (5.8276e-01)	Acc@1  84.38 ( 81.62)	Acc@5  96.88 ( 97.99)
Epoch: [23][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.4502e-01 (5.8732e-01)	Acc@1  78.91 ( 81.52)	Acc@5  98.44 ( 97.97)
Epoch: [23][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.5439e-01 (5.9069e-01)	Acc@1  75.78 ( 81.43)	Acc@5  96.88 ( 97.94)
Epoch: [23][130/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.3145e-01 (5.9444e-01)	Acc@1  74.22 ( 81.42)	Acc@5  98.44 ( 97.92)
Epoch: [23][140/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.2451e-01 (5.9803e-01)	Acc@1  82.03 ( 81.29)	Acc@5  97.66 ( 97.88)
Epoch: [23][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4165e-01 (5.9955e-01)	Acc@1  84.38 ( 81.21)	Acc@5  98.44 ( 97.85)
Epoch: [23][160/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.2217e-01 (6.0493e-01)	Acc@1  79.69 ( 81.03)	Acc@5  96.09 ( 97.82)
Epoch: [23][170/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.3955e-01 (6.0738e-01)	Acc@1  82.81 ( 80.93)	Acc@5  98.44 ( 97.78)
Epoch: [23][180/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.6152e-01 (6.0803e-01)	Acc@1  80.47 ( 80.88)	Acc@5  98.44 ( 97.79)
Epoch: [23][190/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.5732e-01 (6.1319e-01)	Acc@1  75.00 ( 80.76)	Acc@5  95.31 ( 97.74)
Epoch: [23][200/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.5537e-01 (6.1551e-01)	Acc@1  78.12 ( 80.74)	Acc@5  97.66 ( 97.72)
Epoch: [23][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3682e-01 (6.1648e-01)	Acc@1  74.22 ( 80.74)	Acc@5  95.31 ( 97.70)
Epoch: [23][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9971e-01 (6.1774e-01)	Acc@1  78.91 ( 80.67)	Acc@5  97.66 ( 97.68)
Epoch: [23][230/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4551e-01 (6.1833e-01)	Acc@1  79.69 ( 80.60)	Acc@5  97.66 ( 97.69)
Epoch: [23][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0938e-01 (6.1920e-01)	Acc@1  81.25 ( 80.61)	Acc@5  98.44 ( 97.67)
Epoch: [23][250/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3682e-01 (6.2240e-01)	Acc@1  77.34 ( 80.52)	Acc@5  92.19 ( 97.61)
Epoch: [23][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.5459e-01 (6.2623e-01)	Acc@1  72.66 ( 80.38)	Acc@5  93.75 ( 97.57)
Epoch: [23][270/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.4766e-01 (6.2871e-01)	Acc@1  78.12 ( 80.31)	Acc@5  96.88 ( 97.56)
Epoch: [23][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.8545e-01 (6.3152e-01)	Acc@1  79.69 ( 80.19)	Acc@5  96.09 ( 97.53)
Epoch: [23][290/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.1338e-01 (6.3352e-01)	Acc@1  79.69 ( 80.18)	Acc@5  95.31 ( 97.48)
Epoch: [23][300/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.7490e-01 (6.3563e-01)	Acc@1  74.22 ( 80.06)	Acc@5  96.09 ( 97.47)
Epoch: [23][310/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.3799e-01 (6.3894e-01)	Acc@1  75.00 ( 79.98)	Acc@5  92.97 ( 97.44)
Epoch: [23][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9277e-01 (6.4163e-01)	Acc@1  80.47 ( 79.89)	Acc@5  99.22 ( 97.44)
Epoch: [23][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2314e-01 (6.4363e-01)	Acc@1  72.66 ( 79.80)	Acc@5  96.88 ( 97.43)
Epoch: [23][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.3740e-01 (6.4506e-01)	Acc@1  70.31 ( 79.74)	Acc@5  96.09 ( 97.42)
Epoch: [23][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1182e-01 (6.4538e-01)	Acc@1  81.25 ( 79.73)	Acc@5  96.88 ( 97.40)
Epoch: [23][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.2471e-01 (6.4861e-01)	Acc@1  78.91 ( 79.63)	Acc@5  97.66 ( 97.37)
Epoch: [23][370/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9473e-01 (6.4917e-01)	Acc@1  83.59 ( 79.60)	Acc@5  97.66 ( 97.36)
Epoch: [23][380/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2900e-01 (6.5174e-01)	Acc@1  76.56 ( 79.56)	Acc@5  96.88 ( 97.33)
Epoch: [23][390/391]	Time  0.109 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.6836e-01 (6.5275e-01)	Acc@1  82.50 ( 79.53)	Acc@5  97.50 ( 97.32)
## e[23] optimizer.zero_grad (sum) time: 0.6810111999511719
## e[23]       loss.backward (sum) time: 14.411388874053955
## e[23]      optimizer.step (sum) time: 8.122633218765259
## epoch[23] training(only) time: 47.4844388961792
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.3740e+00 (1.3740e+00)	Acc@1  68.00 ( 68.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.050 ( 0.062)	Loss 1.4492e+00 (1.4283e+00)	Acc@1  68.00 ( 64.36)	Acc@5  90.00 ( 89.18)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 1.2646e+00 (1.3965e+00)	Acc@1  66.00 ( 64.52)	Acc@5  89.00 ( 89.71)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.8105e+00 (1.4139e+00)	Acc@1  54.00 ( 64.26)	Acc@5  86.00 ( 89.68)
Test: [ 40/100]	Time  0.057 ( 0.052)	Loss 1.5439e+00 (1.4111e+00)	Acc@1  62.00 ( 64.29)	Acc@5  89.00 ( 89.88)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.5713e+00 (1.4241e+00)	Acc@1  59.00 ( 63.90)	Acc@5  89.00 ( 89.57)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1338e+00 (1.4049e+00)	Acc@1  65.00 ( 64.07)	Acc@5  95.00 ( 89.67)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 1.5234e+00 (1.4115e+00)	Acc@1  66.00 ( 64.18)	Acc@5  83.00 ( 89.44)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6152e+00 (1.4283e+00)	Acc@1  64.00 ( 63.91)	Acc@5  87.00 ( 89.21)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.6729e+00 (1.4196e+00)	Acc@1  62.00 ( 64.19)	Acc@5  86.00 ( 89.21)
 * Acc@1 64.450 Acc@5 89.350
### epoch[23] execution time: 52.48405337333679
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.279 ( 0.279)	Data  0.145 ( 0.145)	Loss 7.8467e-01 (7.8467e-01)	Acc@1  78.12 ( 78.12)	Acc@5  96.09 ( 96.09)
Epoch: [24][ 10/391]	Time  0.119 ( 0.135)	Data  0.001 ( 0.014)	Loss 5.1904e-01 (5.7710e-01)	Acc@1  80.47 ( 81.68)	Acc@5  99.22 ( 98.30)
Epoch: [24][ 20/391]	Time  0.130 ( 0.128)	Data  0.001 ( 0.008)	Loss 6.3818e-01 (5.5802e-01)	Acc@1  81.25 ( 82.51)	Acc@5  95.31 ( 98.29)
Epoch: [24][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.3433e-01 (5.3960e-01)	Acc@1  85.94 ( 83.34)	Acc@5 100.00 ( 98.41)
Epoch: [24][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.4346e-01 (5.4689e-01)	Acc@1  80.47 ( 82.89)	Acc@5  99.22 ( 98.21)
Epoch: [24][ 50/391]	Time  0.126 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.0391e-01 (5.4207e-01)	Acc@1  83.59 ( 83.00)	Acc@5  98.44 ( 98.24)
Epoch: [24][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.6885e-01 (5.4320e-01)	Acc@1  82.03 ( 83.03)	Acc@5  96.88 ( 98.26)
Epoch: [24][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.3477e-01 (5.5133e-01)	Acc@1  77.34 ( 82.72)	Acc@5  97.66 ( 98.20)
Epoch: [24][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.1416e-01 (5.5086e-01)	Acc@1  81.25 ( 82.71)	Acc@5 100.00 ( 98.20)
Epoch: [24][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.0928e-01 (5.5388e-01)	Acc@1  81.25 ( 82.64)	Acc@5  99.22 ( 98.15)
Epoch: [24][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.8506e-01 (5.6416e-01)	Acc@1  75.78 ( 82.38)	Acc@5  96.09 ( 98.00)
Epoch: [24][110/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.7627e-01 (5.6598e-01)	Acc@1  78.12 ( 82.31)	Acc@5  96.09 ( 97.97)
Epoch: [24][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.7676e-01 (5.6701e-01)	Acc@1  80.47 ( 82.32)	Acc@5  95.31 ( 97.89)
Epoch: [24][130/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.3379e-01 (5.7128e-01)	Acc@1  79.69 ( 82.22)	Acc@5  97.66 ( 97.85)
Epoch: [24][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8457e-01 (5.7509e-01)	Acc@1  79.69 ( 82.05)	Acc@5  97.66 ( 97.84)
Epoch: [24][150/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8262e-01 (5.7438e-01)	Acc@1  80.47 ( 82.06)	Acc@5  96.09 ( 97.82)
Epoch: [24][160/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8936e-01 (5.7907e-01)	Acc@1  84.38 ( 82.04)	Acc@5  96.88 ( 97.76)
Epoch: [24][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7324e-01 (5.8506e-01)	Acc@1  85.94 ( 81.82)	Acc@5  96.88 ( 97.73)
Epoch: [24][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9053e-01 (5.8820e-01)	Acc@1  77.34 ( 81.69)	Acc@5  96.88 ( 97.72)
Epoch: [24][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8193e-01 (5.9178e-01)	Acc@1  81.25 ( 81.56)	Acc@5  98.44 ( 97.68)
Epoch: [24][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0742e-01 (5.9338e-01)	Acc@1  78.12 ( 81.48)	Acc@5  96.09 ( 97.67)
Epoch: [24][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5391e-01 (5.9622e-01)	Acc@1  75.00 ( 81.36)	Acc@5  98.44 ( 97.66)
Epoch: [24][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4414e-01 (5.9645e-01)	Acc@1  72.66 ( 81.31)	Acc@5  96.09 ( 97.67)
Epoch: [24][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3682e-01 (5.9748e-01)	Acc@1  74.22 ( 81.29)	Acc@5  96.88 ( 97.66)
Epoch: [24][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.0762e-01 (6.0138e-01)	Acc@1  78.91 ( 81.20)	Acc@5  93.75 ( 97.62)
Epoch: [24][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4941e-01 (6.0386e-01)	Acc@1  81.25 ( 81.10)	Acc@5  95.31 ( 97.62)
Epoch: [24][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3594e-01 (6.0698e-01)	Acc@1  78.91 ( 81.00)	Acc@5  94.53 ( 97.62)
Epoch: [24][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4014e-01 (6.0875e-01)	Acc@1  81.25 ( 80.92)	Acc@5  97.66 ( 97.61)
Epoch: [24][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2656e-01 (6.1155e-01)	Acc@1  76.56 ( 80.79)	Acc@5  96.88 ( 97.59)
Epoch: [24][290/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.3867e-01 (6.1406e-01)	Acc@1  82.81 ( 80.69)	Acc@5  96.88 ( 97.56)
Epoch: [24][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.2734e-01 (6.1555e-01)	Acc@1  80.47 ( 80.62)	Acc@5  98.44 ( 97.53)
Epoch: [24][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0068e-01 (6.1535e-01)	Acc@1  79.69 ( 80.63)	Acc@5  96.09 ( 97.53)
Epoch: [24][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.3857e-01 (6.1727e-01)	Acc@1  83.59 ( 80.53)	Acc@5  99.22 ( 97.55)
Epoch: [24][330/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.8809e-01 (6.1974e-01)	Acc@1  73.44 ( 80.47)	Acc@5  95.31 ( 97.52)
Epoch: [24][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1719e-01 (6.2051e-01)	Acc@1  82.03 ( 80.48)	Acc@5  97.66 ( 97.52)
Epoch: [24][350/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.9385e-01 (6.2142e-01)	Acc@1  79.69 ( 80.46)	Acc@5  96.09 ( 97.51)
Epoch: [24][360/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.1582e-01 (6.2323e-01)	Acc@1  76.56 ( 80.41)	Acc@5  96.09 ( 97.49)
Epoch: [24][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.3516e-01 (6.2407e-01)	Acc@1  83.59 ( 80.40)	Acc@5  97.66 ( 97.49)
Epoch: [24][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.2373e-01 (6.2650e-01)	Acc@1  76.56 ( 80.33)	Acc@5  96.09 ( 97.47)
Epoch: [24][390/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5332e-01 (6.2842e-01)	Acc@1  80.00 ( 80.28)	Acc@5  98.75 ( 97.45)
## e[24] optimizer.zero_grad (sum) time: 0.6896040439605713
## e[24]       loss.backward (sum) time: 14.300288438796997
## e[24]      optimizer.step (sum) time: 8.106925010681152
## epoch[24] training(only) time: 47.30612778663635
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.5312e+00 (1.5312e+00)	Acc@1  61.00 ( 61.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.050 ( 0.059)	Loss 1.6113e+00 (1.5290e+00)	Acc@1  64.00 ( 64.27)	Acc@5  90.00 ( 88.55)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.2881e+00 (1.4341e+00)	Acc@1  66.00 ( 65.52)	Acc@5  90.00 ( 89.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.9014e+00 (1.4479e+00)	Acc@1  54.00 ( 65.13)	Acc@5  85.00 ( 89.32)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.5684e+00 (1.4490e+00)	Acc@1  61.00 ( 64.85)	Acc@5  87.00 ( 89.39)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.5439e+00 (1.4606e+00)	Acc@1  67.00 ( 64.51)	Acc@5  87.00 ( 89.04)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.3281e+00 (1.4522e+00)	Acc@1  70.00 ( 64.79)	Acc@5  93.00 ( 89.00)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 1.2393e+00 (1.4587e+00)	Acc@1  72.00 ( 64.70)	Acc@5  89.00 ( 88.96)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 1.9482e+00 (1.4673e+00)	Acc@1  62.00 ( 64.51)	Acc@5  84.00 ( 88.91)
Test: [ 90/100]	Time  0.050 ( 0.050)	Loss 1.9639e+00 (1.4625e+00)	Acc@1  52.00 ( 64.58)	Acc@5  80.00 ( 89.03)
 * Acc@1 64.640 Acc@5 89.000
### epoch[24] execution time: 52.36034297943115
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.290 ( 0.290)	Data  0.156 ( 0.156)	Loss 6.3965e-01 (6.3965e-01)	Acc@1  80.47 ( 80.47)	Acc@5  96.88 ( 96.88)
Epoch: [25][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.015)	Loss 6.2305e-01 (5.3984e-01)	Acc@1  83.59 ( 82.67)	Acc@5  96.09 ( 98.22)
Epoch: [25][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.008)	Loss 6.7969e-01 (5.6702e-01)	Acc@1  81.25 ( 82.89)	Acc@5  96.88 ( 98.07)
Epoch: [25][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 5.2832e-01 (5.4980e-01)	Acc@1  83.59 ( 83.39)	Acc@5  97.66 ( 98.03)
Epoch: [25][ 40/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.9927e-01 (5.4967e-01)	Acc@1  84.38 ( 83.08)	Acc@5  98.44 ( 98.13)
Epoch: [25][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.3662e-01 (5.4446e-01)	Acc@1  82.03 ( 83.13)	Acc@5  98.44 ( 98.22)
Epoch: [25][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.4385e-01 (5.4323e-01)	Acc@1  87.50 ( 82.95)	Acc@5 100.00 ( 98.27)
Epoch: [25][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.2441e-01 (5.4462e-01)	Acc@1  82.81 ( 82.82)	Acc@5  97.66 ( 98.31)
Epoch: [25][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.1523e-01 (5.4272e-01)	Acc@1  78.12 ( 82.80)	Acc@5  96.88 ( 98.30)
Epoch: [25][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.0684e-01 (5.3919e-01)	Acc@1  80.47 ( 82.85)	Acc@5  99.22 ( 98.31)
Epoch: [25][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.6055e-01 (5.4042e-01)	Acc@1  82.81 ( 82.80)	Acc@5  97.66 ( 98.28)
Epoch: [25][110/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5923e-01 (5.4453e-01)	Acc@1  86.72 ( 82.75)	Acc@5  98.44 ( 98.23)
Epoch: [25][120/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.4453e-01 (5.4761e-01)	Acc@1  76.56 ( 82.67)	Acc@5  96.09 ( 98.19)
Epoch: [25][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5820e-01 (5.5169e-01)	Acc@1  79.69 ( 82.62)	Acc@5  97.66 ( 98.13)
Epoch: [25][140/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9082e-01 (5.5242e-01)	Acc@1  78.91 ( 82.57)	Acc@5  96.88 ( 98.12)
Epoch: [25][150/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4033e-01 (5.5133e-01)	Acc@1  87.50 ( 82.51)	Acc@5 100.00 ( 98.13)
Epoch: [25][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0850e-01 (5.5590e-01)	Acc@1  77.34 ( 82.35)	Acc@5  97.66 ( 98.09)
Epoch: [25][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1328e-01 (5.5574e-01)	Acc@1  82.03 ( 82.32)	Acc@5  97.66 ( 98.09)
Epoch: [25][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2939e-01 (5.5432e-01)	Acc@1  78.12 ( 82.35)	Acc@5  99.22 ( 98.10)
Epoch: [25][190/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2109e-01 (5.5592e-01)	Acc@1  80.47 ( 82.23)	Acc@5  98.44 ( 98.10)
Epoch: [25][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7881e-01 (5.5945e-01)	Acc@1  78.12 ( 82.12)	Acc@5  96.09 ( 98.10)
Epoch: [25][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0215e-01 (5.6391e-01)	Acc@1  81.25 ( 81.97)	Acc@5  97.66 ( 98.05)
Epoch: [25][220/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2109e-01 (5.6474e-01)	Acc@1  78.12 ( 81.95)	Acc@5  98.44 ( 98.07)
Epoch: [25][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6113e-01 (5.6946e-01)	Acc@1  82.81 ( 81.81)	Acc@5  96.09 ( 98.03)
Epoch: [25][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7080e-01 (5.7178e-01)	Acc@1  79.69 ( 81.74)	Acc@5  96.09 ( 98.02)
Epoch: [25][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7666e-01 (5.7559e-01)	Acc@1  84.38 ( 81.66)	Acc@5  96.09 ( 97.97)
Epoch: [25][260/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4150e-01 (5.7721e-01)	Acc@1  79.69 ( 81.62)	Acc@5  96.88 ( 97.95)
Epoch: [25][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2109e-01 (5.7964e-01)	Acc@1  80.47 ( 81.54)	Acc@5  96.88 ( 97.93)
Epoch: [25][280/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8887e-01 (5.8038e-01)	Acc@1  80.47 ( 81.52)	Acc@5  98.44 ( 97.93)
Epoch: [25][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8594e-01 (5.8298e-01)	Acc@1  85.16 ( 81.44)	Acc@5  96.09 ( 97.91)
Epoch: [25][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6455e-01 (5.8681e-01)	Acc@1  80.47 ( 81.32)	Acc@5  97.66 ( 97.87)
Epoch: [25][310/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6523e-01 (5.8869e-01)	Acc@1  72.66 ( 81.29)	Acc@5  96.88 ( 97.87)
Epoch: [25][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8135e-01 (5.9220e-01)	Acc@1  72.66 ( 81.21)	Acc@5  94.53 ( 97.85)
Epoch: [25][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7002e-01 (5.9541e-01)	Acc@1  76.56 ( 81.14)	Acc@5  96.09 ( 97.83)
Epoch: [25][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6016e-01 (5.9782e-01)	Acc@1  77.34 ( 81.03)	Acc@5  97.66 ( 97.82)
Epoch: [25][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4854e-01 (6.0010e-01)	Acc@1  76.56 ( 80.97)	Acc@5  96.88 ( 97.80)
Epoch: [25][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4551e-01 (6.0231e-01)	Acc@1  78.91 ( 80.90)	Acc@5  98.44 ( 97.78)
Epoch: [25][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0771e-01 (6.0523e-01)	Acc@1  71.09 ( 80.82)	Acc@5  96.88 ( 97.76)
Epoch: [25][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6855e-01 (6.0864e-01)	Acc@1  76.56 ( 80.71)	Acc@5  96.88 ( 97.73)
Epoch: [25][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.2471e-01 (6.1123e-01)	Acc@1  75.00 ( 80.69)	Acc@5  97.50 ( 97.69)
## e[25] optimizer.zero_grad (sum) time: 0.6936056613922119
## e[25]       loss.backward (sum) time: 14.249048233032227
## e[25]      optimizer.step (sum) time: 8.13916802406311
## epoch[25] training(only) time: 47.24165201187134
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.3926e+00 (1.3926e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.046 ( 0.063)	Loss 1.3047e+00 (1.5312e+00)	Acc@1  69.00 ( 64.82)	Acc@5  93.00 ( 87.00)
Test: [ 20/100]	Time  0.046 ( 0.056)	Loss 1.3486e+00 (1.4599e+00)	Acc@1  66.00 ( 65.14)	Acc@5  89.00 ( 88.05)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6602e+00 (1.4843e+00)	Acc@1  57.00 ( 64.00)	Acc@5  90.00 ( 87.94)
Test: [ 40/100]	Time  0.056 ( 0.052)	Loss 1.4658e+00 (1.4826e+00)	Acc@1  72.00 ( 63.63)	Acc@5  88.00 ( 88.24)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.6543e+00 (1.4969e+00)	Acc@1  64.00 ( 63.29)	Acc@5  87.00 ( 88.12)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.5430e+00 (1.4685e+00)	Acc@1  57.00 ( 63.46)	Acc@5  93.00 ( 88.64)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.3486e+00 (1.4681e+00)	Acc@1  67.00 ( 63.59)	Acc@5  88.00 ( 88.52)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.6172e+00 (1.4764e+00)	Acc@1  62.00 ( 63.42)	Acc@5  87.00 ( 88.42)
Test: [ 90/100]	Time  0.056 ( 0.049)	Loss 1.7354e+00 (1.4674e+00)	Acc@1  59.00 ( 63.76)	Acc@5  86.00 ( 88.56)
 * Acc@1 63.920 Acc@5 88.710
### epoch[25] execution time: 52.26198434829712
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.286 ( 0.286)	Data  0.156 ( 0.156)	Loss 5.5273e-01 (5.5273e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 97.66)
Epoch: [26][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.015)	Loss 5.7227e-01 (5.6552e-01)	Acc@1  80.47 ( 82.24)	Acc@5  98.44 ( 98.01)
Epoch: [26][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.008)	Loss 4.2480e-01 (5.4170e-01)	Acc@1  87.50 ( 83.15)	Acc@5  99.22 ( 98.14)
Epoch: [26][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 5.1514e-01 (5.3881e-01)	Acc@1  82.03 ( 83.09)	Acc@5  98.44 ( 98.19)
Epoch: [26][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.6265e-01 (5.1807e-01)	Acc@1  87.50 ( 83.80)	Acc@5  98.44 ( 98.30)
Epoch: [26][ 50/391]	Time  0.128 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.9624e-01 (5.1480e-01)	Acc@1  86.72 ( 83.85)	Acc@5  97.66 ( 98.38)
Epoch: [26][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.8477e-01 (5.0958e-01)	Acc@1  88.28 ( 84.14)	Acc@5  98.44 ( 98.37)
Epoch: [26][ 70/391]	Time  0.129 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.0342e-01 (5.1087e-01)	Acc@1  85.94 ( 83.99)	Acc@5  99.22 ( 98.34)
Epoch: [26][ 80/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.2881e-01 (5.1898e-01)	Acc@1  83.59 ( 83.68)	Acc@5  99.22 ( 98.35)
Epoch: [26][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.6069e-01 (5.1863e-01)	Acc@1  85.94 ( 83.74)	Acc@5  98.44 ( 98.36)
Epoch: [26][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.3774e-01 (5.1666e-01)	Acc@1  82.81 ( 83.84)	Acc@5  99.22 ( 98.38)
Epoch: [26][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0879e-01 (5.1627e-01)	Acc@1  85.16 ( 83.78)	Acc@5  98.44 ( 98.39)
Epoch: [26][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.8086e-01 (5.1715e-01)	Acc@1  90.62 ( 83.68)	Acc@5 100.00 ( 98.39)
Epoch: [26][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.5566e-01 (5.1812e-01)	Acc@1  82.81 ( 83.67)	Acc@5  98.44 ( 98.41)
Epoch: [26][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3223e-01 (5.1678e-01)	Acc@1  83.59 ( 83.70)	Acc@5  97.66 ( 98.39)
Epoch: [26][150/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8486e-01 (5.1485e-01)	Acc@1  79.69 ( 83.76)	Acc@5  99.22 ( 98.37)
Epoch: [26][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1660e-01 (5.1700e-01)	Acc@1  82.81 ( 83.65)	Acc@5  96.88 ( 98.38)
Epoch: [26][170/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9482e-01 (5.1951e-01)	Acc@1  78.91 ( 83.53)	Acc@5 100.00 ( 98.40)
Epoch: [26][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6938e-01 (5.2006e-01)	Acc@1  85.16 ( 83.55)	Acc@5 100.00 ( 98.39)
Epoch: [26][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3516e-01 (5.2291e-01)	Acc@1  83.59 ( 83.44)	Acc@5  98.44 ( 98.36)
Epoch: [26][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7632e-01 (5.2564e-01)	Acc@1  85.16 ( 83.31)	Acc@5  99.22 ( 98.36)
Epoch: [26][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3027e-01 (5.2920e-01)	Acc@1  79.69 ( 83.15)	Acc@5 100.00 ( 98.33)
Epoch: [26][220/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4072e-01 (5.3215e-01)	Acc@1  75.78 ( 82.99)	Acc@5  97.66 ( 98.33)
Epoch: [26][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4346e-01 (5.3441e-01)	Acc@1  80.47 ( 82.92)	Acc@5  97.66 ( 98.31)
Epoch: [26][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4736e-01 (5.3533e-01)	Acc@1  81.25 ( 82.92)	Acc@5  96.88 ( 98.29)
Epoch: [26][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6641e-01 (5.3727e-01)	Acc@1  79.69 ( 82.87)	Acc@5  98.44 ( 98.26)
Epoch: [26][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2041e-01 (5.4134e-01)	Acc@1  69.53 ( 82.72)	Acc@5  94.53 ( 98.21)
Epoch: [26][270/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4883e-01 (5.4467e-01)	Acc@1  82.81 ( 82.64)	Acc@5  96.09 ( 98.19)
Epoch: [26][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1904e-01 (5.4749e-01)	Acc@1  82.81 ( 82.58)	Acc@5  98.44 ( 98.18)
Epoch: [26][290/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7979e-01 (5.5225e-01)	Acc@1  72.66 ( 82.43)	Acc@5  96.88 ( 98.14)
Epoch: [26][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3711e-01 (5.5488e-01)	Acc@1  81.25 ( 82.35)	Acc@5  99.22 ( 98.12)
Epoch: [26][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5293e-01 (5.5953e-01)	Acc@1  77.34 ( 82.22)	Acc@5  95.31 ( 98.07)
Epoch: [26][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0107e-01 (5.6183e-01)	Acc@1  80.47 ( 82.15)	Acc@5  97.66 ( 98.05)
Epoch: [26][330/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0508e-01 (5.6434e-01)	Acc@1  78.12 ( 82.09)	Acc@5  97.66 ( 98.03)
Epoch: [26][340/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1230e-01 (5.6460e-01)	Acc@1  81.25 ( 82.05)	Acc@5  98.44 ( 98.04)
Epoch: [26][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.4268e-01 (5.6506e-01)	Acc@1  78.12 ( 82.06)	Acc@5  95.31 ( 98.03)
Epoch: [26][360/391]	Time  0.141 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.7178e-01 (5.6825e-01)	Acc@1  78.12 ( 81.93)	Acc@5  96.09 ( 98.00)
Epoch: [26][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0791e-01 (5.7013e-01)	Acc@1  82.03 ( 81.88)	Acc@5  97.66 ( 97.97)
Epoch: [26][380/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.2109e-01 (5.7213e-01)	Acc@1  78.91 ( 81.81)	Acc@5  98.44 ( 97.95)
Epoch: [26][390/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1846e-01 (5.7369e-01)	Acc@1  86.25 ( 81.75)	Acc@5  98.75 ( 97.95)
## e[26] optimizer.zero_grad (sum) time: 0.6884620189666748
## e[26]       loss.backward (sum) time: 14.30103588104248
## e[26]      optimizer.step (sum) time: 8.097028732299805
## epoch[26] training(only) time: 47.24819540977478
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.4521e+00 (1.4521e+00)	Acc@1  67.00 ( 67.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.1494e+00 (1.5488e+00)	Acc@1  73.00 ( 65.27)	Acc@5  93.00 ( 87.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.2031e+00 (1.4567e+00)	Acc@1  66.00 ( 65.90)	Acc@5  88.00 ( 88.90)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.8896e+00 (1.4689e+00)	Acc@1  51.00 ( 65.29)	Acc@5  88.00 ( 88.77)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.4307e+00 (1.4519e+00)	Acc@1  66.00 ( 65.29)	Acc@5  90.00 ( 88.85)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.4893e+00 (1.4662e+00)	Acc@1  61.00 ( 64.92)	Acc@5  90.00 ( 88.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2549e+00 (1.4451e+00)	Acc@1  66.00 ( 64.80)	Acc@5  90.00 ( 89.05)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.7529e+00 (1.4524e+00)	Acc@1  63.00 ( 64.96)	Acc@5  91.00 ( 88.89)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 1.5000e+00 (1.4536e+00)	Acc@1  64.00 ( 64.79)	Acc@5  82.00 ( 88.80)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7266e+00 (1.4349e+00)	Acc@1  56.00 ( 64.97)	Acc@5  88.00 ( 89.03)
 * Acc@1 65.170 Acc@5 89.270
### epoch[26] execution time: 52.265681743621826
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.279 ( 0.279)	Data  0.148 ( 0.148)	Loss 6.8164e-01 (6.8164e-01)	Acc@1  81.25 ( 81.25)	Acc@5  95.31 ( 95.31)
Epoch: [27][ 10/391]	Time  0.117 ( 0.133)	Data  0.001 ( 0.014)	Loss 5.5859e-01 (5.3602e-01)	Acc@1  81.25 ( 82.67)	Acc@5  97.66 ( 98.37)
Epoch: [27][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 5.0488e-01 (4.9817e-01)	Acc@1  87.50 ( 83.56)	Acc@5  98.44 ( 98.59)
Epoch: [27][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.8853e-01 (4.8489e-01)	Acc@1  86.72 ( 84.20)	Acc@5  99.22 ( 98.66)
Epoch: [27][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.005)	Loss 4.5020e-01 (4.8739e-01)	Acc@1  83.59 ( 84.30)	Acc@5  99.22 ( 98.63)
Epoch: [27][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.0869e-01 (4.8575e-01)	Acc@1  85.94 ( 84.51)	Acc@5  96.88 ( 98.59)
Epoch: [27][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.5117e-01 (4.8633e-01)	Acc@1  83.59 ( 84.70)	Acc@5 100.00 ( 98.59)
Epoch: [27][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.3711e-01 (4.8774e-01)	Acc@1  84.38 ( 84.61)	Acc@5  98.44 ( 98.56)
Epoch: [27][ 80/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.9131e-01 (4.9396e-01)	Acc@1  84.38 ( 84.44)	Acc@5  97.66 ( 98.54)
Epoch: [27][ 90/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.5376e-01 (4.9081e-01)	Acc@1  90.62 ( 84.41)	Acc@5 100.00 ( 98.57)
Epoch: [27][100/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5371e-01 (4.9818e-01)	Acc@1  83.59 ( 84.20)	Acc@5  96.09 ( 98.49)
Epoch: [27][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (4.9629e-01)	Acc@1  80.47 ( 84.21)	Acc@5  97.66 ( 98.51)
Epoch: [27][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8208e-01 (4.9680e-01)	Acc@1  87.50 ( 84.12)	Acc@5  98.44 ( 98.45)
Epoch: [27][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5088e-01 (4.9993e-01)	Acc@1  80.47 ( 84.02)	Acc@5  96.09 ( 98.43)
Epoch: [27][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0400e-01 (5.0104e-01)	Acc@1  82.81 ( 84.06)	Acc@5  97.66 ( 98.42)
Epoch: [27][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1191e-01 (5.0209e-01)	Acc@1  76.56 ( 84.00)	Acc@5  97.66 ( 98.45)
Epoch: [27][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6494e-01 (5.0279e-01)	Acc@1  81.25 ( 84.02)	Acc@5  96.88 ( 98.42)
Epoch: [27][170/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7227e-01 (5.0624e-01)	Acc@1  82.03 ( 83.95)	Acc@5  99.22 ( 98.41)
Epoch: [27][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0088e-01 (5.0905e-01)	Acc@1  88.28 ( 83.89)	Acc@5  99.22 ( 98.41)
Epoch: [27][190/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2207e-01 (5.1458e-01)	Acc@1  77.34 ( 83.72)	Acc@5  97.66 ( 98.38)
Epoch: [27][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0977e-01 (5.1887e-01)	Acc@1  84.38 ( 83.64)	Acc@5 100.00 ( 98.35)
Epoch: [27][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8203e-01 (5.2097e-01)	Acc@1  80.47 ( 83.55)	Acc@5  95.31 ( 98.32)
Epoch: [27][220/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9180e-01 (5.2296e-01)	Acc@1  78.12 ( 83.46)	Acc@5  98.44 ( 98.30)
Epoch: [27][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0312e-01 (5.2487e-01)	Acc@1  73.44 ( 83.37)	Acc@5  96.88 ( 98.28)
Epoch: [27][240/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (5.2626e-01)	Acc@1  82.81 ( 83.35)	Acc@5  99.22 ( 98.27)
Epoch: [27][250/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0020e-01 (5.2829e-01)	Acc@1  78.12 ( 83.29)	Acc@5  98.44 ( 98.25)
Epoch: [27][260/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4111e-01 (5.2901e-01)	Acc@1  76.56 ( 83.26)	Acc@5  99.22 ( 98.25)
Epoch: [27][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9731e-01 (5.3074e-01)	Acc@1  84.38 ( 83.22)	Acc@5  97.66 ( 98.23)
Epoch: [27][280/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1338e-01 (5.3210e-01)	Acc@1  78.12 ( 83.17)	Acc@5  94.53 ( 98.24)
Epoch: [27][290/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6367e-01 (5.3526e-01)	Acc@1  79.69 ( 83.09)	Acc@5  96.09 ( 98.21)
Epoch: [27][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9805e-01 (5.3585e-01)	Acc@1  80.47 ( 83.05)	Acc@5  97.66 ( 98.20)
Epoch: [27][310/391]	Time  0.136 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1182e-01 (5.3872e-01)	Acc@1  82.03 ( 82.98)	Acc@5  97.66 ( 98.19)
Epoch: [27][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4653e-01 (5.3858e-01)	Acc@1  85.94 ( 82.98)	Acc@5  98.44 ( 98.20)
Epoch: [27][330/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0693e-01 (5.3961e-01)	Acc@1  83.59 ( 82.98)	Acc@5  96.88 ( 98.19)
Epoch: [27][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.7031e-01 (5.4002e-01)	Acc@1  79.69 ( 82.98)	Acc@5  97.66 ( 98.19)
Epoch: [27][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.2793e-01 (5.4087e-01)	Acc@1  82.03 ( 82.93)	Acc@5  99.22 ( 98.20)
Epoch: [27][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.1104e-01 (5.4243e-01)	Acc@1  78.12 ( 82.87)	Acc@5  93.75 ( 98.18)
Epoch: [27][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.0615e-01 (5.4401e-01)	Acc@1  73.44 ( 82.83)	Acc@5  96.09 ( 98.16)
Epoch: [27][380/391]	Time  0.123 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.4209e-01 (5.4582e-01)	Acc@1  77.34 ( 82.76)	Acc@5  97.66 ( 98.15)
Epoch: [27][390/391]	Time  0.115 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.1592e-01 (5.4781e-01)	Acc@1  75.00 ( 82.73)	Acc@5  93.75 ( 98.11)
## e[27] optimizer.zero_grad (sum) time: 0.6896188259124756
## e[27]       loss.backward (sum) time: 14.251399278640747
## e[27]      optimizer.step (sum) time: 8.0669424533844
## epoch[27] training(only) time: 47.175087690353394
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.0781e+00 (1.0781e+00)	Acc@1  68.00 ( 68.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.4082e+00 (1.4078e+00)	Acc@1  62.00 ( 65.55)	Acc@5  91.00 ( 88.09)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.1787e+00 (1.3964e+00)	Acc@1  69.00 ( 65.10)	Acc@5  93.00 ( 89.00)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.9102e+00 (1.4272e+00)	Acc@1  50.00 ( 64.97)	Acc@5  81.00 ( 88.26)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5723e+00 (1.4262e+00)	Acc@1  63.00 ( 64.73)	Acc@5  89.00 ( 88.27)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.6016e+00 (1.4258e+00)	Acc@1  61.00 ( 64.65)	Acc@5  89.00 ( 88.22)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.3643e+00 (1.4191e+00)	Acc@1  60.00 ( 64.43)	Acc@5  93.00 ( 88.57)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.7109e+00 (1.4195e+00)	Acc@1  57.00 ( 64.54)	Acc@5  90.00 ( 88.66)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3955e+00 (1.4306e+00)	Acc@1  67.00 ( 64.40)	Acc@5  90.00 ( 88.62)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5859e+00 (1.4135e+00)	Acc@1  59.00 ( 64.78)	Acc@5  85.00 ( 88.69)
 * Acc@1 65.170 Acc@5 88.780
### epoch[27] execution time: 52.172298431396484
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.278 ( 0.278)	Data  0.140 ( 0.140)	Loss 5.1025e-01 (5.1025e-01)	Acc@1  83.59 ( 83.59)	Acc@5  97.66 ( 97.66)
Epoch: [28][ 10/391]	Time  0.122 ( 0.135)	Data  0.001 ( 0.014)	Loss 4.6777e-01 (5.0999e-01)	Acc@1  84.38 ( 83.38)	Acc@5  99.22 ( 98.65)
Epoch: [28][ 20/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.7192e-01 (4.9685e-01)	Acc@1  83.59 ( 84.08)	Acc@5  99.22 ( 98.74)
Epoch: [28][ 30/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 4.6387e-01 (4.8948e-01)	Acc@1  84.38 ( 84.27)	Acc@5  99.22 ( 98.77)
Epoch: [28][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 6.8652e-01 (4.8677e-01)	Acc@1  81.25 ( 84.53)	Acc@5  96.88 ( 98.74)
Epoch: [28][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.2593e-01 (4.8249e-01)	Acc@1  89.84 ( 84.44)	Acc@5  99.22 ( 98.82)
Epoch: [28][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.1377e-01 (4.7729e-01)	Acc@1  78.12 ( 84.43)	Acc@5  98.44 ( 98.87)
Epoch: [28][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.8730e-01 (4.7914e-01)	Acc@1  85.16 ( 84.33)	Acc@5  99.22 ( 98.89)
Epoch: [28][ 80/391]	Time  0.137 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.3525e-01 (4.7790e-01)	Acc@1  78.91 ( 84.42)	Acc@5  96.88 ( 98.83)
Epoch: [28][ 90/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.2051e-01 (4.7485e-01)	Acc@1  80.47 ( 84.60)	Acc@5 100.00 ( 98.86)
Epoch: [28][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6631e-01 (4.7395e-01)	Acc@1  87.50 ( 84.75)	Acc@5  97.66 ( 98.82)
Epoch: [28][110/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (4.7381e-01)	Acc@1  82.81 ( 84.81)	Acc@5  97.66 ( 98.82)
Epoch: [28][120/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4873e-01 (4.7401e-01)	Acc@1  85.94 ( 84.73)	Acc@5  99.22 ( 98.81)
Epoch: [28][130/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.4883e-01 (4.7587e-01)	Acc@1  86.72 ( 84.74)	Acc@5  98.44 ( 98.78)
Epoch: [28][140/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.4209e-01 (4.7838e-01)	Acc@1  80.47 ( 84.64)	Acc@5  97.66 ( 98.74)
Epoch: [28][150/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9438e-01 (4.8318e-01)	Acc@1  82.81 ( 84.47)	Acc@5  97.66 ( 98.67)
Epoch: [28][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9766e-01 (4.8540e-01)	Acc@1  78.12 ( 84.35)	Acc@5  99.22 ( 98.63)
Epoch: [28][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9648e-01 (4.8774e-01)	Acc@1  88.28 ( 84.27)	Acc@5  96.88 ( 98.58)
Epoch: [28][180/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7129e-01 (4.9040e-01)	Acc@1  84.38 ( 84.19)	Acc@5  97.66 ( 98.59)
Epoch: [28][190/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2490e-01 (4.9205e-01)	Acc@1  85.16 ( 84.15)	Acc@5  99.22 ( 98.60)
Epoch: [28][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3711e-01 (4.9358e-01)	Acc@1  85.16 ( 84.13)	Acc@5  96.88 ( 98.59)
Epoch: [28][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9893e-01 (4.9307e-01)	Acc@1  86.72 ( 84.17)	Acc@5  99.22 ( 98.55)
Epoch: [28][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6045e-01 (4.9392e-01)	Acc@1  89.06 ( 84.16)	Acc@5  96.09 ( 98.52)
Epoch: [28][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3857e-01 (4.9758e-01)	Acc@1  80.47 ( 84.01)	Acc@5  99.22 ( 98.50)
Epoch: [28][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7861e-01 (4.9951e-01)	Acc@1  79.69 ( 83.95)	Acc@5 100.00 ( 98.49)
Epoch: [28][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3662e-01 (5.0344e-01)	Acc@1  82.81 ( 83.84)	Acc@5  97.66 ( 98.47)
Epoch: [28][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6543e-01 (5.0714e-01)	Acc@1  78.91 ( 83.73)	Acc@5  99.22 ( 98.46)
Epoch: [28][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5332e-01 (5.0871e-01)	Acc@1  81.25 ( 83.70)	Acc@5  97.66 ( 98.44)
Epoch: [28][280/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.9385e-01 (5.1146e-01)	Acc@1  77.34 ( 83.57)	Acc@5  97.66 ( 98.44)
Epoch: [28][290/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5977e-01 (5.1495e-01)	Acc@1  78.12 ( 83.51)	Acc@5  96.09 ( 98.41)
Epoch: [28][300/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0879e-01 (5.1822e-01)	Acc@1  84.38 ( 83.41)	Acc@5  98.44 ( 98.38)
Epoch: [28][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.9512e-01 (5.1922e-01)	Acc@1  85.16 ( 83.40)	Acc@5  99.22 ( 98.36)
Epoch: [28][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.9297e-01 (5.2262e-01)	Acc@1  77.34 ( 83.31)	Acc@5  97.66 ( 98.35)
Epoch: [28][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7095e-01 (5.2410e-01)	Acc@1  85.94 ( 83.28)	Acc@5  97.66 ( 98.34)
Epoch: [28][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.1729e-01 (5.2736e-01)	Acc@1  77.34 ( 83.20)	Acc@5  96.88 ( 98.31)
Epoch: [28][350/391]	Time  0.136 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.6699e-01 (5.2843e-01)	Acc@1  78.12 ( 83.18)	Acc@5  97.66 ( 98.30)
Epoch: [28][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5923e-01 (5.2928e-01)	Acc@1  85.94 ( 83.17)	Acc@5  98.44 ( 98.30)
Epoch: [28][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.8447e-01 (5.3186e-01)	Acc@1  80.47 ( 83.09)	Acc@5  98.44 ( 98.28)
Epoch: [28][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.8799e-01 (5.3306e-01)	Acc@1  77.34 ( 83.04)	Acc@5  96.09 ( 98.28)
Epoch: [28][390/391]	Time  0.104 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5469e-01 (5.3383e-01)	Acc@1  81.25 ( 82.98)	Acc@5  96.25 ( 98.27)
## e[28] optimizer.zero_grad (sum) time: 0.687488317489624
## e[28]       loss.backward (sum) time: 14.342755556106567
## e[28]      optimizer.step (sum) time: 8.098157167434692
## epoch[28] training(only) time: 47.33783984184265
# Switched to evaluate mode...
Test: [  0/100]	Time  0.201 ( 0.201)	Loss 1.3584e+00 (1.3584e+00)	Acc@1  63.00 ( 63.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.2979e+00 (1.4609e+00)	Acc@1  67.00 ( 64.91)	Acc@5  90.00 ( 88.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0312e+00 (1.3804e+00)	Acc@1  74.00 ( 66.71)	Acc@5  91.00 ( 89.71)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 1.7734e+00 (1.4110e+00)	Acc@1  61.00 ( 66.13)	Acc@5  87.00 ( 89.39)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.5264e+00 (1.4051e+00)	Acc@1  69.00 ( 65.71)	Acc@5  88.00 ( 89.68)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.5410e+00 (1.4214e+00)	Acc@1  69.00 ( 65.43)	Acc@5  88.00 ( 89.29)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.1963e+00 (1.4094e+00)	Acc@1  69.00 ( 65.26)	Acc@5  91.00 ( 89.43)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4658e+00 (1.4128e+00)	Acc@1  65.00 ( 65.13)	Acc@5  87.00 ( 89.61)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.7949e+00 (1.4239e+00)	Acc@1  66.00 ( 65.06)	Acc@5  86.00 ( 89.43)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.5557e+00 (1.4056e+00)	Acc@1  63.00 ( 65.38)	Acc@5  91.00 ( 89.51)
 * Acc@1 65.470 Acc@5 89.550
### epoch[28] execution time: 52.373637437820435
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.291 ( 0.291)	Data  0.156 ( 0.156)	Loss 4.5801e-01 (4.5801e-01)	Acc@1  83.59 ( 83.59)	Acc@5  98.44 ( 98.44)
Epoch: [29][ 10/391]	Time  0.119 ( 0.136)	Data  0.001 ( 0.015)	Loss 4.4995e-01 (4.7239e-01)	Acc@1  88.28 ( 85.65)	Acc@5  97.66 ( 98.51)
Epoch: [29][ 20/391]	Time  0.119 ( 0.129)	Data  0.001 ( 0.008)	Loss 5.7178e-01 (5.0916e-01)	Acc@1  81.25 ( 83.82)	Acc@5  98.44 ( 98.36)
Epoch: [29][ 30/391]	Time  0.121 ( 0.126)	Data  0.001 ( 0.006)	Loss 4.9365e-01 (5.0499e-01)	Acc@1  87.50 ( 84.38)	Acc@5  99.22 ( 98.36)
Epoch: [29][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.7729e-01 (4.9865e-01)	Acc@1  87.50 ( 84.39)	Acc@5  97.66 ( 98.32)
Epoch: [29][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.2490e-01 (4.8977e-01)	Acc@1  82.81 ( 84.65)	Acc@5  98.44 ( 98.44)
Epoch: [29][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.3311e-01 (4.9609e-01)	Acc@1  85.94 ( 84.36)	Acc@5 100.00 ( 98.42)
Epoch: [29][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.8750e-01 (4.9633e-01)	Acc@1  76.56 ( 84.26)	Acc@5  97.66 ( 98.49)
Epoch: [29][ 80/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.4116e-01 (4.9427e-01)	Acc@1  87.50 ( 84.29)	Acc@5  99.22 ( 98.51)
Epoch: [29][ 90/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.5176e-01 (4.9154e-01)	Acc@1  78.91 ( 84.35)	Acc@5  99.22 ( 98.53)
Epoch: [29][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.8364e-01 (4.8885e-01)	Acc@1  81.25 ( 84.50)	Acc@5 100.00 ( 98.54)
Epoch: [29][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.9478e-01 (4.8775e-01)	Acc@1  86.72 ( 84.55)	Acc@5 100.00 ( 98.59)
Epoch: [29][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4009e-01 (4.8687e-01)	Acc@1  87.50 ( 84.56)	Acc@5  99.22 ( 98.59)
Epoch: [29][130/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6899e-01 (4.8320e-01)	Acc@1  84.38 ( 84.71)	Acc@5  96.09 ( 98.59)
Epoch: [29][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9048e-01 (4.8575e-01)	Acc@1  87.50 ( 84.64)	Acc@5  97.66 ( 98.58)
Epoch: [29][150/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7168e-01 (4.8704e-01)	Acc@1  85.94 ( 84.57)	Acc@5  98.44 ( 98.58)
Epoch: [29][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6680e-01 (4.8632e-01)	Acc@1  86.72 ( 84.58)	Acc@5  97.66 ( 98.59)
Epoch: [29][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6885e-01 (4.8963e-01)	Acc@1  82.81 ( 84.44)	Acc@5  99.22 ( 98.52)
Epoch: [29][180/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2979e-01 (4.9221e-01)	Acc@1  83.59 ( 84.43)	Acc@5  98.44 ( 98.50)
Epoch: [29][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3213e-01 (4.9523e-01)	Acc@1  86.72 ( 84.37)	Acc@5  99.22 ( 98.50)
Epoch: [29][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9438e-01 (4.9503e-01)	Acc@1  84.38 ( 84.35)	Acc@5  98.44 ( 98.49)
Epoch: [29][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5420e-01 (4.9714e-01)	Acc@1  81.25 ( 84.24)	Acc@5  99.22 ( 98.46)
Epoch: [29][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9326e-01 (4.9878e-01)	Acc@1  81.25 ( 84.13)	Acc@5  98.44 ( 98.47)
Epoch: [29][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4102e-01 (4.9787e-01)	Acc@1  82.03 ( 84.14)	Acc@5 100.00 ( 98.49)
Epoch: [29][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4492e-01 (4.9934e-01)	Acc@1  85.16 ( 84.10)	Acc@5  98.44 ( 98.50)
Epoch: [29][250/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3818e-01 (5.0121e-01)	Acc@1  82.03 ( 84.07)	Acc@5 100.00 ( 98.50)
Epoch: [29][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5918e-01 (5.0165e-01)	Acc@1  78.12 ( 84.06)	Acc@5  97.66 ( 98.50)
Epoch: [29][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6411e-01 (5.0337e-01)	Acc@1  85.16 ( 83.96)	Acc@5 100.00 ( 98.49)
Epoch: [29][280/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5195e-01 (5.0807e-01)	Acc@1  78.12 ( 83.84)	Acc@5  95.31 ( 98.46)
Epoch: [29][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5723e-01 (5.1109e-01)	Acc@1  82.03 ( 83.75)	Acc@5  97.66 ( 98.43)
Epoch: [29][300/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7080e-01 (5.1353e-01)	Acc@1  82.03 ( 83.67)	Acc@5  96.88 ( 98.41)
Epoch: [29][310/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8413e-01 (5.1543e-01)	Acc@1  86.72 ( 83.60)	Acc@5  97.66 ( 98.39)
Epoch: [29][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0479e-01 (5.1727e-01)	Acc@1  87.50 ( 83.52)	Acc@5  97.66 ( 98.38)
Epoch: [29][330/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8853e-01 (5.1813e-01)	Acc@1  85.16 ( 83.49)	Acc@5  98.44 ( 98.38)
Epoch: [29][340/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0879e-01 (5.1925e-01)	Acc@1  82.03 ( 83.44)	Acc@5  99.22 ( 98.37)
Epoch: [29][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.3472e-01 (5.1889e-01)	Acc@1  89.06 ( 83.42)	Acc@5 100.00 ( 98.39)
Epoch: [29][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4360e-01 (5.2035e-01)	Acc@1  83.59 ( 83.36)	Acc@5  99.22 ( 98.37)
Epoch: [29][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5273e-01 (5.2194e-01)	Acc@1  84.38 ( 83.31)	Acc@5  97.66 ( 98.36)
Epoch: [29][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9131e-01 (5.2401e-01)	Acc@1  79.69 ( 83.23)	Acc@5  97.66 ( 98.36)
Epoch: [29][390/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8560e-01 (5.2432e-01)	Acc@1  83.75 ( 83.23)	Acc@5 100.00 ( 98.36)
## e[29] optimizer.zero_grad (sum) time: 0.6872768402099609
## e[29]       loss.backward (sum) time: 14.289332866668701
## e[29]      optimizer.step (sum) time: 8.111363410949707
## epoch[29] training(only) time: 47.358848571777344
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.3965e+00 (1.3965e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.5166e+00 (1.5159e+00)	Acc@1  67.00 ( 65.09)	Acc@5  92.00 ( 89.27)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.3428e+00 (1.4724e+00)	Acc@1  66.00 ( 64.95)	Acc@5  93.00 ( 89.76)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.6377e+00 (1.4534e+00)	Acc@1  63.00 ( 65.10)	Acc@5  89.00 ( 89.42)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 1.3672e+00 (1.4367e+00)	Acc@1  69.00 ( 65.41)	Acc@5  88.00 ( 89.61)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.5244e+00 (1.4472e+00)	Acc@1  64.00 ( 65.12)	Acc@5  89.00 ( 89.31)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.3330e+00 (1.4353e+00)	Acc@1  64.00 ( 64.92)	Acc@5  95.00 ( 89.46)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.6309e+00 (1.4523e+00)	Acc@1  61.00 ( 64.77)	Acc@5  86.00 ( 89.27)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.6455e+00 (1.4627e+00)	Acc@1  67.00 ( 64.65)	Acc@5  82.00 ( 88.99)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.7109e+00 (1.4516e+00)	Acc@1  56.00 ( 64.73)	Acc@5  84.00 ( 89.11)
 * Acc@1 65.010 Acc@5 89.130
### epoch[29] execution time: 52.39008092880249
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.285 ( 0.285)	Data  0.152 ( 0.152)	Loss 5.0635e-01 (5.0635e-01)	Acc@1  85.16 ( 85.16)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.118 ( 0.136)	Data  0.001 ( 0.015)	Loss 3.7720e-01 (4.4276e-01)	Acc@1  86.72 ( 86.36)	Acc@5 100.00 ( 99.29)
Epoch: [30][ 20/391]	Time  0.121 ( 0.128)	Data  0.001 ( 0.008)	Loss 3.7573e-01 (4.3149e-01)	Acc@1  85.94 ( 85.83)	Acc@5 100.00 ( 99.18)
Epoch: [30][ 30/391]	Time  0.121 ( 0.126)	Data  0.001 ( 0.006)	Loss 3.3691e-01 (4.1110e-01)	Acc@1  88.28 ( 86.87)	Acc@5  99.22 ( 99.24)
Epoch: [30][ 40/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.2373e-01 (3.9230e-01)	Acc@1  90.62 ( 87.61)	Acc@5  98.44 ( 99.24)
Epoch: [30][ 50/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.0029e-01 (3.7746e-01)	Acc@1  90.62 ( 88.20)	Acc@5 100.00 ( 99.31)
Epoch: [30][ 60/391]	Time  0.125 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.3423e-01 (3.6701e-01)	Acc@1  90.62 ( 88.60)	Acc@5  99.22 ( 99.32)
Epoch: [30][ 70/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.1655e-01 (3.5580e-01)	Acc@1  95.31 ( 89.03)	Acc@5 100.00 ( 99.32)
Epoch: [30][ 80/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.2314e-01 (3.4815e-01)	Acc@1  96.88 ( 89.29)	Acc@5  99.22 ( 99.34)
Epoch: [30][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.7002e-01 (3.4192e-01)	Acc@1  91.41 ( 89.57)	Acc@5 100.00 ( 99.37)
Epoch: [30][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.6538e-01 (3.3294e-01)	Acc@1  93.75 ( 89.96)	Acc@5 100.00 ( 99.40)
Epoch: [30][110/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0786e-01 (3.3119e-01)	Acc@1  91.41 ( 90.05)	Acc@5 100.00 ( 99.39)
Epoch: [30][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.5684e-01 (3.2436e-01)	Acc@1  92.19 ( 90.28)	Acc@5 100.00 ( 99.42)
Epoch: [30][130/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.4207e-01 (3.2108e-01)	Acc@1  92.97 ( 90.34)	Acc@5 100.00 ( 99.40)
Epoch: [30][140/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0740e-01 (3.1540e-01)	Acc@1  93.75 ( 90.54)	Acc@5 100.00 ( 99.43)
Epoch: [30][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9590e-01 (3.1086e-01)	Acc@1  92.97 ( 90.72)	Acc@5  98.44 ( 99.44)
Epoch: [30][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1250e-01 (3.0842e-01)	Acc@1  87.50 ( 90.76)	Acc@5 100.00 ( 99.45)
Epoch: [30][170/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2156e-01 (3.0469e-01)	Acc@1  94.53 ( 90.86)	Acc@5 100.00 ( 99.46)
Epoch: [30][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6028e-01 (3.0107e-01)	Acc@1  96.09 ( 91.01)	Acc@5 100.00 ( 99.47)
Epoch: [30][190/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.5659e-01 (2.9836e-01)	Acc@1  92.97 ( 91.13)	Acc@5 100.00 ( 99.48)
Epoch: [30][200/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.9092e-01 (2.9528e-01)	Acc@1  94.53 ( 91.24)	Acc@5 100.00 ( 99.49)
Epoch: [30][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8955e-01 (2.9228e-01)	Acc@1  92.97 ( 91.32)	Acc@5  98.44 ( 99.49)
Epoch: [30][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9849e-01 (2.9023e-01)	Acc@1  94.53 ( 91.42)	Acc@5  99.22 ( 99.50)
Epoch: [30][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9492e-01 (2.8812e-01)	Acc@1  92.19 ( 91.50)	Acc@5  99.22 ( 99.48)
Epoch: [30][240/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0679e-01 (2.8541e-01)	Acc@1  94.53 ( 91.61)	Acc@5 100.00 ( 99.48)
Epoch: [30][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6882e-01 (2.8306e-01)	Acc@1  96.09 ( 91.67)	Acc@5 100.00 ( 99.50)
Epoch: [30][260/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6880e-01 (2.8098e-01)	Acc@1  96.09 ( 91.74)	Acc@5  98.44 ( 99.50)
Epoch: [30][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1519e-01 (2.7995e-01)	Acc@1  92.19 ( 91.78)	Acc@5  99.22 ( 99.50)
Epoch: [30][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8760e-01 (2.7852e-01)	Acc@1  92.97 ( 91.83)	Acc@5 100.00 ( 99.51)
Epoch: [30][290/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5269e-01 (2.7803e-01)	Acc@1  92.19 ( 91.84)	Acc@5 100.00 ( 99.50)
Epoch: [30][300/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4316e-01 (2.7649e-01)	Acc@1  94.53 ( 91.86)	Acc@5  99.22 ( 99.51)
Epoch: [30][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.1436e-01 (2.7546e-01)	Acc@1  92.97 ( 91.90)	Acc@5 100.00 ( 99.51)
Epoch: [30][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.2559e-01 (2.7380e-01)	Acc@1  96.88 ( 91.97)	Acc@5  98.44 ( 99.51)
Epoch: [30][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.0374e-01 (2.7209e-01)	Acc@1  94.53 ( 92.03)	Acc@5  98.44 ( 99.52)
Epoch: [30][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.5049e-01 (2.7136e-01)	Acc@1  92.19 ( 92.06)	Acc@5 100.00 ( 99.52)
Epoch: [30][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.5466e-01 (2.7011e-01)	Acc@1  96.09 ( 92.11)	Acc@5 100.00 ( 99.53)
Epoch: [30][360/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.7808e-01 (2.6822e-01)	Acc@1  94.53 ( 92.17)	Acc@5  99.22 ( 99.54)
Epoch: [30][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.3938e-01 (2.6703e-01)	Acc@1  91.41 ( 92.19)	Acc@5 100.00 ( 99.54)
Epoch: [30][380/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.6025e-01 (2.6556e-01)	Acc@1  92.97 ( 92.23)	Acc@5 100.00 ( 99.55)
Epoch: [30][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1543e-01 (2.6444e-01)	Acc@1  92.50 ( 92.25)	Acc@5 100.00 ( 99.56)
## e[30] optimizer.zero_grad (sum) time: 0.691565752029419
## e[30]       loss.backward (sum) time: 14.231764793395996
## e[30]      optimizer.step (sum) time: 8.221394300460815
## epoch[30] training(only) time: 47.41751194000244
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 9.0869e-01 (9.0869e-01)	Acc@1  75.00 ( 75.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.0576e+00 (1.1273e+00)	Acc@1  75.00 ( 73.18)	Acc@5  93.00 ( 91.91)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 9.6338e-01 (1.0978e+00)	Acc@1  76.00 ( 73.71)	Acc@5  92.00 ( 92.24)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 1.3809e+00 (1.1052e+00)	Acc@1  71.00 ( 73.39)	Acc@5  92.00 ( 92.23)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.1367e+00 (1.1011e+00)	Acc@1  75.00 ( 73.44)	Acc@5  91.00 ( 92.39)
Test: [ 50/100]	Time  0.055 ( 0.051)	Loss 1.3008e+00 (1.1086e+00)	Acc@1  68.00 ( 72.88)	Acc@5  93.00 ( 92.33)
Test: [ 60/100]	Time  0.050 ( 0.050)	Loss 1.0146e+00 (1.0878e+00)	Acc@1  70.00 ( 72.61)	Acc@5  96.00 ( 92.72)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3193e+00 (1.0927e+00)	Acc@1  69.00 ( 72.39)	Acc@5  91.00 ( 92.69)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.3955e+00 (1.0980e+00)	Acc@1  72.00 ( 72.33)	Acc@5  88.00 ( 92.58)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 1.3926e+00 (1.0842e+00)	Acc@1  66.00 ( 72.46)	Acc@5  90.00 ( 92.76)
 * Acc@1 72.550 Acc@5 92.830
### epoch[30] execution time: 52.43374013900757
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.300 ( 0.300)	Data  0.152 ( 0.152)	Loss 1.5295e-01 (1.5295e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.118 ( 0.137)	Data  0.001 ( 0.015)	Loss 1.9202e-01 (1.8121e-01)	Acc@1  96.09 ( 95.95)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 20/391]	Time  0.119 ( 0.129)	Data  0.001 ( 0.008)	Loss 1.7786e-01 (1.8120e-01)	Acc@1  92.97 ( 95.54)	Acc@5 100.00 ( 99.85)
Epoch: [31][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.006)	Loss 2.0068e-01 (1.8367e-01)	Acc@1  95.31 ( 95.51)	Acc@5 100.00 ( 99.82)
Epoch: [31][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 2.3755e-01 (1.9052e-01)	Acc@1  92.97 ( 95.06)	Acc@5 100.00 ( 99.79)
Epoch: [31][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.8237e-01 (1.9223e-01)	Acc@1  96.88 ( 94.93)	Acc@5 100.00 ( 99.80)
Epoch: [31][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.7151e-01 (1.8987e-01)	Acc@1  96.88 ( 95.12)	Acc@5 100.00 ( 99.83)
Epoch: [31][ 70/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.0776e-01 (1.9069e-01)	Acc@1  96.09 ( 95.00)	Acc@5  98.44 ( 99.81)
Epoch: [31][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.8176e-01 (1.9443e-01)	Acc@1  96.09 ( 94.81)	Acc@5 100.00 ( 99.80)
Epoch: [31][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.2498e-01 (1.9350e-01)	Acc@1  92.97 ( 94.81)	Acc@5 100.00 ( 99.82)
Epoch: [31][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4807e-01 (1.9248e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.81)
Epoch: [31][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6760e-01 (1.9024e-01)	Acc@1  94.53 ( 94.86)	Acc@5 100.00 ( 99.81)
Epoch: [31][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.7749e-01 (1.9090e-01)	Acc@1  95.31 ( 94.89)	Acc@5  99.22 ( 99.78)
Epoch: [31][130/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0532e-01 (1.9172e-01)	Acc@1  96.09 ( 94.89)	Acc@5 100.00 ( 99.79)
Epoch: [31][140/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6174e-01 (1.8965e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.79)
Epoch: [31][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.2546e-01 (1.9021e-01)	Acc@1  92.19 ( 94.86)	Acc@5  99.22 ( 99.79)
Epoch: [31][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8188e-01 (1.9030e-01)	Acc@1  96.88 ( 94.85)	Acc@5 100.00 ( 99.79)
Epoch: [31][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1106e-01 (1.8969e-01)	Acc@1  94.53 ( 94.93)	Acc@5  99.22 ( 99.78)
Epoch: [31][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6272e-01 (1.8763e-01)	Acc@1  96.09 ( 95.01)	Acc@5 100.00 ( 99.78)
Epoch: [31][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8396e-01 (1.8822e-01)	Acc@1  94.53 ( 94.99)	Acc@5 100.00 ( 99.78)
Epoch: [31][200/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9141e-01 (1.8700e-01)	Acc@1  96.09 ( 95.06)	Acc@5 100.00 ( 99.79)
Epoch: [31][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2708e-01 (1.8700e-01)	Acc@1  97.66 ( 95.05)	Acc@5 100.00 ( 99.79)
Epoch: [31][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6016e-01 (1.8719e-01)	Acc@1  96.88 ( 95.06)	Acc@5 100.00 ( 99.79)
Epoch: [31][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4976e-01 (1.8640e-01)	Acc@1  93.75 ( 95.10)	Acc@5  99.22 ( 99.80)
Epoch: [31][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1680e-01 (1.8596e-01)	Acc@1  93.75 ( 95.12)	Acc@5  99.22 ( 99.80)
Epoch: [31][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4514e-01 (1.8619e-01)	Acc@1  96.88 ( 95.12)	Acc@5  99.22 ( 99.80)
Epoch: [31][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3867e-01 (1.8591e-01)	Acc@1  98.44 ( 95.14)	Acc@5  99.22 ( 99.80)
Epoch: [31][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1594e-01 (1.8606e-01)	Acc@1  94.53 ( 95.13)	Acc@5 100.00 ( 99.80)
Epoch: [31][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0176e-01 (1.8605e-01)	Acc@1  90.62 ( 95.14)	Acc@5 100.00 ( 99.80)
Epoch: [31][290/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.2180e-01 (1.8651e-01)	Acc@1  93.75 ( 95.12)	Acc@5 100.00 ( 99.80)
Epoch: [31][300/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4868e-01 (1.8602e-01)	Acc@1  97.66 ( 95.13)	Acc@5 100.00 ( 99.81)
Epoch: [31][310/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8591e-01 (1.8547e-01)	Acc@1  93.75 ( 95.14)	Acc@5 100.00 ( 99.81)
Epoch: [31][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6272e-01 (1.8456e-01)	Acc@1  96.88 ( 95.19)	Acc@5  99.22 ( 99.81)
Epoch: [31][330/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.0325e-01 (1.8422e-01)	Acc@1  96.09 ( 95.20)	Acc@5  99.22 ( 99.81)
Epoch: [31][340/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.1985e-01 (1.8444e-01)	Acc@1  94.53 ( 95.18)	Acc@5 100.00 ( 99.81)
Epoch: [31][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.5149e-01 (1.8367e-01)	Acc@1  96.88 ( 95.21)	Acc@5  99.22 ( 99.80)
Epoch: [31][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.9800e-01 (1.8331e-01)	Acc@1  93.75 ( 95.23)	Acc@5 100.00 ( 99.81)
Epoch: [31][370/391]	Time  0.135 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.6895e-01 (1.8339e-01)	Acc@1  95.31 ( 95.21)	Acc@5 100.00 ( 99.81)
Epoch: [31][380/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.9116e-01 (1.8319e-01)	Acc@1  94.53 ( 95.23)	Acc@5 100.00 ( 99.81)
Epoch: [31][390/391]	Time  0.106 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.3203e-01 (1.8335e-01)	Acc@1  88.75 ( 95.21)	Acc@5  98.75 ( 99.81)
## e[31] optimizer.zero_grad (sum) time: 0.6848659515380859
## e[31]       loss.backward (sum) time: 14.270034551620483
## e[31]      optimizer.step (sum) time: 8.150044918060303
## epoch[31] training(only) time: 47.38299059867859
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 9.0430e-01 (9.0430e-01)	Acc@1  73.00 ( 73.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.0205e+00 (1.1069e+00)	Acc@1  76.00 ( 73.45)	Acc@5  93.00 ( 91.73)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 9.4482e-01 (1.0752e+00)	Acc@1  77.00 ( 73.86)	Acc@5  92.00 ( 92.14)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.4082e+00 (1.0874e+00)	Acc@1  71.00 ( 73.48)	Acc@5  92.00 ( 92.23)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.1396e+00 (1.0849e+00)	Acc@1  74.00 ( 73.34)	Acc@5  94.00 ( 92.46)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2734e+00 (1.0923e+00)	Acc@1  67.00 ( 72.94)	Acc@5  94.00 ( 92.43)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0273e+00 (1.0692e+00)	Acc@1  72.00 ( 72.95)	Acc@5  95.00 ( 92.70)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3633e+00 (1.0757e+00)	Acc@1  69.00 ( 72.87)	Acc@5  90.00 ( 92.70)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3633e+00 (1.0825e+00)	Acc@1  74.00 ( 72.90)	Acc@5  89.00 ( 92.58)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.3691e+00 (1.0701e+00)	Acc@1  67.00 ( 72.99)	Acc@5  90.00 ( 92.78)
 * Acc@1 73.150 Acc@5 92.820
### epoch[31] execution time: 52.3817572593689
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.296 ( 0.296)	Data  0.162 ( 0.162)	Loss 1.6907e-01 (1.6907e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.119 ( 0.137)	Data  0.001 ( 0.015)	Loss 1.5027e-01 (1.4582e-01)	Acc@1  96.88 ( 96.66)	Acc@5  99.22 ( 99.86)
Epoch: [32][ 20/391]	Time  0.119 ( 0.129)	Data  0.001 ( 0.009)	Loss 1.2781e-01 (1.4675e-01)	Acc@1  97.66 ( 96.61)	Acc@5 100.00 ( 99.85)
Epoch: [32][ 30/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.006)	Loss 1.7883e-01 (1.5293e-01)	Acc@1  95.31 ( 96.24)	Acc@5 100.00 ( 99.87)
Epoch: [32][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 1.1584e-01 (1.5477e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 50/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.3733e-01 (1.5597e-01)	Acc@1  96.09 ( 95.97)	Acc@5 100.00 ( 99.88)
Epoch: [32][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.1346e-01 (1.5573e-01)	Acc@1  97.66 ( 95.93)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.4661e-01 (1.5416e-01)	Acc@1  96.09 ( 96.01)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 80/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.1667e-01 (1.5452e-01)	Acc@1  93.75 ( 96.03)	Acc@5  99.22 ( 99.89)
Epoch: [32][ 90/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.5869e-01 (1.5516e-01)	Acc@1  96.09 ( 96.02)	Acc@5 100.00 ( 99.88)
Epoch: [32][100/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.4551e-01 (1.5407e-01)	Acc@1  96.88 ( 96.07)	Acc@5  99.22 ( 99.88)
Epoch: [32][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5576e-01 (1.5332e-01)	Acc@1  95.31 ( 96.11)	Acc@5 100.00 ( 99.89)
Epoch: [32][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0398e-01 (1.5274e-01)	Acc@1  93.75 ( 96.15)	Acc@5 100.00 ( 99.90)
Epoch: [32][130/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3928e-01 (1.5221e-01)	Acc@1  94.53 ( 96.18)	Acc@5 100.00 ( 99.90)
Epoch: [32][140/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2244e-01 (1.5144e-01)	Acc@1  97.66 ( 96.24)	Acc@5 100.00 ( 99.90)
Epoch: [32][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1676e-01 (1.5242e-01)	Acc@1  96.09 ( 96.20)	Acc@5 100.00 ( 99.89)
Epoch: [32][160/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0959e-01 (1.5274e-01)	Acc@1  95.31 ( 96.19)	Acc@5  99.22 ( 99.88)
Epoch: [32][170/391]	Time  0.132 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4587e-01 (1.5325e-01)	Acc@1  97.66 ( 96.18)	Acc@5 100.00 ( 99.89)
Epoch: [32][180/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1945e-01 (1.5307e-01)	Acc@1  98.44 ( 96.19)	Acc@5 100.00 ( 99.88)
Epoch: [32][190/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0264e-01 (1.5337e-01)	Acc@1  95.31 ( 96.19)	Acc@5 100.00 ( 99.88)
Epoch: [32][200/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0364e-01 (1.5304e-01)	Acc@1  99.22 ( 96.21)	Acc@5 100.00 ( 99.88)
Epoch: [32][210/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.8018e-01 (1.5259e-01)	Acc@1  94.53 ( 96.22)	Acc@5 100.00 ( 99.89)
Epoch: [32][220/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6321e-01 (1.5284e-01)	Acc@1  96.09 ( 96.21)	Acc@5  99.22 ( 99.88)
Epoch: [32][230/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1969e-01 (1.5281e-01)	Acc@1  98.44 ( 96.22)	Acc@5 100.00 ( 99.89)
Epoch: [32][240/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1670e-01 (1.5250e-01)	Acc@1  98.44 ( 96.23)	Acc@5 100.00 ( 99.89)
Epoch: [32][250/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4331e-01 (1.5248e-01)	Acc@1  95.31 ( 96.23)	Acc@5 100.00 ( 99.90)
Epoch: [32][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8994e-01 (1.5262e-01)	Acc@1  94.53 ( 96.23)	Acc@5 100.00 ( 99.90)
Epoch: [32][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2915e-01 (1.5295e-01)	Acc@1  96.09 ( 96.21)	Acc@5 100.00 ( 99.90)
Epoch: [32][280/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0590e-01 (1.5310e-01)	Acc@1  98.44 ( 96.22)	Acc@5 100.00 ( 99.89)
Epoch: [32][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6272e-01 (1.5290e-01)	Acc@1  96.09 ( 96.22)	Acc@5 100.00 ( 99.89)
Epoch: [32][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1847e-01 (1.5256e-01)	Acc@1  97.66 ( 96.23)	Acc@5 100.00 ( 99.89)
Epoch: [32][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.4097e-01 (1.5286e-01)	Acc@1  92.19 ( 96.22)	Acc@5  99.22 ( 99.89)
Epoch: [32][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0431e-01 (1.5336e-01)	Acc@1  97.66 ( 96.19)	Acc@5 100.00 ( 99.89)
Epoch: [32][330/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.9897e-01 (1.5353e-01)	Acc@1  94.53 ( 96.20)	Acc@5  99.22 ( 99.88)
Epoch: [32][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2744e-01 (1.5393e-01)	Acc@1  96.88 ( 96.19)	Acc@5 100.00 ( 99.88)
Epoch: [32][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.6077e-01 (1.5326e-01)	Acc@1  96.09 ( 96.21)	Acc@5 100.00 ( 99.88)
Epoch: [32][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3428e-01 (1.5358e-01)	Acc@1  98.44 ( 96.19)	Acc@5 100.00 ( 99.88)
Epoch: [32][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.7708e-02 (1.5374e-01)	Acc@1  99.22 ( 96.20)	Acc@5 100.00 ( 99.88)
Epoch: [32][380/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0626e-01 (1.5361e-01)	Acc@1  97.66 ( 96.20)	Acc@5 100.00 ( 99.88)
Epoch: [32][390/391]	Time  0.107 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.0933e-01 (1.5370e-01)	Acc@1  90.00 ( 96.20)	Acc@5  98.75 ( 99.88)
## e[32] optimizer.zero_grad (sum) time: 0.6748440265655518
## e[32]       loss.backward (sum) time: 14.369019746780396
## e[32]      optimizer.step (sum) time: 8.118385791778564
## epoch[32] training(only) time: 47.501179695129395
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 8.7061e-01 (8.7061e-01)	Acc@1  74.00 ( 74.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.0293e+00 (1.1200e+00)	Acc@1  75.00 ( 72.91)	Acc@5  93.00 ( 91.73)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0225e+00 (1.0825e+00)	Acc@1  74.00 ( 73.48)	Acc@5  92.00 ( 92.33)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.3936e+00 (1.0905e+00)	Acc@1  69.00 ( 73.26)	Acc@5  91.00 ( 92.29)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.0811e+00 (1.0854e+00)	Acc@1  76.00 ( 73.46)	Acc@5  93.00 ( 92.54)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.3096e+00 (1.0926e+00)	Acc@1  67.00 ( 73.12)	Acc@5  94.00 ( 92.57)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0137e+00 (1.0685e+00)	Acc@1  74.00 ( 73.15)	Acc@5  96.00 ( 92.97)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.3535e+00 (1.0733e+00)	Acc@1  68.00 ( 73.14)	Acc@5  89.00 ( 92.96)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3857e+00 (1.0788e+00)	Acc@1  75.00 ( 73.06)	Acc@5  88.00 ( 92.84)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3711e+00 (1.0669e+00)	Acc@1  65.00 ( 73.15)	Acc@5  91.00 ( 93.01)
 * Acc@1 73.390 Acc@5 93.020
### epoch[32] execution time: 52.51571202278137
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.293 ( 0.293)	Data  0.148 ( 0.148)	Loss 8.6487e-02 (8.6487e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.120 ( 0.136)	Data  0.001 ( 0.014)	Loss 1.8433e-01 (1.3206e-01)	Acc@1  95.31 ( 97.37)	Acc@5  99.22 ( 99.93)
Epoch: [33][ 20/391]	Time  0.123 ( 0.129)	Data  0.001 ( 0.008)	Loss 1.7041e-01 (1.3456e-01)	Acc@1  96.09 ( 97.21)	Acc@5 100.00 ( 99.96)
Epoch: [33][ 30/391]	Time  0.126 ( 0.126)	Data  0.001 ( 0.006)	Loss 1.3745e-01 (1.3824e-01)	Acc@1  96.09 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 40/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.005)	Loss 2.1729e-01 (1.3725e-01)	Acc@1  93.75 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [33][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.3318e-01 (1.4008e-01)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.8030e-01 (1.3962e-01)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 70/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.7566e-01 (1.3999e-01)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.98)
Epoch: [33][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3599e-01 (1.3978e-01)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.96)
Epoch: [33][ 90/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.7627e-01 (1.3986e-01)	Acc@1  93.75 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [33][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4673e-01 (1.3858e-01)	Acc@1  95.31 ( 96.77)	Acc@5  99.22 ( 99.96)
Epoch: [33][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3696e-01 (1.3778e-01)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.96)
Epoch: [33][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4819e-01 (1.3859e-01)	Acc@1  97.66 ( 96.80)	Acc@5  99.22 ( 99.95)
Epoch: [33][130/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6882e-01 (1.3970e-01)	Acc@1  93.75 ( 96.77)	Acc@5 100.00 ( 99.95)
Epoch: [33][140/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5735e-01 (1.4005e-01)	Acc@1  95.31 ( 96.75)	Acc@5 100.00 ( 99.94)
Epoch: [33][150/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5210e-01 (1.3886e-01)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [33][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6187e-01 (1.3884e-01)	Acc@1  96.09 ( 96.79)	Acc@5 100.00 ( 99.95)
Epoch: [33][170/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0291e-01 (1.3842e-01)	Acc@1  99.22 ( 96.80)	Acc@5 100.00 ( 99.95)
Epoch: [33][180/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4172e-01 (1.3841e-01)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.95)
Epoch: [33][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4368e-01 (1.3874e-01)	Acc@1  94.53 ( 96.78)	Acc@5 100.00 ( 99.95)
Epoch: [33][200/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4343e-01 (1.3889e-01)	Acc@1  96.09 ( 96.77)	Acc@5 100.00 ( 99.94)
Epoch: [33][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8970e-01 (1.3975e-01)	Acc@1  92.97 ( 96.70)	Acc@5  99.22 ( 99.94)
Epoch: [33][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5125e-01 (1.3969e-01)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.93)
Epoch: [33][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6272e-01 (1.3989e-01)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.93)
Epoch: [33][240/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2219e-01 (1.3993e-01)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.93)
Epoch: [33][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5674e-01 (1.4037e-01)	Acc@1  95.31 ( 96.69)	Acc@5 100.00 ( 99.93)
Epoch: [33][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2006e-01 (1.4094e-01)	Acc@1  98.44 ( 96.65)	Acc@5 100.00 ( 99.92)
Epoch: [33][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.4057e-01)	Acc@1  95.31 ( 96.65)	Acc@5 100.00 ( 99.92)
Epoch: [33][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2683e-01 (1.4055e-01)	Acc@1  97.66 ( 96.64)	Acc@5 100.00 ( 99.92)
Epoch: [33][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0637e-02 (1.4025e-01)	Acc@1  99.22 ( 96.64)	Acc@5 100.00 ( 99.92)
Epoch: [33][300/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5894e-01 (1.3998e-01)	Acc@1  96.09 ( 96.66)	Acc@5 100.00 ( 99.92)
Epoch: [33][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.5576e-01 (1.3976e-01)	Acc@1  96.09 ( 96.68)	Acc@5 100.00 ( 99.92)
Epoch: [33][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.5784e-01 (1.3988e-01)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.92)
Epoch: [33][330/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1627e-01 (1.4003e-01)	Acc@1  98.44 ( 96.65)	Acc@5 100.00 ( 99.92)
Epoch: [33][340/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.1643e-01 (1.4061e-01)	Acc@1  92.97 ( 96.63)	Acc@5  99.22 ( 99.91)
Epoch: [33][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2231e-01 (1.4038e-01)	Acc@1  96.09 ( 96.63)	Acc@5 100.00 ( 99.92)
Epoch: [33][360/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0242e-01 (1.4011e-01)	Acc@1  97.66 ( 96.63)	Acc@5 100.00 ( 99.92)
Epoch: [33][370/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.2896e-02 (1.3974e-01)	Acc@1 100.00 ( 96.67)	Acc@5 100.00 ( 99.92)
Epoch: [33][380/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2091e-01 (1.3945e-01)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.92)
Epoch: [33][390/391]	Time  0.103 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.7102e-01 (1.3955e-01)	Acc@1  96.25 ( 96.67)	Acc@5 100.00 ( 99.91)
## e[33] optimizer.zero_grad (sum) time: 0.6817302703857422
## e[33]       loss.backward (sum) time: 14.309134483337402
## e[33]      optimizer.step (sum) time: 8.125500917434692
## epoch[33] training(only) time: 47.407918214797974
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 8.4521e-01 (8.4521e-01)	Acc@1  74.00 ( 74.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.0684e+00 (1.1109e+00)	Acc@1  75.00 ( 73.82)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0312e+00 (1.0728e+00)	Acc@1  72.00 ( 73.81)	Acc@5  91.00 ( 92.76)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 1.4531e+00 (1.0939e+00)	Acc@1  70.00 ( 73.71)	Acc@5  90.00 ( 92.48)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.1865e+00 (1.0928e+00)	Acc@1  75.00 ( 73.88)	Acc@5  93.00 ( 92.56)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.3242e+00 (1.0994e+00)	Acc@1  67.00 ( 73.37)	Acc@5  93.00 ( 92.43)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0479e+00 (1.0753e+00)	Acc@1  71.00 ( 73.16)	Acc@5  97.00 ( 92.79)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3750e+00 (1.0779e+00)	Acc@1  68.00 ( 73.15)	Acc@5  89.00 ( 92.87)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4336e+00 (1.0841e+00)	Acc@1  73.00 ( 73.16)	Acc@5  89.00 ( 92.72)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3828e+00 (1.0720e+00)	Acc@1  65.00 ( 73.21)	Acc@5  91.00 ( 92.89)
 * Acc@1 73.340 Acc@5 92.940
### epoch[33] execution time: 52.4085009098053
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.285 ( 0.285)	Data  0.151 ( 0.151)	Loss 1.3452e-01 (1.3452e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.118 ( 0.136)	Data  0.001 ( 0.015)	Loss 1.0071e-01 (1.2202e-01)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [34][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 1.4355e-01 (1.2419e-01)	Acc@1  97.66 ( 97.28)	Acc@5  99.22 ( 99.96)
Epoch: [34][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 9.9487e-02 (1.2338e-01)	Acc@1  99.22 ( 97.25)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 8.5632e-02 (1.2550e-01)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 ( 99.90)
Epoch: [34][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.0016e-01 (1.2970e-01)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.89)
Epoch: [34][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.0632e-01 (1.3093e-01)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.91)
Epoch: [34][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0907e-01 (1.2928e-01)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.91)
Epoch: [34][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.6504e-01 (1.2896e-01)	Acc@1  97.66 ( 97.09)	Acc@5  99.22 ( 99.91)
Epoch: [34][ 90/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.4746e-01 (1.2799e-01)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.92)
Epoch: [34][100/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3123e-01 (1.2875e-01)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.91)
Epoch: [34][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3049e-01 (1.2933e-01)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.92)
Epoch: [34][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2651e-02 (1.2779e-01)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.93)
Epoch: [34][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6675e-01 (1.2807e-01)	Acc@1  95.31 ( 97.07)	Acc@5 100.00 ( 99.93)
Epoch: [34][140/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3293e-01 (1.2714e-01)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.94)
Epoch: [34][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1487e-01 (1.2686e-01)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.94)
Epoch: [34][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.6650e-01 (1.2733e-01)	Acc@1  95.31 ( 97.08)	Acc@5 100.00 ( 99.94)
Epoch: [34][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4441e-01 (1.2797e-01)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.93)
Epoch: [34][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3330e-01 (1.2821e-01)	Acc@1  96.09 ( 97.04)	Acc@5 100.00 ( 99.93)
Epoch: [34][190/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8369e-02 (1.2783e-01)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.93)
Epoch: [34][200/391]	Time  0.134 ( 0.121)	Data  0.003 ( 0.002)	Loss 2.0422e-01 (1.2777e-01)	Acc@1  93.75 ( 97.09)	Acc@5 100.00 ( 99.93)
Epoch: [34][210/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5459e-02 (1.2845e-01)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.93)
Epoch: [34][220/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2683e-01 (1.2863e-01)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.92)
Epoch: [34][230/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8674e-02 (1.2785e-01)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.92)
Epoch: [34][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.7993e-01 (1.2782e-01)	Acc@1  94.53 ( 97.10)	Acc@5 100.00 ( 99.92)
Epoch: [34][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3477e-01 (1.2784e-01)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.92)
Epoch: [34][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.1003e-02 (1.2727e-01)	Acc@1  99.22 ( 97.15)	Acc@5  99.22 ( 99.92)
Epoch: [34][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3147e-01 (1.2697e-01)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.92)
Epoch: [34][280/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2213e-01 (1.2678e-01)	Acc@1  96.09 ( 97.15)	Acc@5  99.22 ( 99.92)
Epoch: [34][290/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5833e-01 (1.2653e-01)	Acc@1  95.31 ( 97.15)	Acc@5 100.00 ( 99.92)
Epoch: [34][300/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0663e-01 (1.2588e-01)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.92)
Epoch: [34][310/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4001e-01 (1.2522e-01)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.91)
Epoch: [34][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3501e-01 (1.2555e-01)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.91)
Epoch: [34][330/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2683e-01 (1.2549e-01)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.92)
Epoch: [34][340/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.5215e-02 (1.2533e-01)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.92)
Epoch: [34][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.8999e-02 (1.2493e-01)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.92)
Epoch: [34][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.8267e-02 (1.2482e-01)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.92)
Epoch: [34][370/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0028e-01 (1.2467e-01)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.92)
Epoch: [34][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4954e-01 (1.2446e-01)	Acc@1  94.53 ( 97.20)	Acc@5 100.00 ( 99.92)
Epoch: [34][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.5808e-01 (1.2458e-01)	Acc@1  95.00 ( 97.19)	Acc@5 100.00 ( 99.92)
## e[34] optimizer.zero_grad (sum) time: 0.6852383613586426
## e[34]       loss.backward (sum) time: 14.280156135559082
## e[34]      optimizer.step (sum) time: 8.06521487236023
## epoch[34] training(only) time: 47.296525955200195
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 8.5547e-01 (8.5547e-01)	Acc@1  75.00 ( 75.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.0410e+00 (1.1071e+00)	Acc@1  77.00 ( 73.82)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.0127e+00 (1.0659e+00)	Acc@1  75.00 ( 74.00)	Acc@5  92.00 ( 92.90)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 1.4834e+00 (1.0833e+00)	Acc@1  69.00 ( 73.61)	Acc@5  91.00 ( 92.61)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 1.1689e+00 (1.0829e+00)	Acc@1  74.00 ( 73.54)	Acc@5  93.00 ( 92.78)
Test: [ 50/100]	Time  0.051 ( 0.051)	Loss 1.2783e+00 (1.0887e+00)	Acc@1  68.00 ( 73.25)	Acc@5  91.00 ( 92.67)
Test: [ 60/100]	Time  0.050 ( 0.050)	Loss 1.0342e+00 (1.0661e+00)	Acc@1  73.00 ( 73.31)	Acc@5  95.00 ( 92.97)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3711e+00 (1.0699e+00)	Acc@1  66.00 ( 73.23)	Acc@5  89.00 ( 92.97)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4170e+00 (1.0758e+00)	Acc@1  75.00 ( 73.25)	Acc@5  89.00 ( 92.86)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3906e+00 (1.0639e+00)	Acc@1  66.00 ( 73.29)	Acc@5  91.00 ( 92.99)
 * Acc@1 73.510 Acc@5 93.020
### epoch[34] execution time: 52.30622744560242
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.295 ( 0.295)	Data  0.161 ( 0.161)	Loss 1.5112e-01 (1.5112e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.120 ( 0.137)	Data  0.001 ( 0.015)	Loss 1.2274e-01 (1.1093e-01)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.009)	Loss 9.1736e-02 (1.1303e-01)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [35][ 30/391]	Time  0.124 ( 0.126)	Data  0.001 ( 0.006)	Loss 9.4299e-02 (1.1518e-01)	Acc@1 100.00 ( 97.78)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 8.0994e-02 (1.1384e-01)	Acc@1  99.22 ( 97.71)	Acc@5 100.00 ( 99.96)
Epoch: [35][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 9.8877e-02 (1.1258e-01)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 60/391]	Time  0.127 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.0779e-01 (1.1474e-01)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.5271e-01 (1.1579e-01)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.2134e-01 (1.1508e-01)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 90/391]	Time  0.126 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.8921e-01 (1.1767e-01)	Acc@1  95.31 ( 97.27)	Acc@5  99.22 ( 99.96)
Epoch: [35][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3733e-01 (1.1858e-01)	Acc@1  93.75 ( 97.17)	Acc@5 100.00 ( 99.95)
Epoch: [35][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.9844e-02 (1.1922e-01)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.96)
Epoch: [35][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.8450e-02 (1.1999e-01)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.95)
Epoch: [35][130/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.3384e-02 (1.1893e-01)	Acc@1  97.66 ( 97.18)	Acc@5 100.00 ( 99.96)
Epoch: [35][140/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6223e-01 (1.1943e-01)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.96)
Epoch: [35][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1060e-01 (1.1927e-01)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.96)
Epoch: [35][160/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0468e-01 (1.1903e-01)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.96)
Epoch: [35][170/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0065e-01 (1.1906e-01)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.96)
Epoch: [35][180/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.4685e-01 (1.2001e-01)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.96)
Epoch: [35][190/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.0496e-02 (1.1975e-01)	Acc@1  98.44 ( 97.22)	Acc@5 100.00 ( 99.96)
Epoch: [35][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0181e-01 (1.1968e-01)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.95)
Epoch: [35][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0468e-01 (1.1972e-01)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.94)
Epoch: [35][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1777e-02 (1.1902e-01)	Acc@1  99.22 ( 97.27)	Acc@5 100.00 ( 99.95)
Epoch: [35][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4246e-01 (1.1884e-01)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.95)
Epoch: [35][240/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.7717e-02 (1.1883e-01)	Acc@1  98.44 ( 97.27)	Acc@5 100.00 ( 99.95)
Epoch: [35][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.0322e-02 (1.1773e-01)	Acc@1  99.22 ( 97.31)	Acc@5 100.00 ( 99.95)
Epoch: [35][260/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0071e-01 (1.1804e-01)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [35][270/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.8042e-01 (1.1823e-01)	Acc@1  95.31 ( 97.31)	Acc@5 100.00 ( 99.95)
Epoch: [35][280/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3354e-01 (1.1843e-01)	Acc@1  95.31 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [35][290/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3733e-01 (1.1839e-01)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [35][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0425e-01 (1.1791e-01)	Acc@1 100.00 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [35][310/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0223e-01 (1.1857e-01)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [35][320/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0138e-01 (1.1837e-01)	Acc@1  99.22 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [35][330/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2598e-01 (1.1845e-01)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [35][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.8872e-01 (1.1868e-01)	Acc@1  95.31 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [35][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.6663e-01 (1.1879e-01)	Acc@1  96.09 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [35][360/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0437e-01 (1.1839e-01)	Acc@1  99.22 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [35][370/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1633e-01 (1.1861e-01)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [35][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3379e-01 (1.1893e-01)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.94)
Epoch: [35][390/391]	Time  0.111 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.5283e-01 (1.1882e-01)	Acc@1  96.25 ( 97.32)	Acc@5 100.00 ( 99.94)
## e[35] optimizer.zero_grad (sum) time: 0.6827867031097412
## e[35]       loss.backward (sum) time: 14.319433689117432
## e[35]      optimizer.step (sum) time: 8.124626398086548
## epoch[35] training(only) time: 47.44955110549927
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 8.2568e-01 (8.2568e-01)	Acc@1  76.00 ( 76.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.057 ( 0.062)	Loss 1.0498e+00 (1.1088e+00)	Acc@1  75.00 ( 73.55)	Acc@5  92.00 ( 92.55)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0371e+00 (1.0686e+00)	Acc@1  73.00 ( 73.71)	Acc@5  93.00 ( 93.05)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5098e+00 (1.0842e+00)	Acc@1  67.00 ( 73.61)	Acc@5  91.00 ( 92.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.1465e+00 (1.0878e+00)	Acc@1  76.00 ( 73.63)	Acc@5  95.00 ( 92.88)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.3203e+00 (1.0941e+00)	Acc@1  68.00 ( 73.37)	Acc@5  93.00 ( 92.78)
Test: [ 60/100]	Time  0.054 ( 0.050)	Loss 1.0449e+00 (1.0693e+00)	Acc@1  76.00 ( 73.48)	Acc@5  96.00 ( 93.15)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3594e+00 (1.0727e+00)	Acc@1  67.00 ( 73.34)	Acc@5  90.00 ( 93.14)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4072e+00 (1.0779e+00)	Acc@1  77.00 ( 73.43)	Acc@5  90.00 ( 92.99)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3721e+00 (1.0668e+00)	Acc@1  67.00 ( 73.49)	Acc@5  93.00 ( 93.09)
 * Acc@1 73.630 Acc@5 93.130
### epoch[35] execution time: 52.44035458564758
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.286 ( 0.286)	Data  0.156 ( 0.156)	Loss 1.3025e-01 (1.3025e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.118 ( 0.137)	Data  0.001 ( 0.015)	Loss 8.9783e-02 (9.9182e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.008)	Loss 6.5857e-02 (1.0304e-01)	Acc@1 100.00 ( 98.07)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 1.2012e-01 (1.0801e-01)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 40/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.005)	Loss 1.2756e-01 (1.0958e-01)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 9.1492e-02 (1.0762e-01)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.97)
Epoch: [36][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 1.0559e-01 (1.0997e-01)	Acc@1  99.22 ( 97.76)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 70/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.0144e-01 (1.0996e-01)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.1545e-01 (1.1065e-01)	Acc@1  94.53 ( 97.78)	Acc@5  99.22 ( 99.94)
Epoch: [36][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0748e-01 (1.1059e-01)	Acc@1  96.88 ( 97.75)	Acc@5 100.00 ( 99.94)
Epoch: [36][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.9055e-01 (1.1134e-01)	Acc@1  94.53 ( 97.69)	Acc@5  99.22 ( 99.94)
Epoch: [36][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.0820e-02 (1.1051e-01)	Acc@1  98.44 ( 97.70)	Acc@5 100.00 ( 99.94)
Epoch: [36][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0956e-01 (1.1072e-01)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [36][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.0271e-02 (1.1164e-01)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.94)
Epoch: [36][140/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0382e-01 (1.1181e-01)	Acc@1  96.09 ( 97.60)	Acc@5 100.00 ( 99.94)
Epoch: [36][150/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.5093e-02 (1.1154e-01)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [36][160/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1945e-01 (1.1086e-01)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.95)
Epoch: [36][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0327e-01 (1.1110e-01)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.95)
Epoch: [36][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3618e-02 (1.1119e-01)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.94)
Epoch: [36][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0187e-01 (1.1125e-01)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [36][200/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2939e-01 (1.1069e-01)	Acc@1  96.88 ( 97.67)	Acc@5 100.00 ( 99.95)
Epoch: [36][210/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0266e-01 (1.1080e-01)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [36][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2415e-01 (1.1097e-01)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [36][230/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.3628e-02 (1.1135e-01)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.95)
Epoch: [36][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1438e-01 (1.1126e-01)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.95)
Epoch: [36][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.3323e-02 (1.1122e-01)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [36][260/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.8877e-02 (1.1091e-01)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.96)
Epoch: [36][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6406e-02 (1.1103e-01)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [36][280/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0820e-02 (1.1119e-01)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.96)
Epoch: [36][290/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3647e-01 (1.1071e-01)	Acc@1  96.88 ( 97.63)	Acc@5 100.00 ( 99.96)
Epoch: [36][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3611e-01 (1.1039e-01)	Acc@1  96.09 ( 97.65)	Acc@5 100.00 ( 99.96)
Epoch: [36][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1072e-01 (1.1031e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [36][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.8145e-02 (1.1051e-01)	Acc@1 100.00 ( 97.65)	Acc@5 100.00 ( 99.96)
Epoch: [36][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4575e-01 (1.1033e-01)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [36][340/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3867e-01 (1.1040e-01)	Acc@1  96.09 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [36][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4709e-01 (1.1037e-01)	Acc@1  96.88 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [36][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.3059e-02 (1.1020e-01)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [36][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.9519e-02 (1.0993e-01)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [36][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.0088e-02 (1.0996e-01)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [36][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.1980e-02 (1.0991e-01)	Acc@1 100.00 ( 97.65)	Acc@5 100.00 ( 99.95)
## e[36] optimizer.zero_grad (sum) time: 0.6837527751922607
## e[36]       loss.backward (sum) time: 14.296777486801147
## e[36]      optimizer.step (sum) time: 8.141916275024414
## epoch[36] training(only) time: 47.35175895690918
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 8.3301e-01 (8.3301e-01)	Acc@1  75.00 ( 75.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.050 ( 0.062)	Loss 1.0391e+00 (1.1047e+00)	Acc@1  75.00 ( 73.73)	Acc@5  92.00 ( 92.36)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0332e+00 (1.0649e+00)	Acc@1  74.00 ( 73.86)	Acc@5  92.00 ( 92.86)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 1.4727e+00 (1.0901e+00)	Acc@1  67.00 ( 73.61)	Acc@5  90.00 ( 92.61)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.1729e+00 (1.0931e+00)	Acc@1  74.00 ( 73.76)	Acc@5  94.00 ( 92.93)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2988e+00 (1.0985e+00)	Acc@1  69.00 ( 73.49)	Acc@5  93.00 ( 92.86)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0420e+00 (1.0737e+00)	Acc@1  73.00 ( 73.39)	Acc@5  96.00 ( 93.28)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3662e+00 (1.0768e+00)	Acc@1  68.00 ( 73.35)	Acc@5  90.00 ( 93.32)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.4150e+00 (1.0817e+00)	Acc@1  76.00 ( 73.42)	Acc@5  89.00 ( 93.21)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.4209e+00 (1.0713e+00)	Acc@1  64.00 ( 73.40)	Acc@5  92.00 ( 93.30)
 * Acc@1 73.600 Acc@5 93.300
### epoch[36] execution time: 52.38192939758301
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.301 ( 0.301)	Data  0.161 ( 0.161)	Loss 1.3745e-01 (1.3745e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.119 ( 0.137)	Data  0.001 ( 0.015)	Loss 9.2285e-02 (1.0177e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.009)	Loss 8.6670e-02 (9.8947e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.006)	Loss 1.2524e-01 (1.0154e-01)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 9.6436e-02 (1.0070e-01)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 50/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 8.6670e-02 (9.9491e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 60/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.0791e-01 (1.0293e-01)	Acc@1  96.88 ( 97.98)	Acc@5 100.00 ( 99.95)
Epoch: [37][ 70/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.3159e-01 (1.0438e-01)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.2021e-02 (1.0437e-01)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 ( 99.95)
Epoch: [37][ 90/391]	Time  0.127 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.1713e-01 (1.0647e-01)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 ( 99.96)
Epoch: [37][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.7271e-02 (1.0584e-01)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.96)
Epoch: [37][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.6313e-02 (1.0418e-01)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.96)
Epoch: [37][120/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3257e-01 (1.0359e-01)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 ( 99.96)
Epoch: [37][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.5369e-02 (1.0316e-01)	Acc@1  99.22 ( 97.93)	Acc@5 100.00 ( 99.96)
Epoch: [37][140/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1462e-01 (1.0385e-01)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 ( 99.96)
Epoch: [37][150/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0486e-01 (1.0414e-01)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.95)
Epoch: [37][160/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3489e-01 (1.0505e-01)	Acc@1  96.88 ( 97.86)	Acc@5 100.00 ( 99.96)
Epoch: [37][170/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.2286e-01 (1.0453e-01)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.95)
Epoch: [37][180/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.8511e-02 (1.0509e-01)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.95)
Epoch: [37][190/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.4849e-02 (1.0468e-01)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.95)
Epoch: [37][200/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.5442e-01 (1.0529e-01)	Acc@1  95.31 ( 97.87)	Acc@5 100.00 ( 99.95)
Epoch: [37][210/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.6187e-01 (1.0559e-01)	Acc@1  96.09 ( 97.85)	Acc@5 100.00 ( 99.95)
Epoch: [37][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0907e-01 (1.0589e-01)	Acc@1  97.66 ( 97.84)	Acc@5 100.00 ( 99.95)
Epoch: [37][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2275e-02 (1.0559e-01)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.95)
Epoch: [37][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9529e-02 (1.0533e-01)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.95)
Epoch: [37][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.5625e-01 (1.0557e-01)	Acc@1  95.31 ( 97.87)	Acc@5 100.00 ( 99.95)
Epoch: [37][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1462e-01 (1.0551e-01)	Acc@1  96.88 ( 97.85)	Acc@5  99.22 ( 99.95)
Epoch: [37][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1798e-01 (1.0543e-01)	Acc@1  97.66 ( 97.86)	Acc@5  99.22 ( 99.95)
Epoch: [37][280/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2144e-02 (1.0547e-01)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 ( 99.95)
Epoch: [37][290/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0028e-01 (1.0531e-01)	Acc@1  97.66 ( 97.84)	Acc@5 100.00 ( 99.95)
Epoch: [37][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7332e-02 (1.0500e-01)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.95)
Epoch: [37][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3184e-01 (1.0478e-01)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.95)
Epoch: [37][320/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0608e-01 (1.0451e-01)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.95)
Epoch: [37][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3025e-01 (1.0458e-01)	Acc@1  96.88 ( 97.86)	Acc@5 100.00 ( 99.96)
Epoch: [37][340/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1200e-01 (1.0453e-01)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 ( 99.96)
Epoch: [37][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.1533e-02 (1.0414e-01)	Acc@1 100.00 ( 97.88)	Acc@5 100.00 ( 99.96)
Epoch: [37][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1719e-01 (1.0418e-01)	Acc@1  94.53 ( 97.87)	Acc@5 100.00 ( 99.96)
Epoch: [37][370/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.0820e-02 (1.0413e-01)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.96)
Epoch: [37][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1542e-01 (1.0417e-01)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 ( 99.95)
Epoch: [37][390/391]	Time  0.109 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2744e-01 (1.0423e-01)	Acc@1  98.75 ( 97.86)	Acc@5 100.00 ( 99.95)
## e[37] optimizer.zero_grad (sum) time: 0.699387788772583
## e[37]       loss.backward (sum) time: 14.261645793914795
## e[37]      optimizer.step (sum) time: 8.237816572189331
## epoch[37] training(only) time: 47.45993614196777
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 8.2471e-01 (8.2471e-01)	Acc@1  75.00 ( 75.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.0049e+00 (1.1077e+00)	Acc@1  76.00 ( 73.55)	Acc@5  94.00 ( 92.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0439e+00 (1.0695e+00)	Acc@1  72.00 ( 73.95)	Acc@5  92.00 ( 92.90)
Test: [ 30/100]	Time  0.051 ( 0.053)	Loss 1.4736e+00 (1.0929e+00)	Acc@1  70.00 ( 73.74)	Acc@5  90.00 ( 92.58)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 1.1963e+00 (1.0970e+00)	Acc@1  74.00 ( 73.61)	Acc@5  94.00 ( 92.88)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.3223e+00 (1.1039e+00)	Acc@1  68.00 ( 73.31)	Acc@5  93.00 ( 92.84)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0312e+00 (1.0795e+00)	Acc@1  74.00 ( 73.30)	Acc@5  96.00 ( 93.25)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 1.3457e+00 (1.0825e+00)	Acc@1  68.00 ( 73.32)	Acc@5  90.00 ( 93.25)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3916e+00 (1.0882e+00)	Acc@1  76.00 ( 73.32)	Acc@5  90.00 ( 93.11)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4307e+00 (1.0786e+00)	Acc@1  63.00 ( 73.29)	Acc@5  92.00 ( 93.23)
 * Acc@1 73.580 Acc@5 93.260
### epoch[37] execution time: 52.47798824310303
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.292 ( 0.292)	Data  0.130 ( 0.130)	Loss 8.7708e-02 (8.7708e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.117 ( 0.138)	Data  0.001 ( 0.013)	Loss 1.0913e-01 (1.1503e-01)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.86)
Epoch: [38][ 20/391]	Time  0.121 ( 0.129)	Data  0.001 ( 0.007)	Loss 8.1177e-02 (1.0692e-01)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.005)	Loss 1.0193e-01 (1.0463e-01)	Acc@1  96.88 ( 97.71)	Acc@5 100.00 ( 99.92)
Epoch: [38][ 40/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.004)	Loss 1.6064e-01 (1.0476e-01)	Acc@1  96.09 ( 97.69)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 50/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.003)	Loss 1.3257e-01 (1.0219e-01)	Acc@1  96.09 ( 97.75)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.7637e-02 (1.0015e-01)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 70/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.6711e-02 (9.8298e-02)	Acc@1 100.00 ( 97.99)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.4543e-02 (9.7702e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 90/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.0332e-02 (9.6742e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.97)
Epoch: [38][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0175e-01 (9.8184e-02)	Acc@1  96.88 ( 98.00)	Acc@5 100.00 ( 99.97)
Epoch: [38][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.0505e-02 (9.8074e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.97)
Epoch: [38][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0315e-01 (9.8108e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 ( 99.97)
Epoch: [38][130/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.1639e-01 (9.7770e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.98)
Epoch: [38][140/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2164e-01 (9.7813e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [38][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.5266e-02 (9.7344e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [38][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.7158e-02 (9.6939e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [38][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3782e-01 (9.6756e-02)	Acc@1  96.09 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [38][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.4482e-02 (9.7131e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [38][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2988e-01 (9.7157e-02)	Acc@1  96.09 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [38][200/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.8816e-02 (9.6919e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [38][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3000e-01 (9.6567e-02)	Acc@1  96.09 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [38][220/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6182e-02 (9.6511e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [38][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0950e-01 (9.6327e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [38][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9233e-02 (9.6749e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [38][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.3379e-01 (9.6822e-02)	Acc@1  96.09 ( 98.08)	Acc@5 100.00 ( 99.98)
Epoch: [38][260/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5806e-02 (9.6729e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [38][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.1472e-02 (9.6670e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [38][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0590e-01 (9.6854e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [38][290/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.3262e-02 (9.6448e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [38][300/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.7900e-02 (9.6379e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [38][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0496e-02 (9.6108e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [38][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0663e-01 (9.6072e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [38][330/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.9905e-02 (9.5832e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [38][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0679e-02 (9.5407e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [38][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0748e-01 (9.5473e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [38][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1194e-01 (9.5400e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [38][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0956e-01 (9.5463e-02)	Acc@1  98.44 ( 98.13)	Acc@5  99.22 ( 99.97)
Epoch: [38][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.2073e-02 (9.5498e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [38][390/391]	Time  0.107 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.0811e-02 (9.5437e-02)	Acc@1  98.75 ( 98.13)	Acc@5 100.00 ( 99.98)
## e[38] optimizer.zero_grad (sum) time: 0.689661979675293
## e[38]       loss.backward (sum) time: 14.253736019134521
## e[38]      optimizer.step (sum) time: 8.109992027282715
## epoch[38] training(only) time: 47.26305937767029
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 8.0078e-01 (8.0078e-01)	Acc@1  77.00 ( 77.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.050 ( 0.061)	Loss 1.0117e+00 (1.1089e+00)	Acc@1  76.00 ( 73.64)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.0732e+00 (1.0744e+00)	Acc@1  75.00 ( 74.00)	Acc@5  93.00 ( 93.24)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.4678e+00 (1.0955e+00)	Acc@1  67.00 ( 73.55)	Acc@5  90.00 ( 92.97)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 1.1992e+00 (1.1016e+00)	Acc@1  74.00 ( 73.46)	Acc@5  94.00 ( 93.15)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.3633e+00 (1.1091e+00)	Acc@1  69.00 ( 73.20)	Acc@5  91.00 ( 93.04)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0811e+00 (1.0830e+00)	Acc@1  75.00 ( 73.23)	Acc@5  96.00 ( 93.44)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3496e+00 (1.0848e+00)	Acc@1  68.00 ( 73.24)	Acc@5  90.00 ( 93.42)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4150e+00 (1.0904e+00)	Acc@1  74.00 ( 73.28)	Acc@5  90.00 ( 93.23)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 1.4033e+00 (1.0780e+00)	Acc@1  65.00 ( 73.35)	Acc@5  93.00 ( 93.35)
 * Acc@1 73.570 Acc@5 93.370
### epoch[38] execution time: 52.26859188079834
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.297 ( 0.297)	Data  0.157 ( 0.157)	Loss 1.0138e-01 (1.0138e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.118 ( 0.138)	Data  0.001 ( 0.015)	Loss 1.0931e-01 (9.4310e-02)	Acc@1  98.44 ( 98.15)	Acc@5  99.22 ( 99.93)
Epoch: [39][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.008)	Loss 6.1218e-02 (9.2406e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 8.2947e-02 (9.3765e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 1.6272e-01 (9.7151e-02)	Acc@1  96.09 ( 98.00)	Acc@5  99.22 ( 99.94)
Epoch: [39][ 50/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.004)	Loss 1.0211e-01 (9.5906e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.95)
Epoch: [39][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 9.6680e-02 (9.5228e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [39][ 70/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 1.2054e-01 (9.5255e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 80/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.2286e-02 (9.4774e-02)	Acc@1 100.00 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1005e-01 (9.5137e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [39][100/391]	Time  0.132 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0565e-01 (9.5224e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [39][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.7717e-02 (9.5083e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [39][120/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.2866e-02 (9.4820e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 ( 99.97)
Epoch: [39][130/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.6233e-02 (9.3652e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [39][140/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.3098e-01 (9.2897e-02)	Acc@1  95.31 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [39][150/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0382e-01 (9.2263e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [39][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6609e-02 (9.2479e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [39][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1096e-01 (9.3121e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [39][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2549e-01 (9.3030e-02)	Acc@1  96.09 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [39][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4707e-02 (9.2906e-02)	Acc@1 100.00 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [39][200/391]	Time  0.134 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4136e-01 (9.3243e-02)	Acc@1  96.88 ( 98.12)	Acc@5  98.44 ( 99.98)
Epoch: [39][210/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1328e-01 (9.3019e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [39][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8196e-02 (9.3289e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [39][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2354e-01 (9.3127e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [39][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2769e-01 (9.3298e-02)	Acc@1  96.09 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [39][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5817e-02 (9.3185e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [39][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6721e-02 (9.3222e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [39][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4219e-02 (9.2893e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [39][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6721e-02 (9.2886e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [39][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.8083e-02 (9.2834e-02)	Acc@1  96.88 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [39][300/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2012e-01 (9.2781e-02)	Acc@1  96.88 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [39][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0858e-01 (9.2693e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [39][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.1125e-02 (9.2648e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [39][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2079e-01 (9.2729e-02)	Acc@1  94.53 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [39][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.6802e-02 (9.2753e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [39][350/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0413e-01 (9.2819e-02)	Acc@1  95.31 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [39][360/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.4697e-02 (9.2841e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [39][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1340e-02 (9.2778e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [39][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0107e-01 (9.2819e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [39][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4941e-01 (9.2724e-02)	Acc@1  96.25 ( 98.13)	Acc@5 100.00 ( 99.97)
## e[39] optimizer.zero_grad (sum) time: 0.6852257251739502
## e[39]       loss.backward (sum) time: 14.281715154647827
## e[39]      optimizer.step (sum) time: 8.121795654296875
## epoch[39] training(only) time: 47.35318398475647
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 7.9395e-01 (7.9395e-01)	Acc@1  79.00 ( 79.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.0293e+00 (1.1253e+00)	Acc@1  76.00 ( 73.55)	Acc@5  92.00 ( 92.36)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 1.0439e+00 (1.0847e+00)	Acc@1  73.00 ( 73.90)	Acc@5  93.00 ( 93.05)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.4746e+00 (1.1026e+00)	Acc@1  68.00 ( 73.65)	Acc@5  89.00 ( 92.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2061e+00 (1.1074e+00)	Acc@1  77.00 ( 73.73)	Acc@5  93.00 ( 92.90)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.3408e+00 (1.1154e+00)	Acc@1  67.00 ( 73.33)	Acc@5  92.00 ( 92.78)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0400e+00 (1.0900e+00)	Acc@1  76.00 ( 73.39)	Acc@5  95.00 ( 93.20)
Test: [ 70/100]	Time  0.051 ( 0.050)	Loss 1.3047e+00 (1.0915e+00)	Acc@1  69.00 ( 73.51)	Acc@5  91.00 ( 93.17)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.4014e+00 (1.0963e+00)	Acc@1  75.00 ( 73.53)	Acc@5  90.00 ( 93.04)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4482e+00 (1.0829e+00)	Acc@1  64.00 ( 73.60)	Acc@5  92.00 ( 93.24)
 * Acc@1 73.870 Acc@5 93.240
### epoch[39] execution time: 52.359962701797485
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.284 ( 0.284)	Data  0.155 ( 0.155)	Loss 8.1726e-02 (8.1726e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.131 ( 0.135)	Data  0.001 ( 0.015)	Loss 8.1604e-02 (8.0769e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.93)
Epoch: [40][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.008)	Loss 8.6182e-02 (8.5895e-02)	Acc@1  96.88 ( 98.51)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 1.2122e-01 (8.9910e-02)	Acc@1  96.09 ( 98.39)	Acc@5 100.00 ( 99.95)
Epoch: [40][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 7.9651e-02 (8.9801e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 50/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.6782e-02 (8.8080e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 9.6069e-02 (8.7823e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 70/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.7698e-02 (8.9009e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.3049e-01 (9.0457e-02)	Acc@1  96.88 ( 98.28)	Acc@5  99.22 ( 99.96)
Epoch: [40][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.9670e-02 (8.9880e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [40][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.6416e-02 (8.9860e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [40][110/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.3252e-02 (9.0254e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.97)
Epoch: [40][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.0190e-02 (8.9261e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [40][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.2585e-01 (8.9112e-02)	Acc@1  95.31 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [40][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1969e-01 (9.0052e-02)	Acc@1  96.88 ( 98.26)	Acc@5  99.22 ( 99.97)
Epoch: [40][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6477e-02 (8.9581e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [40][160/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.1543e-02 (8.9009e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [40][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0190e-02 (8.9592e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [40][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0903e-02 (8.9963e-02)	Acc@1 100.00 ( 98.25)	Acc@5 100.00 ( 99.97)
Epoch: [40][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.4604e-02 (9.0227e-02)	Acc@1  97.66 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [40][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6975e-02 (8.9995e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [40][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2896e-02 (8.9666e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [40][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.7280e-02 (8.9532e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [40][230/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.7708e-02 (8.9567e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [40][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.7839e-02 (8.9535e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [40][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3782e-02 (8.9685e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [40][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6243e-02 (8.9750e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [40][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.9182e-02 (8.9349e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [40][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.3323e-02 (8.9344e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [40][290/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.5144e-02 (8.9518e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [40][300/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9346e-02 (8.9537e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [40][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.7952e-02 (8.9707e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [40][320/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2754e-02 (8.9271e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [40][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.9111e-02 (8.9266e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [40][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.3680e-02 (8.9021e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [40][350/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5674e-02 (8.8938e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [40][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.3679e-02 (8.8965e-02)	Acc@1  98.44 ( 98.29)	Acc@5  99.22 ( 99.98)
Epoch: [40][370/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.3862e-02 (8.9148e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [40][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5012e-02 (8.9089e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [40][390/391]	Time  0.111 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1041e-01 (8.9242e-02)	Acc@1  97.50 ( 98.29)	Acc@5 100.00 ( 99.98)
## e[40] optimizer.zero_grad (sum) time: 0.6912014484405518
## e[40]       loss.backward (sum) time: 14.258071184158325
## e[40]      optimizer.step (sum) time: 8.140278100967407
## epoch[40] training(only) time: 47.2883198261261
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 8.0957e-01 (8.0957e-01)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.0576e+00 (1.1196e+00)	Acc@1  74.00 ( 73.09)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.055 ( 0.056)	Loss 1.0049e+00 (1.0777e+00)	Acc@1  76.00 ( 73.86)	Acc@5  93.00 ( 93.05)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.4609e+00 (1.1019e+00)	Acc@1  70.00 ( 73.61)	Acc@5  89.00 ( 92.74)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.1816e+00 (1.1055e+00)	Acc@1  75.00 ( 73.63)	Acc@5  94.00 ( 92.95)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.3730e+00 (1.1129e+00)	Acc@1  67.00 ( 73.33)	Acc@5  93.00 ( 92.84)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0488e+00 (1.0888e+00)	Acc@1  74.00 ( 73.30)	Acc@5  96.00 ( 93.26)
Test: [ 70/100]	Time  0.059 ( 0.050)	Loss 1.3398e+00 (1.0901e+00)	Acc@1  70.00 ( 73.31)	Acc@5  90.00 ( 93.31)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 1.3945e+00 (1.0949e+00)	Acc@1  75.00 ( 73.38)	Acc@5  90.00 ( 93.20)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4580e+00 (1.0825e+00)	Acc@1  65.00 ( 73.49)	Acc@5  92.00 ( 93.37)
 * Acc@1 73.730 Acc@5 93.330
### epoch[40] execution time: 52.27765083312988
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.285 ( 0.285)	Data  0.155 ( 0.155)	Loss 1.0510e-01 (1.0510e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.119 ( 0.136)	Data  0.001 ( 0.015)	Loss 8.1238e-02 (8.4728e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.93)
Epoch: [41][ 20/391]	Time  0.116 ( 0.128)	Data  0.001 ( 0.008)	Loss 6.4880e-02 (8.7713e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 9.4177e-02 (8.7440e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 1.0809e-01 (9.0521e-02)	Acc@1  96.09 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 6.4819e-02 (8.9317e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.004)	Loss 7.6721e-02 (8.6125e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [41][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.4839e-02 (8.4911e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [41][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.3059e-02 (8.3685e-02)	Acc@1 100.00 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [41][ 90/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 5.7465e-02 (8.2969e-02)	Acc@1 100.00 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [41][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.003)	Loss 6.5063e-02 (8.2902e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [41][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0028e-02 (8.3434e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [41][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.5510e-02 (8.3047e-02)	Acc@1  98.44 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [41][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5256e-02 (8.2480e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [41][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0271e-02 (8.2746e-02)	Acc@1  96.88 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [41][150/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9275e-02 (8.2859e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 ( 99.98)
Epoch: [41][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1359e-01 (8.3157e-02)	Acc@1  96.09 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [41][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2388e-02 (8.2023e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [41][180/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.1492e-02 (8.2148e-02)	Acc@1 100.00 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [41][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1938e-01 (8.2190e-02)	Acc@1  97.66 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [41][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4351e-02 (8.2253e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [41][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9906e-02 (8.1712e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [41][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.1116e-02 (8.1979e-02)	Acc@1  96.88 ( 98.57)	Acc@5 100.00 ( 99.99)
Epoch: [41][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2266e-02 (8.2384e-02)	Acc@1  99.22 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [41][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0496e-02 (8.2226e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [41][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7983e-02 (8.2218e-02)	Acc@1  99.22 ( 98.57)	Acc@5 100.00 ( 99.98)
Epoch: [41][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4402e-02 (8.2210e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 ( 99.98)
Epoch: [41][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8989e-02 (8.2394e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.98)
Epoch: [41][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7087e-02 (8.2432e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.98)
Epoch: [41][290/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0278e-01 (8.2764e-02)	Acc@1  96.88 ( 98.52)	Acc@5 100.00 ( 99.98)
Epoch: [41][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.9519e-02 (8.3237e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [41][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.7383e-02 (8.3015e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [41][320/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.8318e-02 (8.3234e-02)	Acc@1  97.66 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [41][330/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0862e-02 (8.3273e-02)	Acc@1  98.44 ( 98.49)	Acc@5 100.00 ( 99.98)
Epoch: [41][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1700e-01 (8.3439e-02)	Acc@1  96.09 ( 98.48)	Acc@5  99.22 ( 99.98)
Epoch: [41][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.8806e-02 (8.3605e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [41][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.5876e-02 (8.3684e-02)	Acc@1  96.09 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [41][370/391]	Time  0.133 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.2529e-02 (8.3692e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [41][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.8562e-02 (8.3386e-02)	Acc@1  97.66 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [41][390/391]	Time  0.102 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.1326e-01 (8.3575e-02)	Acc@1  91.25 ( 98.46)	Acc@5 100.00 ( 99.98)
## e[41] optimizer.zero_grad (sum) time: 0.6795315742492676
## e[41]       loss.backward (sum) time: 14.319238185882568
## e[41]      optimizer.step (sum) time: 8.041076898574829
## epoch[41] training(only) time: 47.27547335624695
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 8.3398e-01 (8.3398e-01)	Acc@1  76.00 ( 76.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.050 ( 0.061)	Loss 1.0381e+00 (1.1207e+00)	Acc@1  76.00 ( 73.91)	Acc@5  94.00 ( 92.55)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0352e+00 (1.0797e+00)	Acc@1  74.00 ( 73.90)	Acc@5  93.00 ( 93.19)
Test: [ 30/100]	Time  0.051 ( 0.053)	Loss 1.5088e+00 (1.1031e+00)	Acc@1  69.00 ( 73.71)	Acc@5  92.00 ( 92.90)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.2080e+00 (1.1095e+00)	Acc@1  76.00 ( 73.73)	Acc@5  94.00 ( 93.15)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.3682e+00 (1.1170e+00)	Acc@1  68.00 ( 73.29)	Acc@5  92.00 ( 93.02)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0850e+00 (1.0917e+00)	Acc@1  75.00 ( 73.33)	Acc@5  96.00 ( 93.43)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3965e+00 (1.0931e+00)	Acc@1  69.00 ( 73.42)	Acc@5  90.00 ( 93.45)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 1.3818e+00 (1.0987e+00)	Acc@1  75.00 ( 73.49)	Acc@5  91.00 ( 93.25)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.4131e+00 (1.0858e+00)	Acc@1  66.00 ( 73.62)	Acc@5  93.00 ( 93.37)
 * Acc@1 73.800 Acc@5 93.350
### epoch[41] execution time: 52.270451068878174
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.279 ( 0.279)	Data  0.143 ( 0.143)	Loss 5.4901e-02 (5.4901e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.141 ( 0.136)	Data  0.001 ( 0.014)	Loss 7.7454e-02 (8.3513e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.93)
Epoch: [42][ 20/391]	Time  0.123 ( 0.128)	Data  0.001 ( 0.008)	Loss 9.3689e-02 (7.7395e-02)	Acc@1  97.66 ( 98.74)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 30/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.006)	Loss 1.0870e-01 (7.9746e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 40/391]	Time  0.127 ( 0.124)	Data  0.001 ( 0.004)	Loss 8.3374e-02 (7.8341e-02)	Acc@1  97.66 ( 98.65)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.0740e-02 (7.9718e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 9.2102e-02 (7.9059e-02)	Acc@1  97.66 ( 98.53)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.5938e-02 (7.9731e-02)	Acc@1  97.66 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.9224e-02 (8.1017e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 90/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0260e-01 (8.2252e-02)	Acc@1  95.31 ( 98.39)	Acc@5 100.00 ( 99.97)
Epoch: [42][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9927e-02 (8.1531e-02)	Acc@1 100.00 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [42][110/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.8218e-02 (8.0984e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [42][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4839e-02 (8.1216e-02)	Acc@1  96.88 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [42][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1594e-02 (8.1143e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [42][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0571e-01 (8.2015e-02)	Acc@1  96.09 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [42][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1655e-02 (8.2123e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [42][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.9600e-02 (8.1812e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [42][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.7261e-02 (8.1456e-02)	Acc@1 100.00 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [42][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.1003e-02 (8.1323e-02)	Acc@1  96.88 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [42][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5125e-02 (8.1305e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.99)
Epoch: [42][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.5144e-02 (8.1804e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [42][210/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2876e-02 (8.1947e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [42][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4514e-02 (8.1720e-02)	Acc@1  99.22 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [42][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0315e-01 (8.1998e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [42][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8867e-02 (8.2000e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [42][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6467e-02 (8.1975e-02)	Acc@1 100.00 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [42][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6111e-02 (8.1956e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [42][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.7036e-02 (8.1774e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [42][280/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5500e-02 (8.1751e-02)	Acc@1  99.22 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [42][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.8369e-02 (8.1662e-02)	Acc@1  98.44 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [42][300/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3220e-01 (8.1515e-02)	Acc@1  95.31 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [42][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.2407e-02 (8.1629e-02)	Acc@1  97.66 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [42][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5298e-02 (8.1626e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [42][330/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.5642e-02 (8.1500e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [42][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3245e-01 (8.1441e-02)	Acc@1  96.88 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [42][350/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.9304e-02 (8.1384e-02)	Acc@1  97.66 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [42][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.2439e-01 (8.1557e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [42][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0327e-01 (8.1618e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [42][380/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9662e-02 (8.1275e-02)	Acc@1  99.22 ( 98.50)	Acc@5 100.00 ( 99.98)
Epoch: [42][390/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.0872e-02 (8.1240e-02)	Acc@1  98.75 ( 98.50)	Acc@5 100.00 ( 99.98)
## e[42] optimizer.zero_grad (sum) time: 0.6884329319000244
## e[42]       loss.backward (sum) time: 14.278525590896606
## e[42]      optimizer.step (sum) time: 8.100475072860718
## epoch[42] training(only) time: 47.26209807395935
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 8.6084e-01 (8.6084e-01)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.0254e+00 (1.1352e+00)	Acc@1  77.00 ( 73.27)	Acc@5  95.00 ( 92.64)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 1.0303e+00 (1.0853e+00)	Acc@1  75.00 ( 73.86)	Acc@5  92.00 ( 93.10)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.5264e+00 (1.1091e+00)	Acc@1  67.00 ( 73.65)	Acc@5  91.00 ( 92.97)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2100e+00 (1.1153e+00)	Acc@1  77.00 ( 73.71)	Acc@5  94.00 ( 93.17)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4121e+00 (1.1239e+00)	Acc@1  68.00 ( 73.41)	Acc@5  92.00 ( 92.94)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0820e+00 (1.0978e+00)	Acc@1  74.00 ( 73.43)	Acc@5  96.00 ( 93.38)
Test: [ 70/100]	Time  0.057 ( 0.050)	Loss 1.3809e+00 (1.1001e+00)	Acc@1  68.00 ( 73.42)	Acc@5  90.00 ( 93.41)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4160e+00 (1.1055e+00)	Acc@1  76.00 ( 73.41)	Acc@5  92.00 ( 93.27)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4238e+00 (1.0928e+00)	Acc@1  64.00 ( 73.57)	Acc@5  91.00 ( 93.37)
 * Acc@1 73.860 Acc@5 93.380
### epoch[42] execution time: 52.24775671958923
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.282 ( 0.282)	Data  0.147 ( 0.147)	Loss 8.3679e-02 (8.3679e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.119 ( 0.136)	Data  0.001 ( 0.014)	Loss 5.8899e-02 (7.3595e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 5.7098e-02 (6.8819e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 9.4482e-02 (7.0861e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 9.1675e-02 (7.3119e-02)	Acc@1  96.88 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.6294e-02 (7.3758e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.0354e-02 (7.6506e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.5015e-01 (7.7176e-02)	Acc@1  96.88 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.0698e-02 (7.6964e-02)	Acc@1  96.09 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.8685e-02 (7.6863e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [43][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.1492e-02 (7.6231e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [43][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.9214e-02 (7.6458e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [43][120/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.8918e-02 (7.6152e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [43][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.5693e-02 (7.6400e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [43][140/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0120e-01 (7.6822e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4626e-02 (7.6071e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0027e-02 (7.5866e-02)	Acc@1  97.66 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [43][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6477e-02 (7.6069e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][180/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.5520e-02 (7.6360e-02)	Acc@1  98.44 ( 98.67)	Acc@5  99.22 ( 99.99)
Epoch: [43][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9580e-02 (7.5731e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [43][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.4270e-01 (7.6234e-02)	Acc@1  95.31 ( 98.67)	Acc@5  99.22 ( 99.98)
Epoch: [43][210/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.5693e-02 (7.6272e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][220/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0815e-01 (7.6745e-02)	Acc@1  97.66 ( 98.64)	Acc@5 100.00 ( 99.98)
Epoch: [43][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8308e-02 (7.6690e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 ( 99.98)
Epoch: [43][240/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.5327e-02 (7.6953e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.98)
Epoch: [43][250/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.1309e-02 (7.6908e-02)	Acc@1  97.66 ( 98.65)	Acc@5 100.00 ( 99.98)
Epoch: [43][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0028e-02 (7.7004e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.0942e-02 (7.7071e-02)	Acc@1  96.09 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [43][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0602e-01 (7.6877e-02)	Acc@1  96.88 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][290/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3196e-01 (7.7020e-02)	Acc@1  96.88 ( 98.62)	Acc@5  99.22 ( 99.98)
Epoch: [43][300/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2876e-02 (7.6808e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.98)
Epoch: [43][310/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0089e-01 (7.6929e-02)	Acc@1  96.09 ( 98.62)	Acc@5 100.00 ( 99.98)
Epoch: [43][320/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.5093e-02 (7.7191e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [43][330/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.3547e-02 (7.7137e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.98)
Epoch: [43][340/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.0383e-02 (7.7310e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.98)
Epoch: [43][350/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.2336e-02 (7.7270e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.98)
Epoch: [43][360/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.1655e-02 (7.7643e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.98)
Epoch: [43][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.1248e-02 (7.7502e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [43][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5735e-02 (7.7421e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][390/391]	Time  0.114 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.6802e-02 (7.7716e-02)	Acc@1  98.75 ( 98.63)	Acc@5 100.00 ( 99.98)
## e[43] optimizer.zero_grad (sum) time: 0.6861050128936768
## e[43]       loss.backward (sum) time: 14.358814716339111
## e[43]      optimizer.step (sum) time: 8.066884756088257
## epoch[43] training(only) time: 47.29948377609253
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 8.1885e-01 (8.1885e-01)	Acc@1  76.00 ( 76.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.045 ( 0.061)	Loss 1.0898e+00 (1.1310e+00)	Acc@1  74.00 ( 73.45)	Acc@5  92.00 ( 92.18)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.0576e+00 (1.0870e+00)	Acc@1  75.00 ( 74.10)	Acc@5  94.00 ( 93.10)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 1.5166e+00 (1.1091e+00)	Acc@1  67.00 ( 74.03)	Acc@5  89.00 ( 92.81)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.1953e+00 (1.1166e+00)	Acc@1  75.00 ( 73.90)	Acc@5  95.00 ( 93.00)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.3750e+00 (1.1219e+00)	Acc@1  69.00 ( 73.65)	Acc@5  92.00 ( 92.86)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0410e+00 (1.0961e+00)	Acc@1  75.00 ( 73.69)	Acc@5  96.00 ( 93.26)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 1.4053e+00 (1.0995e+00)	Acc@1  67.00 ( 73.61)	Acc@5  90.00 ( 93.28)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4248e+00 (1.1030e+00)	Acc@1  75.00 ( 73.69)	Acc@5  90.00 ( 93.17)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4365e+00 (1.0905e+00)	Acc@1  66.00 ( 73.77)	Acc@5  91.00 ( 93.32)
 * Acc@1 74.020 Acc@5 93.300
### epoch[43] execution time: 52.28522276878357
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.279 ( 0.279)	Data  0.140 ( 0.140)	Loss 5.6458e-02 (5.6458e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.014)	Loss 8.7280e-02 (7.0593e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.124 ( 0.128)	Data  0.001 ( 0.008)	Loss 7.3242e-02 (7.2667e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.005)	Loss 6.5491e-02 (7.1144e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.130 ( 0.124)	Data  0.001 ( 0.004)	Loss 7.7209e-02 (7.2612e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.1056e-02 (7.2419e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.8308e-02 (7.1839e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][ 70/391]	Time  0.129 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.9407e-02 (7.2365e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [44][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.0486e-02 (7.1084e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1453e-02 (7.0908e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.0881e-02 (7.1128e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [44][110/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.0272e-02 (7.1484e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [44][120/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.0364e-02 (7.0708e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [44][130/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.4504e-02 (7.0377e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [44][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.3435e-02 (7.0543e-02)	Acc@1  96.88 ( 98.80)	Acc@5 100.00 ( 99.98)
Epoch: [44][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5979e-02 (7.0258e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.98)
Epoch: [44][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8684e-02 (6.9995e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [44][170/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8848e-02 (7.0113e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [44][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8157e-02 (7.0525e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [44][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0435e-02 (7.0995e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [44][200/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.7017e-02 (7.1223e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [44][210/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2825e-02 (7.1865e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6467e-02 (7.2468e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [44][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0040e-01 (7.3020e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [44][240/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8542e-02 (7.3027e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [44][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4148e-02 (7.2923e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [44][260/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.4136e-01 (7.3121e-02)	Acc@1  96.09 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [44][270/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2021e-02 (7.3558e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [44][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.3171e-02 (7.3797e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [44][290/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.7097e-02 (7.3715e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [44][300/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9307e-02 (7.3851e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [44][310/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.1749e-01 (7.3877e-02)	Acc@1  96.88 ( 98.73)	Acc@5  99.22 ( 99.99)
Epoch: [44][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.8755e-02 (7.4124e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [44][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.2092e-02 (7.4024e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [44][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.7627e-02 (7.4187e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [44][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.7524e-02 (7.4498e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [44][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1432e-02 (7.4476e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [44][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2754e-02 (7.4573e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [44][380/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2998e-02 (7.4138e-02)	Acc@1  96.88 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [44][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0889e-01 (7.4238e-02)	Acc@1  95.00 ( 98.70)	Acc@5 100.00 ( 99.99)
## e[44] optimizer.zero_grad (sum) time: 0.6845917701721191
## e[44]       loss.backward (sum) time: 14.35833740234375
## e[44]      optimizer.step (sum) time: 8.075048923492432
## epoch[44] training(only) time: 47.30723762512207
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 8.4570e-01 (8.4570e-01)	Acc@1  77.00 ( 77.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.0381e+00 (1.1244e+00)	Acc@1  75.00 ( 73.55)	Acc@5  92.00 ( 92.45)
Test: [ 20/100]	Time  0.050 ( 0.054)	Loss 1.0361e+00 (1.0869e+00)	Acc@1  73.00 ( 73.67)	Acc@5  94.00 ( 93.10)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5029e+00 (1.1070e+00)	Acc@1  69.00 ( 73.65)	Acc@5  91.00 ( 92.81)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2285e+00 (1.1175e+00)	Acc@1  76.00 ( 73.66)	Acc@5  94.00 ( 92.98)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.3711e+00 (1.1253e+00)	Acc@1  69.00 ( 73.25)	Acc@5  91.00 ( 92.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0635e+00 (1.0983e+00)	Acc@1  77.00 ( 73.36)	Acc@5  96.00 ( 93.18)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 1.3818e+00 (1.1015e+00)	Acc@1  70.00 ( 73.34)	Acc@5  90.00 ( 93.24)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4473e+00 (1.1062e+00)	Acc@1  75.00 ( 73.43)	Acc@5  92.00 ( 93.14)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.4814e+00 (1.0942e+00)	Acc@1  64.00 ( 73.52)	Acc@5  91.00 ( 93.27)
 * Acc@1 73.810 Acc@5 93.260
### epoch[44] execution time: 52.30197072029114
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.293 ( 0.293)	Data  0.134 ( 0.134)	Loss 5.1941e-02 (5.1941e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.119 ( 0.136)	Data  0.001 ( 0.013)	Loss 8.4412e-02 (6.4833e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.93)
Epoch: [45][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.007)	Loss 6.6284e-02 (6.7830e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.93)
Epoch: [45][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.005)	Loss 3.9001e-02 (7.0327e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 ( 99.92)
Epoch: [45][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.2826e-02 (6.7572e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 ( 99.94)
Epoch: [45][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.5420e-02 (6.7417e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.95)
Epoch: [45][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.0557e-02 (6.8010e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.96)
Epoch: [45][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.1346e-01 (6.8853e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.0210e-02 (6.9374e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.97)
Epoch: [45][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.3293e-02 (6.9488e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.97)
Epoch: [45][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.3843e-02 (6.9413e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 ( 99.98)
Epoch: [45][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.0566e-02 (6.9467e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.98)
Epoch: [45][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.1167e-02 (6.8354e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 ( 99.98)
Epoch: [45][130/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.2927e-02 (6.8428e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.98)
Epoch: [45][140/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.3008e-02 (6.9259e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 ( 99.98)
Epoch: [45][150/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.6772e-02 (6.9206e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.98)
Epoch: [45][160/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2439e-02 (6.9004e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.98)
Epoch: [45][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9275e-02 (6.9200e-02)	Acc@1  96.88 ( 98.95)	Acc@5 100.00 ( 99.98)
Epoch: [45][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5613e-02 (6.9742e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.98)
Epoch: [45][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8064e-02 (7.0401e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [45][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3364e-02 (7.0391e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 ( 99.98)
Epoch: [45][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7455e-02 (7.0518e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [45][220/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2285e-02 (7.0920e-02)	Acc@1  96.09 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [45][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8605e-02 (7.1460e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [45][240/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2439e-02 (7.1587e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 ( 99.98)
Epoch: [45][250/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3599e-02 (7.1368e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 ( 99.98)
Epoch: [45][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2664e-02 (7.1763e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 ( 99.98)
Epoch: [45][270/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5500e-02 (7.1690e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 ( 99.98)
Epoch: [45][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.6355e-02 (7.1894e-02)	Acc@1  97.66 ( 98.79)	Acc@5 100.00 ( 99.98)
Epoch: [45][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.1482e-02 (7.2240e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 ( 99.98)
Epoch: [45][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0364e-02 (7.2065e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 ( 99.98)
Epoch: [45][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.5215e-02 (7.2099e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.98)
Epoch: [45][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4962e-02 (7.1888e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 ( 99.98)
Epoch: [45][330/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.6111e-02 (7.2023e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 ( 99.98)
Epoch: [45][340/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7913e-02 (7.1803e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 ( 99.98)
Epoch: [45][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.9214e-02 (7.1757e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 ( 99.98)
Epoch: [45][360/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.4116e-02 (7.1880e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.98)
Epoch: [45][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.6833e-02 (7.1675e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [45][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.0498e-01 (7.1969e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [45][390/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.3171e-01 (7.2070e-02)	Acc@1  97.50 ( 98.77)	Acc@5 100.00 ( 99.99)
## e[45] optimizer.zero_grad (sum) time: 0.68941330909729
## e[45]       loss.backward (sum) time: 14.306190013885498
## e[45]      optimizer.step (sum) time: 8.152275800704956
## epoch[45] training(only) time: 47.3357412815094
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 8.2910e-01 (8.2910e-01)	Acc@1  75.00 ( 75.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.0713e+00 (1.1336e+00)	Acc@1  74.00 ( 73.09)	Acc@5  93.00 ( 92.45)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0557e+00 (1.0956e+00)	Acc@1  73.00 ( 73.38)	Acc@5  93.00 ( 93.10)
Test: [ 30/100]	Time  0.054 ( 0.052)	Loss 1.5996e+00 (1.1172e+00)	Acc@1  68.00 ( 73.26)	Acc@5  89.00 ( 92.87)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2588e+00 (1.1286e+00)	Acc@1  76.00 ( 73.32)	Acc@5  94.00 ( 93.05)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.3857e+00 (1.1347e+00)	Acc@1  67.00 ( 72.98)	Acc@5  91.00 ( 92.88)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0635e+00 (1.1074e+00)	Acc@1  75.00 ( 73.11)	Acc@5  96.00 ( 93.28)
Test: [ 70/100]	Time  0.049 ( 0.049)	Loss 1.4092e+00 (1.1093e+00)	Acc@1  68.00 ( 73.20)	Acc@5  90.00 ( 93.31)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 1.4785e+00 (1.1133e+00)	Acc@1  74.00 ( 73.23)	Acc@5  91.00 ( 93.16)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.4365e+00 (1.1002e+00)	Acc@1  65.00 ( 73.41)	Acc@5  93.00 ( 93.30)
 * Acc@1 73.710 Acc@5 93.350
### epoch[45] execution time: 52.32258057594299
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.295 ( 0.295)	Data  0.160 ( 0.160)	Loss 8.2825e-02 (8.2825e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.119 ( 0.135)	Data  0.001 ( 0.015)	Loss 6.1249e-02 (6.6692e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.009)	Loss 5.4047e-02 (6.8066e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.006)	Loss 7.5134e-02 (6.6961e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 6.9641e-02 (6.5547e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.7495e-02 (6.4187e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.9875e-02 (6.3648e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.7515e-02 (6.3887e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.4238e-02 (6.4005e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 9.5886e-02 (6.4775e-02)	Acc@1  96.88 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.8336e-02 (6.4751e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.7617e-02 (6.5267e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3976e-02 (6.5118e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1035e-02 (6.5265e-02)	Acc@1  97.66 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9683e-02 (6.5037e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3242e-02 (6.5134e-02)	Acc@1  98.44 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.7688e-02 (6.5290e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7140e-02 (6.5457e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.9121e-02 (6.5652e-02)	Acc@1  96.88 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0278e-01 (6.6003e-02)	Acc@1  97.66 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2397e-02 (6.6194e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1178e-02 (6.6020e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9712e-02 (6.6341e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9448e-02 (6.6416e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0278e-01 (6.6430e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8745e-02 (6.6651e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3679e-02 (6.6995e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [46][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.7942e-02 (6.6909e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [46][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8440e-02 (6.7500e-02)	Acc@1  97.66 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [46][290/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.9365e-02 (6.7415e-02)	Acc@1  96.09 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][300/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9998e-02 (6.7405e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1412e-02 (6.7473e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][320/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8481e-02 (6.7441e-02)	Acc@1  97.66 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4443e-02 (6.7556e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2815e-02 (6.7643e-02)	Acc@1  97.66 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [46][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.7343e-02 (6.7625e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [46][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0333e-02 (6.7719e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5573e-02 (6.7578e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.8604e-02 (6.7617e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [46][390/391]	Time  0.110 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.3679e-02 (6.7533e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.99)
## e[46] optimizer.zero_grad (sum) time: 0.690767765045166
## e[46]       loss.backward (sum) time: 14.278658151626587
## e[46]      optimizer.step (sum) time: 8.122303247451782
## epoch[46] training(only) time: 47.21759653091431
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 8.1006e-01 (8.1006e-01)	Acc@1  77.00 ( 77.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 1.1006e+00 (1.1390e+00)	Acc@1  74.00 ( 73.36)	Acc@5  92.00 ( 92.45)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0586e+00 (1.0950e+00)	Acc@1  73.00 ( 74.24)	Acc@5  94.00 ( 93.14)
Test: [ 30/100]	Time  0.049 ( 0.052)	Loss 1.5059e+00 (1.1151e+00)	Acc@1  69.00 ( 73.94)	Acc@5  90.00 ( 92.90)
Test: [ 40/100]	Time  0.052 ( 0.051)	Loss 1.2529e+00 (1.1240e+00)	Acc@1  76.00 ( 73.80)	Acc@5  94.00 ( 93.07)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.3730e+00 (1.1310e+00)	Acc@1  68.00 ( 73.53)	Acc@5  92.00 ( 92.98)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0713e+00 (1.1037e+00)	Acc@1  73.00 ( 73.66)	Acc@5  96.00 ( 93.36)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4043e+00 (1.1063e+00)	Acc@1  68.00 ( 73.59)	Acc@5  90.00 ( 93.39)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4424e+00 (1.1093e+00)	Acc@1  75.00 ( 73.65)	Acc@5  90.00 ( 93.26)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.4219e+00 (1.0953e+00)	Acc@1  65.00 ( 73.80)	Acc@5  93.00 ( 93.42)
 * Acc@1 74.030 Acc@5 93.380
### epoch[46] execution time: 52.24003982543945
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.284 ( 0.284)	Data  0.146 ( 0.146)	Loss 7.3486e-02 (7.3486e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.119 ( 0.135)	Data  0.001 ( 0.014)	Loss 8.2642e-02 (6.0627e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.008)	Loss 7.3242e-02 (6.2222e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.006)	Loss 9.7717e-02 (6.1861e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 9.6130e-02 (6.2589e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.7180e-02 (6.1870e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 9.8877e-02 (6.3745e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.9763e-02 (6.3907e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.1382e-02 (6.3451e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.3314e-02 (6.3445e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1890e-02 (6.4725e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7098e-02 (6.3737e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6650e-02 (6.3479e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9092e-02 (6.3234e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1727e-02 (6.4091e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5012e-02 (6.4398e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [47][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2224e-02 (6.5317e-02)	Acc@1  96.09 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0669e-02 (6.5157e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4291e-02 (6.4722e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.5745e-02 (6.4669e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5939e-02 (6.4978e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0303e-01 (6.5301e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0419e-01 (6.5310e-02)	Acc@1  96.88 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3730e-02 (6.5233e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2643e-02 (6.5346e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1880e-02 (6.5182e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8594e-02 (6.5351e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.6692e-02 (6.5804e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4474e-02 (6.5650e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5145e-02 (6.6185e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.6589e-02 (6.6343e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1544e-02 (6.6201e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.4341e-02 (6.6380e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9113e-02 (6.6669e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.6843e-02 (6.6414e-02)	Acc@1  98.44 ( 98.94)	Acc@5  99.22 ( 99.99)
Epoch: [47][350/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0873e-02 (6.6238e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [47][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8401e-02 (6.6073e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [47][370/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.6772e-02 (6.6011e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [47][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4556e-02 (6.6032e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [47][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.2262e-01 (6.6117e-02)	Acc@1  97.50 ( 98.94)	Acc@5 100.00 ( 99.99)
## e[47] optimizer.zero_grad (sum) time: 0.6818177700042725
## e[47]       loss.backward (sum) time: 14.309902429580688
## e[47]      optimizer.step (sum) time: 8.038063287734985
## epoch[47] training(only) time: 47.15076160430908
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 8.1055e-01 (8.1055e-01)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 1.0703e+00 (1.1418e+00)	Acc@1  74.00 ( 73.18)	Acc@5  92.00 ( 92.18)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0830e+00 (1.0947e+00)	Acc@1  73.00 ( 74.19)	Acc@5  92.00 ( 92.90)
Test: [ 30/100]	Time  0.055 ( 0.052)	Loss 1.5439e+00 (1.1184e+00)	Acc@1  68.00 ( 73.71)	Acc@5  90.00 ( 92.74)
Test: [ 40/100]	Time  0.050 ( 0.051)	Loss 1.2334e+00 (1.1271e+00)	Acc@1  77.00 ( 73.76)	Acc@5  94.00 ( 92.98)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.3965e+00 (1.1342e+00)	Acc@1  68.00 ( 73.39)	Acc@5  93.00 ( 92.90)
Test: [ 60/100]	Time  0.050 ( 0.050)	Loss 1.0615e+00 (1.1080e+00)	Acc@1  75.00 ( 73.51)	Acc@5  96.00 ( 93.25)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 1.4414e+00 (1.1115e+00)	Acc@1  67.00 ( 73.48)	Acc@5  90.00 ( 93.31)
Test: [ 80/100]	Time  0.060 ( 0.049)	Loss 1.4697e+00 (1.1157e+00)	Acc@1  73.00 ( 73.43)	Acc@5  90.00 ( 93.17)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.4473e+00 (1.1034e+00)	Acc@1  65.00 ( 73.59)	Acc@5  92.00 ( 93.31)
 * Acc@1 73.840 Acc@5 93.290
### epoch[47] execution time: 52.173300981521606
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.292 ( 0.292)	Data  0.157 ( 0.157)	Loss 1.0852e-01 (1.0852e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.123 ( 0.137)	Data  0.001 ( 0.015)	Loss 8.9661e-02 (7.2940e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.93)
Epoch: [48][ 20/391]	Time  0.123 ( 0.129)	Data  0.001 ( 0.008)	Loss 6.7932e-02 (7.1301e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 ( 99.96)
Epoch: [48][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.006)	Loss 4.5441e-02 (6.8535e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.8615e-02 (6.5826e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 50/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.8431e-02 (6.5510e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.98)
Epoch: [48][ 60/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.5685e-02 (6.4664e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.6570e-02 (6.3469e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.97)
Epoch: [48][ 80/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.6792e-02 (6.3332e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 ( 99.96)
Epoch: [48][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.8441e-02 (6.3985e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.97)
Epoch: [48][100/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.5562e-02 (6.3849e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 ( 99.97)
Epoch: [48][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9408e-02 (6.4020e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.97)
Epoch: [48][120/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.8848e-02 (6.4512e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.97)
Epoch: [48][130/391]	Time  0.135 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.8938e-02 (6.4831e-02)	Acc@1  98.44 ( 99.03)	Acc@5  99.22 ( 99.97)
Epoch: [48][140/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6641e-02 (6.4737e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 ( 99.97)
Epoch: [48][150/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8044e-02 (6.4254e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.97)
Epoch: [48][160/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1401e-02 (6.4570e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 ( 99.98)
Epoch: [48][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8868e-02 (6.4184e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.98)
Epoch: [48][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.7351e-02 (6.4094e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 ( 99.98)
Epoch: [48][190/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6295e-02 (6.3326e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.98)
Epoch: [48][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1737e-01 (6.3663e-02)	Acc@1  96.88 ( 99.04)	Acc@5 100.00 ( 99.98)
Epoch: [48][210/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6458e-02 (6.4043e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.98)
Epoch: [48][220/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6345e-02 (6.3972e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 ( 99.98)
Epoch: [48][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4167e-02 (6.3895e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 ( 99.98)
Epoch: [48][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.2468e-02 (6.4069e-02)	Acc@1  96.88 ( 99.02)	Acc@5 100.00 ( 99.98)
Epoch: [48][250/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5959e-02 (6.3682e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.98)
Epoch: [48][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0608e-01 (6.4282e-02)	Acc@1  96.88 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [48][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6294e-02 (6.4574e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [48][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2876e-02 (6.4448e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [48][290/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9285e-02 (6.4245e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [48][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8857e-02 (6.4224e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][310/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.9590e-02 (6.4066e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][320/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4382e-02 (6.3901e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.7627e-02 (6.4098e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.6284e-02 (6.3873e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [48][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.9713e-02 (6.3738e-02)	Acc@1 100.00 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][360/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0852e-02 (6.3525e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [48][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.3833e-02 (6.3673e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][380/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.2683e-02 (6.3693e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 ( 99.99)
Epoch: [48][390/391]	Time  0.109 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.8970e-02 (6.3661e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
## e[48] optimizer.zero_grad (sum) time: 0.6853868961334229
## e[48]       loss.backward (sum) time: 14.308642625808716
## e[48]      optimizer.step (sum) time: 8.132097005844116
## epoch[48] training(only) time: 47.2419638633728
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 8.3398e-01 (8.3398e-01)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.0635e+00 (1.1526e+00)	Acc@1  74.00 ( 73.18)	Acc@5  92.00 ( 92.00)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0654e+00 (1.1037e+00)	Acc@1  72.00 ( 73.90)	Acc@5  93.00 ( 92.95)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5234e+00 (1.1255e+00)	Acc@1  68.00 ( 73.81)	Acc@5  90.00 ( 92.74)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.2734e+00 (1.1338e+00)	Acc@1  75.00 ( 73.80)	Acc@5  94.00 ( 92.93)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4355e+00 (1.1412e+00)	Acc@1  67.00 ( 73.45)	Acc@5  91.00 ( 92.82)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0967e+00 (1.1164e+00)	Acc@1  73.00 ( 73.44)	Acc@5  97.00 ( 93.25)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4014e+00 (1.1185e+00)	Acc@1  68.00 ( 73.45)	Acc@5  90.00 ( 93.28)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4395e+00 (1.1214e+00)	Acc@1  73.00 ( 73.46)	Acc@5  90.00 ( 93.11)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5049e+00 (1.1086e+00)	Acc@1  65.00 ( 73.65)	Acc@5  93.00 ( 93.30)
 * Acc@1 73.870 Acc@5 93.290
### epoch[48] execution time: 52.24875259399414
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.289 ( 0.289)	Data  0.150 ( 0.150)	Loss 3.9337e-02 (3.9337e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.118 ( 0.136)	Data  0.001 ( 0.014)	Loss 7.3059e-02 (5.9487e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.129 ( 0.129)	Data  0.001 ( 0.008)	Loss 7.4707e-02 (5.8171e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.121 ( 0.126)	Data  0.001 ( 0.006)	Loss 5.2094e-02 (5.7674e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 7.1838e-02 (5.8322e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 6.3721e-02 (5.9604e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.3436e-02 (6.0151e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.0669e-02 (5.9984e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4607e-02 (5.9861e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.6040e-02 (6.0026e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.2673e-02 (5.9948e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.8065e-02 (6.0271e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8807e-02 (6.0404e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5359e-02 (6.0063e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0089e-02 (5.9663e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9458e-02 (5.9483e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4575e-02 (6.0163e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4016e-02 (5.9990e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3660e-02 (6.0147e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.0266e-01 (6.0165e-02)	Acc@1  96.09 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.1299e-02 (6.0742e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3131e-02 (6.0463e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1788e-02 (6.0345e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.1597e-01 (6.0372e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.1003e-02 (6.1010e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6345e-02 (6.1172e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4779e-02 (6.1401e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.7993e-02 (6.1563e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5105e-02 (6.1576e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9387e-02 (6.1597e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.137 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.1289e-02 (6.1463e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5908e-02 (6.1449e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.1777e-02 (6.1422e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2023e-02 (6.1335e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7302e-02 (6.1599e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.9417e-02 (6.1855e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.7648e-02 (6.1910e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.4951e-02 (6.1999e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9723e-02 (6.2054e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.111 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.4219e-02 (6.2050e-02)	Acc@1  97.50 ( 99.05)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.6880667209625244
## e[49]       loss.backward (sum) time: 14.30722427368164
## e[49]      optimizer.step (sum) time: 8.080578565597534
## epoch[49] training(only) time: 47.21050262451172
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 8.5645e-01 (8.5645e-01)	Acc@1  76.00 ( 76.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.1035e+00 (1.1438e+00)	Acc@1  74.00 ( 73.00)	Acc@5  93.00 ( 92.00)
Test: [ 20/100]	Time  0.050 ( 0.054)	Loss 1.0840e+00 (1.1030e+00)	Acc@1  74.00 ( 74.19)	Acc@5  92.00 ( 92.95)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.5830e+00 (1.1289e+00)	Acc@1  69.00 ( 73.94)	Acc@5  89.00 ( 92.77)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 1.2773e+00 (1.1389e+00)	Acc@1  76.00 ( 74.02)	Acc@5  94.00 ( 92.88)
Test: [ 50/100]	Time  0.050 ( 0.050)	Loss 1.3955e+00 (1.1453e+00)	Acc@1  67.00 ( 73.57)	Acc@5  91.00 ( 92.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0908e+00 (1.1191e+00)	Acc@1  75.00 ( 73.67)	Acc@5  95.00 ( 93.11)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4238e+00 (1.1218e+00)	Acc@1  68.00 ( 73.62)	Acc@5  90.00 ( 93.18)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.4512e+00 (1.1268e+00)	Acc@1  75.00 ( 73.58)	Acc@5  91.00 ( 93.10)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5068e+00 (1.1143e+00)	Acc@1  65.00 ( 73.73)	Acc@5  91.00 ( 93.21)
 * Acc@1 73.970 Acc@5 93.230
### epoch[49] execution time: 52.18394708633423
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.283 ( 0.283)	Data  0.150 ( 0.150)	Loss 6.1310e-02 (6.1310e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.014)	Loss 5.2277e-02 (6.3049e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 4.3640e-02 (5.8757e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 6.4697e-02 (6.0362e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 5.0232e-02 (6.0784e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.4474e-02 (6.0625e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.8806e-02 (6.1314e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.1483e-02 (6.0706e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.003)	Loss 1.0498e-01 (6.0133e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.8359e-02 (6.0563e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0577e-02 (6.0909e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0944e-02 (6.1316e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8298e-02 (6.1079e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2664e-02 (6.0774e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3669e-02 (6.0905e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3975e-02 (6.0624e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0985e-02 (6.0210e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4424e-02 (5.9678e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.4229e-02 (5.9923e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2561e-02 (5.9832e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0913e-02 (6.0411e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6997e-02 (6.0154e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.3792e-02 (6.0300e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [50][230/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9174e-02 (6.0283e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [50][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.6365e-02 (6.0398e-02)	Acc@1  96.88 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [50][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2593e-02 (6.0053e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [50][260/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3253e-02 (5.9838e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [50][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3374e-02 (5.9862e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [50][280/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.9042e-02 (5.9776e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [50][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8950e-02 (5.9785e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [50][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.6294e-02 (5.9857e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [50][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1351e-02 (5.9609e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [50][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5867e-02 (5.9647e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1351e-02 (5.9710e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9103e-02 (5.9466e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.122 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.7871e-02 (5.9437e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.4678e-02 (5.9263e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.2155e-02 (5.9109e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.3191e-02 (5.9261e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.6152e-02 (5.9394e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.680962085723877
## e[50]       loss.backward (sum) time: 14.306520700454712
## e[50]      optimizer.step (sum) time: 8.032841682434082
## epoch[50] training(only) time: 47.14386487007141
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 8.2178e-01 (8.2178e-01)	Acc@1  77.00 ( 77.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.1104e+00 (1.1481e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 92.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0771e+00 (1.1067e+00)	Acc@1  73.00 ( 74.05)	Acc@5  92.00 ( 93.10)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.5439e+00 (1.1296e+00)	Acc@1  68.00 ( 73.97)	Acc@5  90.00 ( 92.84)
Test: [ 40/100]	Time  0.055 ( 0.051)	Loss 1.2822e+00 (1.1396e+00)	Acc@1  75.00 ( 73.93)	Acc@5  94.00 ( 92.95)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.3672e+00 (1.1450e+00)	Acc@1  69.00 ( 73.65)	Acc@5  93.00 ( 92.80)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0898e+00 (1.1178e+00)	Acc@1  75.00 ( 73.70)	Acc@5  96.00 ( 93.18)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3887e+00 (1.1204e+00)	Acc@1  70.00 ( 73.68)	Acc@5  90.00 ( 93.17)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4316e+00 (1.1235e+00)	Acc@1  76.00 ( 73.72)	Acc@5  91.00 ( 93.12)
Test: [ 90/100]	Time  0.054 ( 0.049)	Loss 1.5068e+00 (1.1114e+00)	Acc@1  65.00 ( 73.79)	Acc@5  91.00 ( 93.18)
 * Acc@1 73.990 Acc@5 93.200
### epoch[50] execution time: 52.12916350364685
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.286 ( 0.286)	Data  0.154 ( 0.154)	Loss 6.7810e-02 (6.7810e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.015)	Loss 5.5298e-02 (5.8064e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.128 ( 0.127)	Data  0.001 ( 0.008)	Loss 7.2632e-02 (5.8819e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.5583e-02 (5.8852e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 6.8665e-02 (5.7782e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.98)
Epoch: [51][ 50/391]	Time  0.133 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.5593e-02 (5.6767e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 ( 99.98)
Epoch: [51][ 60/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.3425e-02 (5.6900e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [51][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.0070e-02 (5.7398e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [51][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.8309e-02 (5.7567e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [51][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 8.1421e-02 (5.8018e-02)	Acc@1  97.66 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [51][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.2224e-02 (5.8283e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [51][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 1.0553e-01 (5.8340e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [51][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8910e-02 (5.8909e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [51][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9336e-02 (5.8992e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [51][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4922e-02 (5.8812e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [51][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3955e-02 (5.8767e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [51][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9967e-02 (5.9091e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2532e-02 (5.8326e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7922e-02 (5.8161e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5481e-02 (5.8332e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.1167e-02 (5.8357e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.133 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8369e-02 (5.8459e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8787e-02 (5.8543e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9099e-02 (5.8585e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.135 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.8257e-02 (5.9092e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6040e-02 (5.9047e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7729e-02 (5.8803e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8563e-02 (5.8781e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8737e-02 (5.8455e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2053e-02 (5.8510e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.2898e-02 (5.8206e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5012e-02 (5.8537e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.4148e-02 (5.8669e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4535e-02 (5.8543e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.2520e-02 (5.8845e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.2460e-02 (5.8860e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3243e-02 (5.8756e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.4270e-02 (5.8659e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.4524e-02 (5.8927e-02)	Acc@1  96.88 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.115 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3762e-02 (5.9146e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.6788794994354248
## e[51]       loss.backward (sum) time: 14.410186290740967
## e[51]      optimizer.step (sum) time: 8.028780460357666
## epoch[51] training(only) time: 47.26258301734924
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 8.0371e-01 (8.0371e-01)	Acc@1  78.00 ( 78.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.0703e+00 (1.1413e+00)	Acc@1  74.00 ( 73.45)	Acc@5  92.00 ( 92.36)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 1.0527e+00 (1.1001e+00)	Acc@1  75.00 ( 74.29)	Acc@5  93.00 ( 92.90)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5742e+00 (1.1238e+00)	Acc@1  68.00 ( 74.23)	Acc@5  89.00 ( 92.65)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.2578e+00 (1.1366e+00)	Acc@1  74.00 ( 74.05)	Acc@5  94.00 ( 92.83)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.3877e+00 (1.1397e+00)	Acc@1  67.00 ( 73.84)	Acc@5  91.00 ( 92.73)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.0781e+00 (1.1141e+00)	Acc@1  75.00 ( 73.87)	Acc@5  96.00 ( 93.15)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3760e+00 (1.1156e+00)	Acc@1  69.00 ( 73.69)	Acc@5  90.00 ( 93.24)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4531e+00 (1.1188e+00)	Acc@1  76.00 ( 73.67)	Acc@5  89.00 ( 93.11)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4697e+00 (1.1059e+00)	Acc@1  64.00 ( 73.74)	Acc@5  91.00 ( 93.26)
 * Acc@1 74.000 Acc@5 93.270
### epoch[51] execution time: 52.21596622467041
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.274 ( 0.274)	Data  0.142 ( 0.142)	Loss 4.7089e-02 (4.7089e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.120 ( 0.134)	Data  0.001 ( 0.014)	Loss 5.0323e-02 (5.6144e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.116 ( 0.127)	Data  0.001 ( 0.008)	Loss 3.1860e-02 (5.4587e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.005)	Loss 7.8491e-02 (5.5000e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 7.2205e-02 (5.5262e-02)	Acc@1  96.88 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.5248e-02 (5.5151e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.6783e-02 (5.5836e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.4830e-02 (5.5307e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.8055e-02 (5.5293e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.9143e-02 (5.4857e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.3069e-02 (5.4718e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.2286e-02 (5.5327e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.4504e-02 (5.5319e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0812e-02 (5.5550e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1340e-02 (5.5289e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8350e-02 (5.4971e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4250e-02 (5.5012e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9174e-02 (5.5400e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7821e-02 (5.5237e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7760e-02 (5.5347e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6356e-02 (5.5470e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6458e-02 (5.5607e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3417e-02 (5.5708e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.9182e-02 (5.6111e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.3384e-02 (5.6258e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0070e-02 (5.6035e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.6509e-02 (5.5666e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.9072e-02 (5.5833e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0181e-02 (5.5763e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8187e-02 (5.5681e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1523e-02 (5.5828e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9764e-02 (5.5916e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.7872e-02 (5.5829e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.5571e-02 (5.6056e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1504e-02 (5.5859e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7394e-02 (5.6004e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8065e-02 (5.5936e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.3955e-02 (5.5887e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9814e-02 (5.5743e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.7810e-02 (5.5693e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.6895339488983154
## e[52]       loss.backward (sum) time: 14.330296754837036
## e[52]      optimizer.step (sum) time: 8.160578966140747
## epoch[52] training(only) time: 47.335381746292114
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 8.5303e-01 (8.5303e-01)	Acc@1  78.00 ( 78.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.1191e+00 (1.1489e+00)	Acc@1  74.00 ( 73.55)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.0615e+00 (1.1032e+00)	Acc@1  75.00 ( 74.14)	Acc@5  93.00 ( 93.00)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.5449e+00 (1.1262e+00)	Acc@1  69.00 ( 74.03)	Acc@5  91.00 ( 92.81)
Test: [ 40/100]	Time  0.058 ( 0.051)	Loss 1.2480e+00 (1.1375e+00)	Acc@1  73.00 ( 73.78)	Acc@5  94.00 ( 93.05)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.3486e+00 (1.1412e+00)	Acc@1  70.00 ( 73.71)	Acc@5  91.00 ( 92.88)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0664e+00 (1.1176e+00)	Acc@1  76.00 ( 73.77)	Acc@5  96.00 ( 93.25)
Test: [ 70/100]	Time  0.049 ( 0.049)	Loss 1.4258e+00 (1.1219e+00)	Acc@1  68.00 ( 73.68)	Acc@5  90.00 ( 93.32)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.4756e+00 (1.1253e+00)	Acc@1  76.00 ( 73.68)	Acc@5  90.00 ( 93.22)
Test: [ 90/100]	Time  0.056 ( 0.049)	Loss 1.5254e+00 (1.1120e+00)	Acc@1  65.00 ( 73.79)	Acc@5  90.00 ( 93.30)
 * Acc@1 74.000 Acc@5 93.270
### epoch[52] execution time: 52.30335998535156
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.287 ( 0.287)	Data  0.152 ( 0.152)	Loss 4.5807e-02 (4.5807e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.121 ( 0.135)	Data  0.001 ( 0.015)	Loss 5.5634e-02 (5.8624e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 7.0496e-02 (5.6934e-02)	Acc@1  96.88 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.5807e-02 (5.2622e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.8004e-02 (5.1448e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.129 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.6173e-02 (5.0911e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.9866e-02 (5.0885e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.0975e-02 (5.1836e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4790e-02 (5.2209e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.9927e-02 (5.3259e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.6863e-02 (5.4478e-02)	Acc@1  97.66 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6204e-02 (5.5349e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.9540e-02 (5.5247e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0446e-02 (5.6135e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4230e-02 (5.6137e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6631e-02 (5.5756e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5450e-02 (5.6081e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3986e-02 (5.6049e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7709e-02 (5.6191e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9398e-02 (5.6039e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1880e-02 (5.6030e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6671e-02 (5.5876e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0771e-02 (5.5814e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8411e-02 (5.6080e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0730e-02 (5.5984e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3630e-02 (5.5661e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7373e-02 (5.5535e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9398e-02 (5.5383e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3354e-02 (5.5433e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1809e-02 (5.5509e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.8258e-02 (5.5371e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3732e-02 (5.5396e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3640e-02 (5.5440e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9326e-02 (5.5511e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.6824e-02 (5.5518e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9814e-02 (5.5446e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7943e-02 (5.5433e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.2083e-02 (5.5401e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2175e-02 (5.5498e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.6467e-02 (5.5467e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.6837787628173828
## e[53]       loss.backward (sum) time: 14.383102655410767
## e[53]      optimizer.step (sum) time: 8.035295724868774
## epoch[53] training(only) time: 47.22112822532654
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 8.2275e-01 (8.2275e-01)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.045 ( 0.059)	Loss 1.0889e+00 (1.1427e+00)	Acc@1  73.00 ( 73.45)	Acc@5  92.00 ( 92.45)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.1182e+00 (1.1059e+00)	Acc@1  72.00 ( 74.05)	Acc@5  92.00 ( 93.00)
Test: [ 30/100]	Time  0.050 ( 0.052)	Loss 1.5098e+00 (1.1296e+00)	Acc@1  68.00 ( 74.03)	Acc@5  91.00 ( 92.81)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 1.2549e+00 (1.1383e+00)	Acc@1  74.00 ( 73.98)	Acc@5  94.00 ( 93.00)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4111e+00 (1.1438e+00)	Acc@1  69.00 ( 73.73)	Acc@5  91.00 ( 92.84)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0518e+00 (1.1175e+00)	Acc@1  78.00 ( 73.85)	Acc@5  96.00 ( 93.23)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.4248e+00 (1.1208e+00)	Acc@1  69.00 ( 73.70)	Acc@5  90.00 ( 93.25)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.4355e+00 (1.1236e+00)	Acc@1  75.00 ( 73.72)	Acc@5  91.00 ( 93.21)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5381e+00 (1.1107e+00)	Acc@1  64.00 ( 73.78)	Acc@5  90.00 ( 93.33)
 * Acc@1 73.970 Acc@5 93.320
### epoch[53] execution time: 52.22624588012695
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.294 ( 0.294)	Data  0.155 ( 0.155)	Loss 4.5929e-02 (4.5929e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.119 ( 0.137)	Data  0.001 ( 0.015)	Loss 4.2419e-02 (5.3827e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.008)	Loss 6.8359e-02 (5.6431e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.122 ( 0.126)	Data  0.001 ( 0.006)	Loss 7.1411e-02 (5.6975e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 5.1636e-02 (5.5814e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.6366e-02 (5.4577e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.1779e-02 (5.4787e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.8176e-02 (5.5479e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.5562e-02 (5.5001e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.0974e-02 (5.4644e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.6040e-02 (5.4891e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4952e-02 (5.4259e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6926e-02 (5.3984e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2275e-02 (5.4479e-02)	Acc@1  96.88 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4830e-02 (5.4746e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7872e-02 (5.4207e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7863e-02 (5.4666e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1951e-02 (5.5018e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3843e-02 (5.5465e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6833e-02 (5.5594e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4098e-02 (5.5757e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6234e-02 (5.5597e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6285e-02 (5.5324e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7384e-02 (5.5271e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4697e-02 (5.5202e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4779e-02 (5.5065e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7201e-02 (5.4843e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4454e-02 (5.4737e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2195e-02 (5.4819e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.2073e-02 (5.4800e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1534e-02 (5.4760e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1025e-02 (5.4409e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5491e-02 (5.4405e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0629e-02 (5.4554e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.9957e-02 (5.4550e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3091e-02 (5.4552e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1249e-02 (5.4515e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3610e-02 (5.4452e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.8380e-02 (5.4480e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.114 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4861e-02 (5.4525e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.6864933967590332
## e[54]       loss.backward (sum) time: 14.337604999542236
## e[54]      optimizer.step (sum) time: 8.08247709274292
## epoch[54] training(only) time: 47.245312213897705
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 8.2080e-01 (8.2080e-01)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.1191e+00 (1.1522e+00)	Acc@1  74.00 ( 73.09)	Acc@5  92.00 ( 92.55)
Test: [ 20/100]	Time  0.050 ( 0.054)	Loss 1.0996e+00 (1.1170e+00)	Acc@1  73.00 ( 73.90)	Acc@5  91.00 ( 92.95)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 1.5381e+00 (1.1379e+00)	Acc@1  69.00 ( 73.68)	Acc@5  89.00 ( 92.74)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 1.2607e+00 (1.1497e+00)	Acc@1  75.00 ( 73.66)	Acc@5  94.00 ( 93.00)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.3887e+00 (1.1539e+00)	Acc@1  67.00 ( 73.41)	Acc@5  91.00 ( 92.88)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0488e+00 (1.1281e+00)	Acc@1  73.00 ( 73.41)	Acc@5  97.00 ( 93.26)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4336e+00 (1.1290e+00)	Acc@1  69.00 ( 73.45)	Acc@5  90.00 ( 93.31)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4424e+00 (1.1330e+00)	Acc@1  75.00 ( 73.51)	Acc@5  91.00 ( 93.26)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5176e+00 (1.1216e+00)	Acc@1  64.00 ( 73.54)	Acc@5  93.00 ( 93.37)
 * Acc@1 73.770 Acc@5 93.380
### epoch[54] execution time: 52.215086221694946
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.290 ( 0.290)	Data  0.145 ( 0.145)	Loss 5.6458e-02 (5.6458e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.118 ( 0.136)	Data  0.001 ( 0.014)	Loss 5.0568e-02 (5.5223e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.1199e-02 (5.3395e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.124 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.7964e-02 (5.2347e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 6.7810e-02 (5.4191e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.4647e-02 (5.3946e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.2826e-02 (5.3485e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.8096e-02 (5.4005e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.2205e-02 (5.4276e-02)	Acc@1  97.66 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.6753e-02 (5.4776e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9846e-02 (5.3727e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6641e-02 (5.3503e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3427e-02 (5.2697e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1616e-02 (5.2033e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5227e-02 (5.1794e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 ( 99.99)
Epoch: [55][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8665e-02 (5.1900e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 ( 99.99)
Epoch: [55][160/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6885e-02 (5.2557e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3314e-02 (5.1959e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5145e-02 (5.2316e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1859e-02 (5.2425e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8624e-02 (5.2554e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0344e-02 (5.2347e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1331e-02 (5.2103e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7668e-02 (5.2051e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1340e-02 (5.2222e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9683e-02 (5.2001e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 9.9854e-02 (5.2176e-02)	Acc@1  96.88 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5074e-02 (5.2401e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.8971e-02 (5.2214e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.3040e-02 (5.2354e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.4575e-02 (5.2617e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8523e-02 (5.2496e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1279e-02 (5.2657e-02)	Acc@1  97.66 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.7939e-02 (5.2521e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5929e-02 (5.2456e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1971e-02 (5.2212e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.5439e-02 (5.2079e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9398e-02 (5.1996e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1840e-02 (5.2159e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.102 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.0773e-01 (5.2277e-02)	Acc@1  96.25 ( 99.27)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.6871552467346191
## e[55]       loss.backward (sum) time: 14.308131694793701
## e[55]      optimizer.step (sum) time: 8.137086153030396
## epoch[55] training(only) time: 47.202332496643066
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 8.3691e-01 (8.3691e-01)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.0752e+00 (1.1510e+00)	Acc@1  75.00 ( 74.00)	Acc@5  92.00 ( 92.55)
Test: [ 20/100]	Time  0.045 ( 0.055)	Loss 1.0889e+00 (1.1162e+00)	Acc@1  75.00 ( 74.19)	Acc@5  91.00 ( 92.95)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5664e+00 (1.1370e+00)	Acc@1  69.00 ( 73.87)	Acc@5  89.00 ( 92.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.2812e+00 (1.1510e+00)	Acc@1  76.00 ( 73.66)	Acc@5  94.00 ( 92.98)
Test: [ 50/100]	Time  0.053 ( 0.051)	Loss 1.4150e+00 (1.1554e+00)	Acc@1  70.00 ( 73.43)	Acc@5  91.00 ( 92.76)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0625e+00 (1.1272e+00)	Acc@1  74.00 ( 73.59)	Acc@5  96.00 ( 93.20)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4268e+00 (1.1286e+00)	Acc@1  69.00 ( 73.63)	Acc@5  90.00 ( 93.30)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4834e+00 (1.1331e+00)	Acc@1  76.00 ( 73.75)	Acc@5  90.00 ( 93.17)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5146e+00 (1.1204e+00)	Acc@1  64.00 ( 73.88)	Acc@5  91.00 ( 93.30)
 * Acc@1 74.080 Acc@5 93.320
### epoch[55] execution time: 52.210092544555664
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.282 ( 0.282)	Data  0.146 ( 0.146)	Loss 5.1208e-02 (5.1208e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.117 ( 0.134)	Data  0.001 ( 0.014)	Loss 3.9795e-02 (4.5901e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.121 ( 0.128)	Data  0.001 ( 0.008)	Loss 3.4454e-02 (5.0005e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.0649e-02 (5.0988e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.97)
Epoch: [56][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.2999e-02 (5.1866e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.6519e-02 (5.1686e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 60/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.7292e-02 (5.0491e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.1462e-02 (5.1089e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.0192e-02 (5.0315e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.9459e-02 (5.1300e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [56][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.6152e-02 (5.1555e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [56][110/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6173e-02 (5.1194e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [56][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7598e-02 (5.1035e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [56][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7598e-02 (5.0879e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [56][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1493e-02 (5.0811e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [56][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3864e-02 (5.0893e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [56][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1962e-02 (5.1003e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9001e-02 (5.0542e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0507e-02 (5.0829e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2684e-02 (5.0866e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2511e-02 (5.0727e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3375e-02 (5.0768e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8624e-02 (5.0493e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4006e-02 (5.0685e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6499e-02 (5.0921e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2297e-02 (5.1140e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1962e-02 (5.1231e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7302e-02 (5.1352e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5156e-02 (5.1215e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9856e-02 (5.1045e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9500e-02 (5.1041e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7668e-02 (5.1076e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5857e-02 (5.1310e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.3853e-02 (5.1503e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1931e-02 (5.1487e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1077e-02 (5.1621e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.4307e-02 (5.1479e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.2307e-02 (5.1516e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1554e-02 (5.1702e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.7942e-02 (5.1632e-02)	Acc@1  98.75 ( 99.32)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.6870932579040527
## e[56]       loss.backward (sum) time: 14.320174932479858
## e[56]      optimizer.step (sum) time: 8.098348140716553
## epoch[56] training(only) time: 47.22032427787781
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 8.1152e-01 (8.1152e-01)	Acc@1  79.00 ( 79.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.045 ( 0.062)	Loss 1.1094e+00 (1.1541e+00)	Acc@1  74.00 ( 73.73)	Acc@5  93.00 ( 92.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0547e+00 (1.1144e+00)	Acc@1  74.00 ( 74.24)	Acc@5  92.00 ( 93.24)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5801e+00 (1.1393e+00)	Acc@1  70.00 ( 73.94)	Acc@5  89.00 ( 92.94)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2812e+00 (1.1544e+00)	Acc@1  75.00 ( 73.95)	Acc@5  93.00 ( 93.07)
Test: [ 50/100]	Time  0.050 ( 0.050)	Loss 1.4365e+00 (1.1597e+00)	Acc@1  69.00 ( 73.67)	Acc@5  91.00 ( 92.84)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0654e+00 (1.1320e+00)	Acc@1  77.00 ( 73.79)	Acc@5  96.00 ( 93.21)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4385e+00 (1.1347e+00)	Acc@1  69.00 ( 73.75)	Acc@5  90.00 ( 93.31)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4521e+00 (1.1384e+00)	Acc@1  74.00 ( 73.74)	Acc@5  91.00 ( 93.23)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5254e+00 (1.1260e+00)	Acc@1  65.00 ( 73.81)	Acc@5  90.00 ( 93.31)
 * Acc@1 74.060 Acc@5 93.310
### epoch[56] execution time: 52.18113827705383
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.299 ( 0.299)	Data  0.162 ( 0.162)	Loss 5.9052e-02 (5.9052e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.119 ( 0.137)	Data  0.001 ( 0.016)	Loss 5.3650e-02 (5.1838e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.118 ( 0.128)	Data  0.001 ( 0.009)	Loss 3.6194e-02 (4.8548e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.006)	Loss 6.1707e-02 (4.8391e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.2267e-02 (4.7736e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.2521e-02 (4.7095e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.7913e-02 (4.7589e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.7648e-02 (4.7322e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.6671e-02 (4.7327e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.2196e-02 (4.7271e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.1788e-02 (4.7574e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1403e-02 (4.8165e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5461e-02 (4.7731e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (4.7804e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7537e-02 (4.8410e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3162e-02 (4.8368e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4800e-02 (4.8347e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5009e-02 (4.7986e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7485e-02 (4.8195e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3904e-02 (4.8217e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7119e-02 (4.8316e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9326e-02 (4.8052e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6051e-02 (4.8176e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8910e-02 (4.8289e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4312e-02 (4.8124e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5328e-02 (4.8068e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6274e-02 (4.8286e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0374e-02 (4.8593e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 2.8854e-02 (4.8679e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.4912e-02 (4.8574e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.4626e-02 (4.8700e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3630e-02 (4.8621e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 7.0007e-02 (4.8688e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.3997e-02 (4.8497e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.135 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9459e-02 (4.8577e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9978e-02 (4.8553e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.8167e-02 (4.8634e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.4657e-02 (4.8891e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.124 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0594e-02 (4.8859e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.110 ( 0.120)	Data  0.001 ( 0.001)	Loss 1.1353e-01 (4.8996e-02)	Acc@1  96.25 ( 99.41)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.6984930038452148
## e[57]       loss.backward (sum) time: 14.242138147354126
## e[57]      optimizer.step (sum) time: 8.068819046020508
## epoch[57] training(only) time: 47.140156984329224
# Switched to evaluate mode...
Test: [  0/100]	Time  0.198 ( 0.198)	Loss 8.4033e-01 (8.4033e-01)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.1250e+00 (1.1496e+00)	Acc@1  76.00 ( 73.82)	Acc@5  92.00 ( 92.64)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0674e+00 (1.1112e+00)	Acc@1  75.00 ( 74.29)	Acc@5  92.00 ( 93.14)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 1.5762e+00 (1.1371e+00)	Acc@1  69.00 ( 73.90)	Acc@5  88.00 ( 92.81)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2852e+00 (1.1505e+00)	Acc@1  74.00 ( 73.76)	Acc@5  94.00 ( 93.10)
Test: [ 50/100]	Time  0.060 ( 0.051)	Loss 1.4346e+00 (1.1546e+00)	Acc@1  69.00 ( 73.59)	Acc@5  91.00 ( 92.94)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0879e+00 (1.1275e+00)	Acc@1  74.00 ( 73.75)	Acc@5  96.00 ( 93.33)
Test: [ 70/100]	Time  0.051 ( 0.050)	Loss 1.4600e+00 (1.1304e+00)	Acc@1  69.00 ( 73.65)	Acc@5  90.00 ( 93.37)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.4561e+00 (1.1362e+00)	Acc@1  76.00 ( 73.78)	Acc@5  90.00 ( 93.22)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.4873e+00 (1.1233e+00)	Acc@1  66.00 ( 73.87)	Acc@5  91.00 ( 93.29)
 * Acc@1 74.090 Acc@5 93.300
### epoch[57] execution time: 52.1526358127594
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.291 ( 0.291)	Data  0.125 ( 0.125)	Loss 4.6051e-02 (4.6051e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.012)	Loss 4.9927e-02 (4.5289e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.007)	Loss 4.5532e-02 (4.4864e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.005)	Loss 4.2114e-02 (4.6894e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.8147e-02 (4.5480e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.9988e-02 (4.5979e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.5695e-02 (4.5900e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.0802e-02 (4.6151e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5461e-02 (4.6606e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7506e-02 (4.5826e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2856e-02 (4.5722e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5370e-02 (4.5622e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8624e-02 (4.5094e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6528e-02 (4.5365e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1412e-02 (4.5545e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5410e-02 (4.5826e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2887e-02 (4.6363e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4368e-02 (4.6894e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5522e-02 (4.6835e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9602e-02 (4.6390e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8676e-02 (4.6788e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6346e-02 (4.6599e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5237e-02 (4.6949e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.8793e-02 (4.6820e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.7323e-02 (4.6811e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.0106e-02 (4.6786e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.2469e-02 (4.6931e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4779e-02 (4.7048e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5959e-02 (4.7475e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0293e-02 (4.7232e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.8594e-02 (4.7462e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.134 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.9633e-02 (4.7314e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1005e-02 (4.7348e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.9633e-02 (4.7277e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7791e-02 (4.7383e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5044e-02 (4.7339e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.6732e-02 (4.7455e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.4026e-02 (4.7463e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.8258e-02 (4.7622e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.1901e-02 (4.7574e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.6833310127258301
## e[58]       loss.backward (sum) time: 14.382351875305176
## e[58]      optimizer.step (sum) time: 7.997810363769531
## epoch[58] training(only) time: 47.204179525375366
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 8.1885e-01 (8.1885e-01)	Acc@1  77.00 ( 77.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.0957e+00 (1.1613e+00)	Acc@1  74.00 ( 74.09)	Acc@5  92.00 ( 92.82)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.0967e+00 (1.1201e+00)	Acc@1  72.00 ( 74.43)	Acc@5  92.00 ( 92.95)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5771e+00 (1.1444e+00)	Acc@1  69.00 ( 74.10)	Acc@5  89.00 ( 92.58)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.3164e+00 (1.1584e+00)	Acc@1  73.00 ( 73.85)	Acc@5  94.00 ( 92.73)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.4961e+00 (1.1653e+00)	Acc@1  67.00 ( 73.53)	Acc@5  91.00 ( 92.61)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0645e+00 (1.1354e+00)	Acc@1  75.00 ( 73.57)	Acc@5  97.00 ( 93.10)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4932e+00 (1.1377e+00)	Acc@1  69.00 ( 73.49)	Acc@5  90.00 ( 93.21)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4893e+00 (1.1415e+00)	Acc@1  74.00 ( 73.51)	Acc@5  91.00 ( 93.12)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5244e+00 (1.1291e+00)	Acc@1  63.00 ( 73.57)	Acc@5  90.00 ( 93.23)
 * Acc@1 73.770 Acc@5 93.260
### epoch[58] execution time: 52.17937517166138
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.272 ( 0.272)	Data  0.141 ( 0.141)	Loss 3.1342e-02 (3.1342e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.119 ( 0.134)	Data  0.001 ( 0.014)	Loss 4.0924e-02 (4.3654e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.121 ( 0.127)	Data  0.001 ( 0.008)	Loss 2.9526e-02 (4.3525e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.1229e-02 (4.5053e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.1748e-02 (4.4928e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.7119e-02 (4.6403e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.116 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4027e-02 (4.5839e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.3813e-02 (4.6428e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.5135e-02 (4.5924e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.5317e-02 (4.5716e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0975e-02 (4.5678e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5624e-02 (4.6250e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8553e-02 (4.6018e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9978e-02 (4.5689e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0894e-02 (4.6017e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1128e-02 (4.5912e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2754e-02 (4.6510e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3030e-02 (4.6309e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3030e-02 (4.6148e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1250e-02 (4.5889e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7476e-02 (4.5953e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2236e-02 (4.6250e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6040e-02 (4.6265e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0486e-02 (4.6402e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8542e-02 (4.6317e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1779e-02 (4.6583e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8503e-02 (4.6591e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.8086e-02 (4.6692e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2023e-02 (4.6752e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.6671e-02 (4.6767e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7150e-02 (4.6795e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.9984e-02 (4.6619e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.6713e-02 (4.6693e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.2013e-02 (4.6736e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.9083e-02 (4.6531e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9185e-02 (4.6593e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.2216e-02 (4.6556e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1311e-02 (4.6682e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.9397e-02 (4.6598e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.106 ( 0.120)	Data  0.001 ( 0.001)	Loss 7.1106e-02 (4.6693e-02)	Acc@1  98.75 ( 99.40)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.6899409294128418
## e[59]       loss.backward (sum) time: 14.30160903930664
## e[59]      optimizer.step (sum) time: 8.105677843093872
## epoch[59] training(only) time: 47.204432010650635
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 8.4619e-01 (8.4619e-01)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.054 ( 0.061)	Loss 1.1025e+00 (1.1549e+00)	Acc@1  75.00 ( 73.36)	Acc@5  94.00 ( 92.64)
Test: [ 20/100]	Time  0.045 ( 0.055)	Loss 1.0869e+00 (1.1158e+00)	Acc@1  74.00 ( 74.05)	Acc@5  92.00 ( 92.95)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5820e+00 (1.1386e+00)	Acc@1  68.00 ( 73.77)	Acc@5  89.00 ( 92.58)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 1.2949e+00 (1.1532e+00)	Acc@1  75.00 ( 73.68)	Acc@5  94.00 ( 92.80)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4385e+00 (1.1574e+00)	Acc@1  68.00 ( 73.47)	Acc@5  91.00 ( 92.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0625e+00 (1.1289e+00)	Acc@1  76.00 ( 73.54)	Acc@5  97.00 ( 93.20)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4717e+00 (1.1305e+00)	Acc@1  68.00 ( 73.52)	Acc@5  90.00 ( 93.27)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5039e+00 (1.1354e+00)	Acc@1  76.00 ( 73.58)	Acc@5  91.00 ( 93.15)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5537e+00 (1.1236e+00)	Acc@1  62.00 ( 73.69)	Acc@5  90.00 ( 93.24)
 * Acc@1 73.930 Acc@5 93.240
### epoch[59] execution time: 52.18062162399292
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.285 ( 0.285)	Data  0.146 ( 0.146)	Loss 7.1106e-02 (7.1106e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.123 ( 0.134)	Data  0.001 ( 0.014)	Loss 3.9093e-02 (4.1584e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 6.6589e-02 (4.3448e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.127 ( 0.125)	Data  0.001 ( 0.006)	Loss 6.9336e-02 (4.4595e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.5715e-02 (4.6536e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.7476e-02 (4.5985e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.128 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.0568e-02 (4.6240e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.0964e-02 (4.6785e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.8519e-02 (4.6058e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.1677e-02 (4.5888e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4241e-02 (4.5191e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1290e-02 (4.4295e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8716e-02 (4.4239e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (4.4469e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4576e-02 (4.4749e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6711e-02 (4.4844e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9814e-02 (4.4966e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9459e-02 (4.4958e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5776e-02 (4.5074e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1555e-02 (4.4958e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7842e-02 (4.5039e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6804e-02 (4.4797e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6743e-02 (4.4563e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.116 ( 0.120)	Data  0.001 ( 0.002)	Loss 6.4087e-02 (4.4678e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.127 ( 0.120)	Data  0.001 ( 0.002)	Loss 4.5166e-02 (4.4443e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 5.5847e-02 (4.4769e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4535e-02 (4.4927e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.6926e-02 (4.5078e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5126e-02 (4.5145e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.002)	Loss 3.5736e-02 (4.5091e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0640e-02 (4.4963e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 8.4534e-02 (4.5184e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.120 ( 0.120)	Data  0.001 ( 0.001)	Loss 4.9042e-02 (4.4976e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.119 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.7343e-02 (4.5055e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.131 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.9763e-02 (4.5036e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.118 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.9062e-02 (4.4972e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.121 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.7781e-02 (4.4922e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.128 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.1514e-02 (4.4844e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.117 ( 0.120)	Data  0.001 ( 0.001)	Loss 6.2988e-02 (4.4910e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.2166e-02 (4.4944e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.6890552043914795
## e[60]       loss.backward (sum) time: 14.305458307266235
## e[60]      optimizer.step (sum) time: 8.030542612075806
## epoch[60] training(only) time: 47.125170946121216
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 8.2764e-01 (8.2764e-01)	Acc@1  80.00 ( 80.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.055 ( 0.062)	Loss 1.1143e+00 (1.1538e+00)	Acc@1  74.00 ( 73.82)	Acc@5  93.00 ( 92.45)
Test: [ 20/100]	Time  0.045 ( 0.054)	Loss 1.0635e+00 (1.1137e+00)	Acc@1  74.00 ( 74.24)	Acc@5  93.00 ( 93.05)
Test: [ 30/100]	Time  0.053 ( 0.052)	Loss 1.5947e+00 (1.1367e+00)	Acc@1  70.00 ( 73.97)	Acc@5  90.00 ( 92.77)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2705e+00 (1.1526e+00)	Acc@1  75.00 ( 73.80)	Acc@5  94.00 ( 92.93)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.4355e+00 (1.1582e+00)	Acc@1  69.00 ( 73.57)	Acc@5  91.00 ( 92.75)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.0654e+00 (1.1314e+00)	Acc@1  75.00 ( 73.57)	Acc@5  96.00 ( 93.16)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4375e+00 (1.1330e+00)	Acc@1  68.00 ( 73.54)	Acc@5  90.00 ( 93.21)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 1.4824e+00 (1.1359e+00)	Acc@1  74.00 ( 73.65)	Acc@5  92.00 ( 93.15)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5342e+00 (1.1241e+00)	Acc@1  61.00 ( 73.71)	Acc@5  91.00 ( 93.25)
 * Acc@1 73.920 Acc@5 93.300
### epoch[60] execution time: 52.12644362449646
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.280 ( 0.280)	Data  0.149 ( 0.149)	Loss 4.4586e-02 (4.4586e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.014)	Loss 3.4698e-02 (4.6270e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.6753e-02 (4.4197e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.7262e-02 (4.4007e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.7964e-02 (4.3467e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.0899e-02 (4.2839e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.6581e-02 (4.3720e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 70/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.8279e-02 (4.4070e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.3793e-02 (4.4094e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [61][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.8177e-02 (4.4265e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [61][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.9570e-02 (4.4550e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [61][110/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.0161e-02 (4.3779e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [61][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5380e-02 (4.3490e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [61][130/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6743e-02 (4.3777e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [61][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4973e-02 (4.4343e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [61][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3722e-02 (4.4214e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [61][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5807e-02 (4.4125e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [61][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4312e-02 (4.4325e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [61][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0258e-02 (4.4472e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [61][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3182e-02 (4.4586e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [61][200/391]	Time  0.136 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6285e-02 (4.4528e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [61][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7109e-02 (4.4976e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [61][220/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5206e-02 (4.5008e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [61][230/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6650e-02 (4.5164e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [61][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4983e-02 (4.5553e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [61][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2074e-02 (4.5367e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [61][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8798e-02 (4.5315e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [61][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7882e-02 (4.5173e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [61][280/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4363e-02 (4.5124e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [61][290/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8940e-02 (4.4809e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [61][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4321e-02 (4.4800e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [61][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6163e-02 (4.4917e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [61][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.2135e-02 (4.4786e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.6556e-02 (4.4755e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.135 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.6732e-02 (4.4795e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8035e-02 (4.4695e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7974e-02 (4.4797e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4626e-02 (4.4765e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.1057e-02 (4.4688e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5918e-02 (4.4613e-02)	Acc@1  98.75 ( 99.45)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.6901443004608154
## e[61]       loss.backward (sum) time: 14.353731632232666
## e[61]      optimizer.step (sum) time: 8.115776538848877
## epoch[61] training(only) time: 47.250848054885864
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 8.0859e-01 (8.0859e-01)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.0859e+00 (1.1531e+00)	Acc@1  75.00 ( 73.27)	Acc@5  93.00 ( 92.27)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 1.0674e+00 (1.1144e+00)	Acc@1  74.00 ( 74.14)	Acc@5  92.00 ( 92.86)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5879e+00 (1.1404e+00)	Acc@1  68.00 ( 73.84)	Acc@5  89.00 ( 92.65)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 1.2852e+00 (1.1536e+00)	Acc@1  74.00 ( 73.83)	Acc@5  94.00 ( 92.90)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.4268e+00 (1.1582e+00)	Acc@1  68.00 ( 73.57)	Acc@5  91.00 ( 92.69)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.0674e+00 (1.1308e+00)	Acc@1  75.00 ( 73.52)	Acc@5  97.00 ( 93.10)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 1.4434e+00 (1.1328e+00)	Acc@1  69.00 ( 73.56)	Acc@5  90.00 ( 93.20)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.4795e+00 (1.1372e+00)	Acc@1  74.00 ( 73.63)	Acc@5  90.00 ( 93.06)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5508e+00 (1.1247e+00)	Acc@1  65.00 ( 73.77)	Acc@5  92.00 ( 93.16)
 * Acc@1 74.010 Acc@5 93.230
### epoch[61] execution time: 52.211596965789795
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.286 ( 0.286)	Data  0.151 ( 0.151)	Loss 6.2042e-02 (6.2042e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.122 ( 0.136)	Data  0.001 ( 0.015)	Loss 2.8625e-02 (4.7968e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.121 ( 0.128)	Data  0.001 ( 0.008)	Loss 5.4626e-02 (4.4596e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.122 ( 0.125)	Data  0.001 ( 0.006)	Loss 5.0140e-02 (4.3612e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.4393e-02 (4.2963e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.8971e-02 (4.2894e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.128 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.5359e-02 (4.2703e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.4891e-02 (4.3592e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.4647e-02 (4.3364e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.132 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.3721e-02 (4.3607e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4058e-02 (4.3638e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.7267e-02 (4.3325e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4515e-02 (4.2963e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4871e-02 (4.3253e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0934e-02 (4.3310e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0771e-02 (4.3847e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5837e-02 (4.3943e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.8970e-02 (4.4336e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 1.9730e-02 (4.4223e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5248e-02 (4.4147e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5674e-02 (4.4559e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6814e-02 (4.4326e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1107e-02 (4.4290e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1006e-02 (4.4466e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5085e-02 (4.4593e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4760e-02 (4.4796e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1229e-02 (4.4669e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3091e-02 (4.4689e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4769e-02 (4.4561e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.0466e-02 (4.4639e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.6346e-02 (4.4652e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.8308e-02 (4.4819e-02)	Acc@1  96.88 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 1.8082e-02 (4.4851e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7852e-02 (4.4927e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.3009e-02 (4.5267e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.6661e-02 (4.5276e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.0100e-02 (4.5110e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5898e-02 (4.4903e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.136 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1971e-02 (4.4936e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.110 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.4482e-02 (4.4811e-02)	Acc@1  97.50 ( 99.46)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.6919069290161133
## e[62]       loss.backward (sum) time: 14.308951377868652
## e[62]      optimizer.step (sum) time: 8.135738134384155
## epoch[62] training(only) time: 47.28296637535095
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 8.1348e-01 (8.1348e-01)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.058 ( 0.061)	Loss 1.0977e+00 (1.1515e+00)	Acc@1  75.00 ( 73.64)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.045 ( 0.055)	Loss 1.0508e+00 (1.1095e+00)	Acc@1  77.00 ( 74.43)	Acc@5  93.00 ( 92.86)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5684e+00 (1.1354e+00)	Acc@1  68.00 ( 74.06)	Acc@5  88.00 ( 92.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2754e+00 (1.1500e+00)	Acc@1  73.00 ( 73.85)	Acc@5  94.00 ( 92.80)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4521e+00 (1.1544e+00)	Acc@1  68.00 ( 73.71)	Acc@5  91.00 ( 92.71)
Test: [ 60/100]	Time  0.057 ( 0.050)	Loss 1.0615e+00 (1.1270e+00)	Acc@1  75.00 ( 73.67)	Acc@5  97.00 ( 93.13)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4697e+00 (1.1289e+00)	Acc@1  67.00 ( 73.56)	Acc@5  90.00 ( 93.24)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.4580e+00 (1.1331e+00)	Acc@1  75.00 ( 73.60)	Acc@5  92.00 ( 93.17)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5410e+00 (1.1216e+00)	Acc@1  63.00 ( 73.68)	Acc@5  91.00 ( 93.26)
 * Acc@1 73.950 Acc@5 93.310
### epoch[62] execution time: 52.25487399101257
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.282 ( 0.282)	Data  0.150 ( 0.150)	Loss 5.1239e-02 (5.1239e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.015)	Loss 3.7170e-02 (3.9913e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 4.9561e-02 (4.2633e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.0649e-02 (4.2899e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.5197e-02 (4.2747e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.0380e-02 (4.3724e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.6732e-02 (4.5250e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4424e-02 (4.5679e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4943e-02 (4.5576e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.2562e-02 (4.5405e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.9500e-02 (4.5300e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7119e-02 (4.5098e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4637e-02 (4.4880e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4250e-02 (4.4932e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4912e-02 (4.4754e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1525e-02 (4.4412e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1412e-02 (4.4605e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0161e-02 (4.4736e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8472e-02 (4.4808e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6779e-02 (4.4609e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5573e-02 (4.4826e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4261e-02 (4.4527e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9093e-02 (4.4306e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0289e-02 (4.4096e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.3191e-02 (4.4242e-02)	Acc@1  98.44 ( 99.53)	Acc@5  99.22 (100.00)
Epoch: [63][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9092e-02 (4.4330e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7272e-02 (4.4354e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7546e-02 (4.4497e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7109e-02 (4.4487e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9652e-02 (4.4333e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6438e-02 (4.4135e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8696e-02 (4.4079e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2856e-02 (4.4295e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.7026e-02 (4.4363e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.0375e-02 (4.4347e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9479e-02 (4.4340e-02)	Acc@1  98.44 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.7648e-02 (4.4384e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.9275e-02 (4.4438e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.6112e-02 (4.4388e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.108 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.0563e-02 (4.4452e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.691361665725708
## e[63]       loss.backward (sum) time: 14.301194190979004
## e[63]      optimizer.step (sum) time: 8.097315073013306
## epoch[63] training(only) time: 47.2301299571991
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 8.0908e-01 (8.0908e-01)	Acc@1  78.00 ( 78.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.050 ( 0.061)	Loss 1.1143e+00 (1.1625e+00)	Acc@1  74.00 ( 73.64)	Acc@5  93.00 ( 92.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0781e+00 (1.1204e+00)	Acc@1  72.00 ( 74.24)	Acc@5  93.00 ( 92.81)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5869e+00 (1.1420e+00)	Acc@1  69.00 ( 73.87)	Acc@5  89.00 ( 92.52)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 1.3018e+00 (1.1558e+00)	Acc@1  76.00 ( 73.83)	Acc@5  94.00 ( 92.76)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.4482e+00 (1.1613e+00)	Acc@1  67.00 ( 73.59)	Acc@5  91.00 ( 92.59)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0840e+00 (1.1333e+00)	Acc@1  76.00 ( 73.74)	Acc@5  97.00 ( 93.10)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4512e+00 (1.1338e+00)	Acc@1  68.00 ( 73.73)	Acc@5  90.00 ( 93.18)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4932e+00 (1.1372e+00)	Acc@1  73.00 ( 73.81)	Acc@5  90.00 ( 93.02)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5215e+00 (1.1252e+00)	Acc@1  64.00 ( 73.93)	Acc@5  90.00 ( 93.10)
 * Acc@1 74.160 Acc@5 93.110
### epoch[63] execution time: 52.19849967956543
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.288 ( 0.288)	Data  0.155 ( 0.155)	Loss 2.9175e-02 (2.9175e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.118 ( 0.137)	Data  0.001 ( 0.015)	Loss 4.4739e-02 (4.0364e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.008)	Loss 4.3457e-02 (3.9307e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.006)	Loss 4.4830e-02 (4.1099e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.005)	Loss 3.0991e-02 (4.0592e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.3650e-02 (4.3066e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.1899e-02 (4.3803e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.9892e-02 (4.3357e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.1077e-02 (4.3412e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.0883e-02 (4.3534e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.8126e-02 (4.3359e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0842e-02 (4.4041e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.5894e-02 (4.3936e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0710e-02 (4.3890e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2622e-02 (4.3960e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7323e-02 (4.3686e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7390e-02 (4.3984e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4607e-02 (4.3909e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6438e-02 (4.3912e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7115e-02 (4.3860e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0192e-02 (4.3967e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0293e-02 (4.4107e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4556e-02 (4.4169e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9764e-02 (4.4192e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7781e-02 (4.4087e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8492e-02 (4.4300e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1021e-02 (4.4014e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9133e-02 (4.4035e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5918e-02 (4.3992e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4169e-02 (4.3842e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.9541e-02 (4.3744e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.3173e-02 (4.3645e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0751e-02 (4.3844e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.0701e-02 (4.3756e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.6356e-02 (4.3737e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.2246e-02 (4.3724e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5715e-02 (4.3685e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5685e-02 (4.3687e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.4026e-02 (4.3856e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8340e-02 (4.3866e-02)	Acc@1  98.75 ( 99.52)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.680058479309082
## e[64]       loss.backward (sum) time: 14.38813829421997
## e[64]      optimizer.step (sum) time: 8.067281723022461
## epoch[64] training(only) time: 47.29279065132141
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 8.3545e-01 (8.3545e-01)	Acc@1  80.00 ( 80.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.0957e+00 (1.1529e+00)	Acc@1  74.00 ( 73.64)	Acc@5  92.00 ( 92.45)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.0547e+00 (1.1134e+00)	Acc@1  77.00 ( 74.48)	Acc@5  92.00 ( 92.95)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.5459e+00 (1.1345e+00)	Acc@1  67.00 ( 74.10)	Acc@5  89.00 ( 92.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2520e+00 (1.1496e+00)	Acc@1  75.00 ( 74.02)	Acc@5  94.00 ( 92.90)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4229e+00 (1.1545e+00)	Acc@1  69.00 ( 73.80)	Acc@5  91.00 ( 92.75)
Test: [ 60/100]	Time  0.050 ( 0.050)	Loss 1.0703e+00 (1.1276e+00)	Acc@1  76.00 ( 73.77)	Acc@5  96.00 ( 93.13)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4600e+00 (1.1289e+00)	Acc@1  69.00 ( 73.77)	Acc@5  90.00 ( 93.23)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4736e+00 (1.1338e+00)	Acc@1  75.00 ( 73.81)	Acc@5  92.00 ( 93.12)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 1.5527e+00 (1.1220e+00)	Acc@1  63.00 ( 73.90)	Acc@5  91.00 ( 93.25)
 * Acc@1 74.120 Acc@5 93.280
### epoch[64] execution time: 52.26928448677063
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.286 ( 0.286)	Data  0.147 ( 0.147)	Loss 4.0100e-02 (4.0100e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.014)	Loss 3.7628e-02 (4.1988e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.2389e-02 (4.1258e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 6.2103e-02 (4.3520e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.3081e-02 (4.3354e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.5807e-02 (4.2279e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.0441e-02 (4.2548e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.7628e-02 (4.3220e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.0441e-02 (4.3498e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.3579e-02 (4.3519e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6173e-02 (4.3627e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.6184e-02 (4.4123e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.7648e-02 (4.3927e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9530e-02 (4.4041e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7007e-02 (4.3853e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7954e-02 (4.3849e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1809e-02 (4.3713e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9795e-02 (4.3654e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7969e-02 (4.3621e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9185e-02 (4.3536e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3030e-02 (4.3612e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9835e-02 (4.3636e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7668e-02 (4.3578e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.0920e-02 (4.3186e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7481e-02 (4.3067e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0171e-02 (4.3077e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8116e-02 (4.3055e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0975e-02 (4.2903e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3182e-02 (4.3143e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2786e-02 (4.2905e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.0472e-02 (4.3061e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8065e-02 (4.3011e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.135 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.6570e-02 (4.3100e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.6204e-02 (4.2995e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0669e-02 (4.2975e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.8086e-02 (4.3014e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.5156e-02 (4.3068e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1342e-02 (4.3126e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2419e-02 (4.3014e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.106 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2389e-02 (4.2970e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.6954782009124756
## e[65]       loss.backward (sum) time: 14.337441682815552
## e[65]      optimizer.step (sum) time: 8.183584213256836
## epoch[65] training(only) time: 47.30865693092346
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 8.3789e-01 (8.3789e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.0996e+00 (1.1587e+00)	Acc@1  75.00 ( 73.64)	Acc@5  92.00 ( 92.36)
Test: [ 20/100]	Time  0.045 ( 0.056)	Loss 1.0723e+00 (1.1197e+00)	Acc@1  73.00 ( 74.05)	Acc@5  93.00 ( 92.86)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 1.5918e+00 (1.1430e+00)	Acc@1  67.00 ( 73.87)	Acc@5  89.00 ( 92.61)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.2881e+00 (1.1575e+00)	Acc@1  77.00 ( 73.76)	Acc@5  94.00 ( 92.76)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4717e+00 (1.1626e+00)	Acc@1  68.00 ( 73.49)	Acc@5  91.00 ( 92.63)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0840e+00 (1.1344e+00)	Acc@1  76.00 ( 73.54)	Acc@5  96.00 ( 93.08)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.4326e+00 (1.1346e+00)	Acc@1  70.00 ( 73.62)	Acc@5  90.00 ( 93.20)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 1.5127e+00 (1.1388e+00)	Acc@1  74.00 ( 73.70)	Acc@5  91.00 ( 93.07)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5732e+00 (1.1273e+00)	Acc@1  62.00 ( 73.77)	Acc@5  91.00 ( 93.24)
 * Acc@1 73.970 Acc@5 93.270
### epoch[65] execution time: 52.27851724624634
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.278 ( 0.278)	Data  0.149 ( 0.149)	Loss 2.5986e-02 (2.5986e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.117 ( 0.135)	Data  0.001 ( 0.014)	Loss 3.3997e-02 (3.6450e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.121 ( 0.129)	Data  0.001 ( 0.008)	Loss 3.9856e-02 (3.8236e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 8.9294e-02 (4.2448e-02)	Acc@1  96.88 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.4698e-02 (4.1962e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.5115e-02 (4.1388e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.0110e-02 (4.2472e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4210e-02 (4.2495e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.4708e-02 (4.2846e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.9713e-02 (4.3174e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.8309e-02 (4.3392e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.8635e-02 (4.3449e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7180e-02 (4.3304e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5715e-02 (4.3844e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4474e-02 (4.4226e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (4.4561e-02)	Acc@1  97.66 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8777e-02 (4.4455e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4119e-02 (4.4064e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1464e-02 (4.3784e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7847e-02 (4.3448e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.0679e-02 (4.3463e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6366e-02 (4.3516e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8915e-02 (4.3511e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8574e-02 (4.3600e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4302e-02 (4.3474e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3802e-02 (4.3569e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7638e-02 (4.3662e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0598e-02 (4.3832e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8492e-02 (4.3969e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0121e-02 (4.3663e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5939e-02 (4.3558e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2023e-02 (4.3571e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5624e-02 (4.3759e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5013e-02 (4.3744e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.7810e-02 (4.3840e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.7170e-02 (4.3765e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 9.4421e-02 (4.3957e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5847e-02 (4.3839e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9551e-02 (4.3844e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 3.6133e-02 (4.3679e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.6951923370361328
## e[66]       loss.backward (sum) time: 14.266036748886108
## e[66]      optimizer.step (sum) time: 8.173115253448486
## epoch[66] training(only) time: 47.2089307308197
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 8.0762e-01 (8.0762e-01)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.0898e+00 (1.1411e+00)	Acc@1  75.00 ( 73.73)	Acc@5  92.00 ( 92.64)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 1.0615e+00 (1.1034e+00)	Acc@1  74.00 ( 74.57)	Acc@5  93.00 ( 93.14)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5645e+00 (1.1295e+00)	Acc@1  67.00 ( 74.10)	Acc@5  88.00 ( 92.87)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 1.2773e+00 (1.1449e+00)	Acc@1  75.00 ( 74.00)	Acc@5  94.00 ( 93.12)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4434e+00 (1.1510e+00)	Acc@1  65.00 ( 73.71)	Acc@5  91.00 ( 93.00)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0869e+00 (1.1246e+00)	Acc@1  73.00 ( 73.67)	Acc@5  96.00 ( 93.36)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4570e+00 (1.1261e+00)	Acc@1  69.00 ( 73.63)	Acc@5  90.00 ( 93.42)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5137e+00 (1.1313e+00)	Acc@1  75.00 ( 73.68)	Acc@5  91.00 ( 93.31)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5068e+00 (1.1197e+00)	Acc@1  64.00 ( 73.79)	Acc@5  92.00 ( 93.40)
 * Acc@1 74.050 Acc@5 93.440
### epoch[66] execution time: 52.208003520965576
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.285 ( 0.285)	Data  0.149 ( 0.149)	Loss 3.4546e-02 (3.4546e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.129 ( 0.135)	Data  0.001 ( 0.014)	Loss 3.2867e-02 (4.1802e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.118 ( 0.127)	Data  0.001 ( 0.008)	Loss 4.0710e-02 (4.2801e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.117 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.6906e-02 (4.5060e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 2.0935e-02 (4.3667e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.0029e-02 (4.4257e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.3661e-02 (4.3700e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.8757e-02 (4.3831e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.7451e-02 (4.3447e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4058e-02 (4.3732e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5248e-02 (4.4327e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8055e-02 (4.4133e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8177e-02 (4.4143e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1036e-02 (4.3982e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0258e-02 (4.3531e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7922e-02 (4.3465e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4637e-02 (4.3681e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1138e-02 (4.3713e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3976e-02 (4.3986e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7506e-02 (4.4058e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4281e-02 (4.4124e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5593e-02 (4.4034e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6824e-02 (4.3929e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8818e-02 (4.4047e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.9519e-02 (4.3932e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1199e-02 (4.3797e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0472e-02 (4.3715e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4668e-02 (4.3622e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3915e-02 (4.3679e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2856e-02 (4.3682e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.6345e-02 (4.3988e-02)	Acc@1  97.66 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1779e-02 (4.4082e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5898e-02 (4.4232e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.4943e-02 (4.4256e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.2354e-02 (4.4226e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2938e-02 (4.4126e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5176e-02 (4.4144e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9612e-02 (4.3923e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9886e-02 (4.4001e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.1300e-02 (4.3925e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.6865799427032471
## e[67]       loss.backward (sum) time: 14.32424807548523
## e[67]      optimizer.step (sum) time: 8.090512037277222
## epoch[67] training(only) time: 47.19186592102051
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 8.3643e-01 (8.3643e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.0850e+00 (1.1545e+00)	Acc@1  75.00 ( 73.55)	Acc@5  93.00 ( 92.45)
Test: [ 20/100]	Time  0.056 ( 0.054)	Loss 1.0732e+00 (1.1165e+00)	Acc@1  74.00 ( 74.24)	Acc@5  92.00 ( 92.95)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5605e+00 (1.1394e+00)	Acc@1  69.00 ( 73.77)	Acc@5  89.00 ( 92.65)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 1.2812e+00 (1.1555e+00)	Acc@1  74.00 ( 73.66)	Acc@5  93.00 ( 92.83)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4561e+00 (1.1609e+00)	Acc@1  68.00 ( 73.47)	Acc@5  91.00 ( 92.69)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0684e+00 (1.1321e+00)	Acc@1  76.00 ( 73.46)	Acc@5  96.00 ( 93.08)
Test: [ 70/100]	Time  0.060 ( 0.049)	Loss 1.4707e+00 (1.1348e+00)	Acc@1  70.00 ( 73.48)	Acc@5  90.00 ( 93.15)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4590e+00 (1.1393e+00)	Acc@1  75.00 ( 73.53)	Acc@5  92.00 ( 93.09)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5264e+00 (1.1269e+00)	Acc@1  64.00 ( 73.67)	Acc@5  91.00 ( 93.15)
 * Acc@1 73.860 Acc@5 93.180
### epoch[67] execution time: 52.14783787727356
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.289 ( 0.289)	Data  0.141 ( 0.141)	Loss 4.2847e-02 (4.2847e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.121 ( 0.137)	Data  0.001 ( 0.014)	Loss 3.7018e-02 (4.3701e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.126 ( 0.128)	Data  0.001 ( 0.008)	Loss 5.2124e-02 (4.2341e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.005)	Loss 4.8584e-02 (4.2232e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 2.9297e-02 (4.3375e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.0985e-02 (4.3725e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.1113e-02 (4.3573e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.7485e-02 (4.3936e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 80/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.7577e-02 (4.4024e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [68][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.3875e-02 (4.4028e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [68][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6407e-02 (4.3589e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [68][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.0699e-02 (4.3318e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [68][120/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9255e-02 (4.3135e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [68][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.8401e-02 (4.3139e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [68][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3884e-02 (4.3181e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 ( 99.99)
Epoch: [68][150/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9113e-02 (4.3286e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 ( 99.99)
Epoch: [68][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7394e-02 (4.3198e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2623e-02 (4.3370e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4169e-02 (4.3575e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3538e-02 (4.3979e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.3599e-02 (4.4005e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5095e-02 (4.4044e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1270e-02 (4.4115e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4596e-02 (4.4470e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5441e-02 (4.4543e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9877e-02 (4.4289e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1281e-02 (4.4143e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8767e-02 (4.4301e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7455e-02 (4.4237e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6580e-02 (4.4353e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3905e-02 (4.4229e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4220e-02 (4.4277e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2440e-02 (4.4214e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3966e-02 (4.4412e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.4271e-02 (4.4459e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.5787e-02 (4.4234e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5542e-02 (4.4238e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3579e-02 (4.4172e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9551e-02 (4.3988e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.109 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.5126e-02 (4.3880e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.6970288753509521
## e[68]       loss.backward (sum) time: 14.27366852760315
## e[68]      optimizer.step (sum) time: 8.212262153625488
## epoch[68] training(only) time: 47.2797954082489
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 8.3838e-01 (8.3838e-01)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.0928e+00 (1.1585e+00)	Acc@1  75.00 ( 74.09)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.0684e+00 (1.1189e+00)	Acc@1  74.00 ( 74.38)	Acc@5  92.00 ( 93.00)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5605e+00 (1.1415e+00)	Acc@1  67.00 ( 74.03)	Acc@5  88.00 ( 92.65)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 1.3047e+00 (1.1569e+00)	Acc@1  76.00 ( 73.90)	Acc@5  95.00 ( 92.88)
Test: [ 50/100]	Time  0.050 ( 0.050)	Loss 1.4668e+00 (1.1628e+00)	Acc@1  67.00 ( 73.57)	Acc@5  91.00 ( 92.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0801e+00 (1.1339e+00)	Acc@1  75.00 ( 73.59)	Acc@5  97.00 ( 93.20)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4473e+00 (1.1356e+00)	Acc@1  69.00 ( 73.58)	Acc@5  90.00 ( 93.24)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4844e+00 (1.1403e+00)	Acc@1  74.00 ( 73.65)	Acc@5  92.00 ( 93.15)
Test: [ 90/100]	Time  0.045 ( 0.049)	Loss 1.5391e+00 (1.1281e+00)	Acc@1  63.00 ( 73.74)	Acc@5  90.00 ( 93.22)
 * Acc@1 73.970 Acc@5 93.250
### epoch[68] execution time: 52.26516127586365
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.280 ( 0.280)	Data  0.149 ( 0.149)	Loss 3.8849e-02 (3.8849e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.137 ( 0.135)	Data  0.001 ( 0.014)	Loss 2.9953e-02 (3.7703e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.120 ( 0.128)	Data  0.001 ( 0.008)	Loss 3.1860e-02 (4.1821e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.5126e-02 (4.4165e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.128 ( 0.124)	Data  0.001 ( 0.005)	Loss 5.5298e-02 (4.4575e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.9007e-02 (4.4494e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.6875e-02 (4.5062e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.9236e-02 (4.4041e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.8605e-02 (4.3676e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.003)	Loss 3.8300e-02 (4.4274e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.8125e-02 (4.4405e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9469e-02 (4.4219e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4026e-02 (4.4178e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7678e-02 (4.4048e-02)	Acc@1  97.66 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8096e-02 (4.4023e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8641e-02 (4.4480e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7159e-02 (4.4323e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4943e-02 (4.4413e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0472e-02 (4.4620e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2664e-02 (4.4514e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8889e-02 (4.4805e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7220e-02 (4.4592e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2673e-02 (4.4944e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5309e-02 (4.4891e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9124e-02 (4.5066e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6539e-02 (4.5017e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.1494e-02 (4.4791e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.6538e-02 (4.4984e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4749e-02 (4.5124e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1554e-02 (4.5129e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.0792e-02 (4.4841e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.8177e-02 (4.4705e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1829e-02 (4.4763e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.6346e-02 (4.4684e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4342e-02 (4.4699e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1952e-02 (4.4646e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1586e-02 (4.4513e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.7262e-02 (4.4432e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.8176e-02 (4.4561e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.110 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.3070e-02 (4.4471e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.6860551834106445
## e[69]       loss.backward (sum) time: 14.322346925735474
## e[69]      optimizer.step (sum) time: 8.097229957580566
## epoch[69] training(only) time: 47.199090242385864
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 8.2910e-01 (8.2910e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.1191e+00 (1.1651e+00)	Acc@1  74.00 ( 73.64)	Acc@5  93.00 ( 92.27)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0547e+00 (1.1228e+00)	Acc@1  75.00 ( 74.33)	Acc@5  92.00 ( 92.90)
Test: [ 30/100]	Time  0.045 ( 0.052)	Loss 1.5684e+00 (1.1447e+00)	Acc@1  67.00 ( 73.90)	Acc@5  89.00 ( 92.68)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 1.2881e+00 (1.1580e+00)	Acc@1  74.00 ( 73.83)	Acc@5  94.00 ( 92.85)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4463e+00 (1.1618e+00)	Acc@1  67.00 ( 73.65)	Acc@5  92.00 ( 92.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0596e+00 (1.1330e+00)	Acc@1  75.00 ( 73.66)	Acc@5  97.00 ( 93.16)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4492e+00 (1.1348e+00)	Acc@1  69.00 ( 73.69)	Acc@5  90.00 ( 93.25)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.4922e+00 (1.1386e+00)	Acc@1  75.00 ( 73.70)	Acc@5  92.00 ( 93.17)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5566e+00 (1.1265e+00)	Acc@1  63.00 ( 73.79)	Acc@5  91.00 ( 93.23)
 * Acc@1 74.010 Acc@5 93.260
### epoch[69] execution time: 52.18955111503601
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.282 ( 0.282)	Data  0.147 ( 0.147)	Loss 2.8122e-02 (2.8122e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.119 ( 0.135)	Data  0.001 ( 0.014)	Loss 4.9316e-02 (4.5958e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.117 ( 0.127)	Data  0.001 ( 0.008)	Loss 3.6652e-02 (4.8660e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.3884e-02 (4.6100e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.5156e-02 (4.6286e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.6047e-02 (4.5475e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.4169e-02 (4.5644e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.2175e-02 (4.4952e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.4658e-02 (4.4182e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.5074e-02 (4.4536e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8452e-02 (4.4447e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3671e-02 (4.4142e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0781e-02 (4.3746e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8147e-02 (4.4452e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9099e-02 (4.4891e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [70][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9042e-02 (4.4952e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [70][160/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6993e-02 (4.4810e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.4399e-02 (4.4569e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8981e-02 (4.4802e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4392e-02 (4.4913e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4861e-02 (4.4866e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9622e-02 (4.4840e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9642e-02 (4.4775e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3365e-02 (4.4658e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8767e-02 (4.4440e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2389e-02 (4.4530e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2084e-02 (4.4725e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.9478e-02 (4.4718e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.5665e-02 (4.4450e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1453e-02 (4.4562e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.0441e-02 (4.4580e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1870e-02 (4.4502e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.8777e-02 (4.4562e-02)	Acc@1  96.88 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.8839e-02 (4.4604e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.0869e-02 (4.4575e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.0405e-02 (4.4587e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5918e-02 (4.4728e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.9347e-02 (4.4926e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.8746e-02 (4.4873e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.105 ( 0.120)	Data  0.001 ( 0.001)	Loss 5.6091e-02 (4.4831e-02)	Acc@1  98.75 ( 99.47)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.6812865734100342
## e[70]       loss.backward (sum) time: 14.380189418792725
## e[70]      optimizer.step (sum) time: 8.056912660598755
## epoch[70] training(only) time: 47.17298364639282
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 8.2617e-01 (8.2617e-01)	Acc@1  76.00 ( 76.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.050 ( 0.061)	Loss 1.0762e+00 (1.1510e+00)	Acc@1  75.00 ( 73.18)	Acc@5  93.00 ( 92.36)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0840e+00 (1.1125e+00)	Acc@1  74.00 ( 74.05)	Acc@5  92.00 ( 92.81)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5723e+00 (1.1366e+00)	Acc@1  67.00 ( 73.81)	Acc@5  90.00 ( 92.58)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 1.2803e+00 (1.1515e+00)	Acc@1  75.00 ( 73.78)	Acc@5  94.00 ( 92.83)
Test: [ 50/100]	Time  0.045 ( 0.051)	Loss 1.4639e+00 (1.1572e+00)	Acc@1  68.00 ( 73.55)	Acc@5  91.00 ( 92.73)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0791e+00 (1.1289e+00)	Acc@1  73.00 ( 73.59)	Acc@5  96.00 ( 93.18)
Test: [ 70/100]	Time  0.045 ( 0.050)	Loss 1.4482e+00 (1.1300e+00)	Acc@1  68.00 ( 73.59)	Acc@5  90.00 ( 93.27)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4922e+00 (1.1342e+00)	Acc@1  75.00 ( 73.65)	Acc@5  91.00 ( 93.21)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 1.5518e+00 (1.1226e+00)	Acc@1  62.00 ( 73.73)	Acc@5  91.00 ( 93.30)
 * Acc@1 74.000 Acc@5 93.330
### epoch[70] execution time: 52.17603802680969
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.278 ( 0.278)	Data  0.135 ( 0.135)	Loss 4.0344e-02 (4.0344e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.118 ( 0.134)	Data  0.001 ( 0.013)	Loss 3.9703e-02 (4.0605e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.120 ( 0.127)	Data  0.001 ( 0.007)	Loss 5.2734e-02 (4.3666e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.005)	Loss 5.8075e-02 (4.2964e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.129 ( 0.124)	Data  0.001 ( 0.004)	Loss 2.3087e-02 (4.2776e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.7537e-02 (4.2096e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.3915e-02 (4.2546e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.6438e-02 (4.2265e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.3579e-02 (4.2913e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5288e-02 (4.2228e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.136 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7811e-02 (4.2983e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2898e-02 (4.2985e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5187e-02 (4.2806e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2999e-02 (4.2849e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7313e-02 (4.2957e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9225e-02 (4.3332e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4912e-02 (4.3523e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4148e-02 (4.3558e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0680e-02 (4.3427e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2236e-02 (4.3565e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2318e-02 (4.3698e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1086e-02 (4.3695e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6804e-02 (4.3224e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3325e-02 (4.3114e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.3557e-02 (4.3406e-02)	Acc@1  97.66 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1361e-02 (4.3417e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1046e-02 (4.3446e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.0436e-02 (4.3337e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.8553e-02 (4.3480e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.8422e-02 (4.3454e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.7587e-02 (4.3672e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.6377e-02 (4.3698e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.9083e-02 (4.3760e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.5436e-02 (4.3637e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4016e-02 (4.3918e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1473e-02 (4.3986e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.8030e-02 (4.3916e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9093e-02 (4.3930e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.136 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.6123e-02 (4.3893e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.106 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4769e-02 (4.3785e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.6888260841369629
## e[71]       loss.backward (sum) time: 14.36036205291748
## e[71]      optimizer.step (sum) time: 8.12224531173706
## epoch[71] training(only) time: 47.25829768180847
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 8.1885e-01 (8.1885e-01)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.1172e+00 (1.1590e+00)	Acc@1  75.00 ( 73.64)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0664e+00 (1.1149e+00)	Acc@1  74.00 ( 74.29)	Acc@5  92.00 ( 92.90)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5820e+00 (1.1383e+00)	Acc@1  68.00 ( 74.06)	Acc@5  88.00 ( 92.65)
Test: [ 40/100]	Time  0.045 ( 0.051)	Loss 1.2773e+00 (1.1539e+00)	Acc@1  75.00 ( 73.88)	Acc@5  94.00 ( 92.93)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4424e+00 (1.1580e+00)	Acc@1  68.00 ( 73.75)	Acc@5  91.00 ( 92.80)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0879e+00 (1.1313e+00)	Acc@1  73.00 ( 73.67)	Acc@5  97.00 ( 93.23)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4502e+00 (1.1325e+00)	Acc@1  68.00 ( 73.70)	Acc@5  90.00 ( 93.31)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4980e+00 (1.1366e+00)	Acc@1  75.00 ( 73.73)	Acc@5  92.00 ( 93.23)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5752e+00 (1.1251e+00)	Acc@1  62.00 ( 73.79)	Acc@5  90.00 ( 93.35)
 * Acc@1 74.000 Acc@5 93.330
### epoch[71] execution time: 52.26069116592407
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.294 ( 0.294)	Data  0.157 ( 0.157)	Loss 3.2257e-02 (3.2257e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.122 ( 0.135)	Data  0.001 ( 0.015)	Loss 3.6926e-02 (4.5256e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.121 ( 0.128)	Data  0.001 ( 0.008)	Loss 5.6641e-02 (4.3833e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.006)	Loss 3.4424e-02 (4.4034e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.8248e-02 (4.2849e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.0222e-02 (4.3554e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.004)	Loss 4.1046e-02 (4.3415e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 7.2876e-02 (4.3384e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.8330e-02 (4.3006e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.9866e-02 (4.3256e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.1147e-02 (4.3276e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8696e-02 (4.3545e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.5328e-02 (4.3331e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.0822e-02 (4.3380e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3453e-02 (4.3471e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2205e-02 (4.3232e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.2256e-02 (4.3431e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6753e-02 (4.4052e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5553e-02 (4.4124e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5309e-02 (4.3987e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.2764e-02 (4.4311e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.5735e-02 (4.4341e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.8946e-02 (4.4325e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.7678e-02 (4.4413e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7394e-02 (4.4505e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.9861e-02 (4.4426e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0497e-02 (4.4435e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.131 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0771e-02 (4.4282e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6173e-02 (4.4066e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7211e-02 (4.4156e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0649e-02 (4.4355e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4434e-02 (4.4338e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1290e-02 (4.4256e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0049e-02 (4.4182e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (4.4220e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.3905e-02 (4.4157e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9307e-02 (4.4079e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1891e-02 (4.4233e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.8605e-02 (4.4279e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.9834e-02 (4.4297e-02)	Acc@1  97.50 ( 99.50)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.691795825958252
## e[72]       loss.backward (sum) time: 14.303338766098022
## e[72]      optimizer.step (sum) time: 8.13712763786316
## epoch[72] training(only) time: 47.21032643318176
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 8.1738e-01 (8.1738e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.1133e+00 (1.1578e+00)	Acc@1  74.00 ( 73.09)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.0547e+00 (1.1165e+00)	Acc@1  74.00 ( 73.81)	Acc@5  92.00 ( 92.86)
Test: [ 30/100]	Time  0.055 ( 0.052)	Loss 1.5723e+00 (1.1386e+00)	Acc@1  68.00 ( 73.77)	Acc@5  89.00 ( 92.61)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.2754e+00 (1.1535e+00)	Acc@1  75.00 ( 73.73)	Acc@5  94.00 ( 92.88)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.4580e+00 (1.1587e+00)	Acc@1  69.00 ( 73.57)	Acc@5  91.00 ( 92.69)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.0947e+00 (1.1320e+00)	Acc@1  74.00 ( 73.57)	Acc@5  96.00 ( 93.07)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4531e+00 (1.1337e+00)	Acc@1  69.00 ( 73.58)	Acc@5  90.00 ( 93.20)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 1.4922e+00 (1.1377e+00)	Acc@1  74.00 ( 73.62)	Acc@5  92.00 ( 93.15)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5479e+00 (1.1261e+00)	Acc@1  64.00 ( 73.74)	Acc@5  91.00 ( 93.25)
 * Acc@1 73.960 Acc@5 93.280
### epoch[72] execution time: 52.2103590965271
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.291 ( 0.291)	Data  0.148 ( 0.148)	Loss 3.2135e-02 (3.2135e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.118 ( 0.135)	Data  0.001 ( 0.014)	Loss 5.0354e-02 (4.5715e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.117 ( 0.128)	Data  0.001 ( 0.008)	Loss 4.4312e-02 (4.3529e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.006)	Loss 3.1250e-02 (4.2095e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.5929e-02 (4.2849e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 2.9114e-02 (4.2782e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.4403e-02 (4.3091e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.9612e-02 (4.3433e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.7237e-02 (4.3287e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.4037e-02 (4.3649e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3060e-02 (4.3945e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9093e-02 (4.3657e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.4331e-02 (4.3636e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5288e-02 (4.3739e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0588e-02 (4.3795e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9103e-02 (4.3758e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.3060e-02 (4.3505e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.7018e-02 (4.3806e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.8818e-02 (4.3899e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2083e-02 (4.4095e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.6244e-02 (4.4280e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.1667e-02 (4.4118e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0161e-02 (4.3970e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0334e-02 (4.3985e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6814e-02 (4.4202e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9377e-02 (4.4338e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1351e-02 (4.4391e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.2521e-02 (4.4441e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6783e-02 (4.4527e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.8442e-02 (4.4310e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.3416e-02 (4.4319e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.2002e-02 (4.4442e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0293e-02 (4.4253e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.6865e-02 (4.4194e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.5583e-02 (4.4198e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1025e-02 (4.4234e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.4331e-02 (4.4186e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.3406e-02 (4.4306e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.8727e-02 (4.4279e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.112 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0120e-02 (4.4259e-02)	Acc@1  97.50 ( 99.48)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.6829655170440674
## e[73]       loss.backward (sum) time: 14.329789400100708
## e[73]      optimizer.step (sum) time: 8.165080070495605
## epoch[73] training(only) time: 47.29063653945923
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 8.1738e-01 (8.1738e-01)	Acc@1  81.00 ( 81.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.061 ( 0.063)	Loss 1.1211e+00 (1.1575e+00)	Acc@1  74.00 ( 74.18)	Acc@5  93.00 ( 92.45)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.0781e+00 (1.1177e+00)	Acc@1  74.00 ( 74.57)	Acc@5  93.00 ( 92.90)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.5781e+00 (1.1380e+00)	Acc@1  68.00 ( 74.19)	Acc@5  88.00 ( 92.61)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.2969e+00 (1.1533e+00)	Acc@1  76.00 ( 74.00)	Acc@5  94.00 ( 92.88)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.4141e+00 (1.1570e+00)	Acc@1  69.00 ( 73.80)	Acc@5  91.00 ( 92.75)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.0801e+00 (1.1286e+00)	Acc@1  75.00 ( 73.82)	Acc@5  95.00 ( 93.13)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4463e+00 (1.1314e+00)	Acc@1  68.00 ( 73.73)	Acc@5  90.00 ( 93.20)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.5029e+00 (1.1350e+00)	Acc@1  74.00 ( 73.78)	Acc@5  90.00 ( 93.09)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5352e+00 (1.1228e+00)	Acc@1  64.00 ( 73.86)	Acc@5  91.00 ( 93.19)
 * Acc@1 74.110 Acc@5 93.210
### epoch[73] execution time: 52.30956721305847
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.285 ( 0.285)	Data  0.151 ( 0.151)	Loss 2.7985e-02 (2.7985e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.118 ( 0.136)	Data  0.001 ( 0.015)	Loss 4.3213e-02 (4.7188e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.008)	Loss 4.4067e-02 (4.2759e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.117 ( 0.126)	Data  0.001 ( 0.006)	Loss 4.6783e-02 (4.2732e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.005)	Loss 5.9204e-02 (4.3022e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.9021e-02 (4.3381e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.2450e-02 (4.3374e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.7933e-02 (4.3558e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.9337e-02 (4.4341e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.0354e-02 (4.4887e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [74][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.9174e-02 (4.4252e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [74][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.2877e-02 (4.4213e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [74][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.3955e-02 (4.3981e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [74][130/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.1005e-02 (4.4322e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [74][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0823e-02 (4.4124e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [74][150/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.2358e-02 (4.4363e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [74][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4882e-02 (4.4060e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 8.7341e-02 (4.4427e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8859e-02 (4.4285e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5675e-02 (4.4246e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7496e-02 (4.4197e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6530e-02 (4.4039e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1473e-02 (4.4096e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6779e-02 (4.4106e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8594e-02 (4.3971e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5135e-02 (4.3971e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8370e-02 (4.4411e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.127 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.2654e-02 (4.4386e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4271e-02 (4.4444e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6560e-02 (4.4329e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.129 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.3112e-02 (4.4333e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.4058e-02 (4.4317e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.7689e-02 (4.4154e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.133 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9520e-02 (4.4104e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4321e-02 (4.4191e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.6753e-02 (4.4186e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.124 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5227e-02 (4.4116e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4403e-02 (4.4020e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.7119e-02 (4.4122e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.115 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.7892e-02 (4.4015e-02)	Acc@1  98.75 ( 99.50)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.6956064701080322
## e[74]       loss.backward (sum) time: 14.28221845626831
## e[74]      optimizer.step (sum) time: 8.183941841125488
## epoch[74] training(only) time: 47.3389458656311
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 8.0518e-01 (8.0518e-01)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.1270e+00 (1.1531e+00)	Acc@1  77.00 ( 74.55)	Acc@5  92.00 ( 92.36)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0781e+00 (1.1153e+00)	Acc@1  74.00 ( 74.95)	Acc@5  93.00 ( 92.90)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.5791e+00 (1.1372e+00)	Acc@1  68.00 ( 74.52)	Acc@5  91.00 ( 92.71)
Test: [ 40/100]	Time  0.050 ( 0.051)	Loss 1.2891e+00 (1.1504e+00)	Acc@1  75.00 ( 74.22)	Acc@5  94.00 ( 93.02)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4336e+00 (1.1547e+00)	Acc@1  69.00 ( 73.86)	Acc@5  91.00 ( 92.90)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.0879e+00 (1.1279e+00)	Acc@1  75.00 ( 73.89)	Acc@5  97.00 ( 93.34)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 1.4316e+00 (1.1298e+00)	Acc@1  69.00 ( 73.86)	Acc@5  90.00 ( 93.39)
Test: [ 80/100]	Time  0.061 ( 0.049)	Loss 1.4912e+00 (1.1347e+00)	Acc@1  75.00 ( 73.98)	Acc@5  90.00 ( 93.27)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5488e+00 (1.1228e+00)	Acc@1  62.00 ( 74.01)	Acc@5  91.00 ( 93.35)
 * Acc@1 74.230 Acc@5 93.380
### epoch[74] execution time: 52.33559226989746
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.283 ( 0.283)	Data  0.145 ( 0.145)	Loss 2.9922e-02 (2.9922e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.121 ( 0.134)	Data  0.001 ( 0.014)	Loss 3.0594e-02 (3.8158e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.008)	Loss 3.3661e-02 (3.8458e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.006)	Loss 4.5227e-02 (4.1972e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.005)	Loss 3.2806e-02 (4.1790e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 3.9520e-02 (4.1554e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.8929e-02 (4.2593e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.1260e-02 (4.1847e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.7476e-02 (4.2303e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.4209e-02 (4.3667e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2867e-02 (4.3554e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0385e-02 (4.3470e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6591e-02 (4.3379e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7476e-02 (4.3526e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 9.1003e-02 (4.3700e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0354e-02 (4.3932e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.133 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.5693e-02 (4.4354e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4332e-02 (4.4093e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4485e-02 (4.3902e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.8003e-02 (4.3868e-02)	Acc@1  96.88 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.2053e-02 (4.4091e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.6509e-02 (4.4049e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9673e-02 (4.3766e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.4596e-02 (4.3761e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8248e-02 (4.3931e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.134 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8676e-02 (4.4015e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6346e-02 (4.3876e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.6255e-02 (4.3685e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1443e-02 (4.3552e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.3558e-02 (4.3516e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.0924e-02 (4.3387e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.4250e-02 (4.3537e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3773e-02 (4.3723e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.3020e-02 (4.3674e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9093e-02 (4.3722e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.2460e-02 (4.3726e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.1157e-02 (4.3797e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.0435e-02 (4.3891e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.6030e-02 (4.4172e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.106 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.6011e-02 (4.4105e-02)	Acc@1  98.75 ( 99.45)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.6877388954162598
## e[75]       loss.backward (sum) time: 14.385007381439209
## e[75]      optimizer.step (sum) time: 8.170409440994263
## epoch[75] training(only) time: 47.47267413139343
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 8.4668e-01 (8.4668e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.0996e+00 (1.1580e+00)	Acc@1  75.00 ( 74.09)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.0820e+00 (1.1230e+00)	Acc@1  73.00 ( 74.38)	Acc@5  92.00 ( 93.00)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5615e+00 (1.1446e+00)	Acc@1  68.00 ( 74.03)	Acc@5  89.00 ( 92.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.2676e+00 (1.1601e+00)	Acc@1  75.00 ( 73.90)	Acc@5  94.00 ( 92.93)
Test: [ 50/100]	Time  0.045 ( 0.051)	Loss 1.4570e+00 (1.1663e+00)	Acc@1  68.00 ( 73.53)	Acc@5  91.00 ( 92.78)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0654e+00 (1.1357e+00)	Acc@1  74.00 ( 73.59)	Acc@5  96.00 ( 93.23)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4502e+00 (1.1375e+00)	Acc@1  70.00 ( 73.63)	Acc@5  90.00 ( 93.34)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4736e+00 (1.1415e+00)	Acc@1  74.00 ( 73.68)	Acc@5  91.00 ( 93.15)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5488e+00 (1.1293e+00)	Acc@1  62.00 ( 73.73)	Acc@5  90.00 ( 93.20)
 * Acc@1 73.930 Acc@5 93.220
### epoch[75] execution time: 52.476736545562744
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.279 ( 0.279)	Data  0.149 ( 0.149)	Loss 5.7861e-02 (5.7861e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.119 ( 0.135)	Data  0.001 ( 0.014)	Loss 5.7922e-02 (4.6040e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.128 ( 0.128)	Data  0.001 ( 0.008)	Loss 4.9896e-02 (4.3347e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 4.8004e-02 (4.3053e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.005)	Loss 7.2083e-02 (4.3304e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.7506e-02 (4.2740e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.9357e-02 (4.3128e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.1738e-02 (4.3488e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.5156e-02 (4.3693e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 3.4454e-02 (4.3449e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.9459e-02 (4.3553e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.8279e-02 (4.3852e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1729e-02 (4.3772e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5431e-02 (4.3468e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1117e-02 (4.3682e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6234e-02 (4.3719e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.2460e-02 (4.3588e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0090e-02 (4.3697e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.4219e-02 (4.3921e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.5400e-02 (4.3491e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3834e-02 (4.3480e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.122 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.8411e-02 (4.3735e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1870e-02 (4.3814e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.8553e-02 (4.4033e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.9357e-02 (4.3877e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.7180e-02 (4.3825e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3457e-02 (4.3939e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.7924e-02 (4.3643e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.117 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1250e-02 (4.3670e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1229e-02 (4.3618e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.6337e-02 (4.3649e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5145e-02 (4.3730e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.1840e-02 (4.3814e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5502e-02 (4.3687e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.6957e-02 (4.3511e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.9988e-02 (4.3560e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.126 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.0272e-02 (4.3558e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3182e-02 (4.3518e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.4331e-02 (4.3583e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.9561e-02 (4.3797e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.6819360256195068
## e[76]       loss.backward (sum) time: 14.347338438034058
## e[76]      optimizer.step (sum) time: 8.088698387145996
## epoch[76] training(only) time: 47.38580560684204
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 8.1396e-01 (8.1396e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.052 ( 0.061)	Loss 1.1133e+00 (1.1602e+00)	Acc@1  74.00 ( 74.00)	Acc@5  92.00 ( 92.18)
Test: [ 20/100]	Time  0.046 ( 0.056)	Loss 1.0820e+00 (1.1188e+00)	Acc@1  74.00 ( 74.29)	Acc@5  93.00 ( 92.86)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5830e+00 (1.1403e+00)	Acc@1  68.00 ( 74.06)	Acc@5  89.00 ( 92.52)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.2861e+00 (1.1541e+00)	Acc@1  77.00 ( 74.00)	Acc@5  94.00 ( 92.78)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4316e+00 (1.1589e+00)	Acc@1  67.00 ( 73.73)	Acc@5  91.00 ( 92.61)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0664e+00 (1.1312e+00)	Acc@1  77.00 ( 73.69)	Acc@5  95.00 ( 93.02)
Test: [ 70/100]	Time  0.051 ( 0.050)	Loss 1.4482e+00 (1.1323e+00)	Acc@1  68.00 ( 73.63)	Acc@5  90.00 ( 93.11)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4873e+00 (1.1364e+00)	Acc@1  76.00 ( 73.73)	Acc@5  90.00 ( 93.02)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5439e+00 (1.1245e+00)	Acc@1  63.00 ( 73.84)	Acc@5  92.00 ( 93.19)
 * Acc@1 74.030 Acc@5 93.220
### epoch[76] execution time: 52.40178155899048
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.287 ( 0.287)	Data  0.151 ( 0.151)	Loss 3.8269e-02 (3.8269e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.120 ( 0.136)	Data  0.001 ( 0.015)	Loss 4.4403e-02 (4.0701e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.122 ( 0.129)	Data  0.001 ( 0.008)	Loss 3.0624e-02 (3.9444e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.006)	Loss 3.2166e-02 (4.2644e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 4.2267e-02 (4.4243e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.1311e-02 (4.4218e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 8.2764e-02 (4.5511e-02)	Acc@1  96.88 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.7292e-02 (4.4698e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.1819e-02 (4.4761e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 2.5986e-02 (4.4238e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.0283e-02 (4.3787e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.1901e-02 (4.4230e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4729e-02 (4.4249e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.2917e-02 (4.4197e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.5725e-02 (4.4322e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.3589e-02 (4.4227e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6438e-02 (4.4236e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.7159e-02 (4.4389e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5461e-02 (4.4196e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.135 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.3232e-02 (4.4550e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.3224e-02 (4.4591e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2013e-02 (4.4292e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9042e-02 (4.4245e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2318e-02 (4.4458e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.133 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.5002e-02 (4.4371e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6285e-02 (4.4395e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4882e-02 (4.4302e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3945e-02 (4.4321e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4800e-02 (4.4335e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7546e-02 (4.4375e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.2084e-02 (4.4352e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.9052e-02 (4.4215e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.126 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.5267e-02 (4.4256e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.5156e-02 (4.4097e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.0558e-02 (4.4173e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.9449e-02 (4.4341e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.4851e-02 (4.4307e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4647e-02 (4.4361e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5898e-02 (4.4258e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.106 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.5563e-02 (4.4288e-02)	Acc@1  98.75 ( 99.49)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.6830422878265381
## e[77]       loss.backward (sum) time: 14.323100090026855
## e[77]      optimizer.step (sum) time: 8.187525749206543
## epoch[77] training(only) time: 47.545915365219116
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 8.3447e-01 (8.3447e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.1084e+00 (1.1572e+00)	Acc@1  76.00 ( 74.00)	Acc@5  93.00 ( 92.45)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0879e+00 (1.1168e+00)	Acc@1  74.00 ( 74.48)	Acc@5  92.00 ( 93.05)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5977e+00 (1.1404e+00)	Acc@1  67.00 ( 73.94)	Acc@5  89.00 ( 92.77)
Test: [ 40/100]	Time  0.057 ( 0.052)	Loss 1.2939e+00 (1.1566e+00)	Acc@1  74.00 ( 73.73)	Acc@5  94.00 ( 92.95)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.4453e+00 (1.1607e+00)	Acc@1  69.00 ( 73.49)	Acc@5  91.00 ( 92.80)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0566e+00 (1.1328e+00)	Acc@1  74.00 ( 73.62)	Acc@5  97.00 ( 93.25)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4648e+00 (1.1345e+00)	Acc@1  69.00 ( 73.61)	Acc@5  90.00 ( 93.32)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.4912e+00 (1.1385e+00)	Acc@1  74.00 ( 73.64)	Acc@5  90.00 ( 93.23)
Test: [ 90/100]	Time  0.055 ( 0.049)	Loss 1.5625e+00 (1.1271e+00)	Acc@1  62.00 ( 73.75)	Acc@5  91.00 ( 93.31)
 * Acc@1 73.950 Acc@5 93.330
### epoch[77] execution time: 52.56658744812012
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.300 ( 0.300)	Data  0.157 ( 0.157)	Loss 4.0741e-02 (4.0741e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.119 ( 0.137)	Data  0.001 ( 0.015)	Loss 3.5187e-02 (4.3908e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.128 ( 0.129)	Data  0.001 ( 0.008)	Loss 2.8275e-02 (4.5105e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.118 ( 0.126)	Data  0.001 ( 0.006)	Loss 3.2440e-02 (4.6162e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.005)	Loss 2.7328e-02 (4.4490e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.129 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.7842e-02 (4.4857e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 5.6854e-02 (4.4013e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 9.0271e-02 (4.4535e-02)	Acc@1  97.66 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.0813e-02 (4.3794e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.6833e-02 (4.4550e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6997e-02 (4.4145e-02)	Acc@1  97.66 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.2195e-02 (4.4489e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 ( 99.99)
Epoch: [78][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.2572e-02 (4.4580e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 ( 99.99)
Epoch: [78][130/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1483e-02 (4.4159e-02)	Acc@1  98.44 ( 99.53)	Acc@5 100.00 ( 99.99)
Epoch: [78][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7292e-02 (4.4041e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 ( 99.99)
Epoch: [78][150/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5319e-02 (4.3634e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 ( 99.99)
Epoch: [78][160/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9042e-02 (4.3605e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.3295e-02 (4.3586e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6997e-02 (4.3353e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7344e-02 (4.3351e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1656e-02 (4.3205e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1016e-02 (4.3436e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.0049e-02 (4.3548e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.3300e-02 (4.3843e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.9042e-02 (4.3904e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 5.1666e-02 (4.3804e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1534e-02 (4.3823e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5593e-02 (4.3907e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.124 ( 0.121)	Data  0.002 ( 0.002)	Loss 5.1147e-02 (4.3940e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.9343e-02 (4.3999e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1331e-02 (4.4199e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.9042e-02 (4.4219e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.136 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.2196e-02 (4.4178e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.7506e-02 (4.4018e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5206e-02 (4.3927e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.0833e-02 (4.4080e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.132 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5695e-02 (4.4126e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1708e-02 (4.4165e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.4189e-02 (4.4011e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.5308e-02 (4.3905e-02)	Acc@1  98.75 ( 99.51)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.6881437301635742
## e[78]       loss.backward (sum) time: 14.281894207000732
## e[78]      optimizer.step (sum) time: 8.12906002998352
## epoch[78] training(only) time: 47.465643882751465
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 8.4131e-01 (8.4131e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.0986e+00 (1.1563e+00)	Acc@1  74.00 ( 73.73)	Acc@5  93.00 ( 92.64)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 1.0742e+00 (1.1204e+00)	Acc@1  73.00 ( 74.19)	Acc@5  92.00 ( 93.14)
Test: [ 30/100]	Time  0.049 ( 0.052)	Loss 1.5820e+00 (1.1438e+00)	Acc@1  68.00 ( 73.74)	Acc@5  89.00 ( 92.77)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2959e+00 (1.1596e+00)	Acc@1  77.00 ( 73.66)	Acc@5  94.00 ( 92.98)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4473e+00 (1.1648e+00)	Acc@1  68.00 ( 73.47)	Acc@5  91.00 ( 92.80)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0703e+00 (1.1356e+00)	Acc@1  76.00 ( 73.61)	Acc@5  95.00 ( 93.18)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4580e+00 (1.1376e+00)	Acc@1  69.00 ( 73.63)	Acc@5  90.00 ( 93.25)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5156e+00 (1.1420e+00)	Acc@1  75.00 ( 73.73)	Acc@5  91.00 ( 93.17)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5684e+00 (1.1301e+00)	Acc@1  64.00 ( 73.81)	Acc@5  90.00 ( 93.25)
 * Acc@1 74.060 Acc@5 93.270
### epoch[78] execution time: 52.47880411148071
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.290 ( 0.290)	Data  0.154 ( 0.154)	Loss 4.3854e-02 (4.3854e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.121 ( 0.137)	Data  0.001 ( 0.015)	Loss 3.6774e-02 (4.6821e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.121 ( 0.130)	Data  0.001 ( 0.008)	Loss 5.8136e-02 (4.7038e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.119 ( 0.127)	Data  0.001 ( 0.006)	Loss 3.0350e-02 (4.4476e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.118 ( 0.125)	Data  0.001 ( 0.005)	Loss 4.4830e-02 (4.3788e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.6469e-02 (4.3126e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.004)	Loss 5.5939e-02 (4.3572e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.1281e-02 (4.2812e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.9845e-02 (4.3518e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.9642e-02 (4.3465e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.0746e-02 (4.2956e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 5.4260e-02 (4.3213e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.6138e-02 (4.3223e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.9368e-02 (4.3196e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3213e-02 (4.3274e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.0996e-02 (4.3143e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5553e-02 (4.3097e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1453e-02 (4.3488e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9938e-02 (4.3179e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0659e-02 (4.3315e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6692e-02 (4.3366e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0629e-02 (4.3667e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.4565e-02 (4.3712e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5776e-02 (4.3839e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0228e-02 (4.3819e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1361e-02 (4.3828e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.2490e-02 (4.3699e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.7892e-02 (4.3878e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0262e-02 (4.3825e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0182e-02 (4.3894e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.2603e-02 (4.3679e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.1768e-02 (4.3769e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6560e-02 (4.3758e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2196e-02 (4.3696e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7262e-02 (4.3406e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1208e-02 (4.3638e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.133 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.1952e-02 (4.3584e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.9032e-02 (4.3602e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.6448e-02 (4.3570e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.106 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0232e-02 (4.3522e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.6871650218963623
## e[79]       loss.backward (sum) time: 14.291929006576538
## e[79]      optimizer.step (sum) time: 8.199777841567993
## epoch[79] training(only) time: 47.595473766326904
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 8.4277e-01 (8.4277e-01)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.0869e+00 (1.1658e+00)	Acc@1  75.00 ( 74.00)	Acc@5  92.00 ( 92.36)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0967e+00 (1.1242e+00)	Acc@1  74.00 ( 74.57)	Acc@5  93.00 ( 93.00)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.5693e+00 (1.1447e+00)	Acc@1  68.00 ( 74.42)	Acc@5  90.00 ( 92.65)
Test: [ 40/100]	Time  0.057 ( 0.051)	Loss 1.3027e+00 (1.1587e+00)	Acc@1  76.00 ( 74.20)	Acc@5  94.00 ( 92.88)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4668e+00 (1.1642e+00)	Acc@1  67.00 ( 73.82)	Acc@5  90.00 ( 92.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0908e+00 (1.1360e+00)	Acc@1  74.00 ( 73.84)	Acc@5  96.00 ( 93.18)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4395e+00 (1.1375e+00)	Acc@1  69.00 ( 73.82)	Acc@5  90.00 ( 93.25)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.5225e+00 (1.1422e+00)	Acc@1  74.00 ( 73.83)	Acc@5  90.00 ( 93.12)
Test: [ 90/100]	Time  0.057 ( 0.049)	Loss 1.5234e+00 (1.1295e+00)	Acc@1  64.00 ( 73.98)	Acc@5  91.00 ( 93.22)
 * Acc@1 74.160 Acc@5 93.250
### epoch[79] execution time: 52.58793091773987
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.293 ( 0.293)	Data  0.149 ( 0.149)	Loss 4.3518e-02 (4.3518e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.120 ( 0.137)	Data  0.001 ( 0.014)	Loss 5.0171e-02 (3.9610e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.119 ( 0.129)	Data  0.001 ( 0.008)	Loss 4.5898e-02 (4.1930e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.122 ( 0.127)	Data  0.001 ( 0.006)	Loss 4.5441e-02 (4.1442e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 3.3234e-02 (4.0951e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.128 ( 0.125)	Data  0.001 ( 0.004)	Loss 2.3239e-02 (4.0688e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.117 ( 0.124)	Data  0.001 ( 0.003)	Loss 4.6295e-02 (4.1819e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.3752e-02 (4.1147e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.128 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.9948e-02 (4.1470e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.4352e-02 (4.2250e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.124 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.4760e-02 (4.2530e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 5.2582e-02 (4.2994e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.117 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.9795e-02 (4.3405e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.5054e-02 (4.2990e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6713e-02 (4.3085e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1921e-02 (4.2876e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.6993e-02 (4.2821e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.3682e-02 (4.3433e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2349e-02 (4.3794e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1788e-02 (4.3628e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.1260e-02 (4.3269e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.9673e-02 (4.3151e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6997e-02 (4.3450e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.4209e-02 (4.3893e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5319e-02 (4.3872e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.5253e-02 (4.3757e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.5613e-02 (4.3720e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5828e-02 (4.3803e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7201e-02 (4.3742e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.8279e-02 (4.3751e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3610e-02 (4.3788e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.2551e-02 (4.3689e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.8244e-02 (4.3658e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5919e-02 (4.3760e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.4454e-02 (4.3717e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.8361e-02 (4.3459e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.4668e-02 (4.3565e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.7720e-02 (4.3623e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 8.2458e-02 (4.3964e-02)	Acc@1  97.66 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.116 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.3925e-02 (4.4092e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.6916613578796387
## e[80]       loss.backward (sum) time: 14.293163776397705
## e[80]      optimizer.step (sum) time: 8.177376747131348
## epoch[80] training(only) time: 47.57576298713684
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 8.0859e-01 (8.0859e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.1172e+00 (1.1570e+00)	Acc@1  73.00 ( 73.27)	Acc@5  93.00 ( 92.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0918e+00 (1.1189e+00)	Acc@1  72.00 ( 73.95)	Acc@5  92.00 ( 93.10)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5693e+00 (1.1417e+00)	Acc@1  68.00 ( 73.71)	Acc@5  89.00 ( 92.84)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2588e+00 (1.1557e+00)	Acc@1  76.00 ( 73.73)	Acc@5  94.00 ( 93.02)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.4434e+00 (1.1615e+00)	Acc@1  67.00 ( 73.51)	Acc@5  91.00 ( 92.88)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0801e+00 (1.1338e+00)	Acc@1  74.00 ( 73.59)	Acc@5  97.00 ( 93.30)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4775e+00 (1.1350e+00)	Acc@1  69.00 ( 73.56)	Acc@5  90.00 ( 93.39)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4688e+00 (1.1390e+00)	Acc@1  74.00 ( 73.63)	Acc@5  90.00 ( 93.26)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5322e+00 (1.1266e+00)	Acc@1  64.00 ( 73.76)	Acc@5  92.00 ( 93.33)
 * Acc@1 73.970 Acc@5 93.370
### epoch[80] execution time: 52.58201241493225
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.290 ( 0.290)	Data  0.146 ( 0.146)	Loss 3.7811e-02 (3.7811e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.120 ( 0.135)	Data  0.001 ( 0.014)	Loss 4.4647e-02 (3.8431e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.119 ( 0.129)	Data  0.001 ( 0.008)	Loss 6.5796e-02 (4.1122e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.006)	Loss 4.4281e-02 (4.0190e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.005)	Loss 6.6711e-02 (4.0870e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.4525e-02 (4.1671e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.5125e-02 (4.2036e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.3680e-02 (4.2441e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.8605e-02 (4.2652e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.003)	Loss 6.8665e-02 (4.2668e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4373e-02 (4.2931e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.9734e-02 (4.2729e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.8687e-02 (4.2655e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9388e-02 (4.2423e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.8177e-02 (4.2636e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7638e-02 (4.2550e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.138 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.1483e-02 (4.2475e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.7145e-02 (4.2577e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2349e-02 (4.2214e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9805e-02 (4.2207e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.3752e-02 (4.2001e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7089e-02 (4.1916e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.1350e-02 (4.1991e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7119e-02 (4.2068e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.8732e-02 (4.2325e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.2328e-02 (4.2285e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4525e-02 (4.2197e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7781e-02 (4.2422e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.133 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7384e-02 (4.2440e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.129 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.7007e-02 (4.2565e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4393e-02 (4.2454e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.3284e-02 (4.2653e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.5706e-02 (4.2745e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.6804e-02 (4.2669e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 7.5745e-02 (4.2769e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5115e-02 (4.2883e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.0899e-02 (4.2910e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.3234e-02 (4.2898e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0323e-02 (4.3005e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.105 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1372e-02 (4.3041e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.6925170421600342
## e[81]       loss.backward (sum) time: 14.272676229476929
## e[81]      optimizer.step (sum) time: 8.194200992584229
## epoch[81] training(only) time: 47.53415393829346
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 8.4326e-01 (8.4326e-01)	Acc@1  79.00 ( 79.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.1201e+00 (1.1602e+00)	Acc@1  73.00 ( 73.45)	Acc@5  93.00 ( 92.73)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0801e+00 (1.1203e+00)	Acc@1  74.00 ( 74.14)	Acc@5  92.00 ( 93.10)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5830e+00 (1.1443e+00)	Acc@1  68.00 ( 73.90)	Acc@5  89.00 ( 92.77)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.2793e+00 (1.1599e+00)	Acc@1  75.00 ( 73.78)	Acc@5  94.00 ( 93.00)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4424e+00 (1.1637e+00)	Acc@1  69.00 ( 73.49)	Acc@5  91.00 ( 92.78)
Test: [ 60/100]	Time  0.049 ( 0.051)	Loss 1.0625e+00 (1.1353e+00)	Acc@1  74.00 ( 73.48)	Acc@5  96.00 ( 93.20)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4521e+00 (1.1374e+00)	Acc@1  70.00 ( 73.55)	Acc@5  90.00 ( 93.30)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.4756e+00 (1.1418e+00)	Acc@1  75.00 ( 73.59)	Acc@5  91.00 ( 93.16)
Test: [ 90/100]	Time  0.049 ( 0.050)	Loss 1.5801e+00 (1.1296e+00)	Acc@1  62.00 ( 73.69)	Acc@5  91.00 ( 93.23)
 * Acc@1 73.950 Acc@5 93.250
### epoch[81] execution time: 52.57309412956238
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.292 ( 0.292)	Data  0.133 ( 0.133)	Loss 3.9551e-02 (3.9551e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.120 ( 0.136)	Data  0.001 ( 0.013)	Loss 4.9133e-02 (3.9961e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.116 ( 0.128)	Data  0.001 ( 0.007)	Loss 4.4708e-02 (4.2447e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.122 ( 0.126)	Data  0.001 ( 0.005)	Loss 5.6519e-02 (4.3521e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.004)	Loss 4.9896e-02 (4.1608e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.6326e-02 (4.2923e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.0699e-02 (4.2993e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.5664e-02 (4.2549e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.127 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.4901e-02 (4.2908e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 7.4707e-02 (4.3901e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4067e-02 (4.3913e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7607e-02 (4.4096e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.2725e-02 (4.4297e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.1218e-02 (4.4381e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.134 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9377e-02 (4.4805e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6652e-02 (4.5067e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3274e-02 (4.4660e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3274e-02 (4.4978e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.2826e-02 (4.4786e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5990e-02 (4.4720e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.7312e-02 (4.4583e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2867e-02 (4.4670e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2288e-02 (4.4679e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.5696e-02 (4.4697e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.4270e-02 (4.4635e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9709e-02 (4.4500e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.9723e-02 (4.4518e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5511e-02 (4.4679e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.3671e-02 (4.4630e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.3467e-02 (4.4385e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.6702e-02 (4.4337e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.4565e-02 (4.4487e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.4749e-02 (4.4357e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.5645e-02 (4.4277e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.5645e-02 (4.4194e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1727e-02 (4.4081e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1403e-02 (4.4133e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.9448e-02 (4.4054e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.8809e-02 (4.3982e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.2217e-02 (4.3906e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.68644118309021
## e[82]       loss.backward (sum) time: 14.295226812362671
## e[82]      optimizer.step (sum) time: 8.13637089729309
## epoch[82] training(only) time: 47.52972221374512
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 8.4668e-01 (8.4668e-01)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.1279e+00 (1.1551e+00)	Acc@1  75.00 ( 73.91)	Acc@5  92.00 ( 92.55)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0488e+00 (1.1127e+00)	Acc@1  75.00 ( 74.43)	Acc@5  92.00 ( 93.00)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.5908e+00 (1.1391e+00)	Acc@1  69.00 ( 74.00)	Acc@5  88.00 ( 92.71)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 1.3047e+00 (1.1548e+00)	Acc@1  74.00 ( 73.73)	Acc@5  94.00 ( 92.90)
Test: [ 50/100]	Time  0.058 ( 0.051)	Loss 1.4287e+00 (1.1575e+00)	Acc@1  68.00 ( 73.49)	Acc@5  91.00 ( 92.75)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0693e+00 (1.1300e+00)	Acc@1  74.00 ( 73.43)	Acc@5  96.00 ( 93.13)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4648e+00 (1.1329e+00)	Acc@1  69.00 ( 73.35)	Acc@5  90.00 ( 93.18)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4736e+00 (1.1367e+00)	Acc@1  75.00 ( 73.38)	Acc@5  91.00 ( 93.07)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5459e+00 (1.1248e+00)	Acc@1  64.00 ( 73.48)	Acc@5  91.00 ( 93.19)
 * Acc@1 73.710 Acc@5 93.220
### epoch[82] execution time: 52.54378128051758
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.277 ( 0.277)	Data  0.135 ( 0.135)	Loss 3.7231e-02 (3.7231e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.121 ( 0.135)	Data  0.001 ( 0.013)	Loss 3.2104e-02 (4.9386e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.007)	Loss 4.4464e-02 (4.8325e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.121 ( 0.125)	Data  0.001 ( 0.005)	Loss 3.6896e-02 (4.6213e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 6.1615e-02 (4.5760e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.125 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.0862e-02 (4.5578e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.1708e-02 (4.4795e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.003)	Loss 4.9438e-02 (4.4155e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.003)	Loss 5.2002e-02 (4.3841e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.0833e-02 (4.3586e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.3040e-02 (4.4018e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.8976e-02 (4.3683e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.0894e-02 (4.3441e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6539e-02 (4.3275e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5227e-02 (4.2974e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5898e-02 (4.3304e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.9703e-02 (4.3060e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 6.1523e-02 (4.3095e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 7.2144e-02 (4.3132e-02)	Acc@1  97.66 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5776e-02 (4.3634e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.5685e-02 (4.3566e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 3.0045e-02 (4.3853e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.6154e-02 (4.3800e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 2.7267e-02 (4.3520e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.002)	Loss 4.1565e-02 (4.3481e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.5583e-02 (4.3369e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9978e-02 (4.3486e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.5085e-02 (4.3472e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3243e-02 (4.3436e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.2562e-02 (4.3360e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.125 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.8778e-02 (4.3337e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.130 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.6194e-02 (4.3370e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.0222e-02 (4.3385e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.3671e-02 (4.3398e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9429e-02 (4.3446e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.123 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.5328e-02 (4.3549e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.4474e-02 (4.3652e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.5278e-02 (4.3718e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.119 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.1403e-02 (4.3567e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.104 ( 0.121)	Data  0.001 ( 0.001)	Loss 4.2847e-02 (4.3550e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.6782958507537842
## e[83]       loss.backward (sum) time: 14.320162534713745
## e[83]      optimizer.step (sum) time: 8.043422937393188
## epoch[83] training(only) time: 47.40345048904419
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 8.4180e-01 (8.4180e-01)	Acc@1  80.00 ( 80.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.1016e+00 (1.1620e+00)	Acc@1  75.00 ( 73.82)	Acc@5  92.00 ( 92.00)
Test: [ 20/100]	Time  0.061 ( 0.056)	Loss 1.0508e+00 (1.1202e+00)	Acc@1  75.00 ( 74.33)	Acc@5  93.00 ( 92.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6055e+00 (1.1445e+00)	Acc@1  68.00 ( 73.97)	Acc@5  89.00 ( 92.55)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.2998e+00 (1.1607e+00)	Acc@1  75.00 ( 73.71)	Acc@5  94.00 ( 92.76)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4482e+00 (1.1645e+00)	Acc@1  67.00 ( 73.43)	Acc@5  91.00 ( 92.61)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0791e+00 (1.1375e+00)	Acc@1  74.00 ( 73.48)	Acc@5  96.00 ( 93.10)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4229e+00 (1.1394e+00)	Acc@1  68.00 ( 73.45)	Acc@5  90.00 ( 93.18)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4932e+00 (1.1422e+00)	Acc@1  75.00 ( 73.56)	Acc@5  90.00 ( 93.10)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 1.5645e+00 (1.1296e+00)	Acc@1  62.00 ( 73.67)	Acc@5  91.00 ( 93.22)
 * Acc@1 73.910 Acc@5 93.270
### epoch[83] execution time: 52.41523313522339
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.280 ( 0.280)	Data  0.129 ( 0.129)	Loss 4.4312e-02 (4.4312e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.123 ( 0.136)	Data  0.001 ( 0.013)	Loss 4.6997e-02 (4.1122e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.119 ( 0.129)	Data  0.001 ( 0.007)	Loss 5.3284e-02 (4.4793e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.123 ( 0.126)	Data  0.001 ( 0.005)	Loss 6.0303e-02 (4.4711e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.122 ( 0.125)	Data  0.001 ( 0.004)	Loss 5.1208e-02 (4.4673e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.003)	Loss 4.2999e-02 (4.4937e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.003)	Loss 3.3997e-02 (4.5849e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.8909e-02 (4.5893e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.7089e-02 (4.5488e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.2593e-02 (4.5206e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6295e-02 (4.4964e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0690e-02 (4.4663e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.2175e-02 (4.4885e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.6406e-02 (4.4846e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7607e-02 (4.4786e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5187e-02 (4.4458e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9205e-02 (4.4244e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.2399e-02 (4.4283e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.3264e-02 (4.4267e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5197e-02 (4.4261e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9561e-02 (4.4092e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.4535e-02 (4.4079e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.6824e-02 (4.4082e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7241e-02 (4.4129e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.3274e-02 (4.4494e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.7451e-02 (4.4245e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 6.6772e-02 (4.4316e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.9032e-02 (4.4169e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.3243e-02 (4.4209e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.3976e-02 (4.4141e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.1138e-02 (4.4109e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.7527e-02 (4.4181e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.9795e-02 (4.4014e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 6.0333e-02 (4.3984e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 6.2103e-02 (4.4008e-02)	Acc@1  99.22 ( 99.47)	Acc@5  99.22 (100.00)
Epoch: [84][350/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.5258e-02 (4.3982e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.1392e-02 (4.4086e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.4424e-02 (4.4018e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.121 ( 0.121)	Data  0.001 ( 0.001)	Loss 3.9642e-02 (4.3897e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.104 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.2256e-02 (4.4001e-02)	Acc@1  98.75 ( 99.45)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.688727617263794
## e[84]       loss.backward (sum) time: 14.32576584815979
## e[84]      optimizer.step (sum) time: 8.125566482543945
## epoch[84] training(only) time: 47.550320625305176
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 8.3838e-01 (8.3838e-01)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.0850e+00 (1.1578e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 92.73)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0674e+00 (1.1216e+00)	Acc@1  73.00 ( 74.38)	Acc@5  92.00 ( 93.10)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.5508e+00 (1.1420e+00)	Acc@1  68.00 ( 74.10)	Acc@5  89.00 ( 92.74)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.2930e+00 (1.1568e+00)	Acc@1  78.00 ( 73.90)	Acc@5  94.00 ( 92.98)
Test: [ 50/100]	Time  0.056 ( 0.051)	Loss 1.4443e+00 (1.1622e+00)	Acc@1  68.00 ( 73.76)	Acc@5  91.00 ( 92.86)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0947e+00 (1.1336e+00)	Acc@1  72.00 ( 73.74)	Acc@5  95.00 ( 93.31)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4219e+00 (1.1352e+00)	Acc@1  69.00 ( 73.72)	Acc@5  90.00 ( 93.38)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 1.4941e+00 (1.1386e+00)	Acc@1  75.00 ( 73.75)	Acc@5  90.00 ( 93.26)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5361e+00 (1.1265e+00)	Acc@1  63.00 ( 73.88)	Acc@5  91.00 ( 93.34)
 * Acc@1 74.100 Acc@5 93.320
### epoch[84] execution time: 52.570900201797485
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.294 ( 0.294)	Data  0.150 ( 0.150)	Loss 3.9825e-02 (3.9825e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.122 ( 0.136)	Data  0.001 ( 0.014)	Loss 4.6844e-02 (4.3415e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.118 ( 0.129)	Data  0.001 ( 0.008)	Loss 2.4551e-02 (4.1070e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.120 ( 0.126)	Data  0.001 ( 0.006)	Loss 5.4779e-02 (4.0694e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.127 ( 0.125)	Data  0.001 ( 0.005)	Loss 3.9886e-02 (4.2812e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.3630e-02 (4.1462e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.121 ( 0.124)	Data  0.001 ( 0.003)	Loss 5.7983e-02 (4.1227e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.2633e-02 (4.1911e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.124 ( 0.123)	Data  0.001 ( 0.003)	Loss 7.2998e-02 (4.2318e-02)	Acc@1  98.44 ( 99.59)	Acc@5  99.22 ( 99.98)
Epoch: [85][ 90/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 2.2705e-02 (4.2220e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 ( 99.98)
Epoch: [85][100/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.3386e-02 (4.2202e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 ( 99.98)
Epoch: [85][110/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.2521e-02 (4.2370e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 ( 99.99)
Epoch: [85][120/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.6610e-02 (4.2540e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 ( 99.99)
Epoch: [85][130/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.3223e-02 (4.2371e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 ( 99.99)
Epoch: [85][140/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.8391e-02 (4.2698e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 ( 99.99)
Epoch: [85][150/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.5054e-02 (4.2877e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 ( 99.99)
Epoch: [85][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4281e-02 (4.2976e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 ( 99.99)
Epoch: [85][170/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0411e-02 (4.3139e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 ( 99.99)
Epoch: [85][180/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.4575e-02 (4.3246e-02)	Acc@1  97.66 ( 99.53)	Acc@5 100.00 ( 99.99)
Epoch: [85][190/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6478e-02 (4.3349e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [85][200/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5126e-02 (4.3355e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [85][210/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0079e-02 (4.3414e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [85][220/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (4.3503e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [85][230/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9469e-02 (4.3462e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [85][240/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.1168e-02 (4.3641e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [85][250/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.0558e-02 (4.3505e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [85][260/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.1188e-02 (4.3363e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 ( 99.99)
Epoch: [85][270/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6326e-02 (4.3420e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [85][280/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5736e-02 (4.3591e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [85][290/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 6.2622e-02 (4.3834e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [85][300/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.5023e-02 (4.3861e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [85][310/391]	Time  0.132 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.9642e-02 (4.3881e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [85][320/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.8584e-02 (4.3962e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [85][330/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.3203e-02 (4.3811e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [85][340/391]	Time  0.128 ( 0.121)	Data  0.001 ( 0.001)	Loss 2.5909e-02 (4.3843e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [85][350/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.7281e-02 (4.3829e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [85][360/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.5583e-02 (4.3818e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [85][370/391]	Time  0.118 ( 0.121)	Data  0.001 ( 0.001)	Loss 7.7087e-02 (4.4092e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [85][380/391]	Time  0.120 ( 0.121)	Data  0.001 ( 0.001)	Loss 6.7078e-02 (4.4023e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [85][390/391]	Time  0.109 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.1331e-02 (4.4122e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 ( 99.99)
## e[85] optimizer.zero_grad (sum) time: 0.688873291015625
## e[85]       loss.backward (sum) time: 14.290873050689697
## e[85]      optimizer.step (sum) time: 8.122397422790527
## epoch[85] training(only) time: 47.55333590507507
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 8.4326e-01 (8.4326e-01)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.0986e+00 (1.1602e+00)	Acc@1  75.00 ( 73.91)	Acc@5  93.00 ( 92.55)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0889e+00 (1.1222e+00)	Acc@1  73.00 ( 74.48)	Acc@5  93.00 ( 93.00)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.5801e+00 (1.1423e+00)	Acc@1  70.00 ( 74.26)	Acc@5  88.00 ( 92.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2852e+00 (1.1568e+00)	Acc@1  76.00 ( 74.05)	Acc@5  94.00 ( 92.90)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.4600e+00 (1.1624e+00)	Acc@1  67.00 ( 73.67)	Acc@5  91.00 ( 92.76)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0723e+00 (1.1347e+00)	Acc@1  75.00 ( 73.64)	Acc@5  97.00 ( 93.20)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 1.4619e+00 (1.1353e+00)	Acc@1  68.00 ( 73.62)	Acc@5  90.00 ( 93.30)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.5039e+00 (1.1399e+00)	Acc@1  74.00 ( 73.64)	Acc@5  90.00 ( 93.14)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5264e+00 (1.1273e+00)	Acc@1  63.00 ( 73.74)	Acc@5  92.00 ( 93.24)
 * Acc@1 74.000 Acc@5 93.270
### epoch[85] execution time: 52.578319787979126
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.294 ( 0.294)	Data  0.155 ( 0.155)	Loss 5.2643e-02 (5.2643e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.121 ( 0.137)	Data  0.001 ( 0.015)	Loss 2.8412e-02 (3.9651e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.126 ( 0.129)	Data  0.001 ( 0.008)	Loss 6.9824e-02 (4.2049e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.122 ( 0.126)	Data  0.001 ( 0.006)	Loss 8.0933e-02 (4.2847e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.005)	Loss 3.8696e-02 (4.3192e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.119 ( 0.124)	Data  0.001 ( 0.004)	Loss 4.7089e-02 (4.3017e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.004)	Loss 7.7942e-02 (4.3070e-02)	Acc@1  97.66 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.3284e-02 (4.3575e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.6549e-02 (4.3841e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.5796e-02 (4.3809e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.121 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.8391e-02 (4.4081e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.9215e-02 (4.4247e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.1687e-02 (4.4092e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3732e-02 (4.4177e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.1779e-02 (4.3971e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.6489e-02 (4.3884e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.3483e-02 (4.3881e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.0863e-02 (4.3522e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7058e-02 (4.3764e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.8336e-02 (4.3713e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.131 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0685e-02 (4.3445e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.1652e-02 (4.3283e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.6427e-02 (4.3366e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4973e-02 (4.3403e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.8544e-02 (4.3599e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 8.9966e-02 (4.3779e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.5063e-02 (4.3909e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0476e-02 (4.3687e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.8187e-02 (4.3549e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.0497e-02 (4.3819e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.2715e-02 (4.3797e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.6825e-02 (4.3662e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.1281e-02 (4.3779e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.8920e-02 (4.3842e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.1168e-02 (4.3747e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.4342e-02 (4.3713e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.1565e-02 (4.3572e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 6.9702e-02 (4.3778e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.128 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.8336e-02 (4.3730e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.115 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.6763e-02 (4.3751e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.686556339263916
## e[86]       loss.backward (sum) time: 14.3097562789917
## e[86]      optimizer.step (sum) time: 8.128540515899658
## epoch[86] training(only) time: 47.58038592338562
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 8.1055e-01 (8.1055e-01)	Acc@1  77.00 ( 77.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.1182e+00 (1.1602e+00)	Acc@1  74.00 ( 73.64)	Acc@5  93.00 ( 92.64)
Test: [ 20/100]	Time  0.046 ( 0.056)	Loss 1.0439e+00 (1.1177e+00)	Acc@1  76.00 ( 74.29)	Acc@5  92.00 ( 92.90)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.5869e+00 (1.1427e+00)	Acc@1  68.00 ( 74.00)	Acc@5  89.00 ( 92.68)
Test: [ 40/100]	Time  0.051 ( 0.052)	Loss 1.3184e+00 (1.1596e+00)	Acc@1  74.00 ( 73.88)	Acc@5  94.00 ( 92.90)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.4248e+00 (1.1623e+00)	Acc@1  67.00 ( 73.59)	Acc@5  92.00 ( 92.75)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.0957e+00 (1.1339e+00)	Acc@1  72.00 ( 73.66)	Acc@5  97.00 ( 93.16)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4619e+00 (1.1356e+00)	Acc@1  69.00 ( 73.63)	Acc@5  90.00 ( 93.25)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4795e+00 (1.1403e+00)	Acc@1  75.00 ( 73.68)	Acc@5  90.00 ( 93.12)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.5303e+00 (1.1279e+00)	Acc@1  65.00 ( 73.79)	Acc@5  91.00 ( 93.23)
 * Acc@1 74.060 Acc@5 93.300
### epoch[86] execution time: 52.60055232048035
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.293 ( 0.293)	Data  0.154 ( 0.154)	Loss 3.5370e-02 (3.5370e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.117 ( 0.135)	Data  0.001 ( 0.015)	Loss 6.2744e-02 (4.1447e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.119 ( 0.128)	Data  0.001 ( 0.008)	Loss 2.8549e-02 (3.8868e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.129 ( 0.126)	Data  0.001 ( 0.006)	Loss 3.4821e-02 (4.0470e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.119 ( 0.125)	Data  0.001 ( 0.005)	Loss 3.1616e-02 (3.9970e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 3.2837e-02 (4.1871e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.131 ( 0.124)	Data  0.001 ( 0.003)	Loss 4.6417e-02 (4.0918e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.118 ( 0.123)	Data  0.001 ( 0.003)	Loss 5.2307e-02 (4.0904e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.6509e-02 (4.2026e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 3.8483e-02 (4.1782e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 2.9663e-02 (4.1778e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3304e-02 (4.3174e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0121e-02 (4.2806e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9327e-02 (4.3110e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.8879e-02 (4.2661e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.7079e-02 (4.2875e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5746e-02 (4.3248e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2410e-02 (4.3034e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.0192e-02 (4.2919e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.3181e-02 (4.3229e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.8431e-02 (4.3314e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.7831e-02 (4.3588e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.9846e-02 (4.3556e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.4536e-02 (4.3608e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.0344e-02 (4.3724e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1250e-02 (4.3734e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 6.2500e-02 (4.3744e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1555e-02 (4.3757e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9438e-02 (4.3762e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.2043e-02 (4.3647e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0518e-02 (4.3682e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.3203e-02 (4.3611e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.6448e-02 (4.3708e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.7729e-02 (4.3495e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.136 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.8147e-02 (4.3325e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.1758e-02 (4.3359e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.5044e-02 (4.3409e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.127 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.5635e-02 (4.3283e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.9042e-02 (4.3306e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.111 ( 0.122)	Data  0.001 ( 0.001)	Loss 6.8420e-02 (4.3349e-02)	Acc@1  98.75 ( 99.48)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.6904003620147705
## e[87]       loss.backward (sum) time: 14.358611345291138
## e[87]      optimizer.step (sum) time: 8.171015977859497
## epoch[87] training(only) time: 47.63673782348633
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 8.3203e-01 (8.3203e-01)	Acc@1  78.00 ( 78.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.058 ( 0.061)	Loss 1.1191e+00 (1.1462e+00)	Acc@1  75.00 ( 73.73)	Acc@5  91.00 ( 92.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0820e+00 (1.1117e+00)	Acc@1  74.00 ( 74.29)	Acc@5  93.00 ( 92.81)
Test: [ 30/100]	Time  0.050 ( 0.052)	Loss 1.5566e+00 (1.1328e+00)	Acc@1  68.00 ( 73.84)	Acc@5  89.00 ( 92.65)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.2939e+00 (1.1475e+00)	Acc@1  75.00 ( 73.71)	Acc@5  93.00 ( 92.85)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.4053e+00 (1.1514e+00)	Acc@1  67.00 ( 73.41)	Acc@5  91.00 ( 92.76)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0791e+00 (1.1245e+00)	Acc@1  77.00 ( 73.49)	Acc@5  96.00 ( 93.20)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4443e+00 (1.1278e+00)	Acc@1  68.00 ( 73.45)	Acc@5  90.00 ( 93.25)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4824e+00 (1.1330e+00)	Acc@1  75.00 ( 73.57)	Acc@5  91.00 ( 93.16)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5039e+00 (1.1210e+00)	Acc@1  64.00 ( 73.74)	Acc@5  91.00 ( 93.23)
 * Acc@1 73.930 Acc@5 93.260
### epoch[87] execution time: 52.65456295013428
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.287 ( 0.287)	Data  0.144 ( 0.144)	Loss 5.8289e-02 (5.8289e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.122 ( 0.137)	Data  0.001 ( 0.014)	Loss 4.3243e-02 (4.7910e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.008)	Loss 2.9022e-02 (4.2799e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.119 ( 0.126)	Data  0.001 ( 0.006)	Loss 4.4037e-02 (4.3230e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.004)	Loss 3.2440e-02 (4.2387e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.120 ( 0.124)	Data  0.001 ( 0.004)	Loss 2.3849e-02 (4.2258e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.118 ( 0.124)	Data  0.001 ( 0.003)	Loss 4.0894e-02 (4.2140e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.1199e-02 (4.1424e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.5319e-02 (4.1219e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.003)	Loss 4.4342e-02 (4.1543e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 4.4891e-02 (4.1829e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3549e-02 (4.2253e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.4169e-02 (4.2447e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6600e-02 (4.2035e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.5776e-02 (4.2059e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6743e-02 (4.2009e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1586e-02 (4.1792e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.132 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.8502e-02 (4.2326e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 7.1838e-02 (4.2860e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.3925e-02 (4.3102e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.133 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.0323e-02 (4.3143e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.2969e-02 (4.3242e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1311e-02 (4.3099e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.130 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4525e-02 (4.3358e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.0701e-02 (4.3304e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6051e-02 (4.3428e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.6661e-02 (4.3267e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.4861e-02 (4.3096e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4393e-02 (4.3323e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1860e-02 (4.3218e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.9194e-02 (4.3180e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.7634e-02 (4.3213e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.6072e-02 (4.3195e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.4149e-02 (4.3210e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 6.1279e-02 (4.3205e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.001)	Loss 4.5929e-02 (4.3077e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.5176e-02 (4.3243e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.2654e-02 (4.3119e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.1941e-02 (4.3200e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.108 ( 0.121)	Data  0.001 ( 0.001)	Loss 5.0110e-02 (4.3090e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.6974184513092041
## e[88]       loss.backward (sum) time: 14.284547328948975
## e[88]      optimizer.step (sum) time: 8.140547275543213
## epoch[88] training(only) time: 47.587042570114136
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 8.0859e-01 (8.0859e-01)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.1309e+00 (1.1513e+00)	Acc@1  74.00 ( 73.64)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.052 ( 0.055)	Loss 1.0879e+00 (1.1140e+00)	Acc@1  72.00 ( 74.00)	Acc@5  92.00 ( 92.90)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.5615e+00 (1.1393e+00)	Acc@1  66.00 ( 73.52)	Acc@5  89.00 ( 92.71)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.2754e+00 (1.1544e+00)	Acc@1  75.00 ( 73.44)	Acc@5  95.00 ( 92.95)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4424e+00 (1.1593e+00)	Acc@1  67.00 ( 73.29)	Acc@5  91.00 ( 92.84)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0625e+00 (1.1311e+00)	Acc@1  75.00 ( 73.33)	Acc@5  96.00 ( 93.28)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4395e+00 (1.1314e+00)	Acc@1  70.00 ( 73.42)	Acc@5  90.00 ( 93.35)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 1.4844e+00 (1.1360e+00)	Acc@1  75.00 ( 73.54)	Acc@5  91.00 ( 93.21)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 1.5244e+00 (1.1244e+00)	Acc@1  62.00 ( 73.67)	Acc@5  92.00 ( 93.29)
 * Acc@1 73.910 Acc@5 93.310
### epoch[88] execution time: 52.60294198989868
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.291 ( 0.291)	Data  0.149 ( 0.149)	Loss 4.4708e-02 (4.4708e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.122 ( 0.137)	Data  0.001 ( 0.014)	Loss 3.5675e-02 (3.8742e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.120 ( 0.129)	Data  0.001 ( 0.008)	Loss 5.6458e-02 (3.9568e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.121 ( 0.127)	Data  0.001 ( 0.006)	Loss 3.7842e-02 (4.1398e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.123 ( 0.126)	Data  0.001 ( 0.005)	Loss 8.0139e-02 (4.4271e-02)	Acc@1  96.88 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.120 ( 0.125)	Data  0.001 ( 0.004)	Loss 6.2988e-02 (4.5542e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.133 ( 0.124)	Data  0.001 ( 0.003)	Loss 5.6946e-02 (4.5232e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.122 ( 0.124)	Data  0.001 ( 0.003)	Loss 4.1809e-02 (4.4708e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.123 ( 0.124)	Data  0.001 ( 0.003)	Loss 3.5583e-02 (4.3960e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.128 ( 0.123)	Data  0.001 ( 0.003)	Loss 6.6895e-02 (4.4423e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 4.6234e-02 (4.5057e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [89][110/391]	Time  0.117 ( 0.123)	Data  0.001 ( 0.002)	Loss 7.5317e-02 (4.5726e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [89][120/391]	Time  0.122 ( 0.123)	Data  0.001 ( 0.002)	Loss 4.7058e-02 (4.5628e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [89][130/391]	Time  0.123 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.6438e-02 (4.5146e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [89][140/391]	Time  0.120 ( 0.123)	Data  0.001 ( 0.002)	Loss 4.7516e-02 (4.5001e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [89][150/391]	Time  0.119 ( 0.123)	Data  0.001 ( 0.002)	Loss 3.5065e-02 (4.4902e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [89][160/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.4332e-02 (4.5063e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.124 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6072e-02 (4.4963e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.125 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.5450e-02 (4.5096e-02)	Acc@1  97.66 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5797e-02 (4.5031e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.1799e-02 (4.4918e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.3640e-02 (4.5115e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6255e-02 (4.5052e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [89][230/391]	Time  0.118 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.5574e-02 (4.4990e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [89][240/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5675e-02 (4.5069e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [89][250/391]	Time  0.122 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.6652e-02 (4.5043e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [89][260/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.7577e-02 (4.5056e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [89][270/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 4.2145e-02 (4.5028e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [89][280/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.6122e-02 (4.4972e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [89][290/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 2.7634e-02 (4.4899e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [89][300/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.002)	Loss 3.5431e-02 (4.4709e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [89][310/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.002)	Loss 5.3741e-02 (4.4795e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [89][320/391]	Time  0.121 ( 0.122)	Data  0.001 ( 0.001)	Loss 6.0913e-02 (4.4809e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.9368e-02 (4.4555e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 5.3436e-02 (4.4488e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.134 ( 0.122)	Data  0.001 ( 0.001)	Loss 2.2049e-02 (4.4299e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.119 ( 0.122)	Data  0.001 ( 0.001)	Loss 7.0923e-02 (4.4509e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.123 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.3020e-02 (4.4453e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.120 ( 0.122)	Data  0.001 ( 0.001)	Loss 3.4546e-02 (4.4496e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.112 ( 0.122)	Data  0.001 ( 0.001)	Loss 7.3425e-02 (4.4312e-02)	Acc@1  98.75 ( 99.50)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.694162130355835
## e[89]       loss.backward (sum) time: 14.344080209732056
## e[89]      optimizer.step (sum) time: 8.11584758758545
## epoch[89] training(only) time: 47.64627003669739
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 8.4424e-01 (8.4424e-01)	Acc@1  78.00 ( 78.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.059 ( 0.062)	Loss 1.0928e+00 (1.1504e+00)	Acc@1  74.00 ( 73.82)	Acc@5  93.00 ( 92.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0361e+00 (1.1108e+00)	Acc@1  73.00 ( 74.38)	Acc@5  92.00 ( 93.29)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5791e+00 (1.1348e+00)	Acc@1  67.00 ( 74.26)	Acc@5  89.00 ( 92.90)
Test: [ 40/100]	Time  0.050 ( 0.051)	Loss 1.2969e+00 (1.1500e+00)	Acc@1  75.00 ( 74.02)	Acc@5  94.00 ( 93.15)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.4248e+00 (1.1549e+00)	Acc@1  66.00 ( 73.76)	Acc@5  92.00 ( 93.00)
Test: [ 60/100]	Time  0.057 ( 0.051)	Loss 1.0977e+00 (1.1273e+00)	Acc@1  74.00 ( 73.70)	Acc@5  97.00 ( 93.43)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4326e+00 (1.1302e+00)	Acc@1  69.00 ( 73.72)	Acc@5  90.00 ( 93.48)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4980e+00 (1.1349e+00)	Acc@1  75.00 ( 73.78)	Acc@5  91.00 ( 93.37)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.5557e+00 (1.1228e+00)	Acc@1  62.00 ( 73.81)	Acc@5  91.00 ( 93.44)
 * Acc@1 74.080 Acc@5 93.460
### epoch[89] execution time: 52.69582509994507
### Training complete:
#### total training(only) time: 4272.502882480621
##### Total run time: 4727.444730758667
