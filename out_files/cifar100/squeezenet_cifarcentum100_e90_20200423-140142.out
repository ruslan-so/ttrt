# Model: squeezenet
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.squeezenet
<function squeezenet at 0x7fc23871ef28>
# model requested: 'squeezenet'
# printing out the model
SqueezeNet(
  (stem): Sequential(
    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (fire2): Fire(
    (squeeze): Sequential(
      (0): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire3): Fire(
    (squeeze): Sequential(
      (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire4): Fire(
    (squeeze): Sequential(
      (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire5): Fire(
    (squeeze): Sequential(
      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire6): Fire(
    (squeeze): Sequential(
      (0): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire7): Fire(
    (squeeze): Sequential(
      (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire8): Fire(
    (squeeze): Sequential(
      (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (fire9): Fire(
    (squeeze): Sequential(
      (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_1x1): Sequential(
      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (expand_3x3): Sequential(
      (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  )
  (conv10): Conv2d(512, 100, kernel_size=(1, 1), stride=(1, 1))
  (avg): AdaptiveAvgPool2d(output_size=1)
  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
# model is full precision
# Model: squeezenet
# Dataset: cifarcentum
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.134 ( 3.134)	Data  0.112 ( 0.112)	Loss 4.6095e+00 (4.6095e+00)	Acc@1   1.56 (  1.56)	Acc@5   3.12 (  3.12)
Epoch: [0][ 10/391]	Time  0.041 ( 0.323)	Data  0.001 ( 0.011)	Loss 4.6850e+00 (4.6499e+00)	Acc@1   2.34 (  1.21)	Acc@5   9.38 (  5.89)
Epoch: [0][ 20/391]	Time  0.040 ( 0.188)	Data  0.001 ( 0.006)	Loss 4.4937e+00 (4.5999e+00)	Acc@1   5.47 (  2.31)	Acc@5  12.50 (  8.33)
Epoch: [0][ 30/391]	Time  0.039 ( 0.141)	Data  0.001 ( 0.005)	Loss 4.3028e+00 (4.5415e+00)	Acc@1   3.91 (  2.57)	Acc@5  14.06 ( 10.21)
Epoch: [0][ 40/391]	Time  0.042 ( 0.116)	Data  0.001 ( 0.004)	Loss 4.3437e+00 (4.5119e+00)	Acc@1   6.25 (  2.95)	Acc@5  16.41 ( 11.59)
Epoch: [0][ 50/391]	Time  0.041 ( 0.102)	Data  0.001 ( 0.003)	Loss 4.2662e+00 (4.4789e+00)	Acc@1   3.12 (  3.06)	Acc@5  19.53 ( 12.42)
Epoch: [0][ 60/391]	Time  0.039 ( 0.091)	Data  0.001 ( 0.003)	Loss 4.1805e+00 (4.4373e+00)	Acc@1   4.69 (  3.41)	Acc@5  19.53 ( 13.67)
Epoch: [0][ 70/391]	Time  0.040 ( 0.084)	Data  0.001 ( 0.003)	Loss 4.0928e+00 (4.4028e+00)	Acc@1  11.72 (  3.71)	Acc@5  23.44 ( 14.59)
Epoch: [0][ 80/391]	Time  0.040 ( 0.079)	Data  0.001 ( 0.002)	Loss 4.2653e+00 (4.3710e+00)	Acc@1   5.47 (  4.02)	Acc@5  16.41 ( 15.44)
Epoch: [0][ 90/391]	Time  0.056 ( 0.075)	Data  0.001 ( 0.002)	Loss 4.1118e+00 (4.3385e+00)	Acc@1   4.69 (  4.22)	Acc@5  21.88 ( 16.26)
Epoch: [0][100/391]	Time  0.040 ( 0.072)	Data  0.002 ( 0.002)	Loss 3.9262e+00 (4.3080e+00)	Acc@1   8.59 (  4.49)	Acc@5  30.47 ( 17.05)
Epoch: [0][110/391]	Time  0.040 ( 0.069)	Data  0.001 ( 0.002)	Loss 3.8871e+00 (4.2835e+00)	Acc@1   7.03 (  4.62)	Acc@5  32.03 ( 17.93)
Epoch: [0][120/391]	Time  0.041 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1463e+00 (4.2574e+00)	Acc@1   5.47 (  4.82)	Acc@5  23.44 ( 18.64)
Epoch: [0][130/391]	Time  0.040 ( 0.064)	Data  0.002 ( 0.002)	Loss 3.8572e+00 (4.2374e+00)	Acc@1   6.25 (  4.94)	Acc@5  32.03 ( 19.35)
Epoch: [0][140/391]	Time  0.043 ( 0.062)	Data  0.001 ( 0.002)	Loss 3.9798e+00 (4.2162e+00)	Acc@1   4.69 (  5.06)	Acc@5  22.66 ( 20.00)
Epoch: [0][150/391]	Time  0.039 ( 0.061)	Data  0.001 ( 0.002)	Loss 3.9374e+00 (4.1993e+00)	Acc@1   8.59 (  5.25)	Acc@5  26.56 ( 20.49)
Epoch: [0][160/391]	Time  0.040 ( 0.060)	Data  0.001 ( 0.002)	Loss 3.9827e+00 (4.1791e+00)	Acc@1   6.25 (  5.55)	Acc@5  25.00 ( 21.04)
Epoch: [0][170/391]	Time  0.039 ( 0.059)	Data  0.001 ( 0.002)	Loss 3.6478e+00 (4.1613e+00)	Acc@1  14.06 (  5.77)	Acc@5  43.75 ( 21.56)
Epoch: [0][180/391]	Time  0.039 ( 0.058)	Data  0.001 ( 0.002)	Loss 3.8060e+00 (4.1440e+00)	Acc@1  11.72 (  5.92)	Acc@5  31.25 ( 22.06)
Epoch: [0][190/391]	Time  0.043 ( 0.057)	Data  0.001 ( 0.002)	Loss 3.7752e+00 (4.1274e+00)	Acc@1   8.59 (  6.16)	Acc@5  42.19 ( 22.60)
Epoch: [0][200/391]	Time  0.045 ( 0.056)	Data  0.001 ( 0.002)	Loss 3.7397e+00 (4.1075e+00)	Acc@1  10.16 (  6.41)	Acc@5  33.59 ( 23.23)
Epoch: [0][210/391]	Time  0.045 ( 0.055)	Data  0.001 ( 0.002)	Loss 3.7596e+00 (4.0904e+00)	Acc@1  14.06 (  6.62)	Acc@5  34.38 ( 23.79)
Epoch: [0][220/391]	Time  0.042 ( 0.055)	Data  0.001 ( 0.002)	Loss 3.6955e+00 (4.0765e+00)	Acc@1  11.72 (  6.79)	Acc@5  36.72 ( 24.18)
Epoch: [0][230/391]	Time  0.042 ( 0.054)	Data  0.002 ( 0.002)	Loss 3.4992e+00 (4.0615e+00)	Acc@1  14.06 (  6.98)	Acc@5  43.75 ( 24.75)
Epoch: [0][240/391]	Time  0.043 ( 0.054)	Data  0.002 ( 0.002)	Loss 3.9129e+00 (4.0499e+00)	Acc@1   7.03 (  7.16)	Acc@5  35.94 ( 25.06)
Epoch: [0][250/391]	Time  0.037 ( 0.053)	Data  0.001 ( 0.002)	Loss 3.6700e+00 (4.0365e+00)	Acc@1  13.28 (  7.33)	Acc@5  33.59 ( 25.45)
Epoch: [0][260/391]	Time  0.039 ( 0.053)	Data  0.001 ( 0.002)	Loss 3.8933e+00 (4.0242e+00)	Acc@1  10.16 (  7.46)	Acc@5  33.59 ( 25.85)
Epoch: [0][270/391]	Time  0.041 ( 0.052)	Data  0.001 ( 0.001)	Loss 3.4373e+00 (4.0096e+00)	Acc@1  21.09 (  7.65)	Acc@5  44.53 ( 26.32)
Epoch: [0][280/391]	Time  0.039 ( 0.052)	Data  0.001 ( 0.001)	Loss 3.6524e+00 (3.9973e+00)	Acc@1  14.06 (  7.85)	Acc@5  41.41 ( 26.73)
Epoch: [0][290/391]	Time  0.044 ( 0.051)	Data  0.001 ( 0.001)	Loss 3.7580e+00 (3.9872e+00)	Acc@1  14.06 (  8.02)	Acc@5  32.81 ( 27.04)
Epoch: [0][300/391]	Time  0.039 ( 0.051)	Data  0.001 ( 0.001)	Loss 3.5749e+00 (3.9738e+00)	Acc@1  12.50 (  8.19)	Acc@5  38.28 ( 27.41)
Epoch: [0][310/391]	Time  0.041 ( 0.051)	Data  0.001 ( 0.001)	Loss 3.8390e+00 (3.9629e+00)	Acc@1   9.38 (  8.33)	Acc@5  29.69 ( 27.78)
Epoch: [0][320/391]	Time  0.054 ( 0.050)	Data  0.001 ( 0.001)	Loss 3.7182e+00 (3.9502e+00)	Acc@1  15.62 (  8.52)	Acc@5  35.94 ( 28.16)
Epoch: [0][330/391]	Time  0.041 ( 0.050)	Data  0.002 ( 0.001)	Loss 3.4904e+00 (3.9404e+00)	Acc@1  13.28 (  8.69)	Acc@5  42.19 ( 28.41)
Epoch: [0][340/391]	Time  0.039 ( 0.050)	Data  0.001 ( 0.001)	Loss 3.5686e+00 (3.9301e+00)	Acc@1  14.06 (  8.86)	Acc@5  40.62 ( 28.76)
Epoch: [0][350/391]	Time  0.042 ( 0.050)	Data  0.002 ( 0.001)	Loss 3.4359e+00 (3.9205e+00)	Acc@1  13.28 (  8.98)	Acc@5  44.53 ( 29.05)
Epoch: [0][360/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.001)	Loss 3.6794e+00 (3.9107e+00)	Acc@1  14.84 (  9.15)	Acc@5  36.72 ( 29.36)
Epoch: [0][370/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.001)	Loss 3.5406e+00 (3.9001e+00)	Acc@1  15.62 (  9.33)	Acc@5  39.84 ( 29.66)
Epoch: [0][380/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.001)	Loss 3.5714e+00 (3.8898e+00)	Acc@1  20.31 (  9.51)	Acc@5  40.62 ( 30.00)
Epoch: [0][390/391]	Time  0.243 ( 0.049)	Data  0.001 ( 0.001)	Loss 3.6624e+00 (3.8805e+00)	Acc@1  15.00 (  9.69)	Acc@5  35.00 ( 30.31)
## e[0] optimizer.zero_grad (sum) time: 0.2648005485534668
## e[0]       loss.backward (sum) time: 4.3036110401153564
## e[0]      optimizer.step (sum) time: 1.8637127876281738
## epoch[0] training(only) time: 19.26174020767212
# Switched to evaluate mode...
Test: [  0/100]	Time  0.246 ( 0.246)	Loss 3.8769e+00 (3.8769e+00)	Acc@1  12.00 ( 12.00)	Acc@5  38.00 ( 38.00)
Test: [ 10/100]	Time  0.023 ( 0.042)	Loss 3.8340e+00 (3.6709e+00)	Acc@1   5.00 ( 13.82)	Acc@5  37.00 ( 39.18)
Test: [ 20/100]	Time  0.024 ( 0.033)	Loss 3.6670e+00 (3.6219e+00)	Acc@1  11.00 ( 13.90)	Acc@5  40.00 ( 39.19)
Test: [ 30/100]	Time  0.019 ( 0.029)	Loss 3.8709e+00 (3.6297e+00)	Acc@1  12.00 ( 14.06)	Acc@5  34.00 ( 39.03)
Test: [ 40/100]	Time  0.024 ( 0.028)	Loss 3.6865e+00 (3.6213e+00)	Acc@1  13.00 ( 14.15)	Acc@5  40.00 ( 38.98)
Test: [ 50/100]	Time  0.023 ( 0.026)	Loss 3.4964e+00 (3.6106e+00)	Acc@1  23.00 ( 14.29)	Acc@5  37.00 ( 39.39)
Test: [ 60/100]	Time  0.021 ( 0.026)	Loss 3.4818e+00 (3.5990e+00)	Acc@1  14.00 ( 14.28)	Acc@5  41.00 ( 39.62)
Test: [ 70/100]	Time  0.018 ( 0.025)	Loss 3.8565e+00 (3.5997e+00)	Acc@1   7.00 ( 14.14)	Acc@5  35.00 ( 39.76)
Test: [ 80/100]	Time  0.018 ( 0.024)	Loss 3.7474e+00 (3.6135e+00)	Acc@1  12.00 ( 14.22)	Acc@5  42.00 ( 39.53)
Test: [ 90/100]	Time  0.018 ( 0.024)	Loss 3.7088e+00 (3.6142e+00)	Acc@1  12.00 ( 14.21)	Acc@5  38.00 ( 39.57)
 * Acc@1 14.350 Acc@5 39.670
### epoch[0] execution time: 21.667131185531616
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.217 ( 0.217)	Data  0.158 ( 0.158)	Loss 3.5831e+00 (3.5831e+00)	Acc@1  20.31 ( 20.31)	Acc@5  41.41 ( 41.41)
Epoch: [1][ 10/391]	Time  0.044 ( 0.058)	Data  0.001 ( 0.015)	Loss 3.4731e+00 (3.4496e+00)	Acc@1  14.06 ( 16.48)	Acc@5  35.94 ( 40.91)
Epoch: [1][ 20/391]	Time  0.039 ( 0.050)	Data  0.001 ( 0.008)	Loss 3.2767e+00 (3.4293e+00)	Acc@1  21.88 ( 16.59)	Acc@5  42.97 ( 41.78)
Epoch: [1][ 30/391]	Time  0.038 ( 0.047)	Data  0.001 ( 0.006)	Loss 3.7101e+00 (3.4450e+00)	Acc@1  12.50 ( 16.28)	Acc@5  32.81 ( 42.21)
Epoch: [1][ 40/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.005)	Loss 3.4072e+00 (3.4453e+00)	Acc@1  10.16 ( 16.20)	Acc@5  43.75 ( 42.26)
Epoch: [1][ 50/391]	Time  0.037 ( 0.045)	Data  0.001 ( 0.004)	Loss 3.3090e+00 (3.4279e+00)	Acc@1  19.53 ( 16.42)	Acc@5  43.75 ( 42.71)
Epoch: [1][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.6441e+00 (3.4174e+00)	Acc@1  16.41 ( 16.65)	Acc@5  42.19 ( 43.38)
Epoch: [1][ 70/391]	Time  0.040 ( 0.043)	Data  0.002 ( 0.003)	Loss 3.4867e+00 (3.4167e+00)	Acc@1  22.66 ( 16.86)	Acc@5  40.62 ( 43.63)
Epoch: [1][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.2980e+00 (3.4063e+00)	Acc@1  16.41 ( 17.28)	Acc@5  53.91 ( 44.07)
Epoch: [1][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.2050e+00 (3.3955e+00)	Acc@1  21.09 ( 17.50)	Acc@5  50.78 ( 44.55)
Epoch: [1][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2141e+00 (3.3895e+00)	Acc@1  14.84 ( 17.57)	Acc@5  47.66 ( 44.57)
Epoch: [1][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3408e+00 (3.3806e+00)	Acc@1  14.84 ( 17.80)	Acc@5  42.19 ( 44.83)
Epoch: [1][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4107e+00 (3.3743e+00)	Acc@1  14.84 ( 17.81)	Acc@5  39.84 ( 45.00)
Epoch: [1][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4473e+00 (3.3671e+00)	Acc@1  13.28 ( 17.84)	Acc@5  42.97 ( 45.22)
Epoch: [1][140/391]	Time  0.038 ( 0.042)	Data  0.002 ( 0.002)	Loss 2.9702e+00 (3.3535e+00)	Acc@1  21.09 ( 17.91)	Acc@5  55.47 ( 45.62)
Epoch: [1][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3831e+00 (3.3442e+00)	Acc@1  11.72 ( 17.98)	Acc@5  47.66 ( 45.92)
Epoch: [1][160/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2605e+00 (3.3337e+00)	Acc@1  19.53 ( 18.20)	Acc@5  44.53 ( 46.14)
Epoch: [1][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2696e+00 (3.3275e+00)	Acc@1  24.22 ( 18.34)	Acc@5  53.12 ( 46.35)
Epoch: [1][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2392e+00 (3.3189e+00)	Acc@1  20.31 ( 18.54)	Acc@5  47.66 ( 46.55)
Epoch: [1][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0703e+00 (3.3116e+00)	Acc@1  27.34 ( 18.57)	Acc@5  49.22 ( 46.69)
Epoch: [1][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1767e+00 (3.3015e+00)	Acc@1  20.31 ( 18.70)	Acc@5  49.22 ( 46.94)
Epoch: [1][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0003e+00 (3.2884e+00)	Acc@1  23.44 ( 18.98)	Acc@5  52.34 ( 47.28)
Epoch: [1][220/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0934e+00 (3.2805e+00)	Acc@1  17.97 ( 19.10)	Acc@5  53.12 ( 47.47)
Epoch: [1][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1479e+00 (3.2706e+00)	Acc@1  21.88 ( 19.32)	Acc@5  50.78 ( 47.70)
Epoch: [1][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1716e+00 (3.2650e+00)	Acc@1  18.75 ( 19.44)	Acc@5  47.66 ( 47.89)
Epoch: [1][250/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.3817e+00 (3.2606e+00)	Acc@1  21.09 ( 19.57)	Acc@5  42.19 ( 48.00)
Epoch: [1][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1652e+00 (3.2545e+00)	Acc@1  21.09 ( 19.68)	Acc@5  53.91 ( 48.17)
Epoch: [1][270/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2726e+00 (3.2478e+00)	Acc@1  23.44 ( 19.76)	Acc@5  47.66 ( 48.36)
Epoch: [1][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9312e+00 (3.2394e+00)	Acc@1  25.00 ( 19.92)	Acc@5  60.16 ( 48.63)
Epoch: [1][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0983e+00 (3.2333e+00)	Acc@1  20.31 ( 20.03)	Acc@5  54.69 ( 48.79)
Epoch: [1][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0706e+00 (3.2256e+00)	Acc@1  21.09 ( 20.16)	Acc@5  57.03 ( 49.02)
Epoch: [1][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1354e+00 (3.2186e+00)	Acc@1  25.00 ( 20.27)	Acc@5  51.56 ( 49.21)
Epoch: [1][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9786e+00 (3.2130e+00)	Acc@1  25.78 ( 20.34)	Acc@5  50.00 ( 49.34)
Epoch: [1][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2320e+00 (3.2088e+00)	Acc@1  17.19 ( 20.45)	Acc@5  48.44 ( 49.44)
Epoch: [1][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1597e+00 (3.2034e+00)	Acc@1  18.75 ( 20.57)	Acc@5  58.59 ( 49.64)
Epoch: [1][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1068e+00 (3.1974e+00)	Acc@1  21.88 ( 20.66)	Acc@5  47.66 ( 49.77)
Epoch: [1][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9139e+00 (3.1890e+00)	Acc@1  26.56 ( 20.78)	Acc@5  60.94 ( 50.00)
Epoch: [1][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0440e+00 (3.1839e+00)	Acc@1  25.00 ( 20.89)	Acc@5  59.38 ( 50.15)
Epoch: [1][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0736e+00 (3.1774e+00)	Acc@1  18.75 ( 21.00)	Acc@5  52.34 ( 50.33)
Epoch: [1][390/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9474e+00 (3.1713e+00)	Acc@1  27.50 ( 21.11)	Acc@5  61.25 ( 50.49)
## e[1] optimizer.zero_grad (sum) time: 0.2670154571533203
## e[1]       loss.backward (sum) time: 4.0025880336761475
## e[1]      optimizer.step (sum) time: 1.902634620666504
## epoch[1] training(only) time: 16.088672637939453
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 3.7508e+00 (3.7508e+00)	Acc@1  20.00 ( 20.00)	Acc@5  43.00 ( 43.00)
Test: [ 10/100]	Time  0.017 ( 0.034)	Loss 3.6172e+00 (3.5498e+00)	Acc@1  17.00 ( 17.73)	Acc@5  44.00 ( 47.09)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 3.1896e+00 (3.4689e+00)	Acc@1  19.00 ( 19.19)	Acc@5  53.00 ( 48.52)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 3.7615e+00 (3.4683e+00)	Acc@1  17.00 ( 19.35)	Acc@5  41.00 ( 48.55)
Test: [ 40/100]	Time  0.025 ( 0.024)	Loss 3.7985e+00 (3.4774e+00)	Acc@1  22.00 ( 19.41)	Acc@5  37.00 ( 48.37)
Test: [ 50/100]	Time  0.017 ( 0.023)	Loss 3.3478e+00 (3.4800e+00)	Acc@1  27.00 ( 19.37)	Acc@5  50.00 ( 48.41)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 3.1962e+00 (3.4547e+00)	Acc@1  25.00 ( 19.70)	Acc@5  57.00 ( 49.15)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 3.6105e+00 (3.4569e+00)	Acc@1  16.00 ( 19.59)	Acc@5  46.00 ( 49.17)
Test: [ 80/100]	Time  0.018 ( 0.022)	Loss 3.6948e+00 (3.4644e+00)	Acc@1  16.00 ( 19.67)	Acc@5  41.00 ( 49.02)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 3.4096e+00 (3.4550e+00)	Acc@1  18.00 ( 19.77)	Acc@5  52.00 ( 49.32)
 * Acc@1 19.950 Acc@5 49.320
### epoch[1] execution time: 18.33223056793213
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.194 ( 0.194)	Data  0.144 ( 0.144)	Loss 2.8535e+00 (2.8535e+00)	Acc@1  28.91 ( 28.91)	Acc@5  57.81 ( 57.81)
Epoch: [2][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.014)	Loss 2.9705e+00 (2.9138e+00)	Acc@1  22.66 ( 26.92)	Acc@5  59.38 ( 58.38)
Epoch: [2][ 20/391]	Time  0.040 ( 0.049)	Data  0.002 ( 0.008)	Loss 2.8394e+00 (2.9235e+00)	Acc@1  28.12 ( 26.41)	Acc@5  57.81 ( 57.81)
Epoch: [2][ 30/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.7615e+00 (2.8966e+00)	Acc@1  29.69 ( 26.59)	Acc@5  62.50 ( 58.19)
Epoch: [2][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.9161e+00 (2.8894e+00)	Acc@1  26.56 ( 26.77)	Acc@5  60.16 ( 58.54)
Epoch: [2][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.7148e+00 (2.8623e+00)	Acc@1  32.81 ( 27.31)	Acc@5  59.38 ( 58.85)
Epoch: [2][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.9645e+00 (2.8628e+00)	Acc@1  21.88 ( 27.16)	Acc@5  50.00 ( 58.71)
Epoch: [2][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.8938e+00 (2.8605e+00)	Acc@1  35.16 ( 27.48)	Acc@5  57.03 ( 58.71)
Epoch: [2][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.7088e+00 (2.8579e+00)	Acc@1  32.03 ( 27.45)	Acc@5  60.16 ( 58.75)
Epoch: [2][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.7829e+00 (2.8440e+00)	Acc@1  26.56 ( 27.70)	Acc@5  57.81 ( 59.08)
Epoch: [2][100/391]	Time  0.047 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5630e+00 (2.8403e+00)	Acc@1  32.81 ( 27.71)	Acc@5  66.41 ( 59.14)
Epoch: [2][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8404e+00 (2.8379e+00)	Acc@1  31.25 ( 27.65)	Acc@5  57.81 ( 59.03)
Epoch: [2][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0054e+00 (2.8383e+00)	Acc@1  24.22 ( 27.47)	Acc@5  54.69 ( 59.12)
Epoch: [2][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7319e+00 (2.8355e+00)	Acc@1  31.25 ( 27.53)	Acc@5  57.81 ( 59.14)
Epoch: [2][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7433e+00 (2.8333e+00)	Acc@1  21.09 ( 27.43)	Acc@5  64.06 ( 59.26)
Epoch: [2][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8693e+00 (2.8326e+00)	Acc@1  29.69 ( 27.46)	Acc@5  56.25 ( 59.31)
Epoch: [2][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6890e+00 (2.8263e+00)	Acc@1  29.69 ( 27.54)	Acc@5  62.50 ( 59.50)
Epoch: [2][170/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9385e+00 (2.8196e+00)	Acc@1  23.44 ( 27.65)	Acc@5  58.59 ( 59.68)
Epoch: [2][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6313e+00 (2.8202e+00)	Acc@1  31.25 ( 27.64)	Acc@5  64.84 ( 59.71)
Epoch: [2][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6683e+00 (2.8159e+00)	Acc@1  31.25 ( 27.74)	Acc@5  65.62 ( 59.90)
Epoch: [2][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6518e+00 (2.8105e+00)	Acc@1  33.59 ( 27.85)	Acc@5  63.28 ( 60.02)
Epoch: [2][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7444e+00 (2.8104e+00)	Acc@1  26.56 ( 27.87)	Acc@5  64.84 ( 60.05)
Epoch: [2][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7657e+00 (2.8063e+00)	Acc@1  25.00 ( 27.99)	Acc@5  57.81 ( 60.09)
Epoch: [2][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7679e+00 (2.8028e+00)	Acc@1  25.00 ( 28.00)	Acc@5  60.94 ( 60.19)
Epoch: [2][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7078e+00 (2.7989e+00)	Acc@1  32.03 ( 28.15)	Acc@5  59.38 ( 60.24)
Epoch: [2][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5804e+00 (2.7931e+00)	Acc@1  31.25 ( 28.23)	Acc@5  66.41 ( 60.33)
Epoch: [2][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7061e+00 (2.7880e+00)	Acc@1  26.56 ( 28.28)	Acc@5  63.28 ( 60.49)
Epoch: [2][270/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4835e+00 (2.7819e+00)	Acc@1  34.38 ( 28.32)	Acc@5  62.50 ( 60.61)
Epoch: [2][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7641e+00 (2.7794e+00)	Acc@1  25.00 ( 28.35)	Acc@5  64.06 ( 60.66)
Epoch: [2][290/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6573e+00 (2.7767e+00)	Acc@1  28.12 ( 28.38)	Acc@5  65.62 ( 60.81)
Epoch: [2][300/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7327e+00 (2.7755e+00)	Acc@1  28.91 ( 28.43)	Acc@5  62.50 ( 60.85)
Epoch: [2][310/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.6873e+00 (2.7698e+00)	Acc@1  29.69 ( 28.56)	Acc@5  63.28 ( 60.99)
Epoch: [2][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5586e+00 (2.7655e+00)	Acc@1  31.25 ( 28.66)	Acc@5  67.19 ( 61.09)
Epoch: [2][330/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6906e+00 (2.7627e+00)	Acc@1  29.69 ( 28.69)	Acc@5  64.84 ( 61.14)
Epoch: [2][340/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.001)	Loss 2.8526e+00 (2.7608e+00)	Acc@1  28.91 ( 28.73)	Acc@5  57.81 ( 61.16)
Epoch: [2][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.7609e+00 (2.7569e+00)	Acc@1  31.25 ( 28.81)	Acc@5  62.50 ( 61.23)
Epoch: [2][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.7393e+00 (2.7501e+00)	Acc@1  31.25 ( 28.98)	Acc@5  61.72 ( 61.38)
Epoch: [2][370/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8882e+00 (2.7437e+00)	Acc@1  28.12 ( 29.14)	Acc@5  60.94 ( 61.54)
Epoch: [2][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.7160e+00 (2.7413e+00)	Acc@1  29.69 ( 29.23)	Acc@5  65.62 ( 61.60)
Epoch: [2][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.5704e+00 (2.7386e+00)	Acc@1  22.50 ( 29.28)	Acc@5  72.50 ( 61.69)
## e[2] optimizer.zero_grad (sum) time: 0.26732826232910156
## e[2]       loss.backward (sum) time: 4.039572715759277
## e[2]      optimizer.step (sum) time: 1.8138558864593506
## epoch[2] training(only) time: 16.146801948547363
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 2.7894e+00 (2.7894e+00)	Acc@1  33.00 ( 33.00)	Acc@5  63.00 ( 63.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 2.6134e+00 (2.6436e+00)	Acc@1  25.00 ( 31.73)	Acc@5  67.00 ( 66.00)
Test: [ 20/100]	Time  0.018 ( 0.027)	Loss 2.2948e+00 (2.5906e+00)	Acc@1  43.00 ( 32.57)	Acc@5  75.00 ( 66.19)
Test: [ 30/100]	Time  0.016 ( 0.025)	Loss 2.7090e+00 (2.6008e+00)	Acc@1  32.00 ( 32.45)	Acc@5  61.00 ( 66.06)
Test: [ 40/100]	Time  0.019 ( 0.024)	Loss 2.6521e+00 (2.5970e+00)	Acc@1  33.00 ( 32.49)	Acc@5  59.00 ( 65.83)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 2.5439e+00 (2.6082e+00)	Acc@1  35.00 ( 32.33)	Acc@5  66.00 ( 65.35)
Test: [ 60/100]	Time  0.024 ( 0.023)	Loss 2.4075e+00 (2.5997e+00)	Acc@1  30.00 ( 32.07)	Acc@5  71.00 ( 65.72)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 2.7701e+00 (2.6020e+00)	Acc@1  30.00 ( 31.94)	Acc@5  62.00 ( 65.61)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 2.6506e+00 (2.6079e+00)	Acc@1  26.00 ( 31.84)	Acc@5  66.00 ( 65.52)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 2.4969e+00 (2.5996e+00)	Acc@1  34.00 ( 32.21)	Acc@5  66.00 ( 65.65)
 * Acc@1 32.310 Acc@5 65.600
### epoch[2] execution time: 18.49515676498413
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.197 ( 0.197)	Data  0.147 ( 0.147)	Loss 2.5766e+00 (2.5766e+00)	Acc@1  34.38 ( 34.38)	Acc@5  63.28 ( 63.28)
Epoch: [3][ 10/391]	Time  0.042 ( 0.057)	Data  0.002 ( 0.014)	Loss 2.4466e+00 (2.5116e+00)	Acc@1  39.84 ( 33.52)	Acc@5  69.53 ( 66.26)
Epoch: [3][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 2.5915e+00 (2.4905e+00)	Acc@1  36.72 ( 34.30)	Acc@5  62.50 ( 67.19)
Epoch: [3][ 30/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.006)	Loss 2.5543e+00 (2.5151e+00)	Acc@1  25.78 ( 33.62)	Acc@5  69.53 ( 66.86)
Epoch: [3][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.4267e+00 (2.5062e+00)	Acc@1  33.59 ( 33.82)	Acc@5  71.88 ( 67.13)
Epoch: [3][ 50/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.004)	Loss 2.7347e+00 (2.5130e+00)	Acc@1  27.34 ( 33.55)	Acc@5  61.72 ( 66.99)
Epoch: [3][ 60/391]	Time  0.040 ( 0.044)	Data  0.002 ( 0.003)	Loss 2.7397e+00 (2.5224e+00)	Acc@1  28.12 ( 33.76)	Acc@5  65.62 ( 66.64)
Epoch: [3][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.5853e+00 (2.5135e+00)	Acc@1  39.06 ( 33.98)	Acc@5  69.53 ( 66.95)
Epoch: [3][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.4773e+00 (2.5168e+00)	Acc@1  36.72 ( 33.91)	Acc@5  67.97 ( 66.76)
Epoch: [3][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.6087e+00 (2.5162e+00)	Acc@1  28.91 ( 33.98)	Acc@5  62.50 ( 66.92)
Epoch: [3][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.5590e+00 (2.5247e+00)	Acc@1  28.12 ( 33.67)	Acc@5  67.19 ( 66.78)
Epoch: [3][110/391]	Time  0.045 ( 0.042)	Data  0.002 ( 0.002)	Loss 2.5816e+00 (2.5279e+00)	Acc@1  34.38 ( 33.67)	Acc@5  65.62 ( 66.59)
Epoch: [3][120/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 2.2814e+00 (2.5291e+00)	Acc@1  41.41 ( 33.72)	Acc@5  75.00 ( 66.59)
Epoch: [3][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7449e+00 (2.5255e+00)	Acc@1  31.25 ( 33.67)	Acc@5  61.72 ( 66.62)
Epoch: [3][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3545e+00 (2.5195e+00)	Acc@1  36.72 ( 33.85)	Acc@5  73.44 ( 66.78)
Epoch: [3][150/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5872e+00 (2.5217e+00)	Acc@1  29.69 ( 33.73)	Acc@5  61.72 ( 66.70)
Epoch: [3][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7214e+00 (2.5241e+00)	Acc@1  25.00 ( 33.69)	Acc@5  66.41 ( 66.71)
Epoch: [3][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3254e+00 (2.5190e+00)	Acc@1  41.41 ( 33.87)	Acc@5  71.88 ( 66.82)
Epoch: [3][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4377e+00 (2.5151e+00)	Acc@1  34.38 ( 34.04)	Acc@5  67.19 ( 66.92)
Epoch: [3][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5334e+00 (2.5143e+00)	Acc@1  37.50 ( 34.01)	Acc@5  63.28 ( 66.95)
Epoch: [3][200/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4924e+00 (2.5103e+00)	Acc@1  35.16 ( 34.14)	Acc@5  67.97 ( 67.08)
Epoch: [3][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4866e+00 (2.5086e+00)	Acc@1  32.81 ( 34.14)	Acc@5  70.31 ( 67.18)
Epoch: [3][220/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1609e+00 (2.5023e+00)	Acc@1  36.72 ( 34.31)	Acc@5  78.12 ( 67.40)
Epoch: [3][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2796e+00 (2.4986e+00)	Acc@1  39.84 ( 34.39)	Acc@5  70.31 ( 67.46)
Epoch: [3][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2454e+00 (2.4961e+00)	Acc@1  41.41 ( 34.40)	Acc@5  73.44 ( 67.51)
Epoch: [3][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1909e+00 (2.4930e+00)	Acc@1  43.75 ( 34.50)	Acc@5  75.78 ( 67.54)
Epoch: [3][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7796e+00 (2.4906e+00)	Acc@1  28.91 ( 34.54)	Acc@5  58.59 ( 67.54)
Epoch: [3][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2890e+00 (2.4875e+00)	Acc@1  42.97 ( 34.55)	Acc@5  71.88 ( 67.56)
Epoch: [3][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3587e+00 (2.4863e+00)	Acc@1  35.94 ( 34.56)	Acc@5  72.66 ( 67.60)
Epoch: [3][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5146e+00 (2.4832e+00)	Acc@1  34.38 ( 34.57)	Acc@5  66.41 ( 67.65)
Epoch: [3][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5856e+00 (2.4818e+00)	Acc@1  35.94 ( 34.58)	Acc@5  62.50 ( 67.69)
Epoch: [3][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3380e+00 (2.4784e+00)	Acc@1  45.31 ( 34.64)	Acc@5  67.19 ( 67.73)
Epoch: [3][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.4395e+00 (2.4774e+00)	Acc@1  36.72 ( 34.66)	Acc@5  67.97 ( 67.75)
Epoch: [3][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3777e+00 (2.4773e+00)	Acc@1  38.28 ( 34.64)	Acc@5  70.31 ( 67.72)
Epoch: [3][340/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.3789e+00 (2.4772e+00)	Acc@1  40.62 ( 34.66)	Acc@5  67.97 ( 67.75)
Epoch: [3][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.3421e+00 (2.4745e+00)	Acc@1  38.28 ( 34.74)	Acc@5  66.41 ( 67.80)
Epoch: [3][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2903e+00 (2.4721e+00)	Acc@1  36.72 ( 34.76)	Acc@5  70.31 ( 67.89)
Epoch: [3][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2850e+00 (2.4694e+00)	Acc@1  39.84 ( 34.88)	Acc@5  74.22 ( 67.94)
Epoch: [3][380/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2019e+00 (2.4657e+00)	Acc@1  38.28 ( 34.99)	Acc@5  75.00 ( 68.01)
Epoch: [3][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.5318e+00 (2.4627e+00)	Acc@1  33.75 ( 35.09)	Acc@5  61.25 ( 68.06)
## e[3] optimizer.zero_grad (sum) time: 0.26882171630859375
## e[3]       loss.backward (sum) time: 4.0748937129974365
## e[3]      optimizer.step (sum) time: 1.8074390888214111
## epoch[3] training(only) time: 16.19985318183899
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 2.6128e+00 (2.6128e+00)	Acc@1  37.00 ( 37.00)	Acc@5  68.00 ( 68.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 2.6363e+00 (2.5996e+00)	Acc@1  33.00 ( 35.55)	Acc@5  67.00 ( 66.45)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 2.2612e+00 (2.5615e+00)	Acc@1  42.00 ( 35.62)	Acc@5  75.00 ( 66.71)
Test: [ 30/100]	Time  0.026 ( 0.026)	Loss 2.5485e+00 (2.5563e+00)	Acc@1  36.00 ( 35.55)	Acc@5  70.00 ( 67.03)
Test: [ 40/100]	Time  0.017 ( 0.025)	Loss 2.4826e+00 (2.5393e+00)	Acc@1  37.00 ( 35.56)	Acc@5  68.00 ( 67.24)
Test: [ 50/100]	Time  0.024 ( 0.024)	Loss 2.3337e+00 (2.5413e+00)	Acc@1  40.00 ( 35.45)	Acc@5  69.00 ( 66.82)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 2.4307e+00 (2.5301e+00)	Acc@1  37.00 ( 35.70)	Acc@5  69.00 ( 67.13)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 2.4479e+00 (2.5269e+00)	Acc@1  37.00 ( 35.52)	Acc@5  67.00 ( 67.08)
Test: [ 80/100]	Time  0.019 ( 0.023)	Loss 2.7030e+00 (2.5356e+00)	Acc@1  33.00 ( 35.23)	Acc@5  58.00 ( 66.89)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 2.6117e+00 (2.5324e+00)	Acc@1  36.00 ( 35.31)	Acc@5  67.00 ( 66.85)
 * Acc@1 35.460 Acc@5 66.780
### epoch[3] execution time: 18.564757823944092
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.194 ( 0.194)	Data  0.144 ( 0.144)	Loss 2.2856e+00 (2.2856e+00)	Acc@1  39.84 ( 39.84)	Acc@5  68.75 ( 68.75)
Epoch: [4][ 10/391]	Time  0.044 ( 0.057)	Data  0.001 ( 0.014)	Loss 2.3012e+00 (2.2535e+00)	Acc@1  44.53 ( 40.06)	Acc@5  71.09 ( 72.73)
Epoch: [4][ 20/391]	Time  0.042 ( 0.050)	Data  0.001 ( 0.008)	Loss 2.2283e+00 (2.2713e+00)	Acc@1  40.62 ( 39.32)	Acc@5  75.00 ( 72.95)
Epoch: [4][ 30/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.006)	Loss 2.6355e+00 (2.3156e+00)	Acc@1  28.91 ( 38.10)	Acc@5  67.19 ( 72.08)
Epoch: [4][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.004)	Loss 2.2743e+00 (2.3190e+00)	Acc@1  38.28 ( 38.00)	Acc@5  68.75 ( 71.67)
Epoch: [4][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.3141e+00 (2.3098e+00)	Acc@1  35.94 ( 38.30)	Acc@5  71.09 ( 71.68)
Epoch: [4][ 60/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.2935e+00 (2.2999e+00)	Acc@1  42.19 ( 38.60)	Acc@5  69.53 ( 71.86)
Epoch: [4][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.2585e+00 (2.3017e+00)	Acc@1  39.84 ( 38.47)	Acc@5  71.09 ( 71.83)
Epoch: [4][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.1968e+00 (2.3014e+00)	Acc@1  33.59 ( 38.26)	Acc@5  76.56 ( 71.90)
Epoch: [4][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.2071e+00 (2.2930e+00)	Acc@1  40.62 ( 38.39)	Acc@5  69.53 ( 72.01)
Epoch: [4][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0388e+00 (2.2901e+00)	Acc@1  39.06 ( 38.29)	Acc@5  77.34 ( 72.20)
Epoch: [4][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1652e+00 (2.2950e+00)	Acc@1  41.41 ( 38.13)	Acc@5  81.25 ( 72.24)
Epoch: [4][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4123e+00 (2.2948e+00)	Acc@1  37.50 ( 38.26)	Acc@5  63.28 ( 72.06)
Epoch: [4][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.2566e+00 (2.2910e+00)	Acc@1  35.16 ( 38.39)	Acc@5  70.31 ( 72.11)
Epoch: [4][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1642e+00 (2.2893e+00)	Acc@1  42.97 ( 38.47)	Acc@5  72.66 ( 72.14)
Epoch: [4][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0581e+00 (2.2909e+00)	Acc@1  43.75 ( 38.43)	Acc@5  75.00 ( 72.17)
Epoch: [4][160/391]	Time  0.044 ( 0.042)	Data  0.002 ( 0.002)	Loss 2.3880e+00 (2.2874e+00)	Acc@1  35.94 ( 38.50)	Acc@5  71.09 ( 72.26)
Epoch: [4][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4110e+00 (2.2822e+00)	Acc@1  38.28 ( 38.64)	Acc@5  70.31 ( 72.39)
Epoch: [4][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.2865e+00 (2.2793e+00)	Acc@1  38.28 ( 38.72)	Acc@5  66.41 ( 72.41)
Epoch: [4][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3965e+00 (2.2785e+00)	Acc@1  38.28 ( 38.73)	Acc@5  73.44 ( 72.40)
Epoch: [4][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.2659e+00 (2.2774e+00)	Acc@1  39.84 ( 38.78)	Acc@5  71.88 ( 72.38)
Epoch: [4][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9785e+00 (2.2735e+00)	Acc@1  42.97 ( 38.88)	Acc@5  75.78 ( 72.44)
Epoch: [4][220/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3686e+00 (2.2731e+00)	Acc@1  35.16 ( 38.92)	Acc@5  69.53 ( 72.47)
Epoch: [4][230/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1149e+00 (2.2723e+00)	Acc@1  42.97 ( 38.95)	Acc@5  78.12 ( 72.54)
Epoch: [4][240/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9523e+00 (2.2727e+00)	Acc@1  48.44 ( 38.94)	Acc@5  80.47 ( 72.53)
Epoch: [4][250/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0691e+00 (2.2720e+00)	Acc@1  41.41 ( 38.94)	Acc@5  78.91 ( 72.53)
Epoch: [4][260/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.1382e+00 (2.2694e+00)	Acc@1  46.09 ( 39.09)	Acc@5  74.22 ( 72.61)
Epoch: [4][270/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3327e+00 (2.2709e+00)	Acc@1  43.75 ( 39.06)	Acc@5  67.97 ( 72.59)
Epoch: [4][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1271e+00 (2.2666e+00)	Acc@1  46.88 ( 39.19)	Acc@5  74.22 ( 72.63)
Epoch: [4][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1341e+00 (2.2620e+00)	Acc@1  44.53 ( 39.32)	Acc@5  75.78 ( 72.74)
Epoch: [4][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1615e+00 (2.2608e+00)	Acc@1  41.41 ( 39.35)	Acc@5  73.44 ( 72.77)
Epoch: [4][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.3108e+00 (2.2599e+00)	Acc@1  35.16 ( 39.35)	Acc@5  71.88 ( 72.76)
Epoch: [4][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.2367e+00 (2.2572e+00)	Acc@1  39.84 ( 39.44)	Acc@5  75.00 ( 72.79)
Epoch: [4][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0895e+00 (2.2525e+00)	Acc@1  40.62 ( 39.56)	Acc@5  75.00 ( 72.85)
Epoch: [4][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0718e+00 (2.2493e+00)	Acc@1  42.97 ( 39.64)	Acc@5  74.22 ( 72.88)
Epoch: [4][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0328e+00 (2.2479e+00)	Acc@1  46.09 ( 39.69)	Acc@5  77.34 ( 72.89)
Epoch: [4][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.3270e+00 (2.2473e+00)	Acc@1  46.09 ( 39.74)	Acc@5  74.22 ( 72.90)
Epoch: [4][370/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1967e+00 (2.2465e+00)	Acc@1  45.31 ( 39.78)	Acc@5  75.00 ( 72.95)
Epoch: [4][380/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.001)	Loss 2.1033e+00 (2.2458e+00)	Acc@1  43.75 ( 39.82)	Acc@5  74.22 ( 72.94)
Epoch: [4][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9464e+00 (2.2433e+00)	Acc@1  41.25 ( 39.88)	Acc@5  85.00 ( 72.98)
## e[4] optimizer.zero_grad (sum) time: 0.2648143768310547
## e[4]       loss.backward (sum) time: 4.037164211273193
## e[4]      optimizer.step (sum) time: 1.8111541271209717
## epoch[4] training(only) time: 16.186915397644043
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.6385e+00 (2.6385e+00)	Acc@1  33.00 ( 33.00)	Acc@5  68.00 ( 68.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 2.6521e+00 (2.5068e+00)	Acc@1  29.00 ( 37.00)	Acc@5  66.00 ( 68.64)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 2.3947e+00 (2.4615e+00)	Acc@1  36.00 ( 36.76)	Acc@5  70.00 ( 69.05)
Test: [ 30/100]	Time  0.017 ( 0.026)	Loss 2.6977e+00 (2.4541e+00)	Acc@1  35.00 ( 37.19)	Acc@5  67.00 ( 69.26)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 2.2735e+00 (2.4350e+00)	Acc@1  35.00 ( 37.32)	Acc@5  77.00 ( 69.80)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 2.3685e+00 (2.4522e+00)	Acc@1  38.00 ( 36.47)	Acc@5  68.00 ( 69.24)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 2.3177e+00 (2.4343e+00)	Acc@1  38.00 ( 36.61)	Acc@5  75.00 ( 69.61)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 2.6515e+00 (2.4397e+00)	Acc@1  38.00 ( 36.61)	Acc@5  66.00 ( 69.89)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 2.4531e+00 (2.4451e+00)	Acc@1  35.00 ( 36.51)	Acc@5  71.00 ( 69.90)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 2.4970e+00 (2.4433e+00)	Acc@1  35.00 ( 36.59)	Acc@5  63.00 ( 69.79)
 * Acc@1 36.840 Acc@5 69.880
### epoch[4] execution time: 18.46341109275818
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.201 ( 0.201)	Data  0.145 ( 0.145)	Loss 2.3077e+00 (2.3077e+00)	Acc@1  40.62 ( 40.62)	Acc@5  67.19 ( 67.19)
Epoch: [5][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 2.3245e+00 (2.1495e+00)	Acc@1  41.41 ( 42.05)	Acc@5  64.84 ( 72.80)
Epoch: [5][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 2.2078e+00 (2.1614e+00)	Acc@1  36.72 ( 41.44)	Acc@5  75.78 ( 74.37)
Epoch: [5][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.1865e+00 (2.1707e+00)	Acc@1  40.62 ( 41.28)	Acc@5  73.44 ( 74.09)
Epoch: [5][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.1032e+00 (2.1517e+00)	Acc@1  40.62 ( 41.86)	Acc@5  77.34 ( 74.73)
Epoch: [5][ 50/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.1532e+00 (2.1512e+00)	Acc@1  42.97 ( 42.17)	Acc@5  71.88 ( 74.54)
Epoch: [5][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.3338e+00 (2.1509e+00)	Acc@1  38.28 ( 42.43)	Acc@5  71.88 ( 74.55)
Epoch: [5][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.1969e+00 (2.1479e+00)	Acc@1  41.41 ( 42.52)	Acc@5  76.56 ( 74.57)
Epoch: [5][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.1773e+00 (2.1467e+00)	Acc@1  40.62 ( 42.29)	Acc@5  70.31 ( 74.56)
Epoch: [5][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.3585e+00 (2.1412e+00)	Acc@1  35.16 ( 42.51)	Acc@5  69.53 ( 74.67)
Epoch: [5][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8354e+00 (2.1394e+00)	Acc@1  53.91 ( 42.46)	Acc@5  80.47 ( 74.65)
Epoch: [5][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1262e+00 (2.1314e+00)	Acc@1  46.09 ( 42.76)	Acc@5  74.22 ( 74.86)
Epoch: [5][120/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1417e+00 (2.1294e+00)	Acc@1  36.72 ( 42.79)	Acc@5  81.25 ( 74.98)
Epoch: [5][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0718e+00 (2.1260e+00)	Acc@1  46.88 ( 42.76)	Acc@5  75.00 ( 75.12)
Epoch: [5][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1947e+00 (2.1186e+00)	Acc@1  41.41 ( 42.92)	Acc@5  69.53 ( 75.23)
Epoch: [5][150/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0966e+00 (2.1195e+00)	Acc@1  41.41 ( 42.95)	Acc@5  75.00 ( 75.18)
Epoch: [5][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1081e+00 (2.1193e+00)	Acc@1  44.53 ( 42.95)	Acc@5  73.44 ( 75.17)
Epoch: [5][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1166e+00 (2.1190e+00)	Acc@1  43.75 ( 42.96)	Acc@5  75.78 ( 75.21)
Epoch: [5][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.2979e+00 (2.1144e+00)	Acc@1  35.16 ( 43.07)	Acc@5  72.66 ( 75.31)
Epoch: [5][190/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0584e+00 (2.1166e+00)	Acc@1  44.53 ( 42.95)	Acc@5  80.47 ( 75.26)
Epoch: [5][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1673e+00 (2.1141e+00)	Acc@1  42.19 ( 43.01)	Acc@5  73.44 ( 75.32)
Epoch: [5][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0120e+00 (2.1162e+00)	Acc@1  45.31 ( 42.95)	Acc@5  73.44 ( 75.29)
Epoch: [5][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8412e+00 (2.1126e+00)	Acc@1  50.00 ( 43.03)	Acc@5  78.91 ( 75.36)
Epoch: [5][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2409e+00 (2.1099e+00)	Acc@1  43.75 ( 43.09)	Acc@5  72.66 ( 75.44)
Epoch: [5][240/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3734e+00 (2.1119e+00)	Acc@1  36.72 ( 43.04)	Acc@5  65.62 ( 75.33)
Epoch: [5][250/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.0382e+00 (2.1062e+00)	Acc@1  39.84 ( 43.17)	Acc@5  77.34 ( 75.43)
Epoch: [5][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7780e+00 (2.1063e+00)	Acc@1  46.88 ( 43.18)	Acc@5  85.94 ( 75.46)
Epoch: [5][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9492e+00 (2.1032e+00)	Acc@1  46.88 ( 43.20)	Acc@5  82.03 ( 75.54)
Epoch: [5][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8251e+00 (2.1029e+00)	Acc@1  49.22 ( 43.26)	Acc@5  82.03 ( 75.53)
Epoch: [5][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2979e+00 (2.1017e+00)	Acc@1  42.19 ( 43.26)	Acc@5  69.53 ( 75.59)
Epoch: [5][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9648e+00 (2.0989e+00)	Acc@1  45.31 ( 43.27)	Acc@5  78.12 ( 75.65)
Epoch: [5][310/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0770e+00 (2.0985e+00)	Acc@1  44.53 ( 43.32)	Acc@5  78.91 ( 75.65)
Epoch: [5][320/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.1747e+00 (2.0969e+00)	Acc@1  46.09 ( 43.33)	Acc@5  73.44 ( 75.69)
Epoch: [5][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0659e+00 (2.0943e+00)	Acc@1  46.09 ( 43.43)	Acc@5  75.78 ( 75.74)
Epoch: [5][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0585e+00 (2.0936e+00)	Acc@1  42.19 ( 43.50)	Acc@5  75.00 ( 75.76)
Epoch: [5][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0062e+00 (2.0897e+00)	Acc@1  45.31 ( 43.60)	Acc@5  77.34 ( 75.86)
Epoch: [5][360/391]	Time  0.047 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1998e+00 (2.0886e+00)	Acc@1  42.97 ( 43.61)	Acc@5  71.88 ( 75.90)
Epoch: [5][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1774e+00 (2.0886e+00)	Acc@1  42.19 ( 43.64)	Acc@5  76.56 ( 75.88)
Epoch: [5][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1283e+00 (2.0882e+00)	Acc@1  44.53 ( 43.62)	Acc@5  74.22 ( 75.88)
Epoch: [5][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9594e+00 (2.0881e+00)	Acc@1  50.00 ( 43.60)	Acc@5  76.25 ( 75.88)
## e[5] optimizer.zero_grad (sum) time: 0.2687056064605713
## e[5]       loss.backward (sum) time: 4.057373046875
## e[5]      optimizer.step (sum) time: 1.8467552661895752
## epoch[5] training(only) time: 16.149402856826782
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 2.2496e+00 (2.2496e+00)	Acc@1  41.00 ( 41.00)	Acc@5  74.00 ( 74.00)
Test: [ 10/100]	Time  0.019 ( 0.033)	Loss 2.6206e+00 (2.3225e+00)	Acc@1  28.00 ( 40.55)	Acc@5  74.00 ( 72.82)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 2.0406e+00 (2.2765e+00)	Acc@1  46.00 ( 41.24)	Acc@5  78.00 ( 72.90)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 2.4786e+00 (2.2856e+00)	Acc@1  37.00 ( 40.87)	Acc@5  68.00 ( 72.65)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 2.2581e+00 (2.2781e+00)	Acc@1  43.00 ( 40.80)	Acc@5  74.00 ( 72.63)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 2.0379e+00 (2.2949e+00)	Acc@1  46.00 ( 40.63)	Acc@5  75.00 ( 71.92)
Test: [ 60/100]	Time  0.024 ( 0.024)	Loss 2.2614e+00 (2.2792e+00)	Acc@1  45.00 ( 40.85)	Acc@5  71.00 ( 72.11)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 2.4822e+00 (2.2765e+00)	Acc@1  35.00 ( 41.06)	Acc@5  70.00 ( 72.08)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 2.2416e+00 (2.2858e+00)	Acc@1  39.00 ( 40.83)	Acc@5  73.00 ( 71.93)
Test: [ 90/100]	Time  0.020 ( 0.023)	Loss 2.3670e+00 (2.2866e+00)	Acc@1  35.00 ( 40.65)	Acc@5  73.00 ( 72.05)
 * Acc@1 40.810 Acc@5 72.210
### epoch[5] execution time: 18.523812294006348
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.196 ( 0.196)	Data  0.142 ( 0.142)	Loss 1.8633e+00 (1.8633e+00)	Acc@1  47.66 ( 47.66)	Acc@5  81.25 ( 81.25)
Epoch: [6][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.7395e+00 (2.0048e+00)	Acc@1  48.44 ( 46.73)	Acc@5  85.94 ( 77.84)
Epoch: [6][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.008)	Loss 2.0301e+00 (2.0098e+00)	Acc@1  41.41 ( 45.83)	Acc@5  76.56 ( 77.46)
Epoch: [6][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.9749e+00 (1.9756e+00)	Acc@1  46.88 ( 46.40)	Acc@5  78.91 ( 78.25)
Epoch: [6][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 2.0506e+00 (1.9760e+00)	Acc@1  41.41 ( 46.28)	Acc@5  76.56 ( 78.11)
Epoch: [6][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.0304e+00 (1.9734e+00)	Acc@1  49.22 ( 46.32)	Acc@5  77.34 ( 78.17)
Epoch: [6][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.6706e+00 (1.9587e+00)	Acc@1  51.56 ( 46.57)	Acc@5  85.16 ( 78.65)
Epoch: [6][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.1064e+00 (1.9620e+00)	Acc@1  43.75 ( 46.63)	Acc@5  73.44 ( 78.51)
Epoch: [6][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.9122e+00 (1.9705e+00)	Acc@1  46.09 ( 46.40)	Acc@5  81.25 ( 78.37)
Epoch: [6][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.7301e+00 (1.9690e+00)	Acc@1  51.56 ( 46.42)	Acc@5  83.59 ( 78.45)
Epoch: [6][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8758e+00 (1.9752e+00)	Acc@1  45.31 ( 46.27)	Acc@5  82.03 ( 78.30)
Epoch: [6][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9236e+00 (1.9759e+00)	Acc@1  50.00 ( 46.30)	Acc@5  76.56 ( 78.15)
Epoch: [6][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8650e+00 (1.9789e+00)	Acc@1  45.31 ( 46.20)	Acc@5  77.34 ( 78.09)
Epoch: [6][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7711e+00 (1.9793e+00)	Acc@1  51.56 ( 46.21)	Acc@5  82.03 ( 78.15)
Epoch: [6][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0472e+00 (1.9793e+00)	Acc@1  42.97 ( 46.04)	Acc@5  78.12 ( 78.11)
Epoch: [6][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8493e+00 (1.9792e+00)	Acc@1  50.78 ( 46.08)	Acc@5  82.81 ( 78.08)
Epoch: [6][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8335e+00 (1.9726e+00)	Acc@1  51.56 ( 46.25)	Acc@5  80.47 ( 78.22)
Epoch: [6][170/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0845e+00 (1.9708e+00)	Acc@1  49.22 ( 46.30)	Acc@5  70.31 ( 78.24)
Epoch: [6][180/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0872e+00 (1.9684e+00)	Acc@1  43.75 ( 46.33)	Acc@5  75.78 ( 78.32)
Epoch: [6][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1031e+00 (1.9722e+00)	Acc@1  46.88 ( 46.27)	Acc@5  75.78 ( 78.24)
Epoch: [6][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0350e+00 (1.9733e+00)	Acc@1  48.44 ( 46.24)	Acc@5  76.56 ( 78.21)
Epoch: [6][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9343e+00 (1.9731e+00)	Acc@1  46.09 ( 46.29)	Acc@5  83.59 ( 78.25)
Epoch: [6][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1871e+00 (1.9737e+00)	Acc@1  46.09 ( 46.39)	Acc@5  72.66 ( 78.13)
Epoch: [6][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9416e+00 (1.9763e+00)	Acc@1  45.31 ( 46.35)	Acc@5  80.47 ( 78.08)
Epoch: [6][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8676e+00 (1.9749e+00)	Acc@1  46.09 ( 46.34)	Acc@5  83.59 ( 78.06)
Epoch: [6][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.2314e+00 (1.9763e+00)	Acc@1  40.62 ( 46.34)	Acc@5  71.09 ( 77.98)
Epoch: [6][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0924e+00 (1.9765e+00)	Acc@1  45.31 ( 46.37)	Acc@5  75.00 ( 77.94)
Epoch: [6][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8760e+00 (1.9755e+00)	Acc@1  52.34 ( 46.42)	Acc@5  79.69 ( 77.98)
Epoch: [6][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0129e+00 (1.9765e+00)	Acc@1  40.62 ( 46.30)	Acc@5  76.56 ( 77.98)
Epoch: [6][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.1070e+00 (1.9774e+00)	Acc@1  39.06 ( 46.28)	Acc@5  73.44 ( 78.01)
Epoch: [6][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8766e+00 (1.9783e+00)	Acc@1  46.88 ( 46.26)	Acc@5  85.16 ( 78.04)
Epoch: [6][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0403e+00 (1.9782e+00)	Acc@1  46.09 ( 46.24)	Acc@5  78.12 ( 78.04)
Epoch: [6][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7599e+00 (1.9768e+00)	Acc@1  48.44 ( 46.22)	Acc@5  85.16 ( 78.12)
Epoch: [6][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9548e+00 (1.9755e+00)	Acc@1  42.97 ( 46.26)	Acc@5  77.34 ( 78.14)
Epoch: [6][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9930e+00 (1.9734e+00)	Acc@1  39.06 ( 46.28)	Acc@5  81.25 ( 78.22)
Epoch: [6][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9057e+00 (1.9737e+00)	Acc@1  49.22 ( 46.28)	Acc@5  81.25 ( 78.22)
Epoch: [6][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.1290e+00 (1.9727e+00)	Acc@1  45.31 ( 46.32)	Acc@5  74.22 ( 78.24)
Epoch: [6][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0692e+00 (1.9717e+00)	Acc@1  46.88 ( 46.36)	Acc@5  73.44 ( 78.24)
Epoch: [6][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8465e+00 (1.9704e+00)	Acc@1  46.88 ( 46.40)	Acc@5  80.47 ( 78.28)
Epoch: [6][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8490e+00 (1.9693e+00)	Acc@1  52.50 ( 46.40)	Acc@5  76.25 ( 78.29)
## e[6] optimizer.zero_grad (sum) time: 0.2661097049713135
## e[6]       loss.backward (sum) time: 4.045862197875977
## e[6]      optimizer.step (sum) time: 1.8520514965057373
## epoch[6] training(only) time: 16.11915135383606
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.9902e+00 (1.9902e+00)	Acc@1  55.00 ( 55.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 2.3882e+00 (2.1384e+00)	Acc@1  35.00 ( 45.45)	Acc@5  77.00 ( 76.00)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.8835e+00 (2.1213e+00)	Acc@1  45.00 ( 45.81)	Acc@5  83.00 ( 76.00)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.1579e+00 (2.1253e+00)	Acc@1  40.00 ( 45.23)	Acc@5  78.00 ( 75.65)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.8884e+00 (2.1127e+00)	Acc@1  53.00 ( 45.66)	Acc@5  83.00 ( 76.24)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.7957e+00 (2.1066e+00)	Acc@1  52.00 ( 45.53)	Acc@5  79.00 ( 76.08)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 2.1774e+00 (2.1001e+00)	Acc@1  48.00 ( 45.49)	Acc@5  75.00 ( 76.41)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 2.0954e+00 (2.1008e+00)	Acc@1  45.00 ( 45.24)	Acc@5  78.00 ( 76.44)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 2.4677e+00 (2.1125e+00)	Acc@1  38.00 ( 44.95)	Acc@5  68.00 ( 76.21)
Test: [ 90/100]	Time  0.019 ( 0.023)	Loss 2.0973e+00 (2.1082e+00)	Acc@1  45.00 ( 45.09)	Acc@5  76.00 ( 76.30)
 * Acc@1 45.050 Acc@5 76.430
### epoch[6] execution time: 18.517523527145386
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.199 ( 0.199)	Data  0.148 ( 0.148)	Loss 1.8051e+00 (1.8051e+00)	Acc@1  47.66 ( 47.66)	Acc@5  79.69 ( 79.69)
Epoch: [7][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.8422e+00 (1.8390e+00)	Acc@1  42.19 ( 47.94)	Acc@5  82.03 ( 80.97)
Epoch: [7][ 20/391]	Time  0.041 ( 0.049)	Data  0.002 ( 0.008)	Loss 1.9207e+00 (1.8399e+00)	Acc@1  48.44 ( 48.88)	Acc@5  78.91 ( 80.95)
Epoch: [7][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.9734e+00 (1.8416e+00)	Acc@1  42.19 ( 48.77)	Acc@5  78.91 ( 81.43)
Epoch: [7][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.7526e+00 (1.8298e+00)	Acc@1  53.91 ( 49.01)	Acc@5  81.25 ( 81.48)
Epoch: [7][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.6200e+00 (1.8289e+00)	Acc@1  50.00 ( 48.90)	Acc@5  84.38 ( 81.27)
Epoch: [7][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.7253e+00 (1.8229e+00)	Acc@1  51.56 ( 49.09)	Acc@5  81.25 ( 81.60)
Epoch: [7][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8495e+00 (1.8248e+00)	Acc@1  46.88 ( 49.00)	Acc@5  79.69 ( 81.42)
Epoch: [7][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8988e+00 (1.8254e+00)	Acc@1  52.34 ( 48.99)	Acc@5  76.56 ( 81.39)
Epoch: [7][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.0999e+00 (1.8297e+00)	Acc@1  36.72 ( 48.92)	Acc@5  81.25 ( 81.35)
Epoch: [7][100/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8018e+00 (1.8362e+00)	Acc@1  47.66 ( 48.84)	Acc@5  83.59 ( 81.23)
Epoch: [7][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9055e+00 (1.8385e+00)	Acc@1  46.88 ( 48.77)	Acc@5  80.47 ( 81.19)
Epoch: [7][120/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.8214e+00 (1.8372e+00)	Acc@1  46.88 ( 48.72)	Acc@5  80.47 ( 81.22)
Epoch: [7][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8048e+00 (1.8406e+00)	Acc@1  50.00 ( 48.61)	Acc@5  80.47 ( 81.17)
Epoch: [7][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8686e+00 (1.8470e+00)	Acc@1  49.22 ( 48.52)	Acc@5  79.69 ( 80.93)
Epoch: [7][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5514e+00 (1.8507e+00)	Acc@1  58.59 ( 48.43)	Acc@5  83.59 ( 80.88)
Epoch: [7][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8191e+00 (1.8524e+00)	Acc@1  51.56 ( 48.41)	Acc@5  79.69 ( 80.80)
Epoch: [7][170/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7840e+00 (1.8537e+00)	Acc@1  53.91 ( 48.45)	Acc@5  81.25 ( 80.72)
Epoch: [7][180/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6971e+00 (1.8519e+00)	Acc@1  50.78 ( 48.44)	Acc@5  84.38 ( 80.76)
Epoch: [7][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1888e+00 (1.8518e+00)	Acc@1  39.84 ( 48.49)	Acc@5  73.44 ( 80.70)
Epoch: [7][200/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9774e+00 (1.8535e+00)	Acc@1  49.22 ( 48.48)	Acc@5  75.00 ( 80.62)
Epoch: [7][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0773e+00 (1.8514e+00)	Acc@1  37.50 ( 48.56)	Acc@5  78.12 ( 80.65)
Epoch: [7][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7188e+00 (1.8528e+00)	Acc@1  55.47 ( 48.53)	Acc@5  83.59 ( 80.63)
Epoch: [7][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6774e+00 (1.8505e+00)	Acc@1  53.91 ( 48.60)	Acc@5  83.59 ( 80.66)
Epoch: [7][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.1520e+00 (1.8498e+00)	Acc@1  39.84 ( 48.64)	Acc@5  72.66 ( 80.70)
Epoch: [7][250/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9881e+00 (1.8519e+00)	Acc@1  40.62 ( 48.66)	Acc@5  78.12 ( 80.62)
Epoch: [7][260/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7044e+00 (1.8512e+00)	Acc@1  56.25 ( 48.71)	Acc@5  80.47 ( 80.63)
Epoch: [7][270/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8394e+00 (1.8494e+00)	Acc@1  47.66 ( 48.82)	Acc@5  81.25 ( 80.63)
Epoch: [7][280/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8928e+00 (1.8484e+00)	Acc@1  46.09 ( 48.88)	Acc@5  82.81 ( 80.64)
Epoch: [7][290/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.9471e+00 (1.8466e+00)	Acc@1  50.00 ( 48.92)	Acc@5  78.91 ( 80.68)
Epoch: [7][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8059e+00 (1.8466e+00)	Acc@1  46.09 ( 48.91)	Acc@5  80.47 ( 80.67)
Epoch: [7][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6029e+00 (1.8475e+00)	Acc@1  54.69 ( 48.88)	Acc@5  83.59 ( 80.63)
Epoch: [7][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8054e+00 (1.8474e+00)	Acc@1  51.56 ( 48.87)	Acc@5  79.69 ( 80.63)
Epoch: [7][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.0203e+00 (1.8481e+00)	Acc@1  46.88 ( 48.85)	Acc@5  75.78 ( 80.63)
Epoch: [7][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8762e+00 (1.8474e+00)	Acc@1  42.97 ( 48.90)	Acc@5  82.81 ( 80.62)
Epoch: [7][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8981e+00 (1.8485e+00)	Acc@1  52.34 ( 48.93)	Acc@5  80.47 ( 80.58)
Epoch: [7][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8131e+00 (1.8484e+00)	Acc@1  45.31 ( 48.90)	Acc@5  79.69 ( 80.57)
Epoch: [7][370/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8458e+00 (1.8474e+00)	Acc@1  48.44 ( 48.90)	Acc@5  82.03 ( 80.61)
Epoch: [7][380/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7448e+00 (1.8472e+00)	Acc@1  46.88 ( 48.92)	Acc@5  85.16 ( 80.59)
Epoch: [7][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7430e+00 (1.8463e+00)	Acc@1  56.25 ( 48.97)	Acc@5  85.00 ( 80.65)
## e[7] optimizer.zero_grad (sum) time: 0.26569128036499023
## e[7]       loss.backward (sum) time: 4.076918601989746
## e[7]      optimizer.step (sum) time: 1.8216118812561035
## epoch[7] training(only) time: 16.198644399642944
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 2.1025e+00 (2.1025e+00)	Acc@1  46.00 ( 46.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 2.1570e+00 (2.1009e+00)	Acc@1  37.00 ( 43.27)	Acc@5  80.00 ( 78.18)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 1.6596e+00 (2.0226e+00)	Acc@1  54.00 ( 45.14)	Acc@5  84.00 ( 78.71)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 2.0858e+00 (2.0547e+00)	Acc@1  40.00 ( 44.58)	Acc@5  74.00 ( 78.13)
Test: [ 40/100]	Time  0.017 ( 0.026)	Loss 2.0256e+00 (2.0431e+00)	Acc@1  42.00 ( 44.80)	Acc@5  76.00 ( 78.24)
Test: [ 50/100]	Time  0.020 ( 0.025)	Loss 1.8131e+00 (2.0621e+00)	Acc@1  54.00 ( 44.59)	Acc@5  81.00 ( 77.80)
Test: [ 60/100]	Time  0.018 ( 0.024)	Loss 1.8786e+00 (2.0546e+00)	Acc@1  50.00 ( 44.80)	Acc@5  83.00 ( 77.85)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 2.1444e+00 (2.0607e+00)	Acc@1  42.00 ( 44.56)	Acc@5  74.00 ( 77.69)
Test: [ 80/100]	Time  0.020 ( 0.023)	Loss 2.3531e+00 (2.0702e+00)	Acc@1  46.00 ( 44.42)	Acc@5  72.00 ( 77.44)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 2.0729e+00 (2.0649e+00)	Acc@1  48.00 ( 44.64)	Acc@5  76.00 ( 77.63)
 * Acc@1 44.780 Acc@5 77.910
### epoch[7] execution time: 18.52130436897278
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.186 ( 0.186)	Data  0.139 ( 0.139)	Loss 1.6241e+00 (1.6241e+00)	Acc@1  56.25 ( 56.25)	Acc@5  85.94 ( 85.94)
Epoch: [8][ 10/391]	Time  0.040 ( 0.055)	Data  0.001 ( 0.014)	Loss 1.6949e+00 (1.7868e+00)	Acc@1  53.91 ( 51.85)	Acc@5  83.59 ( 80.61)
Epoch: [8][ 20/391]	Time  0.040 ( 0.049)	Data  0.002 ( 0.008)	Loss 1.7906e+00 (1.7717e+00)	Acc@1  44.53 ( 51.38)	Acc@5  85.94 ( 81.51)
Epoch: [8][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.005)	Loss 1.9009e+00 (1.7748e+00)	Acc@1  46.88 ( 51.11)	Acc@5  79.69 ( 81.60)
Epoch: [8][ 40/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.004)	Loss 1.9315e+00 (1.7834e+00)	Acc@1  50.00 ( 50.72)	Acc@5  78.12 ( 81.46)
Epoch: [8][ 50/391]	Time  0.040 ( 0.044)	Data  0.002 ( 0.004)	Loss 1.5831e+00 (1.7901e+00)	Acc@1  56.25 ( 50.31)	Acc@5  85.94 ( 81.56)
Epoch: [8][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.9703e+00 (1.8011e+00)	Acc@1  46.09 ( 50.26)	Acc@5  78.91 ( 81.28)
Epoch: [8][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8354e+00 (1.7973e+00)	Acc@1  50.00 ( 50.32)	Acc@5  78.91 ( 81.33)
Epoch: [8][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4901e+00 (1.7877e+00)	Acc@1  57.81 ( 50.47)	Acc@5  86.72 ( 81.51)
Epoch: [8][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7549e+00 (1.7770e+00)	Acc@1  50.78 ( 50.65)	Acc@5  80.47 ( 81.64)
Epoch: [8][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8173e+00 (1.7705e+00)	Acc@1  52.34 ( 50.86)	Acc@5  80.47 ( 81.73)
Epoch: [8][110/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.6713e+00 (1.7734e+00)	Acc@1  53.12 ( 50.84)	Acc@5  83.59 ( 81.66)
Epoch: [8][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6098e+00 (1.7735e+00)	Acc@1  60.16 ( 50.88)	Acc@5  83.59 ( 81.54)
Epoch: [8][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6241e+00 (1.7792e+00)	Acc@1  50.00 ( 50.76)	Acc@5  79.69 ( 81.34)
Epoch: [8][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7338e+00 (1.7800e+00)	Acc@1  53.91 ( 50.75)	Acc@5  82.03 ( 81.35)
Epoch: [8][150/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.9765e+00 (1.7791e+00)	Acc@1  46.09 ( 50.71)	Acc@5  78.12 ( 81.53)
Epoch: [8][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8993e+00 (1.7790e+00)	Acc@1  51.56 ( 50.81)	Acc@5  75.78 ( 81.55)
Epoch: [8][170/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.7608e+00 (1.7773e+00)	Acc@1  52.34 ( 50.87)	Acc@5  81.25 ( 81.58)
Epoch: [8][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7449e+00 (1.7773e+00)	Acc@1  53.12 ( 50.81)	Acc@5  84.38 ( 81.55)
Epoch: [8][190/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4388e+00 (1.7742e+00)	Acc@1  59.38 ( 50.81)	Acc@5  86.72 ( 81.66)
Epoch: [8][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6733e+00 (1.7717e+00)	Acc@1  53.12 ( 50.84)	Acc@5  82.81 ( 81.69)
Epoch: [8][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6042e+00 (1.7708e+00)	Acc@1  52.34 ( 50.83)	Acc@5  82.81 ( 81.68)
Epoch: [8][220/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6563e+00 (1.7665e+00)	Acc@1  54.69 ( 50.95)	Acc@5  84.38 ( 81.73)
Epoch: [8][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7943e+00 (1.7677e+00)	Acc@1  48.44 ( 50.91)	Acc@5  84.38 ( 81.76)
Epoch: [8][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8514e+00 (1.7678e+00)	Acc@1  44.53 ( 50.86)	Acc@5  77.34 ( 81.78)
Epoch: [8][250/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7936e+00 (1.7692e+00)	Acc@1  52.34 ( 50.83)	Acc@5  77.34 ( 81.74)
Epoch: [8][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9505e+00 (1.7717e+00)	Acc@1  51.56 ( 50.78)	Acc@5  79.69 ( 81.71)
Epoch: [8][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5006e+00 (1.7712e+00)	Acc@1  57.03 ( 50.78)	Acc@5  85.94 ( 81.70)
Epoch: [8][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8285e+00 (1.7709e+00)	Acc@1  50.78 ( 50.83)	Acc@5  80.47 ( 81.74)
Epoch: [8][290/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.0435e+00 (1.7712e+00)	Acc@1  46.88 ( 50.81)	Acc@5  75.00 ( 81.75)
Epoch: [8][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6620e+00 (1.7687e+00)	Acc@1  55.47 ( 50.89)	Acc@5  82.03 ( 81.76)
Epoch: [8][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8023e+00 (1.7688e+00)	Acc@1  56.25 ( 50.89)	Acc@5  79.69 ( 81.77)
Epoch: [8][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.9686e+00 (1.7688e+00)	Acc@1  42.97 ( 50.88)	Acc@5  79.69 ( 81.78)
Epoch: [8][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5684e+00 (1.7688e+00)	Acc@1  58.59 ( 50.91)	Acc@5  86.72 ( 81.80)
Epoch: [8][340/391]	Time  0.047 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5654e+00 (1.7673e+00)	Acc@1  54.69 ( 50.95)	Acc@5  85.16 ( 81.85)
Epoch: [8][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0118e+00 (1.7683e+00)	Acc@1  42.19 ( 50.93)	Acc@5  78.91 ( 81.83)
Epoch: [8][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9150e+00 (1.7680e+00)	Acc@1  52.34 ( 50.92)	Acc@5  76.56 ( 81.86)
Epoch: [8][370/391]	Time  0.049 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9271e+00 (1.7670e+00)	Acc@1  46.09 ( 50.89)	Acc@5  81.25 ( 81.91)
Epoch: [8][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5494e+00 (1.7659e+00)	Acc@1  60.16 ( 50.89)	Acc@5  86.72 ( 81.96)
Epoch: [8][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8359e+00 (1.7667e+00)	Acc@1  46.25 ( 50.86)	Acc@5  80.00 ( 81.92)
## e[8] optimizer.zero_grad (sum) time: 0.2642045021057129
## e[8]       loss.backward (sum) time: 3.987468957901001
## e[8]      optimizer.step (sum) time: 1.8851227760314941
## epoch[8] training(only) time: 16.020013093948364
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.8983e+00 (1.8983e+00)	Acc@1  46.00 ( 46.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.019 ( 0.035)	Loss 2.4335e+00 (2.1293e+00)	Acc@1  41.00 ( 43.64)	Acc@5  76.00 ( 78.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.9450e+00 (2.0789e+00)	Acc@1  46.00 ( 44.76)	Acc@5  79.00 ( 78.24)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.1007e+00 (2.0807e+00)	Acc@1  40.00 ( 45.48)	Acc@5  78.00 ( 78.00)
Test: [ 40/100]	Time  0.023 ( 0.025)	Loss 1.8758e+00 (2.0698e+00)	Acc@1  48.00 ( 46.02)	Acc@5  80.00 ( 77.93)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 1.7374e+00 (2.0620e+00)	Acc@1  54.00 ( 46.02)	Acc@5  79.00 ( 77.53)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.8267e+00 (2.0341e+00)	Acc@1  49.00 ( 46.41)	Acc@5  85.00 ( 77.97)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 2.2953e+00 (2.0399e+00)	Acc@1  48.00 ( 46.27)	Acc@5  71.00 ( 77.76)
Test: [ 80/100]	Time  0.019 ( 0.023)	Loss 2.1531e+00 (2.0470e+00)	Acc@1  41.00 ( 46.26)	Acc@5  78.00 ( 77.59)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 2.2175e+00 (2.0470e+00)	Acc@1  38.00 ( 46.16)	Acc@5  73.00 ( 77.67)
 * Acc@1 46.240 Acc@5 77.620
### epoch[8] execution time: 18.35049057006836
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.203 ( 0.203)	Data  0.149 ( 0.149)	Loss 1.6485e+00 (1.6485e+00)	Acc@1  50.00 ( 50.00)	Acc@5  86.72 ( 86.72)
Epoch: [9][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.7681e+00 (1.6981e+00)	Acc@1  54.69 ( 51.99)	Acc@5  85.16 ( 84.66)
Epoch: [9][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.8480e+00 (1.6673e+00)	Acc@1  49.22 ( 53.57)	Acc@5  79.69 ( 84.41)
Epoch: [9][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.5934e+00 (1.6658e+00)	Acc@1  55.47 ( 53.86)	Acc@5  86.72 ( 84.17)
Epoch: [9][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.7830e+00 (1.6619e+00)	Acc@1  53.12 ( 53.83)	Acc@5  82.81 ( 84.38)
Epoch: [9][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.6468e+00 (1.6660e+00)	Acc@1  52.34 ( 53.83)	Acc@5  85.16 ( 84.10)
Epoch: [9][ 60/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.4411e+00 (1.6619e+00)	Acc@1  60.94 ( 53.89)	Acc@5  84.38 ( 84.08)
Epoch: [9][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5434e+00 (1.6707e+00)	Acc@1  53.91 ( 53.70)	Acc@5  88.28 ( 83.77)
Epoch: [9][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8003e+00 (1.6776e+00)	Acc@1  48.44 ( 53.57)	Acc@5  83.59 ( 83.65)
Epoch: [9][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.7686e+00 (1.6730e+00)	Acc@1  49.22 ( 53.49)	Acc@5  81.25 ( 83.84)
Epoch: [9][100/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.6163e+00 (1.6727e+00)	Acc@1  52.34 ( 53.30)	Acc@5  86.72 ( 83.88)
Epoch: [9][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5307e+00 (1.6735e+00)	Acc@1  57.03 ( 53.29)	Acc@5  85.16 ( 83.87)
Epoch: [9][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8028e+00 (1.6757e+00)	Acc@1  49.22 ( 53.19)	Acc@5  82.03 ( 83.70)
Epoch: [9][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4666e+00 (1.6722e+00)	Acc@1  57.81 ( 53.26)	Acc@5  87.50 ( 83.82)
Epoch: [9][140/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7177e+00 (1.6733e+00)	Acc@1  49.22 ( 53.21)	Acc@5  82.03 ( 83.75)
Epoch: [9][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6155e+00 (1.6746e+00)	Acc@1  50.00 ( 53.17)	Acc@5  85.16 ( 83.72)
Epoch: [9][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8730e+00 (1.6781e+00)	Acc@1  49.22 ( 53.15)	Acc@5  76.56 ( 83.61)
Epoch: [9][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5530e+00 (1.6806e+00)	Acc@1  57.03 ( 53.15)	Acc@5  84.38 ( 83.56)
Epoch: [9][180/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6777e+00 (1.6793e+00)	Acc@1  57.81 ( 53.16)	Acc@5  78.91 ( 83.51)
Epoch: [9][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5845e+00 (1.6804e+00)	Acc@1  55.47 ( 53.08)	Acc@5  85.94 ( 83.54)
Epoch: [9][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5601e+00 (1.6807e+00)	Acc@1  56.25 ( 53.09)	Acc@5  87.50 ( 83.54)
Epoch: [9][210/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5931e+00 (1.6781e+00)	Acc@1  53.12 ( 53.13)	Acc@5  85.16 ( 83.55)
Epoch: [9][220/391]	Time  0.048 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7386e+00 (1.6823e+00)	Acc@1  48.44 ( 53.01)	Acc@5  82.03 ( 83.46)
Epoch: [9][230/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6056e+00 (1.6854e+00)	Acc@1  55.47 ( 52.96)	Acc@5  83.59 ( 83.41)
Epoch: [9][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5789e+00 (1.6848e+00)	Acc@1  57.03 ( 53.03)	Acc@5  82.81 ( 83.48)
Epoch: [9][250/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6544e+00 (1.6851e+00)	Acc@1  53.91 ( 53.05)	Acc@5  83.59 ( 83.44)
Epoch: [9][260/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5527e+00 (1.6867e+00)	Acc@1  57.81 ( 52.98)	Acc@5  82.81 ( 83.41)
Epoch: [9][270/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5687e+00 (1.6849e+00)	Acc@1  54.69 ( 52.97)	Acc@5  85.16 ( 83.41)
Epoch: [9][280/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5544e+00 (1.6857e+00)	Acc@1  58.59 ( 52.98)	Acc@5  82.03 ( 83.37)
Epoch: [9][290/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5846e+00 (1.6852e+00)	Acc@1  52.34 ( 53.03)	Acc@5  82.81 ( 83.37)
Epoch: [9][300/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6268e+00 (1.6854e+00)	Acc@1  53.12 ( 52.99)	Acc@5  85.94 ( 83.38)
Epoch: [9][310/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4075e+00 (1.6829e+00)	Acc@1  62.50 ( 53.07)	Acc@5  89.06 ( 83.46)
Epoch: [9][320/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 1.7684e+00 (1.6858e+00)	Acc@1  46.88 ( 52.98)	Acc@5  85.16 ( 83.44)
Epoch: [9][330/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.001)	Loss 1.7680e+00 (1.6871e+00)	Acc@1  48.44 ( 52.97)	Acc@5  82.03 ( 83.44)
Epoch: [9][340/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 1.6801e+00 (1.6860e+00)	Acc@1  53.12 ( 52.94)	Acc@5  86.72 ( 83.49)
Epoch: [9][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6293e+00 (1.6878e+00)	Acc@1  58.59 ( 52.92)	Acc@5  87.50 ( 83.44)
Epoch: [9][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7836e+00 (1.6896e+00)	Acc@1  51.56 ( 52.89)	Acc@5  80.47 ( 83.37)
Epoch: [9][370/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.001)	Loss 1.7598e+00 (1.6909e+00)	Acc@1  53.12 ( 52.82)	Acc@5  85.16 ( 83.36)
Epoch: [9][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8899e+00 (1.6924e+00)	Acc@1  50.00 ( 52.77)	Acc@5  82.81 ( 83.36)
Epoch: [9][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7339e+00 (1.6928e+00)	Acc@1  55.00 ( 52.76)	Acc@5  82.50 ( 83.38)
## e[9] optimizer.zero_grad (sum) time: 0.2673642635345459
## e[9]       loss.backward (sum) time: 4.073253870010376
## e[9]      optimizer.step (sum) time: 1.8094959259033203
## epoch[9] training(only) time: 16.27120280265808
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 2.1388e+00 (2.1388e+00)	Acc@1  46.00 ( 46.00)	Acc@5  72.00 ( 72.00)
Test: [ 10/100]	Time  0.019 ( 0.034)	Loss 2.4222e+00 (2.2200e+00)	Acc@1  40.00 ( 43.64)	Acc@5  77.00 ( 76.00)
Test: [ 20/100]	Time  0.025 ( 0.029)	Loss 1.9751e+00 (2.1919e+00)	Acc@1  49.00 ( 43.90)	Acc@5  82.00 ( 76.24)
Test: [ 30/100]	Time  0.026 ( 0.027)	Loss 2.1409e+00 (2.1874e+00)	Acc@1  48.00 ( 44.55)	Acc@5  75.00 ( 76.55)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 2.2781e+00 (2.1767e+00)	Acc@1  44.00 ( 44.34)	Acc@5  76.00 ( 76.39)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 2.1103e+00 (2.1905e+00)	Acc@1  42.00 ( 44.08)	Acc@5  72.00 ( 75.80)
Test: [ 60/100]	Time  0.024 ( 0.024)	Loss 2.0462e+00 (2.1746e+00)	Acc@1  48.00 ( 44.13)	Acc@5  77.00 ( 75.90)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 2.1301e+00 (2.1688e+00)	Acc@1  37.00 ( 43.94)	Acc@5  79.00 ( 75.82)
Test: [ 80/100]	Time  0.017 ( 0.024)	Loss 2.4847e+00 (2.1879e+00)	Acc@1  41.00 ( 43.93)	Acc@5  74.00 ( 75.62)
Test: [ 90/100]	Time  0.016 ( 0.023)	Loss 2.4433e+00 (2.1868e+00)	Acc@1  47.00 ( 44.25)	Acc@5  73.00 ( 75.82)
 * Acc@1 44.230 Acc@5 75.960
### epoch[9] execution time: 18.640323638916016
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.203 ( 0.203)	Data  0.154 ( 0.154)	Loss 1.7420e+00 (1.7420e+00)	Acc@1  47.66 ( 47.66)	Acc@5  81.25 ( 81.25)
Epoch: [10][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.015)	Loss 1.4796e+00 (1.6029e+00)	Acc@1  62.50 ( 55.04)	Acc@5  84.38 ( 83.95)
Epoch: [10][ 20/391]	Time  0.042 ( 0.050)	Data  0.001 ( 0.008)	Loss 1.5710e+00 (1.6325e+00)	Acc@1  58.59 ( 54.50)	Acc@5  85.16 ( 83.59)
Epoch: [10][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.006)	Loss 1.6007e+00 (1.6222e+00)	Acc@1  53.12 ( 54.74)	Acc@5  87.50 ( 83.97)
Epoch: [10][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.9182e+00 (1.6299e+00)	Acc@1  46.88 ( 54.71)	Acc@5  81.25 ( 83.92)
Epoch: [10][ 50/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.004)	Loss 1.6036e+00 (1.6303e+00)	Acc@1  54.69 ( 54.50)	Acc@5  82.81 ( 83.99)
Epoch: [10][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.5716e+00 (1.6137e+00)	Acc@1  53.91 ( 55.12)	Acc@5  85.94 ( 84.36)
Epoch: [10][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5855e+00 (1.6052e+00)	Acc@1  57.81 ( 55.40)	Acc@5  83.59 ( 84.43)
Epoch: [10][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5312e+00 (1.6046e+00)	Acc@1  60.16 ( 55.53)	Acc@5  82.81 ( 84.57)
Epoch: [10][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4569e+00 (1.6010e+00)	Acc@1  60.94 ( 55.62)	Acc@5  84.38 ( 84.62)
Epoch: [10][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3044e+00 (1.6077e+00)	Acc@1  64.84 ( 55.52)	Acc@5  89.06 ( 84.48)
Epoch: [10][110/391]	Time  0.055 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5395e+00 (1.6115e+00)	Acc@1  53.12 ( 55.26)	Acc@5  86.72 ( 84.49)
Epoch: [10][120/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5051e+00 (1.6099e+00)	Acc@1  57.03 ( 55.26)	Acc@5  91.41 ( 84.48)
Epoch: [10][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6214e+00 (1.6164e+00)	Acc@1  59.38 ( 55.17)	Acc@5  85.16 ( 84.39)
Epoch: [10][140/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4310e+00 (1.6206e+00)	Acc@1  58.59 ( 55.13)	Acc@5  85.94 ( 84.18)
Epoch: [10][150/391]	Time  0.046 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.5184e+00 (1.6202e+00)	Acc@1  57.81 ( 55.05)	Acc@5  84.38 ( 84.14)
Epoch: [10][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7227e+00 (1.6207e+00)	Acc@1  47.66 ( 55.06)	Acc@5  84.38 ( 84.20)
Epoch: [10][170/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6863e+00 (1.6237e+00)	Acc@1  52.34 ( 54.90)	Acc@5  85.94 ( 84.21)
Epoch: [10][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3777e+00 (1.6248e+00)	Acc@1  60.94 ( 54.86)	Acc@5  88.28 ( 84.23)
Epoch: [10][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8399e+00 (1.6302e+00)	Acc@1  52.34 ( 54.78)	Acc@5  78.91 ( 84.09)
Epoch: [10][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8000e+00 (1.6334e+00)	Acc@1  50.00 ( 54.72)	Acc@5  80.47 ( 84.05)
Epoch: [10][210/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7351e+00 (1.6319e+00)	Acc@1  53.12 ( 54.66)	Acc@5  81.25 ( 84.09)
Epoch: [10][220/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4612e+00 (1.6299e+00)	Acc@1  57.81 ( 54.73)	Acc@5  82.03 ( 84.07)
Epoch: [10][230/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6768e+00 (1.6287e+00)	Acc@1  56.25 ( 54.80)	Acc@5  82.81 ( 84.09)
Epoch: [10][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4419e+00 (1.6274e+00)	Acc@1  58.59 ( 54.78)	Acc@5  85.94 ( 84.11)
Epoch: [10][250/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7982e+00 (1.6298e+00)	Acc@1  54.69 ( 54.72)	Acc@5  78.91 ( 84.06)
Epoch: [10][260/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6496e+00 (1.6306e+00)	Acc@1  51.56 ( 54.69)	Acc@5  82.81 ( 83.99)
Epoch: [10][270/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5246e+00 (1.6274e+00)	Acc@1  57.03 ( 54.73)	Acc@5  84.38 ( 84.04)
Epoch: [10][280/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.8126e+00 (1.6274e+00)	Acc@1  50.00 ( 54.73)	Acc@5  79.69 ( 84.02)
Epoch: [10][290/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5646e+00 (1.6291e+00)	Acc@1  54.69 ( 54.67)	Acc@5  85.16 ( 84.03)
Epoch: [10][300/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9229e+00 (1.6324e+00)	Acc@1  45.31 ( 54.61)	Acc@5  81.25 ( 83.97)
Epoch: [10][310/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6695e+00 (1.6330e+00)	Acc@1  52.34 ( 54.58)	Acc@5  83.59 ( 83.98)
Epoch: [10][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7481e+00 (1.6336e+00)	Acc@1  51.56 ( 54.56)	Acc@5  83.59 ( 83.99)
Epoch: [10][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4675e+00 (1.6346e+00)	Acc@1  57.03 ( 54.58)	Acc@5  89.06 ( 84.00)
Epoch: [10][340/391]	Time  0.048 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5709e+00 (1.6363e+00)	Acc@1  57.81 ( 54.58)	Acc@5  83.59 ( 83.96)
Epoch: [10][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6997e+00 (1.6374e+00)	Acc@1  53.12 ( 54.53)	Acc@5  85.94 ( 83.96)
Epoch: [10][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4424e+00 (1.6345e+00)	Acc@1  58.59 ( 54.61)	Acc@5  86.72 ( 84.04)
Epoch: [10][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4649e+00 (1.6333e+00)	Acc@1  51.56 ( 54.59)	Acc@5  87.50 ( 84.06)
Epoch: [10][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7650e+00 (1.6319e+00)	Acc@1  53.91 ( 54.60)	Acc@5  83.59 ( 84.14)
Epoch: [10][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.8518e+00 (1.6322e+00)	Acc@1  50.00 ( 54.60)	Acc@5  83.75 ( 84.15)
## e[10] optimizer.zero_grad (sum) time: 0.26499223709106445
## e[10]       loss.backward (sum) time: 4.076757431030273
## e[10]      optimizer.step (sum) time: 1.8196723461151123
## epoch[10] training(only) time: 16.250704526901245
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.9579e+00 (1.9579e+00)	Acc@1  51.00 ( 51.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.021 ( 0.032)	Loss 2.3078e+00 (2.0380e+00)	Acc@1  38.00 ( 47.73)	Acc@5  80.00 ( 78.27)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 1.7724e+00 (1.9940e+00)	Acc@1  53.00 ( 48.29)	Acc@5  81.00 ( 79.29)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 2.0017e+00 (1.9900e+00)	Acc@1  45.00 ( 47.74)	Acc@5  81.00 ( 79.06)
Test: [ 40/100]	Time  0.024 ( 0.024)	Loss 1.9404e+00 (1.9616e+00)	Acc@1  51.00 ( 48.32)	Acc@5  79.00 ( 79.29)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 1.6550e+00 (1.9733e+00)	Acc@1  54.00 ( 48.16)	Acc@5  85.00 ( 78.96)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.9764e+00 (1.9513e+00)	Acc@1  45.00 ( 48.30)	Acc@5  77.00 ( 79.16)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 2.0571e+00 (1.9436e+00)	Acc@1  50.00 ( 48.41)	Acc@5  81.00 ( 79.39)
Test: [ 80/100]	Time  0.020 ( 0.023)	Loss 2.0575e+00 (1.9543e+00)	Acc@1  47.00 ( 48.28)	Acc@5  82.00 ( 79.27)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.9503e+00 (1.9417e+00)	Acc@1  48.00 ( 48.38)	Acc@5  80.00 ( 79.64)
 * Acc@1 48.640 Acc@5 79.740
### epoch[10] execution time: 18.56275200843811
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.225 ( 0.225)	Data  0.170 ( 0.170)	Loss 1.5247e+00 (1.5247e+00)	Acc@1  53.91 ( 53.91)	Acc@5  87.50 ( 87.50)
Epoch: [11][ 10/391]	Time  0.039 ( 0.058)	Data  0.001 ( 0.016)	Loss 1.6205e+00 (1.4938e+00)	Acc@1  51.56 ( 57.46)	Acc@5  85.16 ( 87.22)
Epoch: [11][ 20/391]	Time  0.038 ( 0.050)	Data  0.001 ( 0.009)	Loss 1.4040e+00 (1.5122e+00)	Acc@1  59.38 ( 57.29)	Acc@5  87.50 ( 86.76)
Epoch: [11][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.4972e+00 (1.5201e+00)	Acc@1  56.25 ( 57.11)	Acc@5  87.50 ( 86.37)
Epoch: [11][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.6193e+00 (1.5269e+00)	Acc@1  57.81 ( 56.94)	Acc@5  85.16 ( 86.01)
Epoch: [11][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.5877e+00 (1.5387e+00)	Acc@1  50.00 ( 56.40)	Acc@5  87.50 ( 85.94)
Epoch: [11][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.5811e+00 (1.5227e+00)	Acc@1  58.59 ( 56.98)	Acc@5  86.72 ( 86.21)
Epoch: [11][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5253e+00 (1.5303e+00)	Acc@1  54.69 ( 56.65)	Acc@5  85.16 ( 86.07)
Epoch: [11][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4852e+00 (1.5330e+00)	Acc@1  59.38 ( 56.45)	Acc@5  85.16 ( 86.06)
Epoch: [11][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.6158e+00 (1.5423e+00)	Acc@1  56.25 ( 56.28)	Acc@5  84.38 ( 85.88)
Epoch: [11][100/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3980e+00 (1.5468e+00)	Acc@1  58.59 ( 56.21)	Acc@5  91.41 ( 85.85)
Epoch: [11][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6019e+00 (1.5435e+00)	Acc@1  57.03 ( 56.37)	Acc@5  87.50 ( 85.99)
Epoch: [11][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5921e+00 (1.5488e+00)	Acc@1  55.47 ( 56.15)	Acc@5  84.38 ( 85.86)
Epoch: [11][130/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6017e+00 (1.5481e+00)	Acc@1  56.25 ( 56.23)	Acc@5  86.72 ( 85.88)
Epoch: [11][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2336e+00 (1.5463e+00)	Acc@1  60.94 ( 56.26)	Acc@5  89.84 ( 85.92)
Epoch: [11][150/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.7473e+00 (1.5482e+00)	Acc@1  57.81 ( 56.23)	Acc@5  81.25 ( 85.86)
Epoch: [11][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6236e+00 (1.5477e+00)	Acc@1  57.81 ( 56.31)	Acc@5  82.03 ( 85.76)
Epoch: [11][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4924e+00 (1.5505e+00)	Acc@1  58.59 ( 56.24)	Acc@5  89.06 ( 85.72)
Epoch: [11][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4279e+00 (1.5510e+00)	Acc@1  61.72 ( 56.21)	Acc@5  87.50 ( 85.74)
Epoch: [11][190/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6536e+00 (1.5543e+00)	Acc@1  56.25 ( 56.16)	Acc@5  86.72 ( 85.66)
Epoch: [11][200/391]	Time  0.036 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.5859e+00 (1.5575e+00)	Acc@1  53.91 ( 56.06)	Acc@5  85.94 ( 85.62)
Epoch: [11][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5677e+00 (1.5589e+00)	Acc@1  59.38 ( 56.00)	Acc@5  84.38 ( 85.56)
Epoch: [11][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4176e+00 (1.5613e+00)	Acc@1  57.81 ( 56.01)	Acc@5  86.72 ( 85.52)
Epoch: [11][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3732e+00 (1.5618e+00)	Acc@1  53.91 ( 55.98)	Acc@5  89.84 ( 85.51)
Epoch: [11][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3964e+00 (1.5617e+00)	Acc@1  61.72 ( 55.98)	Acc@5  86.72 ( 85.46)
Epoch: [11][250/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6498e+00 (1.5650e+00)	Acc@1  60.16 ( 55.93)	Acc@5  83.59 ( 85.40)
Epoch: [11][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8233e+00 (1.5682e+00)	Acc@1  50.78 ( 55.84)	Acc@5  82.03 ( 85.37)
Epoch: [11][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4747e+00 (1.5677e+00)	Acc@1  62.50 ( 55.90)	Acc@5  85.16 ( 85.37)
Epoch: [11][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6286e+00 (1.5675e+00)	Acc@1  59.38 ( 55.92)	Acc@5  85.16 ( 85.37)
Epoch: [11][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6741e+00 (1.5691e+00)	Acc@1  53.91 ( 55.92)	Acc@5  82.03 ( 85.31)
Epoch: [11][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6384e+00 (1.5688e+00)	Acc@1  52.34 ( 55.88)	Acc@5  85.94 ( 85.31)
Epoch: [11][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7689e+00 (1.5697e+00)	Acc@1  48.44 ( 55.87)	Acc@5  82.03 ( 85.27)
Epoch: [11][320/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.9583e+00 (1.5708e+00)	Acc@1  46.09 ( 55.85)	Acc@5  78.12 ( 85.25)
Epoch: [11][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7536e+00 (1.5710e+00)	Acc@1  47.66 ( 55.83)	Acc@5  78.91 ( 85.22)
Epoch: [11][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6866e+00 (1.5727e+00)	Acc@1  50.78 ( 55.78)	Acc@5  81.25 ( 85.18)
Epoch: [11][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7496e+00 (1.5747e+00)	Acc@1  51.56 ( 55.72)	Acc@5  77.34 ( 85.13)
Epoch: [11][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5422e+00 (1.5764e+00)	Acc@1  55.47 ( 55.66)	Acc@5  85.16 ( 85.11)
Epoch: [11][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4140e+00 (1.5766e+00)	Acc@1  56.25 ( 55.67)	Acc@5  90.62 ( 85.10)
Epoch: [11][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.9359e+00 (1.5784e+00)	Acc@1  51.56 ( 55.65)	Acc@5  78.12 ( 85.05)
Epoch: [11][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.7998e+00 (1.5800e+00)	Acc@1  52.50 ( 55.59)	Acc@5  82.50 ( 85.02)
## e[11] optimizer.zero_grad (sum) time: 0.2654597759246826
## e[11]       loss.backward (sum) time: 4.019375562667847
## e[11]      optimizer.step (sum) time: 1.8524327278137207
## epoch[11] training(only) time: 16.073150396347046
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 2.0073e+00 (2.0073e+00)	Acc@1  53.00 ( 53.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.019 ( 0.032)	Loss 2.0616e+00 (1.9195e+00)	Acc@1  47.00 ( 51.09)	Acc@5  81.00 ( 81.00)
Test: [ 20/100]	Time  0.022 ( 0.026)	Loss 1.5859e+00 (1.9045e+00)	Acc@1  54.00 ( 51.24)	Acc@5  87.00 ( 81.10)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 1.9580e+00 (1.9037e+00)	Acc@1  48.00 ( 50.84)	Acc@5  79.00 ( 80.58)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 1.7746e+00 (1.8854e+00)	Acc@1  55.00 ( 50.63)	Acc@5  79.00 ( 80.68)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 1.4742e+00 (1.8958e+00)	Acc@1  61.00 ( 50.35)	Acc@5  87.00 ( 80.31)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 1.8263e+00 (1.8765e+00)	Acc@1  52.00 ( 50.70)	Acc@5  84.00 ( 80.67)
Test: [ 70/100]	Time  0.024 ( 0.023)	Loss 2.0931e+00 (1.8711e+00)	Acc@1  49.00 ( 50.86)	Acc@5  77.00 ( 80.76)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 2.0123e+00 (1.8849e+00)	Acc@1  51.00 ( 50.57)	Acc@5  80.00 ( 80.53)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 1.9562e+00 (1.8748e+00)	Acc@1  47.00 ( 50.74)	Acc@5  81.00 ( 80.57)
 * Acc@1 51.150 Acc@5 80.780
### epoch[11] execution time: 18.382254600524902
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.199 ( 0.199)	Data  0.149 ( 0.149)	Loss 1.5154e+00 (1.5154e+00)	Acc@1  56.25 ( 56.25)	Acc@5  85.16 ( 85.16)
Epoch: [12][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.5883e+00 (1.4843e+00)	Acc@1  54.69 ( 57.03)	Acc@5  84.38 ( 86.58)
Epoch: [12][ 20/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.4823e+00 (1.5316e+00)	Acc@1  57.81 ( 56.77)	Acc@5  84.38 ( 86.01)
Epoch: [12][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.006)	Loss 1.4624e+00 (1.5123e+00)	Acc@1  54.69 ( 56.93)	Acc@5  87.50 ( 86.19)
Epoch: [12][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.3463e+00 (1.4950e+00)	Acc@1  61.72 ( 57.41)	Acc@5  92.19 ( 86.62)
Epoch: [12][ 50/391]	Time  0.045 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.6194e+00 (1.5012e+00)	Acc@1  50.00 ( 57.09)	Acc@5  82.81 ( 86.41)
Epoch: [12][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.3888e+00 (1.5067e+00)	Acc@1  63.28 ( 56.84)	Acc@5  87.50 ( 86.31)
Epoch: [12][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3361e+00 (1.5085e+00)	Acc@1  61.72 ( 56.99)	Acc@5  88.28 ( 86.19)
Epoch: [12][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5754e+00 (1.5142e+00)	Acc@1  57.03 ( 56.85)	Acc@5  85.16 ( 86.35)
Epoch: [12][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5665e+00 (1.5180e+00)	Acc@1  53.91 ( 56.77)	Acc@5  81.25 ( 86.20)
Epoch: [12][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6826e+00 (1.5152e+00)	Acc@1  55.47 ( 56.84)	Acc@5  84.38 ( 86.32)
Epoch: [12][110/391]	Time  0.044 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.3387e+00 (1.5102e+00)	Acc@1  64.06 ( 57.01)	Acc@5  89.06 ( 86.42)
Epoch: [12][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4737e+00 (1.5104e+00)	Acc@1  57.03 ( 56.99)	Acc@5  85.16 ( 86.36)
Epoch: [12][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5809e+00 (1.5093e+00)	Acc@1  53.12 ( 56.98)	Acc@5  85.16 ( 86.37)
Epoch: [12][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6642e+00 (1.5123e+00)	Acc@1  57.81 ( 56.99)	Acc@5  82.03 ( 86.35)
Epoch: [12][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5507e+00 (1.5100e+00)	Acc@1  56.25 ( 57.11)	Acc@5  85.94 ( 86.40)
Epoch: [12][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5165e+00 (1.5082e+00)	Acc@1  53.91 ( 57.17)	Acc@5  85.16 ( 86.42)
Epoch: [12][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5522e+00 (1.5115e+00)	Acc@1  57.03 ( 57.14)	Acc@5  83.59 ( 86.33)
Epoch: [12][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4644e+00 (1.5097e+00)	Acc@1  61.72 ( 57.16)	Acc@5  88.28 ( 86.42)
Epoch: [12][190/391]	Time  0.039 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.5956e+00 (1.5078e+00)	Acc@1  51.56 ( 57.25)	Acc@5  83.59 ( 86.40)
Epoch: [12][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6416e+00 (1.5111e+00)	Acc@1  53.91 ( 57.20)	Acc@5  83.59 ( 86.36)
Epoch: [12][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6848e+00 (1.5135e+00)	Acc@1  55.47 ( 57.18)	Acc@5  84.38 ( 86.32)
Epoch: [12][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5105e+00 (1.5130e+00)	Acc@1  54.69 ( 57.19)	Acc@5  86.72 ( 86.32)
Epoch: [12][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4558e+00 (1.5136e+00)	Acc@1  56.25 ( 57.17)	Acc@5  86.72 ( 86.30)
Epoch: [12][240/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9163e+00 (1.5165e+00)	Acc@1  46.09 ( 57.04)	Acc@5  81.25 ( 86.25)
Epoch: [12][250/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5652e+00 (1.5167e+00)	Acc@1  60.94 ( 57.07)	Acc@5  85.94 ( 86.26)
Epoch: [12][260/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4757e+00 (1.5148e+00)	Acc@1  60.16 ( 57.11)	Acc@5  83.59 ( 86.28)
Epoch: [12][270/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4516e+00 (1.5170e+00)	Acc@1  63.28 ( 57.07)	Acc@5  82.81 ( 86.21)
Epoch: [12][280/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4878e+00 (1.5171e+00)	Acc@1  54.69 ( 57.08)	Acc@5  87.50 ( 86.18)
Epoch: [12][290/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4273e+00 (1.5148e+00)	Acc@1  57.81 ( 57.11)	Acc@5  89.06 ( 86.22)
Epoch: [12][300/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4878e+00 (1.5170e+00)	Acc@1  60.94 ( 57.04)	Acc@5  86.72 ( 86.16)
Epoch: [12][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4988e+00 (1.5176e+00)	Acc@1  53.12 ( 57.01)	Acc@5  89.84 ( 86.17)
Epoch: [12][320/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.5218e+00 (1.5163e+00)	Acc@1  58.59 ( 57.02)	Acc@5  86.72 ( 86.20)
Epoch: [12][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4644e+00 (1.5161e+00)	Acc@1  58.59 ( 57.04)	Acc@5  85.94 ( 86.20)
Epoch: [12][340/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3022e+00 (1.5163e+00)	Acc@1  61.72 ( 57.05)	Acc@5  89.84 ( 86.20)
Epoch: [12][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5847e+00 (1.5174e+00)	Acc@1  55.47 ( 56.97)	Acc@5  86.72 ( 86.21)
Epoch: [12][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4442e+00 (1.5182e+00)	Acc@1  57.03 ( 56.96)	Acc@5  84.38 ( 86.18)
Epoch: [12][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5490e+00 (1.5176e+00)	Acc@1  58.59 ( 56.98)	Acc@5  82.81 ( 86.20)
Epoch: [12][380/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.001)	Loss 1.5849e+00 (1.5184e+00)	Acc@1  60.16 ( 57.01)	Acc@5  83.59 ( 86.19)
Epoch: [12][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6844e+00 (1.5193e+00)	Acc@1  57.50 ( 56.99)	Acc@5  85.00 ( 86.16)
## e[12] optimizer.zero_grad (sum) time: 0.268634557723999
## e[12]       loss.backward (sum) time: 4.082693815231323
## e[12]      optimizer.step (sum) time: 1.8128159046173096
## epoch[12] training(only) time: 16.22550082206726
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.6919e+00 (1.6919e+00)	Acc@1  50.00 ( 50.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.022 ( 0.033)	Loss 1.8565e+00 (1.7570e+00)	Acc@1  49.00 ( 52.36)	Acc@5  86.00 ( 83.36)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 1.5029e+00 (1.6962e+00)	Acc@1  56.00 ( 53.48)	Acc@5  86.00 ( 84.24)
Test: [ 30/100]	Time  0.019 ( 0.027)	Loss 1.9790e+00 (1.7040e+00)	Acc@1  48.00 ( 53.65)	Acc@5  79.00 ( 84.00)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.6267e+00 (1.7044e+00)	Acc@1  57.00 ( 53.83)	Acc@5  83.00 ( 83.83)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 1.5340e+00 (1.7173e+00)	Acc@1  59.00 ( 53.65)	Acc@5  82.00 ( 83.33)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 1.7161e+00 (1.7117e+00)	Acc@1  55.00 ( 53.85)	Acc@5  83.00 ( 83.30)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.6432e+00 (1.7118e+00)	Acc@1  53.00 ( 54.07)	Acc@5  84.00 ( 83.20)
Test: [ 80/100]	Time  0.018 ( 0.024)	Loss 2.1036e+00 (1.7243e+00)	Acc@1  48.00 ( 53.73)	Acc@5  79.00 ( 82.93)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8407e+00 (1.7187e+00)	Acc@1  50.00 ( 53.82)	Acc@5  81.00 ( 82.97)
 * Acc@1 53.940 Acc@5 83.120
### epoch[12] execution time: 18.65070605278015
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.198 ( 0.198)	Data  0.147 ( 0.147)	Loss 1.5752e+00 (1.5752e+00)	Acc@1  54.69 ( 54.69)	Acc@5  85.94 ( 85.94)
Epoch: [13][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.4584e+00 (1.4547e+00)	Acc@1  58.59 ( 58.31)	Acc@5  88.28 ( 88.57)
Epoch: [13][ 20/391]	Time  0.036 ( 0.048)	Data  0.001 ( 0.008)	Loss 1.3387e+00 (1.4572e+00)	Acc@1  57.81 ( 58.48)	Acc@5  85.94 ( 87.31)
Epoch: [13][ 30/391]	Time  0.039 ( 0.046)	Data  0.002 ( 0.006)	Loss 1.4260e+00 (1.4612e+00)	Acc@1  62.50 ( 58.49)	Acc@5  84.38 ( 87.25)
Epoch: [13][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.3901e+00 (1.4563e+00)	Acc@1  60.94 ( 58.61)	Acc@5  89.06 ( 87.25)
Epoch: [13][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.4552e+00 (1.4375e+00)	Acc@1  59.38 ( 59.16)	Acc@5  89.84 ( 87.52)
Epoch: [13][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4790e+00 (1.4404e+00)	Acc@1  60.16 ( 58.79)	Acc@5  89.06 ( 87.58)
Epoch: [13][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4541e+00 (1.4481e+00)	Acc@1  59.38 ( 58.68)	Acc@5  89.06 ( 87.44)
Epoch: [13][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2711e+00 (1.4444e+00)	Acc@1  59.38 ( 58.68)	Acc@5  90.62 ( 87.41)
Epoch: [13][ 90/391]	Time  0.047 ( 0.042)	Data  0.002 ( 0.003)	Loss 1.2410e+00 (1.4440e+00)	Acc@1  65.62 ( 58.74)	Acc@5  91.41 ( 87.51)
Epoch: [13][100/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.003)	Loss 1.5101e+00 (1.4553e+00)	Acc@1  60.94 ( 58.59)	Acc@5  83.59 ( 87.31)
Epoch: [13][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6428e+00 (1.4609e+00)	Acc@1  51.56 ( 58.50)	Acc@5  83.59 ( 87.25)
Epoch: [13][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3179e+00 (1.4612e+00)	Acc@1  64.84 ( 58.58)	Acc@5  90.62 ( 87.22)
Epoch: [13][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5128e+00 (1.4642e+00)	Acc@1  52.34 ( 58.50)	Acc@5  85.94 ( 87.14)
Epoch: [13][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4883e+00 (1.4652e+00)	Acc@1  59.38 ( 58.55)	Acc@5  87.50 ( 87.07)
Epoch: [13][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3486e+00 (1.4668e+00)	Acc@1  63.28 ( 58.50)	Acc@5  89.06 ( 87.03)
Epoch: [13][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5121e+00 (1.4649e+00)	Acc@1  59.38 ( 58.55)	Acc@5  86.72 ( 87.09)
Epoch: [13][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4839e+00 (1.4654e+00)	Acc@1  53.91 ( 58.47)	Acc@5  90.62 ( 87.10)
Epoch: [13][180/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.4570e+00 (1.4651e+00)	Acc@1  56.25 ( 58.44)	Acc@5  85.94 ( 87.08)
Epoch: [13][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3791e+00 (1.4613e+00)	Acc@1  63.28 ( 58.58)	Acc@5  85.94 ( 87.11)
Epoch: [13][200/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.6451e+00 (1.4596e+00)	Acc@1  54.69 ( 58.63)	Acc@5  83.59 ( 87.12)
Epoch: [13][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5763e+00 (1.4615e+00)	Acc@1  60.16 ( 58.64)	Acc@5  84.38 ( 87.07)
Epoch: [13][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4114e+00 (1.4647e+00)	Acc@1  54.69 ( 58.57)	Acc@5  85.94 ( 87.04)
Epoch: [13][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5557e+00 (1.4673e+00)	Acc@1  52.34 ( 58.55)	Acc@5  89.84 ( 87.00)
Epoch: [13][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5465e+00 (1.4672e+00)	Acc@1  62.50 ( 58.60)	Acc@5  85.16 ( 87.02)
Epoch: [13][250/391]	Time  0.041 ( 0.041)	Data  0.000 ( 0.002)	Loss 1.4632e+00 (1.4681e+00)	Acc@1  57.81 ( 58.59)	Acc@5  85.16 ( 87.02)
Epoch: [13][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4748e+00 (1.4714e+00)	Acc@1  60.16 ( 58.48)	Acc@5  84.38 ( 86.92)
Epoch: [13][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3585e+00 (1.4701e+00)	Acc@1  60.94 ( 58.52)	Acc@5  89.84 ( 86.92)
Epoch: [13][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3584e+00 (1.4687e+00)	Acc@1  62.50 ( 58.57)	Acc@5  86.72 ( 86.91)
Epoch: [13][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.8547e+00 (1.4720e+00)	Acc@1  48.44 ( 58.46)	Acc@5  77.34 ( 86.85)
Epoch: [13][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6313e+00 (1.4741e+00)	Acc@1  53.91 ( 58.44)	Acc@5  86.72 ( 86.84)
Epoch: [13][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4768e+00 (1.4757e+00)	Acc@1  60.94 ( 58.41)	Acc@5  84.38 ( 86.78)
Epoch: [13][320/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.4355e+00 (1.4768e+00)	Acc@1  59.38 ( 58.38)	Acc@5  84.38 ( 86.71)
Epoch: [13][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2212e+00 (1.4767e+00)	Acc@1  64.84 ( 58.39)	Acc@5  92.19 ( 86.71)
Epoch: [13][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4421e+00 (1.4761e+00)	Acc@1  61.72 ( 58.40)	Acc@5  88.28 ( 86.73)
Epoch: [13][350/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4641e+00 (1.4755e+00)	Acc@1  56.25 ( 58.40)	Acc@5  83.59 ( 86.76)
Epoch: [13][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3683e+00 (1.4757e+00)	Acc@1  64.84 ( 58.42)	Acc@5  87.50 ( 86.74)
Epoch: [13][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7404e+00 (1.4766e+00)	Acc@1  57.03 ( 58.37)	Acc@5  81.25 ( 86.69)
Epoch: [13][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5999e+00 (1.4764e+00)	Acc@1  59.38 ( 58.40)	Acc@5  83.59 ( 86.67)
Epoch: [13][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1493e+00 (1.4777e+00)	Acc@1  67.50 ( 58.35)	Acc@5  93.75 ( 86.65)
## e[13] optimizer.zero_grad (sum) time: 0.26556873321533203
## e[13]       loss.backward (sum) time: 4.03851842880249
## e[13]      optimizer.step (sum) time: 1.8346710205078125
## epoch[13] training(only) time: 16.09054970741272
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.7554e+00 (1.7554e+00)	Acc@1  54.00 ( 54.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 1.8992e+00 (1.8078e+00)	Acc@1  51.00 ( 53.45)	Acc@5  82.00 ( 81.55)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.4990e+00 (1.7634e+00)	Acc@1  58.00 ( 53.29)	Acc@5  87.00 ( 82.24)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 1.9274e+00 (1.7857e+00)	Acc@1  53.00 ( 53.23)	Acc@5  83.00 ( 82.16)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.9188e+00 (1.7918e+00)	Acc@1  54.00 ( 53.12)	Acc@5  79.00 ( 82.20)
Test: [ 50/100]	Time  0.020 ( 0.025)	Loss 1.4738e+00 (1.7964e+00)	Acc@1  63.00 ( 53.41)	Acc@5  84.00 ( 81.90)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.8449e+00 (1.7852e+00)	Acc@1  53.00 ( 53.21)	Acc@5  83.00 ( 82.13)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 1.7271e+00 (1.7831e+00)	Acc@1  54.00 ( 53.11)	Acc@5  80.00 ( 82.03)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.8619e+00 (1.7945e+00)	Acc@1  51.00 ( 52.75)	Acc@5  82.00 ( 81.91)
Test: [ 90/100]	Time  0.020 ( 0.023)	Loss 1.8818e+00 (1.7918e+00)	Acc@1  55.00 ( 52.89)	Acc@5  84.00 ( 82.04)
 * Acc@1 52.880 Acc@5 82.120
### epoch[13] execution time: 18.434504747390747
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.201 ( 0.201)	Data  0.149 ( 0.149)	Loss 1.4186e+00 (1.4186e+00)	Acc@1  60.94 ( 60.94)	Acc@5  85.16 ( 85.16)
Epoch: [14][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.4953e+00 (1.4128e+00)	Acc@1  57.03 ( 60.09)	Acc@5  91.41 ( 88.42)
Epoch: [14][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.008)	Loss 1.4651e+00 (1.3661e+00)	Acc@1  59.38 ( 61.12)	Acc@5  88.28 ( 88.88)
Epoch: [14][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.006)	Loss 1.2718e+00 (1.3743e+00)	Acc@1  63.28 ( 60.76)	Acc@5  89.06 ( 88.73)
Epoch: [14][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.2915e+00 (1.3980e+00)	Acc@1  64.06 ( 59.74)	Acc@5  91.41 ( 88.21)
Epoch: [14][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.2607e+00 (1.4042e+00)	Acc@1  63.28 ( 59.57)	Acc@5  90.62 ( 88.04)
Epoch: [14][ 60/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.5012e+00 (1.4066e+00)	Acc@1  58.59 ( 59.58)	Acc@5  84.38 ( 88.03)
Epoch: [14][ 70/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4630e+00 (1.4065e+00)	Acc@1  57.81 ( 59.54)	Acc@5  91.41 ( 88.00)
Epoch: [14][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4722e+00 (1.4065e+00)	Acc@1  60.16 ( 59.58)	Acc@5  88.28 ( 87.93)
Epoch: [14][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7195e+00 (1.4113e+00)	Acc@1  48.44 ( 59.38)	Acc@5  82.03 ( 87.85)
Epoch: [14][100/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5859e+00 (1.4133e+00)	Acc@1  54.69 ( 59.27)	Acc@5  86.72 ( 87.95)
Epoch: [14][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5080e+00 (1.4158e+00)	Acc@1  62.50 ( 59.32)	Acc@5  88.28 ( 87.95)
Epoch: [14][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4182e+00 (1.4131e+00)	Acc@1  60.94 ( 59.33)	Acc@5  85.16 ( 88.00)
Epoch: [14][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1916e+00 (1.4165e+00)	Acc@1  66.41 ( 59.16)	Acc@5  91.41 ( 87.95)
Epoch: [14][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2692e+00 (1.4161e+00)	Acc@1  59.38 ( 59.17)	Acc@5  91.41 ( 87.94)
Epoch: [14][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2612e+00 (1.4189e+00)	Acc@1  61.72 ( 59.22)	Acc@5  89.84 ( 87.89)
Epoch: [14][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3840e+00 (1.4184e+00)	Acc@1  53.12 ( 59.26)	Acc@5  92.19 ( 87.92)
Epoch: [14][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4950e+00 (1.4146e+00)	Acc@1  54.69 ( 59.36)	Acc@5  84.38 ( 87.91)
Epoch: [14][180/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5981e+00 (1.4158e+00)	Acc@1  50.00 ( 59.27)	Acc@5  85.94 ( 87.87)
Epoch: [14][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4523e+00 (1.4209e+00)	Acc@1  61.72 ( 59.12)	Acc@5  85.94 ( 87.79)
Epoch: [14][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3225e+00 (1.4220e+00)	Acc@1  62.50 ( 59.07)	Acc@5  90.62 ( 87.80)
Epoch: [14][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3892e+00 (1.4247e+00)	Acc@1  57.81 ( 59.06)	Acc@5  89.06 ( 87.70)
Epoch: [14][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3564e+00 (1.4275e+00)	Acc@1  67.97 ( 59.02)	Acc@5  87.50 ( 87.67)
Epoch: [14][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4839e+00 (1.4297e+00)	Acc@1  54.69 ( 59.01)	Acc@5  88.28 ( 87.69)
Epoch: [14][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3355e+00 (1.4302e+00)	Acc@1  61.72 ( 59.03)	Acc@5  89.84 ( 87.69)
Epoch: [14][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5676e+00 (1.4312e+00)	Acc@1  55.47 ( 58.97)	Acc@5  85.16 ( 87.68)
Epoch: [14][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5135e+00 (1.4315e+00)	Acc@1  57.03 ( 58.97)	Acc@5  89.06 ( 87.69)
Epoch: [14][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4959e+00 (1.4329e+00)	Acc@1  60.94 ( 58.98)	Acc@5  85.16 ( 87.67)
Epoch: [14][280/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.4467e+00 (1.4343e+00)	Acc@1  60.94 ( 58.97)	Acc@5  82.03 ( 87.60)
Epoch: [14][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7164e+00 (1.4394e+00)	Acc@1  54.69 ( 58.85)	Acc@5  82.81 ( 87.49)
Epoch: [14][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4526e+00 (1.4383e+00)	Acc@1  58.59 ( 58.90)	Acc@5  89.06 ( 87.51)
Epoch: [14][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6604e+00 (1.4383e+00)	Acc@1  53.12 ( 58.88)	Acc@5  85.16 ( 87.50)
Epoch: [14][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5771e+00 (1.4370e+00)	Acc@1  53.12 ( 58.97)	Acc@5  86.72 ( 87.51)
Epoch: [14][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5365e+00 (1.4359e+00)	Acc@1  53.12 ( 58.93)	Acc@5  88.28 ( 87.57)
Epoch: [14][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7210e+00 (1.4351e+00)	Acc@1  53.91 ( 58.96)	Acc@5  80.47 ( 87.56)
Epoch: [14][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5351e+00 (1.4348e+00)	Acc@1  53.91 ( 58.99)	Acc@5  84.38 ( 87.54)
Epoch: [14][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5671e+00 (1.4360e+00)	Acc@1  58.59 ( 58.98)	Acc@5  86.72 ( 87.50)
Epoch: [14][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4253e+00 (1.4359e+00)	Acc@1  62.50 ( 59.00)	Acc@5  85.16 ( 87.47)
Epoch: [14][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4127e+00 (1.4353e+00)	Acc@1  58.59 ( 59.04)	Acc@5  89.84 ( 87.46)
Epoch: [14][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5031e+00 (1.4376e+00)	Acc@1  55.00 ( 58.99)	Acc@5  85.00 ( 87.43)
## e[14] optimizer.zero_grad (sum) time: 0.2674417495727539
## e[14]       loss.backward (sum) time: 4.007381200790405
## e[14]      optimizer.step (sum) time: 1.8156464099884033
## epoch[14] training(only) time: 16.183116912841797
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.9750e+00 (1.9750e+00)	Acc@1  53.00 ( 53.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.9350e+00 (1.7690e+00)	Acc@1  47.00 ( 55.00)	Acc@5  83.00 ( 82.18)
Test: [ 20/100]	Time  0.024 ( 0.029)	Loss 1.3981e+00 (1.7504e+00)	Acc@1  58.00 ( 54.05)	Acc@5  88.00 ( 82.67)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.0257e+00 (1.7829e+00)	Acc@1  48.00 ( 53.74)	Acc@5  80.00 ( 81.94)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 1.7564e+00 (1.7823e+00)	Acc@1  57.00 ( 53.34)	Acc@5  86.00 ( 81.95)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4764e+00 (1.7949e+00)	Acc@1  61.00 ( 53.31)	Acc@5  82.00 ( 81.55)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.7339e+00 (1.7822e+00)	Acc@1  54.00 ( 53.38)	Acc@5  88.00 ( 81.67)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.6230e+00 (1.7819e+00)	Acc@1  51.00 ( 53.11)	Acc@5  86.00 ( 81.68)
Test: [ 80/100]	Time  0.019 ( 0.024)	Loss 1.8820e+00 (1.7880e+00)	Acc@1  51.00 ( 52.89)	Acc@5  79.00 ( 81.67)
Test: [ 90/100]	Time  0.021 ( 0.024)	Loss 1.9393e+00 (1.7849e+00)	Acc@1  53.00 ( 53.18)	Acc@5  77.00 ( 81.78)
 * Acc@1 53.320 Acc@5 81.870
### epoch[14] execution time: 18.608750581741333
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.197 ( 0.197)	Data  0.149 ( 0.149)	Loss 1.3731e+00 (1.3731e+00)	Acc@1  64.06 ( 64.06)	Acc@5  86.72 ( 86.72)
Epoch: [15][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.015)	Loss 1.2739e+00 (1.3209e+00)	Acc@1  60.16 ( 62.57)	Acc@5  91.41 ( 89.20)
Epoch: [15][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.3871e+00 (1.3490e+00)	Acc@1  55.47 ( 61.05)	Acc@5  89.84 ( 88.80)
Epoch: [15][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.2272e+00 (1.3444e+00)	Acc@1  66.41 ( 60.99)	Acc@5  87.50 ( 88.48)
Epoch: [15][ 40/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.2426e+00 (1.3404e+00)	Acc@1  64.84 ( 60.96)	Acc@5  91.41 ( 88.61)
Epoch: [15][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.2956e+00 (1.3449e+00)	Acc@1  60.16 ( 60.89)	Acc@5  91.41 ( 88.47)
Epoch: [15][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.4434e+00 (1.3555e+00)	Acc@1  57.03 ( 60.77)	Acc@5  87.50 ( 88.28)
Epoch: [15][ 70/391]	Time  0.041 ( 0.043)	Data  0.002 ( 0.003)	Loss 1.4084e+00 (1.3594e+00)	Acc@1  58.59 ( 60.62)	Acc@5  89.06 ( 88.25)
Epoch: [15][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4288e+00 (1.3615e+00)	Acc@1  58.59 ( 60.68)	Acc@5  87.50 ( 88.22)
Epoch: [15][ 90/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4784e+00 (1.3566e+00)	Acc@1  57.81 ( 60.82)	Acc@5  85.16 ( 88.32)
Epoch: [15][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4470e+00 (1.3625e+00)	Acc@1  56.25 ( 60.55)	Acc@5  89.06 ( 88.27)
Epoch: [15][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5762e+00 (1.3741e+00)	Acc@1  56.25 ( 60.34)	Acc@5  82.81 ( 88.03)
Epoch: [15][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3415e+00 (1.3741e+00)	Acc@1  64.06 ( 60.36)	Acc@5  89.06 ( 88.02)
Epoch: [15][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5928e+00 (1.3773e+00)	Acc@1  53.12 ( 60.36)	Acc@5  85.16 ( 87.99)
Epoch: [15][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2656e+00 (1.3799e+00)	Acc@1  64.84 ( 60.34)	Acc@5  89.84 ( 87.89)
Epoch: [15][150/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.3990e+00 (1.3817e+00)	Acc@1  62.50 ( 60.36)	Acc@5  85.94 ( 87.87)
Epoch: [15][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6785e+00 (1.3863e+00)	Acc@1  57.81 ( 60.25)	Acc@5  84.38 ( 87.76)
Epoch: [15][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5207e+00 (1.3836e+00)	Acc@1  51.56 ( 60.21)	Acc@5  85.94 ( 87.82)
Epoch: [15][180/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2851e+00 (1.3844e+00)	Acc@1  64.84 ( 60.19)	Acc@5  88.28 ( 87.82)
Epoch: [15][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1879e+00 (1.3818e+00)	Acc@1  65.62 ( 60.20)	Acc@5  94.53 ( 87.87)
Epoch: [15][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3679e+00 (1.3825e+00)	Acc@1  60.94 ( 60.14)	Acc@5  86.72 ( 87.85)
Epoch: [15][210/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1743e+00 (1.3840e+00)	Acc@1  61.72 ( 60.15)	Acc@5  91.41 ( 87.82)
Epoch: [15][220/391]	Time  0.048 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4850e+00 (1.3868e+00)	Acc@1  60.94 ( 60.11)	Acc@5  85.16 ( 87.83)
Epoch: [15][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3603e+00 (1.3883e+00)	Acc@1  60.16 ( 60.10)	Acc@5  88.28 ( 87.80)
Epoch: [15][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3961e+00 (1.3884e+00)	Acc@1  57.03 ( 60.12)	Acc@5  91.41 ( 87.80)
Epoch: [15][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2395e+00 (1.3892e+00)	Acc@1  62.50 ( 60.13)	Acc@5  87.50 ( 87.79)
Epoch: [15][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3752e+00 (1.3925e+00)	Acc@1  58.59 ( 60.01)	Acc@5  92.19 ( 87.80)
Epoch: [15][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3968e+00 (1.3948e+00)	Acc@1  60.94 ( 59.99)	Acc@5  89.84 ( 87.77)
Epoch: [15][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4558e+00 (1.3953e+00)	Acc@1  60.94 ( 59.98)	Acc@5  84.38 ( 87.76)
Epoch: [15][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4129e+00 (1.3946e+00)	Acc@1  57.81 ( 60.01)	Acc@5  89.06 ( 87.81)
Epoch: [15][300/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.4761e+00 (1.3953e+00)	Acc@1  53.12 ( 59.99)	Acc@5  87.50 ( 87.82)
Epoch: [15][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4200e+00 (1.3952e+00)	Acc@1  60.94 ( 59.99)	Acc@5  84.38 ( 87.82)
Epoch: [15][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3411e+00 (1.3951e+00)	Acc@1  60.16 ( 60.03)	Acc@5  87.50 ( 87.84)
Epoch: [15][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5611e+00 (1.3966e+00)	Acc@1  58.59 ( 60.04)	Acc@5  84.38 ( 87.83)
Epoch: [15][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3985e+00 (1.3967e+00)	Acc@1  59.38 ( 60.04)	Acc@5  88.28 ( 87.83)
Epoch: [15][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5778e+00 (1.3978e+00)	Acc@1  57.81 ( 60.06)	Acc@5  85.94 ( 87.82)
Epoch: [15][360/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.001)	Loss 1.3463e+00 (1.3977e+00)	Acc@1  53.91 ( 60.05)	Acc@5  88.28 ( 87.84)
Epoch: [15][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2234e+00 (1.3984e+00)	Acc@1  61.72 ( 60.01)	Acc@5  90.62 ( 87.84)
Epoch: [15][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5415e+00 (1.3986e+00)	Acc@1  59.38 ( 59.99)	Acc@5  82.81 ( 87.83)
Epoch: [15][390/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2866e+00 (1.3983e+00)	Acc@1  61.25 ( 59.99)	Acc@5  87.50 ( 87.83)
## e[15] optimizer.zero_grad (sum) time: 0.2682762145996094
## e[15]       loss.backward (sum) time: 4.050252914428711
## e[15]      optimizer.step (sum) time: 1.8314034938812256
## epoch[15] training(only) time: 16.16988968849182
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.7283e+00 (1.7283e+00)	Acc@1  55.00 ( 55.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 1.9464e+00 (1.6600e+00)	Acc@1  50.00 ( 57.18)	Acc@5  87.00 ( 84.55)
Test: [ 20/100]	Time  0.022 ( 0.030)	Loss 1.4092e+00 (1.6437e+00)	Acc@1  56.00 ( 56.67)	Acc@5  90.00 ( 84.86)
Test: [ 30/100]	Time  0.020 ( 0.027)	Loss 1.8328e+00 (1.6542e+00)	Acc@1  49.00 ( 56.19)	Acc@5  82.00 ( 84.42)
Test: [ 40/100]	Time  0.020 ( 0.025)	Loss 1.4902e+00 (1.6425e+00)	Acc@1  59.00 ( 56.27)	Acc@5  86.00 ( 84.41)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 1.5069e+00 (1.6498e+00)	Acc@1  58.00 ( 56.20)	Acc@5  87.00 ( 83.96)
Test: [ 60/100]	Time  0.024 ( 0.024)	Loss 1.6442e+00 (1.6385e+00)	Acc@1  55.00 ( 56.00)	Acc@5  85.00 ( 84.07)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6010e+00 (1.6418e+00)	Acc@1  50.00 ( 55.70)	Acc@5  83.00 ( 83.99)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.8033e+00 (1.6537e+00)	Acc@1  50.00 ( 55.30)	Acc@5  80.00 ( 83.95)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 1.6716e+00 (1.6506e+00)	Acc@1  52.00 ( 55.34)	Acc@5  82.00 ( 83.96)
 * Acc@1 55.470 Acc@5 84.120
### epoch[15] execution time: 18.5409414768219
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.196 ( 0.196)	Data  0.147 ( 0.147)	Loss 1.3202e+00 (1.3202e+00)	Acc@1  61.72 ( 61.72)	Acc@5  87.50 ( 87.50)
Epoch: [16][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.014)	Loss 1.3995e+00 (1.3201e+00)	Acc@1  60.94 ( 63.00)	Acc@5  86.72 ( 88.85)
Epoch: [16][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.008)	Loss 1.4782e+00 (1.3063e+00)	Acc@1  57.03 ( 63.76)	Acc@5  87.50 ( 89.32)
Epoch: [16][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.006)	Loss 1.3821e+00 (1.3074e+00)	Acc@1  59.38 ( 62.88)	Acc@5  86.72 ( 89.62)
Epoch: [16][ 40/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.005)	Loss 1.1453e+00 (1.3003e+00)	Acc@1  67.97 ( 63.03)	Acc@5  93.75 ( 89.86)
Epoch: [16][ 50/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.004)	Loss 1.1834e+00 (1.3032e+00)	Acc@1  68.75 ( 62.73)	Acc@5  87.50 ( 89.51)
Epoch: [16][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.3576e+00 (1.2989e+00)	Acc@1  61.72 ( 62.77)	Acc@5  89.06 ( 89.56)
Epoch: [16][ 70/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.2968e+00 (1.2965e+00)	Acc@1  60.16 ( 62.72)	Acc@5  89.06 ( 89.56)
Epoch: [16][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2239e+00 (1.3014e+00)	Acc@1  61.72 ( 62.48)	Acc@5  90.62 ( 89.58)
Epoch: [16][ 90/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3292e+00 (1.3105e+00)	Acc@1  59.38 ( 62.31)	Acc@5  89.84 ( 89.38)
Epoch: [16][100/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0625e+00 (1.3100e+00)	Acc@1  67.19 ( 62.28)	Acc@5  92.97 ( 89.35)
Epoch: [16][110/391]	Time  0.046 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.3879e+00 (1.3096e+00)	Acc@1  58.59 ( 62.34)	Acc@5  90.62 ( 89.36)
Epoch: [16][120/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.3997e+00 (1.3109e+00)	Acc@1  61.72 ( 62.14)	Acc@5  89.06 ( 89.35)
Epoch: [16][130/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.3022e+00 (1.3133e+00)	Acc@1  62.50 ( 62.11)	Acc@5  89.06 ( 89.28)
Epoch: [16][140/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3014e+00 (1.3187e+00)	Acc@1  63.28 ( 61.97)	Acc@5  90.62 ( 89.21)
Epoch: [16][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2686e+00 (1.3185e+00)	Acc@1  62.50 ( 61.94)	Acc@5  85.94 ( 89.21)
Epoch: [16][160/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.4715e+00 (1.3227e+00)	Acc@1  62.50 ( 61.84)	Acc@5  85.16 ( 89.12)
Epoch: [16][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3913e+00 (1.3329e+00)	Acc@1  60.94 ( 61.66)	Acc@5  85.94 ( 88.99)
Epoch: [16][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5488e+00 (1.3397e+00)	Acc@1  59.38 ( 61.50)	Acc@5  85.16 ( 88.82)
Epoch: [16][190/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5109e+00 (1.3429e+00)	Acc@1  57.03 ( 61.42)	Acc@5  87.50 ( 88.76)
Epoch: [16][200/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4361e+00 (1.3430e+00)	Acc@1  58.59 ( 61.45)	Acc@5  88.28 ( 88.74)
Epoch: [16][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2035e+00 (1.3403e+00)	Acc@1  67.97 ( 61.55)	Acc@5  89.84 ( 88.77)
Epoch: [16][220/391]	Time  0.037 ( 0.042)	Data  0.000 ( 0.002)	Loss 1.4651e+00 (1.3430e+00)	Acc@1  52.34 ( 61.48)	Acc@5  86.72 ( 88.76)
Epoch: [16][230/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.4085e+00 (1.3445e+00)	Acc@1  60.16 ( 61.42)	Acc@5  85.94 ( 88.74)
Epoch: [16][240/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4206e+00 (1.3432e+00)	Acc@1  58.59 ( 61.49)	Acc@5  85.16 ( 88.75)
Epoch: [16][250/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3495e+00 (1.3442e+00)	Acc@1  63.28 ( 61.50)	Acc@5  89.06 ( 88.69)
Epoch: [16][260/391]	Time  0.037 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.4299e+00 (1.3446e+00)	Acc@1  56.25 ( 61.46)	Acc@5  87.50 ( 88.67)
Epoch: [16][270/391]	Time  0.039 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.3937e+00 (1.3445e+00)	Acc@1  63.28 ( 61.45)	Acc@5  82.81 ( 88.68)
Epoch: [16][280/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2405e+00 (1.3440e+00)	Acc@1  65.62 ( 61.45)	Acc@5  89.84 ( 88.66)
Epoch: [16][290/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2325e+00 (1.3434e+00)	Acc@1  67.97 ( 61.49)	Acc@5  89.84 ( 88.68)
Epoch: [16][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2910e+00 (1.3470e+00)	Acc@1  63.28 ( 61.41)	Acc@5  87.50 ( 88.63)
Epoch: [16][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2086e+00 (1.3493e+00)	Acc@1  62.50 ( 61.32)	Acc@5  89.06 ( 88.60)
Epoch: [16][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7007e+00 (1.3512e+00)	Acc@1  50.78 ( 61.29)	Acc@5  79.69 ( 88.57)
Epoch: [16][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4578e+00 (1.3504e+00)	Acc@1  58.59 ( 61.26)	Acc@5  87.50 ( 88.60)
Epoch: [16][340/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4464e+00 (1.3511e+00)	Acc@1  57.03 ( 61.22)	Acc@5  89.06 ( 88.59)
Epoch: [16][350/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3861e+00 (1.3519e+00)	Acc@1  64.84 ( 61.21)	Acc@5  88.28 ( 88.57)
Epoch: [16][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.7088e+00 (1.3539e+00)	Acc@1  45.31 ( 61.13)	Acc@5  83.59 ( 88.56)
Epoch: [16][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4573e+00 (1.3551e+00)	Acc@1  60.16 ( 61.12)	Acc@5  86.72 ( 88.52)
Epoch: [16][380/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.001)	Loss 1.3354e+00 (1.3547e+00)	Acc@1  57.81 ( 61.14)	Acc@5  90.62 ( 88.52)
Epoch: [16][390/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4717e+00 (1.3558e+00)	Acc@1  66.25 ( 61.14)	Acc@5  82.50 ( 88.51)
## e[16] optimizer.zero_grad (sum) time: 0.26268649101257324
## e[16]       loss.backward (sum) time: 3.9773800373077393
## e[16]      optimizer.step (sum) time: 1.845932960510254
## epoch[16] training(only) time: 16.18395185470581
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.7145e+00 (1.7145e+00)	Acc@1  53.00 ( 53.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.8230e+00 (1.7692e+00)	Acc@1  51.00 ( 53.00)	Acc@5  84.00 ( 82.55)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 1.5427e+00 (1.7506e+00)	Acc@1  53.00 ( 53.14)	Acc@5  86.00 ( 83.00)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 2.0017e+00 (1.7641e+00)	Acc@1  45.00 ( 53.29)	Acc@5  81.00 ( 82.39)
Test: [ 40/100]	Time  0.020 ( 0.025)	Loss 1.4084e+00 (1.7394e+00)	Acc@1  58.00 ( 53.85)	Acc@5  86.00 ( 82.56)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.5692e+00 (1.7530e+00)	Acc@1  56.00 ( 53.33)	Acc@5  87.00 ( 82.18)
Test: [ 60/100]	Time  0.019 ( 0.023)	Loss 1.7400e+00 (1.7361e+00)	Acc@1  52.00 ( 53.64)	Acc@5  84.00 ( 82.61)
Test: [ 70/100]	Time  0.021 ( 0.023)	Loss 1.6978e+00 (1.7391e+00)	Acc@1  59.00 ( 53.63)	Acc@5  83.00 ( 82.54)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 1.7337e+00 (1.7488e+00)	Acc@1  58.00 ( 53.44)	Acc@5  86.00 ( 82.44)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.8419e+00 (1.7369e+00)	Acc@1  53.00 ( 53.77)	Acc@5  80.00 ( 82.58)
 * Acc@1 53.920 Acc@5 82.720
### epoch[16] execution time: 18.51014542579651
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.198 ( 0.198)	Data  0.149 ( 0.149)	Loss 1.3733e+00 (1.3733e+00)	Acc@1  60.94 ( 60.94)	Acc@5  85.94 ( 85.94)
Epoch: [17][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.014)	Loss 1.5034e+00 (1.3311e+00)	Acc@1  54.69 ( 60.37)	Acc@5  87.50 ( 88.64)
Epoch: [17][ 20/391]	Time  0.045 ( 0.050)	Data  0.001 ( 0.008)	Loss 1.1736e+00 (1.2900e+00)	Acc@1  67.19 ( 62.09)	Acc@5  89.06 ( 89.84)
Epoch: [17][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.3140e+00 (1.3119e+00)	Acc@1  63.28 ( 61.21)	Acc@5  86.72 ( 89.67)
Epoch: [17][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.3930e+00 (1.3206e+00)	Acc@1  58.59 ( 60.99)	Acc@5  84.38 ( 89.42)
Epoch: [17][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.5200e+00 (1.3222e+00)	Acc@1  63.28 ( 61.37)	Acc@5  82.03 ( 89.26)
Epoch: [17][ 60/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.2630e+00 (1.3159e+00)	Acc@1  64.06 ( 61.54)	Acc@5  91.41 ( 89.37)
Epoch: [17][ 70/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.6255e+00 (1.3136e+00)	Acc@1  52.34 ( 61.72)	Acc@5  84.38 ( 89.36)
Epoch: [17][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3748e+00 (1.3142e+00)	Acc@1  57.81 ( 61.70)	Acc@5  91.41 ( 89.43)
Epoch: [17][ 90/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3984e+00 (1.3205e+00)	Acc@1  64.06 ( 61.59)	Acc@5  86.72 ( 89.38)
Epoch: [17][100/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.2123e+00 (1.3244e+00)	Acc@1  61.72 ( 61.44)	Acc@5  92.97 ( 89.43)
Epoch: [17][110/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.3263e+00 (1.3211e+00)	Acc@1  57.81 ( 61.56)	Acc@5  88.28 ( 89.41)
Epoch: [17][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0615e+00 (1.3167e+00)	Acc@1  76.56 ( 61.83)	Acc@5  94.53 ( 89.49)
Epoch: [17][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1604e+00 (1.3164e+00)	Acc@1  66.41 ( 61.77)	Acc@5  90.62 ( 89.44)
Epoch: [17][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4349e+00 (1.3179e+00)	Acc@1  60.94 ( 61.69)	Acc@5  88.28 ( 89.37)
Epoch: [17][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4719e+00 (1.3192e+00)	Acc@1  56.25 ( 61.69)	Acc@5  87.50 ( 89.26)
Epoch: [17][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2631e+00 (1.3235e+00)	Acc@1  64.84 ( 61.67)	Acc@5  86.72 ( 89.22)
Epoch: [17][170/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1982e+00 (1.3250e+00)	Acc@1  64.06 ( 61.59)	Acc@5  91.41 ( 89.15)
Epoch: [17][180/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4497e+00 (1.3315e+00)	Acc@1  57.81 ( 61.46)	Acc@5  86.72 ( 89.06)
Epoch: [17][190/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1927e+00 (1.3281e+00)	Acc@1  62.50 ( 61.56)	Acc@5  92.97 ( 89.12)
Epoch: [17][200/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4855e+00 (1.3313e+00)	Acc@1  59.38 ( 61.50)	Acc@5  85.16 ( 89.04)
Epoch: [17][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2527e+00 (1.3308e+00)	Acc@1  60.16 ( 61.50)	Acc@5  89.84 ( 89.06)
Epoch: [17][220/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3812e+00 (1.3288e+00)	Acc@1  61.72 ( 61.58)	Acc@5  87.50 ( 89.08)
Epoch: [17][230/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2688e+00 (1.3315e+00)	Acc@1  60.94 ( 61.45)	Acc@5  89.06 ( 89.01)
Epoch: [17][240/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2693e+00 (1.3305e+00)	Acc@1  61.72 ( 61.54)	Acc@5  89.84 ( 89.00)
Epoch: [17][250/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.3877e+00 (1.3320e+00)	Acc@1  54.69 ( 61.50)	Acc@5  91.41 ( 89.00)
Epoch: [17][260/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3148e+00 (1.3347e+00)	Acc@1  64.84 ( 61.45)	Acc@5  89.06 ( 88.98)
Epoch: [17][270/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3362e+00 (1.3318e+00)	Acc@1  60.16 ( 61.49)	Acc@5  89.84 ( 89.04)
Epoch: [17][280/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4860e+00 (1.3313e+00)	Acc@1  64.06 ( 61.54)	Acc@5  87.50 ( 89.04)
Epoch: [17][290/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3387e+00 (1.3322e+00)	Acc@1  59.38 ( 61.53)	Acc@5  89.84 ( 89.00)
Epoch: [17][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5208e+00 (1.3345e+00)	Acc@1  55.47 ( 61.44)	Acc@5  89.06 ( 88.96)
Epoch: [17][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2640e+00 (1.3337e+00)	Acc@1  59.38 ( 61.42)	Acc@5  89.06 ( 88.97)
Epoch: [17][320/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3228e+00 (1.3346e+00)	Acc@1  62.50 ( 61.47)	Acc@5  85.16 ( 88.93)
Epoch: [17][330/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2388e+00 (1.3346e+00)	Acc@1  64.06 ( 61.45)	Acc@5  89.84 ( 88.94)
Epoch: [17][340/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 1.0935e+00 (1.3338e+00)	Acc@1  65.62 ( 61.44)	Acc@5  92.19 ( 88.94)
Epoch: [17][350/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 1.6329e+00 (1.3338e+00)	Acc@1  59.38 ( 61.47)	Acc@5  82.03 ( 88.93)
Epoch: [17][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5040e+00 (1.3357e+00)	Acc@1  58.59 ( 61.41)	Acc@5  86.72 ( 88.93)
Epoch: [17][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.5580e+00 (1.3353e+00)	Acc@1  53.91 ( 61.39)	Acc@5  87.50 ( 88.97)
Epoch: [17][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3029e+00 (1.3352e+00)	Acc@1  66.41 ( 61.41)	Acc@5  89.84 ( 88.99)
Epoch: [17][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1179e+00 (1.3327e+00)	Acc@1  61.25 ( 61.47)	Acc@5  91.25 ( 89.06)
## e[17] optimizer.zero_grad (sum) time: 0.2654249668121338
## e[17]       loss.backward (sum) time: 4.078911781311035
## e[17]      optimizer.step (sum) time: 1.8122572898864746
## epoch[17] training(only) time: 16.24657130241394
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.7529e+00 (1.7529e+00)	Acc@1  54.00 ( 54.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 1.8372e+00 (1.7086e+00)	Acc@1  47.00 ( 55.00)	Acc@5  88.00 ( 83.36)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 1.5019e+00 (1.6615e+00)	Acc@1  60.00 ( 55.67)	Acc@5  87.00 ( 84.05)
Test: [ 30/100]	Time  0.024 ( 0.028)	Loss 1.7702e+00 (1.6738e+00)	Acc@1  54.00 ( 56.10)	Acc@5  80.00 ( 83.55)
Test: [ 40/100]	Time  0.024 ( 0.026)	Loss 1.6475e+00 (1.6632e+00)	Acc@1  58.00 ( 55.80)	Acc@5  90.00 ( 84.20)
Test: [ 50/100]	Time  0.024 ( 0.026)	Loss 1.4388e+00 (1.6679e+00)	Acc@1  64.00 ( 55.63)	Acc@5  89.00 ( 84.06)
Test: [ 60/100]	Time  0.020 ( 0.025)	Loss 1.7098e+00 (1.6533e+00)	Acc@1  55.00 ( 55.72)	Acc@5  83.00 ( 84.20)
Test: [ 70/100]	Time  0.021 ( 0.025)	Loss 1.5971e+00 (1.6574e+00)	Acc@1  55.00 ( 55.72)	Acc@5  86.00 ( 84.06)
Test: [ 80/100]	Time  0.018 ( 0.024)	Loss 1.7568e+00 (1.6687e+00)	Acc@1  51.00 ( 55.33)	Acc@5  85.00 ( 83.84)
Test: [ 90/100]	Time  0.019 ( 0.024)	Loss 1.6852e+00 (1.6583e+00)	Acc@1  58.00 ( 55.45)	Acc@5  81.00 ( 83.91)
 * Acc@1 55.650 Acc@5 84.040
### epoch[17] execution time: 18.665016412734985
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.200 ( 0.200)	Data  0.149 ( 0.149)	Loss 9.5046e-01 (9.5046e-01)	Acc@1  73.44 ( 73.44)	Acc@5  93.75 ( 93.75)
Epoch: [18][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.3820e+00 (1.2490e+00)	Acc@1  62.50 ( 65.13)	Acc@5  86.72 ( 90.13)
Epoch: [18][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.4156e+00 (1.2088e+00)	Acc@1  60.16 ( 66.00)	Acc@5  88.28 ( 90.81)
Epoch: [18][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.2517e+00 (1.2095e+00)	Acc@1  64.06 ( 65.95)	Acc@5  89.06 ( 90.65)
Epoch: [18][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.1939e+00 (1.2230e+00)	Acc@1  69.53 ( 65.19)	Acc@5  93.75 ( 90.55)
Epoch: [18][ 50/391]	Time  0.046 ( 0.044)	Data  0.002 ( 0.004)	Loss 1.0974e+00 (1.2253e+00)	Acc@1  69.53 ( 64.80)	Acc@5  94.53 ( 90.75)
Epoch: [18][ 60/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.0532e+00 (1.2174e+00)	Acc@1  74.22 ( 64.98)	Acc@5  92.97 ( 90.86)
Epoch: [18][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5085e+00 (1.2337e+00)	Acc@1  48.44 ( 64.35)	Acc@5  87.50 ( 90.59)
Epoch: [18][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2718e+00 (1.2398e+00)	Acc@1  63.28 ( 64.35)	Acc@5  92.19 ( 90.41)
Epoch: [18][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3775e+00 (1.2460e+00)	Acc@1  63.28 ( 64.04)	Acc@5  89.06 ( 90.34)
Epoch: [18][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.9205e-01 (1.2502e+00)	Acc@1  71.09 ( 64.01)	Acc@5  93.75 ( 90.28)
Epoch: [18][110/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2339e+00 (1.2513e+00)	Acc@1  60.94 ( 63.98)	Acc@5  89.84 ( 90.30)
Epoch: [18][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3727e+00 (1.2594e+00)	Acc@1  60.16 ( 63.67)	Acc@5  89.06 ( 90.13)
Epoch: [18][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2286e+00 (1.2638e+00)	Acc@1  63.28 ( 63.54)	Acc@5  91.41 ( 90.08)
Epoch: [18][140/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5347e+00 (1.2647e+00)	Acc@1  60.94 ( 63.58)	Acc@5  82.81 ( 90.05)
Epoch: [18][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.5525e+00 (1.2646e+00)	Acc@1  55.47 ( 63.59)	Acc@5  85.94 ( 90.05)
Epoch: [18][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0431e+00 (1.2632e+00)	Acc@1  71.09 ( 63.53)	Acc@5  92.19 ( 90.07)
Epoch: [18][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4925e+00 (1.2705e+00)	Acc@1  60.94 ( 63.40)	Acc@5  86.72 ( 89.96)
Epoch: [18][180/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3608e+00 (1.2733e+00)	Acc@1  61.72 ( 63.33)	Acc@5  89.06 ( 89.92)
Epoch: [18][190/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3348e+00 (1.2754e+00)	Acc@1  58.59 ( 63.31)	Acc@5  88.28 ( 89.85)
Epoch: [18][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3708e+00 (1.2787e+00)	Acc@1  63.28 ( 63.27)	Acc@5  85.16 ( 89.77)
Epoch: [18][210/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.3191e+00 (1.2817e+00)	Acc@1  61.72 ( 63.20)	Acc@5  86.72 ( 89.71)
Epoch: [18][220/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3836e+00 (1.2842e+00)	Acc@1  60.16 ( 63.13)	Acc@5  85.16 ( 89.69)
Epoch: [18][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4499e+00 (1.2868e+00)	Acc@1  57.81 ( 63.06)	Acc@5  84.38 ( 89.65)
Epoch: [18][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2683e+00 (1.2874e+00)	Acc@1  60.94 ( 63.05)	Acc@5  91.41 ( 89.65)
Epoch: [18][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2687e+00 (1.2874e+00)	Acc@1  65.62 ( 63.09)	Acc@5  92.19 ( 89.66)
Epoch: [18][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1959e+00 (1.2859e+00)	Acc@1  63.28 ( 63.13)	Acc@5  90.62 ( 89.71)
Epoch: [18][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3256e+00 (1.2876e+00)	Acc@1  59.38 ( 63.04)	Acc@5  86.72 ( 89.66)
Epoch: [18][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3094e+00 (1.2879e+00)	Acc@1  57.03 ( 62.97)	Acc@5  89.06 ( 89.67)
Epoch: [18][290/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.2760e+00 (1.2897e+00)	Acc@1  60.94 ( 62.93)	Acc@5  89.06 ( 89.64)
Epoch: [18][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0625e+00 (1.2888e+00)	Acc@1  70.31 ( 62.92)	Acc@5  93.75 ( 89.66)
Epoch: [18][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2983e+00 (1.2891e+00)	Acc@1  67.97 ( 62.94)	Acc@5  85.94 ( 89.66)
Epoch: [18][320/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2206e+00 (1.2876e+00)	Acc@1  62.50 ( 62.99)	Acc@5  89.84 ( 89.66)
Epoch: [18][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3370e+00 (1.2882e+00)	Acc@1  62.50 ( 62.97)	Acc@5  88.28 ( 89.65)
Epoch: [18][340/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.4651e+00 (1.2884e+00)	Acc@1  51.56 ( 62.89)	Acc@5  90.62 ( 89.66)
Epoch: [18][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4577e+00 (1.2882e+00)	Acc@1  59.38 ( 62.93)	Acc@5  86.72 ( 89.66)
Epoch: [18][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4485e+00 (1.2899e+00)	Acc@1  57.81 ( 62.87)	Acc@5  92.19 ( 89.66)
Epoch: [18][370/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5240e+00 (1.2905e+00)	Acc@1  57.03 ( 62.82)	Acc@5  83.59 ( 89.66)
Epoch: [18][380/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.6106e+00 (1.2928e+00)	Acc@1  53.12 ( 62.76)	Acc@5  85.16 ( 89.63)
Epoch: [18][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.6727e+00 (1.2948e+00)	Acc@1  51.25 ( 62.64)	Acc@5  90.00 ( 89.62)
## e[18] optimizer.zero_grad (sum) time: 0.2653634548187256
## e[18]       loss.backward (sum) time: 4.026981592178345
## e[18]      optimizer.step (sum) time: 1.8108787536621094
## epoch[18] training(only) time: 16.092872619628906
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.6113e+00 (1.6113e+00)	Acc@1  55.00 ( 55.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 1.7743e+00 (1.6688e+00)	Acc@1  50.00 ( 56.18)	Acc@5  89.00 ( 84.45)
Test: [ 20/100]	Time  0.025 ( 0.028)	Loss 1.4071e+00 (1.6273e+00)	Acc@1  59.00 ( 57.43)	Acc@5  86.00 ( 84.67)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 1.7328e+00 (1.6516e+00)	Acc@1  53.00 ( 56.77)	Acc@5  86.00 ( 84.52)
Test: [ 40/100]	Time  0.024 ( 0.026)	Loss 1.4867e+00 (1.6429e+00)	Acc@1  60.00 ( 57.00)	Acc@5  87.00 ( 84.66)
Test: [ 50/100]	Time  0.020 ( 0.025)	Loss 1.4304e+00 (1.6519e+00)	Acc@1  62.00 ( 56.65)	Acc@5  89.00 ( 84.39)
Test: [ 60/100]	Time  0.017 ( 0.024)	Loss 1.6124e+00 (1.6350e+00)	Acc@1  51.00 ( 56.59)	Acc@5  83.00 ( 84.69)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 1.6781e+00 (1.6353e+00)	Acc@1  58.00 ( 56.63)	Acc@5  85.00 ( 84.66)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 1.8170e+00 (1.6446e+00)	Acc@1  55.00 ( 56.37)	Acc@5  82.00 ( 84.60)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 1.6434e+00 (1.6325e+00)	Acc@1  54.00 ( 56.65)	Acc@5  85.00 ( 84.75)
 * Acc@1 56.820 Acc@5 84.990
### epoch[18] execution time: 18.429439067840576
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.208 ( 0.208)	Data  0.150 ( 0.150)	Loss 1.2431e+00 (1.2431e+00)	Acc@1  66.41 ( 66.41)	Acc@5  90.62 ( 90.62)
Epoch: [19][ 10/391]	Time  0.045 ( 0.058)	Data  0.001 ( 0.015)	Loss 1.3761e+00 (1.2193e+00)	Acc@1  64.84 ( 64.77)	Acc@5  88.28 ( 90.48)
Epoch: [19][ 20/391]	Time  0.042 ( 0.050)	Data  0.001 ( 0.008)	Loss 1.1823e+00 (1.2144e+00)	Acc@1  60.94 ( 65.40)	Acc@5  87.50 ( 90.10)
Epoch: [19][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.006)	Loss 1.1174e+00 (1.2094e+00)	Acc@1  67.19 ( 65.32)	Acc@5  95.31 ( 90.50)
Epoch: [19][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.2290e+00 (1.2216e+00)	Acc@1  64.06 ( 64.86)	Acc@5  92.97 ( 90.36)
Epoch: [19][ 50/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.004)	Loss 1.1193e+00 (1.2202e+00)	Acc@1  70.31 ( 65.04)	Acc@5  89.84 ( 90.38)
Epoch: [19][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.2648e+00 (1.2205e+00)	Acc@1  61.72 ( 64.86)	Acc@5  91.41 ( 90.50)
Epoch: [19][ 70/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.3363e+00 (1.2111e+00)	Acc@1  66.41 ( 65.06)	Acc@5  89.06 ( 90.66)
Epoch: [19][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2389e+00 (1.2157e+00)	Acc@1  62.50 ( 65.08)	Acc@5  90.62 ( 90.63)
Epoch: [19][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5056e+00 (1.2309e+00)	Acc@1  60.94 ( 64.81)	Acc@5  81.25 ( 90.38)
Epoch: [19][100/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3659e+00 (1.2371e+00)	Acc@1  58.59 ( 64.60)	Acc@5  89.06 ( 90.28)
Epoch: [19][110/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.002)	Loss 1.1685e+00 (1.2417e+00)	Acc@1  70.31 ( 64.48)	Acc@5  86.72 ( 90.20)
Epoch: [19][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1935e+00 (1.2487e+00)	Acc@1  64.06 ( 64.19)	Acc@5  96.09 ( 90.11)
Epoch: [19][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3822e+00 (1.2459e+00)	Acc@1  56.25 ( 64.22)	Acc@5  90.62 ( 90.13)
Epoch: [19][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3940e+00 (1.2457e+00)	Acc@1  64.84 ( 64.28)	Acc@5  90.62 ( 90.16)
Epoch: [19][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3662e+00 (1.2511e+00)	Acc@1  65.62 ( 64.17)	Acc@5  85.94 ( 90.08)
Epoch: [19][160/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1552e+00 (1.2528e+00)	Acc@1  64.06 ( 64.07)	Acc@5  92.97 ( 90.14)
Epoch: [19][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2881e+00 (1.2536e+00)	Acc@1  61.72 ( 64.03)	Acc@5  89.84 ( 90.11)
Epoch: [19][180/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4703e+00 (1.2582e+00)	Acc@1  59.38 ( 63.93)	Acc@5  87.50 ( 90.06)
Epoch: [19][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3869e+00 (1.2644e+00)	Acc@1  62.50 ( 63.84)	Acc@5  90.62 ( 89.98)
Epoch: [19][200/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0977e+00 (1.2670e+00)	Acc@1  69.53 ( 63.78)	Acc@5  92.19 ( 89.94)
Epoch: [19][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1447e+00 (1.2663e+00)	Acc@1  65.62 ( 63.85)	Acc@5  89.84 ( 89.90)
Epoch: [19][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.6467e+00 (1.2683e+00)	Acc@1  52.34 ( 63.74)	Acc@5  83.59 ( 89.85)
Epoch: [19][230/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2546e+00 (1.2681e+00)	Acc@1  66.41 ( 63.75)	Acc@5  85.94 ( 89.83)
Epoch: [19][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3938e+00 (1.2709e+00)	Acc@1  59.38 ( 63.61)	Acc@5  84.38 ( 89.80)
Epoch: [19][250/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5898e+00 (1.2709e+00)	Acc@1  54.69 ( 63.64)	Acc@5  86.72 ( 89.78)
Epoch: [19][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3313e+00 (1.2700e+00)	Acc@1  65.62 ( 63.61)	Acc@5  89.84 ( 89.81)
Epoch: [19][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4059e+00 (1.2721e+00)	Acc@1  56.25 ( 63.56)	Acc@5  86.72 ( 89.77)
Epoch: [19][280/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.3007e+00 (1.2727e+00)	Acc@1  63.28 ( 63.56)	Acc@5  91.41 ( 89.76)
Epoch: [19][290/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.3072e+00 (1.2725e+00)	Acc@1  63.28 ( 63.55)	Acc@5  89.84 ( 89.77)
Epoch: [19][300/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.4829e+00 (1.2749e+00)	Acc@1  59.38 ( 63.47)	Acc@5  87.50 ( 89.72)
Epoch: [19][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2079e+00 (1.2743e+00)	Acc@1  64.84 ( 63.52)	Acc@5  91.41 ( 89.74)
Epoch: [19][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2810e+00 (1.2746e+00)	Acc@1  62.50 ( 63.54)	Acc@5  89.84 ( 89.74)
Epoch: [19][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2380e+00 (1.2720e+00)	Acc@1  58.59 ( 63.58)	Acc@5  90.62 ( 89.81)
Epoch: [19][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1496e+00 (1.2718e+00)	Acc@1  69.53 ( 63.56)	Acc@5  87.50 ( 89.79)
Epoch: [19][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2656e+00 (1.2739e+00)	Acc@1  64.06 ( 63.49)	Acc@5  89.06 ( 89.76)
Epoch: [19][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2100e+00 (1.2743e+00)	Acc@1  64.06 ( 63.49)	Acc@5  91.41 ( 89.73)
Epoch: [19][370/391]	Time  0.047 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4521e+00 (1.2748e+00)	Acc@1  60.16 ( 63.45)	Acc@5  85.94 ( 89.75)
Epoch: [19][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2712e+00 (1.2740e+00)	Acc@1  60.94 ( 63.43)	Acc@5  89.06 ( 89.75)
Epoch: [19][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2640e+00 (1.2768e+00)	Acc@1  60.00 ( 63.36)	Acc@5  92.50 ( 89.70)
## e[19] optimizer.zero_grad (sum) time: 0.2648506164550781
## e[19]       loss.backward (sum) time: 4.024561882019043
## e[19]      optimizer.step (sum) time: 1.8402767181396484
## epoch[19] training(only) time: 16.20828151702881
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.5986e+00 (1.5986e+00)	Acc@1  56.00 ( 56.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.8016e+00 (1.7209e+00)	Acc@1  50.00 ( 55.27)	Acc@5  86.00 ( 83.09)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.5639e+00 (1.6851e+00)	Acc@1  53.00 ( 55.19)	Acc@5  87.00 ( 83.95)
Test: [ 30/100]	Time  0.019 ( 0.026)	Loss 1.7813e+00 (1.6868e+00)	Acc@1  51.00 ( 55.19)	Acc@5  87.00 ( 83.74)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.8140e+00 (1.6930e+00)	Acc@1  57.00 ( 55.20)	Acc@5  85.00 ( 83.73)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 1.5308e+00 (1.7012e+00)	Acc@1  60.00 ( 55.20)	Acc@5  88.00 ( 83.67)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 1.8607e+00 (1.6879e+00)	Acc@1  51.00 ( 55.18)	Acc@5  82.00 ( 83.90)
Test: [ 70/100]	Time  0.017 ( 0.024)	Loss 1.4344e+00 (1.6868e+00)	Acc@1  61.00 ( 55.24)	Acc@5  87.00 ( 83.93)
Test: [ 80/100]	Time  0.025 ( 0.023)	Loss 1.9088e+00 (1.7010e+00)	Acc@1  56.00 ( 54.94)	Acc@5  81.00 ( 83.81)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.8700e+00 (1.6912e+00)	Acc@1  53.00 ( 55.16)	Acc@5  85.00 ( 84.02)
 * Acc@1 55.250 Acc@5 83.980
### epoch[19] execution time: 18.562371253967285
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.206 ( 0.206)	Data  0.152 ( 0.152)	Loss 1.2883e+00 (1.2883e+00)	Acc@1  60.94 ( 60.94)	Acc@5  90.62 ( 90.62)
Epoch: [20][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.015)	Loss 1.1087e+00 (1.2380e+00)	Acc@1  69.53 ( 64.49)	Acc@5  89.84 ( 90.34)
Epoch: [20][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.008)	Loss 1.4699e+00 (1.2430e+00)	Acc@1  54.69 ( 63.95)	Acc@5  89.84 ( 90.51)
Epoch: [20][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.006)	Loss 1.2877e+00 (1.2244e+00)	Acc@1  65.62 ( 64.47)	Acc@5  86.72 ( 90.57)
Epoch: [20][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.4279e+00 (1.2176e+00)	Acc@1  63.28 ( 64.98)	Acc@5  84.38 ( 90.53)
Epoch: [20][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.2176e+00 (1.2191e+00)	Acc@1  63.28 ( 64.77)	Acc@5  88.28 ( 90.81)
Epoch: [20][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.1428e+00 (1.2214e+00)	Acc@1  63.28 ( 64.52)	Acc@5  94.53 ( 90.78)
Epoch: [20][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2978e+00 (1.2264e+00)	Acc@1  60.16 ( 64.30)	Acc@5  92.19 ( 90.76)
Epoch: [20][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1507e+00 (1.2320e+00)	Acc@1  64.06 ( 64.39)	Acc@5  89.06 ( 90.48)
Epoch: [20][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1665e+00 (1.2310e+00)	Acc@1  66.41 ( 64.46)	Acc@5  93.75 ( 90.58)
Epoch: [20][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2379e+00 (1.2407e+00)	Acc@1  64.06 ( 64.26)	Acc@5  85.94 ( 90.41)
Epoch: [20][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4332e+00 (1.2398e+00)	Acc@1  61.72 ( 64.15)	Acc@5  83.59 ( 90.27)
Epoch: [20][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0987e+00 (1.2420e+00)	Acc@1  70.31 ( 64.09)	Acc@5  94.53 ( 90.25)
Epoch: [20][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2857e+00 (1.2436e+00)	Acc@1  61.72 ( 64.14)	Acc@5  89.06 ( 90.14)
Epoch: [20][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2974e+00 (1.2426e+00)	Acc@1  61.72 ( 64.21)	Acc@5  90.62 ( 90.18)
Epoch: [20][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3122e+00 (1.2446e+00)	Acc@1  57.81 ( 64.11)	Acc@5  88.28 ( 90.21)
Epoch: [20][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5509e+00 (1.2466e+00)	Acc@1  52.34 ( 63.98)	Acc@5  82.81 ( 90.15)
Epoch: [20][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2501e+00 (1.2462e+00)	Acc@1  64.84 ( 63.99)	Acc@5  87.50 ( 90.11)
Epoch: [20][180/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3057e+00 (1.2469e+00)	Acc@1  62.50 ( 63.98)	Acc@5  89.06 ( 90.12)
Epoch: [20][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2778e+00 (1.2461e+00)	Acc@1  63.28 ( 64.01)	Acc@5  87.50 ( 90.07)
Epoch: [20][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0112e+00 (1.2445e+00)	Acc@1  71.09 ( 64.05)	Acc@5  92.97 ( 90.08)
Epoch: [20][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2038e+00 (1.2466e+00)	Acc@1  63.28 ( 63.97)	Acc@5  88.28 ( 90.04)
Epoch: [20][220/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2774e+00 (1.2447e+00)	Acc@1  63.28 ( 63.97)	Acc@5  90.62 ( 90.14)
Epoch: [20][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2031e+00 (1.2458e+00)	Acc@1  64.84 ( 63.94)	Acc@5  89.06 ( 90.10)
Epoch: [20][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2946e+00 (1.2481e+00)	Acc@1  63.28 ( 63.94)	Acc@5  87.50 ( 90.05)
Epoch: [20][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3407e+00 (1.2475e+00)	Acc@1  61.72 ( 63.96)	Acc@5  89.84 ( 90.07)
Epoch: [20][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0680e+00 (1.2438e+00)	Acc@1  71.88 ( 64.04)	Acc@5  92.19 ( 90.16)
Epoch: [20][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0273e+00 (1.2437e+00)	Acc@1  66.41 ( 64.00)	Acc@5  92.19 ( 90.19)
Epoch: [20][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3679e+00 (1.2428e+00)	Acc@1  65.62 ( 64.06)	Acc@5  88.28 ( 90.18)
Epoch: [20][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3577e+00 (1.2437e+00)	Acc@1  57.03 ( 64.03)	Acc@5  89.06 ( 90.18)
Epoch: [20][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1464e+00 (1.2430e+00)	Acc@1  63.28 ( 63.99)	Acc@5  93.75 ( 90.20)
Epoch: [20][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1212e+00 (1.2422e+00)	Acc@1  63.28 ( 63.99)	Acc@5  91.41 ( 90.19)
Epoch: [20][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2835e+00 (1.2438e+00)	Acc@1  64.84 ( 63.93)	Acc@5  92.19 ( 90.19)
Epoch: [20][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2048e+00 (1.2449e+00)	Acc@1  65.62 ( 63.90)	Acc@5  91.41 ( 90.16)
Epoch: [20][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3267e+00 (1.2456e+00)	Acc@1  60.16 ( 63.84)	Acc@5  88.28 ( 90.16)
Epoch: [20][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.4088e+00 (1.2453e+00)	Acc@1  59.38 ( 63.88)	Acc@5  91.41 ( 90.15)
Epoch: [20][360/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.001)	Loss 8.9980e-01 (1.2443e+00)	Acc@1  78.91 ( 63.93)	Acc@5  92.97 ( 90.16)
Epoch: [20][370/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.001)	Loss 1.4562e+00 (1.2459e+00)	Acc@1  57.81 ( 63.89)	Acc@5  83.59 ( 90.12)
Epoch: [20][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1753e+00 (1.2457e+00)	Acc@1  67.97 ( 63.91)	Acc@5  90.62 ( 90.13)
Epoch: [20][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3594e+00 (1.2473e+00)	Acc@1  63.75 ( 63.90)	Acc@5  85.00 ( 90.11)
## e[20] optimizer.zero_grad (sum) time: 0.266326904296875
## e[20]       loss.backward (sum) time: 4.001494646072388
## e[20]      optimizer.step (sum) time: 1.9043478965759277
## epoch[20] training(only) time: 16.0566987991333
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.4891e+00 (1.4891e+00)	Acc@1  63.00 ( 63.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.7238e+00 (1.5760e+00)	Acc@1  53.00 ( 58.82)	Acc@5  90.00 ( 85.09)
Test: [ 20/100]	Time  0.026 ( 0.028)	Loss 1.5589e+00 (1.5511e+00)	Acc@1  63.00 ( 58.76)	Acc@5  85.00 ( 86.48)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.7558e+00 (1.5661e+00)	Acc@1  54.00 ( 58.26)	Acc@5  88.00 ( 85.84)
Test: [ 40/100]	Time  0.024 ( 0.026)	Loss 1.3541e+00 (1.5633e+00)	Acc@1  63.00 ( 58.39)	Acc@5  88.00 ( 85.68)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 1.4489e+00 (1.5765e+00)	Acc@1  62.00 ( 57.88)	Acc@5  83.00 ( 85.27)
Test: [ 60/100]	Time  0.018 ( 0.024)	Loss 1.7370e+00 (1.5664e+00)	Acc@1  57.00 ( 57.84)	Acc@5  83.00 ( 85.21)
Test: [ 70/100]	Time  0.017 ( 0.023)	Loss 1.5493e+00 (1.5586e+00)	Acc@1  56.00 ( 58.14)	Acc@5  89.00 ( 85.31)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 1.6410e+00 (1.5662e+00)	Acc@1  58.00 ( 57.94)	Acc@5  83.00 ( 85.14)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 1.7004e+00 (1.5618e+00)	Acc@1  54.00 ( 58.11)	Acc@5  83.00 ( 85.31)
 * Acc@1 58.190 Acc@5 85.490
### epoch[20] execution time: 18.37375807762146
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.200 ( 0.200)	Data  0.149 ( 0.149)	Loss 1.1628e+00 (1.1628e+00)	Acc@1  64.84 ( 64.84)	Acc@5  89.06 ( 89.06)
Epoch: [21][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.1282e+00 (1.2772e+00)	Acc@1  58.59 ( 62.36)	Acc@5  95.31 ( 90.27)
Epoch: [21][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.1996e+00 (1.2492e+00)	Acc@1  63.28 ( 63.58)	Acc@5  90.62 ( 90.40)
Epoch: [21][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.0340e+00 (1.2122e+00)	Acc@1  67.19 ( 64.72)	Acc@5  92.97 ( 90.62)
Epoch: [21][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.1736e+00 (1.1937e+00)	Acc@1  65.62 ( 65.19)	Acc@5  91.41 ( 90.97)
Epoch: [21][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0908e+00 (1.1795e+00)	Acc@1  67.97 ( 65.53)	Acc@5  92.19 ( 91.12)
Epoch: [21][ 60/391]	Time  0.041 ( 0.044)	Data  0.002 ( 0.003)	Loss 1.2125e+00 (1.1840e+00)	Acc@1  61.72 ( 65.46)	Acc@5  91.41 ( 91.10)
Epoch: [21][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3412e+00 (1.1776e+00)	Acc@1  63.28 ( 65.85)	Acc@5  89.84 ( 91.15)
Epoch: [21][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1830e+00 (1.1685e+00)	Acc@1  66.41 ( 66.11)	Acc@5  92.19 ( 91.32)
Epoch: [21][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1864e+00 (1.1706e+00)	Acc@1  67.19 ( 65.86)	Acc@5  92.97 ( 91.25)
Epoch: [21][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0815e+00 (1.1686e+00)	Acc@1  67.97 ( 65.91)	Acc@5  93.75 ( 91.17)
Epoch: [21][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3215e+00 (1.1778e+00)	Acc@1  59.38 ( 65.70)	Acc@5  86.72 ( 90.99)
Epoch: [21][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2418e+00 (1.1807e+00)	Acc@1  68.75 ( 65.63)	Acc@5  86.72 ( 90.90)
Epoch: [21][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0510e+00 (1.1821e+00)	Acc@1  65.62 ( 65.75)	Acc@5  96.09 ( 90.83)
Epoch: [21][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0705e+00 (1.1808e+00)	Acc@1  67.19 ( 65.79)	Acc@5  93.75 ( 90.85)
Epoch: [21][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.6441e-01 (1.1859e+00)	Acc@1  73.44 ( 65.73)	Acc@5  92.97 ( 90.87)
Epoch: [21][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2028e+00 (1.1904e+00)	Acc@1  70.31 ( 65.66)	Acc@5  92.19 ( 90.82)
Epoch: [21][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2093e+00 (1.1910e+00)	Acc@1  64.06 ( 65.68)	Acc@5  92.19 ( 90.79)
Epoch: [21][180/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1124e+00 (1.1933e+00)	Acc@1  71.09 ( 65.62)	Acc@5  93.75 ( 90.75)
Epoch: [21][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1532e+00 (1.1957e+00)	Acc@1  67.97 ( 65.54)	Acc@5  89.84 ( 90.67)
Epoch: [21][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3078e+00 (1.1975e+00)	Acc@1  63.28 ( 65.44)	Acc@5  87.50 ( 90.59)
Epoch: [21][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1028e+00 (1.2004e+00)	Acc@1  69.53 ( 65.36)	Acc@5  91.41 ( 90.60)
Epoch: [21][220/391]	Time  0.047 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2872e+00 (1.2021e+00)	Acc@1  60.16 ( 65.27)	Acc@5  90.62 ( 90.58)
Epoch: [21][230/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.2585e+00 (1.2023e+00)	Acc@1  67.19 ( 65.35)	Acc@5  88.28 ( 90.56)
Epoch: [21][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0681e+00 (1.2025e+00)	Acc@1  67.97 ( 65.36)	Acc@5  89.84 ( 90.56)
Epoch: [21][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0131e+00 (1.2012e+00)	Acc@1  65.62 ( 65.31)	Acc@5  94.53 ( 90.60)
Epoch: [21][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3569e+00 (1.2037e+00)	Acc@1  55.47 ( 65.22)	Acc@5  89.06 ( 90.55)
Epoch: [21][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2729e+00 (1.2057e+00)	Acc@1  58.59 ( 65.19)	Acc@5  90.62 ( 90.50)
Epoch: [21][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2219e+00 (1.2047e+00)	Acc@1  61.72 ( 65.20)	Acc@5  92.19 ( 90.53)
Epoch: [21][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3427e+00 (1.2075e+00)	Acc@1  64.84 ( 65.12)	Acc@5  87.50 ( 90.50)
Epoch: [21][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3749e+00 (1.2093e+00)	Acc@1  60.94 ( 65.07)	Acc@5  84.38 ( 90.45)
Epoch: [21][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1306e+00 (1.2105e+00)	Acc@1  67.97 ( 65.05)	Acc@5  90.62 ( 90.44)
Epoch: [21][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3233e+00 (1.2106e+00)	Acc@1  60.94 ( 65.05)	Acc@5  92.19 ( 90.44)
Epoch: [21][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1145e+00 (1.2120e+00)	Acc@1  69.53 ( 64.98)	Acc@5  91.41 ( 90.44)
Epoch: [21][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3407e+00 (1.2152e+00)	Acc@1  59.38 ( 64.86)	Acc@5  92.19 ( 90.41)
Epoch: [21][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1665e+00 (1.2158e+00)	Acc@1  67.19 ( 64.86)	Acc@5  89.84 ( 90.41)
Epoch: [21][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2849e+00 (1.2159e+00)	Acc@1  64.84 ( 64.86)	Acc@5  89.84 ( 90.43)
Epoch: [21][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1264e+00 (1.2174e+00)	Acc@1  67.97 ( 64.84)	Acc@5  92.19 ( 90.40)
Epoch: [21][380/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3116e+00 (1.2168e+00)	Acc@1  63.28 ( 64.88)	Acc@5  87.50 ( 90.40)
Epoch: [21][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0559e+00 (1.2176e+00)	Acc@1  71.25 ( 64.86)	Acc@5  92.50 ( 90.39)
## e[21] optimizer.zero_grad (sum) time: 0.26427698135375977
## e[21]       loss.backward (sum) time: 4.074612855911255
## e[21]      optimizer.step (sum) time: 1.795252799987793
## epoch[21] training(only) time: 16.12283706665039
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.6282e+00 (1.6282e+00)	Acc@1  56.00 ( 56.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 1.7736e+00 (1.6944e+00)	Acc@1  53.00 ( 57.82)	Acc@5  90.00 ( 85.45)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.5773e+00 (1.6472e+00)	Acc@1  56.00 ( 57.29)	Acc@5  84.00 ( 85.57)
Test: [ 30/100]	Time  0.020 ( 0.027)	Loss 1.8380e+00 (1.6718e+00)	Acc@1  52.00 ( 56.81)	Acc@5  82.00 ( 84.65)
Test: [ 40/100]	Time  0.017 ( 0.025)	Loss 1.4815e+00 (1.6821e+00)	Acc@1  56.00 ( 56.20)	Acc@5  88.00 ( 84.32)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 1.4877e+00 (1.6853e+00)	Acc@1  65.00 ( 56.33)	Acc@5  86.00 ( 83.94)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 1.5286e+00 (1.6697e+00)	Acc@1  57.00 ( 56.49)	Acc@5  86.00 ( 83.87)
Test: [ 70/100]	Time  0.020 ( 0.024)	Loss 1.6871e+00 (1.6670e+00)	Acc@1  49.00 ( 56.48)	Acc@5  84.00 ( 83.85)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 1.7849e+00 (1.6806e+00)	Acc@1  55.00 ( 56.04)	Acc@5  79.00 ( 83.53)
Test: [ 90/100]	Time  0.025 ( 0.023)	Loss 2.1051e+00 (1.6730e+00)	Acc@1  51.00 ( 56.18)	Acc@5  79.00 ( 83.64)
 * Acc@1 56.280 Acc@5 83.680
### epoch[21] execution time: 18.474658489227295
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.200 ( 0.200)	Data  0.147 ( 0.147)	Loss 1.2510e+00 (1.2510e+00)	Acc@1  66.41 ( 66.41)	Acc@5  89.06 ( 89.06)
Epoch: [22][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.0924e+00 (1.0994e+00)	Acc@1  67.97 ( 67.97)	Acc@5  92.19 ( 91.41)
Epoch: [22][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.0289e+00 (1.1200e+00)	Acc@1  71.88 ( 66.96)	Acc@5  91.41 ( 91.22)
Epoch: [22][ 30/391]	Time  0.040 ( 0.047)	Data  0.001 ( 0.006)	Loss 1.0857e+00 (1.1321e+00)	Acc@1  71.09 ( 67.04)	Acc@5  92.19 ( 91.23)
Epoch: [22][ 40/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.2462e+00 (1.1592e+00)	Acc@1  61.72 ( 66.41)	Acc@5  89.84 ( 90.85)
Epoch: [22][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0015e+00 (1.1484e+00)	Acc@1  72.66 ( 66.59)	Acc@5  93.75 ( 91.21)
Epoch: [22][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.2373e+00 (1.1520e+00)	Acc@1  66.41 ( 66.64)	Acc@5  86.72 ( 91.07)
Epoch: [22][ 70/391]	Time  0.038 ( 0.043)	Data  0.002 ( 0.003)	Loss 9.2958e-01 (1.1530e+00)	Acc@1  70.31 ( 66.65)	Acc@5  93.75 ( 91.11)
Epoch: [22][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.7597e-01 (1.1449e+00)	Acc@1  72.66 ( 66.90)	Acc@5  93.75 ( 91.19)
Epoch: [22][ 90/391]	Time  0.047 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0761e+00 (1.1423e+00)	Acc@1  67.19 ( 66.77)	Acc@5  92.97 ( 91.27)
Epoch: [22][100/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2228e+00 (1.1565e+00)	Acc@1  64.84 ( 66.46)	Acc@5  92.97 ( 91.15)
Epoch: [22][110/391]	Time  0.048 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4192e+00 (1.1629e+00)	Acc@1  57.81 ( 66.27)	Acc@5  89.84 ( 91.15)
Epoch: [22][120/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3006e+00 (1.1676e+00)	Acc@1  60.94 ( 66.14)	Acc@5  89.06 ( 91.12)
Epoch: [22][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0501e+00 (1.1681e+00)	Acc@1  66.41 ( 66.13)	Acc@5  91.41 ( 91.11)
Epoch: [22][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0796e+00 (1.1688e+00)	Acc@1  64.06 ( 66.06)	Acc@5  95.31 ( 91.12)
Epoch: [22][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1444e+00 (1.1664e+00)	Acc@1  63.28 ( 66.03)	Acc@5  92.19 ( 91.18)
Epoch: [22][160/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1801e+00 (1.1678e+00)	Acc@1  69.53 ( 65.97)	Acc@5  91.41 ( 91.15)
Epoch: [22][170/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1689e+00 (1.1720e+00)	Acc@1  68.75 ( 65.94)	Acc@5  91.41 ( 91.10)
Epoch: [22][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0372e+00 (1.1727e+00)	Acc@1  72.66 ( 65.91)	Acc@5  92.97 ( 91.10)
Epoch: [22][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2917e+00 (1.1751e+00)	Acc@1  60.16 ( 65.80)	Acc@5  89.06 ( 91.11)
Epoch: [22][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1198e+00 (1.1771e+00)	Acc@1  66.41 ( 65.67)	Acc@5  92.97 ( 91.10)
Epoch: [22][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3834e+00 (1.1778e+00)	Acc@1  58.59 ( 65.70)	Acc@5  87.50 ( 91.05)
Epoch: [22][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5585e+00 (1.1789e+00)	Acc@1  56.25 ( 65.67)	Acc@5  85.94 ( 91.05)
Epoch: [22][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1432e+00 (1.1810e+00)	Acc@1  69.53 ( 65.64)	Acc@5  89.06 ( 90.99)
Epoch: [22][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3145e+00 (1.1827e+00)	Acc@1  61.72 ( 65.56)	Acc@5  89.84 ( 91.01)
Epoch: [22][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3072e+00 (1.1829e+00)	Acc@1  60.94 ( 65.53)	Acc@5  90.62 ( 90.96)
Epoch: [22][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0280e+00 (1.1822e+00)	Acc@1  68.75 ( 65.55)	Acc@5  96.88 ( 90.99)
Epoch: [22][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0331e+00 (1.1830e+00)	Acc@1  67.97 ( 65.50)	Acc@5  92.19 ( 90.98)
Epoch: [22][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0443e+00 (1.1825e+00)	Acc@1  66.41 ( 65.56)	Acc@5  93.75 ( 90.99)
Epoch: [22][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0974e+00 (1.1831e+00)	Acc@1  67.97 ( 65.56)	Acc@5  91.41 ( 90.98)
Epoch: [22][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2310e+00 (1.1828e+00)	Acc@1  64.06 ( 65.58)	Acc@5  91.41 ( 90.98)
Epoch: [22][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3482e+00 (1.1837e+00)	Acc@1  61.72 ( 65.56)	Acc@5  87.50 ( 90.97)
Epoch: [22][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2378e+00 (1.1830e+00)	Acc@1  64.06 ( 65.59)	Acc@5  88.28 ( 90.97)
Epoch: [22][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3601e+00 (1.1842e+00)	Acc@1  63.28 ( 65.59)	Acc@5  87.50 ( 90.95)
Epoch: [22][340/391]	Time  0.054 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1410e+00 (1.1865e+00)	Acc@1  67.97 ( 65.52)	Acc@5  92.19 ( 90.93)
Epoch: [22][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2997e+00 (1.1899e+00)	Acc@1  62.50 ( 65.41)	Acc@5  90.62 ( 90.88)
Epoch: [22][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3289e+00 (1.1924e+00)	Acc@1  60.94 ( 65.35)	Acc@5  89.06 ( 90.87)
Epoch: [22][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0962e+00 (1.1931e+00)	Acc@1  67.19 ( 65.31)	Acc@5  92.97 ( 90.86)
Epoch: [22][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3123e+00 (1.1932e+00)	Acc@1  64.84 ( 65.32)	Acc@5  89.84 ( 90.85)
Epoch: [22][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2787e+00 (1.1927e+00)	Acc@1  66.25 ( 65.33)	Acc@5  91.25 ( 90.87)
## e[22] optimizer.zero_grad (sum) time: 0.2649204730987549
## e[22]       loss.backward (sum) time: 4.02164101600647
## e[22]      optimizer.step (sum) time: 1.840057373046875
## epoch[22] training(only) time: 16.1352059841156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.5335e+00 (1.5335e+00)	Acc@1  60.00 ( 60.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.019 ( 0.032)	Loss 1.7387e+00 (1.6500e+00)	Acc@1  55.00 ( 58.27)	Acc@5  87.00 ( 84.73)
Test: [ 20/100]	Time  0.021 ( 0.026)	Loss 1.3039e+00 (1.5880e+00)	Acc@1  66.00 ( 58.76)	Acc@5  91.00 ( 85.52)
Test: [ 30/100]	Time  0.018 ( 0.025)	Loss 1.8818e+00 (1.6334e+00)	Acc@1  49.00 ( 57.74)	Acc@5  85.00 ( 84.58)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 1.4327e+00 (1.6330e+00)	Acc@1  63.00 ( 57.90)	Acc@5  91.00 ( 84.73)
Test: [ 50/100]	Time  0.018 ( 0.023)	Loss 1.5335e+00 (1.6398e+00)	Acc@1  63.00 ( 57.51)	Acc@5  87.00 ( 84.59)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.6622e+00 (1.6232e+00)	Acc@1  58.00 ( 57.39)	Acc@5  84.00 ( 84.74)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.6604e+00 (1.6274e+00)	Acc@1  59.00 ( 57.28)	Acc@5  84.00 ( 84.61)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.7100e+00 (1.6334e+00)	Acc@1  60.00 ( 56.99)	Acc@5  81.00 ( 84.48)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 1.5946e+00 (1.6168e+00)	Acc@1  60.00 ( 57.24)	Acc@5  86.00 ( 84.67)
 * Acc@1 57.430 Acc@5 84.840
### epoch[22] execution time: 18.415918111801147
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.204 ( 0.204)	Data  0.150 ( 0.150)	Loss 1.4371e+00 (1.4371e+00)	Acc@1  57.03 ( 57.03)	Acc@5  87.50 ( 87.50)
Epoch: [23][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.015)	Loss 1.1109e+00 (1.1855e+00)	Acc@1  70.31 ( 66.48)	Acc@5  92.19 ( 90.48)
Epoch: [23][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.0670e+00 (1.1364e+00)	Acc@1  71.88 ( 66.89)	Acc@5  92.19 ( 91.41)
Epoch: [23][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.0209e+00 (1.1267e+00)	Acc@1  71.88 ( 67.19)	Acc@5  89.84 ( 91.36)
Epoch: [23][ 40/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.2626e+00 (1.1159e+00)	Acc@1  65.62 ( 67.51)	Acc@5  90.62 ( 91.86)
Epoch: [23][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0025e+00 (1.1190e+00)	Acc@1  66.41 ( 67.20)	Acc@5  94.53 ( 91.88)
Epoch: [23][ 60/391]	Time  0.045 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.0974e+00 (1.1297e+00)	Acc@1  64.06 ( 66.97)	Acc@5  92.19 ( 91.65)
Epoch: [23][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2123e+00 (1.1334e+00)	Acc@1  61.72 ( 66.71)	Acc@5  92.19 ( 91.64)
Epoch: [23][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0742e+00 (1.1374e+00)	Acc@1  62.50 ( 66.54)	Acc@5  92.19 ( 91.48)
Epoch: [23][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1509e+00 (1.1386e+00)	Acc@1  69.53 ( 66.57)	Acc@5  91.41 ( 91.46)
Epoch: [23][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1929e+00 (1.1402e+00)	Acc@1  64.84 ( 66.61)	Acc@5  89.84 ( 91.49)
Epoch: [23][110/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2254e+00 (1.1460e+00)	Acc@1  64.06 ( 66.32)	Acc@5  92.19 ( 91.47)
Epoch: [23][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4203e+00 (1.1503e+00)	Acc@1  55.47 ( 66.18)	Acc@5  89.84 ( 91.43)
Epoch: [23][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3143e+00 (1.1529e+00)	Acc@1  60.94 ( 66.04)	Acc@5  86.72 ( 91.38)
Epoch: [23][140/391]	Time  0.044 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.2631e+00 (1.1540e+00)	Acc@1  64.06 ( 65.98)	Acc@5  92.97 ( 91.37)
Epoch: [23][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1614e+00 (1.1582e+00)	Acc@1  63.28 ( 65.78)	Acc@5  91.41 ( 91.35)
Epoch: [23][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0102e+00 (1.1584e+00)	Acc@1  73.44 ( 65.85)	Acc@5  92.97 ( 91.33)
Epoch: [23][170/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0602e+00 (1.1578e+00)	Acc@1  67.19 ( 65.91)	Acc@5  91.41 ( 91.31)
Epoch: [23][180/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1901e+00 (1.1602e+00)	Acc@1  65.62 ( 65.87)	Acc@5  92.97 ( 91.28)
Epoch: [23][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1480e+00 (1.1629e+00)	Acc@1  65.62 ( 65.76)	Acc@5  91.41 ( 91.25)
Epoch: [23][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0503e+00 (1.1620e+00)	Acc@1  70.31 ( 65.87)	Acc@5  89.84 ( 91.26)
Epoch: [23][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4590e+00 (1.1669e+00)	Acc@1  58.59 ( 65.71)	Acc@5  86.72 ( 91.15)
Epoch: [23][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0217e+00 (1.1658e+00)	Acc@1  76.56 ( 65.82)	Acc@5  90.62 ( 91.16)
Epoch: [23][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4357e+00 (1.1682e+00)	Acc@1  55.47 ( 65.74)	Acc@5  87.50 ( 91.15)
Epoch: [23][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1329e+00 (1.1685e+00)	Acc@1  68.75 ( 65.73)	Acc@5  89.84 ( 91.10)
Epoch: [23][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2052e+00 (1.1676e+00)	Acc@1  66.41 ( 65.74)	Acc@5  89.84 ( 91.11)
Epoch: [23][260/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.0564e+00 (1.1688e+00)	Acc@1  63.28 ( 65.72)	Acc@5  91.41 ( 91.07)
Epoch: [23][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3370e+00 (1.1711e+00)	Acc@1  60.16 ( 65.66)	Acc@5  89.84 ( 91.01)
Epoch: [23][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1308e+00 (1.1731e+00)	Acc@1  63.28 ( 65.63)	Acc@5  93.75 ( 90.99)
Epoch: [23][290/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.3107e+00 (1.1763e+00)	Acc@1  60.16 ( 65.53)	Acc@5  88.28 ( 90.96)
Epoch: [23][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3452e+00 (1.1800e+00)	Acc@1  65.62 ( 65.46)	Acc@5  85.94 ( 90.88)
Epoch: [23][310/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4490e+00 (1.1811e+00)	Acc@1  59.38 ( 65.41)	Acc@5  89.06 ( 90.89)
Epoch: [23][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2237e+00 (1.1801e+00)	Acc@1  62.50 ( 65.43)	Acc@5  94.53 ( 90.92)
Epoch: [23][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2852e+00 (1.1827e+00)	Acc@1  60.16 ( 65.36)	Acc@5  90.62 ( 90.90)
Epoch: [23][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7442e-01 (1.1821e+00)	Acc@1  71.88 ( 65.40)	Acc@5  92.19 ( 90.92)
Epoch: [23][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1770e+00 (1.1833e+00)	Acc@1  63.28 ( 65.36)	Acc@5  90.62 ( 90.91)
Epoch: [23][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2879e+00 (1.1824e+00)	Acc@1  62.50 ( 65.42)	Acc@5  90.62 ( 90.92)
Epoch: [23][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0943e+00 (1.1831e+00)	Acc@1  67.97 ( 65.38)	Acc@5  92.19 ( 90.92)
Epoch: [23][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3133e+00 (1.1846e+00)	Acc@1  58.59 ( 65.37)	Acc@5  91.41 ( 90.89)
Epoch: [23][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0214e+00 (1.1855e+00)	Acc@1  71.25 ( 65.29)	Acc@5  90.00 ( 90.87)
## e[23] optimizer.zero_grad (sum) time: 0.26551008224487305
## e[23]       loss.backward (sum) time: 3.9675707817077637
## e[23]      optimizer.step (sum) time: 1.8641245365142822
## epoch[23] training(only) time: 16.103099822998047
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.7017e+00 (1.7017e+00)	Acc@1  58.00 ( 58.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 1.8101e+00 (1.8495e+00)	Acc@1  50.00 ( 53.18)	Acc@5  91.00 ( 81.82)
Test: [ 20/100]	Time  0.019 ( 0.028)	Loss 1.7210e+00 (1.8251e+00)	Acc@1  57.00 ( 53.86)	Acc@5  85.00 ( 82.48)
Test: [ 30/100]	Time  0.020 ( 0.026)	Loss 1.9467e+00 (1.8285e+00)	Acc@1  44.00 ( 53.65)	Acc@5  83.00 ( 82.23)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 1.5698e+00 (1.8110e+00)	Acc@1  53.00 ( 53.83)	Acc@5  84.00 ( 82.39)
Test: [ 50/100]	Time  0.021 ( 0.023)	Loss 1.8899e+00 (1.8244e+00)	Acc@1  51.00 ( 53.57)	Acc@5  81.00 ( 82.22)
Test: [ 60/100]	Time  0.024 ( 0.023)	Loss 1.7294e+00 (1.8142e+00)	Acc@1  59.00 ( 53.80)	Acc@5  81.00 ( 82.33)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 1.7530e+00 (1.8090e+00)	Acc@1  56.00 ( 54.01)	Acc@5  83.00 ( 82.55)
Test: [ 80/100]	Time  0.022 ( 0.022)	Loss 1.9944e+00 (1.8206e+00)	Acc@1  53.00 ( 53.80)	Acc@5  79.00 ( 82.36)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 2.0452e+00 (1.8166e+00)	Acc@1  51.00 ( 53.98)	Acc@5  78.00 ( 82.42)
 * Acc@1 54.310 Acc@5 82.420
### epoch[23] execution time: 18.333696365356445
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.201 ( 0.201)	Data  0.152 ( 0.152)	Loss 1.2293e+00 (1.2293e+00)	Acc@1  62.50 ( 62.50)	Acc@5  92.97 ( 92.97)
Epoch: [24][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.015)	Loss 1.2732e+00 (1.1155e+00)	Acc@1  57.81 ( 67.76)	Acc@5  92.19 ( 91.90)
Epoch: [24][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.008)	Loss 1.0837e+00 (1.1127e+00)	Acc@1  67.97 ( 67.19)	Acc@5  92.97 ( 92.08)
Epoch: [24][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.006)	Loss 1.2165e+00 (1.1058e+00)	Acc@1  68.75 ( 67.49)	Acc@5  90.62 ( 92.06)
Epoch: [24][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.1596e+00 (1.0994e+00)	Acc@1  68.75 ( 67.51)	Acc@5  88.28 ( 92.19)
Epoch: [24][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0773e+00 (1.1193e+00)	Acc@1  64.84 ( 66.68)	Acc@5  92.19 ( 92.00)
Epoch: [24][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.2701e+00 (1.1267e+00)	Acc@1  63.28 ( 66.55)	Acc@5  87.50 ( 91.79)
Epoch: [24][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2234e+00 (1.1361e+00)	Acc@1  64.84 ( 66.23)	Acc@5  87.50 ( 91.62)
Epoch: [24][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 8.9642e-01 (1.1288e+00)	Acc@1  76.56 ( 66.52)	Acc@5  95.31 ( 91.68)
Epoch: [24][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0667e+00 (1.1272e+00)	Acc@1  65.62 ( 66.63)	Acc@5  91.41 ( 91.66)
Epoch: [24][100/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.003)	Loss 1.1808e+00 (1.1288e+00)	Acc@1  64.06 ( 66.64)	Acc@5  92.97 ( 91.54)
Epoch: [24][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0601e+00 (1.1327e+00)	Acc@1  67.97 ( 66.50)	Acc@5  94.53 ( 91.53)
Epoch: [24][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1060e+00 (1.1280e+00)	Acc@1  71.88 ( 66.65)	Acc@5  93.75 ( 91.68)
Epoch: [24][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4179e+00 (1.1301e+00)	Acc@1  61.72 ( 66.67)	Acc@5  89.06 ( 91.61)
Epoch: [24][140/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.1224e+00 (1.1313e+00)	Acc@1  68.75 ( 66.62)	Acc@5  90.62 ( 91.58)
Epoch: [24][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0935e+00 (1.1288e+00)	Acc@1  67.97 ( 66.77)	Acc@5  95.31 ( 91.62)
Epoch: [24][160/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0292e+00 (1.1309e+00)	Acc@1  69.53 ( 66.78)	Acc@5  91.41 ( 91.62)
Epoch: [24][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0811e+00 (1.1325e+00)	Acc@1  66.41 ( 66.84)	Acc@5  91.41 ( 91.60)
Epoch: [24][180/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3664e+00 (1.1373e+00)	Acc@1  59.38 ( 66.76)	Acc@5  86.72 ( 91.50)
Epoch: [24][190/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.2000e+00 (1.1395e+00)	Acc@1  64.84 ( 66.64)	Acc@5  89.84 ( 91.50)
Epoch: [24][200/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2592e+00 (1.1386e+00)	Acc@1  67.19 ( 66.69)	Acc@5  91.41 ( 91.52)
Epoch: [24][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1771e+00 (1.1381e+00)	Acc@1  61.72 ( 66.64)	Acc@5  93.75 ( 91.57)
Epoch: [24][220/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0554e+00 (1.1378e+00)	Acc@1  67.97 ( 66.61)	Acc@5  92.97 ( 91.58)
Epoch: [24][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1817e+00 (1.1412e+00)	Acc@1  66.41 ( 66.51)	Acc@5  89.84 ( 91.55)
Epoch: [24][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1654e+00 (1.1429e+00)	Acc@1  70.31 ( 66.49)	Acc@5  89.84 ( 91.54)
Epoch: [24][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2759e+00 (1.1422e+00)	Acc@1  63.28 ( 66.52)	Acc@5  89.06 ( 91.51)
Epoch: [24][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2396e+00 (1.1422e+00)	Acc@1  63.28 ( 66.53)	Acc@5  89.84 ( 91.55)
Epoch: [24][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1169e+00 (1.1433e+00)	Acc@1  60.16 ( 66.44)	Acc@5  93.75 ( 91.54)
Epoch: [24][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3224e+00 (1.1456e+00)	Acc@1  67.97 ( 66.39)	Acc@5  86.72 ( 91.47)
Epoch: [24][290/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.1265e+00 (1.1469e+00)	Acc@1  67.19 ( 66.36)	Acc@5  92.19 ( 91.48)
Epoch: [24][300/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.2925e+00 (1.1480e+00)	Acc@1  63.28 ( 66.37)	Acc@5  88.28 ( 91.45)
Epoch: [24][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2773e+00 (1.1475e+00)	Acc@1  67.97 ( 66.37)	Acc@5  86.72 ( 91.43)
Epoch: [24][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0386e+00 (1.1465e+00)	Acc@1  67.97 ( 66.40)	Acc@5  93.75 ( 91.46)
Epoch: [24][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2290e+00 (1.1491e+00)	Acc@1  66.41 ( 66.34)	Acc@5  89.06 ( 91.41)
Epoch: [24][340/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0878e+00 (1.1498e+00)	Acc@1  65.62 ( 66.29)	Acc@5  94.53 ( 91.41)
Epoch: [24][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2677e+00 (1.1513e+00)	Acc@1  62.50 ( 66.25)	Acc@5  89.06 ( 91.40)
Epoch: [24][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1242e+00 (1.1544e+00)	Acc@1  64.06 ( 66.19)	Acc@5  92.19 ( 91.35)
Epoch: [24][370/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.2923e+00 (1.1572e+00)	Acc@1  60.16 ( 66.11)	Acc@5  89.84 ( 91.33)
Epoch: [24][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3086e+00 (1.1581e+00)	Acc@1  60.94 ( 66.06)	Acc@5  90.62 ( 91.33)
Epoch: [24][390/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3258e+00 (1.1607e+00)	Acc@1  60.00 ( 66.00)	Acc@5  85.00 ( 91.28)
## e[24] optimizer.zero_grad (sum) time: 0.26535916328430176
## e[24]       loss.backward (sum) time: 3.9656081199645996
## e[24]      optimizer.step (sum) time: 1.8710696697235107
## epoch[24] training(only) time: 15.992265462875366
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.5607e+00 (1.5607e+00)	Acc@1  55.00 ( 55.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.020 ( 0.034)	Loss 1.6845e+00 (1.7348e+00)	Acc@1  52.00 ( 54.73)	Acc@5  87.00 ( 84.64)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.6189e+00 (1.7273e+00)	Acc@1  58.00 ( 55.43)	Acc@5  85.00 ( 84.33)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 1.7795e+00 (1.7197e+00)	Acc@1  52.00 ( 55.13)	Acc@5  84.00 ( 84.65)
Test: [ 40/100]	Time  0.020 ( 0.026)	Loss 1.5735e+00 (1.7183e+00)	Acc@1  61.00 ( 55.37)	Acc@5  85.00 ( 84.59)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.8067e+00 (1.7310e+00)	Acc@1  57.00 ( 55.41)	Acc@5  81.00 ( 84.00)
Test: [ 60/100]	Time  0.020 ( 0.025)	Loss 2.1592e+00 (1.7160e+00)	Acc@1  48.00 ( 55.66)	Acc@5  74.00 ( 84.11)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6391e+00 (1.7145e+00)	Acc@1  54.00 ( 55.59)	Acc@5  86.00 ( 84.03)
Test: [ 80/100]	Time  0.019 ( 0.024)	Loss 1.7421e+00 (1.7256e+00)	Acc@1  58.00 ( 55.64)	Acc@5  82.00 ( 83.74)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 2.1149e+00 (1.7223e+00)	Acc@1  52.00 ( 55.73)	Acc@5  77.00 ( 83.92)
 * Acc@1 55.890 Acc@5 84.040
### epoch[24] execution time: 18.402456521987915
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.196 ( 0.196)	Data  0.141 ( 0.141)	Loss 1.2248e+00 (1.2248e+00)	Acc@1  69.53 ( 69.53)	Acc@5  89.84 ( 89.84)
Epoch: [25][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.014)	Loss 9.7225e-01 (1.1076e+00)	Acc@1  68.75 ( 67.05)	Acc@5  94.53 ( 92.90)
Epoch: [25][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.2283e+00 (1.1099e+00)	Acc@1  63.28 ( 66.74)	Acc@5  89.06 ( 92.26)
Epoch: [25][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.1521e+00 (1.1103e+00)	Acc@1  65.62 ( 67.14)	Acc@5  92.97 ( 92.39)
Epoch: [25][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.1105e+00 (1.0969e+00)	Acc@1  68.75 ( 67.38)	Acc@5  92.19 ( 92.64)
Epoch: [25][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0806e+00 (1.0898e+00)	Acc@1  68.75 ( 67.68)	Acc@5  92.19 ( 92.72)
Epoch: [25][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1715e+00 (1.0897e+00)	Acc@1  60.94 ( 67.71)	Acc@5  93.75 ( 92.60)
Epoch: [25][ 70/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3952e+00 (1.0932e+00)	Acc@1  60.16 ( 67.67)	Acc@5  85.94 ( 92.57)
Epoch: [25][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1148e+00 (1.0967e+00)	Acc@1  64.06 ( 67.59)	Acc@5  90.62 ( 92.47)
Epoch: [25][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0264e+00 (1.1026e+00)	Acc@1  72.66 ( 67.51)	Acc@5  93.75 ( 92.41)
Epoch: [25][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1606e+00 (1.0992e+00)	Acc@1  67.19 ( 67.48)	Acc@5  89.84 ( 92.42)
Epoch: [25][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0307e+00 (1.0998e+00)	Acc@1  64.84 ( 67.39)	Acc@5  92.19 ( 92.40)
Epoch: [25][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.5654e-01 (1.1035e+00)	Acc@1  75.78 ( 67.20)	Acc@5  96.88 ( 92.32)
Epoch: [25][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.8889e-01 (1.1041e+00)	Acc@1  72.66 ( 67.23)	Acc@5  95.31 ( 92.31)
Epoch: [25][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.6429e-01 (1.1023e+00)	Acc@1  72.66 ( 67.29)	Acc@5  91.41 ( 92.27)
Epoch: [25][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2033e+00 (1.1045e+00)	Acc@1  67.97 ( 67.24)	Acc@5  88.28 ( 92.20)
Epoch: [25][160/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0667e+00 (1.1034e+00)	Acc@1  70.31 ( 67.27)	Acc@5  91.41 ( 92.24)
Epoch: [25][170/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3417e+00 (1.1085e+00)	Acc@1  63.28 ( 67.12)	Acc@5  89.84 ( 92.21)
Epoch: [25][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0864e+00 (1.1095e+00)	Acc@1  71.09 ( 67.11)	Acc@5  90.62 ( 92.16)
Epoch: [25][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1514e+00 (1.1102e+00)	Acc@1  65.62 ( 67.14)	Acc@5  88.28 ( 92.12)
Epoch: [25][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.7242e-01 (1.1104e+00)	Acc@1  70.31 ( 67.13)	Acc@5  93.75 ( 92.10)
Epoch: [25][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3213e+00 (1.1095e+00)	Acc@1  60.94 ( 67.12)	Acc@5  90.62 ( 92.11)
Epoch: [25][220/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.4401e+00 (1.1125e+00)	Acc@1  61.72 ( 67.12)	Acc@5  86.72 ( 92.03)
Epoch: [25][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3795e+00 (1.1147e+00)	Acc@1  59.38 ( 67.07)	Acc@5  85.94 ( 91.98)
Epoch: [25][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1775e+00 (1.1143e+00)	Acc@1  65.62 ( 67.11)	Acc@5  90.62 ( 91.96)
Epoch: [25][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1255e+00 (1.1167e+00)	Acc@1  66.41 ( 67.07)	Acc@5  91.41 ( 91.92)
Epoch: [25][260/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.2077e+00 (1.1184e+00)	Acc@1  68.75 ( 67.06)	Acc@5  92.19 ( 91.89)
Epoch: [25][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.0906e-01 (1.1189e+00)	Acc@1  77.34 ( 67.04)	Acc@5  95.31 ( 91.90)
Epoch: [25][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1382e+00 (1.1202e+00)	Acc@1  69.53 ( 67.02)	Acc@5  92.19 ( 91.90)
Epoch: [25][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1653e+00 (1.1240e+00)	Acc@1  64.84 ( 66.94)	Acc@5  92.19 ( 91.84)
Epoch: [25][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3393e+00 (1.1278e+00)	Acc@1  61.72 ( 66.87)	Acc@5  90.62 ( 91.78)
Epoch: [25][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3044e+00 (1.1288e+00)	Acc@1  63.28 ( 66.86)	Acc@5  90.62 ( 91.76)
Epoch: [25][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3502e+00 (1.1294e+00)	Acc@1  61.72 ( 66.90)	Acc@5  90.62 ( 91.74)
Epoch: [25][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0787e+00 (1.1300e+00)	Acc@1  72.66 ( 66.89)	Acc@5  87.50 ( 91.71)
Epoch: [25][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4782e+00 (1.1326e+00)	Acc@1  53.91 ( 66.82)	Acc@5  89.84 ( 91.66)
Epoch: [25][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.7316e-01 (1.1334e+00)	Acc@1  73.44 ( 66.84)	Acc@5  93.75 ( 91.60)
Epoch: [25][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.5419e-01 (1.1335e+00)	Acc@1  73.44 ( 66.81)	Acc@5  93.75 ( 91.62)
Epoch: [25][370/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1960e+00 (1.1354e+00)	Acc@1  64.06 ( 66.76)	Acc@5  92.19 ( 91.60)
Epoch: [25][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0866e+00 (1.1360e+00)	Acc@1  71.88 ( 66.77)	Acc@5  91.41 ( 91.59)
Epoch: [25][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.3611e+00 (1.1361e+00)	Acc@1  57.50 ( 66.78)	Acc@5  92.50 ( 91.59)
## e[25] optimizer.zero_grad (sum) time: 0.26499319076538086
## e[25]       loss.backward (sum) time: 4.038878917694092
## e[25]      optimizer.step (sum) time: 1.8587050437927246
## epoch[25] training(only) time: 16.128740072250366
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.4747e+00 (1.4747e+00)	Acc@1  63.00 ( 63.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.6745e+00 (1.6029e+00)	Acc@1  55.00 ( 59.27)	Acc@5  86.00 ( 84.64)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 1.2348e+00 (1.5581e+00)	Acc@1  65.00 ( 60.14)	Acc@5  89.00 ( 85.81)
Test: [ 30/100]	Time  0.021 ( 0.028)	Loss 1.6720e+00 (1.5800e+00)	Acc@1  56.00 ( 59.19)	Acc@5  84.00 ( 85.55)
Test: [ 40/100]	Time  0.024 ( 0.026)	Loss 1.4682e+00 (1.5824e+00)	Acc@1  56.00 ( 58.41)	Acc@5  87.00 ( 85.80)
Test: [ 50/100]	Time  0.017 ( 0.026)	Loss 1.4589e+00 (1.5956e+00)	Acc@1  63.00 ( 58.53)	Acc@5  90.00 ( 85.53)
Test: [ 60/100]	Time  0.018 ( 0.025)	Loss 1.6722e+00 (1.5811e+00)	Acc@1  56.00 ( 58.64)	Acc@5  84.00 ( 85.69)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5593e+00 (1.5851e+00)	Acc@1  59.00 ( 58.35)	Acc@5  88.00 ( 85.66)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.8199e+00 (1.5967e+00)	Acc@1  58.00 ( 58.25)	Acc@5  81.00 ( 85.53)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 1.6944e+00 (1.5826e+00)	Acc@1  58.00 ( 58.46)	Acc@5  85.00 ( 85.65)
 * Acc@1 58.780 Acc@5 85.900
### epoch[25] execution time: 18.49970769882202
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.197 ( 0.197)	Data  0.147 ( 0.147)	Loss 1.2983e+00 (1.2983e+00)	Acc@1  67.97 ( 67.97)	Acc@5  88.28 ( 88.28)
Epoch: [26][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.014)	Loss 1.1684e+00 (1.0707e+00)	Acc@1  63.28 ( 68.39)	Acc@5  89.84 ( 92.47)
Epoch: [26][ 20/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.0023e+00 (1.0301e+00)	Acc@1  71.88 ( 69.64)	Acc@5  92.19 ( 92.93)
Epoch: [26][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.006)	Loss 9.8618e-01 (1.0410e+00)	Acc@1  69.53 ( 68.98)	Acc@5  94.53 ( 92.89)
Epoch: [26][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.1965e+00 (1.0635e+00)	Acc@1  65.62 ( 68.29)	Acc@5  88.28 ( 92.44)
Epoch: [26][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.1439e+00 (1.0642e+00)	Acc@1  58.59 ( 68.09)	Acc@5  92.97 ( 92.43)
Epoch: [26][ 60/391]	Time  0.041 ( 0.044)	Data  0.002 ( 0.003)	Loss 1.2557e+00 (1.0617e+00)	Acc@1  59.38 ( 68.30)	Acc@5  88.28 ( 92.42)
Epoch: [26][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.4756e-01 (1.0630e+00)	Acc@1  69.53 ( 68.22)	Acc@5  96.88 ( 92.47)
Epoch: [26][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0723e+00 (1.0623e+00)	Acc@1  68.75 ( 68.34)	Acc@5  89.84 ( 92.43)
Epoch: [26][ 90/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0473e+00 (1.0696e+00)	Acc@1  70.31 ( 68.20)	Acc@5  92.97 ( 92.29)
Epoch: [26][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0915e+00 (1.0712e+00)	Acc@1  67.19 ( 68.24)	Acc@5  89.84 ( 92.26)
Epoch: [26][110/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1338e+00 (1.0770e+00)	Acc@1  67.19 ( 68.05)	Acc@5  89.84 ( 92.24)
Epoch: [26][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0480e+00 (1.0810e+00)	Acc@1  64.06 ( 67.96)	Acc@5  96.09 ( 92.21)
Epoch: [26][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.4204e-01 (1.0820e+00)	Acc@1  75.78 ( 67.94)	Acc@5  96.09 ( 92.19)
Epoch: [26][140/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1755e+00 (1.0823e+00)	Acc@1  66.41 ( 67.87)	Acc@5  92.19 ( 92.22)
Epoch: [26][150/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2972e+00 (1.0830e+00)	Acc@1  63.28 ( 67.83)	Acc@5  89.06 ( 92.19)
Epoch: [26][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.9565e-01 (1.0862e+00)	Acc@1  72.66 ( 67.80)	Acc@5  92.97 ( 92.12)
Epoch: [26][170/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2213e+00 (1.0885e+00)	Acc@1  63.28 ( 67.74)	Acc@5  90.62 ( 92.11)
Epoch: [26][180/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1506e+00 (1.0932e+00)	Acc@1  73.44 ( 67.70)	Acc@5  88.28 ( 92.05)
Epoch: [26][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2217e+00 (1.0948e+00)	Acc@1  67.19 ( 67.76)	Acc@5  91.41 ( 92.02)
Epoch: [26][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.0973e-01 (1.0975e+00)	Acc@1  70.31 ( 67.66)	Acc@5  97.66 ( 92.07)
Epoch: [26][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1576e+00 (1.0984e+00)	Acc@1  66.41 ( 67.62)	Acc@5  92.97 ( 92.02)
Epoch: [26][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0976e+00 (1.0991e+00)	Acc@1  71.09 ( 67.66)	Acc@5  89.84 ( 91.99)
Epoch: [26][230/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.2037e-01 (1.0997e+00)	Acc@1  73.44 ( 67.62)	Acc@5  94.53 ( 91.99)
Epoch: [26][240/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.2779e+00 (1.1022e+00)	Acc@1  65.62 ( 67.57)	Acc@5  90.62 ( 91.97)
Epoch: [26][250/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.4807e+00 (1.1035e+00)	Acc@1  54.69 ( 67.47)	Acc@5  82.03 ( 91.92)
Epoch: [26][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2563e+00 (1.1053e+00)	Acc@1  59.38 ( 67.44)	Acc@5  92.19 ( 91.92)
Epoch: [26][270/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4201e+00 (1.1074e+00)	Acc@1  55.47 ( 67.35)	Acc@5  91.41 ( 91.91)
Epoch: [26][280/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1493e+00 (1.1089e+00)	Acc@1  67.19 ( 67.33)	Acc@5  90.62 ( 91.89)
Epoch: [26][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1711e+00 (1.1117e+00)	Acc@1  67.97 ( 67.25)	Acc@5  91.41 ( 91.88)
Epoch: [26][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0288e+00 (1.1121e+00)	Acc@1  67.97 ( 67.21)	Acc@5  93.75 ( 91.89)
Epoch: [26][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2386e+00 (1.1133e+00)	Acc@1  64.06 ( 67.19)	Acc@5  89.84 ( 91.86)
Epoch: [26][320/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6012e-01 (1.1126e+00)	Acc@1  67.97 ( 67.21)	Acc@5  92.97 ( 91.86)
Epoch: [26][330/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.002)	Loss 9.0664e-01 (1.1144e+00)	Acc@1  71.88 ( 67.17)	Acc@5  92.97 ( 91.83)
Epoch: [26][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1802e+00 (1.1166e+00)	Acc@1  66.41 ( 67.07)	Acc@5  92.97 ( 91.83)
Epoch: [26][350/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.001)	Loss 1.0555e+00 (1.1169e+00)	Acc@1  71.09 ( 67.08)	Acc@5  93.75 ( 91.82)
Epoch: [26][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2898e+00 (1.1182e+00)	Acc@1  61.72 ( 67.06)	Acc@5  90.62 ( 91.83)
Epoch: [26][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.9070e-01 (1.1190e+00)	Acc@1  74.22 ( 67.04)	Acc@5  96.09 ( 91.81)
Epoch: [26][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0957e+00 (1.1205e+00)	Acc@1  67.19 ( 67.02)	Acc@5  90.62 ( 91.75)
Epoch: [26][390/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0680e+00 (1.1218e+00)	Acc@1  63.75 ( 67.02)	Acc@5  97.50 ( 91.74)
## e[26] optimizer.zero_grad (sum) time: 0.2654891014099121
## e[26]       loss.backward (sum) time: 4.085224866867065
## e[26]      optimizer.step (sum) time: 1.808420181274414
## epoch[26] training(only) time: 16.189836263656616
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 1.5212e+00 (1.5212e+00)	Acc@1  57.00 ( 57.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 1.8330e+00 (1.7467e+00)	Acc@1  57.00 ( 56.45)	Acc@5  86.00 ( 84.18)
Test: [ 20/100]	Time  0.026 ( 0.028)	Loss 1.7622e+00 (1.7245e+00)	Acc@1  58.00 ( 56.10)	Acc@5  85.00 ( 84.14)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 1.8659e+00 (1.7333e+00)	Acc@1  50.00 ( 55.94)	Acc@5  82.00 ( 83.81)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.9104e+00 (1.7452e+00)	Acc@1  50.00 ( 55.44)	Acc@5  83.00 ( 83.90)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6611e+00 (1.7568e+00)	Acc@1  57.00 ( 55.39)	Acc@5  84.00 ( 83.47)
Test: [ 60/100]	Time  0.021 ( 0.025)	Loss 1.9836e+00 (1.7460e+00)	Acc@1  54.00 ( 55.57)	Acc@5  80.00 ( 83.66)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.5410e+00 (1.7600e+00)	Acc@1  57.00 ( 55.32)	Acc@5  87.00 ( 83.61)
Test: [ 80/100]	Time  0.024 ( 0.024)	Loss 1.7472e+00 (1.7771e+00)	Acc@1  58.00 ( 55.05)	Acc@5  85.00 ( 83.32)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 2.0016e+00 (1.7659e+00)	Acc@1  49.00 ( 55.32)	Acc@5  82.00 ( 83.52)
 * Acc@1 55.170 Acc@5 83.530
### epoch[26] execution time: 18.61647653579712
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.209 ( 0.209)	Data  0.151 ( 0.151)	Loss 8.8210e-01 (8.8210e-01)	Acc@1  75.00 ( 75.00)	Acc@5  96.09 ( 96.09)
Epoch: [27][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.015)	Loss 1.0100e+00 (9.8418e-01)	Acc@1  73.44 ( 72.09)	Acc@5  93.75 ( 92.47)
Epoch: [27][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.008)	Loss 9.9969e-01 (1.0213e+00)	Acc@1  72.66 ( 70.87)	Acc@5  94.53 ( 92.37)
Epoch: [27][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.1258e+00 (1.0371e+00)	Acc@1  71.88 ( 70.34)	Acc@5  91.41 ( 92.46)
Epoch: [27][ 40/391]	Time  0.037 ( 0.045)	Data  0.001 ( 0.005)	Loss 9.8355e-01 (1.0570e+00)	Acc@1  66.41 ( 69.55)	Acc@5  95.31 ( 92.34)
Epoch: [27][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.1493e+00 (1.0539e+00)	Acc@1  68.75 ( 69.53)	Acc@5  88.28 ( 92.36)
Epoch: [27][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.0882e+00 (1.0585e+00)	Acc@1  67.19 ( 69.31)	Acc@5  92.19 ( 92.14)
Epoch: [27][ 70/391]	Time  0.037 ( 0.043)	Data  0.002 ( 0.003)	Loss 1.3118e+00 (1.0548e+00)	Acc@1  60.94 ( 69.47)	Acc@5  89.06 ( 92.18)
Epoch: [27][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.7640e-01 (1.0634e+00)	Acc@1  71.88 ( 69.22)	Acc@5  92.19 ( 92.06)
Epoch: [27][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.0133e+00 (1.0555e+00)	Acc@1  67.19 ( 69.39)	Acc@5  93.75 ( 92.23)
Epoch: [27][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1139e+00 (1.0583e+00)	Acc@1  67.19 ( 69.29)	Acc@5  89.84 ( 92.32)
Epoch: [27][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2874e+00 (1.0639e+00)	Acc@1  60.94 ( 69.09)	Acc@5  89.84 ( 92.30)
Epoch: [27][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0521e+00 (1.0670e+00)	Acc@1  70.31 ( 69.00)	Acc@5  94.53 ( 92.30)
Epoch: [27][130/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2744e+00 (1.0701e+00)	Acc@1  65.62 ( 68.90)	Acc@5  87.50 ( 92.23)
Epoch: [27][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3420e+00 (1.0741e+00)	Acc@1  54.69 ( 68.66)	Acc@5  88.28 ( 92.27)
Epoch: [27][150/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2005e+00 (1.0763e+00)	Acc@1  66.41 ( 68.67)	Acc@5  94.53 ( 92.30)
Epoch: [27][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5067e+00 (1.0873e+00)	Acc@1  55.47 ( 68.45)	Acc@5  85.16 ( 92.13)
Epoch: [27][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1196e+00 (1.0886e+00)	Acc@1  65.62 ( 68.28)	Acc@5  92.97 ( 92.12)
Epoch: [27][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3538e+00 (1.0930e+00)	Acc@1  57.81 ( 68.09)	Acc@5  89.06 ( 92.03)
Epoch: [27][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4082e+00 (1.0973e+00)	Acc@1  59.38 ( 67.95)	Acc@5  90.62 ( 91.96)
Epoch: [27][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0852e+00 (1.0981e+00)	Acc@1  67.19 ( 67.95)	Acc@5  89.84 ( 91.95)
Epoch: [27][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1247e+00 (1.0989e+00)	Acc@1  67.19 ( 67.95)	Acc@5  91.41 ( 91.97)
Epoch: [27][220/391]	Time  0.048 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1751e+00 (1.0976e+00)	Acc@1  67.19 ( 67.94)	Acc@5  90.62 ( 92.01)
Epoch: [27][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0596e+00 (1.0974e+00)	Acc@1  64.06 ( 67.95)	Acc@5  95.31 ( 92.02)
Epoch: [27][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2131e+00 (1.0996e+00)	Acc@1  61.72 ( 67.85)	Acc@5  94.53 ( 92.04)
Epoch: [27][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1995e+00 (1.0995e+00)	Acc@1  64.84 ( 67.83)	Acc@5  92.19 ( 92.06)
Epoch: [27][260/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.0160e+00 (1.1015e+00)	Acc@1  66.41 ( 67.76)	Acc@5  95.31 ( 92.04)
Epoch: [27][270/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3406e+00 (1.1029e+00)	Acc@1  60.16 ( 67.65)	Acc@5  89.06 ( 92.05)
Epoch: [27][280/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.0601e+00 (1.1050e+00)	Acc@1  68.75 ( 67.58)	Acc@5  92.19 ( 92.02)
Epoch: [27][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1287e+00 (1.1064e+00)	Acc@1  62.50 ( 67.50)	Acc@5  93.75 ( 92.02)
Epoch: [27][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1879e+00 (1.1055e+00)	Acc@1  66.41 ( 67.52)	Acc@5  90.62 ( 92.00)
Epoch: [27][310/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.1088e+00 (1.1052e+00)	Acc@1  67.19 ( 67.56)	Acc@5  89.84 ( 92.00)
Epoch: [27][320/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1738e+00 (1.1069e+00)	Acc@1  68.75 ( 67.56)	Acc@5  92.19 ( 91.98)
Epoch: [27][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2751e+00 (1.1092e+00)	Acc@1  60.94 ( 67.51)	Acc@5  86.72 ( 91.91)
Epoch: [27][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3733e+00 (1.1139e+00)	Acc@1  64.06 ( 67.37)	Acc@5  85.94 ( 91.84)
Epoch: [27][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2027e+00 (1.1162e+00)	Acc@1  67.97 ( 67.30)	Acc@5  92.19 ( 91.80)
Epoch: [27][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1623e+00 (1.1173e+00)	Acc@1  64.06 ( 67.28)	Acc@5  91.41 ( 91.79)
Epoch: [27][370/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.2789e-01 (1.1159e+00)	Acc@1  74.22 ( 67.32)	Acc@5  96.09 ( 91.82)
Epoch: [27][380/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 9.5396e-01 (1.1159e+00)	Acc@1  70.31 ( 67.34)	Acc@5  94.53 ( 91.80)
Epoch: [27][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0843e+00 (1.1150e+00)	Acc@1  70.00 ( 67.39)	Acc@5  93.75 ( 91.80)
## e[27] optimizer.zero_grad (sum) time: 0.26423120498657227
## e[27]       loss.backward (sum) time: 3.970841646194458
## e[27]      optimizer.step (sum) time: 1.874131202697754
## epoch[27] training(only) time: 16.010689735412598
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.6709e+00 (1.6709e+00)	Acc@1  56.00 ( 56.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.025 ( 0.035)	Loss 1.6150e+00 (1.6734e+00)	Acc@1  58.00 ( 56.64)	Acc@5  90.00 ( 85.55)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 1.4186e+00 (1.6518e+00)	Acc@1  60.00 ( 57.05)	Acc@5  88.00 ( 85.29)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.9040e+00 (1.6739e+00)	Acc@1  48.00 ( 56.97)	Acc@5  80.00 ( 84.42)
Test: [ 40/100]	Time  0.020 ( 0.027)	Loss 1.7226e+00 (1.6879e+00)	Acc@1  58.00 ( 56.54)	Acc@5  83.00 ( 84.34)
Test: [ 50/100]	Time  0.016 ( 0.025)	Loss 1.5449e+00 (1.7002e+00)	Acc@1  60.00 ( 56.45)	Acc@5  81.00 ( 83.80)
Test: [ 60/100]	Time  0.018 ( 0.025)	Loss 1.7983e+00 (1.6844e+00)	Acc@1  53.00 ( 56.38)	Acc@5  80.00 ( 84.00)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 1.7526e+00 (1.6880e+00)	Acc@1  58.00 ( 56.49)	Acc@5  83.00 ( 83.85)
Test: [ 80/100]	Time  0.017 ( 0.024)	Loss 1.6724e+00 (1.7019e+00)	Acc@1  57.00 ( 56.07)	Acc@5  84.00 ( 83.84)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.8023e+00 (1.6959e+00)	Acc@1  50.00 ( 56.25)	Acc@5  81.00 ( 84.01)
 * Acc@1 56.370 Acc@5 84.130
### epoch[27] execution time: 18.368415117263794
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.204 ( 0.204)	Data  0.152 ( 0.152)	Loss 1.0552e+00 (1.0552e+00)	Acc@1  71.88 ( 71.88)	Acc@5  95.31 ( 95.31)
Epoch: [28][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.015)	Loss 1.0612e+00 (1.0529e+00)	Acc@1  67.19 ( 68.75)	Acc@5  94.53 ( 93.32)
Epoch: [28][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.008)	Loss 1.0190e+00 (1.0497e+00)	Acc@1  67.19 ( 68.90)	Acc@5  94.53 ( 93.15)
Epoch: [28][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.0641e+00 (1.0461e+00)	Acc@1  70.31 ( 69.18)	Acc@5  90.62 ( 92.99)
Epoch: [28][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.1384e+00 (1.0411e+00)	Acc@1  65.62 ( 69.38)	Acc@5  89.84 ( 92.91)
Epoch: [28][ 50/391]	Time  0.044 ( 0.043)	Data  0.002 ( 0.004)	Loss 9.9671e-01 (1.0384e+00)	Acc@1  73.44 ( 69.44)	Acc@5  92.97 ( 92.89)
Epoch: [28][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.2505e+00 (1.0392e+00)	Acc@1  63.28 ( 69.35)	Acc@5  86.72 ( 92.85)
Epoch: [28][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.4249e-01 (1.0373e+00)	Acc@1  81.25 ( 69.44)	Acc@5  96.88 ( 92.83)
Epoch: [28][ 80/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.8655e-01 (1.0372e+00)	Acc@1  72.66 ( 69.33)	Acc@5  93.75 ( 92.93)
Epoch: [28][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2648e+00 (1.0428e+00)	Acc@1  58.59 ( 69.14)	Acc@5  92.97 ( 92.87)
Epoch: [28][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1820e+00 (1.0467e+00)	Acc@1  63.28 ( 69.06)	Acc@5  91.41 ( 92.81)
Epoch: [28][110/391]	Time  0.053 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1877e+00 (1.0548e+00)	Acc@1  64.84 ( 68.83)	Acc@5  89.06 ( 92.70)
Epoch: [28][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1388e+00 (1.0573e+00)	Acc@1  68.75 ( 68.84)	Acc@5  90.62 ( 92.70)
Epoch: [28][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1372e+00 (1.0593e+00)	Acc@1  69.53 ( 68.87)	Acc@5  89.06 ( 92.59)
Epoch: [28][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2071e+00 (1.0579e+00)	Acc@1  67.19 ( 68.76)	Acc@5  87.50 ( 92.61)
Epoch: [28][150/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.3163e+00 (1.0639e+00)	Acc@1  62.50 ( 68.61)	Acc@5  90.62 ( 92.52)
Epoch: [28][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.4349e+00 (1.0711e+00)	Acc@1  53.12 ( 68.36)	Acc@5  86.72 ( 92.44)
Epoch: [28][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3796e+00 (1.0769e+00)	Acc@1  62.50 ( 68.16)	Acc@5  87.50 ( 92.36)
Epoch: [28][180/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.3921e+00 (1.0827e+00)	Acc@1  61.72 ( 68.02)	Acc@5  85.94 ( 92.29)
Epoch: [28][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1371e+00 (1.0869e+00)	Acc@1  70.31 ( 68.00)	Acc@5  89.84 ( 92.20)
Epoch: [28][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1897e+00 (1.0866e+00)	Acc@1  63.28 ( 67.98)	Acc@5  89.84 ( 92.18)
Epoch: [28][210/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2495e+00 (1.0888e+00)	Acc@1  64.84 ( 67.92)	Acc@5  90.62 ( 92.15)
Epoch: [28][220/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0312e+00 (1.0907e+00)	Acc@1  74.22 ( 67.92)	Acc@5  92.19 ( 92.12)
Epoch: [28][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1351e+00 (1.0926e+00)	Acc@1  68.75 ( 67.90)	Acc@5  89.84 ( 92.09)
Epoch: [28][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0332e+00 (1.0933e+00)	Acc@1  69.53 ( 67.88)	Acc@5  90.62 ( 92.07)
Epoch: [28][250/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0693e+00 (1.0944e+00)	Acc@1  68.75 ( 67.86)	Acc@5  93.75 ( 92.06)
Epoch: [28][260/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.9830e-01 (1.0943e+00)	Acc@1  72.66 ( 67.86)	Acc@5  96.09 ( 92.09)
Epoch: [28][270/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0063e+00 (1.0953e+00)	Acc@1  74.22 ( 67.88)	Acc@5  93.75 ( 92.07)
Epoch: [28][280/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1468e+00 (1.0962e+00)	Acc@1  63.28 ( 67.82)	Acc@5  93.75 ( 92.06)
Epoch: [28][290/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.9585e-01 (1.0949e+00)	Acc@1  74.22 ( 67.85)	Acc@5  94.53 ( 92.10)
Epoch: [28][300/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0826e+00 (1.0985e+00)	Acc@1  65.62 ( 67.73)	Acc@5  89.06 ( 92.02)
Epoch: [28][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3783e+00 (1.1005e+00)	Acc@1  60.94 ( 67.66)	Acc@5  89.06 ( 91.96)
Epoch: [28][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1970e+00 (1.0998e+00)	Acc@1  64.84 ( 67.66)	Acc@5  91.41 ( 91.97)
Epoch: [28][330/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.0170e+00 (1.0989e+00)	Acc@1  70.31 ( 67.65)	Acc@5  92.97 ( 92.02)
Epoch: [28][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1918e+00 (1.0997e+00)	Acc@1  66.41 ( 67.63)	Acc@5  89.06 ( 92.02)
Epoch: [28][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1247e+00 (1.1009e+00)	Acc@1  66.41 ( 67.57)	Acc@5  91.41 ( 92.03)
Epoch: [28][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1166e+00 (1.1022e+00)	Acc@1  69.53 ( 67.58)	Acc@5  94.53 ( 92.00)
Epoch: [28][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1505e+00 (1.1027e+00)	Acc@1  69.53 ( 67.58)	Acc@5  92.19 ( 91.98)
Epoch: [28][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2152e+00 (1.1023e+00)	Acc@1  64.84 ( 67.57)	Acc@5  87.50 ( 91.98)
Epoch: [28][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 9.8537e-01 (1.1021e+00)	Acc@1  68.75 ( 67.59)	Acc@5  92.50 ( 91.98)
## e[28] optimizer.zero_grad (sum) time: 0.2647836208343506
## e[28]       loss.backward (sum) time: 4.040122985839844
## e[28]      optimizer.step (sum) time: 1.830901861190796
## epoch[28] training(only) time: 16.198394536972046
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.5874e+00 (1.5874e+00)	Acc@1  58.00 ( 58.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 1.5014e+00 (1.5743e+00)	Acc@1  54.00 ( 58.27)	Acc@5  91.00 ( 85.45)
Test: [ 20/100]	Time  0.021 ( 0.028)	Loss 1.3552e+00 (1.5365e+00)	Acc@1  64.00 ( 58.81)	Acc@5  86.00 ( 85.57)
Test: [ 30/100]	Time  0.018 ( 0.026)	Loss 1.5890e+00 (1.5372e+00)	Acc@1  49.00 ( 58.42)	Acc@5  88.00 ( 86.19)
Test: [ 40/100]	Time  0.018 ( 0.024)	Loss 1.5341e+00 (1.5264e+00)	Acc@1  56.00 ( 58.61)	Acc@5  88.00 ( 86.32)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 1.2903e+00 (1.5379e+00)	Acc@1  63.00 ( 58.47)	Acc@5  90.00 ( 86.00)
Test: [ 60/100]	Time  0.024 ( 0.023)	Loss 1.6205e+00 (1.5234e+00)	Acc@1  55.00 ( 58.66)	Acc@5  88.00 ( 86.39)
Test: [ 70/100]	Time  0.017 ( 0.023)	Loss 1.6927e+00 (1.5260e+00)	Acc@1  57.00 ( 58.34)	Acc@5  82.00 ( 86.44)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.7793e+00 (1.5480e+00)	Acc@1  59.00 ( 58.06)	Acc@5  84.00 ( 86.14)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 1.6861e+00 (1.5417e+00)	Acc@1  60.00 ( 58.29)	Acc@5  86.00 ( 86.31)
 * Acc@1 58.610 Acc@5 86.550
### epoch[28] execution time: 18.502685070037842
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.197 ( 0.197)	Data  0.144 ( 0.144)	Loss 1.0842e+00 (1.0842e+00)	Acc@1  70.31 ( 70.31)	Acc@5  92.97 ( 92.97)
Epoch: [29][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 9.7211e-01 (1.0588e+00)	Acc@1  70.31 ( 67.68)	Acc@5  93.75 ( 92.61)
Epoch: [29][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.008)	Loss 8.4198e-01 (1.0336e+00)	Acc@1  76.56 ( 69.05)	Acc@5  92.19 ( 92.93)
Epoch: [29][ 30/391]	Time  0.040 ( 0.046)	Data  0.002 ( 0.006)	Loss 8.7918e-01 (1.0323e+00)	Acc@1  67.97 ( 68.83)	Acc@5  96.09 ( 93.15)
Epoch: [29][ 40/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 1.1059e+00 (1.0422e+00)	Acc@1  64.06 ( 68.64)	Acc@5  96.88 ( 92.66)
Epoch: [29][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.2046e+00 (1.0369e+00)	Acc@1  60.16 ( 68.78)	Acc@5  92.19 ( 92.88)
Epoch: [29][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0444e+00 (1.0371e+00)	Acc@1  73.44 ( 68.84)	Acc@5  92.97 ( 92.87)
Epoch: [29][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.4571e-01 (1.0375e+00)	Acc@1  69.53 ( 69.08)	Acc@5  94.53 ( 92.74)
Epoch: [29][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.7806e-01 (1.0340e+00)	Acc@1  71.09 ( 69.10)	Acc@5  93.75 ( 92.92)
Epoch: [29][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.5407e-01 (1.0292e+00)	Acc@1  77.34 ( 69.27)	Acc@5  95.31 ( 93.02)
Epoch: [29][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.1134e+00 (1.0338e+00)	Acc@1  68.75 ( 69.07)	Acc@5  96.09 ( 93.02)
Epoch: [29][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.7404e-01 (1.0288e+00)	Acc@1  71.88 ( 69.28)	Acc@5  96.09 ( 93.15)
Epoch: [29][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2818e+00 (1.0348e+00)	Acc@1  63.28 ( 69.17)	Acc@5  89.84 ( 93.08)
Epoch: [29][130/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.0460e+00 (1.0381e+00)	Acc@1  62.50 ( 69.07)	Acc@5  92.19 ( 93.01)
Epoch: [29][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.4565e-01 (1.0406e+00)	Acc@1  71.88 ( 69.10)	Acc@5  96.88 ( 92.94)
Epoch: [29][150/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 1.0943e+00 (1.0412e+00)	Acc@1  68.75 ( 69.07)	Acc@5  92.19 ( 92.94)
Epoch: [29][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0759e+00 (1.0440e+00)	Acc@1  71.09 ( 69.08)	Acc@5  89.84 ( 92.85)
Epoch: [29][170/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.8661e-01 (1.0484e+00)	Acc@1  72.66 ( 69.03)	Acc@5  92.97 ( 92.87)
Epoch: [29][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.0797e+00 (1.0521e+00)	Acc@1  68.75 ( 68.92)	Acc@5  91.41 ( 92.77)
Epoch: [29][190/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.2262e+00 (1.0544e+00)	Acc@1  64.84 ( 68.87)	Acc@5  87.50 ( 92.78)
Epoch: [29][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0404e+00 (1.0577e+00)	Acc@1  70.31 ( 68.77)	Acc@5  90.62 ( 92.70)
Epoch: [29][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.4051e+00 (1.0599e+00)	Acc@1  59.38 ( 68.65)	Acc@5  84.38 ( 92.67)
Epoch: [29][220/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2141e+00 (1.0643e+00)	Acc@1  61.72 ( 68.50)	Acc@5  89.84 ( 92.60)
Epoch: [29][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.8544e-01 (1.0648e+00)	Acc@1  72.66 ( 68.51)	Acc@5  92.97 ( 92.58)
Epoch: [29][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2970e+00 (1.0647e+00)	Acc@1  63.28 ( 68.53)	Acc@5  90.62 ( 92.62)
Epoch: [29][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1922e+00 (1.0651e+00)	Acc@1  68.75 ( 68.57)	Acc@5  89.84 ( 92.59)
Epoch: [29][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1693e+00 (1.0671e+00)	Acc@1  66.41 ( 68.59)	Acc@5  89.84 ( 92.52)
Epoch: [29][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1720e+00 (1.0674e+00)	Acc@1  67.97 ( 68.58)	Acc@5  89.84 ( 92.52)
Epoch: [29][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1542e+00 (1.0690e+00)	Acc@1  63.28 ( 68.53)	Acc@5  90.62 ( 92.49)
Epoch: [29][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2349e+00 (1.0718e+00)	Acc@1  60.16 ( 68.44)	Acc@5  92.97 ( 92.46)
Epoch: [29][300/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 1.2743e+00 (1.0735e+00)	Acc@1  64.06 ( 68.39)	Acc@5  89.06 ( 92.46)
Epoch: [29][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.5239e+00 (1.0751e+00)	Acc@1  56.25 ( 68.32)	Acc@5  85.94 ( 92.43)
Epoch: [29][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.0213e+00 (1.0783e+00)	Acc@1  69.53 ( 68.25)	Acc@5  94.53 ( 92.37)
Epoch: [29][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2098e+00 (1.0789e+00)	Acc@1  64.84 ( 68.28)	Acc@5  86.72 ( 92.37)
Epoch: [29][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.3860e+00 (1.0802e+00)	Acc@1  59.38 ( 68.23)	Acc@5  89.84 ( 92.38)
Epoch: [29][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.1351e+00 (1.0812e+00)	Acc@1  63.28 ( 68.22)	Acc@5  91.41 ( 92.35)
Epoch: [29][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 1.2566e+00 (1.0820e+00)	Acc@1  67.19 ( 68.22)	Acc@5  89.06 ( 92.33)
Epoch: [29][370/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.2073e+00 (1.0821e+00)	Acc@1  65.62 ( 68.20)	Acc@5  90.62 ( 92.33)
Epoch: [29][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.0010e+00 (1.0815e+00)	Acc@1  71.09 ( 68.22)	Acc@5  93.75 ( 92.34)
Epoch: [29][390/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.001)	Loss 1.1539e+00 (1.0818e+00)	Acc@1  67.50 ( 68.20)	Acc@5  92.50 ( 92.35)
## e[29] optimizer.zero_grad (sum) time: 0.26639890670776367
## e[29]       loss.backward (sum) time: 4.007644414901733
## e[29]      optimizer.step (sum) time: 1.903963565826416
## epoch[29] training(only) time: 16.00157594680786
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.6363e+00 (1.6363e+00)	Acc@1  62.00 ( 62.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 2.0538e+00 (1.7420e+00)	Acc@1  55.00 ( 57.73)	Acc@5  83.00 ( 84.09)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.5169e+00 (1.7048e+00)	Acc@1  63.00 ( 57.71)	Acc@5  88.00 ( 85.14)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 2.2313e+00 (1.7274e+00)	Acc@1  48.00 ( 57.00)	Acc@5  78.00 ( 84.87)
Test: [ 40/100]	Time  0.018 ( 0.025)	Loss 1.6846e+00 (1.7270e+00)	Acc@1  59.00 ( 57.15)	Acc@5  85.00 ( 84.63)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 1.6163e+00 (1.7298e+00)	Acc@1  56.00 ( 56.86)	Acc@5  86.00 ( 84.39)
Test: [ 60/100]	Time  0.024 ( 0.024)	Loss 1.7202e+00 (1.7133e+00)	Acc@1  57.00 ( 57.15)	Acc@5  85.00 ( 84.74)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6607e+00 (1.7086e+00)	Acc@1  55.00 ( 57.20)	Acc@5  83.00 ( 84.82)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 2.2673e+00 (1.7304e+00)	Acc@1  49.00 ( 56.78)	Acc@5  80.00 ( 84.58)
Test: [ 90/100]	Time  0.019 ( 0.023)	Loss 1.9149e+00 (1.7220e+00)	Acc@1  58.00 ( 56.99)	Acc@5  83.00 ( 84.67)
 * Acc@1 57.050 Acc@5 84.690
### epoch[29] execution time: 18.3816556930542
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.205 ( 0.205)	Data  0.153 ( 0.153)	Loss 1.0607e+00 (1.0607e+00)	Acc@1  67.19 ( 67.19)	Acc@5  92.97 ( 92.97)
Epoch: [30][ 10/391]	Time  0.046 ( 0.057)	Data  0.001 ( 0.015)	Loss 8.1342e-01 (9.6687e-01)	Acc@1  78.12 ( 72.09)	Acc@5  94.53 ( 94.32)
Epoch: [30][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.008)	Loss 9.8579e-01 (9.4765e-01)	Acc@1  72.66 ( 72.54)	Acc@5  92.97 ( 94.31)
Epoch: [30][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.006)	Loss 9.5635e-01 (9.5203e-01)	Acc@1  71.88 ( 72.76)	Acc@5  96.09 ( 94.41)
Epoch: [30][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 9.0700e-01 (9.3413e-01)	Acc@1  75.78 ( 73.27)	Acc@5  92.97 ( 94.34)
Epoch: [30][ 50/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.004)	Loss 8.7768e-01 (9.3153e-01)	Acc@1  70.31 ( 73.25)	Acc@5  95.31 ( 94.33)
Epoch: [30][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.1772e-01 (9.2024e-01)	Acc@1  73.44 ( 73.67)	Acc@5  95.31 ( 94.39)
Epoch: [30][ 70/391]	Time  0.043 ( 0.043)	Data  0.002 ( 0.003)	Loss 6.6974e-01 (9.0107e-01)	Acc@1  80.47 ( 74.21)	Acc@5  96.09 ( 94.55)
Epoch: [30][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.9781e-01 (8.9556e-01)	Acc@1  69.53 ( 74.18)	Acc@5  91.41 ( 94.59)
Epoch: [30][ 90/391]	Time  0.038 ( 0.043)	Data  0.002 ( 0.003)	Loss 7.2990e-01 (8.8532e-01)	Acc@1  77.34 ( 74.44)	Acc@5  98.44 ( 94.72)
Epoch: [30][100/391]	Time  0.041 ( 0.043)	Data  0.002 ( 0.003)	Loss 7.5924e-01 (8.7334e-01)	Acc@1  78.12 ( 74.72)	Acc@5  93.75 ( 94.87)
Epoch: [30][110/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 8.7457e-01 (8.7023e-01)	Acc@1  74.22 ( 74.75)	Acc@5  93.75 ( 94.90)
Epoch: [30][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.6969e-01 (8.6437e-01)	Acc@1  75.78 ( 74.98)	Acc@5  95.31 ( 94.91)
Epoch: [30][130/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 8.4171e-01 (8.5892e-01)	Acc@1  72.66 ( 75.11)	Acc@5  96.88 ( 94.95)
Epoch: [30][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.0898e-01 (8.5564e-01)	Acc@1  74.22 ( 75.19)	Acc@5  96.09 ( 94.97)
Epoch: [30][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.3445e-01 (8.5290e-01)	Acc@1  74.22 ( 75.25)	Acc@5  93.75 ( 94.97)
Epoch: [30][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.5918e-01 (8.5170e-01)	Acc@1  73.44 ( 75.24)	Acc@5  93.75 ( 95.03)
Epoch: [30][170/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.7018e-01 (8.4848e-01)	Acc@1  83.59 ( 75.33)	Acc@5  97.66 ( 95.04)
Epoch: [30][180/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.7341e-01 (8.4700e-01)	Acc@1  73.44 ( 75.39)	Acc@5  95.31 ( 95.03)
Epoch: [30][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.6711e-01 (8.4577e-01)	Acc@1  73.44 ( 75.43)	Acc@5  96.88 ( 95.05)
Epoch: [30][200/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.1226e-01 (8.4035e-01)	Acc@1  75.00 ( 75.57)	Acc@5  92.97 ( 95.07)
Epoch: [30][210/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.5267e-01 (8.3882e-01)	Acc@1  73.44 ( 75.59)	Acc@5  96.09 ( 95.06)
Epoch: [30][220/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.3787e-01 (8.3693e-01)	Acc@1  82.03 ( 75.69)	Acc@5  96.09 ( 95.09)
Epoch: [30][230/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3643e-01 (8.3642e-01)	Acc@1  76.56 ( 75.64)	Acc@5  97.66 ( 95.13)
Epoch: [30][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8058e-01 (8.3228e-01)	Acc@1  77.34 ( 75.78)	Acc@5  98.44 ( 95.16)
Epoch: [30][250/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.4094e-01 (8.3043e-01)	Acc@1  77.34 ( 75.80)	Acc@5  93.75 ( 95.17)
Epoch: [30][260/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3931e-01 (8.2936e-01)	Acc@1  77.34 ( 75.77)	Acc@5  95.31 ( 95.18)
Epoch: [30][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.2237e-01 (8.2681e-01)	Acc@1  74.22 ( 75.79)	Acc@5  96.09 ( 95.20)
Epoch: [30][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5019e-01 (8.2482e-01)	Acc@1  80.47 ( 75.86)	Acc@5  92.19 ( 95.21)
Epoch: [30][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.8909e-01 (8.2173e-01)	Acc@1  73.44 ( 75.96)	Acc@5  94.53 ( 95.22)
Epoch: [30][300/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 8.8833e-01 (8.2120e-01)	Acc@1  74.22 ( 75.98)	Acc@5  95.31 ( 95.23)
Epoch: [30][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7549e-01 (8.1896e-01)	Acc@1  75.78 ( 76.10)	Acc@5  95.31 ( 95.24)
Epoch: [30][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4401e-01 (8.1677e-01)	Acc@1  78.91 ( 76.19)	Acc@5  96.88 ( 95.26)
Epoch: [30][330/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 9.2958e-01 (8.1585e-01)	Acc@1  76.56 ( 76.21)	Acc@5  92.97 ( 95.28)
Epoch: [30][340/391]	Time  0.048 ( 0.041)	Data  0.002 ( 0.002)	Loss 8.7711e-01 (8.1541e-01)	Acc@1  73.44 ( 76.24)	Acc@5  96.09 ( 95.31)
Epoch: [30][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.7907e-01 (8.1357e-01)	Acc@1  78.91 ( 76.30)	Acc@5  94.53 ( 95.32)
Epoch: [30][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0118e-01 (8.1165e-01)	Acc@1  68.75 ( 76.35)	Acc@5  94.53 ( 95.33)
Epoch: [30][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.9062e-01 (8.0981e-01)	Acc@1  70.31 ( 76.39)	Acc@5  92.97 ( 95.36)
Epoch: [30][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0390e-01 (8.0899e-01)	Acc@1  78.12 ( 76.40)	Acc@5  94.53 ( 95.38)
Epoch: [30][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5526e-01 (8.0726e-01)	Acc@1  83.75 ( 76.47)	Acc@5  97.50 ( 95.39)
## e[30] optimizer.zero_grad (sum) time: 0.26614809036254883
## e[30]       loss.backward (sum) time: 3.9560489654541016
## e[30]      optimizer.step (sum) time: 1.8587687015533447
## epoch[30] training(only) time: 16.0734965801239
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.1691e+00 (1.1691e+00)	Acc@1  72.00 ( 72.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.025 ( 0.035)	Loss 1.2676e+00 (1.2377e+00)	Acc@1  66.00 ( 67.73)	Acc@5  91.00 ( 89.09)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 1.0702e+00 (1.1880e+00)	Acc@1  73.00 ( 68.48)	Acc@5  92.00 ( 90.19)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 1.2828e+00 (1.1924e+00)	Acc@1  65.00 ( 68.23)	Acc@5  88.00 ( 90.06)
Test: [ 40/100]	Time  0.027 ( 0.026)	Loss 1.2183e+00 (1.1912e+00)	Acc@1  65.00 ( 67.54)	Acc@5  91.00 ( 90.05)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 1.1991e+00 (1.2020e+00)	Acc@1  68.00 ( 67.06)	Acc@5  91.00 ( 89.88)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 1.2159e+00 (1.1858e+00)	Acc@1  64.00 ( 67.36)	Acc@5  91.00 ( 90.28)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.1017e+00 (1.1831e+00)	Acc@1  67.00 ( 67.17)	Acc@5  88.00 ( 90.25)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3207e+00 (1.1923e+00)	Acc@1  69.00 ( 67.16)	Acc@5  90.00 ( 90.16)
Test: [ 90/100]	Time  0.021 ( 0.024)	Loss 1.3186e+00 (1.1832e+00)	Acc@1  65.00 ( 67.31)	Acc@5  91.00 ( 90.36)
 * Acc@1 67.440 Acc@5 90.500
### epoch[30] execution time: 18.49981188774109
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.197 ( 0.197)	Data  0.150 ( 0.150)	Loss 7.8270e-01 (7.8270e-01)	Acc@1  71.88 ( 71.88)	Acc@5  96.09 ( 96.09)
Epoch: [31][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.015)	Loss 6.3900e-01 (7.0702e-01)	Acc@1  82.81 ( 78.69)	Acc@5  96.88 ( 96.88)
Epoch: [31][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.008)	Loss 7.7134e-01 (7.2971e-01)	Acc@1  78.12 ( 78.42)	Acc@5  95.31 ( 96.39)
Epoch: [31][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.2507e-01 (7.1561e-01)	Acc@1  82.03 ( 78.58)	Acc@5  98.44 ( 96.60)
Epoch: [31][ 40/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.7189e-01 (7.1414e-01)	Acc@1  78.12 ( 78.68)	Acc@5  97.66 ( 96.57)
Epoch: [31][ 50/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.4472e-01 (7.1208e-01)	Acc@1  80.47 ( 78.72)	Acc@5  98.44 ( 96.58)
Epoch: [31][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 8.5511e-01 (7.1011e-01)	Acc@1  71.88 ( 78.79)	Acc@5  95.31 ( 96.67)
Epoch: [31][ 70/391]	Time  0.043 ( 0.043)	Data  0.002 ( 0.003)	Loss 7.8157e-01 (7.1091e-01)	Acc@1  79.69 ( 78.82)	Acc@5  92.97 ( 96.51)
Epoch: [31][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.7902e-01 (7.1427e-01)	Acc@1  75.00 ( 78.91)	Acc@5  95.31 ( 96.41)
Epoch: [31][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.2579e-01 (7.1856e-01)	Acc@1  69.53 ( 78.75)	Acc@5  90.62 ( 96.33)
Epoch: [31][100/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.1708e-01 (7.1805e-01)	Acc@1  78.91 ( 78.75)	Acc@5  96.88 ( 96.33)
Epoch: [31][110/391]	Time  0.044 ( 0.042)	Data  0.002 ( 0.002)	Loss 7.4106e-01 (7.1950e-01)	Acc@1  82.81 ( 78.80)	Acc@5  98.44 ( 96.30)
Epoch: [31][120/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.9772e-01 (7.2311e-01)	Acc@1  77.34 ( 78.60)	Acc@5  94.53 ( 96.25)
Epoch: [31][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0904e-01 (7.2291e-01)	Acc@1  76.56 ( 78.67)	Acc@5  96.09 ( 96.23)
Epoch: [31][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.6645e-01 (7.2710e-01)	Acc@1  81.25 ( 78.68)	Acc@5  94.53 ( 96.18)
Epoch: [31][150/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.5122e-01 (7.2445e-01)	Acc@1  73.44 ( 78.77)	Acc@5  97.66 ( 96.22)
Epoch: [31][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.8962e-01 (7.2686e-01)	Acc@1  75.78 ( 78.66)	Acc@5  93.75 ( 96.19)
Epoch: [31][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8917e-01 (7.2503e-01)	Acc@1  84.38 ( 78.76)	Acc@5  97.66 ( 96.20)
Epoch: [31][180/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3648e-01 (7.2429e-01)	Acc@1  85.94 ( 78.84)	Acc@5  98.44 ( 96.21)
Epoch: [31][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1958e-01 (7.2317e-01)	Acc@1  82.03 ( 78.86)	Acc@5  98.44 ( 96.22)
Epoch: [31][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0979e-01 (7.2212e-01)	Acc@1  81.25 ( 78.86)	Acc@5  96.88 ( 96.19)
Epoch: [31][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0501e-01 (7.2484e-01)	Acc@1  74.22 ( 78.76)	Acc@5  95.31 ( 96.16)
Epoch: [31][220/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5329e-01 (7.2699e-01)	Acc@1  74.22 ( 78.72)	Acc@5  94.53 ( 96.15)
Epoch: [31][230/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2870e-01 (7.2672e-01)	Acc@1  82.81 ( 78.71)	Acc@5  96.88 ( 96.16)
Epoch: [31][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9437e-01 (7.2625e-01)	Acc@1  78.91 ( 78.70)	Acc@5  99.22 ( 96.17)
Epoch: [31][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6662e-01 (7.2562e-01)	Acc@1  82.81 ( 78.72)	Acc@5  96.88 ( 96.17)
Epoch: [31][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0736e-01 (7.2785e-01)	Acc@1  81.25 ( 78.65)	Acc@5  98.44 ( 96.15)
Epoch: [31][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5847e-01 (7.2782e-01)	Acc@1  75.78 ( 78.66)	Acc@5  96.09 ( 96.16)
Epoch: [31][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9556e-01 (7.2820e-01)	Acc@1  78.12 ( 78.61)	Acc@5  96.09 ( 96.14)
Epoch: [31][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1452e-01 (7.2699e-01)	Acc@1  85.16 ( 78.68)	Acc@5  98.44 ( 96.15)
Epoch: [31][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5663e-01 (7.2879e-01)	Acc@1  72.66 ( 78.63)	Acc@5  94.53 ( 96.11)
Epoch: [31][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4370e-01 (7.2768e-01)	Acc@1  79.69 ( 78.67)	Acc@5  93.75 ( 96.13)
Epoch: [31][320/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9492e-01 (7.2832e-01)	Acc@1  82.03 ( 78.67)	Acc@5  96.88 ( 96.12)
Epoch: [31][330/391]	Time  0.046 ( 0.041)	Data  0.002 ( 0.002)	Loss 8.1309e-01 (7.2742e-01)	Acc@1  71.88 ( 78.65)	Acc@5  95.31 ( 96.13)
Epoch: [31][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3710e-01 (7.2643e-01)	Acc@1  79.69 ( 78.69)	Acc@5  96.09 ( 96.16)
Epoch: [31][350/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0195e-01 (7.2730e-01)	Acc@1  75.78 ( 78.64)	Acc@5  96.09 ( 96.14)
Epoch: [31][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9696e-01 (7.2849e-01)	Acc@1  81.25 ( 78.61)	Acc@5  95.31 ( 96.11)
Epoch: [31][370/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.9296e-01 (7.2861e-01)	Acc@1  76.56 ( 78.62)	Acc@5  96.88 ( 96.10)
Epoch: [31][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.8622e-01 (7.2840e-01)	Acc@1  80.47 ( 78.59)	Acc@5  96.09 ( 96.11)
Epoch: [31][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.3286e-01 (7.2921e-01)	Acc@1  81.25 ( 78.56)	Acc@5  96.25 ( 96.10)
## e[31] optimizer.zero_grad (sum) time: 0.2645740509033203
## e[31]       loss.backward (sum) time: 4.028426170349121
## e[31]      optimizer.step (sum) time: 1.870814561843872
## epoch[31] training(only) time: 16.094057083129883
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.1383e+00 (1.1383e+00)	Acc@1  69.00 ( 69.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.2470e+00 (1.2553e+00)	Acc@1  64.00 ( 66.73)	Acc@5  92.00 ( 90.36)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 1.0021e+00 (1.1894e+00)	Acc@1  74.00 ( 68.24)	Acc@5  91.00 ( 90.81)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 1.3722e+00 (1.1962e+00)	Acc@1  59.00 ( 67.61)	Acc@5  91.00 ( 90.81)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 1.1281e+00 (1.1921e+00)	Acc@1  65.00 ( 67.37)	Acc@5  93.00 ( 90.80)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 1.1882e+00 (1.1987e+00)	Acc@1  67.00 ( 66.88)	Acc@5  94.00 ( 90.65)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 1.2491e+00 (1.1826e+00)	Acc@1  67.00 ( 67.13)	Acc@5  88.00 ( 90.80)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.1458e+00 (1.1771e+00)	Acc@1  63.00 ( 67.00)	Acc@5  93.00 ( 90.92)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 1.2610e+00 (1.1871e+00)	Acc@1  72.00 ( 66.98)	Acc@5  91.00 ( 90.62)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.3305e+00 (1.1754e+00)	Acc@1  67.00 ( 67.41)	Acc@5  88.00 ( 90.74)
 * Acc@1 67.590 Acc@5 90.850
### epoch[31] execution time: 18.419450283050537
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.195 ( 0.195)	Data  0.144 ( 0.144)	Loss 8.4804e-01 (8.4804e-01)	Acc@1  77.34 ( 77.34)	Acc@5  94.53 ( 94.53)
Epoch: [32][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 6.5806e-01 (7.3820e-01)	Acc@1  80.47 ( 79.12)	Acc@5  97.66 ( 95.67)
Epoch: [32][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 5.8913e-01 (7.1199e-01)	Acc@1  83.59 ( 79.06)	Acc@5  98.44 ( 96.09)
Epoch: [32][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.006)	Loss 6.9381e-01 (7.1797e-01)	Acc@1  80.47 ( 78.68)	Acc@5  96.09 ( 96.17)
Epoch: [32][ 40/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.2631e-01 (7.0675e-01)	Acc@1  84.38 ( 79.29)	Acc@5  98.44 ( 96.09)
Epoch: [32][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.5334e-01 (7.0504e-01)	Acc@1  76.56 ( 79.37)	Acc@5  92.97 ( 96.16)
Epoch: [32][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.9591e-01 (6.9247e-01)	Acc@1  87.50 ( 79.56)	Acc@5  98.44 ( 96.32)
Epoch: [32][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.1013e-01 (6.9656e-01)	Acc@1  80.47 ( 79.60)	Acc@5  96.88 ( 96.30)
Epoch: [32][ 80/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.0417e-01 (6.9228e-01)	Acc@1  77.34 ( 79.68)	Acc@5  98.44 ( 96.44)
Epoch: [32][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.2081e-01 (6.8930e-01)	Acc@1  85.16 ( 79.81)	Acc@5  97.66 ( 96.52)
Epoch: [32][100/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.002)	Loss 6.3551e-01 (6.8964e-01)	Acc@1  83.59 ( 79.76)	Acc@5  96.88 ( 96.55)
Epoch: [32][110/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.002)	Loss 6.6544e-01 (6.9117e-01)	Acc@1  79.69 ( 79.67)	Acc@5  96.88 ( 96.55)
Epoch: [32][120/391]	Time  0.045 ( 0.043)	Data  0.002 ( 0.002)	Loss 8.4968e-01 (6.9331e-01)	Acc@1  73.44 ( 79.52)	Acc@5  98.44 ( 96.60)
Epoch: [32][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.6902e-01 (6.9209e-01)	Acc@1  77.34 ( 79.50)	Acc@5  96.09 ( 96.59)
Epoch: [32][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7433e-01 (6.9230e-01)	Acc@1  85.94 ( 79.48)	Acc@5  98.44 ( 96.58)
Epoch: [32][150/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.6168e-01 (6.9290e-01)	Acc@1  82.81 ( 79.50)	Acc@5  96.88 ( 96.60)
Epoch: [32][160/391]	Time  0.039 ( 0.042)	Data  0.003 ( 0.002)	Loss 9.7339e-01 (6.9535e-01)	Acc@1  70.31 ( 79.45)	Acc@5  89.84 ( 96.53)
Epoch: [32][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.8225e-01 (6.9753e-01)	Acc@1  73.44 ( 79.41)	Acc@5  93.75 ( 96.48)
Epoch: [32][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.1500e-01 (7.0208e-01)	Acc@1  67.97 ( 79.26)	Acc@5  95.31 ( 96.40)
Epoch: [32][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.5864e-01 (7.0209e-01)	Acc@1  82.81 ( 79.21)	Acc@5  93.75 ( 96.38)
Epoch: [32][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.6919e-01 (7.0318e-01)	Acc@1  78.12 ( 79.22)	Acc@5  96.88 ( 96.35)
Epoch: [32][210/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0956e-01 (7.0266e-01)	Acc@1  76.56 ( 79.25)	Acc@5  97.66 ( 96.33)
Epoch: [32][220/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.6399e-01 (7.0341e-01)	Acc@1  78.12 ( 79.23)	Acc@5  96.88 ( 96.33)
Epoch: [32][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0206e-01 (7.0451e-01)	Acc@1  82.81 ( 79.27)	Acc@5  95.31 ( 96.30)
Epoch: [32][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1919e-01 (7.0314e-01)	Acc@1  77.34 ( 79.24)	Acc@5  96.88 ( 96.30)
Epoch: [32][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6788e-01 (7.0401e-01)	Acc@1  81.25 ( 79.22)	Acc@5  96.88 ( 96.31)
Epoch: [32][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5939e-01 (7.0377e-01)	Acc@1  80.47 ( 79.22)	Acc@5  94.53 ( 96.29)
Epoch: [32][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0870e-01 (7.0496e-01)	Acc@1  79.69 ( 79.18)	Acc@5  98.44 ( 96.28)
Epoch: [32][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2762e-01 (7.0485e-01)	Acc@1  79.69 ( 79.17)	Acc@5  96.09 ( 96.28)
Epoch: [32][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2317e-01 (7.0564e-01)	Acc@1  76.56 ( 79.11)	Acc@5  96.09 ( 96.28)
Epoch: [32][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9794e-01 (7.0207e-01)	Acc@1  81.25 ( 79.22)	Acc@5  97.66 ( 96.31)
Epoch: [32][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4494e-01 (7.0120e-01)	Acc@1  85.16 ( 79.23)	Acc@5  93.75 ( 96.31)
Epoch: [32][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4987e-01 (7.0031e-01)	Acc@1  83.59 ( 79.26)	Acc@5  99.22 ( 96.33)
Epoch: [32][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0608e-01 (7.0145e-01)	Acc@1  73.44 ( 79.19)	Acc@5  95.31 ( 96.31)
Epoch: [32][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3851e-01 (7.0002e-01)	Acc@1  79.69 ( 79.23)	Acc@5  96.88 ( 96.32)
Epoch: [32][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5767e-01 (6.9980e-01)	Acc@1  78.12 ( 79.24)	Acc@5  95.31 ( 96.32)
Epoch: [32][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6204e-01 (6.9989e-01)	Acc@1  81.25 ( 79.22)	Acc@5  96.88 ( 96.33)
Epoch: [32][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7912e-01 (6.9911e-01)	Acc@1  82.81 ( 79.24)	Acc@5  96.88 ( 96.33)
Epoch: [32][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0768e-01 (6.9878e-01)	Acc@1  75.00 ( 79.26)	Acc@5  95.31 ( 96.33)
Epoch: [32][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3789e-01 (6.9856e-01)	Acc@1  72.50 ( 79.23)	Acc@5  96.25 ( 96.34)
## e[32] optimizer.zero_grad (sum) time: 0.26579737663269043
## e[32]       loss.backward (sum) time: 3.9951987266540527
## e[32]      optimizer.step (sum) time: 1.8898954391479492
## epoch[32] training(only) time: 16.1222665309906
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.2018e+00 (1.2018e+00)	Acc@1  69.00 ( 69.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2844e+00 (1.2424e+00)	Acc@1  65.00 ( 67.09)	Acc@5  91.00 ( 90.00)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 1.0215e+00 (1.1919e+00)	Acc@1  76.00 ( 67.67)	Acc@5  93.00 ( 90.86)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 1.2990e+00 (1.1986e+00)	Acc@1  65.00 ( 67.55)	Acc@5  92.00 ( 90.71)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 1.1821e+00 (1.1958e+00)	Acc@1  65.00 ( 67.32)	Acc@5  93.00 ( 90.85)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 1.1343e+00 (1.1983e+00)	Acc@1  71.00 ( 67.16)	Acc@5  91.00 ( 90.51)
Test: [ 60/100]	Time  0.024 ( 0.023)	Loss 1.2800e+00 (1.1810e+00)	Acc@1  63.00 ( 67.41)	Acc@5  88.00 ( 90.75)
Test: [ 70/100]	Time  0.024 ( 0.023)	Loss 1.1832e+00 (1.1796e+00)	Acc@1  66.00 ( 67.42)	Acc@5  91.00 ( 90.76)
Test: [ 80/100]	Time  0.025 ( 0.023)	Loss 1.2720e+00 (1.1864e+00)	Acc@1  72.00 ( 67.46)	Acc@5  90.00 ( 90.63)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.3315e+00 (1.1738e+00)	Acc@1  65.00 ( 67.86)	Acc@5  88.00 ( 90.76)
 * Acc@1 68.030 Acc@5 90.900
### epoch[32] execution time: 18.443240880966187
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.208 ( 0.208)	Data  0.156 ( 0.156)	Loss 6.1886e-01 (6.1886e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [33][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.015)	Loss 7.9822e-01 (6.5976e-01)	Acc@1  74.22 ( 80.82)	Acc@5  96.88 ( 97.02)
Epoch: [33][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 6.8213e-01 (6.7136e-01)	Acc@1  75.00 ( 80.28)	Acc@5  96.09 ( 96.80)
Epoch: [33][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.006)	Loss 6.5840e-01 (6.6998e-01)	Acc@1  82.03 ( 80.24)	Acc@5  93.75 ( 96.82)
Epoch: [33][ 40/391]	Time  0.042 ( 0.045)	Data  0.002 ( 0.005)	Loss 6.0304e-01 (6.5615e-01)	Acc@1  80.47 ( 80.70)	Acc@5  98.44 ( 96.88)
Epoch: [33][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 7.4163e-01 (6.6399e-01)	Acc@1  75.78 ( 80.41)	Acc@5  97.66 ( 96.81)
Epoch: [33][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 7.9632e-01 (6.6854e-01)	Acc@1  76.56 ( 80.17)	Acc@5  92.97 ( 96.81)
Epoch: [33][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.2318e-01 (6.6831e-01)	Acc@1  82.81 ( 80.34)	Acc@5  97.66 ( 96.81)
Epoch: [33][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.0407e-01 (6.6785e-01)	Acc@1  73.44 ( 80.30)	Acc@5  96.88 ( 96.77)
Epoch: [33][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.9682e-01 (6.6206e-01)	Acc@1  76.56 ( 80.46)	Acc@5  97.66 ( 96.83)
Epoch: [33][100/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.0726e-01 (6.6054e-01)	Acc@1  82.03 ( 80.55)	Acc@5  96.88 ( 96.86)
Epoch: [33][110/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.002)	Loss 8.5835e-01 (6.6540e-01)	Acc@1  71.88 ( 80.36)	Acc@5  95.31 ( 96.76)
Epoch: [33][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2015e-01 (6.6703e-01)	Acc@1  78.91 ( 80.22)	Acc@5 100.00 ( 96.77)
Epoch: [33][130/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.4231e-01 (6.6853e-01)	Acc@1  79.69 ( 80.23)	Acc@5  97.66 ( 96.72)
Epoch: [33][140/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0672e-01 (6.6615e-01)	Acc@1  79.69 ( 80.31)	Acc@5  95.31 ( 96.73)
Epoch: [33][150/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.2836e-01 (6.6466e-01)	Acc@1  76.56 ( 80.33)	Acc@5  93.75 ( 96.75)
Epoch: [33][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.5086e-01 (6.6697e-01)	Acc@1  76.56 ( 80.19)	Acc@5  96.88 ( 96.73)
Epoch: [33][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0440e-01 (6.6689e-01)	Acc@1  78.91 ( 80.18)	Acc@5  94.53 ( 96.69)
Epoch: [33][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.8361e-01 (6.6898e-01)	Acc@1  73.44 ( 80.09)	Acc@5  97.66 ( 96.68)
Epoch: [33][190/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 9.5650e-01 (6.7209e-01)	Acc@1  75.78 ( 80.06)	Acc@5  91.41 ( 96.62)
Epoch: [33][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0186e-01 (6.7303e-01)	Acc@1  82.81 ( 80.02)	Acc@5  99.22 ( 96.60)
Epoch: [33][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5597e-01 (6.7381e-01)	Acc@1  85.94 ( 80.01)	Acc@5  98.44 ( 96.59)
Epoch: [33][220/391]	Time  0.056 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8924e-01 (6.7234e-01)	Acc@1  84.38 ( 80.09)	Acc@5  95.31 ( 96.59)
Epoch: [33][230/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.4035e-01 (6.7205e-01)	Acc@1  80.47 ( 80.06)	Acc@5  96.09 ( 96.60)
Epoch: [33][240/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.3296e-01 (6.7192e-01)	Acc@1  79.69 ( 80.05)	Acc@5  96.09 ( 96.63)
Epoch: [33][250/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8304e-01 (6.7271e-01)	Acc@1  82.81 ( 80.01)	Acc@5  98.44 ( 96.64)
Epoch: [33][260/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.4673e-01 (6.7333e-01)	Acc@1  83.59 ( 80.03)	Acc@5  97.66 ( 96.65)
Epoch: [33][270/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1209e-01 (6.7207e-01)	Acc@1  78.91 ( 80.04)	Acc@5  97.66 ( 96.64)
Epoch: [33][280/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.2829e-01 (6.7284e-01)	Acc@1  76.56 ( 80.02)	Acc@5  96.88 ( 96.62)
Epoch: [33][290/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.5087e-01 (6.7286e-01)	Acc@1  85.16 ( 80.05)	Acc@5  95.31 ( 96.63)
Epoch: [33][300/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0880e-01 (6.7369e-01)	Acc@1  74.22 ( 79.99)	Acc@5  98.44 ( 96.65)
Epoch: [33][310/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.001)	Loss 7.8929e-01 (6.7453e-01)	Acc@1  77.34 ( 79.98)	Acc@5  94.53 ( 96.62)
Epoch: [33][320/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.001)	Loss 5.2024e-01 (6.7469e-01)	Acc@1  85.16 ( 79.99)	Acc@5  97.66 ( 96.62)
Epoch: [33][330/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.9045e-01 (6.7533e-01)	Acc@1  83.59 ( 79.96)	Acc@5 100.00 ( 96.62)
Epoch: [33][340/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.001)	Loss 6.3878e-01 (6.7517e-01)	Acc@1  81.25 ( 79.97)	Acc@5  96.88 ( 96.59)
Epoch: [33][350/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 9.1683e-01 (6.7627e-01)	Acc@1  70.31 ( 79.94)	Acc@5  96.09 ( 96.59)
Epoch: [33][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3478e-01 (6.7529e-01)	Acc@1  83.59 ( 79.94)	Acc@5 100.00 ( 96.61)
Epoch: [33][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.9572e-01 (6.7553e-01)	Acc@1  77.34 ( 79.91)	Acc@5  97.66 ( 96.61)
Epoch: [33][380/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.001)	Loss 6.3959e-01 (6.7584e-01)	Acc@1  82.03 ( 79.93)	Acc@5  97.66 ( 96.62)
Epoch: [33][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4465e-01 (6.7464e-01)	Acc@1  81.25 ( 79.96)	Acc@5 100.00 ( 96.64)
## e[33] optimizer.zero_grad (sum) time: 0.26610493659973145
## e[33]       loss.backward (sum) time: 4.10272216796875
## e[33]      optimizer.step (sum) time: 1.8073198795318604
## epoch[33] training(only) time: 16.264585971832275
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.1820e+00 (1.1820e+00)	Acc@1  69.00 ( 69.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 1.2664e+00 (1.2529e+00)	Acc@1  63.00 ( 66.64)	Acc@5  90.00 ( 90.00)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 1.0050e+00 (1.1838e+00)	Acc@1  75.00 ( 68.10)	Acc@5  91.00 ( 90.67)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.3492e+00 (1.1930e+00)	Acc@1  62.00 ( 67.48)	Acc@5  91.00 ( 90.68)
Test: [ 40/100]	Time  0.026 ( 0.025)	Loss 1.2134e+00 (1.1929e+00)	Acc@1  65.00 ( 67.12)	Acc@5  91.00 ( 90.76)
Test: [ 50/100]	Time  0.026 ( 0.025)	Loss 1.2212e+00 (1.2029e+00)	Acc@1  70.00 ( 67.02)	Acc@5  92.00 ( 90.63)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 1.3238e+00 (1.1844e+00)	Acc@1  63.00 ( 67.33)	Acc@5  90.00 ( 90.89)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 1.2244e+00 (1.1853e+00)	Acc@1  64.00 ( 67.28)	Acc@5  92.00 ( 90.79)
Test: [ 80/100]	Time  0.019 ( 0.024)	Loss 1.3243e+00 (1.1960e+00)	Acc@1  70.00 ( 67.28)	Acc@5  90.00 ( 90.57)
Test: [ 90/100]	Time  0.019 ( 0.024)	Loss 1.3837e+00 (1.1833e+00)	Acc@1  69.00 ( 67.75)	Acc@5  89.00 ( 90.70)
 * Acc@1 67.950 Acc@5 90.860
### epoch[33] execution time: 18.65621781349182
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.201 ( 0.201)	Data  0.151 ( 0.151)	Loss 8.1596e-01 (8.1596e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.09 ( 96.09)
Epoch: [34][ 10/391]	Time  0.038 ( 0.057)	Data  0.001 ( 0.015)	Loss 4.6555e-01 (7.0034e-01)	Acc@1  86.72 ( 79.47)	Acc@5  98.44 ( 96.88)
Epoch: [34][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.008)	Loss 7.1170e-01 (6.7377e-01)	Acc@1  73.44 ( 80.32)	Acc@5  98.44 ( 96.95)
Epoch: [34][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.9759e-01 (6.5397e-01)	Acc@1  85.16 ( 80.90)	Acc@5  99.22 ( 96.98)
Epoch: [34][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 8.0967e-01 (6.5010e-01)	Acc@1  74.22 ( 80.85)	Acc@5  92.19 ( 96.93)
Epoch: [34][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 7.1116e-01 (6.5397e-01)	Acc@1  80.47 ( 80.93)	Acc@5  94.53 ( 96.81)
Epoch: [34][ 60/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.7375e-01 (6.5326e-01)	Acc@1  85.94 ( 80.85)	Acc@5  98.44 ( 96.81)
Epoch: [34][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.1991e-01 (6.5886e-01)	Acc@1  81.25 ( 80.73)	Acc@5  97.66 ( 96.73)
Epoch: [34][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.3964e-01 (6.5794e-01)	Acc@1  81.25 ( 80.70)	Acc@5  96.09 ( 96.77)
Epoch: [34][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.7723e-01 (6.5782e-01)	Acc@1  82.81 ( 80.67)	Acc@5  97.66 ( 96.78)
Epoch: [34][100/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.2709e-01 (6.6024e-01)	Acc@1  78.12 ( 80.58)	Acc@5  95.31 ( 96.78)
Epoch: [34][110/391]	Time  0.049 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.5498e-01 (6.5991e-01)	Acc@1  78.12 ( 80.56)	Acc@5  96.09 ( 96.73)
Epoch: [34][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0377e-01 (6.6042e-01)	Acc@1  82.81 ( 80.53)	Acc@5  96.88 ( 96.73)
Epoch: [34][130/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.6950e-01 (6.6213e-01)	Acc@1  75.78 ( 80.50)	Acc@5  96.88 ( 96.71)
Epoch: [34][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1054e-01 (6.6605e-01)	Acc@1  80.47 ( 80.33)	Acc@5  96.88 ( 96.69)
Epoch: [34][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.6956e-01 (6.6302e-01)	Acc@1  83.59 ( 80.39)	Acc@5  98.44 ( 96.73)
Epoch: [34][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.5036e-01 (6.6051e-01)	Acc@1  82.03 ( 80.40)	Acc@5  96.09 ( 96.77)
Epoch: [34][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.5449e-01 (6.6166e-01)	Acc@1  81.25 ( 80.35)	Acc@5  96.88 ( 96.75)
Epoch: [34][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8083e-01 (6.6004e-01)	Acc@1  85.16 ( 80.40)	Acc@5  96.88 ( 96.74)
Epoch: [34][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.5906e-01 (6.6252e-01)	Acc@1  75.00 ( 80.30)	Acc@5  96.09 ( 96.73)
Epoch: [34][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.6038e-01 (6.6378e-01)	Acc@1  76.56 ( 80.30)	Acc@5  96.88 ( 96.71)
Epoch: [34][210/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.5379e-01 (6.6386e-01)	Acc@1  82.81 ( 80.36)	Acc@5  95.31 ( 96.69)
Epoch: [34][220/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.9221e-01 (6.6288e-01)	Acc@1  76.56 ( 80.35)	Acc@5  98.44 ( 96.71)
Epoch: [34][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.4919e-01 (6.6365e-01)	Acc@1  80.47 ( 80.33)	Acc@5  96.09 ( 96.71)
Epoch: [34][240/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4368e-01 (6.6207e-01)	Acc@1  90.62 ( 80.43)	Acc@5  98.44 ( 96.71)
Epoch: [34][250/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0311e-01 (6.6119e-01)	Acc@1  82.81 ( 80.49)	Acc@5  96.88 ( 96.70)
Epoch: [34][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8586e-01 (6.6207e-01)	Acc@1  78.91 ( 80.47)	Acc@5  95.31 ( 96.69)
Epoch: [34][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5018e-01 (6.6153e-01)	Acc@1  78.91 ( 80.46)	Acc@5  96.09 ( 96.71)
Epoch: [34][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5426e-01 (6.6147e-01)	Acc@1  75.78 ( 80.43)	Acc@5  96.09 ( 96.72)
Epoch: [34][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7851e-01 (6.6234e-01)	Acc@1  84.38 ( 80.38)	Acc@5  97.66 ( 96.71)
Epoch: [34][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0450e-01 (6.6109e-01)	Acc@1  76.56 ( 80.42)	Acc@5  95.31 ( 96.71)
Epoch: [34][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4748e-01 (6.6096e-01)	Acc@1  81.25 ( 80.44)	Acc@5  96.09 ( 96.70)
Epoch: [34][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9400e-01 (6.6076e-01)	Acc@1  76.56 ( 80.43)	Acc@5  95.31 ( 96.72)
Epoch: [34][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9134e-01 (6.6062e-01)	Acc@1  85.94 ( 80.44)	Acc@5  98.44 ( 96.72)
Epoch: [34][340/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8627e-01 (6.6071e-01)	Acc@1  79.69 ( 80.44)	Acc@5  94.53 ( 96.71)
Epoch: [34][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7957e-01 (6.6114e-01)	Acc@1  81.25 ( 80.41)	Acc@5  98.44 ( 96.70)
Epoch: [34][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0294e-01 (6.6005e-01)	Acc@1  84.38 ( 80.43)	Acc@5  99.22 ( 96.70)
Epoch: [34][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.7286e-01 (6.6168e-01)	Acc@1  78.91 ( 80.37)	Acc@5  97.66 ( 96.70)
Epoch: [34][380/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.001)	Loss 6.7572e-01 (6.6224e-01)	Acc@1  79.69 ( 80.33)	Acc@5  95.31 ( 96.70)
Epoch: [34][390/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.1888e-01 (6.6288e-01)	Acc@1  76.25 ( 80.33)	Acc@5  98.75 ( 96.68)
## e[34] optimizer.zero_grad (sum) time: 0.26516079902648926
## e[34]       loss.backward (sum) time: 4.080219507217407
## e[34]      optimizer.step (sum) time: 1.7956550121307373
## epoch[34] training(only) time: 16.177271604537964
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.1660e+00 (1.1660e+00)	Acc@1  69.00 ( 69.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.1690e+00 (1.2257e+00)	Acc@1  67.00 ( 69.18)	Acc@5  91.00 ( 90.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.0254e+00 (1.1742e+00)	Acc@1  74.00 ( 68.95)	Acc@5  92.00 ( 90.90)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.3645e+00 (1.1876e+00)	Acc@1  61.00 ( 68.19)	Acc@5  90.00 ( 90.65)
Test: [ 40/100]	Time  0.018 ( 0.025)	Loss 1.1827e+00 (1.1881e+00)	Acc@1  67.00 ( 67.68)	Acc@5  94.00 ( 90.90)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.2657e+00 (1.1968e+00)	Acc@1  65.00 ( 67.51)	Acc@5  91.00 ( 90.61)
Test: [ 60/100]	Time  0.016 ( 0.023)	Loss 1.3274e+00 (1.1809e+00)	Acc@1  64.00 ( 67.85)	Acc@5  89.00 ( 90.85)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 1.2090e+00 (1.1777e+00)	Acc@1  64.00 ( 67.89)	Acc@5  90.00 ( 90.83)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 1.2706e+00 (1.1895e+00)	Acc@1  75.00 ( 67.89)	Acc@5  88.00 ( 90.59)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 1.3493e+00 (1.1783e+00)	Acc@1  68.00 ( 68.18)	Acc@5  89.00 ( 90.76)
 * Acc@1 68.350 Acc@5 90.940
### epoch[34] execution time: 18.49521541595459
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.202 ( 0.202)	Data  0.150 ( 0.150)	Loss 5.9867e-01 (5.9867e-01)	Acc@1  83.59 ( 83.59)	Acc@5  96.88 ( 96.88)
Epoch: [35][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.015)	Loss 5.8430e-01 (6.6335e-01)	Acc@1  81.25 ( 80.54)	Acc@5  98.44 ( 96.45)
Epoch: [35][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.008)	Loss 7.5568e-01 (6.5030e-01)	Acc@1  77.34 ( 80.77)	Acc@5  95.31 ( 96.88)
Epoch: [35][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.3174e-01 (6.4381e-01)	Acc@1  77.34 ( 81.17)	Acc@5  96.88 ( 96.77)
Epoch: [35][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.7213e-01 (6.3788e-01)	Acc@1  80.47 ( 81.19)	Acc@5  96.88 ( 96.72)
Epoch: [35][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.1238e-01 (6.3192e-01)	Acc@1  78.91 ( 81.37)	Acc@5  94.53 ( 96.78)
Epoch: [35][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.4493e-01 (6.2842e-01)	Acc@1  85.16 ( 81.33)	Acc@5  99.22 ( 96.96)
Epoch: [35][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.3054e-01 (6.2906e-01)	Acc@1  83.59 ( 81.40)	Acc@5  96.88 ( 96.99)
Epoch: [35][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.3496e-01 (6.2585e-01)	Acc@1  80.47 ( 81.44)	Acc@5  93.75 ( 97.05)
Epoch: [35][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.1878e-01 (6.3128e-01)	Acc@1  85.16 ( 81.39)	Acc@5  97.66 ( 96.99)
Epoch: [35][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.4399e-01 (6.2496e-01)	Acc@1  82.81 ( 81.52)	Acc@5  96.09 ( 97.05)
Epoch: [35][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7403e-01 (6.2550e-01)	Acc@1  83.59 ( 81.51)	Acc@5 100.00 ( 97.04)
Epoch: [35][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.5739e-01 (6.2894e-01)	Acc@1  81.25 ( 81.44)	Acc@5  96.09 ( 96.97)
Epoch: [35][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.1335e-01 (6.3043e-01)	Acc@1  78.91 ( 81.35)	Acc@5  97.66 ( 96.96)
Epoch: [35][140/391]	Time  0.038 ( 0.042)	Data  0.002 ( 0.002)	Loss 7.4016e-01 (6.3006e-01)	Acc@1  78.91 ( 81.29)	Acc@5  96.88 ( 96.93)
Epoch: [35][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5651e-01 (6.3355e-01)	Acc@1  78.91 ( 81.17)	Acc@5  96.88 ( 96.90)
Epoch: [35][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4872e-01 (6.3578e-01)	Acc@1  80.47 ( 81.21)	Acc@5  96.09 ( 96.86)
Epoch: [35][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4866e-01 (6.3663e-01)	Acc@1  75.78 ( 81.15)	Acc@5  95.31 ( 96.84)
Epoch: [35][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6227e-01 (6.3756e-01)	Acc@1  84.38 ( 81.09)	Acc@5  97.66 ( 96.83)
Epoch: [35][190/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6365e-01 (6.3950e-01)	Acc@1  78.12 ( 81.02)	Acc@5  95.31 ( 96.81)
Epoch: [35][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6114e-01 (6.3984e-01)	Acc@1  81.25 ( 81.00)	Acc@5  97.66 ( 96.82)
Epoch: [35][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2965e-01 (6.3956e-01)	Acc@1  84.38 ( 81.03)	Acc@5  98.44 ( 96.83)
Epoch: [35][220/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9686e-01 (6.4091e-01)	Acc@1  79.69 ( 80.98)	Acc@5 100.00 ( 96.82)
Epoch: [35][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4613e-01 (6.4019e-01)	Acc@1  75.78 ( 80.98)	Acc@5  92.97 ( 96.84)
Epoch: [35][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5004e-01 (6.4330e-01)	Acc@1  83.59 ( 80.87)	Acc@5  98.44 ( 96.81)
Epoch: [35][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3468e-01 (6.4406e-01)	Acc@1  81.25 ( 80.86)	Acc@5  96.88 ( 96.81)
Epoch: [35][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8789e-01 (6.4190e-01)	Acc@1  81.25 ( 80.92)	Acc@5  96.88 ( 96.82)
Epoch: [35][270/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.8231e-01 (6.4429e-01)	Acc@1  75.00 ( 80.84)	Acc@5  95.31 ( 96.79)
Epoch: [35][280/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.2186e-01 (6.4335e-01)	Acc@1  82.03 ( 80.83)	Acc@5  99.22 ( 96.80)
Epoch: [35][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2234e-01 (6.4368e-01)	Acc@1  82.81 ( 80.83)	Acc@5  94.53 ( 96.80)
Epoch: [35][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8648e-01 (6.4583e-01)	Acc@1  85.94 ( 80.77)	Acc@5  95.31 ( 96.79)
Epoch: [35][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0039e-01 (6.4534e-01)	Acc@1  79.69 ( 80.76)	Acc@5  99.22 ( 96.81)
Epoch: [35][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2731e-01 (6.4418e-01)	Acc@1  83.59 ( 80.78)	Acc@5  98.44 ( 96.84)
Epoch: [35][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6491e-01 (6.4370e-01)	Acc@1  75.00 ( 80.79)	Acc@5  98.44 ( 96.84)
Epoch: [35][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1021e-01 (6.4271e-01)	Acc@1  82.81 ( 80.85)	Acc@5  99.22 ( 96.84)
Epoch: [35][350/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8460e-01 (6.4419e-01)	Acc@1  77.34 ( 80.85)	Acc@5  99.22 ( 96.82)
Epoch: [35][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0299e-01 (6.4472e-01)	Acc@1  78.91 ( 80.83)	Acc@5  93.75 ( 96.80)
Epoch: [35][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8291e-01 (6.4566e-01)	Acc@1  82.03 ( 80.83)	Acc@5  98.44 ( 96.79)
Epoch: [35][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.5801e-01 (6.4649e-01)	Acc@1  74.22 ( 80.80)	Acc@5  94.53 ( 96.79)
Epoch: [35][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.7438e-01 (6.4751e-01)	Acc@1  71.25 ( 80.77)	Acc@5  96.25 ( 96.78)
## e[35] optimizer.zero_grad (sum) time: 0.26435327529907227
## e[35]       loss.backward (sum) time: 3.998222589492798
## e[35]      optimizer.step (sum) time: 1.8354990482330322
## epoch[35] training(only) time: 16.086957454681396
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.1841e+00 (1.1841e+00)	Acc@1  69.00 ( 69.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.024 ( 0.037)	Loss 1.1833e+00 (1.2665e+00)	Acc@1  66.00 ( 66.36)	Acc@5  91.00 ( 89.64)
Test: [ 20/100]	Time  0.021 ( 0.031)	Loss 9.7792e-01 (1.2019e+00)	Acc@1  73.00 ( 67.67)	Acc@5  93.00 ( 90.62)
Test: [ 30/100]	Time  0.020 ( 0.027)	Loss 1.4154e+00 (1.2111e+00)	Acc@1  61.00 ( 67.26)	Acc@5  90.00 ( 90.74)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.1687e+00 (1.2110e+00)	Acc@1  69.00 ( 67.07)	Acc@5  94.00 ( 90.95)
Test: [ 50/100]	Time  0.025 ( 0.024)	Loss 1.1743e+00 (1.2152e+00)	Acc@1  69.00 ( 67.06)	Acc@5  92.00 ( 90.69)
Test: [ 60/100]	Time  0.018 ( 0.024)	Loss 1.3570e+00 (1.1977e+00)	Acc@1  65.00 ( 67.26)	Acc@5  90.00 ( 90.97)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.2568e+00 (1.1958e+00)	Acc@1  63.00 ( 67.27)	Acc@5  89.00 ( 90.90)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 1.3376e+00 (1.2068e+00)	Acc@1  73.00 ( 67.37)	Acc@5  90.00 ( 90.74)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.3778e+00 (1.1938e+00)	Acc@1  69.00 ( 67.84)	Acc@5  89.00 ( 90.84)
 * Acc@1 67.950 Acc@5 90.980
### epoch[35] execution time: 18.439377307891846
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.201 ( 0.201)	Data  0.147 ( 0.147)	Loss 4.4213e-01 (4.4213e-01)	Acc@1  89.06 ( 89.06)	Acc@5  98.44 ( 98.44)
Epoch: [36][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.014)	Loss 6.4086e-01 (6.4724e-01)	Acc@1  78.91 ( 80.54)	Acc@5  98.44 ( 97.73)
Epoch: [36][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 5.8876e-01 (6.1513e-01)	Acc@1  81.25 ( 81.44)	Acc@5  98.44 ( 97.84)
Epoch: [36][ 30/391]	Time  0.040 ( 0.046)	Data  0.002 ( 0.006)	Loss 5.9342e-01 (6.0100e-01)	Acc@1  79.69 ( 81.83)	Acc@5  97.66 ( 97.91)
Epoch: [36][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.0172e-01 (5.9957e-01)	Acc@1  80.47 ( 81.78)	Acc@5 100.00 ( 97.90)
Epoch: [36][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 7.0124e-01 (6.1040e-01)	Acc@1  77.34 ( 81.69)	Acc@5  96.88 ( 97.66)
Epoch: [36][ 60/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.004)	Loss 6.8131e-01 (6.0934e-01)	Acc@1  82.03 ( 81.70)	Acc@5  96.09 ( 97.64)
Epoch: [36][ 70/391]	Time  0.043 ( 0.043)	Data  0.002 ( 0.003)	Loss 7.0487e-01 (6.0776e-01)	Acc@1  79.69 ( 81.88)	Acc@5  95.31 ( 97.48)
Epoch: [36][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.5300e-01 (6.0988e-01)	Acc@1  83.59 ( 81.78)	Acc@5  96.09 ( 97.38)
Epoch: [36][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.3152e-01 (6.1442e-01)	Acc@1  75.00 ( 81.64)	Acc@5  92.97 ( 97.26)
Epoch: [36][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1213e-01 (6.0976e-01)	Acc@1  81.25 ( 81.83)	Acc@5  95.31 ( 97.29)
Epoch: [36][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4803e-01 (6.1213e-01)	Acc@1  82.03 ( 81.78)	Acc@5  96.09 ( 97.21)
Epoch: [36][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.7997e-01 (6.1771e-01)	Acc@1  83.59 ( 81.60)	Acc@5  94.53 ( 97.15)
Epoch: [36][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9024e-01 (6.1732e-01)	Acc@1  85.16 ( 81.60)	Acc@5  97.66 ( 97.13)
Epoch: [36][140/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.9568e-01 (6.2240e-01)	Acc@1  77.34 ( 81.43)	Acc@5  94.53 ( 97.05)
Epoch: [36][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.8688e-01 (6.2324e-01)	Acc@1  75.78 ( 81.41)	Acc@5  95.31 ( 97.06)
Epoch: [36][160/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 6.4047e-01 (6.2533e-01)	Acc@1  80.47 ( 81.30)	Acc@5  96.09 ( 97.04)
Epoch: [36][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.5554e-01 (6.2756e-01)	Acc@1  75.00 ( 81.28)	Acc@5  93.75 ( 96.99)
Epoch: [36][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8046e-01 (6.2663e-01)	Acc@1  85.94 ( 81.25)	Acc@5  98.44 ( 97.00)
Epoch: [36][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8923e-01 (6.2708e-01)	Acc@1  86.72 ( 81.20)	Acc@5  96.09 ( 96.97)
Epoch: [36][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2029e-01 (6.2767e-01)	Acc@1  81.25 ( 81.19)	Acc@5  95.31 ( 96.96)
Epoch: [36][210/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2110e-01 (6.2809e-01)	Acc@1  82.03 ( 81.22)	Acc@5  96.88 ( 96.97)
Epoch: [36][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1927e-01 (6.2761e-01)	Acc@1  80.47 ( 81.22)	Acc@5  98.44 ( 96.98)
Epoch: [36][230/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.002)	Loss 6.2004e-01 (6.2782e-01)	Acc@1  78.12 ( 81.20)	Acc@5  96.88 ( 96.97)
Epoch: [36][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1683e-01 (6.2809e-01)	Acc@1  78.91 ( 81.20)	Acc@5  96.88 ( 97.00)
Epoch: [36][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9580e-01 (6.2824e-01)	Acc@1  85.94 ( 81.17)	Acc@5  97.66 ( 97.00)
Epoch: [36][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3992e-01 (6.2836e-01)	Acc@1  78.91 ( 81.17)	Acc@5  97.66 ( 97.00)
Epoch: [36][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5437e-01 (6.3005e-01)	Acc@1  82.03 ( 81.14)	Acc@5  96.09 ( 96.97)
Epoch: [36][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.6742e-01 (6.3064e-01)	Acc@1  72.66 ( 81.13)	Acc@5  93.75 ( 96.96)
Epoch: [36][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3835e-01 (6.3063e-01)	Acc@1  75.78 ( 81.11)	Acc@5  94.53 ( 96.95)
Epoch: [36][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5044e-01 (6.3059e-01)	Acc@1  80.47 ( 81.12)	Acc@5  97.66 ( 96.97)
Epoch: [36][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0584e-01 (6.2984e-01)	Acc@1  80.47 ( 81.13)	Acc@5  99.22 ( 96.98)
Epoch: [36][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2520e-01 (6.3016e-01)	Acc@1  85.94 ( 81.13)	Acc@5  97.66 ( 96.97)
Epoch: [36][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9451e-01 (6.3124e-01)	Acc@1  79.69 ( 81.08)	Acc@5  97.66 ( 96.96)
Epoch: [36][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2327e-01 (6.3364e-01)	Acc@1  84.38 ( 81.03)	Acc@5  96.88 ( 96.93)
Epoch: [36][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8122e-01 (6.3451e-01)	Acc@1  81.25 ( 80.99)	Acc@5  96.88 ( 96.94)
Epoch: [36][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7368e-01 (6.3554e-01)	Acc@1  84.38 ( 80.94)	Acc@5  99.22 ( 96.95)
Epoch: [36][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1460e-01 (6.3601e-01)	Acc@1  75.78 ( 80.96)	Acc@5  96.09 ( 96.94)
Epoch: [36][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2200e-01 (6.3589e-01)	Acc@1  82.03 ( 80.98)	Acc@5  96.88 ( 96.94)
Epoch: [36][390/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.8942e-01 (6.3641e-01)	Acc@1  77.50 ( 80.96)	Acc@5  96.25 ( 96.92)
## e[36] optimizer.zero_grad (sum) time: 0.267319917678833
## e[36]       loss.backward (sum) time: 4.059027671813965
## e[36]      optimizer.step (sum) time: 1.8189294338226318
## epoch[36] training(only) time: 16.14427661895752
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1627e+00 (1.1627e+00)	Acc@1  69.00 ( 69.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.1752e+00 (1.2551e+00)	Acc@1  65.00 ( 67.27)	Acc@5  92.00 ( 90.27)
Test: [ 20/100]	Time  0.024 ( 0.029)	Loss 9.4537e-01 (1.1928e+00)	Acc@1  77.00 ( 68.90)	Acc@5  94.00 ( 91.05)
Test: [ 30/100]	Time  0.017 ( 0.026)	Loss 1.3799e+00 (1.2008e+00)	Acc@1  62.00 ( 68.68)	Acc@5  93.00 ( 90.68)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 1.2095e+00 (1.2003e+00)	Acc@1  69.00 ( 68.22)	Acc@5  92.00 ( 90.83)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.1479e+00 (1.2063e+00)	Acc@1  70.00 ( 67.96)	Acc@5  91.00 ( 90.51)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.3210e+00 (1.1879e+00)	Acc@1  63.00 ( 68.21)	Acc@5  91.00 ( 90.80)
Test: [ 70/100]	Time  0.024 ( 0.023)	Loss 1.2466e+00 (1.1885e+00)	Acc@1  63.00 ( 68.10)	Acc@5  90.00 ( 90.79)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 1.2821e+00 (1.1966e+00)	Acc@1  73.00 ( 68.04)	Acc@5  88.00 ( 90.62)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 1.3635e+00 (1.1832e+00)	Acc@1  69.00 ( 68.41)	Acc@5  90.00 ( 90.82)
 * Acc@1 68.410 Acc@5 91.010
### epoch[36] execution time: 18.48199486732483
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.206 ( 0.206)	Data  0.151 ( 0.151)	Loss 6.2151e-01 (6.2151e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [37][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.015)	Loss 5.0445e-01 (5.7444e-01)	Acc@1  89.06 ( 83.59)	Acc@5  96.88 ( 97.59)
Epoch: [37][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 5.5142e-01 (5.9277e-01)	Acc@1  82.03 ( 82.37)	Acc@5  98.44 ( 97.51)
Epoch: [37][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 7.6019e-01 (5.9515e-01)	Acc@1  75.00 ( 82.59)	Acc@5  93.75 ( 97.25)
Epoch: [37][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.2430e-01 (6.1076e-01)	Acc@1  83.59 ( 82.13)	Acc@5  96.09 ( 97.12)
Epoch: [37][ 50/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.8319e-01 (6.0862e-01)	Acc@1  84.38 ( 82.29)	Acc@5  95.31 ( 97.00)
Epoch: [37][ 60/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.6302e-01 (6.0833e-01)	Acc@1  85.16 ( 82.34)	Acc@5  96.09 ( 97.02)
Epoch: [37][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.9676e-01 (6.1054e-01)	Acc@1  83.59 ( 82.30)	Acc@5  98.44 ( 97.05)
Epoch: [37][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.4419e-01 (6.1496e-01)	Acc@1  82.03 ( 82.11)	Acc@5  93.75 ( 96.99)
Epoch: [37][ 90/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.5158e-01 (6.1683e-01)	Acc@1  83.59 ( 81.93)	Acc@5  99.22 ( 97.06)
Epoch: [37][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.2694e-01 (6.1970e-01)	Acc@1  82.03 ( 81.85)	Acc@5  96.88 ( 97.04)
Epoch: [37][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5056e-01 (6.2094e-01)	Acc@1  85.94 ( 81.80)	Acc@5  96.09 ( 97.02)
Epoch: [37][120/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8983e-01 (6.2255e-01)	Acc@1  87.50 ( 81.75)	Acc@5  97.66 ( 96.98)
Epoch: [37][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9442e-01 (6.2200e-01)	Acc@1  84.38 ( 81.71)	Acc@5  96.88 ( 97.02)
Epoch: [37][140/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2464e-01 (6.1854e-01)	Acc@1  78.91 ( 81.75)	Acc@5  96.88 ( 97.07)
Epoch: [37][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1455e-01 (6.1691e-01)	Acc@1  83.59 ( 81.79)	Acc@5  97.66 ( 97.09)
Epoch: [37][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7730e-01 (6.1393e-01)	Acc@1  82.81 ( 81.87)	Acc@5  96.09 ( 97.13)
Epoch: [37][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8804e-01 (6.1457e-01)	Acc@1  82.81 ( 81.86)	Acc@5  95.31 ( 97.14)
Epoch: [37][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3227e-01 (6.1752e-01)	Acc@1  81.25 ( 81.75)	Acc@5  98.44 ( 97.12)
Epoch: [37][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3615e-01 (6.1483e-01)	Acc@1  83.59 ( 81.82)	Acc@5  98.44 ( 97.16)
Epoch: [37][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9799e-01 (6.1559e-01)	Acc@1  80.47 ( 81.76)	Acc@5  96.09 ( 97.14)
Epoch: [37][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2374e-01 (6.1528e-01)	Acc@1  84.38 ( 81.71)	Acc@5  99.22 ( 97.20)
Epoch: [37][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8687e-01 (6.1572e-01)	Acc@1  82.03 ( 81.67)	Acc@5  97.66 ( 97.17)
Epoch: [37][230/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.4661e-01 (6.1871e-01)	Acc@1  85.94 ( 81.61)	Acc@5  97.66 ( 97.11)
Epoch: [37][240/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.1596e-01 (6.2006e-01)	Acc@1  74.22 ( 81.53)	Acc@5  93.75 ( 97.11)
Epoch: [37][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8931e-01 (6.2092e-01)	Acc@1  74.22 ( 81.45)	Acc@5  96.88 ( 97.09)
Epoch: [37][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.9689e-01 (6.2229e-01)	Acc@1  76.56 ( 81.42)	Acc@5  97.66 ( 97.08)
Epoch: [37][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1466e-01 (6.2074e-01)	Acc@1  85.16 ( 81.46)	Acc@5 100.00 ( 97.10)
Epoch: [37][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5833e-01 (6.2163e-01)	Acc@1  85.16 ( 81.44)	Acc@5  95.31 ( 97.10)
Epoch: [37][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6387e-01 (6.2311e-01)	Acc@1  80.47 ( 81.40)	Acc@5  96.88 ( 97.10)
Epoch: [37][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8166e-01 (6.2158e-01)	Acc@1  83.59 ( 81.46)	Acc@5  96.88 ( 97.11)
Epoch: [37][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7733e-01 (6.2075e-01)	Acc@1  85.94 ( 81.49)	Acc@5  97.66 ( 97.10)
Epoch: [37][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0500e-01 (6.2200e-01)	Acc@1  87.50 ( 81.47)	Acc@5  98.44 ( 97.08)
Epoch: [37][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9575e-01 (6.2351e-01)	Acc@1  78.91 ( 81.43)	Acc@5  96.09 ( 97.07)
Epoch: [37][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5033e-01 (6.2231e-01)	Acc@1  81.25 ( 81.43)	Acc@5  97.66 ( 97.09)
Epoch: [37][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7861e-01 (6.2150e-01)	Acc@1  81.25 ( 81.45)	Acc@5  95.31 ( 97.10)
Epoch: [37][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2075e-01 (6.2080e-01)	Acc@1  87.50 ( 81.48)	Acc@5  95.31 ( 97.08)
Epoch: [37][370/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4395e-01 (6.2172e-01)	Acc@1  82.81 ( 81.42)	Acc@5  99.22 ( 97.08)
Epoch: [37][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.0514e-01 (6.2243e-01)	Acc@1  75.78 ( 81.40)	Acc@5  96.09 ( 97.06)
Epoch: [37][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0743e-01 (6.2277e-01)	Acc@1  82.50 ( 81.39)	Acc@5  97.50 ( 97.07)
## e[37] optimizer.zero_grad (sum) time: 0.2660825252532959
## e[37]       loss.backward (sum) time: 4.018016815185547
## e[37]      optimizer.step (sum) time: 1.8656542301177979
## epoch[37] training(only) time: 16.075441598892212
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2057e+00 (1.2057e+00)	Acc@1  70.00 ( 70.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.1953e+00 (1.2556e+00)	Acc@1  67.00 ( 67.64)	Acc@5  92.00 ( 90.09)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 9.9532e-01 (1.2006e+00)	Acc@1  76.00 ( 68.48)	Acc@5  93.00 ( 90.90)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 1.3492e+00 (1.2108e+00)	Acc@1  60.00 ( 67.74)	Acc@5  91.00 ( 90.74)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 1.2116e+00 (1.2105e+00)	Acc@1  66.00 ( 67.49)	Acc@5  94.00 ( 90.88)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 1.1438e+00 (1.2166e+00)	Acc@1  68.00 ( 67.25)	Acc@5  93.00 ( 90.69)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 1.2919e+00 (1.1998e+00)	Acc@1  65.00 ( 67.54)	Acc@5  92.00 ( 91.02)
Test: [ 70/100]	Time  0.023 ( 0.022)	Loss 1.2770e+00 (1.2007e+00)	Acc@1  64.00 ( 67.42)	Acc@5  88.00 ( 90.89)
Test: [ 80/100]	Time  0.024 ( 0.022)	Loss 1.2385e+00 (1.2095e+00)	Acc@1  73.00 ( 67.41)	Acc@5  89.00 ( 90.70)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 1.3704e+00 (1.1970e+00)	Acc@1  67.00 ( 67.68)	Acc@5  88.00 ( 90.82)
 * Acc@1 67.790 Acc@5 90.980
### epoch[37] execution time: 18.347861289978027
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.202 ( 0.202)	Data  0.148 ( 0.148)	Loss 6.9163e-01 (6.9163e-01)	Acc@1  80.47 ( 80.47)	Acc@5  93.75 ( 93.75)
Epoch: [38][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.014)	Loss 6.0667e-01 (5.9074e-01)	Acc@1  81.25 ( 81.68)	Acc@5  98.44 ( 97.66)
Epoch: [38][ 20/391]	Time  0.040 ( 0.050)	Data  0.002 ( 0.008)	Loss 6.5625e-01 (5.9686e-01)	Acc@1  81.25 ( 82.18)	Acc@5  96.09 ( 97.32)
Epoch: [38][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.006)	Loss 5.8761e-01 (5.7900e-01)	Acc@1  82.03 ( 82.91)	Acc@5  96.09 ( 97.33)
Epoch: [38][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.5956e-01 (5.8261e-01)	Acc@1  87.50 ( 82.76)	Acc@5 100.00 ( 97.41)
Epoch: [38][ 50/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.004)	Loss 5.0545e-01 (5.8524e-01)	Acc@1  85.16 ( 82.66)	Acc@5  97.66 ( 97.35)
Epoch: [38][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.003)	Loss 6.1940e-01 (5.8657e-01)	Acc@1  80.47 ( 82.42)	Acc@5  96.09 ( 97.41)
Epoch: [38][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.4475e-01 (5.8903e-01)	Acc@1  85.16 ( 82.48)	Acc@5  97.66 ( 97.34)
Epoch: [38][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.6095e-01 (5.8412e-01)	Acc@1  80.47 ( 82.59)	Acc@5  98.44 ( 97.42)
Epoch: [38][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.4029e-01 (5.8590e-01)	Acc@1  83.59 ( 82.62)	Acc@5  97.66 ( 97.37)
Epoch: [38][100/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.7218e-01 (5.8939e-01)	Acc@1  82.03 ( 82.44)	Acc@5  95.31 ( 97.33)
Epoch: [38][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.4304e-01 (5.9251e-01)	Acc@1  75.78 ( 82.30)	Acc@5  96.09 ( 97.28)
Epoch: [38][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7079e-01 (5.9355e-01)	Acc@1  83.59 ( 82.19)	Acc@5  97.66 ( 97.29)
Epoch: [38][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0457e-01 (5.9523e-01)	Acc@1  81.25 ( 82.08)	Acc@5  98.44 ( 97.33)
Epoch: [38][140/391]	Time  0.038 ( 0.042)	Data  0.002 ( 0.002)	Loss 6.7657e-01 (5.9438e-01)	Acc@1  82.81 ( 82.23)	Acc@5  96.09 ( 97.31)
Epoch: [38][150/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1497e-01 (5.9433e-01)	Acc@1  78.91 ( 82.21)	Acc@5  98.44 ( 97.31)
Epoch: [38][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.3733e-01 (5.9428e-01)	Acc@1  75.78 ( 82.16)	Acc@5  93.75 ( 97.31)
Epoch: [38][170/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.9922e-01 (5.9672e-01)	Acc@1  76.56 ( 82.11)	Acc@5  93.75 ( 97.30)
Epoch: [38][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 8.7754e-01 (6.0029e-01)	Acc@1  75.00 ( 81.96)	Acc@5  94.53 ( 97.27)
Epoch: [38][190/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8101e-01 (6.0055e-01)	Acc@1  78.12 ( 81.92)	Acc@5  96.09 ( 97.27)
Epoch: [38][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1344e-01 (6.0164e-01)	Acc@1  80.47 ( 81.90)	Acc@5  97.66 ( 97.25)
Epoch: [38][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.8970e-01 (6.0591e-01)	Acc@1  79.69 ( 81.84)	Acc@5  98.44 ( 97.22)
Epoch: [38][220/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 6.6757e-01 (6.0721e-01)	Acc@1  83.59 ( 81.79)	Acc@5  96.09 ( 97.18)
Epoch: [38][230/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8901e-01 (6.0954e-01)	Acc@1  78.12 ( 81.73)	Acc@5  97.66 ( 97.18)
Epoch: [38][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4654e-01 (6.0969e-01)	Acc@1  83.59 ( 81.77)	Acc@5  96.88 ( 97.18)
Epoch: [38][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2700e-01 (6.0936e-01)	Acc@1  83.59 ( 81.79)	Acc@5  96.09 ( 97.20)
Epoch: [38][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9345e-01 (6.0863e-01)	Acc@1  85.16 ( 81.80)	Acc@5  98.44 ( 97.20)
Epoch: [38][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.0012e-01 (6.0871e-01)	Acc@1  74.22 ( 81.75)	Acc@5  96.09 ( 97.21)
Epoch: [38][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0106e-01 (6.0922e-01)	Acc@1  82.81 ( 81.74)	Acc@5  96.88 ( 97.20)
Epoch: [38][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1561e-01 (6.0879e-01)	Acc@1  88.28 ( 81.76)	Acc@5  98.44 ( 97.19)
Epoch: [38][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4481e-01 (6.0939e-01)	Acc@1  77.34 ( 81.75)	Acc@5  98.44 ( 97.18)
Epoch: [38][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0127e-01 (6.0841e-01)	Acc@1  85.16 ( 81.75)	Acc@5  99.22 ( 97.20)
Epoch: [38][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8412e-01 (6.0902e-01)	Acc@1  81.25 ( 81.71)	Acc@5  99.22 ( 97.20)
Epoch: [38][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1942e-01 (6.0877e-01)	Acc@1  80.47 ( 81.71)	Acc@5  97.66 ( 97.21)
Epoch: [38][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3230e-01 (6.0923e-01)	Acc@1  75.78 ( 81.69)	Acc@5  94.53 ( 97.20)
Epoch: [38][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4019e-01 (6.0901e-01)	Acc@1  83.59 ( 81.72)	Acc@5  98.44 ( 97.18)
Epoch: [38][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1851e-01 (6.0951e-01)	Acc@1  80.47 ( 81.73)	Acc@5  97.66 ( 97.19)
Epoch: [38][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6656e-01 (6.0930e-01)	Acc@1  81.25 ( 81.74)	Acc@5  97.66 ( 97.18)
Epoch: [38][380/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.001)	Loss 7.1234e-01 (6.1084e-01)	Acc@1  77.34 ( 81.68)	Acc@5  95.31 ( 97.16)
Epoch: [38][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 8.2717e-01 (6.1213e-01)	Acc@1  72.50 ( 81.64)	Acc@5  93.75 ( 97.15)
## e[38] optimizer.zero_grad (sum) time: 0.2654683589935303
## e[38]       loss.backward (sum) time: 4.014990329742432
## e[38]      optimizer.step (sum) time: 1.8578424453735352
## epoch[38] training(only) time: 16.13098168373108
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2660e+00 (1.2660e+00)	Acc@1  69.00 ( 69.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.022 ( 0.033)	Loss 1.1992e+00 (1.2570e+00)	Acc@1  66.00 ( 68.18)	Acc@5  91.00 ( 89.27)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 9.7595e-01 (1.1886e+00)	Acc@1  78.00 ( 69.05)	Acc@5  93.00 ( 90.67)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 1.3432e+00 (1.2132e+00)	Acc@1  61.00 ( 68.06)	Acc@5  91.00 ( 90.77)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 1.2221e+00 (1.2118e+00)	Acc@1  67.00 ( 67.93)	Acc@5  93.00 ( 91.02)
Test: [ 50/100]	Time  0.024 ( 0.023)	Loss 1.1848e+00 (1.2186e+00)	Acc@1  67.00 ( 67.69)	Acc@5  92.00 ( 90.69)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.2981e+00 (1.2012e+00)	Acc@1  66.00 ( 68.03)	Acc@5  91.00 ( 91.02)
Test: [ 70/100]	Time  0.026 ( 0.022)	Loss 1.1838e+00 (1.1979e+00)	Acc@1  67.00 ( 68.14)	Acc@5  91.00 ( 91.00)
Test: [ 80/100]	Time  0.018 ( 0.022)	Loss 1.3422e+00 (1.2070e+00)	Acc@1  69.00 ( 67.98)	Acc@5  89.00 ( 90.75)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 1.3986e+00 (1.1942e+00)	Acc@1  68.00 ( 68.42)	Acc@5  90.00 ( 90.93)
 * Acc@1 68.530 Acc@5 91.020
### epoch[38] execution time: 18.41213345527649
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.205 ( 0.205)	Data  0.153 ( 0.153)	Loss 4.5367e-01 (4.5367e-01)	Acc@1  84.38 ( 84.38)	Acc@5  96.88 ( 96.88)
Epoch: [39][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.015)	Loss 6.6026e-01 (5.7271e-01)	Acc@1  82.03 ( 83.10)	Acc@5  95.31 ( 97.09)
Epoch: [39][ 20/391]	Time  0.040 ( 0.049)	Data  0.002 ( 0.008)	Loss 7.3058e-01 (5.8953e-01)	Acc@1  74.22 ( 82.22)	Acc@5  96.09 ( 97.10)
Epoch: [39][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.5863e-01 (5.8704e-01)	Acc@1  84.38 ( 82.66)	Acc@5  96.88 ( 97.05)
Epoch: [39][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.7141e-01 (5.9408e-01)	Acc@1  82.81 ( 82.49)	Acc@5  95.31 ( 96.93)
Epoch: [39][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.4120e-01 (5.9204e-01)	Acc@1  84.38 ( 82.55)	Acc@5  96.09 ( 96.97)
Epoch: [39][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.2970e-01 (5.9581e-01)	Acc@1  84.38 ( 82.22)	Acc@5  99.22 ( 97.07)
Epoch: [39][ 70/391]	Time  0.040 ( 0.043)	Data  0.002 ( 0.003)	Loss 5.8981e-01 (5.9238e-01)	Acc@1  85.16 ( 82.30)	Acc@5  96.09 ( 97.18)
Epoch: [39][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.3940e-01 (5.9336e-01)	Acc@1  78.12 ( 82.23)	Acc@5  97.66 ( 97.19)
Epoch: [39][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.4813e-01 (5.9072e-01)	Acc@1  76.56 ( 82.25)	Acc@5  97.66 ( 97.24)
Epoch: [39][100/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.7416e-01 (5.9003e-01)	Acc@1  79.69 ( 82.46)	Acc@5  97.66 ( 97.21)
Epoch: [39][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.6019e-01 (5.9265e-01)	Acc@1  80.47 ( 82.37)	Acc@5  94.53 ( 97.21)
Epoch: [39][120/391]	Time  0.039 ( 0.042)	Data  0.002 ( 0.002)	Loss 5.0527e-01 (5.9189e-01)	Acc@1  85.16 ( 82.42)	Acc@5  96.88 ( 97.18)
Epoch: [39][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.3819e-01 (5.9229e-01)	Acc@1  80.47 ( 82.42)	Acc@5  96.88 ( 97.21)
Epoch: [39][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.0113e-01 (5.9452e-01)	Acc@1  78.12 ( 82.31)	Acc@5  96.09 ( 97.20)
Epoch: [39][150/391]	Time  0.044 ( 0.042)	Data  0.002 ( 0.002)	Loss 5.7007e-01 (5.9728e-01)	Acc@1  83.59 ( 82.14)	Acc@5  96.88 ( 97.17)
Epoch: [39][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3710e-01 (5.9560e-01)	Acc@1  83.59 ( 82.27)	Acc@5  96.88 ( 97.19)
Epoch: [39][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4374e-01 (5.9573e-01)	Acc@1  82.03 ( 82.23)	Acc@5  98.44 ( 97.16)
Epoch: [39][180/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7268e-01 (5.9283e-01)	Acc@1  85.16 ( 82.28)	Acc@5  96.09 ( 97.20)
Epoch: [39][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5353e-01 (5.8938e-01)	Acc@1  84.38 ( 82.40)	Acc@5  96.88 ( 97.23)
Epoch: [39][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1821e-01 (5.8699e-01)	Acc@1  82.81 ( 82.46)	Acc@5  99.22 ( 97.26)
Epoch: [39][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.6261e-01 (5.8795e-01)	Acc@1  79.69 ( 82.46)	Acc@5  96.88 ( 97.28)
Epoch: [39][220/391]	Time  0.053 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9331e-01 (5.8901e-01)	Acc@1  85.16 ( 82.43)	Acc@5  98.44 ( 97.29)
Epoch: [39][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0240e-01 (5.8859e-01)	Acc@1  81.25 ( 82.44)	Acc@5  97.66 ( 97.29)
Epoch: [39][240/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1982e-01 (5.8995e-01)	Acc@1  86.72 ( 82.42)	Acc@5  96.09 ( 97.27)
Epoch: [39][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1241e-01 (5.9080e-01)	Acc@1  82.81 ( 82.37)	Acc@5  97.66 ( 97.25)
Epoch: [39][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5998e-01 (5.8805e-01)	Acc@1  89.06 ( 82.47)	Acc@5  97.66 ( 97.28)
Epoch: [39][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8329e-01 (5.9207e-01)	Acc@1  80.47 ( 82.39)	Acc@5  94.53 ( 97.20)
Epoch: [39][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9180e-01 (5.9174e-01)	Acc@1  83.59 ( 82.38)	Acc@5  97.66 ( 97.21)
Epoch: [39][290/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1451e-01 (5.9437e-01)	Acc@1  81.25 ( 82.29)	Acc@5  97.66 ( 97.18)
Epoch: [39][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2916e-01 (5.9569e-01)	Acc@1  79.69 ( 82.25)	Acc@5  93.75 ( 97.15)
Epoch: [39][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0854e-01 (5.9551e-01)	Acc@1  82.81 ( 82.23)	Acc@5  97.66 ( 97.18)
Epoch: [39][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7791e-01 (5.9595e-01)	Acc@1  81.25 ( 82.21)	Acc@5  97.66 ( 97.17)
Epoch: [39][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9959e-01 (5.9545e-01)	Acc@1  82.81 ( 82.18)	Acc@5  99.22 ( 97.17)
Epoch: [39][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1727e-01 (5.9661e-01)	Acc@1  83.59 ( 82.18)	Acc@5  96.09 ( 97.17)
Epoch: [39][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5086e-01 (5.9512e-01)	Acc@1  85.16 ( 82.22)	Acc@5  96.88 ( 97.18)
Epoch: [39][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4435e-01 (5.9615e-01)	Acc@1  82.81 ( 82.17)	Acc@5 100.00 ( 97.16)
Epoch: [39][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9043e-01 (5.9678e-01)	Acc@1  85.16 ( 82.16)	Acc@5  98.44 ( 97.16)
Epoch: [39][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0104e-01 (5.9586e-01)	Acc@1  84.38 ( 82.17)	Acc@5  96.09 ( 97.18)
Epoch: [39][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9373e-01 (5.9613e-01)	Acc@1  85.00 ( 82.16)	Acc@5 100.00 ( 97.19)
## e[39] optimizer.zero_grad (sum) time: 0.2658975124359131
## e[39]       loss.backward (sum) time: 4.083463907241821
## e[39]      optimizer.step (sum) time: 1.8388819694519043
## epoch[39] training(only) time: 16.205585718154907
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 1.1854e+00 (1.1854e+00)	Acc@1  71.00 ( 71.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 1.1623e+00 (1.2682e+00)	Acc@1  66.00 ( 67.91)	Acc@5  91.00 ( 88.91)
Test: [ 20/100]	Time  0.023 ( 0.030)	Loss 1.0469e+00 (1.2003e+00)	Acc@1  76.00 ( 68.67)	Acc@5  93.00 ( 90.38)
Test: [ 30/100]	Time  0.025 ( 0.027)	Loss 1.3550e+00 (1.2109e+00)	Acc@1  61.00 ( 68.10)	Acc@5  92.00 ( 90.42)
Test: [ 40/100]	Time  0.018 ( 0.026)	Loss 1.1806e+00 (1.2150e+00)	Acc@1  69.00 ( 68.22)	Acc@5  92.00 ( 90.78)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.1700e+00 (1.2188e+00)	Acc@1  69.00 ( 68.00)	Acc@5  91.00 ( 90.71)
Test: [ 60/100]	Time  0.020 ( 0.024)	Loss 1.3473e+00 (1.2037e+00)	Acc@1  65.00 ( 68.00)	Acc@5  91.00 ( 91.03)
Test: [ 70/100]	Time  0.018 ( 0.024)	Loss 1.3508e+00 (1.2059e+00)	Acc@1  63.00 ( 67.92)	Acc@5  89.00 ( 90.96)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.2731e+00 (1.2147e+00)	Acc@1  74.00 ( 67.86)	Acc@5  89.00 ( 90.80)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.3459e+00 (1.1999e+00)	Acc@1  68.00 ( 68.25)	Acc@5  90.00 ( 91.05)
 * Acc@1 68.300 Acc@5 91.170
### epoch[39] execution time: 18.515617847442627
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.210 ( 0.210)	Data  0.157 ( 0.157)	Loss 6.1566e-01 (6.1566e-01)	Acc@1  78.12 ( 78.12)	Acc@5  97.66 ( 97.66)
Epoch: [40][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.015)	Loss 6.5325e-01 (6.3210e-01)	Acc@1  82.81 ( 80.75)	Acc@5  95.31 ( 96.95)
Epoch: [40][ 20/391]	Time  0.039 ( 0.050)	Data  0.001 ( 0.009)	Loss 4.9990e-01 (6.0569e-01)	Acc@1  84.38 ( 81.51)	Acc@5  96.88 ( 97.21)
Epoch: [40][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.006)	Loss 7.0211e-01 (6.0121e-01)	Acc@1  78.91 ( 81.85)	Acc@5  96.09 ( 97.20)
Epoch: [40][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.7421e-01 (5.8577e-01)	Acc@1  82.81 ( 82.49)	Acc@5  96.88 ( 97.39)
Epoch: [40][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.7824e-01 (5.8077e-01)	Acc@1  85.94 ( 82.60)	Acc@5  98.44 ( 97.49)
Epoch: [40][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.2724e-01 (5.7978e-01)	Acc@1  84.38 ( 82.65)	Acc@5  98.44 ( 97.43)
Epoch: [40][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.6564e-01 (5.7987e-01)	Acc@1  78.91 ( 82.64)	Acc@5  92.19 ( 97.39)
Epoch: [40][ 80/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.8051e-01 (5.8052e-01)	Acc@1  82.03 ( 82.47)	Acc@5  96.09 ( 97.40)
Epoch: [40][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.2610e-01 (5.8298e-01)	Acc@1  87.50 ( 82.36)	Acc@5  98.44 ( 97.44)
Epoch: [40][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2399e-01 (5.8270e-01)	Acc@1  86.72 ( 82.44)	Acc@5  99.22 ( 97.46)
Epoch: [40][110/391]	Time  0.048 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.4381e-01 (5.8526e-01)	Acc@1  78.12 ( 82.37)	Acc@5  97.66 ( 97.42)
Epoch: [40][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.5127e-01 (5.8970e-01)	Acc@1  81.25 ( 82.21)	Acc@5  95.31 ( 97.35)
Epoch: [40][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2193e-01 (5.8751e-01)	Acc@1  81.25 ( 82.26)	Acc@5  99.22 ( 97.41)
Epoch: [40][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8207e-01 (5.8730e-01)	Acc@1  83.59 ( 82.24)	Acc@5  99.22 ( 97.40)
Epoch: [40][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.2262e-01 (5.8862e-01)	Acc@1  78.12 ( 82.23)	Acc@5  97.66 ( 97.39)
Epoch: [40][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0940e-01 (5.8725e-01)	Acc@1  84.38 ( 82.36)	Acc@5  97.66 ( 97.40)
Epoch: [40][170/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1915e-01 (5.8638e-01)	Acc@1  85.94 ( 82.42)	Acc@5  96.88 ( 97.40)
Epoch: [40][180/391]	Time  0.044 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.4161e-01 (5.8435e-01)	Acc@1  84.38 ( 82.44)	Acc@5  99.22 ( 97.44)
Epoch: [40][190/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.7179e-01 (5.8293e-01)	Acc@1  83.59 ( 82.46)	Acc@5  99.22 ( 97.47)
Epoch: [40][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2303e-01 (5.8213e-01)	Acc@1  86.72 ( 82.52)	Acc@5  98.44 ( 97.47)
Epoch: [40][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2183e-01 (5.8135e-01)	Acc@1  84.38 ( 82.52)	Acc@5  98.44 ( 97.48)
Epoch: [40][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3324e-01 (5.8167e-01)	Acc@1  86.72 ( 82.53)	Acc@5  97.66 ( 97.47)
Epoch: [40][230/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2390e-01 (5.8170e-01)	Acc@1  81.25 ( 82.52)	Acc@5  99.22 ( 97.47)
Epoch: [40][240/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1166e-01 (5.8177e-01)	Acc@1  84.38 ( 82.52)	Acc@5  99.22 ( 97.46)
Epoch: [40][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9278e-01 (5.8124e-01)	Acc@1  75.78 ( 82.51)	Acc@5  96.09 ( 97.47)
Epoch: [40][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6388e-01 (5.8192e-01)	Acc@1  75.78 ( 82.50)	Acc@5  96.09 ( 97.45)
Epoch: [40][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8511e-01 (5.8205e-01)	Acc@1  81.25 ( 82.49)	Acc@5  98.44 ( 97.44)
Epoch: [40][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5943e-01 (5.8152e-01)	Acc@1  77.34 ( 82.52)	Acc@5  96.88 ( 97.42)
Epoch: [40][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3243e-01 (5.8233e-01)	Acc@1  83.59 ( 82.49)	Acc@5  94.53 ( 97.40)
Epoch: [40][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2636e-01 (5.8115e-01)	Acc@1  88.28 ( 82.54)	Acc@5  98.44 ( 97.41)
Epoch: [40][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5672e-01 (5.8281e-01)	Acc@1  75.78 ( 82.44)	Acc@5  98.44 ( 97.39)
Epoch: [40][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6115e-01 (5.8370e-01)	Acc@1  81.25 ( 82.38)	Acc@5  96.09 ( 97.38)
Epoch: [40][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6701e-01 (5.8510e-01)	Acc@1  79.69 ( 82.35)	Acc@5  99.22 ( 97.35)
Epoch: [40][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5954e-01 (5.8467e-01)	Acc@1  84.38 ( 82.37)	Acc@5  98.44 ( 97.35)
Epoch: [40][350/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.9954e-01 (5.8519e-01)	Acc@1  85.16 ( 82.40)	Acc@5  99.22 ( 97.34)
Epoch: [40][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8280e-01 (5.8522e-01)	Acc@1  82.81 ( 82.38)	Acc@5  96.88 ( 97.35)
Epoch: [40][370/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.4974e-01 (5.8652e-01)	Acc@1  82.81 ( 82.34)	Acc@5  98.44 ( 97.36)
Epoch: [40][380/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2263e-01 (5.8604e-01)	Acc@1  86.72 ( 82.35)	Acc@5  99.22 ( 97.37)
Epoch: [40][390/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.6565e-01 (5.8773e-01)	Acc@1  76.25 ( 82.32)	Acc@5  93.75 ( 97.35)
## e[40] optimizer.zero_grad (sum) time: 0.26611971855163574
## e[40]       loss.backward (sum) time: 4.054152250289917
## e[40]      optimizer.step (sum) time: 1.8419196605682373
## epoch[40] training(only) time: 16.09037685394287
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2220e+00 (1.2220e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 1.2047e+00 (1.2651e+00)	Acc@1  60.00 ( 66.64)	Acc@5  93.00 ( 90.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.0224e+00 (1.2133e+00)	Acc@1  78.00 ( 68.00)	Acc@5  92.00 ( 90.62)
Test: [ 30/100]	Time  0.018 ( 0.027)	Loss 1.3754e+00 (1.2278e+00)	Acc@1  61.00 ( 67.23)	Acc@5  90.00 ( 90.58)
Test: [ 40/100]	Time  0.017 ( 0.025)	Loss 1.2515e+00 (1.2326e+00)	Acc@1  68.00 ( 67.07)	Acc@5  91.00 ( 90.78)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.2545e+00 (1.2424e+00)	Acc@1  70.00 ( 66.94)	Acc@5  90.00 ( 90.43)
Test: [ 60/100]	Time  0.026 ( 0.024)	Loss 1.3828e+00 (1.2242e+00)	Acc@1  63.00 ( 67.38)	Acc@5  90.00 ( 90.79)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.2458e+00 (1.2224e+00)	Acc@1  65.00 ( 67.48)	Acc@5  90.00 ( 90.82)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 1.2316e+00 (1.2269e+00)	Acc@1  71.00 ( 67.52)	Acc@5  90.00 ( 90.72)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 1.4366e+00 (1.2141e+00)	Acc@1  67.00 ( 67.81)	Acc@5  88.00 ( 90.82)
 * Acc@1 68.040 Acc@5 90.940
### epoch[40] execution time: 18.453965187072754
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.200 ( 0.200)	Data  0.146 ( 0.146)	Loss 6.0132e-01 (6.0132e-01)	Acc@1  77.34 ( 77.34)	Acc@5  99.22 ( 99.22)
Epoch: [41][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.014)	Loss 6.3376e-01 (5.7209e-01)	Acc@1  78.91 ( 81.96)	Acc@5  96.09 ( 97.87)
Epoch: [41][ 20/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.008)	Loss 5.2179e-01 (5.4640e-01)	Acc@1  82.81 ( 82.44)	Acc@5  97.66 ( 98.14)
Epoch: [41][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.5686e-01 (5.3215e-01)	Acc@1  81.25 ( 83.04)	Acc@5  98.44 ( 98.11)
Epoch: [41][ 40/391]	Time  0.035 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.9327e-01 (5.3815e-01)	Acc@1  86.72 ( 83.19)	Acc@5  98.44 ( 98.00)
Epoch: [41][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.4649e-01 (5.4555e-01)	Acc@1  88.28 ( 83.18)	Acc@5  97.66 ( 97.84)
Epoch: [41][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.2347e-01 (5.5556e-01)	Acc@1  82.81 ( 82.86)	Acc@5  99.22 ( 97.77)
Epoch: [41][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.9679e-01 (5.5149e-01)	Acc@1  87.50 ( 83.07)	Acc@5  99.22 ( 97.83)
Epoch: [41][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.6145e-01 (5.5390e-01)	Acc@1  83.59 ( 83.07)	Acc@5  97.66 ( 97.83)
Epoch: [41][ 90/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.3946e-01 (5.5861e-01)	Acc@1  78.91 ( 82.96)	Acc@5  96.88 ( 97.81)
Epoch: [41][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.5248e-01 (5.6136e-01)	Acc@1  79.69 ( 82.83)	Acc@5  97.66 ( 97.77)
Epoch: [41][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7200e-01 (5.6504e-01)	Acc@1  81.25 ( 82.66)	Acc@5  99.22 ( 97.73)
Epoch: [41][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0266e-01 (5.7066e-01)	Acc@1  84.38 ( 82.62)	Acc@5  96.88 ( 97.66)
Epoch: [41][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2958e-01 (5.6936e-01)	Acc@1  80.47 ( 82.66)	Acc@5  98.44 ( 97.69)
Epoch: [41][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3374e-01 (5.7188e-01)	Acc@1  81.25 ( 82.60)	Acc@5 100.00 ( 97.62)
Epoch: [41][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3226e-01 (5.7631e-01)	Acc@1  81.25 ( 82.50)	Acc@5  98.44 ( 97.59)
Epoch: [41][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1000e-01 (5.7698e-01)	Acc@1  82.03 ( 82.50)	Acc@5  98.44 ( 97.59)
Epoch: [41][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1717e-01 (5.7722e-01)	Acc@1  79.69 ( 82.50)	Acc@5 100.00 ( 97.58)
Epoch: [41][180/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6498e-01 (5.7551e-01)	Acc@1  85.16 ( 82.49)	Acc@5 100.00 ( 97.60)
Epoch: [41][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8401e-01 (5.7415e-01)	Acc@1  85.16 ( 82.54)	Acc@5  96.88 ( 97.61)
Epoch: [41][200/391]	Time  0.045 ( 0.042)	Data  0.002 ( 0.002)	Loss 6.2003e-01 (5.7493e-01)	Acc@1  80.47 ( 82.54)	Acc@5  96.09 ( 97.59)
Epoch: [41][210/391]	Time  0.044 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.8955e-01 (5.7477e-01)	Acc@1  85.94 ( 82.50)	Acc@5  97.66 ( 97.58)
Epoch: [41][220/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9335e-01 (5.7423e-01)	Acc@1  83.59 ( 82.54)	Acc@5  98.44 ( 97.59)
Epoch: [41][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.3790e-01 (5.7543e-01)	Acc@1  76.56 ( 82.47)	Acc@5  95.31 ( 97.58)
Epoch: [41][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7985e-01 (5.7583e-01)	Acc@1  85.94 ( 82.49)	Acc@5  96.88 ( 97.57)
Epoch: [41][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1710e-01 (5.7710e-01)	Acc@1  85.94 ( 82.46)	Acc@5  96.88 ( 97.54)
Epoch: [41][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2770e-01 (5.7622e-01)	Acc@1  85.16 ( 82.48)	Acc@5  98.44 ( 97.55)
Epoch: [41][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6061e-01 (5.7749e-01)	Acc@1  82.03 ( 82.46)	Acc@5  93.75 ( 97.53)
Epoch: [41][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6441e-01 (5.7785e-01)	Acc@1  82.81 ( 82.46)	Acc@5  97.66 ( 97.52)
Epoch: [41][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0331e-01 (5.7980e-01)	Acc@1  84.38 ( 82.38)	Acc@5  99.22 ( 97.51)
Epoch: [41][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5133e-01 (5.8151e-01)	Acc@1  80.47 ( 82.35)	Acc@5  96.88 ( 97.48)
Epoch: [41][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5038e-01 (5.8058e-01)	Acc@1  82.03 ( 82.35)	Acc@5  99.22 ( 97.50)
Epoch: [41][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.6813e-01 (5.8162e-01)	Acc@1  76.56 ( 82.31)	Acc@5  96.09 ( 97.49)
Epoch: [41][330/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3146e-01 (5.8164e-01)	Acc@1  82.03 ( 82.29)	Acc@5  98.44 ( 97.50)
Epoch: [41][340/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 7.4479e-01 (5.8197e-01)	Acc@1  79.69 ( 82.31)	Acc@5  95.31 ( 97.48)
Epoch: [41][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0568e-01 (5.8365e-01)	Acc@1  82.03 ( 82.27)	Acc@5  95.31 ( 97.46)
Epoch: [41][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9354e-01 (5.8451e-01)	Acc@1  80.47 ( 82.24)	Acc@5  97.66 ( 97.45)
Epoch: [41][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0645e-01 (5.8322e-01)	Acc@1  82.81 ( 82.30)	Acc@5  97.66 ( 97.46)
Epoch: [41][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1044e-01 (5.8388e-01)	Acc@1  82.03 ( 82.27)	Acc@5  94.53 ( 97.44)
Epoch: [41][390/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3156e-01 (5.8366e-01)	Acc@1  82.50 ( 82.26)	Acc@5 100.00 ( 97.43)
## e[41] optimizer.zero_grad (sum) time: 0.2648134231567383
## e[41]       loss.backward (sum) time: 3.9993081092834473
## e[41]      optimizer.step (sum) time: 1.8322217464447021
## epoch[41] training(only) time: 16.16395664215088
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2724e+00 (1.2724e+00)	Acc@1  70.00 ( 70.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 1.1569e+00 (1.2994e+00)	Acc@1  65.00 ( 66.73)	Acc@5  93.00 ( 89.45)
Test: [ 20/100]	Time  0.019 ( 0.029)	Loss 9.9954e-01 (1.2380e+00)	Acc@1  75.00 ( 68.24)	Acc@5  93.00 ( 90.38)
Test: [ 30/100]	Time  0.017 ( 0.026)	Loss 1.3523e+00 (1.2486e+00)	Acc@1  64.00 ( 67.74)	Acc@5  90.00 ( 90.45)
Test: [ 40/100]	Time  0.016 ( 0.024)	Loss 1.2133e+00 (1.2449e+00)	Acc@1  68.00 ( 67.61)	Acc@5  93.00 ( 90.73)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 1.2277e+00 (1.2516e+00)	Acc@1  69.00 ( 67.37)	Acc@5  93.00 ( 90.43)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.3457e+00 (1.2278e+00)	Acc@1  63.00 ( 67.77)	Acc@5  91.00 ( 90.84)
Test: [ 70/100]	Time  0.019 ( 0.023)	Loss 1.2092e+00 (1.2274e+00)	Acc@1  67.00 ( 67.87)	Acc@5  89.00 ( 90.73)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.2665e+00 (1.2359e+00)	Acc@1  71.00 ( 67.75)	Acc@5  90.00 ( 90.60)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 1.4502e+00 (1.2226e+00)	Acc@1  68.00 ( 68.11)	Acc@5  87.00 ( 90.67)
 * Acc@1 68.160 Acc@5 90.780
### epoch[41] execution time: 18.452627182006836
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.200 ( 0.200)	Data  0.148 ( 0.148)	Loss 6.6825e-01 (6.6825e-01)	Acc@1  79.69 ( 79.69)	Acc@5  96.09 ( 96.09)
Epoch: [42][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.014)	Loss 5.3571e-01 (5.8398e-01)	Acc@1  82.03 ( 82.39)	Acc@5  98.44 ( 96.95)
Epoch: [42][ 20/391]	Time  0.044 ( 0.050)	Data  0.001 ( 0.008)	Loss 5.8559e-01 (5.4715e-01)	Acc@1  84.38 ( 83.56)	Acc@5  96.09 ( 97.47)
Epoch: [42][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.006)	Loss 7.1130e-01 (5.4472e-01)	Acc@1  78.91 ( 83.64)	Acc@5  93.75 ( 97.48)
Epoch: [42][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.5388e-01 (5.4304e-01)	Acc@1  89.06 ( 83.77)	Acc@5  99.22 ( 97.52)
Epoch: [42][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.2263e-01 (5.3914e-01)	Acc@1  82.03 ( 84.02)	Acc@5  96.09 ( 97.59)
Epoch: [42][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.3856e-01 (5.4372e-01)	Acc@1  84.38 ( 83.85)	Acc@5  99.22 ( 97.62)
Epoch: [42][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.8387e-01 (5.4800e-01)	Acc@1  85.16 ( 83.68)	Acc@5  99.22 ( 97.59)
Epoch: [42][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.8453e-01 (5.5659e-01)	Acc@1  81.25 ( 83.40)	Acc@5  98.44 ( 97.52)
Epoch: [42][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.5092e-01 (5.5955e-01)	Acc@1  84.38 ( 83.28)	Acc@5  95.31 ( 97.48)
Epoch: [42][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2354e-01 (5.6335e-01)	Acc@1  85.16 ( 83.19)	Acc@5 100.00 ( 97.42)
Epoch: [42][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1022e-01 (5.6368e-01)	Acc@1  83.59 ( 83.16)	Acc@5  97.66 ( 97.44)
Epoch: [42][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2087e-01 (5.6249e-01)	Acc@1  86.72 ( 83.23)	Acc@5  96.88 ( 97.45)
Epoch: [42][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8431e-01 (5.6266e-01)	Acc@1  80.47 ( 83.21)	Acc@5  97.66 ( 97.44)
Epoch: [42][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.4452e-01 (5.6633e-01)	Acc@1  78.12 ( 83.07)	Acc@5  96.09 ( 97.41)
Epoch: [42][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3395e-01 (5.6384e-01)	Acc@1  79.69 ( 83.12)	Acc@5  98.44 ( 97.45)
Epoch: [42][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1029e-01 (5.6191e-01)	Acc@1  86.72 ( 83.13)	Acc@5  97.66 ( 97.49)
Epoch: [42][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.6667e-01 (5.6072e-01)	Acc@1  81.25 ( 83.19)	Acc@5  97.66 ( 97.46)
Epoch: [42][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2987e-01 (5.6141e-01)	Acc@1  79.69 ( 83.10)	Acc@5  98.44 ( 97.48)
Epoch: [42][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4849e-01 (5.6166e-01)	Acc@1  82.81 ( 83.10)	Acc@5  96.88 ( 97.45)
Epoch: [42][200/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2083e-01 (5.6315e-01)	Acc@1  77.34 ( 83.03)	Acc@5  97.66 ( 97.46)
Epoch: [42][210/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1273e-01 (5.6080e-01)	Acc@1  84.38 ( 83.13)	Acc@5  98.44 ( 97.49)
Epoch: [42][220/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5447e-01 (5.6048e-01)	Acc@1  84.38 ( 83.13)	Acc@5  96.09 ( 97.49)
Epoch: [42][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8137e-01 (5.6185e-01)	Acc@1  86.72 ( 83.11)	Acc@5  98.44 ( 97.50)
Epoch: [42][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6769e-01 (5.6243e-01)	Acc@1  85.16 ( 83.09)	Acc@5  98.44 ( 97.50)
Epoch: [42][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9778e-01 (5.6246e-01)	Acc@1  85.94 ( 83.10)	Acc@5  97.66 ( 97.49)
Epoch: [42][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3265e-01 (5.6258e-01)	Acc@1  84.38 ( 83.10)	Acc@5 100.00 ( 97.49)
Epoch: [42][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9695e-01 (5.6377e-01)	Acc@1  78.91 ( 83.10)	Acc@5  97.66 ( 97.48)
Epoch: [42][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8604e-01 (5.6488e-01)	Acc@1  81.25 ( 83.08)	Acc@5  94.53 ( 97.48)
Epoch: [42][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4694e-01 (5.6608e-01)	Acc@1  87.50 ( 83.06)	Acc@5  97.66 ( 97.47)
Epoch: [42][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1761e-01 (5.6718e-01)	Acc@1  81.25 ( 83.00)	Acc@5  97.66 ( 97.46)
Epoch: [42][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.5149e-01 (5.6727e-01)	Acc@1  78.91 ( 83.02)	Acc@5  95.31 ( 97.46)
Epoch: [42][320/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.6638e-01 (5.6745e-01)	Acc@1  80.47 ( 82.98)	Acc@5  97.66 ( 97.45)
Epoch: [42][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5273e-01 (5.6722e-01)	Acc@1  78.91 ( 82.96)	Acc@5  97.66 ( 97.44)
Epoch: [42][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.3245e-01 (5.6808e-01)	Acc@1  79.69 ( 82.95)	Acc@5  97.66 ( 97.44)
Epoch: [42][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7821e-01 (5.6829e-01)	Acc@1  83.59 ( 82.90)	Acc@5 100.00 ( 97.44)
Epoch: [42][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.1072e-01 (5.6915e-01)	Acc@1  87.50 ( 82.89)	Acc@5  97.66 ( 97.42)
Epoch: [42][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4058e-01 (5.7034e-01)	Acc@1  85.16 ( 82.85)	Acc@5  96.88 ( 97.42)
Epoch: [42][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3647e-01 (5.7072e-01)	Acc@1  85.16 ( 82.84)	Acc@5  97.66 ( 97.44)
Epoch: [42][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7700e-01 (5.7112e-01)	Acc@1  90.00 ( 82.82)	Acc@5  96.25 ( 97.45)
## e[42] optimizer.zero_grad (sum) time: 0.26739931106567383
## e[42]       loss.backward (sum) time: 4.037632465362549
## e[42]      optimizer.step (sum) time: 1.8316771984100342
## epoch[42] training(only) time: 16.14656090736389
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2065e+00 (1.2065e+00)	Acc@1  69.00 ( 69.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 1.1744e+00 (1.2971e+00)	Acc@1  63.00 ( 67.18)	Acc@5  93.00 ( 90.00)
Test: [ 20/100]	Time  0.019 ( 0.027)	Loss 1.0539e+00 (1.2295e+00)	Acc@1  74.00 ( 68.38)	Acc@5  92.00 ( 91.14)
Test: [ 30/100]	Time  0.021 ( 0.026)	Loss 1.3443e+00 (1.2374e+00)	Acc@1  63.00 ( 67.84)	Acc@5  90.00 ( 90.90)
Test: [ 40/100]	Time  0.024 ( 0.024)	Loss 1.1554e+00 (1.2380e+00)	Acc@1  69.00 ( 67.56)	Acc@5  95.00 ( 91.00)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.2483e+00 (1.2467e+00)	Acc@1  70.00 ( 67.31)	Acc@5  89.00 ( 90.73)
Test: [ 60/100]	Time  0.022 ( 0.023)	Loss 1.3112e+00 (1.2273e+00)	Acc@1  62.00 ( 67.64)	Acc@5  92.00 ( 91.05)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 1.3498e+00 (1.2269e+00)	Acc@1  66.00 ( 67.66)	Acc@5  89.00 ( 90.92)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 1.2531e+00 (1.2304e+00)	Acc@1  73.00 ( 67.65)	Acc@5  91.00 ( 90.77)
Test: [ 90/100]	Time  0.021 ( 0.022)	Loss 1.4223e+00 (1.2161e+00)	Acc@1  67.00 ( 68.03)	Acc@5  90.00 ( 90.89)
 * Acc@1 68.180 Acc@5 91.040
### epoch[42] execution time: 18.434149742126465
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.201 ( 0.201)	Data  0.149 ( 0.149)	Loss 7.6521e-01 (7.6521e-01)	Acc@1  79.69 ( 79.69)	Acc@5  96.09 ( 96.09)
Epoch: [43][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.014)	Loss 6.6147e-01 (5.8023e-01)	Acc@1  78.12 ( 82.67)	Acc@5  97.66 ( 97.66)
Epoch: [43][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 5.2413e-01 (5.6588e-01)	Acc@1  84.38 ( 82.78)	Acc@5  95.31 ( 97.51)
Epoch: [43][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.4911e-01 (5.6302e-01)	Acc@1  82.03 ( 82.76)	Acc@5  97.66 ( 97.51)
Epoch: [43][ 40/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.0617e-01 (5.6079e-01)	Acc@1  84.38 ( 82.87)	Acc@5  99.22 ( 97.54)
Epoch: [43][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.0657e-01 (5.5786e-01)	Acc@1  82.03 ( 82.94)	Acc@5  96.09 ( 97.53)
Epoch: [43][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 5.4078e-01 (5.5278e-01)	Acc@1  83.59 ( 83.09)	Acc@5  97.66 ( 97.68)
Epoch: [43][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.5659e-01 (5.4489e-01)	Acc@1  82.03 ( 83.48)	Acc@5  98.44 ( 97.67)
Epoch: [43][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.7859e-01 (5.4291e-01)	Acc@1  85.94 ( 83.53)	Acc@5  96.88 ( 97.72)
Epoch: [43][ 90/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.4831e-01 (5.4248e-01)	Acc@1  78.91 ( 83.41)	Acc@5  99.22 ( 97.76)
Epoch: [43][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.5275e-01 (5.4675e-01)	Acc@1  77.34 ( 83.37)	Acc@5  95.31 ( 97.72)
Epoch: [43][110/391]	Time  0.038 ( 0.042)	Data  0.002 ( 0.002)	Loss 5.0989e-01 (5.4425e-01)	Acc@1  85.94 ( 83.42)	Acc@5  98.44 ( 97.74)
Epoch: [43][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9865e-01 (5.4661e-01)	Acc@1  83.59 ( 83.39)	Acc@5  99.22 ( 97.73)
Epoch: [43][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3565e-01 (5.4822e-01)	Acc@1  88.28 ( 83.43)	Acc@5  96.88 ( 97.70)
Epoch: [43][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0315e-01 (5.5029e-01)	Acc@1  81.25 ( 83.36)	Acc@5  98.44 ( 97.70)
Epoch: [43][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7949e-01 (5.5068e-01)	Acc@1  84.38 ( 83.38)	Acc@5  97.66 ( 97.67)
Epoch: [43][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8862e-01 (5.4985e-01)	Acc@1  82.81 ( 83.38)	Acc@5  96.88 ( 97.69)
Epoch: [43][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.6560e-01 (5.5050e-01)	Acc@1  80.47 ( 83.37)	Acc@5  97.66 ( 97.70)
Epoch: [43][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8839e-01 (5.4993e-01)	Acc@1  82.03 ( 83.41)	Acc@5  97.66 ( 97.72)
Epoch: [43][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5821e-01 (5.5088e-01)	Acc@1  85.16 ( 83.41)	Acc@5  96.88 ( 97.71)
Epoch: [43][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8754e-01 (5.4943e-01)	Acc@1  78.12 ( 83.40)	Acc@5  98.44 ( 97.74)
Epoch: [43][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1134e-01 (5.4976e-01)	Acc@1  86.72 ( 83.42)	Acc@5  98.44 ( 97.72)
Epoch: [43][220/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1495e-01 (5.5105e-01)	Acc@1  79.69 ( 83.40)	Acc@5  94.53 ( 97.72)
Epoch: [43][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.3852e-01 (5.5243e-01)	Acc@1  81.25 ( 83.37)	Acc@5  97.66 ( 97.71)
Epoch: [43][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2807e-01 (5.5364e-01)	Acc@1  82.03 ( 83.35)	Acc@5  96.88 ( 97.69)
Epoch: [43][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8713e-01 (5.5337e-01)	Acc@1  83.59 ( 83.33)	Acc@5  99.22 ( 97.71)
Epoch: [43][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4278e-01 (5.5476e-01)	Acc@1  86.72 ( 83.25)	Acc@5 100.00 ( 97.72)
Epoch: [43][270/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.5302e-01 (5.5563e-01)	Acc@1  82.81 ( 83.20)	Acc@5  98.44 ( 97.72)
Epoch: [43][280/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8081e-01 (5.5564e-01)	Acc@1  78.12 ( 83.19)	Acc@5  97.66 ( 97.73)
Epoch: [43][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9606e-01 (5.5550e-01)	Acc@1  82.03 ( 83.16)	Acc@5  98.44 ( 97.73)
Epoch: [43][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1327e-01 (5.5475e-01)	Acc@1  81.25 ( 83.19)	Acc@5 100.00 ( 97.73)
Epoch: [43][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5109e-01 (5.5535e-01)	Acc@1  79.69 ( 83.15)	Acc@5  97.66 ( 97.73)
Epoch: [43][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7221e-01 (5.5588e-01)	Acc@1  84.38 ( 83.14)	Acc@5  98.44 ( 97.71)
Epoch: [43][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9542e-01 (5.5620e-01)	Acc@1  84.38 ( 83.15)	Acc@5  96.88 ( 97.69)
Epoch: [43][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6207e-01 (5.5550e-01)	Acc@1  87.50 ( 83.12)	Acc@5  99.22 ( 97.72)
Epoch: [43][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1190e-01 (5.5661e-01)	Acc@1  78.12 ( 83.06)	Acc@5  96.09 ( 97.73)
Epoch: [43][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9563e-01 (5.5749e-01)	Acc@1  84.38 ( 83.01)	Acc@5  97.66 ( 97.71)
Epoch: [43][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.5461e-01 (5.5856e-01)	Acc@1  78.12 ( 82.96)	Acc@5  96.88 ( 97.73)
Epoch: [43][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4279e-01 (5.5968e-01)	Acc@1  84.38 ( 82.95)	Acc@5  96.88 ( 97.70)
Epoch: [43][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.2937e-01 (5.6034e-01)	Acc@1  82.50 ( 82.94)	Acc@5  97.50 ( 97.69)
## e[43] optimizer.zero_grad (sum) time: 0.26517248153686523
## e[43]       loss.backward (sum) time: 4.084520578384399
## e[43]      optimizer.step (sum) time: 1.8443782329559326
## epoch[43] training(only) time: 16.189555406570435
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.1856e+00 (1.1856e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 1.2213e+00 (1.2927e+00)	Acc@1  67.00 ( 68.27)	Acc@5  93.00 ( 89.55)
Test: [ 20/100]	Time  0.019 ( 0.027)	Loss 1.0078e+00 (1.2206e+00)	Acc@1  74.00 ( 68.71)	Acc@5  94.00 ( 90.33)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 1.4424e+00 (1.2368e+00)	Acc@1  61.00 ( 68.03)	Acc@5  92.00 ( 90.52)
Test: [ 40/100]	Time  0.020 ( 0.024)	Loss 1.1721e+00 (1.2395e+00)	Acc@1  69.00 ( 67.76)	Acc@5  94.00 ( 90.88)
Test: [ 50/100]	Time  0.020 ( 0.023)	Loss 1.2170e+00 (1.2470e+00)	Acc@1  66.00 ( 67.47)	Acc@5  93.00 ( 90.61)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 1.4450e+00 (1.2320e+00)	Acc@1  61.00 ( 67.36)	Acc@5  91.00 ( 90.87)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.3279e+00 (1.2333e+00)	Acc@1  63.00 ( 67.48)	Acc@5  91.00 ( 90.80)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 1.2416e+00 (1.2399e+00)	Acc@1  72.00 ( 67.46)	Acc@5  91.00 ( 90.63)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 1.4011e+00 (1.2259e+00)	Acc@1  66.00 ( 67.82)	Acc@5  90.00 ( 90.79)
 * Acc@1 67.970 Acc@5 90.870
### epoch[43] execution time: 18.49166703224182
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.207 ( 0.207)	Data  0.151 ( 0.151)	Loss 5.2635e-01 (5.2635e-01)	Acc@1  85.94 ( 85.94)	Acc@5  99.22 ( 99.22)
Epoch: [44][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.015)	Loss 4.6019e-01 (5.4065e-01)	Acc@1  89.06 ( 84.16)	Acc@5  98.44 ( 97.80)
Epoch: [44][ 20/391]	Time  0.037 ( 0.050)	Data  0.001 ( 0.008)	Loss 5.5014e-01 (5.6556e-01)	Acc@1  85.94 ( 83.41)	Acc@5  96.88 ( 97.32)
Epoch: [44][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.8424e-01 (5.5210e-01)	Acc@1  82.81 ( 83.22)	Acc@5  95.31 ( 97.56)
Epoch: [44][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.1147e-01 (5.4319e-01)	Acc@1  92.19 ( 83.38)	Acc@5 100.00 ( 97.58)
Epoch: [44][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.2408e-01 (5.3883e-01)	Acc@1  84.38 ( 83.73)	Acc@5  96.88 ( 97.64)
Epoch: [44][ 60/391]	Time  0.040 ( 0.044)	Data  0.002 ( 0.004)	Loss 4.9224e-01 (5.3870e-01)	Acc@1  85.94 ( 83.94)	Acc@5  99.22 ( 97.68)
Epoch: [44][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.3134e-01 (5.3888e-01)	Acc@1  85.94 ( 83.76)	Acc@5 100.00 ( 97.70)
Epoch: [44][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.7341e-01 (5.3625e-01)	Acc@1  86.72 ( 83.85)	Acc@5  99.22 ( 97.83)
Epoch: [44][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.3230e-01 (5.3669e-01)	Acc@1  79.69 ( 83.81)	Acc@5  99.22 ( 97.91)
Epoch: [44][100/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.5197e-01 (5.3363e-01)	Acc@1  81.25 ( 84.01)	Acc@5  97.66 ( 97.90)
Epoch: [44][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9390e-01 (5.3382e-01)	Acc@1  88.28 ( 84.00)	Acc@5 100.00 ( 97.88)
Epoch: [44][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3579e-01 (5.3724e-01)	Acc@1  85.94 ( 83.86)	Acc@5  98.44 ( 97.84)
Epoch: [44][130/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9786e-01 (5.3409e-01)	Acc@1  85.16 ( 83.93)	Acc@5  97.66 ( 97.90)
Epoch: [44][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.4591e-01 (5.3461e-01)	Acc@1  77.34 ( 83.84)	Acc@5  96.09 ( 97.91)
Epoch: [44][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4314e-01 (5.3424e-01)	Acc@1  85.94 ( 83.79)	Acc@5  98.44 ( 97.90)
Epoch: [44][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.4114e-01 (5.3724e-01)	Acc@1  82.81 ( 83.72)	Acc@5  97.66 ( 97.86)
Epoch: [44][170/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6759e-01 (5.3933e-01)	Acc@1  85.16 ( 83.64)	Acc@5  98.44 ( 97.83)
Epoch: [44][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5256e-01 (5.3935e-01)	Acc@1  85.16 ( 83.71)	Acc@5 100.00 ( 97.83)
Epoch: [44][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7769e-01 (5.3838e-01)	Acc@1  85.94 ( 83.72)	Acc@5  98.44 ( 97.84)
Epoch: [44][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.9862e-01 (5.3813e-01)	Acc@1  85.16 ( 83.77)	Acc@5  96.88 ( 97.83)
Epoch: [44][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5508e-01 (5.4110e-01)	Acc@1  83.59 ( 83.66)	Acc@5  99.22 ( 97.82)
Epoch: [44][220/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.8512e-01 (5.4320e-01)	Acc@1  76.56 ( 83.58)	Acc@5  98.44 ( 97.82)
Epoch: [44][230/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5500e-01 (5.4550e-01)	Acc@1  79.69 ( 83.52)	Acc@5  95.31 ( 97.80)
Epoch: [44][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5623e-01 (5.4523e-01)	Acc@1  83.59 ( 83.56)	Acc@5  96.88 ( 97.81)
Epoch: [44][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6573e-01 (5.4559e-01)	Acc@1  82.03 ( 83.55)	Acc@5  96.88 ( 97.78)
Epoch: [44][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4822e-01 (5.4670e-01)	Acc@1  82.81 ( 83.48)	Acc@5  99.22 ( 97.77)
Epoch: [44][270/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6215e-01 (5.4612e-01)	Acc@1  87.50 ( 83.50)	Acc@5  99.22 ( 97.77)
Epoch: [44][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4597e-01 (5.4800e-01)	Acc@1  83.59 ( 83.45)	Acc@5  96.09 ( 97.76)
Epoch: [44][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7914e-01 (5.4816e-01)	Acc@1  82.81 ( 83.44)	Acc@5  98.44 ( 97.76)
Epoch: [44][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0370e-01 (5.4942e-01)	Acc@1  76.56 ( 83.40)	Acc@5  96.88 ( 97.76)
Epoch: [44][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2383e-01 (5.4878e-01)	Acc@1  85.16 ( 83.42)	Acc@5  96.88 ( 97.76)
Epoch: [44][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1605e-01 (5.4831e-01)	Acc@1  82.81 ( 83.41)	Acc@5  99.22 ( 97.77)
Epoch: [44][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1037e-01 (5.4743e-01)	Acc@1  85.16 ( 83.41)	Acc@5  97.66 ( 97.78)
Epoch: [44][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3120e-01 (5.4808e-01)	Acc@1  85.16 ( 83.35)	Acc@5  97.66 ( 97.78)
Epoch: [44][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.4847e-01 (5.4900e-01)	Acc@1  76.56 ( 83.32)	Acc@5  93.75 ( 97.75)
Epoch: [44][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1001e-01 (5.5183e-01)	Acc@1  78.91 ( 83.24)	Acc@5  99.22 ( 97.73)
Epoch: [44][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.5857e-01 (5.5165e-01)	Acc@1  78.91 ( 83.26)	Acc@5  96.88 ( 97.73)
Epoch: [44][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.9872e-01 (5.5202e-01)	Acc@1  84.38 ( 83.26)	Acc@5  96.09 ( 97.72)
Epoch: [44][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9404e-01 (5.5216e-01)	Acc@1  88.75 ( 83.24)	Acc@5 100.00 ( 97.72)
## e[44] optimizer.zero_grad (sum) time: 0.26639461517333984
## e[44]       loss.backward (sum) time: 4.031259536743164
## e[44]      optimizer.step (sum) time: 1.8429734706878662
## epoch[44] training(only) time: 16.109255075454712
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.1647e+00 (1.1647e+00)	Acc@1  72.00 ( 72.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.021 ( 0.034)	Loss 1.2481e+00 (1.2908e+00)	Acc@1  65.00 ( 67.36)	Acc@5  92.00 ( 90.27)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 1.0923e+00 (1.2320e+00)	Acc@1  70.00 ( 67.81)	Acc@5  93.00 ( 90.76)
Test: [ 30/100]	Time  0.017 ( 0.026)	Loss 1.3606e+00 (1.2434e+00)	Acc@1  65.00 ( 67.61)	Acc@5  91.00 ( 90.45)
Test: [ 40/100]	Time  0.024 ( 0.024)	Loss 1.1462e+00 (1.2438e+00)	Acc@1  70.00 ( 67.37)	Acc@5  93.00 ( 90.63)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 1.2534e+00 (1.2526e+00)	Acc@1  70.00 ( 67.31)	Acc@5  92.00 ( 90.53)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 1.3655e+00 (1.2333e+00)	Acc@1  63.00 ( 67.75)	Acc@5  90.00 ( 90.79)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 1.3325e+00 (1.2305e+00)	Acc@1  65.00 ( 67.75)	Acc@5  92.00 ( 90.86)
Test: [ 80/100]	Time  0.018 ( 0.022)	Loss 1.2876e+00 (1.2401e+00)	Acc@1  74.00 ( 67.67)	Acc@5  91.00 ( 90.69)
Test: [ 90/100]	Time  0.024 ( 0.022)	Loss 1.4644e+00 (1.2259e+00)	Acc@1  64.00 ( 67.97)	Acc@5  90.00 ( 90.86)
 * Acc@1 68.200 Acc@5 90.970
### epoch[44] execution time: 18.40428352355957
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.194 ( 0.194)	Data  0.139 ( 0.139)	Loss 4.9493e-01 (4.9493e-01)	Acc@1  85.94 ( 85.94)	Acc@5  96.88 ( 96.88)
Epoch: [45][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.013)	Loss 5.6185e-01 (5.2985e-01)	Acc@1  81.25 ( 83.66)	Acc@5  96.88 ( 97.94)
Epoch: [45][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.008)	Loss 6.1965e-01 (5.2912e-01)	Acc@1  83.59 ( 84.34)	Acc@5  94.53 ( 97.77)
Epoch: [45][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.005)	Loss 6.5239e-01 (5.3923e-01)	Acc@1  80.47 ( 83.97)	Acc@5  97.66 ( 97.61)
Epoch: [45][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.004)	Loss 6.7686e-01 (5.4998e-01)	Acc@1  78.12 ( 83.40)	Acc@5  96.88 ( 97.62)
Epoch: [45][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.1158e-01 (5.4533e-01)	Acc@1  81.25 ( 83.61)	Acc@5  95.31 ( 97.67)
Epoch: [45][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 5.6859e-01 (5.4487e-01)	Acc@1  81.25 ( 83.57)	Acc@5  98.44 ( 97.68)
Epoch: [45][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.1950e-01 (5.4081e-01)	Acc@1  82.81 ( 83.73)	Acc@5  97.66 ( 97.73)
Epoch: [45][ 80/391]	Time  0.041 ( 0.043)	Data  0.002 ( 0.003)	Loss 5.5140e-01 (5.4178e-01)	Acc@1  83.59 ( 83.65)	Acc@5  95.31 ( 97.65)
Epoch: [45][ 90/391]	Time  0.047 ( 0.043)	Data  0.001 ( 0.002)	Loss 4.3037e-01 (5.3808e-01)	Acc@1  84.38 ( 83.72)	Acc@5  99.22 ( 97.69)
Epoch: [45][100/391]	Time  0.042 ( 0.043)	Data  0.002 ( 0.002)	Loss 6.1033e-01 (5.3865e-01)	Acc@1  80.47 ( 83.68)	Acc@5  96.88 ( 97.67)
Epoch: [45][110/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.002)	Loss 5.4763e-01 (5.3613e-01)	Acc@1  78.91 ( 83.66)	Acc@5  97.66 ( 97.76)
Epoch: [45][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0737e-01 (5.3644e-01)	Acc@1  82.03 ( 83.65)	Acc@5  98.44 ( 97.75)
Epoch: [45][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8691e-01 (5.3904e-01)	Acc@1  86.72 ( 83.50)	Acc@5  96.88 ( 97.72)
Epoch: [45][140/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7959e-01 (5.3754e-01)	Acc@1  81.25 ( 83.55)	Acc@5  96.09 ( 97.72)
Epoch: [45][150/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.9783e-01 (5.4001e-01)	Acc@1  86.72 ( 83.44)	Acc@5  99.22 ( 97.71)
Epoch: [45][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7422e-01 (5.4115e-01)	Acc@1  82.81 ( 83.43)	Acc@5  98.44 ( 97.70)
Epoch: [45][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.6179e-01 (5.3953e-01)	Acc@1  83.59 ( 83.50)	Acc@5  98.44 ( 97.74)
Epoch: [45][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4328e-01 (5.3993e-01)	Acc@1  83.59 ( 83.47)	Acc@5  97.66 ( 97.74)
Epoch: [45][190/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0560e-01 (5.3754e-01)	Acc@1  84.38 ( 83.52)	Acc@5  96.88 ( 97.75)
Epoch: [45][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3103e-01 (5.3551e-01)	Acc@1  85.16 ( 83.60)	Acc@5  98.44 ( 97.77)
Epoch: [45][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9225e-01 (5.3549e-01)	Acc@1  82.81 ( 83.59)	Acc@5  96.09 ( 97.78)
Epoch: [45][220/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8088e-01 (5.3544e-01)	Acc@1  84.38 ( 83.64)	Acc@5  97.66 ( 97.77)
Epoch: [45][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9502e-01 (5.3589e-01)	Acc@1  84.38 ( 83.63)	Acc@5  97.66 ( 97.75)
Epoch: [45][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1140e-01 (5.3351e-01)	Acc@1  86.72 ( 83.73)	Acc@5  99.22 ( 97.77)
Epoch: [45][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2377e-01 (5.3333e-01)	Acc@1  85.94 ( 83.68)	Acc@5  99.22 ( 97.78)
Epoch: [45][260/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.3878e-01 (5.3518e-01)	Acc@1  84.38 ( 83.65)	Acc@5  97.66 ( 97.77)
Epoch: [45][270/391]	Time  0.048 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3823e-01 (5.3539e-01)	Acc@1  88.28 ( 83.69)	Acc@5  96.88 ( 97.77)
Epoch: [45][280/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.4662e-01 (5.3601e-01)	Acc@1  85.94 ( 83.66)	Acc@5  97.66 ( 97.77)
Epoch: [45][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1407e-01 (5.3627e-01)	Acc@1  84.38 ( 83.66)	Acc@5  98.44 ( 97.77)
Epoch: [45][300/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8319e-01 (5.3673e-01)	Acc@1  84.38 ( 83.66)	Acc@5 100.00 ( 97.77)
Epoch: [45][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 8.3873e-01 (5.3866e-01)	Acc@1  74.22 ( 83.60)	Acc@5  95.31 ( 97.75)
Epoch: [45][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4591e-01 (5.3915e-01)	Acc@1  84.38 ( 83.58)	Acc@5  99.22 ( 97.78)
Epoch: [45][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.6621e-01 (5.3933e-01)	Acc@1  85.16 ( 83.62)	Acc@5  97.66 ( 97.76)
Epoch: [45][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0962e-01 (5.4012e-01)	Acc@1  85.16 ( 83.60)	Acc@5  99.22 ( 97.75)
Epoch: [45][350/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8545e-01 (5.4148e-01)	Acc@1  89.06 ( 83.56)	Acc@5  99.22 ( 97.73)
Epoch: [45][360/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.001)	Loss 5.9727e-01 (5.4192e-01)	Acc@1  79.69 ( 83.53)	Acc@5  96.88 ( 97.72)
Epoch: [45][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.7543e-01 (5.4195e-01)	Acc@1  83.59 ( 83.54)	Acc@5  96.09 ( 97.71)
Epoch: [45][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3469e-01 (5.4339e-01)	Acc@1  87.50 ( 83.49)	Acc@5  97.66 ( 97.69)
Epoch: [45][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.3329e-01 (5.4373e-01)	Acc@1  83.75 ( 83.46)	Acc@5 100.00 ( 97.71)
## e[45] optimizer.zero_grad (sum) time: 0.26581907272338867
## e[45]       loss.backward (sum) time: 3.95839262008667
## e[45]      optimizer.step (sum) time: 1.8925154209136963
## epoch[45] training(only) time: 16.0508770942688
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2449e+00 (1.2449e+00)	Acc@1  70.00 ( 70.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 1.1838e+00 (1.3186e+00)	Acc@1  63.00 ( 66.27)	Acc@5  95.00 ( 90.00)
Test: [ 20/100]	Time  0.024 ( 0.026)	Loss 1.0593e+00 (1.2500e+00)	Acc@1  74.00 ( 67.57)	Acc@5  91.00 ( 90.52)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 1.3594e+00 (1.2637e+00)	Acc@1  64.00 ( 67.19)	Acc@5  91.00 ( 90.29)
Test: [ 40/100]	Time  0.021 ( 0.024)	Loss 1.2510e+00 (1.2644e+00)	Acc@1  69.00 ( 66.90)	Acc@5  92.00 ( 90.51)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 1.2532e+00 (1.2727e+00)	Acc@1  71.00 ( 66.96)	Acc@5  92.00 ( 90.20)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.3660e+00 (1.2481e+00)	Acc@1  63.00 ( 67.23)	Acc@5  91.00 ( 90.56)
Test: [ 70/100]	Time  0.020 ( 0.023)	Loss 1.2589e+00 (1.2437e+00)	Acc@1  63.00 ( 67.32)	Acc@5  89.00 ( 90.59)
Test: [ 80/100]	Time  0.016 ( 0.022)	Loss 1.2482e+00 (1.2501e+00)	Acc@1  75.00 ( 67.32)	Acc@5  90.00 ( 90.49)
Test: [ 90/100]	Time  0.018 ( 0.022)	Loss 1.4651e+00 (1.2363e+00)	Acc@1  64.00 ( 67.56)	Acc@5  91.00 ( 90.80)
 * Acc@1 67.770 Acc@5 90.880
### epoch[45] execution time: 18.289059162139893
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.205 ( 0.205)	Data  0.152 ( 0.152)	Loss 4.0689e-01 (4.0689e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.015)	Loss 5.2456e-01 (5.1488e-01)	Acc@1  85.94 ( 84.30)	Acc@5  98.44 ( 98.51)
Epoch: [46][ 20/391]	Time  0.035 ( 0.049)	Data  0.001 ( 0.008)	Loss 4.2778e-01 (5.1593e-01)	Acc@1  86.72 ( 84.52)	Acc@5  97.66 ( 98.03)
Epoch: [46][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.7224e-01 (5.2140e-01)	Acc@1  78.12 ( 84.12)	Acc@5  99.22 ( 98.06)
Epoch: [46][ 40/391]	Time  0.037 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.8855e-01 (5.1905e-01)	Acc@1  82.03 ( 84.30)	Acc@5  96.88 ( 98.04)
Epoch: [46][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.8469e-01 (5.1759e-01)	Acc@1  88.28 ( 84.67)	Acc@5  97.66 ( 97.90)
Epoch: [46][ 60/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.6266e-01 (5.1920e-01)	Acc@1  83.59 ( 84.27)	Acc@5  96.88 ( 97.95)
Epoch: [46][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.4801e-01 (5.1733e-01)	Acc@1  82.81 ( 84.43)	Acc@5  98.44 ( 97.95)
Epoch: [46][ 80/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.7231e-01 (5.1644e-01)	Acc@1  78.91 ( 84.48)	Acc@5  98.44 ( 97.90)
Epoch: [46][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.2027e-01 (5.1634e-01)	Acc@1  77.34 ( 84.41)	Acc@5  96.88 ( 97.95)
Epoch: [46][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.8961e-01 (5.2059e-01)	Acc@1  78.12 ( 84.15)	Acc@5  95.31 ( 97.96)
Epoch: [46][110/391]	Time  0.050 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1985e-01 (5.2373e-01)	Acc@1  83.59 ( 84.02)	Acc@5  96.09 ( 97.91)
Epoch: [46][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2165e-01 (5.2542e-01)	Acc@1  83.59 ( 83.97)	Acc@5  98.44 ( 97.94)
Epoch: [46][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3144e-01 (5.2517e-01)	Acc@1  83.59 ( 84.04)	Acc@5  98.44 ( 97.96)
Epoch: [46][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3213e-01 (5.2406e-01)	Acc@1  83.59 ( 84.01)	Acc@5  96.88 ( 97.96)
Epoch: [46][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6803e-01 (5.2600e-01)	Acc@1  83.59 ( 84.05)	Acc@5  96.88 ( 97.95)
Epoch: [46][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8796e-01 (5.2661e-01)	Acc@1  82.81 ( 84.00)	Acc@5  97.66 ( 97.91)
Epoch: [46][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0917e-01 (5.2722e-01)	Acc@1  89.84 ( 84.00)	Acc@5  97.66 ( 97.89)
Epoch: [46][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8371e-01 (5.2688e-01)	Acc@1  89.84 ( 83.98)	Acc@5  98.44 ( 97.91)
Epoch: [46][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2626e-01 (5.2831e-01)	Acc@1  81.25 ( 83.92)	Acc@5  98.44 ( 97.87)
Epoch: [46][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4178e-01 (5.3054e-01)	Acc@1  82.81 ( 83.84)	Acc@5  97.66 ( 97.85)
Epoch: [46][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9305e-01 (5.3059e-01)	Acc@1  88.28 ( 83.86)	Acc@5  99.22 ( 97.85)
Epoch: [46][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2514e-01 (5.3021e-01)	Acc@1  86.72 ( 83.84)	Acc@5  99.22 ( 97.86)
Epoch: [46][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6290e-01 (5.3012e-01)	Acc@1  85.94 ( 83.88)	Acc@5 100.00 ( 97.87)
Epoch: [46][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8987e-01 (5.3098e-01)	Acc@1  82.81 ( 83.86)	Acc@5  97.66 ( 97.87)
Epoch: [46][250/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5954e-01 (5.3065e-01)	Acc@1  85.94 ( 83.84)	Acc@5  99.22 ( 97.89)
Epoch: [46][260/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.5929e-01 (5.3201e-01)	Acc@1  82.81 ( 83.82)	Acc@5  98.44 ( 97.86)
Epoch: [46][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1115e-01 (5.3377e-01)	Acc@1  82.81 ( 83.78)	Acc@5  97.66 ( 97.84)
Epoch: [46][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4767e-01 (5.3284e-01)	Acc@1  85.94 ( 83.79)	Acc@5  99.22 ( 97.85)
Epoch: [46][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3755e-01 (5.3243e-01)	Acc@1  86.72 ( 83.82)	Acc@5  98.44 ( 97.84)
Epoch: [46][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.6924e-01 (5.3435e-01)	Acc@1  79.69 ( 83.77)	Acc@5  96.88 ( 97.81)
Epoch: [46][310/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.3899e-01 (5.3390e-01)	Acc@1  81.25 ( 83.73)	Acc@5  98.44 ( 97.83)
Epoch: [46][320/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 6.4495e-01 (5.3549e-01)	Acc@1  78.91 ( 83.64)	Acc@5  94.53 ( 97.82)
Epoch: [46][330/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0446e-01 (5.3546e-01)	Acc@1  82.81 ( 83.63)	Acc@5 100.00 ( 97.83)
Epoch: [46][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.9519e-01 (5.3567e-01)	Acc@1  79.69 ( 83.66)	Acc@5  96.09 ( 97.82)
Epoch: [46][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.0035e-01 (5.3674e-01)	Acc@1  78.91 ( 83.60)	Acc@5  96.88 ( 97.81)
Epoch: [46][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4802e-01 (5.3671e-01)	Acc@1  81.25 ( 83.62)	Acc@5  95.31 ( 97.80)
Epoch: [46][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1171e-01 (5.3693e-01)	Acc@1  79.69 ( 83.65)	Acc@5  96.88 ( 97.78)
Epoch: [46][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5576e-01 (5.3781e-01)	Acc@1  82.81 ( 83.64)	Acc@5  98.44 ( 97.76)
Epoch: [46][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4867e-01 (5.3799e-01)	Acc@1  87.50 ( 83.63)	Acc@5 100.00 ( 97.77)
## e[46] optimizer.zero_grad (sum) time: 0.26488494873046875
## e[46]       loss.backward (sum) time: 4.038100719451904
## e[46]      optimizer.step (sum) time: 1.8592534065246582
## epoch[46] training(only) time: 16.162286043167114
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2228e+00 (1.2228e+00)	Acc@1  67.00 ( 67.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2457e+00 (1.3164e+00)	Acc@1  61.00 ( 65.82)	Acc@5  93.00 ( 90.64)
Test: [ 20/100]	Time  0.018 ( 0.028)	Loss 1.0192e+00 (1.2406e+00)	Acc@1  74.00 ( 67.43)	Acc@5  92.00 ( 91.24)
Test: [ 30/100]	Time  0.016 ( 0.025)	Loss 1.4704e+00 (1.2585e+00)	Acc@1  64.00 ( 67.19)	Acc@5  90.00 ( 90.84)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.1730e+00 (1.2658e+00)	Acc@1  67.00 ( 66.83)	Acc@5  96.00 ( 91.00)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 1.2663e+00 (1.2746e+00)	Acc@1  69.00 ( 66.65)	Acc@5  92.00 ( 90.71)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 1.3940e+00 (1.2531e+00)	Acc@1  63.00 ( 67.03)	Acc@5  90.00 ( 90.87)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.3806e+00 (1.2500e+00)	Acc@1  66.00 ( 67.17)	Acc@5  90.00 ( 90.85)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.2855e+00 (1.2564e+00)	Acc@1  71.00 ( 67.15)	Acc@5  91.00 ( 90.75)
Test: [ 90/100]	Time  0.019 ( 0.023)	Loss 1.4670e+00 (1.2400e+00)	Acc@1  66.00 ( 67.58)	Acc@5  88.00 ( 90.87)
 * Acc@1 67.850 Acc@5 90.960
### epoch[46] execution time: 18.503642320632935
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.195 ( 0.195)	Data  0.147 ( 0.147)	Loss 6.2144e-01 (6.2144e-01)	Acc@1  82.81 ( 82.81)	Acc@5  93.75 ( 93.75)
Epoch: [47][ 10/391]	Time  0.040 ( 0.055)	Data  0.001 ( 0.014)	Loss 6.1132e-01 (5.3893e-01)	Acc@1  82.81 ( 83.17)	Acc@5  97.66 ( 97.59)
Epoch: [47][ 20/391]	Time  0.039 ( 0.048)	Data  0.002 ( 0.008)	Loss 5.4037e-01 (5.2518e-01)	Acc@1  84.38 ( 83.89)	Acc@5  98.44 ( 97.62)
Epoch: [47][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.0740e-01 (5.2547e-01)	Acc@1  84.38 ( 83.72)	Acc@5  97.66 ( 97.78)
Epoch: [47][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.3129e-01 (5.2035e-01)	Acc@1  88.28 ( 83.97)	Acc@5  99.22 ( 97.96)
Epoch: [47][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.2788e-01 (5.3496e-01)	Acc@1  78.91 ( 83.49)	Acc@5  98.44 ( 97.95)
Epoch: [47][ 60/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.7570e-01 (5.3209e-01)	Acc@1  84.38 ( 83.82)	Acc@5  97.66 ( 97.89)
Epoch: [47][ 70/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.6690e-01 (5.3295e-01)	Acc@1  87.50 ( 83.64)	Acc@5 100.00 ( 97.90)
Epoch: [47][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.7271e-01 (5.3162e-01)	Acc@1  84.38 ( 83.76)	Acc@5 100.00 ( 97.86)
Epoch: [47][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2508e-01 (5.2723e-01)	Acc@1  86.72 ( 84.00)	Acc@5  99.22 ( 97.89)
Epoch: [47][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3053e-01 (5.2685e-01)	Acc@1  90.62 ( 83.95)	Acc@5  99.22 ( 97.91)
Epoch: [47][110/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.6689e-01 (5.2586e-01)	Acc@1  80.47 ( 83.99)	Acc@5  97.66 ( 97.92)
Epoch: [47][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4459e-01 (5.2578e-01)	Acc@1  85.16 ( 84.05)	Acc@5  97.66 ( 97.86)
Epoch: [47][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.5415e-01 (5.2658e-01)	Acc@1  81.25 ( 84.21)	Acc@5  96.09 ( 97.76)
Epoch: [47][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2358e-01 (5.2656e-01)	Acc@1  83.59 ( 84.18)	Acc@5  96.88 ( 97.78)
Epoch: [47][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.6406e-01 (5.2636e-01)	Acc@1  77.34 ( 84.18)	Acc@5  97.66 ( 97.80)
Epoch: [47][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1221e-01 (5.2461e-01)	Acc@1  78.12 ( 84.15)	Acc@5  95.31 ( 97.83)
Epoch: [47][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3359e-01 (5.2663e-01)	Acc@1  84.38 ( 84.05)	Acc@5  96.88 ( 97.82)
Epoch: [47][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0714e-01 (5.2766e-01)	Acc@1  88.28 ( 84.04)	Acc@5  96.09 ( 97.79)
Epoch: [47][190/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.4450e-01 (5.2943e-01)	Acc@1  83.59 ( 83.97)	Acc@5  97.66 ( 97.76)
Epoch: [47][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2809e-01 (5.2989e-01)	Acc@1  86.72 ( 84.00)	Acc@5  98.44 ( 97.77)
Epoch: [47][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9452e-01 (5.2900e-01)	Acc@1  84.38 ( 84.03)	Acc@5  98.44 ( 97.79)
Epoch: [47][220/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1697e-01 (5.2701e-01)	Acc@1  81.25 ( 84.09)	Acc@5  97.66 ( 97.80)
Epoch: [47][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8592e-01 (5.2608e-01)	Acc@1  87.50 ( 84.12)	Acc@5  96.09 ( 97.82)
Epoch: [47][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1754e-01 (5.2677e-01)	Acc@1  86.72 ( 84.09)	Acc@5  98.44 ( 97.83)
Epoch: [47][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3752e-01 (5.2504e-01)	Acc@1  88.28 ( 84.14)	Acc@5  97.66 ( 97.83)
Epoch: [47][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6413e-01 (5.2452e-01)	Acc@1  83.59 ( 84.15)	Acc@5  99.22 ( 97.84)
Epoch: [47][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9774e-01 (5.2409e-01)	Acc@1  84.38 ( 84.15)	Acc@5  98.44 ( 97.83)
Epoch: [47][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8772e-01 (5.2435e-01)	Acc@1  85.94 ( 84.12)	Acc@5  96.88 ( 97.85)
Epoch: [47][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7911e-01 (5.2430e-01)	Acc@1  89.84 ( 84.12)	Acc@5  99.22 ( 97.85)
Epoch: [47][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4838e-01 (5.2571e-01)	Acc@1  88.28 ( 84.12)	Acc@5  99.22 ( 97.83)
Epoch: [47][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0697e-01 (5.2700e-01)	Acc@1  89.06 ( 84.08)	Acc@5  97.66 ( 97.81)
Epoch: [47][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1802e-01 (5.2789e-01)	Acc@1  84.38 ( 84.05)	Acc@5  95.31 ( 97.81)
Epoch: [47][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3627e-01 (5.2679e-01)	Acc@1  86.72 ( 84.09)	Acc@5  98.44 ( 97.82)
Epoch: [47][340/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2361e-01 (5.2736e-01)	Acc@1  82.81 ( 84.07)	Acc@5  96.88 ( 97.82)
Epoch: [47][350/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4512e-01 (5.2622e-01)	Acc@1  80.47 ( 84.05)	Acc@5  99.22 ( 97.85)
Epoch: [47][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8183e-01 (5.2638e-01)	Acc@1  85.94 ( 84.08)	Acc@5  99.22 ( 97.85)
Epoch: [47][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3505e-01 (5.2537e-01)	Acc@1  83.59 ( 84.10)	Acc@5  97.66 ( 97.87)
Epoch: [47][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.9595e-01 (5.2573e-01)	Acc@1  80.47 ( 84.12)	Acc@5 100.00 ( 97.86)
Epoch: [47][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1214e-01 (5.2642e-01)	Acc@1  83.75 ( 84.10)	Acc@5  93.75 ( 97.85)
## e[47] optimizer.zero_grad (sum) time: 0.26593708992004395
## e[47]       loss.backward (sum) time: 3.962575912475586
## e[47]      optimizer.step (sum) time: 1.8744378089904785
## epoch[47] training(only) time: 16.045981407165527
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.2429e+00 (1.2429e+00)	Acc@1  70.00 ( 70.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.2282e+00 (1.2945e+00)	Acc@1  60.00 ( 67.36)	Acc@5  93.00 ( 89.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.0177e+00 (1.2268e+00)	Acc@1  74.00 ( 68.00)	Acc@5  91.00 ( 90.81)
Test: [ 30/100]	Time  0.025 ( 0.027)	Loss 1.3733e+00 (1.2556e+00)	Acc@1  62.00 ( 67.52)	Acc@5  91.00 ( 90.77)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.2171e+00 (1.2693e+00)	Acc@1  69.00 ( 67.15)	Acc@5  94.00 ( 90.78)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 1.3143e+00 (1.2753e+00)	Acc@1  70.00 ( 66.96)	Acc@5  90.00 ( 90.49)
Test: [ 60/100]	Time  0.020 ( 0.024)	Loss 1.3957e+00 (1.2538e+00)	Acc@1  65.00 ( 67.34)	Acc@5  91.00 ( 90.80)
Test: [ 70/100]	Time  0.018 ( 0.024)	Loss 1.3067e+00 (1.2490e+00)	Acc@1  65.00 ( 67.61)	Acc@5  92.00 ( 90.90)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 1.3394e+00 (1.2537e+00)	Acc@1  74.00 ( 67.74)	Acc@5  89.00 ( 90.81)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 1.4625e+00 (1.2390e+00)	Acc@1  65.00 ( 68.08)	Acc@5  87.00 ( 90.93)
 * Acc@1 68.110 Acc@5 91.150
### epoch[47] execution time: 18.42130756378174
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.232 ( 0.232)	Data  0.175 ( 0.175)	Loss 4.1445e-01 (4.1445e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [48][ 10/391]	Time  0.041 ( 0.060)	Data  0.001 ( 0.017)	Loss 6.1807e-01 (5.1279e-01)	Acc@1  82.03 ( 84.23)	Acc@5  97.66 ( 98.22)
Epoch: [48][ 20/391]	Time  0.040 ( 0.051)	Data  0.001 ( 0.009)	Loss 4.2649e-01 (4.9975e-01)	Acc@1  85.94 ( 84.64)	Acc@5  99.22 ( 98.40)
Epoch: [48][ 30/391]	Time  0.040 ( 0.048)	Data  0.001 ( 0.007)	Loss 4.3477e-01 (4.8743e-01)	Acc@1  85.94 ( 84.88)	Acc@5 100.00 ( 98.49)
Epoch: [48][ 40/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.005)	Loss 4.5486e-01 (4.9401e-01)	Acc@1  87.50 ( 84.76)	Acc@5  99.22 ( 98.36)
Epoch: [48][ 50/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.004)	Loss 4.1904e-01 (4.9136e-01)	Acc@1  87.50 ( 85.02)	Acc@5  98.44 ( 98.22)
Epoch: [48][ 60/391]	Time  0.043 ( 0.044)	Data  0.002 ( 0.004)	Loss 5.0106e-01 (5.0023e-01)	Acc@1  85.94 ( 84.76)	Acc@5  97.66 ( 98.09)
Epoch: [48][ 70/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.9008e-01 (4.9909e-01)	Acc@1  87.50 ( 84.85)	Acc@5  97.66 ( 98.15)
Epoch: [48][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.6691e-01 (4.9761e-01)	Acc@1  82.81 ( 84.78)	Acc@5 100.00 ( 98.23)
Epoch: [48][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.0783e-01 (4.9957e-01)	Acc@1  79.69 ( 84.74)	Acc@5  96.88 ( 98.21)
Epoch: [48][100/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.7022e-01 (5.0225e-01)	Acc@1  85.16 ( 84.66)	Acc@5  99.22 ( 98.25)
Epoch: [48][110/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.6004e-01 (5.0355e-01)	Acc@1  86.72 ( 84.61)	Acc@5  96.09 ( 98.22)
Epoch: [48][120/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.6344e-01 (5.0532e-01)	Acc@1  85.94 ( 84.65)	Acc@5  99.22 ( 98.21)
Epoch: [48][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0030e-01 (5.0645e-01)	Acc@1  86.72 ( 84.64)	Acc@5  99.22 ( 98.20)
Epoch: [48][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5725e-01 (5.0725e-01)	Acc@1  85.16 ( 84.62)	Acc@5  98.44 ( 98.17)
Epoch: [48][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7045e-01 (5.0784e-01)	Acc@1  82.81 ( 84.63)	Acc@5  97.66 ( 98.12)
Epoch: [48][160/391]	Time  0.038 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.7035e-01 (5.0818e-01)	Acc@1  85.16 ( 84.62)	Acc@5  97.66 ( 98.07)
Epoch: [48][170/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9160e-01 (5.0964e-01)	Acc@1  82.03 ( 84.55)	Acc@5  96.88 ( 98.05)
Epoch: [48][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7231e-01 (5.1122e-01)	Acc@1  85.94 ( 84.47)	Acc@5  99.22 ( 98.05)
Epoch: [48][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5000e-01 (5.1182e-01)	Acc@1  89.84 ( 84.41)	Acc@5 100.00 ( 98.06)
Epoch: [48][200/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8390e-01 (5.1253e-01)	Acc@1  82.03 ( 84.36)	Acc@5  99.22 ( 98.06)
Epoch: [48][210/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.3606e-01 (5.1256e-01)	Acc@1  86.72 ( 84.32)	Acc@5 100.00 ( 98.08)
Epoch: [48][220/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.1752e-01 (5.1433e-01)	Acc@1  86.72 ( 84.25)	Acc@5  96.09 ( 98.06)
Epoch: [48][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5620e-01 (5.1356e-01)	Acc@1  81.25 ( 84.30)	Acc@5  96.88 ( 98.06)
Epoch: [48][240/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7474e-01 (5.1377e-01)	Acc@1  83.59 ( 84.30)	Acc@5  99.22 ( 98.05)
Epoch: [48][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5051e-01 (5.1409e-01)	Acc@1  78.91 ( 84.26)	Acc@5  96.09 ( 98.03)
Epoch: [48][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5364e-01 (5.1335e-01)	Acc@1  83.59 ( 84.30)	Acc@5  96.88 ( 98.04)
Epoch: [48][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1105e-01 (5.1532e-01)	Acc@1  82.81 ( 84.28)	Acc@5  95.31 ( 98.01)
Epoch: [48][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0377e-01 (5.1809e-01)	Acc@1  78.91 ( 84.20)	Acc@5  99.22 ( 98.00)
Epoch: [48][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5196e-01 (5.1796e-01)	Acc@1  86.72 ( 84.19)	Acc@5 100.00 ( 98.01)
Epoch: [48][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0101e-01 (5.1747e-01)	Acc@1  89.84 ( 84.20)	Acc@5  99.22 ( 98.02)
Epoch: [48][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6611e-01 (5.1812e-01)	Acc@1  87.50 ( 84.22)	Acc@5  96.09 ( 98.00)
Epoch: [48][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7479e-01 (5.1790e-01)	Acc@1  81.25 ( 84.25)	Acc@5  93.75 ( 98.00)
Epoch: [48][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5945e-01 (5.1958e-01)	Acc@1  86.72 ( 84.18)	Acc@5  96.88 ( 97.97)
Epoch: [48][340/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.9493e-01 (5.2030e-01)	Acc@1  84.38 ( 84.16)	Acc@5  97.66 ( 97.97)
Epoch: [48][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.7136e-01 (5.2117e-01)	Acc@1  79.69 ( 84.15)	Acc@5  96.88 ( 97.96)
Epoch: [48][360/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.9705e-01 (5.2163e-01)	Acc@1  85.16 ( 84.12)	Acc@5  99.22 ( 97.96)
Epoch: [48][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8813e-01 (5.2215e-01)	Acc@1  77.34 ( 84.11)	Acc@5  97.66 ( 97.94)
Epoch: [48][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7678e-01 (5.2235e-01)	Acc@1  83.59 ( 84.11)	Acc@5  96.88 ( 97.95)
Epoch: [48][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5546e-01 (5.2305e-01)	Acc@1  86.25 ( 84.08)	Acc@5  98.75 ( 97.94)
## e[48] optimizer.zero_grad (sum) time: 0.26704931259155273
## e[48]       loss.backward (sum) time: 4.02384090423584
## e[48]      optimizer.step (sum) time: 1.8878722190856934
## epoch[48] training(only) time: 16.06328320503235
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2301e+00 (1.2301e+00)	Acc@1  69.00 ( 69.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 1.2924e+00 (1.3313e+00)	Acc@1  61.00 ( 67.00)	Acc@5  93.00 ( 90.55)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 1.0593e+00 (1.2610e+00)	Acc@1  75.00 ( 68.14)	Acc@5  91.00 ( 90.76)
Test: [ 30/100]	Time  0.022 ( 0.024)	Loss 1.4381e+00 (1.2799e+00)	Acc@1  64.00 ( 67.68)	Acc@5  91.00 ( 90.77)
Test: [ 40/100]	Time  0.022 ( 0.024)	Loss 1.1768e+00 (1.2850e+00)	Acc@1  69.00 ( 67.41)	Acc@5  94.00 ( 90.95)
Test: [ 50/100]	Time  0.025 ( 0.023)	Loss 1.2657e+00 (1.2876e+00)	Acc@1  67.00 ( 67.16)	Acc@5  94.00 ( 90.73)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.4444e+00 (1.2628e+00)	Acc@1  63.00 ( 67.48)	Acc@5  92.00 ( 90.97)
Test: [ 70/100]	Time  0.018 ( 0.022)	Loss 1.3329e+00 (1.2596e+00)	Acc@1  65.00 ( 67.61)	Acc@5  91.00 ( 90.90)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 1.3241e+00 (1.2666e+00)	Acc@1  70.00 ( 67.49)	Acc@5  91.00 ( 90.75)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 1.4694e+00 (1.2530e+00)	Acc@1  69.00 ( 67.82)	Acc@5  89.00 ( 90.87)
 * Acc@1 67.960 Acc@5 91.000
### epoch[48] execution time: 18.337403535842896
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.199 ( 0.199)	Data  0.151 ( 0.151)	Loss 5.2439e-01 (5.2439e-01)	Acc@1  81.25 ( 81.25)	Acc@5  97.66 ( 97.66)
Epoch: [49][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.015)	Loss 6.2942e-01 (5.0790e-01)	Acc@1  80.47 ( 83.95)	Acc@5  96.09 ( 98.08)
Epoch: [49][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.008)	Loss 4.1263e-01 (4.9898e-01)	Acc@1  88.28 ( 84.93)	Acc@5  98.44 ( 98.03)
Epoch: [49][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.9767e-01 (5.0579e-01)	Acc@1  89.84 ( 85.03)	Acc@5  98.44 ( 97.86)
Epoch: [49][ 40/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.005)	Loss 4.9986e-01 (5.0940e-01)	Acc@1  82.81 ( 84.78)	Acc@5  96.09 ( 97.81)
Epoch: [49][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.9305e-01 (5.0798e-01)	Acc@1  85.16 ( 84.71)	Acc@5  96.88 ( 97.89)
Epoch: [49][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.9301e-01 (5.0367e-01)	Acc@1  84.38 ( 84.91)	Acc@5  96.88 ( 97.91)
Epoch: [49][ 70/391]	Time  0.046 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.3182e-01 (5.0628e-01)	Acc@1  81.25 ( 84.79)	Acc@5  94.53 ( 97.89)
Epoch: [49][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.6446e-01 (5.0877e-01)	Acc@1  80.47 ( 84.51)	Acc@5  96.09 ( 97.90)
Epoch: [49][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4177e-01 (5.0309e-01)	Acc@1  84.38 ( 84.81)	Acc@5 100.00 ( 97.96)
Epoch: [49][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2463e-01 (5.0030e-01)	Acc@1  82.81 ( 84.85)	Acc@5  97.66 ( 97.96)
Epoch: [49][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2934e-01 (5.0486e-01)	Acc@1  82.03 ( 84.71)	Acc@5  98.44 ( 97.94)
Epoch: [49][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9966e-01 (5.0462e-01)	Acc@1  83.59 ( 84.72)	Acc@5 100.00 ( 97.97)
Epoch: [49][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7346e-01 (5.0318e-01)	Acc@1  82.03 ( 84.80)	Acc@5  97.66 ( 97.98)
Epoch: [49][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8696e-01 (5.0470e-01)	Acc@1  84.38 ( 84.74)	Acc@5  99.22 ( 97.98)
Epoch: [49][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0407e-01 (5.0918e-01)	Acc@1  87.50 ( 84.63)	Acc@5  96.09 ( 97.95)
Epoch: [49][160/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3864e-01 (5.1195e-01)	Acc@1  83.59 ( 84.57)	Acc@5  97.66 ( 97.92)
Epoch: [49][170/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.7095e-01 (5.1319e-01)	Acc@1  84.38 ( 84.52)	Acc@5  99.22 ( 97.93)
Epoch: [49][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4671e-01 (5.1063e-01)	Acc@1  82.03 ( 84.59)	Acc@5  96.09 ( 97.95)
Epoch: [49][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7242e-01 (5.1403e-01)	Acc@1  82.81 ( 84.48)	Acc@5  96.88 ( 97.93)
Epoch: [49][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7197e-01 (5.1357e-01)	Acc@1  85.94 ( 84.44)	Acc@5  98.44 ( 97.94)
Epoch: [49][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0319e-01 (5.1417e-01)	Acc@1  82.03 ( 84.40)	Acc@5  99.22 ( 97.92)
Epoch: [49][220/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.6909e-01 (5.1416e-01)	Acc@1  85.16 ( 84.38)	Acc@5  98.44 ( 97.92)
Epoch: [49][230/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7294e-01 (5.1364e-01)	Acc@1  83.59 ( 84.43)	Acc@5  95.31 ( 97.90)
Epoch: [49][240/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.5668e-01 (5.1580e-01)	Acc@1  88.28 ( 84.40)	Acc@5  96.88 ( 97.85)
Epoch: [49][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6250e-01 (5.1411e-01)	Acc@1  88.28 ( 84.44)	Acc@5 100.00 ( 97.88)
Epoch: [49][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1568e-01 (5.1415e-01)	Acc@1  86.72 ( 84.42)	Acc@5  98.44 ( 97.88)
Epoch: [49][270/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1617e-01 (5.1454e-01)	Acc@1  84.38 ( 84.41)	Acc@5  98.44 ( 97.88)
Epoch: [49][280/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4262e-01 (5.1463e-01)	Acc@1  88.28 ( 84.41)	Acc@5  97.66 ( 97.88)
Epoch: [49][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0619e-01 (5.1479e-01)	Acc@1  85.94 ( 84.44)	Acc@5  97.66 ( 97.87)
Epoch: [49][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4438e-01 (5.1483e-01)	Acc@1  81.25 ( 84.44)	Acc@5  97.66 ( 97.88)
Epoch: [49][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3310e-01 (5.1553e-01)	Acc@1  87.50 ( 84.39)	Acc@5  99.22 ( 97.89)
Epoch: [49][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9037e-01 (5.1549e-01)	Acc@1  82.03 ( 84.39)	Acc@5  96.09 ( 97.90)
Epoch: [49][330/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2648e-01 (5.1584e-01)	Acc@1  82.03 ( 84.34)	Acc@5  99.22 ( 97.93)
Epoch: [49][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9448e-01 (5.1465e-01)	Acc@1  83.59 ( 84.40)	Acc@5  96.88 ( 97.92)
Epoch: [49][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7312e-01 (5.1468e-01)	Acc@1  83.59 ( 84.43)	Acc@5  98.44 ( 97.91)
Epoch: [49][360/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8572e-01 (5.1409e-01)	Acc@1  85.16 ( 84.45)	Acc@5  98.44 ( 97.91)
Epoch: [49][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5125e-01 (5.1404e-01)	Acc@1  85.94 ( 84.44)	Acc@5  98.44 ( 97.90)
Epoch: [49][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7533e-01 (5.1404e-01)	Acc@1  85.94 ( 84.43)	Acc@5  96.88 ( 97.90)
Epoch: [49][390/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.3234e-01 (5.1453e-01)	Acc@1  81.25 ( 84.43)	Acc@5  95.00 ( 97.90)
## e[49] optimizer.zero_grad (sum) time: 0.26853013038635254
## e[49]       loss.backward (sum) time: 4.0734474658966064
## e[49]      optimizer.step (sum) time: 1.8871991634368896
## epoch[49] training(only) time: 16.129361867904663
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3550e+00 (1.3550e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 1.2381e+00 (1.3286e+00)	Acc@1  62.00 ( 66.82)	Acc@5  93.00 ( 89.82)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.0491e+00 (1.2571e+00)	Acc@1  72.00 ( 68.05)	Acc@5  92.00 ( 90.57)
Test: [ 30/100]	Time  0.018 ( 0.026)	Loss 1.4472e+00 (1.2773e+00)	Acc@1  62.00 ( 67.35)	Acc@5  89.00 ( 90.06)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 1.2086e+00 (1.2879e+00)	Acc@1  69.00 ( 67.15)	Acc@5  91.00 ( 90.20)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.3378e+00 (1.2936e+00)	Acc@1  65.00 ( 67.02)	Acc@5  95.00 ( 90.24)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 1.3959e+00 (1.2727e+00)	Acc@1  65.00 ( 67.28)	Acc@5  91.00 ( 90.67)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 1.3687e+00 (1.2677e+00)	Acc@1  63.00 ( 67.20)	Acc@5  89.00 ( 90.70)
Test: [ 80/100]	Time  0.019 ( 0.023)	Loss 1.3312e+00 (1.2714e+00)	Acc@1  70.00 ( 67.11)	Acc@5  92.00 ( 90.65)
Test: [ 90/100]	Time  0.026 ( 0.023)	Loss 1.3805e+00 (1.2564e+00)	Acc@1  66.00 ( 67.43)	Acc@5  91.00 ( 90.85)
 * Acc@1 67.650 Acc@5 91.080
### epoch[49] execution time: 18.49278473854065
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.202 ( 0.202)	Data  0.145 ( 0.145)	Loss 4.8813e-01 (4.8813e-01)	Acc@1  86.72 ( 86.72)	Acc@5  96.88 ( 96.88)
Epoch: [50][ 10/391]	Time  0.046 ( 0.057)	Data  0.001 ( 0.014)	Loss 5.9638e-01 (5.1877e-01)	Acc@1  82.81 ( 85.01)	Acc@5  97.66 ( 97.59)
Epoch: [50][ 20/391]	Time  0.039 ( 0.050)	Data  0.001 ( 0.008)	Loss 5.0274e-01 (5.1428e-01)	Acc@1  83.59 ( 84.78)	Acc@5  98.44 ( 97.62)
Epoch: [50][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.006)	Loss 5.5892e-01 (4.9949e-01)	Acc@1  83.59 ( 85.08)	Acc@5  97.66 ( 97.83)
Epoch: [50][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.3539e-01 (4.9604e-01)	Acc@1  82.03 ( 84.98)	Acc@5  98.44 ( 97.96)
Epoch: [50][ 50/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.8309e-01 (4.9485e-01)	Acc@1  85.94 ( 84.99)	Acc@5  97.66 ( 97.99)
Epoch: [50][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.6283e-01 (4.9668e-01)	Acc@1  84.38 ( 84.95)	Acc@5 100.00 ( 98.05)
Epoch: [50][ 70/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.1026e-01 (5.0032e-01)	Acc@1  86.72 ( 84.76)	Acc@5 100.00 ( 98.06)
Epoch: [50][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.5764e-01 (5.0594e-01)	Acc@1  87.50 ( 84.55)	Acc@5 100.00 ( 98.04)
Epoch: [50][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.7869e-01 (5.0610e-01)	Acc@1  77.34 ( 84.42)	Acc@5  98.44 ( 98.05)
Epoch: [50][100/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.7613e-01 (5.0727e-01)	Acc@1  83.59 ( 84.43)	Acc@5  98.44 ( 98.03)
Epoch: [50][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5959e-01 (5.0895e-01)	Acc@1  82.03 ( 84.29)	Acc@5  96.09 ( 98.02)
Epoch: [50][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4896e-01 (5.0918e-01)	Acc@1  82.81 ( 84.32)	Acc@5  99.22 ( 97.99)
Epoch: [50][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0740e-01 (5.0633e-01)	Acc@1  83.59 ( 84.46)	Acc@5  99.22 ( 98.04)
Epoch: [50][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3451e-01 (5.0662e-01)	Acc@1  86.72 ( 84.46)	Acc@5 100.00 ( 98.08)
Epoch: [50][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8231e-01 (5.0589e-01)	Acc@1  82.81 ( 84.48)	Acc@5  96.88 ( 98.06)
Epoch: [50][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1010e-01 (5.0554e-01)	Acc@1  81.25 ( 84.50)	Acc@5  97.66 ( 98.07)
Epoch: [50][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0923e-01 (5.0596e-01)	Acc@1  82.03 ( 84.50)	Acc@5  95.31 ( 98.08)
Epoch: [50][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7712e-01 (5.0550e-01)	Acc@1  87.50 ( 84.55)	Acc@5  98.44 ( 98.08)
Epoch: [50][190/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9146e-01 (5.0486e-01)	Acc@1  82.81 ( 84.55)	Acc@5  94.53 ( 98.06)
Epoch: [50][200/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8839e-01 (5.0559e-01)	Acc@1  82.03 ( 84.53)	Acc@5  96.09 ( 98.05)
Epoch: [50][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5052e-01 (5.0596e-01)	Acc@1  78.12 ( 84.46)	Acc@5  97.66 ( 98.06)
Epoch: [50][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3502e-01 (5.0696e-01)	Acc@1  87.50 ( 84.41)	Acc@5  98.44 ( 98.04)
Epoch: [50][230/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9997e-01 (5.0568e-01)	Acc@1  79.69 ( 84.46)	Acc@5  98.44 ( 98.05)
Epoch: [50][240/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2569e-01 (5.0602e-01)	Acc@1  85.94 ( 84.47)	Acc@5  96.09 ( 98.05)
Epoch: [50][250/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7310e-01 (5.0678e-01)	Acc@1  85.16 ( 84.42)	Acc@5 100.00 ( 98.03)
Epoch: [50][260/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4559e-01 (5.0529e-01)	Acc@1  85.16 ( 84.46)	Acc@5  98.44 ( 98.04)
Epoch: [50][270/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.0335e-01 (5.0502e-01)	Acc@1  86.72 ( 84.49)	Acc@5 100.00 ( 98.05)
Epoch: [50][280/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.8067e-01 (5.0635e-01)	Acc@1  81.25 ( 84.43)	Acc@5  94.53 ( 98.03)
Epoch: [50][290/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0960e-01 (5.0518e-01)	Acc@1  80.47 ( 84.43)	Acc@5  98.44 ( 98.04)
Epoch: [50][300/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2491e-01 (5.0496e-01)	Acc@1  86.72 ( 84.44)	Acc@5  96.09 ( 98.04)
Epoch: [50][310/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.2113e-01 (5.0557e-01)	Acc@1  88.28 ( 84.43)	Acc@5  99.22 ( 98.04)
Epoch: [50][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.5921e-01 (5.0716e-01)	Acc@1  80.47 ( 84.39)	Acc@5  96.88 ( 98.02)
Epoch: [50][330/391]	Time  0.051 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0482e-01 (5.0656e-01)	Acc@1  84.38 ( 84.41)	Acc@5  96.88 ( 98.02)
Epoch: [50][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5798e-01 (5.0687e-01)	Acc@1  78.12 ( 84.38)	Acc@5  96.88 ( 98.01)
Epoch: [50][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6043e-01 (5.0650e-01)	Acc@1  83.59 ( 84.42)	Acc@5  99.22 ( 98.01)
Epoch: [50][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.8328e-01 (5.0756e-01)	Acc@1  81.25 ( 84.40)	Acc@5  94.53 ( 98.00)
Epoch: [50][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8103e-01 (5.0812e-01)	Acc@1  85.94 ( 84.37)	Acc@5  99.22 ( 98.00)
Epoch: [50][380/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6033e-01 (5.0757e-01)	Acc@1  88.28 ( 84.38)	Acc@5  96.88 ( 98.01)
Epoch: [50][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.5466e-01 (5.0776e-01)	Acc@1  86.25 ( 84.40)	Acc@5  96.25 ( 98.00)
## e[50] optimizer.zero_grad (sum) time: 0.2658827304840088
## e[50]       loss.backward (sum) time: 4.11309814453125
## e[50]      optimizer.step (sum) time: 1.8170695304870605
## epoch[50] training(only) time: 16.209203004837036
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.2489e+00 (1.2489e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.020 ( 0.035)	Loss 1.2384e+00 (1.3378e+00)	Acc@1  62.00 ( 66.73)	Acc@5  93.00 ( 89.82)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 1.0149e+00 (1.2795e+00)	Acc@1  76.00 ( 68.00)	Acc@5  93.00 ( 90.33)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4269e+00 (1.2902e+00)	Acc@1  63.00 ( 67.39)	Acc@5  89.00 ( 90.26)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.1825e+00 (1.2907e+00)	Acc@1  69.00 ( 67.07)	Acc@5  92.00 ( 90.56)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 1.3286e+00 (1.3001e+00)	Acc@1  68.00 ( 66.80)	Acc@5  91.00 ( 90.27)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.4679e+00 (1.2756e+00)	Acc@1  61.00 ( 67.20)	Acc@5  90.00 ( 90.51)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 1.3419e+00 (1.2686e+00)	Acc@1  64.00 ( 67.34)	Acc@5  91.00 ( 90.59)
Test: [ 80/100]	Time  0.023 ( 0.022)	Loss 1.3771e+00 (1.2754e+00)	Acc@1  72.00 ( 67.44)	Acc@5  90.00 ( 90.49)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 1.4664e+00 (1.2613e+00)	Acc@1  66.00 ( 67.77)	Acc@5  88.00 ( 90.65)
 * Acc@1 67.950 Acc@5 90.780
### epoch[50] execution time: 18.508936166763306
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.198 ( 0.198)	Data  0.149 ( 0.149)	Loss 5.1499e-01 (5.1499e-01)	Acc@1  85.16 ( 85.16)	Acc@5  97.66 ( 97.66)
Epoch: [51][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 4.9127e-01 (4.6969e-01)	Acc@1  85.16 ( 85.80)	Acc@5  96.88 ( 98.15)
Epoch: [51][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 3.2520e-01 (4.6792e-01)	Acc@1  93.75 ( 85.90)	Acc@5  98.44 ( 98.29)
Epoch: [51][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.2188e-01 (4.5880e-01)	Acc@1  87.50 ( 86.47)	Acc@5 100.00 ( 98.41)
Epoch: [51][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.8111e-01 (4.7652e-01)	Acc@1  82.81 ( 85.96)	Acc@5  96.09 ( 98.17)
Epoch: [51][ 50/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.3589e-01 (4.7506e-01)	Acc@1  88.28 ( 85.94)	Acc@5  97.66 ( 98.19)
Epoch: [51][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 5.2181e-01 (4.7440e-01)	Acc@1  84.38 ( 85.98)	Acc@5  97.66 ( 98.16)
Epoch: [51][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.7350e-01 (4.7286e-01)	Acc@1  83.59 ( 85.93)	Acc@5  98.44 ( 98.25)
Epoch: [51][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.3271e-01 (4.7900e-01)	Acc@1  82.81 ( 85.58)	Acc@5  99.22 ( 98.25)
Epoch: [51][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.6593e-01 (4.7950e-01)	Acc@1  85.16 ( 85.63)	Acc@5  96.88 ( 98.22)
Epoch: [51][100/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.2086e-01 (4.8275e-01)	Acc@1  87.50 ( 85.50)	Acc@5  98.44 ( 98.19)
Epoch: [51][110/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2455e-01 (4.8106e-01)	Acc@1  86.72 ( 85.52)	Acc@5  99.22 ( 98.21)
Epoch: [51][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3860e-01 (4.7901e-01)	Acc@1  84.38 ( 85.56)	Acc@5 100.00 ( 98.20)
Epoch: [51][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9535e-01 (4.7505e-01)	Acc@1  88.28 ( 85.66)	Acc@5  99.22 ( 98.23)
Epoch: [51][140/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 6.5134e-01 (4.7981e-01)	Acc@1  82.81 ( 85.51)	Acc@5  95.31 ( 98.19)
Epoch: [51][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3982e-01 (4.7792e-01)	Acc@1  82.03 ( 85.52)	Acc@5  97.66 ( 98.19)
Epoch: [51][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.0960e-01 (4.8367e-01)	Acc@1  81.25 ( 85.30)	Acc@5  96.09 ( 98.12)
Epoch: [51][170/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2186e-01 (4.8737e-01)	Acc@1  79.69 ( 85.18)	Acc@5  98.44 ( 98.09)
Epoch: [51][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5651e-01 (4.8622e-01)	Acc@1  83.59 ( 85.23)	Acc@5  97.66 ( 98.09)
Epoch: [51][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2753e-01 (4.8693e-01)	Acc@1  83.59 ( 85.16)	Acc@5  99.22 ( 98.09)
Epoch: [51][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.5672e-01 (4.8709e-01)	Acc@1  82.81 ( 85.13)	Acc@5  97.66 ( 98.10)
Epoch: [51][210/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4187e-01 (4.8797e-01)	Acc@1  85.94 ( 85.12)	Acc@5 100.00 ( 98.12)
Epoch: [51][220/391]	Time  0.055 ( 0.042)	Data  0.002 ( 0.002)	Loss 6.7521e-01 (4.8963e-01)	Acc@1  81.25 ( 85.06)	Acc@5  96.09 ( 98.09)
Epoch: [51][230/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7338e-01 (4.9143e-01)	Acc@1  84.38 ( 84.95)	Acc@5  97.66 ( 98.09)
Epoch: [51][240/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0996e-01 (4.9209e-01)	Acc@1  83.59 ( 84.93)	Acc@5  97.66 ( 98.09)
Epoch: [51][250/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.6217e-01 (4.9293e-01)	Acc@1  78.91 ( 84.87)	Acc@5  98.44 ( 98.10)
Epoch: [51][260/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.5588e-01 (4.9351e-01)	Acc@1  87.50 ( 84.90)	Acc@5  98.44 ( 98.11)
Epoch: [51][270/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4624e-01 (4.9330e-01)	Acc@1  86.72 ( 84.89)	Acc@5  96.88 ( 98.12)
Epoch: [51][280/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8223e-01 (4.9337e-01)	Acc@1  79.69 ( 84.88)	Acc@5  97.66 ( 98.13)
Epoch: [51][290/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3841e-01 (4.9274e-01)	Acc@1  88.28 ( 84.88)	Acc@5  99.22 ( 98.14)
Epoch: [51][300/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.7150e-01 (4.9421e-01)	Acc@1  78.12 ( 84.82)	Acc@5  96.09 ( 98.14)
Epoch: [51][310/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9058e-01 (4.9547e-01)	Acc@1  81.25 ( 84.78)	Acc@5  98.44 ( 98.15)
Epoch: [51][320/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0141e-01 (4.9518e-01)	Acc@1  87.50 ( 84.76)	Acc@5  98.44 ( 98.15)
Epoch: [51][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1467e-01 (4.9553e-01)	Acc@1  82.81 ( 84.74)	Acc@5  95.31 ( 98.15)
Epoch: [51][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7849e-01 (4.9502e-01)	Acc@1  85.94 ( 84.76)	Acc@5  99.22 ( 98.16)
Epoch: [51][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6359e-01 (4.9479e-01)	Acc@1  86.72 ( 84.78)	Acc@5  98.44 ( 98.14)
Epoch: [51][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4729e-01 (4.9592e-01)	Acc@1  82.81 ( 84.75)	Acc@5  98.44 ( 98.12)
Epoch: [51][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0193e-01 (4.9617e-01)	Acc@1  84.38 ( 84.75)	Acc@5  99.22 ( 98.12)
Epoch: [51][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0392e-01 (4.9628e-01)	Acc@1  87.50 ( 84.75)	Acc@5  98.44 ( 98.13)
Epoch: [51][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.1988e-01 (4.9726e-01)	Acc@1  77.50 ( 84.69)	Acc@5  97.50 ( 98.13)
## e[51] optimizer.zero_grad (sum) time: 0.2644693851470947
## e[51]       loss.backward (sum) time: 4.0540900230407715
## e[51]      optimizer.step (sum) time: 1.8105721473693848
## epoch[51] training(only) time: 16.24447798728943
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3229e+00 (1.3229e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2409e+00 (1.3369e+00)	Acc@1  61.00 ( 65.64)	Acc@5  92.00 ( 89.91)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.0051e+00 (1.2527e+00)	Acc@1  74.00 ( 67.33)	Acc@5  93.00 ( 90.81)
Test: [ 30/100]	Time  0.018 ( 0.027)	Loss 1.5134e+00 (1.2736e+00)	Acc@1  63.00 ( 67.19)	Acc@5  88.00 ( 90.52)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.2454e+00 (1.2827e+00)	Acc@1  69.00 ( 66.76)	Acc@5  90.00 ( 90.37)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 1.3029e+00 (1.2903e+00)	Acc@1  73.00 ( 66.59)	Acc@5  91.00 ( 90.16)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 1.4214e+00 (1.2655e+00)	Acc@1  66.00 ( 67.05)	Acc@5  90.00 ( 90.44)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.3792e+00 (1.2577e+00)	Acc@1  66.00 ( 67.31)	Acc@5  89.00 ( 90.46)
Test: [ 80/100]	Time  0.023 ( 0.023)	Loss 1.3227e+00 (1.2607e+00)	Acc@1  69.00 ( 67.35)	Acc@5  90.00 ( 90.43)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.4726e+00 (1.2458e+00)	Acc@1  67.00 ( 67.81)	Acc@5  90.00 ( 90.65)
 * Acc@1 67.910 Acc@5 90.820
### epoch[51] execution time: 18.54578161239624
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.201 ( 0.201)	Data  0.150 ( 0.150)	Loss 5.4478e-01 (5.4478e-01)	Acc@1  83.59 ( 83.59)	Acc@5  98.44 ( 98.44)
Epoch: [52][ 10/391]	Time  0.045 ( 0.057)	Data  0.001 ( 0.015)	Loss 5.3102e-01 (4.6540e-01)	Acc@1  83.59 ( 85.72)	Acc@5  98.44 ( 98.72)
Epoch: [52][ 20/391]	Time  0.042 ( 0.050)	Data  0.002 ( 0.008)	Loss 5.2739e-01 (4.7015e-01)	Acc@1  80.47 ( 85.64)	Acc@5  98.44 ( 98.47)
Epoch: [52][ 30/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.006)	Loss 3.9641e-01 (4.6948e-01)	Acc@1  89.84 ( 85.69)	Acc@5 100.00 ( 98.49)
Epoch: [52][ 40/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.005)	Loss 6.5112e-01 (4.8679e-01)	Acc@1  80.47 ( 84.93)	Acc@5  96.88 ( 98.29)
Epoch: [52][ 50/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.004)	Loss 5.3916e-01 (4.9131e-01)	Acc@1  82.81 ( 85.03)	Acc@5  97.66 ( 98.31)
Epoch: [52][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.8895e-01 (4.8851e-01)	Acc@1  84.38 ( 85.00)	Acc@5  97.66 ( 98.31)
Epoch: [52][ 70/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.3032e-01 (4.8495e-01)	Acc@1  88.28 ( 85.12)	Acc@5  97.66 ( 98.36)
Epoch: [52][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.1773e-01 (4.8233e-01)	Acc@1  86.72 ( 85.33)	Acc@5  96.09 ( 98.33)
Epoch: [52][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.8272e-01 (4.8520e-01)	Acc@1  85.94 ( 85.17)	Acc@5  99.22 ( 98.29)
Epoch: [52][100/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.1211e-01 (4.9219e-01)	Acc@1  85.16 ( 84.99)	Acc@5  97.66 ( 98.28)
Epoch: [52][110/391]	Time  0.046 ( 0.043)	Data  0.001 ( 0.002)	Loss 6.2480e-01 (4.9648e-01)	Acc@1  80.47 ( 84.89)	Acc@5  97.66 ( 98.23)
Epoch: [52][120/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.002)	Loss 5.1316e-01 (4.9896e-01)	Acc@1  86.72 ( 84.82)	Acc@5  97.66 ( 98.18)
Epoch: [52][130/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.4887e-01 (4.9624e-01)	Acc@1  88.28 ( 84.91)	Acc@5 100.00 ( 98.20)
Epoch: [52][140/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2861e-01 (4.9506e-01)	Acc@1  78.91 ( 84.92)	Acc@5  97.66 ( 98.20)
Epoch: [52][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2955e-01 (4.9556e-01)	Acc@1  82.03 ( 84.88)	Acc@5  99.22 ( 98.25)
Epoch: [52][160/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3992e-01 (4.9345e-01)	Acc@1  90.62 ( 85.01)	Acc@5  98.44 ( 98.21)
Epoch: [52][170/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 5.1099e-01 (4.9614e-01)	Acc@1  85.16 ( 84.90)	Acc@5  98.44 ( 98.22)
Epoch: [52][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3601e-01 (4.9378e-01)	Acc@1  83.59 ( 84.98)	Acc@5  99.22 ( 98.25)
Epoch: [52][190/391]	Time  0.038 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.7112e-01 (4.9361e-01)	Acc@1  86.72 ( 85.05)	Acc@5  97.66 ( 98.23)
Epoch: [52][200/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5840e-01 (4.9381e-01)	Acc@1  85.16 ( 85.00)	Acc@5  98.44 ( 98.24)
Epoch: [52][210/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8303e-01 (4.9330e-01)	Acc@1  84.38 ( 85.00)	Acc@5 100.00 ( 98.24)
Epoch: [52][220/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8523e-01 (4.9559e-01)	Acc@1  83.59 ( 84.91)	Acc@5  99.22 ( 98.20)
Epoch: [52][230/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3743e-01 (4.9623e-01)	Acc@1  85.94 ( 84.91)	Acc@5  96.09 ( 98.18)
Epoch: [52][240/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2705e-01 (4.9590e-01)	Acc@1  85.16 ( 84.91)	Acc@5 100.00 ( 98.17)
Epoch: [52][250/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.3553e-01 (4.9526e-01)	Acc@1  81.25 ( 84.90)	Acc@5  98.44 ( 98.18)
Epoch: [52][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8707e-01 (4.9407e-01)	Acc@1  86.72 ( 84.96)	Acc@5  97.66 ( 98.20)
Epoch: [52][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8018e-01 (4.9424e-01)	Acc@1  84.38 ( 84.94)	Acc@5  98.44 ( 98.20)
Epoch: [52][280/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 5.6780e-01 (4.9522e-01)	Acc@1  80.47 ( 84.89)	Acc@5  97.66 ( 98.20)
Epoch: [52][290/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7891e-01 (4.9417e-01)	Acc@1  85.16 ( 84.94)	Acc@5 100.00 ( 98.22)
Epoch: [52][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2192e-01 (4.9512e-01)	Acc@1  85.94 ( 84.90)	Acc@5  98.44 ( 98.21)
Epoch: [52][310/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4594e-01 (4.9490e-01)	Acc@1  87.50 ( 84.89)	Acc@5  99.22 ( 98.20)
Epoch: [52][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6396e-01 (4.9598e-01)	Acc@1  85.16 ( 84.86)	Acc@5  99.22 ( 98.19)
Epoch: [52][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4613e-01 (4.9666e-01)	Acc@1  82.03 ( 84.82)	Acc@5  98.44 ( 98.18)
Epoch: [52][340/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5647e-01 (4.9643e-01)	Acc@1  84.38 ( 84.86)	Acc@5  97.66 ( 98.18)
Epoch: [52][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4097e-01 (4.9817e-01)	Acc@1  78.91 ( 84.81)	Acc@5  96.09 ( 98.14)
Epoch: [52][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1408e-01 (4.9742e-01)	Acc@1  85.94 ( 84.84)	Acc@5  97.66 ( 98.14)
Epoch: [52][370/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.2633e-01 (4.9769e-01)	Acc@1  82.81 ( 84.85)	Acc@5  96.88 ( 98.13)
Epoch: [52][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5829e-01 (4.9714e-01)	Acc@1  82.81 ( 84.88)	Acc@5  96.88 ( 98.13)
Epoch: [52][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2447e-01 (4.9713e-01)	Acc@1  86.25 ( 84.88)	Acc@5  97.50 ( 98.13)
## e[52] optimizer.zero_grad (sum) time: 0.26619887351989746
## e[52]       loss.backward (sum) time: 4.059839248657227
## e[52]      optimizer.step (sum) time: 1.8351552486419678
## epoch[52] training(only) time: 16.2271625995636
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.2486e+00 (1.2486e+00)	Acc@1  63.00 ( 63.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 1.2361e+00 (1.3371e+00)	Acc@1  58.00 ( 65.73)	Acc@5  93.00 ( 90.00)
Test: [ 20/100]	Time  0.021 ( 0.028)	Loss 9.5042e-01 (1.2598e+00)	Acc@1  78.00 ( 68.10)	Acc@5  93.00 ( 91.00)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4737e+00 (1.2771e+00)	Acc@1  60.00 ( 67.35)	Acc@5  87.00 ( 90.42)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 1.2390e+00 (1.2872e+00)	Acc@1  70.00 ( 67.12)	Acc@5  92.00 ( 90.44)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.2705e+00 (1.2926e+00)	Acc@1  67.00 ( 67.04)	Acc@5  91.00 ( 90.16)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4423e+00 (1.2702e+00)	Acc@1  66.00 ( 67.30)	Acc@5  92.00 ( 90.51)
Test: [ 70/100]	Time  0.017 ( 0.024)	Loss 1.3457e+00 (1.2670e+00)	Acc@1  66.00 ( 67.58)	Acc@5  90.00 ( 90.48)
Test: [ 80/100]	Time  0.017 ( 0.024)	Loss 1.3054e+00 (1.2696e+00)	Acc@1  72.00 ( 67.59)	Acc@5  93.00 ( 90.51)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.4239e+00 (1.2577e+00)	Acc@1  69.00 ( 67.88)	Acc@5  90.00 ( 90.71)
 * Acc@1 67.940 Acc@5 90.890
### epoch[52] execution time: 18.63695001602173
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.193 ( 0.193)	Data  0.144 ( 0.144)	Loss 4.9421e-01 (4.9421e-01)	Acc@1  85.16 ( 85.16)	Acc@5  96.09 ( 96.09)
Epoch: [53][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.014)	Loss 4.7053e-01 (4.9650e-01)	Acc@1  82.03 ( 84.73)	Acc@5  98.44 ( 97.80)
Epoch: [53][ 20/391]	Time  0.042 ( 0.049)	Data  0.002 ( 0.008)	Loss 3.5095e-01 (4.6910e-01)	Acc@1  90.62 ( 85.97)	Acc@5  99.22 ( 98.07)
Epoch: [53][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.6520e-01 (4.7754e-01)	Acc@1  87.50 ( 85.64)	Acc@5 100.00 ( 98.21)
Epoch: [53][ 40/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.6857e-01 (4.7809e-01)	Acc@1  92.19 ( 85.90)	Acc@5  97.66 ( 98.15)
Epoch: [53][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.8248e-01 (4.7632e-01)	Acc@1  85.16 ( 85.98)	Acc@5  99.22 ( 98.15)
Epoch: [53][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.6307e-01 (4.7220e-01)	Acc@1  84.38 ( 86.08)	Acc@5  97.66 ( 98.25)
Epoch: [53][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.5776e-01 (4.7210e-01)	Acc@1  85.94 ( 86.03)	Acc@5  96.88 ( 98.21)
Epoch: [53][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.6437e-01 (4.7006e-01)	Acc@1  83.59 ( 86.14)	Acc@5  96.88 ( 98.26)
Epoch: [53][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9766e-01 (4.7147e-01)	Acc@1  88.28 ( 86.11)	Acc@5  99.22 ( 98.23)
Epoch: [53][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.1312e-01 (4.7098e-01)	Acc@1  83.59 ( 86.02)	Acc@5  99.22 ( 98.28)
Epoch: [53][110/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6397e-01 (4.6662e-01)	Acc@1  85.94 ( 86.13)	Acc@5  97.66 ( 98.33)
Epoch: [53][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0976e-01 (4.6942e-01)	Acc@1  79.69 ( 85.92)	Acc@5  99.22 ( 98.33)
Epoch: [53][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9583e-01 (4.6871e-01)	Acc@1  85.94 ( 85.90)	Acc@5  97.66 ( 98.34)
Epoch: [53][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7177e-01 (4.7103e-01)	Acc@1  82.03 ( 85.84)	Acc@5  97.66 ( 98.31)
Epoch: [53][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7244e-01 (4.7534e-01)	Acc@1  82.03 ( 85.72)	Acc@5  98.44 ( 98.28)
Epoch: [53][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4732e-01 (4.7485e-01)	Acc@1  85.16 ( 85.70)	Acc@5  98.44 ( 98.27)
Epoch: [53][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4864e-01 (4.7650e-01)	Acc@1  82.81 ( 85.64)	Acc@5 100.00 ( 98.27)
Epoch: [53][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.9656e-01 (4.7674e-01)	Acc@1  82.81 ( 85.62)	Acc@5  95.31 ( 98.26)
Epoch: [53][190/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4407e-01 (4.7793e-01)	Acc@1  85.16 ( 85.53)	Acc@5  96.09 ( 98.22)
Epoch: [53][200/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6268e-01 (4.7903e-01)	Acc@1  85.16 ( 85.53)	Acc@5  99.22 ( 98.21)
Epoch: [53][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3227e-01 (4.7903e-01)	Acc@1  85.16 ( 85.48)	Acc@5 100.00 ( 98.20)
Epoch: [53][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5694e-01 (4.8007e-01)	Acc@1  85.94 ( 85.46)	Acc@5  97.66 ( 98.18)
Epoch: [53][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3427e-01 (4.8043e-01)	Acc@1  87.50 ( 85.47)	Acc@5  99.22 ( 98.17)
Epoch: [53][240/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7890e-01 (4.7956e-01)	Acc@1  85.94 ( 85.49)	Acc@5  98.44 ( 98.16)
Epoch: [53][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.5842e-01 (4.7948e-01)	Acc@1  82.81 ( 85.49)	Acc@5  99.22 ( 98.18)
Epoch: [53][260/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1092e-01 (4.7959e-01)	Acc@1  83.59 ( 85.52)	Acc@5  99.22 ( 98.17)
Epoch: [53][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7906e-01 (4.7995e-01)	Acc@1  85.94 ( 85.57)	Acc@5  99.22 ( 98.17)
Epoch: [53][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0336e-01 (4.7975e-01)	Acc@1  90.62 ( 85.57)	Acc@5 100.00 ( 98.17)
Epoch: [53][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9858e-01 (4.8031e-01)	Acc@1  84.38 ( 85.54)	Acc@5  98.44 ( 98.17)
Epoch: [53][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1217e-01 (4.8111e-01)	Acc@1  86.72 ( 85.53)	Acc@5 100.00 ( 98.17)
Epoch: [53][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8670e-01 (4.8072e-01)	Acc@1  82.81 ( 85.53)	Acc@5  96.88 ( 98.17)
Epoch: [53][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3620e-01 (4.8148e-01)	Acc@1  87.50 ( 85.49)	Acc@5  96.88 ( 98.17)
Epoch: [53][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4984e-01 (4.8177e-01)	Acc@1  80.47 ( 85.45)	Acc@5  98.44 ( 98.19)
Epoch: [53][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0990e-01 (4.8246e-01)	Acc@1  85.94 ( 85.44)	Acc@5  99.22 ( 98.19)
Epoch: [53][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2512e-01 (4.8286e-01)	Acc@1  82.81 ( 85.45)	Acc@5  97.66 ( 98.19)
Epoch: [53][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0506e-01 (4.8177e-01)	Acc@1  89.06 ( 85.48)	Acc@5  99.22 ( 98.20)
Epoch: [53][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0919e-01 (4.8189e-01)	Acc@1  84.38 ( 85.46)	Acc@5  98.44 ( 98.22)
Epoch: [53][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.3226e-01 (4.8368e-01)	Acc@1  77.34 ( 85.41)	Acc@5  96.88 ( 98.21)
Epoch: [53][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5232e-01 (4.8506e-01)	Acc@1  86.25 ( 85.36)	Acc@5 100.00 ( 98.19)
## e[53] optimizer.zero_grad (sum) time: 0.266176700592041
## e[53]       loss.backward (sum) time: 4.017314672470093
## e[53]      optimizer.step (sum) time: 1.8493013381958008
## epoch[53] training(only) time: 16.13261389732361
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2932e+00 (1.2932e+00)	Acc@1  67.00 ( 67.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 1.3264e+00 (1.3546e+00)	Acc@1  64.00 ( 67.64)	Acc@5  92.00 ( 89.82)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 1.0083e+00 (1.2662e+00)	Acc@1  76.00 ( 68.62)	Acc@5  93.00 ( 90.67)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.5147e+00 (1.2894e+00)	Acc@1  59.00 ( 67.77)	Acc@5  90.00 ( 90.19)
Test: [ 40/100]	Time  0.019 ( 0.026)	Loss 1.2601e+00 (1.3093e+00)	Acc@1  65.00 ( 67.34)	Acc@5  91.00 ( 90.07)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 1.2848e+00 (1.3120e+00)	Acc@1  70.00 ( 67.43)	Acc@5  91.00 ( 89.98)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 1.4457e+00 (1.2893e+00)	Acc@1  60.00 ( 67.61)	Acc@5  92.00 ( 90.43)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 1.3291e+00 (1.2829e+00)	Acc@1  67.00 ( 67.79)	Acc@5  90.00 ( 90.51)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 1.4190e+00 (1.2856e+00)	Acc@1  68.00 ( 67.68)	Acc@5  88.00 ( 90.37)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 1.4160e+00 (1.2669e+00)	Acc@1  67.00 ( 67.99)	Acc@5  89.00 ( 90.62)
 * Acc@1 68.180 Acc@5 90.720
### epoch[53] execution time: 18.435951709747314
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.202 ( 0.202)	Data  0.148 ( 0.148)	Loss 4.8494e-01 (4.8494e-01)	Acc@1  85.16 ( 85.16)	Acc@5  97.66 ( 97.66)
Epoch: [54][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.014)	Loss 4.7867e-01 (4.6843e-01)	Acc@1  84.38 ( 85.44)	Acc@5  96.88 ( 98.37)
Epoch: [54][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 5.4970e-01 (4.8073e-01)	Acc@1  81.25 ( 85.27)	Acc@5  97.66 ( 98.51)
Epoch: [54][ 30/391]	Time  0.040 ( 0.047)	Data  0.002 ( 0.006)	Loss 4.9122e-01 (4.6572e-01)	Acc@1  84.38 ( 85.79)	Acc@5  97.66 ( 98.46)
Epoch: [54][ 40/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.5872e-01 (4.6655e-01)	Acc@1  75.78 ( 85.63)	Acc@5  98.44 ( 98.57)
Epoch: [54][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.2122e-01 (4.7283e-01)	Acc@1  82.03 ( 85.28)	Acc@5  95.31 ( 98.51)
Epoch: [54][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.1111e-01 (4.7227e-01)	Acc@1  88.28 ( 85.13)	Acc@5  99.22 ( 98.45)
Epoch: [54][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.7308e-01 (4.6726e-01)	Acc@1  85.94 ( 85.29)	Acc@5  99.22 ( 98.48)
Epoch: [54][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.3911e-01 (4.6403e-01)	Acc@1  83.59 ( 85.45)	Acc@5  97.66 ( 98.49)
Epoch: [54][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.8861e-01 (4.6494e-01)	Acc@1  82.03 ( 85.41)	Acc@5  96.09 ( 98.45)
Epoch: [54][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.1352e-01 (4.6484e-01)	Acc@1  85.16 ( 85.43)	Acc@5  99.22 ( 98.51)
Epoch: [54][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3771e-01 (4.6846e-01)	Acc@1  86.72 ( 85.42)	Acc@5  97.66 ( 98.49)
Epoch: [54][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7715e-01 (4.6381e-01)	Acc@1  84.38 ( 85.58)	Acc@5  97.66 ( 98.48)
Epoch: [54][130/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.9697e-01 (4.6644e-01)	Acc@1  84.38 ( 85.46)	Acc@5  98.44 ( 98.44)
Epoch: [54][140/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.9224e-01 (4.7016e-01)	Acc@1  84.38 ( 85.36)	Acc@5  98.44 ( 98.42)
Epoch: [54][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0927e-01 (4.7227e-01)	Acc@1  85.16 ( 85.29)	Acc@5  97.66 ( 98.39)
Epoch: [54][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.3566e-01 (4.7560e-01)	Acc@1  80.47 ( 85.20)	Acc@5  96.88 ( 98.36)
Epoch: [54][170/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9478e-01 (4.7451e-01)	Acc@1  87.50 ( 85.27)	Acc@5  98.44 ( 98.36)
Epoch: [54][180/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7309e-01 (4.7419e-01)	Acc@1  85.94 ( 85.30)	Acc@5  98.44 ( 98.36)
Epoch: [54][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4990e-01 (4.7573e-01)	Acc@1  85.94 ( 85.27)	Acc@5  97.66 ( 98.37)
Epoch: [54][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8313e-01 (4.7647e-01)	Acc@1  86.72 ( 85.24)	Acc@5  98.44 ( 98.36)
Epoch: [54][210/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6755e-01 (4.8069e-01)	Acc@1  87.50 ( 85.08)	Acc@5  99.22 ( 98.28)
Epoch: [54][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9393e-01 (4.7966e-01)	Acc@1  85.16 ( 85.05)	Acc@5  99.22 ( 98.31)
Epoch: [54][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.2700e-01 (4.8135e-01)	Acc@1  80.47 ( 85.03)	Acc@5  98.44 ( 98.27)
Epoch: [54][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.1171e-01 (4.8244e-01)	Acc@1  78.12 ( 85.03)	Acc@5  96.88 ( 98.25)
Epoch: [54][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1823e-01 (4.8302e-01)	Acc@1  85.16 ( 84.97)	Acc@5  99.22 ( 98.26)
Epoch: [54][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.8542e-01 (4.8528e-01)	Acc@1  80.47 ( 84.94)	Acc@5  95.31 ( 98.23)
Epoch: [54][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.4027e-01 (4.8461e-01)	Acc@1  78.12 ( 84.96)	Acc@5 100.00 ( 98.26)
Epoch: [54][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2730e-01 (4.8538e-01)	Acc@1  85.94 ( 84.97)	Acc@5  99.22 ( 98.26)
Epoch: [54][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8729e-01 (4.8444e-01)	Acc@1  88.28 ( 84.95)	Acc@5  99.22 ( 98.27)
Epoch: [54][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7141e-01 (4.8327e-01)	Acc@1  89.06 ( 84.98)	Acc@5  99.22 ( 98.29)
Epoch: [54][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3074e-01 (4.8316e-01)	Acc@1  86.72 ( 84.98)	Acc@5  97.66 ( 98.30)
Epoch: [54][320/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2839e-01 (4.8298e-01)	Acc@1  84.38 ( 84.98)	Acc@5  96.88 ( 98.28)
Epoch: [54][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6773e-01 (4.8314e-01)	Acc@1  84.38 ( 85.00)	Acc@5  97.66 ( 98.29)
Epoch: [54][340/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1349e-01 (4.8302e-01)	Acc@1  82.81 ( 85.02)	Acc@5  98.44 ( 98.30)
Epoch: [54][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0110e-01 (4.8275e-01)	Acc@1  88.28 ( 85.05)	Acc@5  99.22 ( 98.30)
Epoch: [54][360/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1948e-01 (4.8235e-01)	Acc@1  88.28 ( 85.05)	Acc@5  96.09 ( 98.31)
Epoch: [54][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3933e-01 (4.8250e-01)	Acc@1  85.94 ( 85.05)	Acc@5  96.88 ( 98.31)
Epoch: [54][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7053e-01 (4.8301e-01)	Acc@1  85.16 ( 85.02)	Acc@5  98.44 ( 98.32)
Epoch: [54][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3128e-01 (4.8361e-01)	Acc@1  82.50 ( 85.02)	Acc@5  98.75 ( 98.30)
## e[54] optimizer.zero_grad (sum) time: 0.26617980003356934
## e[54]       loss.backward (sum) time: 3.986807107925415
## e[54]      optimizer.step (sum) time: 1.847533941268921
## epoch[54] training(only) time: 16.117133617401123
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.2178e+00 (1.2178e+00)	Acc@1  69.00 ( 69.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2978e+00 (1.3280e+00)	Acc@1  66.00 ( 67.73)	Acc@5  92.00 ( 90.27)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.0235e+00 (1.2636e+00)	Acc@1  76.00 ( 68.71)	Acc@5  92.00 ( 90.76)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 1.5132e+00 (1.2915e+00)	Acc@1  60.00 ( 67.65)	Acc@5  90.00 ( 90.39)
Test: [ 40/100]	Time  0.024 ( 0.026)	Loss 1.2428e+00 (1.3006e+00)	Acc@1  70.00 ( 67.54)	Acc@5  95.00 ( 90.49)
Test: [ 50/100]	Time  0.024 ( 0.026)	Loss 1.3690e+00 (1.3085e+00)	Acc@1  66.00 ( 67.33)	Acc@5  91.00 ( 90.33)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4207e+00 (1.2823e+00)	Acc@1  62.00 ( 67.70)	Acc@5  94.00 ( 90.80)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.3506e+00 (1.2770e+00)	Acc@1  66.00 ( 67.79)	Acc@5  94.00 ( 90.97)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 1.4226e+00 (1.2833e+00)	Acc@1  70.00 ( 67.65)	Acc@5  88.00 ( 90.84)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 1.3885e+00 (1.2675e+00)	Acc@1  69.00 ( 68.03)	Acc@5  90.00 ( 91.02)
 * Acc@1 68.130 Acc@5 91.160
### epoch[54] execution time: 18.59057641029358
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.199 ( 0.199)	Data  0.150 ( 0.150)	Loss 3.0296e-01 (3.0296e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.015)	Loss 4.0492e-01 (4.4403e-01)	Acc@1  86.72 ( 86.29)	Acc@5  99.22 ( 98.58)
Epoch: [55][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 5.8319e-01 (4.4989e-01)	Acc@1  78.91 ( 86.05)	Acc@5  98.44 ( 98.77)
Epoch: [55][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.0175e-01 (4.5284e-01)	Acc@1  85.16 ( 86.29)	Acc@5  96.88 ( 98.54)
Epoch: [55][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.9209e-01 (4.4987e-01)	Acc@1  85.16 ( 86.34)	Acc@5  98.44 ( 98.59)
Epoch: [55][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.9117e-01 (4.4559e-01)	Acc@1  88.28 ( 86.47)	Acc@5  99.22 ( 98.68)
Epoch: [55][ 60/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.4712e-01 (4.4534e-01)	Acc@1  87.50 ( 86.64)	Acc@5  98.44 ( 98.63)
Epoch: [55][ 70/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.9312e-01 (4.5047e-01)	Acc@1  86.72 ( 86.44)	Acc@5  98.44 ( 98.65)
Epoch: [55][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.6525e-01 (4.5086e-01)	Acc@1  87.50 ( 86.43)	Acc@5  99.22 ( 98.63)
Epoch: [55][ 90/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.003)	Loss 4.5721e-01 (4.5288e-01)	Acc@1  86.72 ( 86.34)	Acc@5  96.88 ( 98.60)
Epoch: [55][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4240e-01 (4.5335e-01)	Acc@1  88.28 ( 86.28)	Acc@5  99.22 ( 98.56)
Epoch: [55][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7770e-01 (4.5541e-01)	Acc@1  82.03 ( 86.18)	Acc@5  96.09 ( 98.54)
Epoch: [55][120/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6600e-01 (4.5556e-01)	Acc@1  86.72 ( 86.18)	Acc@5  97.66 ( 98.52)
Epoch: [55][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3603e-01 (4.5879e-01)	Acc@1  85.16 ( 86.06)	Acc@5  98.44 ( 98.52)
Epoch: [55][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4158e-01 (4.6113e-01)	Acc@1  85.16 ( 85.97)	Acc@5  99.22 ( 98.54)
Epoch: [55][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5917e-01 (4.6275e-01)	Acc@1  87.50 ( 85.96)	Acc@5  97.66 ( 98.48)
Epoch: [55][160/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.7387e-01 (4.6543e-01)	Acc@1  82.03 ( 85.86)	Acc@5  97.66 ( 98.44)
Epoch: [55][170/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.1391e-01 (4.6846e-01)	Acc@1  81.25 ( 85.77)	Acc@5  96.88 ( 98.42)
Epoch: [55][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9643e-01 (4.6794e-01)	Acc@1  87.50 ( 85.73)	Acc@5 100.00 ( 98.44)
Epoch: [55][190/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5000e-01 (4.6752e-01)	Acc@1  85.16 ( 85.70)	Acc@5  96.88 ( 98.45)
Epoch: [55][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (4.6854e-01)	Acc@1  82.81 ( 85.67)	Acc@5  96.88 ( 98.45)
Epoch: [55][210/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3765e-01 (4.6876e-01)	Acc@1  79.69 ( 85.62)	Acc@5  98.44 ( 98.44)
Epoch: [55][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9025e-01 (4.7084e-01)	Acc@1  86.72 ( 85.51)	Acc@5  99.22 ( 98.41)
Epoch: [55][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0159e-01 (4.7293e-01)	Acc@1  82.03 ( 85.42)	Acc@5  96.09 ( 98.38)
Epoch: [55][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6075e-01 (4.7434e-01)	Acc@1  86.72 ( 85.37)	Acc@5  97.66 ( 98.38)
Epoch: [55][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5620e-01 (4.7553e-01)	Acc@1  90.62 ( 85.29)	Acc@5  98.44 ( 98.38)
Epoch: [55][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3351e-01 (4.7641e-01)	Acc@1  82.81 ( 85.27)	Acc@5  96.88 ( 98.35)
Epoch: [55][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.2697e-01 (4.7596e-01)	Acc@1  76.56 ( 85.27)	Acc@5  96.09 ( 98.35)
Epoch: [55][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4321e-01 (4.7707e-01)	Acc@1  85.16 ( 85.25)	Acc@5  97.66 ( 98.34)
Epoch: [55][290/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9604e-01 (4.7688e-01)	Acc@1  85.16 ( 85.26)	Acc@5  97.66 ( 98.34)
Epoch: [55][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9184e-01 (4.7681e-01)	Acc@1  85.94 ( 85.32)	Acc@5  99.22 ( 98.33)
Epoch: [55][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8138e-01 (4.7769e-01)	Acc@1  85.16 ( 85.31)	Acc@5  99.22 ( 98.33)
Epoch: [55][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8929e-01 (4.7755e-01)	Acc@1  86.72 ( 85.31)	Acc@5  99.22 ( 98.33)
Epoch: [55][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4203e-01 (4.7709e-01)	Acc@1  85.16 ( 85.35)	Acc@5  97.66 ( 98.33)
Epoch: [55][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9057e-01 (4.7765e-01)	Acc@1  84.38 ( 85.31)	Acc@5  97.66 ( 98.32)
Epoch: [55][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3118e-01 (4.7791e-01)	Acc@1  87.50 ( 85.29)	Acc@5  96.88 ( 98.31)
Epoch: [55][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7789e-01 (4.7811e-01)	Acc@1  85.94 ( 85.26)	Acc@5 100.00 ( 98.31)
Epoch: [55][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0147e-01 (4.7709e-01)	Acc@1  85.94 ( 85.32)	Acc@5  97.66 ( 98.32)
Epoch: [55][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9959e-01 (4.7736e-01)	Acc@1  83.59 ( 85.31)	Acc@5  98.44 ( 98.32)
Epoch: [55][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6700e-01 (4.7719e-01)	Acc@1  90.00 ( 85.34)	Acc@5  98.75 ( 98.31)
## e[55] optimizer.zero_grad (sum) time: 0.2651705741882324
## e[55]       loss.backward (sum) time: 4.0511391162872314
## e[55]      optimizer.step (sum) time: 1.8242764472961426
## epoch[55] training(only) time: 16.174484968185425
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.3034e+00 (1.3034e+00)	Acc@1  69.00 ( 69.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.025 ( 0.035)	Loss 1.2330e+00 (1.3509e+00)	Acc@1  62.00 ( 67.00)	Acc@5  94.00 ( 89.73)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 9.3531e-01 (1.2630e+00)	Acc@1  75.00 ( 68.10)	Acc@5  94.00 ( 90.67)
Test: [ 30/100]	Time  0.019 ( 0.027)	Loss 1.5316e+00 (1.2949e+00)	Acc@1  61.00 ( 67.45)	Acc@5  90.00 ( 90.45)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.2798e+00 (1.3047e+00)	Acc@1  70.00 ( 67.27)	Acc@5  92.00 ( 90.51)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.3368e+00 (1.3092e+00)	Acc@1  66.00 ( 67.27)	Acc@5  91.00 ( 90.25)
Test: [ 60/100]	Time  0.019 ( 0.024)	Loss 1.5048e+00 (1.2905e+00)	Acc@1  63.00 ( 67.54)	Acc@5  92.00 ( 90.67)
Test: [ 70/100]	Time  0.024 ( 0.023)	Loss 1.3748e+00 (1.2847e+00)	Acc@1  65.00 ( 67.61)	Acc@5  91.00 ( 90.63)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 1.3626e+00 (1.2839e+00)	Acc@1  69.00 ( 67.56)	Acc@5  91.00 ( 90.64)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 1.3407e+00 (1.2678e+00)	Acc@1  68.00 ( 67.88)	Acc@5  90.00 ( 90.86)
 * Acc@1 67.920 Acc@5 90.980
### epoch[55] execution time: 18.52766513824463
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.208 ( 0.208)	Data  0.157 ( 0.157)	Loss 6.7069e-01 (6.7069e-01)	Acc@1  79.69 ( 79.69)	Acc@5  96.09 ( 96.09)
Epoch: [56][ 10/391]	Time  0.041 ( 0.059)	Data  0.001 ( 0.015)	Loss 4.4708e-01 (4.5330e-01)	Acc@1  89.84 ( 86.36)	Acc@5  96.88 ( 98.51)
Epoch: [56][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.008)	Loss 5.7980e-01 (4.5218e-01)	Acc@1  80.47 ( 86.20)	Acc@5  98.44 ( 98.77)
Epoch: [56][ 30/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.006)	Loss 5.7006e-01 (4.6392e-01)	Acc@1  85.94 ( 85.94)	Acc@5  96.88 ( 98.54)
Epoch: [56][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.3422e-01 (4.6355e-01)	Acc@1  83.59 ( 85.92)	Acc@5  99.22 ( 98.55)
Epoch: [56][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.3703e-01 (4.6032e-01)	Acc@1  89.84 ( 86.06)	Acc@5  99.22 ( 98.45)
Epoch: [56][ 60/391]	Time  0.040 ( 0.044)	Data  0.002 ( 0.004)	Loss 4.8477e-01 (4.5898e-01)	Acc@1  85.94 ( 86.19)	Acc@5  98.44 ( 98.42)
Epoch: [56][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.9667e-01 (4.6076e-01)	Acc@1  85.94 ( 86.26)	Acc@5  96.09 ( 98.37)
Epoch: [56][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.3556e-01 (4.5827e-01)	Acc@1  90.62 ( 86.26)	Acc@5  96.88 ( 98.41)
Epoch: [56][ 90/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.2012e-01 (4.5431e-01)	Acc@1  85.94 ( 86.52)	Acc@5  98.44 ( 98.43)
Epoch: [56][100/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.9621e-01 (4.5461e-01)	Acc@1  92.19 ( 86.44)	Acc@5 100.00 ( 98.42)
Epoch: [56][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1730e-01 (4.5541e-01)	Acc@1  84.38 ( 86.35)	Acc@5  98.44 ( 98.43)
Epoch: [56][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2802e-01 (4.5823e-01)	Acc@1  89.84 ( 86.25)	Acc@5  99.22 ( 98.43)
Epoch: [56][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4305e-01 (4.6023e-01)	Acc@1  82.03 ( 86.17)	Acc@5  99.22 ( 98.38)
Epoch: [56][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8507e-01 (4.6068e-01)	Acc@1  85.94 ( 86.13)	Acc@5  99.22 ( 98.37)
Epoch: [56][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8494e-01 (4.6219e-01)	Acc@1  87.50 ( 86.05)	Acc@5  99.22 ( 98.37)
Epoch: [56][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3835e-01 (4.6174e-01)	Acc@1  89.06 ( 86.08)	Acc@5  99.22 ( 98.37)
Epoch: [56][170/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.2624e-01 (4.5951e-01)	Acc@1  83.59 ( 86.14)	Acc@5  96.88 ( 98.37)
Epoch: [56][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1387e-01 (4.5900e-01)	Acc@1  89.06 ( 86.12)	Acc@5  96.88 ( 98.39)
Epoch: [56][190/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5006e-01 (4.5991e-01)	Acc@1  88.28 ( 86.06)	Acc@5  97.66 ( 98.41)
Epoch: [56][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1209e-01 (4.6094e-01)	Acc@1  82.81 ( 86.05)	Acc@5  99.22 ( 98.40)
Epoch: [56][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5332e-01 (4.6055e-01)	Acc@1  88.28 ( 86.03)	Acc@5  99.22 ( 98.43)
Epoch: [56][220/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.7840e-01 (4.6235e-01)	Acc@1  80.47 ( 85.97)	Acc@5  97.66 ( 98.39)
Epoch: [56][230/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 5.4533e-01 (4.6434e-01)	Acc@1  86.72 ( 85.91)	Acc@5  96.09 ( 98.37)
Epoch: [56][240/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6317e-01 (4.6479e-01)	Acc@1  86.72 ( 85.88)	Acc@5  97.66 ( 98.38)
Epoch: [56][250/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2908e-01 (4.6469e-01)	Acc@1  85.16 ( 85.89)	Acc@5  95.31 ( 98.38)
Epoch: [56][260/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3486e-01 (4.6439e-01)	Acc@1  86.72 ( 85.88)	Acc@5  98.44 ( 98.38)
Epoch: [56][270/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2498e-01 (4.6344e-01)	Acc@1  85.94 ( 85.90)	Acc@5  99.22 ( 98.39)
Epoch: [56][280/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.6318e-01 (4.6429e-01)	Acc@1  83.59 ( 85.86)	Acc@5  96.88 ( 98.38)
Epoch: [56][290/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3808e-01 (4.6395e-01)	Acc@1  86.72 ( 85.88)	Acc@5  98.44 ( 98.39)
Epoch: [56][300/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 6.5254e-01 (4.6552e-01)	Acc@1  81.25 ( 85.85)	Acc@5  96.88 ( 98.38)
Epoch: [56][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2917e-01 (4.6597e-01)	Acc@1  85.16 ( 85.82)	Acc@5  98.44 ( 98.37)
Epoch: [56][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.0971e-01 (4.6799e-01)	Acc@1  82.03 ( 85.74)	Acc@5  96.88 ( 98.35)
Epoch: [56][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2412e-01 (4.6800e-01)	Acc@1  83.59 ( 85.74)	Acc@5  97.66 ( 98.36)
Epoch: [56][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.6002e-01 (4.6884e-01)	Acc@1  79.69 ( 85.70)	Acc@5  98.44 ( 98.37)
Epoch: [56][350/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4753e-01 (4.6913e-01)	Acc@1  85.94 ( 85.69)	Acc@5  98.44 ( 98.37)
Epoch: [56][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7276e-01 (4.6819e-01)	Acc@1  89.84 ( 85.74)	Acc@5  98.44 ( 98.37)
Epoch: [56][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8181e-01 (4.6830e-01)	Acc@1  85.16 ( 85.74)	Acc@5  99.22 ( 98.36)
Epoch: [56][380/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.2497e-01 (4.6892e-01)	Acc@1  85.16 ( 85.71)	Acc@5 100.00 ( 98.36)
Epoch: [56][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7050e-01 (4.6860e-01)	Acc@1  87.50 ( 85.73)	Acc@5  98.75 ( 98.36)
## e[56] optimizer.zero_grad (sum) time: 0.26362037658691406
## e[56]       loss.backward (sum) time: 4.044648170471191
## e[56]      optimizer.step (sum) time: 1.8055825233459473
## epoch[56] training(only) time: 16.198286533355713
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.3331e+00 (1.3331e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.021 ( 0.032)	Loss 1.2782e+00 (1.3476e+00)	Acc@1  68.00 ( 67.55)	Acc@5  93.00 ( 90.27)
Test: [ 20/100]	Time  0.019 ( 0.027)	Loss 1.0911e+00 (1.2882e+00)	Acc@1  73.00 ( 68.24)	Acc@5  92.00 ( 91.05)
Test: [ 30/100]	Time  0.020 ( 0.025)	Loss 1.4340e+00 (1.3101e+00)	Acc@1  61.00 ( 67.26)	Acc@5  89.00 ( 90.52)
Test: [ 40/100]	Time  0.018 ( 0.023)	Loss 1.3289e+00 (1.3239e+00)	Acc@1  69.00 ( 67.12)	Acc@5  92.00 ( 90.41)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 1.3044e+00 (1.3237e+00)	Acc@1  71.00 ( 67.10)	Acc@5  88.00 ( 90.24)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 1.4302e+00 (1.2952e+00)	Acc@1  63.00 ( 67.54)	Acc@5  91.00 ( 90.43)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.4435e+00 (1.2876e+00)	Acc@1  62.00 ( 67.55)	Acc@5  91.00 ( 90.48)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.4170e+00 (1.2937e+00)	Acc@1  68.00 ( 67.47)	Acc@5  90.00 ( 90.41)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 1.4422e+00 (1.2804e+00)	Acc@1  66.00 ( 67.79)	Acc@5  90.00 ( 90.60)
 * Acc@1 67.890 Acc@5 90.800
### epoch[56] execution time: 18.504825353622437
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.205 ( 0.205)	Data  0.153 ( 0.153)	Loss 4.2112e-01 (4.2112e-01)	Acc@1  85.16 ( 85.16)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.015)	Loss 3.2759e-01 (4.4127e-01)	Acc@1  90.62 ( 86.65)	Acc@5  99.22 ( 98.65)
Epoch: [57][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.008)	Loss 2.9637e-01 (4.3727e-01)	Acc@1  92.97 ( 86.68)	Acc@5  98.44 ( 98.59)
Epoch: [57][ 30/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.006)	Loss 5.0918e-01 (4.5699e-01)	Acc@1  83.59 ( 85.91)	Acc@5  98.44 ( 98.34)
Epoch: [57][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.1801e-01 (4.5931e-01)	Acc@1  89.84 ( 86.09)	Acc@5  98.44 ( 98.34)
Epoch: [57][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.0539e-01 (4.4876e-01)	Acc@1  86.72 ( 86.50)	Acc@5  99.22 ( 98.38)
Epoch: [57][ 60/391]	Time  0.040 ( 0.043)	Data  0.002 ( 0.004)	Loss 3.1071e-01 (4.4365e-01)	Acc@1  89.06 ( 86.65)	Acc@5 100.00 ( 98.41)
Epoch: [57][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.9679e-01 (4.4268e-01)	Acc@1  92.19 ( 86.80)	Acc@5  98.44 ( 98.46)
Epoch: [57][ 80/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2248e-01 (4.4352e-01)	Acc@1  88.28 ( 86.77)	Acc@5  99.22 ( 98.48)
Epoch: [57][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2369e-01 (4.4107e-01)	Acc@1  88.28 ( 86.84)	Acc@5  99.22 ( 98.52)
Epoch: [57][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.8606e-01 (4.4478e-01)	Acc@1  83.59 ( 86.67)	Acc@5  99.22 ( 98.48)
Epoch: [57][110/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.003)	Loss 4.2822e-01 (4.4327e-01)	Acc@1  88.28 ( 86.70)	Acc@5  97.66 ( 98.45)
Epoch: [57][120/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9111e-01 (4.4347e-01)	Acc@1  84.38 ( 86.64)	Acc@5  97.66 ( 98.44)
Epoch: [57][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0261e-01 (4.4371e-01)	Acc@1  83.59 ( 86.60)	Acc@5 100.00 ( 98.43)
Epoch: [57][140/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.8543e-01 (4.4221e-01)	Acc@1  87.50 ( 86.67)	Acc@5  97.66 ( 98.45)
Epoch: [57][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3044e-01 (4.4620e-01)	Acc@1  87.50 ( 86.58)	Acc@5  98.44 ( 98.42)
Epoch: [57][160/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0495e-01 (4.4736e-01)	Acc@1  84.38 ( 86.53)	Acc@5  98.44 ( 98.42)
Epoch: [57][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0472e-01 (4.4982e-01)	Acc@1  85.16 ( 86.40)	Acc@5  98.44 ( 98.42)
Epoch: [57][180/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.7856e-01 (4.5059e-01)	Acc@1  85.94 ( 86.30)	Acc@5  98.44 ( 98.42)
Epoch: [57][190/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6571e-01 (4.5024e-01)	Acc@1  88.28 ( 86.30)	Acc@5  96.88 ( 98.42)
Epoch: [57][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0197e-01 (4.5046e-01)	Acc@1  86.72 ( 86.31)	Acc@5  98.44 ( 98.42)
Epoch: [57][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7142e-01 (4.5206e-01)	Acc@1  90.62 ( 86.28)	Acc@5  98.44 ( 98.41)
Epoch: [57][220/391]	Time  0.047 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8749e-01 (4.5195e-01)	Acc@1  89.06 ( 86.29)	Acc@5  97.66 ( 98.39)
Epoch: [57][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9584e-01 (4.5220e-01)	Acc@1  82.81 ( 86.25)	Acc@5  97.66 ( 98.40)
Epoch: [57][240/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0105e-01 (4.5216e-01)	Acc@1  87.50 ( 86.24)	Acc@5  98.44 ( 98.42)
Epoch: [57][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.2200e-01 (4.5354e-01)	Acc@1  86.72 ( 86.21)	Acc@5  96.88 ( 98.42)
Epoch: [57][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1109e-01 (4.5505e-01)	Acc@1  82.03 ( 86.14)	Acc@5  99.22 ( 98.42)
Epoch: [57][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2369e-01 (4.5648e-01)	Acc@1  85.94 ( 86.10)	Acc@5  98.44 ( 98.42)
Epoch: [57][280/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.2827e-01 (4.5751e-01)	Acc@1  85.16 ( 86.09)	Acc@5  97.66 ( 98.42)
Epoch: [57][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7320e-01 (4.5756e-01)	Acc@1  86.72 ( 86.09)	Acc@5 100.00 ( 98.42)
Epoch: [57][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9525e-01 (4.5926e-01)	Acc@1  83.59 ( 86.04)	Acc@5  99.22 ( 98.41)
Epoch: [57][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8583e-01 (4.6068e-01)	Acc@1  87.50 ( 86.01)	Acc@5  99.22 ( 98.39)
Epoch: [57][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4620e-01 (4.6295e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 98.38)
Epoch: [57][330/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.2957e-01 (4.6281e-01)	Acc@1  85.94 ( 85.96)	Acc@5  99.22 ( 98.38)
Epoch: [57][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8396e-01 (4.6163e-01)	Acc@1  86.72 ( 86.02)	Acc@5  98.44 ( 98.37)
Epoch: [57][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.8469e-01 (4.6215e-01)	Acc@1  79.69 ( 86.00)	Acc@5  99.22 ( 98.36)
Epoch: [57][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1026e-01 (4.6196e-01)	Acc@1  82.81 ( 86.00)	Acc@5  97.66 ( 98.36)
Epoch: [57][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7411e-01 (4.6299e-01)	Acc@1  89.06 ( 85.97)	Acc@5  98.44 ( 98.36)
Epoch: [57][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6275e-01 (4.6210e-01)	Acc@1  90.62 ( 86.02)	Acc@5  99.22 ( 98.37)
Epoch: [57][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8119e-01 (4.6197e-01)	Acc@1  88.75 ( 86.03)	Acc@5 100.00 ( 98.36)
## e[57] optimizer.zero_grad (sum) time: 0.2661616802215576
## e[57]       loss.backward (sum) time: 3.9612767696380615
## e[57]      optimizer.step (sum) time: 1.919661521911621
## epoch[57] training(only) time: 15.972978353500366
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2772e+00 (1.2772e+00)	Acc@1  66.00 ( 66.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 1.4060e+00 (1.3859e+00)	Acc@1  62.00 ( 66.55)	Acc@5  91.00 ( 89.82)
Test: [ 20/100]	Time  0.021 ( 0.028)	Loss 1.0117e+00 (1.3195e+00)	Acc@1  75.00 ( 67.29)	Acc@5  93.00 ( 90.52)
Test: [ 30/100]	Time  0.018 ( 0.026)	Loss 1.5477e+00 (1.3382e+00)	Acc@1  58.00 ( 66.97)	Acc@5  90.00 ( 90.13)
Test: [ 40/100]	Time  0.018 ( 0.025)	Loss 1.2451e+00 (1.3407e+00)	Acc@1  71.00 ( 67.05)	Acc@5  92.00 ( 90.49)
Test: [ 50/100]	Time  0.024 ( 0.024)	Loss 1.2762e+00 (1.3358e+00)	Acc@1  71.00 ( 67.06)	Acc@5  92.00 ( 90.24)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.3972e+00 (1.3090e+00)	Acc@1  63.00 ( 67.26)	Acc@5  91.00 ( 90.62)
Test: [ 70/100]	Time  0.019 ( 0.023)	Loss 1.4917e+00 (1.3038e+00)	Acc@1  66.00 ( 67.38)	Acc@5  90.00 ( 90.59)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 1.3718e+00 (1.3076e+00)	Acc@1  69.00 ( 67.28)	Acc@5  91.00 ( 90.41)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.4349e+00 (1.2915e+00)	Acc@1  69.00 ( 67.88)	Acc@5  90.00 ( 90.53)
 * Acc@1 67.920 Acc@5 90.620
### epoch[57] execution time: 18.302221536636353
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.196 ( 0.196)	Data  0.142 ( 0.142)	Loss 4.6577e-01 (4.6577e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [58][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.014)	Loss 3.1528e-01 (3.8926e-01)	Acc@1  89.84 ( 88.92)	Acc@5  99.22 ( 98.86)
Epoch: [58][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.008)	Loss 4.4559e-01 (4.0187e-01)	Acc@1  85.94 ( 88.06)	Acc@5  98.44 ( 98.85)
Epoch: [58][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.8107e-01 (4.0064e-01)	Acc@1  92.97 ( 88.08)	Acc@5  99.22 ( 98.87)
Epoch: [58][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.004)	Loss 3.8407e-01 (4.1325e-01)	Acc@1  88.28 ( 87.69)	Acc@5  99.22 ( 98.70)
Epoch: [58][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.9658e-01 (4.1737e-01)	Acc@1  86.72 ( 87.48)	Acc@5 100.00 ( 98.74)
Epoch: [58][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.7418e-01 (4.2344e-01)	Acc@1  84.38 ( 87.24)	Acc@5  99.22 ( 98.72)
Epoch: [58][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.0450e-01 (4.2913e-01)	Acc@1  80.47 ( 86.93)	Acc@5  97.66 ( 98.65)
Epoch: [58][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.5159e-01 (4.2652e-01)	Acc@1  90.62 ( 87.04)	Acc@5 100.00 ( 98.72)
Epoch: [58][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.2687e-01 (4.2884e-01)	Acc@1  82.03 ( 86.98)	Acc@5  99.22 ( 98.72)
Epoch: [58][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4256e-01 (4.2989e-01)	Acc@1  85.16 ( 86.86)	Acc@5  98.44 ( 98.71)
Epoch: [58][110/391]	Time  0.054 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4300e-01 (4.2998e-01)	Acc@1  87.50 ( 86.83)	Acc@5  98.44 ( 98.70)
Epoch: [58][120/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 5.7165e-01 (4.3289e-01)	Acc@1  83.59 ( 86.79)	Acc@5  97.66 ( 98.66)
Epoch: [58][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0448e-01 (4.3890e-01)	Acc@1  85.16 ( 86.67)	Acc@5  98.44 ( 98.60)
Epoch: [58][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0840e-01 (4.4102e-01)	Acc@1  85.16 ( 86.56)	Acc@5  98.44 ( 98.61)
Epoch: [58][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4361e-01 (4.4418e-01)	Acc@1  86.72 ( 86.44)	Acc@5  96.09 ( 98.58)
Epoch: [58][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8446e-01 (4.4462e-01)	Acc@1  88.28 ( 86.45)	Acc@5  99.22 ( 98.60)
Epoch: [58][170/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.5137e-01 (4.4633e-01)	Acc@1  89.06 ( 86.40)	Acc@5  99.22 ( 98.57)
Epoch: [58][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0257e-01 (4.4538e-01)	Acc@1  86.72 ( 86.42)	Acc@5  99.22 ( 98.58)
Epoch: [58][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6140e-01 (4.4638e-01)	Acc@1  85.94 ( 86.39)	Acc@5  96.88 ( 98.57)
Epoch: [58][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6701e-01 (4.4752e-01)	Acc@1  83.59 ( 86.31)	Acc@5 100.00 ( 98.57)
Epoch: [58][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1828e-01 (4.4760e-01)	Acc@1  84.38 ( 86.29)	Acc@5  99.22 ( 98.59)
Epoch: [58][220/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8940e-01 (4.4780e-01)	Acc@1  85.94 ( 86.24)	Acc@5  99.22 ( 98.58)
Epoch: [58][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2613e-01 (4.4848e-01)	Acc@1  88.28 ( 86.22)	Acc@5  97.66 ( 98.55)
Epoch: [58][240/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.0895e-01 (4.5105e-01)	Acc@1  80.47 ( 86.14)	Acc@5  99.22 ( 98.53)
Epoch: [58][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.1939e-01 (4.5218e-01)	Acc@1  77.34 ( 86.08)	Acc@5  98.44 ( 98.54)
Epoch: [58][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9534e-01 (4.5308e-01)	Acc@1  86.72 ( 86.05)	Acc@5  98.44 ( 98.53)
Epoch: [58][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1266e-01 (4.5182e-01)	Acc@1  88.28 ( 86.11)	Acc@5 100.00 ( 98.56)
Epoch: [58][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6875e-01 (4.5215e-01)	Acc@1  85.94 ( 86.08)	Acc@5  96.09 ( 98.55)
Epoch: [58][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7962e-01 (4.5183e-01)	Acc@1  92.97 ( 86.12)	Acc@5 100.00 ( 98.55)
Epoch: [58][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7876e-01 (4.5199e-01)	Acc@1  84.38 ( 86.11)	Acc@5  98.44 ( 98.56)
Epoch: [58][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8023e-01 (4.5238e-01)	Acc@1  88.28 ( 86.11)	Acc@5  97.66 ( 98.55)
Epoch: [58][320/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3937e-01 (4.5246e-01)	Acc@1  85.16 ( 86.12)	Acc@5  98.44 ( 98.56)
Epoch: [58][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.7355e-01 (4.5359e-01)	Acc@1  86.72 ( 86.10)	Acc@5  97.66 ( 98.54)
Epoch: [58][340/391]	Time  0.053 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4826e-01 (4.5365e-01)	Acc@1  85.94 ( 86.10)	Acc@5  97.66 ( 98.53)
Epoch: [58][350/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.001)	Loss 4.0656e-01 (4.5429e-01)	Acc@1  88.28 ( 86.08)	Acc@5  98.44 ( 98.52)
Epoch: [58][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.3199e-01 (4.5491e-01)	Acc@1  82.03 ( 86.07)	Acc@5  96.09 ( 98.52)
Epoch: [58][370/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.4927e-01 (4.5545e-01)	Acc@1  82.03 ( 86.08)	Acc@5  97.66 ( 98.51)
Epoch: [58][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8737e-01 (4.5544e-01)	Acc@1  86.72 ( 86.10)	Acc@5  97.66 ( 98.51)
Epoch: [58][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4919e-01 (4.5677e-01)	Acc@1  83.75 ( 86.05)	Acc@5  97.50 ( 98.50)
## e[58] optimizer.zero_grad (sum) time: 0.2646815776824951
## e[58]       loss.backward (sum) time: 4.040599822998047
## e[58]      optimizer.step (sum) time: 1.8234913349151611
## epoch[58] training(only) time: 16.15311622619629
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2210e+00 (1.2210e+00)	Acc@1  68.00 ( 68.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 1.3677e+00 (1.3511e+00)	Acc@1  61.00 ( 67.36)	Acc@5  93.00 ( 89.82)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 1.0486e+00 (1.2986e+00)	Acc@1  74.00 ( 67.43)	Acc@5  93.00 ( 90.95)
Test: [ 30/100]	Time  0.024 ( 0.025)	Loss 1.5161e+00 (1.3251e+00)	Acc@1  63.00 ( 66.81)	Acc@5  90.00 ( 90.45)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 1.3076e+00 (1.3335e+00)	Acc@1  67.00 ( 66.68)	Acc@5  93.00 ( 90.46)
Test: [ 50/100]	Time  0.022 ( 0.023)	Loss 1.3280e+00 (1.3353e+00)	Acc@1  62.00 ( 66.39)	Acc@5  90.00 ( 90.27)
Test: [ 60/100]	Time  0.024 ( 0.023)	Loss 1.4594e+00 (1.3089e+00)	Acc@1  65.00 ( 66.77)	Acc@5  90.00 ( 90.67)
Test: [ 70/100]	Time  0.021 ( 0.023)	Loss 1.4660e+00 (1.3071e+00)	Acc@1  66.00 ( 66.96)	Acc@5  90.00 ( 90.72)
Test: [ 80/100]	Time  0.021 ( 0.023)	Loss 1.5244e+00 (1.3124e+00)	Acc@1  65.00 ( 66.91)	Acc@5  88.00 ( 90.51)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.3845e+00 (1.2969e+00)	Acc@1  70.00 ( 67.34)	Acc@5  91.00 ( 90.65)
 * Acc@1 67.630 Acc@5 90.740
### epoch[58] execution time: 18.49023723602295
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.196 ( 0.196)	Data  0.144 ( 0.144)	Loss 4.5041e-01 (4.5041e-01)	Acc@1  83.59 ( 83.59)	Acc@5  98.44 ( 98.44)
Epoch: [59][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.014)	Loss 3.3135e-01 (4.3364e-01)	Acc@1  90.62 ( 86.58)	Acc@5  99.22 ( 98.58)
Epoch: [59][ 20/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.008)	Loss 4.9607e-01 (4.3273e-01)	Acc@1  85.94 ( 86.16)	Acc@5  98.44 ( 98.77)
Epoch: [59][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.5980e-01 (4.2602e-01)	Acc@1  83.59 ( 86.67)	Acc@5 100.00 ( 98.74)
Epoch: [59][ 40/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.1405e-01 (4.2393e-01)	Acc@1  88.28 ( 86.70)	Acc@5  98.44 ( 98.74)
Epoch: [59][ 50/391]	Time  0.045 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.0380e-01 (4.3384e-01)	Acc@1  85.16 ( 86.46)	Acc@5  99.22 ( 98.76)
Epoch: [59][ 60/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.5314e-01 (4.3518e-01)	Acc@1  87.50 ( 86.46)	Acc@5 100.00 ( 98.69)
Epoch: [59][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.1705e-01 (4.3667e-01)	Acc@1  82.03 ( 86.56)	Acc@5  96.09 ( 98.61)
Epoch: [59][ 80/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.3532e-01 (4.3879e-01)	Acc@1  85.94 ( 86.57)	Acc@5  99.22 ( 98.57)
Epoch: [59][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3196e-01 (4.3671e-01)	Acc@1  87.50 ( 86.60)	Acc@5  98.44 ( 98.59)
Epoch: [59][100/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.003)	Loss 4.7036e-01 (4.3964e-01)	Acc@1  85.16 ( 86.59)	Acc@5  99.22 ( 98.55)
Epoch: [59][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9583e-01 (4.4127e-01)	Acc@1  83.59 ( 86.59)	Acc@5  98.44 ( 98.51)
Epoch: [59][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1952e-01 (4.4179e-01)	Acc@1  90.62 ( 86.57)	Acc@5  98.44 ( 98.49)
Epoch: [59][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5130e-01 (4.4408e-01)	Acc@1  89.06 ( 86.41)	Acc@5 100.00 ( 98.52)
Epoch: [59][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0973e-01 (4.4038e-01)	Acc@1  87.50 ( 86.55)	Acc@5  98.44 ( 98.57)
Epoch: [59][150/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.6557e-01 (4.4295e-01)	Acc@1  82.03 ( 86.48)	Acc@5  96.88 ( 98.56)
Epoch: [59][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9807e-01 (4.4325e-01)	Acc@1  92.19 ( 86.49)	Acc@5 100.00 ( 98.56)
Epoch: [59][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1904e-01 (4.4470e-01)	Acc@1  84.38 ( 86.45)	Acc@5  99.22 ( 98.54)
Epoch: [59][180/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6759e-01 (4.4634e-01)	Acc@1  82.03 ( 86.31)	Acc@5  99.22 ( 98.57)
Epoch: [59][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 7.7727e-01 (4.4966e-01)	Acc@1  78.12 ( 86.21)	Acc@5  93.75 ( 98.53)
Epoch: [59][200/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8065e-01 (4.4883e-01)	Acc@1  88.28 ( 86.23)	Acc@5  98.44 ( 98.54)
Epoch: [59][210/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6491e-01 (4.4985e-01)	Acc@1  85.16 ( 86.18)	Acc@5  96.88 ( 98.52)
Epoch: [59][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.7137e-01 (4.5028e-01)	Acc@1  78.91 ( 86.14)	Acc@5  95.31 ( 98.51)
Epoch: [59][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9260e-01 (4.4937e-01)	Acc@1  83.59 ( 86.15)	Acc@5  97.66 ( 98.52)
Epoch: [59][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3288e-01 (4.4938e-01)	Acc@1  82.81 ( 86.08)	Acc@5  99.22 ( 98.55)
Epoch: [59][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 6.5580e-01 (4.4970e-01)	Acc@1  81.25 ( 86.09)	Acc@5  96.09 ( 98.53)
Epoch: [59][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7863e-01 (4.5032e-01)	Acc@1  89.06 ( 86.10)	Acc@5  96.88 ( 98.52)
Epoch: [59][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9554e-01 (4.5211e-01)	Acc@1  86.72 ( 86.03)	Acc@5  99.22 ( 98.50)
Epoch: [59][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4832e-01 (4.5189e-01)	Acc@1  82.03 ( 86.03)	Acc@5  99.22 ( 98.50)
Epoch: [59][290/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7036e-01 (4.5220e-01)	Acc@1  89.84 ( 86.04)	Acc@5 100.00 ( 98.51)
Epoch: [59][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9383e-01 (4.5291e-01)	Acc@1  85.16 ( 86.02)	Acc@5  96.09 ( 98.50)
Epoch: [59][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7814e-01 (4.5396e-01)	Acc@1  89.06 ( 86.01)	Acc@5  97.66 ( 98.48)
Epoch: [59][320/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1158e-01 (4.5504e-01)	Acc@1  89.84 ( 85.96)	Acc@5 100.00 ( 98.46)
Epoch: [59][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8782e-01 (4.5591e-01)	Acc@1  83.59 ( 85.94)	Acc@5  98.44 ( 98.46)
Epoch: [59][340/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.3431e-01 (4.5524e-01)	Acc@1  84.38 ( 85.98)	Acc@5  98.44 ( 98.46)
Epoch: [59][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0670e-01 (4.5493e-01)	Acc@1  87.50 ( 85.98)	Acc@5  99.22 ( 98.46)
Epoch: [59][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0255e-01 (4.5466e-01)	Acc@1  87.50 ( 85.98)	Acc@5  99.22 ( 98.46)
Epoch: [59][370/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8721e-01 (4.5503e-01)	Acc@1  85.94 ( 85.96)	Acc@5  97.66 ( 98.45)
Epoch: [59][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9569e-01 (4.5458e-01)	Acc@1  85.94 ( 85.97)	Acc@5  98.44 ( 98.46)
Epoch: [59][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4183e-01 (4.5526e-01)	Acc@1  85.00 ( 85.97)	Acc@5  97.50 ( 98.44)
## e[59] optimizer.zero_grad (sum) time: 0.26688408851623535
## e[59]       loss.backward (sum) time: 4.014470815658569
## e[59]      optimizer.step (sum) time: 1.8523528575897217
## epoch[59] training(only) time: 16.08828830718994
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2893e+00 (1.2893e+00)	Acc@1  68.00 ( 68.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.026 ( 0.035)	Loss 1.4112e+00 (1.4091e+00)	Acc@1  63.00 ( 67.64)	Acc@5  93.00 ( 90.36)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 1.1470e+00 (1.3316e+00)	Acc@1  73.00 ( 68.57)	Acc@5  91.00 ( 90.57)
Test: [ 30/100]	Time  0.018 ( 0.027)	Loss 1.5836e+00 (1.3553e+00)	Acc@1  58.00 ( 67.61)	Acc@5  89.00 ( 90.42)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 1.3258e+00 (1.3639e+00)	Acc@1  73.00 ( 67.15)	Acc@5  92.00 ( 90.32)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3146e+00 (1.3709e+00)	Acc@1  72.00 ( 66.86)	Acc@5  90.00 ( 90.06)
Test: [ 60/100]	Time  0.019 ( 0.024)	Loss 1.4979e+00 (1.3488e+00)	Acc@1  61.00 ( 67.31)	Acc@5  93.00 ( 90.30)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 1.4874e+00 (1.3401e+00)	Acc@1  65.00 ( 67.48)	Acc@5  89.00 ( 90.42)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.5426e+00 (1.3434e+00)	Acc@1  69.00 ( 67.33)	Acc@5  87.00 ( 90.37)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 1.5279e+00 (1.3292e+00)	Acc@1  66.00 ( 67.66)	Acc@5  87.00 ( 90.49)
 * Acc@1 67.650 Acc@5 90.610
### epoch[59] execution time: 18.370405197143555
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.195 ( 0.195)	Data  0.143 ( 0.143)	Loss 4.3024e-01 (4.3024e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [60][ 10/391]	Time  0.040 ( 0.056)	Data  0.001 ( 0.014)	Loss 3.9825e-01 (4.6579e-01)	Acc@1  89.06 ( 85.44)	Acc@5 100.00 ( 98.72)
Epoch: [60][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.008)	Loss 4.3366e-01 (4.3838e-01)	Acc@1  84.38 ( 86.53)	Acc@5  99.22 ( 98.77)
Epoch: [60][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.6741e-01 (4.2864e-01)	Acc@1  83.59 ( 86.62)	Acc@5  99.22 ( 98.89)
Epoch: [60][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.0350e-01 (4.2767e-01)	Acc@1  86.72 ( 86.81)	Acc@5  99.22 ( 98.82)
Epoch: [60][ 50/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.5944e-01 (4.2757e-01)	Acc@1  89.06 ( 86.95)	Acc@5  99.22 ( 98.81)
Epoch: [60][ 60/391]	Time  0.045 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.6106e-01 (4.2579e-01)	Acc@1  81.25 ( 86.90)	Acc@5 100.00 ( 98.86)
Epoch: [60][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.5407e-01 (4.2169e-01)	Acc@1  89.06 ( 87.05)	Acc@5  99.22 ( 98.84)
Epoch: [60][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.9401e-01 (4.2087e-01)	Acc@1  86.72 ( 87.05)	Acc@5  99.22 ( 98.87)
Epoch: [60][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.9605e-01 (4.2262e-01)	Acc@1  83.59 ( 87.04)	Acc@5  99.22 ( 98.88)
Epoch: [60][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3026e-01 (4.1961e-01)	Acc@1  86.72 ( 87.14)	Acc@5  98.44 ( 98.88)
Epoch: [60][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.8156e-01 (4.2159e-01)	Acc@1  84.38 ( 87.12)	Acc@5  96.09 ( 98.85)
Epoch: [60][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3002e-01 (4.2273e-01)	Acc@1  92.19 ( 87.13)	Acc@5 100.00 ( 98.84)
Epoch: [60][130/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5735e-01 (4.2198e-01)	Acc@1  95.31 ( 87.18)	Acc@5 100.00 ( 98.83)
Epoch: [60][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4981e-01 (4.2190e-01)	Acc@1  90.62 ( 87.13)	Acc@5  98.44 ( 98.83)
Epoch: [60][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3784e-01 (4.2101e-01)	Acc@1  86.72 ( 87.22)	Acc@5  98.44 ( 98.80)
Epoch: [60][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8789e-01 (4.2129e-01)	Acc@1  90.62 ( 87.25)	Acc@5  98.44 ( 98.75)
Epoch: [60][170/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4347e-01 (4.2006e-01)	Acc@1  86.72 ( 87.33)	Acc@5  99.22 ( 98.77)
Epoch: [60][180/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2881e-01 (4.2194e-01)	Acc@1  86.72 ( 87.30)	Acc@5  97.66 ( 98.74)
Epoch: [60][190/391]	Time  0.045 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.4724e-01 (4.1928e-01)	Acc@1  91.41 ( 87.39)	Acc@5  98.44 ( 98.75)
Epoch: [60][200/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 5.0988e-01 (4.1934e-01)	Acc@1  86.72 ( 87.35)	Acc@5  97.66 ( 98.74)
Epoch: [60][210/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7203e-01 (4.2051e-01)	Acc@1  88.28 ( 87.34)	Acc@5 100.00 ( 98.73)
Epoch: [60][220/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4802e-01 (4.2033e-01)	Acc@1  92.97 ( 87.42)	Acc@5  98.44 ( 98.71)
Epoch: [60][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7924e-01 (4.1980e-01)	Acc@1  85.16 ( 87.49)	Acc@5  98.44 ( 98.70)
Epoch: [60][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0239e-01 (4.1997e-01)	Acc@1  87.50 ( 87.50)	Acc@5  97.66 ( 98.69)
Epoch: [60][250/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6435e-01 (4.1982e-01)	Acc@1  87.50 ( 87.54)	Acc@5  98.44 ( 98.69)
Epoch: [60][260/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.9241e-01 (4.2121e-01)	Acc@1  81.25 ( 87.51)	Acc@5  96.88 ( 98.67)
Epoch: [60][270/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7248e-01 (4.2041e-01)	Acc@1  89.06 ( 87.51)	Acc@5  99.22 ( 98.68)
Epoch: [60][280/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6189e-01 (4.1902e-01)	Acc@1  88.28 ( 87.55)	Acc@5 100.00 ( 98.68)
Epoch: [60][290/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.7761e-01 (4.1833e-01)	Acc@1  90.62 ( 87.57)	Acc@5  98.44 ( 98.68)
Epoch: [60][300/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 2.9456e-01 (4.1708e-01)	Acc@1  91.41 ( 87.63)	Acc@5 100.00 ( 98.70)
Epoch: [60][310/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.7133e-01 (4.1596e-01)	Acc@1  89.84 ( 87.68)	Acc@5  98.44 ( 98.71)
Epoch: [60][320/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.001)	Loss 3.5831e-01 (4.1434e-01)	Acc@1  88.28 ( 87.72)	Acc@5  98.44 ( 98.72)
Epoch: [60][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5133e-01 (4.1478e-01)	Acc@1  87.50 ( 87.69)	Acc@5  97.66 ( 98.71)
Epoch: [60][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1490e-01 (4.1413e-01)	Acc@1  88.28 ( 87.72)	Acc@5  99.22 ( 98.72)
Epoch: [60][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9161e-01 (4.1309e-01)	Acc@1  92.19 ( 87.77)	Acc@5  97.66 ( 98.72)
Epoch: [60][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2657e-01 (4.1339e-01)	Acc@1  89.84 ( 87.74)	Acc@5  99.22 ( 98.71)
Epoch: [60][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1889e-01 (4.1187e-01)	Acc@1  95.31 ( 87.79)	Acc@5  99.22 ( 98.72)
Epoch: [60][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8903e-01 (4.1141e-01)	Acc@1  86.72 ( 87.80)	Acc@5 100.00 ( 98.73)
Epoch: [60][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1119e-01 (4.1104e-01)	Acc@1  86.25 ( 87.81)	Acc@5  98.75 ( 98.74)
## e[60] optimizer.zero_grad (sum) time: 0.26497364044189453
## e[60]       loss.backward (sum) time: 4.068796634674072
## e[60]      optimizer.step (sum) time: 1.8256659507751465
## epoch[60] training(only) time: 16.221232414245605
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.2525e+00 (1.2525e+00)	Acc@1  69.00 ( 69.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.019 ( 0.032)	Loss 1.3078e+00 (1.3613e+00)	Acc@1  59.00 ( 67.09)	Acc@5  93.00 ( 90.36)
Test: [ 20/100]	Time  0.024 ( 0.027)	Loss 1.0711e+00 (1.2806e+00)	Acc@1  74.00 ( 68.00)	Acc@5  93.00 ( 91.14)
Test: [ 30/100]	Time  0.022 ( 0.025)	Loss 1.4659e+00 (1.2975e+00)	Acc@1  60.00 ( 67.42)	Acc@5  88.00 ( 90.97)
Test: [ 40/100]	Time  0.025 ( 0.024)	Loss 1.2253e+00 (1.3064e+00)	Acc@1  72.00 ( 67.27)	Acc@5  92.00 ( 91.02)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.2830e+00 (1.3087e+00)	Acc@1  70.00 ( 67.16)	Acc@5  91.00 ( 90.73)
Test: [ 60/100]	Time  0.020 ( 0.023)	Loss 1.4345e+00 (1.2871e+00)	Acc@1  62.00 ( 67.61)	Acc@5  94.00 ( 90.93)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.3988e+00 (1.2793e+00)	Acc@1  65.00 ( 67.85)	Acc@5  90.00 ( 91.00)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.3996e+00 (1.2818e+00)	Acc@1  68.00 ( 67.88)	Acc@5  89.00 ( 90.86)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 1.4913e+00 (1.2670e+00)	Acc@1  66.00 ( 68.24)	Acc@5  90.00 ( 91.09)
 * Acc@1 68.350 Acc@5 91.200
### epoch[60] execution time: 18.50948405265808
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.198 ( 0.198)	Data  0.146 ( 0.146)	Loss 4.8266e-01 (4.8266e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 99.22)
Epoch: [61][ 10/391]	Time  0.042 ( 0.056)	Data  0.002 ( 0.014)	Loss 4.1472e-01 (4.2589e-01)	Acc@1  89.06 ( 87.78)	Acc@5  98.44 ( 98.51)
Epoch: [61][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.008)	Loss 3.7835e-01 (4.2083e-01)	Acc@1  89.84 ( 87.76)	Acc@5  99.22 ( 98.47)
Epoch: [61][ 30/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.5199e-01 (4.0515e-01)	Acc@1  89.06 ( 87.95)	Acc@5  99.22 ( 98.66)
Epoch: [61][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.2505e-01 (4.1019e-01)	Acc@1  85.16 ( 87.48)	Acc@5  99.22 ( 98.69)
Epoch: [61][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.4702e-01 (4.1196e-01)	Acc@1  82.81 ( 87.42)	Acc@5  96.88 ( 98.65)
Epoch: [61][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.1490e-01 (4.0675e-01)	Acc@1  88.28 ( 87.76)	Acc@5  98.44 ( 98.68)
Epoch: [61][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.5316e-01 (4.0713e-01)	Acc@1  89.06 ( 87.79)	Acc@5  98.44 ( 98.69)
Epoch: [61][ 80/391]	Time  0.046 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.0046e-01 (4.0075e-01)	Acc@1  91.41 ( 87.93)	Acc@5 100.00 ( 98.75)
Epoch: [61][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.5743e-01 (4.0026e-01)	Acc@1  88.28 ( 88.02)	Acc@5  96.88 ( 98.70)
Epoch: [61][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8739e-01 (3.9878e-01)	Acc@1  89.84 ( 88.02)	Acc@5  98.44 ( 98.71)
Epoch: [61][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5800e-01 (3.9900e-01)	Acc@1  89.06 ( 87.92)	Acc@5  99.22 ( 98.71)
Epoch: [61][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8651e-01 (3.9692e-01)	Acc@1  87.50 ( 88.00)	Acc@5  99.22 ( 98.73)
Epoch: [61][130/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5175e-01 (3.9736e-01)	Acc@1  85.94 ( 87.98)	Acc@5  96.09 ( 98.72)
Epoch: [61][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6132e-01 (3.9777e-01)	Acc@1  90.62 ( 87.94)	Acc@5  99.22 ( 98.70)
Epoch: [61][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1267e-01 (3.9872e-01)	Acc@1  87.50 ( 87.87)	Acc@5  99.22 ( 98.74)
Epoch: [61][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0672e-01 (3.9854e-01)	Acc@1  92.19 ( 87.90)	Acc@5  99.22 ( 98.74)
Epoch: [61][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1644e-01 (3.9843e-01)	Acc@1  92.19 ( 87.95)	Acc@5  98.44 ( 98.73)
Epoch: [61][180/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.1263e-01 (3.9823e-01)	Acc@1  86.72 ( 87.92)	Acc@5  99.22 ( 98.74)
Epoch: [61][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2389e-01 (3.9611e-01)	Acc@1  92.19 ( 87.97)	Acc@5  99.22 ( 98.76)
Epoch: [61][200/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.9290e-01 (3.9535e-01)	Acc@1  89.84 ( 88.01)	Acc@5  98.44 ( 98.78)
Epoch: [61][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8118e-01 (3.9732e-01)	Acc@1  87.50 ( 87.90)	Acc@5 100.00 ( 98.75)
Epoch: [61][220/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6464e-01 (3.9881e-01)	Acc@1  85.16 ( 87.87)	Acc@5  96.88 ( 98.73)
Epoch: [61][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9970e-01 (3.9887e-01)	Acc@1  83.59 ( 87.84)	Acc@5  97.66 ( 98.73)
Epoch: [61][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2726e-01 (3.9815e-01)	Acc@1  90.62 ( 87.90)	Acc@5 100.00 ( 98.75)
Epoch: [61][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4842e-01 (3.9878e-01)	Acc@1  85.16 ( 87.84)	Acc@5  97.66 ( 98.74)
Epoch: [61][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8213e-01 (3.9881e-01)	Acc@1  83.59 ( 87.86)	Acc@5  98.44 ( 98.75)
Epoch: [61][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4605e-01 (3.9934e-01)	Acc@1  89.06 ( 87.86)	Acc@5  96.88 ( 98.74)
Epoch: [61][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7956e-01 (3.9845e-01)	Acc@1  90.62 ( 87.90)	Acc@5  99.22 ( 98.75)
Epoch: [61][290/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.4506e-01 (3.9913e-01)	Acc@1  80.47 ( 87.88)	Acc@5  96.88 ( 98.75)
Epoch: [61][300/391]	Time  0.037 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.1552e-01 (3.9819e-01)	Acc@1  88.28 ( 87.92)	Acc@5  97.66 ( 98.76)
Epoch: [61][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2389e-01 (3.9870e-01)	Acc@1  91.41 ( 87.92)	Acc@5  99.22 ( 98.77)
Epoch: [61][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9587e-01 (3.9860e-01)	Acc@1  86.72 ( 87.93)	Acc@5  97.66 ( 98.78)
Epoch: [61][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2966e-01 (3.9786e-01)	Acc@1  85.16 ( 87.93)	Acc@5  99.22 ( 98.78)
Epoch: [61][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0218e-01 (3.9794e-01)	Acc@1  88.28 ( 87.95)	Acc@5  99.22 ( 98.80)
Epoch: [61][350/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.8183e-01 (3.9737e-01)	Acc@1  85.94 ( 87.95)	Acc@5  96.88 ( 98.80)
Epoch: [61][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3858e-01 (3.9815e-01)	Acc@1  91.41 ( 87.95)	Acc@5  99.22 ( 98.80)
Epoch: [61][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9566e-01 (3.9839e-01)	Acc@1  90.62 ( 87.98)	Acc@5  97.66 ( 98.78)
Epoch: [61][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8499e-01 (3.9815e-01)	Acc@1  85.94 ( 88.00)	Acc@5  99.22 ( 98.80)
Epoch: [61][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4734e-01 (3.9862e-01)	Acc@1  90.00 ( 87.97)	Acc@5  98.75 ( 98.80)
## e[61] optimizer.zero_grad (sum) time: 0.26527833938598633
## e[61]       loss.backward (sum) time: 3.9711802005767822
## e[61]      optimizer.step (sum) time: 1.8780653476715088
## epoch[61] training(only) time: 16.053819179534912
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2767e+00 (1.2767e+00)	Acc@1  69.00 ( 69.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.025 ( 0.035)	Loss 1.2878e+00 (1.3479e+00)	Acc@1  61.00 ( 68.18)	Acc@5  94.00 ( 90.45)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 1.0616e+00 (1.2665e+00)	Acc@1  74.00 ( 69.00)	Acc@5  92.00 ( 91.29)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 1.4421e+00 (1.2892e+00)	Acc@1  62.00 ( 68.35)	Acc@5  88.00 ( 90.84)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.2426e+00 (1.2992e+00)	Acc@1  72.00 ( 67.90)	Acc@5  91.00 ( 90.80)
Test: [ 50/100]	Time  0.024 ( 0.024)	Loss 1.3288e+00 (1.3027e+00)	Acc@1  69.00 ( 67.78)	Acc@5  90.00 ( 90.51)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.4422e+00 (1.2818e+00)	Acc@1  62.00 ( 68.03)	Acc@5  94.00 ( 90.79)
Test: [ 70/100]	Time  0.018 ( 0.024)	Loss 1.4144e+00 (1.2747e+00)	Acc@1  65.00 ( 68.27)	Acc@5  89.00 ( 90.85)
Test: [ 80/100]	Time  0.019 ( 0.023)	Loss 1.3952e+00 (1.2760e+00)	Acc@1  68.00 ( 68.33)	Acc@5  89.00 ( 90.77)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 1.4988e+00 (1.2622e+00)	Acc@1  66.00 ( 68.66)	Acc@5  89.00 ( 90.96)
 * Acc@1 68.760 Acc@5 91.080
### epoch[61] execution time: 18.42974591255188
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.203 ( 0.203)	Data  0.151 ( 0.151)	Loss 3.4037e-01 (3.4037e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.043 ( 0.057)	Data  0.001 ( 0.015)	Loss 4.1683e-01 (3.8179e-01)	Acc@1  88.28 ( 88.07)	Acc@5  99.22 ( 99.01)
Epoch: [62][ 20/391]	Time  0.039 ( 0.050)	Data  0.001 ( 0.008)	Loss 3.5495e-01 (3.8182e-01)	Acc@1  88.28 ( 88.65)	Acc@5  99.22 ( 98.85)
Epoch: [62][ 30/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.006)	Loss 3.8799e-01 (3.8699e-01)	Acc@1  87.50 ( 88.53)	Acc@5  99.22 ( 98.89)
Epoch: [62][ 40/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.005)	Loss 3.4949e-01 (3.8901e-01)	Acc@1  87.50 ( 88.59)	Acc@5 100.00 ( 98.84)
Epoch: [62][ 50/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.004)	Loss 3.8037e-01 (3.9216e-01)	Acc@1  89.06 ( 88.47)	Acc@5  97.66 ( 98.82)
Epoch: [62][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.8434e-01 (3.9005e-01)	Acc@1  92.19 ( 88.42)	Acc@5 100.00 ( 98.87)
Epoch: [62][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.9055e-01 (3.9104e-01)	Acc@1  88.28 ( 88.26)	Acc@5  99.22 ( 98.92)
Epoch: [62][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.8670e-01 (3.9425e-01)	Acc@1  92.19 ( 88.14)	Acc@5  99.22 ( 98.90)
Epoch: [62][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1056e-01 (3.9338e-01)	Acc@1  89.84 ( 88.21)	Acc@5 100.00 ( 98.94)
Epoch: [62][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2808e-01 (3.9181e-01)	Acc@1  91.41 ( 88.31)	Acc@5  98.44 ( 98.86)
Epoch: [62][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2738e-01 (3.9330e-01)	Acc@1  86.72 ( 88.26)	Acc@5  98.44 ( 98.85)
Epoch: [62][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1022e-01 (3.9519e-01)	Acc@1  85.16 ( 88.21)	Acc@5 100.00 ( 98.88)
Epoch: [62][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4102e-01 (3.9339e-01)	Acc@1  88.28 ( 88.23)	Acc@5  99.22 ( 98.91)
Epoch: [62][140/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.9198e-01 (3.9466e-01)	Acc@1  88.28 ( 88.19)	Acc@5  98.44 ( 98.90)
Epoch: [62][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8646e-01 (3.9583e-01)	Acc@1  89.06 ( 88.09)	Acc@5  97.66 ( 98.89)
Epoch: [62][160/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9352e-01 (3.9638e-01)	Acc@1  92.19 ( 88.09)	Acc@5  99.22 ( 98.85)
Epoch: [62][170/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.1888e-01 (3.9771e-01)	Acc@1  85.94 ( 88.06)	Acc@5  99.22 ( 98.84)
Epoch: [62][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2440e-01 (3.9748e-01)	Acc@1  83.59 ( 88.07)	Acc@5  99.22 ( 98.86)
Epoch: [62][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7557e-01 (3.9802e-01)	Acc@1  89.84 ( 88.05)	Acc@5  97.66 ( 98.86)
Epoch: [62][200/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2353e-01 (3.9802e-01)	Acc@1  90.62 ( 88.05)	Acc@5 100.00 ( 98.85)
Epoch: [62][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0819e-01 (3.9688e-01)	Acc@1  92.97 ( 88.09)	Acc@5 100.00 ( 98.86)
Epoch: [62][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1331e-01 (3.9659e-01)	Acc@1  84.38 ( 88.08)	Acc@5  98.44 ( 98.88)
Epoch: [62][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4915e-01 (3.9731e-01)	Acc@1  85.16 ( 88.10)	Acc@5  97.66 ( 98.88)
Epoch: [62][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9540e-01 (3.9572e-01)	Acc@1  87.50 ( 88.15)	Acc@5 100.00 ( 98.89)
Epoch: [62][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7888e-01 (3.9549e-01)	Acc@1  85.16 ( 88.13)	Acc@5  99.22 ( 98.88)
Epoch: [62][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9157e-01 (3.9478e-01)	Acc@1  88.28 ( 88.16)	Acc@5 100.00 ( 98.90)
Epoch: [62][270/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4800e-01 (3.9432e-01)	Acc@1  88.28 ( 88.21)	Acc@5 100.00 ( 98.89)
Epoch: [62][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9394e-01 (3.9487e-01)	Acc@1  88.28 ( 88.23)	Acc@5  98.44 ( 98.87)
Epoch: [62][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0830e-01 (3.9525e-01)	Acc@1  91.41 ( 88.21)	Acc@5 100.00 ( 98.87)
Epoch: [62][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9538e-01 (3.9484e-01)	Acc@1  88.28 ( 88.23)	Acc@5  98.44 ( 98.87)
Epoch: [62][310/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1141e-01 (3.9552e-01)	Acc@1  85.94 ( 88.22)	Acc@5  99.22 ( 98.86)
Epoch: [62][320/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.3895e-01 (3.9433e-01)	Acc@1  89.84 ( 88.27)	Acc@5  99.22 ( 98.86)
Epoch: [62][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1025e-01 (3.9392e-01)	Acc@1  88.28 ( 88.30)	Acc@5  99.22 ( 98.87)
Epoch: [62][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2468e-01 (3.9325e-01)	Acc@1  92.97 ( 88.30)	Acc@5  98.44 ( 98.87)
Epoch: [62][350/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.5811e-01 (3.9303e-01)	Acc@1  90.62 ( 88.32)	Acc@5  98.44 ( 98.87)
Epoch: [62][360/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.3845e-01 (3.9298e-01)	Acc@1  86.72 ( 88.29)	Acc@5  98.44 ( 98.87)
Epoch: [62][370/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.5016e-01 (3.9347e-01)	Acc@1  84.38 ( 88.28)	Acc@5  97.66 ( 98.86)
Epoch: [62][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.1828e-01 (3.9410e-01)	Acc@1  88.28 ( 88.29)	Acc@5  96.88 ( 98.86)
Epoch: [62][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.6573e-01 (3.9357e-01)	Acc@1  96.25 ( 88.32)	Acc@5  98.75 ( 98.86)
## e[62] optimizer.zero_grad (sum) time: 0.26524996757507324
## e[62]       loss.backward (sum) time: 3.965226650238037
## e[62]      optimizer.step (sum) time: 1.9129152297973633
## epoch[62] training(only) time: 16.034647226333618
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.2761e+00 (1.2761e+00)	Acc@1  67.00 ( 67.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 1.3118e+00 (1.3465e+00)	Acc@1  60.00 ( 67.27)	Acc@5  94.00 ( 90.73)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.0410e+00 (1.2654e+00)	Acc@1  74.00 ( 68.29)	Acc@5  92.00 ( 91.38)
Test: [ 30/100]	Time  0.020 ( 0.026)	Loss 1.4484e+00 (1.2861e+00)	Acc@1  61.00 ( 67.74)	Acc@5  89.00 ( 91.10)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.2220e+00 (1.2958e+00)	Acc@1  71.00 ( 67.49)	Acc@5  93.00 ( 91.20)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 1.2898e+00 (1.2976e+00)	Acc@1  70.00 ( 67.47)	Acc@5  90.00 ( 90.86)
Test: [ 60/100]	Time  0.019 ( 0.023)	Loss 1.4163e+00 (1.2761e+00)	Acc@1  62.00 ( 67.69)	Acc@5  94.00 ( 91.11)
Test: [ 70/100]	Time  0.025 ( 0.023)	Loss 1.4192e+00 (1.2698e+00)	Acc@1  65.00 ( 68.00)	Acc@5  90.00 ( 91.13)
Test: [ 80/100]	Time  0.017 ( 0.023)	Loss 1.3626e+00 (1.2722e+00)	Acc@1  69.00 ( 68.04)	Acc@5  88.00 ( 90.98)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.4654e+00 (1.2577e+00)	Acc@1  67.00 ( 68.42)	Acc@5  90.00 ( 91.11)
 * Acc@1 68.510 Acc@5 91.260
### epoch[62] execution time: 18.336758375167847
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.204 ( 0.204)	Data  0.150 ( 0.150)	Loss 4.5402e-01 (4.5402e-01)	Acc@1  86.72 ( 86.72)	Acc@5  98.44 ( 98.44)
Epoch: [63][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.015)	Loss 3.6308e-01 (3.6730e-01)	Acc@1  88.28 ( 88.92)	Acc@5  99.22 ( 99.22)
Epoch: [63][ 20/391]	Time  0.045 ( 0.049)	Data  0.001 ( 0.008)	Loss 4.6136e-01 (3.8274e-01)	Acc@1  86.72 ( 88.62)	Acc@5  97.66 ( 99.03)
Epoch: [63][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.9607e-01 (3.8901e-01)	Acc@1  91.41 ( 88.46)	Acc@5  98.44 ( 98.84)
Epoch: [63][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.0307e-01 (3.9115e-01)	Acc@1  91.41 ( 88.55)	Acc@5  98.44 ( 98.76)
Epoch: [63][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.6945e-01 (3.8822e-01)	Acc@1  89.84 ( 88.51)	Acc@5  99.22 ( 98.85)
Epoch: [63][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.6171e-01 (3.8996e-01)	Acc@1  88.28 ( 88.49)	Acc@5  98.44 ( 98.80)
Epoch: [63][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.8131e-01 (3.9343e-01)	Acc@1  81.25 ( 88.31)	Acc@5  96.88 ( 98.72)
Epoch: [63][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.5670e-01 (3.9606e-01)	Acc@1  87.50 ( 88.17)	Acc@5 100.00 ( 98.78)
Epoch: [63][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.0255e-01 (3.9485e-01)	Acc@1  86.72 ( 88.27)	Acc@5  98.44 ( 98.75)
Epoch: [63][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4012e-01 (3.9674e-01)	Acc@1  89.06 ( 88.23)	Acc@5  98.44 ( 98.76)
Epoch: [63][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8597e-01 (3.9674e-01)	Acc@1  86.72 ( 88.25)	Acc@5  97.66 ( 98.80)
Epoch: [63][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1918e-01 (3.9649e-01)	Acc@1  87.50 ( 88.32)	Acc@5  97.66 ( 98.79)
Epoch: [63][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2347e-01 (3.9494e-01)	Acc@1  92.19 ( 88.33)	Acc@5  99.22 ( 98.84)
Epoch: [63][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0311e-01 (3.9438e-01)	Acc@1  88.28 ( 88.34)	Acc@5  97.66 ( 98.87)
Epoch: [63][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6705e-01 (3.9174e-01)	Acc@1  88.28 ( 88.44)	Acc@5  99.22 ( 98.88)
Epoch: [63][160/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3454e-01 (3.9056e-01)	Acc@1  85.94 ( 88.50)	Acc@5  99.22 ( 98.88)
Epoch: [63][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6222e-01 (3.9164e-01)	Acc@1  87.50 ( 88.43)	Acc@5  99.22 ( 98.87)
Epoch: [63][180/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.5582e-01 (3.9067e-01)	Acc@1  89.06 ( 88.50)	Acc@5  99.22 ( 98.87)
Epoch: [63][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2693e-01 (3.9021e-01)	Acc@1  86.72 ( 88.49)	Acc@5  99.22 ( 98.87)
Epoch: [63][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7083e-01 (3.9130e-01)	Acc@1  88.28 ( 88.46)	Acc@5 100.00 ( 98.85)
Epoch: [63][210/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6333e-01 (3.9270e-01)	Acc@1  86.72 ( 88.41)	Acc@5 100.00 ( 98.83)
Epoch: [63][220/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8031e-01 (3.9343e-01)	Acc@1  90.62 ( 88.42)	Acc@5  99.22 ( 98.84)
Epoch: [63][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8518e-01 (3.9543e-01)	Acc@1  85.94 ( 88.34)	Acc@5  97.66 ( 98.83)
Epoch: [63][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7564e-01 (3.9498e-01)	Acc@1  85.94 ( 88.32)	Acc@5  97.66 ( 98.84)
Epoch: [63][250/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8929e-01 (3.9497e-01)	Acc@1  87.50 ( 88.30)	Acc@5  99.22 ( 98.85)
Epoch: [63][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2347e-01 (3.9487e-01)	Acc@1  90.62 ( 88.25)	Acc@5 100.00 ( 98.86)
Epoch: [63][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5218e-01 (3.9529e-01)	Acc@1  88.28 ( 88.24)	Acc@5  99.22 ( 98.85)
Epoch: [63][280/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9373e-01 (3.9470e-01)	Acc@1  90.62 ( 88.25)	Acc@5  99.22 ( 98.86)
Epoch: [63][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7588e-01 (3.9438e-01)	Acc@1  83.59 ( 88.24)	Acc@5  98.44 ( 98.86)
Epoch: [63][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2870e-01 (3.9346e-01)	Acc@1  90.62 ( 88.27)	Acc@5  99.22 ( 98.86)
Epoch: [63][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7616e-01 (3.9233e-01)	Acc@1  92.97 ( 88.33)	Acc@5  98.44 ( 98.86)
Epoch: [63][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6781e-01 (3.9229e-01)	Acc@1  86.72 ( 88.32)	Acc@5  97.66 ( 98.86)
Epoch: [63][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0889e-01 (3.9278e-01)	Acc@1  87.50 ( 88.26)	Acc@5  99.22 ( 98.86)
Epoch: [63][340/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3399e-01 (3.9217e-01)	Acc@1  91.41 ( 88.29)	Acc@5  99.22 ( 98.86)
Epoch: [63][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4597e-01 (3.9143e-01)	Acc@1  89.06 ( 88.31)	Acc@5 100.00 ( 98.86)
Epoch: [63][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7282e-01 (3.9219e-01)	Acc@1  82.81 ( 88.31)	Acc@5  99.22 ( 98.85)
Epoch: [63][370/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0959e-01 (3.9193e-01)	Acc@1  89.06 ( 88.32)	Acc@5  97.66 ( 98.85)
Epoch: [63][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2129e-01 (3.9102e-01)	Acc@1  85.94 ( 88.35)	Acc@5  99.22 ( 98.86)
Epoch: [63][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2365e-01 (3.9110e-01)	Acc@1  90.00 ( 88.35)	Acc@5 100.00 ( 98.86)
## e[63] optimizer.zero_grad (sum) time: 0.26575541496276855
## e[63]       loss.backward (sum) time: 3.999274253845215
## e[63]      optimizer.step (sum) time: 1.8850009441375732
## epoch[63] training(only) time: 16.120147705078125
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.2773e+00 (1.2773e+00)	Acc@1  67.00 ( 67.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 1.2683e+00 (1.3389e+00)	Acc@1  60.00 ( 67.55)	Acc@5  94.00 ( 90.82)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 1.0652e+00 (1.2621e+00)	Acc@1  72.00 ( 68.67)	Acc@5  92.00 ( 91.52)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.4714e+00 (1.2801e+00)	Acc@1  59.00 ( 68.16)	Acc@5  89.00 ( 91.16)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.2273e+00 (1.2928e+00)	Acc@1  71.00 ( 67.78)	Acc@5  93.00 ( 91.10)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 1.2952e+00 (1.2977e+00)	Acc@1  69.00 ( 67.76)	Acc@5  92.00 ( 90.78)
Test: [ 60/100]	Time  0.023 ( 0.023)	Loss 1.4095e+00 (1.2760e+00)	Acc@1  61.00 ( 67.89)	Acc@5  93.00 ( 91.05)
Test: [ 70/100]	Time  0.027 ( 0.023)	Loss 1.4390e+00 (1.2716e+00)	Acc@1  66.00 ( 68.07)	Acc@5  90.00 ( 91.08)
Test: [ 80/100]	Time  0.021 ( 0.022)	Loss 1.4003e+00 (1.2748e+00)	Acc@1  66.00 ( 68.09)	Acc@5  88.00 ( 91.02)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 1.4564e+00 (1.2602e+00)	Acc@1  67.00 ( 68.52)	Acc@5  89.00 ( 91.19)
 * Acc@1 68.550 Acc@5 91.270
### epoch[63] execution time: 18.391467809677124
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.202 ( 0.202)	Data  0.148 ( 0.148)	Loss 4.4186e-01 (4.4186e-01)	Acc@1  83.59 ( 83.59)	Acc@5  98.44 ( 98.44)
Epoch: [64][ 10/391]	Time  0.038 ( 0.056)	Data  0.001 ( 0.014)	Loss 4.4907e-01 (3.9452e-01)	Acc@1  84.38 ( 87.14)	Acc@5  98.44 ( 98.79)
Epoch: [64][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.008)	Loss 3.2345e-01 (3.7071e-01)	Acc@1  92.97 ( 88.58)	Acc@5 100.00 ( 99.22)
Epoch: [64][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.006)	Loss 4.1924e-01 (3.6834e-01)	Acc@1  88.28 ( 88.86)	Acc@5  99.22 ( 99.22)
Epoch: [64][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.0204e-01 (3.6833e-01)	Acc@1  87.50 ( 88.85)	Acc@5  99.22 ( 99.20)
Epoch: [64][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.8695e-01 (3.6956e-01)	Acc@1  85.94 ( 89.02)	Acc@5  99.22 ( 99.19)
Epoch: [64][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.8934e-01 (3.7282e-01)	Acc@1  89.06 ( 88.93)	Acc@5  99.22 ( 99.08)
Epoch: [64][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.3950e-01 (3.7278e-01)	Acc@1  86.72 ( 88.95)	Acc@5  99.22 ( 99.08)
Epoch: [64][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1578e-01 (3.7283e-01)	Acc@1  91.41 ( 88.93)	Acc@5  98.44 ( 99.05)
Epoch: [64][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1564e-01 (3.7379e-01)	Acc@1  89.84 ( 88.78)	Acc@5 100.00 ( 99.04)
Epoch: [64][100/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.002)	Loss 5.3106e-01 (3.7908e-01)	Acc@1  84.38 ( 88.67)	Acc@5  96.88 ( 98.99)
Epoch: [64][110/391]	Time  0.050 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7051e-01 (3.7883e-01)	Acc@1  93.75 ( 88.72)	Acc@5 100.00 ( 98.96)
Epoch: [64][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5455e-01 (3.7711e-01)	Acc@1  85.16 ( 88.74)	Acc@5  98.44 ( 98.98)
Epoch: [64][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3149e-01 (3.7694e-01)	Acc@1  85.94 ( 88.67)	Acc@5 100.00 ( 98.98)
Epoch: [64][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1950e-01 (3.7754e-01)	Acc@1  90.62 ( 88.61)	Acc@5  99.22 ( 98.96)
Epoch: [64][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2425e-01 (3.7987e-01)	Acc@1  84.38 ( 88.58)	Acc@5  97.66 ( 98.90)
Epoch: [64][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4551e-01 (3.8073e-01)	Acc@1  86.72 ( 88.55)	Acc@5  97.66 ( 98.90)
Epoch: [64][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8562e-01 (3.8370e-01)	Acc@1  87.50 ( 88.47)	Acc@5  99.22 ( 98.89)
Epoch: [64][180/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1024e-01 (3.8200e-01)	Acc@1  92.97 ( 88.55)	Acc@5  99.22 ( 98.91)
Epoch: [64][190/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.8333e-01 (3.8246e-01)	Acc@1  84.38 ( 88.55)	Acc@5  96.88 ( 98.89)
Epoch: [64][200/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5612e-01 (3.8341e-01)	Acc@1  91.41 ( 88.51)	Acc@5 100.00 ( 98.90)
Epoch: [64][210/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6315e-01 (3.8442e-01)	Acc@1  85.94 ( 88.46)	Acc@5  99.22 ( 98.88)
Epoch: [64][220/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5133e-01 (3.8508e-01)	Acc@1  85.94 ( 88.48)	Acc@5  99.22 ( 98.90)
Epoch: [64][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4813e-01 (3.8377e-01)	Acc@1  89.84 ( 88.53)	Acc@5  99.22 ( 98.90)
Epoch: [64][240/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2105e-01 (3.8308e-01)	Acc@1  89.84 ( 88.58)	Acc@5  98.44 ( 98.90)
Epoch: [64][250/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5581e-01 (3.8528e-01)	Acc@1  85.94 ( 88.51)	Acc@5  98.44 ( 98.89)
Epoch: [64][260/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6397e-01 (3.8565e-01)	Acc@1  86.72 ( 88.51)	Acc@5  99.22 ( 98.90)
Epoch: [64][270/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4727e-01 (3.8637e-01)	Acc@1  85.94 ( 88.47)	Acc@5  97.66 ( 98.87)
Epoch: [64][280/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1191e-01 (3.8749e-01)	Acc@1  84.38 ( 88.42)	Acc@5  99.22 ( 98.85)
Epoch: [64][290/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4472e-01 (3.8683e-01)	Acc@1  86.72 ( 88.48)	Acc@5  99.22 ( 98.86)
Epoch: [64][300/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2120e-01 (3.8689e-01)	Acc@1  87.50 ( 88.46)	Acc@5  97.66 ( 98.87)
Epoch: [64][310/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9525e-01 (3.8586e-01)	Acc@1  93.75 ( 88.51)	Acc@5  99.22 ( 98.87)
Epoch: [64][320/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.4052e-01 (3.8506e-01)	Acc@1  86.72 ( 88.52)	Acc@5 100.00 ( 98.88)
Epoch: [64][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0365e-01 (3.8454e-01)	Acc@1  88.28 ( 88.52)	Acc@5  96.88 ( 98.87)
Epoch: [64][340/391]	Time  0.051 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6817e-01 (3.8456e-01)	Acc@1  83.59 ( 88.50)	Acc@5  99.22 ( 98.87)
Epoch: [64][350/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.9109e-01 (3.8482e-01)	Acc@1  93.75 ( 88.52)	Acc@5  99.22 ( 98.86)
Epoch: [64][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2739e-01 (3.8561e-01)	Acc@1  87.50 ( 88.47)	Acc@5  99.22 ( 98.84)
Epoch: [64][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6421e-01 (3.8525e-01)	Acc@1  87.50 ( 88.49)	Acc@5  99.22 ( 98.85)
Epoch: [64][380/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7315e-01 (3.8609e-01)	Acc@1  89.84 ( 88.44)	Acc@5  99.22 ( 98.86)
Epoch: [64][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0235e-01 (3.8599e-01)	Acc@1  83.75 ( 88.44)	Acc@5  96.25 ( 98.85)
## e[64] optimizer.zero_grad (sum) time: 0.26427245140075684
## e[64]       loss.backward (sum) time: 4.112142086029053
## e[64]      optimizer.step (sum) time: 1.8251843452453613
## epoch[64] training(only) time: 16.189228057861328
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.2448e+00 (1.2448e+00)	Acc@1  69.00 ( 69.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.036)	Loss 1.2895e+00 (1.3460e+00)	Acc@1  62.00 ( 67.73)	Acc@5  94.00 ( 90.73)
Test: [ 20/100]	Time  0.024 ( 0.029)	Loss 1.0508e+00 (1.2681e+00)	Acc@1  75.00 ( 68.76)	Acc@5  92.00 ( 91.38)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.4562e+00 (1.2859e+00)	Acc@1  60.00 ( 68.13)	Acc@5  88.00 ( 91.13)
Test: [ 40/100]	Time  0.018 ( 0.026)	Loss 1.2088e+00 (1.2967e+00)	Acc@1  71.00 ( 67.63)	Acc@5  92.00 ( 90.95)
Test: [ 50/100]	Time  0.017 ( 0.025)	Loss 1.3051e+00 (1.3014e+00)	Acc@1  68.00 ( 67.45)	Acc@5  91.00 ( 90.61)
Test: [ 60/100]	Time  0.018 ( 0.024)	Loss 1.4428e+00 (1.2812e+00)	Acc@1  61.00 ( 67.64)	Acc@5  94.00 ( 90.90)
Test: [ 70/100]	Time  0.018 ( 0.024)	Loss 1.4144e+00 (1.2750e+00)	Acc@1  66.00 ( 67.94)	Acc@5  90.00 ( 90.93)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 1.3918e+00 (1.2761e+00)	Acc@1  70.00 ( 67.93)	Acc@5  89.00 ( 90.85)
Test: [ 90/100]	Time  0.022 ( 0.023)	Loss 1.4922e+00 (1.2617e+00)	Acc@1  66.00 ( 68.34)	Acc@5  90.00 ( 91.05)
 * Acc@1 68.470 Acc@5 91.190
### epoch[64] execution time: 18.571196794509888
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.212 ( 0.212)	Data  0.159 ( 0.159)	Loss 4.4933e-01 (4.4933e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 99.22)
Epoch: [65][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.015)	Loss 5.1372e-01 (4.1005e-01)	Acc@1  82.81 ( 87.50)	Acc@5  98.44 ( 98.86)
Epoch: [65][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 4.5051e-01 (3.9274e-01)	Acc@1  87.50 ( 88.62)	Acc@5  97.66 ( 98.88)
Epoch: [65][ 30/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.9875e-01 (3.8808e-01)	Acc@1  84.38 ( 88.31)	Acc@5  99.22 ( 99.02)
Epoch: [65][ 40/391]	Time  0.040 ( 0.045)	Data  0.002 ( 0.005)	Loss 3.3097e-01 (3.8946e-01)	Acc@1  86.72 ( 88.19)	Acc@5 100.00 ( 98.99)
Epoch: [65][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.5472e-01 (3.8828e-01)	Acc@1  92.97 ( 88.39)	Acc@5  96.88 ( 98.93)
Epoch: [65][ 60/391]	Time  0.041 ( 0.044)	Data  0.002 ( 0.004)	Loss 3.5262e-01 (3.8092e-01)	Acc@1  86.72 ( 88.69)	Acc@5 100.00 ( 98.96)
Epoch: [65][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.2669e-01 (3.8199e-01)	Acc@1  85.16 ( 88.57)	Acc@5  98.44 ( 98.94)
Epoch: [65][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.7540e-01 (3.8392e-01)	Acc@1  89.06 ( 88.53)	Acc@5  99.22 ( 98.89)
Epoch: [65][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.6364e-01 (3.8442e-01)	Acc@1  88.28 ( 88.59)	Acc@5  99.22 ( 98.90)
Epoch: [65][100/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.8249e-01 (3.7924e-01)	Acc@1  89.84 ( 88.75)	Acc@5 100.00 ( 98.95)
Epoch: [65][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9671e-01 (3.7878e-01)	Acc@1  88.28 ( 88.75)	Acc@5  99.22 ( 98.99)
Epoch: [65][120/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7499e-01 (3.7871e-01)	Acc@1  85.94 ( 88.71)	Acc@5 100.00 ( 98.97)
Epoch: [65][130/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0981e-01 (3.8093e-01)	Acc@1  86.72 ( 88.65)	Acc@5  97.66 ( 98.90)
Epoch: [65][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0220e-01 (3.8129e-01)	Acc@1  88.28 ( 88.65)	Acc@5  99.22 ( 98.89)
Epoch: [65][150/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1793e-01 (3.8208e-01)	Acc@1  86.72 ( 88.59)	Acc@5  95.31 ( 98.88)
Epoch: [65][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8747e-01 (3.8492e-01)	Acc@1  83.59 ( 88.50)	Acc@5  98.44 ( 98.87)
Epoch: [65][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8348e-01 (3.8462e-01)	Acc@1  86.72 ( 88.52)	Acc@5  99.22 ( 98.87)
Epoch: [65][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3332e-01 (3.8563e-01)	Acc@1  89.84 ( 88.51)	Acc@5 100.00 ( 98.84)
Epoch: [65][190/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0834e-01 (3.8550e-01)	Acc@1  92.19 ( 88.50)	Acc@5  99.22 ( 98.85)
Epoch: [65][200/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0876e-01 (3.8490e-01)	Acc@1  89.06 ( 88.51)	Acc@5 100.00 ( 98.85)
Epoch: [65][210/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5015e-01 (3.8627e-01)	Acc@1  87.50 ( 88.44)	Acc@5  99.22 ( 98.84)
Epoch: [65][220/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.1307e-01 (3.8695e-01)	Acc@1  90.62 ( 88.45)	Acc@5  98.44 ( 98.84)
Epoch: [65][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8831e-01 (3.8843e-01)	Acc@1  89.06 ( 88.42)	Acc@5  99.22 ( 98.83)
Epoch: [65][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2173e-01 (3.8842e-01)	Acc@1  85.16 ( 88.45)	Acc@5  98.44 ( 98.82)
Epoch: [65][250/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2312e-01 (3.8856e-01)	Acc@1  87.50 ( 88.42)	Acc@5  97.66 ( 98.81)
Epoch: [65][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5279e-01 (3.8885e-01)	Acc@1  86.72 ( 88.40)	Acc@5  97.66 ( 98.82)
Epoch: [65][270/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.0934e-01 (3.8951e-01)	Acc@1  92.19 ( 88.42)	Acc@5 100.00 ( 98.82)
Epoch: [65][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2296e-01 (3.8818e-01)	Acc@1  92.97 ( 88.47)	Acc@5  99.22 ( 98.83)
Epoch: [65][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3881e-01 (3.8910e-01)	Acc@1  90.62 ( 88.45)	Acc@5  99.22 ( 98.85)
Epoch: [65][300/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3628e-01 (3.8908e-01)	Acc@1  92.19 ( 88.42)	Acc@5 100.00 ( 98.84)
Epoch: [65][310/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.4233e-01 (3.8963e-01)	Acc@1  85.16 ( 88.41)	Acc@5  96.88 ( 98.83)
Epoch: [65][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1059e-01 (3.8939e-01)	Acc@1  91.41 ( 88.41)	Acc@5 100.00 ( 98.82)
Epoch: [65][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1651e-01 (3.8842e-01)	Acc@1  85.94 ( 88.43)	Acc@5  97.66 ( 98.82)
Epoch: [65][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4507e-01 (3.8860e-01)	Acc@1  90.62 ( 88.43)	Acc@5  98.44 ( 98.81)
Epoch: [65][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9706e-01 (3.8816e-01)	Acc@1  85.16 ( 88.43)	Acc@5  98.44 ( 98.82)
Epoch: [65][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4789e-01 (3.8917e-01)	Acc@1  85.16 ( 88.40)	Acc@5  97.66 ( 98.81)
Epoch: [65][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3687e-01 (3.8978e-01)	Acc@1  89.84 ( 88.38)	Acc@5  99.22 ( 98.80)
Epoch: [65][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9338e-01 (3.8972e-01)	Acc@1  87.50 ( 88.39)	Acc@5  98.44 ( 98.82)
Epoch: [65][390/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0027e-01 (3.8938e-01)	Acc@1  88.75 ( 88.38)	Acc@5 100.00 ( 98.83)
## e[65] optimizer.zero_grad (sum) time: 0.26707935333251953
## e[65]       loss.backward (sum) time: 4.018418550491333
## e[65]      optimizer.step (sum) time: 1.853114366531372
## epoch[65] training(only) time: 16.121544361114502
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2645e+00 (1.2645e+00)	Acc@1  69.00 ( 69.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.2945e+00 (1.3414e+00)	Acc@1  59.00 ( 67.09)	Acc@5  93.00 ( 91.00)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 1.0688e+00 (1.2681e+00)	Acc@1  75.00 ( 68.48)	Acc@5  93.00 ( 91.57)
Test: [ 30/100]	Time  0.017 ( 0.027)	Loss 1.4424e+00 (1.2874e+00)	Acc@1  59.00 ( 67.90)	Acc@5  89.00 ( 91.32)
Test: [ 40/100]	Time  0.025 ( 0.025)	Loss 1.2116e+00 (1.2968e+00)	Acc@1  70.00 ( 67.54)	Acc@5  93.00 ( 91.29)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 1.3111e+00 (1.3010e+00)	Acc@1  70.00 ( 67.61)	Acc@5  91.00 ( 91.02)
Test: [ 60/100]	Time  0.017 ( 0.024)	Loss 1.4192e+00 (1.2798e+00)	Acc@1  63.00 ( 67.82)	Acc@5  93.00 ( 91.25)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 1.3933e+00 (1.2744e+00)	Acc@1  66.00 ( 68.06)	Acc@5  90.00 ( 91.21)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 1.3996e+00 (1.2762e+00)	Acc@1  70.00 ( 68.02)	Acc@5  90.00 ( 91.11)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 1.4222e+00 (1.2613e+00)	Acc@1  70.00 ( 68.54)	Acc@5  89.00 ( 91.23)
 * Acc@1 68.650 Acc@5 91.350
### epoch[65] execution time: 18.487738847732544
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.199 ( 0.199)	Data  0.144 ( 0.144)	Loss 3.8700e-01 (3.8700e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [66][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 4.1177e-01 (3.9417e-01)	Acc@1  85.94 ( 88.28)	Acc@5  98.44 ( 98.93)
Epoch: [66][ 20/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.008)	Loss 2.7279e-01 (3.8110e-01)	Acc@1  91.41 ( 88.91)	Acc@5  99.22 ( 98.96)
Epoch: [66][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.9658e-01 (3.7931e-01)	Acc@1  89.84 ( 88.94)	Acc@5  98.44 ( 99.07)
Epoch: [66][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 5.6307e-01 (3.8891e-01)	Acc@1  81.25 ( 88.38)	Acc@5  96.09 ( 98.82)
Epoch: [66][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.7859e-01 (3.9289e-01)	Acc@1  89.06 ( 88.28)	Acc@5  97.66 ( 98.73)
Epoch: [66][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.3966e-01 (3.9133e-01)	Acc@1  89.84 ( 88.35)	Acc@5  99.22 ( 98.74)
Epoch: [66][ 70/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.6313e-01 (3.9464e-01)	Acc@1  85.94 ( 88.16)	Acc@5  96.88 ( 98.72)
Epoch: [66][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.9471e-01 (3.9290e-01)	Acc@1  86.72 ( 88.20)	Acc@5  98.44 ( 98.73)
Epoch: [66][ 90/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.0087e-01 (3.9677e-01)	Acc@1  83.59 ( 88.11)	Acc@5  99.22 ( 98.76)
Epoch: [66][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5774e-01 (3.9569e-01)	Acc@1  85.16 ( 88.08)	Acc@5  97.66 ( 98.78)
Epoch: [66][110/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.4725e-01 (3.9512e-01)	Acc@1  89.06 ( 88.06)	Acc@5  99.22 ( 98.79)
Epoch: [66][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1417e-01 (3.9483e-01)	Acc@1  88.28 ( 88.11)	Acc@5  97.66 ( 98.79)
Epoch: [66][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0659e-01 (3.9062e-01)	Acc@1  92.97 ( 88.25)	Acc@5 100.00 ( 98.81)
Epoch: [66][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3005e-01 (3.8998e-01)	Acc@1  91.41 ( 88.30)	Acc@5 100.00 ( 98.84)
Epoch: [66][150/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.5279e-01 (3.8895e-01)	Acc@1  83.59 ( 88.30)	Acc@5 100.00 ( 98.87)
Epoch: [66][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8160e-01 (3.8885e-01)	Acc@1  89.06 ( 88.32)	Acc@5  98.44 ( 98.87)
Epoch: [66][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6978e-01 (3.9029e-01)	Acc@1  89.06 ( 88.28)	Acc@5  98.44 ( 98.84)
Epoch: [66][180/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5311e-01 (3.9007e-01)	Acc@1  86.72 ( 88.27)	Acc@5  97.66 ( 98.84)
Epoch: [66][190/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.2793e-01 (3.9078e-01)	Acc@1  89.84 ( 88.26)	Acc@5  99.22 ( 98.81)
Epoch: [66][200/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1757e-01 (3.9030e-01)	Acc@1  82.03 ( 88.32)	Acc@5  96.88 ( 98.82)
Epoch: [66][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6359e-01 (3.9205e-01)	Acc@1  85.94 ( 88.22)	Acc@5  99.22 ( 98.83)
Epoch: [66][220/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8320e-01 (3.9135e-01)	Acc@1  89.06 ( 88.24)	Acc@5  99.22 ( 98.84)
Epoch: [66][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0328e-01 (3.9094e-01)	Acc@1  84.38 ( 88.28)	Acc@5  97.66 ( 98.84)
Epoch: [66][240/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7934e-01 (3.9140e-01)	Acc@1  90.62 ( 88.26)	Acc@5 100.00 ( 98.84)
Epoch: [66][250/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4937e-01 (3.9177e-01)	Acc@1  89.84 ( 88.27)	Acc@5  99.22 ( 98.82)
Epoch: [66][260/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8515e-01 (3.9052e-01)	Acc@1  87.50 ( 88.33)	Acc@5 100.00 ( 98.82)
Epoch: [66][270/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9096e-01 (3.9047e-01)	Acc@1  81.25 ( 88.31)	Acc@5  98.44 ( 98.82)
Epoch: [66][280/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.4563e-01 (3.8927e-01)	Acc@1  89.06 ( 88.33)	Acc@5 100.00 ( 98.83)
Epoch: [66][290/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.3538e-01 (3.8856e-01)	Acc@1  84.38 ( 88.36)	Acc@5  99.22 ( 98.85)
Epoch: [66][300/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.001)	Loss 3.4773e-01 (3.8800e-01)	Acc@1  92.19 ( 88.39)	Acc@5  98.44 ( 98.84)
Epoch: [66][310/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.8703e-01 (3.8865e-01)	Acc@1  85.94 ( 88.36)	Acc@5 100.00 ( 98.85)
Epoch: [66][320/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.0178e-01 (3.8916e-01)	Acc@1  87.50 ( 88.35)	Acc@5  99.22 ( 98.84)
Epoch: [66][330/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.7143e-01 (3.8918e-01)	Acc@1  85.94 ( 88.35)	Acc@5 100.00 ( 98.84)
Epoch: [66][340/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.001)	Loss 4.1593e-01 (3.8865e-01)	Acc@1  87.50 ( 88.37)	Acc@5  97.66 ( 98.85)
Epoch: [66][350/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.6610e-01 (3.8827e-01)	Acc@1  85.94 ( 88.37)	Acc@5  97.66 ( 98.86)
Epoch: [66][360/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.9182e-01 (3.8825e-01)	Acc@1  88.28 ( 88.39)	Acc@5  96.88 ( 98.86)
Epoch: [66][370/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.6882e-01 (3.8855e-01)	Acc@1  89.84 ( 88.38)	Acc@5  99.22 ( 98.86)
Epoch: [66][380/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.8965e-01 (3.8913e-01)	Acc@1  86.72 ( 88.37)	Acc@5  96.88 ( 98.86)
Epoch: [66][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5735e-01 (3.9015e-01)	Acc@1  85.00 ( 88.34)	Acc@5  98.75 ( 98.85)
## e[66] optimizer.zero_grad (sum) time: 0.26569604873657227
## e[66]       loss.backward (sum) time: 4.146945953369141
## e[66]      optimizer.step (sum) time: 1.8267481327056885
## epoch[66] training(only) time: 16.297470808029175
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.2770e+00 (1.2770e+00)	Acc@1  67.00 ( 67.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.018 ( 0.033)	Loss 1.2475e+00 (1.3418e+00)	Acc@1  61.00 ( 67.27)	Acc@5  94.00 ( 90.45)
Test: [ 20/100]	Time  0.020 ( 0.027)	Loss 1.0771e+00 (1.2679e+00)	Acc@1  73.00 ( 68.14)	Acc@5  92.00 ( 91.24)
Test: [ 30/100]	Time  0.019 ( 0.025)	Loss 1.4512e+00 (1.2870e+00)	Acc@1  60.00 ( 67.68)	Acc@5  88.00 ( 90.94)
Test: [ 40/100]	Time  0.017 ( 0.024)	Loss 1.2087e+00 (1.2964e+00)	Acc@1  73.00 ( 67.54)	Acc@5  94.00 ( 91.02)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 1.3015e+00 (1.3016e+00)	Acc@1  71.00 ( 67.61)	Acc@5  91.00 ( 90.75)
Test: [ 60/100]	Time  0.017 ( 0.023)	Loss 1.4276e+00 (1.2800e+00)	Acc@1  63.00 ( 67.90)	Acc@5  93.00 ( 91.00)
Test: [ 70/100]	Time  0.022 ( 0.022)	Loss 1.4200e+00 (1.2735e+00)	Acc@1  66.00 ( 68.18)	Acc@5  90.00 ( 90.97)
Test: [ 80/100]	Time  0.024 ( 0.022)	Loss 1.3731e+00 (1.2755e+00)	Acc@1  70.00 ( 68.20)	Acc@5  88.00 ( 90.86)
Test: [ 90/100]	Time  0.019 ( 0.022)	Loss 1.4751e+00 (1.2611e+00)	Acc@1  68.00 ( 68.57)	Acc@5  90.00 ( 91.07)
 * Acc@1 68.600 Acc@5 91.200
### epoch[66] execution time: 18.53692865371704
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.199 ( 0.199)	Data  0.150 ( 0.150)	Loss 3.5613e-01 (3.5613e-01)	Acc@1  92.97 ( 92.97)	Acc@5  98.44 ( 98.44)
Epoch: [67][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.015)	Loss 3.2372e-01 (3.7746e-01)	Acc@1  91.41 ( 88.57)	Acc@5 100.00 ( 99.01)
Epoch: [67][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 3.8162e-01 (3.7609e-01)	Acc@1  89.06 ( 88.62)	Acc@5  99.22 ( 99.14)
Epoch: [67][ 30/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.006)	Loss 3.6420e-01 (3.7621e-01)	Acc@1  87.50 ( 88.79)	Acc@5  99.22 ( 99.09)
Epoch: [67][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.5501e-01 (3.8055e-01)	Acc@1  88.28 ( 88.59)	Acc@5  99.22 ( 99.05)
Epoch: [67][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.0994e-01 (3.7404e-01)	Acc@1  90.62 ( 88.76)	Acc@5 100.00 ( 99.10)
Epoch: [67][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.9076e-01 (3.7723e-01)	Acc@1  88.28 ( 88.67)	Acc@5  99.22 ( 99.04)
Epoch: [67][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.5060e-01 (3.8054e-01)	Acc@1  89.84 ( 88.57)	Acc@5  99.22 ( 99.05)
Epoch: [67][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4885e-01 (3.8094e-01)	Acc@1  87.50 ( 88.59)	Acc@5 100.00 ( 99.04)
Epoch: [67][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3256e-01 (3.7913e-01)	Acc@1  85.16 ( 88.67)	Acc@5  98.44 ( 99.04)
Epoch: [67][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.6420e-01 (3.7930e-01)	Acc@1  88.28 ( 88.56)	Acc@5  98.44 ( 99.05)
Epoch: [67][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4497e-01 (3.8062e-01)	Acc@1  88.28 ( 88.55)	Acc@5  98.44 ( 99.01)
Epoch: [67][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1658e-01 (3.8266e-01)	Acc@1  85.16 ( 88.58)	Acc@5  99.22 ( 99.01)
Epoch: [67][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0773e-01 (3.8128e-01)	Acc@1  89.84 ( 88.68)	Acc@5 100.00 ( 99.00)
Epoch: [67][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5221e-01 (3.8230e-01)	Acc@1  87.50 ( 88.58)	Acc@5  99.22 ( 99.01)
Epoch: [67][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6721e-01 (3.7957e-01)	Acc@1  89.06 ( 88.62)	Acc@5  98.44 ( 99.02)
Epoch: [67][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6121e-01 (3.8013e-01)	Acc@1  89.06 ( 88.63)	Acc@5  99.22 ( 99.04)
Epoch: [67][170/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7494e-01 (3.8133e-01)	Acc@1  88.28 ( 88.57)	Acc@5  99.22 ( 99.01)
Epoch: [67][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1159e-01 (3.8261e-01)	Acc@1  87.50 ( 88.54)	Acc@5  98.44 ( 98.99)
Epoch: [67][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9725e-01 (3.8230e-01)	Acc@1  86.72 ( 88.61)	Acc@5  99.22 ( 98.96)
Epoch: [67][200/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5284e-01 (3.8130e-01)	Acc@1  91.41 ( 88.68)	Acc@5  98.44 ( 98.97)
Epoch: [67][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5105e-01 (3.8241e-01)	Acc@1  89.84 ( 88.67)	Acc@5 100.00 ( 98.96)
Epoch: [67][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4653e-01 (3.8321e-01)	Acc@1  89.84 ( 88.66)	Acc@5  99.22 ( 98.97)
Epoch: [67][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7979e-01 (3.8403e-01)	Acc@1  83.59 ( 88.64)	Acc@5  98.44 ( 98.96)
Epoch: [67][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9147e-01 (3.8347e-01)	Acc@1  92.19 ( 88.70)	Acc@5  99.22 ( 98.94)
Epoch: [67][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3572e-01 (3.8320e-01)	Acc@1  92.19 ( 88.71)	Acc@5 100.00 ( 98.95)
Epoch: [67][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4327e-01 (3.8309e-01)	Acc@1  91.41 ( 88.73)	Acc@5  99.22 ( 98.95)
Epoch: [67][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0096e-01 (3.8325e-01)	Acc@1  86.72 ( 88.70)	Acc@5  98.44 ( 98.94)
Epoch: [67][280/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5562e-01 (3.8241e-01)	Acc@1  89.06 ( 88.71)	Acc@5  97.66 ( 98.94)
Epoch: [67][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6333e-01 (3.8258e-01)	Acc@1  85.16 ( 88.72)	Acc@5  98.44 ( 98.94)
Epoch: [67][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1387e-01 (3.8263e-01)	Acc@1  89.06 ( 88.72)	Acc@5  99.22 ( 98.94)
Epoch: [67][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4029e-01 (3.8248e-01)	Acc@1  85.16 ( 88.70)	Acc@5  98.44 ( 98.94)
Epoch: [67][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8310e-01 (3.8321e-01)	Acc@1  83.59 ( 88.67)	Acc@5  99.22 ( 98.93)
Epoch: [67][330/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3347e-01 (3.8246e-01)	Acc@1  86.72 ( 88.68)	Acc@5  99.22 ( 98.95)
Epoch: [67][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9784e-01 (3.8305e-01)	Acc@1  89.06 ( 88.62)	Acc@5  97.66 ( 98.96)
Epoch: [67][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4876e-01 (3.8306e-01)	Acc@1  87.50 ( 88.64)	Acc@5 100.00 ( 98.95)
Epoch: [67][360/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1330e-01 (3.8378e-01)	Acc@1  89.06 ( 88.61)	Acc@5  96.88 ( 98.94)
Epoch: [67][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9322e-01 (3.8368e-01)	Acc@1  81.25 ( 88.61)	Acc@5  98.44 ( 98.94)
Epoch: [67][380/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.4509e-01 (3.8414e-01)	Acc@1  87.50 ( 88.59)	Acc@5  98.44 ( 98.94)
Epoch: [67][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3388e-01 (3.8441e-01)	Acc@1  91.25 ( 88.59)	Acc@5  98.75 ( 98.92)
## e[67] optimizer.zero_grad (sum) time: 0.2676379680633545
## e[67]       loss.backward (sum) time: 4.037822723388672
## e[67]      optimizer.step (sum) time: 1.8602592945098877
## epoch[67] training(only) time: 16.11530828475952
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2569e+00 (1.2569e+00)	Acc@1  67.00 ( 67.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.021 ( 0.033)	Loss 1.2993e+00 (1.3523e+00)	Acc@1  60.00 ( 67.55)	Acc@5  94.00 ( 90.64)
Test: [ 20/100]	Time  0.023 ( 0.027)	Loss 1.0787e+00 (1.2727e+00)	Acc@1  74.00 ( 68.57)	Acc@5  93.00 ( 91.14)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 1.4304e+00 (1.2920e+00)	Acc@1  61.00 ( 68.03)	Acc@5  88.00 ( 90.90)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.2096e+00 (1.3006e+00)	Acc@1  70.00 ( 67.71)	Acc@5  93.00 ( 90.88)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.2977e+00 (1.3027e+00)	Acc@1  71.00 ( 67.67)	Acc@5  90.00 ( 90.57)
Test: [ 60/100]	Time  0.018 ( 0.024)	Loss 1.4251e+00 (1.2821e+00)	Acc@1  62.00 ( 67.80)	Acc@5  92.00 ( 90.82)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.3801e+00 (1.2761e+00)	Acc@1  66.00 ( 68.11)	Acc@5  90.00 ( 90.87)
Test: [ 80/100]	Time  0.024 ( 0.024)	Loss 1.3882e+00 (1.2775e+00)	Acc@1  71.00 ( 68.12)	Acc@5  90.00 ( 90.90)
Test: [ 90/100]	Time  0.017 ( 0.023)	Loss 1.5075e+00 (1.2622e+00)	Acc@1  68.00 ( 68.59)	Acc@5  90.00 ( 91.12)
 * Acc@1 68.750 Acc@5 91.250
### epoch[67] execution time: 18.472582578659058
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.196 ( 0.196)	Data  0.145 ( 0.145)	Loss 2.7982e-01 (2.7982e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.043 ( 0.057)	Data  0.001 ( 0.014)	Loss 3.8816e-01 (3.6826e-01)	Acc@1  89.06 ( 88.99)	Acc@5  98.44 ( 99.22)
Epoch: [68][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.008)	Loss 3.9293e-01 (3.6056e-01)	Acc@1  90.62 ( 89.36)	Acc@5  99.22 ( 99.18)
Epoch: [68][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.006)	Loss 3.8061e-01 (3.5745e-01)	Acc@1  88.28 ( 89.59)	Acc@5  98.44 ( 99.19)
Epoch: [68][ 40/391]	Time  0.045 ( 0.045)	Data  0.002 ( 0.005)	Loss 4.1072e-01 (3.5424e-01)	Acc@1  88.28 ( 89.56)	Acc@5  98.44 ( 99.18)
Epoch: [68][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.1248e-01 (3.5827e-01)	Acc@1  85.94 ( 89.32)	Acc@5 100.00 ( 99.20)
Epoch: [68][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.7179e-01 (3.6052e-01)	Acc@1  94.53 ( 89.31)	Acc@5  99.22 ( 99.19)
Epoch: [68][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.0987e-01 (3.6644e-01)	Acc@1  87.50 ( 89.12)	Acc@5  99.22 ( 99.09)
Epoch: [68][ 80/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4553e-01 (3.6419e-01)	Acc@1  92.19 ( 89.29)	Acc@5  97.66 ( 99.07)
Epoch: [68][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.4005e-01 (3.6464e-01)	Acc@1  86.72 ( 89.28)	Acc@5  97.66 ( 99.06)
Epoch: [68][100/391]	Time  0.040 ( 0.043)	Data  0.002 ( 0.002)	Loss 3.3007e-01 (3.6724e-01)	Acc@1  90.62 ( 89.12)	Acc@5  99.22 ( 99.06)
Epoch: [68][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9968e-01 (3.6761e-01)	Acc@1  83.59 ( 89.06)	Acc@5  97.66 ( 99.08)
Epoch: [68][120/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.1557e-01 (3.6768e-01)	Acc@1  89.06 ( 89.04)	Acc@5 100.00 ( 99.09)
Epoch: [68][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9437e-01 (3.6691e-01)	Acc@1  87.50 ( 89.09)	Acc@5  97.66 ( 99.11)
Epoch: [68][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9761e-01 (3.6866e-01)	Acc@1  91.41 ( 89.08)	Acc@5  97.66 ( 99.07)
Epoch: [68][150/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5823e-01 (3.7024e-01)	Acc@1  90.62 ( 89.00)	Acc@5  96.88 ( 99.06)
Epoch: [68][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6744e-01 (3.7262e-01)	Acc@1  92.97 ( 88.93)	Acc@5  99.22 ( 99.05)
Epoch: [68][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6749e-01 (3.7033e-01)	Acc@1  92.97 ( 89.09)	Acc@5  97.66 ( 99.05)
Epoch: [68][180/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1751e-01 (3.7252e-01)	Acc@1  87.50 ( 88.99)	Acc@5  97.66 ( 99.03)
Epoch: [68][190/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3511e-01 (3.7395e-01)	Acc@1  85.94 ( 88.93)	Acc@5 100.00 ( 99.04)
Epoch: [68][200/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6235e-01 (3.7401e-01)	Acc@1  92.19 ( 88.92)	Acc@5 100.00 ( 99.03)
Epoch: [68][210/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5382e-01 (3.7502e-01)	Acc@1  87.50 ( 88.88)	Acc@5  98.44 ( 99.03)
Epoch: [68][220/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8682e-01 (3.7649e-01)	Acc@1  84.38 ( 88.84)	Acc@5  97.66 ( 99.00)
Epoch: [68][230/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8952e-01 (3.7755e-01)	Acc@1  85.94 ( 88.84)	Acc@5  99.22 ( 98.98)
Epoch: [68][240/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5598e-01 (3.7900e-01)	Acc@1  83.59 ( 88.75)	Acc@5 100.00 ( 98.98)
Epoch: [68][250/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0571e-01 (3.7763e-01)	Acc@1  85.94 ( 88.80)	Acc@5  96.88 ( 98.98)
Epoch: [68][260/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9432e-01 (3.7938e-01)	Acc@1  88.28 ( 88.74)	Acc@5  98.44 ( 98.97)
Epoch: [68][270/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.0268e-01 (3.8027e-01)	Acc@1  87.50 ( 88.76)	Acc@5  97.66 ( 98.95)
Epoch: [68][280/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5545e-01 (3.8006e-01)	Acc@1  85.16 ( 88.73)	Acc@5  98.44 ( 98.95)
Epoch: [68][290/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.2930e-01 (3.8121e-01)	Acc@1  86.72 ( 88.71)	Acc@5  98.44 ( 98.94)
Epoch: [68][300/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3656e-01 (3.8073e-01)	Acc@1  87.50 ( 88.72)	Acc@5  99.22 ( 98.95)
Epoch: [68][310/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.1036e-01 (3.8122e-01)	Acc@1  90.62 ( 88.71)	Acc@5  99.22 ( 98.95)
Epoch: [68][320/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3008e-01 (3.8132e-01)	Acc@1  87.50 ( 88.69)	Acc@5 100.00 ( 98.95)
Epoch: [68][330/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1095e-01 (3.8193e-01)	Acc@1  89.06 ( 88.67)	Acc@5  98.44 ( 98.94)
Epoch: [68][340/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7513e-01 (3.8171e-01)	Acc@1  91.41 ( 88.69)	Acc@5 100.00 ( 98.95)
Epoch: [68][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6426e-01 (3.8181e-01)	Acc@1  85.16 ( 88.69)	Acc@5  96.88 ( 98.94)
Epoch: [68][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2564e-01 (3.8286e-01)	Acc@1  85.16 ( 88.65)	Acc@5  98.44 ( 98.94)
Epoch: [68][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3755e-01 (3.8254e-01)	Acc@1  85.16 ( 88.66)	Acc@5 100.00 ( 98.95)
Epoch: [68][380/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5414e-01 (3.8293e-01)	Acc@1  92.19 ( 88.66)	Acc@5  97.66 ( 98.94)
Epoch: [68][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2742e-01 (3.8344e-01)	Acc@1  93.75 ( 88.64)	Acc@5  98.75 ( 98.93)
## e[68] optimizer.zero_grad (sum) time: 0.26627135276794434
## e[68]       loss.backward (sum) time: 4.103377819061279
## e[68]      optimizer.step (sum) time: 1.8027808666229248
## epoch[68] training(only) time: 16.202995538711548
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.2732e+00 (1.2732e+00)	Acc@1  67.00 ( 67.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2664e+00 (1.3367e+00)	Acc@1  60.00 ( 66.64)	Acc@5  94.00 ( 90.55)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 1.0724e+00 (1.2649e+00)	Acc@1  73.00 ( 67.95)	Acc@5  92.00 ( 91.14)
Test: [ 30/100]	Time  0.026 ( 0.028)	Loss 1.4780e+00 (1.2874e+00)	Acc@1  61.00 ( 67.61)	Acc@5  88.00 ( 90.74)
Test: [ 40/100]	Time  0.019 ( 0.026)	Loss 1.2506e+00 (1.3007e+00)	Acc@1  70.00 ( 67.24)	Acc@5  93.00 ( 90.73)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.3147e+00 (1.3057e+00)	Acc@1  71.00 ( 67.22)	Acc@5  90.00 ( 90.45)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 1.4486e+00 (1.2828e+00)	Acc@1  62.00 ( 67.59)	Acc@5  91.00 ( 90.75)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.4592e+00 (1.2776e+00)	Acc@1  65.00 ( 67.89)	Acc@5  90.00 ( 90.79)
Test: [ 80/100]	Time  0.020 ( 0.024)	Loss 1.3659e+00 (1.2796e+00)	Acc@1  72.00 ( 67.91)	Acc@5  90.00 ( 90.72)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 1.5250e+00 (1.2654e+00)	Acc@1  68.00 ( 68.33)	Acc@5  89.00 ( 90.91)
 * Acc@1 68.480 Acc@5 91.120
### epoch[68] execution time: 18.633779525756836
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.218 ( 0.218)	Data  0.167 ( 0.167)	Loss 4.0074e-01 (4.0074e-01)	Acc@1  88.28 ( 88.28)	Acc@5  98.44 ( 98.44)
Epoch: [69][ 10/391]	Time  0.040 ( 0.058)	Data  0.001 ( 0.016)	Loss 4.3641e-01 (4.0609e-01)	Acc@1  88.28 ( 88.42)	Acc@5  99.22 ( 98.51)
Epoch: [69][ 20/391]	Time  0.042 ( 0.050)	Data  0.001 ( 0.009)	Loss 5.0042e-01 (4.0875e-01)	Acc@1  85.16 ( 88.54)	Acc@5  97.66 ( 98.36)
Epoch: [69][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.006)	Loss 4.0025e-01 (3.9415e-01)	Acc@1  87.50 ( 88.84)	Acc@5  97.66 ( 98.49)
Epoch: [69][ 40/391]	Time  0.037 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.3684e-01 (4.0270e-01)	Acc@1  85.94 ( 88.13)	Acc@5  98.44 ( 98.59)
Epoch: [69][ 50/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.5531e-01 (4.0484e-01)	Acc@1  89.06 ( 88.11)	Acc@5  99.22 ( 98.67)
Epoch: [69][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.7111e-01 (4.0283e-01)	Acc@1  89.06 ( 88.17)	Acc@5  98.44 ( 98.63)
Epoch: [69][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.9375e-01 (3.9930e-01)	Acc@1  91.41 ( 88.34)	Acc@5 100.00 ( 98.71)
Epoch: [69][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.0119e-01 (3.9680e-01)	Acc@1  85.16 ( 88.37)	Acc@5  98.44 ( 98.75)
Epoch: [69][ 90/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.6928e-01 (3.9529e-01)	Acc@1  89.06 ( 88.45)	Acc@5  98.44 ( 98.76)
Epoch: [69][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2723e-01 (3.9479e-01)	Acc@1  87.50 ( 88.47)	Acc@5  97.66 ( 98.75)
Epoch: [69][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9607e-01 (3.9226e-01)	Acc@1  87.50 ( 88.51)	Acc@5  99.22 ( 98.80)
Epoch: [69][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8828e-01 (3.9101e-01)	Acc@1  86.72 ( 88.51)	Acc@5  98.44 ( 98.81)
Epoch: [69][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7081e-01 (3.8823e-01)	Acc@1  89.06 ( 88.65)	Acc@5  98.44 ( 98.83)
Epoch: [69][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8262e-01 (3.8670e-01)	Acc@1  91.41 ( 88.66)	Acc@5  99.22 ( 98.85)
Epoch: [69][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7154e-01 (3.8732e-01)	Acc@1  87.50 ( 88.61)	Acc@5  98.44 ( 98.87)
Epoch: [69][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6594e-01 (3.8601e-01)	Acc@1  88.28 ( 88.57)	Acc@5  99.22 ( 98.88)
Epoch: [69][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6456e-01 (3.8980e-01)	Acc@1  87.50 ( 88.40)	Acc@5  99.22 ( 98.86)
Epoch: [69][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9115e-01 (3.8808e-01)	Acc@1  85.16 ( 88.48)	Acc@5 100.00 ( 98.89)
Epoch: [69][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5681e-01 (3.8792e-01)	Acc@1  85.94 ( 88.47)	Acc@5  98.44 ( 98.88)
Epoch: [69][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3645e-01 (3.8919e-01)	Acc@1  92.97 ( 88.43)	Acc@5  99.22 ( 98.86)
Epoch: [69][210/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5569e-01 (3.8755e-01)	Acc@1  89.84 ( 88.46)	Acc@5  98.44 ( 98.86)
Epoch: [69][220/391]	Time  0.047 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8252e-01 (3.8722e-01)	Acc@1  87.50 ( 88.48)	Acc@5 100.00 ( 98.85)
Epoch: [69][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3618e-01 (3.8729e-01)	Acc@1  87.50 ( 88.46)	Acc@5  96.09 ( 98.84)
Epoch: [69][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3793e-01 (3.8673e-01)	Acc@1  89.84 ( 88.49)	Acc@5  98.44 ( 98.84)
Epoch: [69][250/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.7178e-01 (3.8773e-01)	Acc@1  85.94 ( 88.46)	Acc@5  99.22 ( 98.83)
Epoch: [69][260/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1784e-01 (3.8738e-01)	Acc@1  89.84 ( 88.45)	Acc@5 100.00 ( 98.83)
Epoch: [69][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5695e-01 (3.8828e-01)	Acc@1  90.62 ( 88.42)	Acc@5  98.44 ( 98.82)
Epoch: [69][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5714e-01 (3.8856e-01)	Acc@1  86.72 ( 88.39)	Acc@5  99.22 ( 98.82)
Epoch: [69][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3254e-01 (3.8790e-01)	Acc@1  89.06 ( 88.39)	Acc@5 100.00 ( 98.83)
Epoch: [69][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0958e-01 (3.8802e-01)	Acc@1  95.31 ( 88.38)	Acc@5 100.00 ( 98.83)
Epoch: [69][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5327e-01 (3.8927e-01)	Acc@1  88.28 ( 88.36)	Acc@5 100.00 ( 98.82)
Epoch: [69][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3561e-01 (3.8821e-01)	Acc@1  92.19 ( 88.41)	Acc@5  98.44 ( 98.82)
Epoch: [69][330/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3136e-01 (3.8767e-01)	Acc@1  94.53 ( 88.45)	Acc@5 100.00 ( 98.82)
Epoch: [69][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5994e-01 (3.8748e-01)	Acc@1  86.72 ( 88.45)	Acc@5  96.88 ( 98.82)
Epoch: [69][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2886e-01 (3.8648e-01)	Acc@1  89.84 ( 88.50)	Acc@5  99.22 ( 98.83)
Epoch: [69][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6500e-01 (3.8589e-01)	Acc@1  91.41 ( 88.49)	Acc@5 100.00 ( 98.84)
Epoch: [69][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8570e-01 (3.8557e-01)	Acc@1  90.62 ( 88.50)	Acc@5 100.00 ( 98.84)
Epoch: [69][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8314e-01 (3.8566e-01)	Acc@1  88.28 ( 88.49)	Acc@5 100.00 ( 98.84)
Epoch: [69][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1804e-01 (3.8581e-01)	Acc@1  90.00 ( 88.51)	Acc@5  97.50 ( 98.82)
## e[69] optimizer.zero_grad (sum) time: 0.26570725440979004
## e[69]       loss.backward (sum) time: 4.065365314483643
## e[69]      optimizer.step (sum) time: 1.8432724475860596
## epoch[69] training(only) time: 16.231833696365356
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.2638e+00 (1.2638e+00)	Acc@1  68.00 ( 68.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2588e+00 (1.3378e+00)	Acc@1  60.00 ( 67.64)	Acc@5  93.00 ( 90.64)
Test: [ 20/100]	Time  0.020 ( 0.028)	Loss 1.0699e+00 (1.2668e+00)	Acc@1  76.00 ( 68.71)	Acc@5  93.00 ( 91.43)
Test: [ 30/100]	Time  0.018 ( 0.026)	Loss 1.4633e+00 (1.2873e+00)	Acc@1  61.00 ( 67.94)	Acc@5  89.00 ( 91.03)
Test: [ 40/100]	Time  0.018 ( 0.025)	Loss 1.2483e+00 (1.3007e+00)	Acc@1  71.00 ( 67.51)	Acc@5  92.00 ( 91.10)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 1.3215e+00 (1.3071e+00)	Acc@1  69.00 ( 67.43)	Acc@5  89.00 ( 90.75)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.4730e+00 (1.2863e+00)	Acc@1  61.00 ( 67.61)	Acc@5  92.00 ( 91.07)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.4050e+00 (1.2794e+00)	Acc@1  67.00 ( 67.97)	Acc@5  88.00 ( 91.06)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.3706e+00 (1.2817e+00)	Acc@1  69.00 ( 67.91)	Acc@5  90.00 ( 90.91)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 1.4833e+00 (1.2668e+00)	Acc@1  69.00 ( 68.30)	Acc@5  90.00 ( 91.10)
 * Acc@1 68.380 Acc@5 91.280
### epoch[69] execution time: 18.563366174697876
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.208 ( 0.208)	Data  0.155 ( 0.155)	Loss 4.1760e-01 (4.1760e-01)	Acc@1  90.62 ( 90.62)	Acc@5  97.66 ( 97.66)
Epoch: [70][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.015)	Loss 4.4322e-01 (3.8659e-01)	Acc@1  89.06 ( 89.20)	Acc@5  97.66 ( 98.65)
Epoch: [70][ 20/391]	Time  0.042 ( 0.050)	Data  0.001 ( 0.008)	Loss 4.4892e-01 (3.7749e-01)	Acc@1  84.38 ( 89.25)	Acc@5  99.22 ( 98.66)
Epoch: [70][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.006)	Loss 3.6036e-01 (3.6768e-01)	Acc@1  87.50 ( 88.99)	Acc@5 100.00 ( 99.02)
Epoch: [70][ 40/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.005)	Loss 4.0828e-01 (3.7899e-01)	Acc@1  89.84 ( 88.62)	Acc@5  96.88 ( 98.82)
Epoch: [70][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.3466e-01 (3.7632e-01)	Acc@1  89.06 ( 88.73)	Acc@5 100.00 ( 98.90)
Epoch: [70][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.2269e-01 (3.7996e-01)	Acc@1  89.84 ( 88.63)	Acc@5  99.22 ( 98.87)
Epoch: [70][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.9319e-01 (3.7438e-01)	Acc@1  92.19 ( 88.85)	Acc@5 100.00 ( 98.92)
Epoch: [70][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.9789e-01 (3.7776e-01)	Acc@1  82.81 ( 88.63)	Acc@5  96.09 ( 98.90)
Epoch: [70][ 90/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.9199e-01 (3.7693e-01)	Acc@1  86.72 ( 88.67)	Acc@5 100.00 ( 98.91)
Epoch: [70][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.6105e-01 (3.8152e-01)	Acc@1  87.50 ( 88.52)	Acc@5  97.66 ( 98.86)
Epoch: [70][110/391]	Time  0.053 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3259e-01 (3.8279e-01)	Acc@1  91.41 ( 88.53)	Acc@5 100.00 ( 98.89)
Epoch: [70][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7657e-01 (3.8431e-01)	Acc@1  81.25 ( 88.45)	Acc@5  99.22 ( 98.90)
Epoch: [70][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3084e-01 (3.8591e-01)	Acc@1  90.62 ( 88.42)	Acc@5  99.22 ( 98.90)
Epoch: [70][140/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5568e-01 (3.8409e-01)	Acc@1  86.72 ( 88.50)	Acc@5  96.88 ( 98.91)
Epoch: [70][150/391]	Time  0.049 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.8067e-01 (3.8369e-01)	Acc@1  88.28 ( 88.52)	Acc@5  98.44 ( 98.90)
Epoch: [70][160/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.2166e-01 (3.8190e-01)	Acc@1  85.94 ( 88.57)	Acc@5  99.22 ( 98.91)
Epoch: [70][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9490e-01 (3.8296e-01)	Acc@1  85.94 ( 88.50)	Acc@5  96.88 ( 98.88)
Epoch: [70][180/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 5.1933e-01 (3.8375e-01)	Acc@1  86.72 ( 88.44)	Acc@5  97.66 ( 98.89)
Epoch: [70][190/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8542e-01 (3.8105e-01)	Acc@1  89.84 ( 88.56)	Acc@5 100.00 ( 98.90)
Epoch: [70][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6621e-01 (3.7997e-01)	Acc@1  86.72 ( 88.63)	Acc@5  96.09 ( 98.90)
Epoch: [70][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5473e-01 (3.8161e-01)	Acc@1  85.16 ( 88.58)	Acc@5  98.44 ( 98.87)
Epoch: [70][220/391]	Time  0.038 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.6411e-01 (3.8056e-01)	Acc@1  91.41 ( 88.62)	Acc@5  98.44 ( 98.87)
Epoch: [70][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5954e-01 (3.7952e-01)	Acc@1  89.06 ( 88.65)	Acc@5 100.00 ( 98.89)
Epoch: [70][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4290e-01 (3.7808e-01)	Acc@1  89.06 ( 88.67)	Acc@5 100.00 ( 98.91)
Epoch: [70][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7342e-01 (3.7896e-01)	Acc@1  88.28 ( 88.67)	Acc@5  96.88 ( 98.90)
Epoch: [70][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1591e-01 (3.8008e-01)	Acc@1  90.62 ( 88.66)	Acc@5 100.00 ( 98.90)
Epoch: [70][270/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8791e-01 (3.8041e-01)	Acc@1  88.28 ( 88.65)	Acc@5  98.44 ( 98.90)
Epoch: [70][280/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.8228e-01 (3.7963e-01)	Acc@1  85.16 ( 88.64)	Acc@5 100.00 ( 98.91)
Epoch: [70][290/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.3143e-01 (3.8020e-01)	Acc@1  81.25 ( 88.62)	Acc@5  97.66 ( 98.91)
Epoch: [70][300/391]	Time  0.039 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.9563e-01 (3.8108e-01)	Acc@1  84.38 ( 88.60)	Acc@5  97.66 ( 98.89)
Epoch: [70][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8414e-01 (3.8068e-01)	Acc@1  88.28 ( 88.61)	Acc@5  99.22 ( 98.90)
Epoch: [70][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3837e-01 (3.7987e-01)	Acc@1  85.16 ( 88.65)	Acc@5  97.66 ( 98.91)
Epoch: [70][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0844e-01 (3.7982e-01)	Acc@1  85.16 ( 88.65)	Acc@5  99.22 ( 98.90)
Epoch: [70][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5984e-01 (3.7961e-01)	Acc@1  83.59 ( 88.67)	Acc@5  99.22 ( 98.91)
Epoch: [70][350/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1208e-01 (3.7903e-01)	Acc@1  93.75 ( 88.68)	Acc@5  97.66 ( 98.92)
Epoch: [70][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4959e-01 (3.7879e-01)	Acc@1  89.84 ( 88.70)	Acc@5  99.22 ( 98.92)
Epoch: [70][370/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.3222e-01 (3.7817e-01)	Acc@1  85.94 ( 88.71)	Acc@5  99.22 ( 98.93)
Epoch: [70][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8540e-01 (3.7882e-01)	Acc@1  85.94 ( 88.71)	Acc@5  98.44 ( 98.91)
Epoch: [70][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5230e-01 (3.7822e-01)	Acc@1  91.25 ( 88.74)	Acc@5  98.75 ( 98.93)
## e[70] optimizer.zero_grad (sum) time: 0.26667261123657227
## e[70]       loss.backward (sum) time: 3.952540636062622
## e[70]      optimizer.step (sum) time: 1.9077322483062744
## epoch[70] training(only) time: 16.03771734237671
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2531e+00 (1.2531e+00)	Acc@1  69.00 ( 69.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.020 ( 0.033)	Loss 1.2896e+00 (1.3488e+00)	Acc@1  59.00 ( 67.55)	Acc@5  94.00 ( 90.18)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 1.0757e+00 (1.2734e+00)	Acc@1  73.00 ( 68.48)	Acc@5  92.00 ( 91.05)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4446e+00 (1.2909e+00)	Acc@1  62.00 ( 67.87)	Acc@5  89.00 ( 90.90)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.2347e+00 (1.3020e+00)	Acc@1  73.00 ( 67.68)	Acc@5  92.00 ( 90.93)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3421e+00 (1.3061e+00)	Acc@1  66.00 ( 67.63)	Acc@5  90.00 ( 90.69)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.4558e+00 (1.2847e+00)	Acc@1  62.00 ( 67.89)	Acc@5  92.00 ( 90.98)
Test: [ 70/100]	Time  0.020 ( 0.024)	Loss 1.3993e+00 (1.2767e+00)	Acc@1  68.00 ( 68.32)	Acc@5  90.00 ( 91.03)
Test: [ 80/100]	Time  0.017 ( 0.023)	Loss 1.4103e+00 (1.2798e+00)	Acc@1  69.00 ( 68.20)	Acc@5  88.00 ( 90.86)
Test: [ 90/100]	Time  0.018 ( 0.023)	Loss 1.5070e+00 (1.2649e+00)	Acc@1  68.00 ( 68.63)	Acc@5  90.00 ( 91.08)
 * Acc@1 68.790 Acc@5 91.200
### epoch[70] execution time: 18.38175392150879
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.200 ( 0.200)	Data  0.147 ( 0.147)	Loss 4.8322e-01 (4.8322e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [71][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.014)	Loss 5.0947e-01 (4.0632e-01)	Acc@1  84.38 ( 87.29)	Acc@5  98.44 ( 98.65)
Epoch: [71][ 20/391]	Time  0.038 ( 0.048)	Data  0.001 ( 0.008)	Loss 3.3665e-01 (3.9787e-01)	Acc@1  89.06 ( 87.43)	Acc@5 100.00 ( 98.96)
Epoch: [71][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.3597e-01 (3.8027e-01)	Acc@1  89.06 ( 87.95)	Acc@5  99.22 ( 99.07)
Epoch: [71][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.6216e-01 (3.8055e-01)	Acc@1  91.41 ( 88.17)	Acc@5  99.22 ( 99.09)
Epoch: [71][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.2943e-01 (3.7700e-01)	Acc@1  89.06 ( 88.47)	Acc@5  97.66 ( 99.05)
Epoch: [71][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.0790e-01 (3.7185e-01)	Acc@1  91.41 ( 88.72)	Acc@5  99.22 ( 99.05)
Epoch: [71][ 70/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.9436e-01 (3.7138e-01)	Acc@1  90.62 ( 88.83)	Acc@5  97.66 ( 99.01)
Epoch: [71][ 80/391]	Time  0.049 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.7777e-01 (3.7324e-01)	Acc@1  82.81 ( 88.80)	Acc@5  99.22 ( 98.98)
Epoch: [71][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.7730e-01 (3.7287e-01)	Acc@1  93.75 ( 88.80)	Acc@5 100.00 ( 99.04)
Epoch: [71][100/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.5012e-01 (3.7033e-01)	Acc@1  89.84 ( 88.91)	Acc@5 100.00 ( 99.03)
Epoch: [71][110/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7960e-01 (3.6984e-01)	Acc@1  90.62 ( 89.00)	Acc@5  99.22 ( 99.01)
Epoch: [71][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3522e-01 (3.7261e-01)	Acc@1  86.72 ( 88.92)	Acc@5  99.22 ( 99.00)
Epoch: [71][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6946e-01 (3.7492e-01)	Acc@1  91.41 ( 88.84)	Acc@5  97.66 ( 98.96)
Epoch: [71][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7947e-01 (3.7487e-01)	Acc@1  91.41 ( 88.88)	Acc@5 100.00 ( 98.97)
Epoch: [71][150/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.4063e-01 (3.7468e-01)	Acc@1  91.41 ( 88.96)	Acc@5  96.88 ( 98.97)
Epoch: [71][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9013e-01 (3.7475e-01)	Acc@1  88.28 ( 89.00)	Acc@5  99.22 ( 98.96)
Epoch: [71][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6689e-01 (3.7589e-01)	Acc@1  91.41 ( 89.00)	Acc@5  96.88 ( 98.94)
Epoch: [71][180/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8462e-01 (3.7694e-01)	Acc@1  86.72 ( 88.95)	Acc@5  99.22 ( 98.92)
Epoch: [71][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2447e-01 (3.7988e-01)	Acc@1  85.94 ( 88.82)	Acc@5  99.22 ( 98.91)
Epoch: [71][200/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2032e-01 (3.7876e-01)	Acc@1  87.50 ( 88.84)	Acc@5  98.44 ( 98.92)
Epoch: [71][210/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4673e-01 (3.8037e-01)	Acc@1  91.41 ( 88.81)	Acc@5  99.22 ( 98.89)
Epoch: [71][220/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9422e-01 (3.7974e-01)	Acc@1  92.97 ( 88.80)	Acc@5  99.22 ( 98.89)
Epoch: [71][230/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7995e-01 (3.8075e-01)	Acc@1  85.16 ( 88.76)	Acc@5  97.66 ( 98.87)
Epoch: [71][240/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8576e-01 (3.8031e-01)	Acc@1  89.84 ( 88.80)	Acc@5  99.22 ( 98.89)
Epoch: [71][250/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5852e-01 (3.7882e-01)	Acc@1  89.06 ( 88.85)	Acc@5  99.22 ( 98.91)
Epoch: [71][260/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6756e-01 (3.7810e-01)	Acc@1  91.41 ( 88.87)	Acc@5 100.00 ( 98.92)
Epoch: [71][270/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0536e-01 (3.7880e-01)	Acc@1  92.19 ( 88.82)	Acc@5  99.22 ( 98.91)
Epoch: [71][280/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9316e-01 (3.7921e-01)	Acc@1  85.16 ( 88.80)	Acc@5  97.66 ( 98.91)
Epoch: [71][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9143e-01 (3.7960e-01)	Acc@1  87.50 ( 88.78)	Acc@5  99.22 ( 98.93)
Epoch: [71][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6393e-01 (3.7987e-01)	Acc@1  90.62 ( 88.78)	Acc@5  99.22 ( 98.94)
Epoch: [71][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4799e-01 (3.7984e-01)	Acc@1  86.72 ( 88.79)	Acc@5  97.66 ( 98.93)
Epoch: [71][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3903e-01 (3.7987e-01)	Acc@1  92.97 ( 88.75)	Acc@5  99.22 ( 98.95)
Epoch: [71][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5384e-01 (3.7955e-01)	Acc@1  86.72 ( 88.75)	Acc@5  99.22 ( 98.95)
Epoch: [71][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6451e-01 (3.8084e-01)	Acc@1  85.94 ( 88.71)	Acc@5  99.22 ( 98.94)
Epoch: [71][350/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2453e-01 (3.8218e-01)	Acc@1  89.06 ( 88.68)	Acc@5  99.22 ( 98.94)
Epoch: [71][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8165e-01 (3.8097e-01)	Acc@1  92.97 ( 88.69)	Acc@5 100.00 ( 98.95)
Epoch: [71][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2166e-01 (3.8096e-01)	Acc@1  90.62 ( 88.68)	Acc@5 100.00 ( 98.96)
Epoch: [71][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5495e-01 (3.8052e-01)	Acc@1  89.84 ( 88.72)	Acc@5  96.88 ( 98.96)
Epoch: [71][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.9082e-01 (3.8165e-01)	Acc@1  86.25 ( 88.67)	Acc@5  96.25 ( 98.94)
## e[71] optimizer.zero_grad (sum) time: 0.26363468170166016
## e[71]       loss.backward (sum) time: 4.040945768356323
## e[71]      optimizer.step (sum) time: 1.8219375610351562
## epoch[71] training(only) time: 16.20005965232849
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2760e+00 (1.2760e+00)	Acc@1  66.00 ( 66.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.025 ( 0.035)	Loss 1.2646e+00 (1.3422e+00)	Acc@1  60.00 ( 67.55)	Acc@5  93.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.0778e+00 (1.2678e+00)	Acc@1  73.00 ( 68.62)	Acc@5  92.00 ( 91.67)
Test: [ 30/100]	Time  0.020 ( 0.026)	Loss 1.4644e+00 (1.2901e+00)	Acc@1  60.00 ( 68.10)	Acc@5  89.00 ( 91.16)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.2520e+00 (1.3033e+00)	Acc@1  69.00 ( 67.59)	Acc@5  93.00 ( 91.02)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3130e+00 (1.3078e+00)	Acc@1  72.00 ( 67.55)	Acc@5  90.00 ( 90.71)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 1.4224e+00 (1.2853e+00)	Acc@1  64.00 ( 67.87)	Acc@5  93.00 ( 91.02)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.4382e+00 (1.2775e+00)	Acc@1  67.00 ( 68.17)	Acc@5  90.00 ( 91.04)
Test: [ 80/100]	Time  0.017 ( 0.023)	Loss 1.3917e+00 (1.2793e+00)	Acc@1  68.00 ( 68.12)	Acc@5  88.00 ( 90.95)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.4774e+00 (1.2649e+00)	Acc@1  69.00 ( 68.58)	Acc@5  90.00 ( 91.12)
 * Acc@1 68.600 Acc@5 91.240
### epoch[71] execution time: 18.542500019073486
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.199 ( 0.199)	Data  0.151 ( 0.151)	Loss 3.8313e-01 (3.8313e-01)	Acc@1  92.19 ( 92.19)	Acc@5  97.66 ( 97.66)
Epoch: [72][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.015)	Loss 3.3565e-01 (3.6174e-01)	Acc@1  89.06 ( 89.70)	Acc@5  99.22 ( 99.15)
Epoch: [72][ 20/391]	Time  0.044 ( 0.049)	Data  0.002 ( 0.008)	Loss 3.4360e-01 (3.4895e-01)	Acc@1  90.62 ( 89.92)	Acc@5  98.44 ( 99.29)
Epoch: [72][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.8484e-01 (3.5229e-01)	Acc@1  94.53 ( 89.69)	Acc@5 100.00 ( 99.24)
Epoch: [72][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.2028e-01 (3.5169e-01)	Acc@1  89.06 ( 89.79)	Acc@5  99.22 ( 99.26)
Epoch: [72][ 50/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 5.0042e-01 (3.5702e-01)	Acc@1  84.38 ( 89.55)	Acc@5  98.44 ( 99.20)
Epoch: [72][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.5246e-01 (3.5981e-01)	Acc@1  89.84 ( 89.51)	Acc@5  99.22 ( 99.14)
Epoch: [72][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.7189e-01 (3.6088e-01)	Acc@1  92.19 ( 89.47)	Acc@5  99.22 ( 99.12)
Epoch: [72][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.3574e-01 (3.6505e-01)	Acc@1  84.38 ( 89.30)	Acc@5  97.66 ( 99.05)
Epoch: [72][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.4855e-01 (3.7193e-01)	Acc@1  82.81 ( 89.09)	Acc@5  97.66 ( 98.98)
Epoch: [72][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3047e-01 (3.6984e-01)	Acc@1  91.41 ( 89.08)	Acc@5 100.00 ( 99.02)
Epoch: [72][110/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1770e-01 (3.7130e-01)	Acc@1  91.41 ( 89.07)	Acc@5 100.00 ( 99.00)
Epoch: [72][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0890e-01 (3.7053e-01)	Acc@1  91.41 ( 89.09)	Acc@5  99.22 ( 98.99)
Epoch: [72][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3133e-01 (3.6945e-01)	Acc@1  90.62 ( 89.16)	Acc@5 100.00 ( 98.99)
Epoch: [72][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7877e-01 (3.6911e-01)	Acc@1  87.50 ( 89.20)	Acc@5  96.88 ( 98.96)
Epoch: [72][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8514e-01 (3.7107e-01)	Acc@1  86.72 ( 89.13)	Acc@5  99.22 ( 98.96)
Epoch: [72][160/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9956e-01 (3.6881e-01)	Acc@1  89.06 ( 89.20)	Acc@5  98.44 ( 98.98)
Epoch: [72][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0446e-01 (3.6900e-01)	Acc@1  91.41 ( 89.19)	Acc@5 100.00 ( 98.98)
Epoch: [72][180/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5065e-01 (3.7054e-01)	Acc@1  89.84 ( 89.11)	Acc@5 100.00 ( 98.99)
Epoch: [72][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5875e-01 (3.7123e-01)	Acc@1  89.84 ( 89.05)	Acc@5  99.22 ( 98.97)
Epoch: [72][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5599e-01 (3.7313e-01)	Acc@1  85.16 ( 88.97)	Acc@5  99.22 ( 98.98)
Epoch: [72][210/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6708e-01 (3.7420e-01)	Acc@1  89.84 ( 88.98)	Acc@5 100.00 ( 98.98)
Epoch: [72][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4654e-01 (3.7532e-01)	Acc@1  87.50 ( 88.91)	Acc@5  98.44 ( 98.96)
Epoch: [72][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8725e-01 (3.7413e-01)	Acc@1  91.41 ( 88.95)	Acc@5 100.00 ( 98.96)
Epoch: [72][240/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4616e-01 (3.7422e-01)	Acc@1  89.06 ( 88.92)	Acc@5 100.00 ( 98.97)
Epoch: [72][250/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.7357e-01 (3.7560e-01)	Acc@1  85.16 ( 88.88)	Acc@5  98.44 ( 98.96)
Epoch: [72][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2179e-01 (3.7541e-01)	Acc@1  87.50 ( 88.90)	Acc@5  97.66 ( 98.97)
Epoch: [72][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1951e-01 (3.7492e-01)	Acc@1  85.16 ( 88.92)	Acc@5  99.22 ( 98.98)
Epoch: [72][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9290e-01 (3.7550e-01)	Acc@1  89.84 ( 88.91)	Acc@5  99.22 ( 98.96)
Epoch: [72][290/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8048e-01 (3.7551e-01)	Acc@1  92.19 ( 88.91)	Acc@5  99.22 ( 98.94)
Epoch: [72][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8995e-01 (3.7557e-01)	Acc@1  87.50 ( 88.89)	Acc@5  98.44 ( 98.94)
Epoch: [72][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3877e-01 (3.7457e-01)	Acc@1  90.62 ( 88.94)	Acc@5  98.44 ( 98.94)
Epoch: [72][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7719e-01 (3.7431e-01)	Acc@1  89.84 ( 88.94)	Acc@5 100.00 ( 98.94)
Epoch: [72][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8892e-01 (3.7471e-01)	Acc@1  93.75 ( 88.95)	Acc@5 100.00 ( 98.93)
Epoch: [72][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2017e-01 (3.7512e-01)	Acc@1  88.28 ( 88.94)	Acc@5  99.22 ( 98.93)
Epoch: [72][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5767e-01 (3.7585e-01)	Acc@1  85.94 ( 88.89)	Acc@5  98.44 ( 98.92)
Epoch: [72][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.0046e-01 (3.7661e-01)	Acc@1  86.72 ( 88.86)	Acc@5  98.44 ( 98.91)
Epoch: [72][370/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.001)	Loss 4.1989e-01 (3.7535e-01)	Acc@1  88.28 ( 88.91)	Acc@5  96.88 ( 98.90)
Epoch: [72][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8020e-01 (3.7514e-01)	Acc@1  89.06 ( 88.93)	Acc@5  97.66 ( 98.90)
Epoch: [72][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.7763e-01 (3.7518e-01)	Acc@1  91.25 ( 88.92)	Acc@5 100.00 ( 98.91)
## e[72] optimizer.zero_grad (sum) time: 0.26508402824401855
## e[72]       loss.backward (sum) time: 4.040342807769775
## e[72]      optimizer.step (sum) time: 1.8433117866516113
## epoch[72] training(only) time: 16.14848017692566
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 1.2660e+00 (1.2660e+00)	Acc@1  68.00 ( 68.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.024 ( 0.038)	Loss 1.3422e+00 (1.3623e+00)	Acc@1  61.00 ( 66.73)	Acc@5  93.00 ( 90.73)
Test: [ 20/100]	Time  0.024 ( 0.031)	Loss 1.0697e+00 (1.2841e+00)	Acc@1  72.00 ( 67.86)	Acc@5  94.00 ( 91.48)
Test: [ 30/100]	Time  0.021 ( 0.028)	Loss 1.4547e+00 (1.3039e+00)	Acc@1  59.00 ( 67.55)	Acc@5  88.00 ( 91.23)
Test: [ 40/100]	Time  0.024 ( 0.027)	Loss 1.2348e+00 (1.3177e+00)	Acc@1  70.00 ( 67.17)	Acc@5  92.00 ( 91.12)
Test: [ 50/100]	Time  0.021 ( 0.026)	Loss 1.3233e+00 (1.3205e+00)	Acc@1  68.00 ( 67.25)	Acc@5  91.00 ( 90.76)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4332e+00 (1.2971e+00)	Acc@1  63.00 ( 67.52)	Acc@5  92.00 ( 91.03)
Test: [ 70/100]	Time  0.018 ( 0.025)	Loss 1.4258e+00 (1.2913e+00)	Acc@1  67.00 ( 67.90)	Acc@5  90.00 ( 91.03)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3578e+00 (1.2920e+00)	Acc@1  70.00 ( 67.88)	Acc@5  89.00 ( 90.93)
Test: [ 90/100]	Time  0.021 ( 0.024)	Loss 1.4637e+00 (1.2756e+00)	Acc@1  69.00 ( 68.35)	Acc@5  90.00 ( 91.09)
 * Acc@1 68.420 Acc@5 91.230
### epoch[72] execution time: 18.53781819343567
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.199 ( 0.199)	Data  0.147 ( 0.147)	Loss 4.7100e-01 (4.7100e-01)	Acc@1  87.50 ( 87.50)	Acc@5  97.66 ( 97.66)
Epoch: [73][ 10/391]	Time  0.039 ( 0.057)	Data  0.001 ( 0.014)	Loss 4.8752e-01 (4.0561e-01)	Acc@1  85.16 ( 88.28)	Acc@5  97.66 ( 98.58)
Epoch: [73][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.008)	Loss 3.7786e-01 (3.7386e-01)	Acc@1  87.50 ( 89.25)	Acc@5  97.66 ( 98.74)
Epoch: [73][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.0993e-01 (3.8392e-01)	Acc@1  89.84 ( 88.94)	Acc@5 100.00 ( 98.71)
Epoch: [73][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.3726e-01 (3.8253e-01)	Acc@1  85.94 ( 89.02)	Acc@5  98.44 ( 98.76)
Epoch: [73][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.7842e-01 (3.9586e-01)	Acc@1  88.28 ( 88.59)	Acc@5  99.22 ( 98.68)
Epoch: [73][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.2513e-01 (3.8802e-01)	Acc@1  89.84 ( 88.70)	Acc@5 100.00 ( 98.78)
Epoch: [73][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.6066e-01 (3.8683e-01)	Acc@1  90.62 ( 88.75)	Acc@5  99.22 ( 98.79)
Epoch: [73][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.2954e-01 (3.8556e-01)	Acc@1  89.84 ( 88.77)	Acc@5  99.22 ( 98.79)
Epoch: [73][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3862e-01 (3.8270e-01)	Acc@1  89.84 ( 88.74)	Acc@5 100.00 ( 98.85)
Epoch: [73][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.6463e-01 (3.7975e-01)	Acc@1  89.84 ( 88.74)	Acc@5  99.22 ( 98.88)
Epoch: [73][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7672e-01 (3.7767e-01)	Acc@1  87.50 ( 88.82)	Acc@5  98.44 ( 98.87)
Epoch: [73][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4418e-01 (3.7875e-01)	Acc@1  84.38 ( 88.74)	Acc@5  98.44 ( 98.84)
Epoch: [73][130/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3755e-01 (3.7767e-01)	Acc@1  89.84 ( 88.81)	Acc@5 100.00 ( 98.87)
Epoch: [73][140/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1019e-01 (3.7802e-01)	Acc@1  92.19 ( 88.80)	Acc@5  98.44 ( 98.83)
Epoch: [73][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1670e-01 (3.7765e-01)	Acc@1  90.62 ( 88.75)	Acc@5  99.22 ( 98.84)
Epoch: [73][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8071e-01 (3.7947e-01)	Acc@1  88.28 ( 88.67)	Acc@5  98.44 ( 98.82)
Epoch: [73][170/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1582e-01 (3.8070e-01)	Acc@1  86.72 ( 88.61)	Acc@5  96.88 ( 98.80)
Epoch: [73][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0055e-01 (3.8062e-01)	Acc@1  89.06 ( 88.55)	Acc@5 100.00 ( 98.82)
Epoch: [73][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4794e-01 (3.7928e-01)	Acc@1  87.50 ( 88.62)	Acc@5 100.00 ( 98.82)
Epoch: [73][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3775e-01 (3.7744e-01)	Acc@1  88.28 ( 88.70)	Acc@5 100.00 ( 98.83)
Epoch: [73][210/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8347e-01 (3.7574e-01)	Acc@1  84.38 ( 88.72)	Acc@5 100.00 ( 98.85)
Epoch: [73][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6262e-01 (3.7636e-01)	Acc@1  85.94 ( 88.67)	Acc@5  97.66 ( 98.85)
Epoch: [73][230/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8368e-01 (3.7623e-01)	Acc@1  88.28 ( 88.64)	Acc@5 100.00 ( 98.86)
Epoch: [73][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0485e-01 (3.7693e-01)	Acc@1  85.94 ( 88.63)	Acc@5 100.00 ( 98.86)
Epoch: [73][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9951e-01 (3.7656e-01)	Acc@1  85.94 ( 88.66)	Acc@5 100.00 ( 98.87)
Epoch: [73][260/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2933e-01 (3.7705e-01)	Acc@1  92.97 ( 88.66)	Acc@5  99.22 ( 98.88)
Epoch: [73][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3349e-01 (3.7729e-01)	Acc@1  86.72 ( 88.64)	Acc@5 100.00 ( 98.90)
Epoch: [73][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0784e-01 (3.7749e-01)	Acc@1  83.59 ( 88.62)	Acc@5  99.22 ( 98.91)
Epoch: [73][290/391]	Time  0.051 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2423e-01 (3.7676e-01)	Acc@1  86.72 ( 88.64)	Acc@5  99.22 ( 98.92)
Epoch: [73][300/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.8245e-01 (3.7694e-01)	Acc@1  89.06 ( 88.63)	Acc@5  99.22 ( 98.92)
Epoch: [73][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3473e-01 (3.7712e-01)	Acc@1  91.41 ( 88.62)	Acc@5  99.22 ( 98.91)
Epoch: [73][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6304e-01 (3.7765e-01)	Acc@1  92.97 ( 88.60)	Acc@5  98.44 ( 98.91)
Epoch: [73][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4198e-01 (3.7726e-01)	Acc@1  89.84 ( 88.62)	Acc@5  97.66 ( 98.92)
Epoch: [73][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5710e-01 (3.7755e-01)	Acc@1  89.84 ( 88.61)	Acc@5  98.44 ( 98.92)
Epoch: [73][350/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7226e-01 (3.7705e-01)	Acc@1  89.06 ( 88.65)	Acc@5  97.66 ( 98.92)
Epoch: [73][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4485e-01 (3.7713e-01)	Acc@1  88.28 ( 88.67)	Acc@5 100.00 ( 98.93)
Epoch: [73][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0785e-01 (3.7729e-01)	Acc@1  90.62 ( 88.65)	Acc@5 100.00 ( 98.93)
Epoch: [73][380/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.001)	Loss 3.0470e-01 (3.7683e-01)	Acc@1  90.62 ( 88.69)	Acc@5 100.00 ( 98.93)
Epoch: [73][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8317e-01 (3.7633e-01)	Acc@1  86.25 ( 88.70)	Acc@5  98.75 ( 98.93)
## e[73] optimizer.zero_grad (sum) time: 0.26416540145874023
## e[73]       loss.backward (sum) time: 4.047213315963745
## e[73]      optimizer.step (sum) time: 1.8391962051391602
## epoch[73] training(only) time: 16.21492600440979
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2686e+00 (1.2686e+00)	Acc@1  66.00 ( 66.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 1.2903e+00 (1.3567e+00)	Acc@1  61.00 ( 67.09)	Acc@5  94.00 ( 90.45)
Test: [ 20/100]	Time  0.026 ( 0.029)	Loss 1.0863e+00 (1.2809e+00)	Acc@1  72.00 ( 68.05)	Acc@5  92.00 ( 91.19)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4194e+00 (1.2983e+00)	Acc@1  61.00 ( 67.68)	Acc@5  89.00 ( 90.77)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.2306e+00 (1.3097e+00)	Acc@1  71.00 ( 67.49)	Acc@5  91.00 ( 90.80)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.3330e+00 (1.3127e+00)	Acc@1  65.00 ( 67.47)	Acc@5  90.00 ( 90.47)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 1.4385e+00 (1.2909e+00)	Acc@1  63.00 ( 67.69)	Acc@5  94.00 ( 90.75)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 1.4139e+00 (1.2844e+00)	Acc@1  67.00 ( 68.03)	Acc@5  90.00 ( 90.82)
Test: [ 80/100]	Time  0.021 ( 0.024)	Loss 1.3597e+00 (1.2861e+00)	Acc@1  70.00 ( 68.01)	Acc@5  90.00 ( 90.74)
Test: [ 90/100]	Time  0.018 ( 0.024)	Loss 1.4872e+00 (1.2698e+00)	Acc@1  70.00 ( 68.47)	Acc@5  90.00 ( 90.95)
 * Acc@1 68.530 Acc@5 91.060
### epoch[73] execution time: 18.631107330322266
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.197 ( 0.197)	Data  0.146 ( 0.146)	Loss 3.5041e-01 (3.5041e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [74][ 10/391]	Time  0.042 ( 0.058)	Data  0.001 ( 0.014)	Loss 5.1968e-01 (3.9365e-01)	Acc@1  82.03 ( 87.57)	Acc@5  96.88 ( 98.58)
Epoch: [74][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.008)	Loss 3.3956e-01 (3.9603e-01)	Acc@1  87.50 ( 87.80)	Acc@5  99.22 ( 98.44)
Epoch: [74][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.006)	Loss 4.3982e-01 (3.9753e-01)	Acc@1  87.50 ( 88.03)	Acc@5  99.22 ( 98.51)
Epoch: [74][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.0368e-01 (3.8627e-01)	Acc@1  86.72 ( 88.55)	Acc@5 100.00 ( 98.70)
Epoch: [74][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.8101e-01 (3.8988e-01)	Acc@1  90.62 ( 88.39)	Acc@5  97.66 ( 98.71)
Epoch: [74][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.1722e-01 (3.8628e-01)	Acc@1  85.16 ( 88.50)	Acc@5 100.00 ( 98.74)
Epoch: [74][ 70/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.1054e-01 (3.8417e-01)	Acc@1  87.50 ( 88.59)	Acc@5  98.44 ( 98.79)
Epoch: [74][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1955e-01 (3.8127e-01)	Acc@1  89.84 ( 88.63)	Acc@5  99.22 ( 98.83)
Epoch: [74][ 90/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.0090e-01 (3.7668e-01)	Acc@1  88.28 ( 88.73)	Acc@5 100.00 ( 98.84)
Epoch: [74][100/391]	Time  0.042 ( 0.043)	Data  0.002 ( 0.003)	Loss 3.5507e-01 (3.7814e-01)	Acc@1  90.62 ( 88.63)	Acc@5  99.22 ( 98.86)
Epoch: [74][110/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.6967e-01 (3.7755e-01)	Acc@1  89.06 ( 88.64)	Acc@5  99.22 ( 98.89)
Epoch: [74][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6806e-01 (3.7834e-01)	Acc@1  88.28 ( 88.56)	Acc@5  99.22 ( 98.92)
Epoch: [74][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1364e-01 (3.7768e-01)	Acc@1  83.59 ( 88.57)	Acc@5  98.44 ( 98.93)
Epoch: [74][140/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.7430e-01 (3.7848e-01)	Acc@1  89.84 ( 88.58)	Acc@5  99.22 ( 98.92)
Epoch: [74][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7815e-01 (3.7655e-01)	Acc@1  92.19 ( 88.60)	Acc@5  99.22 ( 98.95)
Epoch: [74][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9067e-01 (3.7670e-01)	Acc@1  86.72 ( 88.55)	Acc@5  97.66 ( 98.96)
Epoch: [74][170/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5869e-01 (3.7796e-01)	Acc@1  86.72 ( 88.49)	Acc@5  98.44 ( 98.95)
Epoch: [74][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5925e-01 (3.7930e-01)	Acc@1  89.06 ( 88.36)	Acc@5  98.44 ( 98.93)
Epoch: [74][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3946e-01 (3.8001e-01)	Acc@1  89.84 ( 88.37)	Acc@5 100.00 ( 98.92)
Epoch: [74][200/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.5057e-01 (3.7875e-01)	Acc@1  86.72 ( 88.39)	Acc@5  99.22 ( 98.92)
Epoch: [74][210/391]	Time  0.046 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.3782e-01 (3.7736e-01)	Acc@1  89.84 ( 88.46)	Acc@5 100.00 ( 98.93)
Epoch: [74][220/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1825e-01 (3.7732e-01)	Acc@1  87.50 ( 88.50)	Acc@5  98.44 ( 98.92)
Epoch: [74][230/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9832e-01 (3.7689e-01)	Acc@1  87.50 ( 88.51)	Acc@5 100.00 ( 98.93)
Epoch: [74][240/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6856e-01 (3.7659e-01)	Acc@1  86.72 ( 88.52)	Acc@5 100.00 ( 98.95)
Epoch: [74][250/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8305e-01 (3.7496e-01)	Acc@1  92.19 ( 88.65)	Acc@5  96.88 ( 98.94)
Epoch: [74][260/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8233e-01 (3.7587e-01)	Acc@1  85.16 ( 88.63)	Acc@5  96.09 ( 98.92)
Epoch: [74][270/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 2.2521e-01 (3.7510e-01)	Acc@1  94.53 ( 88.65)	Acc@5 100.00 ( 98.92)
Epoch: [74][280/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3040e-01 (3.7508e-01)	Acc@1  89.06 ( 88.69)	Acc@5  97.66 ( 98.92)
Epoch: [74][290/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3412e-01 (3.7452e-01)	Acc@1  88.28 ( 88.73)	Acc@5  98.44 ( 98.92)
Epoch: [74][300/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2488e-01 (3.7564e-01)	Acc@1  83.59 ( 88.69)	Acc@5  97.66 ( 98.91)
Epoch: [74][310/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4080e-01 (3.7482e-01)	Acc@1  92.97 ( 88.69)	Acc@5 100.00 ( 98.94)
Epoch: [74][320/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1519e-01 (3.7539e-01)	Acc@1  87.50 ( 88.67)	Acc@5  99.22 ( 98.94)
Epoch: [74][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8959e-01 (3.7520e-01)	Acc@1  91.41 ( 88.72)	Acc@5  99.22 ( 98.94)
Epoch: [74][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6376e-01 (3.7495e-01)	Acc@1  88.28 ( 88.70)	Acc@5 100.00 ( 98.95)
Epoch: [74][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0360e-01 (3.7539e-01)	Acc@1  83.59 ( 88.67)	Acc@5  97.66 ( 98.95)
Epoch: [74][360/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9580e-01 (3.7472e-01)	Acc@1  84.38 ( 88.68)	Acc@5  98.44 ( 98.96)
Epoch: [74][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3291e-01 (3.7501e-01)	Acc@1  89.84 ( 88.66)	Acc@5 100.00 ( 98.97)
Epoch: [74][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7369e-01 (3.7532e-01)	Acc@1  89.84 ( 88.64)	Acc@5 100.00 ( 98.96)
Epoch: [74][390/391]	Time  0.027 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5900e-01 (3.7513e-01)	Acc@1  90.00 ( 88.67)	Acc@5 100.00 ( 98.97)
## e[74] optimizer.zero_grad (sum) time: 0.26686811447143555
## e[74]       loss.backward (sum) time: 4.078245401382446
## e[74]      optimizer.step (sum) time: 1.8236234188079834
## epoch[74] training(only) time: 16.240851163864136
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.2926e+00 (1.2926e+00)	Acc@1  67.00 ( 67.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.020 ( 0.032)	Loss 1.2688e+00 (1.3637e+00)	Acc@1  62.00 ( 67.18)	Acc@5  93.00 ( 90.73)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 1.0889e+00 (1.2891e+00)	Acc@1  72.00 ( 67.95)	Acc@5  93.00 ( 91.38)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 1.4587e+00 (1.3065e+00)	Acc@1  61.00 ( 67.55)	Acc@5  88.00 ( 90.97)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.2645e+00 (1.3194e+00)	Acc@1  72.00 ( 67.39)	Acc@5  93.00 ( 90.98)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 1.3172e+00 (1.3234e+00)	Acc@1  67.00 ( 67.24)	Acc@5  90.00 ( 90.65)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 1.4654e+00 (1.3002e+00)	Acc@1  63.00 ( 67.57)	Acc@5  90.00 ( 90.92)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 1.4133e+00 (1.2925e+00)	Acc@1  65.00 ( 67.99)	Acc@5  90.00 ( 90.96)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.3449e+00 (1.2929e+00)	Acc@1  71.00 ( 67.96)	Acc@5  91.00 ( 90.86)
Test: [ 90/100]	Time  0.019 ( 0.023)	Loss 1.5054e+00 (1.2774e+00)	Acc@1  67.00 ( 68.42)	Acc@5  90.00 ( 91.03)
 * Acc@1 68.550 Acc@5 91.120
### epoch[74] execution time: 18.586549282073975
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.206 ( 0.206)	Data  0.157 ( 0.157)	Loss 2.7624e-01 (2.7624e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.040 ( 0.057)	Data  0.001 ( 0.015)	Loss 3.9660e-01 (3.6843e-01)	Acc@1  86.72 ( 88.42)	Acc@5  98.44 ( 99.43)
Epoch: [75][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 4.5449e-01 (3.7400e-01)	Acc@1  88.28 ( 88.73)	Acc@5  98.44 ( 99.14)
Epoch: [75][ 30/391]	Time  0.040 ( 0.047)	Data  0.001 ( 0.006)	Loss 3.4353e-01 (3.7439e-01)	Acc@1  89.06 ( 88.89)	Acc@5  99.22 ( 99.09)
Epoch: [75][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.5508e-01 (3.7137e-01)	Acc@1  88.28 ( 88.97)	Acc@5 100.00 ( 99.16)
Epoch: [75][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.2069e-01 (3.7749e-01)	Acc@1  83.59 ( 88.65)	Acc@5  99.22 ( 99.14)
Epoch: [75][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.1070e-01 (3.7581e-01)	Acc@1  89.84 ( 88.72)	Acc@5  99.22 ( 99.09)
Epoch: [75][ 70/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.1057e-01 (3.6821e-01)	Acc@1  91.41 ( 89.06)	Acc@5  98.44 ( 99.10)
Epoch: [75][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1808e-01 (3.6705e-01)	Acc@1  93.75 ( 89.15)	Acc@5  98.44 ( 99.04)
Epoch: [75][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.9719e-01 (3.6487e-01)	Acc@1  91.41 ( 89.23)	Acc@5  99.22 ( 99.05)
Epoch: [75][100/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.7844e-01 (3.6520e-01)	Acc@1  92.97 ( 89.32)	Acc@5  98.44 ( 99.03)
Epoch: [75][110/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.002)	Loss 4.1030e-01 (3.6949e-01)	Acc@1  85.94 ( 89.20)	Acc@5  98.44 ( 99.00)
Epoch: [75][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8249e-01 (3.6719e-01)	Acc@1  87.50 ( 89.26)	Acc@5 100.00 ( 99.03)
Epoch: [75][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4632e-01 (3.6804e-01)	Acc@1  92.97 ( 89.24)	Acc@5  99.22 ( 99.00)
Epoch: [75][140/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6844e-01 (3.6825e-01)	Acc@1  85.94 ( 89.24)	Acc@5  98.44 ( 99.02)
Epoch: [75][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2606e-01 (3.7002e-01)	Acc@1  92.19 ( 89.10)	Acc@5  98.44 ( 98.98)
Epoch: [75][160/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2099e-01 (3.6999e-01)	Acc@1  92.97 ( 89.11)	Acc@5  99.22 ( 98.99)
Epoch: [75][170/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3860e-01 (3.6995e-01)	Acc@1  89.06 ( 89.11)	Acc@5  99.22 ( 99.00)
Epoch: [75][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0236e-01 (3.6815e-01)	Acc@1  84.38 ( 89.17)	Acc@5  98.44 ( 98.99)
Epoch: [75][190/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9372e-01 (3.7113e-01)	Acc@1  87.50 ( 89.10)	Acc@5 100.00 ( 98.94)
Epoch: [75][200/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8263e-01 (3.7168e-01)	Acc@1  89.84 ( 89.06)	Acc@5 100.00 ( 98.94)
Epoch: [75][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5291e-01 (3.7159e-01)	Acc@1  88.28 ( 89.05)	Acc@5 100.00 ( 98.94)
Epoch: [75][220/391]	Time  0.049 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0676e-01 (3.7042e-01)	Acc@1  89.84 ( 89.06)	Acc@5 100.00 ( 98.96)
Epoch: [75][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9829e-01 (3.7023e-01)	Acc@1  89.84 ( 89.05)	Acc@5  99.22 ( 98.97)
Epoch: [75][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1895e-01 (3.7047e-01)	Acc@1  88.28 ( 89.05)	Acc@5  97.66 ( 98.96)
Epoch: [75][250/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7510e-01 (3.7172e-01)	Acc@1  89.84 ( 88.99)	Acc@5 100.00 ( 98.94)
Epoch: [75][260/391]	Time  0.052 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0093e-01 (3.7149e-01)	Acc@1  89.06 ( 88.95)	Acc@5  98.44 ( 98.96)
Epoch: [75][270/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1150e-01 (3.7268e-01)	Acc@1  89.06 ( 88.89)	Acc@5  97.66 ( 98.94)
Epoch: [75][280/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5068e-01 (3.7294e-01)	Acc@1  89.06 ( 88.89)	Acc@5  98.44 ( 98.94)
Epoch: [75][290/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2208e-01 (3.7266e-01)	Acc@1  87.50 ( 88.85)	Acc@5  99.22 ( 98.94)
Epoch: [75][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.5044e-01 (3.7302e-01)	Acc@1  87.50 ( 88.87)	Acc@5 100.00 ( 98.95)
Epoch: [75][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8826e-01 (3.7254e-01)	Acc@1  91.41 ( 88.89)	Acc@5 100.00 ( 98.94)
Epoch: [75][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6968e-01 (3.7170e-01)	Acc@1  89.06 ( 88.93)	Acc@5  97.66 ( 98.94)
Epoch: [75][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1952e-01 (3.7189e-01)	Acc@1  82.03 ( 88.88)	Acc@5  99.22 ( 98.94)
Epoch: [75][340/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3127e-01 (3.7175e-01)	Acc@1  89.06 ( 88.90)	Acc@5  97.66 ( 98.94)
Epoch: [75][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5831e-01 (3.7221e-01)	Acc@1  89.84 ( 88.88)	Acc@5  98.44 ( 98.93)
Epoch: [75][360/391]	Time  0.052 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8724e-01 (3.7160e-01)	Acc@1  89.06 ( 88.91)	Acc@5 100.00 ( 98.94)
Epoch: [75][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4291e-01 (3.7183e-01)	Acc@1  83.59 ( 88.89)	Acc@5  98.44 ( 98.94)
Epoch: [75][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4180e-01 (3.7243e-01)	Acc@1  85.16 ( 88.85)	Acc@5  99.22 ( 98.95)
Epoch: [75][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0628e-01 (3.7283e-01)	Acc@1  83.75 ( 88.86)	Acc@5 100.00 ( 98.96)
## e[75] optimizer.zero_grad (sum) time: 0.26536083221435547
## e[75]       loss.backward (sum) time: 4.057885646820068
## e[75]      optimizer.step (sum) time: 1.8167884349822998
## epoch[75] training(only) time: 16.23163104057312
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.2506e+00 (1.2506e+00)	Acc@1  68.00 ( 68.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.019 ( 0.035)	Loss 1.2638e+00 (1.3485e+00)	Acc@1  59.00 ( 67.27)	Acc@5  93.00 ( 90.55)
Test: [ 20/100]	Time  0.025 ( 0.029)	Loss 1.0547e+00 (1.2760e+00)	Acc@1  75.00 ( 68.10)	Acc@5  92.00 ( 91.29)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4397e+00 (1.2945e+00)	Acc@1  62.00 ( 67.84)	Acc@5  88.00 ( 90.90)
Test: [ 40/100]	Time  0.025 ( 0.025)	Loss 1.2452e+00 (1.3072e+00)	Acc@1  73.00 ( 67.51)	Acc@5  93.00 ( 90.88)
Test: [ 50/100]	Time  0.023 ( 0.024)	Loss 1.3114e+00 (1.3116e+00)	Acc@1  65.00 ( 67.31)	Acc@5  91.00 ( 90.61)
Test: [ 60/100]	Time  0.021 ( 0.024)	Loss 1.4512e+00 (1.2908e+00)	Acc@1  63.00 ( 67.61)	Acc@5  91.00 ( 90.84)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 1.4050e+00 (1.2844e+00)	Acc@1  68.00 ( 67.99)	Acc@5  89.00 ( 90.85)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.3510e+00 (1.2859e+00)	Acc@1  71.00 ( 67.93)	Acc@5  90.00 ( 90.75)
Test: [ 90/100]	Time  0.026 ( 0.023)	Loss 1.5167e+00 (1.2709e+00)	Acc@1  69.00 ( 68.36)	Acc@5  90.00 ( 90.97)
 * Acc@1 68.370 Acc@5 91.150
### epoch[75] execution time: 18.58430814743042
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.203 ( 0.203)	Data  0.148 ( 0.148)	Loss 1.9843e-01 (1.9843e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.014)	Loss 2.5851e-01 (3.5106e-01)	Acc@1  96.09 ( 89.84)	Acc@5  99.22 ( 98.86)
Epoch: [76][ 20/391]	Time  0.040 ( 0.050)	Data  0.001 ( 0.008)	Loss 5.0509e-01 (3.6200e-01)	Acc@1  85.94 ( 89.55)	Acc@5  98.44 ( 98.74)
Epoch: [76][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.5477e-01 (3.6611e-01)	Acc@1  92.19 ( 89.52)	Acc@5  99.22 ( 98.87)
Epoch: [76][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.0135e-01 (3.6567e-01)	Acc@1  92.19 ( 89.60)	Acc@5 100.00 ( 98.91)
Epoch: [76][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.2877e-01 (3.6721e-01)	Acc@1  89.06 ( 89.32)	Acc@5 100.00 ( 98.94)
Epoch: [76][ 60/391]	Time  0.047 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.4839e-01 (3.6970e-01)	Acc@1  86.72 ( 89.20)	Acc@5  98.44 ( 98.86)
Epoch: [76][ 70/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.2295e-01 (3.6917e-01)	Acc@1  88.28 ( 89.17)	Acc@5  98.44 ( 98.91)
Epoch: [76][ 80/391]	Time  0.043 ( 0.043)	Data  0.002 ( 0.003)	Loss 2.9157e-01 (3.7078e-01)	Acc@1  92.97 ( 89.06)	Acc@5 100.00 ( 98.93)
Epoch: [76][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.8847e-01 (3.6858e-01)	Acc@1  86.72 ( 89.06)	Acc@5  98.44 ( 98.95)
Epoch: [76][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.0032e-01 (3.6902e-01)	Acc@1  87.50 ( 89.01)	Acc@5  99.22 ( 99.02)
Epoch: [76][110/391]	Time  0.052 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3528e-01 (3.7196e-01)	Acc@1  86.72 ( 88.91)	Acc@5  98.44 ( 99.03)
Epoch: [76][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0722e-01 (3.6994e-01)	Acc@1  89.06 ( 88.97)	Acc@5  98.44 ( 99.06)
Epoch: [76][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5326e-01 (3.7207e-01)	Acc@1  88.28 ( 88.89)	Acc@5  99.22 ( 99.06)
Epoch: [76][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3374e-01 (3.7109e-01)	Acc@1  87.50 ( 88.93)	Acc@5 100.00 ( 99.05)
Epoch: [76][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7548e-01 (3.7178e-01)	Acc@1  86.72 ( 88.91)	Acc@5  98.44 ( 99.03)
Epoch: [76][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1500e-01 (3.7392e-01)	Acc@1  85.94 ( 88.84)	Acc@5  97.66 ( 98.98)
Epoch: [76][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3541e-01 (3.7393e-01)	Acc@1  89.06 ( 88.82)	Acc@5  97.66 ( 98.96)
Epoch: [76][180/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.1154e-01 (3.7405e-01)	Acc@1  89.06 ( 88.86)	Acc@5  99.22 ( 98.96)
Epoch: [76][190/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0969e-01 (3.7502e-01)	Acc@1  88.28 ( 88.83)	Acc@5  99.22 ( 98.95)
Epoch: [76][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6329e-01 (3.7300e-01)	Acc@1  89.84 ( 88.86)	Acc@5 100.00 ( 98.96)
Epoch: [76][210/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4211e-01 (3.7333e-01)	Acc@1  90.62 ( 88.87)	Acc@5  98.44 ( 98.94)
Epoch: [76][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7942e-01 (3.7294e-01)	Acc@1  90.62 ( 88.86)	Acc@5 100.00 ( 98.94)
Epoch: [76][230/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4516e-01 (3.7451e-01)	Acc@1  85.16 ( 88.78)	Acc@5  99.22 ( 98.92)
Epoch: [76][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9333e-01 (3.7456e-01)	Acc@1  85.94 ( 88.78)	Acc@5  96.09 ( 98.92)
Epoch: [76][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8832e-01 (3.7302e-01)	Acc@1  85.94 ( 88.84)	Acc@5  96.09 ( 98.93)
Epoch: [76][260/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.7200e-01 (3.7403e-01)	Acc@1  91.41 ( 88.82)	Acc@5  99.22 ( 98.94)
Epoch: [76][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7354e-01 (3.7361e-01)	Acc@1  92.97 ( 88.85)	Acc@5  99.22 ( 98.95)
Epoch: [76][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8587e-01 (3.7370e-01)	Acc@1  87.50 ( 88.82)	Acc@5  99.22 ( 98.96)
Epoch: [76][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3471e-01 (3.7236e-01)	Acc@1  93.75 ( 88.83)	Acc@5 100.00 ( 98.97)
Epoch: [76][300/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8561e-01 (3.7343e-01)	Acc@1  87.50 ( 88.77)	Acc@5  99.22 ( 98.96)
Epoch: [76][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.0992e-01 (3.7349e-01)	Acc@1  84.38 ( 88.80)	Acc@5  98.44 ( 98.94)
Epoch: [76][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.3417e-01 (3.7345e-01)	Acc@1  93.75 ( 88.81)	Acc@5 100.00 ( 98.94)
Epoch: [76][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9107e-01 (3.7303e-01)	Acc@1  85.16 ( 88.81)	Acc@5  97.66 ( 98.94)
Epoch: [76][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3232e-01 (3.7356e-01)	Acc@1  86.72 ( 88.79)	Acc@5  97.66 ( 98.94)
Epoch: [76][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1743e-01 (3.7427e-01)	Acc@1  85.94 ( 88.78)	Acc@5  98.44 ( 98.94)
Epoch: [76][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8488e-01 (3.7485e-01)	Acc@1  84.38 ( 88.74)	Acc@5  98.44 ( 98.94)
Epoch: [76][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1226e-01 (3.7519e-01)	Acc@1  91.41 ( 88.75)	Acc@5  98.44 ( 98.93)
Epoch: [76][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0175e-01 (3.7525e-01)	Acc@1  91.41 ( 88.75)	Acc@5  99.22 ( 98.93)
Epoch: [76][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2212e-01 (3.7556e-01)	Acc@1  92.50 ( 88.75)	Acc@5  98.75 ( 98.93)
## e[76] optimizer.zero_grad (sum) time: 0.2644937038421631
## e[76]       loss.backward (sum) time: 3.985959053039551
## e[76]      optimizer.step (sum) time: 1.9339168071746826
## epoch[76] training(only) time: 16.073046922683716
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.2769e+00 (1.2769e+00)	Acc@1  68.00 ( 68.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 1.2808e+00 (1.3559e+00)	Acc@1  60.00 ( 67.55)	Acc@5  93.00 ( 90.73)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.0827e+00 (1.2798e+00)	Acc@1  75.00 ( 68.43)	Acc@5  92.00 ( 91.24)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4967e+00 (1.3020e+00)	Acc@1  61.00 ( 67.87)	Acc@5  89.00 ( 90.90)
Test: [ 40/100]	Time  0.023 ( 0.025)	Loss 1.2527e+00 (1.3131e+00)	Acc@1  71.00 ( 67.63)	Acc@5  91.00 ( 90.78)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 1.2985e+00 (1.3143e+00)	Acc@1  70.00 ( 67.67)	Acc@5  89.00 ( 90.51)
Test: [ 60/100]	Time  0.019 ( 0.024)	Loss 1.4585e+00 (1.2925e+00)	Acc@1  63.00 ( 67.93)	Acc@5  92.00 ( 90.79)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 1.4355e+00 (1.2859e+00)	Acc@1  68.00 ( 68.17)	Acc@5  90.00 ( 90.83)
Test: [ 80/100]	Time  0.024 ( 0.023)	Loss 1.3525e+00 (1.2877e+00)	Acc@1  70.00 ( 68.17)	Acc@5  89.00 ( 90.75)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 1.4863e+00 (1.2714e+00)	Acc@1  67.00 ( 68.58)	Acc@5  90.00 ( 90.98)
 * Acc@1 68.670 Acc@5 91.100
### epoch[76] execution time: 18.408599376678467
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.198 ( 0.198)	Data  0.143 ( 0.143)	Loss 3.5842e-01 (3.5842e-01)	Acc@1  88.28 ( 88.28)	Acc@5  98.44 ( 98.44)
Epoch: [77][ 10/391]	Time  0.041 ( 0.055)	Data  0.001 ( 0.014)	Loss 3.8131e-01 (3.7723e-01)	Acc@1  89.06 ( 89.20)	Acc@5 100.00 ( 98.86)
Epoch: [77][ 20/391]	Time  0.041 ( 0.049)	Data  0.001 ( 0.008)	Loss 3.2850e-01 (3.5907e-01)	Acc@1  90.62 ( 89.66)	Acc@5 100.00 ( 99.00)
Epoch: [77][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.006)	Loss 4.0601e-01 (3.6647e-01)	Acc@1  86.72 ( 89.34)	Acc@5  97.66 ( 98.87)
Epoch: [77][ 40/391]	Time  0.042 ( 0.045)	Data  0.002 ( 0.005)	Loss 4.0895e-01 (3.6808e-01)	Acc@1  88.28 ( 89.10)	Acc@5  98.44 ( 98.86)
Epoch: [77][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.6360e-01 (3.7031e-01)	Acc@1  89.06 ( 89.03)	Acc@5 100.00 ( 98.94)
Epoch: [77][ 60/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.6442e-01 (3.6878e-01)	Acc@1  82.81 ( 89.00)	Acc@5  99.22 ( 99.03)
Epoch: [77][ 70/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.5993e-01 (3.6579e-01)	Acc@1  89.06 ( 89.19)	Acc@5 100.00 ( 99.00)
Epoch: [77][ 80/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1469e-01 (3.6645e-01)	Acc@1  93.75 ( 89.16)	Acc@5  99.22 ( 98.98)
Epoch: [77][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.1896e-01 (3.6566e-01)	Acc@1  86.72 ( 89.17)	Acc@5  99.22 ( 98.99)
Epoch: [77][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3677e-01 (3.6786e-01)	Acc@1  90.62 ( 89.05)	Acc@5  99.22 ( 99.01)
Epoch: [77][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4947e-01 (3.6748e-01)	Acc@1  89.06 ( 89.07)	Acc@5 100.00 ( 99.01)
Epoch: [77][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7755e-01 (3.6784e-01)	Acc@1  90.62 ( 89.09)	Acc@5  98.44 ( 99.03)
Epoch: [77][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0317e-01 (3.6918e-01)	Acc@1  90.62 ( 89.01)	Acc@5  99.22 ( 99.03)
Epoch: [77][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1861e-01 (3.6620e-01)	Acc@1  90.62 ( 89.09)	Acc@5 100.00 ( 99.04)
Epoch: [77][150/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9637e-01 (3.6809e-01)	Acc@1  86.72 ( 88.96)	Acc@5 100.00 ( 99.03)
Epoch: [77][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0249e-01 (3.6782e-01)	Acc@1  85.94 ( 88.97)	Acc@5 100.00 ( 99.04)
Epoch: [77][170/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0487e-01 (3.6654e-01)	Acc@1  93.75 ( 89.06)	Acc@5  99.22 ( 99.06)
Epoch: [77][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1617e-01 (3.6696e-01)	Acc@1  92.19 ( 89.06)	Acc@5 100.00 ( 99.06)
Epoch: [77][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4671e-01 (3.6694e-01)	Acc@1  89.06 ( 89.12)	Acc@5  97.66 ( 99.04)
Epoch: [77][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7704e-01 (3.6708e-01)	Acc@1  88.28 ( 89.15)	Acc@5  99.22 ( 99.05)
Epoch: [77][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6757e-01 (3.6661e-01)	Acc@1  85.16 ( 89.13)	Acc@5  97.66 ( 99.05)
Epoch: [77][220/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2640e-01 (3.6722e-01)	Acc@1  84.38 ( 89.06)	Acc@5  99.22 ( 99.05)
Epoch: [77][230/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1599e-01 (3.6862e-01)	Acc@1  88.28 ( 89.02)	Acc@5 100.00 ( 99.04)
Epoch: [77][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1985e-01 (3.6961e-01)	Acc@1  88.28 ( 88.97)	Acc@5  97.66 ( 99.01)
Epoch: [77][250/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7616e-01 (3.7110e-01)	Acc@1  83.59 ( 88.91)	Acc@5  96.88 ( 98.99)
Epoch: [77][260/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0637e-01 (3.7060e-01)	Acc@1  90.62 ( 88.92)	Acc@5  99.22 ( 98.98)
Epoch: [77][270/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6641e-01 (3.6958e-01)	Acc@1  92.19 ( 88.93)	Acc@5 100.00 ( 99.01)
Epoch: [77][280/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 2.4887e-01 (3.6837e-01)	Acc@1  92.97 ( 88.98)	Acc@5 100.00 ( 99.02)
Epoch: [77][290/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.001)	Loss 2.9769e-01 (3.6788e-01)	Acc@1  92.19 ( 89.03)	Acc@5  99.22 ( 99.01)
Epoch: [77][300/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.3759e-01 (3.6726e-01)	Acc@1  92.19 ( 89.04)	Acc@5  98.44 ( 99.01)
Epoch: [77][310/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.001)	Loss 2.9654e-01 (3.6770e-01)	Acc@1  89.84 ( 88.99)	Acc@5 100.00 ( 99.00)
Epoch: [77][320/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7896e-01 (3.6751e-01)	Acc@1  87.50 ( 89.01)	Acc@5 100.00 ( 99.01)
Epoch: [77][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2907e-01 (3.6750e-01)	Acc@1  89.06 ( 89.01)	Acc@5  99.22 ( 99.00)
Epoch: [77][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2389e-01 (3.6838e-01)	Acc@1  84.38 ( 88.98)	Acc@5  97.66 ( 98.99)
Epoch: [77][350/391]	Time  0.044 ( 0.041)	Data  0.002 ( 0.001)	Loss 5.4872e-01 (3.6933e-01)	Acc@1  82.03 ( 88.94)	Acc@5  96.88 ( 98.99)
Epoch: [77][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8563e-01 (3.6940e-01)	Acc@1  85.16 ( 88.93)	Acc@5  99.22 ( 99.00)
Epoch: [77][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1789e-01 (3.6931e-01)	Acc@1  92.19 ( 88.92)	Acc@5  99.22 ( 99.00)
Epoch: [77][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7414e-01 (3.6973e-01)	Acc@1  89.06 ( 88.89)	Acc@5 100.00 ( 99.00)
Epoch: [77][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 6.3860e-01 (3.7023e-01)	Acc@1  81.25 ( 88.89)	Acc@5  97.50 ( 99.00)
## e[77] optimizer.zero_grad (sum) time: 0.2670426368713379
## e[77]       loss.backward (sum) time: 4.078000068664551
## e[77]      optimizer.step (sum) time: 1.8068833351135254
## epoch[77] training(only) time: 16.2301025390625
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2977e+00 (1.2977e+00)	Acc@1  68.00 ( 68.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 1.2673e+00 (1.3654e+00)	Acc@1  63.00 ( 67.91)	Acc@5  93.00 ( 90.82)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1087e+00 (1.2878e+00)	Acc@1  72.00 ( 68.57)	Acc@5  92.00 ( 91.43)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 1.4549e+00 (1.3061e+00)	Acc@1  61.00 ( 67.94)	Acc@5  89.00 ( 91.00)
Test: [ 40/100]	Time  0.020 ( 0.025)	Loss 1.2534e+00 (1.3195e+00)	Acc@1  70.00 ( 67.54)	Acc@5  91.00 ( 90.90)
Test: [ 50/100]	Time  0.020 ( 0.024)	Loss 1.3302e+00 (1.3228e+00)	Acc@1  68.00 ( 67.57)	Acc@5  89.00 ( 90.63)
Test: [ 60/100]	Time  0.016 ( 0.024)	Loss 1.4516e+00 (1.3011e+00)	Acc@1  64.00 ( 67.77)	Acc@5  92.00 ( 90.90)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.4053e+00 (1.2945e+00)	Acc@1  66.00 ( 68.13)	Acc@5  89.00 ( 90.92)
Test: [ 80/100]	Time  0.024 ( 0.024)	Loss 1.3705e+00 (1.2961e+00)	Acc@1  67.00 ( 68.05)	Acc@5  91.00 ( 90.83)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.4963e+00 (1.2808e+00)	Acc@1  67.00 ( 68.43)	Acc@5  90.00 ( 91.01)
 * Acc@1 68.540 Acc@5 91.120
### epoch[77] execution time: 18.631007194519043
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.196 ( 0.196)	Data  0.143 ( 0.143)	Loss 4.0505e-01 (4.0505e-01)	Acc@1  89.84 ( 89.84)	Acc@5  97.66 ( 97.66)
Epoch: [78][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.014)	Loss 2.6199e-01 (3.3622e-01)	Acc@1  92.97 ( 90.55)	Acc@5  99.22 ( 99.01)
Epoch: [78][ 20/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.008)	Loss 4.2076e-01 (3.4710e-01)	Acc@1  88.28 ( 89.99)	Acc@5  97.66 ( 98.96)
Epoch: [78][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.7885e-01 (3.5641e-01)	Acc@1  88.28 ( 89.57)	Acc@5  99.22 ( 99.07)
Epoch: [78][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.8877e-01 (3.5877e-01)	Acc@1  89.84 ( 89.67)	Acc@5  99.22 ( 99.10)
Epoch: [78][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.7521e-01 (3.6822e-01)	Acc@1  83.59 ( 89.20)	Acc@5  96.88 ( 99.02)
Epoch: [78][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.9336e-01 (3.7128e-01)	Acc@1  84.38 ( 89.04)	Acc@5  99.22 ( 99.01)
Epoch: [78][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.2807e-01 (3.6978e-01)	Acc@1  88.28 ( 89.10)	Acc@5  99.22 ( 98.99)
Epoch: [78][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1528e-01 (3.6996e-01)	Acc@1  90.62 ( 89.04)	Acc@5 100.00 ( 98.96)
Epoch: [78][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4108e-01 (3.7008e-01)	Acc@1  92.19 ( 89.09)	Acc@5 100.00 ( 98.92)
Epoch: [78][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0269e-01 (3.7331e-01)	Acc@1  92.19 ( 88.99)	Acc@5 100.00 ( 98.92)
Epoch: [78][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0783e-01 (3.7289e-01)	Acc@1  87.50 ( 88.93)	Acc@5  97.66 ( 98.94)
Epoch: [78][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7056e-01 (3.7113e-01)	Acc@1  87.50 ( 88.98)	Acc@5  98.44 ( 98.94)
Epoch: [78][130/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0192e-01 (3.6971e-01)	Acc@1  92.19 ( 89.03)	Acc@5  99.22 ( 98.94)
Epoch: [78][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0161e-01 (3.7165e-01)	Acc@1  92.97 ( 88.97)	Acc@5  99.22 ( 98.92)
Epoch: [78][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8826e-01 (3.7234e-01)	Acc@1  91.41 ( 89.02)	Acc@5  96.88 ( 98.91)
Epoch: [78][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6448e-01 (3.7017e-01)	Acc@1  92.19 ( 89.09)	Acc@5 100.00 ( 98.92)
Epoch: [78][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6315e-01 (3.6930e-01)	Acc@1  92.19 ( 89.10)	Acc@5 100.00 ( 98.95)
Epoch: [78][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6108e-01 (3.7067e-01)	Acc@1  85.16 ( 89.11)	Acc@5  99.22 ( 98.96)
Epoch: [78][190/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.1480e-01 (3.7086e-01)	Acc@1  83.59 ( 89.10)	Acc@5  98.44 ( 98.97)
Epoch: [78][200/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7353e-01 (3.7106e-01)	Acc@1  86.72 ( 89.08)	Acc@5 100.00 ( 98.99)
Epoch: [78][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2657e-01 (3.7285e-01)	Acc@1  90.62 ( 89.06)	Acc@5  97.66 ( 98.99)
Epoch: [78][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6553e-01 (3.7211e-01)	Acc@1  87.50 ( 89.09)	Acc@5  98.44 ( 98.97)
Epoch: [78][230/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0287e-01 (3.7282e-01)	Acc@1  91.41 ( 89.04)	Acc@5  99.22 ( 98.99)
Epoch: [78][240/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.4119e-01 (3.7392e-01)	Acc@1  85.94 ( 88.99)	Acc@5  96.09 ( 98.98)
Epoch: [78][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.6021e-01 (3.7438e-01)	Acc@1  85.94 ( 88.98)	Acc@5  99.22 ( 98.95)
Epoch: [78][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8621e-01 (3.7384e-01)	Acc@1  85.94 ( 89.02)	Acc@5  96.88 ( 98.95)
Epoch: [78][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0032e-01 (3.7282e-01)	Acc@1  90.62 ( 89.07)	Acc@5 100.00 ( 98.94)
Epoch: [78][280/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.1308e-01 (3.7244e-01)	Acc@1  89.84 ( 89.04)	Acc@5 100.00 ( 98.96)
Epoch: [78][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3290e-01 (3.7283e-01)	Acc@1  87.50 ( 89.01)	Acc@5  98.44 ( 98.97)
Epoch: [78][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5035e-01 (3.7314e-01)	Acc@1  88.28 ( 88.99)	Acc@5  98.44 ( 98.97)
Epoch: [78][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5613e-01 (3.7351e-01)	Acc@1  89.84 ( 88.97)	Acc@5  99.22 ( 98.98)
Epoch: [78][320/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7566e-01 (3.7345e-01)	Acc@1  87.50 ( 88.94)	Acc@5  99.22 ( 98.99)
Epoch: [78][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2968e-01 (3.7270e-01)	Acc@1  92.97 ( 88.95)	Acc@5  99.22 ( 98.99)
Epoch: [78][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8074e-01 (3.7299e-01)	Acc@1  88.28 ( 88.94)	Acc@5 100.00 ( 98.97)
Epoch: [78][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.0768e-01 (3.7309e-01)	Acc@1  86.72 ( 88.94)	Acc@5  98.44 ( 98.96)
Epoch: [78][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.2294e-01 (3.7212e-01)	Acc@1  84.38 ( 88.96)	Acc@5 100.00 ( 98.97)
Epoch: [78][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5742e-01 (3.7285e-01)	Acc@1  89.06 ( 88.94)	Acc@5 100.00 ( 98.98)
Epoch: [78][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2691e-01 (3.7229e-01)	Acc@1  91.41 ( 88.95)	Acc@5  98.44 ( 98.98)
Epoch: [78][390/391]	Time  0.028 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0343e-01 (3.7170e-01)	Acc@1  88.75 ( 88.95)	Acc@5 100.00 ( 98.99)
## e[78] optimizer.zero_grad (sum) time: 0.26691627502441406
## e[78]       loss.backward (sum) time: 4.004101991653442
## e[78]      optimizer.step (sum) time: 1.8459217548370361
## epoch[78] training(only) time: 16.15754795074463
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2756e+00 (1.2756e+00)	Acc@1  69.00 ( 69.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.021 ( 0.035)	Loss 1.2916e+00 (1.3602e+00)	Acc@1  59.00 ( 66.64)	Acc@5  94.00 ( 90.82)
Test: [ 20/100]	Time  0.020 ( 0.029)	Loss 1.0925e+00 (1.2838e+00)	Acc@1  72.00 ( 68.00)	Acc@5  93.00 ( 91.43)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 1.4648e+00 (1.2995e+00)	Acc@1  61.00 ( 67.61)	Acc@5  88.00 ( 91.00)
Test: [ 40/100]	Time  0.019 ( 0.025)	Loss 1.2299e+00 (1.3097e+00)	Acc@1  70.00 ( 67.29)	Acc@5  94.00 ( 91.02)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.3022e+00 (1.3127e+00)	Acc@1  68.00 ( 67.27)	Acc@5  91.00 ( 90.86)
Test: [ 60/100]	Time  0.026 ( 0.024)	Loss 1.4427e+00 (1.2919e+00)	Acc@1  64.00 ( 67.51)	Acc@5  91.00 ( 91.08)
Test: [ 70/100]	Time  0.018 ( 0.024)	Loss 1.4246e+00 (1.2841e+00)	Acc@1  66.00 ( 67.89)	Acc@5  89.00 ( 91.07)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.3783e+00 (1.2860e+00)	Acc@1  70.00 ( 67.88)	Acc@5  90.00 ( 90.96)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.4765e+00 (1.2707e+00)	Acc@1  69.00 ( 68.33)	Acc@5  90.00 ( 91.14)
 * Acc@1 68.400 Acc@5 91.250
### epoch[78] execution time: 18.487064599990845
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.205 ( 0.205)	Data  0.152 ( 0.152)	Loss 4.4398e-01 (4.4398e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [79][ 10/391]	Time  0.043 ( 0.057)	Data  0.001 ( 0.015)	Loss 3.7089e-01 (3.7432e-01)	Acc@1  87.50 ( 89.20)	Acc@5 100.00 ( 99.29)
Epoch: [79][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 3.1322e-01 (3.5210e-01)	Acc@1  90.62 ( 90.10)	Acc@5 100.00 ( 99.11)
Epoch: [79][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.006)	Loss 3.6351e-01 (3.5971e-01)	Acc@1  89.84 ( 89.87)	Acc@5  98.44 ( 99.02)
Epoch: [79][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.0815e-01 (3.6397e-01)	Acc@1  87.50 ( 89.42)	Acc@5  97.66 ( 99.01)
Epoch: [79][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.5826e-01 (3.7157e-01)	Acc@1  90.62 ( 89.17)	Acc@5 100.00 ( 98.96)
Epoch: [79][ 60/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.0966e-01 (3.6983e-01)	Acc@1  89.06 ( 89.31)	Acc@5  98.44 ( 98.99)
Epoch: [79][ 70/391]	Time  0.045 ( 0.044)	Data  0.002 ( 0.003)	Loss 3.2921e-01 (3.6901e-01)	Acc@1  88.28 ( 89.25)	Acc@5  99.22 ( 99.04)
Epoch: [79][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.5491e-01 (3.7094e-01)	Acc@1  86.72 ( 89.19)	Acc@5  99.22 ( 99.02)
Epoch: [79][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.2228e-01 (3.7394e-01)	Acc@1  84.38 ( 89.05)	Acc@5  98.44 ( 98.98)
Epoch: [79][100/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.8869e-01 (3.7325e-01)	Acc@1  85.16 ( 89.05)	Acc@5  98.44 ( 98.96)
Epoch: [79][110/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.8776e-01 (3.7775e-01)	Acc@1  87.50 ( 88.85)	Acc@5  97.66 ( 98.93)
Epoch: [79][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9900e-01 (3.7824e-01)	Acc@1  88.28 ( 88.82)	Acc@5  99.22 ( 98.93)
Epoch: [79][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5721e-01 (3.7756e-01)	Acc@1  85.16 ( 88.81)	Acc@5  97.66 ( 98.91)
Epoch: [79][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6383e-01 (3.7645e-01)	Acc@1  92.97 ( 88.84)	Acc@5  99.22 ( 98.91)
Epoch: [79][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3063e-01 (3.7486e-01)	Acc@1  91.41 ( 88.86)	Acc@5  99.22 ( 98.92)
Epoch: [79][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7537e-01 (3.7456e-01)	Acc@1  92.97 ( 88.87)	Acc@5  99.22 ( 98.92)
Epoch: [79][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4898e-01 (3.7451e-01)	Acc@1  82.81 ( 88.82)	Acc@5  98.44 ( 98.92)
Epoch: [79][180/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5847e-01 (3.7368e-01)	Acc@1  93.75 ( 88.87)	Acc@5 100.00 ( 98.95)
Epoch: [79][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7299e-01 (3.7371e-01)	Acc@1  89.84 ( 88.85)	Acc@5  99.22 ( 98.98)
Epoch: [79][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9560e-01 (3.7389e-01)	Acc@1  84.38 ( 88.82)	Acc@5  97.66 ( 98.99)
Epoch: [79][210/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8677e-01 (3.7446e-01)	Acc@1  87.50 ( 88.80)	Acc@5  99.22 ( 98.99)
Epoch: [79][220/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 2.7563e-01 (3.7190e-01)	Acc@1  92.19 ( 88.88)	Acc@5  99.22 ( 99.01)
Epoch: [79][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4286e-01 (3.7110e-01)	Acc@1  88.28 ( 88.89)	Acc@5  99.22 ( 99.03)
Epoch: [79][240/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0126e-01 (3.7206e-01)	Acc@1  85.94 ( 88.88)	Acc@5  99.22 ( 99.02)
Epoch: [79][250/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8980e-01 (3.7074e-01)	Acc@1  92.19 ( 88.93)	Acc@5 100.00 ( 99.03)
Epoch: [79][260/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3652e-01 (3.7086e-01)	Acc@1  85.94 ( 88.89)	Acc@5  98.44 ( 99.01)
Epoch: [79][270/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 5.4861e-01 (3.7138e-01)	Acc@1  81.25 ( 88.86)	Acc@5  98.44 ( 99.00)
Epoch: [79][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1337e-01 (3.7101e-01)	Acc@1  90.62 ( 88.88)	Acc@5  98.44 ( 99.00)
Epoch: [79][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8633e-01 (3.7016e-01)	Acc@1  92.19 ( 88.92)	Acc@5  98.44 ( 98.99)
Epoch: [79][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0623e-01 (3.7039e-01)	Acc@1  87.50 ( 88.90)	Acc@5  97.66 ( 98.98)
Epoch: [79][310/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1469e-01 (3.7016e-01)	Acc@1  89.06 ( 88.89)	Acc@5 100.00 ( 99.00)
Epoch: [79][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8058e-01 (3.6966e-01)	Acc@1  90.62 ( 88.93)	Acc@5 100.00 ( 99.01)
Epoch: [79][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5361e-01 (3.6959e-01)	Acc@1  90.62 ( 88.91)	Acc@5  99.22 ( 99.02)
Epoch: [79][340/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1121e-01 (3.7044e-01)	Acc@1  82.03 ( 88.87)	Acc@5  96.88 ( 99.01)
Epoch: [79][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4264e-01 (3.7048e-01)	Acc@1  87.50 ( 88.88)	Acc@5  99.22 ( 99.02)
Epoch: [79][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2247e-01 (3.7061e-01)	Acc@1  85.94 ( 88.87)	Acc@5  98.44 ( 99.01)
Epoch: [79][370/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4475e-01 (3.7099e-01)	Acc@1  89.84 ( 88.87)	Acc@5  99.22 ( 99.00)
Epoch: [79][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5084e-01 (3.7087e-01)	Acc@1  86.72 ( 88.86)	Acc@5  99.22 ( 99.00)
Epoch: [79][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.3622e-01 (3.7050e-01)	Acc@1  85.00 ( 88.89)	Acc@5 100.00 ( 99.00)
## e[79] optimizer.zero_grad (sum) time: 0.2646939754486084
## e[79]       loss.backward (sum) time: 4.057075262069702
## e[79]      optimizer.step (sum) time: 1.808790922164917
## epoch[79] training(only) time: 16.205892324447632
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.3077e+00 (1.3077e+00)	Acc@1  68.00 ( 68.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 1.2701e+00 (1.3689e+00)	Acc@1  59.00 ( 66.64)	Acc@5  93.00 ( 90.55)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 1.1031e+00 (1.2958e+00)	Acc@1  73.00 ( 67.62)	Acc@5  93.00 ( 91.33)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4937e+00 (1.3155e+00)	Acc@1  62.00 ( 67.32)	Acc@5  89.00 ( 90.87)
Test: [ 40/100]	Time  0.024 ( 0.026)	Loss 1.2527e+00 (1.3238e+00)	Acc@1  70.00 ( 67.07)	Acc@5  91.00 ( 90.78)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.3196e+00 (1.3257e+00)	Acc@1  69.00 ( 67.20)	Acc@5  90.00 ( 90.51)
Test: [ 60/100]	Time  0.018 ( 0.025)	Loss 1.4904e+00 (1.3025e+00)	Acc@1  63.00 ( 67.48)	Acc@5  91.00 ( 90.75)
Test: [ 70/100]	Time  0.018 ( 0.024)	Loss 1.4404e+00 (1.2951e+00)	Acc@1  65.00 ( 67.87)	Acc@5  89.00 ( 90.79)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.3872e+00 (1.2965e+00)	Acc@1  69.00 ( 67.85)	Acc@5  91.00 ( 90.72)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.4942e+00 (1.2802e+00)	Acc@1  70.00 ( 68.35)	Acc@5  90.00 ( 90.90)
 * Acc@1 68.400 Acc@5 91.060
### epoch[79] execution time: 18.55345845222473
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.204 ( 0.204)	Data  0.149 ( 0.149)	Loss 3.6916e-01 (3.6916e-01)	Acc@1  91.41 ( 91.41)	Acc@5  98.44 ( 98.44)
Epoch: [80][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 3.8064e-01 (3.5972e-01)	Acc@1  85.16 ( 89.56)	Acc@5 100.00 ( 99.08)
Epoch: [80][ 20/391]	Time  0.044 ( 0.050)	Data  0.001 ( 0.008)	Loss 4.3211e-01 (3.7068e-01)	Acc@1  89.06 ( 89.03)	Acc@5  97.66 ( 99.00)
Epoch: [80][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.006)	Loss 2.9609e-01 (3.6959e-01)	Acc@1  90.62 ( 88.99)	Acc@5  99.22 ( 98.89)
Epoch: [80][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.8385e-01 (3.7543e-01)	Acc@1  83.59 ( 88.72)	Acc@5  99.22 ( 98.88)
Epoch: [80][ 50/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.5552e-01 (3.7625e-01)	Acc@1  85.94 ( 88.71)	Acc@5  97.66 ( 98.90)
Epoch: [80][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.2105e-01 (3.7823e-01)	Acc@1  86.72 ( 88.60)	Acc@5  97.66 ( 98.96)
Epoch: [80][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.2786e-01 (3.7394e-01)	Acc@1  89.06 ( 88.86)	Acc@5  99.22 ( 99.00)
Epoch: [80][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.0021e-01 (3.7226e-01)	Acc@1  92.97 ( 88.90)	Acc@5 100.00 ( 99.00)
Epoch: [80][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.6906e-01 (3.7388e-01)	Acc@1  84.38 ( 88.91)	Acc@5  98.44 ( 98.94)
Epoch: [80][100/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.8960e-01 (3.7158e-01)	Acc@1  89.84 ( 89.00)	Acc@5  99.22 ( 98.98)
Epoch: [80][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4320e-01 (3.7213e-01)	Acc@1  91.41 ( 88.99)	Acc@5  99.22 ( 98.96)
Epoch: [80][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0576e-01 (3.7074e-01)	Acc@1  90.62 ( 89.00)	Acc@5  99.22 ( 98.96)
Epoch: [80][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6536e-01 (3.7093e-01)	Acc@1  85.16 ( 88.97)	Acc@5  97.66 ( 98.95)
Epoch: [80][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8842e-01 (3.7047e-01)	Acc@1  90.62 ( 88.96)	Acc@5 100.00 ( 98.98)
Epoch: [80][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1227e-01 (3.6841e-01)	Acc@1  82.03 ( 89.01)	Acc@5 100.00 ( 99.00)
Epoch: [80][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.9506e-01 (3.7029e-01)	Acc@1  83.59 ( 88.92)	Acc@5  99.22 ( 99.01)
Epoch: [80][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9738e-01 (3.7115e-01)	Acc@1  90.62 ( 88.88)	Acc@5 100.00 ( 99.00)
Epoch: [80][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1882e-01 (3.7309e-01)	Acc@1  85.16 ( 88.77)	Acc@5  96.88 ( 98.98)
Epoch: [80][190/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2565e-01 (3.7499e-01)	Acc@1  89.84 ( 88.70)	Acc@5  99.22 ( 98.98)
Epoch: [80][200/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5186e-01 (3.7403e-01)	Acc@1  88.28 ( 88.72)	Acc@5  99.22 ( 98.97)
Epoch: [80][210/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5084e-01 (3.7369e-01)	Acc@1  89.06 ( 88.76)	Acc@5  98.44 ( 98.97)
Epoch: [80][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8278e-01 (3.7285e-01)	Acc@1  85.94 ( 88.77)	Acc@5 100.00 ( 98.99)
Epoch: [80][230/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0475e-01 (3.7302e-01)	Acc@1  87.50 ( 88.78)	Acc@5  97.66 ( 98.99)
Epoch: [80][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4689e-01 (3.7436e-01)	Acc@1  87.50 ( 88.71)	Acc@5 100.00 ( 98.99)
Epoch: [80][250/391]	Time  0.045 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.1652e-01 (3.7291e-01)	Acc@1  90.62 ( 88.76)	Acc@5 100.00 ( 98.99)
Epoch: [80][260/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8756e-01 (3.7340e-01)	Acc@1  85.16 ( 88.79)	Acc@5  96.09 ( 98.98)
Epoch: [80][270/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4178e-01 (3.7207e-01)	Acc@1  88.28 ( 88.82)	Acc@5 100.00 ( 99.01)
Epoch: [80][280/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7379e-01 (3.7334e-01)	Acc@1  89.06 ( 88.81)	Acc@5  97.66 ( 98.99)
Epoch: [80][290/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9888e-01 (3.7219e-01)	Acc@1  88.28 ( 88.86)	Acc@5  99.22 ( 99.00)
Epoch: [80][300/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7403e-01 (3.7146e-01)	Acc@1  88.28 ( 88.87)	Acc@5  99.22 ( 99.01)
Epoch: [80][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.0028e-01 (3.7114e-01)	Acc@1  89.84 ( 88.85)	Acc@5 100.00 ( 99.01)
Epoch: [80][320/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.8980e-01 (3.7041e-01)	Acc@1  90.62 ( 88.90)	Acc@5 100.00 ( 99.00)
Epoch: [80][330/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5199e-01 (3.7052e-01)	Acc@1  88.28 ( 88.89)	Acc@5  99.22 ( 98.99)
Epoch: [80][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0066e-01 (3.7119e-01)	Acc@1  86.72 ( 88.86)	Acc@5  99.22 ( 99.00)
Epoch: [80][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8261e-01 (3.7148e-01)	Acc@1  86.72 ( 88.83)	Acc@5  97.66 ( 99.00)
Epoch: [80][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.1959e-01 (3.7281e-01)	Acc@1  83.59 ( 88.77)	Acc@5  96.88 ( 98.99)
Epoch: [80][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7367e-01 (3.7271e-01)	Acc@1  88.28 ( 88.76)	Acc@5  99.22 ( 98.98)
Epoch: [80][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6030e-01 (3.7241e-01)	Acc@1  89.84 ( 88.78)	Acc@5  99.22 ( 98.99)
Epoch: [80][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8246e-01 (3.7240e-01)	Acc@1  90.00 ( 88.77)	Acc@5  98.75 ( 98.99)
## e[80] optimizer.zero_grad (sum) time: 0.26491785049438477
## e[80]       loss.backward (sum) time: 4.083408832550049
## e[80]      optimizer.step (sum) time: 1.839418888092041
## epoch[80] training(only) time: 16.192124605178833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.2908e+00 (1.2908e+00)	Acc@1  69.00 ( 69.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.024 ( 0.036)	Loss 1.3042e+00 (1.3644e+00)	Acc@1  60.00 ( 67.18)	Acc@5  93.00 ( 90.36)
Test: [ 20/100]	Time  0.025 ( 0.029)	Loss 1.0921e+00 (1.2887e+00)	Acc@1  73.00 ( 68.10)	Acc@5  93.00 ( 91.14)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4778e+00 (1.3112e+00)	Acc@1  61.00 ( 67.61)	Acc@5  90.00 ( 90.81)
Test: [ 40/100]	Time  0.021 ( 0.026)	Loss 1.2471e+00 (1.3210e+00)	Acc@1  72.00 ( 67.32)	Acc@5  93.00 ( 90.78)
Test: [ 50/100]	Time  0.018 ( 0.025)	Loss 1.2969e+00 (1.3207e+00)	Acc@1  71.00 ( 67.31)	Acc@5  91.00 ( 90.53)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 1.4348e+00 (1.2979e+00)	Acc@1  64.00 ( 67.61)	Acc@5  92.00 ( 90.80)
Test: [ 70/100]	Time  0.020 ( 0.024)	Loss 1.3977e+00 (1.2894e+00)	Acc@1  67.00 ( 68.03)	Acc@5  89.00 ( 90.82)
Test: [ 80/100]	Time  0.018 ( 0.024)	Loss 1.3663e+00 (1.2916e+00)	Acc@1  69.00 ( 68.00)	Acc@5  88.00 ( 90.69)
Test: [ 90/100]	Time  0.018 ( 0.024)	Loss 1.4928e+00 (1.2760e+00)	Acc@1  70.00 ( 68.47)	Acc@5  89.00 ( 90.88)
 * Acc@1 68.600 Acc@5 91.010
### epoch[80] execution time: 18.57040500640869
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.202 ( 0.202)	Data  0.152 ( 0.152)	Loss 3.5293e-01 (3.5293e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Epoch: [81][ 10/391]	Time  0.042 ( 0.057)	Data  0.001 ( 0.015)	Loss 3.0674e-01 (3.6234e-01)	Acc@1  92.97 ( 89.20)	Acc@5  99.22 ( 98.93)
Epoch: [81][ 20/391]	Time  0.044 ( 0.050)	Data  0.001 ( 0.008)	Loss 3.4012e-01 (3.6729e-01)	Acc@1  90.62 ( 89.21)	Acc@5  99.22 ( 98.66)
Epoch: [81][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.006)	Loss 6.1936e-01 (3.6272e-01)	Acc@1  75.78 ( 89.24)	Acc@5  99.22 ( 98.84)
Epoch: [81][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.005)	Loss 4.3698e-01 (3.7214e-01)	Acc@1  88.28 ( 88.81)	Acc@5  96.88 ( 98.86)
Epoch: [81][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.8342e-01 (3.6911e-01)	Acc@1  86.72 ( 88.96)	Acc@5  98.44 ( 98.93)
Epoch: [81][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.6416e-01 (3.6226e-01)	Acc@1  92.97 ( 89.11)	Acc@5 100.00 ( 99.00)
Epoch: [81][ 70/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.2558e-01 (3.6589e-01)	Acc@1  85.16 ( 88.96)	Acc@5 100.00 ( 98.98)
Epoch: [81][ 80/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.8298e-01 (3.6797e-01)	Acc@1  88.28 ( 88.95)	Acc@5  98.44 ( 98.92)
Epoch: [81][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.9104e-01 (3.7056e-01)	Acc@1  89.06 ( 88.83)	Acc@5 100.00 ( 98.97)
Epoch: [81][100/391]	Time  0.046 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4380e-01 (3.7189e-01)	Acc@1  89.84 ( 88.78)	Acc@5  99.22 ( 98.98)
Epoch: [81][110/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.9182e-01 (3.7303e-01)	Acc@1  86.72 ( 88.70)	Acc@5 100.00 ( 98.97)
Epoch: [81][120/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.002)	Loss 4.1171e-01 (3.7319e-01)	Acc@1  90.62 ( 88.73)	Acc@5 100.00 ( 99.01)
Epoch: [81][130/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.4413e-01 (3.7087e-01)	Acc@1  89.84 ( 88.83)	Acc@5  98.44 ( 99.02)
Epoch: [81][140/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1643e-01 (3.7101e-01)	Acc@1  92.97 ( 88.85)	Acc@5 100.00 ( 99.01)
Epoch: [81][150/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8670e-01 (3.6966e-01)	Acc@1  91.41 ( 88.88)	Acc@5 100.00 ( 99.02)
Epoch: [81][160/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7632e-01 (3.7080e-01)	Acc@1  92.19 ( 88.87)	Acc@5  99.22 ( 99.02)
Epoch: [81][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2755e-01 (3.7086e-01)	Acc@1  90.62 ( 88.86)	Acc@5  99.22 ( 99.02)
Epoch: [81][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6564e-01 (3.6901e-01)	Acc@1  89.84 ( 88.87)	Acc@5 100.00 ( 99.03)
Epoch: [81][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7113e-01 (3.6886e-01)	Acc@1  87.50 ( 88.92)	Acc@5  98.44 ( 99.03)
Epoch: [81][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9622e-01 (3.6844e-01)	Acc@1  85.94 ( 88.96)	Acc@5  99.22 ( 99.03)
Epoch: [81][210/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5301e-01 (3.6692e-01)	Acc@1  89.84 ( 89.02)	Acc@5  99.22 ( 99.05)
Epoch: [81][220/391]	Time  0.051 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1656e-01 (3.6693e-01)	Acc@1  85.16 ( 89.00)	Acc@5  99.22 ( 99.06)
Epoch: [81][230/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 6.2050e-01 (3.6826e-01)	Acc@1  85.16 ( 88.96)	Acc@5  96.88 ( 99.05)
Epoch: [81][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0488e-01 (3.6839e-01)	Acc@1  92.19 ( 88.99)	Acc@5  97.66 ( 99.05)
Epoch: [81][250/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3626e-01 (3.6807e-01)	Acc@1  93.75 ( 89.02)	Acc@5  99.22 ( 99.06)
Epoch: [81][260/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3312e-01 (3.6811e-01)	Acc@1  90.62 ( 89.01)	Acc@5 100.00 ( 99.07)
Epoch: [81][270/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 2.7495e-01 (3.6860e-01)	Acc@1  92.97 ( 88.99)	Acc@5  99.22 ( 99.07)
Epoch: [81][280/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.2651e-01 (3.6873e-01)	Acc@1  90.62 ( 89.00)	Acc@5  98.44 ( 99.06)
Epoch: [81][290/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6136e-01 (3.6908e-01)	Acc@1  89.84 ( 88.99)	Acc@5 100.00 ( 99.07)
Epoch: [81][300/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9600e-01 (3.6959e-01)	Acc@1  89.06 ( 88.96)	Acc@5  98.44 ( 99.06)
Epoch: [81][310/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.4542e-01 (3.6885e-01)	Acc@1  89.84 ( 89.00)	Acc@5  99.22 ( 99.06)
Epoch: [81][320/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.1977e-01 (3.6960e-01)	Acc@1  89.06 ( 88.99)	Acc@5 100.00 ( 99.05)
Epoch: [81][330/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.4956e-01 (3.6887e-01)	Acc@1  92.19 ( 89.03)	Acc@5  99.22 ( 99.05)
Epoch: [81][340/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.6478e-01 (3.6912e-01)	Acc@1  89.84 ( 89.04)	Acc@5 100.00 ( 99.06)
Epoch: [81][350/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.6305e-01 (3.6899e-01)	Acc@1  89.84 ( 89.06)	Acc@5  97.66 ( 99.05)
Epoch: [81][360/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 5.1094e-01 (3.6968e-01)	Acc@1  82.81 ( 89.02)	Acc@5  98.44 ( 99.04)
Epoch: [81][370/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.5607e-01 (3.6945e-01)	Acc@1  91.41 ( 89.04)	Acc@5  99.22 ( 99.03)
Epoch: [81][380/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.1452e-01 (3.6998e-01)	Acc@1  88.28 ( 89.04)	Acc@5  99.22 ( 99.02)
Epoch: [81][390/391]	Time  0.029 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.5596e-01 (3.6945e-01)	Acc@1  90.00 ( 89.05)	Acc@5 100.00 ( 99.02)
## e[81] optimizer.zero_grad (sum) time: 0.26579785346984863
## e[81]       loss.backward (sum) time: 4.1280505657196045
## e[81]      optimizer.step (sum) time: 1.8115003108978271
## epoch[81] training(only) time: 16.322917222976685
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2504e+00 (1.2504e+00)	Acc@1  67.00 ( 67.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 1.2943e+00 (1.3570e+00)	Acc@1  60.00 ( 67.55)	Acc@5  93.00 ( 90.64)
Test: [ 20/100]	Time  0.026 ( 0.029)	Loss 1.0627e+00 (1.2814e+00)	Acc@1  72.00 ( 68.43)	Acc@5  93.00 ( 91.29)
Test: [ 30/100]	Time  0.020 ( 0.027)	Loss 1.4523e+00 (1.3018e+00)	Acc@1  61.00 ( 67.81)	Acc@5  89.00 ( 90.74)
Test: [ 40/100]	Time  0.021 ( 0.025)	Loss 1.2366e+00 (1.3112e+00)	Acc@1  73.00 ( 67.51)	Acc@5  94.00 ( 90.80)
Test: [ 50/100]	Time  0.019 ( 0.025)	Loss 1.3056e+00 (1.3124e+00)	Acc@1  67.00 ( 67.35)	Acc@5  91.00 ( 90.55)
Test: [ 60/100]	Time  0.017 ( 0.024)	Loss 1.4772e+00 (1.2918e+00)	Acc@1  62.00 ( 67.56)	Acc@5  92.00 ( 90.84)
Test: [ 70/100]	Time  0.021 ( 0.024)	Loss 1.4209e+00 (1.2847e+00)	Acc@1  67.00 ( 67.92)	Acc@5  89.00 ( 90.86)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.3493e+00 (1.2866e+00)	Acc@1  71.00 ( 67.88)	Acc@5  89.00 ( 90.75)
Test: [ 90/100]	Time  0.021 ( 0.023)	Loss 1.4964e+00 (1.2712e+00)	Acc@1  68.00 ( 68.34)	Acc@5  89.00 ( 90.97)
 * Acc@1 68.500 Acc@5 91.110
### epoch[81] execution time: 18.688270568847656
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.199 ( 0.199)	Data  0.148 ( 0.148)	Loss 3.2343e-01 (3.2343e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.041 ( 0.057)	Data  0.001 ( 0.014)	Loss 3.4488e-01 (3.2491e-01)	Acc@1  92.97 ( 91.26)	Acc@5  99.22 ( 99.36)
Epoch: [82][ 20/391]	Time  0.042 ( 0.050)	Data  0.001 ( 0.008)	Loss 3.4516e-01 (3.4482e-01)	Acc@1  89.84 ( 90.62)	Acc@5  99.22 ( 99.14)
Epoch: [82][ 30/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.006)	Loss 4.0379e-01 (3.5877e-01)	Acc@1  86.72 ( 89.84)	Acc@5  99.22 ( 99.09)
Epoch: [82][ 40/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.005)	Loss 2.7571e-01 (3.5295e-01)	Acc@1  92.97 ( 89.86)	Acc@5 100.00 ( 99.16)
Epoch: [82][ 50/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.004)	Loss 2.5212e-01 (3.5820e-01)	Acc@1  96.88 ( 89.63)	Acc@5  98.44 ( 99.10)
Epoch: [82][ 60/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.0095e-01 (3.5468e-01)	Acc@1  89.84 ( 89.75)	Acc@5  98.44 ( 99.09)
Epoch: [82][ 70/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.0130e-01 (3.5311e-01)	Acc@1  93.75 ( 89.79)	Acc@5  99.22 ( 99.10)
Epoch: [82][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.8585e-01 (3.5261e-01)	Acc@1  89.84 ( 89.83)	Acc@5  98.44 ( 99.06)
Epoch: [82][ 90/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.4716e-01 (3.5267e-01)	Acc@1  92.97 ( 89.81)	Acc@5 100.00 ( 99.08)
Epoch: [82][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3930e-01 (3.5133e-01)	Acc@1  88.28 ( 89.81)	Acc@5  99.22 ( 99.10)
Epoch: [82][110/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7029e-01 (3.5392e-01)	Acc@1  85.94 ( 89.67)	Acc@5  97.66 ( 99.08)
Epoch: [82][120/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8061e-01 (3.5555e-01)	Acc@1  85.16 ( 89.59)	Acc@5  98.44 ( 99.06)
Epoch: [82][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6649e-01 (3.5468e-01)	Acc@1  88.28 ( 89.63)	Acc@5 100.00 ( 99.06)
Epoch: [82][140/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4351e-01 (3.5585e-01)	Acc@1  93.75 ( 89.56)	Acc@5 100.00 ( 99.06)
Epoch: [82][150/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9119e-01 (3.5686e-01)	Acc@1  90.62 ( 89.50)	Acc@5  98.44 ( 99.07)
Epoch: [82][160/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0972e-01 (3.5675e-01)	Acc@1  94.53 ( 89.49)	Acc@5 100.00 ( 99.09)
Epoch: [82][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.9706e-01 (3.5597e-01)	Acc@1  85.94 ( 89.54)	Acc@5  98.44 ( 99.09)
Epoch: [82][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0583e-01 (3.5660e-01)	Acc@1  90.62 ( 89.52)	Acc@5 100.00 ( 99.09)
Epoch: [82][190/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.2401e-01 (3.5684e-01)	Acc@1  85.94 ( 89.48)	Acc@5  98.44 ( 99.06)
Epoch: [82][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4001e-01 (3.5819e-01)	Acc@1  89.84 ( 89.41)	Acc@5 100.00 ( 99.05)
Epoch: [82][210/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5841e-01 (3.5999e-01)	Acc@1  89.84 ( 89.36)	Acc@5  99.22 ( 99.04)
Epoch: [82][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5951e-01 (3.5903e-01)	Acc@1  92.97 ( 89.42)	Acc@5 100.00 ( 99.04)
Epoch: [82][230/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4150e-01 (3.5963e-01)	Acc@1  89.06 ( 89.40)	Acc@5  97.66 ( 99.02)
Epoch: [82][240/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0219e-01 (3.6143e-01)	Acc@1  91.41 ( 89.34)	Acc@5 100.00 ( 99.00)
Epoch: [82][250/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7077e-01 (3.5994e-01)	Acc@1  85.94 ( 89.36)	Acc@5  99.22 ( 99.02)
Epoch: [82][260/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0012e-01 (3.6135e-01)	Acc@1  91.41 ( 89.33)	Acc@5  97.66 ( 99.00)
Epoch: [82][270/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2109e-01 (3.6138e-01)	Acc@1  92.19 ( 89.31)	Acc@5  98.44 ( 98.99)
Epoch: [82][280/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0136e-01 (3.6223e-01)	Acc@1  88.28 ( 89.29)	Acc@5  96.88 ( 98.99)
Epoch: [82][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3869e-01 (3.6198e-01)	Acc@1  88.28 ( 89.31)	Acc@5  99.22 ( 98.99)
Epoch: [82][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6532e-01 (3.6151e-01)	Acc@1  89.06 ( 89.33)	Acc@5  98.44 ( 98.99)
Epoch: [82][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.5960e-01 (3.6192e-01)	Acc@1  84.38 ( 89.29)	Acc@5  97.66 ( 98.99)
Epoch: [82][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9080e-01 (3.6272e-01)	Acc@1  87.50 ( 89.24)	Acc@5  98.44 ( 98.99)
Epoch: [82][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.2582e-01 (3.6268e-01)	Acc@1  91.41 ( 89.21)	Acc@5  98.44 ( 99.00)
Epoch: [82][340/391]	Time  0.046 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7395e-01 (3.6345e-01)	Acc@1  89.06 ( 89.17)	Acc@5  99.22 ( 98.99)
Epoch: [82][350/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.3873e-01 (3.6320e-01)	Acc@1  92.19 ( 89.18)	Acc@5  99.22 ( 99.00)
Epoch: [82][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8626e-01 (3.6339e-01)	Acc@1  82.81 ( 89.17)	Acc@5  97.66 ( 99.00)
Epoch: [82][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 7.2356e-01 (3.6460e-01)	Acc@1  75.78 ( 89.13)	Acc@5  98.44 ( 99.00)
Epoch: [82][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4508e-01 (3.6551e-01)	Acc@1  82.03 ( 89.08)	Acc@5  98.44 ( 98.99)
Epoch: [82][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.2775e-01 (3.6612e-01)	Acc@1  83.75 ( 89.06)	Acc@5  97.50 ( 98.99)
## e[82] optimizer.zero_grad (sum) time: 0.2659611701965332
## e[82]       loss.backward (sum) time: 4.063302516937256
## e[82]      optimizer.step (sum) time: 1.8328115940093994
## epoch[82] training(only) time: 16.15438151359558
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2793e+00 (1.2793e+00)	Acc@1  66.00 ( 66.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.024 ( 0.035)	Loss 1.2834e+00 (1.3744e+00)	Acc@1  63.00 ( 67.36)	Acc@5  94.00 ( 90.64)
Test: [ 20/100]	Time  0.019 ( 0.029)	Loss 1.0785e+00 (1.2991e+00)	Acc@1  73.00 ( 68.38)	Acc@5  93.00 ( 91.14)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 1.4745e+00 (1.3210e+00)	Acc@1  62.00 ( 67.87)	Acc@5  89.00 ( 90.61)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.2563e+00 (1.3318e+00)	Acc@1  71.00 ( 67.56)	Acc@5  93.00 ( 90.66)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3123e+00 (1.3316e+00)	Acc@1  66.00 ( 67.41)	Acc@5  91.00 ( 90.55)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 1.4822e+00 (1.3098e+00)	Acc@1  63.00 ( 67.67)	Acc@5  91.00 ( 90.77)
Test: [ 70/100]	Time  0.018 ( 0.024)	Loss 1.4177e+00 (1.3034e+00)	Acc@1  67.00 ( 67.96)	Acc@5  89.00 ( 90.80)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3969e+00 (1.3052e+00)	Acc@1  68.00 ( 67.94)	Acc@5  91.00 ( 90.72)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 1.4940e+00 (1.2888e+00)	Acc@1  69.00 ( 68.32)	Acc@5  89.00 ( 90.91)
 * Acc@1 68.470 Acc@5 91.060
### epoch[82] execution time: 18.54694628715515
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.192 ( 0.192)	Data  0.143 ( 0.143)	Loss 4.3928e-01 (4.3928e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [83][ 10/391]	Time  0.041 ( 0.055)	Data  0.001 ( 0.014)	Loss 3.3420e-01 (3.5804e-01)	Acc@1  92.97 ( 90.06)	Acc@5  98.44 ( 98.93)
Epoch: [83][ 20/391]	Time  0.043 ( 0.048)	Data  0.002 ( 0.008)	Loss 3.3726e-01 (3.4602e-01)	Acc@1  90.62 ( 90.10)	Acc@5  97.66 ( 98.81)
Epoch: [83][ 30/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.006)	Loss 2.8688e-01 (3.4947e-01)	Acc@1  92.19 ( 89.59)	Acc@5 100.00 ( 98.97)
Epoch: [83][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.005)	Loss 3.6930e-01 (3.4825e-01)	Acc@1  86.72 ( 89.75)	Acc@5 100.00 ( 99.05)
Epoch: [83][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.6844e-01 (3.5330e-01)	Acc@1  89.84 ( 89.63)	Acc@5  99.22 ( 99.08)
Epoch: [83][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.4905e-01 (3.5823e-01)	Acc@1  85.94 ( 89.57)	Acc@5 100.00 ( 99.01)
Epoch: [83][ 70/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.9628e-01 (3.6201e-01)	Acc@1  81.25 ( 89.44)	Acc@5  96.88 ( 98.99)
Epoch: [83][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3465e-01 (3.6262e-01)	Acc@1  90.62 ( 89.29)	Acc@5  98.44 ( 99.02)
Epoch: [83][ 90/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.9609e-01 (3.6510e-01)	Acc@1  87.50 ( 89.16)	Acc@5  98.44 ( 98.99)
Epoch: [83][100/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.2828e-01 (3.6443e-01)	Acc@1  89.84 ( 89.14)	Acc@5  99.22 ( 99.03)
Epoch: [83][110/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6473e-01 (3.6419e-01)	Acc@1  87.50 ( 89.08)	Acc@5 100.00 ( 99.04)
Epoch: [83][120/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9150e-01 (3.6282e-01)	Acc@1  88.28 ( 89.12)	Acc@5  97.66 ( 99.03)
Epoch: [83][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4663e-01 (3.6090e-01)	Acc@1  91.41 ( 89.16)	Acc@5  99.22 ( 99.06)
Epoch: [83][140/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.9911e-01 (3.5995e-01)	Acc@1  90.62 ( 89.20)	Acc@5 100.00 ( 99.06)
Epoch: [83][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4736e-01 (3.5965e-01)	Acc@1  87.50 ( 89.18)	Acc@5  99.22 ( 99.08)
Epoch: [83][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2805e-01 (3.6248e-01)	Acc@1  85.94 ( 89.03)	Acc@5  99.22 ( 99.03)
Epoch: [83][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3093e-01 (3.6157e-01)	Acc@1  91.41 ( 89.09)	Acc@5  99.22 ( 99.04)
Epoch: [83][180/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0914e-01 (3.6406e-01)	Acc@1  88.28 ( 89.05)	Acc@5  99.22 ( 99.05)
Epoch: [83][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9224e-01 (3.6327e-01)	Acc@1  85.16 ( 89.04)	Acc@5  99.22 ( 99.06)
Epoch: [83][200/391]	Time  0.045 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1571e-01 (3.6316e-01)	Acc@1  88.28 ( 89.07)	Acc@5  97.66 ( 99.06)
Epoch: [83][210/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.8834e-01 (3.6552e-01)	Acc@1  85.16 ( 88.97)	Acc@5  98.44 ( 99.05)
Epoch: [83][220/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 2.7379e-01 (3.6445e-01)	Acc@1  90.62 ( 89.01)	Acc@5  99.22 ( 99.06)
Epoch: [83][230/391]	Time  0.042 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.9057e-01 (3.6632e-01)	Acc@1  91.41 ( 88.94)	Acc@5  96.88 ( 99.05)
Epoch: [83][240/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4090e-01 (3.6689e-01)	Acc@1  86.72 ( 88.92)	Acc@5  98.44 ( 99.05)
Epoch: [83][250/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2980e-01 (3.6660e-01)	Acc@1  86.72 ( 88.93)	Acc@5  99.22 ( 99.06)
Epoch: [83][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4278e-01 (3.6688e-01)	Acc@1  88.28 ( 88.97)	Acc@5  99.22 ( 99.06)
Epoch: [83][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5764e-01 (3.6602e-01)	Acc@1  89.84 ( 88.98)	Acc@5  99.22 ( 99.07)
Epoch: [83][280/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.6509e-01 (3.6519e-01)	Acc@1  92.97 ( 89.02)	Acc@5 100.00 ( 99.08)
Epoch: [83][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7179e-01 (3.6499e-01)	Acc@1  89.84 ( 89.01)	Acc@5  99.22 ( 99.09)
Epoch: [83][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4622e-01 (3.6527e-01)	Acc@1  89.84 ( 89.03)	Acc@5  98.44 ( 99.10)
Epoch: [83][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1669e-01 (3.6538e-01)	Acc@1  85.94 ( 89.01)	Acc@5  99.22 ( 99.11)
Epoch: [83][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7927e-01 (3.6620e-01)	Acc@1  86.72 ( 88.95)	Acc@5  98.44 ( 99.10)
Epoch: [83][330/391]	Time  0.040 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.0722e-01 (3.6605e-01)	Acc@1  89.06 ( 88.96)	Acc@5  98.44 ( 99.10)
Epoch: [83][340/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2459e-01 (3.6561e-01)	Acc@1  86.72 ( 88.98)	Acc@5  99.22 ( 99.10)
Epoch: [83][350/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7431e-01 (3.6576e-01)	Acc@1  92.19 ( 88.97)	Acc@5 100.00 ( 99.09)
Epoch: [83][360/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7994e-01 (3.6491e-01)	Acc@1  88.28 ( 88.99)	Acc@5  98.44 ( 99.10)
Epoch: [83][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9674e-01 (3.6564e-01)	Acc@1  89.84 ( 88.97)	Acc@5  98.44 ( 99.09)
Epoch: [83][380/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4505e-01 (3.6613e-01)	Acc@1  90.62 ( 88.94)	Acc@5 100.00 ( 99.10)
Epoch: [83][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1873e-01 (3.6656e-01)	Acc@1  85.00 ( 88.92)	Acc@5  98.75 ( 99.09)
## e[83] optimizer.zero_grad (sum) time: 0.26677823066711426
## e[83]       loss.backward (sum) time: 4.027288913726807
## e[83]      optimizer.step (sum) time: 1.8614747524261475
## epoch[83] training(only) time: 16.072904586791992
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2538e+00 (1.2538e+00)	Acc@1  67.00 ( 67.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.018 ( 0.032)	Loss 1.2910e+00 (1.3618e+00)	Acc@1  61.00 ( 66.55)	Acc@5  93.00 ( 90.36)
Test: [ 20/100]	Time  0.021 ( 0.027)	Loss 1.1137e+00 (1.2899e+00)	Acc@1  72.00 ( 67.81)	Acc@5  93.00 ( 91.19)
Test: [ 30/100]	Time  0.022 ( 0.025)	Loss 1.4939e+00 (1.3159e+00)	Acc@1  61.00 ( 67.42)	Acc@5  88.00 ( 90.77)
Test: [ 40/100]	Time  0.024 ( 0.024)	Loss 1.2809e+00 (1.3281e+00)	Acc@1  71.00 ( 67.17)	Acc@5  91.00 ( 90.83)
Test: [ 50/100]	Time  0.023 ( 0.023)	Loss 1.3532e+00 (1.3290e+00)	Acc@1  68.00 ( 67.18)	Acc@5  90.00 ( 90.61)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.4668e+00 (1.3086e+00)	Acc@1  63.00 ( 67.43)	Acc@5  92.00 ( 90.92)
Test: [ 70/100]	Time  0.019 ( 0.022)	Loss 1.4664e+00 (1.3009e+00)	Acc@1  67.00 ( 67.85)	Acc@5  89.00 ( 90.90)
Test: [ 80/100]	Time  0.019 ( 0.022)	Loss 1.4165e+00 (1.3035e+00)	Acc@1  70.00 ( 67.84)	Acc@5  90.00 ( 90.81)
Test: [ 90/100]	Time  0.023 ( 0.022)	Loss 1.5158e+00 (1.2887e+00)	Acc@1  70.00 ( 68.30)	Acc@5  91.00 ( 91.03)
 * Acc@1 68.470 Acc@5 91.180
### epoch[83] execution time: 18.35832905769348
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.192 ( 0.192)	Data  0.145 ( 0.145)	Loss 4.4825e-01 (4.4825e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 97.66)
Epoch: [84][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 3.7559e-01 (3.6059e-01)	Acc@1  88.28 ( 88.49)	Acc@5  99.22 ( 99.36)
Epoch: [84][ 20/391]	Time  0.043 ( 0.050)	Data  0.001 ( 0.008)	Loss 4.8896e-01 (3.7389e-01)	Acc@1  85.16 ( 89.21)	Acc@5  99.22 ( 98.81)
Epoch: [84][ 30/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.006)	Loss 2.5680e-01 (3.7270e-01)	Acc@1  92.19 ( 89.21)	Acc@5  99.22 ( 98.87)
Epoch: [84][ 40/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.005)	Loss 2.8883e-01 (3.6283e-01)	Acc@1  91.41 ( 89.37)	Acc@5 100.00 ( 99.03)
Epoch: [84][ 50/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.004)	Loss 3.0518e-01 (3.5905e-01)	Acc@1  91.41 ( 89.63)	Acc@5  99.22 ( 99.07)
Epoch: [84][ 60/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.5297e-01 (3.6195e-01)	Acc@1  90.62 ( 89.61)	Acc@5 100.00 ( 99.00)
Epoch: [84][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4100e-01 (3.6135e-01)	Acc@1  91.41 ( 89.48)	Acc@5  98.44 ( 99.02)
Epoch: [84][ 80/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.9665e-01 (3.6244e-01)	Acc@1  85.94 ( 89.31)	Acc@5  98.44 ( 99.00)
Epoch: [84][ 90/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.2134e-01 (3.6322e-01)	Acc@1  85.94 ( 89.20)	Acc@5  99.22 ( 98.99)
Epoch: [84][100/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.3356e-01 (3.6310e-01)	Acc@1  89.06 ( 89.16)	Acc@5 100.00 ( 99.03)
Epoch: [84][110/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6253e-01 (3.6178e-01)	Acc@1  84.38 ( 89.19)	Acc@5  98.44 ( 99.05)
Epoch: [84][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3540e-01 (3.5948e-01)	Acc@1  92.97 ( 89.31)	Acc@5 100.00 ( 99.04)
Epoch: [84][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7872e-01 (3.6139e-01)	Acc@1  86.72 ( 89.25)	Acc@5  99.22 ( 99.01)
Epoch: [84][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9639e-01 (3.6468e-01)	Acc@1  90.62 ( 89.10)	Acc@5 100.00 ( 98.98)
Epoch: [84][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.5953e-01 (3.6805e-01)	Acc@1  88.28 ( 89.04)	Acc@5  95.31 ( 98.96)
Epoch: [84][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8979e-01 (3.6934e-01)	Acc@1  92.97 ( 89.00)	Acc@5 100.00 ( 98.94)
Epoch: [84][170/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.0881e-01 (3.6727e-01)	Acc@1  93.75 ( 89.00)	Acc@5 100.00 ( 98.99)
Epoch: [84][180/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.3832e-01 (3.6714e-01)	Acc@1  96.09 ( 89.02)	Acc@5  99.22 ( 99.02)
Epoch: [84][190/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2788e-01 (3.6806e-01)	Acc@1  85.94 ( 89.00)	Acc@5  97.66 ( 99.01)
Epoch: [84][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.3730e-01 (3.6831e-01)	Acc@1  84.38 ( 88.99)	Acc@5  96.88 ( 99.02)
Epoch: [84][210/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2397e-01 (3.6913e-01)	Acc@1  91.41 ( 88.97)	Acc@5  99.22 ( 99.03)
Epoch: [84][220/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0999e-01 (3.6858e-01)	Acc@1  87.50 ( 88.99)	Acc@5  98.44 ( 99.04)
Epoch: [84][230/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7944e-01 (3.6749e-01)	Acc@1  90.62 ( 89.07)	Acc@5  99.22 ( 99.04)
Epoch: [84][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5622e-01 (3.6693e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.05)
Epoch: [84][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8164e-01 (3.6719e-01)	Acc@1  89.06 ( 89.05)	Acc@5 100.00 ( 99.05)
Epoch: [84][260/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7394e-01 (3.6587e-01)	Acc@1  91.41 ( 89.14)	Acc@5  97.66 ( 99.05)
Epoch: [84][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0826e-01 (3.6588e-01)	Acc@1  88.28 ( 89.17)	Acc@5  99.22 ( 99.04)
Epoch: [84][280/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2851e-01 (3.6548e-01)	Acc@1  89.06 ( 89.20)	Acc@5 100.00 ( 99.05)
Epoch: [84][290/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4109e-01 (3.6428e-01)	Acc@1  90.62 ( 89.26)	Acc@5  98.44 ( 99.06)
Epoch: [84][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4704e-01 (3.6377e-01)	Acc@1  93.75 ( 89.31)	Acc@5  97.66 ( 99.04)
Epoch: [84][310/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0001e-01 (3.6418e-01)	Acc@1  86.72 ( 89.30)	Acc@5 100.00 ( 99.05)
Epoch: [84][320/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2752e-01 (3.6428e-01)	Acc@1  87.50 ( 89.29)	Acc@5  99.22 ( 99.05)
Epoch: [84][330/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3062e-01 (3.6436e-01)	Acc@1  92.97 ( 89.29)	Acc@5  97.66 ( 99.05)
Epoch: [84][340/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5468e-01 (3.6391e-01)	Acc@1  89.06 ( 89.31)	Acc@5  99.22 ( 99.04)
Epoch: [84][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.5184e-01 (3.6334e-01)	Acc@1  92.97 ( 89.32)	Acc@5 100.00 ( 99.05)
Epoch: [84][360/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2123e-01 (3.6320e-01)	Acc@1  91.41 ( 89.34)	Acc@5  99.22 ( 99.05)
Epoch: [84][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1799e-01 (3.6385e-01)	Acc@1  91.41 ( 89.33)	Acc@5 100.00 ( 99.04)
Epoch: [84][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1361e-01 (3.6329e-01)	Acc@1  86.72 ( 89.34)	Acc@5  99.22 ( 99.04)
Epoch: [84][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1532e-01 (3.6369e-01)	Acc@1  82.50 ( 89.31)	Acc@5  98.75 ( 99.05)
## e[84] optimizer.zero_grad (sum) time: 0.26685428619384766
## e[84]       loss.backward (sum) time: 4.0193750858306885
## e[84]      optimizer.step (sum) time: 1.8745477199554443
## epoch[84] training(only) time: 16.072038173675537
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.2583e+00 (1.2583e+00)	Acc@1  67.00 ( 67.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.024 ( 0.036)	Loss 1.2633e+00 (1.3569e+00)	Acc@1  61.00 ( 67.27)	Acc@5  93.00 ( 90.73)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.0961e+00 (1.2822e+00)	Acc@1  73.00 ( 68.33)	Acc@5  92.00 ( 91.33)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 1.4915e+00 (1.3029e+00)	Acc@1  62.00 ( 67.81)	Acc@5  89.00 ( 91.00)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.2277e+00 (1.3167e+00)	Acc@1  70.00 ( 67.39)	Acc@5  91.00 ( 90.93)
Test: [ 50/100]	Time  0.021 ( 0.025)	Loss 1.3163e+00 (1.3182e+00)	Acc@1  72.00 ( 67.25)	Acc@5  90.00 ( 90.71)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 1.4449e+00 (1.2966e+00)	Acc@1  63.00 ( 67.52)	Acc@5  92.00 ( 90.92)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.4382e+00 (1.2886e+00)	Acc@1  66.00 ( 67.75)	Acc@5  90.00 ( 90.93)
Test: [ 80/100]	Time  0.019 ( 0.023)	Loss 1.3905e+00 (1.2901e+00)	Acc@1  70.00 ( 67.70)	Acc@5  89.00 ( 90.78)
Test: [ 90/100]	Time  0.017 ( 0.023)	Loss 1.4719e+00 (1.2744e+00)	Acc@1  68.00 ( 68.12)	Acc@5  90.00 ( 90.99)
 * Acc@1 68.260 Acc@5 91.140
### epoch[84] execution time: 18.372134685516357
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.203 ( 0.203)	Data  0.150 ( 0.150)	Loss 3.5841e-01 (3.5841e-01)	Acc@1  89.84 ( 89.84)	Acc@5  98.44 ( 98.44)
Epoch: [85][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.014)	Loss 3.4805e-01 (3.5898e-01)	Acc@1  91.41 ( 89.42)	Acc@5  98.44 ( 99.15)
Epoch: [85][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.008)	Loss 3.6312e-01 (3.6222e-01)	Acc@1  89.84 ( 88.91)	Acc@5  98.44 ( 99.22)
Epoch: [85][ 30/391]	Time  0.040 ( 0.047)	Data  0.001 ( 0.006)	Loss 3.5882e-01 (3.6274e-01)	Acc@1  88.28 ( 89.04)	Acc@5  99.22 ( 99.24)
Epoch: [85][ 40/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.9096e-01 (3.6299e-01)	Acc@1  90.62 ( 89.01)	Acc@5 100.00 ( 99.24)
Epoch: [85][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.5381e-01 (3.6128e-01)	Acc@1  88.28 ( 89.02)	Acc@5  99.22 ( 99.22)
Epoch: [85][ 60/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.004)	Loss 3.6295e-01 (3.6083e-01)	Acc@1  88.28 ( 88.97)	Acc@5  98.44 ( 99.15)
Epoch: [85][ 70/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.0556e-01 (3.5615e-01)	Acc@1  87.50 ( 89.17)	Acc@5  97.66 ( 99.14)
Epoch: [85][ 80/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.7484e-01 (3.5703e-01)	Acc@1  90.62 ( 89.21)	Acc@5  99.22 ( 99.15)
Epoch: [85][ 90/391]	Time  0.044 ( 0.042)	Data  0.002 ( 0.003)	Loss 4.6801e-01 (3.6032e-01)	Acc@1  84.38 ( 89.13)	Acc@5  97.66 ( 99.12)
Epoch: [85][100/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.2331e-01 (3.6178e-01)	Acc@1  85.16 ( 89.06)	Acc@5  96.88 ( 99.09)
Epoch: [85][110/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6227e-01 (3.6225e-01)	Acc@1  86.72 ( 89.04)	Acc@5  97.66 ( 99.09)
Epoch: [85][120/391]	Time  0.042 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.4991e-01 (3.6227e-01)	Acc@1  89.84 ( 89.06)	Acc@5 100.00 ( 99.10)
Epoch: [85][130/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0635e-01 (3.6239e-01)	Acc@1  87.50 ( 89.07)	Acc@5 100.00 ( 99.11)
Epoch: [85][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.8445e-01 (3.6428e-01)	Acc@1  85.94 ( 88.98)	Acc@5  98.44 ( 99.10)
Epoch: [85][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1197e-01 (3.6391e-01)	Acc@1  90.62 ( 88.93)	Acc@5  99.22 ( 99.10)
Epoch: [85][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6272e-01 (3.6484e-01)	Acc@1  88.28 ( 88.92)	Acc@5  98.44 ( 99.09)
Epoch: [85][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3946e-01 (3.6581e-01)	Acc@1  92.19 ( 88.88)	Acc@5  99.22 ( 99.07)
Epoch: [85][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1224e-01 (3.6543e-01)	Acc@1  89.06 ( 88.90)	Acc@5  98.44 ( 99.07)
Epoch: [85][190/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5059e-01 (3.6374e-01)	Acc@1  92.97 ( 89.00)	Acc@5 100.00 ( 99.08)
Epoch: [85][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1950e-01 (3.6448e-01)	Acc@1  87.50 ( 88.97)	Acc@5  98.44 ( 99.08)
Epoch: [85][210/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9804e-01 (3.6326e-01)	Acc@1  90.62 ( 89.01)	Acc@5 100.00 ( 99.08)
Epoch: [85][220/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0020e-01 (3.6344e-01)	Acc@1  90.62 ( 89.04)	Acc@5  99.22 ( 99.08)
Epoch: [85][230/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7911e-01 (3.6235e-01)	Acc@1  91.41 ( 89.06)	Acc@5 100.00 ( 99.08)
Epoch: [85][240/391]	Time  0.041 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.4464e-01 (3.6340e-01)	Acc@1  89.84 ( 89.01)	Acc@5  97.66 ( 99.06)
Epoch: [85][250/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 3.3908e-01 (3.6322e-01)	Acc@1  89.84 ( 89.02)	Acc@5  98.44 ( 99.07)
Epoch: [85][260/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4135e-01 (3.6301e-01)	Acc@1  87.50 ( 89.01)	Acc@5 100.00 ( 99.07)
Epoch: [85][270/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6306e-01 (3.6237e-01)	Acc@1  92.97 ( 89.06)	Acc@5 100.00 ( 99.08)
Epoch: [85][280/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.9054e-01 (3.6169e-01)	Acc@1  90.62 ( 89.06)	Acc@5  99.22 ( 99.07)
Epoch: [85][290/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2140e-01 (3.6102e-01)	Acc@1  92.97 ( 89.08)	Acc@5  99.22 ( 99.08)
Epoch: [85][300/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1878e-01 (3.6127e-01)	Acc@1  89.06 ( 89.08)	Acc@5  97.66 ( 99.08)
Epoch: [85][310/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2131e-01 (3.6074e-01)	Acc@1  89.84 ( 89.11)	Acc@5 100.00 ( 99.10)
Epoch: [85][320/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6636e-01 (3.6068e-01)	Acc@1  90.62 ( 89.12)	Acc@5 100.00 ( 99.10)
Epoch: [85][330/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 5.2102e-01 (3.6213e-01)	Acc@1  82.03 ( 89.07)	Acc@5  99.22 ( 99.10)
Epoch: [85][340/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.4800e-01 (3.6219e-01)	Acc@1  94.53 ( 89.07)	Acc@5  99.22 ( 99.10)
Epoch: [85][350/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5110e-01 (3.6285e-01)	Acc@1  89.84 ( 89.06)	Acc@5  98.44 ( 99.09)
Epoch: [85][360/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5575e-01 (3.6247e-01)	Acc@1  94.53 ( 89.11)	Acc@5 100.00 ( 99.09)
Epoch: [85][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3113e-01 (3.6305e-01)	Acc@1  90.62 ( 89.10)	Acc@5  98.44 ( 99.07)
Epoch: [85][380/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.5283e-01 (3.6292e-01)	Acc@1  91.41 ( 89.12)	Acc@5 100.00 ( 99.06)
Epoch: [85][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.8258e-01 (3.6351e-01)	Acc@1  86.25 ( 89.10)	Acc@5  98.75 ( 99.06)
## e[85] optimizer.zero_grad (sum) time: 0.2660801410675049
## e[85]       loss.backward (sum) time: 4.05169153213501
## e[85]      optimizer.step (sum) time: 1.859281063079834
## epoch[85] training(only) time: 16.25131392478943
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.2721e+00 (1.2721e+00)	Acc@1  67.00 ( 67.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 1.2984e+00 (1.3702e+00)	Acc@1  61.00 ( 67.00)	Acc@5  93.00 ( 90.64)
Test: [ 20/100]	Time  0.019 ( 0.029)	Loss 1.0899e+00 (1.2952e+00)	Acc@1  72.00 ( 68.10)	Acc@5  93.00 ( 91.33)
Test: [ 30/100]	Time  0.019 ( 0.026)	Loss 1.5042e+00 (1.3178e+00)	Acc@1  60.00 ( 67.55)	Acc@5  88.00 ( 90.90)
Test: [ 40/100]	Time  0.024 ( 0.025)	Loss 1.2639e+00 (1.3307e+00)	Acc@1  69.00 ( 67.17)	Acc@5  92.00 ( 90.98)
Test: [ 50/100]	Time  0.023 ( 0.024)	Loss 1.3641e+00 (1.3322e+00)	Acc@1  69.00 ( 67.06)	Acc@5  91.00 ( 90.71)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.4590e+00 (1.3105e+00)	Acc@1  64.00 ( 67.36)	Acc@5  93.00 ( 90.98)
Test: [ 70/100]	Time  0.023 ( 0.023)	Loss 1.4688e+00 (1.3038e+00)	Acc@1  65.00 ( 67.76)	Acc@5  89.00 ( 90.97)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.4132e+00 (1.3065e+00)	Acc@1  69.00 ( 67.75)	Acc@5  89.00 ( 90.85)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.4818e+00 (1.2912e+00)	Acc@1  69.00 ( 68.20)	Acc@5  89.00 ( 91.07)
 * Acc@1 68.300 Acc@5 91.230
### epoch[85] execution time: 18.594587087631226
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.198 ( 0.198)	Data  0.143 ( 0.143)	Loss 3.7703e-01 (3.7703e-01)	Acc@1  85.94 ( 85.94)	Acc@5  99.22 ( 99.22)
Epoch: [86][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.014)	Loss 3.2411e-01 (3.3977e-01)	Acc@1  90.62 ( 89.63)	Acc@5  99.22 ( 99.15)
Epoch: [86][ 20/391]	Time  0.041 ( 0.050)	Data  0.001 ( 0.008)	Loss 2.7823e-01 (3.4009e-01)	Acc@1  92.97 ( 89.69)	Acc@5  99.22 ( 99.26)
Epoch: [86][ 30/391]	Time  0.040 ( 0.047)	Data  0.001 ( 0.006)	Loss 4.8057e-01 (3.5200e-01)	Acc@1  85.16 ( 89.21)	Acc@5  97.66 ( 99.09)
Epoch: [86][ 40/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.004)	Loss 4.9927e-01 (3.5920e-01)	Acc@1  82.81 ( 89.10)	Acc@5  98.44 ( 98.95)
Epoch: [86][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.2089e-01 (3.5487e-01)	Acc@1  93.75 ( 89.43)	Acc@5 100.00 ( 99.02)
Epoch: [86][ 60/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.3787e-01 (3.5127e-01)	Acc@1  89.06 ( 89.55)	Acc@5 100.00 ( 99.05)
Epoch: [86][ 70/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.1978e-01 (3.4950e-01)	Acc@1  92.19 ( 89.67)	Acc@5  99.22 ( 99.03)
Epoch: [86][ 80/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3304e-01 (3.5186e-01)	Acc@1  91.41 ( 89.64)	Acc@5 100.00 ( 99.02)
Epoch: [86][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.3469e-01 (3.5172e-01)	Acc@1  85.94 ( 89.56)	Acc@5  98.44 ( 99.01)
Epoch: [86][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7774e-01 (3.4900e-01)	Acc@1  89.06 ( 89.74)	Acc@5  99.22 ( 99.03)
Epoch: [86][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2446e-01 (3.4953e-01)	Acc@1  92.97 ( 89.75)	Acc@5  98.44 ( 98.99)
Epoch: [86][120/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4582e-01 (3.5265e-01)	Acc@1  86.72 ( 89.67)	Acc@5  98.44 ( 98.97)
Epoch: [86][130/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6851e-01 (3.5033e-01)	Acc@1  92.19 ( 89.72)	Acc@5  99.22 ( 99.00)
Epoch: [86][140/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3006e-01 (3.5232e-01)	Acc@1  91.41 ( 89.71)	Acc@5  99.22 ( 98.99)
Epoch: [86][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7749e-01 (3.5494e-01)	Acc@1  85.94 ( 89.57)	Acc@5  98.44 ( 98.98)
Epoch: [86][160/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3215e-01 (3.5778e-01)	Acc@1  91.41 ( 89.49)	Acc@5  99.22 ( 98.97)
Epoch: [86][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0642e-01 (3.5847e-01)	Acc@1  89.06 ( 89.52)	Acc@5 100.00 ( 98.97)
Epoch: [86][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5853e-01 (3.5881e-01)	Acc@1  90.62 ( 89.49)	Acc@5  99.22 ( 98.97)
Epoch: [86][190/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1328e-01 (3.6066e-01)	Acc@1  89.84 ( 89.40)	Acc@5  99.22 ( 98.96)
Epoch: [86][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1539e-01 (3.5928e-01)	Acc@1  91.41 ( 89.43)	Acc@5  99.22 ( 98.97)
Epoch: [86][210/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3407e-01 (3.6040e-01)	Acc@1  86.72 ( 89.40)	Acc@5  98.44 ( 98.96)
Epoch: [86][220/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4560e-01 (3.6183e-01)	Acc@1  85.16 ( 89.36)	Acc@5  97.66 ( 98.96)
Epoch: [86][230/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5909e-01 (3.6069e-01)	Acc@1  89.84 ( 89.39)	Acc@5  99.22 ( 98.96)
Epoch: [86][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 5.1572e-01 (3.5976e-01)	Acc@1  84.38 ( 89.41)	Acc@5  98.44 ( 98.96)
Epoch: [86][250/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3087e-01 (3.5983e-01)	Acc@1  88.28 ( 89.41)	Acc@5  98.44 ( 98.96)
Epoch: [86][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.7536e-01 (3.6085e-01)	Acc@1  92.19 ( 89.38)	Acc@5 100.00 ( 98.96)
Epoch: [86][270/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.0589e-01 (3.5916e-01)	Acc@1  94.53 ( 89.42)	Acc@5 100.00 ( 98.96)
Epoch: [86][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5505e-01 (3.5946e-01)	Acc@1  91.41 ( 89.42)	Acc@5  99.22 ( 98.97)
Epoch: [86][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3309e-01 (3.5968e-01)	Acc@1  87.50 ( 89.43)	Acc@5  97.66 ( 98.96)
Epoch: [86][300/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.9977e-01 (3.6019e-01)	Acc@1  82.81 ( 89.39)	Acc@5  97.66 ( 98.97)
Epoch: [86][310/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0655e-01 (3.6067e-01)	Acc@1  85.94 ( 89.37)	Acc@5 100.00 ( 98.99)
Epoch: [86][320/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5324e-01 (3.6202e-01)	Acc@1  88.28 ( 89.29)	Acc@5 100.00 ( 98.99)
Epoch: [86][330/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6543e-01 (3.6249e-01)	Acc@1  89.06 ( 89.28)	Acc@5  99.22 ( 98.98)
Epoch: [86][340/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.001)	Loss 3.9465e-01 (3.6288e-01)	Acc@1  89.06 ( 89.27)	Acc@5  99.22 ( 98.98)
Epoch: [86][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8502e-01 (3.6365e-01)	Acc@1  84.38 ( 89.24)	Acc@5  99.22 ( 98.97)
Epoch: [86][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6843e-01 (3.6424e-01)	Acc@1  85.16 ( 89.21)	Acc@5  98.44 ( 98.97)
Epoch: [86][370/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.1671e-01 (3.6426e-01)	Acc@1  90.62 ( 89.24)	Acc@5  98.44 ( 98.97)
Epoch: [86][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.6117e-01 (3.6361e-01)	Acc@1  92.97 ( 89.24)	Acc@5 100.00 ( 98.98)
Epoch: [86][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7145e-01 (3.6363e-01)	Acc@1  90.00 ( 89.23)	Acc@5  97.50 ( 98.98)
## e[86] optimizer.zero_grad (sum) time: 0.26601266860961914
## e[86]       loss.backward (sum) time: 3.990144729614258
## e[86]      optimizer.step (sum) time: 1.8688416481018066
## epoch[86] training(only) time: 16.09468102455139
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.2765e+00 (1.2765e+00)	Acc@1  67.00 ( 67.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.024 ( 0.033)	Loss 1.2648e+00 (1.3735e+00)	Acc@1  61.00 ( 66.55)	Acc@5  93.00 ( 90.55)
Test: [ 20/100]	Time  0.024 ( 0.028)	Loss 1.0832e+00 (1.2944e+00)	Acc@1  72.00 ( 68.05)	Acc@5  93.00 ( 91.29)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 1.5046e+00 (1.3173e+00)	Acc@1  60.00 ( 67.55)	Acc@5  88.00 ( 91.00)
Test: [ 40/100]	Time  0.018 ( 0.025)	Loss 1.2501e+00 (1.3286e+00)	Acc@1  72.00 ( 67.39)	Acc@5  93.00 ( 91.05)
Test: [ 50/100]	Time  0.018 ( 0.024)	Loss 1.3253e+00 (1.3289e+00)	Acc@1  68.00 ( 67.41)	Acc@5  92.00 ( 90.82)
Test: [ 60/100]	Time  0.021 ( 0.023)	Loss 1.4636e+00 (1.3075e+00)	Acc@1  62.00 ( 67.62)	Acc@5  92.00 ( 91.10)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 1.4808e+00 (1.2994e+00)	Acc@1  66.00 ( 68.03)	Acc@5  89.00 ( 91.07)
Test: [ 80/100]	Time  0.020 ( 0.023)	Loss 1.4232e+00 (1.3012e+00)	Acc@1  68.00 ( 68.04)	Acc@5  88.00 ( 90.93)
Test: [ 90/100]	Time  0.023 ( 0.023)	Loss 1.4729e+00 (1.2848e+00)	Acc@1  69.00 ( 68.42)	Acc@5  89.00 ( 91.10)
 * Acc@1 68.550 Acc@5 91.230
### epoch[86] execution time: 18.42162585258484
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.206 ( 0.206)	Data  0.158 ( 0.158)	Loss 4.6553e-01 (4.6553e-01)	Acc@1  88.28 ( 88.28)	Acc@5  96.88 ( 96.88)
Epoch: [87][ 10/391]	Time  0.039 ( 0.056)	Data  0.001 ( 0.015)	Loss 3.7977e-01 (3.8264e-01)	Acc@1  89.06 ( 88.57)	Acc@5  97.66 ( 98.37)
Epoch: [87][ 20/391]	Time  0.041 ( 0.049)	Data  0.002 ( 0.009)	Loss 4.3254e-01 (3.7696e-01)	Acc@1  89.84 ( 89.25)	Acc@5  99.22 ( 98.59)
Epoch: [87][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.9774e-01 (3.7919e-01)	Acc@1  89.06 ( 88.96)	Acc@5  97.66 ( 98.69)
Epoch: [87][ 40/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 2.8676e-01 (3.6844e-01)	Acc@1  92.19 ( 89.12)	Acc@5  99.22 ( 98.80)
Epoch: [87][ 50/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 3.5183e-01 (3.6172e-01)	Acc@1  91.41 ( 89.51)	Acc@5  98.44 ( 98.87)
Epoch: [87][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.004)	Loss 2.8945e-01 (3.6832e-01)	Acc@1  92.19 ( 89.15)	Acc@5 100.00 ( 98.83)
Epoch: [87][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.5327e-01 (3.6821e-01)	Acc@1  88.28 ( 89.14)	Acc@5  98.44 ( 98.87)
Epoch: [87][ 80/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.7223e-01 (3.6466e-01)	Acc@1  92.97 ( 89.40)	Acc@5  99.22 ( 98.88)
Epoch: [87][ 90/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.0436e-01 (3.6601e-01)	Acc@1  87.50 ( 89.28)	Acc@5  99.22 ( 98.94)
Epoch: [87][100/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.8837e-01 (3.6574e-01)	Acc@1  93.75 ( 89.30)	Acc@5  99.22 ( 98.94)
Epoch: [87][110/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0787e-01 (3.6711e-01)	Acc@1  89.84 ( 89.20)	Acc@5 100.00 ( 98.99)
Epoch: [87][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2672e-01 (3.6584e-01)	Acc@1  85.94 ( 89.19)	Acc@5  98.44 ( 99.00)
Epoch: [87][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5662e-01 (3.6622e-01)	Acc@1  89.84 ( 89.16)	Acc@5 100.00 ( 99.00)
Epoch: [87][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8846e-01 (3.6710e-01)	Acc@1  89.84 ( 89.15)	Acc@5 100.00 ( 98.99)
Epoch: [87][150/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4764e-01 (3.6906e-01)	Acc@1  89.84 ( 89.08)	Acc@5  99.22 ( 98.96)
Epoch: [87][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5360e-01 (3.6555e-01)	Acc@1  92.97 ( 89.19)	Acc@5 100.00 ( 98.99)
Epoch: [87][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3240e-01 (3.6630e-01)	Acc@1  85.16 ( 89.09)	Acc@5  98.44 ( 99.00)
Epoch: [87][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.6146e-01 (3.6619e-01)	Acc@1  90.62 ( 89.07)	Acc@5  99.22 ( 99.00)
Epoch: [87][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 1.9635e-01 (3.6464e-01)	Acc@1  96.88 ( 89.12)	Acc@5 100.00 ( 99.01)
Epoch: [87][200/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.3251e-01 (3.6466e-01)	Acc@1  90.62 ( 89.12)	Acc@5  99.22 ( 99.00)
Epoch: [87][210/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1225e-01 (3.6345e-01)	Acc@1  92.19 ( 89.16)	Acc@5  99.22 ( 99.02)
Epoch: [87][220/391]	Time  0.049 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.5640e-01 (3.6281e-01)	Acc@1  88.28 ( 89.18)	Acc@5  99.22 ( 99.03)
Epoch: [87][230/391]	Time  0.043 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.6145e-01 (3.6403e-01)	Acc@1  85.94 ( 89.14)	Acc@5  98.44 ( 99.01)
Epoch: [87][240/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.9467e-01 (3.6415e-01)	Acc@1  88.28 ( 89.14)	Acc@5  99.22 ( 99.03)
Epoch: [87][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5365e-01 (3.6377e-01)	Acc@1  90.62 ( 89.17)	Acc@5  99.22 ( 99.02)
Epoch: [87][260/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.2805e-01 (3.6341e-01)	Acc@1  89.84 ( 89.19)	Acc@5  98.44 ( 99.01)
Epoch: [87][270/391]	Time  0.046 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.1736e-01 (3.6335e-01)	Acc@1  91.41 ( 89.20)	Acc@5  98.44 ( 99.01)
Epoch: [87][280/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 2.8981e-01 (3.6212e-01)	Acc@1  92.19 ( 89.21)	Acc@5 100.00 ( 99.03)
Epoch: [87][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1922e-01 (3.6258e-01)	Acc@1  92.19 ( 89.19)	Acc@5  97.66 ( 99.03)
Epoch: [87][300/391]	Time  0.043 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.4324e-01 (3.6269e-01)	Acc@1  82.03 ( 89.15)	Acc@5  99.22 ( 99.03)
Epoch: [87][310/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0784e-01 (3.6215e-01)	Acc@1  86.72 ( 89.17)	Acc@5  99.22 ( 99.04)
Epoch: [87][320/391]	Time  0.043 ( 0.041)	Data  0.002 ( 0.002)	Loss 3.2431e-01 (3.6175e-01)	Acc@1  90.62 ( 89.20)	Acc@5  99.22 ( 99.04)
Epoch: [87][330/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0059e-01 (3.6202e-01)	Acc@1  87.50 ( 89.18)	Acc@5  99.22 ( 99.04)
Epoch: [87][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.7615e-01 (3.6242e-01)	Acc@1  89.84 ( 89.17)	Acc@5  98.44 ( 99.04)
Epoch: [87][350/391]	Time  0.050 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4757e-01 (3.6307e-01)	Acc@1  90.62 ( 89.16)	Acc@5 100.00 ( 99.03)
Epoch: [87][360/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.4576e-01 (3.6301e-01)	Acc@1  94.53 ( 89.17)	Acc@5  98.44 ( 99.03)
Epoch: [87][370/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.5893e-01 (3.6281e-01)	Acc@1  89.06 ( 89.17)	Acc@5 100.00 ( 99.05)
Epoch: [87][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.6511e-01 (3.6349e-01)	Acc@1  84.38 ( 89.13)	Acc@5  97.66 ( 99.04)
Epoch: [87][390/391]	Time  0.031 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.7697e-01 (3.6312e-01)	Acc@1  88.75 ( 89.15)	Acc@5  97.50 ( 99.04)
## e[87] optimizer.zero_grad (sum) time: 0.2654743194580078
## e[87]       loss.backward (sum) time: 4.062540769577026
## e[87]      optimizer.step (sum) time: 1.8247098922729492
## epoch[87] training(only) time: 16.230359077453613
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2605e+00 (1.2605e+00)	Acc@1  68.00 ( 68.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.025 ( 0.034)	Loss 1.2427e+00 (1.3561e+00)	Acc@1  59.00 ( 66.91)	Acc@5  94.00 ( 91.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.0816e+00 (1.2815e+00)	Acc@1  75.00 ( 68.00)	Acc@5  93.00 ( 91.67)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4882e+00 (1.3044e+00)	Acc@1  61.00 ( 67.45)	Acc@5  89.00 ( 91.29)
Test: [ 40/100]	Time  0.020 ( 0.025)	Loss 1.2271e+00 (1.3177e+00)	Acc@1  72.00 ( 67.27)	Acc@5  93.00 ( 91.20)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.3191e+00 (1.3200e+00)	Acc@1  66.00 ( 67.14)	Acc@5  91.00 ( 90.86)
Test: [ 60/100]	Time  0.022 ( 0.024)	Loss 1.4549e+00 (1.2989e+00)	Acc@1  64.00 ( 67.46)	Acc@5  92.00 ( 91.07)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.4679e+00 (1.2913e+00)	Acc@1  64.00 ( 67.85)	Acc@5  89.00 ( 91.04)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.4192e+00 (1.2935e+00)	Acc@1  67.00 ( 67.90)	Acc@5  89.00 ( 90.94)
Test: [ 90/100]	Time  0.024 ( 0.023)	Loss 1.4741e+00 (1.2781e+00)	Acc@1  70.00 ( 68.36)	Acc@5  90.00 ( 91.11)
 * Acc@1 68.530 Acc@5 91.290
### epoch[87] execution time: 18.582080125808716
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.189 ( 0.189)	Data  0.129 ( 0.129)	Loss 3.3196e-01 (3.3196e-01)	Acc@1  91.41 ( 91.41)	Acc@5  98.44 ( 98.44)
Epoch: [88][ 10/391]	Time  0.044 ( 0.056)	Data  0.001 ( 0.013)	Loss 3.6026e-01 (3.7538e-01)	Acc@1  86.72 ( 87.93)	Acc@5  99.22 ( 99.15)
Epoch: [88][ 20/391]	Time  0.039 ( 0.049)	Data  0.001 ( 0.007)	Loss 3.1495e-01 (3.7299e-01)	Acc@1  92.19 ( 88.84)	Acc@5 100.00 ( 99.07)
Epoch: [88][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.005)	Loss 3.7341e-01 (3.6184e-01)	Acc@1  88.28 ( 89.19)	Acc@5  98.44 ( 99.09)
Epoch: [88][ 40/391]	Time  0.042 ( 0.045)	Data  0.002 ( 0.004)	Loss 3.1920e-01 (3.6015e-01)	Acc@1  90.62 ( 89.21)	Acc@5 100.00 ( 99.12)
Epoch: [88][ 50/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.004)	Loss 3.3281e-01 (3.5915e-01)	Acc@1  88.28 ( 89.31)	Acc@5  99.22 ( 99.16)
Epoch: [88][ 60/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.9026e-01 (3.5979e-01)	Acc@1  92.19 ( 89.15)	Acc@5  98.44 ( 99.17)
Epoch: [88][ 70/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.8169e-01 (3.6106e-01)	Acc@1  94.53 ( 89.16)	Acc@5 100.00 ( 99.16)
Epoch: [88][ 80/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.2543e-01 (3.5954e-01)	Acc@1  87.50 ( 89.15)	Acc@5  99.22 ( 99.19)
Epoch: [88][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.4113e-01 (3.5779e-01)	Acc@1  92.19 ( 89.25)	Acc@5 100.00 ( 99.21)
Epoch: [88][100/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.3480e-01 (3.5664e-01)	Acc@1  90.62 ( 89.33)	Acc@5  99.22 ( 99.20)
Epoch: [88][110/391]	Time  0.046 ( 0.043)	Data  0.001 ( 0.002)	Loss 3.9841e-01 (3.5749e-01)	Acc@1  87.50 ( 89.32)	Acc@5  98.44 ( 99.18)
Epoch: [88][120/391]	Time  0.045 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.2153e-01 (3.5829e-01)	Acc@1  95.31 ( 89.32)	Acc@5 100.00 ( 99.16)
Epoch: [88][130/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.4885e-01 (3.5695e-01)	Acc@1  88.28 ( 89.32)	Acc@5  98.44 ( 99.15)
Epoch: [88][140/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1995e-01 (3.5816e-01)	Acc@1  90.62 ( 89.32)	Acc@5  97.66 ( 99.12)
Epoch: [88][150/391]	Time  0.040 ( 0.042)	Data  0.002 ( 0.002)	Loss 4.1841e-01 (3.5990e-01)	Acc@1  87.50 ( 89.24)	Acc@5  97.66 ( 99.09)
Epoch: [88][160/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7596e-01 (3.5922e-01)	Acc@1  88.28 ( 89.25)	Acc@5 100.00 ( 99.08)
Epoch: [88][170/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.6559e-01 (3.5824e-01)	Acc@1  84.38 ( 89.23)	Acc@5  98.44 ( 99.10)
Epoch: [88][180/391]	Time  0.046 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6538e-01 (3.5833e-01)	Acc@1  87.50 ( 89.23)	Acc@5  99.22 ( 99.12)
Epoch: [88][190/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.7601e-01 (3.6017e-01)	Acc@1  80.47 ( 89.14)	Acc@5  99.22 ( 99.10)
Epoch: [88][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8703e-01 (3.5959e-01)	Acc@1  89.84 ( 89.22)	Acc@5  99.22 ( 99.09)
Epoch: [88][210/391]	Time  0.043 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8840e-01 (3.6138e-01)	Acc@1  89.84 ( 89.20)	Acc@5 100.00 ( 99.07)
Epoch: [88][220/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5246e-01 (3.6059e-01)	Acc@1  91.41 ( 89.19)	Acc@5 100.00 ( 99.08)
Epoch: [88][230/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8011e-01 (3.6066e-01)	Acc@1  93.75 ( 89.16)	Acc@5 100.00 ( 99.09)
Epoch: [88][240/391]	Time  0.044 ( 0.042)	Data  0.002 ( 0.002)	Loss 2.5356e-01 (3.5941e-01)	Acc@1  93.75 ( 89.22)	Acc@5  99.22 ( 99.10)
Epoch: [88][250/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.7053e-01 (3.5943e-01)	Acc@1  85.94 ( 89.21)	Acc@5  98.44 ( 99.11)
Epoch: [88][260/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.6602e-01 (3.5893e-01)	Acc@1  89.06 ( 89.23)	Acc@5  98.44 ( 99.10)
Epoch: [88][270/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.2305e-01 (3.5915e-01)	Acc@1  93.75 ( 89.23)	Acc@5 100.00 ( 99.11)
Epoch: [88][280/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.3964e-01 (3.5904e-01)	Acc@1  88.28 ( 89.23)	Acc@5  99.22 ( 99.11)
Epoch: [88][290/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.0202e-01 (3.5882e-01)	Acc@1  91.41 ( 89.25)	Acc@5  99.22 ( 99.11)
Epoch: [88][300/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.4222e-01 (3.5883e-01)	Acc@1  89.84 ( 89.24)	Acc@5  99.22 ( 99.10)
Epoch: [88][310/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.4470e-01 (3.5874e-01)	Acc@1  90.62 ( 89.22)	Acc@5  99.22 ( 99.10)
Epoch: [88][320/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.6548e-01 (3.5828e-01)	Acc@1  89.06 ( 89.24)	Acc@5  98.44 ( 99.10)
Epoch: [88][330/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.1274e-01 (3.5824e-01)	Acc@1  92.97 ( 89.24)	Acc@5  99.22 ( 99.11)
Epoch: [88][340/391]	Time  0.047 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.1034e-01 (3.5814e-01)	Acc@1  88.28 ( 89.25)	Acc@5  98.44 ( 99.11)
Epoch: [88][350/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.3369e-01 (3.5816e-01)	Acc@1  89.84 ( 89.26)	Acc@5  98.44 ( 99.10)
Epoch: [88][360/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.001)	Loss 4.9483e-01 (3.5852e-01)	Acc@1  84.38 ( 89.25)	Acc@5  96.09 ( 99.09)
Epoch: [88][370/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.001)	Loss 3.0893e-01 (3.5943e-01)	Acc@1  91.41 ( 89.20)	Acc@5 100.00 ( 99.09)
Epoch: [88][380/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.001)	Loss 2.9034e-01 (3.6009e-01)	Acc@1  92.97 ( 89.19)	Acc@5  99.22 ( 99.07)
Epoch: [88][390/391]	Time  0.029 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9430e-01 (3.6000e-01)	Acc@1  88.75 ( 89.21)	Acc@5 100.00 ( 99.07)
## e[88] optimizer.zero_grad (sum) time: 0.26520323753356934
## e[88]       loss.backward (sum) time: 4.095412969589233
## e[88]      optimizer.step (sum) time: 1.8262534141540527
## epoch[88] training(only) time: 16.264243841171265
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.2379e+00 (1.2379e+00)	Acc@1  67.00 ( 67.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.018 ( 0.034)	Loss 1.2646e+00 (1.3684e+00)	Acc@1  61.00 ( 66.91)	Acc@5  93.00 ( 90.18)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.0786e+00 (1.2954e+00)	Acc@1  74.00 ( 68.00)	Acc@5  92.00 ( 91.10)
Test: [ 30/100]	Time  0.024 ( 0.026)	Loss 1.4762e+00 (1.3181e+00)	Acc@1  62.00 ( 67.61)	Acc@5  89.00 ( 90.84)
Test: [ 40/100]	Time  0.019 ( 0.025)	Loss 1.2653e+00 (1.3302e+00)	Acc@1  71.00 ( 67.39)	Acc@5  93.00 ( 90.88)
Test: [ 50/100]	Time  0.021 ( 0.024)	Loss 1.2969e+00 (1.3326e+00)	Acc@1  70.00 ( 67.25)	Acc@5  91.00 ( 90.57)
Test: [ 60/100]	Time  0.024 ( 0.023)	Loss 1.4749e+00 (1.3104e+00)	Acc@1  64.00 ( 67.41)	Acc@5  90.00 ( 90.82)
Test: [ 70/100]	Time  0.022 ( 0.023)	Loss 1.4124e+00 (1.3027e+00)	Acc@1  68.00 ( 67.79)	Acc@5  90.00 ( 90.82)
Test: [ 80/100]	Time  0.022 ( 0.023)	Loss 1.3915e+00 (1.3052e+00)	Acc@1  71.00 ( 67.77)	Acc@5  91.00 ( 90.78)
Test: [ 90/100]	Time  0.020 ( 0.023)	Loss 1.5191e+00 (1.2902e+00)	Acc@1  68.00 ( 68.18)	Acc@5  90.00 ( 91.00)
 * Acc@1 68.330 Acc@5 91.170
### epoch[88] execution time: 18.571405172348022
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.204 ( 0.204)	Data  0.146 ( 0.146)	Loss 4.4940e-01 (4.4940e-01)	Acc@1  85.94 ( 85.94)	Acc@5  96.88 ( 96.88)
Epoch: [89][ 10/391]	Time  0.041 ( 0.056)	Data  0.001 ( 0.014)	Loss 2.7671e-01 (3.4544e-01)	Acc@1  92.19 ( 89.35)	Acc@5  99.22 ( 98.93)
Epoch: [89][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.008)	Loss 4.0585e-01 (3.4879e-01)	Acc@1  89.84 ( 89.47)	Acc@5  99.22 ( 98.96)
Epoch: [89][ 30/391]	Time  0.041 ( 0.046)	Data  0.001 ( 0.006)	Loss 3.5319e-01 (3.4469e-01)	Acc@1  88.28 ( 89.49)	Acc@5  99.22 ( 99.07)
Epoch: [89][ 40/391]	Time  0.039 ( 0.045)	Data  0.001 ( 0.005)	Loss 3.3120e-01 (3.5212e-01)	Acc@1  86.72 ( 89.14)	Acc@5 100.00 ( 99.10)
Epoch: [89][ 50/391]	Time  0.040 ( 0.044)	Data  0.002 ( 0.004)	Loss 3.5043e-01 (3.5268e-01)	Acc@1  89.84 ( 89.03)	Acc@5  98.44 ( 99.10)
Epoch: [89][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.0310e-01 (3.5392e-01)	Acc@1  88.28 ( 89.11)	Acc@5  99.22 ( 99.14)
Epoch: [89][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.3490e-01 (3.5936e-01)	Acc@1  86.72 ( 88.97)	Acc@5  98.44 ( 99.08)
Epoch: [89][ 80/391]	Time  0.045 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.4584e-01 (3.5615e-01)	Acc@1  92.19 ( 89.13)	Acc@5 100.00 ( 99.11)
Epoch: [89][ 90/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.2071e-01 (3.5652e-01)	Acc@1  92.19 ( 89.08)	Acc@5  98.44 ( 99.15)
Epoch: [89][100/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.5003e-01 (3.5483e-01)	Acc@1  95.31 ( 89.21)	Acc@5 100.00 ( 99.15)
Epoch: [89][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.8197e-01 (3.5474e-01)	Acc@1  93.75 ( 89.30)	Acc@5  99.22 ( 99.15)
Epoch: [89][120/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8715e-01 (3.5489e-01)	Acc@1  88.28 ( 89.28)	Acc@5  97.66 ( 99.15)
Epoch: [89][130/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.0554e-01 (3.5376e-01)	Acc@1  92.97 ( 89.32)	Acc@5  99.22 ( 99.17)
Epoch: [89][140/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.1959e-01 (3.5535e-01)	Acc@1  87.50 ( 89.30)	Acc@5 100.00 ( 99.15)
Epoch: [89][150/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 2.7861e-01 (3.5506e-01)	Acc@1  91.41 ( 89.32)	Acc@5 100.00 ( 99.14)
Epoch: [89][160/391]	Time  0.044 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.1663e-01 (3.5509e-01)	Acc@1  91.41 ( 89.37)	Acc@5  99.22 ( 99.15)
Epoch: [89][170/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.0664e-01 (3.5558e-01)	Acc@1  87.50 ( 89.43)	Acc@5  99.22 ( 99.14)
Epoch: [89][180/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2561e-01 (3.5597e-01)	Acc@1  90.62 ( 89.46)	Acc@5  97.66 ( 99.12)
Epoch: [89][190/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.4650e-01 (3.5625e-01)	Acc@1  89.84 ( 89.46)	Acc@5  97.66 ( 99.10)
Epoch: [89][200/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.1363e-01 (3.5472e-01)	Acc@1  89.84 ( 89.49)	Acc@5  99.22 ( 99.10)
Epoch: [89][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.0778e-01 (3.5589e-01)	Acc@1  89.84 ( 89.46)	Acc@5  98.44 ( 99.09)
Epoch: [89][220/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.5122e-01 (3.5840e-01)	Acc@1  87.50 ( 89.32)	Acc@5  99.22 ( 99.05)
Epoch: [89][230/391]	Time  0.042 ( 0.042)	Data  0.001 ( 0.002)	Loss 3.8907e-01 (3.5759e-01)	Acc@1  91.41 ( 89.38)	Acc@5  99.22 ( 99.07)
Epoch: [89][240/391]	Time  0.041 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.3979e-01 (3.5793e-01)	Acc@1  86.72 ( 89.36)	Acc@5 100.00 ( 99.08)
Epoch: [89][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.8480e-01 (3.5901e-01)	Acc@1  89.06 ( 89.31)	Acc@5  99.22 ( 99.08)
Epoch: [89][260/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.6141e-01 (3.5788e-01)	Acc@1  90.62 ( 89.36)	Acc@5  99.22 ( 99.10)
Epoch: [89][270/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1937e-01 (3.5939e-01)	Acc@1  86.72 ( 89.29)	Acc@5 100.00 ( 99.08)
Epoch: [89][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.4166e-01 (3.5740e-01)	Acc@1  86.72 ( 89.35)	Acc@5  99.22 ( 99.10)
Epoch: [89][290/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.002)	Loss 3.3551e-01 (3.5705e-01)	Acc@1  92.19 ( 89.37)	Acc@5  98.44 ( 99.10)
Epoch: [89][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.0734e-01 (3.5730e-01)	Acc@1  89.06 ( 89.34)	Acc@5 100.00 ( 99.09)
Epoch: [89][310/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1200e-01 (3.5722e-01)	Acc@1  92.97 ( 89.39)	Acc@5  98.44 ( 99.07)
Epoch: [89][320/391]	Time  0.044 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.1837e-01 (3.5770e-01)	Acc@1  92.19 ( 89.41)	Acc@5  99.22 ( 99.06)
Epoch: [89][330/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.6910e-01 (3.5732e-01)	Acc@1  88.28 ( 89.42)	Acc@5  99.22 ( 99.06)
Epoch: [89][340/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.4436e-01 (3.5725e-01)	Acc@1  89.84 ( 89.38)	Acc@5 100.00 ( 99.07)
Epoch: [89][350/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 3.9567e-01 (3.5846e-01)	Acc@1  87.50 ( 89.34)	Acc@5  98.44 ( 99.06)
Epoch: [89][360/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 2.8568e-01 (3.5824e-01)	Acc@1  92.19 ( 89.37)	Acc@5  99.22 ( 99.07)
Epoch: [89][370/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8431e-01 (3.5774e-01)	Acc@1  87.50 ( 89.38)	Acc@5  98.44 ( 99.07)
Epoch: [89][380/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.001)	Loss 4.8599e-01 (3.5788e-01)	Acc@1  82.81 ( 89.37)	Acc@5  98.44 ( 99.08)
Epoch: [89][390/391]	Time  0.030 ( 0.041)	Data  0.001 ( 0.001)	Loss 5.4578e-01 (3.5787e-01)	Acc@1  81.25 ( 89.36)	Acc@5  97.50 ( 99.08)
## e[89] optimizer.zero_grad (sum) time: 0.2656564712524414
## e[89]       loss.backward (sum) time: 4.074209690093994
## e[89]      optimizer.step (sum) time: 1.8353276252746582
## epoch[89] training(only) time: 16.18308925628662
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.2830e+00 (1.2830e+00)	Acc@1  68.00 ( 68.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.2635e+00 (1.3709e+00)	Acc@1  59.00 ( 66.82)	Acc@5  93.00 ( 90.27)
Test: [ 20/100]	Time  0.021 ( 0.029)	Loss 1.1047e+00 (1.2952e+00)	Acc@1  73.00 ( 68.33)	Acc@5  92.00 ( 91.24)
Test: [ 30/100]	Time  0.021 ( 0.027)	Loss 1.4973e+00 (1.3182e+00)	Acc@1  62.00 ( 67.84)	Acc@5  89.00 ( 91.03)
Test: [ 40/100]	Time  0.016 ( 0.025)	Loss 1.2396e+00 (1.3309e+00)	Acc@1  72.00 ( 67.56)	Acc@5  92.00 ( 91.00)
Test: [ 50/100]	Time  0.022 ( 0.024)	Loss 1.2676e+00 (1.3327e+00)	Acc@1  68.00 ( 67.43)	Acc@5  91.00 ( 90.67)
Test: [ 60/100]	Time  0.018 ( 0.023)	Loss 1.4885e+00 (1.3119e+00)	Acc@1  62.00 ( 67.62)	Acc@5  92.00 ( 90.93)
Test: [ 70/100]	Time  0.018 ( 0.023)	Loss 1.4202e+00 (1.3040e+00)	Acc@1  65.00 ( 67.93)	Acc@5  90.00 ( 90.89)
Test: [ 80/100]	Time  0.018 ( 0.023)	Loss 1.4081e+00 (1.3056e+00)	Acc@1  70.00 ( 67.85)	Acc@5  91.00 ( 90.81)
Test: [ 90/100]	Time  0.022 ( 0.022)	Loss 1.5106e+00 (1.2903e+00)	Acc@1  68.00 ( 68.25)	Acc@5  90.00 ( 91.02)
 * Acc@1 68.370 Acc@5 91.190
### epoch[89] execution time: 18.505159854888916
### Training complete:
#### total training(only) time: 1456.5170273780823
##### Total run time: 1670.9931614398956
