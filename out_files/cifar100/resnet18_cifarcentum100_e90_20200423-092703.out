# Model: resnet18
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.resnet
<function resnet18 at 0x7ffa7d7cdf28>
# model requested: 'resnet18'
# printing out the model
ResNet(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (conv2_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv3_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv4_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv5_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=100, bias=True)
)
# model is full precision
# Model: resnet18
# Dataset: cifarcentum
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.424 ( 3.424)	Data  0.106 ( 0.106)	Loss 4.7525e+00 (4.7525e+00)	Acc@1   0.00 (  0.00)	Acc@5   3.12 (  3.12)
Epoch: [0][ 10/391]	Time  0.045 ( 0.354)	Data  0.001 ( 0.011)	Loss 5.1425e+00 (5.0395e+00)	Acc@1   1.56 (  1.99)	Acc@5   6.25 (  6.32)
Epoch: [0][ 20/391]	Time  0.046 ( 0.207)	Data  0.001 ( 0.007)	Loss 4.5244e+00 (4.9578e+00)	Acc@1   1.56 (  2.27)	Acc@5  13.28 (  8.22)
Epoch: [0][ 30/391]	Time  0.046 ( 0.155)	Data  0.001 ( 0.005)	Loss 4.5832e+00 (4.8768e+00)	Acc@1   4.69 (  2.75)	Acc@5  14.84 (  9.93)
Epoch: [0][ 40/391]	Time  0.047 ( 0.129)	Data  0.001 ( 0.005)	Loss 4.3832e+00 (4.7823e+00)	Acc@1   0.00 (  2.71)	Acc@5  12.50 ( 10.77)
Epoch: [0][ 50/391]	Time  0.046 ( 0.113)	Data  0.001 ( 0.004)	Loss 4.3970e+00 (4.6816e+00)	Acc@1   5.47 (  3.19)	Acc@5  11.72 ( 12.27)
Epoch: [0][ 60/391]	Time  0.045 ( 0.102)	Data  0.001 ( 0.004)	Loss 4.2193e+00 (4.6059e+00)	Acc@1   5.47 (  3.65)	Acc@5  17.97 ( 13.43)
Epoch: [0][ 70/391]	Time  0.046 ( 0.094)	Data  0.001 ( 0.004)	Loss 4.1931e+00 (4.5552e+00)	Acc@1   1.56 (  3.70)	Acc@5  15.62 ( 14.25)
Epoch: [0][ 80/391]	Time  0.046 ( 0.088)	Data  0.001 ( 0.004)	Loss 4.2773e+00 (4.5067e+00)	Acc@1   5.47 (  4.05)	Acc@5  18.75 ( 15.24)
Epoch: [0][ 90/391]	Time  0.046 ( 0.084)	Data  0.001 ( 0.004)	Loss 4.3174e+00 (4.4626e+00)	Acc@1   5.47 (  4.25)	Acc@5  13.28 ( 16.15)
Epoch: [0][100/391]	Time  0.046 ( 0.080)	Data  0.001 ( 0.003)	Loss 4.0121e+00 (4.4297e+00)	Acc@1   7.03 (  4.36)	Acc@5  28.12 ( 16.76)
Epoch: [0][110/391]	Time  0.046 ( 0.077)	Data  0.001 ( 0.003)	Loss 3.8914e+00 (4.3961e+00)	Acc@1  10.94 (  4.59)	Acc@5  35.16 ( 17.63)
Epoch: [0][120/391]	Time  0.046 ( 0.074)	Data  0.001 ( 0.003)	Loss 3.9148e+00 (4.3650e+00)	Acc@1   9.38 (  4.81)	Acc@5  32.81 ( 18.45)
Epoch: [0][130/391]	Time  0.046 ( 0.072)	Data  0.001 ( 0.003)	Loss 3.8048e+00 (4.3381e+00)	Acc@1  10.16 (  5.03)	Acc@5  24.22 ( 19.02)
Epoch: [0][140/391]	Time  0.046 ( 0.070)	Data  0.001 ( 0.003)	Loss 4.0168e+00 (4.3130e+00)	Acc@1   5.47 (  5.18)	Acc@5  21.88 ( 19.53)
Epoch: [0][150/391]	Time  0.046 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.9555e+00 (4.2933e+00)	Acc@1   5.47 (  5.33)	Acc@5  24.22 ( 19.80)
Epoch: [0][160/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9061e+00 (4.2743e+00)	Acc@1   7.81 (  5.45)	Acc@5  27.34 ( 20.14)
Epoch: [0][170/391]	Time  0.046 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.9882e+00 (4.2540e+00)	Acc@1   3.91 (  5.60)	Acc@5  26.56 ( 20.60)
Epoch: [0][180/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.9442e+00 (4.2383e+00)	Acc@1   7.81 (  5.74)	Acc@5  28.91 ( 20.95)
Epoch: [0][190/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.003)	Loss 3.6780e+00 (4.2206e+00)	Acc@1  10.16 (  5.94)	Acc@5  36.72 ( 21.46)
Epoch: [0][200/391]	Time  0.046 ( 0.063)	Data  0.001 ( 0.003)	Loss 3.8804e+00 (4.2079e+00)	Acc@1   7.03 (  6.04)	Acc@5  27.34 ( 21.73)
Epoch: [0][210/391]	Time  0.046 ( 0.062)	Data  0.001 ( 0.003)	Loss 4.1115e+00 (4.1975e+00)	Acc@1   4.69 (  6.11)	Acc@5  21.88 ( 21.95)
Epoch: [0][220/391]	Time  0.046 ( 0.062)	Data  0.001 ( 0.003)	Loss 3.8153e+00 (4.1848e+00)	Acc@1  12.50 (  6.24)	Acc@5  36.72 ( 22.27)
Epoch: [0][230/391]	Time  0.046 ( 0.061)	Data  0.001 ( 0.003)	Loss 3.8733e+00 (4.1716e+00)	Acc@1   9.38 (  6.37)	Acc@5  28.91 ( 22.59)
Epoch: [0][240/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.003)	Loss 3.7333e+00 (4.1589e+00)	Acc@1  11.72 (  6.49)	Acc@5  36.72 ( 22.91)
Epoch: [0][250/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.003)	Loss 3.8220e+00 (4.1480e+00)	Acc@1  11.72 (  6.61)	Acc@5  26.56 ( 23.22)
Epoch: [0][260/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.003)	Loss 3.8490e+00 (4.1360e+00)	Acc@1  10.94 (  6.73)	Acc@5  29.69 ( 23.54)
Epoch: [0][270/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.003)	Loss 3.6247e+00 (4.1227e+00)	Acc@1  14.06 (  6.86)	Acc@5  42.19 ( 23.86)
Epoch: [0][280/391]	Time  0.046 ( 0.058)	Data  0.001 ( 0.003)	Loss 3.7342e+00 (4.1129e+00)	Acc@1   7.03 (  6.95)	Acc@5  35.16 ( 24.10)
Epoch: [0][290/391]	Time  0.044 ( 0.058)	Data  0.001 ( 0.003)	Loss 3.9665e+00 (4.1011e+00)	Acc@1  10.94 (  7.09)	Acc@5  26.56 ( 24.42)
Epoch: [0][300/391]	Time  0.046 ( 0.058)	Data  0.001 ( 0.003)	Loss 3.8430e+00 (4.0927e+00)	Acc@1  11.72 (  7.23)	Acc@5  32.81 ( 24.68)
Epoch: [0][310/391]	Time  0.047 ( 0.057)	Data  0.001 ( 0.003)	Loss 3.6423e+00 (4.0829e+00)	Acc@1  12.50 (  7.34)	Acc@5  35.16 ( 24.95)
Epoch: [0][320/391]	Time  0.046 ( 0.057)	Data  0.001 ( 0.003)	Loss 3.6176e+00 (4.0704e+00)	Acc@1  14.84 (  7.50)	Acc@5  39.06 ( 25.30)
Epoch: [0][330/391]	Time  0.046 ( 0.057)	Data  0.001 ( 0.003)	Loss 3.8195e+00 (4.0610e+00)	Acc@1   9.38 (  7.62)	Acc@5  28.91 ( 25.58)
Epoch: [0][340/391]	Time  0.046 ( 0.056)	Data  0.001 ( 0.003)	Loss 3.6378e+00 (4.0516e+00)	Acc@1  13.28 (  7.75)	Acc@5  33.59 ( 25.85)
Epoch: [0][350/391]	Time  0.047 ( 0.056)	Data  0.001 ( 0.003)	Loss 3.5574e+00 (4.0424e+00)	Acc@1  17.19 (  7.89)	Acc@5  44.53 ( 26.10)
Epoch: [0][360/391]	Time  0.046 ( 0.056)	Data  0.001 ( 0.003)	Loss 3.7436e+00 (4.0334e+00)	Acc@1  11.72 (  8.01)	Acc@5  39.84 ( 26.40)
Epoch: [0][370/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.003)	Loss 3.7100e+00 (4.0264e+00)	Acc@1  16.41 (  8.10)	Acc@5  42.19 ( 26.58)
Epoch: [0][380/391]	Time  0.047 ( 0.055)	Data  0.001 ( 0.003)	Loss 3.5223e+00 (4.0179e+00)	Acc@1  11.72 (  8.18)	Acc@5  41.41 ( 26.83)
Epoch: [0][390/391]	Time  0.312 ( 0.056)	Data  0.001 ( 0.003)	Loss 3.4671e+00 (4.0081e+00)	Acc@1  15.00 (  8.33)	Acc@5  40.00 ( 27.13)
## e[0] optimizer.zero_grad (sum) time: 0.13350582122802734
## e[0]       loss.backward (sum) time: 2.8749489784240723
## e[0]      optimizer.step (sum) time: 1.005674123764038
## epoch[0] training(only) time: 21.868407487869263
# Switched to evaluate mode...
Test: [  0/100]	Time  0.260 ( 0.260)	Loss 3.8397e+00 (3.8397e+00)	Acc@1  13.00 ( 13.00)	Acc@5  38.00 ( 38.00)
Test: [ 10/100]	Time  0.022 ( 0.045)	Loss 3.7982e+00 (3.6790e+00)	Acc@1  10.00 ( 12.91)	Acc@5  32.00 ( 36.91)
Test: [ 20/100]	Time  0.023 ( 0.034)	Loss 3.5401e+00 (3.6411e+00)	Acc@1  14.00 ( 13.05)	Acc@5  44.00 ( 39.05)
Test: [ 30/100]	Time  0.022 ( 0.031)	Loss 3.7711e+00 (3.6370e+00)	Acc@1  12.00 ( 12.94)	Acc@5  35.00 ( 38.81)
Test: [ 40/100]	Time  0.023 ( 0.029)	Loss 3.7065e+00 (3.6257e+00)	Acc@1   9.00 ( 12.98)	Acc@5  32.00 ( 38.73)
Test: [ 50/100]	Time  0.022 ( 0.027)	Loss 3.4665e+00 (3.6145e+00)	Acc@1  20.00 ( 12.96)	Acc@5  47.00 ( 38.94)
Test: [ 60/100]	Time  0.022 ( 0.027)	Loss 3.5423e+00 (3.6078e+00)	Acc@1  15.00 ( 12.89)	Acc@5  42.00 ( 38.92)
Test: [ 70/100]	Time  0.023 ( 0.026)	Loss 3.6954e+00 (3.6098e+00)	Acc@1  10.00 ( 12.89)	Acc@5  40.00 ( 38.86)
Test: [ 80/100]	Time  0.023 ( 0.026)	Loss 3.7962e+00 (3.6235e+00)	Acc@1   7.00 ( 12.63)	Acc@5  36.00 ( 38.67)
Test: [ 90/100]	Time  0.022 ( 0.025)	Loss 3.6047e+00 (3.6209e+00)	Acc@1  11.00 ( 12.59)	Acc@5  40.00 ( 38.87)
 * Acc@1 12.660 Acc@5 38.950
### epoch[0] execution time: 24.472488164901733
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.195 ( 0.195)	Data  0.148 ( 0.148)	Loss 3.7742e+00 (3.7742e+00)	Acc@1  14.06 ( 14.06)	Acc@5  35.94 ( 35.94)
Epoch: [1][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 3.4684e+00 (3.6575e+00)	Acc@1  16.41 ( 13.00)	Acc@5  46.88 ( 38.57)
Epoch: [1][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 3.7844e+00 (3.6432e+00)	Acc@1  10.94 ( 13.58)	Acc@5  36.72 ( 38.50)
Epoch: [1][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.5669e+00 (3.6078e+00)	Acc@1  10.16 ( 13.58)	Acc@5  39.84 ( 39.21)
Epoch: [1][ 40/391]	Time  0.051 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.5792e+00 (3.6026e+00)	Acc@1  22.66 ( 13.62)	Acc@5  42.19 ( 39.21)
Epoch: [1][ 50/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.5784e+00 (3.6023e+00)	Acc@1  16.41 ( 13.86)	Acc@5  40.62 ( 39.48)
Epoch: [1][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.6545e+00 (3.6029e+00)	Acc@1  13.28 ( 13.93)	Acc@5  34.38 ( 39.20)
Epoch: [1][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.6346e+00 (3.5995e+00)	Acc@1  14.84 ( 13.90)	Acc@5  38.28 ( 39.40)
Epoch: [1][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5534e+00 (3.5947e+00)	Acc@1   9.38 ( 13.87)	Acc@5  35.16 ( 39.41)
Epoch: [1][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4717e+00 (3.5862e+00)	Acc@1  11.72 ( 14.01)	Acc@5  39.06 ( 39.68)
Epoch: [1][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3960e+00 (3.5746e+00)	Acc@1  18.75 ( 14.26)	Acc@5  42.19 ( 40.02)
Epoch: [1][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5247e+00 (3.5667e+00)	Acc@1  21.09 ( 14.43)	Acc@5  45.31 ( 40.18)
Epoch: [1][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3624e+00 (3.5579e+00)	Acc@1  21.88 ( 14.59)	Acc@5  42.19 ( 40.50)
Epoch: [1][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5450e+00 (3.5483e+00)	Acc@1  14.06 ( 14.75)	Acc@5  39.84 ( 40.74)
Epoch: [1][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2287e+00 (3.5415e+00)	Acc@1  17.97 ( 14.96)	Acc@5  50.00 ( 40.99)
Epoch: [1][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4141e+00 (3.5337e+00)	Acc@1  17.19 ( 15.06)	Acc@5  42.97 ( 41.16)
Epoch: [1][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5073e+00 (3.5316e+00)	Acc@1  10.94 ( 15.08)	Acc@5  38.28 ( 41.09)
Epoch: [1][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5258e+00 (3.5264e+00)	Acc@1  18.75 ( 15.20)	Acc@5  43.75 ( 41.15)
Epoch: [1][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3021e+00 (3.5192e+00)	Acc@1  23.44 ( 15.34)	Acc@5  48.44 ( 41.33)
Epoch: [1][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4018e+00 (3.5121e+00)	Acc@1  14.06 ( 15.43)	Acc@5  45.31 ( 41.57)
Epoch: [1][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3110e+00 (3.5066e+00)	Acc@1  21.88 ( 15.68)	Acc@5  45.31 ( 41.69)
Epoch: [1][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3726e+00 (3.5011e+00)	Acc@1  16.41 ( 15.75)	Acc@5  44.53 ( 41.85)
Epoch: [1][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4514e+00 (3.4953e+00)	Acc@1  19.53 ( 15.84)	Acc@5  46.09 ( 42.04)
Epoch: [1][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3172e+00 (3.4929e+00)	Acc@1  18.75 ( 15.88)	Acc@5  46.88 ( 42.09)
Epoch: [1][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5231e+00 (3.4864e+00)	Acc@1  12.50 ( 16.00)	Acc@5  42.19 ( 42.25)
Epoch: [1][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2462e+00 (3.4782e+00)	Acc@1  19.53 ( 16.14)	Acc@5  50.00 ( 42.48)
Epoch: [1][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2072e+00 (3.4762e+00)	Acc@1  17.19 ( 16.14)	Acc@5  54.69 ( 42.53)
Epoch: [1][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9960e+00 (3.4664e+00)	Acc@1  27.34 ( 16.35)	Acc@5  57.03 ( 42.82)
Epoch: [1][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3783e+00 (3.4606e+00)	Acc@1  21.09 ( 16.47)	Acc@5  50.00 ( 42.99)
Epoch: [1][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2625e+00 (3.4565e+00)	Acc@1  22.66 ( 16.57)	Acc@5  49.22 ( 43.10)
Epoch: [1][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2569e+00 (3.4515e+00)	Acc@1  18.75 ( 16.70)	Acc@5  46.88 ( 43.28)
Epoch: [1][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0914e+00 (3.4451e+00)	Acc@1  28.91 ( 16.84)	Acc@5  55.47 ( 43.51)
Epoch: [1][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2942e+00 (3.4394e+00)	Acc@1  21.88 ( 16.94)	Acc@5  44.53 ( 43.65)
Epoch: [1][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1712e+00 (3.4363e+00)	Acc@1  20.31 ( 17.03)	Acc@5  53.91 ( 43.74)
Epoch: [1][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3141e+00 (3.4316e+00)	Acc@1  19.53 ( 17.11)	Acc@5  45.31 ( 43.89)
Epoch: [1][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3220e+00 (3.4256e+00)	Acc@1  19.53 ( 17.22)	Acc@5  52.34 ( 44.10)
Epoch: [1][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2983e+00 (3.4199e+00)	Acc@1  15.62 ( 17.33)	Acc@5  47.66 ( 44.24)
Epoch: [1][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3532e+00 (3.4136e+00)	Acc@1  19.53 ( 17.43)	Acc@5  50.00 ( 44.47)
Epoch: [1][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0797e+00 (3.4079e+00)	Acc@1  24.22 ( 17.51)	Acc@5  56.25 ( 44.59)
Epoch: [1][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5915e+00 (3.4024e+00)	Acc@1  17.50 ( 17.63)	Acc@5  40.00 ( 44.73)
## e[1] optimizer.zero_grad (sum) time: 0.1312258243560791
## e[1]       loss.backward (sum) time: 2.511435031890869
## e[1]      optimizer.step (sum) time: 0.9978132247924805
## epoch[1] training(only) time: 18.502837419509888
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 3.5700e+00 (3.5700e+00)	Acc@1  17.00 ( 17.00)	Acc@5  39.00 ( 39.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 3.3128e+00 (3.2329e+00)	Acc@1  22.00 ( 21.64)	Acc@5  44.00 ( 50.27)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 3.1235e+00 (3.1874e+00)	Acc@1  19.00 ( 21.81)	Acc@5  55.00 ( 51.38)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 3.4502e+00 (3.1675e+00)	Acc@1  21.00 ( 21.90)	Acc@5  43.00 ( 51.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 3.1645e+00 (3.1527e+00)	Acc@1  21.00 ( 21.98)	Acc@5  60.00 ( 52.49)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 3.1478e+00 (3.1693e+00)	Acc@1  26.00 ( 21.86)	Acc@5  53.00 ( 52.29)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 2.8778e+00 (3.1643e+00)	Acc@1  24.00 ( 21.67)	Acc@5  62.00 ( 52.21)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 3.3336e+00 (3.1716e+00)	Acc@1  22.00 ( 21.55)	Acc@5  51.00 ( 52.10)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 3.2933e+00 (3.1804e+00)	Acc@1  19.00 ( 21.19)	Acc@5  49.00 ( 51.90)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 3.0361e+00 (3.1775e+00)	Acc@1  27.00 ( 21.43)	Acc@5  56.00 ( 51.88)
 * Acc@1 21.280 Acc@5 51.840
### epoch[1] execution time: 21.002970218658447
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.191 ( 0.191)	Data  0.150 ( 0.150)	Loss 3.2347e+00 (3.2347e+00)	Acc@1  23.44 ( 23.44)	Acc@5  46.09 ( 46.09)
Epoch: [2][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.016)	Loss 3.2259e+00 (3.2030e+00)	Acc@1  14.84 ( 21.52)	Acc@5  53.12 ( 51.28)
Epoch: [2][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 3.0537e+00 (3.1408e+00)	Acc@1  22.66 ( 22.81)	Acc@5  53.12 ( 53.01)
Epoch: [2][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.2413e+00 (3.1118e+00)	Acc@1  25.00 ( 23.11)	Acc@5  49.22 ( 53.00)
Epoch: [2][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.2192e+00 (3.1251e+00)	Acc@1  25.00 ( 22.92)	Acc@5  46.09 ( 52.90)
Epoch: [2][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.1653e+00 (3.1329e+00)	Acc@1  22.66 ( 22.75)	Acc@5  54.69 ( 52.71)
Epoch: [2][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.2194e+00 (3.1248e+00)	Acc@1  17.19 ( 23.00)	Acc@5  50.00 ( 52.98)
Epoch: [2][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.9533e+00 (3.1038e+00)	Acc@1  19.53 ( 23.17)	Acc@5  60.16 ( 53.44)
Epoch: [2][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.0841e+00 (3.0895e+00)	Acc@1  24.22 ( 23.40)	Acc@5  51.56 ( 53.55)
Epoch: [2][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0223e+00 (3.0880e+00)	Acc@1  22.66 ( 23.40)	Acc@5  47.66 ( 53.67)
Epoch: [2][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8901e+00 (3.0751e+00)	Acc@1  25.00 ( 23.55)	Acc@5  59.38 ( 54.10)
Epoch: [2][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2565e+00 (3.0779e+00)	Acc@1  18.75 ( 23.51)	Acc@5  44.53 ( 53.98)
Epoch: [2][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1406e+00 (3.0729e+00)	Acc@1  22.66 ( 23.57)	Acc@5  54.69 ( 54.03)
Epoch: [2][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2576e+00 (3.0706e+00)	Acc@1  20.31 ( 23.62)	Acc@5  46.88 ( 54.06)
Epoch: [2][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1654e+00 (3.0774e+00)	Acc@1  23.44 ( 23.48)	Acc@5  51.56 ( 53.81)
Epoch: [2][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.1570e+00 (3.0746e+00)	Acc@1  17.97 ( 23.50)	Acc@5  51.56 ( 53.83)
Epoch: [2][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.1160e+00 (3.0757e+00)	Acc@1  24.22 ( 23.43)	Acc@5  55.47 ( 53.74)
Epoch: [2][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9741e+00 (3.0753e+00)	Acc@1  22.66 ( 23.43)	Acc@5  56.25 ( 53.76)
Epoch: [2][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0272e+00 (3.0728e+00)	Acc@1  21.88 ( 23.39)	Acc@5  53.12 ( 53.87)
Epoch: [2][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0109e+00 (3.0700e+00)	Acc@1  24.22 ( 23.43)	Acc@5  51.56 ( 53.87)
Epoch: [2][200/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1233e+00 (3.0661e+00)	Acc@1  27.34 ( 23.53)	Acc@5  56.25 ( 53.96)
Epoch: [2][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9918e+00 (3.0639e+00)	Acc@1  28.12 ( 23.60)	Acc@5  54.69 ( 54.03)
Epoch: [2][220/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0139e+00 (3.0591e+00)	Acc@1  27.34 ( 23.72)	Acc@5  52.34 ( 54.04)
Epoch: [2][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7015e+00 (3.0523e+00)	Acc@1  28.91 ( 23.84)	Acc@5  64.84 ( 54.23)
Epoch: [2][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9561e+00 (3.0481e+00)	Acc@1  29.69 ( 23.96)	Acc@5  56.25 ( 54.34)
Epoch: [2][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1507e+00 (3.0434e+00)	Acc@1  21.09 ( 24.13)	Acc@5  54.69 ( 54.42)
Epoch: [2][260/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8825e+00 (3.0383e+00)	Acc@1  30.47 ( 24.23)	Acc@5  56.25 ( 54.56)
Epoch: [2][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8305e+00 (3.0324e+00)	Acc@1  28.91 ( 24.35)	Acc@5  57.81 ( 54.72)
Epoch: [2][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0004e+00 (3.0283e+00)	Acc@1  24.22 ( 24.44)	Acc@5  55.47 ( 54.87)
Epoch: [2][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7559e+00 (3.0205e+00)	Acc@1  28.12 ( 24.61)	Acc@5  56.25 ( 55.07)
Epoch: [2][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8667e+00 (3.0157e+00)	Acc@1  30.47 ( 24.71)	Acc@5  55.47 ( 55.18)
Epoch: [2][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6885e+00 (3.0099e+00)	Acc@1  29.69 ( 24.84)	Acc@5  64.84 ( 55.34)
Epoch: [2][320/391]	Time  0.053 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8854e+00 (3.0056e+00)	Acc@1  28.12 ( 24.92)	Acc@5  57.03 ( 55.49)
Epoch: [2][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6454e+00 (3.0026e+00)	Acc@1  25.78 ( 24.98)	Acc@5  60.94 ( 55.56)
Epoch: [2][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7619e+00 (2.9979e+00)	Acc@1  28.12 ( 25.13)	Acc@5  64.06 ( 55.70)
Epoch: [2][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7196e+00 (2.9927e+00)	Acc@1  31.25 ( 25.28)	Acc@5  62.50 ( 55.85)
Epoch: [2][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6691e+00 (2.9873e+00)	Acc@1  28.12 ( 25.39)	Acc@5  66.41 ( 55.99)
Epoch: [2][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8478e+00 (2.9824e+00)	Acc@1  23.44 ( 25.44)	Acc@5  57.03 ( 56.10)
Epoch: [2][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6021e+00 (2.9780e+00)	Acc@1  31.25 ( 25.48)	Acc@5  64.06 ( 56.17)
Epoch: [2][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8555e+00 (2.9735e+00)	Acc@1  30.00 ( 25.57)	Acc@5  57.50 ( 56.25)
## e[2] optimizer.zero_grad (sum) time: 0.12857389450073242
## e[2]       loss.backward (sum) time: 2.5234150886535645
## e[2]      optimizer.step (sum) time: 0.999485969543457
## epoch[2] training(only) time: 18.55618190765381
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 2.8402e+00 (2.8402e+00)	Acc@1  32.00 ( 32.00)	Acc@5  58.00 ( 58.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 3.0130e+00 (2.8423e+00)	Acc@1  21.00 ( 28.91)	Acc@5  54.00 ( 58.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 2.7757e+00 (2.8408e+00)	Acc@1  30.00 ( 28.86)	Acc@5  61.00 ( 58.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.9150e+00 (2.8185e+00)	Acc@1  28.00 ( 28.58)	Acc@5  50.00 ( 59.39)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 2.8172e+00 (2.8030e+00)	Acc@1  34.00 ( 29.24)	Acc@5  60.00 ( 59.39)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.6997e+00 (2.8072e+00)	Acc@1  33.00 ( 29.27)	Acc@5  62.00 ( 59.20)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 2.7275e+00 (2.8041e+00)	Acc@1  23.00 ( 28.97)	Acc@5  64.00 ( 59.48)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 2.8180e+00 (2.8069e+00)	Acc@1  31.00 ( 28.87)	Acc@5  64.00 ( 59.83)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 3.1450e+00 (2.8172e+00)	Acc@1  20.00 ( 28.70)	Acc@5  48.00 ( 59.72)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 2.8320e+00 (2.8070e+00)	Acc@1  32.00 ( 29.11)	Acc@5  60.00 ( 59.92)
 * Acc@1 29.050 Acc@5 60.040
### epoch[2] execution time: 21.037130117416382
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.186 ( 0.186)	Data  0.142 ( 0.142)	Loss 2.7958e+00 (2.7958e+00)	Acc@1  32.81 ( 32.81)	Acc@5  60.94 ( 60.94)
Epoch: [3][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.6868e+00 (2.7288e+00)	Acc@1  31.25 ( 30.75)	Acc@5  64.84 ( 63.21)
Epoch: [3][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 2.6666e+00 (2.7176e+00)	Acc@1  24.22 ( 29.87)	Acc@5  63.28 ( 63.02)
Epoch: [3][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.9518e+00 (2.7363e+00)	Acc@1  21.88 ( 29.26)	Acc@5  55.47 ( 62.42)
Epoch: [3][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.6916e+00 (2.7273e+00)	Acc@1  30.47 ( 29.44)	Acc@5  60.16 ( 63.00)
Epoch: [3][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.8710e+00 (2.7323e+00)	Acc@1  26.56 ( 29.32)	Acc@5  54.69 ( 62.78)
Epoch: [3][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.6163e+00 (2.7275e+00)	Acc@1  36.72 ( 29.56)	Acc@5  64.06 ( 62.85)
Epoch: [3][ 70/391]	Time  0.049 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.5156e+00 (2.7284e+00)	Acc@1  35.16 ( 29.50)	Acc@5  68.75 ( 62.62)
Epoch: [3][ 80/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9374e+00 (2.7274e+00)	Acc@1  30.47 ( 29.64)	Acc@5  60.94 ( 62.80)
Epoch: [3][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7036e+00 (2.7203e+00)	Acc@1  28.91 ( 29.79)	Acc@5  61.72 ( 62.89)
Epoch: [3][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9813e+00 (2.7163e+00)	Acc@1  24.22 ( 29.99)	Acc@5  54.69 ( 62.86)
Epoch: [3][110/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7317e+00 (2.7170e+00)	Acc@1  29.69 ( 30.17)	Acc@5  61.72 ( 62.75)
Epoch: [3][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4354e+00 (2.7018e+00)	Acc@1  37.50 ( 30.51)	Acc@5  69.53 ( 63.18)
Epoch: [3][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7899e+00 (2.6983e+00)	Acc@1  25.00 ( 30.53)	Acc@5  64.06 ( 63.32)
Epoch: [3][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6505e+00 (2.6992e+00)	Acc@1  31.25 ( 30.58)	Acc@5  65.62 ( 63.28)
Epoch: [3][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7827e+00 (2.6932e+00)	Acc@1  32.81 ( 30.71)	Acc@5  60.94 ( 63.30)
Epoch: [3][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6211e+00 (2.6988e+00)	Acc@1  33.59 ( 30.61)	Acc@5  66.41 ( 63.18)
Epoch: [3][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4738e+00 (2.6947e+00)	Acc@1  38.28 ( 30.67)	Acc@5  67.19 ( 63.21)
Epoch: [3][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6836e+00 (2.6884e+00)	Acc@1  31.25 ( 30.76)	Acc@5  62.50 ( 63.35)
Epoch: [3][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5834e+00 (2.6805e+00)	Acc@1  39.84 ( 30.93)	Acc@5  60.94 ( 63.54)
Epoch: [3][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4232e+00 (2.6727e+00)	Acc@1  39.84 ( 31.12)	Acc@5  71.09 ( 63.73)
Epoch: [3][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4383e+00 (2.6686e+00)	Acc@1  39.06 ( 31.24)	Acc@5  67.97 ( 63.86)
Epoch: [3][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7642e+00 (2.6661e+00)	Acc@1  20.31 ( 31.22)	Acc@5  62.50 ( 64.01)
Epoch: [3][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6511e+00 (2.6605e+00)	Acc@1  25.78 ( 31.33)	Acc@5  64.06 ( 64.11)
Epoch: [3][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6750e+00 (2.6599e+00)	Acc@1  37.50 ( 31.42)	Acc@5  60.94 ( 64.10)
Epoch: [3][250/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5805e+00 (2.6561e+00)	Acc@1  31.25 ( 31.47)	Acc@5  63.28 ( 64.13)
Epoch: [3][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4143e+00 (2.6515e+00)	Acc@1  35.16 ( 31.59)	Acc@5  67.97 ( 64.24)
Epoch: [3][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4723e+00 (2.6456e+00)	Acc@1  41.41 ( 31.68)	Acc@5  69.53 ( 64.43)
Epoch: [3][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5199e+00 (2.6410e+00)	Acc@1  32.03 ( 31.74)	Acc@5  69.53 ( 64.49)
Epoch: [3][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3582e+00 (2.6361e+00)	Acc@1  33.59 ( 31.83)	Acc@5  66.41 ( 64.60)
Epoch: [3][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3774e+00 (2.6288e+00)	Acc@1  33.59 ( 31.99)	Acc@5  70.31 ( 64.74)
Epoch: [3][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4265e+00 (2.6254e+00)	Acc@1  37.50 ( 32.10)	Acc@5  64.06 ( 64.81)
Epoch: [3][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5120e+00 (2.6220e+00)	Acc@1  38.28 ( 32.25)	Acc@5  67.19 ( 64.88)
Epoch: [3][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4344e+00 (2.6169e+00)	Acc@1  38.28 ( 32.39)	Acc@5  65.62 ( 64.98)
Epoch: [3][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3074e+00 (2.6130e+00)	Acc@1  40.62 ( 32.51)	Acc@5  71.09 ( 65.08)
Epoch: [3][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9691e+00 (2.6082e+00)	Acc@1  21.88 ( 32.63)	Acc@5  52.34 ( 65.19)
Epoch: [3][360/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6029e+00 (2.6026e+00)	Acc@1  35.16 ( 32.76)	Acc@5  64.06 ( 65.30)
Epoch: [3][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3649e+00 (2.5990e+00)	Acc@1  42.97 ( 32.85)	Acc@5  74.22 ( 65.39)
Epoch: [3][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5744e+00 (2.5951e+00)	Acc@1  35.94 ( 32.93)	Acc@5  63.28 ( 65.51)
Epoch: [3][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5488e+00 (2.5918e+00)	Acc@1  25.00 ( 33.01)	Acc@5  63.75 ( 65.56)
## e[3] optimizer.zero_grad (sum) time: 0.12906241416931152
## e[3]       loss.backward (sum) time: 2.5121445655822754
## e[3]      optimizer.step (sum) time: 0.9932222366333008
## epoch[3] training(only) time: 18.546271562576294
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 2.3872e+00 (2.3872e+00)	Acc@1  43.00 ( 43.00)	Acc@5  74.00 ( 74.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 2.6235e+00 (2.5335e+00)	Acc@1  28.00 ( 32.82)	Acc@5  64.00 ( 68.27)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 2.3083e+00 (2.4971e+00)	Acc@1  40.00 ( 34.14)	Acc@5  70.00 ( 67.95)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.7479e+00 (2.4769e+00)	Acc@1  30.00 ( 34.74)	Acc@5  57.00 ( 68.26)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 2.4171e+00 (2.4581e+00)	Acc@1  34.00 ( 35.12)	Acc@5  73.00 ( 68.76)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.2433e+00 (2.4751e+00)	Acc@1  40.00 ( 34.96)	Acc@5  72.00 ( 68.25)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 2.4761e+00 (2.4763e+00)	Acc@1  36.00 ( 34.93)	Acc@5  70.00 ( 68.00)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 2.7222e+00 (2.4785e+00)	Acc@1  31.00 ( 35.06)	Acc@5  58.00 ( 67.90)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 2.6701e+00 (2.4924e+00)	Acc@1  27.00 ( 34.88)	Acc@5  63.00 ( 67.58)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 2.3465e+00 (2.4772e+00)	Acc@1  40.00 ( 35.31)	Acc@5  70.00 ( 68.07)
 * Acc@1 35.300 Acc@5 68.040
### epoch[3] execution time: 21.029857397079468
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.197 ( 0.197)	Data  0.157 ( 0.157)	Loss 2.4124e+00 (2.4124e+00)	Acc@1  39.06 ( 39.06)	Acc@5  71.88 ( 71.88)
Epoch: [4][ 10/391]	Time  0.046 ( 0.061)	Data  0.001 ( 0.017)	Loss 2.3494e+00 (2.4267e+00)	Acc@1  35.94 ( 36.79)	Acc@5  68.75 ( 67.26)
Epoch: [4][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 2.4693e+00 (2.3894e+00)	Acc@1  39.06 ( 38.06)	Acc@5  68.75 ( 68.53)
Epoch: [4][ 30/391]	Time  0.048 ( 0.052)	Data  0.001 ( 0.008)	Loss 2.2991e+00 (2.3549e+00)	Acc@1  36.72 ( 38.53)	Acc@5  71.88 ( 69.86)
Epoch: [4][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 2.4282e+00 (2.3549e+00)	Acc@1  39.84 ( 38.15)	Acc@5  66.41 ( 70.06)
Epoch: [4][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.2230e+00 (2.3569e+00)	Acc@1  41.41 ( 37.88)	Acc@5  71.88 ( 70.13)
Epoch: [4][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.3878e+00 (2.3519e+00)	Acc@1  43.75 ( 37.82)	Acc@5  67.19 ( 70.33)
Epoch: [4][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.2977e+00 (2.3551e+00)	Acc@1  42.19 ( 38.00)	Acc@5  74.22 ( 70.44)
Epoch: [4][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.2984e+00 (2.3488e+00)	Acc@1  36.72 ( 37.88)	Acc@5  64.84 ( 70.50)
Epoch: [4][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.3745e+00 (2.3529e+00)	Acc@1  38.28 ( 37.79)	Acc@5  71.88 ( 70.60)
Epoch: [4][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0199e+00 (2.3458e+00)	Acc@1  50.78 ( 38.02)	Acc@5  80.47 ( 70.76)
Epoch: [4][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3979e+00 (2.3419e+00)	Acc@1  39.06 ( 38.18)	Acc@5  68.75 ( 70.91)
Epoch: [4][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4079e+00 (2.3439e+00)	Acc@1  36.72 ( 37.99)	Acc@5  69.53 ( 70.85)
Epoch: [4][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1699e+00 (2.3428e+00)	Acc@1  45.31 ( 37.99)	Acc@5  75.78 ( 70.92)
Epoch: [4][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0176e+00 (2.3364e+00)	Acc@1  44.53 ( 38.15)	Acc@5  79.69 ( 71.11)
Epoch: [4][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1295e+00 (2.3343e+00)	Acc@1  39.84 ( 38.26)	Acc@5  75.78 ( 71.14)
Epoch: [4][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1535e+00 (2.3295e+00)	Acc@1  42.97 ( 38.32)	Acc@5  71.88 ( 71.16)
Epoch: [4][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6321e+00 (2.3264e+00)	Acc@1  28.91 ( 38.46)	Acc@5  64.84 ( 71.16)
Epoch: [4][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4581e+00 (2.3292e+00)	Acc@1  35.16 ( 38.45)	Acc@5  65.62 ( 71.05)
Epoch: [4][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5276e+00 (2.3299e+00)	Acc@1  35.16 ( 38.45)	Acc@5  68.75 ( 71.07)
Epoch: [4][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3294e+00 (2.3280e+00)	Acc@1  35.94 ( 38.49)	Acc@5  72.66 ( 71.12)
Epoch: [4][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5955e+00 (2.3248e+00)	Acc@1  34.38 ( 38.53)	Acc@5  61.72 ( 71.16)
Epoch: [4][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1335e+00 (2.3202e+00)	Acc@1  45.31 ( 38.67)	Acc@5  70.31 ( 71.26)
Epoch: [4][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3560e+00 (2.3200e+00)	Acc@1  39.84 ( 38.69)	Acc@5  67.97 ( 71.26)
Epoch: [4][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2813e+00 (2.3167e+00)	Acc@1  39.84 ( 38.77)	Acc@5  71.88 ( 71.33)
Epoch: [4][250/391]	Time  0.046 ( 0.047)	Data  0.002 ( 0.003)	Loss 2.2193e+00 (2.3121e+00)	Acc@1  39.84 ( 38.88)	Acc@5  71.88 ( 71.39)
Epoch: [4][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1466e+00 (2.3097e+00)	Acc@1  40.62 ( 38.88)	Acc@5  71.88 ( 71.46)
Epoch: [4][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1515e+00 (2.3065e+00)	Acc@1  39.84 ( 38.89)	Acc@5  74.22 ( 71.53)
Epoch: [4][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0756e+00 (2.3017e+00)	Acc@1  44.53 ( 38.97)	Acc@5  75.78 ( 71.66)
Epoch: [4][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3231e+00 (2.3005e+00)	Acc@1  40.62 ( 38.99)	Acc@5  71.09 ( 71.67)
Epoch: [4][300/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3401e+00 (2.2992e+00)	Acc@1  36.72 ( 39.05)	Acc@5  67.19 ( 71.69)
Epoch: [4][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4620e+00 (2.2997e+00)	Acc@1  35.94 ( 39.04)	Acc@5  67.19 ( 71.63)
Epoch: [4][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0151e+00 (2.2955e+00)	Acc@1  42.97 ( 39.08)	Acc@5  76.56 ( 71.74)
Epoch: [4][330/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4985e+00 (2.2914e+00)	Acc@1  36.72 ( 39.12)	Acc@5  67.19 ( 71.85)
Epoch: [4][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3144e+00 (2.2894e+00)	Acc@1  39.84 ( 39.20)	Acc@5  70.31 ( 71.89)
Epoch: [4][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1035e+00 (2.2880e+00)	Acc@1  39.84 ( 39.22)	Acc@5  75.78 ( 71.94)
Epoch: [4][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0998e+00 (2.2860e+00)	Acc@1  44.53 ( 39.28)	Acc@5  75.00 ( 71.99)
Epoch: [4][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0423e+00 (2.2825e+00)	Acc@1  40.62 ( 39.32)	Acc@5  76.56 ( 72.07)
Epoch: [4][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3656e+00 (2.2789e+00)	Acc@1  39.84 ( 39.40)	Acc@5  67.19 ( 72.11)
Epoch: [4][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3869e+00 (2.2780e+00)	Acc@1  42.50 ( 39.45)	Acc@5  70.00 ( 72.13)
## e[4] optimizer.zero_grad (sum) time: 0.12874245643615723
## e[4]       loss.backward (sum) time: 2.5109360218048096
## e[4]      optimizer.step (sum) time: 1.000443696975708
## epoch[4] training(only) time: 18.58147883415222
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 2.1273e+00 (2.1273e+00)	Acc@1  45.00 ( 45.00)	Acc@5  73.00 ( 73.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 2.2695e+00 (2.1943e+00)	Acc@1  39.00 ( 42.45)	Acc@5  75.00 ( 74.55)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.9136e+00 (2.1344e+00)	Acc@1  49.00 ( 43.90)	Acc@5  78.00 ( 75.38)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.1664e+00 (2.1318e+00)	Acc@1  38.00 ( 43.16)	Acc@5  80.00 ( 75.45)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 2.0611e+00 (2.1309e+00)	Acc@1  48.00 ( 43.32)	Acc@5  75.00 ( 75.63)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.1318e+00 (2.1541e+00)	Acc@1  44.00 ( 42.96)	Acc@5  70.00 ( 74.92)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 2.0379e+00 (2.1434e+00)	Acc@1  44.00 ( 42.98)	Acc@5  77.00 ( 75.03)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 2.3114e+00 (2.1474e+00)	Acc@1  44.00 ( 42.92)	Acc@5  69.00 ( 75.00)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 2.3996e+00 (2.1620e+00)	Acc@1  35.00 ( 42.51)	Acc@5  71.00 ( 74.78)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 2.2952e+00 (2.1517e+00)	Acc@1  43.00 ( 42.99)	Acc@5  69.00 ( 74.89)
 * Acc@1 43.050 Acc@5 74.950
### epoch[4] execution time: 21.05146336555481
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.190 ( 0.190)	Data  0.150 ( 0.150)	Loss 1.9194e+00 (1.9194e+00)	Acc@1  42.97 ( 42.97)	Acc@5  82.81 ( 82.81)
Epoch: [5][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.016)	Loss 2.0671e+00 (2.0326e+00)	Acc@1  49.22 ( 43.75)	Acc@5  78.12 ( 78.12)
Epoch: [5][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.010)	Loss 2.2110e+00 (2.0426e+00)	Acc@1  39.06 ( 43.42)	Acc@5  70.31 ( 77.05)
Epoch: [5][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.1314e+00 (2.0572e+00)	Acc@1  43.75 ( 43.27)	Acc@5  75.78 ( 77.34)
Epoch: [5][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.9346e+00 (2.0515e+00)	Acc@1  42.19 ( 43.12)	Acc@5  81.25 ( 77.67)
Epoch: [5][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.006)	Loss 2.3035e+00 (2.0506e+00)	Acc@1  40.62 ( 43.43)	Acc@5  71.09 ( 77.56)
Epoch: [5][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.0007e+00 (2.0473e+00)	Acc@1  44.53 ( 43.58)	Acc@5  77.34 ( 77.47)
Epoch: [5][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.1751e+00 (2.0415e+00)	Acc@1  39.06 ( 43.82)	Acc@5  74.22 ( 77.39)
Epoch: [5][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8138e+00 (2.0472e+00)	Acc@1  50.78 ( 43.89)	Acc@5  83.59 ( 77.37)
Epoch: [5][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0186e+00 (2.0456e+00)	Acc@1  46.88 ( 43.91)	Acc@5  76.56 ( 77.26)
Epoch: [5][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1758e+00 (2.0551e+00)	Acc@1  45.31 ( 43.97)	Acc@5  74.22 ( 77.12)
Epoch: [5][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9161e+00 (2.0513e+00)	Acc@1  46.88 ( 44.08)	Acc@5  82.81 ( 77.17)
Epoch: [5][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2098e+00 (2.0550e+00)	Acc@1  38.28 ( 44.01)	Acc@5  77.34 ( 77.03)
Epoch: [5][130/391]	Time  0.047 ( 0.048)	Data  0.004 ( 0.004)	Loss 1.8818e+00 (2.0564e+00)	Acc@1  49.22 ( 43.97)	Acc@5  83.59 ( 76.94)
Epoch: [5][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9894e+00 (2.0574e+00)	Acc@1  41.41 ( 43.96)	Acc@5  78.12 ( 76.89)
Epoch: [5][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2957e+00 (2.0532e+00)	Acc@1  42.19 ( 44.14)	Acc@5  74.22 ( 76.98)
Epoch: [5][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1264e+00 (2.0494e+00)	Acc@1  42.19 ( 44.22)	Acc@5  76.56 ( 77.07)
Epoch: [5][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1218e+00 (2.0518e+00)	Acc@1  42.97 ( 44.15)	Acc@5  78.12 ( 77.10)
Epoch: [5][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0103e+00 (2.0542e+00)	Acc@1  45.31 ( 44.08)	Acc@5  79.69 ( 77.06)
Epoch: [5][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1434e+00 (2.0525e+00)	Acc@1  41.41 ( 44.09)	Acc@5  73.44 ( 77.09)
Epoch: [5][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1336e+00 (2.0503e+00)	Acc@1  42.97 ( 44.13)	Acc@5  75.78 ( 77.16)
Epoch: [5][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9849e+00 (2.0506e+00)	Acc@1  46.88 ( 44.14)	Acc@5  77.34 ( 77.10)
Epoch: [5][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6937e+00 (2.0462e+00)	Acc@1  48.44 ( 44.21)	Acc@5  84.38 ( 77.21)
Epoch: [5][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8661e+00 (2.0437e+00)	Acc@1  44.53 ( 44.26)	Acc@5  87.50 ( 77.24)
Epoch: [5][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8357e+00 (2.0444e+00)	Acc@1  46.88 ( 44.20)	Acc@5  82.81 ( 77.21)
Epoch: [5][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1682e+00 (2.0459e+00)	Acc@1  46.09 ( 44.16)	Acc@5  74.22 ( 77.14)
Epoch: [5][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0667e+00 (2.0473e+00)	Acc@1  46.88 ( 44.20)	Acc@5  77.34 ( 77.11)
Epoch: [5][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8910e+00 (2.0435e+00)	Acc@1  47.66 ( 44.34)	Acc@5  80.47 ( 77.21)
Epoch: [5][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9346e+00 (2.0413e+00)	Acc@1  50.78 ( 44.41)	Acc@5  79.69 ( 77.29)
Epoch: [5][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9889e+00 (2.0351e+00)	Acc@1  48.44 ( 44.54)	Acc@5  78.12 ( 77.38)
Epoch: [5][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0261e+00 (2.0344e+00)	Acc@1  42.97 ( 44.55)	Acc@5  77.34 ( 77.38)
Epoch: [5][310/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9308e+00 (2.0308e+00)	Acc@1  47.66 ( 44.67)	Acc@5  79.69 ( 77.43)
Epoch: [5][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9733e+00 (2.0294e+00)	Acc@1  42.97 ( 44.72)	Acc@5  79.69 ( 77.46)
Epoch: [5][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1202e+00 (2.0279e+00)	Acc@1  44.53 ( 44.76)	Acc@5  71.88 ( 77.48)
Epoch: [5][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6471e+00 (2.0265e+00)	Acc@1  55.47 ( 44.82)	Acc@5  84.38 ( 77.52)
Epoch: [5][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2058e+00 (2.0258e+00)	Acc@1  40.62 ( 44.83)	Acc@5  74.22 ( 77.54)
Epoch: [5][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1416e+00 (2.0241e+00)	Acc@1  39.06 ( 44.88)	Acc@5  74.22 ( 77.59)
Epoch: [5][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9613e+00 (2.0216e+00)	Acc@1  49.22 ( 44.96)	Acc@5  78.91 ( 77.61)
Epoch: [5][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7716e+00 (2.0185e+00)	Acc@1  49.22 ( 45.04)	Acc@5  82.03 ( 77.66)
Epoch: [5][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8472e+00 (2.0181e+00)	Acc@1  45.00 ( 45.07)	Acc@5  82.50 ( 77.67)
## e[5] optimizer.zero_grad (sum) time: 0.12968850135803223
## e[5]       loss.backward (sum) time: 2.5198211669921875
## e[5]      optimizer.step (sum) time: 1.0048515796661377
## epoch[5] training(only) time: 18.563414573669434
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 2.0336e+00 (2.0336e+00)	Acc@1  47.00 ( 47.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 2.1451e+00 (2.0913e+00)	Acc@1  45.00 ( 45.73)	Acc@5  78.00 ( 75.64)
Test: [ 20/100]	Time  0.024 ( 0.029)	Loss 1.9185e+00 (2.0764e+00)	Acc@1  49.00 ( 45.57)	Acc@5  82.00 ( 76.86)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.2512e+00 (2.0928e+00)	Acc@1  37.00 ( 44.39)	Acc@5  73.00 ( 76.19)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 2.2373e+00 (2.0909e+00)	Acc@1  46.00 ( 44.46)	Acc@5  77.00 ( 76.22)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.8461e+00 (2.0999e+00)	Acc@1  52.00 ( 44.59)	Acc@5  80.00 ( 75.80)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.9554e+00 (2.0939e+00)	Acc@1  58.00 ( 45.00)	Acc@5  76.00 ( 75.93)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 2.2616e+00 (2.1019e+00)	Acc@1  44.00 ( 45.07)	Acc@5  77.00 ( 75.90)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 2.2408e+00 (2.1053e+00)	Acc@1  44.00 ( 45.05)	Acc@5  68.00 ( 75.73)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 2.2047e+00 (2.0950e+00)	Acc@1  43.00 ( 45.32)	Acc@5  73.00 ( 75.88)
 * Acc@1 45.210 Acc@5 75.760
### epoch[5] execution time: 21.078446865081787
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.192 ( 0.192)	Data  0.151 ( 0.151)	Loss 1.7648e+00 (1.7648e+00)	Acc@1  50.00 ( 50.00)	Acc@5  80.47 ( 80.47)
Epoch: [6][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.7738e+00 (1.8131e+00)	Acc@1  53.12 ( 49.43)	Acc@5  82.81 ( 81.75)
Epoch: [6][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.7838e+00 (1.8383e+00)	Acc@1  53.12 ( 48.88)	Acc@5  82.81 ( 80.54)
Epoch: [6][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.2111e+00 (1.8511e+00)	Acc@1  40.62 ( 48.31)	Acc@5  77.34 ( 80.57)
Epoch: [6][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.7388e+00 (1.8435e+00)	Acc@1  57.03 ( 48.11)	Acc@5  82.03 ( 80.75)
Epoch: [6][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.9241e+00 (1.8607e+00)	Acc@1  39.84 ( 47.69)	Acc@5  82.03 ( 80.44)
Epoch: [6][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7536e+00 (1.8427e+00)	Acc@1  53.12 ( 48.27)	Acc@5  82.03 ( 80.89)
Epoch: [6][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.9994e+00 (1.8379e+00)	Acc@1  47.66 ( 48.61)	Acc@5  78.12 ( 80.83)
Epoch: [6][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.8750e+00 (1.8340e+00)	Acc@1  46.88 ( 48.81)	Acc@5  84.38 ( 81.01)
Epoch: [6][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8255e+00 (1.8293e+00)	Acc@1  46.09 ( 48.85)	Acc@5  85.94 ( 81.19)
Epoch: [6][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9060e+00 (1.8320e+00)	Acc@1  46.09 ( 48.85)	Acc@5  76.56 ( 81.02)
Epoch: [6][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9674e+00 (1.8447e+00)	Acc@1  47.66 ( 48.67)	Acc@5  78.12 ( 80.81)
Epoch: [6][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9260e+00 (1.8481e+00)	Acc@1  41.41 ( 48.57)	Acc@5  82.03 ( 80.75)
Epoch: [6][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7985e+00 (1.8477e+00)	Acc@1  53.12 ( 48.61)	Acc@5  85.94 ( 80.79)
Epoch: [6][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7446e+00 (1.8429e+00)	Acc@1  54.69 ( 48.76)	Acc@5  82.81 ( 80.91)
Epoch: [6][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7305e+00 (1.8412e+00)	Acc@1  52.34 ( 48.84)	Acc@5  79.69 ( 80.93)
Epoch: [6][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6788e+00 (1.8363e+00)	Acc@1  54.69 ( 48.95)	Acc@5  85.16 ( 80.96)
Epoch: [6][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9033e+00 (1.8335e+00)	Acc@1  46.88 ( 49.13)	Acc@5  82.81 ( 81.02)
Epoch: [6][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7593e+00 (1.8368e+00)	Acc@1  51.56 ( 49.14)	Acc@5  81.25 ( 80.95)
Epoch: [6][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7601e+00 (1.8367e+00)	Acc@1  53.91 ( 49.19)	Acc@5  80.47 ( 80.98)
Epoch: [6][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1185e+00 (1.8373e+00)	Acc@1  45.31 ( 49.22)	Acc@5  75.78 ( 80.97)
Epoch: [6][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7544e+00 (1.8354e+00)	Acc@1  47.66 ( 49.20)	Acc@5  82.81 ( 81.04)
Epoch: [6][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6227e+00 (1.8339e+00)	Acc@1  50.00 ( 49.23)	Acc@5  82.81 ( 81.09)
Epoch: [6][230/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7169e+00 (1.8291e+00)	Acc@1  56.25 ( 49.33)	Acc@5  83.59 ( 81.16)
Epoch: [6][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8012e+00 (1.8262e+00)	Acc@1  52.34 ( 49.36)	Acc@5  78.91 ( 81.18)
Epoch: [6][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7446e+00 (1.8230e+00)	Acc@1  54.69 ( 49.51)	Acc@5  81.25 ( 81.20)
Epoch: [6][260/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7434e+00 (1.8211e+00)	Acc@1  52.34 ( 49.56)	Acc@5  84.38 ( 81.23)
Epoch: [6][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8048e+00 (1.8186e+00)	Acc@1  47.66 ( 49.62)	Acc@5  81.25 ( 81.31)
Epoch: [6][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7060e+00 (1.8188e+00)	Acc@1  50.78 ( 49.61)	Acc@5  81.25 ( 81.34)
Epoch: [6][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7270e+00 (1.8201e+00)	Acc@1  50.00 ( 49.55)	Acc@5  83.59 ( 81.30)
Epoch: [6][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9335e+00 (1.8189e+00)	Acc@1  48.44 ( 49.60)	Acc@5  77.34 ( 81.29)
Epoch: [6][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7262e+00 (1.8187e+00)	Acc@1  51.56 ( 49.58)	Acc@5  82.81 ( 81.28)
Epoch: [6][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8775e+00 (1.8175e+00)	Acc@1  52.34 ( 49.67)	Acc@5  79.69 ( 81.27)
Epoch: [6][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7400e+00 (1.8155e+00)	Acc@1  52.34 ( 49.73)	Acc@5  82.81 ( 81.27)
Epoch: [6][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6594e+00 (1.8141e+00)	Acc@1  50.00 ( 49.76)	Acc@5  86.72 ( 81.31)
Epoch: [6][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6733e+00 (1.8121e+00)	Acc@1  52.34 ( 49.82)	Acc@5  85.16 ( 81.35)
Epoch: [6][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3012e+00 (1.8101e+00)	Acc@1  62.50 ( 49.83)	Acc@5  88.28 ( 81.37)
Epoch: [6][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8632e+00 (1.8095e+00)	Acc@1  44.53 ( 49.83)	Acc@5  82.81 ( 81.36)
Epoch: [6][380/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7910e+00 (1.8096e+00)	Acc@1  49.22 ( 49.84)	Acc@5  78.91 ( 81.32)
Epoch: [6][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0313e+00 (1.8092e+00)	Acc@1  41.25 ( 49.84)	Acc@5  76.25 ( 81.31)
## e[6] optimizer.zero_grad (sum) time: 0.1280829906463623
## e[6]       loss.backward (sum) time: 2.5122995376586914
## e[6]      optimizer.step (sum) time: 1.0061416625976562
## epoch[6] training(only) time: 18.5917706489563
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.8218e+00 (1.8218e+00)	Acc@1  57.00 ( 57.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 2.0030e+00 (1.8483e+00)	Acc@1  43.00 ( 50.36)	Acc@5  78.00 ( 80.45)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.8396e+00 (1.8375e+00)	Acc@1  44.00 ( 50.43)	Acc@5  80.00 ( 80.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.0967e+00 (1.8473e+00)	Acc@1  39.00 ( 49.71)	Acc@5  77.00 ( 80.42)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.7801e+00 (1.8454e+00)	Acc@1  52.00 ( 49.93)	Acc@5  82.00 ( 80.59)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.6356e+00 (1.8495e+00)	Acc@1  50.00 ( 49.86)	Acc@5  82.00 ( 80.33)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.6698e+00 (1.8481e+00)	Acc@1  53.00 ( 49.90)	Acc@5  87.00 ( 80.54)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.9205e+00 (1.8510e+00)	Acc@1  52.00 ( 49.82)	Acc@5  79.00 ( 80.54)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.9884e+00 (1.8621e+00)	Acc@1  52.00 ( 49.63)	Acc@5  80.00 ( 80.17)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.9698e+00 (1.8577e+00)	Acc@1  53.00 ( 49.74)	Acc@5  81.00 ( 80.33)
 * Acc@1 49.690 Acc@5 80.170
### epoch[6] execution time: 21.09053659439087
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.194 ( 0.194)	Data  0.149 ( 0.149)	Loss 1.4082e+00 (1.4082e+00)	Acc@1  64.84 ( 64.84)	Acc@5  88.28 ( 88.28)
Epoch: [7][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.6253e+00 (1.6547e+00)	Acc@1  57.03 ( 54.40)	Acc@5  85.16 ( 83.81)
Epoch: [7][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.6890e+00 (1.5961e+00)	Acc@1  50.00 ( 55.80)	Acc@5  80.47 ( 84.78)
Epoch: [7][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.6829e+00 (1.6027e+00)	Acc@1  56.25 ( 55.49)	Acc@5  82.81 ( 84.63)
Epoch: [7][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.4776e+00 (1.5754e+00)	Acc@1  55.47 ( 56.40)	Acc@5  85.94 ( 85.00)
Epoch: [7][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.3569e+00 (1.5828e+00)	Acc@1  60.16 ( 56.02)	Acc@5  88.28 ( 85.09)
Epoch: [7][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.5796e+00 (1.5951e+00)	Acc@1  54.69 ( 55.66)	Acc@5  85.16 ( 84.89)
Epoch: [7][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6448e+00 (1.6014e+00)	Acc@1  57.81 ( 55.60)	Acc@5  81.25 ( 84.80)
Epoch: [7][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.6995e+00 (1.5996e+00)	Acc@1  53.91 ( 55.68)	Acc@5  83.59 ( 84.85)
Epoch: [7][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7503e+00 (1.6218e+00)	Acc@1  50.78 ( 55.20)	Acc@5  78.91 ( 84.44)
Epoch: [7][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9052e+00 (1.6289e+00)	Acc@1  45.31 ( 54.84)	Acc@5  79.69 ( 84.38)
Epoch: [7][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8244e+00 (1.6320e+00)	Acc@1  45.31 ( 54.58)	Acc@5  85.16 ( 84.33)
Epoch: [7][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7595e+00 (1.6367e+00)	Acc@1  54.69 ( 54.49)	Acc@5  81.25 ( 84.25)
Epoch: [7][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5935e+00 (1.6384e+00)	Acc@1  52.34 ( 54.44)	Acc@5  86.72 ( 84.20)
Epoch: [7][140/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6531e+00 (1.6371e+00)	Acc@1  53.12 ( 54.46)	Acc@5  82.03 ( 84.15)
Epoch: [7][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6893e+00 (1.6396e+00)	Acc@1  52.34 ( 54.50)	Acc@5  85.94 ( 84.16)
Epoch: [7][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5500e+00 (1.6424e+00)	Acc@1  56.25 ( 54.44)	Acc@5  85.16 ( 84.08)
Epoch: [7][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4181e+00 (1.6401e+00)	Acc@1  61.72 ( 54.66)	Acc@5  89.06 ( 84.08)
Epoch: [7][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4356e+00 (1.6394e+00)	Acc@1  64.06 ( 54.66)	Acc@5  88.28 ( 84.12)
Epoch: [7][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7472e+00 (1.6395e+00)	Acc@1  50.00 ( 54.58)	Acc@5  83.59 ( 84.11)
Epoch: [7][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5052e+00 (1.6395e+00)	Acc@1  54.69 ( 54.53)	Acc@5  85.94 ( 84.08)
Epoch: [7][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5724e+00 (1.6403e+00)	Acc@1  60.94 ( 54.59)	Acc@5  85.16 ( 84.06)
Epoch: [7][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4864e+00 (1.6428e+00)	Acc@1  62.50 ( 54.56)	Acc@5  84.38 ( 83.95)
Epoch: [7][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8427e+00 (1.6431e+00)	Acc@1  50.78 ( 54.56)	Acc@5  82.03 ( 83.95)
Epoch: [7][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6377e+00 (1.6399e+00)	Acc@1  56.25 ( 54.57)	Acc@5  86.72 ( 84.03)
Epoch: [7][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5883e+00 (1.6415e+00)	Acc@1  53.12 ( 54.48)	Acc@5  85.16 ( 84.09)
Epoch: [7][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6702e+00 (1.6406e+00)	Acc@1  53.91 ( 54.54)	Acc@5  84.38 ( 84.13)
Epoch: [7][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4691e+00 (1.6397e+00)	Acc@1  60.16 ( 54.55)	Acc@5  85.94 ( 84.16)
Epoch: [7][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5427e+00 (1.6381e+00)	Acc@1  55.47 ( 54.52)	Acc@5  86.72 ( 84.21)
Epoch: [7][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7535e+00 (1.6392e+00)	Acc@1  49.22 ( 54.44)	Acc@5  87.50 ( 84.25)
Epoch: [7][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7667e+00 (1.6371e+00)	Acc@1  48.44 ( 54.45)	Acc@5  82.03 ( 84.31)
Epoch: [7][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5661e+00 (1.6374e+00)	Acc@1  57.81 ( 54.45)	Acc@5  86.72 ( 84.30)
Epoch: [7][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6442e+00 (1.6382e+00)	Acc@1  53.12 ( 54.45)	Acc@5  85.16 ( 84.28)
Epoch: [7][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5483e+00 (1.6389e+00)	Acc@1  52.34 ( 54.39)	Acc@5  88.28 ( 84.29)
Epoch: [7][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7394e+00 (1.6393e+00)	Acc@1  52.34 ( 54.39)	Acc@5  82.03 ( 84.28)
Epoch: [7][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4591e+00 (1.6381e+00)	Acc@1  57.03 ( 54.44)	Acc@5  86.72 ( 84.25)
Epoch: [7][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6035e+00 (1.6368e+00)	Acc@1  55.47 ( 54.48)	Acc@5  82.03 ( 84.24)
Epoch: [7][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5871e+00 (1.6353e+00)	Acc@1  58.59 ( 54.57)	Acc@5  85.16 ( 84.25)
Epoch: [7][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6731e+00 (1.6356e+00)	Acc@1  49.22 ( 54.52)	Acc@5  84.38 ( 84.23)
Epoch: [7][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5906e+00 (1.6323e+00)	Acc@1  62.50 ( 54.60)	Acc@5  87.50 ( 84.28)
## e[7] optimizer.zero_grad (sum) time: 0.12825489044189453
## e[7]       loss.backward (sum) time: 2.5040082931518555
## e[7]      optimizer.step (sum) time: 1.014216423034668
## epoch[7] training(only) time: 18.615177154541016
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.6164e+00 (1.6164e+00)	Acc@1  58.00 ( 58.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.9225e+00 (1.8108e+00)	Acc@1  47.00 ( 52.82)	Acc@5  86.00 ( 81.55)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5231e+00 (1.7553e+00)	Acc@1  59.00 ( 52.62)	Acc@5  82.00 ( 82.19)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.9383e+00 (1.7664e+00)	Acc@1  47.00 ( 52.29)	Acc@5  83.00 ( 81.71)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.9538e+00 (1.7735e+00)	Acc@1  49.00 ( 51.98)	Acc@5  81.00 ( 81.71)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6723e+00 (1.7977e+00)	Acc@1  58.00 ( 51.51)	Acc@5  80.00 ( 81.22)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.9050e+00 (1.7905e+00)	Acc@1  51.00 ( 51.54)	Acc@5  80.00 ( 81.20)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.7637e+00 (1.7912e+00)	Acc@1  49.00 ( 51.58)	Acc@5  80.00 ( 81.18)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.8012e+00 (1.7986e+00)	Acc@1  48.00 ( 51.41)	Acc@5  84.00 ( 81.27)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.9661e+00 (1.7970e+00)	Acc@1  51.00 ( 51.59)	Acc@5  79.00 ( 81.45)
 * Acc@1 51.780 Acc@5 81.610
### epoch[7] execution time: 21.122965574264526
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.185 ( 0.185)	Data  0.146 ( 0.146)	Loss 1.4757e+00 (1.4757e+00)	Acc@1  58.59 ( 58.59)	Acc@5  86.72 ( 86.72)
Epoch: [8][ 10/391]	Time  0.048 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.3272e+00 (1.4559e+00)	Acc@1  63.28 ( 58.81)	Acc@5  85.16 ( 86.72)
Epoch: [8][ 20/391]	Time  0.045 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.2750e+00 (1.4538e+00)	Acc@1  60.94 ( 58.18)	Acc@5  89.06 ( 86.94)
Epoch: [8][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.5571e+00 (1.4699e+00)	Acc@1  57.81 ( 57.79)	Acc@5  86.72 ( 86.92)
Epoch: [8][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.4092e+00 (1.4696e+00)	Acc@1  61.72 ( 57.81)	Acc@5  88.28 ( 86.66)
Epoch: [8][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.6681e+00 (1.4849e+00)	Acc@1  55.47 ( 57.74)	Acc@5  81.25 ( 86.46)
Epoch: [8][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6069e+00 (1.4779e+00)	Acc@1  51.56 ( 57.72)	Acc@5  88.28 ( 86.86)
Epoch: [8][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4272e+00 (1.4738e+00)	Acc@1  59.38 ( 57.64)	Acc@5  85.16 ( 86.91)
Epoch: [8][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.3606e+00 (1.4787e+00)	Acc@1  55.47 ( 57.60)	Acc@5  90.62 ( 86.75)
Epoch: [8][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.7673e+00 (1.4931e+00)	Acc@1  50.00 ( 57.37)	Acc@5  81.25 ( 86.47)
Epoch: [8][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4833e+00 (1.4924e+00)	Acc@1  59.38 ( 57.52)	Acc@5  86.72 ( 86.49)
Epoch: [8][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5007e+00 (1.4979e+00)	Acc@1  55.47 ( 57.41)	Acc@5  85.94 ( 86.44)
Epoch: [8][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7245e+00 (1.5031e+00)	Acc@1  46.88 ( 57.24)	Acc@5  86.72 ( 86.45)
Epoch: [8][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5339e+00 (1.5017e+00)	Acc@1  56.25 ( 57.34)	Acc@5  85.94 ( 86.43)
Epoch: [8][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4717e+00 (1.5007e+00)	Acc@1  57.81 ( 57.42)	Acc@5  89.06 ( 86.45)
Epoch: [8][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4550e+00 (1.5022e+00)	Acc@1  57.81 ( 57.51)	Acc@5  88.28 ( 86.44)
Epoch: [8][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3941e+00 (1.5025e+00)	Acc@1  57.81 ( 57.52)	Acc@5  88.28 ( 86.40)
Epoch: [8][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2764e+00 (1.5029e+00)	Acc@1  68.75 ( 57.56)	Acc@5  92.97 ( 86.39)
Epoch: [8][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4639e+00 (1.4986e+00)	Acc@1  61.72 ( 57.64)	Acc@5  86.72 ( 86.46)
Epoch: [8][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6640e+00 (1.4987e+00)	Acc@1  53.12 ( 57.60)	Acc@5  84.38 ( 86.43)
Epoch: [8][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4135e+00 (1.4983e+00)	Acc@1  59.38 ( 57.57)	Acc@5  84.38 ( 86.42)
Epoch: [8][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1481e+00 (1.4984e+00)	Acc@1  61.72 ( 57.51)	Acc@5  92.19 ( 86.40)
Epoch: [8][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6298e+00 (1.4971e+00)	Acc@1  54.69 ( 57.50)	Acc@5  81.25 ( 86.38)
Epoch: [8][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3855e+00 (1.4984e+00)	Acc@1  64.06 ( 57.54)	Acc@5  87.50 ( 86.33)
Epoch: [8][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4236e+00 (1.4969e+00)	Acc@1  60.94 ( 57.54)	Acc@5  87.50 ( 86.39)
Epoch: [8][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3309e+00 (1.4949e+00)	Acc@1  61.72 ( 57.57)	Acc@5  89.06 ( 86.46)
Epoch: [8][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2699e+00 (1.4930e+00)	Acc@1  61.72 ( 57.62)	Acc@5  89.84 ( 86.55)
Epoch: [8][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5484e+00 (1.4931e+00)	Acc@1  53.91 ( 57.65)	Acc@5  85.16 ( 86.59)
Epoch: [8][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7892e+00 (1.4950e+00)	Acc@1  44.53 ( 57.58)	Acc@5  83.59 ( 86.56)
Epoch: [8][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3578e+00 (1.4934e+00)	Acc@1  60.16 ( 57.62)	Acc@5  86.72 ( 86.58)
Epoch: [8][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4518e+00 (1.4932e+00)	Acc@1  60.16 ( 57.59)	Acc@5  87.50 ( 86.60)
Epoch: [8][310/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1352e+00 (1.4936e+00)	Acc@1  64.06 ( 57.61)	Acc@5  94.53 ( 86.60)
Epoch: [8][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5807e+00 (1.4923e+00)	Acc@1  54.69 ( 57.61)	Acc@5  85.16 ( 86.62)
Epoch: [8][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3583e+00 (1.4911e+00)	Acc@1  60.94 ( 57.65)	Acc@5  87.50 ( 86.61)
Epoch: [8][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7099e+00 (1.4926e+00)	Acc@1  51.56 ( 57.57)	Acc@5  82.03 ( 86.58)
Epoch: [8][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4145e+00 (1.4914e+00)	Acc@1  54.69 ( 57.62)	Acc@5  88.28 ( 86.59)
Epoch: [8][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3850e+00 (1.4910e+00)	Acc@1  58.59 ( 57.62)	Acc@5  90.62 ( 86.60)
Epoch: [8][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4605e+00 (1.4900e+00)	Acc@1  56.25 ( 57.62)	Acc@5  87.50 ( 86.62)
Epoch: [8][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6513e+00 (1.4910e+00)	Acc@1  52.34 ( 57.58)	Acc@5  82.03 ( 86.60)
Epoch: [8][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6595e+00 (1.4901e+00)	Acc@1  47.50 ( 57.57)	Acc@5  81.25 ( 86.61)
## e[8] optimizer.zero_grad (sum) time: 0.13112783432006836
## e[8]       loss.backward (sum) time: 2.5203659534454346
## e[8]      optimizer.step (sum) time: 0.9924705028533936
## epoch[8] training(only) time: 18.5976345539093
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.5777e+00 (1.5777e+00)	Acc@1  56.00 ( 56.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.6948e+00 (1.6473e+00)	Acc@1  53.00 ( 55.91)	Acc@5  82.00 ( 82.64)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.5321e+00 (1.6159e+00)	Acc@1  59.00 ( 55.57)	Acc@5  84.00 ( 84.05)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8921e+00 (1.6325e+00)	Acc@1  45.00 ( 55.35)	Acc@5  84.00 ( 83.68)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.6666e+00 (1.6419e+00)	Acc@1  56.00 ( 55.51)	Acc@5  85.00 ( 83.78)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4181e+00 (1.6487e+00)	Acc@1  55.00 ( 55.18)	Acc@5  88.00 ( 83.61)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.6237e+00 (1.6326e+00)	Acc@1  54.00 ( 55.25)	Acc@5  82.00 ( 83.75)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.7369e+00 (1.6371e+00)	Acc@1  60.00 ( 55.31)	Acc@5  79.00 ( 83.70)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5440e+00 (1.6470e+00)	Acc@1  60.00 ( 55.05)	Acc@5  83.00 ( 83.53)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7119e+00 (1.6417e+00)	Acc@1  50.00 ( 55.19)	Acc@5  84.00 ( 83.80)
 * Acc@1 55.200 Acc@5 83.850
### epoch[8] execution time: 21.079513549804688
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.188 ( 0.188)	Data  0.146 ( 0.146)	Loss 1.2876e+00 (1.2876e+00)	Acc@1  67.97 ( 67.97)	Acc@5  89.84 ( 89.84)
Epoch: [9][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 1.4805e+00 (1.3403e+00)	Acc@1  60.94 ( 62.22)	Acc@5  88.28 ( 89.35)
Epoch: [9][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.2692e+00 (1.2898e+00)	Acc@1  64.84 ( 63.62)	Acc@5  89.84 ( 89.96)
Epoch: [9][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.2490e+00 (1.3115e+00)	Acc@1  64.06 ( 62.75)	Acc@5  90.62 ( 89.49)
Epoch: [9][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.3821e+00 (1.3375e+00)	Acc@1  58.59 ( 61.87)	Acc@5  91.41 ( 89.23)
Epoch: [9][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.2771e+00 (1.3396e+00)	Acc@1  61.72 ( 61.67)	Acc@5  91.41 ( 88.97)
Epoch: [9][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4532e+00 (1.3469e+00)	Acc@1  60.16 ( 61.31)	Acc@5  83.59 ( 88.78)
Epoch: [9][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4661e+00 (1.3476e+00)	Acc@1  57.03 ( 61.17)	Acc@5  90.62 ( 88.68)
Epoch: [9][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.2752e+00 (1.3430e+00)	Acc@1  60.16 ( 61.43)	Acc@5  89.84 ( 88.71)
Epoch: [9][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3597e+00 (1.3384e+00)	Acc@1  57.81 ( 61.50)	Acc@5  92.97 ( 88.87)
Epoch: [9][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3146e+00 (1.3368e+00)	Acc@1  60.16 ( 61.63)	Acc@5  90.62 ( 88.81)
Epoch: [9][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2704e+00 (1.3338e+00)	Acc@1  63.28 ( 61.70)	Acc@5  87.50 ( 88.87)
Epoch: [9][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5596e+00 (1.3359e+00)	Acc@1  55.47 ( 61.58)	Acc@5  83.59 ( 88.85)
Epoch: [9][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3905e+00 (1.3391e+00)	Acc@1  60.94 ( 61.49)	Acc@5  89.84 ( 88.78)
Epoch: [9][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1770e+00 (1.3380e+00)	Acc@1  63.28 ( 61.47)	Acc@5  92.97 ( 88.81)
Epoch: [9][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1619e+00 (1.3362e+00)	Acc@1  65.62 ( 61.51)	Acc@5  91.41 ( 88.85)
Epoch: [9][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4208e+00 (1.3357e+00)	Acc@1  60.16 ( 61.38)	Acc@5  88.28 ( 88.86)
Epoch: [9][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3368e+00 (1.3360e+00)	Acc@1  59.38 ( 61.49)	Acc@5  89.84 ( 88.83)
Epoch: [9][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3364e+00 (1.3342e+00)	Acc@1  64.84 ( 61.56)	Acc@5  90.62 ( 88.84)
Epoch: [9][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2130e+00 (1.3352e+00)	Acc@1  61.72 ( 61.47)	Acc@5  89.84 ( 88.80)
Epoch: [9][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7292e+00 (1.3352e+00)	Acc@1  49.22 ( 61.46)	Acc@5  81.25 ( 88.79)
Epoch: [9][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2208e+00 (1.3366e+00)	Acc@1  62.50 ( 61.45)	Acc@5  93.75 ( 88.74)
Epoch: [9][220/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4553e+00 (1.3365e+00)	Acc@1  58.59 ( 61.44)	Acc@5  87.50 ( 88.73)
Epoch: [9][230/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3988e+00 (1.3381e+00)	Acc@1  64.84 ( 61.48)	Acc@5  89.06 ( 88.72)
Epoch: [9][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4724e+00 (1.3384e+00)	Acc@1  59.38 ( 61.48)	Acc@5  84.38 ( 88.72)
Epoch: [9][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5509e+00 (1.3410e+00)	Acc@1  58.59 ( 61.39)	Acc@5  86.72 ( 88.72)
Epoch: [9][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5180e+00 (1.3433e+00)	Acc@1  53.12 ( 61.34)	Acc@5  82.81 ( 88.68)
Epoch: [9][270/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4097e+00 (1.3437e+00)	Acc@1  61.72 ( 61.36)	Acc@5  85.16 ( 88.66)
Epoch: [9][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3359e+00 (1.3445e+00)	Acc@1  63.28 ( 61.33)	Acc@5  85.94 ( 88.61)
Epoch: [9][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4046e+00 (1.3458e+00)	Acc@1  57.81 ( 61.26)	Acc@5  87.50 ( 88.60)
Epoch: [9][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4103e+00 (1.3495e+00)	Acc@1  63.28 ( 61.22)	Acc@5  86.72 ( 88.53)
Epoch: [9][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2444e+00 (1.3514e+00)	Acc@1  60.94 ( 61.21)	Acc@5  89.06 ( 88.49)
Epoch: [9][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5830e+00 (1.3552e+00)	Acc@1  49.22 ( 61.04)	Acc@5  91.41 ( 88.48)
Epoch: [9][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5174e+00 (1.3559e+00)	Acc@1  54.69 ( 61.01)	Acc@5  85.94 ( 88.48)
Epoch: [9][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3979e+00 (1.3583e+00)	Acc@1  62.50 ( 60.94)	Acc@5  85.16 ( 88.46)
Epoch: [9][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3537e+00 (1.3582e+00)	Acc@1  62.50 ( 60.94)	Acc@5  91.41 ( 88.46)
Epoch: [9][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4553e+00 (1.3613e+00)	Acc@1  60.94 ( 60.88)	Acc@5  91.41 ( 88.43)
Epoch: [9][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3336e+00 (1.3620e+00)	Acc@1  57.03 ( 60.87)	Acc@5  90.62 ( 88.42)
Epoch: [9][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4057e+00 (1.3634e+00)	Acc@1  57.03 ( 60.87)	Acc@5  86.72 ( 88.36)
Epoch: [9][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3186e+00 (1.3631e+00)	Acc@1  61.25 ( 60.90)	Acc@5  86.25 ( 88.36)
## e[9] optimizer.zero_grad (sum) time: 0.1299433708190918
## e[9]       loss.backward (sum) time: 2.534057140350342
## e[9]      optimizer.step (sum) time: 0.9982113838195801
## epoch[9] training(only) time: 18.625221252441406
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.6537e+00 (1.6537e+00)	Acc@1  61.00 ( 61.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.6097e+00 (1.6125e+00)	Acc@1  56.00 ( 57.55)	Acc@5  88.00 ( 85.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4941e+00 (1.5970e+00)	Acc@1  56.00 ( 57.81)	Acc@5  86.00 ( 85.19)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7988e+00 (1.6165e+00)	Acc@1  47.00 ( 56.61)	Acc@5  85.00 ( 84.55)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.6567e+00 (1.6306e+00)	Acc@1  52.00 ( 56.17)	Acc@5  87.00 ( 84.51)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4173e+00 (1.6466e+00)	Acc@1  58.00 ( 55.57)	Acc@5  81.00 ( 83.94)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.6438e+00 (1.6315e+00)	Acc@1  56.00 ( 55.77)	Acc@5  87.00 ( 84.10)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6368e+00 (1.6234e+00)	Acc@1  49.00 ( 55.92)	Acc@5  87.00 ( 84.18)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6648e+00 (1.6292e+00)	Acc@1  55.00 ( 55.70)	Acc@5  82.00 ( 84.16)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8241e+00 (1.6233e+00)	Acc@1  52.00 ( 55.86)	Acc@5  79.00 ( 84.15)
 * Acc@1 56.070 Acc@5 84.290
### epoch[9] execution time: 21.119118452072144
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.189 ( 0.189)	Data  0.149 ( 0.149)	Loss 9.5361e-01 (9.5361e-01)	Acc@1  71.09 ( 71.09)	Acc@5  94.53 ( 94.53)
Epoch: [10][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.016)	Loss 1.0581e+00 (1.2416e+00)	Acc@1  67.97 ( 64.13)	Acc@5  92.19 ( 90.62)
Epoch: [10][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.0969e+00 (1.2314e+00)	Acc@1  71.09 ( 64.58)	Acc@5  89.06 ( 90.40)
Epoch: [10][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.1622e+00 (1.2279e+00)	Acc@1  66.41 ( 64.79)	Acc@5  90.62 ( 90.50)
Epoch: [10][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.0543e+00 (1.2160e+00)	Acc@1  71.88 ( 65.17)	Acc@5  91.41 ( 90.76)
Epoch: [10][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.3522e+00 (1.2153e+00)	Acc@1  60.16 ( 65.01)	Acc@5  89.06 ( 90.70)
Epoch: [10][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.1914e+00 (1.2134e+00)	Acc@1  64.06 ( 65.06)	Acc@5  88.28 ( 90.56)
Epoch: [10][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.7825e-01 (1.2142e+00)	Acc@1  68.75 ( 65.01)	Acc@5  94.53 ( 90.57)
Epoch: [10][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.2815e+00 (1.2171e+00)	Acc@1  66.41 ( 64.96)	Acc@5  87.50 ( 90.48)
Epoch: [10][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2483e+00 (1.2144e+00)	Acc@1  60.94 ( 64.82)	Acc@5  89.06 ( 90.63)
Epoch: [10][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2233e+00 (1.2152e+00)	Acc@1  64.06 ( 64.69)	Acc@5  90.62 ( 90.66)
Epoch: [10][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3559e+00 (1.2225e+00)	Acc@1  60.16 ( 64.47)	Acc@5  86.72 ( 90.48)
Epoch: [10][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3352e+00 (1.2286e+00)	Acc@1  65.62 ( 64.37)	Acc@5  88.28 ( 90.36)
Epoch: [10][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0342e+00 (1.2279e+00)	Acc@1  71.09 ( 64.42)	Acc@5  93.75 ( 90.37)
Epoch: [10][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2585e+00 (1.2301e+00)	Acc@1  66.41 ( 64.32)	Acc@5  90.62 ( 90.42)
Epoch: [10][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2959e+00 (1.2313e+00)	Acc@1  60.94 ( 64.27)	Acc@5  88.28 ( 90.39)
Epoch: [10][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2141e+00 (1.2294e+00)	Acc@1  65.62 ( 64.33)	Acc@5  85.94 ( 90.43)
Epoch: [10][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1013e+00 (1.2310e+00)	Acc@1  64.84 ( 64.24)	Acc@5  94.53 ( 90.42)
Epoch: [10][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2440e+00 (1.2327e+00)	Acc@1  66.41 ( 64.13)	Acc@5  89.84 ( 90.46)
Epoch: [10][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1431e+00 (1.2302e+00)	Acc@1  64.06 ( 64.14)	Acc@5  90.62 ( 90.50)
Epoch: [10][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3386e+00 (1.2309e+00)	Acc@1  60.16 ( 64.13)	Acc@5  89.84 ( 90.45)
Epoch: [10][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2544e+00 (1.2339e+00)	Acc@1  64.06 ( 64.03)	Acc@5  88.28 ( 90.39)
Epoch: [10][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.4946e-01 (1.2338e+00)	Acc@1  71.88 ( 64.06)	Acc@5  92.19 ( 90.38)
Epoch: [10][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3416e+00 (1.2349e+00)	Acc@1  62.50 ( 64.05)	Acc@5  86.72 ( 90.31)
Epoch: [10][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2622e+00 (1.2391e+00)	Acc@1  66.41 ( 63.94)	Acc@5  89.84 ( 90.32)
Epoch: [10][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5607e+00 (1.2420e+00)	Acc@1  54.69 ( 63.89)	Acc@5  85.16 ( 90.26)
Epoch: [10][260/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2672e+00 (1.2444e+00)	Acc@1  64.06 ( 63.84)	Acc@5  87.50 ( 90.21)
Epoch: [10][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9293e-01 (1.2459e+00)	Acc@1  72.66 ( 63.79)	Acc@5  91.41 ( 90.16)
Epoch: [10][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5712e+00 (1.2489e+00)	Acc@1  56.25 ( 63.79)	Acc@5  86.72 ( 90.11)
Epoch: [10][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3701e+00 (1.2496e+00)	Acc@1  64.84 ( 63.75)	Acc@5  86.72 ( 90.06)
Epoch: [10][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2721e+00 (1.2489e+00)	Acc@1  64.06 ( 63.80)	Acc@5  89.84 ( 90.07)
Epoch: [10][310/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2775e+00 (1.2499e+00)	Acc@1  62.50 ( 63.73)	Acc@5  91.41 ( 90.08)
Epoch: [10][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3371e+00 (1.2514e+00)	Acc@1  63.28 ( 63.69)	Acc@5  88.28 ( 90.06)
Epoch: [10][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1005e+00 (1.2530e+00)	Acc@1  64.06 ( 63.63)	Acc@5  92.19 ( 90.04)
Epoch: [10][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6005e+00 (1.2551e+00)	Acc@1  50.78 ( 63.55)	Acc@5  85.16 ( 90.02)
Epoch: [10][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1593e+00 (1.2552e+00)	Acc@1  60.94 ( 63.50)	Acc@5  92.97 ( 90.04)
Epoch: [10][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2126e+00 (1.2552e+00)	Acc@1  64.84 ( 63.55)	Acc@5  89.84 ( 90.03)
Epoch: [10][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3607e-01 (1.2547e+00)	Acc@1  71.88 ( 63.60)	Acc@5  97.66 ( 90.04)
Epoch: [10][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5599e+00 (1.2576e+00)	Acc@1  57.81 ( 63.55)	Acc@5  90.62 ( 90.00)
Epoch: [10][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4410e+00 (1.2585e+00)	Acc@1  57.50 ( 63.52)	Acc@5  88.75 ( 89.97)
## e[10] optimizer.zero_grad (sum) time: 0.12768173217773438
## e[10]       loss.backward (sum) time: 2.523561716079712
## e[10]      optimizer.step (sum) time: 1.010817527770996
## epoch[10] training(only) time: 18.62330937385559
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.6342e+00 (1.6342e+00)	Acc@1  58.00 ( 58.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.6960e+00 (1.6267e+00)	Acc@1  57.00 ( 57.73)	Acc@5  88.00 ( 84.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.4569e+00 (1.5769e+00)	Acc@1  61.00 ( 58.48)	Acc@5  87.00 ( 85.57)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7373e+00 (1.5793e+00)	Acc@1  51.00 ( 57.94)	Acc@5  84.00 ( 85.00)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.6510e+00 (1.5834e+00)	Acc@1  53.00 ( 57.44)	Acc@5  86.00 ( 85.27)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4371e+00 (1.5978e+00)	Acc@1  61.00 ( 57.22)	Acc@5  84.00 ( 84.84)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.5117e+00 (1.5792e+00)	Acc@1  56.00 ( 57.20)	Acc@5  83.00 ( 84.95)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6191e+00 (1.5715e+00)	Acc@1  56.00 ( 57.54)	Acc@5  85.00 ( 85.20)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5921e+00 (1.5833e+00)	Acc@1  58.00 ( 57.40)	Acc@5  84.00 ( 84.98)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6891e+00 (1.5734e+00)	Acc@1  54.00 ( 57.68)	Acc@5  84.00 ( 85.18)
 * Acc@1 57.820 Acc@5 85.220
### epoch[10] execution time: 21.113895177841187
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.172 ( 0.172)	Data  0.131 ( 0.131)	Loss 1.0673e+00 (1.0673e+00)	Acc@1  63.28 ( 63.28)	Acc@5  91.41 ( 91.41)
Epoch: [11][ 10/391]	Time  0.046 ( 0.058)	Data  0.001 ( 0.013)	Loss 1.0194e+00 (1.1651e+00)	Acc@1  68.75 ( 64.91)	Acc@5  91.41 ( 91.12)
Epoch: [11][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.008)	Loss 1.2392e+00 (1.1566e+00)	Acc@1  61.72 ( 65.22)	Acc@5  89.84 ( 91.00)
Epoch: [11][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.006)	Loss 1.0810e+00 (1.1484e+00)	Acc@1  67.97 ( 66.15)	Acc@5  91.41 ( 91.05)
Epoch: [11][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.0006e+00 (1.1476e+00)	Acc@1  69.53 ( 66.20)	Acc@5  92.97 ( 91.03)
Epoch: [11][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.3080e+00 (1.1419e+00)	Acc@1  64.06 ( 66.51)	Acc@5  90.62 ( 91.22)
Epoch: [11][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0841e+00 (1.1431e+00)	Acc@1  69.53 ( 66.79)	Acc@5  92.19 ( 91.33)
Epoch: [11][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.1746e+00 (1.1450e+00)	Acc@1  64.84 ( 66.46)	Acc@5  91.41 ( 91.47)
Epoch: [11][ 80/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2770e+00 (1.1519e+00)	Acc@1  65.62 ( 66.12)	Acc@5  90.62 ( 91.48)
Epoch: [11][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1822e+00 (1.1506e+00)	Acc@1  60.94 ( 66.06)	Acc@5  92.19 ( 91.47)
Epoch: [11][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1766e+00 (1.1471e+00)	Acc@1  69.53 ( 66.19)	Acc@5  90.62 ( 91.55)
Epoch: [11][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2091e+00 (1.1517e+00)	Acc@1  64.06 ( 66.17)	Acc@5  92.97 ( 91.62)
Epoch: [11][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2358e+00 (1.1541e+00)	Acc@1  62.50 ( 66.06)	Acc@5  89.06 ( 91.60)
Epoch: [11][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1141e+00 (1.1555e+00)	Acc@1  69.53 ( 65.93)	Acc@5  89.84 ( 91.53)
Epoch: [11][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1811e+00 (1.1608e+00)	Acc@1  63.28 ( 65.84)	Acc@5  90.62 ( 91.46)
Epoch: [11][150/391]	Time  0.053 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1588e+00 (1.1657e+00)	Acc@1  64.84 ( 65.79)	Acc@5  92.97 ( 91.41)
Epoch: [11][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2358e+00 (1.1609e+00)	Acc@1  65.62 ( 65.93)	Acc@5  93.75 ( 91.49)
Epoch: [11][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0091e+00 (1.1611e+00)	Acc@1  70.31 ( 65.90)	Acc@5  92.97 ( 91.47)
Epoch: [11][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0307e+00 (1.1610e+00)	Acc@1  70.31 ( 65.94)	Acc@5  92.19 ( 91.44)
Epoch: [11][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1764e+00 (1.1625e+00)	Acc@1  66.41 ( 65.93)	Acc@5  90.62 ( 91.40)
Epoch: [11][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0609e+00 (1.1621e+00)	Acc@1  69.53 ( 65.92)	Acc@5  92.97 ( 91.42)
Epoch: [11][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4598e+00 (1.1634e+00)	Acc@1  61.72 ( 65.87)	Acc@5  87.50 ( 91.45)
Epoch: [11][220/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2049e-01 (1.1622e+00)	Acc@1  70.31 ( 65.88)	Acc@5  94.53 ( 91.46)
Epoch: [11][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1141e+00 (1.1635e+00)	Acc@1  64.84 ( 65.84)	Acc@5  92.19 ( 91.46)
Epoch: [11][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3726e+00 (1.1659e+00)	Acc@1  60.16 ( 65.75)	Acc@5  87.50 ( 91.42)
Epoch: [11][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0599e+00 (1.1657e+00)	Acc@1  67.19 ( 65.79)	Acc@5  92.97 ( 91.41)
Epoch: [11][260/391]	Time  0.051 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1992e+00 (1.1676e+00)	Acc@1  64.84 ( 65.76)	Acc@5  88.28 ( 91.35)
Epoch: [11][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1765e+00 (1.1673e+00)	Acc@1  66.41 ( 65.77)	Acc@5  93.75 ( 91.35)
Epoch: [11][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1701e+00 (1.1678e+00)	Acc@1  67.19 ( 65.76)	Acc@5  87.50 ( 91.36)
Epoch: [11][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1850e+00 (1.1662e+00)	Acc@1  69.53 ( 65.80)	Acc@5  89.06 ( 91.37)
Epoch: [11][300/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1247e+00 (1.1665e+00)	Acc@1  65.62 ( 65.75)	Acc@5  91.41 ( 91.35)
Epoch: [11][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9919e-01 (1.1650e+00)	Acc@1  70.31 ( 65.79)	Acc@5  94.53 ( 91.35)
Epoch: [11][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5799e-01 (1.1667e+00)	Acc@1  69.53 ( 65.74)	Acc@5  95.31 ( 91.34)
Epoch: [11][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2676e+00 (1.1699e+00)	Acc@1  65.62 ( 65.66)	Acc@5  87.50 ( 91.27)
Epoch: [11][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1274e+00 (1.1717e+00)	Acc@1  65.62 ( 65.59)	Acc@5  95.31 ( 91.26)
Epoch: [11][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2010e+00 (1.1729e+00)	Acc@1  64.06 ( 65.56)	Acc@5  89.84 ( 91.26)
Epoch: [11][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2918e+00 (1.1742e+00)	Acc@1  64.84 ( 65.56)	Acc@5  85.94 ( 91.24)
Epoch: [11][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0951e+00 (1.1750e+00)	Acc@1  65.62 ( 65.52)	Acc@5  92.19 ( 91.24)
Epoch: [11][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0310e+00 (1.1763e+00)	Acc@1  68.75 ( 65.48)	Acc@5  95.31 ( 91.21)
Epoch: [11][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1352e+00 (1.1768e+00)	Acc@1  63.75 ( 65.47)	Acc@5  96.25 ( 91.22)
## e[11] optimizer.zero_grad (sum) time: 0.12990474700927734
## e[11]       loss.backward (sum) time: 2.531205177307129
## e[11]      optimizer.step (sum) time: 1.0143334865570068
## epoch[11] training(only) time: 18.625582456588745
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.3881e+00 (1.3881e+00)	Acc@1  62.00 ( 62.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5960e+00 (1.5188e+00)	Acc@1  61.00 ( 58.91)	Acc@5  89.00 ( 86.73)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5176e+00 (1.5138e+00)	Acc@1  62.00 ( 59.24)	Acc@5  85.00 ( 86.19)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7887e+00 (1.5380e+00)	Acc@1  48.00 ( 58.42)	Acc@5  88.00 ( 85.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.6665e+00 (1.5331e+00)	Acc@1  54.00 ( 58.41)	Acc@5  85.00 ( 86.07)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3150e+00 (1.5530e+00)	Acc@1  58.00 ( 58.04)	Acc@5  92.00 ( 85.90)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 1.5149e+00 (1.5388e+00)	Acc@1  53.00 ( 58.08)	Acc@5  85.00 ( 86.15)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6738e+00 (1.5378e+00)	Acc@1  56.00 ( 58.13)	Acc@5  85.00 ( 86.18)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6246e+00 (1.5494e+00)	Acc@1  61.00 ( 58.12)	Acc@5  80.00 ( 85.81)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6264e+00 (1.5358e+00)	Acc@1  53.00 ( 58.31)	Acc@5  82.00 ( 86.04)
 * Acc@1 58.520 Acc@5 86.070
### epoch[11] execution time: 21.110332489013672
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.195 ( 0.195)	Data  0.155 ( 0.155)	Loss 1.0040e+00 (1.0040e+00)	Acc@1  71.09 ( 71.09)	Acc@5  91.41 ( 91.41)
Epoch: [12][ 10/391]	Time  0.048 ( 0.061)	Data  0.001 ( 0.016)	Loss 1.2224e+00 (1.0363e+00)	Acc@1  64.84 ( 69.89)	Acc@5  89.84 ( 93.11)
Epoch: [12][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.0102e+00 (1.0155e+00)	Acc@1  75.00 ( 69.64)	Acc@5  96.09 ( 93.56)
Epoch: [12][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 8.7589e-01 (1.0188e+00)	Acc@1  71.88 ( 69.41)	Acc@5  94.53 ( 93.35)
Epoch: [12][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 1.1913e+00 (1.0207e+00)	Acc@1  64.84 ( 69.46)	Acc@5  89.84 ( 93.37)
Epoch: [12][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 8.2381e-01 (1.0301e+00)	Acc@1  74.22 ( 68.98)	Acc@5  96.88 ( 93.26)
Epoch: [12][ 60/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 9.8561e-01 (1.0431e+00)	Acc@1  71.09 ( 68.70)	Acc@5  92.19 ( 93.02)
Epoch: [12][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.9227e-01 (1.0541e+00)	Acc@1  69.53 ( 68.43)	Acc@5  94.53 ( 92.88)
Epoch: [12][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.1589e+00 (1.0566e+00)	Acc@1  67.97 ( 68.32)	Acc@5  89.06 ( 92.92)
Epoch: [12][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.2097e+00 (1.0580e+00)	Acc@1  61.72 ( 68.35)	Acc@5  93.75 ( 92.93)
Epoch: [12][100/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 9.8364e-01 (1.0611e+00)	Acc@1  75.00 ( 68.39)	Acc@5  91.41 ( 92.95)
Epoch: [12][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.8322e-01 (1.0615e+00)	Acc@1  76.56 ( 68.30)	Acc@5  96.09 ( 92.92)
Epoch: [12][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.2355e-01 (1.0624e+00)	Acc@1  78.91 ( 68.42)	Acc@5  96.09 ( 92.92)
Epoch: [12][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1117e+00 (1.0589e+00)	Acc@1  71.09 ( 68.55)	Acc@5  89.06 ( 92.90)
Epoch: [12][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1285e+00 (1.0626e+00)	Acc@1  64.84 ( 68.42)	Acc@5  92.97 ( 92.88)
Epoch: [12][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0985e+00 (1.0681e+00)	Acc@1  71.88 ( 68.30)	Acc@5  92.97 ( 92.80)
Epoch: [12][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.9258e-01 (1.0631e+00)	Acc@1  70.31 ( 68.47)	Acc@5  94.53 ( 92.86)
Epoch: [12][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2282e+00 (1.0659e+00)	Acc@1  61.72 ( 68.43)	Acc@5  93.75 ( 92.80)
Epoch: [12][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2574e+00 (1.0679e+00)	Acc@1  67.97 ( 68.43)	Acc@5  87.50 ( 92.77)
Epoch: [12][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.9157e-01 (1.0665e+00)	Acc@1  72.66 ( 68.46)	Acc@5  94.53 ( 92.78)
Epoch: [12][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3739e+00 (1.0704e+00)	Acc@1  66.41 ( 68.43)	Acc@5  87.50 ( 92.72)
Epoch: [12][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1072e+00 (1.0736e+00)	Acc@1  69.53 ( 68.32)	Acc@5  89.84 ( 92.67)
Epoch: [12][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2414e+00 (1.0772e+00)	Acc@1  66.41 ( 68.19)	Acc@5  89.06 ( 92.61)
Epoch: [12][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0733e+00 (1.0784e+00)	Acc@1  68.75 ( 68.18)	Acc@5  92.97 ( 92.64)
Epoch: [12][240/391]	Time  0.051 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0079e+00 (1.0802e+00)	Acc@1  71.88 ( 68.16)	Acc@5  93.75 ( 92.58)
Epoch: [12][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0102e+00 (1.0823e+00)	Acc@1  72.66 ( 68.17)	Acc@5  92.19 ( 92.53)
Epoch: [12][260/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0188e+00 (1.0812e+00)	Acc@1  67.19 ( 68.16)	Acc@5  94.53 ( 92.55)
Epoch: [12][270/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1666e+00 (1.0831e+00)	Acc@1  64.84 ( 68.01)	Acc@5  90.62 ( 92.54)
Epoch: [12][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0188e+00 (1.0860e+00)	Acc@1  72.66 ( 67.96)	Acc@5  92.97 ( 92.50)
Epoch: [12][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1372e+00 (1.0862e+00)	Acc@1  67.97 ( 67.96)	Acc@5  88.28 ( 92.49)
Epoch: [12][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1376e+00 (1.0847e+00)	Acc@1  67.97 ( 68.06)	Acc@5  91.41 ( 92.50)
Epoch: [12][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1660e+00 (1.0868e+00)	Acc@1  66.41 ( 68.02)	Acc@5  88.28 ( 92.47)
Epoch: [12][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7014e-01 (1.0895e+00)	Acc@1  68.75 ( 67.98)	Acc@5  92.19 ( 92.40)
Epoch: [12][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2429e+00 (1.0917e+00)	Acc@1  63.28 ( 67.90)	Acc@5  89.84 ( 92.38)
Epoch: [12][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2261e+00 (1.0934e+00)	Acc@1  64.06 ( 67.85)	Acc@5  91.41 ( 92.36)
Epoch: [12][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1994e+00 (1.0930e+00)	Acc@1  66.41 ( 67.86)	Acc@5  90.62 ( 92.37)
Epoch: [12][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0551e+00 (1.0950e+00)	Acc@1  66.41 ( 67.84)	Acc@5  90.62 ( 92.32)
Epoch: [12][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2187e+00 (1.0975e+00)	Acc@1  64.06 ( 67.78)	Acc@5  91.41 ( 92.29)
Epoch: [12][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0811e+00 (1.0992e+00)	Acc@1  65.62 ( 67.75)	Acc@5  91.41 ( 92.25)
Epoch: [12][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2223e+00 (1.0989e+00)	Acc@1  60.00 ( 67.76)	Acc@5  88.75 ( 92.24)
## e[12] optimizer.zero_grad (sum) time: 0.12949132919311523
## e[12]       loss.backward (sum) time: 2.5205938816070557
## e[12]      optimizer.step (sum) time: 1.0092132091522217
## epoch[12] training(only) time: 18.638593435287476
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.5435e+00 (1.5435e+00)	Acc@1  57.00 ( 57.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.6356e+00 (1.5417e+00)	Acc@1  63.00 ( 58.91)	Acc@5  89.00 ( 86.91)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4582e+00 (1.4809e+00)	Acc@1  63.00 ( 60.38)	Acc@5  86.00 ( 87.14)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7679e+00 (1.4997e+00)	Acc@1  45.00 ( 59.84)	Acc@5  82.00 ( 86.58)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.6005e+00 (1.5050e+00)	Acc@1  62.00 ( 59.73)	Acc@5  89.00 ( 86.63)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3670e+00 (1.5278e+00)	Acc@1  65.00 ( 59.29)	Acc@5  89.00 ( 86.24)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3442e+00 (1.5094e+00)	Acc@1  63.00 ( 59.52)	Acc@5  88.00 ( 86.75)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5565e+00 (1.5168e+00)	Acc@1  57.00 ( 59.30)	Acc@5  83.00 ( 86.62)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5581e+00 (1.5178e+00)	Acc@1  60.00 ( 59.10)	Acc@5  87.00 ( 86.56)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6996e+00 (1.5053e+00)	Acc@1  57.00 ( 59.38)	Acc@5  85.00 ( 86.81)
 * Acc@1 59.290 Acc@5 86.800
### epoch[12] execution time: 21.12848711013794
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.192 ( 0.192)	Data  0.146 ( 0.146)	Loss 8.9275e-01 (8.9275e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.88 ( 96.88)
Epoch: [13][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.015)	Loss 9.8566e-01 (9.9532e-01)	Acc@1  71.88 ( 70.24)	Acc@5  95.31 ( 94.53)
Epoch: [13][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 9.4353e-01 (9.7958e-01)	Acc@1  72.66 ( 70.28)	Acc@5  96.09 ( 94.79)
Epoch: [13][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 8.6192e-01 (9.7408e-01)	Acc@1  72.66 ( 70.64)	Acc@5  96.88 ( 94.41)
Epoch: [13][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.1035e+00 (9.7935e-01)	Acc@1  67.97 ( 70.69)	Acc@5  92.97 ( 94.11)
Epoch: [13][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 8.6294e-01 (9.7993e-01)	Acc@1  73.44 ( 70.83)	Acc@5  92.19 ( 94.01)
Epoch: [13][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0072e+00 (9.7049e-01)	Acc@1  67.97 ( 71.12)	Acc@5  94.53 ( 94.06)
Epoch: [13][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 8.8637e-01 (9.6207e-01)	Acc@1  74.22 ( 71.35)	Acc@5  94.53 ( 94.07)
Epoch: [13][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 9.0535e-01 (9.5673e-01)	Acc@1  73.44 ( 71.52)	Acc@5  93.75 ( 94.08)
Epoch: [13][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.0050e+00 (9.6057e-01)	Acc@1  67.19 ( 71.41)	Acc@5  93.75 ( 93.90)
Epoch: [13][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0105e+00 (9.7050e-01)	Acc@1  67.97 ( 71.27)	Acc@5  94.53 ( 93.71)
Epoch: [13][110/391]	Time  0.043 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.2096e-01 (9.7908e-01)	Acc@1  70.31 ( 71.09)	Acc@5  95.31 ( 93.60)
Epoch: [13][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1471e+00 (9.8692e-01)	Acc@1  66.41 ( 70.85)	Acc@5  90.62 ( 93.48)
Epoch: [13][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.9772e-01 (9.8517e-01)	Acc@1  77.34 ( 70.92)	Acc@5  95.31 ( 93.48)
Epoch: [13][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.8585e-01 (9.8577e-01)	Acc@1  69.53 ( 70.87)	Acc@5  92.97 ( 93.47)
Epoch: [13][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0953e+00 (9.8980e-01)	Acc@1  65.62 ( 70.75)	Acc@5  93.75 ( 93.40)
Epoch: [13][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.3670e-01 (9.9284e-01)	Acc@1  75.78 ( 70.76)	Acc@5  94.53 ( 93.32)
Epoch: [13][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0090e+00 (9.9288e-01)	Acc@1  68.75 ( 70.72)	Acc@5  93.75 ( 93.36)
Epoch: [13][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0111e+00 (9.9329e-01)	Acc@1  73.44 ( 70.69)	Acc@5  93.75 ( 93.39)
Epoch: [13][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.5013e-01 (9.9262e-01)	Acc@1  78.91 ( 70.70)	Acc@5  92.19 ( 93.42)
Epoch: [13][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.4061e-01 (9.9280e-01)	Acc@1  72.66 ( 70.65)	Acc@5  93.75 ( 93.43)
Epoch: [13][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0169e+00 (9.9625e-01)	Acc@1  69.53 ( 70.56)	Acc@5  92.97 ( 93.40)
Epoch: [13][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.2845e-01 (1.0003e+00)	Acc@1  72.66 ( 70.39)	Acc@5  94.53 ( 93.37)
Epoch: [13][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.0130e-01 (9.9746e-01)	Acc@1  71.09 ( 70.41)	Acc@5  95.31 ( 93.41)
Epoch: [13][240/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.0113e-01 (9.9910e-01)	Acc@1  72.66 ( 70.34)	Acc@5  94.53 ( 93.40)
Epoch: [13][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.9137e-01 (1.0000e+00)	Acc@1  75.00 ( 70.27)	Acc@5  97.66 ( 93.41)
Epoch: [13][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1901e+00 (1.0021e+00)	Acc@1  66.41 ( 70.17)	Acc@5  91.41 ( 93.41)
Epoch: [13][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1628e+00 (1.0045e+00)	Acc@1  64.84 ( 70.05)	Acc@5  92.97 ( 93.42)
Epoch: [13][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0126e+00 (1.0055e+00)	Acc@1  64.84 ( 70.01)	Acc@5  94.53 ( 93.40)
Epoch: [13][290/391]	Time  0.047 ( 0.047)	Data  0.002 ( 0.003)	Loss 1.0773e+00 (1.0061e+00)	Acc@1  69.53 ( 70.02)	Acc@5  92.97 ( 93.39)
Epoch: [13][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2441e+00 (1.0085e+00)	Acc@1  64.06 ( 69.98)	Acc@5  89.84 ( 93.35)
Epoch: [13][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4117e+00 (1.0102e+00)	Acc@1  58.59 ( 69.95)	Acc@5  85.16 ( 93.32)
Epoch: [13][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9097e-01 (1.0122e+00)	Acc@1  67.19 ( 69.90)	Acc@5  96.09 ( 93.29)
Epoch: [13][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1419e+00 (1.0151e+00)	Acc@1  71.88 ( 69.84)	Acc@5  92.19 ( 93.23)
Epoch: [13][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0860e+00 (1.0185e+00)	Acc@1  65.62 ( 69.71)	Acc@5  93.75 ( 93.18)
Epoch: [13][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.6643e-01 (1.0209e+00)	Acc@1  71.09 ( 69.64)	Acc@5  95.31 ( 93.14)
Epoch: [13][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1410e+00 (1.0225e+00)	Acc@1  64.84 ( 69.58)	Acc@5  90.62 ( 93.12)
Epoch: [13][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0277e+00 (1.0244e+00)	Acc@1  71.09 ( 69.55)	Acc@5  90.62 ( 93.06)
Epoch: [13][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9158e-01 (1.0254e+00)	Acc@1  67.19 ( 69.50)	Acc@5  92.97 ( 93.05)
Epoch: [13][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0022e+00 (1.0256e+00)	Acc@1  66.25 ( 69.49)	Acc@5  95.00 ( 93.06)
## e[13] optimizer.zero_grad (sum) time: 0.12871956825256348
## e[13]       loss.backward (sum) time: 2.5421359539031982
## e[13]      optimizer.step (sum) time: 0.9942491054534912
## epoch[13] training(only) time: 18.63241934776306
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.7027e+00 (1.7027e+00)	Acc@1  61.00 ( 61.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.6142e+00 (1.5383e+00)	Acc@1  61.00 ( 61.45)	Acc@5  89.00 ( 86.64)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.5028e+00 (1.4958e+00)	Acc@1  65.00 ( 61.52)	Acc@5  87.00 ( 86.33)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6747e+00 (1.5009e+00)	Acc@1  54.00 ( 60.71)	Acc@5  82.00 ( 85.90)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.5043e+00 (1.4944e+00)	Acc@1  59.00 ( 60.27)	Acc@5  90.00 ( 86.54)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4671e+00 (1.5098e+00)	Acc@1  61.00 ( 59.94)	Acc@5  84.00 ( 86.18)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.2713e+00 (1.4919e+00)	Acc@1  65.00 ( 60.02)	Acc@5  90.00 ( 86.61)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.8247e+00 (1.4934e+00)	Acc@1  54.00 ( 60.06)	Acc@5  84.00 ( 86.69)
Test: [ 80/100]	Time  0.024 ( 0.024)	Loss 1.5362e+00 (1.5085e+00)	Acc@1  59.00 ( 59.91)	Acc@5  87.00 ( 86.48)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8094e+00 (1.4969e+00)	Acc@1  54.00 ( 60.15)	Acc@5  85.00 ( 86.57)
 * Acc@1 60.390 Acc@5 86.670
### epoch[13] execution time: 21.11902666091919
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.181 ( 0.181)	Data  0.142 ( 0.142)	Loss 1.0244e+00 (1.0244e+00)	Acc@1  66.41 ( 66.41)	Acc@5  92.97 ( 92.97)
Epoch: [14][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 1.0388e+00 (9.6898e-01)	Acc@1  72.66 ( 72.09)	Acc@5  94.53 ( 93.04)
Epoch: [14][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 8.3722e-01 (9.4491e-01)	Acc@1  72.66 ( 71.91)	Acc@5  96.88 ( 93.75)
Epoch: [14][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 6.6153e-01 (9.1478e-01)	Acc@1  76.56 ( 72.68)	Acc@5  96.09 ( 93.95)
Epoch: [14][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 8.0883e-01 (8.9411e-01)	Acc@1  77.34 ( 73.08)	Acc@5  93.75 ( 94.21)
Epoch: [14][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 8.4958e-01 (8.8129e-01)	Acc@1  71.09 ( 73.39)	Acc@5  95.31 ( 94.38)
Epoch: [14][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.2032e-01 (8.9242e-01)	Acc@1  71.09 ( 73.03)	Acc@5  95.31 ( 94.24)
Epoch: [14][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.5336e-01 (8.9273e-01)	Acc@1  76.56 ( 73.15)	Acc@5  92.19 ( 94.11)
Epoch: [14][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 8.3287e-01 (8.8784e-01)	Acc@1  72.66 ( 73.20)	Acc@5  96.88 ( 94.27)
Epoch: [14][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.6278e-01 (8.9573e-01)	Acc@1  73.44 ( 72.92)	Acc@5  95.31 ( 94.21)
Epoch: [14][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0865e+00 (9.0036e-01)	Acc@1  66.41 ( 72.79)	Acc@5  95.31 ( 94.24)
Epoch: [14][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1327e+00 (9.0063e-01)	Acc@1  64.84 ( 72.67)	Acc@5  92.19 ( 94.29)
Epoch: [14][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.8651e-01 (9.0293e-01)	Acc@1  68.75 ( 72.63)	Acc@5  96.09 ( 94.38)
Epoch: [14][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.3886e-01 (9.0292e-01)	Acc@1  79.69 ( 72.64)	Acc@5  99.22 ( 94.41)
Epoch: [14][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.5215e-01 (9.0730e-01)	Acc@1  72.66 ( 72.55)	Acc@5  92.97 ( 94.36)
Epoch: [14][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0157e+00 (9.1196e-01)	Acc@1  74.22 ( 72.55)	Acc@5  92.97 ( 94.34)
Epoch: [14][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.9017e-01 (9.1219e-01)	Acc@1  78.12 ( 72.54)	Acc@5  95.31 ( 94.35)
Epoch: [14][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0566e+00 (9.1564e-01)	Acc@1  68.75 ( 72.39)	Acc@5  92.97 ( 94.30)
Epoch: [14][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.6208e-01 (9.2071e-01)	Acc@1  75.00 ( 72.25)	Acc@5  95.31 ( 94.22)
Epoch: [14][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1717e+00 (9.2391e-01)	Acc@1  63.28 ( 72.21)	Acc@5  90.62 ( 94.16)
Epoch: [14][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.0614e-01 (9.2394e-01)	Acc@1  77.34 ( 72.21)	Acc@5  93.75 ( 94.15)
Epoch: [14][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2068e+00 (9.3089e-01)	Acc@1  64.06 ( 71.97)	Acc@5  91.41 ( 94.10)
Epoch: [14][220/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9717e-01 (9.3221e-01)	Acc@1  71.09 ( 71.91)	Acc@5  92.97 ( 94.09)
Epoch: [14][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0887e+00 (9.3696e-01)	Acc@1  66.41 ( 71.79)	Acc@5  92.19 ( 94.04)
Epoch: [14][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3918e-01 (9.3648e-01)	Acc@1  71.88 ( 71.78)	Acc@5  92.97 ( 94.06)
Epoch: [14][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5478e-01 (9.3818e-01)	Acc@1  71.88 ( 71.71)	Acc@5  94.53 ( 94.05)
Epoch: [14][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.9531e-01 (9.4057e-01)	Acc@1  71.88 ( 71.67)	Acc@5  93.75 ( 94.00)
Epoch: [14][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0254e+00 (9.4431e-01)	Acc@1  71.88 ( 71.60)	Acc@5  92.19 ( 93.93)
Epoch: [14][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1372e+00 (9.4609e-01)	Acc@1  64.84 ( 71.55)	Acc@5  89.84 ( 93.95)
Epoch: [14][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0194e+00 (9.4682e-01)	Acc@1  71.09 ( 71.52)	Acc@5  91.41 ( 93.95)
Epoch: [14][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0084e+00 (9.5000e-01)	Acc@1  67.97 ( 71.43)	Acc@5  92.19 ( 93.93)
Epoch: [14][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0230e+00 (9.5261e-01)	Acc@1  65.62 ( 71.35)	Acc@5  92.97 ( 93.91)
Epoch: [14][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0519e+00 (9.5377e-01)	Acc@1  67.19 ( 71.29)	Acc@5  89.06 ( 93.91)
Epoch: [14][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1373e+00 (9.5489e-01)	Acc@1  61.72 ( 71.28)	Acc@5  92.19 ( 93.89)
Epoch: [14][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1135e+00 (9.5557e-01)	Acc@1  64.06 ( 71.27)	Acc@5  93.75 ( 93.89)
Epoch: [14][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1261e+00 (9.5562e-01)	Acc@1  63.28 ( 71.23)	Acc@5  92.19 ( 93.89)
Epoch: [14][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0992e+00 (9.5743e-01)	Acc@1  67.19 ( 71.20)	Acc@5  91.41 ( 93.87)
Epoch: [14][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.6407e-01 (9.5824e-01)	Acc@1  76.56 ( 71.18)	Acc@5  91.41 ( 93.87)
Epoch: [14][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2713e+00 (9.6055e-01)	Acc@1  64.84 ( 71.14)	Acc@5  88.28 ( 93.82)
Epoch: [14][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0461e+00 (9.6275e-01)	Acc@1  65.00 ( 71.11)	Acc@5  93.75 ( 93.77)
## e[14] optimizer.zero_grad (sum) time: 0.12977361679077148
## e[14]       loss.backward (sum) time: 2.5357470512390137
## e[14]      optimizer.step (sum) time: 1.0159144401550293
## epoch[14] training(only) time: 18.59704828262329
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.5893e+00 (1.5893e+00)	Acc@1  57.00 ( 57.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5493e+00 (1.4964e+00)	Acc@1  67.00 ( 63.00)	Acc@5  92.00 ( 87.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2952e+00 (1.4718e+00)	Acc@1  61.00 ( 61.67)	Acc@5  91.00 ( 87.33)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.0287e+00 (1.5211e+00)	Acc@1  45.00 ( 60.26)	Acc@5  79.00 ( 86.61)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4402e+00 (1.5092e+00)	Acc@1  66.00 ( 60.39)	Acc@5  91.00 ( 87.17)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.2179e+00 (1.5175e+00)	Acc@1  64.00 ( 59.88)	Acc@5  91.00 ( 87.06)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4653e+00 (1.4945e+00)	Acc@1  63.00 ( 60.25)	Acc@5  86.00 ( 87.23)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6467e+00 (1.5033e+00)	Acc@1  53.00 ( 60.15)	Acc@5  87.00 ( 87.00)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6504e+00 (1.5062e+00)	Acc@1  61.00 ( 60.07)	Acc@5  84.00 ( 86.89)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 1.6576e+00 (1.4993e+00)	Acc@1  59.00 ( 60.25)	Acc@5  83.00 ( 87.00)
 * Acc@1 60.460 Acc@5 87.100
### epoch[14] execution time: 21.091623067855835
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.185 ( 0.185)	Data  0.146 ( 0.146)	Loss 8.5845e-01 (8.5845e-01)	Acc@1  75.00 ( 75.00)	Acc@5  95.31 ( 95.31)
Epoch: [15][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.016)	Loss 7.9740e-01 (8.3570e-01)	Acc@1  76.56 ( 74.36)	Acc@5  96.88 ( 96.24)
Epoch: [15][ 20/391]	Time  0.048 ( 0.053)	Data  0.001 ( 0.009)	Loss 9.1386e-01 (8.3058e-01)	Acc@1  70.31 ( 74.63)	Acc@5  96.09 ( 95.83)
Epoch: [15][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 9.0079e-01 (8.4870e-01)	Acc@1  72.66 ( 74.32)	Acc@5  96.09 ( 95.56)
Epoch: [15][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 9.7241e-01 (8.4029e-01)	Acc@1  67.97 ( 74.56)	Acc@5  97.66 ( 95.52)
Epoch: [15][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 8.2542e-01 (8.4229e-01)	Acc@1  73.44 ( 74.48)	Acc@5  94.53 ( 95.37)
Epoch: [15][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 6.9572e-01 (8.3316e-01)	Acc@1  79.69 ( 74.83)	Acc@5  95.31 ( 95.59)
Epoch: [15][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0387e+00 (8.3618e-01)	Acc@1  69.53 ( 74.72)	Acc@5  91.41 ( 95.44)
Epoch: [15][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 7.5635e-01 (8.3292e-01)	Acc@1  71.88 ( 74.67)	Acc@5  97.66 ( 95.43)
Epoch: [15][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 7.0386e-01 (8.4024e-01)	Acc@1  79.69 ( 74.51)	Acc@5  96.88 ( 95.36)
Epoch: [15][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.7981e-01 (8.4574e-01)	Acc@1  76.56 ( 74.30)	Acc@5  96.88 ( 95.27)
Epoch: [15][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0657e+00 (8.5345e-01)	Acc@1  74.22 ( 74.12)	Acc@5  90.62 ( 95.16)
Epoch: [15][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.6373e-01 (8.5774e-01)	Acc@1  76.56 ( 73.98)	Acc@5  97.66 ( 95.14)
Epoch: [15][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0267e+00 (8.6591e-01)	Acc@1  71.09 ( 73.85)	Acc@5  94.53 ( 95.03)
Epoch: [15][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.7049e-01 (8.7000e-01)	Acc@1  77.34 ( 73.76)	Acc@5  93.75 ( 94.99)
Epoch: [15][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0465e+00 (8.7316e-01)	Acc@1  71.09 ( 73.68)	Acc@5  94.53 ( 94.99)
Epoch: [15][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.9830e-01 (8.7536e-01)	Acc@1  70.31 ( 73.59)	Acc@5  92.97 ( 94.94)
Epoch: [15][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.6439e-01 (8.7779e-01)	Acc@1  74.22 ( 73.57)	Acc@5  92.19 ( 94.86)
Epoch: [15][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0016e+00 (8.7562e-01)	Acc@1  72.66 ( 73.67)	Acc@5  92.97 ( 94.92)
Epoch: [15][190/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.6046e-01 (8.7702e-01)	Acc@1  79.69 ( 73.66)	Acc@5  98.44 ( 94.92)
Epoch: [15][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.5350e-01 (8.7826e-01)	Acc@1  70.31 ( 73.58)	Acc@5  94.53 ( 94.91)
Epoch: [15][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.7581e-01 (8.7906e-01)	Acc@1  71.88 ( 73.58)	Acc@5  95.31 ( 94.84)
Epoch: [15][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.6289e-01 (8.7802e-01)	Acc@1  77.34 ( 73.58)	Acc@5  92.97 ( 94.83)
Epoch: [15][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.1981e-01 (8.8154e-01)	Acc@1  71.09 ( 73.46)	Acc@5  96.09 ( 94.81)
Epoch: [15][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0273e+00 (8.8466e-01)	Acc@1  69.53 ( 73.35)	Acc@5  91.41 ( 94.79)
Epoch: [15][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.9153e-01 (8.8787e-01)	Acc@1  67.97 ( 73.24)	Acc@5  94.53 ( 94.80)
Epoch: [15][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.8470e-01 (8.9014e-01)	Acc@1  69.53 ( 73.16)	Acc@5  91.41 ( 94.79)
Epoch: [15][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.1757e-01 (8.9194e-01)	Acc@1  75.78 ( 73.10)	Acc@5  96.09 ( 94.78)
Epoch: [15][280/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.9187e-01 (8.9058e-01)	Acc@1  79.69 ( 73.15)	Acc@5  97.66 ( 94.79)
Epoch: [15][290/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.3420e-01 (8.9262e-01)	Acc@1  75.00 ( 73.09)	Acc@5  95.31 ( 94.79)
Epoch: [15][300/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1036e+00 (8.9594e-01)	Acc@1  62.50 ( 73.01)	Acc@5  95.31 ( 94.76)
Epoch: [15][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3725e-01 (8.9713e-01)	Acc@1  71.88 ( 72.97)	Acc@5  95.31 ( 94.73)
Epoch: [15][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1002e+00 (8.9722e-01)	Acc@1  68.75 ( 72.96)	Acc@5  90.62 ( 94.72)
Epoch: [15][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.8565e-01 (8.9890e-01)	Acc@1  72.66 ( 72.91)	Acc@5  92.19 ( 94.68)
Epoch: [15][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6121e-01 (8.9912e-01)	Acc@1  75.00 ( 72.90)	Acc@5  97.66 ( 94.68)
Epoch: [15][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1414e+00 (8.9914e-01)	Acc@1  66.41 ( 72.93)	Acc@5  91.41 ( 94.67)
Epoch: [15][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1955e-01 (9.0096e-01)	Acc@1  71.88 ( 72.89)	Acc@5  94.53 ( 94.65)
Epoch: [15][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0414e+00 (9.0176e-01)	Acc@1  67.19 ( 72.89)	Acc@5  91.41 ( 94.64)
Epoch: [15][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2850e-01 (9.0118e-01)	Acc@1  74.22 ( 72.90)	Acc@5  94.53 ( 94.64)
Epoch: [15][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4398e-01 (9.0230e-01)	Acc@1  78.75 ( 72.87)	Acc@5  95.00 ( 94.63)
## e[15] optimizer.zero_grad (sum) time: 0.12847256660461426
## e[15]       loss.backward (sum) time: 2.519070625305176
## e[15]      optimizer.step (sum) time: 1.0037121772766113
## epoch[15] training(only) time: 18.636072635650635
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 1.5114e+00 (1.5114e+00)	Acc@1  66.00 ( 66.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.023 ( 0.037)	Loss 1.6592e+00 (1.5266e+00)	Acc@1  60.00 ( 62.00)	Acc@5  89.00 ( 86.45)
Test: [ 20/100]	Time  0.022 ( 0.030)	Loss 1.3987e+00 (1.4564e+00)	Acc@1  63.00 ( 61.95)	Acc@5  90.00 ( 87.29)
Test: [ 30/100]	Time  0.023 ( 0.028)	Loss 1.6506e+00 (1.4909e+00)	Acc@1  56.00 ( 61.13)	Acc@5  87.00 ( 86.84)
Test: [ 40/100]	Time  0.023 ( 0.027)	Loss 1.4161e+00 (1.4827e+00)	Acc@1  59.00 ( 60.85)	Acc@5  90.00 ( 87.44)
Test: [ 50/100]	Time  0.023 ( 0.026)	Loss 1.4231e+00 (1.5017e+00)	Acc@1  64.00 ( 60.73)	Acc@5  87.00 ( 86.82)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4476e+00 (1.4878e+00)	Acc@1  61.00 ( 61.05)	Acc@5  87.00 ( 87.05)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.7477e+00 (1.4979e+00)	Acc@1  59.00 ( 60.99)	Acc@5  83.00 ( 86.87)
Test: [ 80/100]	Time  0.022 ( 0.025)	Loss 1.4091e+00 (1.5107e+00)	Acc@1  68.00 ( 61.02)	Acc@5  88.00 ( 86.74)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6981e+00 (1.4988e+00)	Acc@1  54.00 ( 61.12)	Acc@5  86.00 ( 86.96)
 * Acc@1 61.280 Acc@5 87.150
### epoch[15] execution time: 21.151411056518555
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.183 ( 0.183)	Data  0.144 ( 0.144)	Loss 6.4778e-01 (6.4778e-01)	Acc@1  77.34 ( 77.34)	Acc@5  97.66 ( 97.66)
Epoch: [16][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 7.7951e-01 (7.2165e-01)	Acc@1  75.78 ( 77.84)	Acc@5  94.53 ( 96.09)
Epoch: [16][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 8.8153e-01 (7.4478e-01)	Acc@1  75.00 ( 77.86)	Acc@5  93.75 ( 96.13)
Epoch: [16][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 7.3461e-01 (7.5803e-01)	Acc@1  73.44 ( 77.44)	Acc@5  97.66 ( 96.02)
Epoch: [16][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 6.4533e-01 (7.6562e-01)	Acc@1  79.69 ( 77.50)	Acc@5  98.44 ( 95.96)
Epoch: [16][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 5.0363e-01 (7.5867e-01)	Acc@1  84.38 ( 77.45)	Acc@5  99.22 ( 96.05)
Epoch: [16][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.6302e-01 (7.6331e-01)	Acc@1  77.34 ( 77.27)	Acc@5  95.31 ( 95.89)
Epoch: [16][ 70/391]	Time  0.048 ( 0.049)	Data  0.001 ( 0.005)	Loss 8.2661e-01 (7.6837e-01)	Acc@1  76.56 ( 76.94)	Acc@5  94.53 ( 95.82)
Epoch: [16][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.1074e+00 (7.7584e-01)	Acc@1  60.94 ( 76.70)	Acc@5  92.19 ( 95.76)
Epoch: [16][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.6231e-01 (7.7973e-01)	Acc@1  75.00 ( 76.67)	Acc@5  95.31 ( 95.66)
Epoch: [16][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.9394e-01 (7.8622e-01)	Acc@1  75.78 ( 76.55)	Acc@5  93.75 ( 95.54)
Epoch: [16][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.4194e-01 (7.8596e-01)	Acc@1  71.88 ( 76.60)	Acc@5  92.97 ( 95.50)
Epoch: [16][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.8003e-01 (7.8887e-01)	Acc@1  78.91 ( 76.46)	Acc@5  97.66 ( 95.47)
Epoch: [16][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.5709e-01 (7.8948e-01)	Acc@1  78.91 ( 76.41)	Acc@5  96.88 ( 95.50)
Epoch: [16][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.4428e-01 (7.9364e-01)	Acc@1  76.56 ( 76.22)	Acc@5  95.31 ( 95.48)
Epoch: [16][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0191e+00 (7.9900e-01)	Acc@1  68.75 ( 76.06)	Acc@5  93.75 ( 95.49)
Epoch: [16][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.5568e-01 (8.0471e-01)	Acc@1  77.34 ( 75.78)	Acc@5  96.09 ( 95.45)
Epoch: [16][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.4606e-01 (8.0880e-01)	Acc@1  74.22 ( 75.68)	Acc@5  96.09 ( 95.45)
Epoch: [16][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.7221e-01 (8.1630e-01)	Acc@1  75.00 ( 75.49)	Acc@5  94.53 ( 95.40)
Epoch: [16][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.6992e-01 (8.1556e-01)	Acc@1  78.12 ( 75.50)	Acc@5  97.66 ( 95.40)
Epoch: [16][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0132e+00 (8.2005e-01)	Acc@1  69.53 ( 75.35)	Acc@5  90.62 ( 95.35)
Epoch: [16][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0116e+00 (8.2396e-01)	Acc@1  74.22 ( 75.30)	Acc@5  92.97 ( 95.28)
Epoch: [16][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.4296e-01 (8.2662e-01)	Acc@1  75.78 ( 75.21)	Acc@5  95.31 ( 95.25)
Epoch: [16][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.6745e-01 (8.2530e-01)	Acc@1  74.22 ( 75.22)	Acc@5  95.31 ( 95.28)
Epoch: [16][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.4804e-01 (8.2587e-01)	Acc@1  73.44 ( 75.22)	Acc@5  93.75 ( 95.27)
Epoch: [16][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3438e-01 (8.2838e-01)	Acc@1  69.53 ( 75.14)	Acc@5  94.53 ( 95.25)
Epoch: [16][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3676e-01 (8.3219e-01)	Acc@1  72.66 ( 74.96)	Acc@5  94.53 ( 95.19)
Epoch: [16][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0545e-01 (8.3427e-01)	Acc@1  70.31 ( 74.87)	Acc@5  97.66 ( 95.21)
Epoch: [16][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1244e-01 (8.3483e-01)	Acc@1  78.12 ( 74.87)	Acc@5  94.53 ( 95.22)
Epoch: [16][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3132e-01 (8.3896e-01)	Acc@1  77.34 ( 74.73)	Acc@5  96.09 ( 95.20)
Epoch: [16][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0445e+00 (8.4172e-01)	Acc@1  71.09 ( 74.61)	Acc@5  92.19 ( 95.18)
Epoch: [16][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2969e-01 (8.4267e-01)	Acc@1  78.91 ( 74.57)	Acc@5  96.88 ( 95.19)
Epoch: [16][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4638e-01 (8.4348e-01)	Acc@1  77.34 ( 74.53)	Acc@5  96.09 ( 95.19)
Epoch: [16][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8165e-01 (8.4313e-01)	Acc@1  78.12 ( 74.50)	Acc@5  94.53 ( 95.19)
Epoch: [16][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0243e-01 (8.4264e-01)	Acc@1  74.22 ( 74.56)	Acc@5  94.53 ( 95.19)
Epoch: [16][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3830e-01 (8.4405e-01)	Acc@1  72.66 ( 74.53)	Acc@5  93.75 ( 95.16)
Epoch: [16][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7721e-01 (8.4617e-01)	Acc@1  74.22 ( 74.44)	Acc@5  98.44 ( 95.17)
Epoch: [16][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.9886e-01 (8.4611e-01)	Acc@1  71.88 ( 74.46)	Acc@5  93.75 ( 95.16)
Epoch: [16][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0826e-01 (8.4778e-01)	Acc@1  73.44 ( 74.42)	Acc@5  96.09 ( 95.14)
Epoch: [16][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2091e+00 (8.5031e-01)	Acc@1  65.00 ( 74.32)	Acc@5  91.25 ( 95.11)
## e[16] optimizer.zero_grad (sum) time: 0.13066482543945312
## e[16]       loss.backward (sum) time: 2.521886110305786
## e[16]      optimizer.step (sum) time: 0.9959383010864258
## epoch[16] training(only) time: 18.586829900741577
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.3807e+00 (1.3807e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.024 ( 0.036)	Loss 1.6066e+00 (1.5842e+00)	Acc@1  64.00 ( 62.00)	Acc@5  89.00 ( 86.55)
Test: [ 20/100]	Time  0.024 ( 0.030)	Loss 1.5725e+00 (1.4898e+00)	Acc@1  61.00 ( 62.71)	Acc@5  88.00 ( 87.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6551e+00 (1.5050e+00)	Acc@1  54.00 ( 61.65)	Acc@5  85.00 ( 87.19)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3133e+00 (1.5094e+00)	Acc@1  64.00 ( 61.24)	Acc@5  93.00 ( 87.12)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4831e+00 (1.5229e+00)	Acc@1  63.00 ( 60.92)	Acc@5  86.00 ( 86.86)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4122e+00 (1.5020e+00)	Acc@1  62.00 ( 61.00)	Acc@5  89.00 ( 87.16)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6025e+00 (1.4953e+00)	Acc@1  56.00 ( 61.11)	Acc@5  87.00 ( 87.27)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.4933e+00 (1.4934e+00)	Acc@1  68.00 ( 61.41)	Acc@5  84.00 ( 87.22)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.9886e+00 (1.4859e+00)	Acc@1  55.00 ( 61.65)	Acc@5  84.00 ( 87.33)
 * Acc@1 61.760 Acc@5 87.480
### epoch[16] execution time: 21.084415674209595
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.182 ( 0.182)	Data  0.143 ( 0.143)	Loss 7.4285e-01 (7.4285e-01)	Acc@1  78.91 ( 78.91)	Acc@5  96.88 ( 96.88)
Epoch: [17][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 6.9752e-01 (7.5061e-01)	Acc@1  75.00 ( 76.42)	Acc@5  97.66 ( 96.73)
Epoch: [17][ 20/391]	Time  0.048 ( 0.053)	Data  0.001 ( 0.009)	Loss 6.6584e-01 (7.3889e-01)	Acc@1  78.91 ( 77.57)	Acc@5  98.44 ( 96.47)
Epoch: [17][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 7.1715e-01 (7.4418e-01)	Acc@1  75.78 ( 77.34)	Acc@5  97.66 ( 96.42)
Epoch: [17][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 6.7438e-01 (7.3303e-01)	Acc@1  83.59 ( 78.03)	Acc@5  97.66 ( 96.53)
Epoch: [17][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 5.7525e-01 (7.4496e-01)	Acc@1  78.12 ( 77.47)	Acc@5  99.22 ( 96.37)
Epoch: [17][ 60/391]	Time  0.048 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.1753e-01 (7.4576e-01)	Acc@1  78.12 ( 77.56)	Acc@5  96.09 ( 96.31)
Epoch: [17][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 7.1125e-01 (7.4569e-01)	Acc@1  76.56 ( 77.39)	Acc@5  96.88 ( 96.30)
Epoch: [17][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 6.9361e-01 (7.4279e-01)	Acc@1  80.47 ( 77.49)	Acc@5  96.09 ( 96.27)
Epoch: [17][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.6323e-01 (7.4265e-01)	Acc@1  83.59 ( 77.49)	Acc@5  98.44 ( 96.24)
Epoch: [17][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.0104e-01 (7.4986e-01)	Acc@1  76.56 ( 77.22)	Acc@5  93.75 ( 96.22)
Epoch: [17][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.8947e-01 (7.5016e-01)	Acc@1  74.22 ( 77.08)	Acc@5  96.09 ( 96.27)
Epoch: [17][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.2170e-01 (7.5246e-01)	Acc@1  82.03 ( 77.07)	Acc@5  96.09 ( 96.25)
Epoch: [17][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.1790e-01 (7.6135e-01)	Acc@1  69.53 ( 76.77)	Acc@5  94.53 ( 96.15)
Epoch: [17][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.0614e-01 (7.5824e-01)	Acc@1  78.91 ( 76.81)	Acc@5  96.09 ( 96.18)
Epoch: [17][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0569e+00 (7.6563e-01)	Acc@1  67.19 ( 76.55)	Acc@5  93.75 ( 96.09)
Epoch: [17][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.0586e-01 (7.6576e-01)	Acc@1  79.69 ( 76.61)	Acc@5  94.53 ( 96.08)
Epoch: [17][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.1452e-01 (7.6771e-01)	Acc@1  69.53 ( 76.51)	Acc@5  94.53 ( 96.08)
Epoch: [17][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.9072e-01 (7.6857e-01)	Acc@1  76.56 ( 76.42)	Acc@5  97.66 ( 96.13)
Epoch: [17][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.5470e-01 (7.7300e-01)	Acc@1  66.41 ( 76.21)	Acc@5  96.88 ( 96.09)
Epoch: [17][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.2334e-01 (7.7488e-01)	Acc@1  75.00 ( 76.18)	Acc@5  96.09 ( 96.06)
Epoch: [17][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.2494e-01 (7.7686e-01)	Acc@1  80.47 ( 76.10)	Acc@5  96.09 ( 96.00)
Epoch: [17][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.5079e-01 (7.8159e-01)	Acc@1  75.00 ( 75.97)	Acc@5  96.88 ( 95.97)
Epoch: [17][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.2089e-01 (7.8511e-01)	Acc@1  75.00 ( 75.84)	Acc@5  94.53 ( 95.91)
Epoch: [17][240/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.1980e-01 (7.8463e-01)	Acc@1  75.00 ( 75.81)	Acc@5  96.88 ( 95.93)
Epoch: [17][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8427e-01 (7.8754e-01)	Acc@1  75.00 ( 75.72)	Acc@5  94.53 ( 95.88)
Epoch: [17][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4630e-01 (7.8699e-01)	Acc@1  78.12 ( 75.72)	Acc@5  96.88 ( 95.90)
Epoch: [17][270/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3191e-01 (7.8852e-01)	Acc@1  71.88 ( 75.65)	Acc@5  95.31 ( 95.90)
Epoch: [17][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7420e-01 (7.8903e-01)	Acc@1  78.91 ( 75.67)	Acc@5  92.97 ( 95.87)
Epoch: [17][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2413e-01 (7.9257e-01)	Acc@1  71.88 ( 75.57)	Acc@5  96.88 ( 95.84)
Epoch: [17][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8770e-01 (7.9318e-01)	Acc@1  75.78 ( 75.54)	Acc@5  97.66 ( 95.87)
Epoch: [17][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4658e-01 (7.9294e-01)	Acc@1  74.22 ( 75.54)	Acc@5  98.44 ( 95.88)
Epoch: [17][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6241e-01 (7.9465e-01)	Acc@1  71.88 ( 75.51)	Acc@5  96.09 ( 95.85)
Epoch: [17][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.6145e-01 (7.9567e-01)	Acc@1  75.78 ( 75.47)	Acc@5  95.31 ( 95.83)
Epoch: [17][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1632e+00 (7.9722e-01)	Acc@1  63.28 ( 75.41)	Acc@5  94.53 ( 95.83)
Epoch: [17][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0166e-01 (7.9938e-01)	Acc@1  75.78 ( 75.31)	Acc@5  93.75 ( 95.80)
Epoch: [17][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5008e-01 (8.0083e-01)	Acc@1  74.22 ( 75.27)	Acc@5  98.44 ( 95.79)
Epoch: [17][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0850e-01 (8.0031e-01)	Acc@1  77.34 ( 75.27)	Acc@5  96.88 ( 95.80)
Epoch: [17][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4567e-01 (8.0032e-01)	Acc@1  75.00 ( 75.27)	Acc@5  96.09 ( 95.80)
Epoch: [17][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.9164e-01 (8.0113e-01)	Acc@1  71.25 ( 75.25)	Acc@5  97.50 ( 95.79)
## e[17] optimizer.zero_grad (sum) time: 0.1293785572052002
## e[17]       loss.backward (sum) time: 2.5368423461914062
## e[17]      optimizer.step (sum) time: 1.0044119358062744
## epoch[17] training(only) time: 18.616459608078003
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.2601e+00 (1.2601e+00)	Acc@1  63.00 ( 63.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5139e+00 (1.4160e+00)	Acc@1  61.00 ( 64.64)	Acc@5  91.00 ( 88.45)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3885e+00 (1.3585e+00)	Acc@1  68.00 ( 65.52)	Acc@5  91.00 ( 88.86)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7807e+00 (1.4014e+00)	Acc@1  58.00 ( 64.58)	Acc@5  84.00 ( 88.45)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4123e+00 (1.3981e+00)	Acc@1  60.00 ( 63.95)	Acc@5  91.00 ( 88.51)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3459e+00 (1.4047e+00)	Acc@1  63.00 ( 63.78)	Acc@5  87.00 ( 88.31)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4388e+00 (1.3888e+00)	Acc@1  64.00 ( 63.84)	Acc@5  92.00 ( 88.54)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.7720e+00 (1.3978e+00)	Acc@1  54.00 ( 63.66)	Acc@5  85.00 ( 88.45)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5577e+00 (1.4127e+00)	Acc@1  63.00 ( 63.40)	Acc@5  87.00 ( 88.33)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7348e+00 (1.4038e+00)	Acc@1  61.00 ( 63.62)	Acc@5  85.00 ( 88.49)
 * Acc@1 63.580 Acc@5 88.620
### epoch[17] execution time: 21.097976684570312
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.181 ( 0.181)	Data  0.140 ( 0.140)	Loss 6.6499e-01 (6.6499e-01)	Acc@1  79.69 ( 79.69)	Acc@5  99.22 ( 99.22)
Epoch: [18][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.014)	Loss 6.6149e-01 (6.5070e-01)	Acc@1  75.78 ( 80.33)	Acc@5  95.31 ( 97.16)
Epoch: [18][ 20/391]	Time  0.045 ( 0.053)	Data  0.001 ( 0.009)	Loss 7.0924e-01 (6.8754e-01)	Acc@1  78.12 ( 78.50)	Acc@5  94.53 ( 96.43)
Epoch: [18][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 7.2189e-01 (6.9066e-01)	Acc@1  78.12 ( 78.45)	Acc@5  95.31 ( 96.52)
Epoch: [18][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 7.2641e-01 (6.8404e-01)	Acc@1  76.56 ( 78.66)	Acc@5  97.66 ( 96.78)
Epoch: [18][ 50/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.0778e-01 (6.7120e-01)	Acc@1  73.44 ( 78.95)	Acc@5  99.22 ( 97.00)
Epoch: [18][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.1156e-01 (6.7394e-01)	Acc@1  77.34 ( 78.96)	Acc@5  94.53 ( 96.93)
Epoch: [18][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 7.4720e-01 (6.7354e-01)	Acc@1  80.47 ( 79.01)	Acc@5  95.31 ( 96.97)
Epoch: [18][ 80/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.0487e-01 (6.6768e-01)	Acc@1  76.56 ( 79.14)	Acc@5  96.88 ( 96.99)
Epoch: [18][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0031e-01 (6.6912e-01)	Acc@1  87.50 ( 79.22)	Acc@5  99.22 ( 97.04)
Epoch: [18][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.7414e-01 (6.6658e-01)	Acc@1  81.25 ( 79.36)	Acc@5  95.31 ( 97.03)
Epoch: [18][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8990e-01 (6.6636e-01)	Acc@1  83.59 ( 79.41)	Acc@5  98.44 ( 97.07)
Epoch: [18][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.3711e-01 (6.7145e-01)	Acc@1  75.78 ( 79.27)	Acc@5  98.44 ( 97.00)
Epoch: [18][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.4264e-01 (6.7970e-01)	Acc@1  72.66 ( 79.04)	Acc@5  98.44 ( 96.95)
Epoch: [18][140/391]	Time  0.051 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.9242e-01 (6.8324e-01)	Acc@1  79.69 ( 78.87)	Acc@5 100.00 ( 96.94)
Epoch: [18][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.4954e-01 (6.8509e-01)	Acc@1  74.22 ( 78.82)	Acc@5  95.31 ( 96.90)
Epoch: [18][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.5786e-01 (6.9265e-01)	Acc@1  74.22 ( 78.54)	Acc@5  95.31 ( 96.84)
Epoch: [18][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.6192e-01 (7.0028e-01)	Acc@1  75.78 ( 78.39)	Acc@5  98.44 ( 96.76)
Epoch: [18][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.5849e-01 (7.0493e-01)	Acc@1  76.56 ( 78.27)	Acc@5  94.53 ( 96.72)
Epoch: [18][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.4956e-01 (7.0840e-01)	Acc@1  75.00 ( 78.18)	Acc@5  96.09 ( 96.70)
Epoch: [18][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.7315e-01 (7.1079e-01)	Acc@1  74.22 ( 78.09)	Acc@5  95.31 ( 96.68)
Epoch: [18][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7832e-01 (7.1357e-01)	Acc@1  76.56 ( 78.00)	Acc@5  95.31 ( 96.66)
Epoch: [18][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3921e-01 (7.1626e-01)	Acc@1  69.53 ( 77.91)	Acc@5  96.09 ( 96.63)
Epoch: [18][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3740e-01 (7.1772e-01)	Acc@1  79.69 ( 77.85)	Acc@5  96.88 ( 96.61)
Epoch: [18][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7627e-01 (7.1775e-01)	Acc@1  82.03 ( 77.81)	Acc@5  98.44 ( 96.64)
Epoch: [18][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6165e-01 (7.2094e-01)	Acc@1  79.69 ( 77.72)	Acc@5  98.44 ( 96.59)
Epoch: [18][260/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3243e-01 (7.2381e-01)	Acc@1  78.12 ( 77.66)	Acc@5  94.53 ( 96.53)
Epoch: [18][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7712e-01 (7.2689e-01)	Acc@1  68.75 ( 77.59)	Acc@5  95.31 ( 96.53)
Epoch: [18][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5581e-01 (7.2800e-01)	Acc@1  72.66 ( 77.54)	Acc@5  98.44 ( 96.52)
Epoch: [18][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1859e-01 (7.3139e-01)	Acc@1  83.59 ( 77.42)	Acc@5  96.88 ( 96.48)
Epoch: [18][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4654e-01 (7.3562e-01)	Acc@1  75.00 ( 77.33)	Acc@5  92.97 ( 96.44)
Epoch: [18][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.8914e-01 (7.3794e-01)	Acc@1  72.66 ( 77.24)	Acc@5  95.31 ( 96.42)
Epoch: [18][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5929e-01 (7.3845e-01)	Acc@1  79.69 ( 77.23)	Acc@5  96.88 ( 96.41)
Epoch: [18][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0276e-01 (7.4041e-01)	Acc@1  71.09 ( 77.20)	Acc@5  96.88 ( 96.38)
Epoch: [18][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7906e-01 (7.4216e-01)	Acc@1  76.56 ( 77.15)	Acc@5  96.88 ( 96.37)
Epoch: [18][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.6674e-01 (7.4309e-01)	Acc@1  74.22 ( 77.13)	Acc@5  98.44 ( 96.35)
Epoch: [18][360/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8605e-01 (7.4414e-01)	Acc@1  78.12 ( 77.11)	Acc@5  99.22 ( 96.33)
Epoch: [18][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1337e-01 (7.4421e-01)	Acc@1  76.56 ( 77.13)	Acc@5  97.66 ( 96.34)
Epoch: [18][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5755e-01 (7.4646e-01)	Acc@1  67.97 ( 77.08)	Acc@5  96.88 ( 96.28)
Epoch: [18][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4406e-01 (7.4765e-01)	Acc@1  71.25 ( 77.07)	Acc@5  95.00 ( 96.25)
## e[18] optimizer.zero_grad (sum) time: 0.1289811134338379
## e[18]       loss.backward (sum) time: 2.5374956130981445
## e[18]      optimizer.step (sum) time: 1.0025849342346191
## epoch[18] training(only) time: 18.599076986312866
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.4701e+00 (1.4701e+00)	Acc@1  64.00 ( 64.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.3379e+00 (1.4569e+00)	Acc@1  64.00 ( 63.27)	Acc@5  91.00 ( 87.91)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.5928e+00 (1.4071e+00)	Acc@1  66.00 ( 64.67)	Acc@5  89.00 ( 87.81)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6594e+00 (1.4261e+00)	Acc@1  58.00 ( 64.42)	Acc@5  82.00 ( 87.39)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5268e+00 (1.4287e+00)	Acc@1  61.00 ( 64.02)	Acc@5  92.00 ( 87.80)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3878e+00 (1.4402e+00)	Acc@1  60.00 ( 63.39)	Acc@5  92.00 ( 87.43)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3488e+00 (1.4260e+00)	Acc@1  60.00 ( 63.23)	Acc@5  88.00 ( 87.75)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6952e+00 (1.4366e+00)	Acc@1  60.00 ( 63.17)	Acc@5  87.00 ( 87.79)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.4789e+00 (1.4407e+00)	Acc@1  65.00 ( 62.95)	Acc@5  86.00 ( 87.65)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6847e+00 (1.4332e+00)	Acc@1  57.00 ( 63.07)	Acc@5  82.00 ( 87.76)
 * Acc@1 63.320 Acc@5 87.940
### epoch[18] execution time: 21.086361169815063
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.191 ( 0.191)	Data  0.151 ( 0.151)	Loss 6.0074e-01 (6.0074e-01)	Acc@1  83.59 ( 83.59)	Acc@5  96.88 ( 96.88)
Epoch: [19][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 6.1837e-01 (6.3694e-01)	Acc@1  82.03 ( 81.32)	Acc@5  96.88 ( 97.37)
Epoch: [19][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 6.1698e-01 (6.3475e-01)	Acc@1  81.25 ( 80.99)	Acc@5  97.66 ( 97.43)
Epoch: [19][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 5.4287e-01 (6.2048e-01)	Acc@1  80.47 ( 81.12)	Acc@5  97.66 ( 97.51)
Epoch: [19][ 40/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.006)	Loss 6.5032e-01 (6.2682e-01)	Acc@1  82.81 ( 80.89)	Acc@5  96.88 ( 97.48)
Epoch: [19][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 6.9867e-01 (6.2974e-01)	Acc@1  78.12 ( 80.59)	Acc@5  96.09 ( 97.40)
Epoch: [19][ 60/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 7.0149e-01 (6.4142e-01)	Acc@1  79.69 ( 80.32)	Acc@5  96.09 ( 97.25)
Epoch: [19][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 6.8094e-01 (6.4725e-01)	Acc@1  82.81 ( 80.28)	Acc@5  96.88 ( 97.22)
Epoch: [19][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.6733e-01 (6.5178e-01)	Acc@1  85.16 ( 80.03)	Acc@5  98.44 ( 97.17)
Epoch: [19][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 7.0479e-01 (6.5121e-01)	Acc@1  75.78 ( 80.01)	Acc@5  96.09 ( 97.12)
Epoch: [19][100/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.7501e-01 (6.5187e-01)	Acc@1  80.47 ( 79.94)	Acc@5 100.00 ( 97.18)
Epoch: [19][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.6099e-01 (6.5091e-01)	Acc@1  79.69 ( 79.98)	Acc@5  96.09 ( 97.20)
Epoch: [19][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.6217e-01 (6.5420e-01)	Acc@1  74.22 ( 79.80)	Acc@5  94.53 ( 97.15)
Epoch: [19][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.5264e-01 (6.5521e-01)	Acc@1  78.91 ( 79.72)	Acc@5  96.88 ( 97.13)
Epoch: [19][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.1129e-01 (6.6052e-01)	Acc@1  83.59 ( 79.50)	Acc@5  96.88 ( 97.12)
Epoch: [19][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.4772e-01 (6.6659e-01)	Acc@1  83.59 ( 79.31)	Acc@5  96.09 ( 97.04)
Epoch: [19][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.9167e-01 (6.7000e-01)	Acc@1  75.00 ( 79.18)	Acc@5  97.66 ( 97.00)
Epoch: [19][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.7193e-01 (6.7417e-01)	Acc@1  80.47 ( 79.03)	Acc@5  96.88 ( 96.92)
Epoch: [19][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.2381e-01 (6.7603e-01)	Acc@1  78.12 ( 78.93)	Acc@5  96.09 ( 96.89)
Epoch: [19][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.6502e-01 (6.7921e-01)	Acc@1  72.66 ( 78.76)	Acc@5  97.66 ( 96.90)
Epoch: [19][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.8048e-01 (6.8399e-01)	Acc@1  79.69 ( 78.65)	Acc@5  96.88 ( 96.84)
Epoch: [19][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.1326e-01 (6.8660e-01)	Acc@1  70.31 ( 78.62)	Acc@5  95.31 ( 96.82)
Epoch: [19][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.8829e-01 (6.8967e-01)	Acc@1  84.38 ( 78.51)	Acc@5  98.44 ( 96.79)
Epoch: [19][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.4245e-01 (6.9454e-01)	Acc@1  73.44 ( 78.41)	Acc@5  96.09 ( 96.75)
Epoch: [19][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.3472e-01 (6.9507e-01)	Acc@1  75.78 ( 78.35)	Acc@5  96.09 ( 96.74)
Epoch: [19][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.1451e-01 (6.9606e-01)	Acc@1  78.12 ( 78.33)	Acc@5  95.31 ( 96.71)
Epoch: [19][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.0958e-01 (6.9882e-01)	Acc@1  75.00 ( 78.29)	Acc@5  92.97 ( 96.66)
Epoch: [19][270/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.6815e-01 (7.0245e-01)	Acc@1  78.12 ( 78.18)	Acc@5  96.88 ( 96.64)
Epoch: [19][280/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.6315e-01 (7.0339e-01)	Acc@1  78.12 ( 78.15)	Acc@5  96.88 ( 96.61)
Epoch: [19][290/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5080e-01 (7.0538e-01)	Acc@1  82.03 ( 78.11)	Acc@5  97.66 ( 96.61)
Epoch: [19][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1329e-01 (7.0682e-01)	Acc@1  69.53 ( 78.08)	Acc@5  95.31 ( 96.59)
Epoch: [19][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3762e-01 (7.0668e-01)	Acc@1  77.34 ( 78.08)	Acc@5  94.53 ( 96.60)
Epoch: [19][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8055e-01 (7.0882e-01)	Acc@1  75.78 ( 78.06)	Acc@5  93.75 ( 96.58)
Epoch: [19][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3744e-01 (7.1174e-01)	Acc@1  76.56 ( 78.00)	Acc@5  96.09 ( 96.55)
Epoch: [19][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0566e-01 (7.1423e-01)	Acc@1  74.22 ( 77.91)	Acc@5  97.66 ( 96.52)
Epoch: [19][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4273e-01 (7.1817e-01)	Acc@1  70.31 ( 77.77)	Acc@5  96.09 ( 96.50)
Epoch: [19][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1954e-01 (7.1997e-01)	Acc@1  72.66 ( 77.72)	Acc@5  93.75 ( 96.50)
Epoch: [19][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4023e-01 (7.2197e-01)	Acc@1  75.00 ( 77.66)	Acc@5  96.88 ( 96.48)
Epoch: [19][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0626e-01 (7.2385e-01)	Acc@1  72.66 ( 77.59)	Acc@5  94.53 ( 96.46)
Epoch: [19][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1995e-01 (7.2481e-01)	Acc@1  78.75 ( 77.56)	Acc@5  97.50 ( 96.46)
## e[19] optimizer.zero_grad (sum) time: 0.12773442268371582
## e[19]       loss.backward (sum) time: 2.5438528060913086
## e[19]      optimizer.step (sum) time: 1.0108587741851807
## epoch[19] training(only) time: 18.652597665786743
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.4320e+00 (1.4320e+00)	Acc@1  61.00 ( 61.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.4355e+00 (1.5362e+00)	Acc@1  63.00 ( 62.45)	Acc@5  88.00 ( 86.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5504e+00 (1.4864e+00)	Acc@1  58.00 ( 63.05)	Acc@5  88.00 ( 87.95)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7149e+00 (1.4803e+00)	Acc@1  59.00 ( 62.74)	Acc@5  88.00 ( 88.06)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3668e+00 (1.4662e+00)	Acc@1  69.00 ( 62.88)	Acc@5  89.00 ( 88.39)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5856e+00 (1.4987e+00)	Acc@1  60.00 ( 62.45)	Acc@5  87.00 ( 87.84)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4431e+00 (1.4701e+00)	Acc@1  66.00 ( 62.87)	Acc@5  91.00 ( 88.26)
Test: [ 70/100]	Time  0.025 ( 0.024)	Loss 1.7207e+00 (1.4724e+00)	Acc@1  64.00 ( 62.85)	Acc@5  88.00 ( 88.07)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6233e+00 (1.4771e+00)	Acc@1  61.00 ( 62.68)	Acc@5  87.00 ( 88.04)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6825e+00 (1.4665e+00)	Acc@1  59.00 ( 62.78)	Acc@5  83.00 ( 88.08)
 * Acc@1 62.960 Acc@5 88.140
### epoch[19] execution time: 21.136650562286377
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.190 ( 0.190)	Data  0.150 ( 0.150)	Loss 6.1076e-01 (6.1076e-01)	Acc@1  82.03 ( 82.03)	Acc@5  96.09 ( 96.09)
Epoch: [20][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 5.9815e-01 (6.5666e-01)	Acc@1  78.91 ( 79.62)	Acc@5  98.44 ( 97.09)
Epoch: [20][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 4.5164e-01 (6.2324e-01)	Acc@1  85.16 ( 80.80)	Acc@5  99.22 ( 97.32)
Epoch: [20][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 5.6339e-01 (6.0116e-01)	Acc@1  82.03 ( 81.25)	Acc@5  97.66 ( 97.43)
Epoch: [20][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 6.1535e-01 (5.9849e-01)	Acc@1  81.25 ( 81.29)	Acc@5  99.22 ( 97.60)
Epoch: [20][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 5.9771e-01 (5.8897e-01)	Acc@1  83.59 ( 81.71)	Acc@5  95.31 ( 97.59)
Epoch: [20][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.7320e-01 (5.8863e-01)	Acc@1  87.50 ( 81.69)	Acc@5  98.44 ( 97.68)
Epoch: [20][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 6.4516e-01 (5.9588e-01)	Acc@1  80.47 ( 81.40)	Acc@5  96.09 ( 97.65)
Epoch: [20][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.5956e-01 (5.9943e-01)	Acc@1  81.25 ( 81.19)	Acc@5  99.22 ( 97.68)
Epoch: [20][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 6.1287e-01 (6.0295e-01)	Acc@1  82.81 ( 81.04)	Acc@5  97.66 ( 97.60)
Epoch: [20][100/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 6.1950e-01 (6.0651e-01)	Acc@1  80.47 ( 80.93)	Acc@5  96.09 ( 97.56)
Epoch: [20][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.3895e-01 (6.0698e-01)	Acc@1  81.25 ( 80.88)	Acc@5  98.44 ( 97.54)
Epoch: [20][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.5211e-01 (6.0883e-01)	Acc@1  74.22 ( 80.88)	Acc@5  96.88 ( 97.55)
Epoch: [20][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.8726e-01 (6.1728e-01)	Acc@1  87.50 ( 80.63)	Acc@5  97.66 ( 97.48)
Epoch: [20][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1464e-01 (6.2043e-01)	Acc@1  83.59 ( 80.44)	Acc@5  99.22 ( 97.46)
Epoch: [20][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.4914e-01 (6.2632e-01)	Acc@1  76.56 ( 80.22)	Acc@5  98.44 ( 97.47)
Epoch: [20][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.8473e-01 (6.2946e-01)	Acc@1  75.78 ( 80.14)	Acc@5  97.66 ( 97.43)
Epoch: [20][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.4102e-01 (6.2943e-01)	Acc@1  85.94 ( 80.14)	Acc@5  99.22 ( 97.46)
Epoch: [20][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.1570e-01 (6.3191e-01)	Acc@1  78.12 ( 80.03)	Acc@5  98.44 ( 97.47)
Epoch: [20][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.9179e-01 (6.3250e-01)	Acc@1  76.56 ( 80.00)	Acc@5  96.88 ( 97.49)
Epoch: [20][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.7226e-01 (6.3637e-01)	Acc@1  77.34 ( 79.93)	Acc@5  94.53 ( 97.42)
Epoch: [20][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.9044e-01 (6.3724e-01)	Acc@1  79.69 ( 79.96)	Acc@5  96.88 ( 97.42)
Epoch: [20][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.3773e-01 (6.3892e-01)	Acc@1  75.00 ( 79.89)	Acc@5  97.66 ( 97.43)
Epoch: [20][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.2559e-01 (6.4434e-01)	Acc@1  78.12 ( 79.68)	Acc@5  96.88 ( 97.40)
Epoch: [20][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.2111e-01 (6.4703e-01)	Acc@1  71.88 ( 79.60)	Acc@5  97.66 ( 97.39)
Epoch: [20][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.4953e-01 (6.4739e-01)	Acc@1  82.03 ( 79.63)	Acc@5  96.88 ( 97.37)
Epoch: [20][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.4143e-01 (6.4953e-01)	Acc@1  76.56 ( 79.54)	Acc@5  98.44 ( 97.36)
Epoch: [20][270/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7780e-01 (6.5115e-01)	Acc@1  81.25 ( 79.51)	Acc@5  98.44 ( 97.32)
Epoch: [20][280/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.2416e-01 (6.5426e-01)	Acc@1  76.56 ( 79.44)	Acc@5  97.66 ( 97.30)
Epoch: [20][290/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.3003e-01 (6.5480e-01)	Acc@1  82.03 ( 79.39)	Acc@5  98.44 ( 97.30)
Epoch: [20][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7121e-01 (6.5791e-01)	Acc@1  81.25 ( 79.31)	Acc@5  98.44 ( 97.29)
Epoch: [20][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1542e-01 (6.6114e-01)	Acc@1  81.25 ( 79.20)	Acc@5  96.09 ( 97.25)
Epoch: [20][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7468e-01 (6.6321e-01)	Acc@1  80.47 ( 79.15)	Acc@5  97.66 ( 97.22)
Epoch: [20][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4399e-01 (6.6418e-01)	Acc@1  81.25 ( 79.14)	Acc@5  96.88 ( 97.22)
Epoch: [20][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.9506e-01 (6.6688e-01)	Acc@1  74.22 ( 79.08)	Acc@5  96.88 ( 97.19)
Epoch: [20][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6278e-01 (6.6862e-01)	Acc@1  82.81 ( 79.02)	Acc@5  98.44 ( 97.16)
Epoch: [20][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0249e-01 (6.7204e-01)	Acc@1  74.22 ( 78.91)	Acc@5  97.66 ( 97.14)
Epoch: [20][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6897e-01 (6.7409e-01)	Acc@1  72.66 ( 78.82)	Acc@5  95.31 ( 97.14)
Epoch: [20][380/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3468e-01 (6.7709e-01)	Acc@1  74.22 ( 78.77)	Acc@5  94.53 ( 97.08)
Epoch: [20][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2624e-01 (6.7887e-01)	Acc@1  83.75 ( 78.74)	Acc@5  97.50 ( 97.05)
## e[20] optimizer.zero_grad (sum) time: 0.12885427474975586
## e[20]       loss.backward (sum) time: 2.545325756072998
## e[20]      optimizer.step (sum) time: 1.0059609413146973
## epoch[20] training(only) time: 18.637139558792114
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 1.3475e+00 (1.3475e+00)	Acc@1  67.00 ( 67.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.023 ( 0.036)	Loss 1.3642e+00 (1.4381e+00)	Acc@1  65.00 ( 63.91)	Acc@5  89.00 ( 88.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4092e+00 (1.3829e+00)	Acc@1  66.00 ( 65.52)	Acc@5  89.00 ( 89.10)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7727e+00 (1.4076e+00)	Acc@1  52.00 ( 64.55)	Acc@5  85.00 ( 88.48)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.7235e+00 (1.4097e+00)	Acc@1  62.00 ( 64.83)	Acc@5  88.00 ( 88.56)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6315e+00 (1.4293e+00)	Acc@1  61.00 ( 64.20)	Acc@5  85.00 ( 88.45)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2633e+00 (1.4090e+00)	Acc@1  68.00 ( 64.25)	Acc@5  93.00 ( 88.64)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6075e+00 (1.4121e+00)	Acc@1  64.00 ( 64.28)	Acc@5  86.00 ( 88.70)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3738e+00 (1.4253e+00)	Acc@1  70.00 ( 64.22)	Acc@5  85.00 ( 88.51)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.5707e+00 (1.4137e+00)	Acc@1  55.00 ( 64.30)	Acc@5  89.00 ( 88.65)
 * Acc@1 64.450 Acc@5 88.660
### epoch[20] execution time: 21.133227825164795
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.190 ( 0.190)	Data  0.150 ( 0.150)	Loss 6.0453e-01 (6.0453e-01)	Acc@1  79.69 ( 79.69)	Acc@5  97.66 ( 97.66)
Epoch: [21][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 5.5097e-01 (5.8929e-01)	Acc@1  82.03 ( 82.10)	Acc@5  97.66 ( 97.16)
Epoch: [21][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 5.5061e-01 (5.8048e-01)	Acc@1  84.38 ( 82.37)	Acc@5  97.66 ( 97.32)
Epoch: [21][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 5.1096e-01 (5.8041e-01)	Acc@1  85.94 ( 82.41)	Acc@5  96.88 ( 97.45)
Epoch: [21][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.6770e-01 (5.6887e-01)	Acc@1  84.38 ( 82.49)	Acc@5  99.22 ( 97.60)
Epoch: [21][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 6.0358e-01 (5.6979e-01)	Acc@1  82.03 ( 82.54)	Acc@5  98.44 ( 97.70)
Epoch: [21][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.3752e-01 (5.7110e-01)	Acc@1  84.38 ( 82.48)	Acc@5  97.66 ( 97.71)
Epoch: [21][ 70/391]	Time  0.048 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.4659e-01 (5.7449e-01)	Acc@1  78.91 ( 82.23)	Acc@5  99.22 ( 97.80)
Epoch: [21][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 6.9659e-01 (5.7591e-01)	Acc@1  80.47 ( 82.22)	Acc@5  98.44 ( 97.77)
Epoch: [21][ 90/391]	Time  0.048 ( 0.049)	Data  0.001 ( 0.004)	Loss 6.4259e-01 (5.7838e-01)	Acc@1  78.91 ( 82.01)	Acc@5  99.22 ( 97.82)
Epoch: [21][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.0541e-01 (5.8117e-01)	Acc@1  77.34 ( 81.83)	Acc@5  99.22 ( 97.87)
Epoch: [21][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.7203e-01 (5.8758e-01)	Acc@1  78.91 ( 81.64)	Acc@5  97.66 ( 97.78)
Epoch: [21][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.8541e-01 (5.9602e-01)	Acc@1  81.25 ( 81.39)	Acc@5  95.31 ( 97.71)
Epoch: [21][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.4697e-01 (5.9749e-01)	Acc@1  82.03 ( 81.39)	Acc@5  97.66 ( 97.73)
Epoch: [21][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.8234e-01 (5.9877e-01)	Acc@1  76.56 ( 81.29)	Acc@5  94.53 ( 97.68)
Epoch: [21][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5769e-01 (6.0341e-01)	Acc@1  81.25 ( 81.11)	Acc@5  98.44 ( 97.66)
Epoch: [21][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.4550e-01 (6.0256e-01)	Acc@1  82.81 ( 81.16)	Acc@5  96.88 ( 97.65)
Epoch: [21][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.9534e-01 (6.0744e-01)	Acc@1  80.47 ( 80.99)	Acc@5  96.88 ( 97.62)
Epoch: [21][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.5531e-01 (6.0836e-01)	Acc@1  74.22 ( 80.94)	Acc@5  94.53 ( 97.61)
Epoch: [21][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.8882e-01 (6.0820e-01)	Acc@1  80.47 ( 80.96)	Acc@5  96.88 ( 97.59)
Epoch: [21][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5827e-01 (6.1256e-01)	Acc@1  82.03 ( 80.85)	Acc@5 100.00 ( 97.56)
Epoch: [21][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.1714e-01 (6.1421e-01)	Acc@1  75.78 ( 80.80)	Acc@5  99.22 ( 97.57)
Epoch: [21][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.4976e-01 (6.1557e-01)	Acc@1  71.09 ( 80.73)	Acc@5  97.66 ( 97.54)
Epoch: [21][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.0099e-01 (6.2027e-01)	Acc@1  73.44 ( 80.58)	Acc@5  96.88 ( 97.51)
Epoch: [21][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5931e-01 (6.2045e-01)	Acc@1  88.28 ( 80.58)	Acc@5  98.44 ( 97.49)
Epoch: [21][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.2941e-01 (6.2286e-01)	Acc@1  78.91 ( 80.52)	Acc@5  97.66 ( 97.49)
Epoch: [21][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.1133e-01 (6.2406e-01)	Acc@1  80.47 ( 80.53)	Acc@5  98.44 ( 97.48)
Epoch: [21][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.6906e-01 (6.2493e-01)	Acc@1  79.69 ( 80.54)	Acc@5  96.88 ( 97.48)
Epoch: [21][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2814e-01 (6.2590e-01)	Acc@1  82.03 ( 80.52)	Acc@5  97.66 ( 97.48)
Epoch: [21][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5607e-01 (6.2634e-01)	Acc@1  79.69 ( 80.53)	Acc@5  99.22 ( 97.49)
Epoch: [21][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9928e-01 (6.2889e-01)	Acc@1  78.12 ( 80.45)	Acc@5  93.75 ( 97.44)
Epoch: [21][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8808e-01 (6.3000e-01)	Acc@1  81.25 ( 80.37)	Acc@5  96.88 ( 97.42)
Epoch: [21][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2876e-01 (6.3289e-01)	Acc@1  75.78 ( 80.29)	Acc@5  96.09 ( 97.41)
Epoch: [21][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7862e-01 (6.3431e-01)	Acc@1  78.12 ( 80.25)	Acc@5  98.44 ( 97.40)
Epoch: [21][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6961e-01 (6.3831e-01)	Acc@1  78.12 ( 80.15)	Acc@5  98.44 ( 97.36)
Epoch: [21][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8710e-01 (6.3799e-01)	Acc@1  75.78 ( 80.14)	Acc@5  98.44 ( 97.36)
Epoch: [21][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8643e-01 (6.3885e-01)	Acc@1  79.69 ( 80.13)	Acc@5  96.09 ( 97.35)
Epoch: [21][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1887e-01 (6.4073e-01)	Acc@1  74.22 ( 80.06)	Acc@5  94.53 ( 97.32)
Epoch: [21][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5615e-01 (6.4046e-01)	Acc@1  82.81 ( 80.08)	Acc@5  98.44 ( 97.33)
Epoch: [21][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4391e-01 (6.4259e-01)	Acc@1  73.75 ( 79.98)	Acc@5  95.00 ( 97.32)
## e[21] optimizer.zero_grad (sum) time: 0.13093161582946777
## e[21]       loss.backward (sum) time: 2.538480758666992
## e[21]      optimizer.step (sum) time: 1.0193262100219727
## epoch[21] training(only) time: 18.616130590438843
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.4028e+00 (1.4028e+00)	Acc@1  66.00 ( 66.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.8121e+00 (1.4887e+00)	Acc@1  60.00 ( 62.09)	Acc@5  88.00 ( 87.18)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5478e+00 (1.4154e+00)	Acc@1  61.00 ( 64.33)	Acc@5  90.00 ( 88.19)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6370e+00 (1.4288e+00)	Acc@1  58.00 ( 63.84)	Acc@5  85.00 ( 88.19)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5554e+00 (1.4286e+00)	Acc@1  56.00 ( 63.68)	Acc@5  92.00 ( 88.59)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4221e+00 (1.4464e+00)	Acc@1  64.00 ( 63.14)	Acc@5  88.00 ( 88.25)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4110e+00 (1.4257e+00)	Acc@1  61.00 ( 63.51)	Acc@5  89.00 ( 88.46)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6418e+00 (1.4289e+00)	Acc@1  54.00 ( 63.44)	Acc@5  83.00 ( 88.49)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5451e+00 (1.4323e+00)	Acc@1  61.00 ( 63.52)	Acc@5  84.00 ( 88.28)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6642e+00 (1.4239e+00)	Acc@1  63.00 ( 63.75)	Acc@5  88.00 ( 88.40)
 * Acc@1 64.010 Acc@5 88.450
### epoch[21] execution time: 21.107177019119263
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.183 ( 0.183)	Data  0.142 ( 0.142)	Loss 5.2232e-01 (5.2232e-01)	Acc@1  81.25 ( 81.25)	Acc@5  98.44 ( 98.44)
Epoch: [22][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 5.4357e-01 (5.4724e-01)	Acc@1  86.72 ( 83.24)	Acc@5  97.66 ( 97.73)
Epoch: [22][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 5.8893e-01 (5.3830e-01)	Acc@1  79.69 ( 83.07)	Acc@5  96.88 ( 98.03)
Epoch: [22][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 5.8191e-01 (5.4353e-01)	Acc@1  81.25 ( 82.91)	Acc@5  98.44 ( 98.16)
Epoch: [22][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 5.9659e-01 (5.3970e-01)	Acc@1  83.59 ( 83.14)	Acc@5  96.88 ( 98.17)
Epoch: [22][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 5.2248e-01 (5.3807e-01)	Acc@1  82.81 ( 83.06)	Acc@5  99.22 ( 98.13)
Epoch: [22][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.0159e-01 (5.3639e-01)	Acc@1  84.38 ( 82.99)	Acc@5 100.00 ( 98.18)
Epoch: [22][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.8952e-01 (5.3084e-01)	Acc@1  83.59 ( 83.22)	Acc@5  99.22 ( 98.12)
Epoch: [22][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.4334e-01 (5.2737e-01)	Acc@1  87.50 ( 83.42)	Acc@5  99.22 ( 98.11)
Epoch: [22][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1480e-01 (5.2470e-01)	Acc@1  84.38 ( 83.51)	Acc@5  99.22 ( 98.12)
Epoch: [22][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.6853e-01 (5.2040e-01)	Acc@1  81.25 ( 83.70)	Acc@5  99.22 ( 98.11)
Epoch: [22][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.3442e-01 (5.2454e-01)	Acc@1  79.69 ( 83.51)	Acc@5 100.00 ( 98.13)
Epoch: [22][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.1480e-01 (5.2660e-01)	Acc@1  85.94 ( 83.50)	Acc@5  97.66 ( 98.04)
Epoch: [22][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.8969e-01 (5.2944e-01)	Acc@1  78.91 ( 83.32)	Acc@5  98.44 ( 98.04)
Epoch: [22][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1821e-01 (5.3223e-01)	Acc@1  83.59 ( 83.12)	Acc@5  97.66 ( 98.04)
Epoch: [22][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.2305e-01 (5.3678e-01)	Acc@1  79.69 ( 83.04)	Acc@5  97.66 ( 98.02)
Epoch: [22][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.8067e-01 (5.4323e-01)	Acc@1  85.94 ( 82.85)	Acc@5  98.44 ( 97.98)
Epoch: [22][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.8228e-01 (5.4617e-01)	Acc@1  87.50 ( 82.74)	Acc@5  98.44 ( 97.98)
Epoch: [22][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.3809e-01 (5.5243e-01)	Acc@1  78.91 ( 82.57)	Acc@5  96.88 ( 97.93)
Epoch: [22][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.1889e-01 (5.5479e-01)	Acc@1  78.91 ( 82.47)	Acc@5  98.44 ( 97.90)
Epoch: [22][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.4486e-01 (5.5922e-01)	Acc@1  79.69 ( 82.38)	Acc@5  96.09 ( 97.85)
Epoch: [22][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.7001e-01 (5.6457e-01)	Acc@1  76.56 ( 82.24)	Acc@5  96.09 ( 97.82)
Epoch: [22][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8437e-01 (5.6808e-01)	Acc@1  80.47 ( 82.17)	Acc@5  96.09 ( 97.77)
Epoch: [22][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0536e-01 (5.7038e-01)	Acc@1  81.25 ( 82.11)	Acc@5  97.66 ( 97.76)
Epoch: [22][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3678e-01 (5.7333e-01)	Acc@1  78.91 ( 82.06)	Acc@5  96.88 ( 97.73)
Epoch: [22][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7417e-01 (5.7875e-01)	Acc@1  77.34 ( 81.91)	Acc@5  95.31 ( 97.70)
Epoch: [22][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7747e-01 (5.8124e-01)	Acc@1  80.47 ( 81.82)	Acc@5  95.31 ( 97.68)
Epoch: [22][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4151e-01 (5.8327e-01)	Acc@1  82.03 ( 81.77)	Acc@5  96.09 ( 97.66)
Epoch: [22][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5126e-01 (5.8694e-01)	Acc@1  76.56 ( 81.62)	Acc@5  97.66 ( 97.64)
Epoch: [22][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9469e-01 (5.8909e-01)	Acc@1  75.78 ( 81.55)	Acc@5  96.88 ( 97.62)
Epoch: [22][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2944e-01 (5.9415e-01)	Acc@1  76.56 ( 81.38)	Acc@5  98.44 ( 97.58)
Epoch: [22][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2103e-01 (5.9688e-01)	Acc@1  79.69 ( 81.30)	Acc@5  93.75 ( 97.53)
Epoch: [22][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6462e-01 (5.9869e-01)	Acc@1  78.12 ( 81.24)	Acc@5  98.44 ( 97.54)
Epoch: [22][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7178e-01 (6.0085e-01)	Acc@1  72.66 ( 81.15)	Acc@5  96.09 ( 97.53)
Epoch: [22][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5888e-01 (6.0203e-01)	Acc@1  78.12 ( 81.11)	Acc@5  96.88 ( 97.53)
Epoch: [22][350/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4796e-01 (6.0317e-01)	Acc@1  74.22 ( 81.06)	Acc@5  96.09 ( 97.51)
Epoch: [22][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.5828e-01 (6.0564e-01)	Acc@1  75.78 ( 80.98)	Acc@5  97.66 ( 97.50)
Epoch: [22][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6360e-01 (6.0820e-01)	Acc@1  76.56 ( 80.90)	Acc@5  98.44 ( 97.49)
Epoch: [22][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6677e-01 (6.1018e-01)	Acc@1  84.38 ( 80.83)	Acc@5  99.22 ( 97.47)
Epoch: [22][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.8026e-01 (6.1076e-01)	Acc@1  73.75 ( 80.81)	Acc@5  96.25 ( 97.48)
## e[22] optimizer.zero_grad (sum) time: 0.1298232078552246
## e[22]       loss.backward (sum) time: 2.526902675628662
## e[22]      optimizer.step (sum) time: 1.011702537536621
## epoch[22] training(only) time: 18.59471368789673
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.3963e+00 (1.3963e+00)	Acc@1  65.00 ( 65.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.6863e+00 (1.6044e+00)	Acc@1  61.00 ( 64.36)	Acc@5  88.00 ( 88.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4996e+00 (1.5639e+00)	Acc@1  64.00 ( 64.29)	Acc@5  87.00 ( 87.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7593e+00 (1.5426e+00)	Acc@1  63.00 ( 64.42)	Acc@5  87.00 ( 88.03)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5904e+00 (1.5457e+00)	Acc@1  57.00 ( 64.02)	Acc@5  89.00 ( 88.05)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4653e+00 (1.5470e+00)	Acc@1  60.00 ( 63.90)	Acc@5  89.00 ( 88.02)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4353e+00 (1.5200e+00)	Acc@1  63.00 ( 64.08)	Acc@5  91.00 ( 88.34)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.7053e+00 (1.5270e+00)	Acc@1  55.00 ( 63.52)	Acc@5  86.00 ( 88.31)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.4744e+00 (1.5296e+00)	Acc@1  67.00 ( 63.42)	Acc@5  86.00 ( 88.12)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6019e+00 (1.5146e+00)	Acc@1  64.00 ( 63.47)	Acc@5  88.00 ( 88.32)
 * Acc@1 63.580 Acc@5 88.440
### epoch[22] execution time: 21.074624061584473
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.202 ( 0.202)	Data  0.163 ( 0.163)	Loss 4.9661e-01 (4.9661e-01)	Acc@1  86.72 ( 86.72)	Acc@5  97.66 ( 97.66)
Epoch: [23][ 10/391]	Time  0.047 ( 0.061)	Data  0.001 ( 0.017)	Loss 5.7635e-01 (5.7090e-01)	Acc@1  81.25 ( 81.61)	Acc@5  98.44 ( 98.30)
Epoch: [23][ 20/391]	Time  0.048 ( 0.055)	Data  0.001 ( 0.010)	Loss 6.4354e-01 (5.6396e-01)	Acc@1  82.03 ( 82.48)	Acc@5  96.09 ( 98.07)
Epoch: [23][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.008)	Loss 4.2456e-01 (5.5716e-01)	Acc@1  85.94 ( 82.79)	Acc@5  99.22 ( 98.19)
Epoch: [23][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 3.8003e-01 (5.4447e-01)	Acc@1  89.84 ( 83.23)	Acc@5 100.00 ( 98.29)
Epoch: [23][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.3264e-01 (5.4640e-01)	Acc@1  85.16 ( 83.26)	Acc@5  98.44 ( 98.21)
Epoch: [23][ 60/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 5.8294e-01 (5.4598e-01)	Acc@1  78.91 ( 83.00)	Acc@5  98.44 ( 98.19)
Epoch: [23][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.9402e-01 (5.3870e-01)	Acc@1  81.25 ( 83.11)	Acc@5  98.44 ( 98.18)
Epoch: [23][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.3522e-01 (5.3509e-01)	Acc@1  81.25 ( 83.20)	Acc@5  97.66 ( 98.25)
Epoch: [23][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 6.2473e-01 (5.3973e-01)	Acc@1  78.12 ( 82.98)	Acc@5  98.44 ( 98.28)
Epoch: [23][100/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.6369e-01 (5.3779e-01)	Acc@1  85.16 ( 83.04)	Acc@5  96.88 ( 98.30)
Epoch: [23][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9387e-01 (5.3805e-01)	Acc@1  89.06 ( 83.02)	Acc@5  98.44 ( 98.34)
Epoch: [23][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.1504e-01 (5.4229e-01)	Acc@1  82.03 ( 82.86)	Acc@5  96.88 ( 98.27)
Epoch: [23][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.1295e-01 (5.4149e-01)	Acc@1  78.91 ( 82.85)	Acc@5  96.88 ( 98.28)
Epoch: [23][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8234e-01 (5.4146e-01)	Acc@1  82.81 ( 82.84)	Acc@5  98.44 ( 98.22)
Epoch: [23][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0697e-01 (5.4656e-01)	Acc@1  82.03 ( 82.69)	Acc@5  98.44 ( 98.17)
Epoch: [23][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.2705e-01 (5.4916e-01)	Acc@1  81.25 ( 82.63)	Acc@5  96.88 ( 98.11)
Epoch: [23][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.2584e-01 (5.5016e-01)	Acc@1  79.69 ( 82.62)	Acc@5  99.22 ( 98.08)
Epoch: [23][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.8807e-01 (5.5157e-01)	Acc@1  76.56 ( 82.57)	Acc@5  97.66 ( 98.07)
Epoch: [23][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.0451e-01 (5.5067e-01)	Acc@1  86.72 ( 82.62)	Acc@5 100.00 ( 98.07)
Epoch: [23][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7738e-01 (5.5198e-01)	Acc@1  83.59 ( 82.58)	Acc@5  98.44 ( 98.06)
Epoch: [23][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.2630e-01 (5.5492e-01)	Acc@1  81.25 ( 82.54)	Acc@5 100.00 ( 98.04)
Epoch: [23][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5177e-01 (5.5974e-01)	Acc@1  85.94 ( 82.42)	Acc@5  96.88 ( 97.99)
Epoch: [23][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3885e-01 (5.6114e-01)	Acc@1  85.94 ( 82.39)	Acc@5  98.44 ( 97.98)
Epoch: [23][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.2827e-01 (5.6335e-01)	Acc@1  81.25 ( 82.35)	Acc@5  95.31 ( 97.98)
Epoch: [23][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.6835e-01 (5.6381e-01)	Acc@1  77.34 ( 82.32)	Acc@5  97.66 ( 97.98)
Epoch: [23][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.7260e-01 (5.6477e-01)	Acc@1  76.56 ( 82.30)	Acc@5  94.53 ( 97.97)
Epoch: [23][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3707e-01 (5.6499e-01)	Acc@1  85.16 ( 82.34)	Acc@5  98.44 ( 97.94)
Epoch: [23][280/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7598e-01 (5.6644e-01)	Acc@1  81.25 ( 82.24)	Acc@5  99.22 ( 97.96)
Epoch: [23][290/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.0470e-01 (5.6836e-01)	Acc@1  84.38 ( 82.21)	Acc@5  96.88 ( 97.93)
Epoch: [23][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8460e-01 (5.7034e-01)	Acc@1  79.69 ( 82.11)	Acc@5  96.88 ( 97.92)
Epoch: [23][310/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7804e-01 (5.7049e-01)	Acc@1  80.47 ( 82.11)	Acc@5  95.31 ( 97.90)
Epoch: [23][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2017e-01 (5.7410e-01)	Acc@1  74.22 ( 81.99)	Acc@5  93.75 ( 97.85)
Epoch: [23][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3447e-01 (5.7524e-01)	Acc@1  83.59 ( 81.98)	Acc@5  96.09 ( 97.84)
Epoch: [23][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6292e-01 (5.7626e-01)	Acc@1  80.47 ( 81.95)	Acc@5  96.09 ( 97.85)
Epoch: [23][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0886e-01 (5.7791e-01)	Acc@1  84.38 ( 81.91)	Acc@5 100.00 ( 97.83)
Epoch: [23][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7326e-01 (5.7909e-01)	Acc@1  84.38 ( 81.88)	Acc@5  97.66 ( 97.82)
Epoch: [23][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8263e-01 (5.8182e-01)	Acc@1  78.12 ( 81.81)	Acc@5  97.66 ( 97.81)
Epoch: [23][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1244e-01 (5.8499e-01)	Acc@1  75.78 ( 81.73)	Acc@5  94.53 ( 97.77)
Epoch: [23][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2896e-01 (5.8924e-01)	Acc@1  72.50 ( 81.61)	Acc@5  96.25 ( 97.75)
## e[23] optimizer.zero_grad (sum) time: 0.1291792392730713
## e[23]       loss.backward (sum) time: 2.5307984352111816
## e[23]      optimizer.step (sum) time: 1.0093233585357666
## epoch[23] training(only) time: 18.6509850025177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.6007e+00 (1.6007e+00)	Acc@1  63.00 ( 63.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.6449e+00 (1.6606e+00)	Acc@1  66.00 ( 62.45)	Acc@5  91.00 ( 87.18)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5721e+00 (1.6331e+00)	Acc@1  67.00 ( 62.14)	Acc@5  88.00 ( 87.52)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.2708e+00 (1.6317e+00)	Acc@1  50.00 ( 61.52)	Acc@5  78.00 ( 87.35)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.5699e+00 (1.6360e+00)	Acc@1  62.00 ( 61.63)	Acc@5  90.00 ( 87.51)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5105e+00 (1.6478e+00)	Acc@1  62.00 ( 61.29)	Acc@5  88.00 ( 87.39)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4245e+00 (1.6257e+00)	Acc@1  70.00 ( 61.64)	Acc@5  91.00 ( 87.59)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.8299e+00 (1.6353e+00)	Acc@1  56.00 ( 61.61)	Acc@5  85.00 ( 87.37)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.7123e+00 (1.6409e+00)	Acc@1  64.00 ( 61.38)	Acc@5  86.00 ( 87.27)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 2.1614e+00 (1.6261e+00)	Acc@1  52.00 ( 61.54)	Acc@5  80.00 ( 87.48)
 * Acc@1 61.670 Acc@5 87.600
### epoch[23] execution time: 21.133537769317627
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.190 ( 0.190)	Data  0.149 ( 0.149)	Loss 4.7839e-01 (4.7839e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [24][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.016)	Loss 6.2594e-01 (5.7761e-01)	Acc@1  83.59 ( 82.39)	Acc@5  96.09 ( 98.08)
Epoch: [24][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 5.2197e-01 (5.5255e-01)	Acc@1  84.38 ( 82.63)	Acc@5  98.44 ( 98.29)
Epoch: [24][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.8321e-01 (5.4098e-01)	Acc@1  85.94 ( 82.86)	Acc@5  98.44 ( 98.34)
Epoch: [24][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 6.1058e-01 (5.2451e-01)	Acc@1  79.69 ( 83.38)	Acc@5  96.88 ( 98.40)
Epoch: [24][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 5.6356e-01 (5.1218e-01)	Acc@1  85.16 ( 83.95)	Acc@5  99.22 ( 98.50)
Epoch: [24][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.0760e-01 (5.1645e-01)	Acc@1  84.38 ( 83.80)	Acc@5  97.66 ( 98.35)
Epoch: [24][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.1121e-01 (5.0963e-01)	Acc@1  89.84 ( 84.09)	Acc@5  99.22 ( 98.40)
Epoch: [24][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.7851e-01 (5.0746e-01)	Acc@1  85.94 ( 84.10)	Acc@5 100.00 ( 98.45)
Epoch: [24][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.2113e-01 (5.1074e-01)	Acc@1  81.25 ( 83.99)	Acc@5  96.88 ( 98.41)
Epoch: [24][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.5760e-01 (5.1441e-01)	Acc@1  78.12 ( 83.79)	Acc@5  97.66 ( 98.39)
Epoch: [24][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5645e-01 (5.1668e-01)	Acc@1  82.03 ( 83.74)	Acc@5  98.44 ( 98.41)
Epoch: [24][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5316e-01 (5.1408e-01)	Acc@1  82.03 ( 83.75)	Acc@5  99.22 ( 98.43)
Epoch: [24][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.6753e-01 (5.1576e-01)	Acc@1  86.72 ( 83.76)	Acc@5  97.66 ( 98.41)
Epoch: [24][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5470e-01 (5.1881e-01)	Acc@1  80.47 ( 83.64)	Acc@5  98.44 ( 98.37)
Epoch: [24][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.4289e-01 (5.1988e-01)	Acc@1  80.47 ( 83.54)	Acc@5  99.22 ( 98.42)
Epoch: [24][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8036e-01 (5.2020e-01)	Acc@1  86.72 ( 83.50)	Acc@5  99.22 ( 98.44)
Epoch: [24][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.0617e-01 (5.1928e-01)	Acc@1  85.94 ( 83.55)	Acc@5  97.66 ( 98.44)
Epoch: [24][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.4840e-01 (5.1908e-01)	Acc@1  80.47 ( 83.56)	Acc@5  99.22 ( 98.46)
Epoch: [24][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.4957e-01 (5.2294e-01)	Acc@1  72.66 ( 83.46)	Acc@5  95.31 ( 98.41)
Epoch: [24][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.6406e-01 (5.2572e-01)	Acc@1  78.12 ( 83.36)	Acc@5  98.44 ( 98.40)
Epoch: [24][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.1805e-01 (5.2890e-01)	Acc@1  78.91 ( 83.22)	Acc@5  96.88 ( 98.36)
Epoch: [24][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.6092e-01 (5.3072e-01)	Acc@1  82.03 ( 83.19)	Acc@5  98.44 ( 98.35)
Epoch: [24][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.3047e-01 (5.3346e-01)	Acc@1  75.00 ( 83.11)	Acc@5  96.88 ( 98.31)
Epoch: [24][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.4337e-01 (5.3447e-01)	Acc@1  82.81 ( 83.10)	Acc@5  97.66 ( 98.31)
Epoch: [24][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7472e-01 (5.3569e-01)	Acc@1  80.47 ( 83.05)	Acc@5  97.66 ( 98.31)
Epoch: [24][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7009e-01 (5.3835e-01)	Acc@1  81.25 ( 82.96)	Acc@5  96.88 ( 98.27)
Epoch: [24][270/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0220e-01 (5.3935e-01)	Acc@1  83.59 ( 82.93)	Acc@5  98.44 ( 98.26)
Epoch: [24][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4497e-01 (5.3966e-01)	Acc@1  85.94 ( 82.94)	Acc@5  98.44 ( 98.26)
Epoch: [24][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1546e-01 (5.4248e-01)	Acc@1  79.69 ( 82.86)	Acc@5  97.66 ( 98.24)
Epoch: [24][300/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9987e-01 (5.4564e-01)	Acc@1  78.12 ( 82.75)	Acc@5  96.88 ( 98.20)
Epoch: [24][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2412e-01 (5.4706e-01)	Acc@1  85.94 ( 82.67)	Acc@5  98.44 ( 98.17)
Epoch: [24][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0594e-01 (5.4977e-01)	Acc@1  75.78 ( 82.61)	Acc@5  97.66 ( 98.15)
Epoch: [24][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2905e-01 (5.5167e-01)	Acc@1  81.25 ( 82.55)	Acc@5  98.44 ( 98.16)
Epoch: [24][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3387e-01 (5.5407e-01)	Acc@1  85.94 ( 82.49)	Acc@5  98.44 ( 98.16)
Epoch: [24][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0495e-01 (5.5617e-01)	Acc@1  80.47 ( 82.38)	Acc@5 100.00 ( 98.14)
Epoch: [24][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8168e-01 (5.5782e-01)	Acc@1  85.16 ( 82.33)	Acc@5  99.22 ( 98.13)
Epoch: [24][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2527e-01 (5.5881e-01)	Acc@1  80.47 ( 82.28)	Acc@5  97.66 ( 98.11)
Epoch: [24][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7717e-01 (5.6157e-01)	Acc@1  81.25 ( 82.22)	Acc@5  96.88 ( 98.07)
Epoch: [24][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5510e-01 (5.6424e-01)	Acc@1  83.75 ( 82.16)	Acc@5  98.75 ( 98.05)
## e[24] optimizer.zero_grad (sum) time: 0.12867283821105957
## e[24]       loss.backward (sum) time: 2.5321218967437744
## e[24]      optimizer.step (sum) time: 1.0043606758117676
## epoch[24] training(only) time: 18.608756065368652
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.4926e+00 (1.4926e+00)	Acc@1  65.00 ( 65.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.4547e+00 (1.5325e+00)	Acc@1  63.00 ( 65.64)	Acc@5  91.00 ( 87.91)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 1.5639e+00 (1.4553e+00)	Acc@1  65.00 ( 65.86)	Acc@5  87.00 ( 88.57)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8691e+00 (1.4708e+00)	Acc@1  54.00 ( 65.13)	Acc@5  87.00 ( 88.45)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4084e+00 (1.4641e+00)	Acc@1  65.00 ( 65.02)	Acc@5  89.00 ( 88.66)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4893e+00 (1.4833e+00)	Acc@1  65.00 ( 64.67)	Acc@5  88.00 ( 88.41)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4009e+00 (1.4827e+00)	Acc@1  68.00 ( 64.82)	Acc@5  89.00 ( 88.48)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6794e+00 (1.4894e+00)	Acc@1  58.00 ( 64.49)	Acc@5  85.00 ( 88.49)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5367e+00 (1.4927e+00)	Acc@1  68.00 ( 64.62)	Acc@5  85.00 ( 88.32)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6562e+00 (1.4832e+00)	Acc@1  56.00 ( 64.65)	Acc@5  90.00 ( 88.48)
 * Acc@1 64.840 Acc@5 88.720
### epoch[24] execution time: 21.09999394416809
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.194 ( 0.194)	Data  0.153 ( 0.153)	Loss 6.1385e-01 (6.1385e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.88 ( 96.88)
Epoch: [25][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 4.9820e-01 (4.6930e-01)	Acc@1  81.25 ( 84.66)	Acc@5 100.00 ( 98.65)
Epoch: [25][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 4.8293e-01 (4.8570e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 98.74)
Epoch: [25][ 30/391]	Time  0.044 ( 0.052)	Data  0.001 ( 0.007)	Loss 3.6248e-01 (4.8228e-01)	Acc@1  88.28 ( 84.65)	Acc@5  98.44 ( 98.66)
Epoch: [25][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 4.0456e-01 (4.7768e-01)	Acc@1  84.38 ( 84.93)	Acc@5  99.22 ( 98.70)
Epoch: [25][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 4.4369e-01 (4.8006e-01)	Acc@1  87.50 ( 84.97)	Acc@5  98.44 ( 98.71)
Epoch: [25][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.8148e-01 (4.7541e-01)	Acc@1  86.72 ( 85.09)	Acc@5  97.66 ( 98.66)
Epoch: [25][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.4216e-01 (4.7266e-01)	Acc@1  88.28 ( 85.27)	Acc@5  99.22 ( 98.66)
Epoch: [25][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.5358e-01 (4.6954e-01)	Acc@1  85.94 ( 85.34)	Acc@5  98.44 ( 98.65)
Epoch: [25][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0043e-01 (4.7056e-01)	Acc@1  81.25 ( 85.34)	Acc@5  98.44 ( 98.69)
Epoch: [25][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.3226e-01 (4.6960e-01)	Acc@1  82.81 ( 85.32)	Acc@5  99.22 ( 98.68)
Epoch: [25][110/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8946e-01 (4.6743e-01)	Acc@1  84.38 ( 85.37)	Acc@5  99.22 ( 98.73)
Epoch: [25][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1941e-01 (4.7087e-01)	Acc@1  85.94 ( 85.28)	Acc@5  98.44 ( 98.68)
Epoch: [25][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.6943e-01 (4.7656e-01)	Acc@1  83.59 ( 85.15)	Acc@5  98.44 ( 98.62)
Epoch: [25][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.7909e-01 (4.8203e-01)	Acc@1  84.38 ( 84.95)	Acc@5  96.09 ( 98.56)
Epoch: [25][150/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1883e-01 (4.8252e-01)	Acc@1  82.03 ( 84.91)	Acc@5  99.22 ( 98.56)
Epoch: [25][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.3466e-01 (4.8518e-01)	Acc@1  82.03 ( 84.78)	Acc@5  99.22 ( 98.55)
Epoch: [25][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.4106e-01 (4.8928e-01)	Acc@1  76.56 ( 84.65)	Acc@5  98.44 ( 98.52)
Epoch: [25][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.2901e-01 (4.9022e-01)	Acc@1  85.16 ( 84.61)	Acc@5 100.00 ( 98.54)
Epoch: [25][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.0578e-01 (4.9452e-01)	Acc@1  81.25 ( 84.44)	Acc@5  99.22 ( 98.52)
Epoch: [25][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.6628e-01 (4.9739e-01)	Acc@1  82.03 ( 84.32)	Acc@5  99.22 ( 98.51)
Epoch: [25][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5739e-01 (5.0313e-01)	Acc@1  82.03 ( 84.13)	Acc@5  97.66 ( 98.47)
Epoch: [25][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.8747e-01 (5.0480e-01)	Acc@1  78.12 ( 84.06)	Acc@5  96.09 ( 98.44)
Epoch: [25][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8402e-01 (5.0595e-01)	Acc@1  90.62 ( 84.03)	Acc@5  96.88 ( 98.42)
Epoch: [25][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.0463e-01 (5.0692e-01)	Acc@1  82.03 ( 83.96)	Acc@5  97.66 ( 98.42)
Epoch: [25][250/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.2425e-01 (5.0832e-01)	Acc@1  77.34 ( 83.95)	Acc@5  96.88 ( 98.41)
Epoch: [25][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5880e-01 (5.1068e-01)	Acc@1  84.38 ( 83.88)	Acc@5  97.66 ( 98.37)
Epoch: [25][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1123e-01 (5.1205e-01)	Acc@1  81.25 ( 83.84)	Acc@5  97.66 ( 98.36)
Epoch: [25][280/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5117e-01 (5.1448e-01)	Acc@1  85.16 ( 83.82)	Acc@5  94.53 ( 98.34)
Epoch: [25][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8625e-01 (5.1427e-01)	Acc@1  78.91 ( 83.82)	Acc@5  98.44 ( 98.34)
Epoch: [25][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6712e-01 (5.1593e-01)	Acc@1  77.34 ( 83.74)	Acc@5  96.88 ( 98.32)
Epoch: [25][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7905e-01 (5.1695e-01)	Acc@1  85.94 ( 83.67)	Acc@5  99.22 ( 98.32)
Epoch: [25][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6128e-01 (5.1750e-01)	Acc@1  81.25 ( 83.64)	Acc@5  96.88 ( 98.32)
Epoch: [25][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3155e-01 (5.2118e-01)	Acc@1  77.34 ( 83.48)	Acc@5  98.44 ( 98.31)
Epoch: [25][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4071e-01 (5.2315e-01)	Acc@1  81.25 ( 83.44)	Acc@5  96.88 ( 98.27)
Epoch: [25][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0632e-01 (5.2493e-01)	Acc@1  80.47 ( 83.40)	Acc@5  97.66 ( 98.26)
Epoch: [25][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9007e-01 (5.2580e-01)	Acc@1  79.69 ( 83.36)	Acc@5  97.66 ( 98.26)
Epoch: [25][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3736e-01 (5.2777e-01)	Acc@1  84.38 ( 83.31)	Acc@5  97.66 ( 98.24)
Epoch: [25][380/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8872e-01 (5.3115e-01)	Acc@1  78.12 ( 83.22)	Acc@5  96.88 ( 98.20)
Epoch: [25][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7662e-01 (5.3275e-01)	Acc@1  73.75 ( 83.18)	Acc@5  96.25 ( 98.18)
## e[25] optimizer.zero_grad (sum) time: 0.12947940826416016
## e[25]       loss.backward (sum) time: 2.5391509532928467
## e[25]      optimizer.step (sum) time: 1.0141360759735107
## epoch[25] training(only) time: 18.605486631393433
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.4217e+00 (1.4217e+00)	Acc@1  64.00 ( 64.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.7388e+00 (1.6337e+00)	Acc@1  67.00 ( 63.55)	Acc@5  86.00 ( 86.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5192e+00 (1.5811e+00)	Acc@1  63.00 ( 63.33)	Acc@5  87.00 ( 87.24)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8185e+00 (1.5865e+00)	Acc@1  59.00 ( 62.90)	Acc@5  86.00 ( 87.26)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.4319e+00 (1.5955e+00)	Acc@1  71.00 ( 62.78)	Acc@5  89.00 ( 87.56)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6641e+00 (1.6048e+00)	Acc@1  61.00 ( 62.69)	Acc@5  87.00 ( 87.06)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.5621e+00 (1.5926e+00)	Acc@1  62.00 ( 62.64)	Acc@5  89.00 ( 87.28)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6684e+00 (1.5981e+00)	Acc@1  60.00 ( 62.58)	Acc@5  86.00 ( 87.32)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3688e+00 (1.5977e+00)	Acc@1  67.00 ( 62.28)	Acc@5  87.00 ( 87.25)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 2.0002e+00 (1.5908e+00)	Acc@1  58.00 ( 62.58)	Acc@5  87.00 ( 87.51)
 * Acc@1 62.710 Acc@5 87.630
### epoch[25] execution time: 21.104480266571045
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.185 ( 0.185)	Data  0.145 ( 0.145)	Loss 4.1215e-01 (4.1215e-01)	Acc@1  84.38 ( 84.38)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 4.0597e-01 (4.7875e-01)	Acc@1  89.84 ( 85.30)	Acc@5  98.44 ( 98.44)
Epoch: [26][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 4.3151e-01 (4.6973e-01)	Acc@1  86.72 ( 85.12)	Acc@5 100.00 ( 98.59)
Epoch: [26][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 5.0942e-01 (4.6664e-01)	Acc@1  85.16 ( 85.33)	Acc@5  98.44 ( 98.69)
Epoch: [26][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.7742e-01 (4.5456e-01)	Acc@1  89.06 ( 85.71)	Acc@5  99.22 ( 98.70)
Epoch: [26][ 50/391]	Time  0.048 ( 0.050)	Data  0.001 ( 0.005)	Loss 4.9491e-01 (4.5986e-01)	Acc@1  85.94 ( 85.78)	Acc@5  97.66 ( 98.61)
Epoch: [26][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.9015e-01 (4.5568e-01)	Acc@1  78.91 ( 85.82)	Acc@5  98.44 ( 98.68)
Epoch: [26][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.7384e-01 (4.5117e-01)	Acc@1  83.59 ( 85.98)	Acc@5  99.22 ( 98.69)
Epoch: [26][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.7012e-01 (4.4693e-01)	Acc@1  85.94 ( 85.98)	Acc@5 100.00 ( 98.74)
Epoch: [26][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.9445e-01 (4.4647e-01)	Acc@1  85.94 ( 85.94)	Acc@5 100.00 ( 98.76)
Epoch: [26][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4955e-01 (4.4254e-01)	Acc@1  89.06 ( 85.98)	Acc@5  98.44 ( 98.78)
Epoch: [26][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4529e-01 (4.4230e-01)	Acc@1  85.16 ( 85.94)	Acc@5  98.44 ( 98.77)
Epoch: [26][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.7483e-01 (4.4341e-01)	Acc@1  81.25 ( 85.88)	Acc@5  98.44 ( 98.78)
Epoch: [26][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0781e-01 (4.4568e-01)	Acc@1  86.72 ( 85.90)	Acc@5  97.66 ( 98.74)
Epoch: [26][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.3077e-01 (4.4981e-01)	Acc@1  84.38 ( 85.67)	Acc@5  96.88 ( 98.72)
Epoch: [26][150/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6848e-01 (4.5219e-01)	Acc@1  87.50 ( 85.59)	Acc@5  99.22 ( 98.70)
Epoch: [26][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7056e-01 (4.5608e-01)	Acc@1  82.81 ( 85.49)	Acc@5  98.44 ( 98.68)
Epoch: [26][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.2530e-01 (4.6054e-01)	Acc@1  85.16 ( 85.31)	Acc@5  98.44 ( 98.67)
Epoch: [26][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.7753e-01 (4.6465e-01)	Acc@1  79.69 ( 85.18)	Acc@5  96.88 ( 98.63)
Epoch: [26][190/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.6765e-01 (4.6757e-01)	Acc@1  82.81 ( 85.08)	Acc@5  99.22 ( 98.61)
Epoch: [26][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.3701e-01 (4.6929e-01)	Acc@1  78.91 ( 84.97)	Acc@5  97.66 ( 98.62)
Epoch: [26][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1605e-01 (4.7061e-01)	Acc@1  83.59 ( 84.91)	Acc@5  98.44 ( 98.63)
Epoch: [26][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3010e-01 (4.7166e-01)	Acc@1  84.38 ( 84.87)	Acc@5 100.00 ( 98.66)
Epoch: [26][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5855e-01 (4.7352e-01)	Acc@1  79.69 ( 84.80)	Acc@5  98.44 ( 98.66)
Epoch: [26][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.2326e-01 (4.7459e-01)	Acc@1  86.72 ( 84.74)	Acc@5  98.44 ( 98.67)
Epoch: [26][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.2967e-01 (4.7487e-01)	Acc@1  83.59 ( 84.72)	Acc@5  97.66 ( 98.67)
Epoch: [26][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.4071e-01 (4.7687e-01)	Acc@1  84.38 ( 84.64)	Acc@5  97.66 ( 98.65)
Epoch: [26][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7612e-01 (4.7944e-01)	Acc@1  78.91 ( 84.55)	Acc@5  98.44 ( 98.63)
Epoch: [26][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6736e-01 (4.8068e-01)	Acc@1  85.94 ( 84.52)	Acc@5  98.44 ( 98.62)
Epoch: [26][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6019e-01 (4.8241e-01)	Acc@1  84.38 ( 84.49)	Acc@5  96.09 ( 98.60)
Epoch: [26][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0793e-01 (4.8476e-01)	Acc@1  79.69 ( 84.39)	Acc@5  96.88 ( 98.59)
Epoch: [26][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6109e-01 (4.8672e-01)	Acc@1  81.25 ( 84.29)	Acc@5  97.66 ( 98.58)
Epoch: [26][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2100e-01 (4.8962e-01)	Acc@1  81.25 ( 84.19)	Acc@5 100.00 ( 98.57)
Epoch: [26][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2165e-01 (4.9278e-01)	Acc@1  82.81 ( 84.11)	Acc@5  97.66 ( 98.56)
Epoch: [26][340/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0200e-01 (4.9404e-01)	Acc@1  85.16 ( 84.09)	Acc@5  97.66 ( 98.55)
Epoch: [26][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4545e-01 (4.9508e-01)	Acc@1  79.69 ( 84.07)	Acc@5  98.44 ( 98.55)
Epoch: [26][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4750e-01 (4.9651e-01)	Acc@1  82.03 ( 84.05)	Acc@5  97.66 ( 98.53)
Epoch: [26][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1674e-01 (4.9770e-01)	Acc@1  78.91 ( 84.00)	Acc@5  97.66 ( 98.52)
Epoch: [26][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0187e-01 (5.0013e-01)	Acc@1  79.69 ( 83.93)	Acc@5 100.00 ( 98.49)
Epoch: [26][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8585e-01 (5.0214e-01)	Acc@1  77.50 ( 83.86)	Acc@5  95.00 ( 98.48)
## e[26] optimizer.zero_grad (sum) time: 0.12946438789367676
## e[26]       loss.backward (sum) time: 2.5366756916046143
## e[26]      optimizer.step (sum) time: 1.0026803016662598
## epoch[26] training(only) time: 18.624449968338013
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.4408e+00 (1.4408e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.6185e+00 (1.5375e+00)	Acc@1  63.00 ( 64.09)	Acc@5  90.00 ( 87.27)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4257e+00 (1.4972e+00)	Acc@1  66.00 ( 64.95)	Acc@5  87.00 ( 88.19)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.0866e+00 (1.5409e+00)	Acc@1  48.00 ( 63.71)	Acc@5  88.00 ( 88.10)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4792e+00 (1.5284e+00)	Acc@1  60.00 ( 63.63)	Acc@5  88.00 ( 88.27)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6125e+00 (1.5394e+00)	Acc@1  64.00 ( 63.51)	Acc@5  85.00 ( 88.00)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.5162e+00 (1.5193e+00)	Acc@1  58.00 ( 63.82)	Acc@5  89.00 ( 88.23)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.7587e+00 (1.5249e+00)	Acc@1  55.00 ( 63.68)	Acc@5  84.00 ( 88.11)
Test: [ 80/100]	Time  0.024 ( 0.024)	Loss 1.4699e+00 (1.5381e+00)	Acc@1  66.00 ( 63.42)	Acc@5  89.00 ( 88.05)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.9639e+00 (1.5337e+00)	Acc@1  54.00 ( 63.44)	Acc@5  86.00 ( 88.15)
 * Acc@1 63.610 Acc@5 88.350
### epoch[26] execution time: 21.111997842788696
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.180 ( 0.180)	Data  0.140 ( 0.140)	Loss 4.1648e-01 (4.1648e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 99.22)
Epoch: [27][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 4.7004e-01 (4.1893e-01)	Acc@1  83.59 ( 86.08)	Acc@5  99.22 ( 99.08)
Epoch: [27][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 4.7171e-01 (4.2797e-01)	Acc@1  83.59 ( 85.79)	Acc@5  98.44 ( 98.92)
Epoch: [27][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.5569e-01 (4.2817e-01)	Acc@1  82.81 ( 85.94)	Acc@5  99.22 ( 98.97)
Epoch: [27][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.3637e-01 (4.2793e-01)	Acc@1  85.16 ( 86.13)	Acc@5  99.22 ( 98.95)
Epoch: [27][ 50/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.8396e-01 (4.2481e-01)	Acc@1  88.28 ( 86.38)	Acc@5 100.00 ( 98.99)
Epoch: [27][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.8063e-01 (4.2480e-01)	Acc@1  87.50 ( 86.51)	Acc@5 100.00 ( 99.01)
Epoch: [27][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.6902e-01 (4.2419e-01)	Acc@1  85.16 ( 86.63)	Acc@5  98.44 ( 98.94)
Epoch: [27][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.0321e-01 (4.2045e-01)	Acc@1  89.06 ( 86.72)	Acc@5  99.22 ( 98.98)
Epoch: [27][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.8704e-01 (4.2642e-01)	Acc@1  84.38 ( 86.52)	Acc@5  96.88 ( 98.97)
Epoch: [27][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9508e-01 (4.3017e-01)	Acc@1  88.28 ( 86.39)	Acc@5  98.44 ( 98.92)
Epoch: [27][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.0783e-01 (4.3136e-01)	Acc@1  89.06 ( 86.33)	Acc@5  98.44 ( 98.90)
Epoch: [27][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0216e-01 (4.3041e-01)	Acc@1  82.03 ( 86.29)	Acc@5  99.22 ( 98.90)
Epoch: [27][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.2417e-01 (4.3440e-01)	Acc@1  87.50 ( 86.13)	Acc@5  98.44 ( 98.89)
Epoch: [27][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.3931e-01 (4.3648e-01)	Acc@1  85.16 ( 86.11)	Acc@5  99.22 ( 98.86)
Epoch: [27][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.5845e-01 (4.3885e-01)	Acc@1  85.16 ( 86.02)	Acc@5  99.22 ( 98.83)
Epoch: [27][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5054e-01 (4.3985e-01)	Acc@1  85.16 ( 85.92)	Acc@5  96.88 ( 98.85)
Epoch: [27][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1804e-01 (4.4055e-01)	Acc@1  86.72 ( 85.93)	Acc@5  99.22 ( 98.87)
Epoch: [27][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.4136e-01 (4.4139e-01)	Acc@1  87.50 ( 85.91)	Acc@5 100.00 ( 98.88)
Epoch: [27][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3339e-01 (4.4301e-01)	Acc@1  86.72 ( 85.79)	Acc@5  99.22 ( 98.88)
Epoch: [27][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.4690e-01 (4.4528e-01)	Acc@1  83.59 ( 85.68)	Acc@5  99.22 ( 98.89)
Epoch: [27][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1181e-01 (4.4642e-01)	Acc@1  82.81 ( 85.65)	Acc@5  97.66 ( 98.86)
Epoch: [27][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3577e-01 (4.4992e-01)	Acc@1  81.25 ( 85.51)	Acc@5  98.44 ( 98.83)
Epoch: [27][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4818e-01 (4.5013e-01)	Acc@1  81.25 ( 85.52)	Acc@5  98.44 ( 98.82)
Epoch: [27][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7482e-01 (4.5285e-01)	Acc@1  88.28 ( 85.46)	Acc@5  98.44 ( 98.80)
Epoch: [27][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1301e-01 (4.5429e-01)	Acc@1  85.94 ( 85.46)	Acc@5  96.88 ( 98.78)
Epoch: [27][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8449e-01 (4.5713e-01)	Acc@1  84.38 ( 85.37)	Acc@5  99.22 ( 98.75)
Epoch: [27][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9531e-01 (4.6026e-01)	Acc@1  82.03 ( 85.27)	Acc@5  98.44 ( 98.73)
Epoch: [27][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1857e-01 (4.6429e-01)	Acc@1  82.03 ( 85.17)	Acc@5  98.44 ( 98.71)
Epoch: [27][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4381e-01 (4.6663e-01)	Acc@1  75.78 ( 85.08)	Acc@5  97.66 ( 98.72)
Epoch: [27][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9596e-01 (4.6821e-01)	Acc@1  85.94 ( 85.06)	Acc@5  97.66 ( 98.70)
Epoch: [27][310/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0415e-01 (4.7074e-01)	Acc@1  78.12 ( 84.96)	Acc@5  96.88 ( 98.68)
Epoch: [27][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3149e-01 (4.7276e-01)	Acc@1  82.03 ( 84.91)	Acc@5  98.44 ( 98.65)
Epoch: [27][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0148e-01 (4.7405e-01)	Acc@1  82.81 ( 84.87)	Acc@5  96.88 ( 98.63)
Epoch: [27][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0751e-01 (4.7656e-01)	Acc@1  76.56 ( 84.77)	Acc@5  96.88 ( 98.60)
Epoch: [27][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6699e-01 (4.7880e-01)	Acc@1  87.50 ( 84.70)	Acc@5  98.44 ( 98.58)
Epoch: [27][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0860e-01 (4.8089e-01)	Acc@1  83.59 ( 84.66)	Acc@5  97.66 ( 98.57)
Epoch: [27][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9913e-01 (4.8339e-01)	Acc@1  82.03 ( 84.56)	Acc@5  97.66 ( 98.56)
Epoch: [27][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7042e-01 (4.8575e-01)	Acc@1  79.69 ( 84.53)	Acc@5  98.44 ( 98.54)
Epoch: [27][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1576e-01 (4.8774e-01)	Acc@1  73.75 ( 84.43)	Acc@5  97.50 ( 98.55)
## e[27] optimizer.zero_grad (sum) time: 0.1304302215576172
## e[27]       loss.backward (sum) time: 2.532366991043091
## e[27]      optimizer.step (sum) time: 1.0143952369689941
## epoch[27] training(only) time: 18.583861827850342
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.6505e+00 (1.6505e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.7537e+00 (1.8579e+00)	Acc@1  59.00 ( 61.18)	Acc@5  88.00 ( 86.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.6042e+00 (1.7428e+00)	Acc@1  64.00 ( 61.76)	Acc@5  88.00 ( 87.19)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.3012e+00 (1.7663e+00)	Acc@1  46.00 ( 61.48)	Acc@5  79.00 ( 86.55)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.6020e+00 (1.7443e+00)	Acc@1  67.00 ( 61.32)	Acc@5  90.00 ( 87.05)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.8796e+00 (1.7524e+00)	Acc@1  63.00 ( 61.12)	Acc@5  85.00 ( 86.80)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.5019e+00 (1.7252e+00)	Acc@1  62.00 ( 61.48)	Acc@5  87.00 ( 87.00)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.8098e+00 (1.7210e+00)	Acc@1  59.00 ( 61.54)	Acc@5  85.00 ( 86.97)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6947e+00 (1.7310e+00)	Acc@1  60.00 ( 61.57)	Acc@5  89.00 ( 86.90)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 2.3441e+00 (1.7151e+00)	Acc@1  55.00 ( 61.80)	Acc@5  81.00 ( 86.97)
 * Acc@1 61.830 Acc@5 86.950
### epoch[27] execution time: 21.092271327972412
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.185 ( 0.185)	Data  0.146 ( 0.146)	Loss 4.7401e-01 (4.7401e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [28][ 10/391]	Time  0.045 ( 0.059)	Data  0.001 ( 0.015)	Loss 4.2365e-01 (4.7482e-01)	Acc@1  87.50 ( 85.23)	Acc@5 100.00 ( 98.79)
Epoch: [28][ 20/391]	Time  0.045 ( 0.053)	Data  0.001 ( 0.009)	Loss 5.1428e-01 (4.7056e-01)	Acc@1  85.16 ( 85.45)	Acc@5  97.66 ( 98.77)
Epoch: [28][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 5.3252e-01 (4.5481e-01)	Acc@1  83.59 ( 85.89)	Acc@5  96.09 ( 98.71)
Epoch: [28][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.5794e-01 (4.4181e-01)	Acc@1  94.53 ( 86.38)	Acc@5 100.00 ( 98.80)
Epoch: [28][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 4.7008e-01 (4.4405e-01)	Acc@1  82.81 ( 86.03)	Acc@5  98.44 ( 98.81)
Epoch: [28][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.4433e-01 (4.4228e-01)	Acc@1  89.84 ( 85.94)	Acc@5  98.44 ( 98.81)
Epoch: [28][ 70/391]	Time  0.044 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.5906e-01 (4.3759e-01)	Acc@1  82.03 ( 86.04)	Acc@5  99.22 ( 98.82)
Epoch: [28][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.5351e-01 (4.3452e-01)	Acc@1  86.72 ( 86.09)	Acc@5  99.22 ( 98.84)
Epoch: [28][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5329e-01 (4.3707e-01)	Acc@1  92.97 ( 86.00)	Acc@5  98.44 ( 98.84)
Epoch: [28][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6318e-01 (4.3428e-01)	Acc@1  90.62 ( 86.19)	Acc@5  99.22 ( 98.84)
Epoch: [28][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3252e-01 (4.3069e-01)	Acc@1  91.41 ( 86.29)	Acc@5  99.22 ( 98.89)
Epoch: [28][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8102e-01 (4.3010e-01)	Acc@1  90.62 ( 86.25)	Acc@5 100.00 ( 98.89)
Epoch: [28][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3948e-01 (4.2767e-01)	Acc@1  91.41 ( 86.29)	Acc@5  98.44 ( 98.93)
Epoch: [28][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1009e-01 (4.2883e-01)	Acc@1  85.94 ( 86.31)	Acc@5  99.22 ( 98.91)
Epoch: [28][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.2537e-01 (4.3122e-01)	Acc@1  83.59 ( 86.24)	Acc@5 100.00 ( 98.93)
Epoch: [28][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1520e-01 (4.3372e-01)	Acc@1  81.25 ( 86.11)	Acc@5  96.88 ( 98.87)
Epoch: [28][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.2797e-01 (4.3623e-01)	Acc@1  80.47 ( 86.06)	Acc@5  96.09 ( 98.85)
Epoch: [28][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.5484e-01 (4.3842e-01)	Acc@1  80.47 ( 85.97)	Acc@5  96.88 ( 98.84)
Epoch: [28][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.0977e-01 (4.4108e-01)	Acc@1  74.22 ( 85.83)	Acc@5  99.22 ( 98.82)
Epoch: [28][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.0282e-01 (4.4078e-01)	Acc@1  82.81 ( 85.80)	Acc@5  98.44 ( 98.82)
Epoch: [28][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.6920e-01 (4.4325e-01)	Acc@1  86.72 ( 85.68)	Acc@5  98.44 ( 98.80)
Epoch: [28][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.8534e-01 (4.4525e-01)	Acc@1  85.94 ( 85.62)	Acc@5  98.44 ( 98.78)
Epoch: [28][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.0238e-01 (4.4834e-01)	Acc@1  83.59 ( 85.50)	Acc@5 100.00 ( 98.78)
Epoch: [28][240/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6775e-01 (4.5010e-01)	Acc@1  82.81 ( 85.45)	Acc@5  96.88 ( 98.77)
Epoch: [28][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5525e-01 (4.5251e-01)	Acc@1  81.25 ( 85.42)	Acc@5  99.22 ( 98.74)
Epoch: [28][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3082e-01 (4.5377e-01)	Acc@1  82.81 ( 85.39)	Acc@5  96.88 ( 98.71)
Epoch: [28][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3034e-01 (4.5664e-01)	Acc@1  79.69 ( 85.32)	Acc@5  98.44 ( 98.69)
Epoch: [28][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2092e-01 (4.5853e-01)	Acc@1  82.81 ( 85.28)	Acc@5  98.44 ( 98.66)
Epoch: [28][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5211e-01 (4.6113e-01)	Acc@1  79.69 ( 85.20)	Acc@5  98.44 ( 98.65)
Epoch: [28][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9276e-01 (4.6482e-01)	Acc@1  87.50 ( 85.12)	Acc@5  96.09 ( 98.63)
Epoch: [28][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3129e-01 (4.6702e-01)	Acc@1  85.16 ( 85.05)	Acc@5  99.22 ( 98.63)
Epoch: [28][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3046e-01 (4.6964e-01)	Acc@1  78.91 ( 84.96)	Acc@5  97.66 ( 98.62)
Epoch: [28][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3672e-01 (4.7032e-01)	Acc@1  81.25 ( 84.93)	Acc@5  98.44 ( 98.61)
Epoch: [28][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3618e-01 (4.7174e-01)	Acc@1  86.72 ( 84.87)	Acc@5  99.22 ( 98.61)
Epoch: [28][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9092e-01 (4.7319e-01)	Acc@1  81.25 ( 84.82)	Acc@5  98.44 ( 98.60)
Epoch: [28][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0882e-01 (4.7519e-01)	Acc@1  82.81 ( 84.73)	Acc@5  98.44 ( 98.60)
Epoch: [28][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0926e-01 (4.7690e-01)	Acc@1  82.81 ( 84.67)	Acc@5  98.44 ( 98.58)
Epoch: [28][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9620e-01 (4.7829e-01)	Acc@1  80.47 ( 84.64)	Acc@5  92.97 ( 98.55)
Epoch: [28][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4249e-01 (4.8051e-01)	Acc@1  80.00 ( 84.58)	Acc@5  96.25 ( 98.53)
## e[28] optimizer.zero_grad (sum) time: 0.12887024879455566
## e[28]       loss.backward (sum) time: 2.538620948791504
## e[28]      optimizer.step (sum) time: 1.0000667572021484
## epoch[28] training(only) time: 18.604958057403564
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.6310e+00 (1.6310e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.9109e+00 (1.7580e+00)	Acc@1  62.00 ( 61.64)	Acc@5  88.00 ( 87.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.5272e+00 (1.6335e+00)	Acc@1  65.00 ( 63.67)	Acc@5  89.00 ( 88.24)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.0554e+00 (1.6541e+00)	Acc@1  57.00 ( 63.16)	Acc@5  84.00 ( 87.77)
Test: [ 40/100]	Time  0.027 ( 0.026)	Loss 2.0804e+00 (1.6745e+00)	Acc@1  55.00 ( 62.95)	Acc@5  81.00 ( 87.63)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.7995e+00 (1.6952e+00)	Acc@1  64.00 ( 62.55)	Acc@5  83.00 ( 87.22)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3717e+00 (1.6586e+00)	Acc@1  62.00 ( 62.98)	Acc@5  89.00 ( 87.56)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.7109e+00 (1.6656e+00)	Acc@1  58.00 ( 62.90)	Acc@5  84.00 ( 87.61)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5319e+00 (1.6622e+00)	Acc@1  64.00 ( 62.85)	Acc@5  91.00 ( 87.65)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 2.1456e+00 (1.6495e+00)	Acc@1  60.00 ( 63.25)	Acc@5  87.00 ( 87.70)
 * Acc@1 63.420 Acc@5 87.850
### epoch[28] execution time: 21.088388204574585
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.183 ( 0.183)	Data  0.143 ( 0.143)	Loss 3.9585e-01 (3.9585e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [29][ 10/391]	Time  0.048 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.9941e-01 (4.3517e-01)	Acc@1  90.62 ( 85.58)	Acc@5 100.00 ( 98.72)
Epoch: [29][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 3.5866e-01 (4.2654e-01)	Acc@1  87.50 ( 85.60)	Acc@5 100.00 ( 99.07)
Epoch: [29][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.5675e-01 (4.2963e-01)	Acc@1  87.50 ( 85.66)	Acc@5  98.44 ( 98.94)
Epoch: [29][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.8332e-01 (4.2736e-01)	Acc@1  85.94 ( 86.17)	Acc@5  98.44 ( 98.95)
Epoch: [29][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 4.5378e-01 (4.2565e-01)	Acc@1  85.94 ( 86.12)	Acc@5  97.66 ( 98.96)
Epoch: [29][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.8076e-01 (4.2411e-01)	Acc@1  85.16 ( 86.24)	Acc@5  99.22 ( 98.99)
Epoch: [29][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.2681e-01 (4.2046e-01)	Acc@1  83.59 ( 86.17)	Acc@5 100.00 ( 99.04)
Epoch: [29][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.5228e-01 (4.1928e-01)	Acc@1  85.16 ( 86.11)	Acc@5  99.22 ( 99.10)
Epoch: [29][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6024e-01 (4.1776e-01)	Acc@1  88.28 ( 86.24)	Acc@5 100.00 ( 99.13)
Epoch: [29][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4542e-01 (4.1323e-01)	Acc@1  92.19 ( 86.45)	Acc@5  98.44 ( 99.13)
Epoch: [29][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.3509e-01 (4.1346e-01)	Acc@1  85.94 ( 86.39)	Acc@5  99.22 ( 99.13)
Epoch: [29][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.5974e-01 (4.1366e-01)	Acc@1  83.59 ( 86.45)	Acc@5  97.66 ( 99.09)
Epoch: [29][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.5098e-01 (4.1196e-01)	Acc@1  85.94 ( 86.56)	Acc@5  99.22 ( 99.09)
Epoch: [29][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.6280e-01 (4.1334e-01)	Acc@1  85.16 ( 86.46)	Acc@5  99.22 ( 99.06)
Epoch: [29][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.3160e-01 (4.0911e-01)	Acc@1  88.28 ( 86.59)	Acc@5  99.22 ( 99.08)
Epoch: [29][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5211e-01 (4.0955e-01)	Acc@1  80.47 ( 86.64)	Acc@5  98.44 ( 99.07)
Epoch: [29][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.6598e-01 (4.1084e-01)	Acc@1  82.03 ( 86.59)	Acc@5  98.44 ( 99.06)
Epoch: [29][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.2398e-01 (4.1190e-01)	Acc@1  88.28 ( 86.57)	Acc@5  98.44 ( 99.06)
Epoch: [29][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.7620e-01 (4.1300e-01)	Acc@1  86.72 ( 86.57)	Acc@5  99.22 ( 99.06)
Epoch: [29][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.1485e-01 (4.1210e-01)	Acc@1  86.72 ( 86.64)	Acc@5  98.44 ( 99.06)
Epoch: [29][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.7123e-01 (4.1244e-01)	Acc@1  86.72 ( 86.61)	Acc@5  98.44 ( 99.05)
Epoch: [29][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.9007e-01 (4.1478e-01)	Acc@1  82.03 ( 86.50)	Acc@5 100.00 ( 99.05)
Epoch: [29][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5976e-01 (4.1635e-01)	Acc@1  87.50 ( 86.47)	Acc@5 100.00 ( 99.05)
Epoch: [29][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1535e-01 (4.1966e-01)	Acc@1  85.94 ( 86.33)	Acc@5  99.22 ( 99.04)
Epoch: [29][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1747e-01 (4.2245e-01)	Acc@1  84.38 ( 86.22)	Acc@5 100.00 ( 99.04)
Epoch: [29][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.9258e-01 (4.2477e-01)	Acc@1  86.72 ( 86.17)	Acc@5 100.00 ( 99.04)
Epoch: [29][270/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7620e-01 (4.2719e-01)	Acc@1  82.81 ( 86.08)	Acc@5  99.22 ( 99.04)
Epoch: [29][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1921e-01 (4.3177e-01)	Acc@1  81.25 ( 85.95)	Acc@5  97.66 ( 99.01)
Epoch: [29][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8562e-01 (4.3473e-01)	Acc@1  82.03 ( 85.84)	Acc@5  96.88 ( 99.00)
Epoch: [29][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6294e-01 (4.3611e-01)	Acc@1  85.16 ( 85.81)	Acc@5  98.44 ( 98.99)
Epoch: [29][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7205e-01 (4.3989e-01)	Acc@1  90.62 ( 85.71)	Acc@5 100.00 ( 98.97)
Epoch: [29][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0681e-01 (4.4254e-01)	Acc@1  81.25 ( 85.61)	Acc@5  97.66 ( 98.94)
Epoch: [29][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6580e-01 (4.4475e-01)	Acc@1  82.81 ( 85.53)	Acc@5  98.44 ( 98.92)
Epoch: [29][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4301e-01 (4.4786e-01)	Acc@1  86.72 ( 85.49)	Acc@5  96.88 ( 98.88)
Epoch: [29][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8401e-01 (4.5018e-01)	Acc@1  80.47 ( 85.39)	Acc@5  97.66 ( 98.85)
Epoch: [29][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1791e-01 (4.5166e-01)	Acc@1  85.94 ( 85.37)	Acc@5  96.88 ( 98.82)
Epoch: [29][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3929e-01 (4.5433e-01)	Acc@1  79.69 ( 85.28)	Acc@5  98.44 ( 98.80)
Epoch: [29][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0704e-01 (4.5763e-01)	Acc@1  82.81 ( 85.15)	Acc@5 100.00 ( 98.79)
Epoch: [29][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0407e-01 (4.5972e-01)	Acc@1  83.75 ( 85.06)	Acc@5 100.00 ( 98.77)
## e[29] optimizer.zero_grad (sum) time: 0.12887835502624512
## e[29]       loss.backward (sum) time: 2.548044443130493
## e[29]      optimizer.step (sum) time: 1.0156145095825195
## epoch[29] training(only) time: 18.62382698059082
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.6180e+00 (1.6180e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.4749e+00 (1.5414e+00)	Acc@1  61.00 ( 63.45)	Acc@5  89.00 ( 87.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.7641e+00 (1.5101e+00)	Acc@1  60.00 ( 64.67)	Acc@5  88.00 ( 88.29)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.0968e+00 (1.5456e+00)	Acc@1  50.00 ( 63.97)	Acc@5  85.00 ( 87.77)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.4293e+00 (1.5419e+00)	Acc@1  64.00 ( 63.78)	Acc@5  90.00 ( 87.83)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.9003e+00 (1.5757e+00)	Acc@1  61.00 ( 63.25)	Acc@5  86.00 ( 87.76)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4746e+00 (1.5542e+00)	Acc@1  63.00 ( 63.66)	Acc@5  91.00 ( 87.87)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 2.0815e+00 (1.5734e+00)	Acc@1  59.00 ( 63.62)	Acc@5  80.00 ( 87.63)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5806e+00 (1.5783e+00)	Acc@1  63.00 ( 63.41)	Acc@5  85.00 ( 87.49)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 2.1038e+00 (1.5681e+00)	Acc@1  52.00 ( 63.49)	Acc@5  85.00 ( 87.74)
 * Acc@1 63.690 Acc@5 87.910
### epoch[29] execution time: 21.1184823513031
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.194 ( 0.194)	Data  0.151 ( 0.151)	Loss 4.4360e-01 (4.4360e-01)	Acc@1  86.72 ( 86.72)	Acc@5  99.22 ( 99.22)
Epoch: [30][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 5.1099e-01 (4.2236e-01)	Acc@1  83.59 ( 86.15)	Acc@5 100.00 ( 99.43)
Epoch: [30][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 3.6428e-01 (3.9695e-01)	Acc@1  89.06 ( 86.98)	Acc@5  98.44 ( 99.37)
Epoch: [30][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 4.1888e-01 (3.7991e-01)	Acc@1  87.50 ( 87.58)	Acc@5  99.22 ( 99.34)
Epoch: [30][ 40/391]	Time  0.050 ( 0.051)	Data  0.001 ( 0.006)	Loss 4.1142e-01 (3.6317e-01)	Acc@1  87.50 ( 88.28)	Acc@5  99.22 ( 99.35)
Epoch: [30][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.0172e-01 (3.4864e-01)	Acc@1  92.19 ( 88.71)	Acc@5 100.00 ( 99.33)
Epoch: [30][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.3326e-01 (3.3898e-01)	Acc@1  92.97 ( 89.19)	Acc@5 100.00 ( 99.33)
Epoch: [30][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.2123e-01 (3.2832e-01)	Acc@1  93.75 ( 89.61)	Acc@5  99.22 ( 99.37)
Epoch: [30][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.7937e-01 (3.1830e-01)	Acc@1  90.62 ( 89.96)	Acc@5  99.22 ( 99.40)
Epoch: [30][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.6407e-01 (3.0990e-01)	Acc@1  96.88 ( 90.27)	Acc@5 100.00 ( 99.46)
Epoch: [30][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3185e-01 (3.0336e-01)	Acc@1  93.75 ( 90.60)	Acc@5 100.00 ( 99.49)
Epoch: [30][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.7107e-01 (2.9874e-01)	Acc@1  87.50 ( 90.71)	Acc@5  99.22 ( 99.51)
Epoch: [30][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9909e-01 (2.9783e-01)	Acc@1  90.62 ( 90.79)	Acc@5  99.22 ( 99.47)
Epoch: [30][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5432e-01 (2.9290e-01)	Acc@1  93.75 ( 90.97)	Acc@5 100.00 ( 99.48)
Epoch: [30][140/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1209e-01 (2.9010e-01)	Acc@1  90.62 ( 91.06)	Acc@5 100.00 ( 99.50)
Epoch: [30][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1635e-01 (2.8637e-01)	Acc@1  85.94 ( 91.21)	Acc@5 100.00 ( 99.51)
Epoch: [30][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5642e-01 (2.8356e-01)	Acc@1  89.06 ( 91.33)	Acc@5 100.00 ( 99.53)
Epoch: [30][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7898e-01 (2.8087e-01)	Acc@1  93.75 ( 91.46)	Acc@5  99.22 ( 99.55)
Epoch: [30][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0190e-01 (2.7905e-01)	Acc@1  93.75 ( 91.51)	Acc@5  99.22 ( 99.56)
Epoch: [30][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0336e-01 (2.7799e-01)	Acc@1  95.31 ( 91.53)	Acc@5 100.00 ( 99.55)
Epoch: [30][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8265e-01 (2.7551e-01)	Acc@1  95.31 ( 91.62)	Acc@5 100.00 ( 99.56)
Epoch: [30][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5771e-01 (2.7222e-01)	Acc@1  96.88 ( 91.72)	Acc@5 100.00 ( 99.57)
Epoch: [30][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6619e-01 (2.6890e-01)	Acc@1  96.09 ( 91.85)	Acc@5 100.00 ( 99.57)
Epoch: [30][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5915e-01 (2.6554e-01)	Acc@1  91.41 ( 91.95)	Acc@5 100.00 ( 99.57)
Epoch: [30][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8830e-01 (2.6361e-01)	Acc@1  92.97 ( 91.99)	Acc@5 100.00 ( 99.58)
Epoch: [30][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9362e-01 (2.6046e-01)	Acc@1  94.53 ( 92.08)	Acc@5  99.22 ( 99.58)
Epoch: [30][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5877e-01 (2.5868e-01)	Acc@1  92.19 ( 92.15)	Acc@5  98.44 ( 99.58)
Epoch: [30][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7907e-01 (2.5659e-01)	Acc@1  92.19 ( 92.22)	Acc@5  99.22 ( 99.59)
Epoch: [30][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1526e-01 (2.5455e-01)	Acc@1  92.19 ( 92.30)	Acc@5 100.00 ( 99.60)
Epoch: [30][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7811e-01 (2.5187e-01)	Acc@1  95.31 ( 92.42)	Acc@5 100.00 ( 99.61)
Epoch: [30][300/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8334e-01 (2.4996e-01)	Acc@1  94.53 ( 92.50)	Acc@5 100.00 ( 99.62)
Epoch: [30][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0080e-01 (2.4941e-01)	Acc@1  96.09 ( 92.52)	Acc@5  99.22 ( 99.62)
Epoch: [30][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1779e-01 (2.4720e-01)	Acc@1  98.44 ( 92.60)	Acc@5 100.00 ( 99.63)
Epoch: [30][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5354e-01 (2.4620e-01)	Acc@1  94.53 ( 92.62)	Acc@5 100.00 ( 99.63)
Epoch: [30][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1298e-01 (2.4441e-01)	Acc@1  94.53 ( 92.69)	Acc@5  99.22 ( 99.64)
Epoch: [30][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2225e-01 (2.4315e-01)	Acc@1  93.75 ( 92.73)	Acc@5 100.00 ( 99.65)
Epoch: [30][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5774e-01 (2.4140e-01)	Acc@1  92.19 ( 92.79)	Acc@5  98.44 ( 99.65)
Epoch: [30][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0099e-01 (2.3995e-01)	Acc@1  92.19 ( 92.84)	Acc@5 100.00 ( 99.65)
Epoch: [30][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4521e-01 (2.3833e-01)	Acc@1  96.09 ( 92.91)	Acc@5 100.00 ( 99.66)
Epoch: [30][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7276e-01 (2.3662e-01)	Acc@1  95.00 ( 92.98)	Acc@5 100.00 ( 99.66)
## e[30] optimizer.zero_grad (sum) time: 0.12922096252441406
## e[30]       loss.backward (sum) time: 2.5601441860198975
## e[30]      optimizer.step (sum) time: 1.0142173767089844
## epoch[30] training(only) time: 18.632503271102905
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2318e+00 (1.2318e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.3688e+00 (1.2739e+00)	Acc@1  72.00 ( 70.55)	Acc@5  90.00 ( 91.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3264e+00 (1.2063e+00)	Acc@1  71.00 ( 71.33)	Acc@5  90.00 ( 91.57)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6384e+00 (1.2317e+00)	Acc@1  56.00 ( 70.74)	Acc@5  89.00 ( 91.06)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.1379e+00 (1.2186e+00)	Acc@1  70.00 ( 70.44)	Acc@5  94.00 ( 91.51)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4335e+00 (1.2349e+00)	Acc@1  66.00 ( 70.25)	Acc@5  90.00 ( 91.51)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0202e+00 (1.2040e+00)	Acc@1  73.00 ( 70.79)	Acc@5  94.00 ( 91.74)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.5385e+00 (1.2100e+00)	Acc@1  65.00 ( 70.54)	Acc@5  90.00 ( 91.68)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.2587e+00 (1.2070e+00)	Acc@1  72.00 ( 70.59)	Acc@5  91.00 ( 91.67)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6757e+00 (1.1960e+00)	Acc@1  64.00 ( 70.85)	Acc@5  88.00 ( 91.69)
 * Acc@1 71.100 Acc@5 91.770
### epoch[30] execution time: 21.124067783355713
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.191 ( 0.191)	Data  0.149 ( 0.149)	Loss 1.1008e-01 (1.1008e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.5136e-01 (1.6293e-01)	Acc@1  96.09 ( 95.53)	Acc@5 100.00 (100.00)
Epoch: [31][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.9233e-01 (1.6697e-01)	Acc@1  94.53 ( 95.57)	Acc@5 100.00 ( 99.93)
Epoch: [31][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.9354e-01 (1.6785e-01)	Acc@1  96.09 ( 95.67)	Acc@5 100.00 ( 99.90)
Epoch: [31][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.0321e-01 (1.6028e-01)	Acc@1  96.88 ( 95.73)	Acc@5 100.00 ( 99.92)
Epoch: [31][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.2364e-01 (1.5992e-01)	Acc@1  97.66 ( 95.82)	Acc@5 100.00 ( 99.91)
Epoch: [31][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.5899e-01 (1.6327e-01)	Acc@1  95.31 ( 95.70)	Acc@5 100.00 ( 99.90)
Epoch: [31][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.3879e-01 (1.6515e-01)	Acc@1  96.09 ( 95.72)	Acc@5  99.22 ( 99.89)
Epoch: [31][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.4769e-01 (1.6465e-01)	Acc@1  96.88 ( 95.68)	Acc@5 100.00 ( 99.88)
Epoch: [31][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.2440e-01 (1.6372e-01)	Acc@1  96.88 ( 95.68)	Acc@5 100.00 ( 99.88)
Epoch: [31][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1035e-01 (1.6295e-01)	Acc@1  98.44 ( 95.76)	Acc@5 100.00 ( 99.88)
Epoch: [31][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5814e-01 (1.6250e-01)	Acc@1  97.66 ( 95.80)	Acc@5  99.22 ( 99.86)
Epoch: [31][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2495e-01 (1.6252e-01)	Acc@1  96.09 ( 95.78)	Acc@5 100.00 ( 99.86)
Epoch: [31][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6936e-01 (1.6106e-01)	Acc@1  94.53 ( 95.87)	Acc@5 100.00 ( 99.87)
Epoch: [31][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3957e-01 (1.6085e-01)	Acc@1  96.09 ( 95.86)	Acc@5 100.00 ( 99.87)
Epoch: [31][150/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8771e-01 (1.6039e-01)	Acc@1  95.31 ( 95.89)	Acc@5 100.00 ( 99.87)
Epoch: [31][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0802e-01 (1.6215e-01)	Acc@1  89.84 ( 95.80)	Acc@5  99.22 ( 99.86)
Epoch: [31][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2735e-01 (1.6259e-01)	Acc@1  92.97 ( 95.77)	Acc@5  99.22 ( 99.86)
Epoch: [31][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8970e-01 (1.6231e-01)	Acc@1  96.88 ( 95.79)	Acc@5 100.00 ( 99.85)
Epoch: [31][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5238e-01 (1.6104e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.86)
Epoch: [31][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.3531e-02 (1.5957e-01)	Acc@1  99.22 ( 95.87)	Acc@5 100.00 ( 99.86)
Epoch: [31][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4112e-01 (1.5980e-01)	Acc@1  96.09 ( 95.86)	Acc@5  99.22 ( 99.86)
Epoch: [31][220/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2411e-01 (1.5831e-01)	Acc@1  97.66 ( 95.92)	Acc@5  99.22 ( 99.85)
Epoch: [31][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0308e-01 (1.5857e-01)	Acc@1  94.53 ( 95.89)	Acc@5 100.00 ( 99.86)
Epoch: [31][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7006e-01 (1.5852e-01)	Acc@1  96.09 ( 95.91)	Acc@5 100.00 ( 99.86)
Epoch: [31][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4445e-01 (1.5899e-01)	Acc@1  95.31 ( 95.88)	Acc@5  99.22 ( 99.86)
Epoch: [31][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2183e-01 (1.5803e-01)	Acc@1  98.44 ( 95.92)	Acc@5 100.00 ( 99.85)
Epoch: [31][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0749e-01 (1.5763e-01)	Acc@1  93.75 ( 95.92)	Acc@5 100.00 ( 99.86)
Epoch: [31][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9515e-01 (1.5773e-01)	Acc@1  96.09 ( 95.93)	Acc@5  99.22 ( 99.86)
Epoch: [31][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0789e-01 (1.5775e-01)	Acc@1  97.66 ( 95.92)	Acc@5 100.00 ( 99.86)
Epoch: [31][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4720e-01 (1.5700e-01)	Acc@1  95.31 ( 95.94)	Acc@5  99.22 ( 99.86)
Epoch: [31][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0795e-01 (1.5676e-01)	Acc@1  97.66 ( 95.95)	Acc@5 100.00 ( 99.86)
Epoch: [31][320/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1694e-01 (1.5628e-01)	Acc@1  96.88 ( 95.96)	Acc@5  99.22 ( 99.86)
Epoch: [31][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9108e-01 (1.5574e-01)	Acc@1  93.75 ( 95.99)	Acc@5 100.00 ( 99.87)
Epoch: [31][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9876e-01 (1.5611e-01)	Acc@1  93.75 ( 95.95)	Acc@5  99.22 ( 99.86)
Epoch: [31][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5880e-01 (1.5596e-01)	Acc@1  96.09 ( 95.94)	Acc@5 100.00 ( 99.87)
Epoch: [31][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6198e-01 (1.5572e-01)	Acc@1  93.75 ( 95.95)	Acc@5 100.00 ( 99.87)
Epoch: [31][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0549e-01 (1.5530e-01)	Acc@1  98.44 ( 95.98)	Acc@5 100.00 ( 99.87)
Epoch: [31][380/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0903e-01 (1.5478e-01)	Acc@1  97.66 ( 95.99)	Acc@5 100.00 ( 99.87)
Epoch: [31][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2085e-01 (1.5421e-01)	Acc@1  96.25 ( 95.99)	Acc@5  97.50 ( 99.87)
## e[31] optimizer.zero_grad (sum) time: 0.12790441513061523
## e[31]       loss.backward (sum) time: 2.536658763885498
## e[31]      optimizer.step (sum) time: 1.0037202835083008
## epoch[31] training(only) time: 18.648895025253296
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2239e+00 (1.2239e+00)	Acc@1  71.00 ( 71.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.036)	Loss 1.3966e+00 (1.2901e+00)	Acc@1  71.00 ( 70.18)	Acc@5  90.00 ( 91.00)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2617e+00 (1.2107e+00)	Acc@1  73.00 ( 71.62)	Acc@5  91.00 ( 91.90)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6356e+00 (1.2392e+00)	Acc@1  59.00 ( 71.10)	Acc@5  90.00 ( 91.48)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.1658e+00 (1.2258e+00)	Acc@1  73.00 ( 71.12)	Acc@5  93.00 ( 91.76)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3711e+00 (1.2358e+00)	Acc@1  69.00 ( 70.98)	Acc@5  88.00 ( 91.65)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0203e+00 (1.2025e+00)	Acc@1  73.00 ( 71.54)	Acc@5  96.00 ( 91.97)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5224e+00 (1.2075e+00)	Acc@1  63.00 ( 71.21)	Acc@5  89.00 ( 91.94)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.2318e+00 (1.2046e+00)	Acc@1  74.00 ( 71.25)	Acc@5  90.00 ( 91.86)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6760e+00 (1.1913e+00)	Acc@1  62.00 ( 71.46)	Acc@5  88.00 ( 92.02)
 * Acc@1 71.740 Acc@5 92.110
### epoch[31] execution time: 21.148141384124756
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.192 ( 0.192)	Data  0.149 ( 0.149)	Loss 1.6504e-01 (1.6504e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.7198e-01 (1.3329e-01)	Acc@1  95.31 ( 97.02)	Acc@5  99.22 ( 99.86)
Epoch: [32][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.2781e-01 (1.3359e-01)	Acc@1  97.66 ( 96.99)	Acc@5  99.22 ( 99.85)
Epoch: [32][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.1582e-01 (1.3358e-01)	Acc@1  96.09 ( 96.93)	Acc@5  99.22 ( 99.85)
Epoch: [32][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.1373e-01 (1.3479e-01)	Acc@1  97.66 ( 96.84)	Acc@5 100.00 ( 99.87)
Epoch: [32][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.0965e-01 (1.4079e-01)	Acc@1  96.88 ( 96.57)	Acc@5 100.00 ( 99.88)
Epoch: [32][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.1174e-02 (1.3716e-01)	Acc@1  98.44 ( 96.71)	Acc@5 100.00 ( 99.88)
Epoch: [32][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.5117e-01 (1.3661e-01)	Acc@1  95.31 ( 96.65)	Acc@5 100.00 ( 99.89)
Epoch: [32][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.0967e-01 (1.3328e-01)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.90)
Epoch: [32][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.4767e-01 (1.3275e-01)	Acc@1  94.53 ( 96.69)	Acc@5  99.22 ( 99.91)
Epoch: [32][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9837e-01 (1.3266e-01)	Acc@1  95.31 ( 96.73)	Acc@5 100.00 ( 99.91)
Epoch: [32][110/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8452e-01 (1.3382e-01)	Acc@1  95.31 ( 96.66)	Acc@5 100.00 ( 99.92)
Epoch: [32][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.3717e-02 (1.3411e-01)	Acc@1  98.44 ( 96.64)	Acc@5 100.00 ( 99.92)
Epoch: [32][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4748e-01 (1.3364e-01)	Acc@1  94.53 ( 96.58)	Acc@5 100.00 ( 99.92)
Epoch: [32][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8523e-01 (1.3268e-01)	Acc@1  90.62 ( 96.58)	Acc@5 100.00 ( 99.92)
Epoch: [32][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.7916e-02 (1.3166e-01)	Acc@1  99.22 ( 96.62)	Acc@5 100.00 ( 99.92)
Epoch: [32][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3584e-01 (1.3082e-01)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.93)
Epoch: [32][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.6736e-02 (1.2967e-01)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.93)
Epoch: [32][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.8509e-02 (1.2904e-01)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 ( 99.93)
Epoch: [32][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5297e-01 (1.2957e-01)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.93)
Epoch: [32][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0973e-01 (1.2927e-01)	Acc@1  98.44 ( 96.70)	Acc@5 100.00 ( 99.93)
Epoch: [32][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2884e-01 (1.2926e-01)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.93)
Epoch: [32][220/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4549e-01 (1.3001e-01)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 ( 99.93)
Epoch: [32][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0727e-01 (1.2958e-01)	Acc@1  98.44 ( 96.71)	Acc@5 100.00 ( 99.93)
Epoch: [32][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0694e-01 (1.2933e-01)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.93)
Epoch: [32][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0206e-01 (1.2927e-01)	Acc@1  98.44 ( 96.71)	Acc@5 100.00 ( 99.93)
Epoch: [32][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4822e-02 (1.2911e-01)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.93)
Epoch: [32][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5426e-02 (1.2903e-01)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.93)
Epoch: [32][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.9180e-02 (1.2835e-01)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.93)
Epoch: [32][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6724e-01 (1.2931e-01)	Acc@1  95.31 ( 96.69)	Acc@5 100.00 ( 99.93)
Epoch: [32][300/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.8330e-02 (1.2939e-01)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.92)
Epoch: [32][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3046e-02 (1.2863e-01)	Acc@1  98.44 ( 96.71)	Acc@5 100.00 ( 99.92)
Epoch: [32][320/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2486e-01 (1.2877e-01)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.92)
Epoch: [32][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8817e-01 (1.2889e-01)	Acc@1  94.53 ( 96.71)	Acc@5 100.00 ( 99.92)
Epoch: [32][340/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0093e-01 (1.2911e-01)	Acc@1  96.09 ( 96.70)	Acc@5 100.00 ( 99.92)
Epoch: [32][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3393e-01 (1.2897e-01)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 ( 99.92)
Epoch: [32][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0811e-01 (1.2843e-01)	Acc@1  96.88 ( 96.70)	Acc@5 100.00 ( 99.92)
Epoch: [32][370/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4458e-01 (1.2832e-01)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.92)
Epoch: [32][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5183e-02 (1.2833e-01)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.92)
Epoch: [32][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4152e-01 (1.2804e-01)	Acc@1  96.25 ( 96.69)	Acc@5 100.00 ( 99.93)
## e[32] optimizer.zero_grad (sum) time: 0.12915754318237305
## e[32]       loss.backward (sum) time: 2.5416836738586426
## e[32]      optimizer.step (sum) time: 1.0054452419281006
## epoch[32] training(only) time: 18.6173996925354
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2529e+00 (1.2529e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.3985e+00 (1.3032e+00)	Acc@1  71.00 ( 70.09)	Acc@5  91.00 ( 91.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.2897e+00 (1.2209e+00)	Acc@1  71.00 ( 71.05)	Acc@5  91.00 ( 91.86)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6868e+00 (1.2515e+00)	Acc@1  55.00 ( 70.71)	Acc@5  89.00 ( 91.48)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2076e+00 (1.2355e+00)	Acc@1  71.00 ( 71.07)	Acc@5  92.00 ( 91.83)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3629e+00 (1.2490e+00)	Acc@1  71.00 ( 70.86)	Acc@5  89.00 ( 91.71)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0586e+00 (1.2171e+00)	Acc@1  72.00 ( 71.36)	Acc@5  94.00 ( 91.98)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5766e+00 (1.2214e+00)	Acc@1  67.00 ( 71.23)	Acc@5  89.00 ( 92.00)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.2415e+00 (1.2183e+00)	Acc@1  74.00 ( 71.23)	Acc@5  90.00 ( 91.94)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6517e+00 (1.2045e+00)	Acc@1  64.00 ( 71.43)	Acc@5  89.00 ( 92.08)
 * Acc@1 71.750 Acc@5 92.150
### epoch[32] execution time: 21.11870527267456
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.191 ( 0.191)	Data  0.147 ( 0.147)	Loss 1.5176e-01 (1.5176e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.015)	Loss 8.6803e-02 (9.8619e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 5.3618e-02 (9.7289e-02)	Acc@1 100.00 ( 97.77)	Acc@5 100.00 (100.00)
Epoch: [33][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 9.0959e-02 (9.7856e-02)	Acc@1  97.66 ( 97.78)	Acc@5 100.00 (100.00)
Epoch: [33][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.9331e-01 (1.0448e-01)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [33][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.0690e-01 (1.0564e-01)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 (100.00)
Epoch: [33][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0928e-01 (1.0750e-01)	Acc@1  96.09 ( 97.54)	Acc@5 100.00 (100.00)
Epoch: [33][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0026e-01 (1.0796e-01)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.99)
Epoch: [33][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.4400e-01 (1.0841e-01)	Acc@1  97.66 ( 97.47)	Acc@5  99.22 ( 99.97)
Epoch: [33][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.0560e-02 (1.0693e-01)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.97)
Epoch: [33][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.8171e-02 (1.0714e-01)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [33][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3291e-01 (1.0768e-01)	Acc@1  96.88 ( 97.47)	Acc@5  99.22 ( 99.96)
Epoch: [33][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4230e-01 (1.0739e-01)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.96)
Epoch: [33][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2710e-01 (1.0743e-01)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.96)
Epoch: [33][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0907e-01 (1.0800e-01)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [33][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.8308e-02 (1.0738e-01)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.96)
Epoch: [33][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4650e-01 (1.0757e-01)	Acc@1  96.09 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [33][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5678e-01 (1.0850e-01)	Acc@1  95.31 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [33][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.0879e-02 (1.0866e-01)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [33][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1741e-01 (1.0815e-01)	Acc@1  95.31 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [33][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5965e-01 (1.0908e-01)	Acc@1  95.31 ( 97.34)	Acc@5 100.00 ( 99.97)
Epoch: [33][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7583e-01 (1.0935e-01)	Acc@1  94.53 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [33][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.3806e-02 (1.0984e-01)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [33][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7055e-02 (1.0902e-01)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [33][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2901e-01 (1.0888e-01)	Acc@1  96.88 ( 97.35)	Acc@5  99.22 ( 99.96)
Epoch: [33][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1208e-01 (1.0911e-01)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [33][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3684e-02 (1.0909e-01)	Acc@1 100.00 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [33][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2639e-01 (1.0964e-01)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 ( 99.95)
Epoch: [33][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3746e-01 (1.0940e-01)	Acc@1  95.31 ( 97.32)	Acc@5 100.00 ( 99.95)
Epoch: [33][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4967e-01 (1.0975e-01)	Acc@1  96.09 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [33][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2637e-02 (1.0967e-01)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.95)
Epoch: [33][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5941e-01 (1.1026e-01)	Acc@1  95.31 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [33][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0264e-01 (1.1031e-01)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [33][330/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4724e-01 (1.1092e-01)	Acc@1  93.75 ( 97.24)	Acc@5 100.00 ( 99.95)
Epoch: [33][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2898e-01 (1.1105e-01)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [33][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9369e-02 (1.1103e-01)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.95)
Epoch: [33][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2831e-02 (1.1044e-01)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.95)
Epoch: [33][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0767e-01 (1.1046e-01)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [33][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0574e-01 (1.1025e-01)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [33][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8365e-01 (1.1093e-01)	Acc@1  96.25 ( 97.27)	Acc@5  98.75 ( 99.94)
## e[33] optimizer.zero_grad (sum) time: 0.1293799877166748
## e[33]       loss.backward (sum) time: 2.532092809677124
## e[33]      optimizer.step (sum) time: 1.0049784183502197
## epoch[33] training(only) time: 18.568540334701538
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.2443e+00 (1.2443e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.4364e+00 (1.2860e+00)	Acc@1  70.00 ( 70.18)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3345e+00 (1.2230e+00)	Acc@1  71.00 ( 71.29)	Acc@5  92.00 ( 92.14)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6725e+00 (1.2521e+00)	Acc@1  55.00 ( 70.94)	Acc@5  89.00 ( 91.65)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2057e+00 (1.2336e+00)	Acc@1  73.00 ( 71.12)	Acc@5  93.00 ( 92.02)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.2988e+00 (1.2439e+00)	Acc@1  69.00 ( 70.88)	Acc@5  89.00 ( 91.84)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 9.9489e-01 (1.2111e+00)	Acc@1  72.00 ( 71.46)	Acc@5  95.00 ( 92.13)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5287e+00 (1.2127e+00)	Acc@1  67.00 ( 71.24)	Acc@5  92.00 ( 92.18)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.2247e+00 (1.2103e+00)	Acc@1  74.00 ( 71.27)	Acc@5  89.00 ( 92.14)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6119e+00 (1.1964e+00)	Acc@1  65.00 ( 71.52)	Acc@5  90.00 ( 92.25)
 * Acc@1 71.790 Acc@5 92.330
### epoch[33] execution time: 21.074973344802856
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.189 ( 0.189)	Data  0.150 ( 0.150)	Loss 1.2977e-01 (1.2977e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.22 ( 99.22)
Epoch: [34][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.0220e-01 (9.8795e-02)	Acc@1  96.88 ( 97.59)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 6.3934e-02 (9.7997e-02)	Acc@1 100.00 ( 97.84)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 9.7204e-02 (9.4891e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 40/391]	Time  0.048 ( 0.051)	Data  0.001 ( 0.006)	Loss 1.1449e-01 (9.4312e-02)	Acc@1  95.31 ( 97.92)	Acc@5 100.00 ( 99.94)
Epoch: [34][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.4298e-01 (9.4500e-02)	Acc@1  96.09 ( 97.93)	Acc@5 100.00 ( 99.94)
Epoch: [34][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4289e-01 (9.8035e-02)	Acc@1  96.09 ( 97.77)	Acc@5 100.00 ( 99.95)
Epoch: [34][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0223e-01 (9.8931e-02)	Acc@1  98.44 ( 97.74)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 80/391]	Time  0.048 ( 0.049)	Data  0.001 ( 0.004)	Loss 7.2089e-02 (1.0011e-01)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.91)
Epoch: [34][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.0030e-02 (9.9391e-02)	Acc@1  99.22 ( 97.68)	Acc@5 100.00 ( 99.91)
Epoch: [34][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.3653e-02 (1.0035e-01)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.91)
Epoch: [34][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.5584e-02 (1.0007e-01)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.92)
Epoch: [34][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.9971e-02 (1.0048e-01)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.92)
Epoch: [34][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.5924e-02 (1.0016e-01)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.92)
Epoch: [34][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.9031e-02 (9.9495e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.93)
Epoch: [34][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0942e-01 (1.0031e-01)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.93)
Epoch: [34][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0048e-01 (1.0025e-01)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.93)
Epoch: [34][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0566e-01 (9.9042e-02)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 ( 99.93)
Epoch: [34][180/391]	Time  0.050 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.5166e-02 (9.9403e-02)	Acc@1  98.44 ( 97.70)	Acc@5 100.00 ( 99.94)
Epoch: [34][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.6062e-02 (1.0015e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.93)
Epoch: [34][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2484e-01 (9.9711e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.93)
Epoch: [34][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.7354e-02 (9.9285e-02)	Acc@1  98.44 ( 97.69)	Acc@5  99.22 ( 99.93)
Epoch: [34][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.4916e-02 (9.9360e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.93)
Epoch: [34][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.0427e-02 (9.9838e-02)	Acc@1  99.22 ( 97.63)	Acc@5 100.00 ( 99.93)
Epoch: [34][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.5166e-02 (1.0001e-01)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.94)
Epoch: [34][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0044e-01 (9.9671e-02)	Acc@1  96.88 ( 97.63)	Acc@5 100.00 ( 99.94)
Epoch: [34][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.6715e-02 (9.9361e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.94)
Epoch: [34][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.4987e-02 (9.9504e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.94)
Epoch: [34][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2519e-02 (9.9750e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.94)
Epoch: [34][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1287e-02 (9.9483e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [34][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.9516e-02 (9.9318e-02)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [34][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7985e-02 (9.9155e-02)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.94)
Epoch: [34][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0929e-01 (9.8657e-02)	Acc@1  95.31 ( 97.63)	Acc@5 100.00 ( 99.95)
Epoch: [34][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1615e-02 (9.8400e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [34][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0857e-01 (9.8269e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [34][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3732e-02 (9.8394e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [34][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3210e-02 (9.8460e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [34][370/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1179e-02 (9.8689e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [34][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0258e-02 (9.8650e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [34][390/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1218e-01 (9.8695e-02)	Acc@1  96.25 ( 97.64)	Acc@5 100.00 ( 99.95)
## e[34] optimizer.zero_grad (sum) time: 0.1277306079864502
## e[34]       loss.backward (sum) time: 2.5420985221862793
## e[34]      optimizer.step (sum) time: 1.0175495147705078
## epoch[34] training(only) time: 18.61443781852722
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.2129e+00 (1.2129e+00)	Acc@1  72.00 ( 72.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.4268e+00 (1.3094e+00)	Acc@1  68.00 ( 70.73)	Acc@5  91.00 ( 91.36)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3334e+00 (1.2376e+00)	Acc@1  70.00 ( 71.62)	Acc@5  92.00 ( 92.19)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6975e+00 (1.2705e+00)	Acc@1  60.00 ( 71.26)	Acc@5  89.00 ( 91.61)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2247e+00 (1.2559e+00)	Acc@1  72.00 ( 71.27)	Acc@5  93.00 ( 92.07)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3650e+00 (1.2639e+00)	Acc@1  68.00 ( 71.08)	Acc@5  89.00 ( 91.98)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 9.2884e-01 (1.2286e+00)	Acc@1  74.00 ( 71.79)	Acc@5  97.00 ( 92.31)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5292e+00 (1.2319e+00)	Acc@1  65.00 ( 71.54)	Acc@5  91.00 ( 92.39)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.2155e+00 (1.2298e+00)	Acc@1  74.00 ( 71.52)	Acc@5  90.00 ( 92.32)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7038e+00 (1.2156e+00)	Acc@1  66.00 ( 71.67)	Acc@5  90.00 ( 92.47)
 * Acc@1 71.890 Acc@5 92.520
### epoch[34] execution time: 21.120132446289062
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.178 ( 0.178)	Data  0.138 ( 0.138)	Loss 1.1026e-01 (1.1026e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.048 ( 0.059)	Data  0.001 ( 0.015)	Loss 9.9137e-02 (8.5723e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 9.2285e-02 (8.9194e-02)	Acc@1  96.88 ( 97.95)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 7.1907e-02 (9.2137e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.2277e-01 (9.6258e-02)	Acc@1  99.22 ( 97.75)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 50/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.6802e-02 (9.1701e-02)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.2676e-02 (9.1120e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 7.4599e-02 (9.0713e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 8.1097e-02 (9.0913e-02)	Acc@1  99.22 ( 97.93)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.0521e-02 (8.9824e-02)	Acc@1 100.00 ( 97.96)	Acc@5 100.00 ( 99.99)
Epoch: [35][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1277e-02 (9.0440e-02)	Acc@1 100.00 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [35][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.5339e-02 (9.0155e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.98)
Epoch: [35][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2241e-01 (9.1738e-02)	Acc@1  96.09 ( 97.88)	Acc@5 100.00 ( 99.96)
Epoch: [35][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0057e-01 (9.2529e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.96)
Epoch: [35][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0720e-01 (9.2474e-02)	Acc@1  92.97 ( 97.84)	Acc@5 100.00 ( 99.96)
Epoch: [35][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.8540e-02 (9.3103e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.96)
Epoch: [35][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1510e-01 (9.3509e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.96)
Epoch: [35][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.7861e-02 (9.3782e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [35][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1646e-01 (9.3657e-02)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.96)
Epoch: [35][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.4254e-02 (9.3560e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [35][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.0586e-02 (9.3217e-02)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 ( 99.96)
Epoch: [35][210/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.4884e-02 (9.2778e-02)	Acc@1  99.22 ( 97.86)	Acc@5 100.00 ( 99.96)
Epoch: [35][220/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.5177e-02 (9.3267e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [35][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0521e-01 (9.3189e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [35][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0883e-01 (9.2752e-02)	Acc@1  96.09 ( 97.82)	Acc@5 100.00 ( 99.97)
Epoch: [35][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1223e-01 (9.2257e-02)	Acc@1  96.09 ( 97.84)	Acc@5 100.00 ( 99.97)
Epoch: [35][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9517e-02 (9.2202e-02)	Acc@1  96.09 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [35][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1225e-02 (9.1866e-02)	Acc@1  99.22 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [35][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1701e-01 (9.1867e-02)	Acc@1  96.09 ( 97.82)	Acc@5 100.00 ( 99.97)
Epoch: [35][290/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2389e-01 (9.1908e-02)	Acc@1  96.09 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [35][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5045e-02 (9.1612e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [35][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5206e-02 (9.1561e-02)	Acc@1  98.44 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [35][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2105e-02 (9.1328e-02)	Acc@1  96.09 ( 97.80)	Acc@5 100.00 ( 99.98)
Epoch: [35][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0869e-01 (9.1403e-02)	Acc@1  95.31 ( 97.78)	Acc@5 100.00 ( 99.98)
Epoch: [35][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7841e-02 (9.1499e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.97)
Epoch: [35][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7321e-02 (9.1311e-02)	Acc@1  99.22 ( 97.79)	Acc@5 100.00 ( 99.97)
Epoch: [35][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5382e-02 (9.0944e-02)	Acc@1  99.22 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [35][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0941e-01 (9.0869e-02)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [35][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3504e-02 (9.1117e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.97)
Epoch: [35][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0228e-01 (9.1072e-02)	Acc@1  96.25 ( 97.80)	Acc@5 100.00 ( 99.97)
## e[35] optimizer.zero_grad (sum) time: 0.12967944145202637
## e[35]       loss.backward (sum) time: 2.543421506881714
## e[35]      optimizer.step (sum) time: 1.0106170177459717
## epoch[35] training(only) time: 18.59356689453125
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2369e+00 (1.2369e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.4130e+00 (1.3093e+00)	Acc@1  68.00 ( 70.64)	Acc@5  92.00 ( 91.27)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2852e+00 (1.2371e+00)	Acc@1  73.00 ( 71.86)	Acc@5  92.00 ( 92.05)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8042e+00 (1.2743e+00)	Acc@1  56.00 ( 71.23)	Acc@5  91.00 ( 91.68)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2256e+00 (1.2581e+00)	Acc@1  69.00 ( 71.17)	Acc@5  93.00 ( 92.05)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3231e+00 (1.2695e+00)	Acc@1  69.00 ( 70.96)	Acc@5  89.00 ( 91.84)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 9.5247e-01 (1.2373e+00)	Acc@1  73.00 ( 71.51)	Acc@5  97.00 ( 92.20)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5674e+00 (1.2396e+00)	Acc@1  64.00 ( 71.41)	Acc@5  92.00 ( 92.32)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.1997e+00 (1.2360e+00)	Acc@1  76.00 ( 71.41)	Acc@5  90.00 ( 92.23)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6467e+00 (1.2213e+00)	Acc@1  64.00 ( 71.65)	Acc@5  92.00 ( 92.37)
 * Acc@1 71.930 Acc@5 92.460
### epoch[35] execution time: 21.076871156692505
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.189 ( 0.189)	Data  0.148 ( 0.148)	Loss 8.8648e-02 (8.8648e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.016)	Loss 5.8889e-02 (9.2767e-02)	Acc@1 100.00 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 8.4810e-02 (8.8086e-02)	Acc@1  97.66 ( 97.84)	Acc@5 100.00 (100.00)
Epoch: [36][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.1914e-02 (8.1936e-02)	Acc@1 100.00 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [36][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 8.3903e-02 (8.0220e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [36][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 8.2967e-02 (7.7879e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [36][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.2715e-02 (7.7301e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 (100.00)
Epoch: [36][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.5191e-02 (7.7407e-02)	Acc@1  96.09 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [36][ 80/391]	Time  0.044 ( 0.049)	Data  0.001 ( 0.004)	Loss 9.4697e-02 (7.8930e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [36][ 90/391]	Time  0.048 ( 0.049)	Data  0.001 ( 0.004)	Loss 9.4218e-02 (8.0693e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [36][100/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.7327e-02 (7.9398e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [36][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.3783e-02 (7.9174e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [36][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.8053e-02 (7.9985e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [36][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.1917e-02 (8.0840e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 (100.00)
Epoch: [36][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.3520e-02 (8.1079e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 (100.00)
Epoch: [36][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.0355e-02 (8.0930e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [36][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.5945e-02 (8.1070e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 (100.00)
Epoch: [36][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.0854e-02 (8.1961e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [36][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.0424e-02 (8.2153e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [36][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.9959e-02 (8.1882e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [36][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.3344e-02 (8.1933e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [36][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1735e-01 (8.1873e-02)	Acc@1  96.09 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [36][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.4271e-02 (8.1799e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [36][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.3000e-02 (8.2305e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [36][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.1336e-02 (8.1717e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [36][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.0997e-02 (8.1556e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [36][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.7472e-02 (8.1861e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [36][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.4896e-02 (8.1645e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [36][280/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.8392e-02 (8.1930e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [36][290/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.7366e-02 (8.1328e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [36][300/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.1032e-02 (8.1220e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [36][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5655e-02 (8.1523e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [36][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8103e-02 (8.1610e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [36][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8729e-02 (8.1364e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [36][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5054e-02 (8.1089e-02)	Acc@1  97.66 ( 98.20)	Acc@5  99.22 ( 99.99)
Epoch: [36][350/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9373e-02 (8.0861e-02)	Acc@1 100.00 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [36][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1261e-02 (8.0941e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [36][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1664e-01 (8.0887e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 ( 99.99)
Epoch: [36][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.6078e-02 (8.1256e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [36][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9897e-02 (8.1389e-02)	Acc@1  98.75 ( 98.19)	Acc@5 100.00 ( 99.98)
## e[36] optimizer.zero_grad (sum) time: 0.12824273109436035
## e[36]       loss.backward (sum) time: 2.543483018875122
## e[36]      optimizer.step (sum) time: 1.0056283473968506
## epoch[36] training(only) time: 18.627763032913208
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.2433e+00 (1.2433e+00)	Acc@1  71.00 ( 71.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.4539e+00 (1.2999e+00)	Acc@1  68.00 ( 71.18)	Acc@5  91.00 ( 91.73)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3513e+00 (1.2293e+00)	Acc@1  72.00 ( 71.71)	Acc@5  94.00 ( 92.33)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6525e+00 (1.2613e+00)	Acc@1  55.00 ( 71.16)	Acc@5  90.00 ( 91.84)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.1956e+00 (1.2470e+00)	Acc@1  73.00 ( 71.37)	Acc@5  94.00 ( 92.17)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3019e+00 (1.2622e+00)	Acc@1  68.00 ( 71.08)	Acc@5  88.00 ( 91.84)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 9.7867e-01 (1.2315e+00)	Acc@1  75.00 ( 71.61)	Acc@5  95.00 ( 92.10)
Test: [ 70/100]	Time  0.027 ( 0.025)	Loss 1.5298e+00 (1.2335e+00)	Acc@1  68.00 ( 71.45)	Acc@5  91.00 ( 92.15)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.2448e+00 (1.2313e+00)	Acc@1  73.00 ( 71.56)	Acc@5  90.00 ( 92.10)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6722e+00 (1.2167e+00)	Acc@1  64.00 ( 71.76)	Acc@5  91.00 ( 92.26)
 * Acc@1 72.000 Acc@5 92.340
### epoch[36] execution time: 21.127636909484863
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.192 ( 0.192)	Data  0.150 ( 0.150)	Loss 4.6014e-02 (4.6014e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.048 ( 0.060)	Data  0.001 ( 0.016)	Loss 9.3491e-02 (7.5083e-02)	Acc@1  96.09 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 4.6825e-02 (7.2022e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 (100.00)
Epoch: [37][ 30/391]	Time  0.048 ( 0.051)	Data  0.001 ( 0.007)	Loss 7.1832e-02 (7.5286e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 9.5342e-02 (7.3880e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 6.6198e-02 (7.4534e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 8.8698e-02 (7.4489e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.5961e-02 (7.4780e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 8.0142e-02 (7.4499e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.5977e-02 (7.4030e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [37][100/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.6730e-02 (7.4456e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [37][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.4033e-02 (7.4983e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 ( 99.99)
Epoch: [37][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.7288e-02 (7.4598e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [37][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.5949e-02 (7.5373e-02)	Acc@1  99.22 ( 98.39)	Acc@5  99.22 ( 99.98)
Epoch: [37][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.0329e-02 (7.4856e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [37][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.2405e-02 (7.4960e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 ( 99.97)
Epoch: [37][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.2241e-02 (7.5507e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [37][170/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.8614e-02 (7.5825e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [37][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8423e-02 (7.5369e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [37][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.6211e-02 (7.5351e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [37][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.5379e-02 (7.5515e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [37][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8231e-02 (7.5356e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [37][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.2444e-02 (7.5311e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [37][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.1329e-02 (7.4714e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [37][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.7285e-02 (7.4585e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [37][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.9855e-02 (7.4741e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [37][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0100e-01 (7.5637e-02)	Acc@1  97.66 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [37][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7632e-02 (7.5808e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [37][280/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7378e-02 (7.5566e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [37][290/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.5416e-02 (7.5956e-02)	Acc@1  96.09 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [37][300/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.9313e-02 (7.6135e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [37][310/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.7687e-02 (7.6094e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [37][320/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.6790e-02 (7.5739e-02)	Acc@1 100.00 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [37][330/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.2965e-02 (7.5780e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [37][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3735e-02 (7.6134e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [37][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4214e-02 (7.5901e-02)	Acc@1 100.00 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [37][360/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8283e-02 (7.5832e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [37][370/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3980e-02 (7.5794e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [37][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7834e-02 (7.5630e-02)	Acc@1 100.00 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [37][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0670e-01 (7.5694e-02)	Acc@1  96.25 ( 98.34)	Acc@5 100.00 ( 99.98)
## e[37] optimizer.zero_grad (sum) time: 0.13061976432800293
## e[37]       loss.backward (sum) time: 2.5625412464141846
## e[37]      optimizer.step (sum) time: 1.0067577362060547
## epoch[37] training(only) time: 18.6860830783844
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2908e+00 (1.2908e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5256e+00 (1.3354e+00)	Acc@1  68.00 ( 70.27)	Acc@5  91.00 ( 91.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2910e+00 (1.2566e+00)	Acc@1  72.00 ( 71.52)	Acc@5  92.00 ( 91.90)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 1.6951e+00 (1.2891e+00)	Acc@1  56.00 ( 71.13)	Acc@5  91.00 ( 91.42)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2424e+00 (1.2761e+00)	Acc@1  74.00 ( 71.46)	Acc@5  96.00 ( 91.90)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3338e+00 (1.2870e+00)	Acc@1  70.00 ( 71.20)	Acc@5  89.00 ( 91.78)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0053e+00 (1.2518e+00)	Acc@1  73.00 ( 71.70)	Acc@5  95.00 ( 92.16)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6001e+00 (1.2551e+00)	Acc@1  68.00 ( 71.56)	Acc@5  90.00 ( 92.30)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.2525e+00 (1.2523e+00)	Acc@1  76.00 ( 71.64)	Acc@5  89.00 ( 92.22)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6726e+00 (1.2387e+00)	Acc@1  66.00 ( 71.89)	Acc@5  90.00 ( 92.32)
 * Acc@1 72.180 Acc@5 92.400
### epoch[37] execution time: 21.17805528640747
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.189 ( 0.189)	Data  0.140 ( 0.140)	Loss 1.0836e-01 (1.0836e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.015)	Loss 9.5920e-02 (7.7847e-02)	Acc@1  96.09 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 5.9146e-02 (7.4837e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.6651e-02 (7.3889e-02)	Acc@1  99.22 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [38][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 5.8837e-02 (7.3069e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [38][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 9.2950e-02 (7.3285e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 8.2968e-02 (7.3390e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [38][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 7.0228e-02 (7.5532e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.0355e-01 (7.5813e-02)	Acc@1  95.31 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.4734e-02 (7.4838e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 ( 99.97)
Epoch: [38][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2035e-01 (7.3803e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [38][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.6976e-02 (7.3308e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [38][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.6028e-02 (7.2976e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [38][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.4428e-02 (7.2483e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [38][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.7782e-02 (7.1769e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [38][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.9071e-02 (7.1379e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [38][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.0551e-02 (7.1710e-02)	Acc@1  96.88 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [38][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.6554e-02 (7.1937e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [38][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.6952e-02 (7.1973e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [38][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3455e-02 (7.2138e-02)	Acc@1 100.00 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [38][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.1149e-02 (7.1674e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [38][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3267e-02 (7.1535e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.98)
Epoch: [38][220/391]	Time  0.043 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.7537e-02 (7.1332e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [38][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.4551e-02 (7.1107e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [38][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1764e-02 (7.1063e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [38][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2182e-02 (7.1326e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [38][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9780e-02 (7.0836e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [38][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4593e-02 (7.0664e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.98)
Epoch: [38][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4540e-02 (7.0772e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [38][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1767e-02 (7.0474e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [38][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8764e-02 (7.0084e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [38][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8722e-02 (7.0149e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [38][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.9986e-02 (7.0501e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [38][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0433e-02 (7.0504e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [38][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2239e-02 (7.0492e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 ( 99.97)
Epoch: [38][350/391]	Time  0.047 ( 0.047)	Data  0.002 ( 0.003)	Loss 5.3182e-02 (7.0369e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [38][360/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1550e-01 (7.0553e-02)	Acc@1  96.09 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [38][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.9368e-02 (7.0689e-02)	Acc@1  96.88 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [38][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.9268e-02 (7.0752e-02)	Acc@1  97.66 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [38][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2180e-02 (7.0580e-02)	Acc@1  98.75 ( 98.44)	Acc@5 100.00 ( 99.98)
## e[38] optimizer.zero_grad (sum) time: 0.13057565689086914
## e[38]       loss.backward (sum) time: 2.5420634746551514
## e[38]      optimizer.step (sum) time: 1.0039207935333252
## epoch[38] training(only) time: 18.61054229736328
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2977e+00 (1.2977e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.4967e+00 (1.3197e+00)	Acc@1  68.00 ( 70.91)	Acc@5  92.00 ( 91.55)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2707e+00 (1.2440e+00)	Acc@1  72.00 ( 71.71)	Acc@5  94.00 ( 92.29)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7029e+00 (1.2809e+00)	Acc@1  55.00 ( 71.19)	Acc@5  89.00 ( 91.74)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2654e+00 (1.2689e+00)	Acc@1  71.00 ( 71.46)	Acc@5  95.00 ( 92.07)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3563e+00 (1.2823e+00)	Acc@1  68.00 ( 71.10)	Acc@5  89.00 ( 91.84)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 9.8269e-01 (1.2490e+00)	Acc@1  76.00 ( 71.72)	Acc@5  95.00 ( 92.15)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6163e+00 (1.2540e+00)	Acc@1  68.00 ( 71.48)	Acc@5  88.00 ( 92.25)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.2246e+00 (1.2481e+00)	Acc@1  73.00 ( 71.58)	Acc@5  90.00 ( 92.20)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6783e+00 (1.2343e+00)	Acc@1  66.00 ( 71.79)	Acc@5  92.00 ( 92.33)
 * Acc@1 72.020 Acc@5 92.430
### epoch[38] execution time: 21.109246253967285
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.210 ( 0.210)	Data  0.171 ( 0.171)	Loss 4.9083e-02 (4.9083e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.047 ( 0.062)	Data  0.001 ( 0.018)	Loss 3.5314e-02 (4.9167e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.045 ( 0.055)	Data  0.001 ( 0.011)	Loss 4.8784e-02 (5.6915e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.008)	Loss 4.2392e-02 (5.6633e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 40/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.4628e-02 (5.7683e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 50/391]	Time  0.051 ( 0.050)	Data  0.001 ( 0.006)	Loss 8.3621e-02 (5.8661e-02)	Acc@1  97.66 ( 98.84)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 60/391]	Time  0.049 ( 0.050)	Data  0.001 ( 0.005)	Loss 3.3046e-02 (5.7543e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [39][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.6086e-02 (5.8431e-02)	Acc@1 100.00 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [39][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.1539e-02 (5.8632e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [39][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 9.3922e-02 (5.9581e-02)	Acc@1  96.88 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [39][100/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 9.4978e-02 (5.9515e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5477e-02 (5.9119e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [39][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.7533e-02 (5.9234e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [39][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8528e-02 (5.9443e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [39][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.7105e-02 (5.9353e-02)	Acc@1  97.66 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [39][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.4294e-02 (6.0165e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.2761e-02 (6.0350e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [39][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.6578e-02 (6.0878e-02)	Acc@1  96.88 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [39][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.9211e-02 (6.0476e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [39][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.8626e-02 (6.0737e-02)	Acc@1  96.09 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [39][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5117e-02 (6.0284e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [39][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.8591e-02 (6.0307e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [39][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.4928e-02 (6.0371e-02)	Acc@1 100.00 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [39][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.3237e-02 (5.9965e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [39][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.4244e-02 (6.0168e-02)	Acc@1  96.09 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [39][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7594e-02 (6.0461e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [39][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.1032e-02 (6.0454e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [39][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.5650e-02 (6.0412e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [39][280/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.7423e-02 (6.0870e-02)	Acc@1  96.09 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [39][290/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.6441e-02 (6.0833e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [39][300/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.0867e-02 (6.0789e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [39][310/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.0466e-02 (6.1310e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [39][320/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.6256e-02 (6.1323e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [39][330/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.8358e-02 (6.1885e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [39][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8244e-02 (6.1898e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [39][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7031e-02 (6.1941e-02)	Acc@1  97.66 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [39][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5228e-02 (6.1869e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [39][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6174e-02 (6.1804e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 (100.00)
Epoch: [39][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1463e-02 (6.1917e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [39][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0108e-02 (6.2357e-02)	Acc@1  98.75 ( 98.67)	Acc@5 100.00 ( 99.99)
## e[39] optimizer.zero_grad (sum) time: 0.1289989948272705
## e[39]       loss.backward (sum) time: 2.5405101776123047
## e[39]      optimizer.step (sum) time: 1.0039339065551758
## epoch[39] training(only) time: 18.680291414260864
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2068e+00 (1.2068e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.4735e+00 (1.3293e+00)	Acc@1  67.00 ( 70.91)	Acc@5  92.00 ( 92.00)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3746e+00 (1.2644e+00)	Acc@1  71.00 ( 71.67)	Acc@5  92.00 ( 92.14)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7520e+00 (1.2923e+00)	Acc@1  57.00 ( 71.48)	Acc@5  90.00 ( 91.68)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2508e+00 (1.2751e+00)	Acc@1  73.00 ( 71.68)	Acc@5  94.00 ( 92.20)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3582e+00 (1.2884e+00)	Acc@1  70.00 ( 71.35)	Acc@5  89.00 ( 91.90)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 9.8583e-01 (1.2562e+00)	Acc@1  74.00 ( 71.75)	Acc@5  96.00 ( 92.23)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5689e+00 (1.2585e+00)	Acc@1  67.00 ( 71.61)	Acc@5  90.00 ( 92.38)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.2317e+00 (1.2559e+00)	Acc@1  73.00 ( 71.64)	Acc@5  89.00 ( 92.31)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7130e+00 (1.2423e+00)	Acc@1  68.00 ( 71.90)	Acc@5  91.00 ( 92.43)
 * Acc@1 72.190 Acc@5 92.470
### epoch[39] execution time: 21.202253103256226
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.203 ( 0.203)	Data  0.162 ( 0.162)	Loss 6.3043e-02 (6.3043e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.046 ( 0.061)	Data  0.001 ( 0.017)	Loss 7.5207e-02 (6.1105e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.047 ( 0.055)	Data  0.001 ( 0.010)	Loss 5.8915e-02 (6.0886e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [40][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.008)	Loss 5.0446e-02 (6.1491e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [40][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 5.8329e-02 (6.1007e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [40][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 5.2391e-02 (5.7844e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [40][ 60/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 6.7156e-02 (5.8531e-02)	Acc@1  97.66 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [40][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 6.0776e-02 (5.8506e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [40][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.7062e-02 (5.7508e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [40][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.0783e-02 (5.8789e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [40][100/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.1264e-02 (5.8117e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [40][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.6498e-02 (5.8182e-02)	Acc@1  96.09 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [40][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0217e-01 (5.8432e-02)	Acc@1  96.09 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [40][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.1800e-02 (5.7910e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [40][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.9284e-02 (5.8174e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [40][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.3416e-02 (5.8032e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [40][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.2214e-02 (5.8449e-02)	Acc@1  98.44 ( 98.83)	Acc@5  99.22 ( 99.99)
Epoch: [40][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.0658e-02 (5.9193e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [40][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.9869e-02 (5.8637e-02)	Acc@1  96.88 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [40][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.0347e-02 (5.8870e-02)	Acc@1  97.66 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [40][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.8079e-02 (5.8885e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [40][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.6145e-02 (5.8868e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [40][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.4076e-02 (5.8865e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [40][230/391]	Time  0.049 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3643e-02 (5.8565e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [40][240/391]	Time  0.051 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5538e-02 (5.8652e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [40][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5966e-02 (5.9005e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [40][260/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.9336e-02 (5.9139e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [40][270/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.5619e-02 (5.9064e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [40][280/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0256e-02 (5.8724e-02)	Acc@1 100.00 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [40][290/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7272e-02 (5.8592e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [40][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8018e-02 (5.8771e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [40][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6233e-02 (5.8854e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [40][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5624e-02 (5.9063e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [40][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5386e-02 (5.9708e-02)	Acc@1  96.88 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [40][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0723e-02 (6.0047e-02)	Acc@1  97.66 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [40][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1252e-01 (6.0304e-02)	Acc@1  95.31 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [40][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8834e-02 (6.0305e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [40][370/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9597e-02 (6.0386e-02)	Acc@1  99.22 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [40][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4689e-02 (6.0274e-02)	Acc@1  96.09 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [40][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0671e-02 (6.0272e-02)	Acc@1  96.25 ( 98.70)	Acc@5 100.00 ( 99.99)
## e[40] optimizer.zero_grad (sum) time: 0.1319446563720703
## e[40]       loss.backward (sum) time: 2.5493130683898926
## e[40]      optimizer.step (sum) time: 1.0190789699554443
## epoch[40] training(only) time: 18.655516147613525
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.3047e+00 (1.3047e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.024 ( 0.034)	Loss 1.4876e+00 (1.3359e+00)	Acc@1  67.00 ( 70.64)	Acc@5  90.00 ( 91.18)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3364e+00 (1.2699e+00)	Acc@1  72.00 ( 71.76)	Acc@5  93.00 ( 92.05)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6718e+00 (1.2975e+00)	Acc@1  61.00 ( 71.61)	Acc@5  90.00 ( 91.45)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2535e+00 (1.2847e+00)	Acc@1  73.00 ( 71.76)	Acc@5  95.00 ( 91.93)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3381e+00 (1.3006e+00)	Acc@1  71.00 ( 71.39)	Acc@5  90.00 ( 91.67)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0303e+00 (1.2656e+00)	Acc@1  71.00 ( 72.03)	Acc@5  96.00 ( 92.03)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5960e+00 (1.2696e+00)	Acc@1  65.00 ( 71.76)	Acc@5  91.00 ( 92.15)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.2652e+00 (1.2667e+00)	Acc@1  75.00 ( 71.89)	Acc@5  91.00 ( 92.10)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.7333e+00 (1.2533e+00)	Acc@1  65.00 ( 72.07)	Acc@5  90.00 ( 92.26)
 * Acc@1 72.270 Acc@5 92.310
### epoch[40] execution time: 21.157803773880005
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.199 ( 0.199)	Data  0.152 ( 0.152)	Loss 5.1822e-02 (5.1822e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.048 ( 0.060)	Data  0.001 ( 0.016)	Loss 3.2676e-02 (4.9352e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 6.9193e-02 (5.0528e-02)	Acc@1  97.66 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 4.6193e-02 (5.1730e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 6.3194e-02 (5.1807e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 5.2787e-02 (5.2682e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.5431e-02 (5.3909e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.8794e-02 (5.4636e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.6922e-02 (5.5037e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.4957e-02 (5.5003e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.7752e-02 (5.5380e-02)	Acc@1  96.88 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8108e-02 (5.5196e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [41][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.2045e-02 (5.5236e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [41][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5217e-02 (5.6031e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [41][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.7731e-02 (5.7108e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [41][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8086e-02 (5.7499e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [41][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.1855e-02 (5.7543e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.6324e-02 (5.7495e-02)	Acc@1  97.66 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.4719e-02 (5.7146e-02)	Acc@1  97.66 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.9843e-02 (5.7040e-02)	Acc@1  97.66 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.4555e-02 (5.6802e-02)	Acc@1  96.88 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.6532e-02 (5.6650e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [41][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.8986e-02 (5.6773e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [41][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.0712e-02 (5.7028e-02)	Acc@1 100.00 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [41][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7943e-02 (5.7196e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [41][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5986e-02 (5.7445e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [41][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8714e-02 (5.7599e-02)	Acc@1  96.88 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [41][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5245e-02 (5.7610e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [41][280/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6708e-02 (5.7251e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [41][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8652e-02 (5.7012e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [41][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6349e-02 (5.7191e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [41][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2278e-02 (5.7366e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [41][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7785e-02 (5.7392e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [41][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1532e-02 (5.7233e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [41][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6764e-02 (5.7227e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [41][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4591e-02 (5.7172e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [41][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8914e-02 (5.6949e-02)	Acc@1  96.88 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [41][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6120e-02 (5.6980e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [41][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7024e-02 (5.7137e-02)	Acc@1  96.88 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [41][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4390e-02 (5.7160e-02)	Acc@1  96.25 ( 98.77)	Acc@5 100.00 ( 99.99)
## e[41] optimizer.zero_grad (sum) time: 0.12798070907592773
## e[41]       loss.backward (sum) time: 2.5453102588653564
## e[41]      optimizer.step (sum) time: 1.0130887031555176
## epoch[41] training(only) time: 18.614340782165527
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2387e+00 (1.2387e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5974e+00 (1.3795e+00)	Acc@1  67.00 ( 70.64)	Acc@5  91.00 ( 90.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3456e+00 (1.2981e+00)	Acc@1  72.00 ( 71.71)	Acc@5  93.00 ( 91.95)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6989e+00 (1.3263e+00)	Acc@1  59.00 ( 71.58)	Acc@5  90.00 ( 91.52)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2731e+00 (1.3103e+00)	Acc@1  71.00 ( 71.59)	Acc@5  94.00 ( 91.98)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3636e+00 (1.3194e+00)	Acc@1  69.00 ( 71.29)	Acc@5  89.00 ( 91.78)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 9.8786e-01 (1.2837e+00)	Acc@1  73.00 ( 71.77)	Acc@5  94.00 ( 92.08)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5833e+00 (1.2839e+00)	Acc@1  65.00 ( 71.62)	Acc@5  90.00 ( 92.15)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3003e+00 (1.2818e+00)	Acc@1  73.00 ( 71.63)	Acc@5  91.00 ( 92.10)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.7468e+00 (1.2663e+00)	Acc@1  65.00 ( 71.88)	Acc@5  89.00 ( 92.27)
 * Acc@1 72.050 Acc@5 92.360
### epoch[41] execution time: 21.120527744293213
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.202 ( 0.202)	Data  0.160 ( 0.160)	Loss 5.4639e-02 (5.4639e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.045 ( 0.061)	Data  0.001 ( 0.017)	Loss 6.2688e-02 (5.5399e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 3.1772e-02 (5.0408e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.96)
Epoch: [42][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.008)	Loss 5.5665e-02 (5.2003e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.97)
Epoch: [42][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 5.2554e-02 (4.9867e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 5.7956e-02 (5.0940e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 60/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 4.4703e-02 (4.9823e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 70/391]	Time  0.048 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.4453e-02 (5.1524e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.3975e-02 (5.1173e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.0303e-02 (5.0787e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [42][100/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.8709e-02 (5.1338e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [42][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9973e-02 (5.1733e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [42][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.0899e-02 (5.1588e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [42][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.0343e-02 (5.1275e-02)	Acc@1  99.22 ( 98.99)	Acc@5 100.00 ( 99.99)
Epoch: [42][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.5934e-02 (5.2181e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [42][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.3624e-02 (5.1997e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [42][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4160e-02 (5.1802e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.2942e-02 (5.2266e-02)	Acc@1  96.88 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [42][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8820e-02 (5.2255e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [42][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.9686e-02 (5.2537e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [42][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7038e-02 (5.2548e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [42][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.9757e-02 (5.2472e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [42][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.4813e-02 (5.2178e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [42][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.3509e-02 (5.2668e-02)	Acc@1  96.09 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [42][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.1868e-02 (5.2865e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [42][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3298e-02 (5.3350e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1448e-02 (5.3275e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [42][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2241e-02 (5.3160e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [42][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1397e-02 (5.3013e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [42][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4299e-02 (5.3234e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [42][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9281e-02 (5.3071e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [42][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9408e-02 (5.3264e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [42][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9445e-02 (5.3001e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [42][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8626e-02 (5.2964e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [42][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8230e-02 (5.3065e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [42][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3811e-02 (5.3077e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [42][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1084e-02 (5.3075e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [42][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2439e-02 (5.3223e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [42][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7297e-02 (5.3276e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [42][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5178e-02 (5.3582e-02)	Acc@1  98.75 ( 98.86)	Acc@5 100.00 ( 99.99)
## e[42] optimizer.zero_grad (sum) time: 0.12891769409179688
## e[42]       loss.backward (sum) time: 2.533064365386963
## e[42]      optimizer.step (sum) time: 1.0083160400390625
## epoch[42] training(only) time: 18.636371850967407
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.3000e+00 (1.3000e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5014e+00 (1.3640e+00)	Acc@1  64.00 ( 70.45)	Acc@5  91.00 ( 91.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3881e+00 (1.2980e+00)	Acc@1  70.00 ( 71.24)	Acc@5  93.00 ( 91.95)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7034e+00 (1.3229e+00)	Acc@1  61.00 ( 71.23)	Acc@5  90.00 ( 91.45)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3019e+00 (1.3080e+00)	Acc@1  72.00 ( 71.41)	Acc@5  95.00 ( 91.95)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3418e+00 (1.3181e+00)	Acc@1  69.00 ( 71.27)	Acc@5  90.00 ( 91.80)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 9.7852e-01 (1.2835e+00)	Acc@1  72.00 ( 71.67)	Acc@5  95.00 ( 92.07)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6329e+00 (1.2841e+00)	Acc@1  68.00 ( 71.44)	Acc@5  91.00 ( 92.20)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.2867e+00 (1.2809e+00)	Acc@1  74.00 ( 71.44)	Acc@5  89.00 ( 92.11)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.7464e+00 (1.2659e+00)	Acc@1  66.00 ( 71.66)	Acc@5  91.00 ( 92.25)
 * Acc@1 71.920 Acc@5 92.300
### epoch[42] execution time: 21.133984804153442
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.188 ( 0.188)	Data  0.146 ( 0.146)	Loss 3.5881e-02 (3.5881e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.048 ( 0.059)	Data  0.001 ( 0.015)	Loss 4.4030e-02 (4.7432e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 3.9421e-02 (4.4824e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.0991e-02 (4.4942e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.0058e-02 (4.5737e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.6089e-02 (4.6199e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.2850e-02 (4.7196e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4295e-02 (4.5805e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [43][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.5656e-02 (4.6185e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [43][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.6297e-02 (4.6648e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [43][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1786e-02 (4.6805e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [43][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4336e-02 (4.6706e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.7815e-02 (4.7011e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.5912e-02 (4.6931e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [43][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3486e-02 (4.6386e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [43][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6376e-02 (4.6815e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [43][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2615e-02 (4.6663e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [43][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.3855e-02 (4.7095e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [43][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1906e-02 (4.6931e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [43][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.1346e-02 (4.7052e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [43][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.2069e-02 (4.7095e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [43][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.9955e-02 (4.7239e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [43][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.8577e-02 (4.7247e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [43][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.9029e-02 (4.7378e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [43][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.7165e-02 (4.7171e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [43][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5616e-02 (4.7349e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [43][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.1575e-02 (4.7480e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [43][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1417e-02 (4.7759e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [43][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8064e-02 (4.7593e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [43][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8453e-02 (4.7913e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [43][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9061e-02 (4.8158e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [43][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6479e-02 (4.8208e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [43][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6525e-02 (4.8343e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [43][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1519e-02 (4.8315e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][340/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4948e-02 (4.8312e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9721e-02 (4.8353e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [43][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9644e-02 (4.8206e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3796e-02 (4.8270e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1278e-01 (4.8475e-02)	Acc@1  96.09 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [43][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8433e-02 (4.8673e-02)	Acc@1  98.75 ( 99.13)	Acc@5 100.00 (100.00)
## e[43] optimizer.zero_grad (sum) time: 0.12913227081298828
## e[43]       loss.backward (sum) time: 2.5516867637634277
## e[43]      optimizer.step (sum) time: 1.0070366859436035
## epoch[43] training(only) time: 18.606956005096436
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.3011e+00 (1.3011e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.4945e+00 (1.3569e+00)	Acc@1  66.00 ( 71.09)	Acc@5  92.00 ( 91.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.4158e+00 (1.2873e+00)	Acc@1  69.00 ( 71.90)	Acc@5  94.00 ( 92.29)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7108e+00 (1.3170e+00)	Acc@1  61.00 ( 71.65)	Acc@5  90.00 ( 91.71)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3105e+00 (1.3051e+00)	Acc@1  73.00 ( 71.78)	Acc@5  94.00 ( 92.05)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3484e+00 (1.3206e+00)	Acc@1  70.00 ( 71.57)	Acc@5  90.00 ( 91.67)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 9.5398e-01 (1.2857e+00)	Acc@1  73.00 ( 72.07)	Acc@5  96.00 ( 92.00)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5964e+00 (1.2884e+00)	Acc@1  68.00 ( 71.80)	Acc@5  91.00 ( 92.15)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3179e+00 (1.2849e+00)	Acc@1  73.00 ( 71.78)	Acc@5  89.00 ( 92.09)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7854e+00 (1.2690e+00)	Acc@1  68.00 ( 72.10)	Acc@5  90.00 ( 92.27)
 * Acc@1 72.340 Acc@5 92.370
### epoch[43] execution time: 21.101325273513794
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.199 ( 0.199)	Data  0.158 ( 0.158)	Loss 1.7638e-02 (1.7638e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.046 ( 0.061)	Data  0.001 ( 0.016)	Loss 7.1520e-02 (4.3255e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 3.9714e-02 (4.7901e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 2.8192e-02 (4.6427e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 3.9972e-02 (4.6127e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.9475e-02 (4.7743e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 6.2871e-02 (4.8130e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [44][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.8153e-02 (4.7475e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [44][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.6383e-02 (4.7306e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.5976e-02 (4.6777e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [44][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.5960e-02 (4.6170e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [44][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.1854e-02 (4.6025e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [44][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5409e-02 (4.5764e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [44][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9717e-02 (4.5904e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [44][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9664e-02 (4.5890e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [44][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.8492e-02 (4.6274e-02)	Acc@1  97.66 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [44][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1140e-02 (4.6039e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5960e-02 (4.5813e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [44][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.2774e-02 (4.5724e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [44][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.7500e-02 (4.5516e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [44][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4953e-02 (4.5301e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [44][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.4217e-02 (4.5186e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [44][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.3332e-02 (4.5532e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [44][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3740e-02 (4.5563e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [44][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.8667e-02 (4.6048e-02)	Acc@1  96.88 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [44][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3377e-02 (4.6053e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [44][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.1244e-02 (4.6003e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [44][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.0676e-02 (4.5851e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [44][280/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.1300e-02 (4.6286e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [44][290/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.0106e-02 (4.6025e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [44][300/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.3115e-02 (4.6557e-02)	Acc@1  95.31 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [44][310/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2425e-02 (4.6670e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [44][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1767e-02 (4.6730e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1761e-02 (4.6562e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4266e-02 (4.6637e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8142e-02 (4.6649e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [44][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6527e-02 (4.6752e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [44][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3925e-02 (4.6843e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [44][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8904e-02 (4.6829e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [44][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2070e-01 (4.6940e-02)	Acc@1  97.50 ( 99.13)	Acc@5 100.00 (100.00)
## e[44] optimizer.zero_grad (sum) time: 0.13063788414001465
## e[44]       loss.backward (sum) time: 2.546963691711426
## e[44]      optimizer.step (sum) time: 1.016021728515625
## epoch[44] training(only) time: 18.63300395011902
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.3044e+00 (1.3044e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.4862e+00 (1.3721e+00)	Acc@1  69.00 ( 71.64)	Acc@5  92.00 ( 91.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.4102e+00 (1.3068e+00)	Acc@1  70.00 ( 72.33)	Acc@5  94.00 ( 92.00)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6644e+00 (1.3309e+00)	Acc@1  62.00 ( 71.87)	Acc@5  91.00 ( 91.68)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2778e+00 (1.3147e+00)	Acc@1  72.00 ( 71.68)	Acc@5  94.00 ( 92.05)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3357e+00 (1.3274e+00)	Acc@1  71.00 ( 71.43)	Acc@5  88.00 ( 91.75)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0357e+00 (1.2922e+00)	Acc@1  72.00 ( 71.82)	Acc@5  95.00 ( 92.07)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6602e+00 (1.2942e+00)	Acc@1  67.00 ( 71.68)	Acc@5  89.00 ( 92.17)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3234e+00 (1.2899e+00)	Acc@1  74.00 ( 71.64)	Acc@5  90.00 ( 92.16)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.7726e+00 (1.2739e+00)	Acc@1  66.00 ( 71.92)	Acc@5  90.00 ( 92.31)
 * Acc@1 72.150 Acc@5 92.380
### epoch[44] execution time: 21.12686824798584
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.186 ( 0.186)	Data  0.147 ( 0.147)	Loss 3.9034e-02 (3.9034e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.016)	Loss 3.0836e-02 (4.2860e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.048 ( 0.054)	Data  0.001 ( 0.009)	Loss 5.5517e-02 (4.4167e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 3.5674e-02 (4.3340e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.2698e-02 (4.1810e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 6.2578e-02 (4.3626e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.5181e-02 (4.3887e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [45][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.3378e-02 (4.3834e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [45][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 6.9674e-02 (4.3862e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [45][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.4571e-02 (4.3436e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [45][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0148e-02 (4.4097e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [45][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5573e-02 (4.4393e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [45][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0140e-02 (4.3889e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [45][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.0169e-02 (4.3726e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [45][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0035e-02 (4.3237e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [45][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8142e-02 (4.3219e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [45][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.5176e-02 (4.3210e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3686e-02 (4.3464e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.1455e-02 (4.3494e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [45][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.4990e-02 (4.3152e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [45][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.6507e-02 (4.3353e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [45][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.3905e-02 (4.3244e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [45][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.5045e-02 (4.3218e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [45][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8339e-02 (4.3207e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [45][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.9008e-02 (4.3142e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [45][250/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.5755e-02 (4.3176e-02)	Acc@1  96.88 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [45][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.0926e-02 (4.3421e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [45][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5883e-02 (4.3600e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [45][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0407e-02 (4.3588e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [45][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6320e-02 (4.3501e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [45][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6838e-02 (4.3541e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [45][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8973e-02 (4.3494e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [45][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0807e-02 (4.3571e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2276e-02 (4.3279e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1043e-02 (4.3336e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7328e-02 (4.3304e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5656e-02 (4.3673e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0691e-02 (4.3773e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.051 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3563e-02 (4.3813e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6211e-02 (4.4024e-02)	Acc@1  98.75 ( 99.21)	Acc@5 100.00 (100.00)
## e[45] optimizer.zero_grad (sum) time: 0.1298820972442627
## e[45]       loss.backward (sum) time: 2.5597620010375977
## e[45]      optimizer.step (sum) time: 1.0029079914093018
## epoch[45] training(only) time: 18.64728856086731
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.2578e+00 (1.2578e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.5814e+00 (1.3723e+00)	Acc@1  67.00 ( 71.36)	Acc@5  90.00 ( 91.27)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3998e+00 (1.3029e+00)	Acc@1  70.00 ( 71.86)	Acc@5  94.00 ( 92.14)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7211e+00 (1.3283e+00)	Acc@1  56.00 ( 71.29)	Acc@5  90.00 ( 91.71)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2704e+00 (1.3181e+00)	Acc@1  71.00 ( 71.32)	Acc@5  94.00 ( 92.15)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3794e+00 (1.3303e+00)	Acc@1  72.00 ( 71.20)	Acc@5  89.00 ( 91.84)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 9.9122e-01 (1.2957e+00)	Acc@1  72.00 ( 71.69)	Acc@5  95.00 ( 92.16)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6774e+00 (1.2977e+00)	Acc@1  67.00 ( 71.49)	Acc@5  88.00 ( 92.21)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3534e+00 (1.2965e+00)	Acc@1  73.00 ( 71.49)	Acc@5  89.00 ( 92.10)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8348e+00 (1.2827e+00)	Acc@1  67.00 ( 71.84)	Acc@5  88.00 ( 92.24)
 * Acc@1 72.000 Acc@5 92.310
### epoch[45] execution time: 21.126640558242798
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.197 ( 0.197)	Data  0.154 ( 0.154)	Loss 2.9019e-02 (2.9019e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 3.3042e-02 (3.6549e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 2.9594e-02 (4.0233e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 5.7425e-02 (4.2342e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.006)	Loss 7.3866e-02 (4.3106e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.9137e-02 (4.2698e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.9998e-02 (4.1603e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.8645e-02 (4.1789e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.5533e-02 (4.2106e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.7088e-02 (4.2169e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2860e-02 (4.1674e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4257e-02 (4.1256e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.9762e-02 (4.1512e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.0095e-02 (4.1302e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.9882e-02 (4.1493e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3498e-02 (4.1653e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.5557e-02 (4.1915e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.3818e-02 (4.2301e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5127e-02 (4.2786e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.3912e-02 (4.2733e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5589e-02 (4.2604e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9822e-02 (4.2388e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.3976e-02 (4.2596e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2262e-02 (4.2393e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5536e-02 (4.2743e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7079e-02 (4.2944e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0281e-02 (4.2995e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9320e-02 (4.3183e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5078e-02 (4.3485e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2478e-02 (4.3660e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6469e-02 (4.3758e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2420e-02 (4.3891e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9889e-02 (4.3802e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2479e-02 (4.3822e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4081e-02 (4.3637e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0235e-02 (4.3673e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8786e-02 (4.3583e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6578e-02 (4.3544e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2768e-02 (4.3398e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0469e-02 (4.3406e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
## e[46] optimizer.zero_grad (sum) time: 0.1306295394897461
## e[46]       loss.backward (sum) time: 2.535688877105713
## e[46]      optimizer.step (sum) time: 1.000004768371582
## epoch[46] training(only) time: 18.579648733139038
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.3266e+00 (1.3266e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5333e+00 (1.3863e+00)	Acc@1  66.00 ( 70.64)	Acc@5  90.00 ( 91.45)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3364e+00 (1.2965e+00)	Acc@1  73.00 ( 71.95)	Acc@5  94.00 ( 92.29)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7230e+00 (1.3270e+00)	Acc@1  60.00 ( 71.39)	Acc@5  89.00 ( 91.84)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2408e+00 (1.3143e+00)	Acc@1  73.00 ( 71.54)	Acc@5  96.00 ( 92.32)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3317e+00 (1.3255e+00)	Acc@1  70.00 ( 71.31)	Acc@5  89.00 ( 92.16)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0125e+00 (1.2929e+00)	Acc@1  72.00 ( 71.67)	Acc@5  95.00 ( 92.38)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6300e+00 (1.2957e+00)	Acc@1  66.00 ( 71.48)	Acc@5  89.00 ( 92.46)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3560e+00 (1.2930e+00)	Acc@1  73.00 ( 71.52)	Acc@5  89.00 ( 92.36)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7892e+00 (1.2764e+00)	Acc@1  65.00 ( 71.85)	Acc@5  92.00 ( 92.52)
 * Acc@1 72.110 Acc@5 92.570
### epoch[46] execution time: 21.088409423828125
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.190 ( 0.190)	Data  0.136 ( 0.136)	Loss 4.5471e-02 (4.5471e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.014)	Loss 2.2022e-02 (3.3762e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 3.6195e-02 (3.8433e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.7080e-02 (3.8110e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.4392e-02 (3.8587e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 3.6780e-02 (3.8305e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.3329e-02 (3.8214e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.3289e-02 (3.8036e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.3896e-02 (3.8262e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4671e-02 (3.8007e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4952e-02 (3.8799e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4801e-02 (3.8794e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.2397e-02 (3.9147e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7873e-02 (3.9518e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8290e-02 (3.9857e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.6560e-02 (4.0036e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [47][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.0929e-02 (4.0448e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.2848e-02 (4.0452e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3930e-02 (4.0366e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0879e-02 (4.0357e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7426e-02 (4.0548e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3062e-02 (4.0786e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.1872e-02 (4.0651e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.050 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7213e-02 (4.0526e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.8147e-02 (4.0691e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8915e-02 (4.0627e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3868e-02 (4.0638e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5790e-02 (4.0650e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7549e-02 (4.0562e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7097e-02 (4.0767e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8555e-02 (4.0933e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7041e-02 (4.1244e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0441e-02 (4.1272e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8137e-02 (4.1169e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8694e-02 (4.1337e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4262e-02 (4.1540e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4183e-02 (4.1541e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9364e-02 (4.1589e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6232e-02 (4.1630e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2177e-02 (4.1570e-02)	Acc@1  98.75 ( 99.24)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.12792015075683594
## e[47]       loss.backward (sum) time: 2.5418758392333984
## e[47]      optimizer.step (sum) time: 1.020202398300171
## epoch[47] training(only) time: 18.610043048858643
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2754e+00 (1.2754e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.4970e+00 (1.3632e+00)	Acc@1  67.00 ( 71.00)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3442e+00 (1.2871e+00)	Acc@1  71.00 ( 72.00)	Acc@5  92.00 ( 92.10)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7670e+00 (1.3169e+00)	Acc@1  60.00 ( 71.26)	Acc@5  91.00 ( 91.84)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2900e+00 (1.3104e+00)	Acc@1  70.00 ( 71.29)	Acc@5  96.00 ( 92.32)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3884e+00 (1.3272e+00)	Acc@1  70.00 ( 70.98)	Acc@5  89.00 ( 92.12)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0049e+00 (1.2941e+00)	Acc@1  75.00 ( 71.44)	Acc@5  94.00 ( 92.28)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6599e+00 (1.2965e+00)	Acc@1  63.00 ( 71.30)	Acc@5  89.00 ( 92.34)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3224e+00 (1.2961e+00)	Acc@1  73.00 ( 71.31)	Acc@5  90.00 ( 92.25)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8280e+00 (1.2840e+00)	Acc@1  66.00 ( 71.62)	Acc@5  91.00 ( 92.37)
 * Acc@1 71.750 Acc@5 92.440
### epoch[47] execution time: 21.105724096298218
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.198 ( 0.198)	Data  0.158 ( 0.158)	Loss 3.2673e-02 (3.2673e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.047 ( 0.061)	Data  0.001 ( 0.016)	Loss 1.7948e-02 (4.0224e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 4.4044e-02 (3.9801e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 3.7677e-02 (3.9973e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 2.9765e-02 (4.1570e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.5630e-02 (4.0817e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.7784e-02 (3.9473e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.1524e-02 (4.0494e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 7.5246e-02 (4.0858e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.0397e-02 (4.0736e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5274e-02 (4.0745e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9391e-02 (4.0349e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.050 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.2049e-02 (4.0245e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1466e-02 (3.9671e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6629e-02 (3.9533e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1576e-02 (3.9495e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.9240e-02 (4.0024e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.3637e-02 (3.9800e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.4694e-02 (3.9873e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.4093e-02 (3.9791e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.0861e-02 (3.9993e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2971e-02 (3.9911e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6248e-02 (3.9892e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0090e-02 (3.9766e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.2359e-02 (3.9530e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.050 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.4565e-02 (3.9583e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3617e-02 (3.9317e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.5077e-02 (3.9173e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1214e-02 (3.9252e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9366e-02 (3.9242e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4811e-02 (3.9286e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8669e-02 (3.9424e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1075e-02 (3.9189e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6878e-02 (3.9261e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1702e-02 (3.9394e-02)	Acc@1  96.88 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2519e-02 (3.9422e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7095e-02 (3.9419e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9511e-02 (3.9265e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7995e-02 (3.9301e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0126e-02 (3.9166e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.1291370391845703
## e[48]       loss.backward (sum) time: 2.5266473293304443
## e[48]      optimizer.step (sum) time: 1.021702527999878
## epoch[48] training(only) time: 18.632513523101807
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2680e+00 (1.2680e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5189e+00 (1.3661e+00)	Acc@1  68.00 ( 70.64)	Acc@5  92.00 ( 91.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3348e+00 (1.2966e+00)	Acc@1  70.00 ( 71.86)	Acc@5  93.00 ( 92.29)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7614e+00 (1.3294e+00)	Acc@1  60.00 ( 71.52)	Acc@5  91.00 ( 91.84)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2685e+00 (1.3190e+00)	Acc@1  72.00 ( 71.63)	Acc@5  95.00 ( 92.24)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3640e+00 (1.3351e+00)	Acc@1  73.00 ( 71.35)	Acc@5  89.00 ( 92.08)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0050e+00 (1.3030e+00)	Acc@1  74.00 ( 71.72)	Acc@5  95.00 ( 92.30)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6633e+00 (1.3052e+00)	Acc@1  65.00 ( 71.51)	Acc@5  89.00 ( 92.37)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3616e+00 (1.3023e+00)	Acc@1  73.00 ( 71.57)	Acc@5  89.00 ( 92.27)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8245e+00 (1.2883e+00)	Acc@1  68.00 ( 71.87)	Acc@5  90.00 ( 92.45)
 * Acc@1 72.180 Acc@5 92.550
### epoch[48] execution time: 21.125165939331055
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.181 ( 0.181)	Data  0.141 ( 0.141)	Loss 2.5911e-02 (2.5911e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 3.2585e-02 (3.7824e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 5.5694e-02 (3.6455e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.7682e-02 (3.6681e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 6.4227e-02 (3.6853e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 3.2407e-02 (3.6195e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.5420e-02 (3.6078e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.6157e-02 (3.5844e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 5.1203e-02 (3.6290e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9968e-02 (3.6524e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2215e-02 (3.7407e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.8634e-02 (3.8120e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [49][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.7832e-02 (3.7925e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [49][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4446e-02 (3.7676e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [49][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.0632e-02 (3.7591e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [49][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.8739e-02 (3.7637e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [49][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1859e-02 (3.7442e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9372e-02 (3.6849e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9648e-02 (3.6684e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.3946e-02 (3.6801e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5603e-02 (3.7087e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.4393e-02 (3.6784e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7119e-02 (3.6719e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5900e-02 (3.6762e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2263e-02 (3.6572e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3520e-02 (3.6664e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7423e-02 (3.6844e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6093e-02 (3.6814e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5781e-02 (3.6987e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2124e-02 (3.6983e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0616e-02 (3.6798e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5827e-02 (3.7150e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4604e-02 (3.7277e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2906e-02 (3.7231e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8667e-02 (3.7066e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3122e-02 (3.7129e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3217e-02 (3.7043e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8873e-02 (3.6969e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5864e-02 (3.7068e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5394e-02 (3.6989e-02)	Acc@1  98.75 ( 99.37)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.13096237182617188
## e[49]       loss.backward (sum) time: 2.5609452724456787
## e[49]      optimizer.step (sum) time: 1.015787124633789
## epoch[49] training(only) time: 18.61787486076355
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.2521e+00 (1.2521e+00)	Acc@1  74.00 ( 74.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5092e+00 (1.3741e+00)	Acc@1  69.00 ( 71.36)	Acc@5  92.00 ( 91.27)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3995e+00 (1.3084e+00)	Acc@1  70.00 ( 72.10)	Acc@5  92.00 ( 92.05)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7729e+00 (1.3346e+00)	Acc@1  59.00 ( 71.65)	Acc@5  91.00 ( 91.87)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2458e+00 (1.3240e+00)	Acc@1  74.00 ( 71.85)	Acc@5  93.00 ( 92.12)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3335e+00 (1.3358e+00)	Acc@1  70.00 ( 71.61)	Acc@5  90.00 ( 91.98)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 9.7112e-01 (1.3021e+00)	Acc@1  77.00 ( 72.07)	Acc@5  94.00 ( 92.23)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6782e+00 (1.3048e+00)	Acc@1  65.00 ( 71.87)	Acc@5  89.00 ( 92.25)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3597e+00 (1.3039e+00)	Acc@1  74.00 ( 71.86)	Acc@5  89.00 ( 92.22)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8109e+00 (1.2886e+00)	Acc@1  67.00 ( 72.14)	Acc@5  91.00 ( 92.38)
 * Acc@1 72.280 Acc@5 92.490
### epoch[49] execution time: 21.1158664226532
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.198 ( 0.198)	Data  0.158 ( 0.158)	Loss 5.5457e-02 (5.5457e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.2838e-02 (3.8582e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 3.2702e-02 (3.7408e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.008)	Loss 3.7351e-02 (3.8091e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.006)	Loss 9.0664e-02 (3.7430e-02)	Acc@1  96.09 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.049 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.0173e-02 (3.6749e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.9144e-02 (3.5547e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.1227e-02 (3.6390e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.9764e-02 (3.6269e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.3451e-02 (3.6944e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.5141e-02 (3.6541e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6381e-02 (3.6353e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0452e-02 (3.6031e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.3011e-02 (3.6043e-02)	Acc@1  96.88 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4165e-02 (3.6060e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9268e-02 (3.5967e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9930e-02 (3.5656e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5832e-02 (3.5132e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.5254e-02 (3.5149e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9618e-02 (3.5002e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9555e-02 (3.5054e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8769e-02 (3.5052e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2544e-02 (3.5414e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2804e-02 (3.5307e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5369e-02 (3.5312e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8175e-02 (3.5396e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.3608e-02 (3.5455e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.8545e-02 (3.5468e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6129e-02 (3.5478e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.9139e-02 (3.5620e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3496e-02 (3.5675e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1841e-02 (3.5582e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7511e-02 (3.5536e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7121e-02 (3.5646e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9735e-02 (3.5720e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6952e-02 (3.5795e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2323e-02 (3.5931e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.050 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2454e-02 (3.5834e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3426e-02 (3.6011e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5713e-02 (3.6022e-02)	Acc@1  98.75 ( 99.45)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.1295628547668457
## e[50]       loss.backward (sum) time: 2.544628858566284
## e[50]      optimizer.step (sum) time: 1.0161778926849365
## epoch[50] training(only) time: 18.632715225219727
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2596e+00 (1.2596e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5874e+00 (1.3923e+00)	Acc@1  68.00 ( 70.73)	Acc@5  93.00 ( 91.73)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3985e+00 (1.3240e+00)	Acc@1  72.00 ( 71.90)	Acc@5  94.00 ( 92.33)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8036e+00 (1.3500e+00)	Acc@1  59.00 ( 71.32)	Acc@5  91.00 ( 91.94)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2830e+00 (1.3372e+00)	Acc@1  70.00 ( 71.56)	Acc@5  96.00 ( 92.39)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3740e+00 (1.3489e+00)	Acc@1  70.00 ( 71.45)	Acc@5  89.00 ( 92.08)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0333e+00 (1.3152e+00)	Acc@1  73.00 ( 71.77)	Acc@5  94.00 ( 92.25)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6856e+00 (1.3174e+00)	Acc@1  64.00 ( 71.55)	Acc@5  89.00 ( 92.35)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3620e+00 (1.3150e+00)	Acc@1  74.00 ( 71.65)	Acc@5  88.00 ( 92.20)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8490e+00 (1.2998e+00)	Acc@1  68.00 ( 72.01)	Acc@5  90.00 ( 92.38)
 * Acc@1 72.260 Acc@5 92.460
### epoch[50] execution time: 21.121939182281494
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.193 ( 0.193)	Data  0.152 ( 0.152)	Loss 2.6501e-02 (2.6501e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 4.4187e-02 (3.2622e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 2.7719e-02 (2.9692e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 3.9481e-02 (3.2749e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.8542e-02 (3.3554e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 3.0070e-02 (3.3004e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.8332e-02 (3.2740e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.7673e-02 (3.2436e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.1854e-02 (3.2220e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.1832e-02 (3.2547e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9787e-02 (3.2299e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0414e-02 (3.2018e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.9895e-02 (3.2400e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8066e-02 (3.2230e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5076e-02 (3.2154e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5439e-02 (3.2231e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.7988e-02 (3.2383e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7362e-02 (3.2401e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9076e-02 (3.2339e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2826e-02 (3.2304e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2027e-02 (3.2095e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.4478e-02 (3.2605e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1038e-02 (3.2519e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9522e-02 (3.2882e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.8805e-02 (3.3109e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9069e-02 (3.3426e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6069e-02 (3.3580e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2535e-02 (3.3544e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9494e-02 (3.3609e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4943e-02 (3.3697e-02)	Acc@1  97.66 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.046 ( 0.047)	Data  0.003 ( 0.003)	Loss 4.3135e-02 (3.3840e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4300e-02 (3.3994e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9438e-02 (3.4027e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5801e-02 (3.4259e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6408e-02 (3.4411e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8128e-02 (3.4521e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3681e-02 (3.4472e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7268e-02 (3.4535e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2938e-02 (3.4547e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1484e-02 (3.4534e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.1288154125213623
## e[51]       loss.backward (sum) time: 2.541163444519043
## e[51]      optimizer.step (sum) time: 1.0030450820922852
## epoch[51] training(only) time: 18.59088373184204
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2365e+00 (1.2365e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5053e+00 (1.3596e+00)	Acc@1  69.00 ( 71.36)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3645e+00 (1.2986e+00)	Acc@1  71.00 ( 72.14)	Acc@5  92.00 ( 92.00)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7580e+00 (1.3247e+00)	Acc@1  61.00 ( 71.61)	Acc@5  91.00 ( 91.71)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2858e+00 (1.3187e+00)	Acc@1  72.00 ( 71.71)	Acc@5  96.00 ( 92.20)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3854e+00 (1.3325e+00)	Acc@1  72.00 ( 71.63)	Acc@5  88.00 ( 91.94)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0801e+00 (1.2990e+00)	Acc@1  73.00 ( 72.08)	Acc@5  93.00 ( 92.21)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6614e+00 (1.3018e+00)	Acc@1  66.00 ( 71.85)	Acc@5  91.00 ( 92.31)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3417e+00 (1.3000e+00)	Acc@1  73.00 ( 71.90)	Acc@5  89.00 ( 92.20)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.7867e+00 (1.2869e+00)	Acc@1  67.00 ( 72.14)	Acc@5  91.00 ( 92.33)
 * Acc@1 72.320 Acc@5 92.420
### epoch[51] execution time: 21.082301378250122
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.190 ( 0.190)	Data  0.148 ( 0.148)	Loss 2.0780e-02 (2.0780e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.8943e-02 (2.9494e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 2.0760e-02 (2.8157e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.7159e-02 (2.9349e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.048 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.2598e-02 (2.9087e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 3.3941e-02 (2.9966e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.2913e-02 (2.9226e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.7320e-02 (2.9790e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.4393e-02 (2.9873e-02)	Acc@1  97.66 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0490e-02 (3.0355e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1133e-02 (3.0887e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9074e-02 (3.0646e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7716e-02 (3.0768e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8269e-02 (3.1225e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6420e-02 (3.1415e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.3366e-02 (3.1507e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3458e-02 (3.1591e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4218e-02 (3.1796e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8402e-02 (3.1717e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1491e-02 (3.1912e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0121e-02 (3.1695e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.9515e-02 (3.1625e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.7988e-02 (3.2121e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8237e-02 (3.2048e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8895e-02 (3.2002e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7340e-02 (3.2202e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1106e-02 (3.2216e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4342e-02 (3.2342e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4368e-02 (3.2245e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7060e-02 (3.2430e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9616e-02 (3.2456e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5190e-02 (3.2397e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1930e-02 (3.2282e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9704e-02 (3.2353e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5894e-02 (3.2316e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9344e-02 (3.2290e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3320e-02 (3.2432e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1808e-02 (3.2521e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9970e-02 (3.2647e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2709e-02 (3.2680e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.13020539283752441
## e[52]       loss.backward (sum) time: 2.5568737983703613
## e[52]      optimizer.step (sum) time: 0.9980545043945312
## epoch[52] training(only) time: 18.596686601638794
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2276e+00 (1.2276e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5854e+00 (1.3813e+00)	Acc@1  65.00 ( 70.55)	Acc@5  92.00 ( 91.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4240e+00 (1.3182e+00)	Acc@1  70.00 ( 71.86)	Acc@5  92.00 ( 92.10)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7172e+00 (1.3394e+00)	Acc@1  60.00 ( 71.61)	Acc@5  92.00 ( 91.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2584e+00 (1.3326e+00)	Acc@1  72.00 ( 71.63)	Acc@5  96.00 ( 92.32)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3622e+00 (1.3506e+00)	Acc@1  72.00 ( 71.31)	Acc@5  89.00 ( 92.02)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0638e+00 (1.3158e+00)	Acc@1  73.00 ( 71.82)	Acc@5  95.00 ( 92.26)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6045e+00 (1.3157e+00)	Acc@1  69.00 ( 71.76)	Acc@5  92.00 ( 92.39)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3159e+00 (1.3135e+00)	Acc@1  73.00 ( 71.85)	Acc@5  89.00 ( 92.28)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7505e+00 (1.2995e+00)	Acc@1  64.00 ( 72.07)	Acc@5  90.00 ( 92.41)
 * Acc@1 72.280 Acc@5 92.530
### epoch[52] execution time: 21.10556435585022
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.183 ( 0.183)	Data  0.143 ( 0.143)	Loss 4.2202e-02 (4.2202e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.5564e-02 (2.6070e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 2.8443e-02 (2.9528e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.1627e-02 (2.8383e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 5.0763e-02 (2.9735e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 3.9366e-02 (3.0433e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.2417e-02 (3.0201e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.8041e-02 (3.0200e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.049 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.3485e-02 (3.0801e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8165e-02 (3.1171e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.1395e-02 (3.1673e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.2337e-02 (3.1272e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4578e-02 (3.0860e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7100e-02 (3.0706e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3628e-02 (3.0515e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8430e-02 (3.0800e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4850e-02 (3.1011e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9679e-02 (3.0910e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.7076e-02 (3.1089e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6929e-02 (3.1497e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5407e-02 (3.1914e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2163e-02 (3.1775e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9145e-02 (3.1850e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.1565e-02 (3.1895e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4849e-02 (3.1877e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1628e-02 (3.1748e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7874e-02 (3.1849e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6759e-02 (3.1941e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5021e-02 (3.1627e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4142e-02 (3.1543e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6266e-02 (3.1567e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1154e-02 (3.1616e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8147e-02 (3.1568e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9147e-02 (3.1644e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6349e-02 (3.1846e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5080e-02 (3.1919e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6086e-02 (3.1897e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6758e-02 (3.1870e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.051 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6302e-02 (3.1975e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9228e-02 (3.2096e-02)	Acc@1  97.50 ( 99.48)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.13068628311157227
## e[53]       loss.backward (sum) time: 2.5523605346679688
## e[53]      optimizer.step (sum) time: 1.0231049060821533
## epoch[53] training(only) time: 18.646296977996826
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.3037e+00 (1.3037e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.5377e+00 (1.3835e+00)	Acc@1  66.00 ( 71.36)	Acc@5  91.00 ( 91.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3493e+00 (1.3189e+00)	Acc@1  72.00 ( 72.24)	Acc@5  92.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7154e+00 (1.3480e+00)	Acc@1  60.00 ( 71.81)	Acc@5  91.00 ( 91.55)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2853e+00 (1.3379e+00)	Acc@1  73.00 ( 71.88)	Acc@5  96.00 ( 92.12)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3785e+00 (1.3558e+00)	Acc@1  71.00 ( 71.63)	Acc@5  89.00 ( 91.84)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0590e+00 (1.3222e+00)	Acc@1  72.00 ( 71.98)	Acc@5  94.00 ( 92.08)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6211e+00 (1.3247e+00)	Acc@1  67.00 ( 71.83)	Acc@5  90.00 ( 92.20)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3958e+00 (1.3203e+00)	Acc@1  73.00 ( 71.89)	Acc@5  89.00 ( 92.17)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7649e+00 (1.3052e+00)	Acc@1  66.00 ( 72.13)	Acc@5  92.00 ( 92.35)
 * Acc@1 72.340 Acc@5 92.470
### epoch[53] execution time: 21.12883996963501
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.196 ( 0.196)	Data  0.155 ( 0.155)	Loss 3.8188e-02 (3.8188e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.2013e-02 (3.3066e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.7533e-02 (3.0036e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.048 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.5913e-02 (2.9954e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.0235e-02 (2.9960e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.0590e-02 (3.0747e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4900e-02 (2.9928e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.9667e-02 (2.9800e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.7709e-02 (2.9609e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.7228e-02 (2.9537e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.049 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4246e-02 (3.0319e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.5542e-02 (3.0673e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6569e-02 (3.0824e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5041e-02 (3.0879e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6955e-02 (3.0593e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.7937e-02 (3.0465e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3908e-02 (3.0560e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7311e-02 (3.0808e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5439e-02 (3.1113e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5929e-02 (3.0934e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8533e-02 (3.0778e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9873e-02 (3.0940e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5149e-02 (3.0846e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7088e-02 (3.0769e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.3086e-02 (3.0802e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7368e-02 (3.0688e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1698e-02 (3.0918e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3650e-02 (3.0781e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6710e-02 (3.0922e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0197e-02 (3.0897e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5167e-02 (3.1029e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1554e-02 (3.0870e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0317e-02 (3.0948e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2687e-02 (3.0804e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3789e-02 (3.0796e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2629e-02 (3.0868e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5273e-02 (3.0772e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7658e-02 (3.0721e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2820e-02 (3.0579e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4479e-02 (3.0686e-02)	Acc@1  98.75 ( 99.51)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.1292133331298828
## e[54]       loss.backward (sum) time: 2.5584511756896973
## e[54]      optimizer.step (sum) time: 1.0020570755004883
## epoch[54] training(only) time: 18.631611347198486
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.2866e+00 (1.2866e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.6120e+00 (1.4106e+00)	Acc@1  64.00 ( 70.73)	Acc@5  90.00 ( 91.27)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3657e+00 (1.3235e+00)	Acc@1  70.00 ( 72.10)	Acc@5  94.00 ( 92.14)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7397e+00 (1.3490e+00)	Acc@1  59.00 ( 71.84)	Acc@5  91.00 ( 91.90)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3213e+00 (1.3391e+00)	Acc@1  71.00 ( 71.85)	Acc@5  96.00 ( 92.34)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3555e+00 (1.3568e+00)	Acc@1  71.00 ( 71.59)	Acc@5  89.00 ( 91.98)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0109e+00 (1.3231e+00)	Acc@1  73.00 ( 72.07)	Acc@5  93.00 ( 92.23)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6339e+00 (1.3285e+00)	Acc@1  64.00 ( 71.72)	Acc@5  88.00 ( 92.30)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3638e+00 (1.3265e+00)	Acc@1  74.00 ( 71.70)	Acc@5  89.00 ( 92.25)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.7711e+00 (1.3105e+00)	Acc@1  64.00 ( 71.97)	Acc@5  90.00 ( 92.42)
 * Acc@1 72.140 Acc@5 92.510
### epoch[54] execution time: 21.124626874923706
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.193 ( 0.193)	Data  0.152 ( 0.152)	Loss 3.6553e-02 (3.6553e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.5836e-02 (2.8739e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 2.8442e-02 (2.7825e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 2.3800e-02 (2.7897e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.006)	Loss 1.6825e-02 (2.8030e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.7826e-02 (2.7857e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.5870e-02 (2.8090e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.2101e-02 (2.7693e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.6484e-02 (2.7422e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.2939e-02 (2.7988e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9381e-02 (2.8122e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9410e-02 (2.8090e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.4724e-02 (2.8387e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4856e-02 (2.8188e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0371e-02 (2.8932e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.5131e-03 (2.8950e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0761e-02 (2.8452e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7545e-02 (2.8312e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4677e-02 (2.8125e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.5931e-02 (2.8465e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6755e-02 (2.8643e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.2675e-02 (2.8516e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6679e-02 (2.8282e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.7670e-02 (2.8696e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2337e-02 (2.8747e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.0026e-02 (2.8804e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.7035e-02 (2.8790e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2203e-02 (2.8675e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3896e-02 (2.8661e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7155e-02 (2.8878e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5102e-02 (2.8888e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6437e-02 (2.9152e-02)	Acc@1  98.44 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4416e-02 (2.9161e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3575e-02 (2.9205e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3274e-02 (2.9397e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6822e-02 (2.9509e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3737e-02 (2.9609e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3060e-02 (2.9613e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6294e-02 (2.9768e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8817e-02 (2.9663e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.128676176071167
## e[55]       loss.backward (sum) time: 2.538073778152466
## e[55]      optimizer.step (sum) time: 1.0114309787750244
## epoch[55] training(only) time: 18.636256456375122
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3032e+00 (1.3032e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5416e+00 (1.3978e+00)	Acc@1  68.00 ( 70.73)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3752e+00 (1.3134e+00)	Acc@1  70.00 ( 72.00)	Acc@5  94.00 ( 92.14)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7670e+00 (1.3447e+00)	Acc@1  62.00 ( 71.74)	Acc@5  91.00 ( 91.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2897e+00 (1.3339e+00)	Acc@1  74.00 ( 71.78)	Acc@5  95.00 ( 92.17)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3645e+00 (1.3474e+00)	Acc@1  72.00 ( 71.59)	Acc@5  89.00 ( 91.88)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0712e+00 (1.3160e+00)	Acc@1  73.00 ( 72.00)	Acc@5  93.00 ( 92.10)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6548e+00 (1.3176e+00)	Acc@1  64.00 ( 71.87)	Acc@5  88.00 ( 92.27)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3419e+00 (1.3142e+00)	Acc@1  75.00 ( 71.91)	Acc@5  89.00 ( 92.27)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7834e+00 (1.2994e+00)	Acc@1  64.00 ( 72.05)	Acc@5  92.00 ( 92.38)
 * Acc@1 72.220 Acc@5 92.470
### epoch[55] execution time: 21.128925561904907
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.186 ( 0.186)	Data  0.139 ( 0.139)	Loss 3.3523e-02 (3.3523e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.048 ( 0.059)	Data  0.001 ( 0.014)	Loss 2.9211e-02 (2.7035e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.8829e-02 (2.6405e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.2398e-02 (2.6560e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.6505e-02 (2.6938e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.5173e-02 (2.7405e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.5405e-02 (2.6316e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.7286e-02 (2.5879e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.6265e-02 (2.6578e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.0338e-02 (2.6630e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1575e-02 (2.6385e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.050 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.8492e-02 (2.6520e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.7286e-02 (2.6970e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.3793e-02 (2.7353e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9002e-02 (2.7719e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9329e-02 (2.7666e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2111e-02 (2.7477e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.1264e-02 (2.7547e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1089e-02 (2.7361e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1195e-02 (2.7697e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.6235e-03 (2.7426e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0924e-02 (2.7334e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.3851e-02 (2.7494e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8254e-02 (2.7741e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.6896e-02 (2.7637e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0995e-02 (2.7889e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8269e-02 (2.7766e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3547e-02 (2.7787e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0941e-02 (2.7835e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6095e-02 (2.7869e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2609e-02 (2.7827e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5617e-02 (2.7879e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8054e-02 (2.7888e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3769e-02 (2.7788e-02)	Acc@1  97.66 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0047e-02 (2.7874e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9880e-02 (2.7813e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7333e-02 (2.7799e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9038e-02 (2.7855e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7050e-02 (2.7873e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1900e-02 (2.8030e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.1293950080871582
## e[56]       loss.backward (sum) time: 2.563514471054077
## e[56]      optimizer.step (sum) time: 1.0193462371826172
## epoch[56] training(only) time: 18.639163494110107
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3193e+00 (1.3193e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.6092e+00 (1.4126e+00)	Acc@1  66.00 ( 71.45)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3446e+00 (1.3287e+00)	Acc@1  71.00 ( 72.24)	Acc@5  95.00 ( 92.14)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7651e+00 (1.3575e+00)	Acc@1  60.00 ( 71.71)	Acc@5  90.00 ( 91.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3804e+00 (1.3484e+00)	Acc@1  70.00 ( 71.85)	Acc@5  94.00 ( 92.12)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4142e+00 (1.3622e+00)	Acc@1  72.00 ( 71.73)	Acc@5  88.00 ( 91.84)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0424e+00 (1.3279e+00)	Acc@1  74.00 ( 72.13)	Acc@5  94.00 ( 92.10)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6781e+00 (1.3281e+00)	Acc@1  64.00 ( 71.97)	Acc@5  89.00 ( 92.21)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3405e+00 (1.3228e+00)	Acc@1  74.00 ( 71.96)	Acc@5  89.00 ( 92.14)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8665e+00 (1.3084e+00)	Acc@1  67.00 ( 72.22)	Acc@5  90.00 ( 92.29)
 * Acc@1 72.420 Acc@5 92.330
### epoch[56] execution time: 21.122246026992798
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.180 ( 0.180)	Data  0.140 ( 0.140)	Loss 1.8019e-02 (1.8019e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 4.8893e-02 (2.8475e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 ( 99.93)
Epoch: [57][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.5168e-02 (2.3614e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 ( 99.96)
Epoch: [57][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 6.6009e-02 (2.6452e-02)	Acc@1  97.66 ( 99.62)	Acc@5 100.00 ( 99.97)
Epoch: [57][ 40/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.006)	Loss 2.4910e-02 (2.5972e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 ( 99.98)
Epoch: [57][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.2475e-02 (2.6499e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.98)
Epoch: [57][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.3068e-02 (2.7513e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [57][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.0511e-02 (2.7396e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [57][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.2530e-02 (2.7246e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 ( 99.99)
Epoch: [57][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.7619e-02 (2.7646e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 ( 99.99)
Epoch: [57][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3785e-02 (2.8083e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 ( 99.99)
Epoch: [57][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7384e-02 (2.7960e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 ( 99.99)
Epoch: [57][120/391]	Time  0.049 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1015e-02 (2.7892e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [57][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0645e-02 (2.7791e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [57][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1650e-02 (2.7513e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [57][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0416e-02 (2.7295e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 ( 99.99)
Epoch: [57][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1726e-02 (2.7639e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2061e-02 (2.7707e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6328e-02 (2.7592e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8364e-02 (2.7704e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.7424e-02 (2.7784e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8822e-02 (2.7745e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0962e-02 (2.7722e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.6858e-02 (2.7618e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6684e-02 (2.7713e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7177e-02 (2.7616e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4785e-02 (2.7533e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9554e-02 (2.7556e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1111e-02 (2.7486e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7649e-02 (2.7387e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9369e-02 (2.7382e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8322e-02 (2.7523e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8851e-02 (2.7624e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2162e-02 (2.7713e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8699e-02 (2.7708e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5371e-02 (2.7613e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4528e-02 (2.7602e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2278e-02 (2.7487e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4261e-02 (2.7380e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1626e-02 (2.7566e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.1300048828125
## e[57]       loss.backward (sum) time: 2.552639961242676
## e[57]      optimizer.step (sum) time: 1.0136990547180176
## epoch[57] training(only) time: 18.645533800125122
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3264e+00 (1.3264e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.5335e+00 (1.3989e+00)	Acc@1  68.00 ( 71.73)	Acc@5  92.00 ( 91.36)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3531e+00 (1.3178e+00)	Acc@1  73.00 ( 72.71)	Acc@5  93.00 ( 92.33)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8368e+00 (1.3504e+00)	Acc@1  60.00 ( 72.00)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3301e+00 (1.3431e+00)	Acc@1  74.00 ( 72.02)	Acc@5  96.00 ( 92.34)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4117e+00 (1.3590e+00)	Acc@1  72.00 ( 71.76)	Acc@5  88.00 ( 91.98)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0533e+00 (1.3255e+00)	Acc@1  75.00 ( 72.25)	Acc@5  93.00 ( 92.20)
Test: [ 70/100]	Time  0.024 ( 0.025)	Loss 1.6974e+00 (1.3274e+00)	Acc@1  66.00 ( 72.04)	Acc@5  90.00 ( 92.34)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3754e+00 (1.3241e+00)	Acc@1  73.00 ( 72.06)	Acc@5  88.00 ( 92.27)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 1.8684e+00 (1.3093e+00)	Acc@1  67.00 ( 72.29)	Acc@5  91.00 ( 92.36)
 * Acc@1 72.350 Acc@5 92.430
### epoch[57] execution time: 21.15680718421936
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.193 ( 0.193)	Data  0.154 ( 0.154)	Loss 2.7782e-02 (2.7782e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.0323e-02 (2.7645e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 2.4047e-02 (2.4699e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.7848e-02 (2.5456e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 3.1256e-02 (2.5537e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.1171e-02 (2.6622e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.049 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.5552e-02 (2.6858e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.8997e-02 (2.7204e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.6737e-02 (2.7176e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.5025e-02 (2.7829e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.3680e-02 (2.7381e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.7487e-02 (2.7165e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4681e-02 (2.7396e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8658e-02 (2.7261e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4380e-02 (2.7061e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4412e-02 (2.7042e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0448e-02 (2.7141e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0899e-02 (2.7096e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.4628e-02 (2.6983e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.1681e-02 (2.6933e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8830e-02 (2.6994e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8380e-02 (2.6978e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5095e-02 (2.6713e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3626e-02 (2.6677e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0477e-02 (2.6524e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.0207e-03 (2.6604e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3657e-02 (2.6412e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3026e-02 (2.6454e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6702e-02 (2.6962e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8333e-02 (2.6817e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8175e-02 (2.6767e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.6961e-02 (2.6891e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7728e-02 (2.6884e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2532e-02 (2.6730e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1106e-02 (2.6727e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8418e-02 (2.6770e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1428e-02 (2.6787e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0190e-02 (2.6766e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0776e-02 (2.6886e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8331e-02 (2.6927e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.13056635856628418
## e[58]       loss.backward (sum) time: 2.5468027591705322
## e[58]      optimizer.step (sum) time: 1.016498327255249
## epoch[58] training(only) time: 18.662927865982056
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.3134e+00 (1.3134e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5383e+00 (1.4151e+00)	Acc@1  65.00 ( 71.18)	Acc@5  92.00 ( 91.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3809e+00 (1.3322e+00)	Acc@1  71.00 ( 72.62)	Acc@5  93.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.9393e+00 (1.3676e+00)	Acc@1  61.00 ( 71.94)	Acc@5  91.00 ( 91.61)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3290e+00 (1.3587e+00)	Acc@1  72.00 ( 72.05)	Acc@5  95.00 ( 92.00)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3832e+00 (1.3733e+00)	Acc@1  72.00 ( 71.71)	Acc@5  88.00 ( 91.78)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0709e+00 (1.3394e+00)	Acc@1  75.00 ( 72.21)	Acc@5  93.00 ( 92.07)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.7034e+00 (1.3403e+00)	Acc@1  66.00 ( 71.96)	Acc@5  89.00 ( 92.24)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3411e+00 (1.3363e+00)	Acc@1  74.00 ( 71.88)	Acc@5  89.00 ( 92.15)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 1.8280e+00 (1.3214e+00)	Acc@1  67.00 ( 72.11)	Acc@5  91.00 ( 92.29)
 * Acc@1 72.300 Acc@5 92.340
### epoch[58] execution time: 21.155312061309814
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.186 ( 0.186)	Data  0.147 ( 0.147)	Loss 3.9298e-02 (3.9298e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.048 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.2857e-02 (2.4169e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 2.1538e-02 (2.6120e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.8542e-02 (2.4591e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.8130e-02 (2.5317e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.2559e-02 (2.5120e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.3506e-02 (2.5092e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.1263e-02 (2.4749e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.2328e-02 (2.4774e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.8390e-02 (2.5432e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0754e-02 (2.5408e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9789e-02 (2.5336e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8731e-02 (2.5141e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0719e-02 (2.4941e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6931e-02 (2.5040e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5180e-02 (2.4818e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9024e-02 (2.4742e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.6847e-02 (2.4867e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.1040e-02 (2.5082e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2912e-02 (2.5139e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5794e-02 (2.5111e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4675e-02 (2.4862e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0273e-02 (2.4969e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.050 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.2878e-02 (2.5254e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8954e-02 (2.5308e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2744e-02 (2.5064e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1407e-02 (2.5151e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4787e-02 (2.5124e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2956e-02 (2.5140e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9585e-02 (2.5312e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8046e-02 (2.5190e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0337e-02 (2.5419e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1129e-02 (2.5545e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2362e-02 (2.5406e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0031e-02 (2.5547e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3127e-02 (2.5479e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4689e-02 (2.5579e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6164e-02 (2.5566e-02)	Acc@1  97.66 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0471e-02 (2.5627e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6140e-02 (2.5658e-02)	Acc@1  98.75 ( 99.66)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.12955284118652344
## e[59]       loss.backward (sum) time: 2.561737060546875
## e[59]      optimizer.step (sum) time: 1.008819580078125
## epoch[59] training(only) time: 18.63579773902893
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.2930e+00 (1.2930e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.6066e+00 (1.4117e+00)	Acc@1  65.00 ( 70.18)	Acc@5  90.00 ( 90.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3467e+00 (1.3360e+00)	Acc@1  71.00 ( 71.57)	Acc@5  92.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8403e+00 (1.3608e+00)	Acc@1  61.00 ( 71.23)	Acc@5  91.00 ( 91.71)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3228e+00 (1.3528e+00)	Acc@1  72.00 ( 71.37)	Acc@5  96.00 ( 92.10)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4037e+00 (1.3685e+00)	Acc@1  71.00 ( 71.12)	Acc@5  88.00 ( 91.82)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0957e+00 (1.3349e+00)	Acc@1  74.00 ( 71.54)	Acc@5  95.00 ( 92.10)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.6865e+00 (1.3352e+00)	Acc@1  64.00 ( 71.49)	Acc@5  91.00 ( 92.25)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3618e+00 (1.3318e+00)	Acc@1  73.00 ( 71.57)	Acc@5  89.00 ( 92.14)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8950e+00 (1.3170e+00)	Acc@1  67.00 ( 71.85)	Acc@5  91.00 ( 92.31)
 * Acc@1 72.150 Acc@5 92.380
### epoch[59] execution time: 21.1281955242157
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.183 ( 0.183)	Data  0.143 ( 0.143)	Loss 1.6357e-02 (1.6357e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 3.7831e-02 (1.9380e-02)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 3.7039e-02 (2.2051e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.1076e-02 (2.0935e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.7005e-02 (2.1060e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.048 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.5948e-02 (2.1774e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.8004e-02 (2.2266e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.3322e-02 (2.2120e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.3666e-02 (2.1847e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0928e-02 (2.1785e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1217e-02 (2.2009e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9386e-02 (2.2186e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4342e-02 (2.2211e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6618e-02 (2.2344e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7797e-02 (2.2284e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5479e-02 (2.2344e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9063e-02 (2.2542e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4497e-02 (2.2596e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6869e-02 (2.2801e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.045 ( 0.048)	Data  0.002 ( 0.003)	Loss 3.5390e-02 (2.2987e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2589e-02 (2.3042e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0257e-02 (2.2947e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7346e-02 (2.2994e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8637e-02 (2.3009e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5433e-02 (2.3008e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9889e-02 (2.3047e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8064e-02 (2.3018e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0239e-02 (2.3013e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9991e-02 (2.2846e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9649e-02 (2.2772e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2010e-02 (2.2889e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2616e-02 (2.3015e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.040 ( 0.047)	Data  0.002 ( 0.003)	Loss 3.0813e-02 (2.2981e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1001e-02 (2.2845e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6091e-02 (2.2826e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9316e-02 (2.2784e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5477e-02 (2.2851e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6712e-02 (2.2854e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6023e-02 (2.2959e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8937e-02 (2.3020e-02)	Acc@1  98.75 ( 99.72)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.13045883178710938
## e[60]       loss.backward (sum) time: 2.5662436485290527
## e[60]      optimizer.step (sum) time: 1.013056755065918
## epoch[60] training(only) time: 18.614644050598145
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.2889e+00 (1.2889e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5652e+00 (1.4060e+00)	Acc@1  66.00 ( 71.00)	Acc@5  92.00 ( 90.82)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3786e+00 (1.3302e+00)	Acc@1  70.00 ( 71.95)	Acc@5  92.00 ( 91.62)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7740e+00 (1.3523e+00)	Acc@1  60.00 ( 71.45)	Acc@5  92.00 ( 91.52)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3239e+00 (1.3446e+00)	Acc@1  72.00 ( 71.61)	Acc@5  95.00 ( 91.95)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4553e+00 (1.3643e+00)	Acc@1  73.00 ( 71.35)	Acc@5  87.00 ( 91.63)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0791e+00 (1.3302e+00)	Acc@1  75.00 ( 71.80)	Acc@5  94.00 ( 91.97)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6586e+00 (1.3310e+00)	Acc@1  64.00 ( 71.62)	Acc@5  90.00 ( 92.18)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3587e+00 (1.3277e+00)	Acc@1  75.00 ( 71.68)	Acc@5  90.00 ( 92.17)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8860e+00 (1.3134e+00)	Acc@1  67.00 ( 71.88)	Acc@5  92.00 ( 92.33)
 * Acc@1 72.110 Acc@5 92.370
### epoch[60] execution time: 21.118175745010376
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.194 ( 0.194)	Data  0.151 ( 0.151)	Loss 1.8660e-02 (1.8660e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 3.2655e-02 (2.2961e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.048 ( 0.054)	Data  0.001 ( 0.009)	Loss 2.0321e-02 (2.0936e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.044 ( 0.052)	Data  0.001 ( 0.007)	Loss 3.0412e-02 (2.0121e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 3.6863e-02 (2.0986e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.0466e-02 (2.1327e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.3017e-02 (2.1115e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.9417e-02 (2.1326e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.3068e-02 (2.1164e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.8104e-02 (2.1322e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7722e-02 (2.1198e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1445e-02 (2.1128e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5027e-02 (2.1152e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9520e-02 (2.1469e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.6275e-03 (2.1284e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.6632e-02 (2.1760e-02)	Acc@1  97.66 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4041e-02 (2.2314e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7893e-02 (2.2342e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7493e-02 (2.2197e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2978e-02 (2.2357e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9272e-02 (2.2217e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2591e-02 (2.2245e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3254e-02 (2.2184e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2627e-02 (2.2097e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9081e-02 (2.2186e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1096e-02 (2.2248e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3955e-02 (2.2306e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8999e-02 (2.2402e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9829e-02 (2.2527e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5882e-02 (2.2605e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4878e-02 (2.2582e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5215e-02 (2.2694e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1320e-02 (2.2678e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5661e-02 (2.2681e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1951e-02 (2.2622e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4793e-02 (2.2545e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5255e-02 (2.2626e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3183e-02 (2.2571e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.051 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4503e-02 (2.2648e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0705e-02 (2.2717e-02)	Acc@1  98.75 ( 99.73)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.1313786506652832
## e[61]       loss.backward (sum) time: 2.5557193756103516
## e[61]      optimizer.step (sum) time: 1.0035781860351562
## epoch[61] training(only) time: 18.63861918449402
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.3250e+00 (1.3250e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.6013e+00 (1.4069e+00)	Acc@1  66.00 ( 70.55)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3811e+00 (1.3359e+00)	Acc@1  72.00 ( 72.05)	Acc@5  92.00 ( 91.81)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8713e+00 (1.3615e+00)	Acc@1  60.00 ( 71.65)	Acc@5  91.00 ( 91.58)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3246e+00 (1.3550e+00)	Acc@1  75.00 ( 71.83)	Acc@5  96.00 ( 92.05)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4507e+00 (1.3703e+00)	Acc@1  72.00 ( 71.61)	Acc@5  87.00 ( 91.73)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0744e+00 (1.3357e+00)	Acc@1  73.00 ( 71.92)	Acc@5  94.00 ( 92.00)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6946e+00 (1.3364e+00)	Acc@1  67.00 ( 71.82)	Acc@5  89.00 ( 92.14)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3835e+00 (1.3328e+00)	Acc@1  74.00 ( 71.86)	Acc@5  90.00 ( 92.09)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8685e+00 (1.3170e+00)	Acc@1  67.00 ( 72.09)	Acc@5  91.00 ( 92.26)
 * Acc@1 72.290 Acc@5 92.320
### epoch[61] execution time: 21.1166250705719
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.194 ( 0.194)	Data  0.149 ( 0.149)	Loss 1.4291e-02 (1.4291e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.3761e-02 (1.6835e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 3.6372e-02 (2.1688e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.1194e-02 (2.1986e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.2631e-02 (2.1459e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 3.0581e-02 (2.2211e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.6165e-02 (2.2412e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.6995e-02 (2.2947e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.3364e-02 (2.3287e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.9292e-02 (2.3250e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1346e-02 (2.3708e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3242e-02 (2.3604e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6126e-02 (2.4131e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5964e-02 (2.4020e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.5512e-02 (2.3992e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3234e-02 (2.3969e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0767e-02 (2.4263e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4923e-02 (2.4152e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1761e-02 (2.4047e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4014e-02 (2.3868e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7954e-02 (2.3939e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9356e-02 (2.3928e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5272e-02 (2.4220e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6712e-02 (2.4037e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5669e-02 (2.4036e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8833e-02 (2.3778e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9290e-02 (2.3855e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1452e-02 (2.3774e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6224e-02 (2.3648e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3562e-02 (2.3638e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8810e-02 (2.3547e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3347e-02 (2.3532e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4684e-02 (2.3593e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8627e-02 (2.3604e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5347e-02 (2.3744e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2675e-02 (2.3807e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9473e-02 (2.3678e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5667e-02 (2.3691e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3815e-02 (2.3604e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8042e-02 (2.3690e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.12950825691223145
## e[62]       loss.backward (sum) time: 2.547051429748535
## e[62]      optimizer.step (sum) time: 1.0051672458648682
## epoch[62] training(only) time: 18.632395267486572
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2931e+00 (1.2931e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5388e+00 (1.3899e+00)	Acc@1  66.00 ( 71.45)	Acc@5  91.00 ( 91.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3363e+00 (1.3139e+00)	Acc@1  72.00 ( 72.57)	Acc@5  92.00 ( 92.00)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8408e+00 (1.3429e+00)	Acc@1  61.00 ( 72.16)	Acc@5  91.00 ( 91.74)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3272e+00 (1.3380e+00)	Acc@1  72.00 ( 72.15)	Acc@5  95.00 ( 92.12)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4102e+00 (1.3549e+00)	Acc@1  72.00 ( 71.86)	Acc@5  88.00 ( 91.80)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0670e+00 (1.3230e+00)	Acc@1  75.00 ( 72.26)	Acc@5  94.00 ( 92.07)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6652e+00 (1.3242e+00)	Acc@1  67.00 ( 72.08)	Acc@5  91.00 ( 92.28)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3450e+00 (1.3225e+00)	Acc@1  75.00 ( 72.12)	Acc@5  90.00 ( 92.21)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8435e+00 (1.3080e+00)	Acc@1  68.00 ( 72.30)	Acc@5  91.00 ( 92.34)
 * Acc@1 72.520 Acc@5 92.390
### epoch[62] execution time: 21.1217143535614
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.196 ( 0.196)	Data  0.153 ( 0.153)	Loss 1.6810e-02 (1.6810e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 4.1476e-02 (2.6462e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 3.1014e-02 (2.3781e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 2.7482e-02 (2.2970e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.0755e-02 (2.3740e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.5117e-02 (2.2914e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.8568e-02 (2.3474e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.8047e-02 (2.3294e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.3602e-02 (2.3260e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3113e-02 (2.2762e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9782e-02 (2.3093e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.2785e-03 (2.3031e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7793e-02 (2.3109e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5787e-02 (2.2853e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8638e-02 (2.3296e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4157e-02 (2.3280e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1537e-02 (2.3105e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0983e-02 (2.3052e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5271e-02 (2.2800e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5198e-02 (2.2718e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1765e-02 (2.2518e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0117e-02 (2.2324e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0906e-02 (2.2229e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2355e-02 (2.2285e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.7272e-02 (2.2624e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3574e-02 (2.2636e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8985e-02 (2.2703e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8973e-02 (2.2779e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6534e-02 (2.2791e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0721e-02 (2.2834e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2006e-02 (2.2915e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8277e-02 (2.2844e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9601e-03 (2.2710e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0389e-02 (2.2764e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.050 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1897e-02 (2.2725e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4894e-02 (2.2735e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5703e-02 (2.2754e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0257e-02 (2.2707e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1125e-02 (2.2652e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8876e-02 (2.2734e-02)	Acc@1  96.25 ( 99.71)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.13054895401000977
## e[63]       loss.backward (sum) time: 2.555352210998535
## e[63]      optimizer.step (sum) time: 1.0105435848236084
## epoch[63] training(only) time: 18.619824647903442
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.3209e+00 (1.3209e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5783e+00 (1.4009e+00)	Acc@1  66.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3664e+00 (1.3303e+00)	Acc@1  72.00 ( 72.24)	Acc@5  92.00 ( 91.86)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8196e+00 (1.3570e+00)	Acc@1  60.00 ( 71.97)	Acc@5  92.00 ( 91.77)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3313e+00 (1.3495e+00)	Acc@1  74.00 ( 72.02)	Acc@5  94.00 ( 92.20)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4282e+00 (1.3654e+00)	Acc@1  72.00 ( 71.73)	Acc@5  87.00 ( 91.86)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0899e+00 (1.3319e+00)	Acc@1  74.00 ( 72.11)	Acc@5  93.00 ( 92.11)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6657e+00 (1.3333e+00)	Acc@1  64.00 ( 71.90)	Acc@5  89.00 ( 92.25)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3659e+00 (1.3297e+00)	Acc@1  76.00 ( 71.99)	Acc@5  90.00 ( 92.21)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8736e+00 (1.3155e+00)	Acc@1  67.00 ( 72.16)	Acc@5  91.00 ( 92.35)
 * Acc@1 72.430 Acc@5 92.430
### epoch[63] execution time: 21.108471155166626
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.183 ( 0.183)	Data  0.142 ( 0.142)	Loss 1.7897e-02 (1.7897e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 3.5858e-02 (2.0930e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.7865e-02 (2.0669e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.1162e-02 (2.1984e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.5993e-02 (2.1304e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.3165e-02 (2.1152e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6820e-02 (2.1865e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.3236e-02 (2.2204e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1466e-02 (2.2404e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5887e-02 (2.1922e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4534e-02 (2.2080e-02)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7374e-02 (2.2243e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9526e-02 (2.2495e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8539e-02 (2.2459e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3698e-02 (2.2291e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5673e-02 (2.2588e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6918e-02 (2.2236e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6596e-02 (2.2276e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2758e-02 (2.2245e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1198e-02 (2.2045e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3478e-02 (2.1956e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7021e-02 (2.2058e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4917e-02 (2.1896e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9951e-02 (2.1981e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5785e-02 (2.2035e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8374e-02 (2.2209e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3199e-02 (2.2265e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0007e-02 (2.2282e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3544e-02 (2.2290e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9746e-02 (2.2347e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5252e-02 (2.2323e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2751e-02 (2.2423e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8427e-02 (2.2413e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0084e-02 (2.2401e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3735e-02 (2.2478e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7003e-02 (2.2462e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4840e-02 (2.2503e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6022e-02 (2.2582e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7850e-02 (2.2469e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7117e-02 (2.2588e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.13028836250305176
## e[64]       loss.backward (sum) time: 2.5343053340911865
## e[64]      optimizer.step (sum) time: 1.0050499439239502
## epoch[64] training(only) time: 18.568527221679688
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3024e+00 (1.3024e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5800e+00 (1.4064e+00)	Acc@1  65.00 ( 70.09)	Acc@5  91.00 ( 91.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3719e+00 (1.3351e+00)	Acc@1  71.00 ( 71.76)	Acc@5  93.00 ( 91.86)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7976e+00 (1.3577e+00)	Acc@1  63.00 ( 71.55)	Acc@5  91.00 ( 91.71)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3217e+00 (1.3479e+00)	Acc@1  74.00 ( 71.59)	Acc@5  95.00 ( 92.12)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4211e+00 (1.3634e+00)	Acc@1  72.00 ( 71.31)	Acc@5  89.00 ( 91.80)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0868e+00 (1.3286e+00)	Acc@1  76.00 ( 71.77)	Acc@5  93.00 ( 92.07)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6404e+00 (1.3285e+00)	Acc@1  68.00 ( 71.75)	Acc@5  89.00 ( 92.24)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3508e+00 (1.3250e+00)	Acc@1  75.00 ( 71.73)	Acc@5  90.00 ( 92.20)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8488e+00 (1.3101e+00)	Acc@1  66.00 ( 71.90)	Acc@5  92.00 ( 92.37)
 * Acc@1 72.180 Acc@5 92.460
### epoch[64] execution time: 21.050477027893066
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.178 ( 0.178)	Data  0.136 ( 0.136)	Loss 2.3298e-02 (2.3298e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.046 ( 0.058)	Data  0.001 ( 0.014)	Loss 1.1747e-02 (1.9522e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 3.0224e-02 (2.1302e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.2594e-02 (2.1674e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.0892e-02 (2.1758e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.0925e-02 (2.1328e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7614e-02 (2.2352e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.6572e-02 (2.2843e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0164e-02 (2.2752e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0962e-02 (2.2871e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2229e-02 (2.2992e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.6029e-03 (2.2780e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4458e-02 (2.2670e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3465e-02 (2.2379e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8267e-02 (2.2444e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.0868e-02 (2.2417e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0943e-02 (2.2423e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5688e-02 (2.2329e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8610e-02 (2.2202e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.6012e-02 (2.2247e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2707e-02 (2.2530e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0002e-02 (2.2438e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3602e-02 (2.2371e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6581e-02 (2.2432e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0219e-02 (2.2407e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3092e-02 (2.2513e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1548e-02 (2.2489e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.050 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3989e-02 (2.2554e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1985e-02 (2.2580e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8464e-02 (2.2541e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1767e-02 (2.2369e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5859e-02 (2.2478e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9109e-02 (2.2565e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8207e-02 (2.2596e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9355e-02 (2.2669e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5066e-02 (2.2722e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6883e-02 (2.2668e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9119e-02 (2.2750e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3531e-02 (2.2719e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5426e-02 (2.2821e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.13053059577941895
## e[65]       loss.backward (sum) time: 2.5661776065826416
## e[65]      optimizer.step (sum) time: 1.0167884826660156
## epoch[65] training(only) time: 18.59671711921692
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3010e+00 (1.3010e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5841e+00 (1.4043e+00)	Acc@1  67.00 ( 71.18)	Acc@5  91.00 ( 90.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3659e+00 (1.3308e+00)	Acc@1  72.00 ( 72.29)	Acc@5  93.00 ( 91.95)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8382e+00 (1.3593e+00)	Acc@1  60.00 ( 71.84)	Acc@5  93.00 ( 91.87)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3218e+00 (1.3514e+00)	Acc@1  72.00 ( 71.93)	Acc@5  95.00 ( 92.17)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.4187e+00 (1.3677e+00)	Acc@1  70.00 ( 71.67)	Acc@5  88.00 ( 91.80)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0831e+00 (1.3325e+00)	Acc@1  76.00 ( 72.18)	Acc@5  93.00 ( 92.02)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6676e+00 (1.3328e+00)	Acc@1  66.00 ( 72.00)	Acc@5  89.00 ( 92.18)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3653e+00 (1.3290e+00)	Acc@1  75.00 ( 72.04)	Acc@5  89.00 ( 92.14)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8496e+00 (1.3142e+00)	Acc@1  66.00 ( 72.31)	Acc@5  91.00 ( 92.29)
 * Acc@1 72.510 Acc@5 92.340
### epoch[65] execution time: 21.085601568222046
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.189 ( 0.189)	Data  0.148 ( 0.148)	Loss 1.5604e-02 (1.5604e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.8117e-02 (2.1601e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.5815e-02 (2.2305e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.048 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.8034e-02 (2.3944e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.3954e-02 (2.4411e-02)	Acc@1  97.66 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.2178e-02 (2.4247e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.048 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4713e-02 (2.3772e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.5808e-02 (2.3932e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.044 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.6130e-02 (2.3390e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.5634e-02 (2.3047e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7826e-02 (2.3137e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1815e-02 (2.3086e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0027e-02 (2.2798e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0385e-02 (2.2745e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3273e-02 (2.2674e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9453e-02 (2.2529e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0662e-02 (2.2505e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8416e-02 (2.2983e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5208e-02 (2.2667e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5802e-02 (2.2765e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4349e-02 (2.2964e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9443e-02 (2.2836e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2945e-02 (2.2837e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2389e-02 (2.2968e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2356e-02 (2.2890e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8036e-02 (2.2732e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4566e-02 (2.2781e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6110e-02 (2.2616e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6816e-02 (2.2554e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.050 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0434e-03 (2.2463e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4819e-02 (2.2435e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5905e-02 (2.2332e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4450e-02 (2.2441e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6053e-02 (2.2412e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0244e-02 (2.2525e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6544e-02 (2.2495e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1064e-02 (2.2419e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1886e-02 (2.2323e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4896e-02 (2.2326e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6524e-02 (2.2366e-02)	Acc@1  97.50 ( 99.74)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.13117623329162598
## e[66]       loss.backward (sum) time: 2.5627083778381348
## e[66]      optimizer.step (sum) time: 1.0159807205200195
## epoch[66] training(only) time: 18.58365035057068
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.3479e+00 (1.3479e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5565e+00 (1.4050e+00)	Acc@1  66.00 ( 70.73)	Acc@5  90.00 ( 91.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3506e+00 (1.3261e+00)	Acc@1  72.00 ( 72.14)	Acc@5  92.00 ( 91.76)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8493e+00 (1.3545e+00)	Acc@1  61.00 ( 71.68)	Acc@5  92.00 ( 91.58)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3638e+00 (1.3471e+00)	Acc@1  73.00 ( 71.88)	Acc@5  93.00 ( 92.00)
Test: [ 50/100]	Time  0.027 ( 0.025)	Loss 1.3984e+00 (1.3607e+00)	Acc@1  70.00 ( 71.65)	Acc@5  88.00 ( 91.76)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0924e+00 (1.3279e+00)	Acc@1  76.00 ( 72.02)	Acc@5  93.00 ( 92.00)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.7187e+00 (1.3298e+00)	Acc@1  67.00 ( 71.87)	Acc@5  90.00 ( 92.17)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3500e+00 (1.3276e+00)	Acc@1  76.00 ( 71.88)	Acc@5  90.00 ( 92.14)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8409e+00 (1.3123e+00)	Acc@1  66.00 ( 72.11)	Acc@5  91.00 ( 92.30)
 * Acc@1 72.330 Acc@5 92.360
### epoch[66] execution time: 21.073715209960938
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.195 ( 0.195)	Data  0.156 ( 0.156)	Loss 1.9187e-02 (1.9187e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.7948e-02 (2.2411e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.6225e-02 (2.0756e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.7513e-02 (2.0780e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.0132e-02 (2.1138e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.6834e-02 (2.1089e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6937e-02 (2.1420e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.3762e-02 (2.1131e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.2307e-02 (2.1745e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4181e-02 (2.1587e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1540e-02 (2.1736e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9168e-02 (2.1742e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.4557e-03 (2.1660e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3634e-02 (2.1778e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6112e-02 (2.1973e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9867e-02 (2.1962e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7423e-02 (2.2022e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7310e-02 (2.1960e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8904e-02 (2.1959e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9717e-02 (2.2097e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4744e-02 (2.1905e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9837e-02 (2.1930e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7262e-02 (2.1986e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1044e-02 (2.1933e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7521e-02 (2.1867e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0122e-02 (2.1950e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4065e-02 (2.1940e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9238e-02 (2.1898e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5308e-02 (2.2131e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5398e-02 (2.2107e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7891e-02 (2.2122e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3407e-02 (2.2232e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1103e-02 (2.2067e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8674e-02 (2.2062e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4331e-02 (2.2155e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2540e-02 (2.2005e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2119e-02 (2.2105e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6116e-02 (2.2070e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6481e-02 (2.1956e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9064e-02 (2.1961e-02)	Acc@1  98.75 ( 99.77)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.1325840950012207
## e[67]       loss.backward (sum) time: 2.5530173778533936
## e[67]      optimizer.step (sum) time: 1.0120089054107666
## epoch[67] training(only) time: 18.627054929733276
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.3099e+00 (1.3099e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5903e+00 (1.4108e+00)	Acc@1  66.00 ( 71.18)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3652e+00 (1.3333e+00)	Acc@1  73.00 ( 72.57)	Acc@5  93.00 ( 91.90)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 1.8511e+00 (1.3599e+00)	Acc@1  57.00 ( 72.03)	Acc@5  92.00 ( 91.81)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3416e+00 (1.3523e+00)	Acc@1  72.00 ( 72.07)	Acc@5  94.00 ( 92.20)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.4071e+00 (1.3654e+00)	Acc@1  71.00 ( 71.76)	Acc@5  88.00 ( 91.92)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0679e+00 (1.3312e+00)	Acc@1  77.00 ( 72.18)	Acc@5  93.00 ( 92.13)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6966e+00 (1.3323e+00)	Acc@1  68.00 ( 72.15)	Acc@5  89.00 ( 92.23)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3699e+00 (1.3299e+00)	Acc@1  74.00 ( 72.15)	Acc@5  90.00 ( 92.17)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8295e+00 (1.3147e+00)	Acc@1  68.00 ( 72.38)	Acc@5  91.00 ( 92.31)
 * Acc@1 72.570 Acc@5 92.400
### epoch[67] execution time: 21.123940229415894
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.183 ( 0.183)	Data  0.143 ( 0.143)	Loss 2.4327e-02 (2.4327e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.2688e-02 (2.4590e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.8527e-02 (2.2924e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.8065e-02 (2.2719e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 5.4655e-02 (2.2266e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 4.8305e-02 (2.2074e-02)	Acc@1  97.66 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7945e-02 (2.2891e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.3919e-02 (2.2465e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.1369e-02 (2.1974e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9810e-02 (2.2036e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3512e-02 (2.1949e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.1583e-02 (2.2105e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3944e-02 (2.1863e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4227e-02 (2.1652e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3825e-02 (2.1523e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8205e-02 (2.1603e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.4014e-02 (2.2045e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3340e-02 (2.2113e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2818e-02 (2.2138e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2519e-02 (2.1974e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6825e-02 (2.2075e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2501e-02 (2.1915e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4933e-02 (2.2034e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.1373e-02 (2.2205e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 5.1119e-02 (2.2505e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6218e-02 (2.2389e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0727e-02 (2.2493e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1275e-02 (2.2479e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3589e-02 (2.2479e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7740e-02 (2.2398e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7881e-02 (2.2397e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7799e-02 (2.2647e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1157e-02 (2.2457e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2810e-02 (2.2418e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8035e-02 (2.2448e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6433e-02 (2.2646e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8945e-02 (2.2763e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8627e-02 (2.2783e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7165e-02 (2.2801e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8486e-02 (2.2830e-02)	Acc@1  97.50 ( 99.70)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.13036084175109863
## e[68]       loss.backward (sum) time: 2.5599095821380615
## e[68]      optimizer.step (sum) time: 1.0094614028930664
## epoch[68] training(only) time: 18.62884020805359
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.3129e+00 (1.3129e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5843e+00 (1.4063e+00)	Acc@1  66.00 ( 71.55)	Acc@5  90.00 ( 91.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4156e+00 (1.3298e+00)	Acc@1  71.00 ( 72.48)	Acc@5  93.00 ( 91.95)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7996e+00 (1.3562e+00)	Acc@1  62.00 ( 72.03)	Acc@5  92.00 ( 91.68)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3537e+00 (1.3471e+00)	Acc@1  72.00 ( 72.22)	Acc@5  94.00 ( 92.05)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4293e+00 (1.3645e+00)	Acc@1  70.00 ( 71.86)	Acc@5  87.00 ( 91.67)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0813e+00 (1.3298e+00)	Acc@1  74.00 ( 72.31)	Acc@5  93.00 ( 91.98)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6505e+00 (1.3311e+00)	Acc@1  67.00 ( 72.21)	Acc@5  89.00 ( 92.17)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3732e+00 (1.3274e+00)	Acc@1  74.00 ( 72.23)	Acc@5  89.00 ( 92.12)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8493e+00 (1.3124e+00)	Acc@1  67.00 ( 72.47)	Acc@5  91.00 ( 92.23)
 * Acc@1 72.640 Acc@5 92.300
### epoch[68] execution time: 21.12342405319214
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.196 ( 0.196)	Data  0.156 ( 0.156)	Loss 1.6635e-02 (1.6635e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.047 ( 0.061)	Data  0.001 ( 0.016)	Loss 1.6864e-02 (2.0221e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.9667e-02 (2.0115e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.4046e-02 (2.1106e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.006)	Loss 1.1973e-02 (2.1132e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.6860e-02 (2.1481e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.2954e-02 (2.1552e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7714e-02 (2.1610e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.7549e-02 (2.1066e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 4.8680e-02 (2.1328e-02)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7156e-02 (2.1273e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8125e-02 (2.1271e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6636e-02 (2.1098e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9195e-02 (2.0685e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3486e-02 (2.0677e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6474e-02 (2.0949e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7103e-02 (2.1085e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5572e-02 (2.1137e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9245e-02 (2.1169e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.9595e-03 (2.1097e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7098e-02 (2.0982e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4220e-02 (2.1397e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5997e-02 (2.1380e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3894e-02 (2.1417e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3519e-02 (2.1649e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9009e-02 (2.1724e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3139e-02 (2.1706e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5533e-02 (2.1528e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0087e-02 (2.1628e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0031e-02 (2.1656e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7167e-02 (2.1813e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9860e-02 (2.1931e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4477e-02 (2.1989e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3807e-02 (2.2089e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3382e-02 (2.2160e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9827e-02 (2.2137e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6692e-02 (2.2116e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8873e-02 (2.2140e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3553e-02 (2.2155e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3284e-02 (2.2083e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.13099002838134766
## e[69]       loss.backward (sum) time: 2.5492007732391357
## e[69]      optimizer.step (sum) time: 1.0047364234924316
## epoch[69] training(only) time: 18.59964346885681
# Switched to evaluate mode...
Test: [  0/100]	Time  0.136 ( 0.136)	Loss 1.2836e+00 (1.2836e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 1.5658e+00 (1.3972e+00)	Acc@1  66.00 ( 70.91)	Acc@5  91.00 ( 90.64)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 1.3680e+00 (1.3245e+00)	Acc@1  71.00 ( 72.14)	Acc@5  93.00 ( 91.67)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8262e+00 (1.3509e+00)	Acc@1  61.00 ( 71.74)	Acc@5  91.00 ( 91.52)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3168e+00 (1.3428e+00)	Acc@1  72.00 ( 71.90)	Acc@5  94.00 ( 92.00)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4209e+00 (1.3571e+00)	Acc@1  72.00 ( 71.73)	Acc@5  87.00 ( 91.69)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0741e+00 (1.3235e+00)	Acc@1  75.00 ( 72.20)	Acc@5  93.00 ( 91.97)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6954e+00 (1.3254e+00)	Acc@1  66.00 ( 72.03)	Acc@5  89.00 ( 92.11)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3546e+00 (1.3221e+00)	Acc@1  73.00 ( 72.01)	Acc@5  89.00 ( 92.10)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8334e+00 (1.3079e+00)	Acc@1  66.00 ( 72.30)	Acc@5  91.00 ( 92.24)
 * Acc@1 72.510 Acc@5 92.330
### epoch[69] execution time: 21.075846910476685
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.191 ( 0.191)	Data  0.148 ( 0.148)	Loss 3.8521e-02 (3.8521e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.4116e-02 (2.3615e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.1827e-02 (2.1431e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.5539e-02 (2.1514e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.2340e-02 (2.1270e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.4772e-02 (2.0954e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.1215e-02 (2.1168e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.0995e-02 (2.1032e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.8339e-02 (2.1530e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2889e-02 (2.1446e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6247e-02 (2.1317e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4225e-02 (2.1113e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3166e-02 (2.1444e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0442e-02 (2.1301e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5776e-02 (2.1117e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9285e-02 (2.1209e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5657e-02 (2.1180e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 8.2817e-03 (2.1146e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9586e-02 (2.1239e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2088e-02 (2.1386e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5900e-02 (2.1498e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.6418e-02 (2.1670e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2513e-03 (2.1590e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2903e-02 (2.1835e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0932e-02 (2.1777e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2440e-02 (2.2046e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7007e-02 (2.2034e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8804e-02 (2.2110e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5049e-02 (2.2018e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2114e-02 (2.1878e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2000e-02 (2.1802e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5610e-02 (2.1825e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7555e-02 (2.1854e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7109e-02 (2.1807e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7074e-02 (2.1839e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1986e-02 (2.1880e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4739e-02 (2.1726e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2976e-02 (2.1725e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7912e-02 (2.1681e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9395e-02 (2.1668e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.13118815422058105
## e[70]       loss.backward (sum) time: 2.568464994430542
## e[70]      optimizer.step (sum) time: 1.0149140357971191
## epoch[70] training(only) time: 18.5989191532135
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.2795e+00 (1.2795e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5758e+00 (1.3990e+00)	Acc@1  65.00 ( 71.27)	Acc@5  91.00 ( 91.36)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3397e+00 (1.3224e+00)	Acc@1  72.00 ( 72.52)	Acc@5  93.00 ( 92.19)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8096e+00 (1.3466e+00)	Acc@1  61.00 ( 72.00)	Acc@5  92.00 ( 91.94)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3377e+00 (1.3409e+00)	Acc@1  72.00 ( 72.05)	Acc@5  94.00 ( 92.27)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4333e+00 (1.3570e+00)	Acc@1  72.00 ( 71.78)	Acc@5  88.00 ( 91.94)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0580e+00 (1.3212e+00)	Acc@1  74.00 ( 72.25)	Acc@5  93.00 ( 92.20)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.7111e+00 (1.3238e+00)	Acc@1  65.00 ( 72.07)	Acc@5  88.00 ( 92.30)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3412e+00 (1.3210e+00)	Acc@1  75.00 ( 72.11)	Acc@5  90.00 ( 92.26)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8576e+00 (1.3075e+00)	Acc@1  67.00 ( 72.37)	Acc@5  90.00 ( 92.37)
 * Acc@1 72.580 Acc@5 92.460
### epoch[70] execution time: 21.079781532287598
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.182 ( 0.182)	Data  0.139 ( 0.139)	Loss 5.0395e-02 (5.0395e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.0344e-02 (2.2877e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 4.0407e-02 (2.1566e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.9073e-02 (2.0825e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.7222e-02 (2.0206e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.0283e-02 (2.1272e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.8183e-02 (2.1951e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 3.0374e-02 (2.1548e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1896e-02 (2.1282e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8340e-02 (2.1051e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9263e-02 (2.1570e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0810e-02 (2.1316e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5388e-02 (2.1328e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6435e-02 (2.1184e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0077e-02 (2.1146e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8972e-02 (2.1157e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8933e-02 (2.1382e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0291e-02 (2.1449e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3358e-02 (2.1386e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9291e-02 (2.1168e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5346e-02 (2.1163e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9072e-02 (2.1431e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6050e-02 (2.1536e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4147e-02 (2.1478e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5993e-02 (2.1619e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7147e-02 (2.1556e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8859e-02 (2.1424e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0330e-02 (2.1487e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2600e-02 (2.1459e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1457e-02 (2.1371e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4389e-02 (2.1480e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9720e-02 (2.1420e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8403e-02 (2.1409e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7877e-02 (2.1532e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2267e-02 (2.1468e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9751e-02 (2.1509e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6964e-02 (2.1508e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2190e-02 (2.1465e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0218e-02 (2.1430e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6591e-02 (2.1368e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.13141417503356934
## e[71]       loss.backward (sum) time: 2.565541982650757
## e[71]      optimizer.step (sum) time: 1.0203089714050293
## epoch[71] training(only) time: 18.58230495452881
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.3068e+00 (1.3068e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 1.5800e+00 (1.3983e+00)	Acc@1  64.00 ( 70.00)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 1.3665e+00 (1.3240e+00)	Acc@1  71.00 ( 71.67)	Acc@5  93.00 ( 91.86)
Test: [ 30/100]	Time  0.022 ( 0.026)	Loss 1.8608e+00 (1.3544e+00)	Acc@1  60.00 ( 71.23)	Acc@5  91.00 ( 91.61)
Test: [ 40/100]	Time  0.022 ( 0.025)	Loss 1.3286e+00 (1.3447e+00)	Acc@1  71.00 ( 71.39)	Acc@5  94.00 ( 92.07)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4047e+00 (1.3583e+00)	Acc@1  71.00 ( 71.43)	Acc@5  88.00 ( 91.82)
Test: [ 60/100]	Time  0.023 ( 0.024)	Loss 1.0914e+00 (1.3256e+00)	Acc@1  73.00 ( 71.87)	Acc@5  93.00 ( 92.07)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6694e+00 (1.3273e+00)	Acc@1  66.00 ( 71.69)	Acc@5  91.00 ( 92.24)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3343e+00 (1.3235e+00)	Acc@1  73.00 ( 71.78)	Acc@5  89.00 ( 92.15)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8333e+00 (1.3090e+00)	Acc@1  68.00 ( 72.09)	Acc@5  91.00 ( 92.31)
 * Acc@1 72.290 Acc@5 92.370
### epoch[71] execution time: 21.078393936157227
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.187 ( 0.187)	Data  0.148 ( 0.148)	Loss 1.0481e-02 (1.0481e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.050 ( 0.060)	Data  0.001 ( 0.016)	Loss 8.5095e-03 (1.9019e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.8536e-02 (2.0831e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 3.2271e-02 (2.1267e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.7638e-02 (2.1884e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.7006e-02 (2.2587e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4297e-02 (2.2150e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.8199e-02 (2.2122e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.6689e-02 (2.2203e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.0403e-02 (2.2756e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3907e-02 (2.2497e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8896e-02 (2.2747e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8118e-02 (2.2668e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3516e-02 (2.2478e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.3027e-03 (2.2138e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4460e-02 (2.2207e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2102e-02 (2.2280e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.7114e-02 (2.2143e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2580e-02 (2.2158e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3144e-02 (2.2091e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0009e-02 (2.1955e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6231e-02 (2.2013e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6607e-02 (2.1804e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8652e-02 (2.1646e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.7076e-02 (2.1673e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1108e-02 (2.1764e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3556e-02 (2.1729e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2045e-03 (2.1561e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2898e-02 (2.1615e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5702e-02 (2.1722e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1756e-02 (2.1675e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4868e-02 (2.1728e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9010e-02 (2.1683e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6416e-02 (2.1806e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7271e-02 (2.1686e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0699e-02 (2.1797e-02)	Acc@1  97.66 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0572e-02 (2.1650e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2484e-02 (2.1700e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2307e-02 (2.1748e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4357e-02 (2.1711e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.12944936752319336
## e[72]       loss.backward (sum) time: 2.5516433715820312
## e[72]      optimizer.step (sum) time: 1.0025033950805664
## epoch[72] training(only) time: 18.612130403518677
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2914e+00 (1.2914e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5910e+00 (1.3993e+00)	Acc@1  66.00 ( 71.09)	Acc@5  91.00 ( 90.73)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3572e+00 (1.3225e+00)	Acc@1  71.00 ( 72.14)	Acc@5  93.00 ( 91.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8250e+00 (1.3493e+00)	Acc@1  60.00 ( 71.77)	Acc@5  90.00 ( 91.58)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3483e+00 (1.3421e+00)	Acc@1  72.00 ( 71.85)	Acc@5  95.00 ( 92.05)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4032e+00 (1.3577e+00)	Acc@1  70.00 ( 71.65)	Acc@5  88.00 ( 91.78)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0708e+00 (1.3223e+00)	Acc@1  74.00 ( 72.16)	Acc@5  93.00 ( 92.05)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6766e+00 (1.3233e+00)	Acc@1  66.00 ( 72.08)	Acc@5  89.00 ( 92.23)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3434e+00 (1.3215e+00)	Acc@1  74.00 ( 72.06)	Acc@5  89.00 ( 92.19)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8319e+00 (1.3068e+00)	Acc@1  65.00 ( 72.25)	Acc@5  91.00 ( 92.30)
 * Acc@1 72.450 Acc@5 92.380
### epoch[72] execution time: 21.10109257698059
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.184 ( 0.184)	Data  0.141 ( 0.141)	Loss 3.9461e-02 (3.9461e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.048 ( 0.059)	Data  0.001 ( 0.015)	Loss 1.9297e-02 (2.1502e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 2.2979e-02 (2.1827e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.0350e-02 (2.2599e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.6170e-02 (2.2144e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.5857e-02 (2.2351e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.2471e-02 (2.2005e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.9042e-02 (2.1768e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.1393e-02 (2.1905e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7960e-02 (2.2383e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7806e-02 (2.2277e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2100e-02 (2.1897e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7748e-02 (2.1985e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1660e-02 (2.1628e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4761e-02 (2.1421e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4279e-02 (2.1685e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0645e-02 (2.1517e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4120e-02 (2.1299e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7263e-02 (2.1206e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4399e-02 (2.1427e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4798e-02 (2.1664e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0876e-02 (2.1793e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9279e-02 (2.1574e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6950e-02 (2.1798e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1371e-02 (2.1863e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3423e-02 (2.1863e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2115e-02 (2.1725e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6265e-02 (2.1568e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9029e-02 (2.1590e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5339e-02 (2.1545e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0427e-02 (2.1448e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3129e-02 (2.1444e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1081e-02 (2.1426e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7828e-02 (2.1553e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8804e-02 (2.1536e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6209e-02 (2.1538e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8378e-02 (2.1573e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2099e-02 (2.1573e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6416e-02 (2.1562e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8355e-02 (2.1452e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.132582426071167
## e[73]       loss.backward (sum) time: 2.5786547660827637
## e[73]      optimizer.step (sum) time: 1.008049726486206
## epoch[73] training(only) time: 18.63054370880127
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2946e+00 (1.2946e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5480e+00 (1.3927e+00)	Acc@1  66.00 ( 71.36)	Acc@5  91.00 ( 90.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3492e+00 (1.3192e+00)	Acc@1  74.00 ( 72.62)	Acc@5  93.00 ( 91.86)
Test: [ 30/100]	Time  0.024 ( 0.027)	Loss 1.7924e+00 (1.3461e+00)	Acc@1  62.00 ( 72.03)	Acc@5  91.00 ( 91.61)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3315e+00 (1.3388e+00)	Acc@1  73.00 ( 72.10)	Acc@5  95.00 ( 92.05)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4272e+00 (1.3548e+00)	Acc@1  72.00 ( 71.82)	Acc@5  88.00 ( 91.84)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0817e+00 (1.3216e+00)	Acc@1  74.00 ( 72.31)	Acc@5  93.00 ( 92.03)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6792e+00 (1.3240e+00)	Acc@1  66.00 ( 72.13)	Acc@5  89.00 ( 92.17)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3469e+00 (1.3203e+00)	Acc@1  75.00 ( 72.26)	Acc@5  90.00 ( 92.14)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8197e+00 (1.3058e+00)	Acc@1  65.00 ( 72.45)	Acc@5  91.00 ( 92.25)
 * Acc@1 72.660 Acc@5 92.370
### epoch[73] execution time: 21.126859426498413
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.187 ( 0.187)	Data  0.146 ( 0.146)	Loss 1.8098e-02 (1.8098e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.1561e-02 (2.1588e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.7458e-02 (2.2067e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.2243e-02 (2.2002e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.3819e-02 (2.0981e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.4608e-02 (2.1091e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.1500e-02 (2.0293e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4696e-02 (2.0332e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.4089e-02 (2.0643e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.5147e-02 (2.0444e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3775e-02 (2.0496e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4829e-02 (2.0745e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2715e-02 (2.0700e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2425e-02 (2.0600e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7369e-02 (2.0714e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0517e-02 (2.1027e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5862e-02 (2.1081e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5056e-02 (2.1169e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8584e-02 (2.1399e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5147e-02 (2.1733e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7744e-02 (2.1642e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5114e-02 (2.1471e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.2660e-02 (2.1522e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4443e-02 (2.1405e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2924e-02 (2.1511e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9270e-02 (2.1476e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7602e-02 (2.1544e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0324e-02 (2.1756e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6640e-02 (2.1718e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4255e-02 (2.1727e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3189e-02 (2.1685e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4378e-02 (2.1561e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1676e-02 (2.1449e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9107e-02 (2.1421e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5642e-02 (2.1372e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3109e-02 (2.1312e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8636e-02 (2.1270e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3954e-02 (2.1151e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8433e-02 (2.1157e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6520e-02 (2.1162e-02)	Acc@1  98.75 ( 99.73)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.12970757484436035
## e[74]       loss.backward (sum) time: 2.542048931121826
## e[74]      optimizer.step (sum) time: 1.0134825706481934
## epoch[74] training(only) time: 18.63196897506714
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3326e+00 (1.3326e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5469e+00 (1.3966e+00)	Acc@1  66.00 ( 71.55)	Acc@5  91.00 ( 91.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3604e+00 (1.3229e+00)	Acc@1  72.00 ( 72.67)	Acc@5  92.00 ( 91.86)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8147e+00 (1.3504e+00)	Acc@1  61.00 ( 72.13)	Acc@5  92.00 ( 91.65)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3232e+00 (1.3416e+00)	Acc@1  72.00 ( 72.15)	Acc@5  96.00 ( 92.10)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3759e+00 (1.3575e+00)	Acc@1  74.00 ( 71.88)	Acc@5  87.00 ( 91.76)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0931e+00 (1.3243e+00)	Acc@1  76.00 ( 72.38)	Acc@5  93.00 ( 92.02)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6838e+00 (1.3274e+00)	Acc@1  65.00 ( 72.15)	Acc@5  89.00 ( 92.17)
Test: [ 80/100]	Time  0.028 ( 0.024)	Loss 1.3293e+00 (1.3243e+00)	Acc@1  74.00 ( 72.16)	Acc@5  90.00 ( 92.10)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8232e+00 (1.3096e+00)	Acc@1  66.00 ( 72.34)	Acc@5  91.00 ( 92.26)
 * Acc@1 72.500 Acc@5 92.350
### epoch[74] execution time: 21.115684270858765
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.196 ( 0.196)	Data  0.152 ( 0.152)	Loss 1.5517e-02 (1.5517e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.2125e-02 (1.8706e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 3.0569e-02 (2.1382e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.1731e-02 (2.0861e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.006)	Loss 8.8628e-03 (2.0776e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.6845e-02 (2.0547e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.4619e-02 (2.1025e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.2064e-02 (2.1604e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.8943e-02 (2.1676e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.8378e-02 (2.1118e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0358e-02 (2.1199e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3319e-02 (2.1070e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 ( 99.99)
Epoch: [75][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2802e-02 (2.0954e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 ( 99.99)
Epoch: [75][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4882e-02 (2.0670e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 ( 99.99)
Epoch: [75][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3054e-02 (2.0471e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 ( 99.99)
Epoch: [75][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9588e-02 (2.0631e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 ( 99.99)
Epoch: [75][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1122e-02 (2.0561e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6581e-02 (2.0581e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0932e-02 (2.0792e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2811e-02 (2.0786e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4236e-02 (2.0814e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9746e-02 (2.1099e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4189e-02 (2.1266e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1669e-02 (2.1378e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4385e-02 (2.1372e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6339e-02 (2.1287e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4778e-02 (2.1313e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1080e-02 (2.1138e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3540e-02 (2.1337e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.050 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6862e-02 (2.1542e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1309e-02 (2.1564e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9439e-02 (2.1761e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6121e-02 (2.1773e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2763e-02 (2.1742e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3855e-02 (2.1779e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0317e-02 (2.1777e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3413e-02 (2.1929e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6737e-02 (2.1873e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9056e-02 (2.1893e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3618e-02 (2.1793e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.13147640228271484
## e[75]       loss.backward (sum) time: 2.554354667663574
## e[75]      optimizer.step (sum) time: 1.0105223655700684
## epoch[75] training(only) time: 18.642258644104004
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.3188e+00 (1.3188e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5856e+00 (1.4004e+00)	Acc@1  66.00 ( 71.00)	Acc@5  90.00 ( 90.82)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3492e+00 (1.3232e+00)	Acc@1  71.00 ( 72.05)	Acc@5  93.00 ( 91.76)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8191e+00 (1.3507e+00)	Acc@1  62.00 ( 71.74)	Acc@5  91.00 ( 91.61)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3420e+00 (1.3432e+00)	Acc@1  73.00 ( 71.83)	Acc@5  95.00 ( 92.05)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4347e+00 (1.3591e+00)	Acc@1  72.00 ( 71.73)	Acc@5  87.00 ( 91.78)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0736e+00 (1.3248e+00)	Acc@1  75.00 ( 72.25)	Acc@5  93.00 ( 92.08)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6576e+00 (1.3261e+00)	Acc@1  64.00 ( 72.07)	Acc@5  89.00 ( 92.23)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3591e+00 (1.3234e+00)	Acc@1  75.00 ( 72.09)	Acc@5  90.00 ( 92.20)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8598e+00 (1.3087e+00)	Acc@1  66.00 ( 72.26)	Acc@5  90.00 ( 92.33)
 * Acc@1 72.460 Acc@5 92.400
### epoch[75] execution time: 21.143980503082275
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.185 ( 0.185)	Data  0.144 ( 0.144)	Loss 1.3272e-02 (1.3272e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.5453e-02 (1.9745e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 2.7757e-02 (2.1318e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.050 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.8313e-02 (2.0537e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.7405e-02 (1.9984e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.8555e-02 (2.0504e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.4782e-02 (2.0677e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.5664e-02 (2.0481e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.7701e-02 (2.0245e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3297e-02 (2.0625e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7166e-02 (2.0974e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6834e-02 (2.1015e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4151e-02 (2.1300e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2642e-02 (2.1200e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2258e-02 (2.1180e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6844e-02 (2.1428e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3722e-02 (2.1236e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3425e-02 (2.1481e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6459e-02 (2.1489e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6099e-02 (2.1279e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1096e-02 (2.1391e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2115e-02 (2.1370e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.2498e-02 (2.1569e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1100e-02 (2.1362e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2325e-02 (2.1329e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7422e-02 (2.1372e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4073e-02 (2.1377e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3446e-02 (2.1339e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9530e-02 (2.1230e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7527e-02 (2.1181e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2538e-02 (2.1185e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3426e-02 (2.1051e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0789e-02 (2.0976e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3842e-02 (2.0865e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0900e-02 (2.0732e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6493e-02 (2.0803e-02)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4850e-02 (2.0764e-02)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5101e-02 (2.0654e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5250e-02 (2.0698e-02)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9806e-02 (2.0685e-02)	Acc@1  98.75 ( 99.78)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.13016557693481445
## e[76]       loss.backward (sum) time: 2.5527350902557373
## e[76]      optimizer.step (sum) time: 1.0122942924499512
## epoch[76] training(only) time: 18.60723876953125
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.3112e+00 (1.3112e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5812e+00 (1.4140e+00)	Acc@1  66.00 ( 70.91)	Acc@5  91.00 ( 90.91)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3675e+00 (1.3334e+00)	Acc@1  73.00 ( 72.24)	Acc@5  94.00 ( 91.76)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8444e+00 (1.3596e+00)	Acc@1  62.00 ( 71.77)	Acc@5  92.00 ( 91.52)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3416e+00 (1.3516e+00)	Acc@1  71.00 ( 71.85)	Acc@5  94.00 ( 91.90)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3948e+00 (1.3661e+00)	Acc@1  72.00 ( 71.73)	Acc@5  89.00 ( 91.73)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0748e+00 (1.3317e+00)	Acc@1  75.00 ( 72.11)	Acc@5  93.00 ( 92.03)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.7022e+00 (1.3331e+00)	Acc@1  67.00 ( 72.03)	Acc@5  90.00 ( 92.18)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3341e+00 (1.3308e+00)	Acc@1  75.00 ( 72.06)	Acc@5  90.00 ( 92.12)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8302e+00 (1.3156e+00)	Acc@1  68.00 ( 72.32)	Acc@5  91.00 ( 92.27)
 * Acc@1 72.590 Acc@5 92.340
### epoch[76] execution time: 21.130502939224243
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.194 ( 0.194)	Data  0.154 ( 0.154)	Loss 2.7950e-02 (2.7950e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.8495e-02 (2.2121e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 2.8288e-02 (2.2071e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.1305e-02 (2.2165e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.5146e-02 (2.1815e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.9380e-02 (2.1057e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.8542e-02 (2.0757e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6065e-02 (2.0608e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.0118e-02 (2.0791e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.6927e-02 (2.1726e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4417e-02 (2.2215e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.7169e-02 (2.1742e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.5371e-03 (2.1596e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6617e-02 (2.1802e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4866e-02 (2.1569e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3296e-02 (2.1963e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9647e-02 (2.1883e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4563e-02 (2.1888e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5728e-02 (2.1828e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5513e-02 (2.1765e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0664e-02 (2.1686e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2716e-02 (2.1660e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5416e-02 (2.1819e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.5773e-02 (2.1800e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1856e-02 (2.1656e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7281e-02 (2.1538e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5683e-02 (2.1520e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0659e-02 (2.1538e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0982e-03 (2.1299e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3424e-02 (2.1462e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9906e-02 (2.1642e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9459e-02 (2.1564e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2000e-02 (2.1512e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3863e-02 (2.1516e-02)	Acc@1  97.66 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1236e-02 (2.1482e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5490e-02 (2.1490e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8215e-02 (2.1401e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4095e-02 (2.1450e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7597e-02 (2.1496e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0819e-02 (2.1494e-02)	Acc@1  98.75 ( 99.75)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.13107919692993164
## e[77]       loss.backward (sum) time: 2.5491158962249756
## e[77]      optimizer.step (sum) time: 1.0166232585906982
## epoch[77] training(only) time: 18.634087562561035
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.3173e+00 (1.3173e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5912e+00 (1.4054e+00)	Acc@1  66.00 ( 71.18)	Acc@5  90.00 ( 90.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3711e+00 (1.3279e+00)	Acc@1  71.00 ( 72.05)	Acc@5  93.00 ( 91.95)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8194e+00 (1.3523e+00)	Acc@1  61.00 ( 71.77)	Acc@5  92.00 ( 91.71)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3544e+00 (1.3467e+00)	Acc@1  70.00 ( 71.85)	Acc@5  95.00 ( 92.12)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4213e+00 (1.3624e+00)	Acc@1  72.00 ( 71.78)	Acc@5  87.00 ( 91.84)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0705e+00 (1.3289e+00)	Acc@1  74.00 ( 72.30)	Acc@5  93.00 ( 92.05)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6824e+00 (1.3301e+00)	Acc@1  66.00 ( 72.14)	Acc@5  90.00 ( 92.25)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3384e+00 (1.3273e+00)	Acc@1  77.00 ( 72.17)	Acc@5  89.00 ( 92.19)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8562e+00 (1.3125e+00)	Acc@1  68.00 ( 72.46)	Acc@5  90.00 ( 92.32)
 * Acc@1 72.670 Acc@5 92.420
### epoch[77] execution time: 21.118677854537964
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.188 ( 0.188)	Data  0.147 ( 0.147)	Loss 2.8201e-02 (2.8201e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.0596e-02 (2.0145e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.2069e-02 (1.8790e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.8168e-02 (1.9591e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.4299e-02 (1.9477e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.7209e-02 (2.0163e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.2136e-02 (2.0703e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.7369e-02 (2.1118e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.7327e-02 (2.1403e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7866e-02 (2.1303e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3183e-02 (2.1497e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.4662e-03 (2.1257e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0688e-02 (2.0926e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.4648e-03 (2.0782e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5506e-02 (2.0702e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0331e-02 (2.0808e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.7679e-02 (2.0954e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8765e-02 (2.0938e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7306e-02 (2.0950e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9772e-02 (2.0962e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6797e-02 (2.0862e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3620e-02 (2.0889e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.6165e-03 (2.0765e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1797e-02 (2.0875e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8935e-02 (2.0898e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0009e-02 (2.0839e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7159e-02 (2.0945e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7291e-02 (2.0727e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5872e-02 (2.0676e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9306e-02 (2.0687e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1674e-02 (2.0712e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9145e-02 (2.0618e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7051e-02 (2.0639e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7730e-02 (2.0600e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4665e-02 (2.0573e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.6799e-03 (2.0562e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9385e-02 (2.0528e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7569e-02 (2.0437e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5494e-02 (2.0519e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0237e-02 (2.0616e-02)	Acc@1  98.75 ( 99.78)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.12943124771118164
## e[78]       loss.backward (sum) time: 2.543501138687134
## e[78]      optimizer.step (sum) time: 1.027496337890625
## epoch[78] training(only) time: 18.615499258041382
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.3249e+00 (1.3249e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5492e+00 (1.3954e+00)	Acc@1  67.00 ( 71.55)	Acc@5  91.00 ( 90.73)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3258e+00 (1.3137e+00)	Acc@1  72.00 ( 72.67)	Acc@5  93.00 ( 91.71)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8461e+00 (1.3459e+00)	Acc@1  61.00 ( 72.16)	Acc@5  90.00 ( 91.55)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3380e+00 (1.3384e+00)	Acc@1  71.00 ( 72.12)	Acc@5  94.00 ( 92.00)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3969e+00 (1.3539e+00)	Acc@1  72.00 ( 71.86)	Acc@5  87.00 ( 91.71)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0694e+00 (1.3204e+00)	Acc@1  77.00 ( 72.44)	Acc@5  93.00 ( 91.97)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.7168e+00 (1.3224e+00)	Acc@1  67.00 ( 72.23)	Acc@5  88.00 ( 92.14)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3678e+00 (1.3189e+00)	Acc@1  74.00 ( 72.23)	Acc@5  89.00 ( 92.10)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8279e+00 (1.3045e+00)	Acc@1  68.00 ( 72.48)	Acc@5  91.00 ( 92.23)
 * Acc@1 72.670 Acc@5 92.330
### epoch[78] execution time: 21.10759973526001
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.191 ( 0.191)	Data  0.151 ( 0.151)	Loss 1.1543e-02 (1.1543e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.7224e-02 (1.9323e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.5335e-02 (2.1421e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.4713e-02 (2.2662e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.8029e-02 (2.1449e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.6732e-02 (2.0705e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.3044e-02 (2.0280e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.8799e-02 (2.0645e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.6497e-02 (2.0636e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.0647e-03 (2.0372e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2668e-02 (2.0386e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6328e-02 (2.0217e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1633e-02 (2.0141e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6877e-02 (2.0197e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7753e-02 (2.0348e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7140e-02 (2.0209e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8227e-02 (2.0320e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3595e-02 (2.0415e-02)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3749e-02 (2.0261e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4451e-02 (2.0329e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0717e-02 (2.0535e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1823e-02 (2.0684e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2279e-02 (2.0604e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1898e-02 (2.0877e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9639e-02 (2.0893e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8542e-02 (2.0821e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8182e-02 (2.0758e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0033e-02 (2.0789e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9119e-02 (2.0698e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8128e-02 (2.0774e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2365e-02 (2.0842e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0743e-02 (2.0899e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4105e-02 (2.0789e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3312e-02 (2.0889e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2332e-02 (2.0976e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8518e-02 (2.0928e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6916e-02 (2.0867e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8832e-02 (2.0777e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4709e-02 (2.0866e-02)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4526e-02 (2.0895e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.1304302215576172
## e[79]       loss.backward (sum) time: 2.5585076808929443
## e[79]      optimizer.step (sum) time: 1.0144169330596924
## epoch[79] training(only) time: 18.597516298294067
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.2954e+00 (1.2954e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5691e+00 (1.3990e+00)	Acc@1  66.00 ( 71.00)	Acc@5  91.00 ( 91.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3544e+00 (1.3236e+00)	Acc@1  71.00 ( 72.19)	Acc@5  94.00 ( 91.95)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8134e+00 (1.3531e+00)	Acc@1  63.00 ( 71.81)	Acc@5  90.00 ( 91.68)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3383e+00 (1.3446e+00)	Acc@1  71.00 ( 71.83)	Acc@5  95.00 ( 92.12)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3776e+00 (1.3583e+00)	Acc@1  70.00 ( 71.59)	Acc@5  88.00 ( 91.86)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0733e+00 (1.3243e+00)	Acc@1  76.00 ( 72.16)	Acc@5  93.00 ( 92.11)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.7172e+00 (1.3274e+00)	Acc@1  65.00 ( 72.03)	Acc@5  90.00 ( 92.25)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3399e+00 (1.3237e+00)	Acc@1  76.00 ( 72.14)	Acc@5  90.00 ( 92.27)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 1.8439e+00 (1.3090e+00)	Acc@1  67.00 ( 72.32)	Acc@5  91.00 ( 92.38)
 * Acc@1 72.550 Acc@5 92.470
### epoch[79] execution time: 21.09773302078247
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.183 ( 0.183)	Data  0.143 ( 0.143)	Loss 1.3442e-02 (1.3442e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.1858e-02 (2.0946e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.4546e-02 (1.9225e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.4415e-02 (1.9969e-02)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.1432e-02 (1.9482e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.4079e-02 (2.0450e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.9345e-02 (2.1492e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7629e-02 (2.0798e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.4770e-02 (2.1344e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.2728e-02 (2.1220e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6271e-02 (2.0964e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6044e-02 (2.0809e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8434e-02 (2.0812e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6855e-02 (2.0673e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7228e-02 (2.0757e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0292e-02 (2.0791e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.4019e-02 (2.1152e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2397e-02 (2.1247e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8389e-02 (2.1158e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1362e-02 (2.0943e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2673e-02 (2.0782e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8046e-02 (2.0778e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9740e-02 (2.0827e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0632e-02 (2.0790e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6244e-02 (2.0629e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.3690e-03 (2.0559e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7689e-02 (2.0634e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6513e-02 (2.0587e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1593e-02 (2.0572e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3346e-02 (2.0717e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2628e-02 (2.0700e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7725e-02 (2.0697e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9027e-02 (2.0700e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6751e-02 (2.0672e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5974e-02 (2.0671e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6482e-02 (2.0695e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6595e-02 (2.0671e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5339e-02 (2.0631e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7541e-02 (2.0614e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9368e-02 (2.0681e-02)	Acc@1  98.75 ( 99.77)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.12919974327087402
## e[80]       loss.backward (sum) time: 2.543124198913574
## e[80]      optimizer.step (sum) time: 1.012413740158081
## epoch[80] training(only) time: 18.616716384887695
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.3180e+00 (1.3180e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.5767e+00 (1.4106e+00)	Acc@1  66.00 ( 70.82)	Acc@5  92.00 ( 91.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3477e+00 (1.3310e+00)	Acc@1  71.00 ( 72.14)	Acc@5  93.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8397e+00 (1.3551e+00)	Acc@1  60.00 ( 71.65)	Acc@5  91.00 ( 91.68)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3424e+00 (1.3446e+00)	Acc@1  73.00 ( 71.85)	Acc@5  95.00 ( 92.07)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4106e+00 (1.3612e+00)	Acc@1  71.00 ( 71.55)	Acc@5  88.00 ( 91.80)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0825e+00 (1.3274e+00)	Acc@1  75.00 ( 72.02)	Acc@5  93.00 ( 92.03)
Test: [ 70/100]	Time  0.024 ( 0.025)	Loss 1.7006e+00 (1.3292e+00)	Acc@1  67.00 ( 71.86)	Acc@5  91.00 ( 92.18)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3269e+00 (1.3255e+00)	Acc@1  75.00 ( 71.93)	Acc@5  90.00 ( 92.17)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8190e+00 (1.3101e+00)	Acc@1  67.00 ( 72.20)	Acc@5  92.00 ( 92.31)
 * Acc@1 72.430 Acc@5 92.410
### epoch[80] execution time: 21.105756998062134
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.188 ( 0.188)	Data  0.145 ( 0.145)	Loss 1.0302e-02 (1.0302e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.015)	Loss 3.6227e-02 (1.9342e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.2814e-02 (1.9917e-02)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.5987e-02 (1.9922e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.3887e-02 (1.9673e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.0257e-02 (1.9496e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.3884e-02 (2.0069e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4640e-02 (2.0889e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.048 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.9773e-02 (2.0816e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5906e-02 (2.0573e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.0541e-02 (2.0876e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1460e-02 (2.1188e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5106e-02 (2.1052e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4300e-02 (2.1035e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2088e-02 (2.0841e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0169e-02 (2.0616e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.049 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5176e-02 (2.0427e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9941e-02 (2.0361e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1718e-02 (2.0454e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2322e-02 (2.0525e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8920e-02 (2.0363e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.043 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1961e-02 (2.0397e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7148e-02 (2.0411e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2360e-02 (2.0397e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0617e-02 (2.0517e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5093e-02 (2.0632e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6796e-02 (2.0544e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1907e-02 (2.0704e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0179e-02 (2.0774e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4488e-02 (2.0784e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0344e-02 (2.0936e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0729e-02 (2.1073e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2267e-02 (2.1134e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8729e-02 (2.1170e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3800e-02 (2.1139e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9346e-02 (2.1040e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1128e-02 (2.1127e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0002e-02 (2.1174e-02)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0712e-02 (2.1140e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8271e-02 (2.1089e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.1294395923614502
## e[81]       loss.backward (sum) time: 2.5508058071136475
## e[81]      optimizer.step (sum) time: 1.0154094696044922
## epoch[81] training(only) time: 18.61204957962036
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.2828e+00 (1.2828e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5769e+00 (1.4008e+00)	Acc@1  66.00 ( 70.91)	Acc@5  91.00 ( 90.82)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3604e+00 (1.3274e+00)	Acc@1  72.00 ( 71.81)	Acc@5  93.00 ( 91.90)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7893e+00 (1.3512e+00)	Acc@1  60.00 ( 71.35)	Acc@5  91.00 ( 91.65)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3437e+00 (1.3456e+00)	Acc@1  72.00 ( 71.68)	Acc@5  94.00 ( 92.10)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4249e+00 (1.3613e+00)	Acc@1  72.00 ( 71.55)	Acc@5  87.00 ( 91.82)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0489e+00 (1.3273e+00)	Acc@1  74.00 ( 72.08)	Acc@5  93.00 ( 92.10)
Test: [ 70/100]	Time  0.027 ( 0.025)	Loss 1.6995e+00 (1.3299e+00)	Acc@1  65.00 ( 71.97)	Acc@5  90.00 ( 92.24)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3238e+00 (1.3267e+00)	Acc@1  75.00 ( 72.05)	Acc@5  90.00 ( 92.17)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8676e+00 (1.3127e+00)	Acc@1  69.00 ( 72.35)	Acc@5  90.00 ( 92.29)
 * Acc@1 72.510 Acc@5 92.400
### epoch[81] execution time: 21.126356601715088
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.180 ( 0.180)	Data  0.132 ( 0.132)	Loss 1.6369e-02 (1.6369e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.014)	Loss 2.7095e-02 (2.0660e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.008)	Loss 2.5824e-02 (2.1527e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.9892e-02 (2.2195e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.9171e-02 (2.1258e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.0663e-02 (2.1439e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.7668e-02 (2.1535e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.9015e-02 (2.1155e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.3795e-02 (2.1770e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4496e-02 (2.1367e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1919e-02 (2.1222e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2781e-02 (2.1229e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4553e-02 (2.1448e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0918e-02 (2.1622e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4995e-02 (2.1627e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.4477e-02 (2.1606e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1080e-02 (2.1332e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8645e-02 (2.1431e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.7795e-02 (2.1304e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5181e-02 (2.1192e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9048e-02 (2.1121e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8049e-02 (2.0801e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8872e-02 (2.0665e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1740e-02 (2.0659e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8592e-02 (2.0789e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0563e-02 (2.0919e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4300e-02 (2.0936e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8144e-02 (2.1083e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.0671e-03 (2.1089e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6342e-02 (2.1126e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0633e-03 (2.1129e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4069e-02 (2.1229e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6872e-02 (2.1183e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6440e-02 (2.1169e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9632e-02 (2.1213e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8784e-02 (2.1111e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3123e-03 (2.0935e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0420e-02 (2.0953e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9293e-02 (2.0925e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7049e-02 (2.0910e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.13014507293701172
## e[82]       loss.backward (sum) time: 2.54986572265625
## e[82]      optimizer.step (sum) time: 1.0160932540893555
## epoch[82] training(only) time: 18.633055448532104
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.2978e+00 (1.2978e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5733e+00 (1.3963e+00)	Acc@1  67.00 ( 71.36)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3590e+00 (1.3154e+00)	Acc@1  72.00 ( 72.57)	Acc@5  92.00 ( 92.05)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8490e+00 (1.3460e+00)	Acc@1  60.00 ( 72.10)	Acc@5  91.00 ( 91.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3400e+00 (1.3383e+00)	Acc@1  73.00 ( 72.22)	Acc@5  96.00 ( 92.22)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4008e+00 (1.3541e+00)	Acc@1  71.00 ( 71.94)	Acc@5  89.00 ( 91.90)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0673e+00 (1.3203e+00)	Acc@1  75.00 ( 72.44)	Acc@5  93.00 ( 92.13)
Test: [ 70/100]	Time  0.024 ( 0.024)	Loss 1.6485e+00 (1.3224e+00)	Acc@1  66.00 ( 72.28)	Acc@5  90.00 ( 92.32)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3333e+00 (1.3194e+00)	Acc@1  75.00 ( 72.35)	Acc@5  89.00 ( 92.21)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8169e+00 (1.3043e+00)	Acc@1  68.00 ( 72.57)	Acc@5  90.00 ( 92.33)
 * Acc@1 72.760 Acc@5 92.380
### epoch[82] execution time: 21.119903087615967
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.183 ( 0.183)	Data  0.144 ( 0.144)	Loss 1.4067e-02 (1.4067e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.049 ( 0.059)	Data  0.001 ( 0.015)	Loss 1.2799e-02 (1.8017e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.2391e-02 (1.9895e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.045 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.1178e-02 (2.0602e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.8530e-02 (2.0622e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.9374e-02 (2.0325e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.3803e-02 (2.0029e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6216e-02 (2.0146e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.2233e-02 (2.0316e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2800e-02 (2.0076e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.1317e-03 (2.0078e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2395e-02 (2.0129e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3569e-02 (2.0308e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9229e-02 (2.0571e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8986e-02 (2.0568e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2407e-02 (2.0330e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.9315e-02 (2.0234e-02)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2531e-02 (2.0243e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7643e-02 (2.0266e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6833e-02 (2.0355e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8555e-02 (2.0539e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3745e-02 (2.0493e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1470e-02 (2.0297e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9650e-02 (2.0169e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2539e-02 (2.0177e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5437e-02 (2.0279e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8180e-02 (2.0215e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4137e-02 (2.0134e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9553e-02 (2.0098e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4522e-02 (2.0078e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5664e-02 (2.0120e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0642e-02 (2.0108e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7811e-02 (2.0242e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3005e-02 (2.0284e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0416e-02 (2.0352e-02)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0678e-02 (2.0424e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5591e-02 (2.0363e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7353e-02 (2.0305e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3382e-02 (2.0191e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8463e-02 (2.0120e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.1321091651916504
## e[83]       loss.backward (sum) time: 2.5514795780181885
## e[83]      optimizer.step (sum) time: 1.0127146244049072
## epoch[83] training(only) time: 18.621268033981323
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3159e+00 (1.3159e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5502e+00 (1.3921e+00)	Acc@1  66.00 ( 71.55)	Acc@5  90.00 ( 91.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3612e+00 (1.3180e+00)	Acc@1  72.00 ( 72.38)	Acc@5  92.00 ( 91.86)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7925e+00 (1.3436e+00)	Acc@1  61.00 ( 72.03)	Acc@5  91.00 ( 91.65)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3258e+00 (1.3352e+00)	Acc@1  72.00 ( 72.07)	Acc@5  96.00 ( 92.10)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3959e+00 (1.3507e+00)	Acc@1  71.00 ( 71.78)	Acc@5  87.00 ( 91.86)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0642e+00 (1.3162e+00)	Acc@1  75.00 ( 72.41)	Acc@5  93.00 ( 92.11)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6692e+00 (1.3191e+00)	Acc@1  64.00 ( 72.23)	Acc@5  89.00 ( 92.30)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3250e+00 (1.3158e+00)	Acc@1  76.00 ( 72.26)	Acc@5  90.00 ( 92.25)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8009e+00 (1.3012e+00)	Acc@1  68.00 ( 72.45)	Acc@5  90.00 ( 92.36)
 * Acc@1 72.680 Acc@5 92.440
### epoch[83] execution time: 21.101770162582397
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.190 ( 0.190)	Data  0.150 ( 0.150)	Loss 2.4547e-02 (2.4547e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.6622e-02 (2.0215e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 2.2939e-02 (1.9820e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.4868e-02 (1.9114e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.4918e-02 (2.0690e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.053 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.8702e-02 (2.0528e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.0060e-02 (2.1499e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.6226e-02 (2.1485e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.0444e-02 (2.1273e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0721e-02 (2.0819e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0393e-02 (2.0972e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.2071e-02 (2.0859e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8930e-02 (2.0749e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1252e-02 (2.0823e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9979e-02 (2.0888e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0907e-02 (2.0970e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.5234e-02 (2.1040e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4389e-02 (2.1046e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4057e-02 (2.0934e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8988e-02 (2.0992e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0599e-02 (2.0984e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4678e-02 (2.0764e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3141e-02 (2.0741e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4709e-02 (2.0627e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6365e-02 (2.0581e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7901e-02 (2.0364e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1917e-03 (2.0358e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6736e-02 (2.0503e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3182e-02 (2.0403e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1343e-02 (2.0544e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9192e-02 (2.0570e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3194e-02 (2.0532e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1594e-02 (2.0459e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.9985e-03 (2.0354e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4141e-02 (2.0283e-02)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6847e-02 (2.0260e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3568e-02 (2.0287e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1148e-02 (2.0267e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2612e-02 (2.0290e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8346e-02 (2.0278e-02)	Acc@1  98.75 ( 99.79)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.13042473793029785
## e[84]       loss.backward (sum) time: 2.5486178398132324
## e[84]      optimizer.step (sum) time: 1.0119385719299316
## epoch[84] training(only) time: 18.61753797531128
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.2957e+00 (1.2957e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5799e+00 (1.4086e+00)	Acc@1  65.00 ( 70.82)	Acc@5  91.00 ( 90.91)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3274e+00 (1.3281e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 91.86)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7979e+00 (1.3561e+00)	Acc@1  61.00 ( 71.58)	Acc@5  91.00 ( 91.61)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3211e+00 (1.3464e+00)	Acc@1  71.00 ( 71.80)	Acc@5  95.00 ( 92.10)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4220e+00 (1.3625e+00)	Acc@1  72.00 ( 71.59)	Acc@5  87.00 ( 91.92)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0610e+00 (1.3279e+00)	Acc@1  73.00 ( 72.03)	Acc@5  93.00 ( 92.21)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6998e+00 (1.3288e+00)	Acc@1  66.00 ( 71.93)	Acc@5  90.00 ( 92.34)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3138e+00 (1.3252e+00)	Acc@1  74.00 ( 71.90)	Acc@5  90.00 ( 92.28)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8723e+00 (1.3126e+00)	Acc@1  67.00 ( 72.16)	Acc@5  91.00 ( 92.37)
 * Acc@1 72.410 Acc@5 92.470
### epoch[84] execution time: 21.107424020767212
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.184 ( 0.184)	Data  0.142 ( 0.142)	Loss 1.5943e-02 (1.5943e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 3.4711e-02 (2.0550e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.9892e-02 (2.0356e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.8864e-02 (2.1173e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.1066e-02 (2.1070e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.4916e-02 (2.0963e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.5378e-02 (2.1536e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.8785e-02 (2.1400e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.4753e-02 (2.1443e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4620e-02 (2.1500e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4302e-02 (2.1240e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9204e-02 (2.1234e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0478e-02 (2.0927e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5786e-02 (2.1422e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3300e-02 (2.1046e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.8734e-03 (2.1050e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.7031e-02 (2.1210e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0021e-02 (2.1078e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0407e-02 (2.1175e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.0217e-02 (2.1245e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2221e-02 (2.1150e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8699e-02 (2.1091e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.9780e-02 (2.1200e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0558e-02 (2.1167e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5638e-02 (2.1011e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2807e-02 (2.1149e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2072e-02 (2.1311e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8588e-02 (2.1211e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4085e-02 (2.1403e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6517e-02 (2.1379e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4154e-02 (2.1365e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2551e-02 (2.1265e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9557e-02 (2.1162e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7371e-02 (2.1066e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6681e-02 (2.1102e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0969e-02 (2.1077e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3716e-02 (2.1055e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7802e-02 (2.0986e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4311e-02 (2.1032e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2293e-02 (2.1044e-02)	Acc@1  98.75 ( 99.75)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.12903404235839844
## e[85]       loss.backward (sum) time: 2.5367178916931152
## e[85]      optimizer.step (sum) time: 1.0098216533660889
## epoch[85] training(only) time: 18.606187343597412
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3398e+00 (1.3398e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.5746e+00 (1.4103e+00)	Acc@1  66.00 ( 71.45)	Acc@5  91.00 ( 90.82)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3668e+00 (1.3317e+00)	Acc@1  71.00 ( 72.29)	Acc@5  93.00 ( 91.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8045e+00 (1.3591e+00)	Acc@1  63.00 ( 71.81)	Acc@5  91.00 ( 91.61)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3469e+00 (1.3500e+00)	Acc@1  71.00 ( 71.95)	Acc@5  95.00 ( 92.05)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4127e+00 (1.3648e+00)	Acc@1  71.00 ( 71.76)	Acc@5  87.00 ( 91.75)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.0926e+00 (1.3303e+00)	Acc@1  75.00 ( 72.33)	Acc@5  93.00 ( 92.07)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6710e+00 (1.3326e+00)	Acc@1  66.00 ( 72.20)	Acc@5  90.00 ( 92.25)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3551e+00 (1.3291e+00)	Acc@1  76.00 ( 72.22)	Acc@5  89.00 ( 92.20)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8319e+00 (1.3146e+00)	Acc@1  67.00 ( 72.42)	Acc@5  91.00 ( 92.35)
 * Acc@1 72.610 Acc@5 92.430
### epoch[85] execution time: 21.093003511428833
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.181 ( 0.181)	Data  0.142 ( 0.142)	Loss 1.4210e-02 (1.4210e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 1.2740e-02 (1.6664e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.8416e-02 (1.8119e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.5609e-02 (1.8170e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.5658e-02 (1.8889e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.9349e-02 (1.9194e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.6467e-02 (1.9171e-02)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4642e-02 (1.9664e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4764e-02 (1.9441e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.7952e-03 (1.9349e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6434e-02 (1.9578e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2844e-02 (1.9698e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6481e-02 (1.9776e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4576e-02 (1.9844e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1296e-02 (2.0153e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.0267e-02 (2.0179e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.3801e-02 (2.0232e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4901e-02 (2.0392e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8174e-02 (2.0330e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4510e-02 (2.0291e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3480e-02 (2.0256e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3299e-02 (2.0258e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0648e-02 (2.0404e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.8020e-02 (2.0349e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7609e-02 (2.0503e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0853e-02 (2.0438e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1422e-02 (2.0487e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8346e-02 (2.0519e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3306e-02 (2.0424e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0199e-02 (2.0573e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3650e-02 (2.0473e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9979e-02 (2.0400e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4183e-02 (2.0353e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2530e-02 (2.0339e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7033e-02 (2.0245e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4593e-02 (2.0275e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8679e-02 (2.0235e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0096e-02 (2.0165e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6351e-02 (2.0098e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2425e-02 (2.0057e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.13100743293762207
## e[86]       loss.backward (sum) time: 2.5629115104675293
## e[86]      optimizer.step (sum) time: 1.0219824314117432
## epoch[86] training(only) time: 18.6191143989563
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 1.2732e+00 (1.2732e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.037)	Loss 1.5550e+00 (1.4010e+00)	Acc@1  65.00 ( 71.27)	Acc@5  91.00 ( 91.09)
Test: [ 20/100]	Time  0.022 ( 0.030)	Loss 1.3563e+00 (1.3214e+00)	Acc@1  72.00 ( 72.14)	Acc@5  94.00 ( 92.05)
Test: [ 30/100]	Time  0.022 ( 0.028)	Loss 1.7502e+00 (1.3452e+00)	Acc@1  60.00 ( 71.61)	Acc@5  92.00 ( 91.71)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3292e+00 (1.3366e+00)	Acc@1  71.00 ( 71.71)	Acc@5  94.00 ( 92.10)
Test: [ 50/100]	Time  0.023 ( 0.026)	Loss 1.4189e+00 (1.3522e+00)	Acc@1  72.00 ( 71.63)	Acc@5  88.00 ( 91.80)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0379e+00 (1.3183e+00)	Acc@1  75.00 ( 72.11)	Acc@5  93.00 ( 92.10)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6632e+00 (1.3213e+00)	Acc@1  65.00 ( 71.86)	Acc@5  89.00 ( 92.24)
Test: [ 80/100]	Time  0.022 ( 0.025)	Loss 1.3410e+00 (1.3184e+00)	Acc@1  73.00 ( 71.90)	Acc@5  90.00 ( 92.20)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8864e+00 (1.3044e+00)	Acc@1  67.00 ( 72.25)	Acc@5  91.00 ( 92.35)
 * Acc@1 72.470 Acc@5 92.460
### epoch[86] execution time: 21.12583303451538
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.187 ( 0.187)	Data  0.145 ( 0.145)	Loss 2.1905e-02 (2.1905e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 1.7705e-02 (1.9414e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 2.6487e-02 (2.0431e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.1046e-02 (1.9667e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.8369e-02 (1.9533e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 2.7358e-02 (1.9813e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.8677e-02 (1.9898e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6330e-02 (1.9890e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.4140e-02 (1.9874e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0661e-02 (1.9862e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4306e-02 (1.9992e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1260e-02 (2.0186e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0687e-02 (2.0023e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1487e-02 (1.9909e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2515e-02 (1.9713e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.8610e-02 (1.9912e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8704e-02 (2.0024e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5342e-02 (2.0187e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.9863e-02 (2.0082e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0052e-02 (1.9964e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.4118e-02 (1.9846e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.0835e-02 (1.9888e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6787e-02 (1.9969e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.9085e-02 (2.0173e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.2989e-02 (2.0161e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6195e-02 (2.0240e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9130e-02 (2.0060e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3801e-02 (1.9972e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5776e-02 (2.0012e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9822e-02 (2.0060e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5873e-02 (2.0000e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1296e-02 (1.9907e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8392e-02 (1.9984e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1978e-02 (2.0052e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9732e-02 (2.0199e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8617e-02 (2.0197e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1259e-02 (2.0208e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4219e-02 (2.0219e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3056e-02 (2.0326e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5263e-02 (2.0313e-02)	Acc@1  98.75 ( 99.77)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.13077473640441895
## e[87]       loss.backward (sum) time: 2.5402400493621826
## e[87]      optimizer.step (sum) time: 1.0219407081604004
## epoch[87] training(only) time: 18.623943567276
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3034e+00 (1.3034e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5659e+00 (1.4089e+00)	Acc@1  68.00 ( 71.91)	Acc@5  91.00 ( 91.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3354e+00 (1.3273e+00)	Acc@1  72.00 ( 72.81)	Acc@5  92.00 ( 91.76)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8313e+00 (1.3552e+00)	Acc@1  59.00 ( 72.29)	Acc@5  92.00 ( 91.71)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3808e+00 (1.3489e+00)	Acc@1  69.00 ( 72.29)	Acc@5  95.00 ( 92.17)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4177e+00 (1.3640e+00)	Acc@1  72.00 ( 72.06)	Acc@5  87.00 ( 91.92)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0644e+00 (1.3302e+00)	Acc@1  76.00 ( 72.54)	Acc@5  93.00 ( 92.11)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.7227e+00 (1.3322e+00)	Acc@1  65.00 ( 72.32)	Acc@5  91.00 ( 92.27)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.3464e+00 (1.3297e+00)	Acc@1  76.00 ( 72.33)	Acc@5  89.00 ( 92.22)
Test: [ 90/100]	Time  0.024 ( 0.024)	Loss 1.8614e+00 (1.3152e+00)	Acc@1  66.00 ( 72.60)	Acc@5  90.00 ( 92.33)
 * Acc@1 72.770 Acc@5 92.410
### epoch[87] execution time: 21.127744674682617
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.196 ( 0.196)	Data  0.156 ( 0.156)	Loss 2.0139e-02 (2.0139e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 3.3284e-02 (2.1942e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.047 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.4089e-02 (1.8863e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 2.6633e-02 (2.0268e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.048 ( 0.051)	Data  0.001 ( 0.006)	Loss 2.0700e-02 (2.1109e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.1053e-02 (2.0829e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.5995e-02 (2.1482e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7925e-02 (2.1668e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.0789e-02 (2.1162e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 2.2030e-02 (2.1222e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0438e-02 (2.0817e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6468e-02 (2.0538e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6102e-02 (2.1024e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3748e-02 (2.1362e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4364e-02 (2.1188e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7897e-02 (2.1238e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3904e-02 (2.1116e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.9605e-02 (2.1067e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.8200e-03 (2.0845e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6386e-02 (2.0843e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2713e-02 (2.0720e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2000e-02 (2.0623e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 9.7552e-03 (2.0525e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4037e-02 (2.0386e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.1768e-02 (2.0348e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.6564e-02 (2.0248e-02)	Acc@1  98.44 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2478e-02 (2.0094e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6695e-02 (1.9966e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.6977e-02 (2.0012e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.4275e-02 (1.9952e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.0137e-02 (1.9891e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1455e-02 (1.9912e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2147e-02 (1.9924e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8274e-02 (1.9821e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7390e-02 (1.9868e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6411e-02 (1.9790e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6314e-02 (1.9732e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9428e-02 (1.9615e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9445e-02 (1.9781e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8134e-02 (1.9818e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.1301252841949463
## e[88]       loss.backward (sum) time: 2.5700345039367676
## e[88]      optimizer.step (sum) time: 1.0021965503692627
## epoch[88] training(only) time: 18.67051100730896
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3172e+00 (1.3172e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.5591e+00 (1.4028e+00)	Acc@1  66.00 ( 71.55)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3520e+00 (1.3235e+00)	Acc@1  72.00 ( 72.29)	Acc@5  93.00 ( 92.19)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7970e+00 (1.3495e+00)	Acc@1  61.00 ( 71.90)	Acc@5  90.00 ( 91.81)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3446e+00 (1.3403e+00)	Acc@1  72.00 ( 72.12)	Acc@5  96.00 ( 92.32)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4067e+00 (1.3575e+00)	Acc@1  71.00 ( 71.86)	Acc@5  87.00 ( 91.96)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0629e+00 (1.3238e+00)	Acc@1  76.00 ( 72.36)	Acc@5  93.00 ( 92.20)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6818e+00 (1.3265e+00)	Acc@1  65.00 ( 72.20)	Acc@5  90.00 ( 92.37)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3320e+00 (1.3244e+00)	Acc@1  76.00 ( 72.21)	Acc@5  89.00 ( 92.30)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8524e+00 (1.3101e+00)	Acc@1  68.00 ( 72.46)	Acc@5  91.00 ( 92.44)
 * Acc@1 72.670 Acc@5 92.490
### epoch[88] execution time: 21.179426908493042
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.190 ( 0.190)	Data  0.150 ( 0.150)	Loss 2.0531e-02 (2.0531e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.4061e-02 (1.9991e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.1008e-02 (2.0222e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.047 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.4678e-02 (1.9538e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.0323e-02 (1.9977e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.005)	Loss 1.3829e-02 (2.0177e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.2665e-02 (2.0692e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.3628e-02 (2.0138e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.8174e-02 (1.9833e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.004)	Loss 1.9061e-02 (1.9810e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2262e-02 (1.9843e-02)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7112e-02 (1.9686e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4066e-02 (1.9696e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4598e-02 (1.9679e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2836e-02 (1.9792e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3090e-02 (1.9710e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3207e-02 (1.9704e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2229e-02 (1.9672e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3119e-02 (1.9564e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.8531e-02 (1.9650e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3679e-02 (1.9713e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 2.2463e-02 (1.9761e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.3567e-02 (1.9664e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.3396e-02 (1.9731e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.2915e-02 (1.9787e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.1271e-02 (1.9593e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.5234e-02 (1.9643e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9680e-02 (1.9662e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8539e-02 (1.9789e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2711e-02 (1.9782e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5308e-02 (1.9815e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8449e-02 (1.9785e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3695e-02 (1.9832e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3887e-02 (1.9775e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2213e-02 (1.9745e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7519e-02 (1.9748e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9788e-02 (1.9738e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3476e-02 (1.9726e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1657e-02 (1.9743e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7602e-02 (1.9603e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.1314840316772461
## e[89]       loss.backward (sum) time: 2.55147123336792
## e[89]      optimizer.step (sum) time: 1.0085806846618652
## epoch[89] training(only) time: 18.636823892593384
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.3071e+00 (1.3071e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5770e+00 (1.3992e+00)	Acc@1  66.00 ( 71.27)	Acc@5  92.00 ( 91.36)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3791e+00 (1.3234e+00)	Acc@1  72.00 ( 72.19)	Acc@5  93.00 ( 92.05)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7938e+00 (1.3477e+00)	Acc@1  64.00 ( 71.94)	Acc@5  91.00 ( 91.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3529e+00 (1.3413e+00)	Acc@1  72.00 ( 71.95)	Acc@5  95.00 ( 92.15)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3717e+00 (1.3541e+00)	Acc@1  72.00 ( 71.75)	Acc@5  88.00 ( 91.82)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.0664e+00 (1.3191e+00)	Acc@1  75.00 ( 72.31)	Acc@5  93.00 ( 92.03)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6722e+00 (1.3218e+00)	Acc@1  64.00 ( 72.07)	Acc@5  90.00 ( 92.21)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3227e+00 (1.3184e+00)	Acc@1  75.00 ( 72.10)	Acc@5  90.00 ( 92.12)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8338e+00 (1.3040e+00)	Acc@1  68.00 ( 72.38)	Acc@5  90.00 ( 92.26)
 * Acc@1 72.600 Acc@5 92.370
### epoch[89] execution time: 21.129544973373413
### Training complete:
#### total training(only) time: 1678.757738828659
##### Total run time: 1907.1716210842133
