# Model: resnet18
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: True
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.resnet
<function resnet18 at 0x7f3609773f28>
# model requested: 'resnet18'
# printing out the model
ResNet(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (conv2_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv3_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv4_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv5_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=100, bias=True)
)
# model is full precision
PARAM TOTAL COUNT: 62
PARAM TO FREEZE COUNT: 60

module.conv1.0.weight
[0.1, 0.10088384267912825, 0.10053591041744064, 0.0999578198101117, 0.09915225683538814, 0.09812296437474749, 0.09687472482237458, 0.09541333786475781, 0.09374559353364734, 0.0918792406575792, 0.0898229508585488, 0.08758627826111579, 0.08517961510114358, 0.08261414344042829, 0.07990178321156519, 0.0770551368344512, 0.07408743066175172, 0.07101245352539297, 0.06784449266961103, 0.06459826736823124, 0.06128886053461177, 0.05793164864201136, 0.054542230279991735, 0.05113635367880273, 0.04772984353849288, 0.044338527502719036, 0.040978162618878995, 0.03766436212625511, 0.034412522912332426, 0.031237753974350746, 0.028154806218479014, 0.025178003922785554, 0.022321178182447728, 0.019597602646433312, 0.017019931844240544, 0.01460014238924829, 0.01234947733186318, 0.010278393921015021, 0.0083965150167161, 0.006712584379435493, 0.005234426044027662, 0.003968907966975204, 0.002921910115851916, 0.0020982971492716097, 0.001501895814259612, 0.0011354771660654897, 0.0010007436930289206, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv1.1.weight
[0.1, 0.1008866126508089, 0.10054696487087379, 0.09998259713065175, 0.09919606911662406, 0.09819094812185372, 0.09697179286654929, 0.09554413282201682, 0.0939144431317761, 0.0920901152435864, 0.09007942338557957, 0.08789148703854842, 0.08553622957459649, 0.08302433324974377, 0.08036719075461901, 0.07757685354298, 0.0746659771724178, 0.07164776390515169, 0.06853590282924843, 0.0653445077718459, 0.06208805328597533, 0.05878130900131329, 0.055439272636615425, 0.05207710197765345, 0.04871004612916929, 0.04535337635265247, 0.04202231680362575, 0.038731975482579624, 0.03549727571272717, 0.032332888455361704, 0.0292531657698003, 0.026272075719705325, 0.023403139021016008, 0.020659367718822798, 0.0180532061713154, 0.015596474608471252, 0.013300315521473722, 0.011175143126010655, 0.009230596128661887, 0.0074754940106037055, 0.00591779702690513, 0.004564570102839166, 0.0034219507909567675, 0.002495121434254293, 0.0017882856616880985, 0.001304649322640771, 0.0010464059468106015, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv1.1.bias
[0.1, 0.10088930588436362, 0.10055771366494393, 0.10000669155404017, 0.0992386793478628, 0.09825707762369029, 0.09706623268291323, 0.09567141730663475, 0.09407880740903701, 0.09229545469188671, 0.09032925542125961, 0.08818891546473276, 0.0858839117438521, 0.0834244502725545, 0.08082142096734027, 0.07808634942928602, 0.07523134591139606, 0.07226905169725244, 0.06921258312838677, 0.06607547352820706, 0.06287161327962713, 0.059615188321720615, 0.05632061733772155, 0.05300248791248892, 0.04967549194211448, 0.046354360581664866, 0.04305379901909438, 0.03978842136413393, 0.03657268594045284, 0.03342083126760498, 0.030346813016215324, 0.02736424221555537, 0.024486324987109228, 0.021725804070975963, 0.01909490240401588, 0.016605268999563015, 0.01426792736833592, 0.012093226708926978, 0.010090796083986858, 0.008269501785001587, 0.0066374080744407695, 0.005201741479101468, 0.003968858792748447, 0.002944218929727698, 0.002132358754178525, 0.001536872991867038, 0.0011603983135865684, 0.0010046016605999844, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.0.residual_function.0.weight
[0.1, 0.10089192479888248, 0.10056816640549378, 0.10003012442997306, 0.09928012483210885, 0.0983214098661963, 0.09715812406474557, 0.09579529632163465, 0.09423881815216055, 0.0924954182239713, 0.09057263326898157, 0.08847877550202005, 0.08622289668705777, 0.08381474900635964, 0.0812647429017222, 0.0785839020700498, 0.07578381580782433, 0.07287658891048324, 0.06987478934329154, 0.06679139390992672, 0.06363973215365228, 0.060433428733594724, 0.057186344525232725, 0.05391251669972085, 0.05062609804108576, 0.04734129576362698, 0.04407231009401523, 0.040833272883598254, 0.03763818651629374, 0.03450086337617109, 0.03143486613640388, 0.028453449127724628, 0.025569501039846496, 0.02279548920355386, 0.02014340569433112, 0.017624715490523874, 0.015250306910145256, 0.013030444540589676, 0.010974724864738838, 0.009092034775289103, 0.007390513156642847, 0.0058775157004457065, 0.004559583106872416, 0.003442412809127398, 0.0025308343433950767, 0.001828788470716202, 0.0013393101410461119, 0.001064515373141677, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.0.residual_function.1.weight
[0.1, 0.10089447172804834, 0.10057833236084063, 0.10005291636402151, 0.09932044158727903, 0.09838399990251068, 0.09724754415262754, 0.09591587146608703, 0.09439460300758575, 0.09269016025038772, 0.09080973787044525, 0.08876127337673012, 0.08655341360596935, 0.08419547822321577, 0.08169742038232201, 0.07906978471237544, 0.07632366280743878, 0.0734706464074799, 0.0705227784681203, 0.06749250232574328, 0.06439260917254248, 0.06123618406322558, 0.05803655068128584, 0.054807215097990226, 0.051561808761485056, 0.0483140309566692, 0.045077590978719696, 0.04186615026436246, 0.03869326472515994, 0.03557232752623422, 0.03251651255196423, 0.02953871879729546, 0.026651515919394315, 0.023867091179479924, 0.02119719799879957, 0.018653106345899405, 0.01624555516461207, 0.013984707043569449, 0.0118801053185859, 0.00994063378898876, 0.008174479217938756, 0.006589096775031183, 0.005191178567049325, 0.003986625389705938, 0.002980521819612169, 0.00217711475161425, 0.001579795472094591, 0.0011910853439080207, 0.001012625163379254, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.0.residual_function.1.bias
[0.1, 0.10089694892347166, 0.10058822047486156, 0.10007508724612908, 0.09935966439453914, 0.09844490092389294, 0.09733456752860721, 0.09603324105074956, 0.09454628561409828, 0.09287983051299326, 0.09104074494712067, 0.0890366097063762, 0.08687568592252248, 0.08456688101644841, 0.08211971198139657, 0.07954426615350735, 0.07685115963138489, 0.07405149351608162, 0.0711568081518816, 0.06817903555650454, 0.06513045023681478, 0.06202361859277568, 0.05887134711820764, 0.05568662961186921, 0.05248259361645907, 0.049272446306319724, 0.04606942004689615, 0.042886717850355645, 0.0397374589522031, 0.03663462473322734, 0.0335910052096906, 0.030619146312330543, 0.02773129817149373, 0.024939364621570717, 0.022254854132878102, 0.01968883237324835, 0.01725187659487042, 0.014954032034400912, 0.01280477050606587, 0.010812951358434707, 0.008986784955803951, 0.007333798834722066, 0.00586080667515958, 0.004573880214226949, 0.00347832421821263, 0.0025786546161080718, 0.0018785798847539717, 0.001380985762338959, 0.001087923353262609, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.0.residual_function.3.weight
[0.1, 0.10089935855788261, 0.10059783937952527, 0.10009665627792544, 0.09939782684468748, 0.09850416432788965, 0.09741926630691017, 0.09614750020980434, 0.09469398573153409, 0.0930645742238287, 0.0912658251396457, 0.08930497962705866, 0.08718993137887382, 0.084929194855325, 0.0825318710077713, 0.08000761064138234, 0.0773665755643002, 0.07461939767967851, 0.07177713618527977, 0.06885123305293077, 0.06585346696706094, 0.06279590590775078, 0.059690858569175144, 0.056550824809013904, 0.053388445328303835, 0.05021645078430342, 0.0470476105412245, 0.043894681265143225, 0.04077035557002913, 0.037687210921626246, 0.034657659004881484, 0.03169389575874962, 0.028807852279518077, 0.026011146790297413, 0.02331503787003212, 0.020730379130315876, 0.018267575522466474, 0.015936541450753804, 0.01374666086040192, 0.011706749461037936, 0.009825019237662817, 0.008109045392010811, 0.006565735847380388, 0.0052013034396997775, 0.004021240906776196, 0.003030298776413725, 0.0022324662424145648, 0.0016309551054502817, 0.0012281868434518553, 0.0010257828635683356, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.0.residual_function.4.weight
[0.1, 0.10090170272818744, 0.10060719740689555, 0.10011764199890905, 0.09943496138266815, 0.09856183978385673, 0.0975017102213197, 0.0962587410088077, 0.09483781936562219, 0.09324453220060257, 0.09148514414501023, 0.08956657292068215, 0.08749636214030354, 0.08528265164674592, 0.08293414550809325, 0.08046007779419613, 0.07787017626931679, 0.07517462414362157, 0.07238402003390985, 0.06950933629100939, 0.06656187585769022, 0.06355322782672716, 0.060495221873851784, 0.057399881744758185, 0.054279377979047284, 0.05114598005699307, 0.04801200815728521, 0.044889784715430835, 0.04179158597328288, 0.0387295937101968, 0.035715847345603506, 0.03276219660132597, 0.02988025490976618, 0.027081353751156265, 0.02437649809941482, 0.021776323151790344, 0.019291052512426306, 0.016930457994265336, 0.014703821197347082, 0.012619897014569708, 0.010686879208406462, 0.0089123681939258, 0.0073033411547886945, 0.00586612460972396, 0.004606369537346932, 0.0035290291571282765, 0.0026383394538755647, 0.0019378025223032382, 0.0014301727971784074, 0.001117446223183898, 0.0010008524070815863, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.0.residual_function.4.bias
[0.1, 0.10090398345839507, 0.10061630260063073, 0.10013806231154893, 0.09947109935029422, 0.09861797529594332, 0.0975819667093466, 0.09636705254896637, 0.09497789888904368, 0.09341984099878589, 0.09169886285140272, 0.08982157414168974, 0.08779518490042698, 0.08562747780309007, 0.08332677827922699, 0.08090192253729989, 0.07836222362779796, 0.07571743567496157, 0.07297771641449106, 0.07015358818112083, 0.06725589749589155, 0.06429577340833265, 0.06128458475354853, 0.05823389648836927, 0.05515542527426435, 0.052060994477610124, 0.04896248876013981, 0.04587180843397882, 0.04280082375657209, 0.0397613293410415, 0.03676499885706731, 0.03382334019627259, 0.030947651274304988, 0.02814897663936531, 0.02543806505383449, 0.02282532821191412, 0.020320800751831593, 0.017934101716189782, 0.015674397608478868, 0.013550367187637725, 0.011570168135877308, 0.009741405727783186, 0.00807110362102902, 0.0065656768808838965, 0.005230907342116919, 0.00407192140292617, 0.0030931703361776466, 0.002298413193572201, 0.0016907023683997925, 0.0012723718723295799, 0.001045028371260769, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.1.residual_function.0.weight
[0.1, 0.10090620270242015, 0.1006251627270019, 0.10015793450535349, 0.09950627102725669, 0.09867261726363857, 0.09766010099330491, 0.09647252106785223, 0.09511433315878724, 0.09359063304032898, 0.09190713747061477, 0.09007016274304169, 0.08808660098821652, 0.08596389431542666, 0.08371000689065058, 0.08133339505586806, 0.07884297560177901, 0.07624809231296881, 0.07355848091103848, 0.07078423252722964, 0.06793575584159099, 0.06502373803073579, 0.062059104670710546, 0.05905297874541469, 0.056016638914367305, 0.05296147719639587, 0.04989895622801296, 0.04684056625684325, 0.043797782031456345, 0.040782019749350674, 0.03780459422461442, 0.03487667643596618, 0.03200925161445044, 0.029213078028038037, 0.026498646617767765, 0.023876141636869616, 0.021355402440547473, 0.018945886569782112, 0.016656634267659648, 0.014496234561356819, 0.012472793037039326, 0.010593901428578877, 0.008866609134188196, 0.007297396767841107, 0.005892151844709276, 0.004656146691841794, 0.0035940186669640322, 0.0027097527596126594, 0.0020066666398857727, 0.001487398210903206, 0.0011538957116792416, 0.0010074104075404092, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.1.residual_function.1.weight
[0.1, 0.10090836234676798, 0.1006337852854515, 0.10017727527995178, 0.09954050567049205, 0.09872581053997873, 0.09773617615840621, 0.0965752300366659, 0.0952472276298828, 0.09375703673901867, 0.09211011966791777, 0.09031251320119865, 0.08837080647638307, 0.08629211683137215, 0.08408406371580053, 0.08175474076189714, 0.07931268611722653, 0.07676685114805684, 0.07412656762807167, 0.07140151353269657, 0.06860167756442075, 0.06573732253914692, 0.06281894776777668, 0.05985725057092198, 0.05686308706781099, 0.053847432383116435, 0.05082134041756885, 0.047795903329815724, 0.04478221087804597, 0.04179130977041331, 0.038834163173259856, 0.03592161052556264, 0.03306432780690367, 0.030272788404601264, 0.02755722472344471, 0.02492759067875172, 0.022393525210230875, 0.01996431695038907, 0.01764887017699204, 0.015455672174379688, 0.013392762123273304, 0.01146770163310943, 0.009687547024913923, 0.008058823466314247, 0.00658750105349733, 0.005278972927785502, 0.004138035507043907, 0.003168870904381387, 0.0023750315985892993, 0.001759427412508673, 0.0013243148470568058, 0.0010712888100092846, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.1.residual_function.1.bias
[0.1, 0.10091046421310701, 0.10064217751871336, 0.10019610076723093, 0.09957383155197802, 0.09877759848750854, 0.09781025322798156, 0.09667526025415747, 0.09537668446559742, 0.09391917662250418, 0.0923079566893431, 0.09054879513989715, 0.08864799229071034, 0.08661235573693592, 0.08444917597140067, 0.08216620027420457, 0.07977160496636782, 0.07727396612689752, 0.07468222887814838, 0.07200567534947981, 0.06925389143394585, 0.06643673245707597, 0.06356428788070206, 0.0606468451682422, 0.057694852940855385, 0.05471888355642088, 0.05172959524536286, 0.04873769393892857, 0.045753894926629124, 0.04278888448016437, 0.03985328158127314, 0.03695759989057772, 0.03411221009362885, 0.031327302759005984, 0.028612851841494348, 0.025978578961049194, 0.02343391858548018, 0.0209879842415514, 0.018649535875509732, 0.01642694847993767, 0.014328182099291258, 0.012360753321546326, 0.010531708358053705, 0.008847597808016198, 0.0073144531979662695, 0.005937765380267184, 0.004722464868001513, 0.003672904176676311, 0.0027928422359871966, 0.0020854309274691276, 0.0015532037962487024, 0.001198066977325917, 0.001021292368882242, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.1.residual_function.3.weight
[0.1, 0.10091251006073365, 0.10065034642251354, 0.10021442655257107, 0.0996062759950241, 0.0988280230320864, 0.09788239123593645, 0.09677268993731078, 0.09550280264417821, 0.09407717345102477, 0.0925007914863115, 0.09077917345253252, 0.08891834431997643, 0.08692481624175497, 0.08480556576388765, 0.08256800941019846, 0.08021997772746695, 0.07776968788166586, 0.07522571490118692, 0.07259696166769243, 0.06989262775961218, 0.06712217725732111, 0.0642953056226667, 0.061421905768754094, 0.05851203343873156, 0.05557587201473618, 0.05262369688015442, 0.049665839459915394, 0.046712651064661284, 0.04377446666532584, 0.040861568724895124, 0.037984151213924976, 0.03515228393574658, 0.03237587728620706, 0.029664647571272282, 0.027028083003866138, 0.02447541049894358, 0.022015563383001494, 0.01965715013103104, 0.01740842424031938, 0.015277255346530749, 0.013271101683148469, 0.011396983980659293, 0.009661460896821971, 0.008070606064004514, 0.006629986833915097, 0.005344644794111597, 0.004219078124473945, 0.003257225855384848, 0.0024624540827085143, 0.0018375441878098302, 0.0013846831038388677, 0.0011054556623449276, 0.0010008390470039254, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.1.residual_function.4.weight
[0.1, 0.10091450158893477, 0.10065829875487084, 0.10023226769521713, 0.09963786540912023, 0.09887712471461996, 0.0979526472965408, 0.09686759480889519, 0.0956256780622278, 0.09423114433287923, 0.09268876283757094, 0.09100380842298712, 0.08918204352613437, 0.08722969846717328, 0.08515345014211956, 0.08296039918828393, 0.08065804570054397, 0.07825426358149612, 0.07575727361320943, 0.07317561534267326, 0.0705181178770894, 0.06779386968888654, 0.06501218753372222, 0.06218258458777062, 0.05931473791326476, 0.056418455363558564, 0.0535036420408919, 0.05058026642157033, 0.04765832626440982, 0.04474781441903697, 0.04185868465097842, 0.03900081760041535, 0.036183986991022155, 0.033417826204452536, 0.030711795334786608, 0.02807514883561036, 0.02551690387037261, 0.023045809474259094, 0.020670316633048233, 0.0183985493812768, 0.016238277018558272, 0.01419688753907188, 0.012281362365092068, 0.010498252470968077, 0.008853655979208001, 0.007353197305287373, 0.006002007922505803, 0.00480470881267429, 0.003765394662651294, 0.002887619860773765, 0.0021743863410750622, 0.0016281333168612958, 0.00125072893875685, 0.00104346390574802, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv2_x.1.residual_function.4.bias
[0.1, 0.10091644043925185, 0.10066604104501509, 0.10024963874782547, 0.09966862532340491, 0.09892494274081474, 0.09802107667165245, 0.09696004818198663, 0.09574540363479878, 0.09438120283668201, 0.09287200546841486, 0.0912228558447643, 0.08943926605445676, 0.08752719753666954, 0.08549304115562131, 0.08334359583985983, 0.08108604585764255, 0.07872793680436473, 0.07627715038229389, 0.0737418780569067, 0.07113059367787855, 0.0684520251562375, 0.0657151252923476, 0.06292904185222639, 0.060103086992212465, 0.05724670613417766, 0.054369446395315074, 0.051480924678022226, 0.04859079552653542, 0.04570871885775082, 0.04284432767408789, 0.04000719586631131, 0.037206806213926795, 0.034452518690105405, 0.03175353917707374, 0.029118888696534895, 0.026557373257964013, 0.02407755442555695, 0.021687720702208563, 0.01939585982616619, 0.017209632072953764, 0.015136344651801449, 0.013182927282157853, 0.01135590903191764, 0.009661396494779529, 0.008105053379674949, 0.006692081580486764, 0.005427203789330559, 0.004314647711511509, 0.003358131934916419, 0.0025608535010709136, 0.0019254772194040584, 0.0014541267604360687, 0.0011483775576593572, 0.0010092515418372297, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.0.residual_function.0.weight
[0.1, 0.10091832819765198, 0.10067357960193984, 0.10026655377522097, 0.09969858041881015, 0.09897151502901555, 0.09808773283546955, 0.09705012104155733, 0.09586206939229275, 0.09452745910045496, 0.09305065016716257, 0.09143646713830786, 0.08969018334338225, 0.08781750366818344, 0.08582454591768321, 0.08371782082994103, 0.0815042108063556, 0.0791909474277398, 0.07678558782967111, 0.07429599001429633, 0.07173028717924357, 0.06909686114750598, 0.06640431498509802, 0.06366144489593928, 0.060877211485781284, 0.058060710489054625, 0.05522114305426802, 0.052367785685032975, 0.049509959934913035, 0.04665700195510055, 0.043818231994404905, 0.041002923951191754, 0.03822027507674325, 0.03547937592901453, 0.03278918067494354, 0.030158477838333045, 0.027595861588867312, 0.025109703666059198, 0.022708126029847776, 0.020398974327194164, 0.01818979226135642, 0.01608779694757581, 0.014099855335684437, 0.012232461776658606, 0.010491716806404622, 0.008883307216088435, 0.00741248747411628, 0.006084062560458353, 0.004902372269393989, 0.0038712770319586968, 0.002994145304409839, 0.0022738425639108376, 0.0017127219473838042, 0.0013126165641121948, 0.0010748335072072448, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.0.residual_function.1.weight
[0.1, 0.1009201663966092, 0.10068092052260597, 0.10028302637239904, 0.09972775455893956, 0.09901687825621679, 0.0981526675369031, 0.09713788212323105, 0.09597576257425024, 0.09467001993760735, 0.09322482389889374, 0.09164478946640447, 0.08993496223382864, 0.08810080226793233, 0.08614816667268649, 0.08408329088551814, 0.08191276876541312, 0.07964353153645488, 0.0772828256540409, 0.07483818966445675, 0.0723174301317035, 0.0697285967084524, 0.06707995643073376, 0.0643799673184458, 0.0616372513659865, 0.0588605670092584, 0.056058781156968544, 0.05324084087553763, 0.05041574481803793, 0.04759251448839724, 0.0447801654326323, 0.04198767844910767, 0.03922397090975602, 0.03649786828384073, 0.03381807595519541, 0.03119315142293746, 0.02863147697442843, 0.026141232917745866, 0.0237303714591445, 0.02140659130892494, 0.0191773130968024, 0.017049655675282268, 0.01503041338671416, 0.013126034366618744, 0.011342599952571829, 0.009685805264400809, 0.008160941017707096, 0.006772876628790508, 0.00552604466492727, 0.004424426689657312, 0.0034715405482816644, 0.0026704291341718136, 0.0020236506717639983, 0.0015332705472681997, 0.0012008547131792105, 0.0010274646876514018, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.0.residual_function.1.bias
[0.1, 0.1009219565171006, 0.10068806969981133, 0.10029906968180519, 0.0997561708197323, 0.09906106790231502, 0.0982159308596593, 0.09722339798929856, 0.09608656772011662, 0.09480898893986037, 0.09339464991643741, 0.09184796584758326, 0.09017376507776505, 0.08837727402534878, 0.08646410086708357, 0.08444021803083586, 0.08231194355122645, 0.08008592134637356, 0.07776910047731232, 0.07536871345483774, 0.07289225366149142, 0.07034745195917473, 0.06774225255541311, 0.06508478820361126, 0.06238335481471699, 0.05964638555954992, 0.0568824245426405, 0.05410010012976309, 0.05130809801242712, 0.04851513409341257, 0.04572992727799357, 0.04296117225578888, 0.04021751235820767, 0.03750751257622186, 0.03483963282269716, 0.032222201522750495, 0.029663389614577707, 0.02717118504191475, 0.024753367817759588, 0.022417485737199684, 0.020170830815163396, 0.018020416522650302, 0.01597295589250295, 0.014034840563068418, 0.012212120825169474, 0.010510486734673908, 0.008935250349623464, 0.007491329147373282, 0.006183230673509615, 0.005015038470467486, 0.003990399329775734, 0.0031125119077245627, 0.0023841167399943734, 0.001807487686417857, 0.0013844248325822809, 0.0011162488704316012, 0.001003796975410464, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.0.residual_function.3.weight
[0.1, 0.10092369999052093, 0.10069503282974157, 0.1003146964099233, 0.09978385151796365, 0.09910411829267414, 0.09827757128011609, 0.09730673310208521, 0.09619456675706897, 0.09494446657717404, 0.09356024786862345, 0.09204613526744057, 0.09040674984585828, 0.08864709500880484, 0.08677254122351014, 0.08478880962883767, 0.08270195457436652, 0.08051834514251417, 0.07824464571009465, 0.07588779560863772, 0.0734549879455467, 0.07095364765073178, 0.06839140881572113, 0.06577609139441074, 0.06311567733656202, 0.06041828622688834, 0.05769215050407911, 0.05494559033539403, 0.052186988223509284, 0.04942476342311634, 0.04666734624535332, 0.043923152328492254, 0.0412005569534079, 0.03850786948221735, 0.035853307998103615, 0.03324497422372274, 0.030690828794743115, 0.0281986669639824, 0.025776094810293068, 0.023430506024806664, 0.021169059345385894, 0.018998656708153806, 0.016925922182782143, 0.014957181755828265, 0.013098444023821716, 0.011355381855025298, 0.00973331507583915, 0.008237194234688328, 0.006871585492947313, 0.005640656689013695, 0.004548164618063782, 0.003597443566312363, 0.002791395134769852, 0.002132479383554946, 0.0016227073237901892, 0.0012636347799952644, 0.0010563576417099865, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.0.residual_function.4.weight
[0.1, 0.10092539820051905, 0.10070181541921562, 0.10032991884320241, 0.09981081823863112, 0.09914606263907025, 0.09833763572307747, 0.09738794989476006, 0.09629983908498739, 0.09507655029473719, 0.09372173390581101, 0.092239432787832, 0.09063407023403114, 0.08891043676182175, 0.08707367581755074, 0.08512926842808444, 0.08308301684503, 0.08094102723039887, 0.07870969143537988, 0.07639566792659525, 0.07400586191675683, 0.07154740475901437, 0.0690276326664845, 0.06645406482046255, 0.06383438093264453, 0.06117639832831454, 0.05848804861888346, 0.0557773540333896, 0.05305240347958978, 0.05032132840607645, 0.04759227853744964, 0.04487339755495229, 0.042172798795139126, 0.039498541039096474, 0.036858604464459425, 0.03426086683198732, 0.031713079977758594, 0.02922284668113406, 0.026797597977515943, 0.02444457098360338, 0.022170787301315276, 0.01998303206482484, 0.017887833693230677, 0.015891444409284416, 0.013999821582307958, 0.012218609950974534, 0.010553124779002332, 0.009008335994024844, 0.007588853356969177, 0.006298912706197589, 0.005142363317460917, 0.004122656417382702, 0.0032428348847505, 0.002505524170346567, 0.00191292446241375, 0.0014668041211354003, 0.001168494401721292, 0.0010188854818462593, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.0.residual_function.4.bias
[0.1, 0.10092705248476008, 0.10070842279263954, 0.10034474886335028, 0.09983709186127226, 0.09918693307908132, 0.09839616961548432, 0.09746710883967379, 0.09640246165865378, 0.0952053346070828, 0.0938792207827144, 0.0924279896538851, 0.09085587576879037, 0.08916746639949388, 0.08736768815672356, 0.08546179261451113, 0.08345534098662102, 0.0813541878994814, 0.07916446430896606, 0.07689255961087466, 0.07454510299731064, 0.07212894411335778, 0.06965113307049642, 0.06711889987507899, 0.06453963333189004, 0.06192085948434825, 0.05927021965425977, 0.056595448145200636, 0.053904349674587654, 0.051204776600288694, 0.04850460600822295, 0.04581171672780698, 0.043133966342313694, 0.04047916826122528, 0.03785506892148088, 0.03526932518414364, 0.03272948199244092, 0.030242950356370105, 0.027816985728107882, 0.025458666831321877, 0.023174875006158658, 0.020972274130177073, 0.01885729117381584, 0.01683609744713261, 0.0149145905925346, 0.013098377376044142, 0.011392757327312847, 0.009802707276120596, 0.0083328668304811, 0.0069875248387266655, 0.005770606875075162, 0.004685663785194554, 0.003735861325188183, 0.0029239709242330925, 0.002252361597825169, 0.0017229930352273262, 0.0013374098812910953, 0.00109673722933638, 0.0010016773382410297, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.0.shortcut.0.weight
[0.1, 0.10092866413661632, 0.10071486009868141, 0.10035919796202097, 0.09986269258525862, 0.09922676071398343, 0.09845321693815899, 0.09754426851430986, 0.0965025090672595, 0.09533091118939166, 0.09403281795855258, 0.09261193339879722, 0.09107231191119915, 0.08941834670488243, 0.08765475726128705, 0.08578657586743753, 0.08381913325664135, 0.08175804339759048, 0.07960918747526922, 0.0773786971093778, 0.07507293685815698, 0.07269848605753786, 0.07026212004743741, 0.06777079083876958, 0.06523160727633731, 0.06265181475420924, 0.06003877454146202, 0.05739994277728099, 0.054742849195355006, 0.0520750756382744, 0.049404234423239435, 0.046737946620811516, 0.044083820308687266, 0.041449428862546864, 0.038842289345922376, 0.03626984106074901, 0.033739424319804055, 0.031258259501604956, 0.028833426447531854, 0.02647184425996369, 0.02418025155907252, 0.021965187254612047, 0.019832971887566605, 0.017789689594901344, 0.015841170748875616, 0.013992975320457419, 0.01225037701431031, 0.010618348220622263, 0.00910154582671589, 0.007704297928925246, 0.006430591482656322, 0.005284060925870814, 0.004267977808455275, 0.0033852414570677385, 0.00263837070209884, 0.0020294966903542594, 0.0015603568039667828, 0.0012322897028901373, 0.0010462315051204421, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.0.shortcut.1.weight
[0.1, 0.10093023440678983, 0.10072113231667915, 0.10037327725492227, 0.0998876399541086, 0.09926557564521317, 0.098508820275657, 0.09761948566493094, 0.0966000536113022, 0.09545336896604913, 0.09418263169455177, 0.09279138794639255, 0.09128352015938705, 0.08966323622516101, 0.08793505774650803, 0.08610380741929756, 0.08417459557414537, 0.08215280591540366, 0.080044080497268, 0.07785430397892686, 0.0755895872026923, 0.07325625014093816, 0.07086080425943535, 0.06840993434630092, 0.06591047985726971, 0.06336941582934684, 0.06079383341610407, 0.058190920098938274, 0.0555679396295153, 0.05293221175937225, 0.05029109181324521, 0.04765195016312537, 0.045022151660324215, 0.04240903508294457, 0.039819892656112375, 0.03726194970212019, 0.03474234447727129, 0.03226810825169239, 0.029846145687704273, 0.027483215571507316, 0.025185911951952328, 0.02296064573903141, 0.02081362681344037, 0.018750846697138286, 0.016778061833264215, 0.014900777522070287, 0.013124232557700337, 0.011453384608686484, 0.009892896382961813, 0.008447122615996942, 0.007120097918371848, 0.005915525516695935, 0.00483676691929587, 0.0038868325355108195, 0.003068373274772578, 0.002383673148914899, 0.0018346428983558529, 0.001422814659940257, 0.0011493376913222545, 0.0010149751638196584, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.0.shortcut.1.bias
[0.1, 0.10093176450487032, 0.10072724426279306, 0.10038699749536754, 0.0999119528788594, 0.09930340700945331, 0.09856302086429757, 0.09769281526800025, 0.0966951653769505, 0.09557279419651957, 0.09432876514883609, 0.09296647371142107, 0.0914896381495042, 0.0899022893673172, 0.08820875990606387, 0.0864136721185965, 0.0845219255530742, 0.0825386835800393, 0.08046935929941149, 0.0783196007658657, 0.07609527557291972, 0.07380245483780834, 0.0714473966308558, 0.06903652889457444, 0.0665764318991114, 0.06407382028192876, 0.06153552472073918, 0.05896847328971876, 0.056379672549884276, 0.0537761884252467, 0.05116512691693834, 0.04855361470795251, 0.04594877971143392, 0.043357731615611295, 0.04078754247847359, 0.03824522742515478, 0.035737725500712414, 0.03327188073056061, 0.030854423440251556, 0.028491951885591595, 0.02619091424323121, 0.023957591010883968, 0.021798077865211565, 0.01971826902416344, 0.017723841159181315, 0.015820237901179875, 0.014012654982592743, 0.01230602605603744, 0.010705009228306212, 0.009213974346437098, 0.007836991070566878, 0.0065778177661201635, 0.00543989124565225, 0.004426317388344958, 0.0035398626627584522, 0.0027829465759771294, 0.002157635069759133, 0.001665634881713939, 0.0013082888868989498, 0.0010865724325497253, 0.0010010906759479012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.1.residual_function.0.weight
[0.1, 0.10093325560083033, 0.10073320059591415, 0.1004003690872956, 0.09993564966053739, 0.09934028301239639, 0.0986158586384412, 0.09776431058945521, 0.09678791230795412, 0.09568927055860393, 0.09447131846874313, 0.09313730769758995, 0.09169079975504064, 0.09013565649323602, 0.08847602979628129, 0.08671635049564588, 0.08486131654083587, 0.08291588045692136, 0.08088523612240413, 0.07877480490272556, 0.0765902211764355, 0.07433731729266384, 0.07202210800005493, 0.06965077438873628, 0.06722964738819263, 0.06476519086510256, 0.0622639843662626, 0.05973270555267133, 0.05717811237167051, 0.05460702501473971, 0.05202630770911296, 0.04944285039182987, 0.04686355031514737, 0.04429529363242132, 0.04174493701361979, 0.039219289339550406, 0.036725093523673945, 0.03426900851003611, 0.031857591495378715, 0.029497280422893343, 0.027194376794355515, 0.024955028846526478, 0.022785215136738642, 0.0206907285814868, 0.01867716099063894, 0.01674988813855658, 0.014914055411981653, 0.013174564073006621, 0.01153605817380271, 0.010002912158041066, 0.008579219182108116, 0.007268780187295104, 0.006075093752136323, 0.00500134675198825, 0.0040504058507862695, 0.003224809847694248, 0.0025267628990795314, 0.0019581286339092055, 0.0015204251782783165, 0.0012148211023532908, 0.0010421323005513896, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.1.residual_function.1.weight
[0.1, 0.10093470882646119, 0.10073900582333842, 0.10041340209778125, 0.0999587480117637, 0.09937623096123899, 0.09866737227508049, 0.09783402324190794, 0.09687836027517505, 0.09580287922914604, 0.09461038888060548, 0.09330400359332497, 0.09188713518444451, 0.09036348401401073, 0.08873702932094292, 0.08701201883066578, 0.0851929576615504, 0.08328459655913557, 0.08129191948885783, 0.07922013061969324, 0.07707464073270553, 0.07486105308999613, 0.07258514880096205, 0.07025287172407897, 0.06787031294364158, 0.0654436948620019, 0.06297935494885114, 0.06048372918998646, 0.05796333527878822, 0.055424755594306145, 0.052874620010409074, 0.05031958858089524, 0.0477663341457831, 0.04522152490420914, 0.042691806999445935, 0.04018378716152246, 0.03770401545277779, 0.03525896816141078, 0.03285503088770193, 0.030498481867079763, 0.028195475573586638, 0.025952026646565318, 0.02377399418254457, 0.021667066433347693, 0.019636745950386947, 0.01768833521394198, 0.015826922784953575, 0.014057370015498817, 0.012384298352656031, 0.010812077268916427, 0.009344812850664796, 0.007986337074531547, 0.006740197799622571, 0.005609649501763952, 0.004597644773960052, 0.0037068266152630695, 0.0029395215281926615, 0.0022977334427327183, 0.0017831384827737377, 0.0013970805886690464, 0.0011405680073370442, 0.001014270659076232, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.1.residual_function.1.bias
[0.1, 0.10093612527675169, 0.10074466430621752, 0.10042610626905775, 0.09998126507753086, 0.09941127729595674, 0.09871759923680756, 0.09790200323984538, 0.0969665731438137, 0.09591369896225262, 0.09474607077704067, 0.0934666718652653, 0.0920787710769831, 0.09058591448334397, 0.08899191631641862, 0.08730084922387787, 0.08551703386342296, 0.08364502786355288, 0.08168961417887409, 0.07965578886970436, 0.07754874833719404, 0.07537387604657861, 0.07313672877248463, 0.070843022401432, 0.06849861732780732, 0.06610950348062096, 0.06368178501930481, 0.061221664737652594, 0.05873542821575017, 0.05622942776038819, 0.05371006617498857, 0.05118378040051245, 0.04865702506914747, 0.04613625601279395, 0.043627913768485765, 0.0411384071228896, 0.03867409673792596, 0.036241278899348355, 0.03384616942980286, 0.03149488780747019, 0.0291934415308665, 0.02694771076975141, 0.024763433341359123, 0.022646190050339132, 0.02060139042986194, 0.018634258920321774, 0.01674982152094929, 0.014952892948438982, 0.013248064335401537, 0.01163969150007068, 0.010131883817235874, 0.008728493718835238, 0.007433106851034565, 0.006249032912941077, 0.005179297200358811, 0.004226632876191371, 0.0033934739872409463, 0.0026819492452456787, 0.0020938765880447454, 0.0016307585347672894, 0.0012937783469126876, 0.0010837970051305037, 0.0010013510094244995, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.1.residual_function.3.weight
[0.1, 0.10093750601121243, 0.10075018026479508, 0.10043849103007181, 0.10000321745518422, 0.09944544761940866, 0.09876657581321963, 0.0979682990528989, 0.09705261283840286, 0.09602180616509233, 0.09487845580179469, 0.0936254198495013, 0.09226583059680232, 0.09080308668991827, 0.08924084463690177, 0.08758300966724664, 0.08583372596975308, 0.08399736633305219, 0.0820785212146858, 0.08008198726606641, 0.07801275534200199, 0.07587599802475897, 0.07367705669385041, 0.07142142817387141, 0.06911475099375944, 0.0667627912918277, 0.06437142840180589, 0.06194664015591942, 0.059494487941746325, 0.05702110155020571, 0.054532663852554425, 0.05203539534469545, 0.04953553859743398, 0.047039342651551394, 0.04455304739670582, 0.04208286797320842, 0.039634979235667, 0.03721550031733426, 0.034830479333745966, 0.032485878263886346, 0.03018755804667428, 0.02794126393002514, 0.025752611109112502, 0.02362707068973038, 0.021569956011844218, 0.019586409367518605, 0.017681389146423542, 0.015859657441052527, 0.014125768142636323, 0.012484055557509928, 0.010938623572388773, 0.009493335395638913, 0.008151803900184846, 0.006917382592195964, 0.005793157228127504, 0.004781938101071314, 0.0038862530156989422, 0.0031083409693576807, 0.0024501465551155835, 0.0019133151007464204, 0.0014991885558057853, 0.001208802137079922, 0.0010428817407926775, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.1.residual_function.4.weight
[0.1, 0.1009388520551479, 0.10075555778343792, 0.10045056550759085, 0.10002462121364122, 0.09947876672631689, 0.09881433716082136, 0.09803295765725184, 0.09713653940564, 0.09612727497133844, 0.09500763293218592, 0.09378035184056904, 0.09244843352514809, 0.09101513574863106, 0.08948396423955374, 0.08785866411755598, 0.0861432107331269, 0.0843417999442244, 0.08245883785355151, 0.08049893003159142, 0.07846687025176058, 0.07636762876523315, 0.07420634014411512, 0.07198829072270299, 0.06971890566754478, 0.06740373570792856, 0.06504844355925482, 0.06265879007250034, 0.06024062014365002, 0.05779984841756197, 0.055342444821232344, 0.052874419961844624, 0.050401810425318626, 0.04793066401131758, 0.045467024940827286, 0.043016919072488446, 0.040586339163841835, 0.03818123021353592, 0.035807474920349025, 0.0334708792945915, 0.03117715845708199, 0.028931922660431642, 0.02674066356682735, 0.02460874081587766, 0.022541368915374733, 0.020543604487036946, 0.018620333898428005, 0.016776261311303404, 0.015015897175617604, 0.01334354719733415, 0.011763301807022807, 0.010279026155002813, 0.008894350657502735, 0.007612662116960983, 0.006437095438185303, 0.005370525960633454, 0.004415562425569218, 0.0035745405952957105, 0.0028495175400728035, 0.0022422666066923883, 0.0017542730810179033, 0.0013867305550971812, 0.0011405380077344643, 0.001016297605662108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv3_x.1.residual_function.4.bias
[0.1, 0.10094016440087851, 0.10076080081547092, 0.10046233853688098, 0.10004549191187886, 0.09951125863116653, 0.09886091734148084, 0.0980960245852499, 0.09721841107512699, 0.09623017731231914, 0.09513368855919713, 0.09393156917821988, 0.09262669635072246, 0.09122219319060128, 0.08972142126937849, 0.08812797257053614, 0.08644566089237682, 0.08467851271998741, 0.08283075758815314, 0.08090681795828775, 0.0789112986336083, 0.07684897573788879, 0.07472478528417117, 0.07254381136079326, 0.07031127396301, 0.0680325164993317, 0.06571299300248204, 0.06335825507558543, 0.060973938604827055, 0.058565750270387844, 0.05613945388793937, 0.053700856613389185, 0.051255795043894874, 0.048810121248412414, 0.04636968876121413, 0.04394033857189953, 0.04152788514543074, 0.03913810250565277, 0.036776710415606484, 0.034449360687710616, 0.032161623656578535, 0.02991897484684586, 0.027726781867918528, 0.025590291567007952, 0.02351461747120099, 0.021504727548621778, 0.019565432317977773, 0.017701373334948905, 0.015917012082977113, 0.014216619295044797, 0.012604264732000263, 0.011083807441894585, 0.009658886523643415, 0.008332912417120551, 0.0071090587405293225, 0.005990254694588737, 0.004979178051714371, 0.004078248746973578, 0.003289623086154952, 0.0026151885848144144, 0.002056559450650202, 0.0016150727200194828, 0.0012917850578435244, 0.0010874702285605157, 0.0010026172441792484, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.0.residual_function.0.weight
[0.1, 0.100941444008915, 0.10076591318782366, 0.10047381867197377, 0.10006584461672, 0.09954294659506803, 0.09890634935949363, 0.09815754397327883, 0.09729828431808443, 0.0963305829859388, 0.09525670656526507, 0.09407917033198782, 0.0928007323581542, 0.09142438705186814, 0.08995335814366906, 0.08839109113578231, 0.0867412452319273, 0.0850076847665856, 0.08319447015380335, 0.0813058483767286, 0.07934624304015898, 0.07732024400939926, 0.07523259665969675, 0.07308819076143537, 0.07089204902712157, 0.06864931534698797, 0.06636524274076978, 0.06404518105387372, 0.06169456442675791, 0.05931889856687287, 0.056923747852975076, 0.05451472230201853, 0.05209746442915064, 0.04967763603158958, 0.04726090492733834, 0.04485293167979673, 0.042459356339365394, 0.04008578523309615, 0.03773777783333054, 0.03542083373608327, 0.03314037977967047, 0.03090175733375381, 0.0287102097885727, 0.02657087027366787, 0.024488749634861848, 0.02246872469765712, 0.020515526844542345, 0.01863373093296042, 0.016827744579896303, 0.015101797838181545, 0.01345993328869699, 0.011905996571679556, 0.010443627379311139, 0.009076250930687427, 0.007807069949133821, 0.0066390571606599665, 0.0055749483311230245, 0.004617235858408447, 0.0037681629346367753, 0.0030297182920702263, 0.002403631545025069, 0.0018913691387004374, 0.0014941309144122553, 0.001212847299277319, 0.001048177126930038, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.0.residual_function.1.weight
[0.1, 0.10094269180908678, 0.10077089860549697, 0.1004850141955389, 0.10008569391994637, 0.09957385315162298, 0.09895066519730744, 0.09821755860797017, 0.09737621390410643, 0.09642855972343274, 0.09537676839981735, 0.09422325098357923, 0.0929706517145708, 0.09162184196071398, 0.09017991363588389, 0.08864817211222946, 0.08703012864317738, 0.08532949231448886, 0.08355016154181982, 0.08169621513427504, 0.0797719029444411, 0.0777816361263321, 0.07572997702357925, 0.07362162871104454, 0.0714614242138322, 0.06925431542841176, 0.06700536177124886, 0.0647197185809651, 0.06240262530061278, 0.06005939346715488, 0.057695394535682065, 0.05531604756627761, 0.052926806801756425, 0.0505331491647533, 0.04814056170282196, 0.0457545290103242, 0.04338052065594267, 0.04102397864463749, 0.03869030494278781, 0.036384849095115, 0.03411289596177352, 0.03187965360371978, 0.02969024134413023, 0.027549678033235315, 0.025462870543470312, 0.02343460252131608, 0.021469523421614175, 0.019572137849492743, 0.017746795234335932, 0.015997679859466624, 0.01432880127039806, 0.012743985083642016, 0.011246864217142171, 0.009840870562436072, 0.008529227117635157, 0.007314940599257202, 0.006200794549846982, 0.0051893429571846774, 0.004282904399709141, 0.0034835567315765195, 0.002793132319537649, 0.002213213842553168, 0.0017451306637748347, 0.0013899557832095335, 0.0011485033780517897, 0.0010213269363228575, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.0.residual_function.1.bias
[0.1, 0.10094390870162669, 0.1007757606558569, 0.10049593312837884, 0.10010505395476602, 0.09960400013183263, 0.09899389584995799, 0.09827610997079465, 0.09745225295601925, 0.09652417325401713, 0.0954939531526074, 0.09436390410711407, 0.09313656155426457, 0.09181467922355245, 0.09040122295882674, 0.0888993640639708, 0.08731247218759997, 0.08564410776274328, 0.0838980140184707, 0.08207810858139016, 0.08018847468585692, 0.07823335201261704, 0.07621712717643936, 0.07414432388407881, 0.07201959278465307, 0.06984770103520453, 0.06763352160485932, 0.06538202234158065, 0.06309825482604703, 0.06078734303766366, 0.05845447185813599, 0.056104875438399905, 0.05374382545500886, 0.05137661928232656, 0.04900856810706276, 0.04664498501181883, 0.04429117305437978, 0.0419524133694984, 0.03963395331986704, 0.037340994722862064, 0.0350786821794764, 0.03285209153162543, 0.030666218473724485, 0.028525967344089482, 0.02643614012130864, 0.02440142565027403, 0.02242638912204565, 0.020515461831151556, 0.018672931233305556, 0.016902931325848632, 0.015209433372497982, 0.01359623699321346, 0.012066961639172868, 0.010625038471983352, 0.009273702665348993, 0.008015986146467074, 0.006854710793438811, 0.0057924821039569965, 0.004831683349476128, 0.00397447022798078, 0.0032227660273496544, 0.002578257310166975, 0.0020423901296631403, 0.0016163667852745582, 0.0013011431251021824, 0.0010974264013211047, 0.0010056736833528083, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.0.residual_function.3.weight
[0.1, 0.10094509555821352, 0.10078050281276318, 0.10050658323856144, 0.10012393841166094, 0.0996334086880866, 0.09903607135826452, 0.0983332382811008, 0.09752645300290626, 0.09661748736749454, 0.09560833762489823, 0.09450122004724922, 0.0932985660614512, 0.09200301690933428, 0.0906174178470186, 0.08914481189622643, 0.08758843316126574, 0.08595169972636262, 0.0842382061489409, 0.08245171556533551, 0.08059615142627646, 0.07867558888029386, 0.07669424582396789, 0.07465647363867833, 0.07256674763419804, 0.07042965722011753, 0.06824989582668609, 0.06603225059720509, 0.0637815918746107, 0.0615028625053356, 0.05920106698393989, 0.056881260462351896, 0.05454853764785644, 0.05220802161421243, 0.049864852550472655, 0.04752417647221535, 0.0451911339199797, 0.042870848669725466, 0.040568416480110725, 0.0382888939013003, 0.03603728716988341, 0.03381854121428847, 0.031637528794841795, 0.029499039802320194, 0.027407770738499662, 0.025368314401803174, 0.023385149800699324, 0.021462632317003856, 0.01960498414068747, 0.01781628499719643, 0.016100463187650606, 0.014461286961596348, 0.012902356241260876, 0.011427094715483577, 0.010038742320686904, 0.008740348125400414, 0.007534763633964569, 0.006424636524120525, 0.0054124048322393755, 0.004500291598961203, 0.0036902999870027592, 0.0029842088818562867, 0.002383568985040945, 0.001889699408486694, 0.0015036847775301834, 0.0012263728488847705, 0.001058372648816174, 0.0010000531356125987, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.0.residual_function.4.weight
[0.1, 0.10094625322297446, 0.10078512844053943, 0.1005169720502051, 0.10014236055364026, 0.099662099317268, 0.0990772208408317, 0.09838898253765378, 0.09759886403135912, 0.09670856397487494, 0.09571999639854614, 0.09463528659521668, 0.09345676655112406, 0.09218696993242947, 0.09082862663816223, 0.08938465693128878, 0.08785816516052376, 0.08625243308638063, 0.0845709128258094, 0.08281721943059046, 0.08099512311543916, 0.07910854115253028, 0.07716152944986786, 0.07515827383160585, 0.07310308103906586, 0.07100036947179848, 0.06885465968859394, 0.06667056468886341, 0.064452779995285, 0.06220607355903536, 0.05993527550930936, 0.057645267769164926, 0.05534097356001791, 0.05302734681735072, 0.050709361540389714, 0.04839200109864807, 0.046080247518323764, 0.043779070771585604, 0.04149341809177373, 0.039228203337485716, 0.03698829642841411, 0.03477851287564692, 0.032603603428939525, 0.03046824386321529, 0.028377024926252128, 0.02633444246916667, 0.02434488778091408, 0.02241263814758266, 0.020541847656780507, 0.018736538266882385, 0.017000591160337964, 0.015337738399629774, 0.013751554903820067, 0.012245450762935678, 0.010822663906714113, 0.009486253143471988, 0.008239091584061609, 0.007083860465052876, 0.006023043384420456, 0.00505892096212826, 0.004193565937090548, 0.003428838711050435, 0.0027663833489560852, 0.002207624044432915, 0.0017537620579509798, 0.0014057731342697936, 0.0011644054047128807, 0.0010301777787819258, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.0.residual_function.4.bias
[0.1, 0.10094738251344888, 0.10078964079779112, 0.10052710685193085, 0.10016033323092266, 0.09969009188300937, 0.099117372524903, 0.09844338055872844, 0.09766953453501431, 0.09679746316707044, 0.09582900190303581, 0.09476618906281183, 0.09361126154801158, 0.09236665013395418, 0.09103497435361324, 0.08961903698428725, 0.08812181814859572, 0.0865464690422167, 0.08489630530156694, 0.08317480002538365, 0.08138557646489551, 0.07953240039796777, 0.0776191722032716, 0.0756499186511591, 0.07362878442852204, 0.07156002341547153, 0.06944798973219886, 0.06729712857486102, 0.06511196685977876, 0.06289710369563835, 0.06065720070374983, 0.05839697220673526, 0.0561211753062964, 0.053834599870945354, 0.05154205845477081, 0.049248376168458126, 0.046958380523881384, 0.04467689127364182, 0.04240871026693708, 0.040158611343111786, 0.037931330284160465, 0.035731554847329726, 0.033563914898798064, 0.03143297266919909, 0.029343213151497086, 0.027299034661424727, 0.025304739580350434, 0.02336452530005847, 0.021482475388500857, 0.019662550995113964, 0.017908582513789507, 0.016224261521046605, 0.01461313300637257, 0.013078587911085543, 0.011623855991422044, 0.0102519990208708, 0.008965904346059761, 0.007768278809759121, 0.006661643053790855, 0.005648326213835258, 0.004730461017300241, 0.003909979294571186, 0.0031886079130885527, 0.0025678651428108594, 0.0020490574607128047, 0.0016332768010437807, 0.0013213982571345364, 0.0011140782395886552, 0.0010117530947355393, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.0.shortcut.0.weight
[0.1, 0.10094848422151528, 0.10079404304107846, 0.10053699470499511, 0.10017786889507092, 0.09971740563713279, 0.09915655377610855, 0.09849646902080761, 0.0977385115624314, 0.09688424327172095, 0.0959354244805175, 0.09489401035436787, 0.09376214686364936, 0.09254216636151652, 0.09123658277778147, 0.08984808643863194, 0.08837953852286129, 0.08683396516603448, 0.08521455122473874, 0.08352463371376911, 0.08176769492977168, 0.07994735527551473, 0.07806736579957414, 0.0761316004668057, 0.0741440481755327, 0.07210880453789768, 0.07003006344031659, 0.06791210840142585, 0.06575930374533043, 0.06357608560834166, 0.061366952797736754, 0.05913645752137623, 0.056889196007282865, 0.05462979903251091, 0.05236292238082285, 0.05009323724883632, 0.04782542062041084, 0.045564145629109425, 0.04331407192859384, 0.0410798360907973, 0.03886604205165989, 0.036677251624114886, 0.034517975097874826, 0.0323926619453883, 0.030305691653118222, 0.02826136469703582, 0.026263893680925944, 0.02431739465576435, 0.022425878638055136, 0.02059324334460499, 0.018823265160766767, 0.01711959135870214, 0.015485732581698962, 0.013925055610030535, 0.012440776423263161, 0.011035953573308659, 0.009713481881876988, 0.008476086475316452, 0.007326317169133528, 0.006266543213763692, 0.005298948412420073, 0.004425526621080616, 0.0036480776398863926, 0.002968203504417332, 0.0023873051844878527, 0.00190657969726482, 0.0015270176406566851, 0.0012494011520566496, 0.0010743022966459117, 0.0010020818885784146, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.0.shortcut.1.weight
[0.1, 0.10094955911428277, 0.10079833822844918, 0.100546642451116, 0.10019497961260088, 0.09974405924030573, 0.09919479112714923, 0.0985482834959363, 0.09780584076336864, 0.09696896090820722, 0.09603933244889738, 0.09501883103675413, 0.09390951567158164, 0.0927136245473623, 0.09143357053639715, 0.09007193632100936, 0.08863146918263283, 0.0871150754587998, 0.08552581467921132, 0.08386689339272169, 0.08214165869769197, 0.08035359148876564, 0.07850629943369232, 0.07660350969436945, 0.07464906140678843, 0.07264689793505782, 0.07060105891513223, 0.06851567210429964, 0.06639494505287213, 0.06424315661488321, 0.06206464831492042, 0.05986381558851148, 0.057645098913737786, 0.05541297485196858, 0.053171947015792136, 0.0509265369823674, 0.048681275170529725, 0.04644069170005738, 0.04420930725154131, 0.04199162394529988, 0.03979211625774135, 0.03761522199350186, 0.03546533333157338, 0.03334678796348802, 0.031263860341437695, 0.029220753053988014, 0.0272215883467868, 0.02527039980537503, 0.023371124216881967, 0.021527593627023785, 0.019743527608432416, 0.01802252575591412, 0.016368060423779373, 0.014783469719897847, 0.013271950770613626, 0.011836553270109944, 0.01048017332723808, 0.009205547622225505, 0.008015247885052989, 0.006911675706641023, 0.005897057693314855, 0.004973440974324782, 0.004142689071485566, 0.0034064781392688084, 0.0027662935829343583, 0.0022234270615239683, 0.0017789738817642832, 0.0014338307881371795, 0.0011886941535763354, 0.0010440585744404976, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.0.shortcut.1.bias
[0.1, 0.10095060793494859, 0.10080252932283791, 0.10055605672000568, 0.1002116770780852, 0.0997700707819435, 0.09923211030545682, 0.09859885848777916, 0.09787156643350944, 0.09705167104090587, 0.09614079216303113, 0.09514072940743684, 0.09405345858071015, 0.0928811277849072, 0.09162605317358508, 0.09029071437581765, 0.08887774959723554, 0.0873899504077691, 0.08583025622639234, 0.08420174851376389, 0.08250764468423893, 0.08075129174927953, 0.07893615970482236, 0.07706583467566808, 0.075144011830437, 0.07317448808108866, 0.071161154581429, 0.06910798903942544, 0.06701904785851845, 0.06489845812345585, 0.06275040944648334, 0.06057914569000036, 0.05838895658203509, 0.056184169241103324, 0.05396913962719579, 0.051748243935783735, 0.04952586995184566, 0.04730640838099689, 0.04509424417484889, 0.04289374786773663, 0.04070926694192997, 0.038545117238388556, 0.0364055744300299, 0.03429486557435688, 0.032217160762133615, 0.030176564878609802, 0.02817710949357042, 0.02622274489623322, 0.024317332290731266, 0.02246463616759899, 0.02066831686633372, 0.01893192334372677, 0.017258886162251037, 0.01565251071235856, 0.014115970682078084, 0.012652301786814757, 0.011264395771740175, 0.009954994698621572, 0.008726685528378242, 0.007581895010067779, 0.0065228848864001985, 0.00555174742525224, 0.0046704012860101, 0.0038805877289074635, 0.003183867174847981, 0.002581616122508789, 0.0020750244288160686, 0.0016650929581653977, 0.0013526316050313726, 0.0011382576938732006, 0.001022394759497485, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.1.residual_function.0.weight
[0.1, 0.10095163140362312, 0.10080661919533707, 0.10056524393662077, 0.10022797262677276, 0.09979545779938728, 0.09926853625986885, 0.09864822746642814, 0.09793573155769152, 0.09713242703074017, 0.0962398680740714, 0.09525978156064187, 0.09419406370681092, 0.09304477640364746, 0.09181414322769904, 0.09050454513894064, 0.08911851587422714, 0.08765873704516136, 0.08612803294986265, 0.08452936510867329, 0.08286582653437534, 0.0811406357480031, 0.07935713055182936, 0.07751876157157288, 0.0756290855803216, 0.07369175861708764, 0.07171052891330841, 0.06968922964097883, 0.06763177149644524, 0.0655421351342099, 0.06342436346538396, 0.061282553835690146, 0.05912085009814825, 0.05694343459578095, 0.05475452006985146, 0.052558341509288044, 0.05035914795706542, 0.04816119428939489, 0.045968732983628705, 0.043786005890805675, 0.041617236028755965, 0.039466619411643256, 0.037338316931752095, 0.03523644630922723, 0.03316507412533975, 0.031128207954693978, 0.029129788611597197, 0.027173682525593337, 0.02526367426091218, 0.023403459194306926, 0.02159663636544683, 0.019846701513697253, 0.01815704031475935, 0.01653092183025499, 0.014971492182929657, 0.013481768469710921, 0.012064632924398733, 0.010722827341281178, 0.009458947770464864, 0.00827543949518304, 0.007174592300798704, 0.006158536044656575, 0.005229236535354647, 0.0043884917294080335, 0.003637928252663774, 0.0029789982531963125, 0.0024129765917730354, 0.0019409583753252516, 0.0015638568381966915, 0.00128240157526907, 0.0010971371303828072, 0.0010084219427842092, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.1.residual_function.1.weight
[0.1, 0.10095263021812381, 0.10081061062834464, 0.10057421032814207, 0.10024387724674239, 0.09982023729638603, 0.09930409318635472, 0.09869642290200482, 0.09799837785168924, 0.09721128068507887, 0.09633662278701763, 0.09537606145165943, 0.09433141674224163, 0.09320466804244519, 0.09199795030587564, 0.09071355001077167, 0.08935390082760612, 0.08792157900778838, 0.0864192985022057, 0.08484990581885274, 0.08321637462929302, 0.08152180013417205, 0.0797693931984581, 0.0779624742675233, 0.07610446707559271, 0.07419889215848255, 0.07224936018291972, 0.07025956510508197, 0.06823327717132188, 0.0661743357743371, 0.06408664217832248, 0.061974152126888664, 0.059840868347753796, 0.057690832968409746, 0.055528119857134364, 0.053356826903861315, 0.051181068255534035, 0.04900496652065597, 0.046832644957808006, 0.044668219662933994, 0.04251579177019819, 0.0403794396811919, 0.03826321133721379, 0.03617111654926607, 0.03410711940029951, 0.03207513073410365, 0.030079000745074247, 0.028122511682898366, 0.026209370685980726, 0.024343202757190232, 0.022527543895236115, 0.020765834394688515, 0.019061412327338167, 0.017417507217247347, 0.015837233921475635, 0.01432358672807606, 0.012879433682544163, 0.011507511153470457, 0.010210418647693065, 0.008990613884774876, 0.007850408140137943, 0.006791961865678924, 0.005817280596163798, 0.00492821114915799, 0.004126438125692593, 0.0034134807182970455, 0.0027906898324463517, 0.0022592455268772064, 0.0018201547776229834, 0.0014742495700043093, 0.0012221853221905824, 0.001064439643319303, 0.0010013114285264849, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.1.residual_function.1.bias
[0.1, 0.10095360505473905, 0.100814506318594, 0.10058296193069481, 0.10025940159060959, 0.099844425760909, 0.0993388045528292, 0.09874347629710103, 0.098059545802598, 0.09728828230603426, 0.09643111711651803, 0.09548964095933118, 0.09446560102386481, 0.09336089772118897, 0.09217758115727392, 0.09091784732840807, 0.08958403404587273, 0.08817861659743717, 0.08670420315372496, 0.08516352992797661, 0.08355945609819107, 0.08189495850106945, 0.08017312610760795, 0.0783971542905923, 0.07657033889463193, 0.07469607011973871, 0.0727778262298013, 0.07081916709763052, 0.06882372759855523, 0.06679521086482869, 0.06473738141336381, 0.06265405815955083, 0.060549107330122635, 0.05842643528821955, 0.05628998128396922, 0.05414371014403485, 0.05199160491369853, 0.04983765946513454, 0.04768587108559014, 0.04554023305922861, 0.04340472725640127, 0.0412833167441012, 0.03917993843131212, 0.03709849576290149, 0.03504285147561585, 0.0330168204296226, 0.031024162528900796, 0.02906857574361952, 0.027153689247453035, 0.025283056682568184, 0.023460149564782956, 0.02168835084113523, 0.019970948611816863, 0.018311130028124675, 0.016711975377752014, 0.015176452368397552, 0.013707410620299974, 0.01230757637791884, 0.010979547450576093, 0.009725788391447353, 0.008548625923849328, 0.00745024462331193, 0.006432682863447724, 0.005497829033142642, 0.004647418032088068, 0.0038830280511577045, 0.003206077643604498, 0.00261782309251256, 0.002119356079389783, 0.0017116016582277064, 0.001395316538788304, 0.0011710876823037167, 0.001039331212194843, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.1.residual_function.3.weight
[0.1, 0.10095455656896336, 0.10081830888007043, 0.10059150459581934, 0.10027455598680413, 0.09986803918231502, 0.09937269312308682, 0.09878941821809879, 0.09811927470786777, 0.0973634807372094, 0.09652341014097103, 0.0956005899467609, 0.09459669759921321, 0.09351355791083453, 0.09235313974497411, 0.09111755243694773, 0.08980904195982026, 0.08842998684181681, 0.08698289384278345, 0.08547039339755634, 0.08389523483452253, 0.08226028137806324, 0.08056850494396438, 0.07882298073725438, 0.07702688166228973, 0.07518347255524829, 0.07329610424951521, 0.07136820748474822, 0.06940328667069429, 0.06740491351709367, 0.06537672054125002, 0.06332239446506836, 0.061245669513563555, 0.05915032062702017, 0.05704015659914318, 0.05491901315367165, 0.05279074597204089, 0.05065922368476674, 0.04852832083929181, 0.04640191085707624, 0.044283858992735355, 0.042178015308022315, 0.040088207673427495, 0.03801823481011584, 0.03597185938484981, 0.0339528011704499, 0.03196473028422514, 0.030011260516664084, 0.028095942762513315, 0.02622225856618378, 0.024393613793217885, 0.022613332439320885, 0.02088465058821002, 0.01921071052926473, 0.01759455504567013, 0.016039121883436723, 0.014547238411350149, 0.013121616481557529, 0.011764847500132895, 0.010479397716581858, 0.009267603740848, 0.00813166829596999, 0.007073656214109902, 0.006095490683230841, 0.005198949751246617, 0.004385663093997788, 0.0036571090529292187, 0.003014611947853784, 0.002459339669686916, 0.0019923015575277862, 0.0016143465639460557, 0.0013261617118091476, 0.0011282708454552252, 0.0010210336784818022, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.1.residual_function.4.weight
[0.1, 0.10095548539620526, 0.10082202084681907, 0.10059984399670291, 0.10028935045043497, 0.09989109306790296, 0.09940578097989058, 0.0988342783254104, 0.0981776027130317, 0.09743692340894275, 0.09661355925497395, 0.09570897632028902, 0.09472478529092546, 0.09366273860183245, 0.09252472731651308, 0.09131277775982655, 0.09002904790994622, 0.08867582355589804, 0.08725551422752056, 0.08577064890509459, 0.08422387151628076, 0.08261793622838394, 0.08095570254432659, 0.07924013021106305, 0.07747427394950122, 0.07566127801531478, 0.07380437060033139, 0.0719068580844655, 0.0699721191484307, 0.06800359875771432, 0.0660048020285266, 0.06397928798664612, 0.0619306632302751, 0.0598625755081877, 0.0577787072246064, 0.055682768882371336, 0.05357849247607782, 0.05146962484694589, 0.04935992101125422, 0.04725313747421758, 0.04515302554121328, 0.043063324638266225, 0.040987755653686155, 0.03893001431271299, 0.036893764596966944, 0.03488263222042093, 0.03290019817351183, 0.03094999234688548, 0.029035487246129556, 0.02716009180868495, 0.0253271453339459, 0.023539911537356448, 0.021801572739090325, 0.02011522419766216, 0.018483868598559213, 0.016910410707707168, 0.01539765219929002, 0.013948286667133318, 0.012564894828533762, 0.011249939929074904, 0.010005763356611037, 0.00883458047222908, 0.007738476665611848, 0.006719403641826099, 0.005779175946147524, 0.004919467733110105, 0.00414180978553314, 0.003447586788833611, 0.0028380348654772395, 0.002314239373958448, 0.001877132976228212, 0.0015274939770109734, 0.0012659449379675938, 0.0010929515691719119, 0.001008821899874756, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv4_x.1.residual_function.4.bias
[0.1, 0.10095639215246846, 0.1008256446756485, 0.10060798563418148, 0.10030379469375916, 0.09991360245886756, 0.09943808954724619, 0.09887808540267701, 0.09823456684817411, 0.0975086563820979, 0.09670162022016539, 0.09581486608677267, 0.09484994075948161, 0.09380852737095428, 0.09269244247304126, 0.09150363286814496, 0.09024417221338517, 0.08891625740349006, 0.08752220473872223, 0.08606444588452392, 0.08454552362992919, 0.08296808745214168, 0.08133488889501467, 0.07964877676949483, 0.07791269218440183, 0.07612966341621122, 0.07430280062678965, 0.07243529043829648, 0.07053039037471419, 0.06859142317970422, 0.06662177102069892, 0.06462486958933988, 0.06260420210855329, 0.0605632932567157, 0.058505703019508574, 0.056435020480185775, 0.054354857559085756, 0.052268842713308665, 0.050180614607548246, 0.04809381576711835, 0.046012086224245505, 0.043939057168710015, 0.041878344613911135, 0.03983354308940476, 0.03780821937091535, 0.035805906258759644, 0.033830096415533875, 0.03188423627381379, 0.029971720024494542, 0.028095883696256187, 0.02625999933648256, 0.02446726930378353, 0.02272082068207633, 0.021023699825969685, 0.01937886704696497, 0.01778919144974388, 0.01625744592754914, 0.014786302325388311, 0.013378326779497397, 0.012035975241193726, 0.010761589192925941, 0.00955739156399361, 0.00842548285306075, 0.007367837464226801, 0.006386300263046107, 0.00548258335850329, 0.004658263116557753, 0.0039147774104666185, 0.0032534231126824012, 0.002675353832700394, 0.002181577904801551, 0.0017729566292010272, 0.0014502027696701937, 0.001213879310252918, 0.001064398473244659, 0.0010020210001473841, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.0.residual_function.0.weight
[0.1, 0.10095727743500797, 0.10082918274873424, 0.1006159348425212, 0.10031789813627015, 0.0999355819456832, 0.09946963961189233, 0.09892086738496308, 0.0982902030631807, 0.09757872439044418, 0.09678764721450697, 0.0959183234092116, 0.0949722385642689, 0.0939510094465299, 0.09285638123708878, 0.09169022454893834, 0.09045453223027461, 0.08915141595891332, 0.08778310263363834, 0.08635193056864839, 0.0848603454976059, 0.08331089639411644, 0.08170623111578104, 0.08004909187926548, 0.07834231057411903, 0.07658880392335117, 0.07479156849903598, 0.07295367560146228, 0.07107826601058068, 0.0691685446187165, 0.06722777495372095, 0.06525927360191996, 0.06326640454039106, 0.061252573388254364, 0.05922122158680104, 0.0571758205184055, 0.05511986557427086, 0.05305687018114602, 0.05099035979722142, 0.04892386588746441, 0.04686091988868958, 0.04480504717467729, 0.04275976103165307, 0.04072855665442387, 0.0387149051734304, 0.036722247722923373, 0.03475398956040028, 0.03281349424735184, 0.03090407790126325, 0.029029003528692178, 0.027191475449108493, 0.025394633819024456, 0.02364154926577284, 0.021935217640104216, 0.020278554896569597, 0.018674392110438198, 0.017125470639665557, 0.015634437440179776, 0.014203840542491675, 0.012836124697358797, 0.011533627197944523, 0.010298573885611666, 0.009133075346176456, 0.008039123303122814, 0.0070185872139410625, 0.006073211075407075, 0.005204610443261664, 0.004414269671383185, 0.003703539375171338, 0.0030736341234769917, 0.0025256303630223293, 0.002060464578858211, 0.0016789316940024825, 0.0013816837109939463, 0.0011692285976835073, 0.0010419294191666198, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.0.residual_function.1.weight
[0.1, 0.10095814182296176, 0.10083263737612636, 0.10062369679498803, 0.1003316699144202, 0.09995704568293753, 0.09950045134403618, 0.09896265138598309, 0.09834454626181242, 0.09764717088167282, 0.09687169288004926, 0.09601941066076099, 0.09509175122300886, 0.09409026777211194, 0.09301663711893376, 0.09187265687235253, 0.09066024242947582, 0.08938142376863936, 0.08803834205055937, 0.08663324603332914, 0.08516848830726248, 0.083646521355888, 0.08206989344968958, 0.08044124437946809, 0.07876330103646811, 0.07703887284667006, 0.07527084706689233, 0.07346218395057881, 0.0716159117913665, 0.06973512185273148, 0.06782296319220266, 0.06588263738880953, 0.06391739318259172, 0.06193052103514554, 0.059925347620314956, 0.05790523025425161, 0.05587355127416937, 0.05383371237520556, 0.05178912891487067, 0.04974322419462279, 0.047699423728141274, 0.045661149505896385, 0.04363181426561801, 0.04161481577825647, 0.03961353115900254, 0.03763131121289209, 0.03567147482446247, 0.033737303400854145, 0.031832035377662035, 0.02995886079673491, 0.028120915965002064, 0.026321278203269644, 0.024562960693779026, 0.02284890743515432, 0.0211819883131856, 0.0195649942957015, 0.01800063275957618, 0.016491522957694876, 0.015040191633467863, 0.013649068790235596, 0.012320483622648214, 0.011056660616831925, 0.009859715825871657, 0.008731653326845983, 0.007674361865346684, 0.006689611693100838, 0.005779051603990359, 0.0049442061734318084, 0.004186473205738472, 0.0035071213937387206, 0.002907288194569268, 0.00238797792519989, 0.0019500600808783123, 0.0015942678793107615, 0.0013211970330157244, 0.0011313047519063248, 0.0010249089777713985, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.0.residual_function.1.bias
[0.1, 0.10095898587795911, 0.10083601079816469, 0.10063127650921398, 0.10034511889099147, 0.09997800740363627, 0.09953054431736218, 0.09900346372439434, 0.09839763033464335, 0.09771403805709261, 0.096953808369226, 0.0961181884771714, 0.09520854926957814, 0.09422638306858512, 0.09317330118156966, 0.09205103125769305, 0.09086141445358015, 0.08960640241278235, 0.08828805406397984, 0.08690853224317543, 0.08547010014542178, 0.08397511761190307, 0.08242603725846248, 0.08082540045192665, 0.07917583314082868, 0.07748004154736904, 0.07574080772768248, 0.07396098500769463, 0.07214349330205537, 0.07029131432382965, 0.06840748669280367, 0.06649510095043192, 0.0645572944896035, 0.06259724640754541, 0.06061817229030745, 0.05862331893738448, 0.056615959035131336, 0.05459938578770845, 0.05257690751436678, 0.05055184222193546, 0.048527512161416264, 0.04650723837761531, 0.04449433526075361, 0.042492105108994815, 0.04050383271081087, 0.038532779956073314, 0.03658218048471146, 0.034655234381716145, 0.032755102927192775, 0.030884903410076103, 0.02904770401401558, 0.02724651878382093, 0.025484302680726136, 0.023763946734583815, 0.022088273300943095, 0.020460031430792294, 0.018881892360562407, 0.01735644512979039, 0.015886192333631803, 0.014473546017191186, 0.01312082371840546, 0.011830244665972607, 0.010603926138563043, 0.009443879991286507, 0.008352009355113321, 0.007330105514664475, 0.006379844969493003, 0.005502786683677734, 0.004700369528241515, 0.003973909920589996, 0.0033245996648433966, 0.0027535039966045827, 0.0022615598353709345, 0.00184957424745708, 0.001518223121950301, 0.001268050061870595, 0.0010994654923546557, 0.0010127459873268232, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.0.residual_function.3.weight
[0.1, 0.10095981014470673, 0.10083930518780566, 0.10063867885236799, 0.10035825366412927, 0.09999848043299897, 0.09955993752834098, 0.0990433299491904, 0.09844948819090125, 0.09777936691004681, 0.09703404338971933, 0.09621471580769599, 0.09532270131025514, 0.09435943389473965, 0.09332646210427105, 0.09222544653832057, 0.09105815718313909, 0.08982647056633736, 0.08853236674019127, 0.0871779260985211, 0.08576532603226249, 0.0842968374291055, 0.08277482102282854, 0.0812017235981965, 0.07958007405752368, 0.07791247935522516, 0.07620162030689187, 0.07445024727962697, 0.0726611757705717, 0.07083728188072834, 0.06898149769135709, 0.06709680655037893, 0.06518623827636305, 0.06325286428780784, 0.06129979266554644, 0.05933016315621388, 0.05734714212480862, 0.05535391746446246, 0.053353693471602105, 0.051349685694740604, 0.04934511576518031, 0.047343206217937, 0.04534717531121121, 0.04336023185273508, 0.041385570041311444, 0.03942636433183848, 0.03748576433207434, 0.03556688973934591, 0.03367282532534152, 0.03180661597704978, 0.029971261801816824, 0.0281697133043913, 0.02640486664370978, 0.02467955897704857, 0.022996563899026087, 0.021358586982788075, 0.019768261430544082, 0.018228143840446614, 0.016740710096618924, 0.015308351388937928, 0.013933370368971208, 0.012617977448247684, 0.011364287244812886, 0.010174315183781146, 0.009049974257349925, 0.007993071949484542, 0.007005307330217469, 0.006088268324233237, 0.005243429158129769, 0.00447214799046035, 0.0037756647283657234, 0.003155099034306618, 0.0026114485261008007, 0.002145587173158458, 0.0017582638914940486, 0.0014501013397732567, 0.0012215949183304977, 0.0010731119727662155, 0.0010048912034042504, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.0.residual_function.4.weight
[0.1, 0.10096061515155315, 0.10084252265286404, 0.10064590854613925, 0.1003710825760506, 0.10001847770176531, 0.09958864941486485, 0.0990822748642267, 0.09850015178924823, 0.09784319726309285, 0.09711244624793854, 0.09630904996450465, 0.09543427407842585, 0.09448949670633003, 0.09347620624476183, 0.09239599902537175, 0.09125057680006482, 0.09004174406007114, 0.08877140519316115, 0.08744156148348486, 0.08605430795876082, 0.08461183008978213, 0.08311640034743917, 0.08157037462268248, 0.07997618851506666, 0.0783363534957215, 0.07665345295079558, 0.07493013811160544, 0.07316912387790178, 0.07137318454083286, 0.06954514941234313, 0.06768789836789223, 0.06580435730951666, 0.06389749355638157, 0.06197031117008396, 0.060025846222072424, 0.05806716201063843, 0.05609734423501492, 0.0541194961341843, 0.052136733598054645, 0.050152180258705645, 0.04816896256943751, 0.04619020487937534, 0.04421902451138866, 0.04225852685107961, 0.04031180045457744, 0.03838191218284594, 0.03647190237016913, 0.034584780034427234, 0.03272351813670797, 0.03089104889772179, 0.02909025917839918, 0.027323985931947292, 0.025595011734531277, 0.023906060401620452, 0.02225979269690603, 0.020658802140550176, 0.019105610923369938, 0.01760266593339323, 0.016152334901046245, 0.014756902669045231, 0.01341856759286918, 0.012139438077483983, 0.010921529255773905, 0.009766759813913565, 0.00867694896868132, 0.00765381360147615, 0.006698965553553204, 0.00581390908673875, 0.005000038513625309, 0.004258636000979959, 0.0035908695498266413, 0.0029977911553844432, 0.002480335149760631, 0.0020393167300095877, 0.0016754306738762871, 0.0013892502452476298, 0.0011812262910360185, 0.0010516865309178494, 0.0010008350410459323, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.0.residual_function.4.bias
[0.1, 0.10096140141103256, 0.10084566523817307, 0.10065297017154007, 0.10038361372144054, 0.10003801175903058, 0.09961669787423394, 0.09912032255190922, 0.09854965216753803, 0.09790556780398446, 0.09718906389115288, 0.09640124667064427, 0.09554333248778168, 0.09461664591364166, 0.09362261769999129, 0.09256278257028819, 0.09143877685015413, 0.09025233594098067, 0.08900529164056731, 0.08769956931492906, 0.08633718492564006, 0.08492024191730262, 0.08345092796994748, 0.08193151162137938, 0.08036433876468269, 0.07875182902629484, 0.07709647203023977, 0.07540082355428888, 0.07366750158398441, 0.0718991822706176, 0.07009859579940203, 0.06826852217422179, 0.06641178692546244, 0.0645312567475515, 0.0626298350729441, 0.060710457589387125, 0.05877608770738299, 0.056829711984851185, 0.0548743355160513, 0.05291297729188718, 0.05094866553875531, 0.048984433043134226, 0.0470233124691333, 0.04506833167623063, 0.04312250904442877, 0.04118884881404645, 0.039270336447341105, 0.037369934019123564, 0.03549057564348183, 0.03363516294367445, 0.03180656057218801, 0.0300075917878754, 0.028241034097003438, 0.02650961496494003, 0.024816007605101543, 0.023162826851662127, 0.021552625122397435, 0.01998788847789559, 0.018471032783219887, 0.017004399977949624, 0.01559025446035748, 0.014230779591306364, 0.012928074323263587, 0.011684149959636625, 0.01050092704943421, 0.009380232422047028, 0.008323796366726328, 0.00733324996111489, 0.006410122552955233, 0.00555583939886306, 0.00477171946381144, 0.004058973384723318, 0.0034187016013165145, 0.0028518926570868233, 0.002359421673052702, 0.0019420489966177892, 0.001600419027637461, 0.0013350592235017657, 0.0011463792847710365, 0.001034670522621344, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.0.shortcut.0.weight
[0.1, 0.100962169420389, 0.10084873492766615, 0.10065986817353573, 0.10039585495554888, 0.10005709478462806, 0.09964410028051784, 0.09915749639607695, 0.09859801947158489, 0.097966516120495, 0.09726394194831922, 0.09649136010658364, 0.09564993968404306, 0.09474095393758873, 0.09376577836552652, 0.09272588862614095, 0.09162285830469497, 0.09045835653224406, 0.08923414545986869, 0.08795207759214618, 0.086614092983898, 0.08522221630445377, 0.08377855377387473, 0.08228528997577264, 0.08074468455154728, 0.07915906878104467, 0.07753084205481031, 0.07586246824327593, 0.0741564719683734, 0.07241543478321849, 0.07064199126564476, 0.06883882503149927, 0.06700866467373294, 0.0651542796334304, 0.0632784760090275, 0.061384092310058175, 0.05947399516185588, 0.057551074967709816, 0.055618241535039636, 0.05367841967220728, 0.051734544762629266, 0.049789558322886465, 0.04784640355155285, 0.045908020875479405, 0.043977343500271895, 0.04205729297169609, 0.0401507747547272, 0.03826067383693265, 0.03638985036284221, 0.03454113530591086, 0.032717326184624154, 0.030921182829228445, 0.029155423205491672, 0.02742271930181463, 0.025725693085916157, 0.024066912537210706, 0.02244888776088251, 0.020874067189536105, 0.01934483387817137, 0.017863501898089492, 0.01643231283518659, 0.015053432397933941, 0.013728947140177788, 0.012460861303717607, 0.011251093785441008, 0.010101475233604402, 0.009013745277653706, 0.00798954989577668, 0.007030438924170556, 0.00613786371179405, 0.005313174924152369, 0.004557620499438902, 0.003872343760126267, 0.00325838168286413, 0.0027166633293021648, 0.0022480084402124395, 0.0018531261950386116, 0.0015326141387491934, 0.0012869572776186768, 0.0011165273453047234, 0.0010215822403322407, 0, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.0.shortcut.1.weight
[0.1, 0.10096291966208122, 0.10085173364638331, 0.1006666068655076, 0.10040781390199778, 0.10007573860107542, 0.09967087350131541, 0.09919381910410614, 0.09864528298297782, 0.09802607873411973, 0.09733712476964336, 0.0965794429553813, 0.09575415709524182, 0.09486249126436788, 0.09390576799357225, 0.09288540630774199, 0.09180291962112144, 0.09065991349259594, 0.08945808324430507, 0.0881992114471179, 0.08688516527669987, 0.0855178937440926, 0.08409942480491431, 0.08263186235146869, 0.08111738309222336, 0.07955823332328647, 0.07795672559666995, 0.07631523529028111, 0.07463619708472997, 0.07292210135217818, 0.07117549046258524, 0.06939895501283114, 0.06759512998430788, 0.06576669083467968, 0.06391634952960777, 0.06204685052032649, 0.06016096667303592, 0.0582614951561494, 0.056351253291495464, 0.05443307437562801, 0.052509803477442635, 0.05058429321833187, 0.04865939954113866, 0.046737977474183366, 0.04482287689664707, 0.04291693831159227, 0.04102298863289047, 0.039143836992305135, 0.03728227057294963, 0.03544105047529947, 0.03362290762189077, 0.03183053870677912, 0.030066602195766418, 0.02833371438332847, 0.026634445512091733, 0.02497131596061458, 0.023346792505127896, 0.021763284660779134, 0.02022314110780689, 0.018728646207946703, 0.01728201661623473, 0.015885397993235146, 0.014540861822567346, 0.013250402338453392, 0.01201593356784298, 0.010839286491502665, 0.0097222063282805, 0.008666349946573913, 0.007673283406839964, 0.006744479638793432, 0.005881316256737506, 0.00508507351626759, 0.004356932415378994, 0.003697972942794641, 0.0031091724761112484, 0.0025914043321396954, 0.0021454364715896394, 0.0017719303600199012, 0.0014714399867437492, 0.0012444110431444712, 0.0010911802616199108, 0.0010119749161364147, 0, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.0.shortcut.1.bias
[0.1, 0.10096365260426954, 0.10085466326240518, 0.10067319043355649, 0.10041949796031246, 0.10009395468510185, 0.09969703391393472, 0.09922931272826478, 0.09869147114597313, 0.09808429113269301, 0.09740865546491338, 0.09666554644651365, 0.09585604448059631, 0.0949813264986934, 0.09404266424963051, 0.09304142245053662, 0.09197905680268746, 0.09085711187506756, 0.08967721885872777, 0.08844109319520353, 0.08715053208244135, 0.08580741186186049, 0.08441368529034776, 0.08297137870115256, 0.08148258905780936, 0.07994948090537124, 0.07837428322338676, 0.07675928618519592, 0.07510683782825678, 0.07341934064034349, 0.07169924806657935, 0.06994906094238325, 0.06817132385751541, 0.06636862145650894, 0.06454357468086604, 0.06269883695848191, 0.06083709034583711, 0.05896104162856686, 0.0570734183860766, 0.05517696502592572, 0.053274438793744855, 0.051368605764487964, 0.04946223682084744, 0.04755810362467941, 0.045658974587296085, 0.043767610844484016, 0.041886762242100445, 0.04001916333808376, 0.03816752942669118, 0.03633455259074372, 0.034522897787618176, 0.03273519897467684, 0.030974055279767727, 0.029242027222363538, 0.027541632990833023, 0.0258753447812574, 0.024245585203114886, 0.022654723757058754, 0.021105073389910273, 0.019598887131875003, 0.018138354820871557, 0.01672559991873529, 0.015362676423926174, 0.014051565885228816, 0.01279417452078711, 0.01159233044666148, 0.010447781018938684, 0.009362190293258032, 0.008337136605447815, 0.00737411027678948, 0.006474511447245684, 0.005639648039802657, 0.004870733858886461, 0.004168886825617767, 0.003535127352470973, 0.00297037685970084, 0.0024754564356933343, 0.0020510856431887204, 0.0016978814731122664, 0.001416357447533705, 0.0012069228730596617, 0.001069882245744446, 0.0010054348083845524, 0, 0, 0, 0, 0, 0, 0]
module.conv5_x.1.residual_function.0.weight
[0.1, 0.10096436870128489, 0.10085752558871751, 0.10067962294065219, 0.1004309143131848, 0.10011175417877125, 0.09972259742101502, 0.09926399868634343, 0.0987366115934967, 0.09814118780195656, 0.09747857594064217, 0.09674972039840002, 0.09595565997801196, 0.09509752641563944, 0.0941765427678144, 0.09319402166827553, 0.09215136345713451, 0.09105005418503848, 0.08989166349517351, 0.0886778423861276, 0.08741032085880163, 0.08609090545072236, 0.08472147666127117, 0.08330398627149867, 0.08184045456234504, 0.08033296743523058, 0.0787836734391207, 0.07719478070830235, 0.07556855381523622, 0.07390731054297053, 0.07221341858171619, 0.07048929215429171, 0.0687373885752474, 0.06696020474857306, 0.0651602736089807, 0.06334016051183437, 0.061502459576872476, 0.05964978999093348, 0.0577847922749549, 0.05591012452056557, 0.0540284586016354, 0.052142476366181995, 0.05025486581406144, 0.04836831726589138, 0.046485519528666225, 0.04460915606352975, 0.04274190116116696, 0.04088641613026599, 0.03904534550448272, 0.0372213132733141, 0.035416919142251875, 0.03363473482754743, 0.03187730039086804, 0.030147120619069146, 0.02844666145424195, 0.026778346479124596, 0.025144553462886323, 0.023547610972207086, 0.02198979505248355, 0.02047332598389088, 0.019000365116924073, 0.017573011791928994, 0.016193300347013447, 0.014863197218602804, 0.01358459813877267, 0.01235932543335308, 0.011189125424654962, 0.01007566594252102, 0.009020533947247809, 0.008025233267767667, 0.007091182458313461, 0.006219712776621433, 0.0054120662865536065, 0.004669394087843703, 0.0039927546754901224, 0.003383112431133816, 0.002841336248571507, 0.0023681982953630987, 0.001964372912298367, 0.0016304356522914114, 0.0013668624600727754, 0.0011740289938482686, 0.0010522100898913748, 0.0010015793708323443, 0, 0, 0, 0, 0, 0]
module.conv5_x.1.residual_function.1.weight
[0.1, 0.10096506839408129, 0.10086032238500882, 0.10068590833063501, 0.10044206993348044, 0.10012914790021699, 0.09974757946561064, 0.09929789778158891, 0.09878073117228682, 0.098196802256112, 0.09754692693605582, 0.09683201325966133, 0.09605306015024091, 0.09521115601111649, 0.09430747720483242, 0.09334328640946621, 0.09231993085433365, 0.09123884043755343, 0.09010152572810154, 0.08890957585514749, 0.08766465628762073, 0.08636850650711059, 0.08502293757735085, 0.08362982961368513, 0.08219112915604904, 0.08070884644913935, 0.07918505263357124, 0.07762187685194738, 0.07602150327388313, 0.07438616804414462, 0.07271815615816343, 0.0710197982692943, 0.06929346743227659, 0.0675415757874496, 0.06576657119035534, 0.06397093379143741, 0.06215717257061546, 0.060327821831577366, 0.05848543766068725, 0.056632594355457436, 0.054771880827574615, 0.052905896985506164, 0.05103725010174092, 0.04916855116974072, 0.04730241125569247, 0.04544143785015883, 0.043588231224724924, 0.041745380798731746, 0.03991546152117325, 0.03810103027281208, 0.03630462229354159, 0.03452874763998586, 0.03277588767828724, 0.031048491616982244, 0.02934897308481005, 0.027679706758235296, 0.026043025043397772, 0.02444121481712454, 0.02287651423155884, 0.021351109586870084, 0.01986713227641488, 0.01842665580861725, 0.017031692909729557, 0.015684192711522, 0.014386038027830544, 0.013139042723768472, 0.011944949181277538, 0.010805425864560021, 0.0097220649887933, 0.008696380295384571, 0.007729804936874118, 0.006823689474442501, 0.005979299990819758, 0.00519781632123325, 0.00448033040486605, 0.003827844759129511, 0.003241271078881396, 0.002721428962547333, 0.0022690447669252373, 0.0018847505922729092, 0.0015690833990969978, 0.001322484257877237, 0.0011452977327744576, 0.0010377714001834283, 0.0010000555028031868, 0, 0, 0, 0, 0]
module.conv5_x.1.residual_function.1.bias
[0.1, 0.10096575211067174, 0.10086305535940392, 0.10069205043207537, 0.10045297159099915, 0.10014614635400287, 0.09977199504575689, 0.09933103022196509, 0.09882385596720678, 0.09825116706739187, 0.09761374805796275, 0.09691247214914706, 0.09614830002973335, 0.09532227855100853, 0.09443553929266008, 0.09348929701260729, 0.09248484798288559, 0.09142356821386317, 0.0903069115692231, 0.08913640777429269, 0.08791366032044903, 0.08664034426847114, 0.08531820395384816, 0.08394905059718694, 0.08253475982299277, 0.08107726909022216, 0.07957857503812762, 0.07804073075103048, 0.07646584294576869, 0.07485606908567258, 0.07321361442502235, 0.07154072898803576, 0.06983970448652503, 0.06811287118044501, 0.06636259468563395, 0.06459127273311935, 0.06280133188442898, 0.06099522420740644, 0.05917542391708526, 0.05734442398622313, 0.05550473273013971, 0.05365887037053651, 0.051809365583005744, 0.04995875203295849, 0.048109564904716774, 0.04626433742852519, 0.04442559741023938, 0.04259586376844529, 0.040777643083753534, 0.03897342616499544, 0.03718568463702531, 0.035416867554803055, 0.03366939804839563, 0.03194567000349342, 0.030248044781989003, 0.028578847987110634, 0.026940366277542246, 0.025334844234894058, 0.023764481288815305, 0.02223142870396129, 0.020737786632942443, 0.019285601239292478, 0.01787686189439731, 0.01651349845222392, 0.01519737860558364, 0.013930305327550442, 0.01271401440154022, 0.01155017204343397, 0.01044037261900273, 0.00938613645976108, 0.008388907780241334, 0.007450052699541503, 0.006570857369857596, 0.005752526214563535, 0.004996180278252953, 0.004302855691002665, 0.00367350224896199, 0.003108982113212171, 0.0026100686286785295, 0.0021774452647131673, 0.0018117046787996796, 0.001513347904662437, 0.0012827836658926774, 0.0011203278160316957, 0.00102620290587817, 0, 0, 0, 0, 0]
module.conv5_x.1.residual_function.3.weight
[0.1, 0.1009664202665489, 0.10086572617013555, 0.10069805296199663, 0.10046362585899761, 0.10016275974112379, 0.09979585872853686, 0.09936341563876505, 0.09886601132475588, 0.09830431389468058, 0.09767907781453847, 0.09699114289476612, 0.09624143316221308, 0.09543095561899871, 0.09456079888991778, 0.09363213176021123, 0.0926462016056663, 0.09160433271715458, 0.0905079245218588, 0.08935844970357701, 0.08815745222462945, 0.0869065452520249, 0.08560740899067185, 0.08426178842654518, 0.08287149098283969, 0.0814383840922585, 0.07996439268869728, 0.07845149662169369, 0.07690172799711448, 0.07531716844765263, 0.07369994633680048, 0.07205223390005455, 0.07037624432719207, 0.06867422878953798, 0.0669484734162156, 0.0652012962234425, 0.06343504400099567, 0.06165208916002858, 0.05985482654647385, 0.05804567022431167, 0.056227050233024815, 0.054401409323595544, 0.05257119967742849, 0.05073887961260677, 0.048906910281905365, 0.04707775236699686, 0.04525386277329017, 0.043437691329841335, 0.04163167749876926, 0.03983824709859621, 0.038059809045914175, 0.03629875211975395, 0.0345574417530024, 0.0328382168551784, 0.03114338667083439, 0.029475227677803764, 0.02783598052946016, 0.026227847045095627, 0.0246529872524605, 0.023113516486437204, 0.02161150254774486, 0.02014896292549151, 0.018727862087304255, 0.017350108840677204, 0.016017553769081715, 0.014731986746282306, 0.013495134532197306, 0.012308658453533454, 0.011174152172309427, 0.010093139545266132, 0.009067072577038492, 0.008097329469838339, 0.007185212772267929, 0.0063319476297504776, 0.005538680138927931, 0.004806475808236199, 0.004136318126725524, 0.003529107243048512, 0.00298565875639008, 0.002506702620963268, 0.0020928821655425917, 0.0017447532293516192, 0.0014627834154657062, 0.0012473514627325201, 0.0010987467370540563, 0.001017168842713411, 0, 0, 0, 0]
module.conv5_x.1.residual_function.4.weight
[0.1, 0.1009670732650905, 0.10086833642715681, 0.10070391952946652, 0.10047403912048379, 0.10017899796866013, 0.09981918466366729, 0.09939507310459768, 0.09890722187580639, 0.09835627351121687, 0.09774295364805907, 0.097068070071155, 0.0963325116490099, 0.0955372471631108, 0.09468332403197412, 0.09377186693162276, 0.09280407631430969, 0.09178122682743907, 0.09070466563476719, 0.08957581064209423, 0.08839614862978398, 0.08716723329457095, 0.08589068320323388, 0.08456817966083102, 0.08320146449630467, 0.0817923377683716, 0.0803426553947208, 0.07885432670764105, 0.07732931193929789, 0.07576961963997171, 0.07417730403265775, 0.07255446230751177, 0.0709032318597051, 0.06922578747432695, 0.06752433846204145, 0.06580112574927215, 0.06405841892674649, 0.062298513260287305, 0.06052372666778841, 0.05873639666635584, 0.056938877293635774, 0.0551335360073834, 0.05332275056735685, 0.05150890590364263, 0.04969439097553707, 0.04788159562512126, 0.04607290742967316, 0.04427070855706256, 0.042477372628270796, 0.04069526159116709, 0.03892672260965935, 0.037174084972316446, 0.03543965702453346, 0.03372572312828066, 0.032034540653440205, 0.030368337004693404, 0.028729306687874163, 0.02711960841965251, 0.025541362284354892, 0.023996646941666113, 0.022487496888890124, 0.021015899781375975, 0.0195837938146378, 0.018193065171616736, 0.016845545538447203, 0.015543009691999059, 0.014287173162373334, 0.013079689973430082, 0.011922150464324079, 0.0108160791949179, 0.00976293293783067, 0.008764098759767525, 0.007820892194656431, 0.00693455551099879, 0.006106256075715521, 0.005337084816643877, 0.0046280547857097554, 0.003980099824668009, 0.0033940733351680735, 0.0028707471547647183, 0.0024108105403545014, 0.0020148692603765223, 0.0016834447969734036, 0.0014169736591631002, 0.0012158068079262609, 0.001080209193966202, 0.0010103594087504933, 0, 0, 0]
module.conv5_x.1.residual_function.4.bias
[0.1, 0.10096771149795047, 0.10087088769369651, 0.10070965363906215, 0.10048421757429156, 0.10019487065909832, 0.09984198659662098, 0.09942602115077054, 0.09894751155759303, 0.09840707583140765, 0.09780541196661649, 0.09714329703621682, 0.09642158618818014, 0.09564121154099425, 0.09480318097979501, 0.0939085768546421, 0.09295855458262066, 0.09195434115557378, 0.09089723355539336, 0.08978859707891601, 0.08862986357458742, 0.08742252959317279, 0.08616815445490145, 0.08486835823554228, 0.08352481967401104, 0.08213927400421162, 0.08071351071391197, 0.07924937123354894, 0.07774874655794703, 0.07621357480402279, 0.0746458387076291, 0.07304756306277248, 0.07142081210651029, 0.06976768685290592, 0.06809032237948497, 0.06639088506969694, 0.06467156981494424, 0.06293459717979193, 0.06118221053401965, 0.05941667315521948, 0.05764026530568243, 0.05585528128734812, 0.05406402647862188, 0.05226881435688627, 0.05047196351055214, 0.048675794644508864, 0.04688262758284099, 0.04509477827268229, 0.04331455579307743, 0.04154425937271364, 0.03978617542037477, 0.03804257457195282, 0.036315708757830446, 0.03460780829442284, 0.032921079003634494, 0.03125769936395191, 0.02961981769685131, 0.02800954939215529, 0.026428974175922073, 0.024880133424396062, 0.02336502752748842, 0.02188561330519348, 0.020443801480277152, 0.019041454210502035, 0.017680382683575867, 0.01636234477793011, 0.015089042792349506, 0.013862121247384868, 0.012683164761388987, 0.011553696003918416, 0.010475173729144804, 0.009448990891815296, 0.00847647284819577, 0.007558875644320001, 0.0066973843937559, 0.005893111746983859, 0.005147096454364019, 0.004460302024548562, 0.003833615480071694, 0.0032678462117244227, 0.0027637249331939886, 0.0023219027373177926, 0.0019429502551709102, 0.0016273569190731964, 0.0013755303304678382, 0.0011877957334877223, 0.0010643955948896268, 0.0010054892908986693, 0, 0]
module.fc.weight
[0.1, 0.1009683353454364, 0.10087338148775954, 0.1007152586942135, 0.10049416724094379, 0.10021038715933013, 0.09986427788130284, 0.09945627778409108, 0.09898690363498047, 0.0984567499367827, 0.09786648817484656, 0.09721686596656376, 0.09650870611444724, 0.09574290556398068, 0.09492043426755992, 0.09404233395596451, 0.09310971681891601, 0.09212376409639385, 0.09108572458249296, 0.08999691304371864, 0.08885870855372124, 0.08767255274658074, 0.0864399479908528, 0.08516245548668955, 0.0838416932884451, 0.08247933425527013, 0.08107710393229173, 0.07963677836506176, 0.07816018185004203, 0.07664918462397563, 0.07510570049507098, 0.07353168441899874, 0.07192913002277193, 0.0703000670796456, 0.06864655893823404, 0.06697069990910184, 0.06527461261213922, 0.0635604452880809, 0.06183036907757408, 0.06008657527124176, 0.058331272534224224, 0.0565666841087144, 0.054795044998029854, 0.05301859913578869, 0.05123959654377405, 0.04946029048208766, 0.0476829345952018, 0.04590978005752424, 0.044143072722091954, 0.04238505027600457, 0.04063793940620089, 0.03890395297916802, 0.03718528723815513, 0.03548411902144224, 0.033802603005186775, 0.03214286897434065, 0.030507019125094077, 0.02889712540226279, 0.02731522687499151, 0.02576332715409706, 0.024243391854322623, 0.022757346104717226, 0.021307072110294013, 0.019894406768055147, 0.01852113934040372, 0.01718900918888854, 0.0158997035711528, 0.014654855503877066, 0.013456041694422698, 0.01230478054379619, 0.011202530223463424, 0.010150686828449839, 0.009150582609065764, 0.00820348428349657, 0.007310591433394941, 0.006473034984507297, 0.005691875774258914, 0.004968103208111845, 0.004302634006397687, 0.0036963110432122297, 0.0031499022788428134, 0.0026640997870804196, 0.0022395188786485326, 0.0018766973218591544, 0.0015760946614828567, 0.0013380916366956977, 0.0011629896988402714, 0.0010510106296116142, 0.001002296260151541, 0.0010169082914072915]
module.fc.bias
[0.1, 0.10096894517687327, 0.10087581928357461, 0.10072073800042984, 0.10050389396831168, 0.10022555654934269, 0.09988607149229549, 0.0994858605031061, 0.09902542072103382, 0.0985053241011184, 0.09792621670370139, 0.09728881789189477, 0.09659391943799299, 0.09584238453993914, 0.0950351467490665, 0.09417320881044779, 0.09325764141729216, 0.09228958188093785, 0.09127023271809194, 0.09020086015707271, 0.08908279256490971, 0.08791741879725568, 0.08670618647315986, 0.08545060017684587, 0.08415221958872779, 0.08281265754798622, 0.08143357804911089, 0.08001669417489839, 0.0785637659684729, 0.0770765982469731, 0.07555703835962105, 0.074006973892958, 0.07242833032609784, 0.07082306863891055, 0.06919318287610683, 0.06754069767024994, 0.06586766572677132, 0.06417616527411459, 0.06246829748217482, 0.06074618385224009, 0.059011963581677686, 0.05726779090663821, 0.05551583242607867, 0.053758264410428684, 0.05199727009824258, 0.05023503698419604, 0.04847375410179569, 0.046715609304177064, 0.04496278654636904, 0.04321746317240024, 0.041481807210617776, 0.039757974680577735, 0.038048106914852935, 0.036354327899084764, 0.034678741633583254, 0.03302342951975273, 0.0313904477745898, 0.029781824876465048, 0.028199559045361634, 0.02664561576070057, 0.025121925319835987, 0.023630380440253514, 0.022172833908450026, 0.020751096278415256, 0.019366933622574575, 0.018022065337986188, 0.016718162010518355, 0.015456843339659458, 0.014239676126538675, 0.013068172327656787, 0.011943787176744453, 0.010867917377081166, 0.009841899366520402, 0.008867007657375858, 0.007944453253231325, 0.007075382144640651, 0.006260873885586288, 0.005501940252465059, 0.004799523987266617, 0.004154497626506054, 0.003567662417365174, 0.0030397473223889742, 0.0025714081139734607, 0.002163226559769859, 0.0018157097000169424, 0.0015292892176992472, 0.0013043209023134985, 0.001141084207909431, 0.0010397819059539506, 0.0010005398334498364]
*** Model named parameters and requires_grad:
name:          module.conv1.0.weight  req_grad:  True 
name:          module.conv1.1.weight  req_grad:  True 
name:            module.conv1.1.bias  req_grad:  True 
name: module.conv2_x.0.residual_function.0.weight  req_grad:  True 
name: module.conv2_x.0.residual_function.1.weight  req_grad:  True 
name: module.conv2_x.0.residual_function.1.bias  req_grad:  True 
name: module.conv2_x.0.residual_function.3.weight  req_grad:  True 
name: module.conv2_x.0.residual_function.4.weight  req_grad:  True 
name: module.conv2_x.0.residual_function.4.bias  req_grad:  True 
name: module.conv2_x.1.residual_function.0.weight  req_grad:  True 
name: module.conv2_x.1.residual_function.1.weight  req_grad:  True 
name: module.conv2_x.1.residual_function.1.bias  req_grad:  True 
name: module.conv2_x.1.residual_function.3.weight  req_grad:  True 
name: module.conv2_x.1.residual_function.4.weight  req_grad:  True 
name: module.conv2_x.1.residual_function.4.bias  req_grad:  True 
name: module.conv3_x.0.residual_function.0.weight  req_grad:  True 
name: module.conv3_x.0.residual_function.1.weight  req_grad:  True 
name: module.conv3_x.0.residual_function.1.bias  req_grad:  True 
name: module.conv3_x.0.residual_function.3.weight  req_grad:  True 
name: module.conv3_x.0.residual_function.4.weight  req_grad:  True 
name: module.conv3_x.0.residual_function.4.bias  req_grad:  True 
name: module.conv3_x.0.shortcut.0.weight  req_grad:  True 
name: module.conv3_x.0.shortcut.1.weight  req_grad:  True 
name: module.conv3_x.0.shortcut.1.bias  req_grad:  True 
name: module.conv3_x.1.residual_function.0.weight  req_grad:  True 
name: module.conv3_x.1.residual_function.1.weight  req_grad:  True 
name: module.conv3_x.1.residual_function.1.bias  req_grad:  True 
name: module.conv3_x.1.residual_function.3.weight  req_grad:  True 
name: module.conv3_x.1.residual_function.4.weight  req_grad:  True 
name: module.conv3_x.1.residual_function.4.bias  req_grad:  True 
name: module.conv4_x.0.residual_function.0.weight  req_grad:  True 
name: module.conv4_x.0.residual_function.1.weight  req_grad:  True 
name: module.conv4_x.0.residual_function.1.bias  req_grad:  True 
name: module.conv4_x.0.residual_function.3.weight  req_grad:  True 
name: module.conv4_x.0.residual_function.4.weight  req_grad:  True 
name: module.conv4_x.0.residual_function.4.bias  req_grad:  True 
name: module.conv4_x.0.shortcut.0.weight  req_grad:  True 
name: module.conv4_x.0.shortcut.1.weight  req_grad:  True 
name: module.conv4_x.0.shortcut.1.bias  req_grad:  True 
name: module.conv4_x.1.residual_function.0.weight  req_grad:  True 
name: module.conv4_x.1.residual_function.1.weight  req_grad:  True 
name: module.conv4_x.1.residual_function.1.bias  req_grad:  True 
name: module.conv4_x.1.residual_function.3.weight  req_grad:  True 
name: module.conv4_x.1.residual_function.4.weight  req_grad:  True 
name: module.conv4_x.1.residual_function.4.bias  req_grad:  True 
name: module.conv5_x.0.residual_function.0.weight  req_grad:  True 
name: module.conv5_x.0.residual_function.1.weight  req_grad:  True 
name: module.conv5_x.0.residual_function.1.bias  req_grad:  True 
name: module.conv5_x.0.residual_function.3.weight  req_grad:  True 
name: module.conv5_x.0.residual_function.4.weight  req_grad:  True 
name: module.conv5_x.0.residual_function.4.bias  req_grad:  True 
name: module.conv5_x.0.shortcut.0.weight  req_grad:  True 
name: module.conv5_x.0.shortcut.1.weight  req_grad:  True 
name: module.conv5_x.0.shortcut.1.bias  req_grad:  True 
name: module.conv5_x.1.residual_function.0.weight  req_grad:  True 
name: module.conv5_x.1.residual_function.1.weight  req_grad:  True 
name: module.conv5_x.1.residual_function.1.bias  req_grad:  True 
name: module.conv5_x.1.residual_function.3.weight  req_grad:  True 
name: module.conv5_x.1.residual_function.4.weight  req_grad:  True 
name: module.conv5_x.1.residual_function.4.bias  req_grad:  True 
name:               module.fc.weight  req_grad:  True 
name:                 module.fc.bias  req_grad:  True 


*** Optimizer groups, parameters and req_grads
#        requires_grad:                            True
#           param_name:           module.conv1.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:           module.conv1.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:             module.conv1.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.0.residual_function.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.0.residual_function.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.0.residual_function.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.0.residual_function.3.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.0.residual_function.4.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.0.residual_function.4.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.1.residual_function.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.1.residual_function.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.1.residual_function.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.1.residual_function.3.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.1.residual_function.4.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv2_x.1.residual_function.4.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.0.residual_function.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.0.residual_function.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.0.residual_function.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.0.residual_function.3.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.0.residual_function.4.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.0.residual_function.4.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.0.shortcut.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.0.shortcut.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.0.shortcut.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.1.residual_function.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.1.residual_function.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.1.residual_function.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.1.residual_function.3.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.1.residual_function.4.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv3_x.1.residual_function.4.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.0.residual_function.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.0.residual_function.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.0.residual_function.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.0.residual_function.3.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.0.residual_function.4.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.0.residual_function.4.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.0.shortcut.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.0.shortcut.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.0.shortcut.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.1.residual_function.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.1.residual_function.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.1.residual_function.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.1.residual_function.3.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.1.residual_function.4.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv4_x.1.residual_function.4.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.0.residual_function.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.0.residual_function.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.0.residual_function.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.0.residual_function.3.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.0.residual_function.4.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.0.residual_function.4.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.0.shortcut.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.0.shortcut.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.0.shortcut.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.1.residual_function.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.1.residual_function.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.1.residual_function.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.1.residual_function.3.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.1.residual_function.4.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:  module.conv5_x.1.residual_function.4.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:                module.fc.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:                  module.fc.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False



*** Optimizer group lrs
# group:  0,   name:          module.conv1.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  1,   name:          module.conv1.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  2,   name:            module.conv1.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  3,   name: module.conv2_x.0.residual_function.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  4,   name: module.conv2_x.0.residual_function.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  5,   name: module.conv2_x.0.residual_function.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  6,   name: module.conv2_x.0.residual_function.3.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  7,   name: module.conv2_x.0.residual_function.4.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  8,   name: module.conv2_x.0.residual_function.4.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  9,   name: module.conv2_x.1.residual_function.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 10,   name: module.conv2_x.1.residual_function.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 11,   name: module.conv2_x.1.residual_function.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 12,   name: module.conv2_x.1.residual_function.3.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 13,   name: module.conv2_x.1.residual_function.4.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 14,   name: module.conv2_x.1.residual_function.4.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 15,   name: module.conv3_x.0.residual_function.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 16,   name: module.conv3_x.0.residual_function.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 17,   name: module.conv3_x.0.residual_function.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 18,   name: module.conv3_x.0.residual_function.3.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 19,   name: module.conv3_x.0.residual_function.4.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 20,   name: module.conv3_x.0.residual_function.4.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 21,   name: module.conv3_x.0.shortcut.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 22,   name: module.conv3_x.0.shortcut.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 23,   name: module.conv3_x.0.shortcut.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 24,   name: module.conv3_x.1.residual_function.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 25,   name: module.conv3_x.1.residual_function.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 26,   name: module.conv3_x.1.residual_function.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 27,   name: module.conv3_x.1.residual_function.3.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 28,   name: module.conv3_x.1.residual_function.4.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 29,   name: module.conv3_x.1.residual_function.4.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 30,   name: module.conv4_x.0.residual_function.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 31,   name: module.conv4_x.0.residual_function.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 32,   name: module.conv4_x.0.residual_function.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 33,   name: module.conv4_x.0.residual_function.3.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 34,   name: module.conv4_x.0.residual_function.4.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 35,   name: module.conv4_x.0.residual_function.4.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 36,   name: module.conv4_x.0.shortcut.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 37,   name: module.conv4_x.0.shortcut.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 38,   name: module.conv4_x.0.shortcut.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 39,   name: module.conv4_x.1.residual_function.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 40,   name: module.conv4_x.1.residual_function.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 41,   name: module.conv4_x.1.residual_function.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 42,   name: module.conv4_x.1.residual_function.3.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 43,   name: module.conv4_x.1.residual_function.4.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 44,   name: module.conv4_x.1.residual_function.4.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 45,   name: module.conv5_x.0.residual_function.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 46,   name: module.conv5_x.0.residual_function.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 47,   name: module.conv5_x.0.residual_function.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 48,   name: module.conv5_x.0.residual_function.3.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 49,   name: module.conv5_x.0.residual_function.4.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 50,   name: module.conv5_x.0.residual_function.4.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 51,   name: module.conv5_x.0.shortcut.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 52,   name: module.conv5_x.0.shortcut.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 53,   name: module.conv5_x.0.shortcut.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 54,   name: module.conv5_x.1.residual_function.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 55,   name: module.conv5_x.1.residual_function.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 56,   name: module.conv5_x.1.residual_function.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 57,   name: module.conv5_x.1.residual_function.3.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 58,   name: module.conv5_x.1.residual_function.4.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 59,   name: module.conv5_x.1.residual_function.4.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 60,   name:               module.fc.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 61,   name:                 module.fc.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
---------------
# Model: resnet18
# Dataset: cifarcentum
# Freezeout: True
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	param_name module.conv1.0.weight
#	lr 0.1
#	momentum 0.9
#	weight_decay 0.0001
#	dampening 0
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
i:   0, name:          module.conv1.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   1, name:          module.conv1.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   2, name:            module.conv1.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  60, name:               module.fc.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  61, name:                 module.fc.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006



# Switched to train mode...
Epoch: [0][  0/391]	Time  3.225 ( 3.225)	Data  0.113 ( 0.113)	Loss 4.7114e+00 (4.7114e+00)	Acc@1   1.56 (  1.56)	Acc@5   3.91 (  3.91)
Epoch: [0][ 10/391]	Time  0.046 ( 0.335)	Data  0.001 ( 0.012)	Loss 4.9614e+00 (4.9284e+00)	Acc@1   2.34 (  2.49)	Acc@5   7.81 (  7.67)
Epoch: [0][ 20/391]	Time  0.046 ( 0.198)	Data  0.001 ( 0.007)	Loss 4.5541e+00 (4.8704e+00)	Acc@1   3.12 (  2.83)	Acc@5  14.84 ( 10.38)
Epoch: [0][ 30/391]	Time  0.046 ( 0.149)	Data  0.001 ( 0.006)	Loss 4.7215e+00 (4.7942e+00)	Acc@1   3.91 (  3.12)	Acc@5  16.41 ( 11.47)
Epoch: [0][ 40/391]	Time  0.046 ( 0.124)	Data  0.001 ( 0.005)	Loss 4.3643e+00 (4.7044e+00)	Acc@1   5.47 (  3.35)	Acc@5  17.97 ( 12.69)
Epoch: [0][ 50/391]	Time  0.047 ( 0.109)	Data  0.001 ( 0.004)	Loss 4.5266e+00 (4.6366e+00)	Acc@1   4.69 (  3.54)	Acc@5  12.50 ( 13.73)
Epoch: [0][ 60/391]	Time  0.047 ( 0.098)	Data  0.001 ( 0.004)	Loss 4.3829e+00 (4.5762e+00)	Acc@1   3.91 (  3.56)	Acc@5  15.62 ( 14.22)
Epoch: [0][ 70/391]	Time  0.047 ( 0.091)	Data  0.001 ( 0.004)	Loss 4.2228e+00 (4.5243e+00)	Acc@1   6.25 (  3.76)	Acc@5  24.22 ( 15.15)
Epoch: [0][ 80/391]	Time  0.046 ( 0.086)	Data  0.001 ( 0.004)	Loss 3.9939e+00 (4.4726e+00)	Acc@1   8.59 (  4.06)	Acc@5  32.81 ( 16.23)
Epoch: [0][ 90/391]	Time  0.046 ( 0.081)	Data  0.001 ( 0.004)	Loss 3.9785e+00 (4.4315e+00)	Acc@1   7.03 (  4.22)	Acc@5  24.22 ( 16.93)
Epoch: [0][100/391]	Time  0.045 ( 0.078)	Data  0.001 ( 0.004)	Loss 4.0953e+00 (4.3935e+00)	Acc@1   7.03 (  4.53)	Acc@5  28.12 ( 17.67)
Epoch: [0][110/391]	Time  0.046 ( 0.075)	Data  0.001 ( 0.003)	Loss 3.9865e+00 (4.3610e+00)	Acc@1   7.03 (  4.65)	Acc@5  27.34 ( 18.15)
Epoch: [0][120/391]	Time  0.046 ( 0.073)	Data  0.001 ( 0.003)	Loss 3.9951e+00 (4.3302e+00)	Acc@1   7.03 (  4.80)	Acc@5  20.31 ( 18.81)
Epoch: [0][130/391]	Time  0.046 ( 0.071)	Data  0.001 ( 0.003)	Loss 3.8213e+00 (4.3034e+00)	Acc@1   4.69 (  5.06)	Acc@5  32.81 ( 19.42)
Epoch: [0][140/391]	Time  0.046 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.8494e+00 (4.2832e+00)	Acc@1   6.25 (  5.25)	Acc@5  28.91 ( 19.94)
Epoch: [0][150/391]	Time  0.046 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9512e+00 (4.2620e+00)	Acc@1   7.81 (  5.46)	Acc@5  28.12 ( 20.46)
Epoch: [0][160/391]	Time  0.046 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.9731e+00 (4.2462e+00)	Acc@1   9.38 (  5.57)	Acc@5  28.12 ( 20.80)
Epoch: [0][170/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.003)	Loss 3.9042e+00 (4.2268e+00)	Acc@1   9.38 (  5.72)	Acc@5  31.25 ( 21.33)
Epoch: [0][180/391]	Time  0.046 ( 0.064)	Data  0.001 ( 0.003)	Loss 4.0436e+00 (4.2099e+00)	Acc@1  12.50 (  5.91)	Acc@5  29.69 ( 21.74)
Epoch: [0][190/391]	Time  0.046 ( 0.063)	Data  0.001 ( 0.003)	Loss 3.8020e+00 (4.1934e+00)	Acc@1  10.94 (  6.05)	Acc@5  34.38 ( 22.03)
Epoch: [0][200/391]	Time  0.046 ( 0.062)	Data  0.001 ( 0.003)	Loss 3.8015e+00 (4.1754e+00)	Acc@1   9.38 (  6.27)	Acc@5  33.59 ( 22.54)
Epoch: [0][210/391]	Time  0.047 ( 0.061)	Data  0.001 ( 0.003)	Loss 3.9121e+00 (4.1622e+00)	Acc@1   7.81 (  6.36)	Acc@5  28.91 ( 22.82)
Epoch: [0][220/391]	Time  0.046 ( 0.061)	Data  0.001 ( 0.003)	Loss 4.0002e+00 (4.1505e+00)	Acc@1   9.38 (  6.52)	Acc@5  26.56 ( 23.06)
Epoch: [0][230/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.003)	Loss 3.7154e+00 (4.1391e+00)	Acc@1  10.94 (  6.57)	Acc@5  33.59 ( 23.35)
Epoch: [0][240/391]	Time  0.045 ( 0.059)	Data  0.001 ( 0.003)	Loss 3.7254e+00 (4.1261e+00)	Acc@1  10.94 (  6.73)	Acc@5  36.72 ( 23.67)
Epoch: [0][250/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.003)	Loss 3.8163e+00 (4.1151e+00)	Acc@1   9.38 (  6.88)	Acc@5  34.38 ( 23.99)
Epoch: [0][260/391]	Time  0.046 ( 0.058)	Data  0.001 ( 0.003)	Loss 3.8158e+00 (4.1017e+00)	Acc@1  10.94 (  7.03)	Acc@5  32.81 ( 24.34)
Epoch: [0][270/391]	Time  0.045 ( 0.058)	Data  0.001 ( 0.003)	Loss 3.7512e+00 (4.0926e+00)	Acc@1   9.38 (  7.13)	Acc@5  33.59 ( 24.59)
Epoch: [0][280/391]	Time  0.046 ( 0.058)	Data  0.001 ( 0.003)	Loss 3.6942e+00 (4.0813e+00)	Acc@1  10.94 (  7.23)	Acc@5  43.75 ( 24.91)
Epoch: [0][290/391]	Time  0.046 ( 0.057)	Data  0.001 ( 0.003)	Loss 3.6195e+00 (4.0682e+00)	Acc@1  11.72 (  7.44)	Acc@5  37.50 ( 25.30)
Epoch: [0][300/391]	Time  0.046 ( 0.057)	Data  0.001 ( 0.003)	Loss 3.6784e+00 (4.0555e+00)	Acc@1  10.94 (  7.59)	Acc@5  39.84 ( 25.65)
Epoch: [0][310/391]	Time  0.046 ( 0.056)	Data  0.001 ( 0.003)	Loss 3.6124e+00 (4.0450e+00)	Acc@1  11.72 (  7.69)	Acc@5  35.16 ( 25.95)
Epoch: [0][320/391]	Time  0.046 ( 0.056)	Data  0.001 ( 0.003)	Loss 3.6269e+00 (4.0342e+00)	Acc@1  16.41 (  7.81)	Acc@5  32.81 ( 26.25)
Epoch: [0][330/391]	Time  0.046 ( 0.056)	Data  0.001 ( 0.003)	Loss 3.6362e+00 (4.0236e+00)	Acc@1   9.38 (  7.98)	Acc@5  35.16 ( 26.55)
Epoch: [0][340/391]	Time  0.046 ( 0.056)	Data  0.001 ( 0.003)	Loss 3.8196e+00 (4.0140e+00)	Acc@1   7.81 (  8.12)	Acc@5  35.94 ( 26.84)
Epoch: [0][350/391]	Time  0.046 ( 0.055)	Data  0.001 ( 0.003)	Loss 3.5325e+00 (4.0043e+00)	Acc@1  14.06 (  8.21)	Acc@5  35.94 ( 27.13)
Epoch: [0][360/391]	Time  0.045 ( 0.055)	Data  0.001 ( 0.003)	Loss 3.8634e+00 (3.9944e+00)	Acc@1  10.16 (  8.34)	Acc@5  41.41 ( 27.42)
Epoch: [0][370/391]	Time  0.046 ( 0.055)	Data  0.001 ( 0.003)	Loss 3.6354e+00 (3.9843e+00)	Acc@1  12.50 (  8.45)	Acc@5  32.03 ( 27.71)
Epoch: [0][380/391]	Time  0.046 ( 0.055)	Data  0.001 ( 0.003)	Loss 3.5655e+00 (3.9761e+00)	Acc@1  18.75 (  8.57)	Acc@5  39.06 ( 27.95)
Epoch: [0][390/391]	Time  0.327 ( 0.055)	Data  0.001 ( 0.003)	Loss 3.5752e+00 (3.9680e+00)	Acc@1  16.25 (  8.66)	Acc@5  41.25 ( 28.17)
## e[0] optimizer.zero_grad (sum) time: 0.136275053024292
## e[0]       loss.backward (sum) time: 2.869147300720215
## e[0]      optimizer.step (sum) time: 1.0085456371307373
## epoch[0] training(only) time: 21.61009407043457
# Switched to evaluate mode...
Test: [  0/100]	Time  0.265 ( 0.265)	Loss 3.7222e+00 (3.7222e+00)	Acc@1  14.00 ( 14.00)	Acc@5  36.00 ( 36.00)
Test: [ 10/100]	Time  0.023 ( 0.046)	Loss 3.7725e+00 (3.6496e+00)	Acc@1  11.00 ( 13.64)	Acc@5  37.00 ( 38.45)
Test: [ 20/100]	Time  0.023 ( 0.035)	Loss 3.5901e+00 (3.6416e+00)	Acc@1  16.00 ( 13.95)	Acc@5  38.00 ( 38.52)
Test: [ 30/100]	Time  0.023 ( 0.031)	Loss 3.9294e+00 (3.6452e+00)	Acc@1  12.00 ( 14.03)	Acc@5  29.00 ( 38.39)
Test: [ 40/100]	Time  0.023 ( 0.029)	Loss 3.7081e+00 (3.6352e+00)	Acc@1   8.00 ( 14.00)	Acc@5  42.00 ( 38.54)
Test: [ 50/100]	Time  0.023 ( 0.028)	Loss 3.5914e+00 (3.6338e+00)	Acc@1  16.00 ( 13.80)	Acc@5  39.00 ( 38.73)
Test: [ 60/100]	Time  0.023 ( 0.027)	Loss 3.5446e+00 (3.6282e+00)	Acc@1  18.00 ( 13.90)	Acc@5  46.00 ( 38.80)
Test: [ 70/100]	Time  0.022 ( 0.026)	Loss 3.5312e+00 (3.6250e+00)	Acc@1  15.00 ( 14.20)	Acc@5  46.00 ( 39.06)
Test: [ 80/100]	Time  0.023 ( 0.026)	Loss 3.8956e+00 (3.6292e+00)	Acc@1  12.00 ( 14.37)	Acc@5  35.00 ( 39.11)
Test: [ 90/100]	Time  0.022 ( 0.025)	Loss 3.5296e+00 (3.6260e+00)	Acc@1  19.00 ( 14.56)	Acc@5  39.00 ( 39.18)
 * Acc@1 14.430 Acc@5 39.210
### epoch[0] execution time: 24.237470388412476
EPOCH 1
i:   0, name:          module.conv1.0.weight  changing lr from: 0.100000000000000006   to: 0.100883842679128255
i:   1, name:          module.conv1.1.weight  changing lr from: 0.100000000000000006   to: 0.100886612650808893
i:   2, name:            module.conv1.1.bias  changing lr from: 0.100000000000000006   to: 0.100889305884363623
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100891924798882482
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100894471728048343
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100896948923471660
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100899358557882610
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100901702728187437
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100903983458395075
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100906202702420150
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100908362346767977
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100910464213107010
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100912510060733651
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100914501588934766
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100916440439251853
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100918328197651980
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100920166396609196
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100921956517100606
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100923699990520935
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100925398200519045
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100927052484760083
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.100000000000000006   to: 0.100928664136616317
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.100000000000000006   to: 0.100930234406789832
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.100000000000000006   to: 0.100931764504870319
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100933255600830329
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100934708826461189
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100936125276751693
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100937506011212430
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100938852055147896
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100940164400878510
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100941444008914999
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100942691809086785
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100943908701626686
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100945095558213516
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100946253222974458
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100947382513448883
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.100000000000000006   to: 0.100948484221515283
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.100000000000000006   to: 0.100949559114282772
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.100000000000000006   to: 0.100950607934948589
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100951631403623121
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100952630218123812
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100953605054739048
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100954556568963363
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100955485396205261
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100956392152468460
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100957277435007972
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100958141822961756
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100958985877959109
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100959810144706730
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100960615151553149
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100961401411032561
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.100000000000000006   to: 0.100962169420389006
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.100000000000000006   to: 0.100962919662081221
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.100000000000000006   to: 0.100963652604269538
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.100000000000000006   to: 0.100964368701284890
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.100000000000000006   to: 0.100965068394081287
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.100000000000000006   to: 0.100965752110671741
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.100000000000000006   to: 0.100966420266548895
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.100000000000000006   to: 0.100967073265090496
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.100000000000000006   to: 0.100967711497950474
i:  60, name:               module.fc.weight  changing lr from: 0.100000000000000006   to: 0.100968335345436394
i:  61, name:                 module.fc.bias  changing lr from: 0.100000000000000006   to: 0.100968945176873270



# Switched to train mode...
Epoch: [1][  0/391]	Time  0.205 ( 0.205)	Data  0.152 ( 0.152)	Loss 3.7412e+00 (3.7412e+00)	Acc@1  13.28 ( 13.28)	Acc@5  38.28 ( 38.28)
Epoch: [1][ 10/391]	Time  0.047 ( 0.061)	Data  0.001 ( 0.016)	Loss 3.6268e+00 (3.5962e+00)	Acc@1  14.84 ( 13.49)	Acc@5  39.06 ( 40.77)
Epoch: [1][ 20/391]	Time  0.045 ( 0.054)	Data  0.001 ( 0.009)	Loss 3.5710e+00 (3.5625e+00)	Acc@1  12.50 ( 13.62)	Acc@5  38.28 ( 40.74)
Epoch: [1][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 3.3544e+00 (3.5500e+00)	Acc@1  20.31 ( 13.89)	Acc@5  46.88 ( 41.03)
Epoch: [1][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.3046e+00 (3.5274e+00)	Acc@1  21.88 ( 14.58)	Acc@5  48.44 ( 41.27)
Epoch: [1][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 3.7071e+00 (3.5179e+00)	Acc@1  14.84 ( 15.00)	Acc@5  42.19 ( 41.57)
Epoch: [1][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.5607e+00 (3.5041e+00)	Acc@1  18.75 ( 15.38)	Acc@5  45.31 ( 42.12)
Epoch: [1][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.5581e+00 (3.5106e+00)	Acc@1  13.28 ( 15.35)	Acc@5  39.84 ( 41.88)
Epoch: [1][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5009e+00 (3.5058e+00)	Acc@1  10.94 ( 15.23)	Acc@5  39.06 ( 42.01)
Epoch: [1][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6381e+00 (3.4970e+00)	Acc@1  10.94 ( 15.35)	Acc@5  37.50 ( 42.35)
Epoch: [1][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5273e+00 (3.4931e+00)	Acc@1  14.06 ( 15.62)	Acc@5  39.84 ( 42.47)
Epoch: [1][110/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3693e+00 (3.4839e+00)	Acc@1  11.72 ( 15.79)	Acc@5  50.00 ( 42.62)
Epoch: [1][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3611e+00 (3.4777e+00)	Acc@1  17.97 ( 15.93)	Acc@5  51.56 ( 42.80)
Epoch: [1][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2593e+00 (3.4665e+00)	Acc@1  19.53 ( 16.16)	Acc@5  50.00 ( 43.05)
Epoch: [1][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5247e+00 (3.4646e+00)	Acc@1  11.72 ( 16.10)	Acc@5  41.41 ( 43.12)
Epoch: [1][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 3.3335e+00 (3.4569e+00)	Acc@1  20.31 ( 16.31)	Acc@5  48.44 ( 43.32)
Epoch: [1][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6244e+00 (3.4546e+00)	Acc@1  12.50 ( 16.43)	Acc@5  38.28 ( 43.34)
Epoch: [1][170/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3231e+00 (3.4481e+00)	Acc@1  17.19 ( 16.56)	Acc@5  44.53 ( 43.58)
Epoch: [1][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2168e+00 (3.4391e+00)	Acc@1  22.66 ( 16.76)	Acc@5  52.34 ( 43.93)
Epoch: [1][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4410e+00 (3.4359e+00)	Acc@1  14.06 ( 16.82)	Acc@5  44.53 ( 43.97)
Epoch: [1][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3783e+00 (3.4300e+00)	Acc@1  14.06 ( 16.90)	Acc@5  41.41 ( 44.03)
Epoch: [1][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1642e+00 (3.4236e+00)	Acc@1  17.19 ( 16.92)	Acc@5  57.03 ( 44.22)
Epoch: [1][220/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5585e+00 (3.4189e+00)	Acc@1  14.06 ( 17.00)	Acc@5  42.19 ( 44.38)
Epoch: [1][230/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2545e+00 (3.4124e+00)	Acc@1  17.97 ( 17.05)	Acc@5  47.66 ( 44.52)
Epoch: [1][240/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3814e+00 (3.4077e+00)	Acc@1  15.62 ( 17.13)	Acc@5  45.31 ( 44.65)
Epoch: [1][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4757e+00 (3.4040e+00)	Acc@1  16.41 ( 17.18)	Acc@5  47.66 ( 44.80)
Epoch: [1][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0120e+00 (3.3972e+00)	Acc@1  21.09 ( 17.24)	Acc@5  53.91 ( 44.96)
Epoch: [1][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3033e+00 (3.3926e+00)	Acc@1  17.19 ( 17.34)	Acc@5  46.88 ( 45.06)
Epoch: [1][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2091e+00 (3.3841e+00)	Acc@1  15.62 ( 17.48)	Acc@5  51.56 ( 45.28)
Epoch: [1][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4654e+00 (3.3787e+00)	Acc@1  11.72 ( 17.53)	Acc@5  44.53 ( 45.40)
Epoch: [1][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1766e+00 (3.3706e+00)	Acc@1  21.09 ( 17.71)	Acc@5  52.34 ( 45.60)
Epoch: [1][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0650e+00 (3.3611e+00)	Acc@1  17.19 ( 17.87)	Acc@5  55.47 ( 45.88)
Epoch: [1][320/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1654e+00 (3.3558e+00)	Acc@1  17.97 ( 17.93)	Acc@5  45.31 ( 46.00)
Epoch: [1][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7852e+00 (3.3502e+00)	Acc@1  30.47 ( 18.02)	Acc@5  60.16 ( 46.17)
Epoch: [1][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2862e+00 (3.3464e+00)	Acc@1  18.75 ( 18.10)	Acc@5  45.31 ( 46.24)
Epoch: [1][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0567e+00 (3.3400e+00)	Acc@1  21.09 ( 18.23)	Acc@5  53.91 ( 46.42)
Epoch: [1][360/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9611e+00 (3.3328e+00)	Acc@1  24.22 ( 18.34)	Acc@5  61.72 ( 46.64)
Epoch: [1][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1957e+00 (3.3270e+00)	Acc@1  16.41 ( 18.42)	Acc@5  48.44 ( 46.81)
Epoch: [1][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8819e+00 (3.3208e+00)	Acc@1  27.34 ( 18.54)	Acc@5  64.06 ( 46.99)
Epoch: [1][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8556e+00 (3.3118e+00)	Acc@1  28.75 ( 18.72)	Acc@5  58.75 ( 47.22)
## e[1] optimizer.zero_grad (sum) time: 0.13678359985351562
## e[1]       loss.backward (sum) time: 2.5166196823120117
## e[1]      optimizer.step (sum) time: 1.0159738063812256
## epoch[1] training(only) time: 18.411771059036255
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 3.2959e+00 (3.2959e+00)	Acc@1  22.00 ( 22.00)	Acc@5  49.00 ( 49.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 3.1333e+00 (3.0638e+00)	Acc@1  23.00 ( 23.18)	Acc@5  57.00 ( 55.64)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 3.0976e+00 (3.0733e+00)	Acc@1  27.00 ( 24.00)	Acc@5  49.00 ( 55.14)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 3.1888e+00 (3.0513e+00)	Acc@1  28.00 ( 24.26)	Acc@5  51.00 ( 55.58)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 2.9657e+00 (3.0613e+00)	Acc@1  29.00 ( 24.22)	Acc@5  56.00 ( 55.51)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.8493e+00 (3.0571e+00)	Acc@1  28.00 ( 24.47)	Acc@5  59.00 ( 55.31)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 2.8742e+00 (3.0528e+00)	Acc@1  27.00 ( 24.52)	Acc@5  65.00 ( 55.52)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 3.1451e+00 (3.0607e+00)	Acc@1  21.00 ( 24.42)	Acc@5  57.00 ( 55.30)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 3.2694e+00 (3.0718e+00)	Acc@1  25.00 ( 24.42)	Acc@5  50.00 ( 54.93)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 2.8831e+00 (3.0614e+00)	Acc@1  29.00 ( 24.53)	Acc@5  62.00 ( 55.14)
 * Acc@1 24.710 Acc@5 55.210
### epoch[1] execution time: 20.90942144393921
EPOCH 2
i:   0, name:          module.conv1.0.weight  changing lr from: 0.100883842679128255   to: 0.100535910417440644
i:   1, name:          module.conv1.1.weight  changing lr from: 0.100886612650808893   to: 0.100546964870873792
i:   2, name:            module.conv1.1.bias  changing lr from: 0.100889305884363623   to: 0.100557713664943926
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.100891924798882482   to: 0.100568166405493778
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.100894471728048343   to: 0.100578332360840628
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.100896948923471660   to: 0.100588220474861559
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.100899358557882610   to: 0.100597839379525267
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.100901702728187437   to: 0.100607197406895554
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.100903983458395075   to: 0.100616302600630730
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.100906202702420150   to: 0.100625162727001893
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.100908362346767977   to: 0.100633785285451499
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.100910464213107010   to: 0.100642177518713363
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.100912510060733651   to: 0.100650346422513540
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.100914501588934766   to: 0.100658298754870840
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.100916440439251853   to: 0.100666041045015087
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.100918328197651980   to: 0.100673579601939844
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.100920166396609196   to: 0.100680920522605966
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.100921956517100606   to: 0.100688069699811325
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.100923699990520935   to: 0.100695032829741571
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.100925398200519045   to: 0.100701815419215621
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.100927052484760083   to: 0.100708422792639543
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.100928664136616317   to: 0.100714860098681414
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.100930234406789832   to: 0.100721132316679146
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.100931764504870319   to: 0.100727244262793059
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.100933255600830329   to: 0.100733200595914146
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.100934708826461189   to: 0.100739005823338418
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.100936125276751693   to: 0.100744664306217521
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.100937506011212430   to: 0.100750180264795083
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.100938852055147896   to: 0.100755557783437921
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.100940164400878510   to: 0.100760800815470924
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.100941444008914999   to: 0.100765913187823658
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.100942691809086785   to: 0.100770898605496970
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.100943908701626686   to: 0.100775760655856902
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.100945095558213516   to: 0.100780502812763184
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.100946253222974458   to: 0.100785128440539426
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.100947382513448883   to: 0.100789640797791125
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.100948484221515283   to: 0.100794043041078460
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.100949559114282772   to: 0.100798338228449177
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.100950607934948589   to: 0.100802529322837911
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.100951631403623121   to: 0.100806619195337066
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.100952630218123812   to: 0.100810610628344635
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.100953605054739048   to: 0.100814506318593997
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.100954556568963363   to: 0.100818308880070429
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.100955485396205261   to: 0.100822020846819072
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.100956392152468460   to: 0.100825644675648501
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.100957277435007972   to: 0.100829182748734245
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.100958141822961756   to: 0.100832637376126355
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.100958985877959109   to: 0.100836010798164691
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.100959810144706730   to: 0.100839305187805664
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.100960615151553149   to: 0.100842522652864044
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.100961401411032561   to: 0.100845665238173066
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.100962169420389006   to: 0.100848734927666148
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.100962919662081221   to: 0.100851733646383307
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.100963652604269538   to: 0.100854663262405178
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.100964368701284890   to: 0.100857525588717509
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.100965068394081287   to: 0.100860322385008819
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.100965752110671741   to: 0.100863055359403916
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.100966420266548895   to: 0.100865726170135547
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.100967073265090496   to: 0.100868336427156813
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.100967711497950474   to: 0.100870887693696509
i:  60, name:               module.fc.weight  changing lr from: 0.100968335345436394   to: 0.100873381487759539
i:  61, name:                 module.fc.bias  changing lr from: 0.100968945176873270   to: 0.100875819283574614



# Switched to train mode...
Epoch: [2][  0/391]	Time  0.196 ( 0.196)	Data  0.153 ( 0.153)	Loss 2.8207e+00 (2.8207e+00)	Acc@1  26.56 ( 26.56)	Acc@5  64.06 ( 64.06)
Epoch: [2][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 3.0875e+00 (2.9897e+00)	Acc@1  22.66 ( 24.22)	Acc@5  51.56 ( 56.25)
Epoch: [2][ 20/391]	Time  0.043 ( 0.053)	Data  0.001 ( 0.010)	Loss 2.9064e+00 (2.9857e+00)	Acc@1  28.91 ( 24.96)	Acc@5  52.34 ( 56.58)
Epoch: [2][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.0778e+00 (2.9950e+00)	Acc@1  22.66 ( 24.75)	Acc@5  52.34 ( 56.05)
Epoch: [2][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.7478e+00 (2.9684e+00)	Acc@1  26.56 ( 25.15)	Acc@5  59.38 ( 56.48)
Epoch: [2][ 50/391]	Time  0.045 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.9877e+00 (2.9562e+00)	Acc@1  24.22 ( 25.43)	Acc@5  54.69 ( 56.63)
Epoch: [2][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.0425e+00 (2.9686e+00)	Acc@1  25.78 ( 25.28)	Acc@5  54.69 ( 56.17)
Epoch: [2][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 2.9879e+00 (2.9632e+00)	Acc@1  23.44 ( 25.21)	Acc@5  53.91 ( 56.45)
Epoch: [2][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0978e+00 (2.9613e+00)	Acc@1  19.53 ( 25.30)	Acc@5  53.91 ( 56.56)
Epoch: [2][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8414e+00 (2.9522e+00)	Acc@1  25.78 ( 25.41)	Acc@5  60.94 ( 56.76)
Epoch: [2][100/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8542e+00 (2.9455e+00)	Acc@1  26.56 ( 25.74)	Acc@5  65.62 ( 56.98)
Epoch: [2][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8541e+00 (2.9412e+00)	Acc@1  25.00 ( 25.69)	Acc@5  57.03 ( 57.01)
Epoch: [2][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9751e+00 (2.9396e+00)	Acc@1  21.09 ( 25.71)	Acc@5  50.00 ( 56.92)
Epoch: [2][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9307e+00 (2.9320e+00)	Acc@1  27.34 ( 25.94)	Acc@5  60.94 ( 57.20)
Epoch: [2][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.7793e+00 (2.9237e+00)	Acc@1  28.91 ( 26.09)	Acc@5  57.81 ( 57.39)
Epoch: [2][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.7523e+00 (2.9200e+00)	Acc@1  25.78 ( 26.14)	Acc@5  64.06 ( 57.45)
Epoch: [2][160/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.7899e+00 (2.9188e+00)	Acc@1  29.69 ( 26.10)	Acc@5  63.28 ( 57.50)
Epoch: [2][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6836e+00 (2.9161e+00)	Acc@1  31.25 ( 26.19)	Acc@5  60.94 ( 57.56)
Epoch: [2][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9572e+00 (2.9113e+00)	Acc@1  21.09 ( 26.23)	Acc@5  55.47 ( 57.60)
Epoch: [2][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0849e+00 (2.9072e+00)	Acc@1  25.78 ( 26.31)	Acc@5  50.78 ( 57.68)
Epoch: [2][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6305e+00 (2.9022e+00)	Acc@1  28.91 ( 26.48)	Acc@5  65.62 ( 57.82)
Epoch: [2][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6596e+00 (2.8919e+00)	Acc@1  35.16 ( 26.63)	Acc@5  62.50 ( 58.11)
Epoch: [2][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6503e+00 (2.8832e+00)	Acc@1  32.81 ( 26.76)	Acc@5  64.84 ( 58.34)
Epoch: [2][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8599e+00 (2.8728e+00)	Acc@1  30.47 ( 26.99)	Acc@5  60.94 ( 58.63)
Epoch: [2][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4360e+00 (2.8632e+00)	Acc@1  40.62 ( 27.16)	Acc@5  64.06 ( 58.89)
Epoch: [2][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9559e+00 (2.8582e+00)	Acc@1  26.56 ( 27.26)	Acc@5  57.03 ( 59.00)
Epoch: [2][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8093e+00 (2.8553e+00)	Acc@1  23.44 ( 27.30)	Acc@5  62.50 ( 59.07)
Epoch: [2][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6056e+00 (2.8495e+00)	Acc@1  30.47 ( 27.38)	Acc@5  62.50 ( 59.23)
Epoch: [2][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7236e+00 (2.8455e+00)	Acc@1  30.47 ( 27.47)	Acc@5  60.94 ( 59.31)
Epoch: [2][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6470e+00 (2.8397e+00)	Acc@1  33.59 ( 27.61)	Acc@5  61.72 ( 59.47)
Epoch: [2][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6523e+00 (2.8353e+00)	Acc@1  30.47 ( 27.75)	Acc@5  64.84 ( 59.60)
Epoch: [2][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7417e+00 (2.8324e+00)	Acc@1  29.69 ( 27.78)	Acc@5  57.81 ( 59.66)
Epoch: [2][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5655e+00 (2.8233e+00)	Acc@1  25.78 ( 28.00)	Acc@5  64.84 ( 59.86)
Epoch: [2][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6283e+00 (2.8180e+00)	Acc@1  33.59 ( 28.10)	Acc@5  56.25 ( 59.98)
Epoch: [2][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6383e+00 (2.8113e+00)	Acc@1  34.38 ( 28.25)	Acc@5  64.84 ( 60.15)
Epoch: [2][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6591e+00 (2.8053e+00)	Acc@1  35.16 ( 28.41)	Acc@5  60.16 ( 60.28)
Epoch: [2][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3582e+00 (2.7975e+00)	Acc@1  35.16 ( 28.58)	Acc@5  72.66 ( 60.43)
Epoch: [2][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5633e+00 (2.7898e+00)	Acc@1  35.16 ( 28.75)	Acc@5  61.72 ( 60.59)
Epoch: [2][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5558e+00 (2.7841e+00)	Acc@1  25.78 ( 28.84)	Acc@5  69.53 ( 60.74)
Epoch: [2][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4945e+00 (2.7799e+00)	Acc@1  33.75 ( 28.94)	Acc@5  71.25 ( 60.86)
## e[2] optimizer.zero_grad (sum) time: 0.13718962669372559
## e[2]       loss.backward (sum) time: 2.499814033508301
## e[2]      optimizer.step (sum) time: 1.0056533813476562
## epoch[2] training(only) time: 18.418431043624878
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 2.5960e+00 (2.5960e+00)	Acc@1  33.00 ( 33.00)	Acc@5  58.00 ( 58.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 2.5511e+00 (2.6113e+00)	Acc@1  28.00 ( 32.73)	Acc@5  64.00 ( 64.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 2.3220e+00 (2.5944e+00)	Acc@1  38.00 ( 33.52)	Acc@5  68.00 ( 64.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.6473e+00 (2.5833e+00)	Acc@1  35.00 ( 34.16)	Acc@5  62.00 ( 64.81)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 2.5513e+00 (2.5881e+00)	Acc@1  34.00 ( 33.71)	Acc@5  69.00 ( 64.83)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.4273e+00 (2.5980e+00)	Acc@1  36.00 ( 33.47)	Acc@5  64.00 ( 64.57)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 2.7856e+00 (2.5897e+00)	Acc@1  33.00 ( 33.72)	Acc@5  63.00 ( 64.74)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 2.6227e+00 (2.5878e+00)	Acc@1  38.00 ( 33.87)	Acc@5  61.00 ( 64.72)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 2.8430e+00 (2.6011e+00)	Acc@1  25.00 ( 33.51)	Acc@5  58.00 ( 64.15)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 2.4335e+00 (2.5910e+00)	Acc@1  39.00 ( 33.91)	Acc@5  71.00 ( 64.46)
 * Acc@1 33.930 Acc@5 64.630
### epoch[2] execution time: 20.926837921142578
EPOCH 3
i:   0, name:          module.conv1.0.weight  changing lr from: 0.100535910417440644   to: 0.099957819810111703
i:   1, name:          module.conv1.1.weight  changing lr from: 0.100546964870873792   to: 0.099982597130651746
i:   2, name:            module.conv1.1.bias  changing lr from: 0.100557713664943926   to: 0.100006691554040170
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.100568166405493778   to: 0.100030124429973055
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.100578332360840628   to: 0.100052916364021513
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.100588220474861559   to: 0.100075087246129082
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.100597839379525267   to: 0.100096656277925439
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.100607197406895554   to: 0.100117641998909046
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.100616302600630730   to: 0.100138062311548925
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.100625162727001893   to: 0.100157934505353485
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.100633785285451499   to: 0.100177275279951780
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.100642177518713363   to: 0.100196100767230928
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.100650346422513540   to: 0.100214426552571068
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.100658298754870840   to: 0.100232267695217134
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.100666041045015087   to: 0.100249638747825470
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.100673579601939844   to: 0.100266553775220965
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.100680920522605966   to: 0.100283026372399039
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.100688069699811325   to: 0.100299069681805186
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.100695032829741571   to: 0.100314696409923299
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.100701815419215621   to: 0.100329918843202415
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.100708422792639543   to: 0.100344748863350278
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.100714860098681414   to: 0.100359197962020974
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.100721132316679146   to: 0.100373277254922275
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.100727244262793059   to: 0.100386997495367544
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.100733200595914146   to: 0.100400369087295604
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.100739005823338418   to: 0.100413402097781254
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.100744664306217521   to: 0.100426106269057749
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.100750180264795083   to: 0.100438491030071811
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.100755557783437921   to: 0.100450565507590850
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.100760800815470924   to: 0.100462338536880982
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.100765913187823658   to: 0.100473818671973775
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.100770898605496970   to: 0.100485014195538902
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.100775760655856902   to: 0.100495933128378845
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.100780502812763184   to: 0.100506583238561445
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.100785128440539426   to: 0.100516972050205100
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.100789640797791125   to: 0.100527106851930850
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.100794043041078460   to: 0.100536994704995108
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.100798338228449177   to: 0.100546642451115997
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.100802529322837911   to: 0.100556056720005685
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.100806619195337066   to: 0.100565243936620766
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.100810610628344635   to: 0.100574210328142066
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.100814506318593997   to: 0.100582961930694811
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.100818308880070429   to: 0.100591504595819337
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.100822020846819072   to: 0.100599843996702909
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.100825644675648501   to: 0.100607985634181482
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.100829182748734245   to: 0.100615934842521196
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.100832637376126355   to: 0.100623696794988032
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.100836010798164691   to: 0.100631276509213982
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.100839305187805664   to: 0.100638678852367991
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.100842522652864044   to: 0.100645908546139251
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.100845665238173066   to: 0.100652970171540068
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.100848734927666148   to: 0.100659868173535727
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.100851733646383307   to: 0.100666606865507602
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.100854663262405178   to: 0.100673190433556492
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.100857525588717509   to: 0.100679622940652191
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.100860322385008819   to: 0.100685908330635010
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.100863055359403916   to: 0.100692050432075367
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.100865726170135547   to: 0.100698052961996631
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.100868336427156813   to: 0.100703919529466515
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.100870887693696509   to: 0.100709653639062147
i:  60, name:               module.fc.weight  changing lr from: 0.100873381487759539   to: 0.100715258694213494
i:  61, name:                 module.fc.bias  changing lr from: 0.100875819283574614   to: 0.100720738000429841



# Switched to train mode...
Epoch: [3][  0/391]	Time  0.188 ( 0.188)	Data  0.145 ( 0.145)	Loss 2.3909e+00 (2.3909e+00)	Acc@1  33.59 ( 33.59)	Acc@5  70.31 ( 70.31)
Epoch: [3][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.7533e+00 (2.5060e+00)	Acc@1  28.12 ( 33.52)	Acc@5  63.28 ( 67.19)
Epoch: [3][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 2.4098e+00 (2.4747e+00)	Acc@1  40.62 ( 34.52)	Acc@5  72.66 ( 68.64)
Epoch: [3][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.6120e+00 (2.4315e+00)	Acc@1  28.12 ( 35.74)	Acc@5  64.84 ( 69.18)
Epoch: [3][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.4652e+00 (2.4410e+00)	Acc@1  32.03 ( 35.31)	Acc@5  66.41 ( 68.86)
Epoch: [3][ 50/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.3625e+00 (2.4372e+00)	Acc@1  35.16 ( 35.39)	Acc@5  71.88 ( 69.00)
Epoch: [3][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.2492e+00 (2.4192e+00)	Acc@1  35.94 ( 35.51)	Acc@5  71.88 ( 69.52)
Epoch: [3][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 2.5945e+00 (2.4186e+00)	Acc@1  35.94 ( 35.62)	Acc@5  64.06 ( 69.44)
Epoch: [3][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4698e+00 (2.4197e+00)	Acc@1  38.28 ( 35.64)	Acc@5  66.41 ( 69.41)
Epoch: [3][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6133e+00 (2.4313e+00)	Acc@1  32.03 ( 35.59)	Acc@5  61.72 ( 69.09)
Epoch: [3][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2280e+00 (2.4271e+00)	Acc@1  36.72 ( 35.69)	Acc@5  72.66 ( 69.15)
Epoch: [3][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5376e+00 (2.4299e+00)	Acc@1  30.47 ( 35.71)	Acc@5  67.97 ( 69.06)
Epoch: [3][120/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4473e+00 (2.4238e+00)	Acc@1  35.16 ( 35.88)	Acc@5  68.75 ( 69.13)
Epoch: [3][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5398e+00 (2.4139e+00)	Acc@1  36.72 ( 36.09)	Acc@5  60.94 ( 69.32)
Epoch: [3][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2662e+00 (2.4053e+00)	Acc@1  39.06 ( 36.25)	Acc@5  70.31 ( 69.48)
Epoch: [3][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.3900e+00 (2.4026e+00)	Acc@1  29.69 ( 36.35)	Acc@5  67.97 ( 69.46)
Epoch: [3][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4814e+00 (2.4043e+00)	Acc@1  32.81 ( 36.29)	Acc@5  68.75 ( 69.42)
Epoch: [3][170/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4680e+00 (2.4010e+00)	Acc@1  37.50 ( 36.38)	Acc@5  71.09 ( 69.55)
Epoch: [3][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4490e+00 (2.3936e+00)	Acc@1  36.72 ( 36.60)	Acc@5  68.75 ( 69.67)
Epoch: [3][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3082e+00 (2.3872e+00)	Acc@1  37.50 ( 36.75)	Acc@5  70.31 ( 69.78)
Epoch: [3][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3292e+00 (2.3827e+00)	Acc@1  39.84 ( 36.78)	Acc@5  70.31 ( 69.88)
Epoch: [3][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5151e+00 (2.3811e+00)	Acc@1  33.59 ( 36.80)	Acc@5  65.62 ( 69.93)
Epoch: [3][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3738e+00 (2.3802e+00)	Acc@1  42.97 ( 36.84)	Acc@5  68.75 ( 70.03)
Epoch: [3][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6249e+00 (2.3779e+00)	Acc@1  33.59 ( 36.93)	Acc@5  67.19 ( 70.10)
Epoch: [3][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2802e+00 (2.3730e+00)	Acc@1  41.41 ( 37.06)	Acc@5  73.44 ( 70.20)
Epoch: [3][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5054e+00 (2.3693e+00)	Acc@1  32.81 ( 37.19)	Acc@5  69.53 ( 70.25)
Epoch: [3][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1662e+00 (2.3676e+00)	Acc@1  42.97 ( 37.16)	Acc@5  74.22 ( 70.28)
Epoch: [3][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2605e+00 (2.3645e+00)	Acc@1  35.94 ( 37.19)	Acc@5  71.09 ( 70.37)
Epoch: [3][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1132e+00 (2.3605e+00)	Acc@1  41.41 ( 37.29)	Acc@5  73.44 ( 70.43)
Epoch: [3][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3568e+00 (2.3568e+00)	Acc@1  35.16 ( 37.37)	Acc@5  70.31 ( 70.49)
Epoch: [3][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3199e+00 (2.3533e+00)	Acc@1  37.50 ( 37.42)	Acc@5  67.97 ( 70.57)
Epoch: [3][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1114e+00 (2.3485e+00)	Acc@1  48.44 ( 37.53)	Acc@5  78.12 ( 70.70)
Epoch: [3][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2800e+00 (2.3475e+00)	Acc@1  39.84 ( 37.54)	Acc@5  69.53 ( 70.70)
Epoch: [3][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2544e+00 (2.3440e+00)	Acc@1  37.50 ( 37.66)	Acc@5  74.22 ( 70.79)
Epoch: [3][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4856e+00 (2.3410e+00)	Acc@1  37.50 ( 37.71)	Acc@5  67.19 ( 70.84)
Epoch: [3][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2590e+00 (2.3394e+00)	Acc@1  36.72 ( 37.75)	Acc@5  71.09 ( 70.89)
Epoch: [3][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2478e+00 (2.3388e+00)	Acc@1  41.41 ( 37.79)	Acc@5  71.09 ( 70.91)
Epoch: [3][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2642e+00 (2.3360e+00)	Acc@1  42.97 ( 37.88)	Acc@5  69.53 ( 70.96)
Epoch: [3][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0175e+00 (2.3317e+00)	Acc@1  44.53 ( 37.95)	Acc@5  78.12 ( 71.09)
Epoch: [3][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0498e+00 (2.3290e+00)	Acc@1  47.50 ( 38.04)	Acc@5  76.25 ( 71.14)
## e[3] optimizer.zero_grad (sum) time: 0.1384584903717041
## e[3]       loss.backward (sum) time: 2.532118320465088
## e[3]      optimizer.step (sum) time: 1.0070667266845703
## epoch[3] training(only) time: 18.436267852783203
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 2.3109e+00 (2.3109e+00)	Acc@1  38.00 ( 38.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 2.2634e+00 (2.2972e+00)	Acc@1  36.00 ( 40.09)	Acc@5  74.00 ( 73.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.9590e+00 (2.2395e+00)	Acc@1  46.00 ( 40.67)	Acc@5  78.00 ( 74.14)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.3599e+00 (2.2337e+00)	Acc@1  34.00 ( 40.00)	Acc@5  70.00 ( 73.71)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 2.2369e+00 (2.2409e+00)	Acc@1  40.00 ( 39.83)	Acc@5  77.00 ( 73.56)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 2.1768e+00 (2.2495e+00)	Acc@1  46.00 ( 39.80)	Acc@5  69.00 ( 73.24)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 2.2757e+00 (2.2413e+00)	Acc@1  35.00 ( 39.82)	Acc@5  73.00 ( 73.36)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 2.3888e+00 (2.2405e+00)	Acc@1  37.00 ( 39.86)	Acc@5  68.00 ( 73.28)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 2.4595e+00 (2.2499e+00)	Acc@1  34.00 ( 39.70)	Acc@5  68.00 ( 73.19)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 2.3013e+00 (2.2362e+00)	Acc@1  36.00 ( 40.04)	Acc@5  73.00 ( 73.38)
 * Acc@1 40.020 Acc@5 73.400
### epoch[3] execution time: 20.912827253341675
EPOCH 4
i:   0, name:          module.conv1.0.weight  changing lr from: 0.099957819810111703   to: 0.099152256835388142
i:   1, name:          module.conv1.1.weight  changing lr from: 0.099982597130651746   to: 0.099196069116624055
i:   2, name:            module.conv1.1.bias  changing lr from: 0.100006691554040170   to: 0.099238679347862804
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.100030124429973055   to: 0.099280124832108851
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.100052916364021513   to: 0.099320441587279026
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.100075087246129082   to: 0.099359664394539138
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.100096656277925439   to: 0.099397826844687481
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.100117641998909046   to: 0.099434961382668149
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.100138062311548925   to: 0.099471099350294218
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.100157934505353485   to: 0.099506271027256690
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.100177275279951780   to: 0.099540505670492052
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.100196100767230928   to: 0.099573831551978020
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.100214426552571068   to: 0.099606275995024099
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.100232267695217134   to: 0.099637865409120230
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.100249638747825470   to: 0.099668625323404911
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.100266553775220965   to: 0.099698580418810148
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.100283026372399039   to: 0.099727754558939563
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.100299069681805186   to: 0.099756170819732301
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.100314696409923299   to: 0.099783851517963651
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.100329918843202415   to: 0.099810818238631124
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.100344748863350278   to: 0.099837091861272265
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.100359197962020974   to: 0.099862692585258622
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.100373277254922275   to: 0.099887639954108598
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.100386997495367544   to: 0.099911952878859400
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.100400369087295604   to: 0.099935649660537390
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.100413402097781254   to: 0.099958748011763701
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.100426106269057749   to: 0.099981265077530856
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.100438491030071811   to: 0.100003217455184215
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.100450565507590850   to: 0.100024621213641224
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.100462338536880982   to: 0.100045491911878864
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.100473818671973775   to: 0.100065844616719996
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.100485014195538902   to: 0.100085693919946372
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.100495933128378845   to: 0.100105053954766016
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.100506583238561445   to: 0.100123938411660937
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.100516972050205100   to: 0.100142360553640258
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.100527106851930850   to: 0.100160333230922660
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.100536994704995108   to: 0.100177868895070923
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.100546642451115997   to: 0.100194979612600876
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.100556056720005685   to: 0.100211677078085204
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.100565243936620766   to: 0.100227972626772757
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.100574210328142066   to: 0.100243877246742386
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.100582961930694811   to: 0.100259401590609587
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.100591504595819337   to: 0.100274555986804126
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.100599843996702909   to: 0.100289350450434966
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.100607985634181482   to: 0.100303794693759163
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.100615934842521196   to: 0.100317898136270151
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.100623696794988032   to: 0.100331669914420205
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.100631276509213982   to: 0.100345118890991472
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.100638678852367991   to: 0.100358253664129274
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.100645908546139251   to: 0.100371082576050596
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.100652970171540068   to: 0.100383613721440543
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.100659868173535727   to: 0.100395854955548883
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.100666606865507602   to: 0.100407813901997781
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.100673190433556492   to: 0.100419497960312465
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.100679622940652191   to: 0.100430914313184794
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.100685908330635010   to: 0.100442069933480441
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.100692050432075367   to: 0.100452971590999154
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.100698052961996631   to: 0.100463625858997610
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.100703919529466515   to: 0.100474039120483791
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.100709653639062147   to: 0.100484217574291559
i:  60, name:               module.fc.weight  changing lr from: 0.100715258694213494   to: 0.100494167240943788
i:  61, name:                 module.fc.bias  changing lr from: 0.100720738000429841   to: 0.100503893968311678



# Switched to train mode...
Epoch: [4][  0/391]	Time  0.188 ( 0.188)	Data  0.141 ( 0.141)	Loss 2.1854e+00 (2.1854e+00)	Acc@1  39.06 ( 39.06)	Acc@5  77.34 ( 77.34)
Epoch: [4][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.1073e+00 (2.0759e+00)	Acc@1  38.28 ( 42.26)	Acc@5  76.56 ( 76.70)
Epoch: [4][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.9392e+00 (2.1065e+00)	Acc@1  50.00 ( 42.37)	Acc@5  75.00 ( 76.82)
Epoch: [4][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.1344e+00 (2.1016e+00)	Acc@1  38.28 ( 42.46)	Acc@5  72.66 ( 76.84)
Epoch: [4][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.1036e+00 (2.1096e+00)	Acc@1  43.75 ( 42.32)	Acc@5  77.34 ( 76.66)
Epoch: [4][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.9783e+00 (2.1249e+00)	Acc@1  42.97 ( 41.94)	Acc@5  75.00 ( 75.80)
Epoch: [4][ 60/391]	Time  0.045 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.3612e+00 (2.1215e+00)	Acc@1  39.84 ( 42.02)	Acc@5  65.62 ( 75.64)
Epoch: [4][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 2.3004e+00 (2.1190e+00)	Acc@1  42.19 ( 42.17)	Acc@5  72.66 ( 75.48)
Epoch: [4][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8442e+00 (2.1115e+00)	Acc@1  50.00 ( 42.30)	Acc@5  78.12 ( 75.58)
Epoch: [4][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8392e+00 (2.1051e+00)	Acc@1  53.12 ( 42.48)	Acc@5  77.34 ( 75.69)
Epoch: [4][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8804e+00 (2.0987e+00)	Acc@1  46.09 ( 42.65)	Acc@5  79.69 ( 75.66)
Epoch: [4][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0554e+00 (2.0934e+00)	Acc@1  49.22 ( 42.87)	Acc@5  77.34 ( 75.80)
Epoch: [4][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1372e+00 (2.0949e+00)	Acc@1  42.97 ( 42.86)	Acc@5  76.56 ( 75.76)
Epoch: [4][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9045e+00 (2.0903e+00)	Acc@1  50.78 ( 43.04)	Acc@5  74.22 ( 75.80)
Epoch: [4][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.8686e+00 (2.0861e+00)	Acc@1  46.09 ( 43.05)	Acc@5  78.91 ( 75.93)
Epoch: [4][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0501e+00 (2.0858e+00)	Acc@1  42.97 ( 42.96)	Acc@5  75.00 ( 76.11)
Epoch: [4][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9826e+00 (2.0856e+00)	Acc@1  46.09 ( 43.01)	Acc@5  75.78 ( 76.12)
Epoch: [4][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0901e+00 (2.0825e+00)	Acc@1  39.84 ( 43.11)	Acc@5  76.56 ( 76.18)
Epoch: [4][180/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8587e+00 (2.0844e+00)	Acc@1  46.88 ( 43.06)	Acc@5  78.12 ( 76.07)
Epoch: [4][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1572e+00 (2.0840e+00)	Acc@1  40.62 ( 43.01)	Acc@5  75.00 ( 76.13)
Epoch: [4][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8998e+00 (2.0793e+00)	Acc@1  49.22 ( 43.16)	Acc@5  80.47 ( 76.29)
Epoch: [4][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3324e+00 (2.0783e+00)	Acc@1  39.84 ( 43.20)	Acc@5  68.75 ( 76.28)
Epoch: [4][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0323e+00 (2.0749e+00)	Acc@1  44.53 ( 43.25)	Acc@5  80.47 ( 76.36)
Epoch: [4][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1399e+00 (2.0746e+00)	Acc@1  38.28 ( 43.31)	Acc@5  73.44 ( 76.33)
Epoch: [4][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8610e+00 (2.0678e+00)	Acc@1  46.88 ( 43.43)	Acc@5  82.03 ( 76.51)
Epoch: [4][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9428e+00 (2.0692e+00)	Acc@1  45.31 ( 43.44)	Acc@5  76.56 ( 76.48)
Epoch: [4][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0977e+00 (2.0651e+00)	Acc@1  41.41 ( 43.52)	Acc@5  71.88 ( 76.57)
Epoch: [4][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9488e+00 (2.0622e+00)	Acc@1  47.66 ( 43.57)	Acc@5  78.12 ( 76.62)
Epoch: [4][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0193e+00 (2.0589e+00)	Acc@1  46.09 ( 43.70)	Acc@5  78.12 ( 76.66)
Epoch: [4][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1510e+00 (2.0588e+00)	Acc@1  44.53 ( 43.75)	Acc@5  75.78 ( 76.71)
Epoch: [4][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7878e+00 (2.0551e+00)	Acc@1  50.00 ( 43.83)	Acc@5  85.16 ( 76.78)
Epoch: [4][310/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7541e+00 (2.0546e+00)	Acc@1  50.00 ( 43.82)	Acc@5  81.25 ( 76.82)
Epoch: [4][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2896e+00 (2.0535e+00)	Acc@1  37.50 ( 43.88)	Acc@5  75.78 ( 76.81)
Epoch: [4][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3192e+00 (2.0498e+00)	Acc@1  39.84 ( 43.99)	Acc@5  66.41 ( 76.84)
Epoch: [4][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8361e+00 (2.0469e+00)	Acc@1  51.56 ( 44.05)	Acc@5  78.91 ( 76.89)
Epoch: [4][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0071e+00 (2.0427e+00)	Acc@1  45.31 ( 44.16)	Acc@5  79.69 ( 76.98)
Epoch: [4][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1936e+00 (2.0407e+00)	Acc@1  42.19 ( 44.21)	Acc@5  71.88 ( 77.03)
Epoch: [4][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7955e+00 (2.0393e+00)	Acc@1  53.12 ( 44.28)	Acc@5  82.81 ( 77.04)
Epoch: [4][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0827e+00 (2.0370e+00)	Acc@1  39.84 ( 44.31)	Acc@5  77.34 ( 77.08)
Epoch: [4][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8034e+00 (2.0327e+00)	Acc@1  50.00 ( 44.41)	Acc@5  80.00 ( 77.17)
## e[4] optimizer.zero_grad (sum) time: 0.13742828369140625
## e[4]       loss.backward (sum) time: 2.5078179836273193
## e[4]      optimizer.step (sum) time: 1.0141992568969727
## epoch[4] training(only) time: 18.44334387779236
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 2.1805e+00 (2.1805e+00)	Acc@1  43.00 ( 43.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 2.3217e+00 (2.1200e+00)	Acc@1  34.00 ( 43.82)	Acc@5  77.00 ( 77.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.8842e+00 (2.0966e+00)	Acc@1  46.00 ( 43.76)	Acc@5  79.00 ( 76.52)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.0914e+00 (2.0970e+00)	Acc@1  45.00 ( 43.71)	Acc@5  73.00 ( 76.58)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 2.1090e+00 (2.1030e+00)	Acc@1  50.00 ( 43.46)	Acc@5  71.00 ( 76.20)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.9425e+00 (2.1140e+00)	Acc@1  44.00 ( 43.06)	Acc@5  71.00 ( 75.75)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 2.1214e+00 (2.0946e+00)	Acc@1  35.00 ( 43.07)	Acc@5  81.00 ( 75.95)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 2.2753e+00 (2.0992e+00)	Acc@1  40.00 ( 43.24)	Acc@5  74.00 ( 75.87)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 2.1812e+00 (2.1154e+00)	Acc@1  39.00 ( 43.20)	Acc@5  76.00 ( 75.70)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 2.1561e+00 (2.1063e+00)	Acc@1  45.00 ( 43.49)	Acc@5  75.00 ( 75.87)
 * Acc@1 43.450 Acc@5 75.990
### epoch[4] execution time: 20.94731903076172
EPOCH 5
i:   0, name:          module.conv1.0.weight  changing lr from: 0.099152256835388142   to: 0.098122964374747490
i:   1, name:          module.conv1.1.weight  changing lr from: 0.099196069116624055   to: 0.098190948121853722
i:   2, name:            module.conv1.1.bias  changing lr from: 0.099238679347862804   to: 0.098257077623690292
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.099280124832108851   to: 0.098321409866196294
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.099320441587279026   to: 0.098383999902510677
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.099359664394539138   to: 0.098444900923892936
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.099397826844687481   to: 0.098504164327889651
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.099434961382668149   to: 0.098561839783856728
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.099471099350294218   to: 0.098617975295943322
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.099506271027256690   to: 0.098672617263638573
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.099540505670492052   to: 0.098725810539978726
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.099573831551978020   to: 0.098777598487508542
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.099606275995024099   to: 0.098828023032086404
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.099637865409120230   to: 0.098877124714619957
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.099668625323404911   to: 0.098924942740814745
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.099698580418810148   to: 0.098971515029015550
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.099727754558939563   to: 0.099016878256216787
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.099756170819732301   to: 0.099061067902315017
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.099783851517963651   to: 0.099104118292674140
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.099810818238631124   to: 0.099146062639070248
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.099837091861272265   to: 0.099186933079081316
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.099862692585258622   to: 0.099226760713983425
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.099887639954108598   to: 0.099265575645213169
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.099911952878859400   to: 0.099303407009453309
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.099935649660537390   to: 0.099340283012396391
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.099958748011763701   to: 0.099376230961238987
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.099981265077530856   to: 0.099411277295956738
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.100003217455184215   to: 0.099445447619408656
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.100024621213641224   to: 0.099478766726316892
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.100045491911878864   to: 0.099511258631166527
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.100065844616719996   to: 0.099542946595068030
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.100085693919946372   to: 0.099573853151622982
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.100105053954766016   to: 0.099604000131832626
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.100123938411660937   to: 0.099633408688086594
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.100142360553640258   to: 0.099662099317267994
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.100160333230922660   to: 0.099690091883009369
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.100177868895070923   to: 0.099717405637132786
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.100194979612600876   to: 0.099744059240305730
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.100211677078085204   to: 0.099770070781943498
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.100227972626772757   to: 0.099795457799387283
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.100243877246742386   to: 0.099820237296386033
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.100259401590609587   to: 0.099844425760909003
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.100274555986804126   to: 0.099868039182315016
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.100289350450434966   to: 0.099891093067902961
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.100303794693759163   to: 0.099913602458867559
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.100317898136270151   to: 0.099935581945683194
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.100331669914420205   to: 0.099957045682937534
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.100345118890991472   to: 0.099978007403636271
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.100358253664129274   to: 0.099998480432998971
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.100371082576050596   to: 0.100018477701765307
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.100383613721440543   to: 0.100038011759030585
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.100395854955548883   to: 0.100057094784628056
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.100407813901997781   to: 0.100075738601075423
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.100419497960312465   to: 0.100093954685101849
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.100430914313184794   to: 0.100111754178771245
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.100442069933480441   to: 0.100129147900216986
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.100452971590999154   to: 0.100146146354002874
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.100463625858997610   to: 0.100162759741123791
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.100474039120483791   to: 0.100178997968660130
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.100484217574291559   to: 0.100194870659098323
i:  60, name:               module.fc.weight  changing lr from: 0.100494167240943788   to: 0.100210387159330130
i:  61, name:                 module.fc.bias  changing lr from: 0.100503893968311678   to: 0.100225556549342690



# Switched to train mode...
Epoch: [5][  0/391]	Time  0.200 ( 0.200)	Data  0.156 ( 0.156)	Loss 2.1394e+00 (2.1394e+00)	Acc@1  44.53 ( 44.53)	Acc@5  73.44 ( 73.44)
Epoch: [5][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.9837e+00 (1.9065e+00)	Acc@1  48.44 ( 47.94)	Acc@5  75.78 ( 78.91)
Epoch: [5][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.7817e+00 (1.8473e+00)	Acc@1  49.22 ( 49.18)	Acc@5  85.16 ( 80.62)
Epoch: [5][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.9366e+00 (1.8331e+00)	Acc@1  49.22 ( 49.24)	Acc@5  78.91 ( 80.85)
Epoch: [5][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.8159e+00 (1.8296e+00)	Acc@1  50.00 ( 49.45)	Acc@5  80.47 ( 80.81)
Epoch: [5][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.0012e+00 (1.8302e+00)	Acc@1  50.78 ( 49.33)	Acc@5  79.69 ( 80.78)
Epoch: [5][ 60/391]	Time  0.045 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.5894e+00 (1.8316e+00)	Acc@1  52.34 ( 49.30)	Acc@5  85.94 ( 80.62)
Epoch: [5][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.8972e+00 (1.8180e+00)	Acc@1  47.66 ( 49.57)	Acc@5  81.25 ( 80.83)
Epoch: [5][ 80/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8567e+00 (1.8201e+00)	Acc@1  50.00 ( 49.27)	Acc@5  81.25 ( 81.10)
Epoch: [5][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8804e+00 (1.8309e+00)	Acc@1  44.53 ( 48.99)	Acc@5  81.25 ( 80.73)
Epoch: [5][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8108e+00 (1.8233e+00)	Acc@1  50.00 ( 49.23)	Acc@5  80.47 ( 80.82)
Epoch: [5][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7283e+00 (1.8188e+00)	Acc@1  52.34 ( 49.41)	Acc@5  83.59 ( 81.01)
Epoch: [5][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7268e+00 (1.8191e+00)	Acc@1  51.56 ( 49.56)	Acc@5  81.25 ( 80.99)
Epoch: [5][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0909e+00 (1.8271e+00)	Acc@1  39.84 ( 49.33)	Acc@5  75.78 ( 80.80)
Epoch: [5][140/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.6301e+00 (1.8227e+00)	Acc@1  57.81 ( 49.53)	Acc@5  85.16 ( 80.90)
Epoch: [5][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.6750e+00 (1.8185e+00)	Acc@1  52.34 ( 49.72)	Acc@5  82.03 ( 80.96)
Epoch: [5][160/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7725e+00 (1.8160e+00)	Acc@1  46.88 ( 49.74)	Acc@5  80.47 ( 80.98)
Epoch: [5][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7392e+00 (1.8163e+00)	Acc@1  53.12 ( 49.76)	Acc@5  82.81 ( 80.97)
Epoch: [5][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9176e+00 (1.8207e+00)	Acc@1  46.88 ( 49.56)	Acc@5  79.69 ( 80.90)
Epoch: [5][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6977e+00 (1.8201e+00)	Acc@1  48.44 ( 49.55)	Acc@5  84.38 ( 80.94)
Epoch: [5][200/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8342e+00 (1.8195e+00)	Acc@1  47.66 ( 49.56)	Acc@5  84.38 ( 80.94)
Epoch: [5][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6789e+00 (1.8191e+00)	Acc@1  59.38 ( 49.62)	Acc@5  83.59 ( 81.05)
Epoch: [5][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7756e+00 (1.8181e+00)	Acc@1  53.12 ( 49.68)	Acc@5  81.25 ( 81.05)
Epoch: [5][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6208e+00 (1.8166e+00)	Acc@1  53.12 ( 49.75)	Acc@5  83.59 ( 81.08)
Epoch: [5][240/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8658e+00 (1.8170e+00)	Acc@1  45.31 ( 49.72)	Acc@5  81.25 ( 81.07)
Epoch: [5][250/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7438e+00 (1.8152e+00)	Acc@1  49.22 ( 49.75)	Acc@5  78.91 ( 81.11)
Epoch: [5][260/391]	Time  0.050 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5812e+00 (1.8126e+00)	Acc@1  52.34 ( 49.78)	Acc@5  89.06 ( 81.20)
Epoch: [5][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9098e+00 (1.8121e+00)	Acc@1  50.00 ( 49.82)	Acc@5  77.34 ( 81.20)
Epoch: [5][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9021e+00 (1.8088e+00)	Acc@1  48.44 ( 49.87)	Acc@5  84.38 ( 81.28)
Epoch: [5][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6913e+00 (1.8084e+00)	Acc@1  57.03 ( 49.89)	Acc@5  81.25 ( 81.28)
Epoch: [5][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9144e+00 (1.8088e+00)	Acc@1  46.88 ( 49.90)	Acc@5  80.47 ( 81.31)
Epoch: [5][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8907e+00 (1.8066e+00)	Acc@1  46.88 ( 49.96)	Acc@5  82.81 ( 81.36)
Epoch: [5][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7358e+00 (1.8021e+00)	Acc@1  50.78 ( 50.04)	Acc@5  84.38 ( 81.44)
Epoch: [5][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7336e+00 (1.7988e+00)	Acc@1  53.12 ( 50.17)	Acc@5  78.12 ( 81.45)
Epoch: [5][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5252e+00 (1.7997e+00)	Acc@1  56.25 ( 50.12)	Acc@5  85.94 ( 81.45)
Epoch: [5][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7780e+00 (1.7984e+00)	Acc@1  49.22 ( 50.18)	Acc@5  79.69 ( 81.49)
Epoch: [5][360/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5608e+00 (1.7978e+00)	Acc@1  57.81 ( 50.18)	Acc@5  85.94 ( 81.49)
Epoch: [5][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6590e+00 (1.7981e+00)	Acc@1  51.56 ( 50.19)	Acc@5  87.50 ( 81.50)
Epoch: [5][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8355e+00 (1.7986e+00)	Acc@1  48.44 ( 50.16)	Acc@5  81.25 ( 81.49)
Epoch: [5][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6744e+00 (1.7957e+00)	Acc@1  55.00 ( 50.22)	Acc@5  81.25 ( 81.54)
## e[5] optimizer.zero_grad (sum) time: 0.13813018798828125
## e[5]       loss.backward (sum) time: 2.535289764404297
## e[5]      optimizer.step (sum) time: 1.017540454864502
## epoch[5] training(only) time: 18.45406746864319
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 2.0975e+00 (2.0975e+00)	Acc@1  52.00 ( 52.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.9937e+00 (1.8908e+00)	Acc@1  45.00 ( 51.27)	Acc@5  74.00 ( 80.27)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5867e+00 (1.8220e+00)	Acc@1  58.00 ( 51.00)	Acc@5  87.00 ( 81.62)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7512e+00 (1.8270e+00)	Acc@1  49.00 ( 50.39)	Acc@5  80.00 ( 81.13)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.9533e+00 (1.8391e+00)	Acc@1  57.00 ( 50.59)	Acc@5  79.00 ( 81.07)
Test: [ 50/100]	Time  0.027 ( 0.025)	Loss 1.7566e+00 (1.8601e+00)	Acc@1  53.00 ( 50.04)	Acc@5  78.00 ( 80.24)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.7517e+00 (1.8437e+00)	Acc@1  48.00 ( 50.03)	Acc@5  83.00 ( 80.64)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.8510e+00 (1.8373e+00)	Acc@1  53.00 ( 50.37)	Acc@5  79.00 ( 80.77)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.9931e+00 (1.8518e+00)	Acc@1  44.00 ( 50.05)	Acc@5  76.00 ( 80.41)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.9502e+00 (1.8505e+00)	Acc@1  47.00 ( 50.11)	Acc@5  78.00 ( 80.43)
 * Acc@1 50.210 Acc@5 80.610
### epoch[5] execution time: 20.940659523010254
EPOCH 6
i:   0, name:          module.conv1.0.weight  changing lr from: 0.098122964374747490   to: 0.096874724822374580
i:   1, name:          module.conv1.1.weight  changing lr from: 0.098190948121853722   to: 0.096971792866549292
i:   2, name:            module.conv1.1.bias  changing lr from: 0.098257077623690292   to: 0.097066232682913228
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.098321409866196294   to: 0.097158124064745569
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.098383999902510677   to: 0.097247544152627541
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.098444900923892936   to: 0.097334567528607210
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.098504164327889651   to: 0.097419266306910168
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.098561839783856728   to: 0.097501710221319704
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.098617975295943322   to: 0.097581966709346601
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.098672617263638573   to: 0.097660100993304910
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.098725810539978726   to: 0.097736176158406213
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.098777598487508542   to: 0.097810253227981564
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.098828023032086404   to: 0.097882391235936450
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.098877124714619957   to: 0.097952647296540796
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.098924942740814745   to: 0.098021076671652455
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.098971515029015550   to: 0.098087732835469552
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.099016878256216787   to: 0.098152667536903104
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.099061067902315017   to: 0.098215930859659295
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.099104118292674140   to: 0.098277571280116091
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.099146062639070248   to: 0.098337635723077466
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.099186933079081316   to: 0.098396169615484322
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.099226760713983425   to: 0.098453216938158991
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.099265575645213169   to: 0.098508820275656994
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.099303407009453309   to: 0.098563020864297565
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.099340283012396391   to: 0.098615858638441203
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.099376230961238987   to: 0.098667372275080489
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.099411277295956738   to: 0.098717599236807563
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.099445447619408656   to: 0.098766575813219626
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.099478766726316892   to: 0.098814337160821358
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.099511258631166527   to: 0.098860917341480836
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.099542946595068030   to: 0.098906349359493634
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.099573853151622982   to: 0.098950665197307441
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.099604000131832626   to: 0.098993895849957991
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.099633408688086594   to: 0.099036071358264519
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.099662099317267994   to: 0.099077220840831703
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.099690091883009369   to: 0.099117372524903005
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.099717405637132786   to: 0.099156553776108547
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.099744059240305730   to: 0.099194791127149226
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.099770070781943498   to: 0.099232110305456825
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.099795457799387283   to: 0.099268536259868853
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.099820237296386033   to: 0.099304093186354719
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.099844425760909003   to: 0.099338804552829202
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.099868039182315016   to: 0.099372693123086817
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.099891093067902961   to: 0.099405780979890584
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.099913602458867559   to: 0.099438089547246186
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.099935581945683194   to: 0.099469639611892327
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.099957045682937534   to: 0.099500451344036178
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.099978007403636271   to: 0.099530544317362177
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.099998480432998971   to: 0.099559937528340983
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.100018477701765307   to: 0.099588649414864852
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.100038011759030585   to: 0.099616697874233939
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.100057094784628056   to: 0.099644100280517844
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.100075738601075423   to: 0.099670873501315413
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.100093954685101849   to: 0.099697033913934724
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.100111754178771245   to: 0.099722597421015025
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.100129147900216986   to: 0.099747579465610636
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.100146146354002874   to: 0.099771995045756887
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.100162759741123791   to: 0.099795858728536857
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.100178997968660130   to: 0.099819184663667293
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.100194870659098323   to: 0.099841986596620982
i:  60, name:               module.fc.weight  changing lr from: 0.100210387159330130   to: 0.099864277881302838
i:  61, name:                 module.fc.bias  changing lr from: 0.100225556549342690   to: 0.099886071492295489



# Switched to train mode...
Epoch: [6][  0/391]	Time  0.201 ( 0.201)	Data  0.158 ( 0.158)	Loss 1.6739e+00 (1.6739e+00)	Acc@1  51.56 ( 51.56)	Acc@5  83.59 ( 83.59)
Epoch: [6][ 10/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.6029e+00 (1.6249e+00)	Acc@1  53.91 ( 54.40)	Acc@5  87.50 ( 84.23)
Epoch: [6][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.7622e+00 (1.6509e+00)	Acc@1  50.00 ( 53.76)	Acc@5  78.12 ( 83.26)
Epoch: [6][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.6933e+00 (1.6474e+00)	Acc@1  55.47 ( 54.26)	Acc@5  81.25 ( 83.59)
Epoch: [6][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.5327e+00 (1.6462e+00)	Acc@1  57.03 ( 53.87)	Acc@5  86.72 ( 83.90)
Epoch: [6][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.5439e+00 (1.6480e+00)	Acc@1  57.81 ( 53.81)	Acc@5  82.81 ( 83.79)
Epoch: [6][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7064e+00 (1.6465e+00)	Acc@1  59.38 ( 53.83)	Acc@5  84.38 ( 83.91)
Epoch: [6][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.5247e+00 (1.6340e+00)	Acc@1  54.69 ( 54.19)	Acc@5  83.59 ( 84.06)
Epoch: [6][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5658e+00 (1.6312e+00)	Acc@1  55.47 ( 54.22)	Acc@5  85.16 ( 84.13)
Epoch: [6][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4317e+00 (1.6368e+00)	Acc@1  48.44 ( 54.00)	Acc@5  90.62 ( 84.14)
Epoch: [6][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6401e+00 (1.6406e+00)	Acc@1  50.00 ( 53.90)	Acc@5  85.16 ( 84.19)
Epoch: [6][110/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5421e+00 (1.6388e+00)	Acc@1  59.38 ( 53.95)	Acc@5  84.38 ( 84.27)
Epoch: [6][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3422e+00 (1.6308e+00)	Acc@1  64.84 ( 54.19)	Acc@5  89.06 ( 84.41)
Epoch: [6][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5264e+00 (1.6318e+00)	Acc@1  51.56 ( 54.19)	Acc@5  87.50 ( 84.43)
Epoch: [6][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8315e+00 (1.6388e+00)	Acc@1  42.19 ( 53.97)	Acc@5  81.25 ( 84.31)
Epoch: [6][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3753e+00 (1.6387e+00)	Acc@1  58.59 ( 53.96)	Acc@5  90.62 ( 84.33)
Epoch: [6][160/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.7091e+00 (1.6345e+00)	Acc@1  46.09 ( 54.12)	Acc@5  84.38 ( 84.39)
Epoch: [6][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7325e+00 (1.6318e+00)	Acc@1  50.78 ( 54.18)	Acc@5  79.69 ( 84.37)
Epoch: [6][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5390e+00 (1.6292e+00)	Acc@1  57.81 ( 54.17)	Acc@5  86.72 ( 84.39)
Epoch: [6][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3629e+00 (1.6239e+00)	Acc@1  58.59 ( 54.27)	Acc@5  85.94 ( 84.50)
Epoch: [6][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5643e+00 (1.6268e+00)	Acc@1  57.81 ( 54.20)	Acc@5  89.06 ( 84.52)
Epoch: [6][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6771e+00 (1.6266e+00)	Acc@1  49.22 ( 54.12)	Acc@5  85.94 ( 84.55)
Epoch: [6][220/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8138e+00 (1.6254e+00)	Acc@1  46.09 ( 54.13)	Acc@5  82.03 ( 84.53)
Epoch: [6][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6646e+00 (1.6246e+00)	Acc@1  51.56 ( 54.13)	Acc@5  84.38 ( 84.50)
Epoch: [6][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5146e+00 (1.6212e+00)	Acc@1  59.38 ( 54.22)	Acc@5  85.16 ( 84.59)
Epoch: [6][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5450e+00 (1.6196e+00)	Acc@1  54.69 ( 54.28)	Acc@5  88.28 ( 84.63)
Epoch: [6][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3642e+00 (1.6173e+00)	Acc@1  60.16 ( 54.33)	Acc@5  89.06 ( 84.66)
Epoch: [6][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4796e+00 (1.6160e+00)	Acc@1  53.12 ( 54.34)	Acc@5  87.50 ( 84.64)
Epoch: [6][280/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4705e+00 (1.6121e+00)	Acc@1  58.59 ( 54.48)	Acc@5  89.06 ( 84.69)
Epoch: [6][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7550e+00 (1.6097e+00)	Acc@1  48.44 ( 54.47)	Acc@5  86.72 ( 84.78)
Epoch: [6][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7867e+00 (1.6097e+00)	Acc@1  52.34 ( 54.54)	Acc@5  80.47 ( 84.77)
Epoch: [6][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4823e+00 (1.6093e+00)	Acc@1  59.38 ( 54.57)	Acc@5  85.16 ( 84.73)
Epoch: [6][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6638e+00 (1.6110e+00)	Acc@1  53.12 ( 54.58)	Acc@5  82.81 ( 84.68)
Epoch: [6][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4633e+00 (1.6100e+00)	Acc@1  57.03 ( 54.58)	Acc@5  85.16 ( 84.70)
Epoch: [6][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6477e+00 (1.6077e+00)	Acc@1  50.78 ( 54.66)	Acc@5  81.25 ( 84.73)
Epoch: [6][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2187e+00 (1.6072e+00)	Acc@1  62.50 ( 54.66)	Acc@5  91.41 ( 84.73)
Epoch: [6][360/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4181e+00 (1.6080e+00)	Acc@1  60.16 ( 54.58)	Acc@5  86.72 ( 84.70)
Epoch: [6][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5207e+00 (1.6074e+00)	Acc@1  58.59 ( 54.62)	Acc@5  88.28 ( 84.71)
Epoch: [6][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7733e+00 (1.6080e+00)	Acc@1  55.47 ( 54.60)	Acc@5  83.59 ( 84.70)
Epoch: [6][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7843e+00 (1.6072e+00)	Acc@1  50.00 ( 54.63)	Acc@5  81.25 ( 84.71)
## e[6] optimizer.zero_grad (sum) time: 0.13644123077392578
## e[6]       loss.backward (sum) time: 2.5160229206085205
## e[6]      optimizer.step (sum) time: 1.0169577598571777
## epoch[6] training(only) time: 18.45802354812622
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.9256e+00 (1.9256e+00)	Acc@1  50.00 ( 50.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 2.0326e+00 (1.8662e+00)	Acc@1  50.00 ( 51.73)	Acc@5  85.00 ( 80.45)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.7271e+00 (1.8469e+00)	Acc@1  54.00 ( 51.38)	Acc@5  88.00 ( 81.43)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7052e+00 (1.8452e+00)	Acc@1  51.00 ( 50.90)	Acc@5  85.00 ( 80.81)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.8695e+00 (1.8561e+00)	Acc@1  46.00 ( 50.22)	Acc@5  82.00 ( 80.90)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5034e+00 (1.8447e+00)	Acc@1  59.00 ( 50.80)	Acc@5  86.00 ( 80.71)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.8780e+00 (1.8290e+00)	Acc@1  51.00 ( 50.97)	Acc@5  82.00 ( 81.07)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.9190e+00 (1.8317e+00)	Acc@1  44.00 ( 50.92)	Acc@5  81.00 ( 81.03)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.8162e+00 (1.8411e+00)	Acc@1  52.00 ( 50.79)	Acc@5  80.00 ( 80.96)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.9780e+00 (1.8314e+00)	Acc@1  49.00 ( 51.07)	Acc@5  81.00 ( 81.14)
 * Acc@1 51.350 Acc@5 81.250
### epoch[6] execution time: 20.94977855682373
EPOCH 7
i:   0, name:          module.conv1.0.weight  changing lr from: 0.096874724822374580   to: 0.095413337864757808
i:   1, name:          module.conv1.1.weight  changing lr from: 0.096971792866549292   to: 0.095544132822016820
i:   2, name:            module.conv1.1.bias  changing lr from: 0.097066232682913228   to: 0.095671417306634754
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.097158124064745569   to: 0.095795296321634651
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.097247544152627541   to: 0.095915871466087030
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.097334567528607210   to: 0.096033241050749563
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.097419266306910168   to: 0.096147500209804337
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.097501710221319704   to: 0.096258741008807699
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.097581966709346601   to: 0.096367052548966367
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.097660100993304910   to: 0.096472521067852229
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.097736176158406213   to: 0.096575230036665904
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.097810253227981564   to: 0.096675260254157472
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.097882391235936450   to: 0.096772689937310780
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.097952647296540796   to: 0.096867594808895188
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.098021076671652455   to: 0.096960048181986627
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.098087732835469552   to: 0.097050121041557330
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.098152667536903104   to: 0.097137882123231045
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.098215930859659295   to: 0.097223397989298563
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.098277571280116091   to: 0.097306733102085208
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.098337635723077466   to: 0.097387949894760059
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.098396169615484322   to: 0.097467108839673786
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.098453216938158991   to: 0.097544268514309862
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.098508820275656994   to: 0.097619485664930936
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.098563020864297565   to: 0.097692815268000249
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.098615858638441203   to: 0.097764310589455214
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.098667372275080489   to: 0.097834023241907944
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.098717599236807563   to: 0.097902003239845381
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.098766575813219626   to: 0.097968299052898902
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.098814337160821358   to: 0.098032957657251840
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.098860917341480836   to: 0.098096024585249894
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.098906349359493634   to: 0.098157543973278832
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.098950665197307441   to: 0.098217558607970171
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.098993895849957991   to: 0.098276109970794653
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.099036071358264519   to: 0.098333238281100799
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.099077220840831703   to: 0.098388982537653782
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.099117372524903005   to: 0.098443380558728436
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.099156553776108547   to: 0.098496469020807609
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.099194791127149226   to: 0.098548283495936306
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.099232110305456825   to: 0.098598858487779162
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.099268536259868853   to: 0.098648227466428137
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.099304093186354719   to: 0.098696422902004824
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.099338804552829202   to: 0.098743476297101035
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.099372693123086817   to: 0.098789418218098793
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.099405780979890584   to: 0.098834278325410405
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.099438089547246186   to: 0.098878085402677007
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.099469639611892327   to: 0.098920867384963085
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.099500451344036178   to: 0.098962651385983091
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.099530544317362177   to: 0.099003463724394336
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.099559937528340983   to: 0.099043329949190406
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.099588649414864852   to: 0.099082274864226702
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.099616697874233939   to: 0.099120322551909223
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.099644100280517844   to: 0.099157496396076947
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.099670873501315413   to: 0.099193819104106143
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.099697033913934724   to: 0.099229312728264782
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.099722597421015025   to: 0.099263998686343433
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.099747579465610636   to: 0.099297897781588915
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.099771995045756887   to: 0.099331030221965086
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.099795858728536857   to: 0.099363415638765051
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.099819184663667293   to: 0.099395073104597678
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.099841986596620982   to: 0.099426021150770538
i:  60, name:               module.fc.weight  changing lr from: 0.099864277881302838   to: 0.099456277784091079
i:  61, name:                 module.fc.bias  changing lr from: 0.099886071492295489   to: 0.099485860503106094



# Switched to train mode...
Epoch: [7][  0/391]	Time  0.196 ( 0.196)	Data  0.154 ( 0.154)	Loss 1.6576e+00 (1.6576e+00)	Acc@1  50.00 ( 50.00)	Acc@5  86.72 ( 86.72)
Epoch: [7][ 10/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.7353e+00 (1.5748e+00)	Acc@1  50.00 ( 54.47)	Acc@5  82.81 ( 85.72)
Epoch: [7][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.3922e+00 (1.5548e+00)	Acc@1  59.38 ( 55.06)	Acc@5  87.50 ( 85.49)
Epoch: [7][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.2720e+00 (1.5078e+00)	Acc@1  64.06 ( 56.38)	Acc@5  91.41 ( 86.37)
Epoch: [7][ 40/391]	Time  0.045 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.2134e+00 (1.4848e+00)	Acc@1  67.19 ( 57.26)	Acc@5  88.28 ( 86.60)
Epoch: [7][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.3796e+00 (1.4736e+00)	Acc@1  57.81 ( 57.61)	Acc@5  90.62 ( 86.93)
Epoch: [7][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4546e+00 (1.4628e+00)	Acc@1  56.25 ( 57.90)	Acc@5  85.94 ( 87.13)
Epoch: [7][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 1.3715e+00 (1.4578e+00)	Acc@1  63.28 ( 58.02)	Acc@5  86.72 ( 87.18)
Epoch: [7][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5692e+00 (1.4571e+00)	Acc@1  55.47 ( 57.93)	Acc@5  84.38 ( 87.21)
Epoch: [7][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4173e+00 (1.4562e+00)	Acc@1  58.59 ( 58.04)	Acc@5  90.62 ( 87.19)
Epoch: [7][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6652e+00 (1.4579e+00)	Acc@1  53.91 ( 58.18)	Acc@5  82.03 ( 87.08)
Epoch: [7][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4796e+00 (1.4552e+00)	Acc@1  58.59 ( 58.35)	Acc@5  84.38 ( 87.15)
Epoch: [7][120/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3923e+00 (1.4594e+00)	Acc@1  57.03 ( 58.12)	Acc@5  89.84 ( 87.15)
Epoch: [7][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5710e+00 (1.4682e+00)	Acc@1  53.91 ( 57.93)	Acc@5  85.94 ( 86.96)
Epoch: [7][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.3559e+00 (1.4721e+00)	Acc@1  58.59 ( 57.78)	Acc@5  90.62 ( 86.96)
Epoch: [7][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.3458e+00 (1.4707e+00)	Acc@1  59.38 ( 57.87)	Acc@5  91.41 ( 86.98)
Epoch: [7][160/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3818e+00 (1.4714e+00)	Acc@1  61.72 ( 57.86)	Acc@5  86.72 ( 86.96)
Epoch: [7][170/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4418e+00 (1.4688e+00)	Acc@1  58.59 ( 57.85)	Acc@5  86.72 ( 87.04)
Epoch: [7][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3901e+00 (1.4703e+00)	Acc@1  58.59 ( 57.82)	Acc@5  88.28 ( 87.09)
Epoch: [7][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4787e+00 (1.4694e+00)	Acc@1  59.38 ( 57.94)	Acc@5  87.50 ( 87.10)
Epoch: [7][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3273e+00 (1.4684e+00)	Acc@1  58.59 ( 57.95)	Acc@5  90.62 ( 87.12)
Epoch: [7][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3972e+00 (1.4713e+00)	Acc@1  57.81 ( 57.83)	Acc@5  86.72 ( 87.09)
Epoch: [7][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4586e+00 (1.4732e+00)	Acc@1  54.69 ( 57.76)	Acc@5  88.28 ( 87.05)
Epoch: [7][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3686e+00 (1.4727e+00)	Acc@1  64.06 ( 57.82)	Acc@5  84.38 ( 87.06)
Epoch: [7][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4966e+00 (1.4717e+00)	Acc@1  56.25 ( 57.83)	Acc@5  86.72 ( 87.10)
Epoch: [7][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4548e+00 (1.4717e+00)	Acc@1  60.16 ( 57.83)	Acc@5  86.72 ( 87.08)
Epoch: [7][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7069e+00 (1.4717e+00)	Acc@1  52.34 ( 57.83)	Acc@5  83.59 ( 87.08)
Epoch: [7][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2795e+00 (1.4701e+00)	Acc@1  64.06 ( 57.84)	Acc@5  91.41 ( 87.09)
Epoch: [7][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2681e+00 (1.4680e+00)	Acc@1  64.06 ( 57.98)	Acc@5  88.28 ( 87.10)
Epoch: [7][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7576e+00 (1.4702e+00)	Acc@1  49.22 ( 57.90)	Acc@5  87.50 ( 87.06)
Epoch: [7][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0958e+00 (1.4676e+00)	Acc@1  68.75 ( 57.97)	Acc@5  92.97 ( 87.07)
Epoch: [7][310/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3704e+00 (1.4648e+00)	Acc@1  62.50 ( 58.09)	Acc@5  92.19 ( 87.08)
Epoch: [7][320/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4773e+00 (1.4641e+00)	Acc@1  60.16 ( 58.11)	Acc@5  83.59 ( 87.08)
Epoch: [7][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5367e+00 (1.4647e+00)	Acc@1  55.47 ( 58.10)	Acc@5  85.94 ( 87.09)
Epoch: [7][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7385e+00 (1.4641e+00)	Acc@1  48.44 ( 58.15)	Acc@5  87.50 ( 87.13)
Epoch: [7][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4801e+00 (1.4659e+00)	Acc@1  53.91 ( 58.06)	Acc@5  87.50 ( 87.09)
Epoch: [7][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3444e+00 (1.4657e+00)	Acc@1  59.38 ( 58.03)	Acc@5  87.50 ( 87.10)
Epoch: [7][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4216e+00 (1.4654e+00)	Acc@1  58.59 ( 58.02)	Acc@5  86.72 ( 87.09)
Epoch: [7][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2958e+00 (1.4656e+00)	Acc@1  64.06 ( 58.03)	Acc@5  92.97 ( 87.09)
Epoch: [7][390/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4893e+00 (1.4637e+00)	Acc@1  60.00 ( 58.02)	Acc@5  87.50 ( 87.11)
## e[7] optimizer.zero_grad (sum) time: 0.13947415351867676
## e[7]       loss.backward (sum) time: 2.5238418579101562
## e[7]      optimizer.step (sum) time: 1.019228219985962
## epoch[7] training(only) time: 18.42495346069336
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.6459e+00 (1.6459e+00)	Acc@1  60.00 ( 60.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.6050e+00 (1.6104e+00)	Acc@1  57.00 ( 57.09)	Acc@5  87.00 ( 84.73)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.5104e+00 (1.5899e+00)	Acc@1  63.00 ( 56.71)	Acc@5  86.00 ( 84.81)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6390e+00 (1.5866e+00)	Acc@1  55.00 ( 56.23)	Acc@5  83.00 ( 84.52)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.6489e+00 (1.5825e+00)	Acc@1  54.00 ( 56.10)	Acc@5  86.00 ( 84.83)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4331e+00 (1.6017e+00)	Acc@1  59.00 ( 55.71)	Acc@5  86.00 ( 84.35)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4839e+00 (1.5954e+00)	Acc@1  57.00 ( 55.51)	Acc@5  87.00 ( 84.75)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.5916e+00 (1.6075e+00)	Acc@1  58.00 ( 55.35)	Acc@5  85.00 ( 84.65)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5064e+00 (1.6137e+00)	Acc@1  58.00 ( 55.15)	Acc@5  85.00 ( 84.44)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7616e+00 (1.6080e+00)	Acc@1  51.00 ( 55.34)	Acc@5  84.00 ( 84.65)
 * Acc@1 55.500 Acc@5 84.800
### epoch[7] execution time: 20.917067527770996
EPOCH 8
i:   0, name:          module.conv1.0.weight  changing lr from: 0.095413337864757808   to: 0.093745593533647337
i:   1, name:          module.conv1.1.weight  changing lr from: 0.095544132822016820   to: 0.093914443131776104
i:   2, name:            module.conv1.1.bias  changing lr from: 0.095671417306634754   to: 0.094078807409037013
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.095795296321634651   to: 0.094238818152160550
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.095915871466087030   to: 0.094394603007585745
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.096033241050749563   to: 0.094546285614098280
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.096147500209804337   to: 0.094693985731534092
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.096258741008807699   to: 0.094837819365622189
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.096367052548966367   to: 0.094977898889043677
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.096472521067852229   to: 0.095114333158787243
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.096575230036665904   to: 0.095247227629882802
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.096675260254157472   to: 0.095376684465597425
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.096772689937310780   to: 0.095502802644178209
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.096867594808895188   to: 0.095625678062227798
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.096960048181986627   to: 0.095745403634798776
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.097050121041557330   to: 0.095862069392292748
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.097137882123231045   to: 0.095975762574250237
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.097223397989298563   to: 0.096086567720116617
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.097306733102085208   to: 0.096194566757068972
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.097387949894760059   to: 0.096299839084987390
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.097467108839673786   to: 0.096402461658653779
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.097544268514309862   to: 0.096502509067259504
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.097619485664930936   to: 0.096600053611302195
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.097692815268000249   to: 0.096695165376950498
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.097764310589455214   to: 0.096787912307954124
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.097834023241907944   to: 0.096878360275175046
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.097902003239845381   to: 0.096966573143813697
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.097968299052898902   to: 0.097052612838402857
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.098032957657251840   to: 0.097136539405640002
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.098096024585249894   to: 0.097218411075126993
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.098157543973278832   to: 0.097298284318084433
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.098217558607970171   to: 0.097376213904106432
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.098276109970794653   to: 0.097452252956019253
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.098333238281100799   to: 0.097526453002906258
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.098388982537653782   to: 0.097598864031359117
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.098443380558728436   to: 0.097669534535014313
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.098496469020807609   to: 0.097738511562431393
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.098548283495936306   to: 0.097805840763368637
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.098598858487779162   to: 0.097871566433509444
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.098648227466428137   to: 0.097935731557691516
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.098696422902004824   to: 0.097998377851689245
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.098743476297101035   to: 0.098059545802598000
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.098789418218098793   to: 0.098119274707867771
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.098834278325410405   to: 0.098177602713031706
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.098878085402677007   to: 0.098234566848174110
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.098920867384963085   to: 0.098290203063180703
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.098962651385983091   to: 0.098344546261812424
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.099003463724394336   to: 0.098397630334643355
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.099043329949190406   to: 0.098449488190901255
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.099082274864226702   to: 0.098500151789248230
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.099120322551909223   to: 0.098549652167538027
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.099157496396076947   to: 0.098598019471584891
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.099193819104106143   to: 0.098645282982977825
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.099229312728264782   to: 0.098691471145973125
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.099263998686343433   to: 0.098736611593496698
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.099297897781588915   to: 0.098780731172286820
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.099331030221965086   to: 0.098823855967206775
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.099363415638765051   to: 0.098866011324755879
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.099395073104597678   to: 0.098907221875806389
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.099426021150770538   to: 0.098947511557593029
i:  60, name:               module.fc.weight  changing lr from: 0.099456277784091079   to: 0.098986903634980472
i:  61, name:                 module.fc.bias  changing lr from: 0.099485860503106094   to: 0.099025420721033819



# Switched to train mode...
Epoch: [8][  0/391]	Time  0.195 ( 0.195)	Data  0.151 ( 0.151)	Loss 1.2732e+00 (1.2732e+00)	Acc@1  54.69 ( 54.69)	Acc@5  92.19 ( 92.19)
Epoch: [8][ 10/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.0525e+00 (1.3673e+00)	Acc@1  66.41 ( 60.30)	Acc@5  92.97 ( 88.42)
Epoch: [8][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.3316e+00 (1.3415e+00)	Acc@1  60.94 ( 61.09)	Acc@5  88.28 ( 89.03)
Epoch: [8][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.4233e+00 (1.3222e+00)	Acc@1  61.72 ( 61.82)	Acc@5  89.06 ( 89.34)
Epoch: [8][ 40/391]	Time  0.045 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.2093e+00 (1.3157e+00)	Acc@1  61.72 ( 61.76)	Acc@5  88.28 ( 89.33)
Epoch: [8][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.2995e+00 (1.3121e+00)	Acc@1  61.72 ( 62.09)	Acc@5  88.28 ( 89.25)
Epoch: [8][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0901e+00 (1.3063e+00)	Acc@1  69.53 ( 62.33)	Acc@5  94.53 ( 89.42)
Epoch: [8][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.1130e+00 (1.3028e+00)	Acc@1  69.53 ( 62.49)	Acc@5  94.53 ( 89.49)
Epoch: [8][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4022e+00 (1.3110e+00)	Acc@1  58.59 ( 62.09)	Acc@5  88.28 ( 89.43)
Epoch: [8][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4264e+00 (1.3152e+00)	Acc@1  52.34 ( 61.82)	Acc@5  86.72 ( 89.30)
Epoch: [8][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4663e+00 (1.3141e+00)	Acc@1  54.69 ( 61.64)	Acc@5  85.94 ( 89.33)
Epoch: [8][110/391]	Time  0.048 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3588e+00 (1.3199e+00)	Acc@1  66.41 ( 61.54)	Acc@5  88.28 ( 89.18)
Epoch: [8][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3039e+00 (1.3233e+00)	Acc@1  59.38 ( 61.47)	Acc@5  87.50 ( 89.00)
Epoch: [8][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3120e+00 (1.3298e+00)	Acc@1  63.28 ( 61.26)	Acc@5  89.84 ( 88.91)
Epoch: [8][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4427e+00 (1.3275e+00)	Acc@1  57.03 ( 61.35)	Acc@5  86.72 ( 89.01)
Epoch: [8][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3207e+00 (1.3276e+00)	Acc@1  62.50 ( 61.40)	Acc@5  89.84 ( 88.96)
Epoch: [8][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1397e+00 (1.3268e+00)	Acc@1  68.75 ( 61.40)	Acc@5  90.62 ( 88.98)
Epoch: [8][170/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2503e+00 (1.3274e+00)	Acc@1  63.28 ( 61.44)	Acc@5  90.62 ( 89.00)
Epoch: [8][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2783e+00 (1.3327e+00)	Acc@1  65.62 ( 61.35)	Acc@5  88.28 ( 88.92)
Epoch: [8][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4209e+00 (1.3337e+00)	Acc@1  55.47 ( 61.35)	Acc@5  89.06 ( 88.89)
Epoch: [8][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1767e+00 (1.3316e+00)	Acc@1  63.28 ( 61.40)	Acc@5  90.62 ( 88.93)
Epoch: [8][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2205e+00 (1.3283e+00)	Acc@1  64.84 ( 61.43)	Acc@5  91.41 ( 88.98)
Epoch: [8][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3524e+00 (1.3290e+00)	Acc@1  60.94 ( 61.40)	Acc@5  92.97 ( 88.96)
Epoch: [8][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2138e+00 (1.3302e+00)	Acc@1  60.16 ( 61.39)	Acc@5  89.84 ( 88.94)
Epoch: [8][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3186e+00 (1.3305e+00)	Acc@1  65.62 ( 61.42)	Acc@5  90.62 ( 88.93)
Epoch: [8][250/391]	Time  0.044 ( 0.047)	Data  0.002 ( 0.003)	Loss 1.1726e+00 (1.3353e+00)	Acc@1  71.09 ( 61.34)	Acc@5  91.41 ( 88.83)
Epoch: [8][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3765e+00 (1.3356e+00)	Acc@1  60.94 ( 61.31)	Acc@5  91.41 ( 88.86)
Epoch: [8][270/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0558e+00 (1.3368e+00)	Acc@1  70.31 ( 61.33)	Acc@5  92.19 ( 88.82)
Epoch: [8][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1324e+00 (1.3373e+00)	Acc@1  64.06 ( 61.33)	Acc@5  93.75 ( 88.84)
Epoch: [8][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4277e+00 (1.3361e+00)	Acc@1  58.59 ( 61.36)	Acc@5  89.84 ( 88.85)
Epoch: [8][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6019e+00 (1.3364e+00)	Acc@1  56.25 ( 61.36)	Acc@5  79.69 ( 88.82)
Epoch: [8][310/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3277e+00 (1.3384e+00)	Acc@1  61.72 ( 61.34)	Acc@5  90.62 ( 88.79)
Epoch: [8][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2519e+00 (1.3404e+00)	Acc@1  65.62 ( 61.30)	Acc@5  90.62 ( 88.77)
Epoch: [8][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2569e+00 (1.3414e+00)	Acc@1  60.16 ( 61.27)	Acc@5  91.41 ( 88.77)
Epoch: [8][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2542e+00 (1.3399e+00)	Acc@1  59.38 ( 61.32)	Acc@5  90.62 ( 88.78)
Epoch: [8][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4630e+00 (1.3393e+00)	Acc@1  56.25 ( 61.32)	Acc@5  87.50 ( 88.80)
Epoch: [8][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2307e+00 (1.3371e+00)	Acc@1  63.28 ( 61.40)	Acc@5  92.97 ( 88.85)
Epoch: [8][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2815e+00 (1.3362e+00)	Acc@1  63.28 ( 61.43)	Acc@5  92.19 ( 88.89)
Epoch: [8][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5707e+00 (1.3346e+00)	Acc@1  54.69 ( 61.44)	Acc@5  89.06 ( 88.91)
Epoch: [8][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2380e+00 (1.3341e+00)	Acc@1  68.75 ( 61.47)	Acc@5  93.75 ( 88.92)
## e[8] optimizer.zero_grad (sum) time: 0.1395552158355713
## e[8]       loss.backward (sum) time: 2.5162811279296875
## e[8]      optimizer.step (sum) time: 1.0148370265960693
## epoch[8] training(only) time: 18.475462436676025
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.5888e+00 (1.5888e+00)	Acc@1  59.00 ( 59.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.8943e+00 (1.6452e+00)	Acc@1  54.00 ( 58.09)	Acc@5  80.00 ( 82.91)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4519e+00 (1.6166e+00)	Acc@1  61.00 ( 58.33)	Acc@5  87.00 ( 84.10)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.9066e+00 (1.6497e+00)	Acc@1  53.00 ( 57.19)	Acc@5  80.00 ( 84.10)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.7804e+00 (1.6551e+00)	Acc@1  53.00 ( 56.83)	Acc@5  82.00 ( 84.05)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.3712e+00 (1.6619e+00)	Acc@1  61.00 ( 56.61)	Acc@5  87.00 ( 83.71)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.5657e+00 (1.6431e+00)	Acc@1  53.00 ( 56.54)	Acc@5  89.00 ( 84.25)
Test: [ 70/100]	Time  0.024 ( 0.025)	Loss 1.6684e+00 (1.6574e+00)	Acc@1  54.00 ( 56.31)	Acc@5  85.00 ( 84.00)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.7525e+00 (1.6617e+00)	Acc@1  58.00 ( 56.22)	Acc@5  81.00 ( 83.86)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.9039e+00 (1.6578e+00)	Acc@1  50.00 ( 56.30)	Acc@5  78.00 ( 84.05)
 * Acc@1 56.250 Acc@5 84.250
### epoch[8] execution time: 20.96564507484436
EPOCH 9
i:   0, name:          module.conv1.0.weight  changing lr from: 0.093745593533647337   to: 0.091879240657579200
i:   1, name:          module.conv1.1.weight  changing lr from: 0.093914443131776104   to: 0.092090115243586401
i:   2, name:            module.conv1.1.bias  changing lr from: 0.094078807409037013   to: 0.092295454691886714
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.094238818152160550   to: 0.092495418223971296
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.094394603007585745   to: 0.092690160250387718
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.094546285614098280   to: 0.092879830512993256
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.094693985731534092   to: 0.093064574223828703
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.094837819365622189   to: 0.093244532200602567
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.094977898889043677   to: 0.093419840998785891
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.095114333158787243   to: 0.093590633040328983
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.095247227629882802   to: 0.093757036739018668
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.095376684465597425   to: 0.093919176622504180
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.095502802644178209   to: 0.094077173451024773
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.095625678062227798   to: 0.094231144332879227
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.095745403634798776   to: 0.094381202836682013
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.095862069392292748   to: 0.094527459100454958
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.095975762574250237   to: 0.094670019937607347
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.096086567720116617   to: 0.094808988939860367
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.096194566757068972   to: 0.094944466577174036
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.096299839084987390   to: 0.095076550294737189
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.096402461658653779   to: 0.095205334607082803
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.096502509067259504   to: 0.095330911189391659
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.096600053611302195   to: 0.095453368966049126
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.096695165376950498   to: 0.095572794196519567
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.096787912307954124   to: 0.095689270558603934
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.096878360275175046   to: 0.095802879229146043
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.096966573143813697   to: 0.095913698962252616
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.097052612838402857   to: 0.096021806165092330
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.097136539405640002   to: 0.096127274971338439
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.097218411075126993   to: 0.096230177312319143
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.097298284318084433   to: 0.096330582985938801
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.097376213904106432   to: 0.096428559723432741
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.097452252956019253   to: 0.096524173254017132
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.097526453002906258   to: 0.096617487367494537
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.097598864031359117   to: 0.096708563974874942
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.097669534535014313   to: 0.096797463167070441
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.097738511562431393   to: 0.096884243271720946
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.097805840763368637   to: 0.096968960908207225
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.097871566433509444   to: 0.097051671040905868
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.097935731557691516   to: 0.097132427030740168
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.097998377851689245   to: 0.097211280685078866
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.098059545802598000   to: 0.097288282306034257
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.098119274707867771   to: 0.097363480737209407
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.098177602713031706   to: 0.097436923408942749
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.098234566848174110   to: 0.097508656382097894
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.098290203063180703   to: 0.097578724390444183
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.098344546261812424   to: 0.097647170881672823
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.098397630334643355   to: 0.097714038057092611
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.098449488190901255   to: 0.097779366910046808
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.098500151789248230   to: 0.097843197263092846
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.098549652167538027   to: 0.097905567803984461
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.098598019471584891   to: 0.097966516120495006
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.098645282982977825   to: 0.098026078734119726
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.098691471145973125   to: 0.098084291132693013
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.098736611593496698   to: 0.098141187801956561
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.098780731172286820   to: 0.098196802256112006
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.098823855967206775   to: 0.098251167067391870
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.098866011324755879   to: 0.098304313894680584
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.098907221875806389   to: 0.098356273511216868
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.098947511557593029   to: 0.098407075831407648
i:  60, name:               module.fc.weight  changing lr from: 0.098986903634980472   to: 0.098456749936782703
i:  61, name:                 module.fc.bias  changing lr from: 0.099025420721033819   to: 0.098505324101118397



# Switched to train mode...
Epoch: [9][  0/391]	Time  0.197 ( 0.197)	Data  0.152 ( 0.152)	Loss 1.1808e+00 (1.1808e+00)	Acc@1  67.19 ( 67.19)	Acc@5  89.06 ( 89.06)
Epoch: [9][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.0129e+00 (1.2024e+00)	Acc@1  67.97 ( 64.63)	Acc@5  93.75 ( 89.99)
Epoch: [9][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.2008e+00 (1.1630e+00)	Acc@1  69.53 ( 65.85)	Acc@5  88.28 ( 91.03)
Epoch: [9][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.3763e+00 (1.1842e+00)	Acc@1  58.59 ( 65.32)	Acc@5  90.62 ( 90.62)
Epoch: [9][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.0720e+00 (1.1805e+00)	Acc@1  68.75 ( 65.61)	Acc@5  92.19 ( 90.57)
Epoch: [9][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4005e+00 (1.2040e+00)	Acc@1  55.47 ( 64.87)	Acc@5  86.72 ( 90.33)
Epoch: [9][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.3228e+00 (1.2026e+00)	Acc@1  59.38 ( 64.81)	Acc@5  92.19 ( 90.54)
Epoch: [9][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.2103e+00 (1.2020e+00)	Acc@1  65.62 ( 64.90)	Acc@5  91.41 ( 90.59)
Epoch: [9][ 80/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2323e+00 (1.2017e+00)	Acc@1  64.06 ( 64.83)	Acc@5  90.62 ( 90.76)
Epoch: [9][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3588e+00 (1.2122e+00)	Acc@1  60.16 ( 64.48)	Acc@5  87.50 ( 90.51)
Epoch: [9][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0974e+00 (1.2166e+00)	Acc@1  67.19 ( 64.36)	Acc@5  94.53 ( 90.45)
Epoch: [9][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1906e+00 (1.2191e+00)	Acc@1  67.97 ( 64.20)	Acc@5  89.06 ( 90.55)
Epoch: [9][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1229e+00 (1.2235e+00)	Acc@1  73.44 ( 64.05)	Acc@5  87.50 ( 90.46)
Epoch: [9][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1843e+00 (1.2222e+00)	Acc@1  67.19 ( 64.09)	Acc@5  89.06 ( 90.42)
Epoch: [9][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3823e+00 (1.2195e+00)	Acc@1  60.16 ( 64.16)	Acc@5  89.84 ( 90.47)
Epoch: [9][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3616e+00 (1.2216e+00)	Acc@1  60.94 ( 64.12)	Acc@5  89.06 ( 90.52)
Epoch: [9][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6005e+00 (1.2276e+00)	Acc@1  58.59 ( 63.96)	Acc@5  82.81 ( 90.38)
Epoch: [9][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2592e+00 (1.2310e+00)	Acc@1  60.94 ( 63.88)	Acc@5  88.28 ( 90.33)
Epoch: [9][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2798e+00 (1.2320e+00)	Acc@1  59.38 ( 63.86)	Acc@5  91.41 ( 90.34)
Epoch: [9][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3990e+00 (1.2366e+00)	Acc@1  59.38 ( 63.69)	Acc@5  89.84 ( 90.35)
Epoch: [9][200/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2485e+00 (1.2390e+00)	Acc@1  65.62 ( 63.64)	Acc@5  89.06 ( 90.28)
Epoch: [9][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4476e+00 (1.2384e+00)	Acc@1  55.47 ( 63.68)	Acc@5  84.38 ( 90.27)
Epoch: [9][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1613e+00 (1.2367e+00)	Acc@1  63.28 ( 63.67)	Acc@5  95.31 ( 90.34)
Epoch: [9][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0818e+00 (1.2360e+00)	Acc@1  67.97 ( 63.72)	Acc@5  90.62 ( 90.37)
Epoch: [9][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2545e+00 (1.2372e+00)	Acc@1  63.28 ( 63.72)	Acc@5  92.19 ( 90.36)
Epoch: [9][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4041e+00 (1.2353e+00)	Acc@1  57.81 ( 63.73)	Acc@5  85.16 ( 90.36)
Epoch: [9][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0179e+00 (1.2349e+00)	Acc@1  72.66 ( 63.74)	Acc@5  92.19 ( 90.36)
Epoch: [9][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5191e+00 (1.2383e+00)	Acc@1  51.56 ( 63.62)	Acc@5  85.94 ( 90.31)
Epoch: [9][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2541e+00 (1.2380e+00)	Acc@1  64.06 ( 63.64)	Acc@5  89.06 ( 90.34)
Epoch: [9][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4155e+00 (1.2366e+00)	Acc@1  57.81 ( 63.67)	Acc@5  91.41 ( 90.36)
Epoch: [9][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3849e+00 (1.2395e+00)	Acc@1  59.38 ( 63.61)	Acc@5  84.38 ( 90.33)
Epoch: [9][310/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1586e+00 (1.2412e+00)	Acc@1  68.75 ( 63.57)	Acc@5  89.06 ( 90.28)
Epoch: [9][320/391]	Time  0.051 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3135e+00 (1.2433e+00)	Acc@1  64.06 ( 63.54)	Acc@5  89.06 ( 90.22)
Epoch: [9][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0230e+00 (1.2419e+00)	Acc@1  67.97 ( 63.61)	Acc@5  92.97 ( 90.22)
Epoch: [9][340/391]	Time  0.050 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1573e+00 (1.2392e+00)	Acc@1  67.97 ( 63.72)	Acc@5  89.84 ( 90.28)
Epoch: [9][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2435e+00 (1.2391e+00)	Acc@1  61.72 ( 63.73)	Acc@5  91.41 ( 90.30)
Epoch: [9][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2371e+00 (1.2380e+00)	Acc@1  61.72 ( 63.78)	Acc@5  89.06 ( 90.30)
Epoch: [9][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1724e+00 (1.2375e+00)	Acc@1  63.28 ( 63.80)	Acc@5  93.75 ( 90.33)
Epoch: [9][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2902e+00 (1.2375e+00)	Acc@1  63.28 ( 63.78)	Acc@5  86.72 ( 90.34)
Epoch: [9][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2849e+00 (1.2373e+00)	Acc@1  61.25 ( 63.77)	Acc@5  91.25 ( 90.35)
## e[9] optimizer.zero_grad (sum) time: 0.13841772079467773
## e[9]       loss.backward (sum) time: 2.529886484146118
## e[9]      optimizer.step (sum) time: 1.0181047916412354
## epoch[9] training(only) time: 18.477105379104614
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.3480e+00 (1.3480e+00)	Acc@1  64.00 ( 64.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.7905e+00 (1.5864e+00)	Acc@1  48.00 ( 57.64)	Acc@5  84.00 ( 85.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.5628e+00 (1.5847e+00)	Acc@1  59.00 ( 57.48)	Acc@5  87.00 ( 85.29)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.4916e+00 (1.5632e+00)	Acc@1  56.00 ( 57.65)	Acc@5  91.00 ( 85.65)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.4540e+00 (1.5449e+00)	Acc@1  60.00 ( 57.83)	Acc@5  88.00 ( 86.05)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4222e+00 (1.5488e+00)	Acc@1  60.00 ( 57.71)	Acc@5  85.00 ( 85.75)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3703e+00 (1.5301e+00)	Acc@1  55.00 ( 57.98)	Acc@5  93.00 ( 86.13)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.4763e+00 (1.5361e+00)	Acc@1  59.00 ( 57.86)	Acc@5  88.00 ( 86.03)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5948e+00 (1.5441e+00)	Acc@1  57.00 ( 57.75)	Acc@5  86.00 ( 85.80)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.5565e+00 (1.5322e+00)	Acc@1  57.00 ( 58.23)	Acc@5  81.00 ( 85.75)
 * Acc@1 58.160 Acc@5 85.780
### epoch[9] execution time: 20.977277040481567
EPOCH 10
i:   0, name:          module.conv1.0.weight  changing lr from: 0.091879240657579200   to: 0.089822950858548800
i:   1, name:          module.conv1.1.weight  changing lr from: 0.092090115243586401   to: 0.090079423385579574
i:   2, name:            module.conv1.1.bias  changing lr from: 0.092295454691886714   to: 0.090329255421259608
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.092495418223971296   to: 0.090572633268981573
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.092690160250387718   to: 0.090809737870445245
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.092879830512993256   to: 0.091040744947120666
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.093064574223828703   to: 0.091265825139645707
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.093244532200602567   to: 0.091485144145010230
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.093419840998785891   to: 0.091698862851402718
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.093590633040328983   to: 0.091907137470614769
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.093757036739018668   to: 0.092110119667917767
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.093919176622504180   to: 0.092307956689343107
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.094077173451024773   to: 0.092500791486311504
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.094231144332879227   to: 0.092688762837570937
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.094381202836682013   to: 0.092872005468414856
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.094527459100454958   to: 0.093050650167162574
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.094670019937607347   to: 0.093224823898893736
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.094808988939860367   to: 0.093394649916437408
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.094944466577174036   to: 0.093560247868623450
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.095076550294737189   to: 0.093721733905811011
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.095205334607082803   to: 0.093879220782714407
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.095330911189391659   to: 0.094032817958552575
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.095453368966049126   to: 0.094182631694551766
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.095572794196519567   to: 0.094328765148836091
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.095689270558603934   to: 0.094471318468743126
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.095802879229146043   to: 0.094610388880605481
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.095913698962252616   to: 0.094746070777040670
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.096021806165092330   to: 0.094878455801794687
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.096127274971338439   to: 0.095007632932185920
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.096230177312319143   to: 0.095133688559197127
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.096330582985938801   to: 0.095256706565265067
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.096428559723432741   to: 0.095376768399817347
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.096524173254017132   to: 0.095493953152607405
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.096617487367494537   to: 0.095608337624898232
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.096708563974874942   to: 0.095719996398546142
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.096797463167070441   to: 0.095829001903035810
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.096884243271720946   to: 0.095935424480517506
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.096968960908207225   to: 0.096039332448897383
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.097051671040905868   to: 0.096140792163031133
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.097132427030740168   to: 0.096239868074071394
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.097211280685078866   to: 0.096336622787017628
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.097288282306034257   to: 0.096431117116518028
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.097363480737209407   to: 0.096523410140971030
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.097436923408942749   to: 0.096613559254973946
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.097508656382097894   to: 0.096701620220165388
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.097578724390444183   to: 0.096787647214506972
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.097647170881672823   to: 0.096871692880049265
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.097714038057092611   to: 0.096953808369225999
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.097779366910046808   to: 0.097034043389719327
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.097843197263092846   to: 0.097112446247938536
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.097905567803984461   to: 0.097189063891152880
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.097966516120495006   to: 0.097263941948319221
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.098026078734119726   to: 0.097337124769643360
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.098084291132693013   to: 0.097408655464913382
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.098141187801956561   to: 0.097478575940642173
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.098196802256112006   to: 0.097546926936055822
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.098251167067391870   to: 0.097613748057962746
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.098304313894680584   to: 0.097679077814538467
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.098356273511216868   to: 0.097742953648059072
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.098407075831407648   to: 0.097805411966616490
i:  60, name:               module.fc.weight  changing lr from: 0.098456749936782703   to: 0.097866488174846555
i:  61, name:                 module.fc.bias  changing lr from: 0.098505324101118397   to: 0.097926216703701385



# Switched to train mode...
Epoch: [10][  0/391]	Time  0.192 ( 0.192)	Data  0.149 ( 0.149)	Loss 1.1278e+00 (1.1278e+00)	Acc@1  67.19 ( 67.19)	Acc@5  92.19 ( 92.19)
Epoch: [10][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 1.0185e+00 (1.1300e+00)	Acc@1  73.44 ( 66.90)	Acc@5  90.62 ( 91.48)
Epoch: [10][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.0118e+00 (1.1026e+00)	Acc@1  66.41 ( 67.04)	Acc@5  93.75 ( 92.15)
Epoch: [10][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.3053e+00 (1.1015e+00)	Acc@1  59.38 ( 66.86)	Acc@5  89.84 ( 92.36)
Epoch: [10][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.1261e+00 (1.0939e+00)	Acc@1  67.19 ( 67.09)	Acc@5  92.19 ( 92.49)
Epoch: [10][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.1620e+00 (1.0998e+00)	Acc@1  67.97 ( 67.23)	Acc@5  90.62 ( 92.54)
Epoch: [10][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.1287e+00 (1.1059e+00)	Acc@1  68.75 ( 66.91)	Acc@5  92.19 ( 92.44)
Epoch: [10][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 1.1225e+00 (1.1061e+00)	Acc@1  68.75 ( 66.90)	Acc@5  92.97 ( 92.46)
Epoch: [10][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1351e+00 (1.1040e+00)	Acc@1  66.41 ( 67.11)	Acc@5  91.41 ( 92.35)
Epoch: [10][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1402e+00 (1.1059e+00)	Acc@1  64.84 ( 67.08)	Acc@5  89.84 ( 92.26)
Epoch: [10][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0096e+00 (1.1033e+00)	Acc@1  71.88 ( 67.19)	Acc@5  92.19 ( 92.30)
Epoch: [10][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.8199e-01 (1.1018e+00)	Acc@1  70.31 ( 67.27)	Acc@5  95.31 ( 92.27)
Epoch: [10][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.5099e-01 (1.1042e+00)	Acc@1  71.88 ( 67.17)	Acc@5  95.31 ( 92.25)
Epoch: [10][130/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.2882e-01 (1.1044e+00)	Acc@1  76.56 ( 67.09)	Acc@5  96.88 ( 92.25)
Epoch: [10][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.1013e+00 (1.1092e+00)	Acc@1  67.19 ( 66.98)	Acc@5  93.75 ( 92.20)
Epoch: [10][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 8.9364e-01 (1.1045e+00)	Acc@1  72.66 ( 67.04)	Acc@5  95.31 ( 92.24)
Epoch: [10][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3193e+00 (1.1067e+00)	Acc@1  61.72 ( 67.03)	Acc@5  89.06 ( 92.13)
Epoch: [10][170/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3112e+00 (1.1116e+00)	Acc@1  64.84 ( 67.00)	Acc@5  87.50 ( 92.02)
Epoch: [10][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3191e+00 (1.1173e+00)	Acc@1  64.84 ( 66.81)	Acc@5  86.72 ( 91.94)
Epoch: [10][190/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0344e+00 (1.1178e+00)	Acc@1  67.97 ( 66.84)	Acc@5  92.97 ( 91.93)
Epoch: [10][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4601e+00 (1.1203e+00)	Acc@1  55.47 ( 66.75)	Acc@5  86.72 ( 91.84)
Epoch: [10][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0791e+00 (1.1232e+00)	Acc@1  64.84 ( 66.70)	Acc@5  91.41 ( 91.80)
Epoch: [10][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0963e+00 (1.1244e+00)	Acc@1  69.53 ( 66.72)	Acc@5  91.41 ( 91.76)
Epoch: [10][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1962e+00 (1.1245e+00)	Acc@1  64.84 ( 66.68)	Acc@5  92.19 ( 91.79)
Epoch: [10][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2262e+00 (1.1261e+00)	Acc@1  63.28 ( 66.70)	Acc@5  92.19 ( 91.79)
Epoch: [10][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1812e+00 (1.1263e+00)	Acc@1  64.84 ( 66.68)	Acc@5  88.28 ( 91.76)
Epoch: [10][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1551e+00 (1.1293e+00)	Acc@1  62.50 ( 66.57)	Acc@5  91.41 ( 91.73)
Epoch: [10][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1285e+00 (1.1302e+00)	Acc@1  61.72 ( 66.56)	Acc@5  94.53 ( 91.73)
Epoch: [10][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4584e+00 (1.1336e+00)	Acc@1  57.81 ( 66.48)	Acc@5  85.94 ( 91.69)
Epoch: [10][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0303e+00 (1.1332e+00)	Acc@1  68.75 ( 66.56)	Acc@5  92.97 ( 91.67)
Epoch: [10][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0824e+00 (1.1303e+00)	Acc@1  67.19 ( 66.67)	Acc@5  90.62 ( 91.68)
Epoch: [10][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2152e+00 (1.1318e+00)	Acc@1  65.62 ( 66.64)	Acc@5  90.62 ( 91.65)
Epoch: [10][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2172e-01 (1.1298e+00)	Acc@1  72.66 ( 66.65)	Acc@5  88.28 ( 91.65)
Epoch: [10][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0192e+00 (1.1303e+00)	Acc@1  66.41 ( 66.64)	Acc@5  91.41 ( 91.62)
Epoch: [10][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0791e+00 (1.1314e+00)	Acc@1  61.72 ( 66.60)	Acc@5  92.19 ( 91.61)
Epoch: [10][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1068e+00 (1.1357e+00)	Acc@1  65.62 ( 66.49)	Acc@5  92.19 ( 91.54)
Epoch: [10][360/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1215e+00 (1.1388e+00)	Acc@1  65.62 ( 66.38)	Acc@5  89.84 ( 91.50)
Epoch: [10][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4514e+00 (1.1411e+00)	Acc@1  55.47 ( 66.33)	Acc@5  91.41 ( 91.48)
Epoch: [10][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0856e+00 (1.1408e+00)	Acc@1  64.84 ( 66.38)	Acc@5  92.19 ( 91.47)
Epoch: [10][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0784e+00 (1.1405e+00)	Acc@1  65.00 ( 66.39)	Acc@5  96.25 ( 91.48)
## e[10] optimizer.zero_grad (sum) time: 0.13676071166992188
## e[10]       loss.backward (sum) time: 2.5205392837524414
## e[10]      optimizer.step (sum) time: 1.0121815204620361
## epoch[10] training(only) time: 18.432610511779785
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.3400e+00 (1.3400e+00)	Acc@1  61.00 ( 61.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.6266e+00 (1.4332e+00)	Acc@1  56.00 ( 61.09)	Acc@5  85.00 ( 86.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.2593e+00 (1.4211e+00)	Acc@1  67.00 ( 61.62)	Acc@5  89.00 ( 87.48)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.4854e+00 (1.4269e+00)	Acc@1  66.00 ( 61.52)	Acc@5  88.00 ( 87.13)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4637e+00 (1.4149e+00)	Acc@1  57.00 ( 61.46)	Acc@5  91.00 ( 87.41)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.2991e+00 (1.4357e+00)	Acc@1  64.00 ( 60.69)	Acc@5  89.00 ( 87.20)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4263e+00 (1.4218e+00)	Acc@1  58.00 ( 61.05)	Acc@5  86.00 ( 87.49)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5446e+00 (1.4281e+00)	Acc@1  60.00 ( 61.04)	Acc@5  86.00 ( 87.35)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.4540e+00 (1.4364e+00)	Acc@1  60.00 ( 60.72)	Acc@5  86.00 ( 87.27)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.5319e+00 (1.4267e+00)	Acc@1  56.00 ( 60.78)	Acc@5  87.00 ( 87.44)
 * Acc@1 60.900 Acc@5 87.360
### epoch[10] execution time: 20.918267726898193
EPOCH 11
i:   0, name:          module.conv1.0.weight  changing lr from: 0.089822950858548800   to: 0.087586278261115785
i:   1, name:          module.conv1.1.weight  changing lr from: 0.090079423385579574   to: 0.087891487038548419
i:   2, name:            module.conv1.1.bias  changing lr from: 0.090329255421259608   to: 0.088188915464732759
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.090572633268981573   to: 0.088478775502020046
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.090809737870445245   to: 0.088761273376730121
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.091040744947120666   to: 0.089036609706376196
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.091265825139645707   to: 0.089304979627058662
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.091485144145010230   to: 0.089566572920682153
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.091698862851402718   to: 0.089821574141689745
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.091907137470614769   to: 0.090070162743041693
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.092110119667917767   to: 0.090312513201198652
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.092307956689343107   to: 0.090548795139897151
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.092500791486311504   to: 0.090779173452532516
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.092688762837570937   to: 0.091003808422987123
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.092872005468414856   to: 0.091222855844764306
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.093050650167162574   to: 0.091436467138307859
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.093224823898893736   to: 0.091644789466404475
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.093394649916437408   to: 0.091847965847583257
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.093560247868623450   to: 0.092046135267440571
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.093721733905811011   to: 0.092239432787832004
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.093879220782714407   to: 0.092427989653885101
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.094032817958552575   to: 0.092611933398797225
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.094182631694551766   to: 0.092791387946392551
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.094328765148836091   to: 0.092966473711421069
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.094471318468743126   to: 0.093137307697589947
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.094610388880605481   to: 0.093304003593324969
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.094746070777040670   to: 0.093466671865265305
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.094878455801794687   to: 0.093625419849501304
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.095007632932185920   to: 0.093780351840569043
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.095133688559197127   to: 0.093931569178219876
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.095256706565265067   to: 0.094079170331987816
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.095376768399817347   to: 0.094223250983579226
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.095493953152607405   to: 0.094363904107114066
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.095608337624898232   to: 0.094501220047249215
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.095719996398546142   to: 0.094635286595216681
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.095829001903035810   to: 0.094766189062811826
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.095935424480517506   to: 0.094894010354367869
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.096039332448897383   to: 0.095018831036754126
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.096140792163031133   to: 0.095140729407436841
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.096239868074071394   to: 0.095259781560641868
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.096336622787017628   to: 0.095376061451659430
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.096431117116518028   to: 0.095489640959331185
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.096523410140971030   to: 0.095600589946760905
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.096613559254973946   to: 0.095708976320289019
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.096701620220165388   to: 0.095814866086772671
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.096787647214506972   to: 0.095918323409211595
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.096871692880049265   to: 0.096019410660760987
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.096953808369225999   to: 0.096118188477171407
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.097034043389719327   to: 0.096214715807695986
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.097112446247938536   to: 0.096309049964504650
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.097189063891152880   to: 0.096401246670644272
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.097263941948319221   to: 0.096491360106583643
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.097337124769643360   to: 0.096579442955381303
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.097408655464913382   to: 0.096665546446513650
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.097478575940642173   to: 0.096749720398400019
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.097546926936055822   to: 0.096832013259661329
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.097613748057962746   to: 0.096912472149147055
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.097679077814538467   to: 0.096991142894766125
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.097742953648059072   to: 0.097068070071155005
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.097805411966616490   to: 0.097143297036216825
i:  60, name:               module.fc.weight  changing lr from: 0.097866488174846555   to: 0.097216865966563759
i:  61, name:                 module.fc.bias  changing lr from: 0.097926216703701385   to: 0.097288817891894766



# Switched to train mode...
Epoch: [11][  0/391]	Time  0.189 ( 0.189)	Data  0.147 ( 0.147)	Loss 8.7247e-01 (8.7247e-01)	Acc@1  74.22 ( 74.22)	Acc@5  96.09 ( 96.09)
Epoch: [11][ 10/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.0749e+00 (9.8040e-01)	Acc@1  68.75 ( 70.67)	Acc@5  92.97 ( 94.11)
Epoch: [11][ 20/391]	Time  0.048 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.0265e+00 (1.0150e+00)	Acc@1  67.97 ( 69.57)	Acc@5  95.31 ( 93.75)
Epoch: [11][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 9.9491e-01 (1.0224e+00)	Acc@1  68.75 ( 69.35)	Acc@5  96.09 ( 93.72)
Epoch: [11][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.0483e+00 (1.0290e+00)	Acc@1  71.09 ( 69.46)	Acc@5  92.19 ( 93.50)
Epoch: [11][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.1050e-01 (1.0218e+00)	Acc@1  71.09 ( 69.62)	Acc@5  92.19 ( 93.58)
Epoch: [11][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0695e+00 (1.0253e+00)	Acc@1  68.75 ( 69.47)	Acc@5  93.75 ( 93.60)
Epoch: [11][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 1.1565e+00 (1.0354e+00)	Acc@1  67.19 ( 69.15)	Acc@5  90.62 ( 93.34)
Epoch: [11][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.9961e-01 (1.0305e+00)	Acc@1  74.22 ( 69.40)	Acc@5  91.41 ( 93.42)
Epoch: [11][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.6146e-01 (1.0235e+00)	Acc@1  72.66 ( 69.58)	Acc@5  96.09 ( 93.37)
Epoch: [11][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.9800e-01 (1.0242e+00)	Acc@1  71.09 ( 69.45)	Acc@5  94.53 ( 93.39)
Epoch: [11][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0668e+00 (1.0256e+00)	Acc@1  66.41 ( 69.54)	Acc@5  93.75 ( 93.40)
Epoch: [11][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2178e+00 (1.0261e+00)	Acc@1  63.28 ( 69.51)	Acc@5  89.84 ( 93.38)
Epoch: [11][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.9263e-01 (1.0263e+00)	Acc@1  75.00 ( 69.44)	Acc@5  94.53 ( 93.41)
Epoch: [11][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.2478e+00 (1.0285e+00)	Acc@1  62.50 ( 69.39)	Acc@5  92.97 ( 93.39)
Epoch: [11][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.2545e+00 (1.0314e+00)	Acc@1  58.59 ( 69.19)	Acc@5  93.75 ( 93.38)
Epoch: [11][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1486e+00 (1.0329e+00)	Acc@1  67.97 ( 69.10)	Acc@5  92.19 ( 93.39)
Epoch: [11][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4665e-01 (1.0367e+00)	Acc@1  71.88 ( 69.03)	Acc@5  91.41 ( 93.30)
Epoch: [11][180/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0491e-01 (1.0371e+00)	Acc@1  78.12 ( 69.03)	Acc@5  94.53 ( 93.29)
Epoch: [11][190/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2880e+00 (1.0394e+00)	Acc@1  58.59 ( 68.96)	Acc@5  86.72 ( 93.20)
Epoch: [11][200/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0838e+00 (1.0417e+00)	Acc@1  68.75 ( 68.87)	Acc@5  90.62 ( 93.14)
Epoch: [11][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2097e+00 (1.0453e+00)	Acc@1  61.72 ( 68.79)	Acc@5  93.75 ( 93.05)
Epoch: [11][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5311e-01 (1.0451e+00)	Acc@1  72.66 ( 68.77)	Acc@5  92.97 ( 92.99)
Epoch: [11][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3440e+00 (1.0451e+00)	Acc@1  64.06 ( 68.83)	Acc@5  89.06 ( 92.93)
Epoch: [11][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0812e+00 (1.0462e+00)	Acc@1  67.97 ( 68.78)	Acc@5  91.41 ( 92.90)
Epoch: [11][250/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0919e+00 (1.0475e+00)	Acc@1  68.75 ( 68.82)	Acc@5  94.53 ( 92.85)
Epoch: [11][260/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2817e+00 (1.0511e+00)	Acc@1  61.72 ( 68.70)	Acc@5  91.41 ( 92.86)
Epoch: [11][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7367e-01 (1.0544e+00)	Acc@1  71.09 ( 68.64)	Acc@5  93.75 ( 92.81)
Epoch: [11][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2802e+00 (1.0557e+00)	Acc@1  59.38 ( 68.57)	Acc@5  94.53 ( 92.82)
Epoch: [11][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1620e+00 (1.0571e+00)	Acc@1  64.06 ( 68.49)	Acc@5  92.19 ( 92.79)
Epoch: [11][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2243e+00 (1.0598e+00)	Acc@1  65.62 ( 68.39)	Acc@5  92.19 ( 92.77)
Epoch: [11][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1553e+00 (1.0611e+00)	Acc@1  66.41 ( 68.40)	Acc@5  89.06 ( 92.72)
Epoch: [11][320/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2000e+00 (1.0620e+00)	Acc@1  64.84 ( 68.37)	Acc@5  89.84 ( 92.71)
Epoch: [11][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7015e-01 (1.0629e+00)	Acc@1  71.88 ( 68.30)	Acc@5  94.53 ( 92.71)
Epoch: [11][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9682e-01 (1.0637e+00)	Acc@1  72.66 ( 68.28)	Acc@5  96.09 ( 92.69)
Epoch: [11][350/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1493e+00 (1.0644e+00)	Acc@1  62.50 ( 68.29)	Acc@5  92.19 ( 92.68)
Epoch: [11][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0463e+00 (1.0664e+00)	Acc@1  69.53 ( 68.24)	Acc@5  89.84 ( 92.65)
Epoch: [11][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0739e+00 (1.0669e+00)	Acc@1  68.75 ( 68.23)	Acc@5  92.19 ( 92.64)
Epoch: [11][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0939e+00 (1.0673e+00)	Acc@1  64.06 ( 68.22)	Acc@5  92.97 ( 92.62)
Epoch: [11][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1669e+00 (1.0680e+00)	Acc@1  71.25 ( 68.19)	Acc@5  91.25 ( 92.61)
## e[11] optimizer.zero_grad (sum) time: 0.13652324676513672
## e[11]       loss.backward (sum) time: 2.521698474884033
## e[11]      optimizer.step (sum) time: 1.022993803024292
## epoch[11] training(only) time: 18.41617727279663
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.4453e+00 (1.4453e+00)	Acc@1  60.00 ( 60.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5644e+00 (1.4746e+00)	Acc@1  62.00 ( 60.73)	Acc@5  87.00 ( 87.18)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3442e+00 (1.4225e+00)	Acc@1  65.00 ( 61.62)	Acc@5  89.00 ( 87.24)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.5431e+00 (1.4271e+00)	Acc@1  60.00 ( 61.58)	Acc@5  90.00 ( 87.32)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2736e+00 (1.3944e+00)	Acc@1  58.00 ( 61.71)	Acc@5  88.00 ( 88.10)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4808e+00 (1.4184e+00)	Acc@1  59.00 ( 61.27)	Acc@5  86.00 ( 87.57)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2286e+00 (1.3978e+00)	Acc@1  63.00 ( 61.64)	Acc@5  92.00 ( 87.95)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6338e+00 (1.4130e+00)	Acc@1  58.00 ( 61.13)	Acc@5  88.00 ( 87.86)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5349e+00 (1.4332e+00)	Acc@1  59.00 ( 60.84)	Acc@5  88.00 ( 87.51)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6804e+00 (1.4252e+00)	Acc@1  58.00 ( 61.04)	Acc@5  87.00 ( 87.65)
 * Acc@1 61.110 Acc@5 87.750
### epoch[11] execution time: 20.90421462059021
EPOCH 12
i:   0, name:          module.conv1.0.weight  changing lr from: 0.087586278261115785   to: 0.085179615101143580
i:   1, name:          module.conv1.1.weight  changing lr from: 0.087891487038548419   to: 0.085536229574596487
i:   2, name:            module.conv1.1.bias  changing lr from: 0.088188915464732759   to: 0.085883911743852098
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.088478775502020046   to: 0.086222896687057765
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.088761273376730121   to: 0.086553413605969354
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.089036609706376196   to: 0.086875685922522483
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.089304979627058662   to: 0.087189931378873820
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.089566572920682153   to: 0.087496362140303541
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.089821574141689745   to: 0.087795184900426981
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.090070162743041693   to: 0.088086600988216524
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.090312513201198652   to: 0.088370806476383068
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.090548795139897151   to: 0.088647992290710340
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.090779173452532516   to: 0.088918344319976431
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.091003808422987123   to: 0.089182043526134375
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.091222855844764306   to: 0.089439266054456762
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.091436467138307859   to: 0.089690183343382246
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.091644789466404475   to: 0.089934962233828644
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.091847965847583257   to: 0.090173765077765047
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.092046135267440571   to: 0.090406749845858281
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.092239432787832004   to: 0.090634070234031139
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.092427989653885101   to: 0.090855875768790367
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.092611933398797225   to: 0.091072311911199147
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.092791387946392551   to: 0.091283520159387052
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.092966473711421069   to: 0.091489638149504199
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.093137307697589947   to: 0.091690799755040645
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.093304003593324969   to: 0.091887135184444510
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.093466671865265305   to: 0.092078771076983104
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.093625419849501304   to: 0.092265830596802323
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.093780351840569043   to: 0.092448433525148088
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.093931569178219876   to: 0.092626696350722459
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.094079170331987816   to: 0.092800732358154198
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.094223250983579226   to: 0.092970651714570807
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.094363904107114066   to: 0.093136561554264571
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.094501220047249215   to: 0.093298566061451205
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.094635286595216681   to: 0.093456766551124060
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.094766189062811826   to: 0.093611261548011582
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.094894010354367869   to: 0.093762146863649365
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.095018831036754126   to: 0.093909515671581636
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.095140729407436841   to: 0.094053458580710153
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.095259781560641868   to: 0.094194063706810924
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.095376061451659430   to: 0.094331416742241631
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.095489640959331185   to: 0.094465601023864809
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.095600589946760905   to: 0.094596697599213214
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.095708976320289019   to: 0.094724785290925456
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.095814866086772671   to: 0.094849940759481610
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.095918323409211595   to: 0.094972238564268896
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.096019410660760987   to: 0.095091751223008864
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.096118188477171407   to: 0.095208549269578141
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.096214715807695986   to: 0.095322701310255137
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.096309049964504650   to: 0.095434274078425849
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.096401246670644272   to: 0.095543332487781685
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.096491360106583643   to: 0.095649939684043062
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.096579442955381303   to: 0.095754157095241821
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.096665546446513650   to: 0.095856044480596314
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.096749720398400019   to: 0.095955659978011962
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.096832013259661329   to: 0.096053060150240913
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.096912472149147055   to: 0.096148300029733347
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.096991142894766125   to: 0.096241433162213078
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.097068070071155005   to: 0.096332511649009897
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.097143297036216825   to: 0.096421586188180142
i:  60, name:               module.fc.weight  changing lr from: 0.097216865966563759   to: 0.096508706114447243
i:  61, name:                 module.fc.bias  changing lr from: 0.097288817891894766   to: 0.096593919437992989



# Switched to train mode...
Epoch: [12][  0/391]	Time  0.197 ( 0.197)	Data  0.153 ( 0.153)	Loss 1.0096e+00 (1.0096e+00)	Acc@1  71.88 ( 71.88)	Acc@5  90.62 ( 90.62)
Epoch: [12][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 8.1106e-01 (9.0466e-01)	Acc@1  74.22 ( 73.58)	Acc@5  95.31 ( 93.96)
Epoch: [12][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 8.3888e-01 (9.0911e-01)	Acc@1  74.22 ( 73.29)	Acc@5  93.75 ( 94.01)
Epoch: [12][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 9.2516e-01 (9.1853e-01)	Acc@1  71.09 ( 73.29)	Acc@5  94.53 ( 94.13)
Epoch: [12][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 9.8861e-01 (9.1999e-01)	Acc@1  67.19 ( 72.69)	Acc@5  92.19 ( 94.28)
Epoch: [12][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.2468e-01 (9.3023e-01)	Acc@1  74.22 ( 72.32)	Acc@5  92.19 ( 94.13)
Epoch: [12][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 8.3415e-01 (9.3123e-01)	Acc@1  78.91 ( 72.41)	Acc@5  93.75 ( 94.12)
Epoch: [12][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 8.9541e-01 (9.2825e-01)	Acc@1  74.22 ( 72.45)	Acc@5  96.09 ( 94.15)
Epoch: [12][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.2992e-01 (9.2816e-01)	Acc@1  74.22 ( 72.47)	Acc@5  95.31 ( 94.20)
Epoch: [12][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.7201e-01 (9.2685e-01)	Acc@1  79.69 ( 72.59)	Acc@5  96.09 ( 94.13)
Epoch: [12][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.6667e-01 (9.2496e-01)	Acc@1  72.66 ( 72.66)	Acc@5  92.97 ( 94.19)
Epoch: [12][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.3394e-01 (9.2614e-01)	Acc@1  67.97 ( 72.52)	Acc@5  96.88 ( 94.21)
Epoch: [12][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.2087e-01 (9.2408e-01)	Acc@1  69.53 ( 72.53)	Acc@5  95.31 ( 94.20)
Epoch: [12][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.9035e-01 (9.2982e-01)	Acc@1  70.31 ( 72.44)	Acc@5  94.53 ( 94.13)
Epoch: [12][140/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.6085e-01 (9.3896e-01)	Acc@1  78.91 ( 72.21)	Acc@5  94.53 ( 94.08)
Epoch: [12][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.0452e+00 (9.4156e-01)	Acc@1  71.09 ( 72.12)	Acc@5  90.62 ( 94.01)
Epoch: [12][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.9745e-01 (9.4319e-01)	Acc@1  67.97 ( 72.05)	Acc@5  94.53 ( 93.99)
Epoch: [12][170/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1506e+00 (9.4942e-01)	Acc@1  62.50 ( 71.88)	Acc@5  90.62 ( 93.89)
Epoch: [12][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2150e+00 (9.5307e-01)	Acc@1  62.50 ( 71.67)	Acc@5  88.28 ( 93.86)
Epoch: [12][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0478e+00 (9.5525e-01)	Acc@1  69.53 ( 71.63)	Acc@5  94.53 ( 93.86)
Epoch: [12][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.9187e-01 (9.5638e-01)	Acc@1  75.78 ( 71.61)	Acc@5  94.53 ( 93.86)
Epoch: [12][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1305e+00 (9.5835e-01)	Acc@1  64.84 ( 71.55)	Acc@5  92.19 ( 93.85)
Epoch: [12][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1847e+00 (9.6035e-01)	Acc@1  64.84 ( 71.45)	Acc@5  89.06 ( 93.82)
Epoch: [12][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3959e-01 (9.6115e-01)	Acc@1  75.00 ( 71.39)	Acc@5  95.31 ( 93.83)
Epoch: [12][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0626e+00 (9.6425e-01)	Acc@1  69.53 ( 71.35)	Acc@5  94.53 ( 93.79)
Epoch: [12][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3649e+00 (9.6760e-01)	Acc@1  60.16 ( 71.26)	Acc@5  85.94 ( 93.74)
Epoch: [12][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2177e-01 (9.7071e-01)	Acc@1  71.88 ( 71.18)	Acc@5  94.53 ( 93.72)
Epoch: [12][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1210e-01 (9.7091e-01)	Acc@1  76.56 ( 71.15)	Acc@5  96.88 ( 93.72)
Epoch: [12][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0798e+00 (9.7326e-01)	Acc@1  63.28 ( 71.05)	Acc@5  92.97 ( 93.71)
Epoch: [12][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6502e-01 (9.7440e-01)	Acc@1  75.00 ( 71.04)	Acc@5  94.53 ( 93.71)
Epoch: [12][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1149e+00 (9.7553e-01)	Acc@1  67.19 ( 71.01)	Acc@5  91.41 ( 93.70)
Epoch: [12][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3277e-01 (9.7482e-01)	Acc@1  67.19 ( 70.98)	Acc@5  94.53 ( 93.69)
Epoch: [12][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0106e+00 (9.7519e-01)	Acc@1  70.31 ( 70.94)	Acc@5  93.75 ( 93.70)
Epoch: [12][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0101e+00 (9.7614e-01)	Acc@1  71.88 ( 70.90)	Acc@5  94.53 ( 93.71)
Epoch: [12][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2915e+00 (9.7796e-01)	Acc@1  62.50 ( 70.87)	Acc@5  93.75 ( 93.67)
Epoch: [12][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0594e+00 (9.7841e-01)	Acc@1  67.97 ( 70.83)	Acc@5  90.62 ( 93.66)
Epoch: [12][360/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2516e+00 (9.8087e-01)	Acc@1  60.16 ( 70.73)	Acc@5  91.41 ( 93.64)
Epoch: [12][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0731e+00 (9.8162e-01)	Acc@1  67.97 ( 70.67)	Acc@5  93.75 ( 93.66)
Epoch: [12][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0858e+00 (9.8263e-01)	Acc@1  68.75 ( 70.68)	Acc@5  92.97 ( 93.64)
Epoch: [12][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1182e+00 (9.8282e-01)	Acc@1  61.25 ( 70.65)	Acc@5  93.75 ( 93.66)
## e[12] optimizer.zero_grad (sum) time: 0.1382308006286621
## e[12]       loss.backward (sum) time: 2.519209384918213
## e[12]      optimizer.step (sum) time: 1.0111899375915527
## epoch[12] training(only) time: 18.44297766685486
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3302e+00 (1.3302e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.4462e+00 (1.4650e+00)	Acc@1  62.00 ( 62.45)	Acc@5  90.00 ( 86.82)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3605e+00 (1.4202e+00)	Acc@1  68.00 ( 63.52)	Acc@5  89.00 ( 87.57)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6461e+00 (1.4347e+00)	Acc@1  57.00 ( 62.52)	Acc@5  87.00 ( 87.48)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3710e+00 (1.4077e+00)	Acc@1  61.00 ( 62.78)	Acc@5  89.00 ( 88.12)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.2881e+00 (1.4151e+00)	Acc@1  61.00 ( 61.98)	Acc@5  88.00 ( 88.00)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4871e+00 (1.3952e+00)	Acc@1  63.00 ( 62.21)	Acc@5  87.00 ( 88.36)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.2819e+00 (1.4023e+00)	Acc@1  66.00 ( 62.07)	Acc@5  90.00 ( 88.45)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5756e+00 (1.4100e+00)	Acc@1  59.00 ( 61.86)	Acc@5  84.00 ( 88.32)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.4201e+00 (1.3968e+00)	Acc@1  61.00 ( 62.03)	Acc@5  91.00 ( 88.58)
 * Acc@1 62.110 Acc@5 88.730
### epoch[12] execution time: 20.931017637252808
EPOCH 13
i:   0, name:          module.conv1.0.weight  changing lr from: 0.085179615101143580   to: 0.082614143440428292
i:   1, name:          module.conv1.1.weight  changing lr from: 0.085536229574596487   to: 0.083024333249743773
i:   2, name:            module.conv1.1.bias  changing lr from: 0.085883911743852098   to: 0.083424450272554504
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.086222896687057765   to: 0.083814749006359643
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.086553413605969354   to: 0.084195478223215769
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.086875685922522483   to: 0.084566881016448409
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.087189931378873820   to: 0.084929194855325005
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.087496362140303541   to: 0.085282651646745922
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.087795184900426981   to: 0.085627477803090074
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.088086600988216524   to: 0.085963894315426656
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.088370806476383068   to: 0.086292116831372154
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.088647992290710340   to: 0.086612355736935917
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.088918344319976431   to: 0.086924816241754965
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.089182043526134375   to: 0.087229698467173281
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.089439266054456762   to: 0.087527197536669543
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.089690183343382246   to: 0.087817503668183444
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.089934962233828644   to: 0.088100802267932335
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.090173765077765047   to: 0.088377274025348784
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.090406749845858281   to: 0.088647095008804841
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.090634070234031139   to: 0.088910436761821754
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.090855875768790367   to: 0.089167466399493883
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.091072311911199147   to: 0.089418346704882426
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.091283520159387052   to: 0.089663236225161008
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.091489638149504199   to: 0.089902289367317201
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.091690799755040645   to: 0.090135656493236016
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.091887135184444510   to: 0.090363484014010731
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.092078771076983104   to: 0.090585914483343966
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.092265830596802323   to: 0.090803086689918266
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.092448433525148088   to: 0.091015135748631057
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.092626696350722459   to: 0.091222193190601278
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.092800732358154198   to: 0.091424387051868145
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.092970651714570807   to: 0.091621841960713976
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.093136561554264571   to: 0.091814679223552451
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.093298566061451205   to: 0.092003016909334281
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.093456766551124060   to: 0.092186969932429474
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.093611261548011582   to: 0.092366650133954176
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.093762146863649365   to: 0.092542166361516520
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.093909515671581636   to: 0.092713624547362303
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.094053458580710153   to: 0.092881127784907203
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.094194063706810924   to: 0.093044776403647458
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.094331416742241631   to: 0.093204668042445191
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.094465601023864809   to: 0.093360897721188968
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.094596697599213214   to: 0.093513557910834530
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.094724785290925456   to: 0.093662738601832454
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.094849940759481610   to: 0.093808527370954284
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.094972238564268896   to: 0.093951009446529898
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.095091751223008864   to: 0.094090267772111935
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.095208549269578141   to: 0.094226383068585121
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.095322701310255137   to: 0.094359433894739650
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.095434274078425849   to: 0.094489496706330028
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.095543332487781685   to: 0.094616645913641662
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.095649939684043062   to: 0.094740953937588732
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.095754157095241821   to: 0.094862491264367882
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.095856044480596314   to: 0.094981326498693397
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.095955659978011962   to: 0.095097526415639441
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.096053060150240913   to: 0.095211156011116488
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.096148300029733347   to: 0.095322278551008527
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.096241433162213078   to: 0.095430955618998711
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.096332511649009897   to: 0.095537247163110794
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.096421586188180142   to: 0.095641211540994253
i:  60, name:               module.fc.weight  changing lr from: 0.096508706114447243   to: 0.095742905563980685
i:  61, name:                 module.fc.bias  changing lr from: 0.096593919437992989   to: 0.095842384539939138



# Switched to train mode...
Epoch: [13][  0/391]	Time  0.195 ( 0.195)	Data  0.152 ( 0.152)	Loss 7.5310e-01 (7.5310e-01)	Acc@1  80.47 ( 80.47)	Acc@5  95.31 ( 95.31)
Epoch: [13][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 8.8574e-01 (8.5632e-01)	Acc@1  71.09 ( 75.50)	Acc@5  93.75 ( 94.67)
Epoch: [13][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.010)	Loss 9.2978e-01 (8.3561e-01)	Acc@1  71.88 ( 75.63)	Acc@5  94.53 ( 95.20)
Epoch: [13][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 9.6683e-01 (8.4118e-01)	Acc@1  75.00 ( 75.23)	Acc@5  92.97 ( 95.04)
Epoch: [13][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 7.7968e-01 (8.3379e-01)	Acc@1  74.22 ( 75.40)	Acc@5  95.31 ( 95.10)
Epoch: [13][ 50/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 6.2585e-01 (8.2995e-01)	Acc@1  81.25 ( 75.58)	Acc@5  95.31 ( 95.11)
Epoch: [13][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.5499e-01 (8.3845e-01)	Acc@1  76.56 ( 75.14)	Acc@5  96.09 ( 95.20)
Epoch: [13][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.0342e-01 (8.4587e-01)	Acc@1  74.22 ( 74.92)	Acc@5  95.31 ( 95.19)
Epoch: [13][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.2773e-01 (8.4611e-01)	Acc@1  71.09 ( 74.72)	Acc@5  92.97 ( 95.19)
Epoch: [13][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.9042e-01 (8.4732e-01)	Acc@1  74.22 ( 74.60)	Acc@5  93.75 ( 95.13)
Epoch: [13][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1896e+00 (8.5611e-01)	Acc@1  67.97 ( 74.50)	Acc@5  89.06 ( 94.95)
Epoch: [13][110/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.9071e-01 (8.6098e-01)	Acc@1  72.66 ( 74.22)	Acc@5  94.53 ( 94.96)
Epoch: [13][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.0001e-01 (8.6330e-01)	Acc@1  77.34 ( 74.10)	Acc@5  94.53 ( 94.91)
Epoch: [13][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.3490e-01 (8.6897e-01)	Acc@1  70.31 ( 73.92)	Acc@5  93.75 ( 94.89)
Epoch: [13][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.2369e-01 (8.7187e-01)	Acc@1  77.34 ( 73.70)	Acc@5  91.41 ( 94.86)
Epoch: [13][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 9.4063e-01 (8.7558e-01)	Acc@1  73.44 ( 73.63)	Acc@5  98.44 ( 94.86)
Epoch: [13][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.8222e-01 (8.7379e-01)	Acc@1  72.66 ( 73.73)	Acc@5  93.75 ( 94.88)
Epoch: [13][170/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0211e+00 (8.7767e-01)	Acc@1  71.88 ( 73.62)	Acc@5  93.75 ( 94.86)
Epoch: [13][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0429e-01 (8.7780e-01)	Acc@1  76.56 ( 73.65)	Acc@5  96.09 ( 94.85)
Epoch: [13][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2453e+00 (8.8240e-01)	Acc@1  60.16 ( 73.57)	Acc@5  89.84 ( 94.82)
Epoch: [13][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0301e+00 (8.8711e-01)	Acc@1  69.53 ( 73.41)	Acc@5  90.62 ( 94.75)
Epoch: [13][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7465e-01 (8.9095e-01)	Acc@1  64.84 ( 73.23)	Acc@5  94.53 ( 94.74)
Epoch: [13][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0508e-01 (8.9303e-01)	Acc@1  80.47 ( 73.14)	Acc@5  96.09 ( 94.71)
Epoch: [13][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1144e+00 (8.9672e-01)	Acc@1  67.19 ( 73.01)	Acc@5  94.53 ( 94.68)
Epoch: [13][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7666e-01 (9.0011e-01)	Acc@1  70.31 ( 72.88)	Acc@5  92.97 ( 94.63)
Epoch: [13][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0130e+00 (9.0252e-01)	Acc@1  70.31 ( 72.82)	Acc@5  95.31 ( 94.63)
Epoch: [13][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.6607e-01 (9.0592e-01)	Acc@1  79.69 ( 72.78)	Acc@5  96.09 ( 94.61)
Epoch: [13][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0724e+00 (9.0651e-01)	Acc@1  65.62 ( 72.74)	Acc@5  92.97 ( 94.60)
Epoch: [13][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1087e+00 (9.0845e-01)	Acc@1  67.97 ( 72.69)	Acc@5  92.19 ( 94.59)
Epoch: [13][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7599e-01 (9.0960e-01)	Acc@1  73.44 ( 72.61)	Acc@5  97.66 ( 94.59)
Epoch: [13][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.8781e-01 (9.0868e-01)	Acc@1  68.75 ( 72.64)	Acc@5  93.75 ( 94.59)
Epoch: [13][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1129e-01 (9.0892e-01)	Acc@1  76.56 ( 72.65)	Acc@5  95.31 ( 94.61)
Epoch: [13][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5892e-01 (9.0937e-01)	Acc@1  67.19 ( 72.65)	Acc@5  94.53 ( 94.60)
Epoch: [13][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.5233e-01 (9.0962e-01)	Acc@1  76.56 ( 72.68)	Acc@5  96.88 ( 94.59)
Epoch: [13][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.6639e-01 (9.1017e-01)	Acc@1  76.56 ( 72.67)	Acc@5  96.88 ( 94.58)
Epoch: [13][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4319e-01 (9.1201e-01)	Acc@1  73.44 ( 72.58)	Acc@5  95.31 ( 94.56)
Epoch: [13][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0896e-01 (9.1466e-01)	Acc@1  71.09 ( 72.49)	Acc@5  94.53 ( 94.55)
Epoch: [13][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8334e-01 (9.1488e-01)	Acc@1  74.22 ( 72.47)	Acc@5  96.88 ( 94.54)
Epoch: [13][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8741e-01 (9.1500e-01)	Acc@1  76.56 ( 72.47)	Acc@5  95.31 ( 94.52)
Epoch: [13][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6980e-01 (9.1489e-01)	Acc@1  67.50 ( 72.47)	Acc@5  96.25 ( 94.51)
## e[13] optimizer.zero_grad (sum) time: 0.1371004581451416
## e[13]       loss.backward (sum) time: 2.5186290740966797
## e[13]      optimizer.step (sum) time: 1.0350093841552734
## epoch[13] training(only) time: 18.43683695793152
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2555e+00 (1.2555e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5979e+00 (1.3592e+00)	Acc@1  61.00 ( 66.00)	Acc@5  86.00 ( 87.91)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.2184e+00 (1.3088e+00)	Acc@1  73.00 ( 66.24)	Acc@5  93.00 ( 88.57)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.5175e+00 (1.3103e+00)	Acc@1  58.00 ( 65.10)	Acc@5  86.00 ( 88.39)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.2278e+00 (1.3129e+00)	Acc@1  69.00 ( 64.83)	Acc@5  89.00 ( 88.80)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4490e+00 (1.3260e+00)	Acc@1  64.00 ( 64.16)	Acc@5  91.00 ( 88.71)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4137e+00 (1.3106e+00)	Acc@1  58.00 ( 64.18)	Acc@5  94.00 ( 88.93)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.4292e+00 (1.3187e+00)	Acc@1  64.00 ( 64.14)	Acc@5  87.00 ( 88.87)
Test: [ 80/100]	Time  0.027 ( 0.024)	Loss 1.3559e+00 (1.3267e+00)	Acc@1  67.00 ( 63.93)	Acc@5  87.00 ( 88.80)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.5080e+00 (1.3164e+00)	Acc@1  57.00 ( 64.14)	Acc@5  86.00 ( 88.92)
 * Acc@1 64.190 Acc@5 88.960
### epoch[13] execution time: 20.95270276069641
EPOCH 14
i:   0, name:          module.conv1.0.weight  changing lr from: 0.082614143440428292   to: 0.079901783211565186
i:   1, name:          module.conv1.1.weight  changing lr from: 0.083024333249743773   to: 0.080367190754619011
i:   2, name:            module.conv1.1.bias  changing lr from: 0.083424450272554504   to: 0.080821420967340268
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.083814749006359643   to: 0.081264742901722198
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.084195478223215769   to: 0.081697420382322014
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.084566881016448409   to: 0.082119711981396573
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.084929194855325005   to: 0.082531871007771304
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.085282651646745922   to: 0.082934145508093249
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.085627477803090074   to: 0.083326778279226993
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.085963894315426656   to: 0.083710006890650585
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.086292116831372154   to: 0.084084063715800531
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.086612355736935917   to: 0.084449175971400670
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.086924816241754965   to: 0.084805565763887647
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.087229698467173281   to: 0.085153450142119558
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.087527197536669543   to: 0.085493041155621313
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.087817503668183444   to: 0.085824545917683209
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.088100802267932335   to: 0.086148166672686488
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.088377274025348784   to: 0.086464100867083574
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.088647095008804841   to: 0.086772541223510136
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.088910436761821754   to: 0.087073675817550739
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.089167466399493883   to: 0.087367688156723564
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.089418346704882426   to: 0.087654757261287047
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.089663236225161008   to: 0.087935057746508033
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.089902289367317201   to: 0.088208759906063874
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.090135656493236016   to: 0.088476029796281289
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.090363484014010731   to: 0.088737029320942920
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.090585914483343966   to: 0.088991916316418618
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.090803086689918266   to: 0.089240844636901767
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.091015135748631057   to: 0.089483964239553737
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.091222193190601278   to: 0.089721421269378487
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.091424387051868145   to: 0.089953358143669060
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.091621841960713976   to: 0.090179913635883888
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.091814679223552451   to: 0.090401222958826744
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.092003016909334281   to: 0.090617417847018600
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.092186969932429474   to: 0.090828626638162227
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.092366650133954176   to: 0.091034974353613241
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.092542166361516520   to: 0.091236582777781475
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.092713624547362303   to: 0.091433570536397155
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.092881127784907203   to: 0.091626053173585076
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.093044776403647458   to: 0.091814143227699041
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.093204668042445191   to: 0.091997950305875639
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.093360897721188968   to: 0.092177581157273925
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.093513557910834530   to: 0.092353139744974111
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.093662738601832454   to: 0.092524727316513081
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.093808527370954284   to: 0.092692442473041259
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.093951009446529898   to: 0.092856381237088781
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.094090267772111935   to: 0.093016637118933765
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.094226383068585121   to: 0.093173301181569662
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.094359433894739650   to: 0.093326462104271049
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.094489496706330028   to: 0.093476206244761828
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.094616645913641662   to: 0.093622617699991292
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.094740953937588732   to: 0.093765778365526520
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.094862491264367882   to: 0.093905767993572253
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.094981326498693397   to: 0.094042664249630509
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.095097526415639441   to: 0.094176542767814397
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.095211156011116488   to: 0.094307477204832416
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.095322278551008527   to: 0.094435539292660076
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.095430955618998711   to: 0.094560798889917780
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.095537247163110794   to: 0.094683324031974117
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.095641211540994253   to: 0.094803180979795010
i:  60, name:               module.fc.weight  changing lr from: 0.095742905563980685   to: 0.094920434267559917
i:  61, name:                 module.fc.bias  changing lr from: 0.095842384539939138   to: 0.095035146749066504



# Switched to train mode...
Epoch: [14][  0/391]	Time  0.193 ( 0.193)	Data  0.152 ( 0.152)	Loss 8.5412e-01 (8.5412e-01)	Acc@1  75.78 ( 75.78)	Acc@5  92.97 ( 92.97)
Epoch: [14][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 7.0427e-01 (8.0464e-01)	Acc@1  78.12 ( 74.86)	Acc@5  97.66 ( 95.88)
Epoch: [14][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 7.0984e-01 (8.1184e-01)	Acc@1  75.78 ( 75.07)	Acc@5  95.31 ( 95.54)
Epoch: [14][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 9.5635e-01 (8.2018e-01)	Acc@1  74.22 ( 74.85)	Acc@5  94.53 ( 95.61)
Epoch: [14][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 8.6374e-01 (8.2260e-01)	Acc@1  72.66 ( 74.73)	Acc@5  95.31 ( 95.48)
Epoch: [14][ 50/391]	Time  0.046 ( 0.049)	Data  0.003 ( 0.005)	Loss 9.0419e-01 (8.2481e-01)	Acc@1  74.22 ( 74.94)	Acc@5  96.09 ( 95.40)
Epoch: [14][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.9711e-01 (8.1855e-01)	Acc@1  77.34 ( 75.23)	Acc@5  96.09 ( 95.29)
Epoch: [14][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.7497e-01 (8.1702e-01)	Acc@1  75.78 ( 75.29)	Acc@5  94.53 ( 95.22)
Epoch: [14][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.6144e-01 (8.1059e-01)	Acc@1  79.69 ( 75.43)	Acc@5  96.88 ( 95.38)
Epoch: [14][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.2994e-01 (8.1077e-01)	Acc@1  84.38 ( 75.36)	Acc@5  97.66 ( 95.37)
Epoch: [14][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.1036e-01 (8.1554e-01)	Acc@1  72.66 ( 75.22)	Acc@5  96.88 ( 95.34)
Epoch: [14][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.5465e-01 (8.1972e-01)	Acc@1  68.75 ( 75.08)	Acc@5  93.75 ( 95.25)
Epoch: [14][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.8524e-01 (8.2409e-01)	Acc@1  80.47 ( 75.03)	Acc@5  96.88 ( 95.20)
Epoch: [14][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0526e+00 (8.2604e-01)	Acc@1  66.41 ( 74.99)	Acc@5  91.41 ( 95.18)
Epoch: [14][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 7.4549e-01 (8.2285e-01)	Acc@1  74.22 ( 75.01)	Acc@5  97.66 ( 95.29)
Epoch: [14][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 6.3219e-01 (8.2500e-01)	Acc@1  78.91 ( 74.88)	Acc@5  97.66 ( 95.27)
Epoch: [14][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6172e-01 (8.2707e-01)	Acc@1  79.69 ( 74.84)	Acc@5  94.53 ( 95.24)
Epoch: [14][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1953e-01 (8.2674e-01)	Acc@1  75.00 ( 74.84)	Acc@5  96.09 ( 95.27)
Epoch: [14][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2989e+00 (8.2928e-01)	Acc@1  65.62 ( 74.86)	Acc@5  88.28 ( 95.18)
Epoch: [14][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2117e-01 (8.3072e-01)	Acc@1  75.78 ( 74.85)	Acc@5  96.09 ( 95.18)
Epoch: [14][200/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7455e-01 (8.3461e-01)	Acc@1  70.31 ( 74.66)	Acc@5  94.53 ( 95.15)
Epoch: [14][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9959e-01 (8.3742e-01)	Acc@1  76.56 ( 74.61)	Acc@5  92.97 ( 95.17)
Epoch: [14][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0632e+00 (8.3936e-01)	Acc@1  65.62 ( 74.44)	Acc@5  93.75 ( 95.20)
Epoch: [14][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.6371e-01 (8.4260e-01)	Acc@1  71.09 ( 74.35)	Acc@5  95.31 ( 95.16)
Epoch: [14][240/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1693e-01 (8.4726e-01)	Acc@1  73.44 ( 74.19)	Acc@5  93.75 ( 95.11)
Epoch: [14][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.8398e-01 (8.4985e-01)	Acc@1  75.78 ( 74.13)	Acc@5  96.88 ( 95.11)
Epoch: [14][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4315e-01 (8.4830e-01)	Acc@1  71.09 ( 74.12)	Acc@5  95.31 ( 95.15)
Epoch: [14][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0040e-01 (8.4909e-01)	Acc@1  75.00 ( 74.10)	Acc@5  92.19 ( 95.12)
Epoch: [14][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1342e-01 (8.4998e-01)	Acc@1  77.34 ( 74.05)	Acc@5  98.44 ( 95.13)
Epoch: [14][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8583e-01 (8.5063e-01)	Acc@1  75.78 ( 74.06)	Acc@5  95.31 ( 95.12)
Epoch: [14][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0173e+00 (8.5034e-01)	Acc@1  67.97 ( 74.06)	Acc@5  91.41 ( 95.13)
Epoch: [14][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0154e+00 (8.5121e-01)	Acc@1  72.66 ( 74.06)	Acc@5  94.53 ( 95.13)
Epoch: [14][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5290e-01 (8.5036e-01)	Acc@1  71.88 ( 74.10)	Acc@5  93.75 ( 95.15)
Epoch: [14][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.5781e-01 (8.5055e-01)	Acc@1  73.44 ( 74.07)	Acc@5  94.53 ( 95.16)
Epoch: [14][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0755e-01 (8.5282e-01)	Acc@1  72.66 ( 74.04)	Acc@5  94.53 ( 95.11)
Epoch: [14][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.8051e-01 (8.5423e-01)	Acc@1  74.22 ( 74.04)	Acc@5  95.31 ( 95.12)
Epoch: [14][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0594e-01 (8.5638e-01)	Acc@1  81.25 ( 73.98)	Acc@5  98.44 ( 95.10)
Epoch: [14][370/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7919e-01 (8.5632e-01)	Acc@1  71.88 ( 73.99)	Acc@5  93.75 ( 95.11)
Epoch: [14][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2286e-01 (8.5658e-01)	Acc@1  82.81 ( 74.02)	Acc@5  95.31 ( 95.09)
Epoch: [14][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7414e-01 (8.5774e-01)	Acc@1  71.25 ( 73.99)	Acc@5  95.00 ( 95.07)
## e[14] optimizer.zero_grad (sum) time: 0.13628458976745605
## e[14]       loss.backward (sum) time: 2.5344395637512207
## e[14]      optimizer.step (sum) time: 1.01181960105896
## epoch[14] training(only) time: 18.474862337112427
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.3914e+00 (1.3914e+00)	Acc@1  65.00 ( 65.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.4276e+00 (1.4404e+00)	Acc@1  69.00 ( 64.09)	Acc@5  92.00 ( 87.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.5294e+00 (1.4286e+00)	Acc@1  64.00 ( 64.33)	Acc@5  88.00 ( 87.71)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.4319e+00 (1.4154e+00)	Acc@1  69.00 ( 64.35)	Acc@5  90.00 ( 87.81)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.1438e+00 (1.3770e+00)	Acc@1  69.00 ( 64.78)	Acc@5  92.00 ( 88.63)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3617e+00 (1.3797e+00)	Acc@1  68.00 ( 64.47)	Acc@5  88.00 ( 88.43)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4210e+00 (1.3656e+00)	Acc@1  61.00 ( 64.23)	Acc@5  93.00 ( 88.85)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5143e+00 (1.3761e+00)	Acc@1  63.00 ( 63.99)	Acc@5  83.00 ( 88.75)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5206e+00 (1.3783e+00)	Acc@1  59.00 ( 63.81)	Acc@5  86.00 ( 88.73)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6990e+00 (1.3645e+00)	Acc@1  58.00 ( 64.19)	Acc@5  84.00 ( 88.84)
 * Acc@1 64.430 Acc@5 88.910
### epoch[14] execution time: 20.952638387680054
EPOCH 15
i:   0, name:          module.conv1.0.weight  changing lr from: 0.079901783211565186   to: 0.077055136834451199
i:   1, name:          module.conv1.1.weight  changing lr from: 0.080367190754619011   to: 0.077576853542979995
i:   2, name:            module.conv1.1.bias  changing lr from: 0.080821420967340268   to: 0.078086349429286023
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.081264742901722198   to: 0.078583902070049796
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.081697420382322014   to: 0.079069784712375440
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.082119711981396573   to: 0.079544266153507348
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.082531871007771304   to: 0.080007610641382343
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.082934145508093249   to: 0.080460077794196130
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.083326778279226993   to: 0.080901922537299886
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.083710006890650585   to: 0.081333395055868063
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.084084063715800531   to: 0.081754740761897135
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.084449175971400670   to: 0.082166200274204571
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.084805565763887647   to: 0.082568009410198456
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.085153450142119558   to: 0.082960399188283934
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.085493041155621313   to: 0.083343595839859827
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.085824545917683209   to: 0.083717820829941031
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.086148166672686488   to: 0.084083290885518136
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.086464100867083574   to: 0.084440218030835859
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.086772541223510136   to: 0.084788809628837669
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.087073675817550739   to: 0.085129268428084437
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.087367688156723564   to: 0.085461792614511131
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.087654757261287047   to: 0.085786575867437528
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.087935057746508033   to: 0.086103807419297559
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.088208759906063874   to: 0.086413672118596505
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.088476029796281289   to: 0.086716350495645883
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.088737029320942920   to: 0.087012018830665783
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.088991916316418618   to: 0.087300849223877874
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.089240844636901767   to: 0.087583009667246636
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.089483964239553737   to: 0.087858664117555982
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.089721421269378487   to: 0.088127972570536145
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.089953358143669060   to: 0.088391091135782307
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.090179913635883888   to: 0.088648172112229459
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.090401222958826744   to: 0.088899364063970795
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.090617417847018600   to: 0.089144811896226431
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.090828626638162227   to: 0.089384656931288775
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.091034974353613241   to: 0.089619036984287248
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.091236582777781475   to: 0.089848086438631944
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.091433570536397155   to: 0.090071936321009363
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.091626053173585076   to: 0.090290714375817649
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.091814143227699041   to: 0.090504545138940642
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.091997950305875639   to: 0.090713550010771665
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.092177581157273925   to: 0.090917847328408075
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.092353139744974111   to: 0.091117552436947732
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.092524727316513081   to: 0.091312777759826547
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.092692442473041259   to: 0.091503632868144960
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.092856381237088781   to: 0.091690224548938340
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.093016637118933765   to: 0.091872656872352534
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.093173301181569662   to: 0.092051031257693053
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.093326462104271049   to: 0.092225446538320571
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.093476206244761828   to: 0.092395999025371753
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.093622617699991292   to: 0.092562782570288188
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.093765778365526520   to: 0.092725888626140951
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.093905767993572253   to: 0.092885406307741988
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.094042664249630509   to: 0.093041422450536621
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.094176542767814397   to: 0.093194021668275534
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.094307477204832416   to: 0.093343286409466208
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.094435539292660076   to: 0.093489297012607289
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.094560798889917780   to: 0.093632131760211235
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.094683324031974117   to: 0.093771866931622760
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.094803180979795010   to: 0.093908576854642101
i:  60, name:               module.fc.weight  changing lr from: 0.094920434267559917   to: 0.094042333955964508
i:  61, name:                 module.fc.bias  changing lr from: 0.095035146749066504   to: 0.094173208810447787



# Switched to train mode...
Epoch: [15][  0/391]	Time  0.189 ( 0.189)	Data  0.146 ( 0.146)	Loss 5.4831e-01 (5.4831e-01)	Acc@1  85.94 ( 85.94)	Acc@5  98.44 ( 98.44)
Epoch: [15][ 10/391]	Time  0.048 ( 0.059)	Data  0.001 ( 0.015)	Loss 6.7587e-01 (7.5812e-01)	Acc@1  79.69 ( 76.85)	Acc@5  98.44 ( 96.52)
Epoch: [15][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 7.8509e-01 (7.4035e-01)	Acc@1  74.22 ( 77.68)	Acc@5  93.75 ( 96.43)
Epoch: [15][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 7.3832e-01 (7.4460e-01)	Acc@1  75.00 ( 77.44)	Acc@5  94.53 ( 96.22)
Epoch: [15][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 8.3799e-01 (7.3706e-01)	Acc@1  76.56 ( 77.50)	Acc@5  94.53 ( 96.34)
Epoch: [15][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.8744e-01 (7.3643e-01)	Acc@1  79.69 ( 77.57)	Acc@5  97.66 ( 96.29)
Epoch: [15][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 6.4052e-01 (7.2683e-01)	Acc@1  78.12 ( 77.82)	Acc@5  95.31 ( 96.23)
Epoch: [15][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.7154e-01 (7.3075e-01)	Acc@1  78.12 ( 77.75)	Acc@5  96.88 ( 96.24)
Epoch: [15][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.3010e-01 (7.3386e-01)	Acc@1  72.66 ( 77.58)	Acc@5  97.66 ( 96.30)
Epoch: [15][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.1032e-01 (7.3127e-01)	Acc@1  75.78 ( 77.70)	Acc@5  95.31 ( 96.32)
Epoch: [15][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5013e-01 (7.3320e-01)	Acc@1  82.81 ( 77.69)	Acc@5  97.66 ( 96.29)
Epoch: [15][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.3262e-01 (7.3775e-01)	Acc@1  79.69 ( 77.64)	Acc@5  98.44 ( 96.24)
Epoch: [15][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.4259e-01 (7.4143e-01)	Acc@1  77.34 ( 77.63)	Acc@5  96.88 ( 96.11)
Epoch: [15][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.5223e-01 (7.4538e-01)	Acc@1  71.09 ( 77.47)	Acc@5  92.97 ( 96.06)
Epoch: [15][140/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.004)	Loss 8.6739e-01 (7.4934e-01)	Acc@1  72.66 ( 77.30)	Acc@5  96.09 ( 96.09)
Epoch: [15][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3463e-01 (7.5001e-01)	Acc@1  82.03 ( 77.30)	Acc@5  96.88 ( 96.10)
Epoch: [15][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.6366e-01 (7.5609e-01)	Acc@1  75.00 ( 77.17)	Acc@5  92.97 ( 96.06)
Epoch: [15][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7671e-01 (7.5709e-01)	Acc@1  73.44 ( 77.12)	Acc@5  97.66 ( 96.08)
Epoch: [15][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.5762e-01 (7.6253e-01)	Acc@1  77.34 ( 76.98)	Acc@5  92.97 ( 95.97)
Epoch: [15][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1808e-01 (7.6555e-01)	Acc@1  75.00 ( 76.87)	Acc@5  90.62 ( 95.95)
Epoch: [15][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6570e-01 (7.6888e-01)	Acc@1  75.78 ( 76.80)	Acc@5  96.88 ( 95.99)
Epoch: [15][210/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2433e-01 (7.6936e-01)	Acc@1  72.66 ( 76.72)	Acc@5  96.09 ( 96.03)
Epoch: [15][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6971e-01 (7.7179e-01)	Acc@1  70.31 ( 76.62)	Acc@5  97.66 ( 96.01)
Epoch: [15][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3239e-01 (7.7439e-01)	Acc@1  73.44 ( 76.56)	Acc@5  94.53 ( 95.99)
Epoch: [15][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7946e-01 (7.7795e-01)	Acc@1  72.66 ( 76.43)	Acc@5  96.09 ( 95.95)
Epoch: [15][250/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5657e-01 (7.8045e-01)	Acc@1  77.34 ( 76.35)	Acc@5  97.66 ( 95.91)
Epoch: [15][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1477e-01 (7.8413e-01)	Acc@1  79.69 ( 76.26)	Acc@5  96.09 ( 95.85)
Epoch: [15][270/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9027e-01 (7.8535e-01)	Acc@1  78.91 ( 76.25)	Acc@5  96.88 ( 95.87)
Epoch: [15][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1088e-01 (7.8522e-01)	Acc@1  73.44 ( 76.25)	Acc@5 100.00 ( 95.88)
Epoch: [15][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7528e-01 (7.8592e-01)	Acc@1  69.53 ( 76.20)	Acc@5  95.31 ( 95.88)
Epoch: [15][300/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0200e-01 (7.8735e-01)	Acc@1  77.34 ( 76.14)	Acc@5  94.53 ( 95.84)
Epoch: [15][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.5450e-01 (7.8843e-01)	Acc@1  71.88 ( 76.06)	Acc@5  97.66 ( 95.84)
Epoch: [15][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3846e-01 (7.8850e-01)	Acc@1  75.78 ( 76.03)	Acc@5  96.88 ( 95.84)
Epoch: [15][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3829e-01 (7.9065e-01)	Acc@1  79.69 ( 75.97)	Acc@5  96.09 ( 95.82)
Epoch: [15][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.6375e-01 (7.9251e-01)	Acc@1  68.75 ( 75.91)	Acc@5  94.53 ( 95.78)
Epoch: [15][350/391]	Time  0.051 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2270e-01 (7.9322e-01)	Acc@1  76.56 ( 75.87)	Acc@5  95.31 ( 95.75)
Epoch: [15][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3542e-01 (7.9502e-01)	Acc@1  73.44 ( 75.76)	Acc@5  94.53 ( 95.74)
Epoch: [15][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2659e-01 (7.9807e-01)	Acc@1  76.56 ( 75.67)	Acc@5  95.31 ( 95.72)
Epoch: [15][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1162e-01 (7.9990e-01)	Acc@1  78.12 ( 75.63)	Acc@5  94.53 ( 95.68)
Epoch: [15][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0036e+00 (8.0217e-01)	Acc@1  67.50 ( 75.59)	Acc@5  93.75 ( 95.65)
## e[15] optimizer.zero_grad (sum) time: 0.13712501525878906
## e[15]       loss.backward (sum) time: 2.5379867553710938
## e[15]      optimizer.step (sum) time: 1.0280416011810303
## epoch[15] training(only) time: 18.41710352897644
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3786e+00 (1.3786e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.4595e+00 (1.4040e+00)	Acc@1  65.00 ( 65.55)	Acc@5  90.00 ( 87.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3855e+00 (1.3938e+00)	Acc@1  72.00 ( 65.10)	Acc@5  87.00 ( 87.86)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6187e+00 (1.4000e+00)	Acc@1  59.00 ( 64.71)	Acc@5  87.00 ( 87.87)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5958e+00 (1.3907e+00)	Acc@1  58.00 ( 64.15)	Acc@5  88.00 ( 88.34)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4255e+00 (1.3984e+00)	Acc@1  62.00 ( 63.41)	Acc@5  86.00 ( 87.96)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3187e+00 (1.3779e+00)	Acc@1  63.00 ( 63.64)	Acc@5  91.00 ( 88.51)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.4546e+00 (1.3780e+00)	Acc@1  61.00 ( 63.38)	Acc@5  90.00 ( 88.59)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3568e+00 (1.3881e+00)	Acc@1  64.00 ( 63.14)	Acc@5  89.00 ( 88.49)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.6650e+00 (1.3806e+00)	Acc@1  61.00 ( 63.34)	Acc@5  86.00 ( 88.55)
 * Acc@1 63.410 Acc@5 88.530
### epoch[15] execution time: 20.92318034172058
EPOCH 16
i:   0, name:          module.conv1.0.weight  changing lr from: 0.077055136834451199   to: 0.074087430661751719
i:   1, name:          module.conv1.1.weight  changing lr from: 0.077576853542979995   to: 0.074665977172417797
i:   2, name:            module.conv1.1.bias  changing lr from: 0.078086349429286023   to: 0.075231345911396061
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.078583902070049796   to: 0.075783815807824326
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.079069784712375440   to: 0.076323662807438783
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.079544266153507348   to: 0.076851159631384891
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.080007610641382343   to: 0.077366575564300197
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.080460077794196130   to: 0.077870176269316790
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.080901922537299886   to: 0.078362223627797961
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.081333395055868063   to: 0.078842975601779011
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.081754740761897135   to: 0.079312686117226527
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.082166200274204571   to: 0.079771604966367815
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.082568009410198456   to: 0.080219977727466946
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.082960399188283934   to: 0.080658045700543968
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.083343595839859827   to: 0.081086045857642550
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.083717820829941031   to: 0.081504210806355601
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.084083290885518136   to: 0.081912768765413119
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.084440218030835859   to: 0.082311943551226452
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.084788809628837669   to: 0.082701954574366521
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.085129268428084437   to: 0.083083016845030000
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.085461792614511131   to: 0.083455340986621024
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.085786575867437528   to: 0.083819133256641351
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.086103807419297559   to: 0.084174595574145375
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.086413672118596505   to: 0.084521925553074195
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.086716350495645883   to: 0.084861316540835868
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.087012018830665783   to: 0.085192957661550398
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.087300849223877874   to: 0.085517033863422964
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.087583009667246636   to: 0.085833725969753083
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.087858664117555982   to: 0.086143210733126904
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.088127972570536145   to: 0.086445660892376819
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.088391091135782307   to: 0.086741245231927305
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.088648172112229459   to: 0.087030128643177385
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.088899364063970795   to: 0.087312472187599965
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.089144811896226431   to: 0.087588433161265744
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.089384656931288775   to: 0.087858165160523763
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.089619036984287248   to: 0.088121818148595724
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.089848086438631944   to: 0.088379538522861290
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.090071936321009363   to: 0.088631469182632830
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.090290714375817649   to: 0.088877749597235545
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.090504545138940642   to: 0.089118515874227144
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.090713550010771665   to: 0.089353900827606123
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.090917847328408075   to: 0.089584034045872729
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.091117552436947732   to: 0.089809041959820257
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.091312777759826547   to: 0.090029047909946217
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.091503632868144960   to: 0.090244172213385168
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.091690224548938340   to: 0.090454532230274609
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.091872656872352534   to: 0.090660242429475818
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.092051031257693053   to: 0.090861414453580147
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.092225446538320571   to: 0.091058157183139088
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.092395999025371753   to: 0.091250576800064820
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.092562782570288188   to: 0.091438776850154127
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.092725888626140951   to: 0.091622858304694965
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.092885406307741988   to: 0.091802919621121443
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.093041422450536621   to: 0.091979056802687462
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.093194021668275534   to: 0.092151363457134511
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.093343286409466208   to: 0.092319930854333654
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.093489297012607289   to: 0.092484847982885587
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.093632131760211235   to: 0.092646201605666306
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.093771866931622760   to: 0.092804076314309689
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.093908576854642101   to: 0.092958554582620659
i:  60, name:               module.fc.weight  changing lr from: 0.094042333955964508   to: 0.093109716818916013
i:  61, name:                 module.fc.bias  changing lr from: 0.094173208810447787   to: 0.093257641417292161



# Switched to train mode...
Epoch: [16][  0/391]	Time  0.195 ( 0.195)	Data  0.154 ( 0.154)	Loss 5.9716e-01 (5.9716e-01)	Acc@1  82.03 ( 82.03)	Acc@5  98.44 ( 98.44)
Epoch: [16][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.016)	Loss 7.0948e-01 (7.0809e-01)	Acc@1  79.69 ( 78.91)	Acc@5  95.31 ( 96.31)
Epoch: [16][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 6.5576e-01 (7.0108e-01)	Acc@1  74.22 ( 78.79)	Acc@5  99.22 ( 96.39)
Epoch: [16][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 7.3791e-01 (6.9085e-01)	Acc@1  74.22 ( 79.03)	Acc@5  96.09 ( 96.52)
Epoch: [16][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 8.2738e-01 (6.8917e-01)	Acc@1  74.22 ( 79.33)	Acc@5  95.31 ( 96.67)
Epoch: [16][ 50/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.4559e-01 (6.8996e-01)	Acc@1  71.88 ( 79.17)	Acc@5  94.53 ( 96.77)
Epoch: [16][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.4732e-01 (6.9025e-01)	Acc@1  81.25 ( 78.97)	Acc@5 100.00 ( 96.89)
Epoch: [16][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 6.7901e-01 (6.8340e-01)	Acc@1  80.47 ( 79.15)	Acc@5  96.88 ( 96.96)
Epoch: [16][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.6888e-01 (6.8754e-01)	Acc@1  72.66 ( 79.06)	Acc@5  96.09 ( 96.89)
Epoch: [16][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.9774e-01 (6.9091e-01)	Acc@1  75.78 ( 78.96)	Acc@5  94.53 ( 96.82)
Epoch: [16][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.8959e-01 (6.9067e-01)	Acc@1  82.03 ( 78.91)	Acc@5  98.44 ( 96.77)
Epoch: [16][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.7774e-01 (6.9541e-01)	Acc@1  75.78 ( 78.79)	Acc@5  96.88 ( 96.69)
Epoch: [16][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.6483e-01 (6.9936e-01)	Acc@1  77.34 ( 78.63)	Acc@5  97.66 ( 96.66)
Epoch: [16][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.3689e-01 (7.0102e-01)	Acc@1  75.78 ( 78.52)	Acc@5  94.53 ( 96.59)
Epoch: [16][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 7.8172e-01 (7.0106e-01)	Acc@1  75.78 ( 78.52)	Acc@5  92.97 ( 96.58)
Epoch: [16][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 7.7693e-01 (7.0592e-01)	Acc@1  75.00 ( 78.37)	Acc@5  94.53 ( 96.54)
Epoch: [16][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0753e+00 (7.0911e-01)	Acc@1  69.53 ( 78.28)	Acc@5  96.09 ( 96.54)
Epoch: [16][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3211e-01 (7.1105e-01)	Acc@1  75.78 ( 78.19)	Acc@5  96.88 ( 96.55)
Epoch: [16][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9891e-01 (7.1209e-01)	Acc@1  78.91 ( 78.09)	Acc@5  99.22 ( 96.57)
Epoch: [16][190/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8024e-01 (7.1447e-01)	Acc@1  81.25 ( 78.02)	Acc@5  98.44 ( 96.53)
Epoch: [16][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9676e-01 (7.1691e-01)	Acc@1  82.81 ( 77.95)	Acc@5  98.44 ( 96.52)
Epoch: [16][210/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0610e-01 (7.1849e-01)	Acc@1  77.34 ( 77.96)	Acc@5  96.09 ( 96.49)
Epoch: [16][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4418e-01 (7.1945e-01)	Acc@1  71.88 ( 77.94)	Acc@5  92.19 ( 96.50)
Epoch: [16][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8910e-01 (7.1796e-01)	Acc@1  81.25 ( 77.96)	Acc@5  98.44 ( 96.51)
Epoch: [16][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7728e-01 (7.1909e-01)	Acc@1  71.09 ( 77.95)	Acc@5  91.41 ( 96.46)
Epoch: [16][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.9932e-01 (7.2153e-01)	Acc@1  70.31 ( 77.88)	Acc@5  97.66 ( 96.44)
Epoch: [16][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0591e-01 (7.2245e-01)	Acc@1  78.12 ( 77.87)	Acc@5  97.66 ( 96.43)
Epoch: [16][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0980e-01 (7.2264e-01)	Acc@1  82.81 ( 77.84)	Acc@5  99.22 ( 96.42)
Epoch: [16][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6796e-01 (7.2333e-01)	Acc@1  77.34 ( 77.78)	Acc@5  96.88 ( 96.40)
Epoch: [16][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1474e-01 (7.2460e-01)	Acc@1  68.75 ( 77.72)	Acc@5  95.31 ( 96.41)
Epoch: [16][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.8825e-01 (7.2782e-01)	Acc@1  72.66 ( 77.63)	Acc@5  93.75 ( 96.38)
Epoch: [16][310/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8314e-01 (7.2943e-01)	Acc@1  78.91 ( 77.61)	Acc@5  96.09 ( 96.35)
Epoch: [16][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8330e-01 (7.3082e-01)	Acc@1  77.34 ( 77.55)	Acc@5  97.66 ( 96.33)
Epoch: [16][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4959e-01 (7.3351e-01)	Acc@1  74.22 ( 77.45)	Acc@5  96.09 ( 96.33)
Epoch: [16][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.6266e-01 (7.3406e-01)	Acc@1  75.00 ( 77.43)	Acc@5  96.09 ( 96.33)
Epoch: [16][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.8354e-01 (7.3753e-01)	Acc@1  71.09 ( 77.30)	Acc@5  92.97 ( 96.32)
Epoch: [16][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0463e+00 (7.3979e-01)	Acc@1  70.31 ( 77.22)	Acc@5  92.97 ( 96.31)
Epoch: [16][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8046e-01 (7.4083e-01)	Acc@1  78.12 ( 77.18)	Acc@5  97.66 ( 96.30)
Epoch: [16][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1413e-01 (7.4099e-01)	Acc@1  78.91 ( 77.17)	Acc@5  97.66 ( 96.30)
Epoch: [16][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0602e+00 (7.4291e-01)	Acc@1  68.75 ( 77.15)	Acc@5  92.50 ( 96.28)
## e[16] optimizer.zero_grad (sum) time: 0.13762807846069336
## e[16]       loss.backward (sum) time: 2.5260393619537354
## e[16]      optimizer.step (sum) time: 1.0259335041046143
## epoch[16] training(only) time: 18.418760061264038
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.4270e+00 (1.4270e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5656e+00 (1.4241e+00)	Acc@1  61.00 ( 64.55)	Acc@5  87.00 ( 88.27)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.0573e+00 (1.3931e+00)	Acc@1  65.00 ( 63.90)	Acc@5  94.00 ( 88.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.4841e+00 (1.4170e+00)	Acc@1  66.00 ( 63.84)	Acc@5  87.00 ( 88.10)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4372e+00 (1.4081e+00)	Acc@1  60.00 ( 63.73)	Acc@5  90.00 ( 88.54)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6665e+00 (1.4230e+00)	Acc@1  62.00 ( 63.49)	Acc@5  86.00 ( 88.33)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3538e+00 (1.4020e+00)	Acc@1  63.00 ( 63.64)	Acc@5  90.00 ( 88.69)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.7040e+00 (1.4112e+00)	Acc@1  56.00 ( 63.35)	Acc@5  88.00 ( 88.59)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.3896e+00 (1.4180e+00)	Acc@1  66.00 ( 63.27)	Acc@5  87.00 ( 88.38)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.4753e+00 (1.4019e+00)	Acc@1  59.00 ( 63.49)	Acc@5  89.00 ( 88.65)
 * Acc@1 63.590 Acc@5 88.860
### epoch[16] execution time: 20.936628341674805
EPOCH 17
i:   0, name:          module.conv1.0.weight  changing lr from: 0.074087430661751719   to: 0.071012453525392968
i:   1, name:          module.conv1.1.weight  changing lr from: 0.074665977172417797   to: 0.071647763905151693
i:   2, name:            module.conv1.1.bias  changing lr from: 0.075231345911396061   to: 0.072269051697252437
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.075783815807824326   to: 0.072876588910483237
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.076323662807438783   to: 0.073470646407479898
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.076851159631384891   to: 0.074051493516081618
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.077366575564300197   to: 0.074619397679678512
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.077870176269316790   to: 0.075174624143621568
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.078362223627797961   to: 0.075717435674961572
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.078842975601779011   to: 0.076248092312968807
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.079312686117226527   to: 0.076766851148056842
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.079771604966367815   to: 0.077273966126897520
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.080219977727466946   to: 0.077769687881665855
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.080658045700543968   to: 0.078254263581496120
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.081086045857642550   to: 0.078727936804364726
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.081504210806355601   to: 0.079190947427739800
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.081912768765413119   to: 0.079643531536454884
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.082311943551226452   to: 0.080085921346373559
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.082701954574366521   to: 0.080518345142514172
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.083083016845030000   to: 0.080941027230398865
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.083455340986621024   to: 0.081354187899481401
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.083819133256641351   to: 0.081758043397590477
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.084174595574145375   to: 0.082152805915403665
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.084521925553074195   to: 0.082538683580039296
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.084861316540835868   to: 0.082915880456921359
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.085192957661550398   to: 0.083284596559135574
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.085517033863422964   to: 0.083645027863552879
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.085833725969753083   to: 0.083997366333052187
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.086143210733126904   to: 0.084341799944224402
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.086445660892376819   to: 0.084678512719987414
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.086741245231927305   to: 0.085007684766585601
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.087030128643177385   to: 0.085329492314488858
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.087312472187599965   to: 0.085644107762743282
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.087588433161265744   to: 0.085951699726362615
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.087858165160523763   to: 0.086252433086380628
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.088121818148595724   to: 0.086546469042216703
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.088379538522861290   to: 0.086833965166034477
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.088631469182632830   to: 0.087115075458799804
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.088877749597235545   to: 0.087389950407769104
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.089118515874227144   to: 0.087658737045161361
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.089353900827606123   to: 0.087921579007788381
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.089584034045872729   to: 0.088178616597437171
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.089809041959820257   to: 0.088429986841816813
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.090029047909946217   to: 0.088675823555898037
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.090244172213385168   to: 0.088916257403490059
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.090454532230274609   to: 0.089151415958913321
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.090660242429475818   to: 0.089381423768639362
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.090861414453580147   to: 0.089606402412782352
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.091058157183139088   to: 0.089826470566337357
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.091250576800064820   to: 0.090041744060071136
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.091438776850154127   to: 0.090252335940980671
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.091622858304694965   to: 0.090458356532244058
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.091802919621121443   to: 0.090659913492595945
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.091979056802687462   to: 0.090857111875067564
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.092151363457134511   to: 0.091050054185038476
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.092319930854333654   to: 0.091238840437553426
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.092484847982885587   to: 0.091423568213863174
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.092646201605666306   to: 0.091604332717154582
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.092804076314309689   to: 0.091781226827439069
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.092958554582620659   to: 0.091954341155573785
i:  60, name:               module.fc.weight  changing lr from: 0.093109716818916013   to: 0.092123764096393848
i:  61, name:                 module.fc.bias  changing lr from: 0.093257641417292161   to: 0.092289581880937846



# Switched to train mode...
Epoch: [17][  0/391]	Time  0.193 ( 0.193)	Data  0.152 ( 0.152)	Loss 6.0219e-01 (6.0219e-01)	Acc@1  83.59 ( 83.59)	Acc@5  96.09 ( 96.09)
Epoch: [17][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 8.4570e-01 (6.3114e-01)	Acc@1  76.56 ( 81.11)	Acc@5  96.88 ( 97.23)
Epoch: [17][ 20/391]	Time  0.045 ( 0.054)	Data  0.001 ( 0.010)	Loss 5.7375e-01 (6.4415e-01)	Acc@1  82.81 ( 80.95)	Acc@5  97.66 ( 97.14)
Epoch: [17][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 6.6667e-01 (6.4578e-01)	Acc@1  79.69 ( 80.92)	Acc@5  98.44 ( 97.15)
Epoch: [17][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 5.5738e-01 (6.2779e-01)	Acc@1  81.25 ( 81.23)	Acc@5  97.66 ( 97.31)
Epoch: [17][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.4872e-01 (6.3766e-01)	Acc@1  85.16 ( 80.81)	Acc@5  97.66 ( 97.26)
Epoch: [17][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.2922e-01 (6.3495e-01)	Acc@1  80.47 ( 80.81)	Acc@5  95.31 ( 97.22)
Epoch: [17][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 6.4589e-01 (6.4321e-01)	Acc@1  78.91 ( 80.55)	Acc@5  96.88 ( 97.14)
Epoch: [17][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.7647e-01 (6.4219e-01)	Acc@1  82.81 ( 80.76)	Acc@5  96.88 ( 97.13)
Epoch: [17][ 90/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.4662e-01 (6.4064e-01)	Acc@1  75.78 ( 80.67)	Acc@5  97.66 ( 97.15)
Epoch: [17][100/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.2680e-01 (6.4440e-01)	Acc@1  78.91 ( 80.48)	Acc@5  98.44 ( 97.11)
Epoch: [17][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.2529e-01 (6.4518e-01)	Acc@1  85.94 ( 80.51)	Acc@5 100.00 ( 97.17)
Epoch: [17][120/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.2561e-01 (6.4604e-01)	Acc@1  84.38 ( 80.37)	Acc@5  97.66 ( 97.20)
Epoch: [17][130/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5076e-01 (6.4974e-01)	Acc@1  85.94 ( 80.16)	Acc@5  99.22 ( 97.13)
Epoch: [17][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.8875e-01 (6.5281e-01)	Acc@1  77.34 ( 80.04)	Acc@5  97.66 ( 97.09)
Epoch: [17][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.3556e-01 (6.5723e-01)	Acc@1  79.69 ( 79.90)	Acc@5  97.66 ( 97.05)
Epoch: [17][160/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 6.1147e-01 (6.5797e-01)	Acc@1  78.91 ( 79.80)	Acc@5  98.44 ( 97.08)
Epoch: [17][170/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.003)	Loss 7.3728e-01 (6.6205e-01)	Acc@1  77.34 ( 79.65)	Acc@5  96.09 ( 97.03)
Epoch: [17][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5673e-01 (6.6398e-01)	Acc@1  74.22 ( 79.50)	Acc@5  97.66 ( 97.04)
Epoch: [17][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.6900e-01 (6.6971e-01)	Acc@1  71.09 ( 79.32)	Acc@5  96.09 ( 97.01)
Epoch: [17][200/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6186e-01 (6.7258e-01)	Acc@1  83.59 ( 79.24)	Acc@5  98.44 ( 96.99)
Epoch: [17][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4617e-01 (6.7166e-01)	Acc@1  77.34 ( 79.22)	Acc@5  96.09 ( 97.01)
Epoch: [17][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5606e-01 (6.7636e-01)	Acc@1  71.09 ( 79.03)	Acc@5  96.88 ( 96.99)
Epoch: [17][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6769e-01 (6.7856e-01)	Acc@1  78.91 ( 78.95)	Acc@5  94.53 ( 96.97)
Epoch: [17][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4042e-01 (6.7821e-01)	Acc@1  78.12 ( 78.92)	Acc@5  96.88 ( 96.98)
Epoch: [17][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4792e-01 (6.7939e-01)	Acc@1  76.56 ( 78.85)	Acc@5  97.66 ( 96.95)
Epoch: [17][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2555e-01 (6.8303e-01)	Acc@1  81.25 ( 78.76)	Acc@5  96.88 ( 96.92)
Epoch: [17][270/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3280e-01 (6.8404e-01)	Acc@1  82.03 ( 78.78)	Acc@5  98.44 ( 96.90)
Epoch: [17][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1248e-01 (6.8603e-01)	Acc@1  75.00 ( 78.68)	Acc@5  93.75 ( 96.89)
Epoch: [17][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9119e-01 (6.8739e-01)	Acc@1  80.47 ( 78.65)	Acc@5  96.09 ( 96.88)
Epoch: [17][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8253e-01 (6.8950e-01)	Acc@1  82.03 ( 78.62)	Acc@5  96.88 ( 96.88)
Epoch: [17][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7630e-01 (6.9138e-01)	Acc@1  79.69 ( 78.56)	Acc@5  98.44 ( 96.87)
Epoch: [17][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1467e-01 (6.9335e-01)	Acc@1  72.66 ( 78.50)	Acc@5  93.75 ( 96.85)
Epoch: [17][330/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7003e-01 (6.9422e-01)	Acc@1  81.25 ( 78.51)	Acc@5  96.09 ( 96.81)
Epoch: [17][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2886e-01 (6.9476e-01)	Acc@1  79.69 ( 78.50)	Acc@5  97.66 ( 96.81)
Epoch: [17][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4225e-01 (6.9766e-01)	Acc@1  76.56 ( 78.42)	Acc@5  98.44 ( 96.79)
Epoch: [17][360/391]	Time  0.052 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9458e-01 (6.9926e-01)	Acc@1  75.78 ( 78.37)	Acc@5  99.22 ( 96.78)
Epoch: [17][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1167e-01 (7.0159e-01)	Acc@1  77.34 ( 78.27)	Acc@5  96.88 ( 96.75)
Epoch: [17][380/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4912e-01 (7.0350e-01)	Acc@1  73.44 ( 78.20)	Acc@5  94.53 ( 96.75)
Epoch: [17][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7311e-01 (7.0497e-01)	Acc@1  73.75 ( 78.14)	Acc@5  95.00 ( 96.73)
## e[17] optimizer.zero_grad (sum) time: 0.13761353492736816
## e[17]       loss.backward (sum) time: 2.5542659759521484
## e[17]      optimizer.step (sum) time: 1.0123374462127686
## epoch[17] training(only) time: 18.512112140655518
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.4962e+00 (1.4962e+00)	Acc@1  65.00 ( 65.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.6782e+00 (1.4783e+00)	Acc@1  60.00 ( 63.45)	Acc@5  88.00 ( 87.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5081e+00 (1.4453e+00)	Acc@1  62.00 ( 63.48)	Acc@5  88.00 ( 87.95)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8392e+00 (1.4604e+00)	Acc@1  53.00 ( 63.10)	Acc@5  84.00 ( 87.61)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4384e+00 (1.4416e+00)	Acc@1  61.00 ( 63.05)	Acc@5  92.00 ( 88.10)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.3031e+00 (1.4434e+00)	Acc@1  63.00 ( 62.86)	Acc@5  88.00 ( 87.94)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2586e+00 (1.4161e+00)	Acc@1  66.00 ( 63.23)	Acc@5  91.00 ( 88.41)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5463e+00 (1.4119e+00)	Acc@1  61.00 ( 63.45)	Acc@5  89.00 ( 88.41)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.2811e+00 (1.4241e+00)	Acc@1  66.00 ( 63.26)	Acc@5  88.00 ( 88.17)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7581e+00 (1.4086e+00)	Acc@1  59.00 ( 63.59)	Acc@5  82.00 ( 88.33)
 * Acc@1 63.670 Acc@5 88.290
### epoch[17] execution time: 21.003467321395874
EPOCH 18
i:   0, name:          module.conv1.0.weight  changing lr from: 0.071012453525392968   to: 0.067844492669611026
i:   1, name:          module.conv1.1.weight  changing lr from: 0.071647763905151693   to: 0.068535902829248430
i:   2, name:            module.conv1.1.bias  changing lr from: 0.072269051697252437   to: 0.069212583128386768
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.072876588910483237   to: 0.069874789343291541
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.073470646407479898   to: 0.070522778468120301
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.074051493516081618   to: 0.071156808151881606
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.074619397679678512   to: 0.071777136185279769
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.075174624143621568   to: 0.072384020033909854
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.075717435674961572   to: 0.072977716414491065
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.076248092312968807   to: 0.073558480911038476
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.076766851148056842   to: 0.074126567628071668
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.077273966126897520   to: 0.074682228878148377
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.077769687881665855   to: 0.075225714901186916
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.078254263581496120   to: 0.075757273613209428
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.078727936804364726   to: 0.076277150382293893
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.079190947427739800   to: 0.076785587829671112
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.079643531536454884   to: 0.077282825654040904
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.080085921346373559   to: 0.077769100477312322
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.080518345142514172   to: 0.078244645710094651
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.080941027230398865   to: 0.078709691435379883
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.081354187899481401   to: 0.079164464308966065
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.081758043397590477   to: 0.079609187475269225
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.082152805915403665   to: 0.080044080497267994
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.082538683580039296   to: 0.080469359299411486
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.082915880456921359   to: 0.080885236122404128
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.083284596559135574   to: 0.081291919488857833
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.083645027863552879   to: 0.081689614178874090
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.083997366333052187   to: 0.082078521214685798
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.084341799944224402   to: 0.082458837853551514
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.084678512719987414   to: 0.082830757588153142
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.085007684766585601   to: 0.083194470153803352
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.085329492314488858   to: 0.083550161541819823
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.085644107762743282   to: 0.083898014018470704
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.085951699726362615   to: 0.084238206148940897
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.086252433086380628   to: 0.084570912825809405
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.086546469042216703   to: 0.084896305301566943
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.086833965166034477   to: 0.085214551224738741
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.087115075458799804   to: 0.085525814679211318
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.087389950407769104   to: 0.085830256226392343
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.087658737045161361   to: 0.086128032949862654
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.087921579007788381   to: 0.086419298502205699
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.088178616597437171   to: 0.086704203153724957
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.088429986841816813   to: 0.086982893842783446
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.088675823555898037   to: 0.087255514227520564
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.088916257403490059   to: 0.087522204738722229
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.089151415958913321   to: 0.087783102633638338
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.089381423768639362   to: 0.088038342050559368
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.089606402412782352   to: 0.088288054063979840
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.089826470566337357   to: 0.088532366740191271
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.090041744060071136   to: 0.088771405193161151
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.090252335940980671   to: 0.089005291640567308
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.090458356532244058   to: 0.089234145459868686
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.090659913492595945   to: 0.089458083244305070
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.090857111875067564   to: 0.089677218858727772
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.091050054185038476   to: 0.089891663495173510
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.091238840437553426   to: 0.090101525728101539
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.091423568213863174   to: 0.090306911569223103
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.091604332717154582   to: 0.090507924521858796
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.091781226827439069   to: 0.090704665634767187
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.091954341155573785   to: 0.090897233555393364
i:  60, name:               module.fc.weight  changing lr from: 0.092123764096393848   to: 0.091085724582492963
i:  61, name:                 module.fc.bias  changing lr from: 0.092289581880937846   to: 0.091270232718091940



# Switched to train mode...
Epoch: [18][  0/391]	Time  0.184 ( 0.184)	Data  0.143 ( 0.143)	Loss 7.2565e-01 (7.2565e-01)	Acc@1  75.78 ( 75.78)	Acc@5  97.66 ( 97.66)
Epoch: [18][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.015)	Loss 5.5510e-01 (6.2395e-01)	Acc@1  79.69 ( 79.69)	Acc@5  99.22 ( 97.51)
Epoch: [18][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 5.7757e-01 (5.8798e-01)	Acc@1  82.03 ( 81.32)	Acc@5  99.22 ( 97.92)
Epoch: [18][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 6.3040e-01 (5.9160e-01)	Acc@1  79.69 ( 81.38)	Acc@5  96.09 ( 97.71)
Epoch: [18][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 7.8806e-01 (5.8362e-01)	Acc@1  75.78 ( 81.50)	Acc@5  93.75 ( 97.69)
Epoch: [18][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.8909e-01 (5.8072e-01)	Acc@1  85.16 ( 81.59)	Acc@5  99.22 ( 97.75)
Epoch: [18][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.8833e-01 (5.8256e-01)	Acc@1  83.59 ( 81.53)	Acc@5  96.88 ( 97.64)
Epoch: [18][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 6.4968e-01 (5.8914e-01)	Acc@1  82.03 ( 81.37)	Acc@5  96.88 ( 97.65)
Epoch: [18][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5236e-01 (5.8454e-01)	Acc@1  82.03 ( 81.55)	Acc@5  98.44 ( 97.70)
Epoch: [18][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.5284e-01 (5.8727e-01)	Acc@1  78.91 ( 81.40)	Acc@5  96.88 ( 97.69)
Epoch: [18][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0651e-01 (5.8920e-01)	Acc@1  84.38 ( 81.33)	Acc@5 100.00 ( 97.70)
Epoch: [18][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.2659e-01 (5.9009e-01)	Acc@1  77.34 ( 81.27)	Acc@5  97.66 ( 97.69)
Epoch: [18][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1835e-01 (5.9118e-01)	Acc@1  82.81 ( 81.24)	Acc@5  98.44 ( 97.71)
Epoch: [18][130/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 6.0704e-01 (5.9431e-01)	Acc@1  80.47 ( 81.21)	Acc@5  96.09 ( 97.72)
Epoch: [18][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 7.2903e-01 (5.9549e-01)	Acc@1  76.56 ( 81.22)	Acc@5  91.41 ( 97.65)
Epoch: [18][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5554e-01 (6.0058e-01)	Acc@1  79.69 ( 80.99)	Acc@5  97.66 ( 97.63)
Epoch: [18][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4006e-01 (6.0598e-01)	Acc@1  89.06 ( 80.88)	Acc@5  96.88 ( 97.59)
Epoch: [18][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1010e-01 (6.0420e-01)	Acc@1  86.72 ( 80.93)	Acc@5  97.66 ( 97.61)
Epoch: [18][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2705e-01 (6.0486e-01)	Acc@1  87.50 ( 80.91)	Acc@5 100.00 ( 97.62)
Epoch: [18][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0099e-01 (6.0641e-01)	Acc@1  81.25 ( 80.85)	Acc@5  96.88 ( 97.59)
Epoch: [18][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8284e-01 (6.0691e-01)	Acc@1  83.59 ( 80.87)	Acc@5  98.44 ( 97.59)
Epoch: [18][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0279e-01 (6.0819e-01)	Acc@1  81.25 ( 80.82)	Acc@5  96.09 ( 97.58)
Epoch: [18][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9900e-01 (6.1034e-01)	Acc@1  79.69 ( 80.76)	Acc@5  96.09 ( 97.57)
Epoch: [18][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7690e-01 (6.1328e-01)	Acc@1  78.91 ( 80.73)	Acc@5  95.31 ( 97.52)
Epoch: [18][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3024e-01 (6.1680e-01)	Acc@1  76.56 ( 80.61)	Acc@5  96.88 ( 97.46)
Epoch: [18][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0266e-01 (6.2062e-01)	Acc@1  71.88 ( 80.52)	Acc@5  96.09 ( 97.44)
Epoch: [18][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3111e-01 (6.2177e-01)	Acc@1  73.44 ( 80.49)	Acc@5  96.88 ( 97.46)
Epoch: [18][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1603e-01 (6.2404e-01)	Acc@1  78.12 ( 80.38)	Acc@5  95.31 ( 97.43)
Epoch: [18][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1014e-01 (6.2424e-01)	Acc@1  84.38 ( 80.42)	Acc@5  99.22 ( 97.42)
Epoch: [18][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0726e-01 (6.2722e-01)	Acc@1  78.12 ( 80.36)	Acc@5  96.88 ( 97.39)
Epoch: [18][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5537e-01 (6.2980e-01)	Acc@1  75.78 ( 80.23)	Acc@5  97.66 ( 97.37)
Epoch: [18][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.6697e-01 (6.3346e-01)	Acc@1  76.56 ( 80.16)	Acc@5  97.66 ( 97.33)
Epoch: [18][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4276e-01 (6.3473e-01)	Acc@1  78.12 ( 80.14)	Acc@5  97.66 ( 97.30)
Epoch: [18][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3477e-01 (6.3616e-01)	Acc@1  77.34 ( 80.07)	Acc@5  98.44 ( 97.28)
Epoch: [18][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5576e-01 (6.3780e-01)	Acc@1  78.91 ( 79.98)	Acc@5  95.31 ( 97.26)
Epoch: [18][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1399e-01 (6.3985e-01)	Acc@1  82.81 ( 79.92)	Acc@5  96.09 ( 97.24)
Epoch: [18][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1767e-01 (6.4024e-01)	Acc@1  82.03 ( 79.90)	Acc@5  98.44 ( 97.25)
Epoch: [18][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1010e-01 (6.4231e-01)	Acc@1  76.56 ( 79.84)	Acc@5  97.66 ( 97.22)
Epoch: [18][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0777e-01 (6.4444e-01)	Acc@1  74.22 ( 79.78)	Acc@5  96.88 ( 97.20)
Epoch: [18][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1733e-01 (6.4612e-01)	Acc@1  73.75 ( 79.72)	Acc@5  96.25 ( 97.20)
## e[18] optimizer.zero_grad (sum) time: 0.1383814811706543
## e[18]       loss.backward (sum) time: 2.5426859855651855
## e[18]      optimizer.step (sum) time: 1.0167806148529053
## epoch[18] training(only) time: 18.441658973693848
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.6210e+00 (1.6210e+00)	Acc@1  64.00 ( 64.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.4088e+00 (1.4243e+00)	Acc@1  60.00 ( 64.27)	Acc@5  90.00 ( 88.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2851e+00 (1.4153e+00)	Acc@1  72.00 ( 64.67)	Acc@5  89.00 ( 88.86)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6005e+00 (1.4390e+00)	Acc@1  54.00 ( 63.52)	Acc@5  90.00 ( 88.42)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.1597e+00 (1.4143e+00)	Acc@1  63.00 ( 63.80)	Acc@5  91.00 ( 88.98)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5997e+00 (1.4310e+00)	Acc@1  64.00 ( 63.65)	Acc@5  88.00 ( 88.73)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3583e+00 (1.4124e+00)	Acc@1  61.00 ( 63.89)	Acc@5  89.00 ( 88.77)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.4340e+00 (1.4186e+00)	Acc@1  67.00 ( 63.97)	Acc@5  89.00 ( 88.69)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6363e+00 (1.4309e+00)	Acc@1  61.00 ( 63.75)	Acc@5  86.00 ( 88.63)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.7655e+00 (1.4170e+00)	Acc@1  57.00 ( 63.85)	Acc@5  87.00 ( 88.82)
 * Acc@1 64.180 Acc@5 88.910
### epoch[18] execution time: 20.927653074264526
EPOCH 19
i:   0, name:          module.conv1.0.weight  changing lr from: 0.067844492669611026   to: 0.064598267368231238
i:   1, name:          module.conv1.1.weight  changing lr from: 0.068535902829248430   to: 0.065344507771845903
i:   2, name:            module.conv1.1.bias  changing lr from: 0.069212583128386768   to: 0.066075473528207057
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.069874789343291541   to: 0.066791393909926722
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.070522778468120301   to: 0.067492502325743275
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.071156808151881606   to: 0.068179035556504536
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.071777136185279769   to: 0.068851233052930769
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.072384020033909854   to: 0.069509336291009388
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.072977716414491065   to: 0.070153588181120830
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.073558480911038476   to: 0.070784232527229637
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.074126567628071668   to: 0.071401513532696567
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.074682228878148377   to: 0.072005675349479811
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.075225714901186916   to: 0.072596961667692431
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.075757273613209428   to: 0.073175615342673264
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.076277150382293893   to: 0.073741878056906698
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.076785587829671112   to: 0.074295990014296334
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.077282825654040904   to: 0.074838189664456753
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.077769100477312322   to: 0.075368713454837741
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.078244645710094651   to: 0.075887795608637718
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.078709691435379883   to: 0.076395667926595245
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.079164464308966065   to: 0.076892559610874656
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.079609187475269225   to: 0.077378697109377795
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.080044080497267994   to: 0.077854303978926856
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.080469359299411486   to: 0.078319600765865693
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.080885236122404128   to: 0.078774804902725565
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.081291919488857833   to: 0.079220130619693241
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.081689614178874090   to: 0.079655788869704355
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.082078521214685798   to: 0.080081987266066412
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.082458837853551514   to: 0.080498930031591420
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.082830757588153142   to: 0.080906817958287747
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.083194470153803352   to: 0.081305848376728607
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.083550161541819823   to: 0.081696215134275038
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.083898014018470704   to: 0.082078108581390155
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.084238206148940897   to: 0.082451715565335509
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.084570912825809405   to: 0.082817219430590464
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.084896305301566943   to: 0.083174800025383647
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.085214551224738741   to: 0.083524633713769114
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.085525814679211318   to: 0.083866893392721686
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.085830256226392343   to: 0.084201748513763885
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.086128032949862654   to: 0.084529365108673288
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.086419298502205699   to: 0.084849905818852739
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.086704203153724957   to: 0.085163529927976614
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.086982893842783446   to: 0.085470393397556341
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.087255514227520564   to: 0.085770648905094587
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.087522204738722229   to: 0.086064445884523916
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.087783102633638338   to: 0.086351930568648386
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.088038342050559368   to: 0.086633246033329142
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.088288054063979840   to: 0.086908532243175435
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.088532366740191271   to: 0.087177926098521097
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.088771405193161151   to: 0.087441561483484864
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.089005291640567308   to: 0.087699569314929060
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.089234145459868686   to: 0.087952077592146183
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.089458083244305070   to: 0.088199211447117898
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.089677218858727772   to: 0.088441093195203532
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.089891663495173510   to: 0.088677842386127595
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.090101525728101539   to: 0.088909575855147485
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.090306911569223103   to: 0.089136407774292686
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.090507924521858796   to: 0.089358449703577009
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.090704665634767187   to: 0.089575810642094231
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.090897233555393364   to: 0.089788597078916010
i:  60, name:               module.fc.weight  changing lr from: 0.091085724582492963   to: 0.089996913043718638
i:  61, name:                 module.fc.bias  changing lr from: 0.091270232718091940   to: 0.090200860157072710



# Switched to train mode...
Epoch: [19][  0/391]	Time  0.182 ( 0.182)	Data  0.142 ( 0.142)	Loss 4.9329e-01 (4.9329e-01)	Acc@1  82.81 ( 82.81)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 8.2001e-01 (5.8977e-01)	Acc@1  74.22 ( 81.11)	Acc@5  95.31 ( 98.22)
Epoch: [19][ 20/391]	Time  0.045 ( 0.053)	Data  0.001 ( 0.009)	Loss 4.4044e-01 (5.6407e-01)	Acc@1  83.59 ( 81.92)	Acc@5  98.44 ( 98.40)
Epoch: [19][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 5.6060e-01 (5.5183e-01)	Acc@1  82.03 ( 82.43)	Acc@5  99.22 ( 98.41)
Epoch: [19][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 6.1906e-01 (5.4678e-01)	Acc@1  78.91 ( 82.45)	Acc@5  96.88 ( 98.38)
Epoch: [19][ 50/391]	Time  0.045 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.9375e-01 (5.4711e-01)	Acc@1  87.50 ( 82.66)	Acc@5  99.22 ( 98.35)
Epoch: [19][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.0608e-01 (5.4857e-01)	Acc@1  85.16 ( 82.52)	Acc@5  99.22 ( 98.27)
Epoch: [19][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8467e-01 (5.4972e-01)	Acc@1  86.72 ( 82.61)	Acc@5  99.22 ( 98.15)
Epoch: [19][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8134e-01 (5.4912e-01)	Acc@1  85.16 ( 82.64)	Acc@5  99.22 ( 98.18)
Epoch: [19][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.9406e-01 (5.4791e-01)	Acc@1  81.25 ( 82.70)	Acc@5  96.88 ( 98.17)
Epoch: [19][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.2099e-01 (5.4822e-01)	Acc@1  82.81 ( 82.71)	Acc@5 100.00 ( 98.20)
Epoch: [19][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.0261e-01 (5.4986e-01)	Acc@1  78.91 ( 82.67)	Acc@5  98.44 ( 98.16)
Epoch: [19][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.1744e-01 (5.5072e-01)	Acc@1  82.81 ( 82.66)	Acc@5  97.66 ( 98.19)
Epoch: [19][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.9336e-01 (5.5388e-01)	Acc@1  78.12 ( 82.50)	Acc@5 100.00 ( 98.20)
Epoch: [19][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 5.4527e-01 (5.5480e-01)	Acc@1  81.25 ( 82.46)	Acc@5  99.22 ( 98.19)
Epoch: [19][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9254e-01 (5.5538e-01)	Acc@1  85.94 ( 82.40)	Acc@5  96.88 ( 98.19)
Epoch: [19][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6305e-01 (5.5807e-01)	Acc@1  82.03 ( 82.28)	Acc@5  97.66 ( 98.16)
Epoch: [19][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7640e-01 (5.6226e-01)	Acc@1  80.47 ( 82.19)	Acc@5  95.31 ( 98.11)
Epoch: [19][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9240e-01 (5.6246e-01)	Acc@1  90.62 ( 82.15)	Acc@5  98.44 ( 98.11)
Epoch: [19][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6658e-01 (5.6581e-01)	Acc@1  82.03 ( 82.11)	Acc@5  96.88 ( 98.07)
Epoch: [19][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0003e-01 (5.6818e-01)	Acc@1  85.16 ( 81.99)	Acc@5  99.22 ( 98.08)
Epoch: [19][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6243e-01 (5.7030e-01)	Acc@1  78.91 ( 81.92)	Acc@5  96.88 ( 98.06)
Epoch: [19][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4533e-01 (5.7334e-01)	Acc@1  81.25 ( 81.89)	Acc@5  96.88 ( 98.04)
Epoch: [19][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8311e-01 (5.7350e-01)	Acc@1  81.25 ( 81.89)	Acc@5  98.44 ( 98.02)
Epoch: [19][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3415e-01 (5.7583e-01)	Acc@1  86.72 ( 81.86)	Acc@5  98.44 ( 98.00)
Epoch: [19][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4854e-01 (5.7801e-01)	Acc@1  76.56 ( 81.76)	Acc@5  98.44 ( 97.98)
Epoch: [19][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4174e-01 (5.7994e-01)	Acc@1  81.25 ( 81.73)	Acc@5  95.31 ( 97.96)
Epoch: [19][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0443e-01 (5.8235e-01)	Acc@1  82.03 ( 81.64)	Acc@5  97.66 ( 97.94)
Epoch: [19][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6613e-01 (5.8290e-01)	Acc@1  75.78 ( 81.60)	Acc@5  99.22 ( 97.94)
Epoch: [19][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5397e-01 (5.8422e-01)	Acc@1  83.59 ( 81.53)	Acc@5  96.88 ( 97.93)
Epoch: [19][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7228e-01 (5.8632e-01)	Acc@1  79.69 ( 81.47)	Acc@5  98.44 ( 97.91)
Epoch: [19][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5668e-01 (5.8718e-01)	Acc@1  73.44 ( 81.44)	Acc@5  96.09 ( 97.89)
Epoch: [19][320/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4590e-01 (5.8951e-01)	Acc@1  76.56 ( 81.35)	Acc@5  97.66 ( 97.87)
Epoch: [19][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0770e-01 (5.9103e-01)	Acc@1  84.38 ( 81.34)	Acc@5  96.88 ( 97.85)
Epoch: [19][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1704e-01 (5.9335e-01)	Acc@1  78.12 ( 81.28)	Acc@5  96.09 ( 97.85)
Epoch: [19][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8573e-01 (5.9580e-01)	Acc@1  75.00 ( 81.22)	Acc@5  93.75 ( 97.80)
Epoch: [19][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2817e-01 (5.9835e-01)	Acc@1  82.03 ( 81.15)	Acc@5  98.44 ( 97.79)
Epoch: [19][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7467e-01 (6.0087e-01)	Acc@1  76.56 ( 81.05)	Acc@5  96.88 ( 97.77)
Epoch: [19][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7495e-01 (6.0485e-01)	Acc@1  75.00 ( 80.94)	Acc@5  97.66 ( 97.72)
Epoch: [19][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8755e-01 (6.0698e-01)	Acc@1  77.50 ( 80.88)	Acc@5  97.50 ( 97.70)
## e[19] optimizer.zero_grad (sum) time: 0.13630437850952148
## e[19]       loss.backward (sum) time: 2.530017137527466
## e[19]      optimizer.step (sum) time: 1.0144410133361816
## epoch[19] training(only) time: 18.450295209884644
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2676e+00 (1.2676e+00)	Acc@1  69.00 ( 69.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.6613e+00 (1.4416e+00)	Acc@1  61.00 ( 65.09)	Acc@5  87.00 ( 88.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.4306e+00 (1.3854e+00)	Acc@1  66.00 ( 65.48)	Acc@5  89.00 ( 89.05)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8379e+00 (1.4171e+00)	Acc@1  56.00 ( 64.84)	Acc@5  84.00 ( 88.61)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3256e+00 (1.3924e+00)	Acc@1  66.00 ( 64.46)	Acc@5  91.00 ( 88.95)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6504e+00 (1.4104e+00)	Acc@1  64.00 ( 64.18)	Acc@5  82.00 ( 88.47)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4321e+00 (1.3870e+00)	Acc@1  58.00 ( 64.10)	Acc@5  90.00 ( 88.82)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.3651e+00 (1.3932e+00)	Acc@1  63.00 ( 64.08)	Acc@5  92.00 ( 88.79)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.4225e+00 (1.3996e+00)	Acc@1  65.00 ( 64.00)	Acc@5  86.00 ( 88.74)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.5667e+00 (1.3863e+00)	Acc@1  57.00 ( 64.23)	Acc@5  86.00 ( 88.88)
 * Acc@1 64.350 Acc@5 88.900
### epoch[19] execution time: 20.96044421195984
EPOCH 20
i:   0, name:          module.conv1.0.weight  changing lr from: 0.064598267368231238   to: 0.061288860534611772
i:   1, name:          module.conv1.1.weight  changing lr from: 0.065344507771845903   to: 0.062088053285975331
i:   2, name:            module.conv1.1.bias  changing lr from: 0.066075473528207057   to: 0.062871613279627134
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.066791393909926722   to: 0.063639732153652276
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.067492502325743275   to: 0.064392609172542484
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.068179035556504536   to: 0.065130450236814783
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.068851233052930769   to: 0.065853466967060945
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.069509336291009388   to: 0.066561875857690217
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.070153588181120830   to: 0.067255897495891545
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.070784232527229637   to: 0.067935755841590992
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.071401513532696567   to: 0.068601677564420746
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.072005675349479811   to: 0.069253891433945849
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.072596961667692431   to: 0.069892627759612178
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.073175615342673264   to: 0.070518117877089406
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.073741878056906698   to: 0.071130593677878551
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.074295990014296334   to: 0.071730287179243568
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.074838189664456753   to: 0.072317430131703497
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.075368713454837741   to: 0.072892253661491421
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.075887795608637718   to: 0.073454987945546696
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.076395667926595245   to: 0.074005861916756829
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.076892559610874656   to: 0.074545102997310639
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.077378697109377795   to: 0.075072936858156983
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.077854303978926856   to: 0.075589587202692293
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.078319600765865693   to: 0.076095275572919716
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.078774804902725565   to: 0.076590221176435505
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.079220130619693241   to: 0.077074640732705535
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.079655788869704355   to: 0.077548748337194037
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.080081987266066412   to: 0.078012755342001985
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.080498930031591420   to: 0.078466870251760579
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.080906817958287747   to: 0.078911298633608296
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.081305848376728607   to: 0.079346243040158984
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.081696215134275038   to: 0.079771902944441100
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.082078108581390155   to: 0.080188474685856923
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.082451715565335509   to: 0.080596151426276463
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.082817219430590464   to: 0.080995123115439160
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.083174800025383647   to: 0.081385576464895512
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.083524633713769114   to: 0.081767694929771678
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.083866893392721686   to: 0.082141658697691966
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.084201748513763885   to: 0.082507644684238934
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.084529365108673288   to: 0.082865826534375339
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.084849905818852739   to: 0.083216374629293016
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.085163529927976614   to: 0.083559456098191071
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.085470393397556341   to: 0.083895234834522533
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.085770648905094587   to: 0.084223871516280760
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.086064445884523916   to: 0.084545523629929190
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.086351930568648386   to: 0.084860345497605905
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.086633246033329142   to: 0.085168488307262480
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.086908532243175435   to: 0.085470100145421779
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.087177926098521097   to: 0.085765326032262487
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.087441561483484864   to: 0.086054307958760817
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.087699569314929060   to: 0.086337184925640062
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.087952077592146183   to: 0.086614092983897994
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.088199211447117898   to: 0.086885165276699872
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.088441093195203532   to: 0.087150532082441348
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.088677842386127595   to: 0.087410320858801627
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.088909575855147485   to: 0.087664656287620726
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.089136407774292686   to: 0.087913660320449033
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.089358449703577009   to: 0.088157452224629451
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.089575810642094231   to: 0.088396148629783983
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.089788597078916010   to: 0.088629863574587422
i:  60, name:               module.fc.weight  changing lr from: 0.089996913043718638   to: 0.088858708553721244
i:  61, name:                 module.fc.bias  changing lr from: 0.090200860157072710   to: 0.089082792564909710



# Switched to train mode...
Epoch: [20][  0/391]	Time  0.194 ( 0.194)	Data  0.150 ( 0.150)	Loss 4.9826e-01 (4.9826e-01)	Acc@1  85.16 ( 85.16)	Acc@5 100.00 (100.00)
Epoch: [20][ 10/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.016)	Loss 4.6788e-01 (5.1226e-01)	Acc@1  87.50 ( 84.73)	Acc@5 100.00 ( 98.58)
Epoch: [20][ 20/391]	Time  0.049 ( 0.054)	Data  0.001 ( 0.010)	Loss 4.0030e-01 (5.2517e-01)	Acc@1  85.16 ( 83.93)	Acc@5  98.44 ( 98.55)
Epoch: [20][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.5019e-01 (5.1689e-01)	Acc@1  87.50 ( 83.80)	Acc@5  98.44 ( 98.49)
Epoch: [20][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.7542e-01 (5.1141e-01)	Acc@1  85.94 ( 83.94)	Acc@5  99.22 ( 98.48)
Epoch: [20][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 4.0948e-01 (4.9814e-01)	Acc@1  89.06 ( 84.62)	Acc@5  99.22 ( 98.53)
Epoch: [20][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.4783e-01 (4.9730e-01)	Acc@1  79.69 ( 84.66)	Acc@5 100.00 ( 98.57)
Epoch: [20][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.4074e-01 (4.9543e-01)	Acc@1  89.84 ( 84.89)	Acc@5  99.22 ( 98.57)
Epoch: [20][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.3356e-01 (5.0147e-01)	Acc@1  81.25 ( 84.63)	Acc@5  97.66 ( 98.51)
Epoch: [20][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4124e-01 (4.9917e-01)	Acc@1  87.50 ( 84.63)	Acc@5  97.66 ( 98.50)
Epoch: [20][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1056e-01 (4.9817e-01)	Acc@1  82.81 ( 84.62)	Acc@5  98.44 ( 98.45)
Epoch: [20][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.2310e-01 (4.9629e-01)	Acc@1  85.16 ( 84.73)	Acc@5  99.22 ( 98.45)
Epoch: [20][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5999e-01 (4.9839e-01)	Acc@1  78.12 ( 84.65)	Acc@5  96.88 ( 98.41)
Epoch: [20][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.0267e-01 (4.9996e-01)	Acc@1  78.91 ( 84.55)	Acc@5  97.66 ( 98.38)
Epoch: [20][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.2928e-01 (5.0175e-01)	Acc@1  84.38 ( 84.47)	Acc@5  98.44 ( 98.35)
Epoch: [20][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 5.7958e-01 (5.0552e-01)	Acc@1  81.25 ( 84.29)	Acc@5  99.22 ( 98.32)
Epoch: [20][160/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8464e-01 (5.0664e-01)	Acc@1  76.56 ( 84.16)	Acc@5  99.22 ( 98.35)
Epoch: [20][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4766e-01 (5.1357e-01)	Acc@1  76.56 ( 83.90)	Acc@5  97.66 ( 98.35)
Epoch: [20][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8675e-01 (5.1878e-01)	Acc@1  78.12 ( 83.72)	Acc@5  96.09 ( 98.30)
Epoch: [20][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4424e-01 (5.1956e-01)	Acc@1  80.47 ( 83.68)	Acc@5 100.00 ( 98.31)
Epoch: [20][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9720e-01 (5.2375e-01)	Acc@1  83.59 ( 83.59)	Acc@5  98.44 ( 98.27)
Epoch: [20][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0731e-01 (5.2667e-01)	Acc@1  81.25 ( 83.47)	Acc@5  95.31 ( 98.24)
Epoch: [20][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9395e-01 (5.2962e-01)	Acc@1  81.25 ( 83.40)	Acc@5  97.66 ( 98.20)
Epoch: [20][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0358e-01 (5.3082e-01)	Acc@1  78.91 ( 83.32)	Acc@5  97.66 ( 98.22)
Epoch: [20][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7643e-01 (5.3206e-01)	Acc@1  78.12 ( 83.27)	Acc@5  97.66 ( 98.22)
Epoch: [20][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7192e-01 (5.3380e-01)	Acc@1  81.25 ( 83.14)	Acc@5  99.22 ( 98.22)
Epoch: [20][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8975e-01 (5.3609e-01)	Acc@1  84.38 ( 83.10)	Acc@5  98.44 ( 98.20)
Epoch: [20][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8689e-01 (5.3853e-01)	Acc@1  75.78 ( 83.01)	Acc@5  97.66 ( 98.17)
Epoch: [20][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9422e-01 (5.3947e-01)	Acc@1  82.81 ( 82.96)	Acc@5  99.22 ( 98.15)
Epoch: [20][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3344e-01 (5.4156e-01)	Acc@1  80.47 ( 82.90)	Acc@5  96.88 ( 98.13)
Epoch: [20][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8861e-01 (5.4355e-01)	Acc@1  78.91 ( 82.86)	Acc@5  97.66 ( 98.10)
Epoch: [20][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3095e-01 (5.4546e-01)	Acc@1  84.38 ( 82.79)	Acc@5  96.09 ( 98.09)
Epoch: [20][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0484e-01 (5.4697e-01)	Acc@1  83.59 ( 82.73)	Acc@5  98.44 ( 98.09)
Epoch: [20][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.9714e-01 (5.4946e-01)	Acc@1  73.44 ( 82.68)	Acc@5  94.53 ( 98.05)
Epoch: [20][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5106e-01 (5.5098e-01)	Acc@1  75.78 ( 82.61)	Acc@5  96.88 ( 98.02)
Epoch: [20][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4877e-01 (5.5297e-01)	Acc@1  81.25 ( 82.57)	Acc@5  96.88 ( 98.00)
Epoch: [20][360/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8362e-01 (5.5501e-01)	Acc@1  82.03 ( 82.51)	Acc@5  96.09 ( 97.98)
Epoch: [20][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1425e-01 (5.5744e-01)	Acc@1  83.59 ( 82.44)	Acc@5  98.44 ( 97.95)
Epoch: [20][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8642e-01 (5.5933e-01)	Acc@1  82.03 ( 82.36)	Acc@5  94.53 ( 97.94)
Epoch: [20][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2689e-01 (5.6236e-01)	Acc@1  76.25 ( 82.25)	Acc@5  97.50 ( 97.92)
## e[20] optimizer.zero_grad (sum) time: 0.14036250114440918
## e[20]       loss.backward (sum) time: 2.5322000980377197
## e[20]      optimizer.step (sum) time: 1.018932580947876
## epoch[20] training(only) time: 18.455503702163696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.3416e+00 (1.3416e+00)	Acc@1  74.00 ( 74.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5364e+00 (1.5137e+00)	Acc@1  64.00 ( 64.27)	Acc@5  92.00 ( 87.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5787e+00 (1.4550e+00)	Acc@1  60.00 ( 64.81)	Acc@5  88.00 ( 88.24)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7745e+00 (1.5062e+00)	Acc@1  62.00 ( 64.16)	Acc@5  84.00 ( 87.32)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3645e+00 (1.5016e+00)	Acc@1  63.00 ( 64.10)	Acc@5  89.00 ( 88.00)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5686e+00 (1.5111e+00)	Acc@1  64.00 ( 63.73)	Acc@5  88.00 ( 88.04)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.5777e+00 (1.4785e+00)	Acc@1  61.00 ( 64.05)	Acc@5  91.00 ( 88.46)
Test: [ 70/100]	Time  0.028 ( 0.025)	Loss 1.7790e+00 (1.4757e+00)	Acc@1  57.00 ( 64.07)	Acc@5  87.00 ( 88.51)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6182e+00 (1.4791e+00)	Acc@1  62.00 ( 64.07)	Acc@5  89.00 ( 88.58)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8004e+00 (1.4715e+00)	Acc@1  52.00 ( 64.23)	Acc@5  85.00 ( 88.78)
 * Acc@1 64.440 Acc@5 88.870
### epoch[20] execution time: 20.948000192642212
EPOCH 21
i:   0, name:          module.conv1.0.weight  changing lr from: 0.061288860534611772   to: 0.057931648642011363
i:   1, name:          module.conv1.1.weight  changing lr from: 0.062088053285975331   to: 0.058781309001313287
i:   2, name:            module.conv1.1.bias  changing lr from: 0.062871613279627134   to: 0.059615188321720615
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.063639732153652276   to: 0.060433428733594724
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.064392609172542484   to: 0.061236184063225577
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.065130450236814783   to: 0.062023618592775678
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.065853466967060945   to: 0.062795905907750776
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.066561875857690217   to: 0.063553227826727163
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.067255897495891545   to: 0.064295773408332646
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.067935755841590992   to: 0.065023738030735787
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.068601677564420746   to: 0.065737322539146922
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.069253891433945849   to: 0.066436732457075973
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.069892627759612178   to: 0.067122177257321114
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.070518117877089406   to: 0.067793869688886538
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.071130593677878551   to: 0.068452025156237495
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.071730287179243568   to: 0.069096861147505975
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.072317430131703497   to: 0.069728596708452398
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.072892253661491421   to: 0.070347451959174731
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.073454987945546696   to: 0.070953647650731783
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.074005861916756829   to: 0.071547404759014374
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.074545102997310639   to: 0.072128944113357782
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.075072936858156983   to: 0.072698486057537856
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.075589587202692293   to: 0.073256250140938159
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.076095275572919716   to: 0.073802454837808340
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.076590221176435505   to: 0.074337317292663838
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.077074640732705535   to: 0.074861053089996132
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.077548748337194037   to: 0.075373876046578608
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.078012755342001985   to: 0.075875998024758973
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.078466870251760579   to: 0.076367628765233148
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.078911298633608296   to: 0.076848975737888792
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.079346243040158984   to: 0.077320244009399255
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.079771902944441100   to: 0.077781636126332099
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.080188474685856923   to: 0.078233352012617038
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.080596151426276463   to: 0.078675588880293856
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.080995123115439160   to: 0.079108541152530285
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.081385576464895512   to: 0.079532400397967773
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.081767694929771678   to: 0.079947355275514734
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.082141658697691966   to: 0.080353591488765641
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.082507644684238934   to: 0.080751291749279530
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.082865826534375339   to: 0.081140635748003107
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.083216374629293016   to: 0.081521800134172054
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.083559456098191071   to: 0.081894958501069454
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.083895234834522533   to: 0.082260281378063238
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.084223871516280760   to: 0.082617936228383940
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.084545523629929190   to: 0.082968087452141681
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.084860345497605905   to: 0.083310896394116443
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.085168488307262480   to: 0.083646521355888001
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.085470100145421779   to: 0.083975117611903072
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.085765326032262487   to: 0.084296837429105501
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.086054307958760817   to: 0.084611830089782125
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.086337184925640062   to: 0.084920241917302616
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.086614092983897994   to: 0.085222216304453771
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.086885165276699872   to: 0.085517893744092605
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.087150532082441348   to: 0.085807411861860486
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.087410320858801627   to: 0.086090905450722358
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.087664656287620726   to: 0.086368506507110587
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.087913660320449033   to: 0.086640344268471142
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.088157452224629451   to: 0.086906545252024903
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.088396148629783983   to: 0.087167233294570948
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.088629863574587422   to: 0.087422529593172787
i:  60, name:               module.fc.weight  changing lr from: 0.088858708553721244   to: 0.087672552746580742
i:  61, name:                 module.fc.bias  changing lr from: 0.089082792564909710   to: 0.087917418797255675



# Switched to train mode...
Epoch: [21][  0/391]	Time  0.189 ( 0.189)	Data  0.147 ( 0.147)	Loss 4.4597e-01 (4.4597e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [21][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.015)	Loss 4.8695e-01 (5.2416e-01)	Acc@1  85.16 ( 84.23)	Acc@5  99.22 ( 98.44)
Epoch: [21][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 7.0672e-01 (5.3439e-01)	Acc@1  78.12 ( 83.07)	Acc@5  96.88 ( 98.36)
Epoch: [21][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 5.8397e-01 (5.2857e-01)	Acc@1  82.03 ( 83.22)	Acc@5  98.44 ( 98.26)
Epoch: [21][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.2588e-01 (5.1346e-01)	Acc@1  89.06 ( 83.78)	Acc@5  99.22 ( 98.32)
Epoch: [21][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.6088e-01 (5.0243e-01)	Acc@1  81.25 ( 84.13)	Acc@5  98.44 ( 98.39)
Epoch: [21][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.3529e-01 (4.9719e-01)	Acc@1  85.94 ( 84.32)	Acc@5  98.44 ( 98.48)
Epoch: [21][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.7383e-01 (4.9758e-01)	Acc@1  87.50 ( 84.55)	Acc@5  97.66 ( 98.45)
Epoch: [21][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.1486e-01 (4.9776e-01)	Acc@1  82.81 ( 84.58)	Acc@5  95.31 ( 98.41)
Epoch: [21][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.6696e-01 (4.9210e-01)	Acc@1  82.03 ( 84.74)	Acc@5  96.88 ( 98.45)
Epoch: [21][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.5620e-01 (4.9137e-01)	Acc@1  83.59 ( 84.60)	Acc@5  99.22 ( 98.48)
Epoch: [21][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.6367e-01 (4.8958e-01)	Acc@1  85.94 ( 84.59)	Acc@5  99.22 ( 98.51)
Epoch: [21][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.6764e-01 (4.9004e-01)	Acc@1  83.59 ( 84.54)	Acc@5  97.66 ( 98.51)
Epoch: [21][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.6975e-01 (4.9189e-01)	Acc@1  83.59 ( 84.45)	Acc@5  99.22 ( 98.51)
Epoch: [21][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4079e-01 (4.9023e-01)	Acc@1  82.81 ( 84.50)	Acc@5  98.44 ( 98.46)
Epoch: [21][150/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1240e-01 (4.9300e-01)	Acc@1  80.47 ( 84.36)	Acc@5  98.44 ( 98.44)
Epoch: [21][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7050e-01 (4.9455e-01)	Acc@1  81.25 ( 84.32)	Acc@5  97.66 ( 98.40)
Epoch: [21][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5489e-01 (4.9520e-01)	Acc@1  79.69 ( 84.25)	Acc@5  95.31 ( 98.36)
Epoch: [21][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9565e-01 (4.9573e-01)	Acc@1  82.81 ( 84.20)	Acc@5  99.22 ( 98.35)
Epoch: [21][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7377e-01 (4.9555e-01)	Acc@1  84.38 ( 84.24)	Acc@5  97.66 ( 98.34)
Epoch: [21][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6178e-01 (4.9665e-01)	Acc@1  83.59 ( 84.21)	Acc@5  99.22 ( 98.34)
Epoch: [21][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3744e-01 (4.9674e-01)	Acc@1  88.28 ( 84.19)	Acc@5  98.44 ( 98.33)
Epoch: [21][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8994e-01 (4.9957e-01)	Acc@1  82.81 ( 84.12)	Acc@5  98.44 ( 98.32)
Epoch: [21][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5226e-01 (5.0146e-01)	Acc@1  78.91 ( 84.01)	Acc@5  97.66 ( 98.33)
Epoch: [21][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3341e-01 (5.0339e-01)	Acc@1  85.16 ( 83.99)	Acc@5  97.66 ( 98.31)
Epoch: [21][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8514e-01 (5.0698e-01)	Acc@1  84.38 ( 83.91)	Acc@5  99.22 ( 98.28)
Epoch: [21][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7879e-01 (5.0897e-01)	Acc@1  81.25 ( 83.85)	Acc@5  99.22 ( 98.26)
Epoch: [21][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2653e-01 (5.1069e-01)	Acc@1  82.03 ( 83.81)	Acc@5  96.88 ( 98.24)
Epoch: [21][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7145e-01 (5.1166e-01)	Acc@1  85.94 ( 83.75)	Acc@5  98.44 ( 98.22)
Epoch: [21][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1243e-01 (5.1201e-01)	Acc@1  89.84 ( 83.71)	Acc@5  98.44 ( 98.24)
Epoch: [21][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6109e-01 (5.1314e-01)	Acc@1  84.38 ( 83.67)	Acc@5  96.09 ( 98.23)
Epoch: [21][310/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5734e-01 (5.1518e-01)	Acc@1  83.59 ( 83.59)	Acc@5  96.88 ( 98.22)
Epoch: [21][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9980e-01 (5.1654e-01)	Acc@1  85.94 ( 83.56)	Acc@5  97.66 ( 98.20)
Epoch: [21][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0677e-01 (5.1708e-01)	Acc@1  85.94 ( 83.54)	Acc@5  99.22 ( 98.20)
Epoch: [21][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5839e-01 (5.1855e-01)	Acc@1  84.38 ( 83.47)	Acc@5  99.22 ( 98.19)
Epoch: [21][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3909e-01 (5.1863e-01)	Acc@1  82.81 ( 83.50)	Acc@5  97.66 ( 98.21)
Epoch: [21][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6984e-01 (5.2089e-01)	Acc@1  82.81 ( 83.43)	Acc@5  96.88 ( 98.20)
Epoch: [21][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8216e-01 (5.2205e-01)	Acc@1  78.91 ( 83.40)	Acc@5  96.09 ( 98.18)
Epoch: [21][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5026e-01 (5.2328e-01)	Acc@1  83.59 ( 83.36)	Acc@5  96.88 ( 98.18)
Epoch: [21][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7962e-01 (5.2420e-01)	Acc@1  82.50 ( 83.34)	Acc@5  96.25 ( 98.18)
## e[21] optimizer.zero_grad (sum) time: 0.13800287246704102
## e[21]       loss.backward (sum) time: 2.5299322605133057
## e[21]      optimizer.step (sum) time: 1.0146307945251465
## epoch[21] training(only) time: 18.464258432388306
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.4473e+00 (1.4473e+00)	Acc@1  66.00 ( 66.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5159e+00 (1.5159e+00)	Acc@1  63.00 ( 65.64)	Acc@5  92.00 ( 88.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3184e+00 (1.4382e+00)	Acc@1  72.00 ( 67.48)	Acc@5  90.00 ( 89.10)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8721e+00 (1.4851e+00)	Acc@1  61.00 ( 66.58)	Acc@5  86.00 ( 88.65)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.6535e+00 (1.4790e+00)	Acc@1  59.00 ( 66.07)	Acc@5  90.00 ( 89.22)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5138e+00 (1.4931e+00)	Acc@1  65.00 ( 65.57)	Acc@5  90.00 ( 89.20)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3862e+00 (1.4627e+00)	Acc@1  69.00 ( 65.51)	Acc@5  92.00 ( 89.59)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5951e+00 (1.4654e+00)	Acc@1  64.00 ( 65.70)	Acc@5  89.00 ( 89.51)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.4541e+00 (1.4793e+00)	Acc@1  70.00 ( 65.56)	Acc@5  90.00 ( 89.35)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8286e+00 (1.4605e+00)	Acc@1  63.00 ( 65.85)	Acc@5  85.00 ( 89.58)
 * Acc@1 65.970 Acc@5 89.710
### epoch[21] execution time: 20.965171098709106
EPOCH 22
i:   0, name:          module.conv1.0.weight  changing lr from: 0.057931648642011363   to: 0.054542230279991735
i:   1, name:          module.conv1.1.weight  changing lr from: 0.058781309001313287   to: 0.055439272636615425
i:   2, name:            module.conv1.1.bias  changing lr from: 0.059615188321720615   to: 0.056320617337721549
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.060433428733594724   to: 0.057186344525232725
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.061236184063225577   to: 0.058036550681285839
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.062023618592775678   to: 0.058871347118207643
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.062795905907750776   to: 0.059690858569175144
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.063553227826727163   to: 0.060495221873851784
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.064295773408332646   to: 0.061284584753548532
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.065023738030735787   to: 0.062059104670710546
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.065737322539146922   to: 0.062818947767776676
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.066436732457075973   to: 0.063564287880702064
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.067122177257321114   to: 0.064295305622666707
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.067793869688886538   to: 0.065012187533722218
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.068452025156237495   to: 0.065715125292347604
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.069096861147505975   to: 0.066404314985098023
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.069728596708452398   to: 0.067079956430733764
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.070347451959174731   to: 0.067742252555413107
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.070953647650731783   to: 0.068391408815721130
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.071547404759014374   to: 0.069027632666484498
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.072128944113357782   to: 0.069651133070496418
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.072698486057537856   to: 0.070262120047437412
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.073256250140938159   to: 0.070860804259435348
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.073802454837808340   to: 0.071447396630855795
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.074337317292663838   to: 0.072022108000054930
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.074861053089996132   to: 0.072585148800962049
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.075373876046578608   to: 0.073136728772484630
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.075875998024758973   to: 0.073677056693850412
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.076367628765233148   to: 0.074206340144115118
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.076848975737888792   to: 0.074724785284171175
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.077320244009399255   to: 0.075232596659696754
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.077781636126332099   to: 0.075729977023579254
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.078233352012617038   to: 0.076217127176439359
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.078675588880293856   to: 0.076694245823967888
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.079108541152530285   to: 0.077161529449867858
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.079532400397967773   to: 0.077619172203271594
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.079947355275514734   to: 0.078067365799574143
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.080353591488765641   to: 0.078506299433692320
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.080751291749279530   to: 0.078936159704822359
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.081140635748003107   to: 0.079357130551829358
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.081521800134172054   to: 0.079769393198458105
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.081894958501069454   to: 0.080173126107607948
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.082260281378063238   to: 0.080568504943964378
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.082617936228383940   to: 0.080955702544326588
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.082968087452141681   to: 0.081334888895014668
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.083310896394116443   to: 0.081706231115781042
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.083646521355888001   to: 0.082069893449689582
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.083975117611903072   to: 0.082426037258462481
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.084296837429105501   to: 0.082774821022828543
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.084611830089782125   to: 0.083116400347439168
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.084920241917302616   to: 0.083450927969947478
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.085222216304453771   to: 0.083778553773874734
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.085517893744092605   to: 0.084099424804914308
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.085807411861860486   to: 0.084413685290347762
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.086090905450722358   to: 0.084721476661271167
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.086368506507110587   to: 0.085022937577350854
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.086640344268471142   to: 0.085318203953848162
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.086906545252024903   to: 0.085607408990671846
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.087167233294570948   to: 0.085890683203233884
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.087422529593172787   to: 0.086168154454901447
i:  60, name:               module.fc.weight  changing lr from: 0.087672552746580742   to: 0.086439947990852806
i:  61, name:                 module.fc.bias  changing lr from: 0.087917418797255675   to: 0.086706186473159863



# Switched to train mode...
Epoch: [22][  0/391]	Time  0.192 ( 0.192)	Data  0.151 ( 0.151)	Loss 2.8748e-01 (2.8748e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [22][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.016)	Loss 4.1131e-01 (4.4093e-01)	Acc@1  86.72 ( 86.36)	Acc@5  99.22 ( 99.08)
Epoch: [22][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 6.2459e-01 (4.4488e-01)	Acc@1  79.69 ( 86.27)	Acc@5  99.22 ( 98.92)
Epoch: [22][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.1622e-01 (4.4614e-01)	Acc@1  88.28 ( 86.01)	Acc@5 100.00 ( 98.84)
Epoch: [22][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.9064e-01 (4.4191e-01)	Acc@1  84.38 ( 86.22)	Acc@5  98.44 ( 98.82)
Epoch: [22][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.2786e-01 (4.3979e-01)	Acc@1  85.94 ( 86.15)	Acc@5  98.44 ( 98.79)
Epoch: [22][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.8399e-01 (4.3852e-01)	Acc@1  85.16 ( 86.21)	Acc@5  96.88 ( 98.77)
Epoch: [22][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.0062e-01 (4.4133e-01)	Acc@1  84.38 ( 86.00)	Acc@5  97.66 ( 98.72)
Epoch: [22][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.5926e-01 (4.5176e-01)	Acc@1  78.12 ( 85.67)	Acc@5  97.66 ( 98.63)
Epoch: [22][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7368e-01 (4.5048e-01)	Acc@1  92.97 ( 85.68)	Acc@5  99.22 ( 98.66)
Epoch: [22][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.9794e-01 (4.5037e-01)	Acc@1  89.06 ( 85.75)	Acc@5  97.66 ( 98.65)
Epoch: [22][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4925e-01 (4.5481e-01)	Acc@1  85.16 ( 85.59)	Acc@5 100.00 ( 98.63)
Epoch: [22][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.3916e-01 (4.5654e-01)	Acc@1  84.38 ( 85.56)	Acc@5  98.44 ( 98.65)
Epoch: [22][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5937e-01 (4.5836e-01)	Acc@1  80.47 ( 85.48)	Acc@5  98.44 ( 98.66)
Epoch: [22][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.5401e-01 (4.5928e-01)	Acc@1  85.16 ( 85.44)	Acc@5  96.88 ( 98.65)
Epoch: [22][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 6.3056e-01 (4.6085e-01)	Acc@1  82.81 ( 85.38)	Acc@5  95.31 ( 98.63)
Epoch: [22][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.7809e-01 (4.6304e-01)	Acc@1  78.91 ( 85.25)	Acc@5  96.88 ( 98.63)
Epoch: [22][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6751e-01 (4.6336e-01)	Acc@1  92.19 ( 85.23)	Acc@5  99.22 ( 98.66)
Epoch: [22][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0804e-01 (4.6482e-01)	Acc@1  81.25 ( 85.15)	Acc@5  99.22 ( 98.65)
Epoch: [22][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7416e-01 (4.6631e-01)	Acc@1  85.16 ( 85.07)	Acc@5  98.44 ( 98.64)
Epoch: [22][200/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1779e-01 (4.6800e-01)	Acc@1  78.91 ( 85.02)	Acc@5  97.66 ( 98.62)
Epoch: [22][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9715e-01 (4.6869e-01)	Acc@1  79.69 ( 85.04)	Acc@5  97.66 ( 98.61)
Epoch: [22][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7573e-01 (4.7033e-01)	Acc@1  86.72 ( 85.01)	Acc@5  96.88 ( 98.61)
Epoch: [22][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5794e-01 (4.6908e-01)	Acc@1  80.47 ( 85.06)	Acc@5  99.22 ( 98.60)
Epoch: [22][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7651e-01 (4.6961e-01)	Acc@1  77.34 ( 85.03)	Acc@5  98.44 ( 98.61)
Epoch: [22][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0614e-01 (4.7245e-01)	Acc@1  85.94 ( 84.95)	Acc@5  97.66 ( 98.55)
Epoch: [22][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4540e-01 (4.7453e-01)	Acc@1  85.16 ( 84.87)	Acc@5  99.22 ( 98.54)
Epoch: [22][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0459e-01 (4.7581e-01)	Acc@1  78.91 ( 84.85)	Acc@5  97.66 ( 98.51)
Epoch: [22][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0201e-01 (4.7440e-01)	Acc@1  85.16 ( 84.90)	Acc@5  97.66 ( 98.53)
Epoch: [22][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2986e-01 (4.7591e-01)	Acc@1  83.59 ( 84.84)	Acc@5  97.66 ( 98.52)
Epoch: [22][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9939e-01 (4.7865e-01)	Acc@1  85.16 ( 84.76)	Acc@5  98.44 ( 98.52)
Epoch: [22][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6437e-01 (4.7843e-01)	Acc@1  81.25 ( 84.77)	Acc@5  98.44 ( 98.52)
Epoch: [22][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1405e-01 (4.8087e-01)	Acc@1  82.03 ( 84.71)	Acc@5  98.44 ( 98.50)
Epoch: [22][330/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6380e-01 (4.8338e-01)	Acc@1  81.25 ( 84.62)	Acc@5  96.88 ( 98.48)
Epoch: [22][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3856e-01 (4.8355e-01)	Acc@1  84.38 ( 84.62)	Acc@5  96.88 ( 98.47)
Epoch: [22][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5757e-01 (4.8537e-01)	Acc@1  81.25 ( 84.55)	Acc@5  98.44 ( 98.47)
Epoch: [22][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5812e-01 (4.8701e-01)	Acc@1  83.59 ( 84.49)	Acc@5  98.44 ( 98.47)
Epoch: [22][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9614e-01 (4.8934e-01)	Acc@1  89.06 ( 84.42)	Acc@5  99.22 ( 98.47)
Epoch: [22][380/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9620e-01 (4.9079e-01)	Acc@1  82.81 ( 84.38)	Acc@5  97.66 ( 98.47)
Epoch: [22][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3646e-01 (4.9142e-01)	Acc@1  82.50 ( 84.37)	Acc@5  96.25 ( 98.47)
## e[22] optimizer.zero_grad (sum) time: 0.13611984252929688
## e[22]       loss.backward (sum) time: 2.538719654083252
## e[22]      optimizer.step (sum) time: 1.026367425918579
## epoch[22] training(only) time: 18.473246574401855
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.5053e+00 (1.5053e+00)	Acc@1  70.00 ( 70.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.4610e+00 (1.5065e+00)	Acc@1  63.00 ( 64.00)	Acc@5  89.00 ( 89.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4267e+00 (1.4379e+00)	Acc@1  68.00 ( 65.19)	Acc@5  90.00 ( 89.81)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6341e+00 (1.4524e+00)	Acc@1  59.00 ( 64.94)	Acc@5  89.00 ( 89.58)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3670e+00 (1.4306e+00)	Acc@1  63.00 ( 65.02)	Acc@5  91.00 ( 89.78)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.7390e+00 (1.4440e+00)	Acc@1  64.00 ( 64.65)	Acc@5  81.00 ( 89.43)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.6729e+00 (1.4281e+00)	Acc@1  61.00 ( 64.97)	Acc@5  94.00 ( 89.89)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6450e+00 (1.4355e+00)	Acc@1  61.00 ( 64.73)	Acc@5  88.00 ( 89.76)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.2925e+00 (1.4365e+00)	Acc@1  66.00 ( 64.73)	Acc@5  89.00 ( 89.65)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7994e+00 (1.4193e+00)	Acc@1  60.00 ( 65.11)	Acc@5  87.00 ( 89.82)
 * Acc@1 65.150 Acc@5 89.830
### epoch[22] execution time: 20.98637580871582
EPOCH 23
i:   0, name:          module.conv1.0.weight  changing lr from: 0.054542230279991735   to: 0.051136353678802732
i:   1, name:          module.conv1.1.weight  changing lr from: 0.055439272636615425   to: 0.052077101977653452
i:   2, name:            module.conv1.1.bias  changing lr from: 0.056320617337721549   to: 0.053002487912488919
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.057186344525232725   to: 0.053912516699720851
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.058036550681285839   to: 0.054807215097990226
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.058871347118207643   to: 0.055686629611869209
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.059690858569175144   to: 0.056550824809013904
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.060495221873851784   to: 0.057399881744758185
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.061284584753548532   to: 0.058233896488369269
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.062059104670710546   to: 0.059052978745414687
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.062818947767776676   to: 0.059857250570921977
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.063564287880702064   to: 0.060646845168242203
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.064295305622666707   to: 0.061421905768754094
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.065012187533722218   to: 0.062182584587770623
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.065715125292347604   to: 0.062929041852226392
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.066404314985098023   to: 0.063661444895939279
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.067079956430733764   to: 0.064379967318445794
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.067742252555413107   to: 0.065084788203611257
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.068391408815721130   to: 0.065776091394410735
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.069027632666484498   to: 0.066454064820462552
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.069651133070496418   to: 0.067118899875078991
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.070262120047437412   to: 0.067770790838769579
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.070860804259435348   to: 0.068409934346300924
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.071447396630855795   to: 0.069036528894574442
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.072022108000054930   to: 0.069650774388736278
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.072585148800962049   to: 0.070252871724078972
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.073136728772484630   to: 0.070843022401432004
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.073677056693850412   to: 0.071421428173871412
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.074206340144115118   to: 0.071988290722702991
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.074724785284171175   to: 0.072543811360793256
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.075232596659696754   to: 0.073088190761435370
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.075729977023579254   to: 0.073621628711044537
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.076217127176439359   to: 0.074144323884078808
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.076694245823967888   to: 0.074656473638678333
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.077161529449867858   to: 0.075158273831605851
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.077619172203271594   to: 0.075649918651159095
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.078067365799574143   to: 0.076131600466805699
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.078506299433692320   to: 0.076603509694369451
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.078936159704822359   to: 0.077065834675668077
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.079357130551829358   to: 0.077518761571572878
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.079769393198458105   to: 0.077962474267523299
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.080173126107607948   to: 0.078397154290592297
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.080568504943964378   to: 0.078822980737254383
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.080955702544326588   to: 0.079240130211063048
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.081334888895014668   to: 0.079648776769494833
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.081706231115781042   to: 0.080049091879265483
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.082069893449689582   to: 0.080441244379468091
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.082426037258462481   to: 0.080825400451926654
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.082774821022828543   to: 0.081201723598196496
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.083116400347439168   to: 0.081570374622682476
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.083450927969947478   to: 0.081931511621379377
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.083778553773874734   to: 0.082285289975772641
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.084099424804914308   to: 0.082631862351468688
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.084413685290347762   to: 0.082971378701152565
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.084721476661271167   to: 0.083303986271498667
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.085022937577350854   to: 0.083629829613685133
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.085318203953848162   to: 0.083949050597186936
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.085607408990671846   to: 0.084261788426545178
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.085890683203233884   to: 0.084568179660831022
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.086168154454901447   to: 0.084868358235542285
i:  60, name:               module.fc.weight  changing lr from: 0.086439947990852806   to: 0.085162455486689548
i:  61, name:                 module.fc.bias  changing lr from: 0.086706186473159863   to: 0.085450600176845873



# Switched to train mode...
Epoch: [23][  0/391]	Time  0.194 ( 0.194)	Data  0.153 ( 0.153)	Loss 3.7873e-01 (3.7873e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [23][ 10/391]	Time  0.047 ( 0.059)	Data  0.001 ( 0.016)	Loss 3.0499e-01 (3.9675e-01)	Acc@1  91.41 ( 86.93)	Acc@5  99.22 ( 98.93)
Epoch: [23][ 20/391]	Time  0.045 ( 0.053)	Data  0.001 ( 0.010)	Loss 4.1424e-01 (4.0183e-01)	Acc@1  85.94 ( 86.94)	Acc@5  99.22 ( 99.03)
Epoch: [23][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.4371e-01 (4.1558e-01)	Acc@1  86.72 ( 86.64)	Acc@5  99.22 ( 98.97)
Epoch: [23][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 5.1793e-01 (4.1436e-01)	Acc@1  84.38 ( 86.81)	Acc@5  97.66 ( 99.01)
Epoch: [23][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.7542e-01 (4.1296e-01)	Acc@1  92.19 ( 86.87)	Acc@5  99.22 ( 98.93)
Epoch: [23][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 5.3693e-01 (4.1950e-01)	Acc@1  80.47 ( 86.50)	Acc@5  98.44 ( 98.90)
Epoch: [23][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 3.3014e-01 (4.1792e-01)	Acc@1  89.84 ( 86.43)	Acc@5  99.22 ( 98.88)
Epoch: [23][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4740e-01 (4.2055e-01)	Acc@1  82.81 ( 86.28)	Acc@5 100.00 ( 98.91)
Epoch: [23][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2710e-01 (4.1700e-01)	Acc@1  91.41 ( 86.49)	Acc@5  99.22 ( 98.95)
Epoch: [23][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.7391e-01 (4.1995e-01)	Acc@1  85.16 ( 86.34)	Acc@5  98.44 ( 98.95)
Epoch: [23][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7217e-01 (4.2450e-01)	Acc@1  90.62 ( 86.20)	Acc@5  99.22 ( 98.90)
Epoch: [23][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1019e-01 (4.2943e-01)	Acc@1  83.59 ( 86.10)	Acc@5  97.66 ( 98.90)
Epoch: [23][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1935e-01 (4.3101e-01)	Acc@1  81.25 ( 86.08)	Acc@5 100.00 ( 98.90)
Epoch: [23][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.9097e-01 (4.3051e-01)	Acc@1  90.62 ( 86.09)	Acc@5 100.00 ( 98.89)
Epoch: [23][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 4.1010e-01 (4.2981e-01)	Acc@1  87.50 ( 86.18)	Acc@5  98.44 ( 98.87)
Epoch: [23][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4520e-01 (4.2874e-01)	Acc@1  85.94 ( 86.18)	Acc@5  99.22 ( 98.87)
Epoch: [23][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0565e-01 (4.3080e-01)	Acc@1  85.16 ( 86.12)	Acc@5  98.44 ( 98.86)
Epoch: [23][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5340e-01 (4.3136e-01)	Acc@1  85.94 ( 86.13)	Acc@5 100.00 ( 98.85)
Epoch: [23][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9319e-01 (4.3348e-01)	Acc@1  81.25 ( 86.02)	Acc@5  98.44 ( 98.85)
Epoch: [23][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1410e-01 (4.3487e-01)	Acc@1  84.38 ( 85.97)	Acc@5  99.22 ( 98.81)
Epoch: [23][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8908e-01 (4.3526e-01)	Acc@1  81.25 ( 85.94)	Acc@5 100.00 ( 98.82)
Epoch: [23][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2361e-01 (4.3805e-01)	Acc@1  82.81 ( 85.85)	Acc@5  96.88 ( 98.79)
Epoch: [23][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9326e-01 (4.4222e-01)	Acc@1  81.25 ( 85.75)	Acc@5  97.66 ( 98.76)
Epoch: [23][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8401e-01 (4.4321e-01)	Acc@1  85.94 ( 85.75)	Acc@5  97.66 ( 98.75)
Epoch: [23][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4205e-01 (4.4419e-01)	Acc@1  79.69 ( 85.69)	Acc@5  96.88 ( 98.75)
Epoch: [23][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9598e-01 (4.4398e-01)	Acc@1  80.47 ( 85.68)	Acc@5  96.88 ( 98.75)
Epoch: [23][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8881e-01 (4.4224e-01)	Acc@1  88.28 ( 85.73)	Acc@5  98.44 ( 98.75)
Epoch: [23][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2431e-01 (4.4332e-01)	Acc@1  90.62 ( 85.75)	Acc@5  97.66 ( 98.74)
Epoch: [23][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5384e-01 (4.4440e-01)	Acc@1  89.84 ( 85.72)	Acc@5  99.22 ( 98.74)
Epoch: [23][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3046e-01 (4.4477e-01)	Acc@1  83.59 ( 85.72)	Acc@5  97.66 ( 98.74)
Epoch: [23][310/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6498e-01 (4.4483e-01)	Acc@1  83.59 ( 85.70)	Acc@5  97.66 ( 98.74)
Epoch: [23][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8258e-01 (4.4636e-01)	Acc@1  87.50 ( 85.66)	Acc@5  97.66 ( 98.73)
Epoch: [23][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6327e-01 (4.4812e-01)	Acc@1  84.38 ( 85.59)	Acc@5  98.44 ( 98.73)
Epoch: [23][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8851e-01 (4.5085e-01)	Acc@1  84.38 ( 85.54)	Acc@5  98.44 ( 98.70)
Epoch: [23][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8441e-01 (4.5245e-01)	Acc@1  85.94 ( 85.52)	Acc@5  96.09 ( 98.68)
Epoch: [23][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4563e-01 (4.5503e-01)	Acc@1  79.69 ( 85.41)	Acc@5  96.88 ( 98.66)
Epoch: [23][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3375e-01 (4.5636e-01)	Acc@1  85.94 ( 85.38)	Acc@5 100.00 ( 98.65)
Epoch: [23][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3353e-01 (4.5653e-01)	Acc@1  84.38 ( 85.39)	Acc@5  98.44 ( 98.65)
Epoch: [23][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1169e-01 (4.5742e-01)	Acc@1  78.75 ( 85.35)	Acc@5  97.50 ( 98.64)
## e[23] optimizer.zero_grad (sum) time: 0.13822150230407715
## e[23]       loss.backward (sum) time: 2.5297136306762695
## e[23]      optimizer.step (sum) time: 1.0314087867736816
## epoch[23] training(only) time: 18.424293279647827
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.6282e+00 (1.6282e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.4525e+00 (1.4432e+00)	Acc@1  69.00 ( 66.91)	Acc@5  91.00 ( 89.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4133e+00 (1.3992e+00)	Acc@1  72.00 ( 67.76)	Acc@5  91.00 ( 89.67)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8195e+00 (1.4306e+00)	Acc@1  61.00 ( 66.77)	Acc@5  89.00 ( 89.68)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3734e+00 (1.4086e+00)	Acc@1  64.00 ( 66.51)	Acc@5  93.00 ( 90.17)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.7059e+00 (1.4353e+00)	Acc@1  61.00 ( 65.98)	Acc@5  87.00 ( 89.69)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.1975e+00 (1.4114e+00)	Acc@1  66.00 ( 66.25)	Acc@5  92.00 ( 90.00)
Test: [ 70/100]	Time  0.024 ( 0.025)	Loss 1.4907e+00 (1.4150e+00)	Acc@1  69.00 ( 66.44)	Acc@5  92.00 ( 90.03)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.4307e+00 (1.4270e+00)	Acc@1  67.00 ( 66.31)	Acc@5  89.00 ( 89.91)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7599e+00 (1.4144e+00)	Acc@1  62.00 ( 66.38)	Acc@5  87.00 ( 90.03)
 * Acc@1 66.540 Acc@5 90.080
### epoch[23] execution time: 20.927035331726074
EPOCH 24
i:   0, name:          module.conv1.0.weight  changing lr from: 0.051136353678802732   to: 0.047729843538492883
i:   1, name:          module.conv1.1.weight  changing lr from: 0.052077101977653452   to: 0.048710046129169293
i:   2, name:            module.conv1.1.bias  changing lr from: 0.053002487912488919   to: 0.049675491942114479
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.053912516699720851   to: 0.050626098041085761
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.054807215097990226   to: 0.051561808761485056
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.055686629611869209   to: 0.052482593616459072
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.056550824809013904   to: 0.053388445328303835
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.057399881744758185   to: 0.054279377979047284
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.058233896488369269   to: 0.055155425274264348
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.059052978745414687   to: 0.056016638914367305
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.059857250570921977   to: 0.056863087067810993
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.060646845168242203   to: 0.057694852940855385
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.061421905768754094   to: 0.058512033438731563
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.062182584587770623   to: 0.059314737913264762
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.062929041852226392   to: 0.060103086992212465
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.063661444895939279   to: 0.060877211485781284
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.064379967318445794   to: 0.061637251365986502
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.065084788203611257   to: 0.062383354814716990
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.065776091394410735   to: 0.063115677336562023
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.066454064820462552   to: 0.063834380932644527
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.067118899875078991   to: 0.064539633331890042
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.067770790838769579   to: 0.065231607276337311
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.068409934346300924   to: 0.065910479857269708
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.069036528894574442   to: 0.066576431899111402
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.069650774388736278   to: 0.067229647388192634
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.070252871724078972   to: 0.067870312943641584
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.070843022401432004   to: 0.068498617327807321
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.071421428173871412   to: 0.069114750993759438
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.071988290722702991   to: 0.069718905667544781
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.072543811360793256   to: 0.070311273963009999
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.073088190761435370   to: 0.070892049027121568
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.073621628711044537   to: 0.071461424213832198
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.074144323884078808   to: 0.072019592784653072
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.074656473638678333   to: 0.072566747634198042
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.075158273831605851   to: 0.073103081039065862
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.075649918651159095   to: 0.073628784428522040
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.076131600466805699   to: 0.074144048175532704
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.076603509694369451   to: 0.074649061406788425
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.077065834675668077   to: 0.075144011830436994
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.077518761571572878   to: 0.075629085580321595
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.077962474267523299   to: 0.076104467075592708
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.078397154290592297   to: 0.076570338894631926
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.078822980737254383   to: 0.077026881662289728
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.079240130211063048   to: 0.077474273949501224
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.079648776769494833   to: 0.077912692184401833
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.080049091879265483   to: 0.078342310574119028
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.080441244379468091   to: 0.078763301036468114
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.080825400451926654   to: 0.079175833140828683
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.081201723598196496   to: 0.079580074057523681
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.081570374622682476   to: 0.079976188515066660
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.081931511621379377   to: 0.080364338764682686
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.082285289975772641   to: 0.080744684551547277
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.082631862351468688   to: 0.081117383092223355
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.082971378701152565   to: 0.081482589057809363
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.083303986271498667   to: 0.081840454562345039
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.083629829613685133   to: 0.082191129156049036
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.083949050597186936   to: 0.082534759822992770
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.084261788426545178   to: 0.082871490982839691
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.084568179660831022   to: 0.083201464496304667
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.084868358235542285   to: 0.083524819674011042
i:  60, name:               module.fc.weight  changing lr from: 0.085162455486689548   to: 0.083841693288445096
i:  61, name:                 module.fc.bias  changing lr from: 0.085450600176845873   to: 0.084152219588727789



# Switched to train mode...
Epoch: [24][  0/391]	Time  0.194 ( 0.194)	Data  0.152 ( 0.152)	Loss 3.1224e-01 (3.1224e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [24][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 3.7989e-01 (4.2023e-01)	Acc@1  90.62 ( 86.22)	Acc@5  96.88 ( 98.37)
Epoch: [24][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 3.1386e-01 (4.0937e-01)	Acc@1  89.84 ( 86.50)	Acc@5  98.44 ( 98.59)
Epoch: [24][ 30/391]	Time  0.048 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.5423e-01 (4.0510e-01)	Acc@1  85.94 ( 87.05)	Acc@5  98.44 ( 98.74)
Epoch: [24][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.6076e-01 (4.0339e-01)	Acc@1  89.84 ( 87.08)	Acc@5  98.44 ( 98.84)
Epoch: [24][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.8758e-01 (4.0690e-01)	Acc@1  85.16 ( 86.99)	Acc@5  98.44 ( 98.81)
Epoch: [24][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.1033e-01 (4.0456e-01)	Acc@1  88.28 ( 87.08)	Acc@5 100.00 ( 98.89)
Epoch: [24][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.6151e-01 (4.0137e-01)	Acc@1  85.16 ( 87.11)	Acc@5 100.00 ( 98.97)
Epoch: [24][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.7758e-01 (4.0431e-01)	Acc@1  88.28 ( 87.27)	Acc@5  96.88 ( 98.87)
Epoch: [24][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.8735e-01 (4.0738e-01)	Acc@1  85.94 ( 87.05)	Acc@5  99.22 ( 98.88)
Epoch: [24][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0658e-01 (4.0386e-01)	Acc@1  83.59 ( 87.31)	Acc@5  98.44 ( 98.86)
Epoch: [24][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.8991e-01 (4.0321e-01)	Acc@1  87.50 ( 87.27)	Acc@5  97.66 ( 98.85)
Epoch: [24][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.7550e-01 (4.0235e-01)	Acc@1  85.16 ( 87.22)	Acc@5  98.44 ( 98.84)
Epoch: [24][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0286e-01 (4.0192e-01)	Acc@1  85.94 ( 87.20)	Acc@5  96.09 ( 98.84)
Epoch: [24][140/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.1922e-01 (4.0289e-01)	Acc@1  84.38 ( 87.12)	Acc@5  98.44 ( 98.85)
Epoch: [24][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.9574e-01 (4.0509e-01)	Acc@1  82.03 ( 87.04)	Acc@5  99.22 ( 98.87)
Epoch: [24][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4376e-01 (4.0691e-01)	Acc@1  83.59 ( 86.97)	Acc@5  99.22 ( 98.87)
Epoch: [24][170/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2481e-01 (4.0540e-01)	Acc@1  87.50 ( 86.95)	Acc@5 100.00 ( 98.87)
Epoch: [24][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3972e-01 (4.0744e-01)	Acc@1  88.28 ( 86.95)	Acc@5  97.66 ( 98.87)
Epoch: [24][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3901e-01 (4.0840e-01)	Acc@1  91.41 ( 86.93)	Acc@5  99.22 ( 98.88)
Epoch: [24][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0710e-01 (4.0845e-01)	Acc@1  85.94 ( 86.92)	Acc@5 100.00 ( 98.88)
Epoch: [24][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7428e-01 (4.0898e-01)	Acc@1  86.72 ( 86.93)	Acc@5 100.00 ( 98.89)
Epoch: [24][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2234e-01 (4.1034e-01)	Acc@1  89.84 ( 86.89)	Acc@5  99.22 ( 98.89)
Epoch: [24][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1221e-01 (4.1326e-01)	Acc@1  81.25 ( 86.78)	Acc@5  99.22 ( 98.90)
Epoch: [24][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1506e-01 (4.1207e-01)	Acc@1  86.72 ( 86.84)	Acc@5  99.22 ( 98.91)
Epoch: [24][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2313e-01 (4.1305e-01)	Acc@1  86.72 ( 86.81)	Acc@5  97.66 ( 98.89)
Epoch: [24][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9814e-01 (4.1245e-01)	Acc@1  81.25 ( 86.80)	Acc@5  99.22 ( 98.91)
Epoch: [24][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2657e-01 (4.1423e-01)	Acc@1  90.62 ( 86.79)	Acc@5 100.00 ( 98.90)
Epoch: [24][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6251e-01 (4.1631e-01)	Acc@1  81.25 ( 86.71)	Acc@5  96.88 ( 98.88)
Epoch: [24][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6021e-01 (4.1737e-01)	Acc@1  80.47 ( 86.64)	Acc@5  96.88 ( 98.86)
Epoch: [24][300/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1541e-01 (4.1914e-01)	Acc@1  85.94 ( 86.56)	Acc@5  96.88 ( 98.86)
Epoch: [24][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2236e-01 (4.2188e-01)	Acc@1  79.69 ( 86.45)	Acc@5  99.22 ( 98.85)
Epoch: [24][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4142e-01 (4.2419e-01)	Acc@1  89.06 ( 86.40)	Acc@5  98.44 ( 98.83)
Epoch: [24][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3738e-01 (4.2526e-01)	Acc@1  81.25 ( 86.37)	Acc@5  98.44 ( 98.83)
Epoch: [24][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7027e-01 (4.2692e-01)	Acc@1  84.38 ( 86.29)	Acc@5  99.22 ( 98.83)
Epoch: [24][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7277e-01 (4.2771e-01)	Acc@1  83.59 ( 86.25)	Acc@5  98.44 ( 98.84)
Epoch: [24][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3890e-01 (4.2965e-01)	Acc@1  84.38 ( 86.20)	Acc@5  97.66 ( 98.83)
Epoch: [24][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0858e-01 (4.3119e-01)	Acc@1  85.16 ( 86.13)	Acc@5  99.22 ( 98.82)
Epoch: [24][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9147e-01 (4.3308e-01)	Acc@1  87.50 ( 86.10)	Acc@5  99.22 ( 98.80)
Epoch: [24][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6506e-01 (4.3534e-01)	Acc@1  77.50 ( 86.01)	Acc@5  95.00 ( 98.79)
## e[24] optimizer.zero_grad (sum) time: 0.1372976303100586
## e[24]       loss.backward (sum) time: 2.5469748973846436
## e[24]      optimizer.step (sum) time: 1.017604112625122
## epoch[24] training(only) time: 18.446701526641846
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.3763e+00 (1.3763e+00)	Acc@1  69.00 ( 69.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2428e+00 (1.5234e+00)	Acc@1  70.00 ( 66.45)	Acc@5  92.00 ( 88.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5731e+00 (1.4449e+00)	Acc@1  66.00 ( 68.00)	Acc@5  86.00 ( 89.38)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.9427e+00 (1.4689e+00)	Acc@1  56.00 ( 66.94)	Acc@5  88.00 ( 89.32)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.6288e+00 (1.4538e+00)	Acc@1  62.00 ( 66.46)	Acc@5  87.00 ( 89.54)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4234e+00 (1.4676e+00)	Acc@1  67.00 ( 66.06)	Acc@5  89.00 ( 89.14)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.5464e+00 (1.4350e+00)	Acc@1  62.00 ( 66.57)	Acc@5  85.00 ( 89.44)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.5522e+00 (1.4353e+00)	Acc@1  66.00 ( 66.62)	Acc@5  91.00 ( 89.52)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.4304e+00 (1.4498e+00)	Acc@1  73.00 ( 66.59)	Acc@5  90.00 ( 89.40)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.4955e+00 (1.4345e+00)	Acc@1  61.00 ( 66.70)	Acc@5  87.00 ( 89.58)
 * Acc@1 66.840 Acc@5 89.600
### epoch[24] execution time: 20.93618130683899
EPOCH 25
i:   0, name:          module.conv1.0.weight  changing lr from: 0.047729843538492883   to: 0.044338527502719036
i:   1, name:          module.conv1.1.weight  changing lr from: 0.048710046129169293   to: 0.045353376352652471
i:   2, name:            module.conv1.1.bias  changing lr from: 0.049675491942114479   to: 0.046354360581664866
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.050626098041085761   to: 0.047341295763626980
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.051561808761485056   to: 0.048314030956669203
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.052482593616459072   to: 0.049272446306319724
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.053388445328303835   to: 0.050216450784303417
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.054279377979047284   to: 0.051145980056993071
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.055155425274264348   to: 0.052060994477610124
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.056016638914367305   to: 0.052961477196395872
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.056863087067810993   to: 0.053847432383116435
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.057694852940855385   to: 0.054718883556420878
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.058512033438731563   to: 0.055575872014736183
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.059314737913264762   to: 0.056418455363558564
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.060103086992212465   to: 0.057246706134177663
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.060877211485781284   to: 0.058060710489054625
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.061637251365986502   to: 0.058860567009258402
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.062383354814716990   to: 0.059646385559549923
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.063115677336562023   to: 0.060418286226888343
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.063834380932644527   to: 0.061176398328314542
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.064539633331890042   to: 0.061920859484348251
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.065231607276337311   to: 0.062651814754209237
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.065910479857269708   to: 0.063369415829346840
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.066576431899111402   to: 0.064073820281928762
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.067229647388192634   to: 0.064765190865102559
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.067870312943641584   to: 0.065443694862001897
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.068498617327807321   to: 0.066109503480620960
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.069114750993759438   to: 0.066762791291827703
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.069718905667544781   to: 0.067403735707928555
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.070311273963009999   to: 0.068032516499331702
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.070892049027121568   to: 0.068649315346987969
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.071461424213832198   to: 0.069254315428411761
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.072019592784653072   to: 0.069847701035204532
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.072566747634198042   to: 0.070429657220117528
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.073103081039065862   to: 0.071000369471798483
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.073628784428522040   to: 0.071560023415471533
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.074144048175532704   to: 0.072108804537897683
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.074649061406788425   to: 0.072646897935057825
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.075144011830436994   to: 0.073174488081088659
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.075629085580321595   to: 0.073691758617087641
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.076104467075592708   to: 0.074198892158482549
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.076570338894631926   to: 0.074696070119738708
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.077026881662289728   to: 0.075183472555248293
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.077474273949501224   to: 0.075661278015314776
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.077912692184401833   to: 0.076129663416211218
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.078342310574119028   to: 0.076588803923351170
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.078763301036468114   to: 0.077038872846670062
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.079175833140828683   to: 0.077480041547369038
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.079580074057523681   to: 0.077912479355225162
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.079976188515066660   to: 0.078336353495721503
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.080364338764682686   to: 0.078751829026294837
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.080744684551547277   to: 0.079159068781044672
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.081117383092223355   to: 0.079558233323286473
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.081482589057809363   to: 0.079949480905371240
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.081840454562345039   to: 0.080332967435230579
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.082191129156049036   to: 0.080708846449139351
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.082534759822992770   to: 0.081077269090222157
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.082871490982839691   to: 0.081438384092258495
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.083201464496304667   to: 0.081792337768371595
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.083524819674011042   to: 0.082139274004211615
i:  60, name:               module.fc.weight  changing lr from: 0.083841693288445096   to: 0.082479334255270126
i:  61, name:                 module.fc.bias  changing lr from: 0.084152219588727789   to: 0.082812657547986224



# Switched to train mode...
Epoch: [25][  0/391]	Time  0.202 ( 0.202)	Data  0.158 ( 0.158)	Loss 4.4059e-01 (4.4059e-01)	Acc@1  87.50 ( 87.50)	Acc@5  98.44 ( 98.44)
Epoch: [25][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 3.0051e-01 (3.8117e-01)	Acc@1  89.84 ( 87.50)	Acc@5  99.22 ( 99.01)
Epoch: [25][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 3.6024e-01 (3.6746e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.26)
Epoch: [25][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.5658e-01 (3.5333e-01)	Acc@1  91.41 ( 88.73)	Acc@5 100.00 ( 99.22)
Epoch: [25][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.2882e-01 (3.5794e-01)	Acc@1  89.84 ( 88.40)	Acc@5  99.22 ( 99.24)
Epoch: [25][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.1066e-01 (3.6279e-01)	Acc@1  86.72 ( 88.28)	Acc@5  99.22 ( 99.17)
Epoch: [25][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.9044e-01 (3.6133e-01)	Acc@1  86.72 ( 88.26)	Acc@5 100.00 ( 99.22)
Epoch: [25][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.9602e-01 (3.5796e-01)	Acc@1  89.84 ( 88.40)	Acc@5 100.00 ( 99.24)
Epoch: [25][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5214e-01 (3.5700e-01)	Acc@1  90.62 ( 88.42)	Acc@5  98.44 ( 99.28)
Epoch: [25][ 90/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6899e-01 (3.6013e-01)	Acc@1  87.50 ( 88.35)	Acc@5 100.00 ( 99.25)
Epoch: [25][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1743e-01 (3.5693e-01)	Acc@1  89.06 ( 88.53)	Acc@5 100.00 ( 99.27)
Epoch: [25][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3973e-01 (3.5673e-01)	Acc@1  89.84 ( 88.51)	Acc@5 100.00 ( 99.28)
Epoch: [25][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8951e-01 (3.5906e-01)	Acc@1  91.41 ( 88.35)	Acc@5 100.00 ( 99.26)
Epoch: [25][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.2737e-01 (3.6217e-01)	Acc@1  88.28 ( 88.23)	Acc@5  99.22 ( 99.23)
Epoch: [25][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 3.0896e-01 (3.6536e-01)	Acc@1  89.84 ( 88.20)	Acc@5  99.22 ( 99.21)
Epoch: [25][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 4.8964e-01 (3.6602e-01)	Acc@1  85.94 ( 88.22)	Acc@5  96.88 ( 99.20)
Epoch: [25][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 4.0641e-01 (3.6617e-01)	Acc@1  85.94 ( 88.19)	Acc@5  99.22 ( 99.21)
Epoch: [25][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5085e-01 (3.6602e-01)	Acc@1  85.94 ( 88.18)	Acc@5  98.44 ( 99.21)
Epoch: [25][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8085e-01 (3.6666e-01)	Acc@1  87.50 ( 88.14)	Acc@5 100.00 ( 99.21)
Epoch: [25][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4420e-01 (3.6908e-01)	Acc@1  89.84 ( 88.08)	Acc@5  98.44 ( 99.21)
Epoch: [25][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4077e-01 (3.6965e-01)	Acc@1  85.16 ( 88.09)	Acc@5 100.00 ( 99.21)
Epoch: [25][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7878e-01 (3.7100e-01)	Acc@1  85.16 ( 88.02)	Acc@5  96.88 ( 99.19)
Epoch: [25][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4534e-01 (3.7137e-01)	Acc@1  92.97 ( 88.01)	Acc@5 100.00 ( 99.20)
Epoch: [25][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7592e-01 (3.7104e-01)	Acc@1  88.28 ( 88.05)	Acc@5  98.44 ( 99.21)
Epoch: [25][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9158e-01 (3.7254e-01)	Acc@1  82.03 ( 87.99)	Acc@5 100.00 ( 99.19)
Epoch: [25][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2102e-01 (3.7314e-01)	Acc@1  92.19 ( 87.98)	Acc@5  99.22 ( 99.17)
Epoch: [25][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9683e-01 (3.7427e-01)	Acc@1  85.16 ( 87.93)	Acc@5 100.00 ( 99.16)
Epoch: [25][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2605e-01 (3.7529e-01)	Acc@1  82.03 ( 87.88)	Acc@5  99.22 ( 99.14)
Epoch: [25][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9989e-01 (3.7563e-01)	Acc@1  84.38 ( 87.88)	Acc@5  99.22 ( 99.15)
Epoch: [25][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6733e-01 (3.7598e-01)	Acc@1  88.28 ( 87.88)	Acc@5  97.66 ( 99.14)
Epoch: [25][300/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6630e-01 (3.7763e-01)	Acc@1  87.50 ( 87.83)	Acc@5 100.00 ( 99.14)
Epoch: [25][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6480e-01 (3.7845e-01)	Acc@1  88.28 ( 87.80)	Acc@5  96.88 ( 99.14)
Epoch: [25][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9884e-01 (3.8014e-01)	Acc@1  86.72 ( 87.77)	Acc@5  97.66 ( 99.11)
Epoch: [25][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6032e-01 (3.8047e-01)	Acc@1  85.94 ( 87.76)	Acc@5  98.44 ( 99.12)
Epoch: [25][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9932e-01 (3.8092e-01)	Acc@1  91.41 ( 87.75)	Acc@5 100.00 ( 99.12)
Epoch: [25][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3532e-01 (3.8220e-01)	Acc@1  88.28 ( 87.70)	Acc@5 100.00 ( 99.12)
Epoch: [25][360/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6164e-01 (3.8364e-01)	Acc@1  83.59 ( 87.66)	Acc@5  99.22 ( 99.10)
Epoch: [25][370/391]	Time  0.052 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4493e-01 (3.8662e-01)	Acc@1  81.25 ( 87.56)	Acc@5  98.44 ( 99.09)
Epoch: [25][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1992e-01 (3.8797e-01)	Acc@1  82.81 ( 87.48)	Acc@5  98.44 ( 99.09)
Epoch: [25][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0232e-01 (3.8973e-01)	Acc@1  78.75 ( 87.43)	Acc@5  97.50 ( 99.08)
## e[25] optimizer.zero_grad (sum) time: 0.13822484016418457
## e[25]       loss.backward (sum) time: 2.545530080795288
## e[25]      optimizer.step (sum) time: 1.0225141048431396
## epoch[25] training(only) time: 18.47238850593567
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.5567e+00 (1.5567e+00)	Acc@1  64.00 ( 64.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5353e+00 (1.5501e+00)	Acc@1  61.00 ( 65.55)	Acc@5  93.00 ( 88.82)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.6753e+00 (1.5313e+00)	Acc@1  67.00 ( 65.90)	Acc@5  91.00 ( 89.52)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.9666e+00 (1.5586e+00)	Acc@1  55.00 ( 65.61)	Acc@5  86.00 ( 89.00)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4135e+00 (1.5274e+00)	Acc@1  68.00 ( 66.02)	Acc@5  93.00 ( 89.41)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.7055e+00 (1.5467e+00)	Acc@1  67.00 ( 65.71)	Acc@5  90.00 ( 89.00)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.5414e+00 (1.5212e+00)	Acc@1  61.00 ( 65.70)	Acc@5  93.00 ( 89.43)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6215e+00 (1.5248e+00)	Acc@1  64.00 ( 65.62)	Acc@5  87.00 ( 89.27)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5619e+00 (1.5230e+00)	Acc@1  68.00 ( 65.93)	Acc@5  85.00 ( 89.17)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.9849e+00 (1.5044e+00)	Acc@1  60.00 ( 66.25)	Acc@5  84.00 ( 89.44)
 * Acc@1 66.550 Acc@5 89.620
### epoch[25] execution time: 20.948598861694336
EPOCH 26
i:   0, name:          module.conv1.0.weight  changing lr from: 0.044338527502719036   to: 0.040978162618878995
i:   1, name:          module.conv1.1.weight  changing lr from: 0.045353376352652471   to: 0.042022316803625753
i:   2, name:            module.conv1.1.bias  changing lr from: 0.046354360581664866   to: 0.043053799019094383
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.047341295763626980   to: 0.044072310094015230
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.048314030956669203   to: 0.045077590978719696
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.049272446306319724   to: 0.046069420046896153
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.050216450784303417   to: 0.047047610541224499
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.051145980056993071   to: 0.048012008157285208
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.052060994477610124   to: 0.048962488760139811
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.052961477196395872   to: 0.049898956228012957
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.053847432383116435   to: 0.050821340417568847
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.054718883556420878   to: 0.051729595245362858
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.055575872014736183   to: 0.052623696880154419
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.056418455363558564   to: 0.053503642040891898
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.057246706134177663   to: 0.054369446395315074
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.058060710489054625   to: 0.055221143054268021
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.058860567009258402   to: 0.056058781156968544
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.059646385559549923   to: 0.056882424542640503
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.060418286226888343   to: 0.057692150504079109
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.061176398328314542   to: 0.058488048618883459
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.061920859484348251   to: 0.059270219654259770
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.062651814754209237   to: 0.060038774541462017
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.063369415829346840   to: 0.060793833416104073
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.064073820281928762   to: 0.061535524720739178
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.064765190865102559   to: 0.062263984366262598
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.065443694862001897   to: 0.062979354948851138
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.066109503480620960   to: 0.063681785019304812
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.066762791291827703   to: 0.064371428401805889
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.067403735707928555   to: 0.065048443559254820
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.068032516499331702   to: 0.065712993002482040
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.068649315346987969   to: 0.066365242740769784
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.069254315428411761   to: 0.067005361771248856
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.069847701035204532   to: 0.067633521604859317
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.070429657220117528   to: 0.068249895826686086
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.071000369471798483   to: 0.068854659688593944
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.071560023415471533   to: 0.069447989732198859
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.072108804537897683   to: 0.070030063440316592
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.072646897935057825   to: 0.070601058915132228
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.073174488081088659   to: 0.071161154581429006
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.073691758617087641   to: 0.071710528913308411
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.074198892158482549   to: 0.072249360182919720
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.074696070119738708   to: 0.072777826229801307
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.075183472555248293   to: 0.073296104249515215
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.075661278015314776   to: 0.073804370600331390
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.076129663416211218   to: 0.074302800626789650
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.076588803923351170   to: 0.074791568499035982
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.077038872846670062   to: 0.075270847066892327
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.077480041547369038   to: 0.075740807727682477
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.077912479355225162   to: 0.076201620306891871
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.078336353495721503   to: 0.076653452950795578
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.078751829026294837   to: 0.077096472030239768
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.079159068781044672   to: 0.077530842054810312
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.079558233323286473   to: 0.077956725596669954
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.079949480905371240   to: 0.078374283223386756
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.080332967435230579   to: 0.078783673439120697
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.080708846449139351   to: 0.079185052633571243
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.081077269090222157   to: 0.079578575038127625
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.081438384092258495   to: 0.079964392688697275
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.081792337768371595   to: 0.080342655394720797
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.082139274004211615   to: 0.080713510713911973
i:  60, name:               module.fc.weight  changing lr from: 0.082479334255270126   to: 0.081077103932291728
i:  61, name:                 module.fc.bias  changing lr from: 0.082812657547986224   to: 0.081433578049110888



# Switched to train mode...
Epoch: [26][  0/391]	Time  0.193 ( 0.193)	Data  0.152 ( 0.152)	Loss 2.6245e-01 (2.6245e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [26][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 4.0742e-01 (3.3294e-01)	Acc@1  88.28 ( 89.13)	Acc@5  99.22 ( 99.15)
Epoch: [26][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 2.9697e-01 (3.1706e-01)	Acc@1  92.97 ( 89.92)	Acc@5 100.00 ( 99.48)
Epoch: [26][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.6014e-01 (3.1997e-01)	Acc@1  92.97 ( 89.84)	Acc@5 100.00 ( 99.40)
Epoch: [26][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.4422e-01 (3.1968e-01)	Acc@1  88.28 ( 89.84)	Acc@5 100.00 ( 99.45)
Epoch: [26][ 50/391]	Time  0.044 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.3877e-01 (3.1564e-01)	Acc@1  87.50 ( 89.97)	Acc@5 100.00 ( 99.48)
Epoch: [26][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.9450e-01 (3.1885e-01)	Acc@1  89.84 ( 89.89)	Acc@5  99.22 ( 99.47)
Epoch: [26][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.7966e-01 (3.1610e-01)	Acc@1  89.06 ( 90.01)	Acc@5 100.00 ( 99.46)
Epoch: [26][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6151e-01 (3.1920e-01)	Acc@1  89.06 ( 89.96)	Acc@5  98.44 ( 99.41)
Epoch: [26][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1283e-01 (3.1893e-01)	Acc@1  92.97 ( 89.94)	Acc@5 100.00 ( 99.42)
Epoch: [26][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3740e-01 (3.1931e-01)	Acc@1  91.41 ( 89.91)	Acc@5 100.00 ( 99.44)
Epoch: [26][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0045e-01 (3.1821e-01)	Acc@1  88.28 ( 89.92)	Acc@5 100.00 ( 99.46)
Epoch: [26][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8124e-01 (3.2018e-01)	Acc@1  90.62 ( 89.83)	Acc@5 100.00 ( 99.45)
Epoch: [26][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9560e-01 (3.1770e-01)	Acc@1  90.62 ( 89.95)	Acc@5  99.22 ( 99.45)
Epoch: [26][140/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.3323e-01 (3.1909e-01)	Acc@1  88.28 ( 89.85)	Acc@5 100.00 ( 99.45)
Epoch: [26][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4202e-01 (3.2065e-01)	Acc@1  87.50 ( 89.77)	Acc@5 100.00 ( 99.43)
Epoch: [26][160/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2013e-01 (3.2188e-01)	Acc@1  92.97 ( 89.72)	Acc@5  99.22 ( 99.42)
Epoch: [26][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4451e-01 (3.2031e-01)	Acc@1  93.75 ( 89.76)	Acc@5 100.00 ( 99.43)
Epoch: [26][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4121e-01 (3.2175e-01)	Acc@1  88.28 ( 89.70)	Acc@5  99.22 ( 99.43)
Epoch: [26][190/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7886e-01 (3.2312e-01)	Acc@1  85.94 ( 89.62)	Acc@5  98.44 ( 99.42)
Epoch: [26][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6226e-01 (3.2523e-01)	Acc@1  85.16 ( 89.55)	Acc@5 100.00 ( 99.41)
Epoch: [26][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5824e-01 (3.2585e-01)	Acc@1  90.62 ( 89.45)	Acc@5 100.00 ( 99.42)
Epoch: [26][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5514e-01 (3.2666e-01)	Acc@1  89.84 ( 89.47)	Acc@5 100.00 ( 99.40)
Epoch: [26][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4872e-01 (3.2916e-01)	Acc@1  87.50 ( 89.42)	Acc@5 100.00 ( 99.39)
Epoch: [26][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7175e-01 (3.3111e-01)	Acc@1  88.28 ( 89.35)	Acc@5 100.00 ( 99.38)
Epoch: [26][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6592e-01 (3.3167e-01)	Acc@1  88.28 ( 89.34)	Acc@5  99.22 ( 99.37)
Epoch: [26][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7011e-01 (3.3344e-01)	Acc@1  85.16 ( 89.25)	Acc@5  99.22 ( 99.35)
Epoch: [26][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9951e-01 (3.3335e-01)	Acc@1  90.62 ( 89.26)	Acc@5 100.00 ( 99.34)
Epoch: [26][280/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7970e-01 (3.3560e-01)	Acc@1  88.28 ( 89.15)	Acc@5  99.22 ( 99.32)
Epoch: [26][290/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4024e-01 (3.3794e-01)	Acc@1  82.81 ( 89.08)	Acc@5  99.22 ( 99.30)
Epoch: [26][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5170e-01 (3.4074e-01)	Acc@1  87.50 ( 89.00)	Acc@5 100.00 ( 99.27)
Epoch: [26][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9910e-01 (3.4353e-01)	Acc@1  89.06 ( 88.93)	Acc@5  99.22 ( 99.26)
Epoch: [26][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0281e-01 (3.4479e-01)	Acc@1  79.69 ( 88.89)	Acc@5  98.44 ( 99.24)
Epoch: [26][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6223e-01 (3.4627e-01)	Acc@1  86.72 ( 88.85)	Acc@5 100.00 ( 99.24)
Epoch: [26][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3771e-01 (3.4666e-01)	Acc@1  90.62 ( 88.85)	Acc@5 100.00 ( 99.25)
Epoch: [26][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9399e-01 (3.4824e-01)	Acc@1  88.28 ( 88.76)	Acc@5  97.66 ( 99.24)
Epoch: [26][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6535e-01 (3.4967e-01)	Acc@1  88.28 ( 88.71)	Acc@5  99.22 ( 99.23)
Epoch: [26][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9555e-01 (3.5145e-01)	Acc@1  82.81 ( 88.63)	Acc@5 100.00 ( 99.23)
Epoch: [26][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1410e-01 (3.5329e-01)	Acc@1  84.38 ( 88.58)	Acc@5  99.22 ( 99.23)
Epoch: [26][390/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5397e-01 (3.5505e-01)	Acc@1  85.00 ( 88.48)	Acc@5  97.50 ( 99.23)
## e[26] optimizer.zero_grad (sum) time: 0.14054632186889648
## e[26]       loss.backward (sum) time: 2.5458531379699707
## e[26]      optimizer.step (sum) time: 1.0251095294952393
## epoch[26] training(only) time: 18.456677436828613
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.5670e+00 (1.5670e+00)	Acc@1  67.00 ( 67.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5576e+00 (1.5288e+00)	Acc@1  63.00 ( 65.64)	Acc@5  90.00 ( 89.55)
Test: [ 20/100]	Time  0.028 ( 0.029)	Loss 1.4886e+00 (1.4595e+00)	Acc@1  75.00 ( 67.67)	Acc@5  92.00 ( 89.62)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.1827e+00 (1.5162e+00)	Acc@1  53.00 ( 66.45)	Acc@5  83.00 ( 89.03)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.5733e+00 (1.5021e+00)	Acc@1  66.00 ( 66.46)	Acc@5  90.00 ( 89.20)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5623e+00 (1.5312e+00)	Acc@1  70.00 ( 65.90)	Acc@5  89.00 ( 88.82)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.5089e+00 (1.5092e+00)	Acc@1  65.00 ( 66.05)	Acc@5  92.00 ( 89.20)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6807e+00 (1.5150e+00)	Acc@1  68.00 ( 66.03)	Acc@5  87.00 ( 89.10)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5976e+00 (1.5274e+00)	Acc@1  65.00 ( 65.86)	Acc@5  88.00 ( 88.98)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.7433e+00 (1.5093e+00)	Acc@1  62.00 ( 66.32)	Acc@5  89.00 ( 89.10)
 * Acc@1 66.370 Acc@5 89.200
### epoch[26] execution time: 20.94138240814209
EPOCH 27
i:   0, name:          module.conv1.0.weight  changing lr from: 0.040978162618878995   to: 0.037664362126255110
i:   1, name:          module.conv1.1.weight  changing lr from: 0.042022316803625753   to: 0.038731975482579624
i:   2, name:            module.conv1.1.bias  changing lr from: 0.043053799019094383   to: 0.039788421364133929
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.044072310094015230   to: 0.040833272883598254
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.045077590978719696   to: 0.041866150264362460
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.046069420046896153   to: 0.042886717850355645
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.047047610541224499   to: 0.043894681265143225
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.048012008157285208   to: 0.044889784715430835
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.048962488760139811   to: 0.045871808433978818
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.049898956228012957   to: 0.046840566256843248
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.050821340417568847   to: 0.047795903329815724
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.051729595245362858   to: 0.048737693938928572
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.052623696880154419   to: 0.049665839459915394
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.053503642040891898   to: 0.050580266421570333
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.054369446395315074   to: 0.051480924678022226
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.055221143054268021   to: 0.052367785685032975
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.056058781156968544   to: 0.053240840875537633
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.056882424542640503   to: 0.054100100129763087
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.057692150504079109   to: 0.054945590335394028
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.058488048618883459   to: 0.055777354033389598
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.059270219654259770   to: 0.056595448145200636
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.060038774541462017   to: 0.057399942777280992
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.060793833416104073   to: 0.058190920098938274
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.061535524720739178   to: 0.058968473289718763
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.062263984366262598   to: 0.059732705552671329
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.062979354948851138   to: 0.060483729189986457
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.063681785019304812   to: 0.061221664737652594
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.064371428401805889   to: 0.061946640155919420
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.065048443559254820   to: 0.062658790072500345
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.065712993002482040   to: 0.063358255075585429
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.066365242740769784   to: 0.064045181053873718
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.067005361771248856   to: 0.064719718580965105
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.067633521604859317   to: 0.065382022341580651
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.068249895826686086   to: 0.066032250597205092
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.068854659688593944   to: 0.066670564688863412
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.069447989732198859   to: 0.067297128574861018
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.070030063440316592   to: 0.067912108401425850
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.070601058915132228   to: 0.068515672104299638
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.071161154581429006   to: 0.069107989039425444
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.071710528913308411   to: 0.069689229640978831
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.072249360182919720   to: 0.070259565105081967
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.072777826229801307   to: 0.070819167097630517
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.073296104249515215   to: 0.071368207484748217
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.073804370600331390   to: 0.071906858084465494
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.074302800626789650   to: 0.072435290438296482
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.074791568499035982   to: 0.072953675601462278
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.075270847066892327   to: 0.073462183950578810
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.075740807727682477   to: 0.073960985007694630
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.076201620306891871   to: 0.074450247279626972
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.076653452950795578   to: 0.074930138111605440
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.077096472030239768   to: 0.075400823554288884
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.077530842054810312   to: 0.075862468243275930
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.077956725596669954   to: 0.076315235290281114
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.078374283223386756   to: 0.076759286185195924
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.078783673439120697   to: 0.077194780708302352
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.079185052633571243   to: 0.077621876851947380
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.079578575038127625   to: 0.078040730751030477
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.079964392688697275   to: 0.078451496621693692
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.080342655394720797   to: 0.078854326707641045
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.080713510713911973   to: 0.079249371233548943
i:  60, name:               module.fc.weight  changing lr from: 0.081077103932291728   to: 0.079636778365061764
i:  61, name:                 module.fc.bias  changing lr from: 0.081433578049110888   to: 0.080016694174898387



# Switched to train mode...
Epoch: [27][  0/391]	Time  0.195 ( 0.195)	Data  0.152 ( 0.152)	Loss 2.7956e-01 (2.7956e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [27][ 10/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.016)	Loss 3.7041e-01 (3.2680e-01)	Acc@1  90.62 ( 89.42)	Acc@5  98.44 ( 99.36)
Epoch: [27][ 20/391]	Time  0.045 ( 0.054)	Data  0.001 ( 0.010)	Loss 3.6762e-01 (3.3165e-01)	Acc@1  89.06 ( 89.51)	Acc@5  98.44 ( 99.37)
Epoch: [27][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.5476e-01 (3.2874e-01)	Acc@1  89.06 ( 89.49)	Acc@5  98.44 ( 99.40)
Epoch: [27][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.1445e-01 (3.1567e-01)	Acc@1  96.09 ( 89.84)	Acc@5  98.44 ( 99.43)
Epoch: [27][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.9200e-01 (3.1957e-01)	Acc@1  81.25 ( 89.64)	Acc@5  99.22 ( 99.45)
Epoch: [27][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.4148e-01 (3.1674e-01)	Acc@1  89.84 ( 89.72)	Acc@5  99.22 ( 99.41)
Epoch: [27][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.8898e-01 (3.1500e-01)	Acc@1  83.59 ( 89.80)	Acc@5  98.44 ( 99.39)
Epoch: [27][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6267e-01 (3.1137e-01)	Acc@1  92.19 ( 90.02)	Acc@5  99.22 ( 99.43)
Epoch: [27][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5078e-01 (3.0898e-01)	Acc@1  92.97 ( 90.07)	Acc@5 100.00 ( 99.43)
Epoch: [27][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4947e-01 (3.0801e-01)	Acc@1  89.06 ( 90.11)	Acc@5  98.44 ( 99.44)
Epoch: [27][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1283e-01 (3.0835e-01)	Acc@1  92.19 ( 90.09)	Acc@5 100.00 ( 99.44)
Epoch: [27][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9576e-01 (3.0773e-01)	Acc@1  91.41 ( 90.17)	Acc@5 100.00 ( 99.45)
Epoch: [27][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4607e-01 (3.0856e-01)	Acc@1  92.97 ( 90.08)	Acc@5 100.00 ( 99.46)
Epoch: [27][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.3062e-01 (3.0759e-01)	Acc@1  86.72 ( 90.08)	Acc@5  98.44 ( 99.47)
Epoch: [27][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5963e-01 (3.0801e-01)	Acc@1  92.97 ( 90.02)	Acc@5 100.00 ( 99.47)
Epoch: [27][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8215e-01 (3.0617e-01)	Acc@1  91.41 ( 90.08)	Acc@5  99.22 ( 99.48)
Epoch: [27][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3092e-01 (3.0495e-01)	Acc@1  94.53 ( 90.17)	Acc@5 100.00 ( 99.47)
Epoch: [27][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4316e-01 (3.0480e-01)	Acc@1  92.97 ( 90.20)	Acc@5  99.22 ( 99.48)
Epoch: [27][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0670e-01 (3.0392e-01)	Acc@1  88.28 ( 90.27)	Acc@5  98.44 ( 99.46)
Epoch: [27][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8235e-01 (3.0455e-01)	Acc@1  86.72 ( 90.26)	Acc@5  99.22 ( 99.46)
Epoch: [27][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5812e-01 (3.0216e-01)	Acc@1  92.19 ( 90.33)	Acc@5  99.22 ( 99.46)
Epoch: [27][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5105e-01 (3.0219e-01)	Acc@1  87.50 ( 90.34)	Acc@5  96.88 ( 99.46)
Epoch: [27][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4179e-01 (3.0210e-01)	Acc@1  89.06 ( 90.37)	Acc@5 100.00 ( 99.45)
Epoch: [27][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5070e-01 (3.0291e-01)	Acc@1  84.38 ( 90.30)	Acc@5  98.44 ( 99.45)
Epoch: [27][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5675e-01 (3.0509e-01)	Acc@1  83.59 ( 90.20)	Acc@5 100.00 ( 99.45)
Epoch: [27][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8766e-01 (3.0708e-01)	Acc@1  88.28 ( 90.12)	Acc@5 100.00 ( 99.44)
Epoch: [27][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3750e-01 (3.0985e-01)	Acc@1  89.84 ( 90.04)	Acc@5  99.22 ( 99.44)
Epoch: [27][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9988e-01 (3.1094e-01)	Acc@1  82.81 ( 89.96)	Acc@5  99.22 ( 99.44)
Epoch: [27][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2613e-01 (3.1218e-01)	Acc@1  85.16 ( 89.91)	Acc@5  97.66 ( 99.43)
Epoch: [27][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6123e-01 (3.1355e-01)	Acc@1  89.06 ( 89.87)	Acc@5 100.00 ( 99.41)
Epoch: [27][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0644e-01 (3.1555e-01)	Acc@1  88.28 ( 89.77)	Acc@5  98.44 ( 99.40)
Epoch: [27][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9749e-01 (3.1684e-01)	Acc@1  88.28 ( 89.71)	Acc@5 100.00 ( 99.41)
Epoch: [27][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0275e-01 (3.1996e-01)	Acc@1  85.16 ( 89.63)	Acc@5  99.22 ( 99.39)
Epoch: [27][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7262e-01 (3.2193e-01)	Acc@1  92.19 ( 89.56)	Acc@5  98.44 ( 99.38)
Epoch: [27][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7904e-01 (3.2394e-01)	Acc@1  91.41 ( 89.50)	Acc@5 100.00 ( 99.39)
Epoch: [27][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7340e-01 (3.2530e-01)	Acc@1  89.84 ( 89.46)	Acc@5  98.44 ( 99.37)
Epoch: [27][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9175e-01 (3.2728e-01)	Acc@1  86.72 ( 89.39)	Acc@5  98.44 ( 99.37)
Epoch: [27][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7475e-01 (3.2841e-01)	Acc@1  82.03 ( 89.32)	Acc@5 100.00 ( 99.37)
Epoch: [27][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.5444e-01 (3.3168e-01)	Acc@1  76.25 ( 89.20)	Acc@5  97.50 ( 99.36)
## e[27] optimizer.zero_grad (sum) time: 0.13722968101501465
## e[27]       loss.backward (sum) time: 2.540095567703247
## e[27]      optimizer.step (sum) time: 1.0277798175811768
## epoch[27] training(only) time: 18.430445432662964
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.4753e+00 (1.4753e+00)	Acc@1  69.00 ( 69.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.4954e+00 (1.4959e+00)	Acc@1  64.00 ( 67.09)	Acc@5  90.00 ( 89.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4941e+00 (1.4355e+00)	Acc@1  69.00 ( 68.43)	Acc@5  88.00 ( 89.52)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.1541e+00 (1.4665e+00)	Acc@1  58.00 ( 68.29)	Acc@5  85.00 ( 89.23)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4676e+00 (1.4551e+00)	Acc@1  67.00 ( 68.05)	Acc@5  92.00 ( 89.54)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4908e+00 (1.4810e+00)	Acc@1  70.00 ( 67.20)	Acc@5  91.00 ( 89.49)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4035e+00 (1.4638e+00)	Acc@1  63.00 ( 67.10)	Acc@5  90.00 ( 89.74)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6828e+00 (1.4758e+00)	Acc@1  62.00 ( 67.00)	Acc@5  90.00 ( 89.66)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.7158e+00 (1.4849e+00)	Acc@1  66.00 ( 66.75)	Acc@5  84.00 ( 89.54)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6448e+00 (1.4699e+00)	Acc@1  61.00 ( 66.88)	Acc@5  85.00 ( 89.58)
 * Acc@1 67.090 Acc@5 89.670
### epoch[27] execution time: 20.940175771713257
EPOCH 28
i:   0, name:          module.conv1.0.weight  changing lr from: 0.037664362126255110   to: 0.034412522912332426
i:   1, name:          module.conv1.1.weight  changing lr from: 0.038731975482579624   to: 0.035497275712727171
i:   2, name:            module.conv1.1.bias  changing lr from: 0.039788421364133929   to: 0.036572685940452841
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.040833272883598254   to: 0.037638186516293741
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.041866150264362460   to: 0.038693264725159937
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.042886717850355645   to: 0.039737458952203097
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.043894681265143225   to: 0.040770355570029129
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.044889784715430835   to: 0.041791585973282877
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.045871808433978818   to: 0.042800823756572091
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.046840566256843248   to: 0.043797782031456345
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.047795903329815724   to: 0.044782210878045972
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.048737693938928572   to: 0.045753894926629124
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.049665839459915394   to: 0.046712651064661284
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.050580266421570333   to: 0.047658326264409821
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.051480924678022226   to: 0.048590795526535419
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.052367785685032975   to: 0.049509959934913035
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.053240840875537633   to: 0.050415744818037928
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.054100100129763087   to: 0.051308098012427118
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.054945590335394028   to: 0.052186988223509284
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.055777354033389598   to: 0.053052403479589783
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.056595448145200636   to: 0.053904349674587654
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.057399942777280992   to: 0.054742849195355006
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.058190920098938274   to: 0.055567939629515298
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.058968473289718763   to: 0.056379672549884276
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.059732705552671329   to: 0.057178112371670510
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.060483729189986457   to: 0.057963335278788219
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.061221664737652594   to: 0.058735428215750167
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.061946640155919420   to: 0.059494487941746325
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.062658790072500345   to: 0.060240620143650017
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.063358255075585429   to: 0.060973938604827055
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.064045181053873718   to: 0.061694564426757907
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.064719718580965105   to: 0.062402625300612781
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.065382022341580651   to: 0.063098254826047029
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.066032250597205092   to: 0.063781591874610702
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.066670564688863412   to: 0.064452779995284995
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.067297128574861018   to: 0.065111966859778755
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.067912108401425850   to: 0.065759303745330427
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.068515672104299638   to: 0.066394945052872134
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.069107989039425444   to: 0.067019047858518449
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.069689229640978831   to: 0.067631771496445239
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.070259565105081967   to: 0.068233277171321882
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.070819167097630517   to: 0.068823727598555234
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.071368207484748217   to: 0.069403286670694286
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.071906858084465494   to: 0.069972119148430698
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.072435290438296482   to: 0.070530390374714189
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.072953675601462278   to: 0.071078266010580676
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.073462183950578810   to: 0.071615911791366504
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.073960985007694630   to: 0.072143493302055370
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.074450247279626972   to: 0.072661175770571698
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.074930138111605440   to: 0.073169123877901782
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.075400823554288884   to: 0.073667501583984413
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.075862468243275930   to: 0.074156471968373397
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.076315235290281114   to: 0.074636197084729974
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.076759286185195924   to: 0.075106837828256778
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.077194780708302352   to: 0.075568553815236222
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.077621876851947380   to: 0.076021503273883129
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.078040730751030477   to: 0.076465842945768692
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.078451496621693692   to: 0.076901727997114477
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.078854326707641045   to: 0.077329311939297887
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.079249371233548943   to: 0.077748746557947029
i:  60, name:               module.fc.weight  changing lr from: 0.079636778365061764   to: 0.078160181850042032
i:  61, name:                 module.fc.bias  changing lr from: 0.080016694174898387   to: 0.078563765968472907



# Switched to train mode...
Epoch: [28][  0/391]	Time  0.191 ( 0.191)	Data  0.146 ( 0.146)	Loss 3.0152e-01 (3.0152e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.045 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.5102e-01 (3.0138e-01)	Acc@1  92.97 ( 90.70)	Acc@5  99.22 ( 99.64)
Epoch: [28][ 20/391]	Time  0.045 ( 0.053)	Data  0.001 ( 0.009)	Loss 3.6437e-01 (3.0550e-01)	Acc@1  86.72 ( 90.10)	Acc@5 100.00 ( 99.70)
Epoch: [28][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.1763e-01 (2.9879e-01)	Acc@1  88.28 ( 90.10)	Acc@5  99.22 ( 99.70)
Epoch: [28][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.7065e-01 (2.9028e-01)	Acc@1  92.97 ( 90.36)	Acc@5  99.22 ( 99.66)
Epoch: [28][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4569e-01 (2.7913e-01)	Acc@1  91.41 ( 90.92)	Acc@5 100.00 ( 99.66)
Epoch: [28][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.5955e-01 (2.7486e-01)	Acc@1  89.84 ( 91.10)	Acc@5 100.00 ( 99.67)
Epoch: [28][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5023e-01 (2.6931e-01)	Acc@1  92.19 ( 91.31)	Acc@5 100.00 ( 99.69)
Epoch: [28][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8698e-01 (2.6948e-01)	Acc@1  91.41 ( 91.37)	Acc@5  99.22 ( 99.69)
Epoch: [28][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6908e-01 (2.6890e-01)	Acc@1  91.41 ( 91.38)	Acc@5 100.00 ( 99.69)
Epoch: [28][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.7089e-01 (2.7174e-01)	Acc@1  87.50 ( 91.21)	Acc@5 100.00 ( 99.68)
Epoch: [28][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0605e-01 (2.7454e-01)	Acc@1  90.62 ( 91.08)	Acc@5  99.22 ( 99.70)
Epoch: [28][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.0147e-01 (2.7575e-01)	Acc@1  87.50 ( 91.01)	Acc@5  99.22 ( 99.68)
Epoch: [28][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4963e-01 (2.7645e-01)	Acc@1  86.72 ( 90.96)	Acc@5  98.44 ( 99.68)
Epoch: [28][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.4412e-01 (2.7541e-01)	Acc@1  92.19 ( 91.01)	Acc@5 100.00 ( 99.67)
Epoch: [28][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8810e-01 (2.7674e-01)	Acc@1  92.97 ( 90.99)	Acc@5 100.00 ( 99.67)
Epoch: [28][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8011e-01 (2.7767e-01)	Acc@1  92.19 ( 91.04)	Acc@5  99.22 ( 99.65)
Epoch: [28][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9258e-01 (2.7497e-01)	Acc@1  86.72 ( 91.15)	Acc@5 100.00 ( 99.66)
Epoch: [28][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1740e-01 (2.7268e-01)	Acc@1  92.97 ( 91.23)	Acc@5  99.22 ( 99.66)
Epoch: [28][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1034e-01 (2.7271e-01)	Acc@1  92.19 ( 91.21)	Acc@5 100.00 ( 99.67)
Epoch: [28][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3287e-01 (2.7492e-01)	Acc@1  91.41 ( 91.11)	Acc@5 100.00 ( 99.67)
Epoch: [28][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7179e-01 (2.7468e-01)	Acc@1  85.16 ( 91.12)	Acc@5 100.00 ( 99.67)
Epoch: [28][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1647e-01 (2.7667e-01)	Acc@1  93.75 ( 91.04)	Acc@5  99.22 ( 99.64)
Epoch: [28][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0006e-01 (2.7785e-01)	Acc@1  93.75 ( 90.99)	Acc@5  99.22 ( 99.65)
Epoch: [28][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9774e-01 (2.8024e-01)	Acc@1  85.94 ( 90.89)	Acc@5 100.00 ( 99.64)
Epoch: [28][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9811e-01 (2.8346e-01)	Acc@1  85.94 ( 90.74)	Acc@5 100.00 ( 99.63)
Epoch: [28][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7512e-01 (2.8534e-01)	Acc@1  89.06 ( 90.68)	Acc@5 100.00 ( 99.64)
Epoch: [28][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6468e-01 (2.8630e-01)	Acc@1  84.38 ( 90.63)	Acc@5  98.44 ( 99.63)
Epoch: [28][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9819e-01 (2.8759e-01)	Acc@1  90.62 ( 90.59)	Acc@5 100.00 ( 99.62)
Epoch: [28][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4248e-01 (2.8870e-01)	Acc@1  89.84 ( 90.57)	Acc@5  98.44 ( 99.61)
Epoch: [28][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0590e-01 (2.8925e-01)	Acc@1  90.62 ( 90.55)	Acc@5 100.00 ( 99.60)
Epoch: [28][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1549e-01 (2.9208e-01)	Acc@1  86.72 ( 90.47)	Acc@5  99.22 ( 99.59)
Epoch: [28][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3051e-01 (2.9358e-01)	Acc@1  91.41 ( 90.41)	Acc@5 100.00 ( 99.59)
Epoch: [28][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7978e-01 (2.9570e-01)	Acc@1  87.50 ( 90.32)	Acc@5  99.22 ( 99.59)
Epoch: [28][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9167e-01 (2.9729e-01)	Acc@1  91.41 ( 90.26)	Acc@5  98.44 ( 99.58)
Epoch: [28][350/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7604e-01 (2.9906e-01)	Acc@1  88.28 ( 90.20)	Acc@5 100.00 ( 99.57)
Epoch: [28][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4507e-01 (3.0052e-01)	Acc@1  87.50 ( 90.18)	Acc@5  98.44 ( 99.57)
Epoch: [28][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0125e-01 (3.0129e-01)	Acc@1  89.84 ( 90.11)	Acc@5  99.22 ( 99.57)
Epoch: [28][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6448e-01 (3.0291e-01)	Acc@1  89.84 ( 90.05)	Acc@5  98.44 ( 99.55)
Epoch: [28][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6611e-01 (3.0534e-01)	Acc@1  86.25 ( 89.96)	Acc@5 100.00 ( 99.54)
## e[28] optimizer.zero_grad (sum) time: 0.13834428787231445
## e[28]       loss.backward (sum) time: 2.5551953315734863
## e[28]      optimizer.step (sum) time: 1.0254483222961426
## epoch[28] training(only) time: 18.4594304561615
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.4587e+00 (1.4587e+00)	Acc@1  68.00 ( 68.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.2457e+00 (1.5409e+00)	Acc@1  73.00 ( 67.45)	Acc@5  94.00 ( 89.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3760e+00 (1.4664e+00)	Acc@1  69.00 ( 68.24)	Acc@5  88.00 ( 89.81)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.0661e+00 (1.5065e+00)	Acc@1  55.00 ( 67.39)	Acc@5  90.00 ( 89.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3135e+00 (1.4555e+00)	Acc@1  64.00 ( 67.95)	Acc@5  95.00 ( 90.61)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.8534e+00 (1.4902e+00)	Acc@1  63.00 ( 67.35)	Acc@5  84.00 ( 90.20)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.6160e+00 (1.4836e+00)	Acc@1  64.00 ( 67.39)	Acc@5  89.00 ( 90.28)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.7072e+00 (1.4853e+00)	Acc@1  68.00 ( 67.32)	Acc@5  87.00 ( 90.32)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6311e+00 (1.4764e+00)	Acc@1  70.00 ( 67.42)	Acc@5  90.00 ( 90.30)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8506e+00 (1.4588e+00)	Acc@1  56.00 ( 67.45)	Acc@5  89.00 ( 90.47)
 * Acc@1 67.650 Acc@5 90.540
### epoch[28] execution time: 20.94165587425232
EPOCH 29
i:   0, name:          module.conv1.0.weight  changing lr from: 0.034412522912332426   to: 0.031237753974350746
i:   1, name:          module.conv1.1.weight  changing lr from: 0.035497275712727171   to: 0.032332888455361704
i:   2, name:            module.conv1.1.bias  changing lr from: 0.036572685940452841   to: 0.033420831267604978
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.037638186516293741   to: 0.034500863376171087
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.038693264725159937   to: 0.035572327526234217
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.039737458952203097   to: 0.036634624733227338
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.040770355570029129   to: 0.037687210921626246
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.041791585973282877   to: 0.038729593710196797
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.042800823756572091   to: 0.039761329341041499
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.043797782031456345   to: 0.040782019749350674
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.044782210878045972   to: 0.041791309770413308
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.045753894926629124   to: 0.042788884480164373
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.046712651064661284   to: 0.043774466665325841
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.047658326264409821   to: 0.044747814419036969
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.048590795526535419   to: 0.045708718857750817
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.049509959934913035   to: 0.046657001955100551
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.050415744818037928   to: 0.047592514488397242
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.051308098012427118   to: 0.048515134093412571
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.052186988223509284   to: 0.049424763423116337
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.053052403479589783   to: 0.050321328406076449
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.053904349674587654   to: 0.051204776600288694
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.054742849195355006   to: 0.052075075638274397
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.055567939629515298   to: 0.052932211759372252
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.056379672549884276   to: 0.053776188425246702
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.057178112371670510   to: 0.054607025014739713
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.057963335278788219   to: 0.055424755594306145
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.058735428215750167   to: 0.056229427760388188
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.059494487941746325   to: 0.057021101550205712
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.060240620143650017   to: 0.057799848417561972
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.060973938604827055   to: 0.058565750270387844
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.061694564426757907   to: 0.059318898566872870
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.062402625300612781   to: 0.060059393467154880
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.063098254826047029   to: 0.060787343037663658
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.063781591874610702   to: 0.061502862505335598
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.064452779995284995   to: 0.062206073559035360
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.065111966859778755   to: 0.062897103695638348
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.065759303745330427   to: 0.063576085608341659
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.066394945052872134   to: 0.064243156614883209
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.067019047858518449   to: 0.064898458123455849
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.067631771496445239   to: 0.065542135134209895
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.068233277171321882   to: 0.066174335774337095
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.068823727598555234   to: 0.066795210864828689
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.069403286670694286   to: 0.067404913517093670
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.069972119148430698   to: 0.068003598757714320
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.070530390374714189   to: 0.068591423179704222
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.071078266010580676   to: 0.069168544618716493
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.071615911791366504   to: 0.069735121852731480
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.072143493302055370   to: 0.070291314323829651
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.072661175770571698   to: 0.070837281880728339
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.073169123877901782   to: 0.071373184540832857
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.073667501583984413   to: 0.071899182270617604
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.074156471968373397   to: 0.072415434783218491
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.074636197084729974   to: 0.072922101352178179
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.075106837828256778   to: 0.073419340640343486
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.075568553815236222   to: 0.073907310542970533
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.076021503273883129   to: 0.074386168044144621
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.076465842945768692   to: 0.074856069085672577
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.076901727997114477   to: 0.075317168447652627
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.077329311939297887   to: 0.075769619639971711
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.077748746557947029   to: 0.076213574804022785
i:  60, name:               module.fc.weight  changing lr from: 0.078160181850042032   to: 0.076649184623975630
i:  61, name:                 module.fc.bias  changing lr from: 0.078563765968472907   to: 0.077076598246973105



# Switched to train mode...
Epoch: [29][  0/391]	Time  0.195 ( 0.195)	Data  0.151 ( 0.151)	Loss 3.7984e-01 (3.7984e-01)	Acc@1  85.94 ( 85.94)	Acc@5  99.22 ( 99.22)
Epoch: [29][ 10/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.9157e-01 (2.8029e-01)	Acc@1  93.75 ( 91.48)	Acc@5 100.00 ( 99.72)
Epoch: [29][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 2.8154e-01 (2.8123e-01)	Acc@1  91.41 ( 91.44)	Acc@5 100.00 ( 99.67)
Epoch: [29][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.1412e-01 (2.7687e-01)	Acc@1  94.53 ( 91.41)	Acc@5  98.44 ( 99.62)
Epoch: [29][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.2539e-01 (2.6676e-01)	Acc@1  92.97 ( 91.60)	Acc@5 100.00 ( 99.64)
Epoch: [29][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7662e-01 (2.5966e-01)	Acc@1  93.75 ( 91.84)	Acc@5 100.00 ( 99.65)
Epoch: [29][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.8650e-01 (2.5507e-01)	Acc@1  94.53 ( 91.96)	Acc@5 100.00 ( 99.68)
Epoch: [29][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4056e-01 (2.5425e-01)	Acc@1  91.41 ( 92.00)	Acc@5 100.00 ( 99.70)
Epoch: [29][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1280e-01 (2.5636e-01)	Acc@1  93.75 ( 91.88)	Acc@5 100.00 ( 99.70)
Epoch: [29][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.5396e-01 (2.5660e-01)	Acc@1  87.50 ( 91.84)	Acc@5 100.00 ( 99.68)
Epoch: [29][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5520e-01 (2.5605e-01)	Acc@1  92.19 ( 91.85)	Acc@5 100.00 ( 99.68)
Epoch: [29][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6713e-01 (2.5622e-01)	Acc@1  92.97 ( 91.81)	Acc@5  99.22 ( 99.69)
Epoch: [29][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7495e-01 (2.5720e-01)	Acc@1  92.97 ( 91.78)	Acc@5 100.00 ( 99.68)
Epoch: [29][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3379e-01 (2.5869e-01)	Acc@1  92.19 ( 91.69)	Acc@5 100.00 ( 99.68)
Epoch: [29][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3960e-01 (2.5903e-01)	Acc@1  91.41 ( 91.69)	Acc@5  99.22 ( 99.63)
Epoch: [29][150/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.4522e-01 (2.6047e-01)	Acc@1  89.06 ( 91.58)	Acc@5 100.00 ( 99.63)
Epoch: [29][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5589e-01 (2.6251e-01)	Acc@1  92.19 ( 91.51)	Acc@5  99.22 ( 99.63)
Epoch: [29][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9944e-01 (2.6104e-01)	Acc@1  93.75 ( 91.55)	Acc@5  99.22 ( 99.63)
Epoch: [29][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8062e-01 (2.6124e-01)	Acc@1  92.97 ( 91.56)	Acc@5  98.44 ( 99.62)
Epoch: [29][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8906e-01 (2.6103e-01)	Acc@1  88.28 ( 91.56)	Acc@5  99.22 ( 99.62)
Epoch: [29][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8367e-01 (2.6237e-01)	Acc@1  91.41 ( 91.48)	Acc@5 100.00 ( 99.62)
Epoch: [29][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3826e-01 (2.6299e-01)	Acc@1  91.41 ( 91.48)	Acc@5  99.22 ( 99.61)
Epoch: [29][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8881e-01 (2.6444e-01)	Acc@1  88.28 ( 91.43)	Acc@5  99.22 ( 99.61)
Epoch: [29][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8029e-01 (2.6492e-01)	Acc@1  89.06 ( 91.44)	Acc@5  99.22 ( 99.61)
Epoch: [29][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8291e-01 (2.6638e-01)	Acc@1  89.06 ( 91.40)	Acc@5 100.00 ( 99.60)
Epoch: [29][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0441e-01 (2.6788e-01)	Acc@1  93.75 ( 91.33)	Acc@5 100.00 ( 99.60)
Epoch: [29][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7870e-01 (2.6956e-01)	Acc@1  92.97 ( 91.30)	Acc@5 100.00 ( 99.59)
Epoch: [29][270/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2483e-01 (2.6956e-01)	Acc@1  92.97 ( 91.30)	Acc@5 100.00 ( 99.59)
Epoch: [29][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6322e-01 (2.7027e-01)	Acc@1  85.94 ( 91.27)	Acc@5 100.00 ( 99.59)
Epoch: [29][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2685e-01 (2.7070e-01)	Acc@1  92.97 ( 91.24)	Acc@5 100.00 ( 99.59)
Epoch: [29][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9434e-01 (2.7264e-01)	Acc@1  90.62 ( 91.18)	Acc@5  98.44 ( 99.57)
Epoch: [29][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5779e-01 (2.7264e-01)	Acc@1  88.28 ( 91.19)	Acc@5  98.44 ( 99.58)
Epoch: [29][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0333e-01 (2.7425e-01)	Acc@1  90.62 ( 91.10)	Acc@5  99.22 ( 99.58)
Epoch: [29][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4019e-01 (2.7430e-01)	Acc@1  91.41 ( 91.09)	Acc@5 100.00 ( 99.58)
Epoch: [29][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0294e-01 (2.7510e-01)	Acc@1  89.84 ( 91.06)	Acc@5 100.00 ( 99.58)
Epoch: [29][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4586e-01 (2.7552e-01)	Acc@1  92.97 ( 91.06)	Acc@5 100.00 ( 99.57)
Epoch: [29][360/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0655e-01 (2.7623e-01)	Acc@1  89.06 ( 91.04)	Acc@5  99.22 ( 99.57)
Epoch: [29][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5462e-01 (2.7680e-01)	Acc@1  93.75 ( 91.05)	Acc@5 100.00 ( 99.57)
Epoch: [29][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0109e-01 (2.7688e-01)	Acc@1  95.31 ( 91.02)	Acc@5 100.00 ( 99.57)
Epoch: [29][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1394e-01 (2.7792e-01)	Acc@1  93.75 ( 90.98)	Acc@5 100.00 ( 99.57)
## e[29] optimizer.zero_grad (sum) time: 0.1379532814025879
## e[29]       loss.backward (sum) time: 2.551163911819458
## e[29]      optimizer.step (sum) time: 1.0174126625061035
## epoch[29] training(only) time: 18.4468514919281
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.4692e+00 (1.4692e+00)	Acc@1  72.00 ( 72.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.6282e+00 (1.5888e+00)	Acc@1  64.00 ( 67.64)	Acc@5  93.00 ( 89.82)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.4909e+00 (1.5101e+00)	Acc@1  64.00 ( 68.29)	Acc@5  89.00 ( 90.62)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.0822e+00 (1.5339e+00)	Acc@1  55.00 ( 67.55)	Acc@5  85.00 ( 90.26)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4909e+00 (1.5117e+00)	Acc@1  67.00 ( 67.27)	Acc@5  90.00 ( 90.61)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.7296e+00 (1.5368e+00)	Acc@1  64.00 ( 66.45)	Acc@5  89.00 ( 90.31)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.5574e+00 (1.5000e+00)	Acc@1  63.00 ( 66.92)	Acc@5  87.00 ( 90.51)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6734e+00 (1.5099e+00)	Acc@1  63.00 ( 66.87)	Acc@5  93.00 ( 90.42)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.4977e+00 (1.5070e+00)	Acc@1  68.00 ( 66.90)	Acc@5  87.00 ( 90.36)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 2.0163e+00 (1.4948e+00)	Acc@1  67.00 ( 67.22)	Acc@5  84.00 ( 90.44)
 * Acc@1 67.310 Acc@5 90.400
### epoch[29] execution time: 20.92791986465454
EPOCH 30
i:   0, name:          module.conv1.0.weight  changing lr from: 0.031237753974350746   to: 0.028154806218479014
i:   1, name:          module.conv1.1.weight  changing lr from: 0.032332888455361704   to: 0.029253165769800301
i:   2, name:            module.conv1.1.bias  changing lr from: 0.033420831267604978   to: 0.030346813016215324
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.034500863376171087   to: 0.031434866136403880
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.035572327526234217   to: 0.032516512551964229
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.036634624733227338   to: 0.033591005209690598
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.037687210921626246   to: 0.034657659004881484
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.038729593710196797   to: 0.035715847345603506
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.039761329341041499   to: 0.036764998857067313
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.040782019749350674   to: 0.037804594224614421
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.041791309770413308   to: 0.038834163173259856
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.042788884480164373   to: 0.039853281581273138
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.043774466665325841   to: 0.040861568724895124
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.044747814419036969   to: 0.041858684650978417
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.045708718857750817   to: 0.042844327674087891
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.046657001955100551   to: 0.043818231994404905
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.047592514488397242   to: 0.044780165432632302
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.048515134093412571   to: 0.045729927277993569
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.049424763423116337   to: 0.046667346245353319
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.050321328406076449   to: 0.047592278537449642
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.051204776600288694   to: 0.048504606008222949
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.052075075638274397   to: 0.049404234423239435
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.052932211759372252   to: 0.050291091813245210
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.053776188425246702   to: 0.051165126916938343
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.054607025014739713   to: 0.052026307709112957
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.055424755594306145   to: 0.052874620010409074
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.056229427760388188   to: 0.053710066174988573
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.057021101550205712   to: 0.054532663852554425
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.057799848417561972   to: 0.055342444821232344
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.058565750270387844   to: 0.056139453887939367
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.059318898566872870   to: 0.056923747852975076
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.060059393467154880   to: 0.057695394535682065
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.060787343037663658   to: 0.058454471858135987
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.061502862505335598   to: 0.059201066983939891
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.062206073559035360   to: 0.059935275509309362
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.062897103695638348   to: 0.060657200703749831
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.063576085608341659   to: 0.061366952797736754
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.064243156614883209   to: 0.062064648314920423
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.064898458123455849   to: 0.062750409446483343
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.065542135134209895   to: 0.063424363465383965
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.066174335774337095   to: 0.064086642178322481
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.066795210864828689   to: 0.064737381413363812
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.067404913517093670   to: 0.065376720541250016
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.068003598757714320   to: 0.066004802028526599
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.068591423179704222   to: 0.066621771020698925
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.069168544618716493   to: 0.067227774953720953
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.069735121852731480   to: 0.067822963192202657
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.070291314323829651   to: 0.068407486692803665
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.070837281880728339   to: 0.068981497691357085
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.071373184540832857   to: 0.069545149412343132
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.071899182270617604   to: 0.070098595799402030
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.072415434783218491   to: 0.070641991265644757
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.072922101352178179   to: 0.071175490462585242
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.073419340640343486   to: 0.071699248066579346
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.073907310542970533   to: 0.072213418581716191
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.074386168044144621   to: 0.072718156158163433
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.074856069085672577   to: 0.073213614425022347
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.075317168447652627   to: 0.073699946336800479
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.075769619639971711   to: 0.074177304032657751
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.076213574804022785   to: 0.074645838707629100
i:  60, name:               module.fc.weight  changing lr from: 0.076649184623975630   to: 0.075105700495070982
i:  61, name:                 module.fc.bias  changing lr from: 0.077076598246973105   to: 0.075557038359621048



# Switched to train mode...
Epoch: [30][  0/391]	Time  0.197 ( 0.197)	Data  0.154 ( 0.154)	Loss 1.8899e-01 (1.8899e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.2018e-01 (2.1999e-01)	Acc@1  93.75 ( 93.25)	Acc@5  99.22 ( 99.86)
Epoch: [30][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.7256e-01 (2.1164e-01)	Acc@1  96.88 ( 93.42)	Acc@5  99.22 ( 99.85)
Epoch: [30][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.2452e-01 (2.2269e-01)	Acc@1  89.84 ( 92.99)	Acc@5 100.00 ( 99.85)
Epoch: [30][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.9982e-01 (2.3011e-01)	Acc@1  89.84 ( 92.66)	Acc@5  97.66 ( 99.77)
Epoch: [30][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.3674e-01 (2.3197e-01)	Acc@1  90.62 ( 92.60)	Acc@5  99.22 ( 99.79)
Epoch: [30][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.0584e-01 (2.3456e-01)	Acc@1  92.19 ( 92.48)	Acc@5 100.00 ( 99.76)
Epoch: [30][ 70/391]	Time  0.045 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4538e-01 (2.3552e-01)	Acc@1  89.06 ( 92.41)	Acc@5 100.00 ( 99.77)
Epoch: [30][ 80/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0195e-01 (2.3598e-01)	Acc@1  90.62 ( 92.34)	Acc@5 100.00 ( 99.79)
Epoch: [30][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7850e-01 (2.3397e-01)	Acc@1  92.97 ( 92.41)	Acc@5 100.00 ( 99.79)
Epoch: [30][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7593e-01 (2.3568e-01)	Acc@1  90.62 ( 92.26)	Acc@5 100.00 ( 99.80)
Epoch: [30][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0741e-01 (2.3794e-01)	Acc@1  96.09 ( 92.26)	Acc@5  99.22 ( 99.80)
Epoch: [30][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8846e-01 (2.3931e-01)	Acc@1  89.06 ( 92.17)	Acc@5 100.00 ( 99.76)
Epoch: [30][130/391]	Time  0.049 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4483e-01 (2.4190e-01)	Acc@1  90.62 ( 92.04)	Acc@5 100.00 ( 99.76)
Epoch: [30][140/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.0253e-01 (2.4177e-01)	Acc@1  95.31 ( 92.06)	Acc@5 100.00 ( 99.76)
Epoch: [30][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 4.1253e-01 (2.4447e-01)	Acc@1  85.94 ( 91.93)	Acc@5 100.00 ( 99.75)
Epoch: [30][160/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7522e-01 (2.4475e-01)	Acc@1  91.41 ( 91.89)	Acc@5 100.00 ( 99.75)
Epoch: [30][170/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5625e-01 (2.4557e-01)	Acc@1  90.62 ( 91.93)	Acc@5 100.00 ( 99.74)
Epoch: [30][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9487e-01 (2.4700e-01)	Acc@1  92.19 ( 91.82)	Acc@5 100.00 ( 99.73)
Epoch: [30][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9473e-01 (2.4709e-01)	Acc@1  92.97 ( 91.86)	Acc@5 100.00 ( 99.73)
Epoch: [30][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6531e-01 (2.4720e-01)	Acc@1  95.31 ( 91.86)	Acc@5 100.00 ( 99.71)
Epoch: [30][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8550e-01 (2.4761e-01)	Acc@1  89.84 ( 91.89)	Acc@5 100.00 ( 99.71)
Epoch: [30][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4915e-01 (2.4767e-01)	Acc@1  94.53 ( 91.86)	Acc@5 100.00 ( 99.72)
Epoch: [30][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3040e-01 (2.4875e-01)	Acc@1  92.97 ( 91.79)	Acc@5 100.00 ( 99.71)
Epoch: [30][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7998e-01 (2.5061e-01)	Acc@1  92.97 ( 91.76)	Acc@5  98.44 ( 99.69)
Epoch: [30][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2362e-01 (2.5041e-01)	Acc@1  89.84 ( 91.80)	Acc@5  99.22 ( 99.69)
Epoch: [30][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0496e-01 (2.5027e-01)	Acc@1  89.84 ( 91.77)	Acc@5  99.22 ( 99.69)
Epoch: [30][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0931e-01 (2.5142e-01)	Acc@1  88.28 ( 91.73)	Acc@5 100.00 ( 99.69)
Epoch: [30][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9264e-01 (2.5161e-01)	Acc@1  89.06 ( 91.74)	Acc@5 100.00 ( 99.67)
Epoch: [30][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9975e-01 (2.5177e-01)	Acc@1  92.19 ( 91.73)	Acc@5  99.22 ( 99.68)
Epoch: [30][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4543e-01 (2.5206e-01)	Acc@1  95.31 ( 91.74)	Acc@5 100.00 ( 99.68)
Epoch: [30][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8932e-01 (2.5266e-01)	Acc@1  87.50 ( 91.70)	Acc@5  98.44 ( 99.68)
Epoch: [30][320/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9466e-01 (2.5447e-01)	Acc@1  85.94 ( 91.64)	Acc@5  98.44 ( 99.66)
Epoch: [30][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9558e-01 (2.5542e-01)	Acc@1  91.41 ( 91.60)	Acc@5 100.00 ( 99.65)
Epoch: [30][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4905e-01 (2.5584e-01)	Acc@1  91.41 ( 91.59)	Acc@5  99.22 ( 99.64)
Epoch: [30][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8223e-01 (2.5553e-01)	Acc@1  92.97 ( 91.62)	Acc@5 100.00 ( 99.64)
Epoch: [30][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3687e-01 (2.5683e-01)	Acc@1  90.62 ( 91.58)	Acc@5  99.22 ( 99.63)
Epoch: [30][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3391e-01 (2.5775e-01)	Acc@1  89.84 ( 91.55)	Acc@5  99.22 ( 99.63)
Epoch: [30][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0579e-01 (2.5884e-01)	Acc@1  90.62 ( 91.51)	Acc@5  98.44 ( 99.62)
Epoch: [30][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7618e-01 (2.6068e-01)	Acc@1  91.25 ( 91.43)	Acc@5 100.00 ( 99.62)
## e[30] optimizer.zero_grad (sum) time: 0.13770818710327148
## e[30]       loss.backward (sum) time: 2.5539660453796387
## e[30]      optimizer.step (sum) time: 1.0219197273254395
## epoch[30] training(only) time: 18.438907861709595
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.3475e+00 (1.3475e+00)	Acc@1  67.00 ( 67.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.4762e+00 (1.5687e+00)	Acc@1  70.00 ( 67.45)	Acc@5  90.00 ( 89.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.7081e+00 (1.5526e+00)	Acc@1  70.00 ( 68.62)	Acc@5  89.00 ( 89.19)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.0410e+00 (1.6046e+00)	Acc@1  58.00 ( 67.87)	Acc@5  85.00 ( 88.90)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3667e+00 (1.5570e+00)	Acc@1  70.00 ( 67.98)	Acc@5  92.00 ( 89.59)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.7779e+00 (1.5948e+00)	Acc@1  67.00 ( 67.22)	Acc@5  88.00 ( 89.33)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.7503e+00 (1.5751e+00)	Acc@1  61.00 ( 67.25)	Acc@5  90.00 ( 89.70)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.9686e+00 (1.5780e+00)	Acc@1  68.00 ( 67.23)	Acc@5  87.00 ( 89.70)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.8211e+00 (1.5773e+00)	Acc@1  67.00 ( 67.53)	Acc@5  85.00 ( 89.62)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 2.2059e+00 (1.5633e+00)	Acc@1  62.00 ( 67.59)	Acc@5  81.00 ( 89.64)
 * Acc@1 67.740 Acc@5 89.810
### epoch[30] execution time: 20.94110083580017
EPOCH 31
i:   0, name:          module.conv1.0.weight  changing lr from: 0.028154806218479014   to: 0.025178003922785554
i:   1, name:          module.conv1.1.weight  changing lr from: 0.029253165769800301   to: 0.026272075719705325
i:   2, name:            module.conv1.1.bias  changing lr from: 0.030346813016215324   to: 0.027364242215555369
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.031434866136403880   to: 0.028453449127724628
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.032516512551964229   to: 0.029538718797295461
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.033591005209690598   to: 0.030619146312330543
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.034657659004881484   to: 0.031693895758749618
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.035715847345603506   to: 0.032762196601325969
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.036764998857067313   to: 0.033823340196272589
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.037804594224614421   to: 0.034876676435966182
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.038834163173259856   to: 0.035921610525562642
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.039853281581273138   to: 0.036957599890577723
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.040861568724895124   to: 0.037984151213924976
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.041858684650978417   to: 0.039000817600415350
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.042844327674087891   to: 0.040007195866311310
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.043818231994404905   to: 0.041002923951191754
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.044780165432632302   to: 0.041987678449107668
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.045729927277993569   to: 0.042961172255788882
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.046667346245353319   to: 0.043923152328492254
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.047592278537449642   to: 0.044873397554952288
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.048504606008222949   to: 0.045811716727806977
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.049404234423239435   to: 0.046737946620811516
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.050291091813245210   to: 0.047651950163125373
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.051165126916938343   to: 0.048553614707952511
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.052026307709112957   to: 0.049442850391829871
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.052874620010409074   to: 0.050319588580895240
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.053710066174988573   to: 0.051183780400512448
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.054532663852554425   to: 0.052035395344695451
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.055342444821232344   to: 0.052874419961844624
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.056139453887939367   to: 0.053700856613389185
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.056923747852975076   to: 0.054514722302018530
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.057695394535682065   to: 0.055316047566277608
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.058454471858135987   to: 0.056104875438399905
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.059201066983939891   to: 0.056881260462351896
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.059935275509309362   to: 0.057645267769164926
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.060657200703749831   to: 0.058396972206735259
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.061366952797736754   to: 0.059136457521376230
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.062064648314920423   to: 0.059863815588511483
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.062750409446483343   to: 0.060579145690000363
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.063424363465383965   to: 0.061282553835690146
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.064086642178322481   to: 0.061974152126888664
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.064737381413363812   to: 0.062654058159550829
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.065376720541250016   to: 0.063322394465068363
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.066004802028526599   to: 0.063979287986646122
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.066621771020698925   to: 0.064624869589339884
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.067227774953720953   to: 0.065259273601919965
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.067822963192202657   to: 0.065882637388809534
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.068407486692803665   to: 0.066495100950431915
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.068981497691357085   to: 0.067096806550378932
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.069545149412343132   to: 0.067687898367892227
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.070098595799402030   to: 0.068268522174221788
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.070641991265644757   to: 0.068838825031499268
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.071175490462585242   to: 0.069398955012831137
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.071699248066579346   to: 0.069949060942383248
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.072213418581716191   to: 0.070489292154291713
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.072718156158163433   to: 0.071019798269294304
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.073213614425022347   to: 0.071540728988035762
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.073699946336800479   to: 0.072052233900054552
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.074177304032657751   to: 0.072554462307511766
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.074645838707629100   to: 0.073047563062772478
i:  60, name:               module.fc.weight  changing lr from: 0.075105700495070982   to: 0.073531684418998738
i:  61, name:                 module.fc.bias  changing lr from: 0.075557038359621048   to: 0.074006973892958003



# Switched to train mode...
Epoch: [31][  0/391]	Time  0.192 ( 0.192)	Data  0.150 ( 0.150)	Loss 1.6096e-01 (1.6096e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.045 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.8286e-01 (2.0272e-01)	Acc@1  93.75 ( 93.25)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.9766e-01 (2.1165e-01)	Acc@1  95.31 ( 93.08)	Acc@5 100.00 ( 99.74)
Epoch: [31][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 3.0099e-01 (2.1544e-01)	Acc@1  91.41 ( 92.89)	Acc@5 100.00 ( 99.75)
Epoch: [31][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.3963e-01 (2.1292e-01)	Acc@1  90.62 ( 93.03)	Acc@5 100.00 ( 99.77)
Epoch: [31][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.6662e-01 (2.1208e-01)	Acc@1  92.19 ( 93.09)	Acc@5 100.00 ( 99.79)
Epoch: [31][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4733e-01 (2.1148e-01)	Acc@1  91.41 ( 93.14)	Acc@5 100.00 ( 99.78)
Epoch: [31][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.7017e-01 (2.1647e-01)	Acc@1  92.19 ( 93.01)	Acc@5 100.00 ( 99.79)
Epoch: [31][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4017e-01 (2.1650e-01)	Acc@1  96.88 ( 93.04)	Acc@5  99.22 ( 99.79)
Epoch: [31][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4429e-01 (2.2065e-01)	Acc@1  92.97 ( 92.85)	Acc@5 100.00 ( 99.79)
Epoch: [31][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.8103e-01 (2.2167e-01)	Acc@1  89.06 ( 92.87)	Acc@5  99.22 ( 99.79)
Epoch: [31][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7504e-01 (2.2335e-01)	Acc@1  89.84 ( 92.79)	Acc@5 100.00 ( 99.80)
Epoch: [31][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9464e-01 (2.2494e-01)	Acc@1  89.84 ( 92.69)	Acc@5 100.00 ( 99.81)
Epoch: [31][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4190e-01 (2.2587e-01)	Acc@1  92.97 ( 92.72)	Acc@5  99.22 ( 99.81)
Epoch: [31][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7235e-01 (2.2632e-01)	Acc@1  92.97 ( 92.74)	Acc@5  99.22 ( 99.79)
Epoch: [31][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6458e-01 (2.2650e-01)	Acc@1  92.19 ( 92.70)	Acc@5 100.00 ( 99.79)
Epoch: [31][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2295e-01 (2.2600e-01)	Acc@1  85.94 ( 92.71)	Acc@5  98.44 ( 99.78)
Epoch: [31][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7368e-01 (2.2693e-01)	Acc@1  89.84 ( 92.70)	Acc@5 100.00 ( 99.77)
Epoch: [31][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8109e-01 (2.2772e-01)	Acc@1  94.53 ( 92.68)	Acc@5 100.00 ( 99.77)
Epoch: [31][190/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7295e-01 (2.2742e-01)	Acc@1  90.62 ( 92.67)	Acc@5 100.00 ( 99.77)
Epoch: [31][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5706e-01 (2.2790e-01)	Acc@1  90.62 ( 92.64)	Acc@5  99.22 ( 99.76)
Epoch: [31][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4933e-01 (2.2741e-01)	Acc@1  90.62 ( 92.65)	Acc@5  99.22 ( 99.76)
Epoch: [31][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4025e-01 (2.2728e-01)	Acc@1  92.97 ( 92.63)	Acc@5 100.00 ( 99.77)
Epoch: [31][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0471e-01 (2.2802e-01)	Acc@1  94.53 ( 92.63)	Acc@5  99.22 ( 99.76)
Epoch: [31][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3852e-01 (2.2997e-01)	Acc@1  89.84 ( 92.51)	Acc@5 100.00 ( 99.76)
Epoch: [31][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5529e-01 (2.3061e-01)	Acc@1  88.28 ( 92.49)	Acc@5 100.00 ( 99.75)
Epoch: [31][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4862e-01 (2.3021e-01)	Acc@1  91.41 ( 92.47)	Acc@5 100.00 ( 99.76)
Epoch: [31][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4544e-01 (2.3026e-01)	Acc@1  91.41 ( 92.48)	Acc@5 100.00 ( 99.76)
Epoch: [31][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5488e-01 (2.3140e-01)	Acc@1  92.19 ( 92.45)	Acc@5 100.00 ( 99.76)
Epoch: [31][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5333e-01 (2.3163e-01)	Acc@1  90.62 ( 92.43)	Acc@5 100.00 ( 99.76)
Epoch: [31][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8261e-01 (2.3260e-01)	Acc@1  89.84 ( 92.40)	Acc@5  99.22 ( 99.75)
Epoch: [31][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3063e-01 (2.3501e-01)	Acc@1  93.75 ( 92.35)	Acc@5  99.22 ( 99.76)
Epoch: [31][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2925e-01 (2.3632e-01)	Acc@1  91.41 ( 92.31)	Acc@5 100.00 ( 99.74)
Epoch: [31][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3360e-01 (2.3710e-01)	Acc@1  92.97 ( 92.27)	Acc@5  99.22 ( 99.74)
Epoch: [31][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2841e-01 (2.3747e-01)	Acc@1  90.62 ( 92.28)	Acc@5  99.22 ( 99.73)
Epoch: [31][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4186e-01 (2.3899e-01)	Acc@1  85.94 ( 92.24)	Acc@5 100.00 ( 99.73)
Epoch: [31][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9893e-01 (2.3897e-01)	Acc@1  92.19 ( 92.22)	Acc@5 100.00 ( 99.73)
Epoch: [31][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2754e-01 (2.4050e-01)	Acc@1  90.62 ( 92.18)	Acc@5  99.22 ( 99.72)
Epoch: [31][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4712e-01 (2.4186e-01)	Acc@1  85.94 ( 92.12)	Acc@5  99.22 ( 99.72)
Epoch: [31][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5493e-01 (2.4236e-01)	Acc@1  85.00 ( 92.09)	Acc@5 100.00 ( 99.72)
## e[31] optimizer.zero_grad (sum) time: 0.13788771629333496
## e[31]       loss.backward (sum) time: 2.5352768898010254
## e[31]      optimizer.step (sum) time: 1.0373525619506836
## epoch[31] training(only) time: 18.431415796279907
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.7582e+00 (1.7582e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.6176e+00 (1.6755e+00)	Acc@1  66.00 ( 68.45)	Acc@5  91.00 ( 88.27)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.7632e+00 (1.6200e+00)	Acc@1  65.00 ( 68.57)	Acc@5  86.00 ( 88.52)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8817e+00 (1.6281e+00)	Acc@1  62.00 ( 67.94)	Acc@5  89.00 ( 88.42)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5408e+00 (1.6094e+00)	Acc@1  68.00 ( 67.76)	Acc@5  93.00 ( 89.07)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.8264e+00 (1.6172e+00)	Acc@1  63.00 ( 67.29)	Acc@5  86.00 ( 89.08)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3598e+00 (1.5853e+00)	Acc@1  66.00 ( 67.52)	Acc@5  94.00 ( 89.44)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.6576e+00 (1.5907e+00)	Acc@1  66.00 ( 67.42)	Acc@5  91.00 ( 89.45)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.8747e+00 (1.5939e+00)	Acc@1  65.00 ( 67.41)	Acc@5  86.00 ( 89.38)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.9654e+00 (1.5788e+00)	Acc@1  62.00 ( 67.41)	Acc@5  87.00 ( 89.57)
 * Acc@1 67.480 Acc@5 89.750
### epoch[31] execution time: 20.924068689346313
EPOCH 32
i:   0, name:          module.conv1.0.weight  changing lr from: 0.025178003922785554   to: 0.022321178182447728
i:   1, name:          module.conv1.1.weight  changing lr from: 0.026272075719705325   to: 0.023403139021016008
i:   2, name:            module.conv1.1.bias  changing lr from: 0.027364242215555369   to: 0.024486324987109228
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.028453449127724628   to: 0.025569501039846496
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.029538718797295461   to: 0.026651515919394315
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.030619146312330543   to: 0.027731298171493730
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.031693895758749618   to: 0.028807852279518077
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.032762196601325969   to: 0.029880254909766181
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.033823340196272589   to: 0.030947651274304988
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.034876676435966182   to: 0.032009251614450437
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.035921610525562642   to: 0.033064327806903673
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.036957599890577723   to: 0.034112210093628852
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.037984151213924976   to: 0.035152283935746577
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.039000817600415350   to: 0.036183986991022155
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.040007195866311310   to: 0.037206806213926795
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.041002923951191754   to: 0.038220275076743249
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.041987678449107668   to: 0.039223970909756017
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.042961172255788882   to: 0.040217512358207669
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.043923152328492254   to: 0.041200556953407901
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.044873397554952288   to: 0.042172798795139126
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.045811716727806977   to: 0.043133966342313694
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.046737946620811516   to: 0.044083820308687266
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.047651950163125373   to: 0.045022151660324215
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.048553614707952511   to: 0.045948779711433921
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.049442850391829871   to: 0.046863550315147370
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.050319588580895240   to: 0.047766334145783101
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.051183780400512448   to: 0.048657025069147472
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.052035395344695451   to: 0.049535538597433981
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.052874419961844624   to: 0.050401810425318626
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.053700856613389185   to: 0.051255795043894874
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.054514722302018530   to: 0.052097464429150642
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.055316047566277608   to: 0.052926806801756425
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.056104875438399905   to: 0.053743825455008859
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.056881260462351896   to: 0.054548537647856443
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.057645267769164926   to: 0.055340973560017909
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.058396972206735259   to: 0.056121175306296400
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.059136457521376230   to: 0.056889196007282865
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.059863815588511483   to: 0.057645098913737786
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.060579145690000363   to: 0.058388956582035093
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.061282553835690146   to: 0.059120850098148249
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.061974152126888664   to: 0.059840868347753796
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.062654058159550829   to: 0.060549107330122635
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.063322394465068363   to: 0.061245669513563555
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.063979287986646122   to: 0.061930663230275101
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.064624869589339884   to: 0.062604202108553289
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.065259273601919965   to: 0.063266404540391064
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.065882637388809534   to: 0.063917393182591722
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.066495100950431915   to: 0.064557294489603498
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.067096806550378932   to: 0.065186238276363051
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.067687898367892227   to: 0.065804357309516656
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.068268522174221788   to: 0.066411786925462443
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.068838825031499268   to: 0.067008664673732940
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.069398955012831137   to: 0.067595129984307875
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.069949060942383248   to: 0.068171323857515412
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.070489292154291713   to: 0.068737388575247402
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.071019798269294304   to: 0.069293467432276587
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.071540728988035762   to: 0.069839704486525031
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.072052233900054552   to: 0.070376244327192070
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.072554462307511766   to: 0.070903231859705096
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.073047563062772478   to: 0.071420812106510287
i:  60, name:               module.fc.weight  changing lr from: 0.073531684418998738   to: 0.071929130022771931
i:  61, name:                 module.fc.bias  changing lr from: 0.074006973892958003   to: 0.072428330326097842



# Switched to train mode...
Epoch: [32][  0/391]	Time  0.192 ( 0.192)	Data  0.148 ( 0.148)	Loss 1.3573e-01 (1.3573e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.045 ( 0.059)	Data  0.001 ( 0.016)	Loss 1.8844e-01 (2.0978e-01)	Acc@1  93.75 ( 93.11)	Acc@5 100.00 ( 99.86)
Epoch: [32][ 20/391]	Time  0.045 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.8306e-01 (2.1688e-01)	Acc@1  95.31 ( 92.93)	Acc@5  99.22 ( 99.74)
Epoch: [32][ 30/391]	Time  0.048 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.5834e-01 (2.0777e-01)	Acc@1  93.75 ( 93.02)	Acc@5 100.00 ( 99.80)
Epoch: [32][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.0021e-01 (2.0665e-01)	Acc@1  96.88 ( 93.20)	Acc@5 100.00 ( 99.81)
Epoch: [32][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4075e-01 (2.0423e-01)	Acc@1  92.97 ( 93.18)	Acc@5 100.00 ( 99.82)
Epoch: [32][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.8473e-01 (2.0554e-01)	Acc@1  90.62 ( 93.15)	Acc@5  99.22 ( 99.83)
Epoch: [32][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 2.4518e-01 (2.0732e-01)	Acc@1  92.19 ( 93.03)	Acc@5 100.00 ( 99.81)
Epoch: [32][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2666e-01 (2.0612e-01)	Acc@1  91.41 ( 93.04)	Acc@5 100.00 ( 99.80)
Epoch: [32][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0125e-01 (2.0778e-01)	Acc@1  90.62 ( 93.10)	Acc@5 100.00 ( 99.79)
Epoch: [32][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7708e-01 (2.0835e-01)	Acc@1  91.41 ( 93.10)	Acc@5  99.22 ( 99.78)
Epoch: [32][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3721e-01 (2.0557e-01)	Acc@1  94.53 ( 93.16)	Acc@5 100.00 ( 99.80)
Epoch: [32][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9696e-01 (2.0426e-01)	Acc@1  94.53 ( 93.19)	Acc@5 100.00 ( 99.81)
Epoch: [32][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7344e-01 (2.0390e-01)	Acc@1  92.97 ( 93.18)	Acc@5 100.00 ( 99.80)
Epoch: [32][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2871e-01 (2.0221e-01)	Acc@1  96.09 ( 93.29)	Acc@5 100.00 ( 99.80)
Epoch: [32][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.1847e-01 (2.0251e-01)	Acc@1  97.66 ( 93.26)	Acc@5 100.00 ( 99.80)
Epoch: [32][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7883e-01 (2.0140e-01)	Acc@1  95.31 ( 93.30)	Acc@5 100.00 ( 99.81)
Epoch: [32][170/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6156e-01 (2.0083e-01)	Acc@1  92.19 ( 93.32)	Acc@5  99.22 ( 99.80)
Epoch: [32][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1594e-01 (2.0097e-01)	Acc@1  93.75 ( 93.37)	Acc@5 100.00 ( 99.80)
Epoch: [32][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7061e-01 (2.0083e-01)	Acc@1  91.41 ( 93.40)	Acc@5 100.00 ( 99.81)
Epoch: [32][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3058e-01 (2.0121e-01)	Acc@1  91.41 ( 93.38)	Acc@5 100.00 ( 99.82)
Epoch: [32][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9405e-01 (2.0195e-01)	Acc@1  92.19 ( 93.35)	Acc@5  98.44 ( 99.81)
Epoch: [32][220/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6888e-01 (2.0071e-01)	Acc@1  96.88 ( 93.38)	Acc@5 100.00 ( 99.82)
Epoch: [32][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1902e-01 (1.9962e-01)	Acc@1  94.53 ( 93.42)	Acc@5 100.00 ( 99.82)
Epoch: [32][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8841e-01 (2.0095e-01)	Acc@1  93.75 ( 93.39)	Acc@5 100.00 ( 99.82)
Epoch: [32][250/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5186e-01 (2.0224e-01)	Acc@1  94.53 ( 93.36)	Acc@5 100.00 ( 99.82)
Epoch: [32][260/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6706e-01 (2.0253e-01)	Acc@1  90.62 ( 93.34)	Acc@5 100.00 ( 99.82)
Epoch: [32][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7769e-01 (2.0298e-01)	Acc@1  94.53 ( 93.33)	Acc@5 100.00 ( 99.82)
Epoch: [32][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4993e-01 (2.0310e-01)	Acc@1  90.62 ( 93.31)	Acc@5 100.00 ( 99.82)
Epoch: [32][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2307e-01 (2.0290e-01)	Acc@1  92.97 ( 93.34)	Acc@5 100.00 ( 99.82)
Epoch: [32][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8624e-01 (2.0426e-01)	Acc@1  91.41 ( 93.30)	Acc@5 100.00 ( 99.82)
Epoch: [32][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3918e-01 (2.0400e-01)	Acc@1  96.88 ( 93.33)	Acc@5 100.00 ( 99.82)
Epoch: [32][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5560e-01 (2.0453e-01)	Acc@1  95.31 ( 93.31)	Acc@5 100.00 ( 99.82)
Epoch: [32][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6882e-01 (2.0398e-01)	Acc@1  92.19 ( 93.29)	Acc@5 100.00 ( 99.82)
Epoch: [32][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4182e-01 (2.0373e-01)	Acc@1  91.41 ( 93.31)	Acc@5  99.22 ( 99.82)
Epoch: [32][350/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8381e-01 (2.0381e-01)	Acc@1  95.31 ( 93.31)	Acc@5 100.00 ( 99.82)
Epoch: [32][360/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6169e-01 (2.0441e-01)	Acc@1  92.19 ( 93.31)	Acc@5 100.00 ( 99.81)
Epoch: [32][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9841e-01 (2.0464e-01)	Acc@1  92.97 ( 93.30)	Acc@5 100.00 ( 99.81)
Epoch: [32][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2117e-01 (2.0527e-01)	Acc@1  93.75 ( 93.29)	Acc@5 100.00 ( 99.82)
Epoch: [32][390/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1309e-01 (2.0663e-01)	Acc@1  90.00 ( 93.24)	Acc@5 100.00 ( 99.81)
## e[32] optimizer.zero_grad (sum) time: 0.1381244659423828
## e[32]       loss.backward (sum) time: 2.556884288787842
## e[32]      optimizer.step (sum) time: 1.0323317050933838
## epoch[32] training(only) time: 18.4224112033844
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 1.6407e+00 (1.6407e+00)	Acc@1  70.00 ( 70.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.023 ( 0.033)	Loss 1.7381e+00 (1.6196e+00)	Acc@1  64.00 ( 68.55)	Acc@5  90.00 ( 88.73)
Test: [ 20/100]	Time  0.023 ( 0.028)	Loss 1.5783e+00 (1.6010e+00)	Acc@1  70.00 ( 68.57)	Acc@5  89.00 ( 89.52)
Test: [ 30/100]	Time  0.023 ( 0.026)	Loss 2.0783e+00 (1.6072e+00)	Acc@1  55.00 ( 68.06)	Acc@5  87.00 ( 89.26)
Test: [ 40/100]	Time  0.023 ( 0.025)	Loss 1.3145e+00 (1.5664e+00)	Acc@1  71.00 ( 68.24)	Acc@5  93.00 ( 89.76)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.6924e+00 (1.5782e+00)	Acc@1  64.00 ( 67.75)	Acc@5  86.00 ( 89.59)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3250e+00 (1.5452e+00)	Acc@1  66.00 ( 68.25)	Acc@5  94.00 ( 89.95)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.8696e+00 (1.5574e+00)	Acc@1  63.00 ( 68.14)	Acc@5  87.00 ( 89.97)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.7711e+00 (1.5613e+00)	Acc@1  66.00 ( 68.17)	Acc@5  88.00 ( 89.95)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8295e+00 (1.5527e+00)	Acc@1  61.00 ( 68.18)	Acc@5  87.00 ( 90.14)
 * Acc@1 68.350 Acc@5 90.340
### epoch[32] execution time: 20.914479970932007
EPOCH 33
i:   0, name:          module.conv1.0.weight  changing lr from: 0.022321178182447728   to: 0.019597602646433312
i:   1, name:          module.conv1.1.weight  changing lr from: 0.023403139021016008   to: 0.020659367718822798
i:   2, name:            module.conv1.1.bias  changing lr from: 0.024486324987109228   to: 0.021725804070975963
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.025569501039846496   to: 0.022795489203553860
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.026651515919394315   to: 0.023867091179479924
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.027731298171493730   to: 0.024939364621570717
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.028807852279518077   to: 0.026011146790297413
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.029880254909766181   to: 0.027081353751156265
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.030947651274304988   to: 0.028148976639365310
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.032009251614450437   to: 0.029213078028038037
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.033064327806903673   to: 0.030272788404601264
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.034112210093628852   to: 0.031327302759005984
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.035152283935746577   to: 0.032375877286207062
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.036183986991022155   to: 0.033417826204452536
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.037206806213926795   to: 0.034452518690105405
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.038220275076743249   to: 0.035479375929014532
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.039223970909756017   to: 0.036497868283840731
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.040217512358207669   to: 0.037507512576221862
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.041200556953407901   to: 0.038507869482217350
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.042172798795139126   to: 0.039498541039096474
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.043133966342313694   to: 0.040479168261225280
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.044083820308687266   to: 0.041449428862546864
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.045022151660324215   to: 0.042409035082944573
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.045948779711433921   to: 0.043357731615611295
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.046863550315147370   to: 0.044295293632421323
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.047766334145783101   to: 0.045221524904209137
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.048657025069147472   to: 0.046136256012793947
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.049535538597433981   to: 0.047039342651551394
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.050401810425318626   to: 0.047930664011317577
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.051255795043894874   to: 0.048810121248412414
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.052097464429150642   to: 0.049677636031589582
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.052926806801756425   to: 0.050533149164753301
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.053743825455008859   to: 0.051376619282326563
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.054548537647856443   to: 0.052208021614212431
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.055340973560017909   to: 0.053027346817350718
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.056121175306296400   to: 0.053834599870945354
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.056889196007282865   to: 0.054629799032510910
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.057645098913737786   to: 0.055412974851968583
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.058388956582035093   to: 0.056184169241103324
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.059120850098148249   to: 0.056943434595780953
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.059840868347753796   to: 0.057690832968409746
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.060549107330122635   to: 0.058426435288219551
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.061245669513563555   to: 0.059150320627020171
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.061930663230275101   to: 0.059862575508187701
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.062604202108553289   to: 0.060563293256715701
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.063266404540391064   to: 0.061252573388254364
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.063917393182591722   to: 0.061930521035145542
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.064557294489603498   to: 0.062597246407545409
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.065186238276363051   to: 0.063252864287807839
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.065804357309516656   to: 0.063897493556381565
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.066411786925462443   to: 0.064531256747551502
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.067008664673732940   to: 0.065154279633430401
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.067595129984307875   to: 0.065766690834679675
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.068171323857515412   to: 0.066368621456508944
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.068737388575247402   to: 0.066960204748573060
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.069293467432276587   to: 0.067541575787449598
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.069839704486525031   to: 0.068112871180445012
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.070376244327192070   to: 0.068674228789537980
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.070903231859705096   to: 0.069225787474326950
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.071420812106510287   to: 0.069767686852905922
i:  60, name:               module.fc.weight  changing lr from: 0.071929130022771931   to: 0.070300067079645603
i:  61, name:                 module.fc.bias  changing lr from: 0.072428330326097842   to: 0.070823068638910552



# Switched to train mode...
Epoch: [33][  0/391]	Time  0.188 ( 0.188)	Data  0.145 ( 0.145)	Loss 2.5688e-01 (2.5688e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 2.3771e-01 (1.8603e-01)	Acc@1  92.19 ( 94.25)	Acc@5  99.22 ( 99.86)
Epoch: [33][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 1.4309e-01 (1.8262e-01)	Acc@1  95.31 ( 94.27)	Acc@5  99.22 ( 99.78)
Epoch: [33][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 9.9193e-02 (1.7420e-01)	Acc@1  97.66 ( 94.68)	Acc@5 100.00 ( 99.85)
Epoch: [33][ 40/391]	Time  0.044 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.8313e-01 (1.7172e-01)	Acc@1  93.75 ( 94.59)	Acc@5 100.00 ( 99.83)
Epoch: [33][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7403e-01 (1.7374e-01)	Acc@1  92.97 ( 94.55)	Acc@5 100.00 ( 99.83)
Epoch: [33][ 60/391]	Time  0.045 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4091e-01 (1.7120e-01)	Acc@1  96.09 ( 94.56)	Acc@5 100.00 ( 99.86)
Epoch: [33][ 70/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.005)	Loss 1.9329e-01 (1.7449e-01)	Acc@1  93.75 ( 94.41)	Acc@5 100.00 ( 99.85)
Epoch: [33][ 80/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.2755e-01 (1.7478e-01)	Acc@1  89.84 ( 94.38)	Acc@5 100.00 ( 99.84)
Epoch: [33][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6962e-01 (1.7497e-01)	Acc@1  96.09 ( 94.44)	Acc@5 100.00 ( 99.85)
Epoch: [33][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.5287e-01 (1.7420e-01)	Acc@1  94.53 ( 94.46)	Acc@5 100.00 ( 99.86)
Epoch: [33][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9758e-01 (1.7356e-01)	Acc@1  94.53 ( 94.51)	Acc@5 100.00 ( 99.87)
Epoch: [33][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7716e-01 (1.7491e-01)	Acc@1  87.50 ( 94.46)	Acc@5  99.22 ( 99.86)
Epoch: [33][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.8259e-01 (1.7359e-01)	Acc@1  94.53 ( 94.45)	Acc@5 100.00 ( 99.86)
Epoch: [33][140/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.8237e-01 (1.7075e-01)	Acc@1  94.53 ( 94.55)	Acc@5  99.22 ( 99.86)
Epoch: [33][150/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8837e-01 (1.7166e-01)	Acc@1  91.41 ( 94.50)	Acc@5 100.00 ( 99.86)
Epoch: [33][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9047e-01 (1.7240e-01)	Acc@1  93.75 ( 94.47)	Acc@5 100.00 ( 99.86)
Epoch: [33][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6462e-01 (1.7311e-01)	Acc@1  92.97 ( 94.43)	Acc@5 100.00 ( 99.87)
Epoch: [33][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7046e-01 (1.7415e-01)	Acc@1  96.88 ( 94.44)	Acc@5  98.44 ( 99.86)
Epoch: [33][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6314e-01 (1.7396e-01)	Acc@1  94.53 ( 94.44)	Acc@5  99.22 ( 99.86)
Epoch: [33][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9804e-01 (1.7392e-01)	Acc@1  94.53 ( 94.45)	Acc@5 100.00 ( 99.86)
Epoch: [33][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6513e-01 (1.7525e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.86)
Epoch: [33][220/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6509e-01 (1.7660e-01)	Acc@1  85.16 ( 94.34)	Acc@5  99.22 ( 99.86)
Epoch: [33][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1818e-01 (1.7715e-01)	Acc@1  96.88 ( 94.34)	Acc@5 100.00 ( 99.86)
Epoch: [33][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9679e-01 (1.7715e-01)	Acc@1  93.75 ( 94.33)	Acc@5 100.00 ( 99.86)
Epoch: [33][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8343e-01 (1.7837e-01)	Acc@1  92.97 ( 94.29)	Acc@5 100.00 ( 99.86)
Epoch: [33][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9127e-01 (1.7871e-01)	Acc@1  92.97 ( 94.29)	Acc@5 100.00 ( 99.86)
Epoch: [33][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5650e-01 (1.7915e-01)	Acc@1  94.53 ( 94.28)	Acc@5 100.00 ( 99.86)
Epoch: [33][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6262e-01 (1.8039e-01)	Acc@1  94.53 ( 94.25)	Acc@5 100.00 ( 99.86)
Epoch: [33][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1429e-01 (1.8153e-01)	Acc@1  92.97 ( 94.21)	Acc@5  99.22 ( 99.85)
Epoch: [33][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8018e-01 (1.8210e-01)	Acc@1  94.53 ( 94.20)	Acc@5 100.00 ( 99.85)
Epoch: [33][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8145e-01 (1.8219e-01)	Acc@1  92.97 ( 94.18)	Acc@5 100.00 ( 99.85)
Epoch: [33][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9521e-01 (1.8259e-01)	Acc@1  94.53 ( 94.16)	Acc@5 100.00 ( 99.85)
Epoch: [33][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4797e-01 (1.8292e-01)	Acc@1  92.97 ( 94.15)	Acc@5 100.00 ( 99.85)
Epoch: [33][340/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6776e-01 (1.8389e-01)	Acc@1  92.97 ( 94.11)	Acc@5  98.44 ( 99.85)
Epoch: [33][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3490e-01 (1.8487e-01)	Acc@1  89.06 ( 94.07)	Acc@5 100.00 ( 99.84)
Epoch: [33][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0649e-01 (1.8550e-01)	Acc@1  94.53 ( 94.04)	Acc@5 100.00 ( 99.84)
Epoch: [33][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2384e-01 (1.8554e-01)	Acc@1  92.97 ( 94.05)	Acc@5 100.00 ( 99.85)
Epoch: [33][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5315e-01 (1.8590e-01)	Acc@1  94.53 ( 94.02)	Acc@5 100.00 ( 99.85)
Epoch: [33][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6944e-01 (1.8611e-01)	Acc@1  90.00 ( 94.03)	Acc@5 100.00 ( 99.84)
## e[33] optimizer.zero_grad (sum) time: 0.14021563529968262
## e[33]       loss.backward (sum) time: 2.5790202617645264
## e[33]      optimizer.step (sum) time: 1.033435583114624
## epoch[33] training(only) time: 18.453618049621582
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.3441e+00 (1.3441e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.029 ( 0.035)	Loss 1.6769e+00 (1.6113e+00)	Acc@1  68.00 ( 68.45)	Acc@5  92.00 ( 89.73)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.5517e+00 (1.5463e+00)	Acc@1  73.00 ( 69.19)	Acc@5  89.00 ( 90.14)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.0212e+00 (1.5606e+00)	Acc@1  63.00 ( 68.77)	Acc@5  85.00 ( 89.97)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5281e+00 (1.5475e+00)	Acc@1  67.00 ( 68.85)	Acc@5  92.00 ( 90.44)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.7030e+00 (1.5647e+00)	Acc@1  71.00 ( 68.59)	Acc@5  89.00 ( 90.10)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.5346e+00 (1.5496e+00)	Acc@1  65.00 ( 68.59)	Acc@5  93.00 ( 90.33)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.8101e+00 (1.5496e+00)	Acc@1  67.00 ( 68.63)	Acc@5  90.00 ( 90.38)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6573e+00 (1.5535e+00)	Acc@1  65.00 ( 68.42)	Acc@5  86.00 ( 90.23)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.9554e+00 (1.5346e+00)	Acc@1  62.00 ( 68.62)	Acc@5  92.00 ( 90.36)
 * Acc@1 68.620 Acc@5 90.520
### epoch[33] execution time: 20.94622015953064
EPOCH 34
i:   0, name:          module.conv1.0.weight  changing lr from: 0.019597602646433312   to: 0.017019931844240544
i:   1, name:          module.conv1.1.weight  changing lr from: 0.020659367718822798   to: 0.018053206171315402
i:   2, name:            module.conv1.1.bias  changing lr from: 0.021725804070975963   to: 0.019094902404015880
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.022795489203553860   to: 0.020143405694331120
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.023867091179479924   to: 0.021197197998799568
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.024939364621570717   to: 0.022254854132878102
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.026011146790297413   to: 0.023315037870032120
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.027081353751156265   to: 0.024376498099414820
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.028148976639365310   to: 0.025438065053834492
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.029213078028038037   to: 0.026498646617767765
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.030272788404601264   to: 0.027557224723444709
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.031327302759005984   to: 0.028612851841494348
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.032375877286207062   to: 0.029664647571272282
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.033417826204452536   to: 0.030711795334786608
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.034452518690105405   to: 0.031753539177073742
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.035479375929014532   to: 0.032789180674943540
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.036497868283840731   to: 0.033818075955195412
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.037507512576221862   to: 0.034839632822697163
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.038507869482217350   to: 0.035853307998103615
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.039498541039096474   to: 0.036858604464459425
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.040479168261225280   to: 0.037855068921480883
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.041449428862546864   to: 0.038842289345922376
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.042409035082944573   to: 0.039819892656112375
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.043357731615611295   to: 0.040787542478473590
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.044295293632421323   to: 0.041744937013619789
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.045221524904209137   to: 0.042691806999445935
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.046136256012793947   to: 0.043627913768485765
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.047039342651551394   to: 0.044553047396705819
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.047930664011317577   to: 0.045467024940827286
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.048810121248412414   to: 0.046369688761214133
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.049677636031589582   to: 0.047260904927338337
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.050533149164753301   to: 0.048140561702821959
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.051376619282326563   to: 0.049008568107062757
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.052208021614212431   to: 0.049864852550472655
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.053027346817350718   to: 0.050709361540389714
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.053834599870945354   to: 0.051542058454770813
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.054629799032510910   to: 0.052362922380822852
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.055412974851968583   to: 0.053171947015792136
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.056184169241103324   to: 0.053969139627195788
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.056943434595780953   to: 0.054754520069851459
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.057690832968409746   to: 0.055528119857134364
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.058426435288219551   to: 0.056289981283969219
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.059150320627020171   to: 0.057040156599143181
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.059862575508187701   to: 0.057778707224606402
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.060563293256715701   to: 0.058505703019508574
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.061252573388254364   to: 0.059221221586801043
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.061930521035145542   to: 0.059925347620314956
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.062597246407545409   to: 0.060618172290307450
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.063252864287807839   to: 0.061299792665546443
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.063897493556381565   to: 0.061970311170083961
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.064531256747551502   to: 0.062629835072944104
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.065154279633430401   to: 0.063278476009027501
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.065766690834679675   to: 0.063916349529607772
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.066368621456508944   to: 0.064543574680866037
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.066960204748573060   to: 0.065160273608980704
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.067541575787449598   to: 0.065766571190355338
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.068112871180445012   to: 0.066362594685633949
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.068674228789537980   to: 0.066948473416215598
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.069225787474326950   to: 0.067524338462041450
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.069767686852905922   to: 0.068090322379484966
i:  60, name:               module.fc.weight  changing lr from: 0.070300067079645603   to: 0.068646558938234042
i:  61, name:                 module.fc.bias  changing lr from: 0.070823068638910552   to: 0.069193182876106832



# Switched to train mode...
Epoch: [34][  0/391]	Time  0.192 ( 0.192)	Data  0.146 ( 0.146)	Loss 1.2122e-01 (1.2122e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.015)	Loss 1.5748e-01 (1.6989e-01)	Acc@1  96.09 ( 94.96)	Acc@5  99.22 ( 99.93)
Epoch: [34][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 2.0451e-01 (1.6549e-01)	Acc@1  92.97 ( 94.98)	Acc@5  99.22 ( 99.89)
Epoch: [34][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.7840e-01 (1.5797e-01)	Acc@1  96.09 ( 95.29)	Acc@5 100.00 ( 99.90)
Epoch: [34][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 2.2287e-01 (1.5553e-01)	Acc@1  92.97 ( 95.27)	Acc@5  99.22 ( 99.89)
Epoch: [34][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0274e-01 (1.5532e-01)	Acc@1  97.66 ( 95.14)	Acc@5 100.00 ( 99.91)
Epoch: [34][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.2662e-01 (1.5516e-01)	Acc@1  96.09 ( 95.13)	Acc@5 100.00 ( 99.91)
Epoch: [34][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 2.5924e-01 (1.5808e-01)	Acc@1  93.75 ( 95.07)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0617e-01 (1.5900e-01)	Acc@1  94.53 ( 94.99)	Acc@5 100.00 ( 99.90)
Epoch: [34][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5532e-01 (1.6065e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.91)
Epoch: [34][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9154e-01 (1.6224e-01)	Acc@1  87.50 ( 94.85)	Acc@5 100.00 ( 99.89)
Epoch: [34][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3777e-01 (1.6074e-01)	Acc@1  94.53 ( 94.93)	Acc@5 100.00 ( 99.89)
Epoch: [34][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.0104e-01 (1.6036e-01)	Acc@1  97.66 ( 94.90)	Acc@5 100.00 ( 99.89)
Epoch: [34][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6045e-01 (1.6042e-01)	Acc@1  94.53 ( 94.92)	Acc@5 100.00 ( 99.90)
Epoch: [34][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.9769e-02 (1.6027e-01)	Acc@1  97.66 ( 94.91)	Acc@5 100.00 ( 99.89)
Epoch: [34][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 1.6089e-01 (1.6190e-01)	Acc@1  92.97 ( 94.86)	Acc@5 100.00 ( 99.88)
Epoch: [34][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3745e-01 (1.6221e-01)	Acc@1  96.88 ( 94.89)	Acc@5 100.00 ( 99.88)
Epoch: [34][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1833e-01 (1.6329e-01)	Acc@1  89.06 ( 94.86)	Acc@5  99.22 ( 99.87)
Epoch: [34][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3437e-01 (1.6262e-01)	Acc@1  96.09 ( 94.94)	Acc@5 100.00 ( 99.87)
Epoch: [34][190/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6439e-01 (1.6203e-01)	Acc@1  92.97 ( 94.93)	Acc@5 100.00 ( 99.87)
Epoch: [34][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7964e-01 (1.6175e-01)	Acc@1  91.41 ( 94.94)	Acc@5 100.00 ( 99.88)
Epoch: [34][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2492e-01 (1.6125e-01)	Acc@1  96.09 ( 94.96)	Acc@5 100.00 ( 99.88)
Epoch: [34][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4123e-01 (1.6115e-01)	Acc@1  94.53 ( 94.96)	Acc@5 100.00 ( 99.87)
Epoch: [34][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1441e-01 (1.6172e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.87)
Epoch: [34][240/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6713e-01 (1.6236e-01)	Acc@1  93.75 ( 94.93)	Acc@5 100.00 ( 99.87)
Epoch: [34][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1500e-01 (1.6348e-01)	Acc@1  94.53 ( 94.89)	Acc@5 100.00 ( 99.88)
Epoch: [34][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9123e-01 (1.6305e-01)	Acc@1  92.97 ( 94.89)	Acc@5  99.22 ( 99.87)
Epoch: [34][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8354e-01 (1.6444e-01)	Acc@1  93.75 ( 94.82)	Acc@5  99.22 ( 99.87)
Epoch: [34][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0758e-01 (1.6500e-01)	Acc@1  93.75 ( 94.81)	Acc@5 100.00 ( 99.87)
Epoch: [34][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6351e-01 (1.6519e-01)	Acc@1  93.75 ( 94.81)	Acc@5 100.00 ( 99.87)
Epoch: [34][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0598e-01 (1.6574e-01)	Acc@1  93.75 ( 94.76)	Acc@5 100.00 ( 99.88)
Epoch: [34][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5456e-01 (1.6713e-01)	Acc@1  92.19 ( 94.71)	Acc@5 100.00 ( 99.88)
Epoch: [34][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0629e-01 (1.6846e-01)	Acc@1  90.62 ( 94.66)	Acc@5  99.22 ( 99.87)
Epoch: [34][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4756e-01 (1.6882e-01)	Acc@1  89.06 ( 94.64)	Acc@5 100.00 ( 99.87)
Epoch: [34][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9530e-01 (1.7008e-01)	Acc@1  89.84 ( 94.60)	Acc@5  99.22 ( 99.86)
Epoch: [34][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3642e-01 (1.7120e-01)	Acc@1  92.19 ( 94.56)	Acc@5 100.00 ( 99.86)
Epoch: [34][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0372e-01 (1.7206e-01)	Acc@1  94.53 ( 94.54)	Acc@5 100.00 ( 99.86)
Epoch: [34][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0838e-01 (1.7316e-01)	Acc@1  92.97 ( 94.50)	Acc@5 100.00 ( 99.85)
Epoch: [34][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6077e-01 (1.7336e-01)	Acc@1  94.53 ( 94.49)	Acc@5 100.00 ( 99.85)
Epoch: [34][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6934e-01 (1.7481e-01)	Acc@1  86.25 ( 94.43)	Acc@5 100.00 ( 99.85)
## e[34] optimizer.zero_grad (sum) time: 0.13933372497558594
## e[34]       loss.backward (sum) time: 2.5599474906921387
## e[34]      optimizer.step (sum) time: 1.033311128616333
## epoch[34] training(only) time: 18.467990159988403
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.4406e+00 (1.4406e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5114e+00 (1.4849e+00)	Acc@1  70.00 ( 69.45)	Acc@5  91.00 ( 90.27)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3769e+00 (1.5179e+00)	Acc@1  74.00 ( 69.33)	Acc@5  91.00 ( 90.00)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.3219e+00 (1.5708e+00)	Acc@1  54.00 ( 68.39)	Acc@5  90.00 ( 89.87)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2874e+00 (1.5289e+00)	Acc@1  72.00 ( 68.54)	Acc@5  92.00 ( 90.61)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.9255e+00 (1.5529e+00)	Acc@1  65.00 ( 68.20)	Acc@5  87.00 ( 90.04)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4563e+00 (1.5366e+00)	Acc@1  65.00 ( 68.41)	Acc@5  91.00 ( 90.34)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.9698e+00 (1.5462e+00)	Acc@1  67.00 ( 68.08)	Acc@5  88.00 ( 90.20)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.8844e+00 (1.5562e+00)	Acc@1  66.00 ( 68.11)	Acc@5  84.00 ( 90.02)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8366e+00 (1.5374e+00)	Acc@1  62.00 ( 68.33)	Acc@5  88.00 ( 90.21)
 * Acc@1 68.450 Acc@5 90.330
### epoch[34] execution time: 20.967140436172485
EPOCH 35
i:   0, name:          module.conv1.0.weight  changing lr from: 0.017019931844240544   to: 0.014600142389248290
i:   1, name:          module.conv1.1.weight  changing lr from: 0.018053206171315402   to: 0.015596474608471252
i:   2, name:            module.conv1.1.bias  changing lr from: 0.019094902404015880   to: 0.016605268999563015
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.020143405694331120   to: 0.017624715490523874
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.021197197998799568   to: 0.018653106345899405
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.022254854132878102   to: 0.019688832373248349
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.023315037870032120   to: 0.020730379130315876
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.024376498099414820   to: 0.021776323151790344
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.025438065053834492   to: 0.022825328211914121
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.026498646617767765   to: 0.023876141636869616
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.027557224723444709   to: 0.024927590678751721
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.028612851841494348   to: 0.025978578961049194
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.029664647571272282   to: 0.027028083003866138
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.030711795334786608   to: 0.028075148835610360
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.031753539177073742   to: 0.029118888696534895
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.032789180674943540   to: 0.030158477838333045
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.033818075955195412   to: 0.031193151422937462
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.034839632822697163   to: 0.032222201522750495
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.035853307998103615   to: 0.033244974223722738
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.036858604464459425   to: 0.034260866831987320
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.037855068921480883   to: 0.035269325184143642
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.038842289345922376   to: 0.036269841060749007
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.039819892656112375   to: 0.037261949702120191
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.040787542478473590   to: 0.038245227425154779
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.041744937013619789   to: 0.039219289339550406
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.042691806999445935   to: 0.040183787161522461
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.043627913768485765   to: 0.041138407122889603
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.044553047396705819   to: 0.042082867973208420
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.045467024940827286   to: 0.043016919072488446
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.046369688761214133   to: 0.043940338571899530
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.047260904927338337   to: 0.044852931679796731
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.048140561702821959   to: 0.045754529010324202
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.049008568107062757   to: 0.046644985011818831
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.049864852550472655   to: 0.047524176472215351
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.050709361540389714   to: 0.048392001098648073
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.051542058454770813   to: 0.049248376168458126
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.052362922380822852   to: 0.050093237248836318
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.053171947015792136   to: 0.050926536982367397
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.053969139627195788   to: 0.051748243935783735
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.054754520069851459   to: 0.052558341509288044
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.055528119857134364   to: 0.053356826903861315
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.056289981283969219   to: 0.054143710144034851
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.057040156599143181   to: 0.054919013153671647
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.057778707224606402   to: 0.055682768882371336
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.058505703019508574   to: 0.056435020480185775
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.059221221586801043   to: 0.057175820518405501
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.059925347620314956   to: 0.057905230254251608
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.060618172290307450   to: 0.058623318937384483
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.061299792665546443   to: 0.059330163156213880
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.061970311170083961   to: 0.060025846222072424
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.062629835072944104   to: 0.060710457589387125
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.063278476009027501   to: 0.061384092310058175
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.063916349529607772   to: 0.062046850520326491
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.064543574680866037   to: 0.062698836958481913
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.065160273608980704   to: 0.063340160511834373
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.065766571190355338   to: 0.063970933791437409
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.066362594685633949   to: 0.064591272733119348
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.066948473416215598   to: 0.065201296223442506
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.067524338462041450   to: 0.065801125749272146
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.068090322379484966   to: 0.066390885069696939
i:  60, name:               module.fc.weight  changing lr from: 0.068646558938234042   to: 0.066970699909101844
i:  61, name:                 module.fc.bias  changing lr from: 0.069193182876106832   to: 0.067540697670249936



# Switched to train mode...
Epoch: [35][  0/391]	Time  0.197 ( 0.197)	Data  0.152 ( 0.152)	Loss 1.3783e-01 (1.3783e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 1.2533e-01 (1.4932e-01)	Acc@1  96.88 ( 95.38)	Acc@5 100.00 ( 99.93)
Epoch: [35][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.9146e-01 (1.4724e-01)	Acc@1  92.97 ( 95.76)	Acc@5 100.00 ( 99.81)
Epoch: [35][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.2400e-01 (1.4562e-01)	Acc@1  98.44 ( 95.74)	Acc@5 100.00 ( 99.80)
Epoch: [35][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.2345e-01 (1.4835e-01)	Acc@1  96.09 ( 95.54)	Acc@5 100.00 ( 99.85)
Epoch: [35][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.5998e-01 (1.5203e-01)	Acc@1  89.84 ( 95.37)	Acc@5 100.00 ( 99.83)
Epoch: [35][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.2853e-02 (1.5246e-01)	Acc@1  98.44 ( 95.35)	Acc@5 100.00 ( 99.81)
Epoch: [35][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6953e-01 (1.5299e-01)	Acc@1  94.53 ( 95.37)	Acc@5 100.00 ( 99.80)
Epoch: [35][ 80/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7832e-01 (1.4957e-01)	Acc@1  94.53 ( 95.45)	Acc@5  99.22 ( 99.81)
Epoch: [35][ 90/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2051e-01 (1.4972e-01)	Acc@1  96.09 ( 95.42)	Acc@5 100.00 ( 99.83)
Epoch: [35][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9819e-01 (1.5063e-01)	Acc@1  92.97 ( 95.41)	Acc@5 100.00 ( 99.81)
Epoch: [35][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7574e-01 (1.5125e-01)	Acc@1  94.53 ( 95.45)	Acc@5  99.22 ( 99.81)
Epoch: [35][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9765e-01 (1.5196e-01)	Acc@1  89.84 ( 95.37)	Acc@5 100.00 ( 99.81)
Epoch: [35][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1913e-01 (1.5242e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.81)
Epoch: [35][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5071e-01 (1.5144e-01)	Acc@1  89.84 ( 95.35)	Acc@5 100.00 ( 99.82)
Epoch: [35][150/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.4213e-01 (1.5120e-01)	Acc@1  94.53 ( 95.33)	Acc@5 100.00 ( 99.81)
Epoch: [35][160/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4193e-01 (1.5171e-01)	Acc@1  89.84 ( 95.29)	Acc@5 100.00 ( 99.81)
Epoch: [35][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3640e-01 (1.5198e-01)	Acc@1  96.88 ( 95.28)	Acc@5 100.00 ( 99.81)
Epoch: [35][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9174e-01 (1.5321e-01)	Acc@1  95.31 ( 95.27)	Acc@5  99.22 ( 99.82)
Epoch: [35][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8913e-01 (1.5380e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.83)
Epoch: [35][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7844e-01 (1.5468e-01)	Acc@1  94.53 ( 95.20)	Acc@5 100.00 ( 99.83)
Epoch: [35][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1992e-01 (1.5451e-01)	Acc@1  96.88 ( 95.19)	Acc@5 100.00 ( 99.84)
Epoch: [35][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5179e-01 (1.5480e-01)	Acc@1  94.53 ( 95.16)	Acc@5 100.00 ( 99.84)
Epoch: [35][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6054e-01 (1.5498e-01)	Acc@1  94.53 ( 95.16)	Acc@5 100.00 ( 99.84)
Epoch: [35][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0353e-02 (1.5489e-01)	Acc@1  98.44 ( 95.17)	Acc@5 100.00 ( 99.84)
Epoch: [35][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9780e-01 (1.5637e-01)	Acc@1  95.31 ( 95.11)	Acc@5 100.00 ( 99.84)
Epoch: [35][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5997e-01 (1.5718e-01)	Acc@1  94.53 ( 95.07)	Acc@5 100.00 ( 99.85)
Epoch: [35][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3994e-01 (1.5713e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.85)
Epoch: [35][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1455e-01 (1.5729e-01)	Acc@1  92.19 ( 95.09)	Acc@5 100.00 ( 99.85)
Epoch: [35][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7981e-01 (1.5761e-01)	Acc@1  97.66 ( 95.09)	Acc@5 100.00 ( 99.86)
Epoch: [35][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1538e-01 (1.5746e-01)	Acc@1  97.66 ( 95.09)	Acc@5 100.00 ( 99.86)
Epoch: [35][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6386e-01 (1.5832e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.86)
Epoch: [35][320/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5162e-01 (1.5826e-01)	Acc@1  94.53 ( 95.09)	Acc@5 100.00 ( 99.87)
Epoch: [35][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1548e-01 (1.5904e-01)	Acc@1  96.09 ( 95.05)	Acc@5 100.00 ( 99.87)
Epoch: [35][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4193e-01 (1.5931e-01)	Acc@1  92.19 ( 95.06)	Acc@5  99.22 ( 99.86)
Epoch: [35][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7429e-01 (1.6010e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.86)
Epoch: [35][360/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7365e-01 (1.6126e-01)	Acc@1  96.09 ( 94.95)	Acc@5 100.00 ( 99.86)
Epoch: [35][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5645e-01 (1.6157e-01)	Acc@1  93.75 ( 94.95)	Acc@5 100.00 ( 99.86)
Epoch: [35][380/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0032e-01 (1.6173e-01)	Acc@1  97.66 ( 94.94)	Acc@5 100.00 ( 99.86)
Epoch: [35][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7229e-01 (1.6231e-01)	Acc@1  92.50 ( 94.93)	Acc@5 100.00 ( 99.86)
## e[35] optimizer.zero_grad (sum) time: 0.1382002830505371
## e[35]       loss.backward (sum) time: 2.562053680419922
## e[35]      optimizer.step (sum) time: 1.0314736366271973
## epoch[35] training(only) time: 18.444865465164185
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.5423e+00 (1.5423e+00)	Acc@1  70.00 ( 70.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.4855e+00 (1.5366e+00)	Acc@1  66.00 ( 68.00)	Acc@5  94.00 ( 90.27)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3515e+00 (1.4660e+00)	Acc@1  73.00 ( 69.71)	Acc@5  89.00 ( 90.48)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.9574e+00 (1.5152e+00)	Acc@1  64.00 ( 69.10)	Acc@5  90.00 ( 90.29)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3873e+00 (1.4904e+00)	Acc@1  69.00 ( 68.88)	Acc@5  92.00 ( 90.88)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6900e+00 (1.5100e+00)	Acc@1  69.00 ( 68.65)	Acc@5  92.00 ( 90.76)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.6644e+00 (1.4945e+00)	Acc@1  63.00 ( 68.79)	Acc@5  90.00 ( 91.00)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.7331e+00 (1.5031e+00)	Acc@1  70.00 ( 68.65)	Acc@5  90.00 ( 90.96)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6805e+00 (1.5046e+00)	Acc@1  70.00 ( 68.67)	Acc@5  86.00 ( 90.74)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.7893e+00 (1.4805e+00)	Acc@1  67.00 ( 69.09)	Acc@5  87.00 ( 90.85)
 * Acc@1 69.250 Acc@5 90.920
### epoch[35] execution time: 20.927876234054565
EPOCH 36
i:   0, name:          module.conv1.0.weight  changing lr from: 0.014600142389248290   to: 0.012349477331863180
i:   1, name:          module.conv1.1.weight  changing lr from: 0.015596474608471252   to: 0.013300315521473722
i:   2, name:            module.conv1.1.bias  changing lr from: 0.016605268999563015   to: 0.014267927368335920
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.017624715490523874   to: 0.015250306910145256
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.018653106345899405   to: 0.016245555164612069
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.019688832373248349   to: 0.017251876594870420
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.020730379130315876   to: 0.018267575522466474
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.021776323151790344   to: 0.019291052512426306
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.022825328211914121   to: 0.020320800751831593
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.023876141636869616   to: 0.021355402440547473
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.024927590678751721   to: 0.022393525210230875
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.025978578961049194   to: 0.023433918585480180
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.027028083003866138   to: 0.024475410498943580
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.028075148835610360   to: 0.025516903870372611
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.029118888696534895   to: 0.026557373257964013
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.030158477838333045   to: 0.027595861588867312
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.031193151422937462   to: 0.028631476974428430
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.032222201522750495   to: 0.029663389614577707
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.033244974223722738   to: 0.030690828794743115
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.034260866831987320   to: 0.031713079977758594
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.035269325184143642   to: 0.032729481992440919
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.036269841060749007   to: 0.033739424319804055
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.037261949702120191   to: 0.034742344477271293
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.038245227425154779   to: 0.035737725500712414
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.039219289339550406   to: 0.036725093523673945
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.040183787161522461   to: 0.037704015452777788
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.041138407122889603   to: 0.038674096737925961
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.042082867973208420   to: 0.039634979235667001
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.043016919072488446   to: 0.040586339163841835
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.043940338571899530   to: 0.041527885145430737
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.044852931679796731   to: 0.042459356339365394
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.045754529010324202   to: 0.043380520655942667
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.046644985011818831   to: 0.044291173054379782
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.047524176472215351   to: 0.045191133919979698
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.048392001098648073   to: 0.046080247518323764
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.049248376168458126   to: 0.046958380523881384
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.050093237248836318   to: 0.047825420620410841
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.050926536982367397   to: 0.048681275170529725
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.051748243935783735   to: 0.049525869951845658
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.052558341509288044   to: 0.050359147957065420
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.053356826903861315   to: 0.051181068255534035
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.054143710144034851   to: 0.051991604913698530
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.054919013153671647   to: 0.052790745972040892
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.055682768882371336   to: 0.053578492476077821
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.056435020480185775   to: 0.054354857559085756
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.057175820518405501   to: 0.055119865574270857
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.057905230254251608   to: 0.055873551274169368
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.058623318937384483   to: 0.056615959035131336
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.059330163156213880   to: 0.057347142124808619
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.060025846222072424   to: 0.058067162010638433
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.060710457589387125   to: 0.058776087707382990
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.061384092310058175   to: 0.059473995161855878
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.062046850520326491   to: 0.060160966673035922
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.062698836958481913   to: 0.060837090345837112
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.063340160511834373   to: 0.061502459576872476
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.063970933791437409   to: 0.062157172570615461
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.064591272733119348   to: 0.062801331884428976
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.065201296223442506   to: 0.063435044000995672
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.065801125749272146   to: 0.064058418926746491
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.066390885069696939   to: 0.064671569814944235
i:  60, name:               module.fc.weight  changing lr from: 0.066970699909101844   to: 0.065274612612139224
i:  61, name:                 module.fc.bias  changing lr from: 0.067540697670249936   to: 0.065867665726771316



# Switched to train mode...
Epoch: [36][  0/391]	Time  0.189 ( 0.189)	Data  0.149 ( 0.149)	Loss 1.0042e-01 (1.0042e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.045 ( 0.059)	Data  0.001 ( 0.015)	Loss 1.8193e-01 (1.2491e-01)	Acc@1  92.97 ( 96.31)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 9.3380e-02 (1.3145e-01)	Acc@1  96.88 ( 95.65)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 8.7978e-02 (1.3043e-01)	Acc@1  96.09 ( 95.82)	Acc@5 100.00 ( 99.97)
Epoch: [36][ 40/391]	Time  0.048 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.1257e-01 (1.2546e-01)	Acc@1  97.66 ( 96.15)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0191e-01 (1.2528e-01)	Acc@1  96.88 ( 96.22)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0174e-01 (1.2691e-01)	Acc@1  97.66 ( 96.18)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.0610e-01 (1.2406e-01)	Acc@1  98.44 ( 96.25)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 80/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7800e-01 (1.2495e-01)	Acc@1  94.53 ( 96.29)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3492e-01 (1.2595e-01)	Acc@1  95.31 ( 96.26)	Acc@5 100.00 ( 99.95)
Epoch: [36][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.6936e-02 (1.2438e-01)	Acc@1  98.44 ( 96.28)	Acc@5 100.00 ( 99.95)
Epoch: [36][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.7627e-01 (1.2456e-01)	Acc@1  92.97 ( 96.30)	Acc@5 100.00 ( 99.94)
Epoch: [36][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.3313e-01 (1.2479e-01)	Acc@1  96.88 ( 96.30)	Acc@5  99.22 ( 99.94)
Epoch: [36][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.8052e-02 (1.2499e-01)	Acc@1  96.88 ( 96.29)	Acc@5 100.00 ( 99.94)
Epoch: [36][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2070e-01 (1.2555e-01)	Acc@1  96.09 ( 96.24)	Acc@5 100.00 ( 99.93)
Epoch: [36][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1091e-01 (1.2573e-01)	Acc@1  96.88 ( 96.22)	Acc@5 100.00 ( 99.93)
Epoch: [36][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4133e-02 (1.2526e-01)	Acc@1  97.66 ( 96.22)	Acc@5 100.00 ( 99.94)
Epoch: [36][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.6669e-02 (1.2548e-01)	Acc@1  98.44 ( 96.20)	Acc@5 100.00 ( 99.94)
Epoch: [36][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3159e-02 (1.2576e-01)	Acc@1  97.66 ( 96.20)	Acc@5 100.00 ( 99.94)
Epoch: [36][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2392e-02 (1.2504e-01)	Acc@1  99.22 ( 96.24)	Acc@5 100.00 ( 99.94)
Epoch: [36][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4974e-01 (1.2650e-01)	Acc@1  94.53 ( 96.18)	Acc@5  99.22 ( 99.94)
Epoch: [36][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.6649e-02 (1.2684e-01)	Acc@1  97.66 ( 96.17)	Acc@5 100.00 ( 99.94)
Epoch: [36][220/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3533e-01 (1.2722e-01)	Acc@1  93.75 ( 96.17)	Acc@5  99.22 ( 99.94)
Epoch: [36][230/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6634e-01 (1.2782e-01)	Acc@1  96.88 ( 96.14)	Acc@5  98.44 ( 99.94)
Epoch: [36][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5374e-02 (1.2745e-01)	Acc@1  98.44 ( 96.18)	Acc@5 100.00 ( 99.94)
Epoch: [36][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0749e-01 (1.2743e-01)	Acc@1  96.88 ( 96.18)	Acc@5 100.00 ( 99.93)
Epoch: [36][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5741e-01 (1.2830e-01)	Acc@1  96.09 ( 96.16)	Acc@5 100.00 ( 99.93)
Epoch: [36][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8803e-01 (1.2860e-01)	Acc@1  94.53 ( 96.15)	Acc@5 100.00 ( 99.93)
Epoch: [36][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7810e-02 (1.2863e-01)	Acc@1  98.44 ( 96.15)	Acc@5 100.00 ( 99.93)
Epoch: [36][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3634e-01 (1.2901e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.93)
Epoch: [36][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5978e-01 (1.2910e-01)	Acc@1  95.31 ( 96.15)	Acc@5 100.00 ( 99.93)
Epoch: [36][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.5442e-02 (1.2924e-01)	Acc@1  96.88 ( 96.13)	Acc@5 100.00 ( 99.93)
Epoch: [36][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3031e-01 (1.3007e-01)	Acc@1  95.31 ( 96.09)	Acc@5 100.00 ( 99.93)
Epoch: [36][330/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3791e-02 (1.3000e-01)	Acc@1  97.66 ( 96.08)	Acc@5 100.00 ( 99.94)
Epoch: [36][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3809e-01 (1.3106e-01)	Acc@1  92.19 ( 96.03)	Acc@5 100.00 ( 99.94)
Epoch: [36][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1077e-01 (1.3132e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 ( 99.93)
Epoch: [36][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.4834e-02 (1.3228e-01)	Acc@1  98.44 ( 95.96)	Acc@5 100.00 ( 99.94)
Epoch: [36][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.3580e-01 (1.3313e-01)	Acc@1  91.41 ( 95.91)	Acc@5  99.22 ( 99.93)
Epoch: [36][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1570e-02 (1.3351e-01)	Acc@1  97.66 ( 95.91)	Acc@5 100.00 ( 99.93)
Epoch: [36][390/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8456e-01 (1.3401e-01)	Acc@1  93.75 ( 95.88)	Acc@5 100.00 ( 99.93)
## e[36] optimizer.zero_grad (sum) time: 0.14036011695861816
## e[36]       loss.backward (sum) time: 2.5606279373168945
## e[36]      optimizer.step (sum) time: 1.033851146697998
## epoch[36] training(only) time: 18.491035223007202
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.7610e+00 (1.7610e+00)	Acc@1  69.00 ( 69.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.4652e+00 (1.5689e+00)	Acc@1  66.00 ( 70.18)	Acc@5  93.00 ( 90.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3592e+00 (1.5192e+00)	Acc@1  74.00 ( 70.10)	Acc@5  92.00 ( 90.67)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6595e+00 (1.5556e+00)	Acc@1  69.00 ( 69.55)	Acc@5  86.00 ( 90.23)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.5254e+00 (1.5342e+00)	Acc@1  71.00 ( 69.68)	Acc@5  92.00 ( 90.63)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5509e+00 (1.5510e+00)	Acc@1  67.00 ( 69.16)	Acc@5  91.00 ( 90.18)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.7071e+00 (1.5353e+00)	Acc@1  67.00 ( 69.26)	Acc@5  91.00 ( 90.46)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.7660e+00 (1.5504e+00)	Acc@1  71.00 ( 69.15)	Acc@5  92.00 ( 90.46)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.8856e+00 (1.5605e+00)	Acc@1  69.00 ( 69.11)	Acc@5  85.00 ( 90.37)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8665e+00 (1.5388e+00)	Acc@1  62.00 ( 69.24)	Acc@5  88.00 ( 90.58)
 * Acc@1 69.370 Acc@5 90.630
### epoch[36] execution time: 20.98084306716919
EPOCH 37
i:   0, name:          module.conv1.0.weight  changing lr from: 0.012349477331863180   to: 0.010278393921015021
i:   1, name:          module.conv1.1.weight  changing lr from: 0.013300315521473722   to: 0.011175143126010655
i:   2, name:            module.conv1.1.bias  changing lr from: 0.014267927368335920   to: 0.012093226708926978
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.015250306910145256   to: 0.013030444540589676
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.016245555164612069   to: 0.013984707043569449
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.017251876594870420   to: 0.014954032034400912
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.018267575522466474   to: 0.015936541450753804
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.019291052512426306   to: 0.016930457994265336
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.020320800751831593   to: 0.017934101716189782
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.021355402440547473   to: 0.018945886569782112
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.022393525210230875   to: 0.019964316950389069
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.023433918585480180   to: 0.020987984241551402
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.024475410498943580   to: 0.022015563383001494
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.025516903870372611   to: 0.023045809474259094
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.026557373257964013   to: 0.024077554425556948
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.027595861588867312   to: 0.025109703666059198
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.028631476974428430   to: 0.026141232917745866
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.029663389614577707   to: 0.027171185041914750
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.030690828794743115   to: 0.028198666963982402
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.031713079977758594   to: 0.029222846681134059
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.032729481992440919   to: 0.030242950356370105
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.033739424319804055   to: 0.031258259501604956
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.034742344477271293   to: 0.032268108251692393
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.035737725500712414   to: 0.033271880730560609
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.036725093523673945   to: 0.034269008510036107
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.037704015452777788   to: 0.035258968161410782
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.038674096737925961   to: 0.036241278899348355
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.039634979235667001   to: 0.037215500317334262
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.040586339163841835   to: 0.038181230213535922
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.041527885145430737   to: 0.039138102505652771
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.042459356339365394   to: 0.040085785233096152
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.043380520655942667   to: 0.041023978644637492
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.044291173054379782   to: 0.041952413369498398
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.045191133919979698   to: 0.042870848669725466
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.046080247518323764   to: 0.043779070771585604
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.046958380523881384   to: 0.044676891273641821
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.047825420620410841   to: 0.045564145629109425
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.048681275170529725   to: 0.046440691700057379
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.049525869951845658   to: 0.047306408380996888
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.050359147957065420   to: 0.048161194289394887
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.051181068255534035   to: 0.049004966520655967
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.051991604913698530   to: 0.049837659465134541
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.052790745972040892   to: 0.050659223684766740
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.053578492476077821   to: 0.051469624846945887
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.054354857559085756   to: 0.052268842713308665
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.055119865574270857   to: 0.053056870181146021
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.055873551274169368   to: 0.053833712375205560
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.056615959035131336   to: 0.054599385787708447
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.057347142124808619   to: 0.055353917464462457
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.058067162010638433   to: 0.056097344235014918
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.058776087707382990   to: 0.056829711984851185
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.059473995161855878   to: 0.057551074967709816
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.060160966673035922   to: 0.058261495156149402
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.060837090345837112   to: 0.058961041628566857
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.061502459576872476   to: 0.059649789990933483
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.062157172570615461   to: 0.060327821831577366
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.062801331884428976   to: 0.060995224207406440
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.063435044000995672   to: 0.061652089160028581
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.064058418926746491   to: 0.062298513260287305
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.064671569814944235   to: 0.062934597179791926
i:  60, name:               module.fc.weight  changing lr from: 0.065274612612139224   to: 0.063560445288080900
i:  61, name:                 module.fc.bias  changing lr from: 0.065867665726771316   to: 0.064176165274114585



# Switched to train mode...
Epoch: [37][  0/391]	Time  0.201 ( 0.201)	Data  0.158 ( 0.158)	Loss 7.5805e-02 (7.5805e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.046 ( 0.061)	Data  0.001 ( 0.016)	Loss 8.6393e-02 (1.2095e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 (100.00)
Epoch: [37][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 1.2882e-01 (1.2513e-01)	Acc@1  93.75 ( 96.39)	Acc@5 100.00 ( 99.93)
Epoch: [37][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.007)	Loss 1.1850e-01 (1.2693e-01)	Acc@1  95.31 ( 96.19)	Acc@5 100.00 ( 99.92)
Epoch: [37][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.4321e-01 (1.2392e-01)	Acc@1  95.31 ( 96.28)	Acc@5 100.00 ( 99.94)
Epoch: [37][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 9.5517e-02 (1.2188e-01)	Acc@1  96.09 ( 96.25)	Acc@5 100.00 ( 99.95)
Epoch: [37][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 9.0688e-02 (1.1875e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.5681e-01 (1.2064e-01)	Acc@1  95.31 ( 96.38)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.4683e-02 (1.1870e-01)	Acc@1 100.00 ( 96.42)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.7715e-02 (1.1760e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.97)
Epoch: [37][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4207e-01 (1.1711e-01)	Acc@1  94.53 ( 96.58)	Acc@5 100.00 ( 99.97)
Epoch: [37][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.7414e-02 (1.1641e-01)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.96)
Epoch: [37][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1215e-01 (1.1596e-01)	Acc@1  94.53 ( 96.50)	Acc@5 100.00 ( 99.96)
Epoch: [37][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2502e-01 (1.1622e-01)	Acc@1  96.09 ( 96.46)	Acc@5  99.22 ( 99.95)
Epoch: [37][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.8440e-02 (1.1561e-01)	Acc@1  99.22 ( 96.49)	Acc@5 100.00 ( 99.94)
Epoch: [37][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.1444e-02 (1.1527e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.95)
Epoch: [37][160/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.004)	Loss 5.1771e-02 (1.1529e-01)	Acc@1  98.44 ( 96.51)	Acc@5 100.00 ( 99.94)
Epoch: [37][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.9278e-02 (1.1565e-01)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.94)
Epoch: [37][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2319e-01 (1.1493e-01)	Acc@1  91.41 ( 96.50)	Acc@5 100.00 ( 99.94)
Epoch: [37][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9879e-01 (1.1492e-01)	Acc@1  92.97 ( 96.49)	Acc@5 100.00 ( 99.94)
Epoch: [37][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0488e-01 (1.1401e-01)	Acc@1  97.66 ( 96.51)	Acc@5 100.00 ( 99.94)
Epoch: [37][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.9618e-02 (1.1410e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.94)
Epoch: [37][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.8022e-02 (1.1389e-01)	Acc@1  98.44 ( 96.51)	Acc@5 100.00 ( 99.94)
Epoch: [37][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7688e-02 (1.1360e-01)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 ( 99.94)
Epoch: [37][240/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1852e-01 (1.1351e-01)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.94)
Epoch: [37][250/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4610e-01 (1.1315e-01)	Acc@1  94.53 ( 96.56)	Acc@5 100.00 ( 99.94)
Epoch: [37][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7642e-01 (1.1355e-01)	Acc@1  94.53 ( 96.52)	Acc@5 100.00 ( 99.94)
Epoch: [37][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3550e-01 (1.1416e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.94)
Epoch: [37][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1436e-01 (1.1535e-01)	Acc@1  95.31 ( 96.45)	Acc@5 100.00 ( 99.94)
Epoch: [37][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3286e-02 (1.1575e-01)	Acc@1  96.88 ( 96.44)	Acc@5 100.00 ( 99.94)
Epoch: [37][300/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1877e-02 (1.1675e-01)	Acc@1  97.66 ( 96.40)	Acc@5 100.00 ( 99.94)
Epoch: [37][310/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4085e-01 (1.1743e-01)	Acc@1  96.09 ( 96.39)	Acc@5 100.00 ( 99.94)
Epoch: [37][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3416e-01 (1.1827e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.93)
Epoch: [37][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4986e-02 (1.1872e-01)	Acc@1  96.88 ( 96.35)	Acc@5 100.00 ( 99.93)
Epoch: [37][340/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4311e-01 (1.1951e-01)	Acc@1  95.31 ( 96.33)	Acc@5 100.00 ( 99.93)
Epoch: [37][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5698e-01 (1.1995e-01)	Acc@1  94.53 ( 96.31)	Acc@5 100.00 ( 99.94)
Epoch: [37][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2031e-01 (1.2084e-01)	Acc@1  96.88 ( 96.29)	Acc@5 100.00 ( 99.94)
Epoch: [37][370/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3911e-01 (1.2105e-01)	Acc@1  96.09 ( 96.27)	Acc@5 100.00 ( 99.93)
Epoch: [37][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1699e-01 (1.2178e-01)	Acc@1  96.88 ( 96.24)	Acc@5 100.00 ( 99.93)
Epoch: [37][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2275e-01 (1.2225e-01)	Acc@1  92.50 ( 96.22)	Acc@5 100.00 ( 99.93)
## e[37] optimizer.zero_grad (sum) time: 0.1395120620727539
## e[37]       loss.backward (sum) time: 2.5708937644958496
## e[37]      optimizer.step (sum) time: 1.0327374935150146
## epoch[37] training(only) time: 18.45162296295166
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.7608e+00 (1.7608e+00)	Acc@1  66.00 ( 66.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5400e+00 (1.5626e+00)	Acc@1  69.00 ( 69.09)	Acc@5  92.00 ( 90.45)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.7235e+00 (1.5084e+00)	Acc@1  70.00 ( 70.19)	Acc@5  87.00 ( 90.33)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7623e+00 (1.5481e+00)	Acc@1  66.00 ( 69.55)	Acc@5  90.00 ( 90.00)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3072e+00 (1.5319e+00)	Acc@1  72.00 ( 69.78)	Acc@5  93.00 ( 90.37)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.9717e+00 (1.5571e+00)	Acc@1  65.00 ( 69.27)	Acc@5  85.00 ( 90.04)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4440e+00 (1.5257e+00)	Acc@1  68.00 ( 69.62)	Acc@5  90.00 ( 90.26)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.8086e+00 (1.5345e+00)	Acc@1  68.00 ( 69.41)	Acc@5  88.00 ( 90.24)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.4884e+00 (1.5347e+00)	Acc@1  70.00 ( 69.33)	Acc@5  89.00 ( 90.27)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.9724e+00 (1.5110e+00)	Acc@1  65.00 ( 69.60)	Acc@5  89.00 ( 90.56)
 * Acc@1 69.830 Acc@5 90.520
### epoch[37] execution time: 20.967281341552734
EPOCH 38
i:   0, name:          module.conv1.0.weight  changing lr from: 0.010278393921015021   to: 0.008396515016716099
i:   1, name:          module.conv1.1.weight  changing lr from: 0.011175143126010655   to: 0.009230596128661887
i:   2, name:            module.conv1.1.bias  changing lr from: 0.012093226708926978   to: 0.010090796083986858
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.013030444540589676   to: 0.010974724864738838
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.013984707043569449   to: 0.011880105318585899
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.014954032034400912   to: 0.012804770506065870
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.015936541450753804   to: 0.013746660860401921
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.016930457994265336   to: 0.014703821197347082
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.017934101716189782   to: 0.015674397608478868
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.018945886569782112   to: 0.016656634267659648
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.019964316950389069   to: 0.017648870176992040
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.020987984241551402   to: 0.018649535875509732
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.022015563383001494   to: 0.019657150131031038
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.023045809474259094   to: 0.020670316633048233
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.024077554425556948   to: 0.021687720702208563
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.025109703666059198   to: 0.022708126029847776
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.026141232917745866   to: 0.023730371459144500
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.027171185041914750   to: 0.024753367817759588
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.028198666963982402   to: 0.025776094810293068
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.029222846681134059   to: 0.026797597977515943
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.030242950356370105   to: 0.027816985728107882
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.031258259501604956   to: 0.028833426447531854
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.032268108251692393   to: 0.029846145687704273
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.033271880730560609   to: 0.030854423440251556
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.034269008510036107   to: 0.031857591495378715
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.035258968161410782   to: 0.032855030887701933
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.036241278899348355   to: 0.033846169429802862
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.037215500317334262   to: 0.034830479333745966
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.038181230213535922   to: 0.035807474920349025
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.039138102505652771   to: 0.036776710415606484
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.040085785233096152   to: 0.037737777833330537
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.041023978644637492   to: 0.038690304942787808
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.041952413369498398   to: 0.039633953319867042
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.042870848669725466   to: 0.040568416480110725
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.043779070771585604   to: 0.041493418091773732
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.044676891273641821   to: 0.042408710266937079
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.045564145629109425   to: 0.043314071928593843
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.046440691700057379   to: 0.044209307251541312
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.047306408380996888   to: 0.045094244174848891
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.048161194289394887   to: 0.045968732983628705
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.049004966520655967   to: 0.046832644957808006
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.049837659465134541   to: 0.047685871085590137
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.050659223684766740   to: 0.048528320839291809
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.051469624846945887   to: 0.049359921011254221
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.052268842713308665   to: 0.050180614607548246
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.053056870181146021   to: 0.050990359797221420
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.053833712375205560   to: 0.051789128914870669
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.054599385787708447   to: 0.052576907514366777
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.055353917464462457   to: 0.053353693471602105
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.056097344235014918   to: 0.054119496134184299
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.056829711984851185   to: 0.054874335516051302
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.057551074967709816   to: 0.055618241535039636
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.058261495156149402   to: 0.056351253291495464
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.058961041628566857   to: 0.057073418386076602
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.059649789990933483   to: 0.057784792274954901
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.060327821831577366   to: 0.058485437660687249
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.060995224207406440   to: 0.059175423917085260
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.061652089160028581   to: 0.059854826546473852
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.062298513260287305   to: 0.060523726667788408
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.062934597179791926   to: 0.061182210534019649
i:  60, name:               module.fc.weight  changing lr from: 0.063560445288080900   to: 0.061830369077574082
i:  61, name:                 module.fc.bias  changing lr from: 0.064176165274114585   to: 0.062468297482174820



# Switched to train mode...
Epoch: [38][  0/391]	Time  0.187 ( 0.187)	Data  0.141 ( 0.141)	Loss 9.3005e-02 (9.3005e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.015)	Loss 1.1738e-01 (1.2509e-01)	Acc@1  95.31 ( 95.81)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 5.4921e-02 (1.1329e-01)	Acc@1  98.44 ( 96.43)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.0029e-01 (1.0716e-01)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 7.7879e-02 (1.0580e-01)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 50/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.4035e-01 (1.0592e-01)	Acc@1  96.09 ( 96.72)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.1098e-01 (1.0457e-01)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.9311e-01 (1.0644e-01)	Acc@1  94.53 ( 96.73)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.1459e-02 (1.0461e-01)	Acc@1  98.44 ( 96.78)	Acc@5 100.00 ( 99.94)
Epoch: [38][ 90/391]	Time  0.043 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.1770e-02 (1.0448e-01)	Acc@1 100.00 ( 96.80)	Acc@5 100.00 ( 99.94)
Epoch: [38][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2554e-01 (1.0461e-01)	Acc@1  95.31 ( 96.79)	Acc@5  99.22 ( 99.93)
Epoch: [38][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.1963e-02 (1.0414e-01)	Acc@1 100.00 ( 96.85)	Acc@5 100.00 ( 99.94)
Epoch: [38][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.1529e-02 (1.0247e-01)	Acc@1  98.44 ( 96.92)	Acc@5 100.00 ( 99.94)
Epoch: [38][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.5198e-02 (1.0174e-01)	Acc@1  99.22 ( 96.99)	Acc@5 100.00 ( 99.95)
Epoch: [38][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.003)	Loss 3.4033e-02 (1.0211e-01)	Acc@1 100.00 ( 96.96)	Acc@5 100.00 ( 99.95)
Epoch: [38][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0170e-01 (1.0273e-01)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.94)
Epoch: [38][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1730e-02 (1.0214e-01)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.95)
Epoch: [38][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0674e-01 (1.0174e-01)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.95)
Epoch: [38][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9625e-02 (1.0112e-01)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.95)
Epoch: [38][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9667e-02 (1.0216e-01)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.95)
Epoch: [38][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0236e-01 (1.0253e-01)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.95)
Epoch: [38][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2191e-01 (1.0167e-01)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [38][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2430e-02 (1.0129e-01)	Acc@1  98.44 ( 96.94)	Acc@5  99.22 ( 99.95)
Epoch: [38][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3096e-02 (1.0128e-01)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.95)
Epoch: [38][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3462e-02 (1.0173e-01)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [38][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9176e-02 (1.0127e-01)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.95)
Epoch: [38][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0111e-01 (1.0113e-01)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.95)
Epoch: [38][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.3612e-02 (1.0183e-01)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.95)
Epoch: [38][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2578e-01 (1.0201e-01)	Acc@1  96.88 ( 96.93)	Acc@5  99.22 ( 99.94)
Epoch: [38][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5214e-02 (1.0210e-01)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.95)
Epoch: [38][300/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1309e-02 (1.0235e-01)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.95)
Epoch: [38][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6974e-02 (1.0261e-01)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.95)
Epoch: [38][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0411e-01 (1.0290e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.95)
Epoch: [38][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4219e-02 (1.0312e-01)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.95)
Epoch: [38][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.8766e-02 (1.0347e-01)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.95)
Epoch: [38][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0201e-01 (1.0415e-01)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.95)
Epoch: [38][360/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5426e-01 (1.0477e-01)	Acc@1  96.88 ( 96.83)	Acc@5 100.00 ( 99.95)
Epoch: [38][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1561e-01 (1.0483e-01)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.95)
Epoch: [38][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4113e-02 (1.0528e-01)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.95)
Epoch: [38][390/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6757e-01 (1.0613e-01)	Acc@1  95.00 ( 96.76)	Acc@5 100.00 ( 99.95)
## e[38] optimizer.zero_grad (sum) time: 0.13788843154907227
## e[38]       loss.backward (sum) time: 2.5647716522216797
## e[38]      optimizer.step (sum) time: 1.039665699005127
## epoch[38] training(only) time: 18.443089246749878
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.6005e+00 (1.6005e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.036)	Loss 1.6216e+00 (1.5552e+00)	Acc@1  72.00 ( 70.27)	Acc@5  90.00 ( 90.64)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.6890e+00 (1.5275e+00)	Acc@1  70.00 ( 70.52)	Acc@5  90.00 ( 90.52)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.1965e+00 (1.5878e+00)	Acc@1  60.00 ( 69.58)	Acc@5  87.00 ( 90.00)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5022e+00 (1.5483e+00)	Acc@1  68.00 ( 69.56)	Acc@5  94.00 ( 90.46)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.7524e+00 (1.5705e+00)	Acc@1  67.00 ( 69.27)	Acc@5  88.00 ( 90.08)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4471e+00 (1.5424e+00)	Acc@1  68.00 ( 69.57)	Acc@5  94.00 ( 90.31)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.8920e+00 (1.5420e+00)	Acc@1  64.00 ( 69.54)	Acc@5  89.00 ( 90.39)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5984e+00 (1.5493e+00)	Acc@1  73.00 ( 69.43)	Acc@5  91.00 ( 90.33)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.9094e+00 (1.5273e+00)	Acc@1  61.00 ( 69.68)	Acc@5  88.00 ( 90.52)
 * Acc@1 69.710 Acc@5 90.720
### epoch[38] execution time: 20.940560340881348
EPOCH 39
i:   0, name:          module.conv1.0.weight  changing lr from: 0.008396515016716099   to: 0.006712584379435493
i:   1, name:          module.conv1.1.weight  changing lr from: 0.009230596128661887   to: 0.007475494010603705
i:   2, name:            module.conv1.1.bias  changing lr from: 0.010090796083986858   to: 0.008269501785001587
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.010974724864738838   to: 0.009092034775289103
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.011880105318585899   to: 0.009940633788988761
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.012804770506065870   to: 0.010812951358434707
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.013746660860401921   to: 0.011706749461037936
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.014703821197347082   to: 0.012619897014569708
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.015674397608478868   to: 0.013550367187637725
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.016656634267659648   to: 0.014496234561356819
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.017648870176992040   to: 0.015455672174379688
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.018649535875509732   to: 0.016426948479937670
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.019657150131031038   to: 0.017408424240319381
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.020670316633048233   to: 0.018398549381276798
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.021687720702208563   to: 0.019395859826166189
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.022708126029847776   to: 0.020398974327194164
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.023730371459144500   to: 0.021406591308924940
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.024753367817759588   to: 0.022417485737199684
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.025776094810293068   to: 0.023430506024806664
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.026797597977515943   to: 0.024444570983603382
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.027816985728107882   to: 0.025458666831321877
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.028833426447531854   to: 0.026471844259963691
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.029846145687704273   to: 0.027483215571507316
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.030854423440251556   to: 0.028491951885591595
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.031857591495378715   to: 0.029497280422893343
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.032855030887701933   to: 0.030498481867079763
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.033846169429802862   to: 0.031494887807470187
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.034830479333745966   to: 0.032485878263886346
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.035807474920349025   to: 0.033470879294591499
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.036776710415606484   to: 0.034449360687710616
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.037737777833330537   to: 0.035420833736083270
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.038690304942787808   to: 0.036384849095115003
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.039633953319867042   to: 0.037340994722862064
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.040568416480110725   to: 0.038288893901300303
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.041493418091773732   to: 0.039228203337485716
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.042408710266937079   to: 0.040158611343111786
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.043314071928593843   to: 0.041079836090797302
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.044209307251541312   to: 0.041991623945299879
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.045094244174848891   to: 0.042893747867736629
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.045968732983628705   to: 0.043786005890805675
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.046832644957808006   to: 0.044668219662933994
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.047685871085590137   to: 0.045540233059228608
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.048528320839291809   to: 0.046401910857076241
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.049359921011254221   to: 0.047253137474217582
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.050180614607548246   to: 0.048093815767118353
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.050990359797221420   to: 0.048923865887464411
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.051789128914870669   to: 0.049743224194622793
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.052576907514366777   to: 0.050551842221935463
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.053353693471602105   to: 0.051349685694740604
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.054119496134184299   to: 0.052136733598054645
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.054874335516051302   to: 0.052912977291887181
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.055618241535039636   to: 0.053678419672207281
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.056351253291495464   to: 0.054433074375628013
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.057073418386076602   to: 0.055176965025925721
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.057784792274954901   to: 0.055910124520565568
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.058485437660687249   to: 0.056632594355457436
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.059175423917085260   to: 0.057344423986223129
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.059854826546473852   to: 0.058045670224311667
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.060523726667788408   to: 0.058736396666355842
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.061182210534019649   to: 0.059416673155219479
i:  60, name:               module.fc.weight  changing lr from: 0.061830369077574082   to: 0.060086575271241761
i:  61, name:                 module.fc.bias  changing lr from: 0.062468297482174820   to: 0.060746183852240093



# Switched to train mode...
Epoch: [39][  0/391]	Time  0.195 ( 0.195)	Data  0.147 ( 0.147)	Loss 1.0792e-01 (1.0792e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.015)	Loss 7.1688e-02 (8.5036e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 5.3951e-02 (8.3029e-02)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 9.9517e-02 (8.7359e-02)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 8.5791e-02 (8.6943e-02)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 6.3748e-02 (8.6677e-02)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [39][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.4818e-02 (8.7495e-02)	Acc@1  96.88 ( 97.26)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.3460e-02 (8.7846e-02)	Acc@1  99.22 ( 97.27)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 80/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.7548e-02 (8.9896e-02)	Acc@1  96.09 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.3130e-02 (8.9692e-02)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.97)
Epoch: [39][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.5143e-02 (8.8197e-02)	Acc@1  99.22 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [39][110/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.7237e-02 (8.7373e-02)	Acc@1  99.22 ( 97.49)	Acc@5 100.00 ( 99.97)
Epoch: [39][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.9487e-02 (8.7133e-02)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [39][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.9975e-02 (8.6966e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [39][140/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.004)	Loss 8.2341e-02 (8.7334e-02)	Acc@1  98.44 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [39][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0889e-02 (8.5887e-02)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 ( 99.98)
Epoch: [39][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4339e-02 (8.5130e-02)	Acc@1  96.09 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [39][170/391]	Time  0.052 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7789e-02 (8.5041e-02)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [39][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.6358e-02 (8.5597e-02)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [39][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1053e-01 (8.6045e-02)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 ( 99.98)
Epoch: [39][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0358e-01 (8.6269e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [39][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1007e-01 (8.6499e-02)	Acc@1  96.09 ( 97.50)	Acc@5 100.00 ( 99.99)
Epoch: [39][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5311e-01 (8.7077e-02)	Acc@1  95.31 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [39][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1209e-01 (8.6850e-02)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [39][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.1592e-02 (8.6857e-02)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [39][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9070e-02 (8.6835e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [39][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9906e-02 (8.7090e-02)	Acc@1  99.22 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [39][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.0570e-02 (8.7531e-02)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [39][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2391e-02 (8.7921e-02)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [39][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.9696e-02 (8.8222e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.98)
Epoch: [39][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0827e-02 (8.9230e-02)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.98)
Epoch: [39][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0237e-02 (8.9471e-02)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.98)
Epoch: [39][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.7985e-02 (8.9700e-02)	Acc@1  96.09 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [39][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5652e-02 (9.0015e-02)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.98)
Epoch: [39][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1008e-02 (8.9874e-02)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [39][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2677e-02 (9.0479e-02)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.98)
Epoch: [39][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3084e-01 (9.0695e-02)	Acc@1  96.09 ( 97.30)	Acc@5 100.00 ( 99.98)
Epoch: [39][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1326e-02 (9.1059e-02)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.98)
Epoch: [39][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0453e-02 (9.1778e-02)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.98)
Epoch: [39][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8242e-01 (9.2424e-02)	Acc@1  93.75 ( 97.23)	Acc@5 100.00 ( 99.98)
## e[39] optimizer.zero_grad (sum) time: 0.13927721977233887
## e[39]       loss.backward (sum) time: 2.559615135192871
## e[39]      optimizer.step (sum) time: 1.0344045162200928
## epoch[39] training(only) time: 18.426477193832397
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.5516e+00 (1.5516e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.6455e+00 (1.4891e+00)	Acc@1  71.00 ( 71.55)	Acc@5  91.00 ( 91.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.4018e+00 (1.4643e+00)	Acc@1  72.00 ( 71.48)	Acc@5  92.00 ( 91.33)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.1152e+00 (1.4992e+00)	Acc@1  60.00 ( 70.65)	Acc@5  89.00 ( 91.26)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.5291e+00 (1.4911e+00)	Acc@1  72.00 ( 70.44)	Acc@5  91.00 ( 91.56)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.8627e+00 (1.5082e+00)	Acc@1  66.00 ( 70.20)	Acc@5  87.00 ( 91.06)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3196e+00 (1.4828e+00)	Acc@1  66.00 ( 70.38)	Acc@5  93.00 ( 91.25)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.7189e+00 (1.4917e+00)	Acc@1  67.00 ( 70.51)	Acc@5  91.00 ( 91.15)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.7001e+00 (1.5016e+00)	Acc@1  69.00 ( 70.58)	Acc@5  89.00 ( 91.02)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.5771e+00 (1.4834e+00)	Acc@1  73.00 ( 70.89)	Acc@5  90.00 ( 91.15)
 * Acc@1 70.960 Acc@5 91.240
### epoch[39] execution time: 20.905161380767822
EPOCH 40
i:   0, name:          module.conv1.0.weight  changing lr from: 0.006712584379435493   to: 0.005234426044027662
i:   1, name:          module.conv1.1.weight  changing lr from: 0.007475494010603705   to: 0.005917797026905130
i:   2, name:            module.conv1.1.bias  changing lr from: 0.008269501785001587   to: 0.006637408074440770
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.009092034775289103   to: 0.007390513156642847
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.009940633788988761   to: 0.008174479217938756
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.010812951358434707   to: 0.008986784955803951
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.011706749461037936   to: 0.009825019237662817
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.012619897014569708   to: 0.010686879208406462
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.013550367187637725   to: 0.011570168135877308
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.014496234561356819   to: 0.012472793037039326
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.015455672174379688   to: 0.013392762123273304
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.016426948479937670   to: 0.014328182099291258
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.017408424240319381   to: 0.015277255346530749
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.018398549381276798   to: 0.016238277018558272
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.019395859826166189   to: 0.017209632072953764
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.020398974327194164   to: 0.018189792261356419
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.021406591308924940   to: 0.019177313096802401
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.022417485737199684   to: 0.020170830815163396
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.023430506024806664   to: 0.021169059345385894
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.024444570983603382   to: 0.022170787301315276
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.025458666831321877   to: 0.023174875006158658
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.026471844259963691   to: 0.024180251559072521
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.027483215571507316   to: 0.025185911951952328
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.028491951885591595   to: 0.026190914243231209
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.029497280422893343   to: 0.027194376794355515
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.030498481867079763   to: 0.028195475573586638
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.031494887807470187   to: 0.029193441530866501
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.032485878263886346   to: 0.030187558046674279
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.033470879294591499   to: 0.031177158457081990
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.034449360687710616   to: 0.032161623656578535
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.035420833736083270   to: 0.033140379779670472
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.036384849095115003   to: 0.034112895961773519
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.037340994722862064   to: 0.035078682179476399
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.038288893901300303   to: 0.036037287169883407
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.039228203337485716   to: 0.036988296428414107
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.040158611343111786   to: 0.037931330284160465
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.041079836090797302   to: 0.038866042051659888
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.041991623945299879   to: 0.039792116257741351
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.042893747867736629   to: 0.040709266941929972
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.043786005890805675   to: 0.041617236028755965
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.044668219662933994   to: 0.042515791770198187
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.045540233059228608   to: 0.043404727256401272
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.046401910857076241   to: 0.044283858992735355
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.047253137474217582   to: 0.045153025541213282
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.048093815767118353   to: 0.046012086224245505
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.048923865887464411   to: 0.046860919888689581
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.049743224194622793   to: 0.047699423728141274
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.050551842221935463   to: 0.048527512161416264
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.051349685694740604   to: 0.049345115765180310
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.052136733598054645   to: 0.050152180258705645
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.052912977291887181   to: 0.050948665538755308
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.053678419672207281   to: 0.051734544762629266
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.054433074375628013   to: 0.052509803477442635
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.055176965025925721   to: 0.053274438793744855
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.055910124520565568   to: 0.054028458601635403
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.056632594355457436   to: 0.054771880827574615
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.057344423986223129   to: 0.055504732730139707
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.058045670224311667   to: 0.056227050233024815
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.058736396666355842   to: 0.056938877293635774
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.059416673155219479   to: 0.057640265305682427
i:  60, name:               module.fc.weight  changing lr from: 0.060086575271241761   to: 0.058331272534224224
i:  61, name:                 module.fc.bias  changing lr from: 0.060746183852240093   to: 0.059011963581677686



# Switched to train mode...
Epoch: [40][  0/391]	Time  0.195 ( 0.195)	Data  0.152 ( 0.152)	Loss 4.0477e-02 (4.0477e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.047 ( 0.060)	Data  0.001 ( 0.016)	Loss 5.4941e-02 (8.3507e-02)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.010)	Loss 9.9323e-02 (8.7182e-02)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 9.2112e-02 (8.3284e-02)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 7.0170e-02 (8.2515e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 5.6106e-02 (8.0751e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.5991e-01 (8.3093e-02)	Acc@1  96.88 ( 97.67)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 8.8454e-02 (8.2085e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.8109e-02 (7.9812e-02)	Acc@1  97.66 ( 97.77)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.3333e-02 (8.1248e-02)	Acc@1  96.88 ( 97.71)	Acc@5 100.00 ( 99.95)
Epoch: [40][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1892e-01 (8.1357e-02)	Acc@1  95.31 ( 97.69)	Acc@5 100.00 ( 99.95)
Epoch: [40][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.2249e-02 (8.1057e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.95)
Epoch: [40][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.9162e-02 (8.0822e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.95)
Epoch: [40][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.7370e-02 (8.0692e-02)	Acc@1  97.66 ( 97.70)	Acc@5 100.00 ( 99.96)
Epoch: [40][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.6743e-02 (8.0519e-02)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 ( 99.96)
Epoch: [40][150/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.9500e-02 (8.1020e-02)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.96)
Epoch: [40][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0391e-01 (8.1382e-02)	Acc@1  96.88 ( 97.67)	Acc@5 100.00 ( 99.97)
Epoch: [40][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0632e-02 (8.0572e-02)	Acc@1  99.22 ( 97.72)	Acc@5 100.00 ( 99.96)
Epoch: [40][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7038e-02 (8.0391e-02)	Acc@1 100.00 ( 97.73)	Acc@5 100.00 ( 99.97)
Epoch: [40][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.3487e-02 (8.0007e-02)	Acc@1  99.22 ( 97.74)	Acc@5 100.00 ( 99.97)
Epoch: [40][200/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.1754e-02 (7.9606e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.97)
Epoch: [40][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.8330e-02 (7.9276e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.97)
Epoch: [40][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2581e-02 (7.9197e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.97)
Epoch: [40][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9735e-02 (7.8927e-02)	Acc@1  99.22 ( 97.74)	Acc@5 100.00 ( 99.97)
Epoch: [40][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7074e-02 (7.9273e-02)	Acc@1  99.22 ( 97.74)	Acc@5 100.00 ( 99.97)
Epoch: [40][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4706e-02 (7.9552e-02)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [40][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2637e-02 (7.9434e-02)	Acc@1  99.22 ( 97.74)	Acc@5 100.00 ( 99.97)
Epoch: [40][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0030e-01 (7.9445e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.97)
Epoch: [40][280/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2932e-02 (7.9232e-02)	Acc@1  96.88 ( 97.76)	Acc@5 100.00 ( 99.96)
Epoch: [40][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2181e-02 (7.9837e-02)	Acc@1  99.22 ( 97.73)	Acc@5 100.00 ( 99.97)
Epoch: [40][300/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2393e-02 (7.9816e-02)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [40][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.4423e-02 (8.0237e-02)	Acc@1  96.88 ( 97.70)	Acc@5 100.00 ( 99.97)
Epoch: [40][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3869e-01 (8.0630e-02)	Acc@1  92.97 ( 97.68)	Acc@5 100.00 ( 99.97)
Epoch: [40][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7436e-02 (8.1182e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [40][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6682e-01 (8.1909e-02)	Acc@1  93.75 ( 97.64)	Acc@5 100.00 ( 99.96)
Epoch: [40][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.5325e-02 (8.2039e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.96)
Epoch: [40][360/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1042e-01 (8.2245e-02)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.96)
Epoch: [40][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.3518e-01 (8.2973e-02)	Acc@1  96.88 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [40][380/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3589e-02 (8.3312e-02)	Acc@1 100.00 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [40][390/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1494e-01 (8.3598e-02)	Acc@1  97.50 ( 97.61)	Acc@5 100.00 ( 99.96)
## e[40] optimizer.zero_grad (sum) time: 0.13854050636291504
## e[40]       loss.backward (sum) time: 2.5742297172546387
## e[40]      optimizer.step (sum) time: 1.0184786319732666
## epoch[40] training(only) time: 18.439421892166138
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.7081e+00 (1.7081e+00)	Acc@1  75.00 ( 75.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.6085e+00 (1.5158e+00)	Acc@1  68.00 ( 71.45)	Acc@5  89.00 ( 89.91)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.7890e+00 (1.4952e+00)	Acc@1  68.00 ( 70.86)	Acc@5  88.00 ( 90.67)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7983e+00 (1.5227e+00)	Acc@1  66.00 ( 70.48)	Acc@5  90.00 ( 90.55)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5471e+00 (1.5234e+00)	Acc@1  69.00 ( 70.39)	Acc@5  94.00 ( 90.90)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.8257e+00 (1.5321e+00)	Acc@1  67.00 ( 70.49)	Acc@5  88.00 ( 90.67)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.6105e+00 (1.4915e+00)	Acc@1  66.00 ( 70.77)	Acc@5  93.00 ( 91.05)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5795e+00 (1.4947e+00)	Acc@1  66.00 ( 70.59)	Acc@5  90.00 ( 91.14)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5966e+00 (1.5018e+00)	Acc@1  71.00 ( 70.51)	Acc@5  86.00 ( 91.07)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6522e+00 (1.4846e+00)	Acc@1  68.00 ( 70.70)	Acc@5  93.00 ( 91.13)
 * Acc@1 70.820 Acc@5 91.320
### epoch[40] execution time: 20.952916145324707
EPOCH 41
i:   0, name:          module.conv1.0.weight  changing lr from: 0.005234426044027662   to: 0.003968907966975204
i:   1, name:          module.conv1.1.weight  changing lr from: 0.005917797026905130   to: 0.004564570102839166
i:   2, name:            module.conv1.1.bias  changing lr from: 0.006637408074440770   to: 0.005201741479101468
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.007390513156642847   to: 0.005877515700445706
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.008174479217938756   to: 0.006589096775031183
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.008986784955803951   to: 0.007333798834722066
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.009825019237662817   to: 0.008109045392010811
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.010686879208406462   to: 0.008912368193925799
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.011570168135877308   to: 0.009741405727783186
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.012472793037039326   to: 0.010593901428578877
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.013392762123273304   to: 0.011467701633109430
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.014328182099291258   to: 0.012360753321546326
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.015277255346530749   to: 0.013271101683148469
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.016238277018558272   to: 0.014196887539071879
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.017209632072953764   to: 0.015136344651801449
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.018189792261356419   to: 0.016087796947575811
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.019177313096802401   to: 0.017049655675282268
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.020170830815163396   to: 0.018020416522650302
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.021169059345385894   to: 0.018998656708153806
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.022170787301315276   to: 0.019983032064824841
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.023174875006158658   to: 0.020972274130177073
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.024180251559072521   to: 0.021965187254612047
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.025185911951952328   to: 0.022960645739031411
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.026190914243231209   to: 0.023957591010883968
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.027194376794355515   to: 0.024955028846526478
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.028195475573586638   to: 0.025952026646565318
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.029193441530866501   to: 0.026947710769751412
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.030187558046674279   to: 0.027941263930025141
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.031177158457081990   to: 0.028931922660431642
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.032161623656578535   to: 0.029918974846845858
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.033140379779670472   to: 0.030901757333753811
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.034112895961773519   to: 0.031879653603719783
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.035078682179476399   to: 0.032852091531625431
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.036037287169883407   to: 0.033818541214288469
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.036988296428414107   to: 0.034778512875646921
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.037931330284160465   to: 0.035731554847329726
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.038866042051659888   to: 0.036677251624114886
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.039792116257741351   to: 0.037615221993501861
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.040709266941929972   to: 0.038545117238388556
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.041617236028755965   to: 0.039466619411643256
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.042515791770198187   to: 0.040379439681191898
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.043404727256401272   to: 0.041283316744101198
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.044283858992735355   to: 0.042178015308022315
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.045153025541213282   to: 0.043063324638266225
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.046012086224245505   to: 0.043939057168710015
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.046860919888689581   to: 0.044805047174677287
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.047699423728141274   to: 0.045661149505896385
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.048527512161416264   to: 0.046507238377615313
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.049345115765180310   to: 0.047343206217937001
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.050152180258705645   to: 0.048168962569437507
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.050948665538755308   to: 0.048984433043134226
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.051734544762629266   to: 0.049789558322886465
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.052509803477442635   to: 0.050584293218331869
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.053274438793744855   to: 0.051368605764487964
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.054028458601635403   to: 0.052142476366181995
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.054771880827574615   to: 0.052905896985506164
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.055504732730139707   to: 0.053658870370536509
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.056227050233024815   to: 0.054401409323595544
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.056938877293635774   to: 0.055133536007383403
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.057640265305682427   to: 0.055855281287348117
i:  60, name:               module.fc.weight  changing lr from: 0.058331272534224224   to: 0.056566684108714398
i:  61, name:                 module.fc.bias  changing lr from: 0.059011963581677686   to: 0.057267790906638208



# Switched to train mode...
Epoch: [41][  0/391]	Time  0.203 ( 0.203)	Data  0.160 ( 0.160)	Loss 4.0718e-02 (4.0718e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.046 ( 0.061)	Data  0.001 ( 0.017)	Loss 8.9235e-02 (6.7573e-02)	Acc@1  96.88 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 6.8342e-02 (7.2984e-02)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.008)	Loss 8.2104e-02 (6.9268e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 7.1402e-02 (6.8112e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.006)	Loss 6.3196e-02 (6.9513e-02)	Acc@1  98.44 ( 97.82)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.6812e-02 (6.9438e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 7.0770e-02 (6.7494e-02)	Acc@1  96.88 ( 97.95)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5533e-02 (6.6516e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.4822e-02 (6.7773e-02)	Acc@1 100.00 ( 98.01)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5807e-02 (6.7046e-02)	Acc@1 100.00 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 7.4872e-02 (6.6580e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 (100.00)
Epoch: [41][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.2505e-02 (6.5146e-02)	Acc@1 100.00 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [41][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 8.4874e-02 (6.6151e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 (100.00)
Epoch: [41][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.7963e-02 (6.5573e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [41][150/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.1759e-02 (6.4379e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [41][160/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.004)	Loss 3.3923e-02 (6.4082e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2509e-02 (6.4216e-02)	Acc@1 100.00 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1668e-01 (6.4560e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4443e-02 (6.4186e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.2528e-02 (6.4609e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7516e-02 (6.4379e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [41][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5270e-02 (6.4495e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [41][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2945e-02 (6.4730e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [41][240/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2322e-02 (6.4934e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [41][250/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9586e-02 (6.4506e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [41][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.3161e-02 (6.5125e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [41][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5287e-02 (6.5265e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [41][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0755e-01 (6.5639e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [41][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4815e-02 (6.5669e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [41][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7762e-02 (6.5692e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [41][310/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.1242e-02 (6.5785e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 (100.00)
Epoch: [41][320/391]	Time  0.043 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3105e-02 (6.6011e-02)	Acc@1 100.00 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [41][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2055e-02 (6.6582e-02)	Acc@1  98.44 ( 98.10)	Acc@5  99.22 ( 99.99)
Epoch: [41][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.3763e-02 (6.6816e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.99)
Epoch: [41][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4887e-02 (6.7343e-02)	Acc@1  96.88 ( 98.07)	Acc@5 100.00 ( 99.99)
Epoch: [41][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4987e-02 (6.7648e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 ( 99.99)
Epoch: [41][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.1241e-02 (6.8159e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 ( 99.99)
Epoch: [41][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2202e-01 (6.8553e-02)	Acc@1  95.31 ( 98.03)	Acc@5 100.00 ( 99.99)
Epoch: [41][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4268e-01 (6.9374e-02)	Acc@1  91.25 ( 97.99)	Acc@5 100.00 ( 99.99)
## e[41] optimizer.zero_grad (sum) time: 0.1385505199432373
## e[41]       loss.backward (sum) time: 2.566317081451416
## e[41]      optimizer.step (sum) time: 1.025827169418335
## epoch[41] training(only) time: 18.482367277145386
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.4882e+00 (1.4882e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.7238e+00 (1.5445e+00)	Acc@1  71.00 ( 72.00)	Acc@5  91.00 ( 90.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3786e+00 (1.4423e+00)	Acc@1  74.00 ( 72.81)	Acc@5  91.00 ( 91.38)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 2.0605e+00 (1.5019e+00)	Acc@1  59.00 ( 71.87)	Acc@5  90.00 ( 90.84)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4685e+00 (1.4920e+00)	Acc@1  69.00 ( 71.73)	Acc@5  93.00 ( 91.17)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6469e+00 (1.5113e+00)	Acc@1  70.00 ( 71.33)	Acc@5  90.00 ( 90.94)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.6971e+00 (1.4786e+00)	Acc@1  66.00 ( 71.66)	Acc@5  92.00 ( 91.23)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.8999e+00 (1.4912e+00)	Acc@1  67.00 ( 71.32)	Acc@5  88.00 ( 91.20)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5115e+00 (1.4981e+00)	Acc@1  72.00 ( 71.26)	Acc@5  91.00 ( 91.23)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6322e+00 (1.4749e+00)	Acc@1  72.00 ( 71.62)	Acc@5  91.00 ( 91.45)
 * Acc@1 71.620 Acc@5 91.620
### epoch[41] execution time: 20.971624851226807
EPOCH 42
i:   0, name:          module.conv1.0.weight  changing lr from: 0.003968907966975204   to: 0.002921910115851916
i:   1, name:          module.conv1.1.weight  changing lr from: 0.004564570102839166   to: 0.003421950790956768
i:   2, name:            module.conv1.1.bias  changing lr from: 0.005201741479101468   to: 0.003968858792748447
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.005877515700445706   to: 0.004559583106872416
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.006589096775031183   to: 0.005191178567049325
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.007333798834722066   to: 0.005860806675159580
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.008109045392010811   to: 0.006565735847380388
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.008912368193925799   to: 0.007303341154788695
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.009741405727783186   to: 0.008071103621029019
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.010593901428578877   to: 0.008866609134188196
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.011467701633109430   to: 0.009687547024913923
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.012360753321546326   to: 0.010531708358053705
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.013271101683148469   to: 0.011396983980659293
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.014196887539071879   to: 0.012281362365092068
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.015136344651801449   to: 0.013182927282157853
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.016087796947575811   to: 0.014099855335684437
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.017049655675282268   to: 0.015030413386714160
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.018020416522650302   to: 0.015972955892502951
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.018998656708153806   to: 0.016925922182782143
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.019983032064824841   to: 0.017887833693230677
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.020972274130177073   to: 0.018857291173815838
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.021965187254612047   to: 0.019832971887566605
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.022960645739031411   to: 0.020813626813440369
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.023957591010883968   to: 0.021798077865211565
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.024955028846526478   to: 0.022785215136738642
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.025952026646565318   to: 0.023773994182544569
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.026947710769751412   to: 0.024763433341359123
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.027941263930025141   to: 0.025752611109112502
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.028931922660431642   to: 0.026740663566827350
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.029918974846845858   to: 0.027726781867918528
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.030901757333753811   to: 0.028710209788572701
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.031879653603719783   to: 0.029690241344130230
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.032852091531625431   to: 0.030666218473724485
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.033818541214288469   to: 0.031637528794841795
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.034778512875646921   to: 0.032603603428939525
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.035731554847329726   to: 0.033563914898798064
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.036677251624114886   to: 0.034517975097874826
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.037615221993501861   to: 0.035465333331573382
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.038545117238388556   to: 0.036405574430029897
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.039466619411643256   to: 0.037338316931752095
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.040379439681191898   to: 0.038263211337213790
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.041283316744101198   to: 0.039179938431312122
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.042178015308022315   to: 0.040088207673427495
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.043063324638266225   to: 0.040987755653686155
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.043939057168710015   to: 0.041878344613911135
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.044805047174677287   to: 0.042759761031653069
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.045661149505896385   to: 0.043631814265618012
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.046507238377615313   to: 0.044494335260753609
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.047343206217937001   to: 0.045347175311211213
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.048168962569437507   to: 0.046190204879375342
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.048984433043134226   to: 0.047023312469133299
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.049789558322886465   to: 0.047846403551552848
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.050584293218331869   to: 0.048659399541138661
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.051368605764487964   to: 0.049462236820847438
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.052142476366181995   to: 0.050254865814061439
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.052905896985506164   to: 0.051037250101740921
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.053658870370536509   to: 0.051809365583005744
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.054401409323595544   to: 0.052571199677428487
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.055133536007383403   to: 0.053322750567356851
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.055855281287348117   to: 0.054064026478621879
i:  60, name:               module.fc.weight  changing lr from: 0.056566684108714398   to: 0.054795044998029854
i:  61, name:                 module.fc.bias  changing lr from: 0.057267790906638208   to: 0.055515832426078673



# Switched to train mode...
Epoch: [42][  0/391]	Time  0.193 ( 0.193)	Data  0.152 ( 0.152)	Loss 5.2510e-02 (5.2510e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.2900e-02 (6.7325e-02)	Acc@1 100.00 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.010)	Loss 5.9816e-02 (6.8964e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 7.0283e-02 (7.1222e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 9.0451e-02 (7.1712e-02)	Acc@1  98.44 ( 97.69)	Acc@5 100.00 (100.00)
Epoch: [42][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.7388e-02 (7.1024e-02)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 (100.00)
Epoch: [42][ 60/391]	Time  0.049 ( 0.049)	Data  0.001 ( 0.005)	Loss 8.3746e-02 (7.2039e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 (100.00)
Epoch: [42][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.8465e-02 (7.0084e-02)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.7682e-02 (6.9346e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 90/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.9709e-02 (6.8474e-02)	Acc@1 100.00 ( 97.87)	Acc@5 100.00 ( 99.99)
Epoch: [42][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.3968e-02 (6.7660e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 ( 99.99)
Epoch: [42][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.4642e-02 (6.6918e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.99)
Epoch: [42][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.3551e-02 (6.6594e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [42][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.2916e-01 (6.6599e-02)	Acc@1  96.88 ( 98.00)	Acc@5 100.00 ( 99.99)
Epoch: [42][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.3565e-02 (6.5945e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.99)
Epoch: [42][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 4.0773e-02 (6.5187e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 ( 99.99)
Epoch: [42][160/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9309e-02 (6.4431e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9169e-02 (6.4028e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.99)
Epoch: [42][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2239e-02 (6.3809e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [42][190/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.1689e-02 (6.3459e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [42][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5559e-02 (6.2666e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.99)
Epoch: [42][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3265e-02 (6.2330e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [42][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6606e-02 (6.2206e-02)	Acc@1  99.22 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [42][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1523e-02 (6.1538e-02)	Acc@1 100.00 ( 98.26)	Acc@5 100.00 ( 99.99)
Epoch: [42][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.4508e-02 (6.1170e-02)	Acc@1  97.66 ( 98.29)	Acc@5  99.22 ( 99.99)
Epoch: [42][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7614e-02 (6.0718e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [42][260/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7391e-02 (6.0650e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [42][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8537e-02 (6.0439e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [42][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8610e-02 (6.0550e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [42][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.9245e-02 (6.0626e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [42][300/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8996e-02 (6.0360e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [42][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2934e-02 (6.0441e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [42][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3703e-02 (6.0171e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [42][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.7613e-02 (6.0014e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [42][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.8016e-02 (6.0202e-02)	Acc@1  95.31 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [42][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8636e-02 (6.0080e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [42][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4774e-02 (5.9808e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [42][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.7483e-02 (5.9968e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [42][380/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1909e-01 (6.0276e-02)	Acc@1  95.31 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [42][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6586e-01 (6.0675e-02)	Acc@1  95.00 ( 98.31)	Acc@5 100.00 ( 99.99)
## e[42] optimizer.zero_grad (sum) time: 0.13920259475708008
## e[42]       loss.backward (sum) time: 2.559626579284668
## e[42]      optimizer.step (sum) time: 1.0349726676940918
## epoch[42] training(only) time: 18.488162517547607
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.5676e+00 (1.5676e+00)	Acc@1  70.00 ( 70.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5778e+00 (1.4629e+00)	Acc@1  70.00 ( 71.45)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.6031e+00 (1.3992e+00)	Acc@1  73.00 ( 72.43)	Acc@5  91.00 ( 91.71)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.9201e+00 (1.4493e+00)	Acc@1  60.00 ( 71.48)	Acc@5  88.00 ( 91.23)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3745e+00 (1.4404e+00)	Acc@1  73.00 ( 71.46)	Acc@5  93.00 ( 91.41)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.9095e+00 (1.4723e+00)	Acc@1  70.00 ( 70.96)	Acc@5  88.00 ( 91.02)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.5889e+00 (1.4443e+00)	Acc@1  70.00 ( 71.26)	Acc@5  91.00 ( 91.26)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.7780e+00 (1.4533e+00)	Acc@1  71.00 ( 71.32)	Acc@5  89.00 ( 91.15)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.7515e+00 (1.4714e+00)	Acc@1  65.00 ( 70.95)	Acc@5  89.00 ( 91.14)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.8003e+00 (1.4588e+00)	Acc@1  68.00 ( 71.23)	Acc@5  90.00 ( 91.20)
 * Acc@1 71.360 Acc@5 91.310
### epoch[42] execution time: 20.979387521743774
EPOCH 43
i:   0, name:          module.conv1.0.weight  changing lr from: 0.002921910115851916   to: 0.002098297149271610
i:   1, name:          module.conv1.1.weight  changing lr from: 0.003421950790956768   to: 0.002495121434254293
i:   2, name:            module.conv1.1.bias  changing lr from: 0.003968858792748447   to: 0.002944218929727698
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.004559583106872416   to: 0.003442412809127398
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.005191178567049325   to: 0.003986625389705938
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.005860806675159580   to: 0.004573880214226949
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.006565735847380388   to: 0.005201303439699777
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.007303341154788695   to: 0.005866124609723960
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.008071103621029019   to: 0.006565676880883897
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.008866609134188196   to: 0.007297396767841107
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.009687547024913923   to: 0.008058823466314247
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.010531708358053705   to: 0.008847597808016198
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.011396983980659293   to: 0.009661460896821971
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.012281362365092068   to: 0.010498252470968077
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.013182927282157853   to: 0.011355909031917640
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.014099855335684437   to: 0.012232461776658606
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.015030413386714160   to: 0.013126034366618744
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.015972955892502951   to: 0.014034840563068418
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.016925922182782143   to: 0.014957181755828265
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.017887833693230677   to: 0.015891444409284416
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.018857291173815838   to: 0.016836097447132610
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.019832971887566605   to: 0.017789689594901344
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.020813626813440369   to: 0.018750846697138286
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.021798077865211565   to: 0.019718269024163439
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.022785215136738642   to: 0.020690728581486802
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.023773994182544569   to: 0.021667066433347693
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.024763433341359123   to: 0.022646190050339132
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.025752611109112502   to: 0.023627070689730380
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.026740663566827350   to: 0.024608740815877661
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.027726781867918528   to: 0.025590291567007952
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.028710209788572701   to: 0.026570870273667870
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.029690241344130230   to: 0.027549678033235315
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.030666218473724485   to: 0.028525967344089482
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.031637528794841795   to: 0.029499039802320194
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.032603603428939525   to: 0.030468243863215290
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.033563914898798064   to: 0.031432972669199089
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.034517975097874826   to: 0.032392661945388299
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.035465333331573382   to: 0.033346787963488021
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.036405574430029897   to: 0.034294865574356882
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.037338316931752095   to: 0.035236446309227232
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.038263211337213790   to: 0.036171116549266073
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.039179938431312122   to: 0.037098495762901487
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.040088207673427495   to: 0.038018234810115840
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.040987755653686155   to: 0.038930014312712993
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.041878344613911135   to: 0.039833543089404760
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.042759761031653069   to: 0.040728556654423868
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.043631814265618012   to: 0.041614815778256473
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.044494335260753609   to: 0.042492105108994815
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.045347175311211213   to: 0.043360231852735083
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.046190204879375342   to: 0.044219024511388663
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.047023312469133299   to: 0.045068331676230630
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.047846403551552848   to: 0.045908020875479405
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.048659399541138661   to: 0.046737977474183366
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.049462236820847438   to: 0.047558103624679410
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.050254865814061439   to: 0.048368317265891379
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.051037250101740921   to: 0.049168551169740719
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.051809365583005744   to: 0.049958752032958492
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.052571199677428487   to: 0.050738879612606771
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.053322750567356851   to: 0.051508905903642627
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.054064026478621879   to: 0.052268814356886267
i:  60, name:               module.fc.weight  changing lr from: 0.054795044998029854   to: 0.053018599135788692
i:  61, name:                 module.fc.bias  changing lr from: 0.055515832426078673   to: 0.053758264410428684



# Switched to train mode...
Epoch: [43][  0/391]	Time  0.188 ( 0.188)	Data  0.146 ( 0.146)	Loss 4.8965e-02 (4.8965e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.016)	Loss 5.8837e-02 (5.7597e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 20/391]	Time  0.045 ( 0.054)	Data  0.001 ( 0.009)	Loss 3.9065e-02 (6.2276e-02)	Acc@1 100.00 ( 98.55)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 5.5278e-02 (6.0809e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 40/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.4562e-02 (5.8968e-02)	Acc@1  98.44 ( 98.55)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 5.7087e-02 (5.6413e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.98)
Epoch: [43][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.7160e-02 (5.5036e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.5466e-02 (5.4430e-02)	Acc@1 100.00 ( 98.67)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 80/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.6633e-02 (5.5020e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][ 90/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.7216e-02 (5.4887e-02)	Acc@1  98.44 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.4349e-02 (5.3190e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 9.7085e-02 (5.3628e-02)	Acc@1  96.88 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [43][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.2933e-02 (5.3639e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.0635e-02 (5.3284e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.0865e-02 (5.3406e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.004)	Loss 7.0555e-02 (5.2996e-02)	Acc@1  97.66 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7487e-02 (5.2613e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [43][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0598e-02 (5.3189e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [43][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0203e-02 (5.2785e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [43][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0575e-02 (5.2590e-02)	Acc@1 100.00 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5902e-02 (5.2173e-02)	Acc@1 100.00 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3024e-02 (5.2408e-02)	Acc@1  97.66 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2066e-02 (5.2279e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][230/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3939e-02 (5.2018e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][240/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8444e-02 (5.1993e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7169e-02 (5.2326e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [43][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.9040e-02 (5.2416e-02)	Acc@1  96.88 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [43][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5084e-02 (5.2602e-02)	Acc@1  97.66 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][280/391]	Time  0.050 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6472e-02 (5.2487e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [43][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.2653e-02 (5.2515e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [43][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.4228e-02 (5.2471e-02)	Acc@1  96.88 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [43][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.2423e-02 (5.2624e-02)	Acc@1  96.09 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.9112e-02 (5.2792e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [43][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2380e-02 (5.2801e-02)	Acc@1  96.88 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [43][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2831e-02 (5.2826e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [43][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3878e-02 (5.3178e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [43][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6713e-02 (5.3098e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [43][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7316e-02 (5.3429e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [43][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8899e-02 (5.3444e-02)	Acc@1  97.66 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [43][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.5893e-02 (5.3480e-02)	Acc@1  96.25 ( 98.56)	Acc@5 100.00 ( 99.99)
## e[43] optimizer.zero_grad (sum) time: 0.1381528377532959
## e[43]       loss.backward (sum) time: 2.5570261478424072
## e[43]      optimizer.step (sum) time: 1.0235438346862793
## epoch[43] training(only) time: 18.460360288619995
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.5424e+00 (1.5424e+00)	Acc@1  72.00 ( 72.00)	Acc@5  96.00 ( 96.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.4960e+00 (1.4945e+00)	Acc@1  72.00 ( 71.45)	Acc@5  93.00 ( 91.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.3536e+00 (1.4237e+00)	Acc@1  75.00 ( 72.86)	Acc@5  92.00 ( 91.57)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.1091e+00 (1.4908e+00)	Acc@1  63.00 ( 72.00)	Acc@5  86.00 ( 91.19)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.2649e+00 (1.4849e+00)	Acc@1  78.00 ( 72.12)	Acc@5  94.00 ( 91.34)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.7797e+00 (1.5103e+00)	Acc@1  74.00 ( 71.86)	Acc@5  89.00 ( 91.12)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.6189e+00 (1.4823e+00)	Acc@1  68.00 ( 72.21)	Acc@5  91.00 ( 91.28)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.8306e+00 (1.4790e+00)	Acc@1  68.00 ( 72.21)	Acc@5  87.00 ( 91.25)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5175e+00 (1.4806e+00)	Acc@1  70.00 ( 72.11)	Acc@5  91.00 ( 91.28)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.7694e+00 (1.4579e+00)	Acc@1  69.00 ( 72.44)	Acc@5  90.00 ( 91.44)
 * Acc@1 72.340 Acc@5 91.550
### epoch[43] execution time: 20.965450763702393
EPOCH 44
i:   0, name:          module.conv1.0.weight  changing lr from: 0.002098297149271610   to: 0.001501895814259612
i:   1, name:          module.conv1.1.weight  changing lr from: 0.002495121434254293   to: 0.001788285661688099
i:   2, name:            module.conv1.1.bias  changing lr from: 0.002944218929727698   to: 0.002132358754178525
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.003442412809127398   to: 0.002530834343395077
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.003986625389705938   to: 0.002980521819612169
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.004573880214226949   to: 0.003478324218212630
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.005201303439699777   to: 0.004021240906776196
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.005866124609723960   to: 0.004606369537346932
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.006565676880883897   to: 0.005230907342116919
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.007297396767841107   to: 0.005892151844709276
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.008058823466314247   to: 0.006587501053497330
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.008847597808016198   to: 0.007314453197966270
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.009661460896821971   to: 0.008070606064004514
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.010498252470968077   to: 0.008853655979208001
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.011355909031917640   to: 0.009661396494779529
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.012232461776658606   to: 0.010491716806404622
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.013126034366618744   to: 0.011342599952571829
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.014034840563068418   to: 0.012212120825169474
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.014957181755828265   to: 0.013098444023821716
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.015891444409284416   to: 0.013999821582307958
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.016836097447132610   to: 0.014914590592534600
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.017789689594901344   to: 0.015841170748875616
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.018750846697138286   to: 0.016778061833264215
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.019718269024163439   to: 0.017723841159181315
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.020690728581486802   to: 0.018677160990638941
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.021667066433347693   to: 0.019636745950386947
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.022646190050339132   to: 0.020601390429861938
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.023627070689730380   to: 0.021569956011844218
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.024608740815877661   to: 0.022541368915374733
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.025590291567007952   to: 0.023514617471200989
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.026570870273667870   to: 0.024488749634861848
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.027549678033235315   to: 0.025462870543470312
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.028525967344089482   to: 0.026436140121308641
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.029499039802320194   to: 0.027407770738499662
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.030468243863215290   to: 0.028377024926252128
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.031432972669199089   to: 0.029343213151497086
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.032392661945388299   to: 0.030305691653118222
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.033346787963488021   to: 0.031263860341437695
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.034294865574356882   to: 0.032217160762133615
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.035236446309227232   to: 0.033165074125339748
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.036171116549266073   to: 0.034107119400299507
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.037098495762901487   to: 0.035042851475615852
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.038018234810115840   to: 0.035971859384849807
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.038930014312712993   to: 0.036893764596966944
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.039833543089404760   to: 0.037808219370915351
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.040728556654423868   to: 0.038714905173430403
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.041614815778256473   to: 0.039613531159002541
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.042492105108994815   to: 0.040503832710810872
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.043360231852735083   to: 0.041385570041311444
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.044219024511388663   to: 0.042258526851079607
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.045068331676230630   to: 0.043122509044428770
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.045908020875479405   to: 0.043977343500271895
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.046737977474183366   to: 0.044822876896647067
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.047558103624679410   to: 0.045658974587296085
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.048368317265891379   to: 0.046485519528666225
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.049168551169740719   to: 0.047302411255692470
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.049958752032958492   to: 0.048109564904716774
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.050738879612606771   to: 0.048906910281905365
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.051508905903642627   to: 0.049694390975537073
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.052268814356886267   to: 0.050471963510552137
i:  60, name:               module.fc.weight  changing lr from: 0.053018599135788692   to: 0.051239596543774052
i:  61, name:                 module.fc.bias  changing lr from: 0.053758264410428684   to: 0.051997270098242579



# Switched to train mode...
Epoch: [44][  0/391]	Time  0.189 ( 0.189)	Data  0.136 ( 0.136)	Loss 1.1238e-01 (1.1238e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.046 ( 0.059)	Data  0.001 ( 0.014)	Loss 4.3412e-02 (4.9606e-02)	Acc@1  97.66 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.047 ( 0.053)	Data  0.001 ( 0.009)	Loss 2.8993e-02 (4.5844e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 4.2575e-02 (4.6842e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 5.2488e-02 (4.7068e-02)	Acc@1  96.88 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.2825e-02 (4.7131e-02)	Acc@1 100.00 ( 98.68)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.8813e-02 (4.7137e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [44][ 70/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.1939e-02 (4.6808e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 80/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.9068e-02 (4.6341e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 90/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 6.8118e-02 (4.5156e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [44][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.5287e-02 (4.5503e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [44][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6110e-02 (4.4936e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [44][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.4729e-02 (4.4629e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [44][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.6353e-02 (4.4356e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [44][140/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6829e-02 (4.3903e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [44][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5064e-02 (4.3798e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [44][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8793e-02 (4.3907e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.5980e-02 (4.3612e-02)	Acc@1  97.66 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [44][180/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0619e-02 (4.3224e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [44][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7452e-02 (4.2748e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [44][200/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1415e-02 (4.2423e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [44][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.0611e-02 (4.2743e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [44][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6090e-02 (4.2272e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [44][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6852e-02 (4.2148e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [44][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1578e-02 (4.2440e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [44][250/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.0920e-02 (4.2272e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [44][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.6617e-02 (4.2909e-02)	Acc@1  98.44 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [44][270/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3243e-02 (4.2991e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [44][280/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7146e-02 (4.3046e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [44][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3738e-02 (4.2991e-02)	Acc@1  96.88 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [44][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.4524e-02 (4.3164e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [44][310/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8723e-02 (4.3341e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [44][320/391]	Time  0.046 ( 0.047)	Data  0.002 ( 0.003)	Loss 1.1746e-01 (4.3859e-02)	Acc@1  96.88 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.9275e-02 (4.3940e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1206e-02 (4.4667e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3593e-02 (4.4601e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [44][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.5286e-02 (4.4938e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.0946e-02 (4.5203e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [44][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 9.6510e-02 (4.5256e-02)	Acc@1  97.66 ( 98.82)	Acc@5  99.22 ( 99.99)
Epoch: [44][390/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.1250e-02 (4.5328e-02)	Acc@1  98.75 ( 98.82)	Acc@5 100.00 ( 99.99)
## e[44] optimizer.zero_grad (sum) time: 0.13842082023620605
## e[44]       loss.backward (sum) time: 2.5604872703552246
## e[44]      optimizer.step (sum) time: 1.0352540016174316
## epoch[44] training(only) time: 18.406741619110107
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.4040e+00 (1.4040e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.5764e+00 (1.3912e+00)	Acc@1  70.00 ( 73.27)	Acc@5  90.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.5070e+00 (1.3600e+00)	Acc@1  73.00 ( 73.48)	Acc@5  91.00 ( 92.00)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.1832e+00 (1.4161e+00)	Acc@1  64.00 ( 72.94)	Acc@5  88.00 ( 91.32)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4541e+00 (1.4167e+00)	Acc@1  74.00 ( 72.39)	Acc@5  94.00 ( 91.51)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6127e+00 (1.4371e+00)	Acc@1  71.00 ( 71.80)	Acc@5  92.00 ( 91.43)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.6214e+00 (1.4152e+00)	Acc@1  67.00 ( 72.11)	Acc@5  91.00 ( 91.69)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5479e+00 (1.4139e+00)	Acc@1  70.00 ( 71.94)	Acc@5  90.00 ( 91.69)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5251e+00 (1.4178e+00)	Acc@1  71.00 ( 71.95)	Acc@5  89.00 ( 91.63)
Test: [ 90/100]	Time  0.026 ( 0.024)	Loss 1.5631e+00 (1.3969e+00)	Acc@1  70.00 ( 72.14)	Acc@5  91.00 ( 91.75)
 * Acc@1 72.180 Acc@5 91.900
### epoch[44] execution time: 20.88590931892395
EPOCH 45
i:   0, name:          module.conv1.0.weight  changing lr from: 0.001501895814259612   to: 0.001135477166065490
i:   1, name:          module.conv1.1.weight  changing lr from: 0.001788285661688099   to: 0.001304649322640771
i:   2, name:            module.conv1.1.bias  changing lr from: 0.002132358754178525   to: 0.001536872991867038
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.002530834343395077   to: 0.001828788470716202
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.002980521819612169   to: 0.002177114751614250
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.003478324218212630   to: 0.002578654616108072
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.004021240906776196   to: 0.003030298776413725
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.004606369537346932   to: 0.003529029157128276
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.005230907342116919   to: 0.004071921402926170
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.005892151844709276   to: 0.004656146691841794
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.006587501053497330   to: 0.005278972927785502
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.007314453197966270   to: 0.005937765380267184
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.008070606064004514   to: 0.006629986833915097
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.008853655979208001   to: 0.007353197305287373
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.009661396494779529   to: 0.008105053379674949
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.010491716806404622   to: 0.008883307216088435
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.011342599952571829   to: 0.009685805264400809
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.012212120825169474   to: 0.010510486734673908
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.013098444023821716   to: 0.011355381855025298
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.013999821582307958   to: 0.012218609950974534
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.014914590592534600   to: 0.013098377376044142
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.015841170748875616   to: 0.013992975320457419
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.016778061833264215   to: 0.014900777522070287
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.017723841159181315   to: 0.015820237901179875
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.018677160990638941   to: 0.016749888138556580
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.019636745950386947   to: 0.017688335213941982
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.020601390429861938   to: 0.018634258920321774
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.021569956011844218   to: 0.019586409367518605
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.022541368915374733   to: 0.020543604487036946
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.023514617471200989   to: 0.021504727548621778
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.024488749634861848   to: 0.022468724697657119
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.025462870543470312   to: 0.023434602521316079
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.026436140121308641   to: 0.024401425650274030
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.027407770738499662   to: 0.025368314401803174
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.028377024926252128   to: 0.026334442469166670
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.029343213151497086   to: 0.027299034661424727
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.030305691653118222   to: 0.028261364697035819
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.031263860341437695   to: 0.029220753053988014
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.032217160762133615   to: 0.030176564878609802
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.033165074125339748   to: 0.031128207954693978
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.034107119400299507   to: 0.032075130734103653
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.035042851475615852   to: 0.033016820429622597
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.035971859384849807   to: 0.033952801170449902
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.036893764596966944   to: 0.034882632220420928
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.037808219370915351   to: 0.035805906258759644
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.038714905173430403   to: 0.036722247722923373
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.039613531159002541   to: 0.037631311212892093
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.040503832710810872   to: 0.038532779956073314
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.041385570041311444   to: 0.039426364331838479
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.042258526851079607   to: 0.040311800454577440
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.043122509044428770   to: 0.041188848814046448
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.043977343500271895   to: 0.042057292971696088
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.044822876896647067   to: 0.042916938311592268
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.045658974587296085   to: 0.043767610844484016
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.046485519528666225   to: 0.044609156063529748
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.047302411255692470   to: 0.045441437850158833
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.048109564904716774   to: 0.046264337428525190
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.048906910281905365   to: 0.047077752366996857
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.049694390975537073   to: 0.047881595625121259
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.050471963510552137   to: 0.048675794644508864
i:  60, name:               module.fc.weight  changing lr from: 0.051239596543774052   to: 0.049460290482087658
i:  61, name:                 module.fc.bias  changing lr from: 0.051997270098242579   to: 0.050235036984196041



# Switched to train mode...
Epoch: [45][  0/391]	Time  0.209 ( 0.209)	Data  0.166 ( 0.166)	Loss 4.9282e-02 (4.9282e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.046 ( 0.061)	Data  0.001 ( 0.017)	Loss 3.1831e-02 (3.2352e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.010)	Loss 8.8721e-02 (3.4213e-02)	Acc@1  96.88 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.046 ( 0.052)	Data  0.001 ( 0.008)	Loss 4.3714e-02 (3.3281e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 1.2305e-02 (3.4811e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.4121e-02 (3.5886e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 3.6176e-02 (3.6014e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 4.7381e-02 (3.5325e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.005)	Loss 3.5438e-02 (3.5293e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 5.7903e-02 (3.4763e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [45][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4265e-02 (3.3925e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [45][110/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.7872e-02 (3.3855e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [45][120/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.1597e-02 (3.3275e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [45][130/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.4847e-02 (3.3352e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [45][140/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1237e-02 (3.3879e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [45][150/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0232e-02 (3.3401e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [45][160/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.004)	Loss 4.6626e-02 (3.3779e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3845e-02 (3.4091e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6626e-02 (3.4123e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [45][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.7026e-02 (3.4700e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [45][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.6526e-02 (3.5171e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [45][210/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4577e-02 (3.5451e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [45][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.6149e-02 (3.5455e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [45][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.5701e-02 (3.5892e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [45][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2836e-02 (3.5806e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [45][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.8309e-02 (3.6062e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [45][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7688e-02 (3.6007e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [45][270/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1279e-02 (3.6187e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [45][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8441e-02 (3.6261e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [45][290/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 7.8057e-02 (3.6498e-02)	Acc@1  96.88 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [45][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.8463e-02 (3.6548e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [45][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.0080e-02 (3.6596e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [45][320/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3230e-02 (3.6922e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7487e-02 (3.7023e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.2886e-02 (3.7173e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5797e-02 (3.6947e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.3691e-02 (3.6777e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8007e-02 (3.6389e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9034e-02 (3.6250e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.5708e-02 (3.6285e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 ( 99.99)
## e[45] optimizer.zero_grad (sum) time: 0.13884830474853516
## e[45]       loss.backward (sum) time: 2.5693459510803223
## e[45]      optimizer.step (sum) time: 1.0256702899932861
## epoch[45] training(only) time: 18.45106601715088
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.4269e+00 (1.4269e+00)	Acc@1  70.00 ( 70.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.7769e+00 (1.5100e+00)	Acc@1  70.00 ( 71.64)	Acc@5  90.00 ( 90.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2373e+00 (1.4226e+00)	Acc@1  77.00 ( 72.52)	Acc@5  93.00 ( 90.90)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 2.0235e+00 (1.4421e+00)	Acc@1  65.00 ( 72.42)	Acc@5  89.00 ( 91.16)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3954e+00 (1.4216e+00)	Acc@1  69.00 ( 72.37)	Acc@5  95.00 ( 91.49)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4618e+00 (1.4367e+00)	Acc@1  74.00 ( 72.14)	Acc@5  90.00 ( 91.29)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.6052e+00 (1.4136e+00)	Acc@1  67.00 ( 72.39)	Acc@5  91.00 ( 91.52)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.9835e+00 (1.4169e+00)	Acc@1  70.00 ( 72.28)	Acc@5  89.00 ( 91.66)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.4969e+00 (1.4245e+00)	Acc@1  69.00 ( 72.28)	Acc@5  90.00 ( 91.59)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6563e+00 (1.4086e+00)	Acc@1  65.00 ( 72.51)	Acc@5  92.00 ( 91.74)
 * Acc@1 72.560 Acc@5 91.830
### epoch[45] execution time: 20.93057131767273
EPOCH 46
i:   0, name:          module.conv1.0.weight  changing lr from: 0.001135477166065490   to: 0.001000743693028921
i:   1, name:          module.conv1.1.weight  changing lr from: 0.001304649322640771   to: 0.001046405946810602
i:   2, name:            module.conv1.1.bias  changing lr from: 0.001536872991867038   to: 0.001160398313586568
i:   3, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.001828788470716202   to: 0.001339310141046112
i:   4, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.002177114751614250   to: 0.001579795472094591
i:   5, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.002578654616108072   to: 0.001878579884753972
i:   6, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.003030298776413725   to: 0.002232466242414565
i:   7, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.003529029157128276   to: 0.002638339453875565
i:   8, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.004071921402926170   to: 0.003093170336177647
i:   9, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.004656146691841794   to: 0.003594018666964032
i:  10, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.005278972927785502   to: 0.004138035507043907
i:  11, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.005937765380267184   to: 0.004722464868001513
i:  12, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.006629986833915097   to: 0.005344644794111597
i:  13, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.007353197305287373   to: 0.006002007922505803
i:  14, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.008105053379674949   to: 0.006692081580486764
i:  15, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.008883307216088435   to: 0.007412487474116280
i:  16, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.009685805264400809   to: 0.008160941017707096
i:  17, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.010510486734673908   to: 0.008935250349623464
i:  18, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.011355381855025298   to: 0.009733315075839150
i:  19, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.012218609950974534   to: 0.010553124779002332
i:  20, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.013098377376044142   to: 0.011392757327312847
i:  21, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.013992975320457419   to: 0.012250377014310310
i:  22, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.014900777522070287   to: 0.013124232557700337
i:  23, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.015820237901179875   to: 0.014012654982592743
i:  24, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.016749888138556580   to: 0.014914055411981653
i:  25, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.017688335213941982   to: 0.015826922784953575
i:  26, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.018634258920321774   to: 0.016749821520949289
i:  27, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.019586409367518605   to: 0.017681389146423542
i:  28, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.020543604487036946   to: 0.018620333898428005
i:  29, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.021504727548621778   to: 0.019565432317977773
i:  30, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.022468724697657119   to: 0.020515526844542345
i:  31, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.023434602521316079   to: 0.021469523421614175
i:  32, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.024401425650274030   to: 0.022426389122045649
i:  33, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.025368314401803174   to: 0.023385149800699324
i:  34, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.026334442469166670   to: 0.024344887780914079
i:  35, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.027299034661424727   to: 0.025304739580350434
i:  36, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.028261364697035819   to: 0.026263893680925944
i:  37, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.029220753053988014   to: 0.027221588346786799
i:  38, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.030176564878609802   to: 0.028177109493570419
i:  39, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.031128207954693978   to: 0.029129788611597197
i:  40, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.032075130734103653   to: 0.030079000745074247
i:  41, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.033016820429622597   to: 0.031024162528900796
i:  42, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.033952801170449902   to: 0.031964730284225140
i:  43, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.034882632220420928   to: 0.032900198173511828
i:  44, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.035805906258759644   to: 0.033830096415533875
i:  45, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.036722247722923373   to: 0.034753989560400281
i:  46, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.037631311212892093   to: 0.035671474824462472
i:  47, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.038532779956073314   to: 0.036582180484711457
i:  48, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.039426364331838479   to: 0.037485764332074338
i:  49, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.040311800454577440   to: 0.038381912182845937
i:  50, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.041188848814046448   to: 0.039270336447341105
i:  51, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.042057292971696088   to: 0.040150774754727198
i:  52, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.042916938311592268   to: 0.041022988632890472
i:  53, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.043767610844484016   to: 0.041886762242100445
i:  54, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.044609156063529748   to: 0.042741901161166961
i:  55, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.045441437850158833   to: 0.043588231224724924
i:  56, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.046264337428525190   to: 0.044425597410239381
i:  57, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.047077752366996857   to: 0.045253862773290167
i:  58, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.047881595625121259   to: 0.046072907429673161
i:  59, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.048675794644508864   to: 0.046882627582840987
i:  60, name:               module.fc.weight  changing lr from: 0.049460290482087658   to: 0.047682934595201801
i:  61, name:                 module.fc.bias  changing lr from: 0.050235036984196041   to: 0.048473754101795691



# Switched to train mode...
Epoch: [46][  0/391]	Time  0.191 ( 0.191)	Data  0.149 ( 0.149)	Loss 2.4114e-02 (2.4114e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.9619e-02 (3.1614e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.046 ( 0.054)	Data  0.001 ( 0.009)	Loss 1.2418e-02 (2.5066e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.046 ( 0.051)	Data  0.001 ( 0.007)	Loss 1.4557e-02 (2.5559e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 4.7235e-02 (2.6917e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 2.4612e-02 (2.7420e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6082e-02 (2.8422e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.047 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.7326e-02 (2.8201e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4666e-02 (2.8532e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5454e-02 (2.8225e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6786e-02 (2.8221e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.0747e-02 (2.7786e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 3.1343e-02 (2.8462e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.047 ( 0.048)	Data  0.001 ( 0.004)	Loss 1.6250e-02 (2.8555e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.4546e-02 (2.8781e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1836e-02 (2.9113e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.051 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.7942e-02 (2.9648e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0631e-02 (2.9336e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.6402e-02 (2.9804e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1733e-02 (2.9422e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1652e-02 (2.9543e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.8226e-02 (2.9762e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.4745e-02 (2.9977e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.4237e-02 (3.0145e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8561e-02 (3.0122e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5723e-02 (3.0480e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.9311e-02 (3.0522e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2849e-02 (3.0618e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0422e-02 (3.0352e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8487e-02 (3.0160e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3349e-02 (3.0352e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9969e-02 (3.0424e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.5609e-02 (3.0217e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.0530e-02 (2.9997e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.2574e-02 (3.0097e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.7282e-02 (3.0121e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.049 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.7199e-02 (3.0310e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.3412e-02 (3.0359e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.0334e-02 (3.0341e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 8.6173e-02 (3.0694e-02)	Acc@1  97.50 ( 99.32)	Acc@5 100.00 (100.00)
## e[46] optimizer.zero_grad (sum) time: 0.13854408264160156
## e[46]       loss.backward (sum) time: 2.565992832183838
## e[46]      optimizer.step (sum) time: 1.0334227085113525
## epoch[46] training(only) time: 18.47885251045227
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.4104e+00 (1.4104e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5776e+00 (1.4587e+00)	Acc@1  70.00 ( 72.45)	Acc@5  90.00 ( 90.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.2672e+00 (1.3829e+00)	Acc@1  73.00 ( 73.62)	Acc@5  92.00 ( 91.29)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.9399e+00 (1.4101e+00)	Acc@1  62.00 ( 73.16)	Acc@5  85.00 ( 91.03)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.4425e+00 (1.3886e+00)	Acc@1  71.00 ( 73.05)	Acc@5  93.00 ( 91.49)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.7431e+00 (1.4129e+00)	Acc@1  69.00 ( 72.59)	Acc@5  91.00 ( 91.33)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4124e+00 (1.3878e+00)	Acc@1  68.00 ( 72.56)	Acc@5  93.00 ( 91.56)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6214e+00 (1.3904e+00)	Acc@1  67.00 ( 72.38)	Acc@5  89.00 ( 91.59)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.7860e+00 (1.3975e+00)	Acc@1  69.00 ( 72.31)	Acc@5  89.00 ( 91.57)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.8257e+00 (1.3822e+00)	Acc@1  73.00 ( 72.64)	Acc@5  86.00 ( 91.63)
 * Acc@1 72.880 Acc@5 91.710
### epoch[46] execution time: 20.957846641540527
EPOCH 47
REMOVING: module.conv1.0.weight
REMOVING: module.conv1.1.weight
i:   0, name:            module.conv1.1.bias  changing lr from: 0.001160398313586568   to: 0.001004601660599984
i:   1, name: module.conv2_x.0.residual_function.0.weight  changing lr from: 0.001339310141046112   to: 0.001064515373141677
i:   2, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.001579795472094591   to: 0.001191085343908021
i:   3, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.001878579884753972   to: 0.001380985762338959
i:   4, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.002232466242414565   to: 0.001630955105450282
i:   5, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.002638339453875565   to: 0.001937802522303238
i:   6, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.003093170336177647   to: 0.002298413193572201
i:   7, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.003594018666964032   to: 0.002709752759612659
i:   8, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.004138035507043907   to: 0.003168870904381387
i:   9, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.004722464868001513   to: 0.003672904176676311
i:  10, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.005344644794111597   to: 0.004219078124473945
i:  11, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.006002007922505803   to: 0.004804708812674290
i:  12, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.006692081580486764   to: 0.005427203789330559
i:  13, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.007412487474116280   to: 0.006084062560458353
i:  14, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.008160941017707096   to: 0.006772876628790508
i:  15, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.008935250349623464   to: 0.007491329147373282
i:  16, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.009733315075839150   to: 0.008237194234688328
i:  17, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.010553124779002332   to: 0.009008335994024844
i:  18, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.011392757327312847   to: 0.009802707276120596
i:  19, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.012250377014310310   to: 0.010618348220622263
i:  20, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.013124232557700337   to: 0.011453384608686484
i:  21, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.014012654982592743   to: 0.012306026056037440
i:  22, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.014914055411981653   to: 0.013174564073006621
i:  23, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.015826922784953575   to: 0.014057370015498817
i:  24, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.016749821520949289   to: 0.014952892948438982
i:  25, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.017681389146423542   to: 0.015859657441052527
i:  26, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.018620333898428005   to: 0.016776261311303404
i:  27, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.019565432317977773   to: 0.017701373334948905
i:  28, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.020515526844542345   to: 0.018633730932960418
i:  29, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.021469523421614175   to: 0.019572137849492743
i:  30, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.022426389122045649   to: 0.020515461831151556
i:  31, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.023385149800699324   to: 0.021462632317003856
i:  32, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.024344887780914079   to: 0.022412638147582660
i:  33, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.025304739580350434   to: 0.023364525300058472
i:  34, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.026263893680925944   to: 0.024317394655764352
i:  35, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.027221588346786799   to: 0.025270399805375030
i:  36, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.028177109493570419   to: 0.026222744896233219
i:  37, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.029129788611597197   to: 0.027173682525593337
i:  38, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.030079000745074247   to: 0.028122511682898366
i:  39, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.031024162528900796   to: 0.029068575743619519
i:  40, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.031964730284225140   to: 0.030011260516664084
i:  41, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.032900198173511828   to: 0.030949992346885481
i:  42, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.033830096415533875   to: 0.031884236273813787
i:  43, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.034753989560400281   to: 0.032813494247351839
i:  44, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.035671474824462472   to: 0.033737303400854145
i:  45, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.036582180484711457   to: 0.034655234381716145
i:  46, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.037485764332074338   to: 0.035566889739345908
i:  47, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.038381912182845937   to: 0.036471902370169132
i:  48, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.039270336447341105   to: 0.037369934019123564
i:  49, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.040150774754727198   to: 0.038260673836932650
i:  50, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.041022988632890472   to: 0.039143836992305135
i:  51, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.041886762242100445   to: 0.040019163338083759
i:  52, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.042741901161166961   to: 0.040886416130265987
i:  53, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.043588231224724924   to: 0.041745380798731746
i:  54, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.044425597410239381   to: 0.042595863768445291
i:  55, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.045253862773290167   to: 0.043437691329841335
i:  56, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.046072907429673161   to: 0.044270708557062562
i:  57, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.046882627582840987   to: 0.045094778272682289
i:  58, name:               module.fc.weight  changing lr from: 0.047682934595201801   to: 0.045909780057524240
i:  59, name:                 module.fc.bias  changing lr from: 0.048473754101795691   to: 0.046715609304177064



# Switched to train mode...
Epoch: [47][  0/391]	Time  0.193 ( 0.193)	Data  0.150 ( 0.150)	Loss 2.6112e-02 (2.6112e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.046 ( 0.060)	Data  0.001 ( 0.016)	Loss 2.3975e-02 (2.3559e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.046 ( 0.053)	Data  0.001 ( 0.009)	Loss 2.5068e-02 (2.4636e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.047 ( 0.051)	Data  0.001 ( 0.007)	Loss 2.3368e-02 (2.7638e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.006)	Loss 3.7198e-02 (2.7279e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.046 ( 0.050)	Data  0.001 ( 0.005)	Loss 4.0747e-02 (2.6498e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.045 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.6950e-02 (2.6887e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.046 ( 0.049)	Data  0.001 ( 0.005)	Loss 1.1142e-02 (2.7400e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 4.1646e-02 (2.7216e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.5431e-02 (2.7453e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.3582e-02 (2.7566e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.0601e-02 (2.7298e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.046 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.8306e-02 (2.7444e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.004)	Loss 2.6160e-02 (2.6881e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.004)	Loss 4.1272e-02 (2.7364e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1869e-02 (2.6935e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [47][160/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0334e-02 (2.6713e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8615e-02 (2.6598e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9175e-02 (2.6861e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.0668e-02 (2.7617e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.6225e-02 (2.7842e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.1186e-02 (2.7745e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.048 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.5519e-02 (2.7492e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.9454e-02 (2.7550e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1026e-02 (2.7462e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1529e-02 (2.7470e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.1963e-02 (2.7514e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.9346e-02 (2.7265e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.2250e-02 (2.7118e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1165e-02 (2.7166e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.7418e-02 (2.7220e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 3.1007e-02 (2.7129e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.047 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4213e-02 (2.6869e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 5.8177e-02 (2.6890e-02)	Acc@1  96.88 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.4254e-02 (2.6781e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.0176e-02 (2.6829e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 6.2104e-03 (2.6717e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 2.4497e-02 (2.6828e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.8131e-02 (2.6812e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.046 ( 0.047)	Data  0.001 ( 0.003)	Loss 1.2621e-01 (2.6815e-02)	Acc@1  96.25 ( 99.45)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.1357717514038086
## e[47]       loss.backward (sum) time: 2.5491416454315186
## e[47]      optimizer.step (sum) time: 0.9849090576171875
## epoch[47] training(only) time: 18.394608736038208
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.4810e+00 (1.4810e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.5762e+00 (1.4426e+00)	Acc@1  74.00 ( 73.91)	Acc@5  92.00 ( 90.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2030e+00 (1.3746e+00)	Acc@1  80.00 ( 74.33)	Acc@5  90.00 ( 91.29)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.9847e+00 (1.4196e+00)	Acc@1  64.00 ( 73.19)	Acc@5  89.00 ( 91.00)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5362e+00 (1.3964e+00)	Acc@1  71.00 ( 73.24)	Acc@5  93.00 ( 91.34)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.7696e+00 (1.4175e+00)	Acc@1  67.00 ( 72.94)	Acc@5  88.00 ( 91.10)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.6259e+00 (1.3959e+00)	Acc@1  66.00 ( 73.05)	Acc@5  91.00 ( 91.36)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6862e+00 (1.3913e+00)	Acc@1  72.00 ( 72.93)	Acc@5  91.00 ( 91.45)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6044e+00 (1.3966e+00)	Acc@1  74.00 ( 72.78)	Acc@5  88.00 ( 91.43)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6480e+00 (1.3806e+00)	Acc@1  68.00 ( 73.08)	Acc@5  89.00 ( 91.56)
 * Acc@1 73.250 Acc@5 91.690
### epoch[47] execution time: 20.880508184432983
EPOCH 48
REMOVING: module.conv1.1.bias
REMOVING: module.conv2_x.0.residual_function.0.weight
i:   0, name: module.conv2_x.0.residual_function.1.weight  changing lr from: 0.001191085343908021   to: 0.001012625163379254
i:   1, name: module.conv2_x.0.residual_function.1.bias  changing lr from: 0.001380985762338959   to: 0.001087923353262609
i:   2, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.001630955105450282   to: 0.001228186843451855
i:   3, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.001937802522303238   to: 0.001430172797178407
i:   4, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.002298413193572201   to: 0.001690702368399793
i:   5, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.002709752759612659   to: 0.002006666639885773
i:   6, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.003168870904381387   to: 0.002375031598589299
i:   7, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.003672904176676311   to: 0.002792842235987197
i:   8, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.004219078124473945   to: 0.003257225855384848
i:   9, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.004804708812674290   to: 0.003765394662651294
i:  10, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.005427203789330559   to: 0.004314647711511509
i:  11, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.006084062560458353   to: 0.004902372269393989
i:  12, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.006772876628790508   to: 0.005526044664927270
i:  13, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.007491329147373282   to: 0.006183230673509615
i:  14, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.008237194234688328   to: 0.006871585492947313
i:  15, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.009008335994024844   to: 0.007588853356969177
i:  16, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.009802707276120596   to: 0.008332866830481100
i:  17, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.010618348220622263   to: 0.009101545826715889
i:  18, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.011453384608686484   to: 0.009892896382961813
i:  19, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.012306026056037440   to: 0.010705009228306212
i:  20, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.013174564073006621   to: 0.011536058173802710
i:  21, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.014057370015498817   to: 0.012384298352656031
i:  22, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.014952892948438982   to: 0.013248064335401537
i:  23, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.015859657441052527   to: 0.014125768142636323
i:  24, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.016776261311303404   to: 0.015015897175617604
i:  25, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.017701373334948905   to: 0.015917012082977113
i:  26, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.018633730932960418   to: 0.016827744579896303
i:  27, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.019572137849492743   to: 0.017746795234335932
i:  28, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.020515461831151556   to: 0.018672931233305556
i:  29, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.021462632317003856   to: 0.019604984140687471
i:  30, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.022412638147582660   to: 0.020541847656780507
i:  31, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.023364525300058472   to: 0.021482475388500857
i:  32, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.024317394655764352   to: 0.022425878638055136
i:  33, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.025270399805375030   to: 0.023371124216881967
i:  34, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.026222744896233219   to: 0.024317332290731266
i:  35, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.027173682525593337   to: 0.025263674260912181
i:  36, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.028122511682898366   to: 0.026209370685980726
i:  37, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.029068575743619519   to: 0.027153689247453035
i:  38, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.030011260516664084   to: 0.028095942762513315
i:  39, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.030949992346885481   to: 0.029035487246129556
i:  40, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.031884236273813787   to: 0.029971720024494542
i:  41, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.032813494247351839   to: 0.030904077901263250
i:  42, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.033737303400854145   to: 0.031832035377662035
i:  43, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.034655234381716145   to: 0.032755102927192775
i:  44, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.035566889739345908   to: 0.033672825325341521
i:  45, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.036471902370169132   to: 0.034584780034427234
i:  46, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.037369934019123564   to: 0.035490575643481827
i:  47, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.038260673836932650   to: 0.036389850362842209
i:  48, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.039143836992305135   to: 0.037282270572949631
i:  49, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.040019163338083759   to: 0.038167529426691181
i:  50, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.040886416130265987   to: 0.039045345504482722
i:  51, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.041745380798731746   to: 0.039915461521173250
i:  52, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.042595863768445291   to: 0.040777643083753534
i:  53, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.043437691329841335   to: 0.041631677498769261
i:  54, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.044270708557062562   to: 0.042477372628270796
i:  55, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.045094778272682289   to: 0.043314555793077428
i:  56, name:               module.fc.weight  changing lr from: 0.045909780057524240   to: 0.044143072722091954
i:  57, name:                 module.fc.bias  changing lr from: 0.046715609304177064   to: 0.044962786546369038



# Switched to train mode...
Epoch: [48][  0/391]	Time  0.193 ( 0.193)	Data  0.151 ( 0.151)	Loss 1.1875e-02 (1.1875e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.045 ( 0.058)	Data  0.001 ( 0.016)	Loss 3.2183e-02 (2.1771e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.045 ( 0.052)	Data  0.001 ( 0.009)	Loss 1.0926e-02 (2.4367e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.045 ( 0.050)	Data  0.001 ( 0.007)	Loss 2.1000e-02 (2.2936e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.045 ( 0.049)	Data  0.001 ( 0.006)	Loss 1.8551e-02 (2.2453e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.005)	Loss 1.2892e-02 (2.1989e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.005)	Loss 8.8342e-03 (2.0799e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.005)	Loss 2.1907e-02 (2.0936e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.045 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.5853e-02 (2.0778e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.004)	Loss 1.5935e-02 (2.0733e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.004)	Loss 2.0233e-02 (2.0981e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.004)	Loss 2.8032e-02 (2.0726e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.004)	Loss 4.5506e-02 (2.0588e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.004)	Loss 1.7604e-02 (2.0796e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.003)	Loss 8.3120e-03 (2.0368e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.4574e-02 (2.0697e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.046 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.3850e-02 (2.0981e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 9.9494e-03 (2.0800e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 2.3004e-02 (2.0880e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.3021e-02 (2.0693e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.003)	Loss 3.4586e-02 (2.0836e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 3.6811e-02 (2.0964e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.003)	Loss 9.9986e-03 (2.1250e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.003)	Loss 3.8340e-02 (2.1483e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.003)	Loss 8.5655e-03 (2.1377e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.6582e-02 (2.1252e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.003)	Loss 4.1094e-02 (2.1152e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.8385e-02 (2.0927e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.4906e-02 (2.0886e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.2598e-02 (2.0803e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.046 ( 0.046)	Data  0.001 ( 0.003)	Loss 4.1597e-02 (2.1084e-02)	Acc@1  98.44 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.8216e-02 (2.0992e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 3.7549e-02 (2.1266e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.9211e-02 (2.1250e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.046 ( 0.046)	Data  0.001 ( 0.003)	Loss 3.5624e-02 (2.1307e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.8994e-02 (2.1349e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.003)	Loss 3.6307e-02 (2.1387e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.003)	Loss 3.6429e-02 (2.1323e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.4886e-02 (2.1394e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 3.0334e-02 (2.1420e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.13010883331298828
## e[48]       loss.backward (sum) time: 2.462076187133789
## e[48]      optimizer.step (sum) time: 0.957855224609375
## epoch[48] training(only) time: 17.91295552253723
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.5455e+00 (1.5455e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.5000e+00 (1.3883e+00)	Acc@1  70.00 ( 72.91)	Acc@5  92.00 ( 90.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2320e+00 (1.3236e+00)	Acc@1  77.00 ( 74.52)	Acc@5  92.00 ( 91.57)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.9627e+00 (1.3771e+00)	Acc@1  65.00 ( 73.71)	Acc@5  86.00 ( 91.10)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.5025e+00 (1.3617e+00)	Acc@1  69.00 ( 73.44)	Acc@5  93.00 ( 91.51)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5532e+00 (1.3795e+00)	Acc@1  74.00 ( 73.10)	Acc@5  90.00 ( 91.39)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4023e+00 (1.3559e+00)	Acc@1  67.00 ( 73.05)	Acc@5  90.00 ( 91.69)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.7118e+00 (1.3619e+00)	Acc@1  70.00 ( 72.90)	Acc@5  91.00 ( 91.73)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6115e+00 (1.3672e+00)	Acc@1  67.00 ( 72.68)	Acc@5  90.00 ( 91.86)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.6519e+00 (1.3512e+00)	Acc@1  71.00 ( 72.96)	Acc@5  89.00 ( 91.90)
 * Acc@1 73.000 Acc@5 92.070
### epoch[48] execution time: 20.403029918670654
EPOCH 49
REMOVING: module.conv2_x.0.residual_function.1.weight
REMOVING: module.conv2_x.0.residual_function.1.bias
i:   0, name: module.conv2_x.0.residual_function.3.weight  changing lr from: 0.001228186843451855   to: 0.001025782863568336
i:   1, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.001430172797178407   to: 0.001117446223183898
i:   2, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.001690702368399793   to: 0.001272371872329580
i:   3, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.002006666639885773   to: 0.001487398210903206
i:   4, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.002375031598589299   to: 0.001759427412508673
i:   5, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.002792842235987197   to: 0.002085430927469128
i:   6, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.003257225855384848   to: 0.002462454082708514
i:   7, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.003765394662651294   to: 0.002887619860773765
i:   8, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.004314647711511509   to: 0.003358131934916419
i:   9, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.004902372269393989   to: 0.003871277031958697
i:  10, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.005526044664927270   to: 0.004424426689657312
i:  11, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.006183230673509615   to: 0.005015038470467486
i:  12, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.006871585492947313   to: 0.005640656689013695
i:  13, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.007588853356969177   to: 0.006298912706197589
i:  14, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.008332866830481100   to: 0.006987524838726666
i:  15, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.009101545826715889   to: 0.007704297928925246
i:  16, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.009892896382961813   to: 0.008447122615996942
i:  17, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.010705009228306212   to: 0.009213974346437098
i:  18, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.011536058173802710   to: 0.010002912158041066
i:  19, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.012384298352656031   to: 0.010812077268916427
i:  20, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.013248064335401537   to: 0.011639691500070680
i:  21, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.014125768142636323   to: 0.012484055557509928
i:  22, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.015015897175617604   to: 0.013343547197334150
i:  23, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.015917012082977113   to: 0.014216619295044797
i:  24, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.016827744579896303   to: 0.015101797838181545
i:  25, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.017746795234335932   to: 0.015997679859466624
i:  26, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.018672931233305556   to: 0.016902931325848632
i:  27, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.019604984140687471   to: 0.017816284997196431
i:  28, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.020541847656780507   to: 0.018736538266882385
i:  29, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.021482475388500857   to: 0.019662550995113964
i:  30, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.022425878638055136   to: 0.020593243344604991
i:  31, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.023371124216881967   to: 0.021527593627023785
i:  32, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.024317332290731266   to: 0.022464636167598990
i:  33, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.025263674260912181   to: 0.023403459194306926
i:  34, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.026209370685980726   to: 0.024343202757190232
i:  35, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.027153689247453035   to: 0.025283056682568184
i:  36, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.028095942762513315   to: 0.026222258566183779
i:  37, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.029035487246129556   to: 0.027160091808684950
i:  38, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.029971720024494542   to: 0.028095883696256187
i:  39, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.030904077901263250   to: 0.029029003528692178
i:  40, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.031832035377662035   to: 0.029958860796734911
i:  41, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.032755102927192775   to: 0.030884903410076103
i:  42, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.033672825325341521   to: 0.031806615977049778
i:  43, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.034584780034427234   to: 0.032723518136707971
i:  44, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.035490575643481827   to: 0.033635162943674447
i:  45, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.036389850362842209   to: 0.034541135305910860
i:  46, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.037282270572949631   to: 0.035441050475299468
i:  47, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.038167529426691181   to: 0.036334552590743717
i:  48, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.039045345504482722   to: 0.037221313273314099
i:  49, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.039915461521173250   to: 0.038101030272812079
i:  50, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.040777643083753534   to: 0.038973426164995439
i:  51, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.041631677498769261   to: 0.039838247098596208
i:  52, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.042477372628270796   to: 0.040695261591167091
i:  53, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.043314555793077428   to: 0.041544259372713638
i:  54, name:               module.fc.weight  changing lr from: 0.044143072722091954   to: 0.042385050276004567
i:  55, name:                 module.fc.bias  changing lr from: 0.044962786546369038   to: 0.043217463172400238



# Switched to train mode...
Epoch: [49][  0/391]	Time  0.203 ( 0.203)	Data  0.155 ( 0.155)	Loss 1.0118e-02 (1.0118e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.044 ( 0.059)	Data  0.001 ( 0.016)	Loss 1.7704e-02 (1.6668e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.044 ( 0.052)	Data  0.001 ( 0.010)	Loss 1.6820e-02 (1.9990e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.044 ( 0.049)	Data  0.001 ( 0.007)	Loss 1.4548e-02 (1.9725e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.044 ( 0.048)	Data  0.001 ( 0.006)	Loss 1.5546e-02 (2.0085e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.045 ( 0.048)	Data  0.001 ( 0.005)	Loss 8.7610e-03 (1.9691e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.005)	Loss 1.1094e-02 (1.9645e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.005)	Loss 1.0877e-02 (1.8846e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.004)	Loss 8.0792e-03 (1.9461e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.004)	Loss 1.0032e-02 (1.9457e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.004)	Loss 1.9387e-02 (1.9043e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.004)	Loss 3.3646e-02 (1.9965e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.045 ( 0.046)	Data  0.001 ( 0.004)	Loss 1.1349e-02 (1.9857e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.004)	Loss 1.4169e-02 (1.9836e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 1.6457e-02 (1.9648e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.003)	Loss 8.2074e-03 (1.9217e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 9.2332e-03 (1.9055e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.1845e-02 (1.8851e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.003)	Loss 7.9599e-03 (1.8623e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.6769e-02 (1.8556e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 7.9535e-03 (1.8478e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.003)	Loss 8.4677e-03 (1.8568e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.6681e-02 (1.8621e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.0656e-02 (1.8675e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.003)	Loss 8.0531e-03 (1.8893e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 2.5297e-02 (1.8862e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.4012e-02 (1.8832e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.4502e-02 (1.8782e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 3.3448e-02 (1.8783e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.0741e-02 (1.8940e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.046 ( 0.045)	Data  0.002 ( 0.003)	Loss 1.7056e-02 (1.8897e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.7707e-02 (1.8869e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.003)	Loss 2.4611e-02 (1.8737e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 2.3041e-02 (1.8670e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 5.2585e-02 (1.8703e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 8.9231e-03 (1.8698e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.003)	Loss 2.7023e-02 (1.8725e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.045 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.3744e-02 (1.8929e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.5487e-02 (1.8910e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 2.4897e-02 (1.9071e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.12763547897338867
## e[49]       loss.backward (sum) time: 2.431467056274414
## e[49]      optimizer.step (sum) time: 0.9219708442687988
## epoch[49] training(only) time: 17.682486534118652
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.4074e+00 (1.4074e+00)	Acc@1  77.00 ( 77.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.6060e+00 (1.4086e+00)	Acc@1  72.00 ( 74.00)	Acc@5  92.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.3497e+00 (1.3335e+00)	Acc@1  75.00 ( 74.62)	Acc@5  93.00 ( 91.76)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.9721e+00 (1.3668e+00)	Acc@1  65.00 ( 74.00)	Acc@5  90.00 ( 91.35)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.5557e+00 (1.3600e+00)	Acc@1  72.00 ( 73.80)	Acc@5  93.00 ( 91.76)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.6465e+00 (1.3873e+00)	Acc@1  67.00 ( 73.25)	Acc@5  88.00 ( 91.53)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4132e+00 (1.3644e+00)	Acc@1  71.00 ( 73.33)	Acc@5  91.00 ( 91.79)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.7093e+00 (1.3666e+00)	Acc@1  68.00 ( 73.15)	Acc@5  91.00 ( 91.82)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6987e+00 (1.3709e+00)	Acc@1  70.00 ( 73.16)	Acc@5  87.00 ( 91.93)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.5492e+00 (1.3539e+00)	Acc@1  75.00 ( 73.42)	Acc@5  90.00 ( 92.09)
 * Acc@1 73.490 Acc@5 92.220
### epoch[49] execution time: 20.16627287864685
EPOCH 50
REMOVING: module.conv2_x.0.residual_function.3.weight
i:   0, name: module.conv2_x.0.residual_function.4.weight  changing lr from: 0.001117446223183898   to: 0.001000852407081586
i:   1, name: module.conv2_x.0.residual_function.4.bias  changing lr from: 0.001272371872329580   to: 0.001045028371260769
i:   2, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.001487398210903206   to: 0.001153895711679242
i:   3, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.001759427412508673   to: 0.001324314847056806
i:   4, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.002085430927469128   to: 0.001553203796248702
i:   5, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.002462454082708514   to: 0.001837544187809830
i:   6, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.002887619860773765   to: 0.002174386341075062
i:   7, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.003358131934916419   to: 0.002560853501070914
i:   8, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.003871277031958697   to: 0.002994145304409839
i:   9, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.004424426689657312   to: 0.003471540548281664
i:  10, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.005015038470467486   to: 0.003990399329775734
i:  11, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.005640656689013695   to: 0.004548164618063782
i:  12, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.006298912706197589   to: 0.005142363317460917
i:  13, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.006987524838726666   to: 0.005770606875075162
i:  14, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.007704297928925246   to: 0.006430591482656322
i:  15, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.008447122615996942   to: 0.007120097918371848
i:  16, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.009213974346437098   to: 0.007836991070566878
i:  17, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.010002912158041066   to: 0.008579219182108116
i:  18, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.010812077268916427   to: 0.009344812850664796
i:  19, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.011639691500070680   to: 0.010131883817235874
i:  20, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.012484055557509928   to: 0.010938623572388773
i:  21, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.013343547197334150   to: 0.011763301807022807
i:  22, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.014216619295044797   to: 0.012604264732000263
i:  23, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.015101797838181545   to: 0.013459933288696990
i:  24, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.015997679859466624   to: 0.014328801270398060
i:  25, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.016902931325848632   to: 0.015209433372497982
i:  26, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.017816284997196431   to: 0.016100463187650606
i:  27, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.018736538266882385   to: 0.017000591160337964
i:  28, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.019662550995113964   to: 0.017908582513789507
i:  29, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.020593243344604991   to: 0.018823265160766767
i:  30, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.021527593627023785   to: 0.019743527608432416
i:  31, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.022464636167598990   to: 0.020668316866333719
i:  32, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.023403459194306926   to: 0.021596636365446831
i:  33, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.024343202757190232   to: 0.022527543895236115
i:  34, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.025283056682568184   to: 0.023460149564782956
i:  35, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.026222258566183779   to: 0.024393613793217885
i:  36, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.027160091808684950   to: 0.025327145333945901
i:  37, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.028095883696256187   to: 0.026259999336482560
i:  38, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.029029003528692178   to: 0.027191475449108493
i:  39, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.029958860796734911   to: 0.028120915965002064
i:  40, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.030884903410076103   to: 0.029047704014015581
i:  41, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.031806615977049778   to: 0.029971261801816824
i:  42, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.032723518136707971   to: 0.030891048897721790
i:  43, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.033635162943674447   to: 0.031806560572188010
i:  44, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.034541135305910860   to: 0.032717326184624154
i:  45, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.035441050475299468   to: 0.033622907621890769
i:  46, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.036334552590743717   to: 0.034522897787618176
i:  47, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.037221313273314099   to: 0.035416919142251875
i:  48, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.038101030272812079   to: 0.036304622293541589
i:  49, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.038973426164995439   to: 0.037185684637025311
i:  50, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.039838247098596208   to: 0.038059809045914175
i:  51, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.040695261591167091   to: 0.038926722609659348
i:  52, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.041544259372713638   to: 0.039786175420374770
i:  53, name:               module.fc.weight  changing lr from: 0.042385050276004567   to: 0.040637939406200890
i:  54, name:                 module.fc.bias  changing lr from: 0.043217463172400238   to: 0.041481807210617776



# Switched to train mode...
Epoch: [50][  0/391]	Time  0.193 ( 0.193)	Data  0.152 ( 0.152)	Loss 1.5574e-02 (1.5574e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.043 ( 0.058)	Data  0.001 ( 0.016)	Loss 2.5201e-02 (2.6107e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.043 ( 0.051)	Data  0.001 ( 0.009)	Loss 1.2089e-02 (2.2194e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.043 ( 0.049)	Data  0.001 ( 0.007)	Loss 1.7751e-02 (1.9547e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.043 ( 0.048)	Data  0.001 ( 0.006)	Loss 1.0735e-02 (1.9073e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.044 ( 0.047)	Data  0.001 ( 0.005)	Loss 1.0239e-02 (1.8289e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.005)	Loss 1.2569e-02 (1.7908e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.004)	Loss 1.3130e-02 (1.7606e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.004)	Loss 1.6419e-02 (1.8486e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.044 ( 0.046)	Data  0.001 ( 0.004)	Loss 7.5001e-03 (1.8012e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.004)	Loss 1.3172e-02 (1.7996e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.004)	Loss 1.3122e-02 (1.7738e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.004)	Loss 1.3593e-02 (1.7486e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.3681e-02 (1.7404e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.4624e-02 (1.7658e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.3726e-02 (1.7801e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.9235e-02 (1.7582e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.8506e-02 (1.7628e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 9.6494e-03 (1.8012e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.6477e-02 (1.7803e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.8367e-02 (1.7694e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.003)	Loss 8.5535e-03 (1.7440e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.9433e-02 (1.7376e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.003)	Loss 2.0934e-02 (1.7636e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.003)	Loss 3.0539e-02 (1.7714e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.048 ( 0.045)	Data  0.001 ( 0.003)	Loss 2.1086e-02 (1.7626e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.003)	Loss 6.2191e-02 (1.7764e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.8316e-02 (1.7820e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.003)	Loss 1.3821e-02 (1.7883e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 8.5950e-03 (1.8116e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.2516e-02 (1.8248e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.9374e-02 (1.8262e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.045 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.7737e-02 (1.8275e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.0269e-02 (1.8117e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.1791e-02 (1.8049e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 8.7171e-03 (1.7921e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 7.7055e-03 (1.7970e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.2281e-02 (1.7841e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.6066e-02 (1.7781e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.0204e-02 (1.7755e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.12664484977722168
## e[50]       loss.backward (sum) time: 2.438880681991577
## e[50]      optimizer.step (sum) time: 0.919525146484375
## epoch[50] training(only) time: 17.445268869400024
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3790e+00 (1.3790e+00)	Acc@1  74.00 ( 74.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.5367e+00 (1.4244e+00)	Acc@1  73.00 ( 73.82)	Acc@5  92.00 ( 91.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2881e+00 (1.3387e+00)	Acc@1  79.00 ( 74.33)	Acc@5  91.00 ( 91.52)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.9271e+00 (1.3694e+00)	Acc@1  64.00 ( 73.90)	Acc@5  90.00 ( 91.23)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3860e+00 (1.3474e+00)	Acc@1  73.00 ( 74.02)	Acc@5  93.00 ( 91.71)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.6695e+00 (1.3673e+00)	Acc@1  67.00 ( 73.47)	Acc@5  90.00 ( 91.47)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4992e+00 (1.3419e+00)	Acc@1  67.00 ( 73.54)	Acc@5  90.00 ( 91.69)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5971e+00 (1.3383e+00)	Acc@1  68.00 ( 73.38)	Acc@5  89.00 ( 91.75)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6789e+00 (1.3459e+00)	Acc@1  70.00 ( 73.28)	Acc@5  87.00 ( 91.80)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.5429e+00 (1.3292e+00)	Acc@1  72.00 ( 73.56)	Acc@5  90.00 ( 91.96)
 * Acc@1 73.650 Acc@5 92.090
### epoch[50] execution time: 19.92869544029236
EPOCH 51
REMOVING: module.conv2_x.0.residual_function.4.weight
REMOVING: module.conv2_x.0.residual_function.4.bias
i:   0, name: module.conv2_x.1.residual_function.0.weight  changing lr from: 0.001153895711679242   to: 0.001007410407540409
i:   1, name: module.conv2_x.1.residual_function.1.weight  changing lr from: 0.001324314847056806   to: 0.001071288810009285
i:   2, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.001553203796248702   to: 0.001198066977325917
i:   3, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.001837544187809830   to: 0.001384683103838868
i:   4, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.002174386341075062   to: 0.001628133316861296
i:   5, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.002560853501070914   to: 0.001925477219404058
i:   6, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.002994145304409839   to: 0.002273842563910838
i:   7, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.003471540548281664   to: 0.002670429134171814
i:   8, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.003990399329775734   to: 0.003112511907724563
i:   9, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.004548164618063782   to: 0.003597443566312363
i:  10, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.005142363317460917   to: 0.004122656417382702
i:  11, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.005770606875075162   to: 0.004685663785194554
i:  12, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.006430591482656322   to: 0.005284060925870814
i:  13, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.007120097918371848   to: 0.005915525516695935
i:  14, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.007836991070566878   to: 0.006577817766120164
i:  15, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.008579219182108116   to: 0.007268780187295104
i:  16, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.009344812850664796   to: 0.007986337074531547
i:  17, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.010131883817235874   to: 0.008728493718835238
i:  18, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.010938623572388773   to: 0.009493335395638913
i:  19, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.011763301807022807   to: 0.010279026155002813
i:  20, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.012604264732000263   to: 0.011083807441894585
i:  21, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.013459933288696990   to: 0.011905996571679556
i:  22, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.014328801270398060   to: 0.012743985083642016
i:  23, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.015209433372497982   to: 0.013596236993213461
i:  24, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.016100463187650606   to: 0.014461286961596348
i:  25, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.017000591160337964   to: 0.015337738399629774
i:  26, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.017908582513789507   to: 0.016224261521046605
i:  27, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.018823265160766767   to: 0.017119591358702141
i:  28, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.019743527608432416   to: 0.018022525755914121
i:  29, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.020668316866333719   to: 0.018931923343726769
i:  30, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.021596636365446831   to: 0.019846701513697253
i:  31, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.022527543895236115   to: 0.020765834394688515
i:  32, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.023460149564782956   to: 0.021688350841135230
i:  33, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.024393613793217885   to: 0.022613332439320885
i:  34, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.025327145333945901   to: 0.023539911537356448
i:  35, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.026259999336482560   to: 0.024467269303783529
i:  36, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.027191475449108493   to: 0.025394633819024456
i:  37, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.028120915965002064   to: 0.026321278203269644
i:  38, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.029047704014015581   to: 0.027246518783820929
i:  39, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.029971261801816824   to: 0.028169713304391299
i:  40, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.030891048897721790   to: 0.029090259178399181
i:  41, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.031806560572188010   to: 0.030007591787875400
i:  42, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.032717326184624154   to: 0.030921182829228445
i:  43, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.033622907621890769   to: 0.031830538706779120
i:  44, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.034522897787618176   to: 0.032735198974676839
i:  45, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.035416919142251875   to: 0.033634734827547433
i:  46, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.036304622293541589   to: 0.034528747639985861
i:  47, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.037185684637025311   to: 0.035416867554803055
i:  48, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.038059809045914175   to: 0.036298752119753953
i:  49, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.038926722609659348   to: 0.037174084972316446
i:  50, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.039786175420374770   to: 0.038042574571952821
i:  51, name:               module.fc.weight  changing lr from: 0.040637939406200890   to: 0.038903952979168023
i:  52, name:                 module.fc.bias  changing lr from: 0.041481807210617776   to: 0.039757974680577735



# Switched to train mode...
Epoch: [51][  0/391]	Time  0.184 ( 0.184)	Data  0.143 ( 0.143)	Loss 2.2984e-02 (2.2984e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.043 ( 0.056)	Data  0.001 ( 0.015)	Loss 2.0124e-02 (1.1161e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.044 ( 0.050)	Data  0.001 ( 0.009)	Loss 7.5899e-03 (1.2206e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.043 ( 0.048)	Data  0.001 ( 0.007)	Loss 1.2571e-02 (1.1984e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.006)	Loss 2.3590e-02 (1.4402e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.005)	Loss 1.4724e-02 (1.4120e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.043 ( 0.046)	Data  0.001 ( 0.005)	Loss 5.7259e-03 (1.4094e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.004)	Loss 9.0953e-03 (1.4088e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.004)	Loss 1.1402e-02 (1.4185e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.004)	Loss 2.2113e-02 (1.3896e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.044 ( 0.045)	Data  0.001 ( 0.004)	Loss 5.9111e-03 (1.3883e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.004)	Loss 5.2938e-03 (1.3970e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.3365e-02 (1.3834e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 6.6170e-02 (1.4275e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.5894e-02 (1.4405e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.5295e-02 (1.4390e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.3016e-02 (1.4327e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.8506e-02 (1.4383e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 7.0165e-03 (1.4229e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.0718e-02 (1.4244e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 7.3637e-03 (1.4565e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.4680e-02 (1.4685e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 8.4882e-03 (1.5010e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.2240e-02 (1.5078e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.8089e-02 (1.5070e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.2181e-02 (1.5201e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.4364e-02 (1.5148e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.044 ( 0.044)	Data  0.001 ( 0.003)	Loss 7.0978e-03 (1.5010e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.3725e-02 (1.5114e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 9.7894e-03 (1.5172e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 9.8978e-03 (1.5142e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.003)	Loss 6.9396e-03 (1.5074e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.2075e-02 (1.5101e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.1424e-02 (1.5022e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.9556e-02 (1.5022e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.6067e-02 (1.5006e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.5128e-02 (1.5098e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.049 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.1558e-02 (1.5211e-02)	Acc@1  97.66 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 3.5229e-02 (1.5314e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.8672e-02 (1.5337e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.12069177627563477
## e[51]       loss.backward (sum) time: 2.3860199451446533
## e[51]      optimizer.step (sum) time: 0.8821084499359131
## epoch[51] training(only) time: 17.182893753051758
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.3583e+00 (1.3583e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.4232e+00 (1.4087e+00)	Acc@1  75.00 ( 73.18)	Acc@5  92.00 ( 91.00)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.2292e+00 (1.3059e+00)	Acc@1  78.00 ( 74.57)	Acc@5  91.00 ( 91.76)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8142e+00 (1.3409e+00)	Acc@1  68.00 ( 73.97)	Acc@5  87.00 ( 91.42)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.4018e+00 (1.3382e+00)	Acc@1  72.00 ( 73.68)	Acc@5  94.00 ( 91.85)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6104e+00 (1.3571e+00)	Acc@1  69.00 ( 73.18)	Acc@5  90.00 ( 91.71)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4775e+00 (1.3392e+00)	Acc@1  67.00 ( 73.23)	Acc@5  92.00 ( 91.93)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5856e+00 (1.3392e+00)	Acc@1  73.00 ( 73.17)	Acc@5  91.00 ( 92.01)
Test: [ 80/100]	Time  0.024 ( 0.024)	Loss 1.7344e+00 (1.3449e+00)	Acc@1  69.00 ( 73.10)	Acc@5  87.00 ( 92.07)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.5879e+00 (1.3252e+00)	Acc@1  74.00 ( 73.32)	Acc@5  90.00 ( 92.15)
 * Acc@1 73.440 Acc@5 92.240
### epoch[51] execution time: 19.67515754699707
EPOCH 52
REMOVING: module.conv2_x.1.residual_function.0.weight
REMOVING: module.conv2_x.1.residual_function.1.weight
i:   0, name: module.conv2_x.1.residual_function.1.bias  changing lr from: 0.001198066977325917   to: 0.001021292368882242
i:   1, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.001384683103838868   to: 0.001105455662344928
i:   2, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.001628133316861296   to: 0.001250728938756850
i:   3, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.001925477219404058   to: 0.001454126760436069
i:   4, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.002273842563910838   to: 0.001712721947383804
i:   5, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.002670429134171814   to: 0.002023650671763998
i:   6, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.003112511907724563   to: 0.002384116739994373
i:   7, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.003597443566312363   to: 0.002791395134769852
i:   8, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.004122656417382702   to: 0.003242834884750500
i:   9, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.004685663785194554   to: 0.003735861325188183
i:  10, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.005284060925870814   to: 0.004267977808455275
i:  11, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.005915525516695935   to: 0.004836766919295870
i:  12, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.006577817766120164   to: 0.005439891245652250
i:  13, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.007268780187295104   to: 0.006075093752136323
i:  14, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.007986337074531547   to: 0.006740197799622571
i:  15, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.008728493718835238   to: 0.007433106851034565
i:  16, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.009493335395638913   to: 0.008151803900184846
i:  17, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.010279026155002813   to: 0.008894350657502735
i:  18, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.011083807441894585   to: 0.009658886523643415
i:  19, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.011905996571679556   to: 0.010443627379311139
i:  20, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.012743985083642016   to: 0.011246864217142171
i:  21, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.013596236993213461   to: 0.012066961639172868
i:  22, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.014461286961596348   to: 0.012902356241260876
i:  23, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.015337738399629774   to: 0.013751554903820067
i:  24, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.016224261521046605   to: 0.014613133006372570
i:  25, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.017119591358702141   to: 0.015485732581698962
i:  26, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.018022525755914121   to: 0.016368060423779373
i:  27, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.018931923343726769   to: 0.017258886162251037
i:  28, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.019846701513697253   to: 0.018157040314759350
i:  29, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.020765834394688515   to: 0.019061412327338167
i:  30, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.021688350841135230   to: 0.019970948611816863
i:  31, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.022613332439320885   to: 0.020884650588210020
i:  32, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.023539911537356448   to: 0.021801572739090325
i:  33, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.024467269303783529   to: 0.022720820682076329
i:  34, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.025394633819024456   to: 0.023641549265772839
i:  35, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.026321278203269644   to: 0.024562960693779026
i:  36, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.027246518783820929   to: 0.025484302680726136
i:  37, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.028169713304391299   to: 0.026404866643709779
i:  38, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.029090259178399181   to: 0.027323985931947292
i:  39, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.030007591787875400   to: 0.028241034097003438
i:  40, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.030921182829228445   to: 0.029155423205491672
i:  41, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.031830538706779120   to: 0.030066602195766418
i:  42, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.032735198974676839   to: 0.030974055279767727
i:  43, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.033634734827547433   to: 0.031877300390868037
i:  44, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.034528747639985861   to: 0.032775887678287242
i:  45, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.035416867554803055   to: 0.033669398048395631
i:  46, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.036298752119753953   to: 0.034557441753002398
i:  47, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.037174084972316446   to: 0.035439657024533458
i:  48, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.038042574571952821   to: 0.036315708757830446
i:  49, name:               module.fc.weight  changing lr from: 0.038903952979168023   to: 0.037185287238155132
i:  50, name:                 module.fc.bias  changing lr from: 0.039757974680577735   to: 0.038048106914852935



# Switched to train mode...
Epoch: [52][  0/391]	Time  0.191 ( 0.191)	Data  0.148 ( 0.148)	Loss 1.8715e-02 (1.8715e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.015)	Loss 2.8103e-02 (1.9646e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.043 ( 0.050)	Data  0.001 ( 0.009)	Loss 2.4585e-02 (1.9833e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.042 ( 0.047)	Data  0.001 ( 0.007)	Loss 1.1220e-02 (1.8826e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 6.1327e-03 (1.6654e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.005)	Loss 1.1398e-02 (1.5951e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.043 ( 0.045)	Data  0.001 ( 0.005)	Loss 1.7917e-02 (1.5335e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.004)	Loss 1.1162e-02 (1.4902e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.7235e-02 (1.4866e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.3441e-02 (1.4448e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 9.8795e-03 (1.4136e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.046 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.2210e-03 (1.4461e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 7.2937e-03 (1.4331e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 6.2232e-03 (1.4040e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 1.1838e-02 (1.3974e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.7064e-03 (1.3877e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.7150e-02 (1.4125e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.003)	Loss 2.2598e-02 (1.4109e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 6.8485e-03 (1.3952e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.3993e-03 (1.3928e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.0339e-03 (1.3694e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4790e-02 (1.3551e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.0151e-02 (1.3720e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.7229e-02 (1.3626e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.2999e-03 (1.3725e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8358e-02 (1.3749e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0977e-02 (1.3716e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.1329e-03 (1.3736e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 5.0086e-02 (1.3911e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4169e-02 (1.4006e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.6192e-02 (1.4062e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.0917e-02 (1.4095e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.7239e-03 (1.4156e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.3309e-03 (1.4134e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.0296e-03 (1.4088e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.4424e-03 (1.3967e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.7677e-03 (1.4035e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1602e-02 (1.3990e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0151e-02 (1.4093e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.4630e-03 (1.4059e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.11818718910217285
## e[52]       loss.backward (sum) time: 2.3708086013793945
## e[52]      optimizer.step (sum) time: 0.8441545963287354
## epoch[52] training(only) time: 16.98238444328308
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2986e+00 (1.2986e+00)	Acc@1  75.00 ( 75.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.4283e+00 (1.3764e+00)	Acc@1  73.00 ( 74.27)	Acc@5  91.00 ( 90.73)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.2410e+00 (1.2939e+00)	Acc@1  77.00 ( 74.95)	Acc@5  93.00 ( 91.52)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8167e+00 (1.3323e+00)	Acc@1  65.00 ( 74.26)	Acc@5  90.00 ( 91.35)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4985e+00 (1.3257e+00)	Acc@1  68.00 ( 73.85)	Acc@5  92.00 ( 91.73)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6369e+00 (1.3463e+00)	Acc@1  71.00 ( 73.45)	Acc@5  89.00 ( 91.51)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4726e+00 (1.3237e+00)	Acc@1  66.00 ( 73.64)	Acc@5  91.00 ( 91.77)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5121e+00 (1.3207e+00)	Acc@1  69.00 ( 73.35)	Acc@5  92.00 ( 91.87)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6749e+00 (1.3246e+00)	Acc@1  68.00 ( 73.31)	Acc@5  87.00 ( 91.94)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.4695e+00 (1.3057e+00)	Acc@1  75.00 ( 73.65)	Acc@5  88.00 ( 92.03)
 * Acc@1 73.760 Acc@5 92.190
### epoch[52] execution time: 19.48662543296814
EPOCH 53
REMOVING: module.conv2_x.1.residual_function.1.bias
i:   0, name: module.conv2_x.1.residual_function.3.weight  changing lr from: 0.001105455662344928   to: 0.001000839047003925
i:   1, name: module.conv2_x.1.residual_function.4.weight  changing lr from: 0.001250728938756850   to: 0.001043463905748020
i:   2, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.001454126760436069   to: 0.001148377557659357
i:   3, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.001712721947383804   to: 0.001312616564112195
i:   4, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.002023650671763998   to: 0.001533270547268200
i:   5, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.002384116739994373   to: 0.001807487686417857
i:   6, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.002791395134769852   to: 0.002132479383554946
i:   7, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.003242834884750500   to: 0.002505524170346567
i:   8, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.003735861325188183   to: 0.002923970924233093
i:   9, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.004267977808455275   to: 0.003385241457067738
i:  10, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.004836766919295870   to: 0.003886832535510820
i:  11, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.005439891245652250   to: 0.004426317388344958
i:  12, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.006075093752136323   to: 0.005001346751988250
i:  13, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.006740197799622571   to: 0.005609649501763952
i:  14, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.007433106851034565   to: 0.006249032912941077
i:  15, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.008151803900184846   to: 0.006917382592195964
i:  16, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.008894350657502735   to: 0.007612662116960983
i:  17, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.009658886523643415   to: 0.008332912417120551
i:  18, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.010443627379311139   to: 0.009076250930687427
i:  19, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.011246864217142171   to: 0.009840870562436072
i:  20, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.012066961639172868   to: 0.010625038471983352
i:  21, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.012902356241260876   to: 0.011427094715483577
i:  22, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.013751554903820067   to: 0.012245450762935678
i:  23, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.014613133006372570   to: 0.013078587911085543
i:  24, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.015485732581698962   to: 0.013925055610030535
i:  25, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.016368060423779373   to: 0.014783469719897847
i:  26, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.017258886162251037   to: 0.015652510712358559
i:  27, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.018157040314759350   to: 0.016530921830254990
i:  28, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.019061412327338167   to: 0.017417507217247347
i:  29, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.019970948611816863   to: 0.018311130028124675
i:  30, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.020884650588210020   to: 0.019210710529264732
i:  31, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.021801572739090325   to: 0.020115224197662158
i:  32, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.022720820682076329   to: 0.021023699825969685
i:  33, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.023641549265772839   to: 0.021935217640104216
i:  34, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.024562960693779026   to: 0.022848907435154320
i:  35, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.025484302680726136   to: 0.023763946734583815
i:  36, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.026404866643709779   to: 0.024679558977048571
i:  37, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.027323985931947292   to: 0.025595011734531277
i:  38, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.028241034097003438   to: 0.026509614964940031
i:  39, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.029155423205491672   to: 0.027422719301814630
i:  40, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.030066602195766418   to: 0.028333714383328469
i:  41, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.030974055279767727   to: 0.029242027222363538
i:  42, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.031877300390868037   to: 0.030147120619069146
i:  43, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.032775887678287242   to: 0.031048491616982244
i:  44, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.033669398048395631   to: 0.031945670003493423
i:  45, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.034557441753002398   to: 0.032838216855178397
i:  46, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.035439657024533458   to: 0.033725723128280663
i:  47, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.036315708757830446   to: 0.034607808294422837
i:  48, name:               module.fc.weight  changing lr from: 0.037185287238155132   to: 0.035484119021442238
i:  49, name:                 module.fc.bias  changing lr from: 0.038048106914852935   to: 0.036354327899084764



# Switched to train mode...
Epoch: [53][  0/391]	Time  0.195 ( 0.195)	Data  0.154 ( 0.154)	Loss 6.5222e-03 (6.5222e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.042 ( 0.056)	Data  0.001 ( 0.016)	Loss 1.0279e-02 (8.5613e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.042 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.0085e-02 (9.5378e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.041 ( 0.047)	Data  0.001 ( 0.007)	Loss 3.2225e-03 (9.2939e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.006)	Loss 1.4343e-02 (9.7765e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 8.8505e-03 (9.5259e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.005)	Loss 7.7779e-03 (9.8694e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.2545e-03 (1.0273e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.043 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0357e-02 (1.0445e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 8.5582e-03 (1.0531e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 7.3439e-03 (1.0509e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 1.0937e-02 (1.0698e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.3923e-02 (1.0712e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 3.4316e-03 (1.0553e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0313e-02 (1.0827e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.7441e-03 (1.0823e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8536e-02 (1.0834e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.6805e-02 (1.0865e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.6826e-03 (1.0811e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.8873e-02 (1.0949e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.1594e-02 (1.1415e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2649e-02 (1.1432e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.044 ( 0.043)	Data  0.001 ( 0.003)	Loss 9.8751e-03 (1.1765e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.4897e-03 (1.1709e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0261e-02 (1.1762e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.0749e-02 (1.1848e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 8.7971e-03 (1.2026e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4515e-02 (1.2037e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.1589e-02 (1.2228e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.7722e-02 (1.2411e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3367e-02 (1.2339e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 2.5217e-02 (1.2401e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.5918e-03 (1.2320e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4324e-02 (1.2259e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.5142e-02 (1.2370e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.1673e-03 (1.2445e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.9050e-03 (1.2410e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.4869e-03 (1.2557e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.6124e-02 (1.2538e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.3456e-02 (1.2551e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.11635637283325195
## e[53]       loss.backward (sum) time: 2.316835880279541
## e[53]      optimizer.step (sum) time: 0.8520238399505615
## epoch[53] training(only) time: 16.808146953582764
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.3730e+00 (1.3730e+00)	Acc@1  77.00 ( 77.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.3744e+00 (1.3365e+00)	Acc@1  77.00 ( 74.09)	Acc@5  92.00 ( 91.73)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2204e+00 (1.2779e+00)	Acc@1  78.00 ( 75.19)	Acc@5  93.00 ( 92.00)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.9528e+00 (1.3116e+00)	Acc@1  66.00 ( 74.32)	Acc@5  90.00 ( 91.81)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3955e+00 (1.3167e+00)	Acc@1  76.00 ( 74.10)	Acc@5  94.00 ( 92.15)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.6542e+00 (1.3404e+00)	Acc@1  69.00 ( 73.76)	Acc@5  90.00 ( 91.94)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.4471e+00 (1.3182e+00)	Acc@1  68.00 ( 73.92)	Acc@5  92.00 ( 92.16)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6175e+00 (1.3243e+00)	Acc@1  68.00 ( 73.65)	Acc@5  92.00 ( 92.25)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6660e+00 (1.3297e+00)	Acc@1  69.00 ( 73.51)	Acc@5  86.00 ( 92.19)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.4849e+00 (1.3094e+00)	Acc@1  74.00 ( 73.86)	Acc@5  89.00 ( 92.25)
 * Acc@1 73.910 Acc@5 92.370
### epoch[53] execution time: 19.322120189666748
EPOCH 54
REMOVING: module.conv2_x.1.residual_function.3.weight
REMOVING: module.conv2_x.1.residual_function.4.weight
i:   0, name: module.conv2_x.1.residual_function.4.bias  changing lr from: 0.001148377557659357   to: 0.001009251541837230
i:   1, name: module.conv3_x.0.residual_function.0.weight  changing lr from: 0.001312616564112195   to: 0.001074833507207245
i:   2, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.001533270547268200   to: 0.001200854713179210
i:   3, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.001807487686417857   to: 0.001384424832582281
i:   4, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.002132479383554946   to: 0.001622707323790189
i:   5, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.002505524170346567   to: 0.001912924462413750
i:   6, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.002923970924233093   to: 0.002252361597825169
i:   7, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.003385241457067738   to: 0.002638370702098840
i:   8, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.003886832535510820   to: 0.003068373274772578
i:   9, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.004426317388344958   to: 0.003539862662758452
i:  10, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.005001346751988250   to: 0.004050405850786270
i:  11, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.005609649501763952   to: 0.004597644773960052
i:  12, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.006249032912941077   to: 0.005179297200358811
i:  13, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.006917382592195964   to: 0.005793157228127504
i:  14, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.007612662116960983   to: 0.006437095438185303
i:  15, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.008332912417120551   to: 0.007109058740529322
i:  16, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.009076250930687427   to: 0.007807069949133821
i:  17, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.009840870562436072   to: 0.008529227117635157
i:  18, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.010625038471983352   to: 0.009273702665348993
i:  19, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.011427094715483577   to: 0.010038742320686904
i:  20, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.012245450762935678   to: 0.010822663906714113
i:  21, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.013078587911085543   to: 0.011623855991422044
i:  22, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.013925055610030535   to: 0.012440776423263161
i:  23, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.014783469719897847   to: 0.013271950770613626
i:  24, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.015652510712358559   to: 0.014115970682078084
i:  25, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.016530921830254990   to: 0.014971492182929657
i:  26, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.017417507217247347   to: 0.015837233921475635
i:  27, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.018311130028124675   to: 0.016711975377752014
i:  28, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.019210710529264732   to: 0.017594555045670129
i:  29, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.020115224197662158   to: 0.018483868598559213
i:  30, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.021023699825969685   to: 0.019378867046964971
i:  31, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.021935217640104216   to: 0.020278554896569597
i:  32, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.022848907435154320   to: 0.021181988313185601
i:  33, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.023763946734583815   to: 0.022088273300943095
i:  34, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.024679558977048571   to: 0.022996563899026087
i:  35, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.025595011734531277   to: 0.023906060401620452
i:  36, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.026509614964940031   to: 0.024816007605101543
i:  37, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.027422719301814630   to: 0.025725693085916157
i:  38, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.028333714383328469   to: 0.026634445512091733
i:  39, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.029242027222363538   to: 0.027541632990833023
i:  40, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.030147120619069146   to: 0.028446661454241951
i:  41, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.031048491616982244   to: 0.029348973084810048
i:  42, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.031945670003493423   to: 0.030248044781989003
i:  43, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.032838216855178397   to: 0.031143386670834390
i:  44, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.033725723128280663   to: 0.032034540653440205
i:  45, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.034607808294422837   to: 0.032921079003634494
i:  46, name:               module.fc.weight  changing lr from: 0.035484119021442238   to: 0.033802603005186775
i:  47, name:                 module.fc.bias  changing lr from: 0.036354327899084764   to: 0.034678741633583254



# Switched to train mode...
Epoch: [54][  0/391]	Time  0.192 ( 0.192)	Data  0.151 ( 0.151)	Loss 1.7691e-02 (1.7691e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.041 ( 0.055)	Data  0.001 ( 0.016)	Loss 1.6720e-02 (1.1840e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.040 ( 0.049)	Data  0.001 ( 0.009)	Loss 1.1037e-02 (1.1727e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.040 ( 0.046)	Data  0.001 ( 0.007)	Loss 6.3777e-03 (1.1325e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.042 ( 0.045)	Data  0.001 ( 0.006)	Loss 1.2981e-02 (1.1230e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.041 ( 0.045)	Data  0.001 ( 0.005)	Loss 6.7406e-03 (1.1324e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.7323e-03 (1.1276e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.042 ( 0.044)	Data  0.001 ( 0.004)	Loss 4.0110e-03 (1.1057e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.004)	Loss 6.7777e-03 (1.1121e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.1750e-02 (1.1090e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.5285e-03 (1.0807e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.4411e-02 (1.0923e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.6215e-03 (1.0784e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.8296e-03 (1.0671e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.1782e-03 (1.0637e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.042 ( 0.043)	Data  0.001 ( 0.003)	Loss 7.1836e-03 (1.0514e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 6.5468e-03 (1.0422e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.003)	Loss 1.2381e-02 (1.0329e-02)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.043 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.5854e-03 (1.0255e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.4462e-03 (1.0306e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.6485e-03 (1.0927e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.7502e-02 (1.0898e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2151e-02 (1.0809e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.2205e-02 (1.0854e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.4164e-02 (1.0809e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.6161e-03 (1.0734e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.3509e-03 (1.0693e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.5661e-03 (1.0709e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1230e-02 (1.0934e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.1041e-02 (1.0889e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.5309e-02 (1.0893e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.3736e-03 (1.0933e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 9.4187e-03 (1.0874e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.8753e-02 (1.0944e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.1289e-02 (1.1046e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 3.3752e-02 (1.1160e-02)	Acc@1  98.44 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.0032e-03 (1.1148e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.8476e-03 (1.1123e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1698e-03 (1.1146e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 5.8966e-03 (1.1122e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.11403417587280273
## e[54]       loss.backward (sum) time: 2.323486804962158
## e[54]      optimizer.step (sum) time: 0.8028366565704346
## epoch[54] training(only) time: 16.554813623428345
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.3748e+00 (1.3748e+00)	Acc@1  74.00 ( 74.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.3757e+00 (1.3538e+00)	Acc@1  75.00 ( 74.18)	Acc@5  92.00 ( 90.73)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.2638e+00 (1.2782e+00)	Acc@1  76.00 ( 75.05)	Acc@5  92.00 ( 91.43)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8206e+00 (1.3096e+00)	Acc@1  63.00 ( 74.13)	Acc@5  89.00 ( 91.29)
Test: [ 40/100]	Time  0.024 ( 0.026)	Loss 1.4053e+00 (1.3053e+00)	Acc@1  68.00 ( 73.73)	Acc@5  95.00 ( 91.68)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.6810e+00 (1.3261e+00)	Acc@1  71.00 ( 73.47)	Acc@5  90.00 ( 91.63)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3989e+00 (1.3007e+00)	Acc@1  65.00 ( 73.61)	Acc@5  93.00 ( 91.85)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.7281e+00 (1.3066e+00)	Acc@1  69.00 ( 73.54)	Acc@5  90.00 ( 91.94)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5551e+00 (1.3091e+00)	Acc@1  74.00 ( 73.57)	Acc@5  86.00 ( 91.98)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.5056e+00 (1.2908e+00)	Acc@1  73.00 ( 73.92)	Acc@5  89.00 ( 92.09)
 * Acc@1 73.910 Acc@5 92.200
### epoch[54] execution time: 19.043449640274048
EPOCH 55
REMOVING: module.conv2_x.1.residual_function.4.bias
REMOVING: module.conv3_x.0.residual_function.0.weight
i:   0, name: module.conv3_x.0.residual_function.1.weight  changing lr from: 0.001200854713179210   to: 0.001027464687651402
i:   1, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.001384424832582281   to: 0.001116248870431601
i:   2, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.001622707323790189   to: 0.001263634779995264
i:   3, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.001912924462413750   to: 0.001466804121135400
i:   4, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.002252361597825169   to: 0.001722993035227326
i:   5, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.002638370702098840   to: 0.002029496690354259
i:   6, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.003068373274772578   to: 0.002383673148914899
i:   7, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.003539862662758452   to: 0.002782946575977129
i:   8, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.004050405850786270   to: 0.003224809847694248
i:   9, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.004597644773960052   to: 0.003706826615263070
i:  10, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.005179297200358811   to: 0.004226632876191371
i:  11, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.005793157228127504   to: 0.004781938101071314
i:  12, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.006437095438185303   to: 0.005370525960633454
i:  13, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.007109058740529322   to: 0.005990254694588737
i:  14, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.007807069949133821   to: 0.006639057160659967
i:  15, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.008529227117635157   to: 0.007314940599257202
i:  16, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.009273702665348993   to: 0.008015986146467074
i:  17, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.010038742320686904   to: 0.008740348125400414
i:  18, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.010822663906714113   to: 0.009486253143471988
i:  19, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.011623855991422044   to: 0.010251999020870799
i:  20, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.012440776423263161   to: 0.011035953573308659
i:  21, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.013271950770613626   to: 0.011836553270109944
i:  22, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.014115970682078084   to: 0.012652301786814757
i:  23, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.014971492182929657   to: 0.013481768469710921
i:  24, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.015837233921475635   to: 0.014323586728076061
i:  25, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.016711975377752014   to: 0.015176452368397552
i:  26, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.017594555045670129   to: 0.016039121883436723
i:  27, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.018483868598559213   to: 0.016910410707707168
i:  28, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.019378867046964971   to: 0.017789191449743880
i:  29, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.020278554896569597   to: 0.018674392110438198
i:  30, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.021181988313185601   to: 0.019564994295701500
i:  31, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.022088273300943095   to: 0.020460031430792294
i:  32, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.022996563899026087   to: 0.021358586982788075
i:  33, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.023906060401620452   to: 0.022259792696906030
i:  34, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.024816007605101543   to: 0.023162826851662127
i:  35, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.025725693085916157   to: 0.024066912537210706
i:  36, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.026634445512091733   to: 0.024971315960614578
i:  37, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.027541632990833023   to: 0.025875344781257401
i:  38, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.028446661454241951   to: 0.026778346479124596
i:  39, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.029348973084810048   to: 0.027679706758235296
i:  40, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.030248044781989003   to: 0.028578847987110634
i:  41, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.031143386670834390   to: 0.029475227677803764
i:  42, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.032034540653440205   to: 0.030368337004693404
i:  43, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.032921079003634494   to: 0.031257699363951910
i:  44, name:               module.fc.weight  changing lr from: 0.033802603005186775   to: 0.032142868974340648
i:  45, name:                 module.fc.bias  changing lr from: 0.034678741633583254   to: 0.033023429519752727



# Switched to train mode...
Epoch: [55][  0/391]	Time  0.196 ( 0.196)	Data  0.154 ( 0.154)	Loss 8.5429e-03 (8.5429e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.041 ( 0.055)	Data  0.001 ( 0.016)	Loss 6.5306e-03 (1.0553e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.040 ( 0.048)	Data  0.001 ( 0.009)	Loss 8.9956e-03 (9.7197e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.042 ( 0.046)	Data  0.001 ( 0.007)	Loss 7.0221e-03 (8.9767e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.006)	Loss 7.2170e-03 (8.9715e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.041 ( 0.044)	Data  0.001 ( 0.005)	Loss 5.6220e-03 (1.0298e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 1.6462e-02 (1.1075e-02)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 7.8026e-03 (1.0766e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.004)	Loss 7.0068e-03 (1.0915e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 1.1873e-02 (1.0910e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 4.6898e-03 (1.0812e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2183e-02 (1.1157e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.0360e-02 (1.1157e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7933e-03 (1.0993e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.6607e-03 (1.1108e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.1210e-03 (1.1030e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.5065e-02 (1.0966e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.4939e-02 (1.1062e-02)	Acc@1  98.44 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.3921e-02 (1.1246e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.2228e-03 (1.1135e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 9.8492e-03 (1.1094e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.7854e-02 (1.1060e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.9489e-03 (1.1191e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 9.9375e-03 (1.1418e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 8.5941e-03 (1.1360e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.0356e-02 (1.1329e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.0291e-03 (1.1275e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.9169e-03 (1.1174e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.4698e-03 (1.1149e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1172e-02 (1.1043e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.3111e-02 (1.0994e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.1481e-03 (1.0959e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.2170e-02 (1.0908e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.8429e-02 (1.0997e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.2057e-02 (1.1075e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.2960e-02 (1.1005e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1840e-02 (1.1320e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.8328e-03 (1.1287e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 9.6197e-03 (1.1497e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.002)	Loss 7.0332e-03 (1.1614e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.11209416389465332
## e[55]       loss.backward (sum) time: 2.279088258743286
## e[55]      optimizer.step (sum) time: 0.7770395278930664
## epoch[55] training(only) time: 16.143317461013794
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.3421e+00 (1.3421e+00)	Acc@1  77.00 ( 77.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.3418e+00 (1.3547e+00)	Acc@1  74.00 ( 74.09)	Acc@5  92.00 ( 91.09)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1996e+00 (1.2860e+00)	Acc@1  78.00 ( 75.10)	Acc@5  90.00 ( 91.57)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8646e+00 (1.3229e+00)	Acc@1  71.00 ( 74.42)	Acc@5  90.00 ( 91.65)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3982e+00 (1.3164e+00)	Acc@1  72.00 ( 74.27)	Acc@5  93.00 ( 91.95)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.7137e+00 (1.3397e+00)	Acc@1  68.00 ( 73.82)	Acc@5  90.00 ( 91.76)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3952e+00 (1.3179e+00)	Acc@1  69.00 ( 73.72)	Acc@5  93.00 ( 91.97)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6078e+00 (1.3189e+00)	Acc@1  68.00 ( 73.52)	Acc@5  90.00 ( 91.97)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5833e+00 (1.3214e+00)	Acc@1  69.00 ( 73.60)	Acc@5  87.00 ( 91.85)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.5343e+00 (1.3054e+00)	Acc@1  72.00 ( 73.91)	Acc@5  89.00 ( 91.97)
 * Acc@1 74.070 Acc@5 92.130
### epoch[55] execution time: 18.646336793899536
EPOCH 56
REMOVING: module.conv3_x.0.residual_function.1.weight
i:   0, name: module.conv3_x.0.residual_function.1.bias  changing lr from: 0.001116248870431601   to: 0.001003796975410464
i:   1, name: module.conv3_x.0.residual_function.3.weight  changing lr from: 0.001263634779995264   to: 0.001056357641709987
i:   2, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.001466804121135400   to: 0.001168494401721292
i:   3, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.001722993035227326   to: 0.001337409881291095
i:   4, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.002029496690354259   to: 0.001560356803966783
i:   5, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.002383673148914899   to: 0.001834642898355853
i:   6, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.002782946575977129   to: 0.002157635069759133
i:   7, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.003224809847694248   to: 0.002526762899079531
i:   8, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.003706826615263070   to: 0.002939521528192662
i:   9, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.004226632876191371   to: 0.003393473987240946
i:  10, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.004781938101071314   to: 0.003886253015698942
i:  11, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.005370525960633454   to: 0.004415562425569218
i:  12, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.005990254694588737   to: 0.004979178051714371
i:  13, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.006639057160659967   to: 0.005574948331123025
i:  14, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.007314940599257202   to: 0.006200794549846982
i:  15, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.008015986146467074   to: 0.006854710793438811
i:  16, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.008740348125400414   to: 0.007534763633964569
i:  17, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.009486253143471988   to: 0.008239091584061609
i:  18, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.010251999020870799   to: 0.008965904346059761
i:  19, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.011035953573308659   to: 0.009713481881876988
i:  20, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.011836553270109944   to: 0.010480173327238079
i:  21, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.012652301786814757   to: 0.011264395771740175
i:  22, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.013481768469710921   to: 0.012064632924398733
i:  23, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.014323586728076061   to: 0.012879433682544163
i:  24, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.015176452368397552   to: 0.013707410620299974
i:  25, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.016039121883436723   to: 0.014547238411350149
i:  26, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.016910410707707168   to: 0.015397652199290020
i:  27, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.017789191449743880   to: 0.016257445927549141
i:  28, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.018674392110438198   to: 0.017125470639665557
i:  29, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.019564994295701500   to: 0.018000632759576179
i:  30, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.020460031430792294   to: 0.018881892360562407
i:  31, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.021358586982788075   to: 0.019768261430544082
i:  32, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.022259792696906030   to: 0.020658802140550176
i:  33, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.023162826851662127   to: 0.021552625122397435
i:  34, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.024066912537210706   to: 0.022448887760882510
i:  35, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.024971315960614578   to: 0.023346792505127896
i:  36, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.025875344781257401   to: 0.024245585203114886
i:  37, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.026778346479124596   to: 0.025144553462886323
i:  38, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.027679706758235296   to: 0.026043025043397772
i:  39, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.028578847987110634   to: 0.026940366277542246
i:  40, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.029475227677803764   to: 0.027835980529460159
i:  41, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.030368337004693404   to: 0.028729306687874163
i:  42, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.031257699363951910   to: 0.029619817696851309
i:  43, name:               module.fc.weight  changing lr from: 0.032142868974340648   to: 0.030507019125094077
i:  44, name:                 module.fc.bias  changing lr from: 0.033023429519752727   to: 0.031390447774589797



# Switched to train mode...
Epoch: [56][  0/391]	Time  0.191 ( 0.191)	Data  0.152 ( 0.152)	Loss 1.1960e-02 (1.1960e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.038 ( 0.054)	Data  0.001 ( 0.015)	Loss 5.8156e-03 (1.1041e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.041 ( 0.048)	Data  0.001 ( 0.009)	Loss 4.3198e-03 (9.6549e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.007)	Loss 3.2753e-02 (1.0214e-02)	Acc@1  99.22 ( 99.90)	Acc@5  99.22 ( 99.97)
Epoch: [56][ 40/391]	Time  0.040 ( 0.044)	Data  0.001 ( 0.006)	Loss 1.4755e-02 (1.0645e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 50/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.005)	Loss 4.3977e-03 (1.0347e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 ( 99.98)
Epoch: [56][ 60/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.004)	Loss 4.5600e-03 (1.0101e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 70/391]	Time  0.041 ( 0.043)	Data  0.001 ( 0.004)	Loss 9.4011e-03 (1.0154e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 1.4101e-02 (1.0397e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 ( 99.99)
Epoch: [56][ 90/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 1.1463e-02 (1.0468e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 ( 99.99)
Epoch: [56][100/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 7.7236e-03 (1.0636e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 ( 99.99)
Epoch: [56][110/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.7959e-03 (1.0499e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 ( 99.99)
Epoch: [56][120/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.0853e-02 (1.0394e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 ( 99.99)
Epoch: [56][130/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 2.2662e-02 (1.0547e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 ( 99.99)
Epoch: [56][140/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.003)	Loss 7.0190e-03 (1.0633e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 ( 99.99)
Epoch: [56][150/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.003)	Loss 1.6351e-02 (1.0991e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 ( 99.99)
Epoch: [56][160/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.003)	Loss 6.1967e-03 (1.1133e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.9043e-02 (1.1112e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.9962e-03 (1.1079e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.9247e-03 (1.1080e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.4392e-03 (1.1324e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1249e-02 (1.1287e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.3033e-02 (1.1165e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.9424e-03 (1.1064e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.9522e-03 (1.1326e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.042 ( 0.041)	Data  0.001 ( 0.003)	Loss 8.1587e-03 (1.1229e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.0272e-02 (1.1194e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1902e-02 (1.1135e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.4491e-02 (1.1146e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.3330e-02 (1.1156e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 8.8262e-03 (1.1135e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.3209e-02 (1.1099e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.3554e-03 (1.1069e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.7160e-02 (1.1068e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.8305e-03 (1.0992e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.7354e-02 (1.1119e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.6764e-03 (1.1125e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1927e-02 (1.1368e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.7295e-03 (1.1298e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.5000e-03 (1.1232e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.11340212821960449
## e[56]       loss.backward (sum) time: 2.2807321548461914
## e[56]      optimizer.step (sum) time: 0.7635664939880371
## epoch[56] training(only) time: 16.124016046524048
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2826e+00 (1.2826e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.3183e+00 (1.3251e+00)	Acc@1  77.00 ( 74.91)	Acc@5  94.00 ( 91.64)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.2239e+00 (1.2631e+00)	Acc@1  76.00 ( 75.48)	Acc@5  91.00 ( 92.05)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7761e+00 (1.2932e+00)	Acc@1  66.00 ( 74.58)	Acc@5  91.00 ( 91.74)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4257e+00 (1.2998e+00)	Acc@1  72.00 ( 74.15)	Acc@5  92.00 ( 91.95)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.6379e+00 (1.3223e+00)	Acc@1  70.00 ( 73.80)	Acc@5  89.00 ( 91.73)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3440e+00 (1.2945e+00)	Acc@1  66.00 ( 74.02)	Acc@5  91.00 ( 91.92)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5938e+00 (1.2966e+00)	Acc@1  70.00 ( 73.77)	Acc@5  90.00 ( 91.94)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6061e+00 (1.3012e+00)	Acc@1  71.00 ( 73.77)	Acc@5  87.00 ( 91.91)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.4488e+00 (1.2810e+00)	Acc@1  74.00 ( 74.19)	Acc@5  90.00 ( 92.04)
 * Acc@1 74.280 Acc@5 92.180
### epoch[56] execution time: 18.594669580459595
EPOCH 57
REMOVING: module.conv3_x.0.residual_function.1.bias
REMOVING: module.conv3_x.0.residual_function.3.weight
i:   0, name: module.conv3_x.0.residual_function.4.weight  changing lr from: 0.001168494401721292   to: 0.001018885481846259
i:   1, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.001337409881291095   to: 0.001096737229336380
i:   2, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.001560356803966783   to: 0.001232289702890137
i:   3, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.001834642898355853   to: 0.001422814659940257
i:   4, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.002157635069759133   to: 0.001665634881713939
i:   5, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.002526762899079531   to: 0.001958128633909205
i:   6, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.002939521528192662   to: 0.002297733442732718
i:   7, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.003393473987240946   to: 0.002681949245245679
i:   8, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.003886253015698942   to: 0.003108340969357681
i:   9, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.004415562425569218   to: 0.003574540595295710
i:  10, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.004979178051714371   to: 0.004078248746973578
i:  11, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.005574948331123025   to: 0.004617235858408447
i:  12, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.006200794549846982   to: 0.005189342957184677
i:  13, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.006854710793438811   to: 0.005792482103956996
i:  14, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.007534763633964569   to: 0.006424636524120525
i:  15, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.008239091584061609   to: 0.007083860465052876
i:  16, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.008965904346059761   to: 0.007768278809759121
i:  17, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.009713481881876988   to: 0.008476086475316452
i:  18, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.010480173327238079   to: 0.009205547622225505
i:  19, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.011264395771740175   to: 0.009954994698621572
i:  20, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.012064632924398733   to: 0.010722827341281178
i:  21, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.012879433682544163   to: 0.011507511153470457
i:  22, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.013707410620299974   to: 0.012307576377918839
i:  23, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.014547238411350149   to: 0.013121616481557529
i:  24, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.015397652199290020   to: 0.013948286667133318
i:  25, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.016257445927549141   to: 0.014786302325388311
i:  26, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.017125470639665557   to: 0.015634437440179776
i:  27, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.018000632759576179   to: 0.016491522957694876
i:  28, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.018881892360562407   to: 0.017356445129790388
i:  29, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.019768261430544082   to: 0.018228143840446614
i:  30, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.020658802140550176   to: 0.019105610923369938
i:  31, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.021552625122397435   to: 0.019987888477895590
i:  32, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.022448887760882510   to: 0.020874067189536105
i:  33, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.023346792505127896   to: 0.021763284660779134
i:  34, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.024245585203114886   to: 0.022654723757058754
i:  35, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.025144553462886323   to: 0.023547610972207086
i:  36, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.026043025043397772   to: 0.024441214817124541
i:  37, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.026940366277542246   to: 0.025334844234894058
i:  38, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.027835980529460159   to: 0.026227847045095627
i:  39, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.028729306687874163   to: 0.027119608419652511
i:  40, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.029619817696851309   to: 0.028009549392155289
i:  41, name:               module.fc.weight  changing lr from: 0.030507019125094077   to: 0.028897125402262792
i:  42, name:                 module.fc.bias  changing lr from: 0.031390447774589797   to: 0.029781824876465048



# Switched to train mode...
Epoch: [57][  0/391]	Time  0.193 ( 0.193)	Data  0.152 ( 0.152)	Loss 5.4831e-03 (5.4831e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.040 ( 0.054)	Data  0.001 ( 0.016)	Loss 5.4485e-03 (9.5073e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.009)	Loss 1.9423e-02 (9.7449e-03)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.007)	Loss 5.3077e-03 (9.1735e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.006)	Loss 7.0370e-03 (9.1024e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.005)	Loss 1.3744e-02 (8.8595e-03)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.005)	Loss 4.7028e-03 (8.6159e-03)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 5.7664e-03 (8.8001e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.004)	Loss 7.0630e-03 (8.5299e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.004)	Loss 4.4425e-03 (8.7099e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.004)	Loss 3.9540e-03 (8.8671e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.9790e-03 (8.6154e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 9.5925e-03 (8.6755e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1851e-02 (8.6345e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.2024e-02 (8.8717e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.2106e-03 (8.9194e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.8490e-03 (8.9389e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 9.9924e-03 (8.9657e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.8361e-03 (9.0775e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.1448e-03 (9.2555e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.4655e-02 (9.3636e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.2436e-03 (9.3900e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.9967e-03 (9.6743e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.4298e-02 (9.6574e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.0289e-03 (9.5372e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 8.7895e-03 (9.6062e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.8807e-03 (9.6372e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1707e-02 (9.5753e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.0240e-03 (9.5871e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.7729e-03 (9.6393e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.7965e-03 (9.7347e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.4539e-03 (9.7623e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.4565e-02 (9.7194e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.041 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.1544e-02 (9.7104e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.4741e-03 (9.7088e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.7619e-03 (9.6759e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2867e-02 (9.6885e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.045 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.6236e-02 (9.8015e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.0346e-03 (9.7523e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.5021e-02 (9.7776e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.10943078994750977
## e[57]       loss.backward (sum) time: 2.2774534225463867
## e[57]      optimizer.step (sum) time: 0.7191498279571533
## epoch[57] training(only) time: 15.913391828536987
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2774e+00 (1.2774e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.3440e+00 (1.3185e+00)	Acc@1  74.00 ( 74.45)	Acc@5  94.00 ( 90.82)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.2683e+00 (1.2525e+00)	Acc@1  77.00 ( 75.24)	Acc@5  89.00 ( 91.48)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8012e+00 (1.2875e+00)	Acc@1  67.00 ( 74.45)	Acc@5  89.00 ( 91.45)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.4395e+00 (1.2871e+00)	Acc@1  69.00 ( 74.20)	Acc@5  91.00 ( 91.78)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5618e+00 (1.3110e+00)	Acc@1  73.00 ( 73.92)	Acc@5  90.00 ( 91.55)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3559e+00 (1.2860e+00)	Acc@1  69.00 ( 74.10)	Acc@5  94.00 ( 91.87)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5894e+00 (1.2865e+00)	Acc@1  68.00 ( 73.94)	Acc@5  89.00 ( 91.85)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6151e+00 (1.2917e+00)	Acc@1  70.00 ( 73.93)	Acc@5  88.00 ( 91.89)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.4684e+00 (1.2725e+00)	Acc@1  74.00 ( 74.32)	Acc@5  91.00 ( 92.04)
 * Acc@1 74.460 Acc@5 92.180
### epoch[57] execution time: 18.393614768981934
EPOCH 58
REMOVING: module.conv3_x.0.residual_function.4.weight
i:   0, name: module.conv3_x.0.residual_function.4.bias  changing lr from: 0.001096737229336380   to: 0.001001677338241030
i:   1, name: module.conv3_x.0.shortcut.0.weight  changing lr from: 0.001232289702890137   to: 0.001046231505120442
i:   2, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.001422814659940257   to: 0.001149337691322254
i:   3, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.001665634881713939   to: 0.001308288886898950
i:   4, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.001958128633909205   to: 0.001520425178278317
i:   5, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.002297733442732718   to: 0.001783138482773738
i:   6, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.002681949245245679   to: 0.002093876588044745
i:   7, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.003108340969357681   to: 0.002450146555115584
i:   8, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.003574540595295710   to: 0.002849517540072804
i:   9, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.004078248746973578   to: 0.003289623086154952
i:  10, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.004617235858408447   to: 0.003768162934636775
i:  11, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.005189342957184677   to: 0.004282904399709141
i:  12, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.005792482103956996   to: 0.004831683349476128
i:  13, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.006424636524120525   to: 0.005412404832239376
i:  14, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.007083860465052876   to: 0.006023043384420456
i:  15, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.007768278809759121   to: 0.006661643053790855
i:  16, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.008476086475316452   to: 0.007326317169133528
i:  17, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.009205547622225505   to: 0.008015247885052989
i:  18, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.009954994698621572   to: 0.008726685528378242
i:  19, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.010722827341281178   to: 0.009458947770464864
i:  20, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.011507511153470457   to: 0.010210418647693065
i:  21, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.012307576377918839   to: 0.010979547450576093
i:  22, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.013121616481557529   to: 0.011764847500132895
i:  23, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.013948286667133318   to: 0.012564894828533762
i:  24, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.014786302325388311   to: 0.013378326779497397
i:  25, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.015634437440179776   to: 0.014203840542491675
i:  26, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.016491522957694876   to: 0.015040191633467863
i:  27, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.017356445129790388   to: 0.015886192333631803
i:  28, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.018228143840446614   to: 0.016740710096618924
i:  29, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.019105610923369938   to: 0.017602665933393231
i:  30, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.019987888477895590   to: 0.018471032783219887
i:  31, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.020874067189536105   to: 0.019344833878171371
i:  32, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.021763284660779134   to: 0.020223141107806888
i:  33, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.022654723757058754   to: 0.021105073389910273
i:  34, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.023547610972207086   to: 0.021989795052483552
i:  35, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.024441214817124541   to: 0.022876514231558839
i:  36, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.025334844234894058   to: 0.023764481288815305
i:  37, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.026227847045095627   to: 0.024652987252460502
i:  38, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.027119608419652511   to: 0.025541362284354892
i:  39, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.028009549392155289   to: 0.026428974175922073
i:  40, name:               module.fc.weight  changing lr from: 0.028897125402262792   to: 0.027315226874991511
i:  41, name:                 module.fc.bias  changing lr from: 0.029781824876465048   to: 0.028199559045361634



# Switched to train mode...
Epoch: [58][  0/391]	Time  0.192 ( 0.192)	Data  0.150 ( 0.150)	Loss 6.9046e-03 (6.9046e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.040 ( 0.053)	Data  0.001 ( 0.015)	Loss 5.0903e-03 (7.9383e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.009)	Loss 6.4180e-03 (8.4339e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.040 ( 0.045)	Data  0.001 ( 0.007)	Loss 5.9930e-03 (8.8154e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.040 ( 0.043)	Data  0.001 ( 0.006)	Loss 4.8281e-03 (9.2022e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.005)	Loss 8.7714e-03 (9.0250e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.004)	Loss 8.0407e-03 (8.7559e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 5.5761e-03 (8.4872e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.004)	Loss 8.2202e-03 (8.6766e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.041 ( 0.042)	Data  0.001 ( 0.004)	Loss 1.4173e-02 (9.1032e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.9983e-03 (8.9117e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.5493e-03 (8.7686e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 6.0729e-03 (8.7365e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.2612e-02 (8.7957e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1484e-02 (9.0195e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.1472e-02 (8.8691e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.8722e-03 (8.7853e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.3275e-03 (8.8547e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.8477e-03 (8.8863e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.6141e-02 (8.9688e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.1087e-03 (9.0003e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 8.4015e-03 (8.9392e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.7252e-03 (8.9558e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.7334e-02 (9.0723e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.6165e-03 (9.1486e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.3355e-03 (9.0575e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.0241e-03 (9.3365e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 7.6456e-03 (9.2211e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 3.9559e-02 (9.3920e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 9.4236e-03 (9.3348e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 4.0563e-03 (9.2462e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 8.5810e-03 (9.2277e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 3.0013e-02 (9.4161e-03)	Acc@1  98.44 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.2957e-02 (9.3897e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.2447e-03 (9.3117e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.5038e-03 (9.4112e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.9674e-03 (9.5058e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.1307e-02 (9.5142e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.0045e-03 (9.5713e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 3.9959e-03 (9.5842e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.10563850402832031
## e[58]       loss.backward (sum) time: 2.2435317039489746
## e[58]      optimizer.step (sum) time: 0.7032058238983154
## epoch[58] training(only) time: 15.857950925827026
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.3129e+00 (1.3129e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.4039e+00 (1.3153e+00)	Acc@1  72.00 ( 74.27)	Acc@5  92.00 ( 91.36)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1877e+00 (1.2419e+00)	Acc@1  76.00 ( 74.95)	Acc@5  89.00 ( 92.14)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7632e+00 (1.2844e+00)	Acc@1  64.00 ( 74.13)	Acc@5  90.00 ( 91.81)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4294e+00 (1.2873e+00)	Acc@1  69.00 ( 73.88)	Acc@5  91.00 ( 91.93)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5799e+00 (1.3063e+00)	Acc@1  70.00 ( 73.61)	Acc@5  89.00 ( 91.65)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3683e+00 (1.2822e+00)	Acc@1  68.00 ( 73.79)	Acc@5  94.00 ( 91.93)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6012e+00 (1.2810e+00)	Acc@1  69.00 ( 73.62)	Acc@5  89.00 ( 91.97)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5922e+00 (1.2876e+00)	Acc@1  70.00 ( 73.56)	Acc@5  87.00 ( 92.02)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.4515e+00 (1.2677e+00)	Acc@1  72.00 ( 73.97)	Acc@5  90.00 ( 92.10)
 * Acc@1 74.120 Acc@5 92.240
### epoch[58] execution time: 18.370075225830078
EPOCH 59
REMOVING: module.conv3_x.0.residual_function.4.bias
REMOVING: module.conv3_x.0.shortcut.0.weight
i:   0, name: module.conv3_x.0.shortcut.1.weight  changing lr from: 0.001149337691322254   to: 0.001014975163819658
i:   1, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.001308288886898950   to: 0.001086572432549725
i:   2, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.001520425178278317   to: 0.001214821102353291
i:   3, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.001783138482773738   to: 0.001397080588669046
i:   4, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.002093876588044745   to: 0.001630758534767289
i:   5, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.002450146555115584   to: 0.001913315100746420
i:   6, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.002849517540072804   to: 0.002242266606692388
i:   7, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.003289623086154952   to: 0.002615188584814414
i:   8, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.003768162934636775   to: 0.003029718292070226
i:   9, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.004282904399709141   to: 0.003483556731576519
i:  10, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.004831683349476128   to: 0.003974470227980780
i:  11, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.005412404832239376   to: 0.004500291598961203
i:  12, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.006023043384420456   to: 0.005058920962128260
i:  13, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.006661643053790855   to: 0.005648326213835258
i:  14, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.007326317169133528   to: 0.006266543213763692
i:  15, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.008015247885052989   to: 0.006911675706641023
i:  16, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.008726685528378242   to: 0.007581895010067779
i:  17, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.009458947770464864   to: 0.008275439495183039
i:  18, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.010210418647693065   to: 0.008990613884774876
i:  19, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.010979547450576093   to: 0.009725788391447353
i:  20, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.011764847500132895   to: 0.010479397716581858
i:  21, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.012564894828533762   to: 0.011249939929074904
i:  22, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.013378326779497397   to: 0.012035975241193726
i:  23, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.014203840542491675   to: 0.012836124697358797
i:  24, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.015040191633467863   to: 0.013649068790235596
i:  25, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.015886192333631803   to: 0.014473546017191186
i:  26, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.016740710096618924   to: 0.015308351388937928
i:  27, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.017602665933393231   to: 0.016152334901046245
i:  28, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.018471032783219887   to: 0.017004399977949624
i:  29, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.019344833878171371   to: 0.017863501898089492
i:  30, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.020223141107806888   to: 0.018728646207946703
i:  31, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.021105073389910273   to: 0.019598887131875003
i:  32, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.021989795052483552   to: 0.020473325983890879
i:  33, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.022876514231558839   to: 0.021351109586870084
i:  34, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.023764481288815305   to: 0.022231428703961290
i:  35, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.024652987252460502   to: 0.023113516486437204
i:  36, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.025541362284354892   to: 0.023996646941666113
i:  37, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.026428974175922073   to: 0.024880133424396062
i:  38, name:               module.fc.weight  changing lr from: 0.027315226874991511   to: 0.025763327154097061
i:  39, name:                 module.fc.bias  changing lr from: 0.028199559045361634   to: 0.026645615760700569



# Switched to train mode...
Epoch: [59][  0/391]	Time  0.190 ( 0.190)	Data  0.148 ( 0.148)	Loss 1.0690e-02 (1.0690e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.039 ( 0.053)	Data  0.001 ( 0.015)	Loss 4.1687e-03 (7.5476e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.009)	Loss 4.4692e-03 (7.0596e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.007)	Loss 7.5250e-03 (7.9902e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.006)	Loss 1.3514e-02 (7.8696e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.005)	Loss 4.4410e-03 (7.7534e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.004)	Loss 1.8086e-02 (8.4638e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.004)	Loss 1.5858e-02 (8.4140e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.004)	Loss 6.4143e-03 (8.4676e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.004)	Loss 1.6873e-02 (8.4631e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.3843e-03 (8.3687e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.7940e-03 (8.5880e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.9324e-02 (8.5176e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.1596e-03 (8.6044e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.2270e-03 (8.5756e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 8.8220e-03 (8.4423e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.1656e-03 (8.4118e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.3669e-03 (8.4289e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.6885e-03 (8.4124e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 9.0413e-03 (8.4137e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.1885e-02 (8.3397e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.4791e-03 (8.4799e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.4797e-03 (8.5156e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 5.3915e-03 (8.4554e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.6421e-02 (8.5213e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 7.1257e-03 (8.5120e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 5.6657e-03 (8.4848e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 7.6261e-03 (8.4768e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.1338e-03 (8.6347e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 3.4398e-02 (8.8637e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.1694e-02 (8.9259e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.0017e-02 (8.8952e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.3940e-02 (8.8991e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.6507e-03 (8.8506e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.0798e-03 (8.8670e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.5654e-03 (8.8460e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.3868e-03 (8.7703e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 2.3851e-02 (8.7686e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 1.9178e-02 (8.7840e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.2227e-03 (8.7588e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.10272836685180664
## e[59]       loss.backward (sum) time: 2.207644462585449
## e[59]      optimizer.step (sum) time: 0.6842176914215088
## epoch[59] training(only) time: 15.781214475631714
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2894e+00 (1.2894e+00)	Acc@1  77.00 ( 77.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.3366e+00 (1.2990e+00)	Acc@1  75.00 ( 74.91)	Acc@5  94.00 ( 91.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1521e+00 (1.2221e+00)	Acc@1  77.00 ( 75.43)	Acc@5  91.00 ( 92.10)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7607e+00 (1.2625e+00)	Acc@1  65.00 ( 74.61)	Acc@5  90.00 ( 91.71)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4427e+00 (1.2650e+00)	Acc@1  71.00 ( 74.29)	Acc@5  92.00 ( 91.95)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.6178e+00 (1.2863e+00)	Acc@1  70.00 ( 73.90)	Acc@5  90.00 ( 91.75)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3066e+00 (1.2629e+00)	Acc@1  67.00 ( 74.05)	Acc@5  95.00 ( 92.05)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6159e+00 (1.2633e+00)	Acc@1  70.00 ( 73.94)	Acc@5  89.00 ( 92.01)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6245e+00 (1.2705e+00)	Acc@1  70.00 ( 73.99)	Acc@5  87.00 ( 92.05)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.4458e+00 (1.2505e+00)	Acc@1  75.00 ( 74.34)	Acc@5  91.00 ( 92.16)
 * Acc@1 74.460 Acc@5 92.290
### epoch[59] execution time: 18.282967805862427
EPOCH 60
REMOVING: module.conv3_x.0.shortcut.1.weight
i:   0, name: module.conv3_x.0.shortcut.1.bias  changing lr from: 0.001086572432549725   to: 0.001001090675947901
i:   1, name: module.conv3_x.1.residual_function.0.weight  changing lr from: 0.001214821102353291   to: 0.001042132300551390
i:   2, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.001397080588669046   to: 0.001140568007337044
i:   3, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.001630758534767289   to: 0.001293778346912688
i:   4, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.001913315100746420   to: 0.001499188555805785
i:   5, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.002242266606692388   to: 0.001754273081017903
i:   6, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.002615188584814414   to: 0.002056559450650202
i:   7, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.003029718292070226   to: 0.002403631545025069
i:   8, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.003483556731576519   to: 0.002793132319537649
i:   9, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.003974470227980780   to: 0.003222766027349654
i:  10, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.004500291598961203   to: 0.003690299987002759
i:  11, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.005058920962128260   to: 0.004193565937090548
i:  12, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.005648326213835258   to: 0.004730461017300241
i:  13, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.006266543213763692   to: 0.005298948412420073
i:  14, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.006911675706641023   to: 0.005897057693314855
i:  15, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.007581895010067779   to: 0.006522884886400199
i:  16, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.008275439495183039   to: 0.007174592300798704
i:  17, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.008990613884774876   to: 0.007850408140137943
i:  18, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.009725788391447353   to: 0.008548625923849328
i:  19, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.010479397716581858   to: 0.009267603740848000
i:  20, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.011249939929074904   to: 0.010005763356611037
i:  21, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.012035975241193726   to: 0.010761589192925941
i:  22, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.012836124697358797   to: 0.011533627197944523
i:  23, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.013649068790235596   to: 0.012320483622648214
i:  24, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.014473546017191186   to: 0.013120823718405460
i:  25, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.015308351388937928   to: 0.013933370368971208
i:  26, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.016152334901046245   to: 0.014756902669045231
i:  27, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.017004399977949624   to: 0.015590254460357479
i:  28, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.017863501898089492   to: 0.016432312835186591
i:  29, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.018728646207946703   to: 0.017282016616234731
i:  30, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.019598887131875003   to: 0.018138354820871557
i:  31, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.020473325983890879   to: 0.019000365116924073
i:  32, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.021351109586870084   to: 0.019867132276414880
i:  33, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.022231428703961290   to: 0.020737786632942443
i:  34, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.023113516486437204   to: 0.021611502547744861
i:  35, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.023996646941666113   to: 0.022487496888890124
i:  36, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.024880133424396062   to: 0.023365027527488421
i:  37, name:               module.fc.weight  changing lr from: 0.025763327154097061   to: 0.024243391854322623
i:  38, name:                 module.fc.bias  changing lr from: 0.026645615760700569   to: 0.025121925319835987



# Switched to train mode...
Epoch: [60][  0/391]	Time  0.190 ( 0.190)	Data  0.155 ( 0.155)	Loss 6.7366e-03 (6.7366e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.039 ( 0.053)	Data  0.001 ( 0.016)	Loss 6.0475e-03 (7.2278e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.039 ( 0.047)	Data  0.001 ( 0.009)	Loss 5.1347e-03 (8.5728e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.007)	Loss 4.3966e-03 (7.7624e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.006)	Loss 1.1660e-02 (7.9903e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.005)	Loss 1.1095e-02 (8.2251e-03)	Acc@1  99.22 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.040 ( 0.042)	Data  0.001 ( 0.005)	Loss 3.3972e-02 (8.5166e-03)	Acc@1  98.44 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.004)	Loss 1.5711e-02 (8.4235e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.004)	Loss 5.6452e-03 (8.4416e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.004)	Loss 9.3272e-03 (8.2515e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.004)	Loss 8.5483e-03 (8.0980e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.003)	Loss 9.2552e-03 (8.0758e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.2583e-03 (8.1200e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.2775e-02 (8.0764e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 4.1048e-03 (8.0069e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.0234e-02 (8.1530e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 7.4968e-03 (8.2291e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 3.2812e-03 (8.1250e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 5.2272e-03 (8.3104e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.003)	Loss 2.0219e-02 (8.3261e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.0294e-02 (8.2730e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.3709e-03 (8.2666e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.8414e-03 (8.2314e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 2.8437e-02 (8.3738e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.7706e-03 (8.3384e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 7.0945e-03 (8.2839e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 7.5780e-03 (8.2555e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.6216e-03 (8.1888e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 8.5948e-03 (8.1372e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 8.9386e-03 (8.1379e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.3975e-02 (8.1287e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 2.9084e-03 (8.0857e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.8687e-03 (8.0684e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 7.4435e-03 (8.1461e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 7.3571e-03 (8.1074e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.9793e-03 (8.1601e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.4144e-03 (8.1468e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 6.2048e-03 (8.1562e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 5.3808e-03 (8.1215e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.002)	Loss 8.2539e-03 (8.1160e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.10074472427368164
## e[60]       loss.backward (sum) time: 2.220702886581421
## e[60]      optimizer.step (sum) time: 0.6567418575286865
## epoch[60] training(only) time: 15.8204026222229
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2816e+00 (1.2816e+00)	Acc@1  76.00 ( 76.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.3346e+00 (1.2957e+00)	Acc@1  76.00 ( 74.64)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1945e+00 (1.2274e+00)	Acc@1  76.00 ( 75.52)	Acc@5  90.00 ( 92.00)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7840e+00 (1.2683e+00)	Acc@1  66.00 ( 74.68)	Acc@5  90.00 ( 91.71)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.4347e+00 (1.2706e+00)	Acc@1  67.00 ( 74.27)	Acc@5  92.00 ( 91.88)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5712e+00 (1.2927e+00)	Acc@1  70.00 ( 73.78)	Acc@5  90.00 ( 91.67)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.4038e+00 (1.2673e+00)	Acc@1  65.00 ( 73.97)	Acc@5  93.00 ( 91.97)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6474e+00 (1.2703e+00)	Acc@1  71.00 ( 73.76)	Acc@5  89.00 ( 91.97)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6084e+00 (1.2750e+00)	Acc@1  68.00 ( 73.75)	Acc@5  86.00 ( 91.96)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.4063e+00 (1.2545e+00)	Acc@1  74.00 ( 74.14)	Acc@5  89.00 ( 92.11)
 * Acc@1 74.190 Acc@5 92.240
### epoch[60] execution time: 18.299837827682495
EPOCH 61
REMOVING: module.conv3_x.0.shortcut.1.bias
REMOVING: module.conv3_x.1.residual_function.0.weight
i:   0, name: module.conv3_x.1.residual_function.1.weight  changing lr from: 0.001140568007337044   to: 0.001014270659076232
i:   1, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.001293778346912688   to: 0.001083797005130504
i:   2, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.001499188555805785   to: 0.001208802137079922
i:   3, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.001754273081017903   to: 0.001386730555097181
i:   4, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.002056559450650202   to: 0.001615072720019483
i:   5, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.002403631545025069   to: 0.001891369138700437
i:   6, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.002793132319537649   to: 0.002213213842553168
i:   7, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.003222766027349654   to: 0.002578257310166975
i:   8, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.003690299987002759   to: 0.002984208881856287
i:   9, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.004193565937090548   to: 0.003428838711050435
i:  10, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.004730461017300241   to: 0.003909979294571186
i:  11, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.005298948412420073   to: 0.004425526621080616
i:  12, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.005897057693314855   to: 0.004973440974324782
i:  13, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.006522884886400199   to: 0.005551747425252240
i:  14, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.007174592300798704   to: 0.006158536044656575
i:  15, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.007850408140137943   to: 0.006791961865678924
i:  16, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.008548625923849328   to: 0.007450244623311930
i:  17, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.009267603740848000   to: 0.008131668295969990
i:  18, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.010005763356611037   to: 0.008834580472229080
i:  19, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.010761589192925941   to: 0.009557391563993611
i:  20, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.011533627197944523   to: 0.010298573885611666
i:  21, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.012320483622648214   to: 0.011056660616831925
i:  22, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.013120823718405460   to: 0.011830244665972607
i:  23, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.013933370368971208   to: 0.012617977448247684
i:  24, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.014756902669045231   to: 0.013418567592869180
i:  25, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.015590254460357479   to: 0.014230779591306364
i:  26, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.016432312835186591   to: 0.015053432397933941
i:  27, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.017282016616234731   to: 0.015885397993235146
i:  28, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.018138354820871557   to: 0.016725599918735290
i:  29, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.019000365116924073   to: 0.017573011791928994
i:  30, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.019867132276414880   to: 0.018426655808617250
i:  31, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.020737786632942443   to: 0.019285601239292478
i:  32, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.021611502547744861   to: 0.020148962925491509
i:  33, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.022487496888890124   to: 0.021015899781375975
i:  34, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.023365027527488421   to: 0.021885613305193480
i:  35, name:               module.fc.weight  changing lr from: 0.024243391854322623   to: 0.022757346104717226
i:  36, name:                 module.fc.bias  changing lr from: 0.025121925319835987   to: 0.023630380440253514



# Switched to train mode...
Epoch: [61][  0/391]	Time  0.188 ( 0.188)	Data  0.151 ( 0.151)	Loss 4.7470e-03 (4.7470e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.038 ( 0.052)	Data  0.001 ( 0.015)	Loss 5.4286e-03 (9.8245e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.039 ( 0.046)	Data  0.001 ( 0.009)	Loss 4.6705e-03 (1.0174e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.038 ( 0.044)	Data  0.001 ( 0.007)	Loss 1.0624e-02 (1.0768e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.006)	Loss 1.6374e-02 (1.0162e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.005)	Loss 1.0409e-02 (9.7785e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.004)	Loss 5.1972e-03 (9.5229e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.041 ( 0.041)	Data  0.001 ( 0.004)	Loss 6.3584e-03 (9.4947e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.004)	Loss 4.4691e-03 (9.2760e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.040 ( 0.041)	Data  0.001 ( 0.004)	Loss 9.2839e-03 (9.4701e-03)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.003)	Loss 1.6638e-02 (9.4542e-03)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.1092e-03 (9.2452e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.9955e-02 (9.2787e-03)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 9.6505e-03 (9.2618e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 3.3682e-03 (9.2025e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 5.2417e-03 (9.2802e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.0286e-02 (9.1480e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 8.8513e-03 (9.0719e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.3855e-02 (9.0448e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 8.5800e-03 (9.0614e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.2430e-02 (9.1454e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.003)	Loss 7.3443e-03 (9.0577e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 9.5707e-03 (9.0457e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 5.6412e-03 (9.1191e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.0590e-02 (9.0092e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.6132e-03 (8.9569e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.5391e-03 (8.8756e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 9.6700e-03 (9.0622e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.0088e-02 (8.9880e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.5845e-03 (9.0139e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 6.6401e-03 (8.9683e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 3.4894e-03 (8.9487e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 7.7288e-03 (8.8981e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 5.2107e-03 (8.9435e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 6.0240e-03 (8.8690e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.7090e-02 (9.0331e-03)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.002)	Loss 7.0832e-03 (9.0277e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.1486e-02 (9.0214e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.3518e-02 (9.0302e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 7.9790e-03 (9.0433e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.10080409049987793
## e[61]       loss.backward (sum) time: 2.163403034210205
## e[61]      optimizer.step (sum) time: 0.6354198455810547
## epoch[61] training(only) time: 15.532381534576416
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.3047e+00 (1.3047e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.3156e+00 (1.2871e+00)	Acc@1  76.00 ( 74.18)	Acc@5  94.00 ( 91.55)
Test: [ 20/100]	Time  0.024 ( 0.029)	Loss 1.1727e+00 (1.2259e+00)	Acc@1  76.00 ( 74.90)	Acc@5  90.00 ( 92.05)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.8272e+00 (1.2748e+00)	Acc@1  65.00 ( 73.97)	Acc@5  89.00 ( 91.61)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.4276e+00 (1.2781e+00)	Acc@1  72.00 ( 73.61)	Acc@5  92.00 ( 91.95)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5542e+00 (1.2967e+00)	Acc@1  71.00 ( 73.35)	Acc@5  90.00 ( 91.80)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 1.3636e+00 (1.2695e+00)	Acc@1  68.00 ( 73.66)	Acc@5  92.00 ( 92.07)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6108e+00 (1.2727e+00)	Acc@1  72.00 ( 73.41)	Acc@5  91.00 ( 92.14)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5901e+00 (1.2785e+00)	Acc@1  69.00 ( 73.48)	Acc@5  87.00 ( 92.17)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3640e+00 (1.2561e+00)	Acc@1  74.00 ( 73.90)	Acc@5  90.00 ( 92.36)
 * Acc@1 73.970 Acc@5 92.490
### epoch[61] execution time: 18.013262510299683
EPOCH 62
REMOVING: module.conv3_x.1.residual_function.1.weight
i:   0, name: module.conv3_x.1.residual_function.1.bias  changing lr from: 0.001083797005130504   to: 0.001001351009424500
i:   1, name: module.conv3_x.1.residual_function.3.weight  changing lr from: 0.001208802137079922   to: 0.001042881740792677
i:   2, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.001386730555097181   to: 0.001140538007734464
i:   3, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.001615072720019483   to: 0.001291785057843524
i:   4, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.001891369138700437   to: 0.001494130914412255
i:   5, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.002213213842553168   to: 0.001745130663774835
i:   6, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.002578257310166975   to: 0.002042390129663140
i:   7, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.002984208881856287   to: 0.002383568985040945
i:   8, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.003428838711050435   to: 0.002766383348956085
i:   9, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.003909979294571186   to: 0.003188607913088553
i:  10, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.004425526621080616   to: 0.003648077639886393
i:  11, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.004973440974324782   to: 0.004142689071485566
i:  12, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.005551747425252240   to: 0.004670401286010100
i:  13, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.006158536044656575   to: 0.005229236535354647
i:  14, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.006791961865678924   to: 0.005817280596163798
i:  15, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.007450244623311930   to: 0.006432682863447724
i:  16, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.008131668295969990   to: 0.007073656214109902
i:  17, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.008834580472229080   to: 0.007738476665611848
i:  18, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.009557391563993611   to: 0.008425482853060750
i:  19, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.010298573885611666   to: 0.009133075346176456
i:  20, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.011056660616831925   to: 0.009859715825871657
i:  21, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.011830244665972607   to: 0.010603926138563043
i:  22, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.012617977448247684   to: 0.011364287244812886
i:  23, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.013418567592869180   to: 0.012139438077483983
i:  24, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.014230779591306364   to: 0.012928074323263587
i:  25, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.015053432397933941   to: 0.013728947140177788
i:  26, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.015885397993235146   to: 0.014540861822567346
i:  27, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.016725599918735290   to: 0.015362676423926174
i:  28, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.017573011791928994   to: 0.016193300347013447
i:  29, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.018426655808617250   to: 0.017031692909729557
i:  30, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.019285601239292478   to: 0.017876861894397311
i:  31, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.020148962925491509   to: 0.018727862087304255
i:  32, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.021015899781375975   to: 0.019583793814637800
i:  33, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.021885613305193480   to: 0.020443801480277152
i:  34, name:               module.fc.weight  changing lr from: 0.022757346104717226   to: 0.021307072110294013
i:  35, name:                 module.fc.bias  changing lr from: 0.023630380440253514   to: 0.022172833908450026



# Switched to train mode...
Epoch: [62][  0/391]	Time  0.184 ( 0.184)	Data  0.145 ( 0.145)	Loss 5.3699e-03 (5.3699e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.039 ( 0.052)	Data  0.001 ( 0.015)	Loss 4.3492e-03 (5.6385e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.009)	Loss 4.7590e-03 (7.9521e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.007)	Loss 8.0517e-03 (7.3189e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.005)	Loss 6.9316e-03 (7.3133e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.005)	Loss 9.0462e-03 (8.0723e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.004)	Loss 6.6376e-03 (7.8364e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.004)	Loss 9.6272e-03 (7.8816e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.004)	Loss 5.6195e-03 (7.7386e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.0301e-02 (7.7392e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 4.3732e-03 (7.7090e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 7.2982e-03 (7.6890e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 5.5537e-03 (7.6305e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.2000e-02 (7.6419e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.2695e-02 (7.6870e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.0754e-03 (7.5773e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 4.8759e-03 (7.5749e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 8.4659e-03 (7.5875e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.7516e-03 (7.6832e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 3.3599e-03 (7.7434e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.0752e-02 (7.8011e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 5.1518e-03 (7.7791e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 4.4033e-03 (7.8866e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 7.4441e-03 (7.9230e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 8.7089e-03 (8.0410e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.0286e-02 (8.0818e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 5.7675e-03 (8.1203e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.4520e-03 (8.1722e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.3958e-03 (8.2116e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.3878e-03 (8.1712e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 7.9998e-03 (8.1921e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.1081e-02 (8.1876e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 5.0092e-03 (8.2589e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 4.3900e-03 (8.2965e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 5.3027e-03 (8.3196e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 6.2198e-03 (8.3407e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 2.1554e-03 (8.3222e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.4341e-02 (8.3447e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.0132e-02 (8.3942e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.3058e-02 (8.4014e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.09631109237670898
## e[62]       loss.backward (sum) time: 2.1529300212860107
## e[62]      optimizer.step (sum) time: 0.6181366443634033
## epoch[62] training(only) time: 15.499289512634277
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3124e+00 (1.3124e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.3585e+00 (1.3116e+00)	Acc@1  78.00 ( 74.00)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1923e+00 (1.2366e+00)	Acc@1  77.00 ( 75.24)	Acc@5  90.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.8091e+00 (1.2757e+00)	Acc@1  65.00 ( 74.42)	Acc@5  89.00 ( 91.65)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.4120e+00 (1.2740e+00)	Acc@1  71.00 ( 73.95)	Acc@5  92.00 ( 91.95)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5843e+00 (1.2913e+00)	Acc@1  71.00 ( 73.69)	Acc@5  89.00 ( 91.75)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3765e+00 (1.2673e+00)	Acc@1  69.00 ( 73.87)	Acc@5  93.00 ( 91.98)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.6163e+00 (1.2719e+00)	Acc@1  71.00 ( 73.69)	Acc@5  91.00 ( 92.06)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6377e+00 (1.2784e+00)	Acc@1  68.00 ( 73.73)	Acc@5  88.00 ( 92.10)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3992e+00 (1.2570e+00)	Acc@1  76.00 ( 74.15)	Acc@5  91.00 ( 92.25)
 * Acc@1 74.250 Acc@5 92.420
### epoch[62] execution time: 18.005072832107544
EPOCH 63
REMOVING: module.conv3_x.1.residual_function.1.bias
REMOVING: module.conv3_x.1.residual_function.3.weight
i:   0, name: module.conv3_x.1.residual_function.4.weight  changing lr from: 0.001140538007734464   to: 0.001016297605662108
i:   1, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.001291785057843524   to: 0.001087470228560516
i:   2, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.001494130914412255   to: 0.001212847299277319
i:   3, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.001745130663774835   to: 0.001389955783209533
i:   4, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.002042390129663140   to: 0.001616366785274558
i:   5, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.002383568985040945   to: 0.001889699408486694
i:   6, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.002766383348956085   to: 0.002207624044432915
i:   7, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.003188607913088553   to: 0.002567865142810859
i:   8, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.003648077639886393   to: 0.002968203504417332
i:   9, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.004142689071485566   to: 0.003406478139268808
i:  10, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.004670401286010100   to: 0.003880587728907463
i:  11, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.005229236535354647   to: 0.004388491729408034
i:  12, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.005817280596163798   to: 0.004928211149157990
i:  13, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.006432682863447724   to: 0.005497829033142642
i:  14, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.007073656214109902   to: 0.006095490683230841
i:  15, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.007738476665611848   to: 0.006719403641826099
i:  16, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.008425482853060750   to: 0.007367837464226801
i:  17, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.009133075346176456   to: 0.008039123303122814
i:  18, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.009859715825871657   to: 0.008731653326845983
i:  19, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.010603926138563043   to: 0.009443879991286507
i:  20, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.011364287244812886   to: 0.010174315183781146
i:  21, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.012139438077483983   to: 0.010921529255773905
i:  22, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.012928074323263587   to: 0.011684149959636625
i:  23, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.013728947140177788   to: 0.012460861303717607
i:  24, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.014540861822567346   to: 0.013250402338453392
i:  25, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.015362676423926174   to: 0.014051565885228816
i:  26, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.016193300347013447   to: 0.014863197218602804
i:  27, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.017031692909729557   to: 0.015684192711522001
i:  28, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.017876861894397311   to: 0.016513498452223919
i:  29, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.018727862087304255   to: 0.017350108840677204
i:  30, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.019583793814637800   to: 0.018193065171616736
i:  31, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.020443801480277152   to: 0.019041454210502035
i:  32, name:               module.fc.weight  changing lr from: 0.021307072110294013   to: 0.019894406768055147
i:  33, name:                 module.fc.bias  changing lr from: 0.022172833908450026   to: 0.020751096278415256



# Switched to train mode...
Epoch: [63][  0/391]	Time  0.192 ( 0.192)	Data  0.151 ( 0.151)	Loss 5.6779e-03 (5.6779e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.037 ( 0.052)	Data  0.001 ( 0.015)	Loss 8.8313e-03 (7.8291e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.009)	Loss 7.4983e-03 (7.4409e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.007)	Loss 5.8828e-03 (7.6297e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.006)	Loss 5.2492e-03 (7.5321e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 7.4032e-03 (7.8686e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.004)	Loss 1.0734e-02 (7.9920e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.004)	Loss 7.3810e-03 (7.8067e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.7036e-03 (7.7951e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.9292e-03 (7.7954e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.003)	Loss 4.9427e-03 (7.5884e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 8.3778e-03 (7.7039e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 1.2668e-02 (7.9801e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.003)	Loss 8.3560e-03 (7.9686e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 9.1651e-03 (7.9703e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2428e-03 (8.1082e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3737e-03 (8.0227e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0954e-02 (8.0159e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2658e-02 (7.9380e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.6249e-03 (7.9803e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.2808e-03 (7.9398e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.0880e-03 (7.9113e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.6740e-03 (7.9075e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.1861e-03 (7.9808e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.0584e-03 (7.9790e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.4448e-03 (7.9593e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2448e-03 (7.8929e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.9400e-03 (7.9968e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.6935e-03 (7.9175e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.4311e-02 (7.8994e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.002)	Loss 5.0451e-03 (7.8679e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 7.8900e-03 (7.8477e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 8.4769e-03 (7.8514e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 5.9029e-03 (7.8238e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 3.7378e-03 (7.7872e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.002)	Loss 4.2856e-03 (7.8525e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 6.3900e-03 (7.8663e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 5.1302e-03 (7.8303e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 5.2584e-03 (7.8223e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.002)	Loss 3.7997e-02 (7.9405e-03)	Acc@1  98.75 ( 99.94)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.09472107887268066
## e[63]       loss.backward (sum) time: 2.1281137466430664
## e[63]      optimizer.step (sum) time: 0.5997188091278076
## epoch[63] training(only) time: 15.271631717681885
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.3145e+00 (1.3145e+00)	Acc@1  72.00 ( 72.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2576e+00 (1.2799e+00)	Acc@1  76.00 ( 73.91)	Acc@5  92.00 ( 91.73)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1662e+00 (1.2154e+00)	Acc@1  76.00 ( 74.81)	Acc@5  91.00 ( 92.00)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6517e+00 (1.2520e+00)	Acc@1  69.00 ( 74.16)	Acc@5  90.00 ( 91.65)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3311e+00 (1.2486e+00)	Acc@1  73.00 ( 73.98)	Acc@5  93.00 ( 92.12)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5663e+00 (1.2693e+00)	Acc@1  70.00 ( 73.82)	Acc@5  90.00 ( 91.90)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3263e+00 (1.2451e+00)	Acc@1  70.00 ( 74.07)	Acc@5  93.00 ( 92.18)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5456e+00 (1.2473e+00)	Acc@1  68.00 ( 73.85)	Acc@5  90.00 ( 92.13)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.6083e+00 (1.2525e+00)	Acc@1  70.00 ( 74.06)	Acc@5  88.00 ( 92.21)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3927e+00 (1.2318e+00)	Acc@1  77.00 ( 74.43)	Acc@5  92.00 ( 92.35)
 * Acc@1 74.520 Acc@5 92.510
### epoch[63] execution time: 17.775983333587646
EPOCH 64
REMOVING: module.conv3_x.1.residual_function.4.weight
i:   0, name: module.conv3_x.1.residual_function.4.bias  changing lr from: 0.001087470228560516   to: 0.001002617244179248
i:   1, name: module.conv4_x.0.residual_function.0.weight  changing lr from: 0.001212847299277319   to: 0.001048177126930038
i:   2, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.001389955783209533   to: 0.001148503378051790
i:   3, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.001616366785274558   to: 0.001301143125102182
i:   4, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.001889699408486694   to: 0.001503684777530183
i:   5, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.002207624044432915   to: 0.001753762057950980
i:   6, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.002567865142810859   to: 0.002049057460712805
i:   7, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.002968203504417332   to: 0.002387305184487853
i:   8, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.003406478139268808   to: 0.002766293582934358
i:   9, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.003880587728907463   to: 0.003183867174847981
i:  10, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.004388491729408034   to: 0.003637928252663774
i:  11, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.004928211149157990   to: 0.004126438125692593
i:  12, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.005497829033142642   to: 0.004647418032088068
i:  13, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.006095490683230841   to: 0.005198949751246617
i:  14, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.006719403641826099   to: 0.005779175946147524
i:  15, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.007367837464226801   to: 0.006386300263046107
i:  16, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.008039123303122814   to: 0.007018587213941063
i:  17, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.008731653326845983   to: 0.007674361865346684
i:  18, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.009443879991286507   to: 0.008352009355113321
i:  19, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.010174315183781146   to: 0.009049974257349925
i:  20, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.010921529255773905   to: 0.009766759813913565
i:  21, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.011684149959636625   to: 0.010500927049434210
i:  22, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.012460861303717607   to: 0.011251093785441008
i:  23, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.013250402338453392   to: 0.012015933567842980
i:  24, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.014051565885228816   to: 0.012794174520787110
i:  25, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.014863197218602804   to: 0.013584598138772670
i:  26, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.015684192711522001   to: 0.014386038027830544
i:  27, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.016513498452223919   to: 0.015197378605583640
i:  28, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.017350108840677204   to: 0.016017553769081715
i:  29, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.018193065171616736   to: 0.016845545538447203
i:  30, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.019041454210502035   to: 0.017680382683575867
i:  31, name:               module.fc.weight  changing lr from: 0.019894406768055147   to: 0.018521139340403722
i:  32, name:                 module.fc.bias  changing lr from: 0.020751096278415256   to: 0.019366933622574575



# Switched to train mode...
Epoch: [64][  0/391]	Time  0.203 ( 0.203)	Data  0.158 ( 0.158)	Loss 6.0236e-03 (6.0236e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.038 ( 0.053)	Data  0.001 ( 0.016)	Loss 5.5312e-03 (7.6279e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.038 ( 0.046)	Data  0.001 ( 0.009)	Loss 6.6825e-03 (6.9250e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.038 ( 0.043)	Data  0.001 ( 0.007)	Loss 6.4246e-03 (7.3507e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.039 ( 0.042)	Data  0.001 ( 0.006)	Loss 4.0881e-03 (6.8454e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.005)	Loss 9.4604e-03 (7.0244e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.004)	Loss 9.3946e-03 (6.9411e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.039 ( 0.041)	Data  0.001 ( 0.004)	Loss 6.0609e-03 (6.8680e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.2182e-02 (7.1107e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.9268e-03 (7.1135e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.039 ( 0.040)	Data  0.001 ( 0.004)	Loss 9.9833e-03 (7.1468e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.040 ( 0.040)	Data  0.001 ( 0.003)	Loss 4.8026e-03 (7.1183e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 4.8031e-03 (6.9647e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.003)	Loss 6.6318e-03 (7.0339e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.7373e-03 (7.1847e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.4838e-02 (7.1899e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.3422e-03 (7.3034e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.7511e-03 (7.2560e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.5144e-03 (7.1787e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.8315e-03 (7.0920e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.5059e-02 (7.1785e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2034e-03 (7.1671e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.2929e-03 (7.3042e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.9128e-03 (7.1912e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.2895e-03 (7.2359e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.1342e-03 (7.3321e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.7462e-02 (7.3257e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.9197e-03 (7.3797e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.0178e-03 (7.3417e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.1336e-03 (7.4420e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.7159e-03 (7.4165e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.002)	Loss 6.5228e-03 (7.4152e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.0154e-02 (7.4000e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.3869e-02 (7.4318e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 7.1589e-03 (7.4949e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.1127e-02 (7.5277e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 6.7639e-03 (7.5858e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.002)	Loss 4.1370e-03 (7.5871e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.002)	Loss 1.0912e-02 (7.6137e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.002)	Loss 5.8007e-03 (7.6054e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.09479594230651855
## e[64]       loss.backward (sum) time: 2.1147289276123047
## e[64]      optimizer.step (sum) time: 0.5809471607208252
## epoch[64] training(only) time: 15.279924631118774
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.2761e+00 (1.2761e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.3112e+00 (1.2604e+00)	Acc@1  78.00 ( 75.18)	Acc@5  93.00 ( 91.82)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1835e+00 (1.2011e+00)	Acc@1  76.00 ( 75.57)	Acc@5  91.00 ( 92.19)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7292e+00 (1.2426e+00)	Acc@1  69.00 ( 74.71)	Acc@5  90.00 ( 91.84)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3414e+00 (1.2425e+00)	Acc@1  70.00 ( 74.27)	Acc@5  92.00 ( 92.12)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5123e+00 (1.2628e+00)	Acc@1  70.00 ( 74.10)	Acc@5  90.00 ( 91.92)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3487e+00 (1.2382e+00)	Acc@1  70.00 ( 74.26)	Acc@5  93.00 ( 92.20)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6164e+00 (1.2429e+00)	Acc@1  70.00 ( 74.06)	Acc@5  89.00 ( 92.11)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5989e+00 (1.2508e+00)	Acc@1  68.00 ( 74.11)	Acc@5  87.00 ( 92.17)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3970e+00 (1.2313e+00)	Acc@1  73.00 ( 74.49)	Acc@5  90.00 ( 92.29)
 * Acc@1 74.580 Acc@5 92.460
### epoch[64] execution time: 17.75774312019348
EPOCH 65
REMOVING: module.conv3_x.1.residual_function.4.bias
REMOVING: module.conv4_x.0.residual_function.0.weight
i:   0, name: module.conv4_x.0.residual_function.1.weight  changing lr from: 0.001148503378051790   to: 0.001021326936322857
i:   1, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.001301143125102182   to: 0.001097426401321105
i:   2, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.001503684777530183   to: 0.001226372848884771
i:   3, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.001753762057950980   to: 0.001405773134269794
i:   4, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.002049057460712805   to: 0.001633276801043781
i:   5, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.002387305184487853   to: 0.001906579697264820
i:   6, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.002766293582934358   to: 0.002223427061523968
i:   7, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.003183867174847981   to: 0.002581616122508789
i:   8, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.003637928252663774   to: 0.002978998253196313
i:   9, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.004126438125692593   to: 0.003413480718297046
i:  10, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.004647418032088068   to: 0.003883028051157705
i:  11, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.005198949751246617   to: 0.004385663093997788
i:  12, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.005779175946147524   to: 0.004919467733110105
i:  13, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.006386300263046107   to: 0.005482583358503290
i:  14, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.007018587213941063   to: 0.006073211075407075
i:  15, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.007674361865346684   to: 0.006689611693100838
i:  16, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.008352009355113321   to: 0.007330105514664475
i:  17, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.009049974257349925   to: 0.007993071949484542
i:  18, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.009766759813913565   to: 0.008676948968681319
i:  19, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.010500927049434210   to: 0.009380232422047028
i:  20, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.011251093785441008   to: 0.010101475233604402
i:  21, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.012015933567842980   to: 0.010839286491502665
i:  22, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.012794174520787110   to: 0.011592330446661480
i:  23, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.013584598138772670   to: 0.012359325433353080
i:  24, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.014386038027830544   to: 0.013139042723768472
i:  25, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.015197378605583640   to: 0.013930305327550442
i:  26, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.016017553769081715   to: 0.014731986746282306
i:  27, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.016845545538447203   to: 0.015543009691999059
i:  28, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.017680382683575867   to: 0.016362344777930110
i:  29, name:               module.fc.weight  changing lr from: 0.018521139340403722   to: 0.017189009188888538
i:  30, name:                 module.fc.bias  changing lr from: 0.019366933622574575   to: 0.018022065337986188



# Switched to train mode...
Epoch: [65][  0/391]	Time  0.189 ( 0.189)	Data  0.152 ( 0.152)	Loss 5.4058e-03 (5.4058e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.037 ( 0.051)	Data  0.001 ( 0.015)	Loss 4.6254e-03 (9.8829e-03)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.036 ( 0.045)	Data  0.001 ( 0.009)	Loss 7.7556e-03 (7.9000e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.037 ( 0.042)	Data  0.002 ( 0.007)	Loss 7.0081e-03 (7.7071e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.005)	Loss 9.5274e-03 (7.9026e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 7.6768e-03 (7.8892e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 6.2365e-03 (7.8640e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.3761e-03 (7.6830e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.5476e-03 (7.6380e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.8064e-03 (8.0825e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 8.2558e-03 (7.9449e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 5.9456e-03 (7.8408e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.6382e-03 (7.8753e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 2.1411e-03 (7.8807e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.3513e-03 (7.7755e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5497e-03 (7.5925e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1771e-03 (7.4758e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.2601e-02 (7.4930e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.3870e-03 (7.4339e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.1162e-03 (7.4378e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.8843e-03 (7.3782e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.0168e-03 (7.3603e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2390e-03 (7.3699e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.3424e-03 (7.4026e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.8722e-02 (7.3895e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2480e-02 (7.4700e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.2537e-02 (7.4651e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 7.1615e-03 (7.7848e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 7.2130e-03 (7.7372e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.002)	Loss 7.9135e-03 (7.7078e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.8058e-03 (7.6486e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.002)	Loss 6.2387e-03 (7.6767e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.039 ( 0.038)	Data  0.001 ( 0.002)	Loss 1.5257e-02 (7.8789e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 8.0343e-03 (7.8458e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.9937e-03 (7.7847e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 1.5556e-02 (7.8104e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.5721e-03 (7.7975e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 3.5701e-03 (7.7879e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 1.2824e-02 (7.8309e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 1.6378e-02 (7.8798e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.09108877182006836
## e[65]       loss.backward (sum) time: 2.0539534091949463
## e[65]      optimizer.step (sum) time: 0.5548977851867676
## epoch[65] training(only) time: 14.91075611114502
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.2987e+00 (1.2987e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.3102e+00 (1.2674e+00)	Acc@1  75.00 ( 74.45)	Acc@5  93.00 ( 91.64)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1618e+00 (1.2101e+00)	Acc@1  77.00 ( 74.86)	Acc@5  90.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7164e+00 (1.2524e+00)	Acc@1  66.00 ( 74.16)	Acc@5  90.00 ( 91.65)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3681e+00 (1.2536e+00)	Acc@1  72.00 ( 73.80)	Acc@5  92.00 ( 91.88)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5578e+00 (1.2731e+00)	Acc@1  71.00 ( 73.67)	Acc@5  89.00 ( 91.63)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3701e+00 (1.2485e+00)	Acc@1  67.00 ( 73.87)	Acc@5  93.00 ( 91.87)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.6298e+00 (1.2523e+00)	Acc@1  70.00 ( 73.82)	Acc@5  90.00 ( 91.86)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5986e+00 (1.2590e+00)	Acc@1  70.00 ( 73.94)	Acc@5  87.00 ( 91.90)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3923e+00 (1.2387e+00)	Acc@1  74.00 ( 74.32)	Acc@5  90.00 ( 92.04)
 * Acc@1 74.450 Acc@5 92.170
### epoch[65] execution time: 17.384453535079956
EPOCH 66
REMOVING: module.conv4_x.0.residual_function.1.weight
i:   0, name: module.conv4_x.0.residual_function.1.bias  changing lr from: 0.001097426401321105   to: 0.001005673683352808
i:   1, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.001226372848884771   to: 0.001058372648816174
i:   2, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.001405773134269794   to: 0.001164405404712881
i:   3, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.001633276801043781   to: 0.001321398257134536
i:   4, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.001906579697264820   to: 0.001527017640656685
i:   5, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.002223427061523968   to: 0.001778973881764283
i:   6, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.002581616122508789   to: 0.002075024428816069
i:   7, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.002978998253196313   to: 0.002412976591773035
i:   8, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.003413480718297046   to: 0.002790689832446352
i:   9, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.003883028051157705   to: 0.003206077643604498
i:  10, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.004385663093997788   to: 0.003657109052929219
i:  11, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.004919467733110105   to: 0.004141809785533140
i:  12, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.005482583358503290   to: 0.004658263116557753
i:  13, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.006073211075407075   to: 0.005204610443261664
i:  14, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.006689611693100838   to: 0.005779051603990359
i:  15, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.007330105514664475   to: 0.006379844969493003
i:  16, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.007993071949484542   to: 0.007005307330217469
i:  17, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.008676948968681319   to: 0.007653813601476150
i:  18, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.009380232422047028   to: 0.008323796366726328
i:  19, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.010101475233604402   to: 0.009013745277653706
i:  20, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.010839286491502665   to: 0.009722206328280501
i:  21, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.011592330446661480   to: 0.010447781018938684
i:  22, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.012359325433353080   to: 0.011189125424654962
i:  23, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.013139042723768472   to: 0.011944949181277538
i:  24, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.013930305327550442   to: 0.012714014401540220
i:  25, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.014731986746282306   to: 0.013495134532197306
i:  26, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.015543009691999059   to: 0.014287173162373334
i:  27, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.016362344777930110   to: 0.015089042792349506
i:  28, name:               module.fc.weight  changing lr from: 0.017189009188888538   to: 0.015899703571152800
i:  29, name:                 module.fc.bias  changing lr from: 0.018022065337986188   to: 0.016718162010518355



# Switched to train mode...
Epoch: [66][  0/391]	Time  0.190 ( 0.190)	Data  0.151 ( 0.151)	Loss 6.8167e-03 (6.8167e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.037 ( 0.051)	Data  0.001 ( 0.015)	Loss 6.1885e-03 (5.9249e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.036 ( 0.045)	Data  0.001 ( 0.009)	Loss 7.2869e-03 (6.2281e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 9.6587e-03 (6.5464e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 5.2905e-03 (7.3786e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 3.5549e-03 (7.1455e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 7.5820e-03 (7.2948e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.004)	Loss 1.4132e-02 (7.3818e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.0454e-02 (7.3463e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.8340e-03 (7.2168e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.1343e-03 (7.2378e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 9.1193e-03 (7.3889e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 7.8536e-03 (7.5475e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 3.8239e-03 (7.6429e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0262e-02 (7.5893e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.4608e-03 (7.5396e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.6714e-03 (7.3934e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.0607e-03 (7.3383e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.9508e-03 (7.2665e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.4828e-03 (7.3142e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0685e-03 (7.4583e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.3594e-03 (7.4509e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.5893e-03 (7.4091e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0943e-03 (7.3692e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.042 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.1070e-03 (7.4638e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.5995e-03 (7.5197e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1930e-02 (7.4709e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.8555e-03 (7.4631e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.2390e-03 (7.6213e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.7191e-03 (7.5926e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.6278e-03 (7.5665e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.6911e-03 (7.5552e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.8252e-03 (7.5463e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 6.0485e-03 (7.5458e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 9.4570e-03 (7.5091e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 2.2032e-02 (7.5686e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.002)	Loss 1.4798e-02 (7.5579e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.041 ( 0.038)	Data  0.001 ( 0.002)	Loss 2.0068e-02 (7.5840e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.8187e-03 (7.5329e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.002)	Loss 2.3179e-02 (7.5605e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.08934402465820312
## e[66]       loss.backward (sum) time: 2.0878045558929443
## e[66]      optimizer.step (sum) time: 0.5381546020507812
## epoch[66] training(only) time: 14.948853015899658
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2929e+00 (1.2929e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.2898e+00 (1.2667e+00)	Acc@1  75.00 ( 74.36)	Acc@5  92.00 ( 91.64)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1987e+00 (1.2033e+00)	Acc@1  76.00 ( 75.14)	Acc@5  91.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6856e+00 (1.2425e+00)	Acc@1  65.00 ( 74.42)	Acc@5  91.00 ( 91.71)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3165e+00 (1.2405e+00)	Acc@1  71.00 ( 74.05)	Acc@5  92.00 ( 92.02)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.5577e+00 (1.2605e+00)	Acc@1  70.00 ( 73.80)	Acc@5  90.00 ( 91.78)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3099e+00 (1.2369e+00)	Acc@1  68.00 ( 73.98)	Acc@5  94.00 ( 92.10)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.5990e+00 (1.2418e+00)	Acc@1  71.00 ( 73.80)	Acc@5  90.00 ( 92.04)
Test: [ 80/100]	Time  0.024 ( 0.024)	Loss 1.5779e+00 (1.2483e+00)	Acc@1  70.00 ( 73.88)	Acc@5  88.00 ( 92.07)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.4195e+00 (1.2292e+00)	Acc@1  74.00 ( 74.30)	Acc@5  91.00 ( 92.22)
 * Acc@1 74.470 Acc@5 92.330
### epoch[66] execution time: 17.4685161113739
EPOCH 67
REMOVING: module.conv4_x.0.residual_function.1.bias
i:   0, name: module.conv4_x.0.residual_function.3.weight  changing lr from: 0.001058372648816174   to: 0.001000053135612599
i:   1, name: module.conv4_x.0.residual_function.4.weight  changing lr from: 0.001164405404712881   to: 0.001030177778781926
i:   2, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.001321398257134536   to: 0.001114078239588655
i:   3, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.001527017640656685   to: 0.001249401152056650
i:   4, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.001778973881764283   to: 0.001433830788137180
i:   5, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.002075024428816069   to: 0.001665092958165398
i:   6, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.002412976591773035   to: 0.001940958375325252
i:   7, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.002790689832446352   to: 0.002259245526877206
i:   8, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.003206077643604498   to: 0.002617823092512560
i:   9, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.003657109052929219   to: 0.003014611947853784
i:  10, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.004141809785533140   to: 0.003447586788833611
i:  11, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.004658263116557753   to: 0.003914777410466619
i:  12, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.005204610443261664   to: 0.004414269671383185
i:  13, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.005779051603990359   to: 0.004944206173431808
i:  14, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.006379844969493003   to: 0.005502786683677734
i:  15, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.007005307330217469   to: 0.006088268324233237
i:  16, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.007653813601476150   to: 0.006698965553553204
i:  17, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.008323796366726328   to: 0.007333249961114890
i:  18, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.009013745277653706   to: 0.007989549895776681
i:  19, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.009722206328280501   to: 0.008666349946573913
i:  20, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.010447781018938684   to: 0.009362190293258032
i:  21, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.011189125424654962   to: 0.010075665942521021
i:  22, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.011944949181277538   to: 0.010805425864560021
i:  23, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.012714014401540220   to: 0.011550172043433969
i:  24, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.013495134532197306   to: 0.012308658453533454
i:  25, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.014287173162373334   to: 0.013079689973430082
i:  26, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.015089042792349506   to: 0.013862121247384868
i:  27, name:               module.fc.weight  changing lr from: 0.015899703571152800   to: 0.014654855503877066
i:  28, name:                 module.fc.bias  changing lr from: 0.016718162010518355   to: 0.015456843339659458



# Switched to train mode...
Epoch: [67][  0/391]	Time  0.194 ( 0.194)	Data  0.156 ( 0.156)	Loss 3.9514e-03 (3.9514e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.037 ( 0.051)	Data  0.001 ( 0.016)	Loss 3.2909e-03 (6.9517e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.038 ( 0.045)	Data  0.001 ( 0.009)	Loss 7.4855e-03 (7.2611e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.007)	Loss 1.3630e-02 (7.2194e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.006)	Loss 6.4384e-03 (7.3708e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.0931e-02 (7.1670e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.004)	Loss 4.6222e-03 (6.9346e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 2.0288e-02 (7.2236e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.7501e-03 (7.4761e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.6694e-03 (7.4128e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.0127e-02 (7.7056e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 1.2092e-02 (7.7175e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.038 ( 0.038)	Data  0.003 ( 0.003)	Loss 1.5163e-02 (7.8802e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1562e-02 (7.8597e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.1106e-03 (7.8189e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.5056e-03 (7.7233e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.1059e-02 (7.7785e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8699e-03 (7.6932e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.2767e-03 (7.7833e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.8252e-03 (7.6825e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.3580e-02 (7.6715e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.8303e-03 (7.6299e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.1179e-03 (7.8028e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.8394e-03 (7.7507e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.5574e-02 (7.7369e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.4232e-03 (7.6547e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5249e-03 (7.6823e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.8329e-03 (7.7202e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 7.2205e-03 (7.7708e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.002)	Loss 1.1291e-02 (7.8102e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.5601e-03 (7.8172e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 7.3653e-03 (7.7797e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 2.6292e-02 (7.8355e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.044 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.6814e-03 (7.8473e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.002)	Loss 8.7466e-03 (7.8683e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.040 ( 0.038)	Data  0.001 ( 0.002)	Loss 9.7102e-03 (7.8637e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.002)	Loss 3.6694e-03 (7.8442e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.002)	Loss 6.7424e-03 (7.8190e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.3644e-03 (7.7971e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.002)	Loss 9.5373e-03 (7.7754e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.08788919448852539
## e[67]       loss.backward (sum) time: 2.03118896484375
## e[67]      optimizer.step (sum) time: 0.5156698226928711
## epoch[67] training(only) time: 14.826485395431519
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.3085e+00 (1.3085e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.2547e+00 (1.2474e+00)	Acc@1  80.00 ( 74.82)	Acc@5  92.00 ( 91.73)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1632e+00 (1.1902e+00)	Acc@1  77.00 ( 75.43)	Acc@5  92.00 ( 92.10)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6805e+00 (1.2279e+00)	Acc@1  68.00 ( 74.77)	Acc@5  90.00 ( 91.87)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3356e+00 (1.2308e+00)	Acc@1  71.00 ( 74.37)	Acc@5  92.00 ( 91.93)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5250e+00 (1.2497e+00)	Acc@1  71.00 ( 74.22)	Acc@5  89.00 ( 91.65)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3251e+00 (1.2267e+00)	Acc@1  68.00 ( 74.38)	Acc@5  93.00 ( 91.97)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5892e+00 (1.2325e+00)	Acc@1  71.00 ( 74.14)	Acc@5  90.00 ( 91.97)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5521e+00 (1.2382e+00)	Acc@1  71.00 ( 74.17)	Acc@5  87.00 ( 92.00)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3868e+00 (1.2194e+00)	Acc@1  74.00 ( 74.58)	Acc@5  90.00 ( 92.10)
 * Acc@1 74.690 Acc@5 92.270
### epoch[67] execution time: 17.30977201461792
EPOCH 68
REMOVING: module.conv4_x.0.residual_function.3.weight
REMOVING: module.conv4_x.0.residual_function.4.weight
i:   0, name: module.conv4_x.0.residual_function.4.bias  changing lr from: 0.001114078239588655   to: 0.001011753094735539
i:   1, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.001249401152056650   to: 0.001074302296645912
i:   2, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.001433830788137180   to: 0.001188694153576335
i:   3, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.001665092958165398   to: 0.001352631605031373
i:   4, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.001940958375325252   to: 0.001563856838196691
i:   5, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.002259245526877206   to: 0.001820154777622983
i:   6, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.002617823092512560   to: 0.002119356079389783
i:   7, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.003014611947853784   to: 0.002459339669686916
i:   8, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.003447586788833611   to: 0.002838034865477239
i:   9, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.003914777410466619   to: 0.003253423112682401
i:  10, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.004414269671383185   to: 0.003703539375171338
i:  11, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.004944206173431808   to: 0.004186473205738472
i:  12, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.005502786683677734   to: 0.004700369528241515
i:  13, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.006088268324233237   to: 0.005243429158129769
i:  14, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.006698965553553204   to: 0.005813909086738750
i:  15, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.007333249961114890   to: 0.006410122552955233
i:  16, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.007989549895776681   to: 0.007030438924170556
i:  17, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.008666349946573913   to: 0.007673283406839964
i:  18, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.009362190293258032   to: 0.008337136605447815
i:  19, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.010075665942521021   to: 0.009020533947247809
i:  20, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.010805425864560021   to: 0.009722064988793300
i:  21, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.011550172043433969   to: 0.010440372619002731
i:  22, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.012308658453533454   to: 0.011174152172309427
i:  23, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.013079689973430082   to: 0.011922150464324079
i:  24, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.013862121247384868   to: 0.012683164761388987
i:  25, name:               module.fc.weight  changing lr from: 0.014654855503877066   to: 0.013456041694422698
i:  26, name:                 module.fc.bias  changing lr from: 0.015456843339659458   to: 0.014239676126538675



# Switched to train mode...
Epoch: [68][  0/391]	Time  0.182 ( 0.182)	Data  0.143 ( 0.143)	Loss 5.4262e-03 (5.4262e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.036 ( 0.051)	Data  0.001 ( 0.014)	Loss 3.4782e-03 (6.4351e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 6.7471e-03 (6.2628e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 6.0680e-03 (6.7960e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.005)	Loss 8.6816e-03 (6.9238e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.0821e-03 (7.0060e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.004)	Loss 5.2018e-03 (6.9840e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.2778e-03 (6.7719e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 3.7866e-03 (6.9366e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.003)	Loss 6.6303e-03 (7.0188e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7036e-03 (6.9626e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 2.7065e-03 (6.8455e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.7748e-03 (6.7987e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1241e-02 (6.9381e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.1425e-02 (6.9120e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8715e-03 (6.8281e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.5600e-03 (7.0175e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.3623e-03 (6.9415e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.7776e-03 (6.9017e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.7053e-02 (7.0759e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.6197e-03 (7.0435e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.6826e-02 (7.1653e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.1112e-03 (7.1885e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.5112e-03 (7.1101e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.002)	Loss 5.1156e-03 (7.0233e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.002)	Loss 3.7415e-03 (6.9665e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.039 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.3539e-02 (6.9664e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 6.8110e-03 (7.0259e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.039 ( 0.037)	Data  0.001 ( 0.002)	Loss 4.6562e-03 (7.0246e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 7.7496e-03 (7.1205e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.0506e-03 (7.1650e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 8.0517e-03 (7.1723e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 9.2977e-03 (7.2548e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.8099e-03 (7.2417e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 3.7922e-03 (7.2151e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 6.5245e-03 (7.2635e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.5648e-03 (7.2120e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 6.3141e-03 (7.1745e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 3.7885e-03 (7.1448e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 8.7881e-03 (7.2473e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.08535552024841309
## e[68]       loss.backward (sum) time: 2.0471866130828857
## e[68]      optimizer.step (sum) time: 0.4820427894592285
## epoch[68] training(only) time: 14.687964677810669
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.3107e+00 (1.3107e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2884e+00 (1.2580e+00)	Acc@1  77.00 ( 74.73)	Acc@5  93.00 ( 91.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1476e+00 (1.1958e+00)	Acc@1  77.00 ( 75.43)	Acc@5  92.00 ( 92.14)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7047e+00 (1.2329e+00)	Acc@1  66.00 ( 74.77)	Acc@5  91.00 ( 91.90)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3678e+00 (1.2335e+00)	Acc@1  70.00 ( 74.32)	Acc@5  92.00 ( 92.17)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5161e+00 (1.2534e+00)	Acc@1  71.00 ( 74.18)	Acc@5  89.00 ( 91.84)
Test: [ 60/100]	Time  0.024 ( 0.025)	Loss 1.3573e+00 (1.2318e+00)	Acc@1  67.00 ( 74.31)	Acc@5  94.00 ( 92.16)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.5727e+00 (1.2374e+00)	Acc@1  70.00 ( 74.06)	Acc@5  90.00 ( 92.15)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5656e+00 (1.2428e+00)	Acc@1  70.00 ( 74.11)	Acc@5  88.00 ( 92.19)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3952e+00 (1.2229e+00)	Acc@1  75.00 ( 74.49)	Acc@5  90.00 ( 92.30)
 * Acc@1 74.570 Acc@5 92.440
### epoch[68] execution time: 17.174655199050903
EPOCH 69
REMOVING: module.conv4_x.0.residual_function.4.bias
i:   0, name: module.conv4_x.0.shortcut.0.weight  changing lr from: 0.001074302296645912   to: 0.001002081888578415
i:   1, name: module.conv4_x.0.shortcut.1.weight  changing lr from: 0.001188694153576335   to: 0.001044058574440498
i:   2, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.001352631605031373   to: 0.001138257693873201
i:   3, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.001563856838196691   to: 0.001282401575269070
i:   4, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.001820154777622983   to: 0.001474249570004309
i:   5, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.002119356079389783   to: 0.001711601658227706
i:   6, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.002459339669686916   to: 0.001992301557527786
i:   7, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.002838034865477239   to: 0.002314239373958448
i:   8, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.003253423112682401   to: 0.002675353832700394
i:   9, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.003703539375171338   to: 0.003073634123476992
i:  10, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.004186473205738472   to: 0.003507121393738721
i:  11, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.004700369528241515   to: 0.003973909920589996
i:  12, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.005243429158129769   to: 0.004472147990460350
i:  13, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.005813909086738750   to: 0.005000038513625309
i:  14, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.006410122552955233   to: 0.005555839398863060
i:  15, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.007030438924170556   to: 0.006137863711794050
i:  16, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.007673283406839964   to: 0.006744479638793432
i:  17, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.008337136605447815   to: 0.007374110276789480
i:  18, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.009020533947247809   to: 0.008025233267767667
i:  19, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.009722064988793300   to: 0.008696380295384571
i:  20, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.010440372619002731   to: 0.009386136459761080
i:  21, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.011174152172309427   to: 0.010093139545266132
i:  22, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.011922150464324079   to: 0.010816079194917901
i:  23, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.012683164761388987   to: 0.011553696003918416
i:  24, name:               module.fc.weight  changing lr from: 0.013456041694422698   to: 0.012304780543796190
i:  25, name:                 module.fc.bias  changing lr from: 0.014239676126538675   to: 0.013068172327656787



# Switched to train mode...
Epoch: [69][  0/391]	Time  0.185 ( 0.185)	Data  0.140 ( 0.140)	Loss 4.3185e-03 (4.3185e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.037 ( 0.050)	Data  0.001 ( 0.014)	Loss 5.5027e-03 (5.3327e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.008)	Loss 7.5855e-03 (5.9545e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.006)	Loss 5.9250e-03 (6.0212e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.9012e-03 (6.0372e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 4.5944e-03 (6.1605e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 1.8265e-02 (6.4949e-03)	Acc@1  99.22 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.7313e-03 (6.3039e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 6.6590e-03 (6.4873e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.003)	Loss 4.4054e-03 (6.3916e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.8759e-03 (6.2806e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8187e-03 (6.2488e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.9840e-03 (6.3078e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.6238e-03 (6.5091e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.3630e-03 (6.5253e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.1789e-03 (6.5063e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.0802e-03 (6.4086e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.5832e-03 (6.4344e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.6966e-03 (6.4097e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3273e-03 (6.4025e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.037 ( 0.038)	Data  0.002 ( 0.003)	Loss 4.5536e-03 (6.4475e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 7.3053e-03 (6.4747e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.5231e-03 (6.5714e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.1084e-03 (6.5551e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.2760e-03 (6.5891e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 4.6881e-03 (6.6271e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.039 ( 0.037)	Data  0.001 ( 0.002)	Loss 7.1036e-03 (6.6252e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 8.0817e-03 (6.6732e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.3772e-02 (6.7170e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 7.6050e-03 (6.7243e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 8.3128e-03 (6.7677e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.4024e-03 (6.7264e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 3.4953e-03 (6.7615e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 3.5143e-03 (6.7665e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 6.8182e-03 (6.9071e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 6.2448e-03 (6.9497e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 4.8524e-03 (6.9794e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.0618e-02 (7.0349e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 6.3066e-03 (7.0787e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 9.7350e-03 (7.0871e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.07981729507446289
## e[69]       loss.backward (sum) time: 1.998934030532837
## e[69]      optimizer.step (sum) time: 0.4698464870452881
## epoch[69] training(only) time: 14.66329836845398
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.2705e+00 (1.2705e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2893e+00 (1.2620e+00)	Acc@1  77.00 ( 74.36)	Acc@5  91.00 ( 90.82)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1555e+00 (1.1992e+00)	Acc@1  77.00 ( 75.43)	Acc@5  91.00 ( 91.81)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7082e+00 (1.2309e+00)	Acc@1  66.00 ( 74.81)	Acc@5  90.00 ( 91.74)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3407e+00 (1.2315e+00)	Acc@1  71.00 ( 74.34)	Acc@5  92.00 ( 92.00)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5532e+00 (1.2516e+00)	Acc@1  71.00 ( 74.18)	Acc@5  89.00 ( 91.67)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3078e+00 (1.2287e+00)	Acc@1  69.00 ( 74.34)	Acc@5  94.00 ( 92.00)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.5244e+00 (1.2329e+00)	Acc@1  69.00 ( 74.10)	Acc@5  89.00 ( 92.00)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5539e+00 (1.2393e+00)	Acc@1  70.00 ( 74.07)	Acc@5  88.00 ( 92.04)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.4074e+00 (1.2199e+00)	Acc@1  74.00 ( 74.45)	Acc@5  91.00 ( 92.16)
 * Acc@1 74.570 Acc@5 92.340
### epoch[69] execution time: 17.171573877334595
EPOCH 70
REMOVING: module.conv4_x.0.shortcut.0.weight
REMOVING: module.conv4_x.0.shortcut.1.weight
i:   0, name: module.conv4_x.0.shortcut.1.bias  changing lr from: 0.001138257693873201   to: 0.001022394759497485
i:   1, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.001282401575269070   to: 0.001097137130382807
i:   2, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.001474249570004309   to: 0.001222185322190582
i:   3, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.001711601658227706   to: 0.001395316538788304
i:   4, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.001992301557527786   to: 0.001614346563946056
i:   5, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.002314239373958448   to: 0.001877132976228212
i:   6, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.002675353832700394   to: 0.002181577904801551
i:   7, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.003073634123476992   to: 0.002525630363022329
i:   8, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.003507121393738721   to: 0.002907288194569268
i:   9, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.003973909920589996   to: 0.003324599664843397
i:  10, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.004472147990460350   to: 0.003775664728365723
i:  11, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.005000038513625309   to: 0.004258636000979959
i:  12, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.005555839398863060   to: 0.004771719463811440
i:  13, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.006137863711794050   to: 0.005313174924152369
i:  14, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.006744479638793432   to: 0.005881316256737506
i:  15, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.007374110276789480   to: 0.006474511447245684
i:  16, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.008025233267767667   to: 0.007091182458313461
i:  17, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.008696380295384571   to: 0.007729804936874118
i:  18, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.009386136459761080   to: 0.008388907780241334
i:  19, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.010093139545266132   to: 0.009067072577038492
i:  20, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.010816079194917901   to: 0.009762932937830669
i:  21, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.011553696003918416   to: 0.010475173729144804
i:  22, name:               module.fc.weight  changing lr from: 0.012304780543796190   to: 0.011202530223463424
i:  23, name:                 module.fc.bias  changing lr from: 0.013068172327656787   to: 0.011943787176744453



# Switched to train mode...
Epoch: [70][  0/391]	Time  0.190 ( 0.190)	Data  0.151 ( 0.151)	Loss 1.4750e-02 (1.4750e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.036 ( 0.050)	Data  0.001 ( 0.015)	Loss 8.0559e-03 (7.8121e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.036 ( 0.044)	Data  0.001 ( 0.009)	Loss 1.9710e-02 (7.8840e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 5.1111e-03 (7.4505e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 6.5984e-03 (7.6484e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.3154e-02 (7.3742e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.0873e-03 (7.0950e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.8959e-03 (7.2399e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.004)	Loss 4.7013e-03 (7.4540e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 7.6241e-03 (7.3128e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0529e-02 (7.3623e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.0457e-03 (7.2049e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.038 ( 0.038)	Data  0.004 ( 0.003)	Loss 3.6724e-03 (7.1227e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 1.0395e-02 (7.1561e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 3.9713e-03 (7.1146e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.8839e-03 (7.1070e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.8213e-03 (7.1081e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.3486e-03 (7.0678e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.003)	Loss 9.6801e-03 (7.0063e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.0346e-03 (6.9766e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.037 ( 0.037)	Data  0.002 ( 0.003)	Loss 6.3753e-03 (7.0220e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.6571e-03 (7.0168e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 8.0330e-03 (7.1021e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.1153e-03 (7.0706e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.5337e-03 (7.1748e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 7.0065e-03 (7.1279e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 8.5392e-03 (7.1722e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.6031e-03 (7.1945e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 3.2663e-03 (7.2545e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 7.4950e-03 (7.2176e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 9.4582e-03 (7.1571e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.2859e-03 (7.1547e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.8883e-03 (7.1720e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.5420e-03 (7.1349e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.1429e-02 (7.1363e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.0885e-02 (7.2105e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 6.7597e-03 (7.2545e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 6.0693e-03 (7.2546e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 2.6529e-02 (7.2855e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.0232e-02 (7.2970e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.07368326187133789
## e[70]       loss.backward (sum) time: 1.9776313304901123
## e[70]      optimizer.step (sum) time: 0.4329872131347656
## epoch[70] training(only) time: 14.60275411605835
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2818e+00 (1.2818e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2707e+00 (1.2566e+00)	Acc@1  77.00 ( 74.64)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1837e+00 (1.1914e+00)	Acc@1  77.00 ( 75.62)	Acc@5  91.00 ( 92.00)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7506e+00 (1.2311e+00)	Acc@1  66.00 ( 74.81)	Acc@5  90.00 ( 91.77)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3990e+00 (1.2340e+00)	Acc@1  69.00 ( 74.32)	Acc@5  92.00 ( 92.10)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5467e+00 (1.2555e+00)	Acc@1  72.00 ( 74.25)	Acc@5  89.00 ( 91.71)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3430e+00 (1.2345e+00)	Acc@1  69.00 ( 74.38)	Acc@5  92.00 ( 92.02)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.6237e+00 (1.2392e+00)	Acc@1  70.00 ( 74.18)	Acc@5  89.00 ( 92.04)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.6269e+00 (1.2475e+00)	Acc@1  70.00 ( 74.14)	Acc@5  85.00 ( 92.06)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3600e+00 (1.2275e+00)	Acc@1  73.00 ( 74.51)	Acc@5  91.00 ( 92.22)
 * Acc@1 74.560 Acc@5 92.370
### epoch[70] execution time: 17.084176063537598
EPOCH 71
REMOVING: module.conv4_x.0.shortcut.1.bias
i:   0, name: module.conv4_x.1.residual_function.0.weight  changing lr from: 0.001097137130382807   to: 0.001008421942784209
i:   1, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.001222185322190582   to: 0.001064439643319303
i:   2, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.001395316538788304   to: 0.001171087682303717
i:   3, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.001614346563946056   to: 0.001326161711809148
i:   4, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.001877132976228212   to: 0.001527493977010973
i:   5, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.002181577904801551   to: 0.001772956629201027
i:   6, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.002525630363022329   to: 0.002060464578858211
i:   7, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.002907288194569268   to: 0.002387977925199890
i:   8, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.003324599664843397   to: 0.002753503996604583
i:   9, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.003775664728365723   to: 0.003155099034306618
i:  10, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.004258636000979959   to: 0.003590869549826641
i:  11, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.004771719463811440   to: 0.004058973384723318
i:  12, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.005313174924152369   to: 0.004557620499438902
i:  13, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.005881316256737506   to: 0.005085073516267590
i:  14, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.006474511447245684   to: 0.005639648039802657
i:  15, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.007091182458313461   to: 0.006219712776621433
i:  16, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.007729804936874118   to: 0.006823689474442501
i:  17, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.008388907780241334   to: 0.007450052699541503
i:  18, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.009067072577038492   to: 0.008097329469838339
i:  19, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.009762932937830669   to: 0.008764098759767525
i:  20, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.010475173729144804   to: 0.009448990891815296
i:  21, name:               module.fc.weight  changing lr from: 0.011202530223463424   to: 0.010150686828449839
i:  22, name:                 module.fc.bias  changing lr from: 0.011943787176744453   to: 0.010867917377081166



# Switched to train mode...
Epoch: [71][  0/391]	Time  0.185 ( 0.185)	Data  0.149 ( 0.149)	Loss 5.6651e-03 (5.6651e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.035 ( 0.050)	Data  0.001 ( 0.015)	Loss 3.5428e-03 (6.1059e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.009)	Loss 5.4234e-03 (5.5854e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.007)	Loss 7.8580e-03 (6.4791e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 8.4335e-03 (6.4000e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.5555e-03 (6.1857e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.004)	Loss 5.4142e-03 (6.1932e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.0492e-02 (6.1221e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.0757e-03 (6.1551e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 5.1245e-03 (6.3135e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.036 ( 0.038)	Data  0.002 ( 0.003)	Loss 1.0888e-02 (6.5717e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.0556e-03 (6.6990e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.3469e-03 (6.6705e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.1028e-02 (6.7196e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.4426e-03 (6.8486e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.8331e-02 (7.0296e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 8.8098e-03 (7.0203e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.0693e-03 (7.0734e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.7822e-03 (7.0069e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.3380e-03 (7.0206e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.2810e-03 (6.9975e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.2073e-03 (7.0447e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.6041e-03 (7.0070e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.1847e-02 (6.9895e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.8086e-03 (7.1521e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.4662e-02 (7.1952e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 6.6893e-03 (7.2183e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 2.7497e-02 (7.2704e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 9.6795e-03 (7.2410e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.4528e-03 (7.1951e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 2.1909e-02 (7.2226e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.9903e-03 (7.2010e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 9.5254e-03 (7.2908e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 9.2359e-03 (7.2658e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 9.0553e-03 (7.2576e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 7.6720e-03 (7.2256e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.6745e-02 (7.2101e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 7.0322e-03 (7.1666e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.7329e-03 (7.1596e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 4.3446e-02 (7.1764e-03)	Acc@1  98.75 ( 99.95)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.07138180732727051
## e[71]       loss.backward (sum) time: 1.9294712543487549
## e[71]      optimizer.step (sum) time: 0.42122912406921387
## epoch[71] training(only) time: 14.492010116577148
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2700e+00 (1.2700e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2629e+00 (1.2374e+00)	Acc@1  76.00 ( 74.82)	Acc@5  92.00 ( 91.00)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.1701e+00 (1.1731e+00)	Acc@1  76.00 ( 75.48)	Acc@5  91.00 ( 91.71)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6951e+00 (1.2087e+00)	Acc@1  67.00 ( 74.74)	Acc@5  92.00 ( 91.58)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3679e+00 (1.2116e+00)	Acc@1  69.00 ( 74.34)	Acc@5  92.00 ( 91.90)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5220e+00 (1.2302e+00)	Acc@1  71.00 ( 74.22)	Acc@5  89.00 ( 91.61)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3186e+00 (1.2099e+00)	Acc@1  68.00 ( 74.33)	Acc@5  93.00 ( 91.93)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5960e+00 (1.2145e+00)	Acc@1  71.00 ( 74.11)	Acc@5  89.00 ( 91.90)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5549e+00 (1.2215e+00)	Acc@1  71.00 ( 74.17)	Acc@5  86.00 ( 91.96)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3545e+00 (1.2030e+00)	Acc@1  74.00 ( 74.54)	Acc@5  91.00 ( 92.11)
 * Acc@1 74.650 Acc@5 92.270
### epoch[71] execution time: 16.960729837417603
EPOCH 72
REMOVING: module.conv4_x.1.residual_function.0.weight
i:   0, name: module.conv4_x.1.residual_function.1.weight  changing lr from: 0.001064439643319303   to: 0.001001311428526485
i:   1, name: module.conv4_x.1.residual_function.1.bias  changing lr from: 0.001171087682303717   to: 0.001039331212194843
i:   2, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.001326161711809148   to: 0.001128270845455225
i:   3, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.001527493977010973   to: 0.001265944937967594
i:   4, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.001772956629201027   to: 0.001450202769670194
i:   5, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.002060464578858211   to: 0.001678931694002482
i:   6, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.002387977925199890   to: 0.001950060080878312
i:   7, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.002753503996604583   to: 0.002261559835370935
i:   8, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.003155099034306618   to: 0.002611448526100801
i:   9, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.003590869549826641   to: 0.002997791155384443
i:  10, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.004058973384723318   to: 0.003418701601316514
i:  11, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.004557620499438902   to: 0.003872343760126267
i:  12, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.005085073516267590   to: 0.004356932415378994
i:  13, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.005639648039802657   to: 0.004870733858886461
i:  14, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.006219712776621433   to: 0.005412066286553606
i:  15, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.006823689474442501   to: 0.005979299990819758
i:  16, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.007450052699541503   to: 0.006570857369857596
i:  17, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.008097329469838339   to: 0.007185212772267929
i:  18, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.008764098759767525   to: 0.007820892194656431
i:  19, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.009448990891815296   to: 0.008476472848195769
i:  20, name:               module.fc.weight  changing lr from: 0.010150686828449839   to: 0.009150582609065764
i:  21, name:                 module.fc.bias  changing lr from: 0.010867917377081166   to: 0.009841899366520402



# Switched to train mode...
Epoch: [72][  0/391]	Time  0.189 ( 0.189)	Data  0.151 ( 0.151)	Loss 4.9664e-03 (4.9664e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.036 ( 0.050)	Data  0.001 ( 0.015)	Loss 4.5413e-03 (5.4563e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.037 ( 0.044)	Data  0.001 ( 0.009)	Loss 8.5111e-03 (7.0941e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.007)	Loss 3.6117e-03 (6.8230e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.005)	Loss 5.7080e-03 (7.2844e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.5292e-03 (7.4593e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.004)	Loss 8.6744e-03 (7.1747e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.9342e-03 (6.9477e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.9434e-03 (7.1233e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 6.8414e-03 (7.0263e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.003)	Loss 4.4801e-03 (7.1977e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.003)	Loss 8.2372e-03 (7.1018e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.7905e-03 (7.1291e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.4771e-03 (7.0140e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.8795e-03 (7.0074e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.8276e-03 (6.9989e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.0539e-03 (6.9689e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.2593e-03 (6.9429e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.2208e-02 (6.9701e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.0412e-03 (6.9846e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.0221e-03 (6.9385e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.5247e-03 (6.9821e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.5465e-03 (7.0596e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.1168e-03 (7.0980e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 8.2589e-03 (7.0926e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.2326e-02 (7.1084e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.0433e-02 (7.1562e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 7.5940e-03 (7.2838e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 4.9908e-03 (7.3102e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.002)	Loss 1.1318e-02 (7.3202e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 8.6180e-03 (7.3439e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 3.7885e-03 (7.3958e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.0471e-03 (7.3624e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.9646e-03 (7.3585e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 3.2398e-03 (7.3090e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.8210e-03 (7.2719e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 3.2949e-03 (7.3102e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.2629e-03 (7.3197e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.2254e-03 (7.3372e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.002)	Loss 5.5049e-03 (7.3074e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.06944942474365234
## e[72]       loss.backward (sum) time: 1.9499881267547607
## e[72]      optimizer.step (sum) time: 0.405468225479126
## epoch[72] training(only) time: 14.402637720108032
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.2839e+00 (1.2839e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.2745e+00 (1.2491e+00)	Acc@1  77.00 ( 74.73)	Acc@5  93.00 ( 91.45)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1918e+00 (1.1901e+00)	Acc@1  77.00 ( 75.62)	Acc@5  91.00 ( 92.10)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7802e+00 (1.2262e+00)	Acc@1  67.00 ( 74.61)	Acc@5  92.00 ( 91.90)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3491e+00 (1.2263e+00)	Acc@1  71.00 ( 74.22)	Acc@5  92.00 ( 92.10)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4903e+00 (1.2437e+00)	Acc@1  72.00 ( 74.16)	Acc@5  90.00 ( 91.86)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.3056e+00 (1.2217e+00)	Acc@1  67.00 ( 74.30)	Acc@5  93.00 ( 92.11)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5941e+00 (1.2269e+00)	Acc@1  71.00 ( 74.10)	Acc@5  89.00 ( 92.08)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5274e+00 (1.2328e+00)	Acc@1  70.00 ( 74.10)	Acc@5  87.00 ( 92.10)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3647e+00 (1.2140e+00)	Acc@1  73.00 ( 74.42)	Acc@5  92.00 ( 92.29)
 * Acc@1 74.520 Acc@5 92.430
### epoch[72] execution time: 16.894234895706177
EPOCH 73
REMOVING: module.conv4_x.1.residual_function.1.weight
REMOVING: module.conv4_x.1.residual_function.1.bias
i:   0, name: module.conv4_x.1.residual_function.3.weight  changing lr from: 0.001128270845455225   to: 0.001021033678481802
i:   1, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.001265944937967594   to: 0.001092951569171912
i:   2, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.001450202769670194   to: 0.001213879310252918
i:   3, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.001678931694002482   to: 0.001381683710993946
i:   4, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.001950060080878312   to: 0.001594267879310761
i:   5, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.002261559835370935   to: 0.001849574247457080
i:   6, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.002611448526100801   to: 0.002145587173158458
i:   7, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.002997791155384443   to: 0.002480335149760631
i:   8, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.003418701601316514   to: 0.002851892657086823
i:   9, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.003872343760126267   to: 0.003258381682864130
i:  10, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.004356932415378994   to: 0.003697972942794641
i:  11, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.004870733858886461   to: 0.004168886825617767
i:  12, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.005412066286553606   to: 0.004669394087843703
i:  13, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.005979299990819758   to: 0.005197816321233250
i:  14, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.006570857369857596   to: 0.005752526214563535
i:  15, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.007185212772267929   to: 0.006331947629750478
i:  16, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.007820892194656431   to: 0.006934555510998790
i:  17, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.008476472848195769   to: 0.007558875644320001
i:  18, name:               module.fc.weight  changing lr from: 0.009150582609065764   to: 0.008203484283496570
i:  19, name:                 module.fc.bias  changing lr from: 0.009841899366520402   to: 0.008867007657375858



# Switched to train mode...
Epoch: [73][  0/391]	Time  0.189 ( 0.189)	Data  0.153 ( 0.153)	Loss 5.4421e-03 (5.4421e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.036 ( 0.049)	Data  0.001 ( 0.015)	Loss 7.0568e-03 (7.8729e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.009)	Loss 4.7492e-03 (6.8929e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.007)	Loss 5.0294e-03 (6.9415e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.005)	Loss 1.2184e-02 (7.1290e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.2558e-03 (6.9924e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.4406e-03 (6.7301e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.4603e-03 (6.7790e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 9.4459e-03 (6.6696e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.9458e-03 (6.5752e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 8.5252e-03 (6.6823e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.5126e-03 (6.7104e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.3627e-03 (6.7761e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.4412e-02 (6.7750e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.0276e-02 (6.8207e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.7852e-03 (6.8648e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.8334e-03 (6.8270e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.0346e-02 (6.8838e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 8.6306e-03 (6.9035e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.2512e-03 (6.8842e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.0268e-02 (6.9317e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.8586e-03 (6.9092e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7805e-03 (6.8899e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.035 ( 0.036)	Data  0.002 ( 0.003)	Loss 9.1451e-03 (6.8613e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.7865e-03 (6.8721e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0558e-02 (6.8764e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.2890e-03 (6.9203e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.2485e-03 (7.1869e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.3881e-03 (7.2009e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.3996e-02 (7.2281e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.9442e-03 (7.2145e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2297e-03 (7.1997e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.0611e-03 (7.1928e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3524e-03 (7.1812e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.7228e-03 (7.1764e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.3921e-03 (7.2607e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.7754e-03 (7.2584e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.0644e-03 (7.2674e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.8369e-03 (7.2396e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5633e-03 (7.2440e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.06418681144714355
## e[73]       loss.backward (sum) time: 1.8806910514831543
## e[73]      optimizer.step (sum) time: 0.37226152420043945
## epoch[73] training(only) time: 14.257013320922852
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2694e+00 (1.2694e+00)	Acc@1  76.00 ( 76.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2748e+00 (1.2485e+00)	Acc@1  77.00 ( 74.82)	Acc@5  92.00 ( 91.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1648e+00 (1.1886e+00)	Acc@1  78.00 ( 75.57)	Acc@5  91.00 ( 91.95)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6742e+00 (1.2228e+00)	Acc@1  66.00 ( 74.71)	Acc@5  91.00 ( 91.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3536e+00 (1.2235e+00)	Acc@1  71.00 ( 74.39)	Acc@5  92.00 ( 92.00)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5291e+00 (1.2430e+00)	Acc@1  71.00 ( 74.25)	Acc@5  90.00 ( 91.75)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3079e+00 (1.2210e+00)	Acc@1  68.00 ( 74.38)	Acc@5  93.00 ( 92.08)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.5858e+00 (1.2264e+00)	Acc@1  72.00 ( 74.13)	Acc@5  88.00 ( 92.00)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5532e+00 (1.2325e+00)	Acc@1  71.00 ( 74.17)	Acc@5  87.00 ( 92.06)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3507e+00 (1.2125e+00)	Acc@1  74.00 ( 74.58)	Acc@5  91.00 ( 92.16)
 * Acc@1 74.650 Acc@5 92.330
### epoch[73] execution time: 16.754680395126343
EPOCH 74
REMOVING: module.conv4_x.1.residual_function.3.weight
i:   0, name: module.conv4_x.1.residual_function.4.weight  changing lr from: 0.001092951569171912   to: 0.001008821899874756
i:   1, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.001213879310252918   to: 0.001064398473244659
i:   2, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.001381683710993946   to: 0.001169228597683507
i:   3, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.001594267879310761   to: 0.001321197033015724
i:   4, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.001849574247457080   to: 0.001518223121950301
i:   5, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.002145587173158458   to: 0.001758263891494049
i:   6, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.002480335149760631   to: 0.002039316730009588
i:   7, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.002851892657086823   to: 0.002359421673052702
i:   8, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.003258381682864130   to: 0.002716663329302165
i:   9, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.003697972942794641   to: 0.003109172476111248
i:  10, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.004168886825617767   to: 0.003535127352470973
i:  11, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.004669394087843703   to: 0.003992754675490122
i:  12, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.005197816321233250   to: 0.004480330404866050
i:  13, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.005752526214563535   to: 0.004996180278252953
i:  14, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.006331947629750478   to: 0.005538680138927931
i:  15, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.006934555510998790   to: 0.006106256075715521
i:  16, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.007558875644320001   to: 0.006697384393755900
i:  17, name:               module.fc.weight  changing lr from: 0.008203484283496570   to: 0.007310591433394941
i:  18, name:                 module.fc.bias  changing lr from: 0.008867007657375858   to: 0.007944453253231325



# Switched to train mode...
Epoch: [74][  0/391]	Time  0.183 ( 0.183)	Data  0.147 ( 0.147)	Loss 6.3164e-03 (6.3164e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.035 ( 0.049)	Data  0.001 ( 0.015)	Loss 6.8069e-03 (6.4355e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.009)	Loss 3.5622e-03 (6.4341e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 6.9953e-03 (6.7332e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.5687e-03 (6.8757e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.005)	Loss 9.1970e-03 (6.9877e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.4317e-02 (7.4249e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.2553e-03 (7.2937e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.0959e-02 (7.5164e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.035 ( 0.037)	Data  0.002 ( 0.003)	Loss 5.0820e-03 (7.3472e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.035 ( 0.037)	Data  0.002 ( 0.003)	Loss 7.0382e-03 (7.2341e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.4494e-03 (7.2421e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.9791e-03 (7.1420e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.0986e-03 (7.1693e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 8.7932e-03 (7.0955e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.0662e-02 (7.1188e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1735e-03 (7.0608e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.7349e-03 (7.0600e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2873e-03 (7.0650e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.5680e-02 (7.4412e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.1717e-03 (7.4379e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7205e-03 (7.3732e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.2861e-03 (7.3433e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5677e-03 (7.2958e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4280e-03 (7.2898e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.8317e-03 (7.3190e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4623e-03 (7.2694e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.6862e-03 (7.2291e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.2471e-03 (7.2389e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.9207e-03 (7.2826e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.2015e-03 (7.2891e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.9539e-03 (7.2672e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.1665e-03 (7.2641e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.002)	Loss 4.4350e-03 (7.2617e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0577e-02 (7.3068e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.6096e-03 (7.2922e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0710e-02 (7.3036e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0267e-02 (7.2771e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1637e-03 (7.2214e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.3793e-02 (7.2175e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.06083989143371582
## e[74]       loss.backward (sum) time: 1.8694219589233398
## e[74]      optimizer.step (sum) time: 0.3558511734008789
## epoch[74] training(only) time: 14.162460803985596
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2572e+00 (1.2572e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.035)	Loss 1.2821e+00 (1.2473e+00)	Acc@1  79.00 ( 75.18)	Acc@5  92.00 ( 91.00)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1884e+00 (1.1850e+00)	Acc@1  76.00 ( 75.71)	Acc@5  92.00 ( 91.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6821e+00 (1.2185e+00)	Acc@1  69.00 ( 74.90)	Acc@5  91.00 ( 91.58)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3095e+00 (1.2185e+00)	Acc@1  72.00 ( 74.59)	Acc@5  92.00 ( 91.88)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5298e+00 (1.2372e+00)	Acc@1  70.00 ( 74.37)	Acc@5  89.00 ( 91.67)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2778e+00 (1.2138e+00)	Acc@1  70.00 ( 74.61)	Acc@5  93.00 ( 92.00)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.5264e+00 (1.2184e+00)	Acc@1  71.00 ( 74.37)	Acc@5  89.00 ( 91.94)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5482e+00 (1.2250e+00)	Acc@1  71.00 ( 74.33)	Acc@5  86.00 ( 91.96)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3630e+00 (1.2052e+00)	Acc@1  74.00 ( 74.69)	Acc@5  92.00 ( 92.15)
 * Acc@1 74.780 Acc@5 92.320
### epoch[74] execution time: 16.65813684463501
EPOCH 75
REMOVING: module.conv4_x.1.residual_function.4.weight
i:   0, name: module.conv4_x.1.residual_function.4.bias  changing lr from: 0.001064398473244659   to: 0.001002021000147384
i:   1, name: module.conv5_x.0.residual_function.0.weight  changing lr from: 0.001169228597683507   to: 0.001041929419166620
i:   2, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.001321197033015724   to: 0.001131304751906325
i:   3, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.001518223121950301   to: 0.001268050061870595
i:   4, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.001758263891494049   to: 0.001450101339773257
i:   5, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.002039316730009588   to: 0.001675430673876287
i:   6, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.002359421673052702   to: 0.001942048996617789
i:   7, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.002716663329302165   to: 0.002248008440212440
i:   8, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.003109172476111248   to: 0.002591404332139695
i:   9, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.003535127352470973   to: 0.002970376859700840
i:  10, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.003992754675490122   to: 0.003383112431133816
i:  11, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.004480330404866050   to: 0.003827844759129511
i:  12, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.004996180278252953   to: 0.004302855691002665
i:  13, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.005538680138927931   to: 0.004806475808236199
i:  14, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.006106256075715521   to: 0.005337084816643877
i:  15, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.006697384393755900   to: 0.005893111746983859
i:  16, name:               module.fc.weight  changing lr from: 0.007310591433394941   to: 0.006473034984507297
i:  17, name:                 module.fc.bias  changing lr from: 0.007944453253231325   to: 0.007075382144640651



# Switched to train mode...
Epoch: [75][  0/391]	Time  0.179 ( 0.179)	Data  0.145 ( 0.145)	Loss 7.8417e-03 (7.8417e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.035 ( 0.048)	Data  0.001 ( 0.015)	Loss 4.0617e-03 (6.0827e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.009)	Loss 6.7051e-03 (6.9995e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 5.7638e-03 (7.0713e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.1003e-03 (6.8998e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.005)	Loss 7.0850e-03 (6.7011e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 8.4520e-03 (6.8318e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.4177e-02 (7.2875e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.3657e-03 (7.2144e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.7958e-02 (7.5818e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.2938e-03 (7.5979e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.7403e-03 (7.5817e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.7538e-03 (7.5269e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.7173e-03 (7.5026e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.3364e-03 (7.4941e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.6584e-03 (7.5174e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8008e-03 (7.4562e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3331e-03 (7.5004e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.5908e-02 (7.4906e-03)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.0465e-03 (7.4796e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8302e-03 (7.4084e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6502e-03 (7.3807e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.4360e-03 (7.3463e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.1635e-03 (7.3134e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.0665e-03 (7.3123e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.2099e-03 (7.2146e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5948e-03 (7.1599e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.1134e-03 (7.1291e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2161e-03 (7.1328e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.5185e-03 (7.1713e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1354e-03 (7.1529e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1296e-02 (7.1341e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6877e-03 (7.1124e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4128e-03 (7.1848e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.6226e-02 (7.2250e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.1874e-03 (7.2223e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.3376e-03 (7.2004e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.6922e-03 (7.1865e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.1690e-03 (7.1723e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0646e-02 (7.1908e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.057904958724975586
## e[75]       loss.backward (sum) time: 1.8668031692504883
## e[75]      optimizer.step (sum) time: 0.3295280933380127
## epoch[75] training(only) time: 14.146371364593506
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.2372e+00 (1.2372e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.3015e+00 (1.2478e+00)	Acc@1  78.00 ( 75.18)	Acc@5  92.00 ( 91.27)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.1859e+00 (1.1879e+00)	Acc@1  76.00 ( 75.62)	Acc@5  91.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6684e+00 (1.2210e+00)	Acc@1  67.00 ( 74.87)	Acc@5  91.00 ( 91.68)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3468e+00 (1.2209e+00)	Acc@1  69.00 ( 74.37)	Acc@5  92.00 ( 92.00)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5087e+00 (1.2383e+00)	Acc@1  71.00 ( 74.12)	Acc@5  90.00 ( 91.71)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2957e+00 (1.2158e+00)	Acc@1  69.00 ( 74.31)	Acc@5  93.00 ( 92.02)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5703e+00 (1.2217e+00)	Acc@1  71.00 ( 74.10)	Acc@5  89.00 ( 91.99)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5598e+00 (1.2270e+00)	Acc@1  70.00 ( 74.17)	Acc@5  86.00 ( 92.04)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3419e+00 (1.2067e+00)	Acc@1  74.00 ( 74.52)	Acc@5  90.00 ( 92.15)
 * Acc@1 74.600 Acc@5 92.330
### epoch[75] execution time: 16.618638515472412
EPOCH 76
REMOVING: module.conv4_x.1.residual_function.4.bias
REMOVING: module.conv5_x.0.residual_function.0.weight
i:   0, name: module.conv5_x.0.residual_function.1.weight  changing lr from: 0.001131304751906325   to: 0.001024908977771399
i:   1, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.001268050061870595   to: 0.001099465492354656
i:   2, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.001450101339773257   to: 0.001221594918330498
i:   3, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.001675430673876287   to: 0.001389250245247630
i:   4, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.001942048996617789   to: 0.001600419027637461
i:   5, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.002248008440212440   to: 0.001853126195038612
i:   6, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.002591404332139695   to: 0.002145436471589639
i:   7, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.002970376859700840   to: 0.002475456435693334
i:   8, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.003383112431133816   to: 0.002841336248571507
i:   9, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.003827844759129511   to: 0.003241271078881396
i:  10, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.004302855691002665   to: 0.003673502248961990
i:  11, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.004806475808236199   to: 0.004136318126725524
i:  12, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.005337084816643877   to: 0.004628054785709755
i:  13, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.005893111746983859   to: 0.005147096454364019
i:  14, name:               module.fc.weight  changing lr from: 0.006473034984507297   to: 0.005691875774258914
i:  15, name:                 module.fc.bias  changing lr from: 0.007075382144640651   to: 0.006260873885586288



# Switched to train mode...
Epoch: [76][  0/391]	Time  0.190 ( 0.190)	Data  0.155 ( 0.155)	Loss 7.9897e-03 (7.9897e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.016)	Loss 5.9743e-03 (5.3131e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 2.2127e-02 (6.4124e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.007)	Loss 5.2895e-03 (6.7191e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.034 ( 0.038)	Data  0.002 ( 0.006)	Loss 4.7802e-03 (6.5556e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.005)	Loss 5.9716e-03 (6.7338e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.3835e-03 (6.8066e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.9572e-03 (6.7419e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.5854e-03 (6.6999e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2738e-03 (6.6809e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8046e-03 (6.7169e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2402e-03 (6.7373e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1634e-03 (6.6651e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.9356e-03 (6.6768e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.9777e-03 (6.6557e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.8607e-03 (6.6041e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.0005e-03 (6.5840e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.5912e-03 (6.5457e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.6174e-03 (6.6621e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.7178e-03 (6.8199e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.8986e-03 (6.8086e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.2035e-02 (6.8199e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7588e-02 (6.8684e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5696e-03 (6.8526e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6043e-03 (6.8732e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1630e-03 (6.8398e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3850e-02 (6.9203e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6206e-03 (6.8953e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7834e-03 (6.8796e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0751e-03 (6.9157e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 6.7148e-03 (6.9511e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 4.0864e-03 (6.9562e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5145e-03 (6.9549e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4681e-03 (6.9641e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0546e-02 (6.9732e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3544e-03 (6.9782e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5281e-03 (6.9622e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4408e-03 (6.9652e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4321e-03 (6.9859e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4308e-03 (6.9859e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.05344343185424805
## e[76]       loss.backward (sum) time: 1.7604954242706299
## e[76]      optimizer.step (sum) time: 0.3173506259918213
## epoch[76] training(only) time: 13.693421125411987
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.2456e+00 (1.2456e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2816e+00 (1.2416e+00)	Acc@1  77.00 ( 74.82)	Acc@5  93.00 ( 91.55)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1810e+00 (1.1854e+00)	Acc@1  77.00 ( 75.57)	Acc@5  92.00 ( 92.14)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7339e+00 (1.2234e+00)	Acc@1  66.00 ( 74.74)	Acc@5  91.00 ( 92.00)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3289e+00 (1.2233e+00)	Acc@1  73.00 ( 74.34)	Acc@5  92.00 ( 92.17)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5061e+00 (1.2410e+00)	Acc@1  72.00 ( 74.27)	Acc@5  90.00 ( 91.90)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3100e+00 (1.2197e+00)	Acc@1  68.00 ( 74.44)	Acc@5  93.00 ( 92.13)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5733e+00 (1.2250e+00)	Acc@1  72.00 ( 74.14)	Acc@5  89.00 ( 92.04)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5488e+00 (1.2316e+00)	Acc@1  70.00 ( 74.17)	Acc@5  87.00 ( 92.10)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3723e+00 (1.2124e+00)	Acc@1  73.00 ( 74.47)	Acc@5  91.00 ( 92.23)
 * Acc@1 74.550 Acc@5 92.380
### epoch[76] execution time: 16.17012619972229
EPOCH 77
REMOVING: module.conv5_x.0.residual_function.1.weight
i:   0, name: module.conv5_x.0.residual_function.1.bias  changing lr from: 0.001099465492354656   to: 0.001012745987326823
i:   1, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.001221594918330498   to: 0.001073111972766216
i:   2, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.001389250245247630   to: 0.001181226291036018
i:   3, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.001600419027637461   to: 0.001335059223501766
i:   4, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.001853126195038612   to: 0.001532614138749193
i:   5, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.002145436471589639   to: 0.001771930360019901
i:   6, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.002475456435693334   to: 0.002051085643188720
i:   7, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.002841336248571507   to: 0.002368198295363099
i:   8, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.003241271078881396   to: 0.002721428962547333
i:   9, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.003673502248961990   to: 0.003108982113212171
i:  10, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.004136318126725524   to: 0.003529107243048512
i:  11, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.004628054785709755   to: 0.003980099824668009
i:  12, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.005147096454364019   to: 0.004460302024548562
i:  13, name:               module.fc.weight  changing lr from: 0.005691875774258914   to: 0.004968103208111845
i:  14, name:                 module.fc.bias  changing lr from: 0.006260873885586288   to: 0.005501940252465059



# Switched to train mode...
Epoch: [77][  0/391]	Time  0.186 ( 0.186)	Data  0.152 ( 0.152)	Loss 7.7718e-03 (7.7718e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.034 ( 0.048)	Data  0.001 ( 0.015)	Loss 7.4790e-03 (6.1221e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.035 ( 0.041)	Data  0.001 ( 0.009)	Loss 7.5009e-03 (6.4123e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.007)	Loss 2.3809e-03 (6.2612e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 6.5464e-03 (6.2867e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.005)	Loss 8.7901e-03 (6.3959e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 9.3220e-03 (6.3292e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.1568e-03 (6.2262e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 6.2647e-03 (6.1746e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.2301e-03 (6.3533e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7022e-03 (6.6175e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.3297e-03 (6.7809e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1663e-03 (6.7446e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.0154e-03 (6.6643e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.1456e-03 (6.6296e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5435e-02 (6.7556e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.8238e-02 (6.7553e-03)	Acc@1  99.22 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.8681e-03 (6.7509e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.1710e-03 (6.8403e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.3206e-03 (6.8078e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.8816e-03 (6.8721e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.1048e-03 (6.8595e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7353e-03 (6.9505e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6075e-03 (6.9244e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9989e-03 (6.8932e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2551e-03 (6.8646e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2107e-03 (6.8183e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9887e-03 (6.7956e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1562e-03 (6.8603e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7032e-02 (6.8872e-03)	Acc@1  99.22 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2580e-03 (6.8354e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7311e-03 (6.9078e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5906e-03 (6.9043e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7844e-03 (6.9244e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5097e-03 (6.9044e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5356e-02 (6.9176e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2831e-02 (6.9833e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7401e-03 (6.9455e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1025e-03 (6.9598e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9993e-03 (6.9381e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.050489187240600586
## e[77]       loss.backward (sum) time: 1.7554819583892822
## e[77]      optimizer.step (sum) time: 0.2951629161834717
## epoch[77] training(only) time: 13.723665237426758
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.2509e+00 (1.2509e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2870e+00 (1.2388e+00)	Acc@1  77.00 ( 74.91)	Acc@5  92.00 ( 91.09)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1681e+00 (1.1836e+00)	Acc@1  77.00 ( 75.52)	Acc@5  92.00 ( 91.71)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6844e+00 (1.2192e+00)	Acc@1  67.00 ( 74.74)	Acc@5  90.00 ( 91.55)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3200e+00 (1.2196e+00)	Acc@1  72.00 ( 74.44)	Acc@5  92.00 ( 91.88)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5000e+00 (1.2382e+00)	Acc@1  71.00 ( 74.16)	Acc@5  90.00 ( 91.69)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2871e+00 (1.2162e+00)	Acc@1  69.00 ( 74.31)	Acc@5  93.00 ( 91.97)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5654e+00 (1.2218e+00)	Acc@1  71.00 ( 74.13)	Acc@5  89.00 ( 91.94)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5374e+00 (1.2276e+00)	Acc@1  70.00 ( 74.20)	Acc@5  87.00 ( 92.04)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3536e+00 (1.2091e+00)	Acc@1  74.00 ( 74.51)	Acc@5  90.00 ( 92.14)
 * Acc@1 74.600 Acc@5 92.320
### epoch[77] execution time: 16.212227821350098
EPOCH 78
REMOVING: module.conv5_x.0.residual_function.1.bias
i:   0, name: module.conv5_x.0.residual_function.3.weight  changing lr from: 0.001073111972766216   to: 0.001004891203404250
i:   1, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.001181226291036018   to: 0.001051686530917849
i:   2, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.001335059223501766   to: 0.001146379284771037
i:   3, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.001532614138749193   to: 0.001286957277618677
i:   4, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.001771930360019901   to: 0.001471439986743749
i:   5, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.002051085643188720   to: 0.001697881473112266
i:   6, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.002368198295363099   to: 0.001964372912298367
i:   7, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.002721428962547333   to: 0.002269044766925237
i:   8, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.003108982113212171   to: 0.002610068628678529
i:   9, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.003529107243048512   to: 0.002985658756390080
i:  10, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.003980099824668009   to: 0.003394073335168073
i:  11, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.004460302024548562   to: 0.003833615480071694
i:  12, name:               module.fc.weight  changing lr from: 0.004968103208111845   to: 0.004302634006397687
i:  13, name:                 module.fc.bias  changing lr from: 0.005501940252465059   to: 0.004799523987266617



# Switched to train mode...
Epoch: [78][  0/391]	Time  0.178 ( 0.178)	Data  0.143 ( 0.143)	Loss 6.7635e-03 (6.7635e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.033 ( 0.047)	Data  0.001 ( 0.015)	Loss 5.7192e-03 (7.5105e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.008)	Loss 6.7163e-03 (7.4935e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.006)	Loss 6.9177e-03 (7.1682e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 7.7999e-03 (7.4053e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.0784e-02 (7.2304e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.7455e-03 (7.0711e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.8742e-03 (7.0761e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.7641e-03 (7.0152e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.4482e-02 (7.1458e-03)	Acc@1  99.22 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.8247e-03 (7.1239e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.9090e-03 (7.1330e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.1486e-03 (7.0400e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.2333e-03 (7.2474e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.4817e-03 (7.2219e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.4949e-03 (7.1603e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.2962e-03 (7.2758e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.033 ( 0.035)	Data  0.002 ( 0.003)	Loss 6.8502e-03 (7.2645e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.3205e-03 (7.2046e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.3976e-03 (7.1595e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.2659e-03 (7.2135e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4425e-03 (7.2434e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3233e-03 (7.2831e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8435e-03 (7.2719e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1799e-03 (7.1955e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.041 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.4967e-03 (7.2540e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4379e-03 (7.2002e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2333e-03 (7.2062e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.9158e-03 (7.1840e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6695e-03 (7.2022e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5056e-03 (7.1939e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.8645e-03 (7.1813e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9783e-03 (7.1918e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2304e-03 (7.1788e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8528e-03 (7.1741e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2792e-02 (7.2373e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1212e-03 (7.2232e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9962e-03 (7.1758e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.3742e-03 (7.1802e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9008e-03 (7.1283e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.04914665222167969
## e[78]       loss.backward (sum) time: 1.7399849891662598
## e[78]      optimizer.step (sum) time: 0.2810380458831787
## epoch[78] training(only) time: 13.520676374435425
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2552e+00 (1.2552e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.2494e+00 (1.2351e+00)	Acc@1  77.00 ( 75.00)	Acc@5  93.00 ( 91.27)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1601e+00 (1.1784e+00)	Acc@1  78.00 ( 75.52)	Acc@5  92.00 ( 91.76)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6628e+00 (1.2149e+00)	Acc@1  65.00 ( 74.65)	Acc@5  90.00 ( 91.61)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3178e+00 (1.2153e+00)	Acc@1  71.00 ( 74.41)	Acc@5  91.00 ( 91.80)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5390e+00 (1.2358e+00)	Acc@1  71.00 ( 74.25)	Acc@5  89.00 ( 91.57)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2774e+00 (1.2142e+00)	Acc@1  69.00 ( 74.44)	Acc@5  93.00 ( 91.84)
Test: [ 70/100]	Time  0.028 ( 0.025)	Loss 1.5846e+00 (1.2186e+00)	Acc@1  71.00 ( 74.18)	Acc@5  89.00 ( 91.85)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5729e+00 (1.2249e+00)	Acc@1  71.00 ( 74.25)	Acc@5  86.00 ( 91.95)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3737e+00 (1.2062e+00)	Acc@1  74.00 ( 74.63)	Acc@5  90.00 ( 92.08)
 * Acc@1 74.680 Acc@5 92.260
### epoch[78] execution time: 16.021899461746216
EPOCH 79
REMOVING: module.conv5_x.0.residual_function.3.weight
i:   0, name: module.conv5_x.0.residual_function.4.weight  changing lr from: 0.001051686530917849   to: 0.001000835041045932
i:   1, name: module.conv5_x.0.residual_function.4.bias  changing lr from: 0.001146379284771037   to: 0.001034670522621344
i:   2, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.001286957277618677   to: 0.001116527345304723
i:   3, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.001471439986743749   to: 0.001244411043144471
i:   4, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.001697881473112266   to: 0.001416357447533705
i:   5, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.001964372912298367   to: 0.001630435652291411
i:   6, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.002269044766925237   to: 0.001884750592272909
i:   7, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.002610068628678529   to: 0.002177445264713167
i:   8, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.002985658756390080   to: 0.002506702620963268
i:   9, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.003394073335168073   to: 0.002870747154764718
i:  10, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.003833615480071694   to: 0.003267846211724423
i:  11, name:               module.fc.weight  changing lr from: 0.004302634006397687   to: 0.003696311043212230
i:  12, name:                 module.fc.bias  changing lr from: 0.004799523987266617   to: 0.004154497626506054



# Switched to train mode...
Epoch: [79][  0/391]	Time  0.185 ( 0.185)	Data  0.150 ( 0.150)	Loss 4.3966e-03 (4.3966e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.032 ( 0.047)	Data  0.001 ( 0.015)	Loss 1.7380e-02 (8.8772e-03)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.009)	Loss 4.3471e-03 (8.7554e-03)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.007)	Loss 7.4909e-03 (8.1535e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 8.3610e-03 (7.9068e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 5.3479e-03 (7.5508e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 3.3866e-03 (7.4148e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.2276e-03 (7.1948e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.3561e-03 (7.0615e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.2906e-03 (6.8934e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1782e-02 (6.8327e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.9128e-03 (7.1132e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.0136e-03 (7.1050e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.9857e-03 (7.0117e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.033 ( 0.035)	Data  0.002 ( 0.003)	Loss 4.7991e-03 (6.9085e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.0006e-03 (6.8509e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 8.3789e-03 (6.9827e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 5.6241e-03 (6.9362e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.6376e-03 (6.9032e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.7086e-03 (6.9888e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.1053e-03 (6.9721e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.3367e-03 (6.9482e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7305e-03 (6.8876e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.1486e-03 (6.8416e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9275e-03 (6.8718e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.3791e-03 (6.8411e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.034 ( 0.034)	Data  0.002 ( 0.002)	Loss 3.4994e-03 (6.8187e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.002)	Loss 8.3924e-03 (6.7973e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.6647e-03 (6.7475e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.1644e-03 (6.7121e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.4531e-03 (6.6993e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2393e-03 (6.7984e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3457e-03 (6.7669e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.9639e-03 (6.7828e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9926e-03 (6.7787e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.3147e-03 (6.7392e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7045e-03 (6.7376e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2693e-03 (6.7286e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1787e-03 (6.6790e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7995e-03 (6.6860e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.04626822471618652
## e[79]       loss.backward (sum) time: 1.7435956001281738
## e[79]      optimizer.step (sum) time: 0.2668571472167969
## epoch[79] training(only) time: 13.391555547714233
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2516e+00 (1.2516e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2847e+00 (1.2445e+00)	Acc@1  78.00 ( 75.27)	Acc@5  93.00 ( 91.18)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1571e+00 (1.1844e+00)	Acc@1  76.00 ( 75.67)	Acc@5  90.00 ( 91.67)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6903e+00 (1.2177e+00)	Acc@1  66.00 ( 74.71)	Acc@5  92.00 ( 91.58)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3141e+00 (1.2175e+00)	Acc@1  72.00 ( 74.34)	Acc@5  92.00 ( 91.80)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5298e+00 (1.2359e+00)	Acc@1  72.00 ( 74.24)	Acc@5  89.00 ( 91.61)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.2808e+00 (1.2145e+00)	Acc@1  69.00 ( 74.39)	Acc@5  93.00 ( 91.90)
Test: [ 70/100]	Time  0.023 ( 0.025)	Loss 1.5676e+00 (1.2184e+00)	Acc@1  70.00 ( 74.17)	Acc@5  90.00 ( 91.90)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5689e+00 (1.2248e+00)	Acc@1  71.00 ( 74.21)	Acc@5  86.00 ( 91.95)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3322e+00 (1.2046e+00)	Acc@1  73.00 ( 74.55)	Acc@5  91.00 ( 92.13)
 * Acc@1 74.590 Acc@5 92.310
### epoch[79] execution time: 15.877939462661743
EPOCH 80
REMOVING: module.conv5_x.0.residual_function.4.weight
REMOVING: module.conv5_x.0.residual_function.4.bias
i:   0, name: module.conv5_x.0.shortcut.0.weight  changing lr from: 0.001116527345304723   to: 0.001021582240332241
i:   1, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.001244411043144471   to: 0.001091180261619911
i:   2, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.001416357447533705   to: 0.001206922873059662
i:   3, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.001630435652291411   to: 0.001366862460072775
i:   4, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.001884750592272909   to: 0.001569083399096998
i:   5, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.002177445264713167   to: 0.001811704678799680
i:   6, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.002506702620963268   to: 0.002092882165542592
i:   7, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.002870747154764718   to: 0.002410810540354501
i:   8, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.003267846211724423   to: 0.002763724933193989
i:   9, name:               module.fc.weight  changing lr from: 0.003696311043212230   to: 0.003149902278842813
i:  10, name:                 module.fc.bias  changing lr from: 0.004154497626506054   to: 0.003567662417365174



# Switched to train mode...
Epoch: [80][  0/391]	Time  0.194 ( 0.194)	Data  0.158 ( 0.158)	Loss 4.6042e-03 (4.6042e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.044 ( 0.049)	Data  0.001 ( 0.016)	Loss 2.8961e-02 (8.3323e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.009)	Loss 1.1424e-02 (9.0495e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.007)	Loss 5.4609e-03 (8.2739e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.1064e-02 (7.9880e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 5.7706e-03 (7.9562e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.9234e-03 (7.6768e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 7.1361e-03 (7.4676e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.8170e-03 (7.2748e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.2721e-03 (7.3335e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.8064e-03 (7.4664e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.3815e-03 (7.4485e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.0513e-03 (7.4170e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.8341e-03 (7.3345e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 5.1692e-03 (7.2266e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.9921e-03 (7.1796e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.8026e-03 (7.1189e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.0065e-03 (7.0471e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.0924e-03 (7.0710e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6675e-03 (7.1549e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6379e-03 (7.1050e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3530e-02 (7.1521e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6736e-03 (7.1309e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4233e-03 (7.1259e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.2082e-02 (7.1244e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.4491e-03 (7.1079e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4634e-03 (7.1296e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7646e-03 (7.0741e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.3396e-03 (7.0528e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.6848e-03 (7.0532e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.9622e-03 (7.0180e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.5485e-02 (7.1017e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7198e-03 (7.0915e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.9794e-03 (7.0997e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.7886e-03 (7.1150e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.9771e-03 (7.1021e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.8103e-03 (7.0676e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.4714e-03 (7.0490e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6447e-03 (7.0415e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6472e-03 (7.0463e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.04085850715637207
## e[80]       loss.backward (sum) time: 1.684471607208252
## e[80]      optimizer.step (sum) time: 0.23018336296081543
## epoch[80] training(only) time: 13.362555742263794
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.2427e+00 (1.2427e+00)	Acc@1  77.00 ( 77.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2673e+00 (1.2401e+00)	Acc@1  78.00 ( 75.18)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1845e+00 (1.1833e+00)	Acc@1  78.00 ( 75.81)	Acc@5  91.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6696e+00 (1.2165e+00)	Acc@1  67.00 ( 75.10)	Acc@5  91.00 ( 91.77)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3164e+00 (1.2185e+00)	Acc@1  70.00 ( 74.56)	Acc@5  92.00 ( 91.90)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5025e+00 (1.2384e+00)	Acc@1  71.00 ( 74.39)	Acc@5  90.00 ( 91.61)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.2805e+00 (1.2162e+00)	Acc@1  68.00 ( 74.48)	Acc@5  93.00 ( 91.92)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5714e+00 (1.2218e+00)	Acc@1  71.00 ( 74.23)	Acc@5  89.00 ( 91.93)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5487e+00 (1.2279e+00)	Acc@1  70.00 ( 74.23)	Acc@5  86.00 ( 92.00)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3596e+00 (1.2088e+00)	Acc@1  74.00 ( 74.60)	Acc@5  91.00 ( 92.12)
 * Acc@1 74.720 Acc@5 92.250
### epoch[80] execution time: 15.848923921585083
EPOCH 81
REMOVING: module.conv5_x.0.shortcut.0.weight
i:   0, name: module.conv5_x.0.shortcut.1.weight  changing lr from: 0.001091180261619911   to: 0.001011974916136415
i:   1, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.001206922873059662   to: 0.001069882245744446
i:   2, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.001366862460072775   to: 0.001174028993848269
i:   3, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.001569083399096998   to: 0.001322484257877237
i:   4, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.001811704678799680   to: 0.001513347904662437
i:   5, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.002092882165542592   to: 0.001744753229351619
i:   6, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.002410810540354501   to: 0.002014869260376522
i:   7, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.002763724933193989   to: 0.002321902737317793
i:   8, name:               module.fc.weight  changing lr from: 0.003149902278842813   to: 0.002664099787080420
i:   9, name:                 module.fc.bias  changing lr from: 0.003567662417365174   to: 0.003039747322388974



# Switched to train mode...
Epoch: [81][  0/391]	Time  0.187 ( 0.187)	Data  0.152 ( 0.152)	Loss 6.1233e-03 (6.1233e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.033 ( 0.047)	Data  0.001 ( 0.015)	Loss 5.2667e-03 (6.9815e-03)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.009)	Loss 6.6189e-03 (7.1011e-03)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.007)	Loss 8.7774e-03 (7.8060e-03)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.005)	Loss 1.1783e-02 (7.6479e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.3636e-02 (7.4127e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 4.3190e-03 (7.5525e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.004)	Loss 9.3377e-03 (7.3867e-03)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.4363e-03 (7.3478e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 9.4358e-03 (7.2518e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.4787e-03 (7.1255e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.3340e-03 (7.0331e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.2081e-03 (7.1930e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.5985e-03 (7.1437e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.2015e-03 (7.1118e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.2959e-03 (7.2439e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.6312e-03 (7.1492e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.1199e-03 (7.1648e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2351e-03 (7.1894e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.2948e-03 (7.1266e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.2510e-03 (7.0746e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1315e-02 (7.1337e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.4875e-03 (7.1199e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3440e-02 (7.0920e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.5591e-03 (7.0282e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.8388e-03 (7.0613e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.002)	Loss 6.6800e-03 (7.0575e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.002)	Loss 4.8738e-03 (7.0427e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.2139e-03 (7.0527e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.2121e-03 (7.1063e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 8.5502e-03 (7.0781e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2950e-03 (7.0684e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0790e-02 (7.0464e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0780e-02 (7.0546e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3565e-03 (7.0986e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.035 ( 0.034)	Data  0.002 ( 0.002)	Loss 4.2506e-03 (7.0506e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3311e-03 (7.0003e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.3937e-02 (7.0219e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.5510e-03 (7.0345e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4931e-02 (7.0555e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.03642630577087402
## e[81]       loss.backward (sum) time: 1.6539013385772705
## e[81]      optimizer.step (sum) time: 0.20822453498840332
## epoch[81] training(only) time: 13.338133335113525
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.2502e+00 (1.2502e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2833e+00 (1.2492e+00)	Acc@1  78.00 ( 75.00)	Acc@5  93.00 ( 91.64)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1836e+00 (1.1901e+00)	Acc@1  77.00 ( 75.71)	Acc@5  90.00 ( 91.90)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6905e+00 (1.2244e+00)	Acc@1  66.00 ( 74.81)	Acc@5  91.00 ( 91.81)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3284e+00 (1.2264e+00)	Acc@1  69.00 ( 74.39)	Acc@5  92.00 ( 92.07)
Test: [ 50/100]	Time  0.024 ( 0.025)	Loss 1.5197e+00 (1.2445e+00)	Acc@1  72.00 ( 74.29)	Acc@5  90.00 ( 91.80)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.2968e+00 (1.2224e+00)	Acc@1  68.00 ( 74.39)	Acc@5  93.00 ( 92.11)
Test: [ 70/100]	Time  0.022 ( 0.025)	Loss 1.5638e+00 (1.2255e+00)	Acc@1  71.00 ( 74.17)	Acc@5  89.00 ( 92.10)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5518e+00 (1.2307e+00)	Acc@1  70.00 ( 74.19)	Acc@5  85.00 ( 92.06)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3520e+00 (1.2110e+00)	Acc@1  73.00 ( 74.48)	Acc@5  91.00 ( 92.19)
 * Acc@1 74.590 Acc@5 92.370
### epoch[81] execution time: 15.830275058746338
EPOCH 82
REMOVING: module.conv5_x.0.shortcut.1.weight
i:   0, name: module.conv5_x.0.shortcut.1.bias  changing lr from: 0.001069882245744446   to: 0.001005434808384552
i:   1, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.001174028993848269   to: 0.001052210089891375
i:   2, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.001322484257877237   to: 0.001145297732774458
i:   3, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.001513347904662437   to: 0.001282783665892677
i:   4, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.001744753229351619   to: 0.001462783415465706
i:   5, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.002014869260376522   to: 0.001683444796973404
i:   6, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.002321902737317793   to: 0.001942950255170910
i:   7, name:               module.fc.weight  changing lr from: 0.002664099787080420   to: 0.002239518878648533
i:   8, name:                 module.fc.bias  changing lr from: 0.003039747322388974   to: 0.002571408113973461



# Switched to train mode...
Epoch: [82][  0/391]	Time  0.186 ( 0.186)	Data  0.151 ( 0.151)	Loss 6.3667e-03 (6.3667e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.032 ( 0.047)	Data  0.001 ( 0.015)	Loss 6.9164e-03 (7.0130e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 5.6082e-03 (6.5272e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.007)	Loss 3.6818e-03 (6.3727e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 3.0727e-03 (6.0173e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.005)	Loss 4.3139e-03 (5.9425e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.004)	Loss 1.1175e-02 (6.0046e-03)	Acc@1 100.00 ( 99.99)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.4173e-03 (6.2932e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.5715e-03 (6.3972e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 2.4620e-03 (6.3489e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.4760e-03 (6.3301e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.0193e-02 (6.2969e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 7.5481e-03 (6.2148e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.6860e-03 (6.1481e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4776e-02 (6.3297e-03)	Acc@1  99.22 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.8618e-03 (6.2545e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.9731e-03 (6.2521e-03)	Acc@1 100.00 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3026e-03 (6.3031e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4814e-03 (6.3482e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.8455e-03 (6.3213e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.5400e-03 (6.4512e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.2051e-03 (6.5309e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3814e-03 (6.5292e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.2947e-03 (6.5712e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7270e-03 (6.5560e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.0622e-03 (6.5707e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6363e-03 (6.5961e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.9817e-03 (6.6316e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7962e-03 (6.6654e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.6753e-03 (6.6336e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7230e-03 (6.6680e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 2.1366e-02 (6.6729e-03)	Acc@1  98.44 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.0996e-02 (6.7038e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.5466e-03 (6.6898e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4741e-03 (6.7650e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.4896e-03 (6.7831e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7186e-03 (6.8270e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.9162e-03 (6.8224e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.3292e-03 (6.8424e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.1443e-03 (6.8606e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.03436565399169922
## e[82]       loss.backward (sum) time: 1.6494910717010498
## e[82]      optimizer.step (sum) time: 0.18665313720703125
## epoch[82] training(only) time: 13.276082515716553
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2579e+00 (1.2579e+00)	Acc@1  76.00 ( 76.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.2734e+00 (1.2430e+00)	Acc@1  78.00 ( 75.82)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.023 ( 0.029)	Loss 1.1606e+00 (1.1835e+00)	Acc@1  77.00 ( 76.19)	Acc@5  91.00 ( 91.90)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6797e+00 (1.2153e+00)	Acc@1  68.00 ( 75.42)	Acc@5  91.00 ( 91.81)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3337e+00 (1.2156e+00)	Acc@1  70.00 ( 74.90)	Acc@5  92.00 ( 92.05)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.4977e+00 (1.2347e+00)	Acc@1  72.00 ( 74.65)	Acc@5  90.00 ( 91.75)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2829e+00 (1.2131e+00)	Acc@1  69.00 ( 74.69)	Acc@5  93.00 ( 92.03)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5652e+00 (1.2178e+00)	Acc@1  71.00 ( 74.44)	Acc@5  89.00 ( 92.00)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5514e+00 (1.2234e+00)	Acc@1  71.00 ( 74.46)	Acc@5  86.00 ( 92.07)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3581e+00 (1.2038e+00)	Acc@1  74.00 ( 74.81)	Acc@5  92.00 ( 92.23)
 * Acc@1 74.910 Acc@5 92.350
### epoch[82] execution time: 15.759883642196655
EPOCH 83
REMOVING: module.conv5_x.0.shortcut.1.bias
i:   0, name: module.conv5_x.1.residual_function.0.weight  changing lr from: 0.001052210089891375   to: 0.001001579370832344
i:   1, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.001145297732774458   to: 0.001037771400183428
i:   2, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.001282783665892677   to: 0.001120327816031696
i:   3, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.001462783415465706   to: 0.001247351462732520
i:   4, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.001683444796973404   to: 0.001416973659163100
i:   5, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.001942950255170910   to: 0.001627356919073196
i:   6, name:               module.fc.weight  changing lr from: 0.002239518878648533   to: 0.001876697321859154
i:   7, name:                 module.fc.bias  changing lr from: 0.002571408113973461   to: 0.002163226559769859



# Switched to train mode...
Epoch: [83][  0/391]	Time  0.187 ( 0.187)	Data  0.154 ( 0.154)	Loss 4.2427e-03 (4.2427e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.032 ( 0.047)	Data  0.001 ( 0.016)	Loss 3.7464e-03 (7.3262e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.8423e-02 (8.4755e-03)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.007)	Loss 7.5938e-03 (7.6731e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 6.7205e-03 (7.2071e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.005)	Loss 6.1959e-03 (7.0469e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.004)	Loss 5.9040e-03 (6.9487e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.1383e-03 (7.4371e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.6485e-03 (7.4478e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.1323e-02 (7.4298e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.003)	Loss 1.5235e-02 (7.4128e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4874e-02 (7.2787e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.3821e-03 (7.2372e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.032 ( 0.034)	Data  0.003 ( 0.003)	Loss 7.0004e-03 (7.1745e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1182e-02 (7.0951e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4988e-03 (7.1175e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.7948e-03 (7.0949e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.0133e-03 (7.2118e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.4375e-03 (7.2540e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.9650e-03 (7.2303e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.0785e-03 (7.1776e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.6102e-03 (7.1091e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 9.1097e-03 (7.1189e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.4316e-03 (7.0209e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.1442e-02 (7.1489e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8916e-03 (7.1092e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.2802e-03 (7.0346e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2563e-03 (6.9986e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.6001e-03 (6.9615e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7472e-03 (6.9740e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.7330e-03 (6.9121e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7156e-03 (7.0016e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2526e-03 (6.9755e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.7232e-03 (6.9498e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 1.4845e-02 (7.0002e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.040 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.5992e-03 (6.9633e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 7.8966e-03 (6.9741e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.3609e-03 (6.9598e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.7368e-03 (6.9935e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.2895e-03 (7.0247e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.03116321563720703
## e[83]       loss.backward (sum) time: 1.5755774974822998
## e[83]      optimizer.step (sum) time: 0.17374849319458008
## epoch[83] training(only) time: 13.227864027023315
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2447e+00 (1.2447e+00)	Acc@1  76.00 ( 76.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.2695e+00 (1.2383e+00)	Acc@1  78.00 ( 75.36)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1920e+00 (1.1813e+00)	Acc@1  77.00 ( 75.90)	Acc@5  89.00 ( 91.81)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6757e+00 (1.2189e+00)	Acc@1  67.00 ( 74.90)	Acc@5  91.00 ( 91.74)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3334e+00 (1.2189e+00)	Acc@1  73.00 ( 74.68)	Acc@5  92.00 ( 91.95)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5031e+00 (1.2389e+00)	Acc@1  71.00 ( 74.41)	Acc@5  90.00 ( 91.67)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.2917e+00 (1.2174e+00)	Acc@1  70.00 ( 74.56)	Acc@5  93.00 ( 92.00)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5851e+00 (1.2217e+00)	Acc@1  70.00 ( 74.25)	Acc@5  88.00 ( 91.99)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5662e+00 (1.2274e+00)	Acc@1  71.00 ( 74.27)	Acc@5  86.00 ( 92.00)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3585e+00 (1.2086e+00)	Acc@1  73.00 ( 74.59)	Acc@5  90.00 ( 92.14)
 * Acc@1 74.680 Acc@5 92.320
### epoch[83] execution time: 15.702592372894287
EPOCH 84
REMOVING: module.conv5_x.1.residual_function.0.weight
i:   0, name: module.conv5_x.1.residual_function.1.weight  changing lr from: 0.001037771400183428   to: 0.001000055502803187
i:   1, name: module.conv5_x.1.residual_function.1.bias  changing lr from: 0.001120327816031696   to: 0.001026202905878170
i:   2, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.001247351462732520   to: 0.001098746737054056
i:   3, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.001416973659163100   to: 0.001215806807926261
i:   4, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.001627356919073196   to: 0.001375530330467838
i:   5, name:               module.fc.weight  changing lr from: 0.001876697321859154   to: 0.001576094661482857
i:   6, name:                 module.fc.bias  changing lr from: 0.002163226559769859   to: 0.001815709700016942



# Switched to train mode...
Epoch: [84][  0/391]	Time  0.183 ( 0.183)	Data  0.150 ( 0.150)	Loss 8.7027e-03 (8.7027e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 3.3737e-03 (6.7027e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 7.0833e-03 (6.3844e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.006)	Loss 4.1070e-03 (5.8967e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.005)	Loss 2.3030e-02 (6.3318e-03)	Acc@1  99.22 ( 99.98)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 5.8215e-03 (7.1382e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.9489e-03 (7.4067e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.0659e-02 (7.4156e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.0406e-02 (7.2882e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.003)	Loss 6.5033e-03 (7.5968e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9084e-03 (7.6094e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6577e-03 (7.4882e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.8499e-03 (7.3229e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0609e-02 (7.3287e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.003)	Loss 4.1198e-03 (7.2064e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0842e-02 (7.2061e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.8068e-03 (7.1335e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.4826e-03 (7.1255e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.9059e-03 (7.0974e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9372e-03 (7.1782e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.003)	Loss 4.9043e-03 (7.1207e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 5.0039e-03 (7.1149e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 6.7130e-03 (7.1379e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.8351e-03 (7.0986e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0673e-03 (7.1165e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.5423e-03 (7.0395e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.8052e-03 (7.0510e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.4343e-03 (7.0166e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.5730e-03 (7.0302e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.7945e-03 (7.0238e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.7273e-03 (7.0311e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.5499e-03 (6.9947e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3528e-02 (7.0039e-03)	Acc@1  99.22 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.8890e-03 (6.9992e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.5226e-03 (6.9511e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.6609e-03 (6.8983e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.4418e-03 (6.9568e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.3211e-03 (6.9017e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.2714e-03 (6.9068e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.9317e-03 (6.8946e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.028386354446411133
## e[84]       loss.backward (sum) time: 1.57138991355896
## e[84]      optimizer.step (sum) time: 0.15627145767211914
## epoch[84] training(only) time: 13.099812746047974
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.2312e+00 (1.2312e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.2576e+00 (1.2324e+00)	Acc@1  79.00 ( 74.91)	Acc@5  92.00 ( 91.45)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1228e+00 (1.1725e+00)	Acc@1  78.00 ( 75.62)	Acc@5  91.00 ( 91.86)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.6656e+00 (1.2102e+00)	Acc@1  68.00 ( 74.90)	Acc@5  91.00 ( 91.71)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3171e+00 (1.2121e+00)	Acc@1  71.00 ( 74.44)	Acc@5  92.00 ( 91.93)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4795e+00 (1.2284e+00)	Acc@1  71.00 ( 74.24)	Acc@5  89.00 ( 91.65)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2861e+00 (1.2075e+00)	Acc@1  68.00 ( 74.31)	Acc@5  93.00 ( 91.90)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5593e+00 (1.2124e+00)	Acc@1  71.00 ( 74.11)	Acc@5  89.00 ( 91.92)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5484e+00 (1.2192e+00)	Acc@1  71.00 ( 74.20)	Acc@5  86.00 ( 91.96)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3463e+00 (1.2000e+00)	Acc@1  74.00 ( 74.55)	Acc@5  90.00 ( 92.11)
 * Acc@1 74.650 Acc@5 92.300
### epoch[84] execution time: 15.577209234237671
EPOCH 85
REMOVING: module.conv5_x.1.residual_function.1.weight
REMOVING: module.conv5_x.1.residual_function.1.bias
i:   0, name: module.conv5_x.1.residual_function.3.weight  changing lr from: 0.001098746737054056   to: 0.001017168842713411
i:   1, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.001215806807926261   to: 0.001080209193966202
i:   2, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.001375530330467838   to: 0.001187795733487722
i:   3, name:               module.fc.weight  changing lr from: 0.001576094661482857   to: 0.001338091636695698
i:   4, name:                 module.fc.bias  changing lr from: 0.001815709700016942   to: 0.001529289217699247



# Switched to train mode...
Epoch: [85][  0/391]	Time  0.187 ( 0.187)	Data  0.154 ( 0.154)	Loss 2.2525e-03 (2.2525e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.1764e-02 (6.9122e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 4.2853e-03 (6.5900e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.007)	Loss 5.1302e-03 (6.4692e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 7.6047e-03 (6.5162e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.7175e-03 (6.3629e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.7936e-03 (7.0141e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.5509e-03 (6.9362e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.1800e-03 (6.8142e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.4902e-03 (6.8011e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.7915e-03 (6.8771e-03)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6263e-03 (6.8604e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5270e-02 (6.8772e-03)	Acc@1  99.22 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.2275e-03 (6.9130e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.8849e-03 (6.8705e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.6465e-03 (6.8068e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.4301e-03 (6.7548e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.9611e-03 (6.7156e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1762e-02 (6.7105e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6291e-02 (6.7050e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.2927e-03 (6.8026e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.7958e-03 (6.8121e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.3467e-03 (6.9085e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.2363e-03 (6.8726e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7809e-02 (6.8864e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.7415e-03 (6.8871e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.6926e-03 (6.8322e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.037 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.9880e-03 (6.8958e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1606e-02 (6.9814e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.2501e-03 (6.9741e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2483e-03 (6.9851e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1259e-02 (6.9679e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.4169e-03 (6.9486e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.5231e-03 (6.9320e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.7464e-03 (6.9192e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 5.7429e-03 (6.8694e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.9311e-03 (6.8914e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1476e-02 (6.9311e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.7298e-03 (6.9099e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.9765e-03 (6.8978e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.022638797760009766
## e[85]       loss.backward (sum) time: 1.4926927089691162
## e[85]      optimizer.step (sum) time: 0.11513400077819824
## epoch[85] training(only) time: 13.043480634689331
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2438e+00 (1.2438e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2786e+00 (1.2348e+00)	Acc@1  75.00 ( 75.09)	Acc@5  93.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1703e+00 (1.1784e+00)	Acc@1  78.00 ( 75.67)	Acc@5  91.00 ( 91.71)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.7015e+00 (1.2146e+00)	Acc@1  66.00 ( 74.97)	Acc@5  91.00 ( 91.55)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3355e+00 (1.2161e+00)	Acc@1  71.00 ( 74.54)	Acc@5  92.00 ( 91.78)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5274e+00 (1.2357e+00)	Acc@1  71.00 ( 74.20)	Acc@5  90.00 ( 91.55)
Test: [ 60/100]	Time  0.027 ( 0.025)	Loss 1.2830e+00 (1.2144e+00)	Acc@1  69.00 ( 74.30)	Acc@5  93.00 ( 91.89)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5986e+00 (1.2205e+00)	Acc@1  71.00 ( 74.00)	Acc@5  89.00 ( 91.86)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5609e+00 (1.2270e+00)	Acc@1  72.00 ( 74.06)	Acc@5  86.00 ( 91.91)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3535e+00 (1.2075e+00)	Acc@1  73.00 ( 74.41)	Acc@5  90.00 ( 92.09)
 * Acc@1 74.530 Acc@5 92.250
### epoch[85] execution time: 15.525146007537842
EPOCH 86
REMOVING: module.conv5_x.1.residual_function.3.weight
i:   0, name: module.conv5_x.1.residual_function.4.weight  changing lr from: 0.001080209193966202   to: 0.001010359408750493
i:   1, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.001187795733487722   to: 0.001064395594889627
i:   2, name:               module.fc.weight  changing lr from: 0.001338091636695698   to: 0.001162989698840271
i:   3, name:                 module.fc.bias  changing lr from: 0.001529289217699247   to: 0.001304320902313499



# Switched to train mode...
Epoch: [86][  0/391]	Time  0.181 ( 0.181)	Data  0.149 ( 0.149)	Loss 8.9380e-03 (8.9380e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 4.2081e-03 (7.1474e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.0820e-02 (7.5436e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.007)	Loss 5.7756e-03 (7.5113e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 5.3032e-03 (7.2589e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 5.7030e-03 (6.9080e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.3683e-03 (6.6592e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.6298e-03 (6.6080e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.0240e-02 (6.6728e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.1008e-03 (6.7483e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.1276e-03 (6.6696e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.4300e-03 (6.6547e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.8766e-03 (6.5738e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.6372e-03 (6.4976e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.6093e-03 (6.6985e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0002e-02 (6.7018e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.3936e-03 (6.8206e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.4416e-03 (6.7979e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.9986e-03 (6.7937e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.5852e-03 (6.8279e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0872e-02 (6.8855e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5434e-02 (6.9824e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.3536e-03 (7.0340e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.4948e-03 (7.0146e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.7995e-03 (7.0229e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.8954e-03 (7.0017e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.8276e-03 (7.0070e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.9901e-03 (6.9703e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.8738e-03 (6.9372e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.5875e-03 (6.9213e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3057e-03 (6.8956e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.5750e-03 (6.8639e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.1246e-03 (6.8390e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.0704e-03 (6.8000e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 5.9933e-03 (6.7981e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3786e-02 (6.7985e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.1814e-03 (6.7645e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.1782e-02 (6.7656e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3592e-03 (6.7627e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0061e-02 (6.7941e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.019039154052734375
## e[86]       loss.backward (sum) time: 1.488996982574463
## e[86]      optimizer.step (sum) time: 0.09646201133728027
## epoch[86] training(only) time: 12.860473871231079
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.2577e+00 (1.2577e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2569e+00 (1.2404e+00)	Acc@1  78.00 ( 75.18)	Acc@5  92.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.1547e+00 (1.1798e+00)	Acc@1  76.00 ( 75.71)	Acc@5  91.00 ( 91.95)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6581e+00 (1.2148e+00)	Acc@1  68.00 ( 75.03)	Acc@5  90.00 ( 91.87)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3210e+00 (1.2151e+00)	Acc@1  72.00 ( 74.68)	Acc@5  92.00 ( 92.12)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.4751e+00 (1.2339e+00)	Acc@1  71.00 ( 74.47)	Acc@5  90.00 ( 91.82)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.2652e+00 (1.2113e+00)	Acc@1  70.00 ( 74.61)	Acc@5  93.00 ( 92.10)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5500e+00 (1.2158e+00)	Acc@1  72.00 ( 74.34)	Acc@5  90.00 ( 92.08)
Test: [ 80/100]	Time  0.022 ( 0.024)	Loss 1.5616e+00 (1.2220e+00)	Acc@1  69.00 ( 74.30)	Acc@5  87.00 ( 92.14)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3228e+00 (1.2020e+00)	Acc@1  73.00 ( 74.63)	Acc@5  92.00 ( 92.31)
 * Acc@1 74.650 Acc@5 92.470
### epoch[86] execution time: 15.334862470626831
EPOCH 87
REMOVING: module.conv5_x.1.residual_function.4.weight
i:   0, name: module.conv5_x.1.residual_function.4.bias  changing lr from: 0.001064395594889627   to: 0.001005489290898669
i:   1, name:               module.fc.weight  changing lr from: 0.001162989698840271   to: 0.001051010629611614
i:   2, name:                 module.fc.bias  changing lr from: 0.001304320902313499   to: 0.001141084207909431



# Switched to train mode...
Epoch: [87][  0/391]	Time  0.171 ( 0.171)	Data  0.130 ( 0.130)	Loss 1.1182e-02 (1.1182e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.013)	Loss 5.2309e-03 (6.7723e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.008)	Loss 4.7899e-03 (6.4680e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 3.0102e-03 (6.8002e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.032 ( 0.036)	Data  0.002 ( 0.005)	Loss 1.0472e-02 (6.9689e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.7526e-03 (6.7739e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.7910e-03 (6.6899e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.1722e-03 (6.6215e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.8860e-03 (6.5528e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.8557e-03 (6.5842e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.2948e-03 (6.6091e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0872e-03 (6.6788e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.1610e-03 (6.6109e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6729e-03 (6.7112e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.7090e-03 (6.7446e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5607e-03 (6.6883e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6229e-03 (6.7090e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.0008e-03 (6.7163e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0401e-03 (6.8568e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.9670e-03 (6.7386e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.1895e-03 (6.7870e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.4038e-03 (6.8012e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.8838e-03 (6.7770e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.3249e-03 (6.7328e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.0206e-03 (6.6927e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.9813e-03 (6.6548e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3913e-03 (6.6109e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.5610e-03 (6.6177e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.0000e-03 (6.6284e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 5.9291e-03 (6.6473e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 4.5559e-03 (6.6560e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.0624e-03 (6.6185e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.8958e-03 (6.5745e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.7362e-03 (6.5393e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.8294e-03 (6.5823e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.0620e-03 (6.5782e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.1392e-03 (6.5659e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3922e-03 (6.6678e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.1245e-03 (6.6662e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1967e-02 (6.6951e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.016662120819091797
## e[87]       loss.backward (sum) time: 1.5202794075012207
## e[87]      optimizer.step (sum) time: 0.07622051239013672
## epoch[87] training(only) time: 12.817763805389404
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.2514e+00 (1.2514e+00)	Acc@1  76.00 ( 76.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.035)	Loss 1.2512e+00 (1.2349e+00)	Acc@1  78.00 ( 74.64)	Acc@5  93.00 ( 91.36)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1878e+00 (1.1740e+00)	Acc@1  77.00 ( 75.52)	Acc@5  91.00 ( 92.00)
Test: [ 30/100]	Time  0.023 ( 0.027)	Loss 1.7121e+00 (1.2108e+00)	Acc@1  66.00 ( 74.61)	Acc@5  91.00 ( 91.87)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3222e+00 (1.2121e+00)	Acc@1  71.00 ( 74.34)	Acc@5  92.00 ( 92.02)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5200e+00 (1.2313e+00)	Acc@1  72.00 ( 74.25)	Acc@5  90.00 ( 91.75)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.3028e+00 (1.2110e+00)	Acc@1  69.00 ( 74.36)	Acc@5  93.00 ( 92.02)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5747e+00 (1.2159e+00)	Acc@1  71.00 ( 74.06)	Acc@5  89.00 ( 91.99)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5681e+00 (1.2223e+00)	Acc@1  71.00 ( 74.09)	Acc@5  85.00 ( 91.99)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3426e+00 (1.2034e+00)	Acc@1  73.00 ( 74.42)	Acc@5  90.00 ( 92.12)
 * Acc@1 74.500 Acc@5 92.300
### epoch[87] execution time: 15.292452335357666
EPOCH 88
REMOVING: module.conv5_x.1.residual_function.4.bias
i:   0, name:               module.fc.weight  changing lr from: 0.001051010629611614   to: 0.001002296260151541
i:   1, name:                 module.fc.bias  changing lr from: 0.001141084207909431   to: 0.001039781905953951



# Switched to train mode...
Epoch: [88][  0/391]	Time  0.180 ( 0.180)	Data  0.149 ( 0.149)	Loss 4.2168e-03 (4.2168e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.031 ( 0.045)	Data  0.001 ( 0.015)	Loss 9.7895e-03 (7.0956e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 8.5159e-03 (7.5460e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.006)	Loss 8.1630e-03 (6.8252e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.005)	Loss 2.3793e-02 (7.1658e-03)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 8.3143e-03 (7.0268e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 6.2198e-03 (7.1469e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.9676e-03 (6.9514e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 8.1867e-03 (6.9789e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.4718e-02 (6.9051e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.4733e-03 (7.0298e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9562e-03 (7.0945e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8970e-03 (7.1053e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9628e-03 (7.2025e-03)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.5528e-03 (7.2403e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.9492e-03 (7.1394e-03)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.2331e-03 (7.0994e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7954e-03 (7.0552e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7994e-03 (7.0004e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.5222e-03 (6.9572e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 7.2775e-03 (6.9308e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2347e-03 (6.8647e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.2558e-03 (6.7872e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.7820e-03 (6.7484e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 6.1592e-03 (6.7397e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 6.9994e-03 (6.7162e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.029 ( 0.032)	Data  0.001 ( 0.002)	Loss 7.0988e-03 (6.7664e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.032 ( 0.032)	Data  0.002 ( 0.002)	Loss 4.7045e-03 (6.7958e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 6.8812e-03 (6.7487e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 6.4975e-03 (6.7995e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 1.3595e-02 (6.8820e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 7.6748e-03 (6.9220e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 5.8416e-03 (6.9205e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 5.8896e-03 (6.9143e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 6.9128e-03 (6.9354e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 1.0786e-02 (6.9204e-03)	Acc@1  99.22 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 3.2530e-03 (6.9045e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 6.4199e-03 (6.9157e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 1.3797e-02 (6.9510e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 7.1547e-03 (6.9874e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.013689756393432617
## e[88]       loss.backward (sum) time: 1.4163460731506348
## e[88]      optimizer.step (sum) time: 0.05745291709899902
## epoch[88] training(only) time: 12.709178686141968
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.2455e+00 (1.2455e+00)	Acc@1  76.00 ( 76.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.034)	Loss 1.2689e+00 (1.2268e+00)	Acc@1  76.00 ( 75.27)	Acc@5  93.00 ( 91.64)
Test: [ 20/100]	Time  0.022 ( 0.028)	Loss 1.1495e+00 (1.1717e+00)	Acc@1  78.00 ( 76.05)	Acc@5  91.00 ( 92.19)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6443e+00 (1.2070e+00)	Acc@1  66.00 ( 75.03)	Acc@5  91.00 ( 92.06)
Test: [ 40/100]	Time  0.023 ( 0.026)	Loss 1.3424e+00 (1.2099e+00)	Acc@1  73.00 ( 74.68)	Acc@5  92.00 ( 92.17)
Test: [ 50/100]	Time  0.022 ( 0.025)	Loss 1.5170e+00 (1.2302e+00)	Acc@1  70.00 ( 74.39)	Acc@5  90.00 ( 91.88)
Test: [ 60/100]	Time  0.023 ( 0.025)	Loss 1.2843e+00 (1.2095e+00)	Acc@1  69.00 ( 74.52)	Acc@5  93.00 ( 92.13)
Test: [ 70/100]	Time  0.023 ( 0.024)	Loss 1.5521e+00 (1.2138e+00)	Acc@1  71.00 ( 74.30)	Acc@5  89.00 ( 92.10)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5553e+00 (1.2199e+00)	Acc@1  72.00 ( 74.36)	Acc@5  86.00 ( 92.12)
Test: [ 90/100]	Time  0.022 ( 0.024)	Loss 1.3727e+00 (1.2012e+00)	Acc@1  74.00 ( 74.69)	Acc@5  90.00 ( 92.24)
 * Acc@1 74.780 Acc@5 92.390
### epoch[88] execution time: 15.18127989768982
EPOCH 89
i:   0, name:               module.fc.weight  changing lr from: 0.001002296260151541   to: 0.001016908291407291
i:   1, name:                 module.fc.bias  changing lr from: 0.001039781905953951   to: 0.001000539833449836



# Switched to train mode...
Epoch: [89][  0/391]	Time  0.183 ( 0.183)	Data  0.152 ( 0.152)	Loss 5.9103e-03 (5.9103e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.031 ( 0.045)	Data  0.001 ( 0.015)	Loss 3.3782e-03 (5.3120e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 2.7890e-03 (5.7602e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.007)	Loss 3.6469e-03 (6.1663e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.005)	Loss 7.2109e-03 (6.3850e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 5.0931e-03 (6.3370e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 7.1632e-03 (6.3567e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 9.3248e-03 (6.4530e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.004)	Loss 1.7787e-02 (6.8506e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.3450e-03 (6.7712e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.2385e-03 (6.7630e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.6752e-03 (6.7720e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.4978e-03 (6.7748e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 5.0537e-03 (6.7510e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.1373e-03 (6.7094e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.3703e-03 (6.6843e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.1781e-03 (6.7589e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0513e-02 (6.8103e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2225e-02 (6.7503e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3977e-02 (6.7175e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.003)	Loss 9.4422e-03 (6.6790e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 5.2074e-03 (6.6352e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 1.8465e-02 (6.6656e-03)	Acc@1  99.22 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.032 ( 0.032)	Data  0.002 ( 0.002)	Loss 4.5020e-03 (6.7731e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 5.5202e-03 (6.7892e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 7.5245e-03 (6.7971e-03)	Acc@1 100.00 ( 99.97)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 4.2403e-03 (6.7855e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 6.6176e-03 (6.7938e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 4.3721e-03 (6.7840e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.032 ( 0.032)	Data  0.002 ( 0.002)	Loss 7.1384e-03 (6.8239e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 6.0480e-03 (6.8036e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 4.5899e-03 (6.7812e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 8.7216e-03 (6.8566e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 7.0764e-03 (6.8872e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 5.8524e-03 (6.8888e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 6.2494e-03 (6.8983e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 9.2929e-03 (6.9507e-03)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.032 ( 0.032)	Data  0.001 ( 0.002)	Loss 1.3419e-02 (6.9536e-03)	Acc@1  99.22 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.002)	Loss 1.2957e-02 (6.9546e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.030 ( 0.032)	Data  0.001 ( 0.002)	Loss 9.6312e-03 (6.9870e-03)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.01444864273071289
## e[89]       loss.backward (sum) time: 1.3966846466064453
## e[89]      optimizer.step (sum) time: 0.05778694152832031
## epoch[89] training(only) time: 12.67545771598816
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.2594e+00 (1.2594e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.023 ( 0.034)	Loss 1.2721e+00 (1.2313e+00)	Acc@1  76.00 ( 74.82)	Acc@5  92.00 ( 91.18)
Test: [ 20/100]	Time  0.022 ( 0.029)	Loss 1.1605e+00 (1.1745e+00)	Acc@1  76.00 ( 75.57)	Acc@5  91.00 ( 91.90)
Test: [ 30/100]	Time  0.022 ( 0.027)	Loss 1.6772e+00 (1.2095e+00)	Acc@1  69.00 ( 74.94)	Acc@5  92.00 ( 91.84)
Test: [ 40/100]	Time  0.022 ( 0.026)	Loss 1.3113e+00 (1.2115e+00)	Acc@1  71.00 ( 74.49)	Acc@5  92.00 ( 92.02)
Test: [ 50/100]	Time  0.023 ( 0.025)	Loss 1.5262e+00 (1.2310e+00)	Acc@1  72.00 ( 74.37)	Acc@5  89.00 ( 91.75)
Test: [ 60/100]	Time  0.022 ( 0.025)	Loss 1.2871e+00 (1.2106e+00)	Acc@1  69.00 ( 74.46)	Acc@5  93.00 ( 92.03)
Test: [ 70/100]	Time  0.022 ( 0.024)	Loss 1.5254e+00 (1.2157e+00)	Acc@1  71.00 ( 74.24)	Acc@5  90.00 ( 92.04)
Test: [ 80/100]	Time  0.023 ( 0.024)	Loss 1.5142e+00 (1.2216e+00)	Acc@1  70.00 ( 74.21)	Acc@5  86.00 ( 92.04)
Test: [ 90/100]	Time  0.023 ( 0.024)	Loss 1.3710e+00 (1.2028e+00)	Acc@1  74.00 ( 74.57)	Acc@5  90.00 ( 92.18)
 * Acc@1 74.670 Acc@5 92.320
### epoch[89] execution time: 15.171974420547485
### Training complete:
*** Model named parameters and requires_grad:
name:          module.conv1.0.weight  req_grad: False 
name:          module.conv1.1.weight  req_grad: False 
name:            module.conv1.1.bias  req_grad: False 
name: module.conv2_x.0.residual_function.0.weight  req_grad: False 
name: module.conv2_x.0.residual_function.1.weight  req_grad: False 
name: module.conv2_x.0.residual_function.1.bias  req_grad: False 
name: module.conv2_x.0.residual_function.3.weight  req_grad: False 
name: module.conv2_x.0.residual_function.4.weight  req_grad: False 
name: module.conv2_x.0.residual_function.4.bias  req_grad: False 
name: module.conv2_x.1.residual_function.0.weight  req_grad: False 
name: module.conv2_x.1.residual_function.1.weight  req_grad: False 
name: module.conv2_x.1.residual_function.1.bias  req_grad: False 
name: module.conv2_x.1.residual_function.3.weight  req_grad: False 
name: module.conv2_x.1.residual_function.4.weight  req_grad: False 
name: module.conv2_x.1.residual_function.4.bias  req_grad: False 
name: module.conv3_x.0.residual_function.0.weight  req_grad: False 
name: module.conv3_x.0.residual_function.1.weight  req_grad: False 
name: module.conv3_x.0.residual_function.1.bias  req_grad: False 
name: module.conv3_x.0.residual_function.3.weight  req_grad: False 
name: module.conv3_x.0.residual_function.4.weight  req_grad: False 
name: module.conv3_x.0.residual_function.4.bias  req_grad: False 
name: module.conv3_x.0.shortcut.0.weight  req_grad: False 
name: module.conv3_x.0.shortcut.1.weight  req_grad: False 
name: module.conv3_x.0.shortcut.1.bias  req_grad: False 
name: module.conv3_x.1.residual_function.0.weight  req_grad: False 
name: module.conv3_x.1.residual_function.1.weight  req_grad: False 
name: module.conv3_x.1.residual_function.1.bias  req_grad: False 
name: module.conv3_x.1.residual_function.3.weight  req_grad: False 
name: module.conv3_x.1.residual_function.4.weight  req_grad: False 
name: module.conv3_x.1.residual_function.4.bias  req_grad: False 
name: module.conv4_x.0.residual_function.0.weight  req_grad: False 
name: module.conv4_x.0.residual_function.1.weight  req_grad: False 
name: module.conv4_x.0.residual_function.1.bias  req_grad: False 
name: module.conv4_x.0.residual_function.3.weight  req_grad: False 
name: module.conv4_x.0.residual_function.4.weight  req_grad: False 
name: module.conv4_x.0.residual_function.4.bias  req_grad: False 
name: module.conv4_x.0.shortcut.0.weight  req_grad: False 
name: module.conv4_x.0.shortcut.1.weight  req_grad: False 
name: module.conv4_x.0.shortcut.1.bias  req_grad: False 
name: module.conv4_x.1.residual_function.0.weight  req_grad: False 
name: module.conv4_x.1.residual_function.1.weight  req_grad: False 
name: module.conv4_x.1.residual_function.1.bias  req_grad: False 
name: module.conv4_x.1.residual_function.3.weight  req_grad: False 
name: module.conv4_x.1.residual_function.4.weight  req_grad: False 
name: module.conv4_x.1.residual_function.4.bias  req_grad: False 
name: module.conv5_x.0.residual_function.0.weight  req_grad: False 
name: module.conv5_x.0.residual_function.1.weight  req_grad: False 
name: module.conv5_x.0.residual_function.1.bias  req_grad: False 
name: module.conv5_x.0.residual_function.3.weight  req_grad: False 
name: module.conv5_x.0.residual_function.4.weight  req_grad: False 
name: module.conv5_x.0.residual_function.4.bias  req_grad: False 
name: module.conv5_x.0.shortcut.0.weight  req_grad: False 
name: module.conv5_x.0.shortcut.1.weight  req_grad: False 
name: module.conv5_x.0.shortcut.1.bias  req_grad: False 
name: module.conv5_x.1.residual_function.0.weight  req_grad: False 
name: module.conv5_x.1.residual_function.1.weight  req_grad: False 
name: module.conv5_x.1.residual_function.1.bias  req_grad: False 
name: module.conv5_x.1.residual_function.3.weight  req_grad: False 
name: module.conv5_x.1.residual_function.4.weight  req_grad: False 
name: module.conv5_x.1.residual_function.4.bias  req_grad: False 
name:               module.fc.weight  req_grad:  True 
name:                 module.fc.bias  req_grad:  True 


*** Optimizer groups, parameters and req_grads
#        requires_grad:                            True
#           param_name:                module.fc.weight
#                   lr:           0.0010169082914072915
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:                  module.fc.bias
#                   lr:           0.0010005398334498364
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False



*** Optimizer group lrs
# group:  0,   name:               module.fc.weight,   req_grad:   True   lr: 0.001016908291407291,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  1,   name:                 module.fc.bias,   req_grad:   True   lr: 0.001000539833449836,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
---------------
#### total training(only) time: 1511.288922548294
##### Total run time: 1739.549327135086
