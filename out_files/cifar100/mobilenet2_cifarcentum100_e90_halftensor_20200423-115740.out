# Model: mobilenet2
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.mobilenet
<function mobilenet2 at 0x7fae6a341f28>
# model requested: 'mobilenet2'
# printing out the model
MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
      (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
# model is low precision
# Model: mobilenet2
# Dataset: cifarcentum
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.365 ( 3.365)	Data  0.113 ( 0.113)	Loss 4.6406e+00 (4.6406e+00)	Acc@1   0.00 (  0.00)	Acc@5   2.34 (  2.34)
Epoch: [0][ 10/391]	Time  0.068 ( 0.368)	Data  0.001 ( 0.011)	Loss 4.8750e+00 (4.7614e+00)	Acc@1   0.78 (  0.92)	Acc@5   4.69 (  4.40)
Epoch: [0][ 20/391]	Time  0.062 ( 0.224)	Data  0.001 ( 0.006)	Loss 4.9180e+00 (4.8151e+00)	Acc@1   0.00 (  1.08)	Acc@5   5.47 (  6.25)
Epoch: [0][ 30/391]	Time  0.068 ( 0.173)	Data  0.001 ( 0.005)	Loss 5.0547e+00 (4.8153e+00)	Acc@1   1.56 (  1.54)	Acc@5   6.25 (  7.71)
Epoch: [0][ 40/391]	Time  0.068 ( 0.147)	Data  0.001 ( 0.004)	Loss 4.6094e+00 (4.7749e+00)	Acc@1   5.47 (  1.98)	Acc@5  14.06 (  9.13)
Epoch: [0][ 50/391]	Time  0.068 ( 0.131)	Data  0.001 ( 0.003)	Loss 4.3555e+00 (4.7170e+00)	Acc@1   3.12 (  2.30)	Acc@5  16.41 ( 10.22)
Epoch: [0][ 60/391]	Time  0.063 ( 0.120)	Data  0.001 ( 0.003)	Loss 4.2734e+00 (4.6591e+00)	Acc@1  10.16 (  2.60)	Acc@5  23.44 ( 11.18)
Epoch: [0][ 70/391]	Time  0.065 ( 0.113)	Data  0.001 ( 0.003)	Loss 4.1680e+00 (4.6033e+00)	Acc@1   6.25 (  2.75)	Acc@5  18.75 ( 12.04)
Epoch: [0][ 80/391]	Time  0.068 ( 0.107)	Data  0.001 ( 0.002)	Loss 4.0781e+00 (4.5502e+00)	Acc@1   8.59 (  3.12)	Acc@5  21.09 ( 12.90)
Epoch: [0][ 90/391]	Time  0.067 ( 0.102)	Data  0.001 ( 0.002)	Loss 3.9707e+00 (4.5048e+00)	Acc@1  10.16 (  3.42)	Acc@5  21.88 ( 13.68)
Epoch: [0][100/391]	Time  0.068 ( 0.099)	Data  0.001 ( 0.002)	Loss 4.2461e+00 (4.4734e+00)	Acc@1   4.69 (  3.67)	Acc@5  21.88 ( 14.34)
Epoch: [0][110/391]	Time  0.064 ( 0.096)	Data  0.001 ( 0.002)	Loss 4.1016e+00 (4.4435e+00)	Acc@1   3.91 (  3.82)	Acc@5  26.56 ( 14.91)
Epoch: [0][120/391]	Time  0.068 ( 0.093)	Data  0.001 ( 0.002)	Loss 3.9531e+00 (4.4120e+00)	Acc@1   7.81 (  3.98)	Acc@5  28.91 ( 15.73)
Epoch: [0][130/391]	Time  0.066 ( 0.091)	Data  0.001 ( 0.002)	Loss 3.8398e+00 (4.3856e+00)	Acc@1   9.38 (  4.17)	Acc@5  28.12 ( 16.34)
Epoch: [0][140/391]	Time  0.069 ( 0.090)	Data  0.001 ( 0.002)	Loss 3.9609e+00 (4.3637e+00)	Acc@1  10.94 (  4.38)	Acc@5  25.00 ( 16.86)
Epoch: [0][150/391]	Time  0.068 ( 0.088)	Data  0.001 ( 0.002)	Loss 3.9551e+00 (4.3392e+00)	Acc@1   8.59 (  4.55)	Acc@5  27.34 ( 17.48)
Epoch: [0][160/391]	Time  0.065 ( 0.087)	Data  0.001 ( 0.002)	Loss 3.9727e+00 (4.3177e+00)	Acc@1   8.59 (  4.68)	Acc@5  28.91 ( 18.03)
Epoch: [0][170/391]	Time  0.066 ( 0.086)	Data  0.001 ( 0.002)	Loss 4.1719e+00 (4.3002e+00)	Acc@1   5.47 (  4.80)	Acc@5  17.97 ( 18.40)
Epoch: [0][180/391]	Time  0.070 ( 0.084)	Data  0.001 ( 0.002)	Loss 3.8652e+00 (4.2801e+00)	Acc@1   3.91 (  4.92)	Acc@5  25.00 ( 18.85)
Epoch: [0][190/391]	Time  0.067 ( 0.084)	Data  0.001 ( 0.002)	Loss 3.9414e+00 (4.2639e+00)	Acc@1   7.03 (  5.01)	Acc@5  32.03 ( 19.28)
Epoch: [0][200/391]	Time  0.066 ( 0.083)	Data  0.001 ( 0.002)	Loss 3.9238e+00 (4.2488e+00)	Acc@1  12.50 (  5.15)	Acc@5  28.91 ( 19.70)
Epoch: [0][210/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.002)	Loss 3.9590e+00 (4.2374e+00)	Acc@1   3.91 (  5.24)	Acc@5  29.69 ( 19.99)
Epoch: [0][220/391]	Time  0.068 ( 0.081)	Data  0.001 ( 0.002)	Loss 3.8105e+00 (4.2238e+00)	Acc@1   4.69 (  5.36)	Acc@5  28.12 ( 20.33)
Epoch: [0][230/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.002)	Loss 3.7578e+00 (4.2080e+00)	Acc@1   5.47 (  5.50)	Acc@5  32.81 ( 20.71)
Epoch: [0][240/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.002)	Loss 3.9238e+00 (4.1977e+00)	Acc@1  10.16 (  5.58)	Acc@5  25.78 ( 20.99)
Epoch: [0][250/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.002)	Loss 3.8242e+00 (4.1838e+00)	Acc@1   7.81 (  5.72)	Acc@5  26.56 ( 21.40)
Epoch: [0][260/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.002)	Loss 3.6543e+00 (4.1716e+00)	Acc@1   9.38 (  5.86)	Acc@5  37.50 ( 21.72)
Epoch: [0][270/391]	Time  0.066 ( 0.078)	Data  0.001 ( 0.002)	Loss 3.7598e+00 (4.1575e+00)	Acc@1   9.38 (  6.06)	Acc@5  38.28 ( 22.09)
Epoch: [0][280/391]	Time  0.065 ( 0.078)	Data  0.001 ( 0.001)	Loss 3.9434e+00 (4.1483e+00)	Acc@1   9.38 (  6.14)	Acc@5  28.12 ( 22.34)
Epoch: [0][290/391]	Time  0.067 ( 0.078)	Data  0.001 ( 0.001)	Loss 3.9629e+00 (4.1389e+00)	Acc@1   9.38 (  6.31)	Acc@5  29.69 ( 22.65)
Epoch: [0][300/391]	Time  0.064 ( 0.077)	Data  0.001 ( 0.001)	Loss 3.7539e+00 (4.1278e+00)	Acc@1  11.72 (  6.44)	Acc@5  33.59 ( 22.96)
Epoch: [0][310/391]	Time  0.063 ( 0.077)	Data  0.001 ( 0.001)	Loss 3.8184e+00 (4.1183e+00)	Acc@1  10.16 (  6.56)	Acc@5  37.50 ( 23.27)
Epoch: [0][320/391]	Time  0.069 ( 0.077)	Data  0.001 ( 0.001)	Loss 3.9375e+00 (4.1088e+00)	Acc@1  10.16 (  6.66)	Acc@5  33.59 ( 23.55)
Epoch: [0][330/391]	Time  0.064 ( 0.076)	Data  0.001 ( 0.001)	Loss 3.7500e+00 (4.0984e+00)	Acc@1  11.72 (  6.77)	Acc@5  33.59 ( 23.87)
Epoch: [0][340/391]	Time  0.067 ( 0.076)	Data  0.001 ( 0.001)	Loss 3.8398e+00 (4.0878e+00)	Acc@1  10.94 (  6.89)	Acc@5  33.59 ( 24.18)
Epoch: [0][350/391]	Time  0.067 ( 0.076)	Data  0.001 ( 0.001)	Loss 3.6875e+00 (4.0781e+00)	Acc@1  11.72 (  7.02)	Acc@5  38.28 ( 24.45)
Epoch: [0][360/391]	Time  0.060 ( 0.075)	Data  0.001 ( 0.001)	Loss 3.6641e+00 (4.0674e+00)	Acc@1  12.50 (  7.13)	Acc@5  32.81 ( 24.77)
Epoch: [0][370/391]	Time  0.064 ( 0.075)	Data  0.001 ( 0.001)	Loss 3.6133e+00 (4.0577e+00)	Acc@1  10.16 (  7.25)	Acc@5  37.50 ( 25.05)
Epoch: [0][380/391]	Time  0.069 ( 0.075)	Data  0.001 ( 0.001)	Loss 3.8457e+00 (4.0499e+00)	Acc@1   7.81 (  7.33)	Acc@5  31.25 ( 25.29)
Epoch: [0][390/391]	Time  0.443 ( 0.076)	Data  0.001 ( 0.001)	Loss 3.5957e+00 (4.0415e+00)	Acc@1  11.25 (  7.40)	Acc@5  41.25 ( 25.52)
## e[0] optimizer.zero_grad (sum) time: 0.41687464714050293
## e[0]       loss.backward (sum) time: 7.958277940750122
## e[0]      optimizer.step (sum) time: 3.604220151901245
## epoch[0] training(only) time: 29.59770393371582
# Switched to evaluate mode...
Test: [  0/100]	Time  0.280 ( 0.280)	Loss 3.7383e+00 (3.7383e+00)	Acc@1  12.00 ( 12.00)	Acc@5  33.00 ( 33.00)
Test: [ 10/100]	Time  0.026 ( 0.051)	Loss 3.8184e+00 (3.7729e+00)	Acc@1  12.00 ( 12.27)	Acc@5  37.00 ( 35.27)
Test: [ 20/100]	Time  0.025 ( 0.039)	Loss 3.9395e+00 (3.7983e+00)	Acc@1   7.00 ( 11.76)	Acc@5  36.00 ( 34.76)
Test: [ 30/100]	Time  0.025 ( 0.035)	Loss 4.0742e+00 (3.8111e+00)	Acc@1   6.00 ( 10.84)	Acc@5  28.00 ( 33.90)
Test: [ 40/100]	Time  0.025 ( 0.032)	Loss 3.6289e+00 (3.7867e+00)	Acc@1  11.00 ( 11.02)	Acc@5  45.00 ( 35.17)
Test: [ 50/100]	Time  0.025 ( 0.031)	Loss 3.8184e+00 (3.7845e+00)	Acc@1  12.00 ( 11.18)	Acc@5  35.00 ( 35.20)
Test: [ 60/100]	Time  0.028 ( 0.030)	Loss 3.7871e+00 (3.7839e+00)	Acc@1  10.00 ( 11.13)	Acc@5  38.00 ( 35.02)
Test: [ 70/100]	Time  0.025 ( 0.030)	Loss 4.0469e+00 (3.7839e+00)	Acc@1   5.00 ( 11.10)	Acc@5  34.00 ( 34.94)
Test: [ 80/100]	Time  0.025 ( 0.029)	Loss 4.1523e+00 (3.7900e+00)	Acc@1   6.00 ( 10.99)	Acc@5  27.00 ( 34.84)
Test: [ 90/100]	Time  0.025 ( 0.029)	Loss 3.6055e+00 (3.7916e+00)	Acc@1  12.00 ( 10.91)	Acc@5  42.00 ( 35.01)
 * Acc@1 10.930 Acc@5 35.020
### epoch[0] execution time: 32.50755429267883
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.227 ( 0.227)	Data  0.145 ( 0.145)	Loss 3.7754e+00 (3.7754e+00)	Acc@1  14.06 ( 14.06)	Acc@5  31.25 ( 31.25)
Epoch: [1][ 10/391]	Time  0.069 ( 0.082)	Data  0.001 ( 0.014)	Loss 3.7285e+00 (3.7164e+00)	Acc@1  14.06 ( 11.79)	Acc@5  42.19 ( 34.02)
Epoch: [1][ 20/391]	Time  0.070 ( 0.075)	Data  0.001 ( 0.008)	Loss 3.6973e+00 (3.7046e+00)	Acc@1   7.81 ( 11.27)	Acc@5  36.72 ( 34.56)
Epoch: [1][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.006)	Loss 3.6660e+00 (3.7138e+00)	Acc@1  19.53 ( 11.34)	Acc@5  36.72 ( 34.85)
Epoch: [1][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 3.7617e+00 (3.7017e+00)	Acc@1   8.59 ( 11.64)	Acc@5  32.81 ( 35.48)
Epoch: [1][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 3.5391e+00 (3.6865e+00)	Acc@1  10.16 ( 11.84)	Acc@5  39.06 ( 35.86)
Epoch: [1][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.5723e+00 (3.6804e+00)	Acc@1  14.06 ( 12.01)	Acc@5  46.09 ( 36.18)
Epoch: [1][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.6699e+00 (3.6739e+00)	Acc@1  10.94 ( 12.02)	Acc@5  39.84 ( 36.40)
Epoch: [1][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.5957e+00 (3.6629e+00)	Acc@1  12.50 ( 12.08)	Acc@5  35.16 ( 36.58)
Epoch: [1][ 90/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.6113e+00 (3.6582e+00)	Acc@1  13.28 ( 12.03)	Acc@5  39.06 ( 36.70)
Epoch: [1][100/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.5352e+00 (3.6505e+00)	Acc@1  14.84 ( 12.33)	Acc@5  38.28 ( 36.74)
Epoch: [1][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.4668e+00 (3.6503e+00)	Acc@1  14.06 ( 12.18)	Acc@5  44.53 ( 36.89)
Epoch: [1][120/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6328e+00 (3.6424e+00)	Acc@1  15.62 ( 12.42)	Acc@5  35.94 ( 37.01)
Epoch: [1][130/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5293e+00 (3.6384e+00)	Acc@1  11.72 ( 12.44)	Acc@5  39.84 ( 37.21)
Epoch: [1][140/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5508e+00 (3.6382e+00)	Acc@1  18.75 ( 12.43)	Acc@5  40.62 ( 37.28)
Epoch: [1][150/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5664e+00 (3.6366e+00)	Acc@1  13.28 ( 12.48)	Acc@5  42.97 ( 37.38)
Epoch: [1][160/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4941e+00 (3.6301e+00)	Acc@1  11.72 ( 12.56)	Acc@5  42.19 ( 37.52)
Epoch: [1][170/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4102e+00 (3.6252e+00)	Acc@1  14.06 ( 12.63)	Acc@5  45.31 ( 37.71)
Epoch: [1][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5391e+00 (3.6200e+00)	Acc@1  10.94 ( 12.73)	Acc@5  37.50 ( 37.83)
Epoch: [1][190/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5352e+00 (3.6130e+00)	Acc@1  14.06 ( 12.82)	Acc@5  41.41 ( 38.00)
Epoch: [1][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4121e+00 (3.6069e+00)	Acc@1  17.19 ( 12.88)	Acc@5  44.53 ( 38.21)
Epoch: [1][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6250e+00 (3.6017e+00)	Acc@1  10.94 ( 12.93)	Acc@5  37.50 ( 38.36)
Epoch: [1][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5352e+00 (3.5978e+00)	Acc@1  12.50 ( 13.03)	Acc@5  41.41 ( 38.44)
Epoch: [1][230/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5156e+00 (3.5962e+00)	Acc@1  18.75 ( 13.17)	Acc@5  42.19 ( 38.51)
Epoch: [1][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4980e+00 (3.5892e+00)	Acc@1  17.97 ( 13.30)	Acc@5  42.19 ( 38.65)
Epoch: [1][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4805e+00 (3.5853e+00)	Acc@1  14.84 ( 13.34)	Acc@5  39.84 ( 38.80)
Epoch: [1][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7285e+00 (3.5836e+00)	Acc@1  10.16 ( 13.39)	Acc@5  37.50 ( 38.86)
Epoch: [1][270/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4082e+00 (3.5799e+00)	Acc@1  14.06 ( 13.39)	Acc@5  45.31 ( 38.95)
Epoch: [1][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5703e+00 (3.5734e+00)	Acc@1  14.06 ( 13.49)	Acc@5  39.84 ( 39.17)
Epoch: [1][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5371e+00 (3.5699e+00)	Acc@1  14.84 ( 13.51)	Acc@5  42.97 ( 39.28)
Epoch: [1][300/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4180e+00 (3.5623e+00)	Acc@1  18.75 ( 13.61)	Acc@5  39.06 ( 39.45)
Epoch: [1][310/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4980e+00 (3.5588e+00)	Acc@1  14.06 ( 13.68)	Acc@5  37.50 ( 39.53)
Epoch: [1][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.3438e+00 (3.5561e+00)	Acc@1  17.97 ( 13.75)	Acc@5  49.22 ( 39.62)
Epoch: [1][330/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6406e+00 (3.5541e+00)	Acc@1  14.06 ( 13.84)	Acc@5  35.16 ( 39.65)
Epoch: [1][340/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5664e+00 (3.5509e+00)	Acc@1  14.06 ( 13.93)	Acc@5  38.28 ( 39.81)
Epoch: [1][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.5566e+00 (3.5443e+00)	Acc@1  14.06 ( 14.07)	Acc@5  42.19 ( 40.02)
Epoch: [1][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.2500e+00 (3.5404e+00)	Acc@1  20.31 ( 14.13)	Acc@5  50.78 ( 40.11)
Epoch: [1][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.3730e+00 (3.5366e+00)	Acc@1  20.31 ( 14.22)	Acc@5  41.41 ( 40.21)
Epoch: [1][380/391]	Time  0.076 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.5312e+00 (3.5327e+00)	Acc@1  18.75 ( 14.27)	Acc@5  40.62 ( 40.34)
Epoch: [1][390/391]	Time  0.052 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.3008e+00 (3.5269e+00)	Acc@1  20.00 ( 14.38)	Acc@5  41.25 ( 40.52)
## e[1] optimizer.zero_grad (sum) time: 0.4070878028869629
## e[1]       loss.backward (sum) time: 7.3480775356292725
## e[1]      optimizer.step (sum) time: 3.6076676845550537
## epoch[1] training(only) time: 26.105072498321533
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 3.6758e+00 (3.6758e+00)	Acc@1  16.00 ( 16.00)	Acc@5  42.00 ( 42.00)
Test: [ 10/100]	Time  0.025 ( 0.038)	Loss 3.5605e+00 (3.5865e+00)	Acc@1  14.00 ( 15.09)	Acc@5  42.00 ( 43.91)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 3.4863e+00 (3.5651e+00)	Acc@1  13.00 ( 14.67)	Acc@5  43.00 ( 43.38)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 3.7266e+00 (3.5627e+00)	Acc@1  15.00 ( 14.90)	Acc@5  39.00 ( 43.10)
Test: [ 40/100]	Time  0.027 ( 0.029)	Loss 3.6543e+00 (3.5481e+00)	Acc@1  17.00 ( 15.29)	Acc@5  44.00 ( 43.15)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 3.4980e+00 (3.5565e+00)	Acc@1  12.00 ( 15.24)	Acc@5  46.00 ( 42.94)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 3.3008e+00 (3.5558e+00)	Acc@1  15.00 ( 15.05)	Acc@5  50.00 ( 42.92)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 4.0312e+00 (3.5633e+00)	Acc@1   4.00 ( 14.76)	Acc@5  39.00 ( 42.89)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 3.6582e+00 (3.5699e+00)	Acc@1  10.00 ( 14.64)	Acc@5  46.00 ( 42.74)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 3.3594e+00 (3.5657e+00)	Acc@1  15.00 ( 14.60)	Acc@5  47.00 ( 42.85)
 * Acc@1 14.630 Acc@5 42.830
### epoch[1] execution time: 28.94044041633606
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.224 ( 0.224)	Data  0.149 ( 0.149)	Loss 3.2344e+00 (3.2344e+00)	Acc@1  20.31 ( 20.31)	Acc@5  53.12 ( 53.12)
Epoch: [2][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.014)	Loss 3.2070e+00 (3.3249e+00)	Acc@1  20.31 ( 17.90)	Acc@5  54.69 ( 47.80)
Epoch: [2][ 20/391]	Time  0.069 ( 0.074)	Data  0.001 ( 0.008)	Loss 3.2559e+00 (3.2729e+00)	Acc@1  18.75 ( 18.86)	Acc@5  44.53 ( 48.47)
Epoch: [2][ 30/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.006)	Loss 3.3867e+00 (3.3115e+00)	Acc@1  10.94 ( 18.02)	Acc@5  40.62 ( 47.23)
Epoch: [2][ 40/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.005)	Loss 3.3809e+00 (3.3245e+00)	Acc@1   8.59 ( 17.36)	Acc@5  40.62 ( 46.61)
Epoch: [2][ 50/391]	Time  0.075 ( 0.070)	Data  0.001 ( 0.004)	Loss 3.2539e+00 (3.3246e+00)	Acc@1  20.31 ( 17.45)	Acc@5  49.22 ( 46.68)
Epoch: [2][ 60/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 3.5566e+00 (3.3306e+00)	Acc@1  14.06 ( 17.23)	Acc@5  39.84 ( 46.55)
Epoch: [2][ 70/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.1934e+00 (3.3221e+00)	Acc@1  16.41 ( 17.39)	Acc@5  50.78 ( 46.73)
Epoch: [2][ 80/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.4805e+00 (3.3250e+00)	Acc@1  13.28 ( 17.33)	Acc@5  43.75 ( 46.68)
Epoch: [2][ 90/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.2930e+00 (3.3371e+00)	Acc@1  14.84 ( 16.93)	Acc@5  43.75 ( 46.19)
Epoch: [2][100/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.3301e+00 (3.3294e+00)	Acc@1  17.19 ( 17.14)	Acc@5  48.44 ( 46.38)
Epoch: [2][110/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.5098e+00 (3.3309e+00)	Acc@1  14.06 ( 17.11)	Acc@5  43.75 ( 46.34)
Epoch: [2][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.3105e+00 (3.3293e+00)	Acc@1  14.06 ( 17.19)	Acc@5  47.66 ( 46.41)
Epoch: [2][130/391]	Time  0.075 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.2480e+00 (3.3230e+00)	Acc@1  21.09 ( 17.31)	Acc@5  48.44 ( 46.64)
Epoch: [2][140/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.2539e+00 (3.3152e+00)	Acc@1  21.09 ( 17.53)	Acc@5  46.88 ( 46.83)
Epoch: [2][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.2168e+00 (3.3137e+00)	Acc@1  17.19 ( 17.56)	Acc@5  49.22 ( 46.75)
Epoch: [2][160/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.0781e+00 (3.3097e+00)	Acc@1  20.31 ( 17.68)	Acc@5  60.16 ( 46.82)
Epoch: [2][170/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.9082e+00 (3.3022e+00)	Acc@1  28.91 ( 17.92)	Acc@5  58.59 ( 47.04)
Epoch: [2][180/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.4473e+00 (3.2962e+00)	Acc@1  17.19 ( 18.03)	Acc@5  42.19 ( 47.15)
Epoch: [2][190/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.2578e+00 (3.2899e+00)	Acc@1  19.53 ( 18.17)	Acc@5  46.09 ( 47.34)
Epoch: [2][200/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.3574e+00 (3.2857e+00)	Acc@1  17.19 ( 18.26)	Acc@5  44.53 ( 47.41)
Epoch: [2][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2520e+00 (3.2863e+00)	Acc@1  18.75 ( 18.26)	Acc@5  45.31 ( 47.37)
Epoch: [2][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1699e+00 (3.2819e+00)	Acc@1  18.75 ( 18.29)	Acc@5  55.47 ( 47.54)
Epoch: [2][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.3379e+00 (3.2772e+00)	Acc@1  19.53 ( 18.44)	Acc@5  46.09 ( 47.69)
Epoch: [2][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1406e+00 (3.2692e+00)	Acc@1  21.09 ( 18.61)	Acc@5  46.88 ( 47.83)
Epoch: [2][250/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.0996e+00 (3.2643e+00)	Acc@1  21.09 ( 18.79)	Acc@5  55.47 ( 48.05)
Epoch: [2][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2461e+00 (3.2595e+00)	Acc@1  16.41 ( 18.83)	Acc@5  50.78 ( 48.15)
Epoch: [2][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1406e+00 (3.2537e+00)	Acc@1  25.00 ( 18.93)	Acc@5  50.00 ( 48.36)
Epoch: [2][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.0566e+00 (3.2518e+00)	Acc@1  23.44 ( 18.97)	Acc@5  54.69 ( 48.46)
Epoch: [2][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.0527e+00 (3.2482e+00)	Acc@1  27.34 ( 19.11)	Acc@5  51.56 ( 48.61)
Epoch: [2][300/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9082e+00 (3.2447e+00)	Acc@1  32.81 ( 19.20)	Acc@5  58.59 ( 48.74)
Epoch: [2][310/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9062e+00 (3.2405e+00)	Acc@1  26.56 ( 19.29)	Acc@5  55.47 ( 48.85)
Epoch: [2][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1348e+00 (3.2369e+00)	Acc@1  25.00 ( 19.33)	Acc@5  51.56 ( 48.91)
Epoch: [2][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2285e+00 (3.2339e+00)	Acc@1  20.31 ( 19.43)	Acc@5  48.44 ( 49.02)
Epoch: [2][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1680e+00 (3.2309e+00)	Acc@1  17.97 ( 19.46)	Acc@5  50.00 ( 49.07)
Epoch: [2][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1582e+00 (3.2294e+00)	Acc@1  21.09 ( 19.51)	Acc@5  53.12 ( 49.10)
Epoch: [2][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.8926e+00 (3.2267e+00)	Acc@1  26.56 ( 19.59)	Acc@5  54.69 ( 49.20)
Epoch: [2][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.0801e+00 (3.2240e+00)	Acc@1  21.09 ( 19.63)	Acc@5  57.03 ( 49.25)
Epoch: [2][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.0215e+00 (3.2197e+00)	Acc@1  20.31 ( 19.73)	Acc@5  56.25 ( 49.37)
Epoch: [2][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.9336e+00 (3.2174e+00)	Acc@1  28.75 ( 19.77)	Acc@5  56.25 ( 49.41)
## e[2] optimizer.zero_grad (sum) time: 0.41222095489501953
## e[2]       loss.backward (sum) time: 7.362451791763306
## e[2]      optimizer.step (sum) time: 3.6546287536621094
## epoch[2] training(only) time: 26.284306287765503
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 3.3340e+00 (3.3340e+00)	Acc@1  15.00 ( 15.00)	Acc@5  43.00 ( 43.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 3.1719e+00 (3.2054e+00)	Acc@1  17.00 ( 20.91)	Acc@5  55.00 ( 50.64)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 3.1211e+00 (3.1994e+00)	Acc@1  20.00 ( 20.71)	Acc@5  55.00 ( 50.29)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 3.2578e+00 (3.2095e+00)	Acc@1  25.00 ( 20.71)	Acc@5  46.00 ( 49.94)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 3.2324e+00 (3.2076e+00)	Acc@1  24.00 ( 20.80)	Acc@5  48.00 ( 50.02)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 3.2285e+00 (3.2184e+00)	Acc@1  22.00 ( 21.00)	Acc@5  48.00 ( 49.76)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 3.0898e+00 (3.2175e+00)	Acc@1  21.00 ( 20.95)	Acc@5  59.00 ( 50.25)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 3.4375e+00 (3.2324e+00)	Acc@1  15.00 ( 20.80)	Acc@5  47.00 ( 49.92)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 3.3008e+00 (3.2337e+00)	Acc@1  17.00 ( 20.70)	Acc@5  48.00 ( 50.06)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 3.1074e+00 (3.2274e+00)	Acc@1  25.00 ( 20.90)	Acc@5  48.00 ( 50.13)
 * Acc@1 20.840 Acc@5 50.070
### epoch[2] execution time: 29.109644889831543
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.221 ( 0.221)	Data  0.146 ( 0.146)	Loss 3.1191e+00 (3.1191e+00)	Acc@1  18.75 ( 18.75)	Acc@5  51.56 ( 51.56)
Epoch: [3][ 10/391]	Time  0.068 ( 0.081)	Data  0.001 ( 0.014)	Loss 3.1133e+00 (3.0504e+00)	Acc@1  20.31 ( 21.95)	Acc@5  54.69 ( 53.48)
Epoch: [3][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.008)	Loss 3.0684e+00 (3.0741e+00)	Acc@1  24.22 ( 22.40)	Acc@5  54.69 ( 53.20)
Epoch: [3][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 2.9219e+00 (3.0528e+00)	Acc@1  26.56 ( 23.08)	Acc@5  53.12 ( 53.98)
Epoch: [3][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.005)	Loss 3.0449e+00 (3.0619e+00)	Acc@1  21.09 ( 22.64)	Acc@5  51.56 ( 53.68)
Epoch: [3][ 50/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 3.0723e+00 (3.0581e+00)	Acc@1  29.69 ( 22.98)	Acc@5  51.56 ( 53.49)
Epoch: [3][ 60/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.0176e+00 (3.0419e+00)	Acc@1  21.09 ( 23.12)	Acc@5  59.38 ( 54.14)
Epoch: [3][ 70/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.9297e+00 (3.0285e+00)	Acc@1  22.66 ( 23.22)	Acc@5  59.38 ( 54.59)
Epoch: [3][ 80/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.8789e+00 (3.0226e+00)	Acc@1  25.78 ( 23.31)	Acc@5  57.03 ( 54.69)
Epoch: [3][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.9941e+00 (3.0202e+00)	Acc@1  29.69 ( 23.44)	Acc@5  54.69 ( 54.68)
Epoch: [3][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.8672e+00 (3.0188e+00)	Acc@1  23.44 ( 23.55)	Acc@5  59.38 ( 54.70)
Epoch: [3][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.9570e+00 (3.0112e+00)	Acc@1  24.22 ( 23.59)	Acc@5  56.25 ( 54.89)
Epoch: [3][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.0586e+00 (3.0115e+00)	Acc@1  21.09 ( 23.59)	Acc@5  53.12 ( 54.99)
Epoch: [3][130/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.0742e+00 (3.0076e+00)	Acc@1  24.22 ( 23.73)	Acc@5  51.56 ( 55.07)
Epoch: [3][140/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.7617e+00 (3.0004e+00)	Acc@1  24.22 ( 23.68)	Acc@5  60.94 ( 55.13)
Epoch: [3][150/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9570e+00 (2.9950e+00)	Acc@1  22.66 ( 23.70)	Acc@5  53.91 ( 55.26)
Epoch: [3][160/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9473e+00 (2.9963e+00)	Acc@1  21.09 ( 23.66)	Acc@5  61.72 ( 55.24)
Epoch: [3][170/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8418e+00 (2.9957e+00)	Acc@1  22.66 ( 23.70)	Acc@5  60.16 ( 55.29)
Epoch: [3][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9434e+00 (2.9890e+00)	Acc@1  24.22 ( 23.80)	Acc@5  57.81 ( 55.46)
Epoch: [3][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9434e+00 (2.9863e+00)	Acc@1  18.75 ( 23.81)	Acc@5  61.72 ( 55.58)
Epoch: [3][200/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.0254e+00 (2.9859e+00)	Acc@1  20.31 ( 23.86)	Acc@5  55.47 ( 55.60)
Epoch: [3][210/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9297e+00 (2.9808e+00)	Acc@1  28.91 ( 23.98)	Acc@5  62.50 ( 55.79)
Epoch: [3][220/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.0801e+00 (2.9811e+00)	Acc@1  24.22 ( 23.98)	Acc@5  54.69 ( 55.81)
Epoch: [3][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9648e+00 (2.9747e+00)	Acc@1  24.22 ( 24.14)	Acc@5  57.81 ( 55.91)
Epoch: [3][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9453e+00 (2.9756e+00)	Acc@1  25.78 ( 24.05)	Acc@5  53.91 ( 55.87)
Epoch: [3][250/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1016e+00 (2.9713e+00)	Acc@1  20.31 ( 24.18)	Acc@5  53.91 ( 55.97)
Epoch: [3][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8867e+00 (2.9704e+00)	Acc@1  25.00 ( 24.19)	Acc@5  60.94 ( 56.03)
Epoch: [3][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8867e+00 (2.9686e+00)	Acc@1  26.56 ( 24.29)	Acc@5  55.47 ( 56.07)
Epoch: [3][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9434e+00 (2.9666e+00)	Acc@1  19.53 ( 24.31)	Acc@5  57.03 ( 56.10)
Epoch: [3][290/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2832e+00 (2.9653e+00)	Acc@1  23.44 ( 24.30)	Acc@5  53.12 ( 56.17)
Epoch: [3][300/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6387e+00 (2.9615e+00)	Acc@1  28.91 ( 24.37)	Acc@5  67.97 ( 56.29)
Epoch: [3][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8633e+00 (2.9557e+00)	Acc@1  26.56 ( 24.50)	Acc@5  59.38 ( 56.41)
Epoch: [3][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.7539e+00 (2.9526e+00)	Acc@1  28.91 ( 24.60)	Acc@5  65.62 ( 56.52)
Epoch: [3][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8555e+00 (2.9505e+00)	Acc@1  29.69 ( 24.67)	Acc@5  57.03 ( 56.54)
Epoch: [3][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6484e+00 (2.9472e+00)	Acc@1  25.78 ( 24.70)	Acc@5  64.84 ( 56.59)
Epoch: [3][350/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.7949e+00 (2.9442e+00)	Acc@1  28.91 ( 24.77)	Acc@5  60.16 ( 56.68)
Epoch: [3][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8047e+00 (2.9416e+00)	Acc@1  25.00 ( 24.81)	Acc@5  60.16 ( 56.75)
Epoch: [3][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.7539e+00 (2.9377e+00)	Acc@1  27.34 ( 24.89)	Acc@5  57.81 ( 56.85)
Epoch: [3][380/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.7676e+00 (2.9357e+00)	Acc@1  25.78 ( 24.90)	Acc@5  55.47 ( 56.88)
Epoch: [3][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.0586e+00 (2.9350e+00)	Acc@1  22.50 ( 24.95)	Acc@5  53.75 ( 56.91)
## e[3] optimizer.zero_grad (sum) time: 0.4080057144165039
## e[3]       loss.backward (sum) time: 7.349530458450317
## e[3]      optimizer.step (sum) time: 3.6587350368499756
## epoch[3] training(only) time: 26.276881217956543
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 2.9297e+00 (2.9297e+00)	Acc@1  26.00 ( 26.00)	Acc@5  52.00 ( 52.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 2.9023e+00 (2.8681e+00)	Acc@1  22.00 ( 27.82)	Acc@5  64.00 ( 58.00)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 2.6758e+00 (2.8227e+00)	Acc@1  34.00 ( 28.86)	Acc@5  60.00 ( 59.57)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.9922e+00 (2.8247e+00)	Acc@1  31.00 ( 28.97)	Acc@5  54.00 ( 58.58)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 3.0742e+00 (2.8140e+00)	Acc@1  28.00 ( 28.80)	Acc@5  59.00 ( 58.66)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 2.7832e+00 (2.8186e+00)	Acc@1  34.00 ( 28.61)	Acc@5  58.00 ( 58.61)
Test: [ 60/100]	Time  0.024 ( 0.029)	Loss 2.7188e+00 (2.8133e+00)	Acc@1  29.00 ( 28.30)	Acc@5  62.00 ( 58.64)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 3.1660e+00 (2.8171e+00)	Acc@1  24.00 ( 28.23)	Acc@5  53.00 ( 58.76)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.9141e+00 (2.8270e+00)	Acc@1  23.00 ( 27.78)	Acc@5  63.00 ( 58.80)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.8789e+00 (2.8228e+00)	Acc@1  31.00 ( 27.85)	Acc@5  61.00 ( 59.09)
 * Acc@1 27.810 Acc@5 59.170
### epoch[3] execution time: 29.12389636039734
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.216 ( 0.216)	Data  0.143 ( 0.143)	Loss 2.8223e+00 (2.8223e+00)	Acc@1  30.47 ( 30.47)	Acc@5  59.38 ( 59.38)
Epoch: [4][ 10/391]	Time  0.068 ( 0.079)	Data  0.001 ( 0.014)	Loss 2.7773e+00 (2.7207e+00)	Acc@1  24.22 ( 30.61)	Acc@5  61.72 ( 61.79)
Epoch: [4][ 20/391]	Time  0.069 ( 0.073)	Data  0.001 ( 0.008)	Loss 3.1270e+00 (2.7461e+00)	Acc@1  21.09 ( 29.20)	Acc@5  51.56 ( 61.50)
Epoch: [4][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 2.5625e+00 (2.7292e+00)	Acc@1  32.81 ( 28.88)	Acc@5  67.19 ( 62.17)
Epoch: [4][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 2.8398e+00 (2.7409e+00)	Acc@1  21.88 ( 28.64)	Acc@5  59.38 ( 61.87)
Epoch: [4][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 2.8047e+00 (2.7599e+00)	Acc@1  28.12 ( 28.52)	Acc@5  60.16 ( 61.14)
Epoch: [4][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.6348e+00 (2.7546e+00)	Acc@1  28.91 ( 28.55)	Acc@5  66.41 ( 61.27)
Epoch: [4][ 70/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.9219e+00 (2.7456e+00)	Acc@1  25.78 ( 28.83)	Acc@5  50.00 ( 61.48)
Epoch: [4][ 80/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.7480e+00 (2.7464e+00)	Acc@1  25.00 ( 28.65)	Acc@5  63.28 ( 61.60)
Epoch: [4][ 90/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.8223e+00 (2.7392e+00)	Acc@1  26.56 ( 28.86)	Acc@5  60.94 ( 61.74)
Epoch: [4][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.7754e+00 (2.7404e+00)	Acc@1  28.12 ( 28.78)	Acc@5  57.81 ( 61.73)
Epoch: [4][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.6699e+00 (2.7411e+00)	Acc@1  30.47 ( 28.78)	Acc@5  62.50 ( 61.78)
Epoch: [4][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.8359e+00 (2.7465e+00)	Acc@1  25.78 ( 28.67)	Acc@5  63.28 ( 61.73)
Epoch: [4][130/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.7578e+00 (2.7469e+00)	Acc@1  25.00 ( 28.64)	Acc@5  64.84 ( 61.77)
Epoch: [4][140/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2812e+00 (2.7431e+00)	Acc@1  37.50 ( 28.67)	Acc@5  73.44 ( 61.92)
Epoch: [4][150/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.7715e+00 (2.7393e+00)	Acc@1  35.94 ( 28.83)	Acc@5  60.94 ( 62.06)
Epoch: [4][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8828e+00 (2.7444e+00)	Acc@1  28.12 ( 28.74)	Acc@5  59.38 ( 61.96)
Epoch: [4][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6484e+00 (2.7365e+00)	Acc@1  27.34 ( 28.87)	Acc@5  63.28 ( 62.09)
Epoch: [4][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8027e+00 (2.7383e+00)	Acc@1  25.00 ( 28.85)	Acc@5  59.38 ( 62.02)
Epoch: [4][190/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.0078e+00 (2.7370e+00)	Acc@1  27.34 ( 28.90)	Acc@5  53.91 ( 62.02)
Epoch: [4][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6914e+00 (2.7363e+00)	Acc@1  30.47 ( 28.91)	Acc@5  60.16 ( 62.03)
Epoch: [4][210/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5586e+00 (2.7318e+00)	Acc@1  32.81 ( 29.05)	Acc@5  64.06 ( 62.06)
Epoch: [4][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.7383e+00 (2.7316e+00)	Acc@1  33.59 ( 29.05)	Acc@5  59.38 ( 61.99)
Epoch: [4][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6406e+00 (2.7287e+00)	Acc@1  35.94 ( 29.06)	Acc@5  62.50 ( 62.08)
Epoch: [4][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.7559e+00 (2.7273e+00)	Acc@1  28.91 ( 29.11)	Acc@5  60.16 ( 62.12)
Epoch: [4][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.7305e+00 (2.7271e+00)	Acc@1  29.69 ( 29.10)	Acc@5  61.72 ( 62.16)
Epoch: [4][260/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.7930e+00 (2.7293e+00)	Acc@1  29.69 ( 29.05)	Acc@5  62.50 ( 62.10)
Epoch: [4][270/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5312e+00 (2.7278e+00)	Acc@1  28.12 ( 29.05)	Acc@5  67.19 ( 62.20)
Epoch: [4][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6055e+00 (2.7268e+00)	Acc@1  31.25 ( 29.04)	Acc@5  71.09 ( 62.22)
Epoch: [4][290/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6172e+00 (2.7230e+00)	Acc@1  29.69 ( 29.13)	Acc@5  70.31 ( 62.34)
Epoch: [4][300/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5977e+00 (2.7231e+00)	Acc@1  29.69 ( 29.11)	Acc@5  65.62 ( 62.35)
Epoch: [4][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2988e+00 (2.7199e+00)	Acc@1  38.28 ( 29.15)	Acc@5  72.66 ( 62.40)
Epoch: [4][320/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6699e+00 (2.7172e+00)	Acc@1  31.25 ( 29.23)	Acc@5  61.72 ( 62.45)
Epoch: [4][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5312e+00 (2.7155e+00)	Acc@1  29.69 ( 29.26)	Acc@5  67.19 ( 62.47)
Epoch: [4][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6777e+00 (2.7141e+00)	Acc@1  22.66 ( 29.22)	Acc@5  61.72 ( 62.47)
Epoch: [4][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.6211e+00 (2.7083e+00)	Acc@1  32.81 ( 29.33)	Acc@5  64.84 ( 62.62)
Epoch: [4][360/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.4512e+00 (2.7031e+00)	Acc@1  33.59 ( 29.41)	Acc@5  71.09 ( 62.76)
Epoch: [4][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.6445e+00 (2.6993e+00)	Acc@1  29.69 ( 29.46)	Acc@5  66.41 ( 62.85)
Epoch: [4][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.4766e+00 (2.6977e+00)	Acc@1  35.16 ( 29.51)	Acc@5  70.31 ( 62.90)
Epoch: [4][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.5781e+00 (2.6963e+00)	Acc@1  33.75 ( 29.55)	Acc@5  65.00 ( 62.91)
## e[4] optimizer.zero_grad (sum) time: 0.41400146484375
## e[4]       loss.backward (sum) time: 7.325776815414429
## e[4]      optimizer.step (sum) time: 3.617840528488159
## epoch[4] training(only) time: 26.1783287525177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 2.6504e+00 (2.6504e+00)	Acc@1  31.00 ( 31.00)	Acc@5  64.00 ( 64.00)
Test: [ 10/100]	Time  0.029 ( 0.039)	Loss 2.6875e+00 (2.6536e+00)	Acc@1  30.00 ( 30.73)	Acc@5  65.00 ( 63.00)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 2.3535e+00 (2.6190e+00)	Acc@1  35.00 ( 31.62)	Acc@5  76.00 ( 64.95)
Test: [ 30/100]	Time  0.027 ( 0.032)	Loss 2.7266e+00 (2.6058e+00)	Acc@1  32.00 ( 31.55)	Acc@5  67.00 ( 65.26)
Test: [ 40/100]	Time  0.027 ( 0.031)	Loss 2.5977e+00 (2.6073e+00)	Acc@1  38.00 ( 31.59)	Acc@5  64.00 ( 65.05)
Test: [ 50/100]	Time  0.026 ( 0.030)	Loss 2.6445e+00 (2.6207e+00)	Acc@1  32.00 ( 31.35)	Acc@5  60.00 ( 64.43)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.5977e+00 (2.6161e+00)	Acc@1  38.00 ( 31.72)	Acc@5  65.00 ( 64.61)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 2.9219e+00 (2.6232e+00)	Acc@1  27.00 ( 31.63)	Acc@5  58.00 ( 64.46)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.8809e+00 (2.6378e+00)	Acc@1  27.00 ( 31.22)	Acc@5  55.00 ( 64.15)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.5664e+00 (2.6283e+00)	Acc@1  33.00 ( 31.38)	Acc@5  64.00 ( 64.34)
 * Acc@1 31.280 Acc@5 64.300
### epoch[4] execution time: 29.034403085708618
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.225 ( 0.225)	Data  0.149 ( 0.149)	Loss 2.5859e+00 (2.5859e+00)	Acc@1  33.59 ( 33.59)	Acc@5  66.41 ( 66.41)
Epoch: [5][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.014)	Loss 2.3867e+00 (2.5538e+00)	Acc@1  38.28 ( 31.68)	Acc@5  70.31 ( 66.34)
Epoch: [5][ 20/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.008)	Loss 2.5410e+00 (2.5581e+00)	Acc@1  32.81 ( 31.96)	Acc@5  64.84 ( 66.00)
Epoch: [5][ 30/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.006)	Loss 2.5273e+00 (2.5629e+00)	Acc@1  35.16 ( 32.38)	Acc@5  64.84 ( 66.05)
Epoch: [5][ 40/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.005)	Loss 2.5527e+00 (2.5602e+00)	Acc@1  30.47 ( 32.39)	Acc@5  63.28 ( 65.93)
Epoch: [5][ 50/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.004)	Loss 2.6504e+00 (2.5686e+00)	Acc@1  31.25 ( 32.41)	Acc@5  62.50 ( 65.72)
Epoch: [5][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 2.5645e+00 (2.5737e+00)	Acc@1  37.50 ( 32.24)	Acc@5  68.75 ( 65.66)
Epoch: [5][ 70/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.6602e+00 (2.5731e+00)	Acc@1  28.12 ( 32.21)	Acc@5  60.94 ( 65.55)
Epoch: [5][ 80/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.7402e+00 (2.5766e+00)	Acc@1  28.91 ( 32.19)	Acc@5  65.62 ( 65.37)
Epoch: [5][ 90/391]	Time  0.078 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.5605e+00 (2.5834e+00)	Acc@1  25.78 ( 32.06)	Acc@5  65.62 ( 65.19)
Epoch: [5][100/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.2305e+00 (2.5763e+00)	Acc@1  42.19 ( 32.22)	Acc@5  75.78 ( 65.42)
Epoch: [5][110/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.3945e+00 (2.5733e+00)	Acc@1  33.59 ( 32.27)	Acc@5  69.53 ( 65.43)
Epoch: [5][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.4141e+00 (2.5650e+00)	Acc@1  39.06 ( 32.43)	Acc@5  68.75 ( 65.58)
Epoch: [5][130/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.6133e+00 (2.5628e+00)	Acc@1  33.59 ( 32.52)	Acc@5  60.94 ( 65.65)
Epoch: [5][140/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.4727e+00 (2.5615e+00)	Acc@1  35.16 ( 32.52)	Acc@5  70.31 ( 65.65)
Epoch: [5][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.5645e+00 (2.5550e+00)	Acc@1  28.91 ( 32.63)	Acc@5  67.97 ( 65.78)
Epoch: [5][160/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3906e+00 (2.5557e+00)	Acc@1  42.19 ( 32.64)	Acc@5  71.09 ( 65.73)
Epoch: [5][170/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6035e+00 (2.5558e+00)	Acc@1  32.03 ( 32.69)	Acc@5  64.84 ( 65.72)
Epoch: [5][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3105e+00 (2.5538e+00)	Acc@1  40.62 ( 32.82)	Acc@5  74.22 ( 65.77)
Epoch: [5][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5371e+00 (2.5497e+00)	Acc@1  32.03 ( 32.85)	Acc@5  67.97 ( 65.87)
Epoch: [5][200/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5664e+00 (2.5471e+00)	Acc@1  34.38 ( 32.88)	Acc@5  67.19 ( 65.98)
Epoch: [5][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4277e+00 (2.5428e+00)	Acc@1  30.47 ( 32.92)	Acc@5  65.62 ( 66.01)
Epoch: [5][220/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5586e+00 (2.5420e+00)	Acc@1  29.69 ( 32.96)	Acc@5  61.72 ( 65.97)
Epoch: [5][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6641e+00 (2.5377e+00)	Acc@1  30.47 ( 33.01)	Acc@5  64.84 ( 66.06)
Epoch: [5][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6523e+00 (2.5381e+00)	Acc@1  25.00 ( 33.00)	Acc@5  67.19 ( 66.03)
Epoch: [5][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4629e+00 (2.5360e+00)	Acc@1  30.47 ( 33.00)	Acc@5  69.53 ( 66.09)
Epoch: [5][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4922e+00 (2.5362e+00)	Acc@1  34.38 ( 32.98)	Acc@5  67.19 ( 66.12)
Epoch: [5][270/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6270e+00 (2.5352e+00)	Acc@1  34.38 ( 33.01)	Acc@5  61.72 ( 66.21)
Epoch: [5][280/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4512e+00 (2.5371e+00)	Acc@1  38.28 ( 32.99)	Acc@5  70.31 ( 66.19)
Epoch: [5][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0918e+00 (2.5348e+00)	Acc@1  41.41 ( 33.01)	Acc@5  75.78 ( 66.28)
Epoch: [5][300/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.7910e+00 (2.5350e+00)	Acc@1  32.03 ( 32.97)	Acc@5  62.50 ( 66.27)
Epoch: [5][310/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4648e+00 (2.5340e+00)	Acc@1  32.81 ( 33.04)	Acc@5  68.75 ( 66.32)
Epoch: [5][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4277e+00 (2.5340e+00)	Acc@1  35.94 ( 33.01)	Acc@5  67.97 ( 66.28)
Epoch: [5][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5234e+00 (2.5318e+00)	Acc@1  32.03 ( 33.08)	Acc@5  66.41 ( 66.32)
Epoch: [5][340/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.3574e+00 (2.5300e+00)	Acc@1  36.72 ( 33.13)	Acc@5  70.31 ( 66.37)
Epoch: [5][350/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.5742e+00 (2.5289e+00)	Acc@1  29.69 ( 33.17)	Acc@5  62.50 ( 66.40)
Epoch: [5][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.8770e+00 (2.5278e+00)	Acc@1  22.66 ( 33.18)	Acc@5  59.38 ( 66.44)
Epoch: [5][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.1777e+00 (2.5266e+00)	Acc@1  39.84 ( 33.23)	Acc@5  75.78 ( 66.47)
Epoch: [5][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.3828e+00 (2.5260e+00)	Acc@1  32.81 ( 33.23)	Acc@5  67.19 ( 66.48)
Epoch: [5][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.3672e+00 (2.5250e+00)	Acc@1  41.25 ( 33.29)	Acc@5  72.50 ( 66.51)
## e[5] optimizer.zero_grad (sum) time: 0.4133143424987793
## e[5]       loss.backward (sum) time: 7.369670629501343
## e[5]      optimizer.step (sum) time: 3.549572229385376
## epoch[5] training(only) time: 26.19559955596924
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 2.5977e+00 (2.5977e+00)	Acc@1  27.00 ( 27.00)	Acc@5  64.00 ( 64.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 2.6543e+00 (2.5215e+00)	Acc@1  26.00 ( 33.09)	Acc@5  62.00 ( 65.82)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 2.4609e+00 (2.5219e+00)	Acc@1  35.00 ( 33.57)	Acc@5  72.00 ( 66.90)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 2.7598e+00 (2.5145e+00)	Acc@1  26.00 ( 33.65)	Acc@5  59.00 ( 67.45)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.4238e+00 (2.4987e+00)	Acc@1  37.00 ( 33.90)	Acc@5  73.00 ( 67.78)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 2.3906e+00 (2.5175e+00)	Acc@1  36.00 ( 33.84)	Acc@5  68.00 ( 66.96)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.3379e+00 (2.5142e+00)	Acc@1  39.00 ( 34.07)	Acc@5  67.00 ( 66.82)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 2.7539e+00 (2.5228e+00)	Acc@1  26.00 ( 33.68)	Acc@5  70.00 ( 66.77)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.7129e+00 (2.5281e+00)	Acc@1  30.00 ( 33.40)	Acc@5  71.00 ( 66.88)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.5645e+00 (2.5137e+00)	Acc@1  34.00 ( 33.74)	Acc@5  69.00 ( 67.33)
 * Acc@1 33.780 Acc@5 67.300
### epoch[5] execution time: 29.008657217025757
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.226 ( 0.226)	Data  0.146 ( 0.146)	Loss 2.3262e+00 (2.3262e+00)	Acc@1  37.50 ( 37.50)	Acc@5  71.88 ( 71.88)
Epoch: [6][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.014)	Loss 2.2344e+00 (2.3260e+00)	Acc@1  35.16 ( 36.65)	Acc@5  75.00 ( 71.24)
Epoch: [6][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 2.4902e+00 (2.4116e+00)	Acc@1  33.59 ( 34.82)	Acc@5  70.31 ( 70.05)
Epoch: [6][ 30/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.006)	Loss 2.6270e+00 (2.4025e+00)	Acc@1  36.72 ( 35.61)	Acc@5  63.28 ( 69.51)
Epoch: [6][ 40/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.005)	Loss 2.3984e+00 (2.3757e+00)	Acc@1  35.94 ( 35.94)	Acc@5  69.53 ( 70.29)
Epoch: [6][ 50/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.004)	Loss 2.1738e+00 (2.3730e+00)	Acc@1  34.38 ( 35.89)	Acc@5  73.44 ( 70.27)
Epoch: [6][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 2.4023e+00 (2.3642e+00)	Acc@1  32.03 ( 35.95)	Acc@5  68.75 ( 70.49)
Epoch: [6][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 2.5879e+00 (2.3696e+00)	Acc@1  25.00 ( 35.59)	Acc@5  62.50 ( 70.22)
Epoch: [6][ 80/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 2.3828e+00 (2.3725e+00)	Acc@1  31.25 ( 35.66)	Acc@5  70.31 ( 70.08)
Epoch: [6][ 90/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.4863e+00 (2.3753e+00)	Acc@1  32.03 ( 35.75)	Acc@5  65.62 ( 70.00)
Epoch: [6][100/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.4043e+00 (2.3794e+00)	Acc@1  32.03 ( 35.61)	Acc@5  68.75 ( 69.80)
Epoch: [6][110/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.4668e+00 (2.3859e+00)	Acc@1  37.50 ( 35.56)	Acc@5  69.53 ( 69.78)
Epoch: [6][120/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.1113e+00 (2.3854e+00)	Acc@1  39.06 ( 35.83)	Acc@5  77.34 ( 69.92)
Epoch: [6][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.2090e+00 (2.3790e+00)	Acc@1  37.50 ( 35.90)	Acc@5  73.44 ( 70.00)
Epoch: [6][140/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.5098e+00 (2.3822e+00)	Acc@1  35.94 ( 35.73)	Acc@5  65.62 ( 69.95)
Epoch: [6][150/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4727e+00 (2.3792e+00)	Acc@1  35.94 ( 35.82)	Acc@5  70.31 ( 70.01)
Epoch: [6][160/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5273e+00 (2.3804e+00)	Acc@1  32.81 ( 35.87)	Acc@5  67.97 ( 70.03)
Epoch: [6][170/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2969e+00 (2.3803e+00)	Acc@1  37.50 ( 35.88)	Acc@5  75.00 ( 70.08)
Epoch: [6][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4434e+00 (2.3860e+00)	Acc@1  38.28 ( 35.76)	Acc@5  66.41 ( 69.98)
Epoch: [6][190/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4219e+00 (2.3855e+00)	Acc@1  36.72 ( 35.81)	Acc@5  69.53 ( 70.01)
Epoch: [6][200/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0977e+00 (2.3806e+00)	Acc@1  43.75 ( 35.92)	Acc@5  77.34 ( 70.09)
Epoch: [6][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3887e+00 (2.3825e+00)	Acc@1  37.50 ( 35.90)	Acc@5  67.19 ( 69.96)
Epoch: [6][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2695e+00 (2.3786e+00)	Acc@1  39.84 ( 35.97)	Acc@5  67.97 ( 70.04)
Epoch: [6][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2520e+00 (2.3781e+00)	Acc@1  41.41 ( 36.00)	Acc@5  72.66 ( 70.01)
Epoch: [6][240/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4375e+00 (2.3774e+00)	Acc@1  33.59 ( 36.06)	Acc@5  68.75 ( 69.99)
Epoch: [6][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2656e+00 (2.3770e+00)	Acc@1  35.94 ( 36.07)	Acc@5  70.31 ( 70.02)
Epoch: [6][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4062e+00 (2.3764e+00)	Acc@1  40.62 ( 36.09)	Acc@5  67.97 ( 70.03)
Epoch: [6][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3984e+00 (2.3753e+00)	Acc@1  32.03 ( 36.12)	Acc@5  67.19 ( 70.05)
Epoch: [6][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2129e+00 (2.3754e+00)	Acc@1  39.06 ( 36.09)	Acc@5  70.31 ( 70.05)
Epoch: [6][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0137e+00 (2.3722e+00)	Acc@1  41.41 ( 36.18)	Acc@5  75.00 ( 70.11)
Epoch: [6][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4902e+00 (2.3736e+00)	Acc@1  36.72 ( 36.14)	Acc@5  65.62 ( 70.11)
Epoch: [6][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5273e+00 (2.3741e+00)	Acc@1  28.91 ( 36.10)	Acc@5  64.84 ( 70.05)
Epoch: [6][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3789e+00 (2.3747e+00)	Acc@1  36.72 ( 36.12)	Acc@5  69.53 ( 69.98)
Epoch: [6][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0371e+00 (2.3723e+00)	Acc@1  42.97 ( 36.18)	Acc@5  78.12 ( 70.06)
Epoch: [6][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4355e+00 (2.3720e+00)	Acc@1  33.59 ( 36.16)	Acc@5  65.62 ( 70.04)
Epoch: [6][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3555e+00 (2.3730e+00)	Acc@1  37.50 ( 36.17)	Acc@5  67.97 ( 70.02)
Epoch: [6][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.3359e+00 (2.3721e+00)	Acc@1  39.84 ( 36.24)	Acc@5  67.19 ( 70.05)
Epoch: [6][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.2012e+00 (2.3722e+00)	Acc@1  38.28 ( 36.24)	Acc@5  71.88 ( 70.04)
Epoch: [6][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.4023e+00 (2.3709e+00)	Acc@1  36.72 ( 36.28)	Acc@5  72.66 ( 70.09)
Epoch: [6][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.2793e+00 (2.3700e+00)	Acc@1  37.50 ( 36.29)	Acc@5  70.00 ( 70.09)
## e[6] optimizer.zero_grad (sum) time: 0.41411828994750977
## e[6]       loss.backward (sum) time: 7.34952974319458
## e[6]      optimizer.step (sum) time: 3.6452460289001465
## epoch[6] training(only) time: 26.25711226463318
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 2.7168e+00 (2.7168e+00)	Acc@1  32.00 ( 32.00)	Acc@5  63.00 ( 63.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 2.5312e+00 (2.6188e+00)	Acc@1  32.00 ( 33.45)	Acc@5  70.00 ( 65.27)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 2.4375e+00 (2.5811e+00)	Acc@1  32.00 ( 32.67)	Acc@5  73.00 ( 67.76)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.6348e+00 (2.5864e+00)	Acc@1  27.00 ( 32.81)	Acc@5  64.00 ( 66.81)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.5449e+00 (2.5735e+00)	Acc@1  35.00 ( 32.80)	Acc@5  69.00 ( 66.80)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 2.5742e+00 (2.5755e+00)	Acc@1  35.00 ( 32.82)	Acc@5  64.00 ( 66.51)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.5703e+00 (2.5598e+00)	Acc@1  33.00 ( 33.39)	Acc@5  71.00 ( 66.74)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 2.9004e+00 (2.5654e+00)	Acc@1  27.00 ( 33.17)	Acc@5  57.00 ( 66.35)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.8398e+00 (2.5754e+00)	Acc@1  31.00 ( 33.02)	Acc@5  65.00 ( 66.09)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 2.7578e+00 (2.5674e+00)	Acc@1  32.00 ( 33.30)	Acc@5  68.00 ( 66.36)
 * Acc@1 33.140 Acc@5 66.410
### epoch[6] execution time: 29.117103099822998
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.223 ( 0.223)	Data  0.146 ( 0.146)	Loss 2.0195e+00 (2.0195e+00)	Acc@1  46.88 ( 46.88)	Acc@5  75.78 ( 75.78)
Epoch: [7][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.014)	Loss 2.1465e+00 (2.1731e+00)	Acc@1  42.97 ( 41.41)	Acc@5  71.09 ( 74.43)
Epoch: [7][ 20/391]	Time  0.069 ( 0.074)	Data  0.001 ( 0.008)	Loss 2.1914e+00 (2.1873e+00)	Acc@1  42.97 ( 41.22)	Acc@5  71.88 ( 74.33)
Epoch: [7][ 30/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.006)	Loss 1.9766e+00 (2.1865e+00)	Acc@1  46.88 ( 40.98)	Acc@5  75.00 ( 74.07)
Epoch: [7][ 40/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.005)	Loss 2.3262e+00 (2.2039e+00)	Acc@1  37.50 ( 40.57)	Acc@5  67.19 ( 73.91)
Epoch: [7][ 50/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 2.4590e+00 (2.2289e+00)	Acc@1  33.59 ( 39.63)	Acc@5  68.75 ( 73.25)
Epoch: [7][ 60/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 2.2129e+00 (2.2315e+00)	Acc@1  33.59 ( 39.16)	Acc@5  76.56 ( 73.00)
Epoch: [7][ 70/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.2461e+00 (2.2310e+00)	Acc@1  41.41 ( 39.07)	Acc@5  71.09 ( 73.07)
Epoch: [7][ 80/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.4062e+00 (2.2320e+00)	Acc@1  36.72 ( 39.04)	Acc@5  67.97 ( 73.03)
Epoch: [7][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.3574e+00 (2.2377e+00)	Acc@1  38.28 ( 39.13)	Acc@5  68.75 ( 72.91)
Epoch: [7][100/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.1328e+00 (2.2354e+00)	Acc@1  50.00 ( 39.16)	Acc@5  75.00 ( 72.89)
Epoch: [7][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.3008e+00 (2.2366e+00)	Acc@1  36.72 ( 39.03)	Acc@5  74.22 ( 72.84)
Epoch: [7][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.3457e+00 (2.2406e+00)	Acc@1  34.38 ( 39.13)	Acc@5  70.31 ( 72.67)
Epoch: [7][130/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.1875e+00 (2.2361e+00)	Acc@1  39.06 ( 39.21)	Acc@5  75.78 ( 72.85)
Epoch: [7][140/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1426e+00 (2.2398e+00)	Acc@1  46.88 ( 39.11)	Acc@5  74.22 ( 72.81)
Epoch: [7][150/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2910e+00 (2.2388e+00)	Acc@1  35.94 ( 39.16)	Acc@5  69.53 ( 72.85)
Epoch: [7][160/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3828e+00 (2.2393e+00)	Acc@1  35.16 ( 39.15)	Acc@5  67.19 ( 72.81)
Epoch: [7][170/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5449e+00 (2.2467e+00)	Acc@1  32.03 ( 39.06)	Acc@5  67.19 ( 72.68)
Epoch: [7][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2480e+00 (2.2468e+00)	Acc@1  43.75 ( 39.10)	Acc@5  72.66 ( 72.63)
Epoch: [7][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9844e+00 (2.2412e+00)	Acc@1  42.97 ( 39.20)	Acc@5  77.34 ( 72.67)
Epoch: [7][200/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3262e+00 (2.2445e+00)	Acc@1  38.28 ( 39.20)	Acc@5  72.66 ( 72.66)
Epoch: [7][210/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2969e+00 (2.2453e+00)	Acc@1  37.50 ( 39.22)	Acc@5  71.09 ( 72.65)
Epoch: [7][220/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1367e+00 (2.2442e+00)	Acc@1  40.62 ( 39.23)	Acc@5  72.66 ( 72.71)
Epoch: [7][230/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1680e+00 (2.2415e+00)	Acc@1  40.62 ( 39.26)	Acc@5  74.22 ( 72.76)
Epoch: [7][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1973e+00 (2.2410e+00)	Acc@1  37.50 ( 39.27)	Acc@5  69.53 ( 72.78)
Epoch: [7][250/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3027e+00 (2.2394e+00)	Acc@1  33.59 ( 39.29)	Acc@5  70.31 ( 72.81)
Epoch: [7][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9531e+00 (2.2377e+00)	Acc@1  37.50 ( 39.26)	Acc@5  81.25 ( 72.88)
Epoch: [7][270/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3711e+00 (2.2415e+00)	Acc@1  31.25 ( 39.18)	Acc@5  73.44 ( 72.82)
Epoch: [7][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1270e+00 (2.2404e+00)	Acc@1  46.88 ( 39.23)	Acc@5  75.00 ( 72.86)
Epoch: [7][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4023e+00 (2.2422e+00)	Acc@1  39.84 ( 39.24)	Acc@5  64.84 ( 72.82)
Epoch: [7][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9678e+00 (2.2426e+00)	Acc@1  43.75 ( 39.20)	Acc@5  76.56 ( 72.81)
Epoch: [7][310/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2324e+00 (2.2394e+00)	Acc@1  39.84 ( 39.29)	Acc@5  72.66 ( 72.87)
Epoch: [7][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9463e+00 (2.2382e+00)	Acc@1  42.19 ( 39.30)	Acc@5  77.34 ( 72.93)
Epoch: [7][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2695e+00 (2.2375e+00)	Acc@1  39.06 ( 39.30)	Acc@5  72.66 ( 72.95)
Epoch: [7][340/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5742e+00 (2.2381e+00)	Acc@1  30.47 ( 39.30)	Acc@5  64.06 ( 72.91)
Epoch: [7][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.3438e+00 (2.2372e+00)	Acc@1  39.06 ( 39.27)	Acc@5  68.75 ( 72.97)
Epoch: [7][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.1719e+00 (2.2374e+00)	Acc@1  40.62 ( 39.24)	Acc@5  73.44 ( 72.99)
Epoch: [7][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.3555e+00 (2.2388e+00)	Acc@1  32.03 ( 39.20)	Acc@5  71.88 ( 72.98)
Epoch: [7][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.1172e+00 (2.2389e+00)	Acc@1  40.62 ( 39.20)	Acc@5  75.78 ( 72.97)
Epoch: [7][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9648e+00 (2.2403e+00)	Acc@1  47.50 ( 39.18)	Acc@5  75.00 ( 72.97)
## e[7] optimizer.zero_grad (sum) time: 0.4152977466583252
## e[7]       loss.backward (sum) time: 7.326713800430298
## e[7]      optimizer.step (sum) time: 3.6343982219696045
## epoch[7] training(only) time: 26.217482805252075
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 2.1367e+00 (2.1367e+00)	Acc@1  44.00 ( 44.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 2.2637e+00 (2.2222e+00)	Acc@1  38.00 ( 40.73)	Acc@5  74.00 ( 74.36)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.7812e+00 (2.2005e+00)	Acc@1  49.00 ( 40.24)	Acc@5  82.00 ( 74.24)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.3926e+00 (2.1969e+00)	Acc@1  37.00 ( 40.32)	Acc@5  64.00 ( 74.13)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 2.3652e+00 (2.2053e+00)	Acc@1  41.00 ( 40.05)	Acc@5  64.00 ( 73.61)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 2.2168e+00 (2.2031e+00)	Acc@1  38.00 ( 40.27)	Acc@5  68.00 ( 73.47)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.0781e+00 (2.1938e+00)	Acc@1  39.00 ( 40.46)	Acc@5  80.00 ( 73.69)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 2.4160e+00 (2.2043e+00)	Acc@1  38.00 ( 40.20)	Acc@5  71.00 ( 73.35)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.4727e+00 (2.2182e+00)	Acc@1  34.00 ( 39.95)	Acc@5  65.00 ( 72.99)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.2773e+00 (2.2093e+00)	Acc@1  38.00 ( 40.20)	Acc@5  69.00 ( 73.20)
 * Acc@1 40.260 Acc@5 73.100
### epoch[7] execution time: 29.068726062774658
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.225 ( 0.225)	Data  0.149 ( 0.149)	Loss 1.8662e+00 (1.8662e+00)	Acc@1  53.12 ( 53.12)	Acc@5  84.38 ( 84.38)
Epoch: [8][ 10/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.014)	Loss 2.1738e+00 (2.0625e+00)	Acc@1  44.53 ( 44.18)	Acc@5  76.56 ( 76.78)
Epoch: [8][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.008)	Loss 2.0527e+00 (2.0868e+00)	Acc@1  41.41 ( 43.75)	Acc@5  72.66 ( 75.74)
Epoch: [8][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.9209e+00 (2.0646e+00)	Acc@1  47.66 ( 44.00)	Acc@5  79.69 ( 76.01)
Epoch: [8][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 2.0371e+00 (2.0750e+00)	Acc@1  45.31 ( 43.94)	Acc@5  75.78 ( 76.03)
Epoch: [8][ 50/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.004)	Loss 2.2246e+00 (2.0772e+00)	Acc@1  41.41 ( 43.58)	Acc@5  74.22 ( 76.03)
Epoch: [8][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.004)	Loss 2.0957e+00 (2.0955e+00)	Acc@1  43.75 ( 43.14)	Acc@5  74.22 ( 75.67)
Epoch: [8][ 70/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 2.3184e+00 (2.1094e+00)	Acc@1  36.72 ( 42.74)	Acc@5  71.88 ( 75.44)
Epoch: [8][ 80/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.2031e+00 (2.1101e+00)	Acc@1  43.75 ( 42.53)	Acc@5  74.22 ( 75.30)
Epoch: [8][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.0527e+00 (2.1052e+00)	Acc@1  40.62 ( 42.57)	Acc@5  75.00 ( 75.37)
Epoch: [8][100/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.2168e+00 (2.1118e+00)	Acc@1  36.72 ( 42.40)	Acc@5  76.56 ( 75.33)
Epoch: [8][110/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.3047e+00 (2.1078e+00)	Acc@1  39.84 ( 42.37)	Acc@5  74.22 ( 75.37)
Epoch: [8][120/391]	Time  0.077 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.0781e+00 (2.1103e+00)	Acc@1  42.97 ( 42.25)	Acc@5  78.91 ( 75.39)
Epoch: [8][130/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.2754e+00 (2.1165e+00)	Acc@1  37.50 ( 42.01)	Acc@5  75.00 ( 75.32)
Epoch: [8][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.1250e+00 (2.1216e+00)	Acc@1  41.41 ( 41.88)	Acc@5  79.69 ( 75.32)
Epoch: [8][150/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.2773e+00 (2.1240e+00)	Acc@1  38.28 ( 41.77)	Acc@5  69.53 ( 75.24)
Epoch: [8][160/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.1543e+00 (2.1266e+00)	Acc@1  42.97 ( 41.85)	Acc@5  71.09 ( 75.15)
Epoch: [8][170/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.3027e+00 (2.1354e+00)	Acc@1  42.19 ( 41.76)	Acc@5  71.88 ( 75.00)
Epoch: [8][180/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.0527e+00 (2.1375e+00)	Acc@1  42.19 ( 41.71)	Acc@5  73.44 ( 74.94)
Epoch: [8][190/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.3145e+00 (2.1405e+00)	Acc@1  42.97 ( 41.69)	Acc@5  68.75 ( 74.87)
Epoch: [8][200/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2090e+00 (2.1432e+00)	Acc@1  35.94 ( 41.69)	Acc@5  74.22 ( 74.81)
Epoch: [8][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0352e+00 (2.1413e+00)	Acc@1  44.53 ( 41.70)	Acc@5  75.00 ( 74.85)
Epoch: [8][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1680e+00 (2.1412e+00)	Acc@1  38.28 ( 41.64)	Acc@5  77.34 ( 74.90)
Epoch: [8][230/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9863e+00 (2.1385e+00)	Acc@1  47.66 ( 41.76)	Acc@5  75.00 ( 74.93)
Epoch: [8][240/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1426e+00 (2.1354e+00)	Acc@1  35.94 ( 41.82)	Acc@5  75.00 ( 75.02)
Epoch: [8][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8564e+00 (2.1292e+00)	Acc@1  50.00 ( 42.03)	Acc@5  80.47 ( 75.14)
Epoch: [8][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0996e+00 (2.1291e+00)	Acc@1  46.88 ( 42.09)	Acc@5  71.88 ( 75.12)
Epoch: [8][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1270e+00 (2.1292e+00)	Acc@1  42.19 ( 42.09)	Acc@5  78.91 ( 75.18)
Epoch: [8][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3281e+00 (2.1300e+00)	Acc@1  40.62 ( 42.05)	Acc@5  73.44 ( 75.19)
Epoch: [8][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0215e+00 (2.1325e+00)	Acc@1  42.97 ( 41.98)	Acc@5  76.56 ( 75.12)
Epoch: [8][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4688e+00 (2.1346e+00)	Acc@1  31.25 ( 41.91)	Acc@5  66.41 ( 75.10)
Epoch: [8][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2734e+00 (2.1337e+00)	Acc@1  40.62 ( 41.94)	Acc@5  75.00 ( 75.12)
Epoch: [8][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0684e+00 (2.1338e+00)	Acc@1  41.41 ( 41.89)	Acc@5  80.47 ( 75.14)
Epoch: [8][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9609e+00 (2.1330e+00)	Acc@1  42.97 ( 41.93)	Acc@5  79.69 ( 75.15)
Epoch: [8][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3828e+00 (2.1325e+00)	Acc@1  35.16 ( 41.95)	Acc@5  72.66 ( 75.17)
Epoch: [8][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2324e+00 (2.1322e+00)	Acc@1  42.97 ( 41.95)	Acc@5  71.88 ( 75.21)
Epoch: [8][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.0840e+00 (2.1304e+00)	Acc@1  45.31 ( 42.00)	Acc@5  78.91 ( 75.23)
Epoch: [8][370/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.3730e+00 (2.1311e+00)	Acc@1  41.41 ( 41.98)	Acc@5  67.97 ( 75.25)
Epoch: [8][380/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.3633e+00 (2.1307e+00)	Acc@1  33.59 ( 41.95)	Acc@5  73.44 ( 75.31)
Epoch: [8][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.0859e+00 (2.1314e+00)	Acc@1  40.00 ( 41.96)	Acc@5  80.00 ( 75.32)
## e[8] optimizer.zero_grad (sum) time: 0.41410207748413086
## e[8]       loss.backward (sum) time: 7.39241361618042
## e[8]      optimizer.step (sum) time: 3.558004140853882
## epoch[8] training(only) time: 26.231279611587524
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 2.3086e+00 (2.3086e+00)	Acc@1  38.00 ( 38.00)	Acc@5  68.00 ( 68.00)
Test: [ 10/100]	Time  0.028 ( 0.038)	Loss 2.2285e+00 (2.2987e+00)	Acc@1  40.00 ( 38.64)	Acc@5  75.00 ( 72.91)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.8594e+00 (2.2606e+00)	Acc@1  44.00 ( 39.76)	Acc@5  81.00 ( 73.19)
Test: [ 30/100]	Time  0.033 ( 0.031)	Loss 2.2637e+00 (2.2638e+00)	Acc@1  36.00 ( 39.65)	Acc@5  72.00 ( 72.71)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.1484e+00 (2.2518e+00)	Acc@1  38.00 ( 39.39)	Acc@5  77.00 ( 72.93)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 2.1758e+00 (2.2554e+00)	Acc@1  45.00 ( 39.37)	Acc@5  66.00 ( 72.80)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 2.0957e+00 (2.2382e+00)	Acc@1  47.00 ( 39.98)	Acc@5  77.00 ( 73.10)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 2.4004e+00 (2.2445e+00)	Acc@1  41.00 ( 39.80)	Acc@5  72.00 ( 72.80)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 2.3613e+00 (2.2588e+00)	Acc@1  40.00 ( 39.47)	Acc@5  69.00 ( 72.42)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 2.3672e+00 (2.2569e+00)	Acc@1  42.00 ( 39.82)	Acc@5  69.00 ( 72.51)
 * Acc@1 40.180 Acc@5 72.610
### epoch[8] execution time: 29.072078466415405
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.219 ( 0.219)	Data  0.143 ( 0.143)	Loss 2.3477e+00 (2.3477e+00)	Acc@1  34.38 ( 34.38)	Acc@5  71.09 ( 71.09)
Epoch: [9][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.014)	Loss 1.7080e+00 (2.0196e+00)	Acc@1  52.34 ( 43.96)	Acc@5  82.81 ( 77.34)
Epoch: [9][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.008)	Loss 2.1523e+00 (2.0195e+00)	Acc@1  39.84 ( 44.08)	Acc@5  76.56 ( 78.05)
Epoch: [9][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.9902e+00 (2.0279e+00)	Acc@1  40.62 ( 44.03)	Acc@5  78.12 ( 77.87)
Epoch: [9][ 40/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.005)	Loss 2.3281e+00 (2.0320e+00)	Acc@1  35.16 ( 43.73)	Acc@5  71.88 ( 77.53)
Epoch: [9][ 50/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.004)	Loss 2.1543e+00 (2.0364e+00)	Acc@1  44.53 ( 43.58)	Acc@5  72.66 ( 77.01)
Epoch: [9][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 2.1270e+00 (2.0413e+00)	Acc@1  39.06 ( 43.31)	Acc@5  76.56 ( 77.05)
Epoch: [9][ 70/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.0840e+00 (2.0457e+00)	Acc@1  42.97 ( 43.19)	Acc@5  75.78 ( 76.86)
Epoch: [9][ 80/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.8857e+00 (2.0437e+00)	Acc@1  50.78 ( 43.32)	Acc@5  81.25 ( 76.95)
Epoch: [9][ 90/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.2949e+00 (2.0385e+00)	Acc@1  40.62 ( 43.29)	Acc@5  72.66 ( 77.15)
Epoch: [9][100/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8467e+00 (2.0313e+00)	Acc@1  46.88 ( 43.57)	Acc@5  75.00 ( 77.13)
Epoch: [9][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.0156e+00 (2.0258e+00)	Acc@1  42.97 ( 43.71)	Acc@5  78.91 ( 77.27)
Epoch: [9][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.2090e+00 (2.0244e+00)	Acc@1  38.28 ( 43.78)	Acc@5  78.91 ( 77.49)
Epoch: [9][130/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.2305e+00 (2.0252e+00)	Acc@1  36.72 ( 43.83)	Acc@5  75.00 ( 77.48)
Epoch: [9][140/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9092e+00 (2.0286e+00)	Acc@1  44.53 ( 43.73)	Acc@5  76.56 ( 77.37)
Epoch: [9][150/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7891e+00 (2.0249e+00)	Acc@1  48.44 ( 43.86)	Acc@5  82.03 ( 77.44)
Epoch: [9][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2656e+00 (2.0306e+00)	Acc@1  34.38 ( 43.70)	Acc@5  71.09 ( 77.37)
Epoch: [9][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8457e+00 (2.0312e+00)	Acc@1  48.44 ( 43.66)	Acc@5  82.81 ( 77.31)
Epoch: [9][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.3105e+00 (2.0311e+00)	Acc@1  41.41 ( 43.66)	Acc@5  75.00 ( 77.28)
Epoch: [9][190/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0156e+00 (2.0333e+00)	Acc@1  46.88 ( 43.62)	Acc@5  78.91 ( 77.28)
Epoch: [9][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0957e+00 (2.0333e+00)	Acc@1  42.97 ( 43.66)	Acc@5  75.00 ( 77.28)
Epoch: [9][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0996e+00 (2.0358e+00)	Acc@1  44.53 ( 43.61)	Acc@5  67.97 ( 77.19)
Epoch: [9][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9902e+00 (2.0331e+00)	Acc@1  43.75 ( 43.64)	Acc@5  82.03 ( 77.28)
Epoch: [9][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2793e+00 (2.0340e+00)	Acc@1  41.41 ( 43.72)	Acc@5  71.88 ( 77.24)
Epoch: [9][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2578e+00 (2.0342e+00)	Acc@1  41.41 ( 43.81)	Acc@5  74.22 ( 77.28)
Epoch: [9][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0703e+00 (2.0324e+00)	Acc@1  42.97 ( 43.86)	Acc@5  76.56 ( 77.33)
Epoch: [9][260/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2422e+00 (2.0339e+00)	Acc@1  42.97 ( 43.81)	Acc@5  73.44 ( 77.33)
Epoch: [9][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0781e+00 (2.0373e+00)	Acc@1  42.97 ( 43.68)	Acc@5  76.56 ( 77.26)
Epoch: [9][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0664e+00 (2.0385e+00)	Acc@1  39.84 ( 43.65)	Acc@5  78.12 ( 77.20)
Epoch: [9][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0234e+00 (2.0363e+00)	Acc@1  47.66 ( 43.72)	Acc@5  74.22 ( 77.26)
Epoch: [9][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0195e+00 (2.0372e+00)	Acc@1  41.41 ( 43.64)	Acc@5  78.91 ( 77.28)
Epoch: [9][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9355e+00 (2.0351e+00)	Acc@1  46.88 ( 43.65)	Acc@5  78.12 ( 77.31)
Epoch: [9][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0820e+00 (2.0360e+00)	Acc@1  41.41 ( 43.62)	Acc@5  76.56 ( 77.28)
Epoch: [9][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0840e+00 (2.0369e+00)	Acc@1  43.75 ( 43.63)	Acc@5  77.34 ( 77.25)
Epoch: [9][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9014e+00 (2.0329e+00)	Acc@1  47.66 ( 43.71)	Acc@5  80.47 ( 77.31)
Epoch: [9][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9883e+00 (2.0355e+00)	Acc@1  48.44 ( 43.68)	Acc@5  77.34 ( 77.26)
Epoch: [9][360/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.7832e+00 (2.0335e+00)	Acc@1  50.78 ( 43.78)	Acc@5  84.38 ( 77.28)
Epoch: [9][370/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.0586e+00 (2.0347e+00)	Acc@1  45.31 ( 43.76)	Acc@5  76.56 ( 77.24)
Epoch: [9][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.1133e+00 (2.0337e+00)	Acc@1  41.41 ( 43.83)	Acc@5  72.66 ( 77.24)
Epoch: [9][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.0254e+00 (2.0321e+00)	Acc@1  48.75 ( 43.91)	Acc@5  73.75 ( 77.23)
## e[9] optimizer.zero_grad (sum) time: 0.41642308235168457
## e[9]       loss.backward (sum) time: 7.368858337402344
## e[9]      optimizer.step (sum) time: 3.5866801738739014
## epoch[9] training(only) time: 26.220263957977295
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 2.1270e+00 (2.1270e+00)	Acc@1  43.00 ( 43.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 2.1992e+00 (2.2456e+00)	Acc@1  44.00 ( 41.91)	Acc@5  78.00 ( 75.09)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.9600e+00 (2.2128e+00)	Acc@1  47.00 ( 42.29)	Acc@5  81.00 ( 75.38)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.3281e+00 (2.2045e+00)	Acc@1  39.00 ( 42.03)	Acc@5  69.00 ( 74.97)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.1582e+00 (2.1928e+00)	Acc@1  43.00 ( 42.22)	Acc@5  77.00 ( 74.83)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.0723e+00 (2.1983e+00)	Acc@1  46.00 ( 42.49)	Acc@5  71.00 ( 74.51)
Test: [ 60/100]	Time  0.029 ( 0.028)	Loss 2.0566e+00 (2.1837e+00)	Acc@1  46.00 ( 42.56)	Acc@5  77.00 ( 74.56)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 2.4160e+00 (2.1981e+00)	Acc@1  38.00 ( 42.56)	Acc@5  72.00 ( 74.27)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.4121e+00 (2.2126e+00)	Acc@1  37.00 ( 42.42)	Acc@5  70.00 ( 73.96)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.3438e+00 (2.2086e+00)	Acc@1  40.00 ( 42.46)	Acc@5  70.00 ( 73.87)
 * Acc@1 42.310 Acc@5 73.780
### epoch[9] execution time: 29.035389184951782
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.220 ( 0.220)	Data  0.146 ( 0.146)	Loss 2.1582e+00 (2.1582e+00)	Acc@1  42.97 ( 42.97)	Acc@5  73.44 ( 73.44)
Epoch: [10][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.014)	Loss 1.8701e+00 (2.0370e+00)	Acc@1  46.09 ( 46.09)	Acc@5  81.25 ( 77.41)
Epoch: [10][ 20/391]	Time  0.069 ( 0.073)	Data  0.001 ( 0.008)	Loss 2.0605e+00 (2.0321e+00)	Acc@1  42.97 ( 45.46)	Acc@5  73.44 ( 77.57)
Epoch: [10][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 2.0527e+00 (2.0128e+00)	Acc@1  47.66 ( 45.89)	Acc@5  72.66 ( 77.65)
Epoch: [10][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.005)	Loss 2.1055e+00 (2.0115e+00)	Acc@1  47.66 ( 45.94)	Acc@5  75.00 ( 77.67)
Epoch: [10][ 50/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.8936e+00 (1.9987e+00)	Acc@1  44.53 ( 45.70)	Acc@5  82.03 ( 77.97)
Epoch: [10][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.9785e+00 (1.9877e+00)	Acc@1  46.09 ( 45.90)	Acc@5  81.25 ( 78.24)
Epoch: [10][ 70/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.9531e+00 (1.9845e+00)	Acc@1  42.19 ( 45.90)	Acc@5  82.03 ( 78.20)
Epoch: [10][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.8516e+00 (1.9746e+00)	Acc@1  45.31 ( 45.92)	Acc@5  82.81 ( 78.45)
Epoch: [10][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.0762e+00 (1.9760e+00)	Acc@1  44.53 ( 45.78)	Acc@5  78.12 ( 78.42)
Epoch: [10][100/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.0293e+00 (1.9699e+00)	Acc@1  45.31 ( 45.86)	Acc@5  78.12 ( 78.56)
Epoch: [10][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8779e+00 (1.9683e+00)	Acc@1  43.75 ( 45.90)	Acc@5  80.47 ( 78.60)
Epoch: [10][120/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2285e+00 (1.9725e+00)	Acc@1  42.19 ( 45.75)	Acc@5  73.44 ( 78.52)
Epoch: [10][130/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7754e+00 (1.9704e+00)	Acc@1  50.00 ( 45.70)	Acc@5  82.03 ( 78.54)
Epoch: [10][140/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1582e+00 (1.9722e+00)	Acc@1  41.41 ( 45.55)	Acc@5  75.00 ( 78.49)
Epoch: [10][150/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0645e+00 (1.9714e+00)	Acc@1  39.06 ( 45.45)	Acc@5  77.34 ( 78.47)
Epoch: [10][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6895e+00 (1.9690e+00)	Acc@1  55.47 ( 45.59)	Acc@5  82.03 ( 78.53)
Epoch: [10][170/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0723e+00 (1.9657e+00)	Acc@1  42.97 ( 45.67)	Acc@5  75.78 ( 78.57)
Epoch: [10][180/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2227e+00 (1.9623e+00)	Acc@1  35.94 ( 45.83)	Acc@5  67.19 ( 78.61)
Epoch: [10][190/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2559e+00 (1.9630e+00)	Acc@1  39.84 ( 45.84)	Acc@5  71.88 ( 78.53)
Epoch: [10][200/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9238e+00 (1.9681e+00)	Acc@1  45.31 ( 45.72)	Acc@5  78.12 ( 78.45)
Epoch: [10][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9785e+00 (1.9703e+00)	Acc@1  49.22 ( 45.63)	Acc@5  77.34 ( 78.46)
Epoch: [10][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7217e+00 (1.9647e+00)	Acc@1  50.78 ( 45.68)	Acc@5  81.25 ( 78.54)
Epoch: [10][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9404e+00 (1.9677e+00)	Acc@1  50.00 ( 45.56)	Acc@5  82.03 ( 78.49)
Epoch: [10][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0996e+00 (1.9641e+00)	Acc@1  43.75 ( 45.65)	Acc@5  71.88 ( 78.53)
Epoch: [10][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9014e+00 (1.9634e+00)	Acc@1  53.12 ( 45.67)	Acc@5  78.91 ( 78.53)
Epoch: [10][260/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0859e+00 (1.9650e+00)	Acc@1  47.66 ( 45.61)	Acc@5  78.91 ( 78.47)
Epoch: [10][270/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7422e+00 (1.9640e+00)	Acc@1  52.34 ( 45.62)	Acc@5  85.16 ( 78.49)
Epoch: [10][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7812e+00 (1.9615e+00)	Acc@1  51.56 ( 45.72)	Acc@5  82.81 ( 78.54)
Epoch: [10][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9268e+00 (1.9605e+00)	Acc@1  42.19 ( 45.75)	Acc@5  78.91 ( 78.54)
Epoch: [10][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0488e+00 (1.9594e+00)	Acc@1  48.44 ( 45.79)	Acc@5  82.81 ( 78.56)
Epoch: [10][310/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6748e+00 (1.9582e+00)	Acc@1  52.34 ( 45.85)	Acc@5  86.72 ( 78.57)
Epoch: [10][320/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1035e+00 (1.9589e+00)	Acc@1  37.50 ( 45.76)	Acc@5  78.12 ( 78.59)
Epoch: [10][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9697e+00 (1.9583e+00)	Acc@1  43.75 ( 45.74)	Acc@5  83.59 ( 78.62)
Epoch: [10][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8877e+00 (1.9571e+00)	Acc@1  52.34 ( 45.81)	Acc@5  82.03 ( 78.65)
Epoch: [10][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6846e+00 (1.9573e+00)	Acc@1  50.00 ( 45.82)	Acc@5  83.59 ( 78.61)
Epoch: [10][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.0820e+00 (1.9567e+00)	Acc@1  42.97 ( 45.83)	Acc@5  76.56 ( 78.62)
Epoch: [10][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9180e+00 (1.9573e+00)	Acc@1  50.00 ( 45.83)	Acc@5  78.91 ( 78.62)
Epoch: [10][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.1094e+00 (1.9555e+00)	Acc@1  42.19 ( 45.85)	Acc@5  74.22 ( 78.65)
Epoch: [10][390/391]	Time  0.057 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6562e+00 (1.9549e+00)	Acc@1  52.50 ( 45.89)	Acc@5  87.50 ( 78.67)
## e[10] optimizer.zero_grad (sum) time: 0.4089033603668213
## e[10]       loss.backward (sum) time: 7.343212127685547
## e[10]      optimizer.step (sum) time: 3.558713912963867
## epoch[10] training(only) time: 26.11919116973877
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.9785e+00 (1.9785e+00)	Acc@1  55.00 ( 55.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.025 ( 0.038)	Loss 1.8984e+00 (2.0218e+00)	Acc@1  46.00 ( 45.09)	Acc@5  80.00 ( 76.73)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.8252e+00 (2.0397e+00)	Acc@1  44.00 ( 43.71)	Acc@5  83.00 ( 77.10)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 2.2051e+00 (2.0543e+00)	Acc@1  37.00 ( 43.48)	Acc@5  72.00 ( 76.74)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 2.3984e+00 (2.0600e+00)	Acc@1  39.00 ( 43.37)	Acc@5  71.00 ( 76.49)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 2.0742e+00 (2.0780e+00)	Acc@1  44.00 ( 42.94)	Acc@5  68.00 ( 75.75)
Test: [ 60/100]	Time  0.029 ( 0.028)	Loss 2.0234e+00 (2.0796e+00)	Acc@1  47.00 ( 42.80)	Acc@5  77.00 ( 75.69)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 2.3496e+00 (2.0913e+00)	Acc@1  36.00 ( 42.54)	Acc@5  70.00 ( 75.42)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 2.2480e+00 (2.0990e+00)	Acc@1  39.00 ( 42.26)	Acc@5  74.00 ( 75.28)
Test: [ 90/100]	Time  0.026 ( 0.027)	Loss 2.0176e+00 (2.0939e+00)	Acc@1  51.00 ( 42.62)	Acc@5  74.00 ( 75.38)
 * Acc@1 42.720 Acc@5 75.500
### epoch[10] execution time: 28.924826860427856
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.226 ( 0.226)	Data  0.151 ( 0.151)	Loss 1.6592e+00 (1.6592e+00)	Acc@1  52.34 ( 52.34)	Acc@5  85.94 ( 85.94)
Epoch: [11][ 10/391]	Time  0.070 ( 0.081)	Data  0.001 ( 0.015)	Loss 1.8730e+00 (1.8580e+00)	Acc@1  47.66 ( 48.37)	Acc@5  78.91 ( 81.18)
Epoch: [11][ 20/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.7500e+00 (1.8209e+00)	Acc@1  48.44 ( 49.55)	Acc@5  82.03 ( 80.95)
Epoch: [11][ 30/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.006)	Loss 1.8193e+00 (1.8403e+00)	Acc@1  50.78 ( 48.92)	Acc@5  81.25 ( 80.52)
Epoch: [11][ 40/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.005)	Loss 1.6309e+00 (1.8216e+00)	Acc@1  53.12 ( 49.09)	Acc@5  86.72 ( 81.14)
Epoch: [11][ 50/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.004)	Loss 2.0039e+00 (1.8444e+00)	Acc@1  40.62 ( 48.79)	Acc@5  80.47 ( 80.85)
Epoch: [11][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 2.1172e+00 (1.8551e+00)	Acc@1  46.88 ( 48.54)	Acc@5  75.78 ( 80.83)
Epoch: [11][ 70/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.7852e+00 (1.8692e+00)	Acc@1  50.00 ( 48.07)	Acc@5  81.25 ( 80.47)
Epoch: [11][ 80/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5605e+00 (1.8708e+00)	Acc@1  53.12 ( 48.08)	Acc@5  85.94 ( 80.34)
Epoch: [11][ 90/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.6582e+00 (1.8712e+00)	Acc@1  52.34 ( 47.99)	Acc@5  89.06 ( 80.45)
Epoch: [11][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.8389e+00 (1.8782e+00)	Acc@1  46.09 ( 47.75)	Acc@5  79.69 ( 80.35)
Epoch: [11][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.7305e+00 (1.8727e+00)	Acc@1  47.66 ( 47.83)	Acc@5  85.16 ( 80.42)
Epoch: [11][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.9990e+00 (1.8728e+00)	Acc@1  46.09 ( 47.84)	Acc@5  80.47 ( 80.49)
Epoch: [11][130/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8887e+00 (1.8694e+00)	Acc@1  45.31 ( 48.00)	Acc@5  79.69 ( 80.47)
Epoch: [11][140/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8955e+00 (1.8745e+00)	Acc@1  47.66 ( 47.79)	Acc@5  82.81 ( 80.41)
Epoch: [11][150/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8242e+00 (1.8827e+00)	Acc@1  51.56 ( 47.60)	Acc@5  77.34 ( 80.28)
Epoch: [11][160/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8955e+00 (1.8839e+00)	Acc@1  44.53 ( 47.65)	Acc@5  81.25 ( 80.20)
Epoch: [11][170/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.7793e+00 (1.8816e+00)	Acc@1  46.09 ( 47.62)	Acc@5  84.38 ( 80.31)
Epoch: [11][180/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5449e+00 (1.8747e+00)	Acc@1  53.12 ( 47.81)	Acc@5  84.38 ( 80.37)
Epoch: [11][190/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.7236e+00 (1.8785e+00)	Acc@1  58.59 ( 47.76)	Acc@5  82.81 ( 80.33)
Epoch: [11][200/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.1758e+00 (1.8810e+00)	Acc@1  43.75 ( 47.71)	Acc@5  75.78 ( 80.29)
Epoch: [11][210/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8389e+00 (1.8811e+00)	Acc@1  50.00 ( 47.65)	Acc@5  84.38 ( 80.37)
Epoch: [11][220/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9854e+00 (1.8809e+00)	Acc@1  45.31 ( 47.68)	Acc@5  78.12 ( 80.38)
Epoch: [11][230/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8389e+00 (1.8791e+00)	Acc@1  50.00 ( 47.73)	Acc@5  82.81 ( 80.39)
Epoch: [11][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9551e+00 (1.8812e+00)	Acc@1  44.53 ( 47.72)	Acc@5  75.00 ( 80.30)
Epoch: [11][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9492e+00 (1.8795e+00)	Acc@1  42.19 ( 47.73)	Acc@5  79.69 ( 80.31)
Epoch: [11][260/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0879e+00 (1.8801e+00)	Acc@1  48.44 ( 47.74)	Acc@5  75.00 ( 80.28)
Epoch: [11][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8018e+00 (1.8805e+00)	Acc@1  49.22 ( 47.70)	Acc@5  85.16 ( 80.26)
Epoch: [11][280/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0117e+00 (1.8817e+00)	Acc@1  44.53 ( 47.66)	Acc@5  76.56 ( 80.21)
Epoch: [11][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8789e+00 (1.8807e+00)	Acc@1  46.09 ( 47.69)	Acc@5  82.81 ( 80.23)
Epoch: [11][300/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6533e+00 (1.8784e+00)	Acc@1  51.56 ( 47.74)	Acc@5  85.16 ( 80.23)
Epoch: [11][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7715e+00 (1.8799e+00)	Acc@1  48.44 ( 47.68)	Acc@5  78.12 ( 80.16)
Epoch: [11][320/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0664e+00 (1.8782e+00)	Acc@1  44.53 ( 47.69)	Acc@5  75.78 ( 80.17)
Epoch: [11][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7734e+00 (1.8763e+00)	Acc@1  47.66 ( 47.71)	Acc@5  79.69 ( 80.20)
Epoch: [11][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6631e+00 (1.8759e+00)	Acc@1  51.56 ( 47.72)	Acc@5  85.16 ( 80.24)
Epoch: [11][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6602e+00 (1.8724e+00)	Acc@1  53.91 ( 47.80)	Acc@5  83.59 ( 80.26)
Epoch: [11][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.1484e+00 (1.8736e+00)	Acc@1  45.31 ( 47.81)	Acc@5  74.22 ( 80.24)
Epoch: [11][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9521e+00 (1.8739e+00)	Acc@1  43.75 ( 47.82)	Acc@5  79.69 ( 80.23)
Epoch: [11][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.8867e+00 (1.8725e+00)	Acc@1  47.66 ( 47.86)	Acc@5  78.12 ( 80.28)
Epoch: [11][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.8887e+00 (1.8722e+00)	Acc@1  51.25 ( 47.86)	Acc@5  76.25 ( 80.24)
## e[11] optimizer.zero_grad (sum) time: 0.4129621982574463
## e[11]       loss.backward (sum) time: 7.377631187438965
## e[11]      optimizer.step (sum) time: 3.5928831100463867
## epoch[11] training(only) time: 26.21647620201111
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 2.1016e+00 (2.1016e+00)	Acc@1  49.00 ( 49.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.025 ( 0.038)	Loss 2.0410e+00 (1.9536e+00)	Acc@1  41.00 ( 47.18)	Acc@5  78.00 ( 78.64)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.6533e+00 (1.9284e+00)	Acc@1  51.00 ( 46.86)	Acc@5  83.00 ( 79.14)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 2.2539e+00 (1.9466e+00)	Acc@1  44.00 ( 46.71)	Acc@5  76.00 ( 78.87)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.1270e+00 (1.9563e+00)	Acc@1  51.00 ( 46.37)	Acc@5  77.00 ( 78.56)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.6895e+00 (1.9594e+00)	Acc@1  52.00 ( 46.47)	Acc@5  81.00 ( 78.41)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.9170e+00 (1.9447e+00)	Acc@1  51.00 ( 46.87)	Acc@5  79.00 ( 78.72)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 2.0215e+00 (1.9484e+00)	Acc@1  47.00 ( 46.89)	Acc@5  74.00 ( 78.75)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.2383e+00 (1.9659e+00)	Acc@1  44.00 ( 46.44)	Acc@5  70.00 ( 78.35)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9531e+00 (1.9590e+00)	Acc@1  50.00 ( 46.54)	Acc@5  74.00 ( 78.45)
 * Acc@1 46.690 Acc@5 78.690
### epoch[11] execution time: 29.017696857452393
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.224 ( 0.224)	Data  0.148 ( 0.148)	Loss 1.5898e+00 (1.5898e+00)	Acc@1  57.81 ( 57.81)	Acc@5  82.03 ( 82.03)
Epoch: [12][ 10/391]	Time  0.071 ( 0.081)	Data  0.001 ( 0.014)	Loss 1.6211e+00 (1.7459e+00)	Acc@1  59.38 ( 53.27)	Acc@5  86.72 ( 81.32)
Epoch: [12][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.4336e+00 (1.7261e+00)	Acc@1  56.25 ( 52.05)	Acc@5  87.50 ( 82.22)
Epoch: [12][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.7012e+00 (1.7440e+00)	Acc@1  52.34 ( 51.16)	Acc@5  84.38 ( 82.16)
Epoch: [12][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.005)	Loss 2.0625e+00 (1.7637e+00)	Acc@1  47.66 ( 50.44)	Acc@5  78.91 ( 81.94)
Epoch: [12][ 50/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.7920e+00 (1.7726e+00)	Acc@1  49.22 ( 49.91)	Acc@5  78.12 ( 81.92)
Epoch: [12][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.8291e+00 (1.7670e+00)	Acc@1  53.12 ( 50.18)	Acc@5  81.25 ( 82.17)
Epoch: [12][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.8809e+00 (1.7743e+00)	Acc@1  46.09 ( 50.21)	Acc@5  82.81 ( 82.17)
Epoch: [12][ 80/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.6836e+00 (1.7801e+00)	Acc@1  53.12 ( 50.24)	Acc@5  82.03 ( 82.10)
Epoch: [12][ 90/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.1699e+00 (1.7907e+00)	Acc@1  44.53 ( 49.89)	Acc@5  70.31 ( 81.95)
Epoch: [12][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.9150e+00 (1.7936e+00)	Acc@1  45.31 ( 49.97)	Acc@5  81.25 ( 82.02)
Epoch: [12][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6641e+00 (1.7938e+00)	Acc@1  50.78 ( 49.85)	Acc@5  83.59 ( 82.06)
Epoch: [12][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.7227e+00 (1.8006e+00)	Acc@1  48.44 ( 49.63)	Acc@5  81.25 ( 81.82)
Epoch: [12][130/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5830e+00 (1.8019e+00)	Acc@1  52.34 ( 49.49)	Acc@5  88.28 ( 81.76)
Epoch: [12][140/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.9150e+00 (1.8081e+00)	Acc@1  51.56 ( 49.48)	Acc@5  80.47 ( 81.65)
Epoch: [12][150/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9307e+00 (1.8100e+00)	Acc@1  39.06 ( 49.50)	Acc@5  81.25 ( 81.68)
Epoch: [12][160/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5801e+00 (1.8141e+00)	Acc@1  50.78 ( 49.34)	Acc@5  87.50 ( 81.64)
Epoch: [12][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0137e+00 (1.8129e+00)	Acc@1  47.66 ( 49.46)	Acc@5  76.56 ( 81.62)
Epoch: [12][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.2148e+00 (1.8174e+00)	Acc@1  46.09 ( 49.36)	Acc@5  75.00 ( 81.48)
Epoch: [12][190/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7100e+00 (1.8157e+00)	Acc@1  50.78 ( 49.36)	Acc@5  82.03 ( 81.54)
Epoch: [12][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5635e+00 (1.8094e+00)	Acc@1  57.81 ( 49.54)	Acc@5  84.38 ( 81.64)
Epoch: [12][210/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8809e+00 (1.8069e+00)	Acc@1  50.78 ( 49.57)	Acc@5  78.12 ( 81.71)
Epoch: [12][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7246e+00 (1.8087e+00)	Acc@1  48.44 ( 49.52)	Acc@5  83.59 ( 81.68)
Epoch: [12][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8770e+00 (1.8085e+00)	Acc@1  46.09 ( 49.49)	Acc@5  76.56 ( 81.63)
Epoch: [12][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8887e+00 (1.8094e+00)	Acc@1  50.00 ( 49.44)	Acc@5  80.47 ( 81.63)
Epoch: [12][250/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6729e+00 (1.8080e+00)	Acc@1  47.66 ( 49.51)	Acc@5  82.03 ( 81.63)
Epoch: [12][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8281e+00 (1.8098e+00)	Acc@1  48.44 ( 49.48)	Acc@5  84.38 ( 81.56)
Epoch: [12][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4824e+00 (1.8111e+00)	Acc@1  61.72 ( 49.51)	Acc@5  83.59 ( 81.50)
Epoch: [12][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5898e+00 (1.8088e+00)	Acc@1  54.69 ( 49.54)	Acc@5  84.38 ( 81.52)
Epoch: [12][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9414e+00 (1.8081e+00)	Acc@1  43.75 ( 49.60)	Acc@5  77.34 ( 81.48)
Epoch: [12][300/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6982e+00 (1.8132e+00)	Acc@1  47.66 ( 49.48)	Acc@5  85.94 ( 81.40)
Epoch: [12][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8408e+00 (1.8136e+00)	Acc@1  52.34 ( 49.47)	Acc@5  78.12 ( 81.38)
Epoch: [12][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9365e+00 (1.8136e+00)	Acc@1  43.75 ( 49.43)	Acc@5  82.03 ( 81.39)
Epoch: [12][330/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5107e+00 (1.8129e+00)	Acc@1  50.00 ( 49.41)	Acc@5  89.84 ( 81.42)
Epoch: [12][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6367e+00 (1.8134e+00)	Acc@1  53.91 ( 49.39)	Acc@5  82.81 ( 81.39)
Epoch: [12][350/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8193e+00 (1.8134e+00)	Acc@1  54.69 ( 49.41)	Acc@5  79.69 ( 81.37)
Epoch: [12][360/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6816e+00 (1.8136e+00)	Acc@1  50.00 ( 49.39)	Acc@5  85.16 ( 81.36)
Epoch: [12][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5850e+00 (1.8127e+00)	Acc@1  53.12 ( 49.46)	Acc@5  85.94 ( 81.35)
Epoch: [12][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9209e+00 (1.8144e+00)	Acc@1  46.88 ( 49.42)	Acc@5  77.34 ( 81.31)
Epoch: [12][390/391]	Time  0.053 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.8945e+00 (1.8154e+00)	Acc@1  41.25 ( 49.35)	Acc@5  80.00 ( 81.30)
## e[12] optimizer.zero_grad (sum) time: 0.40845799446105957
## e[12]       loss.backward (sum) time: 7.37240195274353
## e[12]      optimizer.step (sum) time: 3.57660174369812
## epoch[12] training(only) time: 26.16148352622986
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.9248e+00 (1.9248e+00)	Acc@1  48.00 ( 48.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.9102e+00 (1.9300e+00)	Acc@1  45.00 ( 48.82)	Acc@5  82.00 ( 79.27)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.7500e+00 (1.9541e+00)	Acc@1  46.00 ( 48.00)	Acc@5  80.00 ( 78.62)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.9316e+00 (1.9377e+00)	Acc@1  48.00 ( 47.39)	Acc@5  79.00 ( 79.10)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.8408e+00 (1.9209e+00)	Acc@1  45.00 ( 47.17)	Acc@5  84.00 ( 79.56)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.8779e+00 (1.9309e+00)	Acc@1  48.00 ( 47.06)	Acc@5  75.00 ( 79.14)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 1.9619e+00 (1.9311e+00)	Acc@1  50.00 ( 47.20)	Acc@5  78.00 ( 79.15)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 2.0371e+00 (1.9370e+00)	Acc@1  43.00 ( 46.83)	Acc@5  77.00 ( 79.06)
Test: [ 80/100]	Time  0.031 ( 0.028)	Loss 2.1172e+00 (1.9488e+00)	Acc@1  45.00 ( 46.59)	Acc@5  78.00 ( 78.88)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0840e+00 (1.9405e+00)	Acc@1  47.00 ( 46.67)	Acc@5  77.00 ( 79.13)
 * Acc@1 46.850 Acc@5 79.210
### epoch[12] execution time: 28.985275506973267
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.216 ( 0.216)	Data  0.139 ( 0.139)	Loss 1.7344e+00 (1.7344e+00)	Acc@1  48.44 ( 48.44)	Acc@5  84.38 ( 84.38)
Epoch: [13][ 10/391]	Time  0.067 ( 0.080)	Data  0.001 ( 0.014)	Loss 1.8125e+00 (1.7250e+00)	Acc@1  50.78 ( 51.78)	Acc@5  78.91 ( 83.74)
Epoch: [13][ 20/391]	Time  0.068 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.7930e+00 (1.7336e+00)	Acc@1  50.00 ( 51.93)	Acc@5  80.47 ( 82.66)
Epoch: [13][ 30/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.8047e+00 (1.7440e+00)	Acc@1  46.88 ( 51.36)	Acc@5  82.03 ( 82.79)
Epoch: [13][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.7031e+00 (1.7602e+00)	Acc@1  52.34 ( 50.72)	Acc@5  80.47 ( 82.45)
Epoch: [13][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.7412e+00 (1.7541e+00)	Acc@1  52.34 ( 51.19)	Acc@5  81.25 ( 82.54)
Epoch: [13][ 60/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.7607e+00 (1.7471e+00)	Acc@1  53.12 ( 51.24)	Acc@5  82.81 ( 82.74)
Epoch: [13][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5127e+00 (1.7335e+00)	Acc@1  54.69 ( 51.84)	Acc@5  83.59 ( 82.87)
Epoch: [13][ 80/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.7871e+00 (1.7344e+00)	Acc@1  49.22 ( 51.78)	Acc@5  79.69 ( 82.86)
Epoch: [13][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.6123e+00 (1.7300e+00)	Acc@1  53.12 ( 51.81)	Acc@5  81.25 ( 82.85)
Epoch: [13][100/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8066e+00 (1.7344e+00)	Acc@1  46.09 ( 51.54)	Acc@5  85.16 ( 82.67)
Epoch: [13][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6709e+00 (1.7345e+00)	Acc@1  52.34 ( 51.65)	Acc@5  82.81 ( 82.57)
Epoch: [13][120/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6641e+00 (1.7340e+00)	Acc@1  50.78 ( 51.58)	Acc@5  81.25 ( 82.63)
Epoch: [13][130/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8164e+00 (1.7370e+00)	Acc@1  50.00 ( 51.47)	Acc@5  80.47 ( 82.62)
Epoch: [13][140/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7178e+00 (1.7378e+00)	Acc@1  49.22 ( 51.47)	Acc@5  85.94 ( 82.61)
Epoch: [13][150/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0215e+00 (1.7407e+00)	Acc@1  41.41 ( 51.27)	Acc@5  80.47 ( 82.60)
Epoch: [13][160/391]	Time  0.067 ( 0.067)	Data  0.004 ( 0.002)	Loss 1.9658e+00 (1.7442e+00)	Acc@1  46.88 ( 51.16)	Acc@5  75.00 ( 82.48)
Epoch: [13][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7949e+00 (1.7437e+00)	Acc@1  48.44 ( 51.12)	Acc@5  85.16 ( 82.59)
Epoch: [13][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9111e+00 (1.7503e+00)	Acc@1  49.22 ( 50.98)	Acc@5  80.47 ( 82.46)
Epoch: [13][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9297e+00 (1.7571e+00)	Acc@1  48.44 ( 50.90)	Acc@5  75.00 ( 82.33)
Epoch: [13][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8701e+00 (1.7600e+00)	Acc@1  52.34 ( 50.82)	Acc@5  78.91 ( 82.31)
Epoch: [13][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8027e+00 (1.7612e+00)	Acc@1  54.69 ( 50.84)	Acc@5  84.38 ( 82.33)
Epoch: [13][220/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7852e+00 (1.7631e+00)	Acc@1  51.56 ( 50.81)	Acc@5  78.91 ( 82.29)
Epoch: [13][230/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9619e+00 (1.7640e+00)	Acc@1  48.44 ( 50.77)	Acc@5  75.00 ( 82.25)
Epoch: [13][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4980e+00 (1.7636e+00)	Acc@1  57.03 ( 50.84)	Acc@5  82.03 ( 82.22)
Epoch: [13][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9863e+00 (1.7648e+00)	Acc@1  41.41 ( 50.79)	Acc@5  79.69 ( 82.19)
Epoch: [13][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7539e+00 (1.7627e+00)	Acc@1  48.44 ( 50.80)	Acc@5  82.03 ( 82.26)
Epoch: [13][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1074e+00 (1.7645e+00)	Acc@1  46.09 ( 50.73)	Acc@5  75.78 ( 82.20)
Epoch: [13][280/391]	Time  0.066 ( 0.067)	Data  0.002 ( 0.002)	Loss 1.6719e+00 (1.7647e+00)	Acc@1  54.69 ( 50.72)	Acc@5  82.81 ( 82.17)
Epoch: [13][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8154e+00 (1.7679e+00)	Acc@1  49.22 ( 50.60)	Acc@5  85.94 ( 82.13)
Epoch: [13][300/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8008e+00 (1.7683e+00)	Acc@1  50.78 ( 50.55)	Acc@5  82.81 ( 82.14)
Epoch: [13][310/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6553e+00 (1.7690e+00)	Acc@1  57.81 ( 50.58)	Acc@5  83.59 ( 82.13)
Epoch: [13][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7900e+00 (1.7672e+00)	Acc@1  50.00 ( 50.62)	Acc@5  84.38 ( 82.18)
Epoch: [13][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4375e+00 (1.7660e+00)	Acc@1  53.12 ( 50.62)	Acc@5  91.41 ( 82.22)
Epoch: [13][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.8584e+00 (1.7650e+00)	Acc@1  46.88 ( 50.61)	Acc@5  81.25 ( 82.26)
Epoch: [13][350/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6924e+00 (1.7654e+00)	Acc@1  51.56 ( 50.54)	Acc@5  83.59 ( 82.26)
Epoch: [13][360/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5381e+00 (1.7659e+00)	Acc@1  58.59 ( 50.50)	Acc@5  86.72 ( 82.25)
Epoch: [13][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9053e+00 (1.7651e+00)	Acc@1  43.75 ( 50.50)	Acc@5  76.56 ( 82.29)
Epoch: [13][380/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9971e+00 (1.7655e+00)	Acc@1  42.19 ( 50.48)	Acc@5  80.47 ( 82.29)
Epoch: [13][390/391]	Time  0.052 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9658e+00 (1.7646e+00)	Acc@1  41.25 ( 50.53)	Acc@5  82.50 ( 82.32)
## e[13] optimizer.zero_grad (sum) time: 0.41256046295166016
## e[13]       loss.backward (sum) time: 7.355274200439453
## e[13]      optimizer.step (sum) time: 3.5733299255371094
## epoch[13] training(only) time: 26.1311936378479
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 2.0918e+00 (2.0918e+00)	Acc@1  45.00 ( 45.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.9131e+00 (1.9330e+00)	Acc@1  48.00 ( 48.09)	Acc@5  76.00 ( 78.27)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6621e+00 (1.9168e+00)	Acc@1  52.00 ( 47.95)	Acc@5  82.00 ( 78.81)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.8213e+00 (1.9079e+00)	Acc@1  51.00 ( 48.61)	Acc@5  81.00 ( 78.84)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.0410e+00 (1.8898e+00)	Acc@1  47.00 ( 48.85)	Acc@5  77.00 ( 79.00)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.6758e+00 (1.9040e+00)	Acc@1  55.00 ( 48.35)	Acc@5  79.00 ( 78.90)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 1.8750e+00 (1.8870e+00)	Acc@1  49.00 ( 48.54)	Acc@5  79.00 ( 79.08)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.8213e+00 (1.8922e+00)	Acc@1  50.00 ( 48.65)	Acc@5  81.00 ( 78.94)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 2.0234e+00 (1.9058e+00)	Acc@1  45.00 ( 48.36)	Acc@5  74.00 ( 78.69)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.1934e+00 (1.9029e+00)	Acc@1  46.00 ( 48.36)	Acc@5  74.00 ( 78.78)
 * Acc@1 48.450 Acc@5 78.860
### epoch[13] execution time: 28.955265522003174
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.218 ( 0.218)	Data  0.141 ( 0.141)	Loss 1.6797e+00 (1.6797e+00)	Acc@1  56.25 ( 56.25)	Acc@5  82.03 ( 82.03)
Epoch: [14][ 10/391]	Time  0.067 ( 0.080)	Data  0.001 ( 0.014)	Loss 2.0254e+00 (1.7220e+00)	Acc@1  38.28 ( 51.92)	Acc@5  78.91 ( 82.81)
Epoch: [14][ 20/391]	Time  0.069 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.5967e+00 (1.7048e+00)	Acc@1  57.03 ( 52.75)	Acc@5  82.03 ( 82.89)
Epoch: [14][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.6953e+00 (1.6940e+00)	Acc@1  58.59 ( 53.23)	Acc@5  83.59 ( 83.27)
Epoch: [14][ 40/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.005)	Loss 2.0215e+00 (1.6840e+00)	Acc@1  49.22 ( 53.32)	Acc@5  75.78 ( 83.67)
Epoch: [14][ 50/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.6250e+00 (1.6822e+00)	Acc@1  50.78 ( 52.99)	Acc@5  85.94 ( 83.85)
Epoch: [14][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.9102e+00 (1.6807e+00)	Acc@1  42.97 ( 52.96)	Acc@5  83.59 ( 83.86)
Epoch: [14][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.5811e+00 (1.6811e+00)	Acc@1  55.47 ( 53.05)	Acc@5  82.81 ( 83.76)
Epoch: [14][ 80/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4600e+00 (1.6770e+00)	Acc@1  57.81 ( 53.05)	Acc@5  85.16 ( 83.80)
Epoch: [14][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4141e+00 (1.6794e+00)	Acc@1  61.72 ( 52.97)	Acc@5  88.28 ( 83.76)
Epoch: [14][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.9209e+00 (1.6896e+00)	Acc@1  48.44 ( 52.72)	Acc@5  79.69 ( 83.56)
Epoch: [14][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6289e+00 (1.6993e+00)	Acc@1  57.03 ( 52.37)	Acc@5  82.03 ( 83.35)
Epoch: [14][120/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5352e+00 (1.6982e+00)	Acc@1  53.12 ( 52.25)	Acc@5  87.50 ( 83.40)
Epoch: [14][130/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6182e+00 (1.6991e+00)	Acc@1  53.91 ( 52.21)	Acc@5  85.16 ( 83.34)
Epoch: [14][140/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3984e+00 (1.6999e+00)	Acc@1  55.47 ( 52.03)	Acc@5  89.06 ( 83.36)
Epoch: [14][150/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7656e+00 (1.7066e+00)	Acc@1  49.22 ( 51.91)	Acc@5  82.81 ( 83.23)
Epoch: [14][160/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6758e+00 (1.7070e+00)	Acc@1  50.78 ( 51.81)	Acc@5  81.25 ( 83.26)
Epoch: [14][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7754e+00 (1.7042e+00)	Acc@1  49.22 ( 51.94)	Acc@5  83.59 ( 83.32)
Epoch: [14][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5830e+00 (1.7001e+00)	Acc@1  53.12 ( 52.11)	Acc@5  87.50 ( 83.42)
Epoch: [14][190/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5068e+00 (1.6975e+00)	Acc@1  56.25 ( 52.24)	Acc@5  86.72 ( 83.44)
Epoch: [14][200/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5469e+00 (1.6981e+00)	Acc@1  58.59 ( 52.21)	Acc@5  85.16 ( 83.41)
Epoch: [14][210/391]	Time  0.076 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5518e+00 (1.6993e+00)	Acc@1  57.03 ( 52.21)	Acc@5  89.84 ( 83.41)
Epoch: [14][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4414e+00 (1.7002e+00)	Acc@1  61.72 ( 52.20)	Acc@5  85.94 ( 83.34)
Epoch: [14][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6553e+00 (1.6985e+00)	Acc@1  51.56 ( 52.26)	Acc@5  85.16 ( 83.35)
Epoch: [14][240/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8525e+00 (1.6984e+00)	Acc@1  43.75 ( 52.22)	Acc@5  79.69 ( 83.31)
Epoch: [14][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6885e+00 (1.7012e+00)	Acc@1  54.69 ( 52.19)	Acc@5  84.38 ( 83.29)
Epoch: [14][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7607e+00 (1.7037e+00)	Acc@1  46.88 ( 52.14)	Acc@5  82.03 ( 83.25)
Epoch: [14][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8047e+00 (1.7016e+00)	Acc@1  47.66 ( 52.20)	Acc@5  78.91 ( 83.26)
Epoch: [14][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6445e+00 (1.7013e+00)	Acc@1  53.12 ( 52.24)	Acc@5  87.50 ( 83.28)
Epoch: [14][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5703e+00 (1.7014e+00)	Acc@1  57.03 ( 52.20)	Acc@5  82.03 ( 83.30)
Epoch: [14][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7988e+00 (1.7012e+00)	Acc@1  48.44 ( 52.20)	Acc@5  80.47 ( 83.30)
Epoch: [14][310/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0156e+00 (1.7016e+00)	Acc@1  41.41 ( 52.17)	Acc@5  80.47 ( 83.33)
Epoch: [14][320/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9678e+00 (1.7025e+00)	Acc@1  42.19 ( 52.17)	Acc@5  81.25 ( 83.30)
Epoch: [14][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7725e+00 (1.7025e+00)	Acc@1  48.44 ( 52.17)	Acc@5  85.16 ( 83.32)
Epoch: [14][340/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6924e+00 (1.7038e+00)	Acc@1  50.78 ( 52.15)	Acc@5  84.38 ( 83.28)
Epoch: [14][350/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9141e+00 (1.7027e+00)	Acc@1  45.31 ( 52.15)	Acc@5  77.34 ( 83.29)
Epoch: [14][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.7949e+00 (1.7049e+00)	Acc@1  46.88 ( 52.10)	Acc@5  82.81 ( 83.28)
Epoch: [14][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5146e+00 (1.7043e+00)	Acc@1  52.34 ( 52.09)	Acc@5  89.84 ( 83.29)
Epoch: [14][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9229e+00 (1.7042e+00)	Acc@1  42.19 ( 52.08)	Acc@5  81.25 ( 83.30)
Epoch: [14][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.1016e+00 (1.7047e+00)	Acc@1  43.75 ( 52.05)	Acc@5  76.25 ( 83.28)
## e[14] optimizer.zero_grad (sum) time: 0.40983033180236816
## e[14]       loss.backward (sum) time: 7.322281360626221
## e[14]      optimizer.step (sum) time: 3.5883684158325195
## epoch[14] training(only) time: 26.15268635749817
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 2.1367e+00 (2.1367e+00)	Acc@1  43.00 ( 43.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.9932e+00 (2.0541e+00)	Acc@1  44.00 ( 45.09)	Acc@5  80.00 ( 79.18)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.8818e+00 (2.0436e+00)	Acc@1  49.00 ( 45.86)	Acc@5  81.00 ( 78.86)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 2.0352e+00 (2.0250e+00)	Acc@1  41.00 ( 46.19)	Acc@5  78.00 ( 78.77)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.1484e+00 (2.0180e+00)	Acc@1  45.00 ( 46.41)	Acc@5  77.00 ( 78.78)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.8789e+00 (2.0339e+00)	Acc@1  50.00 ( 46.39)	Acc@5  80.00 ( 78.49)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.0742e+00 (2.0231e+00)	Acc@1  50.00 ( 46.57)	Acc@5  77.00 ( 78.49)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 2.0684e+00 (2.0307e+00)	Acc@1  46.00 ( 46.30)	Acc@5  80.00 ( 78.44)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.1426e+00 (2.0421e+00)	Acc@1  41.00 ( 46.11)	Acc@5  79.00 ( 78.31)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.4531e+00 (2.0343e+00)	Acc@1  42.00 ( 46.45)	Acc@5  73.00 ( 78.41)
 * Acc@1 46.610 Acc@5 78.470
### epoch[14] execution time: 28.997559785842896
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.228 ( 0.228)	Data  0.150 ( 0.150)	Loss 1.5781e+00 (1.5781e+00)	Acc@1  50.00 ( 50.00)	Acc@5  88.28 ( 88.28)
Epoch: [15][ 10/391]	Time  0.065 ( 0.082)	Data  0.001 ( 0.015)	Loss 1.5537e+00 (1.6269e+00)	Acc@1  53.91 ( 53.48)	Acc@5  88.28 ( 83.74)
Epoch: [15][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.6406e+00 (1.6097e+00)	Acc@1  48.44 ( 53.57)	Acc@5  83.59 ( 84.15)
Epoch: [15][ 30/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.006)	Loss 1.6680e+00 (1.6267e+00)	Acc@1  50.78 ( 53.60)	Acc@5  85.94 ( 84.38)
Epoch: [15][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.6553e+00 (1.6205e+00)	Acc@1  53.12 ( 54.02)	Acc@5  82.03 ( 84.49)
Epoch: [15][ 50/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.5811e+00 (1.6377e+00)	Acc@1  48.44 ( 53.49)	Acc@5  86.72 ( 84.25)
Epoch: [15][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.7441e+00 (1.6336e+00)	Acc@1  53.12 ( 53.69)	Acc@5  82.03 ( 84.31)
Epoch: [15][ 70/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.6279e+00 (1.6429e+00)	Acc@1  53.91 ( 53.29)	Acc@5  84.38 ( 84.15)
Epoch: [15][ 80/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.8242e+00 (1.6554e+00)	Acc@1  50.00 ( 53.12)	Acc@5  83.59 ( 83.99)
Epoch: [15][ 90/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5205e+00 (1.6547e+00)	Acc@1  59.38 ( 53.31)	Acc@5  84.38 ( 84.02)
Epoch: [15][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4951e+00 (1.6534e+00)	Acc@1  56.25 ( 53.26)	Acc@5  87.50 ( 84.20)
Epoch: [15][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4580e+00 (1.6597e+00)	Acc@1  60.16 ( 53.07)	Acc@5  86.72 ( 84.08)
Epoch: [15][120/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5967e+00 (1.6583e+00)	Acc@1  51.56 ( 53.00)	Acc@5  84.38 ( 84.14)
Epoch: [15][130/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4678e+00 (1.6559e+00)	Acc@1  60.94 ( 53.10)	Acc@5  85.16 ( 84.17)
Epoch: [15][140/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8281e+00 (1.6558e+00)	Acc@1  48.44 ( 53.04)	Acc@5  78.91 ( 84.16)
Epoch: [15][150/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5918e+00 (1.6551e+00)	Acc@1  54.69 ( 53.00)	Acc@5  84.38 ( 84.19)
Epoch: [15][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5693e+00 (1.6485e+00)	Acc@1  54.69 ( 53.15)	Acc@5  82.81 ( 84.37)
Epoch: [15][170/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7910e+00 (1.6450e+00)	Acc@1  50.78 ( 53.27)	Acc@5  82.03 ( 84.35)
Epoch: [15][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5176e+00 (1.6445e+00)	Acc@1  62.50 ( 53.39)	Acc@5  84.38 ( 84.34)
Epoch: [15][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7393e+00 (1.6459e+00)	Acc@1  52.34 ( 53.35)	Acc@5  82.81 ( 84.22)
Epoch: [15][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6250e+00 (1.6473e+00)	Acc@1  53.12 ( 53.39)	Acc@5  85.94 ( 84.22)
Epoch: [15][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7578e+00 (1.6490e+00)	Acc@1  53.12 ( 53.42)	Acc@5  82.81 ( 84.16)
Epoch: [15][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7764e+00 (1.6504e+00)	Acc@1  49.22 ( 53.40)	Acc@5  82.81 ( 84.14)
Epoch: [15][230/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7344e+00 (1.6551e+00)	Acc@1  52.34 ( 53.29)	Acc@5  80.47 ( 84.07)
Epoch: [15][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5684e+00 (1.6565e+00)	Acc@1  57.81 ( 53.29)	Acc@5  85.94 ( 84.11)
Epoch: [15][250/391]	Time  0.065 ( 0.067)	Data  0.002 ( 0.002)	Loss 1.7334e+00 (1.6578e+00)	Acc@1  49.22 ( 53.26)	Acc@5  83.59 ( 84.12)
Epoch: [15][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6221e+00 (1.6591e+00)	Acc@1  53.91 ( 53.22)	Acc@5  88.28 ( 84.09)
Epoch: [15][270/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6689e+00 (1.6626e+00)	Acc@1  57.03 ( 53.11)	Acc@5  81.25 ( 84.01)
Epoch: [15][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5107e+00 (1.6642e+00)	Acc@1  58.59 ( 53.07)	Acc@5  87.50 ( 83.99)
Epoch: [15][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5264e+00 (1.6641e+00)	Acc@1  53.91 ( 53.03)	Acc@5  85.94 ( 83.99)
Epoch: [15][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8799e+00 (1.6630e+00)	Acc@1  46.88 ( 53.07)	Acc@5  77.34 ( 83.96)
Epoch: [15][310/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7402e+00 (1.6654e+00)	Acc@1  47.66 ( 53.03)	Acc@5  82.81 ( 83.91)
Epoch: [15][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7705e+00 (1.6654e+00)	Acc@1  49.22 ( 53.01)	Acc@5  78.12 ( 83.90)
Epoch: [15][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7295e+00 (1.6666e+00)	Acc@1  51.56 ( 52.98)	Acc@5  82.81 ( 83.88)
Epoch: [15][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7520e+00 (1.6659e+00)	Acc@1  50.78 ( 52.99)	Acc@5  80.47 ( 83.87)
Epoch: [15][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6328e+00 (1.6649e+00)	Acc@1  53.12 ( 52.99)	Acc@5  84.38 ( 83.88)
Epoch: [15][360/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4639e+00 (1.6642e+00)	Acc@1  53.12 ( 52.97)	Acc@5  88.28 ( 83.89)
Epoch: [15][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6104e+00 (1.6641e+00)	Acc@1  52.34 ( 52.96)	Acc@5  86.72 ( 83.91)
Epoch: [15][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.7930e+00 (1.6648e+00)	Acc@1  54.69 ( 52.90)	Acc@5  79.69 ( 83.91)
Epoch: [15][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.8477e+00 (1.6634e+00)	Acc@1  47.50 ( 52.91)	Acc@5  81.25 ( 83.95)
## e[15] optimizer.zero_grad (sum) time: 0.41348767280578613
## e[15]       loss.backward (sum) time: 7.34884786605835
## e[15]      optimizer.step (sum) time: 3.586514949798584
## epoch[15] training(only) time: 26.115140199661255
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.9492e+00 (1.9492e+00)	Acc@1  45.00 ( 45.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 2.3398e+00 (2.0328e+00)	Acc@1  42.00 ( 45.45)	Acc@5  75.00 ( 79.00)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.9414e+00 (2.0350e+00)	Acc@1  49.00 ( 46.00)	Acc@5  79.00 ( 78.90)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.0586e+00 (2.0363e+00)	Acc@1  46.00 ( 46.29)	Acc@5  75.00 ( 78.55)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.1094e+00 (2.0153e+00)	Acc@1  45.00 ( 46.34)	Acc@5  76.00 ( 78.63)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.6865e+00 (2.0207e+00)	Acc@1  53.00 ( 46.35)	Acc@5  83.00 ( 78.41)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.9580e+00 (2.0007e+00)	Acc@1  46.00 ( 46.74)	Acc@5  79.00 ( 78.49)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 2.0879e+00 (2.0172e+00)	Acc@1  40.00 ( 46.17)	Acc@5  78.00 ( 78.30)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.1895e+00 (2.0291e+00)	Acc@1  43.00 ( 46.06)	Acc@5  74.00 ( 78.27)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.3164e+00 (2.0194e+00)	Acc@1  41.00 ( 46.11)	Acc@5  76.00 ( 78.46)
 * Acc@1 46.130 Acc@5 78.500
### epoch[15] execution time: 28.93603801727295
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.228 ( 0.228)	Data  0.151 ( 0.151)	Loss 1.5586e+00 (1.5586e+00)	Acc@1  57.81 ( 57.81)	Acc@5  84.38 ( 84.38)
Epoch: [16][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.015)	Loss 1.5254e+00 (1.5310e+00)	Acc@1  52.34 ( 55.82)	Acc@5  82.03 ( 85.65)
Epoch: [16][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.7051e+00 (1.5570e+00)	Acc@1  47.66 ( 55.54)	Acc@5  81.25 ( 84.97)
Epoch: [16][ 30/391]	Time  0.071 ( 0.072)	Data  0.001 ( 0.006)	Loss 1.6426e+00 (1.5464e+00)	Acc@1  54.69 ( 55.92)	Acc@5  81.25 ( 85.38)
Epoch: [16][ 40/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.8779e+00 (1.5704e+00)	Acc@1  48.44 ( 55.09)	Acc@5  81.25 ( 85.16)
Epoch: [16][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.5146e+00 (1.5666e+00)	Acc@1  55.47 ( 55.09)	Acc@5  89.06 ( 85.34)
Epoch: [16][ 60/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.6104e+00 (1.5842e+00)	Acc@1  53.12 ( 54.73)	Acc@5  85.16 ( 85.07)
Epoch: [16][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.4980e+00 (1.5960e+00)	Acc@1  55.47 ( 54.51)	Acc@5  83.59 ( 84.79)
Epoch: [16][ 80/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5479e+00 (1.5930e+00)	Acc@1  53.12 ( 54.31)	Acc@5  85.16 ( 84.95)
Epoch: [16][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.6377e+00 (1.5970e+00)	Acc@1  50.00 ( 54.30)	Acc@5  84.38 ( 84.89)
Epoch: [16][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5381e+00 (1.6047e+00)	Acc@1  53.91 ( 54.08)	Acc@5  81.25 ( 84.72)
Epoch: [16][110/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.7012e+00 (1.6098e+00)	Acc@1  53.91 ( 53.94)	Acc@5  81.25 ( 84.73)
Epoch: [16][120/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6211e+00 (1.6112e+00)	Acc@1  57.81 ( 53.98)	Acc@5  86.72 ( 84.74)
Epoch: [16][130/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8496e+00 (1.6165e+00)	Acc@1  50.78 ( 54.00)	Acc@5  80.47 ( 84.69)
Epoch: [16][140/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8105e+00 (1.6180e+00)	Acc@1  50.78 ( 53.92)	Acc@5  79.69 ( 84.74)
Epoch: [16][150/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8340e+00 (1.6192e+00)	Acc@1  47.66 ( 53.86)	Acc@5  82.03 ( 84.76)
Epoch: [16][160/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3486e+00 (1.6202e+00)	Acc@1  59.38 ( 53.87)	Acc@5  92.19 ( 84.77)
Epoch: [16][170/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6074e+00 (1.6214e+00)	Acc@1  50.78 ( 53.78)	Acc@5  85.94 ( 84.82)
Epoch: [16][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8193e+00 (1.6239e+00)	Acc@1  50.78 ( 53.74)	Acc@5  79.69 ( 84.80)
Epoch: [16][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6670e+00 (1.6221e+00)	Acc@1  57.81 ( 53.85)	Acc@5  80.47 ( 84.80)
Epoch: [16][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7695e+00 (1.6233e+00)	Acc@1  50.00 ( 53.83)	Acc@5  80.47 ( 84.71)
Epoch: [16][210/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6064e+00 (1.6226e+00)	Acc@1  53.12 ( 53.88)	Acc@5  85.16 ( 84.72)
Epoch: [16][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4053e+00 (1.6211e+00)	Acc@1  59.38 ( 53.94)	Acc@5  85.16 ( 84.74)
Epoch: [16][230/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8604e+00 (1.6236e+00)	Acc@1  47.66 ( 53.90)	Acc@5  82.81 ( 84.67)
Epoch: [16][240/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4434e+00 (1.6208e+00)	Acc@1  59.38 ( 54.01)	Acc@5  85.16 ( 84.72)
Epoch: [16][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5205e+00 (1.6199e+00)	Acc@1  57.81 ( 54.09)	Acc@5  87.50 ( 84.69)
Epoch: [16][260/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5039e+00 (1.6174e+00)	Acc@1  56.25 ( 54.09)	Acc@5  83.59 ( 84.74)
Epoch: [16][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6104e+00 (1.6166e+00)	Acc@1  53.91 ( 54.08)	Acc@5  88.28 ( 84.76)
Epoch: [16][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5859e+00 (1.6151e+00)	Acc@1  53.12 ( 54.14)	Acc@5  85.16 ( 84.75)
Epoch: [16][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4707e+00 (1.6178e+00)	Acc@1  53.91 ( 54.02)	Acc@5  89.06 ( 84.73)
Epoch: [16][300/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4668e+00 (1.6184e+00)	Acc@1  57.03 ( 54.04)	Acc@5  88.28 ( 84.71)
Epoch: [16][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6914e+00 (1.6162e+00)	Acc@1  53.12 ( 54.13)	Acc@5  81.25 ( 84.72)
Epoch: [16][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6787e+00 (1.6163e+00)	Acc@1  46.88 ( 54.14)	Acc@5  83.59 ( 84.71)
Epoch: [16][330/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5029e+00 (1.6159e+00)	Acc@1  55.47 ( 54.17)	Acc@5  88.28 ( 84.71)
Epoch: [16][340/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9131e+00 (1.6175e+00)	Acc@1  46.09 ( 54.11)	Acc@5  81.25 ( 84.69)
Epoch: [16][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7090e+00 (1.6191e+00)	Acc@1  48.44 ( 54.03)	Acc@5  82.81 ( 84.67)
Epoch: [16][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4111e+00 (1.6199e+00)	Acc@1  58.59 ( 54.04)	Acc@5  86.72 ( 84.62)
Epoch: [16][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6084e+00 (1.6223e+00)	Acc@1  52.34 ( 54.01)	Acc@5  86.72 ( 84.58)
Epoch: [16][380/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4863e+00 (1.6222e+00)	Acc@1  54.69 ( 54.00)	Acc@5  86.72 ( 84.57)
Epoch: [16][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5293e+00 (1.6249e+00)	Acc@1  56.25 ( 53.95)	Acc@5  90.00 ( 84.50)
## e[16] optimizer.zero_grad (sum) time: 0.41504502296447754
## e[16]       loss.backward (sum) time: 7.336332321166992
## e[16]      optimizer.step (sum) time: 3.6648526191711426
## epoch[16] training(only) time: 26.29441738128662
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.7334e+00 (1.7334e+00)	Acc@1  57.00 ( 57.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.030 ( 0.039)	Loss 1.8135e+00 (1.8028e+00)	Acc@1  49.00 ( 50.36)	Acc@5  83.00 ( 81.45)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.5781e+00 (1.7662e+00)	Acc@1  55.00 ( 51.29)	Acc@5  82.00 ( 81.86)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6533e+00 (1.7756e+00)	Acc@1  54.00 ( 50.71)	Acc@5  83.00 ( 81.68)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.6807e+00 (1.7628e+00)	Acc@1  53.00 ( 51.41)	Acc@5  83.00 ( 81.80)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.7695e+00 (1.7832e+00)	Acc@1  53.00 ( 51.18)	Acc@5  78.00 ( 81.31)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.7705e+00 (1.7705e+00)	Acc@1  54.00 ( 51.52)	Acc@5  76.00 ( 81.38)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.8906e+00 (1.7831e+00)	Acc@1  48.00 ( 51.17)	Acc@5  78.00 ( 81.08)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.9375e+00 (1.8006e+00)	Acc@1  50.00 ( 50.89)	Acc@5  78.00 ( 80.84)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0273e+00 (1.7966e+00)	Acc@1  46.00 ( 51.16)	Acc@5  79.00 ( 80.97)
 * Acc@1 51.270 Acc@5 81.050
### epoch[16] execution time: 29.128149271011353
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.219 ( 0.219)	Data  0.135 ( 0.135)	Loss 1.4033e+00 (1.4033e+00)	Acc@1  66.41 ( 66.41)	Acc@5  85.94 ( 85.94)
Epoch: [17][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.013)	Loss 1.5234e+00 (1.5359e+00)	Acc@1  56.25 ( 56.89)	Acc@5  85.94 ( 85.01)
Epoch: [17][ 20/391]	Time  0.068 ( 0.073)	Data  0.001 ( 0.007)	Loss 1.6602e+00 (1.5576e+00)	Acc@1  49.22 ( 55.95)	Acc@5  84.38 ( 85.23)
Epoch: [17][ 30/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.005)	Loss 1.5781e+00 (1.5521e+00)	Acc@1  51.56 ( 55.67)	Acc@5  82.03 ( 85.31)
Epoch: [17][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.5439e+00 (1.5501e+00)	Acc@1  57.03 ( 55.66)	Acc@5  86.72 ( 85.77)
Epoch: [17][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.5391e+00 (1.5538e+00)	Acc@1  57.03 ( 55.65)	Acc@5  81.25 ( 85.57)
Epoch: [17][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.3721e+00 (1.5479e+00)	Acc@1  57.81 ( 55.93)	Acc@5  85.94 ( 85.63)
Epoch: [17][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.6035e+00 (1.5491e+00)	Acc@1  51.56 ( 55.79)	Acc@5  87.50 ( 85.76)
Epoch: [17][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4893e+00 (1.5524e+00)	Acc@1  53.91 ( 55.53)	Acc@5  86.72 ( 85.67)
Epoch: [17][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.6592e+00 (1.5527e+00)	Acc@1  50.78 ( 55.40)	Acc@5  83.59 ( 85.67)
Epoch: [17][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.7441e+00 (1.5547e+00)	Acc@1  47.66 ( 55.19)	Acc@5  82.81 ( 85.69)
Epoch: [17][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6768e+00 (1.5571e+00)	Acc@1  46.88 ( 55.10)	Acc@5  82.03 ( 85.63)
Epoch: [17][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.8262e+00 (1.5637e+00)	Acc@1  50.00 ( 55.06)	Acc@5  78.12 ( 85.46)
Epoch: [17][130/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4395e+00 (1.5565e+00)	Acc@1  55.47 ( 55.40)	Acc@5  85.16 ( 85.56)
Epoch: [17][140/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3965e+00 (1.5591e+00)	Acc@1  60.16 ( 55.45)	Acc@5  85.94 ( 85.61)
Epoch: [17][150/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6738e+00 (1.5588e+00)	Acc@1  56.25 ( 55.47)	Acc@5  82.03 ( 85.58)
Epoch: [17][160/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6484e+00 (1.5645e+00)	Acc@1  53.12 ( 55.30)	Acc@5  81.25 ( 85.50)
Epoch: [17][170/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6748e+00 (1.5649e+00)	Acc@1  55.47 ( 55.31)	Acc@5  84.38 ( 85.47)
Epoch: [17][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5508e+00 (1.5678e+00)	Acc@1  56.25 ( 55.35)	Acc@5  86.72 ( 85.40)
Epoch: [17][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6494e+00 (1.5733e+00)	Acc@1  51.56 ( 55.23)	Acc@5  82.81 ( 85.31)
Epoch: [17][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3779e+00 (1.5738e+00)	Acc@1  63.28 ( 55.30)	Acc@5  84.38 ( 85.23)
Epoch: [17][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6230e+00 (1.5696e+00)	Acc@1  53.12 ( 55.41)	Acc@5  84.38 ( 85.36)
Epoch: [17][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7686e+00 (1.5693e+00)	Acc@1  53.91 ( 55.37)	Acc@5  83.59 ( 85.37)
Epoch: [17][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4619e+00 (1.5745e+00)	Acc@1  57.03 ( 55.25)	Acc@5  89.84 ( 85.33)
Epoch: [17][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7842e+00 (1.5762e+00)	Acc@1  47.66 ( 55.13)	Acc@5  78.91 ( 85.30)
Epoch: [17][250/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5449e+00 (1.5775e+00)	Acc@1  54.69 ( 55.08)	Acc@5  85.16 ( 85.27)
Epoch: [17][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4297e+00 (1.5788e+00)	Acc@1  58.59 ( 55.02)	Acc@5  88.28 ( 85.27)
Epoch: [17][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6318e+00 (1.5799e+00)	Acc@1  54.69 ( 54.98)	Acc@5  82.03 ( 85.25)
Epoch: [17][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6689e+00 (1.5811e+00)	Acc@1  50.78 ( 54.96)	Acc@5  85.16 ( 85.24)
Epoch: [17][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6045e+00 (1.5809e+00)	Acc@1  53.12 ( 54.92)	Acc@5  85.94 ( 85.26)
Epoch: [17][300/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4834e+00 (1.5821e+00)	Acc@1  60.16 ( 54.95)	Acc@5  84.38 ( 85.23)
Epoch: [17][310/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6230e+00 (1.5818e+00)	Acc@1  55.47 ( 54.95)	Acc@5  84.38 ( 85.25)
Epoch: [17][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5547e+00 (1.5816e+00)	Acc@1  50.78 ( 54.94)	Acc@5  86.72 ( 85.25)
Epoch: [17][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4199e+00 (1.5809e+00)	Acc@1  60.94 ( 54.99)	Acc@5  88.28 ( 85.26)
Epoch: [17][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4189e+00 (1.5788e+00)	Acc@1  63.28 ( 55.07)	Acc@5  82.81 ( 85.28)
Epoch: [17][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4180e+00 (1.5795e+00)	Acc@1  57.81 ( 55.04)	Acc@5  90.62 ( 85.27)
Epoch: [17][360/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.8057e+00 (1.5808e+00)	Acc@1  53.12 ( 55.09)	Acc@5  79.69 ( 85.23)
Epoch: [17][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5068e+00 (1.5814e+00)	Acc@1  54.69 ( 55.07)	Acc@5  87.50 ( 85.21)
Epoch: [17][380/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5645e+00 (1.5812e+00)	Acc@1  53.12 ( 55.06)	Acc@5  86.72 ( 85.22)
Epoch: [17][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6113e+00 (1.5812e+00)	Acc@1  52.50 ( 55.10)	Acc@5  86.25 ( 85.22)
## e[17] optimizer.zero_grad (sum) time: 0.4093899726867676
## e[17]       loss.backward (sum) time: 7.353584289550781
## e[17]      optimizer.step (sum) time: 3.5606420040130615
## epoch[17] training(only) time: 26.131452322006226
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.8232e+00 (1.8232e+00)	Acc@1  50.00 ( 50.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.029 ( 0.040)	Loss 1.7012e+00 (1.6889e+00)	Acc@1  53.00 ( 54.00)	Acc@5  84.00 ( 82.91)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.5518e+00 (1.7059e+00)	Acc@1  56.00 ( 53.10)	Acc@5  88.00 ( 82.67)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.6436e+00 (1.7057e+00)	Acc@1  54.00 ( 53.39)	Acc@5  86.00 ( 82.48)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.6260e+00 (1.6884e+00)	Acc@1  53.00 ( 53.22)	Acc@5  82.00 ( 83.05)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.6963e+00 (1.7013e+00)	Acc@1  52.00 ( 52.73)	Acc@5  84.00 ( 82.78)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.7373e+00 (1.6981e+00)	Acc@1  55.00 ( 52.74)	Acc@5  81.00 ( 82.80)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.9951e+00 (1.7084e+00)	Acc@1  45.00 ( 52.51)	Acc@5  79.00 ( 82.76)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.7939e+00 (1.7224e+00)	Acc@1  49.00 ( 52.36)	Acc@5  76.00 ( 82.52)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0371e+00 (1.7124e+00)	Acc@1  47.00 ( 52.71)	Acc@5  78.00 ( 82.75)
 * Acc@1 52.790 Acc@5 82.880
### epoch[17] execution time: 28.95591974258423
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.225 ( 0.225)	Data  0.146 ( 0.146)	Loss 1.4424e+00 (1.4424e+00)	Acc@1  60.94 ( 60.94)	Acc@5  85.94 ( 85.94)
Epoch: [18][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.014)	Loss 1.4883e+00 (1.5785e+00)	Acc@1  55.47 ( 53.55)	Acc@5  85.94 ( 85.58)
Epoch: [18][ 20/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.3721e+00 (1.5769e+00)	Acc@1  59.38 ( 54.72)	Acc@5  89.84 ( 85.57)
Epoch: [18][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.4688e+00 (1.5628e+00)	Acc@1  59.38 ( 55.32)	Acc@5  85.94 ( 85.86)
Epoch: [18][ 40/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.4092e+00 (1.5445e+00)	Acc@1  61.72 ( 55.95)	Acc@5  84.38 ( 85.96)
Epoch: [18][ 50/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.6973e+00 (1.5305e+00)	Acc@1  57.81 ( 56.19)	Acc@5  81.25 ( 86.32)
Epoch: [18][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.5967e+00 (1.5303e+00)	Acc@1  51.56 ( 55.84)	Acc@5  87.50 ( 86.45)
Epoch: [18][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5889e+00 (1.5368e+00)	Acc@1  53.12 ( 55.81)	Acc@5  83.59 ( 86.18)
Epoch: [18][ 80/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5322e+00 (1.5408e+00)	Acc@1  53.91 ( 55.81)	Acc@5  83.59 ( 86.01)
Epoch: [18][ 90/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.3838e+00 (1.5405e+00)	Acc@1  60.94 ( 55.64)	Acc@5  87.50 ( 85.95)
Epoch: [18][100/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5967e+00 (1.5402e+00)	Acc@1  56.25 ( 55.65)	Acc@5  82.81 ( 86.03)
Epoch: [18][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6094e+00 (1.5372e+00)	Acc@1  57.81 ( 55.85)	Acc@5  84.38 ( 86.06)
Epoch: [18][120/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5615e+00 (1.5370e+00)	Acc@1  53.12 ( 55.89)	Acc@5  86.72 ( 86.07)
Epoch: [18][130/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3916e+00 (1.5377e+00)	Acc@1  59.38 ( 55.81)	Acc@5  88.28 ( 86.06)
Epoch: [18][140/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4404e+00 (1.5407e+00)	Acc@1  61.72 ( 55.80)	Acc@5  87.50 ( 86.08)
Epoch: [18][150/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5732e+00 (1.5374e+00)	Acc@1  51.56 ( 55.81)	Acc@5  89.06 ( 86.17)
Epoch: [18][160/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8750e+00 (1.5381e+00)	Acc@1  44.53 ( 55.73)	Acc@5  77.34 ( 86.17)
Epoch: [18][170/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5459e+00 (1.5376e+00)	Acc@1  58.59 ( 55.85)	Acc@5  88.28 ( 86.19)
Epoch: [18][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7490e+00 (1.5417e+00)	Acc@1  47.66 ( 55.75)	Acc@5  87.50 ( 86.14)
Epoch: [18][190/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4258e+00 (1.5432e+00)	Acc@1  57.81 ( 55.71)	Acc@5  88.28 ( 86.18)
Epoch: [18][200/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4062e+00 (1.5423e+00)	Acc@1  55.47 ( 55.66)	Acc@5  89.84 ( 86.21)
Epoch: [18][210/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6865e+00 (1.5490e+00)	Acc@1  50.00 ( 55.52)	Acc@5  80.47 ( 86.10)
Epoch: [18][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5625e+00 (1.5511e+00)	Acc@1  51.56 ( 55.43)	Acc@5  90.62 ( 86.08)
Epoch: [18][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3525e+00 (1.5506e+00)	Acc@1  64.06 ( 55.44)	Acc@5  87.50 ( 86.07)
Epoch: [18][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3838e+00 (1.5494e+00)	Acc@1  53.91 ( 55.46)	Acc@5  88.28 ( 86.07)
Epoch: [18][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4580e+00 (1.5535e+00)	Acc@1  57.81 ( 55.38)	Acc@5  87.50 ( 86.02)
Epoch: [18][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3467e+00 (1.5559e+00)	Acc@1  64.06 ( 55.37)	Acc@5  86.72 ( 85.93)
Epoch: [18][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4014e+00 (1.5552e+00)	Acc@1  60.16 ( 55.34)	Acc@5  85.94 ( 85.95)
Epoch: [18][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7354e+00 (1.5568e+00)	Acc@1  54.69 ( 55.31)	Acc@5  84.38 ( 85.90)
Epoch: [18][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4941e+00 (1.5559e+00)	Acc@1  55.47 ( 55.36)	Acc@5  85.16 ( 85.90)
Epoch: [18][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4834e+00 (1.5549e+00)	Acc@1  57.03 ( 55.38)	Acc@5  86.72 ( 85.90)
Epoch: [18][310/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7373e+00 (1.5547e+00)	Acc@1  49.22 ( 55.40)	Acc@5  84.38 ( 85.88)
Epoch: [18][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4551e+00 (1.5521e+00)	Acc@1  60.16 ( 55.51)	Acc@5  83.59 ( 85.93)
Epoch: [18][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4404e+00 (1.5537e+00)	Acc@1  64.84 ( 55.47)	Acc@5  89.06 ( 85.91)
Epoch: [18][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4521e+00 (1.5530e+00)	Acc@1  55.47 ( 55.50)	Acc@5  85.16 ( 85.92)
Epoch: [18][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4951e+00 (1.5531e+00)	Acc@1  52.34 ( 55.49)	Acc@5  87.50 ( 85.90)
Epoch: [18][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5791e+00 (1.5550e+00)	Acc@1  53.91 ( 55.46)	Acc@5  80.47 ( 85.86)
Epoch: [18][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6250e+00 (1.5564e+00)	Acc@1  55.47 ( 55.49)	Acc@5  84.38 ( 85.83)
Epoch: [18][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5557e+00 (1.5546e+00)	Acc@1  57.03 ( 55.55)	Acc@5  86.72 ( 85.85)
Epoch: [18][390/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.8291e+00 (1.5562e+00)	Acc@1  52.50 ( 55.53)	Acc@5  81.25 ( 85.82)
## e[18] optimizer.zero_grad (sum) time: 0.41068315505981445
## e[18]       loss.backward (sum) time: 7.382623672485352
## e[18]      optimizer.step (sum) time: 3.574899196624756
## epoch[18] training(only) time: 26.206647396087646
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.6924e+00 (1.6924e+00)	Acc@1  51.00 ( 51.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.6533e+00 (1.7028e+00)	Acc@1  51.00 ( 52.82)	Acc@5  85.00 ( 83.73)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5742e+00 (1.6934e+00)	Acc@1  59.00 ( 54.52)	Acc@5  81.00 ( 82.95)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6885e+00 (1.6915e+00)	Acc@1  53.00 ( 53.71)	Acc@5  87.00 ( 82.90)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.6602e+00 (1.6708e+00)	Acc@1  53.00 ( 53.85)	Acc@5  86.00 ( 83.56)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.7451e+00 (1.6831e+00)	Acc@1  51.00 ( 53.45)	Acc@5  80.00 ( 83.33)
Test: [ 60/100]	Time  0.024 ( 0.028)	Loss 1.6943e+00 (1.6804e+00)	Acc@1  57.00 ( 53.25)	Acc@5  83.00 ( 83.28)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.7100e+00 (1.6836e+00)	Acc@1  50.00 ( 53.18)	Acc@5  84.00 ( 83.28)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.8408e+00 (1.6990e+00)	Acc@1  52.00 ( 52.89)	Acc@5  80.00 ( 83.12)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 2.1367e+00 (1.6939e+00)	Acc@1  46.00 ( 53.04)	Acc@5  74.00 ( 83.29)
 * Acc@1 53.300 Acc@5 83.330
### epoch[18] execution time: 29.013708353042603
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.229 ( 0.229)	Data  0.154 ( 0.154)	Loss 1.3594e+00 (1.3594e+00)	Acc@1  57.03 ( 57.03)	Acc@5  92.19 ( 92.19)
Epoch: [19][ 10/391]	Time  0.069 ( 0.082)	Data  0.001 ( 0.015)	Loss 1.3291e+00 (1.3690e+00)	Acc@1  62.50 ( 60.44)	Acc@5  88.28 ( 89.91)
Epoch: [19][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.5020e+00 (1.4461e+00)	Acc@1  61.72 ( 58.74)	Acc@5  83.59 ( 88.95)
Epoch: [19][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.3438e+00 (1.4867e+00)	Acc@1  63.28 ( 57.94)	Acc@5  90.62 ( 87.60)
Epoch: [19][ 40/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.6221e+00 (1.4846e+00)	Acc@1  53.12 ( 57.74)	Acc@5  83.59 ( 87.50)
Epoch: [19][ 50/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.7148e+00 (1.4895e+00)	Acc@1  51.56 ( 57.11)	Acc@5  82.81 ( 87.44)
Epoch: [19][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.3672e+00 (1.4884e+00)	Acc@1  60.94 ( 57.15)	Acc@5  92.19 ( 87.50)
Epoch: [19][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.3613e+00 (1.5052e+00)	Acc@1  63.28 ( 57.01)	Acc@5  88.28 ( 87.10)
Epoch: [19][ 80/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5596e+00 (1.5042e+00)	Acc@1  50.78 ( 56.97)	Acc@5  90.62 ( 87.09)
Epoch: [19][ 90/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5537e+00 (1.5011e+00)	Acc@1  50.78 ( 57.10)	Acc@5  89.84 ( 87.03)
Epoch: [19][100/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.2021e+00 (1.5013e+00)	Acc@1  64.84 ( 57.12)	Acc@5  89.06 ( 86.94)
Epoch: [19][110/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3672e+00 (1.4983e+00)	Acc@1  61.72 ( 57.09)	Acc@5  89.84 ( 86.94)
Epoch: [19][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4199e+00 (1.4987e+00)	Acc@1  57.03 ( 57.00)	Acc@5  90.62 ( 86.94)
Epoch: [19][130/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4785e+00 (1.4987e+00)	Acc@1  57.81 ( 57.04)	Acc@5  88.28 ( 86.86)
Epoch: [19][140/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.7383e+00 (1.5056e+00)	Acc@1  50.00 ( 56.83)	Acc@5  85.94 ( 86.72)
Epoch: [19][150/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5967e+00 (1.5091e+00)	Acc@1  52.34 ( 56.79)	Acc@5  86.72 ( 86.63)
Epoch: [19][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5146e+00 (1.5084e+00)	Acc@1  56.25 ( 56.82)	Acc@5  85.16 ( 86.60)
Epoch: [19][170/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6582e+00 (1.5111e+00)	Acc@1  57.81 ( 56.80)	Acc@5  83.59 ( 86.52)
Epoch: [19][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4336e+00 (1.5142e+00)	Acc@1  60.94 ( 56.72)	Acc@5  85.16 ( 86.46)
Epoch: [19][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3574e+00 (1.5111e+00)	Acc@1  58.59 ( 56.74)	Acc@5  89.84 ( 86.51)
Epoch: [19][200/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4209e+00 (1.5070e+00)	Acc@1  59.38 ( 56.80)	Acc@5  84.38 ( 86.61)
Epoch: [19][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4434e+00 (1.5103e+00)	Acc@1  62.50 ( 56.81)	Acc@5  85.94 ( 86.56)
Epoch: [19][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6514e+00 (1.5107e+00)	Acc@1  53.12 ( 56.76)	Acc@5  85.94 ( 86.54)
Epoch: [19][230/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3447e+00 (1.5091e+00)	Acc@1  58.59 ( 56.75)	Acc@5  89.84 ( 86.57)
Epoch: [19][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6533e+00 (1.5117e+00)	Acc@1  57.81 ( 56.72)	Acc@5  85.94 ( 86.52)
Epoch: [19][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6377e+00 (1.5145e+00)	Acc@1  54.69 ( 56.68)	Acc@5  81.25 ( 86.43)
Epoch: [19][260/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5195e+00 (1.5141e+00)	Acc@1  51.56 ( 56.67)	Acc@5  90.62 ( 86.44)
Epoch: [19][270/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4639e+00 (1.5162e+00)	Acc@1  62.50 ( 56.60)	Acc@5  85.16 ( 86.43)
Epoch: [19][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5889e+00 (1.5158e+00)	Acc@1  50.78 ( 56.54)	Acc@5  87.50 ( 86.45)
Epoch: [19][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3721e+00 (1.5185e+00)	Acc@1  64.06 ( 56.50)	Acc@5  87.50 ( 86.38)
Epoch: [19][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3848e+00 (1.5205e+00)	Acc@1  64.06 ( 56.48)	Acc@5  89.06 ( 86.32)
Epoch: [19][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5498e+00 (1.5193e+00)	Acc@1  53.12 ( 56.50)	Acc@5  86.72 ( 86.34)
Epoch: [19][320/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3164e+00 (1.5199e+00)	Acc@1  66.41 ( 56.48)	Acc@5  88.28 ( 86.34)
Epoch: [19][330/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4893e+00 (1.5203e+00)	Acc@1  57.03 ( 56.46)	Acc@5  87.50 ( 86.37)
Epoch: [19][340/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3340e+00 (1.5170e+00)	Acc@1  58.59 ( 56.52)	Acc@5  89.06 ( 86.40)
Epoch: [19][350/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5518e+00 (1.5195e+00)	Acc@1  58.59 ( 56.47)	Acc@5  83.59 ( 86.35)
Epoch: [19][360/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4590e+00 (1.5213e+00)	Acc@1  57.03 ( 56.42)	Acc@5  88.28 ( 86.32)
Epoch: [19][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.9473e+00 (1.5235e+00)	Acc@1  50.00 ( 56.40)	Acc@5  77.34 ( 86.28)
Epoch: [19][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4521e+00 (1.5227e+00)	Acc@1  60.16 ( 56.41)	Acc@5  87.50 ( 86.30)
Epoch: [19][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4922e+00 (1.5227e+00)	Acc@1  55.00 ( 56.41)	Acc@5  92.50 ( 86.30)
## e[19] optimizer.zero_grad (sum) time: 0.4132351875305176
## e[19]       loss.backward (sum) time: 7.340169429779053
## e[19]      optimizer.step (sum) time: 3.5422563552856445
## epoch[19] training(only) time: 26.205293655395508
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.8369e+00 (1.8369e+00)	Acc@1  52.00 ( 52.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.034 ( 0.040)	Loss 1.7139e+00 (1.8030e+00)	Acc@1  53.00 ( 51.91)	Acc@5  83.00 ( 82.73)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.7080e+00 (1.8028e+00)	Acc@1  53.00 ( 51.24)	Acc@5  82.00 ( 82.29)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.9912e+00 (1.8058e+00)	Acc@1  47.00 ( 51.00)	Acc@5  77.00 ( 82.23)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.7949e+00 (1.7969e+00)	Acc@1  48.00 ( 50.46)	Acc@5  82.00 ( 82.51)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.8555e+00 (1.8250e+00)	Acc@1  56.00 ( 50.41)	Acc@5  79.00 ( 81.75)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.8867e+00 (1.8206e+00)	Acc@1  50.00 ( 50.30)	Acc@5  83.00 ( 81.87)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.7217e+00 (1.8124e+00)	Acc@1  52.00 ( 50.58)	Acc@5  81.00 ( 81.70)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.9727e+00 (1.8219e+00)	Acc@1  44.00 ( 50.27)	Acc@5  78.00 ( 81.63)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 2.0371e+00 (1.8158e+00)	Acc@1  48.00 ( 50.57)	Acc@5  82.00 ( 81.75)
 * Acc@1 50.410 Acc@5 81.730
### epoch[19] execution time: 29.0329532623291
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.222 ( 0.222)	Data  0.150 ( 0.150)	Loss 1.4297e+00 (1.4297e+00)	Acc@1  65.62 ( 65.62)	Acc@5  88.28 ( 88.28)
Epoch: [20][ 10/391]	Time  0.068 ( 0.079)	Data  0.001 ( 0.015)	Loss 1.6152e+00 (1.4407e+00)	Acc@1  52.34 ( 59.52)	Acc@5  85.94 ( 87.57)
Epoch: [20][ 20/391]	Time  0.068 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.4824e+00 (1.4375e+00)	Acc@1  57.81 ( 59.30)	Acc@5  88.28 ( 87.95)
Epoch: [20][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.5703e+00 (1.4341e+00)	Acc@1  53.12 ( 59.48)	Acc@5  82.03 ( 87.65)
Epoch: [20][ 40/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.3184e+00 (1.4214e+00)	Acc@1  62.50 ( 59.60)	Acc@5  89.84 ( 87.96)
Epoch: [20][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.3311e+00 (1.4223e+00)	Acc@1  58.59 ( 59.39)	Acc@5  89.84 ( 88.11)
Epoch: [20][ 60/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.3330e+00 (1.4108e+00)	Acc@1  60.16 ( 59.76)	Acc@5  86.72 ( 88.22)
Epoch: [20][ 70/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4004e+00 (1.4166e+00)	Acc@1  61.72 ( 59.61)	Acc@5  85.94 ( 87.86)
Epoch: [20][ 80/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4082e+00 (1.4330e+00)	Acc@1  61.72 ( 59.16)	Acc@5  86.72 ( 87.57)
Epoch: [20][ 90/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4902e+00 (1.4269e+00)	Acc@1  57.81 ( 59.36)	Acc@5  83.59 ( 87.61)
Epoch: [20][100/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.3086e+00 (1.4247e+00)	Acc@1  64.06 ( 59.42)	Acc@5  88.28 ( 87.56)
Epoch: [20][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3877e+00 (1.4273e+00)	Acc@1  56.25 ( 59.18)	Acc@5  86.72 ( 87.56)
Epoch: [20][120/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4375e+00 (1.4305e+00)	Acc@1  60.94 ( 59.15)	Acc@5  87.50 ( 87.50)
Epoch: [20][130/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4336e+00 (1.4347e+00)	Acc@1  60.94 ( 59.01)	Acc@5  88.28 ( 87.45)
Epoch: [20][140/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5049e+00 (1.4376e+00)	Acc@1  58.59 ( 58.94)	Acc@5  86.72 ( 87.43)
Epoch: [20][150/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5742e+00 (1.4408e+00)	Acc@1  56.25 ( 58.89)	Acc@5  86.72 ( 87.41)
Epoch: [20][160/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5723e+00 (1.4445e+00)	Acc@1  57.03 ( 58.82)	Acc@5  85.94 ( 87.31)
Epoch: [20][170/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2529e+00 (1.4391e+00)	Acc@1  64.06 ( 58.93)	Acc@5  89.06 ( 87.34)
Epoch: [20][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5127e+00 (1.4398e+00)	Acc@1  54.69 ( 58.84)	Acc@5  82.81 ( 87.34)
Epoch: [20][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5537e+00 (1.4435e+00)	Acc@1  58.59 ( 58.78)	Acc@5  85.94 ( 87.30)
Epoch: [20][200/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3242e+00 (1.4427e+00)	Acc@1  62.50 ( 58.79)	Acc@5  92.19 ( 87.38)
Epoch: [20][210/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4268e+00 (1.4427e+00)	Acc@1  59.38 ( 58.82)	Acc@5  88.28 ( 87.38)
Epoch: [20][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6387e+00 (1.4465e+00)	Acc@1  57.03 ( 58.72)	Acc@5  85.16 ( 87.36)
Epoch: [20][230/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3760e+00 (1.4482e+00)	Acc@1  61.72 ( 58.71)	Acc@5  89.84 ( 87.35)
Epoch: [20][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5420e+00 (1.4528e+00)	Acc@1  57.03 ( 58.55)	Acc@5  85.94 ( 87.29)
Epoch: [20][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5088e+00 (1.4535e+00)	Acc@1  59.38 ( 58.55)	Acc@5  86.72 ( 87.28)
Epoch: [20][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6641e+00 (1.4564e+00)	Acc@1  52.34 ( 58.50)	Acc@5  80.47 ( 87.19)
Epoch: [20][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.9229e+00 (1.4604e+00)	Acc@1  51.56 ( 58.40)	Acc@5  76.56 ( 87.13)
Epoch: [20][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3965e+00 (1.4638e+00)	Acc@1  59.38 ( 58.36)	Acc@5  86.72 ( 87.07)
Epoch: [20][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5430e+00 (1.4674e+00)	Acc@1  53.91 ( 58.27)	Acc@5  87.50 ( 86.99)
Epoch: [20][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5303e+00 (1.4718e+00)	Acc@1  51.56 ( 58.05)	Acc@5  88.28 ( 86.95)
Epoch: [20][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7217e+00 (1.4744e+00)	Acc@1  48.44 ( 57.99)	Acc@5  83.59 ( 86.94)
Epoch: [20][320/391]	Time  0.065 ( 0.067)	Data  0.002 ( 0.002)	Loss 1.5889e+00 (1.4756e+00)	Acc@1  55.47 ( 57.88)	Acc@5  87.50 ( 86.97)
Epoch: [20][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6572e+00 (1.4785e+00)	Acc@1  52.34 ( 57.82)	Acc@5  82.81 ( 86.89)
Epoch: [20][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3262e+00 (1.4791e+00)	Acc@1  62.50 ( 57.80)	Acc@5  88.28 ( 86.87)
Epoch: [20][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3633e+00 (1.4791e+00)	Acc@1  63.28 ( 57.77)	Acc@5  90.62 ( 86.87)
Epoch: [20][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.8760e+00 (1.4821e+00)	Acc@1  49.22 ( 57.67)	Acc@5  80.47 ( 86.86)
Epoch: [20][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5410e+00 (1.4842e+00)	Acc@1  59.38 ( 57.67)	Acc@5  82.81 ( 86.78)
Epoch: [20][380/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5088e+00 (1.4845e+00)	Acc@1  57.03 ( 57.64)	Acc@5  86.72 ( 86.81)
Epoch: [20][390/391]	Time  0.052 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5664e+00 (1.4839e+00)	Acc@1  58.75 ( 57.66)	Acc@5  88.75 ( 86.82)
## e[20] optimizer.zero_grad (sum) time: 0.41309022903442383
## e[20]       loss.backward (sum) time: 7.355844736099243
## e[20]      optimizer.step (sum) time: 3.5560100078582764
## epoch[20] training(only) time: 26.164718866348267
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.7324e+00 (1.7324e+00)	Acc@1  53.00 ( 53.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.8389e+00 (1.6355e+00)	Acc@1  51.00 ( 55.45)	Acc@5  85.00 ( 84.91)
Test: [ 20/100]	Time  0.030 ( 0.034)	Loss 1.5820e+00 (1.5901e+00)	Acc@1  58.00 ( 55.38)	Acc@5  84.00 ( 85.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7275e+00 (1.5941e+00)	Acc@1  54.00 ( 55.45)	Acc@5  84.00 ( 84.97)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.6133e+00 (1.5963e+00)	Acc@1  55.00 ( 55.00)	Acc@5  84.00 ( 85.07)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.5107e+00 (1.6069e+00)	Acc@1  57.00 ( 54.76)	Acc@5  84.00 ( 84.69)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.7100e+00 (1.6083e+00)	Acc@1  54.00 ( 54.75)	Acc@5  81.00 ( 84.64)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.6387e+00 (1.6194e+00)	Acc@1  55.00 ( 54.56)	Acc@5  87.00 ( 84.46)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.6924e+00 (1.6307e+00)	Acc@1  55.00 ( 54.42)	Acc@5  79.00 ( 84.07)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.9932e+00 (1.6215e+00)	Acc@1  50.00 ( 54.49)	Acc@5  80.00 ( 84.32)
 * Acc@1 54.520 Acc@5 84.410
### epoch[20] execution time: 29.001226663589478
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.223 ( 0.223)	Data  0.149 ( 0.149)	Loss 1.3252e+00 (1.3252e+00)	Acc@1  60.94 ( 60.94)	Acc@5  89.84 ( 89.84)
Epoch: [21][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.014)	Loss 1.2861e+00 (1.4279e+00)	Acc@1  57.81 ( 58.24)	Acc@5  89.84 ( 87.50)
Epoch: [21][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.2871e+00 (1.4279e+00)	Acc@1  66.41 ( 58.56)	Acc@5  89.06 ( 87.57)
Epoch: [21][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.3066e+00 (1.4134e+00)	Acc@1  64.06 ( 59.45)	Acc@5  85.94 ( 87.65)
Epoch: [21][ 40/391]	Time  0.072 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.3369e+00 (1.4131e+00)	Acc@1  60.16 ( 59.13)	Acc@5  89.06 ( 87.69)
Epoch: [21][ 50/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.2920e+00 (1.4129e+00)	Acc@1  61.72 ( 59.01)	Acc@5  90.62 ( 87.94)
Epoch: [21][ 60/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.5381e+00 (1.4106e+00)	Acc@1  57.81 ( 59.17)	Acc@5  88.28 ( 87.90)
Epoch: [21][ 70/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4287e+00 (1.4235e+00)	Acc@1  59.38 ( 58.68)	Acc@5  84.38 ( 87.62)
Epoch: [21][ 80/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.3975e+00 (1.4340e+00)	Acc@1  59.38 ( 58.44)	Acc@5  87.50 ( 87.46)
Epoch: [21][ 90/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.2861e+00 (1.4322e+00)	Acc@1  60.94 ( 58.53)	Acc@5  89.84 ( 87.46)
Epoch: [21][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3350e+00 (1.4353e+00)	Acc@1  64.84 ( 58.48)	Acc@5  87.50 ( 87.49)
Epoch: [21][110/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5723e+00 (1.4367e+00)	Acc@1  57.03 ( 58.51)	Acc@5  84.38 ( 87.54)
Epoch: [21][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3770e+00 (1.4378e+00)	Acc@1  58.59 ( 58.41)	Acc@5  90.62 ( 87.60)
Epoch: [21][130/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3486e+00 (1.4361e+00)	Acc@1  56.25 ( 58.43)	Acc@5  90.62 ( 87.66)
Epoch: [21][140/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2891e+00 (1.4322e+00)	Acc@1  65.62 ( 58.51)	Acc@5  88.28 ( 87.69)
Epoch: [21][150/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5186e+00 (1.4303e+00)	Acc@1  54.69 ( 58.47)	Acc@5  84.38 ( 87.72)
Epoch: [21][160/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5850e+00 (1.4324e+00)	Acc@1  53.91 ( 58.44)	Acc@5  85.94 ( 87.63)
Epoch: [21][170/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4375e+00 (1.4368e+00)	Acc@1  57.81 ( 58.31)	Acc@5  84.38 ( 87.58)
Epoch: [21][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5137e+00 (1.4381e+00)	Acc@1  57.81 ( 58.30)	Acc@5  87.50 ( 87.59)
Epoch: [21][190/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5127e+00 (1.4450e+00)	Acc@1  57.81 ( 58.18)	Acc@5  83.59 ( 87.44)
Epoch: [21][200/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2334e+00 (1.4477e+00)	Acc@1  61.72 ( 58.10)	Acc@5  90.62 ( 87.43)
Epoch: [21][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5303e+00 (1.4519e+00)	Acc@1  53.91 ( 57.88)	Acc@5  86.72 ( 87.36)
Epoch: [21][220/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6143e+00 (1.4534e+00)	Acc@1  54.69 ( 57.83)	Acc@5  81.25 ( 87.31)
Epoch: [21][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5098e+00 (1.4521e+00)	Acc@1  55.47 ( 57.80)	Acc@5  82.81 ( 87.36)
Epoch: [21][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3701e+00 (1.4524e+00)	Acc@1  57.81 ( 57.73)	Acc@5  92.19 ( 87.39)
Epoch: [21][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6240e+00 (1.4543e+00)	Acc@1  53.12 ( 57.70)	Acc@5  83.59 ( 87.34)
Epoch: [21][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4180e+00 (1.4563e+00)	Acc@1  60.94 ( 57.62)	Acc@5  87.50 ( 87.31)
Epoch: [21][270/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5156e+00 (1.4587e+00)	Acc@1  54.69 ( 57.56)	Acc@5  85.94 ( 87.28)
Epoch: [21][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4932e+00 (1.4592e+00)	Acc@1  56.25 ( 57.55)	Acc@5  86.72 ( 87.27)
Epoch: [21][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6064e+00 (1.4588e+00)	Acc@1  51.56 ( 57.57)	Acc@5  85.94 ( 87.26)
Epoch: [21][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4814e+00 (1.4594e+00)	Acc@1  64.06 ( 57.61)	Acc@5  86.72 ( 87.24)
Epoch: [21][310/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3496e+00 (1.4597e+00)	Acc@1  61.72 ( 57.62)	Acc@5  90.62 ( 87.23)
Epoch: [21][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4355e+00 (1.4619e+00)	Acc@1  58.59 ( 57.61)	Acc@5  85.94 ( 87.16)
Epoch: [21][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6445e+00 (1.4599e+00)	Acc@1  48.44 ( 57.68)	Acc@5  82.81 ( 87.18)
Epoch: [21][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5791e+00 (1.4629e+00)	Acc@1  51.56 ( 57.65)	Acc@5  86.72 ( 87.13)
Epoch: [21][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.7012e+00 (1.4630e+00)	Acc@1  49.22 ( 57.63)	Acc@5  86.72 ( 87.14)
Epoch: [21][360/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4297e+00 (1.4625e+00)	Acc@1  60.94 ( 57.67)	Acc@5  88.28 ( 87.17)
Epoch: [21][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.7217e+00 (1.4643e+00)	Acc@1  51.56 ( 57.64)	Acc@5  79.69 ( 87.12)
Epoch: [21][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5547e+00 (1.4664e+00)	Acc@1  57.03 ( 57.60)	Acc@5  85.16 ( 87.09)
Epoch: [21][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5068e+00 (1.4668e+00)	Acc@1  53.75 ( 57.60)	Acc@5  86.25 ( 87.08)
## e[21] optimizer.zero_grad (sum) time: 0.407543420791626
## e[21]       loss.backward (sum) time: 7.365862131118774
## e[21]      optimizer.step (sum) time: 3.608004570007324
## epoch[21] training(only) time: 26.244382858276367
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.6396e+00 (1.6396e+00)	Acc@1  56.00 ( 56.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.8330e+00 (1.7348e+00)	Acc@1  53.00 ( 54.36)	Acc@5  82.00 ( 82.09)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6357e+00 (1.7141e+00)	Acc@1  55.00 ( 53.76)	Acc@5  85.00 ( 83.14)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7100e+00 (1.7093e+00)	Acc@1  55.00 ( 53.61)	Acc@5  85.00 ( 83.39)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.5693e+00 (1.6907e+00)	Acc@1  53.00 ( 53.41)	Acc@5  86.00 ( 83.59)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.6338e+00 (1.6977e+00)	Acc@1  55.00 ( 53.41)	Acc@5  79.00 ( 83.47)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.6328e+00 (1.6945e+00)	Acc@1  58.00 ( 53.44)	Acc@5  83.00 ( 83.52)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.6953e+00 (1.6988e+00)	Acc@1  54.00 ( 53.20)	Acc@5  85.00 ( 83.49)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.6914e+00 (1.7169e+00)	Acc@1  53.00 ( 52.70)	Acc@5  80.00 ( 83.17)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.1445e+00 (1.7132e+00)	Acc@1  47.00 ( 53.01)	Acc@5  75.00 ( 83.24)
 * Acc@1 53.200 Acc@5 83.260
### epoch[21] execution time: 29.0830397605896
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.216 ( 0.216)	Data  0.137 ( 0.137)	Loss 1.2871e+00 (1.2871e+00)	Acc@1  64.06 ( 64.06)	Acc@5  87.50 ( 87.50)
Epoch: [22][ 10/391]	Time  0.062 ( 0.079)	Data  0.001 ( 0.013)	Loss 1.3906e+00 (1.3401e+00)	Acc@1  60.16 ( 61.93)	Acc@5  88.28 ( 88.64)
Epoch: [22][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.4814e+00 (1.3633e+00)	Acc@1  60.16 ( 61.72)	Acc@5  87.50 ( 88.58)
Epoch: [22][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.3477e+00 (1.3676e+00)	Acc@1  62.50 ( 61.16)	Acc@5  92.97 ( 88.48)
Epoch: [22][ 40/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.4111e+00 (1.3696e+00)	Acc@1  59.38 ( 60.42)	Acc@5  88.28 ( 88.43)
Epoch: [22][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.2812e+00 (1.3660e+00)	Acc@1  60.94 ( 60.83)	Acc@5  92.19 ( 88.57)
Epoch: [22][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.4746e+00 (1.3611e+00)	Acc@1  58.59 ( 60.71)	Acc@5  85.94 ( 88.60)
Epoch: [22][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.4355e+00 (1.3618e+00)	Acc@1  59.38 ( 60.56)	Acc@5  87.50 ( 88.75)
Epoch: [22][ 80/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4854e+00 (1.3688e+00)	Acc@1  56.25 ( 60.11)	Acc@5  89.84 ( 88.76)
Epoch: [22][ 90/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4307e+00 (1.3773e+00)	Acc@1  56.25 ( 60.09)	Acc@5  86.72 ( 88.50)
Epoch: [22][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2568e+00 (1.3783e+00)	Acc@1  59.38 ( 60.08)	Acc@5  91.41 ( 88.44)
Epoch: [22][110/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6748e+00 (1.3886e+00)	Acc@1  50.00 ( 59.90)	Acc@5  82.03 ( 88.25)
Epoch: [22][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2100e+00 (1.3967e+00)	Acc@1  64.06 ( 59.71)	Acc@5  93.75 ( 88.09)
Epoch: [22][130/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5000e+00 (1.4001e+00)	Acc@1  51.56 ( 59.57)	Acc@5  90.62 ( 88.13)
Epoch: [22][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4297e+00 (1.4020e+00)	Acc@1  58.59 ( 59.43)	Acc@5  87.50 ( 88.10)
Epoch: [22][150/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3994e+00 (1.4095e+00)	Acc@1  65.62 ( 59.28)	Acc@5  85.16 ( 87.94)
Epoch: [22][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2441e+00 (1.4082e+00)	Acc@1  61.72 ( 59.34)	Acc@5  90.62 ( 88.00)
Epoch: [22][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3076e+00 (1.4117e+00)	Acc@1  64.06 ( 59.18)	Acc@5  87.50 ( 87.98)
Epoch: [22][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4072e+00 (1.4122e+00)	Acc@1  53.91 ( 59.07)	Acc@5  91.41 ( 88.00)
Epoch: [22][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4297e+00 (1.4130e+00)	Acc@1  57.03 ( 59.07)	Acc@5  85.16 ( 87.96)
Epoch: [22][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2920e+00 (1.4132e+00)	Acc@1  61.72 ( 58.97)	Acc@5  92.97 ( 87.97)
Epoch: [22][210/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4121e+00 (1.4141e+00)	Acc@1  58.59 ( 58.91)	Acc@5  86.72 ( 87.97)
Epoch: [22][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3799e+00 (1.4141e+00)	Acc@1  64.84 ( 58.92)	Acc@5  85.94 ( 88.00)
Epoch: [22][230/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2939e+00 (1.4162e+00)	Acc@1  60.16 ( 58.80)	Acc@5  89.06 ( 87.98)
Epoch: [22][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2422e+00 (1.4173e+00)	Acc@1  64.84 ( 58.82)	Acc@5  89.84 ( 87.91)
Epoch: [22][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6553e+00 (1.4194e+00)	Acc@1  49.22 ( 58.77)	Acc@5  85.16 ( 87.88)
Epoch: [22][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2705e+00 (1.4187e+00)	Acc@1  65.62 ( 58.82)	Acc@5  90.62 ( 87.93)
Epoch: [22][270/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6641e+00 (1.4228e+00)	Acc@1  52.34 ( 58.76)	Acc@5  80.47 ( 87.81)
Epoch: [22][280/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6328e+00 (1.4251e+00)	Acc@1  57.81 ( 58.67)	Acc@5  83.59 ( 87.80)
Epoch: [22][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4961e+00 (1.4284e+00)	Acc@1  55.47 ( 58.56)	Acc@5  87.50 ( 87.73)
Epoch: [22][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5840e+00 (1.4306e+00)	Acc@1  57.03 ( 58.53)	Acc@5  84.38 ( 87.71)
Epoch: [22][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3936e+00 (1.4297e+00)	Acc@1  61.72 ( 58.52)	Acc@5  88.28 ( 87.72)
Epoch: [22][320/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5342e+00 (1.4300e+00)	Acc@1  51.56 ( 58.48)	Acc@5  87.50 ( 87.74)
Epoch: [22][330/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4688e+00 (1.4317e+00)	Acc@1  59.38 ( 58.47)	Acc@5  86.72 ( 87.73)
Epoch: [22][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5225e+00 (1.4328e+00)	Acc@1  57.81 ( 58.48)	Acc@5  87.50 ( 87.72)
Epoch: [22][350/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4561e+00 (1.4337e+00)	Acc@1  60.16 ( 58.50)	Acc@5  86.72 ( 87.70)
Epoch: [22][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.2500e+00 (1.4334e+00)	Acc@1  70.31 ( 58.57)	Acc@5  88.28 ( 87.70)
Epoch: [22][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5469e+00 (1.4354e+00)	Acc@1  60.16 ( 58.56)	Acc@5  79.69 ( 87.63)
Epoch: [22][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.7422e+00 (1.4360e+00)	Acc@1  48.44 ( 58.55)	Acc@5  82.81 ( 87.63)
Epoch: [22][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5713e+00 (1.4361e+00)	Acc@1  57.50 ( 58.54)	Acc@5  80.00 ( 87.63)
## e[22] optimizer.zero_grad (sum) time: 0.40558767318725586
## e[22]       loss.backward (sum) time: 7.376543760299683
## e[22]      optimizer.step (sum) time: 3.6231722831726074
## epoch[22] training(only) time: 26.240607738494873
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.7451e+00 (1.7451e+00)	Acc@1  54.00 ( 54.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.5811e+00 (1.5853e+00)	Acc@1  52.00 ( 56.82)	Acc@5  89.00 ( 85.73)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.6641e+00 (1.5715e+00)	Acc@1  57.00 ( 56.86)	Acc@5  85.00 ( 85.81)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6680e+00 (1.5678e+00)	Acc@1  49.00 ( 56.52)	Acc@5  85.00 ( 85.29)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.6152e+00 (1.5632e+00)	Acc@1  54.00 ( 56.29)	Acc@5  79.00 ( 85.02)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.4219e+00 (1.5815e+00)	Acc@1  58.00 ( 55.86)	Acc@5  84.00 ( 84.61)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.6240e+00 (1.5802e+00)	Acc@1  54.00 ( 55.70)	Acc@5  81.00 ( 84.46)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.5664e+00 (1.5909e+00)	Acc@1  52.00 ( 55.45)	Acc@5  85.00 ( 84.31)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.6338e+00 (1.6021e+00)	Acc@1  55.00 ( 55.23)	Acc@5  82.00 ( 84.11)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8145e+00 (1.5885e+00)	Acc@1  57.00 ( 55.41)	Acc@5  77.00 ( 84.24)
 * Acc@1 55.550 Acc@5 84.340
### epoch[22] execution time: 29.04094433784485
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.222 ( 0.222)	Data  0.148 ( 0.148)	Loss 1.4316e+00 (1.4316e+00)	Acc@1  61.72 ( 61.72)	Acc@5  85.16 ( 85.16)
Epoch: [23][ 10/391]	Time  0.069 ( 0.081)	Data  0.001 ( 0.014)	Loss 1.4014e+00 (1.3861e+00)	Acc@1  60.16 ( 60.23)	Acc@5  86.72 ( 88.64)
Epoch: [23][ 20/391]	Time  0.069 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.3125e+00 (1.3734e+00)	Acc@1  59.38 ( 60.60)	Acc@5  91.41 ( 88.62)
Epoch: [23][ 30/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.006)	Loss 1.3760e+00 (1.3632e+00)	Acc@1  60.16 ( 60.89)	Acc@5  87.50 ( 88.61)
Epoch: [23][ 40/391]	Time  0.060 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.0479e+00 (1.3493e+00)	Acc@1  63.28 ( 61.01)	Acc@5  93.75 ( 88.81)
Epoch: [23][ 50/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.4961e+00 (1.3479e+00)	Acc@1  63.28 ( 60.94)	Acc@5  84.38 ( 88.86)
Epoch: [23][ 60/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.2539e+00 (1.3578e+00)	Acc@1  64.06 ( 60.76)	Acc@5  88.28 ( 88.61)
Epoch: [23][ 70/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.3672e+00 (1.3635e+00)	Acc@1  59.38 ( 60.63)	Acc@5  88.28 ( 88.40)
Epoch: [23][ 80/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.2246e+00 (1.3571e+00)	Acc@1  67.97 ( 60.87)	Acc@5  87.50 ( 88.56)
Epoch: [23][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.3496e+00 (1.3560e+00)	Acc@1  64.06 ( 60.80)	Acc@5  85.94 ( 88.54)
Epoch: [23][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.6035e+00 (1.3632e+00)	Acc@1  56.25 ( 60.70)	Acc@5  85.16 ( 88.45)
Epoch: [23][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6729e+00 (1.3691e+00)	Acc@1  53.91 ( 60.47)	Acc@5  84.38 ( 88.47)
Epoch: [23][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5645e+00 (1.3804e+00)	Acc@1  62.50 ( 60.20)	Acc@5  85.94 ( 88.31)
Epoch: [23][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3770e+00 (1.3845e+00)	Acc@1  53.91 ( 60.01)	Acc@5  90.62 ( 88.33)
Epoch: [23][140/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3955e+00 (1.3890e+00)	Acc@1  57.03 ( 59.88)	Acc@5  89.06 ( 88.20)
Epoch: [23][150/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4189e+00 (1.3948e+00)	Acc@1  56.25 ( 59.72)	Acc@5  84.38 ( 88.11)
Epoch: [23][160/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3145e+00 (1.4001e+00)	Acc@1  59.38 ( 59.55)	Acc@5  88.28 ( 87.98)
Epoch: [23][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3574e+00 (1.4004e+00)	Acc@1  61.72 ( 59.57)	Acc@5  85.16 ( 87.94)
Epoch: [23][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2900e+00 (1.3998e+00)	Acc@1  62.50 ( 59.61)	Acc@5  87.50 ( 87.98)
Epoch: [23][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2969e+00 (1.3988e+00)	Acc@1  57.03 ( 59.65)	Acc@5  89.84 ( 88.04)
Epoch: [23][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3740e+00 (1.3999e+00)	Acc@1  61.72 ( 59.63)	Acc@5  89.06 ( 88.01)
Epoch: [23][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5361e+00 (1.3987e+00)	Acc@1  58.59 ( 59.67)	Acc@5  85.16 ( 88.01)
Epoch: [23][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2578e+00 (1.3975e+00)	Acc@1  62.50 ( 59.73)	Acc@5  91.41 ( 88.00)
Epoch: [23][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5225e+00 (1.3980e+00)	Acc@1  63.28 ( 59.69)	Acc@5  78.91 ( 87.94)
Epoch: [23][240/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3203e+00 (1.3991e+00)	Acc@1  58.59 ( 59.62)	Acc@5  92.19 ( 87.99)
Epoch: [23][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2295e+00 (1.4002e+00)	Acc@1  61.72 ( 59.58)	Acc@5  89.06 ( 87.93)
Epoch: [23][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1738e+00 (1.4015e+00)	Acc@1  61.72 ( 59.55)	Acc@5  91.41 ( 87.92)
Epoch: [23][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4678e+00 (1.4020e+00)	Acc@1  59.38 ( 59.53)	Acc@5  88.28 ( 87.92)
Epoch: [23][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5908e+00 (1.4044e+00)	Acc@1  50.78 ( 59.42)	Acc@5  87.50 ( 87.92)
Epoch: [23][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5186e+00 (1.4066e+00)	Acc@1  53.12 ( 59.35)	Acc@5  88.28 ( 87.87)
Epoch: [23][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5068e+00 (1.4069e+00)	Acc@1  58.59 ( 59.34)	Acc@5  86.72 ( 87.88)
Epoch: [23][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1826e+00 (1.4042e+00)	Acc@1  62.50 ( 59.38)	Acc@5  92.19 ( 87.93)
Epoch: [23][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3057e+00 (1.4036e+00)	Acc@1  60.94 ( 59.39)	Acc@5  90.62 ( 87.94)
Epoch: [23][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3164e+00 (1.4024e+00)	Acc@1  62.50 ( 59.44)	Acc@5  89.06 ( 87.97)
Epoch: [23][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4111e+00 (1.4051e+00)	Acc@1  59.38 ( 59.41)	Acc@5  89.06 ( 87.94)
Epoch: [23][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6582e+00 (1.4071e+00)	Acc@1  55.47 ( 59.39)	Acc@5  85.16 ( 87.92)
Epoch: [23][360/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4482e+00 (1.4080e+00)	Acc@1  57.03 ( 59.37)	Acc@5  86.72 ( 87.88)
Epoch: [23][370/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.3594e+00 (1.4075e+00)	Acc@1  61.72 ( 59.35)	Acc@5  89.06 ( 87.90)
Epoch: [23][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.3652e+00 (1.4083e+00)	Acc@1  59.38 ( 59.32)	Acc@5  90.62 ( 87.90)
Epoch: [23][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6924e+00 (1.4085e+00)	Acc@1  53.75 ( 59.35)	Acc@5  81.25 ( 87.90)
## e[23] optimizer.zero_grad (sum) time: 0.40846729278564453
## e[23]       loss.backward (sum) time: 7.364377737045288
## e[23]      optimizer.step (sum) time: 3.5048904418945312
## epoch[23] training(only) time: 26.12566089630127
# Switched to evaluate mode...
Test: [  0/100]	Time  0.165 ( 0.165)	Loss 1.7432e+00 (1.7432e+00)	Acc@1  56.00 ( 56.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.9219e+00 (1.7123e+00)	Acc@1  48.00 ( 54.82)	Acc@5  82.00 ( 82.82)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5898e+00 (1.7016e+00)	Acc@1  57.00 ( 53.90)	Acc@5  84.00 ( 83.29)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6973e+00 (1.7070e+00)	Acc@1  47.00 ( 53.26)	Acc@5  79.00 ( 82.68)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.6953e+00 (1.6967e+00)	Acc@1  48.00 ( 53.24)	Acc@5  86.00 ( 83.15)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.6738e+00 (1.7017e+00)	Acc@1  53.00 ( 53.22)	Acc@5  85.00 ( 82.90)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.7246e+00 (1.7063e+00)	Acc@1  49.00 ( 53.00)	Acc@5  82.00 ( 82.75)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.6553e+00 (1.7109e+00)	Acc@1  52.00 ( 53.08)	Acc@5  81.00 ( 82.79)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.7900e+00 (1.7280e+00)	Acc@1  52.00 ( 52.94)	Acc@5  79.00 ( 82.42)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 1.9326e+00 (1.7246e+00)	Acc@1  56.00 ( 53.07)	Acc@5  78.00 ( 82.63)
 * Acc@1 53.310 Acc@5 82.750
### epoch[23] execution time: 28.949402570724487
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.226 ( 0.226)	Data  0.152 ( 0.152)	Loss 1.2744e+00 (1.2744e+00)	Acc@1  60.94 ( 60.94)	Acc@5  92.97 ( 92.97)
Epoch: [24][ 10/391]	Time  0.068 ( 0.080)	Data  0.001 ( 0.015)	Loss 1.2773e+00 (1.2761e+00)	Acc@1  64.84 ( 62.43)	Acc@5  89.84 ( 90.20)
Epoch: [24][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.3525e+00 (1.2931e+00)	Acc@1  59.38 ( 62.83)	Acc@5  89.84 ( 89.77)
Epoch: [24][ 30/391]	Time  0.069 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.0488e+00 (1.2887e+00)	Acc@1  70.31 ( 63.23)	Acc@5  92.19 ( 89.64)
Epoch: [24][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.4785e+00 (1.3006e+00)	Acc@1  62.50 ( 62.86)	Acc@5  86.72 ( 89.33)
Epoch: [24][ 50/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.3838e+00 (1.3177e+00)	Acc@1  57.81 ( 62.24)	Acc@5  92.19 ( 89.34)
Epoch: [24][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.2051e+00 (1.3200e+00)	Acc@1  63.28 ( 61.85)	Acc@5  89.84 ( 89.31)
Epoch: [24][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.5420e+00 (1.3242e+00)	Acc@1  53.12 ( 61.69)	Acc@5  85.94 ( 89.27)
Epoch: [24][ 80/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4854e+00 (1.3315e+00)	Acc@1  59.38 ( 61.20)	Acc@5  85.16 ( 89.32)
Epoch: [24][ 90/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4150e+00 (1.3323e+00)	Acc@1  57.03 ( 61.20)	Acc@5  87.50 ( 89.25)
Epoch: [24][100/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.2432e+00 (1.3401e+00)	Acc@1  64.84 ( 60.95)	Acc@5  92.19 ( 89.15)
Epoch: [24][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4814e+00 (1.3403e+00)	Acc@1  58.59 ( 60.94)	Acc@5  83.59 ( 89.16)
Epoch: [24][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4648e+00 (1.3492e+00)	Acc@1  65.62 ( 60.80)	Acc@5  87.50 ( 88.96)
Epoch: [24][130/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5098e+00 (1.3501e+00)	Acc@1  53.91 ( 60.78)	Acc@5  85.94 ( 88.94)
Epoch: [24][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3740e+00 (1.3508e+00)	Acc@1  61.72 ( 60.81)	Acc@5  89.84 ( 88.92)
Epoch: [24][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2314e+00 (1.3542e+00)	Acc@1  64.84 ( 60.84)	Acc@5  91.41 ( 88.79)
Epoch: [24][160/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4424e+00 (1.3548e+00)	Acc@1  55.47 ( 60.86)	Acc@5  88.28 ( 88.75)
Epoch: [24][170/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2500e+00 (1.3561e+00)	Acc@1  57.81 ( 60.76)	Acc@5  93.75 ( 88.75)
Epoch: [24][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3350e+00 (1.3570e+00)	Acc@1  63.28 ( 60.79)	Acc@5  89.84 ( 88.74)
Epoch: [24][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.8779e+00 (1.3637e+00)	Acc@1  48.44 ( 60.57)	Acc@5  83.59 ( 88.69)
Epoch: [24][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2725e+00 (1.3647e+00)	Acc@1  60.16 ( 60.51)	Acc@5  89.84 ( 88.67)
Epoch: [24][210/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3828e+00 (1.3698e+00)	Acc@1  64.06 ( 60.42)	Acc@5  89.84 ( 88.63)
Epoch: [24][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1826e+00 (1.3731e+00)	Acc@1  66.41 ( 60.35)	Acc@5  90.62 ( 88.57)
Epoch: [24][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1699e+00 (1.3736e+00)	Acc@1  66.41 ( 60.28)	Acc@5  90.62 ( 88.57)
Epoch: [24][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3848e+00 (1.3745e+00)	Acc@1  61.72 ( 60.29)	Acc@5  87.50 ( 88.55)
Epoch: [24][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2178e+00 (1.3751e+00)	Acc@1  65.62 ( 60.26)	Acc@5  89.06 ( 88.55)
Epoch: [24][260/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4375e+00 (1.3753e+00)	Acc@1  53.12 ( 60.20)	Acc@5  86.72 ( 88.54)
Epoch: [24][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3574e+00 (1.3784e+00)	Acc@1  60.16 ( 60.07)	Acc@5  86.72 ( 88.46)
Epoch: [24][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6045e+00 (1.3792e+00)	Acc@1  48.44 ( 60.00)	Acc@5  86.72 ( 88.46)
Epoch: [24][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0586e+00 (1.3768e+00)	Acc@1  68.75 ( 60.09)	Acc@5  89.84 ( 88.46)
Epoch: [24][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2549e+00 (1.3765e+00)	Acc@1  60.94 ( 60.14)	Acc@5  92.97 ( 88.47)
Epoch: [24][310/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5078e+00 (1.3797e+00)	Acc@1  58.59 ( 60.06)	Acc@5  84.38 ( 88.41)
Epoch: [24][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4336e+00 (1.3794e+00)	Acc@1  56.25 ( 60.08)	Acc@5  88.28 ( 88.44)
Epoch: [24][330/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6553e+00 (1.3804e+00)	Acc@1  57.81 ( 60.04)	Acc@5  79.69 ( 88.40)
Epoch: [24][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1143e+00 (1.3811e+00)	Acc@1  71.09 ( 60.06)	Acc@5  90.62 ( 88.40)
Epoch: [24][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4121e+00 (1.3823e+00)	Acc@1  59.38 ( 60.02)	Acc@5  87.50 ( 88.37)
Epoch: [24][360/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3428e+00 (1.3842e+00)	Acc@1  58.59 ( 59.97)	Acc@5  92.97 ( 88.36)
Epoch: [24][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4424e+00 (1.3863e+00)	Acc@1  56.25 ( 59.89)	Acc@5  85.94 ( 88.33)
Epoch: [24][380/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.3438e+00 (1.3864e+00)	Acc@1  64.06 ( 59.90)	Acc@5  86.72 ( 88.34)
Epoch: [24][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4932e+00 (1.3868e+00)	Acc@1  60.00 ( 59.91)	Acc@5  86.25 ( 88.31)
## e[24] optimizer.zero_grad (sum) time: 0.41344380378723145
## e[24]       loss.backward (sum) time: 7.371971130371094
## e[24]      optimizer.step (sum) time: 3.580111265182495
## epoch[24] training(only) time: 26.168201446533203
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.6348e+00 (1.6348e+00)	Acc@1  55.00 ( 55.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.7080e+00 (1.6420e+00)	Acc@1  49.00 ( 55.55)	Acc@5  85.00 ( 83.64)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.6084e+00 (1.6058e+00)	Acc@1  56.00 ( 55.62)	Acc@5  85.00 ( 84.38)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6484e+00 (1.6163e+00)	Acc@1  50.00 ( 55.19)	Acc@5  81.00 ( 84.10)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.6602e+00 (1.5988e+00)	Acc@1  51.00 ( 55.51)	Acc@5  86.00 ( 84.49)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.6230e+00 (1.6057e+00)	Acc@1  58.00 ( 55.22)	Acc@5  82.00 ( 84.33)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.5312e+00 (1.6069e+00)	Acc@1  61.00 ( 55.18)	Acc@5  88.00 ( 84.28)
Test: [ 70/100]	Time  0.030 ( 0.028)	Loss 1.6738e+00 (1.6227e+00)	Acc@1  48.00 ( 54.75)	Acc@5  89.00 ( 84.13)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.7637e+00 (1.6341e+00)	Acc@1  54.00 ( 54.38)	Acc@5  81.00 ( 83.95)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8818e+00 (1.6276e+00)	Acc@1  52.00 ( 54.51)	Acc@5  83.00 ( 84.01)
 * Acc@1 54.690 Acc@5 84.070
### epoch[24] execution time: 28.979415893554688
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.220 ( 0.220)	Data  0.145 ( 0.145)	Loss 1.1133e+00 (1.1133e+00)	Acc@1  65.62 ( 65.62)	Acc@5  92.97 ( 92.97)
Epoch: [25][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.014)	Loss 1.2949e+00 (1.2427e+00)	Acc@1  63.28 ( 63.71)	Acc@5  89.84 ( 91.12)
Epoch: [25][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.4062e+00 (1.2481e+00)	Acc@1  60.16 ( 63.50)	Acc@5  86.72 ( 90.33)
Epoch: [25][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.2568e+00 (1.2581e+00)	Acc@1  67.19 ( 63.33)	Acc@5  88.28 ( 90.20)
Epoch: [25][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.3828e+00 (1.2773e+00)	Acc@1  60.94 ( 62.67)	Acc@5  89.84 ( 89.75)
Epoch: [25][ 50/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.004)	Loss 9.9463e-01 (1.2818e+00)	Acc@1  72.66 ( 62.65)	Acc@5  93.75 ( 89.72)
Epoch: [25][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.3369e+00 (1.2940e+00)	Acc@1  56.25 ( 62.42)	Acc@5  89.06 ( 89.41)
Epoch: [25][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.4727e+00 (1.2998e+00)	Acc@1  60.16 ( 62.46)	Acc@5  86.72 ( 89.44)
Epoch: [25][ 80/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.5879e+00 (1.3081e+00)	Acc@1  50.00 ( 62.12)	Acc@5  85.94 ( 89.41)
Epoch: [25][ 90/391]	Time  0.077 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.3447e+00 (1.3201e+00)	Acc@1  60.16 ( 61.53)	Acc@5  88.28 ( 89.34)
Epoch: [25][100/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3145e+00 (1.3225e+00)	Acc@1  64.84 ( 61.53)	Acc@5  88.28 ( 89.26)
Epoch: [25][110/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3672e+00 (1.3235e+00)	Acc@1  58.59 ( 61.44)	Acc@5  89.06 ( 89.21)
Epoch: [25][120/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6348e+00 (1.3277e+00)	Acc@1  53.91 ( 61.32)	Acc@5  84.38 ( 89.18)
Epoch: [25][130/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2012e+00 (1.3319e+00)	Acc@1  64.06 ( 61.20)	Acc@5  90.62 ( 89.07)
Epoch: [25][140/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4922e+00 (1.3354e+00)	Acc@1  57.81 ( 61.13)	Acc@5  89.84 ( 89.08)
Epoch: [25][150/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4277e+00 (1.3351e+00)	Acc@1  62.50 ( 61.14)	Acc@5  85.16 ( 89.06)
Epoch: [25][160/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6104e+00 (1.3336e+00)	Acc@1  60.94 ( 61.15)	Acc@5  83.59 ( 89.09)
Epoch: [25][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4609e+00 (1.3371e+00)	Acc@1  54.69 ( 61.02)	Acc@5  86.72 ( 89.04)
Epoch: [25][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3086e+00 (1.3400e+00)	Acc@1  60.16 ( 60.93)	Acc@5  88.28 ( 88.97)
Epoch: [25][190/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5254e+00 (1.3395e+00)	Acc@1  60.16 ( 61.00)	Acc@5  85.94 ( 88.97)
Epoch: [25][200/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4336e+00 (1.3475e+00)	Acc@1  63.28 ( 60.84)	Acc@5  89.84 ( 88.79)
Epoch: [25][210/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3320e+00 (1.3491e+00)	Acc@1  63.28 ( 60.79)	Acc@5  87.50 ( 88.80)
Epoch: [25][220/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5371e+00 (1.3516e+00)	Acc@1  56.25 ( 60.71)	Acc@5  88.28 ( 88.79)
Epoch: [25][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2246e+00 (1.3507e+00)	Acc@1  71.09 ( 60.74)	Acc@5  92.19 ( 88.83)
Epoch: [25][240/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3652e+00 (1.3503e+00)	Acc@1  57.81 ( 60.79)	Acc@5  89.06 ( 88.83)
Epoch: [25][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3965e+00 (1.3502e+00)	Acc@1  59.38 ( 60.77)	Acc@5  91.41 ( 88.85)
Epoch: [25][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2783e+00 (1.3520e+00)	Acc@1  63.28 ( 60.70)	Acc@5  91.41 ( 88.85)
Epoch: [25][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1855e+00 (1.3495e+00)	Acc@1  62.50 ( 60.72)	Acc@5  91.41 ( 88.88)
Epoch: [25][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2773e+00 (1.3497e+00)	Acc@1  64.06 ( 60.74)	Acc@5  90.62 ( 88.87)
Epoch: [25][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1084e+00 (1.3480e+00)	Acc@1  66.41 ( 60.77)	Acc@5  89.84 ( 88.86)
Epoch: [25][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1631e+00 (1.3488e+00)	Acc@1  64.84 ( 60.73)	Acc@5  92.97 ( 88.87)
Epoch: [25][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1914e+00 (1.3490e+00)	Acc@1  64.84 ( 60.69)	Acc@5  90.62 ( 88.89)
Epoch: [25][320/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6318e+00 (1.3484e+00)	Acc@1  55.47 ( 60.70)	Acc@5  82.03 ( 88.90)
Epoch: [25][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4268e+00 (1.3509e+00)	Acc@1  53.91 ( 60.62)	Acc@5  87.50 ( 88.86)
Epoch: [25][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1279e+00 (1.3508e+00)	Acc@1  68.75 ( 60.66)	Acc@5  91.41 ( 88.86)
Epoch: [25][350/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4082e+00 (1.3514e+00)	Acc@1  55.47 ( 60.67)	Acc@5  89.84 ( 88.83)
Epoch: [25][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6221e+00 (1.3546e+00)	Acc@1  51.56 ( 60.60)	Acc@5  89.06 ( 88.83)
Epoch: [25][370/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.3369e+00 (1.3556e+00)	Acc@1  60.94 ( 60.57)	Acc@5  90.62 ( 88.82)
Epoch: [25][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4932e+00 (1.3563e+00)	Acc@1  59.38 ( 60.56)	Acc@5  84.38 ( 88.81)
Epoch: [25][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5293e+00 (1.3571e+00)	Acc@1  58.75 ( 60.56)	Acc@5  85.00 ( 88.76)
## e[25] optimizer.zero_grad (sum) time: 0.4055955410003662
## e[25]       loss.backward (sum) time: 7.36386513710022
## e[25]      optimizer.step (sum) time: 3.6024856567382812
## epoch[25] training(only) time: 26.201053142547607
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.5742e+00 (1.5742e+00)	Acc@1  60.00 ( 60.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.6250e+00 (1.5501e+00)	Acc@1  56.00 ( 57.64)	Acc@5  87.00 ( 84.27)
Test: [ 20/100]	Time  0.029 ( 0.033)	Loss 1.4541e+00 (1.5637e+00)	Acc@1  58.00 ( 57.05)	Acc@5  84.00 ( 84.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6875e+00 (1.5706e+00)	Acc@1  55.00 ( 56.84)	Acc@5  83.00 ( 84.29)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.6123e+00 (1.5652e+00)	Acc@1  49.00 ( 56.37)	Acc@5  86.00 ( 84.54)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.4326e+00 (1.5774e+00)	Acc@1  60.00 ( 56.29)	Acc@5  85.00 ( 84.55)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.7676e+00 (1.5770e+00)	Acc@1  46.00 ( 55.90)	Acc@5  84.00 ( 84.80)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.5586e+00 (1.5867e+00)	Acc@1  59.00 ( 55.75)	Acc@5  86.00 ( 84.79)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.8643e+00 (1.6024e+00)	Acc@1  52.00 ( 55.35)	Acc@5  82.00 ( 84.64)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8486e+00 (1.5991e+00)	Acc@1  52.00 ( 55.52)	Acc@5  83.00 ( 84.75)
 * Acc@1 55.830 Acc@5 84.810
### epoch[25] execution time: 29.02506923675537
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.224 ( 0.224)	Data  0.149 ( 0.149)	Loss 1.2207e+00 (1.2207e+00)	Acc@1  63.28 ( 63.28)	Acc@5  91.41 ( 91.41)
Epoch: [26][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.014)	Loss 1.1982e+00 (1.2481e+00)	Acc@1  67.19 ( 64.42)	Acc@5  90.62 ( 90.77)
Epoch: [26][ 20/391]	Time  0.066 ( 0.075)	Data  0.001 ( 0.008)	Loss 1.2812e+00 (1.2665e+00)	Acc@1  68.75 ( 63.91)	Acc@5  88.28 ( 90.36)
Epoch: [26][ 30/391]	Time  0.068 ( 0.072)	Data  0.001 ( 0.006)	Loss 1.1826e+00 (1.2476e+00)	Acc@1  66.41 ( 63.99)	Acc@5  91.41 ( 90.75)
Epoch: [26][ 40/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.005)	Loss 1.3223e+00 (1.2493e+00)	Acc@1  61.72 ( 63.41)	Acc@5  91.41 ( 90.57)
Epoch: [26][ 50/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.3223e+00 (1.2587e+00)	Acc@1  64.06 ( 62.82)	Acc@5  88.28 ( 90.43)
Epoch: [26][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.1592e+00 (1.2669e+00)	Acc@1  67.19 ( 62.56)	Acc@5  92.19 ( 90.34)
Epoch: [26][ 70/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.1777e+00 (1.2739e+00)	Acc@1  65.62 ( 62.28)	Acc@5  92.19 ( 90.24)
Epoch: [26][ 80/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.5020e+00 (1.2792e+00)	Acc@1  58.59 ( 62.41)	Acc@5  87.50 ( 90.08)
Epoch: [26][ 90/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.1045e+00 (1.2885e+00)	Acc@1  64.06 ( 62.17)	Acc@5  93.75 ( 89.89)
Epoch: [26][100/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.3701e+00 (1.2875e+00)	Acc@1  60.94 ( 62.25)	Acc@5  86.72 ( 89.78)
Epoch: [26][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6738e+00 (1.2894e+00)	Acc@1  53.12 ( 62.15)	Acc@5  80.47 ( 89.77)
Epoch: [26][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1475e+00 (1.2865e+00)	Acc@1  62.50 ( 62.21)	Acc@5  91.41 ( 89.83)
Epoch: [26][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0264e+00 (1.2940e+00)	Acc@1  73.44 ( 62.06)	Acc@5  95.31 ( 89.74)
Epoch: [26][140/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4434e+00 (1.2963e+00)	Acc@1  54.69 ( 61.90)	Acc@5  89.06 ( 89.73)
Epoch: [26][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3848e+00 (1.2981e+00)	Acc@1  60.16 ( 61.87)	Acc@5  85.94 ( 89.66)
Epoch: [26][160/391]	Time  0.068 ( 0.068)	Data  0.002 ( 0.002)	Loss 1.4258e+00 (1.3042e+00)	Acc@1  60.16 ( 61.71)	Acc@5  89.84 ( 89.59)
Epoch: [26][170/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3984e+00 (1.3045e+00)	Acc@1  60.16 ( 61.73)	Acc@5  88.28 ( 89.57)
Epoch: [26][180/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5664e+00 (1.3073e+00)	Acc@1  55.47 ( 61.67)	Acc@5  87.50 ( 89.52)
Epoch: [26][190/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2441e+00 (1.3104e+00)	Acc@1  68.75 ( 61.63)	Acc@5  87.50 ( 89.47)
Epoch: [26][200/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4238e+00 (1.3160e+00)	Acc@1  59.38 ( 61.50)	Acc@5  88.28 ( 89.40)
Epoch: [26][210/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2012e+00 (1.3161e+00)	Acc@1  65.62 ( 61.57)	Acc@5  89.06 ( 89.40)
Epoch: [26][220/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.5674e+00 (1.3191e+00)	Acc@1  59.38 ( 61.56)	Acc@5  87.50 ( 89.35)
Epoch: [26][230/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2012e+00 (1.3191e+00)	Acc@1  64.06 ( 61.51)	Acc@5  92.97 ( 89.35)
Epoch: [26][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2949e+00 (1.3196e+00)	Acc@1  61.72 ( 61.49)	Acc@5  89.06 ( 89.34)
Epoch: [26][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2178e+00 (1.3200e+00)	Acc@1  60.94 ( 61.43)	Acc@5  89.06 ( 89.29)
Epoch: [26][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2520e+00 (1.3197e+00)	Acc@1  60.16 ( 61.45)	Acc@5  91.41 ( 89.30)
Epoch: [26][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3037e+00 (1.3194e+00)	Acc@1  63.28 ( 61.46)	Acc@5  89.84 ( 89.30)
Epoch: [26][280/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2803e+00 (1.3195e+00)	Acc@1  58.59 ( 61.42)	Acc@5  91.41 ( 89.32)
Epoch: [26][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3271e+00 (1.3213e+00)	Acc@1  62.50 ( 61.41)	Acc@5  89.06 ( 89.25)
Epoch: [26][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3955e+00 (1.3228e+00)	Acc@1  64.06 ( 61.40)	Acc@5  91.41 ( 89.25)
Epoch: [26][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3994e+00 (1.3213e+00)	Acc@1  59.38 ( 61.44)	Acc@5  86.72 ( 89.26)
Epoch: [26][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2041e+00 (1.3200e+00)	Acc@1  62.50 ( 61.43)	Acc@5  89.84 ( 89.29)
Epoch: [26][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1875e+00 (1.3208e+00)	Acc@1  70.31 ( 61.40)	Acc@5  90.62 ( 89.27)
Epoch: [26][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4307e+00 (1.3224e+00)	Acc@1  56.25 ( 61.40)	Acc@5  86.72 ( 89.25)
Epoch: [26][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.6348e+00 (1.3244e+00)	Acc@1  55.47 ( 61.37)	Acc@5  83.59 ( 89.21)
Epoch: [26][360/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4365e+00 (1.3257e+00)	Acc@1  57.81 ( 61.34)	Acc@5  89.84 ( 89.19)
Epoch: [26][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4209e+00 (1.3269e+00)	Acc@1  60.94 ( 61.29)	Acc@5  87.50 ( 89.18)
Epoch: [26][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.2510e+00 (1.3301e+00)	Acc@1  61.72 ( 61.14)	Acc@5  89.06 ( 89.14)
Epoch: [26][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.3320e+00 (1.3308e+00)	Acc@1  65.00 ( 61.11)	Acc@5  87.50 ( 89.12)
## e[26] optimizer.zero_grad (sum) time: 0.4097175598144531
## e[26]       loss.backward (sum) time: 7.399144172668457
## e[26]      optimizer.step (sum) time: 3.56402325630188
## epoch[26] training(only) time: 26.191946029663086
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.6318e+00 (1.6318e+00)	Acc@1  58.00 ( 58.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.6768e+00 (1.6935e+00)	Acc@1  53.00 ( 54.36)	Acc@5  83.00 ( 83.55)
Test: [ 20/100]	Time  0.030 ( 0.033)	Loss 1.7275e+00 (1.6718e+00)	Acc@1  53.00 ( 55.00)	Acc@5  82.00 ( 83.71)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7471e+00 (1.6719e+00)	Acc@1  50.00 ( 54.39)	Acc@5  85.00 ( 83.71)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.6260e+00 (1.6557e+00)	Acc@1  51.00 ( 54.76)	Acc@5  83.00 ( 84.15)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.6641e+00 (1.6728e+00)	Acc@1  55.00 ( 54.27)	Acc@5  85.00 ( 83.94)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.6309e+00 (1.6748e+00)	Acc@1  54.00 ( 54.11)	Acc@5  82.00 ( 83.92)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.6572e+00 (1.6819e+00)	Acc@1  51.00 ( 53.87)	Acc@5  88.00 ( 83.87)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.6514e+00 (1.6919e+00)	Acc@1  57.00 ( 53.69)	Acc@5  81.00 ( 83.73)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 2.0918e+00 (1.6822e+00)	Acc@1  47.00 ( 53.86)	Acc@5  76.00 ( 83.95)
 * Acc@1 53.970 Acc@5 84.030
### epoch[26] execution time: 28.991191625595093
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.222 ( 0.222)	Data  0.146 ( 0.146)	Loss 1.3213e+00 (1.3213e+00)	Acc@1  60.16 ( 60.16)	Acc@5  92.19 ( 92.19)
Epoch: [27][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.014)	Loss 1.2891e+00 (1.2202e+00)	Acc@1  66.41 ( 62.71)	Acc@5  88.28 ( 91.12)
Epoch: [27][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.2900e+00 (1.2444e+00)	Acc@1  64.06 ( 62.72)	Acc@5  86.72 ( 90.33)
Epoch: [27][ 30/391]	Time  0.062 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.2559e+00 (1.2320e+00)	Acc@1  60.94 ( 62.85)	Acc@5  92.19 ( 90.57)
Epoch: [27][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.3604e+00 (1.2578e+00)	Acc@1  61.72 ( 62.39)	Acc@5  85.94 ( 90.03)
Epoch: [27][ 50/391]	Time  0.074 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.3760e+00 (1.2608e+00)	Acc@1  59.38 ( 62.48)	Acc@5  87.50 ( 89.89)
Epoch: [27][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.1680e+00 (1.2446e+00)	Acc@1  66.41 ( 62.85)	Acc@5  91.41 ( 90.24)
Epoch: [27][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.1416e+00 (1.2495e+00)	Acc@1  65.62 ( 62.82)	Acc@5  92.19 ( 90.20)
Epoch: [27][ 80/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.5459e+00 (1.2659e+00)	Acc@1  57.81 ( 62.43)	Acc@5  85.94 ( 89.86)
Epoch: [27][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.3730e+00 (1.2633e+00)	Acc@1  59.38 ( 62.50)	Acc@5  86.72 ( 89.90)
Epoch: [27][100/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.2871e+00 (1.2643e+00)	Acc@1  60.94 ( 62.52)	Acc@5  89.84 ( 89.77)
Epoch: [27][110/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2529e+00 (1.2664e+00)	Acc@1  63.28 ( 62.54)	Acc@5  89.06 ( 89.72)
Epoch: [27][120/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2227e+00 (1.2755e+00)	Acc@1  65.62 ( 62.38)	Acc@5  91.41 ( 89.60)
Epoch: [27][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3848e+00 (1.2831e+00)	Acc@1  60.16 ( 62.13)	Acc@5  89.84 ( 89.53)
Epoch: [27][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4434e+00 (1.2887e+00)	Acc@1  59.38 ( 61.95)	Acc@5  89.84 ( 89.51)
Epoch: [27][150/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3389e+00 (1.2876e+00)	Acc@1  61.72 ( 62.04)	Acc@5  87.50 ( 89.46)
Epoch: [27][160/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2383e+00 (1.2894e+00)	Acc@1  61.72 ( 62.02)	Acc@5  92.19 ( 89.47)
Epoch: [27][170/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2471e+00 (1.2910e+00)	Acc@1  61.72 ( 61.98)	Acc@5  92.19 ( 89.44)
Epoch: [27][180/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2627e+00 (1.2953e+00)	Acc@1  67.97 ( 61.79)	Acc@5  87.50 ( 89.39)
Epoch: [27][190/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4072e+00 (1.2946e+00)	Acc@1  57.81 ( 61.79)	Acc@5  88.28 ( 89.43)
Epoch: [27][200/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0596e+00 (1.2933e+00)	Acc@1  67.19 ( 61.80)	Acc@5  92.97 ( 89.50)
Epoch: [27][210/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1787e+00 (1.2922e+00)	Acc@1  66.41 ( 61.85)	Acc@5  89.84 ( 89.52)
Epoch: [27][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2793e+00 (1.2923e+00)	Acc@1  61.72 ( 61.82)	Acc@5  90.62 ( 89.52)
Epoch: [27][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1377e+00 (1.2928e+00)	Acc@1  64.06 ( 61.75)	Acc@5  92.19 ( 89.54)
Epoch: [27][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2607e+00 (1.2946e+00)	Acc@1  63.28 ( 61.73)	Acc@5  90.62 ( 89.53)
Epoch: [27][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1943e+00 (1.2971e+00)	Acc@1  60.94 ( 61.70)	Acc@5  92.97 ( 89.51)
Epoch: [27][260/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2666e+00 (1.2958e+00)	Acc@1  67.19 ( 61.81)	Acc@5  89.84 ( 89.56)
Epoch: [27][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2363e+00 (1.2972e+00)	Acc@1  66.41 ( 61.78)	Acc@5  87.50 ( 89.53)
Epoch: [27][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2910e+00 (1.2990e+00)	Acc@1  61.72 ( 61.74)	Acc@5  89.06 ( 89.51)
Epoch: [27][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.7852e-01 (1.2993e+00)	Acc@1  71.88 ( 61.69)	Acc@5  92.19 ( 89.50)
Epoch: [27][300/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3721e+00 (1.3023e+00)	Acc@1  60.16 ( 61.61)	Acc@5  92.19 ( 89.46)
Epoch: [27][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2715e+00 (1.3039e+00)	Acc@1  64.84 ( 61.58)	Acc@5  89.06 ( 89.44)
Epoch: [27][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3330e+00 (1.3071e+00)	Acc@1  65.62 ( 61.53)	Acc@5  89.06 ( 89.38)
Epoch: [27][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3682e+00 (1.3096e+00)	Acc@1  58.59 ( 61.44)	Acc@5  86.72 ( 89.32)
Epoch: [27][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3018e+00 (1.3104e+00)	Acc@1  64.06 ( 61.42)	Acc@5  89.84 ( 89.30)
Epoch: [27][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4639e+00 (1.3119e+00)	Acc@1  62.50 ( 61.39)	Acc@5  86.72 ( 89.30)
Epoch: [27][360/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4629e+00 (1.3131e+00)	Acc@1  60.94 ( 61.36)	Acc@5  81.25 ( 89.28)
Epoch: [27][370/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5498e+00 (1.3161e+00)	Acc@1  53.12 ( 61.27)	Acc@5  90.62 ( 89.25)
Epoch: [27][380/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.2998e+00 (1.3185e+00)	Acc@1  65.62 ( 61.24)	Acc@5  89.84 ( 89.22)
Epoch: [27][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.1406e+00 (1.3210e+00)	Acc@1  68.75 ( 61.17)	Acc@5  93.75 ( 89.17)
## e[27] optimizer.zero_grad (sum) time: 0.41512179374694824
## e[27]       loss.backward (sum) time: 7.3763062953948975
## e[27]      optimizer.step (sum) time: 3.5836904048919678
## epoch[27] training(only) time: 26.237905502319336
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 1.6572e+00 (1.6572e+00)	Acc@1  53.00 ( 53.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.5967e+00 (1.6406e+00)	Acc@1  62.00 ( 55.91)	Acc@5  84.00 ( 84.36)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.7188e+00 (1.6458e+00)	Acc@1  54.00 ( 55.33)	Acc@5  82.00 ( 84.24)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.8320e+00 (1.6457e+00)	Acc@1  52.00 ( 54.94)	Acc@5  83.00 ( 84.68)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.7646e+00 (1.6308e+00)	Acc@1  51.00 ( 55.20)	Acc@5  84.00 ( 84.85)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.6758e+00 (1.6409e+00)	Acc@1  50.00 ( 54.63)	Acc@5  82.00 ( 84.57)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.6289e+00 (1.6485e+00)	Acc@1  47.00 ( 54.23)	Acc@5  82.00 ( 84.30)
Test: [ 70/100]	Time  0.029 ( 0.028)	Loss 1.6494e+00 (1.6651e+00)	Acc@1  52.00 ( 53.87)	Acc@5  83.00 ( 84.03)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.6895e+00 (1.6769e+00)	Acc@1  56.00 ( 53.58)	Acc@5  87.00 ( 83.95)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0234e+00 (1.6706e+00)	Acc@1  51.00 ( 53.87)	Acc@5  79.00 ( 84.01)
 * Acc@1 54.010 Acc@5 84.090
### epoch[27] execution time: 29.070485591888428
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.224 ( 0.224)	Data  0.148 ( 0.148)	Loss 1.1201e+00 (1.1201e+00)	Acc@1  64.06 ( 64.06)	Acc@5  92.97 ( 92.97)
Epoch: [28][ 10/391]	Time  0.069 ( 0.081)	Data  0.001 ( 0.014)	Loss 1.4854e+00 (1.2692e+00)	Acc@1  54.69 ( 62.00)	Acc@5  86.72 ( 90.77)
Epoch: [28][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.0186e+00 (1.2222e+00)	Acc@1  73.44 ( 63.10)	Acc@5  93.75 ( 91.59)
Epoch: [28][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.0479e+00 (1.2205e+00)	Acc@1  69.53 ( 62.90)	Acc@5  92.97 ( 91.46)
Epoch: [28][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.3428e+00 (1.2499e+00)	Acc@1  60.16 ( 62.46)	Acc@5  86.72 ( 90.74)
Epoch: [28][ 50/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.4961e+00 (1.2552e+00)	Acc@1  60.94 ( 62.64)	Acc@5  83.59 ( 90.61)
Epoch: [28][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.2070e+00 (1.2565e+00)	Acc@1  65.62 ( 62.41)	Acc@5  94.53 ( 90.75)
Epoch: [28][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.2734e+00 (1.2563e+00)	Acc@1  60.16 ( 62.37)	Acc@5  89.84 ( 90.66)
Epoch: [28][ 80/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.1865e+00 (1.2619e+00)	Acc@1  63.28 ( 62.37)	Acc@5  91.41 ( 90.56)
Epoch: [28][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.1367e+00 (1.2613e+00)	Acc@1  63.28 ( 62.35)	Acc@5  93.75 ( 90.60)
Epoch: [28][100/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.2891e+00 (1.2613e+00)	Acc@1  63.28 ( 62.46)	Acc@5  92.97 ( 90.58)
Epoch: [28][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1406e+00 (1.2641e+00)	Acc@1  65.62 ( 62.33)	Acc@5  89.06 ( 90.52)
Epoch: [28][120/391]	Time  0.077 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1777e+00 (1.2681e+00)	Acc@1  62.50 ( 62.28)	Acc@5  93.75 ( 90.44)
Epoch: [28][130/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2402e+00 (1.2691e+00)	Acc@1  62.50 ( 62.20)	Acc@5  92.19 ( 90.43)
Epoch: [28][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3262e+00 (1.2726e+00)	Acc@1  62.50 ( 62.21)	Acc@5  87.50 ( 90.37)
Epoch: [28][150/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3203e+00 (1.2746e+00)	Acc@1  61.72 ( 62.16)	Acc@5  90.62 ( 90.31)
Epoch: [28][160/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2217e+00 (1.2801e+00)	Acc@1  70.31 ( 62.04)	Acc@5  89.06 ( 90.20)
Epoch: [28][170/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2422e+00 (1.2846e+00)	Acc@1  62.50 ( 62.02)	Acc@5  87.50 ( 90.04)
Epoch: [28][180/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4004e+00 (1.2841e+00)	Acc@1  57.81 ( 62.05)	Acc@5  84.38 ( 90.04)
Epoch: [28][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.8779e-01 (1.2875e+00)	Acc@1  67.19 ( 61.99)	Acc@5  94.53 ( 89.97)
Epoch: [28][200/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.6943e+00 (1.2891e+00)	Acc@1  50.00 ( 62.00)	Acc@5  82.03 ( 89.91)
Epoch: [28][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5303e+00 (1.2914e+00)	Acc@1  59.38 ( 61.97)	Acc@5  85.94 ( 89.85)
Epoch: [28][220/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3789e+00 (1.2921e+00)	Acc@1  59.38 ( 61.96)	Acc@5  92.97 ( 89.83)
Epoch: [28][230/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5781e+00 (1.2943e+00)	Acc@1  50.00 ( 61.93)	Acc@5  90.62 ( 89.82)
Epoch: [28][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2227e+00 (1.2967e+00)	Acc@1  63.28 ( 61.85)	Acc@5  92.19 ( 89.79)
Epoch: [28][250/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2129e+00 (1.2965e+00)	Acc@1  62.50 ( 61.89)	Acc@5  91.41 ( 89.78)
Epoch: [28][260/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4482e+00 (1.2948e+00)	Acc@1  57.81 ( 61.94)	Acc@5  87.50 ( 89.77)
Epoch: [28][270/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2520e+00 (1.2944e+00)	Acc@1  63.28 ( 61.94)	Acc@5  87.50 ( 89.77)
Epoch: [28][280/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (1.2919e+00)	Acc@1  69.53 ( 62.05)	Acc@5  92.19 ( 89.79)
Epoch: [28][290/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.4121e+00 (1.2935e+00)	Acc@1  57.03 ( 61.97)	Acc@5  91.41 ( 89.76)
Epoch: [28][300/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1816e+00 (1.2966e+00)	Acc@1  65.62 ( 61.87)	Acc@5  90.62 ( 89.73)
Epoch: [28][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3877e+00 (1.2974e+00)	Acc@1  60.94 ( 61.89)	Acc@5  88.28 ( 89.72)
Epoch: [28][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3828e+00 (1.2997e+00)	Acc@1  58.59 ( 61.81)	Acc@5  85.94 ( 89.67)
Epoch: [28][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3311e+00 (1.2998e+00)	Acc@1  58.59 ( 61.84)	Acc@5  88.28 ( 89.66)
Epoch: [28][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2070e+00 (1.2976e+00)	Acc@1  64.06 ( 61.92)	Acc@5  89.06 ( 89.66)
Epoch: [28][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4893e+00 (1.3000e+00)	Acc@1  57.81 ( 61.88)	Acc@5  83.59 ( 89.61)
Epoch: [28][360/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3379e+00 (1.3007e+00)	Acc@1  59.38 ( 61.84)	Acc@5  85.16 ( 89.55)
Epoch: [28][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.4346e+00 (1.3026e+00)	Acc@1  58.59 ( 61.80)	Acc@5  85.94 ( 89.52)
Epoch: [28][380/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.2578e+00 (1.3030e+00)	Acc@1  63.28 ( 61.79)	Acc@5  91.41 ( 89.51)
Epoch: [28][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.6006e+00 (1.3037e+00)	Acc@1  60.00 ( 61.79)	Acc@5  83.75 ( 89.49)
## e[28] optimizer.zero_grad (sum) time: 0.41840600967407227
## e[28]       loss.backward (sum) time: 7.372776985168457
## e[28]      optimizer.step (sum) time: 3.648740291595459
## epoch[28] training(only) time: 26.375098943710327
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.6904e+00 (1.6904e+00)	Acc@1  54.00 ( 54.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.6426e+00 (1.6074e+00)	Acc@1  53.00 ( 56.91)	Acc@5  88.00 ( 83.82)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.6582e+00 (1.6128e+00)	Acc@1  56.00 ( 56.05)	Acc@5  84.00 ( 83.90)
Test: [ 30/100]	Time  0.035 ( 0.031)	Loss 1.5547e+00 (1.6010e+00)	Acc@1  62.00 ( 56.19)	Acc@5  87.00 ( 84.32)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.6514e+00 (1.5894e+00)	Acc@1  55.00 ( 56.27)	Acc@5  86.00 ( 84.88)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.6299e+00 (1.5886e+00)	Acc@1  57.00 ( 56.22)	Acc@5  80.00 ( 84.76)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.5654e+00 (1.5733e+00)	Acc@1  53.00 ( 56.49)	Acc@5  84.00 ( 85.07)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.6123e+00 (1.5830e+00)	Acc@1  52.00 ( 56.30)	Acc@5  84.00 ( 84.87)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.5762e+00 (1.5935e+00)	Acc@1  58.00 ( 56.10)	Acc@5  82.00 ( 84.72)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 2.2031e+00 (1.5889e+00)	Acc@1  46.00 ( 56.41)	Acc@5  77.00 ( 84.86)
 * Acc@1 56.370 Acc@5 84.850
### epoch[28] execution time: 29.200583457946777
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.221 ( 0.221)	Data  0.145 ( 0.145)	Loss 1.2520e+00 (1.2520e+00)	Acc@1  62.50 ( 62.50)	Acc@5  93.75 ( 93.75)
Epoch: [29][ 10/391]	Time  0.067 ( 0.080)	Data  0.001 ( 0.014)	Loss 1.0459e+00 (1.1852e+00)	Acc@1  69.53 ( 64.63)	Acc@5  94.53 ( 92.47)
Epoch: [29][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.1670e+00 (1.1961e+00)	Acc@1  65.62 ( 64.77)	Acc@5  92.19 ( 91.33)
Epoch: [29][ 30/391]	Time  0.078 ( 0.072)	Data  0.001 ( 0.006)	Loss 1.1680e+00 (1.2091e+00)	Acc@1  68.75 ( 64.77)	Acc@5  92.97 ( 90.95)
Epoch: [29][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.5322e+00 (1.2203e+00)	Acc@1  54.69 ( 64.88)	Acc@5  84.38 ( 90.80)
Epoch: [29][ 50/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.5166e+00 (1.2422e+00)	Acc@1  53.91 ( 63.79)	Acc@5  89.84 ( 90.69)
Epoch: [29][ 60/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.2119e+00 (1.2450e+00)	Acc@1  68.75 ( 63.77)	Acc@5  91.41 ( 90.66)
Epoch: [29][ 70/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.1973e+00 (1.2457e+00)	Acc@1  62.50 ( 63.64)	Acc@5  92.19 ( 90.67)
Epoch: [29][ 80/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.0850e+00 (1.2427e+00)	Acc@1  65.62 ( 63.67)	Acc@5  91.41 ( 90.64)
Epoch: [29][ 90/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.3262e+00 (1.2351e+00)	Acc@1  60.94 ( 63.94)	Acc@5  86.72 ( 90.77)
Epoch: [29][100/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.0645e+00 (1.2352e+00)	Acc@1  70.31 ( 63.91)	Acc@5  92.19 ( 90.62)
Epoch: [29][110/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2715e+00 (1.2447e+00)	Acc@1  61.72 ( 63.64)	Acc@5  89.06 ( 90.51)
Epoch: [29][120/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2578e+00 (1.2409e+00)	Acc@1  60.16 ( 63.77)	Acc@5  89.84 ( 90.54)
Epoch: [29][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3105e+00 (1.2406e+00)	Acc@1  63.28 ( 63.78)	Acc@5  90.62 ( 90.51)
Epoch: [29][140/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (1.2393e+00)	Acc@1  70.31 ( 63.83)	Acc@5  92.97 ( 90.50)
Epoch: [29][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2422e+00 (1.2492e+00)	Acc@1  64.84 ( 63.45)	Acc@5  89.06 ( 90.38)
Epoch: [29][160/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3896e+00 (1.2493e+00)	Acc@1  57.81 ( 63.42)	Acc@5  85.94 ( 90.36)
Epoch: [29][170/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1963e+00 (1.2458e+00)	Acc@1  65.62 ( 63.50)	Acc@5  89.06 ( 90.39)
Epoch: [29][180/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1113e+00 (1.2479e+00)	Acc@1  67.19 ( 63.44)	Acc@5  93.75 ( 90.38)
Epoch: [29][190/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.3574e+00 (1.2535e+00)	Acc@1  61.72 ( 63.32)	Acc@5  89.06 ( 90.28)
Epoch: [29][200/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2607e+00 (1.2550e+00)	Acc@1  64.84 ( 63.26)	Acc@5  92.97 ( 90.26)
Epoch: [29][210/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1465e+00 (1.2586e+00)	Acc@1  66.41 ( 63.16)	Acc@5  89.84 ( 90.18)
Epoch: [29][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3516e+00 (1.2586e+00)	Acc@1  60.94 ( 63.12)	Acc@5  88.28 ( 90.19)
Epoch: [29][230/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2520e+00 (1.2601e+00)	Acc@1  63.28 ( 63.07)	Acc@5  88.28 ( 90.13)
Epoch: [29][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2939e+00 (1.2572e+00)	Acc@1  64.06 ( 63.14)	Acc@5  91.41 ( 90.18)
Epoch: [29][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3301e+00 (1.2611e+00)	Acc@1  59.38 ( 63.04)	Acc@5  89.06 ( 90.14)
Epoch: [29][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3164e+00 (1.2630e+00)	Acc@1  66.41 ( 63.01)	Acc@5  87.50 ( 90.13)
Epoch: [29][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4355e+00 (1.2630e+00)	Acc@1  57.03 ( 63.00)	Acc@5  86.72 ( 90.14)
Epoch: [29][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2793e+00 (1.2649e+00)	Acc@1  62.50 ( 62.95)	Acc@5  88.28 ( 90.11)
Epoch: [29][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4277e+00 (1.2686e+00)	Acc@1  58.59 ( 62.91)	Acc@5  83.59 ( 89.99)
Epoch: [29][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1211e+00 (1.2686e+00)	Acc@1  64.06 ( 62.92)	Acc@5  92.19 ( 90.00)
Epoch: [29][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2578e+00 (1.2712e+00)	Acc@1  57.03 ( 62.88)	Acc@5  90.62 ( 89.92)
Epoch: [29][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1016e+00 (1.2725e+00)	Acc@1  65.62 ( 62.79)	Acc@5  92.97 ( 89.89)
Epoch: [29][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4229e+00 (1.2749e+00)	Acc@1  57.03 ( 62.66)	Acc@5  89.84 ( 89.86)
Epoch: [29][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1963e+00 (1.2767e+00)	Acc@1  69.53 ( 62.63)	Acc@5  92.19 ( 89.84)
Epoch: [29][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.2344e+00 (1.2790e+00)	Acc@1  64.06 ( 62.58)	Acc@5  91.41 ( 89.82)
Epoch: [29][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.0215e+00 (1.2788e+00)	Acc@1  68.75 ( 62.57)	Acc@5  94.53 ( 89.84)
Epoch: [29][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.3730e+00 (1.2791e+00)	Acc@1  60.16 ( 62.55)	Acc@5  88.28 ( 89.82)
Epoch: [29][380/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.3906e+00 (1.2816e+00)	Acc@1  61.72 ( 62.50)	Acc@5  85.94 ( 89.77)
Epoch: [29][390/391]	Time  0.052 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.5410e+00 (1.2829e+00)	Acc@1  57.50 ( 62.49)	Acc@5  85.00 ( 89.76)
## e[29] optimizer.zero_grad (sum) time: 0.40719151496887207
## e[29]       loss.backward (sum) time: 7.36113977432251
## e[29]      optimizer.step (sum) time: 3.5913219451904297
## epoch[29] training(only) time: 26.246037006378174
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.4570e+00 (1.4570e+00)	Acc@1  57.00 ( 57.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.5527e+00 (1.5178e+00)	Acc@1  55.00 ( 58.00)	Acc@5  88.00 ( 84.36)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.4980e+00 (1.4924e+00)	Acc@1  64.00 ( 58.24)	Acc@5  87.00 ( 85.14)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6660e+00 (1.5192e+00)	Acc@1  52.00 ( 57.52)	Acc@5  84.00 ( 85.13)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.6094e+00 (1.5175e+00)	Acc@1  53.00 ( 57.46)	Acc@5  86.00 ( 85.46)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.5049e+00 (1.5464e+00)	Acc@1  59.00 ( 56.90)	Acc@5  84.00 ( 84.84)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.6201e+00 (1.5497e+00)	Acc@1  54.00 ( 57.05)	Acc@5  82.00 ( 84.70)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.5693e+00 (1.5604e+00)	Acc@1  57.00 ( 56.92)	Acc@5  89.00 ( 84.82)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.6729e+00 (1.5726e+00)	Acc@1  57.00 ( 56.64)	Acc@5  85.00 ( 84.70)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8799e+00 (1.5650e+00)	Acc@1  53.00 ( 56.84)	Acc@5  78.00 ( 84.95)
 * Acc@1 56.810 Acc@5 84.950
### epoch[29] execution time: 29.088593006134033
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.228 ( 0.228)	Data  0.143 ( 0.143)	Loss 1.1309e+00 (1.1309e+00)	Acc@1  64.06 ( 64.06)	Acc@5  93.75 ( 93.75)
Epoch: [30][ 10/391]	Time  0.067 ( 0.082)	Data  0.001 ( 0.014)	Loss 1.3887e+00 (1.2449e+00)	Acc@1  60.94 ( 64.06)	Acc@5  89.06 ( 90.62)
Epoch: [30][ 20/391]	Time  0.072 ( 0.075)	Data  0.001 ( 0.008)	Loss 1.4092e+00 (1.2335e+00)	Acc@1  57.81 ( 63.80)	Acc@5  87.50 ( 90.44)
Epoch: [30][ 30/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.006)	Loss 1.0762e+00 (1.1868e+00)	Acc@1  68.75 ( 65.30)	Acc@5  92.19 ( 90.85)
Epoch: [30][ 40/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.005)	Loss 1.2266e+00 (1.1779e+00)	Acc@1  71.88 ( 65.72)	Acc@5  90.62 ( 91.20)
Epoch: [30][ 50/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.004)	Loss 9.9902e-01 (1.1526e+00)	Acc@1  72.66 ( 66.62)	Acc@5  92.19 ( 91.42)
Epoch: [30][ 60/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 9.3262e-01 (1.1336e+00)	Acc@1  76.56 ( 67.28)	Acc@5  93.75 ( 91.68)
Epoch: [30][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.1123e+00 (1.1188e+00)	Acc@1  64.84 ( 67.85)	Acc@5  94.53 ( 91.92)
Epoch: [30][ 80/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.0020e+00 (1.1126e+00)	Acc@1  71.88 ( 68.07)	Acc@5  93.75 ( 92.01)
Epoch: [30][ 90/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 9.7168e-01 (1.1051e+00)	Acc@1  75.00 ( 68.30)	Acc@5  92.19 ( 92.02)
Epoch: [30][100/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 9.2188e-01 (1.0913e+00)	Acc@1  71.09 ( 68.63)	Acc@5  92.19 ( 92.16)
Epoch: [30][110/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.2627e+00 (1.0840e+00)	Acc@1  63.28 ( 68.83)	Acc@5  91.41 ( 92.29)
Epoch: [30][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.6094e-01 (1.0815e+00)	Acc@1  73.44 ( 68.92)	Acc@5  93.75 ( 92.33)
Epoch: [30][130/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0117e+00 (1.0768e+00)	Acc@1  69.53 ( 68.99)	Acc@5  95.31 ( 92.47)
Epoch: [30][140/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1729e+00 (1.0706e+00)	Acc@1  61.72 ( 69.07)	Acc@5  92.19 ( 92.60)
Epoch: [30][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.0039e-01 (1.0665e+00)	Acc@1  73.44 ( 69.20)	Acc@5  94.53 ( 92.64)
Epoch: [30][160/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.3555e-01 (1.0623e+00)	Acc@1  71.88 ( 69.27)	Acc@5  95.31 ( 92.67)
Epoch: [30][170/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1387e+00 (1.0606e+00)	Acc@1  69.53 ( 69.32)	Acc@5  92.19 ( 92.73)
Epoch: [30][180/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.7158e-01 (1.0576e+00)	Acc@1  77.34 ( 69.43)	Acc@5  95.31 ( 92.77)
Epoch: [30][190/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.8770e-01 (1.0534e+00)	Acc@1  75.78 ( 69.47)	Acc@5  94.53 ( 92.83)
Epoch: [30][200/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.6973e-01 (1.0488e+00)	Acc@1  71.09 ( 69.55)	Acc@5  93.75 ( 92.88)
Epoch: [30][210/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1699e+00 (1.0456e+00)	Acc@1  62.50 ( 69.65)	Acc@5  93.75 ( 92.92)
Epoch: [30][220/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0000e+00 (1.0444e+00)	Acc@1  70.31 ( 69.65)	Acc@5  92.19 ( 92.90)
Epoch: [30][230/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.8467e-01 (1.0414e+00)	Acc@1  78.91 ( 69.74)	Acc@5  95.31 ( 92.90)
Epoch: [30][240/391]	Time  0.076 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.9561e-01 (1.0378e+00)	Acc@1  72.66 ( 69.85)	Acc@5  91.41 ( 92.94)
Epoch: [30][250/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0635e+00 (1.0354e+00)	Acc@1  69.53 ( 69.91)	Acc@5  92.97 ( 92.97)
Epoch: [30][260/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.8584e-01 (1.0325e+00)	Acc@1  73.44 ( 70.02)	Acc@5  95.31 ( 93.00)
Epoch: [30][270/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0430e+00 (1.0295e+00)	Acc@1  67.19 ( 70.10)	Acc@5  91.41 ( 93.03)
Epoch: [30][280/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1758e+00 (1.0266e+00)	Acc@1  66.41 ( 70.13)	Acc@5  91.41 ( 93.10)
Epoch: [30][290/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.8770e-01 (1.0236e+00)	Acc@1  78.91 ( 70.24)	Acc@5  94.53 ( 93.14)
Epoch: [30][300/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.1621e+00 (1.0213e+00)	Acc@1  61.72 ( 70.25)	Acc@5  93.75 ( 93.17)
Epoch: [30][310/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0078e+00 (1.0216e+00)	Acc@1  68.75 ( 70.29)	Acc@5  96.09 ( 93.16)
Epoch: [30][320/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.7305e-01 (1.0179e+00)	Acc@1  75.78 ( 70.38)	Acc@5  92.97 ( 93.21)
Epoch: [30][330/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.4473e-01 (1.0148e+00)	Acc@1  72.66 ( 70.45)	Acc@5  96.88 ( 93.26)
Epoch: [30][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1934e-01 (1.0124e+00)	Acc@1  75.78 ( 70.48)	Acc@5  96.09 ( 93.32)
Epoch: [30][350/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.0381e-01 (1.0095e+00)	Acc@1  70.31 ( 70.55)	Acc@5  95.31 ( 93.36)
Epoch: [30][360/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1152e+00 (1.0086e+00)	Acc@1  67.19 ( 70.57)	Acc@5  90.62 ( 93.36)
Epoch: [30][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 9.1406e-01 (1.0077e+00)	Acc@1  74.22 ( 70.59)	Acc@5  94.53 ( 93.34)
Epoch: [30][380/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 9.0234e-01 (1.0054e+00)	Acc@1  72.66 ( 70.68)	Acc@5  93.75 ( 93.35)
Epoch: [30][390/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.3545e-01 (1.0040e+00)	Acc@1  75.00 ( 70.70)	Acc@5  93.75 ( 93.36)
## e[30] optimizer.zero_grad (sum) time: 0.4154512882232666
## e[30]       loss.backward (sum) time: 7.42330265045166
## e[30]      optimizer.step (sum) time: 3.6460039615631104
## epoch[30] training(only) time: 26.409008026123047
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2500e+00 (1.2500e+00)	Acc@1  63.00 ( 63.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.2969e+00 (1.2141e+00)	Acc@1  62.00 ( 65.36)	Acc@5  91.00 ( 89.36)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.2637e+00 (1.1791e+00)	Acc@1  65.00 ( 66.24)	Acc@5  88.00 ( 89.71)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.4287e+00 (1.2007e+00)	Acc@1  60.00 ( 65.48)	Acc@5  86.00 ( 89.35)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.3008e+00 (1.1960e+00)	Acc@1  62.00 ( 65.15)	Acc@5  91.00 ( 89.78)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1885e+00 (1.2064e+00)	Acc@1  66.00 ( 64.88)	Acc@5  90.00 ( 89.57)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.2051e+00 (1.2007e+00)	Acc@1  67.00 ( 65.02)	Acc@5  89.00 ( 89.79)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1895e+00 (1.2102e+00)	Acc@1  65.00 ( 65.11)	Acc@5  91.00 ( 89.75)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3652e+00 (1.2208e+00)	Acc@1  63.00 ( 64.93)	Acc@5  88.00 ( 89.64)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.7285e+00 (1.2145e+00)	Acc@1  55.00 ( 64.99)	Acc@5  84.00 ( 89.76)
 * Acc@1 65.150 Acc@5 89.800
### epoch[30] execution time: 29.224746227264404
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.215 ( 0.215)	Data  0.135 ( 0.135)	Loss 9.1211e-01 (9.1211e-01)	Acc@1  74.22 ( 74.22)	Acc@5  95.31 ( 95.31)
Epoch: [31][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.013)	Loss 8.6426e-01 (8.8592e-01)	Acc@1  75.78 ( 73.79)	Acc@5  95.31 ( 95.17)
Epoch: [31][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.007)	Loss 7.4072e-01 (8.8042e-01)	Acc@1  80.47 ( 74.44)	Acc@5  97.66 ( 95.28)
Epoch: [31][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.005)	Loss 9.4629e-01 (8.8924e-01)	Acc@1  72.66 ( 73.99)	Acc@5  95.31 ( 94.98)
Epoch: [31][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 1.1904e+00 (8.9857e-01)	Acc@1  71.09 ( 73.72)	Acc@5  90.62 ( 94.86)
Epoch: [31][ 50/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.1787e+00 (9.0013e-01)	Acc@1  66.41 ( 73.87)	Acc@5  89.84 ( 94.67)
Epoch: [31][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.8760e-01 (8.9940e-01)	Acc@1  74.22 ( 73.78)	Acc@5  96.09 ( 94.63)
Epoch: [31][ 70/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 8.9307e-01 (9.0340e-01)	Acc@1  71.88 ( 73.58)	Acc@5  95.31 ( 94.67)
Epoch: [31][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.9990e-01 (9.0218e-01)	Acc@1  71.88 ( 73.53)	Acc@5  95.31 ( 94.66)
Epoch: [31][ 90/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 9.7998e-01 (9.0154e-01)	Acc@1  68.75 ( 73.49)	Acc@5  95.31 ( 94.66)
Epoch: [31][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.2705e-01 (9.0012e-01)	Acc@1  80.47 ( 73.46)	Acc@5  95.31 ( 94.60)
Epoch: [31][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.2139e-01 (8.9982e-01)	Acc@1  73.44 ( 73.42)	Acc@5  94.53 ( 94.64)
Epoch: [31][120/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5059e-01 (9.0318e-01)	Acc@1  76.56 ( 73.34)	Acc@5  91.41 ( 94.54)
Epoch: [31][130/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.0127e-01 (9.0196e-01)	Acc@1  77.34 ( 73.46)	Acc@5  96.88 ( 94.50)
Epoch: [31][140/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0469e+00 (9.0439e-01)	Acc@1  68.75 ( 73.36)	Acc@5  92.19 ( 94.44)
Epoch: [31][150/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2998e-01 (9.0415e-01)	Acc@1  79.69 ( 73.42)	Acc@5  97.66 ( 94.44)
Epoch: [31][160/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.7021e-01 (9.0621e-01)	Acc@1  71.09 ( 73.38)	Acc@5  92.19 ( 94.42)
Epoch: [31][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.9219e-01 (9.0873e-01)	Acc@1  71.88 ( 73.37)	Acc@5  94.53 ( 94.38)
Epoch: [31][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1396e-01 (9.1162e-01)	Acc@1  73.44 ( 73.30)	Acc@5  97.66 ( 94.33)
Epoch: [31][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0703e+00 (9.1400e-01)	Acc@1  64.84 ( 73.19)	Acc@5  90.62 ( 94.32)
Epoch: [31][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.2422e-01 (9.1384e-01)	Acc@1  76.56 ( 73.22)	Acc@5  94.53 ( 94.28)
Epoch: [31][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.0566e-01 (9.1215e-01)	Acc@1  78.12 ( 73.27)	Acc@5  92.97 ( 94.32)
Epoch: [31][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.8438e-01 (9.1056e-01)	Acc@1  69.53 ( 73.33)	Acc@5  89.84 ( 94.33)
Epoch: [31][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0615e+00 (9.1192e-01)	Acc@1  63.28 ( 73.27)	Acc@5  92.19 ( 94.29)
Epoch: [31][240/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1494e-01 (9.0952e-01)	Acc@1  72.66 ( 73.27)	Acc@5  96.09 ( 94.32)
Epoch: [31][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.6328e-01 (9.0833e-01)	Acc@1  72.66 ( 73.33)	Acc@5  96.09 ( 94.31)
Epoch: [31][260/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.8574e-01 (9.0839e-01)	Acc@1  73.44 ( 73.35)	Acc@5  89.84 ( 94.30)
Epoch: [31][270/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.9990e-01 (9.0953e-01)	Acc@1  68.75 ( 73.32)	Acc@5  96.09 ( 94.31)
Epoch: [31][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.9453e-01 (9.0794e-01)	Acc@1  78.12 ( 73.39)	Acc@5  94.53 ( 94.36)
Epoch: [31][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.2324e-01 (9.0801e-01)	Acc@1  75.00 ( 73.39)	Acc@5  96.09 ( 94.33)
Epoch: [31][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0732e+00 (9.0816e-01)	Acc@1  66.41 ( 73.37)	Acc@5  92.19 ( 94.35)
Epoch: [31][310/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.3457e-01 (9.0689e-01)	Acc@1  71.88 ( 73.39)	Acc@5  96.09 ( 94.39)
Epoch: [31][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1494e-01 (9.0845e-01)	Acc@1  75.00 ( 73.37)	Acc@5  92.97 ( 94.37)
Epoch: [31][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.6133e-01 (9.0882e-01)	Acc@1  75.00 ( 73.35)	Acc@5  95.31 ( 94.36)
Epoch: [31][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 9.2529e-01 (9.0875e-01)	Acc@1  70.31 ( 73.34)	Acc@5  95.31 ( 94.35)
Epoch: [31][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.0508e+00 (9.0855e-01)	Acc@1  71.09 ( 73.36)	Acc@5  89.84 ( 94.33)
Epoch: [31][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 9.1699e-01 (9.0849e-01)	Acc@1  70.31 ( 73.36)	Acc@5  94.53 ( 94.32)
Epoch: [31][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 9.1504e-01 (9.0953e-01)	Acc@1  78.12 ( 73.33)	Acc@5  93.75 ( 94.31)
Epoch: [31][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.7500e-01 (9.0886e-01)	Acc@1  78.12 ( 73.34)	Acc@5  94.53 ( 94.32)
Epoch: [31][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.0078e+00 (9.0802e-01)	Acc@1  70.00 ( 73.34)	Acc@5  92.50 ( 94.34)
## e[31] optimizer.zero_grad (sum) time: 0.408862829208374
## e[31]       loss.backward (sum) time: 7.3328845500946045
## e[31]      optimizer.step (sum) time: 3.685328245162964
## epoch[31] training(only) time: 26.289645433425903
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.1953e+00 (1.1953e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.2607e+00 (1.1967e+00)	Acc@1  64.00 ( 67.27)	Acc@5  92.00 ( 89.82)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.2637e+00 (1.1718e+00)	Acc@1  64.00 ( 67.00)	Acc@5  89.00 ( 90.05)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.4199e+00 (1.1905e+00)	Acc@1  60.00 ( 66.29)	Acc@5  86.00 ( 89.77)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.2607e+00 (1.1821e+00)	Acc@1  65.00 ( 66.05)	Acc@5  94.00 ( 90.29)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1562e+00 (1.1894e+00)	Acc@1  68.00 ( 65.75)	Acc@5  88.00 ( 90.27)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.1953e+00 (1.1802e+00)	Acc@1  70.00 ( 66.08)	Acc@5  89.00 ( 90.39)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1240e+00 (1.1879e+00)	Acc@1  65.00 ( 65.90)	Acc@5  93.00 ( 90.35)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3350e+00 (1.1970e+00)	Acc@1  62.00 ( 65.63)	Acc@5  89.00 ( 90.20)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 1.6982e+00 (1.1900e+00)	Acc@1  54.00 ( 65.67)	Acc@5  87.00 ( 90.33)
 * Acc@1 65.810 Acc@5 90.320
### epoch[31] execution time: 29.10075545310974
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.226 ( 0.226)	Data  0.151 ( 0.151)	Loss 9.3164e-01 (9.3164e-01)	Acc@1  69.53 ( 69.53)	Acc@5  94.53 ( 94.53)
Epoch: [32][ 10/391]	Time  0.073 ( 0.082)	Data  0.001 ( 0.015)	Loss 8.2959e-01 (8.9169e-01)	Acc@1  80.47 ( 74.57)	Acc@5  96.09 ( 95.03)
Epoch: [32][ 20/391]	Time  0.067 ( 0.075)	Data  0.001 ( 0.008)	Loss 8.7354e-01 (8.8084e-01)	Acc@1  71.88 ( 74.96)	Acc@5  95.31 ( 94.87)
Epoch: [32][ 30/391]	Time  0.067 ( 0.072)	Data  0.001 ( 0.006)	Loss 9.2627e-01 (8.7966e-01)	Acc@1  75.00 ( 74.60)	Acc@5  93.75 ( 94.83)
Epoch: [32][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.005)	Loss 7.5635e-01 (8.5776e-01)	Acc@1  78.91 ( 75.06)	Acc@5  96.88 ( 95.01)
Epoch: [32][ 50/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.004)	Loss 8.3057e-01 (8.6430e-01)	Acc@1  74.22 ( 74.89)	Acc@5  97.66 ( 95.04)
Epoch: [32][ 60/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 9.3604e-01 (8.6748e-01)	Acc@1  69.53 ( 74.64)	Acc@5  95.31 ( 94.92)
Epoch: [32][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.0020e-01 (8.6563e-01)	Acc@1  80.47 ( 74.70)	Acc@5  97.66 ( 94.97)
Epoch: [32][ 80/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 8.5986e-01 (8.7239e-01)	Acc@1  75.00 ( 74.56)	Acc@5  93.75 ( 94.88)
Epoch: [32][ 90/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 8.4033e-01 (8.7633e-01)	Acc@1  80.47 ( 74.48)	Acc@5  96.09 ( 94.85)
Epoch: [32][100/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.9209e-01 (8.7495e-01)	Acc@1  76.56 ( 74.48)	Acc@5  95.31 ( 94.86)
Epoch: [32][110/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.9512e-01 (8.7649e-01)	Acc@1  71.09 ( 74.39)	Acc@5  93.75 ( 94.84)
Epoch: [32][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.7490e-01 (8.7665e-01)	Acc@1  74.22 ( 74.36)	Acc@5  96.09 ( 94.93)
Epoch: [32][130/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.0479e-01 (8.7701e-01)	Acc@1  71.88 ( 74.32)	Acc@5  92.97 ( 94.90)
Epoch: [32][140/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.6533e-01 (8.7517e-01)	Acc@1  71.88 ( 74.37)	Acc@5  92.19 ( 94.87)
Epoch: [32][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.9453e-01 (8.7523e-01)	Acc@1  75.00 ( 74.37)	Acc@5  94.53 ( 94.88)
Epoch: [32][160/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.2432e-01 (8.7538e-01)	Acc@1  71.09 ( 74.32)	Acc@5  93.75 ( 94.90)
Epoch: [32][170/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.4570e-01 (8.7608e-01)	Acc@1  75.00 ( 74.29)	Acc@5  96.09 ( 94.89)
Epoch: [32][180/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0332e+00 (8.7505e-01)	Acc@1  67.97 ( 74.36)	Acc@5  92.19 ( 94.89)
Epoch: [32][190/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.9053e-01 (8.7313e-01)	Acc@1  78.12 ( 74.33)	Acc@5  94.53 ( 94.91)
Epoch: [32][200/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.3018e-01 (8.7340e-01)	Acc@1  71.09 ( 74.32)	Acc@5  95.31 ( 94.87)
Epoch: [32][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.2969e-01 (8.7360e-01)	Acc@1  72.66 ( 74.28)	Acc@5  95.31 ( 94.88)
Epoch: [32][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5977e-01 (8.7309e-01)	Acc@1  77.34 ( 74.32)	Acc@5  96.09 ( 94.88)
Epoch: [32][230/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1982e-01 (8.7351e-01)	Acc@1  71.09 ( 74.25)	Acc@5  96.88 ( 94.87)
Epoch: [32][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.7646e-01 (8.7286e-01)	Acc@1  75.00 ( 74.27)	Acc@5  96.09 ( 94.89)
Epoch: [32][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.2861e-01 (8.7085e-01)	Acc@1  71.88 ( 74.33)	Acc@5  96.88 ( 94.90)
Epoch: [32][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.3311e-01 (8.7120e-01)	Acc@1  71.09 ( 74.31)	Acc@5  96.88 ( 94.88)
Epoch: [32][270/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.6436e-01 (8.7203e-01)	Acc@1  67.19 ( 74.29)	Acc@5  92.97 ( 94.85)
Epoch: [32][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4209e-01 (8.7227e-01)	Acc@1  79.69 ( 74.23)	Acc@5  99.22 ( 94.83)
Epoch: [32][290/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1719e-01 (8.7085e-01)	Acc@1  80.47 ( 74.28)	Acc@5  95.31 ( 94.85)
Epoch: [32][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.8184e-01 (8.7068e-01)	Acc@1  72.66 ( 74.27)	Acc@5  94.53 ( 94.84)
Epoch: [32][310/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3379e-01 (8.7036e-01)	Acc@1  82.81 ( 74.28)	Acc@5  96.88 ( 94.86)
Epoch: [32][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.9697e-01 (8.7143e-01)	Acc@1  72.66 ( 74.24)	Acc@5  94.53 ( 94.85)
Epoch: [32][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7979e-01 (8.7102e-01)	Acc@1  78.12 ( 74.23)	Acc@5  95.31 ( 94.85)
Epoch: [32][340/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0801e+00 (8.7177e-01)	Acc@1  64.84 ( 74.24)	Acc@5  95.31 ( 94.84)
Epoch: [32][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.2373e-01 (8.7135e-01)	Acc@1  75.00 ( 74.22)	Acc@5  95.31 ( 94.84)
Epoch: [32][360/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0625e+00 (8.7261e-01)	Acc@1  71.88 ( 74.20)	Acc@5  92.97 ( 94.84)
Epoch: [32][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.5107e-01 (8.7348e-01)	Acc@1  71.88 ( 74.18)	Acc@5  96.09 ( 94.83)
Epoch: [32][380/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.0332e+00 (8.7366e-01)	Acc@1  69.53 ( 74.17)	Acc@5  92.19 ( 94.84)
Epoch: [32][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.5918e-01 (8.7308e-01)	Acc@1  83.75 ( 74.19)	Acc@5  96.25 ( 94.85)
## e[32] optimizer.zero_grad (sum) time: 0.41440701484680176
## e[32]       loss.backward (sum) time: 7.371269702911377
## e[32]      optimizer.step (sum) time: 3.549020528793335
## epoch[32] training(only) time: 26.23888325691223
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.1855e+00 (1.1855e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.028 ( 0.038)	Loss 1.2627e+00 (1.1934e+00)	Acc@1  66.00 ( 67.82)	Acc@5  92.00 ( 89.91)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.2734e+00 (1.1757e+00)	Acc@1  66.00 ( 67.90)	Acc@5  90.00 ( 90.33)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.4385e+00 (1.1955e+00)	Acc@1  57.00 ( 66.81)	Acc@5  88.00 ( 90.03)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.2432e+00 (1.1862e+00)	Acc@1  65.00 ( 66.44)	Acc@5  93.00 ( 90.49)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1426e+00 (1.1898e+00)	Acc@1  64.00 ( 66.16)	Acc@5  90.00 ( 90.39)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2168e+00 (1.1790e+00)	Acc@1  68.00 ( 66.31)	Acc@5  89.00 ( 90.56)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0635e+00 (1.1849e+00)	Acc@1  69.00 ( 66.41)	Acc@5  95.00 ( 90.58)
Test: [ 80/100]	Time  0.030 ( 0.028)	Loss 1.3018e+00 (1.1922e+00)	Acc@1  63.00 ( 66.11)	Acc@5  90.00 ( 90.42)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 1.6885e+00 (1.1851e+00)	Acc@1  48.00 ( 66.11)	Acc@5  86.00 ( 90.54)
 * Acc@1 66.240 Acc@5 90.540
### epoch[32] execution time: 29.053834438323975
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.224 ( 0.224)	Data  0.148 ( 0.148)	Loss 6.4307e-01 (6.4307e-01)	Acc@1  81.25 ( 81.25)	Acc@5  97.66 ( 97.66)
Epoch: [33][ 10/391]	Time  0.068 ( 0.081)	Data  0.001 ( 0.015)	Loss 1.1006e+00 (8.2799e-01)	Acc@1  67.97 ( 76.07)	Acc@5  92.97 ( 95.31)
Epoch: [33][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.008)	Loss 9.0479e-01 (8.6572e-01)	Acc@1  79.69 ( 75.74)	Acc@5  90.62 ( 94.61)
Epoch: [33][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 9.1992e-01 (8.5545e-01)	Acc@1  75.00 ( 75.88)	Acc@5  92.97 ( 94.63)
Epoch: [33][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.0537e+00 (8.5801e-01)	Acc@1  67.97 ( 75.38)	Acc@5  91.41 ( 94.70)
Epoch: [33][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 7.9346e-01 (8.5216e-01)	Acc@1  78.91 ( 75.31)	Acc@5  96.09 ( 94.85)
Epoch: [33][ 60/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 9.2285e-01 (8.5021e-01)	Acc@1  74.22 ( 75.23)	Acc@5  95.31 ( 94.92)
Epoch: [33][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 8.9697e-01 (8.5534e-01)	Acc@1  74.22 ( 75.13)	Acc@5  92.97 ( 94.87)
Epoch: [33][ 80/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.1201e-01 (8.5821e-01)	Acc@1  77.34 ( 75.14)	Acc@5  97.66 ( 94.85)
Epoch: [33][ 90/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.1055e-01 (8.5234e-01)	Acc@1  75.00 ( 75.21)	Acc@5  94.53 ( 94.87)
Epoch: [33][100/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.7373e-01 (8.4756e-01)	Acc@1  82.81 ( 75.33)	Acc@5  98.44 ( 94.95)
Epoch: [33][110/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.2861e-01 (8.4711e-01)	Acc@1  76.56 ( 75.31)	Acc@5  94.53 ( 94.92)
Epoch: [33][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.6768e-01 (8.4831e-01)	Acc@1  72.66 ( 75.16)	Acc@5  93.75 ( 94.92)
Epoch: [33][130/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.9209e-01 (8.4890e-01)	Acc@1  75.00 ( 75.13)	Acc@5  91.41 ( 94.90)
Epoch: [33][140/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.5898e-01 (8.4652e-01)	Acc@1  69.53 ( 75.16)	Acc@5  95.31 ( 94.91)
Epoch: [33][150/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.4287e-01 (8.4649e-01)	Acc@1  70.31 ( 75.14)	Acc@5  92.19 ( 94.92)
Epoch: [33][160/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3828e-01 (8.4493e-01)	Acc@1  74.22 ( 75.07)	Acc@5  96.88 ( 94.96)
Epoch: [33][170/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7432e-01 (8.4873e-01)	Acc@1  78.91 ( 74.91)	Acc@5  96.09 ( 94.90)
Epoch: [33][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.9248e-01 (8.5322e-01)	Acc@1  76.56 ( 74.80)	Acc@5  95.31 ( 94.85)
Epoch: [33][190/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.4424e-01 (8.5419e-01)	Acc@1  71.09 ( 74.77)	Acc@5  96.88 ( 94.87)
Epoch: [33][200/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4756e-01 (8.5229e-01)	Acc@1  76.56 ( 74.87)	Acc@5  96.09 ( 94.87)
Epoch: [33][210/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3936e-01 (8.5118e-01)	Acc@1  73.44 ( 74.88)	Acc@5  96.09 ( 94.90)
Epoch: [33][220/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7627e-01 (8.5130e-01)	Acc@1  77.34 ( 74.89)	Acc@5  99.22 ( 94.91)
Epoch: [33][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7734e-01 (8.4973e-01)	Acc@1  81.25 ( 74.98)	Acc@5  94.53 ( 94.89)
Epoch: [33][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.4375e-01 (8.4999e-01)	Acc@1  73.44 ( 74.97)	Acc@5  96.09 ( 94.92)
Epoch: [33][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.8271e-01 (8.5085e-01)	Acc@1  78.12 ( 74.94)	Acc@5  96.09 ( 94.91)
Epoch: [33][260/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5400e-01 (8.5134e-01)	Acc@1  78.12 ( 74.90)	Acc@5  92.97 ( 94.92)
Epoch: [33][270/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.9209e-01 (8.5147e-01)	Acc@1  73.44 ( 74.89)	Acc@5  96.09 ( 94.93)
Epoch: [33][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.9502e-01 (8.5108e-01)	Acc@1  77.34 ( 74.92)	Acc@5  94.53 ( 94.95)
Epoch: [33][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.9209e-01 (8.5075e-01)	Acc@1  68.75 ( 74.88)	Acc@5  96.88 ( 94.95)
Epoch: [33][300/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3682e-01 (8.5039e-01)	Acc@1  78.91 ( 74.85)	Acc@5  97.66 ( 94.96)
Epoch: [33][310/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.9893e-01 (8.5110e-01)	Acc@1  77.34 ( 74.82)	Acc@5  92.97 ( 94.95)
Epoch: [33][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0010e+00 (8.5338e-01)	Acc@1  67.97 ( 74.73)	Acc@5  92.19 ( 94.91)
Epoch: [33][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3350e-01 (8.5282e-01)	Acc@1  75.78 ( 74.75)	Acc@5  96.88 ( 94.92)
Epoch: [33][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.7021e-01 (8.5173e-01)	Acc@1  74.22 ( 74.78)	Acc@5  92.19 ( 94.93)
Epoch: [33][350/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.4229e-01 (8.5137e-01)	Acc@1  74.22 ( 74.77)	Acc@5  96.09 ( 94.95)
Epoch: [33][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.2324e-01 (8.5220e-01)	Acc@1  75.78 ( 74.72)	Acc@5  94.53 ( 94.95)
Epoch: [33][370/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.6367e-01 (8.5134e-01)	Acc@1  78.91 ( 74.75)	Acc@5  96.09 ( 94.96)
Epoch: [33][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.7051e-01 (8.4959e-01)	Acc@1  75.78 ( 74.77)	Acc@5  93.75 ( 94.98)
Epoch: [33][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.1162e+00 (8.4987e-01)	Acc@1  68.75 ( 74.78)	Acc@5  92.50 ( 94.98)
## e[33] optimizer.zero_grad (sum) time: 0.40412044525146484
## e[33]       loss.backward (sum) time: 7.403168201446533
## e[33]      optimizer.step (sum) time: 3.520294189453125
## epoch[33] training(only) time: 26.168757915496826
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2129e+00 (1.2129e+00)	Acc@1  65.00 ( 65.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.2773e+00 (1.1944e+00)	Acc@1  66.00 ( 67.64)	Acc@5  91.00 ( 89.45)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3154e+00 (1.1719e+00)	Acc@1  64.00 ( 67.24)	Acc@5  90.00 ( 90.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.4756e+00 (1.1965e+00)	Acc@1  56.00 ( 66.52)	Acc@5  86.00 ( 89.71)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.2383e+00 (1.1859e+00)	Acc@1  64.00 ( 66.27)	Acc@5  92.00 ( 90.05)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.2002e+00 (1.1931e+00)	Acc@1  63.00 ( 65.98)	Acc@5  89.00 ( 90.10)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2002e+00 (1.1811e+00)	Acc@1  64.00 ( 66.16)	Acc@5  89.00 ( 90.30)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0576e+00 (1.1850e+00)	Acc@1  70.00 ( 66.27)	Acc@5  93.00 ( 90.34)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2969e+00 (1.1927e+00)	Acc@1  63.00 ( 66.11)	Acc@5  88.00 ( 90.15)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7148e+00 (1.1855e+00)	Acc@1  53.00 ( 66.19)	Acc@5  85.00 ( 90.26)
 * Acc@1 66.260 Acc@5 90.280
### epoch[33] execution time: 28.98489546775818
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.225 ( 0.225)	Data  0.149 ( 0.149)	Loss 9.1016e-01 (9.1016e-01)	Acc@1  72.66 ( 72.66)	Acc@5  96.09 ( 96.09)
Epoch: [34][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.014)	Loss 8.7402e-01 (8.3847e-01)	Acc@1  75.00 ( 75.50)	Acc@5  95.31 ( 95.45)
Epoch: [34][ 20/391]	Time  0.069 ( 0.075)	Data  0.001 ( 0.008)	Loss 7.1582e-01 (8.1224e-01)	Acc@1  78.91 ( 76.23)	Acc@5  97.66 ( 95.39)
Epoch: [34][ 30/391]	Time  0.068 ( 0.072)	Data  0.001 ( 0.006)	Loss 5.4736e-01 (8.2464e-01)	Acc@1  85.16 ( 75.78)	Acc@5 100.00 ( 95.06)
Epoch: [34][ 40/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.005)	Loss 8.6182e-01 (8.2971e-01)	Acc@1  71.09 ( 75.38)	Acc@5  96.09 ( 95.31)
Epoch: [34][ 50/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.004)	Loss 7.7783e-01 (8.3310e-01)	Acc@1  76.56 ( 74.98)	Acc@5  96.09 ( 95.37)
Epoch: [34][ 60/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.004)	Loss 7.9004e-01 (8.2841e-01)	Acc@1  75.78 ( 75.04)	Acc@5  94.53 ( 95.48)
Epoch: [34][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 1.0273e+00 (8.2820e-01)	Acc@1  71.88 ( 75.12)	Acc@5  93.75 ( 95.47)
Epoch: [34][ 80/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.0215e-01 (8.2938e-01)	Acc@1  78.12 ( 75.13)	Acc@5  96.88 ( 95.44)
Epoch: [34][ 90/391]	Time  0.071 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.3535e-01 (8.3033e-01)	Acc@1  82.03 ( 75.13)	Acc@5  97.66 ( 95.34)
Epoch: [34][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.6963e-01 (8.2851e-01)	Acc@1  75.78 ( 75.23)	Acc@5  96.88 ( 95.37)
Epoch: [34][110/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.9678e-01 (8.2556e-01)	Acc@1  78.12 ( 75.38)	Acc@5  98.44 ( 95.38)
Epoch: [34][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.6484e-01 (8.2629e-01)	Acc@1  71.88 ( 75.31)	Acc@5  91.41 ( 95.38)
Epoch: [34][130/391]	Time  0.074 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5098e-01 (8.2378e-01)	Acc@1  74.22 ( 75.36)	Acc@5  96.09 ( 95.41)
Epoch: [34][140/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.5156e-01 (8.2311e-01)	Acc@1  76.56 ( 75.45)	Acc@5  95.31 ( 95.40)
Epoch: [34][150/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.7002e-01 (8.2549e-01)	Acc@1  79.69 ( 75.39)	Acc@5  96.09 ( 95.40)
Epoch: [34][160/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.6279e-01 (8.2417e-01)	Acc@1  78.12 ( 75.44)	Acc@5  91.41 ( 95.37)
Epoch: [34][170/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.3545e-01 (8.2747e-01)	Acc@1  75.00 ( 75.33)	Acc@5  95.31 ( 95.31)
Epoch: [34][180/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.3164e-01 (8.2858e-01)	Acc@1  68.75 ( 75.29)	Acc@5  97.66 ( 95.27)
Epoch: [34][190/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.5166e-01 (8.3081e-01)	Acc@1  71.09 ( 75.23)	Acc@5  95.31 ( 95.27)
Epoch: [34][200/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 1.0488e+00 (8.3283e-01)	Acc@1  69.53 ( 75.25)	Acc@5  92.97 ( 95.20)
Epoch: [34][210/391]	Time  0.075 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2305e-01 (8.3048e-01)	Acc@1  82.81 ( 75.40)	Acc@5  96.09 ( 95.19)
Epoch: [34][220/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.1064e-01 (8.2858e-01)	Acc@1  75.00 ( 75.45)	Acc@5  92.97 ( 95.22)
Epoch: [34][230/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.7939e-01 (8.2996e-01)	Acc@1  76.56 ( 75.42)	Acc@5  92.97 ( 95.18)
Epoch: [34][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1299e-01 (8.3091e-01)	Acc@1  74.22 ( 75.40)	Acc@5  96.09 ( 95.16)
Epoch: [34][250/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.0176e-01 (8.2882e-01)	Acc@1  78.12 ( 75.48)	Acc@5  91.41 ( 95.15)
Epoch: [34][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0625e+00 (8.3053e-01)	Acc@1  68.75 ( 75.37)	Acc@5  90.62 ( 95.13)
Epoch: [34][270/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2852e-01 (8.2819e-01)	Acc@1  77.34 ( 75.45)	Acc@5  97.66 ( 95.18)
Epoch: [34][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0234e+00 (8.3047e-01)	Acc@1  71.09 ( 75.38)	Acc@5  91.41 ( 95.15)
Epoch: [34][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5840e-01 (8.3047e-01)	Acc@1  75.00 ( 75.37)	Acc@5  93.75 ( 95.13)
Epoch: [34][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.5215e-01 (8.3093e-01)	Acc@1  70.31 ( 75.36)	Acc@5  92.97 ( 95.12)
Epoch: [34][310/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.4473e-01 (8.3186e-01)	Acc@1  75.00 ( 75.35)	Acc@5  96.09 ( 95.12)
Epoch: [34][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.8223e-01 (8.3239e-01)	Acc@1  75.78 ( 75.32)	Acc@5  94.53 ( 95.10)
Epoch: [34][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.4775e-01 (8.3290e-01)	Acc@1  72.66 ( 75.30)	Acc@5  92.19 ( 95.10)
Epoch: [34][340/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.4678e-01 (8.3371e-01)	Acc@1  72.66 ( 75.27)	Acc@5  93.75 ( 95.12)
Epoch: [34][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.7012e-01 (8.3516e-01)	Acc@1  71.09 ( 75.22)	Acc@5  96.09 ( 95.10)
Epoch: [34][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.7510e-01 (8.3445e-01)	Acc@1  67.97 ( 75.24)	Acc@5  92.97 ( 95.09)
Epoch: [34][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.8174e-01 (8.3417e-01)	Acc@1  73.44 ( 75.21)	Acc@5  98.44 ( 95.12)
Epoch: [34][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1533e-01 (8.3251e-01)	Acc@1  81.25 ( 75.26)	Acc@5  96.09 ( 95.16)
Epoch: [34][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.3438e-01 (8.3083e-01)	Acc@1  80.00 ( 75.31)	Acc@5  96.25 ( 95.19)
## e[34] optimizer.zero_grad (sum) time: 0.41454243659973145
## e[34]       loss.backward (sum) time: 7.377612829208374
## e[34]      optimizer.step (sum) time: 3.609049081802368
## epoch[34] training(only) time: 26.3101646900177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 1.2119e+00 (1.2119e+00)	Acc@1  68.00 ( 68.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 1.2305e+00 (1.1839e+00)	Acc@1  64.00 ( 67.64)	Acc@5  91.00 ( 89.82)
Test: [ 20/100]	Time  0.029 ( 0.034)	Loss 1.2754e+00 (1.1559e+00)	Acc@1  68.00 ( 67.67)	Acc@5  91.00 ( 90.43)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5293e+00 (1.1799e+00)	Acc@1  58.00 ( 66.81)	Acc@5  88.00 ( 90.29)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.2109e+00 (1.1747e+00)	Acc@1  66.00 ( 66.32)	Acc@5  92.00 ( 90.44)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1738e+00 (1.1780e+00)	Acc@1  64.00 ( 66.20)	Acc@5  88.00 ( 90.49)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2295e+00 (1.1671e+00)	Acc@1  66.00 ( 66.28)	Acc@5  90.00 ( 90.62)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0332e+00 (1.1720e+00)	Acc@1  68.00 ( 66.31)	Acc@5  92.00 ( 90.59)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.2754e+00 (1.1807e+00)	Acc@1  63.00 ( 66.15)	Acc@5  89.00 ( 90.41)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7617e+00 (1.1742e+00)	Acc@1  52.00 ( 66.27)	Acc@5  86.00 ( 90.54)
 * Acc@1 66.410 Acc@5 90.570
### epoch[34] execution time: 29.157661199569702
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.223 ( 0.223)	Data  0.149 ( 0.149)	Loss 8.3936e-01 (8.3936e-01)	Acc@1  74.22 ( 74.22)	Acc@5  95.31 ( 95.31)
Epoch: [35][ 10/391]	Time  0.068 ( 0.081)	Data  0.001 ( 0.014)	Loss 7.8223e-01 (8.2559e-01)	Acc@1  76.56 ( 75.50)	Acc@5  96.09 ( 95.67)
Epoch: [35][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.008)	Loss 9.6680e-01 (8.1157e-01)	Acc@1  71.88 ( 76.04)	Acc@5  91.41 ( 95.72)
Epoch: [35][ 30/391]	Time  0.068 ( 0.072)	Data  0.001 ( 0.006)	Loss 8.6865e-01 (7.8019e-01)	Acc@1  75.78 ( 76.97)	Acc@5  92.19 ( 95.87)
Epoch: [35][ 40/391]	Time  0.070 ( 0.071)	Data  0.001 ( 0.005)	Loss 7.0068e-01 (7.8853e-01)	Acc@1  77.34 ( 76.62)	Acc@5  96.88 ( 95.73)
Epoch: [35][ 50/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 7.5830e-01 (7.9560e-01)	Acc@1  79.69 ( 76.38)	Acc@5  97.66 ( 95.68)
Epoch: [35][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 8.1055e-01 (8.0443e-01)	Acc@1  76.56 ( 76.19)	Acc@5  96.09 ( 95.58)
Epoch: [35][ 70/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.4082e-01 (8.0593e-01)	Acc@1  72.66 ( 76.20)	Acc@5  95.31 ( 95.49)
Epoch: [35][ 80/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.4590e-01 (7.9850e-01)	Acc@1  84.38 ( 76.46)	Acc@5  99.22 ( 95.63)
Epoch: [35][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.3057e-01 (7.9843e-01)	Acc@1  75.78 ( 76.42)	Acc@5  95.31 ( 95.61)
Epoch: [35][100/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.8750e-01 (7.9350e-01)	Acc@1  82.03 ( 76.59)	Acc@5  96.09 ( 95.65)
Epoch: [35][110/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.5801e-01 (8.0069e-01)	Acc@1  71.88 ( 76.18)	Acc@5  93.75 ( 95.65)
Epoch: [35][120/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.9639e-01 (7.9972e-01)	Acc@1  79.69 ( 76.24)	Acc@5  96.88 ( 95.66)
Epoch: [35][130/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.9170e-01 (8.0101e-01)	Acc@1  71.09 ( 76.31)	Acc@5  93.75 ( 95.59)
Epoch: [35][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.2705e-01 (8.0113e-01)	Acc@1  79.69 ( 76.30)	Acc@5  95.31 ( 95.58)
Epoch: [35][150/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.4033e-01 (8.0007e-01)	Acc@1  76.56 ( 76.31)	Acc@5  92.97 ( 95.61)
Epoch: [35][160/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3203e-01 (7.9851e-01)	Acc@1  76.56 ( 76.41)	Acc@5  93.75 ( 95.61)
Epoch: [35][170/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.0674e-01 (8.0596e-01)	Acc@1  67.97 ( 76.08)	Acc@5  96.09 ( 95.56)
Epoch: [35][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5498e-01 (8.0668e-01)	Acc@1  71.09 ( 76.05)	Acc@5  94.53 ( 95.55)
Epoch: [35][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.7256e-01 (8.0769e-01)	Acc@1  74.22 ( 76.04)	Acc@5  94.53 ( 95.48)
Epoch: [35][200/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0557e-01 (8.0403e-01)	Acc@1  78.12 ( 76.09)	Acc@5  96.09 ( 95.56)
Epoch: [35][210/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.6270e-01 (8.0568e-01)	Acc@1  75.00 ( 76.07)	Acc@5  98.44 ( 95.56)
Epoch: [35][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.7402e-01 (8.0727e-01)	Acc@1  71.88 ( 75.99)	Acc@5  95.31 ( 95.55)
Epoch: [35][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5684e-01 (8.0576e-01)	Acc@1  76.56 ( 75.97)	Acc@5  95.31 ( 95.58)
Epoch: [35][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.6719e-01 (8.0691e-01)	Acc@1  74.22 ( 75.93)	Acc@5  92.97 ( 95.55)
Epoch: [35][250/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9580e-01 (8.0759e-01)	Acc@1  81.25 ( 75.91)	Acc@5  95.31 ( 95.52)
Epoch: [35][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4053e-01 (8.0639e-01)	Acc@1  86.72 ( 75.99)	Acc@5  98.44 ( 95.53)
Epoch: [35][270/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3535e-01 (8.0604e-01)	Acc@1  75.78 ( 75.99)	Acc@5  95.31 ( 95.53)
Epoch: [35][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1250e-01 (8.0550e-01)	Acc@1  75.78 ( 76.01)	Acc@5  93.75 ( 95.53)
Epoch: [35][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5645e-01 (8.0503e-01)	Acc@1  75.00 ( 76.01)	Acc@5  95.31 ( 95.56)
Epoch: [35][300/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1172e+00 (8.0538e-01)	Acc@1  71.09 ( 76.02)	Acc@5  89.84 ( 95.53)
Epoch: [35][310/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2939e-01 (8.0661e-01)	Acc@1  79.69 ( 75.97)	Acc@5  98.44 ( 95.53)
Epoch: [35][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5635e-01 (8.0739e-01)	Acc@1  78.12 ( 75.96)	Acc@5  96.88 ( 95.52)
Epoch: [35][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1787e-01 (8.0753e-01)	Acc@1  76.56 ( 75.97)	Acc@5  92.97 ( 95.48)
Epoch: [35][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5107e-01 (8.0803e-01)	Acc@1  74.22 ( 75.94)	Acc@5  94.53 ( 95.45)
Epoch: [35][350/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5986e-01 (8.0689e-01)	Acc@1  75.78 ( 75.96)	Acc@5  92.97 ( 95.45)
Epoch: [35][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3545e-01 (8.0721e-01)	Acc@1  74.22 ( 75.95)	Acc@5  96.09 ( 95.43)
Epoch: [35][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.8594e-01 (8.0622e-01)	Acc@1  82.81 ( 75.96)	Acc@5  97.66 ( 95.44)
Epoch: [35][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.3105e-01 (8.0592e-01)	Acc@1  71.88 ( 75.95)	Acc@5  96.09 ( 95.45)
Epoch: [35][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.1738e-01 (8.0662e-01)	Acc@1  78.75 ( 75.91)	Acc@5  95.00 ( 95.44)
## e[35] optimizer.zero_grad (sum) time: 0.41314196586608887
## e[35]       loss.backward (sum) time: 7.3541858196258545
## e[35]      optimizer.step (sum) time: 3.5395681858062744
## epoch[35] training(only) time: 26.197826385498047
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.1934e+00 (1.1934e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.038)	Loss 1.2402e+00 (1.1937e+00)	Acc@1  68.00 ( 68.45)	Acc@5  93.00 ( 89.82)
Test: [ 20/100]	Time  0.026 ( 0.032)	Loss 1.3389e+00 (1.1624e+00)	Acc@1  63.00 ( 67.90)	Acc@5  88.00 ( 90.43)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 1.5039e+00 (1.1852e+00)	Acc@1  56.00 ( 67.10)	Acc@5  86.00 ( 89.87)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.2500e+00 (1.1786e+00)	Acc@1  64.00 ( 66.76)	Acc@5  91.00 ( 90.37)
Test: [ 50/100]	Time  0.025 ( 0.028)	Loss 1.2051e+00 (1.1853e+00)	Acc@1  63.00 ( 66.49)	Acc@5  88.00 ( 90.27)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.1953e+00 (1.1749e+00)	Acc@1  67.00 ( 66.61)	Acc@5  90.00 ( 90.44)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.9268e-01 (1.1786e+00)	Acc@1  71.00 ( 66.61)	Acc@5  93.00 ( 90.41)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.2949e+00 (1.1868e+00)	Acc@1  65.00 ( 66.36)	Acc@5  88.00 ( 90.23)
Test: [ 90/100]	Time  0.027 ( 0.027)	Loss 1.6963e+00 (1.1794e+00)	Acc@1  54.00 ( 66.43)	Acc@5  87.00 ( 90.35)
 * Acc@1 66.630 Acc@5 90.400
### epoch[35] execution time: 29.007538318634033
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.221 ( 0.221)	Data  0.146 ( 0.146)	Loss 7.6416e-01 (7.6416e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.88 ( 96.88)
Epoch: [36][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.014)	Loss 9.5605e-01 (8.0322e-01)	Acc@1  71.88 ( 76.28)	Acc@5  92.97 ( 95.38)
Epoch: [36][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.008)	Loss 5.9961e-01 (7.9332e-01)	Acc@1  82.81 ( 76.64)	Acc@5  97.66 ( 95.87)
Epoch: [36][ 30/391]	Time  0.071 ( 0.072)	Data  0.001 ( 0.006)	Loss 8.1641e-01 (7.9725e-01)	Acc@1  77.34 ( 76.56)	Acc@5  96.09 ( 95.74)
Epoch: [36][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.005)	Loss 8.1836e-01 (7.9789e-01)	Acc@1  73.44 ( 76.52)	Acc@5  96.09 ( 95.73)
Epoch: [36][ 50/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.004)	Loss 7.3291e-01 (7.8207e-01)	Acc@1  75.00 ( 76.81)	Acc@5  98.44 ( 95.88)
Epoch: [36][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.5342e-01 (7.8140e-01)	Acc@1  78.12 ( 76.69)	Acc@5  96.09 ( 95.97)
Epoch: [36][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.8262e-01 (7.7578e-01)	Acc@1  82.81 ( 76.90)	Acc@5  95.31 ( 96.02)
Epoch: [36][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.8516e-01 (7.7091e-01)	Acc@1  77.34 ( 77.07)	Acc@5  95.31 ( 96.10)
Epoch: [36][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.9785e-01 (7.7864e-01)	Acc@1  75.78 ( 76.83)	Acc@5  95.31 ( 95.87)
Epoch: [36][100/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.4697e-01 (7.7904e-01)	Acc@1  77.34 ( 76.78)	Acc@5  99.22 ( 95.91)
Epoch: [36][110/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.9785e-01 (7.8135e-01)	Acc@1  77.34 ( 76.74)	Acc@5  94.53 ( 95.85)
Epoch: [36][120/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.8320e-01 (7.8052e-01)	Acc@1  75.78 ( 76.78)	Acc@5  96.09 ( 95.82)
Epoch: [36][130/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.3389e-01 (7.8065e-01)	Acc@1  80.47 ( 76.71)	Acc@5  94.53 ( 95.76)
Epoch: [36][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.9678e-01 (7.7919e-01)	Acc@1  78.12 ( 76.77)	Acc@5  97.66 ( 95.79)
Epoch: [36][150/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.1152e-01 (7.8395e-01)	Acc@1  75.00 ( 76.59)	Acc@5  97.66 ( 95.77)
Epoch: [36][160/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7236e-01 (7.7994e-01)	Acc@1  82.81 ( 76.75)	Acc@5  97.66 ( 95.76)
Epoch: [36][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3252e-01 (7.8248e-01)	Acc@1  78.12 ( 76.65)	Acc@5  94.53 ( 95.74)
Epoch: [36][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3643e-01 (7.8698e-01)	Acc@1  72.66 ( 76.51)	Acc@5  94.53 ( 95.68)
Epoch: [36][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.9941e-01 (7.8679e-01)	Acc@1  75.78 ( 76.52)	Acc@5  97.66 ( 95.66)
Epoch: [36][200/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.8613e-01 (7.8739e-01)	Acc@1  75.78 ( 76.54)	Acc@5  95.31 ( 95.60)
Epoch: [36][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8945e-01 (7.8901e-01)	Acc@1  76.56 ( 76.43)	Acc@5  97.66 ( 95.58)
Epoch: [36][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7637e-01 (7.9062e-01)	Acc@1  80.47 ( 76.36)	Acc@5  94.53 ( 95.58)
Epoch: [36][230/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5684e-01 (7.8983e-01)	Acc@1  79.69 ( 76.40)	Acc@5  96.09 ( 95.58)
Epoch: [36][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.7207e-01 (7.9144e-01)	Acc@1  71.09 ( 76.33)	Acc@5  95.31 ( 95.56)
Epoch: [36][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.6289e-01 (7.9346e-01)	Acc@1  71.09 ( 76.26)	Acc@5  92.97 ( 95.53)
Epoch: [36][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7822e-01 (7.9304e-01)	Acc@1  80.47 ( 76.25)	Acc@5  97.66 ( 95.58)
Epoch: [36][270/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1973e-01 (7.9199e-01)	Acc@1  78.12 ( 76.30)	Acc@5  94.53 ( 95.56)
Epoch: [36][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2559e-01 (7.9290e-01)	Acc@1  77.34 ( 76.25)	Acc@5  96.88 ( 95.57)
Epoch: [36][290/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.6719e-01 (7.9407e-01)	Acc@1  77.34 ( 76.20)	Acc@5  95.31 ( 95.56)
Epoch: [36][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3828e-01 (7.9228e-01)	Acc@1  74.22 ( 76.22)	Acc@5  97.66 ( 95.58)
Epoch: [36][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0508e-01 (7.9246e-01)	Acc@1  74.22 ( 76.23)	Acc@5  98.44 ( 95.58)
Epoch: [36][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9775e-01 (7.9426e-01)	Acc@1  81.25 ( 76.17)	Acc@5  94.53 ( 95.54)
Epoch: [36][330/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.8477e-01 (7.9484e-01)	Acc@1  74.22 ( 76.15)	Acc@5  95.31 ( 95.55)
Epoch: [36][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5820e-01 (7.9392e-01)	Acc@1  81.25 ( 76.18)	Acc@5  96.09 ( 95.54)
Epoch: [36][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.9785e-01 (7.9329e-01)	Acc@1  76.56 ( 76.24)	Acc@5  98.44 ( 95.56)
Epoch: [36][360/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 9.4922e-01 (7.9313e-01)	Acc@1  73.44 ( 76.24)	Acc@5  90.62 ( 95.57)
Epoch: [36][370/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.0176e-01 (7.9408e-01)	Acc@1  77.34 ( 76.20)	Acc@5  95.31 ( 95.57)
Epoch: [36][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.2754e-01 (7.9282e-01)	Acc@1  81.25 ( 76.24)	Acc@5  94.53 ( 95.59)
Epoch: [36][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 9.6729e-01 (7.9269e-01)	Acc@1  68.75 ( 76.25)	Acc@5  93.75 ( 95.59)
## e[36] optimizer.zero_grad (sum) time: 0.4067537784576416
## e[36]       loss.backward (sum) time: 7.391841650009155
## e[36]      optimizer.step (sum) time: 3.5929148197174072
## epoch[36] training(only) time: 26.234997272491455
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1904e+00 (1.1904e+00)	Acc@1  69.00 ( 69.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1523e+00 (1.1677e+00)	Acc@1  68.00 ( 69.09)	Acc@5  93.00 ( 89.82)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3125e+00 (1.1543e+00)	Acc@1  65.00 ( 68.19)	Acc@5  90.00 ( 90.38)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.4775e+00 (1.1814e+00)	Acc@1  57.00 ( 67.10)	Acc@5  89.00 ( 90.10)
Test: [ 40/100]	Time  0.026 ( 0.029)	Loss 1.2256e+00 (1.1717e+00)	Acc@1  65.00 ( 66.71)	Acc@5  91.00 ( 90.51)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.1748e+00 (1.1776e+00)	Acc@1  64.00 ( 66.39)	Acc@5  87.00 ( 90.61)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2139e+00 (1.1681e+00)	Acc@1  69.00 ( 66.67)	Acc@5  89.00 ( 90.72)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.0127e+00 (1.1705e+00)	Acc@1  71.00 ( 66.87)	Acc@5  94.00 ( 90.70)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.2285e+00 (1.1779e+00)	Acc@1  63.00 ( 66.63)	Acc@5  90.00 ( 90.54)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7578e+00 (1.1710e+00)	Acc@1  57.00 ( 66.76)	Acc@5  89.00 ( 90.65)
 * Acc@1 66.850 Acc@5 90.680
### epoch[36] execution time: 29.04249596595764
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.230 ( 0.230)	Data  0.154 ( 0.154)	Loss 9.2432e-01 (9.2432e-01)	Acc@1  70.31 ( 70.31)	Acc@5  95.31 ( 95.31)
Epoch: [37][ 10/391]	Time  0.062 ( 0.081)	Data  0.001 ( 0.015)	Loss 7.3584e-01 (8.0748e-01)	Acc@1  78.12 ( 76.78)	Acc@5  96.09 ( 95.45)
Epoch: [37][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.008)	Loss 7.3779e-01 (8.0399e-01)	Acc@1  76.56 ( 76.90)	Acc@5  97.66 ( 95.39)
Epoch: [37][ 30/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.006)	Loss 7.9492e-01 (7.9289e-01)	Acc@1  76.56 ( 76.81)	Acc@5  95.31 ( 95.64)
Epoch: [37][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.005)	Loss 9.0576e-01 (7.8881e-01)	Acc@1  76.56 ( 76.66)	Acc@5  91.41 ( 95.62)
Epoch: [37][ 50/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.004)	Loss 8.5352e-01 (7.8337e-01)	Acc@1  66.41 ( 76.61)	Acc@5  96.88 ( 95.85)
Epoch: [37][ 60/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.004)	Loss 8.0176e-01 (7.8865e-01)	Acc@1  73.44 ( 76.40)	Acc@5  94.53 ( 95.77)
Epoch: [37][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.4600e-01 (7.8263e-01)	Acc@1  82.03 ( 76.51)	Acc@5  96.88 ( 95.93)
Epoch: [37][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.5146e-01 (7.8038e-01)	Acc@1  76.56 ( 76.62)	Acc@5  97.66 ( 96.02)
Epoch: [37][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.7783e-01 (7.8078e-01)	Acc@1  76.56 ( 76.61)	Acc@5  96.09 ( 96.01)
Epoch: [37][100/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.0020e-01 (7.7898e-01)	Acc@1  82.03 ( 76.61)	Acc@5  95.31 ( 96.06)
Epoch: [37][110/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.7686e-01 (7.7917e-01)	Acc@1  75.78 ( 76.63)	Acc@5  96.09 ( 95.97)
Epoch: [37][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.8271e-01 (7.7641e-01)	Acc@1  78.12 ( 76.76)	Acc@5  94.53 ( 96.02)
Epoch: [37][130/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.8506e-01 (7.7840e-01)	Acc@1  81.25 ( 76.84)	Acc@5  95.31 ( 95.93)
Epoch: [37][140/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.1689e-01 (7.7846e-01)	Acc@1  77.34 ( 76.85)	Acc@5  97.66 ( 95.99)
Epoch: [37][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.0137e-01 (7.7737e-01)	Acc@1  68.75 ( 76.81)	Acc@5  93.75 ( 95.99)
Epoch: [37][160/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.9883e-01 (7.7609e-01)	Acc@1  78.12 ( 76.86)	Acc@5  95.31 ( 95.99)
Epoch: [37][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.5361e-01 (7.7841e-01)	Acc@1  69.53 ( 76.75)	Acc@5  95.31 ( 95.95)
Epoch: [37][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.7314e-01 (7.7987e-01)	Acc@1  72.66 ( 76.72)	Acc@5  90.62 ( 95.85)
Epoch: [37][190/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.1748e-01 (7.7846e-01)	Acc@1  73.44 ( 76.69)	Acc@5  95.31 ( 95.87)
Epoch: [37][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1582e-01 (7.7866e-01)	Acc@1  81.25 ( 76.64)	Acc@5  94.53 ( 95.88)
Epoch: [37][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0068e-01 (7.8026e-01)	Acc@1  82.81 ( 76.63)	Acc@5  96.09 ( 95.86)
Epoch: [37][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.4668e-01 (7.7946e-01)	Acc@1  76.56 ( 76.71)	Acc@5  94.53 ( 95.87)
Epoch: [37][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.4766e-01 (7.8014e-01)	Acc@1  78.91 ( 76.70)	Acc@5  92.19 ( 95.84)
Epoch: [37][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.4180e-01 (7.8126e-01)	Acc@1  76.56 ( 76.69)	Acc@5  92.97 ( 95.80)
Epoch: [37][250/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5840e-01 (7.8080e-01)	Acc@1  72.66 ( 76.70)	Acc@5  92.97 ( 95.78)
Epoch: [37][260/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.6963e-01 (7.8047e-01)	Acc@1  74.22 ( 76.70)	Acc@5  96.09 ( 95.77)
Epoch: [37][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1143e-01 (7.7975e-01)	Acc@1  77.34 ( 76.71)	Acc@5  96.88 ( 95.75)
Epoch: [37][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.2666e-01 (7.7864e-01)	Acc@1  72.66 ( 76.74)	Acc@5  96.09 ( 95.78)
Epoch: [37][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3984e-01 (7.8006e-01)	Acc@1  73.44 ( 76.72)	Acc@5  93.75 ( 95.75)
Epoch: [37][300/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4941e-01 (7.8170e-01)	Acc@1  78.12 ( 76.62)	Acc@5  99.22 ( 95.73)
Epoch: [37][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9863e-01 (7.8078e-01)	Acc@1  85.16 ( 76.69)	Acc@5  95.31 ( 95.73)
Epoch: [37][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.6953e-01 (7.8026e-01)	Acc@1  75.78 ( 76.72)	Acc@5  96.09 ( 95.75)
Epoch: [37][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4844e-01 (7.8028e-01)	Acc@1  76.56 ( 76.69)	Acc@5  99.22 ( 95.77)
Epoch: [37][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.0576e-01 (7.8020e-01)	Acc@1  71.09 ( 76.71)	Acc@5  92.97 ( 95.76)
Epoch: [37][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1045e-01 (7.7989e-01)	Acc@1  77.34 ( 76.73)	Acc@5  96.09 ( 95.77)
Epoch: [37][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7441e-01 (7.8019e-01)	Acc@1  75.00 ( 76.73)	Acc@5  96.09 ( 95.77)
Epoch: [37][370/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 9.4043e-01 (7.8103e-01)	Acc@1  71.09 ( 76.69)	Acc@5  92.97 ( 95.76)
Epoch: [37][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 1.0420e+00 (7.8120e-01)	Acc@1  69.53 ( 76.68)	Acc@5  95.31 ( 95.76)
Epoch: [37][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.8926e-01 (7.8015e-01)	Acc@1  82.50 ( 76.72)	Acc@5 100.00 ( 95.76)
## e[37] optimizer.zero_grad (sum) time: 0.40775609016418457
## e[37]       loss.backward (sum) time: 7.373265504837036
## e[37]      optimizer.step (sum) time: 3.506718635559082
## epoch[37] training(only) time: 26.134766101837158
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2275e+00 (1.2275e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.2080e+00 (1.1958e+00)	Acc@1  69.00 ( 67.64)	Acc@5  92.00 ( 89.82)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.3203e+00 (1.1611e+00)	Acc@1  64.00 ( 67.67)	Acc@5  91.00 ( 90.67)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.5615e+00 (1.1901e+00)	Acc@1  58.00 ( 66.97)	Acc@5  88.00 ( 90.26)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.2988e+00 (1.1831e+00)	Acc@1  59.00 ( 66.46)	Acc@5  90.00 ( 90.68)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.1992e+00 (1.1877e+00)	Acc@1  64.00 ( 66.47)	Acc@5  89.00 ( 90.69)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2549e+00 (1.1751e+00)	Acc@1  66.00 ( 66.79)	Acc@5  90.00 ( 90.80)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0332e+00 (1.1794e+00)	Acc@1  72.00 ( 66.89)	Acc@5  93.00 ( 90.85)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.2695e+00 (1.1867e+00)	Acc@1  67.00 ( 66.75)	Acc@5  89.00 ( 90.67)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.7842e+00 (1.1814e+00)	Acc@1  53.00 ( 66.68)	Acc@5  86.00 ( 90.70)
 * Acc@1 66.830 Acc@5 90.720
### epoch[37] execution time: 28.946146249771118
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.221 ( 0.221)	Data  0.145 ( 0.145)	Loss 7.5391e-01 (7.5391e-01)	Acc@1  78.12 ( 78.12)	Acc@5  97.66 ( 97.66)
Epoch: [38][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.014)	Loss 6.7920e-01 (7.0836e-01)	Acc@1  78.91 ( 79.33)	Acc@5  98.44 ( 96.66)
Epoch: [38][ 20/391]	Time  0.067 ( 0.075)	Data  0.001 ( 0.008)	Loss 6.6357e-01 (7.4935e-01)	Acc@1  80.47 ( 77.64)	Acc@5  97.66 ( 96.02)
Epoch: [38][ 30/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.006)	Loss 8.0322e-01 (7.4216e-01)	Acc@1  79.69 ( 77.82)	Acc@5  94.53 ( 96.07)
Epoch: [38][ 40/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.005)	Loss 7.5146e-01 (7.4488e-01)	Acc@1  76.56 ( 77.61)	Acc@5  93.75 ( 96.00)
Epoch: [38][ 50/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 9.2920e-01 (7.5000e-01)	Acc@1  72.66 ( 77.60)	Acc@5  92.97 ( 95.97)
Epoch: [38][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.2158e-01 (7.4877e-01)	Acc@1  78.12 ( 77.39)	Acc@5  96.88 ( 96.06)
Epoch: [38][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.2656e-01 (7.5213e-01)	Acc@1  78.12 ( 77.21)	Acc@5  95.31 ( 96.10)
Epoch: [38][ 80/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 9.2529e-01 (7.4723e-01)	Acc@1  74.22 ( 77.56)	Acc@5  94.53 ( 96.13)
Epoch: [38][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.9697e-01 (7.4759e-01)	Acc@1  70.31 ( 77.53)	Acc@5  96.88 ( 96.16)
Epoch: [38][100/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5830e-01 (7.5018e-01)	Acc@1  74.22 ( 77.51)	Acc@5  95.31 ( 96.12)
Epoch: [38][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.3398e-01 (7.4922e-01)	Acc@1  76.56 ( 77.53)	Acc@5  93.75 ( 96.16)
Epoch: [38][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.6699e-01 (7.5509e-01)	Acc@1  77.34 ( 77.32)	Acc@5  95.31 ( 96.07)
Epoch: [38][130/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.2871e-01 (7.5638e-01)	Acc@1  71.09 ( 77.40)	Acc@5  95.31 ( 96.03)
Epoch: [38][140/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.9404e-01 (7.5361e-01)	Acc@1  78.12 ( 77.52)	Acc@5  92.97 ( 96.09)
Epoch: [38][150/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.2031e-01 (7.5470e-01)	Acc@1  72.66 ( 77.44)	Acc@5  94.53 ( 96.09)
Epoch: [38][160/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3633e-01 (7.5481e-01)	Acc@1  77.34 ( 77.47)	Acc@5  97.66 ( 96.06)
Epoch: [38][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8750e-01 (7.5895e-01)	Acc@1  78.12 ( 77.30)	Acc@5  97.66 ( 96.00)
Epoch: [38][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.5557e-01 (7.6056e-01)	Acc@1  69.53 ( 77.21)	Acc@5  94.53 ( 96.02)
Epoch: [38][190/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1191e-01 (7.5962e-01)	Acc@1  74.22 ( 77.24)	Acc@5  98.44 ( 96.02)
Epoch: [38][200/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.6807e-01 (7.5914e-01)	Acc@1  76.56 ( 77.27)	Acc@5  96.88 ( 96.01)
Epoch: [38][210/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.4238e-01 (7.6257e-01)	Acc@1  68.75 ( 77.17)	Acc@5  96.09 ( 95.98)
Epoch: [38][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.3018e-01 (7.6534e-01)	Acc@1  73.44 ( 77.09)	Acc@5  93.75 ( 95.96)
Epoch: [38][230/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4707e-01 (7.6661e-01)	Acc@1  75.78 ( 77.09)	Acc@5  96.09 ( 95.91)
Epoch: [38][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5781e-01 (7.6578e-01)	Acc@1  75.78 ( 77.10)	Acc@5  95.31 ( 95.93)
Epoch: [38][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.6475e-01 (7.6600e-01)	Acc@1  77.34 ( 77.12)	Acc@5  92.97 ( 95.90)
Epoch: [38][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0703e-01 (7.6519e-01)	Acc@1  81.25 ( 77.17)	Acc@5  96.88 ( 95.92)
Epoch: [38][270/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2939e-01 (7.6411e-01)	Acc@1  81.25 ( 77.19)	Acc@5  94.53 ( 95.94)
Epoch: [38][280/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3242e-01 (7.6270e-01)	Acc@1  75.78 ( 77.23)	Acc@5  97.66 ( 95.95)
Epoch: [38][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8408e-01 (7.6405e-01)	Acc@1  78.12 ( 77.19)	Acc@5  96.09 ( 95.94)
Epoch: [38][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7490e-01 (7.6583e-01)	Acc@1  76.56 ( 77.11)	Acc@5  96.88 ( 95.93)
Epoch: [38][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5586e-01 (7.6746e-01)	Acc@1  71.88 ( 77.07)	Acc@5  96.88 ( 95.91)
Epoch: [38][320/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.8467e-01 (7.6664e-01)	Acc@1  75.00 ( 77.11)	Acc@5  96.09 ( 95.93)
Epoch: [38][330/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.9736e-01 (7.6645e-01)	Acc@1  75.78 ( 77.10)	Acc@5  95.31 ( 95.95)
Epoch: [38][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.2324e-01 (7.6552e-01)	Acc@1  74.22 ( 77.14)	Acc@5  94.53 ( 95.94)
Epoch: [38][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.1074e-01 (7.6506e-01)	Acc@1  85.94 ( 77.13)	Acc@5  99.22 ( 95.95)
Epoch: [38][360/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.4736e-01 (7.6497e-01)	Acc@1  85.94 ( 77.14)	Acc@5  97.66 ( 95.95)
Epoch: [38][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.9004e-01 (7.6566e-01)	Acc@1  77.34 ( 77.14)	Acc@5  94.53 ( 95.94)
Epoch: [38][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.1777e-01 (7.6476e-01)	Acc@1  78.12 ( 77.16)	Acc@5  96.88 ( 95.95)
Epoch: [38][390/391]	Time  0.057 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.4551e-01 (7.6554e-01)	Acc@1  78.75 ( 77.11)	Acc@5  96.25 ( 95.95)
## e[38] optimizer.zero_grad (sum) time: 0.41347789764404297
## e[38]       loss.backward (sum) time: 7.37262225151062
## e[38]      optimizer.step (sum) time: 3.5732924938201904
## epoch[38] training(only) time: 26.203577995300293
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1602e+00 (1.1602e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.2236e+00 (1.1728e+00)	Acc@1  67.00 ( 68.36)	Acc@5  92.00 ( 90.36)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3457e+00 (1.1565e+00)	Acc@1  65.00 ( 67.95)	Acc@5  91.00 ( 90.81)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5020e+00 (1.1845e+00)	Acc@1  58.00 ( 67.13)	Acc@5  87.00 ( 90.29)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.1865e+00 (1.1737e+00)	Acc@1  67.00 ( 67.17)	Acc@5  92.00 ( 90.66)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.1621e+00 (1.1781e+00)	Acc@1  64.00 ( 66.84)	Acc@5  89.00 ( 90.76)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.1963e+00 (1.1693e+00)	Acc@1  66.00 ( 66.95)	Acc@5  90.00 ( 90.82)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.0332e+00 (1.1731e+00)	Acc@1  71.00 ( 67.07)	Acc@5  92.00 ( 90.80)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.2920e+00 (1.1822e+00)	Acc@1  63.00 ( 66.84)	Acc@5  87.00 ( 90.65)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.7744e+00 (1.1768e+00)	Acc@1  54.00 ( 66.85)	Acc@5  82.00 ( 90.68)
 * Acc@1 67.030 Acc@5 90.680
### epoch[38] execution time: 29.011975526809692
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.221 ( 0.221)	Data  0.142 ( 0.142)	Loss 6.9873e-01 (6.9873e-01)	Acc@1  78.91 ( 78.91)	Acc@5  95.31 ( 95.31)
Epoch: [39][ 10/391]	Time  0.069 ( 0.081)	Data  0.001 ( 0.014)	Loss 7.7490e-01 (7.3668e-01)	Acc@1  75.78 ( 77.49)	Acc@5  96.09 ( 96.09)
Epoch: [39][ 20/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.008)	Loss 8.2129e-01 (7.3789e-01)	Acc@1  78.91 ( 77.90)	Acc@5  92.97 ( 96.28)
Epoch: [39][ 30/391]	Time  0.065 ( 0.071)	Data  0.002 ( 0.006)	Loss 8.0225e-01 (7.3691e-01)	Acc@1  74.22 ( 78.05)	Acc@5  95.31 ( 96.22)
Epoch: [39][ 40/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.004)	Loss 7.3877e-01 (7.2994e-01)	Acc@1  76.56 ( 77.97)	Acc@5  95.31 ( 96.21)
Epoch: [39][ 50/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.004)	Loss 6.2451e-01 (7.2915e-01)	Acc@1  81.25 ( 77.77)	Acc@5  96.88 ( 96.12)
Epoch: [39][ 60/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.9102e-01 (7.3288e-01)	Acc@1  75.00 ( 77.75)	Acc@5  95.31 ( 96.08)
Epoch: [39][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.8564e-01 (7.3499e-01)	Acc@1  75.00 ( 77.83)	Acc@5  96.88 ( 96.14)
Epoch: [39][ 80/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.2715e-01 (7.3404e-01)	Acc@1  73.44 ( 77.86)	Acc@5  96.88 ( 96.18)
Epoch: [39][ 90/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.5449e-01 (7.3370e-01)	Acc@1  71.09 ( 77.94)	Acc@5  96.09 ( 96.17)
Epoch: [39][100/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.0723e-01 (7.3743e-01)	Acc@1  72.66 ( 77.83)	Acc@5  93.75 ( 96.12)
Epoch: [39][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.4648e-01 (7.3857e-01)	Acc@1  82.03 ( 77.88)	Acc@5  98.44 ( 96.10)
Epoch: [39][120/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.0166e-01 (7.3920e-01)	Acc@1  76.56 ( 77.76)	Acc@5  96.88 ( 96.15)
Epoch: [39][130/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.0420e-01 (7.3995e-01)	Acc@1  78.12 ( 77.81)	Acc@5  94.53 ( 96.09)
Epoch: [39][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.9102e-01 (7.4265e-01)	Acc@1  76.56 ( 77.78)	Acc@5  96.09 ( 96.10)
Epoch: [39][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.6572e-01 (7.4447e-01)	Acc@1  69.53 ( 77.81)	Acc@5  94.53 ( 96.03)
Epoch: [39][160/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.7139e-01 (7.4476e-01)	Acc@1  78.12 ( 77.83)	Acc@5  96.88 ( 96.03)
Epoch: [39][170/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.5527e-01 (7.4365e-01)	Acc@1  82.81 ( 77.90)	Acc@5  97.66 ( 96.07)
Epoch: [39][180/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.1494e-01 (7.4380e-01)	Acc@1  76.56 ( 77.90)	Acc@5  95.31 ( 96.07)
Epoch: [39][190/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.7734e-01 (7.4482e-01)	Acc@1  75.78 ( 77.86)	Acc@5  96.09 ( 96.06)
Epoch: [39][200/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.8711e-01 (7.4730e-01)	Acc@1  79.69 ( 77.81)	Acc@5  93.75 ( 96.01)
Epoch: [39][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3037e-01 (7.4629e-01)	Acc@1  82.03 ( 77.84)	Acc@5  96.09 ( 96.01)
Epoch: [39][220/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.5918e-01 (7.4783e-01)	Acc@1  77.34 ( 77.79)	Acc@5  97.66 ( 95.98)
Epoch: [39][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.2227e-01 (7.4662e-01)	Acc@1  71.88 ( 77.82)	Acc@5  95.31 ( 95.99)
Epoch: [39][240/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.2480e-01 (7.4880e-01)	Acc@1  75.00 ( 77.71)	Acc@5  92.97 ( 95.95)
Epoch: [39][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0303e-01 (7.5044e-01)	Acc@1  80.47 ( 77.70)	Acc@5  97.66 ( 95.92)
Epoch: [39][260/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2266e-01 (7.5399e-01)	Acc@1  80.47 ( 77.61)	Acc@5  96.09 ( 95.88)
Epoch: [39][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5439e-01 (7.5407e-01)	Acc@1  75.78 ( 77.57)	Acc@5  96.09 ( 95.89)
Epoch: [39][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.9492e-01 (7.5454e-01)	Acc@1  74.22 ( 77.56)	Acc@5  94.53 ( 95.88)
Epoch: [39][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2695e-01 (7.5245e-01)	Acc@1  84.38 ( 77.60)	Acc@5  98.44 ( 95.91)
Epoch: [39][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2754e-01 (7.5319e-01)	Acc@1  75.78 ( 77.59)	Acc@5  94.53 ( 95.89)
Epoch: [39][310/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.6611e-01 (7.5259e-01)	Acc@1  79.69 ( 77.62)	Acc@5  93.75 ( 95.85)
Epoch: [39][320/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1475e-01 (7.5187e-01)	Acc@1  85.16 ( 77.65)	Acc@5  96.09 ( 95.85)
Epoch: [39][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4844e-01 (7.5125e-01)	Acc@1  82.03 ( 77.65)	Acc@5  96.09 ( 95.86)
Epoch: [39][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5889e-01 (7.5073e-01)	Acc@1  73.44 ( 77.67)	Acc@5  96.88 ( 95.87)
Epoch: [39][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.4365e-01 (7.4942e-01)	Acc@1  78.91 ( 77.70)	Acc@5  96.88 ( 95.89)
Epoch: [39][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.2158e-01 (7.4733e-01)	Acc@1  82.81 ( 77.75)	Acc@5  97.66 ( 95.93)
Epoch: [39][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.2852e-01 (7.4771e-01)	Acc@1  75.78 ( 77.73)	Acc@5  98.44 ( 95.93)
Epoch: [39][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.3867e-01 (7.4879e-01)	Acc@1  78.91 ( 77.67)	Acc@5  96.88 ( 95.94)
Epoch: [39][390/391]	Time  0.054 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.9180e-01 (7.4989e-01)	Acc@1  82.50 ( 77.65)	Acc@5  96.25 ( 95.93)
## e[39] optimizer.zero_grad (sum) time: 0.4093470573425293
## e[39]       loss.backward (sum) time: 7.354171276092529
## e[39]      optimizer.step (sum) time: 3.6130406856536865
## epoch[39] training(only) time: 26.26326823234558
# Switched to evaluate mode...
Test: [  0/100]	Time  0.171 ( 0.171)	Loss 1.1855e+00 (1.1855e+00)	Acc@1  69.00 ( 69.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.034 ( 0.040)	Loss 1.1572e+00 (1.1647e+00)	Acc@1  66.00 ( 68.00)	Acc@5  91.00 ( 90.18)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.3398e+00 (1.1518e+00)	Acc@1  63.00 ( 67.81)	Acc@5  91.00 ( 90.43)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.5576e+00 (1.1823e+00)	Acc@1  56.00 ( 66.74)	Acc@5  86.00 ( 89.90)
Test: [ 40/100]	Time  0.030 ( 0.030)	Loss 1.2109e+00 (1.1730e+00)	Acc@1  60.00 ( 66.54)	Acc@5  92.00 ( 90.39)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2285e+00 (1.1784e+00)	Acc@1  63.00 ( 66.37)	Acc@5  88.00 ( 90.55)
Test: [ 60/100]	Time  0.029 ( 0.029)	Loss 1.2217e+00 (1.1672e+00)	Acc@1  64.00 ( 66.66)	Acc@5  91.00 ( 90.77)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.0400e+00 (1.1717e+00)	Acc@1  69.00 ( 66.65)	Acc@5  92.00 ( 90.86)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2656e+00 (1.1801e+00)	Acc@1  65.00 ( 66.44)	Acc@5  88.00 ( 90.70)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7627e+00 (1.1734e+00)	Acc@1  53.00 ( 66.62)	Acc@5  84.00 ( 90.74)
 * Acc@1 66.780 Acc@5 90.730
### epoch[39] execution time: 29.112048625946045
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.226 ( 0.226)	Data  0.148 ( 0.148)	Loss 6.5479e-01 (6.5479e-01)	Acc@1  80.47 ( 80.47)	Acc@5  97.66 ( 97.66)
Epoch: [40][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 5.5811e-01 (6.7942e-01)	Acc@1  85.94 ( 79.76)	Acc@5  97.66 ( 96.66)
Epoch: [40][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.008)	Loss 6.4600e-01 (7.0540e-01)	Acc@1  82.81 ( 78.98)	Acc@5  95.31 ( 96.39)
Epoch: [40][ 30/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.006)	Loss 8.4326e-01 (7.1506e-01)	Acc@1  74.22 ( 78.40)	Acc@5  94.53 ( 96.42)
Epoch: [40][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.005)	Loss 7.6172e-01 (7.0888e-01)	Acc@1  80.47 ( 78.83)	Acc@5  92.97 ( 96.47)
Epoch: [40][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 7.5439e-01 (7.1917e-01)	Acc@1  77.34 ( 78.48)	Acc@5  94.53 ( 96.26)
Epoch: [40][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 7.9199e-01 (7.2282e-01)	Acc@1  75.00 ( 78.42)	Acc@5  96.09 ( 96.20)
Epoch: [40][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.2266e-01 (7.2299e-01)	Acc@1  78.12 ( 78.49)	Acc@5  95.31 ( 96.18)
Epoch: [40][ 80/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.7734e-01 (7.3222e-01)	Acc@1  80.47 ( 78.29)	Acc@5  96.09 ( 96.12)
Epoch: [40][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.5254e-01 (7.2997e-01)	Acc@1  73.44 ( 78.29)	Acc@5  94.53 ( 96.10)
Epoch: [40][100/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.9727e-01 (7.3018e-01)	Acc@1  77.34 ( 78.29)	Acc@5  96.09 ( 96.09)
Epoch: [40][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.1680e-01 (7.3259e-01)	Acc@1  73.44 ( 78.25)	Acc@5  96.88 ( 96.02)
Epoch: [40][120/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1299e-01 (7.3464e-01)	Acc@1  78.12 ( 78.26)	Acc@5  93.75 ( 96.00)
Epoch: [40][130/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4463e-01 (7.3399e-01)	Acc@1  81.25 ( 78.35)	Acc@5  95.31 ( 96.00)
Epoch: [40][140/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8115e-01 (7.3195e-01)	Acc@1  76.56 ( 78.41)	Acc@5  97.66 ( 96.03)
Epoch: [40][150/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1777e-01 (7.3705e-01)	Acc@1  82.03 ( 78.25)	Acc@5  96.09 ( 96.02)
Epoch: [40][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.0859e-01 (7.3820e-01)	Acc@1  75.00 ( 78.21)	Acc@5  93.75 ( 95.99)
Epoch: [40][170/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.8223e-01 (7.3949e-01)	Acc@1  76.56 ( 78.18)	Acc@5  96.88 ( 96.00)
Epoch: [40][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5195e-01 (7.3768e-01)	Acc@1  75.00 ( 78.16)	Acc@5  98.44 ( 96.04)
Epoch: [40][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6299e-01 (7.3779e-01)	Acc@1  80.47 ( 78.15)	Acc@5  97.66 ( 96.08)
Epoch: [40][200/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7148e-01 (7.3466e-01)	Acc@1  77.34 ( 78.23)	Acc@5  96.09 ( 96.10)
Epoch: [40][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4805e-01 (7.3384e-01)	Acc@1  73.44 ( 78.27)	Acc@5  96.88 ( 96.08)
Epoch: [40][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5781e-01 (7.3372e-01)	Acc@1  77.34 ( 78.22)	Acc@5  95.31 ( 96.10)
Epoch: [40][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5576e-01 (7.3397e-01)	Acc@1  81.25 ( 78.23)	Acc@5  98.44 ( 96.13)
Epoch: [40][240/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.8760e-01 (7.3350e-01)	Acc@1  80.47 ( 78.22)	Acc@5  96.88 ( 96.16)
Epoch: [40][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7773e-01 (7.3259e-01)	Acc@1  82.03 ( 78.27)	Acc@5  98.44 ( 96.17)
Epoch: [40][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6543e-01 (7.3199e-01)	Acc@1  84.38 ( 78.27)	Acc@5  99.22 ( 96.18)
Epoch: [40][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1484e-01 (7.3382e-01)	Acc@1  78.91 ( 78.21)	Acc@5  97.66 ( 96.16)
Epoch: [40][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.6660e-01 (7.3512e-01)	Acc@1  77.34 ( 78.15)	Acc@5  96.88 ( 96.16)
Epoch: [40][290/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7432e-01 (7.3574e-01)	Acc@1  81.25 ( 78.12)	Acc@5  96.88 ( 96.18)
Epoch: [40][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8262e-01 (7.3639e-01)	Acc@1  80.47 ( 78.10)	Acc@5  97.66 ( 96.15)
Epoch: [40][310/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5439e-01 (7.3652e-01)	Acc@1  75.00 ( 78.03)	Acc@5  97.66 ( 96.16)
Epoch: [40][320/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7861e-01 (7.3560e-01)	Acc@1  83.59 ( 78.04)	Acc@5  99.22 ( 96.16)
Epoch: [40][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4756e-01 (7.3435e-01)	Acc@1  78.91 ( 78.07)	Acc@5  94.53 ( 96.18)
Epoch: [40][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4990e-01 (7.3456e-01)	Acc@1  82.03 ( 78.07)	Acc@5  96.09 ( 96.18)
Epoch: [40][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7295e-01 (7.3610e-01)	Acc@1  74.22 ( 78.00)	Acc@5  96.09 ( 96.16)
Epoch: [40][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.5303e-01 (7.3668e-01)	Acc@1  73.44 ( 77.97)	Acc@5  96.88 ( 96.17)
Epoch: [40][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4609e-01 (7.3761e-01)	Acc@1  77.34 ( 77.92)	Acc@5  96.88 ( 96.17)
Epoch: [40][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.3965e-01 (7.3750e-01)	Acc@1  84.38 ( 77.89)	Acc@5  95.31 ( 96.18)
Epoch: [40][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.5010e-01 (7.3765e-01)	Acc@1  77.50 ( 77.87)	Acc@5  93.75 ( 96.17)
## e[40] optimizer.zero_grad (sum) time: 0.40142202377319336
## e[40]       loss.backward (sum) time: 7.358099460601807
## e[40]      optimizer.step (sum) time: 3.6053566932678223
## epoch[40] training(only) time: 26.214325666427612
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 1.1719e+00 (1.1719e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.025 ( 0.040)	Loss 1.1572e+00 (1.1725e+00)	Acc@1  68.00 ( 68.82)	Acc@5  92.00 ( 89.64)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.3340e+00 (1.1509e+00)	Acc@1  63.00 ( 67.95)	Acc@5  91.00 ( 90.57)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5469e+00 (1.1782e+00)	Acc@1  56.00 ( 67.19)	Acc@5  89.00 ( 90.23)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.2334e+00 (1.1707e+00)	Acc@1  62.00 ( 66.83)	Acc@5  92.00 ( 90.41)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2500e+00 (1.1762e+00)	Acc@1  64.00 ( 66.75)	Acc@5  89.00 ( 90.43)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2354e+00 (1.1681e+00)	Acc@1  64.00 ( 66.92)	Acc@5  91.00 ( 90.67)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 9.6533e-01 (1.1690e+00)	Acc@1  74.00 ( 67.07)	Acc@5  93.00 ( 90.80)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2061e+00 (1.1776e+00)	Acc@1  65.00 ( 66.68)	Acc@5  90.00 ( 90.67)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 1.7803e+00 (1.1715e+00)	Acc@1  57.00 ( 66.73)	Acc@5  88.00 ( 90.79)
 * Acc@1 67.030 Acc@5 90.860
### epoch[40] execution time: 29.001419067382812
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.225 ( 0.225)	Data  0.148 ( 0.148)	Loss 5.2148e-01 (5.2148e-01)	Acc@1  89.06 ( 89.06)	Acc@5  95.31 ( 95.31)
Epoch: [41][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.014)	Loss 8.2568e-01 (6.7037e-01)	Acc@1  75.00 ( 81.04)	Acc@5  94.53 ( 96.24)
Epoch: [41][ 20/391]	Time  0.066 ( 0.075)	Data  0.001 ( 0.008)	Loss 6.2061e-01 (6.8622e-01)	Acc@1  83.59 ( 80.06)	Acc@5  98.44 ( 96.21)
Epoch: [41][ 30/391]	Time  0.069 ( 0.073)	Data  0.001 ( 0.006)	Loss 6.9238e-01 (6.9553e-01)	Acc@1  78.91 ( 79.91)	Acc@5  98.44 ( 96.32)
Epoch: [41][ 40/391]	Time  0.075 ( 0.071)	Data  0.001 ( 0.005)	Loss 6.6504e-01 (7.0560e-01)	Acc@1  81.25 ( 79.57)	Acc@5  96.09 ( 96.40)
Epoch: [41][ 50/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 6.8066e-01 (7.1026e-01)	Acc@1  78.91 ( 79.18)	Acc@5  96.09 ( 96.40)
Epoch: [41][ 60/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.3857e-01 (7.0965e-01)	Acc@1  85.16 ( 79.20)	Acc@5  96.88 ( 96.44)
Epoch: [41][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 8.2666e-01 (7.1319e-01)	Acc@1  75.78 ( 79.17)	Acc@5  97.66 ( 96.41)
Epoch: [41][ 80/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.5615e-01 (7.0590e-01)	Acc@1  84.38 ( 79.33)	Acc@5  97.66 ( 96.47)
Epoch: [41][ 90/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.0557e-01 (7.1229e-01)	Acc@1  77.34 ( 79.02)	Acc@5  98.44 ( 96.45)
Epoch: [41][100/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 8.4131e-01 (7.1326e-01)	Acc@1  78.12 ( 79.10)	Acc@5  94.53 ( 96.47)
Epoch: [41][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.8203e-01 (7.1503e-01)	Acc@1  88.28 ( 79.09)	Acc@5  96.88 ( 96.42)
Epoch: [41][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.7373e-01 (7.1045e-01)	Acc@1  80.47 ( 79.22)	Acc@5  97.66 ( 96.46)
Epoch: [41][130/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.3037e-01 (7.0796e-01)	Acc@1  80.47 ( 79.32)	Acc@5  97.66 ( 96.47)
Epoch: [41][140/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.6074e-01 (7.1140e-01)	Acc@1  74.22 ( 79.23)	Acc@5  95.31 ( 96.40)
Epoch: [41][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.2422e-01 (7.1363e-01)	Acc@1  76.56 ( 79.13)	Acc@5  96.09 ( 96.39)
Epoch: [41][160/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5488e-01 (7.1609e-01)	Acc@1  72.66 ( 78.99)	Acc@5  95.31 ( 96.35)
Epoch: [41][170/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.1211e-01 (7.1602e-01)	Acc@1  73.44 ( 78.98)	Acc@5  92.19 ( 96.32)
Epoch: [41][180/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.7148e-01 (7.1766e-01)	Acc@1  75.00 ( 78.86)	Acc@5  97.66 ( 96.28)
Epoch: [41][190/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.3291e-01 (7.1790e-01)	Acc@1  78.12 ( 78.81)	Acc@5  92.19 ( 96.25)
Epoch: [41][200/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.6162e-01 (7.1760e-01)	Acc@1  83.59 ( 78.86)	Acc@5  94.53 ( 96.27)
Epoch: [41][210/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 9.3848e-01 (7.1944e-01)	Acc@1  67.97 ( 78.78)	Acc@5  96.88 ( 96.27)
Epoch: [41][220/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.8086e-01 (7.2029e-01)	Acc@1  73.44 ( 78.74)	Acc@5  91.41 ( 96.26)
Epoch: [41][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3975e-01 (7.2049e-01)	Acc@1  82.03 ( 78.74)	Acc@5  96.09 ( 96.27)
Epoch: [41][240/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0947e-01 (7.2101e-01)	Acc@1  78.12 ( 78.69)	Acc@5  96.88 ( 96.26)
Epoch: [41][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5049e-01 (7.1968e-01)	Acc@1  77.34 ( 78.71)	Acc@5  97.66 ( 96.28)
Epoch: [41][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1689e-01 (7.2147e-01)	Acc@1  75.00 ( 78.66)	Acc@5  95.31 ( 96.29)
Epoch: [41][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.2031e-01 (7.2315e-01)	Acc@1  75.78 ( 78.62)	Acc@5  94.53 ( 96.26)
Epoch: [41][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3682e-01 (7.2319e-01)	Acc@1  80.47 ( 78.58)	Acc@5  92.97 ( 96.27)
Epoch: [41][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.0713e-01 (7.2405e-01)	Acc@1  76.56 ( 78.57)	Acc@5  96.88 ( 96.25)
Epoch: [41][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7578e-01 (7.2482e-01)	Acc@1  83.59 ( 78.59)	Acc@5  96.88 ( 96.22)
Epoch: [41][310/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.3164e-01 (7.2618e-01)	Acc@1  71.88 ( 78.52)	Acc@5  93.75 ( 96.21)
Epoch: [41][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.0967e-01 (7.2831e-01)	Acc@1  73.44 ( 78.46)	Acc@5  93.75 ( 96.19)
Epoch: [41][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7979e-01 (7.2853e-01)	Acc@1  75.78 ( 78.45)	Acc@5  96.09 ( 96.19)
Epoch: [41][340/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8652e-01 (7.2906e-01)	Acc@1  77.34 ( 78.40)	Acc@5  96.88 ( 96.18)
Epoch: [41][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1396e-01 (7.3024e-01)	Acc@1  75.78 ( 78.37)	Acc@5  93.75 ( 96.16)
Epoch: [41][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.9248e-01 (7.3000e-01)	Acc@1  74.22 ( 78.38)	Acc@5  95.31 ( 96.17)
Epoch: [41][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1094e-01 (7.2909e-01)	Acc@1  82.81 ( 78.41)	Acc@5  95.31 ( 96.17)
Epoch: [41][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4805e-01 (7.2939e-01)	Acc@1  75.78 ( 78.38)	Acc@5  95.31 ( 96.19)
Epoch: [41][390/391]	Time  0.055 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.2764e-01 (7.3008e-01)	Acc@1  75.00 ( 78.35)	Acc@5  98.75 ( 96.18)
## e[41] optimizer.zero_grad (sum) time: 0.4017453193664551
## e[41]       loss.backward (sum) time: 7.392381191253662
## e[41]      optimizer.step (sum) time: 3.5795722007751465
## epoch[41] training(only) time: 26.22600531578064
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.1582e+00 (1.1582e+00)	Acc@1  68.00 ( 68.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1523e+00 (1.1901e+00)	Acc@1  68.00 ( 67.91)	Acc@5  93.00 ( 89.73)
Test: [ 20/100]	Time  0.026 ( 0.034)	Loss 1.3682e+00 (1.1685e+00)	Acc@1  64.00 ( 67.90)	Acc@5  89.00 ( 90.29)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.5107e+00 (1.1954e+00)	Acc@1  57.00 ( 67.26)	Acc@5  88.00 ( 89.97)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2490e+00 (1.1865e+00)	Acc@1  63.00 ( 67.10)	Acc@5  92.00 ( 90.34)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.2002e+00 (1.1903e+00)	Acc@1  68.00 ( 66.84)	Acc@5  88.00 ( 90.41)
Test: [ 60/100]	Time  0.029 ( 0.029)	Loss 1.2412e+00 (1.1803e+00)	Acc@1  70.00 ( 67.02)	Acc@5  88.00 ( 90.52)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.6826e-01 (1.1815e+00)	Acc@1  75.00 ( 67.10)	Acc@5  92.00 ( 90.62)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2861e+00 (1.1905e+00)	Acc@1  62.00 ( 66.65)	Acc@5  88.00 ( 90.47)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7598e+00 (1.1833e+00)	Acc@1  53.00 ( 66.71)	Acc@5  87.00 ( 90.53)
 * Acc@1 67.020 Acc@5 90.610
### epoch[41] execution time: 29.036197185516357
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.239 ( 0.239)	Data  0.159 ( 0.159)	Loss 8.3350e-01 (8.3350e-01)	Acc@1  78.12 ( 78.12)	Acc@5  92.97 ( 92.97)
Epoch: [42][ 10/391]	Time  0.069 ( 0.082)	Data  0.001 ( 0.015)	Loss 8.4814e-01 (7.4263e-01)	Acc@1  72.66 ( 78.05)	Acc@5  95.31 ( 95.60)
Epoch: [42][ 20/391]	Time  0.069 ( 0.075)	Data  0.001 ( 0.009)	Loss 1.0928e+00 (7.6093e-01)	Acc@1  66.41 ( 76.97)	Acc@5  92.97 ( 95.87)
Epoch: [42][ 30/391]	Time  0.067 ( 0.072)	Data  0.001 ( 0.006)	Loss 5.9473e-01 (7.3502e-01)	Acc@1  83.59 ( 78.05)	Acc@5  98.44 ( 96.17)
Epoch: [42][ 40/391]	Time  0.069 ( 0.071)	Data  0.001 ( 0.005)	Loss 7.3730e-01 (7.2141e-01)	Acc@1  75.00 ( 78.39)	Acc@5  96.09 ( 96.30)
Epoch: [42][ 50/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.004)	Loss 6.4795e-01 (7.1663e-01)	Acc@1  82.81 ( 78.54)	Acc@5  96.09 ( 96.40)
Epoch: [42][ 60/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.2490e-01 (7.1076e-01)	Acc@1  84.38 ( 78.60)	Acc@5  99.22 ( 96.45)
Epoch: [42][ 70/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 8.1396e-01 (7.1600e-01)	Acc@1  79.69 ( 78.60)	Acc@5  94.53 ( 96.42)
Epoch: [42][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.2549e-01 (7.1463e-01)	Acc@1  82.81 ( 78.67)	Acc@5  97.66 ( 96.43)
Epoch: [42][ 90/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.2793e-01 (7.1374e-01)	Acc@1  82.03 ( 78.67)	Acc@5  99.22 ( 96.46)
Epoch: [42][100/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.2031e-01 (7.1037e-01)	Acc@1  76.56 ( 78.76)	Acc@5  93.75 ( 96.46)
Epoch: [42][110/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.8760e-01 (7.1510e-01)	Acc@1  78.91 ( 78.73)	Acc@5  95.31 ( 96.42)
Epoch: [42][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2109e-01 (7.1372e-01)	Acc@1  78.91 ( 78.82)	Acc@5  97.66 ( 96.40)
Epoch: [42][130/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.3477e-01 (7.1247e-01)	Acc@1  80.47 ( 78.79)	Acc@5  95.31 ( 96.43)
Epoch: [42][140/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.1104e-01 (7.1210e-01)	Acc@1  74.22 ( 78.81)	Acc@5  95.31 ( 96.42)
Epoch: [42][150/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1514e-01 (7.0834e-01)	Acc@1  80.47 ( 78.83)	Acc@5  98.44 ( 96.44)
Epoch: [42][160/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5586e-01 (7.0568e-01)	Acc@1  75.00 ( 78.92)	Acc@5  96.09 ( 96.47)
Epoch: [42][170/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0898e-01 (7.0840e-01)	Acc@1  78.91 ( 78.83)	Acc@5  96.88 ( 96.44)
Epoch: [42][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3877e-01 (7.0838e-01)	Acc@1  75.78 ( 78.77)	Acc@5  95.31 ( 96.43)
Epoch: [42][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5088e-01 (7.0895e-01)	Acc@1  79.69 ( 78.78)	Acc@5  97.66 ( 96.39)
Epoch: [42][200/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3730e-01 (7.0909e-01)	Acc@1  74.22 ( 78.77)	Acc@5  95.31 ( 96.39)
Epoch: [42][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1641e-01 (7.0935e-01)	Acc@1  75.00 ( 78.78)	Acc@5  96.09 ( 96.38)
Epoch: [42][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3936e-01 (7.1173e-01)	Acc@1  75.00 ( 78.69)	Acc@5  95.31 ( 96.35)
Epoch: [42][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7188e-01 (7.1225e-01)	Acc@1  78.91 ( 78.68)	Acc@5  96.88 ( 96.35)
Epoch: [42][240/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6699e-01 (7.1252e-01)	Acc@1  79.69 ( 78.64)	Acc@5  97.66 ( 96.35)
Epoch: [42][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7783e-01 (7.1457e-01)	Acc@1  75.78 ( 78.54)	Acc@5  95.31 ( 96.34)
Epoch: [42][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9180e-01 (7.1476e-01)	Acc@1  84.38 ( 78.54)	Acc@5  97.66 ( 96.35)
Epoch: [42][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6162e-01 (7.1592e-01)	Acc@1  80.47 ( 78.51)	Acc@5  96.88 ( 96.37)
Epoch: [42][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5088e-01 (7.1410e-01)	Acc@1  78.91 ( 78.53)	Acc@5  98.44 ( 96.40)
Epoch: [42][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1670e-01 (7.1410e-01)	Acc@1  83.59 ( 78.53)	Acc@5  96.09 ( 96.38)
Epoch: [42][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7383e-01 (7.1644e-01)	Acc@1  82.03 ( 78.45)	Acc@5  97.66 ( 96.37)
Epoch: [42][310/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2314e-01 (7.1721e-01)	Acc@1  79.69 ( 78.41)	Acc@5  96.09 ( 96.37)
Epoch: [42][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4648e-01 (7.1678e-01)	Acc@1  83.59 ( 78.42)	Acc@5  94.53 ( 96.35)
Epoch: [42][330/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0312e-01 (7.1631e-01)	Acc@1  78.12 ( 78.38)	Acc@5  95.31 ( 96.36)
Epoch: [42][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6602e-01 (7.1566e-01)	Acc@1  78.91 ( 78.38)	Acc@5  96.88 ( 96.36)
Epoch: [42][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4443e-01 (7.1605e-01)	Acc@1  83.59 ( 78.42)	Acc@5  99.22 ( 96.34)
Epoch: [42][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3838e-01 (7.1574e-01)	Acc@1  73.44 ( 78.41)	Acc@5  96.09 ( 96.34)
Epoch: [42][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0264e-01 (7.1451e-01)	Acc@1  79.69 ( 78.46)	Acc@5  96.09 ( 96.35)
Epoch: [42][380/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0303e-01 (7.1474e-01)	Acc@1  82.03 ( 78.42)	Acc@5  98.44 ( 96.35)
Epoch: [42][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5479e-01 (7.1541e-01)	Acc@1  75.00 ( 78.39)	Acc@5 100.00 ( 96.35)
## e[42] optimizer.zero_grad (sum) time: 0.40957093238830566
## e[42]       loss.backward (sum) time: 7.399646520614624
## e[42]      optimizer.step (sum) time: 3.5760889053344727
## epoch[42] training(only) time: 26.216633796691895
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 1.1924e+00 (1.1924e+00)	Acc@1  68.00 ( 68.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.026 ( 0.040)	Loss 1.1943e+00 (1.1842e+00)	Acc@1  70.00 ( 68.73)	Acc@5  90.00 ( 89.82)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.3555e+00 (1.1586e+00)	Acc@1  64.00 ( 68.67)	Acc@5  91.00 ( 90.62)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5674e+00 (1.1937e+00)	Acc@1  58.00 ( 67.42)	Acc@5  85.00 ( 89.87)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.2236e+00 (1.1841e+00)	Acc@1  63.00 ( 67.02)	Acc@5  91.00 ( 90.20)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1924e+00 (1.1875e+00)	Acc@1  67.00 ( 66.98)	Acc@5  88.00 ( 90.31)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2061e+00 (1.1766e+00)	Acc@1  66.00 ( 66.92)	Acc@5  92.00 ( 90.56)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.8828e-01 (1.1792e+00)	Acc@1  69.00 ( 67.01)	Acc@5  92.00 ( 90.65)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.2451e+00 (1.1854e+00)	Acc@1  61.00 ( 66.78)	Acc@5  87.00 ( 90.56)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.7559e+00 (1.1765e+00)	Acc@1  53.00 ( 66.88)	Acc@5  87.00 ( 90.69)
 * Acc@1 67.250 Acc@5 90.760
### epoch[42] execution time: 29.025675535202026
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.231 ( 0.231)	Data  0.153 ( 0.153)	Loss 7.4756e-01 (7.4756e-01)	Acc@1  78.12 ( 78.12)	Acc@5  95.31 ( 95.31)
Epoch: [43][ 10/391]	Time  0.069 ( 0.084)	Data  0.001 ( 0.015)	Loss 8.8281e-01 (7.1480e-01)	Acc@1  73.44 ( 79.12)	Acc@5  96.88 ( 95.95)
Epoch: [43][ 20/391]	Time  0.069 ( 0.076)	Data  0.001 ( 0.008)	Loss 7.6709e-01 (7.1061e-01)	Acc@1  75.78 ( 78.12)	Acc@5  96.88 ( 96.39)
Epoch: [43][ 30/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.006)	Loss 8.7109e-01 (7.1015e-01)	Acc@1  77.34 ( 78.20)	Acc@5  94.53 ( 96.70)
Epoch: [43][ 40/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.005)	Loss 8.3496e-01 (7.0479e-01)	Acc@1  72.66 ( 78.49)	Acc@5  96.09 ( 96.82)
Epoch: [43][ 50/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 7.3584e-01 (7.0052e-01)	Acc@1  78.91 ( 78.55)	Acc@5  94.53 ( 96.74)
Epoch: [43][ 60/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.7910e-01 (6.9695e-01)	Acc@1  82.81 ( 78.61)	Acc@5 100.00 ( 96.79)
Epoch: [43][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.6895e-01 (6.9689e-01)	Acc@1  80.47 ( 78.82)	Acc@5  96.09 ( 96.67)
Epoch: [43][ 80/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.3623e-01 (6.9551e-01)	Acc@1  82.03 ( 78.85)	Acc@5  96.09 ( 96.64)
Epoch: [43][ 90/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.3145e-01 (6.9786e-01)	Acc@1  75.00 ( 78.81)	Acc@5  96.09 ( 96.61)
Epoch: [43][100/391]	Time  0.071 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.6611e-01 (7.0224e-01)	Acc@1  79.69 ( 78.76)	Acc@5  94.53 ( 96.53)
Epoch: [43][110/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.7725e-01 (7.0024e-01)	Acc@1  78.91 ( 78.89)	Acc@5  96.09 ( 96.52)
Epoch: [43][120/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.002)	Loss 5.9766e-01 (7.0134e-01)	Acc@1  82.03 ( 78.90)	Acc@5  98.44 ( 96.48)
Epoch: [43][130/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.002)	Loss 7.1289e-01 (7.0089e-01)	Acc@1  77.34 ( 78.97)	Acc@5  95.31 ( 96.46)
Epoch: [43][140/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.002)	Loss 5.8984e-01 (7.0192e-01)	Acc@1  82.81 ( 78.95)	Acc@5  99.22 ( 96.51)
Epoch: [43][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8071e-01 (7.0265e-01)	Acc@1  85.94 ( 78.93)	Acc@5  97.66 ( 96.46)
Epoch: [43][160/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.6953e-01 (7.0587e-01)	Acc@1  71.88 ( 78.77)	Acc@5  96.09 ( 96.43)
Epoch: [43][170/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.4990e-01 (7.0439e-01)	Acc@1  78.91 ( 78.87)	Acc@5  97.66 ( 96.46)
Epoch: [43][180/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.2275e-01 (7.0611e-01)	Acc@1  77.34 ( 78.75)	Acc@5  96.09 ( 96.43)
Epoch: [43][190/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.3379e-01 (7.0495e-01)	Acc@1  83.59 ( 78.82)	Acc@5  98.44 ( 96.46)
Epoch: [43][200/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.8311e-01 (7.0439e-01)	Acc@1  80.47 ( 78.81)	Acc@5  96.09 ( 96.46)
Epoch: [43][210/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.1338e-01 (7.0678e-01)	Acc@1  80.47 ( 78.70)	Acc@5  96.09 ( 96.43)
Epoch: [43][220/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.1621e-01 (7.0977e-01)	Acc@1  81.25 ( 78.59)	Acc@5  97.66 ( 96.38)
Epoch: [43][230/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.3721e-01 (7.0870e-01)	Acc@1  80.47 ( 78.60)	Acc@5  96.88 ( 96.39)
Epoch: [43][240/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.1299e-01 (7.0848e-01)	Acc@1  72.66 ( 78.58)	Acc@5  94.53 ( 96.40)
Epoch: [43][250/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.6914e-01 (7.0891e-01)	Acc@1  74.22 ( 78.54)	Acc@5  96.09 ( 96.43)
Epoch: [43][260/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2305e-01 (7.0888e-01)	Acc@1  82.03 ( 78.56)	Acc@5  96.88 ( 96.43)
Epoch: [43][270/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.8857e-01 (7.0652e-01)	Acc@1  75.78 ( 78.66)	Acc@5  96.09 ( 96.43)
Epoch: [43][280/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.9326e-01 (7.0617e-01)	Acc@1  83.59 ( 78.64)	Acc@5  98.44 ( 96.43)
Epoch: [43][290/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.2949e-01 (7.0553e-01)	Acc@1  76.56 ( 78.66)	Acc@5  96.88 ( 96.45)
Epoch: [43][300/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2891e-01 (7.0565e-01)	Acc@1  78.12 ( 78.65)	Acc@5  96.88 ( 96.44)
Epoch: [43][310/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.1768e-01 (7.0680e-01)	Acc@1  82.03 ( 78.60)	Acc@5  96.09 ( 96.41)
Epoch: [43][320/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.7773e-01 (7.0721e-01)	Acc@1  82.81 ( 78.60)	Acc@5  98.44 ( 96.41)
Epoch: [43][330/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5977e-01 (7.0843e-01)	Acc@1  75.78 ( 78.54)	Acc@5  98.44 ( 96.41)
Epoch: [43][340/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.9541e-01 (7.0931e-01)	Acc@1  78.12 ( 78.50)	Acc@5  97.66 ( 96.42)
Epoch: [43][350/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.8467e-01 (7.0874e-01)	Acc@1  76.56 ( 78.49)	Acc@5  96.88 ( 96.45)
Epoch: [43][360/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5146e-01 (7.0832e-01)	Acc@1  75.78 ( 78.50)	Acc@5  96.09 ( 96.45)
Epoch: [43][370/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.4160e-01 (7.0752e-01)	Acc@1  78.12 ( 78.54)	Acc@5  96.09 ( 96.44)
Epoch: [43][380/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.4219e-01 (7.0732e-01)	Acc@1  78.12 ( 78.56)	Acc@5  95.31 ( 96.44)
Epoch: [43][390/391]	Time  0.051 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5537e-01 (7.0872e-01)	Acc@1  77.50 ( 78.53)	Acc@5  96.25 ( 96.42)
## e[43] optimizer.zero_grad (sum) time: 0.4068784713745117
## e[43]       loss.backward (sum) time: 7.37477707862854
## e[43]      optimizer.step (sum) time: 3.7610676288604736
## epoch[43] training(only) time: 26.527625560760498
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.2080e+00 (1.2080e+00)	Acc@1  70.00 ( 70.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.1797e+00 (1.2009e+00)	Acc@1  70.00 ( 68.00)	Acc@5  91.00 ( 89.73)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.4463e+00 (1.1837e+00)	Acc@1  63.00 ( 67.71)	Acc@5  90.00 ( 89.90)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5537e+00 (1.2087e+00)	Acc@1  58.00 ( 67.03)	Acc@5  88.00 ( 89.61)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.2422e+00 (1.2026e+00)	Acc@1  67.00 ( 66.76)	Acc@5  90.00 ( 90.05)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1865e+00 (1.2045e+00)	Acc@1  66.00 ( 66.59)	Acc@5  87.00 ( 90.12)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 1.2031e+00 (1.1957e+00)	Acc@1  66.00 ( 66.66)	Acc@5  93.00 ( 90.28)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 9.7363e-01 (1.1961e+00)	Acc@1  72.00 ( 66.70)	Acc@5  94.00 ( 90.41)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2607e+00 (1.2015e+00)	Acc@1  64.00 ( 66.58)	Acc@5  87.00 ( 90.35)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8057e+00 (1.1934e+00)	Acc@1  53.00 ( 66.69)	Acc@5  86.00 ( 90.47)
 * Acc@1 66.920 Acc@5 90.520
### epoch[43] execution time: 29.36728048324585
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.220 ( 0.220)	Data  0.124 ( 0.124)	Loss 6.9482e-01 (6.9482e-01)	Acc@1  76.56 ( 76.56)	Acc@5  96.09 ( 96.09)
Epoch: [44][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.012)	Loss 5.5811e-01 (7.1684e-01)	Acc@1  82.81 ( 77.84)	Acc@5  96.88 ( 96.09)
Epoch: [44][ 20/391]	Time  0.064 ( 0.075)	Data  0.001 ( 0.007)	Loss 8.0420e-01 (7.3075e-01)	Acc@1  78.91 ( 78.09)	Acc@5  96.09 ( 95.94)
Epoch: [44][ 30/391]	Time  0.068 ( 0.072)	Data  0.001 ( 0.005)	Loss 6.1230e-01 (7.0760e-01)	Acc@1  83.59 ( 78.68)	Acc@5  97.66 ( 96.30)
Epoch: [44][ 40/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.004)	Loss 5.3857e-01 (7.0237e-01)	Acc@1  82.03 ( 78.98)	Acc@5  98.44 ( 96.17)
Epoch: [44][ 50/391]	Time  0.070 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.9912e-01 (6.9716e-01)	Acc@1  79.69 ( 79.01)	Acc@5  99.22 ( 96.28)
Epoch: [44][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 8.7256e-01 (6.9787e-01)	Acc@1  73.44 ( 78.92)	Acc@5  93.75 ( 96.39)
Epoch: [44][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.8857e-01 (6.9984e-01)	Acc@1  78.12 ( 78.88)	Acc@5  94.53 ( 96.40)
Epoch: [44][ 80/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.7832e-01 (6.9741e-01)	Acc@1  76.56 ( 78.90)	Acc@5  97.66 ( 96.45)
Epoch: [44][ 90/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.002)	Loss 5.8350e-01 (6.9342e-01)	Acc@1  79.69 ( 78.93)	Acc@5  97.66 ( 96.51)
Epoch: [44][100/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.4688e-01 (6.9155e-01)	Acc@1  85.16 ( 79.08)	Acc@5  99.22 ( 96.50)
Epoch: [44][110/391]	Time  0.077 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.7393e-01 (6.9541e-01)	Acc@1  76.56 ( 79.01)	Acc@5  95.31 ( 96.44)
Epoch: [44][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3369e-01 (6.9423e-01)	Acc@1  85.94 ( 78.94)	Acc@5  97.66 ( 96.44)
Epoch: [44][130/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.7969e-01 (6.9319e-01)	Acc@1  76.56 ( 79.00)	Acc@5  96.88 ( 96.46)
Epoch: [44][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.7920e-01 (6.9406e-01)	Acc@1  78.12 ( 78.97)	Acc@5  97.66 ( 96.48)
Epoch: [44][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.9727e-01 (6.9424e-01)	Acc@1  78.12 ( 78.97)	Acc@5  96.88 ( 96.48)
Epoch: [44][160/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.2393e-01 (6.9111e-01)	Acc@1  84.38 ( 79.08)	Acc@5  99.22 ( 96.49)
Epoch: [44][170/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.5088e-01 (6.9059e-01)	Acc@1  83.59 ( 79.10)	Acc@5  94.53 ( 96.52)
Epoch: [44][180/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.1533e-01 (6.9041e-01)	Acc@1  79.69 ( 79.13)	Acc@5  95.31 ( 96.51)
Epoch: [44][190/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2842e-01 (6.8992e-01)	Acc@1  85.16 ( 79.19)	Acc@5  96.09 ( 96.52)
Epoch: [44][200/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.6211e-01 (6.8956e-01)	Acc@1  77.34 ( 79.20)	Acc@5  96.88 ( 96.51)
Epoch: [44][210/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.0371e-01 (6.8995e-01)	Acc@1  78.91 ( 79.25)	Acc@5  95.31 ( 96.50)
Epoch: [44][220/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.1191e-01 (6.9194e-01)	Acc@1  77.34 ( 79.17)	Acc@5  96.09 ( 96.48)
Epoch: [44][230/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2549e-01 (6.8889e-01)	Acc@1  78.12 ( 79.25)	Acc@5  97.66 ( 96.54)
Epoch: [44][240/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5244e-01 (6.9073e-01)	Acc@1  80.47 ( 79.23)	Acc@5  94.53 ( 96.52)
Epoch: [44][250/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.7520e-01 (6.8986e-01)	Acc@1  81.25 ( 79.27)	Acc@5  98.44 ( 96.54)
Epoch: [44][260/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.0322e-01 (6.9003e-01)	Acc@1  75.00 ( 79.23)	Acc@5  95.31 ( 96.54)
Epoch: [44][270/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.8457e-01 (6.8937e-01)	Acc@1  80.47 ( 79.25)	Acc@5  96.88 ( 96.54)
Epoch: [44][280/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.8936e-01 (6.8883e-01)	Acc@1  83.59 ( 79.27)	Acc@5  97.66 ( 96.56)
Epoch: [44][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3145e-01 (6.8946e-01)	Acc@1  76.56 ( 79.23)	Acc@5  96.09 ( 96.54)
Epoch: [44][300/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.8896e-01 (6.9086e-01)	Acc@1  80.47 ( 79.16)	Acc@5  97.66 ( 96.54)
Epoch: [44][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.7100e-01 (6.9177e-01)	Acc@1  76.56 ( 79.15)	Acc@5  97.66 ( 96.53)
Epoch: [44][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.6309e-01 (6.9241e-01)	Acc@1  79.69 ( 79.14)	Acc@5  97.66 ( 96.50)
Epoch: [44][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.1182e-01 (6.9266e-01)	Acc@1  77.34 ( 79.11)	Acc@5  98.44 ( 96.50)
Epoch: [44][340/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.4990e-01 (6.9302e-01)	Acc@1  81.25 ( 79.12)	Acc@5  97.66 ( 96.50)
Epoch: [44][350/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.1240e-01 (6.9291e-01)	Acc@1  78.12 ( 79.11)	Acc@5  96.09 ( 96.50)
Epoch: [44][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.8428e-01 (6.9379e-01)	Acc@1  70.31 ( 79.08)	Acc@5  94.53 ( 96.49)
Epoch: [44][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.7725e-01 (6.9341e-01)	Acc@1  78.12 ( 79.10)	Acc@5  96.88 ( 96.50)
Epoch: [44][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.6758e-01 (6.9312e-01)	Acc@1  75.00 ( 79.09)	Acc@5  95.31 ( 96.51)
Epoch: [44][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.5195e-01 (6.9284e-01)	Acc@1  80.00 ( 79.13)	Acc@5  97.50 ( 96.50)
## e[44] optimizer.zero_grad (sum) time: 0.41837620735168457
## e[44]       loss.backward (sum) time: 7.418938875198364
## e[44]      optimizer.step (sum) time: 3.606391668319702
## epoch[44] training(only) time: 26.358256578445435
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.2129e+00 (1.2129e+00)	Acc@1  70.00 ( 70.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.2461e+00 (1.2022e+00)	Acc@1  71.00 ( 69.09)	Acc@5  92.00 ( 89.64)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.4150e+00 (1.1770e+00)	Acc@1  66.00 ( 68.48)	Acc@5  89.00 ( 90.14)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5156e+00 (1.2020e+00)	Acc@1  55.00 ( 67.61)	Acc@5  87.00 ( 89.71)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.2529e+00 (1.1936e+00)	Acc@1  69.00 ( 67.27)	Acc@5  91.00 ( 90.32)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1768e+00 (1.1975e+00)	Acc@1  68.00 ( 67.16)	Acc@5  88.00 ( 90.45)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2031e+00 (1.1868e+00)	Acc@1  64.00 ( 67.10)	Acc@5  92.00 ( 90.75)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.3848e-01 (1.1882e+00)	Acc@1  76.00 ( 67.20)	Acc@5  94.00 ( 90.79)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.2676e+00 (1.1955e+00)	Acc@1  65.00 ( 66.90)	Acc@5  86.00 ( 90.64)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8359e+00 (1.1874e+00)	Acc@1  55.00 ( 66.97)	Acc@5  83.00 ( 90.73)
 * Acc@1 67.290 Acc@5 90.720
### epoch[44] execution time: 29.198976755142212
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.226 ( 0.226)	Data  0.151 ( 0.151)	Loss 6.6602e-01 (6.6602e-01)	Acc@1  84.38 ( 84.38)	Acc@5  95.31 ( 95.31)
Epoch: [45][ 10/391]	Time  0.069 ( 0.081)	Data  0.001 ( 0.015)	Loss 5.3076e-01 (6.3188e-01)	Acc@1  86.72 ( 81.82)	Acc@5  96.88 ( 96.95)
Epoch: [45][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.008)	Loss 6.0938e-01 (6.4125e-01)	Acc@1  81.25 ( 81.44)	Acc@5  95.31 ( 96.76)
Epoch: [45][ 30/391]	Time  0.067 ( 0.072)	Data  0.001 ( 0.006)	Loss 6.4697e-01 (6.4733e-01)	Acc@1  82.03 ( 81.55)	Acc@5  95.31 ( 96.72)
Epoch: [45][ 40/391]	Time  0.069 ( 0.071)	Data  0.001 ( 0.005)	Loss 7.5244e-01 (6.5575e-01)	Acc@1  79.69 ( 81.15)	Acc@5  96.88 ( 96.82)
Epoch: [45][ 50/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.004)	Loss 9.5215e-01 (6.6725e-01)	Acc@1  69.53 ( 80.51)	Acc@5  96.88 ( 96.77)
Epoch: [45][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.004)	Loss 6.1279e-01 (6.5925e-01)	Acc@1  81.25 ( 80.55)	Acc@5  95.31 ( 96.87)
Epoch: [45][ 70/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.1387e-01 (6.6375e-01)	Acc@1  79.69 ( 80.31)	Acc@5  96.09 ( 96.88)
Epoch: [45][ 80/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.2695e-01 (6.6654e-01)	Acc@1  78.12 ( 80.19)	Acc@5  96.88 ( 96.85)
Epoch: [45][ 90/391]	Time  0.079 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.9473e-01 (6.6231e-01)	Acc@1  84.38 ( 80.27)	Acc@5  97.66 ( 96.89)
Epoch: [45][100/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.0840e-01 (6.6456e-01)	Acc@1  82.81 ( 80.23)	Acc@5  99.22 ( 96.93)
Epoch: [45][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.6133e-01 (6.6958e-01)	Acc@1  75.78 ( 80.17)	Acc@5  96.09 ( 96.86)
Epoch: [45][120/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.5771e-01 (6.6876e-01)	Acc@1  75.00 ( 80.13)	Acc@5  98.44 ( 96.86)
Epoch: [45][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.6504e-01 (6.6870e-01)	Acc@1  79.69 ( 80.06)	Acc@5  99.22 ( 96.90)
Epoch: [45][140/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.8359e-01 (6.6865e-01)	Acc@1  81.25 ( 80.04)	Acc@5  94.53 ( 96.89)
Epoch: [45][150/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.9688e-01 (6.6732e-01)	Acc@1  78.91 ( 80.10)	Acc@5  92.19 ( 96.90)
Epoch: [45][160/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7100e-01 (6.6865e-01)	Acc@1  76.56 ( 80.01)	Acc@5  92.97 ( 96.84)
Epoch: [45][170/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1094e-01 (6.6797e-01)	Acc@1  77.34 ( 80.03)	Acc@5  95.31 ( 96.85)
Epoch: [45][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8164e-01 (6.7026e-01)	Acc@1  83.59 ( 79.95)	Acc@5  96.88 ( 96.81)
Epoch: [45][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.7109e-01 (6.7105e-01)	Acc@1  77.34 ( 80.01)	Acc@5  92.97 ( 96.79)
Epoch: [45][200/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2900e-01 (6.7235e-01)	Acc@1  79.69 ( 79.92)	Acc@5  94.53 ( 96.76)
Epoch: [45][210/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.7354e-01 (6.7316e-01)	Acc@1  74.22 ( 79.87)	Acc@5  95.31 ( 96.78)
Epoch: [45][220/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.3408e-01 (6.7377e-01)	Acc@1  71.09 ( 79.83)	Acc@5  90.62 ( 96.74)
Epoch: [45][230/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4658e-01 (6.7439e-01)	Acc@1  73.44 ( 79.77)	Acc@5  98.44 ( 96.75)
Epoch: [45][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4746e-01 (6.7502e-01)	Acc@1  77.34 ( 79.73)	Acc@5  98.44 ( 96.73)
Epoch: [45][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9482e-01 (6.7448e-01)	Acc@1  75.78 ( 79.74)	Acc@5  98.44 ( 96.76)
Epoch: [45][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2754e-01 (6.7579e-01)	Acc@1  75.00 ( 79.70)	Acc@5  93.75 ( 96.73)
Epoch: [45][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6592e-01 (6.7579e-01)	Acc@1  82.81 ( 79.74)	Acc@5  97.66 ( 96.71)
Epoch: [45][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8408e-01 (6.7552e-01)	Acc@1  79.69 ( 79.77)	Acc@5  97.66 ( 96.72)
Epoch: [45][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5039e-01 (6.7573e-01)	Acc@1  75.78 ( 79.74)	Acc@5  99.22 ( 96.74)
Epoch: [45][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7227e-01 (6.7567e-01)	Acc@1  83.59 ( 79.75)	Acc@5  96.88 ( 96.74)
Epoch: [45][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2793e-01 (6.7613e-01)	Acc@1  78.91 ( 79.71)	Acc@5  98.44 ( 96.74)
Epoch: [45][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1680e-01 (6.7574e-01)	Acc@1  72.66 ( 79.71)	Acc@5  96.09 ( 96.74)
Epoch: [45][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3916e-01 (6.7548e-01)	Acc@1  82.81 ( 79.71)	Acc@5  93.75 ( 96.73)
Epoch: [45][340/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.0371e-01 (6.7464e-01)	Acc@1  71.09 ( 79.71)	Acc@5  99.22 ( 96.75)
Epoch: [45][350/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.6709e-01 (6.7701e-01)	Acc@1  75.00 ( 79.61)	Acc@5  94.53 ( 96.73)
Epoch: [45][360/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.9980e-01 (6.7799e-01)	Acc@1  77.34 ( 79.57)	Acc@5  97.66 ( 96.73)
Epoch: [45][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9424e-01 (6.7883e-01)	Acc@1  84.38 ( 79.55)	Acc@5  96.88 ( 96.72)
Epoch: [45][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.7529e-01 (6.7964e-01)	Acc@1  82.81 ( 79.56)	Acc@5  94.53 ( 96.72)
Epoch: [45][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.9004e-01 (6.8206e-01)	Acc@1  77.50 ( 79.47)	Acc@5  96.25 ( 96.68)
## e[45] optimizer.zero_grad (sum) time: 0.41297388076782227
## e[45]       loss.backward (sum) time: 7.3813254833221436
## e[45]      optimizer.step (sum) time: 3.5533900260925293
## epoch[45] training(only) time: 26.197832822799683
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.1318e+00 (1.1318e+00)	Acc@1  70.00 ( 70.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.028 ( 0.038)	Loss 1.2197e+00 (1.1768e+00)	Acc@1  69.00 ( 69.27)	Acc@5  91.00 ( 90.09)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.4717e+00 (1.1694e+00)	Acc@1  63.00 ( 68.76)	Acc@5  93.00 ( 91.10)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 1.5977e+00 (1.2035e+00)	Acc@1  55.00 ( 67.45)	Acc@5  84.00 ( 90.16)
Test: [ 40/100]	Time  0.026 ( 0.029)	Loss 1.2012e+00 (1.1882e+00)	Acc@1  68.00 ( 67.29)	Acc@5  91.00 ( 90.59)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.1924e+00 (1.1915e+00)	Acc@1  64.00 ( 67.24)	Acc@5  91.00 ( 90.76)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.1953e+00 (1.1831e+00)	Acc@1  69.00 ( 67.07)	Acc@5  92.00 ( 90.95)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0059e+00 (1.1812e+00)	Acc@1  73.00 ( 67.27)	Acc@5  94.00 ( 91.07)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2891e+00 (1.1911e+00)	Acc@1  69.00 ( 66.95)	Acc@5  87.00 ( 90.90)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.7188e+00 (1.1845e+00)	Acc@1  56.00 ( 66.93)	Acc@5  86.00 ( 91.00)
 * Acc@1 67.180 Acc@5 91.030
### epoch[45] execution time: 29.006999254226685
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.224 ( 0.224)	Data  0.145 ( 0.145)	Loss 6.1377e-01 (6.1377e-01)	Acc@1  80.47 ( 80.47)	Acc@5  97.66 ( 97.66)
Epoch: [46][ 10/391]	Time  0.067 ( 0.080)	Data  0.001 ( 0.014)	Loss 7.4512e-01 (6.3892e-01)	Acc@1  75.78 ( 80.40)	Acc@5  96.09 ( 97.44)
Epoch: [46][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 6.9385e-01 (6.5254e-01)	Acc@1  80.47 ( 80.51)	Acc@5  95.31 ( 96.91)
Epoch: [46][ 30/391]	Time  0.065 ( 0.071)	Data  0.002 ( 0.006)	Loss 5.1709e-01 (6.5568e-01)	Acc@1  83.59 ( 80.17)	Acc@5  97.66 ( 96.90)
Epoch: [46][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 6.9287e-01 (6.5842e-01)	Acc@1  77.34 ( 80.14)	Acc@5  96.88 ( 96.80)
Epoch: [46][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 7.2021e-01 (6.5842e-01)	Acc@1  77.34 ( 79.90)	Acc@5  94.53 ( 96.75)
Epoch: [46][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.8950e-01 (6.5539e-01)	Acc@1  85.16 ( 79.94)	Acc@5  99.22 ( 96.90)
Epoch: [46][ 70/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.0840e-01 (6.5927e-01)	Acc@1  77.34 ( 79.70)	Acc@5  98.44 ( 96.90)
Epoch: [46][ 80/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.9678e-01 (6.6301e-01)	Acc@1  78.91 ( 79.57)	Acc@5  97.66 ( 96.95)
Epoch: [46][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.6162e-01 (6.6556e-01)	Acc@1  79.69 ( 79.60)	Acc@5  97.66 ( 96.92)
Epoch: [46][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.0557e-01 (6.6229e-01)	Acc@1  78.91 ( 79.69)	Acc@5  95.31 ( 96.95)
Epoch: [46][110/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.8887e-01 (6.6318e-01)	Acc@1  80.47 ( 79.64)	Acc@5  96.88 ( 96.92)
Epoch: [46][120/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5928e-01 (6.6297e-01)	Acc@1  75.00 ( 79.69)	Acc@5  95.31 ( 96.91)
Epoch: [46][130/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 8.1494e-01 (6.6407e-01)	Acc@1  71.88 ( 79.62)	Acc@5  95.31 ( 96.90)
Epoch: [46][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.0166e-01 (6.6456e-01)	Acc@1  76.56 ( 79.61)	Acc@5  97.66 ( 96.88)
Epoch: [46][150/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7681e-01 (6.6403e-01)	Acc@1  87.50 ( 79.69)	Acc@5  98.44 ( 96.88)
Epoch: [46][160/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0107e-01 (6.6328e-01)	Acc@1  81.25 ( 79.69)	Acc@5  98.44 ( 96.90)
Epoch: [46][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2852e-01 (6.6364e-01)	Acc@1  78.12 ( 79.68)	Acc@5  94.53 ( 96.87)
Epoch: [46][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1768e-01 (6.6421e-01)	Acc@1  83.59 ( 79.68)	Acc@5  96.88 ( 96.84)
Epoch: [46][190/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.4375e-01 (6.6589e-01)	Acc@1  72.66 ( 79.62)	Acc@5  96.88 ( 96.83)
Epoch: [46][200/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2168e-01 (6.6477e-01)	Acc@1  77.34 ( 79.66)	Acc@5  95.31 ( 96.84)
Epoch: [46][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.0762e-01 (6.6298e-01)	Acc@1  75.00 ( 79.77)	Acc@5  94.53 ( 96.81)
Epoch: [46][220/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5732e-01 (6.6331e-01)	Acc@1  71.09 ( 79.79)	Acc@5  96.09 ( 96.81)
Epoch: [46][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2705e-01 (6.6392e-01)	Acc@1  78.91 ( 79.76)	Acc@5  95.31 ( 96.81)
Epoch: [46][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3672e-01 (6.6478e-01)	Acc@1  83.59 ( 79.70)	Acc@5  95.31 ( 96.80)
Epoch: [46][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1719e-01 (6.6382e-01)	Acc@1  82.81 ( 79.78)	Acc@5  97.66 ( 96.82)
Epoch: [46][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9043e-01 (6.6417e-01)	Acc@1  81.25 ( 79.74)	Acc@5  98.44 ( 96.85)
Epoch: [46][270/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9971e-01 (6.6423e-01)	Acc@1  78.91 ( 79.72)	Acc@5  96.88 ( 96.85)
Epoch: [46][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1475e-01 (6.6444e-01)	Acc@1  81.25 ( 79.76)	Acc@5  99.22 ( 96.84)
Epoch: [46][290/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.1787e-01 (6.6428e-01)	Acc@1  78.91 ( 79.80)	Acc@5  95.31 ( 96.83)
Epoch: [46][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8115e-01 (6.6508e-01)	Acc@1  78.91 ( 79.77)	Acc@5  96.88 ( 96.82)
Epoch: [46][310/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1562e-01 (6.6562e-01)	Acc@1  84.38 ( 79.79)	Acc@5  98.44 ( 96.82)
Epoch: [46][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1035e-01 (6.6527e-01)	Acc@1  81.25 ( 79.82)	Acc@5  98.44 ( 96.84)
Epoch: [46][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7266e-01 (6.6479e-01)	Acc@1  88.28 ( 79.87)	Acc@5  97.66 ( 96.83)
Epoch: [46][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6016e-01 (6.6389e-01)	Acc@1  80.47 ( 79.89)	Acc@5  96.88 ( 96.83)
Epoch: [46][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5137e-01 (6.6391e-01)	Acc@1  76.56 ( 79.85)	Acc@5  96.88 ( 96.84)
Epoch: [46][360/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4121e-01 (6.6515e-01)	Acc@1  76.56 ( 79.82)	Acc@5  98.44 ( 96.83)
Epoch: [46][370/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.8564e-01 (6.6592e-01)	Acc@1  71.88 ( 79.77)	Acc@5  96.88 ( 96.82)
Epoch: [46][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.6357e-01 (6.6729e-01)	Acc@1  80.47 ( 79.74)	Acc@5  98.44 ( 96.81)
Epoch: [46][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.6309e-01 (6.6855e-01)	Acc@1  80.00 ( 79.71)	Acc@5  97.50 ( 96.80)
## e[46] optimizer.zero_grad (sum) time: 0.40574049949645996
## e[46]       loss.backward (sum) time: 7.379190683364868
## e[46]      optimizer.step (sum) time: 3.59281325340271
## epoch[46] training(only) time: 26.222100973129272
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.1074e+00 (1.1074e+00)	Acc@1  72.00 ( 72.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.1582e+00 (1.1861e+00)	Acc@1  69.00 ( 68.73)	Acc@5  91.00 ( 90.18)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.4209e+00 (1.1752e+00)	Acc@1  63.00 ( 68.38)	Acc@5  90.00 ( 90.33)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5684e+00 (1.2067e+00)	Acc@1  54.00 ( 67.55)	Acc@5  84.00 ( 89.52)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.3242e+00 (1.1997e+00)	Acc@1  64.00 ( 67.12)	Acc@5  90.00 ( 90.12)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2090e+00 (1.2031e+00)	Acc@1  67.00 ( 67.04)	Acc@5  89.00 ( 90.16)
Test: [ 60/100]	Time  0.029 ( 0.028)	Loss 1.2480e+00 (1.1928e+00)	Acc@1  64.00 ( 67.03)	Acc@5  91.00 ( 90.36)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.7559e-01 (1.1893e+00)	Acc@1  72.00 ( 67.15)	Acc@5  95.00 ( 90.52)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.2744e+00 (1.1990e+00)	Acc@1  64.00 ( 66.83)	Acc@5  89.00 ( 90.36)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 1.7207e+00 (1.1925e+00)	Acc@1  56.00 ( 66.89)	Acc@5  85.00 ( 90.38)
 * Acc@1 67.170 Acc@5 90.430
### epoch[46] execution time: 29.033260583877563
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.223 ( 0.223)	Data  0.138 ( 0.138)	Loss 6.4111e-01 (6.4111e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.09 ( 96.09)
Epoch: [47][ 10/391]	Time  0.069 ( 0.080)	Data  0.001 ( 0.013)	Loss 4.8657e-01 (6.1779e-01)	Acc@1  84.38 ( 81.82)	Acc@5  98.44 ( 97.59)
Epoch: [47][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 6.1963e-01 (6.2689e-01)	Acc@1  79.69 ( 80.77)	Acc@5  98.44 ( 97.40)
Epoch: [47][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.006)	Loss 8.1543e-01 (6.1874e-01)	Acc@1  74.22 ( 81.17)	Acc@5  96.09 ( 97.48)
Epoch: [47][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.9731e-01 (6.2368e-01)	Acc@1  85.16 ( 81.00)	Acc@5  96.09 ( 97.41)
Epoch: [47][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.5605e-01 (6.2356e-01)	Acc@1  85.94 ( 80.99)	Acc@5  98.44 ( 97.29)
Epoch: [47][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.7812e-01 (6.3282e-01)	Acc@1  82.03 ( 80.90)	Acc@5  98.44 ( 97.12)
Epoch: [47][ 70/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.0166e-01 (6.3563e-01)	Acc@1  79.69 ( 80.80)	Acc@5  99.22 ( 97.10)
Epoch: [47][ 80/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.7383e-01 (6.4085e-01)	Acc@1  82.81 ( 80.61)	Acc@5  96.88 ( 97.04)
Epoch: [47][ 90/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.6113e-01 (6.4306e-01)	Acc@1  82.81 ( 80.51)	Acc@5  97.66 ( 97.05)
Epoch: [47][100/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3467e-01 (6.4288e-01)	Acc@1  80.47 ( 80.48)	Acc@5  98.44 ( 97.05)
Epoch: [47][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1123e-01 (6.4222e-01)	Acc@1  83.59 ( 80.61)	Acc@5  97.66 ( 97.09)
Epoch: [47][120/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3242e-01 (6.4609e-01)	Acc@1  78.12 ( 80.53)	Acc@5  95.31 ( 97.06)
Epoch: [47][130/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8887e-01 (6.4788e-01)	Acc@1  82.03 ( 80.56)	Acc@5  99.22 ( 97.04)
Epoch: [47][140/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6357e-01 (6.4908e-01)	Acc@1  79.69 ( 80.48)	Acc@5  97.66 ( 96.99)
Epoch: [47][150/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7441e-01 (6.5161e-01)	Acc@1  72.66 ( 80.35)	Acc@5  96.88 ( 96.95)
Epoch: [47][160/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4795e-01 (6.5105e-01)	Acc@1  84.38 ( 80.35)	Acc@5  96.09 ( 96.93)
Epoch: [47][170/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1436e-01 (6.5302e-01)	Acc@1  78.12 ( 80.37)	Acc@5  96.09 ( 96.95)
Epoch: [47][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3975e-01 (6.5255e-01)	Acc@1  80.47 ( 80.43)	Acc@5  94.53 ( 96.98)
Epoch: [47][190/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0488e-01 (6.4909e-01)	Acc@1  80.47 ( 80.49)	Acc@5  98.44 ( 97.01)
Epoch: [47][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5576e-01 (6.4755e-01)	Acc@1  75.78 ( 80.51)	Acc@5  96.09 ( 97.01)
Epoch: [47][210/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7227e-01 (6.4767e-01)	Acc@1  82.03 ( 80.45)	Acc@5  99.22 ( 97.02)
Epoch: [47][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8853e-01 (6.4643e-01)	Acc@1  85.94 ( 80.43)	Acc@5  99.22 ( 97.06)
Epoch: [47][230/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3438e-01 (6.4764e-01)	Acc@1  76.56 ( 80.41)	Acc@5  96.09 ( 97.03)
Epoch: [47][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7871e-01 (6.4735e-01)	Acc@1  79.69 ( 80.43)	Acc@5  95.31 ( 97.04)
Epoch: [47][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0156e-01 (6.4695e-01)	Acc@1  81.25 ( 80.41)	Acc@5  96.88 ( 97.07)
Epoch: [47][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1816e-01 (6.4662e-01)	Acc@1  80.47 ( 80.44)	Acc@5  96.09 ( 97.05)
Epoch: [47][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6602e-01 (6.4724e-01)	Acc@1  83.59 ( 80.45)	Acc@5  95.31 ( 97.01)
Epoch: [47][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1084e-01 (6.4870e-01)	Acc@1  83.59 ( 80.41)	Acc@5  97.66 ( 96.97)
Epoch: [47][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7529e-01 (6.4840e-01)	Acc@1  78.91 ( 80.43)	Acc@5  95.31 ( 96.96)
Epoch: [47][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.5781e-01 (6.4947e-01)	Acc@1  78.12 ( 80.43)	Acc@5  94.53 ( 96.94)
Epoch: [47][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.9502e-01 (6.5153e-01)	Acc@1  71.09 ( 80.36)	Acc@5  95.31 ( 96.93)
Epoch: [47][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.8906e-01 (6.5322e-01)	Acc@1  78.12 ( 80.32)	Acc@5  96.88 ( 96.89)
Epoch: [47][330/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.6133e-01 (6.5517e-01)	Acc@1  77.34 ( 80.26)	Acc@5  93.75 ( 96.88)
Epoch: [47][340/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.3135e-01 (6.5526e-01)	Acc@1  79.69 ( 80.22)	Acc@5  95.31 ( 96.87)
Epoch: [47][350/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.2559e-01 (6.5617e-01)	Acc@1  84.38 ( 80.23)	Acc@5  96.88 ( 96.87)
Epoch: [47][360/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.6660e-01 (6.5619e-01)	Acc@1  74.22 ( 80.26)	Acc@5  96.09 ( 96.87)
Epoch: [47][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.4160e-01 (6.5802e-01)	Acc@1  78.91 ( 80.15)	Acc@5  97.66 ( 96.84)
Epoch: [47][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.7236e-01 (6.5885e-01)	Acc@1  76.56 ( 80.10)	Acc@5  97.66 ( 96.84)
Epoch: [47][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.8857e-01 (6.5870e-01)	Acc@1  76.25 ( 80.12)	Acc@5  96.25 ( 96.83)
## e[47] optimizer.zero_grad (sum) time: 0.40540027618408203
## e[47]       loss.backward (sum) time: 7.33205246925354
## e[47]      optimizer.step (sum) time: 3.5877737998962402
## epoch[47] training(only) time: 26.14216184616089
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.1455e+00 (1.1455e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.026 ( 0.038)	Loss 1.2461e+00 (1.2093e+00)	Acc@1  71.00 ( 69.09)	Acc@5  90.00 ( 89.27)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.4854e+00 (1.1935e+00)	Acc@1  63.00 ( 68.43)	Acc@5  90.00 ( 90.05)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 1.5342e+00 (1.2195e+00)	Acc@1  60.00 ( 67.39)	Acc@5  85.00 ( 89.61)
Test: [ 40/100]	Time  0.028 ( 0.029)	Loss 1.2900e+00 (1.2023e+00)	Acc@1  67.00 ( 67.17)	Acc@5  90.00 ( 90.20)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2080e+00 (1.2080e+00)	Acc@1  68.00 ( 67.12)	Acc@5  89.00 ( 90.16)
Test: [ 60/100]	Time  0.029 ( 0.028)	Loss 1.2871e+00 (1.1982e+00)	Acc@1  65.00 ( 67.28)	Acc@5  90.00 ( 90.33)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 9.5703e-01 (1.1997e+00)	Acc@1  74.00 ( 67.32)	Acc@5  94.00 ( 90.52)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3281e+00 (1.2096e+00)	Acc@1  63.00 ( 66.94)	Acc@5  85.00 ( 90.28)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 1.8125e+00 (1.2002e+00)	Acc@1  55.00 ( 66.87)	Acc@5  87.00 ( 90.51)
 * Acc@1 67.070 Acc@5 90.500
### epoch[47] execution time: 28.95351505279541
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.220 ( 0.220)	Data  0.145 ( 0.145)	Loss 5.1221e-01 (5.1221e-01)	Acc@1  85.94 ( 85.94)	Acc@5  98.44 ( 98.44)
Epoch: [48][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.014)	Loss 7.0703e-01 (6.2509e-01)	Acc@1  78.91 ( 80.33)	Acc@5  96.88 ( 97.16)
Epoch: [48][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.008)	Loss 7.0947e-01 (6.6341e-01)	Acc@1  77.34 ( 79.20)	Acc@5  96.88 ( 96.76)
Epoch: [48][ 30/391]	Time  0.067 ( 0.072)	Data  0.001 ( 0.006)	Loss 6.6992e-01 (6.5634e-01)	Acc@1  79.69 ( 79.71)	Acc@5  97.66 ( 96.82)
Epoch: [48][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 6.4648e-01 (6.4843e-01)	Acc@1  78.12 ( 79.97)	Acc@5  97.66 ( 97.08)
Epoch: [48][ 50/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.4053e-01 (6.3702e-01)	Acc@1  85.16 ( 80.42)	Acc@5  97.66 ( 97.21)
Epoch: [48][ 60/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.3975e-01 (6.3862e-01)	Acc@1  79.69 ( 80.52)	Acc@5  95.31 ( 97.14)
Epoch: [48][ 70/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.4102e-01 (6.3650e-01)	Acc@1  85.16 ( 80.66)	Acc@5  98.44 ( 97.27)
Epoch: [48][ 80/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.3350e-01 (6.3504e-01)	Acc@1  72.66 ( 80.78)	Acc@5  93.75 ( 97.27)
Epoch: [48][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.8350e-01 (6.3716e-01)	Acc@1  85.16 ( 80.69)	Acc@5  96.88 ( 97.27)
Epoch: [48][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.8604e-01 (6.3745e-01)	Acc@1  80.47 ( 80.73)	Acc@5  97.66 ( 97.21)
Epoch: [48][110/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5518e-01 (6.3670e-01)	Acc@1  81.25 ( 80.79)	Acc@5  97.66 ( 97.23)
Epoch: [48][120/391]	Time  0.079 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.8604e-01 (6.3645e-01)	Acc@1  79.69 ( 80.79)	Acc@5  97.66 ( 97.24)
Epoch: [48][130/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.1768e-01 (6.4084e-01)	Acc@1  82.81 ( 80.66)	Acc@5  98.44 ( 97.16)
Epoch: [48][140/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.0938e-01 (6.3840e-01)	Acc@1  75.78 ( 80.76)	Acc@5  99.22 ( 97.19)
Epoch: [48][150/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.7773e-01 (6.3901e-01)	Acc@1  78.12 ( 80.67)	Acc@5  97.66 ( 97.20)
Epoch: [48][160/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.4502e-01 (6.4029e-01)	Acc@1  78.91 ( 80.65)	Acc@5  96.09 ( 97.14)
Epoch: [48][170/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2363e-01 (6.4122e-01)	Acc@1  76.56 ( 80.56)	Acc@5  98.44 ( 97.14)
Epoch: [48][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0635e-01 (6.4209e-01)	Acc@1  85.94 ( 80.57)	Acc@5  99.22 ( 97.12)
Epoch: [48][190/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3857e-01 (6.4363e-01)	Acc@1  85.16 ( 80.55)	Acc@5  98.44 ( 97.07)
Epoch: [48][200/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5566e-01 (6.4498e-01)	Acc@1  82.03 ( 80.43)	Acc@5  96.88 ( 97.05)
Epoch: [48][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8315e-01 (6.4105e-01)	Acc@1  86.72 ( 80.58)	Acc@5  99.22 ( 97.08)
Epoch: [48][220/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.4277e-01 (6.4037e-01)	Acc@1  75.78 ( 80.61)	Acc@5  95.31 ( 97.10)
Epoch: [48][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3721e-01 (6.4133e-01)	Acc@1  78.12 ( 80.54)	Acc@5  98.44 ( 97.12)
Epoch: [48][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7637e-01 (6.4303e-01)	Acc@1  73.44 ( 80.47)	Acc@5  98.44 ( 97.12)
Epoch: [48][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5322e-01 (6.4155e-01)	Acc@1  84.38 ( 80.57)	Acc@5  97.66 ( 97.11)
Epoch: [48][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0947e-01 (6.4302e-01)	Acc@1  79.69 ( 80.55)	Acc@5  94.53 ( 97.10)
Epoch: [48][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3672e-01 (6.4418e-01)	Acc@1  81.25 ( 80.51)	Acc@5  96.88 ( 97.10)
Epoch: [48][280/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6934e-01 (6.4405e-01)	Acc@1  82.81 ( 80.51)	Acc@5  96.88 ( 97.09)
Epoch: [48][290/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2852e-01 (6.4438e-01)	Acc@1  76.56 ( 80.48)	Acc@5  95.31 ( 97.09)
Epoch: [48][300/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.3301e-01 (6.4488e-01)	Acc@1  75.78 ( 80.48)	Acc@5  94.53 ( 97.07)
Epoch: [48][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8164e-01 (6.4548e-01)	Acc@1  82.03 ( 80.47)	Acc@5  96.09 ( 97.04)
Epoch: [48][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3281e-01 (6.4569e-01)	Acc@1  80.47 ( 80.44)	Acc@5  96.09 ( 97.06)
Epoch: [48][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.0537e-01 (6.4431e-01)	Acc@1  85.94 ( 80.47)	Acc@5  98.44 ( 97.08)
Epoch: [48][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.7627e-01 (6.4428e-01)	Acc@1  77.34 ( 80.47)	Acc@5  96.09 ( 97.07)
Epoch: [48][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.3965e-01 (6.4371e-01)	Acc@1  82.03 ( 80.47)	Acc@5  96.88 ( 97.08)
Epoch: [48][360/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.1348e-01 (6.4477e-01)	Acc@1  75.00 ( 80.43)	Acc@5  95.31 ( 97.08)
Epoch: [48][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.6055e-01 (6.4562e-01)	Acc@1  84.38 ( 80.44)	Acc@5  96.09 ( 97.06)
Epoch: [48][380/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.9229e-01 (6.4594e-01)	Acc@1  82.81 ( 80.46)	Acc@5  97.66 ( 97.07)
Epoch: [48][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.7402e-01 (6.4652e-01)	Acc@1  72.50 ( 80.47)	Acc@5  97.50 ( 97.06)
## e[48] optimizer.zero_grad (sum) time: 0.4141688346862793
## e[48]       loss.backward (sum) time: 7.304551362991333
## e[48]      optimizer.step (sum) time: 3.5283725261688232
## epoch[48] training(only) time: 26.166613340377808
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.2012e+00 (1.2012e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.038)	Loss 1.2471e+00 (1.1959e+00)	Acc@1  70.00 ( 68.55)	Acc@5  90.00 ( 89.64)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5518e+00 (1.1882e+00)	Acc@1  66.00 ( 68.76)	Acc@5  90.00 ( 90.19)
Test: [ 30/100]	Time  0.034 ( 0.031)	Loss 1.5850e+00 (1.2234e+00)	Acc@1  57.00 ( 67.58)	Acc@5  84.00 ( 89.58)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.2676e+00 (1.2089e+00)	Acc@1  65.00 ( 67.20)	Acc@5  90.00 ( 90.05)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2529e+00 (1.2155e+00)	Acc@1  71.00 ( 67.06)	Acc@5  88.00 ( 90.20)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2861e+00 (1.2023e+00)	Acc@1  59.00 ( 67.08)	Acc@5  91.00 ( 90.34)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.7949e-01 (1.2040e+00)	Acc@1  73.00 ( 67.18)	Acc@5  94.00 ( 90.46)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2891e+00 (1.2130e+00)	Acc@1  64.00 ( 66.95)	Acc@5  88.00 ( 90.30)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7363e+00 (1.2054e+00)	Acc@1  58.00 ( 67.12)	Acc@5  86.00 ( 90.47)
 * Acc@1 67.300 Acc@5 90.540
### epoch[48] execution time: 28.999570846557617
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.217 ( 0.217)	Data  0.141 ( 0.141)	Loss 6.3281e-01 (6.3281e-01)	Acc@1  80.47 ( 80.47)	Acc@5  96.88 ( 96.88)
Epoch: [49][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.014)	Loss 7.4658e-01 (6.5798e-01)	Acc@1  79.69 ( 79.97)	Acc@5  94.53 ( 97.16)
Epoch: [49][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.0747e-01 (6.5552e-01)	Acc@1  86.72 ( 80.84)	Acc@5 100.00 ( 97.21)
Epoch: [49][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 7.2949e-01 (6.3939e-01)	Acc@1  76.56 ( 81.05)	Acc@5  96.88 ( 97.30)
Epoch: [49][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.4053e-01 (6.3084e-01)	Acc@1  88.28 ( 81.55)	Acc@5  98.44 ( 97.26)
Epoch: [49][ 50/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 6.6211e-01 (6.2942e-01)	Acc@1  77.34 ( 81.50)	Acc@5  99.22 ( 97.30)
Epoch: [49][ 60/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.0205e-01 (6.3174e-01)	Acc@1  80.47 ( 81.37)	Acc@5  96.09 ( 97.21)
Epoch: [49][ 70/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.6836e-01 (6.3710e-01)	Acc@1  84.38 ( 81.17)	Acc@5  96.88 ( 97.18)
Epoch: [49][ 80/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.8311e-01 (6.3817e-01)	Acc@1  83.59 ( 81.11)	Acc@5  94.53 ( 97.14)
Epoch: [49][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.0166e-01 (6.3139e-01)	Acc@1  81.25 ( 81.35)	Acc@5  92.97 ( 97.14)
Epoch: [49][100/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.9668e-01 (6.2884e-01)	Acc@1  82.81 ( 81.46)	Acc@5  98.44 ( 97.18)
Epoch: [49][110/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.7246e-01 (6.3100e-01)	Acc@1  75.78 ( 81.29)	Acc@5  95.31 ( 97.13)
Epoch: [49][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2109e-01 (6.2808e-01)	Acc@1  82.81 ( 81.39)	Acc@5  96.09 ( 97.17)
Epoch: [49][130/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9683e-01 (6.2835e-01)	Acc@1  82.81 ( 81.30)	Acc@5  98.44 ( 97.15)
Epoch: [49][140/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.0449e-01 (6.2504e-01)	Acc@1  78.91 ( 81.35)	Acc@5  96.09 ( 97.18)
Epoch: [49][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.7275e-01 (6.2616e-01)	Acc@1  79.69 ( 81.20)	Acc@5  99.22 ( 97.20)
Epoch: [49][160/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.5820e-01 (6.2775e-01)	Acc@1  79.69 ( 81.16)	Acc@5  96.09 ( 97.17)
Epoch: [49][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8301e-01 (6.2880e-01)	Acc@1  84.38 ( 81.11)	Acc@5  98.44 ( 97.20)
Epoch: [49][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2305e-01 (6.2938e-01)	Acc@1  80.47 ( 81.06)	Acc@5  96.88 ( 97.20)
Epoch: [49][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2783e-01 (6.2792e-01)	Acc@1  84.38 ( 81.09)	Acc@5  97.66 ( 97.23)
Epoch: [49][200/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1172e-01 (6.2741e-01)	Acc@1  85.16 ( 81.11)	Acc@5  99.22 ( 97.22)
Epoch: [49][210/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7969e-01 (6.2848e-01)	Acc@1  81.25 ( 81.11)	Acc@5  98.44 ( 97.22)
Epoch: [49][220/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2656e-01 (6.3058e-01)	Acc@1  78.91 ( 81.04)	Acc@5  95.31 ( 97.18)
Epoch: [49][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.2080e-01 (6.3251e-01)	Acc@1  78.12 ( 81.01)	Acc@5  93.75 ( 97.16)
Epoch: [49][240/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1719e-01 (6.3277e-01)	Acc@1  85.16 ( 81.00)	Acc@5  97.66 ( 97.16)
Epoch: [49][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (6.3312e-01)	Acc@1  82.81 ( 81.01)	Acc@5  97.66 ( 97.16)
Epoch: [49][260/391]	Time  0.070 ( 0.067)	Data  0.002 ( 0.002)	Loss 6.8213e-01 (6.3345e-01)	Acc@1  77.34 ( 80.99)	Acc@5  95.31 ( 97.15)
Epoch: [49][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6943e-01 (6.3335e-01)	Acc@1  79.69 ( 80.98)	Acc@5  96.88 ( 97.16)
Epoch: [49][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.6270e-01 (6.3498e-01)	Acc@1  77.34 ( 80.89)	Acc@5  95.31 ( 97.14)
Epoch: [49][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7480e-01 (6.3480e-01)	Acc@1  78.12 ( 80.92)	Acc@5  97.66 ( 97.14)
Epoch: [49][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8359e-01 (6.3599e-01)	Acc@1  77.34 ( 80.85)	Acc@5  96.09 ( 97.12)
Epoch: [49][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2881e-01 (6.3671e-01)	Acc@1  82.81 ( 80.81)	Acc@5  98.44 ( 97.13)
Epoch: [49][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7139e-01 (6.3759e-01)	Acc@1  75.00 ( 80.74)	Acc@5  96.09 ( 97.13)
Epoch: [49][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3633e-01 (6.3836e-01)	Acc@1  78.12 ( 80.72)	Acc@5  95.31 ( 97.13)
Epoch: [49][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0557e-01 (6.3888e-01)	Acc@1  82.03 ( 80.74)	Acc@5  95.31 ( 97.10)
Epoch: [49][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.7568e-01 (6.3999e-01)	Acc@1  82.03 ( 80.72)	Acc@5  96.88 ( 97.08)
Epoch: [49][360/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.6787e-01 (6.4092e-01)	Acc@1  84.38 ( 80.68)	Acc@5  96.09 ( 97.07)
Epoch: [49][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.9043e-01 (6.4148e-01)	Acc@1  79.69 ( 80.68)	Acc@5  96.09 ( 97.07)
Epoch: [49][380/391]	Time  0.076 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.9141e-01 (6.4150e-01)	Acc@1  77.34 ( 80.65)	Acc@5  94.53 ( 97.07)
Epoch: [49][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.5244e-01 (6.4149e-01)	Acc@1  81.25 ( 80.63)	Acc@5  93.75 ( 97.05)
## e[49] optimizer.zero_grad (sum) time: 0.4207735061645508
## e[49]       loss.backward (sum) time: 7.363646507263184
## e[49]      optimizer.step (sum) time: 3.5429162979125977
## epoch[49] training(only) time: 26.275734424591064
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2119e+00 (1.2119e+00)	Acc@1  64.00 ( 64.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.029 ( 0.038)	Loss 1.2217e+00 (1.2021e+00)	Acc@1  70.00 ( 67.91)	Acc@5  90.00 ( 90.09)
Test: [ 20/100]	Time  0.028 ( 0.032)	Loss 1.4658e+00 (1.1872e+00)	Acc@1  64.00 ( 68.67)	Acc@5  90.00 ( 90.48)
Test: [ 30/100]	Time  0.029 ( 0.030)	Loss 1.5674e+00 (1.2176e+00)	Acc@1  58.00 ( 67.03)	Acc@5  86.00 ( 90.03)
Test: [ 40/100]	Time  0.026 ( 0.029)	Loss 1.3027e+00 (1.2045e+00)	Acc@1  61.00 ( 66.88)	Acc@5  92.00 ( 90.56)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2500e+00 (1.2109e+00)	Acc@1  67.00 ( 66.88)	Acc@5  86.00 ( 90.57)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 1.3027e+00 (1.1979e+00)	Acc@1  64.00 ( 67.02)	Acc@5  90.00 ( 90.69)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0488e+00 (1.2018e+00)	Acc@1  75.00 ( 67.15)	Acc@5  93.00 ( 90.68)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2930e+00 (1.2097e+00)	Acc@1  63.00 ( 66.95)	Acc@5  90.00 ( 90.62)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7871e+00 (1.2049e+00)	Acc@1  56.00 ( 67.01)	Acc@5  86.00 ( 90.66)
 * Acc@1 67.260 Acc@5 90.720
### epoch[49] execution time: 29.10436725616455
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.231 ( 0.231)	Data  0.152 ( 0.152)	Loss 5.4736e-01 (5.4736e-01)	Acc@1  85.94 ( 85.94)	Acc@5  98.44 ( 98.44)
Epoch: [50][ 10/391]	Time  0.064 ( 0.082)	Data  0.001 ( 0.015)	Loss 5.2783e-01 (5.8203e-01)	Acc@1  83.59 ( 81.61)	Acc@5  98.44 ( 97.87)
Epoch: [50][ 20/391]	Time  0.067 ( 0.076)	Data  0.001 ( 0.008)	Loss 6.1426e-01 (5.9191e-01)	Acc@1  81.25 ( 81.36)	Acc@5  98.44 ( 97.58)
Epoch: [50][ 30/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.006)	Loss 6.7236e-01 (5.9788e-01)	Acc@1  81.25 ( 81.65)	Acc@5  96.88 ( 97.45)
Epoch: [50][ 40/391]	Time  0.070 ( 0.071)	Data  0.001 ( 0.005)	Loss 5.4688e-01 (6.0032e-01)	Acc@1  83.59 ( 81.97)	Acc@5  98.44 ( 97.35)
Epoch: [50][ 50/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.004)	Loss 7.0508e-01 (5.9636e-01)	Acc@1  82.03 ( 82.06)	Acc@5  94.53 ( 97.27)
Epoch: [50][ 60/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 6.1182e-01 (5.9703e-01)	Acc@1  79.69 ( 82.02)	Acc@5  96.09 ( 97.20)
Epoch: [50][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.0059e-01 (5.9939e-01)	Acc@1  82.03 ( 81.82)	Acc@5  96.88 ( 97.23)
Epoch: [50][ 80/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.7568e-01 (5.9715e-01)	Acc@1  82.81 ( 81.90)	Acc@5  97.66 ( 97.25)
Epoch: [50][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.2842e-01 (5.9858e-01)	Acc@1  81.25 ( 81.79)	Acc@5  96.88 ( 97.27)
Epoch: [50][100/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.7520e-01 (6.0136e-01)	Acc@1  82.03 ( 81.75)	Acc@5 100.00 ( 97.30)
Epoch: [50][110/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.7002e-01 (6.0473e-01)	Acc@1  75.00 ( 81.66)	Acc@5  96.09 ( 97.28)
Epoch: [50][120/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2939e-01 (6.1076e-01)	Acc@1  79.69 ( 81.44)	Acc@5  98.44 ( 97.29)
Epoch: [50][130/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.6455e-01 (6.1190e-01)	Acc@1  78.91 ( 81.43)	Acc@5  97.66 ( 97.27)
Epoch: [50][140/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7363e-01 (6.1174e-01)	Acc@1  87.50 ( 81.47)	Acc@5 100.00 ( 97.29)
Epoch: [50][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3857e-01 (6.1203e-01)	Acc@1  79.69 ( 81.44)	Acc@5  96.88 ( 97.30)
Epoch: [50][160/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8755e-01 (6.0939e-01)	Acc@1  85.94 ( 81.50)	Acc@5  96.88 ( 97.30)
Epoch: [50][170/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2109e-01 (6.1105e-01)	Acc@1  79.69 ( 81.43)	Acc@5  96.88 ( 97.28)
Epoch: [50][180/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9805e-01 (6.1162e-01)	Acc@1  82.03 ( 81.39)	Acc@5  97.66 ( 97.31)
Epoch: [50][190/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.7812e-01 (6.1210e-01)	Acc@1  83.59 ( 81.39)	Acc@5  98.44 ( 97.32)
Epoch: [50][200/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.2881e-01 (6.1311e-01)	Acc@1  83.59 ( 81.36)	Acc@5  99.22 ( 97.31)
Epoch: [50][210/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8643e-01 (6.1639e-01)	Acc@1  82.81 ( 81.28)	Acc@5  96.09 ( 97.25)
Epoch: [50][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9863e-01 (6.1729e-01)	Acc@1  78.91 ( 81.24)	Acc@5  98.44 ( 97.24)
Epoch: [50][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7227e-01 (6.1783e-01)	Acc@1  82.03 ( 81.22)	Acc@5  99.22 ( 97.25)
Epoch: [50][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4199e-01 (6.1746e-01)	Acc@1  84.38 ( 81.27)	Acc@5  96.88 ( 97.25)
Epoch: [50][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7920e-01 (6.1674e-01)	Acc@1  77.34 ( 81.28)	Acc@5  96.09 ( 97.26)
Epoch: [50][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.8379e-01 (6.1846e-01)	Acc@1  73.44 ( 81.23)	Acc@5  95.31 ( 97.25)
Epoch: [50][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7422e-01 (6.2053e-01)	Acc@1  83.59 ( 81.18)	Acc@5  97.66 ( 97.23)
Epoch: [50][280/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9336e-01 (6.2374e-01)	Acc@1  78.12 ( 81.06)	Acc@5  96.09 ( 97.19)
Epoch: [50][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4844e-01 (6.2194e-01)	Acc@1  79.69 ( 81.10)	Acc@5  98.44 ( 97.23)
Epoch: [50][300/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5918e-01 (6.2158e-01)	Acc@1  78.12 ( 81.13)	Acc@5  97.66 ( 97.23)
Epoch: [50][310/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8115e-01 (6.2247e-01)	Acc@1  78.91 ( 81.10)	Acc@5  98.44 ( 97.21)
Epoch: [50][320/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4014e-01 (6.2296e-01)	Acc@1  79.69 ( 81.11)	Acc@5  95.31 ( 97.20)
Epoch: [50][330/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.7148e-01 (6.2407e-01)	Acc@1  77.34 ( 81.11)	Acc@5  93.75 ( 97.17)
Epoch: [50][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6104e-01 (6.2315e-01)	Acc@1  85.16 ( 81.13)	Acc@5  96.88 ( 97.19)
Epoch: [50][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2549e-01 (6.2439e-01)	Acc@1  78.12 ( 81.08)	Acc@5  96.88 ( 97.18)
Epoch: [50][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9424e-01 (6.2369e-01)	Acc@1  80.47 ( 81.08)	Acc@5  99.22 ( 97.19)
Epoch: [50][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.8467e-01 (6.2538e-01)	Acc@1  77.34 ( 81.03)	Acc@5  95.31 ( 97.16)
Epoch: [50][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.8750e-01 (6.2593e-01)	Acc@1  78.91 ( 81.03)	Acc@5  97.66 ( 97.17)
Epoch: [50][390/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.8691e-01 (6.2715e-01)	Acc@1  78.75 ( 80.99)	Acc@5  98.75 ( 97.14)
## e[50] optimizer.zero_grad (sum) time: 0.41394710540771484
## e[50]       loss.backward (sum) time: 7.36522102355957
## e[50]      optimizer.step (sum) time: 3.6520540714263916
## epoch[50] training(only) time: 26.317124843597412
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 1.1523e+00 (1.1523e+00)	Acc@1  69.00 ( 69.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.026 ( 0.040)	Loss 1.1455e+00 (1.2059e+00)	Acc@1  72.00 ( 68.55)	Acc@5  92.00 ( 90.00)
Test: [ 20/100]	Time  0.026 ( 0.034)	Loss 1.5430e+00 (1.1985e+00)	Acc@1  62.00 ( 68.43)	Acc@5  89.00 ( 90.38)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6064e+00 (1.2367e+00)	Acc@1  58.00 ( 67.26)	Acc@5  88.00 ( 89.81)
Test: [ 40/100]	Time  0.029 ( 0.030)	Loss 1.2314e+00 (1.2197e+00)	Acc@1  69.00 ( 67.39)	Acc@5  88.00 ( 90.32)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1943e+00 (1.2209e+00)	Acc@1  71.00 ( 67.31)	Acc@5  89.00 ( 90.41)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2793e+00 (1.2063e+00)	Acc@1  65.00 ( 67.41)	Acc@5  90.00 ( 90.69)
Test: [ 70/100]	Time  0.027 ( 0.029)	Loss 9.4238e-01 (1.2061e+00)	Acc@1  76.00 ( 67.63)	Acc@5  94.00 ( 90.75)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.2246e+00 (1.2135e+00)	Acc@1  68.00 ( 67.36)	Acc@5  90.00 ( 90.53)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8408e+00 (1.2094e+00)	Acc@1  55.00 ( 67.32)	Acc@5  84.00 ( 90.57)
 * Acc@1 67.460 Acc@5 90.600
### epoch[50] execution time: 29.16721534729004
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.223 ( 0.223)	Data  0.146 ( 0.146)	Loss 5.1807e-01 (5.1807e-01)	Acc@1  87.50 ( 87.50)	Acc@5  96.88 ( 96.88)
Epoch: [51][ 10/391]	Time  0.068 ( 0.082)	Data  0.001 ( 0.014)	Loss 6.8311e-01 (5.8938e-01)	Acc@1  80.47 ( 82.67)	Acc@5  96.88 ( 96.95)
Epoch: [51][ 20/391]	Time  0.064 ( 0.075)	Data  0.001 ( 0.008)	Loss 7.5244e-01 (6.0525e-01)	Acc@1  79.69 ( 82.03)	Acc@5  94.53 ( 97.10)
Epoch: [51][ 30/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.006)	Loss 7.2266e-01 (6.0696e-01)	Acc@1  78.91 ( 81.93)	Acc@5  94.53 ( 97.13)
Epoch: [51][ 40/391]	Time  0.062 ( 0.071)	Data  0.001 ( 0.005)	Loss 6.3574e-01 (6.1170e-01)	Acc@1  80.47 ( 81.69)	Acc@5  96.88 ( 97.35)
Epoch: [51][ 50/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.004)	Loss 6.6211e-01 (6.1711e-01)	Acc@1  77.34 ( 81.50)	Acc@5  96.09 ( 97.23)
Epoch: [51][ 60/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.003)	Loss 5.3174e-01 (6.1392e-01)	Acc@1  82.03 ( 81.35)	Acc@5  98.44 ( 97.32)
Epoch: [51][ 70/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.6357e-01 (6.0560e-01)	Acc@1  77.34 ( 81.35)	Acc@5  99.22 ( 97.51)
Epoch: [51][ 80/391]	Time  0.074 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.5391e-01 (6.1065e-01)	Acc@1  79.69 ( 81.30)	Acc@5  93.75 ( 97.35)
Epoch: [51][ 90/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 7.8076e-01 (6.1585e-01)	Acc@1  76.56 ( 81.16)	Acc@5  96.09 ( 97.26)
Epoch: [51][100/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.002)	Loss 6.2939e-01 (6.1186e-01)	Acc@1  82.03 ( 81.22)	Acc@5  96.88 ( 97.33)
Epoch: [51][110/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.002)	Loss 5.7910e-01 (6.1027e-01)	Acc@1  84.38 ( 81.37)	Acc@5  98.44 ( 97.34)
Epoch: [51][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.6309e-01 (6.1035e-01)	Acc@1  78.12 ( 81.35)	Acc@5  98.44 ( 97.36)
Epoch: [51][130/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.6797e-01 (6.1108e-01)	Acc@1  76.56 ( 81.37)	Acc@5  99.22 ( 97.40)
Epoch: [51][140/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.9980e-01 (6.1536e-01)	Acc@1  78.12 ( 81.23)	Acc@5  94.53 ( 97.33)
Epoch: [51][150/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2646e-01 (6.1584e-01)	Acc@1  78.91 ( 81.14)	Acc@5  97.66 ( 97.38)
Epoch: [51][160/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5615e-01 (6.1588e-01)	Acc@1  81.25 ( 81.19)	Acc@5  96.09 ( 97.34)
Epoch: [51][170/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.3086e-01 (6.1636e-01)	Acc@1  81.25 ( 81.20)	Acc@5  97.66 ( 97.36)
Epoch: [51][180/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1855e-01 (6.1682e-01)	Acc@1  88.28 ( 81.21)	Acc@5  97.66 ( 97.35)
Epoch: [51][190/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.6455e-01 (6.1762e-01)	Acc@1  79.69 ( 81.17)	Acc@5  96.88 ( 97.35)
Epoch: [51][200/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.7764e-01 (6.1867e-01)	Acc@1  82.03 ( 81.16)	Acc@5  97.66 ( 97.36)
Epoch: [51][210/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3174e-01 (6.1875e-01)	Acc@1  85.16 ( 81.18)	Acc@5  97.66 ( 97.36)
Epoch: [51][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4443e-01 (6.1800e-01)	Acc@1  83.59 ( 81.14)	Acc@5  98.44 ( 97.38)
Epoch: [51][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9482e-01 (6.1826e-01)	Acc@1  79.69 ( 81.15)	Acc@5  96.88 ( 97.38)
Epoch: [51][240/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0635e-01 (6.1785e-01)	Acc@1  84.38 ( 81.12)	Acc@5  98.44 ( 97.40)
Epoch: [51][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9180e-01 (6.1625e-01)	Acc@1  82.03 ( 81.18)	Acc@5  97.66 ( 97.40)
Epoch: [51][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3726e-01 (6.1563e-01)	Acc@1  85.16 ( 81.22)	Acc@5  99.22 ( 97.37)
Epoch: [51][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0098e-01 (6.1356e-01)	Acc@1  84.38 ( 81.27)	Acc@5  99.22 ( 97.39)
Epoch: [51][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6592e-01 (6.1326e-01)	Acc@1  83.59 ( 81.34)	Acc@5  98.44 ( 97.38)
Epoch: [51][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1914e-01 (6.1377e-01)	Acc@1  81.25 ( 81.30)	Acc@5  96.88 ( 97.39)
Epoch: [51][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6484e-01 (6.1220e-01)	Acc@1  87.50 ( 81.35)	Acc@5 100.00 ( 97.41)
Epoch: [51][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8887e-01 (6.1378e-01)	Acc@1  80.47 ( 81.30)	Acc@5  98.44 ( 97.40)
Epoch: [51][320/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0938e-01 (6.1442e-01)	Acc@1  78.12 ( 81.29)	Acc@5  97.66 ( 97.39)
Epoch: [51][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1328e-01 (6.1433e-01)	Acc@1  82.03 ( 81.28)	Acc@5  96.88 ( 97.36)
Epoch: [51][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4941e-01 (6.1449e-01)	Acc@1  84.38 ( 81.27)	Acc@5  96.88 ( 97.36)
Epoch: [51][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3330e-01 (6.1375e-01)	Acc@1  78.12 ( 81.29)	Acc@5  98.44 ( 97.37)
Epoch: [51][360/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.3184e-01 (6.1417e-01)	Acc@1  78.91 ( 81.24)	Acc@5  97.66 ( 97.36)
Epoch: [51][370/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.4346e-01 (6.1540e-01)	Acc@1  82.81 ( 81.21)	Acc@5  99.22 ( 97.34)
Epoch: [51][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.4248e-01 (6.1593e-01)	Acc@1  82.81 ( 81.15)	Acc@5  97.66 ( 97.33)
Epoch: [51][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.001)	Loss 8.7646e-01 (6.1664e-01)	Acc@1  73.75 ( 81.12)	Acc@5  97.50 ( 97.32)
## e[51] optimizer.zero_grad (sum) time: 0.4087257385253906
## e[51]       loss.backward (sum) time: 7.3838255405426025
## e[51]      optimizer.step (sum) time: 3.593811511993408
## epoch[51] training(only) time: 26.27241325378418
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1816e+00 (1.1816e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.2129e+00 (1.1932e+00)	Acc@1  68.00 ( 69.00)	Acc@5  89.00 ( 90.00)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5732e+00 (1.1962e+00)	Acc@1  66.00 ( 68.57)	Acc@5  90.00 ( 90.38)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 1.7080e+00 (1.2362e+00)	Acc@1  56.00 ( 67.45)	Acc@5  89.00 ( 89.68)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.2197e+00 (1.2196e+00)	Acc@1  67.00 ( 67.24)	Acc@5  91.00 ( 90.22)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.2227e+00 (1.2262e+00)	Acc@1  67.00 ( 66.92)	Acc@5  89.00 ( 90.33)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2676e+00 (1.2152e+00)	Acc@1  65.00 ( 66.97)	Acc@5  90.00 ( 90.46)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0723e+00 (1.2176e+00)	Acc@1  68.00 ( 67.03)	Acc@5  92.00 ( 90.51)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2207e+00 (1.2258e+00)	Acc@1  68.00 ( 66.88)	Acc@5  90.00 ( 90.47)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8477e+00 (1.2183e+00)	Acc@1  56.00 ( 66.99)	Acc@5  86.00 ( 90.56)
 * Acc@1 67.080 Acc@5 90.640
### epoch[51] execution time: 29.091828107833862
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.228 ( 0.228)	Data  0.148 ( 0.148)	Loss 6.7725e-01 (6.7725e-01)	Acc@1  75.00 ( 75.00)	Acc@5  95.31 ( 95.31)
Epoch: [52][ 10/391]	Time  0.073 ( 0.083)	Data  0.001 ( 0.014)	Loss 7.2705e-01 (5.7253e-01)	Acc@1  82.03 ( 83.24)	Acc@5  95.31 ( 97.16)
Epoch: [52][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.008)	Loss 6.0547e-01 (5.8993e-01)	Acc@1  82.81 ( 82.81)	Acc@5  98.44 ( 97.28)
Epoch: [52][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.006)	Loss 6.7041e-01 (5.8361e-01)	Acc@1  77.34 ( 82.59)	Acc@5  98.44 ( 97.45)
Epoch: [52][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.005)	Loss 6.4160e-01 (5.8608e-01)	Acc@1  81.25 ( 82.64)	Acc@5  95.31 ( 97.45)
Epoch: [52][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 6.4697e-01 (5.8741e-01)	Acc@1  79.69 ( 82.49)	Acc@5  97.66 ( 97.37)
Epoch: [52][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.2490e-01 (5.8594e-01)	Acc@1  82.81 ( 82.44)	Acc@5  98.44 ( 97.37)
Epoch: [52][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.9570e-01 (5.8428e-01)	Acc@1  82.81 ( 82.56)	Acc@5  98.44 ( 97.45)
Epoch: [52][ 80/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.2012e-01 (5.9041e-01)	Acc@1  83.59 ( 82.46)	Acc@5  94.53 ( 97.30)
Epoch: [52][ 90/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.7715e-01 (5.9117e-01)	Acc@1  82.81 ( 82.49)	Acc@5  96.09 ( 97.26)
Epoch: [52][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.7373e-01 (5.9360e-01)	Acc@1  82.81 ( 82.33)	Acc@5  97.66 ( 97.28)
Epoch: [52][110/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.6270e-01 (5.9355e-01)	Acc@1  72.66 ( 82.29)	Acc@5  95.31 ( 97.32)
Epoch: [52][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3662e-01 (5.9236e-01)	Acc@1  82.81 ( 82.30)	Acc@5 100.00 ( 97.32)
Epoch: [52][130/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.4746e-01 (5.9292e-01)	Acc@1  81.25 ( 82.26)	Acc@5  98.44 ( 97.32)
Epoch: [52][140/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7559e-01 (5.9083e-01)	Acc@1  84.38 ( 82.32)	Acc@5  99.22 ( 97.40)
Epoch: [52][150/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0010e-01 (5.9074e-01)	Acc@1  84.38 ( 82.28)	Acc@5  96.88 ( 97.44)
Epoch: [52][160/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4834e-01 (5.8952e-01)	Acc@1  82.03 ( 82.28)	Acc@5 100.00 ( 97.48)
Epoch: [52][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8594e-01 (5.9143e-01)	Acc@1  81.25 ( 82.24)	Acc@5  98.44 ( 97.46)
Epoch: [52][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2207e-01 (5.9010e-01)	Acc@1  82.81 ( 82.28)	Acc@5  96.88 ( 97.50)
Epoch: [52][190/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8887e-01 (5.9055e-01)	Acc@1  82.81 ( 82.26)	Acc@5  96.88 ( 97.52)
Epoch: [52][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2998e-01 (5.9132e-01)	Acc@1  77.34 ( 82.22)	Acc@5  97.66 ( 97.52)
Epoch: [52][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0547e-01 (5.9112e-01)	Acc@1  79.69 ( 82.11)	Acc@5  96.88 ( 97.55)
Epoch: [52][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5137e-01 (5.9182e-01)	Acc@1  81.25 ( 82.07)	Acc@5  95.31 ( 97.51)
Epoch: [52][230/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1709e-01 (5.9163e-01)	Acc@1  86.72 ( 82.08)	Acc@5  99.22 ( 97.54)
Epoch: [52][240/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3125e-01 (5.9165e-01)	Acc@1  82.03 ( 82.07)	Acc@5  99.22 ( 97.57)
Epoch: [52][250/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6738e-01 (5.9331e-01)	Acc@1  83.59 ( 82.01)	Acc@5  98.44 ( 97.56)
Epoch: [52][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5771e-01 (5.9501e-01)	Acc@1  82.81 ( 81.99)	Acc@5  96.88 ( 97.50)
Epoch: [52][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1680e-01 (5.9464e-01)	Acc@1  79.69 ( 82.02)	Acc@5  94.53 ( 97.51)
Epoch: [52][280/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5469e-01 (5.9569e-01)	Acc@1  80.47 ( 81.98)	Acc@5  97.66 ( 97.49)
Epoch: [52][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6748e-01 (5.9434e-01)	Acc@1  78.12 ( 81.99)	Acc@5  96.09 ( 97.50)
Epoch: [52][300/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3584e-01 (5.9548e-01)	Acc@1  74.22 ( 81.93)	Acc@5  95.31 ( 97.50)
Epoch: [52][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7812e-01 (5.9719e-01)	Acc@1  79.69 ( 81.88)	Acc@5  97.66 ( 97.48)
Epoch: [52][320/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2900e-01 (5.9960e-01)	Acc@1  80.47 ( 81.84)	Acc@5  97.66 ( 97.44)
Epoch: [52][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9717e-01 (5.9908e-01)	Acc@1  84.38 ( 81.90)	Acc@5  96.88 ( 97.45)
Epoch: [52][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2852e-01 (5.9975e-01)	Acc@1  74.22 ( 81.84)	Acc@5  96.09 ( 97.45)
Epoch: [52][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1777e-01 (6.0140e-01)	Acc@1  81.25 ( 81.81)	Acc@5  95.31 ( 97.42)
Epoch: [52][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8389e-01 (6.0202e-01)	Acc@1  85.94 ( 81.77)	Acc@5 100.00 ( 97.43)
Epoch: [52][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.9082e-01 (6.0317e-01)	Acc@1  83.59 ( 81.73)	Acc@5  97.66 ( 97.43)
Epoch: [52][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.5898e-01 (6.0290e-01)	Acc@1  86.72 ( 81.74)	Acc@5  98.44 ( 97.43)
Epoch: [52][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.8145e-01 (6.0422e-01)	Acc@1  86.25 ( 81.69)	Acc@5  98.75 ( 97.41)
## e[52] optimizer.zero_grad (sum) time: 0.41653895378112793
## e[52]       loss.backward (sum) time: 7.394554138183594
## e[52]      optimizer.step (sum) time: 3.5731868743896484
## epoch[52] training(only) time: 26.221380472183228
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.3477e+00 (1.3477e+00)	Acc@1  65.00 ( 65.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.2148e+00 (1.2345e+00)	Acc@1  70.00 ( 68.27)	Acc@5  91.00 ( 89.55)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.5410e+00 (1.2140e+00)	Acc@1  64.00 ( 68.00)	Acc@5  89.00 ( 90.33)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7168e+00 (1.2489e+00)	Acc@1  53.00 ( 66.90)	Acc@5  87.00 ( 89.71)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.3359e+00 (1.2318e+00)	Acc@1  63.00 ( 66.68)	Acc@5  91.00 ( 90.22)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2207e+00 (1.2348e+00)	Acc@1  66.00 ( 66.61)	Acc@5  89.00 ( 90.20)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.3330e+00 (1.2267e+00)	Acc@1  63.00 ( 66.77)	Acc@5  91.00 ( 90.36)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 9.6924e-01 (1.2310e+00)	Acc@1  76.00 ( 66.76)	Acc@5  95.00 ( 90.41)
Test: [ 80/100]	Time  0.033 ( 0.028)	Loss 1.2373e+00 (1.2371e+00)	Acc@1  66.00 ( 66.58)	Acc@5  90.00 ( 90.40)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 1.7412e+00 (1.2296e+00)	Acc@1  55.00 ( 66.62)	Acc@5  86.00 ( 90.56)
 * Acc@1 66.900 Acc@5 90.590
### epoch[52] execution time: 29.025551080703735
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.229 ( 0.229)	Data  0.153 ( 0.153)	Loss 5.3076e-01 (5.3076e-01)	Acc@1  85.94 ( 85.94)	Acc@5  96.09 ( 96.09)
Epoch: [53][ 10/391]	Time  0.069 ( 0.080)	Data  0.001 ( 0.015)	Loss 5.6006e-01 (5.3023e-01)	Acc@1  84.38 ( 84.94)	Acc@5  98.44 ( 98.15)
Epoch: [53][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.8193e-01 (5.5959e-01)	Acc@1  83.59 ( 83.78)	Acc@5 100.00 ( 97.88)
Epoch: [53][ 30/391]	Time  0.062 ( 0.072)	Data  0.001 ( 0.006)	Loss 5.3809e-01 (5.6649e-01)	Acc@1  85.16 ( 83.47)	Acc@5  96.88 ( 97.71)
Epoch: [53][ 40/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.005)	Loss 5.2002e-01 (5.6854e-01)	Acc@1  85.16 ( 83.10)	Acc@5  98.44 ( 97.68)
Epoch: [53][ 50/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.2686e-01 (5.7500e-01)	Acc@1  80.47 ( 82.87)	Acc@5  99.22 ( 97.73)
Epoch: [53][ 60/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.1953e-01 (5.7935e-01)	Acc@1  80.47 ( 82.74)	Acc@5  98.44 ( 97.64)
Epoch: [53][ 70/391]	Time  0.072 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.7656e-01 (5.7855e-01)	Acc@1  83.59 ( 82.74)	Acc@5 100.00 ( 97.66)
Epoch: [53][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.8511e-01 (5.7956e-01)	Acc@1  85.16 ( 82.68)	Acc@5  99.22 ( 97.63)
Epoch: [53][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.6406e-01 (5.8258e-01)	Acc@1  81.25 ( 82.58)	Acc@5  96.09 ( 97.60)
Epoch: [53][100/391]	Time  0.076 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.8838e-01 (5.8660e-01)	Acc@1  82.03 ( 82.46)	Acc@5  97.66 ( 97.59)
Epoch: [53][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7827e-01 (5.8763e-01)	Acc@1  89.06 ( 82.38)	Acc@5  97.66 ( 97.54)
Epoch: [53][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.3525e-01 (5.8743e-01)	Acc@1  82.03 ( 82.46)	Acc@5  97.66 ( 97.55)
Epoch: [53][130/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7607e-01 (5.8959e-01)	Acc@1  87.50 ( 82.32)	Acc@5  98.44 ( 97.55)
Epoch: [53][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.6348e-01 (5.9140e-01)	Acc@1  84.38 ( 82.32)	Acc@5  97.66 ( 97.49)
Epoch: [53][150/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.4053e-01 (5.9112e-01)	Acc@1  80.47 ( 82.25)	Acc@5  96.88 ( 97.52)
Epoch: [53][160/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.1143e-01 (5.9319e-01)	Acc@1  76.56 ( 82.15)	Acc@5  96.09 ( 97.52)
Epoch: [53][170/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2451e-01 (5.9017e-01)	Acc@1  78.91 ( 82.18)	Acc@5  96.88 ( 97.56)
Epoch: [53][180/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.5674e-01 (5.9170e-01)	Acc@1  84.38 ( 82.18)	Acc@5  94.53 ( 97.54)
Epoch: [53][190/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.8164e-01 (5.9131e-01)	Acc@1  79.69 ( 82.16)	Acc@5  96.09 ( 97.53)
Epoch: [53][200/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.7246e-01 (5.9178e-01)	Acc@1  78.12 ( 82.18)	Acc@5  94.53 ( 97.52)
Epoch: [53][210/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0684e-01 (5.9342e-01)	Acc@1  85.94 ( 82.10)	Acc@5  98.44 ( 97.47)
Epoch: [53][220/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9766e-01 (5.9468e-01)	Acc@1  78.12 ( 82.00)	Acc@5  97.66 ( 97.48)
Epoch: [53][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9619e-01 (5.9552e-01)	Acc@1  81.25 ( 81.94)	Acc@5  98.44 ( 97.46)
Epoch: [53][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7031e-01 (5.9634e-01)	Acc@1  85.16 ( 81.94)	Acc@5  96.09 ( 97.45)
Epoch: [53][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8447e-01 (5.9599e-01)	Acc@1  82.81 ( 81.96)	Acc@5  97.66 ( 97.45)
Epoch: [53][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9707e-01 (5.9569e-01)	Acc@1  84.38 ( 81.94)	Acc@5  99.22 ( 97.46)
Epoch: [53][270/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6738e-01 (5.9603e-01)	Acc@1  81.25 ( 81.92)	Acc@5  98.44 ( 97.47)
Epoch: [53][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7676e-01 (5.9702e-01)	Acc@1  79.69 ( 81.87)	Acc@5  96.88 ( 97.46)
Epoch: [53][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6348e-01 (5.9654e-01)	Acc@1  80.47 ( 81.89)	Acc@5  98.44 ( 97.47)
Epoch: [53][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7373e-01 (5.9682e-01)	Acc@1  84.38 ( 81.88)	Acc@5  96.88 ( 97.46)
Epoch: [53][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8545e-01 (5.9643e-01)	Acc@1  82.81 ( 81.92)	Acc@5  98.44 ( 97.47)
Epoch: [53][320/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0938e-01 (5.9735e-01)	Acc@1  81.25 ( 81.91)	Acc@5  97.66 ( 97.47)
Epoch: [53][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2695e-01 (5.9947e-01)	Acc@1  79.69 ( 81.84)	Acc@5  96.88 ( 97.45)
Epoch: [53][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4209e-01 (6.0007e-01)	Acc@1  81.25 ( 81.83)	Acc@5  96.88 ( 97.44)
Epoch: [53][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (6.0047e-01)	Acc@1  79.69 ( 81.80)	Acc@5  96.88 ( 97.43)
Epoch: [53][360/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9033e-01 (6.0212e-01)	Acc@1  81.25 ( 81.72)	Acc@5  96.09 ( 97.42)
Epoch: [53][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2012e-01 (6.0335e-01)	Acc@1  82.03 ( 81.68)	Acc@5  96.88 ( 97.40)
Epoch: [53][380/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8203e-01 (6.0399e-01)	Acc@1  83.59 ( 81.66)	Acc@5  96.09 ( 97.38)
Epoch: [53][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1855e-01 (6.0329e-01)	Acc@1  88.75 ( 81.68)	Acc@5  97.50 ( 97.39)
## e[53] optimizer.zero_grad (sum) time: 0.40421295166015625
## e[53]       loss.backward (sum) time: 7.400623798370361
## e[53]      optimizer.step (sum) time: 3.5988008975982666
## epoch[53] training(only) time: 26.237630605697632
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2217e+00 (1.2217e+00)	Acc@1  65.00 ( 65.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1533e+00 (1.2396e+00)	Acc@1  68.00 ( 67.18)	Acc@5  90.00 ( 90.00)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.5811e+00 (1.2297e+00)	Acc@1  62.00 ( 67.57)	Acc@5  89.00 ( 90.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5996e+00 (1.2494e+00)	Acc@1  57.00 ( 67.03)	Acc@5  87.00 ( 89.55)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.3262e+00 (1.2331e+00)	Acc@1  63.00 ( 67.00)	Acc@5  92.00 ( 89.95)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.1914e+00 (1.2376e+00)	Acc@1  70.00 ( 66.80)	Acc@5  89.00 ( 89.94)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.3252e+00 (1.2265e+00)	Acc@1  65.00 ( 66.85)	Acc@5  92.00 ( 90.25)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0273e+00 (1.2275e+00)	Acc@1  68.00 ( 67.00)	Acc@5  95.00 ( 90.37)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3018e+00 (1.2316e+00)	Acc@1  65.00 ( 66.78)	Acc@5  89.00 ( 90.35)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8232e+00 (1.2225e+00)	Acc@1  58.00 ( 66.90)	Acc@5  84.00 ( 90.42)
 * Acc@1 67.100 Acc@5 90.500
### epoch[53] execution time: 29.035731315612793
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.218 ( 0.218)	Data  0.122 ( 0.122)	Loss 5.7080e-01 (5.7080e-01)	Acc@1  79.69 ( 79.69)	Acc@5  96.88 ( 96.88)
Epoch: [54][ 10/391]	Time  0.068 ( 0.080)	Data  0.001 ( 0.012)	Loss 6.0596e-01 (5.8159e-01)	Acc@1  82.81 ( 82.88)	Acc@5  98.44 ( 97.16)
Epoch: [54][ 20/391]	Time  0.069 ( 0.074)	Data  0.001 ( 0.007)	Loss 6.9824e-01 (5.7259e-01)	Acc@1  77.34 ( 83.11)	Acc@5  96.09 ( 97.32)
Epoch: [54][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.005)	Loss 5.5762e-01 (5.7803e-01)	Acc@1  83.59 ( 82.61)	Acc@5  96.88 ( 97.53)
Epoch: [54][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 6.3135e-01 (5.7296e-01)	Acc@1  83.59 ( 82.89)	Acc@5  98.44 ( 97.56)
Epoch: [54][ 50/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 6.1230e-01 (5.6960e-01)	Acc@1  82.81 ( 82.80)	Acc@5  96.09 ( 97.59)
Epoch: [54][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.0547e-01 (5.6902e-01)	Acc@1  79.69 ( 82.72)	Acc@5  99.22 ( 97.67)
Epoch: [54][ 70/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.7080e-01 (5.7667e-01)	Acc@1  79.69 ( 82.44)	Acc@5  99.22 ( 97.66)
Epoch: [54][ 80/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.5029e-01 (5.7623e-01)	Acc@1  82.81 ( 82.53)	Acc@5  99.22 ( 97.65)
Epoch: [54][ 90/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.0010e-01 (5.7259e-01)	Acc@1  79.69 ( 82.60)	Acc@5  98.44 ( 97.72)
Epoch: [54][100/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.4395e-01 (5.7357e-01)	Acc@1  83.59 ( 82.60)	Acc@5  97.66 ( 97.69)
Epoch: [54][110/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.8252e-01 (5.7519e-01)	Acc@1  79.69 ( 82.50)	Acc@5  97.66 ( 97.66)
Epoch: [54][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.4697e-01 (5.7946e-01)	Acc@1  81.25 ( 82.46)	Acc@5  96.09 ( 97.63)
Epoch: [54][130/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.4209e-01 (5.7912e-01)	Acc@1  77.34 ( 82.44)	Acc@5  95.31 ( 97.64)
Epoch: [54][140/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.0264e-01 (5.8140e-01)	Acc@1  80.47 ( 82.40)	Acc@5  95.31 ( 97.60)
Epoch: [54][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.3389e-01 (5.8346e-01)	Acc@1  77.34 ( 82.28)	Acc@5  95.31 ( 97.60)
Epoch: [54][160/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.1475e-01 (5.8328e-01)	Acc@1  82.03 ( 82.26)	Acc@5  97.66 ( 97.59)
Epoch: [54][170/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.4541e-01 (5.8145e-01)	Acc@1  85.16 ( 82.33)	Acc@5  98.44 ( 97.63)
Epoch: [54][180/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.3428e-01 (5.8253e-01)	Acc@1  80.47 ( 82.31)	Acc@5  96.88 ( 97.63)
Epoch: [54][190/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.4736e-01 (5.8108e-01)	Acc@1  79.69 ( 82.32)	Acc@5  99.22 ( 97.62)
Epoch: [54][200/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.6934e-01 (5.7930e-01)	Acc@1  83.59 ( 82.38)	Acc@5  97.66 ( 97.63)
Epoch: [54][210/391]	Time  0.077 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3027e-01 (5.8199e-01)	Acc@1  84.38 ( 82.29)	Acc@5  97.66 ( 97.60)
Epoch: [54][220/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.6162e-01 (5.8351e-01)	Acc@1  79.69 ( 82.24)	Acc@5  95.31 ( 97.56)
Epoch: [54][230/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.7773e-01 (5.8378e-01)	Acc@1  77.34 ( 82.21)	Acc@5  96.88 ( 97.55)
Epoch: [54][240/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.4395e-01 (5.8419e-01)	Acc@1  83.59 ( 82.18)	Acc@5  96.09 ( 97.55)
Epoch: [54][250/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.2061e-01 (5.8665e-01)	Acc@1  82.81 ( 82.09)	Acc@5  97.66 ( 97.52)
Epoch: [54][260/391]	Time  0.064 ( 0.068)	Data  0.002 ( 0.002)	Loss 5.3955e-01 (5.8809e-01)	Acc@1  84.38 ( 82.06)	Acc@5  96.09 ( 97.50)
Epoch: [54][270/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8584e-01 (5.8969e-01)	Acc@1  85.94 ( 82.02)	Acc@5  97.66 ( 97.49)
Epoch: [54][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8984e-01 (5.8949e-01)	Acc@1  82.81 ( 82.04)	Acc@5  97.66 ( 97.50)
Epoch: [54][290/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1123e-01 (5.8947e-01)	Acc@1  84.38 ( 82.07)	Acc@5  99.22 ( 97.49)
Epoch: [54][300/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.6152e-01 (5.9053e-01)	Acc@1  84.38 ( 82.03)	Acc@5  97.66 ( 97.50)
Epoch: [54][310/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (5.9060e-01)	Acc@1  82.81 ( 82.00)	Acc@5  97.66 ( 97.50)
Epoch: [54][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.8350e-01 (5.9168e-01)	Acc@1  81.25 ( 81.94)	Acc@5  96.88 ( 97.50)
Epoch: [54][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.6299e-01 (5.9218e-01)	Acc@1  79.69 ( 81.88)	Acc@5  99.22 ( 97.50)
Epoch: [54][340/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.6748e-01 (5.9267e-01)	Acc@1  81.25 ( 81.86)	Acc@5  96.88 ( 97.48)
Epoch: [54][350/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.7529e-01 (5.9317e-01)	Acc@1  81.25 ( 81.84)	Acc@5  96.88 ( 97.48)
Epoch: [54][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.7627e-01 (5.9289e-01)	Acc@1  79.69 ( 81.82)	Acc@5  95.31 ( 97.48)
Epoch: [54][370/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.4434e-01 (5.9269e-01)	Acc@1  86.72 ( 81.80)	Acc@5 100.00 ( 97.48)
Epoch: [54][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.3955e-01 (5.9203e-01)	Acc@1  85.94 ( 81.83)	Acc@5  96.88 ( 97.49)
Epoch: [54][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.2168e-01 (5.9276e-01)	Acc@1  76.25 ( 81.81)	Acc@5  97.50 ( 97.49)
## e[54] optimizer.zero_grad (sum) time: 0.4090137481689453
## e[54]       loss.backward (sum) time: 7.403126001358032
## e[54]      optimizer.step (sum) time: 3.6311235427856445
## epoch[54] training(only) time: 26.369421005249023
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.1807e+00 (1.1807e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.2549e+00 (1.2514e+00)	Acc@1  69.00 ( 67.55)	Acc@5  91.00 ( 89.45)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5410e+00 (1.2348e+00)	Acc@1  63.00 ( 68.05)	Acc@5  90.00 ( 90.19)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6270e+00 (1.2671e+00)	Acc@1  58.00 ( 67.03)	Acc@5  84.00 ( 89.55)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2949e+00 (1.2419e+00)	Acc@1  63.00 ( 67.02)	Acc@5  89.00 ( 89.98)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.1650e+00 (1.2399e+00)	Acc@1  66.00 ( 66.92)	Acc@5  90.00 ( 90.25)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3896e+00 (1.2299e+00)	Acc@1  64.00 ( 67.00)	Acc@5  90.00 ( 90.43)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0010e+00 (1.2295e+00)	Acc@1  73.00 ( 67.27)	Acc@5  94.00 ( 90.45)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.2871e+00 (1.2368e+00)	Acc@1  65.00 ( 66.88)	Acc@5  87.00 ( 90.32)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8291e+00 (1.2283e+00)	Acc@1  55.00 ( 66.81)	Acc@5  84.00 ( 90.36)
 * Acc@1 67.060 Acc@5 90.380
### epoch[54] execution time: 29.216066122055054
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.216 ( 0.216)	Data  0.137 ( 0.137)	Loss 4.9414e-01 (4.9414e-01)	Acc@1  89.06 ( 89.06)	Acc@5  96.88 ( 96.88)
Epoch: [55][ 10/391]	Time  0.069 ( 0.079)	Data  0.001 ( 0.013)	Loss 4.3604e-01 (5.2393e-01)	Acc@1  86.72 ( 83.95)	Acc@5 100.00 ( 98.22)
Epoch: [55][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 6.2549e-01 (5.5711e-01)	Acc@1  82.03 ( 82.51)	Acc@5  96.09 ( 97.92)
Epoch: [55][ 30/391]	Time  0.069 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.1235e-01 (5.5444e-01)	Acc@1  86.72 ( 82.79)	Acc@5 100.00 ( 97.86)
Epoch: [55][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 6.3916e-01 (5.5967e-01)	Acc@1  78.91 ( 82.58)	Acc@5  97.66 ( 97.88)
Epoch: [55][ 50/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.8496e-01 (5.5323e-01)	Acc@1  84.38 ( 83.10)	Acc@5  96.88 ( 97.89)
Epoch: [55][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.7827e-01 (5.5114e-01)	Acc@1  86.72 ( 83.09)	Acc@5  97.66 ( 97.95)
Epoch: [55][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.5078e-01 (5.5618e-01)	Acc@1  78.91 ( 82.98)	Acc@5 100.00 ( 97.91)
Epoch: [55][ 80/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.3223e-01 (5.6333e-01)	Acc@1  84.38 ( 82.80)	Acc@5  96.88 ( 97.83)
Epoch: [55][ 90/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.4590e-01 (5.6400e-01)	Acc@1  83.59 ( 82.71)	Acc@5  98.44 ( 97.84)
Epoch: [55][100/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 7.5000e-01 (5.6876e-01)	Acc@1  76.56 ( 82.50)	Acc@5  98.44 ( 97.83)
Epoch: [55][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1475e-01 (5.7063e-01)	Acc@1  82.81 ( 82.40)	Acc@5  96.88 ( 97.85)
Epoch: [55][120/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3809e-01 (5.6809e-01)	Acc@1  80.47 ( 82.52)	Acc@5  97.66 ( 97.86)
Epoch: [55][130/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4946e-01 (5.6995e-01)	Acc@1  87.50 ( 82.48)	Acc@5  99.22 ( 97.86)
Epoch: [55][140/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6689e-01 (5.6990e-01)	Acc@1  82.03 ( 82.49)	Acc@5  98.44 ( 97.83)
Epoch: [55][150/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2344e-01 (5.7324e-01)	Acc@1  86.72 ( 82.44)	Acc@5  97.66 ( 97.79)
Epoch: [55][160/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8193e-01 (5.7402e-01)	Acc@1  87.50 ( 82.49)	Acc@5  98.44 ( 97.76)
Epoch: [55][170/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2842e-01 (5.7534e-01)	Acc@1  76.56 ( 82.49)	Acc@5  97.66 ( 97.73)
Epoch: [55][180/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1572e-01 (5.7432e-01)	Acc@1  78.91 ( 82.53)	Acc@5  97.66 ( 97.74)
Epoch: [55][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.5039e-01 (5.7242e-01)	Acc@1  79.69 ( 82.60)	Acc@5  96.09 ( 97.72)
Epoch: [55][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1963e-01 (5.7405e-01)	Acc@1  79.69 ( 82.55)	Acc@5  97.66 ( 97.70)
Epoch: [55][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8203e-01 (5.7638e-01)	Acc@1  80.47 ( 82.49)	Acc@5  96.09 ( 97.67)
Epoch: [55][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6592e-01 (5.7553e-01)	Acc@1  81.25 ( 82.54)	Acc@5  97.66 ( 97.65)
Epoch: [55][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3662e-01 (5.7384e-01)	Acc@1  82.81 ( 82.59)	Acc@5  97.66 ( 97.68)
Epoch: [55][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3906e-01 (5.7487e-01)	Acc@1  85.16 ( 82.55)	Acc@5  94.53 ( 97.65)
Epoch: [55][250/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8975e-01 (5.7430e-01)	Acc@1  85.94 ( 82.50)	Acc@5  96.88 ( 97.67)
Epoch: [55][260/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5957e-01 (5.7479e-01)	Acc@1  79.69 ( 82.44)	Acc@5  99.22 ( 97.67)
Epoch: [55][270/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5420e-01 (5.7516e-01)	Acc@1  85.16 ( 82.42)	Acc@5  97.66 ( 97.67)
Epoch: [55][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8047e-01 (5.7541e-01)	Acc@1  84.38 ( 82.40)	Acc@5  99.22 ( 97.68)
Epoch: [55][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0342e-01 (5.7579e-01)	Acc@1  87.50 ( 82.37)	Acc@5  98.44 ( 97.68)
Epoch: [55][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2988e-01 (5.7615e-01)	Acc@1  81.25 ( 82.36)	Acc@5  98.44 ( 97.68)
Epoch: [55][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7383e-01 (5.7760e-01)	Acc@1  75.00 ( 82.30)	Acc@5  95.31 ( 97.65)
Epoch: [55][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8789e-01 (5.7849e-01)	Acc@1  82.03 ( 82.30)	Acc@5  97.66 ( 97.64)
Epoch: [55][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9531e-01 (5.7860e-01)	Acc@1  79.69 ( 82.33)	Acc@5  96.88 ( 97.64)
Epoch: [55][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.3574e-01 (5.7970e-01)	Acc@1  82.03 ( 82.26)	Acc@5  95.31 ( 97.64)
Epoch: [55][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.0303e-01 (5.8069e-01)	Acc@1  81.25 ( 82.25)	Acc@5  97.66 ( 97.61)
Epoch: [55][360/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.4697e-01 (5.8061e-01)	Acc@1  81.25 ( 82.24)	Acc@5  96.09 ( 97.60)
Epoch: [55][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.2656e-01 (5.8122e-01)	Acc@1  77.34 ( 82.21)	Acc@5  95.31 ( 97.58)
Epoch: [55][380/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.3174e-01 (5.8194e-01)	Acc@1  85.16 ( 82.17)	Acc@5  96.88 ( 97.57)
Epoch: [55][390/391]	Time  0.055 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.2256e-01 (5.8118e-01)	Acc@1  78.75 ( 82.20)	Acc@5  97.50 ( 97.57)
## e[55] optimizer.zero_grad (sum) time: 0.41448450088500977
## e[55]       loss.backward (sum) time: 7.386786937713623
## e[55]      optimizer.step (sum) time: 3.520490884780884
## epoch[55] training(only) time: 26.1615891456604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.2402e+00 (1.2402e+00)	Acc@1  65.00 ( 65.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.2041e+00 (1.2437e+00)	Acc@1  68.00 ( 67.82)	Acc@5  91.00 ( 89.45)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5635e+00 (1.2386e+00)	Acc@1  62.00 ( 68.19)	Acc@5  88.00 ( 90.14)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6602e+00 (1.2648e+00)	Acc@1  57.00 ( 67.39)	Acc@5  88.00 ( 89.90)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1348e+00 (1.2353e+00)	Acc@1  70.00 ( 67.27)	Acc@5  92.00 ( 90.39)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2773e+00 (1.2445e+00)	Acc@1  66.00 ( 66.94)	Acc@5  90.00 ( 90.39)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.4043e+00 (1.2358e+00)	Acc@1  61.00 ( 66.84)	Acc@5  89.00 ( 90.49)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0557e+00 (1.2324e+00)	Acc@1  70.00 ( 67.04)	Acc@5  92.00 ( 90.59)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3477e+00 (1.2417e+00)	Acc@1  65.00 ( 66.68)	Acc@5  84.00 ( 90.32)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8340e+00 (1.2357e+00)	Acc@1  53.00 ( 66.64)	Acc@5  84.00 ( 90.35)
 * Acc@1 66.910 Acc@5 90.370
### epoch[55] execution time: 28.9952712059021
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.227 ( 0.227)	Data  0.152 ( 0.152)	Loss 6.3330e-01 (6.3330e-01)	Acc@1  80.47 ( 80.47)	Acc@5  98.44 ( 98.44)
Epoch: [56][ 10/391]	Time  0.069 ( 0.082)	Data  0.001 ( 0.015)	Loss 6.9482e-01 (6.0516e-01)	Acc@1  78.12 ( 80.82)	Acc@5  96.88 ( 97.94)
Epoch: [56][ 20/391]	Time  0.065 ( 0.074)	Data  0.001 ( 0.008)	Loss 6.2549e-01 (5.5900e-01)	Acc@1  80.47 ( 82.74)	Acc@5  96.88 ( 97.92)
Epoch: [56][ 30/391]	Time  0.071 ( 0.072)	Data  0.001 ( 0.006)	Loss 6.0303e-01 (5.5485e-01)	Acc@1  79.69 ( 82.89)	Acc@5  96.88 ( 97.73)
Epoch: [56][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.8804e-01 (5.5793e-01)	Acc@1  86.72 ( 83.04)	Acc@5  98.44 ( 97.81)
Epoch: [56][ 50/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.004)	Loss 7.0117e-01 (5.5904e-01)	Acc@1  74.22 ( 83.06)	Acc@5  95.31 ( 97.76)
Epoch: [56][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.6836e-01 (5.5402e-01)	Acc@1  85.16 ( 83.32)	Acc@5  99.22 ( 97.93)
Epoch: [56][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.3125e-01 (5.5789e-01)	Acc@1  84.38 ( 83.22)	Acc@5  98.44 ( 97.98)
Epoch: [56][ 80/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.3828e-01 (5.6095e-01)	Acc@1  75.00 ( 83.04)	Acc@5  97.66 ( 97.97)
Epoch: [56][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.5469e-01 (5.5788e-01)	Acc@1  80.47 ( 83.15)	Acc@5  98.44 ( 97.96)
Epoch: [56][100/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.2793e-01 (5.5754e-01)	Acc@1  82.81 ( 83.22)	Acc@5  96.09 ( 97.94)
Epoch: [56][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3369e-01 (5.5655e-01)	Acc@1  85.16 ( 83.32)	Acc@5  98.44 ( 97.92)
Epoch: [56][120/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8657e-01 (5.5894e-01)	Acc@1  83.59 ( 83.18)	Acc@5  98.44 ( 97.92)
Epoch: [56][130/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.9229e-01 (5.6037e-01)	Acc@1  78.91 ( 83.06)	Acc@5  97.66 ( 97.93)
Epoch: [56][140/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3848e-01 (5.5858e-01)	Acc@1  86.72 ( 83.11)	Acc@5  99.22 ( 97.94)
Epoch: [56][150/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4893e-01 (5.5867e-01)	Acc@1  85.16 ( 83.15)	Acc@5  95.31 ( 97.90)
Epoch: [56][160/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8462e-01 (5.5629e-01)	Acc@1  83.59 ( 83.14)	Acc@5  98.44 ( 97.94)
Epoch: [56][170/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2246e-01 (5.5777e-01)	Acc@1  85.16 ( 83.09)	Acc@5  97.66 ( 97.91)
Epoch: [56][180/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4355e-01 (5.6039e-01)	Acc@1  85.94 ( 83.01)	Acc@5  95.31 ( 97.87)
Epoch: [56][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7969e-01 (5.6225e-01)	Acc@1  78.12 ( 82.97)	Acc@5  97.66 ( 97.86)
Epoch: [56][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8594e-01 (5.6547e-01)	Acc@1  81.25 ( 82.82)	Acc@5  99.22 ( 97.82)
Epoch: [56][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3135e-01 (5.6759e-01)	Acc@1  80.47 ( 82.66)	Acc@5  98.44 ( 97.83)
Epoch: [56][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1172e-01 (5.6607e-01)	Acc@1  85.94 ( 82.71)	Acc@5  98.44 ( 97.84)
Epoch: [56][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3271e-01 (5.6614e-01)	Acc@1  86.72 ( 82.70)	Acc@5  96.88 ( 97.87)
Epoch: [56][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.0371e-01 (5.6719e-01)	Acc@1  81.25 ( 82.70)	Acc@5  93.75 ( 97.85)
Epoch: [56][250/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8408e-01 (5.6848e-01)	Acc@1  82.03 ( 82.70)	Acc@5  96.88 ( 97.83)
Epoch: [56][260/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7695e-01 (5.6731e-01)	Acc@1  89.06 ( 82.71)	Acc@5  99.22 ( 97.82)
Epoch: [56][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4062e-01 (5.6738e-01)	Acc@1  80.47 ( 82.67)	Acc@5  96.09 ( 97.81)
Epoch: [56][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3174e-01 (5.6839e-01)	Acc@1  82.03 ( 82.63)	Acc@5  99.22 ( 97.81)
Epoch: [56][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9619e-01 (5.6894e-01)	Acc@1  82.81 ( 82.60)	Acc@5  99.22 ( 97.79)
Epoch: [56][300/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8022e-01 (5.6944e-01)	Acc@1  84.38 ( 82.56)	Acc@5  96.88 ( 97.79)
Epoch: [56][310/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3271e-01 (5.6985e-01)	Acc@1  82.81 ( 82.53)	Acc@5  99.22 ( 97.79)
Epoch: [56][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8154e-01 (5.6892e-01)	Acc@1  81.25 ( 82.56)	Acc@5  94.53 ( 97.79)
Epoch: [56][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7764e-01 (5.6963e-01)	Acc@1  82.81 ( 82.58)	Acc@5  97.66 ( 97.77)
Epoch: [56][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9922e-01 (5.7026e-01)	Acc@1  80.47 ( 82.55)	Acc@5  92.97 ( 97.77)
Epoch: [56][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7031e-01 (5.7031e-01)	Acc@1  82.81 ( 82.58)	Acc@5  98.44 ( 97.76)
Epoch: [56][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.2998e-01 (5.7054e-01)	Acc@1  79.69 ( 82.57)	Acc@5  94.53 ( 97.75)
Epoch: [56][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.0557e-01 (5.7230e-01)	Acc@1  82.81 ( 82.53)	Acc@5  95.31 ( 97.73)
Epoch: [56][380/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.7339e-01 (5.7442e-01)	Acc@1  82.03 ( 82.45)	Acc@5 100.00 ( 97.70)
Epoch: [56][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.4990e-01 (5.7538e-01)	Acc@1  78.75 ( 82.40)	Acc@5  96.25 ( 97.70)
## e[56] optimizer.zero_grad (sum) time: 0.41440892219543457
## e[56]       loss.backward (sum) time: 7.386082410812378
## e[56]      optimizer.step (sum) time: 3.526599168777466
## epoch[56] training(only) time: 26.200034856796265
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1660e+00 (1.1660e+00)	Acc@1  71.00 ( 71.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.2080e+00 (1.2314e+00)	Acc@1  70.00 ( 68.91)	Acc@5  93.00 ( 89.73)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6221e+00 (1.2269e+00)	Acc@1  60.00 ( 68.29)	Acc@5  89.00 ( 90.00)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.7324e+00 (1.2688e+00)	Acc@1  50.00 ( 66.84)	Acc@5  89.00 ( 89.58)
Test: [ 40/100]	Time  0.029 ( 0.030)	Loss 1.1943e+00 (1.2432e+00)	Acc@1  68.00 ( 66.83)	Acc@5  89.00 ( 90.05)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2754e+00 (1.2510e+00)	Acc@1  68.00 ( 66.55)	Acc@5  88.00 ( 89.96)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3242e+00 (1.2413e+00)	Acc@1  64.00 ( 66.69)	Acc@5  92.00 ( 90.13)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0547e+00 (1.2439e+00)	Acc@1  74.00 ( 66.73)	Acc@5  92.00 ( 90.17)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.3955e+00 (1.2521e+00)	Acc@1  60.00 ( 66.52)	Acc@5  86.00 ( 90.00)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.8682e+00 (1.2425e+00)	Acc@1  56.00 ( 66.63)	Acc@5  82.00 ( 90.07)
 * Acc@1 66.900 Acc@5 90.180
### epoch[56] execution time: 29.025371551513672
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.220 ( 0.220)	Data  0.146 ( 0.146)	Loss 4.8877e-01 (4.8877e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 97.66)
Epoch: [57][ 10/391]	Time  0.069 ( 0.081)	Data  0.001 ( 0.014)	Loss 4.6899e-01 (5.4570e-01)	Acc@1  89.06 ( 83.31)	Acc@5  98.44 ( 98.01)
Epoch: [57][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.6851e-01 (5.4204e-01)	Acc@1  85.16 ( 83.26)	Acc@5 100.00 ( 98.07)
Epoch: [57][ 30/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.006)	Loss 5.0732e-01 (5.3505e-01)	Acc@1  85.16 ( 83.27)	Acc@5  98.44 ( 98.16)
Epoch: [57][ 40/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.005)	Loss 6.0107e-01 (5.3153e-01)	Acc@1  82.03 ( 83.44)	Acc@5  97.66 ( 98.19)
Epoch: [57][ 50/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.3516e-01 (5.3971e-01)	Acc@1  83.59 ( 83.15)	Acc@5  96.88 ( 98.05)
Epoch: [57][ 60/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.9619e-01 (5.3933e-01)	Acc@1  80.47 ( 83.11)	Acc@5  98.44 ( 98.07)
Epoch: [57][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.6113e-01 (5.4882e-01)	Acc@1  78.91 ( 82.97)	Acc@5  98.44 ( 98.01)
Epoch: [57][ 80/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.2051e-01 (5.5093e-01)	Acc@1  80.47 ( 82.87)	Acc@5  99.22 ( 97.96)
Epoch: [57][ 90/391]	Time  0.071 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.9536e-01 (5.4945e-01)	Acc@1  84.38 ( 82.88)	Acc@5  98.44 ( 98.03)
Epoch: [57][100/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.9463e-01 (5.5238e-01)	Acc@1  83.59 ( 82.90)	Acc@5  97.66 ( 97.94)
Epoch: [57][110/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4238e-01 (5.5086e-01)	Acc@1  88.28 ( 82.98)	Acc@5  99.22 ( 97.94)
Epoch: [57][120/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.9375e-01 (5.5156e-01)	Acc@1  78.12 ( 82.95)	Acc@5  99.22 ( 97.91)
Epoch: [57][130/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.0107e-01 (5.5009e-01)	Acc@1  83.59 ( 83.01)	Acc@5  96.88 ( 97.91)
Epoch: [57][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5273e-01 (5.4809e-01)	Acc@1  85.16 ( 83.05)	Acc@5  97.66 ( 97.91)
Epoch: [57][150/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.8643e-01 (5.5109e-01)	Acc@1  82.81 ( 82.98)	Acc@5  97.66 ( 97.88)
Epoch: [57][160/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.1523e-01 (5.5219e-01)	Acc@1  78.12 ( 83.00)	Acc@5  96.88 ( 97.83)
Epoch: [57][170/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.8994e-01 (5.5288e-01)	Acc@1  80.47 ( 82.98)	Acc@5  95.31 ( 97.85)
Epoch: [57][180/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3857e-01 (5.5344e-01)	Acc@1  83.59 ( 83.03)	Acc@5  98.44 ( 97.85)
Epoch: [57][190/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.5625e-01 (5.5218e-01)	Acc@1  80.47 ( 83.12)	Acc@5  96.88 ( 97.86)
Epoch: [57][200/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.8838e-01 (5.5151e-01)	Acc@1  85.16 ( 83.14)	Acc@5  97.66 ( 97.89)
Epoch: [57][210/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5078e-01 (5.5060e-01)	Acc@1  80.47 ( 83.12)	Acc@5  97.66 ( 97.89)
Epoch: [57][220/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (5.5066e-01)	Acc@1  84.38 ( 83.14)	Acc@5  97.66 ( 97.87)
Epoch: [57][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3281e-01 (5.5457e-01)	Acc@1  82.03 ( 83.04)	Acc@5  94.53 ( 97.82)
Epoch: [57][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (5.5550e-01)	Acc@1  80.47 ( 82.97)	Acc@5  96.09 ( 97.81)
Epoch: [57][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9619e-01 (5.5499e-01)	Acc@1  78.12 ( 82.97)	Acc@5  98.44 ( 97.83)
Epoch: [57][260/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7578e-01 (5.5558e-01)	Acc@1  78.91 ( 82.95)	Acc@5  96.88 ( 97.81)
Epoch: [57][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8047e-01 (5.5755e-01)	Acc@1  87.50 ( 82.91)	Acc@5  98.44 ( 97.80)
Epoch: [57][280/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.0605e-01 (5.5976e-01)	Acc@1  78.91 ( 82.84)	Acc@5  95.31 ( 97.76)
Epoch: [57][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8105e-01 (5.6081e-01)	Acc@1  82.03 ( 82.82)	Acc@5  96.88 ( 97.75)
Epoch: [57][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9600e-01 (5.6128e-01)	Acc@1  90.62 ( 82.80)	Acc@5 100.00 ( 97.76)
Epoch: [57][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.4414e-01 (5.6320e-01)	Acc@1  77.34 ( 82.74)	Acc@5  94.53 ( 97.73)
Epoch: [57][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9072e-01 (5.6317e-01)	Acc@1  84.38 ( 82.74)	Acc@5  99.22 ( 97.74)
Epoch: [57][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (5.6294e-01)	Acc@1  84.38 ( 82.77)	Acc@5  98.44 ( 97.75)
Epoch: [57][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7520e-01 (5.6392e-01)	Acc@1  81.25 ( 82.73)	Acc@5  97.66 ( 97.73)
Epoch: [57][350/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4653e-01 (5.6351e-01)	Acc@1  89.84 ( 82.76)	Acc@5  98.44 ( 97.73)
Epoch: [57][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2148e-01 (5.6417e-01)	Acc@1  86.72 ( 82.71)	Acc@5  96.88 ( 97.74)
Epoch: [57][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5044e-01 (5.6437e-01)	Acc@1  88.28 ( 82.71)	Acc@5  96.88 ( 97.74)
Epoch: [57][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.6562e-01 (5.6673e-01)	Acc@1  78.12 ( 82.67)	Acc@5  94.53 ( 97.71)
Epoch: [57][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.6689e-01 (5.6661e-01)	Acc@1  86.25 ( 82.66)	Acc@5  97.50 ( 97.72)
## e[57] optimizer.zero_grad (sum) time: 0.40868568420410156
## e[57]       loss.backward (sum) time: 7.414776802062988
## e[57]      optimizer.step (sum) time: 3.6342427730560303
## epoch[57] training(only) time: 26.27869415283203
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2061e+00 (1.2061e+00)	Acc@1  72.00 ( 72.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.1816e+00 (1.2488e+00)	Acc@1  71.00 ( 70.00)	Acc@5  93.00 ( 90.18)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.5283e+00 (1.2286e+00)	Acc@1  65.00 ( 68.95)	Acc@5  88.00 ( 90.19)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.8066e+00 (1.2732e+00)	Acc@1  55.00 ( 67.26)	Acc@5  85.00 ( 89.68)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.2090e+00 (1.2518e+00)	Acc@1  71.00 ( 67.32)	Acc@5  91.00 ( 90.24)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2666e+00 (1.2545e+00)	Acc@1  65.00 ( 67.27)	Acc@5  87.00 ( 90.29)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2920e+00 (1.2454e+00)	Acc@1  63.00 ( 67.20)	Acc@5  91.00 ( 90.48)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0342e+00 (1.2456e+00)	Acc@1  73.00 ( 67.28)	Acc@5  93.00 ( 90.42)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3213e+00 (1.2545e+00)	Acc@1  64.00 ( 67.10)	Acc@5  87.00 ( 90.38)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9229e+00 (1.2480e+00)	Acc@1  56.00 ( 67.13)	Acc@5  85.00 ( 90.46)
 * Acc@1 67.320 Acc@5 90.500
### epoch[57] execution time: 29.10095191001892
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.225 ( 0.225)	Data  0.148 ( 0.148)	Loss 4.8584e-01 (4.8584e-01)	Acc@1  82.03 ( 82.03)	Acc@5  99.22 ( 99.22)
Epoch: [58][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.014)	Loss 5.9863e-01 (5.2739e-01)	Acc@1  82.81 ( 84.16)	Acc@5  96.88 ( 98.08)
Epoch: [58][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.008)	Loss 5.4346e-01 (5.1790e-01)	Acc@1  86.72 ( 84.38)	Acc@5  95.31 ( 98.03)
Epoch: [58][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 5.1807e-01 (5.1685e-01)	Acc@1  85.94 ( 84.55)	Acc@5  97.66 ( 98.21)
Epoch: [58][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.9634e-01 (5.2784e-01)	Acc@1  82.03 ( 84.18)	Acc@5  99.22 ( 98.13)
Epoch: [58][ 50/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.1113e-01 (5.1872e-01)	Acc@1  92.97 ( 84.45)	Acc@5  99.22 ( 98.28)
Epoch: [58][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.4404e-01 (5.2279e-01)	Acc@1  78.91 ( 84.35)	Acc@5  98.44 ( 98.22)
Epoch: [58][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.2930e-01 (5.2994e-01)	Acc@1  90.62 ( 84.21)	Acc@5  96.88 ( 98.10)
Epoch: [58][ 80/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.4004e-01 (5.2995e-01)	Acc@1  82.03 ( 83.97)	Acc@5  98.44 ( 98.14)
Epoch: [58][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.8594e-01 (5.2764e-01)	Acc@1  82.03 ( 84.05)	Acc@5  98.44 ( 98.15)
Epoch: [58][100/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.7910e-01 (5.3226e-01)	Acc@1  82.03 ( 83.75)	Acc@5  99.22 ( 98.05)
Epoch: [58][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5908e-01 (5.3591e-01)	Acc@1  83.59 ( 83.66)	Acc@5  97.66 ( 98.07)
Epoch: [58][120/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5020e-01 (5.3574e-01)	Acc@1  83.59 ( 83.60)	Acc@5  99.22 ( 98.07)
Epoch: [58][130/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6982e-01 (5.3485e-01)	Acc@1  81.25 ( 83.62)	Acc@5  98.44 ( 98.09)
Epoch: [58][140/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0156e-01 (5.3495e-01)	Acc@1  80.47 ( 83.59)	Acc@5  96.88 ( 98.09)
Epoch: [58][150/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9829e-01 (5.3326e-01)	Acc@1  81.25 ( 83.62)	Acc@5  98.44 ( 98.13)
Epoch: [58][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6846e-01 (5.3397e-01)	Acc@1  78.12 ( 83.67)	Acc@5  96.88 ( 98.10)
Epoch: [58][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5371e-01 (5.3514e-01)	Acc@1  82.81 ( 83.68)	Acc@5  97.66 ( 98.06)
Epoch: [58][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8594e-01 (5.3402e-01)	Acc@1  82.81 ( 83.74)	Acc@5  97.66 ( 98.08)
Epoch: [58][190/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9854e-01 (5.3665e-01)	Acc@1  84.38 ( 83.65)	Acc@5  97.66 ( 98.07)
Epoch: [58][200/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4248e-01 (5.3700e-01)	Acc@1  85.94 ( 83.65)	Acc@5  97.66 ( 98.05)
Epoch: [58][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8984e-01 (5.3888e-01)	Acc@1  82.81 ( 83.59)	Acc@5  98.44 ( 98.03)
Epoch: [58][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7632e-01 (5.3994e-01)	Acc@1  86.72 ( 83.58)	Acc@5  97.66 ( 98.03)
Epoch: [58][230/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6592e-01 (5.4274e-01)	Acc@1  82.81 ( 83.47)	Acc@5  99.22 ( 98.00)
Epoch: [58][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6069e-01 (5.4282e-01)	Acc@1  85.16 ( 83.42)	Acc@5  99.22 ( 98.01)
Epoch: [58][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7422e-01 (5.4279e-01)	Acc@1  79.69 ( 83.44)	Acc@5  99.22 ( 98.01)
Epoch: [58][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.3379e-01 (5.4289e-01)	Acc@1  81.25 ( 83.41)	Acc@5  96.88 ( 98.00)
Epoch: [58][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7192e-01 (5.4167e-01)	Acc@1  86.72 ( 83.46)	Acc@5  97.66 ( 98.00)
Epoch: [58][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2783e-01 (5.4304e-01)	Acc@1  85.16 ( 83.40)	Acc@5  99.22 ( 97.99)
Epoch: [58][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.9287e-01 (5.4498e-01)	Acc@1  80.47 ( 83.34)	Acc@5  96.88 ( 97.97)
Epoch: [58][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4531e-01 (5.4483e-01)	Acc@1  87.50 ( 83.35)	Acc@5  98.44 ( 97.96)
Epoch: [58][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4844e-01 (5.4665e-01)	Acc@1  78.12 ( 83.31)	Acc@5  96.88 ( 97.93)
Epoch: [58][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2393e-01 (5.4672e-01)	Acc@1  82.03 ( 83.31)	Acc@5  99.22 ( 97.92)
Epoch: [58][330/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7666e-01 (5.4762e-01)	Acc@1  84.38 ( 83.29)	Acc@5  96.88 ( 97.90)
Epoch: [58][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3701e-01 (5.4775e-01)	Acc@1  86.72 ( 83.30)	Acc@5  99.22 ( 97.92)
Epoch: [58][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.7031e-01 (5.4851e-01)	Acc@1  82.81 ( 83.27)	Acc@5  96.88 ( 97.89)
Epoch: [58][360/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.8193e-01 (5.4940e-01)	Acc@1  80.47 ( 83.22)	Acc@5  99.22 ( 97.88)
Epoch: [58][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.5430e-01 (5.4998e-01)	Acc@1  80.47 ( 83.20)	Acc@5  98.44 ( 97.87)
Epoch: [58][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.7529e-01 (5.5110e-01)	Acc@1  76.56 ( 83.15)	Acc@5  96.09 ( 97.86)
Epoch: [58][390/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.001)	Loss 7.3486e-01 (5.5253e-01)	Acc@1  78.75 ( 83.14)	Acc@5  96.25 ( 97.84)
## e[58] optimizer.zero_grad (sum) time: 0.41051363945007324
## e[58]       loss.backward (sum) time: 7.3794755935668945
## e[58]      optimizer.step (sum) time: 3.5393645763397217
## epoch[58] training(only) time: 26.14166784286499
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.0938e+00 (1.0938e+00)	Acc@1  67.00 ( 67.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.028 ( 0.040)	Loss 1.1504e+00 (1.2482e+00)	Acc@1  72.00 ( 69.00)	Acc@5  92.00 ( 89.73)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.6445e+00 (1.2413e+00)	Acc@1  62.00 ( 68.52)	Acc@5  88.00 ( 90.00)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7188e+00 (1.2757e+00)	Acc@1  52.00 ( 67.19)	Acc@5  86.00 ( 89.42)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.2324e+00 (1.2545e+00)	Acc@1  64.00 ( 67.12)	Acc@5  91.00 ( 89.90)
Test: [ 50/100]	Time  0.024 ( 0.029)	Loss 1.3164e+00 (1.2642e+00)	Acc@1  66.00 ( 66.96)	Acc@5  91.00 ( 90.08)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.2803e+00 (1.2497e+00)	Acc@1  65.00 ( 67.03)	Acc@5  91.00 ( 90.38)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.0713e+00 (1.2500e+00)	Acc@1  69.00 ( 67.00)	Acc@5  91.00 ( 90.42)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3379e+00 (1.2555e+00)	Acc@1  65.00 ( 66.99)	Acc@5  88.00 ( 90.26)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 1.9551e+00 (1.2503e+00)	Acc@1  56.00 ( 66.98)	Acc@5  83.00 ( 90.32)
 * Acc@1 67.240 Acc@5 90.400
### epoch[58] execution time: 28.938243865966797
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.220 ( 0.220)	Data  0.144 ( 0.144)	Loss 5.4541e-01 (5.4541e-01)	Acc@1  85.94 ( 85.94)	Acc@5  96.09 ( 96.09)
Epoch: [59][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.014)	Loss 4.8462e-01 (5.2923e-01)	Acc@1  85.16 ( 84.23)	Acc@5  99.22 ( 98.51)
Epoch: [59][ 20/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.008)	Loss 3.8916e-01 (5.2813e-01)	Acc@1  86.72 ( 83.63)	Acc@5  99.22 ( 98.29)
Epoch: [59][ 30/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.006)	Loss 5.1660e-01 (5.2581e-01)	Acc@1  83.59 ( 83.62)	Acc@5  98.44 ( 98.16)
Epoch: [59][ 40/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.3652e-01 (5.2828e-01)	Acc@1  85.16 ( 83.59)	Acc@5  99.22 ( 98.06)
Epoch: [59][ 50/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.3857e-01 (5.2971e-01)	Acc@1  83.59 ( 83.70)	Acc@5  97.66 ( 98.02)
Epoch: [59][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.3018e-01 (5.3041e-01)	Acc@1  89.06 ( 83.72)	Acc@5  98.44 ( 98.04)
Epoch: [59][ 70/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.3091e-01 (5.2938e-01)	Acc@1  89.84 ( 83.69)	Acc@5 100.00 ( 97.99)
Epoch: [59][ 80/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.2373e-01 (5.3014e-01)	Acc@1  90.62 ( 83.79)	Acc@5  99.22 ( 97.98)
Epoch: [59][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.9414e-01 (5.3227e-01)	Acc@1  83.59 ( 83.67)	Acc@5  98.44 ( 97.98)
Epoch: [59][100/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (5.3821e-01)	Acc@1  83.59 ( 83.41)	Acc@5  97.66 ( 97.90)
Epoch: [59][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.4883e-01 (5.4096e-01)	Acc@1  85.16 ( 83.28)	Acc@5  97.66 ( 97.88)
Epoch: [59][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3677e-01 (5.4101e-01)	Acc@1  87.50 ( 83.23)	Acc@5  99.22 ( 97.91)
Epoch: [59][130/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.6875e-01 (5.4573e-01)	Acc@1  85.16 ( 83.13)	Acc@5  99.22 ( 97.86)
Epoch: [59][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.8057e-01 (5.4706e-01)	Acc@1  78.91 ( 83.08)	Acc@5  96.88 ( 97.84)
Epoch: [59][150/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5078e-01 (5.4791e-01)	Acc@1  81.25 ( 82.98)	Acc@5  97.66 ( 97.81)
Epoch: [59][160/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.2065e-01 (5.4682e-01)	Acc@1  87.50 ( 83.00)	Acc@5  99.22 ( 97.84)
Epoch: [59][170/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.0156e-01 (5.4779e-01)	Acc@1  80.47 ( 83.01)	Acc@5  96.88 ( 97.82)
Epoch: [59][180/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9146e-01 (5.4796e-01)	Acc@1  85.16 ( 82.99)	Acc@5  96.88 ( 97.85)
Epoch: [59][190/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.1187e-01 (5.4867e-01)	Acc@1  86.72 ( 83.00)	Acc@5  99.22 ( 97.86)
Epoch: [59][200/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.4648e-01 (5.4931e-01)	Acc@1  75.78 ( 82.96)	Acc@5  99.22 ( 97.88)
Epoch: [59][210/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (5.4904e-01)	Acc@1  86.72 ( 82.98)	Acc@5  95.31 ( 97.87)
Epoch: [59][220/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.2588e-01 (5.4892e-01)	Acc@1  86.72 ( 83.06)	Acc@5  98.44 ( 97.89)
Epoch: [59][230/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1953e-01 (5.4936e-01)	Acc@1  82.81 ( 83.02)	Acc@5  99.22 ( 97.89)
Epoch: [59][240/391]	Time  0.074 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5615e-01 (5.5127e-01)	Acc@1  79.69 ( 82.96)	Acc@5  96.88 ( 97.85)
Epoch: [59][250/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (5.5081e-01)	Acc@1  85.16 ( 82.98)	Acc@5  97.66 ( 97.85)
Epoch: [59][260/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2979e-01 (5.4974e-01)	Acc@1  85.16 ( 83.03)	Acc@5  97.66 ( 97.85)
Epoch: [59][270/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6006e-01 (5.5055e-01)	Acc@1  85.94 ( 83.04)	Acc@5  96.88 ( 97.84)
Epoch: [59][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1719e-01 (5.4963e-01)	Acc@1  78.91 ( 83.09)	Acc@5  99.22 ( 97.84)
Epoch: [59][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4248e-01 (5.5002e-01)	Acc@1  79.69 ( 83.05)	Acc@5  99.22 ( 97.85)
Epoch: [59][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3516e-01 (5.4922e-01)	Acc@1  85.16 ( 83.09)	Acc@5  98.44 ( 97.88)
Epoch: [59][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8008e-01 (5.5006e-01)	Acc@1  78.91 ( 83.02)	Acc@5  97.66 ( 97.89)
Epoch: [59][320/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.1582e-01 (5.5138e-01)	Acc@1  74.22 ( 83.01)	Acc@5  97.66 ( 97.86)
Epoch: [59][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4287e-01 (5.5020e-01)	Acc@1  85.94 ( 83.05)	Acc@5  97.66 ( 97.86)
Epoch: [59][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.6650e-01 (5.4974e-01)	Acc@1  82.03 ( 83.05)	Acc@5  95.31 ( 97.86)
Epoch: [59][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7373e-01 (5.5096e-01)	Acc@1  82.03 ( 83.05)	Acc@5  99.22 ( 97.83)
Epoch: [59][360/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7109e-01 (5.5210e-01)	Acc@1  85.16 ( 83.00)	Acc@5 100.00 ( 97.81)
Epoch: [59][370/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.2139e-01 (5.5274e-01)	Acc@1  85.94 ( 82.99)	Acc@5  98.44 ( 97.81)
Epoch: [59][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.1074e-01 (5.5302e-01)	Acc@1  88.28 ( 83.01)	Acc@5  96.09 ( 97.80)
Epoch: [59][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.1084e-01 (5.5481e-01)	Acc@1  77.50 ( 82.97)	Acc@5 100.00 ( 97.78)
## e[59] optimizer.zero_grad (sum) time: 0.41164255142211914
## e[59]       loss.backward (sum) time: 7.372262001037598
## e[59]      optimizer.step (sum) time: 3.6485817432403564
## epoch[59] training(only) time: 26.303133010864258
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2432e+00 (1.2432e+00)	Acc@1  66.00 ( 66.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.034 ( 0.039)	Loss 1.1650e+00 (1.2476e+00)	Acc@1  67.00 ( 68.00)	Acc@5  94.00 ( 89.73)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.6309e+00 (1.2467e+00)	Acc@1  64.00 ( 67.71)	Acc@5  88.00 ( 90.00)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7100e+00 (1.2763e+00)	Acc@1  58.00 ( 67.03)	Acc@5  88.00 ( 89.55)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1670e+00 (1.2523e+00)	Acc@1  72.00 ( 66.98)	Acc@5  92.00 ( 90.02)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.3213e+00 (1.2560e+00)	Acc@1  64.00 ( 66.63)	Acc@5  89.00 ( 90.22)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2803e+00 (1.2437e+00)	Acc@1  66.00 ( 66.77)	Acc@5  90.00 ( 90.51)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0322e+00 (1.2487e+00)	Acc@1  71.00 ( 66.79)	Acc@5  93.00 ( 90.48)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.3457e+00 (1.2553e+00)	Acc@1  62.00 ( 66.57)	Acc@5  91.00 ( 90.44)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 1.8545e+00 (1.2484e+00)	Acc@1  53.00 ( 66.67)	Acc@5  86.00 ( 90.46)
 * Acc@1 66.930 Acc@5 90.470
### epoch[59] execution time: 29.102905750274658
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.225 ( 0.225)	Data  0.149 ( 0.149)	Loss 6.1670e-01 (6.1670e-01)	Acc@1  82.03 ( 82.03)	Acc@5  96.09 ( 96.09)
Epoch: [60][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.015)	Loss 4.8877e-01 (5.2783e-01)	Acc@1  85.16 ( 84.09)	Acc@5 100.00 ( 98.08)
Epoch: [60][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.1797e-01 (5.2415e-01)	Acc@1  86.72 ( 84.26)	Acc@5  98.44 ( 97.95)
Epoch: [60][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 6.9189e-01 (5.2335e-01)	Acc@1  80.47 ( 84.43)	Acc@5  94.53 ( 97.81)
Epoch: [60][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.7632e-01 (5.2266e-01)	Acc@1  85.94 ( 84.53)	Acc@5  96.09 ( 97.88)
Epoch: [60][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.0684e-01 (5.1785e-01)	Acc@1  82.03 ( 84.47)	Acc@5  99.22 ( 97.92)
Epoch: [60][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 6.1328e-01 (5.1518e-01)	Acc@1  83.59 ( 84.52)	Acc@5  96.88 ( 97.99)
Epoch: [60][ 70/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.2383e-01 (5.0947e-01)	Acc@1  85.94 ( 84.52)	Acc@5  99.22 ( 98.06)
Epoch: [60][ 80/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.5532e-01 (5.1067e-01)	Acc@1  86.72 ( 84.51)	Acc@5  98.44 ( 98.05)
Epoch: [60][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.8545e-01 (5.1433e-01)	Acc@1  81.25 ( 84.33)	Acc@5 100.00 ( 98.04)
Epoch: [60][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.2051e-01 (5.1435e-01)	Acc@1  86.72 ( 84.43)	Acc@5  96.09 ( 98.01)
Epoch: [60][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3906e-01 (5.1203e-01)	Acc@1  81.25 ( 84.51)	Acc@5  99.22 ( 98.01)
Epoch: [60][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9780e-01 (5.1074e-01)	Acc@1  84.38 ( 84.52)	Acc@5  97.66 ( 98.03)
Epoch: [60][130/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.9033e-01 (5.1377e-01)	Acc@1  82.81 ( 84.51)	Acc@5  97.66 ( 98.04)
Epoch: [60][140/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6104e-01 (5.1040e-01)	Acc@1  80.47 ( 84.59)	Acc@5 100.00 ( 98.07)
Epoch: [60][150/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2871e-01 (5.0890e-01)	Acc@1  88.28 ( 84.67)	Acc@5  97.66 ( 98.08)
Epoch: [60][160/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1943e-01 (5.0683e-01)	Acc@1  86.72 ( 84.70)	Acc@5 100.00 ( 98.08)
Epoch: [60][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5654e-01 (5.0929e-01)	Acc@1  85.94 ( 84.66)	Acc@5  98.44 ( 98.05)
Epoch: [60][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4160e-01 (5.0932e-01)	Acc@1  81.25 ( 84.63)	Acc@5  96.88 ( 98.05)
Epoch: [60][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2451e-01 (5.1072e-01)	Acc@1  85.16 ( 84.62)	Acc@5  93.75 ( 98.02)
Epoch: [60][200/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6104e-01 (5.0899e-01)	Acc@1  84.38 ( 84.70)	Acc@5  98.44 ( 98.03)
Epoch: [60][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3555e-01 (5.0889e-01)	Acc@1  87.50 ( 84.67)	Acc@5  99.22 ( 98.03)
Epoch: [60][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4688e-01 (5.1083e-01)	Acc@1  82.03 ( 84.63)	Acc@5  96.88 ( 98.02)
Epoch: [60][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8511e-01 (5.1041e-01)	Acc@1  82.03 ( 84.56)	Acc@5  97.66 ( 98.04)
Epoch: [60][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1650e-01 (5.0940e-01)	Acc@1  88.28 ( 84.58)	Acc@5  99.22 ( 98.04)
Epoch: [60][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9683e-01 (5.0876e-01)	Acc@1  82.03 ( 84.56)	Acc@5  99.22 ( 98.08)
Epoch: [60][260/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9185e-01 (5.0806e-01)	Acc@1  88.28 ( 84.61)	Acc@5  99.22 ( 98.09)
Epoch: [60][270/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1968e-01 (5.0715e-01)	Acc@1  88.28 ( 84.65)	Acc@5  99.22 ( 98.10)
Epoch: [60][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0791e-01 (5.0696e-01)	Acc@1  77.34 ( 84.63)	Acc@5  99.22 ( 98.11)
Epoch: [60][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9575e-01 (5.0476e-01)	Acc@1  87.50 ( 84.72)	Acc@5 100.00 ( 98.14)
Epoch: [60][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2148e-01 (5.0441e-01)	Acc@1  82.81 ( 84.74)	Acc@5  98.44 ( 98.14)
Epoch: [60][310/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0928e-01 (5.0426e-01)	Acc@1  82.81 ( 84.74)	Acc@5  99.22 ( 98.15)
Epoch: [60][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6436e-01 (5.0398e-01)	Acc@1  82.81 ( 84.74)	Acc@5  99.22 ( 98.15)
Epoch: [60][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1748e-01 (5.0411e-01)	Acc@1  91.41 ( 84.74)	Acc@5  99.22 ( 98.14)
Epoch: [60][340/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7793e-01 (5.0349e-01)	Acc@1  87.50 ( 84.75)	Acc@5 100.00 ( 98.14)
Epoch: [60][350/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1758e-01 (5.0358e-01)	Acc@1  85.16 ( 84.78)	Acc@5  98.44 ( 98.14)
Epoch: [60][360/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (5.0297e-01)	Acc@1  85.16 ( 84.80)	Acc@5  96.88 ( 98.15)
Epoch: [60][370/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.1431e-01 (5.0275e-01)	Acc@1  88.28 ( 84.81)	Acc@5  99.22 ( 98.14)
Epoch: [60][380/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.0830e-01 (5.0321e-01)	Acc@1  87.50 ( 84.79)	Acc@5  94.53 ( 98.12)
Epoch: [60][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.7998e-01 (5.0267e-01)	Acc@1  88.75 ( 84.81)	Acc@5  98.75 ( 98.13)
## e[60] optimizer.zero_grad (sum) time: 0.4129505157470703
## e[60]       loss.backward (sum) time: 7.374490022659302
## e[60]      optimizer.step (sum) time: 3.5858898162841797
## epoch[60] training(only) time: 26.204246282577515
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.1807e+00 (1.1807e+00)	Acc@1  66.00 ( 66.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.029 ( 0.040)	Loss 1.1367e+00 (1.2249e+00)	Acc@1  70.00 ( 69.18)	Acc@5  94.00 ( 89.64)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.5986e+00 (1.2216e+00)	Acc@1  65.00 ( 68.48)	Acc@5  89.00 ( 90.10)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6738e+00 (1.2552e+00)	Acc@1  60.00 ( 67.42)	Acc@5  86.00 ( 89.61)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1885e+00 (1.2289e+00)	Acc@1  69.00 ( 67.51)	Acc@5  91.00 ( 90.15)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2646e+00 (1.2315e+00)	Acc@1  67.00 ( 67.43)	Acc@5  90.00 ( 90.35)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 1.2686e+00 (1.2193e+00)	Acc@1  63.00 ( 67.52)	Acc@5  91.00 ( 90.66)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0107e+00 (1.2241e+00)	Acc@1  72.00 ( 67.54)	Acc@5  92.00 ( 90.61)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.2871e+00 (1.2315e+00)	Acc@1  63.00 ( 67.25)	Acc@5  89.00 ( 90.52)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8301e+00 (1.2231e+00)	Acc@1  55.00 ( 67.30)	Acc@5  82.00 ( 90.58)
 * Acc@1 67.600 Acc@5 90.640
### epoch[60] execution time: 29.03876042366028
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.227 ( 0.227)	Data  0.146 ( 0.146)	Loss 5.3516e-01 (5.3516e-01)	Acc@1  85.16 ( 85.16)	Acc@5  98.44 ( 98.44)
Epoch: [61][ 10/391]	Time  0.066 ( 0.082)	Data  0.001 ( 0.014)	Loss 5.2930e-01 (4.9698e-01)	Acc@1  86.72 ( 85.51)	Acc@5  99.22 ( 98.30)
Epoch: [61][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.0332e-01 (4.7685e-01)	Acc@1  89.06 ( 86.09)	Acc@5  99.22 ( 98.36)
Epoch: [61][ 30/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.006)	Loss 4.5288e-01 (4.6986e-01)	Acc@1  91.41 ( 86.09)	Acc@5  99.22 ( 98.49)
Epoch: [61][ 40/391]	Time  0.070 ( 0.070)	Data  0.001 ( 0.005)	Loss 5.7812e-01 (4.7743e-01)	Acc@1  82.03 ( 85.90)	Acc@5  98.44 ( 98.38)
Epoch: [61][ 50/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.0977e-01 (4.8586e-01)	Acc@1  82.81 ( 85.60)	Acc@5  98.44 ( 98.21)
Epoch: [61][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.5142e-01 (4.7888e-01)	Acc@1  84.38 ( 85.75)	Acc@5  99.22 ( 98.35)
Epoch: [61][ 70/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.8511e-01 (4.7895e-01)	Acc@1  85.94 ( 85.79)	Acc@5  96.88 ( 98.38)
Epoch: [61][ 80/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.9097e-01 (4.7602e-01)	Acc@1  85.16 ( 85.81)	Acc@5  99.22 ( 98.39)
Epoch: [61][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.6289e-01 (4.7885e-01)	Acc@1  84.38 ( 85.75)	Acc@5  98.44 ( 98.40)
Epoch: [61][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.4824e-01 (4.7532e-01)	Acc@1  89.06 ( 85.95)	Acc@5  99.22 ( 98.48)
Epoch: [61][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.6777e-01 (4.7600e-01)	Acc@1  82.03 ( 85.90)	Acc@5  99.22 ( 98.47)
Epoch: [61][120/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.0342e-01 (4.7717e-01)	Acc@1  86.72 ( 85.88)	Acc@5  98.44 ( 98.44)
Epoch: [61][130/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.2456e-01 (4.7437e-01)	Acc@1  88.28 ( 85.95)	Acc@5  98.44 ( 98.47)
Epoch: [61][140/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0400e-01 (4.7705e-01)	Acc@1  81.25 ( 85.82)	Acc@5  96.88 ( 98.43)
Epoch: [61][150/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1279e-01 (4.7824e-01)	Acc@1  79.69 ( 85.69)	Acc@5  96.09 ( 98.43)
Epoch: [61][160/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3076e-01 (4.7942e-01)	Acc@1  85.94 ( 85.72)	Acc@5  98.44 ( 98.42)
Epoch: [61][170/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8901e-01 (4.7876e-01)	Acc@1  85.94 ( 85.76)	Acc@5  96.88 ( 98.43)
Epoch: [61][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2354e-01 (4.7801e-01)	Acc@1  82.81 ( 85.76)	Acc@5  98.44 ( 98.44)
Epoch: [61][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9805e-01 (4.7770e-01)	Acc@1  85.94 ( 85.76)	Acc@5  96.09 ( 98.45)
Epoch: [61][200/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (4.7778e-01)	Acc@1  87.50 ( 85.77)	Acc@5  95.31 ( 98.41)
Epoch: [61][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0635e-01 (4.7890e-01)	Acc@1  85.94 ( 85.69)	Acc@5  97.66 ( 98.41)
Epoch: [61][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1709e-01 (4.7817e-01)	Acc@1  82.81 ( 85.73)	Acc@5  99.22 ( 98.40)
Epoch: [61][230/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0684e-01 (4.7853e-01)	Acc@1  82.03 ( 85.70)	Acc@5 100.00 ( 98.41)
Epoch: [61][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2051e-01 (4.7917e-01)	Acc@1  84.38 ( 85.71)	Acc@5  99.22 ( 98.41)
Epoch: [61][250/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9878e-01 (4.8101e-01)	Acc@1  83.59 ( 85.70)	Acc@5  98.44 ( 98.38)
Epoch: [61][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6118e-01 (4.8151e-01)	Acc@1  82.03 ( 85.67)	Acc@5 100.00 ( 98.37)
Epoch: [61][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7510e-01 (4.8043e-01)	Acc@1  88.28 ( 85.72)	Acc@5  98.44 ( 98.41)
Epoch: [61][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1611e-01 (4.7986e-01)	Acc@1  84.38 ( 85.72)	Acc@5  97.66 ( 98.42)
Epoch: [61][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7217e-01 (4.7879e-01)	Acc@1  85.16 ( 85.74)	Acc@5  98.44 ( 98.43)
Epoch: [61][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6436e-01 (4.7853e-01)	Acc@1  85.94 ( 85.76)	Acc@5  99.22 ( 98.43)
Epoch: [61][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0161e-01 (4.7911e-01)	Acc@1  87.50 ( 85.73)	Acc@5  98.44 ( 98.41)
Epoch: [61][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3433e-01 (4.7967e-01)	Acc@1  86.72 ( 85.70)	Acc@5  98.44 ( 98.40)
Epoch: [61][330/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1064e-01 (4.7986e-01)	Acc@1  85.94 ( 85.67)	Acc@5  99.22 ( 98.39)
Epoch: [61][340/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6094e-01 (4.7920e-01)	Acc@1  86.72 ( 85.71)	Acc@5  96.88 ( 98.39)
Epoch: [61][350/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2393e-01 (4.7929e-01)	Acc@1  82.81 ( 85.70)	Acc@5  99.22 ( 98.39)
Epoch: [61][360/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.1968e-01 (4.7950e-01)	Acc@1  88.28 ( 85.69)	Acc@5 100.00 ( 98.39)
Epoch: [61][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.5288e-01 (4.7926e-01)	Acc@1  88.28 ( 85.69)	Acc@5  97.66 ( 98.39)
Epoch: [61][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.5303e-01 (4.7879e-01)	Acc@1  92.97 ( 85.72)	Acc@5  97.66 ( 98.39)
Epoch: [61][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.4688e-01 (4.7855e-01)	Acc@1  80.00 ( 85.72)	Acc@5  98.75 ( 98.39)
## e[61] optimizer.zero_grad (sum) time: 0.40529561042785645
## e[61]       loss.backward (sum) time: 7.351235389709473
## e[61]      optimizer.step (sum) time: 3.524827241897583
## epoch[61] training(only) time: 26.11983323097229
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.1611e+00 (1.1611e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.030 ( 0.039)	Loss 1.1318e+00 (1.2191e+00)	Acc@1  71.00 ( 69.00)	Acc@5  94.00 ( 90.27)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6055e+00 (1.2158e+00)	Acc@1  65.00 ( 68.67)	Acc@5  90.00 ( 90.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6797e+00 (1.2494e+00)	Acc@1  61.00 ( 67.52)	Acc@5  87.00 ( 89.68)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1973e+00 (1.2263e+00)	Acc@1  69.00 ( 67.63)	Acc@5  91.00 ( 90.20)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2705e+00 (1.2299e+00)	Acc@1  66.00 ( 67.43)	Acc@5  90.00 ( 90.39)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2734e+00 (1.2168e+00)	Acc@1  63.00 ( 67.69)	Acc@5  90.00 ( 90.61)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0195e+00 (1.2202e+00)	Acc@1  71.00 ( 67.73)	Acc@5  92.00 ( 90.55)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2803e+00 (1.2274e+00)	Acc@1  64.00 ( 67.43)	Acc@5  90.00 ( 90.44)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8643e+00 (1.2197e+00)	Acc@1  54.00 ( 67.46)	Acc@5  85.00 ( 90.53)
 * Acc@1 67.680 Acc@5 90.600
### epoch[61] execution time: 28.914010286331177
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.221 ( 0.221)	Data  0.145 ( 0.145)	Loss 5.2588e-01 (5.2588e-01)	Acc@1  79.69 ( 79.69)	Acc@5  98.44 ( 98.44)
Epoch: [62][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.014)	Loss 4.5850e-01 (5.2754e-01)	Acc@1  86.72 ( 83.88)	Acc@5  98.44 ( 97.80)
Epoch: [62][ 20/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.8706e-01 (4.9644e-01)	Acc@1  82.03 ( 84.75)	Acc@5  99.22 ( 98.33)
Epoch: [62][ 30/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.006)	Loss 5.0195e-01 (4.9969e-01)	Acc@1  86.72 ( 84.70)	Acc@5  97.66 ( 98.36)
Epoch: [62][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.6533e-01 (4.9321e-01)	Acc@1  83.59 ( 85.12)	Acc@5  98.44 ( 98.42)
Epoch: [62][ 50/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.1660e-01 (4.9181e-01)	Acc@1  82.03 ( 85.13)	Acc@5  98.44 ( 98.41)
Epoch: [62][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.1211e-01 (4.8861e-01)	Acc@1  89.84 ( 85.27)	Acc@5  97.66 ( 98.37)
Epoch: [62][ 70/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.5044e-01 (4.8846e-01)	Acc@1  86.72 ( 85.32)	Acc@5 100.00 ( 98.37)
Epoch: [62][ 80/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.4590e-01 (4.8500e-01)	Acc@1  83.59 ( 85.46)	Acc@5  98.44 ( 98.36)
Epoch: [62][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.0098e-01 (4.8041e-01)	Acc@1  83.59 ( 85.59)	Acc@5  98.44 ( 98.41)
Epoch: [62][100/391]	Time  0.076 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.9243e-01 (4.7522e-01)	Acc@1  80.47 ( 85.73)	Acc@5  99.22 ( 98.44)
Epoch: [62][110/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7632e-01 (4.7459e-01)	Acc@1  81.25 ( 85.77)	Acc@5  99.22 ( 98.42)
Epoch: [62][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.0732e-01 (4.7589e-01)	Acc@1  84.38 ( 85.72)	Acc@5  98.44 ( 98.41)
Epoch: [62][130/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3799e-01 (4.7297e-01)	Acc@1  86.72 ( 85.77)	Acc@5  99.22 ( 98.46)
Epoch: [62][140/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9619e-01 (4.7388e-01)	Acc@1  85.94 ( 85.78)	Acc@5  96.88 ( 98.42)
Epoch: [62][150/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7070e-01 (4.7376e-01)	Acc@1  85.16 ( 85.78)	Acc@5  99.22 ( 98.44)
Epoch: [62][160/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7290e-01 (4.7568e-01)	Acc@1  85.16 ( 85.72)	Acc@5 100.00 ( 98.47)
Epoch: [62][170/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3418e-01 (4.7606e-01)	Acc@1  82.81 ( 85.73)	Acc@5  97.66 ( 98.46)
Epoch: [62][180/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4312e-01 (4.7625e-01)	Acc@1  86.72 ( 85.77)	Acc@5  98.44 ( 98.46)
Epoch: [62][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1904e-01 (4.7652e-01)	Acc@1  85.94 ( 85.74)	Acc@5  98.44 ( 98.45)
Epoch: [62][200/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7744e-01 (4.7503e-01)	Acc@1  90.62 ( 85.83)	Acc@5  98.44 ( 98.45)
Epoch: [62][210/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5239e-01 (4.7429e-01)	Acc@1  89.84 ( 85.91)	Acc@5  97.66 ( 98.45)
Epoch: [62][220/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0332e-01 (4.7526e-01)	Acc@1  84.38 ( 85.91)	Acc@5 100.00 ( 98.43)
Epoch: [62][230/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8438e-01 (4.7447e-01)	Acc@1  83.59 ( 85.90)	Acc@5  99.22 ( 98.44)
Epoch: [62][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5459e-01 (4.7578e-01)	Acc@1  87.50 ( 85.81)	Acc@5  99.22 ( 98.43)
Epoch: [62][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7646e-01 (4.7509e-01)	Acc@1  89.06 ( 85.83)	Acc@5 100.00 ( 98.43)
Epoch: [62][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8232e-01 (4.7485e-01)	Acc@1  89.84 ( 85.85)	Acc@5  98.44 ( 98.41)
Epoch: [62][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7241e-01 (4.7489e-01)	Acc@1  84.38 ( 85.86)	Acc@5  98.44 ( 98.42)
Epoch: [62][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3970e-01 (4.7424e-01)	Acc@1  88.28 ( 85.90)	Acc@5  96.88 ( 98.41)
Epoch: [62][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9746e-01 (4.7392e-01)	Acc@1  89.84 ( 85.90)	Acc@5  97.66 ( 98.42)
Epoch: [62][300/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3516e-01 (4.7413e-01)	Acc@1  85.16 ( 85.90)	Acc@5  98.44 ( 98.41)
Epoch: [62][310/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8594e-01 (4.7369e-01)	Acc@1  80.47 ( 85.85)	Acc@5  96.88 ( 98.41)
Epoch: [62][320/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0107e-01 (4.7506e-01)	Acc@1  82.03 ( 85.80)	Acc@5  95.31 ( 98.41)
Epoch: [62][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3237e-01 (4.7559e-01)	Acc@1  84.38 ( 85.76)	Acc@5  98.44 ( 98.42)
Epoch: [62][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.4590e-01 (4.7529e-01)	Acc@1  84.38 ( 85.80)	Acc@5  96.88 ( 98.41)
Epoch: [62][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.4287e-01 (4.7478e-01)	Acc@1  85.16 ( 85.82)	Acc@5 100.00 ( 98.41)
Epoch: [62][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.0586e-01 (4.7600e-01)	Acc@1  85.94 ( 85.77)	Acc@5  98.44 ( 98.41)
Epoch: [62][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.1670e-01 (4.7604e-01)	Acc@1  80.47 ( 85.80)	Acc@5  97.66 ( 98.39)
Epoch: [62][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.8770e-01 (4.7570e-01)	Acc@1  87.50 ( 85.78)	Acc@5  98.44 ( 98.39)
Epoch: [62][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.4932e-01 (4.7575e-01)	Acc@1  83.75 ( 85.78)	Acc@5  96.25 ( 98.39)
## e[62] optimizer.zero_grad (sum) time: 0.41250181198120117
## e[62]       loss.backward (sum) time: 7.355281591415405
## e[62]      optimizer.step (sum) time: 3.529170036315918
## epoch[62] training(only) time: 26.178452253341675
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1543e+00 (1.1543e+00)	Acc@1  67.00 ( 67.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1211e+00 (1.2123e+00)	Acc@1  70.00 ( 69.36)	Acc@5  95.00 ( 90.00)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5781e+00 (1.2131e+00)	Acc@1  64.00 ( 68.90)	Acc@5  90.00 ( 90.33)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6758e+00 (1.2466e+00)	Acc@1  60.00 ( 67.84)	Acc@5  87.00 ( 89.84)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2031e+00 (1.2222e+00)	Acc@1  68.00 ( 67.68)	Acc@5  91.00 ( 90.46)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2432e+00 (1.2241e+00)	Acc@1  67.00 ( 67.53)	Acc@5  90.00 ( 90.63)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 1.2686e+00 (1.2121e+00)	Acc@1  62.00 ( 67.69)	Acc@5  92.00 ( 90.89)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.0166e+00 (1.2161e+00)	Acc@1  76.00 ( 67.82)	Acc@5  93.00 ( 90.82)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.2734e+00 (1.2241e+00)	Acc@1  65.00 ( 67.58)	Acc@5  89.00 ( 90.72)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.8398e+00 (1.2154e+00)	Acc@1  55.00 ( 67.64)	Acc@5  83.00 ( 90.81)
 * Acc@1 67.900 Acc@5 90.870
### epoch[62] execution time: 29.04756760597229
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.214 ( 0.214)	Data  0.135 ( 0.135)	Loss 4.8242e-01 (4.8242e-01)	Acc@1  85.16 ( 85.16)	Acc@5  98.44 ( 98.44)
Epoch: [63][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.013)	Loss 5.2246e-01 (4.8593e-01)	Acc@1  77.34 ( 85.51)	Acc@5  98.44 ( 97.94)
Epoch: [63][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.007)	Loss 4.3726e-01 (4.7952e-01)	Acc@1  87.50 ( 85.90)	Acc@5  99.22 ( 98.18)
Epoch: [63][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.2944e-01 (4.7444e-01)	Acc@1  89.06 ( 85.86)	Acc@5  97.66 ( 98.26)
Epoch: [63][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 3.5742e-01 (4.7759e-01)	Acc@1  89.84 ( 85.65)	Acc@5  98.44 ( 98.19)
Epoch: [63][ 50/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.5264e-01 (4.7909e-01)	Acc@1  85.94 ( 85.40)	Acc@5 100.00 ( 98.16)
Epoch: [63][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.5713e-01 (4.7543e-01)	Acc@1  80.47 ( 85.48)	Acc@5  99.22 ( 98.22)
Epoch: [63][ 70/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.0195e-01 (4.7276e-01)	Acc@1  84.38 ( 85.62)	Acc@5  97.66 ( 98.24)
Epoch: [63][ 80/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.4141e-01 (4.7437e-01)	Acc@1  84.38 ( 85.63)	Acc@5  98.44 ( 98.23)
Epoch: [63][ 90/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.5874e-01 (4.7244e-01)	Acc@1  87.50 ( 85.73)	Acc@5  97.66 ( 98.27)
Epoch: [63][100/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5337e-01 (4.7259e-01)	Acc@1  87.50 ( 85.74)	Acc@5  98.44 ( 98.30)
Epoch: [63][110/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4482e-01 (4.7037e-01)	Acc@1  89.06 ( 85.87)	Acc@5  99.22 ( 98.37)
Epoch: [63][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.2295e-01 (4.7270e-01)	Acc@1  85.16 ( 85.72)	Acc@5  96.88 ( 98.34)
Epoch: [63][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.6704e-01 (4.7383e-01)	Acc@1  88.28 ( 85.74)	Acc@5  98.44 ( 98.34)
Epoch: [63][140/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9004e-01 (4.7455e-01)	Acc@1  94.53 ( 85.70)	Acc@5 100.00 ( 98.34)
Epoch: [63][150/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3359e-01 (4.7497e-01)	Acc@1  87.50 ( 85.66)	Acc@5  97.66 ( 98.35)
Epoch: [63][160/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1416e-01 (4.7575e-01)	Acc@1  85.94 ( 85.62)	Acc@5  96.09 ( 98.34)
Epoch: [63][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2627e-01 (4.7314e-01)	Acc@1  88.28 ( 85.73)	Acc@5  96.88 ( 98.35)
Epoch: [63][180/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5166e-01 (4.7182e-01)	Acc@1  85.94 ( 85.75)	Acc@5 100.00 ( 98.39)
Epoch: [63][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4346e-01 (4.7131e-01)	Acc@1  82.03 ( 85.74)	Acc@5  97.66 ( 98.40)
Epoch: [63][200/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4736e-01 (4.7250e-01)	Acc@1  85.16 ( 85.79)	Acc@5  96.09 ( 98.39)
Epoch: [63][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9438e-01 (4.7233e-01)	Acc@1  87.50 ( 85.84)	Acc@5  98.44 ( 98.39)
Epoch: [63][220/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3701e-01 (4.7342e-01)	Acc@1  85.94 ( 85.77)	Acc@5  99.22 ( 98.41)
Epoch: [63][230/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8657e-01 (4.7307e-01)	Acc@1  84.38 ( 85.80)	Acc@5  97.66 ( 98.41)
Epoch: [63][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8013e-01 (4.7142e-01)	Acc@1  87.50 ( 85.81)	Acc@5  98.44 ( 98.43)
Epoch: [63][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0015e-01 (4.6992e-01)	Acc@1  86.72 ( 85.86)	Acc@5  99.22 ( 98.43)
Epoch: [63][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1016e-01 (4.7102e-01)	Acc@1  87.50 ( 85.82)	Acc@5  99.22 ( 98.42)
Epoch: [63][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1064e-01 (4.7019e-01)	Acc@1  89.06 ( 85.86)	Acc@5  99.22 ( 98.44)
Epoch: [63][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8062e-01 (4.6981e-01)	Acc@1  89.06 ( 85.88)	Acc@5  99.22 ( 98.43)
Epoch: [63][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1089e-01 (4.6937e-01)	Acc@1  88.28 ( 85.90)	Acc@5  99.22 ( 98.42)
Epoch: [63][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2441e-01 (4.6929e-01)	Acc@1  81.25 ( 85.91)	Acc@5  99.22 ( 98.42)
Epoch: [63][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1284e-01 (4.6890e-01)	Acc@1  89.84 ( 85.96)	Acc@5  98.44 ( 98.43)
Epoch: [63][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.1724e-01 (4.6852e-01)	Acc@1  88.28 ( 85.99)	Acc@5  98.44 ( 98.42)
Epoch: [63][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.2393e-01 (4.6914e-01)	Acc@1  88.28 ( 85.99)	Acc@5  97.66 ( 98.41)
Epoch: [63][340/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.6948e-01 (4.7003e-01)	Acc@1  87.50 ( 85.95)	Acc@5  98.44 ( 98.41)
Epoch: [63][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.1113e-01 (4.6908e-01)	Acc@1  88.28 ( 86.00)	Acc@5 100.00 ( 98.42)
Epoch: [63][360/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.8159e-01 (4.6806e-01)	Acc@1  89.06 ( 86.03)	Acc@5  99.22 ( 98.43)
Epoch: [63][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.7949e-01 (4.6798e-01)	Acc@1  85.94 ( 86.02)	Acc@5  96.88 ( 98.42)
Epoch: [63][380/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.0479e-01 (4.6831e-01)	Acc@1  89.06 ( 86.02)	Acc@5  99.22 ( 98.43)
Epoch: [63][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.9414e-01 (4.6922e-01)	Acc@1  82.50 ( 85.99)	Acc@5 100.00 ( 98.43)
## e[63] optimizer.zero_grad (sum) time: 0.41028261184692383
## e[63]       loss.backward (sum) time: 7.3428661823272705
## e[63]      optimizer.step (sum) time: 3.521446943283081
## epoch[63] training(only) time: 26.13218641281128
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.1777e+00 (1.1777e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.028 ( 0.038)	Loss 1.1064e+00 (1.2152e+00)	Acc@1  72.00 ( 69.36)	Acc@5  94.00 ( 89.82)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.5654e+00 (1.2099e+00)	Acc@1  64.00 ( 69.05)	Acc@5  89.00 ( 90.29)
Test: [ 30/100]	Time  0.027 ( 0.030)	Loss 1.6826e+00 (1.2440e+00)	Acc@1  59.00 ( 68.03)	Acc@5  88.00 ( 90.03)
Test: [ 40/100]	Time  0.026 ( 0.029)	Loss 1.2002e+00 (1.2210e+00)	Acc@1  71.00 ( 67.90)	Acc@5  91.00 ( 90.46)
Test: [ 50/100]	Time  0.025 ( 0.028)	Loss 1.2715e+00 (1.2264e+00)	Acc@1  65.00 ( 67.55)	Acc@5  90.00 ( 90.65)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2793e+00 (1.2131e+00)	Acc@1  63.00 ( 67.64)	Acc@5  90.00 ( 90.89)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0098e+00 (1.2158e+00)	Acc@1  75.00 ( 67.80)	Acc@5  94.00 ( 90.83)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.2715e+00 (1.2226e+00)	Acc@1  65.00 ( 67.59)	Acc@5  89.00 ( 90.73)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.8105e+00 (1.2129e+00)	Acc@1  54.00 ( 67.67)	Acc@5  84.00 ( 90.82)
 * Acc@1 67.950 Acc@5 90.870
### epoch[63] execution time: 28.95755124092102
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.227 ( 0.227)	Data  0.151 ( 0.151)	Loss 5.0000e-01 (5.0000e-01)	Acc@1  87.50 ( 87.50)	Acc@5  98.44 ( 98.44)
Epoch: [64][ 10/391]	Time  0.069 ( 0.080)	Data  0.001 ( 0.015)	Loss 4.6606e-01 (4.7412e-01)	Acc@1  86.72 ( 87.14)	Acc@5  97.66 ( 97.87)
Epoch: [64][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.4263e-01 (4.5695e-01)	Acc@1  85.94 ( 86.83)	Acc@5  97.66 ( 98.25)
Epoch: [64][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 6.2744e-01 (4.6380e-01)	Acc@1  83.59 ( 86.67)	Acc@5  95.31 ( 98.26)
Epoch: [64][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.5312e-01 (4.6337e-01)	Acc@1  86.72 ( 86.64)	Acc@5  99.22 ( 98.36)
Epoch: [64][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.3262e-01 (4.6920e-01)	Acc@1  86.72 ( 86.12)	Acc@5  99.22 ( 98.42)
Epoch: [64][ 60/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.1284e-01 (4.6437e-01)	Acc@1  85.94 ( 86.18)	Acc@5  99.22 ( 98.44)
Epoch: [64][ 70/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.9990e-01 (4.6108e-01)	Acc@1  90.62 ( 86.36)	Acc@5  98.44 ( 98.48)
Epoch: [64][ 80/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.2261e-01 (4.6341e-01)	Acc@1  87.50 ( 86.19)	Acc@5  97.66 ( 98.45)
Epoch: [64][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.1797e-01 (4.6543e-01)	Acc@1  88.28 ( 86.18)	Acc@5  98.44 ( 98.42)
Epoch: [64][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.3262e-01 (4.6207e-01)	Acc@1  89.06 ( 86.25)	Acc@5  97.66 ( 98.41)
Epoch: [64][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.1816e-01 (4.6241e-01)	Acc@1  79.69 ( 86.23)	Acc@5  97.66 ( 98.46)
Epoch: [64][120/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1235e-01 (4.6547e-01)	Acc@1  87.50 ( 86.18)	Acc@5 100.00 ( 98.43)
Epoch: [64][130/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2090e-01 (4.6746e-01)	Acc@1  91.41 ( 86.15)	Acc@5  99.22 ( 98.39)
Epoch: [64][140/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2065e-01 (4.6730e-01)	Acc@1  89.06 ( 86.13)	Acc@5  99.22 ( 98.39)
Epoch: [64][150/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1333e-01 (4.6559e-01)	Acc@1  86.72 ( 86.20)	Acc@5  99.22 ( 98.40)
Epoch: [64][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8120e-01 (4.6600e-01)	Acc@1  84.38 ( 86.20)	Acc@5  98.44 ( 98.38)
Epoch: [64][170/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7729e-01 (4.6419e-01)	Acc@1  84.38 ( 86.27)	Acc@5  98.44 ( 98.40)
Epoch: [64][180/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2764e-01 (4.6459e-01)	Acc@1  92.97 ( 86.26)	Acc@5  99.22 ( 98.40)
Epoch: [64][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8721e-01 (4.6590e-01)	Acc@1  88.28 ( 86.22)	Acc@5  98.44 ( 98.39)
Epoch: [64][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8379e-01 (4.6434e-01)	Acc@1  86.72 ( 86.28)	Acc@5  99.22 ( 98.40)
Epoch: [64][210/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3481e-01 (4.6428e-01)	Acc@1  86.72 ( 86.29)	Acc@5  96.88 ( 98.41)
Epoch: [64][220/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1367e-01 (4.6332e-01)	Acc@1  81.25 ( 86.32)	Acc@5  98.44 ( 98.42)
Epoch: [64][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6104e-01 (4.6284e-01)	Acc@1  84.38 ( 86.31)	Acc@5  97.66 ( 98.43)
Epoch: [64][240/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4106e-01 (4.6268e-01)	Acc@1  90.62 ( 86.34)	Acc@5 100.00 ( 98.43)
Epoch: [64][250/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2261e-01 (4.6270e-01)	Acc@1  85.94 ( 86.35)	Acc@5  98.44 ( 98.44)
Epoch: [64][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9990e-01 (4.6121e-01)	Acc@1  87.50 ( 86.40)	Acc@5  98.44 ( 98.44)
Epoch: [64][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8193e-01 (4.6252e-01)	Acc@1  85.16 ( 86.36)	Acc@5  98.44 ( 98.41)
Epoch: [64][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4604e-01 (4.6330e-01)	Acc@1  89.06 ( 86.35)	Acc@5  98.44 ( 98.41)
Epoch: [64][290/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4092e-01 (4.6504e-01)	Acc@1  90.62 ( 86.32)	Acc@5  97.66 ( 98.38)
Epoch: [64][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9258e-01 (4.6583e-01)	Acc@1  89.84 ( 86.26)	Acc@5  98.44 ( 98.36)
Epoch: [64][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4849e-01 (4.6544e-01)	Acc@1  86.72 ( 86.30)	Acc@5  97.66 ( 98.37)
Epoch: [64][320/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9883e-01 (4.6466e-01)	Acc@1  92.97 ( 86.32)	Acc@5 100.00 ( 98.38)
Epoch: [64][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2822e-01 (4.6468e-01)	Acc@1  86.72 ( 86.32)	Acc@5 100.00 ( 98.40)
Epoch: [64][340/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6924e-01 (4.6444e-01)	Acc@1  87.50 ( 86.29)	Acc@5  98.44 ( 98.42)
Epoch: [64][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.2969e-01 (4.6610e-01)	Acc@1  88.28 ( 86.23)	Acc@5  96.88 ( 98.41)
Epoch: [64][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.6787e-01 (4.6660e-01)	Acc@1  82.81 ( 86.21)	Acc@5  97.66 ( 98.41)
Epoch: [64][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.9854e-01 (4.6598e-01)	Acc@1  85.16 ( 86.21)	Acc@5  98.44 ( 98.42)
Epoch: [64][380/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.0762e-01 (4.6714e-01)	Acc@1  92.19 ( 86.17)	Acc@5 100.00 ( 98.41)
Epoch: [64][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.7534e-01 (4.6711e-01)	Acc@1  85.00 ( 86.16)	Acc@5 100.00 ( 98.42)
## e[64] optimizer.zero_grad (sum) time: 0.40265750885009766
## e[64]       loss.backward (sum) time: 7.390377759933472
## e[64]      optimizer.step (sum) time: 3.4979755878448486
## epoch[64] training(only) time: 26.110811471939087
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1680e+00 (1.1680e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.1152e+00 (1.2105e+00)	Acc@1  70.00 ( 69.18)	Acc@5  94.00 ( 89.91)
Test: [ 20/100]	Time  0.029 ( 0.033)	Loss 1.5449e+00 (1.2111e+00)	Acc@1  66.00 ( 68.86)	Acc@5  90.00 ( 90.38)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6680e+00 (1.2446e+00)	Acc@1  59.00 ( 67.74)	Acc@5  88.00 ( 90.00)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.1816e+00 (1.2176e+00)	Acc@1  69.00 ( 67.76)	Acc@5  91.00 ( 90.63)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2588e+00 (1.2212e+00)	Acc@1  64.00 ( 67.39)	Acc@5  90.00 ( 90.71)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2656e+00 (1.2093e+00)	Acc@1  61.00 ( 67.54)	Acc@5  91.00 ( 90.90)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0049e+00 (1.2108e+00)	Acc@1  76.00 ( 67.75)	Acc@5  92.00 ( 90.85)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2891e+00 (1.2172e+00)	Acc@1  66.00 ( 67.54)	Acc@5  89.00 ( 90.72)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.8066e+00 (1.2081e+00)	Acc@1  53.00 ( 67.63)	Acc@5  85.00 ( 90.84)
 * Acc@1 67.790 Acc@5 90.860
### epoch[64] execution time: 28.93058681488037
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.223 ( 0.223)	Data  0.147 ( 0.147)	Loss 4.7290e-01 (4.7290e-01)	Acc@1  80.47 ( 80.47)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.014)	Loss 4.6802e-01 (4.5199e-01)	Acc@1  84.38 ( 86.01)	Acc@5  98.44 ( 98.93)
Epoch: [65][ 20/391]	Time  0.068 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.6167e-01 (4.5219e-01)	Acc@1  86.72 ( 86.01)	Acc@5  98.44 ( 98.92)
Epoch: [65][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 6.0107e-01 (4.5183e-01)	Acc@1  82.81 ( 86.42)	Acc@5  99.22 ( 98.82)
Epoch: [65][ 40/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.6899e-01 (4.5596e-01)	Acc@1  85.94 ( 86.30)	Acc@5 100.00 ( 98.84)
Epoch: [65][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 3.5010e-01 (4.5502e-01)	Acc@1  89.84 ( 86.35)	Acc@5  98.44 ( 98.81)
Epoch: [65][ 60/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.4424e-01 (4.5909e-01)	Acc@1  85.94 ( 86.13)	Acc@5 100.00 ( 98.73)
Epoch: [65][ 70/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.0977e-01 (4.5480e-01)	Acc@1  85.94 ( 86.31)	Acc@5  96.88 ( 98.75)
Epoch: [65][ 80/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.6973e-01 (4.5370e-01)	Acc@1  85.94 ( 86.42)	Acc@5  99.22 ( 98.75)
Epoch: [65][ 90/391]	Time  0.076 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.7666e-01 (4.5758e-01)	Acc@1  80.47 ( 86.26)	Acc@5  98.44 ( 98.76)
Epoch: [65][100/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.2588e-01 (4.6254e-01)	Acc@1  86.72 ( 86.19)	Acc@5  97.66 ( 98.69)
Epoch: [65][110/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1074e-01 (4.6401e-01)	Acc@1  88.28 ( 86.10)	Acc@5  94.53 ( 98.63)
Epoch: [65][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.0986e-01 (4.6829e-01)	Acc@1  81.25 ( 85.95)	Acc@5  96.09 ( 98.57)
Epoch: [65][130/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7520e-01 (4.6832e-01)	Acc@1  82.03 ( 85.94)	Acc@5  97.66 ( 98.54)
Epoch: [65][140/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3223e-01 (4.6971e-01)	Acc@1  80.47 ( 85.97)	Acc@5  98.44 ( 98.50)
Epoch: [65][150/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0137e-01 (4.6803e-01)	Acc@1  85.94 ( 85.97)	Acc@5 100.00 ( 98.52)
Epoch: [65][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.7920e-01 (4.6755e-01)	Acc@1  78.12 ( 85.99)	Acc@5  96.88 ( 98.50)
Epoch: [65][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1284e-01 (4.6808e-01)	Acc@1  85.94 ( 85.98)	Acc@5  99.22 ( 98.48)
Epoch: [65][180/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4507e-01 (4.6997e-01)	Acc@1  85.94 ( 85.96)	Acc@5  99.22 ( 98.45)
Epoch: [65][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1025e-01 (4.6819e-01)	Acc@1  86.72 ( 86.07)	Acc@5  96.88 ( 98.44)
Epoch: [65][200/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8354e-01 (4.6637e-01)	Acc@1  88.28 ( 86.12)	Acc@5 100.00 ( 98.45)
Epoch: [65][210/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7817e-01 (4.6643e-01)	Acc@1  88.28 ( 86.14)	Acc@5 100.00 ( 98.45)
Epoch: [65][220/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8086e-01 (4.6481e-01)	Acc@1  89.06 ( 86.23)	Acc@5  99.22 ( 98.49)
Epoch: [65][230/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2529e-01 (4.6653e-01)	Acc@1  88.28 ( 86.16)	Acc@5  98.44 ( 98.46)
Epoch: [65][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9307e-01 (4.6696e-01)	Acc@1  92.19 ( 86.14)	Acc@5  97.66 ( 98.47)
Epoch: [65][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7827e-01 (4.6685e-01)	Acc@1  87.50 ( 86.16)	Acc@5  99.22 ( 98.46)
Epoch: [65][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9868e-01 (4.6530e-01)	Acc@1  88.28 ( 86.18)	Acc@5  99.22 ( 98.49)
Epoch: [65][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7632e-01 (4.6453e-01)	Acc@1  86.72 ( 86.21)	Acc@5  98.44 ( 98.47)
Epoch: [65][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9209e-01 (4.6434e-01)	Acc@1  84.38 ( 86.17)	Acc@5 100.00 ( 98.49)
Epoch: [65][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8584e-01 (4.6427e-01)	Acc@1  88.28 ( 86.19)	Acc@5  99.22 ( 98.48)
Epoch: [65][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7461e-01 (4.6394e-01)	Acc@1  87.50 ( 86.19)	Acc@5  98.44 ( 98.49)
Epoch: [65][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0293e-01 (4.6403e-01)	Acc@1  83.59 ( 86.19)	Acc@5  98.44 ( 98.50)
Epoch: [65][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1807e-01 (4.6444e-01)	Acc@1  85.94 ( 86.17)	Acc@5  98.44 ( 98.50)
Epoch: [65][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9355e-01 (4.6415e-01)	Acc@1  89.06 ( 86.16)	Acc@5  98.44 ( 98.52)
Epoch: [65][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1123e-01 (4.6425e-01)	Acc@1  84.38 ( 86.15)	Acc@5  96.88 ( 98.52)
Epoch: [65][350/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1187e-01 (4.6445e-01)	Acc@1  86.72 ( 86.14)	Acc@5  99.22 ( 98.52)
Epoch: [65][360/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5898e-01 (4.6513e-01)	Acc@1  85.16 ( 86.08)	Acc@5  97.66 ( 98.51)
Epoch: [65][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1357e-01 (4.6408e-01)	Acc@1  89.84 ( 86.13)	Acc@5  98.44 ( 98.52)
Epoch: [65][380/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.9575e-01 (4.6277e-01)	Acc@1  89.84 ( 86.19)	Acc@5  98.44 ( 98.53)
Epoch: [65][390/391]	Time  0.047 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.0879e-01 (4.6325e-01)	Acc@1  75.00 ( 86.14)	Acc@5 100.00 ( 98.51)
## e[65] optimizer.zero_grad (sum) time: 0.4071669578552246
## e[65]       loss.backward (sum) time: 7.4232096672058105
## e[65]      optimizer.step (sum) time: 3.573979377746582
## epoch[65] training(only) time: 26.286267280578613
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.1650e+00 (1.1650e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1279e+00 (1.2185e+00)	Acc@1  70.00 ( 69.82)	Acc@5  94.00 ( 89.91)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5938e+00 (1.2103e+00)	Acc@1  63.00 ( 69.24)	Acc@5  89.00 ( 90.48)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6885e+00 (1.2442e+00)	Acc@1  58.00 ( 68.10)	Acc@5  88.00 ( 90.06)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.1895e+00 (1.2190e+00)	Acc@1  71.00 ( 68.15)	Acc@5  91.00 ( 90.51)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2627e+00 (1.2240e+00)	Acc@1  69.00 ( 67.90)	Acc@5  90.00 ( 90.59)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2881e+00 (1.2116e+00)	Acc@1  61.00 ( 68.07)	Acc@5  91.00 ( 90.82)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0117e+00 (1.2145e+00)	Acc@1  73.00 ( 68.15)	Acc@5  93.00 ( 90.83)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3008e+00 (1.2204e+00)	Acc@1  65.00 ( 67.86)	Acc@5  89.00 ( 90.72)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8604e+00 (1.2118e+00)	Acc@1  53.00 ( 67.91)	Acc@5  85.00 ( 90.81)
 * Acc@1 68.130 Acc@5 90.840
### epoch[65] execution time: 29.09986925125122
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.214 ( 0.214)	Data  0.134 ( 0.134)	Loss 3.2446e-01 (3.2446e-01)	Acc@1  92.97 ( 92.97)	Acc@5  98.44 ( 98.44)
Epoch: [66][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.013)	Loss 4.9023e-01 (4.2742e-01)	Acc@1  86.72 ( 87.93)	Acc@5  98.44 ( 98.58)
Epoch: [66][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.007)	Loss 5.2295e-01 (4.5129e-01)	Acc@1  85.16 ( 86.98)	Acc@5  97.66 ( 98.40)
Epoch: [66][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.9487e-01 (4.5179e-01)	Acc@1  85.94 ( 87.12)	Acc@5  98.44 ( 98.34)
Epoch: [66][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.0186e-01 (4.4551e-01)	Acc@1  86.72 ( 87.37)	Acc@5  97.66 ( 98.38)
Epoch: [66][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 3.8086e-01 (4.4716e-01)	Acc@1  88.28 ( 87.25)	Acc@5  99.22 ( 98.45)
Epoch: [66][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.3711e-01 (4.5778e-01)	Acc@1  81.25 ( 86.85)	Acc@5  98.44 ( 98.35)
Epoch: [66][ 70/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.2529e-01 (4.5897e-01)	Acc@1  86.72 ( 86.77)	Acc@5  98.44 ( 98.37)
Epoch: [66][ 80/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.4727e-01 (4.5830e-01)	Acc@1  86.72 ( 86.70)	Acc@5  98.44 ( 98.41)
Epoch: [66][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.8853e-01 (4.6286e-01)	Acc@1  85.94 ( 86.54)	Acc@5  98.44 ( 98.39)
Epoch: [66][100/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0161e-01 (4.6239e-01)	Acc@1  88.28 ( 86.56)	Acc@5  98.44 ( 98.38)
Epoch: [66][110/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8706e-01 (4.6171e-01)	Acc@1  89.84 ( 86.51)	Acc@5  95.31 ( 98.40)
Epoch: [66][120/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9600e-01 (4.6050e-01)	Acc@1  88.28 ( 86.43)	Acc@5 100.00 ( 98.42)
Epoch: [66][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (4.6102e-01)	Acc@1  89.84 ( 86.43)	Acc@5  99.22 ( 98.41)
Epoch: [66][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.3813e-01 (4.6094e-01)	Acc@1  91.41 ( 86.46)	Acc@5  99.22 ( 98.42)
Epoch: [66][150/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.4155e-01 (4.5916e-01)	Acc@1  91.41 ( 86.48)	Acc@5  99.22 ( 98.44)
Epoch: [66][160/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6899e-01 (4.5847e-01)	Acc@1  83.59 ( 86.47)	Acc@5  97.66 ( 98.44)
Epoch: [66][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6934e-01 (4.5836e-01)	Acc@1  83.59 ( 86.47)	Acc@5  97.66 ( 98.45)
Epoch: [66][180/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5581e-01 (4.5742e-01)	Acc@1  85.16 ( 86.46)	Acc@5  98.44 ( 98.47)
Epoch: [66][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9365e-01 (4.5731e-01)	Acc@1  85.94 ( 86.47)	Acc@5  99.22 ( 98.48)
Epoch: [66][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0098e-01 (4.5695e-01)	Acc@1  87.50 ( 86.47)	Acc@5  98.44 ( 98.49)
Epoch: [66][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5166e-01 (4.5810e-01)	Acc@1  85.16 ( 86.42)	Acc@5  99.22 ( 98.47)
Epoch: [66][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8428e-01 (4.5907e-01)	Acc@1  87.50 ( 86.36)	Acc@5  99.22 ( 98.46)
Epoch: [66][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9927e-01 (4.6010e-01)	Acc@1  87.50 ( 86.38)	Acc@5  97.66 ( 98.43)
Epoch: [66][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9658e-01 (4.6128e-01)	Acc@1  85.16 ( 86.34)	Acc@5  97.66 ( 98.43)
Epoch: [66][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1968e-01 (4.6294e-01)	Acc@1  88.28 ( 86.26)	Acc@5 100.00 ( 98.42)
Epoch: [66][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7500e-01 (4.6164e-01)	Acc@1  89.84 ( 86.31)	Acc@5  99.22 ( 98.42)
Epoch: [66][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6436e-01 (4.6108e-01)	Acc@1  88.28 ( 86.34)	Acc@5  98.44 ( 98.41)
Epoch: [66][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8633e-01 (4.6062e-01)	Acc@1  89.06 ( 86.37)	Acc@5  97.66 ( 98.42)
Epoch: [66][290/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6606e-01 (4.6039e-01)	Acc@1  84.38 ( 86.38)	Acc@5  97.66 ( 98.43)
Epoch: [66][300/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6816e-01 (4.6139e-01)	Acc@1  90.62 ( 86.36)	Acc@5  99.22 ( 98.42)
Epoch: [66][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6299e-01 (4.6182e-01)	Acc@1  81.25 ( 86.34)	Acc@5  96.88 ( 98.40)
Epoch: [66][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1904e-01 (4.6193e-01)	Acc@1  82.03 ( 86.31)	Acc@5  97.66 ( 98.40)
Epoch: [66][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6021e-01 (4.6186e-01)	Acc@1  85.94 ( 86.32)	Acc@5  98.44 ( 98.41)
Epoch: [66][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.2578e-01 (4.6245e-01)	Acc@1  86.72 ( 86.31)	Acc@5  98.44 ( 98.40)
Epoch: [66][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.6450e-01 (4.6111e-01)	Acc@1  89.06 ( 86.34)	Acc@5  98.44 ( 98.42)
Epoch: [66][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.5107e-01 (4.6060e-01)	Acc@1  91.41 ( 86.36)	Acc@5  98.44 ( 98.43)
Epoch: [66][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.3726e-01 (4.6132e-01)	Acc@1  88.28 ( 86.34)	Acc@5  99.22 ( 98.42)
Epoch: [66][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.3286e-01 (4.6196e-01)	Acc@1  86.72 ( 86.35)	Acc@5  99.22 ( 98.42)
Epoch: [66][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.1382e-01 (4.6092e-01)	Acc@1  92.50 ( 86.39)	Acc@5 100.00 ( 98.44)
## e[66] optimizer.zero_grad (sum) time: 0.40604233741760254
## e[66]       loss.backward (sum) time: 7.359716176986694
## e[66]      optimizer.step (sum) time: 3.536144971847534
## epoch[66] training(only) time: 26.11254596710205
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.1426e+00 (1.1426e+00)	Acc@1  68.00 ( 68.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.1328e+00 (1.2188e+00)	Acc@1  71.00 ( 69.55)	Acc@5  94.00 ( 90.27)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.5811e+00 (1.2091e+00)	Acc@1  65.00 ( 69.19)	Acc@5  88.00 ( 90.29)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7031e+00 (1.2430e+00)	Acc@1  57.00 ( 68.13)	Acc@5  85.00 ( 89.74)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1855e+00 (1.2194e+00)	Acc@1  67.00 ( 67.93)	Acc@5  91.00 ( 90.41)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2598e+00 (1.2234e+00)	Acc@1  68.00 ( 67.73)	Acc@5  90.00 ( 90.55)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.2842e+00 (1.2103e+00)	Acc@1  62.00 ( 67.97)	Acc@5  91.00 ( 90.75)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.9951e-01 (1.2117e+00)	Acc@1  74.00 ( 68.18)	Acc@5  93.00 ( 90.70)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 1.2686e+00 (1.2188e+00)	Acc@1  67.00 ( 67.88)	Acc@5  89.00 ( 90.62)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.8477e+00 (1.2104e+00)	Acc@1  53.00 ( 67.87)	Acc@5  83.00 ( 90.69)
 * Acc@1 68.040 Acc@5 90.730
### epoch[66] execution time: 28.948680877685547
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.227 ( 0.227)	Data  0.149 ( 0.149)	Loss 4.3506e-01 (4.3506e-01)	Acc@1  86.72 ( 86.72)	Acc@5  98.44 ( 98.44)
Epoch: [67][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.015)	Loss 6.3330e-01 (5.2659e-01)	Acc@1  79.69 ( 84.09)	Acc@5  98.44 ( 98.01)
Epoch: [67][ 20/391]	Time  0.067 ( 0.075)	Data  0.001 ( 0.008)	Loss 3.9087e-01 (4.8079e-01)	Acc@1  89.06 ( 85.34)	Acc@5  98.44 ( 98.47)
Epoch: [67][ 30/391]	Time  0.067 ( 0.072)	Data  0.001 ( 0.006)	Loss 5.1367e-01 (4.8215e-01)	Acc@1  82.03 ( 85.36)	Acc@5  98.44 ( 98.46)
Epoch: [67][ 40/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.0625e-01 (4.6826e-01)	Acc@1  86.72 ( 86.11)	Acc@5 100.00 ( 98.49)
Epoch: [67][ 50/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.9805e-01 (4.5897e-01)	Acc@1  86.72 ( 86.27)	Acc@5  96.88 ( 98.56)
Epoch: [67][ 60/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.6548e-01 (4.5119e-01)	Acc@1  86.72 ( 86.58)	Acc@5 100.00 ( 98.54)
Epoch: [67][ 70/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.7437e-01 (4.5232e-01)	Acc@1  85.94 ( 86.54)	Acc@5  96.88 ( 98.51)
Epoch: [67][ 80/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.8706e-01 (4.4713e-01)	Acc@1  85.94 ( 86.75)	Acc@5  99.22 ( 98.53)
Epoch: [67][ 90/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.0444e-01 (4.4649e-01)	Acc@1  92.97 ( 86.78)	Acc@5  99.22 ( 98.55)
Epoch: [67][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.5596e-01 (4.4425e-01)	Acc@1  90.62 ( 86.92)	Acc@5  99.22 ( 98.58)
Epoch: [67][110/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5435e-01 (4.4561e-01)	Acc@1  86.72 ( 86.88)	Acc@5  98.44 ( 98.57)
Epoch: [67][120/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.8745e-01 (4.4599e-01)	Acc@1  85.94 ( 86.78)	Acc@5  99.22 ( 98.58)
Epoch: [67][130/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.8306e-01 (4.4721e-01)	Acc@1  87.50 ( 86.78)	Acc@5 100.00 ( 98.55)
Epoch: [67][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1807e-01 (4.4684e-01)	Acc@1  85.94 ( 86.74)	Acc@5  97.66 ( 98.53)
Epoch: [67][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5078e-01 (4.4800e-01)	Acc@1  85.16 ( 86.75)	Acc@5  97.66 ( 98.53)
Epoch: [67][160/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0186e-01 (4.4939e-01)	Acc@1  85.94 ( 86.72)	Acc@5  99.22 ( 98.52)
Epoch: [67][170/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9634e-01 (4.5036e-01)	Acc@1  78.91 ( 86.68)	Acc@5 100.00 ( 98.53)
Epoch: [67][180/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5371e-01 (4.5128e-01)	Acc@1  82.03 ( 86.64)	Acc@5  97.66 ( 98.53)
Epoch: [67][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1953e-01 (4.5316e-01)	Acc@1  85.94 ( 86.55)	Acc@5  96.88 ( 98.52)
Epoch: [67][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5239e-01 (4.5469e-01)	Acc@1  84.38 ( 86.44)	Acc@5 100.00 ( 98.52)
Epoch: [67][210/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8657e-01 (4.5498e-01)	Acc@1  90.62 ( 86.49)	Acc@5  96.88 ( 98.52)
Epoch: [67][220/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9243e-01 (4.5467e-01)	Acc@1  83.59 ( 86.55)	Acc@5  96.88 ( 98.50)
Epoch: [67][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0527e-01 (4.5509e-01)	Acc@1  92.97 ( 86.57)	Acc@5  99.22 ( 98.51)
Epoch: [67][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4590e-01 (4.5529e-01)	Acc@1  83.59 ( 86.53)	Acc@5  98.44 ( 98.50)
Epoch: [67][250/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6304e-01 (4.5550e-01)	Acc@1  92.19 ( 86.56)	Acc@5  99.22 ( 98.51)
Epoch: [67][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5288e-01 (4.5588e-01)	Acc@1  86.72 ( 86.55)	Acc@5  98.44 ( 98.51)
Epoch: [67][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5273e-01 (4.5539e-01)	Acc@1  84.38 ( 86.55)	Acc@5  96.88 ( 98.51)
Epoch: [67][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9087e-01 (4.5649e-01)	Acc@1  85.16 ( 86.49)	Acc@5 100.00 ( 98.51)
Epoch: [67][290/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9033e-01 (4.5702e-01)	Acc@1  82.03 ( 86.46)	Acc@5  97.66 ( 98.50)
Epoch: [67][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.3740e-01 (4.5637e-01)	Acc@1  90.62 ( 86.45)	Acc@5 100.00 ( 98.52)
Epoch: [67][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3906e-01 (4.5651e-01)	Acc@1  82.81 ( 86.43)	Acc@5  98.44 ( 98.52)
Epoch: [67][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0308e-01 (4.5669e-01)	Acc@1  87.50 ( 86.40)	Acc@5  98.44 ( 98.51)
Epoch: [67][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3286e-01 (4.5674e-01)	Acc@1  85.94 ( 86.42)	Acc@5  98.44 ( 98.51)
Epoch: [67][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0259e-01 (4.5647e-01)	Acc@1  87.50 ( 86.43)	Acc@5  99.22 ( 98.50)
Epoch: [67][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2578e-01 (4.5631e-01)	Acc@1  88.28 ( 86.46)	Acc@5  99.22 ( 98.50)
Epoch: [67][360/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2881e-01 (4.5500e-01)	Acc@1  86.72 ( 86.52)	Acc@5  96.88 ( 98.51)
Epoch: [67][370/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2202e-01 (4.5402e-01)	Acc@1  94.53 ( 86.55)	Acc@5 100.00 ( 98.53)
Epoch: [67][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.6133e-01 (4.5327e-01)	Acc@1  89.84 ( 86.56)	Acc@5  97.66 ( 98.53)
Epoch: [67][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.0400e-01 (4.5410e-01)	Acc@1  80.00 ( 86.54)	Acc@5  96.25 ( 98.52)
## e[67] optimizer.zero_grad (sum) time: 0.4147772789001465
## e[67]       loss.backward (sum) time: 7.397599458694458
## e[67]      optimizer.step (sum) time: 3.599031686782837
## epoch[67] training(only) time: 26.29986572265625
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.1416e+00 (1.1416e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1338e+00 (1.2168e+00)	Acc@1  71.00 ( 70.00)	Acc@5  94.00 ( 90.18)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.5859e+00 (1.2115e+00)	Acc@1  66.00 ( 69.00)	Acc@5  89.00 ( 90.52)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6660e+00 (1.2452e+00)	Acc@1  60.00 ( 68.06)	Acc@5  88.00 ( 90.06)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1680e+00 (1.2201e+00)	Acc@1  70.00 ( 68.05)	Acc@5  91.00 ( 90.56)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2480e+00 (1.2235e+00)	Acc@1  66.00 ( 67.76)	Acc@5  90.00 ( 90.73)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3066e+00 (1.2109e+00)	Acc@1  63.00 ( 67.98)	Acc@5  90.00 ( 90.97)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.9951e-01 (1.2120e+00)	Acc@1  77.00 ( 68.17)	Acc@5  94.00 ( 90.94)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2910e+00 (1.2187e+00)	Acc@1  66.00 ( 67.94)	Acc@5  89.00 ( 90.85)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8096e+00 (1.2099e+00)	Acc@1  56.00 ( 68.00)	Acc@5  84.00 ( 90.90)
 * Acc@1 68.270 Acc@5 90.920
### epoch[67] execution time: 29.1088285446167
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.224 ( 0.224)	Data  0.146 ( 0.146)	Loss 3.2349e-01 (3.2349e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.069 ( 0.082)	Data  0.001 ( 0.014)	Loss 4.1992e-01 (4.5978e-01)	Acc@1  86.72 ( 86.51)	Acc@5 100.00 ( 99.01)
Epoch: [68][ 20/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.6289e-01 (4.6473e-01)	Acc@1  88.28 ( 86.20)	Acc@5  98.44 ( 98.70)
Epoch: [68][ 30/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.006)	Loss 4.0308e-01 (4.5652e-01)	Acc@1  89.06 ( 86.69)	Acc@5  99.22 ( 98.56)
Epoch: [68][ 40/391]	Time  0.070 ( 0.071)	Data  0.001 ( 0.005)	Loss 5.7764e-01 (4.6298e-01)	Acc@1  81.25 ( 86.26)	Acc@5  98.44 ( 98.55)
Epoch: [68][ 50/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 3.3691e-01 (4.5376e-01)	Acc@1  88.28 ( 86.49)	Acc@5 100.00 ( 98.67)
Epoch: [68][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.4492e-01 (4.5441e-01)	Acc@1  82.03 ( 86.37)	Acc@5  96.88 ( 98.63)
Epoch: [68][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.0391e-01 (4.5431e-01)	Acc@1  83.59 ( 86.33)	Acc@5  97.66 ( 98.62)
Epoch: [68][ 80/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.0308e-01 (4.5808e-01)	Acc@1  86.72 ( 86.18)	Acc@5  98.44 ( 98.57)
Epoch: [68][ 90/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.0356e-01 (4.5625e-01)	Acc@1  86.72 ( 86.20)	Acc@5  99.22 ( 98.60)
Epoch: [68][100/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.9062e-01 (4.5577e-01)	Acc@1  86.72 ( 86.15)	Acc@5  97.66 ( 98.58)
Epoch: [68][110/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.6948e-01 (4.5522e-01)	Acc@1  89.06 ( 86.20)	Acc@5  97.66 ( 98.61)
Epoch: [68][120/391]	Time  0.077 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3921e-01 (4.5529e-01)	Acc@1  85.16 ( 86.16)	Acc@5  99.22 ( 98.60)
Epoch: [68][130/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.6631e-01 (4.5689e-01)	Acc@1  89.84 ( 86.12)	Acc@5  99.22 ( 98.61)
Epoch: [68][140/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.2236e-01 (4.5631e-01)	Acc@1  89.84 ( 86.13)	Acc@5  96.88 ( 98.61)
Epoch: [68][150/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.5547e-01 (4.5604e-01)	Acc@1  88.28 ( 86.21)	Acc@5 100.00 ( 98.63)
Epoch: [68][160/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.1064e-01 (4.5483e-01)	Acc@1  89.06 ( 86.28)	Acc@5 100.00 ( 98.63)
Epoch: [68][170/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.2539e-01 (4.5976e-01)	Acc@1  84.38 ( 86.11)	Acc@5  96.88 ( 98.57)
Epoch: [68][180/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.0586e-01 (4.5690e-01)	Acc@1  84.38 ( 86.25)	Acc@5  98.44 ( 98.60)
Epoch: [68][190/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.5327e-01 (4.5725e-01)	Acc@1  88.28 ( 86.26)	Acc@5 100.00 ( 98.59)
Epoch: [68][200/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5044e-01 (4.5781e-01)	Acc@1  85.94 ( 86.21)	Acc@5  97.66 ( 98.57)
Epoch: [68][210/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.4595e-01 (4.5946e-01)	Acc@1  90.62 ( 86.21)	Acc@5 100.00 ( 98.55)
Epoch: [68][220/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9478e-01 (4.5906e-01)	Acc@1  89.84 ( 86.25)	Acc@5  98.44 ( 98.55)
Epoch: [68][230/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3066e-01 (4.5920e-01)	Acc@1  89.06 ( 86.29)	Acc@5  98.44 ( 98.53)
Epoch: [68][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1470e-01 (4.5740e-01)	Acc@1  91.41 ( 86.35)	Acc@5 100.00 ( 98.56)
Epoch: [68][250/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3677e-01 (4.5587e-01)	Acc@1  86.72 ( 86.42)	Acc@5  97.66 ( 98.57)
Epoch: [68][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8696e-01 (4.5685e-01)	Acc@1  86.72 ( 86.33)	Acc@5  99.22 ( 98.56)
Epoch: [68][270/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4326e-01 (4.5711e-01)	Acc@1  89.84 ( 86.29)	Acc@5  98.44 ( 98.56)
Epoch: [68][280/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7207e-01 (4.5694e-01)	Acc@1  90.62 ( 86.32)	Acc@5  97.66 ( 98.55)
Epoch: [68][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6704e-01 (4.5741e-01)	Acc@1  86.72 ( 86.32)	Acc@5  98.44 ( 98.54)
Epoch: [68][300/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5459e-01 (4.5714e-01)	Acc@1  85.16 ( 86.32)	Acc@5  97.66 ( 98.54)
Epoch: [68][310/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2446e-01 (4.5747e-01)	Acc@1  92.97 ( 86.32)	Acc@5  98.44 ( 98.52)
Epoch: [68][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3555e-01 (4.5739e-01)	Acc@1  85.16 ( 86.33)	Acc@5  99.22 ( 98.52)
Epoch: [68][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8462e-01 (4.5727e-01)	Acc@1  85.94 ( 86.35)	Acc@5  97.66 ( 98.51)
Epoch: [68][340/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0137e-01 (4.5590e-01)	Acc@1  88.28 ( 86.37)	Acc@5  98.44 ( 98.52)
Epoch: [68][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9331e-01 (4.5640e-01)	Acc@1  85.16 ( 86.36)	Acc@5  98.44 ( 98.51)
Epoch: [68][360/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (4.5622e-01)	Acc@1  87.50 ( 86.37)	Acc@5  97.66 ( 98.52)
Epoch: [68][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.1953e-01 (4.5674e-01)	Acc@1  87.50 ( 86.39)	Acc@5  98.44 ( 98.50)
Epoch: [68][380/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.0674e-01 (4.5662e-01)	Acc@1  89.84 ( 86.37)	Acc@5 100.00 ( 98.52)
Epoch: [68][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.7529e-01 (4.5718e-01)	Acc@1  80.00 ( 86.36)	Acc@5  98.75 ( 98.52)
## e[68] optimizer.zero_grad (sum) time: 0.4120907783508301
## e[68]       loss.backward (sum) time: 7.366655349731445
## e[68]      optimizer.step (sum) time: 3.6287243366241455
## epoch[68] training(only) time: 26.291261434555054
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.1582e+00 (1.1582e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1572e+00 (1.2134e+00)	Acc@1  70.00 ( 69.64)	Acc@5  93.00 ( 89.91)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5664e+00 (1.2096e+00)	Acc@1  65.00 ( 69.24)	Acc@5  90.00 ( 90.24)
Test: [ 30/100]	Time  0.033 ( 0.031)	Loss 1.6719e+00 (1.2434e+00)	Acc@1  58.00 ( 68.03)	Acc@5  86.00 ( 89.84)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1846e+00 (1.2193e+00)	Acc@1  69.00 ( 67.85)	Acc@5  91.00 ( 90.39)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2441e+00 (1.2232e+00)	Acc@1  67.00 ( 67.47)	Acc@5  90.00 ( 90.57)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2959e+00 (1.2125e+00)	Acc@1  61.00 ( 67.48)	Acc@5  91.00 ( 90.75)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0156e+00 (1.2149e+00)	Acc@1  77.00 ( 67.76)	Acc@5  92.00 ( 90.70)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.3145e+00 (1.2214e+00)	Acc@1  66.00 ( 67.54)	Acc@5  89.00 ( 90.62)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8164e+00 (1.2128e+00)	Acc@1  55.00 ( 67.60)	Acc@5  84.00 ( 90.73)
 * Acc@1 67.860 Acc@5 90.790
### epoch[68] execution time: 29.148281812667847
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.217 ( 0.217)	Data  0.139 ( 0.139)	Loss 2.8394e-01 (2.8394e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.071 ( 0.079)	Data  0.001 ( 0.014)	Loss 3.1396e-01 (4.0605e-01)	Acc@1  91.41 ( 88.78)	Acc@5 100.00 ( 98.51)
Epoch: [69][ 20/391]	Time  0.070 ( 0.074)	Data  0.001 ( 0.008)	Loss 5.6299e-01 (4.3429e-01)	Acc@1  85.94 ( 87.69)	Acc@5  96.88 ( 98.44)
Epoch: [69][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 3.4644e-01 (4.4053e-01)	Acc@1  90.62 ( 87.20)	Acc@5  99.22 ( 98.46)
Epoch: [69][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.1406e-01 (4.4893e-01)	Acc@1  86.72 ( 86.78)	Acc@5 100.00 ( 98.44)
Epoch: [69][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.3042e-01 (4.4966e-01)	Acc@1  86.72 ( 86.87)	Acc@5  98.44 ( 98.42)
Epoch: [69][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.0537e-01 (4.5205e-01)	Acc@1  86.72 ( 86.96)	Acc@5  98.44 ( 98.37)
Epoch: [69][ 70/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.6265e-01 (4.5397e-01)	Acc@1  90.62 ( 86.93)	Acc@5  98.44 ( 98.38)
Epoch: [69][ 80/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.1821e-01 (4.5042e-01)	Acc@1  85.16 ( 86.94)	Acc@5  99.22 ( 98.44)
Epoch: [69][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.2627e-01 (4.5573e-01)	Acc@1  88.28 ( 86.79)	Acc@5  98.44 ( 98.40)
Epoch: [69][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.4736e-01 (4.5441e-01)	Acc@1  84.38 ( 86.82)	Acc@5  98.44 ( 98.43)
Epoch: [69][110/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7266e-01 (4.5775e-01)	Acc@1  86.72 ( 86.80)	Acc@5  98.44 ( 98.40)
Epoch: [69][120/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.3643e-01 (4.5741e-01)	Acc@1  92.97 ( 86.84)	Acc@5  98.44 ( 98.40)
Epoch: [69][130/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0845e-01 (4.5667e-01)	Acc@1  90.62 ( 86.85)	Acc@5  99.22 ( 98.42)
Epoch: [69][140/391]	Time  0.074 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9282e-01 (4.5395e-01)	Acc@1  89.84 ( 86.93)	Acc@5  99.22 ( 98.47)
Epoch: [69][150/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7778e-01 (4.5625e-01)	Acc@1  82.81 ( 86.80)	Acc@5  98.44 ( 98.44)
Epoch: [69][160/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9307e-01 (4.5403e-01)	Acc@1  90.62 ( 86.91)	Acc@5 100.00 ( 98.45)
Epoch: [69][170/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9331e-01 (4.5378e-01)	Acc@1  88.28 ( 86.88)	Acc@5 100.00 ( 98.44)
Epoch: [69][180/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1270e-01 (4.5421e-01)	Acc@1  80.47 ( 86.81)	Acc@5  98.44 ( 98.45)
Epoch: [69][190/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1270e-01 (4.5520e-01)	Acc@1  81.25 ( 86.71)	Acc@5  99.22 ( 98.47)
Epoch: [69][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2822e-01 (4.5580e-01)	Acc@1  87.50 ( 86.70)	Acc@5  98.44 ( 98.45)
Epoch: [69][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9731e-01 (4.5639e-01)	Acc@1  82.81 ( 86.70)	Acc@5  98.44 ( 98.43)
Epoch: [69][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7803e-01 (4.5581e-01)	Acc@1  85.16 ( 86.71)	Acc@5  98.44 ( 98.44)
Epoch: [69][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7324e-01 (4.5578e-01)	Acc@1  84.38 ( 86.68)	Acc@5  97.66 ( 98.44)
Epoch: [69][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3955e-01 (4.5703e-01)	Acc@1  84.38 ( 86.60)	Acc@5  96.88 ( 98.44)
Epoch: [69][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9097e-01 (4.5772e-01)	Acc@1  82.03 ( 86.54)	Acc@5  99.22 ( 98.46)
Epoch: [69][260/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6313e-01 (4.5824e-01)	Acc@1  85.94 ( 86.52)	Acc@5  99.22 ( 98.47)
Epoch: [69][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7656e-01 (4.5761e-01)	Acc@1  80.47 ( 86.53)	Acc@5  99.22 ( 98.47)
Epoch: [69][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3091e-01 (4.5651e-01)	Acc@1  87.50 ( 86.56)	Acc@5  99.22 ( 98.48)
Epoch: [69][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6841e-01 (4.5642e-01)	Acc@1  89.84 ( 86.54)	Acc@5  98.44 ( 98.49)
Epoch: [69][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5078e-01 (4.5710e-01)	Acc@1  82.03 ( 86.50)	Acc@5  97.66 ( 98.48)
Epoch: [69][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8525e-01 (4.5767e-01)	Acc@1  89.06 ( 86.50)	Acc@5  98.44 ( 98.47)
Epoch: [69][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3945e-01 (4.5770e-01)	Acc@1  88.28 ( 86.50)	Acc@5  99.22 ( 98.48)
Epoch: [69][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1123e-01 (4.5867e-01)	Acc@1  85.16 ( 86.49)	Acc@5  98.44 ( 98.48)
Epoch: [69][340/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9844e-01 (4.5715e-01)	Acc@1  85.94 ( 86.54)	Acc@5  99.22 ( 98.50)
Epoch: [69][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7729e-01 (4.5673e-01)	Acc@1  87.50 ( 86.54)	Acc@5  97.66 ( 98.50)
Epoch: [69][360/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.8086e-01 (4.5593e-01)	Acc@1  89.84 ( 86.58)	Acc@5  99.22 ( 98.49)
Epoch: [69][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.4678e-01 (4.5563e-01)	Acc@1  86.72 ( 86.59)	Acc@5 100.00 ( 98.50)
Epoch: [69][380/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.6118e-01 (4.5455e-01)	Acc@1  88.28 ( 86.61)	Acc@5  96.88 ( 98.51)
Epoch: [69][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.8496e-01 (4.5393e-01)	Acc@1  78.75 ( 86.64)	Acc@5  98.75 ( 98.51)
## e[69] optimizer.zero_grad (sum) time: 0.4066917896270752
## e[69]       loss.backward (sum) time: 7.34827446937561
## e[69]      optimizer.step (sum) time: 3.604466199874878
## epoch[69] training(only) time: 26.206141471862793
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 1.1533e+00 (1.1533e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.1279e+00 (1.2192e+00)	Acc@1  71.00 ( 69.00)	Acc@5  94.00 ( 90.09)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6152e+00 (1.2156e+00)	Acc@1  64.00 ( 68.52)	Acc@5  87.00 ( 90.29)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6748e+00 (1.2468e+00)	Acc@1  57.00 ( 67.65)	Acc@5  88.00 ( 89.94)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2041e+00 (1.2213e+00)	Acc@1  69.00 ( 67.76)	Acc@5  91.00 ( 90.49)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2646e+00 (1.2255e+00)	Acc@1  68.00 ( 67.53)	Acc@5  90.00 ( 90.59)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2900e+00 (1.2131e+00)	Acc@1  63.00 ( 67.75)	Acc@5  91.00 ( 90.85)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.9805e-01 (1.2154e+00)	Acc@1  76.00 ( 67.96)	Acc@5  94.00 ( 90.86)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2871e+00 (1.2212e+00)	Acc@1  66.00 ( 67.79)	Acc@5  89.00 ( 90.78)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.8457e+00 (1.2123e+00)	Acc@1  55.00 ( 67.85)	Acc@5  84.00 ( 90.87)
 * Acc@1 68.120 Acc@5 90.920
### epoch[69] execution time: 29.046324253082275
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.226 ( 0.226)	Data  0.148 ( 0.148)	Loss 5.4150e-01 (5.4150e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 99.22)
Epoch: [70][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.014)	Loss 4.3604e-01 (4.2316e-01)	Acc@1  85.94 ( 87.43)	Acc@5  97.66 ( 99.08)
Epoch: [70][ 20/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.0356e-01 (4.3920e-01)	Acc@1  89.06 ( 86.68)	Acc@5  99.22 ( 98.92)
Epoch: [70][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 3.8843e-01 (4.4559e-01)	Acc@1  87.50 ( 86.44)	Acc@5  99.22 ( 98.77)
Epoch: [70][ 40/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.9414e-01 (4.5077e-01)	Acc@1  84.38 ( 86.53)	Acc@5  98.44 ( 98.76)
Epoch: [70][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.0879e-01 (4.4821e-01)	Acc@1  81.25 ( 86.53)	Acc@5  98.44 ( 98.67)
Epoch: [70][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.4189e-01 (4.4788e-01)	Acc@1  86.72 ( 86.54)	Acc@5  99.22 ( 98.68)
Epoch: [70][ 70/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.8120e-01 (4.4483e-01)	Acc@1  83.59 ( 86.77)	Acc@5  99.22 ( 98.64)
Epoch: [70][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.2734e-01 (4.4674e-01)	Acc@1  85.16 ( 86.71)	Acc@5  98.44 ( 98.67)
Epoch: [70][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.9893e-01 (4.4892e-01)	Acc@1  88.28 ( 86.62)	Acc@5 100.00 ( 98.71)
Epoch: [70][100/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.0137e-01 (4.4683e-01)	Acc@1  86.72 ( 86.67)	Acc@5  99.22 ( 98.73)
Epoch: [70][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.7207e-01 (4.4592e-01)	Acc@1  87.50 ( 86.70)	Acc@5  99.22 ( 98.66)
Epoch: [70][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.7500e-01 (4.4430e-01)	Acc@1  85.16 ( 86.76)	Acc@5  98.44 ( 98.68)
Epoch: [70][130/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4199e-01 (4.4498e-01)	Acc@1  84.38 ( 86.78)	Acc@5  96.88 ( 98.65)
Epoch: [70][140/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7192e-01 (4.4601e-01)	Acc@1  85.94 ( 86.82)	Acc@5 100.00 ( 98.65)
Epoch: [70][150/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0723e-01 (4.4646e-01)	Acc@1  89.84 ( 86.77)	Acc@5  99.22 ( 98.65)
Epoch: [70][160/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6865e-01 (4.4388e-01)	Acc@1  90.62 ( 86.81)	Acc@5 100.00 ( 98.70)
Epoch: [70][170/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9697e-01 (4.4052e-01)	Acc@1  88.28 ( 86.93)	Acc@5  98.44 ( 98.71)
Epoch: [70][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5254e-01 (4.3839e-01)	Acc@1  90.62 ( 86.99)	Acc@5  99.22 ( 98.73)
Epoch: [70][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4150e-01 (4.3919e-01)	Acc@1  82.81 ( 86.98)	Acc@5  96.88 ( 98.71)
Epoch: [70][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3164e-01 (4.3956e-01)	Acc@1  91.41 ( 86.99)	Acc@5  99.22 ( 98.70)
Epoch: [70][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (4.3887e-01)	Acc@1  87.50 ( 87.03)	Acc@5  99.22 ( 98.72)
Epoch: [70][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1514e-01 (4.4067e-01)	Acc@1  83.59 ( 87.00)	Acc@5  99.22 ( 98.68)
Epoch: [70][230/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5620e-01 (4.4421e-01)	Acc@1  89.84 ( 86.84)	Acc@5  99.22 ( 98.66)
Epoch: [70][240/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8105e-01 (4.4478e-01)	Acc@1  79.69 ( 86.82)	Acc@5  99.22 ( 98.66)
Epoch: [70][250/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3848e-01 (4.4571e-01)	Acc@1  83.59 ( 86.80)	Acc@5  98.44 ( 98.65)
Epoch: [70][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.6416e-01 (4.4467e-01)	Acc@1  96.09 ( 86.84)	Acc@5  99.22 ( 98.65)
Epoch: [70][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1768e-01 (4.4627e-01)	Acc@1  82.03 ( 86.79)	Acc@5  96.09 ( 98.65)
Epoch: [70][280/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5654e-01 (4.4770e-01)	Acc@1  86.72 ( 86.74)	Acc@5  98.44 ( 98.64)
Epoch: [70][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0845e-01 (4.4784e-01)	Acc@1  89.06 ( 86.73)	Acc@5  98.44 ( 98.64)
Epoch: [70][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1709e-01 (4.4757e-01)	Acc@1  85.94 ( 86.72)	Acc@5  98.44 ( 98.64)
Epoch: [70][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8315e-01 (4.4885e-01)	Acc@1  84.38 ( 86.69)	Acc@5  97.66 ( 98.63)
Epoch: [70][320/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0454e-01 (4.4911e-01)	Acc@1  89.84 ( 86.66)	Acc@5  96.88 ( 98.63)
Epoch: [70][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3369e-01 (4.5061e-01)	Acc@1  85.16 ( 86.61)	Acc@5  98.44 ( 98.62)
Epoch: [70][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.6548e-01 (4.5042e-01)	Acc@1  91.41 ( 86.62)	Acc@5  99.22 ( 98.62)
Epoch: [70][350/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.0586e-01 (4.5033e-01)	Acc@1  85.16 ( 86.61)	Acc@5  98.44 ( 98.63)
Epoch: [70][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.4375e-01 (4.5075e-01)	Acc@1  89.06 ( 86.57)	Acc@5  98.44 ( 98.62)
Epoch: [70][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.8672e-01 (4.5013e-01)	Acc@1  89.06 ( 86.57)	Acc@5  98.44 ( 98.63)
Epoch: [70][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.5908e-01 (4.5027e-01)	Acc@1  88.28 ( 86.58)	Acc@5  96.09 ( 98.62)
Epoch: [70][390/391]	Time  0.057 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.6816e-01 (4.5043e-01)	Acc@1  88.75 ( 86.58)	Acc@5  98.75 ( 98.62)
## e[70] optimizer.zero_grad (sum) time: 0.4085104465484619
## e[70]       loss.backward (sum) time: 7.367064476013184
## e[70]      optimizer.step (sum) time: 3.5773777961730957
## epoch[70] training(only) time: 26.153501510620117
# Switched to evaluate mode...
Test: [  0/100]	Time  0.169 ( 0.169)	Loss 1.1797e+00 (1.1797e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1387e+00 (1.2200e+00)	Acc@1  69.00 ( 69.00)	Acc@5  94.00 ( 89.82)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.6104e+00 (1.2187e+00)	Acc@1  64.00 ( 68.57)	Acc@5  89.00 ( 90.14)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7168e+00 (1.2524e+00)	Acc@1  58.00 ( 67.45)	Acc@5  87.00 ( 89.71)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.2295e+00 (1.2300e+00)	Acc@1  69.00 ( 67.54)	Acc@5  90.00 ( 90.20)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.2637e+00 (1.2344e+00)	Acc@1  65.00 ( 67.31)	Acc@5  90.00 ( 90.29)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2949e+00 (1.2215e+00)	Acc@1  62.00 ( 67.51)	Acc@5  91.00 ( 90.57)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0293e+00 (1.2247e+00)	Acc@1  72.00 ( 67.59)	Acc@5  93.00 ( 90.59)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3037e+00 (1.2294e+00)	Acc@1  64.00 ( 67.44)	Acc@5  89.00 ( 90.54)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8340e+00 (1.2200e+00)	Acc@1  53.00 ( 67.53)	Acc@5  84.00 ( 90.64)
 * Acc@1 67.800 Acc@5 90.690
### epoch[70] execution time: 28.948736906051636
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.224 ( 0.224)	Data  0.127 ( 0.127)	Loss 4.4263e-01 (4.4263e-01)	Acc@1  88.28 ( 88.28)	Acc@5  98.44 ( 98.44)
Epoch: [71][ 10/391]	Time  0.068 ( 0.081)	Data  0.001 ( 0.012)	Loss 4.2554e-01 (4.3761e-01)	Acc@1  87.50 ( 87.00)	Acc@5  97.66 ( 98.30)
Epoch: [71][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.007)	Loss 4.5044e-01 (4.6394e-01)	Acc@1  88.28 ( 86.05)	Acc@5  98.44 ( 98.36)
Epoch: [71][ 30/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.0967e-01 (4.5510e-01)	Acc@1  85.16 ( 86.04)	Acc@5  97.66 ( 98.49)
Epoch: [71][ 40/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.1797e-01 (4.4173e-01)	Acc@1  87.50 ( 86.45)	Acc@5  98.44 ( 98.67)
Epoch: [71][ 50/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.004)	Loss 3.8013e-01 (4.4810e-01)	Acc@1  90.62 ( 86.38)	Acc@5  99.22 ( 98.56)
Epoch: [71][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.2822e-01 (4.4744e-01)	Acc@1  84.38 ( 86.44)	Acc@5  99.22 ( 98.60)
Epoch: [71][ 70/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.9609e-01 (4.4291e-01)	Acc@1  85.16 ( 86.71)	Acc@5  97.66 ( 98.59)
Epoch: [71][ 80/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.3765e-01 (4.4050e-01)	Acc@1  91.41 ( 86.82)	Acc@5  99.22 ( 98.59)
Epoch: [71][ 90/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.4165e-01 (4.4163e-01)	Acc@1  85.94 ( 86.80)	Acc@5  99.22 ( 98.63)
Epoch: [71][100/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5078e-01 (4.4304e-01)	Acc@1  80.47 ( 86.70)	Acc@5  97.66 ( 98.63)
Epoch: [71][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.8257e-01 (4.4542e-01)	Acc@1  89.84 ( 86.61)	Acc@5  99.22 ( 98.62)
Epoch: [71][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.6841e-01 (4.4806e-01)	Acc@1  90.62 ( 86.56)	Acc@5 100.00 ( 98.64)
Epoch: [71][130/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7095e-01 (4.4686e-01)	Acc@1  85.94 ( 86.65)	Acc@5  98.44 ( 98.65)
Epoch: [71][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4507e-01 (4.4569e-01)	Acc@1  85.16 ( 86.66)	Acc@5 100.00 ( 98.67)
Epoch: [71][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1172e-01 (4.4615e-01)	Acc@1  84.38 ( 86.66)	Acc@5  97.66 ( 98.64)
Epoch: [71][160/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0796e-01 (4.4494e-01)	Acc@1  89.84 ( 86.74)	Acc@5  99.22 ( 98.67)
Epoch: [71][170/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9795e-01 (4.4485e-01)	Acc@1  91.41 ( 86.77)	Acc@5  97.66 ( 98.67)
Epoch: [71][180/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9575e-01 (4.4681e-01)	Acc@1  89.84 ( 86.69)	Acc@5  97.66 ( 98.61)
Epoch: [71][190/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0454e-01 (4.4512e-01)	Acc@1  89.06 ( 86.80)	Acc@5  98.44 ( 98.63)
Epoch: [71][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6265e-01 (4.4631e-01)	Acc@1  87.50 ( 86.76)	Acc@5  98.44 ( 98.60)
Epoch: [71][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4497e-01 (4.4518e-01)	Acc@1  89.84 ( 86.78)	Acc@5  98.44 ( 98.61)
Epoch: [71][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0991e-01 (4.4703e-01)	Acc@1  88.28 ( 86.75)	Acc@5  98.44 ( 98.59)
Epoch: [71][230/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1138e-01 (4.4713e-01)	Acc@1  90.62 ( 86.72)	Acc@5  99.22 ( 98.59)
Epoch: [71][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2715e-01 (4.4641e-01)	Acc@1  88.28 ( 86.73)	Acc@5  99.22 ( 98.60)
Epoch: [71][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5947e-01 (4.4729e-01)	Acc@1  86.72 ( 86.68)	Acc@5 100.00 ( 98.60)
Epoch: [71][260/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5474e-01 (4.4641e-01)	Acc@1  91.41 ( 86.73)	Acc@5  99.22 ( 98.60)
Epoch: [71][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8047e-01 (4.4774e-01)	Acc@1  85.94 ( 86.70)	Acc@5  98.44 ( 98.59)
Epoch: [71][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7817e-01 (4.5030e-01)	Acc@1  89.84 ( 86.62)	Acc@5  99.22 ( 98.55)
Epoch: [71][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2051e-01 (4.5021e-01)	Acc@1  84.38 ( 86.62)	Acc@5  98.44 ( 98.56)
Epoch: [71][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7329e-01 (4.4981e-01)	Acc@1  87.50 ( 86.64)	Acc@5 100.00 ( 98.57)
Epoch: [71][310/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.8428e-01 (4.4992e-01)	Acc@1  89.84 ( 86.62)	Acc@5  97.66 ( 98.58)
Epoch: [71][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.9194e-01 (4.5033e-01)	Acc@1  85.16 ( 86.62)	Acc@5  97.66 ( 98.58)
Epoch: [71][330/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.7876e-01 (4.4940e-01)	Acc@1  86.72 ( 86.66)	Acc@5  96.88 ( 98.59)
Epoch: [71][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.9478e-01 (4.4863e-01)	Acc@1  91.41 ( 86.71)	Acc@5  97.66 ( 98.59)
Epoch: [71][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.8623e-01 (4.4814e-01)	Acc@1  88.28 ( 86.71)	Acc@5  99.22 ( 98.60)
Epoch: [71][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.4248e-01 (4.4909e-01)	Acc@1  81.25 ( 86.65)	Acc@5  97.66 ( 98.60)
Epoch: [71][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.9731e-01 (4.4774e-01)	Acc@1  87.50 ( 86.70)	Acc@5  97.66 ( 98.61)
Epoch: [71][380/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.4443e-01 (4.4821e-01)	Acc@1  80.47 ( 86.71)	Acc@5  98.44 ( 98.62)
Epoch: [71][390/391]	Time  0.052 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.2939e-01 (4.4846e-01)	Acc@1  78.75 ( 86.72)	Acc@5  97.50 ( 98.62)
## e[71] optimizer.zero_grad (sum) time: 0.41544127464294434
## e[71]       loss.backward (sum) time: 7.355716228485107
## e[71]      optimizer.step (sum) time: 3.54172682762146
## epoch[71] training(only) time: 26.208507537841797
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.1357e+00 (1.1357e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.025 ( 0.038)	Loss 1.1416e+00 (1.2163e+00)	Acc@1  71.00 ( 70.09)	Acc@5  94.00 ( 89.82)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.6250e+00 (1.2121e+00)	Acc@1  63.00 ( 69.05)	Acc@5  88.00 ( 90.14)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.6611e+00 (1.2452e+00)	Acc@1  58.00 ( 67.97)	Acc@5  86.00 ( 89.71)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.2129e+00 (1.2213e+00)	Acc@1  68.00 ( 67.83)	Acc@5  91.00 ( 90.37)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2480e+00 (1.2267e+00)	Acc@1  66.00 ( 67.53)	Acc@5  90.00 ( 90.41)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2695e+00 (1.2133e+00)	Acc@1  62.00 ( 67.82)	Acc@5  90.00 ( 90.70)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0137e+00 (1.2151e+00)	Acc@1  75.00 ( 68.04)	Acc@5  92.00 ( 90.70)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3164e+00 (1.2224e+00)	Acc@1  65.00 ( 67.85)	Acc@5  89.00 ( 90.58)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8994e+00 (1.2143e+00)	Acc@1  52.00 ( 67.81)	Acc@5  82.00 ( 90.66)
 * Acc@1 68.110 Acc@5 90.740
### epoch[71] execution time: 29.041934490203857
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.219 ( 0.219)	Data  0.141 ( 0.141)	Loss 3.5229e-01 (3.5229e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [72][ 10/391]	Time  0.073 ( 0.080)	Data  0.001 ( 0.014)	Loss 3.8208e-01 (3.9562e-01)	Acc@1  89.84 ( 88.92)	Acc@5  99.22 ( 99.01)
Epoch: [72][ 20/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.008)	Loss 5.5371e-01 (4.1938e-01)	Acc@1  81.25 ( 87.83)	Acc@5  96.88 ( 98.85)
Epoch: [72][ 30/391]	Time  0.063 ( 0.071)	Data  0.001 ( 0.006)	Loss 3.3154e-01 (4.2472e-01)	Acc@1  90.62 ( 87.68)	Acc@5 100.00 ( 98.87)
Epoch: [72][ 40/391]	Time  0.069 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.8486e-01 (4.2592e-01)	Acc@1  85.94 ( 87.63)	Acc@5  96.88 ( 98.86)
Epoch: [72][ 50/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.004)	Loss 3.5962e-01 (4.3677e-01)	Acc@1  89.84 ( 87.22)	Acc@5  99.22 ( 98.73)
Epoch: [72][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.2114e-01 (4.3609e-01)	Acc@1  85.94 ( 87.31)	Acc@5  99.22 ( 98.73)
Epoch: [72][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.0439e-01 (4.4273e-01)	Acc@1  85.16 ( 87.14)	Acc@5  96.88 ( 98.55)
Epoch: [72][ 80/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.6851e-01 (4.4523e-01)	Acc@1  85.94 ( 86.98)	Acc@5  95.31 ( 98.47)
Epoch: [72][ 90/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.7671e-01 (4.4700e-01)	Acc@1  89.84 ( 86.82)	Acc@5  99.22 ( 98.47)
Epoch: [72][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9771e-01 (4.4618e-01)	Acc@1  85.94 ( 86.80)	Acc@5 100.00 ( 98.54)
Epoch: [72][110/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4775e-01 (4.4654e-01)	Acc@1  86.72 ( 86.67)	Acc@5 100.00 ( 98.56)
Epoch: [72][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5459e-01 (4.4594e-01)	Acc@1  86.72 ( 86.70)	Acc@5  98.44 ( 98.59)
Epoch: [72][130/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1367e-01 (4.4752e-01)	Acc@1  81.25 ( 86.62)	Acc@5  99.22 ( 98.58)
Epoch: [72][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9771e-01 (4.4949e-01)	Acc@1  88.28 ( 86.55)	Acc@5  99.22 ( 98.58)
Epoch: [72][150/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8389e-01 (4.4855e-01)	Acc@1  85.16 ( 86.54)	Acc@5  99.22 ( 98.58)
Epoch: [72][160/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4409e-01 (4.4949e-01)	Acc@1  86.72 ( 86.56)	Acc@5 100.00 ( 98.56)
Epoch: [72][170/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7388e-01 (4.4819e-01)	Acc@1  85.94 ( 86.62)	Acc@5  99.22 ( 98.55)
Epoch: [72][180/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.6328e-01 (4.4773e-01)	Acc@1  89.84 ( 86.65)	Acc@5  99.22 ( 98.56)
Epoch: [72][190/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.4053e-01 (4.5046e-01)	Acc@1  81.25 ( 86.58)	Acc@5  96.88 ( 98.55)
Epoch: [72][200/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.5083e-01 (4.4963e-01)	Acc@1  89.84 ( 86.62)	Acc@5  99.22 ( 98.57)
Epoch: [72][210/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4897e-01 (4.4795e-01)	Acc@1  86.72 ( 86.68)	Acc@5 100.00 ( 98.59)
Epoch: [72][220/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9512e-01 (4.4602e-01)	Acc@1  85.94 ( 86.78)	Acc@5  96.88 ( 98.59)
Epoch: [72][230/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0259e-01 (4.4575e-01)	Acc@1  84.38 ( 86.73)	Acc@5  99.22 ( 98.60)
Epoch: [72][240/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4507e-01 (4.4541e-01)	Acc@1  87.50 ( 86.77)	Acc@5  96.88 ( 98.59)
Epoch: [72][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7817e-01 (4.4511e-01)	Acc@1  89.84 ( 86.80)	Acc@5  98.44 ( 98.59)
Epoch: [72][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0039e-01 (4.4566e-01)	Acc@1  89.06 ( 86.80)	Acc@5 100.00 ( 98.57)
Epoch: [72][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9014e-01 (4.4612e-01)	Acc@1  89.84 ( 86.79)	Acc@5 100.00 ( 98.58)
Epoch: [72][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5483e-01 (4.4768e-01)	Acc@1  88.28 ( 86.74)	Acc@5  99.22 ( 98.56)
Epoch: [72][290/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6963e-01 (4.4797e-01)	Acc@1  91.41 ( 86.73)	Acc@5  98.44 ( 98.54)
Epoch: [72][300/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4678e-01 (4.4881e-01)	Acc@1  85.94 ( 86.72)	Acc@5  98.44 ( 98.53)
Epoch: [72][310/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3579e-01 (4.4803e-01)	Acc@1  87.50 ( 86.73)	Acc@5  99.22 ( 98.54)
Epoch: [72][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2842e-01 (4.4717e-01)	Acc@1  82.03 ( 86.76)	Acc@5  96.09 ( 98.54)
Epoch: [72][330/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9805e-01 (4.4712e-01)	Acc@1  82.81 ( 86.80)	Acc@5  96.09 ( 98.53)
Epoch: [72][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1465e-01 (4.4595e-01)	Acc@1  84.38 ( 86.85)	Acc@5  98.44 ( 98.54)
Epoch: [72][350/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7705e-01 (4.4616e-01)	Acc@1  89.84 ( 86.85)	Acc@5  98.44 ( 98.54)
Epoch: [72][360/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.2432e-01 (4.4628e-01)	Acc@1  85.94 ( 86.85)	Acc@5  99.22 ( 98.53)
Epoch: [72][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.7764e-01 (4.4765e-01)	Acc@1  82.03 ( 86.79)	Acc@5  99.22 ( 98.52)
Epoch: [72][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.7524e-01 (4.4726e-01)	Acc@1  92.19 ( 86.84)	Acc@5  98.44 ( 98.53)
Epoch: [72][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.5576e-01 (4.4838e-01)	Acc@1  80.00 ( 86.81)	Acc@5  96.25 ( 98.52)
## e[72] optimizer.zero_grad (sum) time: 0.40587401390075684
## e[72]       loss.backward (sum) time: 7.360795497894287
## e[72]      optimizer.step (sum) time: 3.600778102874756
## epoch[72] training(only) time: 26.197049140930176
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.1562e+00 (1.1562e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1240e+00 (1.2184e+00)	Acc@1  71.00 ( 70.00)	Acc@5  94.00 ( 90.09)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5918e+00 (1.2140e+00)	Acc@1  65.00 ( 69.48)	Acc@5  88.00 ( 90.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7305e+00 (1.2479e+00)	Acc@1  58.00 ( 68.26)	Acc@5  88.00 ( 89.77)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.2090e+00 (1.2256e+00)	Acc@1  70.00 ( 68.15)	Acc@5  91.00 ( 90.34)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2471e+00 (1.2298e+00)	Acc@1  66.00 ( 67.73)	Acc@5  90.00 ( 90.43)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2842e+00 (1.2185e+00)	Acc@1  63.00 ( 67.84)	Acc@5  91.00 ( 90.69)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0234e+00 (1.2204e+00)	Acc@1  73.00 ( 67.93)	Acc@5  94.00 ( 90.68)
Test: [ 80/100]	Time  0.032 ( 0.028)	Loss 1.3164e+00 (1.2275e+00)	Acc@1  65.00 ( 67.70)	Acc@5  89.00 ( 90.63)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8350e+00 (1.2181e+00)	Acc@1  53.00 ( 67.74)	Acc@5  83.00 ( 90.71)
 * Acc@1 67.970 Acc@5 90.810
### epoch[72] execution time: 29.01767349243164
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.216 ( 0.216)	Data  0.139 ( 0.139)	Loss 6.0547e-01 (6.0547e-01)	Acc@1  82.03 ( 82.03)	Acc@5  96.88 ( 96.88)
Epoch: [73][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 3.9526e-01 (4.8841e-01)	Acc@1  88.28 ( 85.37)	Acc@5  96.88 ( 97.94)
Epoch: [73][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 4.6094e-01 (4.6170e-01)	Acc@1  85.94 ( 86.57)	Acc@5  99.22 ( 98.36)
Epoch: [73][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.005)	Loss 3.5498e-01 (4.5997e-01)	Acc@1  91.41 ( 86.67)	Acc@5  97.66 ( 98.39)
Epoch: [73][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.1724e-01 (4.5580e-01)	Acc@1  86.72 ( 86.79)	Acc@5 100.00 ( 98.57)
Epoch: [73][ 50/391]	Time  0.070 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.3213e-01 (4.5566e-01)	Acc@1  86.72 ( 86.87)	Acc@5  99.22 ( 98.58)
Epoch: [73][ 60/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.1040e-01 (4.5205e-01)	Acc@1  86.72 ( 86.91)	Acc@5  97.66 ( 98.60)
Epoch: [73][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.2832e-01 (4.5245e-01)	Acc@1  86.72 ( 86.97)	Acc@5  95.31 ( 98.57)
Epoch: [73][ 80/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.5410e-01 (4.4830e-01)	Acc@1  86.72 ( 86.92)	Acc@5  99.22 ( 98.62)
Epoch: [73][ 90/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.6924e-01 (4.4685e-01)	Acc@1  84.38 ( 86.96)	Acc@5  97.66 ( 98.57)
Epoch: [73][100/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7412e-01 (4.4401e-01)	Acc@1  92.19 ( 87.18)	Acc@5  98.44 ( 98.58)
Epoch: [73][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7656e-01 (4.4575e-01)	Acc@1  86.72 ( 86.99)	Acc@5  99.22 ( 98.61)
Epoch: [73][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1318e-01 (4.4601e-01)	Acc@1  88.28 ( 86.92)	Acc@5  97.66 ( 98.64)
Epoch: [73][130/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.6646e-01 (4.4458e-01)	Acc@1  89.06 ( 86.93)	Acc@5 100.00 ( 98.64)
Epoch: [73][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7852e-01 (4.4552e-01)	Acc@1  86.72 ( 86.86)	Acc@5  97.66 ( 98.64)
Epoch: [73][150/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.0635e-01 (4.4588e-01)	Acc@1  85.94 ( 86.87)	Acc@5  98.44 ( 98.65)
Epoch: [73][160/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9355e-01 (4.4452e-01)	Acc@1  89.84 ( 86.94)	Acc@5  99.22 ( 98.68)
Epoch: [73][170/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9355e-01 (4.4415e-01)	Acc@1  85.94 ( 86.97)	Acc@5 100.00 ( 98.68)
Epoch: [73][180/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.2300e-01 (4.4358e-01)	Acc@1  89.84 ( 86.98)	Acc@5 100.00 ( 98.66)
Epoch: [73][190/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.8159e-01 (4.4502e-01)	Acc@1  85.94 ( 86.89)	Acc@5  99.22 ( 98.62)
Epoch: [73][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3896e-01 (4.4455e-01)	Acc@1  85.16 ( 86.85)	Acc@5  98.44 ( 98.64)
Epoch: [73][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5127e-01 (4.4763e-01)	Acc@1  85.16 ( 86.79)	Acc@5  97.66 ( 98.60)
Epoch: [73][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4482e-01 (4.4865e-01)	Acc@1  89.84 ( 86.78)	Acc@5  99.22 ( 98.59)
Epoch: [73][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4829e-01 (4.4794e-01)	Acc@1  95.31 ( 86.78)	Acc@5 100.00 ( 98.60)
Epoch: [73][240/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4385e-01 (4.4768e-01)	Acc@1  85.16 ( 86.82)	Acc@5  99.22 ( 98.59)
Epoch: [73][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8496e-01 (4.4796e-01)	Acc@1  82.03 ( 86.82)	Acc@5  96.09 ( 98.60)
Epoch: [73][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1270e-01 (4.4732e-01)	Acc@1  84.38 ( 86.86)	Acc@5  97.66 ( 98.60)
Epoch: [73][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0356e-01 (4.4653e-01)	Acc@1  89.06 ( 86.87)	Acc@5 100.00 ( 98.62)
Epoch: [73][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7573e-01 (4.4679e-01)	Acc@1  89.84 ( 86.84)	Acc@5  99.22 ( 98.62)
Epoch: [73][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5425e-01 (4.4817e-01)	Acc@1  87.50 ( 86.76)	Acc@5  98.44 ( 98.61)
Epoch: [73][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5522e-01 (4.4870e-01)	Acc@1  89.84 ( 86.74)	Acc@5  99.22 ( 98.60)
Epoch: [73][310/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4434e-01 (4.4899e-01)	Acc@1  87.50 ( 86.74)	Acc@5  99.22 ( 98.59)
Epoch: [73][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4717e-01 (4.4810e-01)	Acc@1  92.97 ( 86.79)	Acc@5  99.22 ( 98.59)
Epoch: [73][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7192e-01 (4.4736e-01)	Acc@1  85.94 ( 86.81)	Acc@5  98.44 ( 98.61)
Epoch: [73][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.5645e-01 (4.4795e-01)	Acc@1  91.41 ( 86.80)	Acc@5  98.44 ( 98.60)
Epoch: [73][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.7227e-01 (4.4796e-01)	Acc@1  79.69 ( 86.79)	Acc@5  96.88 ( 98.60)
Epoch: [73][360/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.6401e-01 (4.4700e-01)	Acc@1  89.06 ( 86.81)	Acc@5  98.44 ( 98.62)
Epoch: [73][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.6299e-01 (4.4686e-01)	Acc@1  82.03 ( 86.83)	Acc@5  97.66 ( 98.60)
Epoch: [73][380/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.6631e-01 (4.4783e-01)	Acc@1  82.03 ( 86.82)	Acc@5  99.22 ( 98.59)
Epoch: [73][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.3799e-01 (4.4735e-01)	Acc@1  92.50 ( 86.85)	Acc@5  96.25 ( 98.59)
## e[73] optimizer.zero_grad (sum) time: 0.40637636184692383
## e[73]       loss.backward (sum) time: 7.401088237762451
## e[73]      optimizer.step (sum) time: 3.625544548034668
## epoch[73] training(only) time: 26.227641582489014
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.1553e+00 (1.1553e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.029 ( 0.039)	Loss 1.1240e+00 (1.2141e+00)	Acc@1  71.00 ( 69.27)	Acc@5  94.00 ( 89.91)
Test: [ 20/100]	Time  0.029 ( 0.033)	Loss 1.6143e+00 (1.2144e+00)	Acc@1  64.00 ( 68.67)	Acc@5  88.00 ( 90.10)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.6650e+00 (1.2414e+00)	Acc@1  56.00 ( 67.61)	Acc@5  87.00 ( 89.74)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2002e+00 (1.2189e+00)	Acc@1  70.00 ( 67.59)	Acc@5  91.00 ( 90.39)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.2461e+00 (1.2238e+00)	Acc@1  69.00 ( 67.41)	Acc@5  90.00 ( 90.47)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2979e+00 (1.2112e+00)	Acc@1  62.00 ( 67.69)	Acc@5  90.00 ( 90.77)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0186e+00 (1.2134e+00)	Acc@1  74.00 ( 67.89)	Acc@5  94.00 ( 90.79)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3037e+00 (1.2193e+00)	Acc@1  64.00 ( 67.68)	Acc@5  89.00 ( 90.73)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8428e+00 (1.2111e+00)	Acc@1  51.00 ( 67.64)	Acc@5  82.00 ( 90.80)
 * Acc@1 67.910 Acc@5 90.820
### epoch[73] execution time: 29.04635763168335
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.222 ( 0.222)	Data  0.144 ( 0.144)	Loss 3.3325e-01 (3.3325e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.061 ( 0.079)	Data  0.001 ( 0.014)	Loss 4.2505e-01 (3.8170e-01)	Acc@1  86.72 ( 89.20)	Acc@5  98.44 ( 99.43)
Epoch: [74][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.008)	Loss 3.1616e-01 (4.0297e-01)	Acc@1  92.97 ( 88.39)	Acc@5  99.22 ( 99.03)
Epoch: [74][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.006)	Loss 4.8486e-01 (4.2566e-01)	Acc@1  83.59 ( 87.60)	Acc@5  99.22 ( 98.97)
Epoch: [74][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.0088e-01 (4.3236e-01)	Acc@1  89.84 ( 87.25)	Acc@5  98.44 ( 98.95)
Epoch: [74][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.2344e-01 (4.4103e-01)	Acc@1  85.94 ( 86.92)	Acc@5  97.66 ( 98.84)
Epoch: [74][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.6362e-01 (4.4406e-01)	Acc@1  85.94 ( 86.71)	Acc@5  98.44 ( 98.83)
Epoch: [74][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.6201e-01 (4.4534e-01)	Acc@1  85.16 ( 86.85)	Acc@5  97.66 ( 98.72)
Epoch: [74][ 80/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.3164e-01 (4.4718e-01)	Acc@1  85.94 ( 86.88)	Acc@5  99.22 ( 98.63)
Epoch: [74][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.7852e-01 (4.4937e-01)	Acc@1  85.16 ( 86.67)	Acc@5  97.66 ( 98.67)
Epoch: [74][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.7241e-01 (4.5357e-01)	Acc@1  86.72 ( 86.53)	Acc@5  97.66 ( 98.60)
Epoch: [74][110/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5947e-01 (4.5213e-01)	Acc@1  85.16 ( 86.56)	Acc@5  96.88 ( 98.58)
Epoch: [74][120/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7437e-01 (4.4947e-01)	Acc@1  88.28 ( 86.66)	Acc@5  96.09 ( 98.59)
Epoch: [74][130/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.0244e-01 (4.4839e-01)	Acc@1  88.28 ( 86.78)	Acc@5  96.88 ( 98.60)
Epoch: [74][140/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.6152e-01 (4.4758e-01)	Acc@1  83.59 ( 86.79)	Acc@5  96.88 ( 98.60)
Epoch: [74][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5557e-01 (4.4735e-01)	Acc@1  88.28 ( 86.81)	Acc@5  96.88 ( 98.59)
Epoch: [74][160/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5898e-01 (4.4675e-01)	Acc@1  87.50 ( 86.84)	Acc@5  96.88 ( 98.57)
Epoch: [74][170/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7769e-01 (4.4479e-01)	Acc@1  89.84 ( 86.88)	Acc@5 100.00 ( 98.59)
Epoch: [74][180/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3604e-01 (4.4541e-01)	Acc@1  87.50 ( 86.87)	Acc@5  99.22 ( 98.59)
Epoch: [74][190/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1172e-01 (4.4665e-01)	Acc@1  82.03 ( 86.80)	Acc@5  98.44 ( 98.59)
Epoch: [74][200/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (4.4664e-01)	Acc@1  87.50 ( 86.82)	Acc@5 100.00 ( 98.60)
Epoch: [74][210/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6948e-01 (4.4495e-01)	Acc@1  86.72 ( 86.85)	Acc@5  99.22 ( 98.63)
Epoch: [74][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6304e-01 (4.4726e-01)	Acc@1  92.19 ( 86.74)	Acc@5 100.00 ( 98.62)
Epoch: [74][230/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7524e-01 (4.4653e-01)	Acc@1  89.06 ( 86.75)	Acc@5 100.00 ( 98.63)
Epoch: [74][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2407e-01 (4.4678e-01)	Acc@1  87.50 ( 86.74)	Acc@5  99.22 ( 98.63)
Epoch: [74][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0942e-01 (4.4556e-01)	Acc@1  87.50 ( 86.81)	Acc@5  99.22 ( 98.64)
Epoch: [74][260/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7549e-01 (4.4615e-01)	Acc@1  86.72 ( 86.81)	Acc@5 100.00 ( 98.63)
Epoch: [74][270/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6436e-01 (4.4586e-01)	Acc@1  85.94 ( 86.83)	Acc@5  97.66 ( 98.65)
Epoch: [74][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8452e-01 (4.4477e-01)	Acc@1  86.72 ( 86.84)	Acc@5  99.22 ( 98.66)
Epoch: [74][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9766e-01 (4.4489e-01)	Acc@1  80.47 ( 86.87)	Acc@5  96.88 ( 98.67)
Epoch: [74][300/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3994e-01 (4.4539e-01)	Acc@1  90.62 ( 86.86)	Acc@5  98.44 ( 98.67)
Epoch: [74][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4785e-01 (4.4517e-01)	Acc@1  80.47 ( 86.85)	Acc@5  96.88 ( 98.65)
Epoch: [74][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.0493e-01 (4.4521e-01)	Acc@1  87.50 ( 86.86)	Acc@5 100.00 ( 98.64)
Epoch: [74][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8560e-01 (4.4474e-01)	Acc@1  85.94 ( 86.90)	Acc@5  96.88 ( 98.64)
Epoch: [74][340/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5947e-01 (4.4516e-01)	Acc@1  86.72 ( 86.91)	Acc@5  99.22 ( 98.63)
Epoch: [74][350/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4482e-01 (4.4510e-01)	Acc@1  82.81 ( 86.93)	Acc@5  98.44 ( 98.63)
Epoch: [74][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4346e-01 (4.4473e-01)	Acc@1  82.81 ( 86.95)	Acc@5  98.44 ( 98.63)
Epoch: [74][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.6475e-01 (4.4406e-01)	Acc@1  89.06 ( 86.97)	Acc@5  99.22 ( 98.64)
Epoch: [74][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.6055e-01 (4.4472e-01)	Acc@1  77.34 ( 86.93)	Acc@5  99.22 ( 98.64)
Epoch: [74][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.8154e-01 (4.4465e-01)	Acc@1  82.50 ( 86.93)	Acc@5  96.25 ( 98.63)
## e[74] optimizer.zero_grad (sum) time: 0.4054896831512451
## e[74]       loss.backward (sum) time: 7.43869161605835
## e[74]      optimizer.step (sum) time: 3.62930965423584
## epoch[74] training(only) time: 26.300812244415283
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1279e+00 (1.1279e+00)	Acc@1  67.00 ( 67.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1309e+00 (1.2161e+00)	Acc@1  70.00 ( 69.18)	Acc@5  93.00 ( 89.91)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.6377e+00 (1.2168e+00)	Acc@1  63.00 ( 68.71)	Acc@5  88.00 ( 90.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6934e+00 (1.2489e+00)	Acc@1  56.00 ( 67.74)	Acc@5  86.00 ( 89.71)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.2188e+00 (1.2260e+00)	Acc@1  68.00 ( 67.61)	Acc@5  91.00 ( 90.37)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.2480e+00 (1.2317e+00)	Acc@1  69.00 ( 67.41)	Acc@5  90.00 ( 90.45)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2754e+00 (1.2185e+00)	Acc@1  62.00 ( 67.66)	Acc@5  93.00 ( 90.74)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.0400e+00 (1.2199e+00)	Acc@1  73.00 ( 67.83)	Acc@5  93.00 ( 90.72)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3213e+00 (1.2268e+00)	Acc@1  65.00 ( 67.60)	Acc@5  88.00 ( 90.58)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.8711e+00 (1.2188e+00)	Acc@1  53.00 ( 67.62)	Acc@5  82.00 ( 90.64)
 * Acc@1 67.920 Acc@5 90.680
### epoch[74] execution time: 29.15535044670105
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.227 ( 0.227)	Data  0.152 ( 0.152)	Loss 6.4307e-01 (6.4307e-01)	Acc@1  78.91 ( 78.91)	Acc@5  96.88 ( 96.88)
Epoch: [75][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.015)	Loss 4.5996e-01 (5.1108e-01)	Acc@1  82.81 ( 83.81)	Acc@5 100.00 ( 98.15)
Epoch: [75][ 20/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.1602e-01 (4.7383e-01)	Acc@1  88.28 ( 85.49)	Acc@5 100.00 ( 98.21)
Epoch: [75][ 30/391]	Time  0.069 ( 0.072)	Data  0.001 ( 0.006)	Loss 4.2065e-01 (4.5431e-01)	Acc@1  86.72 ( 86.24)	Acc@5  99.22 ( 98.39)
Epoch: [75][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.005)	Loss 5.2686e-01 (4.4790e-01)	Acc@1  84.38 ( 86.55)	Acc@5  97.66 ( 98.38)
Epoch: [75][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.4580e-01 (4.5048e-01)	Acc@1  85.16 ( 86.27)	Acc@5  98.44 ( 98.38)
Epoch: [75][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.5679e-01 (4.5176e-01)	Acc@1  87.50 ( 86.31)	Acc@5  97.66 ( 98.41)
Epoch: [75][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.3809e-01 (4.5387e-01)	Acc@1  82.03 ( 86.33)	Acc@5  97.66 ( 98.48)
Epoch: [75][ 80/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.8301e-01 (4.5178e-01)	Acc@1  84.38 ( 86.49)	Acc@5  96.09 ( 98.49)
Epoch: [75][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.1855e-01 (4.5186e-01)	Acc@1  82.03 ( 86.50)	Acc@5  99.22 ( 98.54)
Epoch: [75][100/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.7354e-01 (4.4802e-01)	Acc@1  89.84 ( 86.63)	Acc@5 100.00 ( 98.56)
Epoch: [75][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.2397e-01 (4.4848e-01)	Acc@1  88.28 ( 86.60)	Acc@5 100.00 ( 98.56)
Epoch: [75][120/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1514e-01 (4.4769e-01)	Acc@1  86.72 ( 86.62)	Acc@5  96.09 ( 98.58)
Epoch: [75][130/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.7427e-01 (4.4808e-01)	Acc@1  89.06 ( 86.67)	Acc@5 100.00 ( 98.57)
Epoch: [75][140/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.1650e-01 (4.4797e-01)	Acc@1  86.72 ( 86.69)	Acc@5  99.22 ( 98.57)
Epoch: [75][150/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.2725e-01 (4.4832e-01)	Acc@1  88.28 ( 86.65)	Acc@5  99.22 ( 98.59)
Epoch: [75][160/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7607e-01 (4.4955e-01)	Acc@1  84.38 ( 86.58)	Acc@5  97.66 ( 98.55)
Epoch: [75][170/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.2969e-01 (4.4815e-01)	Acc@1  88.28 ( 86.72)	Acc@5  98.44 ( 98.57)
Epoch: [75][180/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9771e-01 (4.4847e-01)	Acc@1  88.28 ( 86.69)	Acc@5  99.22 ( 98.53)
Epoch: [75][190/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0405e-01 (4.4893e-01)	Acc@1  87.50 ( 86.68)	Acc@5  97.66 ( 98.52)
Epoch: [75][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2285e-01 (4.4887e-01)	Acc@1  88.28 ( 86.63)	Acc@5  99.22 ( 98.53)
Epoch: [75][210/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8462e-01 (4.5149e-01)	Acc@1  84.38 ( 86.57)	Acc@5  98.44 ( 98.51)
Epoch: [75][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3481e-01 (4.5081e-01)	Acc@1  86.72 ( 86.58)	Acc@5 100.00 ( 98.53)
Epoch: [75][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6802e-01 (4.5040e-01)	Acc@1  85.16 ( 86.59)	Acc@5  98.44 ( 98.55)
Epoch: [75][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7827e-01 (4.5138e-01)	Acc@1  84.38 ( 86.57)	Acc@5  98.44 ( 98.55)
Epoch: [75][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0635e-01 (4.4998e-01)	Acc@1  85.94 ( 86.63)	Acc@5  97.66 ( 98.56)
Epoch: [75][260/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7314e-01 (4.5044e-01)	Acc@1  84.38 ( 86.61)	Acc@5  99.22 ( 98.57)
Epoch: [75][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3848e-01 (4.4922e-01)	Acc@1  86.72 ( 86.64)	Acc@5  99.22 ( 98.58)
Epoch: [75][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4961e-01 (4.4784e-01)	Acc@1  87.50 ( 86.68)	Acc@5  99.22 ( 98.59)
Epoch: [75][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3066e-01 (4.4797e-01)	Acc@1  87.50 ( 86.67)	Acc@5  99.22 ( 98.59)
Epoch: [75][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5239e-01 (4.4767e-01)	Acc@1  88.28 ( 86.67)	Acc@5  99.22 ( 98.58)
Epoch: [75][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.8691e-01 (4.4828e-01)	Acc@1  82.81 ( 86.65)	Acc@5  96.88 ( 98.59)
Epoch: [75][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1709e-01 (4.4918e-01)	Acc@1  82.03 ( 86.61)	Acc@5  98.44 ( 98.57)
Epoch: [75][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.0151e-01 (4.4783e-01)	Acc@1  93.75 ( 86.67)	Acc@5 100.00 ( 98.57)
Epoch: [75][340/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9082e-01 (4.4725e-01)	Acc@1  79.69 ( 86.68)	Acc@5  98.44 ( 98.57)
Epoch: [75][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9160e-01 (4.4669e-01)	Acc@1  87.50 ( 86.72)	Acc@5  99.22 ( 98.58)
Epoch: [75][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6265e-01 (4.4778e-01)	Acc@1  87.50 ( 86.70)	Acc@5  99.22 ( 98.56)
Epoch: [75][370/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.1553e-01 (4.4703e-01)	Acc@1  89.84 ( 86.74)	Acc@5  97.66 ( 98.56)
Epoch: [75][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.1367e-01 (4.4753e-01)	Acc@1  86.72 ( 86.72)	Acc@5  97.66 ( 98.56)
Epoch: [75][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.8901e-01 (4.4791e-01)	Acc@1  83.75 ( 86.69)	Acc@5 100.00 ( 98.57)
## e[75] optimizer.zero_grad (sum) time: 0.40645575523376465
## e[75]       loss.backward (sum) time: 7.384777545928955
## e[75]      optimizer.step (sum) time: 3.6062793731689453
## epoch[75] training(only) time: 26.222169160842896
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1357e+00 (1.1357e+00)	Acc@1  67.00 ( 67.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.1602e+00 (1.2192e+00)	Acc@1  70.00 ( 69.09)	Acc@5  93.00 ( 90.09)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6406e+00 (1.2188e+00)	Acc@1  63.00 ( 68.33)	Acc@5  87.00 ( 90.14)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6602e+00 (1.2472e+00)	Acc@1  57.00 ( 67.35)	Acc@5  87.00 ( 89.71)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1953e+00 (1.2246e+00)	Acc@1  70.00 ( 67.46)	Acc@5  91.00 ( 90.39)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2461e+00 (1.2282e+00)	Acc@1  70.00 ( 67.27)	Acc@5  91.00 ( 90.51)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.3037e+00 (1.2156e+00)	Acc@1  62.00 ( 67.46)	Acc@5  91.00 ( 90.80)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0195e+00 (1.2174e+00)	Acc@1  72.00 ( 67.68)	Acc@5  92.00 ( 90.83)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3311e+00 (1.2242e+00)	Acc@1  64.00 ( 67.48)	Acc@5  89.00 ( 90.73)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8516e+00 (1.2154e+00)	Acc@1  56.00 ( 67.55)	Acc@5  84.00 ( 90.80)
 * Acc@1 67.830 Acc@5 90.820
### epoch[75] execution time: 29.0541410446167
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.232 ( 0.232)	Data  0.154 ( 0.154)	Loss 4.6191e-01 (4.6191e-01)	Acc@1  85.16 ( 85.16)	Acc@5  97.66 ( 97.66)
Epoch: [76][ 10/391]	Time  0.068 ( 0.081)	Data  0.001 ( 0.015)	Loss 3.7158e-01 (4.2330e-01)	Acc@1  89.06 ( 87.57)	Acc@5  99.22 ( 98.79)
Epoch: [76][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.5557e-01 (4.3193e-01)	Acc@1  85.16 ( 87.35)	Acc@5 100.00 ( 98.70)
Epoch: [76][ 30/391]	Time  0.070 ( 0.072)	Data  0.001 ( 0.006)	Loss 4.7339e-01 (4.3979e-01)	Acc@1  86.72 ( 86.87)	Acc@5  98.44 ( 98.61)
Epoch: [76][ 40/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.6704e-01 (4.5487e-01)	Acc@1  85.94 ( 86.39)	Acc@5  98.44 ( 98.53)
Epoch: [76][ 50/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.7754e-01 (4.4647e-01)	Acc@1  87.50 ( 86.84)	Acc@5  98.44 ( 98.59)
Epoch: [76][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.4507e-01 (4.4727e-01)	Acc@1  88.28 ( 86.78)	Acc@5  98.44 ( 98.64)
Epoch: [76][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.1553e-01 (4.4030e-01)	Acc@1  90.62 ( 87.07)	Acc@5  98.44 ( 98.68)
Epoch: [76][ 80/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.4932e-01 (4.4381e-01)	Acc@1  83.59 ( 87.00)	Acc@5  98.44 ( 98.62)
Epoch: [76][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.0195e-01 (4.4415e-01)	Acc@1  89.06 ( 87.15)	Acc@5  97.66 ( 98.58)
Epoch: [76][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.4546e-01 (4.4169e-01)	Acc@1  89.84 ( 87.14)	Acc@5  99.22 ( 98.60)
Epoch: [76][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.6851e-01 (4.4105e-01)	Acc@1  83.59 ( 87.13)	Acc@5  99.22 ( 98.61)
Epoch: [76][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.8745e-01 (4.4012e-01)	Acc@1  89.06 ( 87.13)	Acc@5  98.44 ( 98.59)
Epoch: [76][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4482e-01 (4.3980e-01)	Acc@1  85.94 ( 87.15)	Acc@5  98.44 ( 98.60)
Epoch: [76][140/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.6436e-01 (4.3873e-01)	Acc@1  88.28 ( 87.22)	Acc@5  98.44 ( 98.60)
Epoch: [76][150/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1953e-01 (4.3983e-01)	Acc@1  89.06 ( 87.19)	Acc@5  96.88 ( 98.56)
Epoch: [76][160/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.4692e-01 (4.3870e-01)	Acc@1  89.84 ( 87.18)	Acc@5  98.44 ( 98.55)
Epoch: [76][170/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.5962e-01 (4.3916e-01)	Acc@1  92.19 ( 87.21)	Acc@5  97.66 ( 98.56)
Epoch: [76][180/391]	Time  0.072 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3506e-01 (4.3907e-01)	Acc@1  88.28 ( 87.17)	Acc@5  99.22 ( 98.59)
Epoch: [76][190/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.7085e-01 (4.3848e-01)	Acc@1  91.41 ( 87.22)	Acc@5  99.22 ( 98.59)
Epoch: [76][200/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9585e-01 (4.3935e-01)	Acc@1  84.38 ( 87.20)	Acc@5  99.22 ( 98.60)
Epoch: [76][210/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.6885e-01 (4.3949e-01)	Acc@1  80.47 ( 87.14)	Acc@5  97.66 ( 98.61)
Epoch: [76][220/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.5615e-01 (4.3952e-01)	Acc@1  82.81 ( 87.17)	Acc@5  98.44 ( 98.61)
Epoch: [76][230/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5825e-01 (4.4140e-01)	Acc@1  85.94 ( 87.13)	Acc@5  99.22 ( 98.61)
Epoch: [76][240/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.4912e-01 (4.4070e-01)	Acc@1  89.06 ( 87.19)	Acc@5 100.00 ( 98.61)
Epoch: [76][250/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.4346e-01 (4.4131e-01)	Acc@1  83.59 ( 87.15)	Acc@5  96.88 ( 98.61)
Epoch: [76][260/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3115e-01 (4.4220e-01)	Acc@1  86.72 ( 87.07)	Acc@5 100.00 ( 98.61)
Epoch: [76][270/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.5425e-01 (4.4213e-01)	Acc@1  92.19 ( 87.06)	Acc@5  98.44 ( 98.61)
Epoch: [76][280/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3223e-01 (4.4105e-01)	Acc@1  86.72 ( 87.09)	Acc@5  96.88 ( 98.62)
Epoch: [76][290/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (4.4265e-01)	Acc@1  82.81 ( 87.01)	Acc@5  99.22 ( 98.62)
Epoch: [76][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1797e-01 (4.4324e-01)	Acc@1  89.84 ( 86.99)	Acc@5  98.44 ( 98.61)
Epoch: [76][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8257e-01 (4.4284e-01)	Acc@1  92.97 ( 87.00)	Acc@5 100.00 ( 98.62)
Epoch: [76][320/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6230e-01 (4.4165e-01)	Acc@1  89.06 ( 87.05)	Acc@5 100.00 ( 98.64)
Epoch: [76][330/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8486e-01 (4.4270e-01)	Acc@1  86.72 ( 87.02)	Acc@5  98.44 ( 98.62)
Epoch: [76][340/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6631e-01 (4.4289e-01)	Acc@1  87.50 ( 87.02)	Acc@5  98.44 ( 98.61)
Epoch: [76][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5312e-01 (4.4336e-01)	Acc@1  84.38 ( 86.98)	Acc@5  98.44 ( 98.61)
Epoch: [76][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4531e-01 (4.4226e-01)	Acc@1  85.16 ( 87.01)	Acc@5  99.22 ( 98.63)
Epoch: [76][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4053e-01 (4.4271e-01)	Acc@1  85.94 ( 87.00)	Acc@5  97.66 ( 98.62)
Epoch: [76][380/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1226e-01 (4.4301e-01)	Acc@1  89.06 ( 86.99)	Acc@5 100.00 ( 98.63)
Epoch: [76][390/391]	Time  0.052 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.3691e-01 (4.4324e-01)	Acc@1  91.25 ( 86.99)	Acc@5  98.75 ( 98.63)
## e[76] optimizer.zero_grad (sum) time: 0.4083836078643799
## e[76]       loss.backward (sum) time: 7.380014657974243
## e[76]      optimizer.step (sum) time: 3.6182949542999268
## epoch[76] training(only) time: 26.35482692718506
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 1.1553e+00 (1.1553e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 1.1455e+00 (1.2248e+00)	Acc@1  71.00 ( 69.36)	Acc@5  93.00 ( 89.36)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6309e+00 (1.2219e+00)	Acc@1  64.00 ( 69.10)	Acc@5  88.00 ( 89.86)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6992e+00 (1.2505e+00)	Acc@1  58.00 ( 67.97)	Acc@5  87.00 ( 89.45)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1943e+00 (1.2260e+00)	Acc@1  69.00 ( 67.76)	Acc@5  91.00 ( 90.20)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2588e+00 (1.2300e+00)	Acc@1  68.00 ( 67.55)	Acc@5  90.00 ( 90.31)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.2832e+00 (1.2194e+00)	Acc@1  63.00 ( 67.62)	Acc@5  92.00 ( 90.52)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0283e+00 (1.2209e+00)	Acc@1  72.00 ( 67.80)	Acc@5  93.00 ( 90.51)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3203e+00 (1.2270e+00)	Acc@1  66.00 ( 67.63)	Acc@5  89.00 ( 90.43)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.8369e+00 (1.2182e+00)	Acc@1  53.00 ( 67.62)	Acc@5  83.00 ( 90.52)
 * Acc@1 67.890 Acc@5 90.590
### epoch[76] execution time: 29.177146911621094
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.227 ( 0.227)	Data  0.149 ( 0.149)	Loss 3.9014e-01 (3.9014e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.067 ( 0.082)	Data  0.001 ( 0.015)	Loss 3.5596e-01 (4.0951e-01)	Acc@1  88.28 ( 88.07)	Acc@5 100.00 ( 99.36)
Epoch: [77][ 20/391]	Time  0.066 ( 0.074)	Data  0.001 ( 0.008)	Loss 6.2891e-01 (4.2696e-01)	Acc@1  82.81 ( 87.69)	Acc@5  96.09 ( 99.00)
Epoch: [77][ 30/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.006)	Loss 3.9380e-01 (4.3343e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 98.82)
Epoch: [77][ 40/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.005)	Loss 3.7549e-01 (4.3401e-01)	Acc@1  89.06 ( 87.18)	Acc@5 100.00 ( 98.78)
Epoch: [77][ 50/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.4385e-01 (4.3570e-01)	Acc@1  85.94 ( 87.25)	Acc@5  97.66 ( 98.70)
Epoch: [77][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.6704e-01 (4.3362e-01)	Acc@1  85.94 ( 87.28)	Acc@5  99.22 ( 98.74)
Epoch: [77][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.9307e-01 (4.3681e-01)	Acc@1  87.50 ( 87.04)	Acc@5 100.00 ( 98.79)
Epoch: [77][ 80/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.1138e-01 (4.3464e-01)	Acc@1  85.16 ( 87.10)	Acc@5 100.00 ( 98.79)
Epoch: [77][ 90/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.6655e-01 (4.3414e-01)	Acc@1  87.50 ( 87.19)	Acc@5  98.44 ( 98.76)
Epoch: [77][100/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.1514e-01 (4.3700e-01)	Acc@1  85.94 ( 87.11)	Acc@5  97.66 ( 98.72)
Epoch: [77][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5825e-01 (4.3830e-01)	Acc@1  87.50 ( 87.03)	Acc@5  99.22 ( 98.72)
Epoch: [77][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.9766e-01 (4.3867e-01)	Acc@1  81.25 ( 87.00)	Acc@5  96.88 ( 98.72)
Epoch: [77][130/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8647e-01 (4.3639e-01)	Acc@1  87.50 ( 87.08)	Acc@5  99.22 ( 98.71)
Epoch: [77][140/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9390e-01 (4.3613e-01)	Acc@1  85.16 ( 87.11)	Acc@5 100.00 ( 98.73)
Epoch: [77][150/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9585e-01 (4.3468e-01)	Acc@1  83.59 ( 87.09)	Acc@5  98.44 ( 98.77)
Epoch: [77][160/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0161e-01 (4.3470e-01)	Acc@1  85.94 ( 87.06)	Acc@5 100.00 ( 98.76)
Epoch: [77][170/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3018e-01 (4.3473e-01)	Acc@1  87.50 ( 87.05)	Acc@5  98.44 ( 98.76)
Epoch: [77][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4751e-01 (4.3639e-01)	Acc@1  87.50 ( 87.01)	Acc@5  96.88 ( 98.75)
Epoch: [77][190/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0942e-01 (4.3635e-01)	Acc@1  88.28 ( 87.02)	Acc@5  98.44 ( 98.75)
Epoch: [77][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6670e-01 (4.3508e-01)	Acc@1  89.06 ( 87.08)	Acc@5 100.00 ( 98.78)
Epoch: [77][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9282e-01 (4.3624e-01)	Acc@1  85.94 ( 87.00)	Acc@5 100.00 ( 98.77)
Epoch: [77][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7461e-01 (4.3715e-01)	Acc@1  86.72 ( 87.00)	Acc@5  97.66 ( 98.76)
Epoch: [77][230/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5947e-01 (4.3766e-01)	Acc@1  87.50 ( 86.99)	Acc@5  97.66 ( 98.75)
Epoch: [77][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8037e-01 (4.3773e-01)	Acc@1  91.41 ( 87.01)	Acc@5  98.44 ( 98.75)
Epoch: [77][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6362e-01 (4.3821e-01)	Acc@1  82.81 ( 86.96)	Acc@5  99.22 ( 98.75)
Epoch: [77][260/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1968e-01 (4.3803e-01)	Acc@1  85.16 ( 86.98)	Acc@5  99.22 ( 98.75)
Epoch: [77][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9624e-01 (4.3892e-01)	Acc@1  86.72 ( 86.98)	Acc@5  98.44 ( 98.71)
Epoch: [77][280/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3408e-01 (4.3859e-01)	Acc@1  85.94 ( 86.98)	Acc@5 100.00 ( 98.72)
Epoch: [77][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.3423e-01 (4.3758e-01)	Acc@1  90.62 ( 87.03)	Acc@5 100.00 ( 98.71)
Epoch: [77][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0820e-01 (4.3754e-01)	Acc@1  89.06 ( 87.04)	Acc@5  97.66 ( 98.72)
Epoch: [77][310/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3530e-01 (4.3799e-01)	Acc@1  84.38 ( 87.02)	Acc@5  99.22 ( 98.72)
Epoch: [77][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3286e-01 (4.3804e-01)	Acc@1  85.94 ( 87.00)	Acc@5  98.44 ( 98.71)
Epoch: [77][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4971e-01 (4.3809e-01)	Acc@1  85.94 ( 86.98)	Acc@5  99.22 ( 98.72)
Epoch: [77][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3906e-01 (4.3810e-01)	Acc@1  85.94 ( 87.01)	Acc@5  96.88 ( 98.70)
Epoch: [77][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.0156e-01 (4.3889e-01)	Acc@1  80.47 ( 87.00)	Acc@5  96.88 ( 98.70)
Epoch: [77][360/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.7388e-01 (4.3856e-01)	Acc@1  82.03 ( 86.99)	Acc@5  99.22 ( 98.70)
Epoch: [77][370/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.8350e-01 (4.3948e-01)	Acc@1  84.38 ( 86.97)	Acc@5  94.53 ( 98.68)
Epoch: [77][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.9990e-01 (4.4016e-01)	Acc@1  90.62 ( 86.96)	Acc@5  97.66 ( 98.67)
Epoch: [77][390/391]	Time  0.048 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.3081e-01 (4.4019e-01)	Acc@1  92.50 ( 86.95)	Acc@5  97.50 ( 98.67)
## e[77] optimizer.zero_grad (sum) time: 0.40398120880126953
## e[77]       loss.backward (sum) time: 7.329555511474609
## e[77]      optimizer.step (sum) time: 3.5478134155273438
## epoch[77] training(only) time: 26.121976137161255
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1299e+00 (1.1299e+00)	Acc@1  67.00 ( 67.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.1436e+00 (1.2170e+00)	Acc@1  70.00 ( 68.82)	Acc@5  93.00 ( 89.82)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6396e+00 (1.2210e+00)	Acc@1  63.00 ( 68.52)	Acc@5  88.00 ( 90.14)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6699e+00 (1.2522e+00)	Acc@1  59.00 ( 67.71)	Acc@5  86.00 ( 89.61)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2090e+00 (1.2263e+00)	Acc@1  68.00 ( 67.71)	Acc@5  91.00 ( 90.27)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2305e+00 (1.2299e+00)	Acc@1  68.00 ( 67.43)	Acc@5  91.00 ( 90.41)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 1.2852e+00 (1.2176e+00)	Acc@1  62.00 ( 67.64)	Acc@5  90.00 ( 90.66)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0332e+00 (1.2183e+00)	Acc@1  73.00 ( 67.79)	Acc@5  93.00 ( 90.70)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.2949e+00 (1.2240e+00)	Acc@1  66.00 ( 67.60)	Acc@5  90.00 ( 90.64)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8535e+00 (1.2160e+00)	Acc@1  53.00 ( 67.57)	Acc@5  82.00 ( 90.69)
 * Acc@1 67.780 Acc@5 90.730
### epoch[77] execution time: 28.95071816444397
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.228 ( 0.228)	Data  0.145 ( 0.145)	Loss 4.5776e-01 (4.5776e-01)	Acc@1  83.59 ( 83.59)	Acc@5  99.22 ( 99.22)
Epoch: [78][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.014)	Loss 4.8779e-01 (4.8034e-01)	Acc@1  85.16 ( 84.94)	Acc@5  98.44 ( 98.44)
Epoch: [78][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 3.7109e-01 (4.5068e-01)	Acc@1  88.28 ( 86.72)	Acc@5  99.22 ( 98.62)
Epoch: [78][ 30/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.006)	Loss 4.1284e-01 (4.4990e-01)	Acc@1  87.50 ( 86.74)	Acc@5 100.00 ( 98.64)
Epoch: [78][ 40/391]	Time  0.069 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.1211e-01 (4.5002e-01)	Acc@1  85.94 ( 86.70)	Acc@5  99.22 ( 98.61)
Epoch: [78][ 50/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.8828e-01 (4.4655e-01)	Acc@1  86.72 ( 86.93)	Acc@5  98.44 ( 98.62)
Epoch: [78][ 60/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.3350e-01 (4.4191e-01)	Acc@1  89.06 ( 87.15)	Acc@5  99.22 ( 98.63)
Epoch: [78][ 70/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 6.5625e-01 (4.4332e-01)	Acc@1  81.25 ( 87.07)	Acc@5  96.09 ( 98.56)
Epoch: [78][ 80/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.3091e-01 (4.4184e-01)	Acc@1  88.28 ( 86.98)	Acc@5  99.22 ( 98.63)
Epoch: [78][ 90/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.6182e-01 (4.3892e-01)	Acc@1  89.06 ( 87.10)	Acc@5  99.22 ( 98.63)
Epoch: [78][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.8291e-01 (4.3877e-01)	Acc@1  88.28 ( 87.22)	Acc@5  98.44 ( 98.65)
Epoch: [78][110/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.0391e-01 (4.3718e-01)	Acc@1  86.72 ( 87.24)	Acc@5  98.44 ( 98.68)
Epoch: [78][120/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.1260e-01 (4.3779e-01)	Acc@1  85.94 ( 87.13)	Acc@5  98.44 ( 98.68)
Epoch: [78][130/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.5693e-01 (4.3597e-01)	Acc@1  90.62 ( 87.25)	Acc@5 100.00 ( 98.71)
Epoch: [78][140/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3286e-01 (4.3636e-01)	Acc@1  85.16 ( 87.26)	Acc@5  99.22 ( 98.69)
Epoch: [78][150/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0186e-01 (4.3723e-01)	Acc@1  86.72 ( 87.20)	Acc@5 100.00 ( 98.69)
Epoch: [78][160/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8950e-01 (4.3599e-01)	Acc@1  87.50 ( 87.28)	Acc@5  96.88 ( 98.68)
Epoch: [78][170/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9585e-01 (4.3515e-01)	Acc@1  85.16 ( 87.34)	Acc@5  97.66 ( 98.68)
Epoch: [78][180/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9771e-01 (4.3332e-01)	Acc@1  89.84 ( 87.44)	Acc@5 100.00 ( 98.71)
Epoch: [78][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8379e-01 (4.3346e-01)	Acc@1  89.84 ( 87.44)	Acc@5 100.00 ( 98.72)
Epoch: [78][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9746e-01 (4.3450e-01)	Acc@1  89.06 ( 87.39)	Acc@5  99.22 ( 98.73)
Epoch: [78][210/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4189e-01 (4.3449e-01)	Acc@1  88.28 ( 87.40)	Acc@5  98.44 ( 98.73)
Epoch: [78][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5522e-01 (4.3471e-01)	Acc@1  92.97 ( 87.40)	Acc@5  98.44 ( 98.70)
Epoch: [78][230/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8696e-01 (4.3392e-01)	Acc@1  89.06 ( 87.42)	Acc@5  99.22 ( 98.71)
Epoch: [78][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8486e-01 (4.3399e-01)	Acc@1  85.94 ( 87.43)	Acc@5 100.00 ( 98.72)
Epoch: [78][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7217e-01 (4.3450e-01)	Acc@1  85.94 ( 87.43)	Acc@5  98.44 ( 98.71)
Epoch: [78][260/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8013e-01 (4.3463e-01)	Acc@1  89.84 ( 87.43)	Acc@5  99.22 ( 98.72)
Epoch: [78][270/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4141e-01 (4.3509e-01)	Acc@1  87.50 ( 87.40)	Acc@5  98.44 ( 98.71)
Epoch: [78][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1162e-01 (4.3543e-01)	Acc@1  89.84 ( 87.37)	Acc@5  97.66 ( 98.71)
Epoch: [78][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3076e-01 (4.3659e-01)	Acc@1  82.03 ( 87.28)	Acc@5  98.44 ( 98.71)
Epoch: [78][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7476e-01 (4.3629e-01)	Acc@1  89.06 ( 87.30)	Acc@5  98.44 ( 98.71)
Epoch: [78][310/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3140e-01 (4.3546e-01)	Acc@1  86.72 ( 87.29)	Acc@5  99.22 ( 98.72)
Epoch: [78][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4531e-01 (4.3722e-01)	Acc@1  82.03 ( 87.20)	Acc@5 100.00 ( 98.70)
Epoch: [78][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8804e-01 (4.3754e-01)	Acc@1  88.28 ( 87.20)	Acc@5  97.66 ( 98.68)
Epoch: [78][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3408e-01 (4.3845e-01)	Acc@1  87.50 ( 87.14)	Acc@5  98.44 ( 98.67)
Epoch: [78][350/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7632e-01 (4.3841e-01)	Acc@1  86.72 ( 87.12)	Acc@5  99.22 ( 98.67)
Epoch: [78][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.4824e-01 (4.3908e-01)	Acc@1  86.72 ( 87.09)	Acc@5  98.44 ( 98.66)
Epoch: [78][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.5205e-01 (4.3900e-01)	Acc@1  88.28 ( 87.08)	Acc@5  98.44 ( 98.66)
Epoch: [78][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.5889e-01 (4.3937e-01)	Acc@1  89.84 ( 87.07)	Acc@5  99.22 ( 98.66)
Epoch: [78][390/391]	Time  0.058 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.3467e-01 (4.3896e-01)	Acc@1  83.75 ( 87.10)	Acc@5  98.75 ( 98.66)
## e[78] optimizer.zero_grad (sum) time: 0.4049217700958252
## e[78]       loss.backward (sum) time: 7.346680164337158
## e[78]      optimizer.step (sum) time: 3.563241720199585
## epoch[78] training(only) time: 26.187763214111328
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.1553e+00 (1.1553e+00)	Acc@1  67.00 ( 67.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1621e+00 (1.2191e+00)	Acc@1  70.00 ( 69.00)	Acc@5  93.00 ( 90.09)
Test: [ 20/100]	Time  0.029 ( 0.033)	Loss 1.6357e+00 (1.2217e+00)	Acc@1  63.00 ( 68.67)	Acc@5  89.00 ( 90.33)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 1.6709e+00 (1.2521e+00)	Acc@1  57.00 ( 67.58)	Acc@5  87.00 ( 89.68)
Test: [ 40/100]	Time  0.029 ( 0.029)	Loss 1.1914e+00 (1.2283e+00)	Acc@1  69.00 ( 67.63)	Acc@5  91.00 ( 90.29)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2510e+00 (1.2308e+00)	Acc@1  71.00 ( 67.53)	Acc@5  90.00 ( 90.49)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 1.2646e+00 (1.2185e+00)	Acc@1  63.00 ( 67.79)	Acc@5  91.00 ( 90.72)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0293e+00 (1.2199e+00)	Acc@1  75.00 ( 67.96)	Acc@5  92.00 ( 90.69)
Test: [ 80/100]	Time  0.031 ( 0.028)	Loss 1.2988e+00 (1.2260e+00)	Acc@1  63.00 ( 67.68)	Acc@5  89.00 ( 90.57)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 1.8613e+00 (1.2176e+00)	Acc@1  54.00 ( 67.70)	Acc@5  84.00 ( 90.64)
 * Acc@1 67.960 Acc@5 90.690
### epoch[78] execution time: 28.997554779052734
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.227 ( 0.227)	Data  0.145 ( 0.145)	Loss 5.0195e-01 (5.0195e-01)	Acc@1  84.38 ( 84.38)	Acc@5  98.44 ( 98.44)
Epoch: [79][ 10/391]	Time  0.075 ( 0.082)	Data  0.001 ( 0.014)	Loss 4.3604e-01 (4.4775e-01)	Acc@1  84.38 ( 86.36)	Acc@5  97.66 ( 98.65)
Epoch: [79][ 20/391]	Time  0.068 ( 0.075)	Data  0.001 ( 0.008)	Loss 4.2798e-01 (4.4500e-01)	Acc@1  87.50 ( 87.35)	Acc@5  99.22 ( 98.51)
Epoch: [79][ 30/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.006)	Loss 4.9585e-01 (4.4202e-01)	Acc@1  86.72 ( 87.07)	Acc@5  96.88 ( 98.61)
Epoch: [79][ 40/391]	Time  0.070 ( 0.071)	Data  0.001 ( 0.005)	Loss 3.8159e-01 (4.4328e-01)	Acc@1  86.72 ( 87.16)	Acc@5 100.00 ( 98.65)
Epoch: [79][ 50/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.4590e-01 (4.3971e-01)	Acc@1  82.03 ( 87.30)	Acc@5  96.09 ( 98.65)
Epoch: [79][ 60/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.1284e-01 (4.4361e-01)	Acc@1  87.50 ( 87.04)	Acc@5 100.00 ( 98.67)
Epoch: [79][ 70/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.2783e-01 (4.4102e-01)	Acc@1  85.94 ( 87.07)	Acc@5  96.09 ( 98.61)
Epoch: [79][ 80/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.5483e-01 (4.3606e-01)	Acc@1  83.59 ( 87.13)	Acc@5  97.66 ( 98.61)
Epoch: [79][ 90/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.8755e-01 (4.3388e-01)	Acc@1  85.16 ( 87.18)	Acc@5  99.22 ( 98.68)
Epoch: [79][100/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 5.3711e-01 (4.3677e-01)	Acc@1  85.16 ( 87.07)	Acc@5  98.44 ( 98.67)
Epoch: [79][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3384e-01 (4.3497e-01)	Acc@1  85.16 ( 87.11)	Acc@5  99.22 ( 98.70)
Epoch: [79][120/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.6655e-01 (4.3951e-01)	Acc@1  87.50 ( 86.91)	Acc@5  99.22 ( 98.69)
Epoch: [79][130/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7144e-01 (4.3976e-01)	Acc@1  83.59 ( 86.86)	Acc@5  99.22 ( 98.71)
Epoch: [79][140/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.1968e-01 (4.4025e-01)	Acc@1  88.28 ( 86.82)	Acc@5  98.44 ( 98.69)
Epoch: [79][150/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3188e-01 (4.4005e-01)	Acc@1  87.50 ( 86.85)	Acc@5  97.66 ( 98.68)
Epoch: [79][160/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.9287e-01 (4.4275e-01)	Acc@1  79.69 ( 86.78)	Acc@5  97.66 ( 98.66)
Epoch: [79][170/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.4839e-01 (4.4162e-01)	Acc@1  90.62 ( 86.80)	Acc@5 100.00 ( 98.68)
Epoch: [79][180/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.2832e-01 (4.4284e-01)	Acc@1  82.03 ( 86.78)	Acc@5  98.44 ( 98.69)
Epoch: [79][190/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.7515e-01 (4.4181e-01)	Acc@1  94.53 ( 86.87)	Acc@5  99.22 ( 98.70)
Epoch: [79][200/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.7036e-01 (4.4203e-01)	Acc@1  90.62 ( 86.84)	Acc@5  97.66 ( 98.69)
Epoch: [79][210/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.8086e-01 (4.4069e-01)	Acc@1  88.28 ( 86.88)	Acc@5  99.22 ( 98.70)
Epoch: [79][220/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0820e-01 (4.4054e-01)	Acc@1  89.84 ( 86.92)	Acc@5  98.44 ( 98.70)
Epoch: [79][230/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9307e-01 (4.3949e-01)	Acc@1  88.28 ( 86.95)	Acc@5  99.22 ( 98.69)
Epoch: [79][240/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (4.4180e-01)	Acc@1  85.16 ( 86.86)	Acc@5  97.66 ( 98.66)
Epoch: [79][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3018e-01 (4.4199e-01)	Acc@1  87.50 ( 86.87)	Acc@5  99.22 ( 98.66)
Epoch: [79][260/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4204e-01 (4.4241e-01)	Acc@1  87.50 ( 86.86)	Acc@5 100.00 ( 98.65)
Epoch: [79][270/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9487e-01 (4.4124e-01)	Acc@1  82.81 ( 86.89)	Acc@5  99.22 ( 98.65)
Epoch: [79][280/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9111e-01 (4.4124e-01)	Acc@1  88.28 ( 86.87)	Acc@5  99.22 ( 98.66)
Epoch: [79][290/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7910e-01 (4.4157e-01)	Acc@1  83.59 ( 86.88)	Acc@5  97.66 ( 98.66)
Epoch: [79][300/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0146e-01 (4.4218e-01)	Acc@1  83.59 ( 86.85)	Acc@5  99.22 ( 98.66)
Epoch: [79][310/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.0615e-01 (4.4174e-01)	Acc@1  92.19 ( 86.89)	Acc@5  99.22 ( 98.66)
Epoch: [79][320/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0586e-01 (4.4149e-01)	Acc@1  83.59 ( 86.90)	Acc@5  99.22 ( 98.65)
Epoch: [79][330/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1870e-01 (4.4096e-01)	Acc@1  85.94 ( 86.94)	Acc@5  99.22 ( 98.66)
Epoch: [79][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4595e-01 (4.4054e-01)	Acc@1  91.41 ( 86.95)	Acc@5  99.22 ( 98.66)
Epoch: [79][350/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4214e-01 (4.3992e-01)	Acc@1  89.84 ( 86.97)	Acc@5  96.88 ( 98.67)
Epoch: [79][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6670e-01 (4.3993e-01)	Acc@1  90.62 ( 86.97)	Acc@5  99.22 ( 98.67)
Epoch: [79][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7583e-01 (4.3942e-01)	Acc@1  88.28 ( 86.98)	Acc@5  98.44 ( 98.67)
Epoch: [79][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.1221e-01 (4.3962e-01)	Acc@1  83.59 ( 86.98)	Acc@5  97.66 ( 98.67)
Epoch: [79][390/391]	Time  0.053 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.2593e-01 (4.4000e-01)	Acc@1  91.25 ( 87.01)	Acc@5 100.00 ( 98.66)
## e[79] optimizer.zero_grad (sum) time: 0.4104640483856201
## e[79]       loss.backward (sum) time: 7.391858339309692
## e[79]      optimizer.step (sum) time: 3.6495251655578613
## epoch[79] training(only) time: 26.363789796829224
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.1523e+00 (1.1523e+00)	Acc@1  67.00 ( 67.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.034 ( 0.039)	Loss 1.1182e+00 (1.2194e+00)	Acc@1  71.00 ( 69.09)	Acc@5  93.00 ( 89.36)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6338e+00 (1.2191e+00)	Acc@1  64.00 ( 68.48)	Acc@5  87.00 ( 89.67)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6895e+00 (1.2502e+00)	Acc@1  59.00 ( 67.55)	Acc@5  85.00 ( 89.29)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.2148e+00 (1.2270e+00)	Acc@1  70.00 ( 67.61)	Acc@5  91.00 ( 89.98)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2510e+00 (1.2314e+00)	Acc@1  67.00 ( 67.43)	Acc@5  90.00 ( 90.12)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.3076e+00 (1.2184e+00)	Acc@1  63.00 ( 67.66)	Acc@5  90.00 ( 90.44)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.9512e-01 (1.2185e+00)	Acc@1  77.00 ( 67.93)	Acc@5  92.00 ( 90.44)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.3027e+00 (1.2247e+00)	Acc@1  64.00 ( 67.64)	Acc@5  90.00 ( 90.36)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8320e+00 (1.2162e+00)	Acc@1  51.00 ( 67.64)	Acc@5  83.00 ( 90.47)
 * Acc@1 67.890 Acc@5 90.560
### epoch[79] execution time: 29.20224118232727
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.225 ( 0.225)	Data  0.146 ( 0.146)	Loss 4.8486e-01 (4.8486e-01)	Acc@1  87.50 ( 87.50)	Acc@5  98.44 ( 98.44)
Epoch: [80][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.014)	Loss 4.8291e-01 (4.4276e-01)	Acc@1  85.16 ( 87.57)	Acc@5  97.66 ( 98.86)
Epoch: [80][ 20/391]	Time  0.069 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.0820e-01 (4.3154e-01)	Acc@1  89.06 ( 87.69)	Acc@5  98.44 ( 98.85)
Epoch: [80][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.006)	Loss 3.6377e-01 (4.3032e-01)	Acc@1  91.41 ( 87.78)	Acc@5  99.22 ( 98.79)
Epoch: [80][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.005)	Loss 3.5327e-01 (4.2213e-01)	Acc@1  89.84 ( 88.01)	Acc@5 100.00 ( 98.88)
Epoch: [80][ 50/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.5273e-01 (4.2974e-01)	Acc@1  85.16 ( 87.70)	Acc@5  98.44 ( 98.81)
Epoch: [80][ 60/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.4053e-01 (4.3245e-01)	Acc@1  80.47 ( 87.59)	Acc@5  99.22 ( 98.76)
Epoch: [80][ 70/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.3872e-01 (4.3623e-01)	Acc@1  91.41 ( 87.68)	Acc@5  98.44 ( 98.72)
Epoch: [80][ 80/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.3032e-01 (4.3563e-01)	Acc@1  91.41 ( 87.65)	Acc@5  99.22 ( 98.74)
Epoch: [80][ 90/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.9121e-01 (4.3472e-01)	Acc@1  85.16 ( 87.66)	Acc@5  98.44 ( 98.71)
Epoch: [80][100/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.2285e-01 (4.3716e-01)	Acc@1  86.72 ( 87.59)	Acc@5 100.00 ( 98.72)
Epoch: [80][110/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.0391e-01 (4.3896e-01)	Acc@1  84.38 ( 87.48)	Acc@5  98.44 ( 98.71)
Epoch: [80][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8584e-01 (4.4174e-01)	Acc@1  85.16 ( 87.36)	Acc@5  97.66 ( 98.68)
Epoch: [80][130/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4360e-01 (4.3994e-01)	Acc@1  85.16 ( 87.33)	Acc@5  99.22 ( 98.71)
Epoch: [80][140/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3140e-01 (4.3760e-01)	Acc@1  85.94 ( 87.34)	Acc@5  98.44 ( 98.71)
Epoch: [80][150/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.5947e-01 (4.3882e-01)	Acc@1  85.94 ( 87.26)	Acc@5  99.22 ( 98.73)
Epoch: [80][160/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.7842e-01 (4.3784e-01)	Acc@1  87.50 ( 87.32)	Acc@5 100.00 ( 98.73)
Epoch: [80][170/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4580e-01 (4.3849e-01)	Acc@1  85.94 ( 87.27)	Acc@5 100.00 ( 98.75)
Epoch: [80][180/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9209e-01 (4.3661e-01)	Acc@1  85.16 ( 87.30)	Acc@5  98.44 ( 98.77)
Epoch: [80][190/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.1895e-01 (4.3566e-01)	Acc@1  87.50 ( 87.34)	Acc@5  98.44 ( 98.78)
Epoch: [80][200/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9731e-01 (4.3565e-01)	Acc@1  81.25 ( 87.28)	Acc@5  99.22 ( 98.80)
Epoch: [80][210/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.8843e-01 (4.3594e-01)	Acc@1  88.28 ( 87.21)	Acc@5  97.66 ( 98.79)
Epoch: [80][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1250e-01 (4.3550e-01)	Acc@1  94.53 ( 87.25)	Acc@5  99.22 ( 98.78)
Epoch: [80][230/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8550e-01 (4.3562e-01)	Acc@1  87.50 ( 87.21)	Acc@5 100.00 ( 98.78)
Epoch: [80][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1089e-01 (4.3493e-01)	Acc@1  86.72 ( 87.23)	Acc@5  99.22 ( 98.78)
Epoch: [80][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0259e-01 (4.3516e-01)	Acc@1  89.84 ( 87.21)	Acc@5  97.66 ( 98.77)
Epoch: [80][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5190e-01 (4.3514e-01)	Acc@1  85.94 ( 87.20)	Acc@5  98.44 ( 98.75)
Epoch: [80][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8257e-01 (4.3573e-01)	Acc@1  88.28 ( 87.17)	Acc@5  99.22 ( 98.76)
Epoch: [80][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8853e-01 (4.3586e-01)	Acc@1  84.38 ( 87.17)	Acc@5  97.66 ( 98.75)
Epoch: [80][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3711e-01 (4.3641e-01)	Acc@1  82.81 ( 87.12)	Acc@5  97.66 ( 98.75)
Epoch: [80][300/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5142e-01 (4.3718e-01)	Acc@1  88.28 ( 87.12)	Acc@5  98.44 ( 98.75)
Epoch: [80][310/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (4.3693e-01)	Acc@1  87.50 ( 87.12)	Acc@5  99.22 ( 98.74)
Epoch: [80][320/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3174e-01 (4.3910e-01)	Acc@1  83.59 ( 87.05)	Acc@5  96.88 ( 98.71)
Epoch: [80][330/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4434e-01 (4.3854e-01)	Acc@1  86.72 ( 87.08)	Acc@5  98.44 ( 98.71)
Epoch: [80][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8477e-01 (4.3760e-01)	Acc@1  90.62 ( 87.12)	Acc@5  99.22 ( 98.71)
Epoch: [80][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8428e-01 (4.3674e-01)	Acc@1  89.84 ( 87.13)	Acc@5  99.22 ( 98.71)
Epoch: [80][360/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6777e-01 (4.3804e-01)	Acc@1  85.16 ( 87.06)	Acc@5  98.44 ( 98.70)
Epoch: [80][370/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4043e-01 (4.3773e-01)	Acc@1  86.72 ( 87.07)	Acc@5  97.66 ( 98.69)
Epoch: [80][380/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9600e-01 (4.3705e-01)	Acc@1  87.50 ( 87.09)	Acc@5 100.00 ( 98.70)
Epoch: [80][390/391]	Time  0.049 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.0063e-01 (4.3700e-01)	Acc@1  88.75 ( 87.09)	Acc@5 100.00 ( 98.70)
## e[80] optimizer.zero_grad (sum) time: 0.40770912170410156
## e[80]       loss.backward (sum) time: 7.3850390911102295
## e[80]      optimizer.step (sum) time: 3.645247220993042
## epoch[80] training(only) time: 26.273349285125732
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1533e+00 (1.1533e+00)	Acc@1  69.00 ( 69.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.026 ( 0.040)	Loss 1.1270e+00 (1.2217e+00)	Acc@1  70.00 ( 69.36)	Acc@5  93.00 ( 89.82)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6309e+00 (1.2173e+00)	Acc@1  64.00 ( 68.71)	Acc@5  88.00 ( 90.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6807e+00 (1.2485e+00)	Acc@1  59.00 ( 67.81)	Acc@5  86.00 ( 89.68)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1982e+00 (1.2259e+00)	Acc@1  71.00 ( 67.80)	Acc@5  90.00 ( 90.24)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.2549e+00 (1.2313e+00)	Acc@1  67.00 ( 67.45)	Acc@5  90.00 ( 90.39)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2725e+00 (1.2179e+00)	Acc@1  63.00 ( 67.69)	Acc@5  90.00 ( 90.59)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0166e+00 (1.2201e+00)	Acc@1  76.00 ( 67.87)	Acc@5  92.00 ( 90.55)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3125e+00 (1.2252e+00)	Acc@1  66.00 ( 67.68)	Acc@5  89.00 ( 90.46)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8633e+00 (1.2170e+00)	Acc@1  52.00 ( 67.70)	Acc@5  83.00 ( 90.52)
 * Acc@1 68.010 Acc@5 90.560
### epoch[80] execution time: 29.11547589302063
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.225 ( 0.225)	Data  0.148 ( 0.148)	Loss 4.2554e-01 (4.2554e-01)	Acc@1  87.50 ( 87.50)	Acc@5  98.44 ( 98.44)
Epoch: [81][ 10/391]	Time  0.068 ( 0.082)	Data  0.001 ( 0.015)	Loss 3.4473e-01 (4.2829e-01)	Acc@1  90.62 ( 87.57)	Acc@5 100.00 ( 99.01)
Epoch: [81][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 4.7900e-01 (4.2619e-01)	Acc@1  84.38 ( 87.61)	Acc@5 100.00 ( 98.81)
Epoch: [81][ 30/391]	Time  0.068 ( 0.072)	Data  0.001 ( 0.006)	Loss 3.7231e-01 (4.2228e-01)	Acc@1  90.62 ( 87.80)	Acc@5  99.22 ( 98.84)
Epoch: [81][ 40/391]	Time  0.072 ( 0.071)	Data  0.001 ( 0.005)	Loss 5.2002e-01 (4.2556e-01)	Acc@1  83.59 ( 87.67)	Acc@5  98.44 ( 98.72)
Epoch: [81][ 50/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.1172e-01 (4.2767e-01)	Acc@1  84.38 ( 87.58)	Acc@5  97.66 ( 98.73)
Epoch: [81][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 3.5376e-01 (4.2916e-01)	Acc@1  89.84 ( 87.31)	Acc@5  99.22 ( 98.80)
Epoch: [81][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.3398e-01 (4.2643e-01)	Acc@1  92.97 ( 87.47)	Acc@5  98.44 ( 98.76)
Epoch: [81][ 80/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.3726e-01 (4.3245e-01)	Acc@1  87.50 ( 87.30)	Acc@5 100.00 ( 98.74)
Epoch: [81][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.4312e-01 (4.3441e-01)	Acc@1  88.28 ( 87.23)	Acc@5  97.66 ( 98.71)
Epoch: [81][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.6963e-01 (4.3656e-01)	Acc@1  91.41 ( 87.15)	Acc@5  99.22 ( 98.62)
Epoch: [81][110/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7949e-01 (4.3684e-01)	Acc@1  80.47 ( 87.06)	Acc@5 100.00 ( 98.66)
Epoch: [81][120/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.4326e-01 (4.3344e-01)	Acc@1  87.50 ( 87.14)	Acc@5 100.00 ( 98.72)
Epoch: [81][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0088e-01 (4.3204e-01)	Acc@1  88.28 ( 87.17)	Acc@5  97.66 ( 98.74)
Epoch: [81][140/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.2822e-01 (4.3062e-01)	Acc@1  91.41 ( 87.36)	Acc@5  98.44 ( 98.75)
Epoch: [81][150/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7900e-01 (4.3041e-01)	Acc@1  85.16 ( 87.39)	Acc@5  98.44 ( 98.75)
Epoch: [81][160/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9390e-01 (4.3108e-01)	Acc@1  85.16 ( 87.45)	Acc@5  98.44 ( 98.74)
Epoch: [81][170/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8047e-01 (4.3189e-01)	Acc@1  85.16 ( 87.35)	Acc@5  98.44 ( 98.74)
Epoch: [81][180/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9268e-01 (4.3401e-01)	Acc@1  85.16 ( 87.29)	Acc@5  98.44 ( 98.71)
Epoch: [81][190/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8071e-01 (4.3492e-01)	Acc@1  85.94 ( 87.23)	Acc@5  97.66 ( 98.69)
Epoch: [81][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7666e-01 (4.3367e-01)	Acc@1  82.03 ( 87.29)	Acc@5  96.88 ( 98.69)
Epoch: [81][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4946e-01 (4.3425e-01)	Acc@1  87.50 ( 87.34)	Acc@5 100.00 ( 98.66)
Epoch: [81][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2505e-01 (4.3527e-01)	Acc@1  88.28 ( 87.33)	Acc@5  98.44 ( 98.65)
Epoch: [81][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8574e-01 (4.3561e-01)	Acc@1  85.16 ( 87.30)	Acc@5 100.00 ( 98.65)
Epoch: [81][240/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2593e-01 (4.3450e-01)	Acc@1  89.84 ( 87.33)	Acc@5 100.00 ( 98.65)
Epoch: [81][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4541e-01 (4.3585e-01)	Acc@1  82.81 ( 87.32)	Acc@5  97.66 ( 98.63)
Epoch: [81][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1113e-01 (4.3602e-01)	Acc@1  88.28 ( 87.29)	Acc@5  98.44 ( 98.64)
Epoch: [81][270/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3369e-01 (4.3570e-01)	Acc@1  84.38 ( 87.27)	Acc@5  98.44 ( 98.65)
Epoch: [81][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4897e-01 (4.3636e-01)	Acc@1  85.94 ( 87.26)	Acc@5  96.88 ( 98.62)
Epoch: [81][290/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6035e-01 (4.3713e-01)	Acc@1  90.62 ( 87.19)	Acc@5  98.44 ( 98.63)
Epoch: [81][300/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.5713e-01 (4.3777e-01)	Acc@1  83.59 ( 87.16)	Acc@5  99.22 ( 98.63)
Epoch: [81][310/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7134e-01 (4.3739e-01)	Acc@1  89.84 ( 87.18)	Acc@5  99.22 ( 98.63)
Epoch: [81][320/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8892e-01 (4.3735e-01)	Acc@1  91.41 ( 87.17)	Acc@5 100.00 ( 98.64)
Epoch: [81][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6240e-01 (4.3814e-01)	Acc@1  86.72 ( 87.14)	Acc@5  98.44 ( 98.64)
Epoch: [81][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1807e-01 (4.3761e-01)	Acc@1  85.94 ( 87.14)	Acc@5  99.22 ( 98.64)
Epoch: [81][350/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4482e-01 (4.3854e-01)	Acc@1  89.06 ( 87.13)	Acc@5  98.44 ( 98.63)
Epoch: [81][360/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3750e-01 (4.3927e-01)	Acc@1  89.06 ( 87.10)	Acc@5  99.22 ( 98.63)
Epoch: [81][370/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.8833e-01 (4.3924e-01)	Acc@1  92.19 ( 87.09)	Acc@5 100.00 ( 98.64)
Epoch: [81][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.8462e-01 (4.4024e-01)	Acc@1  86.72 ( 87.07)	Acc@5  98.44 ( 98.64)
Epoch: [81][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.9258e-01 (4.3969e-01)	Acc@1  91.25 ( 87.09)	Acc@5  98.75 ( 98.64)
## e[81] optimizer.zero_grad (sum) time: 0.4062488079071045
## e[81]       loss.backward (sum) time: 7.373041868209839
## e[81]      optimizer.step (sum) time: 3.6132030487060547
## epoch[81] training(only) time: 26.301608562469482
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1611e+00 (1.1611e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.1680e+00 (1.2316e+00)	Acc@1  70.00 ( 69.45)	Acc@5  93.00 ( 89.73)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6318e+00 (1.2289e+00)	Acc@1  62.00 ( 68.62)	Acc@5  88.00 ( 89.90)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6650e+00 (1.2580e+00)	Acc@1  57.00 ( 67.84)	Acc@5  86.00 ( 89.58)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1826e+00 (1.2334e+00)	Acc@1  70.00 ( 67.85)	Acc@5  91.00 ( 90.24)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.2598e+00 (1.2364e+00)	Acc@1  67.00 ( 67.67)	Acc@5  91.00 ( 90.43)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2666e+00 (1.2248e+00)	Acc@1  63.00 ( 67.75)	Acc@5  91.00 ( 90.64)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.0449e+00 (1.2273e+00)	Acc@1  76.00 ( 67.90)	Acc@5  93.00 ( 90.58)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2686e+00 (1.2331e+00)	Acc@1  63.00 ( 67.64)	Acc@5  90.00 ( 90.51)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.8711e+00 (1.2244e+00)	Acc@1  55.00 ( 67.60)	Acc@5  84.00 ( 90.57)
 * Acc@1 67.870 Acc@5 90.640
### epoch[81] execution time: 29.184823274612427
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.226 ( 0.226)	Data  0.147 ( 0.147)	Loss 3.4277e-01 (3.4277e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.014)	Loss 5.1709e-01 (4.4982e-01)	Acc@1  84.38 ( 86.79)	Acc@5  96.88 ( 97.94)
Epoch: [82][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.7949e-01 (4.6040e-01)	Acc@1  86.72 ( 86.50)	Acc@5  96.88 ( 97.99)
Epoch: [82][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 4.5703e-01 (4.6669e-01)	Acc@1  85.94 ( 86.39)	Acc@5  96.88 ( 98.03)
Epoch: [82][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.4116e-01 (4.5464e-01)	Acc@1  85.16 ( 86.78)	Acc@5  99.22 ( 98.17)
Epoch: [82][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.8804e-01 (4.5644e-01)	Acc@1  87.50 ( 86.81)	Acc@5  99.22 ( 98.25)
Epoch: [82][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.2334e-01 (4.5454e-01)	Acc@1  90.62 ( 86.91)	Acc@5  97.66 ( 98.32)
Epoch: [82][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.5996e-01 (4.4805e-01)	Acc@1  86.72 ( 87.05)	Acc@5  98.44 ( 98.43)
Epoch: [82][ 80/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.6353e-01 (4.4312e-01)	Acc@1  90.62 ( 87.16)	Acc@5  98.44 ( 98.48)
Epoch: [82][ 90/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.7168e-01 (4.4219e-01)	Acc@1  84.38 ( 87.22)	Acc@5  97.66 ( 98.51)
Epoch: [82][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.2651e-01 (4.3964e-01)	Acc@1  89.06 ( 87.35)	Acc@5  97.66 ( 98.55)
Epoch: [82][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8389e-01 (4.4005e-01)	Acc@1  81.25 ( 87.35)	Acc@5  98.44 ( 98.54)
Epoch: [82][120/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5020e-01 (4.4311e-01)	Acc@1  84.38 ( 87.18)	Acc@5 100.00 ( 98.51)
Epoch: [82][130/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7559e-01 (4.4248e-01)	Acc@1  85.94 ( 87.21)	Acc@5  97.66 ( 98.51)
Epoch: [82][140/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9243e-01 (4.4024e-01)	Acc@1  85.94 ( 87.22)	Acc@5  97.66 ( 98.53)
Epoch: [82][150/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8940e-01 (4.3899e-01)	Acc@1  86.72 ( 87.24)	Acc@5 100.00 ( 98.57)
Epoch: [82][160/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4067e-01 (4.3863e-01)	Acc@1  85.16 ( 87.25)	Acc@5  99.22 ( 98.59)
Epoch: [82][170/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1406e-01 (4.3604e-01)	Acc@1  88.28 ( 87.30)	Acc@5  97.66 ( 98.65)
Epoch: [82][180/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2773e-01 (4.3732e-01)	Acc@1  85.16 ( 87.17)	Acc@5  97.66 ( 98.65)
Epoch: [82][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7671e-01 (4.3649e-01)	Acc@1  89.84 ( 87.20)	Acc@5  99.22 ( 98.64)
Epoch: [82][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6924e-01 (4.3647e-01)	Acc@1  87.50 ( 87.20)	Acc@5  98.44 ( 98.63)
Epoch: [82][210/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9023e-01 (4.3539e-01)	Acc@1  86.72 ( 87.22)	Acc@5  98.44 ( 98.65)
Epoch: [82][220/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3774e-01 (4.3554e-01)	Acc@1  88.28 ( 87.22)	Acc@5 100.00 ( 98.66)
Epoch: [82][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5840e-01 (4.3468e-01)	Acc@1  92.19 ( 87.27)	Acc@5  99.22 ( 98.68)
Epoch: [82][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6631e-01 (4.3532e-01)	Acc@1  86.72 ( 87.27)	Acc@5  99.22 ( 98.68)
Epoch: [82][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4141e-01 (4.3455e-01)	Acc@1  88.28 ( 87.31)	Acc@5  97.66 ( 98.68)
Epoch: [82][260/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1406e-01 (4.3375e-01)	Acc@1  85.94 ( 87.32)	Acc@5  99.22 ( 98.69)
Epoch: [82][270/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1611e-01 (4.3581e-01)	Acc@1  82.81 ( 87.21)	Acc@5  98.44 ( 98.69)
Epoch: [82][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4531e-01 (4.3614e-01)	Acc@1  85.94 ( 87.21)	Acc@5  98.44 ( 98.67)
Epoch: [82][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8608e-01 (4.3648e-01)	Acc@1  86.72 ( 87.22)	Acc@5  99.22 ( 98.67)
Epoch: [82][300/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0732e-01 (4.3761e-01)	Acc@1  86.72 ( 87.18)	Acc@5  96.09 ( 98.67)
Epoch: [82][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0845e-01 (4.3761e-01)	Acc@1  86.72 ( 87.16)	Acc@5 100.00 ( 98.69)
Epoch: [82][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6499e-01 (4.3649e-01)	Acc@1  89.06 ( 87.19)	Acc@5  98.44 ( 98.70)
Epoch: [82][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0869e-01 (4.3718e-01)	Acc@1  88.28 ( 87.17)	Acc@5 100.00 ( 98.69)
Epoch: [82][340/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5996e-01 (4.3625e-01)	Acc@1  88.28 ( 87.21)	Acc@5  99.22 ( 98.69)
Epoch: [82][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.6484e-01 (4.3707e-01)	Acc@1  86.72 ( 87.20)	Acc@5  97.66 ( 98.67)
Epoch: [82][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.0732e-01 (4.3788e-01)	Acc@1  84.38 ( 87.17)	Acc@5  99.22 ( 98.67)
Epoch: [82][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 2.7905e-01 (4.3756e-01)	Acc@1  91.41 ( 87.15)	Acc@5 100.00 ( 98.66)
Epoch: [82][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.3447e-01 (4.3676e-01)	Acc@1  88.28 ( 87.18)	Acc@5 100.00 ( 98.67)
Epoch: [82][390/391]	Time  0.052 ( 0.066)	Data  0.001 ( 0.001)	Loss 4.7900e-01 (4.3779e-01)	Acc@1  88.75 ( 87.15)	Acc@5  97.50 ( 98.64)
## e[82] optimizer.zero_grad (sum) time: 0.4024338722229004
## e[82]       loss.backward (sum) time: 7.374740123748779
## e[82]      optimizer.step (sum) time: 3.4998626708984375
## epoch[82] training(only) time: 26.056938648223877
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.1338e+00 (1.1338e+00)	Acc@1  69.00 ( 69.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1533e+00 (1.2112e+00)	Acc@1  69.00 ( 69.27)	Acc@5  92.00 ( 90.09)
Test: [ 20/100]	Time  0.031 ( 0.033)	Loss 1.6230e+00 (1.2126e+00)	Acc@1  63.00 ( 68.81)	Acc@5  87.00 ( 90.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6875e+00 (1.2448e+00)	Acc@1  58.00 ( 67.68)	Acc@5  85.00 ( 89.71)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.1836e+00 (1.2210e+00)	Acc@1  70.00 ( 67.73)	Acc@5  91.00 ( 90.37)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2412e+00 (1.2251e+00)	Acc@1  69.00 ( 67.45)	Acc@5  90.00 ( 90.41)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2920e+00 (1.2141e+00)	Acc@1  65.00 ( 67.75)	Acc@5  90.00 ( 90.66)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0176e+00 (1.2147e+00)	Acc@1  76.00 ( 67.96)	Acc@5  93.00 ( 90.68)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2988e+00 (1.2213e+00)	Acc@1  65.00 ( 67.74)	Acc@5  89.00 ( 90.56)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.8320e+00 (1.2136e+00)	Acc@1  56.00 ( 67.74)	Acc@5  86.00 ( 90.65)
 * Acc@1 67.990 Acc@5 90.700
### epoch[82] execution time: 28.87807536125183
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.227 ( 0.227)	Data  0.150 ( 0.150)	Loss 4.1577e-01 (4.1577e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [83][ 10/391]	Time  0.068 ( 0.081)	Data  0.001 ( 0.015)	Loss 4.8022e-01 (4.1630e-01)	Acc@1  85.94 ( 87.36)	Acc@5  98.44 ( 99.36)
Epoch: [83][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.008)	Loss 3.3765e-01 (4.2411e-01)	Acc@1  92.19 ( 87.39)	Acc@5  98.44 ( 99.18)
Epoch: [83][ 30/391]	Time  0.069 ( 0.072)	Data  0.001 ( 0.006)	Loss 4.7876e-01 (4.2391e-01)	Acc@1  85.94 ( 87.58)	Acc@5  99.22 ( 99.04)
Epoch: [83][ 40/391]	Time  0.069 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.2310e-01 (4.2938e-01)	Acc@1  89.06 ( 87.54)	Acc@5  99.22 ( 98.88)
Epoch: [83][ 50/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.004)	Loss 4.7632e-01 (4.2859e-01)	Acc@1  86.72 ( 87.45)	Acc@5  99.22 ( 98.88)
Epoch: [83][ 60/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 2.5928e-01 (4.2871e-01)	Acc@1  93.75 ( 87.59)	Acc@5 100.00 ( 98.81)
Epoch: [83][ 70/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.3384e-01 (4.2766e-01)	Acc@1  88.28 ( 87.58)	Acc@5  97.66 ( 98.82)
Epoch: [83][ 80/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.0942e-01 (4.2732e-01)	Acc@1  85.94 ( 87.52)	Acc@5  99.22 ( 98.80)
Epoch: [83][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.6719e-01 (4.2822e-01)	Acc@1  92.19 ( 87.61)	Acc@5  99.22 ( 98.78)
Epoch: [83][100/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.2920e-01 (4.2619e-01)	Acc@1  88.28 ( 87.69)	Acc@5  98.44 ( 98.79)
Epoch: [83][110/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4678e-01 (4.2703e-01)	Acc@1  86.72 ( 87.58)	Acc@5  98.44 ( 98.80)
Epoch: [83][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9331e-01 (4.2917e-01)	Acc@1  88.28 ( 87.47)	Acc@5  99.22 ( 98.79)
Epoch: [83][130/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.8965e-01 (4.3001e-01)	Acc@1  87.50 ( 87.41)	Acc@5  99.22 ( 98.81)
Epoch: [83][140/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.6875e-01 (4.3226e-01)	Acc@1  85.16 ( 87.26)	Acc@5  98.44 ( 98.80)
Epoch: [83][150/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.7803e-01 (4.3449e-01)	Acc@1  83.59 ( 87.13)	Acc@5 100.00 ( 98.78)
Epoch: [83][160/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9233e-01 (4.3479e-01)	Acc@1  89.06 ( 87.09)	Acc@5  98.44 ( 98.78)
Epoch: [83][170/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.6084e-01 (4.3497e-01)	Acc@1  89.06 ( 87.11)	Acc@5  99.22 ( 98.77)
Epoch: [83][180/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0552e-01 (4.3535e-01)	Acc@1  88.28 ( 87.09)	Acc@5  97.66 ( 98.73)
Epoch: [83][190/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.2163e-01 (4.3471e-01)	Acc@1  91.41 ( 87.13)	Acc@5  98.44 ( 98.75)
Epoch: [83][200/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1377e-01 (4.3457e-01)	Acc@1  77.34 ( 87.10)	Acc@5  99.22 ( 98.78)
Epoch: [83][210/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.2646e-01 (4.3634e-01)	Acc@1  82.03 ( 87.09)	Acc@5  96.09 ( 98.74)
Epoch: [83][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6401e-01 (4.3664e-01)	Acc@1  89.84 ( 87.07)	Acc@5  98.44 ( 98.74)
Epoch: [83][230/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5132e-01 (4.3666e-01)	Acc@1  91.41 ( 87.06)	Acc@5 100.00 ( 98.74)
Epoch: [83][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2261e-01 (4.3736e-01)	Acc@1  88.28 ( 87.00)	Acc@5  97.66 ( 98.71)
Epoch: [83][250/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5425e-01 (4.3755e-01)	Acc@1  91.41 ( 87.03)	Acc@5  99.22 ( 98.70)
Epoch: [83][260/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8208e-01 (4.3749e-01)	Acc@1  87.50 ( 87.02)	Acc@5  99.22 ( 98.69)
Epoch: [83][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.3423e-01 (4.3747e-01)	Acc@1  89.84 ( 87.01)	Acc@5 100.00 ( 98.69)
Epoch: [83][280/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7720e-01 (4.3717e-01)	Acc@1  89.84 ( 87.03)	Acc@5  99.22 ( 98.70)
Epoch: [83][290/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3271e-01 (4.3812e-01)	Acc@1  79.69 ( 86.99)	Acc@5  97.66 ( 98.68)
Epoch: [83][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0869e-01 (4.3873e-01)	Acc@1  90.62 ( 86.97)	Acc@5  99.22 ( 98.67)
Epoch: [83][310/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2197e-01 (4.3839e-01)	Acc@1  83.59 ( 87.00)	Acc@5  98.44 ( 98.66)
Epoch: [83][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9316e-01 (4.3795e-01)	Acc@1  87.50 ( 87.03)	Acc@5  97.66 ( 98.67)
Epoch: [83][330/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6167e-01 (4.3810e-01)	Acc@1  89.06 ( 87.03)	Acc@5  97.66 ( 98.67)
Epoch: [83][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4199e-01 (4.3808e-01)	Acc@1  83.59 ( 87.03)	Acc@5  96.88 ( 98.67)
Epoch: [83][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5532e-01 (4.3684e-01)	Acc@1  89.84 ( 87.09)	Acc@5  97.66 ( 98.68)
Epoch: [83][360/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1455e-01 (4.3653e-01)	Acc@1  86.72 ( 87.10)	Acc@5  99.22 ( 98.69)
Epoch: [83][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8364e-01 (4.3604e-01)	Acc@1  82.81 ( 87.12)	Acc@5 100.00 ( 98.69)
Epoch: [83][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.8193e-01 (4.3633e-01)	Acc@1  81.25 ( 87.10)	Acc@5  98.44 ( 98.68)
Epoch: [83][390/391]	Time  0.058 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.0244e-01 (4.3664e-01)	Acc@1  81.25 ( 87.07)	Acc@5 100.00 ( 98.69)
## e[83] optimizer.zero_grad (sum) time: 0.40181493759155273
## e[83]       loss.backward (sum) time: 7.357970237731934
## e[83]      optimizer.step (sum) time: 3.5997297763824463
## epoch[83] training(only) time: 26.215259075164795
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.1572e+00 (1.1572e+00)	Acc@1  68.00 ( 68.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.1201e+00 (1.2217e+00)	Acc@1  68.00 ( 68.82)	Acc@5  93.00 ( 89.27)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6230e+00 (1.2197e+00)	Acc@1  65.00 ( 68.38)	Acc@5  87.00 ( 89.62)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6934e+00 (1.2504e+00)	Acc@1  59.00 ( 67.39)	Acc@5  87.00 ( 89.39)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.2002e+00 (1.2257e+00)	Acc@1  71.00 ( 67.51)	Acc@5  90.00 ( 90.22)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.2539e+00 (1.2308e+00)	Acc@1  69.00 ( 67.45)	Acc@5  90.00 ( 90.27)
Test: [ 60/100]	Time  0.028 ( 0.028)	Loss 1.3037e+00 (1.2186e+00)	Acc@1  64.00 ( 67.80)	Acc@5  90.00 ( 90.51)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 9.9951e-01 (1.2185e+00)	Acc@1  76.00 ( 68.00)	Acc@5  93.00 ( 90.56)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3018e+00 (1.2248e+00)	Acc@1  68.00 ( 67.81)	Acc@5  89.00 ( 90.54)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.8193e+00 (1.2167e+00)	Acc@1  52.00 ( 67.79)	Acc@5  85.00 ( 90.62)
 * Acc@1 68.060 Acc@5 90.680
### epoch[83] execution time: 29.021642208099365
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.221 ( 0.221)	Data  0.145 ( 0.145)	Loss 3.6182e-01 (3.6182e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [84][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.014)	Loss 5.5078e-01 (4.2791e-01)	Acc@1  83.59 ( 88.07)	Acc@5 100.00 ( 98.86)
Epoch: [84][ 20/391]	Time  0.069 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.8291e-01 (4.3357e-01)	Acc@1  84.38 ( 87.46)	Acc@5  98.44 ( 98.74)
Epoch: [84][ 30/391]	Time  0.069 ( 0.071)	Data  0.001 ( 0.006)	Loss 3.7646e-01 (4.2560e-01)	Acc@1  88.28 ( 87.63)	Acc@5  99.22 ( 98.71)
Epoch: [84][ 40/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.1138e-01 (4.2095e-01)	Acc@1  87.50 ( 87.82)	Acc@5  99.22 ( 98.65)
Epoch: [84][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 2.7881e-01 (4.2371e-01)	Acc@1  91.41 ( 87.76)	Acc@5 100.00 ( 98.68)
Epoch: [84][ 60/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.9658e-01 (4.2859e-01)	Acc@1  86.72 ( 87.79)	Acc@5  97.66 ( 98.57)
Epoch: [84][ 70/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.5425e-01 (4.3068e-01)	Acc@1  89.84 ( 87.69)	Acc@5 100.00 ( 98.55)
Epoch: [84][ 80/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.3921e-01 (4.3387e-01)	Acc@1  87.50 ( 87.45)	Acc@5  98.44 ( 98.52)
Epoch: [84][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.5176e-01 (4.3777e-01)	Acc@1  84.38 ( 87.39)	Acc@5  96.09 ( 98.45)
Epoch: [84][100/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.6777e-01 (4.3781e-01)	Acc@1  83.59 ( 87.38)	Acc@5  97.66 ( 98.47)
Epoch: [84][110/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.5254e-01 (4.3342e-01)	Acc@1  92.19 ( 87.53)	Acc@5  99.22 ( 98.52)
Epoch: [84][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 6.0693e-01 (4.3405e-01)	Acc@1  84.38 ( 87.47)	Acc@5  96.88 ( 98.55)
Epoch: [84][130/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3018e-01 (4.3295e-01)	Acc@1  85.94 ( 87.48)	Acc@5 100.00 ( 98.59)
Epoch: [84][140/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (4.3598e-01)	Acc@1  82.03 ( 87.43)	Acc@5 100.00 ( 98.56)
Epoch: [84][150/391]	Time  0.068 ( 0.067)	Data  0.002 ( 0.002)	Loss 3.3228e-01 (4.3752e-01)	Acc@1  91.41 ( 87.34)	Acc@5 100.00 ( 98.56)
Epoch: [84][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9375e-01 (4.3743e-01)	Acc@1  80.47 ( 87.36)	Acc@5  97.66 ( 98.57)
Epoch: [84][170/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0869e-01 (4.3894e-01)	Acc@1  88.28 ( 87.28)	Acc@5  98.44 ( 98.59)
Epoch: [84][180/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1187e-01 (4.3860e-01)	Acc@1  85.94 ( 87.25)	Acc@5  98.44 ( 98.63)
Epoch: [84][190/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1772e-01 (4.3964e-01)	Acc@1  85.94 ( 87.21)	Acc@5 100.00 ( 98.64)
Epoch: [84][200/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1982e-01 (4.3770e-01)	Acc@1  91.41 ( 87.30)	Acc@5  99.22 ( 98.66)
Epoch: [84][210/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1885e-01 (4.3577e-01)	Acc@1  91.41 ( 87.33)	Acc@5  99.22 ( 98.66)
Epoch: [84][220/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7876e-01 (4.3612e-01)	Acc@1  85.94 ( 87.31)	Acc@5  99.22 ( 98.66)
Epoch: [84][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6133e-01 (4.3524e-01)	Acc@1  92.19 ( 87.34)	Acc@5 100.00 ( 98.67)
Epoch: [84][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6084e-01 (4.3547e-01)	Acc@1  88.28 ( 87.35)	Acc@5 100.00 ( 98.66)
Epoch: [84][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7964e-01 (4.3567e-01)	Acc@1  86.72 ( 87.31)	Acc@5  99.22 ( 98.66)
Epoch: [84][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8901e-01 (4.3463e-01)	Acc@1  85.16 ( 87.38)	Acc@5  98.44 ( 98.67)
Epoch: [84][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9795e-01 (4.3555e-01)	Acc@1  87.50 ( 87.34)	Acc@5  98.44 ( 98.66)
Epoch: [84][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4839e-01 (4.3575e-01)	Acc@1  89.84 ( 87.30)	Acc@5  99.22 ( 98.66)
Epoch: [84][290/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6060e-01 (4.3681e-01)	Acc@1  90.62 ( 87.25)	Acc@5  99.22 ( 98.64)
Epoch: [84][300/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8232e-01 (4.3719e-01)	Acc@1  90.62 ( 87.22)	Acc@5  97.66 ( 98.63)
Epoch: [84][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7314e-01 (4.3566e-01)	Acc@1  86.72 ( 87.27)	Acc@5  96.88 ( 98.64)
Epoch: [84][320/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8320e-01 (4.3421e-01)	Acc@1  92.97 ( 87.33)	Acc@5 100.00 ( 98.64)
Epoch: [84][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9756e-01 (4.3445e-01)	Acc@1  85.16 ( 87.30)	Acc@5  98.44 ( 98.65)
Epoch: [84][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8267e-01 (4.3523e-01)	Acc@1  85.94 ( 87.26)	Acc@5  97.66 ( 98.65)
Epoch: [84][350/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8242e-01 (4.3536e-01)	Acc@1  85.94 ( 87.28)	Acc@5  98.44 ( 98.65)
Epoch: [84][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6792e-01 (4.3485e-01)	Acc@1  88.28 ( 87.27)	Acc@5 100.00 ( 98.67)
Epoch: [84][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.8306e-01 (4.3429e-01)	Acc@1  89.06 ( 87.30)	Acc@5  99.22 ( 98.68)
Epoch: [84][380/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.8135e-01 (4.3397e-01)	Acc@1  87.50 ( 87.29)	Acc@5  99.22 ( 98.68)
Epoch: [84][390/391]	Time  0.052 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.6963e-01 (4.3404e-01)	Acc@1  86.25 ( 87.27)	Acc@5 100.00 ( 98.69)
## e[84] optimizer.zero_grad (sum) time: 0.4067046642303467
## e[84]       loss.backward (sum) time: 7.371392250061035
## e[84]      optimizer.step (sum) time: 3.560654878616333
## epoch[84] training(only) time: 26.174344778060913
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.1426e+00 (1.1426e+00)	Acc@1  68.00 ( 68.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.1572e+00 (1.2189e+00)	Acc@1  70.00 ( 69.27)	Acc@5  92.00 ( 89.64)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6172e+00 (1.2186e+00)	Acc@1  64.00 ( 68.52)	Acc@5  87.00 ( 90.00)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 1.6914e+00 (1.2502e+00)	Acc@1  58.00 ( 67.52)	Acc@5  87.00 ( 89.55)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.1992e+00 (1.2270e+00)	Acc@1  71.00 ( 67.51)	Acc@5  90.00 ( 90.17)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.2539e+00 (1.2321e+00)	Acc@1  65.00 ( 67.22)	Acc@5  90.00 ( 90.33)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2949e+00 (1.2192e+00)	Acc@1  65.00 ( 67.51)	Acc@5  90.00 ( 90.54)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.0088e+00 (1.2202e+00)	Acc@1  75.00 ( 67.72)	Acc@5  92.00 ( 90.63)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.3135e+00 (1.2271e+00)	Acc@1  67.00 ( 67.52)	Acc@5  89.00 ( 90.54)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8516e+00 (1.2192e+00)	Acc@1  53.00 ( 67.52)	Acc@5  85.00 ( 90.60)
 * Acc@1 67.800 Acc@5 90.630
### epoch[84] execution time: 29.01603078842163
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.222 ( 0.222)	Data  0.148 ( 0.148)	Loss 5.1758e-01 (5.1758e-01)	Acc@1  88.28 ( 88.28)	Acc@5  96.09 ( 96.09)
Epoch: [85][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.014)	Loss 4.5435e-01 (4.5428e-01)	Acc@1  87.50 ( 85.80)	Acc@5  96.88 ( 98.08)
Epoch: [85][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.4824e-01 (4.5035e-01)	Acc@1  87.50 ( 86.31)	Acc@5  99.22 ( 98.55)
Epoch: [85][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 3.8110e-01 (4.4481e-01)	Acc@1  86.72 ( 86.54)	Acc@5 100.00 ( 98.64)
Epoch: [85][ 40/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.9463e-01 (4.4816e-01)	Acc@1  83.59 ( 86.70)	Acc@5  99.22 ( 98.59)
Epoch: [85][ 50/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.4312e-01 (4.4452e-01)	Acc@1  87.50 ( 86.95)	Acc@5  97.66 ( 98.53)
Epoch: [85][ 60/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.5068e-01 (4.4359e-01)	Acc@1  85.16 ( 86.99)	Acc@5  99.22 ( 98.60)
Epoch: [85][ 70/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.6509e-01 (4.4399e-01)	Acc@1  86.72 ( 87.03)	Acc@5  96.88 ( 98.53)
Epoch: [85][ 80/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.1660e-01 (4.4884e-01)	Acc@1  82.03 ( 86.67)	Acc@5  98.44 ( 98.54)
Epoch: [85][ 90/391]	Time  0.073 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.3652e-01 (4.4891e-01)	Acc@1  85.94 ( 86.68)	Acc@5  99.22 ( 98.54)
Epoch: [85][100/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.2383e-01 (4.4780e-01)	Acc@1  85.16 ( 86.70)	Acc@5  99.22 ( 98.53)
Epoch: [85][110/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9512e-01 (4.4467e-01)	Acc@1  89.06 ( 86.95)	Acc@5  97.66 ( 98.54)
Epoch: [85][120/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2773e-01 (4.4519e-01)	Acc@1  85.94 ( 86.93)	Acc@5  98.44 ( 98.55)
Epoch: [85][130/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1919e-01 (4.4542e-01)	Acc@1  89.84 ( 86.97)	Acc@5 100.00 ( 98.54)
Epoch: [85][140/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4937e-01 (4.4364e-01)	Acc@1  89.84 ( 87.02)	Acc@5  99.22 ( 98.54)
Epoch: [85][150/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6499e-01 (4.3977e-01)	Acc@1  89.06 ( 87.19)	Acc@5 100.00 ( 98.59)
Epoch: [85][160/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6206e-01 (4.3863e-01)	Acc@1  90.62 ( 87.25)	Acc@5 100.00 ( 98.60)
Epoch: [85][170/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5522e-01 (4.3597e-01)	Acc@1  88.28 ( 87.34)	Acc@5  99.22 ( 98.62)
Epoch: [85][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5889e-01 (4.3528e-01)	Acc@1  90.62 ( 87.37)	Acc@5  99.22 ( 98.61)
Epoch: [85][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2847e-01 (4.3347e-01)	Acc@1  87.50 ( 87.43)	Acc@5  98.44 ( 98.62)
Epoch: [85][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9844e-01 (4.3482e-01)	Acc@1  88.28 ( 87.37)	Acc@5  98.44 ( 98.59)
Epoch: [85][210/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6768e-01 (4.3414e-01)	Acc@1  90.62 ( 87.44)	Acc@5  99.22 ( 98.59)
Epoch: [85][220/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7583e-01 (4.3365e-01)	Acc@1  85.16 ( 87.44)	Acc@5  99.22 ( 98.61)
Epoch: [85][230/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2578e-01 (4.3360e-01)	Acc@1  82.81 ( 87.44)	Acc@5  97.66 ( 98.62)
Epoch: [85][240/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9219e-01 (4.3387e-01)	Acc@1  83.59 ( 87.44)	Acc@5  98.44 ( 98.61)
Epoch: [85][250/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6069e-01 (4.3406e-01)	Acc@1  86.72 ( 87.39)	Acc@5 100.00 ( 98.61)
Epoch: [85][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4092e-01 (4.3360e-01)	Acc@1  85.16 ( 87.40)	Acc@5  98.44 ( 98.61)
Epoch: [85][270/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5508e-01 (4.3402e-01)	Acc@1  86.72 ( 87.39)	Acc@5  99.22 ( 98.62)
Epoch: [85][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3320e-01 (4.3345e-01)	Acc@1  84.38 ( 87.40)	Acc@5  99.22 ( 98.63)
Epoch: [85][290/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9014e-01 (4.3463e-01)	Acc@1  88.28 ( 87.33)	Acc@5  98.44 ( 98.61)
Epoch: [85][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0503e-01 (4.3558e-01)	Acc@1  87.50 ( 87.29)	Acc@5 100.00 ( 98.61)
Epoch: [85][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9062e-01 (4.3628e-01)	Acc@1  89.06 ( 87.28)	Acc@5  99.22 ( 98.61)
Epoch: [85][320/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6157e-01 (4.3492e-01)	Acc@1  89.06 ( 87.34)	Acc@5 100.00 ( 98.62)
Epoch: [85][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7720e-01 (4.3502e-01)	Acc@1  89.06 ( 87.34)	Acc@5  99.22 ( 98.62)
Epoch: [85][340/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8013e-01 (4.3481e-01)	Acc@1  89.06 ( 87.34)	Acc@5  99.22 ( 98.63)
Epoch: [85][350/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.9976e-01 (4.3431e-01)	Acc@1  85.94 ( 87.36)	Acc@5  97.66 ( 98.63)
Epoch: [85][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.8721e-01 (4.3326e-01)	Acc@1  89.06 ( 87.38)	Acc@5  98.44 ( 98.64)
Epoch: [85][370/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.5283e-01 (4.3346e-01)	Acc@1  81.25 ( 87.35)	Acc@5  96.09 ( 98.64)
Epoch: [85][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.4263e-01 (4.3381e-01)	Acc@1  88.28 ( 87.34)	Acc@5  99.22 ( 98.64)
Epoch: [85][390/391]	Time  0.050 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.1768e-01 (4.3433e-01)	Acc@1  85.00 ( 87.33)	Acc@5  96.25 ( 98.64)
## e[85] optimizer.zero_grad (sum) time: 0.40741491317749023
## e[85]       loss.backward (sum) time: 7.350579500198364
## e[85]      optimizer.step (sum) time: 3.559649705886841
## epoch[85] training(only) time: 26.119701623916626
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.1396e+00 (1.1396e+00)	Acc@1  67.00 ( 67.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.1387e+00 (1.2160e+00)	Acc@1  72.00 ( 69.55)	Acc@5  93.00 ( 89.91)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6426e+00 (1.2199e+00)	Acc@1  63.00 ( 68.71)	Acc@5  88.00 ( 90.29)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6660e+00 (1.2490e+00)	Acc@1  59.00 ( 67.61)	Acc@5  87.00 ( 89.87)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.2002e+00 (1.2247e+00)	Acc@1  70.00 ( 67.61)	Acc@5  90.00 ( 90.44)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.2422e+00 (1.2294e+00)	Acc@1  70.00 ( 67.41)	Acc@5  91.00 ( 90.49)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.3018e+00 (1.2184e+00)	Acc@1  65.00 ( 67.64)	Acc@5  92.00 ( 90.74)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0273e+00 (1.2197e+00)	Acc@1  74.00 ( 67.82)	Acc@5  93.00 ( 90.77)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3350e+00 (1.2271e+00)	Acc@1  64.00 ( 67.62)	Acc@5  89.00 ( 90.65)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.8379e+00 (1.2187e+00)	Acc@1  52.00 ( 67.53)	Acc@5  85.00 ( 90.76)
 * Acc@1 67.790 Acc@5 90.800
### epoch[85] execution time: 28.94927215576172
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.227 ( 0.227)	Data  0.152 ( 0.152)	Loss 3.7378e-01 (3.7378e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.064 ( 0.081)	Data  0.001 ( 0.015)	Loss 3.9844e-01 (4.4997e-01)	Acc@1  90.62 ( 86.93)	Acc@5  98.44 ( 98.79)
Epoch: [86][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.008)	Loss 3.7769e-01 (4.2417e-01)	Acc@1  86.72 ( 87.50)	Acc@5  99.22 ( 99.07)
Epoch: [86][ 30/391]	Time  0.071 ( 0.072)	Data  0.001 ( 0.006)	Loss 4.4873e-01 (4.2370e-01)	Acc@1  87.50 ( 87.53)	Acc@5  99.22 ( 99.09)
Epoch: [86][ 40/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.005)	Loss 4.4287e-01 (4.3212e-01)	Acc@1  86.72 ( 87.27)	Acc@5  98.44 ( 98.86)
Epoch: [86][ 50/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 3.9478e-01 (4.3897e-01)	Acc@1  90.62 ( 87.10)	Acc@5 100.00 ( 98.84)
Epoch: [86][ 60/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.9048e-01 (4.3857e-01)	Acc@1  85.16 ( 86.99)	Acc@5  96.88 ( 98.89)
Epoch: [86][ 70/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.1968e-01 (4.4001e-01)	Acc@1  88.28 ( 87.06)	Acc@5  99.22 ( 98.81)
Epoch: [86][ 80/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.6157e-01 (4.4020e-01)	Acc@1  90.62 ( 87.05)	Acc@5  97.66 ( 98.80)
Epoch: [86][ 90/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.7275e-01 (4.4116e-01)	Acc@1  85.16 ( 86.96)	Acc@5  97.66 ( 98.81)
Epoch: [86][100/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.0918e-01 (4.4260e-01)	Acc@1  89.06 ( 86.97)	Acc@5 100.00 ( 98.75)
Epoch: [86][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3408e-01 (4.4437e-01)	Acc@1  86.72 ( 86.92)	Acc@5  96.88 ( 98.72)
Epoch: [86][120/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0649e-01 (4.4176e-01)	Acc@1  84.38 ( 86.93)	Acc@5 100.00 ( 98.74)
Epoch: [86][130/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.1172e-01 (4.4225e-01)	Acc@1  82.03 ( 86.93)	Acc@5  97.66 ( 98.75)
Epoch: [86][140/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 2.9834e-01 (4.4041e-01)	Acc@1  94.53 ( 87.00)	Acc@5 100.00 ( 98.76)
Epoch: [86][150/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4800e-01 (4.4115e-01)	Acc@1  87.50 ( 87.02)	Acc@5  97.66 ( 98.74)
Epoch: [86][160/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.8438e-01 (4.3876e-01)	Acc@1  82.81 ( 87.03)	Acc@5  96.88 ( 98.75)
Epoch: [86][170/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4238e-01 (4.3887e-01)	Acc@1  88.28 ( 87.01)	Acc@5  98.44 ( 98.77)
Epoch: [86][180/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.2832e-01 (4.3786e-01)	Acc@1  83.59 ( 87.09)	Acc@5  97.66 ( 98.77)
Epoch: [86][190/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4180e-01 (4.3780e-01)	Acc@1  89.84 ( 87.05)	Acc@5  99.22 ( 98.77)
Epoch: [86][200/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4165e-01 (4.3910e-01)	Acc@1  86.72 ( 87.03)	Acc@5  98.44 ( 98.74)
Epoch: [86][210/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2627e-01 (4.3725e-01)	Acc@1  87.50 ( 87.11)	Acc@5  98.44 ( 98.76)
Epoch: [86][220/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8535e-01 (4.3834e-01)	Acc@1  88.28 ( 87.08)	Acc@5  97.66 ( 98.73)
Epoch: [86][230/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2285e-01 (4.3827e-01)	Acc@1  86.72 ( 87.10)	Acc@5  98.44 ( 98.72)
Epoch: [86][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0161e-01 (4.3932e-01)	Acc@1  87.50 ( 87.05)	Acc@5 100.00 ( 98.74)
Epoch: [86][250/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7207e-01 (4.3884e-01)	Acc@1  91.41 ( 87.10)	Acc@5  97.66 ( 98.71)
Epoch: [86][260/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0991e-01 (4.3700e-01)	Acc@1  89.06 ( 87.17)	Acc@5  99.22 ( 98.72)
Epoch: [86][270/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8794e-01 (4.3709e-01)	Acc@1  86.72 ( 87.13)	Acc@5  98.44 ( 98.73)
Epoch: [86][280/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6206e-01 (4.3552e-01)	Acc@1  89.06 ( 87.17)	Acc@5  98.44 ( 98.73)
Epoch: [86][290/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5596e-01 (4.3603e-01)	Acc@1  90.62 ( 87.18)	Acc@5 100.00 ( 98.72)
Epoch: [86][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3408e-01 (4.3578e-01)	Acc@1  88.28 ( 87.18)	Acc@5  98.44 ( 98.71)
Epoch: [86][310/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6206e-01 (4.3538e-01)	Acc@1  92.97 ( 87.18)	Acc@5  96.88 ( 98.71)
Epoch: [86][320/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1382e-01 (4.3404e-01)	Acc@1  88.28 ( 87.23)	Acc@5  98.44 ( 98.72)
Epoch: [86][330/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7402e-01 (4.3454e-01)	Acc@1  89.06 ( 87.21)	Acc@5  98.44 ( 98.73)
Epoch: [86][340/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6108e-01 (4.3436e-01)	Acc@1  89.06 ( 87.20)	Acc@5 100.00 ( 98.73)
Epoch: [86][350/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8916e-01 (4.3397e-01)	Acc@1  89.84 ( 87.22)	Acc@5  99.22 ( 98.71)
Epoch: [86][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3604e-01 (4.3294e-01)	Acc@1  90.62 ( 87.28)	Acc@5  97.66 ( 98.72)
Epoch: [86][370/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4131e-01 (4.3238e-01)	Acc@1  90.62 ( 87.29)	Acc@5 100.00 ( 98.72)
Epoch: [86][380/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.3906e-01 (4.3312e-01)	Acc@1  82.81 ( 87.27)	Acc@5  98.44 ( 98.72)
Epoch: [86][390/391]	Time  0.051 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.9893e-01 (4.3303e-01)	Acc@1  87.50 ( 87.27)	Acc@5  98.75 ( 98.71)
## e[86] optimizer.zero_grad (sum) time: 0.409869909286499
## e[86]       loss.backward (sum) time: 7.372340679168701
## e[86]      optimizer.step (sum) time: 3.596060276031494
## epoch[86] training(only) time: 26.248841762542725
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.1572e+00 (1.1572e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.028 ( 0.038)	Loss 1.1406e+00 (1.2204e+00)	Acc@1  70.00 ( 69.64)	Acc@5  94.00 ( 89.91)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6309e+00 (1.2194e+00)	Acc@1  64.00 ( 68.90)	Acc@5  87.00 ( 90.24)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7080e+00 (1.2488e+00)	Acc@1  56.00 ( 67.71)	Acc@5  86.00 ( 89.71)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1973e+00 (1.2263e+00)	Acc@1  70.00 ( 67.76)	Acc@5  90.00 ( 90.34)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2441e+00 (1.2304e+00)	Acc@1  69.00 ( 67.57)	Acc@5  91.00 ( 90.43)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.3057e+00 (1.2192e+00)	Acc@1  64.00 ( 67.82)	Acc@5  91.00 ( 90.61)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0342e+00 (1.2211e+00)	Acc@1  76.00 ( 68.07)	Acc@5  92.00 ( 90.66)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3027e+00 (1.2270e+00)	Acc@1  65.00 ( 67.80)	Acc@5  89.00 ( 90.63)
Test: [ 90/100]	Time  0.025 ( 0.027)	Loss 1.8203e+00 (1.2182e+00)	Acc@1  54.00 ( 67.70)	Acc@5  87.00 ( 90.78)
 * Acc@1 67.970 Acc@5 90.810
### epoch[86] execution time: 29.04743480682373
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.226 ( 0.226)	Data  0.149 ( 0.149)	Loss 4.6460e-01 (4.6460e-01)	Acc@1  86.72 ( 86.72)	Acc@5  98.44 ( 98.44)
Epoch: [87][ 10/391]	Time  0.069 ( 0.082)	Data  0.001 ( 0.014)	Loss 3.8745e-01 (4.3071e-01)	Acc@1  88.28 ( 87.36)	Acc@5  99.22 ( 98.58)
Epoch: [87][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.008)	Loss 3.5913e-01 (4.3555e-01)	Acc@1  90.62 ( 87.24)	Acc@5  98.44 ( 98.51)
Epoch: [87][ 30/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.006)	Loss 5.9424e-01 (4.4273e-01)	Acc@1  82.03 ( 87.15)	Acc@5  96.88 ( 98.56)
Epoch: [87][ 40/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.005)	Loss 5.7520e-01 (4.4104e-01)	Acc@1  82.81 ( 87.04)	Acc@5  97.66 ( 98.61)
Epoch: [87][ 50/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.004)	Loss 5.5078e-01 (4.4128e-01)	Acc@1  81.25 ( 86.92)	Acc@5  98.44 ( 98.67)
Epoch: [87][ 60/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.7427e-01 (4.3973e-01)	Acc@1  89.06 ( 86.92)	Acc@5 100.00 ( 98.57)
Epoch: [87][ 70/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.003)	Loss 3.3887e-01 (4.3997e-01)	Acc@1  92.19 ( 86.96)	Acc@5  98.44 ( 98.53)
Epoch: [87][ 80/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.0781e-01 (4.4180e-01)	Acc@1  85.16 ( 86.90)	Acc@5  96.09 ( 98.51)
Epoch: [87][ 90/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.7681e-01 (4.4371e-01)	Acc@1  89.84 ( 86.80)	Acc@5  99.22 ( 98.48)
Epoch: [87][100/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.2246e-01 (4.4405e-01)	Acc@1  85.94 ( 86.88)	Acc@5  98.44 ( 98.48)
Epoch: [87][110/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.6377e-01 (4.4127e-01)	Acc@1  89.84 ( 86.92)	Acc@5  98.44 ( 98.51)
Epoch: [87][120/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9121e-01 (4.4270e-01)	Acc@1  85.16 ( 86.93)	Acc@5  98.44 ( 98.55)
Epoch: [87][130/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.9673e-01 (4.4079e-01)	Acc@1  90.62 ( 86.92)	Acc@5  99.22 ( 98.54)
Epoch: [87][140/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.2798e-01 (4.4059e-01)	Acc@1  90.62 ( 86.91)	Acc@5  98.44 ( 98.56)
Epoch: [87][150/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.4888e-01 (4.3939e-01)	Acc@1  89.84 ( 86.93)	Acc@5  99.22 ( 98.58)
Epoch: [87][160/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4849e-01 (4.3749e-01)	Acc@1  85.16 ( 86.99)	Acc@5  98.44 ( 98.61)
Epoch: [87][170/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1333e-01 (4.3678e-01)	Acc@1  89.06 ( 87.02)	Acc@5  97.66 ( 98.61)
Epoch: [87][180/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.9297e-01 (4.3784e-01)	Acc@1  89.84 ( 86.99)	Acc@5  99.22 ( 98.62)
Epoch: [87][190/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0488e-01 (4.3800e-01)	Acc@1  85.16 ( 86.95)	Acc@5  97.66 ( 98.63)
Epoch: [87][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2847e-01 (4.3758e-01)	Acc@1  85.94 ( 86.94)	Acc@5  99.22 ( 98.64)
Epoch: [87][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1807e-01 (4.3854e-01)	Acc@1  82.03 ( 86.87)	Acc@5  98.44 ( 98.64)
Epoch: [87][220/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7939e-01 (4.3873e-01)	Acc@1  86.72 ( 86.90)	Acc@5  99.22 ( 98.63)
Epoch: [87][230/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9941e-01 (4.3761e-01)	Acc@1  88.28 ( 86.94)	Acc@5 100.00 ( 98.64)
Epoch: [87][240/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.2715e-01 (4.3644e-01)	Acc@1  91.41 ( 87.01)	Acc@5 100.00 ( 98.65)
Epoch: [87][250/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8145e-01 (4.3598e-01)	Acc@1  85.16 ( 87.02)	Acc@5  98.44 ( 98.65)
Epoch: [87][260/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3564e-01 (4.3683e-01)	Acc@1  84.38 ( 87.00)	Acc@5  96.88 ( 98.64)
Epoch: [87][270/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5361e-01 (4.3521e-01)	Acc@1  90.62 ( 87.06)	Acc@5  97.66 ( 98.67)
Epoch: [87][280/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8745e-01 (4.3647e-01)	Acc@1  90.62 ( 87.01)	Acc@5  99.22 ( 98.65)
Epoch: [87][290/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1821e-01 (4.3611e-01)	Acc@1  85.94 ( 87.04)	Acc@5  99.22 ( 98.66)
Epoch: [87][300/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9160e-01 (4.3533e-01)	Acc@1  88.28 ( 87.05)	Acc@5  97.66 ( 98.67)
Epoch: [87][310/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5752e-01 (4.3522e-01)	Acc@1  88.28 ( 87.09)	Acc@5  98.44 ( 98.66)
Epoch: [87][320/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8389e-01 (4.3689e-01)	Acc@1  85.94 ( 87.06)	Acc@5  98.44 ( 98.63)
Epoch: [87][330/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6426e-01 (4.3520e-01)	Acc@1  87.50 ( 87.09)	Acc@5  99.22 ( 98.65)
Epoch: [87][340/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5605e-01 (4.3377e-01)	Acc@1  86.72 ( 87.16)	Acc@5  98.44 ( 98.68)
Epoch: [87][350/391]	Time  0.077 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9526e-01 (4.3379e-01)	Acc@1  89.84 ( 87.17)	Acc@5  99.22 ( 98.68)
Epoch: [87][360/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.0928e-01 (4.3384e-01)	Acc@1  79.69 ( 87.15)	Acc@5  98.44 ( 98.68)
Epoch: [87][370/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.1772e-01 (4.3381e-01)	Acc@1  89.06 ( 87.17)	Acc@5  98.44 ( 98.69)
Epoch: [87][380/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.2422e-01 (4.3342e-01)	Acc@1  92.19 ( 87.18)	Acc@5 100.00 ( 98.70)
Epoch: [87][390/391]	Time  0.056 ( 0.067)	Data  0.001 ( 0.001)	Loss 5.2148e-01 (4.3312e-01)	Acc@1  86.25 ( 87.17)	Acc@5  98.75 ( 98.70)
## e[87] optimizer.zero_grad (sum) time: 0.4086172580718994
## e[87]       loss.backward (sum) time: 7.387219667434692
## e[87]      optimizer.step (sum) time: 3.5992014408111572
## epoch[87] training(only) time: 26.256855010986328
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.1611e+00 (1.1611e+00)	Acc@1  68.00 ( 68.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.1592e+00 (1.2239e+00)	Acc@1  70.00 ( 68.64)	Acc@5  92.00 ( 89.82)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6260e+00 (1.2214e+00)	Acc@1  64.00 ( 68.10)	Acc@5  86.00 ( 90.10)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.7129e+00 (1.2524e+00)	Acc@1  57.00 ( 67.23)	Acc@5  86.00 ( 89.65)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1885e+00 (1.2296e+00)	Acc@1  69.00 ( 67.41)	Acc@5  90.00 ( 90.32)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2500e+00 (1.2332e+00)	Acc@1  64.00 ( 67.16)	Acc@5  90.00 ( 90.41)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 1.2773e+00 (1.2222e+00)	Acc@1  64.00 ( 67.41)	Acc@5  90.00 ( 90.57)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0283e+00 (1.2223e+00)	Acc@1  73.00 ( 67.69)	Acc@5  92.00 ( 90.66)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.3066e+00 (1.2279e+00)	Acc@1  66.00 ( 67.52)	Acc@5  90.00 ( 90.56)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8213e+00 (1.2188e+00)	Acc@1  54.00 ( 67.63)	Acc@5  84.00 ( 90.62)
 * Acc@1 67.840 Acc@5 90.670
### epoch[87] execution time: 29.06905436515808
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.220 ( 0.220)	Data  0.144 ( 0.144)	Loss 4.8608e-01 (4.8608e-01)	Acc@1  84.38 ( 84.38)	Acc@5  98.44 ( 98.44)
Epoch: [88][ 10/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.014)	Loss 4.3872e-01 (4.3402e-01)	Acc@1  85.94 ( 86.93)	Acc@5 100.00 ( 98.58)
Epoch: [88][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 5.5322e-01 (4.5186e-01)	Acc@1  85.16 ( 86.79)	Acc@5  96.88 ( 98.25)
Epoch: [88][ 30/391]	Time  0.068 ( 0.071)	Data  0.001 ( 0.006)	Loss 4.7876e-01 (4.4512e-01)	Acc@1  87.50 ( 87.05)	Acc@5  99.22 ( 98.49)
Epoch: [88][ 40/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.8315e-01 (4.5326e-01)	Acc@1  84.38 ( 86.60)	Acc@5  98.44 ( 98.44)
Epoch: [88][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 3.9429e-01 (4.4808e-01)	Acc@1  89.06 ( 86.70)	Acc@5  97.66 ( 98.44)
Epoch: [88][ 60/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.7632e-01 (4.4436e-01)	Acc@1  85.94 ( 86.95)	Acc@5  99.22 ( 98.44)
Epoch: [88][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.3179e-01 (4.4152e-01)	Acc@1  86.72 ( 87.07)	Acc@5 100.00 ( 98.49)
Epoch: [88][ 80/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.8574e-01 (4.4421e-01)	Acc@1  88.28 ( 86.96)	Acc@5  98.44 ( 98.44)
Epoch: [88][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.5474e-01 (4.3692e-01)	Acc@1  92.19 ( 87.15)	Acc@5  99.22 ( 98.51)
Epoch: [88][100/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.7314e-01 (4.3812e-01)	Acc@1  82.81 ( 87.14)	Acc@5  97.66 ( 98.49)
Epoch: [88][110/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.1333e-01 (4.3516e-01)	Acc@1  89.84 ( 87.26)	Acc@5  97.66 ( 98.53)
Epoch: [88][120/391]	Time  0.074 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.3433e-01 (4.3612e-01)	Acc@1  85.94 ( 87.12)	Acc@5  99.22 ( 98.53)
Epoch: [88][130/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.9146e-01 (4.3575e-01)	Acc@1  85.16 ( 87.14)	Acc@5  97.66 ( 98.55)
Epoch: [88][140/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.002)	Loss 3.7085e-01 (4.3561e-01)	Acc@1  88.28 ( 87.13)	Acc@5  98.44 ( 98.57)
Epoch: [88][150/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.002)	Loss 5.3662e-01 (4.3645e-01)	Acc@1  84.38 ( 87.11)	Acc@5  96.09 ( 98.56)
Epoch: [88][160/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0991e-01 (4.3809e-01)	Acc@1  88.28 ( 87.13)	Acc@5  98.44 ( 98.54)
Epoch: [88][170/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.4238e-01 (4.3593e-01)	Acc@1  88.28 ( 87.22)	Acc@5  99.22 ( 98.57)
Epoch: [88][180/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.002)	Loss 4.0771e-01 (4.3413e-01)	Acc@1  86.72 ( 87.28)	Acc@5 100.00 ( 98.59)
Epoch: [88][190/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4214e-01 (4.3141e-01)	Acc@1  88.28 ( 87.37)	Acc@5  96.88 ( 98.61)
Epoch: [88][200/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4849e-01 (4.3415e-01)	Acc@1  87.50 ( 87.29)	Acc@5 100.00 ( 98.62)
Epoch: [88][210/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8208e-01 (4.3314e-01)	Acc@1  92.19 ( 87.35)	Acc@5 100.00 ( 98.62)
Epoch: [88][220/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3481e-01 (4.3357e-01)	Acc@1  87.50 ( 87.35)	Acc@5  96.09 ( 98.61)
Epoch: [88][230/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8340e-01 (4.3440e-01)	Acc@1  83.59 ( 87.31)	Acc@5  99.22 ( 98.61)
Epoch: [88][240/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9624e-01 (4.3254e-01)	Acc@1  90.62 ( 87.42)	Acc@5  99.22 ( 98.63)
Epoch: [88][250/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0137e-01 (4.3155e-01)	Acc@1  89.84 ( 87.45)	Acc@5  99.22 ( 98.65)
Epoch: [88][260/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4385e-01 (4.3159e-01)	Acc@1  86.72 ( 87.45)	Acc@5  96.88 ( 98.63)
Epoch: [88][270/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9185e-01 (4.3099e-01)	Acc@1  89.84 ( 87.46)	Acc@5  97.66 ( 98.65)
Epoch: [88][280/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3115e-01 (4.3053e-01)	Acc@1  86.72 ( 87.50)	Acc@5  98.44 ( 98.65)
Epoch: [88][290/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.6104e-01 (4.3148e-01)	Acc@1  84.38 ( 87.49)	Acc@5  97.66 ( 98.65)
Epoch: [88][300/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7974e-01 (4.3149e-01)	Acc@1  82.81 ( 87.48)	Acc@5  99.22 ( 98.64)
Epoch: [88][310/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0674e-01 (4.3196e-01)	Acc@1  86.72 ( 87.48)	Acc@5  98.44 ( 98.64)
Epoch: [88][320/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.3838e-01 (4.3154e-01)	Acc@1  89.84 ( 87.48)	Acc@5  99.22 ( 98.66)
Epoch: [88][330/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4287e-01 (4.3160e-01)	Acc@1  87.50 ( 87.49)	Acc@5  98.44 ( 98.66)
Epoch: [88][340/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9233e-01 (4.3067e-01)	Acc@1  84.38 ( 87.52)	Acc@5 100.00 ( 98.66)
Epoch: [88][350/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7988e-01 (4.3027e-01)	Acc@1  89.84 ( 87.50)	Acc@5  99.22 ( 98.67)
Epoch: [88][360/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.3472e-01 (4.2962e-01)	Acc@1  90.62 ( 87.53)	Acc@5 100.00 ( 98.67)
Epoch: [88][370/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.1553e-01 (4.2944e-01)	Acc@1  86.72 ( 87.56)	Acc@5  99.22 ( 98.67)
Epoch: [88][380/391]	Time  0.076 ( 0.067)	Data  0.001 ( 0.001)	Loss 3.6011e-01 (4.2910e-01)	Acc@1  90.62 ( 87.53)	Acc@5  98.44 ( 98.67)
Epoch: [88][390/391]	Time  0.052 ( 0.067)	Data  0.001 ( 0.001)	Loss 6.1377e-01 (4.2950e-01)	Acc@1  77.50 ( 87.49)	Acc@5  98.75 ( 98.68)
## e[88] optimizer.zero_grad (sum) time: 0.4127168655395508
## e[88]       loss.backward (sum) time: 7.41145133972168
## e[88]      optimizer.step (sum) time: 3.5625264644622803
## epoch[88] training(only) time: 26.233964681625366
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.1641e+00 (1.1641e+00)	Acc@1  68.00 ( 68.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.029 ( 0.039)	Loss 1.1426e+00 (1.2263e+00)	Acc@1  71.00 ( 69.18)	Acc@5  93.00 ( 90.00)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6494e+00 (1.2257e+00)	Acc@1  64.00 ( 68.38)	Acc@5  88.00 ( 90.19)
Test: [ 30/100]	Time  0.032 ( 0.031)	Loss 1.7012e+00 (1.2574e+00)	Acc@1  57.00 ( 67.39)	Acc@5  87.00 ( 89.65)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1914e+00 (1.2341e+00)	Acc@1  70.00 ( 67.41)	Acc@5  92.00 ( 90.34)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2549e+00 (1.2370e+00)	Acc@1  67.00 ( 67.25)	Acc@5  90.00 ( 90.47)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2754e+00 (1.2253e+00)	Acc@1  65.00 ( 67.57)	Acc@5  91.00 ( 90.67)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0410e+00 (1.2254e+00)	Acc@1  73.00 ( 67.77)	Acc@5  93.00 ( 90.69)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3057e+00 (1.2319e+00)	Acc@1  65.00 ( 67.59)	Acc@5  89.00 ( 90.60)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8623e+00 (1.2232e+00)	Acc@1  53.00 ( 67.59)	Acc@5  85.00 ( 90.71)
 * Acc@1 67.830 Acc@5 90.800
### epoch[88] execution time: 29.094798803329468
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.228 ( 0.228)	Data  0.150 ( 0.150)	Loss 2.9712e-01 (2.9712e-01)	Acc@1  91.41 ( 91.41)	Acc@5  98.44 ( 98.44)
Epoch: [89][ 10/391]	Time  0.068 ( 0.081)	Data  0.001 ( 0.015)	Loss 4.7925e-01 (4.0410e-01)	Acc@1  84.38 ( 87.00)	Acc@5  98.44 ( 98.86)
Epoch: [89][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.1748e-01 (4.1068e-01)	Acc@1  87.50 ( 87.09)	Acc@5  98.44 ( 98.92)
Epoch: [89][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.8413e-01 (4.1956e-01)	Acc@1  86.72 ( 87.37)	Acc@5  97.66 ( 98.77)
Epoch: [89][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.0430e-01 (4.1853e-01)	Acc@1  87.50 ( 87.39)	Acc@5  98.44 ( 98.72)
Epoch: [89][ 50/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.004)	Loss 4.9487e-01 (4.2002e-01)	Acc@1  87.50 ( 87.48)	Acc@5  97.66 ( 98.68)
Epoch: [89][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.6143e-01 (4.2120e-01)	Acc@1  89.06 ( 87.53)	Acc@5  97.66 ( 98.67)
Epoch: [89][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.8940e-01 (4.2074e-01)	Acc@1  87.50 ( 87.61)	Acc@5 100.00 ( 98.69)
Epoch: [89][ 80/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.8916e-01 (4.1881e-01)	Acc@1  88.28 ( 87.74)	Acc@5  99.22 ( 98.76)
Epoch: [89][ 90/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 4.0698e-01 (4.2364e-01)	Acc@1  86.72 ( 87.60)	Acc@5  99.22 ( 98.74)
Epoch: [89][100/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.8477e-01 (4.2399e-01)	Acc@1  90.62 ( 87.69)	Acc@5  97.66 ( 98.72)
Epoch: [89][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.1787e-01 (4.2383e-01)	Acc@1  90.62 ( 87.65)	Acc@5 100.00 ( 98.75)
Epoch: [89][120/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6035e-01 (4.2429e-01)	Acc@1  88.28 ( 87.59)	Acc@5  99.22 ( 98.73)
Epoch: [89][130/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8794e-01 (4.2182e-01)	Acc@1  88.28 ( 87.70)	Acc@5  99.22 ( 98.75)
Epoch: [89][140/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4231e-01 (4.1812e-01)	Acc@1  92.19 ( 87.82)	Acc@5 100.00 ( 98.80)
Epoch: [89][150/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4263e-01 (4.1783e-01)	Acc@1  86.72 ( 87.85)	Acc@5  97.66 ( 98.80)
Epoch: [89][160/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5386e-01 (4.1914e-01)	Acc@1  86.72 ( 87.82)	Acc@5  98.44 ( 98.79)
Epoch: [89][170/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.8232e-01 (4.1937e-01)	Acc@1  88.28 ( 87.76)	Acc@5  98.44 ( 98.80)
Epoch: [89][180/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4263e-01 (4.2136e-01)	Acc@1  86.72 ( 87.64)	Acc@5  99.22 ( 98.79)
Epoch: [89][190/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0112e-01 (4.2284e-01)	Acc@1  87.50 ( 87.63)	Acc@5  98.44 ( 98.76)
Epoch: [89][200/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.0771e-01 (4.2380e-01)	Acc@1  89.06 ( 87.56)	Acc@5  97.66 ( 98.77)
Epoch: [89][210/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.4365e-01 (4.2265e-01)	Acc@1  91.41 ( 87.62)	Acc@5 100.00 ( 98.78)
Epoch: [89][220/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.7266e-01 (4.2138e-01)	Acc@1  84.38 ( 87.68)	Acc@5  99.22 ( 98.79)
Epoch: [89][230/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3823e-01 (4.2158e-01)	Acc@1  85.16 ( 87.70)	Acc@5  99.22 ( 98.79)
Epoch: [89][240/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7427e-01 (4.2175e-01)	Acc@1  86.72 ( 87.69)	Acc@5 100.00 ( 98.78)
Epoch: [89][250/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1162e-01 (4.2379e-01)	Acc@1  89.84 ( 87.59)	Acc@5 100.00 ( 98.77)
Epoch: [89][260/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1670e-01 (4.2605e-01)	Acc@1  85.16 ( 87.52)	Acc@5  95.31 ( 98.73)
Epoch: [89][270/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1821e-01 (4.2820e-01)	Acc@1  85.16 ( 87.44)	Acc@5  98.44 ( 98.72)
Epoch: [89][280/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1333e-01 (4.2871e-01)	Acc@1  87.50 ( 87.44)	Acc@5 100.00 ( 98.71)
Epoch: [89][290/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.6118e-01 (4.2852e-01)	Acc@1  85.16 ( 87.43)	Acc@5  98.44 ( 98.71)
Epoch: [89][300/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5996e-01 (4.2891e-01)	Acc@1  84.38 ( 87.42)	Acc@5  99.22 ( 98.70)
Epoch: [89][310/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.2798e-01 (4.3013e-01)	Acc@1  86.72 ( 87.37)	Acc@5  98.44 ( 98.69)
Epoch: [89][320/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.9648e-01 (4.3018e-01)	Acc@1  89.84 ( 87.36)	Acc@5  99.22 ( 98.69)
Epoch: [89][330/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.9219e-01 (4.2995e-01)	Acc@1  82.03 ( 87.37)	Acc@5  99.22 ( 98.69)
Epoch: [89][340/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.1758e-01 (4.3004e-01)	Acc@1  82.81 ( 87.38)	Acc@5  97.66 ( 98.68)
Epoch: [89][350/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.8706e-01 (4.2980e-01)	Acc@1  84.38 ( 87.36)	Acc@5 100.00 ( 98.68)
Epoch: [89][360/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.3701e-01 (4.3108e-01)	Acc@1  86.72 ( 87.34)	Acc@5 100.00 ( 98.67)
Epoch: [89][370/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.4165e-01 (4.3119e-01)	Acc@1  84.38 ( 87.34)	Acc@5  99.22 ( 98.67)
Epoch: [89][380/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.6436e-01 (4.3163e-01)	Acc@1  85.16 ( 87.34)	Acc@5  98.44 ( 98.67)
Epoch: [89][390/391]	Time  0.054 ( 0.067)	Data  0.001 ( 0.001)	Loss 4.4629e-01 (4.3199e-01)	Acc@1  86.25 ( 87.31)	Acc@5  98.75 ( 98.67)
## e[89] optimizer.zero_grad (sum) time: 0.3992583751678467
## e[89]       loss.backward (sum) time: 7.386179447174072
## e[89]      optimizer.step (sum) time: 3.581547975540161
## epoch[89] training(only) time: 26.16631054878235
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.1387e+00 (1.1387e+00)	Acc@1  67.00 ( 67.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.029 ( 0.038)	Loss 1.1572e+00 (1.2250e+00)	Acc@1  71.00 ( 69.45)	Acc@5  94.00 ( 89.91)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6436e+00 (1.2253e+00)	Acc@1  64.00 ( 68.29)	Acc@5  88.00 ( 90.05)
Test: [ 30/100]	Time  0.026 ( 0.030)	Loss 1.6738e+00 (1.2539e+00)	Acc@1  58.00 ( 67.23)	Acc@5  87.00 ( 89.58)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.1729e+00 (1.2282e+00)	Acc@1  68.00 ( 67.32)	Acc@5  91.00 ( 90.32)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2334e+00 (1.2313e+00)	Acc@1  69.00 ( 67.24)	Acc@5  90.00 ( 90.39)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2803e+00 (1.2205e+00)	Acc@1  64.00 ( 67.51)	Acc@5  93.00 ( 90.67)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0303e+00 (1.2208e+00)	Acc@1  75.00 ( 67.72)	Acc@5  92.00 ( 90.73)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2842e+00 (1.2275e+00)	Acc@1  65.00 ( 67.49)	Acc@5  89.00 ( 90.60)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.8545e+00 (1.2191e+00)	Acc@1  55.00 ( 67.54)	Acc@5  84.00 ( 90.65)
 * Acc@1 67.840 Acc@5 90.700
### epoch[89] execution time: 28.985692262649536
### Training complete:
#### total training(only) time: 2363.3217227458954
##### Total run time: 2621.4942915439606
