# Model: densenet121
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.densenet
<function densenet121 at 0x7fe371d6df28>
# model requested: 'densenet121'
# printing out the model
DenseNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (features): Sequential(
    (dense_block_layer_0): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_0): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block_layer_1): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_1): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block_layer_2): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_12): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_13): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_14): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_15): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_16): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_17): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_18): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_19): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_20): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_21): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_22): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_23): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (transition_layer_2): Transition(
      (down_sample): Sequential(
        (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (dense_block3): Sequential(
      (bottle_neck_layer_0): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_1): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_2): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_3): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_4): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_5): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_6): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_7): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_8): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_9): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_10): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_11): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_12): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_13): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_14): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
      (bottle_neck_layer_15): Bottleneck(
        (bottle_neck): Sequential(
          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): ReLU(inplace=True)
          (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): ReLU(inplace=True)
          (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        )
      )
    )
    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (linear): Linear(in_features=1024, out_features=100, bias=True)
)
# model is full precision
# Model: densenet121
# Dataset: cifarcentum
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  4.376 ( 4.376)	Data  0.107 ( 0.107)	Loss 4.6365e+00 (4.6365e+00)	Acc@1   1.56 (  1.56)	Acc@5   8.59 (  8.59)
Epoch: [0][ 10/391]	Time  0.131 ( 0.524)	Data  0.001 ( 0.011)	Loss 4.4771e+00 (4.6072e+00)	Acc@1   4.69 (  2.63)	Acc@5  19.53 ( 11.15)
Epoch: [0][ 20/391]	Time  0.136 ( 0.338)	Data  0.001 ( 0.006)	Loss 4.4342e+00 (4.5737e+00)	Acc@1   7.03 (  3.27)	Acc@5  17.97 ( 12.91)
Epoch: [0][ 30/391]	Time  0.133 ( 0.273)	Data  0.001 ( 0.004)	Loss 4.5298e+00 (4.5243e+00)	Acc@1   5.47 (  3.78)	Acc@5  17.19 ( 14.69)
Epoch: [0][ 40/391]	Time  0.132 ( 0.239)	Data  0.001 ( 0.004)	Loss 4.3968e+00 (4.4872e+00)	Acc@1   7.03 (  4.23)	Acc@5  14.06 ( 15.55)
Epoch: [0][ 50/391]	Time  0.132 ( 0.219)	Data  0.001 ( 0.003)	Loss 4.1866e+00 (4.4336e+00)	Acc@1   6.25 (  4.87)	Acc@5  25.00 ( 16.76)
Epoch: [0][ 60/391]	Time  0.131 ( 0.205)	Data  0.001 ( 0.003)	Loss 4.0817e+00 (4.3912e+00)	Acc@1   8.59 (  5.17)	Acc@5  28.12 ( 17.78)
Epoch: [0][ 70/391]	Time  0.135 ( 0.195)	Data  0.001 ( 0.003)	Loss 4.0842e+00 (4.3487e+00)	Acc@1   7.81 (  5.47)	Acc@5  25.00 ( 18.67)
Epoch: [0][ 80/391]	Time  0.133 ( 0.188)	Data  0.001 ( 0.002)	Loss 3.7870e+00 (4.3145e+00)	Acc@1   9.38 (  5.78)	Acc@5  37.50 ( 19.60)
Epoch: [0][ 90/391]	Time  0.133 ( 0.182)	Data  0.001 ( 0.002)	Loss 4.0063e+00 (4.2834e+00)	Acc@1   9.38 (  6.16)	Acc@5  26.56 ( 20.46)
Epoch: [0][100/391]	Time  0.135 ( 0.177)	Data  0.001 ( 0.002)	Loss 3.9476e+00 (4.2546e+00)	Acc@1   8.59 (  6.36)	Acc@5  25.78 ( 21.03)
Epoch: [0][110/391]	Time  0.132 ( 0.174)	Data  0.001 ( 0.002)	Loss 3.8410e+00 (4.2323e+00)	Acc@1   7.81 (  6.46)	Acc@5  31.25 ( 21.49)
Epoch: [0][120/391]	Time  0.131 ( 0.170)	Data  0.001 ( 0.002)	Loss 4.0700e+00 (4.2125e+00)	Acc@1   7.03 (  6.57)	Acc@5  23.44 ( 21.82)
Epoch: [0][130/391]	Time  0.134 ( 0.168)	Data  0.001 ( 0.002)	Loss 4.0201e+00 (4.1897e+00)	Acc@1   8.59 (  6.66)	Acc@5  28.12 ( 22.39)
Epoch: [0][140/391]	Time  0.134 ( 0.165)	Data  0.001 ( 0.002)	Loss 3.7021e+00 (4.1663e+00)	Acc@1   8.59 (  6.83)	Acc@5  32.81 ( 22.98)
Epoch: [0][150/391]	Time  0.140 ( 0.164)	Data  0.001 ( 0.002)	Loss 3.9962e+00 (4.1483e+00)	Acc@1   7.81 (  7.01)	Acc@5  28.12 ( 23.48)
Epoch: [0][160/391]	Time  0.138 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7392e+00 (4.1303e+00)	Acc@1  11.72 (  7.20)	Acc@5  34.38 ( 23.84)
Epoch: [0][170/391]	Time  0.132 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.9188e+00 (4.1131e+00)	Acc@1   7.03 (  7.37)	Acc@5  28.91 ( 24.36)
Epoch: [0][180/391]	Time  0.145 ( 0.159)	Data  0.001 ( 0.002)	Loss 3.7734e+00 (4.0960e+00)	Acc@1  10.94 (  7.60)	Acc@5  32.03 ( 24.89)
Epoch: [0][190/391]	Time  0.137 ( 0.158)	Data  0.001 ( 0.002)	Loss 3.8160e+00 (4.0802e+00)	Acc@1  12.50 (  7.78)	Acc@5  33.59 ( 25.36)
Epoch: [0][200/391]	Time  0.132 ( 0.156)	Data  0.001 ( 0.002)	Loss 3.8388e+00 (4.0667e+00)	Acc@1  10.94 (  7.94)	Acc@5  36.72 ( 25.78)
Epoch: [0][210/391]	Time  0.132 ( 0.155)	Data  0.001 ( 0.002)	Loss 3.7712e+00 (4.0524e+00)	Acc@1   7.81 (  8.08)	Acc@5  31.25 ( 26.13)
Epoch: [0][220/391]	Time  0.144 ( 0.154)	Data  0.001 ( 0.002)	Loss 3.7738e+00 (4.0429e+00)	Acc@1  14.84 (  8.18)	Acc@5  32.81 ( 26.38)
Epoch: [0][230/391]	Time  0.134 ( 0.154)	Data  0.001 ( 0.002)	Loss 3.6880e+00 (4.0312e+00)	Acc@1  10.94 (  8.30)	Acc@5  39.06 ( 26.76)
Epoch: [0][240/391]	Time  0.134 ( 0.153)	Data  0.001 ( 0.002)	Loss 3.7409e+00 (4.0188e+00)	Acc@1  14.06 (  8.41)	Acc@5  36.72 ( 27.14)
Epoch: [0][250/391]	Time  0.145 ( 0.152)	Data  0.001 ( 0.002)	Loss 3.6915e+00 (4.0028e+00)	Acc@1  19.53 (  8.60)	Acc@5  39.84 ( 27.53)
Epoch: [0][260/391]	Time  0.132 ( 0.152)	Data  0.001 ( 0.002)	Loss 3.7751e+00 (3.9919e+00)	Acc@1  12.50 (  8.73)	Acc@5  33.59 ( 27.83)
Epoch: [0][270/391]	Time  0.134 ( 0.151)	Data  0.001 ( 0.001)	Loss 3.6777e+00 (3.9842e+00)	Acc@1  11.72 (  8.82)	Acc@5  33.59 ( 27.99)
Epoch: [0][280/391]	Time  0.132 ( 0.150)	Data  0.001 ( 0.001)	Loss 3.5387e+00 (3.9741e+00)	Acc@1  10.16 (  8.92)	Acc@5  35.94 ( 28.24)
Epoch: [0][290/391]	Time  0.136 ( 0.150)	Data  0.001 ( 0.001)	Loss 3.5330e+00 (3.9644e+00)	Acc@1  14.06 (  9.06)	Acc@5  42.19 ( 28.51)
Epoch: [0][300/391]	Time  0.132 ( 0.149)	Data  0.001 ( 0.001)	Loss 3.6812e+00 (3.9542e+00)	Acc@1   9.38 (  9.19)	Acc@5  41.41 ( 28.83)
Epoch: [0][310/391]	Time  0.136 ( 0.149)	Data  0.001 ( 0.001)	Loss 3.6022e+00 (3.9428e+00)	Acc@1  13.28 (  9.35)	Acc@5  38.28 ( 29.13)
Epoch: [0][320/391]	Time  0.132 ( 0.148)	Data  0.001 ( 0.001)	Loss 3.8583e+00 (3.9317e+00)	Acc@1  11.72 (  9.48)	Acc@5  30.47 ( 29.41)
Epoch: [0][330/391]	Time  0.133 ( 0.148)	Data  0.001 ( 0.001)	Loss 3.6171e+00 (3.9212e+00)	Acc@1  10.16 (  9.61)	Acc@5  42.97 ( 29.70)
Epoch: [0][340/391]	Time  0.132 ( 0.148)	Data  0.001 ( 0.001)	Loss 3.5187e+00 (3.9109e+00)	Acc@1  18.75 (  9.73)	Acc@5  39.06 ( 29.98)
Epoch: [0][350/391]	Time  0.135 ( 0.147)	Data  0.001 ( 0.001)	Loss 3.5837e+00 (3.9010e+00)	Acc@1  17.97 (  9.89)	Acc@5  38.28 ( 30.29)
Epoch: [0][360/391]	Time  0.135 ( 0.147)	Data  0.001 ( 0.001)	Loss 3.5935e+00 (3.8905e+00)	Acc@1  15.62 ( 10.02)	Acc@5  35.94 ( 30.56)
Epoch: [0][370/391]	Time  0.135 ( 0.147)	Data  0.001 ( 0.001)	Loss 3.6349e+00 (3.8804e+00)	Acc@1  17.19 ( 10.13)	Acc@5  39.06 ( 30.85)
Epoch: [0][380/391]	Time  0.138 ( 0.146)	Data  0.001 ( 0.001)	Loss 3.2337e+00 (3.8696e+00)	Acc@1  20.31 ( 10.29)	Acc@5  50.00 ( 31.14)
Epoch: [0][390/391]	Time  1.432 ( 0.149)	Data  0.001 ( 0.001)	Loss 3.3358e+00 (3.8589e+00)	Acc@1  16.25 ( 10.44)	Acc@5  47.50 ( 31.47)
## e[0] optimizer.zero_grad (sum) time: 0.66994309425354
## e[0]       loss.backward (sum) time: 15.418017148971558
## e[0]      optimizer.step (sum) time: 14.447073698043823
## epoch[0] training(only) time: 58.49640941619873
# Switched to evaluate mode...
Test: [  0/100]	Time  0.634 ( 0.634)	Loss 3.4630e+00 (3.4630e+00)	Acc@1  11.00 ( 11.00)	Acc@5  40.00 ( 40.00)
Test: [ 10/100]	Time  0.046 ( 0.101)	Loss 3.8187e+00 (3.5676e+00)	Acc@1  13.00 ( 13.82)	Acc@5  39.00 ( 41.55)
Test: [ 20/100]	Time  0.046 ( 0.075)	Loss 3.5854e+00 (3.5740e+00)	Acc@1  12.00 ( 14.43)	Acc@5  40.00 ( 41.29)
Test: [ 30/100]	Time  0.045 ( 0.066)	Loss 3.7560e+00 (3.5773e+00)	Acc@1  10.00 ( 14.06)	Acc@5  37.00 ( 40.90)
Test: [ 40/100]	Time  0.045 ( 0.061)	Loss 3.7207e+00 (3.5734e+00)	Acc@1  14.00 ( 14.05)	Acc@5  36.00 ( 40.98)
Test: [ 50/100]	Time  0.045 ( 0.058)	Loss 3.4956e+00 (3.5717e+00)	Acc@1  18.00 ( 14.16)	Acc@5  40.00 ( 41.18)
Test: [ 60/100]	Time  0.045 ( 0.056)	Loss 3.5190e+00 (3.5713e+00)	Acc@1  14.00 ( 14.03)	Acc@5  49.00 ( 40.98)
Test: [ 70/100]	Time  0.045 ( 0.054)	Loss 3.5771e+00 (3.5714e+00)	Acc@1  17.00 ( 13.96)	Acc@5  42.00 ( 40.77)
Test: [ 80/100]	Time  0.044 ( 0.053)	Loss 3.6729e+00 (3.5763e+00)	Acc@1  11.00 ( 14.07)	Acc@5  37.00 ( 40.77)
Test: [ 90/100]	Time  0.045 ( 0.053)	Loss 3.4507e+00 (3.5754e+00)	Acc@1  16.00 ( 14.23)	Acc@5  44.00 ( 41.05)
 * Acc@1 14.310 Acc@5 41.020
### epoch[0] execution time: 63.797996282577515
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.364 ( 0.364)	Data  0.159 ( 0.159)	Loss 3.4045e+00 (3.4045e+00)	Acc@1  23.44 ( 23.44)	Acc@5  45.31 ( 45.31)
Epoch: [1][ 10/391]	Time  0.137 ( 0.155)	Data  0.001 ( 0.015)	Loss 3.3630e+00 (3.3798e+00)	Acc@1  21.88 ( 17.05)	Acc@5  42.97 ( 45.67)
Epoch: [1][ 20/391]	Time  0.137 ( 0.146)	Data  0.001 ( 0.009)	Loss 3.4368e+00 (3.4122e+00)	Acc@1  17.97 ( 17.56)	Acc@5  43.75 ( 44.64)
Epoch: [1][ 30/391]	Time  0.144 ( 0.143)	Data  0.001 ( 0.006)	Loss 3.4357e+00 (3.4071e+00)	Acc@1  17.19 ( 17.77)	Acc@5  39.06 ( 44.41)
Epoch: [1][ 40/391]	Time  0.132 ( 0.141)	Data  0.001 ( 0.005)	Loss 3.2778e+00 (3.3975e+00)	Acc@1  21.88 ( 17.93)	Acc@5  42.19 ( 44.57)
Epoch: [1][ 50/391]	Time  0.132 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.3128e+00 (3.3896e+00)	Acc@1  22.66 ( 18.18)	Acc@5  46.88 ( 45.05)
Epoch: [1][ 60/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.004)	Loss 3.4808e+00 (3.3835e+00)	Acc@1  19.53 ( 18.26)	Acc@5  47.66 ( 45.30)
Epoch: [1][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.5167e+00 (3.3841e+00)	Acc@1  14.06 ( 18.12)	Acc@5  42.97 ( 45.29)
Epoch: [1][ 80/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.4529e+00 (3.3849e+00)	Acc@1  14.84 ( 18.05)	Acc@5  42.97 ( 45.37)
Epoch: [1][ 90/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.003)	Loss 3.1633e+00 (3.3812e+00)	Acc@1  22.66 ( 18.00)	Acc@5  50.00 ( 45.42)
Epoch: [1][100/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.003)	Loss 3.4510e+00 (3.3741e+00)	Acc@1  20.31 ( 18.23)	Acc@5  44.53 ( 45.54)
Epoch: [1][110/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.003)	Loss 3.3289e+00 (3.3739e+00)	Acc@1  17.97 ( 18.24)	Acc@5  43.75 ( 45.47)
Epoch: [1][120/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1549e+00 (3.3560e+00)	Acc@1  21.09 ( 18.42)	Acc@5  50.00 ( 45.93)
Epoch: [1][130/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.2954e+00 (3.3497e+00)	Acc@1  20.31 ( 18.58)	Acc@5  51.56 ( 46.15)
Epoch: [1][140/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.2410e+00 (3.3431e+00)	Acc@1  17.19 ( 18.73)	Acc@5  48.44 ( 46.33)
Epoch: [1][150/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.0430e+00 (3.3372e+00)	Acc@1  21.88 ( 18.77)	Acc@5  53.91 ( 46.52)
Epoch: [1][160/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.3251e+00 (3.3314e+00)	Acc@1  16.41 ( 18.71)	Acc@5  46.88 ( 46.65)
Epoch: [1][170/391]	Time  0.140 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.1119e+00 (3.3268e+00)	Acc@1  19.53 ( 18.76)	Acc@5  50.00 ( 46.72)
Epoch: [1][180/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.0716e+00 (3.3200e+00)	Acc@1  19.53 ( 18.86)	Acc@5  52.34 ( 46.84)
Epoch: [1][190/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.1063e+00 (3.3163e+00)	Acc@1  27.34 ( 19.04)	Acc@5  53.12 ( 46.94)
Epoch: [1][200/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.1373e+00 (3.3103e+00)	Acc@1  22.66 ( 19.16)	Acc@5  52.34 ( 47.12)
Epoch: [1][210/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.1524e+00 (3.3051e+00)	Acc@1  20.31 ( 19.25)	Acc@5  53.12 ( 47.22)
Epoch: [1][220/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.0532e+00 (3.2951e+00)	Acc@1  21.88 ( 19.36)	Acc@5  52.34 ( 47.50)
Epoch: [1][230/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.0546e+00 (3.2870e+00)	Acc@1  25.00 ( 19.50)	Acc@5  55.47 ( 47.77)
Epoch: [1][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.0608e+00 (3.2808e+00)	Acc@1  25.00 ( 19.61)	Acc@5  57.03 ( 47.95)
Epoch: [1][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.1772e+00 (3.2739e+00)	Acc@1  17.19 ( 19.73)	Acc@5  49.22 ( 48.20)
Epoch: [1][260/391]	Time  0.132 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.0245e+00 (3.2678e+00)	Acc@1  21.88 ( 19.77)	Acc@5  51.56 ( 48.36)
Epoch: [1][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.2837e+00 (3.2635e+00)	Acc@1  21.88 ( 19.84)	Acc@5  51.56 ( 48.49)
Epoch: [1][280/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.9159e+00 (3.2566e+00)	Acc@1  25.00 ( 20.03)	Acc@5  59.38 ( 48.67)
Epoch: [1][290/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.8908e+00 (3.2478e+00)	Acc@1  28.91 ( 20.23)	Acc@5  52.34 ( 48.93)
Epoch: [1][300/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.0180e+00 (3.2430e+00)	Acc@1  25.00 ( 20.25)	Acc@5  53.91 ( 49.08)
Epoch: [1][310/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 3.4234e+00 (3.2391e+00)	Acc@1  17.97 ( 20.32)	Acc@5  48.44 ( 49.24)
Epoch: [1][320/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.8648e+00 (3.2324e+00)	Acc@1  29.69 ( 20.49)	Acc@5  59.38 ( 49.43)
Epoch: [1][330/391]	Time  0.134 ( 0.136)	Data  0.001 ( 0.002)	Loss 3.1204e+00 (3.2239e+00)	Acc@1  24.22 ( 20.66)	Acc@5  57.03 ( 49.68)
Epoch: [1][340/391]	Time  0.133 ( 0.136)	Data  0.001 ( 0.002)	Loss 2.9009e+00 (3.2162e+00)	Acc@1  23.44 ( 20.81)	Acc@5  57.81 ( 49.87)
Epoch: [1][350/391]	Time  0.146 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.9733e+00 (3.2071e+00)	Acc@1  26.56 ( 21.00)	Acc@5  58.59 ( 50.11)
Epoch: [1][360/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.8881e+00 (3.2005e+00)	Acc@1  25.00 ( 21.09)	Acc@5  58.59 ( 50.26)
Epoch: [1][370/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.8478e+00 (3.1946e+00)	Acc@1  26.56 ( 21.17)	Acc@5  63.28 ( 50.42)
Epoch: [1][380/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.9723e+00 (3.1868e+00)	Acc@1  22.66 ( 21.33)	Acc@5  53.91 ( 50.64)
Epoch: [1][390/391]	Time  0.111 ( 0.136)	Data  0.001 ( 0.002)	Loss 2.8373e+00 (3.1812e+00)	Acc@1  26.25 ( 21.44)	Acc@5  60.00 ( 50.80)
## e[1] optimizer.zero_grad (sum) time: 0.6693811416625977
## e[1]       loss.backward (sum) time: 13.774465560913086
## e[1]      optimizer.step (sum) time: 14.62367844581604
## epoch[1] training(only) time: 53.42639350891113
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 2.9004e+00 (2.9004e+00)	Acc@1  32.00 ( 32.00)	Acc@5  59.00 ( 59.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 2.9539e+00 (2.9225e+00)	Acc@1  20.00 ( 25.82)	Acc@5  50.00 ( 58.00)
Test: [ 20/100]	Time  0.049 ( 0.054)	Loss 2.8382e+00 (2.9117e+00)	Acc@1  28.00 ( 26.29)	Acc@5  54.00 ( 58.00)
Test: [ 30/100]	Time  0.045 ( 0.051)	Loss 3.0700e+00 (2.9092e+00)	Acc@1  28.00 ( 26.06)	Acc@5  56.00 ( 58.39)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 2.8561e+00 (2.8986e+00)	Acc@1  28.00 ( 26.44)	Acc@5  61.00 ( 58.39)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 2.8436e+00 (2.9063e+00)	Acc@1  30.00 ( 26.55)	Acc@5  57.00 ( 58.24)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.9083e+00 (2.9000e+00)	Acc@1  23.00 ( 26.54)	Acc@5  57.00 ( 58.25)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.0422e+00 (2.9070e+00)	Acc@1  23.00 ( 26.70)	Acc@5  61.00 ( 58.14)
Test: [ 80/100]	Time  0.045 ( 0.048)	Loss 3.2277e+00 (2.9214e+00)	Acc@1  18.00 ( 26.37)	Acc@5  54.00 ( 57.95)
Test: [ 90/100]	Time  0.047 ( 0.048)	Loss 2.8111e+00 (2.9129e+00)	Acc@1  34.00 ( 26.59)	Acc@5  60.00 ( 58.25)
 * Acc@1 26.730 Acc@5 58.380
### epoch[1] execution time: 58.31242489814758
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.310 ( 0.310)	Data  0.158 ( 0.158)	Loss 2.9311e+00 (2.9311e+00)	Acc@1  23.44 ( 23.44)	Acc@5  60.94 ( 60.94)
Epoch: [2][ 10/391]	Time  0.136 ( 0.152)	Data  0.001 ( 0.015)	Loss 2.7497e+00 (2.8717e+00)	Acc@1  25.78 ( 26.14)	Acc@5  60.94 ( 58.66)
Epoch: [2][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.009)	Loss 3.0266e+00 (2.8640e+00)	Acc@1  26.56 ( 27.16)	Acc@5  60.94 ( 58.93)
Epoch: [2][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.6720e+00 (2.8175e+00)	Acc@1  28.91 ( 27.85)	Acc@5  66.41 ( 60.58)
Epoch: [2][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.7950e+00 (2.8265e+00)	Acc@1  31.25 ( 27.95)	Acc@5  62.50 ( 60.33)
Epoch: [2][ 50/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.8853e+00 (2.8300e+00)	Acc@1  23.44 ( 27.99)	Acc@5  59.38 ( 60.28)
Epoch: [2][ 60/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.004)	Loss 2.4522e+00 (2.8265e+00)	Acc@1  34.38 ( 28.01)	Acc@5  67.97 ( 60.34)
Epoch: [2][ 70/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.5534e+00 (2.8179e+00)	Acc@1  32.81 ( 28.26)	Acc@5  65.62 ( 60.40)
Epoch: [2][ 80/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.7774e+00 (2.8011e+00)	Acc@1  28.91 ( 28.52)	Acc@5  60.94 ( 60.78)
Epoch: [2][ 90/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.7040e+00 (2.8003e+00)	Acc@1  27.34 ( 28.43)	Acc@5  64.06 ( 60.84)
Epoch: [2][100/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.003)	Loss 2.6837e+00 (2.7878e+00)	Acc@1  33.59 ( 28.64)	Acc@5  66.41 ( 61.05)
Epoch: [2][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.003)	Loss 2.7601e+00 (2.7790e+00)	Acc@1  28.12 ( 28.95)	Acc@5  57.03 ( 61.26)
Epoch: [2][120/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6051e+00 (2.7739e+00)	Acc@1  29.69 ( 28.99)	Acc@5  64.84 ( 61.47)
Epoch: [2][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8830e+00 (2.7725e+00)	Acc@1  26.56 ( 29.00)	Acc@5  58.59 ( 61.46)
Epoch: [2][140/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9223e+00 (2.7686e+00)	Acc@1  28.91 ( 29.10)	Acc@5  58.59 ( 61.56)
Epoch: [2][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9321e+00 (2.7720e+00)	Acc@1  21.88 ( 29.01)	Acc@5  53.12 ( 61.43)
Epoch: [2][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7277e+00 (2.7685e+00)	Acc@1  28.91 ( 29.01)	Acc@5  64.06 ( 61.48)
Epoch: [2][170/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7410e+00 (2.7636e+00)	Acc@1  31.25 ( 29.08)	Acc@5  62.50 ( 61.65)
Epoch: [2][180/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7670e+00 (2.7577e+00)	Acc@1  30.47 ( 29.18)	Acc@5  59.38 ( 61.77)
Epoch: [2][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.7884e+00 (2.7511e+00)	Acc@1  34.38 ( 29.39)	Acc@5  61.72 ( 61.95)
Epoch: [2][200/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.5436e+00 (2.7470e+00)	Acc@1  35.94 ( 29.61)	Acc@5  68.75 ( 62.07)
Epoch: [2][210/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.5332e+00 (2.7428e+00)	Acc@1  32.81 ( 29.68)	Acc@5  63.28 ( 62.18)
Epoch: [2][220/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.7942e+00 (2.7381e+00)	Acc@1  30.47 ( 29.81)	Acc@5  58.59 ( 62.32)
Epoch: [2][230/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.5424e+00 (2.7265e+00)	Acc@1  28.91 ( 29.98)	Acc@5  62.50 ( 62.55)
Epoch: [2][240/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.6124e+00 (2.7217e+00)	Acc@1  35.16 ( 30.05)	Acc@5  67.19 ( 62.63)
Epoch: [2][250/391]	Time  0.144 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.5376e+00 (2.7166e+00)	Acc@1  35.16 ( 30.15)	Acc@5  75.00 ( 62.76)
Epoch: [2][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.5679e+00 (2.7105e+00)	Acc@1  32.81 ( 30.29)	Acc@5  64.06 ( 62.89)
Epoch: [2][270/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.3130e+00 (2.7023e+00)	Acc@1  36.72 ( 30.42)	Acc@5  75.78 ( 63.11)
Epoch: [2][280/391]	Time  0.143 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.6788e+00 (2.6989e+00)	Acc@1  29.69 ( 30.47)	Acc@5  67.97 ( 63.24)
Epoch: [2][290/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.2554e+00 (2.6923e+00)	Acc@1  35.16 ( 30.59)	Acc@5  73.44 ( 63.39)
Epoch: [2][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.3743e+00 (2.6871e+00)	Acc@1  43.75 ( 30.77)	Acc@5  68.75 ( 63.46)
Epoch: [2][310/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.5777e+00 (2.6824e+00)	Acc@1  32.81 ( 30.91)	Acc@5  68.75 ( 63.55)
Epoch: [2][320/391]	Time  0.143 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.6219e+00 (2.6776e+00)	Acc@1  30.47 ( 31.02)	Acc@5  62.50 ( 63.62)
Epoch: [2][330/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.2929e+00 (2.6729e+00)	Acc@1  35.94 ( 31.12)	Acc@5  69.53 ( 63.73)
Epoch: [2][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.5159e+00 (2.6626e+00)	Acc@1  39.06 ( 31.36)	Acc@5  66.41 ( 63.94)
Epoch: [2][350/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.4221e+00 (2.6592e+00)	Acc@1  27.34 ( 31.31)	Acc@5  70.31 ( 64.06)
Epoch: [2][360/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.4418e+00 (2.6534e+00)	Acc@1  38.28 ( 31.43)	Acc@5  71.09 ( 64.22)
Epoch: [2][370/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.6627e+00 (2.6508e+00)	Acc@1  32.03 ( 31.49)	Acc@5  63.28 ( 64.25)
Epoch: [2][380/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.5872e+00 (2.6461e+00)	Acc@1  36.72 ( 31.57)	Acc@5  64.84 ( 64.36)
Epoch: [2][390/391]	Time  0.105 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.8212e+00 (2.6412e+00)	Acc@1  31.25 ( 31.65)	Acc@5  60.00 ( 64.45)
## e[2] optimizer.zero_grad (sum) time: 0.6694793701171875
## e[2]       loss.backward (sum) time: 13.873783349990845
## e[2]      optimizer.step (sum) time: 14.683874130249023
## epoch[2] training(only) time: 53.71009612083435
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 2.7267e+00 (2.7267e+00)	Acc@1  28.00 ( 28.00)	Acc@5  65.00 ( 65.00)
Test: [ 10/100]	Time  0.046 ( 0.063)	Loss 2.8233e+00 (2.6863e+00)	Acc@1  28.00 ( 32.55)	Acc@5  64.00 ( 65.91)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 2.5630e+00 (2.6530e+00)	Acc@1  32.00 ( 33.14)	Acc@5  68.00 ( 65.95)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 3.0383e+00 (2.6418e+00)	Acc@1  27.00 ( 32.90)	Acc@5  64.00 ( 65.84)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.9025e+00 (2.6501e+00)	Acc@1  34.00 ( 32.73)	Acc@5  63.00 ( 65.98)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.3115e+00 (2.6601e+00)	Acc@1  37.00 ( 32.71)	Acc@5  67.00 ( 65.29)
Test: [ 60/100]	Time  0.049 ( 0.050)	Loss 2.5371e+00 (2.6642e+00)	Acc@1  34.00 ( 32.67)	Acc@5  69.00 ( 65.03)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.9087e+00 (2.6689e+00)	Acc@1  25.00 ( 32.89)	Acc@5  55.00 ( 64.86)
Test: [ 80/100]	Time  0.045 ( 0.049)	Loss 2.6316e+00 (2.6819e+00)	Acc@1  31.00 ( 32.67)	Acc@5  61.00 ( 64.51)
Test: [ 90/100]	Time  0.045 ( 0.049)	Loss 2.6796e+00 (2.6744e+00)	Acc@1  34.00 ( 32.89)	Acc@5  68.00 ( 64.66)
 * Acc@1 33.060 Acc@5 64.810
### epoch[2] execution time: 58.666152238845825
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.298 ( 0.298)	Data  0.154 ( 0.154)	Loss 2.2363e+00 (2.2363e+00)	Acc@1  35.16 ( 35.16)	Acc@5  75.78 ( 75.78)
Epoch: [3][ 10/391]	Time  0.133 ( 0.152)	Data  0.001 ( 0.015)	Loss 2.2490e+00 (2.3139e+00)	Acc@1  42.19 ( 38.28)	Acc@5  74.22 ( 71.95)
Epoch: [3][ 20/391]	Time  0.141 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.1285e+00 (2.3439e+00)	Acc@1  44.53 ( 37.09)	Acc@5  71.88 ( 71.80)
Epoch: [3][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.0117e+00 (2.3458e+00)	Acc@1  41.41 ( 36.82)	Acc@5  73.44 ( 71.30)
Epoch: [3][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.1777e+00 (2.3503e+00)	Acc@1  38.28 ( 36.60)	Acc@5  67.97 ( 70.79)
Epoch: [3][ 50/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.5906e+00 (2.3579e+00)	Acc@1  32.81 ( 36.55)	Acc@5  68.75 ( 70.82)
Epoch: [3][ 60/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.004)	Loss 2.4511e+00 (2.3554e+00)	Acc@1  35.16 ( 36.78)	Acc@5  66.41 ( 70.82)
Epoch: [3][ 70/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.2186e+00 (2.3532e+00)	Acc@1  43.75 ( 36.80)	Acc@5  73.44 ( 70.96)
Epoch: [3][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.9984e+00 (2.3475e+00)	Acc@1  46.09 ( 37.12)	Acc@5  82.81 ( 71.14)
Epoch: [3][ 90/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.2576e+00 (2.3398e+00)	Acc@1  39.84 ( 37.21)	Acc@5  75.00 ( 71.28)
Epoch: [3][100/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.003)	Loss 2.1850e+00 (2.3343e+00)	Acc@1  39.84 ( 37.47)	Acc@5  74.22 ( 71.32)
Epoch: [3][110/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2145e+00 (2.3320e+00)	Acc@1  36.72 ( 37.59)	Acc@5  74.22 ( 71.38)
Epoch: [3][120/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2305e+00 (2.3312e+00)	Acc@1  45.31 ( 37.60)	Acc@5  69.53 ( 71.38)
Epoch: [3][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1375e+00 (2.3276e+00)	Acc@1  39.84 ( 37.65)	Acc@5  78.91 ( 71.53)
Epoch: [3][140/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2258e+00 (2.3302e+00)	Acc@1  42.19 ( 37.48)	Acc@5  71.88 ( 71.46)
Epoch: [3][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0770e+00 (2.3200e+00)	Acc@1  44.53 ( 37.74)	Acc@5  76.56 ( 71.62)
Epoch: [3][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2121e+00 (2.3200e+00)	Acc@1  39.06 ( 37.68)	Acc@5  75.00 ( 71.67)
Epoch: [3][170/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1865e+00 (2.3149e+00)	Acc@1  39.84 ( 37.75)	Acc@5  77.34 ( 71.88)
Epoch: [3][180/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.0897e+00 (2.3114e+00)	Acc@1  46.88 ( 37.93)	Acc@5  72.66 ( 71.87)
Epoch: [3][190/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.1676e+00 (2.3095e+00)	Acc@1  41.41 ( 37.99)	Acc@5  72.66 ( 71.94)
Epoch: [3][200/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.3551e+00 (2.3026e+00)	Acc@1  39.06 ( 38.11)	Acc@5  66.41 ( 72.03)
Epoch: [3][210/391]	Time  0.133 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.1178e+00 (2.2987e+00)	Acc@1  44.53 ( 38.19)	Acc@5  78.12 ( 72.15)
Epoch: [3][220/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.1137e+00 (2.2962e+00)	Acc@1  41.41 ( 38.32)	Acc@5  76.56 ( 72.15)
Epoch: [3][230/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.3348e+00 (2.2912e+00)	Acc@1  43.75 ( 38.42)	Acc@5  65.62 ( 72.23)
Epoch: [3][240/391]	Time  0.139 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.2728e+00 (2.2896e+00)	Acc@1  37.50 ( 38.45)	Acc@5  75.78 ( 72.30)
Epoch: [3][250/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.2428e+00 (2.2864e+00)	Acc@1  39.06 ( 38.55)	Acc@5  75.00 ( 72.35)
Epoch: [3][260/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.2346e+00 (2.2822e+00)	Acc@1  42.19 ( 38.67)	Acc@5  73.44 ( 72.39)
Epoch: [3][270/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.1060e+00 (2.2765e+00)	Acc@1  42.97 ( 38.83)	Acc@5  72.66 ( 72.48)
Epoch: [3][280/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.0638e+00 (2.2728e+00)	Acc@1  42.97 ( 38.95)	Acc@5  74.22 ( 72.53)
Epoch: [3][290/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.2550e+00 (2.2693e+00)	Acc@1  40.62 ( 39.04)	Acc@5  74.22 ( 72.61)
Epoch: [3][300/391]	Time  0.134 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.2961e+00 (2.2684e+00)	Acc@1  35.94 ( 39.08)	Acc@5  73.44 ( 72.66)
Epoch: [3][310/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.0553e+00 (2.2661e+00)	Acc@1  40.62 ( 39.15)	Acc@5  78.91 ( 72.72)
Epoch: [3][320/391]	Time  0.148 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.1124e+00 (2.2677e+00)	Acc@1  42.19 ( 39.15)	Acc@5  71.88 ( 72.70)
Epoch: [3][330/391]	Time  0.135 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.0571e+00 (2.2626e+00)	Acc@1  51.56 ( 39.36)	Acc@5  72.66 ( 72.78)
Epoch: [3][340/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.0896e+00 (2.2612e+00)	Acc@1  47.66 ( 39.39)	Acc@5  75.00 ( 72.81)
Epoch: [3][350/391]	Time  0.138 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.3449e+00 (2.2606e+00)	Acc@1  33.59 ( 39.42)	Acc@5  71.88 ( 72.82)
Epoch: [3][360/391]	Time  0.136 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.2466e+00 (2.2584e+00)	Acc@1  35.16 ( 39.45)	Acc@5  72.66 ( 72.87)
Epoch: [3][370/391]	Time  0.142 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.8291e+00 (2.2554e+00)	Acc@1  46.88 ( 39.52)	Acc@5  85.16 ( 72.92)
Epoch: [3][380/391]	Time  0.137 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.9216e+00 (2.2531e+00)	Acc@1  48.44 ( 39.58)	Acc@5  79.69 ( 72.99)
Epoch: [3][390/391]	Time  0.110 ( 0.137)	Data  0.001 ( 0.002)	Loss 2.2182e+00 (2.2502e+00)	Acc@1  27.50 ( 39.62)	Acc@5  77.50 ( 73.04)
## e[3] optimizer.zero_grad (sum) time: 0.6687717437744141
## e[3]       loss.backward (sum) time: 13.882795333862305
## e[3]      optimizer.step (sum) time: 14.68256163597107
## epoch[3] training(only) time: 53.69394564628601
# Switched to evaluate mode...
Test: [  0/100]	Time  0.206 ( 0.206)	Loss 2.3438e+00 (2.3438e+00)	Acc@1  40.00 ( 40.00)	Acc@5  68.00 ( 68.00)
Test: [ 10/100]	Time  0.051 ( 0.063)	Loss 2.5145e+00 (2.3653e+00)	Acc@1  32.00 ( 40.27)	Acc@5  76.00 ( 70.91)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 2.0641e+00 (2.3514e+00)	Acc@1  39.00 ( 38.81)	Acc@5  78.00 ( 71.43)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 2.4726e+00 (2.3474e+00)	Acc@1  32.00 ( 39.06)	Acc@5  69.00 ( 71.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.5777e+00 (2.3402e+00)	Acc@1  32.00 ( 39.12)	Acc@5  69.00 ( 71.93)
Test: [ 50/100]	Time  0.060 ( 0.051)	Loss 2.2215e+00 (2.3510e+00)	Acc@1  43.00 ( 38.82)	Acc@5  71.00 ( 71.53)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.3316e+00 (2.3409e+00)	Acc@1  37.00 ( 39.05)	Acc@5  73.00 ( 71.89)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.4929e+00 (2.3439e+00)	Acc@1  35.00 ( 38.99)	Acc@5  72.00 ( 71.69)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.3695e+00 (2.3573e+00)	Acc@1  38.00 ( 38.70)	Acc@5  69.00 ( 71.47)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 2.3969e+00 (2.3491e+00)	Acc@1  40.00 ( 39.04)	Acc@5  71.00 ( 71.53)
 * Acc@1 39.060 Acc@5 71.630
### epoch[3] execution time: 58.65154695510864
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.301 ( 0.301)	Data  0.131 ( 0.131)	Loss 2.2791e+00 (2.2791e+00)	Acc@1  39.06 ( 39.06)	Acc@5  67.19 ( 67.19)
Epoch: [4][ 10/391]	Time  0.134 ( 0.151)	Data  0.001 ( 0.013)	Loss 2.1636e+00 (2.0315e+00)	Acc@1  40.62 ( 45.88)	Acc@5  74.22 ( 75.71)
Epoch: [4][ 20/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.007)	Loss 1.9168e+00 (2.0633e+00)	Acc@1  46.88 ( 44.31)	Acc@5  82.03 ( 75.89)
Epoch: [4][ 30/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.8173e+00 (2.0341e+00)	Acc@1  49.22 ( 44.93)	Acc@5  82.03 ( 76.41)
Epoch: [4][ 40/391]	Time  0.140 ( 0.140)	Data  0.003 ( 0.004)	Loss 2.1599e+00 (2.0390e+00)	Acc@1  41.41 ( 44.61)	Acc@5  74.22 ( 76.26)
Epoch: [4][ 50/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.8214e+00 (2.0316e+00)	Acc@1  53.91 ( 44.88)	Acc@5  82.81 ( 76.62)
Epoch: [4][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9094e+00 (2.0364e+00)	Acc@1  45.31 ( 44.72)	Acc@5  75.00 ( 76.69)
Epoch: [4][ 70/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7694e+00 (2.0304e+00)	Acc@1  50.78 ( 45.18)	Acc@5  82.03 ( 76.78)
Epoch: [4][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.9432e+00 (2.0268e+00)	Acc@1  46.09 ( 45.24)	Acc@5  75.78 ( 77.09)
Epoch: [4][ 90/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.8732e+00 (2.0229e+00)	Acc@1  49.22 ( 45.27)	Acc@5  82.03 ( 77.28)
Epoch: [4][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0474e+00 (2.0210e+00)	Acc@1  49.22 ( 45.41)	Acc@5  75.78 ( 77.29)
Epoch: [4][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7631e+00 (2.0183e+00)	Acc@1  50.00 ( 45.46)	Acc@5  79.69 ( 77.38)
Epoch: [4][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0953e+00 (2.0189e+00)	Acc@1  46.09 ( 45.33)	Acc@5  76.56 ( 77.44)
Epoch: [4][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1857e+00 (2.0206e+00)	Acc@1  41.41 ( 45.27)	Acc@5  74.22 ( 77.45)
Epoch: [4][140/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1792e+00 (2.0230e+00)	Acc@1  37.50 ( 45.18)	Acc@5  75.78 ( 77.47)
Epoch: [4][150/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9438e+00 (2.0248e+00)	Acc@1  46.88 ( 45.06)	Acc@5  76.56 ( 77.46)
Epoch: [4][160/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0035e+00 (2.0249e+00)	Acc@1  42.19 ( 45.04)	Acc@5  78.12 ( 77.52)
Epoch: [4][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9847e+00 (2.0226e+00)	Acc@1  47.66 ( 45.08)	Acc@5  76.56 ( 77.60)
Epoch: [4][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0051e+00 (2.0190e+00)	Acc@1  45.31 ( 45.11)	Acc@5  75.78 ( 77.66)
Epoch: [4][190/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8983e+00 (2.0168e+00)	Acc@1  46.88 ( 45.16)	Acc@5  80.47 ( 77.70)
Epoch: [4][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2468e+00 (2.0182e+00)	Acc@1  40.62 ( 45.21)	Acc@5  73.44 ( 77.66)
Epoch: [4][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0898e+00 (2.0205e+00)	Acc@1  42.19 ( 45.18)	Acc@5  73.44 ( 77.66)
Epoch: [4][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1330e+00 (2.0206e+00)	Acc@1  38.28 ( 45.23)	Acc@5  73.44 ( 77.64)
Epoch: [4][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8512e+00 (2.0173e+00)	Acc@1  41.41 ( 45.26)	Acc@5  85.16 ( 77.67)
Epoch: [4][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9321e+00 (2.0155e+00)	Acc@1  42.19 ( 45.27)	Acc@5  80.47 ( 77.69)
Epoch: [4][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8829e+00 (2.0131e+00)	Acc@1  50.00 ( 45.35)	Acc@5  79.69 ( 77.70)
Epoch: [4][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9727e+00 (2.0124e+00)	Acc@1  46.88 ( 45.34)	Acc@5  79.69 ( 77.66)
Epoch: [4][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9855e+00 (2.0129e+00)	Acc@1  46.09 ( 45.27)	Acc@5  77.34 ( 77.65)
Epoch: [4][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8579e+00 (2.0147e+00)	Acc@1  49.22 ( 45.19)	Acc@5  85.94 ( 77.59)
Epoch: [4][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8229e+00 (2.0095e+00)	Acc@1  52.34 ( 45.29)	Acc@5  78.12 ( 77.70)
Epoch: [4][300/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0602e+00 (2.0084e+00)	Acc@1  41.41 ( 45.32)	Acc@5  80.47 ( 77.76)
Epoch: [4][310/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0505e+00 (2.0074e+00)	Acc@1  43.75 ( 45.36)	Acc@5  73.44 ( 77.76)
Epoch: [4][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0777e+00 (2.0042e+00)	Acc@1  42.97 ( 45.44)	Acc@5  75.78 ( 77.83)
Epoch: [4][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1965e+00 (2.0033e+00)	Acc@1  43.75 ( 45.46)	Acc@5  73.44 ( 77.86)
Epoch: [4][340/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9514e+00 (2.0031e+00)	Acc@1  46.88 ( 45.46)	Acc@5  78.91 ( 77.84)
Epoch: [4][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9591e+00 (2.0019e+00)	Acc@1  48.44 ( 45.52)	Acc@5  82.81 ( 77.87)
Epoch: [4][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0194e+00 (1.9986e+00)	Acc@1  39.84 ( 45.59)	Acc@5  77.34 ( 77.94)
Epoch: [4][370/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.9168e+00 (1.9974e+00)	Acc@1  42.19 ( 45.59)	Acc@5  83.59 ( 77.98)
Epoch: [4][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.9152e+00 (1.9945e+00)	Acc@1  45.31 ( 45.67)	Acc@5  82.03 ( 78.05)
Epoch: [4][390/391]	Time  0.109 ( 0.137)	Data  0.001 ( 0.001)	Loss 1.7658e+00 (1.9921e+00)	Acc@1  43.75 ( 45.72)	Acc@5  83.75 ( 78.06)
## e[4] optimizer.zero_grad (sum) time: 0.672154426574707
## e[4]       loss.backward (sum) time: 13.82384705543518
## e[4]      optimizer.step (sum) time: 14.759790182113647
## epoch[4] training(only) time: 53.81872630119324
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 2.0932e+00 (2.0932e+00)	Acc@1  45.00 ( 45.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.0852e+00 (2.1182e+00)	Acc@1  42.00 ( 44.82)	Acc@5  80.00 ( 76.91)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 2.1513e+00 (2.1052e+00)	Acc@1  46.00 ( 45.14)	Acc@5  79.00 ( 77.19)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.4287e+00 (2.0906e+00)	Acc@1  39.00 ( 45.13)	Acc@5  73.00 ( 77.42)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.2842e+00 (2.0738e+00)	Acc@1  45.00 ( 45.37)	Acc@5  81.00 ( 77.73)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.9892e+00 (2.0978e+00)	Acc@1  47.00 ( 44.84)	Acc@5  72.00 ( 77.10)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 2.1930e+00 (2.0988e+00)	Acc@1  45.00 ( 44.66)	Acc@5  83.00 ( 77.07)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.2164e+00 (2.1091e+00)	Acc@1  39.00 ( 44.54)	Acc@5  81.00 ( 77.07)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.0866e+00 (2.1243e+00)	Acc@1  49.00 ( 44.25)	Acc@5  76.00 ( 76.96)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.1017e+00 (2.1113e+00)	Acc@1  49.00 ( 44.64)	Acc@5  77.00 ( 77.18)
 * Acc@1 44.730 Acc@5 77.390
### epoch[4] execution time: 58.75737524032593
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.301 ( 0.301)	Data  0.155 ( 0.155)	Loss 1.8376e+00 (1.8376e+00)	Acc@1  50.00 ( 50.00)	Acc@5  78.12 ( 78.12)
Epoch: [5][ 10/391]	Time  0.138 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.7406e+00 (1.8374e+00)	Acc@1  49.22 ( 49.57)	Acc@5  85.16 ( 79.19)
Epoch: [5][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.9624e+00 (1.8138e+00)	Acc@1  43.75 ( 49.85)	Acc@5  80.47 ( 80.69)
Epoch: [5][ 30/391]	Time  0.144 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.8153e+00 (1.8021e+00)	Acc@1  49.22 ( 50.25)	Acc@5  79.69 ( 80.85)
Epoch: [5][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.7164e+00 (1.8095e+00)	Acc@1  51.56 ( 49.62)	Acc@5  81.25 ( 80.81)
Epoch: [5][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.9708e+00 (1.8013e+00)	Acc@1  42.97 ( 49.89)	Acc@5  83.59 ( 81.16)
Epoch: [5][ 60/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.8926e+00 (1.8213e+00)	Acc@1  48.44 ( 49.53)	Acc@5  83.59 ( 80.93)
Epoch: [5][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7556e+00 (1.8269e+00)	Acc@1  49.22 ( 49.35)	Acc@5  82.81 ( 80.88)
Epoch: [5][ 80/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7815e+00 (1.8320e+00)	Acc@1  53.91 ( 49.28)	Acc@5  78.12 ( 80.70)
Epoch: [5][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7565e+00 (1.8329e+00)	Acc@1  47.66 ( 49.28)	Acc@5  82.03 ( 80.67)
Epoch: [5][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7478e+00 (1.8391e+00)	Acc@1  53.91 ( 49.03)	Acc@5  82.81 ( 80.65)
Epoch: [5][110/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.003)	Loss 1.6613e+00 (1.8352e+00)	Acc@1  54.69 ( 49.17)	Acc@5  83.59 ( 80.70)
Epoch: [5][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9944e+00 (1.8294e+00)	Acc@1  43.75 ( 49.37)	Acc@5  79.69 ( 80.85)
Epoch: [5][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6045e+00 (1.8254e+00)	Acc@1  51.56 ( 49.36)	Acc@5  87.50 ( 80.93)
Epoch: [5][140/391]	Time  0.152 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9691e+00 (1.8259e+00)	Acc@1  45.31 ( 49.22)	Acc@5  78.91 ( 80.95)
Epoch: [5][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7444e+00 (1.8256e+00)	Acc@1  50.00 ( 49.24)	Acc@5  82.03 ( 80.98)
Epoch: [5][160/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6704e+00 (1.8243e+00)	Acc@1  50.78 ( 49.36)	Acc@5  82.81 ( 81.04)
Epoch: [5][170/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5720e+00 (1.8202e+00)	Acc@1  52.34 ( 49.45)	Acc@5  84.38 ( 81.16)
Epoch: [5][180/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8304e+00 (1.8159e+00)	Acc@1  48.44 ( 49.60)	Acc@5  82.03 ( 81.23)
Epoch: [5][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6089e+00 (1.8136e+00)	Acc@1  52.34 ( 49.62)	Acc@5  85.94 ( 81.30)
Epoch: [5][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6475e+00 (1.8164e+00)	Acc@1  52.34 ( 49.58)	Acc@5  88.28 ( 81.25)
Epoch: [5][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8953e+00 (1.8181e+00)	Acc@1  42.19 ( 49.50)	Acc@5  77.34 ( 81.19)
Epoch: [5][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7712e+00 (1.8175e+00)	Acc@1  55.47 ( 49.51)	Acc@5  78.91 ( 81.19)
Epoch: [5][230/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6869e+00 (1.8138e+00)	Acc@1  55.47 ( 49.64)	Acc@5  85.16 ( 81.23)
Epoch: [5][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7659e+00 (1.8140e+00)	Acc@1  56.25 ( 49.63)	Acc@5  79.69 ( 81.20)
Epoch: [5][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8561e+00 (1.8109e+00)	Acc@1  46.09 ( 49.69)	Acc@5  78.12 ( 81.23)
Epoch: [5][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0786e+00 (1.8132e+00)	Acc@1  44.53 ( 49.66)	Acc@5  78.12 ( 81.18)
Epoch: [5][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2644e+00 (1.8138e+00)	Acc@1  43.75 ( 49.64)	Acc@5  69.53 ( 81.18)
Epoch: [5][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7166e+00 (1.8113e+00)	Acc@1  53.12 ( 49.67)	Acc@5  82.81 ( 81.21)
Epoch: [5][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6907e+00 (1.8088e+00)	Acc@1  57.03 ( 49.77)	Acc@5  85.16 ( 81.24)
Epoch: [5][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9509e+00 (1.8086e+00)	Acc@1  45.31 ( 49.71)	Acc@5  78.12 ( 81.31)
Epoch: [5][310/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7416e+00 (1.8090e+00)	Acc@1  49.22 ( 49.67)	Acc@5  88.28 ( 81.29)
Epoch: [5][320/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7042e+00 (1.8088e+00)	Acc@1  51.56 ( 49.75)	Acc@5  80.47 ( 81.26)
Epoch: [5][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5813e+00 (1.8081e+00)	Acc@1  57.03 ( 49.81)	Acc@5  84.38 ( 81.28)
Epoch: [5][340/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9279e+00 (1.8076e+00)	Acc@1  50.78 ( 49.88)	Acc@5  78.12 ( 81.28)
Epoch: [5][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6896e+00 (1.8056e+00)	Acc@1  50.00 ( 49.91)	Acc@5  83.59 ( 81.31)
Epoch: [5][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7825e+00 (1.8015e+00)	Acc@1  54.69 ( 50.00)	Acc@5  82.03 ( 81.38)
Epoch: [5][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7315e+00 (1.7995e+00)	Acc@1  53.12 ( 50.06)	Acc@5  83.59 ( 81.42)
Epoch: [5][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4868e+00 (1.7982e+00)	Acc@1  59.38 ( 50.09)	Acc@5  88.28 ( 81.46)
Epoch: [5][390/391]	Time  0.109 ( 0.137)	Data  0.001 ( 0.002)	Loss 1.6016e+00 (1.7975e+00)	Acc@1  62.50 ( 50.11)	Acc@5  87.50 ( 81.48)
## e[5] optimizer.zero_grad (sum) time: 0.6693570613861084
## e[5]       loss.backward (sum) time: 13.849197149276733
## e[5]      optimizer.step (sum) time: 14.732279062271118
## epoch[5] training(only) time: 53.81901550292969
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 2.0254e+00 (2.0254e+00)	Acc@1  50.00 ( 50.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 2.2288e+00 (2.1303e+00)	Acc@1  42.00 ( 45.91)	Acc@5  78.00 ( 75.73)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 2.0074e+00 (2.1543e+00)	Acc@1  49.00 ( 45.14)	Acc@5  74.00 ( 75.38)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.1235e+00 (2.1438e+00)	Acc@1  45.00 ( 44.94)	Acc@5  72.00 ( 75.10)
Test: [ 40/100]	Time  0.050 ( 0.051)	Loss 2.0682e+00 (2.1357e+00)	Acc@1  48.00 ( 45.05)	Acc@5  78.00 ( 75.49)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.0342e+00 (2.1447e+00)	Acc@1  44.00 ( 45.06)	Acc@5  77.00 ( 75.14)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.9110e+00 (2.1270e+00)	Acc@1  45.00 ( 44.95)	Acc@5  77.00 ( 75.43)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.3429e+00 (2.1390e+00)	Acc@1  40.00 ( 44.86)	Acc@5  74.00 ( 75.20)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.0643e+00 (2.1402e+00)	Acc@1  46.00 ( 44.67)	Acc@5  81.00 ( 75.35)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.1157e+00 (2.1264e+00)	Acc@1  46.00 ( 44.86)	Acc@5  79.00 ( 75.75)
 * Acc@1 45.190 Acc@5 75.980
### epoch[5] execution time: 58.76965045928955
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.307 ( 0.307)	Data  0.149 ( 0.149)	Loss 1.9385e+00 (1.9385e+00)	Acc@1  45.31 ( 45.31)	Acc@5  78.91 ( 78.91)
Epoch: [6][ 10/391]	Time  0.138 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.7694e+00 (1.6543e+00)	Acc@1  52.34 ( 54.47)	Acc@5  82.03 ( 84.23)
Epoch: [6][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.5296e+00 (1.6281e+00)	Acc@1  57.03 ( 54.17)	Acc@5  85.94 ( 84.90)
Epoch: [6][ 30/391]	Time  0.143 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.5377e+00 (1.6247e+00)	Acc@1  54.69 ( 53.98)	Acc@5  89.06 ( 85.33)
Epoch: [6][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.4400e+00 (1.6165e+00)	Acc@1  59.38 ( 54.25)	Acc@5  89.06 ( 85.29)
Epoch: [6][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.6547e+00 (1.6198e+00)	Acc@1  49.22 ( 54.06)	Acc@5  82.81 ( 85.25)
Epoch: [6][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.5553e+00 (1.6153e+00)	Acc@1  59.38 ( 54.28)	Acc@5  83.59 ( 85.27)
Epoch: [6][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6633e+00 (1.6228e+00)	Acc@1  52.34 ( 54.12)	Acc@5  82.03 ( 84.85)
Epoch: [6][ 80/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6894e+00 (1.6273e+00)	Acc@1  55.47 ( 54.09)	Acc@5  81.25 ( 84.65)
Epoch: [6][ 90/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6714e+00 (1.6290e+00)	Acc@1  54.69 ( 54.02)	Acc@5  82.81 ( 84.58)
Epoch: [6][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3491e+00 (1.6280e+00)	Acc@1  60.16 ( 54.09)	Acc@5  86.72 ( 84.58)
Epoch: [6][110/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5728e+00 (1.6329e+00)	Acc@1  55.47 ( 53.84)	Acc@5  85.94 ( 84.53)
Epoch: [6][120/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7166e+00 (1.6310e+00)	Acc@1  49.22 ( 53.85)	Acc@5  83.59 ( 84.68)
Epoch: [6][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7383e+00 (1.6341e+00)	Acc@1  51.56 ( 53.83)	Acc@5  78.91 ( 84.55)
Epoch: [6][140/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6415e+00 (1.6349e+00)	Acc@1  52.34 ( 53.81)	Acc@5  85.16 ( 84.52)
Epoch: [6][150/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7190e+00 (1.6388e+00)	Acc@1  53.91 ( 53.86)	Acc@5  85.16 ( 84.47)
Epoch: [6][160/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6980e+00 (1.6431e+00)	Acc@1  49.22 ( 53.90)	Acc@5  85.94 ( 84.42)
Epoch: [6][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7446e+00 (1.6435e+00)	Acc@1  52.34 ( 53.90)	Acc@5  81.25 ( 84.43)
Epoch: [6][180/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8541e+00 (1.6473e+00)	Acc@1  48.44 ( 53.88)	Acc@5  78.91 ( 84.38)
Epoch: [6][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7999e+00 (1.6496e+00)	Acc@1  46.88 ( 53.82)	Acc@5  82.81 ( 84.29)
Epoch: [6][200/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5538e+00 (1.6491e+00)	Acc@1  55.47 ( 53.79)	Acc@5  89.84 ( 84.31)
Epoch: [6][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9378e+00 (1.6562e+00)	Acc@1  44.53 ( 53.67)	Acc@5  80.47 ( 84.21)
Epoch: [6][220/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6226e+00 (1.6533e+00)	Acc@1  53.12 ( 53.72)	Acc@5  83.59 ( 84.26)
Epoch: [6][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7464e+00 (1.6508e+00)	Acc@1  54.69 ( 53.78)	Acc@5  82.03 ( 84.30)
Epoch: [6][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4863e+00 (1.6498e+00)	Acc@1  60.94 ( 53.81)	Acc@5  85.16 ( 84.28)
Epoch: [6][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6874e+00 (1.6478e+00)	Acc@1  51.56 ( 53.82)	Acc@5  81.25 ( 84.29)
Epoch: [6][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9313e+00 (1.6490e+00)	Acc@1  49.22 ( 53.82)	Acc@5  75.78 ( 84.20)
Epoch: [6][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6249e+00 (1.6477e+00)	Acc@1  58.59 ( 53.87)	Acc@5  85.16 ( 84.22)
Epoch: [6][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6029e+00 (1.6466e+00)	Acc@1  54.69 ( 53.92)	Acc@5  86.72 ( 84.26)
Epoch: [6][290/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7163e+00 (1.6462e+00)	Acc@1  50.00 ( 53.88)	Acc@5  84.38 ( 84.29)
Epoch: [6][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6115e+00 (1.6450e+00)	Acc@1  46.09 ( 53.88)	Acc@5  85.94 ( 84.33)
Epoch: [6][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6747e+00 (1.6470e+00)	Acc@1  57.03 ( 53.86)	Acc@5  82.81 ( 84.28)
Epoch: [6][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8336e+00 (1.6475e+00)	Acc@1  46.88 ( 53.85)	Acc@5  79.69 ( 84.23)
Epoch: [6][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5415e+00 (1.6451e+00)	Acc@1  56.25 ( 53.91)	Acc@5  82.81 ( 84.29)
Epoch: [6][340/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4358e+00 (1.6435e+00)	Acc@1  57.81 ( 53.96)	Acc@5  89.84 ( 84.29)
Epoch: [6][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7378e+00 (1.6422e+00)	Acc@1  53.91 ( 54.00)	Acc@5  84.38 ( 84.35)
Epoch: [6][360/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7870e+00 (1.6442e+00)	Acc@1  51.56 ( 53.92)	Acc@5  85.16 ( 84.33)
Epoch: [6][370/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6263e+00 (1.6437e+00)	Acc@1  57.03 ( 53.93)	Acc@5  79.69 ( 84.34)
Epoch: [6][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7359e+00 (1.6451e+00)	Acc@1  53.12 ( 53.93)	Acc@5  84.38 ( 84.31)
Epoch: [6][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6768e+00 (1.6445e+00)	Acc@1  51.25 ( 53.96)	Acc@5  83.75 ( 84.28)
## e[6] optimizer.zero_grad (sum) time: 0.6669926643371582
## e[6]       loss.backward (sum) time: 13.88921308517456
## e[6]      optimizer.step (sum) time: 14.756752729415894
## epoch[6] training(only) time: 53.960984230041504
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 1.9024e+00 (1.9024e+00)	Acc@1  55.00 ( 55.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.049 ( 0.063)	Loss 1.7956e+00 (1.8031e+00)	Acc@1  52.00 ( 52.91)	Acc@5  81.00 ( 81.18)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.5863e+00 (1.7504e+00)	Acc@1  51.00 ( 52.81)	Acc@5  84.00 ( 82.29)
Test: [ 30/100]	Time  0.053 ( 0.053)	Loss 1.8832e+00 (1.7469e+00)	Acc@1  45.00 ( 52.13)	Acc@5  84.00 ( 82.74)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.7852e+00 (1.7365e+00)	Acc@1  58.00 ( 52.24)	Acc@5  82.00 ( 82.93)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.6066e+00 (1.7546e+00)	Acc@1  55.00 ( 51.69)	Acc@5  85.00 ( 82.63)
Test: [ 60/100]	Time  0.050 ( 0.050)	Loss 1.5151e+00 (1.7453e+00)	Acc@1  54.00 ( 51.38)	Acc@5  89.00 ( 82.64)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.8640e+00 (1.7479e+00)	Acc@1  53.00 ( 51.51)	Acc@5  79.00 ( 82.49)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.7500e+00 (1.7603e+00)	Acc@1  50.00 ( 51.28)	Acc@5  81.00 ( 82.42)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.9964e+00 (1.7493e+00)	Acc@1  44.00 ( 51.57)	Acc@5  77.00 ( 82.69)
 * Acc@1 51.560 Acc@5 82.730
### epoch[6] execution time: 58.962488651275635
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.303 ( 0.303)	Data  0.158 ( 0.158)	Loss 1.2330e+00 (1.2330e+00)	Acc@1  60.94 ( 60.94)	Acc@5  92.19 ( 92.19)
Epoch: [7][ 10/391]	Time  0.135 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.5059e+00 (1.4752e+00)	Acc@1  58.59 ( 57.95)	Acc@5  85.94 ( 86.72)
Epoch: [7][ 20/391]	Time  0.134 ( 0.145)	Data  0.001 ( 0.009)	Loss 1.5402e+00 (1.4770e+00)	Acc@1  57.03 ( 58.18)	Acc@5  85.94 ( 86.90)
Epoch: [7][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.2911e+00 (1.4915e+00)	Acc@1  57.81 ( 57.89)	Acc@5  91.41 ( 86.82)
Epoch: [7][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.4711e+00 (1.4943e+00)	Acc@1  55.47 ( 57.13)	Acc@5  85.94 ( 86.85)
Epoch: [7][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3971e+00 (1.4959e+00)	Acc@1  60.16 ( 57.40)	Acc@5  85.94 ( 86.99)
Epoch: [7][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.4738e+00 (1.5039e+00)	Acc@1  57.81 ( 57.27)	Acc@5  87.50 ( 86.95)
Epoch: [7][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5587e+00 (1.5053e+00)	Acc@1  53.12 ( 57.17)	Acc@5  87.50 ( 86.80)
Epoch: [7][ 80/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5080e+00 (1.5055e+00)	Acc@1  58.59 ( 56.92)	Acc@5  87.50 ( 86.68)
Epoch: [7][ 90/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.4826e+00 (1.5036e+00)	Acc@1  56.25 ( 57.01)	Acc@5  89.06 ( 86.73)
Epoch: [7][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.4268e+00 (1.5008e+00)	Acc@1  59.38 ( 57.22)	Acc@5  86.72 ( 86.76)
Epoch: [7][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.4646e+00 (1.4983e+00)	Acc@1  57.03 ( 57.18)	Acc@5  85.16 ( 86.78)
Epoch: [7][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5563e+00 (1.4962e+00)	Acc@1  57.03 ( 57.28)	Acc@5  84.38 ( 86.77)
Epoch: [7][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5256e+00 (1.4972e+00)	Acc@1  59.38 ( 57.37)	Acc@5  83.59 ( 86.70)
Epoch: [7][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5745e+00 (1.4997e+00)	Acc@1  58.59 ( 57.31)	Acc@5  81.25 ( 86.64)
Epoch: [7][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5333e+00 (1.5035e+00)	Acc@1  58.59 ( 57.33)	Acc@5  85.16 ( 86.46)
Epoch: [7][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5441e+00 (1.5021e+00)	Acc@1  57.81 ( 57.49)	Acc@5  85.16 ( 86.46)
Epoch: [7][170/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5848e+00 (1.5033e+00)	Acc@1  56.25 ( 57.32)	Acc@5  89.84 ( 86.49)
Epoch: [7][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4164e+00 (1.5061e+00)	Acc@1  61.72 ( 57.32)	Acc@5  86.72 ( 86.42)
Epoch: [7][190/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7373e+00 (1.5087e+00)	Acc@1  53.12 ( 57.25)	Acc@5  82.03 ( 86.33)
Epoch: [7][200/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4189e+00 (1.5056e+00)	Acc@1  57.81 ( 57.34)	Acc@5  89.84 ( 86.42)
Epoch: [7][210/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6310e+00 (1.5071e+00)	Acc@1  53.91 ( 57.31)	Acc@5  83.59 ( 86.38)
Epoch: [7][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5775e+00 (1.5039e+00)	Acc@1  57.81 ( 57.38)	Acc@5  81.25 ( 86.37)
Epoch: [7][230/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5134e+00 (1.5037e+00)	Acc@1  56.25 ( 57.40)	Acc@5  83.59 ( 86.35)
Epoch: [7][240/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6793e+00 (1.5049e+00)	Acc@1  50.00 ( 57.41)	Acc@5  87.50 ( 86.33)
Epoch: [7][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5080e+00 (1.5083e+00)	Acc@1  62.50 ( 57.33)	Acc@5  83.59 ( 86.30)
Epoch: [7][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5547e+00 (1.5140e+00)	Acc@1  53.91 ( 57.18)	Acc@5  84.38 ( 86.24)
Epoch: [7][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6121e+00 (1.5187e+00)	Acc@1  54.69 ( 57.01)	Acc@5  84.38 ( 86.12)
Epoch: [7][280/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5112e+00 (1.5221e+00)	Acc@1  55.47 ( 56.93)	Acc@5  85.94 ( 86.08)
Epoch: [7][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5342e+00 (1.5218e+00)	Acc@1  54.69 ( 56.88)	Acc@5  89.06 ( 86.08)
Epoch: [7][300/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4036e+00 (1.5218e+00)	Acc@1  66.41 ( 56.91)	Acc@5  87.50 ( 86.05)
Epoch: [7][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5397e+00 (1.5214e+00)	Acc@1  53.12 ( 56.95)	Acc@5  87.50 ( 86.07)
Epoch: [7][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3713e+00 (1.5198e+00)	Acc@1  61.72 ( 56.99)	Acc@5  89.84 ( 86.12)
Epoch: [7][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6674e+00 (1.5194e+00)	Acc@1  50.78 ( 56.99)	Acc@5  85.16 ( 86.12)
Epoch: [7][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5708e+00 (1.5189e+00)	Acc@1  55.47 ( 56.97)	Acc@5  85.16 ( 86.16)
Epoch: [7][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6583e+00 (1.5193e+00)	Acc@1  57.81 ( 57.02)	Acc@5  84.38 ( 86.17)
Epoch: [7][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7527e+00 (1.5214e+00)	Acc@1  53.12 ( 56.99)	Acc@5  82.81 ( 86.10)
Epoch: [7][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3677e+00 (1.5220e+00)	Acc@1  61.72 ( 56.99)	Acc@5  86.72 ( 86.09)
Epoch: [7][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6252e+00 (1.5230e+00)	Acc@1  53.91 ( 56.98)	Acc@5  85.94 ( 86.07)
Epoch: [7][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3170e+00 (1.5236e+00)	Acc@1  62.50 ( 56.92)	Acc@5  92.50 ( 86.07)
## e[7] optimizer.zero_grad (sum) time: 0.6669931411743164
## e[7]       loss.backward (sum) time: 13.845831871032715
## e[7]      optimizer.step (sum) time: 14.768070459365845
## epoch[7] training(only) time: 53.990888357162476
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.6959e+00 (1.6959e+00)	Acc@1  62.00 ( 62.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.6646e+00 (1.6838e+00)	Acc@1  58.00 ( 56.09)	Acc@5  87.00 ( 84.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.4786e+00 (1.6384e+00)	Acc@1  57.00 ( 55.48)	Acc@5  89.00 ( 84.67)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 2.0337e+00 (1.6415e+00)	Acc@1  45.00 ( 55.00)	Acc@5  76.00 ( 84.39)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.6395e+00 (1.6321e+00)	Acc@1  51.00 ( 54.66)	Acc@5  84.00 ( 84.71)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4115e+00 (1.6509e+00)	Acc@1  63.00 ( 54.53)	Acc@5  87.00 ( 84.18)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.3890e+00 (1.6398e+00)	Acc@1  59.00 ( 54.54)	Acc@5  91.00 ( 84.10)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.8456e+00 (1.6475e+00)	Acc@1  50.00 ( 54.51)	Acc@5  80.00 ( 83.92)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.6086e+00 (1.6576e+00)	Acc@1  58.00 ( 54.11)	Acc@5  81.00 ( 83.91)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.7450e+00 (1.6438e+00)	Acc@1  47.00 ( 54.41)	Acc@5  79.00 ( 84.10)
 * Acc@1 54.530 Acc@5 84.080
### epoch[7] execution time: 58.94761824607849
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.300 ( 0.300)	Data  0.155 ( 0.155)	Loss 1.2620e+00 (1.2620e+00)	Acc@1  63.28 ( 63.28)	Acc@5  89.84 ( 89.84)
Epoch: [8][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.3255e+00 (1.3585e+00)	Acc@1  58.59 ( 60.94)	Acc@5  85.94 ( 87.64)
Epoch: [8][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.2227e+00 (1.3514e+00)	Acc@1  64.06 ( 61.16)	Acc@5  89.06 ( 88.06)
Epoch: [8][ 30/391]	Time  0.148 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.5563e+00 (1.3815e+00)	Acc@1  58.59 ( 60.51)	Acc@5  82.81 ( 87.95)
Epoch: [8][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.3725e+00 (1.3924e+00)	Acc@1  61.72 ( 59.91)	Acc@5  85.94 ( 88.03)
Epoch: [8][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3922e+00 (1.3861e+00)	Acc@1  64.06 ( 60.26)	Acc@5  85.16 ( 88.11)
Epoch: [8][ 60/391]	Time  0.146 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.3425e+00 (1.3896e+00)	Acc@1  61.72 ( 60.28)	Acc@5  92.19 ( 88.06)
Epoch: [8][ 70/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5909e+00 (1.3896e+00)	Acc@1  57.03 ( 60.29)	Acc@5  85.16 ( 88.08)
Epoch: [8][ 80/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1782e+00 (1.3872e+00)	Acc@1  67.19 ( 60.21)	Acc@5  95.31 ( 88.24)
Epoch: [8][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.1506e+00 (1.3896e+00)	Acc@1  61.72 ( 60.08)	Acc@5  93.75 ( 88.28)
Epoch: [8][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3329e+00 (1.3932e+00)	Acc@1  57.81 ( 59.96)	Acc@5  90.62 ( 88.27)
Epoch: [8][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3603e+00 (1.4001e+00)	Acc@1  60.16 ( 59.66)	Acc@5  89.06 ( 88.16)
Epoch: [8][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4478e+00 (1.4003e+00)	Acc@1  54.69 ( 59.70)	Acc@5  87.50 ( 88.13)
Epoch: [8][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5589e+00 (1.4007e+00)	Acc@1  63.28 ( 59.70)	Acc@5  82.81 ( 88.13)
Epoch: [8][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4426e+00 (1.3968e+00)	Acc@1  59.38 ( 59.77)	Acc@5  82.03 ( 88.06)
Epoch: [8][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5902e+00 (1.3997e+00)	Acc@1  54.69 ( 59.77)	Acc@5  89.06 ( 88.03)
Epoch: [8][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3748e+00 (1.4005e+00)	Acc@1  60.94 ( 59.75)	Acc@5  88.28 ( 88.06)
Epoch: [8][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5071e+00 (1.4013e+00)	Acc@1  57.81 ( 59.77)	Acc@5  86.72 ( 88.02)
Epoch: [8][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4867e+00 (1.4028e+00)	Acc@1  60.94 ( 59.75)	Acc@5  83.59 ( 87.97)
Epoch: [8][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3364e+00 (1.4029e+00)	Acc@1  68.75 ( 59.81)	Acc@5  89.84 ( 87.96)
Epoch: [8][200/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3150e+00 (1.4010e+00)	Acc@1  60.94 ( 59.88)	Acc@5  86.72 ( 87.98)
Epoch: [8][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2077e+00 (1.3998e+00)	Acc@1  63.28 ( 59.92)	Acc@5  93.75 ( 87.97)
Epoch: [8][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0783e+00 (1.4020e+00)	Acc@1  71.09 ( 59.91)	Acc@5  90.62 ( 87.94)
Epoch: [8][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2617e+00 (1.4017e+00)	Acc@1  61.72 ( 59.86)	Acc@5  87.50 ( 87.93)
Epoch: [8][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4897e+00 (1.4031e+00)	Acc@1  60.16 ( 59.85)	Acc@5  87.50 ( 87.91)
Epoch: [8][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4018e+00 (1.4007e+00)	Acc@1  58.59 ( 59.89)	Acc@5  87.50 ( 87.94)
Epoch: [8][260/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3719e+00 (1.4032e+00)	Acc@1  60.16 ( 59.86)	Acc@5  85.16 ( 87.89)
Epoch: [8][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3042e+00 (1.4022e+00)	Acc@1  63.28 ( 59.87)	Acc@5  89.06 ( 87.89)
Epoch: [8][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2905e+00 (1.4041e+00)	Acc@1  64.06 ( 59.84)	Acc@5  91.41 ( 87.87)
Epoch: [8][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6038e+00 (1.4059e+00)	Acc@1  57.03 ( 59.82)	Acc@5  83.59 ( 87.84)
Epoch: [8][300/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4910e+00 (1.4073e+00)	Acc@1  57.03 ( 59.80)	Acc@5  82.81 ( 87.80)
Epoch: [8][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4065e+00 (1.4085e+00)	Acc@1  57.03 ( 59.76)	Acc@5  85.16 ( 87.78)
Epoch: [8][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4690e+00 (1.4077e+00)	Acc@1  56.25 ( 59.79)	Acc@5  86.72 ( 87.79)
Epoch: [8][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5758e+00 (1.4095e+00)	Acc@1  58.59 ( 59.79)	Acc@5  81.25 ( 87.74)
Epoch: [8][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3573e+00 (1.4101e+00)	Acc@1  58.59 ( 59.75)	Acc@5  89.84 ( 87.77)
Epoch: [8][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2706e+00 (1.4115e+00)	Acc@1  64.06 ( 59.67)	Acc@5  91.41 ( 87.77)
Epoch: [8][360/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2337e+00 (1.4123e+00)	Acc@1  61.72 ( 59.61)	Acc@5  92.97 ( 87.77)
Epoch: [8][370/391]	Time  0.150 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2603e+00 (1.4124e+00)	Acc@1  64.06 ( 59.61)	Acc@5  89.84 ( 87.77)
Epoch: [8][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4754e+00 (1.4110e+00)	Acc@1  57.03 ( 59.59)	Acc@5  86.72 ( 87.79)
Epoch: [8][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8587e+00 (1.4116e+00)	Acc@1  51.25 ( 59.57)	Acc@5  80.00 ( 87.80)
## e[8] optimizer.zero_grad (sum) time: 0.6697099208831787
## e[8]       loss.backward (sum) time: 13.786385774612427
## e[8]      optimizer.step (sum) time: 14.78341794013977
## epoch[8] training(only) time: 53.98164367675781
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.7817e+00 (1.7817e+00)	Acc@1  60.00 ( 60.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.059 ( 0.063)	Loss 1.8329e+00 (1.6765e+00)	Acc@1  48.00 ( 54.91)	Acc@5  87.00 ( 84.36)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.4980e+00 (1.6780e+00)	Acc@1  56.00 ( 54.52)	Acc@5  84.00 ( 84.24)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.8697e+00 (1.6773e+00)	Acc@1  45.00 ( 54.29)	Acc@5  82.00 ( 83.81)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.7607e+00 (1.6890e+00)	Acc@1  56.00 ( 54.34)	Acc@5  84.00 ( 83.54)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.5075e+00 (1.7007e+00)	Acc@1  62.00 ( 54.43)	Acc@5  87.00 ( 83.45)
Test: [ 60/100]	Time  0.055 ( 0.050)	Loss 1.5503e+00 (1.6874e+00)	Acc@1  55.00 ( 54.59)	Acc@5  86.00 ( 83.52)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.8534e+00 (1.6927e+00)	Acc@1  51.00 ( 54.75)	Acc@5  81.00 ( 83.37)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.7262e+00 (1.6947e+00)	Acc@1  56.00 ( 54.64)	Acc@5  81.00 ( 83.30)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7113e+00 (1.6792e+00)	Acc@1  54.00 ( 54.80)	Acc@5  85.00 ( 83.62)
 * Acc@1 54.800 Acc@5 83.630
### epoch[8] execution time: 58.96834063529968
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.297 ( 0.297)	Data  0.151 ( 0.151)	Loss 1.2510e+00 (1.2510e+00)	Acc@1  67.97 ( 67.97)	Acc@5  92.19 ( 92.19)
Epoch: [9][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.015)	Loss 1.3656e+00 (1.3063e+00)	Acc@1  62.50 ( 62.93)	Acc@5  85.94 ( 89.91)
Epoch: [9][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.008)	Loss 1.3548e+00 (1.2782e+00)	Acc@1  60.16 ( 63.06)	Acc@5  87.50 ( 90.03)
Epoch: [9][ 30/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.006)	Loss 1.3486e+00 (1.2677e+00)	Acc@1  60.94 ( 63.28)	Acc@5  89.06 ( 89.97)
Epoch: [9][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.3287e+00 (1.2763e+00)	Acc@1  64.06 ( 63.19)	Acc@5  87.50 ( 89.65)
Epoch: [9][ 50/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.2485e+00 (1.2846e+00)	Acc@1  63.28 ( 63.07)	Acc@5  89.06 ( 89.63)
Epoch: [9][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.3989e+00 (1.2877e+00)	Acc@1  61.72 ( 62.95)	Acc@5  86.72 ( 89.59)
Epoch: [9][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5009e+00 (1.2952e+00)	Acc@1  59.38 ( 62.81)	Acc@5  85.94 ( 89.51)
Epoch: [9][ 80/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2983e+00 (1.2942e+00)	Acc@1  59.38 ( 62.80)	Acc@5  88.28 ( 89.32)
Epoch: [9][ 90/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3132e+00 (1.2939e+00)	Acc@1  60.16 ( 62.92)	Acc@5  89.84 ( 89.37)
Epoch: [9][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.4623e+00 (1.2957e+00)	Acc@1  61.72 ( 62.98)	Acc@5  85.94 ( 89.33)
Epoch: [9][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.4772e+00 (1.2974e+00)	Acc@1  59.38 ( 62.85)	Acc@5  85.94 ( 89.37)
Epoch: [9][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4694e+00 (1.2991e+00)	Acc@1  56.25 ( 62.74)	Acc@5  89.84 ( 89.42)
Epoch: [9][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3086e+00 (1.2928e+00)	Acc@1  64.06 ( 62.88)	Acc@5  88.28 ( 89.55)
Epoch: [9][140/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0547e+00 (1.2974e+00)	Acc@1  65.62 ( 62.77)	Acc@5  91.41 ( 89.55)
Epoch: [9][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2658e+00 (1.2941e+00)	Acc@1  63.28 ( 62.87)	Acc@5  91.41 ( 89.64)
Epoch: [9][160/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2872e+00 (1.2955e+00)	Acc@1  60.94 ( 62.75)	Acc@5  91.41 ( 89.57)
Epoch: [9][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4674e+00 (1.2969e+00)	Acc@1  53.12 ( 62.69)	Acc@5  84.38 ( 89.55)
Epoch: [9][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2699e+00 (1.3005e+00)	Acc@1  64.84 ( 62.59)	Acc@5  89.06 ( 89.51)
Epoch: [9][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4382e+00 (1.3034e+00)	Acc@1  60.16 ( 62.50)	Acc@5  83.59 ( 89.39)
Epoch: [9][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3102e+00 (1.3089e+00)	Acc@1  60.94 ( 62.35)	Acc@5  91.41 ( 89.35)
Epoch: [9][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3358e+00 (1.3105e+00)	Acc@1  59.38 ( 62.31)	Acc@5  89.06 ( 89.31)
Epoch: [9][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2017e+00 (1.3134e+00)	Acc@1  67.97 ( 62.19)	Acc@5  89.84 ( 89.27)
Epoch: [9][230/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3854e+00 (1.3154e+00)	Acc@1  60.94 ( 62.18)	Acc@5  85.94 ( 89.21)
Epoch: [9][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2862e+00 (1.3196e+00)	Acc@1  60.94 ( 62.05)	Acc@5  87.50 ( 89.15)
Epoch: [9][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1205e+00 (1.3168e+00)	Acc@1  69.53 ( 62.14)	Acc@5  92.19 ( 89.21)
Epoch: [9][260/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2817e+00 (1.3178e+00)	Acc@1  67.19 ( 62.09)	Acc@5  89.06 ( 89.17)
Epoch: [9][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2375e+00 (1.3179e+00)	Acc@1  66.41 ( 62.06)	Acc@5  89.84 ( 89.16)
Epoch: [9][280/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2096e+00 (1.3192e+00)	Acc@1  64.06 ( 62.03)	Acc@5  92.19 ( 89.13)
Epoch: [9][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0689e+00 (1.3172e+00)	Acc@1  70.31 ( 62.09)	Acc@5  92.97 ( 89.15)
Epoch: [9][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2665e+00 (1.3178e+00)	Acc@1  63.28 ( 62.06)	Acc@5  89.06 ( 89.15)
Epoch: [9][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3174e+00 (1.3183e+00)	Acc@1  61.72 ( 62.02)	Acc@5  88.28 ( 89.13)
Epoch: [9][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8142e+00 (1.3205e+00)	Acc@1  47.66 ( 61.96)	Acc@5  85.16 ( 89.12)
Epoch: [9][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3202e+00 (1.3195e+00)	Acc@1  61.72 ( 62.00)	Acc@5  92.97 ( 89.12)
Epoch: [9][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3428e+00 (1.3217e+00)	Acc@1  60.16 ( 61.93)	Acc@5  91.41 ( 89.13)
Epoch: [9][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3358e+00 (1.3207e+00)	Acc@1  59.38 ( 61.98)	Acc@5  86.72 ( 89.10)
Epoch: [9][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3516e+00 (1.3201e+00)	Acc@1  60.94 ( 61.99)	Acc@5  89.06 ( 89.12)
Epoch: [9][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4238e+00 (1.3190e+00)	Acc@1  58.59 ( 62.03)	Acc@5  89.84 ( 89.16)
Epoch: [9][380/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5438e+00 (1.3200e+00)	Acc@1  60.94 ( 62.03)	Acc@5  82.81 ( 89.12)
Epoch: [9][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3919e+00 (1.3216e+00)	Acc@1  57.50 ( 61.98)	Acc@5  85.00 ( 89.15)
## e[9] optimizer.zero_grad (sum) time: 0.6723747253417969
## e[9]       loss.backward (sum) time: 13.820499658584595
## e[9]      optimizer.step (sum) time: 14.773873805999756
## epoch[9] training(only) time: 53.9499249458313
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.6098e+00 (1.6098e+00)	Acc@1  57.00 ( 57.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.5725e+00 (1.6208e+00)	Acc@1  55.00 ( 56.27)	Acc@5  87.00 ( 84.09)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 1.3127e+00 (1.5497e+00)	Acc@1  63.00 ( 57.71)	Acc@5  89.00 ( 85.76)
Test: [ 30/100]	Time  0.045 ( 0.053)	Loss 1.5632e+00 (1.5550e+00)	Acc@1  55.00 ( 57.26)	Acc@5  91.00 ( 85.84)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.4963e+00 (1.5406e+00)	Acc@1  59.00 ( 57.76)	Acc@5  84.00 ( 86.15)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.3723e+00 (1.5474e+00)	Acc@1  55.00 ( 57.59)	Acc@5  88.00 ( 86.06)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.2889e+00 (1.5414e+00)	Acc@1  56.00 ( 57.49)	Acc@5  89.00 ( 86.18)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 1.4262e+00 (1.5327e+00)	Acc@1  60.00 ( 57.72)	Acc@5  86.00 ( 86.15)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4399e+00 (1.5348e+00)	Acc@1  62.00 ( 57.81)	Acc@5  84.00 ( 86.12)
Test: [ 90/100]	Time  0.051 ( 0.049)	Loss 1.6990e+00 (1.5213e+00)	Acc@1  55.00 ( 58.03)	Acc@5  84.00 ( 86.30)
 * Acc@1 58.080 Acc@5 86.380
### epoch[9] execution time: 58.95571732521057
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.307 ( 0.307)	Data  0.132 ( 0.132)	Loss 1.1274e+00 (1.1274e+00)	Acc@1  61.72 ( 61.72)	Acc@5  91.41 ( 91.41)
Epoch: [10][ 10/391]	Time  0.136 ( 0.153)	Data  0.001 ( 0.013)	Loss 1.2103e+00 (1.2026e+00)	Acc@1  68.75 ( 63.92)	Acc@5  88.28 ( 91.83)
Epoch: [10][ 20/391]	Time  0.136 ( 0.146)	Data  0.001 ( 0.007)	Loss 1.2445e+00 (1.1897e+00)	Acc@1  65.62 ( 65.22)	Acc@5  89.06 ( 91.22)
Epoch: [10][ 30/391]	Time  0.143 ( 0.143)	Data  0.001 ( 0.005)	Loss 1.1923e+00 (1.2138e+00)	Acc@1  57.81 ( 64.04)	Acc@5  93.75 ( 90.90)
Epoch: [10][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.2487e+00 (1.2025e+00)	Acc@1  57.81 ( 64.20)	Acc@5  89.84 ( 90.87)
Epoch: [10][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2316e+00 (1.2007e+00)	Acc@1  60.94 ( 64.31)	Acc@5  91.41 ( 90.76)
Epoch: [10][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.0450e+00 (1.2047e+00)	Acc@1  65.62 ( 64.46)	Acc@5  90.62 ( 90.83)
Epoch: [10][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1746e+00 (1.1973e+00)	Acc@1  63.28 ( 64.51)	Acc@5  91.41 ( 91.00)
Epoch: [10][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.4475e-01 (1.1826e+00)	Acc@1  75.00 ( 64.81)	Acc@5  94.53 ( 91.21)
Epoch: [10][ 90/391]	Time  0.147 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2396e+00 (1.1899e+00)	Acc@1  62.50 ( 64.56)	Acc@5  91.41 ( 91.08)
Epoch: [10][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.0143e+00 (1.1973e+00)	Acc@1  70.31 ( 64.47)	Acc@5  92.19 ( 91.07)
Epoch: [10][110/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.3069e+00 (1.2044e+00)	Acc@1  63.28 ( 64.28)	Acc@5  89.06 ( 90.92)
Epoch: [10][120/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2921e+00 (1.2041e+00)	Acc@1  67.97 ( 64.33)	Acc@5  86.72 ( 90.86)
Epoch: [10][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4649e+00 (1.2118e+00)	Acc@1  57.81 ( 64.24)	Acc@5  90.62 ( 90.82)
Epoch: [10][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1959e+00 (1.2145e+00)	Acc@1  65.62 ( 64.17)	Acc@5  89.84 ( 90.81)
Epoch: [10][150/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1642e+00 (1.2141e+00)	Acc@1  65.62 ( 64.23)	Acc@5  89.84 ( 90.78)
Epoch: [10][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2865e+00 (1.2143e+00)	Acc@1  63.28 ( 64.30)	Acc@5  92.97 ( 90.84)
Epoch: [10][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3668e+00 (1.2140e+00)	Acc@1  61.72 ( 64.38)	Acc@5  89.06 ( 90.85)
Epoch: [10][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3545e+00 (1.2180e+00)	Acc@1  60.16 ( 64.19)	Acc@5  86.72 ( 90.81)
Epoch: [10][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0137e+00 (1.2194e+00)	Acc@1  65.62 ( 64.13)	Acc@5  96.09 ( 90.79)
Epoch: [10][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2401e+00 (1.2160e+00)	Acc@1  66.41 ( 64.25)	Acc@5  87.50 ( 90.80)
Epoch: [10][210/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3591e+00 (1.2216e+00)	Acc@1  63.28 ( 64.15)	Acc@5  87.50 ( 90.72)
Epoch: [10][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3130e+00 (1.2243e+00)	Acc@1  59.38 ( 64.06)	Acc@5  91.41 ( 90.67)
Epoch: [10][230/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4689e+00 (1.2282e+00)	Acc@1  58.59 ( 64.00)	Acc@5  85.16 ( 90.55)
Epoch: [10][240/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3801e+00 (1.2303e+00)	Acc@1  60.16 ( 63.94)	Acc@5  88.28 ( 90.51)
Epoch: [10][250/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1042e+00 (1.2319e+00)	Acc@1  71.09 ( 63.95)	Acc@5  92.97 ( 90.48)
Epoch: [10][260/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2948e+00 (1.2338e+00)	Acc@1  59.38 ( 63.85)	Acc@5  88.28 ( 90.44)
Epoch: [10][270/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0813e+00 (1.2328e+00)	Acc@1  71.09 ( 63.89)	Acc@5  93.75 ( 90.45)
Epoch: [10][280/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1735e+00 (1.2323e+00)	Acc@1  65.62 ( 63.89)	Acc@5  91.41 ( 90.48)
Epoch: [10][290/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (1.2327e+00)	Acc@1  67.97 ( 63.87)	Acc@5  89.84 ( 90.46)
Epoch: [10][300/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2855e+00 (1.2320e+00)	Acc@1  63.28 ( 63.90)	Acc@5  86.72 ( 90.44)
Epoch: [10][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1728e+00 (1.2326e+00)	Acc@1  67.19 ( 63.89)	Acc@5  92.19 ( 90.42)
Epoch: [10][320/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2374e+00 (1.2339e+00)	Acc@1  63.28 ( 63.87)	Acc@5  89.06 ( 90.38)
Epoch: [10][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1361e+00 (1.2358e+00)	Acc@1  65.62 ( 63.83)	Acc@5  93.75 ( 90.33)
Epoch: [10][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4329e+00 (1.2362e+00)	Acc@1  57.81 ( 63.78)	Acc@5  89.06 ( 90.38)
Epoch: [10][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2368e+00 (1.2376e+00)	Acc@1  60.94 ( 63.73)	Acc@5  91.41 ( 90.35)
Epoch: [10][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0729e+00 (1.2370e+00)	Acc@1  60.16 ( 63.72)	Acc@5  92.97 ( 90.37)
Epoch: [10][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1253e+00 (1.2365e+00)	Acc@1  62.50 ( 63.71)	Acc@5  89.06 ( 90.36)
Epoch: [10][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1504e+00 (1.2370e+00)	Acc@1  67.97 ( 63.69)	Acc@5  90.62 ( 90.35)
Epoch: [10][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1016e+00 (1.2375e+00)	Acc@1  67.50 ( 63.70)	Acc@5  93.75 ( 90.33)
## e[10] optimizer.zero_grad (sum) time: 0.6664917469024658
## e[10]       loss.backward (sum) time: 13.82429838180542
## e[10]      optimizer.step (sum) time: 14.791484117507935
## epoch[10] training(only) time: 54.11028289794922
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.5323e+00 (1.5323e+00)	Acc@1  59.00 ( 59.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.051 ( 0.061)	Loss 1.6051e+00 (1.5327e+00)	Acc@1  57.00 ( 58.82)	Acc@5  91.00 ( 87.18)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.2917e+00 (1.4962e+00)	Acc@1  64.00 ( 59.67)	Acc@5  87.00 ( 87.14)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 1.6831e+00 (1.4978e+00)	Acc@1  53.00 ( 59.23)	Acc@5  87.00 ( 86.97)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.4375e+00 (1.4939e+00)	Acc@1  65.00 ( 59.66)	Acc@5  84.00 ( 87.00)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.5551e+00 (1.5029e+00)	Acc@1  59.00 ( 59.75)	Acc@5  82.00 ( 86.43)
Test: [ 60/100]	Time  0.056 ( 0.050)	Loss 1.2837e+00 (1.4868e+00)	Acc@1  61.00 ( 59.82)	Acc@5  85.00 ( 86.61)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.5529e+00 (1.4894e+00)	Acc@1  61.00 ( 59.68)	Acc@5  89.00 ( 86.62)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.6246e+00 (1.4886e+00)	Acc@1  60.00 ( 59.67)	Acc@5  82.00 ( 86.64)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5634e+00 (1.4722e+00)	Acc@1  54.00 ( 59.99)	Acc@5  87.00 ( 86.82)
 * Acc@1 60.010 Acc@5 86.810
### epoch[10] execution time: 59.121562480926514
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.312 ( 0.312)	Data  0.153 ( 0.153)	Loss 8.7113e-01 (8.7113e-01)	Acc@1  78.91 ( 78.91)	Acc@5  92.97 ( 92.97)
Epoch: [11][ 10/391]	Time  0.135 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.0553e+00 (1.0824e+00)	Acc@1  67.19 ( 67.54)	Acc@5  92.97 ( 92.83)
Epoch: [11][ 20/391]	Time  0.138 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.2210e+00 (1.1010e+00)	Acc@1  64.06 ( 67.93)	Acc@5  89.06 ( 92.22)
Epoch: [11][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 9.2499e-01 (1.0985e+00)	Acc@1  66.41 ( 67.54)	Acc@5  97.66 ( 92.52)
Epoch: [11][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.0214e+00 (1.1169e+00)	Acc@1  73.44 ( 67.36)	Acc@5  93.75 ( 92.30)
Epoch: [11][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 8.9826e-01 (1.1190e+00)	Acc@1  74.22 ( 67.08)	Acc@5  93.75 ( 92.25)
Epoch: [11][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.1009e+00 (1.1200e+00)	Acc@1  71.88 ( 67.07)	Acc@5  93.75 ( 92.24)
Epoch: [11][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5003e+00 (1.1245e+00)	Acc@1  60.94 ( 66.88)	Acc@5  88.28 ( 92.30)
Epoch: [11][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2137e+00 (1.1220e+00)	Acc@1  65.62 ( 67.01)	Acc@5  89.06 ( 92.29)
Epoch: [11][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0939e+00 (1.1281e+00)	Acc@1  67.19 ( 67.02)	Acc@5  91.41 ( 92.09)
Epoch: [11][100/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2552e+00 (1.1255e+00)	Acc@1  63.28 ( 67.03)	Acc@5  89.84 ( 92.16)
Epoch: [11][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0515e+00 (1.1241e+00)	Acc@1  69.53 ( 67.14)	Acc@5  91.41 ( 92.12)
Epoch: [11][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3148e+00 (1.1308e+00)	Acc@1  61.72 ( 66.83)	Acc@5  90.62 ( 92.01)
Epoch: [11][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8895e-01 (1.1333e+00)	Acc@1  71.09 ( 66.79)	Acc@5  94.53 ( 91.89)
Epoch: [11][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3069e+00 (1.1343e+00)	Acc@1  60.94 ( 66.76)	Acc@5  89.06 ( 91.88)
Epoch: [11][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3353e+00 (1.1400e+00)	Acc@1  60.16 ( 66.62)	Acc@5  91.41 ( 91.81)
Epoch: [11][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1630e+00 (1.1420e+00)	Acc@1  64.84 ( 66.54)	Acc@5  93.75 ( 91.80)
Epoch: [11][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1650e+00 (1.1441e+00)	Acc@1  67.97 ( 66.51)	Acc@5  92.97 ( 91.74)
Epoch: [11][180/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.1307e-01 (1.1451e+00)	Acc@1  71.88 ( 66.52)	Acc@5  95.31 ( 91.73)
Epoch: [11][190/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5150e+00 (1.1501e+00)	Acc@1  51.56 ( 66.37)	Acc@5  87.50 ( 91.63)
Epoch: [11][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2244e+00 (1.1537e+00)	Acc@1  65.62 ( 66.29)	Acc@5  89.84 ( 91.56)
Epoch: [11][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1049e+00 (1.1523e+00)	Acc@1  68.75 ( 66.32)	Acc@5  90.62 ( 91.60)
Epoch: [11][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0418e+00 (1.1545e+00)	Acc@1  65.62 ( 66.17)	Acc@5  94.53 ( 91.56)
Epoch: [11][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4228e+00 (1.1597e+00)	Acc@1  57.81 ( 66.02)	Acc@5  89.84 ( 91.46)
Epoch: [11][240/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2230e+00 (1.1634e+00)	Acc@1  60.16 ( 65.88)	Acc@5  89.84 ( 91.39)
Epoch: [11][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1056e+00 (1.1660e+00)	Acc@1  65.62 ( 65.80)	Acc@5  93.75 ( 91.39)
Epoch: [11][260/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1672e+00 (1.1682e+00)	Acc@1  66.41 ( 65.76)	Acc@5  89.84 ( 91.34)
Epoch: [11][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2754e+00 (1.1694e+00)	Acc@1  63.28 ( 65.73)	Acc@5  90.62 ( 91.31)
Epoch: [11][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1148e+00 (1.1679e+00)	Acc@1  70.31 ( 65.78)	Acc@5  91.41 ( 91.34)
Epoch: [11][290/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2270e+00 (1.1677e+00)	Acc@1  63.28 ( 65.81)	Acc@5  92.97 ( 91.32)
Epoch: [11][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2140e+00 (1.1663e+00)	Acc@1  66.41 ( 65.92)	Acc@5  89.06 ( 91.32)
Epoch: [11][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0280e+00 (1.1667e+00)	Acc@1  67.19 ( 65.88)	Acc@5  89.84 ( 91.33)
Epoch: [11][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2335e+00 (1.1671e+00)	Acc@1  62.50 ( 65.89)	Acc@5  92.97 ( 91.30)
Epoch: [11][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2271e+00 (1.1669e+00)	Acc@1  59.38 ( 65.84)	Acc@5  91.41 ( 91.30)
Epoch: [11][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4740e+00 (1.1698e+00)	Acc@1  58.59 ( 65.80)	Acc@5  84.38 ( 91.23)
Epoch: [11][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2815e+00 (1.1704e+00)	Acc@1  64.06 ( 65.77)	Acc@5  89.06 ( 91.22)
Epoch: [11][360/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1675e+00 (1.1702e+00)	Acc@1  62.50 ( 65.76)	Acc@5  92.19 ( 91.22)
Epoch: [11][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0271e+00 (1.1691e+00)	Acc@1  70.31 ( 65.79)	Acc@5  92.19 ( 91.24)
Epoch: [11][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2011e+00 (1.1686e+00)	Acc@1  67.19 ( 65.82)	Acc@5  89.84 ( 91.25)
Epoch: [11][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3034e+00 (1.1703e+00)	Acc@1  61.25 ( 65.78)	Acc@5  88.75 ( 91.24)
## e[11] optimizer.zero_grad (sum) time: 0.6682515144348145
## e[11]       loss.backward (sum) time: 13.83689546585083
## e[11]      optimizer.step (sum) time: 14.765812635421753
## epoch[11] training(only) time: 54.04916262626648
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.4626e+00 (1.4626e+00)	Acc@1  60.00 ( 60.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.7194e+00 (1.6220e+00)	Acc@1  61.00 ( 57.91)	Acc@5  87.00 ( 84.82)
Test: [ 20/100]	Time  0.050 ( 0.056)	Loss 1.3897e+00 (1.6024e+00)	Acc@1  65.00 ( 58.29)	Acc@5  89.00 ( 85.33)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.7889e+00 (1.5871e+00)	Acc@1  46.00 ( 58.13)	Acc@5  88.00 ( 85.84)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.6505e+00 (1.5862e+00)	Acc@1  63.00 ( 58.56)	Acc@5  83.00 ( 85.85)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.3840e+00 (1.5788e+00)	Acc@1  62.00 ( 58.57)	Acc@5  89.00 ( 85.69)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.6141e+00 (1.5708e+00)	Acc@1  58.00 ( 58.48)	Acc@5  86.00 ( 85.84)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.5018e+00 (1.5688e+00)	Acc@1  61.00 ( 58.85)	Acc@5  90.00 ( 85.79)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.3876e+00 (1.5755e+00)	Acc@1  62.00 ( 58.72)	Acc@5  90.00 ( 85.80)
Test: [ 90/100]	Time  0.050 ( 0.050)	Loss 1.7864e+00 (1.5637e+00)	Acc@1  53.00 ( 58.90)	Acc@5  82.00 ( 86.02)
 * Acc@1 59.110 Acc@5 86.020
### epoch[11] execution time: 59.07800531387329
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.305 ( 0.305)	Data  0.160 ( 0.160)	Loss 9.2673e-01 (9.2673e-01)	Acc@1  67.97 ( 67.97)	Acc@5  96.09 ( 96.09)
Epoch: [12][ 10/391]	Time  0.134 ( 0.153)	Data  0.001 ( 0.016)	Loss 1.3196e+00 (1.0586e+00)	Acc@1  65.62 ( 68.32)	Acc@5  90.62 ( 93.18)
Epoch: [12][ 20/391]	Time  0.134 ( 0.145)	Data  0.001 ( 0.009)	Loss 9.6841e-01 (1.0764e+00)	Acc@1  67.19 ( 67.97)	Acc@5  94.53 ( 92.67)
Epoch: [12][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.2068e+00 (1.0830e+00)	Acc@1  66.41 ( 68.30)	Acc@5  92.19 ( 92.67)
Epoch: [12][ 40/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.005)	Loss 9.8019e-01 (1.0620e+00)	Acc@1  71.88 ( 68.69)	Acc@5  92.97 ( 93.01)
Epoch: [12][ 50/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 9.8172e-01 (1.0529e+00)	Acc@1  73.44 ( 68.81)	Acc@5  93.75 ( 93.11)
Epoch: [12][ 60/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.1816e+00 (1.0537e+00)	Acc@1  62.50 ( 68.92)	Acc@5  92.97 ( 92.99)
Epoch: [12][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0900e+00 (1.0495e+00)	Acc@1  63.28 ( 68.92)	Acc@5  92.19 ( 93.09)
Epoch: [12][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1838e+00 (1.0510e+00)	Acc@1  64.84 ( 68.87)	Acc@5  90.62 ( 93.00)
Epoch: [12][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.1274e+00 (1.0523e+00)	Acc@1  63.28 ( 68.63)	Acc@5  93.75 ( 93.12)
Epoch: [12][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0999e+00 (1.0566e+00)	Acc@1  64.06 ( 68.62)	Acc@5  95.31 ( 93.10)
Epoch: [12][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0366e+00 (1.0523e+00)	Acc@1  68.75 ( 68.71)	Acc@5  92.19 ( 93.15)
Epoch: [12][120/391]	Time  0.153 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.6536e-01 (1.0564e+00)	Acc@1  70.31 ( 68.60)	Acc@5  96.88 ( 93.10)
Epoch: [12][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7378e-01 (1.0580e+00)	Acc@1  71.09 ( 68.56)	Acc@5  93.75 ( 93.09)
Epoch: [12][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2674e+00 (1.0619e+00)	Acc@1  60.94 ( 68.50)	Acc@5  88.28 ( 93.02)
Epoch: [12][150/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7611e-01 (1.0617e+00)	Acc@1  67.97 ( 68.42)	Acc@5  94.53 ( 92.98)
Epoch: [12][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0708e+00 (1.0660e+00)	Acc@1  69.53 ( 68.30)	Acc@5  90.62 ( 92.91)
Epoch: [12][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7831e-01 (1.0629e+00)	Acc@1  72.66 ( 68.40)	Acc@5  94.53 ( 92.89)
Epoch: [12][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0680e+00 (1.0652e+00)	Acc@1  70.31 ( 68.26)	Acc@5  92.19 ( 92.87)
Epoch: [12][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0829e+00 (1.0658e+00)	Acc@1  64.84 ( 68.19)	Acc@5  90.62 ( 92.83)
Epoch: [12][200/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3391e+00 (1.0722e+00)	Acc@1  67.19 ( 68.04)	Acc@5  88.28 ( 92.74)
Epoch: [12][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1388e+00 (1.0745e+00)	Acc@1  64.84 ( 68.05)	Acc@5  91.41 ( 92.69)
Epoch: [12][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0495e+00 (1.0780e+00)	Acc@1  70.31 ( 67.96)	Acc@5  91.41 ( 92.65)
Epoch: [12][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3414e+00 (1.0824e+00)	Acc@1  60.16 ( 67.88)	Acc@5  92.97 ( 92.60)
Epoch: [12][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0495e+00 (1.0834e+00)	Acc@1  66.41 ( 67.81)	Acc@5  93.75 ( 92.59)
Epoch: [12][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1506e+00 (1.0842e+00)	Acc@1  69.53 ( 67.77)	Acc@5  91.41 ( 92.55)
Epoch: [12][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1058e+00 (1.0848e+00)	Acc@1  71.88 ( 67.80)	Acc@5  87.50 ( 92.52)
Epoch: [12][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0522e+00 (1.0851e+00)	Acc@1  70.31 ( 67.83)	Acc@5  91.41 ( 92.49)
Epoch: [12][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2134e+00 (1.0847e+00)	Acc@1  65.62 ( 67.82)	Acc@5  90.62 ( 92.49)
Epoch: [12][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0422e+00 (1.0839e+00)	Acc@1  71.09 ( 67.84)	Acc@5  93.75 ( 92.49)
Epoch: [12][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.9599e-01 (1.0863e+00)	Acc@1  64.84 ( 67.77)	Acc@5  93.75 ( 92.44)
Epoch: [12][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6115e-01 (1.0863e+00)	Acc@1  69.53 ( 67.74)	Acc@5  93.75 ( 92.44)
Epoch: [12][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1642e+00 (1.0886e+00)	Acc@1  64.06 ( 67.72)	Acc@5  91.41 ( 92.41)
Epoch: [12][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2277e+00 (1.0873e+00)	Acc@1  66.41 ( 67.74)	Acc@5  92.19 ( 92.43)
Epoch: [12][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1793e+00 (1.0869e+00)	Acc@1  61.72 ( 67.75)	Acc@5  89.06 ( 92.43)
Epoch: [12][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7281e-01 (1.0869e+00)	Acc@1  71.09 ( 67.73)	Acc@5  96.09 ( 92.45)
Epoch: [12][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0628e+00 (1.0894e+00)	Acc@1  69.53 ( 67.67)	Acc@5  89.84 ( 92.41)
Epoch: [12][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5618e+00 (1.0923e+00)	Acc@1  58.59 ( 67.58)	Acc@5  85.94 ( 92.35)
Epoch: [12][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1250e+00 (1.0918e+00)	Acc@1  68.75 ( 67.62)	Acc@5  89.06 ( 92.34)
Epoch: [12][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4227e+00 (1.0922e+00)	Acc@1  62.50 ( 67.63)	Acc@5  87.50 ( 92.32)
## e[12] optimizer.zero_grad (sum) time: 0.6685948371887207
## e[12]       loss.backward (sum) time: 13.843244075775146
## e[12]      optimizer.step (sum) time: 14.714842319488525
## epoch[12] training(only) time: 54.00555372238159
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.8712e+00 (1.8712e+00)	Acc@1  56.00 ( 56.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.5844e+00 (1.6052e+00)	Acc@1  61.00 ( 61.00)	Acc@5  88.00 ( 86.55)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.2017e+00 (1.5334e+00)	Acc@1  68.00 ( 60.90)	Acc@5  90.00 ( 86.52)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.8946e+00 (1.5581e+00)	Acc@1  46.00 ( 59.87)	Acc@5  86.00 ( 86.55)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.6556e+00 (1.5704e+00)	Acc@1  58.00 ( 59.07)	Acc@5  87.00 ( 86.32)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.5952e+00 (1.5804e+00)	Acc@1  61.00 ( 58.82)	Acc@5  83.00 ( 85.71)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2099e+00 (1.5588e+00)	Acc@1  65.00 ( 58.98)	Acc@5  89.00 ( 85.95)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.7278e+00 (1.5679e+00)	Acc@1  60.00 ( 58.99)	Acc@5  82.00 ( 85.79)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.7006e+00 (1.5630e+00)	Acc@1  61.00 ( 59.01)	Acc@5  76.00 ( 85.78)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.7382e+00 (1.5453e+00)	Acc@1  54.00 ( 59.14)	Acc@5  84.00 ( 86.03)
 * Acc@1 59.190 Acc@5 85.980
### epoch[12] execution time: 59.03869080543518
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.308 ( 0.308)	Data  0.161 ( 0.161)	Loss 1.0964e+00 (1.0964e+00)	Acc@1  67.19 ( 67.19)	Acc@5  94.53 ( 94.53)
Epoch: [13][ 10/391]	Time  0.147 ( 0.152)	Data  0.001 ( 0.016)	Loss 8.9063e-01 (9.7888e-01)	Acc@1  70.31 ( 70.17)	Acc@5  96.88 ( 93.82)
Epoch: [13][ 20/391]	Time  0.140 ( 0.145)	Data  0.001 ( 0.009)	Loss 1.0236e+00 (9.5664e-01)	Acc@1  71.88 ( 71.24)	Acc@5  95.31 ( 94.16)
Epoch: [13][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 9.2962e-01 (9.7347e-01)	Acc@1  70.31 ( 70.64)	Acc@5  92.19 ( 93.88)
Epoch: [13][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.1036e+00 (9.7039e-01)	Acc@1  64.84 ( 70.85)	Acc@5  92.97 ( 93.86)
Epoch: [13][ 50/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.0102e+00 (9.7418e-01)	Acc@1  69.53 ( 70.71)	Acc@5  94.53 ( 93.83)
Epoch: [13][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 8.2577e-01 (9.9118e-01)	Acc@1  71.88 ( 70.06)	Acc@5  96.09 ( 93.67)
Epoch: [13][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.3494e-01 (9.9457e-01)	Acc@1  71.88 ( 70.09)	Acc@5  93.75 ( 93.49)
Epoch: [13][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2372e+00 (9.9506e-01)	Acc@1  64.06 ( 70.07)	Acc@5  88.28 ( 93.47)
Epoch: [13][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1531e+00 (9.9367e-01)	Acc@1  66.41 ( 70.14)	Acc@5  92.19 ( 93.52)
Epoch: [13][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2690e+00 (9.9156e-01)	Acc@1  62.50 ( 70.36)	Acc@5  89.84 ( 93.43)
Epoch: [13][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.4599e-01 (9.8821e-01)	Acc@1  76.56 ( 70.58)	Acc@5  96.09 ( 93.48)
Epoch: [13][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1136e+00 (9.9164e-01)	Acc@1  68.75 ( 70.58)	Acc@5  90.62 ( 93.38)
Epoch: [13][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0776e+00 (9.9155e-01)	Acc@1  66.41 ( 70.49)	Acc@5  92.19 ( 93.40)
Epoch: [13][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2148e+00 (9.9945e-01)	Acc@1  62.50 ( 70.25)	Acc@5  88.28 ( 93.26)
Epoch: [13][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0376e+00 (1.0024e+00)	Acc@1  69.53 ( 70.21)	Acc@5  91.41 ( 93.24)
Epoch: [13][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0328e+00 (1.0084e+00)	Acc@1  66.41 ( 70.02)	Acc@5  93.75 ( 93.19)
Epoch: [13][170/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0945e+00 (1.0094e+00)	Acc@1  69.53 ( 70.04)	Acc@5  92.19 ( 93.17)
Epoch: [13][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0157e+00 (1.0092e+00)	Acc@1  68.75 ( 70.05)	Acc@5  90.62 ( 93.19)
Epoch: [13][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1629e+00 (1.0067e+00)	Acc@1  67.97 ( 70.12)	Acc@5  89.84 ( 93.21)
Epoch: [13][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1243e+00 (1.0086e+00)	Acc@1  67.97 ( 70.12)	Acc@5  89.06 ( 93.17)
Epoch: [13][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1378e+00 (1.0099e+00)	Acc@1  72.66 ( 70.08)	Acc@5  92.19 ( 93.16)
Epoch: [13][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4943e-01 (1.0101e+00)	Acc@1  71.88 ( 70.10)	Acc@5  93.75 ( 93.17)
Epoch: [13][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1706e+00 (1.0119e+00)	Acc@1  64.84 ( 70.09)	Acc@5  92.19 ( 93.15)
Epoch: [13][240/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2621e+00 (1.0163e+00)	Acc@1  68.75 ( 69.97)	Acc@5  89.06 ( 93.10)
Epoch: [13][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.7964e-01 (1.0169e+00)	Acc@1  65.62 ( 69.82)	Acc@5  93.75 ( 93.16)
Epoch: [13][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1359e+00 (1.0191e+00)	Acc@1  67.19 ( 69.74)	Acc@5  92.19 ( 93.15)
Epoch: [13][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0374e+00 (1.0235e+00)	Acc@1  71.09 ( 69.63)	Acc@5  90.62 ( 93.08)
Epoch: [13][280/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1940e+00 (1.0244e+00)	Acc@1  64.06 ( 69.58)	Acc@5  92.19 ( 93.09)
Epoch: [13][290/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1304e+00 (1.0238e+00)	Acc@1  62.50 ( 69.63)	Acc@5  93.75 ( 93.07)
Epoch: [13][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5431e-01 (1.0272e+00)	Acc@1  70.31 ( 69.52)	Acc@5  94.53 ( 93.06)
Epoch: [13][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1446e+00 (1.0290e+00)	Acc@1  64.84 ( 69.44)	Acc@5  93.75 ( 93.04)
Epoch: [13][320/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1087e-01 (1.0311e+00)	Acc@1  73.44 ( 69.32)	Acc@5  95.31 ( 93.02)
Epoch: [13][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7510e-01 (1.0319e+00)	Acc@1  75.78 ( 69.30)	Acc@5  92.97 ( 93.03)
Epoch: [13][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1052e+00 (1.0323e+00)	Acc@1  67.19 ( 69.26)	Acc@5  94.53 ( 93.04)
Epoch: [13][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.0310e-01 (1.0344e+00)	Acc@1  75.78 ( 69.25)	Acc@5  98.44 ( 93.04)
Epoch: [13][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1334e+00 (1.0351e+00)	Acc@1  64.84 ( 69.23)	Acc@5  92.97 ( 93.05)
Epoch: [13][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1162e+00 (1.0340e+00)	Acc@1  63.28 ( 69.23)	Acc@5  95.31 ( 93.09)
Epoch: [13][380/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0003e+00 (1.0325e+00)	Acc@1  71.09 ( 69.27)	Acc@5  92.97 ( 93.09)
Epoch: [13][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1837e+00 (1.0326e+00)	Acc@1  62.50 ( 69.24)	Acc@5  90.00 ( 93.09)
## e[13] optimizer.zero_grad (sum) time: 0.6697206497192383
## e[13]       loss.backward (sum) time: 13.757599592208862
## e[13]      optimizer.step (sum) time: 14.765496253967285
## epoch[13] training(only) time: 54.02367925643921
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.6028e+00 (1.6028e+00)	Acc@1  60.00 ( 60.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.5105e+00 (1.5455e+00)	Acc@1  60.00 ( 59.45)	Acc@5  91.00 ( 86.55)
Test: [ 20/100]	Time  0.057 ( 0.055)	Loss 1.2469e+00 (1.4915e+00)	Acc@1  63.00 ( 60.67)	Acc@5  94.00 ( 87.24)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.7712e+00 (1.5282e+00)	Acc@1  47.00 ( 59.35)	Acc@5  87.00 ( 86.58)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5645e+00 (1.5114e+00)	Acc@1  61.00 ( 59.95)	Acc@5  87.00 ( 86.90)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4523e+00 (1.5144e+00)	Acc@1  63.00 ( 59.90)	Acc@5  84.00 ( 86.71)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.3966e+00 (1.5059e+00)	Acc@1  59.00 ( 59.69)	Acc@5  89.00 ( 86.85)
Test: [ 70/100]	Time  0.055 ( 0.050)	Loss 1.6400e+00 (1.5082e+00)	Acc@1  62.00 ( 59.77)	Acc@5  83.00 ( 86.92)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.8387e+00 (1.5117e+00)	Acc@1  54.00 ( 59.67)	Acc@5  83.00 ( 86.93)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.6130e+00 (1.4933e+00)	Acc@1  60.00 ( 60.16)	Acc@5  85.00 ( 87.11)
 * Acc@1 60.370 Acc@5 87.140
### epoch[13] execution time: 59.0320770740509
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.300 ( 0.300)	Data  0.150 ( 0.150)	Loss 1.1335e+00 (1.1335e+00)	Acc@1  67.97 ( 67.97)	Acc@5  91.41 ( 91.41)
Epoch: [14][ 10/391]	Time  0.134 ( 0.150)	Data  0.001 ( 0.015)	Loss 7.6771e-01 (8.9890e-01)	Acc@1  75.00 ( 72.94)	Acc@5  99.22 ( 95.53)
Epoch: [14][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.008)	Loss 9.1429e-01 (8.9840e-01)	Acc@1  77.34 ( 72.95)	Acc@5  96.09 ( 95.24)
Epoch: [14][ 30/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.006)	Loss 1.0248e+00 (9.0181e-01)	Acc@1  67.19 ( 72.66)	Acc@5  95.31 ( 95.16)
Epoch: [14][ 40/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.005)	Loss 8.3362e-01 (9.0692e-01)	Acc@1  75.00 ( 72.52)	Acc@5  98.44 ( 95.05)
Epoch: [14][ 50/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 7.9848e-01 (9.0191e-01)	Acc@1  78.91 ( 72.86)	Acc@5  94.53 ( 94.99)
Epoch: [14][ 60/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.004)	Loss 8.1918e-01 (8.9812e-01)	Acc@1  72.66 ( 72.98)	Acc@5  95.31 ( 94.93)
Epoch: [14][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.3729e-01 (9.0061e-01)	Acc@1  78.12 ( 73.05)	Acc@5  96.88 ( 94.86)
Epoch: [14][ 80/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.7518e-01 (9.0427e-01)	Acc@1  69.53 ( 73.00)	Acc@5  92.97 ( 94.73)
Epoch: [14][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.5619e-01 (9.1313e-01)	Acc@1  71.88 ( 72.78)	Acc@5  92.19 ( 94.67)
Epoch: [14][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0070e+00 (9.1297e-01)	Acc@1  69.53 ( 72.83)	Acc@5  92.97 ( 94.69)
Epoch: [14][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.4313e-01 (9.1545e-01)	Acc@1  72.66 ( 72.78)	Acc@5  92.19 ( 94.54)
Epoch: [14][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.6098e-01 (9.1965e-01)	Acc@1  76.56 ( 72.78)	Acc@5  93.75 ( 94.40)
Epoch: [14][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3111e+00 (9.2696e-01)	Acc@1  67.97 ( 72.54)	Acc@5  90.62 ( 94.38)
Epoch: [14][140/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0063e+00 (9.2984e-01)	Acc@1  63.28 ( 72.39)	Acc@5  96.09 ( 94.32)
Epoch: [14][150/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5822e-01 (9.3269e-01)	Acc@1  71.88 ( 72.25)	Acc@5  94.53 ( 94.38)
Epoch: [14][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1485e+00 (9.3313e-01)	Acc@1  63.28 ( 72.22)	Acc@5  91.41 ( 94.38)
Epoch: [14][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0746e+00 (9.3262e-01)	Acc@1  67.97 ( 72.28)	Acc@5  94.53 ( 94.41)
Epoch: [14][180/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.3220e-01 (9.3277e-01)	Acc@1  70.31 ( 72.23)	Acc@5  94.53 ( 94.40)
Epoch: [14][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6401e-01 (9.3665e-01)	Acc@1  74.22 ( 72.09)	Acc@5  96.88 ( 94.33)
Epoch: [14][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2955e+00 (9.4203e-01)	Acc@1  62.50 ( 71.94)	Acc@5  92.19 ( 94.31)
Epoch: [14][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6887e-01 (9.4397e-01)	Acc@1  73.44 ( 71.89)	Acc@5  97.66 ( 94.26)
Epoch: [14][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4261e+00 (9.4650e-01)	Acc@1  57.03 ( 71.82)	Acc@5  89.84 ( 94.20)
Epoch: [14][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1388e+00 (9.4961e-01)	Acc@1  66.41 ( 71.76)	Acc@5  90.62 ( 94.17)
Epoch: [14][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6001e-01 (9.5320e-01)	Acc@1  71.88 ( 71.66)	Acc@5  93.75 ( 94.11)
Epoch: [14][250/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6920e-01 (9.5598e-01)	Acc@1  74.22 ( 71.60)	Acc@5  92.97 ( 94.07)
Epoch: [14][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1168e+00 (9.5761e-01)	Acc@1  68.75 ( 71.53)	Acc@5  92.19 ( 94.05)
Epoch: [14][270/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2352e+00 (9.6115e-01)	Acc@1  63.28 ( 71.39)	Acc@5  90.62 ( 94.02)
Epoch: [14][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9467e-01 (9.6347e-01)	Acc@1  75.78 ( 71.31)	Acc@5  92.97 ( 93.96)
Epoch: [14][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0441e+00 (9.6374e-01)	Acc@1  70.31 ( 71.31)	Acc@5  92.97 ( 93.96)
Epoch: [14][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0737e+00 (9.6623e-01)	Acc@1  72.66 ( 71.22)	Acc@5  90.62 ( 93.89)
Epoch: [14][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6380e-01 (9.6810e-01)	Acc@1  71.88 ( 71.16)	Acc@5  96.88 ( 93.89)
Epoch: [14][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.7052e-01 (9.6922e-01)	Acc@1  75.00 ( 71.16)	Acc@5  94.53 ( 93.87)
Epoch: [14][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2401e-01 (9.7045e-01)	Acc@1  72.66 ( 71.14)	Acc@5  93.75 ( 93.84)
Epoch: [14][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0417e+00 (9.7394e-01)	Acc@1  67.19 ( 71.04)	Acc@5  93.75 ( 93.82)
Epoch: [14][350/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1606e+00 (9.7670e-01)	Acc@1  67.19 ( 70.96)	Acc@5  92.19 ( 93.77)
Epoch: [14][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1050e+00 (9.7846e-01)	Acc@1  64.84 ( 70.88)	Acc@5  92.19 ( 93.75)
Epoch: [14][370/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2912e-01 (9.7843e-01)	Acc@1  69.53 ( 70.85)	Acc@5  96.09 ( 93.76)
Epoch: [14][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.3736e-01 (9.7956e-01)	Acc@1  77.34 ( 70.81)	Acc@5  89.84 ( 93.73)
Epoch: [14][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2168e+00 (9.8077e-01)	Acc@1  65.00 ( 70.78)	Acc@5  91.25 ( 93.72)
## e[14] optimizer.zero_grad (sum) time: 0.669400691986084
## e[14]       loss.backward (sum) time: 13.827402353286743
## e[14]      optimizer.step (sum) time: 14.755876541137695
## epoch[14] training(only) time: 54.07815337181091
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.4960e+00 (1.4960e+00)	Acc@1  66.00 ( 66.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.4343e+00 (1.4556e+00)	Acc@1  64.00 ( 62.00)	Acc@5  88.00 ( 87.55)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1943e+00 (1.3856e+00)	Acc@1  64.00 ( 63.33)	Acc@5  91.00 ( 88.57)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 1.4910e+00 (1.3598e+00)	Acc@1  54.00 ( 62.87)	Acc@5  92.00 ( 88.87)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.4269e+00 (1.3615e+00)	Acc@1  60.00 ( 62.83)	Acc@5  90.00 ( 89.07)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2941e+00 (1.3614e+00)	Acc@1  63.00 ( 62.94)	Acc@5  88.00 ( 89.04)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2230e+00 (1.3510e+00)	Acc@1  59.00 ( 62.80)	Acc@5  92.00 ( 89.30)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.1638e+00 (1.3532e+00)	Acc@1  66.00 ( 62.75)	Acc@5  91.00 ( 89.25)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4741e+00 (1.3643e+00)	Acc@1  62.00 ( 62.57)	Acc@5  82.00 ( 89.04)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.6052e+00 (1.3519e+00)	Acc@1  60.00 ( 62.78)	Acc@5  87.00 ( 89.29)
 * Acc@1 63.060 Acc@5 89.440
### epoch[14] execution time: 59.07551312446594
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.302 ( 0.302)	Data  0.129 ( 0.129)	Loss 6.5157e-01 (6.5157e-01)	Acc@1  80.47 ( 80.47)	Acc@5  97.66 ( 97.66)
Epoch: [15][ 10/391]	Time  0.150 ( 0.153)	Data  0.001 ( 0.013)	Loss 8.9474e-01 (8.2135e-01)	Acc@1  76.56 ( 75.57)	Acc@5  95.31 ( 95.60)
Epoch: [15][ 20/391]	Time  0.141 ( 0.146)	Data  0.001 ( 0.007)	Loss 7.8822e-01 (8.1256e-01)	Acc@1  79.69 ( 75.41)	Acc@5  94.53 ( 95.61)
Epoch: [15][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.005)	Loss 9.6203e-01 (8.2516e-01)	Acc@1  65.62 ( 74.85)	Acc@5  96.09 ( 95.46)
Epoch: [15][ 40/391]	Time  0.142 ( 0.142)	Data  0.001 ( 0.004)	Loss 8.3282e-01 (8.3554e-01)	Acc@1  72.66 ( 74.39)	Acc@5  96.88 ( 95.58)
Epoch: [15][ 50/391]	Time  0.144 ( 0.141)	Data  0.001 ( 0.004)	Loss 8.6610e-01 (8.4643e-01)	Acc@1  70.31 ( 74.10)	Acc@5  96.09 ( 95.45)
Epoch: [15][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1002e+00 (8.5804e-01)	Acc@1  67.19 ( 73.69)	Acc@5  92.19 ( 95.34)
Epoch: [15][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0010e+00 (8.5380e-01)	Acc@1  69.53 ( 73.79)	Acc@5  92.97 ( 95.36)
Epoch: [15][ 80/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.2360e-01 (8.5724e-01)	Acc@1  76.56 ( 73.68)	Acc@5  99.22 ( 95.29)
Epoch: [15][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 8.7658e-01 (8.5916e-01)	Acc@1  71.88 ( 73.70)	Acc@5  96.88 ( 95.30)
Epoch: [15][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5805e-01 (8.6252e-01)	Acc@1  71.88 ( 73.59)	Acc@5  95.31 ( 95.24)
Epoch: [15][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8014e-01 (8.6715e-01)	Acc@1  72.66 ( 73.44)	Acc@5  92.97 ( 95.24)
Epoch: [15][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.4838e-01 (8.7211e-01)	Acc@1  72.66 ( 73.36)	Acc@5  92.97 ( 95.14)
Epoch: [15][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3157e-01 (8.7329e-01)	Acc@1  75.78 ( 73.38)	Acc@5  93.75 ( 95.12)
Epoch: [15][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3026e-01 (8.7578e-01)	Acc@1  75.78 ( 73.38)	Acc@5  96.09 ( 95.05)
Epoch: [15][150/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0612e+00 (8.8058e-01)	Acc@1  65.62 ( 73.20)	Acc@5  92.97 ( 95.02)
Epoch: [15][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5236e-01 (8.8243e-01)	Acc@1  72.66 ( 73.15)	Acc@5  92.19 ( 94.96)
Epoch: [15][170/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6240e-01 (8.8757e-01)	Acc@1  69.53 ( 72.98)	Acc@5  95.31 ( 94.91)
Epoch: [15][180/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0758e+00 (8.8967e-01)	Acc@1  65.62 ( 72.95)	Acc@5  89.84 ( 94.89)
Epoch: [15][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2522e-01 (8.9123e-01)	Acc@1  74.22 ( 72.89)	Acc@5  95.31 ( 94.84)
Epoch: [15][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0297e+00 (8.9569e-01)	Acc@1  67.97 ( 72.84)	Acc@5  92.19 ( 94.76)
Epoch: [15][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9584e-01 (8.9934e-01)	Acc@1  75.00 ( 72.79)	Acc@5  97.66 ( 94.70)
Epoch: [15][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8441e-01 (8.9940e-01)	Acc@1  73.44 ( 72.78)	Acc@5  93.75 ( 94.69)
Epoch: [15][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.7836e-01 (9.0176e-01)	Acc@1  69.53 ( 72.69)	Acc@5  92.97 ( 94.65)
Epoch: [15][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0723e+00 (9.0277e-01)	Acc@1  68.75 ( 72.63)	Acc@5  92.19 ( 94.65)
Epoch: [15][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7792e-01 (9.0562e-01)	Acc@1  75.78 ( 72.54)	Acc@5  97.66 ( 94.65)
Epoch: [15][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0187e-01 (9.0929e-01)	Acc@1  73.44 ( 72.49)	Acc@5  91.41 ( 94.59)
Epoch: [15][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5358e-01 (9.1270e-01)	Acc@1  75.78 ( 72.41)	Acc@5  95.31 ( 94.54)
Epoch: [15][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0667e-01 (9.1390e-01)	Acc@1  71.09 ( 72.33)	Acc@5  95.31 ( 94.53)
Epoch: [15][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1710e-01 (9.1467e-01)	Acc@1  76.56 ( 72.29)	Acc@5  96.09 ( 94.54)
Epoch: [15][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.4271e-01 (9.1488e-01)	Acc@1  72.66 ( 72.31)	Acc@5  96.88 ( 94.54)
Epoch: [15][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0318e+00 (9.1729e-01)	Acc@1  69.53 ( 72.24)	Acc@5  93.75 ( 94.54)
Epoch: [15][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2263e+00 (9.1954e-01)	Acc@1  63.28 ( 72.19)	Acc@5  90.62 ( 94.50)
Epoch: [15][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0406e+00 (9.2101e-01)	Acc@1  67.97 ( 72.17)	Acc@5  94.53 ( 94.49)
Epoch: [15][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0565e+00 (9.2353e-01)	Acc@1  67.97 ( 72.11)	Acc@5  92.19 ( 94.44)
Epoch: [15][350/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.0093e-01 (9.2536e-01)	Acc@1  75.00 ( 72.06)	Acc@5  95.31 ( 94.40)
Epoch: [15][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5379e-01 (9.2600e-01)	Acc@1  77.34 ( 72.02)	Acc@5  96.88 ( 94.40)
Epoch: [15][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0844e+00 (9.2834e-01)	Acc@1  67.19 ( 71.96)	Acc@5  91.41 ( 94.36)
Epoch: [15][380/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0048e+00 (9.2953e-01)	Acc@1  71.09 ( 71.95)	Acc@5  92.97 ( 94.34)
Epoch: [15][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0828e+00 (9.3304e-01)	Acc@1  70.00 ( 71.85)	Acc@5  93.75 ( 94.30)
## e[15] optimizer.zero_grad (sum) time: 0.6678776741027832
## e[15]       loss.backward (sum) time: 13.805654764175415
## e[15]      optimizer.step (sum) time: 14.725839138031006
## epoch[15] training(only) time: 53.935890197753906
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.5226e+00 (1.5226e+00)	Acc@1  59.00 ( 59.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.6781e+00 (1.4884e+00)	Acc@1  58.00 ( 60.82)	Acc@5  87.00 ( 86.36)
Test: [ 20/100]	Time  0.058 ( 0.055)	Loss 1.3080e+00 (1.4170e+00)	Acc@1  65.00 ( 62.00)	Acc@5  87.00 ( 87.48)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5209e+00 (1.4324e+00)	Acc@1  56.00 ( 61.26)	Acc@5  92.00 ( 87.74)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 1.4716e+00 (1.4200e+00)	Acc@1  62.00 ( 61.20)	Acc@5  91.00 ( 88.39)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2657e+00 (1.4282e+00)	Acc@1  67.00 ( 61.53)	Acc@5  89.00 ( 88.16)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2582e+00 (1.4142e+00)	Acc@1  60.00 ( 61.66)	Acc@5  92.00 ( 88.25)
Test: [ 70/100]	Time  0.056 ( 0.050)	Loss 1.4639e+00 (1.4147e+00)	Acc@1  65.00 ( 61.89)	Acc@5  88.00 ( 88.15)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.6495e+00 (1.4269e+00)	Acc@1  56.00 ( 61.72)	Acc@5  85.00 ( 88.00)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.6915e+00 (1.4113e+00)	Acc@1  55.00 ( 62.08)	Acc@5  88.00 ( 88.25)
 * Acc@1 62.360 Acc@5 88.300
### epoch[15] execution time: 58.932530641555786
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.296 ( 0.296)	Data  0.143 ( 0.143)	Loss 8.7105e-01 (8.7105e-01)	Acc@1  73.44 ( 73.44)	Acc@5  92.97 ( 92.97)
Epoch: [16][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.014)	Loss 7.4284e-01 (8.9035e-01)	Acc@1  78.12 ( 74.22)	Acc@5  98.44 ( 94.11)
Epoch: [16][ 20/391]	Time  0.137 ( 0.146)	Data  0.001 ( 0.008)	Loss 8.8314e-01 (8.6476e-01)	Acc@1  73.44 ( 74.74)	Acc@5  95.31 ( 94.57)
Epoch: [16][ 30/391]	Time  0.138 ( 0.144)	Data  0.001 ( 0.006)	Loss 8.0214e-01 (8.3478e-01)	Acc@1  75.00 ( 75.23)	Acc@5  96.09 ( 95.06)
Epoch: [16][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 8.4406e-01 (8.2151e-01)	Acc@1  75.78 ( 75.30)	Acc@5  92.19 ( 95.31)
Epoch: [16][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 9.1502e-01 (8.2115e-01)	Acc@1  70.31 ( 75.15)	Acc@5  95.31 ( 95.47)
Epoch: [16][ 60/391]	Time  0.145 ( 0.141)	Data  0.001 ( 0.003)	Loss 7.7927e-01 (8.1873e-01)	Acc@1  78.12 ( 75.23)	Acc@5  94.53 ( 95.58)
Epoch: [16][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.8410e-01 (8.1798e-01)	Acc@1  75.00 ( 75.15)	Acc@5  95.31 ( 95.63)
Epoch: [16][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.2016e-01 (8.1186e-01)	Acc@1  78.91 ( 75.41)	Acc@5  96.09 ( 95.69)
Epoch: [16][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.6839e-01 (8.1191e-01)	Acc@1  77.34 ( 75.30)	Acc@5  96.09 ( 95.64)
Epoch: [16][100/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.8850e-01 (8.0930e-01)	Acc@1  72.66 ( 75.37)	Acc@5  95.31 ( 95.56)
Epoch: [16][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8952e-01 (8.1565e-01)	Acc@1  75.78 ( 75.20)	Acc@5  96.09 ( 95.52)
Epoch: [16][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.5383e-01 (8.1587e-01)	Acc@1  78.12 ( 75.23)	Acc@5  96.88 ( 95.51)
Epoch: [16][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.2161e-01 (8.1457e-01)	Acc@1  78.12 ( 75.33)	Acc@5  94.53 ( 95.53)
Epoch: [16][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9326e-01 (8.1650e-01)	Acc@1  73.44 ( 75.24)	Acc@5  96.09 ( 95.52)
Epoch: [16][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.1286e-01 (8.1523e-01)	Acc@1  67.97 ( 75.22)	Acc@5  95.31 ( 95.56)
Epoch: [16][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0327e+00 (8.2213e-01)	Acc@1  67.97 ( 75.04)	Acc@5  95.31 ( 95.45)
Epoch: [16][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.0693e-01 (8.2692e-01)	Acc@1  74.22 ( 74.95)	Acc@5  94.53 ( 95.37)
Epoch: [16][180/391]	Time  0.154 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1632e+00 (8.3266e-01)	Acc@1  66.41 ( 74.75)	Acc@5  94.53 ( 95.33)
Epoch: [16][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7635e-01 (8.3845e-01)	Acc@1  71.09 ( 74.57)	Acc@5  95.31 ( 95.30)
Epoch: [16][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.4293e-01 (8.4177e-01)	Acc@1  71.09 ( 74.48)	Acc@5  91.41 ( 95.28)
Epoch: [16][210/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8694e-01 (8.4339e-01)	Acc@1  68.75 ( 74.39)	Acc@5  96.88 ( 95.26)
Epoch: [16][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8935e-01 (8.4507e-01)	Acc@1  73.44 ( 74.38)	Acc@5  96.09 ( 95.22)
Epoch: [16][230/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9109e-01 (8.4832e-01)	Acc@1  72.66 ( 74.29)	Acc@5  95.31 ( 95.18)
Epoch: [16][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0285e+00 (8.5253e-01)	Acc@1  70.31 ( 74.23)	Acc@5  92.97 ( 95.10)
Epoch: [16][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.4298e-01 (8.5496e-01)	Acc@1  76.56 ( 74.14)	Acc@5  96.88 ( 95.10)
Epoch: [16][260/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0176e+00 (8.5660e-01)	Acc@1  67.97 ( 74.11)	Acc@5  92.97 ( 95.05)
Epoch: [16][270/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2653e-01 (8.5911e-01)	Acc@1  72.66 ( 74.04)	Acc@5  93.75 ( 95.00)
Epoch: [16][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9558e-01 (8.6027e-01)	Acc@1  78.12 ( 74.04)	Acc@5  93.75 ( 94.96)
Epoch: [16][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.0515e-01 (8.6005e-01)	Acc@1  75.00 ( 74.04)	Acc@5  96.09 ( 94.97)
Epoch: [16][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2092e-01 (8.6000e-01)	Acc@1  78.12 ( 74.06)	Acc@5  96.88 ( 94.96)
Epoch: [16][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9397e-01 (8.6155e-01)	Acc@1  74.22 ( 73.99)	Acc@5  97.66 ( 94.96)
Epoch: [16][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0508e-01 (8.6355e-01)	Acc@1  76.56 ( 73.89)	Acc@5  96.88 ( 94.92)
Epoch: [16][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0329e+00 (8.6389e-01)	Acc@1  65.62 ( 73.90)	Acc@5  95.31 ( 94.91)
Epoch: [16][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1462e+00 (8.6675e-01)	Acc@1  67.19 ( 73.80)	Acc@5  89.06 ( 94.88)
Epoch: [16][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0266e+00 (8.7028e-01)	Acc@1  64.84 ( 73.66)	Acc@5  93.75 ( 94.83)
Epoch: [16][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1449e+00 (8.7125e-01)	Acc@1  66.41 ( 73.61)	Acc@5  89.06 ( 94.81)
Epoch: [16][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.1541e+00 (8.7330e-01)	Acc@1  66.41 ( 73.57)	Acc@5  92.97 ( 94.80)
Epoch: [16][380/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.001)	Loss 7.0884e-01 (8.7367e-01)	Acc@1  76.56 ( 73.55)	Acc@5  97.66 ( 94.81)
Epoch: [16][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.0923e+00 (8.7547e-01)	Acc@1  68.75 ( 73.49)	Acc@5  93.75 ( 94.80)
## e[16] optimizer.zero_grad (sum) time: 0.6667261123657227
## e[16]       loss.backward (sum) time: 13.845969438552856
## e[16]      optimizer.step (sum) time: 14.75784707069397
## epoch[16] training(only) time: 54.07147145271301
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.4131e+00 (1.4131e+00)	Acc@1  67.00 ( 67.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.4517e+00 (1.5208e+00)	Acc@1  70.00 ( 62.82)	Acc@5  90.00 ( 87.09)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.3124e+00 (1.4828e+00)	Acc@1  68.00 ( 62.24)	Acc@5  88.00 ( 87.67)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.5534e+00 (1.4570e+00)	Acc@1  57.00 ( 62.16)	Acc@5  91.00 ( 88.03)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.5798e+00 (1.4421e+00)	Acc@1  64.00 ( 62.41)	Acc@5  87.00 ( 88.29)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2977e+00 (1.4405e+00)	Acc@1  67.00 ( 62.29)	Acc@5  89.00 ( 88.27)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2339e+00 (1.4208e+00)	Acc@1  64.00 ( 62.61)	Acc@5  95.00 ( 88.56)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3091e+00 (1.4270e+00)	Acc@1  62.00 ( 62.54)	Acc@5  88.00 ( 88.44)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.6916e+00 (1.4351e+00)	Acc@1  58.00 ( 62.47)	Acc@5  87.00 ( 88.33)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.5984e+00 (1.4180e+00)	Acc@1  60.00 ( 62.79)	Acc@5  86.00 ( 88.60)
 * Acc@1 62.960 Acc@5 88.570
### epoch[16] execution time: 59.08542084693909
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.297 ( 0.297)	Data  0.151 ( 0.151)	Loss 8.1376e-01 (8.1376e-01)	Acc@1  79.69 ( 79.69)	Acc@5  96.09 ( 96.09)
Epoch: [17][ 10/391]	Time  0.134 ( 0.152)	Data  0.001 ( 0.015)	Loss 7.2667e-01 (8.5260e-01)	Acc@1  77.34 ( 75.57)	Acc@5  96.09 ( 95.24)
Epoch: [17][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.008)	Loss 8.0682e-01 (8.5008e-01)	Acc@1  78.91 ( 75.41)	Acc@5  94.53 ( 95.09)
Epoch: [17][ 30/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.006)	Loss 8.5768e-01 (8.3892e-01)	Acc@1  75.00 ( 75.15)	Acc@5  96.09 ( 95.46)
Epoch: [17][ 40/391]	Time  0.148 ( 0.141)	Data  0.001 ( 0.005)	Loss 6.8840e-01 (8.3757e-01)	Acc@1  78.12 ( 75.15)	Acc@5  98.44 ( 95.43)
Epoch: [17][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.8392e-01 (8.1015e-01)	Acc@1  77.34 ( 75.66)	Acc@5  97.66 ( 95.66)
Epoch: [17][ 60/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 7.2348e-01 (8.0343e-01)	Acc@1  80.47 ( 75.77)	Acc@5  97.66 ( 95.85)
Epoch: [17][ 70/391]	Time  0.147 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.2305e-01 (8.0753e-01)	Acc@1  75.00 ( 75.48)	Acc@5  96.88 ( 95.74)
Epoch: [17][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.9249e-01 (8.0038e-01)	Acc@1  69.53 ( 75.68)	Acc@5  92.97 ( 95.81)
Epoch: [17][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 8.7393e-01 (8.0256e-01)	Acc@1  71.88 ( 75.62)	Acc@5  94.53 ( 95.71)
Epoch: [17][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 8.5942e-01 (7.9915e-01)	Acc@1  75.00 ( 75.79)	Acc@5  92.97 ( 95.71)
Epoch: [17][110/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4843e-01 (7.9764e-01)	Acc@1  72.66 ( 75.84)	Acc@5  96.88 ( 95.74)
Epoch: [17][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7961e-01 (7.9903e-01)	Acc@1  68.75 ( 75.78)	Acc@5  92.97 ( 95.69)
Epoch: [17][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4757e-01 (7.9945e-01)	Acc@1  73.44 ( 75.76)	Acc@5  92.97 ( 95.65)
Epoch: [17][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.6176e-01 (7.9755e-01)	Acc@1  69.53 ( 75.87)	Acc@5  96.09 ( 95.68)
Epoch: [17][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.4669e-01 (7.9650e-01)	Acc@1  73.44 ( 75.90)	Acc@5  92.19 ( 95.67)
Epoch: [17][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.0000e-01 (7.9658e-01)	Acc@1  80.47 ( 75.87)	Acc@5  98.44 ( 95.65)
Epoch: [17][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5367e-01 (7.9816e-01)	Acc@1  77.34 ( 75.91)	Acc@5  94.53 ( 95.62)
Epoch: [17][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.7817e-01 (8.0044e-01)	Acc@1  67.19 ( 75.84)	Acc@5  93.75 ( 95.60)
Epoch: [17][190/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.6416e-01 (8.0430e-01)	Acc@1  73.44 ( 75.72)	Acc@5  96.09 ( 95.61)
Epoch: [17][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0586e-01 (8.0645e-01)	Acc@1  71.88 ( 75.66)	Acc@5  93.75 ( 95.60)
Epoch: [17][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0619e+00 (8.0989e-01)	Acc@1  70.31 ( 75.58)	Acc@5  91.41 ( 95.57)
Epoch: [17][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.0192e-01 (8.1228e-01)	Acc@1  75.00 ( 75.55)	Acc@5  95.31 ( 95.54)
Epoch: [17][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1607e-01 (8.1448e-01)	Acc@1  81.25 ( 75.48)	Acc@5  98.44 ( 95.51)
Epoch: [17][240/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6648e-01 (8.1497e-01)	Acc@1  73.44 ( 75.45)	Acc@5  92.19 ( 95.50)
Epoch: [17][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1203e+00 (8.1752e-01)	Acc@1  64.84 ( 75.37)	Acc@5  92.97 ( 95.48)
Epoch: [17][260/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8713e-01 (8.1801e-01)	Acc@1  78.12 ( 75.36)	Acc@5  96.09 ( 95.47)
Epoch: [17][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1288e+00 (8.2004e-01)	Acc@1  71.88 ( 75.26)	Acc@5  92.19 ( 95.47)
Epoch: [17][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7147e-01 (8.2411e-01)	Acc@1  73.44 ( 75.14)	Acc@5  95.31 ( 95.43)
Epoch: [17][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.0052e-01 (8.2471e-01)	Acc@1  76.56 ( 75.10)	Acc@5  95.31 ( 95.42)
Epoch: [17][300/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0160e+00 (8.2599e-01)	Acc@1  71.09 ( 75.05)	Acc@5  92.97 ( 95.41)
Epoch: [17][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3783e-01 (8.2658e-01)	Acc@1  71.09 ( 75.01)	Acc@5  96.88 ( 95.41)
Epoch: [17][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1814e-01 (8.2942e-01)	Acc@1  71.09 ( 74.90)	Acc@5  92.97 ( 95.39)
Epoch: [17][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6649e-01 (8.3073e-01)	Acc@1  68.75 ( 74.86)	Acc@5  94.53 ( 95.39)
Epoch: [17][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2340e-01 (8.3264e-01)	Acc@1  72.66 ( 74.82)	Acc@5  95.31 ( 95.37)
Epoch: [17][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1903e-01 (8.3363e-01)	Acc@1  70.31 ( 74.77)	Acc@5  91.41 ( 95.35)
Epoch: [17][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0536e-01 (8.3592e-01)	Acc@1  75.00 ( 74.70)	Acc@5  95.31 ( 95.34)
Epoch: [17][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6532e-01 (8.3775e-01)	Acc@1  71.09 ( 74.61)	Acc@5  94.53 ( 95.33)
Epoch: [17][380/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6954e-01 (8.4064e-01)	Acc@1  71.09 ( 74.52)	Acc@5  95.31 ( 95.31)
Epoch: [17][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.0307e+00 (8.4169e-01)	Acc@1  75.00 ( 74.49)	Acc@5  91.25 ( 95.30)
## e[17] optimizer.zero_grad (sum) time: 0.6755120754241943
## e[17]       loss.backward (sum) time: 13.795805931091309
## e[17]      optimizer.step (sum) time: 14.766805648803711
## epoch[17] training(only) time: 54.09162998199463
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.4245e+00 (1.4245e+00)	Acc@1  62.00 ( 62.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.050 ( 0.062)	Loss 1.6248e+00 (1.4570e+00)	Acc@1  63.00 ( 64.36)	Acc@5  87.00 ( 87.09)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.1395e+00 (1.3866e+00)	Acc@1  69.00 ( 65.05)	Acc@5  92.00 ( 88.38)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.7144e+00 (1.3972e+00)	Acc@1  55.00 ( 64.13)	Acc@5  83.00 ( 88.74)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.4030e+00 (1.4024e+00)	Acc@1  66.00 ( 63.12)	Acc@5  87.00 ( 88.80)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.3774e+00 (1.4036e+00)	Acc@1  61.00 ( 62.80)	Acc@5  88.00 ( 88.69)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.3309e+00 (1.3962e+00)	Acc@1  67.00 ( 62.89)	Acc@5  91.00 ( 88.77)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.6604e+00 (1.4021e+00)	Acc@1  58.00 ( 63.04)	Acc@5  88.00 ( 88.68)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.6064e+00 (1.4131e+00)	Acc@1  61.00 ( 62.93)	Acc@5  88.00 ( 88.53)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.6875e+00 (1.3969e+00)	Acc@1  54.00 ( 62.97)	Acc@5  88.00 ( 88.75)
 * Acc@1 63.040 Acc@5 88.790
### epoch[17] execution time: 59.11351799964905
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.296 ( 0.296)	Data  0.147 ( 0.147)	Loss 7.5608e-01 (7.5608e-01)	Acc@1  79.69 ( 79.69)	Acc@5  96.09 ( 96.09)
Epoch: [18][ 10/391]	Time  0.135 ( 0.152)	Data  0.001 ( 0.014)	Loss 6.3095e-01 (7.7080e-01)	Acc@1  80.47 ( 75.85)	Acc@5  98.44 ( 96.38)
Epoch: [18][ 20/391]	Time  0.134 ( 0.146)	Data  0.001 ( 0.008)	Loss 6.0721e-01 (7.4088e-01)	Acc@1  82.81 ( 77.19)	Acc@5  97.66 ( 96.69)
Epoch: [18][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 5.6043e-01 (7.2138e-01)	Acc@1  82.03 ( 78.18)	Acc@5  98.44 ( 96.57)
Epoch: [18][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 6.2749e-01 (7.2108e-01)	Acc@1  81.25 ( 78.28)	Acc@5  98.44 ( 96.51)
Epoch: [18][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 8.4255e-01 (7.3616e-01)	Acc@1  75.78 ( 77.82)	Acc@5  94.53 ( 96.11)
Epoch: [18][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 7.9053e-01 (7.4378e-01)	Acc@1  77.34 ( 77.37)	Acc@5  93.75 ( 95.99)
Epoch: [18][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.2838e-01 (7.3854e-01)	Acc@1  75.78 ( 77.41)	Acc@5  97.66 ( 96.05)
Epoch: [18][ 80/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.9311e-01 (7.4729e-01)	Acc@1  76.56 ( 77.05)	Acc@5  95.31 ( 95.99)
Epoch: [18][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.9139e-01 (7.4414e-01)	Acc@1  79.69 ( 77.16)	Acc@5  96.09 ( 96.09)
Epoch: [18][100/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.4075e-01 (7.5362e-01)	Acc@1  69.53 ( 76.76)	Acc@5  96.09 ( 96.02)
Epoch: [18][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6141e-01 (7.5787e-01)	Acc@1  75.00 ( 76.61)	Acc@5  96.09 ( 96.03)
Epoch: [18][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8338e-01 (7.5761e-01)	Acc@1  70.31 ( 76.61)	Acc@5  93.75 ( 96.03)
Epoch: [18][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0323e+00 (7.6147e-01)	Acc@1  67.97 ( 76.47)	Acc@5  95.31 ( 96.03)
Epoch: [18][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0126e+00 (7.6769e-01)	Acc@1  71.88 ( 76.26)	Acc@5  93.75 ( 96.01)
Epoch: [18][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8038e-01 (7.6739e-01)	Acc@1  78.91 ( 76.32)	Acc@5  95.31 ( 96.05)
Epoch: [18][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8042e-01 (7.6616e-01)	Acc@1  75.78 ( 76.40)	Acc@5  95.31 ( 96.06)
Epoch: [18][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6827e-01 (7.6686e-01)	Acc@1  74.22 ( 76.36)	Acc@5  93.75 ( 96.04)
Epoch: [18][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1137e-01 (7.6675e-01)	Acc@1  75.78 ( 76.36)	Acc@5  95.31 ( 96.04)
Epoch: [18][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2191e-01 (7.6640e-01)	Acc@1  75.78 ( 76.39)	Acc@5  96.88 ( 96.04)
Epoch: [18][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.9820e-01 (7.6924e-01)	Acc@1  71.09 ( 76.31)	Acc@5  94.53 ( 96.05)
Epoch: [18][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.7189e-01 (7.6994e-01)	Acc@1  77.34 ( 76.31)	Acc@5  97.66 ( 96.06)
Epoch: [18][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8705e-01 (7.7120e-01)	Acc@1  80.47 ( 76.27)	Acc@5  95.31 ( 96.07)
Epoch: [18][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1967e-01 (7.7529e-01)	Acc@1  75.78 ( 76.15)	Acc@5  92.97 ( 96.02)
Epoch: [18][240/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2182e-01 (7.7724e-01)	Acc@1  74.22 ( 76.12)	Acc@5  97.66 ( 95.97)
Epoch: [18][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5923e-01 (7.8012e-01)	Acc@1  78.91 ( 76.03)	Acc@5  97.66 ( 95.97)
Epoch: [18][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5581e-01 (7.8055e-01)	Acc@1  82.03 ( 75.98)	Acc@5 100.00 ( 95.98)
Epoch: [18][270/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5551e-01 (7.8169e-01)	Acc@1  78.12 ( 75.94)	Acc@5  97.66 ( 95.97)
Epoch: [18][280/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3111e-01 (7.8292e-01)	Acc@1  77.34 ( 75.93)	Acc@5  96.88 ( 95.97)
Epoch: [18][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0171e-01 (7.8453e-01)	Acc@1  78.12 ( 75.88)	Acc@5  96.09 ( 95.94)
Epoch: [18][300/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3572e-01 (7.8854e-01)	Acc@1  78.12 ( 75.74)	Acc@5  98.44 ( 95.90)
Epoch: [18][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2914e-01 (7.9026e-01)	Acc@1  71.88 ( 75.71)	Acc@5  95.31 ( 95.91)
Epoch: [18][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1735e-01 (7.9173e-01)	Acc@1  78.91 ( 75.67)	Acc@5  94.53 ( 95.91)
Epoch: [18][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0105e+00 (7.9249e-01)	Acc@1  66.41 ( 75.66)	Acc@5  91.41 ( 95.88)
Epoch: [18][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8151e-01 (7.9310e-01)	Acc@1  74.22 ( 75.63)	Acc@5  94.53 ( 95.87)
Epoch: [18][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2776e-01 (7.9336e-01)	Acc@1  75.78 ( 75.61)	Acc@5  95.31 ( 95.88)
Epoch: [18][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6606e-01 (7.9392e-01)	Acc@1  76.56 ( 75.58)	Acc@5  94.53 ( 95.86)
Epoch: [18][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.4548e-01 (7.9394e-01)	Acc@1  73.44 ( 75.56)	Acc@5  96.09 ( 95.85)
Epoch: [18][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.4338e-01 (7.9457e-01)	Acc@1  70.31 ( 75.54)	Acc@5  98.44 ( 95.87)
Epoch: [18][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2579e-01 (7.9704e-01)	Acc@1  73.75 ( 75.48)	Acc@5  93.75 ( 95.85)
## e[18] optimizer.zero_grad (sum) time: 0.6686410903930664
## e[18]       loss.backward (sum) time: 13.831250190734863
## e[18]      optimizer.step (sum) time: 14.743662357330322
## epoch[18] training(only) time: 53.97683048248291
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.3285e+00 (1.3285e+00)	Acc@1  62.00 ( 62.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.4624e+00 (1.4126e+00)	Acc@1  59.00 ( 64.55)	Acc@5  90.00 ( 88.36)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.3037e+00 (1.3873e+00)	Acc@1  63.00 ( 63.90)	Acc@5  85.00 ( 89.00)
Test: [ 30/100]	Time  0.057 ( 0.053)	Loss 1.9109e+00 (1.4282e+00)	Acc@1  52.00 ( 62.81)	Acc@5  86.00 ( 88.55)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.4860e+00 (1.4186e+00)	Acc@1  64.00 ( 63.07)	Acc@5  94.00 ( 89.02)
Test: [ 50/100]	Time  0.050 ( 0.051)	Loss 1.3479e+00 (1.4148e+00)	Acc@1  61.00 ( 63.00)	Acc@5  91.00 ( 88.98)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.3067e+00 (1.4132e+00)	Acc@1  67.00 ( 63.08)	Acc@5  92.00 ( 89.02)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.4504e+00 (1.4176e+00)	Acc@1  65.00 ( 62.85)	Acc@5  92.00 ( 89.04)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 1.7341e+00 (1.4253e+00)	Acc@1  60.00 ( 62.60)	Acc@5  84.00 ( 88.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7153e+00 (1.4116e+00)	Acc@1  53.00 ( 62.74)	Acc@5  85.00 ( 88.90)
 * Acc@1 62.910 Acc@5 88.940
### epoch[18] execution time: 58.99238443374634
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.290 ( 0.290)	Data  0.146 ( 0.146)	Loss 5.7165e-01 (5.7165e-01)	Acc@1  81.25 ( 81.25)	Acc@5 100.00 (100.00)
Epoch: [19][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.014)	Loss 8.7557e-01 (7.3380e-01)	Acc@1  76.56 ( 77.77)	Acc@5  95.31 ( 96.52)
Epoch: [19][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.008)	Loss 5.6631e-01 (6.8335e-01)	Acc@1  78.12 ( 78.65)	Acc@5  99.22 ( 96.76)
Epoch: [19][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 8.5260e-01 (6.8754e-01)	Acc@1  75.78 ( 78.58)	Acc@5  96.88 ( 96.95)
Epoch: [19][ 40/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.005)	Loss 7.6408e-01 (6.7344e-01)	Acc@1  74.22 ( 79.00)	Acc@5  96.88 ( 96.99)
Epoch: [19][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 8.2772e-01 (6.7698e-01)	Acc@1  75.78 ( 79.01)	Acc@5  97.66 ( 96.88)
Epoch: [19][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 6.6013e-01 (6.8493e-01)	Acc@1  82.81 ( 78.78)	Acc@5  98.44 ( 96.82)
Epoch: [19][ 70/391]	Time  0.146 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.9978e-01 (6.8262e-01)	Acc@1  80.47 ( 79.01)	Acc@5  96.88 ( 96.85)
Epoch: [19][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.6517e-01 (6.8902e-01)	Acc@1  78.91 ( 78.84)	Acc@5  98.44 ( 96.82)
Epoch: [19][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.1012e-01 (6.8647e-01)	Acc@1  80.47 ( 78.84)	Acc@5  96.88 ( 96.84)
Epoch: [19][100/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.6571e-01 (6.8344e-01)	Acc@1  78.12 ( 78.92)	Acc@5  99.22 ( 96.93)
Epoch: [19][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.0249e-01 (6.8413e-01)	Acc@1  74.22 ( 78.89)	Acc@5  93.75 ( 96.90)
Epoch: [19][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.5963e-01 (6.8762e-01)	Acc@1  77.34 ( 78.91)	Acc@5  97.66 ( 96.90)
Epoch: [19][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.1381e-01 (6.9328e-01)	Acc@1  75.78 ( 78.63)	Acc@5  96.88 ( 96.86)
Epoch: [19][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.2410e-01 (6.9599e-01)	Acc@1  78.12 ( 78.46)	Acc@5  96.09 ( 96.88)
Epoch: [19][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3309e-01 (6.9873e-01)	Acc@1  73.44 ( 78.34)	Acc@5  96.09 ( 96.84)
Epoch: [19][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6783e-01 (7.0300e-01)	Acc@1  82.81 ( 78.24)	Acc@5 100.00 ( 96.79)
Epoch: [19][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.4159e-01 (7.0280e-01)	Acc@1  78.12 ( 78.28)	Acc@5  96.88 ( 96.80)
Epoch: [19][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.0908e-01 (7.0784e-01)	Acc@1  71.88 ( 78.11)	Acc@5  94.53 ( 96.74)
Epoch: [19][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.7086e-01 (7.1096e-01)	Acc@1  81.25 ( 77.99)	Acc@5  96.88 ( 96.68)
Epoch: [19][200/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.8311e-01 (7.1261e-01)	Acc@1  76.56 ( 77.96)	Acc@5  96.09 ( 96.64)
Epoch: [19][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.7938e-01 (7.0950e-01)	Acc@1  79.69 ( 78.03)	Acc@5  97.66 ( 96.70)
Epoch: [19][220/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.0690e-01 (7.1309e-01)	Acc@1  68.75 ( 77.88)	Acc@5  98.44 ( 96.68)
Epoch: [19][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8157e-01 (7.1618e-01)	Acc@1  76.56 ( 77.84)	Acc@5  95.31 ( 96.63)
Epoch: [19][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0796e-01 (7.1729e-01)	Acc@1  77.34 ( 77.82)	Acc@5  97.66 ( 96.63)
Epoch: [19][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6378e-01 (7.1910e-01)	Acc@1  74.22 ( 77.74)	Acc@5  96.88 ( 96.61)
Epoch: [19][260/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6966e-01 (7.1956e-01)	Acc@1  76.56 ( 77.77)	Acc@5  97.66 ( 96.61)
Epoch: [19][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.3211e-01 (7.2369e-01)	Acc@1  70.31 ( 77.63)	Acc@5  96.09 ( 96.57)
Epoch: [19][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3000e-01 (7.2666e-01)	Acc@1  75.78 ( 77.54)	Acc@5  94.53 ( 96.53)
Epoch: [19][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3630e-01 (7.2771e-01)	Acc@1  81.25 ( 77.53)	Acc@5  98.44 ( 96.51)
Epoch: [19][300/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6663e-01 (7.2993e-01)	Acc@1  75.00 ( 77.47)	Acc@5  93.75 ( 96.48)
Epoch: [19][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1052e-01 (7.3104e-01)	Acc@1  75.78 ( 77.43)	Acc@5  96.88 ( 96.47)
Epoch: [19][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6717e-01 (7.3365e-01)	Acc@1  75.00 ( 77.36)	Acc@5  92.97 ( 96.43)
Epoch: [19][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0007e+00 (7.3544e-01)	Acc@1  66.41 ( 77.28)	Acc@5  93.75 ( 96.40)
Epoch: [19][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8458e-01 (7.3660e-01)	Acc@1  74.22 ( 77.23)	Acc@5  94.53 ( 96.40)
Epoch: [19][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6320e-01 (7.3671e-01)	Acc@1  75.78 ( 77.21)	Acc@5  93.75 ( 96.38)
Epoch: [19][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2327e-01 (7.3815e-01)	Acc@1  77.34 ( 77.19)	Acc@5  96.09 ( 96.36)
Epoch: [19][370/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5362e-01 (7.3845e-01)	Acc@1  78.12 ( 77.18)	Acc@5  94.53 ( 96.34)
Epoch: [19][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0361e-01 (7.3949e-01)	Acc@1  75.00 ( 77.15)	Acc@5  92.97 ( 96.33)
Epoch: [19][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1701e-01 (7.4244e-01)	Acc@1  72.50 ( 77.03)	Acc@5  95.00 ( 96.31)
## e[19] optimizer.zero_grad (sum) time: 0.6746335029602051
## e[19]       loss.backward (sum) time: 13.82163953781128
## e[19]      optimizer.step (sum) time: 14.754919528961182
## epoch[19] training(only) time: 54.07038879394531
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 1.3172e+00 (1.3172e+00)	Acc@1  69.00 ( 69.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.5497e+00 (1.4151e+00)	Acc@1  64.00 ( 65.09)	Acc@5  88.00 ( 88.09)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.4521e+00 (1.3703e+00)	Acc@1  69.00 ( 65.62)	Acc@5  85.00 ( 89.33)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5956e+00 (1.3545e+00)	Acc@1  53.00 ( 65.29)	Acc@5  89.00 ( 89.13)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.3673e+00 (1.3528e+00)	Acc@1  67.00 ( 65.00)	Acc@5  87.00 ( 89.22)
Test: [ 50/100]	Time  0.050 ( 0.051)	Loss 1.2477e+00 (1.3502e+00)	Acc@1  63.00 ( 64.82)	Acc@5  92.00 ( 89.22)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.3195e+00 (1.3345e+00)	Acc@1  65.00 ( 65.11)	Acc@5  89.00 ( 89.33)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 1.2942e+00 (1.3406e+00)	Acc@1  66.00 ( 65.07)	Acc@5  88.00 ( 89.11)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.5094e+00 (1.3489e+00)	Acc@1  62.00 ( 64.84)	Acc@5  85.00 ( 88.95)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.9379e+00 (1.3410e+00)	Acc@1  53.00 ( 64.90)	Acc@5  85.00 ( 89.07)
 * Acc@1 64.820 Acc@5 88.990
### epoch[19] execution time: 59.08550477027893
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.312 ( 0.312)	Data  0.164 ( 0.164)	Loss 8.3768e-01 (8.3768e-01)	Acc@1  71.88 ( 71.88)	Acc@5  92.97 ( 92.97)
Epoch: [20][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.016)	Loss 6.4790e-01 (6.3055e-01)	Acc@1  78.12 ( 80.33)	Acc@5  97.66 ( 97.16)
Epoch: [20][ 20/391]	Time  0.139 ( 0.146)	Data  0.001 ( 0.009)	Loss 7.7165e-01 (6.5839e-01)	Acc@1  69.53 ( 78.94)	Acc@5  96.88 ( 97.25)
Epoch: [20][ 30/391]	Time  0.138 ( 0.144)	Data  0.001 ( 0.006)	Loss 5.6797e-01 (6.4582e-01)	Acc@1  78.12 ( 79.08)	Acc@5  98.44 ( 97.28)
Epoch: [20][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 5.8956e-01 (6.5325e-01)	Acc@1  86.72 ( 79.31)	Acc@5  96.09 ( 97.24)
Epoch: [20][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.5483e-01 (6.5066e-01)	Acc@1  88.28 ( 79.46)	Acc@5  99.22 ( 97.30)
Epoch: [20][ 60/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 6.9816e-01 (6.5157e-01)	Acc@1  81.25 ( 79.46)	Acc@5  96.88 ( 97.32)
Epoch: [20][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.0364e-01 (6.4929e-01)	Acc@1  75.78 ( 79.70)	Acc@5  98.44 ( 97.36)
Epoch: [20][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.5883e-01 (6.4471e-01)	Acc@1  80.47 ( 79.69)	Acc@5  98.44 ( 97.40)
Epoch: [20][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.9920e-01 (6.4148e-01)	Acc@1  76.56 ( 79.93)	Acc@5  95.31 ( 97.40)
Epoch: [20][100/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.3901e-01 (6.4005e-01)	Acc@1  77.34 ( 79.85)	Acc@5  97.66 ( 97.39)
Epoch: [20][110/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.7854e-01 (6.4728e-01)	Acc@1  77.34 ( 79.67)	Acc@5  97.66 ( 97.29)
Epoch: [20][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.3765e-01 (6.4523e-01)	Acc@1  84.38 ( 79.73)	Acc@5  97.66 ( 97.38)
Epoch: [20][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.1263e-01 (6.5001e-01)	Acc@1  81.25 ( 79.59)	Acc@5  97.66 ( 97.39)
Epoch: [20][140/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.1724e-01 (6.5700e-01)	Acc@1  77.34 ( 79.46)	Acc@5  96.09 ( 97.30)
Epoch: [20][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.2903e-01 (6.6523e-01)	Acc@1  73.44 ( 79.23)	Acc@5  96.88 ( 97.22)
Epoch: [20][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5268e-01 (6.6525e-01)	Acc@1  81.25 ( 79.23)	Acc@5  97.66 ( 97.22)
Epoch: [20][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.3738e-01 (6.6871e-01)	Acc@1  76.56 ( 79.13)	Acc@5  91.41 ( 97.15)
Epoch: [20][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5848e-01 (6.7165e-01)	Acc@1  73.44 ( 79.08)	Acc@5  96.88 ( 97.11)
Epoch: [20][190/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.5760e-01 (6.7661e-01)	Acc@1  73.44 ( 78.92)	Acc@5  96.09 ( 97.07)
Epoch: [20][200/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.5195e-01 (6.8097e-01)	Acc@1  76.56 ( 78.82)	Acc@5  96.88 ( 97.03)
Epoch: [20][210/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4609e-01 (6.8238e-01)	Acc@1  83.59 ( 78.83)	Acc@5  97.66 ( 96.96)
Epoch: [20][220/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.5270e-01 (6.8573e-01)	Acc@1  79.69 ( 78.73)	Acc@5  97.66 ( 96.91)
Epoch: [20][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3213e-01 (6.8747e-01)	Acc@1  75.78 ( 78.69)	Acc@5  93.75 ( 96.87)
Epoch: [20][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5807e-01 (6.8870e-01)	Acc@1  77.34 ( 78.66)	Acc@5  93.75 ( 96.87)
Epoch: [20][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6777e-01 (6.9324e-01)	Acc@1  81.25 ( 78.57)	Acc@5  96.88 ( 96.83)
Epoch: [20][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.8952e-01 (6.9711e-01)	Acc@1  71.88 ( 78.42)	Acc@5  94.53 ( 96.79)
Epoch: [20][270/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8790e-01 (7.0078e-01)	Acc@1  76.56 ( 78.27)	Acc@5  95.31 ( 96.75)
Epoch: [20][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8741e-01 (7.0339e-01)	Acc@1  77.34 ( 78.19)	Acc@5  94.53 ( 96.75)
Epoch: [20][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.4269e-01 (7.0588e-01)	Acc@1  74.22 ( 78.15)	Acc@5  96.09 ( 96.72)
Epoch: [20][300/391]	Time  0.150 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9994e-01 (7.0876e-01)	Acc@1  75.00 ( 78.10)	Acc@5  97.66 ( 96.68)
Epoch: [20][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0670e-01 (7.1120e-01)	Acc@1  79.69 ( 78.10)	Acc@5  96.88 ( 96.65)
Epoch: [20][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7028e-01 (7.1369e-01)	Acc@1  77.34 ( 78.04)	Acc@5  97.66 ( 96.62)
Epoch: [20][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3979e-01 (7.1601e-01)	Acc@1  71.09 ( 77.98)	Acc@5  95.31 ( 96.58)
Epoch: [20][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3876e-01 (7.1714e-01)	Acc@1  80.47 ( 77.96)	Acc@5  96.88 ( 96.58)
Epoch: [20][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7944e-01 (7.1841e-01)	Acc@1  80.47 ( 77.93)	Acc@5  98.44 ( 96.57)
Epoch: [20][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5602e-01 (7.2059e-01)	Acc@1  71.88 ( 77.84)	Acc@5  92.97 ( 96.54)
Epoch: [20][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.4598e-01 (7.2102e-01)	Acc@1  75.78 ( 77.82)	Acc@5  92.97 ( 96.54)
Epoch: [20][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8121e-01 (7.2116e-01)	Acc@1  78.12 ( 77.81)	Acc@5  96.88 ( 96.56)
Epoch: [20][390/391]	Time  0.105 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6687e-01 (7.2356e-01)	Acc@1  72.50 ( 77.72)	Acc@5  92.50 ( 96.54)
## e[20] optimizer.zero_grad (sum) time: 0.677513599395752
## e[20]       loss.backward (sum) time: 13.832757234573364
## e[20]      optimizer.step (sum) time: 14.7299644947052
## epoch[20] training(only) time: 54.06389379501343
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.2633e+00 (1.2633e+00)	Acc@1  69.00 ( 69.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.6029e+00 (1.4809e+00)	Acc@1  59.00 ( 63.18)	Acc@5  89.00 ( 87.45)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 1.5319e+00 (1.4278e+00)	Acc@1  63.00 ( 64.14)	Acc@5  89.00 ( 88.62)
Test: [ 30/100]	Time  0.055 ( 0.053)	Loss 1.7249e+00 (1.4190e+00)	Acc@1  59.00 ( 64.35)	Acc@5  84.00 ( 88.42)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 1.3201e+00 (1.4069e+00)	Acc@1  70.00 ( 64.59)	Acc@5  90.00 ( 88.68)
Test: [ 50/100]	Time  0.052 ( 0.051)	Loss 1.2170e+00 (1.3919e+00)	Acc@1  62.00 ( 64.63)	Acc@5  90.00 ( 88.67)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.3251e+00 (1.3876e+00)	Acc@1  63.00 ( 64.46)	Acc@5  94.00 ( 88.97)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3580e+00 (1.3856e+00)	Acc@1  61.00 ( 64.32)	Acc@5  91.00 ( 89.03)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 1.4629e+00 (1.3938e+00)	Acc@1  67.00 ( 64.11)	Acc@5  87.00 ( 88.89)
Test: [ 90/100]	Time  0.052 ( 0.049)	Loss 1.7780e+00 (1.3810e+00)	Acc@1  59.00 ( 64.54)	Acc@5  87.00 ( 89.08)
 * Acc@1 64.720 Acc@5 89.130
### epoch[20] execution time: 59.09221076965332
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.305 ( 0.305)	Data  0.129 ( 0.129)	Loss 4.4254e-01 (4.4254e-01)	Acc@1  84.38 ( 84.38)	Acc@5  98.44 ( 98.44)
Epoch: [21][ 10/391]	Time  0.137 ( 0.151)	Data  0.001 ( 0.013)	Loss 8.0015e-01 (6.4482e-01)	Acc@1  71.09 ( 79.26)	Acc@5  96.88 ( 97.16)
Epoch: [21][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.007)	Loss 8.5455e-01 (6.3458e-01)	Acc@1  73.44 ( 80.10)	Acc@5  93.75 ( 97.17)
Epoch: [21][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.005)	Loss 5.9723e-01 (6.2360e-01)	Acc@1  83.59 ( 80.72)	Acc@5  96.88 ( 97.25)
Epoch: [21][ 40/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.4666e-01 (6.2850e-01)	Acc@1  81.25 ( 80.79)	Acc@5  96.88 ( 97.35)
Epoch: [21][ 50/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.3155e-01 (6.2281e-01)	Acc@1  82.03 ( 81.13)	Acc@5  99.22 ( 97.47)
Epoch: [21][ 60/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.7723e-01 (6.1663e-01)	Acc@1  79.69 ( 81.38)	Acc@5  93.75 ( 97.46)
Epoch: [21][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.3639e-01 (6.1531e-01)	Acc@1  83.59 ( 81.35)	Acc@5  99.22 ( 97.47)
Epoch: [21][ 80/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.1677e-01 (6.1410e-01)	Acc@1  82.81 ( 81.22)	Acc@5  98.44 ( 97.56)
Epoch: [21][ 90/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9108e-01 (6.1613e-01)	Acc@1  85.16 ( 81.16)	Acc@5  97.66 ( 97.55)
Epoch: [21][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.7657e-01 (6.1919e-01)	Acc@1  74.22 ( 80.96)	Acc@5  96.88 ( 97.50)
Epoch: [21][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9584e-01 (6.2107e-01)	Acc@1  72.66 ( 80.78)	Acc@5  95.31 ( 97.55)
Epoch: [21][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2461e-01 (6.2073e-01)	Acc@1  79.69 ( 80.64)	Acc@5  98.44 ( 97.56)
Epoch: [21][130/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0170e-01 (6.2106e-01)	Acc@1  82.03 ( 80.68)	Acc@5  99.22 ( 97.55)
Epoch: [21][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8174e-01 (6.2306e-01)	Acc@1  81.25 ( 80.61)	Acc@5  96.88 ( 97.52)
Epoch: [21][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3688e-01 (6.2730e-01)	Acc@1  74.22 ( 80.49)	Acc@5  98.44 ( 97.56)
Epoch: [21][160/391]	Time  0.161 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.5087e-01 (6.2851e-01)	Acc@1  78.12 ( 80.48)	Acc@5  93.75 ( 97.50)
Epoch: [21][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5573e-01 (6.3005e-01)	Acc@1  78.91 ( 80.51)	Acc@5  94.53 ( 97.50)
Epoch: [21][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9857e-01 (6.3260e-01)	Acc@1  79.69 ( 80.43)	Acc@5  97.66 ( 97.45)
Epoch: [21][190/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5188e-01 (6.3756e-01)	Acc@1  74.22 ( 80.27)	Acc@5  96.88 ( 97.40)
Epoch: [21][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8473e-01 (6.4376e-01)	Acc@1  72.66 ( 80.04)	Acc@5  96.88 ( 97.36)
Epoch: [21][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3082e-01 (6.4806e-01)	Acc@1  76.56 ( 79.94)	Acc@5  97.66 ( 97.32)
Epoch: [21][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0596e-01 (6.5216e-01)	Acc@1  78.91 ( 79.78)	Acc@5  99.22 ( 97.29)
Epoch: [21][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.0083e-01 (6.5516e-01)	Acc@1  75.78 ( 79.67)	Acc@5  96.09 ( 97.26)
Epoch: [21][240/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0964e-01 (6.6243e-01)	Acc@1  71.88 ( 79.46)	Acc@5  92.97 ( 97.15)
Epoch: [21][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1588e-01 (6.6377e-01)	Acc@1  71.88 ( 79.44)	Acc@5  94.53 ( 97.14)
Epoch: [21][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5576e-01 (6.6365e-01)	Acc@1  76.56 ( 79.42)	Acc@5  97.66 ( 97.14)
Epoch: [21][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4523e-01 (6.6510e-01)	Acc@1  81.25 ( 79.37)	Acc@5  99.22 ( 97.15)
Epoch: [21][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5045e-01 (6.6772e-01)	Acc@1  78.12 ( 79.34)	Acc@5  96.09 ( 97.13)
Epoch: [21][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.9584e-01 (6.7009e-01)	Acc@1  70.31 ( 79.26)	Acc@5  92.97 ( 97.10)
Epoch: [21][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 7.1633e-01 (6.7312e-01)	Acc@1  77.34 ( 79.20)	Acc@5  94.53 ( 97.06)
Epoch: [21][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 7.1754e-01 (6.7407e-01)	Acc@1  75.78 ( 79.18)	Acc@5  96.88 ( 97.07)
Epoch: [21][320/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.001)	Loss 7.0799e-01 (6.7710e-01)	Acc@1  77.34 ( 79.09)	Acc@5  96.88 ( 97.05)
Epoch: [21][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.0382e-01 (6.7876e-01)	Acc@1  77.34 ( 78.98)	Acc@5  97.66 ( 97.01)
Epoch: [21][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.0164e-01 (6.8060e-01)	Acc@1  80.47 ( 78.90)	Acc@5  96.88 ( 96.99)
Epoch: [21][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 8.1850e-01 (6.8204e-01)	Acc@1  75.00 ( 78.87)	Acc@5  95.31 ( 96.98)
Epoch: [21][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.001)	Loss 7.1190e-01 (6.8350e-01)	Acc@1  77.34 ( 78.82)	Acc@5  97.66 ( 96.96)
Epoch: [21][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.0038e-01 (6.8418e-01)	Acc@1  81.25 ( 78.81)	Acc@5  96.09 ( 96.95)
Epoch: [21][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.0907e+00 (6.8691e-01)	Acc@1  69.53 ( 78.74)	Acc@5  94.53 ( 96.91)
Epoch: [21][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.001)	Loss 8.0641e-01 (6.9016e-01)	Acc@1  76.25 ( 78.64)	Acc@5  97.50 ( 96.88)
## e[21] optimizer.zero_grad (sum) time: 0.6675219535827637
## e[21]       loss.backward (sum) time: 13.82196593284607
## e[21]      optimizer.step (sum) time: 14.755051851272583
## epoch[21] training(only) time: 53.97821378707886
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.5193e+00 (1.5193e+00)	Acc@1  69.00 ( 69.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.9813e+00 (1.6018e+00)	Acc@1  58.00 ( 62.64)	Acc@5  84.00 ( 86.27)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 1.1808e+00 (1.4718e+00)	Acc@1  67.00 ( 64.81)	Acc@5  92.00 ( 87.67)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6437e+00 (1.4842e+00)	Acc@1  54.00 ( 64.00)	Acc@5  89.00 ( 88.00)
Test: [ 40/100]	Time  0.045 ( 0.052)	Loss 1.3330e+00 (1.4941e+00)	Acc@1  70.00 ( 63.54)	Acc@5  95.00 ( 88.39)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.4377e+00 (1.4825e+00)	Acc@1  63.00 ( 63.51)	Acc@5  92.00 ( 88.27)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2175e+00 (1.4711e+00)	Acc@1  64.00 ( 63.46)	Acc@5  97.00 ( 88.48)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.6862e+00 (1.4795e+00)	Acc@1  62.00 ( 63.51)	Acc@5  88.00 ( 88.41)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.6943e+00 (1.4874e+00)	Acc@1  60.00 ( 63.30)	Acc@5  83.00 ( 88.31)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 1.7780e+00 (1.4740e+00)	Acc@1  58.00 ( 63.53)	Acc@5  85.00 ( 88.37)
 * Acc@1 63.670 Acc@5 88.440
### epoch[21] execution time: 58.975435972213745
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.298 ( 0.298)	Data  0.141 ( 0.141)	Loss 4.9324e-01 (4.9324e-01)	Acc@1  83.59 ( 83.59)	Acc@5  98.44 ( 98.44)
Epoch: [22][ 10/391]	Time  0.139 ( 0.152)	Data  0.001 ( 0.014)	Loss 4.7919e-01 (5.5656e-01)	Acc@1  82.03 ( 81.82)	Acc@5  98.44 ( 97.73)
Epoch: [22][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 4.7912e-01 (5.9252e-01)	Acc@1  85.16 ( 81.25)	Acc@5  97.66 ( 97.77)
Epoch: [22][ 30/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.006)	Loss 4.6965e-01 (5.7275e-01)	Acc@1  85.16 ( 81.93)	Acc@5  96.88 ( 97.88)
Epoch: [22][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 6.5526e-01 (5.7555e-01)	Acc@1  76.56 ( 81.75)	Acc@5  97.66 ( 97.87)
Epoch: [22][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.1709e-01 (5.7758e-01)	Acc@1  83.59 ( 81.57)	Acc@5  98.44 ( 97.82)
Epoch: [22][ 60/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.6854e-01 (5.6644e-01)	Acc@1  82.81 ( 81.97)	Acc@5  98.44 ( 97.95)
Epoch: [22][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.6538e-01 (5.6690e-01)	Acc@1  78.12 ( 81.92)	Acc@5  99.22 ( 97.95)
Epoch: [22][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.6436e-01 (5.7300e-01)	Acc@1  78.91 ( 81.74)	Acc@5  97.66 ( 97.96)
Epoch: [22][ 90/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.6297e-01 (5.7648e-01)	Acc@1  84.38 ( 81.73)	Acc@5  97.66 ( 97.91)
Epoch: [22][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.2602e-01 (5.7871e-01)	Acc@1  85.16 ( 81.61)	Acc@5  98.44 ( 97.90)
Epoch: [22][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.9021e-01 (5.8344e-01)	Acc@1  81.25 ( 81.58)	Acc@5  97.66 ( 97.80)
Epoch: [22][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.1857e-01 (5.8785e-01)	Acc@1  77.34 ( 81.37)	Acc@5  96.09 ( 97.77)
Epoch: [22][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.9439e-01 (5.9094e-01)	Acc@1  78.91 ( 81.31)	Acc@5  94.53 ( 97.72)
Epoch: [22][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8493e-01 (5.9330e-01)	Acc@1  78.91 ( 81.26)	Acc@5  98.44 ( 97.72)
Epoch: [22][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3933e-01 (5.9959e-01)	Acc@1  83.59 ( 81.09)	Acc@5  96.09 ( 97.67)
Epoch: [22][160/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8952e-01 (6.0163e-01)	Acc@1  86.72 ( 81.03)	Acc@5  99.22 ( 97.69)
Epoch: [22][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6368e-01 (6.0159e-01)	Acc@1  78.12 ( 80.96)	Acc@5  97.66 ( 97.72)
Epoch: [22][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5352e-01 (6.0470e-01)	Acc@1  75.00 ( 80.91)	Acc@5  97.66 ( 97.68)
Epoch: [22][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5632e-01 (6.0840e-01)	Acc@1  78.91 ( 80.90)	Acc@5  96.09 ( 97.65)
Epoch: [22][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7232e-01 (6.0998e-01)	Acc@1  78.91 ( 80.91)	Acc@5  98.44 ( 97.65)
Epoch: [22][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9561e-01 (6.1289e-01)	Acc@1  78.12 ( 80.77)	Acc@5  96.88 ( 97.63)
Epoch: [22][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8125e-01 (6.1510e-01)	Acc@1  71.09 ( 80.70)	Acc@5  92.19 ( 97.60)
Epoch: [22][230/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2950e-01 (6.1787e-01)	Acc@1  78.91 ( 80.61)	Acc@5  97.66 ( 97.57)
Epoch: [22][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1835e-01 (6.2206e-01)	Acc@1  77.34 ( 80.45)	Acc@5  97.66 ( 97.54)
Epoch: [22][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6524e-01 (6.2562e-01)	Acc@1  75.00 ( 80.33)	Acc@5  96.09 ( 97.49)
Epoch: [22][260/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9736e-01 (6.2911e-01)	Acc@1  81.25 ( 80.23)	Acc@5  94.53 ( 97.45)
Epoch: [22][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5968e-01 (6.3368e-01)	Acc@1  75.78 ( 80.16)	Acc@5  92.97 ( 97.35)
Epoch: [22][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2771e-01 (6.3612e-01)	Acc@1  73.44 ( 80.07)	Acc@5  96.09 ( 97.35)
Epoch: [22][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8875e-01 (6.3699e-01)	Acc@1  79.69 ( 80.04)	Acc@5  95.31 ( 97.32)
Epoch: [22][300/391]	Time  0.155 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2991e-01 (6.3895e-01)	Acc@1  73.44 ( 79.98)	Acc@5  96.09 ( 97.31)
Epoch: [22][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8186e-01 (6.4171e-01)	Acc@1  78.91 ( 79.91)	Acc@5  93.75 ( 97.27)
Epoch: [22][320/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1087e-01 (6.4285e-01)	Acc@1  75.00 ( 79.87)	Acc@5  98.44 ( 97.28)
Epoch: [22][330/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5094e-01 (6.4515e-01)	Acc@1  73.44 ( 79.77)	Acc@5  96.09 ( 97.26)
Epoch: [22][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6611e-01 (6.4787e-01)	Acc@1  75.00 ( 79.71)	Acc@5  94.53 ( 97.23)
Epoch: [22][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9374e-01 (6.5075e-01)	Acc@1  78.91 ( 79.59)	Acc@5  98.44 ( 97.20)
Epoch: [22][360/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.2217e-01 (6.5191e-01)	Acc@1  78.91 ( 79.58)	Acc@5  94.53 ( 97.19)
Epoch: [22][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3697e-01 (6.5330e-01)	Acc@1  80.47 ( 79.53)	Acc@5  98.44 ( 97.19)
Epoch: [22][380/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7763e-01 (6.5520e-01)	Acc@1  79.69 ( 79.50)	Acc@5  96.09 ( 97.14)
Epoch: [22][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.001)	Loss 8.0058e-01 (6.5665e-01)	Acc@1  78.75 ( 79.48)	Acc@5  96.25 ( 97.13)
## e[22] optimizer.zero_grad (sum) time: 0.6685571670532227
## e[22]       loss.backward (sum) time: 13.847295045852661
## e[22]      optimizer.step (sum) time: 14.715161561965942
## epoch[22] training(only) time: 53.97693586349487
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.3637e+00 (1.3637e+00)	Acc@1  69.00 ( 69.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 1.9319e+00 (1.5652e+00)	Acc@1  60.00 ( 63.91)	Acc@5  88.00 ( 87.09)
Test: [ 20/100]	Time  0.046 ( 0.056)	Loss 1.2953e+00 (1.4758e+00)	Acc@1  70.00 ( 63.95)	Acc@5  90.00 ( 88.05)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.9016e+00 (1.4941e+00)	Acc@1  53.00 ( 63.23)	Acc@5  87.00 ( 88.16)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 1.5472e+00 (1.4781e+00)	Acc@1  61.00 ( 63.22)	Acc@5  91.00 ( 88.41)
Test: [ 50/100]	Time  0.045 ( 0.051)	Loss 1.2966e+00 (1.4814e+00)	Acc@1  63.00 ( 63.29)	Acc@5  91.00 ( 88.25)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1585e+00 (1.4692e+00)	Acc@1  67.00 ( 63.44)	Acc@5  93.00 ( 88.69)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.6320e+00 (1.4715e+00)	Acc@1  66.00 ( 63.65)	Acc@5  88.00 ( 88.69)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6715e+00 (1.4778e+00)	Acc@1  61.00 ( 63.56)	Acc@5  85.00 ( 88.63)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7127e+00 (1.4635e+00)	Acc@1  60.00 ( 63.77)	Acc@5  84.00 ( 88.79)
 * Acc@1 63.770 Acc@5 88.760
### epoch[22] execution time: 58.98154163360596
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.305 ( 0.305)	Data  0.159 ( 0.159)	Loss 5.3647e-01 (5.3647e-01)	Acc@1  83.59 ( 83.59)	Acc@5  97.66 ( 97.66)
Epoch: [23][ 10/391]	Time  0.134 ( 0.153)	Data  0.001 ( 0.015)	Loss 4.8008e-01 (5.4979e-01)	Acc@1  85.94 ( 83.17)	Acc@5  98.44 ( 97.94)
Epoch: [23][ 20/391]	Time  0.135 ( 0.146)	Data  0.001 ( 0.009)	Loss 5.1630e-01 (5.6379e-01)	Acc@1  83.59 ( 82.66)	Acc@5  98.44 ( 97.77)
Epoch: [23][ 30/391]	Time  0.134 ( 0.144)	Data  0.001 ( 0.006)	Loss 6.9270e-01 (5.6606e-01)	Acc@1  78.91 ( 82.84)	Acc@5  96.09 ( 97.86)
Epoch: [23][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 4.7519e-01 (5.5236e-01)	Acc@1  83.59 ( 82.85)	Acc@5  98.44 ( 98.08)
Epoch: [23][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.1308e-01 (5.5593e-01)	Acc@1  83.59 ( 82.74)	Acc@5  98.44 ( 98.05)
Epoch: [23][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.3263e-01 (5.5160e-01)	Acc@1  84.38 ( 82.85)	Acc@5  96.88 ( 98.09)
Epoch: [23][ 70/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.0799e-01 (5.5377e-01)	Acc@1  80.47 ( 82.67)	Acc@5  98.44 ( 98.06)
Epoch: [23][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.2321e-01 (5.5901e-01)	Acc@1  85.16 ( 82.52)	Acc@5  96.88 ( 97.92)
Epoch: [23][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.3147e-01 (5.6191e-01)	Acc@1  85.94 ( 82.39)	Acc@5  97.66 ( 97.92)
Epoch: [23][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.7407e-01 (5.6542e-01)	Acc@1  80.47 ( 82.39)	Acc@5  94.53 ( 97.85)
Epoch: [23][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.2211e-01 (5.6469e-01)	Acc@1  83.59 ( 82.36)	Acc@5  98.44 ( 97.90)
Epoch: [23][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4690e-01 (5.6595e-01)	Acc@1  81.25 ( 82.26)	Acc@5  97.66 ( 97.85)
Epoch: [23][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.0239e-01 (5.6569e-01)	Acc@1  83.59 ( 82.31)	Acc@5  96.88 ( 97.91)
Epoch: [23][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6909e-01 (5.6793e-01)	Acc@1  78.12 ( 82.15)	Acc@5  97.66 ( 97.94)
Epoch: [23][150/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.5342e-01 (5.7231e-01)	Acc@1  79.69 ( 82.02)	Acc@5  96.88 ( 97.93)
Epoch: [23][160/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.4967e-01 (5.7664e-01)	Acc@1  75.00 ( 81.93)	Acc@5  95.31 ( 97.83)
Epoch: [23][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9376e-01 (5.7935e-01)	Acc@1  85.16 ( 81.82)	Acc@5  99.22 ( 97.84)
Epoch: [23][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.3402e-01 (5.8307e-01)	Acc@1  67.19 ( 81.68)	Acc@5  95.31 ( 97.82)
Epoch: [23][190/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2244e-01 (5.8600e-01)	Acc@1  81.25 ( 81.57)	Acc@5  97.66 ( 97.80)
Epoch: [23][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8289e-01 (5.8957e-01)	Acc@1  77.34 ( 81.41)	Acc@5  96.88 ( 97.80)
Epoch: [23][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6888e-01 (5.9488e-01)	Acc@1  75.00 ( 81.24)	Acc@5  96.88 ( 97.77)
Epoch: [23][220/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9993e-01 (5.9621e-01)	Acc@1  83.59 ( 81.16)	Acc@5  98.44 ( 97.77)
Epoch: [23][230/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0340e-01 (5.9785e-01)	Acc@1  85.16 ( 81.13)	Acc@5 100.00 ( 97.78)
Epoch: [23][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4391e-01 (6.0108e-01)	Acc@1  75.78 ( 81.04)	Acc@5  96.09 ( 97.75)
Epoch: [23][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1119e-01 (6.0298e-01)	Acc@1  78.12 ( 80.98)	Acc@5  99.22 ( 97.73)
Epoch: [23][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1138e-01 (6.0581e-01)	Acc@1  80.47 ( 80.92)	Acc@5  97.66 ( 97.70)
Epoch: [23][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8067e-01 (6.0648e-01)	Acc@1  78.12 ( 80.87)	Acc@5  97.66 ( 97.68)
Epoch: [23][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7139e-01 (6.0839e-01)	Acc@1  80.47 ( 80.76)	Acc@5  97.66 ( 97.70)
Epoch: [23][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4474e-01 (6.1157e-01)	Acc@1  83.59 ( 80.65)	Acc@5  97.66 ( 97.66)
Epoch: [23][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4237e-01 (6.1194e-01)	Acc@1  77.34 ( 80.66)	Acc@5  96.88 ( 97.65)
Epoch: [23][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7995e-01 (6.1447e-01)	Acc@1  79.69 ( 80.56)	Acc@5  96.09 ( 97.64)
Epoch: [23][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1573e-01 (6.1544e-01)	Acc@1  78.12 ( 80.55)	Acc@5  96.88 ( 97.64)
Epoch: [23][330/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6613e-01 (6.1746e-01)	Acc@1  81.25 ( 80.47)	Acc@5  98.44 ( 97.63)
Epoch: [23][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0426e-01 (6.1867e-01)	Acc@1  84.38 ( 80.44)	Acc@5  97.66 ( 97.62)
Epoch: [23][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8945e-01 (6.2088e-01)	Acc@1  78.12 ( 80.41)	Acc@5  96.09 ( 97.59)
Epoch: [23][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5653e-01 (6.2322e-01)	Acc@1  76.56 ( 80.35)	Acc@5  96.88 ( 97.55)
Epoch: [23][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9002e-01 (6.2525e-01)	Acc@1  77.34 ( 80.30)	Acc@5  96.09 ( 97.53)
Epoch: [23][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6435e-01 (6.2755e-01)	Acc@1  82.03 ( 80.23)	Acc@5  98.44 ( 97.50)
Epoch: [23][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9985e-01 (6.3003e-01)	Acc@1  73.75 ( 80.16)	Acc@5  96.25 ( 97.50)
## e[23] optimizer.zero_grad (sum) time: 0.66646409034729
## e[23]       loss.backward (sum) time: 13.865505456924438
## e[23]      optimizer.step (sum) time: 14.725579023361206
## epoch[23] training(only) time: 54.02199029922485
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.4215e+00 (1.4215e+00)	Acc@1  69.00 ( 69.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.5243e+00 (1.5410e+00)	Acc@1  60.00 ( 63.27)	Acc@5  87.00 ( 86.91)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.3740e+00 (1.5109e+00)	Acc@1  66.00 ( 63.05)	Acc@5  91.00 ( 87.19)
Test: [ 30/100]	Time  0.051 ( 0.053)	Loss 1.7368e+00 (1.5235e+00)	Acc@1  59.00 ( 62.42)	Acc@5  87.00 ( 86.90)
Test: [ 40/100]	Time  0.054 ( 0.051)	Loss 1.3172e+00 (1.5027e+00)	Acc@1  64.00 ( 62.61)	Acc@5  92.00 ( 87.41)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.4328e+00 (1.5001e+00)	Acc@1  67.00 ( 62.53)	Acc@5  85.00 ( 87.43)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1438e+00 (1.4751e+00)	Acc@1  66.00 ( 62.74)	Acc@5  94.00 ( 87.79)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.5170e+00 (1.4839e+00)	Acc@1  61.00 ( 62.68)	Acc@5  88.00 ( 87.77)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.5694e+00 (1.4921e+00)	Acc@1  64.00 ( 62.51)	Acc@5  88.00 ( 87.73)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.6305e+00 (1.4791e+00)	Acc@1  60.00 ( 62.75)	Acc@5  88.00 ( 87.91)
 * Acc@1 62.980 Acc@5 87.970
### epoch[23] execution time: 59.020938873291016
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.299 ( 0.299)	Data  0.154 ( 0.154)	Loss 6.4270e-01 (6.4270e-01)	Acc@1  83.59 ( 83.59)	Acc@5  96.88 ( 96.88)
Epoch: [24][ 10/391]	Time  0.136 ( 0.153)	Data  0.001 ( 0.015)	Loss 6.4244e-01 (5.4376e-01)	Acc@1  78.91 ( 82.95)	Acc@5  98.44 ( 98.30)
Epoch: [24][ 20/391]	Time  0.145 ( 0.146)	Data  0.001 ( 0.008)	Loss 6.9107e-01 (5.4709e-01)	Acc@1  80.47 ( 83.30)	Acc@5  97.66 ( 98.10)
Epoch: [24][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 5.3119e-01 (5.4684e-01)	Acc@1  84.38 ( 83.32)	Acc@5  99.22 ( 98.14)
Epoch: [24][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 6.9708e-01 (5.4556e-01)	Acc@1  78.12 ( 83.17)	Acc@5  92.19 ( 98.08)
Epoch: [24][ 50/391]	Time  0.148 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.7376e-01 (5.4442e-01)	Acc@1  84.38 ( 83.03)	Acc@5 100.00 ( 98.12)
Epoch: [24][ 60/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.6042e-01 (5.4722e-01)	Acc@1  80.47 ( 82.84)	Acc@5  98.44 ( 98.16)
Epoch: [24][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.1786e-01 (5.4984e-01)	Acc@1  85.94 ( 82.71)	Acc@5  97.66 ( 98.03)
Epoch: [24][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.3454e-01 (5.4574e-01)	Acc@1  84.38 ( 82.90)	Acc@5  96.88 ( 98.04)
Epoch: [24][ 90/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.2012e-01 (5.4538e-01)	Acc@1  81.25 ( 82.80)	Acc@5  96.88 ( 98.06)
Epoch: [24][100/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.2332e-01 (5.4950e-01)	Acc@1  78.12 ( 82.62)	Acc@5  97.66 ( 98.04)
Epoch: [24][110/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5873e-01 (5.5731e-01)	Acc@1  84.38 ( 82.34)	Acc@5  97.66 ( 97.95)
Epoch: [24][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1805e-01 (5.5597e-01)	Acc@1  82.03 ( 82.38)	Acc@5  98.44 ( 97.99)
Epoch: [24][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4796e-01 (5.5653e-01)	Acc@1  81.25 ( 82.37)	Acc@5  96.88 ( 97.97)
Epoch: [24][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.4958e-01 (5.5495e-01)	Acc@1  82.03 ( 82.44)	Acc@5  97.66 ( 97.99)
Epoch: [24][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7880e-01 (5.5568e-01)	Acc@1  84.38 ( 82.37)	Acc@5  96.88 ( 97.97)
Epoch: [24][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0651e-01 (5.5698e-01)	Acc@1  80.47 ( 82.31)	Acc@5  98.44 ( 97.98)
Epoch: [24][170/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3160e-01 (5.5888e-01)	Acc@1  82.81 ( 82.20)	Acc@5  98.44 ( 97.91)
Epoch: [24][180/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3018e-01 (5.6251e-01)	Acc@1  75.00 ( 82.13)	Acc@5  93.75 ( 97.89)
Epoch: [24][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5049e-01 (5.6420e-01)	Acc@1  86.72 ( 82.15)	Acc@5  96.88 ( 97.89)
Epoch: [24][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6195e-01 (5.6802e-01)	Acc@1  75.00 ( 82.00)	Acc@5  96.09 ( 97.85)
Epoch: [24][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7863e-01 (5.6885e-01)	Acc@1  84.38 ( 81.96)	Acc@5  99.22 ( 97.83)
Epoch: [24][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7489e-01 (5.6977e-01)	Acc@1  78.12 ( 81.98)	Acc@5  96.09 ( 97.83)
Epoch: [24][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.4978e-01 (5.7159e-01)	Acc@1  78.91 ( 81.92)	Acc@5  95.31 ( 97.85)
Epoch: [24][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3127e-01 (5.7132e-01)	Acc@1  78.91 ( 81.96)	Acc@5  97.66 ( 97.86)
Epoch: [24][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1907e-01 (5.7234e-01)	Acc@1  77.34 ( 81.94)	Acc@5  97.66 ( 97.86)
Epoch: [24][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1771e-01 (5.7764e-01)	Acc@1  71.88 ( 81.78)	Acc@5  98.44 ( 97.84)
Epoch: [24][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3522e-01 (5.8148e-01)	Acc@1  80.47 ( 81.67)	Acc@5  99.22 ( 97.82)
Epoch: [24][280/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6895e-01 (5.8416e-01)	Acc@1  79.69 ( 81.56)	Acc@5  96.09 ( 97.79)
Epoch: [24][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0767e-01 (5.8697e-01)	Acc@1  78.12 ( 81.39)	Acc@5  94.53 ( 97.80)
Epoch: [24][300/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5773e-01 (5.8608e-01)	Acc@1  87.50 ( 81.44)	Acc@5  97.66 ( 97.78)
Epoch: [24][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8504e-01 (5.8738e-01)	Acc@1  78.91 ( 81.40)	Acc@5  96.09 ( 97.76)
Epoch: [24][320/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1699e-01 (5.8831e-01)	Acc@1  76.56 ( 81.36)	Acc@5  98.44 ( 97.76)
Epoch: [24][330/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0506e-01 (5.9034e-01)	Acc@1  82.03 ( 81.28)	Acc@5  96.88 ( 97.75)
Epoch: [24][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7032e-01 (5.9272e-01)	Acc@1  69.53 ( 81.22)	Acc@5  96.09 ( 97.72)
Epoch: [24][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3908e-01 (5.9360e-01)	Acc@1  82.81 ( 81.19)	Acc@5  96.88 ( 97.71)
Epoch: [24][360/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5593e-01 (5.9598e-01)	Acc@1  71.09 ( 81.08)	Acc@5  98.44 ( 97.70)
Epoch: [24][370/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0145e-01 (5.9707e-01)	Acc@1  82.03 ( 81.05)	Acc@5  97.66 ( 97.69)
Epoch: [24][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5167e-01 (5.9789e-01)	Acc@1  78.91 ( 80.99)	Acc@5  97.66 ( 97.68)
Epoch: [24][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.1405e-01 (6.0056e-01)	Acc@1  78.75 ( 80.91)	Acc@5  98.75 ( 97.65)
## e[24] optimizer.zero_grad (sum) time: 0.669790506362915
## e[24]       loss.backward (sum) time: 13.89366626739502
## e[24]      optimizer.step (sum) time: 14.731366634368896
## epoch[24] training(only) time: 54.01733088493347
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.4411e+00 (1.4411e+00)	Acc@1  66.00 ( 66.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.8223e+00 (1.4891e+00)	Acc@1  54.00 ( 65.00)	Acc@5  89.00 ( 87.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.1392e+00 (1.3597e+00)	Acc@1  72.00 ( 66.52)	Acc@5  92.00 ( 89.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.7569e+00 (1.3965e+00)	Acc@1  58.00 ( 65.29)	Acc@5  90.00 ( 89.10)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.4238e+00 (1.3879e+00)	Acc@1  66.00 ( 65.00)	Acc@5  87.00 ( 89.54)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.2472e+00 (1.3806e+00)	Acc@1  68.00 ( 65.20)	Acc@5  91.00 ( 89.41)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1173e+00 (1.3684e+00)	Acc@1  64.00 ( 65.23)	Acc@5  92.00 ( 89.59)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3217e+00 (1.3791e+00)	Acc@1  69.00 ( 64.94)	Acc@5  88.00 ( 89.48)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4043e+00 (1.3797e+00)	Acc@1  72.00 ( 65.10)	Acc@5  85.00 ( 89.38)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.6394e+00 (1.3604e+00)	Acc@1  63.00 ( 65.46)	Acc@5  90.00 ( 89.60)
 * Acc@1 65.530 Acc@5 89.700
### epoch[24] execution time: 59.0110445022583
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.305 ( 0.305)	Data  0.155 ( 0.155)	Loss 3.3033e-01 (3.3033e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [25][ 10/391]	Time  0.135 ( 0.152)	Data  0.001 ( 0.015)	Loss 5.0745e-01 (4.9767e-01)	Acc@1  86.72 ( 84.80)	Acc@5  99.22 ( 98.30)
Epoch: [25][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.008)	Loss 6.0953e-01 (4.9817e-01)	Acc@1  80.47 ( 83.97)	Acc@5  99.22 ( 98.51)
Epoch: [25][ 30/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.006)	Loss 5.2585e-01 (5.0077e-01)	Acc@1  84.38 ( 83.92)	Acc@5  97.66 ( 98.46)
Epoch: [25][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 4.8720e-01 (4.9623e-01)	Acc@1  85.16 ( 84.20)	Acc@5  97.66 ( 98.53)
Epoch: [25][ 50/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 6.0642e-01 (5.0179e-01)	Acc@1  79.69 ( 84.16)	Acc@5  96.88 ( 98.41)
Epoch: [25][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.8378e-01 (5.0809e-01)	Acc@1  80.47 ( 83.95)	Acc@5  96.09 ( 98.30)
Epoch: [25][ 70/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.8752e-01 (5.0881e-01)	Acc@1  80.47 ( 83.97)	Acc@5  98.44 ( 98.24)
Epoch: [25][ 80/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.7780e-01 (5.0806e-01)	Acc@1  85.16 ( 84.10)	Acc@5  99.22 ( 98.23)
Epoch: [25][ 90/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.2464e-01 (5.1329e-01)	Acc@1  82.81 ( 83.84)	Acc@5  98.44 ( 98.19)
Epoch: [25][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.8126e-01 (5.0992e-01)	Acc@1  86.72 ( 83.97)	Acc@5  99.22 ( 98.27)
Epoch: [25][110/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.6697e-01 (5.1053e-01)	Acc@1  85.94 ( 84.04)	Acc@5  98.44 ( 98.30)
Epoch: [25][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7702e-01 (5.1175e-01)	Acc@1  92.97 ( 83.95)	Acc@5 100.00 ( 98.28)
Epoch: [25][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6018e-01 (5.1509e-01)	Acc@1  86.72 ( 83.82)	Acc@5  98.44 ( 98.29)
Epoch: [25][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (5.1603e-01)	Acc@1  78.91 ( 83.75)	Acc@5  98.44 ( 98.27)
Epoch: [25][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5315e-01 (5.1799e-01)	Acc@1  82.81 ( 83.69)	Acc@5  97.66 ( 98.26)
Epoch: [25][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2117e-01 (5.2061e-01)	Acc@1  83.59 ( 83.50)	Acc@5  99.22 ( 98.28)
Epoch: [25][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7369e-01 (5.2349e-01)	Acc@1  77.34 ( 83.39)	Acc@5  97.66 ( 98.25)
Epoch: [25][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0891e-01 (5.2511e-01)	Acc@1  82.81 ( 83.35)	Acc@5  97.66 ( 98.25)
Epoch: [25][190/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9921e-01 (5.2687e-01)	Acc@1  85.94 ( 83.34)	Acc@5  98.44 ( 98.24)
Epoch: [25][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4241e-01 (5.2978e-01)	Acc@1  78.91 ( 83.21)	Acc@5  99.22 ( 98.24)
Epoch: [25][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0925e-01 (5.3157e-01)	Acc@1  83.59 ( 83.16)	Acc@5  99.22 ( 98.23)
Epoch: [25][220/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1089e-01 (5.3336e-01)	Acc@1  86.72 ( 83.05)	Acc@5  99.22 ( 98.22)
Epoch: [25][230/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0093e-01 (5.3487e-01)	Acc@1  85.16 ( 82.98)	Acc@5 100.00 ( 98.23)
Epoch: [25][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1101e-01 (5.3639e-01)	Acc@1  82.03 ( 82.96)	Acc@5  96.88 ( 98.22)
Epoch: [25][250/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4471e-01 (5.3706e-01)	Acc@1  80.47 ( 82.95)	Acc@5  97.66 ( 98.19)
Epoch: [25][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5856e-01 (5.3724e-01)	Acc@1  84.38 ( 82.95)	Acc@5  96.88 ( 98.18)
Epoch: [25][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5178e-01 (5.4031e-01)	Acc@1  75.00 ( 82.85)	Acc@5  94.53 ( 98.15)
Epoch: [25][280/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5645e-01 (5.4261e-01)	Acc@1  72.66 ( 82.78)	Acc@5  96.09 ( 98.16)
Epoch: [25][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3729e-01 (5.4470e-01)	Acc@1  86.72 ( 82.69)	Acc@5 100.00 ( 98.16)
Epoch: [25][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0631e-01 (5.4778e-01)	Acc@1  79.69 ( 82.61)	Acc@5  97.66 ( 98.13)
Epoch: [25][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0504e-01 (5.5014e-01)	Acc@1  78.91 ( 82.56)	Acc@5  98.44 ( 98.11)
Epoch: [25][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6044e-01 (5.5365e-01)	Acc@1  81.25 ( 82.44)	Acc@5  96.09 ( 98.07)
Epoch: [25][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3350e-01 (5.5829e-01)	Acc@1  76.56 ( 82.32)	Acc@5  95.31 ( 98.05)
Epoch: [25][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6294e-01 (5.6073e-01)	Acc@1  78.91 ( 82.27)	Acc@5  97.66 ( 98.03)
Epoch: [25][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9687e-01 (5.6391e-01)	Acc@1  80.47 ( 82.21)	Acc@5  95.31 ( 98.00)
Epoch: [25][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5911e-01 (5.6454e-01)	Acc@1  76.56 ( 82.18)	Acc@5  99.22 ( 98.00)
Epoch: [25][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6795e-01 (5.6865e-01)	Acc@1  71.09 ( 82.04)	Acc@5  94.53 ( 97.97)
Epoch: [25][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1410e-01 (5.7118e-01)	Acc@1  76.56 ( 81.95)	Acc@5  97.66 ( 97.95)
Epoch: [25][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0958e-01 (5.7364e-01)	Acc@1  82.50 ( 81.86)	Acc@5  97.50 ( 97.92)
## e[25] optimizer.zero_grad (sum) time: 0.6649529933929443
## e[25]       loss.backward (sum) time: 13.890764474868774
## e[25]      optimizer.step (sum) time: 14.707456588745117
## epoch[25] training(only) time: 54.00525140762329
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.2608e+00 (1.2608e+00)	Acc@1  66.00 ( 66.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.6934e+00 (1.5125e+00)	Acc@1  55.00 ( 64.73)	Acc@5  88.00 ( 87.73)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 1.4855e+00 (1.4564e+00)	Acc@1  66.00 ( 65.48)	Acc@5  90.00 ( 88.43)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.8405e+00 (1.4989e+00)	Acc@1  55.00 ( 64.71)	Acc@5  87.00 ( 87.90)
Test: [ 40/100]	Time  0.056 ( 0.052)	Loss 1.4736e+00 (1.4882e+00)	Acc@1  63.00 ( 64.59)	Acc@5  92.00 ( 88.20)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2614e+00 (1.5056e+00)	Acc@1  64.00 ( 64.37)	Acc@5  94.00 ( 88.16)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.4323e+00 (1.5018e+00)	Acc@1  61.00 ( 63.95)	Acc@5  92.00 ( 88.34)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.5132e+00 (1.5054e+00)	Acc@1  61.00 ( 63.97)	Acc@5  89.00 ( 88.28)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.6077e+00 (1.5124e+00)	Acc@1  61.00 ( 63.90)	Acc@5  84.00 ( 88.05)
Test: [ 90/100]	Time  0.059 ( 0.050)	Loss 1.7566e+00 (1.4997e+00)	Acc@1  55.00 ( 64.08)	Acc@5  87.00 ( 88.41)
 * Acc@1 64.420 Acc@5 88.550
### epoch[25] execution time: 59.01721429824829
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.297 ( 0.297)	Data  0.149 ( 0.149)	Loss 4.0311e-01 (4.0311e-01)	Acc@1  84.38 ( 84.38)	Acc@5  99.22 ( 99.22)
Epoch: [26][ 10/391]	Time  0.146 ( 0.154)	Data  0.001 ( 0.015)	Loss 5.3096e-01 (4.9525e-01)	Acc@1  85.94 ( 83.95)	Acc@5  96.09 ( 98.51)
Epoch: [26][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 5.6837e-01 (4.9982e-01)	Acc@1  82.03 ( 84.19)	Acc@5  98.44 ( 98.51)
Epoch: [26][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 6.7346e-01 (5.0234e-01)	Acc@1  83.59 ( 84.15)	Acc@5  96.09 ( 98.51)
Epoch: [26][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 6.4146e-01 (5.0782e-01)	Acc@1  80.47 ( 84.22)	Acc@5  96.88 ( 98.49)
Epoch: [26][ 50/391]	Time  0.147 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.9153e-01 (4.9644e-01)	Acc@1  87.50 ( 84.67)	Acc@5 100.00 ( 98.59)
Epoch: [26][ 60/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.004)	Loss 4.7615e-01 (4.8918e-01)	Acc@1  83.59 ( 84.90)	Acc@5  98.44 ( 98.68)
Epoch: [26][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.0988e-01 (4.8732e-01)	Acc@1  81.25 ( 84.78)	Acc@5  97.66 ( 98.68)
Epoch: [26][ 80/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.9013e-01 (4.8910e-01)	Acc@1  86.72 ( 84.71)	Acc@5  98.44 ( 98.65)
Epoch: [26][ 90/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.0785e-01 (4.8879e-01)	Acc@1  86.72 ( 84.72)	Acc@5  97.66 ( 98.61)
Epoch: [26][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.1509e-01 (4.8765e-01)	Acc@1  82.03 ( 84.80)	Acc@5 100.00 ( 98.62)
Epoch: [26][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.0935e-01 (4.9259e-01)	Acc@1  85.94 ( 84.59)	Acc@5  98.44 ( 98.63)
Epoch: [26][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.8446e-01 (4.9181e-01)	Acc@1  84.38 ( 84.56)	Acc@5  98.44 ( 98.64)
Epoch: [26][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8608e-01 (4.9174e-01)	Acc@1  78.12 ( 84.58)	Acc@5  98.44 ( 98.66)
Epoch: [26][140/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6258e-01 (4.9192e-01)	Acc@1  78.91 ( 84.50)	Acc@5  98.44 ( 98.66)
Epoch: [26][150/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8622e-01 (4.9812e-01)	Acc@1  89.84 ( 84.33)	Acc@5  99.22 ( 98.62)
Epoch: [26][160/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5926e-01 (5.0325e-01)	Acc@1  82.03 ( 84.21)	Acc@5  98.44 ( 98.56)
Epoch: [26][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9422e-01 (5.0533e-01)	Acc@1  75.00 ( 84.11)	Acc@5  96.88 ( 98.53)
Epoch: [26][180/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1929e-01 (5.0718e-01)	Acc@1  82.03 ( 84.08)	Acc@5  98.44 ( 98.50)
Epoch: [26][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5251e-01 (5.0976e-01)	Acc@1  86.72 ( 83.98)	Acc@5  97.66 ( 98.49)
Epoch: [26][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6212e-01 (5.1189e-01)	Acc@1  85.94 ( 83.84)	Acc@5  98.44 ( 98.49)
Epoch: [26][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3583e-01 (5.1657e-01)	Acc@1  82.03 ( 83.67)	Acc@5  98.44 ( 98.46)
Epoch: [26][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2785e-01 (5.1958e-01)	Acc@1  78.91 ( 83.56)	Acc@5  98.44 ( 98.41)
Epoch: [26][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3183e-01 (5.2381e-01)	Acc@1  82.03 ( 83.44)	Acc@5  98.44 ( 98.37)
Epoch: [26][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6061e-01 (5.2665e-01)	Acc@1  82.03 ( 83.39)	Acc@5  98.44 ( 98.34)
Epoch: [26][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7168e-01 (5.2989e-01)	Acc@1  76.56 ( 83.25)	Acc@5  95.31 ( 98.28)
Epoch: [26][260/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0408e-01 (5.3088e-01)	Acc@1  81.25 ( 83.19)	Acc@5 100.00 ( 98.28)
Epoch: [26][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6200e-01 (5.3303e-01)	Acc@1  78.91 ( 83.15)	Acc@5  97.66 ( 98.26)
Epoch: [26][280/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1283e-01 (5.3254e-01)	Acc@1  85.16 ( 83.18)	Acc@5  97.66 ( 98.25)
Epoch: [26][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0713e-01 (5.3365e-01)	Acc@1  77.34 ( 83.13)	Acc@5  96.09 ( 98.23)
Epoch: [26][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4781e-01 (5.3507e-01)	Acc@1  76.56 ( 83.05)	Acc@5  96.88 ( 98.23)
Epoch: [26][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9033e-01 (5.4026e-01)	Acc@1  78.12 ( 82.87)	Acc@5  97.66 ( 98.19)
Epoch: [26][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7833e-01 (5.4404e-01)	Acc@1  87.50 ( 82.79)	Acc@5  99.22 ( 98.17)
Epoch: [26][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.7342e-01 (5.4724e-01)	Acc@1  75.00 ( 82.72)	Acc@5  96.09 ( 98.13)
Epoch: [26][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2389e-01 (5.4926e-01)	Acc@1  82.03 ( 82.67)	Acc@5  96.88 ( 98.12)
Epoch: [26][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9387e-01 (5.5169e-01)	Acc@1  82.81 ( 82.59)	Acc@5 100.00 ( 98.10)
Epoch: [26][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6554e-01 (5.5385e-01)	Acc@1  75.00 ( 82.51)	Acc@5  95.31 ( 98.08)
Epoch: [26][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4749e-01 (5.5471e-01)	Acc@1  78.91 ( 82.47)	Acc@5  99.22 ( 98.08)
Epoch: [26][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0642e-01 (5.5522e-01)	Acc@1  81.25 ( 82.46)	Acc@5  98.44 ( 98.08)
Epoch: [26][390/391]	Time  0.116 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2021e-01 (5.5751e-01)	Acc@1  78.75 ( 82.38)	Acc@5  96.25 ( 98.05)
## e[26] optimizer.zero_grad (sum) time: 0.6701586246490479
## e[26]       loss.backward (sum) time: 13.806790828704834
## e[26]      optimizer.step (sum) time: 14.729998350143433
## epoch[26] training(only) time: 53.923062801361084
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.2860e+00 (1.2860e+00)	Acc@1  70.00 ( 70.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.6406e+00 (1.4117e+00)	Acc@1  67.00 ( 66.36)	Acc@5  91.00 ( 89.64)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.0364e+00 (1.3525e+00)	Acc@1  73.00 ( 66.62)	Acc@5  96.00 ( 90.24)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.7958e+00 (1.3983e+00)	Acc@1  60.00 ( 66.03)	Acc@5  88.00 ( 89.61)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.4477e+00 (1.4085e+00)	Acc@1  68.00 ( 65.68)	Acc@5  91.00 ( 89.51)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.3227e+00 (1.4120e+00)	Acc@1  66.00 ( 65.41)	Acc@5  90.00 ( 89.39)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.3551e+00 (1.4052e+00)	Acc@1  69.00 ( 65.30)	Acc@5  90.00 ( 89.49)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.5786e+00 (1.4062e+00)	Acc@1  66.00 ( 65.45)	Acc@5  94.00 ( 89.54)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.7154e+00 (1.4197e+00)	Acc@1  60.00 ( 65.26)	Acc@5  86.00 ( 89.28)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7261e+00 (1.4120e+00)	Acc@1  61.00 ( 65.33)	Acc@5  89.00 ( 89.46)
 * Acc@1 65.440 Acc@5 89.430
### epoch[26] execution time: 58.881136894226074
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.309 ( 0.309)	Data  0.133 ( 0.133)	Loss 3.6206e-01 (3.6206e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [27][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.013)	Loss 5.3262e-01 (4.8097e-01)	Acc@1  82.03 ( 84.87)	Acc@5  98.44 ( 98.37)
Epoch: [27][ 20/391]	Time  0.134 ( 0.145)	Data  0.001 ( 0.007)	Loss 5.2127e-01 (4.8523e-01)	Acc@1  85.94 ( 84.90)	Acc@5  98.44 ( 98.40)
Epoch: [27][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.005)	Loss 4.3243e-01 (4.7288e-01)	Acc@1  86.72 ( 84.83)	Acc@5  98.44 ( 98.51)
Epoch: [27][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.1501e-01 (4.7341e-01)	Acc@1  86.72 ( 84.78)	Acc@5  98.44 ( 98.49)
Epoch: [27][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.1755e-01 (4.6789e-01)	Acc@1  86.72 ( 84.93)	Acc@5  98.44 ( 98.58)
Epoch: [27][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.8022e-01 (4.6373e-01)	Acc@1  82.81 ( 85.12)	Acc@5  98.44 ( 98.64)
Epoch: [27][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.5420e-01 (4.7000e-01)	Acc@1  82.03 ( 84.89)	Acc@5  98.44 ( 98.60)
Epoch: [27][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.0078e-01 (4.7139e-01)	Acc@1  81.25 ( 84.83)	Acc@5 100.00 ( 98.62)
Epoch: [27][ 90/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.7309e-01 (4.7361e-01)	Acc@1  87.50 ( 84.78)	Acc@5  99.22 ( 98.58)
Epoch: [27][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0219e-01 (4.8116e-01)	Acc@1  82.81 ( 84.57)	Acc@5  97.66 ( 98.53)
Epoch: [27][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.9339e-01 (4.8533e-01)	Acc@1  78.91 ( 84.49)	Acc@5  97.66 ( 98.51)
Epoch: [27][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6118e-01 (4.9061e-01)	Acc@1  77.34 ( 84.41)	Acc@5  98.44 ( 98.50)
Epoch: [27][130/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1581e-01 (4.9242e-01)	Acc@1  78.91 ( 84.33)	Acc@5  94.53 ( 98.46)
Epoch: [27][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0071e-01 (4.9242e-01)	Acc@1  79.69 ( 84.36)	Acc@5  99.22 ( 98.48)
Epoch: [27][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4226e-01 (4.9292e-01)	Acc@1  82.81 ( 84.36)	Acc@5  96.09 ( 98.48)
Epoch: [27][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0109e-01 (4.9435e-01)	Acc@1  79.69 ( 84.29)	Acc@5  94.53 ( 98.44)
Epoch: [27][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3804e-01 (4.9443e-01)	Acc@1  88.28 ( 84.27)	Acc@5 100.00 ( 98.44)
Epoch: [27][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7467e-01 (4.9563e-01)	Acc@1  86.72 ( 84.19)	Acc@5  97.66 ( 98.45)
Epoch: [27][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4259e-01 (4.9558e-01)	Acc@1  85.94 ( 84.22)	Acc@5  96.88 ( 98.46)
Epoch: [27][200/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7072e-01 (4.9638e-01)	Acc@1  78.91 ( 84.22)	Acc@5  99.22 ( 98.45)
Epoch: [27][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1492e-01 (4.9650e-01)	Acc@1  85.94 ( 84.18)	Acc@5  98.44 ( 98.43)
Epoch: [27][220/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0330e-01 (4.9679e-01)	Acc@1  83.59 ( 84.18)	Acc@5  98.44 ( 98.45)
Epoch: [27][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3737e-01 (4.9690e-01)	Acc@1  79.69 ( 84.18)	Acc@5  99.22 ( 98.45)
Epoch: [27][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5827e-01 (4.9803e-01)	Acc@1  84.38 ( 84.14)	Acc@5  96.88 ( 98.45)
Epoch: [27][250/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9494e-01 (5.0208e-01)	Acc@1  70.31 ( 83.99)	Acc@5  95.31 ( 98.40)
Epoch: [27][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9571e-01 (5.0293e-01)	Acc@1  82.81 ( 83.96)	Acc@5  96.88 ( 98.39)
Epoch: [27][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3589e-01 (5.0455e-01)	Acc@1  83.59 ( 83.95)	Acc@5  97.66 ( 98.36)
Epoch: [27][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4699e-01 (5.0936e-01)	Acc@1  78.91 ( 83.82)	Acc@5  96.88 ( 98.33)
Epoch: [27][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9544e-01 (5.1368e-01)	Acc@1  80.47 ( 83.70)	Acc@5  97.66 ( 98.31)
Epoch: [27][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9810e-01 (5.1495e-01)	Acc@1  84.38 ( 83.69)	Acc@5  98.44 ( 98.28)
Epoch: [27][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6322e-01 (5.1732e-01)	Acc@1  81.25 ( 83.64)	Acc@5  96.88 ( 98.25)
Epoch: [27][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2065e-01 (5.1979e-01)	Acc@1  79.69 ( 83.57)	Acc@5  96.88 ( 98.20)
Epoch: [27][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0290e-01 (5.2128e-01)	Acc@1  86.72 ( 83.50)	Acc@5  99.22 ( 98.21)
Epoch: [27][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1129e-01 (5.2149e-01)	Acc@1  83.59 ( 83.50)	Acc@5  97.66 ( 98.20)
Epoch: [27][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8621e-01 (5.2316e-01)	Acc@1  74.22 ( 83.42)	Acc@5  98.44 ( 98.21)
Epoch: [27][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6274e-01 (5.2514e-01)	Acc@1  82.81 ( 83.36)	Acc@5  97.66 ( 98.19)
Epoch: [27][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8338e-01 (5.2891e-01)	Acc@1  78.12 ( 83.26)	Acc@5  96.09 ( 98.16)
Epoch: [27][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.1084e-01 (5.3071e-01)	Acc@1  80.47 ( 83.21)	Acc@5  96.88 ( 98.16)
Epoch: [27][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.0901e-01 (5.3375e-01)	Acc@1  87.50 ( 83.12)	Acc@5  98.75 ( 98.12)
## e[27] optimizer.zero_grad (sum) time: 0.6664333343505859
## e[27]       loss.backward (sum) time: 13.81972074508667
## e[27]      optimizer.step (sum) time: 14.689563274383545
## epoch[27] training(only) time: 53.933013916015625
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.4184e+00 (1.4184e+00)	Acc@1  66.00 ( 66.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.049 ( 0.063)	Loss 1.3352e+00 (1.4708e+00)	Acc@1  67.00 ( 64.18)	Acc@5  90.00 ( 88.36)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 1.2452e+00 (1.4732e+00)	Acc@1  66.00 ( 63.95)	Acc@5  93.00 ( 88.95)
Test: [ 30/100]	Time  0.051 ( 0.053)	Loss 1.6502e+00 (1.4669e+00)	Acc@1  60.00 ( 64.13)	Acc@5  91.00 ( 89.10)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.7986e+00 (1.4837e+00)	Acc@1  60.00 ( 63.85)	Acc@5  88.00 ( 89.02)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.3372e+00 (1.5031e+00)	Acc@1  65.00 ( 63.59)	Acc@5  87.00 ( 88.45)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.4831e+00 (1.4963e+00)	Acc@1  61.00 ( 63.41)	Acc@5  89.00 ( 88.82)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.6152e+00 (1.5047e+00)	Acc@1  59.00 ( 63.14)	Acc@5  86.00 ( 88.76)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.6617e+00 (1.5108e+00)	Acc@1  64.00 ( 63.09)	Acc@5  84.00 ( 88.58)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.9555e+00 (1.4993e+00)	Acc@1  59.00 ( 63.35)	Acc@5  86.00 ( 88.67)
 * Acc@1 63.400 Acc@5 88.690
### epoch[27] execution time: 58.94573259353638
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.301 ( 0.301)	Data  0.154 ( 0.154)	Loss 5.6156e-01 (5.6156e-01)	Acc@1  82.03 ( 82.03)	Acc@5  97.66 ( 97.66)
Epoch: [28][ 10/391]	Time  0.139 ( 0.153)	Data  0.001 ( 0.015)	Loss 3.3810e-01 (4.9548e-01)	Acc@1  89.84 ( 84.45)	Acc@5  99.22 ( 98.37)
Epoch: [28][ 20/391]	Time  0.138 ( 0.146)	Data  0.001 ( 0.008)	Loss 3.4960e-01 (4.9570e-01)	Acc@1  89.06 ( 84.71)	Acc@5  99.22 ( 98.40)
Epoch: [28][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.1083e-01 (4.7653e-01)	Acc@1  85.94 ( 85.48)	Acc@5  99.22 ( 98.64)
Epoch: [28][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 4.8036e-01 (4.6916e-01)	Acc@1  82.03 ( 85.48)	Acc@5  98.44 ( 98.70)
Epoch: [28][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.4440e-01 (4.5999e-01)	Acc@1  86.72 ( 85.80)	Acc@5  97.66 ( 98.82)
Epoch: [28][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 4.4302e-01 (4.5367e-01)	Acc@1  87.50 ( 85.95)	Acc@5  97.66 ( 98.82)
Epoch: [28][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.0314e-01 (4.4624e-01)	Acc@1  92.97 ( 86.15)	Acc@5 100.00 ( 98.89)
Epoch: [28][ 80/391]	Time  0.148 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.8474e-01 (4.4513e-01)	Acc@1  85.94 ( 86.18)	Acc@5  98.44 ( 98.90)
Epoch: [28][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.5097e-01 (4.4778e-01)	Acc@1  78.91 ( 85.98)	Acc@5  96.09 ( 98.85)
Epoch: [28][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.1459e-01 (4.5058e-01)	Acc@1  84.38 ( 85.83)	Acc@5  99.22 ( 98.82)
Epoch: [28][110/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4128e-01 (4.5378e-01)	Acc@1  85.94 ( 85.73)	Acc@5  98.44 ( 98.80)
Epoch: [28][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.3186e-01 (4.5947e-01)	Acc@1  78.12 ( 85.60)	Acc@5 100.00 ( 98.77)
Epoch: [28][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2168e-01 (4.6224e-01)	Acc@1  86.72 ( 85.46)	Acc@5 100.00 ( 98.79)
Epoch: [28][140/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1737e-01 (4.6636e-01)	Acc@1  85.16 ( 85.29)	Acc@5  97.66 ( 98.75)
Epoch: [28][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6126e-01 (4.6992e-01)	Acc@1  86.72 ( 85.20)	Acc@5  97.66 ( 98.71)
Epoch: [28][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0593e-01 (4.7245e-01)	Acc@1  85.94 ( 85.09)	Acc@5 100.00 ( 98.71)
Epoch: [28][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7479e-01 (4.7326e-01)	Acc@1  83.59 ( 85.02)	Acc@5 100.00 ( 98.71)
Epoch: [28][180/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8869e-01 (4.7643e-01)	Acc@1  82.81 ( 84.96)	Acc@5  98.44 ( 98.66)
Epoch: [28][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8815e-01 (4.8064e-01)	Acc@1  85.16 ( 84.83)	Acc@5  98.44 ( 98.61)
Epoch: [28][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3043e-01 (4.8183e-01)	Acc@1  86.72 ( 84.77)	Acc@5  98.44 ( 98.59)
Epoch: [28][210/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5804e-01 (4.8470e-01)	Acc@1  76.56 ( 84.63)	Acc@5  99.22 ( 98.59)
Epoch: [28][220/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8386e-01 (4.8516e-01)	Acc@1  84.38 ( 84.56)	Acc@5  99.22 ( 98.60)
Epoch: [28][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7132e-01 (4.8493e-01)	Acc@1  84.38 ( 84.56)	Acc@5  97.66 ( 98.58)
Epoch: [28][240/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7615e-01 (4.8661e-01)	Acc@1  84.38 ( 84.50)	Acc@5  97.66 ( 98.55)
Epoch: [28][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2828e-01 (4.8808e-01)	Acc@1  84.38 ( 84.46)	Acc@5 100.00 ( 98.56)
Epoch: [28][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7002e-01 (4.9076e-01)	Acc@1  80.47 ( 84.38)	Acc@5  97.66 ( 98.54)
Epoch: [28][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2310e-01 (4.9270e-01)	Acc@1  86.72 ( 84.33)	Acc@5  98.44 ( 98.53)
Epoch: [28][280/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9436e-01 (4.9596e-01)	Acc@1  76.56 ( 84.24)	Acc@5  97.66 ( 98.51)
Epoch: [28][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8657e-01 (4.9784e-01)	Acc@1  75.00 ( 84.17)	Acc@5  96.88 ( 98.50)
Epoch: [28][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8043e-01 (4.9967e-01)	Acc@1  83.59 ( 84.12)	Acc@5  99.22 ( 98.48)
Epoch: [28][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3643e-01 (5.0133e-01)	Acc@1  89.06 ( 84.10)	Acc@5  97.66 ( 98.47)
Epoch: [28][320/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6339e-01 (5.0382e-01)	Acc@1  85.16 ( 84.01)	Acc@5  98.44 ( 98.45)
Epoch: [28][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3008e-01 (5.0636e-01)	Acc@1  86.72 ( 83.93)	Acc@5  99.22 ( 98.44)
Epoch: [28][340/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8695e-01 (5.0631e-01)	Acc@1  79.69 ( 83.91)	Acc@5  98.44 ( 98.44)
Epoch: [28][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7163e-01 (5.0810e-01)	Acc@1  78.91 ( 83.83)	Acc@5  97.66 ( 98.43)
Epoch: [28][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6164e-01 (5.0974e-01)	Acc@1  76.56 ( 83.78)	Acc@5  97.66 ( 98.41)
Epoch: [28][370/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4959e-01 (5.1208e-01)	Acc@1  83.59 ( 83.73)	Acc@5  97.66 ( 98.39)
Epoch: [28][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5776e-01 (5.1471e-01)	Acc@1  78.12 ( 83.63)	Acc@5  97.66 ( 98.37)
Epoch: [28][390/391]	Time  0.113 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1464e-01 (5.1646e-01)	Acc@1  80.00 ( 83.57)	Acc@5  93.75 ( 98.36)
## e[28] optimizer.zero_grad (sum) time: 0.6695940494537354
## e[28]       loss.backward (sum) time: 13.894001483917236
## e[28]      optimizer.step (sum) time: 14.687849521636963
## epoch[28] training(only) time: 54.00895953178406
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 1.4548e+00 (1.4548e+00)	Acc@1  64.00 ( 64.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.5255e+00 (1.4307e+00)	Acc@1  64.00 ( 66.09)	Acc@5  91.00 ( 88.36)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.2936e+00 (1.4421e+00)	Acc@1  71.00 ( 65.90)	Acc@5  88.00 ( 88.67)
Test: [ 30/100]	Time  0.045 ( 0.053)	Loss 1.5968e+00 (1.4621e+00)	Acc@1  54.00 ( 64.74)	Acc@5  89.00 ( 88.71)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 1.4810e+00 (1.4711e+00)	Acc@1  71.00 ( 64.85)	Acc@5  89.00 ( 88.78)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.4900e+00 (1.4733e+00)	Acc@1  67.00 ( 65.08)	Acc@5  88.00 ( 88.51)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.0904e+00 (1.4676e+00)	Acc@1  69.00 ( 65.08)	Acc@5  95.00 ( 88.75)
Test: [ 70/100]	Time  0.046 ( 0.051)	Loss 1.7570e+00 (1.4754e+00)	Acc@1  59.00 ( 64.83)	Acc@5  89.00 ( 88.80)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4576e+00 (1.4828e+00)	Acc@1  70.00 ( 64.70)	Acc@5  87.00 ( 88.77)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.6617e+00 (1.4676e+00)	Acc@1  64.00 ( 64.91)	Acc@5  88.00 ( 88.90)
 * Acc@1 65.160 Acc@5 88.980
### epoch[28] execution time: 59.056180000305176
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.294 ( 0.294)	Data  0.145 ( 0.145)	Loss 4.1471e-01 (4.1471e-01)	Acc@1  88.28 ( 88.28)	Acc@5  98.44 ( 98.44)
Epoch: [29][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.014)	Loss 5.1712e-01 (4.9475e-01)	Acc@1  79.69 ( 83.88)	Acc@5  99.22 ( 98.51)
Epoch: [29][ 20/391]	Time  0.134 ( 0.145)	Data  0.001 ( 0.008)	Loss 4.3931e-01 (4.7859e-01)	Acc@1  86.72 ( 85.23)	Acc@5  99.22 ( 98.66)
Epoch: [29][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 5.2099e-01 (4.7435e-01)	Acc@1  82.03 ( 84.95)	Acc@5  97.66 ( 98.71)
Epoch: [29][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 6.1165e-01 (4.6274e-01)	Acc@1  81.25 ( 85.48)	Acc@5  96.88 ( 98.76)
Epoch: [29][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.8015e-01 (4.6440e-01)	Acc@1  85.94 ( 85.55)	Acc@5  98.44 ( 98.74)
Epoch: [29][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.003)	Loss 4.1505e-01 (4.5521e-01)	Acc@1  86.72 ( 85.62)	Acc@5  98.44 ( 98.73)
Epoch: [29][ 70/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.7816e-01 (4.5294e-01)	Acc@1  88.28 ( 85.74)	Acc@5  99.22 ( 98.73)
Epoch: [29][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.8189e-01 (4.5498e-01)	Acc@1  86.72 ( 85.73)	Acc@5 100.00 ( 98.73)
Epoch: [29][ 90/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.1684e-01 (4.5194e-01)	Acc@1  85.94 ( 85.66)	Acc@5  99.22 ( 98.78)
Epoch: [29][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.8799e-01 (4.5557e-01)	Acc@1  90.62 ( 85.57)	Acc@5  99.22 ( 98.72)
Epoch: [29][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8559e-01 (4.5562e-01)	Acc@1  87.50 ( 85.61)	Acc@5  99.22 ( 98.76)
Epoch: [29][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2993e-01 (4.5514e-01)	Acc@1  86.72 ( 85.61)	Acc@5  98.44 ( 98.77)
Epoch: [29][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2915e-01 (4.5676e-01)	Acc@1  85.16 ( 85.51)	Acc@5  99.22 ( 98.73)
Epoch: [29][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.7081e-01 (4.6036e-01)	Acc@1  81.25 ( 85.36)	Acc@5  99.22 ( 98.75)
Epoch: [29][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8712e-01 (4.6014e-01)	Acc@1  89.06 ( 85.31)	Acc@5  97.66 ( 98.75)
Epoch: [29][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4733e-01 (4.6220e-01)	Acc@1  83.59 ( 85.23)	Acc@5  99.22 ( 98.76)
Epoch: [29][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4440e-01 (4.6164e-01)	Acc@1  82.03 ( 85.24)	Acc@5  99.22 ( 98.77)
Epoch: [29][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8737e-01 (4.6280e-01)	Acc@1  83.59 ( 85.20)	Acc@5  99.22 ( 98.77)
Epoch: [29][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7933e-01 (4.6343e-01)	Acc@1  83.59 ( 85.17)	Acc@5  99.22 ( 98.74)
Epoch: [29][200/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0478e-01 (4.6601e-01)	Acc@1  81.25 ( 85.15)	Acc@5  98.44 ( 98.71)
Epoch: [29][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8835e-01 (4.6797e-01)	Acc@1  77.34 ( 85.09)	Acc@5  96.88 ( 98.71)
Epoch: [29][220/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3785e-01 (4.6966e-01)	Acc@1  84.38 ( 85.03)	Acc@5  97.66 ( 98.68)
Epoch: [29][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8501e-01 (4.7213e-01)	Acc@1  79.69 ( 84.99)	Acc@5  98.44 ( 98.66)
Epoch: [29][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8262e-01 (4.7267e-01)	Acc@1  89.84 ( 84.98)	Acc@5  99.22 ( 98.65)
Epoch: [29][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6726e-01 (4.7559e-01)	Acc@1  85.94 ( 84.90)	Acc@5  98.44 ( 98.62)
Epoch: [29][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8402e-01 (4.7913e-01)	Acc@1  81.25 ( 84.76)	Acc@5  98.44 ( 98.60)
Epoch: [29][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3695e-01 (4.7997e-01)	Acc@1  80.47 ( 84.70)	Acc@5  97.66 ( 98.59)
Epoch: [29][280/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6825e-01 (4.8136e-01)	Acc@1  85.16 ( 84.62)	Acc@5  96.88 ( 98.58)
Epoch: [29][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3663e-01 (4.8287e-01)	Acc@1  78.12 ( 84.54)	Acc@5 100.00 ( 98.57)
Epoch: [29][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5066e-01 (4.8475e-01)	Acc@1  85.94 ( 84.48)	Acc@5 100.00 ( 98.55)
Epoch: [29][310/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7312e-01 (4.8703e-01)	Acc@1  88.28 ( 84.40)	Acc@5  98.44 ( 98.55)
Epoch: [29][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2604e-01 (4.8950e-01)	Acc@1  79.69 ( 84.33)	Acc@5 100.00 ( 98.55)
Epoch: [29][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1828e-01 (4.9088e-01)	Acc@1  84.38 ( 84.32)	Acc@5  98.44 ( 98.55)
Epoch: [29][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1784e-01 (4.9182e-01)	Acc@1  83.59 ( 84.33)	Acc@5  98.44 ( 98.54)
Epoch: [29][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3788e-01 (4.9363e-01)	Acc@1  82.03 ( 84.30)	Acc@5  98.44 ( 98.52)
Epoch: [29][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4861e-01 (4.9608e-01)	Acc@1  82.03 ( 84.23)	Acc@5  98.44 ( 98.51)
Epoch: [29][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9854e-01 (4.9674e-01)	Acc@1  83.59 ( 84.20)	Acc@5  98.44 ( 98.51)
Epoch: [29][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4639e-01 (4.9988e-01)	Acc@1  82.03 ( 84.11)	Acc@5  96.88 ( 98.48)
Epoch: [29][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9209e-01 (5.0211e-01)	Acc@1  78.75 ( 84.05)	Acc@5  98.75 ( 98.47)
## e[29] optimizer.zero_grad (sum) time: 0.6690723896026611
## e[29]       loss.backward (sum) time: 13.871306657791138
## e[29]      optimizer.step (sum) time: 14.698041915893555
## epoch[29] training(only) time: 54.00073719024658
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 1.2187e+00 (1.2187e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.6307e+00 (1.4878e+00)	Acc@1  63.00 ( 65.82)	Acc@5  89.00 ( 88.91)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.2937e+00 (1.4311e+00)	Acc@1  67.00 ( 66.05)	Acc@5  92.00 ( 89.71)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.8843e+00 (1.4756e+00)	Acc@1  58.00 ( 65.58)	Acc@5  89.00 ( 89.32)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 1.7392e+00 (1.4676e+00)	Acc@1  63.00 ( 65.83)	Acc@5  92.00 ( 89.66)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.3342e+00 (1.4795e+00)	Acc@1  69.00 ( 65.71)	Acc@5  93.00 ( 89.49)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.4190e+00 (1.4707e+00)	Acc@1  65.00 ( 65.89)	Acc@5  89.00 ( 89.64)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.5993e+00 (1.4728e+00)	Acc@1  64.00 ( 65.79)	Acc@5  91.00 ( 89.65)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.4063e+00 (1.4806e+00)	Acc@1  68.00 ( 65.70)	Acc@5  91.00 ( 89.63)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.8665e+00 (1.4638e+00)	Acc@1  58.00 ( 65.90)	Acc@5  86.00 ( 89.81)
 * Acc@1 66.220 Acc@5 89.840
### epoch[29] execution time: 59.007445335388184
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.315 ( 0.315)	Data  0.158 ( 0.158)	Loss 5.5998e-01 (5.5998e-01)	Acc@1  82.81 ( 82.81)	Acc@5  98.44 ( 98.44)
Epoch: [30][ 10/391]	Time  0.135 ( 0.154)	Data  0.001 ( 0.015)	Loss 3.3918e-01 (4.4142e-01)	Acc@1  88.28 ( 85.30)	Acc@5 100.00 ( 99.15)
Epoch: [30][ 20/391]	Time  0.138 ( 0.146)	Data  0.001 ( 0.009)	Loss 4.3240e-01 (4.2015e-01)	Acc@1  88.28 ( 86.31)	Acc@5  99.22 ( 99.33)
Epoch: [30][ 30/391]	Time  0.139 ( 0.144)	Data  0.001 ( 0.006)	Loss 4.0348e-01 (4.0867e-01)	Acc@1  86.72 ( 86.95)	Acc@5 100.00 ( 99.27)
Epoch: [30][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 4.9779e-01 (3.9580e-01)	Acc@1  82.03 ( 87.37)	Acc@5  99.22 ( 99.24)
Epoch: [30][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.4325e-01 (3.7456e-01)	Acc@1  86.72 ( 88.16)	Acc@5 100.00 ( 99.25)
Epoch: [30][ 60/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.3561e-01 (3.5948e-01)	Acc@1  93.75 ( 88.78)	Acc@5 100.00 ( 99.26)
Epoch: [30][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4917e-01 (3.5053e-01)	Acc@1  92.19 ( 89.05)	Acc@5  99.22 ( 99.28)
Epoch: [30][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.1923e-01 (3.4320e-01)	Acc@1  87.50 ( 89.37)	Acc@5  97.66 ( 99.23)
Epoch: [30][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.6371e-01 (3.3337e-01)	Acc@1  93.75 ( 89.74)	Acc@5 100.00 ( 99.26)
Epoch: [30][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.9691e-01 (3.2545e-01)	Acc@1  92.97 ( 90.01)	Acc@5  98.44 ( 99.29)
Epoch: [30][110/391]	Time  0.153 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2603e-01 (3.1826e-01)	Acc@1  92.19 ( 90.22)	Acc@5 100.00 ( 99.31)
Epoch: [30][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1386e-01 (3.1506e-01)	Acc@1  89.06 ( 90.31)	Acc@5 100.00 ( 99.32)
Epoch: [30][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0595e-01 (3.0912e-01)	Acc@1  94.53 ( 90.57)	Acc@5 100.00 ( 99.36)
Epoch: [30][140/391]	Time  0.152 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3755e-01 (3.0534e-01)	Acc@1  89.84 ( 90.67)	Acc@5 100.00 ( 99.38)
Epoch: [30][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4572e-01 (3.0329e-01)	Acc@1  93.75 ( 90.79)	Acc@5  99.22 ( 99.39)
Epoch: [30][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8414e-01 (2.9929e-01)	Acc@1  91.41 ( 90.96)	Acc@5 100.00 ( 99.39)
Epoch: [30][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7585e-01 (2.9536e-01)	Acc@1  90.62 ( 91.14)	Acc@5  99.22 ( 99.42)
Epoch: [30][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6258e-01 (2.9127e-01)	Acc@1  92.19 ( 91.30)	Acc@5  99.22 ( 99.43)
Epoch: [30][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7237e-01 (2.8836e-01)	Acc@1  95.31 ( 91.41)	Acc@5 100.00 ( 99.44)
Epoch: [30][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6014e-01 (2.8596e-01)	Acc@1  93.75 ( 91.52)	Acc@5  99.22 ( 99.45)
Epoch: [30][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2937e-01 (2.8356e-01)	Acc@1  96.09 ( 91.62)	Acc@5 100.00 ( 99.46)
Epoch: [30][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7354e-01 (2.8099e-01)	Acc@1  96.09 ( 91.73)	Acc@5 100.00 ( 99.47)
Epoch: [30][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5675e-01 (2.7774e-01)	Acc@1  96.09 ( 91.82)	Acc@5 100.00 ( 99.48)
Epoch: [30][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1618e-01 (2.7375e-01)	Acc@1  92.19 ( 91.94)	Acc@5 100.00 ( 99.49)
Epoch: [30][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4620e-01 (2.7161e-01)	Acc@1  92.97 ( 92.00)	Acc@5  99.22 ( 99.50)
Epoch: [30][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0302e-01 (2.6912e-01)	Acc@1  94.53 ( 92.09)	Acc@5 100.00 ( 99.52)
Epoch: [30][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8712e-01 (2.6708e-01)	Acc@1  96.88 ( 92.17)	Acc@5 100.00 ( 99.53)
Epoch: [30][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8892e-01 (2.6533e-01)	Acc@1  95.31 ( 92.20)	Acc@5 100.00 ( 99.55)
Epoch: [30][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6582e-01 (2.6349e-01)	Acc@1  89.84 ( 92.26)	Acc@5 100.00 ( 99.55)
Epoch: [30][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4095e-01 (2.6146e-01)	Acc@1  91.41 ( 92.31)	Acc@5 100.00 ( 99.56)
Epoch: [30][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0746e-01 (2.5990e-01)	Acc@1  92.97 ( 92.35)	Acc@5 100.00 ( 99.57)
Epoch: [30][320/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0647e-01 (2.5778e-01)	Acc@1  93.75 ( 92.41)	Acc@5  99.22 ( 99.57)
Epoch: [30][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1449e-01 (2.5599e-01)	Acc@1  94.53 ( 92.47)	Acc@5 100.00 ( 99.58)
Epoch: [30][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9472e-01 (2.5441e-01)	Acc@1  94.53 ( 92.52)	Acc@5 100.00 ( 99.59)
Epoch: [30][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7464e-01 (2.5317e-01)	Acc@1  90.62 ( 92.56)	Acc@5 100.00 ( 99.61)
Epoch: [30][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9199e-01 (2.5145e-01)	Acc@1  96.88 ( 92.63)	Acc@5  99.22 ( 99.61)
Epoch: [30][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7954e-01 (2.4975e-01)	Acc@1  95.31 ( 92.70)	Acc@5  98.44 ( 99.61)
Epoch: [30][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0003e-01 (2.4808e-01)	Acc@1  95.31 ( 92.76)	Acc@5 100.00 ( 99.62)
Epoch: [30][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7250e-01 (2.4638e-01)	Acc@1  90.00 ( 92.81)	Acc@5  98.75 ( 99.62)
## e[30] optimizer.zero_grad (sum) time: 0.668743371963501
## e[30]       loss.backward (sum) time: 13.847472667694092
## e[30]      optimizer.step (sum) time: 14.748240232467651
## epoch[30] training(only) time: 53.99920845031738
# Switched to evaluate mode...
Test: [  0/100]	Time  0.202 ( 0.202)	Loss 1.1088e+00 (1.1088e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.050 ( 0.062)	Loss 1.2777e+00 (1.1683e+00)	Acc@1  69.00 ( 73.00)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 9.0460e-01 (1.0644e+00)	Acc@1  75.00 ( 73.71)	Acc@5  94.00 ( 92.67)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5380e+00 (1.0923e+00)	Acc@1  64.00 ( 73.03)	Acc@5  92.00 ( 92.32)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.2666e+00 (1.0871e+00)	Acc@1  68.00 ( 72.66)	Acc@5  92.00 ( 92.63)
Test: [ 50/100]	Time  0.058 ( 0.051)	Loss 9.8777e-01 (1.0894e+00)	Acc@1  76.00 ( 72.75)	Acc@5  93.00 ( 92.53)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 9.4651e-01 (1.0772e+00)	Acc@1  76.00 ( 72.82)	Acc@5  99.00 ( 92.75)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.0873e+00 (1.0824e+00)	Acc@1  74.00 ( 72.80)	Acc@5  96.00 ( 92.82)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.0952e+00 (1.0846e+00)	Acc@1  73.00 ( 72.80)	Acc@5  92.00 ( 92.85)
Test: [ 90/100]	Time  0.050 ( 0.050)	Loss 1.3819e+00 (1.0718e+00)	Acc@1  65.00 ( 72.88)	Acc@5  90.00 ( 93.02)
 * Acc@1 73.140 Acc@5 93.070
### epoch[30] execution time: 59.00047850608826
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.321 ( 0.321)	Data  0.165 ( 0.165)	Loss 1.2038e-01 (1.2038e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.016)	Loss 1.0497e-01 (1.5371e-01)	Acc@1  99.22 ( 96.24)	Acc@5 100.00 ( 99.86)
Epoch: [31][ 20/391]	Time  0.140 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.7804e-01 (1.4871e-01)	Acc@1  96.09 ( 96.54)	Acc@5 100.00 ( 99.93)
Epoch: [31][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.9864e-01 (1.6230e-01)	Acc@1  96.88 ( 96.04)	Acc@5 100.00 ( 99.90)
Epoch: [31][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.6537e-01 (1.6504e-01)	Acc@1  95.31 ( 95.77)	Acc@5 100.00 ( 99.92)
Epoch: [31][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 9.3382e-02 (1.5893e-01)	Acc@1  98.44 ( 95.99)	Acc@5 100.00 ( 99.91)
Epoch: [31][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.4368e-01 (1.6064e-01)	Acc@1  97.66 ( 95.95)	Acc@5 100.00 ( 99.90)
Epoch: [31][ 70/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6723e-01 (1.6078e-01)	Acc@1  96.88 ( 95.94)	Acc@5  99.22 ( 99.89)
Epoch: [31][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7235e-01 (1.6439e-01)	Acc@1  93.75 ( 95.81)	Acc@5  98.44 ( 99.86)
Epoch: [31][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6820e-01 (1.6570e-01)	Acc@1  93.75 ( 95.75)	Acc@5 100.00 ( 99.87)
Epoch: [31][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7782e-01 (1.6535e-01)	Acc@1  95.31 ( 95.70)	Acc@5 100.00 ( 99.87)
Epoch: [31][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2701e-01 (1.6477e-01)	Acc@1  96.09 ( 95.74)	Acc@5 100.00 ( 99.87)
Epoch: [31][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0913e-01 (1.6346e-01)	Acc@1  97.66 ( 95.76)	Acc@5 100.00 ( 99.88)
Epoch: [31][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5590e-01 (1.6276e-01)	Acc@1  96.09 ( 95.80)	Acc@5 100.00 ( 99.88)
Epoch: [31][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4139e-01 (1.6400e-01)	Acc@1  92.97 ( 95.79)	Acc@5  99.22 ( 99.86)
Epoch: [31][150/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3401e-01 (1.6259e-01)	Acc@1  98.44 ( 95.85)	Acc@5 100.00 ( 99.86)
Epoch: [31][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5576e-01 (1.6413e-01)	Acc@1  93.75 ( 95.81)	Acc@5  99.22 ( 99.85)
Epoch: [31][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7860e-01 (1.6331e-01)	Acc@1  95.31 ( 95.83)	Acc@5 100.00 ( 99.86)
Epoch: [31][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0688e-02 (1.6447e-01)	Acc@1  98.44 ( 95.76)	Acc@5 100.00 ( 99.86)
Epoch: [31][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6325e-01 (1.6394e-01)	Acc@1  94.53 ( 95.78)	Acc@5 100.00 ( 99.86)
Epoch: [31][200/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6436e-01 (1.6288e-01)	Acc@1  95.31 ( 95.80)	Acc@5 100.00 ( 99.87)
Epoch: [31][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0800e-01 (1.6156e-01)	Acc@1  96.88 ( 95.86)	Acc@5 100.00 ( 99.87)
Epoch: [31][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5650e-01 (1.6111e-01)	Acc@1  94.53 ( 95.89)	Acc@5 100.00 ( 99.88)
Epoch: [31][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2896e-01 (1.6223e-01)	Acc@1  92.97 ( 95.85)	Acc@5 100.00 ( 99.87)
Epoch: [31][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1039e-01 (1.6258e-01)	Acc@1  97.66 ( 95.81)	Acc@5 100.00 ( 99.87)
Epoch: [31][250/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4683e-01 (1.6150e-01)	Acc@1  95.31 ( 95.85)	Acc@5 100.00 ( 99.87)
Epoch: [31][260/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.8716e-02 (1.6054e-01)	Acc@1  99.22 ( 95.91)	Acc@5 100.00 ( 99.88)
Epoch: [31][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5137e-01 (1.6015e-01)	Acc@1  96.88 ( 95.93)	Acc@5 100.00 ( 99.88)
Epoch: [31][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7440e-01 (1.6024e-01)	Acc@1  94.53 ( 95.91)	Acc@5 100.00 ( 99.87)
Epoch: [31][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6955e-01 (1.6018e-01)	Acc@1  89.84 ( 95.90)	Acc@5 100.00 ( 99.87)
Epoch: [31][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3406e-01 (1.5979e-01)	Acc@1  96.09 ( 95.89)	Acc@5 100.00 ( 99.87)
Epoch: [31][310/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1336e-01 (1.5954e-01)	Acc@1  98.44 ( 95.88)	Acc@5 100.00 ( 99.87)
Epoch: [31][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3634e-01 (1.5887e-01)	Acc@1  97.66 ( 95.91)	Acc@5 100.00 ( 99.88)
Epoch: [31][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5424e-01 (1.5847e-01)	Acc@1  96.88 ( 95.91)	Acc@5 100.00 ( 99.87)
Epoch: [31][340/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8764e-02 (1.5829e-01)	Acc@1  99.22 ( 95.90)	Acc@5 100.00 ( 99.87)
Epoch: [31][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2282e-01 (1.5796e-01)	Acc@1  96.88 ( 95.90)	Acc@5 100.00 ( 99.88)
Epoch: [31][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2283e-01 (1.5836e-01)	Acc@1  93.75 ( 95.88)	Acc@5 100.00 ( 99.88)
Epoch: [31][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2316e-01 (1.5833e-01)	Acc@1  93.75 ( 95.88)	Acc@5 100.00 ( 99.88)
Epoch: [31][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1039e-01 (1.5796e-01)	Acc@1  92.19 ( 95.88)	Acc@5 100.00 ( 99.88)
Epoch: [31][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3761e-01 (1.5843e-01)	Acc@1  93.75 ( 95.85)	Acc@5  98.75 ( 99.88)
## e[31] optimizer.zero_grad (sum) time: 0.6708111763000488
## e[31]       loss.backward (sum) time: 13.872629642486572
## e[31]      optimizer.step (sum) time: 14.679530143737793
## epoch[31] training(only) time: 53.98464512825012
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.0997e+00 (1.0997e+00)	Acc@1  73.00 ( 73.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.2758e+00 (1.1828e+00)	Acc@1  73.00 ( 73.64)	Acc@5  91.00 ( 91.55)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 9.3565e-01 (1.0822e+00)	Acc@1  73.00 ( 73.52)	Acc@5  92.00 ( 92.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.4278e+00 (1.1090e+00)	Acc@1  66.00 ( 72.71)	Acc@5  92.00 ( 92.39)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.2414e+00 (1.1018e+00)	Acc@1  72.00 ( 72.76)	Acc@5  93.00 ( 92.71)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.0283e+00 (1.0998e+00)	Acc@1  75.00 ( 72.76)	Acc@5  92.00 ( 92.61)
Test: [ 60/100]	Time  0.050 ( 0.050)	Loss 9.4776e-01 (1.0859e+00)	Acc@1  74.00 ( 73.03)	Acc@5  98.00 ( 92.87)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.0762e+00 (1.0916e+00)	Acc@1  76.00 ( 73.13)	Acc@5  94.00 ( 92.92)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.1131e+00 (1.0929e+00)	Acc@1  76.00 ( 73.11)	Acc@5  92.00 ( 93.00)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3985e+00 (1.0784e+00)	Acc@1  68.00 ( 73.26)	Acc@5  90.00 ( 93.18)
 * Acc@1 73.490 Acc@5 93.200
### epoch[31] execution time: 58.95942783355713
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.295 ( 0.295)	Data  0.143 ( 0.143)	Loss 1.1170e-01 (1.1170e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.142 ( 0.151)	Data  0.001 ( 0.014)	Loss 1.0586e-01 (1.3197e-01)	Acc@1  98.44 ( 96.80)	Acc@5 100.00 ( 99.72)
Epoch: [32][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.008)	Loss 1.4496e-01 (1.3176e-01)	Acc@1  97.66 ( 96.88)	Acc@5  99.22 ( 99.81)
Epoch: [32][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.0603e-01 (1.3050e-01)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.82)
Epoch: [32][ 40/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.6559e-01 (1.3103e-01)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 ( 99.83)
Epoch: [32][ 50/391]	Time  0.141 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.8582e-01 (1.3384e-01)	Acc@1  94.53 ( 96.77)	Acc@5 100.00 ( 99.86)
Epoch: [32][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5174e-01 (1.3294e-01)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.88)
Epoch: [32][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2469e-01 (1.3498e-01)	Acc@1  98.44 ( 96.69)	Acc@5 100.00 ( 99.89)
Epoch: [32][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0947e-01 (1.3323e-01)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.87)
Epoch: [32][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6702e-01 (1.3253e-01)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.89)
Epoch: [32][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7384e-01 (1.3309e-01)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.87)
Epoch: [32][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5907e-01 (1.3286e-01)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 ( 99.87)
Epoch: [32][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2183e-01 (1.3470e-01)	Acc@1  96.88 ( 96.66)	Acc@5 100.00 ( 99.86)
Epoch: [32][130/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5011e-01 (1.3300e-01)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.87)
Epoch: [32][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6261e-01 (1.3388e-01)	Acc@1  95.31 ( 96.72)	Acc@5 100.00 ( 99.87)
Epoch: [32][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2219e-01 (1.3430e-01)	Acc@1  95.31 ( 96.68)	Acc@5 100.00 ( 99.86)
Epoch: [32][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2627e-01 (1.3376e-01)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.87)
Epoch: [32][170/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0466e-01 (1.3300e-01)	Acc@1  96.88 ( 96.72)	Acc@5 100.00 ( 99.88)
Epoch: [32][180/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5043e-01 (1.3299e-01)	Acc@1  95.31 ( 96.70)	Acc@5 100.00 ( 99.88)
Epoch: [32][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1584e-01 (1.3216e-01)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.88)
Epoch: [32][200/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5026e-01 (1.3214e-01)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 ( 99.88)
Epoch: [32][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4028e-01 (1.3141e-01)	Acc@1  96.09 ( 96.75)	Acc@5 100.00 ( 99.89)
Epoch: [32][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5268e-01 (1.3061e-01)	Acc@1  94.53 ( 96.77)	Acc@5 100.00 ( 99.89)
Epoch: [32][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6526e-02 (1.2984e-01)	Acc@1  98.44 ( 96.79)	Acc@5 100.00 ( 99.90)
Epoch: [32][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3842e-01 (1.2997e-01)	Acc@1  96.09 ( 96.78)	Acc@5 100.00 ( 99.89)
Epoch: [32][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1196e-01 (1.2985e-01)	Acc@1  96.88 ( 96.78)	Acc@5 100.00 ( 99.89)
Epoch: [32][260/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2032e-01 (1.3015e-01)	Acc@1  98.44 ( 96.78)	Acc@5 100.00 ( 99.89)
Epoch: [32][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6555e-02 (1.2974e-01)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.90)
Epoch: [32][280/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1811e-01 (1.2960e-01)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.90)
Epoch: [32][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2923e-01 (1.2967e-01)	Acc@1  96.09 ( 96.78)	Acc@5  99.22 ( 99.89)
Epoch: [32][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1588e-01 (1.2913e-01)	Acc@1  96.09 ( 96.80)	Acc@5 100.00 ( 99.89)
Epoch: [32][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4586e-01 (1.2938e-01)	Acc@1  93.75 ( 96.79)	Acc@5 100.00 ( 99.89)
Epoch: [32][320/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9319e-02 (1.2900e-01)	Acc@1  99.22 ( 96.81)	Acc@5 100.00 ( 99.90)
Epoch: [32][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.7542e-02 (1.2855e-01)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.90)
Epoch: [32][340/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7839e-01 (1.2840e-01)	Acc@1  92.19 ( 96.82)	Acc@5 100.00 ( 99.90)
Epoch: [32][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1943e-01 (1.2818e-01)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.90)
Epoch: [32][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8669e-01 (1.2789e-01)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.90)
Epoch: [32][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0050e-01 (1.2757e-01)	Acc@1  97.66 ( 96.85)	Acc@5 100.00 ( 99.90)
Epoch: [32][380/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3832e-01 (1.2716e-01)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.90)
Epoch: [32][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2935e-01 (1.2715e-01)	Acc@1  97.50 ( 96.86)	Acc@5 100.00 ( 99.90)
## e[32] optimizer.zero_grad (sum) time: 0.6671488285064697
## e[32]       loss.backward (sum) time: 13.874145746231079
## e[32]      optimizer.step (sum) time: 14.707913398742676
## epoch[32] training(only) time: 54.00901675224304
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1171e+00 (1.1171e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.2645e+00 (1.1851e+00)	Acc@1  74.00 ( 73.55)	Acc@5  91.00 ( 91.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 9.1626e-01 (1.0843e+00)	Acc@1  76.00 ( 73.95)	Acc@5  92.00 ( 92.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.5050e+00 (1.1161e+00)	Acc@1  63.00 ( 73.42)	Acc@5  91.00 ( 92.55)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.3013e+00 (1.1100e+00)	Acc@1  73.00 ( 73.27)	Acc@5  92.00 ( 92.80)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.0008e+00 (1.1041e+00)	Acc@1  75.00 ( 73.39)	Acc@5  92.00 ( 92.86)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 9.1221e-01 (1.0883e+00)	Acc@1  75.00 ( 73.39)	Acc@5  99.00 ( 93.11)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.1395e+00 (1.0955e+00)	Acc@1  74.00 ( 73.30)	Acc@5  95.00 ( 93.20)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.1753e+00 (1.0979e+00)	Acc@1  74.00 ( 73.26)	Acc@5  93.00 ( 93.25)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3650e+00 (1.0838e+00)	Acc@1  73.00 ( 73.44)	Acc@5  90.00 ( 93.42)
 * Acc@1 73.750 Acc@5 93.410
### epoch[32] execution time: 58.991326332092285
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.310 ( 0.310)	Data  0.151 ( 0.151)	Loss 1.1962e-01 (1.1962e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.1368e-01 (1.1628e-01)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 ( 99.93)
Epoch: [33][ 20/391]	Time  0.142 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.2546e-01 (1.1095e-01)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.89)
Epoch: [33][ 30/391]	Time  0.147 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.2427e-01 (1.1489e-01)	Acc@1  96.09 ( 97.45)	Acc@5 100.00 ( 99.85)
Epoch: [33][ 40/391]	Time  0.133 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.1745e-01 (1.1348e-01)	Acc@1  99.22 ( 97.62)	Acc@5 100.00 ( 99.87)
Epoch: [33][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 9.9141e-02 (1.1360e-01)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.89)
Epoch: [33][ 60/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2094e-01 (1.1317e-01)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.90)
Epoch: [33][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.9710e-02 (1.1185e-01)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.91)
Epoch: [33][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4526e-01 (1.1023e-01)	Acc@1  96.88 ( 97.63)	Acc@5 100.00 ( 99.92)
Epoch: [33][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0006e-01 (1.0891e-01)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.93)
Epoch: [33][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.1015e-01 (1.0759e-01)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.94)
Epoch: [33][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2137e-01 (1.0647e-01)	Acc@1  97.66 ( 97.68)	Acc@5 100.00 ( 99.94)
Epoch: [33][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8236e-02 (1.0668e-01)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.94)
Epoch: [33][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.1915e-02 (1.0805e-01)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.93)
Epoch: [33][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2994e-01 (1.0826e-01)	Acc@1  96.09 ( 97.59)	Acc@5 100.00 ( 99.94)
Epoch: [33][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.1273e-02 (1.0789e-01)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.93)
Epoch: [33][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.1842e-02 (1.0828e-01)	Acc@1  97.66 ( 97.55)	Acc@5 100.00 ( 99.93)
Epoch: [33][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2058e-01 (1.0848e-01)	Acc@1  99.22 ( 97.56)	Acc@5 100.00 ( 99.93)
Epoch: [33][180/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2954e-01 (1.0872e-01)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 ( 99.93)
Epoch: [33][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1648e-01 (1.0783e-01)	Acc@1  95.31 ( 97.58)	Acc@5 100.00 ( 99.93)
Epoch: [33][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4551e-02 (1.0747e-01)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.93)
Epoch: [33][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0423e-01 (1.0736e-01)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.93)
Epoch: [33][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0241e-01 (1.0746e-01)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.93)
Epoch: [33][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3535e-02 (1.0752e-01)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.93)
Epoch: [33][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0118e-02 (1.0709e-01)	Acc@1  99.22 ( 97.60)	Acc@5 100.00 ( 99.94)
Epoch: [33][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3118e-01 (1.0788e-01)	Acc@1  96.09 ( 97.56)	Acc@5 100.00 ( 99.93)
Epoch: [33][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1434e-01 (1.0791e-01)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.94)
Epoch: [33][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0502e-01 (1.0786e-01)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.94)
Epoch: [33][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1757e-01 (1.0770e-01)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.94)
Epoch: [33][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5790e-01 (1.0809e-01)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 ( 99.94)
Epoch: [33][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6075e-01 (1.0827e-01)	Acc@1  96.09 ( 97.54)	Acc@5 100.00 ( 99.94)
Epoch: [33][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3601e-02 (1.0806e-01)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.94)
Epoch: [33][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5814e-02 (1.0775e-01)	Acc@1 100.00 ( 97.57)	Acc@5 100.00 ( 99.94)
Epoch: [33][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5656e-02 (1.0798e-01)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.94)
Epoch: [33][340/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5958e-01 (1.0795e-01)	Acc@1  93.75 ( 97.53)	Acc@5 100.00 ( 99.95)
Epoch: [33][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1984e-01 (1.0807e-01)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.94)
Epoch: [33][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0019e-01 (1.0791e-01)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.95)
Epoch: [33][370/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0598e-01 (1.0773e-01)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.95)
Epoch: [33][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0319e-01 (1.0791e-01)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.95)
Epoch: [33][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0087e-01 (1.0792e-01)	Acc@1  98.75 ( 97.51)	Acc@5 100.00 ( 99.95)
## e[33] optimizer.zero_grad (sum) time: 0.6677143573760986
## e[33]       loss.backward (sum) time: 13.841410160064697
## e[33]      optimizer.step (sum) time: 14.716360092163086
## epoch[33] training(only) time: 53.99037528038025
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1132e+00 (1.1132e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.3100e+00 (1.2149e+00)	Acc@1  74.00 ( 73.55)	Acc@5  91.00 ( 91.09)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 9.6019e-01 (1.1092e+00)	Acc@1  75.00 ( 74.00)	Acc@5  92.00 ( 92.48)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.4597e+00 (1.1335e+00)	Acc@1  66.00 ( 73.32)	Acc@5  90.00 ( 92.19)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.2999e+00 (1.1278e+00)	Acc@1  71.00 ( 73.15)	Acc@5  91.00 ( 92.51)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.0442e+00 (1.1246e+00)	Acc@1  75.00 ( 73.20)	Acc@5  92.00 ( 92.53)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 9.3753e-01 (1.1066e+00)	Acc@1  79.00 ( 73.52)	Acc@5  97.00 ( 92.75)
Test: [ 70/100]	Time  0.050 ( 0.050)	Loss 1.1345e+00 (1.1123e+00)	Acc@1  76.00 ( 73.62)	Acc@5  96.00 ( 92.89)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.1873e+00 (1.1137e+00)	Acc@1  76.00 ( 73.53)	Acc@5  92.00 ( 92.94)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4555e+00 (1.0977e+00)	Acc@1  70.00 ( 73.60)	Acc@5  90.00 ( 93.16)
 * Acc@1 73.910 Acc@5 93.210
### epoch[33] execution time: 58.98375678062439
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.298 ( 0.298)	Data  0.148 ( 0.148)	Loss 9.9023e-02 (9.9023e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.136 ( 0.153)	Data  0.001 ( 0.014)	Loss 7.3086e-02 (1.0309e-01)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 7.3445e-02 (9.5496e-02)	Acc@1  99.22 ( 97.99)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 6.7405e-02 (9.8721e-02)	Acc@1 100.00 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [34][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 6.5939e-02 (9.6325e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 ( 99.96)
Epoch: [34][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2966e-01 (9.5568e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 60/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.1201e-01 (9.5027e-02)	Acc@1  96.09 ( 98.01)	Acc@5 100.00 ( 99.97)
Epoch: [34][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5715e-01 (9.5069e-02)	Acc@1  93.75 ( 97.98)	Acc@5 100.00 ( 99.98)
Epoch: [34][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2544e-01 (9.5871e-02)	Acc@1  96.88 ( 97.91)	Acc@5  99.22 ( 99.96)
Epoch: [34][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.5289e-02 (9.4785e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.96)
Epoch: [34][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2975e-01 (9.4788e-02)	Acc@1  95.31 ( 97.90)	Acc@5  99.22 ( 99.95)
Epoch: [34][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.9384e-02 (9.3890e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [34][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.5409e-02 (9.3634e-02)	Acc@1  96.09 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [34][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0129e-01 (9.5369e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.96)
Epoch: [34][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9118e-02 (9.5599e-02)	Acc@1  99.22 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [34][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2305e-01 (9.5254e-02)	Acc@1  95.31 ( 97.90)	Acc@5 100.00 ( 99.97)
Epoch: [34][160/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0387e-01 (9.5351e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [34][170/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1084e-01 (9.5429e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [34][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6867e-02 (9.5576e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [34][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0639e-01 (9.6345e-02)	Acc@1  97.66 ( 97.86)	Acc@5  99.22 ( 99.97)
Epoch: [34][200/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2080e-01 (9.6195e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 ( 99.97)
Epoch: [34][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5718e-02 (9.6138e-02)	Acc@1  97.66 ( 97.84)	Acc@5 100.00 ( 99.97)
Epoch: [34][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0986e-02 (9.6523e-02)	Acc@1  97.66 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [34][230/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4314e-02 (9.6448e-02)	Acc@1  99.22 ( 97.84)	Acc@5 100.00 ( 99.97)
Epoch: [34][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1301e-01 (9.6548e-02)	Acc@1  96.88 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [34][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2293e-01 (9.6141e-02)	Acc@1  96.88 ( 97.84)	Acc@5 100.00 ( 99.98)
Epoch: [34][260/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1589e-02 (9.6241e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.98)
Epoch: [34][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1104e-02 (9.6037e-02)	Acc@1 100.00 ( 97.86)	Acc@5 100.00 ( 99.98)
Epoch: [34][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8816e-02 (9.6486e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.97)
Epoch: [34][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9238e-01 (9.7011e-02)	Acc@1  94.53 ( 97.82)	Acc@5  99.22 ( 99.97)
Epoch: [34][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.3116e-02 (9.6999e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.97)
Epoch: [34][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5808e-02 (9.7281e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [34][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0423e-01 (9.7718e-02)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [34][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6022e-02 (9.7365e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [34][340/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8751e-02 (9.7445e-02)	Acc@1  98.44 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [34][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1677e-01 (9.7123e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [34][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0425e-01 (9.6977e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [34][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6717e-02 (9.7191e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.97)
Epoch: [34][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2980e-02 (9.7164e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.97)
Epoch: [34][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8673e-02 (9.7077e-02)	Acc@1 100.00 ( 97.80)	Acc@5 100.00 ( 99.97)
## e[34] optimizer.zero_grad (sum) time: 0.6707463264465332
## e[34]       loss.backward (sum) time: 13.88730788230896
## e[34]      optimizer.step (sum) time: 14.679338932037354
## epoch[34] training(only) time: 54.00064778327942
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.1311e+00 (1.1311e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.3079e+00 (1.2210e+00)	Acc@1  73.00 ( 73.18)	Acc@5  91.00 ( 90.91)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 9.2741e-01 (1.1155e+00)	Acc@1  74.00 ( 73.62)	Acc@5  93.00 ( 92.48)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4622e+00 (1.1486e+00)	Acc@1  69.00 ( 73.03)	Acc@5  92.00 ( 92.13)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.3217e+00 (1.1462e+00)	Acc@1  68.00 ( 72.85)	Acc@5  92.00 ( 92.27)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.0310e+00 (1.1372e+00)	Acc@1  76.00 ( 73.18)	Acc@5  92.00 ( 92.20)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 9.7810e-01 (1.1220e+00)	Acc@1  77.00 ( 73.39)	Acc@5  97.00 ( 92.52)
Test: [ 70/100]	Time  0.045 ( 0.050)	Loss 1.1456e+00 (1.1265e+00)	Acc@1  74.00 ( 73.32)	Acc@5  96.00 ( 92.63)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.2075e+00 (1.1266e+00)	Acc@1  73.00 ( 73.32)	Acc@5  94.00 ( 92.77)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4265e+00 (1.1100e+00)	Acc@1  64.00 ( 73.40)	Acc@5  90.00 ( 92.96)
 * Acc@1 73.760 Acc@5 93.020
### epoch[34] execution time: 58.99312615394592
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.297 ( 0.297)	Data  0.149 ( 0.149)	Loss 1.0399e-01 (1.0399e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.140 ( 0.152)	Data  0.001 ( 0.014)	Loss 8.7544e-02 (8.0462e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.134 ( 0.146)	Data  0.001 ( 0.008)	Loss 7.7192e-02 (8.1032e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 5.0647e-02 (7.9917e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 ( 99.95)
Epoch: [35][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 8.6181e-02 (8.1541e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.94)
Epoch: [35][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.2243e-02 (8.1373e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.95)
Epoch: [35][ 60/391]	Time  0.148 ( 0.140)	Data  0.001 ( 0.004)	Loss 8.8461e-02 (8.2765e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 ( 99.96)
Epoch: [35][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.1079e-02 (8.2412e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.9848e-02 (8.1692e-02)	Acc@1 100.00 ( 98.36)	Acc@5 100.00 ( 99.97)
Epoch: [35][ 90/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2066e-01 (8.2702e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [35][100/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.0135e-02 (8.3463e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [35][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1035e-02 (8.1994e-02)	Acc@1 100.00 ( 98.38)	Acc@5 100.00 ( 99.97)
Epoch: [35][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1141e-01 (8.1973e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 ( 99.96)
Epoch: [35][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.5161e-02 (8.3543e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.96)
Epoch: [35][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.4610e-02 (8.3464e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 ( 99.97)
Epoch: [35][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.6849e-02 (8.3113e-02)	Acc@1  96.88 ( 98.38)	Acc@5 100.00 ( 99.97)
Epoch: [35][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3084e-02 (8.2833e-02)	Acc@1  98.44 ( 98.39)	Acc@5 100.00 ( 99.97)
Epoch: [35][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.7394e-02 (8.3358e-02)	Acc@1  96.88 ( 98.35)	Acc@5 100.00 ( 99.97)
Epoch: [35][180/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2302e-01 (8.3666e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.97)
Epoch: [35][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5960e-02 (8.4383e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [35][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0439e-02 (8.4834e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [35][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.3845e-02 (8.4545e-02)	Acc@1  96.88 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [35][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5347e-02 (8.4330e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [35][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2571e-02 (8.4415e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [35][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4301e-02 (8.4226e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [35][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5153e-02 (8.4470e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [35][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2216e-01 (8.4542e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.97)
Epoch: [35][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8353e-02 (8.4556e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [35][280/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0162e-01 (8.4447e-02)	Acc@1  97.66 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [35][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5045e-02 (8.4567e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.97)
Epoch: [35][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1406e-02 (8.4923e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [35][310/391]	Time  0.150 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9552e-02 (8.5433e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [35][320/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8683e-02 (8.5349e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [35][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2275e-01 (8.5400e-02)	Acc@1  96.09 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [35][340/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9480e-02 (8.5324e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [35][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0006e-02 (8.5487e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [35][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4118e-01 (8.5527e-02)	Acc@1  95.31 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [35][370/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9471e-02 (8.5554e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [35][380/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3397e-02 (8.5130e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [35][390/391]	Time  0.105 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5760e-01 (8.5552e-02)	Acc@1  96.25 ( 98.14)	Acc@5 100.00 ( 99.97)
## e[35] optimizer.zero_grad (sum) time: 0.6728236675262451
## e[35]       loss.backward (sum) time: 13.905817031860352
## e[35]      optimizer.step (sum) time: 14.706976413726807
## epoch[35] training(only) time: 53.96523332595825
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.1982e+00 (1.1982e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.058 ( 0.062)	Loss 1.3971e+00 (1.2287e+00)	Acc@1  75.00 ( 73.27)	Acc@5  90.00 ( 91.27)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 9.7203e-01 (1.1239e+00)	Acc@1  75.00 ( 73.86)	Acc@5  93.00 ( 92.67)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 1.4691e+00 (1.1551e+00)	Acc@1  70.00 ( 73.23)	Acc@5  92.00 ( 92.39)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.2738e+00 (1.1493e+00)	Acc@1  73.00 ( 73.32)	Acc@5  92.00 ( 92.66)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.0625e+00 (1.1417e+00)	Acc@1  77.00 ( 73.47)	Acc@5  93.00 ( 92.63)
Test: [ 60/100]	Time  0.054 ( 0.050)	Loss 9.9729e-01 (1.1288e+00)	Acc@1  74.00 ( 73.59)	Acc@5  96.00 ( 92.85)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.1473e+00 (1.1357e+00)	Acc@1  77.00 ( 73.56)	Acc@5  96.00 ( 92.87)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.2109e+00 (1.1361e+00)	Acc@1  71.00 ( 73.38)	Acc@5  91.00 ( 92.91)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4907e+00 (1.1216e+00)	Acc@1  68.00 ( 73.48)	Acc@5  91.00 ( 93.11)
 * Acc@1 73.800 Acc@5 93.150
### epoch[35] execution time: 58.96591329574585
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.295 ( 0.295)	Data  0.141 ( 0.141)	Loss 7.4665e-02 (7.4665e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.140 ( 0.152)	Data  0.001 ( 0.014)	Loss 6.8736e-02 (7.4469e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [36][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.008)	Loss 8.5699e-02 (7.8113e-02)	Acc@1  97.66 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [36][ 30/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.006)	Loss 8.1389e-02 (7.5298e-02)	Acc@1  97.66 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [36][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.0205e-01 (7.7357e-02)	Acc@1  96.09 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [36][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.2138e-02 (7.6325e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 ( 99.98)
Epoch: [36][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.5855e-02 (7.9150e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.8020e-02 (7.8504e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 80/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.4500e-02 (7.7462e-02)	Acc@1  98.44 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [36][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.8884e-02 (7.8007e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.98)
Epoch: [36][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.5836e-02 (7.8206e-02)	Acc@1  98.44 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [36][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.8355e-02 (7.7507e-02)	Acc@1  98.44 ( 98.50)	Acc@5 100.00 ( 99.99)
Epoch: [36][120/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1207e-01 (7.7571e-02)	Acc@1  96.09 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [36][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4269e-02 (7.7023e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [36][140/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3007e-02 (7.6900e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [36][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0655e-01 (7.6670e-02)	Acc@1  96.09 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [36][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5197e-02 (7.6176e-02)	Acc@1  97.66 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [36][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0547e-01 (7.6463e-02)	Acc@1  96.88 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [36][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5980e-02 (7.5749e-02)	Acc@1 100.00 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [36][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3235e-02 (7.5581e-02)	Acc@1  98.44 ( 98.46)	Acc@5 100.00 ( 99.98)
Epoch: [36][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2940e-02 (7.5719e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [36][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8178e-02 (7.5845e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [36][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3076e-01 (7.6243e-02)	Acc@1  95.31 ( 98.41)	Acc@5  99.22 ( 99.98)
Epoch: [36][230/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0308e-01 (7.6763e-02)	Acc@1  96.09 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [36][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8951e-02 (7.6888e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [36][250/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1665e-01 (7.7168e-02)	Acc@1  96.09 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [36][260/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1073e-02 (7.7013e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [36][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1989e-02 (7.7263e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [36][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1958e-01 (7.7300e-02)	Acc@1  96.88 ( 98.33)	Acc@5  99.22 ( 99.98)
Epoch: [36][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1083e-01 (7.7437e-02)	Acc@1  96.09 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [36][300/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7239e-02 (7.7380e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [36][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3909e-02 (7.7613e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [36][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1114e-02 (7.7593e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [36][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1938e-01 (7.7723e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [36][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6742e-02 (7.7550e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [36][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8808e-02 (7.7505e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [36][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4428e-02 (7.7674e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [36][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.3019e-02 (7.7825e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [36][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8627e-02 (7.7817e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [36][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8760e-02 (7.7568e-02)	Acc@1  98.75 ( 98.31)	Acc@5 100.00 ( 99.98)
## e[36] optimizer.zero_grad (sum) time: 0.6685726642608643
## e[36]       loss.backward (sum) time: 13.805895328521729
## e[36]      optimizer.step (sum) time: 14.710418224334717
## epoch[36] training(only) time: 53.89820146560669
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.1469e+00 (1.1469e+00)	Acc@1  70.00 ( 70.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.3627e+00 (1.2411e+00)	Acc@1  72.00 ( 73.18)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 9.7244e-01 (1.1386e+00)	Acc@1  75.00 ( 73.90)	Acc@5  93.00 ( 92.38)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.4592e+00 (1.1641e+00)	Acc@1  66.00 ( 73.26)	Acc@5  91.00 ( 92.23)
Test: [ 40/100]	Time  0.050 ( 0.051)	Loss 1.3861e+00 (1.1626e+00)	Acc@1  72.00 ( 73.10)	Acc@5  90.00 ( 92.51)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.0424e+00 (1.1533e+00)	Acc@1  77.00 ( 73.47)	Acc@5  92.00 ( 92.45)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0224e+00 (1.1375e+00)	Acc@1  77.00 ( 73.66)	Acc@5  97.00 ( 92.74)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.2232e+00 (1.1451e+00)	Acc@1  76.00 ( 73.65)	Acc@5  96.00 ( 92.85)
Test: [ 80/100]	Time  0.050 ( 0.049)	Loss 1.2459e+00 (1.1490e+00)	Acc@1  73.00 ( 73.44)	Acc@5  93.00 ( 92.94)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.4540e+00 (1.1331e+00)	Acc@1  71.00 ( 73.62)	Acc@5  90.00 ( 93.13)
 * Acc@1 73.860 Acc@5 93.160
### epoch[36] execution time: 58.88423538208008
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.307 ( 0.307)	Data  0.156 ( 0.156)	Loss 8.1278e-02 (8.1278e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.138 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.1050e-01 (7.6726e-02)	Acc@1  98.44 ( 98.22)	Acc@5  99.22 ( 99.93)
Epoch: [37][ 20/391]	Time  0.139 ( 0.145)	Data  0.001 ( 0.008)	Loss 6.5366e-02 (6.6953e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 30/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.006)	Loss 6.8917e-02 (6.6186e-02)	Acc@1  98.44 ( 98.74)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 40/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.005)	Loss 8.1906e-02 (6.7379e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 50/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.9371e-02 (6.7985e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.98)
Epoch: [37][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 7.2338e-02 (6.7252e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4614e-01 (6.7016e-02)	Acc@1  96.09 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0046e-01 (6.8063e-02)	Acc@1  96.09 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [37][ 90/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.003)	Loss 8.4450e-02 (6.7636e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [37][100/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.2272e-02 (6.7306e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [37][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.7956e-02 (6.7042e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [37][120/391]	Time  0.151 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6802e-02 (6.6822e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [37][130/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.9414e-02 (6.6576e-02)	Acc@1  97.66 ( 98.79)	Acc@5  99.22 ( 99.99)
Epoch: [37][140/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6666e-02 (6.6203e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [37][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8705e-02 (6.6570e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 ( 99.98)
Epoch: [37][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8332e-02 (6.6647e-02)	Acc@1  98.44 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [37][170/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7574e-02 (6.6503e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [37][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3695e-02 (6.6722e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [37][190/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0149e-02 (6.7141e-02)	Acc@1  98.44 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [37][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8322e-02 (6.7247e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [37][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4246e-02 (6.7665e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [37][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1631e-02 (6.8486e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [37][230/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6363e-02 (6.8788e-02)	Acc@1  98.44 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [37][240/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4838e-02 (6.8938e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [37][250/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0167e-02 (6.9002e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [37][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2025e-02 (6.9045e-02)	Acc@1  97.66 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [37][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4260e-02 (6.8788e-02)	Acc@1  98.44 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [37][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6178e-02 (6.8795e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [37][290/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4287e-02 (6.9259e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [37][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2416e-02 (6.9583e-02)	Acc@1  97.66 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [37][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0192e-01 (6.9863e-02)	Acc@1  97.66 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [37][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3779e-02 (6.9912e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [37][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4568e-01 (7.0000e-02)	Acc@1  96.09 ( 98.56)	Acc@5  99.22 ( 99.99)
Epoch: [37][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9966e-02 (7.0393e-02)	Acc@1  96.88 ( 98.55)	Acc@5 100.00 ( 99.99)
Epoch: [37][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6231e-02 (7.0540e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 ( 99.99)
Epoch: [37][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9569e-02 (7.0498e-02)	Acc@1  98.44 ( 98.53)	Acc@5 100.00 ( 99.98)
Epoch: [37][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6894e-02 (7.0722e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [37][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2899e-02 (7.0680e-02)	Acc@1  99.22 ( 98.52)	Acc@5 100.00 ( 99.99)
Epoch: [37][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4969e-02 (7.0791e-02)	Acc@1  98.75 ( 98.51)	Acc@5 100.00 ( 99.99)
## e[37] optimizer.zero_grad (sum) time: 0.6694159507751465
## e[37]       loss.backward (sum) time: 13.86331820487976
## e[37]      optimizer.step (sum) time: 14.69442629814148
## epoch[37] training(only) time: 53.96761918067932
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.1866e+00 (1.1866e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.4197e+00 (1.2571e+00)	Acc@1  70.00 ( 73.09)	Acc@5  91.00 ( 91.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 9.7292e-01 (1.1448e+00)	Acc@1  78.00 ( 73.67)	Acc@5  93.00 ( 92.52)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.4854e+00 (1.1731e+00)	Acc@1  68.00 ( 73.26)	Acc@5  92.00 ( 92.29)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.3419e+00 (1.1636e+00)	Acc@1  69.00 ( 73.15)	Acc@5  90.00 ( 92.51)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.1102e+00 (1.1530e+00)	Acc@1  75.00 ( 73.41)	Acc@5  93.00 ( 92.47)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0696e+00 (1.1384e+00)	Acc@1  75.00 ( 73.64)	Acc@5  97.00 ( 92.70)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2191e+00 (1.1480e+00)	Acc@1  74.00 ( 73.68)	Acc@5  95.00 ( 92.83)
Test: [ 80/100]	Time  0.051 ( 0.050)	Loss 1.2931e+00 (1.1514e+00)	Acc@1  74.00 ( 73.64)	Acc@5  92.00 ( 92.86)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4613e+00 (1.1345e+00)	Acc@1  68.00 ( 73.77)	Acc@5  91.00 ( 92.99)
 * Acc@1 74.010 Acc@5 93.060
### epoch[37] execution time: 58.9657564163208
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.300 ( 0.300)	Data  0.157 ( 0.157)	Loss 5.5523e-02 (5.5523e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.137 ( 0.154)	Data  0.001 ( 0.015)	Loss 7.7229e-02 (6.8645e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.135 ( 0.146)	Data  0.001 ( 0.008)	Loss 4.8895e-02 (6.9945e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.7555e-02 (6.4543e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 6.5378e-02 (6.6510e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 5.6696e-02 (6.6414e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 60/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.9538e-02 (6.7441e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.0621e-02 (6.5256e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.9379e-02 (6.7071e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0303e-01 (6.6620e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 ( 99.97)
Epoch: [38][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.8466e-02 (6.7512e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.98)
Epoch: [38][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2751e-02 (6.6994e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 ( 99.98)
Epoch: [38][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8522e-02 (6.7048e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.98)
Epoch: [38][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2205e-02 (6.6678e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.98)
Epoch: [38][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.3061e-02 (6.5916e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.98)
Epoch: [38][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.6260e-02 (6.5508e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 ( 99.98)
Epoch: [38][160/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.2716e-02 (6.5692e-02)	Acc@1  97.66 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [38][170/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2316e-02 (6.5296e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [38][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.5770e-02 (6.5224e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.98)
Epoch: [38][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0145e-02 (6.4774e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.98)
Epoch: [38][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2121e-02 (6.4802e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.98)
Epoch: [38][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3270e-02 (6.4650e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [38][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2619e-02 (6.4654e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [38][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5907e-02 (6.4223e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [38][240/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4466e-02 (6.4142e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [38][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9591e-02 (6.4133e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [38][260/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3080e-02 (6.4049e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [38][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3723e-02 (6.3773e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [38][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9409e-02 (6.3265e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.98)
Epoch: [38][290/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1965e-02 (6.3233e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 ( 99.98)
Epoch: [38][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0087e-02 (6.3096e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.98)
Epoch: [38][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3650e-02 (6.3138e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.98)
Epoch: [38][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9967e-02 (6.3010e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [38][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9083e-02 (6.2923e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [38][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5728e-02 (6.2714e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [38][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1847e-01 (6.3147e-02)	Acc@1  95.31 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [38][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3450e-02 (6.3208e-02)	Acc@1  97.66 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [38][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6805e-02 (6.3183e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [38][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5159e-02 (6.3287e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [38][390/391]	Time  0.105 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8280e-02 (6.3564e-02)	Acc@1  97.50 ( 98.81)	Acc@5 100.00 ( 99.99)
## e[38] optimizer.zero_grad (sum) time: 0.6680164337158203
## e[38]       loss.backward (sum) time: 13.85303521156311
## e[38]      optimizer.step (sum) time: 14.697995901107788
## epoch[38] training(only) time: 53.97349739074707
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1509e+00 (1.1509e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.4422e+00 (1.2666e+00)	Acc@1  70.00 ( 73.36)	Acc@5  91.00 ( 90.91)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0583e+00 (1.1556e+00)	Acc@1  74.00 ( 73.81)	Acc@5  93.00 ( 92.38)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.6007e+00 (1.1851e+00)	Acc@1  65.00 ( 73.23)	Acc@5  91.00 ( 92.35)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.3875e+00 (1.1811e+00)	Acc@1  70.00 ( 73.22)	Acc@5  91.00 ( 92.63)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.0409e+00 (1.1701e+00)	Acc@1  77.00 ( 73.41)	Acc@5  94.00 ( 92.49)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.0727e+00 (1.1554e+00)	Acc@1  73.00 ( 73.61)	Acc@5  98.00 ( 92.72)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.2186e+00 (1.1627e+00)	Acc@1  74.00 ( 73.69)	Acc@5  95.00 ( 92.80)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3052e+00 (1.1661e+00)	Acc@1  75.00 ( 73.64)	Acc@5  93.00 ( 92.85)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4823e+00 (1.1509e+00)	Acc@1  68.00 ( 73.77)	Acc@5  92.00 ( 93.10)
 * Acc@1 74.060 Acc@5 93.110
### epoch[38] execution time: 58.96819853782654
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.296 ( 0.296)	Data  0.146 ( 0.146)	Loss 5.1743e-02 (5.1743e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.014)	Loss 5.3134e-02 (6.1629e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 3.2656e-02 (6.0784e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [39][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.006)	Loss 4.8555e-02 (5.7189e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [39][ 40/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.0904e-01 (5.8750e-02)	Acc@1  96.88 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [39][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 7.0268e-02 (5.9203e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [39][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.6646e-02 (5.8548e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [39][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.3296e-02 (5.8714e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [39][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.7744e-02 (5.8777e-02)	Acc@1  96.88 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [39][ 90/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.2281e-02 (5.9427e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [39][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.0482e-02 (5.9269e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [39][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.5228e-02 (5.9208e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [39][120/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2001e-02 (5.8640e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [39][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.0083e-02 (5.8758e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [39][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.3831e-02 (5.8702e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [39][150/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4869e-02 (5.9009e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [39][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.0174e-02 (5.9618e-02)	Acc@1  96.88 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [39][170/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7511e-02 (5.8924e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [39][180/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4642e-02 (5.8895e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [39][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4190e-02 (5.8635e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [39][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7615e-02 (5.8840e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [39][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2750e-02 (5.8906e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [39][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2270e-02 (5.8961e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [39][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6932e-02 (5.9368e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [39][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9129e-02 (5.9209e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [39][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1619e-02 (5.9316e-02)	Acc@1  98.44 ( 98.87)	Acc@5  99.22 (100.00)
Epoch: [39][260/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9966e-02 (5.9301e-02)	Acc@1  99.22 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [39][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1019e-02 (5.9687e-02)	Acc@1 100.00 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [39][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1159e-02 (6.0116e-02)	Acc@1  96.88 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [39][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1227e-02 (6.0258e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [39][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5695e-02 (6.0547e-02)	Acc@1 100.00 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [39][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6708e-02 (6.0441e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [39][320/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.7098e-02 (6.0191e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [39][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0428e-02 (6.0470e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [39][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7143e-02 (6.0676e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [39][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1253e-02 (6.0623e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [39][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7212e-02 (6.0606e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [39][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5760e-02 (6.0401e-02)	Acc@1 100.00 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [39][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0249e-02 (6.0447e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [39][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4120e-02 (6.0680e-02)	Acc@1  98.75 ( 98.78)	Acc@5 100.00 (100.00)
## e[39] optimizer.zero_grad (sum) time: 0.6669905185699463
## e[39]       loss.backward (sum) time: 13.84885048866272
## e[39]      optimizer.step (sum) time: 14.700174570083618
## epoch[39] training(only) time: 53.954747438430786
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1501e+00 (1.1501e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.3820e+00 (1.2642e+00)	Acc@1  70.00 ( 73.45)	Acc@5  90.00 ( 91.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0099e+00 (1.1521e+00)	Acc@1  75.00 ( 74.10)	Acc@5  93.00 ( 92.38)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6085e+00 (1.1870e+00)	Acc@1  66.00 ( 73.23)	Acc@5  91.00 ( 92.39)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.4246e+00 (1.1874e+00)	Acc@1  69.00 ( 73.05)	Acc@5  92.00 ( 92.71)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.0854e+00 (1.1786e+00)	Acc@1  77.00 ( 73.27)	Acc@5  92.00 ( 92.51)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0810e+00 (1.1643e+00)	Acc@1  76.00 ( 73.36)	Acc@5  96.00 ( 92.79)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2081e+00 (1.1711e+00)	Acc@1  76.00 ( 73.41)	Acc@5  95.00 ( 92.92)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3085e+00 (1.1756e+00)	Acc@1  75.00 ( 73.38)	Acc@5  89.00 ( 92.79)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.4745e+00 (1.1581e+00)	Acc@1  71.00 ( 73.54)	Acc@5  92.00 ( 93.05)
 * Acc@1 73.820 Acc@5 93.090
### epoch[39] execution time: 58.93550443649292
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.308 ( 0.308)	Data  0.157 ( 0.157)	Loss 5.7243e-02 (5.7243e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.148 ( 0.154)	Data  0.001 ( 0.015)	Loss 4.7213e-02 (5.2979e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.139 ( 0.145)	Data  0.001 ( 0.008)	Loss 4.3365e-02 (4.8565e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [40][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 4.0960e-02 (4.9206e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [40][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 5.1171e-02 (5.3175e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [40][ 50/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.4989e-02 (5.2105e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [40][ 60/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.8547e-02 (5.2218e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [40][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.9839e-02 (5.2094e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [40][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.8174e-02 (5.2186e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [40][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.3762e-02 (5.0879e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [40][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.5460e-02 (5.1239e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [40][110/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.0509e-02 (5.1172e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [40][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7355e-02 (5.1491e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [40][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.1179e-02 (5.0950e-02)	Acc@1  96.88 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [40][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0782e-02 (5.0358e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [40][150/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7974e-02 (5.0277e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [40][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7139e-02 (5.0963e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [40][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9721e-02 (5.0926e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [40][180/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6108e-02 (5.1519e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [40][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5305e-02 (5.2039e-02)	Acc@1  96.09 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [40][200/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0238e-02 (5.2016e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [40][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9635e-02 (5.2361e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [40][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.0055e-02 (5.2840e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [40][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4104e-02 (5.2699e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [40][240/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6152e-02 (5.2494e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [40][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6824e-02 (5.2594e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [40][260/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.8687e-02 (5.2991e-02)	Acc@1  98.44 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [40][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9402e-02 (5.3528e-02)	Acc@1  96.88 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [40][280/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3645e-02 (5.3395e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [40][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0045e-02 (5.3213e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [40][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3477e-02 (5.3533e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [40][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7645e-02 (5.3600e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [40][320/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3204e-02 (5.3566e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [40][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1504e-02 (5.3762e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [40][340/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6994e-02 (5.4018e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [40][350/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.001)	Loss 5.6436e-02 (5.4115e-02)	Acc@1  96.88 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [40][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.6622e-02 (5.4404e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [40][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.7386e-02 (5.4667e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [40][380/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.3401e-02 (5.4465e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [40][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.001)	Loss 7.0237e-02 (5.4606e-02)	Acc@1  98.75 ( 98.98)	Acc@5 100.00 (100.00)
## e[40] optimizer.zero_grad (sum) time: 0.6668562889099121
## e[40]       loss.backward (sum) time: 13.854081630706787
## e[40]      optimizer.step (sum) time: 14.725326776504517
## epoch[40] training(only) time: 53.97495079040527
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.1050e+00 (1.1050e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.4169e+00 (1.2701e+00)	Acc@1  69.00 ( 73.55)	Acc@5  91.00 ( 91.00)
Test: [ 20/100]	Time  0.055 ( 0.056)	Loss 9.7702e-01 (1.1685e+00)	Acc@1  76.00 ( 73.81)	Acc@5  93.00 ( 92.33)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.6205e+00 (1.2039e+00)	Acc@1  65.00 ( 73.16)	Acc@5  90.00 ( 92.19)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.3666e+00 (1.2001e+00)	Acc@1  69.00 ( 72.95)	Acc@5  91.00 ( 92.49)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.0948e+00 (1.1902e+00)	Acc@1  77.00 ( 73.25)	Acc@5  92.00 ( 92.41)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.0851e+00 (1.1763e+00)	Acc@1  74.00 ( 73.46)	Acc@5  98.00 ( 92.70)
Test: [ 70/100]	Time  0.054 ( 0.050)	Loss 1.1978e+00 (1.1841e+00)	Acc@1  76.00 ( 73.44)	Acc@5  95.00 ( 92.85)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3187e+00 (1.1869e+00)	Acc@1  75.00 ( 73.44)	Acc@5  89.00 ( 92.80)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 1.5663e+00 (1.1708e+00)	Acc@1  71.00 ( 73.62)	Acc@5  91.00 ( 92.98)
 * Acc@1 73.910 Acc@5 93.010
### epoch[40] execution time: 58.9641854763031
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.307 ( 0.307)	Data  0.157 ( 0.157)	Loss 4.8316e-02 (4.8316e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.138 ( 0.155)	Data  0.001 ( 0.015)	Loss 8.4840e-02 (5.4108e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.138 ( 0.147)	Data  0.001 ( 0.008)	Loss 3.4868e-02 (4.5537e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 3.7853e-02 (4.2655e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [41][ 40/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.005)	Loss 7.1506e-02 (4.5187e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [41][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.4017e-02 (4.6354e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [41][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.1114e-02 (4.6492e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [41][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.4481e-02 (4.7356e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [41][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.3901e-02 (4.7226e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [41][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 5.0890e-02 (4.7226e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [41][100/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.0624e-02 (4.7793e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [41][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4993e-02 (4.7754e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [41][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3845e-02 (4.7554e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [41][130/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5343e-02 (4.7709e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [41][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5807e-02 (4.7687e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [41][150/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.3936e-02 (4.7867e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [41][160/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6346e-02 (4.8004e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [41][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.3981e-02 (4.8462e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [41][180/391]	Time  0.150 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.8088e-02 (4.8325e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [41][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9369e-02 (4.8825e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [41][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8869e-02 (4.9071e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [41][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9842e-02 (4.9205e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [41][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6570e-02 (4.9512e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [41][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5336e-02 (4.9588e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [41][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6889e-02 (4.9663e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [41][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7633e-02 (4.9366e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [41][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4248e-02 (4.9712e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [41][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0371e-02 (4.9641e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [41][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1733e-02 (4.9765e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [41][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4016e-02 (4.9772e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [41][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3919e-02 (4.9752e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [41][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0731e-02 (4.9816e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [41][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6659e-02 (4.9865e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [41][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9499e-02 (4.9701e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [41][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5245e-02 (4.9715e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [41][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8685e-02 (4.9642e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [41][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7545e-02 (4.9577e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [41][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6530e-02 (4.9603e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [41][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 8.8495e-02 (4.9731e-02)	Acc@1  96.88 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [41][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.2935e-02 (4.9818e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
## e[41] optimizer.zero_grad (sum) time: 0.6693747043609619
## e[41]       loss.backward (sum) time: 13.855571985244751
## e[41]      optimizer.step (sum) time: 14.701749801635742
## epoch[41] training(only) time: 54.02321791648865
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.1818e+00 (1.1818e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.3823e+00 (1.2711e+00)	Acc@1  73.00 ( 73.27)	Acc@5  90.00 ( 90.91)
Test: [ 20/100]	Time  0.054 ( 0.056)	Loss 1.0245e+00 (1.1791e+00)	Acc@1  76.00 ( 74.14)	Acc@5  93.00 ( 92.43)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6524e+00 (1.2128e+00)	Acc@1  63.00 ( 73.23)	Acc@5  90.00 ( 92.23)
Test: [ 40/100]	Time  0.045 ( 0.052)	Loss 1.4299e+00 (1.2129e+00)	Acc@1  70.00 ( 73.12)	Acc@5  91.00 ( 92.56)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.0974e+00 (1.1994e+00)	Acc@1  76.00 ( 73.27)	Acc@5  92.00 ( 92.51)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0880e+00 (1.1840e+00)	Acc@1  75.00 ( 73.46)	Acc@5  97.00 ( 92.79)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2362e+00 (1.1941e+00)	Acc@1  75.00 ( 73.44)	Acc@5  95.00 ( 92.83)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3191e+00 (1.1970e+00)	Acc@1  76.00 ( 73.36)	Acc@5  93.00 ( 92.85)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4585e+00 (1.1797e+00)	Acc@1  70.00 ( 73.52)	Acc@5  91.00 ( 93.01)
 * Acc@1 73.910 Acc@5 93.040
### epoch[41] execution time: 59.032689809799194
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.298 ( 0.298)	Data  0.151 ( 0.151)	Loss 4.1695e-02 (4.1695e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.150 ( 0.152)	Data  0.001 ( 0.015)	Loss 3.1057e-02 (3.4383e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.008)	Loss 6.6251e-02 (3.7693e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 7.1918e-02 (3.9907e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.147 ( 0.142)	Data  0.001 ( 0.005)	Loss 5.9791e-02 (4.3275e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 6.3942e-02 (4.3344e-02)	Acc@1  97.66 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [42][ 60/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.0027e-02 (4.2895e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [42][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 7.6649e-02 (4.3703e-02)	Acc@1  96.88 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [42][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.8117e-02 (4.3804e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [42][ 90/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.1482e-02 (4.4254e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.9012e-02 (4.5256e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [42][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1277e-02 (4.4597e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 ( 99.99)
Epoch: [42][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4451e-02 (4.4372e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [42][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8411e-02 (4.5470e-02)	Acc@1  97.66 ( 99.21)	Acc@5 100.00 ( 99.99)
Epoch: [42][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.8215e-02 (4.5403e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 ( 99.99)
Epoch: [42][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7781e-02 (4.5438e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [42][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.7756e-02 (4.5596e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.8264e-02 (4.5478e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [42][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9486e-02 (4.5551e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1337e-02 (4.5441e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][200/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2814e-02 (4.5480e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0003e-01 (4.5554e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][220/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1814e-02 (4.5842e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7293e-02 (4.5753e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [42][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0835e-02 (4.5639e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9629e-02 (4.5367e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [42][260/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6611e-02 (4.5227e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [42][270/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0432e-02 (4.5102e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [42][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4398e-02 (4.5548e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [42][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5436e-02 (4.5625e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 ( 99.99)
Epoch: [42][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1156e-02 (4.5851e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [42][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0217e-02 (4.5788e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 ( 99.99)
Epoch: [42][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9325e-02 (4.5966e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [42][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5752e-02 (4.5971e-02)	Acc@1  97.66 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [42][340/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4486e-02 (4.6179e-02)	Acc@1  96.88 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][350/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7206e-02 (4.6229e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [42][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5701e-02 (4.6196e-02)	Acc@1  97.66 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [42][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9227e-02 (4.6272e-02)	Acc@1  96.88 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][380/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6510e-02 (4.6242e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [42][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7253e-01 (4.6440e-02)	Acc@1  93.75 ( 99.20)	Acc@5 100.00 (100.00)
## e[42] optimizer.zero_grad (sum) time: 0.6661920547485352
## e[42]       loss.backward (sum) time: 13.821513652801514
## e[42]      optimizer.step (sum) time: 14.718477010726929
## epoch[42] training(only) time: 53.97230005264282
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.2053e+00 (1.2053e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.051 ( 0.063)	Loss 1.3602e+00 (1.2967e+00)	Acc@1  70.00 ( 73.00)	Acc@5  90.00 ( 90.91)
Test: [ 20/100]	Time  0.050 ( 0.056)	Loss 1.0424e+00 (1.1819e+00)	Acc@1  75.00 ( 74.05)	Acc@5  94.00 ( 92.38)
Test: [ 30/100]	Time  0.052 ( 0.054)	Loss 1.7037e+00 (1.2179e+00)	Acc@1  65.00 ( 73.10)	Acc@5  90.00 ( 92.16)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.4322e+00 (1.2199e+00)	Acc@1  71.00 ( 73.15)	Acc@5  93.00 ( 92.63)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1329e+00 (1.2039e+00)	Acc@1  78.00 ( 73.45)	Acc@5  93.00 ( 92.63)
Test: [ 60/100]	Time  0.050 ( 0.051)	Loss 1.0611e+00 (1.1919e+00)	Acc@1  74.00 ( 73.49)	Acc@5  97.00 ( 92.85)
Test: [ 70/100]	Time  0.057 ( 0.050)	Loss 1.2426e+00 (1.1998e+00)	Acc@1  76.00 ( 73.48)	Acc@5  94.00 ( 92.85)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3522e+00 (1.2047e+00)	Acc@1  76.00 ( 73.38)	Acc@5  90.00 ( 92.77)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.5170e+00 (1.1879e+00)	Acc@1  68.00 ( 73.48)	Acc@5  90.00 ( 92.90)
 * Acc@1 73.810 Acc@5 92.930
### epoch[42] execution time: 59.01204586029053
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.304 ( 0.304)	Data  0.158 ( 0.158)	Loss 6.0905e-02 (6.0905e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.136 ( 0.154)	Data  0.001 ( 0.015)	Loss 4.5691e-02 (4.3148e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.137 ( 0.146)	Data  0.001 ( 0.009)	Loss 2.5736e-02 (4.0367e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 6.5264e-02 (4.2348e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.6460e-02 (4.3011e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.9801e-02 (4.2540e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 5.0535e-02 (4.2591e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.6557e-02 (4.1907e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [43][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.6895e-02 (4.1696e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [43][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.1487e-02 (4.1040e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [43][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.3636e-02 (4.1671e-02)	Acc@1  97.66 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [43][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 5.5905e-02 (4.2015e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [43][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1241e-02 (4.1946e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [43][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6168e-02 (4.2546e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [43][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2818e-02 (4.2557e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [43][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3973e-02 (4.2683e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [43][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6893e-02 (4.2981e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [43][170/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7271e-02 (4.3221e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [43][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3737e-02 (4.2718e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][190/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4262e-02 (4.2398e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [43][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6057e-02 (4.2071e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [43][210/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.5704e-02 (4.2283e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [43][220/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7111e-02 (4.2282e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [43][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6401e-02 (4.2385e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [43][240/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.5346e-02 (4.2542e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [43][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3076e-02 (4.2421e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [43][260/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2237e-02 (4.2602e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [43][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4187e-02 (4.2885e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [43][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2184e-02 (4.3305e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3624e-02 (4.3405e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0092e-02 (4.3158e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [43][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9269e-02 (4.3128e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][320/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.6013e-02 (4.3285e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [43][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1917e-02 (4.3497e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0722e-02 (4.3419e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][350/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1996e-02 (4.3587e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][360/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3111e-02 (4.3665e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4315e-02 (4.3534e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [43][380/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2776e-02 (4.3585e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [43][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.9146e-02 (4.3564e-02)	Acc@1  98.75 ( 99.30)	Acc@5 100.00 (100.00)
## e[43] optimizer.zero_grad (sum) time: 0.6680023670196533
## e[43]       loss.backward (sum) time: 13.911902666091919
## e[43]      optimizer.step (sum) time: 14.708921909332275
## epoch[43] training(only) time: 54.03058958053589
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.1640e+00 (1.1640e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.3922e+00 (1.2771e+00)	Acc@1  72.00 ( 73.45)	Acc@5  91.00 ( 91.00)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.0347e+00 (1.1827e+00)	Acc@1  76.00 ( 73.95)	Acc@5  93.00 ( 92.29)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6855e+00 (1.2166e+00)	Acc@1  68.00 ( 73.26)	Acc@5  91.00 ( 92.19)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 1.4314e+00 (1.2149e+00)	Acc@1  68.00 ( 73.22)	Acc@5  90.00 ( 92.41)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1546e+00 (1.2067e+00)	Acc@1  77.00 ( 73.39)	Acc@5  92.00 ( 92.31)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0939e+00 (1.1932e+00)	Acc@1  74.00 ( 73.67)	Acc@5  97.00 ( 92.70)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.2213e+00 (1.2017e+00)	Acc@1  76.00 ( 73.65)	Acc@5  95.00 ( 92.82)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.3440e+00 (1.2049e+00)	Acc@1  77.00 ( 73.60)	Acc@5  89.00 ( 92.74)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5510e+00 (1.1887e+00)	Acc@1  68.00 ( 73.68)	Acc@5  91.00 ( 92.96)
 * Acc@1 73.960 Acc@5 92.990
### epoch[43] execution time: 59.04962491989136
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.303 ( 0.303)	Data  0.137 ( 0.137)	Loss 3.7109e-02 (3.7109e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.138 ( 0.152)	Data  0.001 ( 0.013)	Loss 3.5671e-02 (4.2833e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.008)	Loss 4.7628e-02 (4.2818e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.005)	Loss 4.7981e-02 (4.2425e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.148 ( 0.142)	Data  0.001 ( 0.004)	Loss 3.5721e-02 (4.1943e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 4.0785e-02 (4.1393e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.003)	Loss 3.6498e-02 (3.9919e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [44][ 70/391]	Time  0.153 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.1284e-02 (3.9832e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [44][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3202e-02 (3.9872e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [44][ 90/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.5721e-02 (4.0307e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [44][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0209e-02 (4.0541e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [44][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7519e-02 (4.0457e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [44][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1161e-02 (4.1178e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [44][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5111e-02 (4.0873e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [44][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5704e-02 (4.0783e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [44][150/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6914e-02 (4.0512e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [44][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.0383e-02 (4.0436e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8681e-02 (4.0149e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [44][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7943e-02 (3.9822e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [44][190/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0913e-02 (4.0242e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [44][200/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1534e-02 (4.0324e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [44][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5108e-02 (4.0460e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [44][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1118e-02 (4.0541e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [44][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5321e-02 (4.0736e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [44][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2466e-02 (4.0530e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [44][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8377e-02 (4.0196e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [44][260/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7138e-02 (4.0311e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [44][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2967e-02 (4.0483e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [44][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2921e-02 (4.0734e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7132e-02 (4.0748e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7670e-02 (4.0906e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3367e-02 (4.0976e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [44][320/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1615e-02 (4.1184e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1901e-02 (4.1262e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.7065e-02 (4.1497e-02)	Acc@1  96.88 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.2971e-02 (4.1313e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [44][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4622e-02 (4.1008e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [44][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8001e-02 (4.1055e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [44][380/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6533e-02 (4.1007e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [44][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8340e-02 (4.1058e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
## e[44] optimizer.zero_grad (sum) time: 0.6665794849395752
## e[44]       loss.backward (sum) time: 13.819280862808228
## e[44]      optimizer.step (sum) time: 14.734694004058838
## epoch[44] training(only) time: 53.99830508232117
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.1512e+00 (1.1512e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.063)	Loss 1.4114e+00 (1.3024e+00)	Acc@1  72.00 ( 73.36)	Acc@5  90.00 ( 91.00)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0194e+00 (1.1979e+00)	Acc@1  76.00 ( 73.95)	Acc@5  93.00 ( 92.43)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6659e+00 (1.2358e+00)	Acc@1  64.00 ( 73.06)	Acc@5  91.00 ( 92.23)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.3969e+00 (1.2339e+00)	Acc@1  69.00 ( 72.93)	Acc@5  91.00 ( 92.46)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1419e+00 (1.2198e+00)	Acc@1  78.00 ( 73.24)	Acc@5  94.00 ( 92.37)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.0917e+00 (1.2020e+00)	Acc@1  76.00 ( 73.48)	Acc@5  97.00 ( 92.69)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2565e+00 (1.2099e+00)	Acc@1  75.00 ( 73.55)	Acc@5  94.00 ( 92.82)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3698e+00 (1.2161e+00)	Acc@1  76.00 ( 73.42)	Acc@5  91.00 ( 92.73)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.4945e+00 (1.1981e+00)	Acc@1  72.00 ( 73.57)	Acc@5  92.00 ( 92.95)
 * Acc@1 73.880 Acc@5 92.990
### epoch[44] execution time: 59.00820565223694
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.312 ( 0.312)	Data  0.153 ( 0.153)	Loss 2.9149e-02 (2.9149e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.140 ( 0.154)	Data  0.001 ( 0.015)	Loss 1.9872e-02 (3.6187e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.008)	Loss 5.2704e-02 (3.6621e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.7254e-02 (3.7915e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.6345e-02 (3.8569e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.7649e-02 (3.9534e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.98)
Epoch: [45][ 60/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.8380e-02 (3.8974e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.4360e-02 (3.9021e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8628e-02 (3.8886e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [45][ 90/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.7153e-02 (3.8482e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [45][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.8714e-02 (3.8622e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [45][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3001e-02 (3.8380e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [45][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4325e-02 (3.7747e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [45][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0655e-02 (3.7340e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [45][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1909e-02 (3.7514e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [45][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.9125e-02 (3.7554e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [45][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7430e-02 (3.7154e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6078e-02 (3.7132e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6972e-02 (3.6998e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8108e-02 (3.6850e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [45][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9318e-02 (3.7187e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [45][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8902e-02 (3.7321e-02)	Acc@1  97.66 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7999e-02 (3.7345e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][230/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3593e-02 (3.7325e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [45][240/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1364e-02 (3.7491e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3836e-02 (3.7409e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][260/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2644e-02 (3.7357e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [45][270/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1712e-02 (3.7333e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [45][280/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4344e-02 (3.7344e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [45][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.0365e-02 (3.7452e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][300/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2085e-02 (3.7539e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.8118e-02 (3.7486e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7040e-02 (3.7554e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0058e-02 (3.7525e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2510e-02 (3.7450e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8557e-02 (3.7285e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1486e-02 (3.7345e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6594e-02 (3.7468e-02)	Acc@1  96.88 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8154e-02 (3.7557e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1511e-02 (3.7767e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
## e[45] optimizer.zero_grad (sum) time: 0.6681857109069824
## e[45]       loss.backward (sum) time: 13.886319875717163
## e[45]      optimizer.step (sum) time: 14.695400476455688
## epoch[45] training(only) time: 54.0905225276947
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.1232e+00 (1.1232e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.052 ( 0.061)	Loss 1.4332e+00 (1.2977e+00)	Acc@1  73.00 ( 73.73)	Acc@5  90.00 ( 91.00)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0753e+00 (1.1996e+00)	Acc@1  76.00 ( 74.38)	Acc@5  92.00 ( 92.29)
Test: [ 30/100]	Time  0.055 ( 0.053)	Loss 1.6721e+00 (1.2322e+00)	Acc@1  64.00 ( 73.71)	Acc@5  90.00 ( 92.10)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.4206e+00 (1.2301e+00)	Acc@1  69.00 ( 73.54)	Acc@5  91.00 ( 92.41)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.1737e+00 (1.2168e+00)	Acc@1  78.00 ( 73.75)	Acc@5  93.00 ( 92.43)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0519e+00 (1.1997e+00)	Acc@1  74.00 ( 73.87)	Acc@5  98.00 ( 92.79)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3427e+00 (1.2092e+00)	Acc@1  73.00 ( 73.87)	Acc@5  95.00 ( 92.87)
Test: [ 80/100]	Time  0.059 ( 0.049)	Loss 1.3674e+00 (1.2141e+00)	Acc@1  73.00 ( 73.77)	Acc@5  89.00 ( 92.84)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5099e+00 (1.1969e+00)	Acc@1  70.00 ( 73.81)	Acc@5  90.00 ( 93.03)
 * Acc@1 74.100 Acc@5 93.010
### epoch[45] execution time: 59.081424713134766
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.297 ( 0.297)	Data  0.149 ( 0.149)	Loss 2.8440e-02 (2.8440e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.137 ( 0.151)	Data  0.001 ( 0.014)	Loss 5.0005e-02 (3.6016e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 5.2725e-02 (3.6322e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.6166e-02 (3.5648e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.6797e-02 (3.6050e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.9642e-02 (3.4953e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.7271e-02 (3.5013e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.3353e-02 (3.5531e-02)	Acc@1  97.66 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.5246e-02 (3.6281e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.1236e-02 (3.6026e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [46][100/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.3478e-02 (3.5830e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [46][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5696e-02 (3.6209e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [46][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.7219e-02 (3.5974e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [46][130/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8464e-02 (3.6075e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [46][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2585e-02 (3.6259e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [46][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1307e-02 (3.5959e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][160/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4127e-02 (3.5915e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2372e-02 (3.5699e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0148e-02 (3.5906e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6664e-02 (3.5976e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1357e-02 (3.5838e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0223e-02 (3.6035e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1961e-02 (3.6307e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0193e-02 (3.6339e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5088e-02 (3.6139e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2727e-02 (3.6223e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9179e-02 (3.6475e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2901e-02 (3.6447e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3020e-02 (3.6443e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8913e-02 (3.6312e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7227e-02 (3.6525e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1627e-02 (3.6337e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4901e-02 (3.6460e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7435e-02 (3.6502e-02)	Acc@1  97.66 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6005e-02 (3.6315e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0519e-02 (3.6215e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6050e-03 (3.6139e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5515e-02 (3.6239e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4425e-02 (3.6171e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6238e-02 (3.6126e-02)	Acc@1  98.75 ( 99.45)	Acc@5 100.00 (100.00)
## e[46] optimizer.zero_grad (sum) time: 0.6692929267883301
## e[46]       loss.backward (sum) time: 13.820279359817505
## e[46]      optimizer.step (sum) time: 14.717002153396606
## epoch[46] training(only) time: 54.02516746520996
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.1438e+00 (1.1438e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.051 ( 0.062)	Loss 1.4646e+00 (1.3019e+00)	Acc@1  73.00 ( 74.00)	Acc@5  92.00 ( 90.73)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 1.0376e+00 (1.2029e+00)	Acc@1  78.00 ( 74.52)	Acc@5  92.00 ( 92.19)
Test: [ 30/100]	Time  0.046 ( 0.054)	Loss 1.6407e+00 (1.2441e+00)	Acc@1  67.00 ( 73.39)	Acc@5  91.00 ( 91.94)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.4422e+00 (1.2488e+00)	Acc@1  71.00 ( 73.20)	Acc@5  90.00 ( 92.22)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.2177e+00 (1.2341e+00)	Acc@1  79.00 ( 73.37)	Acc@5  92.00 ( 92.33)
Test: [ 60/100]	Time  0.051 ( 0.051)	Loss 1.1194e+00 (1.2191e+00)	Acc@1  74.00 ( 73.61)	Acc@5  98.00 ( 92.64)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 1.3522e+00 (1.2281e+00)	Acc@1  74.00 ( 73.59)	Acc@5  95.00 ( 92.80)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.3736e+00 (1.2318e+00)	Acc@1  75.00 ( 73.48)	Acc@5  91.00 ( 92.77)
Test: [ 90/100]	Time  0.050 ( 0.050)	Loss 1.4875e+00 (1.2120e+00)	Acc@1  68.00 ( 73.62)	Acc@5  91.00 ( 92.99)
 * Acc@1 73.870 Acc@5 93.000
### epoch[46] execution time: 59.046003580093384
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.302 ( 0.302)	Data  0.157 ( 0.157)	Loss 3.1134e-02 (3.1134e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.015)	Loss 4.6951e-02 (4.0182e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.009)	Loss 2.3134e-02 (3.7850e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 5.6983e-02 (3.6477e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 4.5297e-02 (3.4923e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.1366e-02 (3.3880e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.7316e-02 (3.5316e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4517e-02 (3.5087e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4123e-02 (3.4852e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7379e-02 (3.4565e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.3776e-02 (3.4586e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.4469e-02 (3.4500e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5901e-02 (3.4781e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6652e-02 (3.5491e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1927e-02 (3.5592e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3160e-02 (3.5440e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6330e-02 (3.5369e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8946e-02 (3.5212e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.1919e-02 (3.5238e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2261e-02 (3.5236e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.2061e-02 (3.5328e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9756e-02 (3.5133e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3839e-02 (3.5041e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3046e-02 (3.4864e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.0739e-02 (3.4630e-02)	Acc@1  97.66 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3012e-02 (3.4462e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9128e-02 (3.4792e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2837e-02 (3.4912e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0961e-02 (3.4777e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3969e-02 (3.5113e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2546e-02 (3.5234e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2528e-02 (3.5212e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7014e-02 (3.5188e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5796e-02 (3.5208e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.7212e-02 (3.5185e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0055e-02 (3.5008e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4964e-02 (3.4965e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6575e-02 (3.5262e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2562e-02 (3.5318e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7241e-02 (3.5389e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.669419527053833
## e[47]       loss.backward (sum) time: 13.888743162155151
## e[47]      optimizer.step (sum) time: 14.697821378707886
## epoch[47] training(only) time: 54.1233434677124
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 1.2001e+00 (1.2001e+00)	Acc@1  72.00 ( 72.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.4806e+00 (1.3153e+00)	Acc@1  72.00 ( 73.09)	Acc@5  89.00 ( 91.18)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0757e+00 (1.2121e+00)	Acc@1  76.00 ( 74.05)	Acc@5  94.00 ( 92.52)
Test: [ 30/100]	Time  0.057 ( 0.053)	Loss 1.6701e+00 (1.2525e+00)	Acc@1  66.00 ( 73.26)	Acc@5  91.00 ( 92.23)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.4070e+00 (1.2521e+00)	Acc@1  71.00 ( 73.07)	Acc@5  93.00 ( 92.56)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.1512e+00 (1.2397e+00)	Acc@1  76.00 ( 73.24)	Acc@5  92.00 ( 92.33)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0704e+00 (1.2250e+00)	Acc@1  74.00 ( 73.44)	Acc@5  97.00 ( 92.59)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.2812e+00 (1.2347e+00)	Acc@1  76.00 ( 73.45)	Acc@5  95.00 ( 92.65)
Test: [ 80/100]	Time  0.061 ( 0.050)	Loss 1.3222e+00 (1.2359e+00)	Acc@1  75.00 ( 73.44)	Acc@5  91.00 ( 92.65)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.6426e+00 (1.2181e+00)	Acc@1  70.00 ( 73.62)	Acc@5  90.00 ( 92.85)
 * Acc@1 73.870 Acc@5 92.880
### epoch[47] execution time: 59.13857889175415
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.302 ( 0.302)	Data  0.148 ( 0.148)	Loss 3.5628e-02 (3.5628e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.014)	Loss 2.8561e-02 (2.8534e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.8587e-02 (3.0184e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.8561e-02 (3.2187e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.005)	Loss 3.3337e-02 (3.0445e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.004)	Loss 3.5215e-02 (3.0398e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.8752e-02 (3.0905e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8599e-02 (3.1009e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.0874e-02 (3.0991e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.2799e-02 (3.0271e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.4202e-02 (3.0726e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0289e-02 (3.0684e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7914e-02 (3.0331e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4958e-02 (3.0892e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2457e-02 (3.1495e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5264e-02 (3.1371e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2171e-02 (3.1240e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3737e-02 (3.1349e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1586e-02 (3.1190e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5126e-02 (3.1113e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1201e-02 (3.1448e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0661e-02 (3.1460e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9672e-02 (3.1480e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.5919e-02 (3.1374e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9362e-02 (3.1507e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0164e-02 (3.1510e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3045e-02 (3.1364e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1366e-02 (3.1371e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0202e-02 (3.1350e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1355e-02 (3.1405e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9259e-02 (3.1363e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3986e-02 (3.1352e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6046e-02 (3.1583e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5799e-02 (3.1537e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6576e-02 (3.1823e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4831e-02 (3.1817e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1616e-02 (3.1915e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.9038e-02 (3.1886e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.6711e-02 (3.2019e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.4334e-02 (3.2049e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.6680619716644287
## e[48]       loss.backward (sum) time: 13.817015886306763
## e[48]      optimizer.step (sum) time: 14.730620384216309
## epoch[48] training(only) time: 54.061285972595215
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.2433e+00 (1.2433e+00)	Acc@1  68.00 ( 68.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.063)	Loss 1.4391e+00 (1.3172e+00)	Acc@1  71.00 ( 72.45)	Acc@5  89.00 ( 91.18)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.0551e+00 (1.2109e+00)	Acc@1  75.00 ( 73.81)	Acc@5  94.00 ( 92.48)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.7299e+00 (1.2507e+00)	Acc@1  63.00 ( 73.06)	Acc@5  90.00 ( 92.23)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.4178e+00 (1.2513e+00)	Acc@1  72.00 ( 73.15)	Acc@5  92.00 ( 92.51)
Test: [ 50/100]	Time  0.050 ( 0.051)	Loss 1.1802e+00 (1.2369e+00)	Acc@1  76.00 ( 73.35)	Acc@5  92.00 ( 92.41)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1032e+00 (1.2267e+00)	Acc@1  73.00 ( 73.48)	Acc@5  97.00 ( 92.69)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.3019e+00 (1.2354e+00)	Acc@1  77.00 ( 73.46)	Acc@5  95.00 ( 92.76)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3473e+00 (1.2387e+00)	Acc@1  75.00 ( 73.43)	Acc@5  93.00 ( 92.74)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5756e+00 (1.2208e+00)	Acc@1  69.00 ( 73.57)	Acc@5  92.00 ( 92.89)
 * Acc@1 73.880 Acc@5 92.890
### epoch[48] execution time: 59.045888900756836
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.295 ( 0.295)	Data  0.149 ( 0.149)	Loss 2.2192e-02 (2.2192e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.135 ( 0.152)	Data  0.001 ( 0.015)	Loss 2.5175e-02 (2.6061e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.146 ( 0.146)	Data  0.001 ( 0.008)	Loss 3.3788e-02 (2.7830e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 3.0492e-02 (2.8613e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.134 ( 0.142)	Data  0.001 ( 0.005)	Loss 4.3077e-02 (3.0687e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.1771e-02 (3.0492e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 4.7723e-02 (3.0263e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.1926e-02 (3.0270e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.6724e-02 (3.0420e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.0197e-02 (3.0343e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.7535e-02 (3.0636e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6939e-02 (3.0837e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4269e-02 (3.0589e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6737e-02 (3.0856e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4043e-02 (3.0756e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3901e-02 (3.0789e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5892e-02 (3.0338e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1053e-02 (3.0494e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8942e-02 (3.0922e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9008e-02 (3.0988e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.4396e-02 (3.1500e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1970e-02 (3.1368e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0992e-02 (3.1306e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6312e-02 (3.1072e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5854e-02 (3.1100e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2286e-02 (3.1060e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5491e-02 (3.1207e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1417e-02 (3.1318e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.9699e-02 (3.1445e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 ( 99.99)
Epoch: [49][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1154e-02 (3.1432e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [49][300/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1160e-02 (3.1520e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [49][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3841e-02 (3.1403e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [49][320/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8517e-02 (3.1403e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.6392e-02 (3.1531e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6010e-02 (3.1376e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7290e-02 (3.1431e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4965e-02 (3.1495e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0764e-02 (3.1394e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6153e-02 (3.1371e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.105 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4201e-02 (3.1329e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.6713602542877197
## e[49]       loss.backward (sum) time: 13.814014434814453
## e[49]      optimizer.step (sum) time: 14.732310056686401
## epoch[49] training(only) time: 54.11118984222412
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.2307e+00 (1.2307e+00)	Acc@1  71.00 ( 71.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 1.4897e+00 (1.3433e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 90.91)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.1040e+00 (1.2314e+00)	Acc@1  74.00 ( 73.86)	Acc@5  94.00 ( 92.24)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 1.6715e+00 (1.2648e+00)	Acc@1  64.00 ( 73.00)	Acc@5  91.00 ( 92.13)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.4706e+00 (1.2652e+00)	Acc@1  69.00 ( 73.00)	Acc@5  90.00 ( 92.41)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1273e+00 (1.2521e+00)	Acc@1  77.00 ( 73.27)	Acc@5  92.00 ( 92.27)
Test: [ 60/100]	Time  0.049 ( 0.050)	Loss 1.1627e+00 (1.2368e+00)	Acc@1  71.00 ( 73.43)	Acc@5  96.00 ( 92.48)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.2445e+00 (1.2451e+00)	Acc@1  74.00 ( 73.41)	Acc@5  96.00 ( 92.56)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4487e+00 (1.2508e+00)	Acc@1  74.00 ( 73.35)	Acc@5  90.00 ( 92.53)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5982e+00 (1.2326e+00)	Acc@1  72.00 ( 73.54)	Acc@5  91.00 ( 92.69)
 * Acc@1 73.830 Acc@5 92.700
### epoch[49] execution time: 59.11518979072571
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.298 ( 0.298)	Data  0.142 ( 0.142)	Loss 2.3855e-02 (2.3855e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.014)	Loss 1.8367e-02 (2.5072e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.7495e-02 (2.8172e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.7069e-02 (2.8042e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.3025e-02 (2.7812e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.4936e-02 (2.8444e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.8553e-02 (2.8409e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.2875e-02 (2.9139e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.3932e-02 (2.8749e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.2494e-02 (2.9219e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.6443e-02 (2.9102e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1424e-02 (2.8878e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.4930e-02 (2.8902e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4602e-02 (2.9105e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6117e-02 (2.9207e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8069e-02 (2.9254e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [50][160/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6097e-02 (2.9173e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5150e-02 (2.8967e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4733e-02 (2.8979e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1383e-02 (2.9068e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6912e-02 (2.9010e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9628e-02 (2.9423e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0717e-02 (2.9428e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9417e-02 (2.9299e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8821e-02 (2.9313e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5594e-02 (2.9170e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7683e-02 (2.9056e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1109e-02 (2.9252e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4865e-02 (2.9283e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.1143e-02 (2.9390e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4348e-02 (2.9495e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5471e-02 (2.9573e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1021e-02 (2.9671e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1487e-02 (2.9636e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5723e-02 (2.9562e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.3692e-02 (2.9516e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.8744e-02 (2.9700e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 4.0386e-02 (2.9717e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.6301e-02 (2.9638e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.6833e-02 (2.9780e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.6658973693847656
## e[50]       loss.backward (sum) time: 13.867298364639282
## e[50]      optimizer.step (sum) time: 14.72040605545044
## epoch[50] training(only) time: 54.06299114227295
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.1850e+00 (1.1850e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.051 ( 0.063)	Loss 1.4843e+00 (1.3331e+00)	Acc@1  71.00 ( 72.73)	Acc@5  90.00 ( 91.18)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.1099e+00 (1.2313e+00)	Acc@1  75.00 ( 73.38)	Acc@5  93.00 ( 92.48)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6180e+00 (1.2589e+00)	Acc@1  64.00 ( 72.97)	Acc@5  92.00 ( 92.23)
Test: [ 40/100]	Time  0.055 ( 0.052)	Loss 1.5437e+00 (1.2623e+00)	Acc@1  70.00 ( 72.90)	Acc@5  91.00 ( 92.51)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1725e+00 (1.2505e+00)	Acc@1  79.00 ( 73.22)	Acc@5  92.00 ( 92.43)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0755e+00 (1.2361e+00)	Acc@1  74.00 ( 73.30)	Acc@5  99.00 ( 92.69)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2660e+00 (1.2451e+00)	Acc@1  75.00 ( 73.28)	Acc@5  94.00 ( 92.77)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.4348e+00 (1.2508e+00)	Acc@1  75.00 ( 73.27)	Acc@5  92.00 ( 92.70)
Test: [ 90/100]	Time  0.055 ( 0.050)	Loss 1.5413e+00 (1.2321e+00)	Acc@1  71.00 ( 73.45)	Acc@5  91.00 ( 92.88)
 * Acc@1 73.750 Acc@5 92.890
### epoch[50] execution time: 59.078824043273926
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.311 ( 0.311)	Data  0.162 ( 0.162)	Loss 2.1591e-02 (2.1591e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.137 ( 0.154)	Data  0.001 ( 0.016)	Loss 1.4653e-02 (2.3875e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.148 ( 0.147)	Data  0.001 ( 0.009)	Loss 2.6421e-02 (2.3217e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.135 ( 0.144)	Data  0.001 ( 0.006)	Loss 2.9204e-02 (2.3796e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.3896e-02 (2.4511e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.142 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.6404e-02 (2.5601e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.7579e-02 (2.5339e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.141 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5117e-02 (2.5305e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.2547e-02 (2.5717e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.7858e-02 (2.5803e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.2031e-02 (2.5834e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5622e-02 (2.5802e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6220e-02 (2.5965e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1020e-02 (2.5976e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7577e-02 (2.6230e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0668e-02 (2.6404e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.6461e-02 (2.6807e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3862e-02 (2.6615e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1443e-02 (2.6609e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7682e-02 (2.6917e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.1428e-02 (2.6975e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4447e-02 (2.7284e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1487e-02 (2.7453e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.5087e-02 (2.7550e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7416e-02 (2.7749e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0000e-02 (2.7505e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0781e-02 (2.7784e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.3087e-02 (2.7982e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4120e-02 (2.8032e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6285e-02 (2.8067e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9989e-02 (2.8161e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8462e-02 (2.8370e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1220e-02 (2.8391e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9357e-02 (2.8353e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7936e-02 (2.8562e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9073e-02 (2.8517e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0398e-02 (2.8478e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4713e-02 (2.8488e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0974e-02 (2.8644e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.119 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8169e-02 (2.8673e-02)	Acc@1  98.75 ( 99.63)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.6692070960998535
## e[51]       loss.backward (sum) time: 13.819328546524048
## e[51]      optimizer.step (sum) time: 14.71134901046753
## epoch[51] training(only) time: 54.112247943878174
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.1693e+00 (1.1693e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.5297e+00 (1.3286e+00)	Acc@1  71.00 ( 72.73)	Acc@5  92.00 ( 90.64)
Test: [ 20/100]	Time  0.051 ( 0.055)	Loss 1.1320e+00 (1.2358e+00)	Acc@1  76.00 ( 73.62)	Acc@5  93.00 ( 92.19)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.6921e+00 (1.2706e+00)	Acc@1  64.00 ( 72.90)	Acc@5  91.00 ( 91.90)
Test: [ 40/100]	Time  0.050 ( 0.051)	Loss 1.4950e+00 (1.2740e+00)	Acc@1  68.00 ( 72.59)	Acc@5  93.00 ( 92.22)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1924e+00 (1.2619e+00)	Acc@1  76.00 ( 72.86)	Acc@5  92.00 ( 92.18)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.1092e+00 (1.2466e+00)	Acc@1  74.00 ( 73.00)	Acc@5  96.00 ( 92.51)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3263e+00 (1.2549e+00)	Acc@1  77.00 ( 73.10)	Acc@5  95.00 ( 92.61)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4095e+00 (1.2574e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.65)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.4937e+00 (1.2345e+00)	Acc@1  70.00 ( 73.36)	Acc@5  91.00 ( 92.81)
 * Acc@1 73.660 Acc@5 92.820
### epoch[51] execution time: 59.112972259521484
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.299 ( 0.299)	Data  0.151 ( 0.151)	Loss 2.3062e-02 (2.3062e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.138 ( 0.153)	Data  0.001 ( 0.015)	Loss 3.1083e-02 (2.2189e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.135 ( 0.146)	Data  0.001 ( 0.008)	Loss 1.9611e-02 (2.8224e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.3482e-02 (2.6760e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.9767e-02 (2.6318e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.8790e-02 (2.5708e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.5535e-02 (2.5192e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.133 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5051e-02 (2.4731e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.1508e-02 (2.4539e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [52][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6423e-02 (2.4264e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [52][100/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.0224e-02 (2.4916e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.3409e-02 (2.4993e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4956e-02 (2.5210e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.3523e-02 (2.5398e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0103e-02 (2.5279e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7910e-02 (2.5159e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [52][160/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6210e-02 (2.5225e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7352e-02 (2.5422e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5423e-02 (2.5366e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.150 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7703e-02 (2.5602e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3513e-02 (2.5602e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1285e-02 (2.5771e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0343e-02 (2.5656e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0957e-02 (2.6385e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7567e-02 (2.6306e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6576e-02 (2.6301e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3772e-02 (2.6416e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3683e-02 (2.6310e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4451e-02 (2.6351e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9304e-02 (2.6368e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7370e-02 (2.6376e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0972e-02 (2.6489e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8919e-02 (2.6503e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5434e-02 (2.6555e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0633e-02 (2.6548e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8535e-02 (2.6461e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6975e-02 (2.6450e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0013e-02 (2.6445e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4158e-02 (2.6509e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7758e-02 (2.6631e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.6706333160400391
## e[52]       loss.backward (sum) time: 13.860486030578613
## e[52]      optimizer.step (sum) time: 14.758542537689209
## epoch[52] training(only) time: 54.11706805229187
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.2230e+00 (1.2230e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.5708e+00 (1.3469e+00)	Acc@1  71.00 ( 72.91)	Acc@5  89.00 ( 90.73)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1593e+00 (1.2514e+00)	Acc@1  74.00 ( 73.48)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6524e+00 (1.2786e+00)	Acc@1  64.00 ( 72.90)	Acc@5  91.00 ( 92.03)
Test: [ 40/100]	Time  0.057 ( 0.052)	Loss 1.5393e+00 (1.2851e+00)	Acc@1  70.00 ( 72.80)	Acc@5  91.00 ( 92.29)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1871e+00 (1.2706e+00)	Acc@1  78.00 ( 73.22)	Acc@5  92.00 ( 92.29)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0531e+00 (1.2543e+00)	Acc@1  75.00 ( 73.46)	Acc@5  97.00 ( 92.64)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.2545e+00 (1.2648e+00)	Acc@1  75.00 ( 73.45)	Acc@5  95.00 ( 92.80)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.4019e+00 (1.2662e+00)	Acc@1  74.00 ( 73.43)	Acc@5  93.00 ( 92.70)
Test: [ 90/100]	Time  0.056 ( 0.049)	Loss 1.5570e+00 (1.2454e+00)	Acc@1  71.00 ( 73.64)	Acc@5  91.00 ( 92.90)
 * Acc@1 73.950 Acc@5 92.940
### epoch[52] execution time: 59.123751163482666
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.301 ( 0.301)	Data  0.154 ( 0.154)	Loss 4.5323e-02 (4.5323e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.138 ( 0.153)	Data  0.001 ( 0.015)	Loss 2.2612e-02 (2.3089e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.143 ( 0.146)	Data  0.001 ( 0.008)	Loss 3.4086e-02 (2.6609e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.4662e-02 (2.5014e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.0026e-02 (2.4283e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.149 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.8313e-02 (2.4443e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.9821e-02 (2.4526e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4353e-02 (2.5443e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.144 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1830e-02 (2.5091e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4952e-02 (2.5320e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3268e-02 (2.5100e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.4525e-02 (2.5232e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0503e-02 (2.4859e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1798e-02 (2.4896e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0248e-02 (2.4638e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4912e-02 (2.4895e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2024e-02 (2.4449e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3183e-02 (2.4313e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3665e-02 (2.4505e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0155e-02 (2.4800e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9321e-02 (2.4990e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0420e-02 (2.4706e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7657e-02 (2.4978e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3443e-02 (2.5069e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2177e-02 (2.5188e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3291e-02 (2.5172e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0109e-02 (2.5399e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6452e-02 (2.5372e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7183e-02 (2.5449e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2274e-02 (2.5710e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1973e-02 (2.5765e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4607e-02 (2.5797e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9468e-02 (2.5774e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7849e-02 (2.5762e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1784e-02 (2.5743e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6657e-02 (2.5923e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6286e-02 (2.5920e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8226e-02 (2.6038e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4836e-02 (2.5977e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.117 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.8125e-02 (2.6129e-02)	Acc@1  98.75 ( 99.63)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.6706547737121582
## e[53]       loss.backward (sum) time: 13.792380809783936
## e[53]      optimizer.step (sum) time: 14.719083547592163
## epoch[53] training(only) time: 54.09362173080444
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.2613e+00 (1.2613e+00)	Acc@1  71.00 ( 71.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.5336e+00 (1.3472e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 90.73)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.1730e+00 (1.2498e+00)	Acc@1  73.00 ( 73.33)	Acc@5  93.00 ( 92.29)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 1.6360e+00 (1.2817e+00)	Acc@1  65.00 ( 72.90)	Acc@5  91.00 ( 92.03)
Test: [ 40/100]	Time  0.050 ( 0.052)	Loss 1.5930e+00 (1.2871e+00)	Acc@1  70.00 ( 72.78)	Acc@5  90.00 ( 92.34)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.2067e+00 (1.2746e+00)	Acc@1  78.00 ( 73.14)	Acc@5  92.00 ( 92.24)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1446e+00 (1.2596e+00)	Acc@1  73.00 ( 73.30)	Acc@5  97.00 ( 92.48)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2341e+00 (1.2701e+00)	Acc@1  74.00 ( 73.28)	Acc@5  96.00 ( 92.56)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4288e+00 (1.2720e+00)	Acc@1  74.00 ( 73.22)	Acc@5  92.00 ( 92.56)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5635e+00 (1.2505e+00)	Acc@1  72.00 ( 73.45)	Acc@5  91.00 ( 92.77)
 * Acc@1 73.760 Acc@5 92.810
### epoch[53] execution time: 59.095499753952026
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.304 ( 0.304)	Data  0.157 ( 0.157)	Loss 3.2930e-02 (3.2930e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.134 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.5808e-02 (2.1887e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.140 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.6332e-02 (2.3500e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 3.3562e-02 (2.3542e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 5.3272e-02 (2.4936e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3398e-02 (2.4319e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.3183e-02 (2.4029e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4839e-02 (2.4937e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9269e-02 (2.4873e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.9085e-02 (2.5025e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.1292e-02 (2.5081e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8475e-02 (2.5120e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7428e-02 (2.5077e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6965e-02 (2.4987e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [54][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7154e-02 (2.4899e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [54][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4576e-02 (2.5021e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [54][160/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8250e-02 (2.4997e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5845e-02 (2.4784e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9156e-02 (2.4891e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7070e-02 (2.4844e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7870e-02 (2.4927e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5245e-02 (2.4727e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7482e-02 (2.4765e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8617e-02 (2.4823e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0473e-02 (2.4770e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5067e-02 (2.4719e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2186e-02 (2.4796e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0512e-02 (2.4815e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.152 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1932e-02 (2.4747e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3294e-02 (2.4807e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1251e-02 (2.4694e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1573e-02 (2.4635e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9916e-02 (2.4572e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6318e-02 (2.4634e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8038e-02 (2.4576e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2923e-02 (2.4553e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2631e-02 (2.4630e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4951e-02 (2.4610e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8976e-02 (2.4680e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.001)	Loss 6.7328e-02 (2.4828e-02)	Acc@1  97.50 ( 99.68)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.6744747161865234
## e[54]       loss.backward (sum) time: 13.833354711532593
## e[54]      optimizer.step (sum) time: 14.73842167854309
## epoch[54] training(only) time: 54.037219762802124
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.2056e+00 (1.2056e+00)	Acc@1  73.00 ( 73.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.5639e+00 (1.3451e+00)	Acc@1  70.00 ( 72.91)	Acc@5  89.00 ( 90.91)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.1178e+00 (1.2496e+00)	Acc@1  76.00 ( 73.67)	Acc@5  93.00 ( 92.29)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 1.6123e+00 (1.2759e+00)	Acc@1  65.00 ( 73.23)	Acc@5  91.00 ( 92.03)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5112e+00 (1.2807e+00)	Acc@1  70.00 ( 73.15)	Acc@5  90.00 ( 92.34)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2421e+00 (1.2681e+00)	Acc@1  77.00 ( 73.45)	Acc@5  93.00 ( 92.29)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.1450e+00 (1.2515e+00)	Acc@1  72.00 ( 73.69)	Acc@5  95.00 ( 92.59)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2826e+00 (1.2581e+00)	Acc@1  76.00 ( 73.61)	Acc@5  96.00 ( 92.72)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.4057e+00 (1.2624e+00)	Acc@1  74.00 ( 73.51)	Acc@5  90.00 ( 92.62)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.5376e+00 (1.2440e+00)	Acc@1  72.00 ( 73.64)	Acc@5  91.00 ( 92.85)
 * Acc@1 73.910 Acc@5 92.830
### epoch[54] execution time: 59.062230825424194
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.300 ( 0.300)	Data  0.150 ( 0.150)	Loss 3.4251e-02 (3.4251e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.137 ( 0.151)	Data  0.001 ( 0.015)	Loss 1.9372e-02 (2.3489e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.008)	Loss 1.0188e-02 (2.4103e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.3962e-02 (2.3280e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.2507e-02 (2.3037e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3322e-02 (2.2190e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.1569e-02 (2.1840e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8722e-02 (2.2524e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.149 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.9252e-02 (2.2737e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 4.8315e-02 (2.3269e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.4983e-02 (2.3036e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.154 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4744e-02 (2.3183e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6509e-02 (2.3026e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9486e-02 (2.3010e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7583e-02 (2.3249e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0513e-02 (2.3456e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0582e-02 (2.3449e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.7260e-02 (2.3890e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7528e-02 (2.3917e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.4616e-02 (2.3957e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8466e-02 (2.3874e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5030e-02 (2.3894e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8130e-02 (2.4062e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5048e-02 (2.4074e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1793e-02 (2.3918e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6172e-02 (2.3942e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3828e-02 (2.3999e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9235e-02 (2.4031e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0082e-03 (2.4060e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.4610e-02 (2.4028e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3740e-02 (2.3910e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6688e-02 (2.3946e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2931e-02 (2.4031e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6677e-02 (2.4024e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6663e-02 (2.3986e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0465e-02 (2.4126e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1920e-02 (2.4183e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7618e-02 (2.4256e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1327e-02 (2.4302e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.113 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5673e-02 (2.4252e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.6701467037200928
## e[55]       loss.backward (sum) time: 13.873795986175537
## e[55]      optimizer.step (sum) time: 14.700995922088623
## epoch[55] training(only) time: 54.132211685180664
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.2212e+00 (1.2212e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.5791e+00 (1.3442e+00)	Acc@1  70.00 ( 73.45)	Acc@5  89.00 ( 90.82)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.1619e+00 (1.2451e+00)	Acc@1  77.00 ( 74.24)	Acc@5  94.00 ( 92.29)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6026e+00 (1.2743e+00)	Acc@1  69.00 ( 73.52)	Acc@5  91.00 ( 91.97)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.5313e+00 (1.2776e+00)	Acc@1  69.00 ( 73.27)	Acc@5  92.00 ( 92.24)
Test: [ 50/100]	Time  0.054 ( 0.051)	Loss 1.1896e+00 (1.2627e+00)	Acc@1  75.00 ( 73.45)	Acc@5  93.00 ( 92.22)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.1756e+00 (1.2473e+00)	Acc@1  73.00 ( 73.59)	Acc@5  96.00 ( 92.48)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2447e+00 (1.2558e+00)	Acc@1  76.00 ( 73.63)	Acc@5  96.00 ( 92.65)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.4286e+00 (1.2589e+00)	Acc@1  76.00 ( 73.57)	Acc@5  92.00 ( 92.57)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.5571e+00 (1.2395e+00)	Acc@1  71.00 ( 73.69)	Acc@5  91.00 ( 92.79)
 * Acc@1 74.020 Acc@5 92.780
### epoch[55] execution time: 59.15386772155762
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.299 ( 0.299)	Data  0.149 ( 0.149)	Loss 1.6536e-02 (1.6536e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.134 ( 0.152)	Data  0.001 ( 0.014)	Loss 1.4192e-02 (2.3169e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.134 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.2217e-02 (2.1449e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 3.0536e-02 (2.1672e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.1923e-02 (2.2209e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 3.2432e-02 (2.2075e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.1488e-02 (2.2810e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4698e-02 (2.2853e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3060e-02 (2.2237e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.2754e-02 (2.2367e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2377e-02 (2.2359e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.002)	Loss 1.9016e-02 (2.2047e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4975e-02 (2.2522e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4865e-02 (2.2364e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 5.2055e-02 (2.2660e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8415e-02 (2.2907e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7877e-02 (2.3071e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5486e-02 (2.3129e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3890e-02 (2.2977e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6189e-02 (2.2992e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2999e-02 (2.2909e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9448e-02 (2.2853e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3187e-02 (2.2624e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5276e-02 (2.2658e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7078e-02 (2.2680e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8380e-02 (2.2646e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4386e-02 (2.2765e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1218e-02 (2.2810e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9509e-02 (2.3039e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.6928e-02 (2.3072e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6943e-02 (2.2988e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9839e-02 (2.3018e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.150 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8437e-02 (2.2950e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1805e-02 (2.2979e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.7634e-02 (2.3192e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2653e-02 (2.3276e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2486e-02 (2.3271e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7199e-02 (2.3164e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9777e-02 (2.3188e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.1756e-02 (2.3192e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.6719543933868408
## e[56]       loss.backward (sum) time: 13.872261047363281
## e[56]      optimizer.step (sum) time: 14.716971397399902
## epoch[56] training(only) time: 54.21782612800598
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.2225e+00 (1.2225e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.5388e+00 (1.3545e+00)	Acc@1  70.00 ( 72.27)	Acc@5  89.00 ( 90.82)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1372e+00 (1.2422e+00)	Acc@1  77.00 ( 73.67)	Acc@5  94.00 ( 92.24)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6675e+00 (1.2793e+00)	Acc@1  67.00 ( 73.10)	Acc@5  90.00 ( 92.03)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5172e+00 (1.2828e+00)	Acc@1  70.00 ( 73.07)	Acc@5  92.00 ( 92.37)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1982e+00 (1.2679e+00)	Acc@1  77.00 ( 73.20)	Acc@5  94.00 ( 92.39)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1038e+00 (1.2506e+00)	Acc@1  74.00 ( 73.41)	Acc@5  95.00 ( 92.62)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2646e+00 (1.2588e+00)	Acc@1  75.00 ( 73.38)	Acc@5  95.00 ( 92.69)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3861e+00 (1.2636e+00)	Acc@1  75.00 ( 73.38)	Acc@5  91.00 ( 92.60)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5346e+00 (1.2429e+00)	Acc@1  70.00 ( 73.57)	Acc@5  92.00 ( 92.80)
 * Acc@1 73.960 Acc@5 92.830
### epoch[56] execution time: 59.1925892829895
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.305 ( 0.305)	Data  0.139 ( 0.139)	Loss 2.3070e-02 (2.3070e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.134 ( 0.153)	Data  0.001 ( 0.014)	Loss 1.7215e-02 (1.9648e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.136 ( 0.146)	Data  0.001 ( 0.008)	Loss 2.0653e-02 (1.9010e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.3564e-02 (1.9741e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.146 ( 0.142)	Data  0.001 ( 0.004)	Loss 2.0704e-02 (2.0028e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.8029e-02 (2.0086e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.5027e-02 (2.0040e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.140 ( 0.141)	Data  0.001 ( 0.003)	Loss 2.0909e-02 (2.0434e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0358e-02 (2.0278e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.2204e-02 (2.0738e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3977e-02 (2.0925e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.002)	Loss 2.4710e-02 (2.1032e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8738e-02 (2.1787e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5086e-02 (2.1737e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6460e-02 (2.1794e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8715e-02 (2.1744e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2272e-02 (2.1975e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9024e-02 (2.1923e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.4001e-03 (2.1615e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1388e-02 (2.1652e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9948e-02 (2.1778e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7848e-02 (2.1620e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9562e-02 (2.1645e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5330e-02 (2.1840e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1834e-02 (2.1709e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.0825e-02 (2.1891e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5810e-02 (2.1906e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1187e-02 (2.1900e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0132e-02 (2.2081e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2825e-02 (2.2045e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8201e-02 (2.2136e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6559e-02 (2.2082e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7998e-02 (2.2061e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9069e-02 (2.2080e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8516e-02 (2.2147e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0963e-02 (2.2212e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3466e-02 (2.2298e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1408e-02 (2.2216e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7265e-02 (2.2292e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.105 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3058e-02 (2.2249e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.6750564575195312
## e[57]       loss.backward (sum) time: 13.86966586112976
## e[57]      optimizer.step (sum) time: 14.732963800430298
## epoch[57] training(only) time: 54.14552593231201
# Switched to evaluate mode...
Test: [  0/100]	Time  0.204 ( 0.204)	Loss 1.2068e+00 (1.2068e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.063)	Loss 1.4982e+00 (1.3402e+00)	Acc@1  69.00 ( 73.09)	Acc@5  91.00 ( 91.36)
Test: [ 20/100]	Time  0.049 ( 0.056)	Loss 1.1345e+00 (1.2447e+00)	Acc@1  77.00 ( 74.00)	Acc@5  94.00 ( 92.57)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6536e+00 (1.2837e+00)	Acc@1  70.00 ( 73.45)	Acc@5  90.00 ( 92.06)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5600e+00 (1.2903e+00)	Acc@1  70.00 ( 73.27)	Acc@5  90.00 ( 92.22)
Test: [ 50/100]	Time  0.056 ( 0.051)	Loss 1.2258e+00 (1.2762e+00)	Acc@1  77.00 ( 73.51)	Acc@5  94.00 ( 92.18)
Test: [ 60/100]	Time  0.047 ( 0.051)	Loss 1.0764e+00 (1.2612e+00)	Acc@1  76.00 ( 73.75)	Acc@5  96.00 ( 92.49)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.3178e+00 (1.2698e+00)	Acc@1  75.00 ( 73.75)	Acc@5  95.00 ( 92.62)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.4174e+00 (1.2760e+00)	Acc@1  76.00 ( 73.75)	Acc@5  91.00 ( 92.53)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.5268e+00 (1.2559e+00)	Acc@1  70.00 ( 73.81)	Acc@5  91.00 ( 92.76)
 * Acc@1 74.180 Acc@5 92.770
### epoch[57] execution time: 59.20002770423889
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.311 ( 0.311)	Data  0.156 ( 0.156)	Loss 2.3493e-02 (2.3493e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.138 ( 0.154)	Data  0.001 ( 0.015)	Loss 1.8067e-02 (1.8486e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.138 ( 0.147)	Data  0.001 ( 0.009)	Loss 2.1067e-02 (1.7130e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.4406e-02 (1.8676e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.138 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.7165e-02 (1.9616e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3034e-02 (2.0165e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.141 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.9835e-02 (2.0095e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7961e-02 (2.0198e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.2966e-02 (1.9859e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7382e-02 (2.0116e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7805e-02 (2.0215e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6129e-02 (2.0315e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9960e-02 (2.0509e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0413e-02 (2.0641e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0117e-02 (2.0473e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9085e-02 (2.0866e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7786e-02 (2.0674e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8184e-02 (2.0552e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3300e-02 (2.0480e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3618e-02 (2.0588e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3226e-02 (2.0624e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7229e-02 (2.0637e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2769e-02 (2.0932e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6709e-02 (2.0826e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6937e-02 (2.0960e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9954e-02 (2.1074e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7640e-02 (2.1215e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1862e-02 (2.1290e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5580e-02 (2.1241e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3767e-02 (2.1403e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2676e-02 (2.1416e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3836e-02 (2.1440e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9080e-02 (2.1598e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1347e-02 (2.1649e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.156 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1916e-02 (2.1592e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6866e-02 (2.1752e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6431e-02 (2.1734e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5956e-02 (2.1740e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8835e-02 (2.1784e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3114e-02 (2.1804e-02)	Acc@1  98.75 ( 99.73)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.6744110584259033
## e[58]       loss.backward (sum) time: 13.814769506454468
## e[58]      optimizer.step (sum) time: 14.771358251571655
## epoch[58] training(only) time: 54.106712102890015
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.1564e+00 (1.1564e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.5378e+00 (1.3529e+00)	Acc@1  72.00 ( 73.55)	Acc@5  89.00 ( 91.00)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 1.1288e+00 (1.2593e+00)	Acc@1  78.00 ( 74.38)	Acc@5  94.00 ( 92.14)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 1.7346e+00 (1.2923e+00)	Acc@1  69.00 ( 73.45)	Acc@5  90.00 ( 91.90)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5831e+00 (1.2974e+00)	Acc@1  68.00 ( 73.17)	Acc@5  92.00 ( 92.17)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2583e+00 (1.2841e+00)	Acc@1  77.00 ( 73.31)	Acc@5  94.00 ( 92.16)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0941e+00 (1.2665e+00)	Acc@1  75.00 ( 73.54)	Acc@5  95.00 ( 92.44)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2705e+00 (1.2770e+00)	Acc@1  74.00 ( 73.37)	Acc@5  95.00 ( 92.56)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.4599e+00 (1.2827e+00)	Acc@1  74.00 ( 73.36)	Acc@5  91.00 ( 92.46)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5846e+00 (1.2606e+00)	Acc@1  70.00 ( 73.55)	Acc@5  91.00 ( 92.70)
 * Acc@1 73.870 Acc@5 92.710
### epoch[58] execution time: 59.1152138710022
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.310 ( 0.310)	Data  0.158 ( 0.158)	Loss 2.6286e-02 (2.6286e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.141 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.7172e-02 (2.4434e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.139 ( 0.145)	Data  0.001 ( 0.009)	Loss 5.2567e-02 (2.2915e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.0996e-02 (2.1158e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.3460e-02 (2.0733e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.5577e-02 (2.0972e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.7746e-02 (2.1018e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2214e-02 (2.0699e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2077e-02 (2.0414e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4931e-02 (2.0744e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5292e-02 (2.0532e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3784e-02 (2.0614e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2158e-02 (2.0351e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0553e-02 (2.0563e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3173e-02 (2.0293e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0556e-02 (2.0271e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0675e-02 (1.9974e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 4.1648e-02 (1.9984e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1781e-02 (1.9977e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0832e-02 (1.9995e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3099e-02 (2.0061e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2264e-02 (2.0075e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3519e-02 (2.0016e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9157e-02 (2.0226e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1514e-02 (2.0301e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8908e-02 (2.0349e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9498e-02 (2.0339e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5885e-02 (2.0448e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0824e-02 (2.0438e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4032e-02 (2.0399e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7469e-02 (2.0533e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2675e-02 (2.0467e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7272e-02 (2.0366e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9074e-02 (2.0337e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5879e-02 (2.0466e-02)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6307e-02 (2.0407e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7548e-02 (2.0382e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5464e-02 (2.0447e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0732e-02 (2.0473e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0157e-02 (2.0500e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.6669580936431885
## e[59]       loss.backward (sum) time: 13.801186323165894
## e[59]      optimizer.step (sum) time: 14.723283529281616
## epoch[59] training(only) time: 54.077043771743774
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1305e+00 (1.1305e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.049 ( 0.062)	Loss 1.5758e+00 (1.3450e+00)	Acc@1  73.00 ( 73.55)	Acc@5  90.00 ( 91.36)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.1075e+00 (1.2482e+00)	Acc@1  79.00 ( 74.76)	Acc@5  93.00 ( 92.48)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.7015e+00 (1.2817e+00)	Acc@1  68.00 ( 73.90)	Acc@5  91.00 ( 92.19)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.6054e+00 (1.2914e+00)	Acc@1  70.00 ( 73.61)	Acc@5  92.00 ( 92.44)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2401e+00 (1.2762e+00)	Acc@1  78.00 ( 73.67)	Acc@5  94.00 ( 92.39)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0799e+00 (1.2611e+00)	Acc@1  74.00 ( 73.85)	Acc@5  97.00 ( 92.67)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.2663e+00 (1.2750e+00)	Acc@1  77.00 ( 73.85)	Acc@5  94.00 ( 92.72)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4107e+00 (1.2808e+00)	Acc@1  74.00 ( 73.72)	Acc@5  91.00 ( 92.65)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 1.6745e+00 (1.2612e+00)	Acc@1  71.00 ( 73.82)	Acc@5  91.00 ( 92.86)
 * Acc@1 74.120 Acc@5 92.870
### epoch[59] execution time: 59.10197925567627
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.305 ( 0.305)	Data  0.156 ( 0.156)	Loss 1.8549e-02 (1.8549e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.015)	Loss 1.4477e-02 (1.6700e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.139 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.4258e-02 (1.7008e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.143 ( 0.142)	Data  0.001 ( 0.006)	Loss 1.0820e-02 (1.8489e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.5078e-02 (1.8740e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.6126e-02 (1.9840e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.147 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.8068e-02 (1.9637e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.6892e-02 (1.9564e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.0347e-02 (1.9743e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.2469e-02 (1.9918e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1438e-02 (2.0579e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8642e-03 (2.0268e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.9814e-02 (2.0252e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5277e-02 (1.9968e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4640e-02 (2.0100e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5977e-02 (1.9972e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9598e-02 (2.0022e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1909e-02 (1.9941e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.6948e-03 (1.9892e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.8775e-03 (1.9902e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6193e-02 (1.9824e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1776e-02 (1.9847e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2241e-02 (1.9744e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3037e-02 (1.9593e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5813e-02 (1.9589e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.0546e-03 (1.9573e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0804e-02 (1.9505e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9772e-02 (1.9421e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1413e-02 (1.9552e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2517e-02 (1.9575e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0310e-02 (1.9553e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3625e-02 (1.9437e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2764e-02 (1.9323e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2423e-02 (1.9317e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.152 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6894e-02 (1.9257e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0676e-02 (1.9335e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2090e-02 (1.9289e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7416e-02 (1.9284e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6056e-02 (1.9338e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.105 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9728e-02 (1.9252e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.6675260066986084
## e[60]       loss.backward (sum) time: 13.8651864528656
## e[60]      optimizer.step (sum) time: 14.710230827331543
## epoch[60] training(only) time: 54.15943264961243
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.0955e+00 (1.0955e+00)	Acc@1  76.00 ( 76.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.059 ( 0.061)	Loss 1.5748e+00 (1.3454e+00)	Acc@1  70.00 ( 73.36)	Acc@5  90.00 ( 91.55)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1037e+00 (1.2491e+00)	Acc@1  79.00 ( 74.48)	Acc@5  94.00 ( 92.71)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6730e+00 (1.2792e+00)	Acc@1  70.00 ( 73.71)	Acc@5  91.00 ( 92.29)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5623e+00 (1.2877e+00)	Acc@1  70.00 ( 73.61)	Acc@5  92.00 ( 92.49)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.2593e+00 (1.2735e+00)	Acc@1  79.00 ( 73.75)	Acc@5  93.00 ( 92.47)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0644e+00 (1.2599e+00)	Acc@1  73.00 ( 73.89)	Acc@5  96.00 ( 92.80)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2628e+00 (1.2716e+00)	Acc@1  77.00 ( 73.83)	Acc@5  96.00 ( 92.85)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4151e+00 (1.2777e+00)	Acc@1  75.00 ( 73.70)	Acc@5  91.00 ( 92.74)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5807e+00 (1.2573e+00)	Acc@1  73.00 ( 73.87)	Acc@5  91.00 ( 92.93)
 * Acc@1 74.180 Acc@5 92.980
### epoch[60] execution time: 59.175719261169434
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.292 ( 0.292)	Data  0.141 ( 0.141)	Loss 1.0542e-02 (1.0542e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.134 ( 0.152)	Data  0.001 ( 0.014)	Loss 1.2138e-02 (1.8499e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.140 ( 0.146)	Data  0.001 ( 0.008)	Loss 2.5640e-02 (1.8719e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.4545e-02 (1.9037e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.004)	Loss 2.0304e-02 (1.9272e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.4455e-02 (1.9322e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 6.3577e-02 (1.9997e-02)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5596e-02 (1.9763e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7788e-02 (1.9737e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.4131e-02 (1.9399e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8246e-02 (1.9228e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2956e-02 (1.8781e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8198e-02 (1.8697e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 6.2290e-03 (1.8231e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2899e-02 (1.8175e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2816e-02 (1.8006e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6108e-02 (1.8034e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.2639e-02 (1.8026e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.8896e-02 (1.8053e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9644e-02 (1.7877e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.150 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0389e-02 (1.7803e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7333e-02 (1.7916e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1738e-02 (1.8054e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.152 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5607e-02 (1.8036e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.2698e-03 (1.8014e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2260e-02 (1.8051e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3207e-02 (1.7963e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3576e-02 (1.8056e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2279e-02 (1.8067e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3612e-02 (1.7963e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0643e-02 (1.7872e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2139e-02 (1.7918e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1051e-02 (1.8023e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4858e-02 (1.7994e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1963e-02 (1.8016e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6829e-02 (1.8148e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.1822e-02 (1.8113e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.7565e-02 (1.8163e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.2155e-02 (1.8182e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.0392e-02 (1.8260e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.6708436012268066
## e[61]       loss.backward (sum) time: 13.823045015335083
## e[61]      optimizer.step (sum) time: 14.699696779251099
## epoch[61] training(only) time: 54.08551573753357
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 1.1326e+00 (1.1326e+00)	Acc@1  74.00 ( 74.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.047 ( 0.064)	Loss 1.5596e+00 (1.3421e+00)	Acc@1  71.00 ( 73.00)	Acc@5  90.00 ( 91.27)
Test: [ 20/100]	Time  0.048 ( 0.057)	Loss 1.1018e+00 (1.2432e+00)	Acc@1  77.00 ( 74.00)	Acc@5  94.00 ( 92.62)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6625e+00 (1.2783e+00)	Acc@1  69.00 ( 73.32)	Acc@5  91.00 ( 92.19)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5578e+00 (1.2869e+00)	Acc@1  70.00 ( 73.07)	Acc@5  90.00 ( 92.39)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.2282e+00 (1.2711e+00)	Acc@1  79.00 ( 73.43)	Acc@5  92.00 ( 92.24)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.1032e+00 (1.2559e+00)	Acc@1  74.00 ( 73.61)	Acc@5  95.00 ( 92.48)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2127e+00 (1.2678e+00)	Acc@1  76.00 ( 73.63)	Acc@5  96.00 ( 92.63)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.3982e+00 (1.2743e+00)	Acc@1  75.00 ( 73.54)	Acc@5  90.00 ( 92.52)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.6246e+00 (1.2535e+00)	Acc@1  69.00 ( 73.69)	Acc@5  91.00 ( 92.69)
 * Acc@1 73.980 Acc@5 92.730
### epoch[61] execution time: 59.109676361083984
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.312 ( 0.312)	Data  0.155 ( 0.155)	Loss 2.7255e-02 (2.7255e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.140 ( 0.155)	Data  0.001 ( 0.015)	Loss 1.4268e-02 (1.7680e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.141 ( 0.147)	Data  0.001 ( 0.008)	Loss 1.9710e-02 (1.7390e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.136 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.8373e-02 (1.6372e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.5672e-02 (1.7086e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.138 ( 0.142)	Data  0.002 ( 0.004)	Loss 4.0784e-02 (1.8004e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.150 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.9414e-02 (1.7564e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.003)	Loss 1.3672e-02 (1.8299e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.3256e-02 (1.8293e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.148 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8467e-02 (1.8099e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.0051e-02 (1.7993e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0981e-02 (1.7982e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2839e-02 (1.7738e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4848e-02 (1.7756e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1342e-02 (1.7705e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8337e-02 (1.7795e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6384e-02 (1.7966e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1123e-02 (1.7880e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3794e-02 (1.8161e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5405e-02 (1.8138e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7450e-02 (1.8048e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5321e-02 (1.8107e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1537e-02 (1.7957e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6648e-02 (1.7913e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7476e-02 (1.7852e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9800e-02 (1.7706e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5669e-02 (1.7621e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8740e-02 (1.7817e-02)	Acc@1  98.44 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8784e-03 (1.7728e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0477e-02 (1.7672e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2311e-02 (1.7910e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3190e-02 (1.7856e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9458e-02 (1.7800e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4498e-02 (1.7723e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0486e-02 (1.7691e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6725e-02 (1.7656e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9140e-02 (1.7638e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8432e-02 (1.7643e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7937e-02 (1.7732e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7881e-02 (1.7793e-02)	Acc@1  98.75 ( 99.85)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.6657779216766357
## e[62]       loss.backward (sum) time: 13.834816932678223
## e[62]      optimizer.step (sum) time: 14.700079202651978
## epoch[62] training(only) time: 54.1014609336853
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.1408e+00 (1.1408e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.060 ( 0.062)	Loss 1.5871e+00 (1.3425e+00)	Acc@1  69.00 ( 72.36)	Acc@5  90.00 ( 91.18)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1299e+00 (1.2437e+00)	Acc@1  77.00 ( 73.67)	Acc@5  93.00 ( 92.48)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6696e+00 (1.2781e+00)	Acc@1  68.00 ( 73.03)	Acc@5  92.00 ( 92.13)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.5569e+00 (1.2839e+00)	Acc@1  69.00 ( 72.93)	Acc@5  91.00 ( 92.39)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2216e+00 (1.2685e+00)	Acc@1  79.00 ( 73.35)	Acc@5  93.00 ( 92.33)
Test: [ 60/100]	Time  0.056 ( 0.050)	Loss 1.0926e+00 (1.2541e+00)	Acc@1  74.00 ( 73.62)	Acc@5  96.00 ( 92.61)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.2671e+00 (1.2650e+00)	Acc@1  76.00 ( 73.54)	Acc@5  96.00 ( 92.72)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4055e+00 (1.2701e+00)	Acc@1  75.00 ( 73.46)	Acc@5  91.00 ( 92.63)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5598e+00 (1.2494e+00)	Acc@1  71.00 ( 73.62)	Acc@5  91.00 ( 92.82)
 * Acc@1 73.920 Acc@5 92.860
### epoch[62] execution time: 59.12227725982666
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.312 ( 0.312)	Data  0.166 ( 0.166)	Loss 1.1657e-02 (1.1657e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.136 ( 0.154)	Data  0.001 ( 0.016)	Loss 2.9369e-02 (1.8706e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.139 ( 0.146)	Data  0.001 ( 0.009)	Loss 2.0744e-02 (1.8205e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.140 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.5128e-02 (1.8516e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 3.1476e-02 (1.9109e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 8.5723e-03 (1.8420e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3428e-02 (1.8556e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4645e-02 (1.8134e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.6805e-02 (1.8096e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5660e-02 (1.8387e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8368e-02 (1.8587e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5264e-02 (1.8456e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.1533e-03 (1.7973e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1777e-02 (1.7774e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9224e-02 (1.7787e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1957e-02 (1.7484e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7844e-02 (1.7515e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4421e-02 (1.7511e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7512e-02 (1.7545e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3789e-02 (1.7546e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8857e-02 (1.7651e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0772e-02 (1.7493e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.3929e-03 (1.7495e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.150 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.8193e-02 (1.7719e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5669e-02 (1.7657e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7048e-02 (1.7602e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5319e-02 (1.7616e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0268e-02 (1.7679e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4419e-02 (1.7809e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0988e-02 (1.7882e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3274e-02 (1.7838e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4303e-02 (1.7850e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2121e-02 (1.7814e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0150e-03 (1.7753e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1408e-02 (1.7701e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4581e-03 (1.7717e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6431e-02 (1.7711e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8181e-02 (1.7706e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3775e-02 (1.7581e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.113 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9602e-02 (1.7606e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.6677005290985107
## e[63]       loss.backward (sum) time: 13.872516393661499
## e[63]      optimizer.step (sum) time: 14.688049554824829
## epoch[63] training(only) time: 54.13756036758423
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.1414e+00 (1.1414e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.5825e+00 (1.3494e+00)	Acc@1  71.00 ( 73.45)	Acc@5  90.00 ( 91.55)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.1265e+00 (1.2444e+00)	Acc@1  78.00 ( 74.38)	Acc@5  94.00 ( 92.67)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6836e+00 (1.2801e+00)	Acc@1  67.00 ( 73.61)	Acc@5  93.00 ( 92.42)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.5719e+00 (1.2889e+00)	Acc@1  70.00 ( 73.37)	Acc@5  92.00 ( 92.68)
Test: [ 50/100]	Time  0.051 ( 0.051)	Loss 1.2709e+00 (1.2746e+00)	Acc@1  78.00 ( 73.61)	Acc@5  93.00 ( 92.53)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0664e+00 (1.2596e+00)	Acc@1  74.00 ( 73.72)	Acc@5  98.00 ( 92.84)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.2349e+00 (1.2704e+00)	Acc@1  75.00 ( 73.62)	Acc@5  96.00 ( 92.96)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.3973e+00 (1.2763e+00)	Acc@1  75.00 ( 73.54)	Acc@5  91.00 ( 92.81)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.6023e+00 (1.2564e+00)	Acc@1  69.00 ( 73.70)	Acc@5  91.00 ( 92.99)
 * Acc@1 74.070 Acc@5 92.980
### epoch[63] execution time: 59.142632722854614
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.310 ( 0.310)	Data  0.161 ( 0.161)	Loss 2.8389e-02 (2.8389e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.134 ( 0.153)	Data  0.001 ( 0.016)	Loss 1.1611e-02 (1.7291e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.139 ( 0.147)	Data  0.001 ( 0.009)	Loss 8.4026e-03 (1.6405e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.7996e-02 (1.6771e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 9.5764e-03 (1.6872e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3497e-02 (1.6257e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.0486e-02 (1.6530e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2123e-02 (1.6568e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.4911e-02 (1.7603e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.149 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0952e-02 (1.7304e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5461e-02 (1.7284e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.7886e-03 (1.7507e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.0452e-02 (1.7669e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5883e-02 (1.7886e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7918e-02 (1.8177e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5661e-02 (1.7919e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3976e-02 (1.7898e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1484e-02 (1.7920e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4074e-02 (1.8001e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3894e-02 (1.8072e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6972e-02 (1.8283e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1668e-02 (1.8185e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2772e-02 (1.8102e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2001e-02 (1.8071e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6135e-02 (1.8081e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7281e-02 (1.8113e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4882e-02 (1.8199e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1356e-02 (1.8223e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8625e-02 (1.8134e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9825e-02 (1.8040e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5930e-02 (1.8014e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5388e-02 (1.7897e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7898e-02 (1.7993e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7028e-03 (1.7991e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2753e-02 (1.7910e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3085e-02 (1.7890e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4536e-02 (1.7826e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0126e-02 (1.7753e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1496e-02 (1.7722e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8044e-02 (1.7636e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.6700050830841064
## e[64]       loss.backward (sum) time: 13.893209457397461
## e[64]      optimizer.step (sum) time: 14.702826738357544
## epoch[64] training(only) time: 54.08530855178833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1178e+00 (1.1178e+00)	Acc@1  74.00 ( 74.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.6369e+00 (1.3535e+00)	Acc@1  71.00 ( 73.36)	Acc@5  89.00 ( 91.27)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 1.1367e+00 (1.2527e+00)	Acc@1  77.00 ( 74.19)	Acc@5  94.00 ( 92.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.6308e+00 (1.2853e+00)	Acc@1  70.00 ( 73.58)	Acc@5  91.00 ( 92.29)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5589e+00 (1.2911e+00)	Acc@1  70.00 ( 73.44)	Acc@5  91.00 ( 92.54)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2420e+00 (1.2751e+00)	Acc@1  80.00 ( 73.76)	Acc@5  94.00 ( 92.37)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1057e+00 (1.2588e+00)	Acc@1  74.00 ( 73.85)	Acc@5  96.00 ( 92.69)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2214e+00 (1.2687e+00)	Acc@1  76.00 ( 73.79)	Acc@5  95.00 ( 92.76)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4208e+00 (1.2749e+00)	Acc@1  74.00 ( 73.68)	Acc@5  90.00 ( 92.65)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5957e+00 (1.2539e+00)	Acc@1  72.00 ( 73.87)	Acc@5  91.00 ( 92.85)
 * Acc@1 74.170 Acc@5 92.880
### epoch[64] execution time: 59.08293008804321
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.301 ( 0.301)	Data  0.154 ( 0.154)	Loss 2.8362e-02 (2.8362e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.136 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.1274e-02 (1.7260e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.008)	Loss 1.7996e-02 (1.8226e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.006)	Loss 9.8250e-03 (1.7505e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.6276e-02 (1.7183e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.142 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.6863e-02 (1.8115e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 9.4981e-03 (1.7735e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0779e-02 (1.8523e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6185e-02 (1.8209e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.8334e-02 (1.8166e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.9858e-02 (1.8234e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.2629e-02 (1.8305e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8514e-02 (1.8442e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3150e-02 (1.8294e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.3381e-03 (1.7951e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5896e-02 (1.7790e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5156e-02 (1.7812e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3664e-02 (1.7746e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1365e-02 (1.7673e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5688e-02 (1.7511e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5940e-02 (1.7507e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0399e-02 (1.7472e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5921e-02 (1.7532e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3974e-02 (1.7573e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5820e-02 (1.7519e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.5687e-03 (1.7612e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4318e-02 (1.7493e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6724e-02 (1.7405e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7003e-02 (1.7554e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.154 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3716e-02 (1.7522e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1922e-02 (1.7644e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4998e-02 (1.7539e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4545e-02 (1.7569e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4107e-02 (1.7492e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.8876e-03 (1.7404e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2606e-02 (1.7388e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3458e-03 (1.7379e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0897e-02 (1.7377e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8567e-02 (1.7267e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5044e-02 (1.7316e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.6683447360992432
## e[65]       loss.backward (sum) time: 13.830380916595459
## e[65]      optimizer.step (sum) time: 14.71329951286316
## epoch[65] training(only) time: 54.0122709274292
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.1450e+00 (1.1450e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.051 ( 0.063)	Loss 1.5714e+00 (1.3369e+00)	Acc@1  69.00 ( 73.09)	Acc@5  90.00 ( 91.36)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.0903e+00 (1.2430e+00)	Acc@1  78.00 ( 74.00)	Acc@5  94.00 ( 92.62)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6562e+00 (1.2781e+00)	Acc@1  70.00 ( 73.55)	Acc@5  91.00 ( 92.19)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.5738e+00 (1.2870e+00)	Acc@1  70.00 ( 73.22)	Acc@5  91.00 ( 92.44)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.2500e+00 (1.2716e+00)	Acc@1  79.00 ( 73.49)	Acc@5  94.00 ( 92.45)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0719e+00 (1.2573e+00)	Acc@1  73.00 ( 73.66)	Acc@5  96.00 ( 92.79)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2574e+00 (1.2689e+00)	Acc@1  74.00 ( 73.55)	Acc@5  95.00 ( 92.83)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 1.4000e+00 (1.2749e+00)	Acc@1  74.00 ( 73.49)	Acc@5  91.00 ( 92.70)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.5634e+00 (1.2555e+00)	Acc@1  72.00 ( 73.71)	Acc@5  91.00 ( 92.87)
 * Acc@1 74.100 Acc@5 92.910
### epoch[65] execution time: 59.033591508865356
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.303 ( 0.303)	Data  0.158 ( 0.158)	Loss 1.3307e-02 (1.3307e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.3216e-02 (1.6897e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.009)	Loss 1.7575e-02 (1.6280e-02)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.5374e-02 (1.5441e-02)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.4451e-02 (1.5866e-02)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2666e-02 (1.6022e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.3231e-02 (1.6163e-02)	Acc@1 100.00 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5062e-02 (1.6119e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.9971e-03 (1.5879e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6453e-02 (1.5948e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3946e-02 (1.6445e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1868e-02 (1.6930e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.156 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8383e-03 (1.6947e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5243e-03 (1.6919e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7320e-02 (1.6712e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6102e-02 (1.6559e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.9409e-02 (1.6691e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.7316e-02 (1.6786e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8321e-02 (1.6955e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0780e-02 (1.6990e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4052e-02 (1.7014e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5237e-02 (1.7239e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0458e-02 (1.7183e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9852e-02 (1.7510e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9995e-02 (1.7578e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6908e-02 (1.7426e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1062e-02 (1.7488e-02)	Acc@1  97.66 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3193e-02 (1.7592e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1836e-02 (1.7493e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5116e-02 (1.7559e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1332e-02 (1.7523e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7295e-02 (1.7447e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6121e-02 (1.7412e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9951e-02 (1.7486e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3455e-02 (1.7550e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5406e-02 (1.7699e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4682e-02 (1.7629e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4376e-02 (1.7709e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0829e-02 (1.7713e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.119 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3720e-02 (1.7677e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.6665265560150146
## e[66]       loss.backward (sum) time: 13.908933877944946
## e[66]      optimizer.step (sum) time: 14.685553789138794
## epoch[66] training(only) time: 54.126736879348755
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.1876e+00 (1.1876e+00)	Acc@1  71.00 ( 71.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.5767e+00 (1.3468e+00)	Acc@1  70.00 ( 72.55)	Acc@5  90.00 ( 91.27)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1100e+00 (1.2392e+00)	Acc@1  78.00 ( 73.86)	Acc@5  94.00 ( 92.67)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6803e+00 (1.2763e+00)	Acc@1  67.00 ( 73.13)	Acc@5  91.00 ( 92.26)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5852e+00 (1.2829e+00)	Acc@1  69.00 ( 73.05)	Acc@5  91.00 ( 92.59)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2005e+00 (1.2678e+00)	Acc@1  78.00 ( 73.45)	Acc@5  94.00 ( 92.51)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0848e+00 (1.2540e+00)	Acc@1  74.00 ( 73.54)	Acc@5  98.00 ( 92.84)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2513e+00 (1.2638e+00)	Acc@1  75.00 ( 73.48)	Acc@5  96.00 ( 92.86)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.3949e+00 (1.2697e+00)	Acc@1  75.00 ( 73.44)	Acc@5  91.00 ( 92.70)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 1.5654e+00 (1.2502e+00)	Acc@1  72.00 ( 73.67)	Acc@5  92.00 ( 92.92)
 * Acc@1 73.980 Acc@5 92.950
### epoch[66] execution time: 59.12666368484497
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.294 ( 0.294)	Data  0.149 ( 0.149)	Loss 1.2724e-02 (1.2724e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.144 ( 0.152)	Data  0.001 ( 0.014)	Loss 1.0536e-02 (1.7618e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.144 ( 0.146)	Data  0.001 ( 0.008)	Loss 2.4680e-02 (1.9531e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 9.9808e-03 (1.8150e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.1524e-02 (1.8127e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 8.4680e-03 (1.7510e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5760e-02 (1.7625e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0723e-02 (1.7234e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 3.3270e-02 (1.7432e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4264e-02 (1.7640e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8542e-02 (1.7384e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.9814e-02 (1.7338e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4088e-02 (1.7368e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3493e-02 (1.7449e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1382e-02 (1.7414e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5478e-02 (1.7379e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8281e-02 (1.7231e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4443e-02 (1.7093e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7431e-02 (1.6866e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5245e-02 (1.6907e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1609e-02 (1.6971e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0050e-02 (1.6926e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0911e-02 (1.6803e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.9785e-03 (1.6880e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7930e-02 (1.6924e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3541e-02 (1.6845e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1465e-02 (1.6950e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1829e-03 (1.7026e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5788e-02 (1.7031e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2769e-02 (1.7076e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0868e-02 (1.7045e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4538e-02 (1.7028e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4233e-02 (1.7067e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0494e-02 (1.7051e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3471e-02 (1.7116e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.2341e-02 (1.7020e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.3947e-02 (1.7015e-02)	Acc@1  98.44 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.3154e-02 (1.7107e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.1145e-02 (1.7202e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.3292e-02 (1.7202e-02)	Acc@1  98.75 ( 99.84)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.6690990924835205
## e[67]       loss.backward (sum) time: 13.88447618484497
## e[67]      optimizer.step (sum) time: 14.648154258728027
## epoch[67] training(only) time: 54.01520776748657
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1617e+00 (1.1617e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.6336e+00 (1.3433e+00)	Acc@1  68.00 ( 73.18)	Acc@5  88.00 ( 91.55)
Test: [ 20/100]	Time  0.056 ( 0.055)	Loss 1.1023e+00 (1.2413e+00)	Acc@1  77.00 ( 73.95)	Acc@5  94.00 ( 92.71)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.6357e+00 (1.2740e+00)	Acc@1  70.00 ( 73.29)	Acc@5  91.00 ( 92.39)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5609e+00 (1.2819e+00)	Acc@1  70.00 ( 73.15)	Acc@5  93.00 ( 92.68)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2141e+00 (1.2672e+00)	Acc@1  79.00 ( 73.47)	Acc@5  94.00 ( 92.61)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0699e+00 (1.2522e+00)	Acc@1  75.00 ( 73.69)	Acc@5  98.00 ( 92.89)
Test: [ 70/100]	Time  0.054 ( 0.050)	Loss 1.2229e+00 (1.2627e+00)	Acc@1  76.00 ( 73.66)	Acc@5  96.00 ( 93.00)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4290e+00 (1.2689e+00)	Acc@1  74.00 ( 73.60)	Acc@5  89.00 ( 92.78)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 1.5614e+00 (1.2494e+00)	Acc@1  71.00 ( 73.75)	Acc@5  91.00 ( 92.92)
 * Acc@1 74.090 Acc@5 92.910
### epoch[67] execution time: 59.00791811943054
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.303 ( 0.303)	Data  0.154 ( 0.154)	Loss 1.5374e-02 (1.5374e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.138 ( 0.153)	Data  0.001 ( 0.015)	Loss 3.1491e-02 (1.7652e-02)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.139 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.1866e-02 (1.8084e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.5391e-02 (1.7789e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.5915e-02 (1.7266e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3651e-02 (1.6828e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.4373e-02 (1.6474e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7149e-02 (1.7024e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4645e-02 (1.7173e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1503e-02 (1.7004e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2166e-02 (1.6824e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5888e-02 (1.6948e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3977e-02 (1.6785e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1560e-02 (1.6539e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1637e-02 (1.6485e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.9180e-02 (1.6715e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5266e-02 (1.6659e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2930e-02 (1.6707e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2329e-02 (1.6650e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.4238e-02 (1.6761e-02)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4421e-02 (1.6869e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.156 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8958e-02 (1.7024e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4556e-02 (1.7001e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6007e-02 (1.7002e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2335e-02 (1.7155e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1765e-03 (1.7177e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4319e-02 (1.7137e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9737e-03 (1.7026e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0017e-02 (1.6996e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3860e-02 (1.6847e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.6031e-03 (1.6915e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0152e-02 (1.6994e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4410e-02 (1.7002e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5948e-02 (1.6998e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5658e-02 (1.6974e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7291e-02 (1.6954e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7898e-02 (1.6984e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.2108e-03 (1.6979e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4141e-02 (1.6902e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.104 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.5767e-02 (1.6876e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.6677227020263672
## e[68]       loss.backward (sum) time: 13.84162425994873
## e[68]      optimizer.step (sum) time: 14.728576898574829
## epoch[68] training(only) time: 54.00162863731384
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.1561e+00 (1.1561e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.5597e+00 (1.3460e+00)	Acc@1  69.00 ( 73.45)	Acc@5  90.00 ( 91.45)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1247e+00 (1.2448e+00)	Acc@1  78.00 ( 74.29)	Acc@5  94.00 ( 92.76)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6485e+00 (1.2782e+00)	Acc@1  67.00 ( 73.58)	Acc@5  92.00 ( 92.26)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5747e+00 (1.2869e+00)	Acc@1  70.00 ( 73.20)	Acc@5  91.00 ( 92.54)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.2343e+00 (1.2703e+00)	Acc@1  78.00 ( 73.41)	Acc@5  93.00 ( 92.41)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.1011e+00 (1.2566e+00)	Acc@1  74.00 ( 73.57)	Acc@5  97.00 ( 92.70)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2577e+00 (1.2664e+00)	Acc@1  75.00 ( 73.54)	Acc@5  96.00 ( 92.79)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.4228e+00 (1.2733e+00)	Acc@1  75.00 ( 73.58)	Acc@5  91.00 ( 92.69)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5658e+00 (1.2528e+00)	Acc@1  72.00 ( 73.78)	Acc@5  91.00 ( 92.88)
 * Acc@1 74.120 Acc@5 92.910
### epoch[68] execution time: 59.01030611991882
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.304 ( 0.304)	Data  0.159 ( 0.159)	Loss 5.4058e-02 (5.4058e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.151 ( 0.155)	Data  0.001 ( 0.015)	Loss 1.6069e-02 (2.0490e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.137 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.7742e-02 (1.8319e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.135 ( 0.144)	Data  0.001 ( 0.006)	Loss 1.3723e-02 (1.8256e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.144 ( 0.142)	Data  0.001 ( 0.005)	Loss 2.0928e-02 (1.8189e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.143 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2475e-02 (1.8336e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.3779e-02 (1.8703e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.8895e-03 (1.8732e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.4149e-02 (1.8623e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7612e-02 (1.8039e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.8244e-02 (1.8154e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.3475e-03 (1.7857e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7271e-02 (1.7879e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6195e-02 (1.8066e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6180e-02 (1.8063e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5278e-02 (1.8053e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0356e-02 (1.8040e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8209e-02 (1.8190e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3713e-02 (1.8140e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2469e-03 (1.8044e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.3659e-03 (1.7828e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2901e-02 (1.7872e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5422e-02 (1.7843e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9162e-02 (1.7789e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4504e-02 (1.7782e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0447e-02 (1.7600e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7617e-02 (1.7534e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5486e-02 (1.7560e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4459e-02 (1.7600e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2167e-02 (1.7418e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1328e-02 (1.7309e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4543e-02 (1.7235e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3425e-02 (1.7178e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7321e-02 (1.7208e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4953e-02 (1.7187e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.155 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9645e-03 (1.7227e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7411e-03 (1.7170e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5294e-02 (1.7269e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8428e-02 (1.7221e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.110 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9110e-02 (1.7273e-02)	Acc@1  98.75 ( 99.81)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.6763138771057129
## e[69]       loss.backward (sum) time: 13.915882349014282
## e[69]      optimizer.step (sum) time: 14.641380310058594
## epoch[69] training(only) time: 53.97954750061035
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 1.1338e+00 (1.1338e+00)	Acc@1  73.00 ( 73.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.5788e+00 (1.3443e+00)	Acc@1  71.00 ( 73.55)	Acc@5  89.00 ( 91.27)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1306e+00 (1.2469e+00)	Acc@1  78.00 ( 74.67)	Acc@5  94.00 ( 92.52)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.6430e+00 (1.2803e+00)	Acc@1  69.00 ( 73.74)	Acc@5  92.00 ( 92.26)
Test: [ 40/100]	Time  0.049 ( 0.052)	Loss 1.5814e+00 (1.2894e+00)	Acc@1  70.00 ( 73.41)	Acc@5  91.00 ( 92.46)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2381e+00 (1.2745e+00)	Acc@1  79.00 ( 73.69)	Acc@5  94.00 ( 92.43)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1169e+00 (1.2604e+00)	Acc@1  74.00 ( 73.79)	Acc@5  98.00 ( 92.80)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2832e+00 (1.2723e+00)	Acc@1  73.00 ( 73.63)	Acc@5  93.00 ( 92.83)
Test: [ 80/100]	Time  0.050 ( 0.050)	Loss 1.3973e+00 (1.2783e+00)	Acc@1  75.00 ( 73.54)	Acc@5  91.00 ( 92.77)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.5812e+00 (1.2575e+00)	Acc@1  72.00 ( 73.79)	Acc@5  91.00 ( 92.97)
 * Acc@1 74.130 Acc@5 92.970
### epoch[69] execution time: 58.98561143875122
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.300 ( 0.300)	Data  0.155 ( 0.155)	Loss 1.2862e-02 (1.2862e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.139 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.8044e-02 (1.5872e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.134 ( 0.146)	Data  0.001 ( 0.008)	Loss 3.6247e-02 (1.6656e-02)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.0916e-02 (1.6810e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.1115e-02 (1.6251e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.0132e-02 (1.6342e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 2.0085e-02 (1.6681e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9145e-02 (1.6201e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7928e-02 (1.6003e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7890e-02 (1.5692e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.8691e-02 (1.5898e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.5306e-02 (1.6154e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3259e-02 (1.6069e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5188e-02 (1.6173e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.8102e-03 (1.6148e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3438e-02 (1.6407e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1069e-02 (1.6472e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0298e-02 (1.6449e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9461e-02 (1.6572e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2245e-03 (1.6554e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8166e-02 (1.6464e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9292e-03 (1.6490e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6315e-02 (1.6656e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3680e-02 (1.6532e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4440e-02 (1.6480e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2009e-02 (1.6506e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2619e-02 (1.6484e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9978e-02 (1.6586e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2725e-02 (1.6606e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2829e-02 (1.6620e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0264e-02 (1.6651e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2618e-02 (1.6619e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6568e-03 (1.6541e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3853e-02 (1.6540e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7209e-02 (1.6445e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6497e-03 (1.6422e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9351e-02 (1.6441e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7868e-02 (1.6356e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2313e-02 (1.6292e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9142e-02 (1.6313e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.6688950061798096
## e[70]       loss.backward (sum) time: 13.865796566009521
## e[70]      optimizer.step (sum) time: 14.69609260559082
## epoch[70] training(only) time: 53.9778528213501
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.1564e+00 (1.1564e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.5722e+00 (1.3470e+00)	Acc@1  71.00 ( 73.00)	Acc@5  89.00 ( 91.45)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1191e+00 (1.2424e+00)	Acc@1  77.00 ( 74.05)	Acc@5  94.00 ( 92.67)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 1.6762e+00 (1.2751e+00)	Acc@1  67.00 ( 73.48)	Acc@5  92.00 ( 92.29)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5700e+00 (1.2809e+00)	Acc@1  70.00 ( 73.32)	Acc@5  91.00 ( 92.56)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2026e+00 (1.2650e+00)	Acc@1  79.00 ( 73.59)	Acc@5  94.00 ( 92.47)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1063e+00 (1.2512e+00)	Acc@1  72.00 ( 73.62)	Acc@5  98.00 ( 92.77)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2406e+00 (1.2611e+00)	Acc@1  73.00 ( 73.55)	Acc@5  95.00 ( 92.83)
Test: [ 80/100]	Time  0.051 ( 0.049)	Loss 1.4185e+00 (1.2677e+00)	Acc@1  75.00 ( 73.57)	Acc@5  90.00 ( 92.70)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.5798e+00 (1.2484e+00)	Acc@1  72.00 ( 73.74)	Acc@5  92.00 ( 92.97)
 * Acc@1 74.000 Acc@5 92.980
### epoch[70] execution time: 58.97538733482361
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.308 ( 0.308)	Data  0.157 ( 0.157)	Loss 1.9636e-02 (1.9636e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.135 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.2944e-02 (1.4254e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.144 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.4772e-02 (1.4647e-02)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.2102e-02 (1.5007e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.133 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.1292e-02 (1.5404e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.8666e-02 (1.6054e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 9.7439e-03 (1.6564e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2425e-02 (1.6451e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3613e-02 (1.6176e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.8366e-03 (1.6059e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2159e-02 (1.5854e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0264e-02 (1.6066e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0284e-02 (1.6350e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6384e-02 (1.6519e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1696e-02 (1.6513e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4235e-02 (1.6686e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3234e-02 (1.6596e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1409e-02 (1.6674e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5257e-02 (1.6779e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6900e-02 (1.6695e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2398e-02 (1.6646e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6232e-02 (1.6602e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2182e-02 (1.6605e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6272e-02 (1.6672e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4325e-02 (1.6621e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1256e-02 (1.6714e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9635e-02 (1.6603e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1282e-02 (1.6552e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1503e-02 (1.6508e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2366e-02 (1.6591e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8148e-02 (1.6586e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3576e-02 (1.6535e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1786e-02 (1.6668e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9486e-02 (1.6634e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2077e-02 (1.6674e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0965e-02 (1.6699e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0169e-02 (1.6668e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3629e-02 (1.6718e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.149 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0929e-02 (1.6635e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9466e-02 (1.6615e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.6685822010040283
## e[71]       loss.backward (sum) time: 13.88612174987793
## e[71]      optimizer.step (sum) time: 14.716408491134644
## epoch[71] training(only) time: 54.108314037323
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.1598e+00 (1.1598e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.051 ( 0.063)	Loss 1.5375e+00 (1.3380e+00)	Acc@1  69.00 ( 73.09)	Acc@5  90.00 ( 91.36)
Test: [ 20/100]	Time  0.050 ( 0.056)	Loss 1.0960e+00 (1.2432e+00)	Acc@1  77.00 ( 73.81)	Acc@5  94.00 ( 92.67)
Test: [ 30/100]	Time  0.046 ( 0.054)	Loss 1.6421e+00 (1.2754e+00)	Acc@1  69.00 ( 73.35)	Acc@5  92.00 ( 92.26)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.5566e+00 (1.2818e+00)	Acc@1  70.00 ( 73.12)	Acc@5  91.00 ( 92.54)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2000e+00 (1.2670e+00)	Acc@1  79.00 ( 73.43)	Acc@5  94.00 ( 92.55)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.0790e+00 (1.2547e+00)	Acc@1  75.00 ( 73.59)	Acc@5  97.00 ( 92.84)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2522e+00 (1.2649e+00)	Acc@1  77.00 ( 73.54)	Acc@5  94.00 ( 92.90)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4242e+00 (1.2714e+00)	Acc@1  75.00 ( 73.51)	Acc@5  91.00 ( 92.78)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.5549e+00 (1.2514e+00)	Acc@1  72.00 ( 73.70)	Acc@5  92.00 ( 93.00)
 * Acc@1 74.010 Acc@5 93.010
### epoch[71] execution time: 59.12192749977112
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.304 ( 0.304)	Data  0.159 ( 0.159)	Loss 9.8228e-03 (9.8228e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.6699e-02 (1.6470e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.139 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.5203e-02 (1.6865e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.134 ( 0.143)	Data  0.001 ( 0.006)	Loss 6.5969e-03 (1.6086e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.0130e-02 (1.6275e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2278e-02 (1.6493e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2127e-02 (1.6418e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.9896e-02 (1.6442e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3552e-02 (1.6608e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.1049e-02 (1.6711e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1202e-02 (1.6739e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1081e-02 (1.6674e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4052e-02 (1.6354e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0110e-02 (1.6186e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7378e-02 (1.6318e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6805e-02 (1.6368e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2284e-02 (1.6453e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.136 ( 0.139)	Data  0.002 ( 0.002)	Loss 1.7376e-02 (1.6555e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0381e-02 (1.6376e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5766e-02 (1.6252e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1564e-02 (1.6308e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3686e-02 (1.6289e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6294e-02 (1.6410e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5455e-02 (1.6399e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.150 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2757e-02 (1.6524e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2736e-02 (1.6566e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2572e-02 (1.6593e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1546e-02 (1.6519e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4331e-02 (1.6546e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2223e-02 (1.6494e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.6810e-02 (1.6526e-02)	Acc@1  98.44 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3514e-02 (1.6404e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0741e-02 (1.6372e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4379e-02 (1.6354e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2659e-02 (1.6463e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6532e-02 (1.6420e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8157e-02 (1.6326e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1187e-02 (1.6257e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4107e-02 (1.6332e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.3878e-02 (1.6320e-02)	Acc@1  98.75 ( 99.85)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.6704609394073486
## e[72]       loss.backward (sum) time: 13.926906108856201
## e[72]      optimizer.step (sum) time: 14.6993088722229
## epoch[72] training(only) time: 54.03103804588318
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.1555e+00 (1.1555e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.061)	Loss 1.5620e+00 (1.3270e+00)	Acc@1  69.00 ( 72.91)	Acc@5  89.00 ( 91.27)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1070e+00 (1.2325e+00)	Acc@1  78.00 ( 73.95)	Acc@5  94.00 ( 92.62)
Test: [ 30/100]	Time  0.056 ( 0.053)	Loss 1.6137e+00 (1.2671e+00)	Acc@1  70.00 ( 73.42)	Acc@5  91.00 ( 92.13)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5558e+00 (1.2762e+00)	Acc@1  70.00 ( 73.24)	Acc@5  91.00 ( 92.46)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2004e+00 (1.2598e+00)	Acc@1  78.00 ( 73.53)	Acc@5  94.00 ( 92.37)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1088e+00 (1.2486e+00)	Acc@1  73.00 ( 73.62)	Acc@5  97.00 ( 92.74)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2847e+00 (1.2591e+00)	Acc@1  76.00 ( 73.55)	Acc@5  94.00 ( 92.83)
Test: [ 80/100]	Time  0.057 ( 0.049)	Loss 1.4273e+00 (1.2662e+00)	Acc@1  74.00 ( 73.49)	Acc@5  91.00 ( 92.64)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5319e+00 (1.2462e+00)	Acc@1  71.00 ( 73.67)	Acc@5  92.00 ( 92.89)
 * Acc@1 73.990 Acc@5 92.880
### epoch[72] execution time: 59.041093587875366
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.302 ( 0.302)	Data  0.156 ( 0.156)	Loss 8.3386e-03 (8.3386e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.133 ( 0.151)	Data  0.001 ( 0.015)	Loss 1.0637e-02 (1.3809e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.009)	Loss 2.0289e-02 (1.5810e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.0098e-02 (1.6395e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.4856e-02 (1.7083e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.6749e-02 (1.7285e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.004)	Loss 8.2712e-03 (1.6985e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.1402e-02 (1.6743e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1834e-02 (1.6812e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1007e-02 (1.6760e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.8505e-02 (1.6799e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3108e-02 (1.6497e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7322e-02 (1.7160e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2506e-02 (1.7157e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1649e-02 (1.7308e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8320e-02 (1.7177e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4929e-02 (1.7197e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9530e-02 (1.7178e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9624e-02 (1.7028e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9973e-02 (1.7084e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7503e-02 (1.6979e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1490e-02 (1.6848e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3029e-02 (1.6866e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5258e-02 (1.6768e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2976e-03 (1.6857e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4570e-02 (1.6907e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8750e-03 (1.6835e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9606e-02 (1.6850e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2414e-02 (1.6770e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.9346e-03 (1.6660e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1060e-03 (1.6606e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8536e-02 (1.6632e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6033e-02 (1.6648e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0726e-02 (1.6577e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5571e-02 (1.6518e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2273e-02 (1.6526e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5801e-02 (1.6554e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1775e-02 (1.6475e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5183e-02 (1.6448e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.113 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8298e-02 (1.6427e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.6653711795806885
## e[73]       loss.backward (sum) time: 13.844302654266357
## e[73]      optimizer.step (sum) time: 14.66755986213684
## epoch[73] training(only) time: 53.90130662918091
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1614e+00 (1.1614e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.5691e+00 (1.3559e+00)	Acc@1  71.00 ( 72.82)	Acc@5  90.00 ( 91.09)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.1277e+00 (1.2560e+00)	Acc@1  76.00 ( 73.86)	Acc@5  94.00 ( 92.43)
Test: [ 30/100]	Time  0.050 ( 0.053)	Loss 1.6543e+00 (1.2913e+00)	Acc@1  68.00 ( 73.26)	Acc@5  92.00 ( 92.16)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.5790e+00 (1.2975e+00)	Acc@1  70.00 ( 73.02)	Acc@5  91.00 ( 92.44)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2173e+00 (1.2798e+00)	Acc@1  79.00 ( 73.29)	Acc@5  94.00 ( 92.33)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0825e+00 (1.2637e+00)	Acc@1  75.00 ( 73.57)	Acc@5  96.00 ( 92.57)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2461e+00 (1.2730e+00)	Acc@1  77.00 ( 73.55)	Acc@5  95.00 ( 92.73)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4159e+00 (1.2794e+00)	Acc@1  75.00 ( 73.53)	Acc@5  90.00 ( 92.62)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 1.5425e+00 (1.2590e+00)	Acc@1  71.00 ( 73.70)	Acc@5  91.00 ( 92.80)
 * Acc@1 74.090 Acc@5 92.850
### epoch[73] execution time: 58.900694131851196
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.314 ( 0.314)	Data  0.160 ( 0.160)	Loss 3.0899e-02 (3.0899e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.134 ( 0.155)	Data  0.001 ( 0.016)	Loss 7.9486e-03 (1.7051e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.133 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.3315e-02 (1.5157e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.4687e-02 (1.5580e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.144 ( 0.142)	Data  0.001 ( 0.005)	Loss 7.4794e-03 (1.6159e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.004)	Loss 1.4396e-02 (1.5572e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.1779e-02 (1.5952e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.1451e-02 (1.5976e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0216e-02 (1.5848e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.5337e-02 (1.5909e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4604e-02 (1.5936e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2355e-02 (1.5598e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1609e-02 (1.5562e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3031e-02 (1.5588e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1302e-02 (1.5743e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.1201e-03 (1.5554e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.5337e-02 (1.5575e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3073e-02 (1.5656e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4287e-02 (1.5670e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1978e-02 (1.5569e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9146e-02 (1.5672e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3185e-02 (1.5717e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.4447e-03 (1.5711e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5506e-02 (1.5670e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5599e-02 (1.5711e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5493e-02 (1.5660e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0338e-02 (1.5643e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2728e-02 (1.5707e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9166e-02 (1.5789e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0336e-02 (1.5807e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7876e-02 (1.5888e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3171e-02 (1.5889e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1488e-02 (1.5860e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6737e-02 (1.5837e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5304e-02 (1.5846e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2593e-03 (1.5917e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3880e-02 (1.5909e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.0055e-02 (1.6052e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1029e-02 (1.6062e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.106 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8910e-02 (1.6078e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.6695277690887451
## e[74]       loss.backward (sum) time: 13.908700704574585
## e[74]      optimizer.step (sum) time: 14.670717477798462
## epoch[74] training(only) time: 53.97701168060303
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1472e+00 (1.1472e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.5830e+00 (1.3517e+00)	Acc@1  70.00 ( 73.00)	Acc@5  89.00 ( 91.36)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.1128e+00 (1.2514e+00)	Acc@1  77.00 ( 74.10)	Acc@5  94.00 ( 92.81)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.6416e+00 (1.2857e+00)	Acc@1  69.00 ( 73.45)	Acc@5  92.00 ( 92.35)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5565e+00 (1.2910e+00)	Acc@1  69.00 ( 73.32)	Acc@5  90.00 ( 92.59)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.1916e+00 (1.2718e+00)	Acc@1  79.00 ( 73.61)	Acc@5  94.00 ( 92.63)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0750e+00 (1.2573e+00)	Acc@1  74.00 ( 73.74)	Acc@5  97.00 ( 92.92)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2586e+00 (1.2676e+00)	Acc@1  75.00 ( 73.65)	Acc@5  95.00 ( 93.00)
Test: [ 80/100]	Time  0.057 ( 0.049)	Loss 1.4029e+00 (1.2740e+00)	Acc@1  74.00 ( 73.63)	Acc@5  92.00 ( 92.88)
Test: [ 90/100]	Time  0.051 ( 0.049)	Loss 1.5304e+00 (1.2537e+00)	Acc@1  71.00 ( 73.84)	Acc@5  91.00 ( 93.04)
 * Acc@1 74.190 Acc@5 93.050
### epoch[74] execution time: 58.99295687675476
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.311 ( 0.311)	Data  0.166 ( 0.166)	Loss 1.3156e-02 (1.3156e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.137 ( 0.153)	Data  0.001 ( 0.016)	Loss 3.7057e-02 (1.7297e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.009)	Loss 1.0320e-02 (1.6402e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.2902e-02 (1.7289e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 8.6623e-03 (1.6516e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.5155e-02 (1.6855e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.135 ( 0.141)	Data  0.001 ( 0.004)	Loss 8.9628e-03 (1.6627e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0974e-02 (1.6738e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2210e-02 (1.6881e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.0867e-02 (1.6546e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7749e-02 (1.6428e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3277e-02 (1.6178e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3807e-02 (1.6358e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5807e-02 (1.6090e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.5062e-03 (1.5967e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1854e-02 (1.5873e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5651e-02 (1.5926e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.6472e-03 (1.5998e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4032e-02 (1.5908e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0251e-02 (1.6125e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2533e-02 (1.6055e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0504e-02 (1.6160e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4170e-02 (1.6104e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0193e-02 (1.6169e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0599e-02 (1.6266e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5750e-02 (1.6389e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5352e-03 (1.6240e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.7359e-02 (1.6305e-02)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.9515e-02 (1.6305e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.8562e-03 (1.6246e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1303e-02 (1.6302e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2277e-03 (1.6286e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1951e-02 (1.6252e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4239e-02 (1.6245e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3891e-02 (1.6239e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2313e-02 (1.6139e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2694e-02 (1.6137e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3200e-02 (1.6140e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0905e-02 (1.6077e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8578e-02 (1.6112e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.6666843891143799
## e[75]       loss.backward (sum) time: 13.868776321411133
## e[75]      optimizer.step (sum) time: 14.736825466156006
## epoch[75] training(only) time: 54.00900149345398
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1773e+00 (1.1773e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.050 ( 0.062)	Loss 1.5389e+00 (1.3448e+00)	Acc@1  71.00 ( 72.36)	Acc@5  90.00 ( 91.18)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0978e+00 (1.2472e+00)	Acc@1  78.00 ( 73.76)	Acc@5  94.00 ( 92.52)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6635e+00 (1.2831e+00)	Acc@1  67.00 ( 73.10)	Acc@5  91.00 ( 92.10)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.5646e+00 (1.2902e+00)	Acc@1  70.00 ( 72.93)	Acc@5  91.00 ( 92.29)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.2438e+00 (1.2735e+00)	Acc@1  79.00 ( 73.27)	Acc@5  92.00 ( 92.24)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1030e+00 (1.2590e+00)	Acc@1  73.00 ( 73.46)	Acc@5  97.00 ( 92.54)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2505e+00 (1.2684e+00)	Acc@1  76.00 ( 73.46)	Acc@5  94.00 ( 92.65)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.3990e+00 (1.2743e+00)	Acc@1  74.00 ( 73.42)	Acc@5  91.00 ( 92.49)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5278e+00 (1.2542e+00)	Acc@1  70.00 ( 73.60)	Acc@5  92.00 ( 92.75)
 * Acc@1 73.970 Acc@5 92.780
### epoch[75] execution time: 59.00530743598938
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.294 ( 0.294)	Data  0.147 ( 0.147)	Loss 8.4379e-03 (8.4379e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.139 ( 0.152)	Data  0.001 ( 0.014)	Loss 1.3428e-02 (1.5040e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.153 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.6229e-02 (1.6766e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.1324e-02 (1.7201e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.1074e-02 (1.6984e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.9420e-02 (1.6863e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2817e-02 (1.7240e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 8.1215e-03 (1.7042e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.003)	Loss 6.8984e-03 (1.6440e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1151e-02 (1.6453e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1849e-02 (1.6374e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3756e-02 (1.6121e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1213e-02 (1.6255e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6513e-02 (1.6323e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4874e-02 (1.6134e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2655e-02 (1.6269e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1274e-02 (1.6204e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0563e-02 (1.6090e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1398e-02 (1.5972e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1165e-02 (1.6061e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2743e-02 (1.6017e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2756e-02 (1.6169e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1820e-02 (1.6082e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8898e-02 (1.5946e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4404e-02 (1.5967e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4937e-02 (1.6060e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4857e-02 (1.6034e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6082e-02 (1.6035e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6082e-02 (1.5921e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8061e-02 (1.5968e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4702e-02 (1.6015e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4726e-02 (1.5945e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.0770e-02 (1.5983e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.1037e-02 (1.6053e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 9.8867e-03 (1.6077e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.9593e-02 (1.6092e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.8237e-02 (1.5994e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.9357e-02 (1.6010e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.4311e-02 (1.6003e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.105 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.9212e-02 (1.6031e-02)	Acc@1  98.75 ( 99.85)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.6651577949523926
## e[76]       loss.backward (sum) time: 13.851614713668823
## e[76]      optimizer.step (sum) time: 14.692061185836792
## epoch[76] training(only) time: 53.90834879875183
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.1652e+00 (1.1652e+00)	Acc@1  72.00 ( 72.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.5922e+00 (1.3482e+00)	Acc@1  70.00 ( 72.91)	Acc@5  89.00 ( 91.09)
Test: [ 20/100]	Time  0.049 ( 0.055)	Loss 1.1048e+00 (1.2463e+00)	Acc@1  78.00 ( 74.19)	Acc@5  94.00 ( 92.62)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.6846e+00 (1.2796e+00)	Acc@1  67.00 ( 73.42)	Acc@5  92.00 ( 92.19)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5435e+00 (1.2834e+00)	Acc@1  69.00 ( 73.32)	Acc@5  90.00 ( 92.41)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.1881e+00 (1.2668e+00)	Acc@1  79.00 ( 73.55)	Acc@5  94.00 ( 92.39)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0885e+00 (1.2505e+00)	Acc@1  75.00 ( 73.74)	Acc@5  96.00 ( 92.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2368e+00 (1.2622e+00)	Acc@1  74.00 ( 73.63)	Acc@5  95.00 ( 92.82)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4085e+00 (1.2684e+00)	Acc@1  75.00 ( 73.58)	Acc@5  91.00 ( 92.72)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5059e+00 (1.2489e+00)	Acc@1  71.00 ( 73.76)	Acc@5  91.00 ( 92.92)
 * Acc@1 74.020 Acc@5 92.950
### epoch[76] execution time: 58.87002682685852
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.296 ( 0.296)	Data  0.152 ( 0.152)	Loss 8.9034e-03 (8.9034e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.136 ( 0.150)	Data  0.001 ( 0.015)	Loss 1.1707e-02 (1.5665e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.1087e-02 (1.5373e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.144 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.6244e-02 (1.5001e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.3557e-02 (1.4496e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.145 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.9912e-02 (1.5202e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.004)	Loss 9.0369e-03 (1.4634e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2087e-02 (1.4800e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7416e-02 (1.4896e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 8.4827e-03 (1.5013e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.141 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0936e-02 (1.4967e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7648e-02 (1.4878e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1419e-02 (1.5193e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8057e-02 (1.5186e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2952e-02 (1.5411e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3970e-02 (1.5592e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1002e-02 (1.5626e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.3720e-02 (1.5776e-02)	Acc@1  98.44 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.0447e-02 (1.5920e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6722e-02 (1.6073e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1082e-02 (1.6078e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0762e-02 (1.6025e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6625e-02 (1.6215e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.7338e-03 (1.6170e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4314e-03 (1.6238e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9533e-02 (1.6174e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4757e-02 (1.6253e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3517e-02 (1.6181e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.1094e-02 (1.6258e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.8497e-03 (1.6127e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.2802e-03 (1.6102e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2034e-02 (1.6221e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3064e-02 (1.6264e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3730e-02 (1.6399e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8565e-02 (1.6387e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.3161e-02 (1.6454e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0078e-02 (1.6425e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4949e-02 (1.6417e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5211e-02 (1.6500e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.111 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.9370e-02 (1.6532e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.6705846786499023
## e[77]       loss.backward (sum) time: 13.920524597167969
## e[77]      optimizer.step (sum) time: 14.672154664993286
## epoch[77] training(only) time: 54.006826639175415
# Switched to evaluate mode...
Test: [  0/100]	Time  0.194 ( 0.194)	Loss 1.1930e+00 (1.1930e+00)	Acc@1  72.00 ( 72.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.051 ( 0.063)	Loss 1.5675e+00 (1.3522e+00)	Acc@1  73.00 ( 72.45)	Acc@5  90.00 ( 91.36)
Test: [ 20/100]	Time  0.050 ( 0.055)	Loss 1.1202e+00 (1.2517e+00)	Acc@1  77.00 ( 73.86)	Acc@5  94.00 ( 92.71)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6533e+00 (1.2856e+00)	Acc@1  67.00 ( 73.29)	Acc@5  91.00 ( 92.19)
Test: [ 40/100]	Time  0.055 ( 0.052)	Loss 1.5556e+00 (1.2926e+00)	Acc@1  70.00 ( 73.00)	Acc@5  92.00 ( 92.54)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.1815e+00 (1.2746e+00)	Acc@1  79.00 ( 73.41)	Acc@5  94.00 ( 92.41)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0913e+00 (1.2600e+00)	Acc@1  73.00 ( 73.54)	Acc@5  96.00 ( 92.72)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2462e+00 (1.2704e+00)	Acc@1  77.00 ( 73.52)	Acc@5  94.00 ( 92.76)
Test: [ 80/100]	Time  0.049 ( 0.050)	Loss 1.3971e+00 (1.2761e+00)	Acc@1  74.00 ( 73.48)	Acc@5  91.00 ( 92.60)
Test: [ 90/100]	Time  0.056 ( 0.049)	Loss 1.5383e+00 (1.2558e+00)	Acc@1  72.00 ( 73.70)	Acc@5  92.00 ( 92.82)
 * Acc@1 74.100 Acc@5 92.840
### epoch[77] execution time: 59.00896596908569
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.293 ( 0.293)	Data  0.148 ( 0.148)	Loss 2.1259e-02 (2.1259e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.134 ( 0.150)	Data  0.001 ( 0.014)	Loss 9.1434e-03 (1.2716e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.152 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.5508e-02 (1.4526e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.3369e-02 (1.6186e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.4209e-02 (1.6119e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.142 ( 0.140)	Data  0.001 ( 0.004)	Loss 8.5057e-03 (1.5766e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.7101e-02 (1.5466e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5230e-02 (1.5342e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6607e-02 (1.5537e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5729e-02 (1.5344e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6901e-02 (1.5307e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3065e-02 (1.5189e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3273e-02 (1.5531e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.146 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2395e-02 (1.5574e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.2160e-02 (1.5435e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1309e-02 (1.5521e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9761e-02 (1.5685e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1633e-02 (1.5615e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2010e-02 (1.5691e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2947e-02 (1.5807e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1885e-02 (1.5774e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5012e-02 (1.5683e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3525e-02 (1.5686e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4866e-03 (1.5509e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4260e-02 (1.5506e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4395e-02 (1.5548e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5613e-02 (1.5604e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5889e-02 (1.5591e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3479e-02 (1.5560e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0434e-02 (1.5511e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7889e-02 (1.5550e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1917e-02 (1.5477e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7417e-02 (1.5622e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5664e-02 (1.5718e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3107e-02 (1.5733e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7696e-02 (1.5690e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.6898e-02 (1.5756e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.9949e-02 (1.5747e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.5668e-02 (1.5774e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.113 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.2864e-02 (1.5747e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.6673974990844727
## e[78]       loss.backward (sum) time: 13.85041880607605
## e[78]      optimizer.step (sum) time: 14.696205615997314
## epoch[78] training(only) time: 53.960973262786865
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1601e+00 (1.1601e+00)	Acc@1  73.00 ( 73.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.049 ( 0.061)	Loss 1.6004e+00 (1.3521e+00)	Acc@1  71.00 ( 73.18)	Acc@5  89.00 ( 91.45)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1243e+00 (1.2475e+00)	Acc@1  77.00 ( 74.24)	Acc@5  94.00 ( 92.81)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.6624e+00 (1.2837e+00)	Acc@1  67.00 ( 73.61)	Acc@5  91.00 ( 92.29)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5701e+00 (1.2892e+00)	Acc@1  70.00 ( 73.29)	Acc@5  92.00 ( 92.56)
Test: [ 50/100]	Time  0.052 ( 0.051)	Loss 1.2134e+00 (1.2716e+00)	Acc@1  80.00 ( 73.49)	Acc@5  94.00 ( 92.49)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1192e+00 (1.2582e+00)	Acc@1  72.00 ( 73.61)	Acc@5  97.00 ( 92.82)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2510e+00 (1.2680e+00)	Acc@1  74.00 ( 73.58)	Acc@5  95.00 ( 92.86)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4179e+00 (1.2744e+00)	Acc@1  75.00 ( 73.64)	Acc@5  91.00 ( 92.74)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5408e+00 (1.2537e+00)	Acc@1  70.00 ( 73.82)	Acc@5  92.00 ( 92.95)
 * Acc@1 74.140 Acc@5 92.960
### epoch[78] execution time: 58.96408152580261
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.309 ( 0.309)	Data  0.159 ( 0.159)	Loss 1.1753e-02 (1.1753e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.140 ( 0.152)	Data  0.001 ( 0.015)	Loss 6.7293e-03 (1.3059e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.009)	Loss 9.0727e-03 (1.3773e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.139 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.2506e-02 (1.3830e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.5629e-02 (1.4315e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.143 ( 0.141)	Data  0.001 ( 0.004)	Loss 2.0196e-02 (1.5107e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.0165e-02 (1.5415e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 9.3234e-03 (1.5441e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.4854e-02 (1.5417e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 8.1184e-03 (1.5511e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.5559e-03 (1.5662e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.3171e-02 (1.5549e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4108e-02 (1.5461e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1769e-02 (1.5536e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.5400e-02 (1.5616e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6870e-02 (1.5689e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.1127e-03 (1.5594e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2696e-02 (1.5596e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8864e-02 (1.5621e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3408e-02 (1.5614e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4656e-02 (1.5671e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8401e-02 (1.5569e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3376e-02 (1.5477e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3453e-02 (1.5526e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7480e-02 (1.5559e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6648e-02 (1.5517e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1032e-02 (1.5500e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0177e-02 (1.5597e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9648e-02 (1.5733e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4961e-02 (1.5666e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1350e-02 (1.5664e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8614e-02 (1.5607e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5289e-02 (1.5686e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1838e-02 (1.5707e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0242e-02 (1.5731e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1591e-02 (1.5734e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4759e-02 (1.5708e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0865e-02 (1.5683e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4870e-02 (1.5660e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.113 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.9561e-02 (1.5771e-02)	Acc@1  98.75 ( 99.89)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.6678934097290039
## e[79]       loss.backward (sum) time: 13.857470989227295
## e[79]      optimizer.step (sum) time: 14.739675760269165
## epoch[79] training(only) time: 53.90068197250366
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.1531e+00 (1.1531e+00)	Acc@1  75.00 ( 75.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.047 ( 0.062)	Loss 1.5715e+00 (1.3471e+00)	Acc@1  70.00 ( 73.18)	Acc@5  89.00 ( 91.45)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0937e+00 (1.2458e+00)	Acc@1  78.00 ( 74.24)	Acc@5  94.00 ( 92.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.6835e+00 (1.2821e+00)	Acc@1  70.00 ( 73.61)	Acc@5  91.00 ( 92.29)
Test: [ 40/100]	Time  0.055 ( 0.051)	Loss 1.5409e+00 (1.2877e+00)	Acc@1  70.00 ( 73.32)	Acc@5  92.00 ( 92.61)
Test: [ 50/100]	Time  0.051 ( 0.051)	Loss 1.2304e+00 (1.2719e+00)	Acc@1  79.00 ( 73.53)	Acc@5  94.00 ( 92.63)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1192e+00 (1.2586e+00)	Acc@1  73.00 ( 73.66)	Acc@5  97.00 ( 92.95)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2709e+00 (1.2695e+00)	Acc@1  73.00 ( 73.52)	Acc@5  95.00 ( 93.00)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4156e+00 (1.2752e+00)	Acc@1  74.00 ( 73.57)	Acc@5  91.00 ( 92.88)
Test: [ 90/100]	Time  0.054 ( 0.049)	Loss 1.5413e+00 (1.2542e+00)	Acc@1  73.00 ( 73.80)	Acc@5  92.00 ( 93.09)
 * Acc@1 74.160 Acc@5 93.080
### epoch[79] execution time: 58.86048913002014
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.296 ( 0.296)	Data  0.152 ( 0.152)	Loss 8.6815e-03 (8.6815e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.134 ( 0.151)	Data  0.001 ( 0.015)	Loss 1.3115e-02 (1.3968e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.137 ( 0.144)	Data  0.001 ( 0.008)	Loss 4.3037e-02 (1.6439e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.006)	Loss 1.9353e-02 (1.5757e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.005)	Loss 7.3910e-03 (1.4688e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.148 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.2808e-02 (1.4789e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.2532e-02 (1.4733e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.8492e-02 (1.4841e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.4696e-02 (1.4987e-02)	Acc@1  99.22 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.1275e-02 (1.4941e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.9459e-02 (1.5071e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.003)	Loss 1.0817e-02 (1.5271e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4314e-02 (1.5377e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0259e-02 (1.5707e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4485e-02 (1.5756e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5180e-02 (1.5755e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1428e-02 (1.5640e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9105e-02 (1.5655e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6344e-02 (1.5719e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4137e-02 (1.5710e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8349e-03 (1.5660e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2342e-02 (1.5565e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4263e-02 (1.5734e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2700e-02 (1.5746e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.133 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8514e-02 (1.5746e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6500e-03 (1.5841e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.8988e-03 (1.5780e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9362e-02 (1.5808e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5265e-03 (1.5720e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6029e-02 (1.5740e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3257e-02 (1.5856e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8115e-02 (1.5887e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1095e-02 (1.5919e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0328e-02 (1.5864e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9997e-02 (1.5834e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2622e-02 (1.5819e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6635e-02 (1.5792e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0366e-02 (1.5875e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3378e-02 (1.5865e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.123 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9836e-02 (1.5862e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.6685292720794678
## e[80]       loss.backward (sum) time: 13.902440309524536
## e[80]      optimizer.step (sum) time: 14.679630517959595
## epoch[80] training(only) time: 53.892478466033936
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.1974e+00 (1.1974e+00)	Acc@1  72.00 ( 72.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.5702e+00 (1.3532e+00)	Acc@1  70.00 ( 72.64)	Acc@5  90.00 ( 91.45)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.1053e+00 (1.2514e+00)	Acc@1  78.00 ( 74.05)	Acc@5  94.00 ( 92.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.6635e+00 (1.2871e+00)	Acc@1  66.00 ( 73.35)	Acc@5  92.00 ( 92.26)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5431e+00 (1.2936e+00)	Acc@1  70.00 ( 73.10)	Acc@5  91.00 ( 92.54)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2429e+00 (1.2766e+00)	Acc@1  78.00 ( 73.27)	Acc@5  93.00 ( 92.53)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0987e+00 (1.2618e+00)	Acc@1  73.00 ( 73.44)	Acc@5  98.00 ( 92.87)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2351e+00 (1.2712e+00)	Acc@1  75.00 ( 73.38)	Acc@5  95.00 ( 92.94)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4117e+00 (1.2769e+00)	Acc@1  74.00 ( 73.42)	Acc@5  91.00 ( 92.80)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5740e+00 (1.2574e+00)	Acc@1  70.00 ( 73.57)	Acc@5  91.00 ( 92.97)
 * Acc@1 73.940 Acc@5 92.970
### epoch[80] execution time: 58.85491967201233
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.298 ( 0.298)	Data  0.151 ( 0.151)	Loss 1.9210e-02 (1.9210e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.8652e-02 (1.7289e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.9848e-02 (1.8003e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.006)	Loss 1.2955e-02 (1.7585e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.136 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.9757e-02 (1.8455e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.141 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.1879e-02 (1.7845e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.2872e-02 (1.7673e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5076e-02 (1.7297e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.0222e-02 (1.6916e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.7778e-02 (1.6800e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 8.9512e-03 (1.6592e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2508e-02 (1.6600e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4834e-02 (1.6565e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.8236e-02 (1.6396e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5942e-02 (1.6343e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1596e-02 (1.6351e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 ( 99.99)
Epoch: [81][160/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4585e-03 (1.6645e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9124e-03 (1.6526e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3282e-02 (1.6390e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1850e-02 (1.6325e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0307e-02 (1.6414e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.3831e-03 (1.6405e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8620e-02 (1.6416e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8080e-02 (1.6572e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4392e-02 (1.6467e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.1021e-03 (1.6456e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3160e-02 (1.6500e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7997e-02 (1.6422e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2412e-02 (1.6345e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1088e-02 (1.6210e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.4068e-03 (1.6092e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2304e-02 (1.6101e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2887e-02 (1.6233e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1542e-02 (1.6372e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3347e-02 (1.6197e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2681e-02 (1.6114e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5532e-02 (1.6247e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3926e-02 (1.6254e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2829e-02 (1.6250e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6552e-02 (1.6211e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.6718347072601318
## e[81]       loss.backward (sum) time: 13.854194164276123
## e[81]      optimizer.step (sum) time: 14.685517311096191
## epoch[81] training(only) time: 53.94963788986206
# Switched to evaluate mode...
Test: [  0/100]	Time  0.200 ( 0.200)	Loss 1.1469e+00 (1.1469e+00)	Acc@1  74.00 ( 74.00)	Acc@5  95.00 ( 95.00)
Test: [ 10/100]	Time  0.049 ( 0.063)	Loss 1.5764e+00 (1.3478e+00)	Acc@1  71.00 ( 73.36)	Acc@5  90.00 ( 91.73)
Test: [ 20/100]	Time  0.046 ( 0.056)	Loss 1.1052e+00 (1.2524e+00)	Acc@1  78.00 ( 74.19)	Acc@5  94.00 ( 92.90)
Test: [ 30/100]	Time  0.049 ( 0.053)	Loss 1.6183e+00 (1.2852e+00)	Acc@1  69.00 ( 73.39)	Acc@5  93.00 ( 92.29)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.5305e+00 (1.2901e+00)	Acc@1  70.00 ( 73.12)	Acc@5  91.00 ( 92.56)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2170e+00 (1.2743e+00)	Acc@1  79.00 ( 73.31)	Acc@5  94.00 ( 92.47)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1363e+00 (1.2612e+00)	Acc@1  73.00 ( 73.49)	Acc@5  97.00 ( 92.82)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2352e+00 (1.2697e+00)	Acc@1  75.00 ( 73.52)	Acc@5  96.00 ( 92.89)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4206e+00 (1.2757e+00)	Acc@1  74.00 ( 73.58)	Acc@5  91.00 ( 92.75)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.5326e+00 (1.2539e+00)	Acc@1  72.00 ( 73.73)	Acc@5  92.00 ( 92.99)
 * Acc@1 74.010 Acc@5 93.020
### epoch[81] execution time: 58.931713342666626
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.306 ( 0.306)	Data  0.157 ( 0.157)	Loss 1.7700e-02 (1.7700e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.138 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.1275e-02 (1.4699e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.135 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.3564e-02 (1.6814e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.3378e-02 (1.6921e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.137 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.5604e-02 (1.7217e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.1088e-02 (1.6927e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.7704e-02 (1.6276e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4547e-02 (1.5891e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.134 ( 0.140)	Data  0.001 ( 0.003)	Loss 4.3120e-02 (1.6231e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.1219e-03 (1.6401e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.144 ( 0.139)	Data  0.001 ( 0.003)	Loss 9.0909e-03 (1.6086e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.147 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.3010e-02 (1.6029e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1501e-02 (1.5966e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6966e-02 (1.5857e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.148 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6604e-02 (1.5753e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3292e-02 (1.5729e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.4567e-02 (1.5685e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1695e-02 (1.5790e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1468e-02 (1.5722e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3341e-02 (1.5707e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3503e-02 (1.5823e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.4242e-03 (1.5904e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1932e-02 (1.5814e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6730e-02 (1.5724e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7450e-02 (1.5707e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3879e-02 (1.5634e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0088e-02 (1.5624e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1441e-02 (1.5621e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4340e-02 (1.5587e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1334e-02 (1.5532e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2448e-02 (1.5491e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8546e-02 (1.5523e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.5943e-03 (1.5469e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1560e-02 (1.5437e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5046e-02 (1.5486e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2599e-02 (1.5543e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8977e-02 (1.5553e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3148e-02 (1.5543e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4950e-02 (1.5558e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.8072e-02 (1.5531e-02)	Acc@1  98.75 ( 99.86)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.6676697731018066
## e[82]       loss.backward (sum) time: 13.92782974243164
## e[82]      optimizer.step (sum) time: 14.699105978012085
## epoch[82] training(only) time: 54.04814672470093
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 1.1861e+00 (1.1861e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.5700e+00 (1.3490e+00)	Acc@1  70.00 ( 72.55)	Acc@5  89.00 ( 91.36)
Test: [ 20/100]	Time  0.047 ( 0.056)	Loss 1.1147e+00 (1.2504e+00)	Acc@1  78.00 ( 73.81)	Acc@5  94.00 ( 92.43)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.6539e+00 (1.2814e+00)	Acc@1  69.00 ( 73.32)	Acc@5  92.00 ( 91.97)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.5449e+00 (1.2868e+00)	Acc@1  69.00 ( 73.22)	Acc@5  92.00 ( 92.41)
Test: [ 50/100]	Time  0.059 ( 0.051)	Loss 1.2400e+00 (1.2719e+00)	Acc@1  76.00 ( 73.47)	Acc@5  94.00 ( 92.39)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1097e+00 (1.2587e+00)	Acc@1  74.00 ( 73.66)	Acc@5  98.00 ( 92.72)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2710e+00 (1.2689e+00)	Acc@1  78.00 ( 73.66)	Acc@5  95.00 ( 92.85)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4260e+00 (1.2752e+00)	Acc@1  74.00 ( 73.59)	Acc@5  91.00 ( 92.74)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5346e+00 (1.2545e+00)	Acc@1  73.00 ( 73.79)	Acc@5  92.00 ( 92.96)
 * Acc@1 74.130 Acc@5 92.970
### epoch[82] execution time: 59.05920743942261
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.297 ( 0.297)	Data  0.144 ( 0.144)	Loss 1.3639e-02 (1.3639e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.136 ( 0.152)	Data  0.001 ( 0.014)	Loss 1.5947e-02 (1.5662e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.008)	Loss 1.5114e-02 (1.7002e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.135 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.1517e-02 (1.5643e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.0948e-02 (1.6499e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.3297e-02 (1.6206e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.7714e-02 (1.6158e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.138 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.8472e-02 (1.6446e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6528e-02 (1.6224e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.7132e-02 (1.6136e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.4834e-02 (1.5982e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3195e-02 (1.5928e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.5878e-02 (1.6034e-02)	Acc@1  99.22 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.7410e-03 (1.5943e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7156e-02 (1.6102e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6827e-02 (1.5970e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2086e-02 (1.5981e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0323e-02 (1.5780e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2036e-02 (1.5958e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4409e-02 (1.5856e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8444e-02 (1.5801e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4970e-02 (1.5753e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2940e-02 (1.5731e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4819e-02 (1.5640e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6732e-02 (1.5598e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2233e-02 (1.5632e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.2391e-03 (1.5839e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8609e-02 (1.5895e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.147 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9137e-02 (1.5879e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1662e-02 (1.5934e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0127e-02 (1.5887e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3164e-02 (1.5863e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6232e-02 (1.5943e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5093e-02 (1.5956e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.4554e-03 (1.5970e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.143 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7625e-02 (1.6020e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7937e-02 (1.6048e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0853e-02 (1.6069e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0175e-02 (1.5966e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.0509e-02 (1.5942e-02)	Acc@1  98.75 ( 99.86)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.6660010814666748
## e[83]       loss.backward (sum) time: 13.90302062034607
## e[83]      optimizer.step (sum) time: 14.70365047454834
## epoch[83] training(only) time: 53.95097064971924
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1749e+00 (1.1749e+00)	Acc@1  72.00 ( 72.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.5497e+00 (1.3502e+00)	Acc@1  70.00 ( 73.00)	Acc@5  90.00 ( 91.36)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1106e+00 (1.2523e+00)	Acc@1  76.00 ( 74.05)	Acc@5  94.00 ( 92.57)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.6483e+00 (1.2825e+00)	Acc@1  67.00 ( 73.35)	Acc@5  92.00 ( 92.16)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 1.5359e+00 (1.2882e+00)	Acc@1  70.00 ( 73.15)	Acc@5  92.00 ( 92.46)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2104e+00 (1.2718e+00)	Acc@1  78.00 ( 73.45)	Acc@5  93.00 ( 92.41)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1100e+00 (1.2570e+00)	Acc@1  75.00 ( 73.72)	Acc@5  95.00 ( 92.72)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2432e+00 (1.2660e+00)	Acc@1  75.00 ( 73.59)	Acc@5  95.00 ( 92.87)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4405e+00 (1.2735e+00)	Acc@1  75.00 ( 73.57)	Acc@5  89.00 ( 92.73)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5220e+00 (1.2520e+00)	Acc@1  73.00 ( 73.75)	Acc@5  91.00 ( 92.95)
 * Acc@1 74.040 Acc@5 92.970
### epoch[83] execution time: 58.954506158828735
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.299 ( 0.299)	Data  0.150 ( 0.150)	Loss 1.5268e-02 (1.5268e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.134 ( 0.152)	Data  0.001 ( 0.014)	Loss 1.3523e-02 (1.8095e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.145 ( 0.146)	Data  0.001 ( 0.008)	Loss 2.1407e-02 (1.7820e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.136 ( 0.143)	Data  0.001 ( 0.006)	Loss 9.9649e-03 (1.6693e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.135 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.5748e-02 (1.6399e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 8.4885e-03 (1.5890e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.5086e-03 (1.5753e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.0500e-03 (1.5724e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.139 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5132e-02 (1.5744e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.4206e-02 (1.5759e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.6562e-02 (1.5754e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.139 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4832e-02 (1.5540e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.5773e-03 (1.5247e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1908e-02 (1.5276e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.142 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2699e-02 (1.5302e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7952e-02 (1.5196e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.4671e-03 (1.5185e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.152 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.1435e-02 (1.5150e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9676e-02 (1.5156e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0625e-02 (1.5210e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9837e-02 (1.5413e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.9209e-03 (1.5495e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6115e-02 (1.5523e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4227e-02 (1.5456e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3300e-02 (1.5494e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1342e-03 (1.5548e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8406e-02 (1.5660e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8419e-02 (1.5775e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1905e-02 (1.5763e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.0873e-03 (1.5717e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8840e-02 (1.5668e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.0544e-02 (1.5659e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.5428e-02 (1.5632e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.7955e-02 (1.5626e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 9.7526e-03 (1.5678e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.3214e-02 (1.5617e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.3972e-02 (1.5680e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.4977e-02 (1.5741e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.5291e-02 (1.5765e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.001)	Loss 2.8521e-02 (1.5792e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.6702747344970703
## e[84]       loss.backward (sum) time: 13.885840892791748
## e[84]      optimizer.step (sum) time: 14.699460983276367
## epoch[84] training(only) time: 53.99120473861694
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 1.1810e+00 (1.1810e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.5876e+00 (1.3586e+00)	Acc@1  71.00 ( 73.27)	Acc@5  90.00 ( 91.45)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 1.0941e+00 (1.2538e+00)	Acc@1  78.00 ( 74.14)	Acc@5  94.00 ( 92.81)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6324e+00 (1.2864e+00)	Acc@1  69.00 ( 73.42)	Acc@5  91.00 ( 92.26)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5453e+00 (1.2901e+00)	Acc@1  69.00 ( 73.29)	Acc@5  90.00 ( 92.54)
Test: [ 50/100]	Time  0.055 ( 0.052)	Loss 1.2318e+00 (1.2755e+00)	Acc@1  77.00 ( 73.53)	Acc@5  94.00 ( 92.51)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.0997e+00 (1.2606e+00)	Acc@1  75.00 ( 73.75)	Acc@5  95.00 ( 92.74)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2471e+00 (1.2693e+00)	Acc@1  75.00 ( 73.75)	Acc@5  96.00 ( 92.83)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4441e+00 (1.2740e+00)	Acc@1  75.00 ( 73.69)	Acc@5  90.00 ( 92.72)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.5399e+00 (1.2525e+00)	Acc@1  73.00 ( 73.88)	Acc@5  92.00 ( 92.96)
 * Acc@1 74.190 Acc@5 92.990
### epoch[84] execution time: 59.0190851688385
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.327 ( 0.327)	Data  0.165 ( 0.165)	Loss 1.9355e-02 (1.9355e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.137 ( 0.154)	Data  0.001 ( 0.016)	Loss 6.0129e-03 (1.3649e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.135 ( 0.146)	Data  0.001 ( 0.009)	Loss 1.3042e-02 (1.4519e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.137 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.3468e-02 (1.3827e-02)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.139 ( 0.142)	Data  0.001 ( 0.005)	Loss 1.5343e-02 (1.4101e-02)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.004)	Loss 8.0069e-03 (1.4008e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.004)	Loss 9.7198e-03 (1.4188e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.143 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1277e-02 (1.4337e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.136 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.1556e-02 (1.4657e-02)	Acc@1 100.00 ( 99.90)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.6633e-03 (1.4575e-02)	Acc@1 100.00 ( 99.91)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 3.1460e-02 (1.4839e-02)	Acc@1  98.44 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5021e-02 (1.4801e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2368e-02 (1.4703e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3275e-02 (1.4839e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 3.0545e-02 (1.4984e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5185e-02 (1.5113e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 2.1711e-02 (1.5126e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2143e-02 (1.5194e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.0717e-02 (1.5220e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0327e-02 (1.5043e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7770e-02 (1.5199e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3923e-02 (1.5138e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2150e-02 (1.5161e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4326e-02 (1.5132e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8041e-02 (1.5183e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0182e-02 (1.5117e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1224e-03 (1.5150e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4977e-02 (1.5160e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0681e-02 (1.5170e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.4819e-02 (1.5145e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6014e-02 (1.5195e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.150 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.6851e-03 (1.5220e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1550e-03 (1.5117e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0259e-02 (1.5160e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.154 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5116e-02 (1.5239e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7195e-03 (1.5234e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4981e-02 (1.5234e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2252e-02 (1.5246e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.1343e-02 (1.5312e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 5.7965e-02 (1.5379e-02)	Acc@1  97.50 ( 99.86)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.6679360866546631
## e[85]       loss.backward (sum) time: 13.874703168869019
## e[85]      optimizer.step (sum) time: 14.69947600364685
## epoch[85] training(only) time: 54.00007677078247
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.2233e+00 (1.2233e+00)	Acc@1  72.00 ( 72.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.049 ( 0.062)	Loss 1.5736e+00 (1.3628e+00)	Acc@1  69.00 ( 72.64)	Acc@5  90.00 ( 91.45)
Test: [ 20/100]	Time  0.048 ( 0.056)	Loss 1.1121e+00 (1.2537e+00)	Acc@1  79.00 ( 73.90)	Acc@5  94.00 ( 92.71)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6822e+00 (1.2873e+00)	Acc@1  65.00 ( 73.42)	Acc@5  92.00 ( 92.35)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.5332e+00 (1.2941e+00)	Acc@1  69.00 ( 73.10)	Acc@5  90.00 ( 92.66)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.2285e+00 (1.2777e+00)	Acc@1  77.00 ( 73.31)	Acc@5  93.00 ( 92.67)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.0931e+00 (1.2637e+00)	Acc@1  74.00 ( 73.51)	Acc@5  98.00 ( 92.97)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2383e+00 (1.2716e+00)	Acc@1  75.00 ( 73.49)	Acc@5  95.00 ( 93.01)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4361e+00 (1.2775e+00)	Acc@1  75.00 ( 73.53)	Acc@5  92.00 ( 92.85)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.5429e+00 (1.2572e+00)	Acc@1  70.00 ( 73.79)	Acc@5  91.00 ( 93.00)
 * Acc@1 74.090 Acc@5 93.000
### epoch[85] execution time: 59.003600120544434
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.304 ( 0.304)	Data  0.154 ( 0.154)	Loss 1.4980e-02 (1.4980e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.135 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.6382e-02 (1.6693e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.136 ( 0.145)	Data  0.001 ( 0.008)	Loss 5.4706e-03 (1.6249e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 1.5022e-02 (1.6720e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.134 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.0384e-02 (1.6239e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.137 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.0899e-02 (1.6356e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 9.4219e-03 (1.5815e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.135 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.5826e-02 (1.5799e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.2524e-02 (1.5613e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.9461e-02 (1.5439e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.5904e-02 (1.5755e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.003)	Loss 7.9355e-03 (1.5670e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 7.5848e-03 (1.5413e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.3740e-02 (1.5314e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7047e-02 (1.5337e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.2564e-02 (1.5557e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4195e-02 (1.5521e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.143 ( 0.139)	Data  0.001 ( 0.002)	Loss 8.7058e-03 (1.5820e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5464e-02 (1.5865e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1257e-02 (1.5644e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.6811e-03 (1.5624e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3329e-02 (1.5604e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0470e-02 (1.5616e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4156e-02 (1.5607e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.5117e-02 (1.5576e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3493e-02 (1.5461e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2526e-02 (1.5527e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0182e-02 (1.5471e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8853e-02 (1.5475e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.5550e-03 (1.5521e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2594e-02 (1.5479e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4297e-02 (1.5468e-02)	Acc@1  99.22 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5456e-02 (1.5427e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2658e-02 (1.5371e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.145 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5365e-02 (1.5373e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8712e-02 (1.5430e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.0120e-03 (1.5509e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3766e-02 (1.5425e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0584e-02 (1.5532e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.108 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7843e-02 (1.5454e-02)	Acc@1  98.75 ( 99.88)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.6717250347137451
## e[86]       loss.backward (sum) time: 13.8599214553833
## e[86]      optimizer.step (sum) time: 14.734647512435913
## epoch[86] training(only) time: 53.99229669570923
# Switched to evaluate mode...
Test: [  0/100]	Time  0.196 ( 0.196)	Loss 1.1590e+00 (1.1590e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.049 ( 0.063)	Loss 1.6073e+00 (1.3607e+00)	Acc@1  71.00 ( 73.36)	Acc@5  89.00 ( 91.27)
Test: [ 20/100]	Time  0.052 ( 0.056)	Loss 1.1243e+00 (1.2535e+00)	Acc@1  76.00 ( 74.33)	Acc@5  94.00 ( 92.52)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6504e+00 (1.2873e+00)	Acc@1  70.00 ( 73.71)	Acc@5  91.00 ( 92.10)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5093e+00 (1.2892e+00)	Acc@1  69.00 ( 73.44)	Acc@5  91.00 ( 92.44)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2460e+00 (1.2739e+00)	Acc@1  77.00 ( 73.75)	Acc@5  95.00 ( 92.49)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1293e+00 (1.2598e+00)	Acc@1  76.00 ( 73.90)	Acc@5  97.00 ( 92.82)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2400e+00 (1.2691e+00)	Acc@1  76.00 ( 73.86)	Acc@5  95.00 ( 92.90)
Test: [ 80/100]	Time  0.047 ( 0.050)	Loss 1.4127e+00 (1.2746e+00)	Acc@1  75.00 ( 73.84)	Acc@5  93.00 ( 92.81)
Test: [ 90/100]	Time  0.049 ( 0.049)	Loss 1.5628e+00 (1.2538e+00)	Acc@1  74.00 ( 73.99)	Acc@5  91.00 ( 93.03)
 * Acc@1 74.290 Acc@5 93.020
### epoch[86] execution time: 58.982144594192505
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.306 ( 0.306)	Data  0.161 ( 0.161)	Loss 1.6656e-02 (1.6656e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.135 ( 0.151)	Data  0.001 ( 0.016)	Loss 1.4325e-02 (1.7472e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.137 ( 0.145)	Data  0.001 ( 0.009)	Loss 1.3961e-02 (1.6298e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.150 ( 0.142)	Data  0.001 ( 0.006)	Loss 2.2676e-02 (1.7532e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.005)	Loss 2.8607e-02 (1.7903e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.7495e-02 (1.7726e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.145 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.9590e-02 (1.8192e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 2.4898e-02 (1.7679e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2454e-02 (1.7376e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.145 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.5359e-02 (1.7362e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3273e-02 (1.7216e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.2342e-02 (1.6886e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.8254e-02 (1.6722e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4345e-02 (1.6762e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.142 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1237e-02 (1.6621e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1772e-02 (1.6458e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.9227e-03 (1.6203e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1877e-02 (1.6235e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6911e-02 (1.6210e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5404e-02 (1.6142e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1299e-02 (1.6153e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 6.1323e-03 (1.6160e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.6441e-03 (1.6132e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3422e-02 (1.6036e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3185e-02 (1.5991e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1839e-02 (1.5946e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2968e-02 (1.6031e-02)	Acc@1  98.44 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3003e-02 (1.5983e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3952e-02 (1.5953e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7939e-02 (1.5983e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0783e-02 (1.5921e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.7970e-02 (1.5919e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0476e-02 (1.5926e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7061e-02 (1.5819e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.148 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3584e-02 (1.5775e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4210e-03 (1.5722e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6548e-02 (1.5609e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3308e-02 (1.5642e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8137e-02 (1.5640e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.112 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0542e-02 (1.5709e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.6664626598358154
## e[87]       loss.backward (sum) time: 13.802457332611084
## e[87]      optimizer.step (sum) time: 14.718486309051514
## epoch[87] training(only) time: 53.97203278541565
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.1730e+00 (1.1730e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.057 ( 0.062)	Loss 1.5710e+00 (1.3550e+00)	Acc@1  70.00 ( 73.18)	Acc@5  89.00 ( 91.09)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.1053e+00 (1.2529e+00)	Acc@1  79.00 ( 74.38)	Acc@5  94.00 ( 92.48)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6594e+00 (1.2851e+00)	Acc@1  68.00 ( 73.74)	Acc@5  91.00 ( 92.06)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5198e+00 (1.2888e+00)	Acc@1  69.00 ( 73.46)	Acc@5  91.00 ( 92.41)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2269e+00 (1.2745e+00)	Acc@1  78.00 ( 73.67)	Acc@5  94.00 ( 92.35)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1136e+00 (1.2593e+00)	Acc@1  75.00 ( 73.77)	Acc@5  98.00 ( 92.72)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2745e+00 (1.2689e+00)	Acc@1  76.00 ( 73.66)	Acc@5  94.00 ( 92.82)
Test: [ 80/100]	Time  0.049 ( 0.049)	Loss 1.4192e+00 (1.2741e+00)	Acc@1  75.00 ( 73.60)	Acc@5  92.00 ( 92.75)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.5369e+00 (1.2537e+00)	Acc@1  73.00 ( 73.82)	Acc@5  92.00 ( 92.97)
 * Acc@1 74.200 Acc@5 92.970
### epoch[87] execution time: 58.95818519592285
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.299 ( 0.299)	Data  0.150 ( 0.150)	Loss 1.6868e-02 (1.6868e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.139 ( 0.153)	Data  0.001 ( 0.015)	Loss 1.0977e-02 (1.4176e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 7.9530e-03 (1.4030e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.138 ( 0.143)	Data  0.001 ( 0.006)	Loss 2.3680e-02 (1.4551e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.140 ( 0.142)	Data  0.001 ( 0.005)	Loss 7.7330e-03 (1.4596e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.139 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.1999e-02 (1.4947e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.004)	Loss 9.8785e-03 (1.4740e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.140 ( 0.140)	Data  0.001 ( 0.003)	Loss 1.3069e-02 (1.4639e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.6266e-02 (1.4452e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.133 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.1802e-02 (1.4853e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3073e-02 (1.4980e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.2579e-03 (1.4798e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6086e-02 (1.4880e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6634e-02 (1.5056e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7390e-02 (1.5232e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.149 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.5517e-02 (1.5252e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.140 ( 0.139)	Data  0.001 ( 0.002)	Loss 9.6279e-03 (1.5354e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.138 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.7538e-02 (1.5435e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.136 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.4884e-02 (1.5595e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2331e-02 (1.5390e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.151 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8862e-02 (1.5327e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8514e-02 (1.5418e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4194e-02 (1.5356e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.146 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.4611e-02 (1.5417e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 7.3203e-03 (1.5338e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.7871e-02 (1.5356e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 3.2119e-02 (1.5443e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3750e-02 (1.5304e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.8100e-02 (1.5246e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4561e-02 (1.5192e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3181e-02 (1.5194e-02)	Acc@1  99.22 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0432e-02 (1.5197e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.144 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.3323e-02 (1.5212e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0851e-02 (1.5151e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0805e-02 (1.5214e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5178e-02 (1.5281e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 4.2166e-02 (1.5344e-02)	Acc@1  98.44 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.8276e-02 (1.5443e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.001)	Loss 1.8285e-02 (1.5432e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.107 ( 0.138)	Data  0.001 ( 0.001)	Loss 3.4892e-02 (1.5503e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.6720151901245117
## e[88]       loss.backward (sum) time: 13.86643123626709
## e[88]      optimizer.step (sum) time: 14.71121597290039
## epoch[88] training(only) time: 54.03865599632263
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.2017e+00 (1.2017e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.051 ( 0.062)	Loss 1.6195e+00 (1.3645e+00)	Acc@1  70.00 ( 73.27)	Acc@5  89.00 ( 90.73)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1105e+00 (1.2568e+00)	Acc@1  78.00 ( 74.38)	Acc@5  94.00 ( 92.29)
Test: [ 30/100]	Time  0.051 ( 0.053)	Loss 1.6248e+00 (1.2861e+00)	Acc@1  69.00 ( 73.71)	Acc@5  92.00 ( 91.94)
Test: [ 40/100]	Time  0.047 ( 0.052)	Loss 1.5534e+00 (1.2909e+00)	Acc@1  70.00 ( 73.44)	Acc@5  92.00 ( 92.32)
Test: [ 50/100]	Time  0.049 ( 0.051)	Loss 1.2292e+00 (1.2749e+00)	Acc@1  76.00 ( 73.69)	Acc@5  93.00 ( 92.29)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.1256e+00 (1.2605e+00)	Acc@1  75.00 ( 73.84)	Acc@5  97.00 ( 92.62)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2544e+00 (1.2705e+00)	Acc@1  75.00 ( 73.80)	Acc@5  94.00 ( 92.73)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.4379e+00 (1.2770e+00)	Acc@1  75.00 ( 73.70)	Acc@5  92.00 ( 92.65)
Test: [ 90/100]	Time  0.046 ( 0.050)	Loss 1.5851e+00 (1.2554e+00)	Acc@1  73.00 ( 73.87)	Acc@5  91.00 ( 92.86)
 * Acc@1 74.130 Acc@5 92.870
### epoch[88] execution time: 59.05957055091858
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.300 ( 0.300)	Data  0.153 ( 0.153)	Loss 9.8530e-03 (9.8530e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.137 ( 0.152)	Data  0.001 ( 0.015)	Loss 1.9475e-02 (1.3616e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.138 ( 0.145)	Data  0.001 ( 0.008)	Loss 2.0505e-02 (1.4252e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.136 ( 0.142)	Data  0.001 ( 0.006)	Loss 3.1670e-02 (1.4258e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.138 ( 0.141)	Data  0.001 ( 0.005)	Loss 1.9985e-02 (1.4472e-02)	Acc@1 100.00 ( 99.89)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.144 ( 0.141)	Data  0.001 ( 0.004)	Loss 1.2918e-02 (1.5206e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.147 ( 0.140)	Data  0.001 ( 0.004)	Loss 1.5797e-02 (1.5330e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.137 ( 0.140)	Data  0.001 ( 0.003)	Loss 8.3226e-03 (1.5323e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.0704e-02 (1.5325e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.151 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.4588e-02 (1.5093e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.135 ( 0.139)	Data  0.001 ( 0.003)	Loss 1.3277e-02 (1.5455e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.137 ( 0.139)	Data  0.001 ( 0.003)	Loss 2.0844e-02 (1.5785e-02)	Acc@1  99.22 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.134 ( 0.139)	Data  0.001 ( 0.002)	Loss 1.6131e-02 (1.5875e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4075e-02 (1.5762e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.9301e-02 (1.5848e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0317e-02 (1.5721e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6714e-02 (1.5763e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1189e-02 (1.5714e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5917e-02 (1.5737e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.1438e-02 (1.5872e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.2704e-02 (1.5980e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5879e-02 (1.5958e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.4656e-03 (1.6127e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.135 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.0436e-02 (1.6065e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 8.1958e-03 (1.6006e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2105e-02 (1.5907e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.137 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.6013e-02 (1.5988e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4102e-02 (1.5875e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 9.9029e-03 (1.5861e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.3608e-02 (1.5838e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.139 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0497e-02 (1.5926e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4214e-02 (1.5980e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.134 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0251e-02 (1.5936e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.141 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4734e-02 (1.5845e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.0414e-02 (1.5807e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.6131e-02 (1.5866e-02)	Acc@1  99.22 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.138 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.1465e-02 (1.5792e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.140 ( 0.138)	Data  0.001 ( 0.002)	Loss 2.2570e-02 (1.5793e-02)	Acc@1 100.00 ( 99.84)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.136 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.4971e-02 (1.5785e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.109 ( 0.138)	Data  0.001 ( 0.002)	Loss 1.5571e-02 (1.5771e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.6659822463989258
## e[89]       loss.backward (sum) time: 13.801196098327637
## e[89]      optimizer.step (sum) time: 14.713203430175781
## epoch[89] training(only) time: 53.95236349105835
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.2074e+00 (1.2074e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.065 ( 0.062)	Loss 1.5523e+00 (1.3523e+00)	Acc@1  71.00 ( 73.00)	Acc@5  90.00 ( 91.27)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.0975e+00 (1.2523e+00)	Acc@1  77.00 ( 74.05)	Acc@5  94.00 ( 92.43)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.6088e+00 (1.2851e+00)	Acc@1  67.00 ( 73.35)	Acc@5  92.00 ( 92.06)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.5299e+00 (1.2921e+00)	Acc@1  70.00 ( 73.27)	Acc@5  90.00 ( 92.41)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.2394e+00 (1.2751e+00)	Acc@1  76.00 ( 73.47)	Acc@5  92.00 ( 92.31)
Test: [ 60/100]	Time  0.055 ( 0.050)	Loss 1.1001e+00 (1.2604e+00)	Acc@1  75.00 ( 73.67)	Acc@5  96.00 ( 92.66)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.2646e+00 (1.2688e+00)	Acc@1  75.00 ( 73.61)	Acc@5  96.00 ( 92.80)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4158e+00 (1.2740e+00)	Acc@1  75.00 ( 73.60)	Acc@5  92.00 ( 92.69)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.5190e+00 (1.2530e+00)	Acc@1  70.00 ( 73.78)	Acc@5  92.00 ( 92.91)
 * Acc@1 74.040 Acc@5 92.950
### epoch[89] execution time: 58.94466781616211
### Training complete:
#### total training(only) time: 4864.72088265419
##### Total run time: 5318.898813962936
