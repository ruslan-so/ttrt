# Model: vgg11_bn
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.vgg
<function vgg11_bn at 0x7f8737ea70d0>
# model requested: 'vgg11_bn'
# printing out the model
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(inplace=True)
    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU(inplace=True)
    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (27): ReLU(inplace=True)
    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=512, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=100, bias=True)
  )
)
# model is full precision
# Model: vgg11_bn
# Dataset: cifarcentum
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.171 ( 3.171)	Data  0.103 ( 0.103)	Loss 4.6176e+00 (4.6176e+00)	Acc@1   1.56 (  1.56)	Acc@5   6.25 (  6.25)
Epoch: [0][ 10/391]	Time  0.032 ( 0.318)	Data  0.001 ( 0.011)	Loss 4.6250e+00 (4.7228e+00)	Acc@1   1.56 (  1.56)	Acc@5   6.25 (  6.11)
Epoch: [0][ 20/391]	Time  0.031 ( 0.182)	Data  0.001 ( 0.007)	Loss 4.6073e+00 (4.6731e+00)	Acc@1   0.00 (  1.38)	Acc@5   3.12 (  5.92)
Epoch: [0][ 30/391]	Time  0.032 ( 0.134)	Data  0.001 ( 0.005)	Loss 4.5204e+00 (4.6493e+00)	Acc@1   1.56 (  1.41)	Acc@5   7.03 (  6.02)
Epoch: [0][ 40/391]	Time  0.033 ( 0.109)	Data  0.001 ( 0.004)	Loss 4.6424e+00 (4.6371e+00)	Acc@1   0.78 (  1.26)	Acc@5   3.12 (  6.14)
Epoch: [0][ 50/391]	Time  0.032 ( 0.094)	Data  0.001 ( 0.004)	Loss 4.5469e+00 (4.6240e+00)	Acc@1   1.56 (  1.29)	Acc@5   7.81 (  6.45)
Epoch: [0][ 60/391]	Time  0.031 ( 0.084)	Data  0.001 ( 0.004)	Loss 4.5327e+00 (4.6153e+00)	Acc@1   0.78 (  1.34)	Acc@5  10.16 (  6.74)
Epoch: [0][ 70/391]	Time  0.033 ( 0.077)	Data  0.001 ( 0.003)	Loss 4.5790e+00 (4.6082e+00)	Acc@1   0.00 (  1.38)	Acc@5   7.81 (  6.84)
Epoch: [0][ 80/391]	Time  0.033 ( 0.071)	Data  0.001 ( 0.003)	Loss 4.4945e+00 (4.5977e+00)	Acc@1   1.56 (  1.44)	Acc@5   6.25 (  6.89)
Epoch: [0][ 90/391]	Time  0.032 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4720e+00 (4.5888e+00)	Acc@1   1.56 (  1.44)	Acc@5  10.16 (  6.98)
Epoch: [0][100/391]	Time  0.033 ( 0.064)	Data  0.001 ( 0.003)	Loss 4.4138e+00 (4.5802e+00)	Acc@1   1.56 (  1.52)	Acc@5  12.50 (  7.04)
Epoch: [0][110/391]	Time  0.034 ( 0.061)	Data  0.001 ( 0.003)	Loss 4.4077e+00 (4.5714e+00)	Acc@1   2.34 (  1.53)	Acc@5   9.38 (  7.28)
Epoch: [0][120/391]	Time  0.031 ( 0.058)	Data  0.001 ( 0.003)	Loss 4.4357e+00 (4.5657e+00)	Acc@1   0.78 (  1.53)	Acc@5  11.72 (  7.39)
Epoch: [0][130/391]	Time  0.032 ( 0.056)	Data  0.001 ( 0.003)	Loss 4.5056e+00 (4.5580e+00)	Acc@1   0.78 (  1.59)	Acc@5   4.69 (  7.60)
Epoch: [0][140/391]	Time  0.032 ( 0.055)	Data  0.001 ( 0.003)	Loss 4.4302e+00 (4.5486e+00)	Acc@1   3.91 (  1.64)	Acc@5   8.59 (  7.80)
Epoch: [0][150/391]	Time  0.033 ( 0.053)	Data  0.001 ( 0.003)	Loss 4.4467e+00 (4.5400e+00)	Acc@1   0.78 (  1.67)	Acc@5   7.81 (  8.02)
Epoch: [0][160/391]	Time  0.033 ( 0.052)	Data  0.001 ( 0.003)	Loss 4.4277e+00 (4.5333e+00)	Acc@1   2.34 (  1.68)	Acc@5  11.72 (  8.12)
Epoch: [0][170/391]	Time  0.032 ( 0.051)	Data  0.001 ( 0.003)	Loss 4.4557e+00 (4.5271e+00)	Acc@1   3.12 (  1.73)	Acc@5   8.59 (  8.26)
Epoch: [0][180/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.003)	Loss 4.3806e+00 (4.5199e+00)	Acc@1   4.69 (  1.78)	Acc@5  18.75 (  8.46)
Epoch: [0][190/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.003)	Loss 4.3577e+00 (4.5124e+00)	Acc@1   0.78 (  1.81)	Acc@5  10.16 (  8.68)
Epoch: [0][200/391]	Time  0.032 ( 0.048)	Data  0.001 ( 0.003)	Loss 4.3423e+00 (4.5070e+00)	Acc@1   6.25 (  1.87)	Acc@5  16.41 (  8.82)
Epoch: [0][210/391]	Time  0.033 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.3995e+00 (4.5011e+00)	Acc@1   1.56 (  1.85)	Acc@5  10.16 (  8.92)
Epoch: [0][220/391]	Time  0.033 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2773e+00 (4.4936e+00)	Acc@1   4.69 (  1.90)	Acc@5  14.84 (  9.07)
Epoch: [0][230/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.003)	Loss 4.3974e+00 (4.4873e+00)	Acc@1   0.00 (  1.94)	Acc@5   7.03 (  9.22)
Epoch: [0][240/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.002)	Loss 4.3039e+00 (4.4817e+00)	Acc@1   3.91 (  1.97)	Acc@5  12.50 (  9.33)
Epoch: [0][250/391]	Time  0.029 ( 0.045)	Data  0.001 ( 0.002)	Loss 4.3732e+00 (4.4769e+00)	Acc@1   3.91 (  2.01)	Acc@5   9.38 (  9.43)
Epoch: [0][260/391]	Time  0.033 ( 0.045)	Data  0.001 ( 0.002)	Loss 4.3267e+00 (4.4722e+00)	Acc@1   3.12 (  2.03)	Acc@5  14.84 (  9.51)
Epoch: [0][270/391]	Time  0.032 ( 0.044)	Data  0.001 ( 0.002)	Loss 4.3031e+00 (4.4658e+00)	Acc@1   1.56 (  2.07)	Acc@5  11.72 (  9.68)
Epoch: [0][280/391]	Time  0.031 ( 0.044)	Data  0.001 ( 0.002)	Loss 4.3837e+00 (4.4617e+00)	Acc@1   2.34 (  2.09)	Acc@5  14.06 (  9.81)
Epoch: [0][290/391]	Time  0.031 ( 0.043)	Data  0.001 ( 0.002)	Loss 4.3177e+00 (4.4570e+00)	Acc@1   3.12 (  2.12)	Acc@5  10.16 (  9.93)
Epoch: [0][300/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.002)	Loss 4.2839e+00 (4.4529e+00)	Acc@1   4.69 (  2.16)	Acc@5  15.62 ( 10.04)
Epoch: [0][310/391]	Time  0.032 ( 0.043)	Data  0.001 ( 0.002)	Loss 4.4114e+00 (4.4506e+00)	Acc@1   0.78 (  2.16)	Acc@5  14.06 ( 10.11)
Epoch: [0][320/391]	Time  0.032 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3762e+00 (4.4459e+00)	Acc@1   2.34 (  2.18)	Acc@5  11.72 ( 10.24)
Epoch: [0][330/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.3452e+00 (4.4425e+00)	Acc@1   5.47 (  2.21)	Acc@5  14.06 ( 10.34)
Epoch: [0][340/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2682e+00 (4.4374e+00)	Acc@1   4.69 (  2.26)	Acc@5  16.41 ( 10.50)
Epoch: [0][350/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.3606e+00 (4.4331e+00)	Acc@1   3.12 (  2.28)	Acc@5  14.06 ( 10.67)
Epoch: [0][360/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1957e+00 (4.4290e+00)	Acc@1   3.91 (  2.31)	Acc@5  16.41 ( 10.82)
Epoch: [0][370/391]	Time  0.032 ( 0.041)	Data  0.002 ( 0.002)	Loss 4.2421e+00 (4.4241e+00)	Acc@1   7.03 (  2.33)	Acc@5  16.41 ( 10.95)
Epoch: [0][380/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2134e+00 (4.4180e+00)	Acc@1   3.91 (  2.38)	Acc@5  15.62 ( 11.15)
Epoch: [0][390/391]	Time  0.319 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2857e+00 (4.4143e+00)	Acc@1   3.75 (  2.41)	Acc@5  22.50 ( 11.27)
## e[0] optimizer.zero_grad (sum) time: 0.1152791976928711
## e[0]       loss.backward (sum) time: 2.4437193870544434
## e[0]      optimizer.step (sum) time: 0.6965353488922119
## epoch[0] training(only) time: 16.194716453552246
# Switched to evaluate mode...
Test: [  0/100]	Time  0.249 ( 0.249)	Loss 4.1427e+00 (4.1427e+00)	Acc@1   1.00 (  1.00)	Acc@5  21.00 ( 21.00)
Test: [ 10/100]	Time  0.017 ( 0.040)	Loss 4.2111e+00 (4.1869e+00)	Acc@1   4.00 (  3.73)	Acc@5  14.00 ( 16.55)
Test: [ 20/100]	Time  0.017 ( 0.029)	Loss 4.1408e+00 (4.1873e+00)	Acc@1   0.00 (  3.67)	Acc@5  14.00 ( 16.86)
Test: [ 30/100]	Time  0.026 ( 0.025)	Loss 4.2566e+00 (4.1759e+00)	Acc@1   1.00 (  3.90)	Acc@5  13.00 ( 17.03)
Test: [ 40/100]	Time  0.017 ( 0.023)	Loss 4.2498e+00 (4.1703e+00)	Acc@1   1.00 (  4.27)	Acc@5  17.00 ( 17.51)
Test: [ 50/100]	Time  0.017 ( 0.022)	Loss 4.0464e+00 (4.1675e+00)	Acc@1   7.00 (  4.45)	Acc@5  22.00 ( 17.75)
Test: [ 60/100]	Time  0.017 ( 0.021)	Loss 4.2113e+00 (4.1674e+00)	Acc@1   6.00 (  4.38)	Acc@5  20.00 ( 17.85)
Test: [ 70/100]	Time  0.017 ( 0.021)	Loss 4.1961e+00 (4.1663e+00)	Acc@1   6.00 (  4.44)	Acc@5  14.00 ( 17.89)
Test: [ 80/100]	Time  0.017 ( 0.020)	Loss 4.2934e+00 (4.1696e+00)	Acc@1   0.00 (  4.47)	Acc@5   7.00 ( 17.80)
Test: [ 90/100]	Time  0.018 ( 0.020)	Loss 4.0661e+00 (4.1669e+00)	Acc@1   5.00 (  4.54)	Acc@5  19.00 ( 17.84)
 * Acc@1 4.570 Acc@5 17.850
### epoch[0] execution time: 18.2413272857666
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.191 ( 0.191)	Data  0.156 ( 0.156)	Loss 4.2802e+00 (4.2802e+00)	Acc@1   2.34 (  2.34)	Acc@5  17.19 ( 17.19)
Epoch: [1][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.016)	Loss 4.2358e+00 (4.2394e+00)	Acc@1   5.47 (  3.69)	Acc@5  17.97 ( 16.26)
Epoch: [1][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 4.1958e+00 (4.2357e+00)	Acc@1   3.12 (  3.46)	Acc@5  21.88 ( 16.89)
Epoch: [1][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 4.1637e+00 (4.2396e+00)	Acc@1   4.69 (  3.63)	Acc@5  19.53 ( 16.99)
Epoch: [1][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 4.1921e+00 (4.2306e+00)	Acc@1   3.91 (  3.73)	Acc@5  20.31 ( 17.44)
Epoch: [1][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 4.2982e+00 (4.2238e+00)	Acc@1   4.69 (  3.78)	Acc@5  17.97 ( 17.60)
Epoch: [1][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.2048e+00 (4.2115e+00)	Acc@1   4.69 (  4.14)	Acc@5  17.19 ( 18.21)
Epoch: [1][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.1192e+00 (4.2002e+00)	Acc@1   4.69 (  4.19)	Acc@5  15.62 ( 18.29)
Epoch: [1][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.1170e+00 (4.1983e+00)	Acc@1   9.38 (  4.27)	Acc@5  24.22 ( 18.38)
Epoch: [1][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.1461e+00 (4.1949e+00)	Acc@1   6.25 (  4.35)	Acc@5  17.97 ( 18.44)
Epoch: [1][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.1691e+00 (4.1906e+00)	Acc@1   7.81 (  4.40)	Acc@5  20.31 ( 18.46)
Epoch: [1][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.2423e+00 (4.1894e+00)	Acc@1   3.91 (  4.42)	Acc@5  17.19 ( 18.52)
Epoch: [1][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.1246e+00 (4.1817e+00)	Acc@1   7.81 (  4.49)	Acc@5  21.09 ( 18.86)
Epoch: [1][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.1281e+00 (4.1776e+00)	Acc@1   5.47 (  4.53)	Acc@5  19.53 ( 18.87)
Epoch: [1][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.2161e+00 (4.1713e+00)	Acc@1   3.91 (  4.55)	Acc@5  14.84 ( 19.00)
Epoch: [1][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1763e+00 (4.1655e+00)	Acc@1   3.91 (  4.66)	Acc@5  20.31 ( 19.22)
Epoch: [1][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0593e+00 (4.1599e+00)	Acc@1   3.91 (  4.77)	Acc@5  21.88 ( 19.41)
Epoch: [1][170/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1397e+00 (4.1552e+00)	Acc@1   3.91 (  4.83)	Acc@5  17.97 ( 19.58)
Epoch: [1][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1631e+00 (4.1532e+00)	Acc@1   3.91 (  4.82)	Acc@5  18.75 ( 19.63)
Epoch: [1][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1608e+00 (4.1507e+00)	Acc@1   4.69 (  4.86)	Acc@5  21.88 ( 19.80)
Epoch: [1][200/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9679e+00 (4.1460e+00)	Acc@1   5.47 (  4.90)	Acc@5  17.97 ( 19.92)
Epoch: [1][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8502e+00 (4.1415e+00)	Acc@1   8.59 (  4.92)	Acc@5  28.91 ( 19.99)
Epoch: [1][220/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.1970e+00 (4.1371e+00)	Acc@1   6.25 (  4.98)	Acc@5  21.09 ( 20.15)
Epoch: [1][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1106e+00 (4.1347e+00)	Acc@1   8.59 (  5.05)	Acc@5  16.41 ( 20.19)
Epoch: [1][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0474e+00 (4.1305e+00)	Acc@1   3.91 (  5.05)	Acc@5  25.78 ( 20.25)
Epoch: [1][250/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.1244e+00 (4.1290e+00)	Acc@1   7.81 (  5.04)	Acc@5  26.56 ( 20.32)
Epoch: [1][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0724e+00 (4.1250e+00)	Acc@1   8.59 (  5.11)	Acc@5  28.12 ( 20.45)
Epoch: [1][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9934e+00 (4.1218e+00)	Acc@1   7.81 (  5.15)	Acc@5  25.78 ( 20.61)
Epoch: [1][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0346e+00 (4.1186e+00)	Acc@1   3.91 (  5.19)	Acc@5  25.78 ( 20.74)
Epoch: [1][290/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1112e+00 (4.1154e+00)	Acc@1   5.47 (  5.19)	Acc@5  20.31 ( 20.83)
Epoch: [1][300/391]	Time  0.034 ( 0.033)	Data  0.002 ( 0.003)	Loss 3.9042e+00 (4.1128e+00)	Acc@1   6.25 (  5.23)	Acc@5  25.00 ( 20.88)
Epoch: [1][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9344e+00 (4.1111e+00)	Acc@1   7.03 (  5.24)	Acc@5  25.78 ( 20.88)
Epoch: [1][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9906e+00 (4.1095e+00)	Acc@1   4.69 (  5.26)	Acc@5  25.00 ( 20.94)
Epoch: [1][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9397e+00 (4.1062e+00)	Acc@1   5.47 (  5.27)	Acc@5  22.66 ( 21.04)
Epoch: [1][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0084e+00 (4.1023e+00)	Acc@1   7.81 (  5.34)	Acc@5  24.22 ( 21.17)
Epoch: [1][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.0225e+00 (4.0988e+00)	Acc@1   6.25 (  5.38)	Acc@5  21.88 ( 21.27)
Epoch: [1][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.9456e+00 (4.0971e+00)	Acc@1   3.91 (  5.39)	Acc@5  23.44 ( 21.34)
Epoch: [1][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.8037e+00 (4.0942e+00)	Acc@1   8.59 (  5.41)	Acc@5  32.03 ( 21.42)
Epoch: [1][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.9269e+00 (4.0912e+00)	Acc@1   4.69 (  5.46)	Acc@5  29.69 ( 21.57)
Epoch: [1][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.9318e+00 (4.0887e+00)	Acc@1  11.25 (  5.47)	Acc@5  27.50 ( 21.63)
## e[1] optimizer.zero_grad (sum) time: 0.11481738090515137
## e[1]       loss.backward (sum) time: 2.0630130767822266
## e[1]      optimizer.step (sum) time: 0.7364020347595215
## epoch[1] training(only) time: 12.947161197662354
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 3.8830e+00 (3.8830e+00)	Acc@1   5.00 (  5.00)	Acc@5  28.00 ( 28.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 3.7742e+00 (3.8569e+00)	Acc@1   9.00 (  7.45)	Acc@5  38.00 ( 29.18)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 3.9323e+00 (3.8779e+00)	Acc@1   8.00 (  8.00)	Acc@5  27.00 ( 28.48)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 3.9911e+00 (3.8656e+00)	Acc@1   8.00 (  8.19)	Acc@5  25.00 ( 28.68)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 3.8164e+00 (3.8571e+00)	Acc@1   7.00 (  8.07)	Acc@5  36.00 ( 29.12)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 3.9385e+00 (3.8540e+00)	Acc@1   5.00 (  8.20)	Acc@5  23.00 ( 29.10)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 3.7950e+00 (3.8529e+00)	Acc@1   9.00 (  8.36)	Acc@5  30.00 ( 28.98)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 3.9562e+00 (3.8517e+00)	Acc@1   2.00 (  8.21)	Acc@5  25.00 ( 29.06)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 3.9727e+00 (3.8590e+00)	Acc@1   8.00 (  8.27)	Acc@5  25.00 ( 28.84)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 3.7554e+00 (3.8505e+00)	Acc@1   7.00 (  8.27)	Acc@5  28.00 ( 28.92)
 * Acc@1 8.230 Acc@5 28.870
### epoch[1] execution time: 14.882769346237183
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.178 ( 0.178)	Data  0.152 ( 0.152)	Loss 3.8215e+00 (3.8215e+00)	Acc@1   6.25 (  6.25)	Acc@5  25.00 ( 25.00)
Epoch: [2][ 10/391]	Time  0.031 ( 0.045)	Data  0.001 ( 0.015)	Loss 3.8253e+00 (3.9134e+00)	Acc@1   7.81 (  6.11)	Acc@5  32.03 ( 26.49)
Epoch: [2][ 20/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.009)	Loss 3.8383e+00 (3.9386e+00)	Acc@1   6.25 (  6.58)	Acc@5  29.69 ( 25.71)
Epoch: [2][ 30/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.007)	Loss 3.9015e+00 (3.9336e+00)	Acc@1   9.38 (  7.06)	Acc@5  31.25 ( 26.23)
Epoch: [2][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 3.8984e+00 (3.9466e+00)	Acc@1   4.69 (  6.76)	Acc@5  27.34 ( 25.88)
Epoch: [2][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.7968e+00 (3.9518e+00)	Acc@1  10.94 (  6.94)	Acc@5  32.03 ( 25.95)
Epoch: [2][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.8071e+00 (3.9555e+00)	Acc@1   6.25 (  6.98)	Acc@5  27.34 ( 25.88)
Epoch: [2][ 70/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.8446e+00 (3.9487e+00)	Acc@1   7.03 (  7.01)	Acc@5  28.12 ( 25.88)
Epoch: [2][ 80/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.7443e+00 (3.9425e+00)	Acc@1  10.16 (  6.96)	Acc@5  28.12 ( 26.02)
Epoch: [2][ 90/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.004)	Loss 4.0443e+00 (3.9403e+00)	Acc@1   4.69 (  6.96)	Acc@5  25.00 ( 26.18)
Epoch: [2][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.8659e+00 (3.9304e+00)	Acc@1   7.81 (  7.06)	Acc@5  31.25 ( 26.60)
Epoch: [2][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.7658e+00 (3.9243e+00)	Acc@1  10.16 (  7.08)	Acc@5  29.69 ( 26.63)
Epoch: [2][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9504e+00 (3.9181e+00)	Acc@1  11.72 (  7.16)	Acc@5  18.75 ( 26.82)
Epoch: [2][130/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8449e+00 (3.9116e+00)	Acc@1   7.81 (  7.27)	Acc@5  32.81 ( 27.14)
Epoch: [2][140/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6439e+00 (3.9116e+00)	Acc@1  16.41 (  7.26)	Acc@5  36.72 ( 27.27)
Epoch: [2][150/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9687e+00 (3.9104e+00)	Acc@1   7.03 (  7.19)	Acc@5  31.25 ( 27.44)
Epoch: [2][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9103e+00 (3.9093e+00)	Acc@1  11.72 (  7.23)	Acc@5  29.69 ( 27.48)
Epoch: [2][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9834e+00 (3.9061e+00)	Acc@1   4.69 (  7.25)	Acc@5  25.00 ( 27.61)
Epoch: [2][180/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9876e+00 (3.9024e+00)	Acc@1   7.81 (  7.33)	Acc@5  27.34 ( 27.75)
Epoch: [2][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9403e+00 (3.9052e+00)	Acc@1   7.03 (  7.32)	Acc@5  27.34 ( 27.77)
Epoch: [2][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8960e+00 (3.9026e+00)	Acc@1   4.69 (  7.33)	Acc@5  25.00 ( 27.83)
Epoch: [2][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9226e+00 (3.9020e+00)	Acc@1   7.03 (  7.33)	Acc@5  30.47 ( 27.86)
Epoch: [2][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0215e+00 (3.9003e+00)	Acc@1   6.25 (  7.38)	Acc@5  21.88 ( 27.90)
Epoch: [2][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7280e+00 (3.8966e+00)	Acc@1   9.38 (  7.43)	Acc@5  31.25 ( 28.03)
Epoch: [2][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9924e+00 (3.8944e+00)	Acc@1  10.16 (  7.52)	Acc@5  30.47 ( 28.15)
Epoch: [2][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7883e+00 (3.8919e+00)	Acc@1  10.16 (  7.59)	Acc@5  30.47 ( 28.21)
Epoch: [2][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7785e+00 (3.8886e+00)	Acc@1   7.81 (  7.66)	Acc@5  32.03 ( 28.36)
Epoch: [2][270/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6876e+00 (3.8830e+00)	Acc@1  10.94 (  7.75)	Acc@5  35.94 ( 28.56)
Epoch: [2][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7110e+00 (3.8801e+00)	Acc@1  10.94 (  7.78)	Acc@5  38.28 ( 28.62)
Epoch: [2][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8063e+00 (3.8771e+00)	Acc@1   8.59 (  7.83)	Acc@5  28.91 ( 28.70)
Epoch: [2][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6948e+00 (3.8720e+00)	Acc@1   9.38 (  7.92)	Acc@5  32.81 ( 28.90)
Epoch: [2][310/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 3.6363e+00 (3.8708e+00)	Acc@1  10.94 (  7.91)	Acc@5  35.16 ( 28.92)
Epoch: [2][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6827e+00 (3.8666e+00)	Acc@1  11.72 (  7.95)	Acc@5  37.50 ( 29.02)
Epoch: [2][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6994e+00 (3.8644e+00)	Acc@1   7.81 (  7.97)	Acc@5  32.81 ( 29.08)
Epoch: [2][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6511e+00 (3.8603e+00)	Acc@1  11.72 (  8.03)	Acc@5  38.28 ( 29.24)
Epoch: [2][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.8647e+00 (3.8568e+00)	Acc@1   9.38 (  8.07)	Acc@5  30.47 ( 29.33)
Epoch: [2][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.7496e+00 (3.8524e+00)	Acc@1  11.72 (  8.15)	Acc@5  37.50 ( 29.53)
Epoch: [2][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6789e+00 (3.8477e+00)	Acc@1  12.50 (  8.20)	Acc@5  30.47 ( 29.68)
Epoch: [2][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6785e+00 (3.8452e+00)	Acc@1   7.03 (  8.23)	Acc@5  36.72 ( 29.82)
Epoch: [2][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.7945e+00 (3.8403e+00)	Acc@1  11.25 (  8.29)	Acc@5  35.00 ( 29.97)
## e[2] optimizer.zero_grad (sum) time: 0.11884307861328125
## e[2]       loss.backward (sum) time: 2.0826728343963623
## e[2]      optimizer.step (sum) time: 0.7237999439239502
## epoch[2] training(only) time: 12.953391075134277
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 3.6242e+00 (3.6242e+00)	Acc@1  17.00 ( 17.00)	Acc@5  38.00 ( 38.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 3.6688e+00 (3.6341e+00)	Acc@1  13.00 ( 11.36)	Acc@5  38.00 ( 37.27)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 3.7199e+00 (3.6681e+00)	Acc@1  13.00 ( 11.29)	Acc@5  31.00 ( 35.81)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 3.7628e+00 (3.6571e+00)	Acc@1  10.00 ( 11.00)	Acc@5  34.00 ( 35.87)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 3.8118e+00 (3.6571e+00)	Acc@1   8.00 ( 11.00)	Acc@5  37.00 ( 35.95)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 3.5406e+00 (3.6420e+00)	Acc@1  15.00 ( 11.51)	Acc@5  42.00 ( 36.41)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 3.5712e+00 (3.6241e+00)	Acc@1  11.00 ( 11.84)	Acc@5  45.00 ( 37.26)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 3.8514e+00 (3.6265e+00)	Acc@1   8.00 ( 11.54)	Acc@5  28.00 ( 37.10)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 3.7721e+00 (3.6327e+00)	Acc@1  13.00 ( 11.57)	Acc@5  32.00 ( 36.84)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 3.4140e+00 (3.6234e+00)	Acc@1  18.00 ( 11.85)	Acc@5  46.00 ( 37.09)
 * Acc@1 11.830 Acc@5 37.050
### epoch[2] execution time: 14.876781702041626
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.172 ( 0.172)	Data  0.143 ( 0.143)	Loss 3.7388e+00 (3.7388e+00)	Acc@1  13.28 ( 13.28)	Acc@5  35.16 ( 35.16)
Epoch: [3][ 10/391]	Time  0.034 ( 0.045)	Data  0.001 ( 0.015)	Loss 3.5833e+00 (3.7137e+00)	Acc@1  14.84 ( 10.65)	Acc@5  39.06 ( 32.46)
Epoch: [3][ 20/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.009)	Loss 3.8937e+00 (3.7303e+00)	Acc@1  10.16 ( 10.45)	Acc@5  35.16 ( 33.22)
Epoch: [3][ 30/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.006)	Loss 3.7422e+00 (3.7288e+00)	Acc@1   7.03 ( 10.36)	Acc@5  26.56 ( 33.27)
Epoch: [3][ 40/391]	Time  0.032 ( 0.036)	Data  0.002 ( 0.005)	Loss 3.4946e+00 (3.7045e+00)	Acc@1  13.28 ( 10.63)	Acc@5  35.16 ( 33.92)
Epoch: [3][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.7476e+00 (3.6983e+00)	Acc@1   8.59 ( 10.66)	Acc@5  30.47 ( 34.16)
Epoch: [3][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.6809e+00 (3.6895e+00)	Acc@1  12.50 ( 10.86)	Acc@5  33.59 ( 34.35)
Epoch: [3][ 70/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.4708e+00 (3.6814e+00)	Acc@1  16.41 ( 10.97)	Acc@5  40.62 ( 34.56)
Epoch: [3][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.6196e+00 (3.6766e+00)	Acc@1  10.16 ( 10.97)	Acc@5  32.03 ( 34.75)
Epoch: [3][ 90/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.5674e+00 (3.6704e+00)	Acc@1   7.03 ( 10.93)	Acc@5  39.06 ( 35.05)
Epoch: [3][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6047e+00 (3.6639e+00)	Acc@1  13.28 ( 11.08)	Acc@5  35.94 ( 35.23)
Epoch: [3][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6688e+00 (3.6530e+00)	Acc@1  14.06 ( 11.25)	Acc@5  37.50 ( 35.67)
Epoch: [3][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5645e+00 (3.6457e+00)	Acc@1  11.72 ( 11.36)	Acc@5  40.62 ( 35.87)
Epoch: [3][130/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4700e+00 (3.6405e+00)	Acc@1  13.28 ( 11.49)	Acc@5  47.66 ( 36.13)
Epoch: [3][140/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 3.4797e+00 (3.6376e+00)	Acc@1  20.31 ( 11.67)	Acc@5  43.75 ( 36.32)
Epoch: [3][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6122e+00 (3.6369e+00)	Acc@1  14.06 ( 11.67)	Acc@5  37.50 ( 36.41)
Epoch: [3][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4354e+00 (3.6334e+00)	Acc@1  10.94 ( 11.67)	Acc@5  42.19 ( 36.55)
Epoch: [3][170/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4565e+00 (3.6317e+00)	Acc@1  17.97 ( 11.75)	Acc@5  45.31 ( 36.75)
Epoch: [3][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6754e+00 (3.6325e+00)	Acc@1   9.38 ( 11.67)	Acc@5  36.72 ( 36.77)
Epoch: [3][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8161e+00 (3.6290e+00)	Acc@1  10.16 ( 11.81)	Acc@5  35.94 ( 36.94)
Epoch: [3][200/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5047e+00 (3.6271e+00)	Acc@1  10.94 ( 11.81)	Acc@5  40.62 ( 37.00)
Epoch: [3][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6148e+00 (3.6255e+00)	Acc@1  13.28 ( 11.90)	Acc@5  34.38 ( 37.00)
Epoch: [3][220/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 3.5438e+00 (3.6248e+00)	Acc@1  16.41 ( 11.96)	Acc@5  39.84 ( 37.05)
Epoch: [3][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5398e+00 (3.6220e+00)	Acc@1  14.06 ( 12.02)	Acc@5  39.84 ( 37.17)
Epoch: [3][240/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5493e+00 (3.6167e+00)	Acc@1   9.38 ( 12.04)	Acc@5  38.28 ( 37.27)
Epoch: [3][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4662e+00 (3.6113e+00)	Acc@1  12.50 ( 12.08)	Acc@5  45.31 ( 37.44)
Epoch: [3][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5758e+00 (3.6054e+00)	Acc@1  17.97 ( 12.22)	Acc@5  39.06 ( 37.64)
Epoch: [3][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6534e+00 (3.6028e+00)	Acc@1  14.06 ( 12.30)	Acc@5  38.28 ( 37.75)
Epoch: [3][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4484e+00 (3.5996e+00)	Acc@1  16.41 ( 12.36)	Acc@5  45.31 ( 37.89)
Epoch: [3][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5273e+00 (3.5942e+00)	Acc@1  10.94 ( 12.44)	Acc@5  39.84 ( 38.05)
Epoch: [3][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.5250e+00 (3.5873e+00)	Acc@1  12.50 ( 12.56)	Acc@5  41.41 ( 38.27)
Epoch: [3][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.3785e+00 (3.5845e+00)	Acc@1  13.28 ( 12.61)	Acc@5  42.97 ( 38.35)
Epoch: [3][320/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.4968e+00 (3.5814e+00)	Acc@1  15.62 ( 12.66)	Acc@5  40.62 ( 38.43)
Epoch: [3][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6388e+00 (3.5792e+00)	Acc@1  14.06 ( 12.73)	Acc@5  37.50 ( 38.48)
Epoch: [3][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.3133e+00 (3.5750e+00)	Acc@1  21.09 ( 12.83)	Acc@5  46.88 ( 38.63)
Epoch: [3][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.4864e+00 (3.5710e+00)	Acc@1  14.06 ( 12.91)	Acc@5  44.53 ( 38.78)
Epoch: [3][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.5223e+00 (3.5697e+00)	Acc@1  11.72 ( 12.92)	Acc@5  42.97 ( 38.85)
Epoch: [3][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.4605e+00 (3.5659e+00)	Acc@1  15.62 ( 13.01)	Acc@5  39.84 ( 38.98)
Epoch: [3][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.3180e+00 (3.5615e+00)	Acc@1  17.19 ( 13.11)	Acc@5  46.88 ( 39.13)
Epoch: [3][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.0905e+00 (3.5583e+00)	Acc@1  22.50 ( 13.20)	Acc@5  50.00 ( 39.23)
## e[3] optimizer.zero_grad (sum) time: 0.11525344848632812
## e[3]       loss.backward (sum) time: 2.101522207260132
## e[3]      optimizer.step (sum) time: 0.7260808944702148
## epoch[3] training(only) time: 12.93393850326538
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 3.5283e+00 (3.5283e+00)	Acc@1  15.00 ( 15.00)	Acc@5  41.00 ( 41.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 3.6788e+00 (3.5747e+00)	Acc@1  12.00 ( 12.64)	Acc@5  36.00 ( 39.91)
Test: [ 20/100]	Time  0.023 ( 0.024)	Loss 3.4269e+00 (3.5815e+00)	Acc@1  15.00 ( 12.90)	Acc@5  48.00 ( 39.24)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 3.5661e+00 (3.5520e+00)	Acc@1  13.00 ( 13.19)	Acc@5  38.00 ( 40.26)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 3.5071e+00 (3.5465e+00)	Acc@1  15.00 ( 13.44)	Acc@5  45.00 ( 40.59)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 3.5474e+00 (3.5399e+00)	Acc@1  17.00 ( 13.33)	Acc@5  41.00 ( 40.55)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 3.7009e+00 (3.5295e+00)	Acc@1  13.00 ( 13.75)	Acc@5  39.00 ( 41.15)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 3.7273e+00 (3.5288e+00)	Acc@1  14.00 ( 13.82)	Acc@5  38.00 ( 41.17)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 3.7003e+00 (3.5326e+00)	Acc@1  11.00 ( 13.75)	Acc@5  37.00 ( 41.17)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 3.4149e+00 (3.5282e+00)	Acc@1  18.00 ( 13.84)	Acc@5  40.00 ( 41.29)
 * Acc@1 13.780 Acc@5 41.300
### epoch[3] execution time: 14.878051996231079
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.184 ( 0.184)	Data  0.147 ( 0.147)	Loss 3.3702e+00 (3.3702e+00)	Acc@1  18.75 ( 18.75)	Acc@5  40.62 ( 40.62)
Epoch: [4][ 10/391]	Time  0.031 ( 0.046)	Data  0.001 ( 0.015)	Loss 3.4251e+00 (3.4259e+00)	Acc@1  15.62 ( 15.55)	Acc@5  46.09 ( 44.25)
Epoch: [4][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 3.3444e+00 (3.4158e+00)	Acc@1  16.41 ( 16.18)	Acc@5  44.53 ( 43.86)
Epoch: [4][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 3.5225e+00 (3.3981e+00)	Acc@1  13.28 ( 16.00)	Acc@5  44.53 ( 44.35)
Epoch: [4][ 40/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.2383e+00 (3.3922e+00)	Acc@1  17.19 ( 16.23)	Acc@5  46.09 ( 44.44)
Epoch: [4][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.3990e+00 (3.3873e+00)	Acc@1  14.84 ( 16.18)	Acc@5  46.09 ( 44.30)
Epoch: [4][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.3795e+00 (3.3917e+00)	Acc@1  14.06 ( 16.11)	Acc@5  45.31 ( 44.12)
Epoch: [4][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.2383e+00 (3.3997e+00)	Acc@1  19.53 ( 15.86)	Acc@5  49.22 ( 43.85)
Epoch: [4][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.4365e+00 (3.3875e+00)	Acc@1  14.84 ( 16.02)	Acc@5  45.31 ( 44.22)
Epoch: [4][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.5241e+00 (3.3872e+00)	Acc@1  12.50 ( 15.93)	Acc@5  36.72 ( 44.23)
Epoch: [4][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2995e+00 (3.3891e+00)	Acc@1  13.28 ( 15.93)	Acc@5  43.75 ( 44.21)
Epoch: [4][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.2286e+00 (3.3841e+00)	Acc@1  16.41 ( 15.93)	Acc@5  54.69 ( 44.48)
Epoch: [4][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4602e+00 (3.3879e+00)	Acc@1  14.06 ( 15.88)	Acc@5  40.62 ( 44.43)
Epoch: [4][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1685e+00 (3.3785e+00)	Acc@1  17.19 ( 15.98)	Acc@5  52.34 ( 44.69)
Epoch: [4][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3532e+00 (3.3765e+00)	Acc@1  18.75 ( 16.04)	Acc@5  46.09 ( 44.83)
Epoch: [4][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3377e+00 (3.3757e+00)	Acc@1  18.75 ( 16.11)	Acc@5  53.12 ( 44.93)
Epoch: [4][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3047e+00 (3.3720e+00)	Acc@1  21.09 ( 16.17)	Acc@5  46.09 ( 44.96)
Epoch: [4][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1609e+00 (3.3677e+00)	Acc@1  17.19 ( 16.24)	Acc@5  57.03 ( 45.15)
Epoch: [4][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2195e+00 (3.3670e+00)	Acc@1  20.31 ( 16.24)	Acc@5  46.88 ( 45.11)
Epoch: [4][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3852e+00 (3.3644e+00)	Acc@1  14.84 ( 16.20)	Acc@5  47.66 ( 45.21)
Epoch: [4][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5605e+00 (3.3656e+00)	Acc@1  14.84 ( 16.22)	Acc@5  40.62 ( 45.16)
Epoch: [4][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3914e+00 (3.3656e+00)	Acc@1  14.84 ( 16.19)	Acc@5  47.66 ( 45.12)
Epoch: [4][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2918e+00 (3.3634e+00)	Acc@1  18.75 ( 16.23)	Acc@5  48.44 ( 45.18)
Epoch: [4][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3104e+00 (3.3592e+00)	Acc@1  17.97 ( 16.32)	Acc@5  46.88 ( 45.26)
Epoch: [4][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4912e+00 (3.3594e+00)	Acc@1  20.31 ( 16.31)	Acc@5  45.31 ( 45.26)
Epoch: [4][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0820e+00 (3.3537e+00)	Acc@1  19.53 ( 16.44)	Acc@5  57.81 ( 45.39)
Epoch: [4][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4061e+00 (3.3504e+00)	Acc@1  17.19 ( 16.47)	Acc@5  44.53 ( 45.51)
Epoch: [4][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3152e+00 (3.3506e+00)	Acc@1  18.75 ( 16.47)	Acc@5  43.75 ( 45.51)
Epoch: [4][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2024e+00 (3.3461e+00)	Acc@1  18.75 ( 16.58)	Acc@5  47.66 ( 45.65)
Epoch: [4][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2871e+00 (3.3425e+00)	Acc@1  18.75 ( 16.65)	Acc@5  45.31 ( 45.74)
Epoch: [4][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.5467e+00 (3.3396e+00)	Acc@1  13.28 ( 16.65)	Acc@5  35.94 ( 45.80)
Epoch: [4][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.4348e+00 (3.3385e+00)	Acc@1  17.97 ( 16.70)	Acc@5  41.41 ( 45.79)
Epoch: [4][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2078e+00 (3.3371e+00)	Acc@1  14.06 ( 16.73)	Acc@5  46.09 ( 45.86)
Epoch: [4][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2889e+00 (3.3371e+00)	Acc@1  15.62 ( 16.70)	Acc@5  46.88 ( 45.87)
Epoch: [4][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2345e+00 (3.3357e+00)	Acc@1  23.44 ( 16.77)	Acc@5  47.66 ( 45.92)
Epoch: [4][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2069e+00 (3.3315e+00)	Acc@1  14.84 ( 16.84)	Acc@5  53.91 ( 46.03)
Epoch: [4][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.1881e+00 (3.3297e+00)	Acc@1  19.53 ( 16.85)	Acc@5  44.53 ( 46.07)
Epoch: [4][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.0484e+00 (3.3258e+00)	Acc@1  21.88 ( 16.95)	Acc@5  60.16 ( 46.21)
Epoch: [4][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.1972e+00 (3.3232e+00)	Acc@1  20.31 ( 16.99)	Acc@5  46.88 ( 46.26)
Epoch: [4][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2460e+00 (3.3199e+00)	Acc@1  21.25 ( 17.00)	Acc@5  56.25 ( 46.40)
## e[4] optimizer.zero_grad (sum) time: 0.11748838424682617
## e[4]       loss.backward (sum) time: 2.077422618865967
## e[4]      optimizer.step (sum) time: 0.7203729152679443
## epoch[4] training(only) time: 12.961115837097168
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 3.3488e+00 (3.3488e+00)	Acc@1  25.00 ( 25.00)	Acc@5  44.00 ( 44.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 3.4130e+00 (3.3391e+00)	Acc@1  15.00 ( 18.36)	Acc@5  47.00 ( 47.55)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 3.2695e+00 (3.3255e+00)	Acc@1  18.00 ( 18.24)	Acc@5  51.00 ( 48.14)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 3.4195e+00 (3.3142e+00)	Acc@1  13.00 ( 17.84)	Acc@5  40.00 ( 48.42)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 3.2824e+00 (3.3252e+00)	Acc@1  18.00 ( 17.56)	Acc@5  51.00 ( 47.85)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 3.2373e+00 (3.3332e+00)	Acc@1  21.00 ( 17.65)	Acc@5  47.00 ( 47.39)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 3.3853e+00 (3.3303e+00)	Acc@1  16.00 ( 17.72)	Acc@5  44.00 ( 47.23)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 3.5655e+00 (3.3419e+00)	Acc@1  13.00 ( 17.66)	Acc@5  43.00 ( 46.94)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 3.2824e+00 (3.3435e+00)	Acc@1  13.00 ( 17.36)	Acc@5  50.00 ( 46.89)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 3.2439e+00 (3.3417e+00)	Acc@1  21.00 ( 17.40)	Acc@5  48.00 ( 47.03)
 * Acc@1 17.320 Acc@5 46.760
### epoch[4] execution time: 14.895403861999512
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.178 ( 0.178)	Data  0.150 ( 0.150)	Loss 2.8045e+00 (2.8045e+00)	Acc@1  29.69 ( 29.69)	Acc@5  60.94 ( 60.94)
Epoch: [5][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 3.2097e+00 (3.1242e+00)	Acc@1  15.62 ( 19.89)	Acc@5  46.09 ( 51.78)
Epoch: [5][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 3.0924e+00 (3.1652e+00)	Acc@1  21.09 ( 19.12)	Acc@5  52.34 ( 51.08)
Epoch: [5][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 3.1460e+00 (3.1582e+00)	Acc@1  20.31 ( 19.48)	Acc@5  53.12 ( 50.93)
Epoch: [5][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.2731e+00 (3.1439e+00)	Acc@1  23.44 ( 19.93)	Acc@5  46.09 ( 50.93)
Epoch: [5][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.0766e+00 (3.1415e+00)	Acc@1  18.75 ( 19.93)	Acc@5  54.69 ( 51.27)
Epoch: [5][ 60/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.8993e+00 (3.1430e+00)	Acc@1  23.44 ( 19.85)	Acc@5  64.84 ( 51.42)
Epoch: [5][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.0109e+00 (3.1343e+00)	Acc@1  24.22 ( 20.08)	Acc@5  57.03 ( 51.73)
Epoch: [5][ 80/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.004)	Loss 3.2192e+00 (3.1383e+00)	Acc@1  16.41 ( 19.94)	Acc@5  50.00 ( 51.71)
Epoch: [5][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.1409e+00 (3.1365e+00)	Acc@1  14.06 ( 19.79)	Acc@5  55.47 ( 51.87)
Epoch: [5][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0377e+00 (3.1314e+00)	Acc@1  20.31 ( 19.77)	Acc@5  56.25 ( 52.10)
Epoch: [5][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1068e+00 (3.1349e+00)	Acc@1  18.75 ( 19.81)	Acc@5  53.91 ( 52.07)
Epoch: [5][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0429e+00 (3.1339e+00)	Acc@1  16.41 ( 19.71)	Acc@5  55.47 ( 52.10)
Epoch: [5][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7463e+00 (3.1313e+00)	Acc@1  25.78 ( 19.89)	Acc@5  61.72 ( 52.20)
Epoch: [5][140/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3060e+00 (3.1312e+00)	Acc@1  18.75 ( 19.88)	Acc@5  49.22 ( 52.19)
Epoch: [5][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8335e+00 (3.1335e+00)	Acc@1  30.47 ( 19.83)	Acc@5  61.72 ( 52.20)
Epoch: [5][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1601e+00 (3.1289e+00)	Acc@1  22.66 ( 20.03)	Acc@5  47.66 ( 52.21)
Epoch: [5][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1882e+00 (3.1315e+00)	Acc@1  24.22 ( 20.03)	Acc@5  55.47 ( 52.19)
Epoch: [5][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9959e+00 (3.1304e+00)	Acc@1  17.97 ( 19.98)	Acc@5  58.59 ( 52.24)
Epoch: [5][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1622e+00 (3.1325e+00)	Acc@1  16.41 ( 20.02)	Acc@5  48.44 ( 52.20)
Epoch: [5][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0265e+00 (3.1312e+00)	Acc@1  21.09 ( 20.07)	Acc@5  56.25 ( 52.25)
Epoch: [5][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2397e+00 (3.1293e+00)	Acc@1  17.97 ( 20.19)	Acc@5  50.00 ( 52.25)
Epoch: [5][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1142e+00 (3.1269e+00)	Acc@1  23.44 ( 20.27)	Acc@5  46.88 ( 52.34)
Epoch: [5][230/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0362e+00 (3.1254e+00)	Acc@1  25.78 ( 20.37)	Acc@5  47.66 ( 52.37)
Epoch: [5][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0735e+00 (3.1206e+00)	Acc@1  21.88 ( 20.52)	Acc@5  51.56 ( 52.50)
Epoch: [5][250/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1070e+00 (3.1147e+00)	Acc@1  20.31 ( 20.66)	Acc@5  53.12 ( 52.65)
Epoch: [5][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2842e+00 (3.1133e+00)	Acc@1  28.12 ( 20.68)	Acc@5  53.12 ( 52.69)
Epoch: [5][270/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9590e+00 (3.1102e+00)	Acc@1  24.22 ( 20.71)	Acc@5  54.69 ( 52.76)
Epoch: [5][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7811e+00 (3.1071e+00)	Acc@1  32.03 ( 20.84)	Acc@5  60.94 ( 52.83)
Epoch: [5][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9676e+00 (3.1031e+00)	Acc@1  25.78 ( 20.90)	Acc@5  53.12 ( 52.88)
Epoch: [5][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0462e+00 (3.1028e+00)	Acc@1  26.56 ( 20.94)	Acc@5  56.25 ( 52.87)
Epoch: [5][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2160e+00 (3.1014e+00)	Acc@1  14.84 ( 20.95)	Acc@5  49.22 ( 52.90)
Epoch: [5][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1437e+00 (3.1039e+00)	Acc@1  16.41 ( 20.93)	Acc@5  51.56 ( 52.82)
Epoch: [5][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9131e+00 (3.0992e+00)	Acc@1  29.69 ( 21.02)	Acc@5  60.94 ( 52.96)
Epoch: [5][340/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9738e+00 (3.0961e+00)	Acc@1  26.56 ( 21.05)	Acc@5  57.03 ( 53.09)
Epoch: [5][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8105e+00 (3.0908e+00)	Acc@1  33.59 ( 21.18)	Acc@5  63.28 ( 53.20)
Epoch: [5][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.1483e+00 (3.0885e+00)	Acc@1  17.19 ( 21.25)	Acc@5  54.69 ( 53.28)
Epoch: [5][370/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6940e+00 (3.0850e+00)	Acc@1  28.91 ( 21.33)	Acc@5  64.84 ( 53.33)
Epoch: [5][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9250e+00 (3.0821e+00)	Acc@1  26.56 ( 21.41)	Acc@5  53.91 ( 53.41)
Epoch: [5][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9797e+00 (3.0787e+00)	Acc@1  21.25 ( 21.43)	Acc@5  56.25 ( 53.53)
## e[5] optimizer.zero_grad (sum) time: 0.11496996879577637
## e[5]       loss.backward (sum) time: 2.033196449279785
## e[5]      optimizer.step (sum) time: 0.7219352722167969
## epoch[5] training(only) time: 12.974026679992676
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 2.9443e+00 (2.9443e+00)	Acc@1  24.00 ( 24.00)	Acc@5  54.00 ( 54.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.8598e+00 (2.8477e+00)	Acc@1  25.00 ( 25.45)	Acc@5  47.00 ( 59.09)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 2.8328e+00 (2.8446e+00)	Acc@1  26.00 ( 24.67)	Acc@5  63.00 ( 59.10)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 2.9856e+00 (2.8634e+00)	Acc@1  26.00 ( 24.61)	Acc@5  60.00 ( 58.45)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 3.0590e+00 (2.8716e+00)	Acc@1  27.00 ( 24.68)	Acc@5  55.00 ( 58.27)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 2.6734e+00 (2.8736e+00)	Acc@1  32.00 ( 24.90)	Acc@5  65.00 ( 58.14)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.7300e+00 (2.8592e+00)	Acc@1  25.00 ( 25.20)	Acc@5  61.00 ( 58.18)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 3.1017e+00 (2.8667e+00)	Acc@1  18.00 ( 25.17)	Acc@5  51.00 ( 57.96)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.9484e+00 (2.8731e+00)	Acc@1  21.00 ( 24.89)	Acc@5  62.00 ( 58.04)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.7357e+00 (2.8700e+00)	Acc@1  33.00 ( 25.05)	Acc@5  62.00 ( 58.21)
 * Acc@1 24.900 Acc@5 58.100
### epoch[5] execution time: 14.91453766822815
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.180 ( 0.180)	Data  0.152 ( 0.152)	Loss 3.1239e+00 (3.1239e+00)	Acc@1  21.88 ( 21.88)	Acc@5  50.78 ( 50.78)
Epoch: [6][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 3.0425e+00 (2.9563e+00)	Acc@1  18.75 ( 23.72)	Acc@5  57.03 ( 57.67)
Epoch: [6][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 3.0772e+00 (2.9572e+00)	Acc@1  19.53 ( 22.95)	Acc@5  53.91 ( 57.59)
Epoch: [6][ 30/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.9121e+00 (2.9587e+00)	Acc@1  21.88 ( 23.41)	Acc@5  62.50 ( 57.28)
Epoch: [6][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 3.0702e+00 (2.9584e+00)	Acc@1  19.53 ( 23.06)	Acc@5  51.56 ( 57.05)
Epoch: [6][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.8396e+00 (2.9310e+00)	Acc@1  28.12 ( 23.87)	Acc@5  63.28 ( 57.69)
Epoch: [6][ 60/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.9520e+00 (2.9244e+00)	Acc@1  24.22 ( 24.05)	Acc@5  53.12 ( 57.76)
Epoch: [6][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.7975e+00 (2.9214e+00)	Acc@1  21.88 ( 23.95)	Acc@5  58.59 ( 57.78)
Epoch: [6][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.9911e+00 (2.9206e+00)	Acc@1  19.53 ( 24.11)	Acc@5  53.12 ( 57.62)
Epoch: [6][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.9313e+00 (2.9262e+00)	Acc@1  25.00 ( 24.05)	Acc@5  62.50 ( 57.70)
Epoch: [6][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9995e+00 (2.9265e+00)	Acc@1  26.56 ( 24.09)	Acc@5  56.25 ( 57.70)
Epoch: [6][110/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7685e+00 (2.9264e+00)	Acc@1  29.69 ( 24.26)	Acc@5  56.25 ( 57.77)
Epoch: [6][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7764e+00 (2.9278e+00)	Acc@1  26.56 ( 24.41)	Acc@5  59.38 ( 57.79)
Epoch: [6][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9288e+00 (2.9246e+00)	Acc@1  30.47 ( 24.47)	Acc@5  57.03 ( 57.78)
Epoch: [6][140/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0470e+00 (2.9308e+00)	Acc@1  28.12 ( 24.31)	Acc@5  57.03 ( 57.66)
Epoch: [6][150/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.7993e+00 (2.9271e+00)	Acc@1  32.03 ( 24.48)	Acc@5  66.41 ( 57.79)
Epoch: [6][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0562e+00 (2.9327e+00)	Acc@1  20.31 ( 24.31)	Acc@5  57.03 ( 57.70)
Epoch: [6][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8879e+00 (2.9320e+00)	Acc@1  25.00 ( 24.31)	Acc@5  56.25 ( 57.70)
Epoch: [6][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9300e+00 (2.9311e+00)	Acc@1  19.53 ( 24.37)	Acc@5  54.69 ( 57.74)
Epoch: [6][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6621e+00 (2.9245e+00)	Acc@1  26.56 ( 24.55)	Acc@5  67.19 ( 57.89)
Epoch: [6][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6153e+00 (2.9209e+00)	Acc@1  23.44 ( 24.66)	Acc@5  67.19 ( 58.00)
Epoch: [6][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6769e+00 (2.9163e+00)	Acc@1  31.25 ( 24.78)	Acc@5  61.72 ( 58.07)
Epoch: [6][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8689e+00 (2.9104e+00)	Acc@1  21.88 ( 24.87)	Acc@5  59.38 ( 58.23)
Epoch: [6][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1304e+00 (2.9100e+00)	Acc@1  18.75 ( 24.85)	Acc@5  53.12 ( 58.22)
Epoch: [6][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0169e+00 (2.9106e+00)	Acc@1  23.44 ( 24.86)	Acc@5  59.38 ( 58.16)
Epoch: [6][250/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9860e+00 (2.9108e+00)	Acc@1  24.22 ( 24.85)	Acc@5  58.59 ( 58.19)
Epoch: [6][260/391]	Time  0.034 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.6160e+00 (2.9067e+00)	Acc@1  30.47 ( 24.98)	Acc@5  64.06 ( 58.25)
Epoch: [6][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6491e+00 (2.9062e+00)	Acc@1  28.91 ( 24.96)	Acc@5  69.53 ( 58.29)
Epoch: [6][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9430e+00 (2.9018e+00)	Acc@1  21.09 ( 25.06)	Acc@5  59.38 ( 58.40)
Epoch: [6][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7433e+00 (2.8990e+00)	Acc@1  24.22 ( 25.12)	Acc@5  58.59 ( 58.41)
Epoch: [6][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7968e+00 (2.8936e+00)	Acc@1  34.38 ( 25.27)	Acc@5  58.59 ( 58.52)
Epoch: [6][310/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6448e+00 (2.8917e+00)	Acc@1  34.38 ( 25.29)	Acc@5  61.72 ( 58.56)
Epoch: [6][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6246e+00 (2.8890e+00)	Acc@1  33.59 ( 25.37)	Acc@5  65.62 ( 58.63)
Epoch: [6][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9874e+00 (2.8854e+00)	Acc@1  21.88 ( 25.45)	Acc@5  57.81 ( 58.72)
Epoch: [6][340/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9051e+00 (2.8844e+00)	Acc@1  27.34 ( 25.51)	Acc@5  57.81 ( 58.73)
Epoch: [6][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8941e+00 (2.8828e+00)	Acc@1  28.91 ( 25.56)	Acc@5  60.94 ( 58.75)
Epoch: [6][360/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8661e+00 (2.8806e+00)	Acc@1  30.47 ( 25.61)	Acc@5  63.28 ( 58.85)
Epoch: [6][370/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.0493e+00 (2.8806e+00)	Acc@1  26.56 ( 25.61)	Acc@5  56.25 ( 58.85)
Epoch: [6][380/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.7768e+00 (2.8770e+00)	Acc@1  32.03 ( 25.67)	Acc@5  63.28 ( 58.97)
Epoch: [6][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8869e+00 (2.8752e+00)	Acc@1  23.75 ( 25.72)	Acc@5  56.25 ( 59.06)
## e[6] optimizer.zero_grad (sum) time: 0.11560940742492676
## e[6]       loss.backward (sum) time: 2.0850000381469727
## e[6]      optimizer.step (sum) time: 0.732811450958252
## epoch[6] training(only) time: 13.015260457992554
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 2.8130e+00 (2.8130e+00)	Acc@1  27.00 ( 27.00)	Acc@5  58.00 ( 58.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 2.8888e+00 (2.7477e+00)	Acc@1  17.00 ( 27.82)	Acc@5  59.00 ( 62.82)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 2.5668e+00 (2.7253e+00)	Acc@1  29.00 ( 28.62)	Acc@5  65.00 ( 62.38)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 2.9307e+00 (2.7293e+00)	Acc@1  28.00 ( 28.77)	Acc@5  55.00 ( 62.35)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 2.8197e+00 (2.7256e+00)	Acc@1  30.00 ( 28.61)	Acc@5  61.00 ( 62.61)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 2.6356e+00 (2.7461e+00)	Acc@1  34.00 ( 28.41)	Acc@5  60.00 ( 62.20)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.5483e+00 (2.7405e+00)	Acc@1  35.00 ( 28.20)	Acc@5  65.00 ( 62.15)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 3.0905e+00 (2.7539e+00)	Acc@1  24.00 ( 27.85)	Acc@5  58.00 ( 62.01)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.9735e+00 (2.7640e+00)	Acc@1  24.00 ( 27.75)	Acc@5  63.00 ( 62.01)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 2.5969e+00 (2.7579e+00)	Acc@1  37.00 ( 27.90)	Acc@5  65.00 ( 62.16)
 * Acc@1 28.050 Acc@5 62.170
### epoch[6] execution time: 14.935253381729126
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.189 ( 0.189)	Data  0.160 ( 0.160)	Loss 2.7139e+00 (2.7139e+00)	Acc@1  28.12 ( 28.12)	Acc@5  67.19 ( 67.19)
Epoch: [7][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.016)	Loss 2.9077e+00 (2.7821e+00)	Acc@1  28.12 ( 28.34)	Acc@5  61.72 ( 60.80)
Epoch: [7][ 20/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.009)	Loss 2.7360e+00 (2.7589e+00)	Acc@1  25.00 ( 28.53)	Acc@5  58.59 ( 61.72)
Epoch: [7][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.9196e+00 (2.7418e+00)	Acc@1  27.34 ( 28.60)	Acc@5  60.94 ( 61.79)
Epoch: [7][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.9145e+00 (2.7427e+00)	Acc@1  25.78 ( 28.89)	Acc@5  58.59 ( 61.78)
Epoch: [7][ 50/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.9477e+00 (2.7489e+00)	Acc@1  21.88 ( 28.78)	Acc@5  61.72 ( 61.83)
Epoch: [7][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.4971e+00 (2.7493e+00)	Acc@1  31.25 ( 28.91)	Acc@5  65.62 ( 61.71)
Epoch: [7][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.9613e+00 (2.7525e+00)	Acc@1  30.47 ( 28.99)	Acc@5  59.38 ( 61.80)
Epoch: [7][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.4417e+00 (2.7372e+00)	Acc@1  38.28 ( 29.30)	Acc@5  70.31 ( 62.38)
Epoch: [7][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.7986e+00 (2.7336e+00)	Acc@1  28.91 ( 29.33)	Acc@5  58.59 ( 62.40)
Epoch: [7][100/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.6451e+00 (2.7314e+00)	Acc@1  32.81 ( 29.32)	Acc@5  66.41 ( 62.50)
Epoch: [7][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8603e+00 (2.7269e+00)	Acc@1  21.88 ( 29.41)	Acc@5  62.50 ( 62.74)
Epoch: [7][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6949e+00 (2.7284e+00)	Acc@1  28.12 ( 29.29)	Acc@5  63.28 ( 62.69)
Epoch: [7][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.5856e+00 (2.7265e+00)	Acc@1  30.47 ( 29.22)	Acc@5  66.41 ( 62.70)
Epoch: [7][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7999e+00 (2.7331e+00)	Acc@1  32.03 ( 29.12)	Acc@5  60.16 ( 62.56)
Epoch: [7][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7444e+00 (2.7275e+00)	Acc@1  27.34 ( 29.16)	Acc@5  62.50 ( 62.72)
Epoch: [7][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9057e+00 (2.7287e+00)	Acc@1  31.25 ( 29.15)	Acc@5  58.59 ( 62.67)
Epoch: [7][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0955e+00 (2.7282e+00)	Acc@1  22.66 ( 29.18)	Acc@5  62.50 ( 62.77)
Epoch: [7][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6434e+00 (2.7271e+00)	Acc@1  26.56 ( 29.18)	Acc@5  68.75 ( 62.76)
Epoch: [7][190/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.5636e+00 (2.7204e+00)	Acc@1  32.03 ( 29.36)	Acc@5  62.50 ( 62.91)
Epoch: [7][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7862e+00 (2.7213e+00)	Acc@1  27.34 ( 29.25)	Acc@5  57.81 ( 62.95)
Epoch: [7][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7262e+00 (2.7204e+00)	Acc@1  24.22 ( 29.21)	Acc@5  60.94 ( 62.96)
Epoch: [7][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9112e+00 (2.7205e+00)	Acc@1  25.78 ( 29.18)	Acc@5  66.41 ( 62.97)
Epoch: [7][230/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4090e+00 (2.7144e+00)	Acc@1  39.84 ( 29.31)	Acc@5  69.53 ( 63.19)
Epoch: [7][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6185e+00 (2.7066e+00)	Acc@1  30.47 ( 29.46)	Acc@5  64.84 ( 63.37)
Epoch: [7][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6031e+00 (2.7038e+00)	Acc@1  28.12 ( 29.50)	Acc@5  62.50 ( 63.41)
Epoch: [7][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5780e+00 (2.7008e+00)	Acc@1  32.81 ( 29.53)	Acc@5  68.75 ( 63.48)
Epoch: [7][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5162e+00 (2.7003e+00)	Acc@1  34.38 ( 29.56)	Acc@5  67.97 ( 63.47)
Epoch: [7][280/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3634e+00 (2.6978e+00)	Acc@1  38.28 ( 29.69)	Acc@5  68.75 ( 63.51)
Epoch: [7][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6630e+00 (2.6934e+00)	Acc@1  27.34 ( 29.76)	Acc@5  64.84 ( 63.62)
Epoch: [7][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7005e+00 (2.6924e+00)	Acc@1  23.44 ( 29.79)	Acc@5  63.28 ( 63.65)
Epoch: [7][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8059e+00 (2.6916e+00)	Acc@1  28.12 ( 29.78)	Acc@5  59.38 ( 63.66)
Epoch: [7][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6294e+00 (2.6901e+00)	Acc@1  28.12 ( 29.82)	Acc@5  69.53 ( 63.68)
Epoch: [7][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3191e+00 (2.6870e+00)	Acc@1  32.81 ( 29.84)	Acc@5  71.88 ( 63.77)
Epoch: [7][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6399e+00 (2.6852e+00)	Acc@1  31.25 ( 29.88)	Acc@5  65.62 ( 63.83)
Epoch: [7][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.5383e+00 (2.6855e+00)	Acc@1  27.34 ( 29.86)	Acc@5  69.53 ( 63.81)
Epoch: [7][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.5558e+00 (2.6819e+00)	Acc@1  32.81 ( 29.97)	Acc@5  67.19 ( 63.90)
Epoch: [7][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6149e+00 (2.6825e+00)	Acc@1  28.12 ( 29.93)	Acc@5  71.09 ( 63.89)
Epoch: [7][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8115e+00 (2.6809e+00)	Acc@1  22.66 ( 29.97)	Acc@5  57.81 ( 63.94)
Epoch: [7][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9576e+00 (2.6815e+00)	Acc@1  32.50 ( 29.96)	Acc@5  56.25 ( 63.93)
## e[7] optimizer.zero_grad (sum) time: 0.11567974090576172
## e[7]       loss.backward (sum) time: 2.0905492305755615
## e[7]      optimizer.step (sum) time: 0.7384779453277588
## epoch[7] training(only) time: 13.003958940505981
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 2.8135e+00 (2.8135e+00)	Acc@1  28.00 ( 28.00)	Acc@5  61.00 ( 61.00)
Test: [ 10/100]	Time  0.017 ( 0.028)	Loss 2.8930e+00 (2.7409e+00)	Acc@1  25.00 ( 30.45)	Acc@5  58.00 ( 61.73)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 2.4313e+00 (2.7116e+00)	Acc@1  36.00 ( 30.48)	Acc@5  71.00 ( 62.05)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 2.9821e+00 (2.7233e+00)	Acc@1  25.00 ( 30.52)	Acc@5  54.00 ( 62.00)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 2.7478e+00 (2.7189e+00)	Acc@1  28.00 ( 30.76)	Acc@5  62.00 ( 62.17)
Test: [ 50/100]	Time  0.017 ( 0.019)	Loss 2.5701e+00 (2.7224e+00)	Acc@1  35.00 ( 30.65)	Acc@5  63.00 ( 61.96)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.5694e+00 (2.7125e+00)	Acc@1  35.00 ( 30.62)	Acc@5  68.00 ( 62.02)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.8349e+00 (2.7138e+00)	Acc@1  23.00 ( 30.41)	Acc@5  64.00 ( 62.08)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 2.9472e+00 (2.7251e+00)	Acc@1  30.00 ( 30.19)	Acc@5  61.00 ( 61.89)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 2.7958e+00 (2.7219e+00)	Acc@1  35.00 ( 30.19)	Acc@5  61.00 ( 61.93)
 * Acc@1 30.250 Acc@5 61.870
### epoch[7] execution time: 14.95525336265564
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.176 ( 0.176)	Data  0.147 ( 0.147)	Loss 2.2600e+00 (2.2600e+00)	Acc@1  33.59 ( 33.59)	Acc@5  77.34 ( 77.34)
Epoch: [8][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 2.5040e+00 (2.5210e+00)	Acc@1  39.06 ( 33.31)	Acc@5  64.06 ( 66.12)
Epoch: [8][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 2.4114e+00 (2.5373e+00)	Acc@1  40.62 ( 33.04)	Acc@5  67.97 ( 66.82)
Epoch: [8][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.6054e+00 (2.5636e+00)	Acc@1  32.81 ( 32.48)	Acc@5  67.19 ( 66.86)
Epoch: [8][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.7793e+00 (2.5591e+00)	Acc@1  22.66 ( 32.20)	Acc@5  67.19 ( 67.00)
Epoch: [8][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.6514e+00 (2.5513e+00)	Acc@1  25.00 ( 32.35)	Acc@5  66.41 ( 67.02)
Epoch: [8][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.6531e+00 (2.5483e+00)	Acc@1  30.47 ( 32.33)	Acc@5  63.28 ( 67.00)
Epoch: [8][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.5652e+00 (2.5548e+00)	Acc@1  35.16 ( 32.36)	Acc@5  64.06 ( 66.57)
Epoch: [8][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.5520e+00 (2.5529e+00)	Acc@1  32.03 ( 32.39)	Acc@5  61.72 ( 66.59)
Epoch: [8][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.7857e+00 (2.5559e+00)	Acc@1  22.66 ( 32.13)	Acc@5  63.28 ( 66.60)
Epoch: [8][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3675e+00 (2.5540e+00)	Acc@1  37.50 ( 32.22)	Acc@5  72.66 ( 66.74)
Epoch: [8][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3371e+00 (2.5455e+00)	Acc@1  42.97 ( 32.51)	Acc@5  67.97 ( 66.90)
Epoch: [8][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6860e+00 (2.5457e+00)	Acc@1  32.03 ( 32.50)	Acc@5  63.28 ( 66.88)
Epoch: [8][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3406e+00 (2.5466e+00)	Acc@1  36.72 ( 32.47)	Acc@5  71.88 ( 66.76)
Epoch: [8][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4762e+00 (2.5474e+00)	Acc@1  38.28 ( 32.66)	Acc@5  71.88 ( 66.74)
Epoch: [8][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4778e+00 (2.5454e+00)	Acc@1  36.72 ( 32.73)	Acc@5  67.97 ( 66.72)
Epoch: [8][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4711e+00 (2.5459e+00)	Acc@1  32.81 ( 32.70)	Acc@5  70.31 ( 66.71)
Epoch: [8][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6436e+00 (2.5461e+00)	Acc@1  32.03 ( 32.70)	Acc@5  67.97 ( 66.74)
Epoch: [8][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8086e+00 (2.5430e+00)	Acc@1  28.91 ( 32.77)	Acc@5  61.72 ( 66.86)
Epoch: [8][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5673e+00 (2.5458e+00)	Acc@1  31.25 ( 32.73)	Acc@5  66.41 ( 66.74)
Epoch: [8][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7338e+00 (2.5464e+00)	Acc@1  27.34 ( 32.75)	Acc@5  60.94 ( 66.76)
Epoch: [8][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3914e+00 (2.5466e+00)	Acc@1  32.81 ( 32.72)	Acc@5  71.88 ( 66.80)
Epoch: [8][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6564e+00 (2.5459e+00)	Acc@1  32.03 ( 32.78)	Acc@5  69.53 ( 66.90)
Epoch: [8][230/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6432e+00 (2.5490e+00)	Acc@1  34.38 ( 32.73)	Acc@5  66.41 ( 66.90)
Epoch: [8][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5021e+00 (2.5470e+00)	Acc@1  38.28 ( 32.79)	Acc@5  67.19 ( 67.02)
Epoch: [8][250/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8332e+00 (2.5480e+00)	Acc@1  27.34 ( 32.73)	Acc@5  57.03 ( 67.04)
Epoch: [8][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4564e+00 (2.5484e+00)	Acc@1  31.25 ( 32.69)	Acc@5  66.41 ( 67.01)
Epoch: [8][270/391]	Time  0.036 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.5126e+00 (2.5453e+00)	Acc@1  34.38 ( 32.73)	Acc@5  66.41 ( 67.02)
Epoch: [8][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4187e+00 (2.5417e+00)	Acc@1  32.03 ( 32.78)	Acc@5  72.66 ( 67.08)
Epoch: [8][290/391]	Time  0.036 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2700e+00 (2.5391e+00)	Acc@1  42.19 ( 32.85)	Acc@5  80.47 ( 67.18)
Epoch: [8][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6485e+00 (2.5413e+00)	Acc@1  30.47 ( 32.82)	Acc@5  65.62 ( 67.09)
Epoch: [8][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4391e+00 (2.5398e+00)	Acc@1  36.72 ( 32.86)	Acc@5  69.53 ( 67.11)
Epoch: [8][320/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3954e+00 (2.5401e+00)	Acc@1  36.72 ( 32.83)	Acc@5  67.19 ( 67.11)
Epoch: [8][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3640e+00 (2.5405e+00)	Acc@1  30.47 ( 32.82)	Acc@5  73.44 ( 67.13)
Epoch: [8][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1162e+00 (2.5407e+00)	Acc@1  39.84 ( 32.85)	Acc@5  77.34 ( 67.10)
Epoch: [8][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4449e+00 (2.5407e+00)	Acc@1  35.16 ( 32.88)	Acc@5  67.97 ( 67.09)
Epoch: [8][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3128e+00 (2.5376e+00)	Acc@1  37.50 ( 32.95)	Acc@5  76.56 ( 67.17)
Epoch: [8][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4371e+00 (2.5372e+00)	Acc@1  37.50 ( 32.97)	Acc@5  66.41 ( 67.19)
Epoch: [8][380/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.5894e+00 (2.5355e+00)	Acc@1  31.25 ( 33.03)	Acc@5  66.41 ( 67.20)
Epoch: [8][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6592e+00 (2.5346e+00)	Acc@1  32.50 ( 33.07)	Acc@5  62.50 ( 67.22)
## e[8] optimizer.zero_grad (sum) time: 0.11605596542358398
## e[8]       loss.backward (sum) time: 2.1008541584014893
## e[8]      optimizer.step (sum) time: 0.7278954982757568
## epoch[8] training(only) time: 12.975405216217041
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 2.4610e+00 (2.4610e+00)	Acc@1  40.00 ( 40.00)	Acc@5  70.00 ( 70.00)
Test: [ 10/100]	Time  0.024 ( 0.030)	Loss 2.6215e+00 (2.4201e+00)	Acc@1  33.00 ( 38.27)	Acc@5  68.00 ( 69.09)
Test: [ 20/100]	Time  0.019 ( 0.024)	Loss 2.2324e+00 (2.4171e+00)	Acc@1  40.00 ( 37.67)	Acc@5  78.00 ( 69.24)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 2.4954e+00 (2.4347e+00)	Acc@1  43.00 ( 37.00)	Acc@5  69.00 ( 69.42)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 2.4131e+00 (2.4438e+00)	Acc@1  36.00 ( 36.27)	Acc@5  68.00 ( 68.88)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 2.3158e+00 (2.4454e+00)	Acc@1  35.00 ( 36.25)	Acc@5  71.00 ( 68.75)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.6419e+00 (2.4431e+00)	Acc@1  37.00 ( 36.30)	Acc@5  68.00 ( 68.93)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.7943e+00 (2.4581e+00)	Acc@1  31.00 ( 36.23)	Acc@5  65.00 ( 68.90)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.5516e+00 (2.4702e+00)	Acc@1  35.00 ( 35.89)	Acc@5  64.00 ( 68.72)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2662e+00 (2.4636e+00)	Acc@1  43.00 ( 36.09)	Acc@5  74.00 ( 69.04)
 * Acc@1 36.230 Acc@5 69.170
### epoch[8] execution time: 14.913187265396118
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.173 ( 0.173)	Data  0.147 ( 0.147)	Loss 2.4979e+00 (2.4979e+00)	Acc@1  32.03 ( 32.03)	Acc@5  71.88 ( 71.88)
Epoch: [9][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 2.6385e+00 (2.5262e+00)	Acc@1  35.94 ( 33.38)	Acc@5  62.50 ( 68.68)
Epoch: [9][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 2.3491e+00 (2.4744e+00)	Acc@1  37.50 ( 34.90)	Acc@5  71.88 ( 69.23)
Epoch: [9][ 30/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.2849e+00 (2.4458e+00)	Acc@1  37.50 ( 35.84)	Acc@5  71.09 ( 69.25)
Epoch: [9][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.4157e+00 (2.4310e+00)	Acc@1  33.59 ( 35.99)	Acc@5  68.75 ( 69.65)
Epoch: [9][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.5159e+00 (2.4075e+00)	Acc@1  30.47 ( 36.21)	Acc@5  71.09 ( 69.94)
Epoch: [9][ 60/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.5558e+00 (2.4173e+00)	Acc@1  28.12 ( 35.80)	Acc@5  64.84 ( 69.62)
Epoch: [9][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.3773e+00 (2.4227e+00)	Acc@1  41.41 ( 35.85)	Acc@5  70.31 ( 69.63)
Epoch: [9][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.2299e+00 (2.4289e+00)	Acc@1  46.09 ( 35.73)	Acc@5  73.44 ( 69.59)
Epoch: [9][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.5435e+00 (2.4272e+00)	Acc@1  35.94 ( 35.88)	Acc@5  62.50 ( 69.51)
Epoch: [9][100/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3922e+00 (2.4310e+00)	Acc@1  35.16 ( 35.80)	Acc@5  69.53 ( 69.53)
Epoch: [9][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2091e+00 (2.4319e+00)	Acc@1  34.38 ( 35.73)	Acc@5  76.56 ( 69.57)
Epoch: [9][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6121e+00 (2.4269e+00)	Acc@1  33.59 ( 35.78)	Acc@5  67.19 ( 69.70)
Epoch: [9][130/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.5783e+00 (2.4302e+00)	Acc@1  33.59 ( 35.84)	Acc@5  68.75 ( 69.53)
Epoch: [9][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4984e+00 (2.4363e+00)	Acc@1  36.72 ( 35.78)	Acc@5  67.97 ( 69.31)
Epoch: [9][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.5472e+00 (2.4349e+00)	Acc@1  35.16 ( 35.82)	Acc@5  69.53 ( 69.38)
Epoch: [9][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2285e+00 (2.4338e+00)	Acc@1  41.41 ( 35.88)	Acc@5  75.78 ( 69.47)
Epoch: [9][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5056e+00 (2.4352e+00)	Acc@1  34.38 ( 35.94)	Acc@5  67.97 ( 69.49)
Epoch: [9][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4777e+00 (2.4371e+00)	Acc@1  38.28 ( 35.96)	Acc@5  67.97 ( 69.45)
Epoch: [9][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4229e+00 (2.4370e+00)	Acc@1  36.72 ( 35.93)	Acc@5  69.53 ( 69.49)
Epoch: [9][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4047e+00 (2.4357e+00)	Acc@1  41.41 ( 35.94)	Acc@5  70.31 ( 69.59)
Epoch: [9][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5156e+00 (2.4351e+00)	Acc@1  31.25 ( 35.91)	Acc@5  69.53 ( 69.68)
Epoch: [9][220/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.4231e+00 (2.4334e+00)	Acc@1  32.03 ( 35.90)	Acc@5  68.75 ( 69.69)
Epoch: [9][230/391]	Time  0.028 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.2036e+00 (2.4316e+00)	Acc@1  43.75 ( 35.96)	Acc@5  74.22 ( 69.74)
Epoch: [9][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4487e+00 (2.4306e+00)	Acc@1  41.41 ( 35.97)	Acc@5  70.31 ( 69.80)
Epoch: [9][250/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6037e+00 (2.4292e+00)	Acc@1  35.94 ( 36.06)	Acc@5  65.62 ( 69.83)
Epoch: [9][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5686e+00 (2.4301e+00)	Acc@1  35.16 ( 36.05)	Acc@5  67.97 ( 69.80)
Epoch: [9][270/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4944e+00 (2.4276e+00)	Acc@1  30.47 ( 36.11)	Acc@5  75.00 ( 69.84)
Epoch: [9][280/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2432e+00 (2.4271e+00)	Acc@1  44.53 ( 36.17)	Acc@5  75.00 ( 69.86)
Epoch: [9][290/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.5781e+00 (2.4243e+00)	Acc@1  33.59 ( 36.18)	Acc@5  67.97 ( 69.93)
Epoch: [9][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3228e+00 (2.4255e+00)	Acc@1  38.28 ( 36.12)	Acc@5  71.88 ( 69.87)
Epoch: [9][310/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.6582e+00 (2.4246e+00)	Acc@1  34.38 ( 36.14)	Acc@5  67.19 ( 69.88)
Epoch: [9][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5567e+00 (2.4233e+00)	Acc@1  25.00 ( 36.16)	Acc@5  72.66 ( 69.95)
Epoch: [9][330/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1898e+00 (2.4229e+00)	Acc@1  46.09 ( 36.22)	Acc@5  71.09 ( 69.93)
Epoch: [9][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3509e+00 (2.4198e+00)	Acc@1  37.50 ( 36.27)	Acc@5  68.75 ( 69.97)
Epoch: [9][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3743e+00 (2.4174e+00)	Acc@1  32.81 ( 36.33)	Acc@5  74.22 ( 70.03)
Epoch: [9][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1756e+00 (2.4152e+00)	Acc@1  41.41 ( 36.38)	Acc@5  77.34 ( 70.14)
Epoch: [9][370/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.2677e+00 (2.4114e+00)	Acc@1  41.41 ( 36.44)	Acc@5  71.88 ( 70.18)
Epoch: [9][380/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3380e+00 (2.4088e+00)	Acc@1  39.06 ( 36.51)	Acc@5  71.88 ( 70.22)
Epoch: [9][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2953e+00 (2.4076e+00)	Acc@1  40.00 ( 36.53)	Acc@5  72.50 ( 70.25)
## e[9] optimizer.zero_grad (sum) time: 0.11629557609558105
## e[9]       loss.backward (sum) time: 2.1067962646484375
## e[9]      optimizer.step (sum) time: 0.7216286659240723
## epoch[9] training(only) time: 13.02070140838623
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 2.4706e+00 (2.4706e+00)	Acc@1  33.00 ( 33.00)	Acc@5  68.00 ( 68.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.4944e+00 (2.3603e+00)	Acc@1  34.00 ( 38.91)	Acc@5  67.00 ( 70.09)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 2.2138e+00 (2.3472e+00)	Acc@1  41.00 ( 40.14)	Acc@5  71.00 ( 70.90)
Test: [ 30/100]	Time  0.018 ( 0.022)	Loss 2.3009e+00 (2.3365e+00)	Acc@1  45.00 ( 39.52)	Acc@5  69.00 ( 71.32)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 2.3010e+00 (2.3485e+00)	Acc@1  37.00 ( 38.63)	Acc@5  73.00 ( 70.85)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 2.1708e+00 (2.3497e+00)	Acc@1  43.00 ( 38.49)	Acc@5  70.00 ( 70.51)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.5520e+00 (2.3344e+00)	Acc@1  39.00 ( 38.72)	Acc@5  68.00 ( 70.95)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.5531e+00 (2.3461e+00)	Acc@1  32.00 ( 38.37)	Acc@5  72.00 ( 70.86)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.4617e+00 (2.3610e+00)	Acc@1  39.00 ( 37.98)	Acc@5  63.00 ( 70.60)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.4154e+00 (2.3535e+00)	Acc@1  34.00 ( 38.14)	Acc@5  69.00 ( 70.75)
 * Acc@1 38.220 Acc@5 70.720
### epoch[9] execution time: 14.960027933120728
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.177 ( 0.177)	Data  0.151 ( 0.151)	Loss 2.0708e+00 (2.0708e+00)	Acc@1  45.31 ( 45.31)	Acc@5  81.25 ( 81.25)
Epoch: [10][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 2.2847e+00 (2.2229e+00)	Acc@1  38.28 ( 40.84)	Acc@5  67.19 ( 72.80)
Epoch: [10][ 20/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.009)	Loss 2.1735e+00 (2.2213e+00)	Acc@1  38.28 ( 40.51)	Acc@5  78.12 ( 74.40)
Epoch: [10][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.1387e+00 (2.3017e+00)	Acc@1  42.19 ( 38.96)	Acc@5  78.12 ( 72.66)
Epoch: [10][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.2721e+00 (2.3110e+00)	Acc@1  33.59 ( 38.99)	Acc@5  75.00 ( 72.60)
Epoch: [10][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.3012e+00 (2.3097e+00)	Acc@1  37.50 ( 38.77)	Acc@5  72.66 ( 72.59)
Epoch: [10][ 60/391]	Time  0.033 ( 0.035)	Data  0.002 ( 0.004)	Loss 2.0625e+00 (2.3024e+00)	Acc@1  42.19 ( 38.82)	Acc@5  81.25 ( 72.64)
Epoch: [10][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.0486e+00 (2.2963e+00)	Acc@1  42.97 ( 38.86)	Acc@5  78.12 ( 72.88)
Epoch: [10][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1519e+00 (2.3102e+00)	Acc@1  39.06 ( 38.58)	Acc@5  78.91 ( 72.53)
Epoch: [10][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.2217e+00 (2.3123e+00)	Acc@1  38.28 ( 38.53)	Acc@5  74.22 ( 72.54)
Epoch: [10][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4673e+00 (2.3113e+00)	Acc@1  32.03 ( 38.37)	Acc@5  67.19 ( 72.62)
Epoch: [10][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2630e+00 (2.3127e+00)	Acc@1  43.75 ( 38.27)	Acc@5  71.88 ( 72.58)
Epoch: [10][120/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.3546e+00 (2.3110e+00)	Acc@1  38.28 ( 38.30)	Acc@5  71.88 ( 72.59)
Epoch: [10][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4225e+00 (2.3100e+00)	Acc@1  36.72 ( 38.39)	Acc@5  70.31 ( 72.63)
Epoch: [10][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2287e+00 (2.3063e+00)	Acc@1  42.97 ( 38.62)	Acc@5  73.44 ( 72.63)
Epoch: [10][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5573e+00 (2.3018e+00)	Acc@1  39.06 ( 38.70)	Acc@5  64.06 ( 72.66)
Epoch: [10][160/391]	Time  0.035 ( 0.033)	Data  0.004 ( 0.003)	Loss 2.1953e+00 (2.2988e+00)	Acc@1  38.28 ( 38.80)	Acc@5  79.69 ( 72.82)
Epoch: [10][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4024e+00 (2.2998e+00)	Acc@1  36.72 ( 38.80)	Acc@5  67.97 ( 72.66)
Epoch: [10][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0749e+00 (2.2998e+00)	Acc@1  42.97 ( 38.81)	Acc@5  78.12 ( 72.63)
Epoch: [10][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3426e+00 (2.3004e+00)	Acc@1  35.16 ( 38.80)	Acc@5  72.66 ( 72.64)
Epoch: [10][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5260e+00 (2.2966e+00)	Acc@1  32.03 ( 38.92)	Acc@5  67.19 ( 72.66)
Epoch: [10][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5559e+00 (2.2972e+00)	Acc@1  32.03 ( 38.92)	Acc@5  62.50 ( 72.57)
Epoch: [10][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3747e+00 (2.2970e+00)	Acc@1  34.38 ( 38.89)	Acc@5  70.31 ( 72.56)
Epoch: [10][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1920e+00 (2.2967e+00)	Acc@1  39.84 ( 38.94)	Acc@5  69.53 ( 72.53)
Epoch: [10][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1683e+00 (2.2949e+00)	Acc@1  41.41 ( 39.02)	Acc@5  75.78 ( 72.50)
Epoch: [10][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2151e+00 (2.2943e+00)	Acc@1  40.62 ( 39.06)	Acc@5  74.22 ( 72.49)
Epoch: [10][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3215e+00 (2.2950e+00)	Acc@1  38.28 ( 39.01)	Acc@5  72.66 ( 72.48)
Epoch: [10][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2927e+00 (2.2927e+00)	Acc@1  39.84 ( 39.06)	Acc@5  68.75 ( 72.54)
Epoch: [10][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1559e+00 (2.2917e+00)	Acc@1  42.97 ( 39.07)	Acc@5  77.34 ( 72.52)
Epoch: [10][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2405e+00 (2.2938e+00)	Acc@1  43.75 ( 39.06)	Acc@5  76.56 ( 72.52)
Epoch: [10][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3525e+00 (2.2934e+00)	Acc@1  41.41 ( 39.10)	Acc@5  70.31 ( 72.55)
Epoch: [10][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2014e+00 (2.2910e+00)	Acc@1  44.53 ( 39.16)	Acc@5  74.22 ( 72.60)
Epoch: [10][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0424e+00 (2.2919e+00)	Acc@1  39.84 ( 39.13)	Acc@5  78.91 ( 72.62)
Epoch: [10][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2016e+00 (2.2909e+00)	Acc@1  41.41 ( 39.18)	Acc@5  75.78 ( 72.65)
Epoch: [10][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0143e+00 (2.2902e+00)	Acc@1  47.66 ( 39.22)	Acc@5  76.56 ( 72.68)
Epoch: [10][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2642e+00 (2.2879e+00)	Acc@1  39.06 ( 39.27)	Acc@5  73.44 ( 72.70)
Epoch: [10][360/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.0083e+00 (2.2846e+00)	Acc@1  45.31 ( 39.33)	Acc@5  78.12 ( 72.77)
Epoch: [10][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3331e+00 (2.2858e+00)	Acc@1  42.19 ( 39.35)	Acc@5  71.88 ( 72.75)
Epoch: [10][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3479e+00 (2.2865e+00)	Acc@1  39.06 ( 39.38)	Acc@5  73.44 ( 72.69)
Epoch: [10][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3808e+00 (2.2870e+00)	Acc@1  36.25 ( 39.40)	Acc@5  73.75 ( 72.65)
## e[10] optimizer.zero_grad (sum) time: 0.11547255516052246
## e[10]       loss.backward (sum) time: 2.0578882694244385
## e[10]      optimizer.step (sum) time: 0.740048885345459
## epoch[10] training(only) time: 12.982359409332275
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 2.2096e+00 (2.2096e+00)	Acc@1  46.00 ( 46.00)	Acc@5  70.00 ( 70.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.5107e+00 (2.3545e+00)	Acc@1  35.00 ( 39.36)	Acc@5  69.00 ( 72.36)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 2.1692e+00 (2.3468e+00)	Acc@1  45.00 ( 39.67)	Acc@5  73.00 ( 71.43)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 2.3273e+00 (2.3474e+00)	Acc@1  37.00 ( 39.19)	Acc@5  71.00 ( 70.87)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 2.4439e+00 (2.3472e+00)	Acc@1  32.00 ( 39.10)	Acc@5  69.00 ( 70.85)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 2.1265e+00 (2.3565e+00)	Acc@1  43.00 ( 38.84)	Acc@5  71.00 ( 70.61)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.2735e+00 (2.3434e+00)	Acc@1  44.00 ( 38.95)	Acc@5  70.00 ( 70.66)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.4565e+00 (2.3459e+00)	Acc@1  38.00 ( 38.93)	Acc@5  72.00 ( 70.66)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.6169e+00 (2.3523e+00)	Acc@1  31.00 ( 38.63)	Acc@5  66.00 ( 70.65)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 2.4358e+00 (2.3441e+00)	Acc@1  39.00 ( 38.90)	Acc@5  71.00 ( 70.77)
 * Acc@1 39.050 Acc@5 70.780
### epoch[10] execution time: 14.896982669830322
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.173 ( 0.173)	Data  0.145 ( 0.145)	Loss 2.1118e+00 (2.1118e+00)	Acc@1  39.84 ( 39.84)	Acc@5  74.22 ( 74.22)
Epoch: [11][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 2.2789e+00 (2.1334e+00)	Acc@1  41.41 ( 42.68)	Acc@5  74.22 ( 75.57)
Epoch: [11][ 20/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.009)	Loss 2.1872e+00 (2.1540e+00)	Acc@1  44.53 ( 41.26)	Acc@5  71.88 ( 75.41)
Epoch: [11][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.2888e+00 (2.1797e+00)	Acc@1  42.97 ( 41.63)	Acc@5  73.44 ( 75.18)
Epoch: [11][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.0045e+00 (2.1639e+00)	Acc@1  46.88 ( 42.04)	Acc@5  75.78 ( 75.40)
Epoch: [11][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.1526e+00 (2.1726e+00)	Acc@1  43.75 ( 41.93)	Acc@5  76.56 ( 75.14)
Epoch: [11][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.5723e+00 (2.1798e+00)	Acc@1  28.12 ( 41.78)	Acc@5  67.97 ( 75.06)
Epoch: [11][ 70/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.2799e+00 (2.1791e+00)	Acc@1  37.50 ( 41.74)	Acc@5  76.56 ( 75.25)
Epoch: [11][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.2159e+00 (2.1762e+00)	Acc@1  36.72 ( 41.75)	Acc@5  74.22 ( 75.16)
Epoch: [11][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1864e+00 (2.1825e+00)	Acc@1  40.62 ( 41.47)	Acc@5  76.56 ( 74.97)
Epoch: [11][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2450e+00 (2.1928e+00)	Acc@1  39.06 ( 41.26)	Acc@5  75.00 ( 74.80)
Epoch: [11][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1798e+00 (2.1861e+00)	Acc@1  37.50 ( 41.39)	Acc@5  75.78 ( 74.96)
Epoch: [11][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3158e+00 (2.1830e+00)	Acc@1  35.94 ( 41.48)	Acc@5  77.34 ( 75.09)
Epoch: [11][130/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9691e+00 (2.1838e+00)	Acc@1  42.19 ( 41.30)	Acc@5  81.25 ( 75.01)
Epoch: [11][140/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.3803e+00 (2.1876e+00)	Acc@1  41.41 ( 41.23)	Acc@5  66.41 ( 74.93)
Epoch: [11][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1221e+00 (2.1885e+00)	Acc@1  42.19 ( 41.27)	Acc@5  73.44 ( 74.85)
Epoch: [11][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2101e+00 (2.1889e+00)	Acc@1  39.06 ( 41.25)	Acc@5  75.00 ( 74.84)
Epoch: [11][170/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2310e+00 (2.1900e+00)	Acc@1  45.31 ( 41.24)	Acc@5  75.00 ( 74.76)
Epoch: [11][180/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.3089e+00 (2.1916e+00)	Acc@1  39.84 ( 41.20)	Acc@5  75.00 ( 74.73)
Epoch: [11][190/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.0316e+00 (2.1907e+00)	Acc@1  43.75 ( 41.25)	Acc@5  78.12 ( 74.63)
Epoch: [11][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3714e+00 (2.1920e+00)	Acc@1  34.38 ( 41.22)	Acc@5  75.00 ( 74.64)
Epoch: [11][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2031e+00 (2.1940e+00)	Acc@1  43.75 ( 41.27)	Acc@5  73.44 ( 74.54)
Epoch: [11][220/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.3535e+00 (2.1938e+00)	Acc@1  37.50 ( 41.23)	Acc@5  70.31 ( 74.58)
Epoch: [11][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2019e+00 (2.1950e+00)	Acc@1  38.28 ( 41.21)	Acc@5  78.12 ( 74.59)
Epoch: [11][240/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.3405e+00 (2.1934e+00)	Acc@1  32.81 ( 41.22)	Acc@5  71.88 ( 74.64)
Epoch: [11][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9410e+00 (2.1909e+00)	Acc@1  45.31 ( 41.30)	Acc@5  76.56 ( 74.67)
Epoch: [11][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3427e+00 (2.1931e+00)	Acc@1  40.62 ( 41.30)	Acc@5  73.44 ( 74.66)
Epoch: [11][270/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.3548e+00 (2.1931e+00)	Acc@1  37.50 ( 41.37)	Acc@5  70.31 ( 74.63)
Epoch: [11][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0594e+00 (2.1947e+00)	Acc@1  43.75 ( 41.42)	Acc@5  80.47 ( 74.55)
Epoch: [11][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9772e+00 (2.1933e+00)	Acc@1  46.88 ( 41.48)	Acc@5  76.56 ( 74.58)
Epoch: [11][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1706e+00 (2.1920e+00)	Acc@1  44.53 ( 41.55)	Acc@5  75.00 ( 74.61)
Epoch: [11][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0410e+00 (2.1928e+00)	Acc@1  43.75 ( 41.55)	Acc@5  78.91 ( 74.59)
Epoch: [11][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2426e+00 (2.1898e+00)	Acc@1  42.19 ( 41.64)	Acc@5  70.31 ( 74.63)
Epoch: [11][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0352e+00 (2.1885e+00)	Acc@1  47.66 ( 41.71)	Acc@5  78.12 ( 74.70)
Epoch: [11][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.3237e+00 (2.1866e+00)	Acc@1  39.06 ( 41.75)	Acc@5  74.22 ( 74.76)
Epoch: [11][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3733e+00 (2.1875e+00)	Acc@1  36.72 ( 41.72)	Acc@5  71.88 ( 74.74)
Epoch: [11][360/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8305e+00 (2.1869e+00)	Acc@1  51.56 ( 41.73)	Acc@5  81.25 ( 74.72)
Epoch: [11][370/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3442e+00 (2.1873e+00)	Acc@1  37.50 ( 41.74)	Acc@5  67.19 ( 74.68)
Epoch: [11][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1132e+00 (2.1866e+00)	Acc@1  35.94 ( 41.71)	Acc@5  81.25 ( 74.73)
Epoch: [11][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8944e+00 (2.1834e+00)	Acc@1  46.25 ( 41.82)	Acc@5  81.25 ( 74.77)
## e[11] optimizer.zero_grad (sum) time: 0.11625409126281738
## e[11]       loss.backward (sum) time: 2.082695960998535
## e[11]      optimizer.step (sum) time: 0.7291440963745117
## epoch[11] training(only) time: 12.956522703170776
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 2.3388e+00 (2.3388e+00)	Acc@1  43.00 ( 43.00)	Acc@5  73.00 ( 73.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.4023e+00 (2.3576e+00)	Acc@1  39.00 ( 41.18)	Acc@5  76.00 ( 72.55)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 2.3347e+00 (2.3404e+00)	Acc@1  44.00 ( 40.76)	Acc@5  70.00 ( 72.57)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 2.4395e+00 (2.3442e+00)	Acc@1  41.00 ( 40.45)	Acc@5  68.00 ( 72.35)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 2.2249e+00 (2.3431e+00)	Acc@1  39.00 ( 40.15)	Acc@5  78.00 ( 72.56)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 2.2984e+00 (2.3445e+00)	Acc@1  39.00 ( 39.86)	Acc@5  70.00 ( 72.41)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.2932e+00 (2.3386e+00)	Acc@1  42.00 ( 39.85)	Acc@5  72.00 ( 72.28)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.4202e+00 (2.3473e+00)	Acc@1  41.00 ( 39.76)	Acc@5  70.00 ( 72.11)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.6332e+00 (2.3664e+00)	Acc@1  40.00 ( 39.25)	Acc@5  62.00 ( 71.80)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2886e+00 (2.3640e+00)	Acc@1  49.00 ( 39.30)	Acc@5  75.00 ( 71.87)
 * Acc@1 39.260 Acc@5 71.830
### epoch[11] execution time: 14.915984869003296
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.185 ( 0.185)	Data  0.159 ( 0.159)	Loss 2.1441e+00 (2.1441e+00)	Acc@1  35.94 ( 35.94)	Acc@5  76.56 ( 76.56)
Epoch: [12][ 10/391]	Time  0.031 ( 0.046)	Data  0.001 ( 0.016)	Loss 2.1020e+00 (2.1503e+00)	Acc@1  41.41 ( 40.84)	Acc@5  78.12 ( 74.50)
Epoch: [12][ 20/391]	Time  0.033 ( 0.040)	Data  0.002 ( 0.009)	Loss 1.8751e+00 (2.0854e+00)	Acc@1  42.97 ( 43.53)	Acc@5  79.69 ( 76.38)
Epoch: [12][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.9062e+00 (2.0855e+00)	Acc@1  41.41 ( 43.40)	Acc@5  80.47 ( 76.06)
Epoch: [12][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.0781e+00 (2.0784e+00)	Acc@1  47.66 ( 43.83)	Acc@5  78.12 ( 76.54)
Epoch: [12][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.2335e+00 (2.0715e+00)	Acc@1  41.41 ( 43.81)	Acc@5  78.12 ( 76.49)
Epoch: [12][ 60/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.1056e+00 (2.0875e+00)	Acc@1  42.19 ( 43.49)	Acc@5  75.78 ( 76.08)
Epoch: [12][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.0319e+00 (2.0801e+00)	Acc@1  46.09 ( 43.53)	Acc@5  74.22 ( 76.29)
Epoch: [12][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8361e+00 (2.0846e+00)	Acc@1  53.12 ( 43.57)	Acc@5  82.81 ( 76.24)
Epoch: [12][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1884e+00 (2.0842e+00)	Acc@1  40.62 ( 43.60)	Acc@5  77.34 ( 76.41)
Epoch: [12][100/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8193e+00 (2.0860e+00)	Acc@1  48.44 ( 43.51)	Acc@5  82.03 ( 76.48)
Epoch: [12][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1913e+00 (2.0873e+00)	Acc@1  42.97 ( 43.68)	Acc@5  75.00 ( 76.44)
Epoch: [12][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0619e+00 (2.0860e+00)	Acc@1  46.09 ( 43.83)	Acc@5  74.22 ( 76.53)
Epoch: [12][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9017e+00 (2.0827e+00)	Acc@1  44.53 ( 43.82)	Acc@5  81.25 ( 76.57)
Epoch: [12][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0658e+00 (2.0868e+00)	Acc@1  48.44 ( 43.71)	Acc@5  72.66 ( 76.49)
Epoch: [12][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9063e+00 (2.0809e+00)	Acc@1  42.97 ( 43.89)	Acc@5  78.12 ( 76.53)
Epoch: [12][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0984e+00 (2.0855e+00)	Acc@1  41.41 ( 43.79)	Acc@5  76.56 ( 76.36)
Epoch: [12][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1942e+00 (2.0844e+00)	Acc@1  42.97 ( 43.83)	Acc@5  78.12 ( 76.34)
Epoch: [12][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4187e+00 (2.0866e+00)	Acc@1  38.28 ( 43.81)	Acc@5  76.56 ( 76.39)
Epoch: [12][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7739e+00 (2.0860e+00)	Acc@1  50.00 ( 43.77)	Acc@5  85.94 ( 76.39)
Epoch: [12][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1465e+00 (2.0896e+00)	Acc@1  46.09 ( 43.70)	Acc@5  75.00 ( 76.38)
Epoch: [12][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9353e+00 (2.0908e+00)	Acc@1  50.78 ( 43.72)	Acc@5  78.12 ( 76.34)
Epoch: [12][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0312e+00 (2.0893e+00)	Acc@1  44.53 ( 43.76)	Acc@5  75.78 ( 76.34)
Epoch: [12][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1677e+00 (2.0898e+00)	Acc@1  39.84 ( 43.75)	Acc@5  74.22 ( 76.37)
Epoch: [12][240/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2413e+00 (2.0953e+00)	Acc@1  40.62 ( 43.67)	Acc@5  73.44 ( 76.25)
Epoch: [12][250/391]	Time  0.028 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1719e+00 (2.0927e+00)	Acc@1  40.62 ( 43.76)	Acc@5  74.22 ( 76.27)
Epoch: [12][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9525e+00 (2.0900e+00)	Acc@1  43.75 ( 43.80)	Acc@5  78.91 ( 76.33)
Epoch: [12][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0918e+00 (2.0908e+00)	Acc@1  45.31 ( 43.79)	Acc@5  75.00 ( 76.28)
Epoch: [12][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9758e+00 (2.0904e+00)	Acc@1  42.19 ( 43.80)	Acc@5  79.69 ( 76.28)
Epoch: [12][290/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0752e+00 (2.0915e+00)	Acc@1  47.66 ( 43.80)	Acc@5  74.22 ( 76.22)
Epoch: [12][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1682e+00 (2.0894e+00)	Acc@1  45.31 ( 43.90)	Acc@5  75.78 ( 76.25)
Epoch: [12][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0197e+00 (2.0864e+00)	Acc@1  49.22 ( 43.99)	Acc@5  78.91 ( 76.26)
Epoch: [12][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8584e+00 (2.0857e+00)	Acc@1  48.44 ( 44.01)	Acc@5  82.03 ( 76.29)
Epoch: [12][330/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1801e+00 (2.0856e+00)	Acc@1  41.41 ( 44.01)	Acc@5  75.00 ( 76.27)
Epoch: [12][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1257e+00 (2.0880e+00)	Acc@1  41.41 ( 43.96)	Acc@5  71.88 ( 76.23)
Epoch: [12][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0108e+00 (2.0890e+00)	Acc@1  46.88 ( 43.91)	Acc@5  75.78 ( 76.20)
Epoch: [12][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7853e+00 (2.0891e+00)	Acc@1  47.66 ( 43.89)	Acc@5  85.16 ( 76.24)
Epoch: [12][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1250e+00 (2.0888e+00)	Acc@1  42.19 ( 43.90)	Acc@5  74.22 ( 76.25)
Epoch: [12][380/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.7106e+00 (2.0886e+00)	Acc@1  51.56 ( 43.90)	Acc@5  81.25 ( 76.26)
Epoch: [12][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2359e+00 (2.0907e+00)	Acc@1  38.75 ( 43.86)	Acc@5  72.50 ( 76.21)
## e[12] optimizer.zero_grad (sum) time: 0.11570286750793457
## e[12]       loss.backward (sum) time: 2.0618064403533936
## e[12]      optimizer.step (sum) time: 0.727341890335083
## epoch[12] training(only) time: 13.00154972076416
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 2.4425e+00 (2.4425e+00)	Acc@1  42.00 ( 42.00)	Acc@5  70.00 ( 70.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.3119e+00 (2.2127e+00)	Acc@1  40.00 ( 42.82)	Acc@5  73.00 ( 73.91)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 2.1049e+00 (2.2188e+00)	Acc@1  46.00 ( 42.71)	Acc@5  73.00 ( 74.19)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 2.2060e+00 (2.2074e+00)	Acc@1  44.00 ( 42.32)	Acc@5  72.00 ( 74.45)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 2.0510e+00 (2.2116e+00)	Acc@1  49.00 ( 42.37)	Acc@5  75.00 ( 74.07)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 2.1968e+00 (2.2325e+00)	Acc@1  43.00 ( 42.18)	Acc@5  71.00 ( 73.67)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.2659e+00 (2.2158e+00)	Acc@1  45.00 ( 42.46)	Acc@5  73.00 ( 73.84)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 2.1740e+00 (2.2120e+00)	Acc@1  44.00 ( 42.65)	Acc@5  75.00 ( 73.68)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.3425e+00 (2.2265e+00)	Acc@1  43.00 ( 42.16)	Acc@5  71.00 ( 73.42)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.3142e+00 (2.2212e+00)	Acc@1  40.00 ( 42.24)	Acc@5  72.00 ( 73.58)
 * Acc@1 42.290 Acc@5 73.670
### epoch[12] execution time: 14.942742586135864
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.178 ( 0.178)	Data  0.150 ( 0.150)	Loss 2.0239e+00 (2.0239e+00)	Acc@1  44.53 ( 44.53)	Acc@5  77.34 ( 77.34)
Epoch: [13][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.8400e+00 (1.9659e+00)	Acc@1  44.53 ( 46.09)	Acc@5  80.47 ( 78.48)
Epoch: [13][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 2.0627e+00 (1.9743e+00)	Acc@1  42.19 ( 46.99)	Acc@5  76.56 ( 79.09)
Epoch: [13][ 30/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.2978e+00 (1.9767e+00)	Acc@1  38.28 ( 46.50)	Acc@5  73.44 ( 78.63)
Epoch: [13][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.0599e+00 (1.9878e+00)	Acc@1  46.09 ( 46.40)	Acc@5  77.34 ( 78.72)
Epoch: [13][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.0921e+00 (2.0040e+00)	Acc@1  44.53 ( 46.11)	Acc@5  79.69 ( 78.71)
Epoch: [13][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.1026e+00 (2.0116e+00)	Acc@1  41.41 ( 45.93)	Acc@5  81.25 ( 78.59)
Epoch: [13][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.9315e+00 (2.0096e+00)	Acc@1  51.56 ( 46.20)	Acc@5  78.91 ( 78.31)
Epoch: [13][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1341e+00 (2.0139e+00)	Acc@1  41.41 ( 46.14)	Acc@5  75.00 ( 78.12)
Epoch: [13][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.2178e+00 (2.0135e+00)	Acc@1  40.62 ( 46.09)	Acc@5  74.22 ( 77.94)
Epoch: [13][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9903e+00 (2.0215e+00)	Acc@1  50.00 ( 45.79)	Acc@5  78.91 ( 77.87)
Epoch: [13][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3716e+00 (2.0303e+00)	Acc@1  40.62 ( 45.63)	Acc@5  67.19 ( 77.64)
Epoch: [13][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6624e+00 (2.0219e+00)	Acc@1  57.81 ( 45.93)	Acc@5  86.72 ( 77.74)
Epoch: [13][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1229e+00 (2.0186e+00)	Acc@1  50.78 ( 46.05)	Acc@5  74.22 ( 77.76)
Epoch: [13][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0065e+00 (2.0153e+00)	Acc@1  46.09 ( 46.05)	Acc@5  80.47 ( 77.80)
Epoch: [13][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0258e+00 (2.0122e+00)	Acc@1  46.09 ( 46.18)	Acc@5  74.22 ( 77.76)
Epoch: [13][160/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.1217e+00 (2.0086e+00)	Acc@1  41.41 ( 46.21)	Acc@5  76.56 ( 77.85)
Epoch: [13][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0017e+00 (2.0074e+00)	Acc@1  47.66 ( 46.13)	Acc@5  80.47 ( 77.91)
Epoch: [13][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9533e+00 (2.0069e+00)	Acc@1  39.84 ( 46.06)	Acc@5  80.47 ( 77.88)
Epoch: [13][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7110e+00 (2.0037e+00)	Acc@1  54.69 ( 46.12)	Acc@5  85.16 ( 77.99)
Epoch: [13][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0717e+00 (2.0030e+00)	Acc@1  46.88 ( 46.18)	Acc@5  72.66 ( 78.01)
Epoch: [13][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8187e+00 (2.0001e+00)	Acc@1  49.22 ( 46.18)	Acc@5  82.03 ( 78.10)
Epoch: [13][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1080e+00 (2.0020e+00)	Acc@1  46.09 ( 46.13)	Acc@5  74.22 ( 78.07)
Epoch: [13][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9143e+00 (2.0013e+00)	Acc@1  42.19 ( 46.11)	Acc@5  78.12 ( 78.03)
Epoch: [13][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0427e+00 (2.0028e+00)	Acc@1  42.97 ( 46.07)	Acc@5  79.69 ( 78.04)
Epoch: [13][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2170e+00 (2.0009e+00)	Acc@1  35.94 ( 46.09)	Acc@5  75.00 ( 78.07)
Epoch: [13][260/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.4632e+00 (2.0054e+00)	Acc@1  39.84 ( 45.99)	Acc@5  69.53 ( 77.99)
Epoch: [13][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1309e+00 (2.0098e+00)	Acc@1  40.62 ( 45.81)	Acc@5  75.78 ( 77.97)
Epoch: [13][280/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6727e+00 (2.0108e+00)	Acc@1  52.34 ( 45.81)	Acc@5  82.81 ( 77.92)
Epoch: [13][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8378e+00 (2.0121e+00)	Acc@1  48.44 ( 45.83)	Acc@5  81.25 ( 77.90)
Epoch: [13][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0084e+00 (2.0145e+00)	Acc@1  45.31 ( 45.76)	Acc@5  79.69 ( 77.89)
Epoch: [13][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9350e+00 (2.0125e+00)	Acc@1  49.22 ( 45.82)	Acc@5  82.81 ( 77.91)
Epoch: [13][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2797e+00 (2.0114e+00)	Acc@1  42.19 ( 45.91)	Acc@5  75.00 ( 77.91)
Epoch: [13][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0616e+00 (2.0126e+00)	Acc@1  49.22 ( 45.91)	Acc@5  78.12 ( 77.91)
Epoch: [13][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8392e+00 (2.0120e+00)	Acc@1  49.22 ( 45.93)	Acc@5  83.59 ( 77.90)
Epoch: [13][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2443e+00 (2.0130e+00)	Acc@1  39.84 ( 45.91)	Acc@5  76.56 ( 77.89)
Epoch: [13][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3941e+00 (2.0135e+00)	Acc@1  37.50 ( 45.86)	Acc@5  70.31 ( 77.89)
Epoch: [13][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8493e+00 (2.0124e+00)	Acc@1  44.53 ( 45.86)	Acc@5  82.81 ( 77.90)
Epoch: [13][380/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9577e+00 (2.0140e+00)	Acc@1  48.44 ( 45.83)	Acc@5  78.12 ( 77.86)
Epoch: [13][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8682e+00 (2.0122e+00)	Acc@1  45.00 ( 45.88)	Acc@5  82.50 ( 77.88)
## e[13] optimizer.zero_grad (sum) time: 0.11572384834289551
## e[13]       loss.backward (sum) time: 2.0887818336486816
## e[13]      optimizer.step (sum) time: 0.7213850021362305
## epoch[13] training(only) time: 13.000529766082764
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.9206e+00 (1.9206e+00)	Acc@1  54.00 ( 54.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.0907e+00 (1.9359e+00)	Acc@1  44.00 ( 49.09)	Acc@5  80.00 ( 80.00)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.9592e+00 (1.9432e+00)	Acc@1  45.00 ( 48.10)	Acc@5  74.00 ( 79.10)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.8559e+00 (1.9396e+00)	Acc@1  47.00 ( 48.16)	Acc@5  82.00 ( 78.97)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.9516e+00 (1.9437e+00)	Acc@1  47.00 ( 47.85)	Acc@5  79.00 ( 78.51)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.7992e+00 (1.9548e+00)	Acc@1  56.00 ( 47.98)	Acc@5  81.00 ( 78.24)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9305e+00 (1.9452e+00)	Acc@1  51.00 ( 47.84)	Acc@5  81.00 ( 78.44)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.0803e+00 (1.9491e+00)	Acc@1  45.00 ( 47.75)	Acc@5  76.00 ( 78.41)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.0920e+00 (1.9624e+00)	Acc@1  46.00 ( 47.43)	Acc@5  75.00 ( 78.36)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.9642e+00 (1.9541e+00)	Acc@1  49.00 ( 47.63)	Acc@5  78.00 ( 78.52)
 * Acc@1 47.710 Acc@5 78.490
### epoch[13] execution time: 14.934336423873901
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.179 ( 0.179)	Data  0.152 ( 0.152)	Loss 1.7559e+00 (1.7559e+00)	Acc@1  53.91 ( 53.91)	Acc@5  81.25 ( 81.25)
Epoch: [14][ 10/391]	Time  0.031 ( 0.045)	Data  0.001 ( 0.015)	Loss 1.6842e+00 (1.8278e+00)	Acc@1  57.03 ( 50.71)	Acc@5  84.38 ( 80.54)
Epoch: [14][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.8368e+00 (1.8805e+00)	Acc@1  47.66 ( 49.14)	Acc@5  78.12 ( 79.99)
Epoch: [14][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.0447e+00 (1.9120e+00)	Acc@1  46.88 ( 47.98)	Acc@5  80.47 ( 79.96)
Epoch: [14][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.1854e+00 (1.9106e+00)	Acc@1  40.62 ( 48.15)	Acc@5  76.56 ( 80.05)
Epoch: [14][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.8502e+00 (1.9184e+00)	Acc@1  53.12 ( 47.90)	Acc@5  82.81 ( 79.76)
Epoch: [14][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.9265e+00 (1.9084e+00)	Acc@1  45.31 ( 48.07)	Acc@5  84.38 ( 79.98)
Epoch: [14][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.0701e+00 (1.9187e+00)	Acc@1  46.09 ( 47.98)	Acc@5  73.44 ( 79.70)
Epoch: [14][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7480e+00 (1.9246e+00)	Acc@1  54.69 ( 47.81)	Acc@5  78.91 ( 79.63)
Epoch: [14][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7088e+00 (1.9316e+00)	Acc@1  54.69 ( 47.57)	Acc@5  81.25 ( 79.67)
Epoch: [14][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9868e+00 (1.9327e+00)	Acc@1  42.19 ( 47.42)	Acc@5  78.12 ( 79.63)
Epoch: [14][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1298e+00 (1.9356e+00)	Acc@1  44.53 ( 47.49)	Acc@5  78.12 ( 79.49)
Epoch: [14][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0190e+00 (1.9320e+00)	Acc@1  42.19 ( 47.55)	Acc@5  72.66 ( 79.40)
Epoch: [14][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0886e+00 (1.9368e+00)	Acc@1  46.88 ( 47.58)	Acc@5  75.00 ( 79.31)
Epoch: [14][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8916e+00 (1.9356e+00)	Acc@1  45.31 ( 47.57)	Acc@5  81.25 ( 79.29)
Epoch: [14][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2261e+00 (1.9366e+00)	Acc@1  39.84 ( 47.55)	Acc@5  70.31 ( 79.28)
Epoch: [14][160/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.6585e+00 (1.9341e+00)	Acc@1  53.12 ( 47.64)	Acc@5  86.72 ( 79.31)
Epoch: [14][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3764e+00 (1.9367e+00)	Acc@1  39.84 ( 47.54)	Acc@5  71.09 ( 79.24)
Epoch: [14][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7349e+00 (1.9368e+00)	Acc@1  57.81 ( 47.54)	Acc@5  81.25 ( 79.25)
Epoch: [14][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8407e+00 (1.9401e+00)	Acc@1  46.88 ( 47.42)	Acc@5  78.12 ( 79.21)
Epoch: [14][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7613e+00 (1.9412e+00)	Acc@1  49.22 ( 47.35)	Acc@5  81.25 ( 79.19)
Epoch: [14][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1331e+00 (1.9423e+00)	Acc@1  41.41 ( 47.29)	Acc@5  73.44 ( 79.13)
Epoch: [14][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0801e+00 (1.9463e+00)	Acc@1  45.31 ( 47.35)	Acc@5  74.22 ( 79.03)
Epoch: [14][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9519e+00 (1.9486e+00)	Acc@1  46.88 ( 47.30)	Acc@5  80.47 ( 79.00)
Epoch: [14][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7756e+00 (1.9492e+00)	Acc@1  54.69 ( 47.31)	Acc@5  81.25 ( 78.96)
Epoch: [14][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0254e+00 (1.9486e+00)	Acc@1  48.44 ( 47.31)	Acc@5  79.69 ( 79.03)
Epoch: [14][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8536e+00 (1.9504e+00)	Acc@1  53.12 ( 47.30)	Acc@5  78.91 ( 79.02)
Epoch: [14][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9466e+00 (1.9494e+00)	Acc@1  42.97 ( 47.28)	Acc@5  81.25 ( 79.06)
Epoch: [14][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7291e+00 (1.9490e+00)	Acc@1  48.44 ( 47.23)	Acc@5  82.81 ( 79.10)
Epoch: [14][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8047e+00 (1.9469e+00)	Acc@1  50.78 ( 47.29)	Acc@5  82.03 ( 79.13)
Epoch: [14][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8362e+00 (1.9484e+00)	Acc@1  50.78 ( 47.28)	Acc@5  85.94 ( 79.10)
Epoch: [14][310/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0265e+00 (1.9490e+00)	Acc@1  47.66 ( 47.30)	Acc@5  74.22 ( 79.06)
Epoch: [14][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0032e+00 (1.9491e+00)	Acc@1  49.22 ( 47.34)	Acc@5  78.91 ( 79.09)
Epoch: [14][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9606e+00 (1.9507e+00)	Acc@1  49.22 ( 47.32)	Acc@5  79.69 ( 79.05)
Epoch: [14][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8801e+00 (1.9503e+00)	Acc@1  45.31 ( 47.31)	Acc@5  75.00 ( 79.06)
Epoch: [14][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9563e+00 (1.9500e+00)	Acc@1  41.41 ( 47.31)	Acc@5  77.34 ( 79.08)
Epoch: [14][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6758e+00 (1.9519e+00)	Acc@1  53.12 ( 47.28)	Acc@5  81.25 ( 79.06)
Epoch: [14][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8567e+00 (1.9510e+00)	Acc@1  51.56 ( 47.30)	Acc@5  80.47 ( 79.07)
Epoch: [14][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9249e+00 (1.9513e+00)	Acc@1  47.66 ( 47.28)	Acc@5  77.34 ( 79.05)
Epoch: [14][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8679e+00 (1.9510e+00)	Acc@1  43.75 ( 47.27)	Acc@5  81.25 ( 79.09)
## e[14] optimizer.zero_grad (sum) time: 0.11658859252929688
## e[14]       loss.backward (sum) time: 2.0940258502960205
## e[14]      optimizer.step (sum) time: 0.7295849323272705
## epoch[14] training(only) time: 12.97889256477356
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 2.0708e+00 (2.0708e+00)	Acc@1  43.00 ( 43.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0864e+00 (1.9730e+00)	Acc@1  40.00 ( 46.73)	Acc@5  77.00 ( 77.36)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.9917e+00 (2.0095e+00)	Acc@1  52.00 ( 45.76)	Acc@5  77.00 ( 76.86)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 2.0742e+00 (2.0338e+00)	Acc@1  42.00 ( 45.58)	Acc@5  81.00 ( 76.87)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.8530e+00 (2.0217e+00)	Acc@1  47.00 ( 45.83)	Acc@5  81.00 ( 76.95)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.9823e+00 (2.0461e+00)	Acc@1  48.00 ( 45.45)	Acc@5  75.00 ( 76.59)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0105e+00 (2.0350e+00)	Acc@1  50.00 ( 45.44)	Acc@5  78.00 ( 76.72)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 2.1343e+00 (2.0374e+00)	Acc@1  47.00 ( 45.49)	Acc@5  74.00 ( 76.82)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.0201e+00 (2.0417e+00)	Acc@1  42.00 ( 45.33)	Acc@5  76.00 ( 76.91)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.8655e+00 (2.0250e+00)	Acc@1  52.00 ( 45.54)	Acc@5  79.00 ( 77.45)
 * Acc@1 45.650 Acc@5 77.560
### epoch[14] execution time: 14.914882898330688
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.176 ( 0.176)	Data  0.150 ( 0.150)	Loss 1.8355e+00 (1.8355e+00)	Acc@1  50.78 ( 50.78)	Acc@5  82.03 ( 82.03)
Epoch: [15][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 2.0994e+00 (1.8798e+00)	Acc@1  45.31 ( 49.29)	Acc@5  75.78 ( 80.33)
Epoch: [15][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.7778e+00 (1.8262e+00)	Acc@1  54.69 ( 50.97)	Acc@5  80.47 ( 80.51)
Epoch: [15][ 30/391]	Time  0.030 ( 0.037)	Data  0.002 ( 0.007)	Loss 1.8394e+00 (1.8393e+00)	Acc@1  52.34 ( 50.25)	Acc@5  78.12 ( 80.65)
Epoch: [15][ 40/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.0821e+00 (1.8395e+00)	Acc@1  42.19 ( 50.27)	Acc@5  77.34 ( 80.89)
Epoch: [15][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.8918e+00 (1.8542e+00)	Acc@1  50.78 ( 49.79)	Acc@5  81.25 ( 80.56)
Epoch: [15][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.9721e+00 (1.8531e+00)	Acc@1  44.53 ( 49.62)	Acc@5  85.16 ( 80.90)
Epoch: [15][ 70/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7013e+00 (1.8575e+00)	Acc@1  51.56 ( 49.45)	Acc@5  81.25 ( 80.72)
Epoch: [15][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7918e+00 (1.8526e+00)	Acc@1  52.34 ( 49.64)	Acc@5  85.16 ( 80.87)
Epoch: [15][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7366e+00 (1.8574e+00)	Acc@1  54.69 ( 49.62)	Acc@5  82.81 ( 80.91)
Epoch: [15][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6714e+00 (1.8483e+00)	Acc@1  53.12 ( 49.74)	Acc@5  79.69 ( 80.99)
Epoch: [15][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0973e+00 (1.8539e+00)	Acc@1  48.44 ( 49.69)	Acc@5  77.34 ( 80.89)
Epoch: [15][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7083e+00 (1.8496e+00)	Acc@1  53.12 ( 49.68)	Acc@5  84.38 ( 80.92)
Epoch: [15][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8571e+00 (1.8526e+00)	Acc@1  47.66 ( 49.55)	Acc@5  84.38 ( 80.97)
Epoch: [15][140/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.8136e+00 (1.8524e+00)	Acc@1  57.03 ( 49.58)	Acc@5  82.03 ( 80.99)
Epoch: [15][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9575e+00 (1.8519e+00)	Acc@1  49.22 ( 49.64)	Acc@5  81.25 ( 80.98)
Epoch: [15][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0554e+00 (1.8569e+00)	Acc@1  45.31 ( 49.52)	Acc@5  80.47 ( 80.92)
Epoch: [15][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7455e+00 (1.8592e+00)	Acc@1  47.66 ( 49.44)	Acc@5  80.47 ( 80.85)
Epoch: [15][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1465e+00 (1.8641e+00)	Acc@1  48.44 ( 49.44)	Acc@5  75.00 ( 80.67)
Epoch: [15][190/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.1671e+00 (1.8689e+00)	Acc@1  41.41 ( 49.32)	Acc@5  76.56 ( 80.59)
Epoch: [15][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9110e+00 (1.8653e+00)	Acc@1  50.78 ( 49.53)	Acc@5  75.78 ( 80.66)
Epoch: [15][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7442e+00 (1.8630e+00)	Acc@1  50.00 ( 49.56)	Acc@5  85.94 ( 80.72)
Epoch: [15][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0873e+00 (1.8673e+00)	Acc@1  51.56 ( 49.48)	Acc@5  75.00 ( 80.68)
Epoch: [15][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7618e+00 (1.8682e+00)	Acc@1  48.44 ( 49.46)	Acc@5  82.03 ( 80.64)
Epoch: [15][240/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.6999e+00 (1.8664e+00)	Acc@1  53.91 ( 49.48)	Acc@5  85.94 ( 80.67)
Epoch: [15][250/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1647e+00 (1.8678e+00)	Acc@1  47.66 ( 49.43)	Acc@5  74.22 ( 80.62)
Epoch: [15][260/391]	Time  0.035 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.8049e+00 (1.8718e+00)	Acc@1  47.66 ( 49.40)	Acc@5  82.81 ( 80.51)
Epoch: [15][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9109e+00 (1.8723e+00)	Acc@1  46.88 ( 49.42)	Acc@5  80.47 ( 80.51)
Epoch: [15][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9823e+00 (1.8719e+00)	Acc@1  50.00 ( 49.51)	Acc@5  78.91 ( 80.50)
Epoch: [15][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7803e+00 (1.8744e+00)	Acc@1  51.56 ( 49.46)	Acc@5  82.81 ( 80.46)
Epoch: [15][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8062e+00 (1.8774e+00)	Acc@1  49.22 ( 49.34)	Acc@5  84.38 ( 80.42)
Epoch: [15][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0238e+00 (1.8790e+00)	Acc@1  44.53 ( 49.29)	Acc@5  79.69 ( 80.40)
Epoch: [15][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8776e+00 (1.8815e+00)	Acc@1  47.66 ( 49.24)	Acc@5  83.59 ( 80.34)
Epoch: [15][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8364e+00 (1.8806e+00)	Acc@1  52.34 ( 49.24)	Acc@5  81.25 ( 80.33)
Epoch: [15][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8882e+00 (1.8787e+00)	Acc@1  49.22 ( 49.28)	Acc@5  78.91 ( 80.35)
Epoch: [15][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1205e+00 (1.8791e+00)	Acc@1  48.44 ( 49.27)	Acc@5  80.47 ( 80.36)
Epoch: [15][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9284e+00 (1.8796e+00)	Acc@1  44.53 ( 49.23)	Acc@5  78.91 ( 80.37)
Epoch: [15][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6665e+00 (1.8795e+00)	Acc@1  53.91 ( 49.22)	Acc@5  84.38 ( 80.36)
Epoch: [15][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8007e+00 (1.8804e+00)	Acc@1  52.34 ( 49.21)	Acc@5  78.91 ( 80.34)
Epoch: [15][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6220e+00 (1.8773e+00)	Acc@1  57.50 ( 49.26)	Acc@5  88.75 ( 80.38)
## e[15] optimizer.zero_grad (sum) time: 0.11623454093933105
## e[15]       loss.backward (sum) time: 2.069751024246216
## e[15]      optimizer.step (sum) time: 0.7280616760253906
## epoch[15] training(only) time: 12.955425262451172
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.8791e+00 (1.8791e+00)	Acc@1  55.00 ( 55.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.020 ( 0.029)	Loss 1.9524e+00 (1.9132e+00)	Acc@1  46.00 ( 51.82)	Acc@5  84.00 ( 79.91)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.8661e+00 (1.9268e+00)	Acc@1  52.00 ( 49.81)	Acc@5  78.00 ( 79.95)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.7718e+00 (1.9105e+00)	Acc@1  52.00 ( 49.81)	Acc@5  82.00 ( 79.90)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.6626e+00 (1.9267e+00)	Acc@1  55.00 ( 49.54)	Acc@5  83.00 ( 79.56)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.9083e+00 (1.9440e+00)	Acc@1  53.00 ( 49.22)	Acc@5  77.00 ( 79.06)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.1393e+00 (1.9331e+00)	Acc@1  43.00 ( 49.03)	Acc@5  74.00 ( 79.13)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.0378e+00 (1.9390e+00)	Acc@1  51.00 ( 48.93)	Acc@5  73.00 ( 79.11)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.0068e+00 (1.9444e+00)	Acc@1  50.00 ( 48.77)	Acc@5  77.00 ( 79.04)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2279e+00 (1.9394e+00)	Acc@1  45.00 ( 48.89)	Acc@5  73.00 ( 79.27)
 * Acc@1 49.010 Acc@5 79.440
### epoch[15] execution time: 14.903197765350342
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.188 ( 0.188)	Data  0.151 ( 0.151)	Loss 1.7383e+00 (1.7383e+00)	Acc@1  52.34 ( 52.34)	Acc@5  82.81 ( 82.81)
Epoch: [16][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.7364e+00 (1.7633e+00)	Acc@1  51.56 ( 52.06)	Acc@5  82.81 ( 82.10)
Epoch: [16][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.8051e+00 (1.7830e+00)	Acc@1  52.34 ( 51.30)	Acc@5  82.81 ( 81.88)
Epoch: [16][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.7694e+00 (1.7937e+00)	Acc@1  47.66 ( 51.11)	Acc@5  84.38 ( 81.63)
Epoch: [16][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.9367e+00 (1.7998e+00)	Acc@1  46.88 ( 50.82)	Acc@5  78.12 ( 81.92)
Epoch: [16][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.7263e+00 (1.7942e+00)	Acc@1  50.78 ( 50.84)	Acc@5  81.25 ( 81.79)
Epoch: [16][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.7671e+00 (1.7789e+00)	Acc@1  57.03 ( 51.32)	Acc@5  81.25 ( 82.15)
Epoch: [16][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.9648e+00 (1.7851e+00)	Acc@1  52.34 ( 51.10)	Acc@5  81.25 ( 82.09)
Epoch: [16][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.9374e+00 (1.7931e+00)	Acc@1  50.78 ( 50.94)	Acc@5  83.59 ( 81.88)
Epoch: [16][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.9432e+00 (1.7879e+00)	Acc@1  46.09 ( 51.08)	Acc@5  79.69 ( 81.90)
Epoch: [16][100/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6776e+00 (1.7963e+00)	Acc@1  55.47 ( 50.97)	Acc@5  85.94 ( 81.80)
Epoch: [16][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7227e+00 (1.7986e+00)	Acc@1  46.09 ( 50.97)	Acc@5  84.38 ( 81.73)
Epoch: [16][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8590e+00 (1.8048e+00)	Acc@1  51.56 ( 50.85)	Acc@5  78.91 ( 81.58)
Epoch: [16][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9246e+00 (1.8058e+00)	Acc@1  53.91 ( 50.95)	Acc@5  82.03 ( 81.56)
Epoch: [16][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7223e+00 (1.8096e+00)	Acc@1  50.78 ( 50.85)	Acc@5  81.25 ( 81.44)
Epoch: [16][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5440e+00 (1.8074e+00)	Acc@1  60.16 ( 50.93)	Acc@5  84.38 ( 81.49)
Epoch: [16][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8686e+00 (1.8071e+00)	Acc@1  50.00 ( 50.93)	Acc@5  78.91 ( 81.51)
Epoch: [16][170/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3245e+00 (1.8084e+00)	Acc@1  37.50 ( 50.90)	Acc@5  71.09 ( 81.38)
Epoch: [16][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6005e+00 (1.8088e+00)	Acc@1  57.03 ( 50.90)	Acc@5  85.94 ( 81.41)
Epoch: [16][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8095e+00 (1.8101e+00)	Acc@1  48.44 ( 50.85)	Acc@5  84.38 ( 81.36)
Epoch: [16][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0628e+00 (1.8145e+00)	Acc@1  43.75 ( 50.77)	Acc@5  76.56 ( 81.27)
Epoch: [16][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6657e+00 (1.8161e+00)	Acc@1  57.81 ( 50.69)	Acc@5  82.03 ( 81.21)
Epoch: [16][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8168e+00 (1.8164e+00)	Acc@1  50.00 ( 50.71)	Acc@5  81.25 ( 81.22)
Epoch: [16][230/391]	Time  0.028 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7745e+00 (1.8169e+00)	Acc@1  52.34 ( 50.76)	Acc@5  83.59 ( 81.22)
Epoch: [16][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1177e+00 (1.8163e+00)	Acc@1  35.16 ( 50.75)	Acc@5  76.56 ( 81.26)
Epoch: [16][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6668e+00 (1.8133e+00)	Acc@1  53.91 ( 50.83)	Acc@5  83.59 ( 81.31)
Epoch: [16][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7220e+00 (1.8150e+00)	Acc@1  50.78 ( 50.80)	Acc@5  75.78 ( 81.24)
Epoch: [16][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7332e+00 (1.8168e+00)	Acc@1  50.78 ( 50.81)	Acc@5  82.03 ( 81.18)
Epoch: [16][280/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8210e+00 (1.8193e+00)	Acc@1  50.00 ( 50.74)	Acc@5  78.91 ( 81.12)
Epoch: [16][290/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6712e+00 (1.8186e+00)	Acc@1  56.25 ( 50.80)	Acc@5  84.38 ( 81.16)
Epoch: [16][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9379e+00 (1.8202e+00)	Acc@1  49.22 ( 50.78)	Acc@5  78.12 ( 81.14)
Epoch: [16][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8545e+00 (1.8210e+00)	Acc@1  47.66 ( 50.75)	Acc@5  82.03 ( 81.14)
Epoch: [16][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8805e+00 (1.8211e+00)	Acc@1  48.44 ( 50.80)	Acc@5  77.34 ( 81.13)
Epoch: [16][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6040e+00 (1.8220e+00)	Acc@1  57.81 ( 50.81)	Acc@5  83.59 ( 81.12)
Epoch: [16][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6611e+00 (1.8199e+00)	Acc@1  55.47 ( 50.87)	Acc@5  80.47 ( 81.11)
Epoch: [16][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8758e+00 (1.8191e+00)	Acc@1  50.78 ( 50.89)	Acc@5  78.12 ( 81.10)
Epoch: [16][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5852e+00 (1.8191e+00)	Acc@1  56.25 ( 50.87)	Acc@5  81.25 ( 81.09)
Epoch: [16][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8900e+00 (1.8179e+00)	Acc@1  50.00 ( 50.91)	Acc@5  78.12 ( 81.11)
Epoch: [16][380/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0203e+00 (1.8177e+00)	Acc@1  48.44 ( 50.94)	Acc@5  81.25 ( 81.14)
Epoch: [16][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7863e+00 (1.8183e+00)	Acc@1  46.25 ( 50.93)	Acc@5  82.50 ( 81.13)
## e[16] optimizer.zero_grad (sum) time: 0.11536979675292969
## e[16]       loss.backward (sum) time: 2.096587657928467
## e[16]      optimizer.step (sum) time: 0.7464315891265869
## epoch[16] training(only) time: 12.990181684494019
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.8653e+00 (1.8653e+00)	Acc@1  55.00 ( 55.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.1130e+00 (1.9549e+00)	Acc@1  42.00 ( 48.91)	Acc@5  80.00 ( 78.09)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.9458e+00 (1.9866e+00)	Acc@1  47.00 ( 47.19)	Acc@5  78.00 ( 77.90)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 2.0198e+00 (1.9770e+00)	Acc@1  49.00 ( 47.71)	Acc@5  78.00 ( 77.94)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.8889e+00 (1.9821e+00)	Acc@1  46.00 ( 47.34)	Acc@5  83.00 ( 78.27)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.9425e+00 (2.0033e+00)	Acc@1  52.00 ( 47.14)	Acc@5  76.00 ( 77.63)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0788e+00 (1.9945e+00)	Acc@1  50.00 ( 47.00)	Acc@5  75.00 ( 77.64)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.1878e+00 (1.9949e+00)	Acc@1  44.00 ( 47.01)	Acc@5  72.00 ( 77.75)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.1243e+00 (2.0003e+00)	Acc@1  48.00 ( 46.91)	Acc@5  73.00 ( 77.70)
Test: [ 90/100]	Time  0.018 ( 0.018)	Loss 2.0405e+00 (1.9937e+00)	Acc@1  51.00 ( 47.07)	Acc@5  71.00 ( 77.84)
 * Acc@1 47.170 Acc@5 77.990
### epoch[16] execution time: 14.915372610092163
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.182 ( 0.182)	Data  0.157 ( 0.157)	Loss 1.6980e+00 (1.6980e+00)	Acc@1  53.91 ( 53.91)	Acc@5  82.81 ( 82.81)
Epoch: [17][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.8524e+00 (1.7344e+00)	Acc@1  49.22 ( 53.05)	Acc@5  83.59 ( 82.88)
Epoch: [17][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.7243e+00 (1.7253e+00)	Acc@1  54.69 ( 53.01)	Acc@5  88.28 ( 83.07)
Epoch: [17][ 30/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.7235e+00 (1.7396e+00)	Acc@1  51.56 ( 52.55)	Acc@5  83.59 ( 82.56)
Epoch: [17][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.4675e+00 (1.7332e+00)	Acc@1  57.03 ( 52.84)	Acc@5  89.06 ( 82.68)
Epoch: [17][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.0149e+00 (1.7571e+00)	Acc@1  47.66 ( 52.16)	Acc@5  78.12 ( 82.54)
Epoch: [17][ 60/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.7708e+00 (1.7532e+00)	Acc@1  46.09 ( 52.32)	Acc@5  85.16 ( 82.67)
Epoch: [17][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.8259e+00 (1.7561e+00)	Acc@1  57.03 ( 52.33)	Acc@5  81.25 ( 82.75)
Epoch: [17][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7777e+00 (1.7609e+00)	Acc@1  51.56 ( 52.06)	Acc@5  82.81 ( 82.69)
Epoch: [17][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7783e+00 (1.7649e+00)	Acc@1  53.12 ( 52.06)	Acc@5  81.25 ( 82.47)
Epoch: [17][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6565e+00 (1.7624e+00)	Acc@1  55.47 ( 52.13)	Acc@5  83.59 ( 82.45)
Epoch: [17][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8046e+00 (1.7625e+00)	Acc@1  53.12 ( 52.06)	Acc@5  82.03 ( 82.47)
Epoch: [17][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6474e+00 (1.7664e+00)	Acc@1  53.12 ( 51.83)	Acc@5  85.94 ( 82.44)
Epoch: [17][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5885e+00 (1.7626e+00)	Acc@1  57.81 ( 51.96)	Acc@5  80.47 ( 82.48)
Epoch: [17][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7761e+00 (1.7668e+00)	Acc@1  53.91 ( 51.86)	Acc@5  86.72 ( 82.51)
Epoch: [17][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6965e+00 (1.7688e+00)	Acc@1  52.34 ( 51.89)	Acc@5  84.38 ( 82.41)
Epoch: [17][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7336e+00 (1.7694e+00)	Acc@1  53.91 ( 51.91)	Acc@5  84.38 ( 82.39)
Epoch: [17][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7654e+00 (1.7682e+00)	Acc@1  49.22 ( 51.94)	Acc@5  83.59 ( 82.36)
Epoch: [17][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7401e+00 (1.7701e+00)	Acc@1  53.12 ( 51.90)	Acc@5  82.03 ( 82.26)
Epoch: [17][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7251e+00 (1.7685e+00)	Acc@1  53.12 ( 51.98)	Acc@5  80.47 ( 82.28)
Epoch: [17][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7859e+00 (1.7702e+00)	Acc@1  50.00 ( 51.91)	Acc@5  87.50 ( 82.28)
Epoch: [17][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6442e+00 (1.7667e+00)	Acc@1  52.34 ( 51.93)	Acc@5  92.19 ( 82.39)
Epoch: [17][220/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9954e+00 (1.7649e+00)	Acc@1  46.09 ( 52.00)	Acc@5  80.47 ( 82.42)
Epoch: [17][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7421e+00 (1.7620e+00)	Acc@1  53.91 ( 52.14)	Acc@5  79.69 ( 82.40)
Epoch: [17][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7440e+00 (1.7633e+00)	Acc@1  51.56 ( 52.11)	Acc@5  78.91 ( 82.39)
Epoch: [17][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0660e+00 (1.7642e+00)	Acc@1  43.75 ( 52.11)	Acc@5  81.25 ( 82.39)
Epoch: [17][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8315e+00 (1.7642e+00)	Acc@1  52.34 ( 52.16)	Acc@5  80.47 ( 82.35)
Epoch: [17][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9337e+00 (1.7629e+00)	Acc@1  48.44 ( 52.25)	Acc@5  81.25 ( 82.31)
Epoch: [17][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8056e+00 (1.7636e+00)	Acc@1  52.34 ( 52.26)	Acc@5  80.47 ( 82.30)
Epoch: [17][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8698e+00 (1.7658e+00)	Acc@1  52.34 ( 52.20)	Acc@5  82.03 ( 82.32)
Epoch: [17][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7120e+00 (1.7657e+00)	Acc@1  53.12 ( 52.19)	Acc@5  82.81 ( 82.33)
Epoch: [17][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8925e+00 (1.7674e+00)	Acc@1  48.44 ( 52.16)	Acc@5  78.12 ( 82.28)
Epoch: [17][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9371e+00 (1.7692e+00)	Acc@1  46.09 ( 52.13)	Acc@5  73.44 ( 82.21)
Epoch: [17][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6657e+00 (1.7692e+00)	Acc@1  55.47 ( 52.12)	Acc@5  79.69 ( 82.19)
Epoch: [17][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7570e+00 (1.7696e+00)	Acc@1  56.25 ( 52.07)	Acc@5  81.25 ( 82.19)
Epoch: [17][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3682e+00 (1.7699e+00)	Acc@1  64.06 ( 52.04)	Acc@5  89.06 ( 82.18)
Epoch: [17][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6308e+00 (1.7717e+00)	Acc@1  53.12 ( 52.01)	Acc@5  85.16 ( 82.13)
Epoch: [17][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7633e+00 (1.7727e+00)	Acc@1  49.22 ( 51.99)	Acc@5  80.47 ( 82.07)
Epoch: [17][380/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7129e+00 (1.7719e+00)	Acc@1  50.78 ( 52.00)	Acc@5  81.25 ( 82.07)
Epoch: [17][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6607e+00 (1.7694e+00)	Acc@1  52.50 ( 52.03)	Acc@5  87.50 ( 82.13)
## e[17] optimizer.zero_grad (sum) time: 0.11591887474060059
## e[17]       loss.backward (sum) time: 2.1283798217773438
## e[17]      optimizer.step (sum) time: 0.7135486602783203
## epoch[17] training(only) time: 13.016597747802734
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.7710e+00 (1.7710e+00)	Acc@1  57.00 ( 57.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.8451e+00 (1.7926e+00)	Acc@1  48.00 ( 53.27)	Acc@5  83.00 ( 80.91)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.6097e+00 (1.8053e+00)	Acc@1  60.00 ( 52.81)	Acc@5  84.00 ( 81.24)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.7581e+00 (1.8152e+00)	Acc@1  53.00 ( 52.13)	Acc@5  81.00 ( 80.39)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.6491e+00 (1.8163e+00)	Acc@1  56.00 ( 52.00)	Acc@5  84.00 ( 80.66)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.9063e+00 (1.8387e+00)	Acc@1  50.00 ( 51.33)	Acc@5  72.00 ( 80.20)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.7945e+00 (1.8165e+00)	Acc@1  52.00 ( 51.61)	Acc@5  82.00 ( 80.74)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.9615e+00 (1.8238e+00)	Acc@1  43.00 ( 51.44)	Acc@5  78.00 ( 80.83)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.9846e+00 (1.8324e+00)	Acc@1  48.00 ( 51.20)	Acc@5  72.00 ( 80.58)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7973e+00 (1.8239e+00)	Acc@1  51.00 ( 51.47)	Acc@5  78.00 ( 80.70)
 * Acc@1 51.520 Acc@5 80.780
### epoch[17] execution time: 14.979522228240967
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.174 ( 0.174)	Data  0.146 ( 0.146)	Loss 1.5367e+00 (1.5367e+00)	Acc@1  57.81 ( 57.81)	Acc@5  85.16 ( 85.16)
Epoch: [18][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 1.5012e+00 (1.5986e+00)	Acc@1  58.59 ( 55.68)	Acc@5  82.03 ( 84.52)
Epoch: [18][ 20/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.7074e+00 (1.6189e+00)	Acc@1  54.69 ( 55.47)	Acc@5  79.69 ( 84.08)
Epoch: [18][ 30/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.7523e+00 (1.6272e+00)	Acc@1  56.25 ( 55.72)	Acc@5  83.59 ( 84.12)
Epoch: [18][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.4924e+00 (1.6514e+00)	Acc@1  59.38 ( 55.20)	Acc@5  84.38 ( 83.57)
Epoch: [18][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.5503e+00 (1.6439e+00)	Acc@1  63.28 ( 55.50)	Acc@5  88.28 ( 83.66)
Epoch: [18][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.8134e+00 (1.6647e+00)	Acc@1  52.34 ( 55.01)	Acc@5  82.03 ( 83.21)
Epoch: [18][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.6355e+00 (1.6621e+00)	Acc@1  58.59 ( 55.08)	Acc@5  84.38 ( 83.30)
Epoch: [18][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6374e+00 (1.6661e+00)	Acc@1  57.81 ( 54.95)	Acc@5  85.16 ( 83.34)
Epoch: [18][ 90/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6821e+00 (1.6681e+00)	Acc@1  44.53 ( 54.78)	Acc@5  83.59 ( 83.40)
Epoch: [18][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4736e+00 (1.6582e+00)	Acc@1  53.12 ( 55.04)	Acc@5  87.50 ( 83.61)
Epoch: [18][110/391]	Time  0.030 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.4273e+00 (1.6565e+00)	Acc@1  61.72 ( 54.98)	Acc@5  88.28 ( 83.66)
Epoch: [18][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9560e+00 (1.6645e+00)	Acc@1  46.09 ( 54.72)	Acc@5  81.25 ( 83.60)
Epoch: [18][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6021e+00 (1.6712e+00)	Acc@1  52.34 ( 54.55)	Acc@5  85.94 ( 83.54)
Epoch: [18][140/391]	Time  0.036 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6702e+00 (1.6798e+00)	Acc@1  55.47 ( 54.27)	Acc@5  82.81 ( 83.39)
Epoch: [18][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4608e+00 (1.6807e+00)	Acc@1  65.62 ( 54.22)	Acc@5  88.28 ( 83.38)
Epoch: [18][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5789e+00 (1.6832e+00)	Acc@1  52.34 ( 54.08)	Acc@5  84.38 ( 83.27)
Epoch: [18][170/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8791e+00 (1.6840e+00)	Acc@1  47.66 ( 54.04)	Acc@5  81.25 ( 83.28)
Epoch: [18][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7808e+00 (1.6859e+00)	Acc@1  51.56 ( 53.98)	Acc@5  79.69 ( 83.25)
Epoch: [18][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6161e+00 (1.6858e+00)	Acc@1  60.16 ( 54.03)	Acc@5  81.25 ( 83.25)
Epoch: [18][200/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6600e+00 (1.6866e+00)	Acc@1  51.56 ( 54.05)	Acc@5  80.47 ( 83.22)
Epoch: [18][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6216e+00 (1.6865e+00)	Acc@1  53.12 ( 54.12)	Acc@5  87.50 ( 83.23)
Epoch: [18][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9970e+00 (1.6902e+00)	Acc@1  46.88 ( 54.00)	Acc@5  78.91 ( 83.20)
Epoch: [18][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9270e+00 (1.6909e+00)	Acc@1  46.09 ( 53.96)	Acc@5  82.81 ( 83.20)
Epoch: [18][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8082e+00 (1.6950e+00)	Acc@1  51.56 ( 53.81)	Acc@5  80.47 ( 83.14)
Epoch: [18][250/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9605e+00 (1.6974e+00)	Acc@1  50.78 ( 53.71)	Acc@5  84.38 ( 83.11)
Epoch: [18][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6875e+00 (1.7024e+00)	Acc@1  56.25 ( 53.63)	Acc@5  79.69 ( 83.05)
Epoch: [18][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0054e+00 (1.7051e+00)	Acc@1  46.09 ( 53.58)	Acc@5  75.00 ( 83.02)
Epoch: [18][280/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8649e+00 (1.7066e+00)	Acc@1  57.81 ( 53.57)	Acc@5  78.91 ( 82.98)
Epoch: [18][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6601e+00 (1.7062e+00)	Acc@1  58.59 ( 53.60)	Acc@5  80.47 ( 82.98)
Epoch: [18][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6121e+00 (1.7053e+00)	Acc@1  56.25 ( 53.61)	Acc@5  84.38 ( 83.00)
Epoch: [18][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9437e+00 (1.7082e+00)	Acc@1  55.47 ( 53.59)	Acc@5  77.34 ( 82.96)
Epoch: [18][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6720e+00 (1.7101e+00)	Acc@1  57.81 ( 53.51)	Acc@5  84.38 ( 82.97)
Epoch: [18][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5487e+00 (1.7083e+00)	Acc@1  57.81 ( 53.54)	Acc@5  86.72 ( 83.04)
Epoch: [18][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7763e+00 (1.7089e+00)	Acc@1  53.12 ( 53.56)	Acc@5  83.59 ( 83.02)
Epoch: [18][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9419e+00 (1.7125e+00)	Acc@1  48.44 ( 53.48)	Acc@5  82.03 ( 83.00)
Epoch: [18][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9900e+00 (1.7127e+00)	Acc@1  45.31 ( 53.51)	Acc@5  79.69 ( 83.01)
Epoch: [18][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6886e+00 (1.7121e+00)	Acc@1  53.12 ( 53.49)	Acc@5  81.25 ( 83.00)
Epoch: [18][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9748e+00 (1.7130e+00)	Acc@1  43.75 ( 53.45)	Acc@5  77.34 ( 82.99)
Epoch: [18][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8789e+00 (1.7136e+00)	Acc@1  51.25 ( 53.45)	Acc@5  76.25 ( 82.97)
## e[18] optimizer.zero_grad (sum) time: 0.11590933799743652
## e[18]       loss.backward (sum) time: 2.128429651260376
## e[18]      optimizer.step (sum) time: 0.7401871681213379
## epoch[18] training(only) time: 12.962629556655884
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.8699e+00 (1.8699e+00)	Acc@1  49.00 ( 49.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.016 ( 0.029)	Loss 2.1933e+00 (2.0243e+00)	Acc@1  49.00 ( 49.73)	Acc@5  78.00 ( 78.27)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 2.0092e+00 (2.0238e+00)	Acc@1  51.00 ( 49.38)	Acc@5  76.00 ( 78.00)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 2.2042e+00 (2.0341e+00)	Acc@1  45.00 ( 48.81)	Acc@5  77.00 ( 78.00)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 2.1198e+00 (2.0457e+00)	Acc@1  44.00 ( 48.73)	Acc@5  75.00 ( 77.56)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.8904e+00 (2.0536e+00)	Acc@1  51.00 ( 48.35)	Acc@5  78.00 ( 77.08)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.2269e+00 (2.0507e+00)	Acc@1  46.00 ( 48.25)	Acc@5  76.00 ( 77.21)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.3667e+00 (2.0533e+00)	Acc@1  46.00 ( 48.03)	Acc@5  72.00 ( 77.18)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.3091e+00 (2.0771e+00)	Acc@1  46.00 ( 47.75)	Acc@5  71.00 ( 76.69)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 2.2019e+00 (2.0731e+00)	Acc@1  45.00 ( 47.85)	Acc@5  72.00 ( 76.60)
 * Acc@1 47.920 Acc@5 76.670
### epoch[18] execution time: 14.904738187789917
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.184 ( 0.184)	Data  0.156 ( 0.156)	Loss 1.8208e+00 (1.8208e+00)	Acc@1  52.34 ( 52.34)	Acc@5  84.38 ( 84.38)
Epoch: [19][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.5876e+00 (1.6498e+00)	Acc@1  54.69 ( 54.33)	Acc@5  83.59 ( 84.52)
Epoch: [19][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.7016e+00 (1.6488e+00)	Acc@1  49.22 ( 54.84)	Acc@5  87.50 ( 84.67)
Epoch: [19][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.3792e+00 (1.6288e+00)	Acc@1  63.28 ( 55.44)	Acc@5  90.62 ( 84.83)
Epoch: [19][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.5888e+00 (1.6245e+00)	Acc@1  55.47 ( 55.66)	Acc@5  81.25 ( 84.81)
Epoch: [19][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.6434e+00 (1.6354e+00)	Acc@1  56.25 ( 55.68)	Acc@5  85.16 ( 84.77)
Epoch: [19][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4973e+00 (1.6258e+00)	Acc@1  57.03 ( 55.84)	Acc@5  85.94 ( 84.82)
Epoch: [19][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.7242e+00 (1.6386e+00)	Acc@1  55.47 ( 55.67)	Acc@5  82.03 ( 84.44)
Epoch: [19][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.9049e+00 (1.6421e+00)	Acc@1  51.56 ( 55.52)	Acc@5  81.25 ( 84.44)
Epoch: [19][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.9240e+00 (1.6546e+00)	Acc@1  46.88 ( 54.95)	Acc@5  78.91 ( 84.22)
Epoch: [19][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8342e+00 (1.6562e+00)	Acc@1  46.88 ( 54.89)	Acc@5  81.25 ( 84.14)
Epoch: [19][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6498e+00 (1.6559e+00)	Acc@1  51.56 ( 54.86)	Acc@5  87.50 ( 84.08)
Epoch: [19][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6274e+00 (1.6608e+00)	Acc@1  54.69 ( 54.82)	Acc@5  83.59 ( 83.87)
Epoch: [19][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7129e+00 (1.6667e+00)	Acc@1  52.34 ( 54.63)	Acc@5  83.59 ( 83.81)
Epoch: [19][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5393e+00 (1.6687e+00)	Acc@1  56.25 ( 54.57)	Acc@5  87.50 ( 83.82)
Epoch: [19][150/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7344e+00 (1.6681e+00)	Acc@1  56.25 ( 54.48)	Acc@5  81.25 ( 83.81)
Epoch: [19][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5883e+00 (1.6626e+00)	Acc@1  55.47 ( 54.70)	Acc@5  82.81 ( 83.88)
Epoch: [19][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9675e+00 (1.6676e+00)	Acc@1  45.31 ( 54.71)	Acc@5  77.34 ( 83.81)
Epoch: [19][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5543e+00 (1.6716e+00)	Acc@1  59.38 ( 54.64)	Acc@5  86.72 ( 83.71)
Epoch: [19][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6681e+00 (1.6741e+00)	Acc@1  55.47 ( 54.57)	Acc@5  81.25 ( 83.66)
Epoch: [19][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7146e+00 (1.6762e+00)	Acc@1  52.34 ( 54.51)	Acc@5  83.59 ( 83.64)
Epoch: [19][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9347e+00 (1.6739e+00)	Acc@1  50.00 ( 54.61)	Acc@5  79.69 ( 83.68)
Epoch: [19][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6234e+00 (1.6733e+00)	Acc@1  57.03 ( 54.61)	Acc@5  85.16 ( 83.69)
Epoch: [19][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7365e+00 (1.6743e+00)	Acc@1  53.12 ( 54.59)	Acc@5  78.12 ( 83.67)
Epoch: [19][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4328e+00 (1.6732e+00)	Acc@1  59.38 ( 54.63)	Acc@5  86.72 ( 83.70)
Epoch: [19][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5546e+00 (1.6711e+00)	Acc@1  55.47 ( 54.64)	Acc@5  86.72 ( 83.69)
Epoch: [19][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8712e+00 (1.6713e+00)	Acc@1  54.69 ( 54.60)	Acc@5  82.03 ( 83.71)
Epoch: [19][270/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.7174e+00 (1.6703e+00)	Acc@1  52.34 ( 54.60)	Acc@5  80.47 ( 83.70)
Epoch: [19][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8191e+00 (1.6705e+00)	Acc@1  53.12 ( 54.59)	Acc@5  78.91 ( 83.69)
Epoch: [19][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7557e+00 (1.6705e+00)	Acc@1  53.12 ( 54.60)	Acc@5  82.03 ( 83.70)
Epoch: [19][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7418e+00 (1.6710e+00)	Acc@1  53.12 ( 54.58)	Acc@5  82.03 ( 83.65)
Epoch: [19][310/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6243e+00 (1.6702e+00)	Acc@1  55.47 ( 54.62)	Acc@5  82.81 ( 83.64)
Epoch: [19][320/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7297e+00 (1.6726e+00)	Acc@1  50.00 ( 54.54)	Acc@5  82.03 ( 83.58)
Epoch: [19][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9042e+00 (1.6745e+00)	Acc@1  52.34 ( 54.49)	Acc@5  81.25 ( 83.54)
Epoch: [19][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8005e+00 (1.6761e+00)	Acc@1  55.47 ( 54.48)	Acc@5  82.03 ( 83.53)
Epoch: [19][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8124e+00 (1.6777e+00)	Acc@1  51.56 ( 54.46)	Acc@5  82.03 ( 83.49)
Epoch: [19][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5736e+00 (1.6791e+00)	Acc@1  60.94 ( 54.43)	Acc@5  82.03 ( 83.49)
Epoch: [19][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6676e+00 (1.6800e+00)	Acc@1  56.25 ( 54.43)	Acc@5  81.25 ( 83.48)
Epoch: [19][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7270e+00 (1.6814e+00)	Acc@1  58.59 ( 54.43)	Acc@5  80.47 ( 83.42)
Epoch: [19][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7623e+00 (1.6828e+00)	Acc@1  58.75 ( 54.41)	Acc@5  78.75 ( 83.38)
## e[19] optimizer.zero_grad (sum) time: 0.11595702171325684
## e[19]       loss.backward (sum) time: 2.1096136569976807
## e[19]      optimizer.step (sum) time: 0.7228224277496338
## epoch[19] training(only) time: 13.005008220672607
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.8451e+00 (1.8451e+00)	Acc@1  55.00 ( 55.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.018 ( 0.030)	Loss 2.0867e+00 (1.9774e+00)	Acc@1  47.00 ( 50.27)	Acc@5  77.00 ( 78.64)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.9260e+00 (1.9446e+00)	Acc@1  55.00 ( 50.10)	Acc@5  77.00 ( 78.71)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.8421e+00 (1.9303e+00)	Acc@1  51.00 ( 49.84)	Acc@5  83.00 ( 79.03)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.6170e+00 (1.9173e+00)	Acc@1  56.00 ( 49.71)	Acc@5  84.00 ( 79.39)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.8070e+00 (1.9311e+00)	Acc@1  53.00 ( 49.61)	Acc@5  80.00 ( 79.10)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9373e+00 (1.9169e+00)	Acc@1  52.00 ( 49.98)	Acc@5  76.00 ( 79.34)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6913e+00 (1.9114e+00)	Acc@1  58.00 ( 50.08)	Acc@5  81.00 ( 79.41)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.8691e+00 (1.9204e+00)	Acc@1  49.00 ( 49.84)	Acc@5  79.00 ( 79.20)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.9208e+00 (1.9160e+00)	Acc@1  47.00 ( 50.20)	Acc@5  81.00 ( 79.43)
 * Acc@1 50.260 Acc@5 79.610
### epoch[19] execution time: 14.94102954864502
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.182 ( 0.182)	Data  0.156 ( 0.156)	Loss 1.4331e+00 (1.4331e+00)	Acc@1  57.03 ( 57.03)	Acc@5  90.62 ( 90.62)
Epoch: [20][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.6484e+00 (1.6183e+00)	Acc@1  53.12 ( 56.46)	Acc@5  85.94 ( 85.16)
Epoch: [20][ 20/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.5887e+00 (1.6117e+00)	Acc@1  54.69 ( 55.77)	Acc@5  85.94 ( 84.86)
Epoch: [20][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.4021e+00 (1.6169e+00)	Acc@1  60.16 ( 55.65)	Acc@5  83.59 ( 84.27)
Epoch: [20][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.6875e+00 (1.6170e+00)	Acc@1  53.12 ( 55.70)	Acc@5  85.16 ( 84.41)
Epoch: [20][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.3320e+00 (1.6018e+00)	Acc@1  64.84 ( 56.40)	Acc@5  84.38 ( 84.53)
Epoch: [20][ 60/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.7257e+00 (1.6039e+00)	Acc@1  50.00 ( 56.34)	Acc@5  85.16 ( 84.64)
Epoch: [20][ 70/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.004)	Loss 1.9864e+00 (1.6057e+00)	Acc@1  52.34 ( 56.23)	Acc@5  78.12 ( 84.66)
Epoch: [20][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5340e+00 (1.6062e+00)	Acc@1  57.81 ( 56.18)	Acc@5  88.28 ( 84.72)
Epoch: [20][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6708e+00 (1.6104e+00)	Acc@1  54.69 ( 55.99)	Acc@5  82.81 ( 84.76)
Epoch: [20][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6642e+00 (1.6152e+00)	Acc@1  57.81 ( 55.85)	Acc@5  85.94 ( 84.71)
Epoch: [20][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7715e+00 (1.6184e+00)	Acc@1  55.47 ( 55.81)	Acc@5  78.91 ( 84.52)
Epoch: [20][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5738e+00 (1.6203e+00)	Acc@1  57.81 ( 55.76)	Acc@5  89.06 ( 84.53)
Epoch: [20][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8309e+00 (1.6213e+00)	Acc@1  50.78 ( 55.68)	Acc@5  81.25 ( 84.54)
Epoch: [20][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6724e+00 (1.6240e+00)	Acc@1  53.12 ( 55.59)	Acc@5  82.81 ( 84.54)
Epoch: [20][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5294e+00 (1.6251e+00)	Acc@1  59.38 ( 55.66)	Acc@5  88.28 ( 84.53)
Epoch: [20][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6927e+00 (1.6336e+00)	Acc@1  56.25 ( 55.43)	Acc@5  83.59 ( 84.40)
Epoch: [20][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7889e+00 (1.6340e+00)	Acc@1  52.34 ( 55.45)	Acc@5  78.12 ( 84.43)
Epoch: [20][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7896e+00 (1.6383e+00)	Acc@1  49.22 ( 55.40)	Acc@5  83.59 ( 84.36)
Epoch: [20][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7040e+00 (1.6382e+00)	Acc@1  53.91 ( 55.48)	Acc@5  82.03 ( 84.31)
Epoch: [20][200/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.6932e+00 (1.6369e+00)	Acc@1  51.56 ( 55.47)	Acc@5  80.47 ( 84.32)
Epoch: [20][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5834e+00 (1.6367e+00)	Acc@1  58.59 ( 55.53)	Acc@5  82.03 ( 84.33)
Epoch: [20][220/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9062e+00 (1.6415e+00)	Acc@1  46.88 ( 55.39)	Acc@5  78.12 ( 84.19)
Epoch: [20][230/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5493e+00 (1.6444e+00)	Acc@1  64.06 ( 55.37)	Acc@5  86.72 ( 84.19)
Epoch: [20][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5143e+00 (1.6452e+00)	Acc@1  62.50 ( 55.35)	Acc@5  86.72 ( 84.16)
Epoch: [20][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4514e+00 (1.6426e+00)	Acc@1  59.38 ( 55.36)	Acc@5  85.94 ( 84.21)
Epoch: [20][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2917e+00 (1.6382e+00)	Acc@1  60.94 ( 55.41)	Acc@5  91.41 ( 84.32)
Epoch: [20][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4934e+00 (1.6383e+00)	Acc@1  59.38 ( 55.38)	Acc@5  86.72 ( 84.33)
Epoch: [20][280/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7086e+00 (1.6377e+00)	Acc@1  48.44 ( 55.38)	Acc@5  85.16 ( 84.35)
Epoch: [20][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6613e+00 (1.6397e+00)	Acc@1  56.25 ( 55.35)	Acc@5  82.03 ( 84.30)
Epoch: [20][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6031e+00 (1.6399e+00)	Acc@1  54.69 ( 55.35)	Acc@5  87.50 ( 84.27)
Epoch: [20][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7413e+00 (1.6400e+00)	Acc@1  54.69 ( 55.34)	Acc@5  81.25 ( 84.29)
Epoch: [20][320/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4594e+00 (1.6393e+00)	Acc@1  60.16 ( 55.38)	Acc@5  88.28 ( 84.29)
Epoch: [20][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4733e+00 (1.6363e+00)	Acc@1  58.59 ( 55.48)	Acc@5  88.28 ( 84.31)
Epoch: [20][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5998e+00 (1.6375e+00)	Acc@1  52.34 ( 55.46)	Acc@5  84.38 ( 84.28)
Epoch: [20][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7209e+00 (1.6389e+00)	Acc@1  51.56 ( 55.42)	Acc@5  82.81 ( 84.29)
Epoch: [20][360/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8180e+00 (1.6408e+00)	Acc@1  50.00 ( 55.40)	Acc@5  83.59 ( 84.27)
Epoch: [20][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5825e+00 (1.6399e+00)	Acc@1  56.25 ( 55.42)	Acc@5  85.16 ( 84.29)
Epoch: [20][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7952e+00 (1.6401e+00)	Acc@1  52.34 ( 55.39)	Acc@5  82.81 ( 84.26)
Epoch: [20][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7770e+00 (1.6407e+00)	Acc@1  55.00 ( 55.43)	Acc@5  81.25 ( 84.23)
## e[20] optimizer.zero_grad (sum) time: 0.11593222618103027
## e[20]       loss.backward (sum) time: 2.1062092781066895
## e[20]      optimizer.step (sum) time: 0.7352793216705322
## epoch[20] training(only) time: 12.987659215927124
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.7068e+00 (1.7068e+00)	Acc@1  53.00 ( 53.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.9816e+00 (1.8630e+00)	Acc@1  44.00 ( 51.27)	Acc@5  79.00 ( 79.55)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7135e+00 (1.8495e+00)	Acc@1  53.00 ( 51.29)	Acc@5  86.00 ( 79.90)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.6092e+00 (1.8498e+00)	Acc@1  53.00 ( 50.39)	Acc@5  84.00 ( 79.84)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.6535e+00 (1.8461e+00)	Acc@1  57.00 ( 50.56)	Acc@5  80.00 ( 80.07)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5363e+00 (1.8527e+00)	Acc@1  58.00 ( 50.69)	Acc@5  83.00 ( 79.65)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.7456e+00 (1.8539e+00)	Acc@1  53.00 ( 50.66)	Acc@5  81.00 ( 79.74)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 2.0777e+00 (1.8594e+00)	Acc@1  48.00 ( 50.61)	Acc@5  77.00 ( 79.83)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.9326e+00 (1.8722e+00)	Acc@1  49.00 ( 50.36)	Acc@5  80.00 ( 79.80)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 1.8733e+00 (1.8677e+00)	Acc@1  49.00 ( 50.56)	Acc@5  81.00 ( 79.90)
 * Acc@1 50.680 Acc@5 79.820
### epoch[20] execution time: 14.932285785675049
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.183 ( 0.183)	Data  0.155 ( 0.155)	Loss 1.5932e+00 (1.5932e+00)	Acc@1  57.03 ( 57.03)	Acc@5  83.59 ( 83.59)
Epoch: [21][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.6855e+00 (1.6159e+00)	Acc@1  50.78 ( 55.68)	Acc@5  81.25 ( 83.52)
Epoch: [21][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.4443e+00 (1.6040e+00)	Acc@1  60.94 ( 55.92)	Acc@5  87.50 ( 83.85)
Epoch: [21][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.4717e+00 (1.5924e+00)	Acc@1  63.28 ( 56.96)	Acc@5  83.59 ( 84.22)
Epoch: [21][ 40/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.4353e+00 (1.6009e+00)	Acc@1  59.38 ( 56.73)	Acc@5  87.50 ( 84.39)
Epoch: [21][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.5284e+00 (1.6012e+00)	Acc@1  57.03 ( 56.43)	Acc@5  83.59 ( 84.57)
Epoch: [21][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.6216e+00 (1.5935e+00)	Acc@1  56.25 ( 56.54)	Acc@5  85.16 ( 84.62)
Epoch: [21][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.7004e+00 (1.5861e+00)	Acc@1  53.91 ( 56.57)	Acc@5  81.25 ( 84.89)
Epoch: [21][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8245e+00 (1.5888e+00)	Acc@1  50.78 ( 56.41)	Acc@5  85.94 ( 84.80)
Epoch: [21][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5374e+00 (1.5833e+00)	Acc@1  55.47 ( 56.63)	Acc@5  85.94 ( 84.85)
Epoch: [21][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7015e+00 (1.5837e+00)	Acc@1  56.25 ( 56.74)	Acc@5  85.16 ( 84.89)
Epoch: [21][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4673e+00 (1.5787e+00)	Acc@1  57.81 ( 56.81)	Acc@5  86.72 ( 85.00)
Epoch: [21][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5172e+00 (1.5759e+00)	Acc@1  60.16 ( 56.88)	Acc@5  83.59 ( 84.95)
Epoch: [21][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5064e+00 (1.5748e+00)	Acc@1  59.38 ( 56.89)	Acc@5  85.94 ( 85.05)
Epoch: [21][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5836e+00 (1.5697e+00)	Acc@1  56.25 ( 56.98)	Acc@5  85.16 ( 85.12)
Epoch: [21][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5912e+00 (1.5699e+00)	Acc@1  54.69 ( 57.00)	Acc@5  85.16 ( 85.10)
Epoch: [21][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8149e+00 (1.5741e+00)	Acc@1  53.12 ( 56.88)	Acc@5  82.03 ( 85.05)
Epoch: [21][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8934e+00 (1.5769e+00)	Acc@1  52.34 ( 56.86)	Acc@5  76.56 ( 84.97)
Epoch: [21][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5935e+00 (1.5729e+00)	Acc@1  57.03 ( 56.91)	Acc@5  85.94 ( 85.07)
Epoch: [21][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5200e+00 (1.5709e+00)	Acc@1  57.03 ( 56.96)	Acc@5  87.50 ( 85.12)
Epoch: [21][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6646e+00 (1.5712e+00)	Acc@1  53.12 ( 56.95)	Acc@5  83.59 ( 85.19)
Epoch: [21][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5566e+00 (1.5680e+00)	Acc@1  60.94 ( 57.05)	Acc@5  85.94 ( 85.26)
Epoch: [21][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5360e+00 (1.5713e+00)	Acc@1  57.81 ( 56.96)	Acc@5  85.94 ( 85.29)
Epoch: [21][230/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4231e+00 (1.5750e+00)	Acc@1  64.06 ( 56.93)	Acc@5  88.28 ( 85.17)
Epoch: [21][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4570e+00 (1.5754e+00)	Acc@1  59.38 ( 56.90)	Acc@5  85.94 ( 85.17)
Epoch: [21][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7340e+00 (1.5759e+00)	Acc@1  53.91 ( 56.90)	Acc@5  82.81 ( 85.18)
Epoch: [21][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4999e+00 (1.5742e+00)	Acc@1  62.50 ( 56.94)	Acc@5  85.94 ( 85.21)
Epoch: [21][270/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7183e+00 (1.5749e+00)	Acc@1  59.38 ( 56.96)	Acc@5  79.69 ( 85.17)
Epoch: [21][280/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6442e+00 (1.5747e+00)	Acc@1  54.69 ( 56.95)	Acc@5  84.38 ( 85.15)
Epoch: [21][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6309e+00 (1.5766e+00)	Acc@1  55.47 ( 56.90)	Acc@5  85.94 ( 85.11)
Epoch: [21][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6946e+00 (1.5755e+00)	Acc@1  58.59 ( 56.94)	Acc@5  78.91 ( 85.13)
Epoch: [21][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6423e+00 (1.5789e+00)	Acc@1  51.56 ( 56.88)	Acc@5  83.59 ( 85.09)
Epoch: [21][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5140e+00 (1.5816e+00)	Acc@1  59.38 ( 56.80)	Acc@5  82.81 ( 85.03)
Epoch: [21][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3229e+00 (1.5817e+00)	Acc@1  64.84 ( 56.84)	Acc@5  85.16 ( 85.03)
Epoch: [21][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.0389e+00 (1.5835e+00)	Acc@1  50.00 ( 56.85)	Acc@5  79.69 ( 84.98)
Epoch: [21][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6157e+00 (1.5844e+00)	Acc@1  55.47 ( 56.86)	Acc@5  85.16 ( 84.96)
Epoch: [21][360/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6955e+00 (1.5846e+00)	Acc@1  53.12 ( 56.86)	Acc@5  86.72 ( 84.98)
Epoch: [21][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5568e+00 (1.5838e+00)	Acc@1  53.12 ( 56.88)	Acc@5  83.59 ( 84.99)
Epoch: [21][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5866e+00 (1.5869e+00)	Acc@1  57.81 ( 56.81)	Acc@5  85.16 ( 84.93)
Epoch: [21][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5971e+00 (1.5871e+00)	Acc@1  55.00 ( 56.79)	Acc@5  81.25 ( 84.91)
## e[21] optimizer.zero_grad (sum) time: 0.11588406562805176
## e[21]       loss.backward (sum) time: 2.1043190956115723
## e[21]      optimizer.step (sum) time: 0.7247636318206787
## epoch[21] training(only) time: 12.981401920318604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.7144e+00 (1.7144e+00)	Acc@1  58.00 ( 58.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.1436e+00 (1.7967e+00)	Acc@1  45.00 ( 53.36)	Acc@5  76.00 ( 81.82)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7810e+00 (1.7986e+00)	Acc@1  48.00 ( 52.14)	Acc@5  79.00 ( 81.19)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7501e+00 (1.8149e+00)	Acc@1  54.00 ( 51.90)	Acc@5  79.00 ( 80.45)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.7589e+00 (1.8096e+00)	Acc@1  52.00 ( 51.76)	Acc@5  83.00 ( 80.78)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5823e+00 (1.8181e+00)	Acc@1  58.00 ( 51.76)	Acc@5  82.00 ( 80.51)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9713e+00 (1.8130e+00)	Acc@1  51.00 ( 51.61)	Acc@5  77.00 ( 80.41)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.9352e+00 (1.8219e+00)	Acc@1  53.00 ( 51.38)	Acc@5  81.00 ( 80.44)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.1220e+00 (1.8309e+00)	Acc@1  41.00 ( 51.12)	Acc@5  76.00 ( 80.32)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.8848e+00 (1.8259e+00)	Acc@1  51.00 ( 51.27)	Acc@5  80.00 ( 80.55)
 * Acc@1 51.390 Acc@5 80.540
### epoch[21] execution time: 14.91414737701416
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.185 ( 0.185)	Data  0.157 ( 0.157)	Loss 1.4169e+00 (1.4169e+00)	Acc@1  61.72 ( 61.72)	Acc@5  87.50 ( 87.50)
Epoch: [22][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.5469e+00 (1.5632e+00)	Acc@1  58.59 ( 57.95)	Acc@5  84.38 ( 84.80)
Epoch: [22][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.3286e+00 (1.5307e+00)	Acc@1  64.84 ( 58.30)	Acc@5  83.59 ( 85.60)
Epoch: [22][ 30/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.5489e+00 (1.5303e+00)	Acc@1  57.81 ( 58.32)	Acc@5  85.94 ( 85.81)
Epoch: [22][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.6011e+00 (1.5161e+00)	Acc@1  53.91 ( 58.48)	Acc@5  88.28 ( 86.01)
Epoch: [22][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.4049e+00 (1.5105e+00)	Acc@1  58.59 ( 58.66)	Acc@5  90.62 ( 86.00)
Epoch: [22][ 60/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.3546e+00 (1.5092e+00)	Acc@1  58.59 ( 58.64)	Acc@5  87.50 ( 85.98)
Epoch: [22][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.5122e+00 (1.5047e+00)	Acc@1  57.03 ( 58.57)	Acc@5  87.50 ( 86.26)
Epoch: [22][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5199e+00 (1.5133e+00)	Acc@1  63.28 ( 58.39)	Acc@5  85.16 ( 86.01)
Epoch: [22][ 90/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.004)	Loss 1.5073e+00 (1.5189e+00)	Acc@1  60.94 ( 58.22)	Acc@5  84.38 ( 86.05)
Epoch: [22][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5028e+00 (1.5309e+00)	Acc@1  56.25 ( 57.95)	Acc@5  87.50 ( 85.84)
Epoch: [22][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3548e+00 (1.5281e+00)	Acc@1  62.50 ( 57.92)	Acc@5  87.50 ( 85.93)
Epoch: [22][120/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4713e+00 (1.5303e+00)	Acc@1  60.94 ( 57.86)	Acc@5  85.94 ( 85.86)
Epoch: [22][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4827e+00 (1.5329e+00)	Acc@1  50.78 ( 57.73)	Acc@5  86.72 ( 85.83)
Epoch: [22][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4207e+00 (1.5359e+00)	Acc@1  59.38 ( 57.66)	Acc@5  89.06 ( 85.90)
Epoch: [22][150/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4507e+00 (1.5326e+00)	Acc@1  59.38 ( 57.80)	Acc@5  88.28 ( 86.02)
Epoch: [22][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2481e+00 (1.5294e+00)	Acc@1  63.28 ( 57.82)	Acc@5  90.62 ( 86.00)
Epoch: [22][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5262e+00 (1.5286e+00)	Acc@1  57.81 ( 57.84)	Acc@5  84.38 ( 86.00)
Epoch: [22][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6280e+00 (1.5292e+00)	Acc@1  52.34 ( 57.83)	Acc@5  84.38 ( 86.01)
Epoch: [22][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3208e+00 (1.5304e+00)	Acc@1  60.94 ( 57.80)	Acc@5  93.75 ( 85.96)
Epoch: [22][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5680e+00 (1.5356e+00)	Acc@1  57.03 ( 57.68)	Acc@5  84.38 ( 85.93)
Epoch: [22][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3563e+00 (1.5365e+00)	Acc@1  62.50 ( 57.63)	Acc@5  89.84 ( 85.96)
Epoch: [22][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5654e+00 (1.5368e+00)	Acc@1  60.16 ( 57.64)	Acc@5  85.16 ( 85.93)
Epoch: [22][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6235e+00 (1.5403e+00)	Acc@1  57.03 ( 57.53)	Acc@5  85.94 ( 85.90)
Epoch: [22][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2428e+00 (1.5417e+00)	Acc@1  67.19 ( 57.49)	Acc@5  91.41 ( 85.93)
Epoch: [22][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7870e+00 (1.5445e+00)	Acc@1  57.81 ( 57.44)	Acc@5  78.12 ( 85.86)
Epoch: [22][260/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.5414e+00 (1.5443e+00)	Acc@1  60.16 ( 57.48)	Acc@5  88.28 ( 85.84)
Epoch: [22][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4255e+00 (1.5416e+00)	Acc@1  60.94 ( 57.55)	Acc@5  85.94 ( 85.84)
Epoch: [22][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5356e+00 (1.5434e+00)	Acc@1  53.91 ( 57.48)	Acc@5  85.94 ( 85.81)
Epoch: [22][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3563e+00 (1.5432e+00)	Acc@1  67.19 ( 57.51)	Acc@5  86.72 ( 85.78)
Epoch: [22][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6514e+00 (1.5440e+00)	Acc@1  53.91 ( 57.47)	Acc@5  82.81 ( 85.73)
Epoch: [22][310/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5758e+00 (1.5446e+00)	Acc@1  51.56 ( 57.48)	Acc@5  85.16 ( 85.71)
Epoch: [22][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6000e+00 (1.5452e+00)	Acc@1  60.16 ( 57.51)	Acc@5  88.28 ( 85.67)
Epoch: [22][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4195e+00 (1.5460e+00)	Acc@1  59.38 ( 57.49)	Acc@5  88.28 ( 85.63)
Epoch: [22][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6624e+00 (1.5479e+00)	Acc@1  51.56 ( 57.46)	Acc@5  83.59 ( 85.59)
Epoch: [22][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5039e+00 (1.5486e+00)	Acc@1  61.72 ( 57.45)	Acc@5  84.38 ( 85.58)
Epoch: [22][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6749e+00 (1.5465e+00)	Acc@1  53.91 ( 57.54)	Acc@5  81.25 ( 85.59)
Epoch: [22][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8613e+00 (1.5462e+00)	Acc@1  52.34 ( 57.59)	Acc@5  80.47 ( 85.60)
Epoch: [22][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7015e+00 (1.5472e+00)	Acc@1  53.91 ( 57.60)	Acc@5  86.72 ( 85.58)
Epoch: [22][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4821e+00 (1.5491e+00)	Acc@1  66.25 ( 57.59)	Acc@5  85.00 ( 85.55)
## e[22] optimizer.zero_grad (sum) time: 0.11587047576904297
## e[22]       loss.backward (sum) time: 2.0890731811523438
## e[22]      optimizer.step (sum) time: 0.7366471290588379
## epoch[22] training(only) time: 12.962508916854858
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.8006e+00 (1.8006e+00)	Acc@1  59.00 ( 59.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.9213e+00 (1.8718e+00)	Acc@1  49.00 ( 53.27)	Acc@5  84.00 ( 79.82)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7585e+00 (1.8706e+00)	Acc@1  57.00 ( 53.00)	Acc@5  82.00 ( 80.10)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.9595e+00 (1.8790e+00)	Acc@1  47.00 ( 52.10)	Acc@5  84.00 ( 80.10)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.7196e+00 (1.8694e+00)	Acc@1  53.00 ( 52.12)	Acc@5  84.00 ( 80.41)
Test: [ 50/100]	Time  0.025 ( 0.020)	Loss 1.7163e+00 (1.8766e+00)	Acc@1  55.00 ( 51.65)	Acc@5  80.00 ( 80.18)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9786e+00 (1.8669e+00)	Acc@1  51.00 ( 51.95)	Acc@5  81.00 ( 80.34)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.8727e+00 (1.8684e+00)	Acc@1  53.00 ( 52.00)	Acc@5  82.00 ( 80.20)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.9351e+00 (1.8705e+00)	Acc@1  50.00 ( 51.84)	Acc@5  76.00 ( 80.09)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.8840e+00 (1.8679e+00)	Acc@1  53.00 ( 52.00)	Acc@5  78.00 ( 80.11)
 * Acc@1 52.000 Acc@5 80.050
### epoch[22] execution time: 14.897754669189453
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.184 ( 0.184)	Data  0.158 ( 0.158)	Loss 1.6401e+00 (1.6401e+00)	Acc@1  54.69 ( 54.69)	Acc@5  82.03 ( 82.03)
Epoch: [23][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.5530e+00 (1.5507e+00)	Acc@1  63.28 ( 57.10)	Acc@5  86.72 ( 85.09)
Epoch: [23][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.4318e+00 (1.5202e+00)	Acc@1  59.38 ( 58.07)	Acc@5  87.50 ( 85.49)
Epoch: [23][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.5203e+00 (1.4980e+00)	Acc@1  57.81 ( 58.42)	Acc@5  85.16 ( 86.19)
Epoch: [23][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.5113e+00 (1.5106e+00)	Acc@1  60.16 ( 58.31)	Acc@5  85.16 ( 86.26)
Epoch: [23][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.1924e+00 (1.4983e+00)	Acc@1  67.97 ( 58.64)	Acc@5  91.41 ( 86.44)
Epoch: [23][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.6603e+00 (1.4929e+00)	Acc@1  50.00 ( 58.72)	Acc@5  86.72 ( 86.51)
Epoch: [23][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4751e+00 (1.4810e+00)	Acc@1  59.38 ( 58.81)	Acc@5  88.28 ( 86.73)
Epoch: [23][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5112e+00 (1.4830e+00)	Acc@1  59.38 ( 58.82)	Acc@5  85.94 ( 86.72)
Epoch: [23][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7402e+00 (1.4979e+00)	Acc@1  53.12 ( 58.36)	Acc@5  81.25 ( 86.44)
Epoch: [23][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.3749e+00 (1.5002e+00)	Acc@1  58.59 ( 58.26)	Acc@5  87.50 ( 86.34)
Epoch: [23][110/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.5130e+00 (1.4990e+00)	Acc@1  59.38 ( 58.26)	Acc@5  84.38 ( 86.35)
Epoch: [23][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8331e+00 (1.5086e+00)	Acc@1  49.22 ( 58.13)	Acc@5  83.59 ( 86.25)
Epoch: [23][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4349e+00 (1.5125e+00)	Acc@1  57.81 ( 57.99)	Acc@5  88.28 ( 86.24)
Epoch: [23][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4031e+00 (1.5110e+00)	Acc@1  57.81 ( 58.07)	Acc@5  87.50 ( 86.21)
Epoch: [23][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5426e+00 (1.5088e+00)	Acc@1  60.94 ( 58.17)	Acc@5  89.84 ( 86.25)
Epoch: [23][160/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.3400e+00 (1.5099e+00)	Acc@1  61.72 ( 58.18)	Acc@5  85.94 ( 86.29)
Epoch: [23][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7913e+00 (1.5103e+00)	Acc@1  50.00 ( 58.21)	Acc@5  80.47 ( 86.27)
Epoch: [23][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5143e+00 (1.5088e+00)	Acc@1  61.72 ( 58.24)	Acc@5  83.59 ( 86.28)
Epoch: [23][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4828e+00 (1.5082e+00)	Acc@1  57.81 ( 58.22)	Acc@5  90.62 ( 86.26)
Epoch: [23][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4107e+00 (1.5089e+00)	Acc@1  60.16 ( 58.26)	Acc@5  86.72 ( 86.26)
Epoch: [23][210/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.2897e+00 (1.5070e+00)	Acc@1  67.97 ( 58.33)	Acc@5  89.06 ( 86.28)
Epoch: [23][220/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5228e+00 (1.5086e+00)	Acc@1  59.38 ( 58.32)	Acc@5  89.84 ( 86.26)
Epoch: [23][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4140e+00 (1.5087e+00)	Acc@1  59.38 ( 58.40)	Acc@5  89.84 ( 86.26)
Epoch: [23][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3816e+00 (1.5112e+00)	Acc@1  60.94 ( 58.33)	Acc@5  90.62 ( 86.18)
Epoch: [23][250/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4596e+00 (1.5144e+00)	Acc@1  55.47 ( 58.27)	Acc@5  93.75 ( 86.17)
Epoch: [23][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5776e+00 (1.5139e+00)	Acc@1  60.16 ( 58.29)	Acc@5  87.50 ( 86.13)
Epoch: [23][270/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.4709e+00 (1.5116e+00)	Acc@1  62.50 ( 58.37)	Acc@5  85.16 ( 86.13)
Epoch: [23][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6990e+00 (1.5136e+00)	Acc@1  54.69 ( 58.37)	Acc@5  79.69 ( 86.09)
Epoch: [23][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5769e+00 (1.5134e+00)	Acc@1  57.81 ( 58.33)	Acc@5  85.94 ( 86.10)
Epoch: [23][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6609e+00 (1.5174e+00)	Acc@1  57.81 ( 58.28)	Acc@5  83.59 ( 86.05)
Epoch: [23][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8223e+00 (1.5195e+00)	Acc@1  55.47 ( 58.22)	Acc@5  80.47 ( 86.01)
Epoch: [23][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5710e+00 (1.5201e+00)	Acc@1  57.03 ( 58.24)	Acc@5  87.50 ( 86.03)
Epoch: [23][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5178e+00 (1.5208e+00)	Acc@1  60.94 ( 58.21)	Acc@5  82.03 ( 86.01)
Epoch: [23][340/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6387e+00 (1.5228e+00)	Acc@1  60.16 ( 58.19)	Acc@5  84.38 ( 85.96)
Epoch: [23][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5443e+00 (1.5234e+00)	Acc@1  61.72 ( 58.20)	Acc@5  84.38 ( 85.96)
Epoch: [23][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3831e+00 (1.5209e+00)	Acc@1  62.50 ( 58.26)	Acc@5  89.06 ( 85.99)
Epoch: [23][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6260e+00 (1.5191e+00)	Acc@1  57.81 ( 58.32)	Acc@5  84.38 ( 86.00)
Epoch: [23][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6441e+00 (1.5181e+00)	Acc@1  59.38 ( 58.39)	Acc@5  82.81 ( 85.98)
Epoch: [23][390/391]	Time  0.031 ( 0.033)	Data  0.000 ( 0.002)	Loss 1.7119e+00 (1.5187e+00)	Acc@1  53.75 ( 58.39)	Acc@5  80.00 ( 85.99)
## e[23] optimizer.zero_grad (sum) time: 0.11640453338623047
## e[23]       loss.backward (sum) time: 2.1075961589813232
## e[23]      optimizer.step (sum) time: 0.734511137008667
## epoch[23] training(only) time: 13.003098487854004
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.8159e+00 (1.8159e+00)	Acc@1  51.00 ( 51.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.8947e+00 (1.7265e+00)	Acc@1  53.00 ( 53.91)	Acc@5  86.00 ( 82.64)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.5868e+00 (1.7488e+00)	Acc@1  57.00 ( 53.81)	Acc@5  83.00 ( 82.05)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.8687e+00 (1.7574e+00)	Acc@1  51.00 ( 53.45)	Acc@5  81.00 ( 82.19)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.5843e+00 (1.7551e+00)	Acc@1  57.00 ( 53.73)	Acc@5  83.00 ( 82.15)
Test: [ 50/100]	Time  0.017 ( 0.019)	Loss 1.6519e+00 (1.7673e+00)	Acc@1  56.00 ( 53.43)	Acc@5  84.00 ( 81.80)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.7330e+00 (1.7552e+00)	Acc@1  54.00 ( 53.82)	Acc@5  86.00 ( 82.13)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.8531e+00 (1.7530e+00)	Acc@1  57.00 ( 53.62)	Acc@5  82.00 ( 82.31)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.8701e+00 (1.7569e+00)	Acc@1  52.00 ( 53.59)	Acc@5  80.00 ( 82.20)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 1.9078e+00 (1.7472e+00)	Acc@1  50.00 ( 53.74)	Acc@5  79.00 ( 82.22)
 * Acc@1 54.030 Acc@5 82.380
### epoch[23] execution time: 14.920586109161377
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.189 ( 0.189)	Data  0.161 ( 0.161)	Loss 1.3119e+00 (1.3119e+00)	Acc@1  59.38 ( 59.38)	Acc@5  87.50 ( 87.50)
Epoch: [24][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.2182e+00 (1.3847e+00)	Acc@1  64.84 ( 60.44)	Acc@5  91.41 ( 87.22)
Epoch: [24][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.2824e+00 (1.4186e+00)	Acc@1  64.84 ( 60.08)	Acc@5  91.41 ( 87.57)
Epoch: [24][ 30/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.007)	Loss 1.5225e+00 (1.4262e+00)	Acc@1  54.69 ( 60.26)	Acc@5  86.72 ( 87.12)
Epoch: [24][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.4481e+00 (1.4195e+00)	Acc@1  57.81 ( 60.50)	Acc@5  84.38 ( 87.29)
Epoch: [24][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.2637e+00 (1.4053e+00)	Acc@1  64.84 ( 60.83)	Acc@5  89.06 ( 87.38)
Epoch: [24][ 60/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.005)	Loss 1.5483e+00 (1.3995e+00)	Acc@1  57.81 ( 60.94)	Acc@5  82.81 ( 87.44)
Epoch: [24][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.7691e+00 (1.4170e+00)	Acc@1  57.03 ( 60.50)	Acc@5  82.81 ( 87.25)
Epoch: [24][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.2564e+00 (1.4300e+00)	Acc@1  63.28 ( 60.13)	Acc@5  89.06 ( 87.13)
Epoch: [24][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4674e+00 (1.4278e+00)	Acc@1  57.03 ( 60.20)	Acc@5  83.59 ( 87.10)
Epoch: [24][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4381e+00 (1.4264e+00)	Acc@1  59.38 ( 60.26)	Acc@5  85.16 ( 87.21)
Epoch: [24][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4999e+00 (1.4291e+00)	Acc@1  59.38 ( 60.15)	Acc@5  85.94 ( 87.04)
Epoch: [24][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6266e+00 (1.4338e+00)	Acc@1  59.38 ( 60.09)	Acc@5  80.47 ( 86.88)
Epoch: [24][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8577e+00 (1.4399e+00)	Acc@1  53.91 ( 59.94)	Acc@5  79.69 ( 86.73)
Epoch: [24][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5315e+00 (1.4404e+00)	Acc@1  60.16 ( 60.08)	Acc@5  85.94 ( 86.77)
Epoch: [24][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6093e+00 (1.4434e+00)	Acc@1  51.56 ( 60.08)	Acc@5  88.28 ( 86.77)
Epoch: [24][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5360e+00 (1.4481e+00)	Acc@1  55.47 ( 59.96)	Acc@5  89.84 ( 86.69)
Epoch: [24][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3613e+00 (1.4493e+00)	Acc@1  60.94 ( 59.96)	Acc@5  88.28 ( 86.73)
Epoch: [24][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7507e+00 (1.4497e+00)	Acc@1  49.22 ( 59.95)	Acc@5  82.03 ( 86.78)
Epoch: [24][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4636e+00 (1.4500e+00)	Acc@1  60.16 ( 59.88)	Acc@5  82.03 ( 86.80)
Epoch: [24][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6709e+00 (1.4587e+00)	Acc@1  57.03 ( 59.69)	Acc@5  83.59 ( 86.67)
Epoch: [24][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3291e+00 (1.4618e+00)	Acc@1  58.59 ( 59.65)	Acc@5  88.28 ( 86.59)
Epoch: [24][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4125e+00 (1.4647e+00)	Acc@1  60.16 ( 59.54)	Acc@5  87.50 ( 86.58)
Epoch: [24][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4356e+00 (1.4642e+00)	Acc@1  62.50 ( 59.52)	Acc@5  88.28 ( 86.57)
Epoch: [24][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4913e+00 (1.4633e+00)	Acc@1  60.16 ( 59.55)	Acc@5  89.84 ( 86.58)
Epoch: [24][250/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3461e+00 (1.4634e+00)	Acc@1  57.03 ( 59.56)	Acc@5  89.84 ( 86.58)
Epoch: [24][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3583e+00 (1.4653e+00)	Acc@1  57.81 ( 59.54)	Acc@5  90.62 ( 86.57)
Epoch: [24][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4926e+00 (1.4656e+00)	Acc@1  61.72 ( 59.52)	Acc@5  87.50 ( 86.59)
Epoch: [24][280/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5271e+00 (1.4699e+00)	Acc@1  56.25 ( 59.48)	Acc@5  87.50 ( 86.52)
Epoch: [24][290/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4657e+00 (1.4732e+00)	Acc@1  59.38 ( 59.40)	Acc@5  83.59 ( 86.44)
Epoch: [24][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3867e+00 (1.4733e+00)	Acc@1  63.28 ( 59.41)	Acc@5  86.72 ( 86.43)
Epoch: [24][310/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.3113e+00 (1.4746e+00)	Acc@1  63.28 ( 59.45)	Acc@5  88.28 ( 86.41)
Epoch: [24][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4816e+00 (1.4733e+00)	Acc@1  59.38 ( 59.46)	Acc@5  89.84 ( 86.48)
Epoch: [24][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4147e+00 (1.4733e+00)	Acc@1  64.06 ( 59.46)	Acc@5  87.50 ( 86.49)
Epoch: [24][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0839e+00 (1.4711e+00)	Acc@1  74.22 ( 59.52)	Acc@5  89.84 ( 86.51)
Epoch: [24][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7479e+00 (1.4746e+00)	Acc@1  57.03 ( 59.50)	Acc@5  85.94 ( 86.44)
Epoch: [24][360/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.5149e+00 (1.4765e+00)	Acc@1  57.03 ( 59.46)	Acc@5  82.03 ( 86.40)
Epoch: [24][370/391]	Time  0.032 ( 0.033)	Data  0.000 ( 0.002)	Loss 1.4968e+00 (1.4786e+00)	Acc@1  58.59 ( 59.43)	Acc@5  83.59 ( 86.36)
Epoch: [24][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4462e+00 (1.4781e+00)	Acc@1  62.50 ( 59.43)	Acc@5  85.16 ( 86.37)
Epoch: [24][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7559e+00 (1.4780e+00)	Acc@1  53.75 ( 59.45)	Acc@5  85.00 ( 86.40)
## e[24] optimizer.zero_grad (sum) time: 0.11615252494812012
## e[24]       loss.backward (sum) time: 2.141287326812744
## e[24]      optimizer.step (sum) time: 0.7386195659637451
## epoch[24] training(only) time: 12.976373195648193
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.5773e+00 (1.5773e+00)	Acc@1  62.00 ( 62.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.021 ( 0.030)	Loss 2.1165e+00 (1.8071e+00)	Acc@1  46.00 ( 55.45)	Acc@5  82.00 ( 81.00)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.6234e+00 (1.8273e+00)	Acc@1  60.00 ( 53.67)	Acc@5  85.00 ( 80.95)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8415e+00 (1.8203e+00)	Acc@1  48.00 ( 53.42)	Acc@5  82.00 ( 80.71)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.7409e+00 (1.8112e+00)	Acc@1  50.00 ( 53.59)	Acc@5  80.00 ( 80.73)
Test: [ 50/100]	Time  0.019 ( 0.020)	Loss 1.8444e+00 (1.8313e+00)	Acc@1  52.00 ( 53.33)	Acc@5  77.00 ( 80.24)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.9834e+00 (1.8195e+00)	Acc@1  51.00 ( 53.38)	Acc@5  81.00 ( 80.41)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 1.9111e+00 (1.8182e+00)	Acc@1  54.00 ( 53.35)	Acc@5  79.00 ( 80.42)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.8931e+00 (1.8315e+00)	Acc@1  59.00 ( 53.28)	Acc@5  77.00 ( 80.21)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.0168e+00 (1.8300e+00)	Acc@1  48.00 ( 53.34)	Acc@5  78.00 ( 80.27)
 * Acc@1 53.440 Acc@5 80.350
### epoch[24] execution time: 14.947642087936401
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.169 ( 0.169)	Data  0.144 ( 0.144)	Loss 1.5622e+00 (1.5622e+00)	Acc@1  56.25 ( 56.25)	Acc@5  85.94 ( 85.94)
Epoch: [25][ 10/391]	Time  0.031 ( 0.046)	Data  0.002 ( 0.015)	Loss 1.7066e+00 (1.4479e+00)	Acc@1  50.78 ( 61.36)	Acc@5  82.03 ( 86.65)
Epoch: [25][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.3490e+00 (1.4339e+00)	Acc@1  60.94 ( 61.31)	Acc@5  90.62 ( 86.20)
Epoch: [25][ 30/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.3749e+00 (1.4048e+00)	Acc@1  60.94 ( 61.44)	Acc@5  92.97 ( 87.15)
Epoch: [25][ 40/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.2187e+00 (1.4043e+00)	Acc@1  59.38 ( 61.89)	Acc@5  95.31 ( 87.44)
Epoch: [25][ 50/391]	Time  0.033 ( 0.035)	Data  0.002 ( 0.005)	Loss 1.4025e+00 (1.4124e+00)	Acc@1  60.94 ( 61.57)	Acc@5  87.50 ( 87.44)
Epoch: [25][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2096e+00 (1.4141e+00)	Acc@1  71.88 ( 61.28)	Acc@5  89.06 ( 87.44)
Epoch: [25][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.3121e+00 (1.4176e+00)	Acc@1  61.72 ( 61.06)	Acc@5  90.62 ( 87.36)
Epoch: [25][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5638e+00 (1.4167e+00)	Acc@1  59.38 ( 60.94)	Acc@5  82.81 ( 87.42)
Epoch: [25][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4743e+00 (1.4168e+00)	Acc@1  63.28 ( 61.13)	Acc@5  85.94 ( 87.42)
Epoch: [25][100/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.4750e+00 (1.4191e+00)	Acc@1  56.25 ( 61.15)	Acc@5  88.28 ( 87.41)
Epoch: [25][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3150e+00 (1.4120e+00)	Acc@1  61.72 ( 61.18)	Acc@5  86.72 ( 87.54)
Epoch: [25][120/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.4141e+00 (1.4162e+00)	Acc@1  59.38 ( 61.12)	Acc@5  89.84 ( 87.44)
Epoch: [25][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6272e+00 (1.4160e+00)	Acc@1  56.25 ( 61.14)	Acc@5  85.94 ( 87.49)
Epoch: [25][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.9924e-01 (1.4137e+00)	Acc@1  69.53 ( 61.24)	Acc@5  94.53 ( 87.44)
Epoch: [25][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2521e+00 (1.4107e+00)	Acc@1  65.62 ( 61.37)	Acc@5  87.50 ( 87.49)
Epoch: [25][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5181e+00 (1.4208e+00)	Acc@1  57.81 ( 61.17)	Acc@5  83.59 ( 87.33)
Epoch: [25][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5360e+00 (1.4217e+00)	Acc@1  57.81 ( 61.16)	Acc@5  84.38 ( 87.30)
Epoch: [25][180/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.3868e+00 (1.4237e+00)	Acc@1  60.16 ( 61.06)	Acc@5  88.28 ( 87.36)
Epoch: [25][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1891e+00 (1.4283e+00)	Acc@1  66.41 ( 60.96)	Acc@5  87.50 ( 87.25)
Epoch: [25][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4190e+00 (1.4331e+00)	Acc@1  58.59 ( 60.78)	Acc@5  89.06 ( 87.19)
Epoch: [25][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1930e+00 (1.4337e+00)	Acc@1  66.41 ( 60.74)	Acc@5  92.19 ( 87.25)
Epoch: [25][220/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4929e+00 (1.4300e+00)	Acc@1  57.81 ( 60.76)	Acc@5  88.28 ( 87.28)
Epoch: [25][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6211e+00 (1.4336e+00)	Acc@1  56.25 ( 60.70)	Acc@5  84.38 ( 87.27)
Epoch: [25][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4534e+00 (1.4348e+00)	Acc@1  58.59 ( 60.68)	Acc@5  86.72 ( 87.24)
Epoch: [25][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4808e+00 (1.4320e+00)	Acc@1  60.94 ( 60.72)	Acc@5  87.50 ( 87.28)
Epoch: [25][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6604e+00 (1.4334e+00)	Acc@1  59.38 ( 60.72)	Acc@5  83.59 ( 87.23)
Epoch: [25][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3539e+00 (1.4338e+00)	Acc@1  60.16 ( 60.71)	Acc@5  87.50 ( 87.19)
Epoch: [25][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4381e+00 (1.4342e+00)	Acc@1  63.28 ( 60.73)	Acc@5  85.16 ( 87.17)
Epoch: [25][290/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6547e+00 (1.4329e+00)	Acc@1  53.12 ( 60.77)	Acc@5  83.59 ( 87.20)
Epoch: [25][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5834e+00 (1.4345e+00)	Acc@1  60.94 ( 60.74)	Acc@5  82.81 ( 87.18)
Epoch: [25][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7275e+00 (1.4359e+00)	Acc@1  53.91 ( 60.70)	Acc@5  82.81 ( 87.17)
Epoch: [25][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6900e+00 (1.4372e+00)	Acc@1  53.91 ( 60.65)	Acc@5  84.38 ( 87.15)
Epoch: [25][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2850e+00 (1.4377e+00)	Acc@1  64.84 ( 60.68)	Acc@5  89.06 ( 87.15)
Epoch: [25][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2142e+00 (1.4376e+00)	Acc@1  65.62 ( 60.68)	Acc@5  93.75 ( 87.16)
Epoch: [25][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2469e+00 (1.4376e+00)	Acc@1  67.19 ( 60.68)	Acc@5  89.84 ( 87.17)
Epoch: [25][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3690e+00 (1.4398e+00)	Acc@1  61.72 ( 60.61)	Acc@5  89.84 ( 87.15)
Epoch: [25][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5259e+00 (1.4427e+00)	Acc@1  58.59 ( 60.51)	Acc@5  87.50 ( 87.09)
Epoch: [25][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4041e+00 (1.4455e+00)	Acc@1  64.84 ( 60.45)	Acc@5  88.28 ( 87.05)
Epoch: [25][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5750e+00 (1.4459e+00)	Acc@1  52.50 ( 60.46)	Acc@5  87.50 ( 87.04)
## e[25] optimizer.zero_grad (sum) time: 0.11598920822143555
## e[25]       loss.backward (sum) time: 2.120985507965088
## e[25]      optimizer.step (sum) time: 0.7437524795532227
## epoch[25] training(only) time: 12.96555471420288
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.6909e+00 (1.6909e+00)	Acc@1  58.00 ( 58.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.9126e+00 (1.7434e+00)	Acc@1  52.00 ( 56.36)	Acc@5  81.00 ( 81.55)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.6295e+00 (1.7073e+00)	Acc@1  56.00 ( 55.86)	Acc@5  81.00 ( 82.71)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.6996e+00 (1.7081e+00)	Acc@1  55.00 ( 55.13)	Acc@5  84.00 ( 82.68)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.5178e+00 (1.6989e+00)	Acc@1  61.00 ( 55.27)	Acc@5  88.00 ( 83.17)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6518e+00 (1.7185e+00)	Acc@1  58.00 ( 54.96)	Acc@5  85.00 ( 82.82)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.7232e+00 (1.7129e+00)	Acc@1  55.00 ( 55.00)	Acc@5  79.00 ( 82.89)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6143e+00 (1.7105e+00)	Acc@1  54.00 ( 54.92)	Acc@5  79.00 ( 82.72)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.9912e+00 (1.7187e+00)	Acc@1  51.00 ( 54.90)	Acc@5  78.00 ( 82.54)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7793e+00 (1.7116e+00)	Acc@1  51.00 ( 55.05)	Acc@5  83.00 ( 82.73)
 * Acc@1 55.100 Acc@5 82.920
### epoch[25] execution time: 14.891964673995972
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.186 ( 0.186)	Data  0.158 ( 0.158)	Loss 1.4810e+00 (1.4810e+00)	Acc@1  53.91 ( 53.91)	Acc@5  84.38 ( 84.38)
Epoch: [26][ 10/391]	Time  0.031 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.4358e+00 (1.3692e+00)	Acc@1  61.72 ( 61.43)	Acc@5  89.84 ( 87.07)
Epoch: [26][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.5589e+00 (1.4026e+00)	Acc@1  60.16 ( 60.97)	Acc@5  85.16 ( 86.90)
Epoch: [26][ 30/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.007)	Loss 1.7064e+00 (1.4024e+00)	Acc@1  51.56 ( 60.71)	Acc@5  82.81 ( 87.53)
Epoch: [26][ 40/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.2164e+00 (1.3842e+00)	Acc@1  68.75 ( 61.28)	Acc@5  89.06 ( 88.09)
Epoch: [26][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.3999e+00 (1.3897e+00)	Acc@1  61.72 ( 61.11)	Acc@5  87.50 ( 88.11)
Epoch: [26][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4112e+00 (1.3905e+00)	Acc@1  56.25 ( 60.99)	Acc@5  87.50 ( 88.00)
Epoch: [26][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2795e+00 (1.3932e+00)	Acc@1  59.38 ( 61.01)	Acc@5  90.62 ( 87.80)
Epoch: [26][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5556e+00 (1.3901e+00)	Acc@1  59.38 ( 61.11)	Acc@5  84.38 ( 87.81)
Epoch: [26][ 90/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.004)	Loss 1.4794e+00 (1.4038e+00)	Acc@1  57.81 ( 60.73)	Acc@5  88.28 ( 87.64)
Epoch: [26][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4990e+00 (1.4087e+00)	Acc@1  60.94 ( 60.68)	Acc@5  87.50 ( 87.62)
Epoch: [26][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2402e+00 (1.4020e+00)	Acc@1  60.94 ( 60.73)	Acc@5  96.88 ( 87.77)
Epoch: [26][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3792e+00 (1.3972e+00)	Acc@1  64.06 ( 60.81)	Acc@5  85.94 ( 87.87)
Epoch: [26][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4503e+00 (1.3927e+00)	Acc@1  60.94 ( 60.89)	Acc@5  87.50 ( 87.89)
Epoch: [26][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6258e+00 (1.3921e+00)	Acc@1  58.59 ( 60.93)	Acc@5  82.81 ( 87.93)
Epoch: [26][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4246e+00 (1.3930e+00)	Acc@1  62.50 ( 60.94)	Acc@5  86.72 ( 87.91)
Epoch: [26][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0892e+00 (1.3937e+00)	Acc@1  67.19 ( 60.96)	Acc@5  92.97 ( 87.89)
Epoch: [26][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3321e+00 (1.3944e+00)	Acc@1  65.62 ( 60.97)	Acc@5  85.94 ( 87.87)
Epoch: [26][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4102e+00 (1.3971e+00)	Acc@1  60.16 ( 60.94)	Acc@5  87.50 ( 87.78)
Epoch: [26][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6990e+00 (1.4031e+00)	Acc@1  53.12 ( 60.78)	Acc@5  87.50 ( 87.73)
Epoch: [26][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5651e+00 (1.4064e+00)	Acc@1  60.16 ( 60.80)	Acc@5  83.59 ( 87.66)
Epoch: [26][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4684e+00 (1.4037e+00)	Acc@1  58.59 ( 60.87)	Acc@5  88.28 ( 87.68)
Epoch: [26][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2637e+00 (1.4025e+00)	Acc@1  62.50 ( 60.92)	Acc@5  89.84 ( 87.67)
Epoch: [26][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7000e+00 (1.4030e+00)	Acc@1  54.69 ( 60.96)	Acc@5  86.72 ( 87.65)
Epoch: [26][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4389e+00 (1.4052e+00)	Acc@1  60.94 ( 60.96)	Acc@5  89.06 ( 87.61)
Epoch: [26][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3314e+00 (1.4053e+00)	Acc@1  56.25 ( 60.99)	Acc@5  89.84 ( 87.57)
Epoch: [26][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3448e+00 (1.4052e+00)	Acc@1  62.50 ( 60.95)	Acc@5  86.72 ( 87.63)
Epoch: [26][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4479e+00 (1.4088e+00)	Acc@1  57.03 ( 60.92)	Acc@5  86.72 ( 87.52)
Epoch: [26][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4727e+00 (1.4130e+00)	Acc@1  63.28 ( 60.85)	Acc@5  87.50 ( 87.49)
Epoch: [26][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6091e+00 (1.4145e+00)	Acc@1  57.03 ( 60.81)	Acc@5  82.81 ( 87.45)
Epoch: [26][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5328e+00 (1.4146e+00)	Acc@1  59.38 ( 60.81)	Acc@5  86.72 ( 87.43)
Epoch: [26][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2982e+00 (1.4158e+00)	Acc@1  66.41 ( 60.77)	Acc@5  90.62 ( 87.42)
Epoch: [26][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1833e+00 (1.4141e+00)	Acc@1  68.75 ( 60.77)	Acc@5  89.06 ( 87.44)
Epoch: [26][330/391]	Time  0.033 ( 0.033)	Data  0.004 ( 0.003)	Loss 1.3317e+00 (1.4140e+00)	Acc@1  60.16 ( 60.79)	Acc@5  89.06 ( 87.45)
Epoch: [26][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3364e+00 (1.4149e+00)	Acc@1  63.28 ( 60.79)	Acc@5  89.84 ( 87.42)
Epoch: [26][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4426e+00 (1.4169e+00)	Acc@1  61.72 ( 60.74)	Acc@5  87.50 ( 87.37)
Epoch: [26][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5305e+00 (1.4174e+00)	Acc@1  55.47 ( 60.75)	Acc@5  83.59 ( 87.36)
Epoch: [26][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3294e+00 (1.4193e+00)	Acc@1  57.81 ( 60.72)	Acc@5  89.06 ( 87.34)
Epoch: [26][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4160e+00 (1.4195e+00)	Acc@1  60.94 ( 60.72)	Acc@5  85.94 ( 87.36)
Epoch: [26][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8601e+00 (1.4206e+00)	Acc@1  55.00 ( 60.72)	Acc@5  82.50 ( 87.35)
## e[26] optimizer.zero_grad (sum) time: 0.11780285835266113
## e[26]       loss.backward (sum) time: 2.065850257873535
## e[26]      optimizer.step (sum) time: 0.7282054424285889
## epoch[26] training(only) time: 13.004842042922974
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.8123e+00 (1.8123e+00)	Acc@1  52.00 ( 52.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.9080e+00 (1.7789e+00)	Acc@1  47.00 ( 55.27)	Acc@5  82.00 ( 82.09)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7627e+00 (1.7581e+00)	Acc@1  60.00 ( 55.81)	Acc@5  80.00 ( 81.71)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.9434e+00 (1.7759e+00)	Acc@1  52.00 ( 54.26)	Acc@5  82.00 ( 81.74)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.7204e+00 (1.7688e+00)	Acc@1  55.00 ( 54.51)	Acc@5  83.00 ( 82.05)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.7234e+00 (1.7892e+00)	Acc@1  55.00 ( 54.06)	Acc@5  83.00 ( 81.57)
Test: [ 60/100]	Time  0.020 ( 0.019)	Loss 1.7014e+00 (1.7840e+00)	Acc@1  55.00 ( 54.13)	Acc@5  85.00 ( 81.74)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.8020e+00 (1.7897e+00)	Acc@1  49.00 ( 53.87)	Acc@5  81.00 ( 81.59)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.8996e+00 (1.7947e+00)	Acc@1  51.00 ( 53.93)	Acc@5  76.00 ( 81.42)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.8859e+00 (1.7875e+00)	Acc@1  54.00 ( 54.03)	Acc@5  80.00 ( 81.52)
 * Acc@1 54.170 Acc@5 81.590
### epoch[26] execution time: 14.950024127960205
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.184 ( 0.184)	Data  0.156 ( 0.156)	Loss 1.3054e+00 (1.3054e+00)	Acc@1  65.62 ( 65.62)	Acc@5  89.84 ( 89.84)
Epoch: [27][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.1610e+00 (1.3423e+00)	Acc@1  64.06 ( 61.93)	Acc@5  94.53 ( 89.20)
Epoch: [27][ 20/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.1723e+00 (1.3677e+00)	Acc@1  69.53 ( 62.50)	Acc@5  92.19 ( 88.73)
Epoch: [27][ 30/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.007)	Loss 1.5109e+00 (1.3701e+00)	Acc@1  51.56 ( 61.67)	Acc@5  88.28 ( 88.48)
Epoch: [27][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.4571e+00 (1.3722e+00)	Acc@1  60.94 ( 61.59)	Acc@5  86.72 ( 88.57)
Epoch: [27][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.0903e+00 (1.3505e+00)	Acc@1  66.41 ( 61.87)	Acc@5  91.41 ( 89.05)
Epoch: [27][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.3498e+00 (1.3449e+00)	Acc@1  64.84 ( 62.09)	Acc@5  90.62 ( 89.09)
Epoch: [27][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4963e+00 (1.3514e+00)	Acc@1  57.03 ( 62.07)	Acc@5  85.94 ( 88.88)
Epoch: [27][ 80/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6004e+00 (1.3539e+00)	Acc@1  59.38 ( 62.01)	Acc@5  82.03 ( 88.83)
Epoch: [27][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4666e+00 (1.3532e+00)	Acc@1  60.16 ( 62.23)	Acc@5  85.94 ( 88.68)
Epoch: [27][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5323e+00 (1.3623e+00)	Acc@1  59.38 ( 62.05)	Acc@5  87.50 ( 88.54)
Epoch: [27][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3962e+00 (1.3663e+00)	Acc@1  59.38 ( 62.06)	Acc@5  85.16 ( 88.51)
Epoch: [27][120/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5292e+00 (1.3751e+00)	Acc@1  57.03 ( 61.89)	Acc@5  90.62 ( 88.40)
Epoch: [27][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6211e+00 (1.3771e+00)	Acc@1  60.94 ( 61.85)	Acc@5  82.03 ( 88.28)
Epoch: [27][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4708e+00 (1.3760e+00)	Acc@1  58.59 ( 61.87)	Acc@5  86.72 ( 88.20)
Epoch: [27][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5393e+00 (1.3824e+00)	Acc@1  59.38 ( 61.75)	Acc@5  85.16 ( 88.03)
Epoch: [27][160/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6882e+00 (1.3856e+00)	Acc@1  56.25 ( 61.74)	Acc@5  81.25 ( 87.97)
Epoch: [27][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4427e+00 (1.3865e+00)	Acc@1  63.28 ( 61.64)	Acc@5  88.28 ( 88.01)
Epoch: [27][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4283e+00 (1.3916e+00)	Acc@1  55.47 ( 61.52)	Acc@5  87.50 ( 87.91)
Epoch: [27][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3859e+00 (1.3840e+00)	Acc@1  60.94 ( 61.71)	Acc@5  89.06 ( 88.01)
Epoch: [27][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2212e+00 (1.3865e+00)	Acc@1  64.06 ( 61.71)	Acc@5  90.62 ( 87.98)
Epoch: [27][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2646e+00 (1.3891e+00)	Acc@1  64.06 ( 61.63)	Acc@5  90.62 ( 87.97)
Epoch: [27][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4450e+00 (1.3916e+00)	Acc@1  60.16 ( 61.58)	Acc@5  87.50 ( 87.97)
Epoch: [27][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3231e+00 (1.3921e+00)	Acc@1  60.16 ( 61.58)	Acc@5  93.75 ( 87.93)
Epoch: [27][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6444e+00 (1.3900e+00)	Acc@1  57.03 ( 61.65)	Acc@5  83.59 ( 87.96)
Epoch: [27][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4569e+00 (1.3903e+00)	Acc@1  66.41 ( 61.66)	Acc@5  87.50 ( 87.96)
Epoch: [27][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4138e+00 (1.3905e+00)	Acc@1  64.06 ( 61.61)	Acc@5  87.50 ( 87.95)
Epoch: [27][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5096e+00 (1.3888e+00)	Acc@1  59.38 ( 61.68)	Acc@5  85.16 ( 87.99)
Epoch: [27][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3790e+00 (1.3880e+00)	Acc@1  59.38 ( 61.67)	Acc@5  89.84 ( 88.01)
Epoch: [27][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3980e+00 (1.3861e+00)	Acc@1  62.50 ( 61.72)	Acc@5  89.06 ( 88.01)
Epoch: [27][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4099e+00 (1.3860e+00)	Acc@1  62.50 ( 61.78)	Acc@5  87.50 ( 88.01)
Epoch: [27][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3452e+00 (1.3868e+00)	Acc@1  58.59 ( 61.75)	Acc@5  90.62 ( 88.00)
Epoch: [27][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3480e+00 (1.3864e+00)	Acc@1  64.06 ( 61.75)	Acc@5  85.94 ( 88.02)
Epoch: [27][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2326e+00 (1.3863e+00)	Acc@1  63.28 ( 61.75)	Acc@5  90.62 ( 88.03)
Epoch: [27][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2838e+00 (1.3861e+00)	Acc@1  62.50 ( 61.78)	Acc@5  89.84 ( 88.05)
Epoch: [27][350/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.8285e+00 (1.3881e+00)	Acc@1  46.09 ( 61.73)	Acc@5  82.81 ( 88.03)
Epoch: [27][360/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.2635e+00 (1.3893e+00)	Acc@1  64.84 ( 61.69)	Acc@5  89.84 ( 88.04)
Epoch: [27][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6274e+00 (1.3900e+00)	Acc@1  55.47 ( 61.65)	Acc@5  84.38 ( 88.02)
Epoch: [27][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3245e+00 (1.3912e+00)	Acc@1  60.94 ( 61.62)	Acc@5  84.38 ( 87.99)
Epoch: [27][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2800e+00 (1.3923e+00)	Acc@1  60.00 ( 61.59)	Acc@5  87.50 ( 87.94)
## e[27] optimizer.zero_grad (sum) time: 0.11606955528259277
## e[27]       loss.backward (sum) time: 2.1270673274993896
## e[27]      optimizer.step (sum) time: 0.7272593975067139
## epoch[27] training(only) time: 12.998430013656616
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.8443e+00 (1.8443e+00)	Acc@1  54.00 ( 54.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.9108e+00 (1.8636e+00)	Acc@1  49.00 ( 53.55)	Acc@5  83.00 ( 81.18)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.9884e+00 (1.8521e+00)	Acc@1  49.00 ( 53.29)	Acc@5  76.00 ( 81.48)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 2.0743e+00 (1.8814e+00)	Acc@1  50.00 ( 53.26)	Acc@5  82.00 ( 81.13)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.8542e+00 (1.8803e+00)	Acc@1  50.00 ( 53.34)	Acc@5  81.00 ( 80.88)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.7228e+00 (1.8875e+00)	Acc@1  57.00 ( 53.16)	Acc@5  81.00 ( 80.69)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9088e+00 (1.8743e+00)	Acc@1  47.00 ( 53.21)	Acc@5  81.00 ( 80.97)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.9243e+00 (1.8696e+00)	Acc@1  52.00 ( 53.30)	Acc@5  83.00 ( 81.11)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.0482e+00 (1.8690e+00)	Acc@1  54.00 ( 53.28)	Acc@5  79.00 ( 81.01)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.4072e+00 (1.8630e+00)	Acc@1  45.00 ( 53.54)	Acc@5  75.00 ( 81.19)
 * Acc@1 53.530 Acc@5 81.120
### epoch[27] execution time: 14.952855110168457
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.180 ( 0.180)	Data  0.149 ( 0.149)	Loss 1.3794e+00 (1.3794e+00)	Acc@1  54.69 ( 54.69)	Acc@5  87.50 ( 87.50)
Epoch: [28][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.3322e+00 (1.3267e+00)	Acc@1  61.72 ( 61.86)	Acc@5  92.19 ( 88.99)
Epoch: [28][ 20/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.3871e+00 (1.3165e+00)	Acc@1  67.19 ( 63.10)	Acc@5  84.38 ( 89.40)
Epoch: [28][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.2349e+00 (1.3326e+00)	Acc@1  65.62 ( 63.00)	Acc@5  89.84 ( 89.14)
Epoch: [28][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.2971e+00 (1.3199e+00)	Acc@1  64.06 ( 63.40)	Acc@5  88.28 ( 88.83)
Epoch: [28][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.1908e+00 (1.3112e+00)	Acc@1  62.50 ( 63.66)	Acc@5  90.62 ( 88.79)
Epoch: [28][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.1302e+00 (1.3069e+00)	Acc@1  66.41 ( 63.67)	Acc@5  93.75 ( 88.81)
Epoch: [28][ 70/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5257e+00 (1.3005e+00)	Acc@1  62.50 ( 64.01)	Acc@5  84.38 ( 88.96)
Epoch: [28][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.2618e+00 (1.2960e+00)	Acc@1  62.50 ( 63.97)	Acc@5  89.06 ( 88.94)
Epoch: [28][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.1401e+00 (1.3045e+00)	Acc@1  67.19 ( 63.87)	Acc@5  92.97 ( 88.86)
Epoch: [28][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2442e+00 (1.3074e+00)	Acc@1  64.06 ( 63.83)	Acc@5  89.84 ( 88.94)
Epoch: [28][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4721e+00 (1.3141e+00)	Acc@1  63.28 ( 63.62)	Acc@5  85.16 ( 88.93)
Epoch: [28][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4714e+00 (1.3105e+00)	Acc@1  60.16 ( 63.59)	Acc@5  88.28 ( 88.95)
Epoch: [28][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4510e+00 (1.3194e+00)	Acc@1  58.59 ( 63.29)	Acc@5  85.94 ( 88.86)
Epoch: [28][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3621e+00 (1.3232e+00)	Acc@1  62.50 ( 63.18)	Acc@5  86.72 ( 88.76)
Epoch: [28][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4744e+00 (1.3267e+00)	Acc@1  57.81 ( 63.07)	Acc@5  88.28 ( 88.72)
Epoch: [28][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2830e+00 (1.3321e+00)	Acc@1  64.06 ( 62.99)	Acc@5  91.41 ( 88.74)
Epoch: [28][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4841e+00 (1.3328e+00)	Acc@1  57.81 ( 63.02)	Acc@5  87.50 ( 88.72)
Epoch: [28][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2881e+00 (1.3375e+00)	Acc@1  66.41 ( 62.86)	Acc@5  86.72 ( 88.66)
Epoch: [28][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0357e+00 (1.3356e+00)	Acc@1  71.09 ( 62.94)	Acc@5  90.62 ( 88.65)
Epoch: [28][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1279e+00 (1.3350e+00)	Acc@1  68.75 ( 62.99)	Acc@5  91.41 ( 88.67)
Epoch: [28][210/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3614e+00 (1.3386e+00)	Acc@1  66.41 ( 62.93)	Acc@5  87.50 ( 88.66)
Epoch: [28][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4282e+00 (1.3408e+00)	Acc@1  57.03 ( 62.88)	Acc@5  87.50 ( 88.60)
Epoch: [28][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2671e+00 (1.3395e+00)	Acc@1  64.06 ( 62.91)	Acc@5  89.84 ( 88.62)
Epoch: [28][240/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.0846e+00 (1.3413e+00)	Acc@1  64.06 ( 62.87)	Acc@5  92.97 ( 88.56)
Epoch: [28][250/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3397e+00 (1.3454e+00)	Acc@1  57.03 ( 62.81)	Acc@5  85.16 ( 88.51)
Epoch: [28][260/391]	Time  0.030 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.4918e+00 (1.3476e+00)	Acc@1  57.81 ( 62.75)	Acc@5  85.16 ( 88.45)
Epoch: [28][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4780e+00 (1.3489e+00)	Acc@1  59.38 ( 62.75)	Acc@5  87.50 ( 88.41)
Epoch: [28][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3634e+00 (1.3508e+00)	Acc@1  60.16 ( 62.72)	Acc@5  86.72 ( 88.39)
Epoch: [28][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6649e+00 (1.3505e+00)	Acc@1  53.12 ( 62.71)	Acc@5  85.16 ( 88.40)
Epoch: [28][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4409e+00 (1.3517e+00)	Acc@1  57.03 ( 62.67)	Acc@5  85.16 ( 88.35)
Epoch: [28][310/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3262e+00 (1.3510e+00)	Acc@1  67.19 ( 62.70)	Acc@5  90.62 ( 88.38)
Epoch: [28][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7203e+00 (1.3524e+00)	Acc@1  54.69 ( 62.64)	Acc@5  86.72 ( 88.37)
Epoch: [28][330/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3354e+00 (1.3553e+00)	Acc@1  60.94 ( 62.62)	Acc@5  87.50 ( 88.33)
Epoch: [28][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2359e+00 (1.3543e+00)	Acc@1  71.88 ( 62.66)	Acc@5  88.28 ( 88.32)
Epoch: [28][350/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.4535e+00 (1.3558e+00)	Acc@1  59.38 ( 62.63)	Acc@5  85.94 ( 88.32)
Epoch: [28][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3692e+00 (1.3578e+00)	Acc@1  62.50 ( 62.65)	Acc@5  89.84 ( 88.31)
Epoch: [28][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1685e+00 (1.3585e+00)	Acc@1  64.84 ( 62.62)	Acc@5  90.62 ( 88.31)
Epoch: [28][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1432e+00 (1.3592e+00)	Acc@1  65.62 ( 62.58)	Acc@5  90.62 ( 88.30)
Epoch: [28][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4376e+00 (1.3602e+00)	Acc@1  53.75 ( 62.54)	Acc@5  86.25 ( 88.28)
## e[28] optimizer.zero_grad (sum) time: 0.11676764488220215
## e[28]       loss.backward (sum) time: 2.091661214828491
## e[28]      optimizer.step (sum) time: 0.7343268394470215
## epoch[28] training(only) time: 13.014211177825928
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.9897e+00 (1.9897e+00)	Acc@1  50.00 ( 50.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.017 ( 0.031)	Loss 1.5358e+00 (1.6638e+00)	Acc@1  54.00 ( 57.27)	Acc@5  86.00 ( 83.82)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.5931e+00 (1.6615e+00)	Acc@1  63.00 ( 57.00)	Acc@5  87.00 ( 84.00)
Test: [ 30/100]	Time  0.018 ( 0.022)	Loss 1.6718e+00 (1.6711e+00)	Acc@1  55.00 ( 56.68)	Acc@5  84.00 ( 83.35)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 1.4994e+00 (1.6800e+00)	Acc@1  58.00 ( 56.24)	Acc@5  87.00 ( 83.34)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5679e+00 (1.6946e+00)	Acc@1  53.00 ( 55.78)	Acc@5  80.00 ( 82.88)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.0188e+00 (1.6925e+00)	Acc@1  57.00 ( 56.11)	Acc@5  80.00 ( 83.05)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.8061e+00 (1.6961e+00)	Acc@1  54.00 ( 56.06)	Acc@5  86.00 ( 83.00)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6209e+00 (1.6973e+00)	Acc@1  54.00 ( 55.91)	Acc@5  84.00 ( 82.90)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7902e+00 (1.6873e+00)	Acc@1  58.00 ( 56.27)	Acc@5  86.00 ( 82.97)
 * Acc@1 56.400 Acc@5 83.050
### epoch[28] execution time: 14.996385335922241
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.184 ( 0.184)	Data  0.157 ( 0.157)	Loss 1.1757e+00 (1.1757e+00)	Acc@1  71.09 ( 71.09)	Acc@5  89.06 ( 89.06)
Epoch: [29][ 10/391]	Time  0.031 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.3156e+00 (1.2176e+00)	Acc@1  59.38 ( 65.48)	Acc@5  91.41 ( 90.34)
Epoch: [29][ 20/391]	Time  0.031 ( 0.040)	Data  0.002 ( 0.009)	Loss 7.8528e-01 (1.2017e+00)	Acc@1  73.44 ( 65.85)	Acc@5  95.31 ( 89.92)
Epoch: [29][ 30/391]	Time  0.032 ( 0.037)	Data  0.002 ( 0.007)	Loss 1.1825e+00 (1.2166e+00)	Acc@1  60.16 ( 65.32)	Acc@5  94.53 ( 90.37)
Epoch: [29][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 9.8084e-01 (1.2325e+00)	Acc@1  72.66 ( 65.53)	Acc@5  93.75 ( 89.96)
Epoch: [29][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.1983e+00 (1.2323e+00)	Acc@1  67.19 ( 65.59)	Acc@5  88.28 ( 89.84)
Epoch: [29][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4827e+00 (1.2482e+00)	Acc@1  60.94 ( 65.11)	Acc@5  83.59 ( 89.60)
Epoch: [29][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4086e+00 (1.2608e+00)	Acc@1  61.72 ( 64.95)	Acc@5  88.28 ( 89.37)
Epoch: [29][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.3591e+00 (1.2564e+00)	Acc@1  59.38 ( 65.04)	Acc@5  89.06 ( 89.48)
Epoch: [29][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4698e+00 (1.2549e+00)	Acc@1  58.59 ( 65.01)	Acc@5  83.59 ( 89.38)
Epoch: [29][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4007e+00 (1.2550e+00)	Acc@1  64.06 ( 64.92)	Acc@5  88.28 ( 89.47)
Epoch: [29][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2753e+00 (1.2574e+00)	Acc@1  65.62 ( 65.05)	Acc@5  89.84 ( 89.44)
Epoch: [29][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3916e+00 (1.2682e+00)	Acc@1  61.72 ( 64.76)	Acc@5  87.50 ( 89.33)
Epoch: [29][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4867e+00 (1.2699e+00)	Acc@1  60.94 ( 64.69)	Acc@5  86.72 ( 89.38)
Epoch: [29][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1151e+00 (1.2769e+00)	Acc@1  69.53 ( 64.55)	Acc@5  89.06 ( 89.29)
Epoch: [29][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5399e+00 (1.2867e+00)	Acc@1  57.81 ( 64.30)	Acc@5  86.72 ( 89.19)
Epoch: [29][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6133e+00 (1.2913e+00)	Acc@1  61.72 ( 64.24)	Acc@5  83.59 ( 89.14)
Epoch: [29][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2805e+00 (1.2960e+00)	Acc@1  62.50 ( 64.12)	Acc@5  90.62 ( 89.05)
Epoch: [29][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3634e+00 (1.3001e+00)	Acc@1  64.84 ( 64.00)	Acc@5  88.28 ( 89.00)
Epoch: [29][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3375e+00 (1.3034e+00)	Acc@1  70.31 ( 63.95)	Acc@5  87.50 ( 89.00)
Epoch: [29][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3974e+00 (1.3031e+00)	Acc@1  60.16 ( 63.90)	Acc@5  89.84 ( 89.01)
Epoch: [29][210/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.1911e+00 (1.3041e+00)	Acc@1  68.75 ( 63.86)	Acc@5  91.41 ( 89.00)
Epoch: [29][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1107e+00 (1.3059e+00)	Acc@1  70.31 ( 63.77)	Acc@5  91.41 ( 89.00)
Epoch: [29][230/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3247e+00 (1.3110e+00)	Acc@1  64.84 ( 63.66)	Acc@5  93.75 ( 88.96)
Epoch: [29][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3515e+00 (1.3109e+00)	Acc@1  60.94 ( 63.69)	Acc@5  89.84 ( 88.97)
Epoch: [29][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2445e+00 (1.3106e+00)	Acc@1  65.62 ( 63.73)	Acc@5  91.41 ( 88.98)
Epoch: [29][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3054e+00 (1.3104e+00)	Acc@1  62.50 ( 63.70)	Acc@5  89.06 ( 89.01)
Epoch: [29][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1491e+00 (1.3127e+00)	Acc@1  68.75 ( 63.61)	Acc@5  87.50 ( 88.97)
Epoch: [29][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4865e+00 (1.3147e+00)	Acc@1  59.38 ( 63.61)	Acc@5  85.94 ( 88.90)
Epoch: [29][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2856e+00 (1.3164e+00)	Acc@1  62.50 ( 63.62)	Acc@5  88.28 ( 88.87)
Epoch: [29][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4150e+00 (1.3191e+00)	Acc@1  61.72 ( 63.55)	Acc@5  88.28 ( 88.85)
Epoch: [29][310/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3970e+00 (1.3214e+00)	Acc@1  64.84 ( 63.50)	Acc@5  84.38 ( 88.83)
Epoch: [29][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7325e+00 (1.3250e+00)	Acc@1  57.81 ( 63.43)	Acc@5  85.16 ( 88.75)
Epoch: [29][330/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.5170e+00 (1.3271e+00)	Acc@1  59.38 ( 63.40)	Acc@5  85.94 ( 88.75)
Epoch: [29][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3297e+00 (1.3273e+00)	Acc@1  63.28 ( 63.44)	Acc@5  86.72 ( 88.76)
Epoch: [29][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3295e+00 (1.3288e+00)	Acc@1  62.50 ( 63.42)	Acc@5  89.06 ( 88.73)
Epoch: [29][360/391]	Time  0.030 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.3989e+00 (1.3323e+00)	Acc@1  64.06 ( 63.35)	Acc@5  89.06 ( 88.71)
Epoch: [29][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3855e+00 (1.3333e+00)	Acc@1  59.38 ( 63.31)	Acc@5  87.50 ( 88.71)
Epoch: [29][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2895e+00 (1.3344e+00)	Acc@1  60.16 ( 63.27)	Acc@5  86.72 ( 88.71)
Epoch: [29][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4574e+00 (1.3356e+00)	Acc@1  60.00 ( 63.28)	Acc@5  86.25 ( 88.69)
## e[29] optimizer.zero_grad (sum) time: 0.1163797378540039
## e[29]       loss.backward (sum) time: 2.114651918411255
## e[29]      optimizer.step (sum) time: 0.7349967956542969
## epoch[29] training(only) time: 13.002683162689209
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.8945e+00 (1.8945e+00)	Acc@1  56.00 ( 56.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.020 ( 0.029)	Loss 1.8979e+00 (1.7123e+00)	Acc@1  49.00 ( 56.36)	Acc@5  85.00 ( 83.64)
Test: [ 20/100]	Time  0.016 ( 0.023)	Loss 1.7219e+00 (1.7109e+00)	Acc@1  54.00 ( 56.76)	Acc@5  87.00 ( 83.86)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.8294e+00 (1.7171e+00)	Acc@1  53.00 ( 56.00)	Acc@5  86.00 ( 83.68)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.6433e+00 (1.7124e+00)	Acc@1  58.00 ( 55.95)	Acc@5  83.00 ( 83.68)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6582e+00 (1.7155e+00)	Acc@1  60.00 ( 56.00)	Acc@5  83.00 ( 83.22)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0631e+00 (1.7133e+00)	Acc@1  47.00 ( 55.85)	Acc@5  77.00 ( 83.25)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.8597e+00 (1.7074e+00)	Acc@1  54.00 ( 56.11)	Acc@5  84.00 ( 83.41)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.7372e+00 (1.7128e+00)	Acc@1  54.00 ( 55.89)	Acc@5  82.00 ( 83.25)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.8365e+00 (1.7013e+00)	Acc@1  53.00 ( 56.38)	Acc@5  84.00 ( 83.26)
 * Acc@1 56.440 Acc@5 83.370
### epoch[29] execution time: 14.938659191131592
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.179 ( 0.179)	Data  0.151 ( 0.151)	Loss 1.4434e+00 (1.4434e+00)	Acc@1  60.94 ( 60.94)	Acc@5  89.06 ( 89.06)
Epoch: [30][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.4078e+00 (1.2367e+00)	Acc@1  61.72 ( 65.20)	Acc@5  84.38 ( 90.06)
Epoch: [30][ 20/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.0689e+00 (1.1630e+00)	Acc@1  68.75 ( 66.85)	Acc@5  90.62 ( 91.15)
Epoch: [30][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.0606e+00 (1.1475e+00)	Acc@1  65.62 ( 66.83)	Acc@5  94.53 ( 91.26)
Epoch: [30][ 40/391]	Time  0.031 ( 0.036)	Data  0.002 ( 0.006)	Loss 9.2813e-01 (1.1190e+00)	Acc@1  72.66 ( 67.44)	Acc@5  93.75 ( 91.60)
Epoch: [30][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 9.4244e-01 (1.0967e+00)	Acc@1  71.88 ( 68.17)	Acc@5  94.53 ( 91.74)
Epoch: [30][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 9.0478e-01 (1.0885e+00)	Acc@1  70.31 ( 68.53)	Acc@5  95.31 ( 91.84)
Epoch: [30][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.7948e-01 (1.0751e+00)	Acc@1  72.66 ( 69.01)	Acc@5  92.97 ( 91.87)
Epoch: [30][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.0337e+00 (1.0679e+00)	Acc@1  71.88 ( 69.15)	Acc@5  93.75 ( 92.11)
Epoch: [30][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.0494e+00 (1.0603e+00)	Acc@1  69.53 ( 69.46)	Acc@5  94.53 ( 92.20)
Epoch: [30][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0113e+00 (1.0533e+00)	Acc@1  72.66 ( 69.71)	Acc@5  92.97 ( 92.20)
Epoch: [30][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.8801e-01 (1.0455e+00)	Acc@1  79.69 ( 69.89)	Acc@5  92.97 ( 92.31)
Epoch: [30][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.1250e-01 (1.0382e+00)	Acc@1  80.47 ( 70.13)	Acc@5  92.19 ( 92.37)
Epoch: [30][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.9866e-01 (1.0313e+00)	Acc@1  72.66 ( 70.25)	Acc@5  95.31 ( 92.47)
Epoch: [30][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0373e+00 (1.0288e+00)	Acc@1  71.09 ( 70.28)	Acc@5  91.41 ( 92.44)
Epoch: [30][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.5196e-01 (1.0244e+00)	Acc@1  78.91 ( 70.37)	Acc@5  97.66 ( 92.52)
Epoch: [30][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.1736e-01 (1.0182e+00)	Acc@1  70.31 ( 70.53)	Acc@5  96.88 ( 92.53)
Epoch: [30][170/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.5363e-01 (1.0138e+00)	Acc@1  76.56 ( 70.68)	Acc@5  91.41 ( 92.53)
Epoch: [30][180/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2372e+00 (1.0101e+00)	Acc@1  67.19 ( 70.81)	Acc@5  89.06 ( 92.58)
Epoch: [30][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.4186e-01 (1.0076e+00)	Acc@1  73.44 ( 70.87)	Acc@5  94.53 ( 92.63)
Epoch: [30][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.8873e-01 (1.0066e+00)	Acc@1  75.00 ( 70.93)	Acc@5  96.88 ( 92.61)
Epoch: [30][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0033e+00 (1.0049e+00)	Acc@1  70.31 ( 70.98)	Acc@5  91.41 ( 92.61)
Epoch: [30][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.3751e-01 (1.0022e+00)	Acc@1  81.25 ( 71.08)	Acc@5  94.53 ( 92.64)
Epoch: [30][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.8244e-01 (1.0004e+00)	Acc@1  69.53 ( 71.16)	Acc@5  94.53 ( 92.64)
Epoch: [30][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.2358e-01 (9.9606e-01)	Acc@1  78.12 ( 71.29)	Acc@5  95.31 ( 92.68)
Epoch: [30][250/391]	Time  0.030 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.0317e+00 (9.9164e-01)	Acc@1  71.09 ( 71.44)	Acc@5  91.41 ( 92.74)
Epoch: [30][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.3644e-01 (9.8954e-01)	Acc@1  80.47 ( 71.53)	Acc@5  96.88 ( 92.76)
Epoch: [30][270/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 7.6465e-01 (9.8536e-01)	Acc@1  79.69 ( 71.64)	Acc@5  93.75 ( 92.80)
Epoch: [30][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.8553e-01 (9.8242e-01)	Acc@1  74.22 ( 71.72)	Acc@5  92.19 ( 92.84)
Epoch: [30][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.2951e-01 (9.8155e-01)	Acc@1  78.12 ( 71.80)	Acc@5  97.66 ( 92.84)
Epoch: [30][300/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 9.0314e-01 (9.7973e-01)	Acc@1  72.66 ( 71.85)	Acc@5  92.97 ( 92.87)
Epoch: [30][310/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 9.1373e-01 (9.7679e-01)	Acc@1  69.53 ( 71.93)	Acc@5  93.75 ( 92.89)
Epoch: [30][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.4246e-01 (9.7317e-01)	Acc@1  75.78 ( 71.99)	Acc@5  95.31 ( 92.93)
Epoch: [30][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2564e+00 (9.7052e-01)	Acc@1  64.06 ( 72.02)	Acc@5  92.19 ( 92.99)
Epoch: [30][340/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.4530e-01 (9.6823e-01)	Acc@1  78.12 ( 72.07)	Acc@5  94.53 ( 93.01)
Epoch: [30][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.1077e-01 (9.6759e-01)	Acc@1  74.22 ( 72.13)	Acc@5  91.41 ( 93.00)
Epoch: [30][360/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 8.4867e-01 (9.6619e-01)	Acc@1  71.88 ( 72.19)	Acc@5  95.31 ( 93.00)
Epoch: [30][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.8375e-01 (9.6283e-01)	Acc@1  80.47 ( 72.26)	Acc@5  96.88 ( 93.05)
Epoch: [30][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0348e-01 (9.5863e-01)	Acc@1  80.47 ( 72.36)	Acc@5  95.31 ( 93.11)
Epoch: [30][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1406e+00 (9.5671e-01)	Acc@1  67.50 ( 72.39)	Acc@5  93.75 ( 93.12)
## e[30] optimizer.zero_grad (sum) time: 0.11644768714904785
## e[30]       loss.backward (sum) time: 2.0959110260009766
## e[30]      optimizer.step (sum) time: 0.7330887317657471
## epoch[30] training(only) time: 13.004250526428223
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.3643e+00 (1.3643e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.3604e+00 (1.3394e+00)	Acc@1  64.00 ( 64.55)	Acc@5  91.00 ( 88.64)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.3049e+00 (1.3188e+00)	Acc@1  68.00 ( 65.29)	Acc@5  93.00 ( 88.81)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.3744e+00 (1.3243e+00)	Acc@1  64.00 ( 64.68)	Acc@5  90.00 ( 88.74)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.2216e+00 (1.3200e+00)	Acc@1  67.00 ( 64.73)	Acc@5  88.00 ( 88.95)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3809e+00 (1.3303e+00)	Acc@1  66.00 ( 64.78)	Acc@5  85.00 ( 88.73)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.5655e+00 (1.3220e+00)	Acc@1  64.00 ( 65.08)	Acc@5  87.00 ( 88.90)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3875e+00 (1.3228e+00)	Acc@1  64.00 ( 64.96)	Acc@5  89.00 ( 89.00)
Test: [ 80/100]	Time  0.025 ( 0.019)	Loss 1.3370e+00 (1.3279e+00)	Acc@1  68.00 ( 64.89)	Acc@5  87.00 ( 88.86)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.5891e+00 (1.3235e+00)	Acc@1  59.00 ( 64.87)	Acc@5  85.00 ( 88.88)
 * Acc@1 64.750 Acc@5 88.970
### epoch[30] execution time: 14.93906307220459
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.184 ( 0.184)	Data  0.159 ( 0.159)	Loss 7.5341e-01 (7.5341e-01)	Acc@1  78.91 ( 78.91)	Acc@5  96.88 ( 96.88)
Epoch: [31][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 7.4649e-01 (8.4967e-01)	Acc@1  79.69 ( 75.85)	Acc@5  95.31 ( 94.32)
Epoch: [31][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 8.9397e-01 (8.4510e-01)	Acc@1  72.66 ( 75.26)	Acc@5  94.53 ( 94.64)
Epoch: [31][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 7.6634e-01 (8.3867e-01)	Acc@1  74.22 ( 75.38)	Acc@5  96.88 ( 94.78)
Epoch: [31][ 40/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.006)	Loss 8.5036e-01 (8.3471e-01)	Acc@1  74.22 ( 75.71)	Acc@5  94.53 ( 94.63)
Epoch: [31][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 9.1663e-01 (8.2280e-01)	Acc@1  71.88 ( 76.04)	Acc@5  92.97 ( 94.65)
Epoch: [31][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 9.5915e-01 (8.3202e-01)	Acc@1  69.53 ( 75.78)	Acc@5  94.53 ( 94.53)
Epoch: [31][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 9.7417e-01 (8.3242e-01)	Acc@1  69.53 ( 75.84)	Acc@5  95.31 ( 94.56)
Epoch: [31][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 8.9647e-01 (8.3870e-01)	Acc@1  72.66 ( 75.65)	Acc@5  92.97 ( 94.44)
Epoch: [31][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 6.4291e-01 (8.3446e-01)	Acc@1  82.81 ( 75.76)	Acc@5  96.09 ( 94.41)
Epoch: [31][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 8.7550e-01 (8.3758e-01)	Acc@1  72.66 ( 75.63)	Acc@5  94.53 ( 94.43)
Epoch: [31][110/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.4908e-01 (8.3306e-01)	Acc@1  75.00 ( 75.80)	Acc@5  96.09 ( 94.48)
Epoch: [31][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0974e+00 (8.3460e-01)	Acc@1  71.88 ( 75.85)	Acc@5  92.19 ( 94.48)
Epoch: [31][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.0183e-01 (8.3822e-01)	Acc@1  79.69 ( 75.75)	Acc@5  96.09 ( 94.41)
Epoch: [31][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.1271e-01 (8.3847e-01)	Acc@1  71.88 ( 75.79)	Acc@5  94.53 ( 94.44)
Epoch: [31][150/391]	Time  0.029 ( 0.034)	Data  0.002 ( 0.003)	Loss 8.9000e-01 (8.4263e-01)	Acc@1  76.56 ( 75.75)	Acc@5  92.19 ( 94.37)
Epoch: [31][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.7332e-01 (8.4155e-01)	Acc@1  72.66 ( 75.67)	Acc@5  93.75 ( 94.41)
Epoch: [31][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.2034e-01 (8.4255e-01)	Acc@1  74.22 ( 75.61)	Acc@5  95.31 ( 94.40)
Epoch: [31][180/391]	Time  0.028 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.9761e-01 (8.4299e-01)	Acc@1  71.88 ( 75.55)	Acc@5  92.97 ( 94.42)
Epoch: [31][190/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.6105e-01 (8.4585e-01)	Acc@1  77.34 ( 75.52)	Acc@5  96.09 ( 94.36)
Epoch: [31][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.4205e-01 (8.4350e-01)	Acc@1  71.09 ( 75.51)	Acc@5  94.53 ( 94.38)
Epoch: [31][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.4798e-01 (8.4561e-01)	Acc@1  74.22 ( 75.45)	Acc@5  92.19 ( 94.34)
Epoch: [31][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.3041e-01 (8.4622e-01)	Acc@1  77.34 ( 75.42)	Acc@5  97.66 ( 94.33)
Epoch: [31][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.3788e-01 (8.4681e-01)	Acc@1  78.12 ( 75.39)	Acc@5  93.75 ( 94.35)
Epoch: [31][240/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 6.8001e-01 (8.4486e-01)	Acc@1  77.34 ( 75.46)	Acc@5  97.66 ( 94.37)
Epoch: [31][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.1469e-01 (8.4527e-01)	Acc@1  75.00 ( 75.43)	Acc@5  91.41 ( 94.32)
Epoch: [31][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.9002e-01 (8.4379e-01)	Acc@1  79.69 ( 75.47)	Acc@5  91.41 ( 94.32)
Epoch: [31][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.3087e-01 (8.4253e-01)	Acc@1  74.22 ( 75.49)	Acc@5  93.75 ( 94.34)
Epoch: [31][280/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.1180e-01 (8.3991e-01)	Acc@1  82.03 ( 75.56)	Acc@5  96.88 ( 94.36)
Epoch: [31][290/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 7.2448e-01 (8.3695e-01)	Acc@1  75.00 ( 75.67)	Acc@5  98.44 ( 94.41)
Epoch: [31][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.4481e-01 (8.3468e-01)	Acc@1  79.69 ( 75.75)	Acc@5  96.88 ( 94.45)
Epoch: [31][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.4135e-01 (8.3471e-01)	Acc@1  74.22 ( 75.72)	Acc@5  96.09 ( 94.46)
Epoch: [31][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.5238e-01 (8.3384e-01)	Acc@1  82.03 ( 75.73)	Acc@5  92.19 ( 94.45)
Epoch: [31][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.2757e-01 (8.3266e-01)	Acc@1  75.00 ( 75.75)	Acc@5  96.09 ( 94.46)
Epoch: [31][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.0927e-01 (8.3275e-01)	Acc@1  77.34 ( 75.74)	Acc@5  96.09 ( 94.45)
Epoch: [31][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.7766e-01 (8.3309e-01)	Acc@1  78.91 ( 75.72)	Acc@5  96.88 ( 94.42)
Epoch: [31][360/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.2356e-01 (8.3352e-01)	Acc@1  77.34 ( 75.72)	Acc@5  93.75 ( 94.42)
Epoch: [31][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.5441e-01 (8.3246e-01)	Acc@1  75.78 ( 75.72)	Acc@5  95.31 ( 94.43)
Epoch: [31][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0840e+00 (8.3341e-01)	Acc@1  73.44 ( 75.70)	Acc@5  92.19 ( 94.43)
Epoch: [31][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.6809e-01 (8.3194e-01)	Acc@1  72.50 ( 75.75)	Acc@5  92.50 ( 94.45)
## e[31] optimizer.zero_grad (sum) time: 0.11620783805847168
## e[31]       loss.backward (sum) time: 2.1071908473968506
## e[31]      optimizer.step (sum) time: 0.7503662109375
## epoch[31] training(only) time: 12.967374324798584
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.3140e+00 (1.3140e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.3546e+00 (1.3399e+00)	Acc@1  65.00 ( 63.91)	Acc@5  91.00 ( 88.64)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.3358e+00 (1.3207e+00)	Acc@1  66.00 ( 64.86)	Acc@5  91.00 ( 88.67)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.4125e+00 (1.3220e+00)	Acc@1  62.00 ( 64.52)	Acc@5  89.00 ( 88.42)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.1563e+00 (1.3125e+00)	Acc@1  67.00 ( 64.78)	Acc@5  91.00 ( 88.88)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.2733e+00 (1.3236e+00)	Acc@1  66.00 ( 64.57)	Acc@5  89.00 ( 88.80)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.5938e+00 (1.3144e+00)	Acc@1  64.00 ( 64.90)	Acc@5  86.00 ( 88.87)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.2908e+00 (1.3108e+00)	Acc@1  68.00 ( 65.00)	Acc@5  87.00 ( 89.03)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3081e+00 (1.3148e+00)	Acc@1  66.00 ( 65.09)	Acc@5  87.00 ( 88.96)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.5951e+00 (1.3083e+00)	Acc@1  60.00 ( 65.18)	Acc@5  88.00 ( 89.03)
 * Acc@1 65.180 Acc@5 89.120
### epoch[31] execution time: 14.908519744873047
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.178 ( 0.178)	Data  0.150 ( 0.150)	Loss 7.3496e-01 (7.3496e-01)	Acc@1  76.56 ( 76.56)	Acc@5  96.09 ( 96.09)
Epoch: [32][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 5.2913e-01 (7.0849e-01)	Acc@1  84.38 ( 78.05)	Acc@5  97.66 ( 96.45)
Epoch: [32][ 20/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.009)	Loss 8.2768e-01 (7.4779e-01)	Acc@1  72.66 ( 77.68)	Acc@5  95.31 ( 95.76)
Epoch: [32][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 7.6170e-01 (7.5905e-01)	Acc@1  77.34 ( 77.72)	Acc@5  94.53 ( 95.44)
Epoch: [32][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 7.1121e-01 (7.7438e-01)	Acc@1  78.12 ( 77.31)	Acc@5  96.09 ( 95.24)
Epoch: [32][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 6.2370e-01 (7.7153e-01)	Acc@1  83.59 ( 77.16)	Acc@5  95.31 ( 95.17)
Epoch: [32][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.0663e-01 (7.7661e-01)	Acc@1  78.12 ( 77.13)	Acc@5  92.19 ( 95.01)
Epoch: [32][ 70/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 6.6773e-01 (7.8392e-01)	Acc@1  78.12 ( 76.95)	Acc@5  96.09 ( 94.96)
Epoch: [32][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.8194e-01 (7.8351e-01)	Acc@1  81.25 ( 77.01)	Acc@5  94.53 ( 94.94)
Epoch: [32][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 7.7329e-01 (7.8316e-01)	Acc@1  78.91 ( 76.97)	Acc@5  93.75 ( 94.91)
Epoch: [32][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 9.1217e-01 (7.8380e-01)	Acc@1  74.22 ( 76.90)	Acc@5  92.19 ( 94.93)
Epoch: [32][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.0106e-01 (7.8194e-01)	Acc@1  82.03 ( 76.99)	Acc@5  96.09 ( 94.90)
Epoch: [32][120/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.5947e-01 (7.8800e-01)	Acc@1  75.00 ( 76.98)	Acc@5  96.09 ( 94.78)
Epoch: [32][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.0947e-01 (7.8190e-01)	Acc@1  82.81 ( 77.21)	Acc@5  96.09 ( 94.86)
Epoch: [32][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.7549e-01 (7.8363e-01)	Acc@1  78.12 ( 77.15)	Acc@5  95.31 ( 94.85)
Epoch: [32][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.1833e-01 (7.8564e-01)	Acc@1  75.78 ( 77.03)	Acc@5  93.75 ( 94.82)
Epoch: [32][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.4644e-01 (7.8161e-01)	Acc@1  78.91 ( 77.13)	Acc@5  95.31 ( 94.90)
Epoch: [32][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.2239e-01 (7.8115e-01)	Acc@1  76.56 ( 77.17)	Acc@5  95.31 ( 94.91)
Epoch: [32][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.1048e-01 (7.7960e-01)	Acc@1  78.12 ( 77.14)	Acc@5  99.22 ( 94.94)
Epoch: [32][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.6159e-01 (7.7539e-01)	Acc@1  77.34 ( 77.26)	Acc@5  95.31 ( 94.96)
Epoch: [32][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.1655e-01 (7.7763e-01)	Acc@1  79.69 ( 77.24)	Acc@5  96.88 ( 94.95)
Epoch: [32][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.9410e-01 (7.7860e-01)	Acc@1  76.56 ( 77.18)	Acc@5  95.31 ( 94.95)
Epoch: [32][220/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.3343e-01 (7.7655e-01)	Acc@1  76.56 ( 77.17)	Acc@5  97.66 ( 94.98)
Epoch: [32][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.1110e-01 (7.7624e-01)	Acc@1  73.44 ( 77.19)	Acc@5  92.19 ( 94.97)
Epoch: [32][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.4264e-01 (7.7748e-01)	Acc@1  77.34 ( 77.21)	Acc@5  91.41 ( 94.95)
Epoch: [32][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0484e+00 (7.7941e-01)	Acc@1  71.09 ( 77.19)	Acc@5  92.19 ( 94.92)
Epoch: [32][260/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4633e-01 (7.7667e-01)	Acc@1  85.16 ( 77.27)	Acc@5  97.66 ( 94.95)
Epoch: [32][270/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 6.9403e-01 (7.7476e-01)	Acc@1  79.69 ( 77.29)	Acc@5  96.09 ( 94.98)
Epoch: [32][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6909e-01 (7.7415e-01)	Acc@1  85.94 ( 77.31)	Acc@5  97.66 ( 94.97)
Epoch: [32][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.5353e-01 (7.7513e-01)	Acc@1  81.25 ( 77.34)	Acc@5  96.88 ( 94.97)
Epoch: [32][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.8902e-01 (7.7488e-01)	Acc@1  79.69 ( 77.35)	Acc@5  95.31 ( 94.96)
Epoch: [32][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.7777e-01 (7.7509e-01)	Acc@1  73.44 ( 77.33)	Acc@5  93.75 ( 94.97)
Epoch: [32][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.7807e-01 (7.7598e-01)	Acc@1  70.31 ( 77.27)	Acc@5  96.88 ( 95.00)
Epoch: [32][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.1978e-01 (7.7770e-01)	Acc@1  76.56 ( 77.25)	Acc@5  92.19 ( 94.98)
Epoch: [32][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.9945e-01 (7.7828e-01)	Acc@1  73.44 ( 77.22)	Acc@5  96.09 ( 95.00)
Epoch: [32][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.9654e-01 (7.7949e-01)	Acc@1  70.31 ( 77.16)	Acc@5  95.31 ( 95.01)
Epoch: [32][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0926e-01 (7.7931e-01)	Acc@1  78.91 ( 77.17)	Acc@5  96.09 ( 95.00)
Epoch: [32][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.5816e-01 (7.7954e-01)	Acc@1  77.34 ( 77.15)	Acc@5  96.88 ( 95.01)
Epoch: [32][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.7430e-01 (7.8018e-01)	Acc@1  75.00 ( 77.13)	Acc@5  94.53 ( 95.00)
Epoch: [32][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.9581e-01 (7.7913e-01)	Acc@1  72.50 ( 77.13)	Acc@5  97.50 ( 95.01)
## e[32] optimizer.zero_grad (sum) time: 0.11632204055786133
## e[32]       loss.backward (sum) time: 2.1081957817077637
## e[32]      optimizer.step (sum) time: 0.7314727306365967
## epoch[32] training(only) time: 13.01731276512146
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2529e+00 (1.2529e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.3380e+00 (1.3106e+00)	Acc@1  63.00 ( 65.27)	Acc@5  87.00 ( 88.82)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.2448e+00 (1.2914e+00)	Acc@1  69.00 ( 65.95)	Acc@5  91.00 ( 89.00)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.3598e+00 (1.2988e+00)	Acc@1  62.00 ( 65.23)	Acc@5  88.00 ( 88.74)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.1615e+00 (1.2903e+00)	Acc@1  69.00 ( 65.56)	Acc@5  92.00 ( 89.20)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3920e+00 (1.3109e+00)	Acc@1  66.00 ( 65.10)	Acc@5  86.00 ( 88.94)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.5977e+00 (1.3028e+00)	Acc@1  65.00 ( 65.49)	Acc@5  85.00 ( 89.00)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3408e+00 (1.3033e+00)	Acc@1  67.00 ( 65.35)	Acc@5  89.00 ( 89.20)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3439e+00 (1.3056e+00)	Acc@1  68.00 ( 65.40)	Acc@5  86.00 ( 89.14)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.6731e+00 (1.3003e+00)	Acc@1  59.00 ( 65.42)	Acc@5  87.00 ( 89.21)
 * Acc@1 65.340 Acc@5 89.270
### epoch[32] execution time: 14.958341360092163
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.179 ( 0.179)	Data  0.152 ( 0.152)	Loss 7.3699e-01 (7.3699e-01)	Acc@1  78.12 ( 78.12)	Acc@5  94.53 ( 94.53)
Epoch: [33][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 7.3468e-01 (7.1524e-01)	Acc@1  77.34 ( 79.62)	Acc@5  94.53 ( 95.67)
Epoch: [33][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 7.7613e-01 (7.5346e-01)	Acc@1  78.12 ( 78.46)	Acc@5  93.75 ( 95.16)
Epoch: [33][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 6.9253e-01 (7.6679e-01)	Acc@1  77.34 ( 78.00)	Acc@5  96.09 ( 95.04)
Epoch: [33][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.006)	Loss 6.2316e-01 (7.6182e-01)	Acc@1  82.03 ( 78.11)	Acc@5  96.88 ( 95.08)
Epoch: [33][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 8.4752e-01 (7.5976e-01)	Acc@1  74.22 ( 77.99)	Acc@5  95.31 ( 95.22)
Epoch: [33][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.2671e-01 (7.5242e-01)	Acc@1  75.78 ( 78.07)	Acc@5  93.75 ( 95.35)
Epoch: [33][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.2493e-01 (7.5070e-01)	Acc@1  73.44 ( 78.17)	Acc@5  94.53 ( 95.36)
Epoch: [33][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 8.8479e-01 (7.5875e-01)	Acc@1  75.78 ( 77.98)	Acc@5  92.19 ( 95.26)
Epoch: [33][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 9.2278e-01 (7.5856e-01)	Acc@1  73.44 ( 77.94)	Acc@5  93.75 ( 95.30)
Epoch: [33][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.3215e-01 (7.5288e-01)	Acc@1  81.25 ( 78.12)	Acc@5  96.88 ( 95.40)
Epoch: [33][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.2305e-01 (7.5314e-01)	Acc@1  76.56 ( 78.12)	Acc@5  92.97 ( 95.36)
Epoch: [33][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.5766e-01 (7.4881e-01)	Acc@1  81.25 ( 78.17)	Acc@5  96.88 ( 95.38)
Epoch: [33][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.0089e-01 (7.4474e-01)	Acc@1  78.12 ( 78.27)	Acc@5  99.22 ( 95.44)
Epoch: [33][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.0871e-01 (7.4975e-01)	Acc@1  78.12 ( 78.13)	Acc@5  97.66 ( 95.42)
Epoch: [33][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.5994e-01 (7.5021e-01)	Acc@1  77.34 ( 78.05)	Acc@5  95.31 ( 95.39)
Epoch: [33][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.0080e-01 (7.4862e-01)	Acc@1  76.56 ( 78.11)	Acc@5  92.97 ( 95.33)
Epoch: [33][170/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.6764e-01 (7.4749e-01)	Acc@1  82.03 ( 78.13)	Acc@5  95.31 ( 95.35)
Epoch: [33][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.6854e-01 (7.4529e-01)	Acc@1  84.38 ( 78.23)	Acc@5  96.09 ( 95.39)
Epoch: [33][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.1936e-01 (7.4493e-01)	Acc@1  80.47 ( 78.28)	Acc@5  93.75 ( 95.40)
Epoch: [33][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.4875e-01 (7.4140e-01)	Acc@1  78.12 ( 78.37)	Acc@5  94.53 ( 95.45)
Epoch: [33][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.9448e-01 (7.4170e-01)	Acc@1  81.25 ( 78.35)	Acc@5  93.75 ( 95.42)
Epoch: [33][220/391]	Time  0.029 ( 0.033)	Data  0.002 ( 0.003)	Loss 9.8507e-01 (7.4364e-01)	Acc@1  73.44 ( 78.26)	Acc@5  92.19 ( 95.44)
Epoch: [33][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9049e-01 (7.4350e-01)	Acc@1  75.78 ( 78.27)	Acc@5  97.66 ( 95.45)
Epoch: [33][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.5433e-01 (7.4235e-01)	Acc@1  82.81 ( 78.27)	Acc@5  95.31 ( 95.46)
Epoch: [33][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.5761e-01 (7.4164e-01)	Acc@1  76.56 ( 78.27)	Acc@5  91.41 ( 95.46)
Epoch: [33][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.3027e-01 (7.4067e-01)	Acc@1  72.66 ( 78.27)	Acc@5  96.09 ( 95.50)
Epoch: [33][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.6527e-01 (7.4149e-01)	Acc@1  76.56 ( 78.27)	Acc@5  97.66 ( 95.49)
Epoch: [33][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.4292e-01 (7.4062e-01)	Acc@1  75.78 ( 78.29)	Acc@5  96.09 ( 95.53)
Epoch: [33][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.4044e-01 (7.3942e-01)	Acc@1  78.12 ( 78.32)	Acc@5  94.53 ( 95.54)
Epoch: [33][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.4841e-01 (7.3900e-01)	Acc@1  80.47 ( 78.33)	Acc@5  96.09 ( 95.55)
Epoch: [33][310/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.3208e-01 (7.3947e-01)	Acc@1  83.59 ( 78.33)	Acc@5  97.66 ( 95.56)
Epoch: [33][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0521e-01 (7.3865e-01)	Acc@1  76.56 ( 78.35)	Acc@5  95.31 ( 95.55)
Epoch: [33][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.5825e-01 (7.4139e-01)	Acc@1  77.34 ( 78.27)	Acc@5  93.75 ( 95.52)
Epoch: [33][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 6.7089e-01 (7.4113e-01)	Acc@1  83.59 ( 78.27)	Acc@5  96.09 ( 95.51)
Epoch: [33][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.9168e-01 (7.4235e-01)	Acc@1  74.22 ( 78.23)	Acc@5  94.53 ( 95.50)
Epoch: [33][360/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 7.4955e-01 (7.4373e-01)	Acc@1  78.12 ( 78.18)	Acc@5  95.31 ( 95.48)
Epoch: [33][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0347e-01 (7.4360e-01)	Acc@1  78.12 ( 78.16)	Acc@5  96.09 ( 95.50)
Epoch: [33][380/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.1421e-01 (7.4408e-01)	Acc@1  74.22 ( 78.14)	Acc@5  95.31 ( 95.48)
Epoch: [33][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0170e-01 (7.4276e-01)	Acc@1  78.75 ( 78.17)	Acc@5  95.00 ( 95.50)
## e[33] optimizer.zero_grad (sum) time: 0.11617732048034668
## e[33]       loss.backward (sum) time: 2.1384952068328857
## e[33]      optimizer.step (sum) time: 0.72806715965271
## epoch[33] training(only) time: 13.03435206413269
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.2323e+00 (1.2323e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.3676e+00 (1.3131e+00)	Acc@1  65.00 ( 65.91)	Acc@5  91.00 ( 89.45)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.2798e+00 (1.2980e+00)	Acc@1  67.00 ( 66.05)	Acc@5  91.00 ( 89.43)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.3898e+00 (1.3091e+00)	Acc@1  59.00 ( 65.35)	Acc@5  86.00 ( 88.71)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.1988e+00 (1.2966e+00)	Acc@1  66.00 ( 65.32)	Acc@5  91.00 ( 89.17)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3964e+00 (1.3169e+00)	Acc@1  67.00 ( 65.31)	Acc@5  86.00 ( 88.80)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.6757e+00 (1.3085e+00)	Acc@1  63.00 ( 65.57)	Acc@5  86.00 ( 88.89)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3322e+00 (1.3101e+00)	Acc@1  68.00 ( 65.56)	Acc@5  87.00 ( 89.03)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3513e+00 (1.3142e+00)	Acc@1  66.00 ( 65.51)	Acc@5  86.00 ( 88.95)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.6335e+00 (1.3043e+00)	Acc@1  58.00 ( 65.56)	Acc@5  86.00 ( 88.96)
 * Acc@1 65.600 Acc@5 89.000
### epoch[33] execution time: 14.96544623374939
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.181 ( 0.181)	Data  0.155 ( 0.155)	Loss 6.3873e-01 (6.3873e-01)	Acc@1  75.78 ( 75.78)	Acc@5  97.66 ( 97.66)
Epoch: [34][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.016)	Loss 7.5926e-01 (6.9112e-01)	Acc@1  77.34 ( 79.12)	Acc@5  96.88 ( 96.24)
Epoch: [34][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 6.2284e-01 (6.9439e-01)	Acc@1  82.81 ( 79.61)	Acc@5  96.09 ( 96.28)
Epoch: [34][ 30/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.007)	Loss 8.6966e-01 (6.8376e-01)	Acc@1  74.22 ( 79.99)	Acc@5  93.75 ( 96.14)
Epoch: [34][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 8.5343e-01 (6.9217e-01)	Acc@1  75.00 ( 79.40)	Acc@5  92.19 ( 96.06)
Epoch: [34][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 5.8929e-01 (6.9806e-01)	Acc@1  80.47 ( 79.27)	Acc@5  95.31 ( 95.96)
Epoch: [34][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.2750e-01 (6.9514e-01)	Acc@1  81.25 ( 79.35)	Acc@5  96.09 ( 95.93)
Epoch: [34][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.6062e-01 (6.9027e-01)	Acc@1  86.72 ( 79.51)	Acc@5  96.09 ( 95.98)
Epoch: [34][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 6.5780e-01 (6.8508e-01)	Acc@1  78.91 ( 79.58)	Acc@5  97.66 ( 96.07)
Epoch: [34][ 90/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 8.2652e-01 (6.8530e-01)	Acc@1  76.56 ( 79.62)	Acc@5  93.75 ( 96.07)
Epoch: [34][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.1035e-01 (6.8881e-01)	Acc@1  75.78 ( 79.47)	Acc@5  95.31 ( 96.02)
Epoch: [34][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.7928e-01 (6.9095e-01)	Acc@1  77.34 ( 79.31)	Acc@5  97.66 ( 96.01)
Epoch: [34][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.0560e-01 (6.9626e-01)	Acc@1  71.88 ( 79.29)	Acc@5  92.97 ( 95.97)
Epoch: [34][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.0289e-01 (6.9742e-01)	Acc@1  81.25 ( 79.19)	Acc@5  96.88 ( 95.97)
Epoch: [34][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.6982e-01 (6.9765e-01)	Acc@1  69.53 ( 79.11)	Acc@5  92.97 ( 95.91)
Epoch: [34][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.7443e-01 (6.9691e-01)	Acc@1  83.59 ( 79.15)	Acc@5  98.44 ( 95.91)
Epoch: [34][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9550e-01 (6.9911e-01)	Acc@1  82.03 ( 79.20)	Acc@5  96.09 ( 95.89)
Epoch: [34][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.8656e-01 (6.9867e-01)	Acc@1  83.59 ( 79.23)	Acc@5  96.88 ( 95.89)
Epoch: [34][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.1507e-01 (6.9823e-01)	Acc@1  79.69 ( 79.19)	Acc@5  96.09 ( 95.88)
Epoch: [34][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9815e-01 (6.9860e-01)	Acc@1  78.91 ( 79.25)	Acc@5  96.09 ( 95.87)
Epoch: [34][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.0331e-01 (7.0130e-01)	Acc@1  76.56 ( 79.19)	Acc@5  94.53 ( 95.83)
Epoch: [34][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9192e-01 (7.0031e-01)	Acc@1  76.56 ( 79.19)	Acc@5  98.44 ( 95.83)
Epoch: [34][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.2882e-01 (7.0185e-01)	Acc@1  80.47 ( 79.13)	Acc@5  96.88 ( 95.83)
Epoch: [34][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.8990e-01 (7.0348e-01)	Acc@1  78.12 ( 79.11)	Acc@5  92.97 ( 95.79)
Epoch: [34][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.2471e-01 (7.0196e-01)	Acc@1  79.69 ( 79.19)	Acc@5  94.53 ( 95.81)
Epoch: [34][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.2388e-01 (7.0325e-01)	Acc@1  83.59 ( 79.19)	Acc@5  96.88 ( 95.79)
Epoch: [34][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.3737e-01 (7.0442e-01)	Acc@1  74.22 ( 79.13)	Acc@5  94.53 ( 95.80)
Epoch: [34][270/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 5.9167e-01 (7.0514e-01)	Acc@1  82.81 ( 79.09)	Acc@5  96.88 ( 95.82)
Epoch: [34][280/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.2345e-01 (7.0711e-01)	Acc@1  81.25 ( 79.08)	Acc@5  97.66 ( 95.77)
Epoch: [34][290/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9091e-01 (7.0864e-01)	Acc@1  78.12 ( 79.05)	Acc@5  94.53 ( 95.78)
Epoch: [34][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.9499e-01 (7.0956e-01)	Acc@1  75.00 ( 78.99)	Acc@5  92.97 ( 95.73)
Epoch: [34][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.5466e-01 (7.1158e-01)	Acc@1  75.00 ( 78.91)	Acc@5  95.31 ( 95.73)
Epoch: [34][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.8001e-01 (7.1254e-01)	Acc@1  82.81 ( 78.91)	Acc@5  96.09 ( 95.72)
Epoch: [34][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.3014e-01 (7.1329e-01)	Acc@1  73.44 ( 78.91)	Acc@5  92.97 ( 95.69)
Epoch: [34][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.6850e-01 (7.1369e-01)	Acc@1  86.72 ( 78.92)	Acc@5  97.66 ( 95.67)
Epoch: [34][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.7096e-01 (7.1297e-01)	Acc@1  78.12 ( 78.92)	Acc@5  96.88 ( 95.67)
Epoch: [34][360/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 6.8369e-01 (7.1267e-01)	Acc@1  78.12 ( 78.92)	Acc@5  97.66 ( 95.67)
Epoch: [34][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.8754e-01 (7.1249e-01)	Acc@1  83.59 ( 78.92)	Acc@5  95.31 ( 95.66)
Epoch: [34][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.7593e-01 (7.1213e-01)	Acc@1  78.12 ( 78.93)	Acc@5  98.44 ( 95.68)
Epoch: [34][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.5249e-01 (7.1011e-01)	Acc@1  81.25 ( 78.99)	Acc@5  95.00 ( 95.69)
## e[34] optimizer.zero_grad (sum) time: 0.11620235443115234
## e[34]       loss.backward (sum) time: 2.1097376346588135
## e[34]      optimizer.step (sum) time: 0.7214667797088623
## epoch[34] training(only) time: 13.01843523979187
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.3014e+00 (1.3014e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.4125e+00 (1.3462e+00)	Acc@1  64.00 ( 65.55)	Acc@5  89.00 ( 88.64)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.3068e+00 (1.3111e+00)	Acc@1  68.00 ( 66.19)	Acc@5  91.00 ( 88.95)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.4279e+00 (1.3183e+00)	Acc@1  61.00 ( 65.81)	Acc@5  88.00 ( 88.65)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.1071e+00 (1.3035e+00)	Acc@1  69.00 ( 65.88)	Acc@5  95.00 ( 89.15)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3323e+00 (1.3222e+00)	Acc@1  65.00 ( 65.80)	Acc@5  88.00 ( 88.90)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.6604e+00 (1.3113e+00)	Acc@1  61.00 ( 66.03)	Acc@5  87.00 ( 89.03)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 1.4084e+00 (1.3102e+00)	Acc@1  67.00 ( 65.83)	Acc@5  86.00 ( 89.06)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3872e+00 (1.3125e+00)	Acc@1  66.00 ( 65.90)	Acc@5  85.00 ( 89.00)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.5994e+00 (1.3019e+00)	Acc@1  60.00 ( 66.05)	Acc@5  88.00 ( 89.07)
 * Acc@1 66.030 Acc@5 89.190
### epoch[34] execution time: 14.963914394378662
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.189 ( 0.189)	Data  0.160 ( 0.160)	Loss 5.5652e-01 (5.5652e-01)	Acc@1  82.81 ( 82.81)	Acc@5  98.44 ( 98.44)
Epoch: [35][ 10/391]	Time  0.033 ( 0.047)	Data  0.001 ( 0.016)	Loss 7.3419e-01 (6.9092e-01)	Acc@1  78.12 ( 79.40)	Acc@5  96.88 ( 95.88)
Epoch: [35][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 5.4610e-01 (6.8408e-01)	Acc@1  89.06 ( 79.84)	Acc@5  96.88 ( 95.91)
Epoch: [35][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 5.9578e-01 (6.8381e-01)	Acc@1  76.56 ( 79.66)	Acc@5  98.44 ( 95.99)
Epoch: [35][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 7.5404e-01 (6.7303e-01)	Acc@1  77.34 ( 79.88)	Acc@5  94.53 ( 96.06)
Epoch: [35][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 5.6445e-01 (6.8034e-01)	Acc@1  80.47 ( 79.75)	Acc@5  97.66 ( 95.99)
Epoch: [35][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 6.5221e-01 (6.7627e-01)	Acc@1  79.69 ( 79.75)	Acc@5  96.88 ( 96.06)
Epoch: [35][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.0327e-01 (6.7409e-01)	Acc@1  77.34 ( 79.89)	Acc@5  92.97 ( 96.05)
Epoch: [35][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 7.6732e-01 (6.7862e-01)	Acc@1  76.56 ( 79.79)	Acc@5  95.31 ( 95.97)
Epoch: [35][ 90/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 7.6192e-01 (6.7671e-01)	Acc@1  77.34 ( 79.97)	Acc@5  98.44 ( 96.02)
Epoch: [35][100/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 7.1187e-01 (6.8027e-01)	Acc@1  81.25 ( 79.86)	Acc@5  94.53 ( 95.98)
Epoch: [35][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.1553e-01 (6.8070e-01)	Acc@1  81.25 ( 79.84)	Acc@5  99.22 ( 96.00)
Epoch: [35][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.6638e-01 (6.7887e-01)	Acc@1  77.34 ( 79.79)	Acc@5  96.09 ( 96.06)
Epoch: [35][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.0582e-01 (6.7932e-01)	Acc@1  77.34 ( 79.72)	Acc@5  97.66 ( 96.11)
Epoch: [35][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.9894e-01 (6.7731e-01)	Acc@1  79.69 ( 79.72)	Acc@5  96.88 ( 96.11)
Epoch: [35][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.4189e-01 (6.7907e-01)	Acc@1  77.34 ( 79.72)	Acc@5  95.31 ( 96.11)
Epoch: [35][160/391]	Time  0.034 ( 0.034)	Data  0.002 ( 0.003)	Loss 5.5381e-01 (6.7755e-01)	Acc@1  84.38 ( 79.74)	Acc@5  96.09 ( 96.09)
Epoch: [35][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.8823e-01 (6.7960e-01)	Acc@1  72.66 ( 79.67)	Acc@5  96.09 ( 96.08)
Epoch: [35][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.5479e-01 (6.8048e-01)	Acc@1  74.22 ( 79.64)	Acc@5  94.53 ( 96.05)
Epoch: [35][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6579e-01 (6.7801e-01)	Acc@1  82.03 ( 79.74)	Acc@5  97.66 ( 96.07)
Epoch: [35][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.8000e-01 (6.7877e-01)	Acc@1  80.47 ( 79.75)	Acc@5  96.09 ( 96.01)
Epoch: [35][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4593e-01 (6.8032e-01)	Acc@1  78.91 ( 79.67)	Acc@5  99.22 ( 96.01)
Epoch: [35][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.0024e-01 (6.7797e-01)	Acc@1  82.03 ( 79.73)	Acc@5  96.88 ( 96.00)
Epoch: [35][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.8236e-01 (6.8037e-01)	Acc@1  78.91 ( 79.68)	Acc@5  95.31 ( 95.98)
Epoch: [35][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4953e-01 (6.7991e-01)	Acc@1  83.59 ( 79.68)	Acc@5  96.88 ( 95.95)
Epoch: [35][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.5391e-01 (6.7782e-01)	Acc@1  77.34 ( 79.77)	Acc@5  92.97 ( 95.97)
Epoch: [35][260/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 6.0181e-01 (6.7746e-01)	Acc@1  82.03 ( 79.78)	Acc@5  97.66 ( 96.00)
Epoch: [35][270/391]	Time  0.036 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.1099e-01 (6.7786e-01)	Acc@1  82.81 ( 79.77)	Acc@5  97.66 ( 96.00)
Epoch: [35][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9744e-01 (6.7848e-01)	Acc@1  81.25 ( 79.78)	Acc@5  94.53 ( 96.00)
Epoch: [35][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.2198e-01 (6.7963e-01)	Acc@1  75.00 ( 79.73)	Acc@5  94.53 ( 95.99)
Epoch: [35][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6802e-01 (6.7768e-01)	Acc@1  82.03 ( 79.75)	Acc@5  97.66 ( 96.03)
Epoch: [35][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.2308e-01 (6.7704e-01)	Acc@1  85.16 ( 79.82)	Acc@5  94.53 ( 96.01)
Epoch: [35][320/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.6372e-01 (6.7703e-01)	Acc@1  77.34 ( 79.86)	Acc@5  92.97 ( 95.98)
Epoch: [35][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.8596e-01 (6.7945e-01)	Acc@1  81.25 ( 79.81)	Acc@5  96.88 ( 95.97)
Epoch: [35][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.2669e-01 (6.7884e-01)	Acc@1  82.03 ( 79.81)	Acc@5  99.22 ( 95.99)
Epoch: [35][350/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 6.5193e-01 (6.8060e-01)	Acc@1  75.78 ( 79.73)	Acc@5  96.88 ( 95.97)
Epoch: [35][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.3874e-01 (6.8201e-01)	Acc@1  78.91 ( 79.69)	Acc@5  93.75 ( 95.94)
Epoch: [35][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.9135e-01 (6.8083e-01)	Acc@1  82.81 ( 79.72)	Acc@5  98.44 ( 95.96)
Epoch: [35][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.6936e-01 (6.8043e-01)	Acc@1  78.12 ( 79.75)	Acc@5  96.09 ( 95.97)
Epoch: [35][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.1091e-01 (6.8011e-01)	Acc@1  80.00 ( 79.78)	Acc@5  91.25 ( 95.95)
## e[35] optimizer.zero_grad (sum) time: 0.11560750007629395
## e[35]       loss.backward (sum) time: 2.0672028064727783
## e[35]      optimizer.step (sum) time: 0.7298264503479004
## epoch[35] training(only) time: 12.970704078674316
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2868e+00 (1.2868e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.5098e+00 (1.3482e+00)	Acc@1  67.00 ( 66.00)	Acc@5  88.00 ( 88.64)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.2699e+00 (1.3239e+00)	Acc@1  66.00 ( 66.19)	Acc@5  89.00 ( 88.81)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.3934e+00 (1.3189e+00)	Acc@1  65.00 ( 65.77)	Acc@5  88.00 ( 88.48)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.1174e+00 (1.3102e+00)	Acc@1  70.00 ( 65.80)	Acc@5  93.00 ( 89.00)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3204e+00 (1.3254e+00)	Acc@1  66.00 ( 65.73)	Acc@5  88.00 ( 88.82)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.6961e+00 (1.3167e+00)	Acc@1  63.00 ( 66.03)	Acc@5  84.00 ( 89.00)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3915e+00 (1.3181e+00)	Acc@1  67.00 ( 65.93)	Acc@5  86.00 ( 89.06)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3509e+00 (1.3203e+00)	Acc@1  69.00 ( 65.93)	Acc@5  82.00 ( 89.00)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.6637e+00 (1.3117e+00)	Acc@1  57.00 ( 65.86)	Acc@5  87.00 ( 89.08)
 * Acc@1 65.810 Acc@5 89.200
### epoch[35] execution time: 14.91494107246399
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.178 ( 0.178)	Data  0.150 ( 0.150)	Loss 6.8555e-01 (6.8555e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.09 ( 96.09)
Epoch: [36][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 6.2436e-01 (5.9934e-01)	Acc@1  77.34 ( 81.82)	Acc@5  99.22 ( 96.66)
Epoch: [36][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 8.3951e-01 (6.5181e-01)	Acc@1  71.88 ( 80.80)	Acc@5  93.75 ( 95.87)
Epoch: [36][ 30/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.007)	Loss 6.8739e-01 (6.4773e-01)	Acc@1  82.03 ( 80.77)	Acc@5  96.88 ( 96.04)
Epoch: [36][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 7.0964e-01 (6.3943e-01)	Acc@1  78.91 ( 81.02)	Acc@5  95.31 ( 96.04)
Epoch: [36][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 5.8976e-01 (6.3925e-01)	Acc@1  82.81 ( 80.79)	Acc@5  97.66 ( 96.19)
Epoch: [36][ 60/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.2842e-01 (6.2887e-01)	Acc@1  88.28 ( 81.07)	Acc@5  96.88 ( 96.44)
Epoch: [36][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.2485e-01 (6.3798e-01)	Acc@1  82.81 ( 80.94)	Acc@5  95.31 ( 96.30)
Epoch: [36][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 9.7517e-01 (6.4801e-01)	Acc@1  74.22 ( 80.80)	Acc@5  89.84 ( 96.13)
Epoch: [36][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 6.2471e-01 (6.5171e-01)	Acc@1  84.38 ( 80.74)	Acc@5  94.53 ( 96.15)
Epoch: [36][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4589e-01 (6.4833e-01)	Acc@1  83.59 ( 80.77)	Acc@5  96.09 ( 96.23)
Epoch: [36][110/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.0531e-01 (6.4492e-01)	Acc@1  82.03 ( 80.93)	Acc@5  96.09 ( 96.23)
Epoch: [36][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.4853e-01 (6.5021e-01)	Acc@1  82.81 ( 80.62)	Acc@5  97.66 ( 96.20)
Epoch: [36][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.1332e-01 (6.4703e-01)	Acc@1  83.59 ( 80.71)	Acc@5  96.88 ( 96.21)
Epoch: [36][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.9818e-01 (6.4306e-01)	Acc@1  78.91 ( 80.78)	Acc@5  99.22 ( 96.30)
Epoch: [36][150/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.3637e-01 (6.4131e-01)	Acc@1  78.12 ( 80.80)	Acc@5  96.09 ( 96.33)
Epoch: [36][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6843e-01 (6.3952e-01)	Acc@1  83.59 ( 80.90)	Acc@5  98.44 ( 96.34)
Epoch: [36][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8471e-01 (6.3744e-01)	Acc@1  83.59 ( 80.92)	Acc@5  97.66 ( 96.40)
Epoch: [36][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.8556e-01 (6.3981e-01)	Acc@1  83.59 ( 80.84)	Acc@5  96.88 ( 96.35)
Epoch: [36][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.7907e-01 (6.4054e-01)	Acc@1  82.81 ( 80.77)	Acc@5  96.88 ( 96.33)
Epoch: [36][200/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.7213e-01 (6.4116e-01)	Acc@1  82.81 ( 80.75)	Acc@5  97.66 ( 96.33)
Epoch: [36][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.6200e-01 (6.4178e-01)	Acc@1  78.12 ( 80.74)	Acc@5  93.75 ( 96.33)
Epoch: [36][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9393e-01 (6.4271e-01)	Acc@1  77.34 ( 80.70)	Acc@5  97.66 ( 96.36)
Epoch: [36][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.1888e-01 (6.4427e-01)	Acc@1  78.91 ( 80.71)	Acc@5  96.09 ( 96.33)
Epoch: [36][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.2295e-01 (6.4602e-01)	Acc@1  75.00 ( 80.69)	Acc@5  92.19 ( 96.30)
Epoch: [36][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.9289e-01 (6.4308e-01)	Acc@1  83.59 ( 80.76)	Acc@5  96.09 ( 96.34)
Epoch: [36][260/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 7.2511e-01 (6.4332e-01)	Acc@1  77.34 ( 80.73)	Acc@5  96.88 ( 96.34)
Epoch: [36][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.8150e-01 (6.4314e-01)	Acc@1  78.91 ( 80.72)	Acc@5  98.44 ( 96.33)
Epoch: [36][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.0206e-01 (6.4138e-01)	Acc@1  83.59 ( 80.77)	Acc@5  96.88 ( 96.35)
Epoch: [36][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.8214e-01 (6.4137e-01)	Acc@1  82.03 ( 80.73)	Acc@5  96.09 ( 96.35)
Epoch: [36][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0070e-01 (6.4097e-01)	Acc@1  79.69 ( 80.70)	Acc@5  95.31 ( 96.36)
Epoch: [36][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.8818e-01 (6.4124e-01)	Acc@1  84.38 ( 80.73)	Acc@5  94.53 ( 96.34)
Epoch: [36][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.4042e-01 (6.4189e-01)	Acc@1  79.69 ( 80.71)	Acc@5  92.19 ( 96.34)
Epoch: [36][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0115e-01 (6.4301e-01)	Acc@1  75.00 ( 80.66)	Acc@5  94.53 ( 96.32)
Epoch: [36][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.2945e-01 (6.4471e-01)	Acc@1  73.44 ( 80.61)	Acc@5  92.97 ( 96.31)
Epoch: [36][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.0012e-01 (6.4645e-01)	Acc@1  78.91 ( 80.58)	Acc@5  95.31 ( 96.28)
Epoch: [36][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.4327e-01 (6.4751e-01)	Acc@1  81.25 ( 80.55)	Acc@5  94.53 ( 96.26)
Epoch: [36][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.1032e-01 (6.4737e-01)	Acc@1  82.03 ( 80.60)	Acc@5  95.31 ( 96.25)
Epoch: [36][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.1144e-01 (6.4772e-01)	Acc@1  82.03 ( 80.58)	Acc@5  96.88 ( 96.26)
Epoch: [36][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.0022e-01 (6.4931e-01)	Acc@1  82.50 ( 80.53)	Acc@5  96.25 ( 96.23)
## e[36] optimizer.zero_grad (sum) time: 0.11642694473266602
## e[36]       loss.backward (sum) time: 2.12369441986084
## e[36]      optimizer.step (sum) time: 0.7226665019989014
## epoch[36] training(only) time: 12.960415124893188
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2729e+00 (1.2729e+00)	Acc@1  70.00 ( 70.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.4478e+00 (1.3449e+00)	Acc@1  66.00 ( 65.18)	Acc@5  92.00 ( 89.45)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.3101e+00 (1.3273e+00)	Acc@1  67.00 ( 65.95)	Acc@5  90.00 ( 89.10)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.4775e+00 (1.3364e+00)	Acc@1  64.00 ( 65.52)	Acc@5  88.00 ( 88.55)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.2158e+00 (1.3277e+00)	Acc@1  67.00 ( 65.44)	Acc@5  91.00 ( 88.80)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.2794e+00 (1.3371e+00)	Acc@1  69.00 ( 65.39)	Acc@5  88.00 ( 88.80)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.7168e+00 (1.3291e+00)	Acc@1  66.00 ( 65.75)	Acc@5  85.00 ( 88.87)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3272e+00 (1.3281e+00)	Acc@1  68.00 ( 65.61)	Acc@5  86.00 ( 89.03)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3142e+00 (1.3327e+00)	Acc@1  69.00 ( 65.75)	Acc@5  86.00 ( 88.95)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7497e+00 (1.3228e+00)	Acc@1  53.00 ( 65.86)	Acc@5  85.00 ( 88.98)
 * Acc@1 65.810 Acc@5 88.980
### epoch[36] execution time: 14.92743992805481
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.176 ( 0.176)	Data  0.151 ( 0.151)	Loss 5.5568e-01 (5.5568e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.88 ( 96.88)
Epoch: [37][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 9.3517e-01 (6.6432e-01)	Acc@1  72.66 ( 80.40)	Acc@5  95.31 ( 96.16)
Epoch: [37][ 20/391]	Time  0.031 ( 0.040)	Data  0.001 ( 0.009)	Loss 7.0916e-01 (6.4260e-01)	Acc@1  80.47 ( 80.77)	Acc@5  94.53 ( 96.54)
Epoch: [37][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 6.6859e-01 (6.3867e-01)	Acc@1  80.47 ( 81.12)	Acc@5  93.75 ( 96.24)
Epoch: [37][ 40/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.006)	Loss 5.0541e-01 (6.1246e-01)	Acc@1  84.38 ( 81.65)	Acc@5  96.88 ( 96.61)
Epoch: [37][ 50/391]	Time  0.033 ( 0.036)	Data  0.002 ( 0.005)	Loss 8.2807e-01 (6.2764e-01)	Acc@1  78.91 ( 81.42)	Acc@5  92.97 ( 96.37)
Epoch: [37][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.9091e-01 (6.2419e-01)	Acc@1  75.00 ( 81.69)	Acc@5  93.75 ( 96.34)
Epoch: [37][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.8732e-01 (6.2444e-01)	Acc@1  80.47 ( 81.62)	Acc@5  96.09 ( 96.38)
Epoch: [37][ 80/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.4154e-01 (6.2251e-01)	Acc@1  83.59 ( 81.56)	Acc@5  98.44 ( 96.43)
Epoch: [37][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.4696e-01 (6.2133e-01)	Acc@1  79.69 ( 81.46)	Acc@5 100.00 ( 96.51)
Epoch: [37][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.4875e-01 (6.2066e-01)	Acc@1  84.38 ( 81.47)	Acc@5  97.66 ( 96.47)
Epoch: [37][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.0763e-01 (6.2253e-01)	Acc@1  84.38 ( 81.43)	Acc@5 100.00 ( 96.49)
Epoch: [37][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.7408e-01 (6.2343e-01)	Acc@1  78.12 ( 81.30)	Acc@5  96.88 ( 96.50)
Epoch: [37][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.4565e-01 (6.2096e-01)	Acc@1  75.78 ( 81.27)	Acc@5  94.53 ( 96.56)
Epoch: [37][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.2541e-01 (6.2065e-01)	Acc@1  78.91 ( 81.29)	Acc@5  95.31 ( 96.59)
Epoch: [37][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.7407e-01 (6.1992e-01)	Acc@1  87.50 ( 81.33)	Acc@5  97.66 ( 96.56)
Epoch: [37][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8960e-01 (6.1849e-01)	Acc@1  86.72 ( 81.34)	Acc@5  99.22 ( 96.57)
Epoch: [37][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.2091e-01 (6.2085e-01)	Acc@1  80.47 ( 81.27)	Acc@5  96.09 ( 96.60)
Epoch: [37][180/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 8.0232e-01 (6.2150e-01)	Acc@1  78.12 ( 81.24)	Acc@5  94.53 ( 96.55)
Epoch: [37][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.3469e-01 (6.2619e-01)	Acc@1  72.66 ( 81.10)	Acc@5  94.53 ( 96.47)
Epoch: [37][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.0523e-01 (6.2652e-01)	Acc@1  79.69 ( 81.13)	Acc@5  93.75 ( 96.49)
Epoch: [37][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.2030e-01 (6.2606e-01)	Acc@1  85.16 ( 81.14)	Acc@5  97.66 ( 96.47)
Epoch: [37][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.6430e-01 (6.2740e-01)	Acc@1  76.56 ( 81.13)	Acc@5  92.97 ( 96.46)
Epoch: [37][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.1775e-01 (6.2655e-01)	Acc@1  78.91 ( 81.17)	Acc@5  97.66 ( 96.45)
Epoch: [37][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.7314e-01 (6.2666e-01)	Acc@1  85.16 ( 81.20)	Acc@5  95.31 ( 96.41)
Epoch: [37][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.0399e-01 (6.2464e-01)	Acc@1  80.47 ( 81.24)	Acc@5  96.88 ( 96.45)
Epoch: [37][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.8015e-01 (6.2884e-01)	Acc@1  77.34 ( 81.16)	Acc@5  93.75 ( 96.41)
Epoch: [37][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.3357e-01 (6.2777e-01)	Acc@1  75.00 ( 81.19)	Acc@5  95.31 ( 96.43)
Epoch: [37][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.1138e-01 (6.2947e-01)	Acc@1  71.88 ( 81.11)	Acc@5  96.88 ( 96.43)
Epoch: [37][290/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9601e-01 (6.3140e-01)	Acc@1  82.03 ( 81.09)	Acc@5  95.31 ( 96.41)
Epoch: [37][300/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 7.1197e-01 (6.3263e-01)	Acc@1  74.22 ( 81.02)	Acc@5  96.09 ( 96.40)
Epoch: [37][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.1788e-01 (6.3393e-01)	Acc@1  81.25 ( 80.99)	Acc@5  96.09 ( 96.39)
Epoch: [37][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.5181e-01 (6.3535e-01)	Acc@1  75.78 ( 80.94)	Acc@5  94.53 ( 96.36)
Epoch: [37][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.6103e-01 (6.3492e-01)	Acc@1  85.94 ( 80.95)	Acc@5  97.66 ( 96.37)
Epoch: [37][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.0921e-01 (6.3478e-01)	Acc@1  78.12 ( 80.95)	Acc@5  96.09 ( 96.38)
Epoch: [37][350/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.4539e-01 (6.3520e-01)	Acc@1  80.47 ( 80.95)	Acc@5  96.09 ( 96.39)
Epoch: [37][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.5402e-01 (6.3419e-01)	Acc@1  85.16 ( 80.99)	Acc@5  96.09 ( 96.42)
Epoch: [37][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.9961e-01 (6.3548e-01)	Acc@1  80.47 ( 80.93)	Acc@5  96.09 ( 96.41)
Epoch: [37][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.6680e-01 (6.3687e-01)	Acc@1  81.25 ( 80.89)	Acc@5  96.09 ( 96.40)
Epoch: [37][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.8926e-01 (6.3638e-01)	Acc@1  81.25 ( 80.87)	Acc@5  97.50 ( 96.40)
## e[37] optimizer.zero_grad (sum) time: 0.11645722389221191
## e[37]       loss.backward (sum) time: 2.085982322692871
## e[37]      optimizer.step (sum) time: 0.7181293964385986
## epoch[37] training(only) time: 13.000680208206177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.1963e+00 (1.1963e+00)	Acc@1  71.00 ( 71.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.019 ( 0.029)	Loss 1.5279e+00 (1.3476e+00)	Acc@1  63.00 ( 65.36)	Acc@5  90.00 ( 89.09)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.2977e+00 (1.3242e+00)	Acc@1  65.00 ( 65.86)	Acc@5  90.00 ( 88.81)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.5071e+00 (1.3312e+00)	Acc@1  60.00 ( 65.77)	Acc@5  86.00 ( 88.68)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.2017e+00 (1.3183e+00)	Acc@1  63.00 ( 66.02)	Acc@5  94.00 ( 89.00)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3586e+00 (1.3358e+00)	Acc@1  65.00 ( 65.88)	Acc@5  90.00 ( 88.96)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.7207e+00 (1.3251e+00)	Acc@1  63.00 ( 66.30)	Acc@5  86.00 ( 89.15)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3756e+00 (1.3250e+00)	Acc@1  72.00 ( 66.14)	Acc@5  85.00 ( 89.14)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.2807e+00 (1.3268e+00)	Acc@1  70.00 ( 66.15)	Acc@5  88.00 ( 89.17)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7485e+00 (1.3207e+00)	Acc@1  57.00 ( 66.23)	Acc@5  84.00 ( 89.19)
 * Acc@1 66.240 Acc@5 89.300
### epoch[37] execution time: 14.939343690872192
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.177 ( 0.177)	Data  0.149 ( 0.149)	Loss 9.2650e-01 (9.2650e-01)	Acc@1  73.44 ( 73.44)	Acc@5  94.53 ( 94.53)
Epoch: [38][ 10/391]	Time  0.034 ( 0.046)	Data  0.001 ( 0.015)	Loss 6.6850e-01 (6.5421e-01)	Acc@1  84.38 ( 80.68)	Acc@5  97.66 ( 96.45)
Epoch: [38][ 20/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.009)	Loss 5.8315e-01 (6.1973e-01)	Acc@1  83.59 ( 81.51)	Acc@5  95.31 ( 96.58)
Epoch: [38][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.006)	Loss 6.3396e-01 (6.2249e-01)	Acc@1  82.03 ( 81.50)	Acc@5  96.88 ( 96.65)
Epoch: [38][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 4.8896e-01 (6.1610e-01)	Acc@1  85.94 ( 81.63)	Acc@5  96.88 ( 96.65)
Epoch: [38][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 7.5751e-01 (6.1962e-01)	Acc@1  77.34 ( 81.74)	Acc@5  95.31 ( 96.68)
Epoch: [38][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.8878e-01 (6.1928e-01)	Acc@1  82.81 ( 81.83)	Acc@5  96.09 ( 96.80)
Epoch: [38][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.3090e-01 (6.1970e-01)	Acc@1  78.12 ( 81.69)	Acc@5  96.88 ( 96.69)
Epoch: [38][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 6.4145e-01 (6.1476e-01)	Acc@1  83.59 ( 81.89)	Acc@5  95.31 ( 96.65)
Epoch: [38][ 90/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.004)	Loss 6.4400e-01 (6.1569e-01)	Acc@1  78.12 ( 81.91)	Acc@5  97.66 ( 96.62)
Epoch: [38][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4909e-01 (6.1419e-01)	Acc@1  80.47 ( 81.88)	Acc@5  98.44 ( 96.60)
Epoch: [38][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.8968e-01 (6.1209e-01)	Acc@1  82.81 ( 81.93)	Acc@5  97.66 ( 96.59)
Epoch: [38][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4960e-01 (6.1406e-01)	Acc@1  81.25 ( 81.76)	Acc@5  97.66 ( 96.65)
Epoch: [38][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.2340e-01 (6.1279e-01)	Acc@1  81.25 ( 81.74)	Acc@5  95.31 ( 96.62)
Epoch: [38][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.3506e-01 (6.0699e-01)	Acc@1  84.38 ( 81.93)	Acc@5  97.66 ( 96.70)
Epoch: [38][150/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.0670e-01 (6.0798e-01)	Acc@1  80.47 ( 81.84)	Acc@5  96.88 ( 96.72)
Epoch: [38][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9821e-01 (6.0748e-01)	Acc@1  84.38 ( 81.82)	Acc@5  92.97 ( 96.71)
Epoch: [38][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.5375e-01 (6.0837e-01)	Acc@1  81.25 ( 81.75)	Acc@5  96.88 ( 96.69)
Epoch: [38][180/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 5.6833e-01 (6.0842e-01)	Acc@1  82.03 ( 81.75)	Acc@5  97.66 ( 96.67)
Epoch: [38][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.1895e-01 (6.0943e-01)	Acc@1  79.69 ( 81.77)	Acc@5  96.09 ( 96.66)
Epoch: [38][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.1178e-01 (6.0786e-01)	Acc@1  82.03 ( 81.83)	Acc@5  97.66 ( 96.70)
Epoch: [38][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.7553e-01 (6.0859e-01)	Acc@1  81.25 ( 81.75)	Acc@5  98.44 ( 96.72)
Epoch: [38][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.1581e-01 (6.0615e-01)	Acc@1  82.81 ( 81.85)	Acc@5  95.31 ( 96.72)
Epoch: [38][230/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 6.2348e-01 (6.0744e-01)	Acc@1  81.25 ( 81.79)	Acc@5  98.44 ( 96.74)
Epoch: [38][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.7609e-01 (6.0573e-01)	Acc@1  83.59 ( 81.82)	Acc@5  97.66 ( 96.76)
Epoch: [38][250/391]	Time  0.029 ( 0.033)	Data  0.005 ( 0.003)	Loss 5.9060e-01 (6.0804e-01)	Acc@1  80.47 ( 81.74)	Acc@5  98.44 ( 96.74)
Epoch: [38][260/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 6.8743e-01 (6.0834e-01)	Acc@1  79.69 ( 81.74)	Acc@5  95.31 ( 96.75)
Epoch: [38][270/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.1817e-01 (6.0832e-01)	Acc@1  79.69 ( 81.72)	Acc@5  94.53 ( 96.74)
Epoch: [38][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4167e-01 (6.0965e-01)	Acc@1  85.94 ( 81.69)	Acc@5  97.66 ( 96.71)
Epoch: [38][290/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.0377e-01 (6.0851e-01)	Acc@1  79.69 ( 81.69)	Acc@5  97.66 ( 96.72)
Epoch: [38][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.8282e-01 (6.0693e-01)	Acc@1  84.38 ( 81.72)	Acc@5  94.53 ( 96.75)
Epoch: [38][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.9484e-01 (6.0768e-01)	Acc@1  85.16 ( 81.72)	Acc@5  96.09 ( 96.72)
Epoch: [38][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.0539e-01 (6.0898e-01)	Acc@1  82.03 ( 81.70)	Acc@5  96.09 ( 96.69)
Epoch: [38][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.9151e-01 (6.0895e-01)	Acc@1  81.25 ( 81.72)	Acc@5  92.97 ( 96.68)
Epoch: [38][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.1977e-01 (6.0808e-01)	Acc@1  82.81 ( 81.74)	Acc@5  97.66 ( 96.70)
Epoch: [38][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.8102e-01 (6.0844e-01)	Acc@1  83.59 ( 81.74)	Acc@5  95.31 ( 96.69)
Epoch: [38][360/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.9428e-01 (6.0982e-01)	Acc@1  77.34 ( 81.70)	Acc@5  96.09 ( 96.67)
Epoch: [38][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.7546e-01 (6.0941e-01)	Acc@1  80.47 ( 81.71)	Acc@5  97.66 ( 96.68)
Epoch: [38][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.2666e-01 (6.0983e-01)	Acc@1  84.38 ( 81.73)	Acc@5  96.88 ( 96.66)
Epoch: [38][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.6781e-01 (6.0956e-01)	Acc@1  80.00 ( 81.74)	Acc@5  93.75 ( 96.67)
## e[38] optimizer.zero_grad (sum) time: 0.11736059188842773
## e[38]       loss.backward (sum) time: 2.102471113204956
## e[38]      optimizer.step (sum) time: 0.7427825927734375
## epoch[38] training(only) time: 12.959413051605225
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.2294e+00 (1.2294e+00)	Acc@1  69.00 ( 69.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.4835e+00 (1.3450e+00)	Acc@1  63.00 ( 66.27)	Acc@5  91.00 ( 88.73)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.4277e+00 (1.3402e+00)	Acc@1  65.00 ( 66.19)	Acc@5  90.00 ( 88.76)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.4818e+00 (1.3461e+00)	Acc@1  61.00 ( 65.68)	Acc@5  86.00 ( 88.61)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.1911e+00 (1.3297e+00)	Acc@1  66.00 ( 65.78)	Acc@5  92.00 ( 89.07)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.2721e+00 (1.3408e+00)	Acc@1  66.00 ( 65.86)	Acc@5  89.00 ( 88.98)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.7392e+00 (1.3350e+00)	Acc@1  62.00 ( 66.20)	Acc@5  85.00 ( 88.95)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3715e+00 (1.3339e+00)	Acc@1  69.00 ( 66.04)	Acc@5  87.00 ( 89.07)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.3461e+00 (1.3365e+00)	Acc@1  67.00 ( 66.01)	Acc@5  86.00 ( 89.00)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7245e+00 (1.3261e+00)	Acc@1  59.00 ( 66.24)	Acc@5  87.00 ( 89.03)
 * Acc@1 66.170 Acc@5 89.080
### epoch[38] execution time: 14.889393091201782
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.175 ( 0.175)	Data  0.147 ( 0.147)	Loss 6.3706e-01 (6.3706e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.88 ( 96.88)
Epoch: [39][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 6.3817e-01 (5.8358e-01)	Acc@1  82.81 ( 83.74)	Acc@5  96.88 ( 96.80)
Epoch: [39][ 20/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.009)	Loss 4.2134e-01 (5.6859e-01)	Acc@1  89.06 ( 83.74)	Acc@5  99.22 ( 96.99)
Epoch: [39][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 6.5255e-01 (5.6136e-01)	Acc@1  81.25 ( 83.47)	Acc@5  96.09 ( 96.98)
Epoch: [39][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 7.3937e-01 (5.7754e-01)	Acc@1  85.16 ( 83.14)	Acc@5  93.75 ( 96.89)
Epoch: [39][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 6.8096e-01 (5.7224e-01)	Acc@1  78.12 ( 83.10)	Acc@5  96.09 ( 97.00)
Epoch: [39][ 60/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.004)	Loss 4.9975e-01 (5.6802e-01)	Acc@1  85.94 ( 83.07)	Acc@5  98.44 ( 97.09)
Epoch: [39][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.2564e-01 (5.7280e-01)	Acc@1  80.47 ( 82.79)	Acc@5  96.09 ( 97.00)
Epoch: [39][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 6.4327e-01 (5.7619e-01)	Acc@1  81.25 ( 82.82)	Acc@5  96.88 ( 96.94)
Epoch: [39][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.6016e-01 (5.7629e-01)	Acc@1  83.59 ( 82.79)	Acc@5  97.66 ( 96.94)
Epoch: [39][100/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4856e-01 (5.7820e-01)	Acc@1  82.81 ( 82.69)	Acc@5  97.66 ( 96.86)
Epoch: [39][110/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.7316e-01 (5.7496e-01)	Acc@1  87.50 ( 82.84)	Acc@5  97.66 ( 96.95)
Epoch: [39][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.9218e-01 (5.8182e-01)	Acc@1  82.03 ( 82.70)	Acc@5  96.09 ( 96.85)
Epoch: [39][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.1548e-01 (5.7712e-01)	Acc@1  81.25 ( 82.78)	Acc@5  94.53 ( 96.86)
Epoch: [39][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.5938e-01 (5.8083e-01)	Acc@1  83.59 ( 82.64)	Acc@5  98.44 ( 96.85)
Epoch: [39][150/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.9532e-01 (5.8329e-01)	Acc@1  76.56 ( 82.53)	Acc@5  94.53 ( 96.85)
Epoch: [39][160/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.8110e-01 (5.8177e-01)	Acc@1  82.81 ( 82.55)	Acc@5  98.44 ( 96.82)
Epoch: [39][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4647e-01 (5.8408e-01)	Acc@1  82.03 ( 82.45)	Acc@5  98.44 ( 96.82)
Epoch: [39][180/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 5.2766e-01 (5.8573e-01)	Acc@1  80.47 ( 82.39)	Acc@5  98.44 ( 96.81)
Epoch: [39][190/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 6.8659e-01 (5.8481e-01)	Acc@1  82.03 ( 82.44)	Acc@5  97.66 ( 96.83)
Epoch: [39][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6190e-01 (5.8748e-01)	Acc@1  86.72 ( 82.37)	Acc@5  96.09 ( 96.80)
Epoch: [39][210/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 5.4740e-01 (5.8768e-01)	Acc@1  83.59 ( 82.32)	Acc@5  98.44 ( 96.83)
Epoch: [39][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.9037e-01 (5.8863e-01)	Acc@1  80.47 ( 82.27)	Acc@5  97.66 ( 96.81)
Epoch: [39][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.6706e-01 (5.8919e-01)	Acc@1  80.47 ( 82.27)	Acc@5  96.09 ( 96.80)
Epoch: [39][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.0679e-01 (5.8952e-01)	Acc@1  81.25 ( 82.27)	Acc@5  96.88 ( 96.82)
Epoch: [39][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7208e-01 (5.9095e-01)	Acc@1  82.81 ( 82.20)	Acc@5  96.09 ( 96.80)
Epoch: [39][260/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.0954e-01 (5.9001e-01)	Acc@1  86.72 ( 82.23)	Acc@5  98.44 ( 96.83)
Epoch: [39][270/391]	Time  0.030 ( 0.033)	Data  0.002 ( 0.003)	Loss 7.0288e-01 (5.8975e-01)	Acc@1  78.12 ( 82.23)	Acc@5  96.09 ( 96.85)
Epoch: [39][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9217e-01 (5.8889e-01)	Acc@1  82.81 ( 82.28)	Acc@5  95.31 ( 96.84)
Epoch: [39][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6647e-01 (5.8795e-01)	Acc@1  82.03 ( 82.29)	Acc@5  99.22 ( 96.85)
Epoch: [39][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.1868e-01 (5.8942e-01)	Acc@1  84.38 ( 82.23)	Acc@5  99.22 ( 96.84)
Epoch: [39][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9064e-01 (5.9030e-01)	Acc@1  81.25 ( 82.20)	Acc@5  96.09 ( 96.84)
Epoch: [39][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8223e-01 (5.9005e-01)	Acc@1  87.50 ( 82.20)	Acc@5  95.31 ( 96.84)
Epoch: [39][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.4127e-01 (5.8997e-01)	Acc@1  78.91 ( 82.23)	Acc@5  95.31 ( 96.83)
Epoch: [39][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 6.6849e-01 (5.9003e-01)	Acc@1  82.03 ( 82.24)	Acc@5  93.75 ( 96.81)
Epoch: [39][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.4050e-01 (5.9040e-01)	Acc@1  82.81 ( 82.21)	Acc@5  98.44 ( 96.81)
Epoch: [39][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.0364e-01 (5.9066e-01)	Acc@1  79.69 ( 82.17)	Acc@5  96.88 ( 96.81)
Epoch: [39][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.1158e-01 (5.9063e-01)	Acc@1  82.81 ( 82.19)	Acc@5  96.09 ( 96.81)
Epoch: [39][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.6895e-01 (5.9298e-01)	Acc@1  76.56 ( 82.14)	Acc@5  95.31 ( 96.77)
Epoch: [39][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.4260e-01 (5.9235e-01)	Acc@1  82.50 ( 82.14)	Acc@5  93.75 ( 96.77)
## e[39] optimizer.zero_grad (sum) time: 0.11610984802246094
## e[39]       loss.backward (sum) time: 2.132857084274292
## e[39]      optimizer.step (sum) time: 0.7281112670898438
## epoch[39] training(only) time: 13.012413024902344
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2890e+00 (1.2890e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.5352e+00 (1.3843e+00)	Acc@1  67.00 ( 65.55)	Acc@5  89.00 ( 89.00)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.3615e+00 (1.3656e+00)	Acc@1  66.00 ( 66.24)	Acc@5  88.00 ( 88.86)
Test: [ 30/100]	Time  0.016 ( 0.021)	Loss 1.5280e+00 (1.3718e+00)	Acc@1  63.00 ( 65.61)	Acc@5  88.00 ( 88.61)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.1664e+00 (1.3556e+00)	Acc@1  69.00 ( 65.98)	Acc@5  91.00 ( 88.90)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.2623e+00 (1.3671e+00)	Acc@1  67.00 ( 65.86)	Acc@5  89.00 ( 88.86)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 1.8722e+00 (1.3568e+00)	Acc@1  61.00 ( 66.23)	Acc@5  86.00 ( 89.00)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3447e+00 (1.3542e+00)	Acc@1  69.00 ( 66.25)	Acc@5  87.00 ( 88.99)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3119e+00 (1.3501e+00)	Acc@1  70.00 ( 66.43)	Acc@5  86.00 ( 88.93)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7942e+00 (1.3387e+00)	Acc@1  55.00 ( 66.52)	Acc@5  86.00 ( 89.05)
 * Acc@1 66.520 Acc@5 89.140
### epoch[39] execution time: 14.97153353691101
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.173 ( 0.173)	Data  0.145 ( 0.145)	Loss 5.3809e-01 (5.3809e-01)	Acc@1  82.81 ( 82.81)	Acc@5  98.44 ( 98.44)
Epoch: [40][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 4.9955e-01 (5.6272e-01)	Acc@1  81.25 ( 82.46)	Acc@5  98.44 ( 97.59)
Epoch: [40][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 6.0049e-01 (5.6222e-01)	Acc@1  80.47 ( 82.25)	Acc@5  97.66 ( 97.62)
Epoch: [40][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 5.2001e-01 (5.4675e-01)	Acc@1  84.38 ( 83.22)	Acc@5  97.66 ( 97.66)
Epoch: [40][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 5.2609e-01 (5.4914e-01)	Acc@1  80.47 ( 83.10)	Acc@5  96.88 ( 97.64)
Epoch: [40][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 6.2624e-01 (5.6617e-01)	Acc@1  84.38 ( 82.83)	Acc@5  96.09 ( 97.38)
Epoch: [40][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.1077e-01 (5.5722e-01)	Acc@1  85.16 ( 83.18)	Acc@5  96.09 ( 97.37)
Epoch: [40][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.1471e-01 (5.6393e-01)	Acc@1  85.16 ( 83.10)	Acc@5  97.66 ( 97.28)
Epoch: [40][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.4370e-01 (5.5803e-01)	Acc@1  91.41 ( 83.24)	Acc@5  98.44 ( 97.32)
Epoch: [40][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.0807e-01 (5.5720e-01)	Acc@1  83.59 ( 83.20)	Acc@5  97.66 ( 97.29)
Epoch: [40][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.2233e-01 (5.6001e-01)	Acc@1  81.25 ( 83.03)	Acc@5  96.09 ( 97.30)
Epoch: [40][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.9287e-01 (5.6081e-01)	Acc@1  81.25 ( 83.06)	Acc@5  94.53 ( 97.25)
Epoch: [40][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.3990e-01 (5.6742e-01)	Acc@1  76.56 ( 82.90)	Acc@5  95.31 ( 97.19)
Epoch: [40][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.0228e-01 (5.6462e-01)	Acc@1  78.91 ( 82.89)	Acc@5  97.66 ( 97.22)
Epoch: [40][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.9777e-01 (5.6704e-01)	Acc@1  79.69 ( 82.72)	Acc@5  98.44 ( 97.17)
Epoch: [40][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.2478e-01 (5.6548e-01)	Acc@1  78.91 ( 82.71)	Acc@5  99.22 ( 97.19)
Epoch: [40][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.8014e-01 (5.6375e-01)	Acc@1  80.47 ( 82.76)	Acc@5  98.44 ( 97.18)
Epoch: [40][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4072e-01 (5.5906e-01)	Acc@1  85.16 ( 82.92)	Acc@5  98.44 ( 97.19)
Epoch: [40][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.5294e-01 (5.5934e-01)	Acc@1  78.91 ( 82.91)	Acc@5  96.09 ( 97.16)
Epoch: [40][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.7323e-01 (5.5814e-01)	Acc@1  82.81 ( 82.95)	Acc@5  98.44 ( 97.14)
Epoch: [40][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.9063e-01 (5.5808e-01)	Acc@1  82.81 ( 82.96)	Acc@5  94.53 ( 97.11)
Epoch: [40][210/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.9467e-01 (5.6110e-01)	Acc@1  75.00 ( 82.85)	Acc@5  96.88 ( 97.10)
Epoch: [40][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.8109e-01 (5.6018e-01)	Acc@1  79.69 ( 82.84)	Acc@5  95.31 ( 97.13)
Epoch: [40][230/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.2895e-01 (5.6160e-01)	Acc@1  86.72 ( 82.82)	Acc@5  96.88 ( 97.12)
Epoch: [40][240/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4275e-01 (5.6193e-01)	Acc@1  89.84 ( 82.82)	Acc@5  97.66 ( 97.10)
Epoch: [40][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.0923e-01 (5.6306e-01)	Acc@1  85.16 ( 82.75)	Acc@5  96.09 ( 97.09)
Epoch: [40][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4630e-01 (5.6351e-01)	Acc@1  85.94 ( 82.74)	Acc@5  97.66 ( 97.10)
Epoch: [40][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7671e-01 (5.6409e-01)	Acc@1  86.72 ( 82.69)	Acc@5  98.44 ( 97.10)
Epoch: [40][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.5984e-01 (5.6351e-01)	Acc@1  82.03 ( 82.73)	Acc@5  96.88 ( 97.09)
Epoch: [40][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9571e-01 (5.6215e-01)	Acc@1  89.06 ( 82.73)	Acc@5  98.44 ( 97.12)
Epoch: [40][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7768e-01 (5.6395e-01)	Acc@1  85.16 ( 82.69)	Acc@5  99.22 ( 97.11)
Epoch: [40][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3533e-01 (5.6475e-01)	Acc@1  82.81 ( 82.66)	Acc@5  96.09 ( 97.10)
Epoch: [40][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.3058e-01 (5.6572e-01)	Acc@1  79.69 ( 82.65)	Acc@5  98.44 ( 97.12)
Epoch: [40][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.7252e-01 (5.6634e-01)	Acc@1  82.03 ( 82.66)	Acc@5  95.31 ( 97.11)
Epoch: [40][340/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3328e-01 (5.6722e-01)	Acc@1  80.47 ( 82.63)	Acc@5  98.44 ( 97.08)
Epoch: [40][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.8713e-01 (5.6606e-01)	Acc@1  85.94 ( 82.68)	Acc@5  96.88 ( 97.08)
Epoch: [40][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.3520e-01 (5.6755e-01)	Acc@1  72.66 ( 82.64)	Acc@5  96.09 ( 97.07)
Epoch: [40][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6939e-01 (5.6725e-01)	Acc@1  89.06 ( 82.65)	Acc@5  99.22 ( 97.06)
Epoch: [40][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3618e-01 (5.6832e-01)	Acc@1  82.03 ( 82.64)	Acc@5  95.31 ( 97.05)
Epoch: [40][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.2114e-01 (5.6848e-01)	Acc@1  83.75 ( 82.61)	Acc@5  98.75 ( 97.04)
## e[40] optimizer.zero_grad (sum) time: 0.11615180969238281
## e[40]       loss.backward (sum) time: 2.112006902694702
## e[40]      optimizer.step (sum) time: 0.7365329265594482
## epoch[40] training(only) time: 13.000875473022461
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.2956e+00 (1.2956e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.5603e+00 (1.4168e+00)	Acc@1  67.00 ( 64.82)	Acc@5  88.00 ( 87.91)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.3640e+00 (1.3874e+00)	Acc@1  66.00 ( 65.76)	Acc@5  89.00 ( 88.38)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.6546e+00 (1.3954e+00)	Acc@1  62.00 ( 65.23)	Acc@5  87.00 ( 88.45)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.1577e+00 (1.3743e+00)	Acc@1  67.00 ( 65.46)	Acc@5  93.00 ( 88.90)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.2598e+00 (1.3908e+00)	Acc@1  65.00 ( 65.35)	Acc@5  92.00 ( 88.59)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 1.7994e+00 (1.3802e+00)	Acc@1  61.00 ( 65.72)	Acc@5  86.00 ( 88.80)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3659e+00 (1.3745e+00)	Acc@1  64.00 ( 65.59)	Acc@5  87.00 ( 88.93)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.3356e+00 (1.3742e+00)	Acc@1  67.00 ( 65.64)	Acc@5  89.00 ( 88.85)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7412e+00 (1.3630e+00)	Acc@1  59.00 ( 65.85)	Acc@5  87.00 ( 88.90)
 * Acc@1 65.950 Acc@5 89.000
### epoch[40] execution time: 14.93459939956665
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.187 ( 0.187)	Data  0.159 ( 0.159)	Loss 6.0755e-01 (6.0755e-01)	Acc@1  83.59 ( 83.59)	Acc@5  98.44 ( 98.44)
Epoch: [41][ 10/391]	Time  0.032 ( 0.047)	Data  0.001 ( 0.016)	Loss 5.0980e-01 (5.7342e-01)	Acc@1  80.47 ( 82.10)	Acc@5  97.66 ( 97.02)
Epoch: [41][ 20/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.009)	Loss 5.6495e-01 (5.6638e-01)	Acc@1  82.03 ( 82.25)	Acc@5  98.44 ( 97.14)
Epoch: [41][ 30/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.007)	Loss 4.9841e-01 (5.5226e-01)	Acc@1  85.16 ( 82.71)	Acc@5  96.09 ( 97.15)
Epoch: [41][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 4.3741e-01 (5.5521e-01)	Acc@1  87.50 ( 82.74)	Acc@5  98.44 ( 97.07)
Epoch: [41][ 50/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 5.7261e-01 (5.5845e-01)	Acc@1  82.03 ( 82.77)	Acc@5  97.66 ( 96.98)
Epoch: [41][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 5.1511e-01 (5.5297e-01)	Acc@1  85.16 ( 83.04)	Acc@5  96.88 ( 97.11)
Epoch: [41][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.9958e-01 (5.4561e-01)	Acc@1  85.16 ( 83.14)	Acc@5  96.88 ( 97.22)
Epoch: [41][ 80/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.5125e-01 (5.4280e-01)	Acc@1  90.62 ( 83.24)	Acc@5  99.22 ( 97.19)
Epoch: [41][ 90/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.004)	Loss 4.9642e-01 (5.4173e-01)	Acc@1  87.50 ( 83.17)	Acc@5  96.88 ( 97.24)
Epoch: [41][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.7550e-01 (5.4698e-01)	Acc@1  82.03 ( 83.04)	Acc@5  98.44 ( 97.22)
Epoch: [41][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.8258e-01 (5.4578e-01)	Acc@1  80.47 ( 83.02)	Acc@5  96.88 ( 97.28)
Epoch: [41][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.0095e-01 (5.4499e-01)	Acc@1  88.28 ( 83.01)	Acc@5  99.22 ( 97.34)
Epoch: [41][130/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.3414e-01 (5.4513e-01)	Acc@1  81.25 ( 83.05)	Acc@5  96.09 ( 97.32)
Epoch: [41][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.6689e-01 (5.4503e-01)	Acc@1  79.69 ( 83.03)	Acc@5  99.22 ( 97.31)
Epoch: [41][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.0116e-01 (5.4391e-01)	Acc@1  84.38 ( 83.03)	Acc@5  98.44 ( 97.34)
Epoch: [41][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4184e-01 (5.4170e-01)	Acc@1  88.28 ( 83.16)	Acc@5  99.22 ( 97.36)
Epoch: [41][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6395e-01 (5.4435e-01)	Acc@1  84.38 ( 83.13)	Acc@5  96.88 ( 97.34)
Epoch: [41][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6574e-01 (5.4368e-01)	Acc@1  85.16 ( 83.23)	Acc@5  97.66 ( 97.33)
Epoch: [41][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.4239e-01 (5.4257e-01)	Acc@1  82.81 ( 83.28)	Acc@5  94.53 ( 97.34)
Epoch: [41][200/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 5.4163e-01 (5.4042e-01)	Acc@1  82.81 ( 83.31)	Acc@5  96.88 ( 97.35)
Epoch: [41][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.3321e-01 (5.4085e-01)	Acc@1  84.38 ( 83.28)	Acc@5  96.88 ( 97.36)
Epoch: [41][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6680e-01 (5.4151e-01)	Acc@1  87.50 ( 83.28)	Acc@5  98.44 ( 97.33)
Epoch: [41][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7408e-01 (5.4029e-01)	Acc@1  84.38 ( 83.34)	Acc@5  97.66 ( 97.32)
Epoch: [41][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4519e-01 (5.4223e-01)	Acc@1  87.50 ( 83.31)	Acc@5  99.22 ( 97.29)
Epoch: [41][250/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 5.1947e-01 (5.4175e-01)	Acc@1  83.59 ( 83.34)	Acc@5  98.44 ( 97.28)
Epoch: [41][260/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 6.1328e-01 (5.4067e-01)	Acc@1  85.94 ( 83.37)	Acc@5  94.53 ( 97.28)
Epoch: [41][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4173e-01 (5.4017e-01)	Acc@1  85.94 ( 83.41)	Acc@5  97.66 ( 97.27)
Epoch: [41][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.9995e-01 (5.4111e-01)	Acc@1  85.94 ( 83.42)	Acc@5  97.66 ( 97.24)
Epoch: [41][290/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8990e-01 (5.4100e-01)	Acc@1  83.59 ( 83.42)	Acc@5  98.44 ( 97.24)
Epoch: [41][300/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.4924e-01 (5.4087e-01)	Acc@1  78.91 ( 83.42)	Acc@5  96.09 ( 97.24)
Epoch: [41][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6060e-01 (5.4203e-01)	Acc@1  87.50 ( 83.36)	Acc@5 100.00 ( 97.22)
Epoch: [41][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.3601e-01 (5.4317e-01)	Acc@1  80.47 ( 83.34)	Acc@5  96.09 ( 97.20)
Epoch: [41][330/391]	Time  0.034 ( 0.033)	Data  0.004 ( 0.003)	Loss 3.8686e-01 (5.4475e-01)	Acc@1  86.72 ( 83.32)	Acc@5  97.66 ( 97.20)
Epoch: [41][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.2790e-01 (5.4565e-01)	Acc@1  78.91 ( 83.31)	Acc@5  96.88 ( 97.20)
Epoch: [41][350/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 5.9775e-01 (5.4748e-01)	Acc@1  80.47 ( 83.26)	Acc@5  96.09 ( 97.17)
Epoch: [41][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.9835e-01 (5.4810e-01)	Acc@1  81.25 ( 83.26)	Acc@5  93.75 ( 97.16)
Epoch: [41][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.1919e-01 (5.4845e-01)	Acc@1  79.69 ( 83.25)	Acc@5  97.66 ( 97.15)
Epoch: [41][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.4102e-01 (5.4891e-01)	Acc@1  82.81 ( 83.23)	Acc@5  97.66 ( 97.15)
Epoch: [41][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.3939e-01 (5.4949e-01)	Acc@1  80.00 ( 83.20)	Acc@5  97.50 ( 97.14)
## e[41] optimizer.zero_grad (sum) time: 0.11530351638793945
## e[41]       loss.backward (sum) time: 2.0899949073791504
## e[41]      optimizer.step (sum) time: 0.7353518009185791
## epoch[41] training(only) time: 12.96638298034668
# Switched to evaluate mode...
Test: [  0/100]	Time  0.130 ( 0.130)	Loss 1.3008e+00 (1.3008e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.7358e+00 (1.4367e+00)	Acc@1  62.00 ( 65.91)	Acc@5  88.00 ( 87.91)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.3740e+00 (1.4159e+00)	Acc@1  69.00 ( 66.14)	Acc@5  91.00 ( 88.38)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.5677e+00 (1.4164e+00)	Acc@1  59.00 ( 65.52)	Acc@5  87.00 ( 88.32)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.2110e+00 (1.3931e+00)	Acc@1  67.00 ( 65.59)	Acc@5  93.00 ( 88.85)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.2529e+00 (1.3976e+00)	Acc@1  66.00 ( 65.49)	Acc@5  89.00 ( 88.75)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.7764e+00 (1.3861e+00)	Acc@1  63.00 ( 65.77)	Acc@5  83.00 ( 88.77)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3477e+00 (1.3774e+00)	Acc@1  68.00 ( 65.77)	Acc@5  87.00 ( 88.92)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.2945e+00 (1.3755e+00)	Acc@1  72.00 ( 65.89)	Acc@5  88.00 ( 88.91)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 1.6332e+00 (1.3626e+00)	Acc@1  61.00 ( 66.01)	Acc@5  86.00 ( 88.99)
 * Acc@1 66.030 Acc@5 89.010
### epoch[41] execution time: 14.900643587112427
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.174 ( 0.174)	Data  0.146 ( 0.146)	Loss 4.9075e-01 (4.9075e-01)	Acc@1  80.47 ( 80.47)	Acc@5  98.44 ( 98.44)
Epoch: [42][ 10/391]	Time  0.031 ( 0.045)	Data  0.001 ( 0.015)	Loss 4.9883e-01 (5.1565e-01)	Acc@1  82.81 ( 83.81)	Acc@5  97.66 ( 97.44)
Epoch: [42][ 20/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.009)	Loss 3.5483e-01 (5.1059e-01)	Acc@1  86.72 ( 84.30)	Acc@5 100.00 ( 97.77)
Epoch: [42][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.006)	Loss 5.7213e-01 (5.1228e-01)	Acc@1  82.03 ( 84.53)	Acc@5  97.66 ( 97.63)
Epoch: [42][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.5284e-01 (5.0669e-01)	Acc@1  90.62 ( 84.58)	Acc@5  97.66 ( 97.69)
Epoch: [42][ 50/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.005)	Loss 4.8820e-01 (5.0774e-01)	Acc@1  85.94 ( 84.56)	Acc@5  98.44 ( 97.72)
Epoch: [42][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.8592e-01 (5.0185e-01)	Acc@1  87.50 ( 84.85)	Acc@5  97.66 ( 97.71)
Epoch: [42][ 70/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.8424e-01 (5.1249e-01)	Acc@1  85.16 ( 84.42)	Acc@5  98.44 ( 97.67)
Epoch: [42][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.5331e-01 (5.1082e-01)	Acc@1  85.94 ( 84.51)	Acc@5  98.44 ( 97.60)
Epoch: [42][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.3993e-01 (5.1025e-01)	Acc@1  88.28 ( 84.53)	Acc@5  96.88 ( 97.60)
Epoch: [42][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.2121e-01 (5.1490e-01)	Acc@1  82.81 ( 84.44)	Acc@5  96.88 ( 97.56)
Epoch: [42][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.0363e-01 (5.1778e-01)	Acc@1  82.81 ( 84.40)	Acc@5  95.31 ( 97.55)
Epoch: [42][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.1592e-01 (5.1616e-01)	Acc@1  85.16 ( 84.40)	Acc@5  96.88 ( 97.54)
Epoch: [42][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.5313e-01 (5.1816e-01)	Acc@1  80.47 ( 84.38)	Acc@5  96.09 ( 97.50)
Epoch: [42][140/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.2639e-01 (5.1596e-01)	Acc@1  85.94 ( 84.49)	Acc@5  96.88 ( 97.54)
Epoch: [42][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7588e-01 (5.1554e-01)	Acc@1  89.84 ( 84.52)	Acc@5  99.22 ( 97.52)
Epoch: [42][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6808e-01 (5.1372e-01)	Acc@1  85.94 ( 84.52)	Acc@5  98.44 ( 97.54)
Epoch: [42][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6042e-01 (5.1746e-01)	Acc@1  79.69 ( 84.38)	Acc@5  99.22 ( 97.49)
Epoch: [42][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.3536e-01 (5.2025e-01)	Acc@1  83.59 ( 84.27)	Acc@5  98.44 ( 97.46)
Epoch: [42][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.5529e-01 (5.2223e-01)	Acc@1  79.69 ( 84.24)	Acc@5  96.09 ( 97.41)
Epoch: [42][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.5633e-01 (5.2214e-01)	Acc@1  78.91 ( 84.20)	Acc@5  97.66 ( 97.38)
Epoch: [42][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7076e-01 (5.2114e-01)	Acc@1  84.38 ( 84.19)	Acc@5  99.22 ( 97.42)
Epoch: [42][220/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.4744e-01 (5.2053e-01)	Acc@1  87.50 ( 84.20)	Acc@5  97.66 ( 97.45)
Epoch: [42][230/391]	Time  0.034 ( 0.033)	Data  0.002 ( 0.003)	Loss 5.1016e-01 (5.2023e-01)	Acc@1  87.50 ( 84.22)	Acc@5  96.88 ( 97.43)
Epoch: [42][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.8397e-01 (5.2076e-01)	Acc@1  78.12 ( 84.18)	Acc@5  96.88 ( 97.41)
Epoch: [42][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4334e-01 (5.1848e-01)	Acc@1  89.06 ( 84.27)	Acc@5  96.88 ( 97.43)
Epoch: [42][260/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.9232e-01 (5.1941e-01)	Acc@1  82.81 ( 84.21)	Acc@5 100.00 ( 97.45)
Epoch: [42][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0667e-01 (5.2149e-01)	Acc@1  88.28 ( 84.15)	Acc@5  98.44 ( 97.43)
Epoch: [42][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4043e-01 (5.2205e-01)	Acc@1  89.06 ( 84.14)	Acc@5  96.88 ( 97.41)
Epoch: [42][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.7968e-01 (5.2334e-01)	Acc@1  80.47 ( 84.11)	Acc@5  96.88 ( 97.41)
Epoch: [42][300/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.6157e-01 (5.2573e-01)	Acc@1  78.12 ( 84.05)	Acc@5  94.53 ( 97.40)
Epoch: [42][310/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.8521e-01 (5.2615e-01)	Acc@1  84.38 ( 84.03)	Acc@5  97.66 ( 97.39)
Epoch: [42][320/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.002)	Loss 6.3671e-01 (5.2799e-01)	Acc@1  77.34 ( 83.93)	Acc@5  96.88 ( 97.37)
Epoch: [42][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.3467e-01 (5.2710e-01)	Acc@1  84.38 ( 83.96)	Acc@5  98.44 ( 97.38)
Epoch: [42][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.0019e-01 (5.2729e-01)	Acc@1  82.03 ( 83.93)	Acc@5  96.88 ( 97.39)
Epoch: [42][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.6009e-01 (5.2766e-01)	Acc@1  82.81 ( 83.93)	Acc@5  97.66 ( 97.38)
Epoch: [42][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.2729e-01 (5.2807e-01)	Acc@1  85.94 ( 83.90)	Acc@5  96.09 ( 97.39)
Epoch: [42][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.0402e-01 (5.2756e-01)	Acc@1  79.69 ( 83.92)	Acc@5  98.44 ( 97.41)
Epoch: [42][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.8920e-01 (5.2845e-01)	Acc@1  82.81 ( 83.91)	Acc@5  96.09 ( 97.40)
Epoch: [42][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.7523e-01 (5.2835e-01)	Acc@1  81.25 ( 83.93)	Acc@5  96.25 ( 97.40)
## e[42] optimizer.zero_grad (sum) time: 0.11580634117126465
## e[42]       loss.backward (sum) time: 2.092888355255127
## e[42]      optimizer.step (sum) time: 0.7375109195709229
## epoch[42] training(only) time: 12.97430682182312
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.2786e+00 (1.2786e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 1.7134e+00 (1.4146e+00)	Acc@1  65.00 ( 66.36)	Acc@5  89.00 ( 87.82)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.4753e+00 (1.4022e+00)	Acc@1  66.00 ( 66.05)	Acc@5  90.00 ( 88.14)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.6777e+00 (1.4151e+00)	Acc@1  58.00 ( 65.48)	Acc@5  86.00 ( 88.03)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.1694e+00 (1.3918e+00)	Acc@1  65.00 ( 65.71)	Acc@5  91.00 ( 88.56)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.2689e+00 (1.3987e+00)	Acc@1  67.00 ( 65.76)	Acc@5  90.00 ( 88.63)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.7059e+00 (1.3846e+00)	Acc@1  66.00 ( 66.30)	Acc@5  83.00 ( 88.72)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3818e+00 (1.3837e+00)	Acc@1  67.00 ( 66.24)	Acc@5  88.00 ( 88.89)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4346e+00 (1.3836e+00)	Acc@1  68.00 ( 66.30)	Acc@5  85.00 ( 88.78)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7595e+00 (1.3710e+00)	Acc@1  56.00 ( 66.41)	Acc@5  88.00 ( 88.84)
 * Acc@1 66.320 Acc@5 88.910
### epoch[42] execution time: 14.901667833328247
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.178 ( 0.178)	Data  0.150 ( 0.150)	Loss 5.1805e-01 (5.1805e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [43][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 3.4973e-01 (4.7957e-01)	Acc@1  87.50 ( 85.94)	Acc@5  99.22 ( 97.94)
Epoch: [43][ 20/391]	Time  0.032 ( 0.039)	Data  0.002 ( 0.009)	Loss 5.4441e-01 (4.9266e-01)	Acc@1  82.81 ( 85.27)	Acc@5  99.22 ( 97.81)
Epoch: [43][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 6.0775e-01 (4.9535e-01)	Acc@1  82.81 ( 85.13)	Acc@5  97.66 ( 97.83)
Epoch: [43][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 3.4617e-01 (5.0360e-01)	Acc@1  89.84 ( 84.93)	Acc@5  98.44 ( 97.66)
Epoch: [43][ 50/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.6993e-01 (5.0437e-01)	Acc@1  89.84 ( 84.99)	Acc@5  97.66 ( 97.72)
Epoch: [43][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.1326e-01 (4.9898e-01)	Acc@1  84.38 ( 85.11)	Acc@5  98.44 ( 97.72)
Epoch: [43][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 6.1533e-01 (5.0487e-01)	Acc@1  80.47 ( 84.75)	Acc@5  94.53 ( 97.66)
Epoch: [43][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.5244e-01 (5.0717e-01)	Acc@1  84.38 ( 84.68)	Acc@5  95.31 ( 97.54)
Epoch: [43][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.2417e-01 (5.0103e-01)	Acc@1  85.16 ( 84.79)	Acc@5  95.31 ( 97.61)
Epoch: [43][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.8885e-01 (4.9990e-01)	Acc@1  83.59 ( 84.77)	Acc@5  97.66 ( 97.64)
Epoch: [43][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.1091e-01 (4.9725e-01)	Acc@1  87.50 ( 84.83)	Acc@5 100.00 ( 97.67)
Epoch: [43][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6935e-01 (4.9961e-01)	Acc@1  85.94 ( 84.79)	Acc@5  98.44 ( 97.62)
Epoch: [43][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.5483e-01 (4.9832e-01)	Acc@1  83.59 ( 84.83)	Acc@5  96.88 ( 97.62)
Epoch: [43][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.1306e-01 (5.0178e-01)	Acc@1  85.94 ( 84.70)	Acc@5  98.44 ( 97.62)
Epoch: [43][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6285e-01 (4.9923e-01)	Acc@1  83.59 ( 84.66)	Acc@5  99.22 ( 97.66)
Epoch: [43][160/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.0412e-01 (5.0393e-01)	Acc@1  81.25 ( 84.52)	Acc@5  98.44 ( 97.64)
Epoch: [43][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0116e-01 (5.0053e-01)	Acc@1  89.06 ( 84.59)	Acc@5 100.00 ( 97.67)
Epoch: [43][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6524e-01 (5.0261e-01)	Acc@1  86.72 ( 84.56)	Acc@5  96.88 ( 97.61)
Epoch: [43][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.9649e-01 (5.0129e-01)	Acc@1  85.94 ( 84.62)	Acc@5  98.44 ( 97.61)
Epoch: [43][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.9453e-01 (5.0346e-01)	Acc@1  85.94 ( 84.55)	Acc@5  97.66 ( 97.62)
Epoch: [43][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6224e-01 (5.0280e-01)	Acc@1  85.94 ( 84.57)	Acc@5  96.88 ( 97.64)
Epoch: [43][220/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 3.7299e-01 (5.0298e-01)	Acc@1  87.50 ( 84.55)	Acc@5  99.22 ( 97.65)
Epoch: [43][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.0949e-01 (5.0164e-01)	Acc@1  82.03 ( 84.60)	Acc@5  96.09 ( 97.67)
Epoch: [43][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.2187e-01 (5.0365e-01)	Acc@1  89.06 ( 84.55)	Acc@5  98.44 ( 97.65)
Epoch: [43][250/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.1114e-01 (5.0267e-01)	Acc@1  86.72 ( 84.59)	Acc@5  96.88 ( 97.65)
Epoch: [43][260/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.9310e-01 (5.0366e-01)	Acc@1  83.59 ( 84.58)	Acc@5  96.09 ( 97.61)
Epoch: [43][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4557e-01 (5.0510e-01)	Acc@1  83.59 ( 84.54)	Acc@5  99.22 ( 97.60)
Epoch: [43][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.3774e-01 (5.0852e-01)	Acc@1  77.34 ( 84.43)	Acc@5  98.44 ( 97.58)
Epoch: [43][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.9435e-01 (5.0993e-01)	Acc@1  83.59 ( 84.39)	Acc@5  93.75 ( 97.57)
Epoch: [43][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.6260e-01 (5.1070e-01)	Acc@1  78.91 ( 84.33)	Acc@5  96.09 ( 97.58)
Epoch: [43][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.5504e-01 (5.1060e-01)	Acc@1  82.81 ( 84.33)	Acc@5  97.66 ( 97.58)
Epoch: [43][320/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4360e-01 (5.1034e-01)	Acc@1  84.38 ( 84.33)	Acc@5  98.44 ( 97.57)
Epoch: [43][330/391]	Time  0.032 ( 0.033)	Data  0.003 ( 0.003)	Loss 4.6055e-01 (5.1007e-01)	Acc@1  84.38 ( 84.34)	Acc@5  99.22 ( 97.57)
Epoch: [43][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.2226e-01 (5.1147e-01)	Acc@1  75.78 ( 84.31)	Acc@5  94.53 ( 97.55)
Epoch: [43][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.6953e-01 (5.1269e-01)	Acc@1  80.47 ( 84.26)	Acc@5  96.88 ( 97.54)
Epoch: [43][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.1678e-01 (5.1321e-01)	Acc@1  81.25 ( 84.25)	Acc@5  99.22 ( 97.54)
Epoch: [43][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.1455e-01 (5.1330e-01)	Acc@1  86.72 ( 84.23)	Acc@5  99.22 ( 97.54)
Epoch: [43][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.6209e-01 (5.1318e-01)	Acc@1  80.47 ( 84.25)	Acc@5  98.44 ( 97.53)
Epoch: [43][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.3046e-01 (5.1445e-01)	Acc@1  82.50 ( 84.20)	Acc@5 100.00 ( 97.54)
## e[43] optimizer.zero_grad (sum) time: 0.11529922485351562
## e[43]       loss.backward (sum) time: 2.108764171600342
## e[43]      optimizer.step (sum) time: 0.7333300113677979
## epoch[43] training(only) time: 12.988075494766235
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.3216e+00 (1.3216e+00)	Acc@1  70.00 ( 70.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.7106e+00 (1.4803e+00)	Acc@1  66.00 ( 65.82)	Acc@5  87.00 ( 87.91)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.5138e+00 (1.4256e+00)	Acc@1  67.00 ( 66.24)	Acc@5  90.00 ( 88.52)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.6581e+00 (1.4317e+00)	Acc@1  59.00 ( 65.65)	Acc@5  85.00 ( 88.13)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.2067e+00 (1.4137e+00)	Acc@1  67.00 ( 65.80)	Acc@5  92.00 ( 88.68)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3821e+00 (1.4247e+00)	Acc@1  63.00 ( 65.71)	Acc@5  87.00 ( 88.47)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.8120e+00 (1.4178e+00)	Acc@1  64.00 ( 66.08)	Acc@5  86.00 ( 88.48)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3622e+00 (1.4124e+00)	Acc@1  69.00 ( 66.04)	Acc@5  89.00 ( 88.58)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3306e+00 (1.4116e+00)	Acc@1  72.00 ( 66.00)	Acc@5  89.00 ( 88.58)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.7217e+00 (1.3978e+00)	Acc@1  60.00 ( 66.20)	Acc@5  84.00 ( 88.63)
 * Acc@1 66.280 Acc@5 88.780
### epoch[43] execution time: 14.930140018463135
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.172 ( 0.172)	Data  0.146 ( 0.146)	Loss 5.0826e-01 (5.0826e-01)	Acc@1  88.28 ( 88.28)	Acc@5  96.88 ( 96.88)
Epoch: [44][ 10/391]	Time  0.033 ( 0.045)	Data  0.001 ( 0.015)	Loss 4.1550e-01 (5.4088e-01)	Acc@1  87.50 ( 82.10)	Acc@5  98.44 ( 97.59)
Epoch: [44][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 4.3610e-01 (5.2623e-01)	Acc@1  84.38 ( 82.81)	Acc@5  98.44 ( 97.77)
Epoch: [44][ 30/391]	Time  0.033 ( 0.037)	Data  0.002 ( 0.007)	Loss 5.3550e-01 (5.1306e-01)	Acc@1  83.59 ( 83.62)	Acc@5  97.66 ( 97.86)
Epoch: [44][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 5.3083e-01 (5.0062e-01)	Acc@1  84.38 ( 84.18)	Acc@5  99.22 ( 98.06)
Epoch: [44][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 5.0892e-01 (4.9848e-01)	Acc@1  85.94 ( 84.42)	Acc@5  98.44 ( 97.93)
Epoch: [44][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.5900e-01 (4.9208e-01)	Acc@1  85.94 ( 84.64)	Acc@5  97.66 ( 97.89)
Epoch: [44][ 70/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.9424e-01 (4.8584e-01)	Acc@1  89.06 ( 84.85)	Acc@5  98.44 ( 97.92)
Epoch: [44][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.3912e-01 (4.8587e-01)	Acc@1  84.38 ( 84.84)	Acc@5  95.31 ( 97.96)
Epoch: [44][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.2913e-01 (4.8832e-01)	Acc@1  89.06 ( 84.86)	Acc@5  99.22 ( 97.93)
Epoch: [44][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.3630e-01 (4.8777e-01)	Acc@1  85.94 ( 84.99)	Acc@5  92.97 ( 97.87)
Epoch: [44][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5092e-01 (4.8587e-01)	Acc@1  90.62 ( 85.09)	Acc@5  99.22 ( 97.86)
Epoch: [44][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.3992e-01 (4.8556e-01)	Acc@1  85.16 ( 85.11)	Acc@5  99.22 ( 97.86)
Epoch: [44][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.8124e-01 (4.8346e-01)	Acc@1  88.28 ( 85.20)	Acc@5  98.44 ( 97.84)
Epoch: [44][140/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6254e-01 (4.8411e-01)	Acc@1  81.25 ( 85.19)	Acc@5  98.44 ( 97.82)
Epoch: [44][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.5633e-01 (4.8348e-01)	Acc@1  78.91 ( 85.18)	Acc@5  96.88 ( 97.81)
Epoch: [44][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1190e-01 (4.8429e-01)	Acc@1  89.06 ( 85.18)	Acc@5  98.44 ( 97.80)
Epoch: [44][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8991e-01 (4.8389e-01)	Acc@1  87.50 ( 85.17)	Acc@5  97.66 ( 97.81)
Epoch: [44][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.3553e-01 (4.8162e-01)	Acc@1  84.38 ( 85.26)	Acc@5  96.88 ( 97.82)
Epoch: [44][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.5313e-01 (4.8340e-01)	Acc@1  85.16 ( 85.25)	Acc@5  97.66 ( 97.77)
Epoch: [44][200/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.9472e-01 (4.8455e-01)	Acc@1  85.94 ( 85.24)	Acc@5  97.66 ( 97.75)
Epoch: [44][210/391]	Time  0.036 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.0421e-01 (4.8270e-01)	Acc@1  88.28 ( 85.30)	Acc@5  96.09 ( 97.77)
Epoch: [44][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4544e-01 (4.8068e-01)	Acc@1  90.62 ( 85.34)	Acc@5 100.00 ( 97.79)
Epoch: [44][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1576e-01 (4.8102e-01)	Acc@1  88.28 ( 85.31)	Acc@5 100.00 ( 97.83)
Epoch: [44][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0637e-01 (4.8050e-01)	Acc@1  85.16 ( 85.30)	Acc@5  98.44 ( 97.84)
Epoch: [44][250/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7968e-01 (4.8047e-01)	Acc@1  89.06 ( 85.32)	Acc@5  99.22 ( 97.84)
Epoch: [44][260/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 5.7938e-01 (4.8193e-01)	Acc@1  84.38 ( 85.27)	Acc@5  94.53 ( 97.83)
Epoch: [44][270/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.5884e-01 (4.8369e-01)	Acc@1  88.28 ( 85.19)	Acc@5  96.09 ( 97.81)
Epoch: [44][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.9290e-01 (4.8438e-01)	Acc@1  83.59 ( 85.16)	Acc@5  96.09 ( 97.81)
Epoch: [44][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1781e-01 (4.8479e-01)	Acc@1  85.94 ( 85.14)	Acc@5  97.66 ( 97.81)
Epoch: [44][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6609e-01 (4.8453e-01)	Acc@1  82.81 ( 85.15)	Acc@5  95.31 ( 97.81)
Epoch: [44][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4706e-01 (4.8519e-01)	Acc@1  78.12 ( 85.10)	Acc@5  98.44 ( 97.82)
Epoch: [44][320/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 6.1469e-01 (4.8599e-01)	Acc@1  82.81 ( 85.08)	Acc@5  96.09 ( 97.81)
Epoch: [44][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.6875e-01 (4.8589e-01)	Acc@1  87.50 ( 85.10)	Acc@5  98.44 ( 97.81)
Epoch: [44][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.2666e-01 (4.8735e-01)	Acc@1  84.38 ( 85.04)	Acc@5  96.09 ( 97.80)
Epoch: [44][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.6146e-01 (4.8699e-01)	Acc@1  86.72 ( 85.04)	Acc@5  99.22 ( 97.80)
Epoch: [44][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.0088e-01 (4.8849e-01)	Acc@1  83.59 ( 84.98)	Acc@5  98.44 ( 97.78)
Epoch: [44][370/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.8075e-01 (4.8982e-01)	Acc@1  86.72 ( 84.99)	Acc@5  96.09 ( 97.75)
Epoch: [44][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.1767e-01 (4.9121e-01)	Acc@1  81.25 ( 84.96)	Acc@5  97.66 ( 97.74)
Epoch: [44][390/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.6993e-01 (4.9210e-01)	Acc@1  83.75 ( 84.93)	Acc@5  96.25 ( 97.72)
## e[44] optimizer.zero_grad (sum) time: 0.11629748344421387
## e[44]       loss.backward (sum) time: 2.082439422607422
## e[44]      optimizer.step (sum) time: 0.7284839153289795
## epoch[44] training(only) time: 12.993903636932373
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.3150e+00 (1.3150e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.6565e+00 (1.4830e+00)	Acc@1  66.00 ( 65.73)	Acc@5  88.00 ( 88.09)
Test: [ 20/100]	Time  0.018 ( 0.023)	Loss 1.4271e+00 (1.4481e+00)	Acc@1  69.00 ( 66.43)	Acc@5  92.00 ( 88.67)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7688e+00 (1.4513e+00)	Acc@1  63.00 ( 66.03)	Acc@5  85.00 ( 88.26)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.1982e+00 (1.4316e+00)	Acc@1  69.00 ( 66.17)	Acc@5  91.00 ( 88.54)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3500e+00 (1.4452e+00)	Acc@1  68.00 ( 66.08)	Acc@5  88.00 ( 88.39)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.8348e+00 (1.4324e+00)	Acc@1  60.00 ( 66.33)	Acc@5  84.00 ( 88.51)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.4386e+00 (1.4365e+00)	Acc@1  66.00 ( 66.04)	Acc@5  89.00 ( 88.48)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3589e+00 (1.4325e+00)	Acc@1  69.00 ( 66.12)	Acc@5  87.00 ( 88.48)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.8667e+00 (1.4245e+00)	Acc@1  57.00 ( 66.11)	Acc@5  84.00 ( 88.49)
 * Acc@1 65.900 Acc@5 88.600
### epoch[44] execution time: 14.930055856704712
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.179 ( 0.179)	Data  0.151 ( 0.151)	Loss 4.0091e-01 (4.0091e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [45][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 4.0585e-01 (4.5523e-01)	Acc@1  85.94 ( 85.58)	Acc@5 100.00 ( 98.08)
Epoch: [45][ 20/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.009)	Loss 3.9510e-01 (4.4664e-01)	Acc@1  89.06 ( 86.09)	Acc@5  98.44 ( 98.25)
Epoch: [45][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 4.9376e-01 (4.6025e-01)	Acc@1  82.81 ( 85.89)	Acc@5  98.44 ( 97.91)
Epoch: [45][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 4.7593e-01 (4.6519e-01)	Acc@1  84.38 ( 85.73)	Acc@5  96.88 ( 97.73)
Epoch: [45][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 4.5226e-01 (4.6854e-01)	Acc@1  86.72 ( 85.46)	Acc@5  99.22 ( 97.78)
Epoch: [45][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.0923e-01 (4.7075e-01)	Acc@1  82.81 ( 85.28)	Acc@5  96.88 ( 97.80)
Epoch: [45][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.3558e-01 (4.7041e-01)	Acc@1  85.94 ( 85.30)	Acc@5  97.66 ( 97.79)
Epoch: [45][ 80/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.8711e-01 (4.6913e-01)	Acc@1  84.38 ( 85.31)	Acc@5  99.22 ( 97.87)
Epoch: [45][ 90/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.4284e-01 (4.6972e-01)	Acc@1  89.84 ( 85.35)	Acc@5  96.88 ( 97.81)
Epoch: [45][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1049e-01 (4.6664e-01)	Acc@1  90.62 ( 85.37)	Acc@5  99.22 ( 97.86)
Epoch: [45][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.1794e-01 (4.6956e-01)	Acc@1  82.03 ( 85.35)	Acc@5  96.09 ( 97.80)
Epoch: [45][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6549e-01 (4.6792e-01)	Acc@1  86.72 ( 85.40)	Acc@5  98.44 ( 97.82)
Epoch: [45][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4242e-01 (4.6883e-01)	Acc@1  83.59 ( 85.41)	Acc@5  96.88 ( 97.81)
Epoch: [45][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3251e-01 (4.6857e-01)	Acc@1  89.06 ( 85.42)	Acc@5  99.22 ( 97.85)
Epoch: [45][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4353e-01 (4.6892e-01)	Acc@1  86.72 ( 85.44)	Acc@5  96.88 ( 97.84)
Epoch: [45][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.3779e-01 (4.6725e-01)	Acc@1  89.06 ( 85.49)	Acc@5  96.88 ( 97.86)
Epoch: [45][170/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8061e-01 (4.6838e-01)	Acc@1  86.72 ( 85.46)	Acc@5  96.88 ( 97.83)
Epoch: [45][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8193e-01 (4.6886e-01)	Acc@1  86.72 ( 85.44)	Acc@5  98.44 ( 97.83)
Epoch: [45][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.5780e-01 (4.7025e-01)	Acc@1  81.25 ( 85.40)	Acc@5  97.66 ( 97.82)
Epoch: [45][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4761e-01 (4.7200e-01)	Acc@1  85.16 ( 85.37)	Acc@5  98.44 ( 97.81)
Epoch: [45][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.5861e-01 (4.7087e-01)	Acc@1  84.38 ( 85.41)	Acc@5  97.66 ( 97.83)
Epoch: [45][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.5244e-01 (4.7016e-01)	Acc@1  82.81 ( 85.42)	Acc@5  96.09 ( 97.85)
Epoch: [45][230/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4897e-01 (4.6820e-01)	Acc@1  92.19 ( 85.45)	Acc@5  98.44 ( 97.85)
Epoch: [45][240/391]	Time  0.036 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7304e-01 (4.6962e-01)	Acc@1  85.94 ( 85.40)	Acc@5  97.66 ( 97.83)
Epoch: [45][250/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.4644e-01 (4.6836e-01)	Acc@1  87.50 ( 85.43)	Acc@5  97.66 ( 97.83)
Epoch: [45][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1780e-01 (4.6936e-01)	Acc@1  90.62 ( 85.38)	Acc@5  99.22 ( 97.85)
Epoch: [45][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.5192e-01 (4.7073e-01)	Acc@1  85.16 ( 85.36)	Acc@5  97.66 ( 97.84)
Epoch: [45][280/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6091e-01 (4.7228e-01)	Acc@1  83.59 ( 85.35)	Acc@5  96.09 ( 97.82)
Epoch: [45][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4914e-01 (4.7200e-01)	Acc@1  85.94 ( 85.37)	Acc@5  96.09 ( 97.82)
Epoch: [45][300/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.0748e-01 (4.7353e-01)	Acc@1  82.03 ( 85.34)	Acc@5  98.44 ( 97.81)
Epoch: [45][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.9447e-01 (4.7421e-01)	Acc@1  83.59 ( 85.33)	Acc@5  97.66 ( 97.82)
Epoch: [45][320/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.0943e-01 (4.7411e-01)	Acc@1  84.38 ( 85.34)	Acc@5  98.44 ( 97.82)
Epoch: [45][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.1813e-01 (4.7447e-01)	Acc@1  80.47 ( 85.31)	Acc@5  96.88 ( 97.83)
Epoch: [45][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.5020e-01 (4.7629e-01)	Acc@1  88.28 ( 85.29)	Acc@5 100.00 ( 97.81)
Epoch: [45][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.5110e-01 (4.7771e-01)	Acc@1  83.59 ( 85.26)	Acc@5  98.44 ( 97.80)
Epoch: [45][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.9478e-01 (4.7952e-01)	Acc@1  84.38 ( 85.21)	Acc@5  96.09 ( 97.79)
Epoch: [45][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.4223e-01 (4.7951e-01)	Acc@1  85.94 ( 85.24)	Acc@5  99.22 ( 97.79)
Epoch: [45][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.8931e-01 (4.7888e-01)	Acc@1  83.59 ( 85.25)	Acc@5  96.88 ( 97.80)
Epoch: [45][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3188e-01 (4.7949e-01)	Acc@1  82.50 ( 85.23)	Acc@5  97.50 ( 97.81)
## e[45] optimizer.zero_grad (sum) time: 0.11578130722045898
## e[45]       loss.backward (sum) time: 2.0861947536468506
## e[45]      optimizer.step (sum) time: 0.7250931262969971
## epoch[45] training(only) time: 13.008568525314331
# Switched to evaluate mode...
Test: [  0/100]	Time  0.175 ( 0.175)	Loss 1.3835e+00 (1.3835e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.017 ( 0.033)	Loss 1.6307e+00 (1.4746e+00)	Acc@1  62.00 ( 66.00)	Acc@5  88.00 ( 88.00)
Test: [ 20/100]	Time  0.017 ( 0.025)	Loss 1.5168e+00 (1.4406e+00)	Acc@1  68.00 ( 66.62)	Acc@5  91.00 ( 88.81)
Test: [ 30/100]	Time  0.017 ( 0.023)	Loss 1.6206e+00 (1.4447e+00)	Acc@1  62.00 ( 66.06)	Acc@5  83.00 ( 88.32)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.2130e+00 (1.4201e+00)	Acc@1  70.00 ( 66.44)	Acc@5  90.00 ( 88.63)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4036e+00 (1.4400e+00)	Acc@1  67.00 ( 66.20)	Acc@5  88.00 ( 88.35)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.8373e+00 (1.4283e+00)	Acc@1  62.00 ( 66.52)	Acc@5  85.00 ( 88.44)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.4099e+00 (1.4276e+00)	Acc@1  67.00 ( 66.34)	Acc@5  88.00 ( 88.55)
Test: [ 80/100]	Time  0.023 ( 0.019)	Loss 1.4236e+00 (1.4258e+00)	Acc@1  68.00 ( 66.46)	Acc@5  85.00 ( 88.49)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.9742e+00 (1.4154e+00)	Acc@1  55.00 ( 66.57)	Acc@5  86.00 ( 88.64)
 * Acc@1 66.490 Acc@5 88.810
### epoch[45] execution time: 14.981737613677979
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.183 ( 0.183)	Data  0.156 ( 0.156)	Loss 5.1562e-01 (5.1562e-01)	Acc@1  85.16 ( 85.16)	Acc@5  96.09 ( 96.09)
Epoch: [46][ 10/391]	Time  0.031 ( 0.046)	Data  0.001 ( 0.016)	Loss 4.9270e-01 (5.1879e-01)	Acc@1  84.38 ( 83.88)	Acc@5  98.44 ( 97.09)
Epoch: [46][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 5.8010e-01 (4.9841e-01)	Acc@1  84.38 ( 84.52)	Acc@5  98.44 ( 97.58)
Epoch: [46][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 3.6029e-01 (4.8050e-01)	Acc@1  90.62 ( 85.16)	Acc@5  97.66 ( 97.68)
Epoch: [46][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 3.9496e-01 (4.6791e-01)	Acc@1  85.94 ( 85.50)	Acc@5 100.00 ( 97.83)
Epoch: [46][ 50/391]	Time  0.033 ( 0.035)	Data  0.002 ( 0.005)	Loss 3.6158e-01 (4.5880e-01)	Acc@1  89.84 ( 85.94)	Acc@5  98.44 ( 97.87)
Epoch: [46][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 5.3948e-01 (4.5656e-01)	Acc@1  85.94 ( 86.00)	Acc@5  96.88 ( 97.94)
Epoch: [46][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.4674e-01 (4.5220e-01)	Acc@1  90.62 ( 86.07)	Acc@5  99.22 ( 97.96)
Epoch: [46][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.6952e-01 (4.5264e-01)	Acc@1  89.84 ( 86.02)	Acc@5  99.22 ( 98.00)
Epoch: [46][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.8920e-01 (4.5207e-01)	Acc@1  85.16 ( 86.15)	Acc@5  97.66 ( 98.03)
Epoch: [46][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.2140e-01 (4.5590e-01)	Acc@1  86.72 ( 86.08)	Acc@5  97.66 ( 97.99)
Epoch: [46][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.7823e-01 (4.5464e-01)	Acc@1  84.38 ( 86.03)	Acc@5  97.66 ( 97.97)
Epoch: [46][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.0532e-01 (4.5551e-01)	Acc@1  84.38 ( 86.02)	Acc@5  98.44 ( 97.99)
Epoch: [46][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.3604e-01 (4.5252e-01)	Acc@1  86.72 ( 86.11)	Acc@5  99.22 ( 98.01)
Epoch: [46][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.8796e-01 (4.5217e-01)	Acc@1  86.72 ( 86.14)	Acc@5  96.88 ( 97.99)
Epoch: [46][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5658e-01 (4.5292e-01)	Acc@1  85.94 ( 86.15)	Acc@5  98.44 ( 97.99)
Epoch: [46][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.1780e-01 (4.5505e-01)	Acc@1  85.16 ( 86.11)	Acc@5  97.66 ( 97.93)
Epoch: [46][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.3496e-01 (4.5258e-01)	Acc@1  78.91 ( 86.12)	Acc@5  97.66 ( 97.97)
Epoch: [46][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7563e-01 (4.5290e-01)	Acc@1  83.59 ( 86.06)	Acc@5  97.66 ( 97.96)
Epoch: [46][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1799e-01 (4.5226e-01)	Acc@1  85.94 ( 86.08)	Acc@5  99.22 ( 97.98)
Epoch: [46][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.6177e-01 (4.5362e-01)	Acc@1  80.47 ( 86.04)	Acc@5  97.66 ( 97.98)
Epoch: [46][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0300e-01 (4.5143e-01)	Acc@1  88.28 ( 86.09)	Acc@5  96.88 ( 98.00)
Epoch: [46][220/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.5444e-01 (4.5089e-01)	Acc@1  85.16 ( 86.11)	Acc@5 100.00 ( 98.02)
Epoch: [46][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7567e-01 (4.5178e-01)	Acc@1  92.97 ( 86.10)	Acc@5  98.44 ( 98.00)
Epoch: [46][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0928e-01 (4.5443e-01)	Acc@1  89.84 ( 86.02)	Acc@5  96.09 ( 97.97)
Epoch: [46][250/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.3091e-01 (4.5283e-01)	Acc@1  86.72 ( 86.08)	Acc@5  98.44 ( 97.97)
Epoch: [46][260/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7802e-01 (4.5386e-01)	Acc@1  87.50 ( 86.07)	Acc@5  98.44 ( 97.95)
Epoch: [46][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7189e-01 (4.5613e-01)	Acc@1  85.94 ( 86.01)	Acc@5  96.88 ( 97.92)
Epoch: [46][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1674e-01 (4.5706e-01)	Acc@1  89.06 ( 85.92)	Acc@5  98.44 ( 97.94)
Epoch: [46][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.8525e-01 (4.5822e-01)	Acc@1  80.47 ( 85.89)	Acc@5  96.09 ( 97.92)
Epoch: [46][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.2580e-01 (4.5900e-01)	Acc@1  82.81 ( 85.88)	Acc@5  96.09 ( 97.92)
Epoch: [46][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6062e-01 (4.5981e-01)	Acc@1  85.16 ( 85.88)	Acc@5  97.66 ( 97.91)
Epoch: [46][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6803e-01 (4.5985e-01)	Acc@1  85.94 ( 85.85)	Acc@5  99.22 ( 97.92)
Epoch: [46][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1929e-01 (4.5932e-01)	Acc@1  89.84 ( 85.87)	Acc@5 100.00 ( 97.93)
Epoch: [46][340/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.1354e-01 (4.6051e-01)	Acc@1  81.25 ( 85.83)	Acc@5  95.31 ( 97.92)
Epoch: [46][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.7430e-01 (4.6004e-01)	Acc@1  78.91 ( 85.83)	Acc@5  99.22 ( 97.93)
Epoch: [46][360/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.9360e-01 (4.6232e-01)	Acc@1  85.16 ( 85.78)	Acc@5  97.66 ( 97.91)
Epoch: [46][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.4804e-01 (4.6427e-01)	Acc@1  85.16 ( 85.71)	Acc@5  96.09 ( 97.89)
Epoch: [46][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.7523e-01 (4.6417e-01)	Acc@1  85.94 ( 85.69)	Acc@5  98.44 ( 97.87)
Epoch: [46][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.7240e-01 (4.6487e-01)	Acc@1  81.25 ( 85.66)	Acc@5  96.25 ( 97.88)
## e[46] optimizer.zero_grad (sum) time: 0.11524724960327148
## e[46]       loss.backward (sum) time: 2.079869508743286
## e[46]      optimizer.step (sum) time: 0.739309549331665
## epoch[46] training(only) time: 13.01890230178833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.3330e+00 (1.3330e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.7839e+00 (1.4868e+00)	Acc@1  66.00 ( 65.64)	Acc@5  88.00 ( 87.64)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.4746e+00 (1.4447e+00)	Acc@1  65.00 ( 65.95)	Acc@5  88.00 ( 88.19)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.6798e+00 (1.4662e+00)	Acc@1  60.00 ( 65.45)	Acc@5  85.00 ( 87.74)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.3034e+00 (1.4468e+00)	Acc@1  71.00 ( 65.93)	Acc@5  91.00 ( 88.27)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3741e+00 (1.4586e+00)	Acc@1  68.00 ( 65.96)	Acc@5  87.00 ( 88.14)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.7864e+00 (1.4409e+00)	Acc@1  58.00 ( 66.31)	Acc@5  85.00 ( 88.34)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3882e+00 (1.4415e+00)	Acc@1  66.00 ( 66.20)	Acc@5  88.00 ( 88.44)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4889e+00 (1.4417e+00)	Acc@1  68.00 ( 66.17)	Acc@5  83.00 ( 88.32)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.9128e+00 (1.4299e+00)	Acc@1  56.00 ( 66.32)	Acc@5  85.00 ( 88.38)
 * Acc@1 66.380 Acc@5 88.570
### epoch[46] execution time: 14.95861291885376
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.193 ( 0.193)	Data  0.164 ( 0.164)	Loss 2.8532e-01 (2.8532e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [47][ 10/391]	Time  0.032 ( 0.047)	Data  0.001 ( 0.016)	Loss 3.2135e-01 (3.8769e-01)	Acc@1  90.62 ( 88.00)	Acc@5  98.44 ( 98.65)
Epoch: [47][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.010)	Loss 5.3414e-01 (4.2065e-01)	Acc@1  82.03 ( 86.90)	Acc@5  97.66 ( 98.25)
Epoch: [47][ 30/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.007)	Loss 5.0497e-01 (4.1508e-01)	Acc@1  85.16 ( 87.07)	Acc@5  96.88 ( 98.31)
Epoch: [47][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 3.4016e-01 (4.2282e-01)	Acc@1  89.06 ( 86.78)	Acc@5  98.44 ( 98.30)
Epoch: [47][ 50/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 5.3736e-01 (4.3031e-01)	Acc@1  82.03 ( 86.76)	Acc@5  95.31 ( 98.10)
Epoch: [47][ 60/391]	Time  0.029 ( 0.035)	Data  0.002 ( 0.005)	Loss 4.2200e-01 (4.3274e-01)	Acc@1  85.16 ( 86.68)	Acc@5  98.44 ( 98.05)
Epoch: [47][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.9255e-01 (4.3099e-01)	Acc@1  84.38 ( 86.75)	Acc@5  99.22 ( 98.09)
Epoch: [47][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.4441e-01 (4.2974e-01)	Acc@1  84.38 ( 86.81)	Acc@5  96.09 ( 98.06)
Epoch: [47][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.2498e-01 (4.2996e-01)	Acc@1  90.62 ( 86.86)	Acc@5  99.22 ( 98.08)
Epoch: [47][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.6973e-01 (4.3272e-01)	Acc@1  90.62 ( 86.76)	Acc@5 100.00 ( 98.08)
Epoch: [47][110/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.1765e-01 (4.3172e-01)	Acc@1  83.59 ( 86.68)	Acc@5  97.66 ( 98.10)
Epoch: [47][120/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.003)	Loss 4.5768e-01 (4.3262e-01)	Acc@1  83.59 ( 86.58)	Acc@5  97.66 ( 98.11)
Epoch: [47][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5701e-01 (4.3797e-01)	Acc@1  88.28 ( 86.39)	Acc@5 100.00 ( 98.12)
Epoch: [47][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6898e-01 (4.3759e-01)	Acc@1  89.06 ( 86.43)	Acc@5  98.44 ( 98.12)
Epoch: [47][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6014e-01 (4.4083e-01)	Acc@1  86.72 ( 86.41)	Acc@5  97.66 ( 98.10)
Epoch: [47][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.4216e-01 (4.4076e-01)	Acc@1  81.25 ( 86.40)	Acc@5  98.44 ( 98.09)
Epoch: [47][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5937e-01 (4.4318e-01)	Acc@1  88.28 ( 86.32)	Acc@5  99.22 ( 98.08)
Epoch: [47][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4686e-01 (4.4265e-01)	Acc@1  88.28 ( 86.29)	Acc@5  98.44 ( 98.09)
Epoch: [47][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8568e-01 (4.4018e-01)	Acc@1  89.84 ( 86.33)	Acc@5  98.44 ( 98.09)
Epoch: [47][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4724e-01 (4.3955e-01)	Acc@1  88.28 ( 86.31)	Acc@5  99.22 ( 98.10)
Epoch: [47][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.5795e-01 (4.4043e-01)	Acc@1  89.84 ( 86.35)	Acc@5  96.88 ( 98.08)
Epoch: [47][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4728e-01 (4.4073e-01)	Acc@1  86.72 ( 86.32)	Acc@5  98.44 ( 98.09)
Epoch: [47][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8697e-01 (4.4100e-01)	Acc@1  86.72 ( 86.35)	Acc@5  95.31 ( 98.06)
Epoch: [47][240/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8806e-01 (4.4116e-01)	Acc@1  86.72 ( 86.38)	Acc@5 100.00 ( 98.06)
Epoch: [47][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.1209e-01 (4.4198e-01)	Acc@1  83.59 ( 86.40)	Acc@5  96.09 ( 98.00)
Epoch: [47][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8901e-01 (4.4300e-01)	Acc@1  87.50 ( 86.36)	Acc@5  98.44 ( 98.00)
Epoch: [47][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.2793e-01 (4.4370e-01)	Acc@1  81.25 ( 86.34)	Acc@5  95.31 ( 98.00)
Epoch: [47][280/391]	Time  0.036 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.9794e-01 (4.4423e-01)	Acc@1  85.16 ( 86.32)	Acc@5  96.88 ( 97.99)
Epoch: [47][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5327e-01 (4.4567e-01)	Acc@1  92.19 ( 86.29)	Acc@5  97.66 ( 97.95)
Epoch: [47][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.3854e-01 (4.4459e-01)	Acc@1  86.72 ( 86.33)	Acc@5  96.09 ( 97.96)
Epoch: [47][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8541e-01 (4.4356e-01)	Acc@1  85.16 ( 86.36)	Acc@5  97.66 ( 97.97)
Epoch: [47][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7594e-01 (4.4315e-01)	Acc@1  86.72 ( 86.38)	Acc@5 100.00 ( 97.98)
Epoch: [47][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.7132e-01 (4.4407e-01)	Acc@1  81.25 ( 86.33)	Acc@5  98.44 ( 97.98)
Epoch: [47][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 6.2014e-01 (4.4547e-01)	Acc@1  82.81 ( 86.26)	Acc@5  96.88 ( 97.97)
Epoch: [47][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.1363e-01 (4.4590e-01)	Acc@1  82.81 ( 86.25)	Acc@5  99.22 ( 97.97)
Epoch: [47][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.9100e-01 (4.4480e-01)	Acc@1  83.59 ( 86.28)	Acc@5  99.22 ( 97.99)
Epoch: [47][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.7639e-01 (4.4429e-01)	Acc@1  89.06 ( 86.29)	Acc@5  97.66 ( 97.99)
Epoch: [47][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.1875e-01 (4.4408e-01)	Acc@1  89.84 ( 86.32)	Acc@5  98.44 ( 98.01)
Epoch: [47][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.6757e-01 (4.4440e-01)	Acc@1  82.50 ( 86.31)	Acc@5  96.25 ( 98.00)
## e[47] optimizer.zero_grad (sum) time: 0.11568188667297363
## e[47]       loss.backward (sum) time: 2.1043307781219482
## e[47]      optimizer.step (sum) time: 0.7328197956085205
## epoch[47] training(only) time: 12.994073152542114
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.4272e+00 (1.4272e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.8257e+00 (1.5219e+00)	Acc@1  61.00 ( 65.00)	Acc@5  89.00 ( 88.73)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.4024e+00 (1.4742e+00)	Acc@1  67.00 ( 65.81)	Acc@5  84.00 ( 88.62)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.5140e+00 (1.4836e+00)	Acc@1  60.00 ( 65.42)	Acc@5  87.00 ( 88.19)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.2998e+00 (1.4693e+00)	Acc@1  69.00 ( 65.73)	Acc@5  90.00 ( 88.44)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3380e+00 (1.4807e+00)	Acc@1  65.00 ( 65.61)	Acc@5  89.00 ( 88.29)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.8348e+00 (1.4650e+00)	Acc@1  63.00 ( 66.02)	Acc@5  83.00 ( 88.43)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.5770e+00 (1.4609e+00)	Acc@1  67.00 ( 65.94)	Acc@5  86.00 ( 88.48)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4532e+00 (1.4577e+00)	Acc@1  66.00 ( 65.99)	Acc@5  87.00 ( 88.40)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.8557e+00 (1.4473e+00)	Acc@1  55.00 ( 66.10)	Acc@5  84.00 ( 88.51)
 * Acc@1 66.050 Acc@5 88.510
### epoch[47] execution time: 14.940739631652832
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.176 ( 0.176)	Data  0.147 ( 0.147)	Loss 4.8811e-01 (4.8811e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 97.66)
Epoch: [48][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 2.9780e-01 (4.2720e-01)	Acc@1  90.62 ( 86.22)	Acc@5  99.22 ( 97.94)
Epoch: [48][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 4.9578e-01 (4.1495e-01)	Acc@1  83.59 ( 86.64)	Acc@5  98.44 ( 98.18)
Epoch: [48][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 3.9113e-01 (4.0480e-01)	Acc@1  86.72 ( 87.05)	Acc@5  99.22 ( 98.29)
Epoch: [48][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.005)	Loss 4.3753e-01 (4.1368e-01)	Acc@1  87.50 ( 86.91)	Acc@5  98.44 ( 98.19)
Epoch: [48][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.5592e-01 (4.1945e-01)	Acc@1  88.28 ( 86.70)	Acc@5  97.66 ( 98.16)
Epoch: [48][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.2140e-01 (4.1055e-01)	Acc@1  90.62 ( 87.10)	Acc@5  99.22 ( 98.23)
Epoch: [48][ 70/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.2087e-01 (4.1392e-01)	Acc@1  88.28 ( 86.96)	Acc@5  99.22 ( 98.20)
Epoch: [48][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.8057e-01 (4.1473e-01)	Acc@1  89.06 ( 86.93)	Acc@5  97.66 ( 98.23)
Epoch: [48][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.3375e-01 (4.2097e-01)	Acc@1  87.50 ( 86.67)	Acc@5  96.88 ( 98.14)
Epoch: [48][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5631e-01 (4.1636e-01)	Acc@1  85.94 ( 86.83)	Acc@5  97.66 ( 98.17)
Epoch: [48][110/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.5516e-01 (4.1876e-01)	Acc@1  86.72 ( 86.84)	Acc@5  96.09 ( 98.17)
Epoch: [48][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.0086e-01 (4.1865e-01)	Acc@1  85.94 ( 86.85)	Acc@5  98.44 ( 98.14)
Epoch: [48][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3327e-01 (4.1829e-01)	Acc@1  90.62 ( 86.90)	Acc@5  98.44 ( 98.15)
Epoch: [48][140/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9241e-01 (4.1910e-01)	Acc@1  87.50 ( 86.86)	Acc@5  99.22 ( 98.13)
Epoch: [48][150/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6015e-01 (4.1667e-01)	Acc@1  89.84 ( 86.92)	Acc@5  99.22 ( 98.16)
Epoch: [48][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6217e-01 (4.1833e-01)	Acc@1  87.50 ( 86.92)	Acc@5  96.88 ( 98.13)
Epoch: [48][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4689e-01 (4.1531e-01)	Acc@1  92.97 ( 87.06)	Acc@5 100.00 ( 98.15)
Epoch: [48][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.2547e-01 (4.1750e-01)	Acc@1  85.16 ( 87.02)	Acc@5  97.66 ( 98.16)
Epoch: [48][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5897e-01 (4.1890e-01)	Acc@1  92.19 ( 87.01)	Acc@5  98.44 ( 98.14)
Epoch: [48][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8525e-01 (4.1986e-01)	Acc@1  84.38 ( 86.98)	Acc@5  99.22 ( 98.13)
Epoch: [48][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6648e-01 (4.1955e-01)	Acc@1  87.50 ( 87.02)	Acc@5  98.44 ( 98.14)
Epoch: [48][220/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.2561e-01 (4.2010e-01)	Acc@1  88.28 ( 86.98)	Acc@5  98.44 ( 98.16)
Epoch: [48][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.7584e-01 (4.2265e-01)	Acc@1  83.59 ( 86.92)	Acc@5  96.88 ( 98.16)
Epoch: [48][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.5163e-01 (4.2557e-01)	Acc@1  81.25 ( 86.84)	Acc@5  97.66 ( 98.16)
Epoch: [48][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.0108e-01 (4.2503e-01)	Acc@1  84.38 ( 86.87)	Acc@5  97.66 ( 98.14)
Epoch: [48][260/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7451e-01 (4.2493e-01)	Acc@1  89.06 ( 86.84)	Acc@5 100.00 ( 98.15)
Epoch: [48][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7984e-01 (4.2557e-01)	Acc@1  88.28 ( 86.83)	Acc@5 100.00 ( 98.15)
Epoch: [48][280/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.1640e-01 (4.2747e-01)	Acc@1  82.81 ( 86.76)	Acc@5  98.44 ( 98.14)
Epoch: [48][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7369e-01 (4.2770e-01)	Acc@1  85.16 ( 86.74)	Acc@5  97.66 ( 98.15)
Epoch: [48][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.3974e-01 (4.2713e-01)	Acc@1  88.28 ( 86.77)	Acc@5  97.66 ( 98.15)
Epoch: [48][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8412e-01 (4.2742e-01)	Acc@1  88.28 ( 86.75)	Acc@5 100.00 ( 98.16)
Epoch: [48][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.0718e-01 (4.2750e-01)	Acc@1  82.03 ( 86.72)	Acc@5  97.66 ( 98.17)
Epoch: [48][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6061e-01 (4.2652e-01)	Acc@1  89.84 ( 86.72)	Acc@5  99.22 ( 98.19)
Epoch: [48][340/391]	Time  0.038 ( 0.033)	Data  0.013 ( 0.003)	Loss 4.9397e-01 (4.2780e-01)	Acc@1  85.16 ( 86.65)	Acc@5  96.88 ( 98.19)
Epoch: [48][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3573e-01 (4.2832e-01)	Acc@1  86.72 ( 86.63)	Acc@5  97.66 ( 98.19)
Epoch: [48][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.2300e-01 (4.2871e-01)	Acc@1  86.72 ( 86.63)	Acc@5  98.44 ( 98.19)
Epoch: [48][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.9868e-01 (4.2744e-01)	Acc@1  89.06 ( 86.66)	Acc@5  98.44 ( 98.20)
Epoch: [48][380/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 4.4302e-01 (4.2720e-01)	Acc@1  85.16 ( 86.68)	Acc@5  97.66 ( 98.21)
Epoch: [48][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3937e-01 (4.2803e-01)	Acc@1  81.25 ( 86.64)	Acc@5  96.25 ( 98.20)
## e[48] optimizer.zero_grad (sum) time: 0.1155388355255127
## e[48]       loss.backward (sum) time: 2.1325528621673584
## e[48]      optimizer.step (sum) time: 0.7221689224243164
## epoch[48] training(only) time: 12.985093355178833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.4396e+00 (1.4396e+00)	Acc@1  69.00 ( 69.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.6977e+00 (1.5233e+00)	Acc@1  62.00 ( 65.18)	Acc@5  88.00 ( 88.09)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.4092e+00 (1.4861e+00)	Acc@1  65.00 ( 65.71)	Acc@5  89.00 ( 88.29)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.5944e+00 (1.4770e+00)	Acc@1  59.00 ( 65.45)	Acc@5  86.00 ( 88.42)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.2565e+00 (1.4591e+00)	Acc@1  70.00 ( 65.68)	Acc@5  91.00 ( 88.88)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3474e+00 (1.4749e+00)	Acc@1  68.00 ( 65.73)	Acc@5  89.00 ( 88.75)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9514e+00 (1.4639e+00)	Acc@1  60.00 ( 66.13)	Acc@5  82.00 ( 88.79)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.4638e+00 (1.4685e+00)	Acc@1  70.00 ( 65.92)	Acc@5  87.00 ( 88.85)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4366e+00 (1.4721e+00)	Acc@1  73.00 ( 65.95)	Acc@5  85.00 ( 88.63)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.0271e+00 (1.4624e+00)	Acc@1  52.00 ( 66.01)	Acc@5  83.00 ( 88.67)
 * Acc@1 66.030 Acc@5 88.780
### epoch[48] execution time: 14.91879677772522
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.184 ( 0.184)	Data  0.156 ( 0.156)	Loss 5.3509e-01 (5.3509e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.88 ( 96.88)
Epoch: [49][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.016)	Loss 4.9942e-01 (4.3030e-01)	Acc@1  82.81 ( 86.58)	Acc@5 100.00 ( 98.30)
Epoch: [49][ 20/391]	Time  0.030 ( 0.040)	Data  0.001 ( 0.009)	Loss 4.2606e-01 (4.2232e-01)	Acc@1  87.50 ( 86.87)	Acc@5  98.44 ( 98.21)
Epoch: [49][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 4.2437e-01 (4.0425e-01)	Acc@1  88.28 ( 87.80)	Acc@5  96.88 ( 98.24)
Epoch: [49][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 4.8151e-01 (4.0505e-01)	Acc@1  85.16 ( 87.37)	Acc@5  98.44 ( 98.42)
Epoch: [49][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.6012e-01 (4.1349e-01)	Acc@1  89.84 ( 87.13)	Acc@5  98.44 ( 98.27)
Epoch: [49][ 60/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.6731e-01 (4.0949e-01)	Acc@1  88.28 ( 87.24)	Acc@5 100.00 ( 98.35)
Epoch: [49][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.9828e-01 (4.1235e-01)	Acc@1  90.62 ( 87.25)	Acc@5  99.22 ( 98.31)
Epoch: [49][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.0112e-01 (4.1007e-01)	Acc@1  89.84 ( 87.40)	Acc@5  97.66 ( 98.24)
Epoch: [49][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.5588e-01 (4.1379e-01)	Acc@1  82.03 ( 87.24)	Acc@5  99.22 ( 98.28)
Epoch: [49][100/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.4040e-01 (4.1833e-01)	Acc@1  86.72 ( 87.13)	Acc@5  98.44 ( 98.27)
Epoch: [49][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.7740e-01 (4.1594e-01)	Acc@1  87.50 ( 87.11)	Acc@5  96.09 ( 98.29)
Epoch: [49][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5927e-01 (4.1160e-01)	Acc@1  90.62 ( 87.26)	Acc@5 100.00 ( 98.36)
Epoch: [49][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.3467e-01 (4.1196e-01)	Acc@1  84.38 ( 87.29)	Acc@5  97.66 ( 98.36)
Epoch: [49][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.2640e-01 (4.1152e-01)	Acc@1  87.50 ( 87.29)	Acc@5  99.22 ( 98.36)
Epoch: [49][150/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.2728e-01 (4.0952e-01)	Acc@1  83.59 ( 87.37)	Acc@5  98.44 ( 98.37)
Epoch: [49][160/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.2443e-01 (4.0987e-01)	Acc@1  86.72 ( 87.34)	Acc@5  97.66 ( 98.35)
Epoch: [49][170/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1921e-01 (4.0851e-01)	Acc@1  85.94 ( 87.33)	Acc@5  98.44 ( 98.38)
Epoch: [49][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2897e-01 (4.0730e-01)	Acc@1  91.41 ( 87.36)	Acc@5  98.44 ( 98.36)
Epoch: [49][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6663e-01 (4.0795e-01)	Acc@1  83.59 ( 87.34)	Acc@5  97.66 ( 98.36)
Epoch: [49][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1049e-01 (4.0826e-01)	Acc@1  86.72 ( 87.28)	Acc@5  98.44 ( 98.33)
Epoch: [49][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7188e-01 (4.0797e-01)	Acc@1  89.06 ( 87.32)	Acc@5  98.44 ( 98.34)
Epoch: [49][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8791e-01 (4.0743e-01)	Acc@1  90.62 ( 87.32)	Acc@5 100.00 ( 98.34)
Epoch: [49][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9751e-01 (4.0869e-01)	Acc@1  88.28 ( 87.28)	Acc@5  98.44 ( 98.34)
Epoch: [49][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.3967e-01 (4.1001e-01)	Acc@1  86.72 ( 87.25)	Acc@5  99.22 ( 98.34)
Epoch: [49][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0012e-01 (4.1233e-01)	Acc@1  89.84 ( 87.18)	Acc@5  98.44 ( 98.31)
Epoch: [49][260/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 3.9315e-01 (4.1216e-01)	Acc@1  86.72 ( 87.16)	Acc@5  98.44 ( 98.31)
Epoch: [49][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.8735e-01 (4.1210e-01)	Acc@1  84.38 ( 87.18)	Acc@5  96.09 ( 98.30)
Epoch: [49][280/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8700e-01 (4.1401e-01)	Acc@1  85.16 ( 87.15)	Acc@5 100.00 ( 98.28)
Epoch: [49][290/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5799e-01 (4.1338e-01)	Acc@1  91.41 ( 87.15)	Acc@5  99.22 ( 98.28)
Epoch: [49][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4397e-01 (4.1298e-01)	Acc@1  92.19 ( 87.17)	Acc@5  99.22 ( 98.28)
Epoch: [49][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7144e-01 (4.1323e-01)	Acc@1  88.28 ( 87.21)	Acc@5  95.31 ( 98.26)
Epoch: [49][320/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.5712e-01 (4.1375e-01)	Acc@1  86.72 ( 87.16)	Acc@5  96.09 ( 98.26)
Epoch: [49][330/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5308e-01 (4.1422e-01)	Acc@1  89.06 ( 87.12)	Acc@5  97.66 ( 98.26)
Epoch: [49][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.3143e-01 (4.1333e-01)	Acc@1  86.72 ( 87.14)	Acc@5 100.00 ( 98.27)
Epoch: [49][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.7926e-01 (4.1449e-01)	Acc@1  85.94 ( 87.15)	Acc@5  98.44 ( 98.26)
Epoch: [49][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.6923e-01 (4.1496e-01)	Acc@1  85.16 ( 87.11)	Acc@5  99.22 ( 98.28)
Epoch: [49][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.8210e-01 (4.1498e-01)	Acc@1  87.50 ( 87.12)	Acc@5  98.44 ( 98.27)
Epoch: [49][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.1717e-01 (4.1632e-01)	Acc@1  86.72 ( 87.05)	Acc@5  98.44 ( 98.25)
Epoch: [49][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.6922e-01 (4.1723e-01)	Acc@1  81.25 ( 87.05)	Acc@5 100.00 ( 98.26)
## e[49] optimizer.zero_grad (sum) time: 0.11514997482299805
## e[49]       loss.backward (sum) time: 2.101310968399048
## e[49]      optimizer.step (sum) time: 0.734870195388794
## epoch[49] training(only) time: 12.949880123138428
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.4337e+00 (1.4337e+00)	Acc@1  65.00 ( 65.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.7401e+00 (1.5042e+00)	Acc@1  65.00 ( 65.91)	Acc@5  88.00 ( 87.91)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.5746e+00 (1.4689e+00)	Acc@1  68.00 ( 66.38)	Acc@5  87.00 ( 88.48)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.6389e+00 (1.4853e+00)	Acc@1  57.00 ( 65.65)	Acc@5  89.00 ( 88.35)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.2915e+00 (1.4692e+00)	Acc@1  68.00 ( 65.85)	Acc@5  92.00 ( 88.73)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4549e+00 (1.4824e+00)	Acc@1  65.00 ( 65.80)	Acc@5  88.00 ( 88.53)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9314e+00 (1.4718e+00)	Acc@1  62.00 ( 66.21)	Acc@5  82.00 ( 88.46)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.5071e+00 (1.4689e+00)	Acc@1  66.00 ( 66.10)	Acc@5  88.00 ( 88.55)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4721e+00 (1.4701e+00)	Acc@1  69.00 ( 66.22)	Acc@5  86.00 ( 88.44)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.8931e+00 (1.4608e+00)	Acc@1  54.00 ( 66.24)	Acc@5  84.00 ( 88.46)
 * Acc@1 66.310 Acc@5 88.610
### epoch[49] execution time: 14.89065146446228
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.178 ( 0.178)	Data  0.150 ( 0.150)	Loss 3.7057e-01 (3.7057e-01)	Acc@1  89.06 ( 89.06)	Acc@5  98.44 ( 98.44)
Epoch: [50][ 10/391]	Time  0.032 ( 0.046)	Data  0.002 ( 0.015)	Loss 3.8743e-01 (3.9826e-01)	Acc@1  88.28 ( 88.00)	Acc@5 100.00 ( 98.44)
Epoch: [50][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 3.8964e-01 (3.9592e-01)	Acc@1  87.50 ( 87.65)	Acc@5  99.22 ( 98.55)
Epoch: [50][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 4.1049e-01 (3.9302e-01)	Acc@1  86.72 ( 87.85)	Acc@5 100.00 ( 98.56)
Epoch: [50][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.2651e-01 (3.8236e-01)	Acc@1  92.19 ( 88.11)	Acc@5 100.00 ( 98.59)
Epoch: [50][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 5.1328e-01 (3.8992e-01)	Acc@1  88.28 ( 87.82)	Acc@5  96.09 ( 98.45)
Epoch: [50][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.8825e-01 (3.9658e-01)	Acc@1  86.72 ( 87.67)	Acc@5  96.09 ( 98.45)
Epoch: [50][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.3049e-01 (4.0020e-01)	Acc@1  85.16 ( 87.60)	Acc@5  96.88 ( 98.40)
Epoch: [50][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.8604e-01 (4.0004e-01)	Acc@1  87.50 ( 87.63)	Acc@5  98.44 ( 98.38)
Epoch: [50][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.5801e-01 (3.9473e-01)	Acc@1  86.72 ( 87.77)	Acc@5  98.44 ( 98.46)
Epoch: [50][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6863e-01 (3.9772e-01)	Acc@1  85.16 ( 87.65)	Acc@5  96.09 ( 98.41)
Epoch: [50][110/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6011e-01 (4.0345e-01)	Acc@1  88.28 ( 87.56)	Acc@5  99.22 ( 98.36)
Epoch: [50][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3223e-01 (4.0196e-01)	Acc@1  87.50 ( 87.60)	Acc@5 100.00 ( 98.38)
Epoch: [50][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.8638e-01 (4.0170e-01)	Acc@1  88.28 ( 87.66)	Acc@5 100.00 ( 98.39)
Epoch: [50][140/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 4.0131e-01 (4.0224e-01)	Acc@1  87.50 ( 87.59)	Acc@5  98.44 ( 98.38)
Epoch: [50][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4211e-01 (3.9859e-01)	Acc@1  92.19 ( 87.72)	Acc@5  97.66 ( 98.40)
Epoch: [50][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.0863e-01 (3.9926e-01)	Acc@1  78.91 ( 87.63)	Acc@5  97.66 ( 98.44)
Epoch: [50][170/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.3113e-01 (4.0155e-01)	Acc@1  83.59 ( 87.59)	Acc@5  98.44 ( 98.43)
Epoch: [50][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4423e-01 (4.0128e-01)	Acc@1  84.38 ( 87.63)	Acc@5  97.66 ( 98.40)
Epoch: [50][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7784e-01 (4.0200e-01)	Acc@1  90.62 ( 87.59)	Acc@5  99.22 ( 98.43)
Epoch: [50][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8920e-01 (4.0181e-01)	Acc@1  83.59 ( 87.61)	Acc@5  97.66 ( 98.41)
Epoch: [50][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7612e-01 (3.9813e-01)	Acc@1  91.41 ( 87.72)	Acc@5 100.00 ( 98.46)
Epoch: [50][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9964e-01 (3.9865e-01)	Acc@1  87.50 ( 87.68)	Acc@5  99.22 ( 98.47)
Epoch: [50][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7053e-01 (4.0033e-01)	Acc@1  89.84 ( 87.64)	Acc@5  97.66 ( 98.46)
Epoch: [50][240/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1750e-01 (4.0175e-01)	Acc@1  85.94 ( 87.57)	Acc@5  98.44 ( 98.45)
Epoch: [50][250/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8039e-01 (4.0277e-01)	Acc@1  89.06 ( 87.55)	Acc@5 100.00 ( 98.44)
Epoch: [50][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6686e-01 (4.0194e-01)	Acc@1  90.62 ( 87.57)	Acc@5  99.22 ( 98.46)
Epoch: [50][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1868e-01 (4.0222e-01)	Acc@1  82.81 ( 87.54)	Acc@5  98.44 ( 98.46)
Epoch: [50][280/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.7853e-01 (4.0245e-01)	Acc@1  89.84 ( 87.51)	Acc@5 100.00 ( 98.48)
Epoch: [50][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.5285e-01 (4.0318e-01)	Acc@1  83.59 ( 87.47)	Acc@5  98.44 ( 98.48)
Epoch: [50][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7027e-01 (4.0241e-01)	Acc@1  88.28 ( 87.47)	Acc@5  98.44 ( 98.49)
Epoch: [50][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5651e-01 (4.0167e-01)	Acc@1  89.84 ( 87.49)	Acc@5  99.22 ( 98.50)
Epoch: [50][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0019e-01 (4.0084e-01)	Acc@1  89.06 ( 87.51)	Acc@5 100.00 ( 98.52)
Epoch: [50][330/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.1662e-01 (4.0252e-01)	Acc@1  88.28 ( 87.48)	Acc@5  96.88 ( 98.50)
Epoch: [50][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.4013e-01 (4.0228e-01)	Acc@1  88.28 ( 87.49)	Acc@5  97.66 ( 98.49)
Epoch: [50][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.3232e-01 (4.0270e-01)	Acc@1  88.28 ( 87.47)	Acc@5  96.88 ( 98.47)
Epoch: [50][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.5166e-01 (4.0235e-01)	Acc@1  85.16 ( 87.45)	Acc@5  96.09 ( 98.48)
Epoch: [50][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.0624e-01 (4.0243e-01)	Acc@1  83.59 ( 87.45)	Acc@5  97.66 ( 98.48)
Epoch: [50][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.9808e-01 (4.0368e-01)	Acc@1  81.25 ( 87.41)	Acc@5  99.22 ( 98.47)
Epoch: [50][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.4037e-01 (4.0438e-01)	Acc@1  90.00 ( 87.38)	Acc@5 100.00 ( 98.47)
## e[50] optimizer.zero_grad (sum) time: 0.11528587341308594
## e[50]       loss.backward (sum) time: 2.073188304901123
## e[50]      optimizer.step (sum) time: 0.7300291061401367
## epoch[50] training(only) time: 13.033400535583496
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.4064e+00 (1.4064e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.7207e+00 (1.5101e+00)	Acc@1  60.00 ( 66.73)	Acc@5  92.00 ( 87.91)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.5912e+00 (1.5079e+00)	Acc@1  67.00 ( 66.38)	Acc@5  86.00 ( 87.95)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.6724e+00 (1.5262e+00)	Acc@1  61.00 ( 65.74)	Acc@5  88.00 ( 87.94)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3635e+00 (1.5062e+00)	Acc@1  69.00 ( 65.83)	Acc@5  92.00 ( 88.27)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4595e+00 (1.5223e+00)	Acc@1  68.00 ( 65.78)	Acc@5  86.00 ( 88.08)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9349e+00 (1.5040e+00)	Acc@1  62.00 ( 66.11)	Acc@5  85.00 ( 88.23)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.5293e+00 (1.5036e+00)	Acc@1  67.00 ( 65.99)	Acc@5  87.00 ( 88.32)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3251e+00 (1.4995e+00)	Acc@1  74.00 ( 66.11)	Acc@5  88.00 ( 88.32)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.0440e+00 (1.4891e+00)	Acc@1  54.00 ( 66.21)	Acc@5  83.00 ( 88.34)
 * Acc@1 66.140 Acc@5 88.470
### epoch[50] execution time: 14.97636365890503
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.158 ( 0.158)	Data  0.129 ( 0.129)	Loss 4.2802e-01 (4.2802e-01)	Acc@1  83.59 ( 83.59)	Acc@5  99.22 ( 99.22)
Epoch: [51][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 2.7687e-01 (3.6509e-01)	Acc@1  92.19 ( 88.64)	Acc@5  99.22 ( 98.58)
Epoch: [51][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 2.9993e-01 (3.7002e-01)	Acc@1  92.97 ( 88.47)	Acc@5  99.22 ( 98.62)
Epoch: [51][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 4.4550e-01 (3.6600e-01)	Acc@1  82.81 ( 88.36)	Acc@5  99.22 ( 98.77)
Epoch: [51][ 40/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.9154e-01 (3.7325e-01)	Acc@1  87.50 ( 88.22)	Acc@5  97.66 ( 98.67)
Epoch: [51][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 4.3086e-01 (3.7906e-01)	Acc@1  85.16 ( 88.11)	Acc@5  99.22 ( 98.64)
Epoch: [51][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.4035e-01 (3.7906e-01)	Acc@1  88.28 ( 88.20)	Acc@5  98.44 ( 98.60)
Epoch: [51][ 70/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.6815e-01 (3.8071e-01)	Acc@1  85.16 ( 88.09)	Acc@5  96.88 ( 98.58)
Epoch: [51][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.4024e-01 (3.7980e-01)	Acc@1  88.28 ( 88.06)	Acc@5  99.22 ( 98.62)
Epoch: [51][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.4442e-01 (3.8099e-01)	Acc@1  85.94 ( 88.10)	Acc@5  98.44 ( 98.59)
Epoch: [51][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 5.8581e-01 (3.8359e-01)	Acc@1  81.25 ( 88.06)	Acc@5  96.09 ( 98.55)
Epoch: [51][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9983e-01 (3.8027e-01)	Acc@1  89.84 ( 88.17)	Acc@5  99.22 ( 98.59)
Epoch: [51][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4425e-01 (3.7590e-01)	Acc@1  92.97 ( 88.24)	Acc@5  99.22 ( 98.62)
Epoch: [51][130/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4835e-01 (3.7816e-01)	Acc@1  87.50 ( 88.07)	Acc@5 100.00 ( 98.59)
Epoch: [51][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0068e-01 (3.7827e-01)	Acc@1  91.41 ( 88.11)	Acc@5 100.00 ( 98.59)
Epoch: [51][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6782e-01 (3.8011e-01)	Acc@1  88.28 ( 88.01)	Acc@5  98.44 ( 98.62)
Epoch: [51][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.1687e-01 (3.8028e-01)	Acc@1  82.81 ( 88.00)	Acc@5  99.22 ( 98.61)
Epoch: [51][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4558e-01 (3.8169e-01)	Acc@1  85.16 ( 87.94)	Acc@5  98.44 ( 98.58)
Epoch: [51][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9802e-01 (3.8211e-01)	Acc@1  88.28 ( 87.93)	Acc@5  96.88 ( 98.58)
Epoch: [51][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.3246e-01 (3.8388e-01)	Acc@1  86.72 ( 87.87)	Acc@5  96.09 ( 98.54)
Epoch: [51][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8266e-01 (3.8568e-01)	Acc@1  87.50 ( 87.83)	Acc@5  96.88 ( 98.50)
Epoch: [51][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8681e-01 (3.8458e-01)	Acc@1  90.62 ( 87.86)	Acc@5  99.22 ( 98.51)
Epoch: [51][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0171e-01 (3.8534e-01)	Acc@1  90.62 ( 87.86)	Acc@5  99.22 ( 98.49)
Epoch: [51][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1355e-01 (3.8563e-01)	Acc@1  86.72 ( 87.81)	Acc@5  98.44 ( 98.49)
Epoch: [51][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0242e-01 (3.8474e-01)	Acc@1  85.16 ( 87.81)	Acc@5  98.44 ( 98.51)
Epoch: [51][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1664e-01 (3.8584e-01)	Acc@1  83.59 ( 87.80)	Acc@5 100.00 ( 98.52)
Epoch: [51][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.5292e-01 (3.8691e-01)	Acc@1  85.94 ( 87.77)	Acc@5  97.66 ( 98.49)
Epoch: [51][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6927e-01 (3.8723e-01)	Acc@1  88.28 ( 87.77)	Acc@5  96.09 ( 98.48)
Epoch: [51][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.5959e-01 (3.8925e-01)	Acc@1  85.16 ( 87.72)	Acc@5  95.31 ( 98.45)
Epoch: [51][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4649e-01 (3.8892e-01)	Acc@1  85.16 ( 87.75)	Acc@5  97.66 ( 98.43)
Epoch: [51][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.1187e-01 (3.8838e-01)	Acc@1  84.38 ( 87.75)	Acc@5  95.31 ( 98.44)
Epoch: [51][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.7623e-01 (3.8773e-01)	Acc@1  90.62 ( 87.79)	Acc@5  97.66 ( 98.45)
Epoch: [51][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.1555e-01 (3.8768e-01)	Acc@1  86.72 ( 87.77)	Acc@5  99.22 ( 98.45)
Epoch: [51][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4186e-01 (3.8657e-01)	Acc@1  90.62 ( 87.80)	Acc@5 100.00 ( 98.46)
Epoch: [51][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.1338e-01 (3.8588e-01)	Acc@1  87.50 ( 87.83)	Acc@5  98.44 ( 98.47)
Epoch: [51][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.2452e-01 (3.8674e-01)	Acc@1  85.16 ( 87.80)	Acc@5  96.09 ( 98.44)
Epoch: [51][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6908e-01 (3.8794e-01)	Acc@1  90.62 ( 87.78)	Acc@5  99.22 ( 98.44)
Epoch: [51][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.3557e-01 (3.8850e-01)	Acc@1  86.72 ( 87.78)	Acc@5  98.44 ( 98.43)
Epoch: [51][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.4904e-01 (3.8860e-01)	Acc@1  85.16 ( 87.79)	Acc@5  98.44 ( 98.43)
Epoch: [51][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.5691e-01 (3.8822e-01)	Acc@1  85.00 ( 87.79)	Acc@5  97.50 ( 98.45)
## e[51] optimizer.zero_grad (sum) time: 0.11623549461364746
## e[51]       loss.backward (sum) time: 2.12168288230896
## e[51]      optimizer.step (sum) time: 0.73970627784729
## epoch[51] training(only) time: 12.991109371185303
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.5114e+00 (1.5114e+00)	Acc@1  68.00 ( 68.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.9097e+00 (1.5884e+00)	Acc@1  67.00 ( 66.64)	Acc@5  89.00 ( 88.09)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.5355e+00 (1.5434e+00)	Acc@1  67.00 ( 65.90)	Acc@5  89.00 ( 88.52)
Test: [ 30/100]	Time  0.018 ( 0.021)	Loss 1.7733e+00 (1.5656e+00)	Acc@1  62.00 ( 65.19)	Acc@5  85.00 ( 88.00)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4120e+00 (1.5417e+00)	Acc@1  67.00 ( 65.39)	Acc@5  91.00 ( 88.32)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3016e+00 (1.5491e+00)	Acc@1  67.00 ( 65.22)	Acc@5  90.00 ( 88.14)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9186e+00 (1.5291e+00)	Acc@1  63.00 ( 65.61)	Acc@5  85.00 ( 88.36)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.5535e+00 (1.5308e+00)	Acc@1  70.00 ( 65.54)	Acc@5  89.00 ( 88.45)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4676e+00 (1.5296e+00)	Acc@1  71.00 ( 65.60)	Acc@5  85.00 ( 88.37)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.8017e+00 (1.5155e+00)	Acc@1  59.00 ( 65.76)	Acc@5  87.00 ( 88.47)
 * Acc@1 65.780 Acc@5 88.600
### epoch[51] execution time: 14.934061288833618
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.185 ( 0.185)	Data  0.152 ( 0.152)	Loss 2.3570e-01 (2.3570e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 2.5963e-01 (3.3856e-01)	Acc@1  89.84 ( 89.06)	Acc@5 100.00 ( 99.08)
Epoch: [52][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 2.9687e-01 (3.6064e-01)	Acc@1  89.84 ( 89.14)	Acc@5  98.44 ( 98.51)
Epoch: [52][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 3.0481e-01 (3.6248e-01)	Acc@1  92.19 ( 89.16)	Acc@5  98.44 ( 98.41)
Epoch: [52][ 40/391]	Time  0.030 ( 0.036)	Data  0.002 ( 0.006)	Loss 3.5707e-01 (3.5742e-01)	Acc@1  88.28 ( 88.95)	Acc@5  98.44 ( 98.57)
Epoch: [52][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.2222e-01 (3.5190e-01)	Acc@1  88.28 ( 89.02)	Acc@5  99.22 ( 98.58)
Epoch: [52][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.3872e-01 (3.5290e-01)	Acc@1  85.16 ( 88.95)	Acc@5  97.66 ( 98.59)
Epoch: [52][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.8472e-01 (3.5679e-01)	Acc@1  88.28 ( 88.78)	Acc@5  98.44 ( 98.62)
Epoch: [52][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.0604e-01 (3.5765e-01)	Acc@1  83.59 ( 88.76)	Acc@5  99.22 ( 98.67)
Epoch: [52][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 5.9957e-01 (3.6237e-01)	Acc@1  82.81 ( 88.62)	Acc@5  96.09 ( 98.59)
Epoch: [52][100/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.004)	Loss 3.6731e-01 (3.6289e-01)	Acc@1  86.72 ( 88.56)	Acc@5  99.22 ( 98.61)
Epoch: [52][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0080e-01 (3.6381e-01)	Acc@1  91.41 ( 88.59)	Acc@5  99.22 ( 98.61)
Epoch: [52][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5144e-01 (3.6825e-01)	Acc@1  89.06 ( 88.47)	Acc@5  97.66 ( 98.59)
Epoch: [52][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4568e-01 (3.7026e-01)	Acc@1  92.19 ( 88.43)	Acc@5  99.22 ( 98.56)
Epoch: [52][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9079e-01 (3.7097e-01)	Acc@1  89.06 ( 88.43)	Acc@5  99.22 ( 98.56)
Epoch: [52][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9777e-01 (3.7052e-01)	Acc@1  86.72 ( 88.41)	Acc@5  97.66 ( 98.58)
Epoch: [52][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4705e-01 (3.6981e-01)	Acc@1  87.50 ( 88.41)	Acc@5 100.00 ( 98.61)
Epoch: [52][170/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8440e-01 (3.6719e-01)	Acc@1  85.94 ( 88.54)	Acc@5  97.66 ( 98.62)
Epoch: [52][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9513e-01 (3.6535e-01)	Acc@1  91.41 ( 88.60)	Acc@5  99.22 ( 98.62)
Epoch: [52][190/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.9487e-01 (3.6465e-01)	Acc@1  85.16 ( 88.64)	Acc@5  96.88 ( 98.62)
Epoch: [52][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4620e-01 (3.6619e-01)	Acc@1  89.06 ( 88.59)	Acc@5  99.22 ( 98.60)
Epoch: [52][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4690e-01 (3.6523e-01)	Acc@1  89.84 ( 88.60)	Acc@5  98.44 ( 98.63)
Epoch: [52][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1418e-01 (3.6529e-01)	Acc@1  85.16 ( 88.57)	Acc@5  99.22 ( 98.64)
Epoch: [52][230/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.4355e-01 (3.6571e-01)	Acc@1  83.59 ( 88.53)	Acc@5  98.44 ( 98.63)
Epoch: [52][240/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.6337e-01 (3.6625e-01)	Acc@1  91.41 ( 88.53)	Acc@5 100.00 ( 98.63)
Epoch: [52][250/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.0354e-01 (3.6530e-01)	Acc@1  83.59 ( 88.56)	Acc@5  98.44 ( 98.66)
Epoch: [52][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0399e-01 (3.6676e-01)	Acc@1  85.94 ( 88.52)	Acc@5  97.66 ( 98.64)
Epoch: [52][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1216e-01 (3.6829e-01)	Acc@1  85.94 ( 88.47)	Acc@5  98.44 ( 98.63)
Epoch: [52][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8210e-01 (3.6831e-01)	Acc@1  89.06 ( 88.44)	Acc@5  99.22 ( 98.63)
Epoch: [52][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0825e-01 (3.7043e-01)	Acc@1  88.28 ( 88.42)	Acc@5  97.66 ( 98.61)
Epoch: [52][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6084e-01 (3.7055e-01)	Acc@1  86.72 ( 88.40)	Acc@5  97.66 ( 98.61)
Epoch: [52][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8396e-01 (3.7125e-01)	Acc@1  88.28 ( 88.37)	Acc@5  96.88 ( 98.61)
Epoch: [52][320/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 4.2532e-01 (3.7014e-01)	Acc@1  85.16 ( 88.38)	Acc@5  98.44 ( 98.61)
Epoch: [52][330/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.002)	Loss 3.0171e-01 (3.6881e-01)	Acc@1  93.75 ( 88.43)	Acc@5  98.44 ( 98.61)
Epoch: [52][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.0772e-01 (3.7047e-01)	Acc@1  89.84 ( 88.37)	Acc@5 100.00 ( 98.61)
Epoch: [52][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.3940e-01 (3.7043e-01)	Acc@1  86.72 ( 88.37)	Acc@5  97.66 ( 98.61)
Epoch: [52][360/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 3.2693e-01 (3.7167e-01)	Acc@1  91.41 ( 88.34)	Acc@5 100.00 ( 98.61)
Epoch: [52][370/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.1788e-01 (3.7211e-01)	Acc@1  83.59 ( 88.33)	Acc@5  99.22 ( 98.61)
Epoch: [52][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4969e-01 (3.7310e-01)	Acc@1  92.19 ( 88.30)	Acc@5  99.22 ( 98.60)
Epoch: [52][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.4160e-01 (3.7419e-01)	Acc@1  87.50 ( 88.25)	Acc@5  98.75 ( 98.60)
## e[52] optimizer.zero_grad (sum) time: 0.11629319190979004
## e[52]       loss.backward (sum) time: 2.1217856407165527
## e[52]      optimizer.step (sum) time: 0.7294254302978516
## epoch[52] training(only) time: 12.982286930084229
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.4548e+00 (1.4548e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.7351e+00 (1.5526e+00)	Acc@1  65.00 ( 65.73)	Acc@5  91.00 ( 88.27)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.5962e+00 (1.5083e+00)	Acc@1  65.00 ( 65.90)	Acc@5  86.00 ( 88.81)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.7425e+00 (1.5262e+00)	Acc@1  60.00 ( 65.10)	Acc@5  83.00 ( 88.26)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4188e+00 (1.5132e+00)	Acc@1  66.00 ( 65.34)	Acc@5  91.00 ( 88.61)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3619e+00 (1.5310e+00)	Acc@1  69.00 ( 65.59)	Acc@5  88.00 ( 88.29)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9133e+00 (1.5175e+00)	Acc@1  65.00 ( 65.84)	Acc@5  85.00 ( 88.39)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.5520e+00 (1.5149e+00)	Acc@1  63.00 ( 65.77)	Acc@5  87.00 ( 88.61)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4608e+00 (1.5126e+00)	Acc@1  65.00 ( 65.94)	Acc@5  87.00 ( 88.56)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.9484e+00 (1.5005e+00)	Acc@1  57.00 ( 66.04)	Acc@5  84.00 ( 88.62)
 * Acc@1 66.100 Acc@5 88.740
### epoch[52] execution time: 14.937634468078613
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.182 ( 0.182)	Data  0.152 ( 0.152)	Loss 2.6012e-01 (2.6012e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.031 ( 0.046)	Data  0.001 ( 0.015)	Loss 2.6922e-01 (3.0492e-01)	Acc@1  90.62 ( 90.13)	Acc@5  98.44 ( 99.36)
Epoch: [53][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 4.6082e-01 (3.2372e-01)	Acc@1  88.28 ( 89.73)	Acc@5  97.66 ( 99.11)
Epoch: [53][ 30/391]	Time  0.029 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.1760e-01 (3.2714e-01)	Acc@1  92.97 ( 89.94)	Acc@5  98.44 ( 98.84)
Epoch: [53][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 3.6259e-01 (3.4146e-01)	Acc@1  90.62 ( 89.25)	Acc@5  98.44 ( 98.74)
Epoch: [53][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 4.2316e-01 (3.5325e-01)	Acc@1  85.94 ( 88.92)	Acc@5  98.44 ( 98.71)
Epoch: [53][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.3435e-01 (3.5258e-01)	Acc@1  89.84 ( 88.92)	Acc@5  98.44 ( 98.71)
Epoch: [53][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.5296e-01 (3.5585e-01)	Acc@1  87.50 ( 88.90)	Acc@5  99.22 ( 98.72)
Epoch: [53][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.2966e-01 (3.6178e-01)	Acc@1  89.06 ( 88.79)	Acc@5  98.44 ( 98.70)
Epoch: [53][ 90/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.5331e-01 (3.5989e-01)	Acc@1  93.75 ( 88.87)	Acc@5 100.00 ( 98.71)
Epoch: [53][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.5406e-01 (3.6126e-01)	Acc@1  84.38 ( 88.69)	Acc@5  98.44 ( 98.72)
Epoch: [53][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3535e-01 (3.6261e-01)	Acc@1  88.28 ( 88.69)	Acc@5  99.22 ( 98.63)
Epoch: [53][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0657e-01 (3.5950e-01)	Acc@1  92.97 ( 88.74)	Acc@5 100.00 ( 98.66)
Epoch: [53][130/391]	Time  0.034 ( 0.034)	Data  0.003 ( 0.003)	Loss 3.2214e-01 (3.5840e-01)	Acc@1  89.84 ( 88.71)	Acc@5  98.44 ( 98.68)
Epoch: [53][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.7036e-01 (3.5714e-01)	Acc@1  88.28 ( 88.79)	Acc@5  98.44 ( 98.67)
Epoch: [53][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8060e-01 (3.5660e-01)	Acc@1  88.28 ( 88.81)	Acc@5  99.22 ( 98.69)
Epoch: [53][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5842e-01 (3.5512e-01)	Acc@1  89.06 ( 88.90)	Acc@5  97.66 ( 98.68)
Epoch: [53][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7068e-01 (3.5866e-01)	Acc@1  82.81 ( 88.82)	Acc@5  96.88 ( 98.64)
Epoch: [53][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.2318e-01 (3.5776e-01)	Acc@1  84.38 ( 88.83)	Acc@5  98.44 ( 98.64)
Epoch: [53][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6873e-01 (3.5679e-01)	Acc@1  90.62 ( 88.87)	Acc@5  99.22 ( 98.67)
Epoch: [53][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0740e-01 (3.5748e-01)	Acc@1  85.16 ( 88.83)	Acc@5  97.66 ( 98.69)
Epoch: [53][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6825e-01 (3.5677e-01)	Acc@1  92.97 ( 88.81)	Acc@5 100.00 ( 98.70)
Epoch: [53][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7021e-01 (3.5678e-01)	Acc@1  86.72 ( 88.79)	Acc@5 100.00 ( 98.71)
Epoch: [53][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6287e-01 (3.5727e-01)	Acc@1  87.50 ( 88.75)	Acc@5  98.44 ( 98.70)
Epoch: [53][240/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9845e-01 (3.5707e-01)	Acc@1  92.19 ( 88.77)	Acc@5  98.44 ( 98.71)
Epoch: [53][250/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2276e-01 (3.5831e-01)	Acc@1  92.19 ( 88.79)	Acc@5  99.22 ( 98.70)
Epoch: [53][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9466e-01 (3.5944e-01)	Acc@1  85.16 ( 88.74)	Acc@5  99.22 ( 98.68)
Epoch: [53][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8454e-01 (3.5997e-01)	Acc@1  92.97 ( 88.75)	Acc@5  97.66 ( 98.68)
Epoch: [53][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7797e-01 (3.6068e-01)	Acc@1  91.41 ( 88.72)	Acc@5  99.22 ( 98.69)
Epoch: [53][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.7313e-01 (3.6179e-01)	Acc@1  88.28 ( 88.69)	Acc@5  99.22 ( 98.68)
Epoch: [53][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8578e-01 (3.6175e-01)	Acc@1  91.41 ( 88.70)	Acc@5  98.44 ( 98.69)
Epoch: [53][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.0502e-01 (3.6193e-01)	Acc@1  87.50 ( 88.68)	Acc@5  99.22 ( 98.69)
Epoch: [53][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.5219e-01 (3.6211e-01)	Acc@1  85.94 ( 88.66)	Acc@5  98.44 ( 98.70)
Epoch: [53][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6439e-01 (3.6189e-01)	Acc@1  87.50 ( 88.69)	Acc@5  99.22 ( 98.69)
Epoch: [53][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.7207e-01 (3.6224e-01)	Acc@1  86.72 ( 88.68)	Acc@5  98.44 ( 98.68)
Epoch: [53][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.1158e-01 (3.6223e-01)	Acc@1  88.28 ( 88.69)	Acc@5  98.44 ( 98.69)
Epoch: [53][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8973e-01 (3.6243e-01)	Acc@1  90.62 ( 88.68)	Acc@5  97.66 ( 98.69)
Epoch: [53][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.3414e-01 (3.6176e-01)	Acc@1  85.16 ( 88.72)	Acc@5  96.88 ( 98.69)
Epoch: [53][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6067e-01 (3.6126e-01)	Acc@1  91.41 ( 88.74)	Acc@5 100.00 ( 98.69)
Epoch: [53][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.4564e-01 (3.6275e-01)	Acc@1  87.50 ( 88.69)	Acc@5 100.00 ( 98.68)
## e[53] optimizer.zero_grad (sum) time: 0.1156914234161377
## e[53]       loss.backward (sum) time: 2.1084704399108887
## e[53]      optimizer.step (sum) time: 0.7346317768096924
## epoch[53] training(only) time: 13.024781942367554
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.4219e+00 (1.4219e+00)	Acc@1  66.00 ( 66.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.8008e+00 (1.5615e+00)	Acc@1  71.00 ( 65.91)	Acc@5  90.00 ( 88.55)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.4623e+00 (1.5187e+00)	Acc@1  67.00 ( 66.33)	Acc@5  89.00 ( 88.86)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.7835e+00 (1.5337e+00)	Acc@1  56.00 ( 65.45)	Acc@5  88.00 ( 88.61)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 1.3773e+00 (1.5237e+00)	Acc@1  74.00 ( 65.73)	Acc@5  91.00 ( 88.80)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4344e+00 (1.5417e+00)	Acc@1  71.00 ( 65.76)	Acc@5  89.00 ( 88.47)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.0856e+00 (1.5284e+00)	Acc@1  60.00 ( 66.10)	Acc@5  85.00 ( 88.54)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.5741e+00 (1.5328e+00)	Acc@1  66.00 ( 65.90)	Acc@5  87.00 ( 88.63)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4464e+00 (1.5389e+00)	Acc@1  69.00 ( 65.93)	Acc@5  86.00 ( 88.52)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.9540e+00 (1.5292e+00)	Acc@1  56.00 ( 65.97)	Acc@5  83.00 ( 88.56)
 * Acc@1 65.910 Acc@5 88.590
### epoch[53] execution time: 14.982101202011108
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.176 ( 0.176)	Data  0.147 ( 0.147)	Loss 2.2219e-01 (2.2219e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 2.6614e-01 (3.3516e-01)	Acc@1  92.19 ( 89.77)	Acc@5  98.44 ( 98.58)
Epoch: [54][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 4.3453e-01 (3.5379e-01)	Acc@1  88.28 ( 89.51)	Acc@5  98.44 ( 98.40)
Epoch: [54][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 3.0750e-01 (3.4625e-01)	Acc@1  89.84 ( 89.54)	Acc@5  98.44 ( 98.51)
Epoch: [54][ 40/391]	Time  0.032 ( 0.036)	Data  0.002 ( 0.006)	Loss 4.1599e-01 (3.5153e-01)	Acc@1  86.72 ( 89.33)	Acc@5  97.66 ( 98.53)
Epoch: [54][ 50/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.2120e-01 (3.4497e-01)	Acc@1  88.28 ( 89.40)	Acc@5  98.44 ( 98.64)
Epoch: [54][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 5.4240e-01 (3.4598e-01)	Acc@1  83.59 ( 89.31)	Acc@5  99.22 ( 98.64)
Epoch: [54][ 70/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.004)	Loss 4.0137e-01 (3.4397e-01)	Acc@1  85.94 ( 89.24)	Acc@5  97.66 ( 98.71)
Epoch: [54][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.8761e-01 (3.4422e-01)	Acc@1  89.06 ( 89.25)	Acc@5  96.09 ( 98.69)
Epoch: [54][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.4207e-01 (3.4330e-01)	Acc@1  87.50 ( 89.21)	Acc@5  98.44 ( 98.72)
Epoch: [54][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 4.6268e-01 (3.4437e-01)	Acc@1  84.38 ( 89.16)	Acc@5  98.44 ( 98.75)
Epoch: [54][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9843e-01 (3.4196e-01)	Acc@1  90.62 ( 89.20)	Acc@5  98.44 ( 98.79)
Epoch: [54][120/391]	Time  0.029 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4816e-01 (3.4434e-01)	Acc@1  88.28 ( 89.20)	Acc@5  99.22 ( 98.73)
Epoch: [54][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7158e-01 (3.4217e-01)	Acc@1  92.19 ( 89.25)	Acc@5  97.66 ( 98.74)
Epoch: [54][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6522e-01 (3.4200e-01)	Acc@1  91.41 ( 89.28)	Acc@5 100.00 ( 98.74)
Epoch: [54][150/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.3703e-01 (3.3879e-01)	Acc@1  92.19 ( 89.38)	Acc@5  98.44 ( 98.75)
Epoch: [54][160/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1626e-01 (3.3981e-01)	Acc@1  91.41 ( 89.39)	Acc@5  97.66 ( 98.75)
Epoch: [54][170/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8054e-01 (3.3926e-01)	Acc@1  87.50 ( 89.40)	Acc@5 100.00 ( 98.78)
Epoch: [54][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.3615e-01 (3.3959e-01)	Acc@1  82.81 ( 89.32)	Acc@5  99.22 ( 98.79)
Epoch: [54][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5772e-01 (3.3758e-01)	Acc@1  89.06 ( 89.38)	Acc@5 100.00 ( 98.81)
Epoch: [54][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4461e-01 (3.3785e-01)	Acc@1  85.16 ( 89.35)	Acc@5  99.22 ( 98.83)
Epoch: [54][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4027e-01 (3.3895e-01)	Acc@1  91.41 ( 89.34)	Acc@5  99.22 ( 98.83)
Epoch: [54][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8854e-01 (3.3875e-01)	Acc@1  90.62 ( 89.31)	Acc@5  98.44 ( 98.83)
Epoch: [54][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9680e-01 (3.3803e-01)	Acc@1  91.41 ( 89.33)	Acc@5 100.00 ( 98.84)
Epoch: [54][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4714e-01 (3.3701e-01)	Acc@1  83.59 ( 89.36)	Acc@5  98.44 ( 98.85)
Epoch: [54][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3626e-01 (3.3893e-01)	Acc@1  88.28 ( 89.31)	Acc@5  97.66 ( 98.83)
Epoch: [54][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4009e-01 (3.3892e-01)	Acc@1  89.84 ( 89.33)	Acc@5  99.22 ( 98.84)
Epoch: [54][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.2636e-01 (3.4199e-01)	Acc@1  85.94 ( 89.24)	Acc@5  98.44 ( 98.81)
Epoch: [54][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3883e-01 (3.4229e-01)	Acc@1  90.62 ( 89.23)	Acc@5  98.44 ( 98.80)
Epoch: [54][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4969e-01 (3.4410e-01)	Acc@1  85.16 ( 89.17)	Acc@5  99.22 ( 98.79)
Epoch: [54][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4356e-01 (3.4508e-01)	Acc@1  88.28 ( 89.16)	Acc@5  95.31 ( 98.77)
Epoch: [54][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.6063e-01 (3.4528e-01)	Acc@1  82.81 ( 89.15)	Acc@5  96.88 ( 98.78)
Epoch: [54][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1371e-01 (3.4558e-01)	Acc@1  85.16 ( 89.10)	Acc@5  98.44 ( 98.80)
Epoch: [54][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3402e-01 (3.4432e-01)	Acc@1  95.31 ( 89.14)	Acc@5  99.22 ( 98.81)
Epoch: [54][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.3564e-01 (3.4615e-01)	Acc@1  87.50 ( 89.09)	Acc@5  97.66 ( 98.80)
Epoch: [54][350/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.9007e-01 (3.4762e-01)	Acc@1  85.94 ( 89.04)	Acc@5  98.44 ( 98.80)
Epoch: [54][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.8541e-01 (3.4815e-01)	Acc@1  82.81 ( 89.02)	Acc@5  96.88 ( 98.79)
Epoch: [54][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2592e-01 (3.4808e-01)	Acc@1  92.19 ( 89.04)	Acc@5 100.00 ( 98.79)
Epoch: [54][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.4867e-01 (3.4911e-01)	Acc@1  89.84 ( 88.99)	Acc@5  99.22 ( 98.80)
Epoch: [54][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.0561e-01 (3.5028e-01)	Acc@1  85.00 ( 88.99)	Acc@5  96.25 ( 98.78)
## e[54] optimizer.zero_grad (sum) time: 0.11612153053283691
## e[54]       loss.backward (sum) time: 2.0725011825561523
## e[54]      optimizer.step (sum) time: 0.7444560527801514
## epoch[54] training(only) time: 13.002968311309814
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.5800e+00 (1.5800e+00)	Acc@1  65.00 ( 65.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.8918e+00 (1.5911e+00)	Acc@1  65.00 ( 65.45)	Acc@5  88.00 ( 88.00)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7402e+00 (1.5604e+00)	Acc@1  63.00 ( 65.67)	Acc@5  89.00 ( 88.71)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.7180e+00 (1.5733e+00)	Acc@1  58.00 ( 65.00)	Acc@5  88.00 ( 88.39)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3309e+00 (1.5703e+00)	Acc@1  65.00 ( 64.95)	Acc@5  93.00 ( 88.71)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3548e+00 (1.5861e+00)	Acc@1  67.00 ( 64.96)	Acc@5  88.00 ( 88.39)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0325e+00 (1.5680e+00)	Acc@1  64.00 ( 65.41)	Acc@5  87.00 ( 88.49)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.5598e+00 (1.5638e+00)	Acc@1  68.00 ( 65.44)	Acc@5  87.00 ( 88.66)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4365e+00 (1.5568e+00)	Acc@1  70.00 ( 65.74)	Acc@5  85.00 ( 88.65)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.9238e+00 (1.5392e+00)	Acc@1  54.00 ( 65.85)	Acc@5  86.00 ( 88.79)
 * Acc@1 65.920 Acc@5 88.820
### epoch[54] execution time: 14.948325872421265
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.168 ( 0.168)	Data  0.141 ( 0.141)	Loss 3.0208e-01 (3.0208e-01)	Acc@1  90.62 ( 90.62)	Acc@5  98.44 ( 98.44)
Epoch: [55][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.014)	Loss 3.5254e-01 (3.0829e-01)	Acc@1  87.50 ( 90.27)	Acc@5  99.22 ( 99.08)
Epoch: [55][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.008)	Loss 2.4486e-01 (3.0580e-01)	Acc@1  93.75 ( 90.40)	Acc@5  99.22 ( 99.07)
Epoch: [55][ 30/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.006)	Loss 2.3284e-01 (3.0190e-01)	Acc@1  96.09 ( 90.78)	Acc@5  98.44 ( 99.09)
Epoch: [55][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.8984e-01 (2.9837e-01)	Acc@1  89.84 ( 90.61)	Acc@5  99.22 ( 99.16)
Epoch: [55][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.7228e-01 (3.0868e-01)	Acc@1  92.19 ( 90.40)	Acc@5 100.00 ( 99.13)
Epoch: [55][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.3171e-01 (3.1389e-01)	Acc@1  87.50 ( 90.20)	Acc@5  96.88 ( 99.01)
Epoch: [55][ 70/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.7049e-01 (3.1595e-01)	Acc@1  88.28 ( 90.07)	Acc@5  98.44 ( 99.01)
Epoch: [55][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.6305e-01 (3.2106e-01)	Acc@1  86.72 ( 89.92)	Acc@5  98.44 ( 98.95)
Epoch: [55][ 90/391]	Time  0.028 ( 0.034)	Data  0.002 ( 0.004)	Loss 2.7262e-01 (3.2266e-01)	Acc@1  90.62 ( 89.89)	Acc@5 100.00 ( 98.97)
Epoch: [55][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.8144e-01 (3.2390e-01)	Acc@1  89.06 ( 89.89)	Acc@5  98.44 ( 98.97)
Epoch: [55][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6822e-01 (3.2388e-01)	Acc@1  92.19 ( 89.94)	Acc@5  98.44 ( 98.94)
Epoch: [55][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.3137e-01 (3.2816e-01)	Acc@1  86.72 ( 89.77)	Acc@5  96.88 ( 98.92)
Epoch: [55][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.2202e-01 (3.2944e-01)	Acc@1  86.72 ( 89.72)	Acc@5  99.22 ( 98.89)
Epoch: [55][140/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.1801e-01 (3.3171e-01)	Acc@1  86.72 ( 89.65)	Acc@5  98.44 ( 98.87)
Epoch: [55][150/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.0544e-01 (3.3324e-01)	Acc@1  85.94 ( 89.55)	Acc@5  97.66 ( 98.87)
Epoch: [55][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2507e-01 (3.3270e-01)	Acc@1  89.06 ( 89.52)	Acc@5  98.44 ( 98.88)
Epoch: [55][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.4404e-01 (3.3258e-01)	Acc@1  85.94 ( 89.56)	Acc@5  97.66 ( 98.88)
Epoch: [55][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3533e-01 (3.3278e-01)	Acc@1  90.62 ( 89.56)	Acc@5  99.22 ( 98.89)
Epoch: [55][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4602e-01 (3.3245e-01)	Acc@1  90.62 ( 89.57)	Acc@5  98.44 ( 98.89)
Epoch: [55][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3392e-01 (3.3191e-01)	Acc@1  89.06 ( 89.58)	Acc@5  99.22 ( 98.90)
Epoch: [55][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7125e-01 (3.3179e-01)	Acc@1  92.97 ( 89.59)	Acc@5 100.00 ( 98.91)
Epoch: [55][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3595e-01 (3.3000e-01)	Acc@1  94.53 ( 89.60)	Acc@5 100.00 ( 98.94)
Epoch: [55][230/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3908e-01 (3.2879e-01)	Acc@1  92.97 ( 89.65)	Acc@5 100.00 ( 98.95)
Epoch: [55][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8555e-01 (3.3044e-01)	Acc@1  84.38 ( 89.59)	Acc@5  99.22 ( 98.96)
Epoch: [55][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2911e-01 (3.3054e-01)	Acc@1  92.19 ( 89.60)	Acc@5 100.00 ( 98.94)
Epoch: [55][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.0325e-01 (3.3302e-01)	Acc@1  86.72 ( 89.55)	Acc@5  97.66 ( 98.93)
Epoch: [55][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7067e-01 (3.3347e-01)	Acc@1  87.50 ( 89.53)	Acc@5  99.22 ( 98.93)
Epoch: [55][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0755e-01 (3.3485e-01)	Acc@1  89.84 ( 89.51)	Acc@5  99.22 ( 98.92)
Epoch: [55][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3135e-01 (3.3525e-01)	Acc@1  90.62 ( 89.51)	Acc@5  98.44 ( 98.91)
Epoch: [55][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.4405e-01 (3.3593e-01)	Acc@1  85.94 ( 89.48)	Acc@5  97.66 ( 98.90)
Epoch: [55][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.8986e-01 (3.3556e-01)	Acc@1  88.28 ( 89.50)	Acc@5  99.22 ( 98.91)
Epoch: [55][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.9701e-01 (3.3630e-01)	Acc@1  88.28 ( 89.48)	Acc@5  97.66 ( 98.90)
Epoch: [55][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.5688e-01 (3.3771e-01)	Acc@1  86.72 ( 89.44)	Acc@5  98.44 ( 98.89)
Epoch: [55][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.4548e-01 (3.3800e-01)	Acc@1  82.81 ( 89.45)	Acc@5  97.66 ( 98.89)
Epoch: [55][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.5716e-01 (3.3897e-01)	Acc@1  85.94 ( 89.44)	Acc@5  98.44 ( 98.89)
Epoch: [55][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.9076e-01 (3.3780e-01)	Acc@1  88.28 ( 89.48)	Acc@5  98.44 ( 98.91)
Epoch: [55][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.0662e-01 (3.3933e-01)	Acc@1  92.97 ( 89.43)	Acc@5  98.44 ( 98.89)
Epoch: [55][380/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.2949e-01 (3.3924e-01)	Acc@1  84.38 ( 89.42)	Acc@5  97.66 ( 98.89)
Epoch: [55][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6622e-01 (3.3929e-01)	Acc@1  88.75 ( 89.41)	Acc@5  97.50 ( 98.89)
## e[55] optimizer.zero_grad (sum) time: 0.1151127815246582
## e[55]       loss.backward (sum) time: 2.08455228805542
## e[55]      optimizer.step (sum) time: 0.7277429103851318
## epoch[55] training(only) time: 12.980286598205566
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.5172e+00 (1.5172e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.031)	Loss 1.9432e+00 (1.6684e+00)	Acc@1  64.00 ( 65.64)	Acc@5  90.00 ( 87.64)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.5503e+00 (1.5853e+00)	Acc@1  65.00 ( 65.90)	Acc@5  88.00 ( 88.62)
Test: [ 30/100]	Time  0.018 ( 0.022)	Loss 1.8985e+00 (1.5917e+00)	Acc@1  55.00 ( 65.58)	Acc@5  86.00 ( 88.26)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.2976e+00 (1.5758e+00)	Acc@1  68.00 ( 65.49)	Acc@5  89.00 ( 88.37)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4057e+00 (1.5910e+00)	Acc@1  68.00 ( 65.59)	Acc@5  89.00 ( 88.29)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.8329e+00 (1.5711e+00)	Acc@1  64.00 ( 65.89)	Acc@5  87.00 ( 88.41)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.4814e+00 (1.5682e+00)	Acc@1  65.00 ( 65.90)	Acc@5  89.00 ( 88.63)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4333e+00 (1.5627e+00)	Acc@1  68.00 ( 66.07)	Acc@5  88.00 ( 88.60)
Test: [ 90/100]	Time  0.023 ( 0.019)	Loss 2.1982e+00 (1.5459e+00)	Acc@1  55.00 ( 66.24)	Acc@5  82.00 ( 88.74)
 * Acc@1 66.260 Acc@5 88.810
### epoch[55] execution time: 14.942736148834229
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.204 ( 0.204)	Data  0.176 ( 0.176)	Loss 3.9221e-01 (3.9221e-01)	Acc@1  85.16 ( 85.16)	Acc@5  98.44 ( 98.44)
Epoch: [56][ 10/391]	Time  0.031 ( 0.048)	Data  0.002 ( 0.018)	Loss 2.5668e-01 (3.0133e-01)	Acc@1  90.62 ( 89.63)	Acc@5  99.22 ( 99.01)
Epoch: [56][ 20/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.010)	Loss 2.8907e-01 (2.9516e-01)	Acc@1  92.19 ( 90.33)	Acc@5  99.22 ( 99.11)
Epoch: [56][ 30/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.008)	Loss 3.3125e-01 (2.9822e-01)	Acc@1  87.50 ( 90.27)	Acc@5  99.22 ( 99.04)
Epoch: [56][ 40/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.006)	Loss 2.4683e-01 (2.9482e-01)	Acc@1  90.62 ( 90.36)	Acc@5  99.22 ( 99.12)
Epoch: [56][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 3.6855e-01 (3.0557e-01)	Acc@1  89.84 ( 90.24)	Acc@5  98.44 ( 99.05)
Epoch: [56][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.4867e-01 (3.0117e-01)	Acc@1  92.97 ( 90.41)	Acc@5  99.22 ( 99.05)
Epoch: [56][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.8110e-01 (3.0974e-01)	Acc@1  92.19 ( 90.25)	Acc@5  98.44 ( 98.97)
Epoch: [56][ 80/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.6937e-01 (3.0588e-01)	Acc@1  86.72 ( 90.39)	Acc@5  98.44 ( 98.99)
Epoch: [56][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.6207e-01 (3.1059e-01)	Acc@1  91.41 ( 90.26)	Acc@5 100.00 ( 98.99)
Epoch: [56][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.3425e-01 (3.0690e-01)	Acc@1  90.62 ( 90.28)	Acc@5 100.00 ( 99.03)
Epoch: [56][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.6831e-01 (3.0438e-01)	Acc@1  90.62 ( 90.39)	Acc@5 100.00 ( 99.06)
Epoch: [56][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9046e-01 (3.0737e-01)	Acc@1  92.97 ( 90.24)	Acc@5 100.00 ( 99.06)
Epoch: [56][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.8769e-01 (3.0948e-01)	Acc@1  85.16 ( 90.17)	Acc@5  99.22 ( 99.08)
Epoch: [56][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7641e-01 (3.1002e-01)	Acc@1  96.09 ( 90.13)	Acc@5  99.22 ( 99.05)
Epoch: [56][150/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8011e-01 (3.0924e-01)	Acc@1  91.41 ( 90.15)	Acc@5  99.22 ( 99.08)
Epoch: [56][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7498e-01 (3.0770e-01)	Acc@1  91.41 ( 90.22)	Acc@5  99.22 ( 99.09)
Epoch: [56][170/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.5063e-01 (3.0768e-01)	Acc@1  90.62 ( 90.19)	Acc@5  98.44 ( 99.10)
Epoch: [56][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.2896e-01 (3.1009e-01)	Acc@1  78.12 ( 90.15)	Acc@5  97.66 ( 99.11)
Epoch: [56][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9478e-01 (3.1142e-01)	Acc@1  90.62 ( 90.08)	Acc@5 100.00 ( 99.11)
Epoch: [56][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9747e-01 (3.1163e-01)	Acc@1  95.31 ( 90.10)	Acc@5 100.00 ( 99.12)
Epoch: [56][210/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2748e-01 (3.1273e-01)	Acc@1  92.97 ( 90.07)	Acc@5 100.00 ( 99.10)
Epoch: [56][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4522e-01 (3.1309e-01)	Acc@1  89.84 ( 90.08)	Acc@5  99.22 ( 99.10)
Epoch: [56][230/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 3.1114e-01 (3.1471e-01)	Acc@1  89.84 ( 90.00)	Acc@5  99.22 ( 99.10)
Epoch: [56][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5976e-01 (3.1607e-01)	Acc@1  92.97 ( 89.97)	Acc@5  98.44 ( 99.07)
Epoch: [56][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7797e-01 (3.1676e-01)	Acc@1  90.62 ( 89.96)	Acc@5  96.88 ( 99.06)
Epoch: [56][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0840e-01 (3.1698e-01)	Acc@1  87.50 ( 89.95)	Acc@5 100.00 ( 99.07)
Epoch: [56][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8278e-01 (3.1660e-01)	Acc@1  89.84 ( 89.94)	Acc@5  97.66 ( 99.07)
Epoch: [56][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9942e-01 (3.1819e-01)	Acc@1  89.06 ( 89.91)	Acc@5  98.44 ( 99.05)
Epoch: [56][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2097e-01 (3.2034e-01)	Acc@1  89.84 ( 89.83)	Acc@5  99.22 ( 99.03)
Epoch: [56][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5923e-01 (3.2037e-01)	Acc@1  91.41 ( 89.83)	Acc@5  98.44 ( 99.03)
Epoch: [56][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7381e-01 (3.2051e-01)	Acc@1  88.28 ( 89.81)	Acc@5 100.00 ( 99.02)
Epoch: [56][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0718e-01 (3.2033e-01)	Acc@1  89.84 ( 89.82)	Acc@5  99.22 ( 99.03)
Epoch: [56][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2113e-01 (3.2129e-01)	Acc@1  92.97 ( 89.78)	Acc@5 100.00 ( 99.03)
Epoch: [56][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5029e-01 (3.2114e-01)	Acc@1  87.50 ( 89.77)	Acc@5 100.00 ( 99.04)
Epoch: [56][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1127e-01 (3.2212e-01)	Acc@1  90.62 ( 89.75)	Acc@5  97.66 ( 99.03)
Epoch: [56][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1443e-01 (3.2193e-01)	Acc@1  92.19 ( 89.76)	Acc@5  96.88 ( 99.02)
Epoch: [56][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8529e-01 (3.2194e-01)	Acc@1  85.94 ( 89.77)	Acc@5  99.22 ( 99.02)
Epoch: [56][380/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9657e-01 (3.2071e-01)	Acc@1  92.19 ( 89.81)	Acc@5  98.44 ( 99.03)
Epoch: [56][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.7543e-01 (3.2102e-01)	Acc@1  88.75 ( 89.80)	Acc@5 100.00 ( 99.04)
## e[56] optimizer.zero_grad (sum) time: 0.116302490234375
## e[56]       loss.backward (sum) time: 2.1221675872802734
## e[56]      optimizer.step (sum) time: 0.7434914112091064
## epoch[56] training(only) time: 13.009873151779175
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.5419e+00 (1.5419e+00)	Acc@1  65.00 ( 65.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.028)	Loss 1.8314e+00 (1.6382e+00)	Acc@1  66.00 ( 65.82)	Acc@5  91.00 ( 87.82)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.6185e+00 (1.5672e+00)	Acc@1  66.00 ( 66.19)	Acc@5  87.00 ( 88.43)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7217e+00 (1.5801e+00)	Acc@1  60.00 ( 65.61)	Acc@5  87.00 ( 88.29)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4517e+00 (1.5672e+00)	Acc@1  69.00 ( 65.71)	Acc@5  89.00 ( 88.59)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3865e+00 (1.5888e+00)	Acc@1  69.00 ( 65.75)	Acc@5  89.00 ( 88.47)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0496e+00 (1.5705e+00)	Acc@1  62.00 ( 66.05)	Acc@5  85.00 ( 88.62)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6843e+00 (1.5677e+00)	Acc@1  62.00 ( 65.94)	Acc@5  88.00 ( 88.80)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5392e+00 (1.5684e+00)	Acc@1  67.00 ( 66.01)	Acc@5  88.00 ( 88.70)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 2.2276e+00 (1.5524e+00)	Acc@1  53.00 ( 66.16)	Acc@5  85.00 ( 88.84)
 * Acc@1 66.130 Acc@5 88.850
### epoch[56] execution time: 14.953677892684937
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.177 ( 0.177)	Data  0.149 ( 0.149)	Loss 2.4003e-01 (2.4003e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 3.4853e-01 (2.7242e-01)	Acc@1  86.72 ( 91.26)	Acc@5  98.44 ( 99.15)
Epoch: [57][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 4.2007e-01 (2.9313e-01)	Acc@1  89.84 ( 90.70)	Acc@5  97.66 ( 99.00)
Epoch: [57][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 3.4157e-01 (2.9659e-01)	Acc@1  92.19 ( 90.47)	Acc@5  98.44 ( 99.04)
Epoch: [57][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.8286e-01 (2.9740e-01)	Acc@1  90.62 ( 90.51)	Acc@5 100.00 ( 99.07)
Epoch: [57][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 3.2856e-01 (2.9698e-01)	Acc@1  90.62 ( 90.47)	Acc@5  97.66 ( 99.03)
Epoch: [57][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.5409e-01 (2.9479e-01)	Acc@1  91.41 ( 90.55)	Acc@5 100.00 ( 99.07)
Epoch: [57][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.1494e-01 (2.9425e-01)	Acc@1  89.06 ( 90.57)	Acc@5  99.22 ( 99.09)
Epoch: [57][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.7938e-01 (2.9526e-01)	Acc@1  93.75 ( 90.54)	Acc@5  98.44 ( 99.05)
Epoch: [57][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.8742e-01 (2.9533e-01)	Acc@1  89.06 ( 90.50)	Acc@5  98.44 ( 99.07)
Epoch: [57][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.8947e-01 (2.9629e-01)	Acc@1  90.62 ( 90.50)	Acc@5  98.44 ( 99.03)
Epoch: [57][110/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 3.5285e-01 (2.9598e-01)	Acc@1  90.62 ( 90.53)	Acc@5  96.88 ( 99.02)
Epoch: [57][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.6716e-01 (2.9745e-01)	Acc@1  88.28 ( 90.52)	Acc@5  96.88 ( 99.01)
Epoch: [57][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0565e-01 (2.9806e-01)	Acc@1  95.31 ( 90.49)	Acc@5 100.00 ( 99.02)
Epoch: [57][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.6623e-01 (2.9970e-01)	Acc@1  88.28 ( 90.43)	Acc@5  98.44 ( 99.04)
Epoch: [57][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4295e-01 (3.0147e-01)	Acc@1  89.84 ( 90.39)	Acc@5  97.66 ( 99.03)
Epoch: [57][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7150e-01 (3.0429e-01)	Acc@1  88.28 ( 90.33)	Acc@5  99.22 ( 99.04)
Epoch: [57][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4943e-01 (3.0457e-01)	Acc@1  91.41 ( 90.38)	Acc@5  99.22 ( 99.05)
Epoch: [57][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4057e-01 (3.0583e-01)	Acc@1  89.84 ( 90.32)	Acc@5  97.66 ( 99.05)
Epoch: [57][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9907e-01 (3.0636e-01)	Acc@1  92.19 ( 90.29)	Acc@5 100.00 ( 99.06)
Epoch: [57][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1818e-01 (3.0414e-01)	Acc@1  91.41 ( 90.31)	Acc@5 100.00 ( 99.07)
Epoch: [57][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9580e-01 (3.0413e-01)	Acc@1  82.81 ( 90.27)	Acc@5 100.00 ( 99.09)
Epoch: [57][220/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5788e-01 (3.0385e-01)	Acc@1  89.06 ( 90.31)	Acc@5  99.22 ( 99.07)
Epoch: [57][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.4975e-01 (3.0537e-01)	Acc@1  82.81 ( 90.27)	Acc@5  96.88 ( 99.05)
Epoch: [57][240/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6878e-01 (3.0517e-01)	Acc@1  89.06 ( 90.26)	Acc@5 100.00 ( 99.05)
Epoch: [57][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5403e-01 (3.0653e-01)	Acc@1  92.97 ( 90.22)	Acc@5 100.00 ( 99.05)
Epoch: [57][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6194e-01 (3.0719e-01)	Acc@1  87.50 ( 90.20)	Acc@5  98.44 ( 99.05)
Epoch: [57][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9333e-01 (3.0745e-01)	Acc@1  91.41 ( 90.22)	Acc@5 100.00 ( 99.05)
Epoch: [57][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0411e-01 (3.0849e-01)	Acc@1  89.84 ( 90.19)	Acc@5 100.00 ( 99.06)
Epoch: [57][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7574e-01 (3.0845e-01)	Acc@1  89.06 ( 90.18)	Acc@5  99.22 ( 99.06)
Epoch: [57][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3783e-01 (3.0808e-01)	Acc@1  88.28 ( 90.19)	Acc@5  99.22 ( 99.06)
Epoch: [57][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.7519e-01 (3.0868e-01)	Acc@1  91.41 ( 90.18)	Acc@5  99.22 ( 99.05)
Epoch: [57][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.7999e-01 (3.0797e-01)	Acc@1  88.28 ( 90.21)	Acc@5 100.00 ( 99.05)
Epoch: [57][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.8853e-01 (3.0855e-01)	Acc@1  85.94 ( 90.17)	Acc@5  97.66 ( 99.05)
Epoch: [57][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.6815e-01 (3.1005e-01)	Acc@1  83.59 ( 90.13)	Acc@5  97.66 ( 99.04)
Epoch: [57][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.6119e-01 (3.1078e-01)	Acc@1  89.84 ( 90.14)	Acc@5  98.44 ( 99.04)
Epoch: [57][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.3779e-01 (3.1110e-01)	Acc@1  89.84 ( 90.11)	Acc@5  96.09 ( 99.03)
Epoch: [57][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2265e-01 (3.1117e-01)	Acc@1  89.84 ( 90.12)	Acc@5  98.44 ( 99.03)
Epoch: [57][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1787e-01 (3.1158e-01)	Acc@1  91.41 ( 90.08)	Acc@5 100.00 ( 99.04)
Epoch: [57][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.3003e-01 (3.1328e-01)	Acc@1  77.50 ( 90.04)	Acc@5  95.00 ( 99.03)
## e[57] optimizer.zero_grad (sum) time: 0.11533474922180176
## e[57]       loss.backward (sum) time: 2.1263504028320312
## e[57]      optimizer.step (sum) time: 0.7324471473693848
## epoch[57] training(only) time: 12.998056650161743
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.4836e+00 (1.4836e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0222e+00 (1.6819e+00)	Acc@1  64.00 ( 66.09)	Acc@5  87.00 ( 86.73)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7175e+00 (1.6159e+00)	Acc@1  67.00 ( 66.24)	Acc@5  87.00 ( 87.57)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7158e+00 (1.6286e+00)	Acc@1  61.00 ( 65.26)	Acc@5  85.00 ( 87.26)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3976e+00 (1.6076e+00)	Acc@1  65.00 ( 65.51)	Acc@5  90.00 ( 87.59)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4832e+00 (1.6211e+00)	Acc@1  65.00 ( 65.35)	Acc@5  89.00 ( 87.47)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9825e+00 (1.5970e+00)	Acc@1  61.00 ( 65.64)	Acc@5  85.00 ( 87.61)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.5259e+00 (1.5921e+00)	Acc@1  67.00 ( 65.58)	Acc@5  88.00 ( 87.85)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.7742e+00 (1.5978e+00)	Acc@1  65.00 ( 65.35)	Acc@5  84.00 ( 87.77)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 2.2751e+00 (1.5857e+00)	Acc@1  57.00 ( 65.56)	Acc@5  82.00 ( 87.87)
 * Acc@1 65.670 Acc@5 88.060
### epoch[57] execution time: 14.945746421813965
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.170 ( 0.170)	Data  0.144 ( 0.144)	Loss 1.8213e-01 (1.8213e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.22 ( 99.22)
Epoch: [58][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.014)	Loss 2.4879e-01 (2.8334e-01)	Acc@1  92.97 ( 91.48)	Acc@5 100.00 ( 99.01)
Epoch: [58][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.6653e-01 (2.8760e-01)	Acc@1  92.19 ( 91.07)	Acc@5 100.00 ( 99.11)
Epoch: [58][ 30/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.006)	Loss 3.4563e-01 (2.9463e-01)	Acc@1  89.06 ( 90.62)	Acc@5 100.00 ( 99.14)
Epoch: [58][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.8401e-01 (2.8545e-01)	Acc@1  91.41 ( 91.03)	Acc@5  99.22 ( 99.14)
Epoch: [58][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.8461e-01 (2.8779e-01)	Acc@1  91.41 ( 91.12)	Acc@5  99.22 ( 99.17)
Epoch: [58][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.4476e-01 (2.8507e-01)	Acc@1  92.19 ( 91.16)	Acc@5 100.00 ( 99.22)
Epoch: [58][ 70/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1471e-01 (2.8741e-01)	Acc@1  91.41 ( 90.92)	Acc@5 100.00 ( 99.24)
Epoch: [58][ 80/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.004)	Loss 4.2938e-01 (2.8768e-01)	Acc@1  87.50 ( 90.93)	Acc@5  97.66 ( 99.21)
Epoch: [58][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.3011e-01 (2.8769e-01)	Acc@1  92.19 ( 90.97)	Acc@5  98.44 ( 99.23)
Epoch: [58][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2035e-01 (2.8867e-01)	Acc@1  91.41 ( 90.85)	Acc@5 100.00 ( 99.23)
Epoch: [58][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.9135e-01 (2.9021e-01)	Acc@1  89.06 ( 90.86)	Acc@5  98.44 ( 99.21)
Epoch: [58][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1937e-01 (2.9049e-01)	Acc@1  89.06 ( 90.86)	Acc@5 100.00 ( 99.23)
Epoch: [58][130/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7607e-01 (2.8978e-01)	Acc@1  88.28 ( 90.79)	Acc@5  99.22 ( 99.24)
Epoch: [58][140/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4791e-01 (2.8922e-01)	Acc@1  88.28 ( 90.76)	Acc@5  98.44 ( 99.22)
Epoch: [58][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7505e-01 (2.9296e-01)	Acc@1  89.06 ( 90.66)	Acc@5  99.22 ( 99.18)
Epoch: [58][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8337e-01 (2.9656e-01)	Acc@1  84.38 ( 90.52)	Acc@5 100.00 ( 99.14)
Epoch: [58][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4694e-01 (2.9705e-01)	Acc@1  85.16 ( 90.47)	Acc@5 100.00 ( 99.16)
Epoch: [58][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.2518e-01 (2.9649e-01)	Acc@1  90.62 ( 90.51)	Acc@5  98.44 ( 99.15)
Epoch: [58][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.6579e-01 (2.9769e-01)	Acc@1  86.72 ( 90.47)	Acc@5  97.66 ( 99.12)
Epoch: [58][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4003e-01 (2.9870e-01)	Acc@1  89.06 ( 90.42)	Acc@5  99.22 ( 99.14)
Epoch: [58][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6139e-01 (2.9802e-01)	Acc@1  88.28 ( 90.48)	Acc@5  99.22 ( 99.15)
Epoch: [58][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1995e-01 (2.9896e-01)	Acc@1  89.84 ( 90.46)	Acc@5  99.22 ( 99.14)
Epoch: [58][230/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5996e-01 (2.9920e-01)	Acc@1  88.28 ( 90.45)	Acc@5  99.22 ( 99.14)
Epoch: [58][240/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 3.1998e-01 (3.0215e-01)	Acc@1  92.97 ( 90.38)	Acc@5  98.44 ( 99.12)
Epoch: [58][250/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.0048e-01 (3.0382e-01)	Acc@1  84.38 ( 90.32)	Acc@5  97.66 ( 99.11)
Epoch: [58][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9627e-01 (3.0505e-01)	Acc@1  88.28 ( 90.27)	Acc@5  99.22 ( 99.09)
Epoch: [58][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5948e-01 (3.0553e-01)	Acc@1  92.97 ( 90.26)	Acc@5 100.00 ( 99.09)
Epoch: [58][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8664e-01 (3.0469e-01)	Acc@1  91.41 ( 90.29)	Acc@5  98.44 ( 99.10)
Epoch: [58][290/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8981e-01 (3.0549e-01)	Acc@1  87.50 ( 90.25)	Acc@5  99.22 ( 99.09)
Epoch: [58][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 4.8223e-01 (3.0601e-01)	Acc@1  89.84 ( 90.26)	Acc@5  98.44 ( 99.10)
Epoch: [58][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3184e-01 (3.0636e-01)	Acc@1  88.28 ( 90.25)	Acc@5  98.44 ( 99.09)
Epoch: [58][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.6072e-01 (3.0532e-01)	Acc@1  88.28 ( 90.29)	Acc@5  99.22 ( 99.10)
Epoch: [58][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.1840e-01 (3.0679e-01)	Acc@1  87.50 ( 90.26)	Acc@5  98.44 ( 99.08)
Epoch: [58][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.6128e-01 (3.0621e-01)	Acc@1  88.28 ( 90.29)	Acc@5 100.00 ( 99.09)
Epoch: [58][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.8039e-01 (3.0711e-01)	Acc@1  87.50 ( 90.26)	Acc@5 100.00 ( 99.09)
Epoch: [58][360/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.1484e-01 (3.0621e-01)	Acc@1  92.97 ( 90.29)	Acc@5 100.00 ( 99.10)
Epoch: [58][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2316e-01 (3.0642e-01)	Acc@1  92.19 ( 90.30)	Acc@5  99.22 ( 99.09)
Epoch: [58][380/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0790e-01 (3.0579e-01)	Acc@1  92.19 ( 90.30)	Acc@5 100.00 ( 99.11)
Epoch: [58][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.7969e-01 (3.0612e-01)	Acc@1  91.25 ( 90.27)	Acc@5 100.00 ( 99.11)
## e[58] optimizer.zero_grad (sum) time: 0.11572861671447754
## e[58]       loss.backward (sum) time: 2.1022400856018066
## e[58]      optimizer.step (sum) time: 0.7215249538421631
## epoch[58] training(only) time: 12.956472158432007
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.5572e+00 (1.5572e+00)	Acc@1  70.00 ( 70.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.9045e+00 (1.6490e+00)	Acc@1  67.00 ( 66.27)	Acc@5  89.00 ( 88.00)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7804e+00 (1.6086e+00)	Acc@1  61.00 ( 65.81)	Acc@5  88.00 ( 88.14)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8720e+00 (1.6207e+00)	Acc@1  60.00 ( 65.26)	Acc@5  83.00 ( 87.87)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3825e+00 (1.6033e+00)	Acc@1  65.00 ( 65.39)	Acc@5  88.00 ( 88.20)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4173e+00 (1.6121e+00)	Acc@1  64.00 ( 65.45)	Acc@5  89.00 ( 88.08)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.0110e+00 (1.5955e+00)	Acc@1  60.00 ( 66.00)	Acc@5  86.00 ( 88.20)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7356e+00 (1.5984e+00)	Acc@1  64.00 ( 65.75)	Acc@5  88.00 ( 88.23)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6606e+00 (1.6049e+00)	Acc@1  60.00 ( 65.69)	Acc@5  87.00 ( 88.11)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.1536e+00 (1.5902e+00)	Acc@1  53.00 ( 65.88)	Acc@5  86.00 ( 88.30)
 * Acc@1 65.880 Acc@5 88.290
### epoch[58] execution time: 14.928092002868652
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.178 ( 0.178)	Data  0.151 ( 0.151)	Loss 1.7749e-01 (1.7749e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.031 ( 0.045)	Data  0.001 ( 0.015)	Loss 2.6849e-01 (2.5603e-01)	Acc@1  91.41 ( 92.19)	Acc@5  99.22 ( 99.36)
Epoch: [59][ 20/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.009)	Loss 3.0863e-01 (2.6911e-01)	Acc@1  90.62 ( 91.56)	Acc@5  99.22 ( 99.44)
Epoch: [59][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.8250e-01 (2.6886e-01)	Acc@1  89.84 ( 91.38)	Acc@5 100.00 ( 99.42)
Epoch: [59][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.0484e-01 (2.6656e-01)	Acc@1  92.19 ( 91.37)	Acc@5 100.00 ( 99.45)
Epoch: [59][ 50/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.005)	Loss 3.0341e-01 (2.7589e-01)	Acc@1  92.19 ( 91.22)	Acc@5  98.44 ( 99.36)
Epoch: [59][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.1360e-01 (2.7334e-01)	Acc@1  92.19 ( 91.29)	Acc@5 100.00 ( 99.36)
Epoch: [59][ 70/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.4860e-01 (2.7495e-01)	Acc@1  89.84 ( 91.22)	Acc@5 100.00 ( 99.28)
Epoch: [59][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.7684e-01 (2.7107e-01)	Acc@1  90.62 ( 91.41)	Acc@5  98.44 ( 99.30)
Epoch: [59][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1310e-01 (2.7208e-01)	Acc@1  92.97 ( 91.41)	Acc@5 100.00 ( 99.26)
Epoch: [59][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.8501e-01 (2.7644e-01)	Acc@1  87.50 ( 91.21)	Acc@5  97.66 ( 99.23)
Epoch: [59][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.4293e-01 (2.7687e-01)	Acc@1  89.06 ( 91.18)	Acc@5  99.22 ( 99.23)
Epoch: [59][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.3874e-01 (2.7349e-01)	Acc@1  88.28 ( 91.24)	Acc@5 100.00 ( 99.28)
Epoch: [59][130/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.003)	Loss 3.0521e-01 (2.7664e-01)	Acc@1  89.06 ( 91.15)	Acc@5 100.00 ( 99.24)
Epoch: [59][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1158e-01 (2.7940e-01)	Acc@1  95.31 ( 91.05)	Acc@5  99.22 ( 99.26)
Epoch: [59][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2976e-01 (2.8247e-01)	Acc@1  92.19 ( 90.94)	Acc@5  99.22 ( 99.21)
Epoch: [59][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5839e-01 (2.8136e-01)	Acc@1  92.19 ( 91.00)	Acc@5  98.44 ( 99.23)
Epoch: [59][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1172e-01 (2.8145e-01)	Acc@1  86.72 ( 90.97)	Acc@5  99.22 ( 99.22)
Epoch: [59][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3947e-01 (2.8293e-01)	Acc@1  90.62 ( 90.94)	Acc@5 100.00 ( 99.24)
Epoch: [59][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5027e-01 (2.8225e-01)	Acc@1  92.97 ( 90.94)	Acc@5 100.00 ( 99.25)
Epoch: [59][200/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.8119e-01 (2.8333e-01)	Acc@1  85.16 ( 90.93)	Acc@5  98.44 ( 99.23)
Epoch: [59][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6785e-01 (2.8396e-01)	Acc@1  87.50 ( 90.94)	Acc@5 100.00 ( 99.25)
Epoch: [59][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6508e-01 (2.8314e-01)	Acc@1  89.84 ( 90.96)	Acc@5 100.00 ( 99.26)
Epoch: [59][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4011e-01 (2.8230e-01)	Acc@1  96.88 ( 90.98)	Acc@5 100.00 ( 99.26)
Epoch: [59][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5094e-01 (2.8305e-01)	Acc@1  92.19 ( 90.98)	Acc@5  99.22 ( 99.24)
Epoch: [59][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5858e-01 (2.8240e-01)	Acc@1  91.41 ( 91.00)	Acc@5  99.22 ( 99.22)
Epoch: [59][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9580e-01 (2.8432e-01)	Acc@1  87.50 ( 90.90)	Acc@5  96.88 ( 99.22)
Epoch: [59][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1975e-01 (2.8371e-01)	Acc@1  94.53 ( 90.93)	Acc@5 100.00 ( 99.21)
Epoch: [59][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3456e-01 (2.8384e-01)	Acc@1  92.19 ( 90.93)	Acc@5  99.22 ( 99.20)
Epoch: [59][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4654e-01 (2.8398e-01)	Acc@1  92.19 ( 90.93)	Acc@5  99.22 ( 99.21)
Epoch: [59][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4846e-01 (2.8470e-01)	Acc@1  92.19 ( 90.93)	Acc@5  98.44 ( 99.21)
Epoch: [59][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0375e-01 (2.8621e-01)	Acc@1  90.62 ( 90.92)	Acc@5  99.22 ( 99.20)
Epoch: [59][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6321e-01 (2.8621e-01)	Acc@1  90.62 ( 90.91)	Acc@5 100.00 ( 99.19)
Epoch: [59][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4595e-01 (2.8672e-01)	Acc@1  89.84 ( 90.90)	Acc@5 100.00 ( 99.20)
Epoch: [59][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.9756e-01 (2.8665e-01)	Acc@1  89.06 ( 90.89)	Acc@5 100.00 ( 99.20)
Epoch: [59][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0367e-01 (2.8641e-01)	Acc@1  92.97 ( 90.89)	Acc@5 100.00 ( 99.21)
Epoch: [59][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.3460e-01 (2.8768e-01)	Acc@1  92.19 ( 90.85)	Acc@5 100.00 ( 99.21)
Epoch: [59][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9890e-01 (2.8850e-01)	Acc@1  92.19 ( 90.80)	Acc@5 100.00 ( 99.20)
Epoch: [59][380/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 3.4532e-01 (2.9006e-01)	Acc@1  89.84 ( 90.75)	Acc@5  98.44 ( 99.19)
Epoch: [59][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.5570e-01 (2.9013e-01)	Acc@1  86.25 ( 90.75)	Acc@5 100.00 ( 99.19)
## e[59] optimizer.zero_grad (sum) time: 0.11571002006530762
## e[59]       loss.backward (sum) time: 2.1030690670013428
## e[59]      optimizer.step (sum) time: 0.7358484268188477
## epoch[59] training(only) time: 12.979130268096924
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.5704e+00 (1.5704e+00)	Acc@1  66.00 ( 66.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0761e+00 (1.7195e+00)	Acc@1  66.00 ( 65.00)	Acc@5  91.00 ( 87.27)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7479e+00 (1.6847e+00)	Acc@1  63.00 ( 65.76)	Acc@5  84.00 ( 87.33)
Test: [ 30/100]	Time  0.018 ( 0.021)	Loss 1.8381e+00 (1.6740e+00)	Acc@1  61.00 ( 65.29)	Acc@5  85.00 ( 87.32)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3423e+00 (1.6627e+00)	Acc@1  67.00 ( 65.44)	Acc@5  91.00 ( 87.49)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4152e+00 (1.6675e+00)	Acc@1  66.00 ( 65.43)	Acc@5  90.00 ( 87.49)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0757e+00 (1.6463e+00)	Acc@1  65.00 ( 65.89)	Acc@5  85.00 ( 87.72)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6551e+00 (1.6430e+00)	Acc@1  64.00 ( 65.62)	Acc@5  89.00 ( 88.06)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5883e+00 (1.6409e+00)	Acc@1  68.00 ( 65.65)	Acc@5  88.00 ( 87.98)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.4289e+00 (1.6315e+00)	Acc@1  54.00 ( 65.68)	Acc@5  86.00 ( 88.14)
 * Acc@1 65.710 Acc@5 88.210
### epoch[59] execution time: 14.912304878234863
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.193 ( 0.193)	Data  0.165 ( 0.165)	Loss 4.1766e-01 (4.1766e-01)	Acc@1  88.28 ( 88.28)	Acc@5  98.44 ( 98.44)
Epoch: [60][ 10/391]	Time  0.031 ( 0.047)	Data  0.001 ( 0.017)	Loss 2.5450e-01 (3.0888e-01)	Acc@1  92.19 ( 89.84)	Acc@5  98.44 ( 99.01)
Epoch: [60][ 20/391]	Time  0.031 ( 0.040)	Data  0.001 ( 0.010)	Loss 2.7541e-01 (2.9061e-01)	Acc@1  90.62 ( 90.81)	Acc@5  97.66 ( 99.11)
Epoch: [60][ 30/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.007)	Loss 2.0827e-01 (2.6705e-01)	Acc@1  91.41 ( 91.36)	Acc@5 100.00 ( 99.22)
Epoch: [60][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.8249e-01 (2.5967e-01)	Acc@1  93.75 ( 91.67)	Acc@5 100.00 ( 99.33)
Epoch: [60][ 50/391]	Time  0.032 ( 0.036)	Data  0.002 ( 0.005)	Loss 2.8902e-01 (2.6322e-01)	Acc@1  91.41 ( 91.61)	Acc@5  99.22 ( 99.36)
Epoch: [60][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.6116e-01 (2.6242e-01)	Acc@1  91.41 ( 91.68)	Acc@5  99.22 ( 99.31)
Epoch: [60][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.2277e-01 (2.6363e-01)	Acc@1  87.50 ( 91.62)	Acc@5  99.22 ( 99.30)
Epoch: [60][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5836e-01 (2.6229e-01)	Acc@1  94.53 ( 91.60)	Acc@5 100.00 ( 99.26)
Epoch: [60][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8911e-01 (2.6357e-01)	Acc@1  94.53 ( 91.56)	Acc@5  99.22 ( 99.26)
Epoch: [60][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.3657e-01 (2.6040e-01)	Acc@1  92.19 ( 91.63)	Acc@5  99.22 ( 99.25)
Epoch: [60][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8082e-01 (2.5918e-01)	Acc@1  91.41 ( 91.62)	Acc@5  98.44 ( 99.26)
Epoch: [60][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8033e-01 (2.5825e-01)	Acc@1  91.41 ( 91.69)	Acc@5  98.44 ( 99.23)
Epoch: [60][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8376e-01 (2.5561e-01)	Acc@1  91.41 ( 91.81)	Acc@5  98.44 ( 99.23)
Epoch: [60][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4692e-01 (2.5480e-01)	Acc@1  91.41 ( 91.85)	Acc@5 100.00 ( 99.25)
Epoch: [60][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.9340e-01 (2.5585e-01)	Acc@1  91.41 ( 91.87)	Acc@5 100.00 ( 99.25)
Epoch: [60][160/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5873e-01 (2.5317e-01)	Acc@1  91.41 ( 91.92)	Acc@5 100.00 ( 99.29)
Epoch: [60][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1477e-01 (2.5342e-01)	Acc@1  92.19 ( 91.89)	Acc@5 100.00 ( 99.30)
Epoch: [60][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0287e-01 (2.5398e-01)	Acc@1  90.62 ( 91.89)	Acc@5  99.22 ( 99.31)
Epoch: [60][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1577e-01 (2.5212e-01)	Acc@1  96.09 ( 91.95)	Acc@5 100.00 ( 99.33)
Epoch: [60][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9782e-01 (2.5115e-01)	Acc@1  93.75 ( 91.96)	Acc@5  99.22 ( 99.33)
Epoch: [60][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9902e-01 (2.5094e-01)	Acc@1  93.75 ( 91.97)	Acc@5 100.00 ( 99.33)
Epoch: [60][220/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1107e-01 (2.5041e-01)	Acc@1  91.41 ( 91.97)	Acc@5 100.00 ( 99.32)
Epoch: [60][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4888e-01 (2.5026e-01)	Acc@1  91.41 ( 91.97)	Acc@5  99.22 ( 99.31)
Epoch: [60][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0387e-01 (2.5000e-01)	Acc@1  95.31 ( 92.00)	Acc@5  99.22 ( 99.31)
Epoch: [60][250/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1299e-01 (2.4939e-01)	Acc@1  93.75 ( 92.02)	Acc@5  99.22 ( 99.32)
Epoch: [60][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7131e-01 (2.4892e-01)	Acc@1  95.31 ( 92.06)	Acc@5 100.00 ( 99.33)
Epoch: [60][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2925e-01 (2.4808e-01)	Acc@1  96.88 ( 92.10)	Acc@5 100.00 ( 99.33)
Epoch: [60][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0160e-01 (2.4774e-01)	Acc@1  92.19 ( 92.10)	Acc@5 100.00 ( 99.33)
Epoch: [60][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9019e-01 (2.4805e-01)	Acc@1  92.19 ( 92.12)	Acc@5  99.22 ( 99.32)
Epoch: [60][300/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.3022e-01 (2.4771e-01)	Acc@1  92.97 ( 92.13)	Acc@5  99.22 ( 99.32)
Epoch: [60][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8127e-01 (2.4814e-01)	Acc@1  94.53 ( 92.11)	Acc@5 100.00 ( 99.32)
Epoch: [60][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6066e-01 (2.4774e-01)	Acc@1  92.19 ( 92.12)	Acc@5  99.22 ( 99.32)
Epoch: [60][330/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8563e-01 (2.4680e-01)	Acc@1  91.41 ( 92.13)	Acc@5  98.44 ( 99.32)
Epoch: [60][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7113e-01 (2.4600e-01)	Acc@1  96.09 ( 92.16)	Acc@5 100.00 ( 99.32)
Epoch: [60][350/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.7454e-01 (2.4409e-01)	Acc@1  94.53 ( 92.25)	Acc@5 100.00 ( 99.33)
Epoch: [60][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9900e-01 (2.4341e-01)	Acc@1  94.53 ( 92.26)	Acc@5  99.22 ( 99.34)
Epoch: [60][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1638e-01 (2.4262e-01)	Acc@1  92.19 ( 92.28)	Acc@5 100.00 ( 99.35)
Epoch: [60][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2522e-01 (2.4256e-01)	Acc@1  94.53 ( 92.30)	Acc@5  98.44 ( 99.34)
Epoch: [60][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5858e-01 (2.4217e-01)	Acc@1  95.00 ( 92.30)	Acc@5 100.00 ( 99.35)
## e[60] optimizer.zero_grad (sum) time: 0.11629319190979004
## e[60]       loss.backward (sum) time: 2.0766613483428955
## e[60]      optimizer.step (sum) time: 0.7364063262939453
## epoch[60] training(only) time: 13.002925157546997
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.5644e+00 (1.5644e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0079e+00 (1.6886e+00)	Acc@1  63.00 ( 66.82)	Acc@5  91.00 ( 87.82)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7154e+00 (1.6418e+00)	Acc@1  63.00 ( 66.81)	Acc@5  87.00 ( 88.00)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7625e+00 (1.6300e+00)	Acc@1  59.00 ( 66.26)	Acc@5  86.00 ( 88.13)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.2735e+00 (1.6104e+00)	Acc@1  73.00 ( 66.73)	Acc@5  92.00 ( 88.51)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4596e+00 (1.6205e+00)	Acc@1  68.00 ( 66.63)	Acc@5  89.00 ( 88.37)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0261e+00 (1.5996e+00)	Acc@1  64.00 ( 67.02)	Acc@5  86.00 ( 88.49)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6155e+00 (1.5962e+00)	Acc@1  65.00 ( 66.79)	Acc@5  86.00 ( 88.63)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5116e+00 (1.5932e+00)	Acc@1  68.00 ( 66.77)	Acc@5  88.00 ( 88.48)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2423e+00 (1.5813e+00)	Acc@1  56.00 ( 66.85)	Acc@5  86.00 ( 88.62)
 * Acc@1 66.840 Acc@5 88.750
### epoch[60] execution time: 14.94442081451416
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.183 ( 0.183)	Data  0.148 ( 0.148)	Loss 1.8813e-01 (1.8813e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.031 ( 0.046)	Data  0.001 ( 0.015)	Loss 2.7747e-01 (2.3797e-01)	Acc@1  89.84 ( 92.68)	Acc@5  99.22 ( 99.36)
Epoch: [61][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 2.1054e-01 (2.2991e-01)	Acc@1  92.97 ( 92.86)	Acc@5 100.00 ( 99.37)
Epoch: [61][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.0928e-01 (2.2693e-01)	Acc@1  99.22 ( 92.84)	Acc@5 100.00 ( 99.34)
Epoch: [61][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.7776e-01 (2.2102e-01)	Acc@1  94.53 ( 93.08)	Acc@5 100.00 ( 99.37)
Epoch: [61][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.0513e-01 (2.2149e-01)	Acc@1  93.75 ( 93.08)	Acc@5 100.00 ( 99.39)
Epoch: [61][ 60/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.5908e-01 (2.2095e-01)	Acc@1  94.53 ( 93.11)	Acc@5 100.00 ( 99.41)
Epoch: [61][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.6950e-01 (2.2360e-01)	Acc@1  90.62 ( 92.99)	Acc@5  99.22 ( 99.38)
Epoch: [61][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.6482e-01 (2.2905e-01)	Acc@1  89.84 ( 92.75)	Acc@5 100.00 ( 99.34)
Epoch: [61][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8379e-01 (2.2880e-01)	Acc@1  94.53 ( 92.83)	Acc@5 100.00 ( 99.37)
Epoch: [61][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9729e-01 (2.3039e-01)	Acc@1  93.75 ( 92.70)	Acc@5 100.00 ( 99.39)
Epoch: [61][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1220e-01 (2.2876e-01)	Acc@1  91.41 ( 92.74)	Acc@5  99.22 ( 99.42)
Epoch: [61][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2301e-01 (2.2943e-01)	Acc@1  92.97 ( 92.73)	Acc@5 100.00 ( 99.41)
Epoch: [61][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9355e-01 (2.2806e-01)	Acc@1  92.97 ( 92.74)	Acc@5  99.22 ( 99.42)
Epoch: [61][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5343e-01 (2.2773e-01)	Acc@1  94.53 ( 92.77)	Acc@5  99.22 ( 99.42)
Epoch: [61][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3213e-01 (2.2556e-01)	Acc@1  96.09 ( 92.82)	Acc@5 100.00 ( 99.45)
Epoch: [61][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3366e-01 (2.2402e-01)	Acc@1  96.88 ( 92.87)	Acc@5 100.00 ( 99.46)
Epoch: [61][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8359e-01 (2.2587e-01)	Acc@1  94.53 ( 92.85)	Acc@5 100.00 ( 99.47)
Epoch: [61][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3388e-01 (2.2554e-01)	Acc@1  96.09 ( 92.87)	Acc@5 100.00 ( 99.48)
Epoch: [61][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4288e-01 (2.2581e-01)	Acc@1  94.53 ( 92.82)	Acc@5 100.00 ( 99.48)
Epoch: [61][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0481e-01 (2.2568e-01)	Acc@1  93.75 ( 92.83)	Acc@5  99.22 ( 99.47)
Epoch: [61][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5278e-01 (2.2575e-01)	Acc@1  93.75 ( 92.84)	Acc@5  99.22 ( 99.47)
Epoch: [61][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9586e-01 (2.2566e-01)	Acc@1  90.62 ( 92.82)	Acc@5  99.22 ( 99.46)
Epoch: [61][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8754e-01 (2.2562e-01)	Acc@1  89.84 ( 92.83)	Acc@5 100.00 ( 99.46)
Epoch: [61][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7087e-01 (2.2607e-01)	Acc@1  95.31 ( 92.81)	Acc@5 100.00 ( 99.46)
Epoch: [61][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6265e-01 (2.2738e-01)	Acc@1  96.09 ( 92.77)	Acc@5 100.00 ( 99.45)
Epoch: [61][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9997e-01 (2.2691e-01)	Acc@1  94.53 ( 92.78)	Acc@5 100.00 ( 99.46)
Epoch: [61][270/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.5347e-01 (2.2749e-01)	Acc@1  91.41 ( 92.76)	Acc@5  99.22 ( 99.46)
Epoch: [61][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7684e-01 (2.2601e-01)	Acc@1  90.62 ( 92.79)	Acc@5 100.00 ( 99.47)
Epoch: [61][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4075e-01 (2.2677e-01)	Acc@1  89.84 ( 92.78)	Acc@5  99.22 ( 99.46)
Epoch: [61][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4887e-01 (2.2676e-01)	Acc@1  96.88 ( 92.79)	Acc@5  99.22 ( 99.47)
Epoch: [61][310/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2050e-01 (2.2608e-01)	Acc@1  92.19 ( 92.83)	Acc@5 100.00 ( 99.47)
Epoch: [61][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1901e-01 (2.2658e-01)	Acc@1  90.62 ( 92.81)	Acc@5 100.00 ( 99.46)
Epoch: [61][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6338e-01 (2.2677e-01)	Acc@1  90.62 ( 92.80)	Acc@5  98.44 ( 99.47)
Epoch: [61][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4740e-01 (2.2681e-01)	Acc@1  90.62 ( 92.80)	Acc@5 100.00 ( 99.47)
Epoch: [61][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3124e-01 (2.2707e-01)	Acc@1  91.41 ( 92.78)	Acc@5  99.22 ( 99.47)
Epoch: [61][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.0325e-01 (2.2756e-01)	Acc@1  90.62 ( 92.78)	Acc@5  99.22 ( 99.47)
Epoch: [61][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4759e-01 (2.2726e-01)	Acc@1  92.97 ( 92.79)	Acc@5  98.44 ( 99.48)
Epoch: [61][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6523e-01 (2.2749e-01)	Acc@1  91.41 ( 92.77)	Acc@5  98.44 ( 99.48)
Epoch: [61][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9476e-01 (2.2764e-01)	Acc@1  95.00 ( 92.77)	Acc@5  98.75 ( 99.47)
## e[61] optimizer.zero_grad (sum) time: 0.11733174324035645
## e[61]       loss.backward (sum) time: 2.0977301597595215
## e[61]      optimizer.step (sum) time: 0.7291085720062256
## epoch[61] training(only) time: 13.030332088470459
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.5297e+00 (1.5297e+00)	Acc@1  67.00 ( 67.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0140e+00 (1.6806e+00)	Acc@1  64.00 ( 66.73)	Acc@5  90.00 ( 87.64)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7059e+00 (1.6430e+00)	Acc@1  65.00 ( 66.33)	Acc@5  87.00 ( 88.14)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7602e+00 (1.6344e+00)	Acc@1  62.00 ( 65.68)	Acc@5  84.00 ( 88.10)
Test: [ 40/100]	Time  0.018 ( 0.020)	Loss 1.2604e+00 (1.6147e+00)	Acc@1  71.00 ( 66.17)	Acc@5  92.00 ( 88.46)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4976e+00 (1.6273e+00)	Acc@1  68.00 ( 66.20)	Acc@5  89.00 ( 88.25)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9498e+00 (1.6058e+00)	Acc@1  65.00 ( 66.66)	Acc@5  85.00 ( 88.34)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6708e+00 (1.6030e+00)	Acc@1  66.00 ( 66.48)	Acc@5  87.00 ( 88.58)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5281e+00 (1.6008e+00)	Acc@1  70.00 ( 66.54)	Acc@5  87.00 ( 88.53)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 2.2154e+00 (1.5873e+00)	Acc@1  59.00 ( 66.60)	Acc@5  86.00 ( 88.66)
 * Acc@1 66.630 Acc@5 88.740
### epoch[61] execution time: 14.954727411270142
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.183 ( 0.183)	Data  0.155 ( 0.155)	Loss 2.2764e-01 (2.2764e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [62][ 10/391]	Time  0.031 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.9766e-01 (2.2011e-01)	Acc@1  94.53 ( 92.47)	Acc@5 100.00 ( 99.43)
Epoch: [62][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 2.9975e-01 (2.2842e-01)	Acc@1  91.41 ( 92.75)	Acc@5  99.22 ( 99.44)
Epoch: [62][ 30/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.007)	Loss 1.5312e-01 (2.2926e-01)	Acc@1  96.09 ( 92.59)	Acc@5  99.22 ( 99.47)
Epoch: [62][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 9.6805e-02 (2.3493e-01)	Acc@1  96.09 ( 92.57)	Acc@5 100.00 ( 99.37)
Epoch: [62][ 50/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.4055e-01 (2.3366e-01)	Acc@1  94.53 ( 92.49)	Acc@5 100.00 ( 99.46)
Epoch: [62][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 4.0804e-01 (2.3568e-01)	Acc@1  85.94 ( 92.41)	Acc@5  97.66 ( 99.46)
Epoch: [62][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.0266e-01 (2.3469e-01)	Acc@1  95.31 ( 92.46)	Acc@5 100.00 ( 99.45)
Epoch: [62][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.3604e-01 (2.3074e-01)	Acc@1  92.97 ( 92.60)	Acc@5 100.00 ( 99.45)
Epoch: [62][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5444e-01 (2.2673e-01)	Acc@1  96.09 ( 92.80)	Acc@5 100.00 ( 99.46)
Epoch: [62][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.9129e-01 (2.2569e-01)	Acc@1  95.31 ( 92.88)	Acc@5  99.22 ( 99.46)
Epoch: [62][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4237e-01 (2.2241e-01)	Acc@1  96.09 ( 92.95)	Acc@5 100.00 ( 99.49)
Epoch: [62][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7309e-01 (2.2391e-01)	Acc@1  91.41 ( 92.83)	Acc@5 100.00 ( 99.48)
Epoch: [62][130/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.3371e-01 (2.2251e-01)	Acc@1  92.19 ( 92.89)	Acc@5 100.00 ( 99.47)
Epoch: [62][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8105e-01 (2.2233e-01)	Acc@1  93.75 ( 92.89)	Acc@5  99.22 ( 99.48)
Epoch: [62][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5203e-01 (2.2142e-01)	Acc@1  94.53 ( 92.90)	Acc@5 100.00 ( 99.47)
Epoch: [62][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9590e-01 (2.1991e-01)	Acc@1  94.53 ( 92.95)	Acc@5  99.22 ( 99.46)
Epoch: [62][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4729e-01 (2.1943e-01)	Acc@1  91.41 ( 92.96)	Acc@5 100.00 ( 99.47)
Epoch: [62][180/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0025e-01 (2.1929e-01)	Acc@1  91.41 ( 93.00)	Acc@5  98.44 ( 99.47)
Epoch: [62][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8409e-01 (2.1921e-01)	Acc@1  93.75 ( 93.03)	Acc@5 100.00 ( 99.47)
Epoch: [62][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6426e-01 (2.1696e-01)	Acc@1  93.75 ( 93.10)	Acc@5 100.00 ( 99.49)
Epoch: [62][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4974e-01 (2.1587e-01)	Acc@1  95.31 ( 93.15)	Acc@5 100.00 ( 99.50)
Epoch: [62][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7249e-01 (2.1547e-01)	Acc@1  93.75 ( 93.16)	Acc@5 100.00 ( 99.52)
Epoch: [62][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6956e-01 (2.1643e-01)	Acc@1  96.09 ( 93.15)	Acc@5 100.00 ( 99.51)
Epoch: [62][240/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8802e-01 (2.1649e-01)	Acc@1  91.41 ( 93.16)	Acc@5 100.00 ( 99.51)
Epoch: [62][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0034e-01 (2.1551e-01)	Acc@1  93.75 ( 93.18)	Acc@5 100.00 ( 99.52)
Epoch: [62][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0079e-01 (2.1657e-01)	Acc@1  91.41 ( 93.14)	Acc@5  98.44 ( 99.52)
Epoch: [62][270/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6781e-01 (2.1792e-01)	Acc@1  89.06 ( 93.10)	Acc@5 100.00 ( 99.52)
Epoch: [62][280/391]	Time  0.034 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.5154e-01 (2.1738e-01)	Acc@1  96.88 ( 93.14)	Acc@5 100.00 ( 99.52)
Epoch: [62][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8371e-01 (2.1671e-01)	Acc@1  90.62 ( 93.16)	Acc@5  97.66 ( 99.52)
Epoch: [62][300/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.4870e-01 (2.1636e-01)	Acc@1  92.19 ( 93.18)	Acc@5  98.44 ( 99.51)
Epoch: [62][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4699e-01 (2.1538e-01)	Acc@1  94.53 ( 93.20)	Acc@5 100.00 ( 99.52)
Epoch: [62][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7152e-01 (2.1604e-01)	Acc@1  96.88 ( 93.18)	Acc@5 100.00 ( 99.52)
Epoch: [62][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7306e-01 (2.1551e-01)	Acc@1  94.53 ( 93.20)	Acc@5 100.00 ( 99.52)
Epoch: [62][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1975e-01 (2.1551e-01)	Acc@1  91.41 ( 93.20)	Acc@5 100.00 ( 99.53)
Epoch: [62][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8873e-01 (2.1552e-01)	Acc@1  89.84 ( 93.22)	Acc@5  99.22 ( 99.52)
Epoch: [62][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0968e-01 (2.1564e-01)	Acc@1  94.53 ( 93.21)	Acc@5 100.00 ( 99.53)
Epoch: [62][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8198e-01 (2.1463e-01)	Acc@1  95.31 ( 93.24)	Acc@5  99.22 ( 99.53)
Epoch: [62][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.7334e-01 (2.1573e-01)	Acc@1  90.62 ( 93.19)	Acc@5  99.22 ( 99.53)
Epoch: [62][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9221e-01 (2.1548e-01)	Acc@1  93.75 ( 93.18)	Acc@5 100.00 ( 99.54)
## e[62] optimizer.zero_grad (sum) time: 0.11665725708007812
## e[62]       loss.backward (sum) time: 2.1159160137176514
## e[62]      optimizer.step (sum) time: 0.7359259128570557
## epoch[62] training(only) time: 12.986733436584473
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.5343e+00 (1.5343e+00)	Acc@1  70.00 ( 70.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0170e+00 (1.7007e+00)	Acc@1  64.00 ( 66.73)	Acc@5  91.00 ( 87.73)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.6303e+00 (1.6390e+00)	Acc@1  64.00 ( 66.48)	Acc@5  87.00 ( 88.29)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7830e+00 (1.6302e+00)	Acc@1  63.00 ( 66.29)	Acc@5  85.00 ( 88.26)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3060e+00 (1.6156e+00)	Acc@1  69.00 ( 66.46)	Acc@5  91.00 ( 88.59)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5868e+00 (1.6310e+00)	Acc@1  67.00 ( 66.45)	Acc@5  89.00 ( 88.47)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9840e+00 (1.6093e+00)	Acc@1  65.00 ( 66.82)	Acc@5  87.00 ( 88.57)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6637e+00 (1.6062e+00)	Acc@1  65.00 ( 66.69)	Acc@5  87.00 ( 88.77)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5262e+00 (1.6065e+00)	Acc@1  73.00 ( 66.78)	Acc@5  87.00 ( 88.65)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2536e+00 (1.5939e+00)	Acc@1  58.00 ( 66.93)	Acc@5  85.00 ( 88.75)
 * Acc@1 67.010 Acc@5 88.900
### epoch[62] execution time: 14.928050518035889
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.180 ( 0.180)	Data  0.150 ( 0.150)	Loss 1.6994e-01 (1.6994e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.035 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.6803e-01 (2.0694e-01)	Acc@1  93.75 ( 93.54)	Acc@5 100.00 ( 99.57)
Epoch: [63][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.8096e-01 (2.0880e-01)	Acc@1  92.97 ( 93.30)	Acc@5 100.00 ( 99.55)
Epoch: [63][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.1231e-01 (2.1202e-01)	Acc@1  96.09 ( 93.32)	Acc@5 100.00 ( 99.45)
Epoch: [63][ 40/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.5173e-01 (2.0837e-01)	Acc@1  95.31 ( 93.43)	Acc@5  99.22 ( 99.52)
Epoch: [63][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.9873e-01 (2.0830e-01)	Acc@1  89.06 ( 93.29)	Acc@5  99.22 ( 99.53)
Epoch: [63][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.6142e-01 (2.0760e-01)	Acc@1  93.75 ( 93.29)	Acc@5  99.22 ( 99.53)
Epoch: [63][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.1020e-01 (2.0871e-01)	Acc@1  92.19 ( 93.16)	Acc@5 100.00 ( 99.54)
Epoch: [63][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 3.2628e-01 (2.0796e-01)	Acc@1  89.84 ( 93.10)	Acc@5  99.22 ( 99.56)
Epoch: [63][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.6368e-01 (2.0998e-01)	Acc@1  92.19 ( 93.01)	Acc@5  98.44 ( 99.57)
Epoch: [63][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2445e-01 (2.0993e-01)	Acc@1  92.97 ( 93.02)	Acc@5  99.22 ( 99.56)
Epoch: [63][110/391]	Time  0.034 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.0874e-01 (2.0978e-01)	Acc@1  95.31 ( 93.08)	Acc@5  98.44 ( 99.54)
Epoch: [63][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4881e-01 (2.1098e-01)	Acc@1  96.88 ( 93.14)	Acc@5 100.00 ( 99.53)
Epoch: [63][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3513e-01 (2.1013e-01)	Acc@1  89.06 ( 93.16)	Acc@5 100.00 ( 99.53)
Epoch: [63][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0535e-01 (2.1137e-01)	Acc@1  92.19 ( 93.12)	Acc@5 100.00 ( 99.51)
Epoch: [63][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7532e-01 (2.1183e-01)	Acc@1  94.53 ( 93.16)	Acc@5  99.22 ( 99.51)
Epoch: [63][160/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0545e-01 (2.1104e-01)	Acc@1  92.19 ( 93.21)	Acc@5 100.00 ( 99.51)
Epoch: [63][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5982e-01 (2.1333e-01)	Acc@1  95.31 ( 93.14)	Acc@5  99.22 ( 99.50)
Epoch: [63][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6235e-01 (2.1167e-01)	Acc@1  96.09 ( 93.18)	Acc@5  99.22 ( 99.50)
Epoch: [63][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9632e-01 (2.1373e-01)	Acc@1  92.19 ( 93.15)	Acc@5 100.00 ( 99.46)
Epoch: [63][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7475e-01 (2.1343e-01)	Acc@1  93.75 ( 93.17)	Acc@5 100.00 ( 99.46)
Epoch: [63][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8945e-01 (2.1296e-01)	Acc@1  95.31 ( 93.19)	Acc@5 100.00 ( 99.48)
Epoch: [63][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.7974e-02 (2.1323e-01)	Acc@1  97.66 ( 93.19)	Acc@5 100.00 ( 99.47)
Epoch: [63][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3467e-01 (2.1386e-01)	Acc@1  92.97 ( 93.16)	Acc@5 100.00 ( 99.47)
Epoch: [63][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4990e-01 (2.1366e-01)	Acc@1  90.62 ( 93.17)	Acc@5  98.44 ( 99.47)
Epoch: [63][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7079e-01 (2.1343e-01)	Acc@1  92.97 ( 93.16)	Acc@5  99.22 ( 99.48)
Epoch: [63][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9088e-01 (2.1293e-01)	Acc@1  90.62 ( 93.18)	Acc@5 100.00 ( 99.49)
Epoch: [63][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1408e-01 (2.1113e-01)	Acc@1  96.09 ( 93.24)	Acc@5  98.44 ( 99.49)
Epoch: [63][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6010e-01 (2.1010e-01)	Acc@1  88.28 ( 93.26)	Acc@5  99.22 ( 99.49)
Epoch: [63][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4584e-01 (2.1081e-01)	Acc@1  89.84 ( 93.22)	Acc@5  99.22 ( 99.48)
Epoch: [63][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5818e-01 (2.1045e-01)	Acc@1  94.53 ( 93.23)	Acc@5 100.00 ( 99.47)
Epoch: [63][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.1242e-01 (2.1194e-01)	Acc@1  89.84 ( 93.20)	Acc@5  99.22 ( 99.45)
Epoch: [63][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.1445e-01 (2.1160e-01)	Acc@1  89.06 ( 93.20)	Acc@5  99.22 ( 99.46)
Epoch: [63][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7017e-01 (2.1135e-01)	Acc@1  93.75 ( 93.20)	Acc@5  99.22 ( 99.46)
Epoch: [63][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7227e-01 (2.1153e-01)	Acc@1  94.53 ( 93.20)	Acc@5 100.00 ( 99.46)
Epoch: [63][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4173e-01 (2.1150e-01)	Acc@1  91.41 ( 93.21)	Acc@5  98.44 ( 99.45)
Epoch: [63][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4273e-01 (2.1139e-01)	Acc@1  93.75 ( 93.22)	Acc@5  98.44 ( 99.44)
Epoch: [63][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8806e-01 (2.1170e-01)	Acc@1  88.28 ( 93.20)	Acc@5  99.22 ( 99.45)
Epoch: [63][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.5116e-01 (2.1130e-01)	Acc@1  93.75 ( 93.22)	Acc@5  99.22 ( 99.45)
Epoch: [63][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2344e-01 (2.1097e-01)	Acc@1  90.00 ( 93.22)	Acc@5 100.00 ( 99.46)
## e[63] optimizer.zero_grad (sum) time: 0.11642289161682129
## e[63]       loss.backward (sum) time: 2.0897812843322754
## e[63]      optimizer.step (sum) time: 0.7287909984588623
## epoch[63] training(only) time: 13.026806354522705
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.5111e+00 (1.5111e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0040e+00 (1.7028e+00)	Acc@1  63.00 ( 66.45)	Acc@5  89.00 ( 87.09)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7027e+00 (1.6501e+00)	Acc@1  64.00 ( 66.52)	Acc@5  86.00 ( 87.71)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7258e+00 (1.6400e+00)	Acc@1  64.00 ( 66.19)	Acc@5  86.00 ( 88.00)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3122e+00 (1.6269e+00)	Acc@1  70.00 ( 66.39)	Acc@5  91.00 ( 88.29)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4470e+00 (1.6412e+00)	Acc@1  67.00 ( 66.39)	Acc@5  89.00 ( 88.25)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 1.9641e+00 (1.6206e+00)	Acc@1  66.00 ( 66.84)	Acc@5  87.00 ( 88.43)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 1.7191e+00 (1.6201e+00)	Acc@1  65.00 ( 66.58)	Acc@5  87.00 ( 88.58)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.4996e+00 (1.6169e+00)	Acc@1  70.00 ( 66.64)	Acc@5  89.00 ( 88.46)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2542e+00 (1.6029e+00)	Acc@1  58.00 ( 66.74)	Acc@5  85.00 ( 88.62)
 * Acc@1 66.760 Acc@5 88.650
### epoch[63] execution time: 14.958804845809937
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.181 ( 0.181)	Data  0.152 ( 0.152)	Loss 1.9351e-01 (1.9351e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.5989e-01 (1.9639e-01)	Acc@1  95.31 ( 93.54)	Acc@5  98.44 ( 99.57)
Epoch: [64][ 20/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.9649e-01 (1.8777e-01)	Acc@1  91.41 ( 93.82)	Acc@5 100.00 ( 99.70)
Epoch: [64][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.7202e-01 (2.0189e-01)	Acc@1  96.09 ( 93.45)	Acc@5  99.22 ( 99.62)
Epoch: [64][ 40/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.5701e-01 (2.0929e-01)	Acc@1  92.97 ( 93.25)	Acc@5  99.22 ( 99.56)
Epoch: [64][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.4653e-01 (2.0803e-01)	Acc@1  92.19 ( 93.34)	Acc@5 100.00 ( 99.59)
Epoch: [64][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2314e-01 (2.0811e-01)	Acc@1  96.88 ( 93.35)	Acc@5 100.00 ( 99.59)
Epoch: [64][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.5101e-01 (2.0876e-01)	Acc@1  94.53 ( 93.38)	Acc@5 100.00 ( 99.61)
Epoch: [64][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1643e-01 (2.0597e-01)	Acc@1  92.97 ( 93.43)	Acc@5 100.00 ( 99.63)
Epoch: [64][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8893e-01 (2.0437e-01)	Acc@1  96.09 ( 93.52)	Acc@5  99.22 ( 99.63)
Epoch: [64][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8253e-01 (2.0277e-01)	Acc@1  93.75 ( 93.55)	Acc@5  99.22 ( 99.64)
Epoch: [64][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9491e-01 (1.9986e-01)	Acc@1  94.53 ( 93.62)	Acc@5 100.00 ( 99.65)
Epoch: [64][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8947e-01 (2.0004e-01)	Acc@1  92.97 ( 93.58)	Acc@5 100.00 ( 99.64)
Epoch: [64][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9570e-01 (1.9929e-01)	Acc@1  92.97 ( 93.59)	Acc@5 100.00 ( 99.63)
Epoch: [64][140/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6858e-01 (1.9904e-01)	Acc@1  90.62 ( 93.58)	Acc@5  99.22 ( 99.63)
Epoch: [64][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0980e-01 (1.9836e-01)	Acc@1  93.75 ( 93.61)	Acc@5  99.22 ( 99.63)
Epoch: [64][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9196e-01 (1.9986e-01)	Acc@1  94.53 ( 93.54)	Acc@5 100.00 ( 99.63)
Epoch: [64][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5097e-01 (2.0054e-01)	Acc@1  92.97 ( 93.51)	Acc@5 100.00 ( 99.63)
Epoch: [64][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9111e-01 (2.0142e-01)	Acc@1  94.53 ( 93.48)	Acc@5 100.00 ( 99.62)
Epoch: [64][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1729e-01 (2.0243e-01)	Acc@1  93.75 ( 93.44)	Acc@5  99.22 ( 99.60)
Epoch: [64][200/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.2935e-01 (2.0260e-01)	Acc@1  95.31 ( 93.39)	Acc@5 100.00 ( 99.59)
Epoch: [64][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1123e-01 (2.0382e-01)	Acc@1  94.53 ( 93.39)	Acc@5  99.22 ( 99.58)
Epoch: [64][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5606e-01 (2.0525e-01)	Acc@1  93.75 ( 93.31)	Acc@5  98.44 ( 99.58)
Epoch: [64][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.5035e-01 (2.0470e-01)	Acc@1  88.28 ( 93.35)	Acc@5  99.22 ( 99.57)
Epoch: [64][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1946e-01 (2.0336e-01)	Acc@1  90.62 ( 93.41)	Acc@5 100.00 ( 99.59)
Epoch: [64][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7353e-01 (2.0402e-01)	Acc@1  92.97 ( 93.38)	Acc@5 100.00 ( 99.59)
Epoch: [64][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6992e-01 (2.0525e-01)	Acc@1  89.84 ( 93.34)	Acc@5 100.00 ( 99.59)
Epoch: [64][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8758e-01 (2.0535e-01)	Acc@1  94.53 ( 93.38)	Acc@5 100.00 ( 99.59)
Epoch: [64][280/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6042e-01 (2.0689e-01)	Acc@1  91.41 ( 93.28)	Acc@5  98.44 ( 99.58)
Epoch: [64][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0910e-01 (2.0689e-01)	Acc@1  92.19 ( 93.30)	Acc@5  98.44 ( 99.58)
Epoch: [64][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8756e-01 (2.0648e-01)	Acc@1  92.97 ( 93.31)	Acc@5 100.00 ( 99.58)
Epoch: [64][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0717e-01 (2.0673e-01)	Acc@1  93.75 ( 93.29)	Acc@5 100.00 ( 99.58)
Epoch: [64][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3147e-01 (2.0608e-01)	Acc@1  92.97 ( 93.29)	Acc@5  99.22 ( 99.58)
Epoch: [64][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9509e-01 (2.0557e-01)	Acc@1  91.41 ( 93.30)	Acc@5 100.00 ( 99.59)
Epoch: [64][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8907e-01 (2.0571e-01)	Acc@1  92.97 ( 93.28)	Acc@5 100.00 ( 99.60)
Epoch: [64][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.3495e-01 (2.0696e-01)	Acc@1  88.28 ( 93.26)	Acc@5  99.22 ( 99.59)
Epoch: [64][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4601e-01 (2.0593e-01)	Acc@1  93.75 ( 93.30)	Acc@5 100.00 ( 99.60)
Epoch: [64][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9096e-01 (2.0670e-01)	Acc@1  89.84 ( 93.27)	Acc@5 100.00 ( 99.59)
Epoch: [64][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9706e-01 (2.0737e-01)	Acc@1  88.28 ( 93.26)	Acc@5  98.44 ( 99.59)
Epoch: [64][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.5984e-01 (2.0757e-01)	Acc@1  87.50 ( 93.25)	Acc@5  97.50 ( 99.58)
## e[64] optimizer.zero_grad (sum) time: 0.11636495590209961
## e[64]       loss.backward (sum) time: 2.128185749053955
## e[64]      optimizer.step (sum) time: 0.739607572555542
## epoch[64] training(only) time: 13.009587049484253
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.5261e+00 (1.5261e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.9965e+00 (1.7174e+00)	Acc@1  64.00 ( 66.82)	Acc@5  91.00 ( 87.82)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7034e+00 (1.6579e+00)	Acc@1  62.00 ( 66.33)	Acc@5  87.00 ( 88.33)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7603e+00 (1.6481e+00)	Acc@1  61.00 ( 66.06)	Acc@5  86.00 ( 88.42)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3581e+00 (1.6313e+00)	Acc@1  68.00 ( 66.44)	Acc@5  91.00 ( 88.66)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5216e+00 (1.6465e+00)	Acc@1  66.00 ( 66.37)	Acc@5  89.00 ( 88.49)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0086e+00 (1.6272e+00)	Acc@1  64.00 ( 66.77)	Acc@5  87.00 ( 88.64)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6998e+00 (1.6257e+00)	Acc@1  64.00 ( 66.56)	Acc@5  86.00 ( 88.76)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.4907e+00 (1.6223e+00)	Acc@1  71.00 ( 66.69)	Acc@5  87.00 ( 88.63)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 2.2537e+00 (1.6086e+00)	Acc@1  55.00 ( 66.76)	Acc@5  84.00 ( 88.70)
 * Acc@1 66.800 Acc@5 88.830
### epoch[64] execution time: 14.95649528503418
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.174 ( 0.174)	Data  0.146 ( 0.146)	Loss 2.5387e-01 (2.5387e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.033 ( 0.045)	Data  0.001 ( 0.015)	Loss 1.4162e-01 (1.8006e-01)	Acc@1  95.31 ( 94.74)	Acc@5  99.22 ( 99.86)
Epoch: [65][ 20/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.009)	Loss 2.1176e-01 (1.9230e-01)	Acc@1  93.75 ( 94.35)	Acc@5 100.00 ( 99.67)
Epoch: [65][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.8437e-01 (1.9377e-01)	Acc@1  91.41 ( 94.18)	Acc@5 100.00 ( 99.75)
Epoch: [65][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.6955e-01 (1.9755e-01)	Acc@1  94.53 ( 93.92)	Acc@5 100.00 ( 99.75)
Epoch: [65][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.4177e-01 (2.0546e-01)	Acc@1  90.62 ( 93.67)	Acc@5 100.00 ( 99.71)
Epoch: [65][ 60/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.6502e-01 (2.0334e-01)	Acc@1  92.97 ( 93.78)	Acc@5 100.00 ( 99.68)
Epoch: [65][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2076e-01 (2.0205e-01)	Acc@1  97.66 ( 93.73)	Acc@5  99.22 ( 99.66)
Epoch: [65][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.5704e-01 (2.0504e-01)	Acc@1  93.75 ( 93.65)	Acc@5  99.22 ( 99.61)
Epoch: [65][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.1464e-01 (2.0296e-01)	Acc@1  98.44 ( 93.76)	Acc@5  99.22 ( 99.61)
Epoch: [65][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0789e-01 (2.0335e-01)	Acc@1  92.19 ( 93.66)	Acc@5  97.66 ( 99.57)
Epoch: [65][110/391]	Time  0.030 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.2832e-01 (2.0482e-01)	Acc@1  93.75 ( 93.65)	Acc@5 100.00 ( 99.58)
Epoch: [65][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8725e-01 (2.0200e-01)	Acc@1  95.31 ( 93.74)	Acc@5  99.22 ( 99.59)
Epoch: [65][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.1092e-01 (1.9925e-01)	Acc@1  90.62 ( 93.86)	Acc@5  99.22 ( 99.61)
Epoch: [65][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3236e-01 (2.0091e-01)	Acc@1  93.75 ( 93.78)	Acc@5  99.22 ( 99.59)
Epoch: [65][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5700e-01 (1.9965e-01)	Acc@1  96.88 ( 93.86)	Acc@5  98.44 ( 99.59)
Epoch: [65][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0295e-01 (2.0104e-01)	Acc@1  93.75 ( 93.81)	Acc@5  99.22 ( 99.59)
Epoch: [65][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5738e-01 (2.0075e-01)	Acc@1  89.84 ( 93.80)	Acc@5 100.00 ( 99.59)
Epoch: [65][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3089e-01 (2.0259e-01)	Acc@1  93.75 ( 93.77)	Acc@5  99.22 ( 99.59)
Epoch: [65][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1915e-01 (2.0257e-01)	Acc@1  96.09 ( 93.76)	Acc@5  99.22 ( 99.60)
Epoch: [65][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7891e-01 (2.0107e-01)	Acc@1  93.75 ( 93.79)	Acc@5 100.00 ( 99.62)
Epoch: [65][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4310e-01 (2.0115e-01)	Acc@1  94.53 ( 93.82)	Acc@5 100.00 ( 99.63)
Epoch: [65][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1706e-01 (2.0168e-01)	Acc@1  98.44 ( 93.82)	Acc@5 100.00 ( 99.60)
Epoch: [65][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2210e-01 (2.0025e-01)	Acc@1  92.97 ( 93.85)	Acc@5  99.22 ( 99.60)
Epoch: [65][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2265e-01 (1.9973e-01)	Acc@1  92.97 ( 93.89)	Acc@5  99.22 ( 99.59)
Epoch: [65][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6960e-01 (1.9887e-01)	Acc@1  95.31 ( 93.93)	Acc@5 100.00 ( 99.59)
Epoch: [65][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6893e-01 (1.9857e-01)	Acc@1  96.09 ( 93.95)	Acc@5  99.22 ( 99.59)
Epoch: [65][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3880e-01 (1.9857e-01)	Acc@1  91.41 ( 93.91)	Acc@5  99.22 ( 99.60)
Epoch: [65][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6308e-01 (1.9905e-01)	Acc@1  93.75 ( 93.89)	Acc@5 100.00 ( 99.59)
Epoch: [65][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7877e-01 (1.9947e-01)	Acc@1  94.53 ( 93.87)	Acc@5 100.00 ( 99.59)
Epoch: [65][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3537e-01 (1.9825e-01)	Acc@1  95.31 ( 93.90)	Acc@5 100.00 ( 99.60)
Epoch: [65][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.3453e-01 (1.9870e-01)	Acc@1  89.84 ( 93.88)	Acc@5  99.22 ( 99.60)
Epoch: [65][320/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6643e-01 (1.9881e-01)	Acc@1  94.53 ( 93.88)	Acc@5 100.00 ( 99.60)
Epoch: [65][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3132e-01 (1.9848e-01)	Acc@1  94.53 ( 93.89)	Acc@5 100.00 ( 99.59)
Epoch: [65][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1856e-01 (1.9830e-01)	Acc@1  92.97 ( 93.90)	Acc@5  98.44 ( 99.58)
Epoch: [65][350/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9534e-01 (1.9927e-01)	Acc@1  93.75 ( 93.86)	Acc@5  99.22 ( 99.57)
Epoch: [65][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8017e-01 (1.9872e-01)	Acc@1  95.31 ( 93.86)	Acc@5  98.44 ( 99.57)
Epoch: [65][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.5960e-01 (1.9903e-01)	Acc@1  93.75 ( 93.86)	Acc@5  98.44 ( 99.56)
Epoch: [65][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2975e-01 (1.9888e-01)	Acc@1  95.31 ( 93.85)	Acc@5 100.00 ( 99.56)
Epoch: [65][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6327e-01 (1.9929e-01)	Acc@1  88.75 ( 93.83)	Acc@5 100.00 ( 99.57)
## e[65] optimizer.zero_grad (sum) time: 0.11611580848693848
## e[65]       loss.backward (sum) time: 2.1402266025543213
## e[65]      optimizer.step (sum) time: 0.7108964920043945
## epoch[65] training(only) time: 13.012584686279297
# Switched to evaluate mode...
Test: [  0/100]	Time  0.173 ( 0.173)	Loss 1.5237e+00 (1.5237e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.031)	Loss 1.9808e+00 (1.6950e+00)	Acc@1  67.00 ( 67.27)	Acc@5  90.00 ( 87.82)
Test: [ 20/100]	Time  0.018 ( 0.025)	Loss 1.7608e+00 (1.6502e+00)	Acc@1  63.00 ( 66.52)	Acc@5  87.00 ( 88.29)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.7541e+00 (1.6478e+00)	Acc@1  60.00 ( 65.97)	Acc@5  86.00 ( 88.26)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.3028e+00 (1.6300e+00)	Acc@1  69.00 ( 66.24)	Acc@5  91.00 ( 88.56)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4917e+00 (1.6455e+00)	Acc@1  68.00 ( 66.22)	Acc@5  89.00 ( 88.47)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.9830e+00 (1.6241e+00)	Acc@1  66.00 ( 66.82)	Acc@5  86.00 ( 88.61)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7172e+00 (1.6241e+00)	Acc@1  65.00 ( 66.63)	Acc@5  86.00 ( 88.72)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5291e+00 (1.6192e+00)	Acc@1  70.00 ( 66.78)	Acc@5  87.00 ( 88.60)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2036e+00 (1.6033e+00)	Acc@1  59.00 ( 66.86)	Acc@5  87.00 ( 88.76)
 * Acc@1 66.930 Acc@5 88.840
### epoch[65] execution time: 14.967878580093384
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.177 ( 0.177)	Data  0.149 ( 0.149)	Loss 2.2291e-01 (2.2291e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [66][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 1.9780e-01 (1.8229e-01)	Acc@1  95.31 ( 93.75)	Acc@5 100.00 ( 99.86)
Epoch: [66][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.2475e-01 (1.7235e-01)	Acc@1  96.09 ( 94.16)	Acc@5 100.00 ( 99.89)
Epoch: [66][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.5241e-01 (1.7550e-01)	Acc@1  93.75 ( 94.10)	Acc@5  99.22 ( 99.85)
Epoch: [66][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.8771e-01 (1.8267e-01)	Acc@1  94.53 ( 93.92)	Acc@5 100.00 ( 99.79)
Epoch: [66][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.0212e-01 (1.8275e-01)	Acc@1  96.09 ( 94.13)	Acc@5 100.00 ( 99.75)
Epoch: [66][ 60/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.1321e-01 (1.7986e-01)	Acc@1  95.31 ( 94.30)	Acc@5  99.22 ( 99.72)
Epoch: [66][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.3505e-01 (1.8611e-01)	Acc@1  93.75 ( 94.11)	Acc@5 100.00 ( 99.69)
Epoch: [66][ 80/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.004)	Loss 1.1483e-01 (1.8358e-01)	Acc@1  96.88 ( 94.21)	Acc@5 100.00 ( 99.68)
Epoch: [66][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.2344e-01 (1.8659e-01)	Acc@1  92.19 ( 94.13)	Acc@5 100.00 ( 99.67)
Epoch: [66][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.0385e-01 (1.8918e-01)	Acc@1  92.97 ( 93.99)	Acc@5 100.00 ( 99.68)
Epoch: [66][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6842e-01 (1.9070e-01)	Acc@1  95.31 ( 93.92)	Acc@5  99.22 ( 99.66)
Epoch: [66][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2681e-01 (1.8944e-01)	Acc@1  97.66 ( 93.99)	Acc@5 100.00 ( 99.66)
Epoch: [66][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5163e-01 (1.8914e-01)	Acc@1  96.09 ( 94.02)	Acc@5  99.22 ( 99.64)
Epoch: [66][140/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0348e-01 (1.8839e-01)	Acc@1  96.09 ( 94.06)	Acc@5 100.00 ( 99.65)
Epoch: [66][150/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.6851e-01 (1.8705e-01)	Acc@1  93.75 ( 94.06)	Acc@5 100.00 ( 99.65)
Epoch: [66][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7861e-01 (1.8673e-01)	Acc@1  92.97 ( 94.08)	Acc@5  98.44 ( 99.65)
Epoch: [66][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6106e-01 (1.8774e-01)	Acc@1  94.53 ( 94.03)	Acc@5  99.22 ( 99.62)
Epoch: [66][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1986e-01 (1.8939e-01)	Acc@1  96.09 ( 94.00)	Acc@5 100.00 ( 99.62)
Epoch: [66][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3739e-01 (1.9019e-01)	Acc@1  92.97 ( 93.97)	Acc@5  98.44 ( 99.61)
Epoch: [66][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1912e-01 (1.9219e-01)	Acc@1  92.19 ( 93.89)	Acc@5 100.00 ( 99.60)
Epoch: [66][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8363e-01 (1.9404e-01)	Acc@1  95.31 ( 93.81)	Acc@5 100.00 ( 99.60)
Epoch: [66][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0710e-01 (1.9451e-01)	Acc@1  92.19 ( 93.80)	Acc@5 100.00 ( 99.59)
Epoch: [66][230/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4161e-01 (1.9479e-01)	Acc@1  92.97 ( 93.80)	Acc@5  99.22 ( 99.59)
Epoch: [66][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3950e-01 (1.9539e-01)	Acc@1  95.31 ( 93.74)	Acc@5 100.00 ( 99.59)
Epoch: [66][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2703e-01 (1.9567e-01)	Acc@1  95.31 ( 93.75)	Acc@5  99.22 ( 99.59)
Epoch: [66][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1119e-01 (1.9467e-01)	Acc@1  96.09 ( 93.81)	Acc@5  99.22 ( 99.60)
Epoch: [66][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6935e-01 (1.9596e-01)	Acc@1  92.97 ( 93.76)	Acc@5 100.00 ( 99.60)
Epoch: [66][280/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.7157e-01 (1.9582e-01)	Acc@1  95.31 ( 93.77)	Acc@5  99.22 ( 99.60)
Epoch: [66][290/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 9.4595e-02 (1.9575e-01)	Acc@1  98.44 ( 93.79)	Acc@5 100.00 ( 99.60)
Epoch: [66][300/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3867e-01 (1.9623e-01)	Acc@1  92.97 ( 93.76)	Acc@5 100.00 ( 99.59)
Epoch: [66][310/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.4515e-01 (1.9588e-01)	Acc@1  97.66 ( 93.77)	Acc@5 100.00 ( 99.60)
Epoch: [66][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6542e-01 (1.9496e-01)	Acc@1  95.31 ( 93.78)	Acc@5 100.00 ( 99.61)
Epoch: [66][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.7824e-01 (1.9588e-01)	Acc@1  91.41 ( 93.74)	Acc@5 100.00 ( 99.61)
Epoch: [66][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2053e-01 (1.9691e-01)	Acc@1  93.75 ( 93.70)	Acc@5  98.44 ( 99.61)
Epoch: [66][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7554e-01 (1.9612e-01)	Acc@1  93.75 ( 93.73)	Acc@5 100.00 ( 99.61)
Epoch: [66][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3478e-01 (1.9639e-01)	Acc@1  95.31 ( 93.72)	Acc@5 100.00 ( 99.61)
Epoch: [66][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4521e-01 (1.9644e-01)	Acc@1  92.19 ( 93.72)	Acc@5  99.22 ( 99.61)
Epoch: [66][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8327e-01 (1.9699e-01)	Acc@1  92.19 ( 93.71)	Acc@5  97.66 ( 99.60)
Epoch: [66][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.8166e-01 (1.9683e-01)	Acc@1  92.50 ( 93.71)	Acc@5 100.00 ( 99.60)
## e[66] optimizer.zero_grad (sum) time: 0.11627602577209473
## e[66]       loss.backward (sum) time: 2.1061899662017822
## e[66]      optimizer.step (sum) time: 0.7404336929321289
## epoch[66] training(only) time: 12.986060619354248
# Switched to evaluate mode...
Test: [  0/100]	Time  0.134 ( 0.134)	Loss 1.5410e+00 (1.5410e+00)	Acc@1  70.00 ( 70.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.028)	Loss 2.0466e+00 (1.7122e+00)	Acc@1  66.00 ( 67.27)	Acc@5  90.00 ( 87.82)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7194e+00 (1.6674e+00)	Acc@1  63.00 ( 66.67)	Acc@5  88.00 ( 88.33)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7798e+00 (1.6597e+00)	Acc@1  60.00 ( 66.26)	Acc@5  87.00 ( 88.39)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3765e+00 (1.6456e+00)	Acc@1  73.00 ( 66.66)	Acc@5  89.00 ( 88.56)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5385e+00 (1.6618e+00)	Acc@1  68.00 ( 66.71)	Acc@5  89.00 ( 88.49)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0285e+00 (1.6423e+00)	Acc@1  63.00 ( 67.07)	Acc@5  86.00 ( 88.66)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7490e+00 (1.6424e+00)	Acc@1  64.00 ( 66.93)	Acc@5  87.00 ( 88.73)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5338e+00 (1.6396e+00)	Acc@1  69.00 ( 67.01)	Acc@5  88.00 ( 88.64)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 2.3019e+00 (1.6253e+00)	Acc@1  58.00 ( 67.03)	Acc@5  86.00 ( 88.84)
 * Acc@1 67.040 Acc@5 88.880
### epoch[66] execution time: 14.907790899276733
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.183 ( 0.183)	Data  0.154 ( 0.154)	Loss 1.6823e-01 (1.6823e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 2.0866e-01 (2.0477e-01)	Acc@1  93.75 ( 93.54)	Acc@5 100.00 ( 99.50)
Epoch: [67][ 20/391]	Time  0.031 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.8719e-01 (2.0780e-01)	Acc@1  92.19 ( 93.15)	Acc@5 100.00 ( 99.52)
Epoch: [67][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.6555e-01 (2.0428e-01)	Acc@1  94.53 ( 93.42)	Acc@5 100.00 ( 99.50)
Epoch: [67][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.3965e-01 (2.0749e-01)	Acc@1  93.75 ( 93.50)	Acc@5  99.22 ( 99.54)
Epoch: [67][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.1285e-01 (2.0135e-01)	Acc@1  96.88 ( 93.69)	Acc@5 100.00 ( 99.57)
Epoch: [67][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4742e-01 (2.0032e-01)	Acc@1  98.44 ( 93.87)	Acc@5 100.00 ( 99.58)
Epoch: [67][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.0071e-01 (1.9671e-01)	Acc@1  94.53 ( 93.94)	Acc@5  99.22 ( 99.58)
Epoch: [67][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5427e-01 (1.9769e-01)	Acc@1  95.31 ( 93.92)	Acc@5 100.00 ( 99.59)
Epoch: [67][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4351e-01 (1.9700e-01)	Acc@1  95.31 ( 93.96)	Acc@5 100.00 ( 99.58)
Epoch: [67][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7940e-01 (1.9607e-01)	Acc@1  95.31 ( 93.99)	Acc@5 100.00 ( 99.58)
Epoch: [67][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5331e-01 (1.9605e-01)	Acc@1  93.75 ( 94.00)	Acc@5  99.22 ( 99.57)
Epoch: [67][120/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9458e-01 (1.9558e-01)	Acc@1  92.19 ( 93.96)	Acc@5 100.00 ( 99.58)
Epoch: [67][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2604e-01 (1.9402e-01)	Acc@1  92.97 ( 93.98)	Acc@5  99.22 ( 99.58)
Epoch: [67][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0021e-01 (1.9615e-01)	Acc@1  94.53 ( 93.97)	Acc@5  99.22 ( 99.58)
Epoch: [67][150/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.3495e-01 (1.9583e-01)	Acc@1  96.88 ( 93.94)	Acc@5  99.22 ( 99.59)
Epoch: [67][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5720e-01 (1.9588e-01)	Acc@1  94.53 ( 93.90)	Acc@5 100.00 ( 99.58)
Epoch: [67][170/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1341e-01 (1.9702e-01)	Acc@1  94.53 ( 93.91)	Acc@5  99.22 ( 99.56)
Epoch: [67][180/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.6236e-01 (1.9710e-01)	Acc@1  89.84 ( 93.88)	Acc@5 100.00 ( 99.56)
Epoch: [67][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8972e-01 (1.9590e-01)	Acc@1  92.19 ( 93.88)	Acc@5 100.00 ( 99.57)
Epoch: [67][200/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3464e-01 (1.9695e-01)	Acc@1  90.62 ( 93.83)	Acc@5  99.22 ( 99.57)
Epoch: [67][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6269e-01 (1.9672e-01)	Acc@1  92.97 ( 93.80)	Acc@5 100.00 ( 99.59)
Epoch: [67][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3893e-01 (1.9663e-01)	Acc@1  96.88 ( 93.85)	Acc@5 100.00 ( 99.57)
Epoch: [67][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1148e-01 (1.9684e-01)	Acc@1  95.31 ( 93.82)	Acc@5  99.22 ( 99.58)
Epoch: [67][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9919e-01 (1.9730e-01)	Acc@1  92.97 ( 93.81)	Acc@5  99.22 ( 99.58)
Epoch: [67][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7644e-01 (1.9779e-01)	Acc@1  93.75 ( 93.80)	Acc@5 100.00 ( 99.59)
Epoch: [67][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2178e-01 (1.9786e-01)	Acc@1  92.97 ( 93.78)	Acc@5  99.22 ( 99.58)
Epoch: [67][270/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.3245e-01 (1.9784e-01)	Acc@1  94.53 ( 93.77)	Acc@5  99.22 ( 99.58)
Epoch: [67][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.0926e-01 (1.9770e-01)	Acc@1  88.28 ( 93.78)	Acc@5 100.00 ( 99.57)
Epoch: [67][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3148e-01 (1.9799e-01)	Acc@1  96.09 ( 93.77)	Acc@5 100.00 ( 99.58)
Epoch: [67][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1651e-01 (1.9649e-01)	Acc@1  90.62 ( 93.81)	Acc@5 100.00 ( 99.59)
Epoch: [67][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0318e-01 (1.9638e-01)	Acc@1  92.97 ( 93.83)	Acc@5  99.22 ( 99.59)
Epoch: [67][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7019e-01 (1.9655e-01)	Acc@1  92.97 ( 93.82)	Acc@5  99.22 ( 99.58)
Epoch: [67][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9960e-01 (1.9674e-01)	Acc@1  96.09 ( 93.81)	Acc@5  99.22 ( 99.58)
Epoch: [67][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.5826e-01 (1.9653e-01)	Acc@1  92.97 ( 93.82)	Acc@5  98.44 ( 99.58)
Epoch: [67][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3183e-01 (1.9605e-01)	Acc@1  93.75 ( 93.83)	Acc@5 100.00 ( 99.58)
Epoch: [67][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2249e-01 (1.9772e-01)	Acc@1  92.97 ( 93.77)	Acc@5  99.22 ( 99.57)
Epoch: [67][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3942e-01 (1.9730e-01)	Acc@1  96.09 ( 93.79)	Acc@5 100.00 ( 99.57)
Epoch: [67][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0482e-01 (1.9702e-01)	Acc@1  93.75 ( 93.80)	Acc@5 100.00 ( 99.58)
Epoch: [67][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.7165e-01 (1.9745e-01)	Acc@1  91.25 ( 93.78)	Acc@5  98.75 ( 99.58)
## e[67] optimizer.zero_grad (sum) time: 0.1150977611541748
## e[67]       loss.backward (sum) time: 2.1001551151275635
## e[67]      optimizer.step (sum) time: 0.7202000617980957
## epoch[67] training(only) time: 13.013055801391602
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.5712e+00 (1.5712e+00)	Acc@1  67.00 ( 67.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0366e+00 (1.7104e+00)	Acc@1  66.00 ( 67.00)	Acc@5  89.00 ( 88.09)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7684e+00 (1.6669e+00)	Acc@1  62.00 ( 66.33)	Acc@5  87.00 ( 88.62)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7578e+00 (1.6628e+00)	Acc@1  63.00 ( 65.94)	Acc@5  85.00 ( 88.29)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3637e+00 (1.6476e+00)	Acc@1  70.00 ( 66.20)	Acc@5  89.00 ( 88.46)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5804e+00 (1.6643e+00)	Acc@1  68.00 ( 66.16)	Acc@5  89.00 ( 88.37)
Test: [ 60/100]	Time  0.018 ( 0.019)	Loss 1.9820e+00 (1.6405e+00)	Acc@1  65.00 ( 66.79)	Acc@5  87.00 ( 88.54)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7272e+00 (1.6376e+00)	Acc@1  65.00 ( 66.72)	Acc@5  87.00 ( 88.69)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5154e+00 (1.6344e+00)	Acc@1  74.00 ( 66.84)	Acc@5  88.00 ( 88.62)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 2.2760e+00 (1.6197e+00)	Acc@1  58.00 ( 66.90)	Acc@5  87.00 ( 88.78)
 * Acc@1 66.990 Acc@5 88.890
### epoch[67] execution time: 14.955562353134155
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.181 ( 0.181)	Data  0.153 ( 0.153)	Loss 1.4291e-01 (1.4291e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 2.3096e-01 (2.1009e-01)	Acc@1  92.97 ( 93.82)	Acc@5  99.22 ( 99.36)
Epoch: [68][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 9.2302e-02 (1.9163e-01)	Acc@1  96.88 ( 94.16)	Acc@5  99.22 ( 99.40)
Epoch: [68][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.3988e-01 (1.9140e-01)	Acc@1  95.31 ( 93.88)	Acc@5 100.00 ( 99.47)
Epoch: [68][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.0850e-01 (1.9133e-01)	Acc@1  91.41 ( 93.77)	Acc@5  99.22 ( 99.47)
Epoch: [68][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.8464e-01 (1.8611e-01)	Acc@1  89.84 ( 93.78)	Acc@5 100.00 ( 99.53)
Epoch: [68][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4652e-01 (1.8828e-01)	Acc@1  94.53 ( 93.69)	Acc@5  99.22 ( 99.54)
Epoch: [68][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.1593e-01 (1.8499e-01)	Acc@1  96.88 ( 93.82)	Acc@5 100.00 ( 99.57)
Epoch: [68][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.4342e-01 (1.8626e-01)	Acc@1  90.62 ( 93.81)	Acc@5 100.00 ( 99.58)
Epoch: [68][ 90/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.004)	Loss 1.5860e-01 (1.8681e-01)	Acc@1  96.09 ( 93.79)	Acc@5  99.22 ( 99.55)
Epoch: [68][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5928e-01 (1.8859e-01)	Acc@1  96.09 ( 93.68)	Acc@5 100.00 ( 99.57)
Epoch: [68][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.5084e-01 (1.8813e-01)	Acc@1  91.41 ( 93.75)	Acc@5  99.22 ( 99.56)
Epoch: [68][120/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6724e-01 (1.9134e-01)	Acc@1  93.75 ( 93.69)	Acc@5  99.22 ( 99.55)
Epoch: [68][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.7360e-01 (1.9206e-01)	Acc@1  89.06 ( 93.68)	Acc@5 100.00 ( 99.55)
Epoch: [68][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3425e-01 (1.9268e-01)	Acc@1  92.19 ( 93.61)	Acc@5  99.22 ( 99.57)
Epoch: [68][150/391]	Time  0.035 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6386e-01 (1.9374e-01)	Acc@1  94.53 ( 93.62)	Acc@5 100.00 ( 99.58)
Epoch: [68][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2381e-01 (1.9355e-01)	Acc@1  95.31 ( 93.69)	Acc@5 100.00 ( 99.57)
Epoch: [68][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7525e-01 (1.9468e-01)	Acc@1  94.53 ( 93.65)	Acc@5  99.22 ( 99.56)
Epoch: [68][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6070e-01 (1.9529e-01)	Acc@1  95.31 ( 93.67)	Acc@5 100.00 ( 99.56)
Epoch: [68][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2609e-01 (1.9451e-01)	Acc@1  96.88 ( 93.69)	Acc@5 100.00 ( 99.57)
Epoch: [68][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9722e-01 (1.9455e-01)	Acc@1  95.31 ( 93.70)	Acc@5  99.22 ( 99.56)
Epoch: [68][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0826e-01 (1.9494e-01)	Acc@1  92.97 ( 93.71)	Acc@5 100.00 ( 99.56)
Epoch: [68][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0680e-01 (1.9472e-01)	Acc@1  98.44 ( 93.76)	Acc@5 100.00 ( 99.55)
Epoch: [68][230/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.5204e-01 (1.9610e-01)	Acc@1  89.06 ( 93.71)	Acc@5  99.22 ( 99.54)
Epoch: [68][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.4970e-01 (1.9613e-01)	Acc@1  87.50 ( 93.69)	Acc@5  99.22 ( 99.55)
Epoch: [68][250/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.3263e-01 (1.9761e-01)	Acc@1  90.62 ( 93.64)	Acc@5  96.88 ( 99.55)
Epoch: [68][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5970e-01 (1.9752e-01)	Acc@1  96.88 ( 93.63)	Acc@5 100.00 ( 99.56)
Epoch: [68][270/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7181e-01 (1.9667e-01)	Acc@1  93.75 ( 93.65)	Acc@5 100.00 ( 99.57)
Epoch: [68][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1197e-01 (1.9667e-01)	Acc@1  97.66 ( 93.64)	Acc@5 100.00 ( 99.58)
Epoch: [68][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6262e-01 (1.9605e-01)	Acc@1  92.97 ( 93.64)	Acc@5  99.22 ( 99.58)
Epoch: [68][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0352e-01 (1.9499e-01)	Acc@1  94.53 ( 93.69)	Acc@5  99.22 ( 99.59)
Epoch: [68][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3735e-01 (1.9540e-01)	Acc@1  92.19 ( 93.67)	Acc@5  99.22 ( 99.59)
Epoch: [68][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8955e-01 (1.9509e-01)	Acc@1  90.62 ( 93.68)	Acc@5 100.00 ( 99.59)
Epoch: [68][330/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6336e-01 (1.9512e-01)	Acc@1  93.75 ( 93.66)	Acc@5 100.00 ( 99.60)
Epoch: [68][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5832e-01 (1.9491e-01)	Acc@1  96.09 ( 93.66)	Acc@5 100.00 ( 99.60)
Epoch: [68][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9976e-01 (1.9605e-01)	Acc@1  92.97 ( 93.61)	Acc@5 100.00 ( 99.60)
Epoch: [68][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9654e-01 (1.9526e-01)	Acc@1  93.75 ( 93.63)	Acc@5  99.22 ( 99.60)
Epoch: [68][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.0432e-01 (1.9521e-01)	Acc@1  89.84 ( 93.63)	Acc@5  99.22 ( 99.60)
Epoch: [68][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3665e-01 (1.9460e-01)	Acc@1  96.09 ( 93.65)	Acc@5 100.00 ( 99.61)
Epoch: [68][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4755e-01 (1.9560e-01)	Acc@1  95.00 ( 93.62)	Acc@5 100.00 ( 99.60)
## e[68] optimizer.zero_grad (sum) time: 0.11530375480651855
## e[68]       loss.backward (sum) time: 2.115727424621582
## e[68]      optimizer.step (sum) time: 0.7334916591644287
## epoch[68] training(only) time: 13.002634763717651
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.5453e+00 (1.5453e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 2.0669e+00 (1.7097e+00)	Acc@1  65.00 ( 66.91)	Acc@5  89.00 ( 87.73)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7511e+00 (1.6699e+00)	Acc@1  63.00 ( 66.57)	Acc@5  88.00 ( 88.05)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.7613e+00 (1.6675e+00)	Acc@1  61.00 ( 66.00)	Acc@5  89.00 ( 87.97)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3149e+00 (1.6559e+00)	Acc@1  69.00 ( 66.15)	Acc@5  90.00 ( 88.27)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.5491e+00 (1.6711e+00)	Acc@1  70.00 ( 66.14)	Acc@5  89.00 ( 88.27)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 1.9870e+00 (1.6485e+00)	Acc@1  65.00 ( 66.80)	Acc@5  86.00 ( 88.44)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6839e+00 (1.6462e+00)	Acc@1  68.00 ( 66.69)	Acc@5  86.00 ( 88.56)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5450e+00 (1.6434e+00)	Acc@1  70.00 ( 66.77)	Acc@5  87.00 ( 88.51)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 2.3273e+00 (1.6291e+00)	Acc@1  57.00 ( 66.79)	Acc@5  86.00 ( 88.69)
 * Acc@1 66.880 Acc@5 88.750
### epoch[68] execution time: 14.944650650024414
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.179 ( 0.179)	Data  0.150 ( 0.150)	Loss 2.0863e-01 (2.0863e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.6612e-01 (1.8770e-01)	Acc@1  95.31 ( 93.82)	Acc@5  99.22 ( 99.72)
Epoch: [69][ 20/391]	Time  0.031 ( 0.040)	Data  0.002 ( 0.009)	Loss 1.9315e-01 (1.7425e-01)	Acc@1  93.75 ( 94.61)	Acc@5 100.00 ( 99.74)
Epoch: [69][ 30/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.1331e-01 (1.7692e-01)	Acc@1  96.09 ( 94.46)	Acc@5 100.00 ( 99.67)
Epoch: [69][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.3329e-01 (1.8783e-01)	Acc@1  92.97 ( 94.07)	Acc@5 100.00 ( 99.60)
Epoch: [69][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.2151e-01 (1.9093e-01)	Acc@1  92.19 ( 93.95)	Acc@5 100.00 ( 99.59)
Epoch: [69][ 60/391]	Time  0.033 ( 0.035)	Data  0.002 ( 0.004)	Loss 1.6952e-01 (1.9412e-01)	Acc@1  94.53 ( 93.88)	Acc@5  99.22 ( 99.59)
Epoch: [69][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.3493e-01 (1.9209e-01)	Acc@1  93.75 ( 93.93)	Acc@5 100.00 ( 99.63)
Epoch: [69][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7960e-01 (1.9278e-01)	Acc@1  93.75 ( 93.84)	Acc@5 100.00 ( 99.64)
Epoch: [69][ 90/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.004)	Loss 1.2581e-01 (1.8890e-01)	Acc@1  96.88 ( 93.97)	Acc@5 100.00 ( 99.66)
Epoch: [69][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0860e-01 (1.8918e-01)	Acc@1  88.28 ( 93.90)	Acc@5  99.22 ( 99.67)
Epoch: [69][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0658e-01 (1.8922e-01)	Acc@1  92.97 ( 93.93)	Acc@5 100.00 ( 99.69)
Epoch: [69][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3076e-01 (1.8690e-01)	Acc@1  96.09 ( 93.99)	Acc@5 100.00 ( 99.69)
Epoch: [69][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0477e-01 (1.8933e-01)	Acc@1  93.75 ( 93.91)	Acc@5  99.22 ( 99.67)
Epoch: [69][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3233e-01 (1.8905e-01)	Acc@1  96.88 ( 93.90)	Acc@5 100.00 ( 99.66)
Epoch: [69][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2938e-01 (1.9135e-01)	Acc@1  93.75 ( 93.80)	Acc@5  99.22 ( 99.66)
Epoch: [69][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9703e-01 (1.9141e-01)	Acc@1  94.53 ( 93.82)	Acc@5  98.44 ( 99.64)
Epoch: [69][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1990e-01 (1.9217e-01)	Acc@1  94.53 ( 93.83)	Acc@5 100.00 ( 99.62)
Epoch: [69][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7223e-01 (1.9192e-01)	Acc@1  91.41 ( 93.84)	Acc@5 100.00 ( 99.62)
Epoch: [69][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0203e-01 (1.9255e-01)	Acc@1  92.19 ( 93.82)	Acc@5 100.00 ( 99.62)
Epoch: [69][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1816e-01 (1.9138e-01)	Acc@1  96.88 ( 93.88)	Acc@5 100.00 ( 99.62)
Epoch: [69][210/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.9224e-01 (1.9075e-01)	Acc@1  92.19 ( 93.87)	Acc@5 100.00 ( 99.63)
Epoch: [69][220/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7656e-01 (1.9071e-01)	Acc@1  91.41 ( 93.86)	Acc@5  98.44 ( 99.62)
Epoch: [69][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6509e-01 (1.9014e-01)	Acc@1  96.09 ( 93.88)	Acc@5  99.22 ( 99.61)
Epoch: [69][240/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0386e-01 (1.8967e-01)	Acc@1  96.88 ( 93.91)	Acc@5 100.00 ( 99.61)
Epoch: [69][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4568e-01 (1.9087e-01)	Acc@1  92.19 ( 93.86)	Acc@5  99.22 ( 99.60)
Epoch: [69][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9820e-01 (1.8975e-01)	Acc@1  92.97 ( 93.91)	Acc@5 100.00 ( 99.61)
Epoch: [69][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0562e-01 (1.8967e-01)	Acc@1  96.09 ( 93.91)	Acc@5 100.00 ( 99.62)
Epoch: [69][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8420e-01 (1.8950e-01)	Acc@1  95.31 ( 93.92)	Acc@5 100.00 ( 99.62)
Epoch: [69][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4962e-01 (1.8943e-01)	Acc@1  96.09 ( 93.93)	Acc@5 100.00 ( 99.62)
Epoch: [69][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7864e-01 (1.8940e-01)	Acc@1  93.75 ( 93.92)	Acc@5 100.00 ( 99.62)
Epoch: [69][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8327e-01 (1.8969e-01)	Acc@1  89.06 ( 93.92)	Acc@5  99.22 ( 99.62)
Epoch: [69][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9817e-01 (1.8958e-01)	Acc@1  93.75 ( 93.92)	Acc@5 100.00 ( 99.62)
Epoch: [69][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3671e-01 (1.8885e-01)	Acc@1  94.53 ( 93.94)	Acc@5 100.00 ( 99.62)
Epoch: [69][340/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4767e-01 (1.8922e-01)	Acc@1  95.31 ( 93.94)	Acc@5 100.00 ( 99.61)
Epoch: [69][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4020e-01 (1.8961e-01)	Acc@1  93.75 ( 93.91)	Acc@5 100.00 ( 99.60)
Epoch: [69][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.2572e-02 (1.8998e-01)	Acc@1  96.88 ( 93.90)	Acc@5 100.00 ( 99.60)
Epoch: [69][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4885e-01 (1.9003e-01)	Acc@1  92.19 ( 93.89)	Acc@5 100.00 ( 99.60)
Epoch: [69][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.5858e-01 (1.8943e-01)	Acc@1  88.28 ( 93.91)	Acc@5 100.00 ( 99.61)
Epoch: [69][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1330e-01 (1.8928e-01)	Acc@1  93.75 ( 93.91)	Acc@5 100.00 ( 99.60)
## e[69] optimizer.zero_grad (sum) time: 0.11661720275878906
## e[69]       loss.backward (sum) time: 2.124150037765503
## e[69]      optimizer.step (sum) time: 0.7240097522735596
## epoch[69] training(only) time: 12.989277839660645
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.5934e+00 (1.5934e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.0748e+00 (1.7354e+00)	Acc@1  69.00 ( 66.82)	Acc@5  90.00 ( 87.82)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7434e+00 (1.6901e+00)	Acc@1  62.00 ( 66.10)	Acc@5  86.00 ( 88.19)
Test: [ 30/100]	Time  0.018 ( 0.022)	Loss 1.7780e+00 (1.6870e+00)	Acc@1  60.00 ( 65.61)	Acc@5  88.00 ( 88.10)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3562e+00 (1.6773e+00)	Acc@1  71.00 ( 65.95)	Acc@5  91.00 ( 88.34)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5779e+00 (1.6919e+00)	Acc@1  67.00 ( 65.94)	Acc@5  89.00 ( 88.31)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.0562e+00 (1.6707e+00)	Acc@1  65.00 ( 66.54)	Acc@5  85.00 ( 88.52)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7304e+00 (1.6678e+00)	Acc@1  67.00 ( 66.45)	Acc@5  87.00 ( 88.63)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5374e+00 (1.6665e+00)	Acc@1  69.00 ( 66.57)	Acc@5  88.00 ( 88.49)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.3130e+00 (1.6516e+00)	Acc@1  55.00 ( 66.62)	Acc@5  86.00 ( 88.66)
 * Acc@1 66.750 Acc@5 88.760
### epoch[69] execution time: 14.955009698867798
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.177 ( 0.177)	Data  0.153 ( 0.153)	Loss 2.8796e-01 (2.8796e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.9149e-01 (1.9549e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.57)
Epoch: [70][ 20/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.4686e-01 (1.9964e-01)	Acc@1  95.31 ( 94.05)	Acc@5 100.00 ( 99.70)
Epoch: [70][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.7904e-01 (1.8731e-01)	Acc@1  93.75 ( 94.33)	Acc@5  99.22 ( 99.65)
Epoch: [70][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.8627e-01 (1.9203e-01)	Acc@1  93.75 ( 94.00)	Acc@5 100.00 ( 99.66)
Epoch: [70][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.6064e-01 (1.8735e-01)	Acc@1  93.75 ( 94.18)	Acc@5 100.00 ( 99.69)
Epoch: [70][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.5842e-01 (1.8556e-01)	Acc@1  94.53 ( 94.26)	Acc@5 100.00 ( 99.71)
Epoch: [70][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.7778e-01 (1.8655e-01)	Acc@1  94.53 ( 94.21)	Acc@5  99.22 ( 99.67)
Epoch: [70][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.2106e-01 (1.8490e-01)	Acc@1  92.97 ( 94.28)	Acc@5  98.44 ( 99.66)
Epoch: [70][ 90/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.004)	Loss 3.2056e-01 (1.8263e-01)	Acc@1  89.84 ( 94.32)	Acc@5  99.22 ( 99.68)
Epoch: [70][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6238e-01 (1.8125e-01)	Acc@1  95.31 ( 94.40)	Acc@5 100.00 ( 99.69)
Epoch: [70][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1557e-01 (1.8230e-01)	Acc@1  92.19 ( 94.31)	Acc@5 100.00 ( 99.70)
Epoch: [70][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7262e-01 (1.8085e-01)	Acc@1  93.75 ( 94.37)	Acc@5  99.22 ( 99.70)
Epoch: [70][130/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.003)	Loss 2.3783e-01 (1.8292e-01)	Acc@1  91.41 ( 94.26)	Acc@5 100.00 ( 99.68)
Epoch: [70][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7876e-01 (1.8253e-01)	Acc@1  94.53 ( 94.24)	Acc@5 100.00 ( 99.68)
Epoch: [70][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7977e-01 (1.8208e-01)	Acc@1  94.53 ( 94.21)	Acc@5 100.00 ( 99.70)
Epoch: [70][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0410e-01 (1.8046e-01)	Acc@1  99.22 ( 94.28)	Acc@5 100.00 ( 99.70)
Epoch: [70][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3504e-01 (1.8143e-01)	Acc@1  96.09 ( 94.24)	Acc@5 100.00 ( 99.69)
Epoch: [70][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2335e-01 (1.8192e-01)	Acc@1  96.88 ( 94.22)	Acc@5 100.00 ( 99.69)
Epoch: [70][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8160e-01 (1.8283e-01)	Acc@1  89.84 ( 94.19)	Acc@5  99.22 ( 99.69)
Epoch: [70][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0614e-01 (1.8159e-01)	Acc@1  94.53 ( 94.25)	Acc@5 100.00 ( 99.70)
Epoch: [70][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0827e-01 (1.8233e-01)	Acc@1  92.97 ( 94.22)	Acc@5  99.22 ( 99.69)
Epoch: [70][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4712e-01 (1.8110e-01)	Acc@1  95.31 ( 94.27)	Acc@5 100.00 ( 99.69)
Epoch: [70][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8520e-01 (1.8360e-01)	Acc@1  87.50 ( 94.13)	Acc@5 100.00 ( 99.68)
Epoch: [70][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6297e-01 (1.8337e-01)	Acc@1  94.53 ( 94.15)	Acc@5 100.00 ( 99.68)
Epoch: [70][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5001e-01 (1.8417e-01)	Acc@1  92.19 ( 94.10)	Acc@5  99.22 ( 99.67)
Epoch: [70][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6198e-01 (1.8439e-01)	Acc@1  96.09 ( 94.09)	Acc@5  99.22 ( 99.66)
Epoch: [70][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1537e-01 (1.8466e-01)	Acc@1  97.66 ( 94.07)	Acc@5 100.00 ( 99.66)
Epoch: [70][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9629e-01 (1.8463e-01)	Acc@1  92.19 ( 94.08)	Acc@5 100.00 ( 99.67)
Epoch: [70][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7541e-01 (1.8421e-01)	Acc@1  95.31 ( 94.10)	Acc@5  99.22 ( 99.67)
Epoch: [70][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8366e-01 (1.8437e-01)	Acc@1  94.53 ( 94.10)	Acc@5  99.22 ( 99.66)
Epoch: [70][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7224e-01 (1.8412e-01)	Acc@1  93.75 ( 94.11)	Acc@5 100.00 ( 99.66)
Epoch: [70][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9720e-01 (1.8478e-01)	Acc@1  94.53 ( 94.09)	Acc@5 100.00 ( 99.65)
Epoch: [70][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3204e-01 (1.8524e-01)	Acc@1  96.09 ( 94.07)	Acc@5 100.00 ( 99.65)
Epoch: [70][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1878e-01 (1.8552e-01)	Acc@1  96.88 ( 94.09)	Acc@5 100.00 ( 99.65)
Epoch: [70][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8613e-01 (1.8562e-01)	Acc@1  95.31 ( 94.08)	Acc@5  99.22 ( 99.64)
Epoch: [70][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4490e-01 (1.8576e-01)	Acc@1  93.75 ( 94.07)	Acc@5 100.00 ( 99.64)
Epoch: [70][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1244e-01 (1.8559e-01)	Acc@1  96.88 ( 94.08)	Acc@5 100.00 ( 99.64)
Epoch: [70][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3019e-01 (1.8640e-01)	Acc@1  93.75 ( 94.06)	Acc@5 100.00 ( 99.63)
Epoch: [70][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.4657e-01 (1.8685e-01)	Acc@1  87.50 ( 94.05)	Acc@5  98.75 ( 99.63)
## e[70] optimizer.zero_grad (sum) time: 0.11657094955444336
## e[70]       loss.backward (sum) time: 2.1355886459350586
## e[70]      optimizer.step (sum) time: 0.7374544143676758
## epoch[70] training(only) time: 13.019477844238281
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.5832e+00 (1.5832e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.1100e+00 (1.7304e+00)	Acc@1  68.00 ( 67.00)	Acc@5  91.00 ( 88.09)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7754e+00 (1.6839e+00)	Acc@1  62.00 ( 66.62)	Acc@5  88.00 ( 88.52)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8090e+00 (1.6831e+00)	Acc@1  59.00 ( 65.97)	Acc@5  86.00 ( 88.35)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.4324e+00 (1.6788e+00)	Acc@1  68.00 ( 66.17)	Acc@5  90.00 ( 88.56)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.5732e+00 (1.6940e+00)	Acc@1  67.00 ( 66.16)	Acc@5  89.00 ( 88.43)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.1010e+00 (1.6713e+00)	Acc@1  64.00 ( 66.75)	Acc@5  86.00 ( 88.57)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7063e+00 (1.6702e+00)	Acc@1  67.00 ( 66.54)	Acc@5  88.00 ( 88.75)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5714e+00 (1.6684e+00)	Acc@1  70.00 ( 66.59)	Acc@5  87.00 ( 88.67)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2729e+00 (1.6509e+00)	Acc@1  58.00 ( 66.71)	Acc@5  85.00 ( 88.78)
 * Acc@1 66.820 Acc@5 88.960
### epoch[70] execution time: 14.977051734924316
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.179 ( 0.179)	Data  0.145 ( 0.145)	Loss 2.2450e-01 (2.2450e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.031 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.5356e-01 (1.5742e-01)	Acc@1  95.31 ( 95.24)	Acc@5 100.00 ( 99.79)
Epoch: [71][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 7.5383e-02 (1.7599e-01)	Acc@1  96.88 ( 94.53)	Acc@5 100.00 ( 99.81)
Epoch: [71][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.4854e-01 (1.7734e-01)	Acc@1  92.97 ( 94.53)	Acc@5  99.22 ( 99.77)
Epoch: [71][ 40/391]	Time  0.032 ( 0.036)	Data  0.002 ( 0.005)	Loss 2.3037e-01 (1.7281e-01)	Acc@1  91.41 ( 94.72)	Acc@5 100.00 ( 99.77)
Epoch: [71][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.3982e-01 (1.7559e-01)	Acc@1  92.97 ( 94.61)	Acc@5  98.44 ( 99.71)
Epoch: [71][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.8857e-01 (1.7869e-01)	Acc@1  93.75 ( 94.48)	Acc@5 100.00 ( 99.64)
Epoch: [71][ 70/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.3343e-01 (1.7165e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.69)
Epoch: [71][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.3751e-01 (1.7124e-01)	Acc@1  91.41 ( 94.65)	Acc@5  99.22 ( 99.67)
Epoch: [71][ 90/391]	Time  0.033 ( 0.034)	Data  0.002 ( 0.004)	Loss 1.6121e-01 (1.7520e-01)	Acc@1  94.53 ( 94.56)	Acc@5 100.00 ( 99.63)
Epoch: [71][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1235e-01 (1.7648e-01)	Acc@1  92.97 ( 94.52)	Acc@5 100.00 ( 99.63)
Epoch: [71][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0571e-01 (1.7781e-01)	Acc@1  95.31 ( 94.45)	Acc@5  98.44 ( 99.62)
Epoch: [71][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5187e-01 (1.7693e-01)	Acc@1  96.09 ( 94.47)	Acc@5 100.00 ( 99.64)
Epoch: [71][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7584e-01 (1.7692e-01)	Acc@1  94.53 ( 94.45)	Acc@5 100.00 ( 99.63)
Epoch: [71][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2719e-01 (1.7557e-01)	Acc@1  92.19 ( 94.46)	Acc@5 100.00 ( 99.63)
Epoch: [71][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6953e-01 (1.7771e-01)	Acc@1  95.31 ( 94.39)	Acc@5 100.00 ( 99.62)
Epoch: [71][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5757e-01 (1.7836e-01)	Acc@1  94.53 ( 94.39)	Acc@5 100.00 ( 99.63)
Epoch: [71][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9408e-01 (1.7974e-01)	Acc@1  94.53 ( 94.34)	Acc@5  99.22 ( 99.63)
Epoch: [71][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7551e-01 (1.8076e-01)	Acc@1  94.53 ( 94.30)	Acc@5 100.00 ( 99.63)
Epoch: [71][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9073e-01 (1.7999e-01)	Acc@1  92.19 ( 94.32)	Acc@5  99.22 ( 99.62)
Epoch: [71][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1735e-01 (1.8083e-01)	Acc@1  89.84 ( 94.27)	Acc@5 100.00 ( 99.63)
Epoch: [71][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8623e-01 (1.8189e-01)	Acc@1  89.84 ( 94.24)	Acc@5  99.22 ( 99.62)
Epoch: [71][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2300e-01 (1.8164e-01)	Acc@1  95.31 ( 94.24)	Acc@5 100.00 ( 99.63)
Epoch: [71][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9842e-01 (1.8133e-01)	Acc@1  94.53 ( 94.24)	Acc@5 100.00 ( 99.64)
Epoch: [71][240/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8672e-01 (1.8220e-01)	Acc@1  95.31 ( 94.23)	Acc@5  99.22 ( 99.64)
Epoch: [71][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6921e-01 (1.8202e-01)	Acc@1  96.09 ( 94.23)	Acc@5 100.00 ( 99.64)
Epoch: [71][260/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6460e-01 (1.8130e-01)	Acc@1  92.97 ( 94.25)	Acc@5  99.22 ( 99.63)
Epoch: [71][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5687e-01 (1.8100e-01)	Acc@1  95.31 ( 94.25)	Acc@5 100.00 ( 99.64)
Epoch: [71][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3550e-01 (1.7991e-01)	Acc@1  95.31 ( 94.30)	Acc@5 100.00 ( 99.64)
Epoch: [71][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.1615e-02 (1.8004e-01)	Acc@1  97.66 ( 94.30)	Acc@5 100.00 ( 99.63)
Epoch: [71][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7062e-01 (1.8140e-01)	Acc@1  94.53 ( 94.27)	Acc@5 100.00 ( 99.62)
Epoch: [71][310/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4427e-01 (1.8183e-01)	Acc@1  90.62 ( 94.26)	Acc@5  99.22 ( 99.61)
Epoch: [71][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3219e-01 (1.8168e-01)	Acc@1  92.19 ( 94.27)	Acc@5 100.00 ( 99.61)
Epoch: [71][330/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0863e-01 (1.8172e-01)	Acc@1  96.88 ( 94.26)	Acc@5 100.00 ( 99.62)
Epoch: [71][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2327e-01 (1.8145e-01)	Acc@1  97.66 ( 94.27)	Acc@5 100.00 ( 99.62)
Epoch: [71][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6285e-01 (1.8123e-01)	Acc@1  95.31 ( 94.27)	Acc@5 100.00 ( 99.62)
Epoch: [71][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8388e-01 (1.8187e-01)	Acc@1  92.97 ( 94.23)	Acc@5 100.00 ( 99.63)
Epoch: [71][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2527e-01 (1.8237e-01)	Acc@1  91.41 ( 94.22)	Acc@5 100.00 ( 99.63)
Epoch: [71][380/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9000e-01 (1.8278e-01)	Acc@1  93.75 ( 94.20)	Acc@5 100.00 ( 99.63)
Epoch: [71][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2317e-01 (1.8190e-01)	Acc@1  93.75 ( 94.22)	Acc@5 100.00 ( 99.63)
## e[71] optimizer.zero_grad (sum) time: 0.11624670028686523
## e[71]       loss.backward (sum) time: 2.1443686485290527
## e[71]      optimizer.step (sum) time: 0.7213587760925293
## epoch[71] training(only) time: 12.984729766845703
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.5487e+00 (1.5487e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.022 ( 0.031)	Loss 2.0703e+00 (1.7411e+00)	Acc@1  66.00 ( 67.36)	Acc@5  92.00 ( 88.00)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7554e+00 (1.6940e+00)	Acc@1  61.00 ( 66.76)	Acc@5  88.00 ( 88.33)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8360e+00 (1.6939e+00)	Acc@1  62.00 ( 66.13)	Acc@5  83.00 ( 87.94)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.3776e+00 (1.6842e+00)	Acc@1  71.00 ( 66.41)	Acc@5  89.00 ( 88.17)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5495e+00 (1.6985e+00)	Acc@1  68.00 ( 66.39)	Acc@5  88.00 ( 88.12)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.0211e+00 (1.6733e+00)	Acc@1  66.00 ( 66.89)	Acc@5  85.00 ( 88.26)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6848e+00 (1.6713e+00)	Acc@1  66.00 ( 66.82)	Acc@5  87.00 ( 88.41)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5625e+00 (1.6709e+00)	Acc@1  73.00 ( 66.84)	Acc@5  86.00 ( 88.33)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2713e+00 (1.6535e+00)	Acc@1  57.00 ( 66.87)	Acc@5  85.00 ( 88.48)
 * Acc@1 66.940 Acc@5 88.660
### epoch[71] execution time: 14.956905126571655
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.178 ( 0.178)	Data  0.152 ( 0.152)	Loss 1.3336e-01 (1.3336e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 2.0522e-01 (1.6640e-01)	Acc@1  92.97 ( 94.46)	Acc@5 100.00 ( 99.79)
Epoch: [72][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.6447e-01 (1.7144e-01)	Acc@1  93.75 ( 94.57)	Acc@5 100.00 ( 99.70)
Epoch: [72][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.2696e-01 (1.7553e-01)	Acc@1  94.53 ( 94.43)	Acc@5 100.00 ( 99.67)
Epoch: [72][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.0216e-01 (1.7682e-01)	Acc@1  90.62 ( 94.40)	Acc@5 100.00 ( 99.68)
Epoch: [72][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.3433e-01 (1.7637e-01)	Acc@1  96.09 ( 94.32)	Acc@5  99.22 ( 99.69)
Epoch: [72][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.2019e-01 (1.7543e-01)	Acc@1  92.97 ( 94.43)	Acc@5  99.22 ( 99.68)
Epoch: [72][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.5724e-01 (1.7572e-01)	Acc@1  92.97 ( 94.44)	Acc@5 100.00 ( 99.70)
Epoch: [72][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.0249e-01 (1.7832e-01)	Acc@1  92.97 ( 94.36)	Acc@5  99.22 ( 99.64)
Epoch: [72][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8562e-01 (1.7562e-01)	Acc@1  93.75 ( 94.51)	Acc@5 100.00 ( 99.67)
Epoch: [72][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.8376e-01 (1.7757e-01)	Acc@1  89.06 ( 94.48)	Acc@5  99.22 ( 99.67)
Epoch: [72][110/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1023e-01 (1.7576e-01)	Acc@1  96.09 ( 94.54)	Acc@5 100.00 ( 99.67)
Epoch: [72][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0740e-01 (1.7522e-01)	Acc@1  97.66 ( 94.54)	Acc@5 100.00 ( 99.66)
Epoch: [72][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2270e-01 (1.7636e-01)	Acc@1  92.97 ( 94.50)	Acc@5 100.00 ( 99.67)
Epoch: [72][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1645e-01 (1.7622e-01)	Acc@1  95.31 ( 94.46)	Acc@5 100.00 ( 99.67)
Epoch: [72][150/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1775e-01 (1.7948e-01)	Acc@1  92.97 ( 94.34)	Acc@5  99.22 ( 99.67)
Epoch: [72][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1082e-01 (1.7962e-01)	Acc@1  96.88 ( 94.35)	Acc@5 100.00 ( 99.66)
Epoch: [72][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1868e-01 (1.7908e-01)	Acc@1  92.97 ( 94.39)	Acc@5  98.44 ( 99.65)
Epoch: [72][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4233e-01 (1.7796e-01)	Acc@1  92.19 ( 94.43)	Acc@5 100.00 ( 99.65)
Epoch: [72][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1944e-01 (1.7886e-01)	Acc@1  93.75 ( 94.43)	Acc@5  99.22 ( 99.65)
Epoch: [72][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4001e-01 (1.7905e-01)	Acc@1  90.62 ( 94.43)	Acc@5 100.00 ( 99.65)
Epoch: [72][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5117e-01 (1.7879e-01)	Acc@1  96.09 ( 94.44)	Acc@5 100.00 ( 99.63)
Epoch: [72][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7941e-01 (1.7833e-01)	Acc@1  96.09 ( 94.46)	Acc@5  98.44 ( 99.63)
Epoch: [72][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0724e-01 (1.7733e-01)	Acc@1  96.88 ( 94.49)	Acc@5 100.00 ( 99.64)
Epoch: [72][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.9093e-02 (1.7707e-01)	Acc@1  97.66 ( 94.49)	Acc@5 100.00 ( 99.65)
Epoch: [72][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4165e-01 (1.7816e-01)	Acc@1  94.53 ( 94.48)	Acc@5  97.66 ( 99.62)
Epoch: [72][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7707e-01 (1.7901e-01)	Acc@1  90.62 ( 94.42)	Acc@5 100.00 ( 99.62)
Epoch: [72][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7395e-01 (1.7990e-01)	Acc@1  94.53 ( 94.39)	Acc@5  99.22 ( 99.61)
Epoch: [72][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4174e-01 (1.8056e-01)	Acc@1  94.53 ( 94.35)	Acc@5 100.00 ( 99.60)
Epoch: [72][290/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.8650e-01 (1.8058e-01)	Acc@1  92.19 ( 94.33)	Acc@5 100.00 ( 99.60)
Epoch: [72][300/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.9632e-01 (1.8067e-01)	Acc@1  94.53 ( 94.34)	Acc@5 100.00 ( 99.61)
Epoch: [72][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9764e-01 (1.8113e-01)	Acc@1  92.19 ( 94.30)	Acc@5 100.00 ( 99.62)
Epoch: [72][320/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.6842e-01 (1.8115e-01)	Acc@1  93.75 ( 94.28)	Acc@5 100.00 ( 99.63)
Epoch: [72][330/391]	Time  0.037 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7624e-01 (1.8158e-01)	Acc@1  93.75 ( 94.26)	Acc@5 100.00 ( 99.62)
Epoch: [72][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 3.2950e-01 (1.8214e-01)	Acc@1  91.41 ( 94.24)	Acc@5  99.22 ( 99.62)
Epoch: [72][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2089e-01 (1.8210e-01)	Acc@1  92.19 ( 94.24)	Acc@5 100.00 ( 99.62)
Epoch: [72][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6394e-01 (1.8172e-01)	Acc@1  95.31 ( 94.27)	Acc@5 100.00 ( 99.62)
Epoch: [72][370/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6105e-01 (1.8072e-01)	Acc@1  94.53 ( 94.30)	Acc@5 100.00 ( 99.62)
Epoch: [72][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6758e-01 (1.8080e-01)	Acc@1  94.53 ( 94.29)	Acc@5 100.00 ( 99.61)
Epoch: [72][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3712e-01 (1.8134e-01)	Acc@1  95.00 ( 94.28)	Acc@5 100.00 ( 99.61)
## e[72] optimizer.zero_grad (sum) time: 0.11619257926940918
## e[72]       loss.backward (sum) time: 2.1137804985046387
## e[72]      optimizer.step (sum) time: 0.7410988807678223
## epoch[72] training(only) time: 12.982259035110474
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.5748e+00 (1.5748e+00)	Acc@1  66.00 ( 66.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0942e+00 (1.7401e+00)	Acc@1  66.00 ( 66.73)	Acc@5  91.00 ( 87.82)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7880e+00 (1.6987e+00)	Acc@1  62.00 ( 66.33)	Acc@5  86.00 ( 88.19)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.8143e+00 (1.6996e+00)	Acc@1  59.00 ( 65.68)	Acc@5  86.00 ( 88.13)
Test: [ 40/100]	Time  0.018 ( 0.020)	Loss 1.4233e+00 (1.6840e+00)	Acc@1  69.00 ( 66.07)	Acc@5  90.00 ( 88.46)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5672e+00 (1.7009e+00)	Acc@1  67.00 ( 65.96)	Acc@5  89.00 ( 88.35)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0802e+00 (1.6798e+00)	Acc@1  67.00 ( 66.48)	Acc@5  86.00 ( 88.49)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7541e+00 (1.6819e+00)	Acc@1  64.00 ( 66.31)	Acc@5  86.00 ( 88.62)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6230e+00 (1.6796e+00)	Acc@1  69.00 ( 66.38)	Acc@5  85.00 ( 88.53)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2333e+00 (1.6606e+00)	Acc@1  59.00 ( 66.57)	Acc@5  86.00 ( 88.65)
 * Acc@1 66.720 Acc@5 88.730
### epoch[72] execution time: 14.933283805847168
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.168 ( 0.168)	Data  0.142 ( 0.142)	Loss 1.0542e-01 (1.0542e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.034 ( 0.045)	Data  0.001 ( 0.015)	Loss 1.0272e-01 (1.4306e-01)	Acc@1  97.66 ( 95.81)	Acc@5 100.00 ( 99.72)
Epoch: [73][ 20/391]	Time  0.032 ( 0.039)	Data  0.002 ( 0.009)	Loss 1.4492e-01 (1.5826e-01)	Acc@1  96.09 ( 95.13)	Acc@5  99.22 ( 99.67)
Epoch: [73][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.0661e-01 (1.6302e-01)	Acc@1  96.88 ( 94.88)	Acc@5 100.00 ( 99.72)
Epoch: [73][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.4560e-01 (1.6182e-01)	Acc@1  95.31 ( 94.84)	Acc@5 100.00 ( 99.71)
Epoch: [73][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.1296e-01 (1.6754e-01)	Acc@1  93.75 ( 94.61)	Acc@5 100.00 ( 99.71)
Epoch: [73][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.5743e-01 (1.7239e-01)	Acc@1  90.62 ( 94.40)	Acc@5  99.22 ( 99.64)
Epoch: [73][ 70/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8336e-01 (1.7111e-01)	Acc@1  95.31 ( 94.43)	Acc@5 100.00 ( 99.65)
Epoch: [73][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7874e-01 (1.7289e-01)	Acc@1  94.53 ( 94.43)	Acc@5  99.22 ( 99.63)
Epoch: [73][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4073e-01 (1.7216e-01)	Acc@1  95.31 ( 94.43)	Acc@5 100.00 ( 99.66)
Epoch: [73][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2203e-01 (1.7392e-01)	Acc@1  93.75 ( 94.38)	Acc@5  99.22 ( 99.66)
Epoch: [73][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9879e-01 (1.7494e-01)	Acc@1  94.53 ( 94.31)	Acc@5  99.22 ( 99.68)
Epoch: [73][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1660e-01 (1.7354e-01)	Acc@1  92.97 ( 94.35)	Acc@5 100.00 ( 99.69)
Epoch: [73][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9511e-01 (1.7615e-01)	Acc@1  94.53 ( 94.30)	Acc@5  99.22 ( 99.66)
Epoch: [73][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.6204e-01 (1.7624e-01)	Acc@1  92.19 ( 94.34)	Acc@5  98.44 ( 99.63)
Epoch: [73][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0116e-01 (1.7385e-01)	Acc@1  96.88 ( 94.42)	Acc@5  99.22 ( 99.63)
Epoch: [73][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2149e-01 (1.7376e-01)	Acc@1  91.41 ( 94.41)	Acc@5  98.44 ( 99.62)
Epoch: [73][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2790e-01 (1.7233e-01)	Acc@1  96.88 ( 94.48)	Acc@5 100.00 ( 99.63)
Epoch: [73][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6299e-01 (1.7346e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.64)
Epoch: [73][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9181e-01 (1.7364e-01)	Acc@1  95.31 ( 94.36)	Acc@5 100.00 ( 99.66)
Epoch: [73][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0297e-01 (1.7446e-01)	Acc@1  93.75 ( 94.36)	Acc@5 100.00 ( 99.65)
Epoch: [73][210/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9159e-01 (1.7578e-01)	Acc@1  92.97 ( 94.32)	Acc@5 100.00 ( 99.65)
Epoch: [73][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4623e-01 (1.7660e-01)	Acc@1  96.09 ( 94.27)	Acc@5  99.22 ( 99.66)
Epoch: [73][230/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4985e-01 (1.7739e-01)	Acc@1  95.31 ( 94.23)	Acc@5 100.00 ( 99.66)
Epoch: [73][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3967e-01 (1.7807e-01)	Acc@1  95.31 ( 94.18)	Acc@5 100.00 ( 99.66)
Epoch: [73][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3035e-01 (1.7693e-01)	Acc@1  93.75 ( 94.22)	Acc@5 100.00 ( 99.68)
Epoch: [73][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0588e-01 (1.7853e-01)	Acc@1  92.97 ( 94.19)	Acc@5  98.44 ( 99.66)
Epoch: [73][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7725e-01 (1.7923e-01)	Acc@1  95.31 ( 94.18)	Acc@5  99.22 ( 99.66)
Epoch: [73][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3009e-01 (1.7987e-01)	Acc@1  92.97 ( 94.16)	Acc@5 100.00 ( 99.66)
Epoch: [73][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1408e-01 (1.8006e-01)	Acc@1  97.66 ( 94.15)	Acc@5 100.00 ( 99.66)
Epoch: [73][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5332e-01 (1.7941e-01)	Acc@1  94.53 ( 94.21)	Acc@5  99.22 ( 99.67)
Epoch: [73][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8090e-01 (1.7947e-01)	Acc@1  93.75 ( 94.20)	Acc@5 100.00 ( 99.68)
Epoch: [73][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6030e-01 (1.7975e-01)	Acc@1  94.53 ( 94.20)	Acc@5 100.00 ( 99.67)
Epoch: [73][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4343e-01 (1.7915e-01)	Acc@1  95.31 ( 94.24)	Acc@5 100.00 ( 99.67)
Epoch: [73][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4557e-01 (1.7867e-01)	Acc@1  94.53 ( 94.25)	Acc@5 100.00 ( 99.67)
Epoch: [73][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8735e-01 (1.7827e-01)	Acc@1  94.53 ( 94.27)	Acc@5 100.00 ( 99.68)
Epoch: [73][360/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0266e-01 (1.7869e-01)	Acc@1  93.75 ( 94.26)	Acc@5 100.00 ( 99.67)
Epoch: [73][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8654e-01 (1.7840e-01)	Acc@1  94.53 ( 94.27)	Acc@5 100.00 ( 99.68)
Epoch: [73][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6734e-01 (1.7848e-01)	Acc@1  94.53 ( 94.27)	Acc@5 100.00 ( 99.68)
Epoch: [73][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2881e-01 (1.7885e-01)	Acc@1  92.50 ( 94.26)	Acc@5 100.00 ( 99.69)
## e[73] optimizer.zero_grad (sum) time: 0.11593413352966309
## e[73]       loss.backward (sum) time: 2.0781142711639404
## e[73]      optimizer.step (sum) time: 0.7404649257659912
## epoch[73] training(only) time: 13.008310079574585
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.5710e+00 (1.5710e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.1210e+00 (1.7631e+00)	Acc@1  67.00 ( 67.18)	Acc@5  90.00 ( 87.91)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7611e+00 (1.7152e+00)	Acc@1  62.00 ( 66.43)	Acc@5  88.00 ( 88.19)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.7932e+00 (1.7131e+00)	Acc@1  62.00 ( 65.94)	Acc@5  86.00 ( 88.03)
Test: [ 40/100]	Time  0.022 ( 0.021)	Loss 1.4494e+00 (1.6985e+00)	Acc@1  69.00 ( 66.32)	Acc@5  90.00 ( 88.29)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5865e+00 (1.7179e+00)	Acc@1  69.00 ( 66.35)	Acc@5  89.00 ( 88.22)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.0639e+00 (1.6919e+00)	Acc@1  65.00 ( 66.80)	Acc@5  87.00 ( 88.39)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7650e+00 (1.6918e+00)	Acc@1  64.00 ( 66.61)	Acc@5  88.00 ( 88.59)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6061e+00 (1.6888e+00)	Acc@1  76.00 ( 66.73)	Acc@5  87.00 ( 88.51)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.3088e+00 (1.6724e+00)	Acc@1  57.00 ( 66.77)	Acc@5  85.00 ( 88.60)
 * Acc@1 66.900 Acc@5 88.770
### epoch[73] execution time: 14.98728060722351
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.172 ( 0.172)	Data  0.148 ( 0.148)	Loss 2.5996e-01 (2.5996e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.22 ( 99.22)
Epoch: [74][ 10/391]	Time  0.033 ( 0.045)	Data  0.001 ( 0.015)	Loss 2.7919e-01 (1.8839e-01)	Acc@1  88.28 ( 93.82)	Acc@5 100.00 ( 99.57)
Epoch: [74][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 2.0162e-01 (1.8162e-01)	Acc@1  92.97 ( 93.97)	Acc@5 100.00 ( 99.74)
Epoch: [74][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.3851e-01 (1.8702e-01)	Acc@1  94.53 ( 93.90)	Acc@5 100.00 ( 99.72)
Epoch: [74][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.8412e-01 (1.8407e-01)	Acc@1  94.53 ( 94.09)	Acc@5  98.44 ( 99.64)
Epoch: [74][ 50/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.9627e-01 (1.8449e-01)	Acc@1  94.53 ( 94.19)	Acc@5 100.00 ( 99.62)
Epoch: [74][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.2376e-01 (1.8118e-01)	Acc@1  93.75 ( 94.29)	Acc@5  99.22 ( 99.62)
Epoch: [74][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4447e-01 (1.8113e-01)	Acc@1  94.53 ( 94.28)	Acc@5 100.00 ( 99.60)
Epoch: [74][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.2149e-01 (1.7863e-01)	Acc@1  93.75 ( 94.40)	Acc@5 100.00 ( 99.60)
Epoch: [74][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.3580e-01 (1.7988e-01)	Acc@1  92.19 ( 94.39)	Acc@5  99.22 ( 99.63)
Epoch: [74][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6761e-01 (1.7896e-01)	Acc@1  96.88 ( 94.40)	Acc@5  99.22 ( 99.61)
Epoch: [74][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.2215e-02 (1.7856e-01)	Acc@1  97.66 ( 94.37)	Acc@5  99.22 ( 99.62)
Epoch: [74][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8387e-01 (1.7953e-01)	Acc@1  94.53 ( 94.38)	Acc@5 100.00 ( 99.62)
Epoch: [74][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.5421e-02 (1.8000e-01)	Acc@1  97.66 ( 94.41)	Acc@5 100.00 ( 99.63)
Epoch: [74][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2595e-01 (1.7964e-01)	Acc@1  95.31 ( 94.36)	Acc@5 100.00 ( 99.63)
Epoch: [74][150/391]	Time  0.029 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.1667e-01 (1.8058e-01)	Acc@1  96.09 ( 94.32)	Acc@5  99.22 ( 99.63)
Epoch: [74][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2930e-01 (1.7943e-01)	Acc@1  96.88 ( 94.37)	Acc@5 100.00 ( 99.64)
Epoch: [74][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4122e-01 (1.7944e-01)	Acc@1  95.31 ( 94.35)	Acc@5 100.00 ( 99.64)
Epoch: [74][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5234e-01 (1.7953e-01)	Acc@1  96.09 ( 94.35)	Acc@5 100.00 ( 99.64)
Epoch: [74][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6854e-01 (1.7773e-01)	Acc@1  94.53 ( 94.40)	Acc@5  98.44 ( 99.64)
Epoch: [74][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.1824e-02 (1.7768e-01)	Acc@1  97.66 ( 94.41)	Acc@5 100.00 ( 99.64)
Epoch: [74][210/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.0719e-01 (1.7795e-01)	Acc@1  92.97 ( 94.42)	Acc@5 100.00 ( 99.64)
Epoch: [74][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7454e-01 (1.7851e-01)	Acc@1  94.53 ( 94.40)	Acc@5 100.00 ( 99.64)
Epoch: [74][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3588e-01 (1.7835e-01)	Acc@1  96.09 ( 94.40)	Acc@5 100.00 ( 99.63)
Epoch: [74][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7845e-01 (1.7871e-01)	Acc@1  93.75 ( 94.39)	Acc@5 100.00 ( 99.64)
Epoch: [74][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3177e-01 (1.7967e-01)	Acc@1  90.62 ( 94.36)	Acc@5 100.00 ( 99.63)
Epoch: [74][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0403e-01 (1.7890e-01)	Acc@1  93.75 ( 94.39)	Acc@5  98.44 ( 99.63)
Epoch: [74][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5046e-01 (1.7868e-01)	Acc@1  95.31 ( 94.42)	Acc@5 100.00 ( 99.63)
Epoch: [74][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2188e-01 (1.7879e-01)	Acc@1  95.31 ( 94.40)	Acc@5 100.00 ( 99.63)
Epoch: [74][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0363e-01 (1.7863e-01)	Acc@1  92.19 ( 94.40)	Acc@5 100.00 ( 99.63)
Epoch: [74][300/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5558e-01 (1.7865e-01)	Acc@1  94.53 ( 94.37)	Acc@5 100.00 ( 99.64)
Epoch: [74][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2504e-01 (1.7915e-01)	Acc@1  96.88 ( 94.36)	Acc@5 100.00 ( 99.64)
Epoch: [74][320/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.0366e-01 (1.7951e-01)	Acc@1  96.88 ( 94.32)	Acc@5 100.00 ( 99.63)
Epoch: [74][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2742e-01 (1.7949e-01)	Acc@1  91.41 ( 94.32)	Acc@5  99.22 ( 99.64)
Epoch: [74][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5310e-01 (1.7971e-01)	Acc@1  94.53 ( 94.28)	Acc@5  99.22 ( 99.63)
Epoch: [74][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2382e-01 (1.7927e-01)	Acc@1  93.75 ( 94.31)	Acc@5 100.00 ( 99.63)
Epoch: [74][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3224e-01 (1.7890e-01)	Acc@1  93.75 ( 94.31)	Acc@5 100.00 ( 99.63)
Epoch: [74][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5241e-01 (1.7914e-01)	Acc@1  97.66 ( 94.32)	Acc@5  98.44 ( 99.62)
Epoch: [74][380/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.1225e-02 (1.7912e-01)	Acc@1  99.22 ( 94.33)	Acc@5 100.00 ( 99.62)
Epoch: [74][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6420e-01 (1.7933e-01)	Acc@1  95.00 ( 94.33)	Acc@5  98.75 ( 99.61)
## e[74] optimizer.zero_grad (sum) time: 0.11606693267822266
## e[74]       loss.backward (sum) time: 2.1121902465820312
## e[74]      optimizer.step (sum) time: 0.7356832027435303
## epoch[74] training(only) time: 12.97636890411377
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.5820e+00 (1.5820e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.0427e+00 (1.7364e+00)	Acc@1  68.00 ( 67.27)	Acc@5  91.00 ( 88.09)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.7119e+00 (1.6935e+00)	Acc@1  62.00 ( 66.62)	Acc@5  87.00 ( 88.43)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.8138e+00 (1.6925e+00)	Acc@1  59.00 ( 66.10)	Acc@5  87.00 ( 88.26)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.3837e+00 (1.6792e+00)	Acc@1  67.00 ( 66.24)	Acc@5  92.00 ( 88.41)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5891e+00 (1.6971e+00)	Acc@1  69.00 ( 66.24)	Acc@5  88.00 ( 88.27)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0881e+00 (1.6754e+00)	Acc@1  64.00 ( 66.72)	Acc@5  84.00 ( 88.43)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6743e+00 (1.6738e+00)	Acc@1  68.00 ( 66.55)	Acc@5  87.00 ( 88.54)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5746e+00 (1.6710e+00)	Acc@1  71.00 ( 66.68)	Acc@5  87.00 ( 88.46)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2350e+00 (1.6540e+00)	Acc@1  62.00 ( 66.81)	Acc@5  85.00 ( 88.60)
 * Acc@1 66.900 Acc@5 88.680
### epoch[74] execution time: 14.923139810562134
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.170 ( 0.170)	Data  0.142 ( 0.142)	Loss 1.9426e-01 (1.9426e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.032 ( 0.045)	Data  0.002 ( 0.015)	Loss 1.4068e-01 (1.5684e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.22 ( 99.57)
Epoch: [75][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.8152e-01 (1.5850e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.70)
Epoch: [75][ 30/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.006)	Loss 1.4038e-01 (1.6287e-01)	Acc@1  96.09 ( 94.86)	Acc@5 100.00 ( 99.70)
Epoch: [75][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.8755e-01 (1.6563e-01)	Acc@1  92.19 ( 94.61)	Acc@5  99.22 ( 99.73)
Epoch: [75][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.9749e-01 (1.6670e-01)	Acc@1  92.97 ( 94.47)	Acc@5 100.00 ( 99.77)
Epoch: [75][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 7.2043e-02 (1.6122e-01)	Acc@1  98.44 ( 94.68)	Acc@5  99.22 ( 99.73)
Epoch: [75][ 70/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4066e-01 (1.6058e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.69)
Epoch: [75][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6105e-01 (1.6220e-01)	Acc@1  92.19 ( 94.75)	Acc@5 100.00 ( 99.70)
Epoch: [75][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1274e-01 (1.6448e-01)	Acc@1  92.19 ( 94.69)	Acc@5  99.22 ( 99.66)
Epoch: [75][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 4.9953e-02 (1.6442e-01)	Acc@1  99.22 ( 94.74)	Acc@5 100.00 ( 99.67)
Epoch: [75][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2991e-01 (1.6478e-01)	Acc@1  93.75 ( 94.74)	Acc@5  97.66 ( 99.66)
Epoch: [75][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8228e-01 (1.6604e-01)	Acc@1  92.97 ( 94.72)	Acc@5 100.00 ( 99.66)
Epoch: [75][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4359e-01 (1.6529e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 ( 99.67)
Epoch: [75][140/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9842e-01 (1.6753e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.64)
Epoch: [75][150/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0936e-01 (1.6945e-01)	Acc@1  93.75 ( 94.65)	Acc@5  99.22 ( 99.64)
Epoch: [75][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3750e-01 (1.6915e-01)	Acc@1  96.88 ( 94.66)	Acc@5  99.22 ( 99.65)
Epoch: [75][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8957e-01 (1.7017e-01)	Acc@1  93.75 ( 94.59)	Acc@5  99.22 ( 99.64)
Epoch: [75][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1608e-01 (1.7022e-01)	Acc@1  95.31 ( 94.61)	Acc@5 100.00 ( 99.64)
Epoch: [75][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0490e-01 (1.7049e-01)	Acc@1  97.66 ( 94.63)	Acc@5 100.00 ( 99.63)
Epoch: [75][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2794e-01 (1.7067e-01)	Acc@1  94.53 ( 94.65)	Acc@5  99.22 ( 99.63)
Epoch: [75][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2583e-01 (1.6989e-01)	Acc@1  93.75 ( 94.66)	Acc@5  99.22 ( 99.64)
Epoch: [75][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6835e-01 (1.6956e-01)	Acc@1  88.28 ( 94.65)	Acc@5 100.00 ( 99.65)
Epoch: [75][230/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.8218e-01 (1.7022e-01)	Acc@1  92.19 ( 94.62)	Acc@5 100.00 ( 99.66)
Epoch: [75][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6827e-01 (1.7025e-01)	Acc@1  95.31 ( 94.61)	Acc@5 100.00 ( 99.65)
Epoch: [75][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3604e-01 (1.6946e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.65)
Epoch: [75][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8287e-01 (1.7055e-01)	Acc@1  93.75 ( 94.64)	Acc@5 100.00 ( 99.65)
Epoch: [75][270/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.3455e-01 (1.7042e-01)	Acc@1  92.97 ( 94.65)	Acc@5  99.22 ( 99.65)
Epoch: [75][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1890e-01 (1.7067e-01)	Acc@1  95.31 ( 94.64)	Acc@5 100.00 ( 99.66)
Epoch: [75][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6167e-01 (1.7045e-01)	Acc@1  94.53 ( 94.65)	Acc@5 100.00 ( 99.65)
Epoch: [75][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2257e-01 (1.7019e-01)	Acc@1  92.97 ( 94.66)	Acc@5 100.00 ( 99.66)
Epoch: [75][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7237e-01 (1.7004e-01)	Acc@1  92.97 ( 94.67)	Acc@5 100.00 ( 99.66)
Epoch: [75][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1425e-01 (1.6970e-01)	Acc@1  95.31 ( 94.67)	Acc@5 100.00 ( 99.66)
Epoch: [75][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3955e-01 (1.6978e-01)	Acc@1  93.75 ( 94.64)	Acc@5 100.00 ( 99.66)
Epoch: [75][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.6731e-02 (1.7008e-01)	Acc@1  96.88 ( 94.64)	Acc@5 100.00 ( 99.67)
Epoch: [75][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8258e-01 (1.7141e-01)	Acc@1  95.31 ( 94.59)	Acc@5  99.22 ( 99.66)
Epoch: [75][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0796e-01 (1.7171e-01)	Acc@1  94.53 ( 94.57)	Acc@5  99.22 ( 99.66)
Epoch: [75][370/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2815e-01 (1.7137e-01)	Acc@1  96.09 ( 94.59)	Acc@5 100.00 ( 99.66)
Epoch: [75][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4543e-01 (1.7108e-01)	Acc@1  95.31 ( 94.59)	Acc@5  99.22 ( 99.66)
Epoch: [75][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5550e-01 (1.7157e-01)	Acc@1  95.00 ( 94.58)	Acc@5 100.00 ( 99.66)
## e[75] optimizer.zero_grad (sum) time: 0.11793398857116699
## e[75]       loss.backward (sum) time: 2.1235344409942627
## e[75]      optimizer.step (sum) time: 0.7285788059234619
## epoch[75] training(only) time: 12.991403579711914
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.6212e+00 (1.6212e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 2.0772e+00 (1.7630e+00)	Acc@1  67.00 ( 66.91)	Acc@5  90.00 ( 87.91)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 1.7013e+00 (1.7132e+00)	Acc@1  61.00 ( 66.33)	Acc@5  88.00 ( 88.38)
Test: [ 30/100]	Time  0.018 ( 0.022)	Loss 1.8025e+00 (1.7100e+00)	Acc@1  59.00 ( 65.87)	Acc@5  84.00 ( 88.19)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.4371e+00 (1.6971e+00)	Acc@1  67.00 ( 66.17)	Acc@5  91.00 ( 88.41)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6158e+00 (1.7171e+00)	Acc@1  67.00 ( 66.29)	Acc@5  89.00 ( 88.24)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1082e+00 (1.6947e+00)	Acc@1  64.00 ( 66.79)	Acc@5  86.00 ( 88.36)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7134e+00 (1.6936e+00)	Acc@1  63.00 ( 66.58)	Acc@5  88.00 ( 88.52)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5617e+00 (1.6929e+00)	Acc@1  70.00 ( 66.62)	Acc@5  87.00 ( 88.46)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.3273e+00 (1.6758e+00)	Acc@1  60.00 ( 66.76)	Acc@5  85.00 ( 88.56)
 * Acc@1 66.840 Acc@5 88.680
### epoch[75] execution time: 14.953090906143188
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.177 ( 0.177)	Data  0.150 ( 0.150)	Loss 2.3208e-01 (2.3208e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [76][ 10/391]	Time  0.031 ( 0.045)	Data  0.001 ( 0.015)	Loss 1.3864e-01 (1.8735e-01)	Acc@1  94.53 ( 94.18)	Acc@5 100.00 ( 99.50)
Epoch: [76][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.4088e-01 (1.7788e-01)	Acc@1  95.31 ( 94.53)	Acc@5  99.22 ( 99.48)
Epoch: [76][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.4609e-01 (1.7757e-01)	Acc@1  93.75 ( 94.30)	Acc@5 100.00 ( 99.55)
Epoch: [76][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.1944e-01 (1.7265e-01)	Acc@1  98.44 ( 94.57)	Acc@5 100.00 ( 99.60)
Epoch: [76][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.5146e-01 (1.7512e-01)	Acc@1  94.53 ( 94.47)	Acc@5  99.22 ( 99.62)
Epoch: [76][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.0780e-01 (1.7609e-01)	Acc@1  93.75 ( 94.48)	Acc@5 100.00 ( 99.64)
Epoch: [76][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.3421e-01 (1.7836e-01)	Acc@1  91.41 ( 94.38)	Acc@5  98.44 ( 99.60)
Epoch: [76][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5374e-01 (1.7631e-01)	Acc@1  96.88 ( 94.47)	Acc@5  99.22 ( 99.59)
Epoch: [76][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 9.8117e-02 (1.7437e-01)	Acc@1  96.88 ( 94.52)	Acc@5 100.00 ( 99.59)
Epoch: [76][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5338e-01 (1.7354e-01)	Acc@1  95.31 ( 94.56)	Acc@5 100.00 ( 99.61)
Epoch: [76][110/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7068e-01 (1.7186e-01)	Acc@1  92.97 ( 94.61)	Acc@5 100.00 ( 99.61)
Epoch: [76][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.9825e-02 (1.7260e-01)	Acc@1  96.88 ( 94.56)	Acc@5 100.00 ( 99.63)
Epoch: [76][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4663e-01 (1.7340e-01)	Acc@1  94.53 ( 94.48)	Acc@5  99.22 ( 99.62)
Epoch: [76][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7166e-01 (1.7184e-01)	Acc@1  93.75 ( 94.54)	Acc@5 100.00 ( 99.64)
Epoch: [76][150/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3604e-01 (1.7281e-01)	Acc@1  96.09 ( 94.56)	Acc@5 100.00 ( 99.64)
Epoch: [76][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0119e-01 (1.7153e-01)	Acc@1  94.53 ( 94.60)	Acc@5  99.22 ( 99.65)
Epoch: [76][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9982e-01 (1.7194e-01)	Acc@1  93.75 ( 94.62)	Acc@5  98.44 ( 99.65)
Epoch: [76][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3684e-01 (1.7225e-01)	Acc@1  96.09 ( 94.61)	Acc@5  99.22 ( 99.64)
Epoch: [76][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0020e-01 (1.7368e-01)	Acc@1  96.09 ( 94.58)	Acc@5 100.00 ( 99.62)
Epoch: [76][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.8904e-02 (1.7350e-01)	Acc@1  97.66 ( 94.59)	Acc@5 100.00 ( 99.63)
Epoch: [76][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0936e-01 (1.7339e-01)	Acc@1  94.53 ( 94.61)	Acc@5 100.00 ( 99.64)
Epoch: [76][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4663e-01 (1.7360e-01)	Acc@1  95.31 ( 94.58)	Acc@5 100.00 ( 99.64)
Epoch: [76][230/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.2711e-01 (1.7396e-01)	Acc@1  96.88 ( 94.59)	Acc@5  99.22 ( 99.64)
Epoch: [76][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8573e-01 (1.7530e-01)	Acc@1  93.75 ( 94.55)	Acc@5 100.00 ( 99.63)
Epoch: [76][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0987e-01 (1.7434e-01)	Acc@1  96.88 ( 94.59)	Acc@5 100.00 ( 99.62)
Epoch: [76][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4680e-01 (1.7407e-01)	Acc@1  96.09 ( 94.59)	Acc@5 100.00 ( 99.63)
Epoch: [76][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2916e-01 (1.7422e-01)	Acc@1  96.09 ( 94.60)	Acc@5 100.00 ( 99.63)
Epoch: [76][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9527e-01 (1.7425e-01)	Acc@1  92.97 ( 94.61)	Acc@5 100.00 ( 99.63)
Epoch: [76][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5522e-01 (1.7448e-01)	Acc@1  96.09 ( 94.60)	Acc@5  99.22 ( 99.63)
Epoch: [76][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5564e-01 (1.7350e-01)	Acc@1  95.31 ( 94.61)	Acc@5 100.00 ( 99.65)
Epoch: [76][310/391]	Time  0.034 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.4344e-01 (1.7320e-01)	Acc@1  94.53 ( 94.63)	Acc@5 100.00 ( 99.65)
Epoch: [76][320/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4220e-01 (1.7256e-01)	Acc@1  96.09 ( 94.65)	Acc@5  99.22 ( 99.65)
Epoch: [76][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0404e-01 (1.7201e-01)	Acc@1  91.41 ( 94.63)	Acc@5  99.22 ( 99.66)
Epoch: [76][340/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2929e-01 (1.7243e-01)	Acc@1  92.19 ( 94.60)	Acc@5 100.00 ( 99.66)
Epoch: [76][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3216e-01 (1.7190e-01)	Acc@1  96.09 ( 94.61)	Acc@5 100.00 ( 99.67)
Epoch: [76][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.4467e-01 (1.7204e-01)	Acc@1  93.75 ( 94.62)	Acc@5  99.22 ( 99.66)
Epoch: [76][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5744e-01 (1.7163e-01)	Acc@1  94.53 ( 94.63)	Acc@5 100.00 ( 99.66)
Epoch: [76][380/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6916e-01 (1.7137e-01)	Acc@1  90.62 ( 94.62)	Acc@5 100.00 ( 99.66)
Epoch: [76][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5586e-01 (1.7192e-01)	Acc@1  95.00 ( 94.59)	Acc@5 100.00 ( 99.67)
## e[76] optimizer.zero_grad (sum) time: 0.11638832092285156
## e[76]       loss.backward (sum) time: 2.1059656143188477
## e[76]      optimizer.step (sum) time: 0.7381753921508789
## epoch[76] training(only) time: 12.963091373443604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.5806e+00 (1.5806e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.0921e+00 (1.7587e+00)	Acc@1  69.00 ( 67.18)	Acc@5  91.00 ( 88.00)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.8044e+00 (1.7193e+00)	Acc@1  62.00 ( 66.43)	Acc@5  87.00 ( 88.24)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.9251e+00 (1.7215e+00)	Acc@1  58.00 ( 65.74)	Acc@5  85.00 ( 88.13)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4603e+00 (1.7058e+00)	Acc@1  68.00 ( 65.95)	Acc@5  91.00 ( 88.34)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.6065e+00 (1.7265e+00)	Acc@1  67.00 ( 65.84)	Acc@5  88.00 ( 88.20)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1280e+00 (1.7055e+00)	Acc@1  65.00 ( 66.39)	Acc@5  84.00 ( 88.31)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7245e+00 (1.7049e+00)	Acc@1  67.00 ( 66.18)	Acc@5  87.00 ( 88.54)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6729e+00 (1.7013e+00)	Acc@1  66.00 ( 66.32)	Acc@5  87.00 ( 88.43)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.3145e+00 (1.6846e+00)	Acc@1  58.00 ( 66.44)	Acc@5  85.00 ( 88.57)
 * Acc@1 66.550 Acc@5 88.730
### epoch[76] execution time: 14.923178672790527
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.182 ( 0.182)	Data  0.153 ( 0.153)	Loss 1.6443e-01 (1.6443e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.2139e-01 (1.6385e-01)	Acc@1  95.31 ( 94.46)	Acc@5 100.00 ( 99.86)
Epoch: [77][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.2166e-01 (1.7782e-01)	Acc@1  96.88 ( 94.23)	Acc@5  99.22 ( 99.55)
Epoch: [77][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.4263e-01 (1.7548e-01)	Acc@1  95.31 ( 94.51)	Acc@5 100.00 ( 99.62)
Epoch: [77][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.8287e-01 (1.6877e-01)	Acc@1  94.53 ( 94.57)	Acc@5 100.00 ( 99.68)
Epoch: [77][ 50/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.005)	Loss 9.6846e-02 (1.6749e-01)	Acc@1  96.09 ( 94.68)	Acc@5 100.00 ( 99.68)
Epoch: [77][ 60/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.2111e-01 (1.6564e-01)	Acc@1  96.88 ( 94.84)	Acc@5 100.00 ( 99.68)
Epoch: [77][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4350e-01 (1.6901e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.67)
Epoch: [77][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6548e-01 (1.6524e-01)	Acc@1  94.53 ( 94.90)	Acc@5  99.22 ( 99.69)
Epoch: [77][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7127e-01 (1.6657e-01)	Acc@1  94.53 ( 94.81)	Acc@5 100.00 ( 99.69)
Epoch: [77][100/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4849e-01 (1.6756e-01)	Acc@1  94.53 ( 94.74)	Acc@5  99.22 ( 99.68)
Epoch: [77][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7161e-01 (1.6627e-01)	Acc@1  95.31 ( 94.79)	Acc@5  99.22 ( 99.70)
Epoch: [77][120/391]	Time  0.031 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.2948e-01 (1.6451e-01)	Acc@1  96.88 ( 94.87)	Acc@5 100.00 ( 99.71)
Epoch: [77][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4046e-01 (1.6376e-01)	Acc@1  95.31 ( 94.90)	Acc@5 100.00 ( 99.71)
Epoch: [77][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5079e-01 (1.6379e-01)	Acc@1  94.53 ( 94.91)	Acc@5 100.00 ( 99.71)
Epoch: [77][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7007e-01 (1.6513e-01)	Acc@1  94.53 ( 94.84)	Acc@5  99.22 ( 99.69)
Epoch: [77][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7721e-01 (1.6521e-01)	Acc@1  93.75 ( 94.80)	Acc@5 100.00 ( 99.69)
Epoch: [77][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6938e-01 (1.6581e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.70)
Epoch: [77][180/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9259e-01 (1.6587e-01)	Acc@1  90.62 ( 94.74)	Acc@5  99.22 ( 99.71)
Epoch: [77][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7765e-01 (1.6468e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.72)
Epoch: [77][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6754e-01 (1.6441e-01)	Acc@1  95.31 ( 94.80)	Acc@5  99.22 ( 99.71)
Epoch: [77][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3274e-01 (1.6520e-01)	Acc@1  95.31 ( 94.79)	Acc@5  99.22 ( 99.71)
Epoch: [77][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7433e-01 (1.6612e-01)	Acc@1  93.75 ( 94.75)	Acc@5  99.22 ( 99.71)
Epoch: [77][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3487e-01 (1.6786e-01)	Acc@1  96.09 ( 94.68)	Acc@5 100.00 ( 99.70)
Epoch: [77][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7439e-01 (1.6918e-01)	Acc@1  93.75 ( 94.60)	Acc@5 100.00 ( 99.69)
Epoch: [77][250/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1803e-01 (1.6827e-01)	Acc@1  95.31 ( 94.65)	Acc@5 100.00 ( 99.69)
Epoch: [77][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5719e-01 (1.6790e-01)	Acc@1  93.75 ( 94.66)	Acc@5  99.22 ( 99.69)
Epoch: [77][270/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1760e-01 (1.6719e-01)	Acc@1  96.09 ( 94.67)	Acc@5 100.00 ( 99.69)
Epoch: [77][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.5992e-02 (1.6744e-01)	Acc@1  96.09 ( 94.65)	Acc@5 100.00 ( 99.69)
Epoch: [77][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3572e-01 (1.6793e-01)	Acc@1  97.66 ( 94.63)	Acc@5 100.00 ( 99.69)
Epoch: [77][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.5528e-02 (1.6740e-01)	Acc@1  97.66 ( 94.66)	Acc@5 100.00 ( 99.69)
Epoch: [77][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8303e-01 (1.6742e-01)	Acc@1  95.31 ( 94.68)	Acc@5 100.00 ( 99.69)
Epoch: [77][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4872e-01 (1.6802e-01)	Acc@1  95.31 ( 94.66)	Acc@5 100.00 ( 99.69)
Epoch: [77][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.5564e-01 (1.6859e-01)	Acc@1  92.19 ( 94.64)	Acc@5 100.00 ( 99.69)
Epoch: [77][340/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.002)	Loss 9.8431e-02 (1.6761e-01)	Acc@1  96.09 ( 94.66)	Acc@5 100.00 ( 99.69)
Epoch: [77][350/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1763e-01 (1.6802e-01)	Acc@1  91.41 ( 94.64)	Acc@5  99.22 ( 99.70)
Epoch: [77][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1912e-01 (1.6725e-01)	Acc@1  96.09 ( 94.67)	Acc@5 100.00 ( 99.70)
Epoch: [77][370/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5934e-01 (1.6675e-01)	Acc@1  96.09 ( 94.69)	Acc@5 100.00 ( 99.70)
Epoch: [77][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0090e-01 (1.6699e-01)	Acc@1  96.09 ( 94.68)	Acc@5 100.00 ( 99.70)
Epoch: [77][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2810e-01 (1.6656e-01)	Acc@1  97.50 ( 94.69)	Acc@5 100.00 ( 99.70)
## e[77] optimizer.zero_grad (sum) time: 0.11728096008300781
## e[77]       loss.backward (sum) time: 2.0869863033294678
## e[77]      optimizer.step (sum) time: 0.7334449291229248
## epoch[77] training(only) time: 13.001255750656128
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.5747e+00 (1.5747e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.0664e+00 (1.7373e+00)	Acc@1  65.00 ( 66.82)	Acc@5  90.00 ( 88.18)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.8016e+00 (1.7072e+00)	Acc@1  63.00 ( 66.38)	Acc@5  85.00 ( 88.29)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8550e+00 (1.7054e+00)	Acc@1  58.00 ( 65.84)	Acc@5  86.00 ( 88.06)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.4148e+00 (1.6922e+00)	Acc@1  68.00 ( 65.98)	Acc@5  91.00 ( 88.39)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5601e+00 (1.7104e+00)	Acc@1  69.00 ( 66.08)	Acc@5  89.00 ( 88.27)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0803e+00 (1.6875e+00)	Acc@1  65.00 ( 66.54)	Acc@5  85.00 ( 88.43)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 1.7234e+00 (1.6892e+00)	Acc@1  67.00 ( 66.28)	Acc@5  87.00 ( 88.55)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6187e+00 (1.6867e+00)	Acc@1  69.00 ( 66.43)	Acc@5  86.00 ( 88.46)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2195e+00 (1.6686e+00)	Acc@1  62.00 ( 66.60)	Acc@5  85.00 ( 88.57)
 * Acc@1 66.650 Acc@5 88.760
### epoch[77] execution time: 14.941890716552734
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.182 ( 0.182)	Data  0.157 ( 0.157)	Loss 1.3484e-01 (1.3484e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [78][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.5372e-01 (1.6138e-01)	Acc@1  92.97 ( 94.67)	Acc@5 100.00 ( 99.64)
Epoch: [78][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.7883e-01 (1.6548e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 ( 99.55)
Epoch: [78][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.1909e-01 (1.6656e-01)	Acc@1  94.53 ( 94.56)	Acc@5  99.22 ( 99.65)
Epoch: [78][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.5532e-01 (1.6417e-01)	Acc@1  94.53 ( 94.76)	Acc@5  99.22 ( 99.64)
Epoch: [78][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.1018e-01 (1.6711e-01)	Acc@1  93.75 ( 94.67)	Acc@5  99.22 ( 99.62)
Epoch: [78][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.1060e-01 (1.6848e-01)	Acc@1  92.97 ( 94.63)	Acc@5  99.22 ( 99.63)
Epoch: [78][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 2.0668e-01 (1.7084e-01)	Acc@1  92.97 ( 94.60)	Acc@5  99.22 ( 99.59)
Epoch: [78][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5526e-01 (1.6960e-01)	Acc@1  94.53 ( 94.70)	Acc@5 100.00 ( 99.61)
Epoch: [78][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8723e-01 (1.6774e-01)	Acc@1  93.75 ( 94.74)	Acc@5 100.00 ( 99.63)
Epoch: [78][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.9984e-01 (1.6966e-01)	Acc@1  92.19 ( 94.61)	Acc@5  99.22 ( 99.63)
Epoch: [78][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8118e-01 (1.6670e-01)	Acc@1  93.75 ( 94.64)	Acc@5 100.00 ( 99.66)
Epoch: [78][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8684e-01 (1.6718e-01)	Acc@1  92.97 ( 94.63)	Acc@5 100.00 ( 99.66)
Epoch: [78][130/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1190e-01 (1.6574e-01)	Acc@1  96.88 ( 94.66)	Acc@5 100.00 ( 99.66)
Epoch: [78][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0264e-01 (1.6575e-01)	Acc@1  92.19 ( 94.68)	Acc@5  99.22 ( 99.65)
Epoch: [78][150/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1802e-01 (1.6422e-01)	Acc@1  92.97 ( 94.73)	Acc@5  99.22 ( 99.65)
Epoch: [78][160/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7662e-01 (1.6350e-01)	Acc@1  92.19 ( 94.78)	Acc@5 100.00 ( 99.66)
Epoch: [78][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5189e-01 (1.6359e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.66)
Epoch: [78][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1679e-01 (1.6188e-01)	Acc@1  96.09 ( 94.85)	Acc@5 100.00 ( 99.67)
Epoch: [78][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.8563e-02 (1.6157e-01)	Acc@1  95.31 ( 94.85)	Acc@5 100.00 ( 99.67)
Epoch: [78][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5991e-01 (1.6256e-01)	Acc@1  92.97 ( 94.81)	Acc@5  99.22 ( 99.65)
Epoch: [78][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0072e-01 (1.6185e-01)	Acc@1  93.75 ( 94.82)	Acc@5  99.22 ( 99.65)
Epoch: [78][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4098e-01 (1.6172e-01)	Acc@1  95.31 ( 94.82)	Acc@5 100.00 ( 99.66)
Epoch: [78][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5520e-01 (1.6244e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.66)
Epoch: [78][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0279e-01 (1.6289e-01)	Acc@1  96.88 ( 94.78)	Acc@5 100.00 ( 99.65)
Epoch: [78][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9549e-01 (1.6401e-01)	Acc@1  95.31 ( 94.75)	Acc@5  98.44 ( 99.65)
Epoch: [78][260/391]	Time  0.034 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.0337e-01 (1.6419e-01)	Acc@1  93.75 ( 94.73)	Acc@5  99.22 ( 99.65)
Epoch: [78][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3925e-01 (1.6455e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.65)
Epoch: [78][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9054e-01 (1.6413e-01)	Acc@1  96.09 ( 94.78)	Acc@5  99.22 ( 99.66)
Epoch: [78][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9408e-01 (1.6458e-01)	Acc@1  93.75 ( 94.79)	Acc@5  98.44 ( 99.65)
Epoch: [78][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.4538e-01 (1.6480e-01)	Acc@1  93.75 ( 94.77)	Acc@5  99.22 ( 99.65)
Epoch: [78][310/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.9337e-01 (1.6540e-01)	Acc@1  92.97 ( 94.75)	Acc@5  99.22 ( 99.65)
Epoch: [78][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.7903e-02 (1.6549e-01)	Acc@1  96.09 ( 94.75)	Acc@5 100.00 ( 99.65)
Epoch: [78][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2293e-01 (1.6533e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.65)
Epoch: [78][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1841e-01 (1.6495e-01)	Acc@1  94.53 ( 94.75)	Acc@5  98.44 ( 99.65)
Epoch: [78][350/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6469e-01 (1.6522e-01)	Acc@1  93.75 ( 94.74)	Acc@5 100.00 ( 99.65)
Epoch: [78][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3767e-01 (1.6561e-01)	Acc@1  90.62 ( 94.71)	Acc@5 100.00 ( 99.65)
Epoch: [78][370/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8212e-01 (1.6541e-01)	Acc@1  94.53 ( 94.73)	Acc@5 100.00 ( 99.66)
Epoch: [78][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3457e-01 (1.6509e-01)	Acc@1  96.88 ( 94.75)	Acc@5 100.00 ( 99.66)
Epoch: [78][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.9107e-01 (1.6484e-01)	Acc@1  92.50 ( 94.73)	Acc@5  98.75 ( 99.67)
## e[78] optimizer.zero_grad (sum) time: 0.11596798896789551
## e[78]       loss.backward (sum) time: 2.123722791671753
## e[78]      optimizer.step (sum) time: 0.7462966442108154
## epoch[78] training(only) time: 12.97153377532959
# Switched to evaluate mode...
Test: [  0/100]	Time  0.132 ( 0.132)	Loss 1.6275e+00 (1.6275e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.028)	Loss 2.1305e+00 (1.7870e+00)	Acc@1  68.00 ( 66.91)	Acc@5  91.00 ( 88.55)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.8049e+00 (1.7384e+00)	Acc@1  62.00 ( 66.52)	Acc@5  87.00 ( 88.57)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.8726e+00 (1.7362e+00)	Acc@1  57.00 ( 65.77)	Acc@5  84.00 ( 88.29)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4672e+00 (1.7273e+00)	Acc@1  68.00 ( 66.02)	Acc@5  90.00 ( 88.32)
Test: [ 50/100]	Time  0.017 ( 0.019)	Loss 1.5794e+00 (1.7487e+00)	Acc@1  69.00 ( 66.06)	Acc@5  89.00 ( 88.18)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1373e+00 (1.7238e+00)	Acc@1  65.00 ( 66.57)	Acc@5  86.00 ( 88.31)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7470e+00 (1.7223e+00)	Acc@1  65.00 ( 66.34)	Acc@5  88.00 ( 88.46)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6428e+00 (1.7196e+00)	Acc@1  68.00 ( 66.35)	Acc@5  86.00 ( 88.32)
Test: [ 90/100]	Time  0.017 ( 0.018)	Loss 2.3132e+00 (1.7013e+00)	Acc@1  58.00 ( 66.45)	Acc@5  86.00 ( 88.44)
 * Acc@1 66.490 Acc@5 88.640
### epoch[78] execution time: 14.922657012939453
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.174 ( 0.174)	Data  0.148 ( 0.148)	Loss 9.1228e-02 (9.1228e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.9472e-01 (1.4770e-01)	Acc@1  89.84 ( 95.31)	Acc@5 100.00 ( 99.79)
Epoch: [79][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 7.4836e-02 (1.4560e-01)	Acc@1  98.44 ( 95.42)	Acc@5 100.00 ( 99.81)
Epoch: [79][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 7.8694e-02 (1.4799e-01)	Acc@1  97.66 ( 95.44)	Acc@5 100.00 ( 99.85)
Epoch: [79][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.1772e-01 (1.4755e-01)	Acc@1  96.09 ( 95.48)	Acc@5 100.00 ( 99.79)
Epoch: [79][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.9920e-01 (1.4955e-01)	Acc@1  93.75 ( 95.44)	Acc@5  99.22 ( 99.65)
Epoch: [79][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4426e-01 (1.5128e-01)	Acc@1  93.75 ( 95.39)	Acc@5 100.00 ( 99.62)
Epoch: [79][ 70/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.6574e-01 (1.5496e-01)	Acc@1  96.88 ( 95.36)	Acc@5  99.22 ( 99.61)
Epoch: [79][ 80/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6978e-01 (1.5763e-01)	Acc@1  93.75 ( 95.24)	Acc@5  99.22 ( 99.62)
Epoch: [79][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1165e-01 (1.5991e-01)	Acc@1  93.75 ( 95.12)	Acc@5 100.00 ( 99.66)
Epoch: [79][100/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4439e-01 (1.5829e-01)	Acc@1  96.09 ( 95.11)	Acc@5 100.00 ( 99.68)
Epoch: [79][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.2430e-01 (1.6082e-01)	Acc@1  90.62 ( 94.98)	Acc@5 100.00 ( 99.69)
Epoch: [79][120/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7779e-01 (1.6423e-01)	Acc@1  92.97 ( 94.81)	Acc@5 100.00 ( 99.69)
Epoch: [79][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.5356e-01 (1.6524e-01)	Acc@1  92.19 ( 94.79)	Acc@5  99.22 ( 99.68)
Epoch: [79][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.5944e-01 (1.6542e-01)	Acc@1  92.19 ( 94.75)	Acc@5  97.66 ( 99.67)
Epoch: [79][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4501e-01 (1.6343e-01)	Acc@1  96.09 ( 94.82)	Acc@5 100.00 ( 99.68)
Epoch: [79][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.5569e-01 (1.6399e-01)	Acc@1  92.19 ( 94.81)	Acc@5  99.22 ( 99.68)
Epoch: [79][170/391]	Time  0.036 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5811e-01 (1.6610e-01)	Acc@1  94.53 ( 94.74)	Acc@5  99.22 ( 99.67)
Epoch: [79][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6276e-01 (1.6533e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.67)
Epoch: [79][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4077e-01 (1.6619e-01)	Acc@1  96.09 ( 94.72)	Acc@5 100.00 ( 99.67)
Epoch: [79][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0375e-01 (1.6451e-01)	Acc@1  96.88 ( 94.78)	Acc@5 100.00 ( 99.68)
Epoch: [79][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9727e-01 (1.6532e-01)	Acc@1  92.97 ( 94.78)	Acc@5  99.22 ( 99.68)
Epoch: [79][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9677e-01 (1.6486e-01)	Acc@1  94.53 ( 94.81)	Acc@5  99.22 ( 99.68)
Epoch: [79][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3815e-01 (1.6537e-01)	Acc@1  95.31 ( 94.81)	Acc@5 100.00 ( 99.67)
Epoch: [79][240/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2833e-01 (1.6468e-01)	Acc@1  96.88 ( 94.80)	Acc@5 100.00 ( 99.69)
Epoch: [79][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7746e-01 (1.6349e-01)	Acc@1  96.88 ( 94.86)	Acc@5 100.00 ( 99.69)
Epoch: [79][260/391]	Time  0.034 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.3314e-01 (1.6401e-01)	Acc@1  92.97 ( 94.84)	Acc@5  98.44 ( 99.69)
Epoch: [79][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7082e-01 (1.6349e-01)	Acc@1  93.75 ( 94.87)	Acc@5 100.00 ( 99.69)
Epoch: [79][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6188e-01 (1.6327e-01)	Acc@1  95.31 ( 94.89)	Acc@5 100.00 ( 99.69)
Epoch: [79][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8199e-01 (1.6308e-01)	Acc@1  93.75 ( 94.92)	Acc@5 100.00 ( 99.69)
Epoch: [79][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6755e-01 (1.6300e-01)	Acc@1  92.97 ( 94.92)	Acc@5 100.00 ( 99.69)
Epoch: [79][310/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2131e-01 (1.6328e-01)	Acc@1  96.88 ( 94.92)	Acc@5 100.00 ( 99.69)
Epoch: [79][320/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.002)	Loss 2.2330e-01 (1.6289e-01)	Acc@1  94.53 ( 94.94)	Acc@5  99.22 ( 99.69)
Epoch: [79][330/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9019e-01 (1.6322e-01)	Acc@1  94.53 ( 94.92)	Acc@5  98.44 ( 99.69)
Epoch: [79][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0467e-01 (1.6356e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.69)
Epoch: [79][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0021e-01 (1.6294e-01)	Acc@1  94.53 ( 94.94)	Acc@5  99.22 ( 99.68)
Epoch: [79][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1599e-01 (1.6339e-01)	Acc@1  92.19 ( 94.93)	Acc@5 100.00 ( 99.69)
Epoch: [79][370/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5830e-01 (1.6369e-01)	Acc@1  94.53 ( 94.92)	Acc@5 100.00 ( 99.68)
Epoch: [79][380/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1658e-01 (1.6340e-01)	Acc@1  95.31 ( 94.92)	Acc@5 100.00 ( 99.69)
Epoch: [79][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.6350e-01 (1.6408e-01)	Acc@1  90.00 ( 94.89)	Acc@5 100.00 ( 99.68)
## e[79] optimizer.zero_grad (sum) time: 0.11545634269714355
## e[79]       loss.backward (sum) time: 2.113220453262329
## e[79]      optimizer.step (sum) time: 0.7311215400695801
## epoch[79] training(only) time: 13.004040241241455
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.6151e+00 (1.6151e+00)	Acc@1  71.00 ( 71.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.1187e+00 (1.7810e+00)	Acc@1  67.00 ( 66.91)	Acc@5  90.00 ( 88.00)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.8802e+00 (1.7399e+00)	Acc@1  62.00 ( 66.52)	Acc@5  88.00 ( 88.38)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8676e+00 (1.7371e+00)	Acc@1  58.00 ( 65.84)	Acc@5  84.00 ( 88.10)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.4666e+00 (1.7268e+00)	Acc@1  68.00 ( 66.10)	Acc@5  90.00 ( 88.22)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.6248e+00 (1.7458e+00)	Acc@1  69.00 ( 66.22)	Acc@5  90.00 ( 88.16)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0972e+00 (1.7208e+00)	Acc@1  65.00 ( 66.70)	Acc@5  87.00 ( 88.36)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7410e+00 (1.7214e+00)	Acc@1  65.00 ( 66.38)	Acc@5  88.00 ( 88.48)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.5923e+00 (1.7178e+00)	Acc@1  71.00 ( 66.52)	Acc@5  86.00 ( 88.43)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.2932e+00 (1.7003e+00)	Acc@1  62.00 ( 66.60)	Acc@5  85.00 ( 88.56)
 * Acc@1 66.650 Acc@5 88.730
### epoch[79] execution time: 14.967015027999878
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.169 ( 0.169)	Data  0.144 ( 0.144)	Loss 2.2960e-01 (2.2960e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 7.6433e-02 (1.5369e-01)	Acc@1  96.88 ( 94.53)	Acc@5 100.00 ( 99.86)
Epoch: [80][ 20/391]	Time  0.032 ( 0.039)	Data  0.002 ( 0.009)	Loss 1.8979e-01 (1.5716e-01)	Acc@1  92.19 ( 94.83)	Acc@5 100.00 ( 99.67)
Epoch: [80][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.0461e-01 (1.5652e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.70)
Epoch: [80][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.005)	Loss 2.8735e-01 (1.6452e-01)	Acc@1  92.19 ( 94.66)	Acc@5  98.44 ( 99.64)
Epoch: [80][ 50/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.6856e-01 (1.6270e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.71)
Epoch: [80][ 60/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.004)	Loss 1.5850e-01 (1.6091e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.71)
Epoch: [80][ 70/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1386e-01 (1.6049e-01)	Acc@1  92.19 ( 94.83)	Acc@5  99.22 ( 99.69)
Epoch: [80][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.3086e-01 (1.6200e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.69)
Epoch: [80][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.2850e-01 (1.5996e-01)	Acc@1  96.88 ( 94.78)	Acc@5 100.00 ( 99.72)
Epoch: [80][100/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.1127e-01 (1.6044e-01)	Acc@1  91.41 ( 94.78)	Acc@5 100.00 ( 99.72)
Epoch: [80][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8290e-01 (1.6162e-01)	Acc@1  93.75 ( 94.74)	Acc@5 100.00 ( 99.74)
Epoch: [80][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6089e-01 (1.6238e-01)	Acc@1  95.31 ( 94.69)	Acc@5  99.22 ( 99.74)
Epoch: [80][130/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2174e-01 (1.6052e-01)	Acc@1  96.09 ( 94.79)	Acc@5 100.00 ( 99.76)
Epoch: [80][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5107e-01 (1.6049e-01)	Acc@1  96.09 ( 94.78)	Acc@5  99.22 ( 99.75)
Epoch: [80][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3137e-01 (1.6006e-01)	Acc@1  96.88 ( 94.80)	Acc@5  98.44 ( 99.74)
Epoch: [80][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6525e-01 (1.6009e-01)	Acc@1  93.75 ( 94.80)	Acc@5 100.00 ( 99.76)
Epoch: [80][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.4562e-02 (1.5925e-01)	Acc@1  96.88 ( 94.83)	Acc@5 100.00 ( 99.76)
Epoch: [80][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0719e-01 (1.5973e-01)	Acc@1  95.31 ( 94.83)	Acc@5 100.00 ( 99.76)
Epoch: [80][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4428e-01 (1.6043e-01)	Acc@1  95.31 ( 94.77)	Acc@5 100.00 ( 99.77)
Epoch: [80][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6371e-01 (1.6046e-01)	Acc@1  92.97 ( 94.75)	Acc@5 100.00 ( 99.77)
Epoch: [80][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0504e-01 (1.6125e-01)	Acc@1  91.41 ( 94.72)	Acc@5  99.22 ( 99.75)
Epoch: [80][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2807e-01 (1.6211e-01)	Acc@1  94.53 ( 94.70)	Acc@5 100.00 ( 99.75)
Epoch: [80][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0690e-01 (1.6233e-01)	Acc@1  96.88 ( 94.69)	Acc@5 100.00 ( 99.75)
Epoch: [80][240/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2111e-01 (1.6251e-01)	Acc@1  95.31 ( 94.68)	Acc@5 100.00 ( 99.75)
Epoch: [80][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2324e-01 (1.6184e-01)	Acc@1  96.88 ( 94.73)	Acc@5 100.00 ( 99.74)
Epoch: [80][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3333e-01 (1.6205e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.74)
Epoch: [80][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.5002e-02 (1.6200e-01)	Acc@1  98.44 ( 94.72)	Acc@5 100.00 ( 99.74)
Epoch: [80][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4202e-01 (1.6244e-01)	Acc@1  96.88 ( 94.73)	Acc@5 100.00 ( 99.74)
Epoch: [80][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1589e-01 (1.6291e-01)	Acc@1  93.75 ( 94.72)	Acc@5  99.22 ( 99.74)
Epoch: [80][300/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3833e-01 (1.6330e-01)	Acc@1  97.66 ( 94.69)	Acc@5 100.00 ( 99.75)
Epoch: [80][310/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9439e-01 (1.6295e-01)	Acc@1  95.31 ( 94.71)	Acc@5 100.00 ( 99.75)
Epoch: [80][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3724e-01 (1.6312e-01)	Acc@1  92.97 ( 94.71)	Acc@5  99.22 ( 99.75)
Epoch: [80][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3636e-01 (1.6313e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.74)
Epoch: [80][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5644e-01 (1.6309e-01)	Acc@1  94.53 ( 94.74)	Acc@5  99.22 ( 99.74)
Epoch: [80][350/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 6.8253e-02 (1.6301e-01)	Acc@1  99.22 ( 94.75)	Acc@5 100.00 ( 99.73)
Epoch: [80][360/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6732e-01 (1.6318e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 ( 99.74)
Epoch: [80][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2275e-01 (1.6321e-01)	Acc@1  96.09 ( 94.73)	Acc@5 100.00 ( 99.73)
Epoch: [80][380/391]	Time  0.037 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1909e-01 (1.6367e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.73)
Epoch: [80][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2615e-01 (1.6341e-01)	Acc@1  91.25 ( 94.72)	Acc@5 100.00 ( 99.73)
## e[80] optimizer.zero_grad (sum) time: 0.11701130867004395
## e[80]       loss.backward (sum) time: 2.113600254058838
## e[80]      optimizer.step (sum) time: 0.7422478199005127
## epoch[80] training(only) time: 13.005161046981812
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.6527e+00 (1.6527e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.1355e+00 (1.7799e+00)	Acc@1  65.00 ( 66.82)	Acc@5  89.00 ( 88.36)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.8556e+00 (1.7539e+00)	Acc@1  62.00 ( 66.14)	Acc@5  87.00 ( 88.48)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8349e+00 (1.7473e+00)	Acc@1  61.00 ( 65.65)	Acc@5  85.00 ( 88.29)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.4723e+00 (1.7326e+00)	Acc@1  69.00 ( 66.05)	Acc@5  91.00 ( 88.59)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6039e+00 (1.7510e+00)	Acc@1  68.00 ( 66.16)	Acc@5  89.00 ( 88.47)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.0918e+00 (1.7268e+00)	Acc@1  62.00 ( 66.62)	Acc@5  87.00 ( 88.61)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7826e+00 (1.7262e+00)	Acc@1  66.00 ( 66.39)	Acc@5  87.00 ( 88.72)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.6826e+00 (1.7241e+00)	Acc@1  70.00 ( 66.51)	Acc@5  85.00 ( 88.62)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.3705e+00 (1.7068e+00)	Acc@1  60.00 ( 66.69)	Acc@5  84.00 ( 88.71)
 * Acc@1 66.720 Acc@5 88.840
### epoch[80] execution time: 14.950525522232056
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.177 ( 0.177)	Data  0.147 ( 0.147)	Loss 9.2746e-02 (9.2746e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.033 ( 0.045)	Data  0.001 ( 0.015)	Loss 2.8203e-01 (1.8228e-01)	Acc@1  88.28 ( 94.32)	Acc@5  99.22 ( 99.50)
Epoch: [81][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.8847e-01 (1.6987e-01)	Acc@1  95.31 ( 94.68)	Acc@5  98.44 ( 99.55)
Epoch: [81][ 30/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.007)	Loss 3.3723e-01 (1.7184e-01)	Acc@1  90.62 ( 94.86)	Acc@5  99.22 ( 99.55)
Epoch: [81][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.5073e-01 (1.6214e-01)	Acc@1  96.09 ( 95.12)	Acc@5 100.00 ( 99.58)
Epoch: [81][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.0226e-01 (1.6262e-01)	Acc@1  96.09 ( 95.07)	Acc@5 100.00 ( 99.57)
Epoch: [81][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.8117e-01 (1.5926e-01)	Acc@1  94.53 ( 95.11)	Acc@5  99.22 ( 99.62)
Epoch: [81][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.0354e-01 (1.5833e-01)	Acc@1  96.88 ( 95.21)	Acc@5 100.00 ( 99.63)
Epoch: [81][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5959e-01 (1.5721e-01)	Acc@1  94.53 ( 95.25)	Acc@5 100.00 ( 99.65)
Epoch: [81][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7088e-01 (1.6053e-01)	Acc@1  94.53 ( 95.12)	Acc@5  99.22 ( 99.64)
Epoch: [81][100/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8358e-01 (1.6030e-01)	Acc@1  93.75 ( 95.13)	Acc@5 100.00 ( 99.64)
Epoch: [81][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4980e-01 (1.5914e-01)	Acc@1  95.31 ( 95.13)	Acc@5 100.00 ( 99.65)
Epoch: [81][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 6.4179e-02 (1.5905e-01)	Acc@1  97.66 ( 95.16)	Acc@5 100.00 ( 99.62)
Epoch: [81][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2557e-01 (1.5878e-01)	Acc@1  96.88 ( 95.15)	Acc@5 100.00 ( 99.64)
Epoch: [81][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0708e-01 (1.5909e-01)	Acc@1  94.53 ( 95.16)	Acc@5 100.00 ( 99.65)
Epoch: [81][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5463e-01 (1.6063e-01)	Acc@1  94.53 ( 95.10)	Acc@5  99.22 ( 99.65)
Epoch: [81][160/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6753e-01 (1.6189e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.66)
Epoch: [81][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9993e-01 (1.6037e-01)	Acc@1  93.75 ( 95.08)	Acc@5 100.00 ( 99.67)
Epoch: [81][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.2374e-02 (1.5978e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.67)
Epoch: [81][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5497e-01 (1.5883e-01)	Acc@1  94.53 ( 95.11)	Acc@5 100.00 ( 99.68)
Epoch: [81][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1076e-01 (1.5958e-01)	Acc@1  91.41 ( 95.08)	Acc@5 100.00 ( 99.67)
Epoch: [81][210/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4144e-01 (1.6012e-01)	Acc@1  94.53 ( 95.05)	Acc@5 100.00 ( 99.68)
Epoch: [81][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2213e-01 (1.5857e-01)	Acc@1  95.31 ( 95.09)	Acc@5 100.00 ( 99.69)
Epoch: [81][230/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.9517e-01 (1.5855e-01)	Acc@1  93.75 ( 95.09)	Acc@5 100.00 ( 99.68)
Epoch: [81][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0218e-01 (1.5767e-01)	Acc@1  96.09 ( 95.10)	Acc@5 100.00 ( 99.69)
Epoch: [81][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 5.2252e-02 (1.5685e-01)	Acc@1  99.22 ( 95.14)	Acc@5 100.00 ( 99.69)
Epoch: [81][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2370e-01 (1.5750e-01)	Acc@1  92.97 ( 95.11)	Acc@5  99.22 ( 99.68)
Epoch: [81][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1130e-01 (1.5691e-01)	Acc@1  96.09 ( 95.13)	Acc@5 100.00 ( 99.69)
Epoch: [81][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.6508e-01 (1.5704e-01)	Acc@1  92.19 ( 95.11)	Acc@5  99.22 ( 99.69)
Epoch: [81][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3053e-01 (1.5774e-01)	Acc@1  92.19 ( 95.07)	Acc@5  99.22 ( 99.69)
Epoch: [81][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6031e-01 (1.5713e-01)	Acc@1  94.53 ( 95.08)	Acc@5  99.22 ( 99.69)
Epoch: [81][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8063e-01 (1.5763e-01)	Acc@1  96.88 ( 95.07)	Acc@5  98.44 ( 99.69)
Epoch: [81][320/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.4566e-01 (1.5777e-01)	Acc@1  94.53 ( 95.06)	Acc@5 100.00 ( 99.69)
Epoch: [81][330/391]	Time  0.030 ( 0.033)	Data  0.002 ( 0.002)	Loss 7.8693e-02 (1.5845e-01)	Acc@1  97.66 ( 95.04)	Acc@5 100.00 ( 99.68)
Epoch: [81][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3313e-01 (1.5860e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.68)
Epoch: [81][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0137e-01 (1.5885e-01)	Acc@1  91.41 ( 95.00)	Acc@5 100.00 ( 99.68)
Epoch: [81][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2327e-01 (1.5908e-01)	Acc@1  93.75 ( 94.98)	Acc@5  99.22 ( 99.68)
Epoch: [81][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3562e-01 (1.5945e-01)	Acc@1  96.88 ( 94.97)	Acc@5  99.22 ( 99.67)
Epoch: [81][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 5.9486e-02 (1.5912e-01)	Acc@1  98.44 ( 94.99)	Acc@5 100.00 ( 99.68)
Epoch: [81][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.8301e-02 (1.5912e-01)	Acc@1  97.50 ( 94.99)	Acc@5 100.00 ( 99.67)
## e[81] optimizer.zero_grad (sum) time: 0.11692667007446289
## e[81]       loss.backward (sum) time: 2.148475408554077
## e[81]      optimizer.step (sum) time: 0.7363495826721191
## epoch[81] training(only) time: 13.020605564117432
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.6069e+00 (1.6069e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.016 ( 0.029)	Loss 2.2252e+00 (1.8185e+00)	Acc@1  65.00 ( 66.73)	Acc@5  89.00 ( 87.82)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.8549e+00 (1.7743e+00)	Acc@1  61.00 ( 66.14)	Acc@5  89.00 ( 88.33)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.8607e+00 (1.7702e+00)	Acc@1  61.00 ( 65.61)	Acc@5  89.00 ( 88.32)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4084e+00 (1.7563e+00)	Acc@1  69.00 ( 65.98)	Acc@5  91.00 ( 88.54)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6134e+00 (1.7754e+00)	Acc@1  70.00 ( 66.00)	Acc@5  89.00 ( 88.33)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1011e+00 (1.7503e+00)	Acc@1  63.00 ( 66.44)	Acc@5  88.00 ( 88.54)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7422e+00 (1.7514e+00)	Acc@1  66.00 ( 66.25)	Acc@5  86.00 ( 88.61)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6575e+00 (1.7501e+00)	Acc@1  70.00 ( 66.30)	Acc@5  86.00 ( 88.48)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.4376e+00 (1.7316e+00)	Acc@1  59.00 ( 66.47)	Acc@5  87.00 ( 88.68)
 * Acc@1 66.530 Acc@5 88.840
### epoch[81] execution time: 14.977933883666992
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.177 ( 0.177)	Data  0.148 ( 0.148)	Loss 9.6406e-02 (9.6406e-02)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.033 ( 0.045)	Data  0.001 ( 0.015)	Loss 1.5201e-01 (1.5108e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.64)
Epoch: [82][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.5076e-01 (1.5582e-01)	Acc@1  94.53 ( 94.83)	Acc@5  99.22 ( 99.70)
Epoch: [82][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.1076e-01 (1.5403e-01)	Acc@1  96.09 ( 95.04)	Acc@5 100.00 ( 99.70)
Epoch: [82][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.0765e-01 (1.5046e-01)	Acc@1  97.66 ( 95.24)	Acc@5 100.00 ( 99.75)
Epoch: [82][ 50/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.6357e-01 (1.5209e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.77)
Epoch: [82][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.7701e-01 (1.5443e-01)	Acc@1  94.53 ( 94.84)	Acc@5 100.00 ( 99.77)
Epoch: [82][ 70/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6152e-01 (1.5285e-01)	Acc@1  95.31 ( 94.95)	Acc@5  99.22 ( 99.78)
Epoch: [82][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.2488e-01 (1.5454e-01)	Acc@1  95.31 ( 94.88)	Acc@5 100.00 ( 99.77)
Epoch: [82][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.9040e-01 (1.5200e-01)	Acc@1  94.53 ( 94.99)	Acc@5  99.22 ( 99.78)
Epoch: [82][100/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6211e-01 (1.5143e-01)	Acc@1  96.09 ( 95.03)	Acc@5  99.22 ( 99.78)
Epoch: [82][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6276e-01 (1.5361e-01)	Acc@1  93.75 ( 94.99)	Acc@5 100.00 ( 99.78)
Epoch: [82][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9925e-01 (1.5461e-01)	Acc@1  92.19 ( 94.96)	Acc@5  98.44 ( 99.77)
Epoch: [82][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.3735e-01 (1.5450e-01)	Acc@1  92.19 ( 94.94)	Acc@5  99.22 ( 99.76)
Epoch: [82][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8191e-01 (1.5219e-01)	Acc@1  93.75 ( 95.06)	Acc@5  99.22 ( 99.77)
Epoch: [82][150/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3307e-01 (1.5313e-01)	Acc@1  99.22 ( 95.05)	Acc@5  99.22 ( 99.76)
Epoch: [82][160/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1803e-01 (1.5565e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.75)
Epoch: [82][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6926e-01 (1.5552e-01)	Acc@1  92.97 ( 94.94)	Acc@5 100.00 ( 99.75)
Epoch: [82][180/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5763e-01 (1.5678e-01)	Acc@1  93.75 ( 94.89)	Acc@5  99.22 ( 99.72)
Epoch: [82][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3561e-01 (1.5722e-01)	Acc@1  96.09 ( 94.89)	Acc@5  98.44 ( 99.72)
Epoch: [82][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5402e-01 (1.5747e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.72)
Epoch: [82][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9213e-01 (1.5886e-01)	Acc@1  93.75 ( 94.83)	Acc@5  99.22 ( 99.71)
Epoch: [82][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8911e-01 (1.5832e-01)	Acc@1  94.53 ( 94.85)	Acc@5 100.00 ( 99.72)
Epoch: [82][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4288e-01 (1.5886e-01)	Acc@1  97.66 ( 94.84)	Acc@5  99.22 ( 99.72)
Epoch: [82][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8382e-01 (1.5791e-01)	Acc@1  92.19 ( 94.88)	Acc@5  99.22 ( 99.72)
Epoch: [82][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4002e-01 (1.5693e-01)	Acc@1  92.97 ( 94.90)	Acc@5 100.00 ( 99.73)
Epoch: [82][260/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1860e-01 (1.5625e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.74)
Epoch: [82][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1732e-01 (1.5614e-01)	Acc@1  92.97 ( 94.96)	Acc@5 100.00 ( 99.74)
Epoch: [82][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.1272e-02 (1.5578e-01)	Acc@1  97.66 ( 94.96)	Acc@5 100.00 ( 99.74)
Epoch: [82][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3881e-01 (1.5633e-01)	Acc@1  93.75 ( 94.95)	Acc@5 100.00 ( 99.74)
Epoch: [82][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.0292e-01 (1.5633e-01)	Acc@1  93.75 ( 94.95)	Acc@5  99.22 ( 99.74)
Epoch: [82][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9695e-01 (1.5642e-01)	Acc@1  93.75 ( 94.94)	Acc@5  99.22 ( 99.74)
Epoch: [82][320/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2363e-01 (1.5668e-01)	Acc@1  96.09 ( 94.93)	Acc@5  99.22 ( 99.74)
Epoch: [82][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0583e-01 (1.5572e-01)	Acc@1  97.66 ( 94.95)	Acc@5  99.22 ( 99.74)
Epoch: [82][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3834e-01 (1.5527e-01)	Acc@1  96.09 ( 94.96)	Acc@5 100.00 ( 99.74)
Epoch: [82][350/391]	Time  0.032 ( 0.033)	Data  0.000 ( 0.002)	Loss 1.8674e-01 (1.5623e-01)	Acc@1  92.97 ( 94.93)	Acc@5 100.00 ( 99.73)
Epoch: [82][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9790e-01 (1.5605e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.73)
Epoch: [82][370/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.7968e-01 (1.5688e-01)	Acc@1  89.84 ( 94.91)	Acc@5  99.22 ( 99.73)
Epoch: [82][380/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9836e-01 (1.5851e-01)	Acc@1  95.31 ( 94.86)	Acc@5  99.22 ( 99.72)
Epoch: [82][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.8076e-02 (1.5956e-01)	Acc@1  96.25 ( 94.83)	Acc@5 100.00 ( 99.72)
## e[82] optimizer.zero_grad (sum) time: 0.1176137924194336
## e[82]       loss.backward (sum) time: 2.1552672386169434
## e[82]      optimizer.step (sum) time: 0.7368650436401367
## epoch[82] training(only) time: 13.012154817581177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.6162e+00 (1.6162e+00)	Acc@1  71.00 ( 71.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.1858e+00 (1.8155e+00)	Acc@1  66.00 ( 66.91)	Acc@5  89.00 ( 88.00)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7707e+00 (1.7649e+00)	Acc@1  63.00 ( 66.57)	Acc@5  87.00 ( 88.24)
Test: [ 30/100]	Time  0.021 ( 0.022)	Loss 1.8692e+00 (1.7600e+00)	Acc@1  60.00 ( 65.97)	Acc@5  87.00 ( 88.16)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4365e+00 (1.7395e+00)	Acc@1  69.00 ( 66.32)	Acc@5  91.00 ( 88.37)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6773e+00 (1.7584e+00)	Acc@1  68.00 ( 66.18)	Acc@5  90.00 ( 88.31)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1285e+00 (1.7347e+00)	Acc@1  63.00 ( 66.52)	Acc@5  85.00 ( 88.41)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.8804e+00 (1.7362e+00)	Acc@1  65.00 ( 66.41)	Acc@5  87.00 ( 88.54)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6537e+00 (1.7358e+00)	Acc@1  71.00 ( 66.48)	Acc@5  85.00 ( 88.40)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.3388e+00 (1.7166e+00)	Acc@1  59.00 ( 66.65)	Acc@5  83.00 ( 88.51)
 * Acc@1 66.670 Acc@5 88.670
### epoch[82] execution time: 14.964364528656006
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.181 ( 0.181)	Data  0.153 ( 0.153)	Loss 1.2046e-01 (1.2046e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.034 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.7440e-01 (1.5780e-01)	Acc@1  93.75 ( 94.74)	Acc@5 100.00 ( 99.50)
Epoch: [83][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.6791e-01 (1.6345e-01)	Acc@1  93.75 ( 94.68)	Acc@5 100.00 ( 99.55)
Epoch: [83][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.5919e-01 (1.6406e-01)	Acc@1  93.75 ( 94.73)	Acc@5 100.00 ( 99.52)
Epoch: [83][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.9892e-01 (1.6886e-01)	Acc@1  93.75 ( 94.55)	Acc@5  99.22 ( 99.56)
Epoch: [83][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.1651e-01 (1.6594e-01)	Acc@1  94.53 ( 94.70)	Acc@5 100.00 ( 99.59)
Epoch: [83][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4412e-01 (1.6185e-01)	Acc@1  96.09 ( 94.92)	Acc@5 100.00 ( 99.65)
Epoch: [83][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 8.5872e-02 (1.5794e-01)	Acc@1  97.66 ( 95.04)	Acc@5 100.00 ( 99.66)
Epoch: [83][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.3656e-01 (1.5499e-01)	Acc@1  98.44 ( 95.10)	Acc@5  99.22 ( 99.67)
Epoch: [83][ 90/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4054e-01 (1.5808e-01)	Acc@1  97.66 ( 94.95)	Acc@5  98.44 ( 99.65)
Epoch: [83][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6876e-01 (1.5889e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.68)
Epoch: [83][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7388e-01 (1.5904e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.68)
Epoch: [83][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4813e-01 (1.5945e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.67)
Epoch: [83][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3987e-01 (1.5868e-01)	Acc@1  96.09 ( 94.97)	Acc@5 100.00 ( 99.68)
Epoch: [83][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1855e-01 (1.5871e-01)	Acc@1  96.09 ( 94.98)	Acc@5 100.00 ( 99.68)
Epoch: [83][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5789e-01 (1.5753e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.71)
Epoch: [83][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2618e-01 (1.5605e-01)	Acc@1  96.09 ( 95.09)	Acc@5  99.22 ( 99.71)
Epoch: [83][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6421e-01 (1.5599e-01)	Acc@1  95.31 ( 95.13)	Acc@5  99.22 ( 99.69)
Epoch: [83][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1936e-01 (1.5664e-01)	Acc@1  92.19 ( 95.11)	Acc@5  99.22 ( 99.68)
Epoch: [83][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3542e-01 (1.5726e-01)	Acc@1  92.19 ( 95.11)	Acc@5 100.00 ( 99.70)
Epoch: [83][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0960e-01 (1.5719e-01)	Acc@1  96.88 ( 95.13)	Acc@5  99.22 ( 99.70)
Epoch: [83][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9683e-01 (1.5799e-01)	Acc@1  90.62 ( 95.06)	Acc@5 100.00 ( 99.70)
Epoch: [83][220/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8574e-01 (1.5777e-01)	Acc@1  94.53 ( 95.05)	Acc@5  99.22 ( 99.70)
Epoch: [83][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.7310e-02 (1.5573e-01)	Acc@1  98.44 ( 95.13)	Acc@5 100.00 ( 99.71)
Epoch: [83][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.2816e-02 (1.5566e-01)	Acc@1  96.88 ( 95.11)	Acc@5 100.00 ( 99.71)
Epoch: [83][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1623e-01 (1.5510e-01)	Acc@1  92.97 ( 95.12)	Acc@5 100.00 ( 99.71)
Epoch: [83][260/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.003)	Loss 2.9304e-01 (1.5547e-01)	Acc@1  91.41 ( 95.09)	Acc@5  99.22 ( 99.71)
Epoch: [83][270/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0448e-01 (1.5524e-01)	Acc@1  96.09 ( 95.08)	Acc@5 100.00 ( 99.72)
Epoch: [83][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3595e-01 (1.5498e-01)	Acc@1  94.53 ( 95.08)	Acc@5 100.00 ( 99.72)
Epoch: [83][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2582e-01 (1.5530e-01)	Acc@1  92.97 ( 95.07)	Acc@5  99.22 ( 99.72)
Epoch: [83][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3873e-01 (1.5575e-01)	Acc@1  95.31 ( 95.04)	Acc@5 100.00 ( 99.72)
Epoch: [83][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8833e-01 (1.5604e-01)	Acc@1  93.75 ( 95.02)	Acc@5  99.22 ( 99.72)
Epoch: [83][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6170e-01 (1.5599e-01)	Acc@1  94.53 ( 95.02)	Acc@5  99.22 ( 99.72)
Epoch: [83][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7159e-01 (1.5558e-01)	Acc@1  93.75 ( 95.04)	Acc@5 100.00 ( 99.72)
Epoch: [83][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.8421e-01 (1.5636e-01)	Acc@1  92.97 ( 95.00)	Acc@5 100.00 ( 99.72)
Epoch: [83][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7180e-01 (1.5720e-01)	Acc@1  93.75 ( 94.97)	Acc@5 100.00 ( 99.71)
Epoch: [83][360/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8433e-01 (1.5729e-01)	Acc@1  92.19 ( 94.95)	Acc@5  99.22 ( 99.71)
Epoch: [83][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3944e-01 (1.5708e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.71)
Epoch: [83][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5561e-01 (1.5775e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.71)
Epoch: [83][390/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6899e-01 (1.5726e-01)	Acc@1  93.75 ( 94.95)	Acc@5 100.00 ( 99.71)
## e[83] optimizer.zero_grad (sum) time: 0.11704516410827637
## e[83]       loss.backward (sum) time: 2.1234049797058105
## e[83]      optimizer.step (sum) time: 0.7424287796020508
## epoch[83] training(only) time: 12.982173919677734
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.6022e+00 (1.6022e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.1859e+00 (1.8250e+00)	Acc@1  65.00 ( 66.45)	Acc@5  89.00 ( 87.73)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.7972e+00 (1.7669e+00)	Acc@1  62.00 ( 66.38)	Acc@5  90.00 ( 88.24)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8487e+00 (1.7651e+00)	Acc@1  62.00 ( 65.58)	Acc@5  88.00 ( 88.00)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4570e+00 (1.7501e+00)	Acc@1  68.00 ( 66.07)	Acc@5  92.00 ( 88.24)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6120e+00 (1.7645e+00)	Acc@1  68.00 ( 66.12)	Acc@5  89.00 ( 88.08)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1388e+00 (1.7404e+00)	Acc@1  63.00 ( 66.54)	Acc@5  85.00 ( 88.30)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7416e+00 (1.7410e+00)	Acc@1  66.00 ( 66.35)	Acc@5  88.00 ( 88.44)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.6395e+00 (1.7396e+00)	Acc@1  72.00 ( 66.53)	Acc@5  86.00 ( 88.28)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.3400e+00 (1.7215e+00)	Acc@1  62.00 ( 66.71)	Acc@5  85.00 ( 88.40)
 * Acc@1 66.760 Acc@5 88.590
### epoch[83] execution time: 14.92009711265564
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.178 ( 0.178)	Data  0.150 ( 0.150)	Loss 1.3817e-01 (1.3817e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.4290e-01 (1.3137e-01)	Acc@1  95.31 ( 94.89)	Acc@5  99.22 ( 99.93)
Epoch: [84][ 20/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.009)	Loss 6.1642e-02 (1.4329e-01)	Acc@1  98.44 ( 95.13)	Acc@5 100.00 ( 99.81)
Epoch: [84][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.8077e-01 (1.4703e-01)	Acc@1  95.31 ( 95.19)	Acc@5  98.44 ( 99.75)
Epoch: [84][ 40/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.1440e-01 (1.4913e-01)	Acc@1  95.31 ( 94.91)	Acc@5 100.00 ( 99.77)
Epoch: [84][ 50/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.005)	Loss 2.0332e-01 (1.5131e-01)	Acc@1  92.97 ( 94.85)	Acc@5 100.00 ( 99.80)
Epoch: [84][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.1275e-01 (1.5141e-01)	Acc@1  96.88 ( 94.88)	Acc@5 100.00 ( 99.80)
Epoch: [84][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.5815e-01 (1.5081e-01)	Acc@1  95.31 ( 94.86)	Acc@5 100.00 ( 99.81)
Epoch: [84][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5038e-01 (1.5129e-01)	Acc@1  97.66 ( 94.92)	Acc@5 100.00 ( 99.80)
Epoch: [84][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4737e-01 (1.5101e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.80)
Epoch: [84][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.0712e-01 (1.5180e-01)	Acc@1  96.09 ( 95.03)	Acc@5 100.00 ( 99.80)
Epoch: [84][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.4722e-02 (1.5054e-01)	Acc@1  99.22 ( 95.07)	Acc@5 100.00 ( 99.80)
Epoch: [84][120/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 9.7130e-02 (1.5333e-01)	Acc@1  97.66 ( 95.02)	Acc@5 100.00 ( 99.75)
Epoch: [84][130/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 3.0452e-01 (1.5627e-01)	Acc@1  91.41 ( 94.95)	Acc@5  98.44 ( 99.73)
Epoch: [84][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.9620e-01 (1.5501e-01)	Acc@1  93.75 ( 94.95)	Acc@5  98.44 ( 99.73)
Epoch: [84][150/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 7.1693e-02 (1.5547e-01)	Acc@1  98.44 ( 94.98)	Acc@5 100.00 ( 99.72)
Epoch: [84][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2556e-01 (1.5507e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.72)
Epoch: [84][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0921e-01 (1.5377e-01)	Acc@1  96.88 ( 95.06)	Acc@5 100.00 ( 99.73)
Epoch: [84][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6853e-01 (1.5419e-01)	Acc@1  95.31 ( 95.07)	Acc@5  99.22 ( 99.72)
Epoch: [84][190/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.8272e-01 (1.5462e-01)	Acc@1  91.41 ( 95.07)	Acc@5 100.00 ( 99.72)
Epoch: [84][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0019e-01 (1.5558e-01)	Acc@1  96.88 ( 95.06)	Acc@5 100.00 ( 99.70)
Epoch: [84][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4468e-01 (1.5645e-01)	Acc@1  96.09 ( 95.00)	Acc@5 100.00 ( 99.70)
Epoch: [84][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9405e-01 (1.5712e-01)	Acc@1  93.75 ( 94.99)	Acc@5  99.22 ( 99.71)
Epoch: [84][230/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5657e-01 (1.5844e-01)	Acc@1  93.75 ( 94.94)	Acc@5 100.00 ( 99.71)
Epoch: [84][240/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.2766e-01 (1.5717e-01)	Acc@1  94.53 ( 94.98)	Acc@5 100.00 ( 99.71)
Epoch: [84][250/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.0058e-02 (1.5648e-01)	Acc@1  97.66 ( 95.00)	Acc@5 100.00 ( 99.72)
Epoch: [84][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.8111e-02 (1.5583e-01)	Acc@1  96.88 ( 95.00)	Acc@5 100.00 ( 99.72)
Epoch: [84][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4419e-01 (1.5647e-01)	Acc@1  93.75 ( 94.96)	Acc@5 100.00 ( 99.72)
Epoch: [84][280/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1447e-01 (1.5566e-01)	Acc@1  95.31 ( 94.99)	Acc@5 100.00 ( 99.72)
Epoch: [84][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2129e-01 (1.5500e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.73)
Epoch: [84][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2976e-01 (1.5503e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.73)
Epoch: [84][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5547e-01 (1.5486e-01)	Acc@1  93.75 ( 95.02)	Acc@5  99.22 ( 99.73)
Epoch: [84][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1046e-01 (1.5481e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.73)
Epoch: [84][330/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1815e-01 (1.5553e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.73)
Epoch: [84][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.9648e-01 (1.5543e-01)	Acc@1  94.53 ( 94.99)	Acc@5  98.44 ( 99.73)
Epoch: [84][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1408e-01 (1.5487e-01)	Acc@1  93.75 ( 95.01)	Acc@5 100.00 ( 99.74)
Epoch: [84][360/391]	Time  0.031 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.1676e-01 (1.5470e-01)	Acc@1  96.09 ( 95.02)	Acc@5 100.00 ( 99.74)
Epoch: [84][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8187e-01 (1.5462e-01)	Acc@1  94.53 ( 95.02)	Acc@5 100.00 ( 99.74)
Epoch: [84][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2304e-01 (1.5411e-01)	Acc@1  94.53 ( 95.04)	Acc@5 100.00 ( 99.74)
Epoch: [84][390/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6728e-01 (1.5494e-01)	Acc@1  90.00 ( 95.02)	Acc@5 100.00 ( 99.73)
## e[84] optimizer.zero_grad (sum) time: 0.11712360382080078
## e[84]       loss.backward (sum) time: 2.1246867179870605
## e[84]      optimizer.step (sum) time: 0.7358989715576172
## epoch[84] training(only) time: 13.018134117126465
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.6720e+00 (1.6720e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.1548e+00 (1.8086e+00)	Acc@1  66.00 ( 66.82)	Acc@5  89.00 ( 87.73)
Test: [ 20/100]	Time  0.025 ( 0.024)	Loss 1.8303e+00 (1.7594e+00)	Acc@1  61.00 ( 66.43)	Acc@5  89.00 ( 88.05)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8673e+00 (1.7545e+00)	Acc@1  60.00 ( 65.87)	Acc@5  87.00 ( 88.16)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.4389e+00 (1.7379e+00)	Acc@1  70.00 ( 66.29)	Acc@5  90.00 ( 88.39)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5555e+00 (1.7559e+00)	Acc@1  68.00 ( 66.14)	Acc@5  90.00 ( 88.29)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 2.1223e+00 (1.7309e+00)	Acc@1  63.00 ( 66.66)	Acc@5  84.00 ( 88.41)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7839e+00 (1.7332e+00)	Acc@1  65.00 ( 66.46)	Acc@5  88.00 ( 88.58)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6615e+00 (1.7311e+00)	Acc@1  69.00 ( 66.53)	Acc@5  87.00 ( 88.52)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.3905e+00 (1.7120e+00)	Acc@1  58.00 ( 66.73)	Acc@5  86.00 ( 88.65)
 * Acc@1 66.720 Acc@5 88.780
### epoch[84] execution time: 15.001408815383911
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.177 ( 0.177)	Data  0.148 ( 0.148)	Loss 1.9424e-01 (1.9424e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.034 ( 0.046)	Data  0.001 ( 0.015)	Loss 1.1365e-01 (1.7468e-01)	Acc@1  96.09 ( 94.67)	Acc@5 100.00 ( 99.72)
Epoch: [85][ 20/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.009)	Loss 2.5451e-01 (1.7557e-01)	Acc@1  92.19 ( 94.42)	Acc@5  97.66 ( 99.59)
Epoch: [85][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 2.3240e-01 (1.6650e-01)	Acc@1  92.19 ( 94.71)	Acc@5  99.22 ( 99.65)
Epoch: [85][ 40/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.006)	Loss 1.0956e-01 (1.5950e-01)	Acc@1  96.09 ( 94.74)	Acc@5 100.00 ( 99.66)
Epoch: [85][ 50/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 8.8186e-02 (1.6028e-01)	Acc@1  96.88 ( 94.82)	Acc@5 100.00 ( 99.62)
Epoch: [85][ 60/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.5825e-01 (1.5996e-01)	Acc@1  95.31 ( 94.83)	Acc@5  99.22 ( 99.62)
Epoch: [85][ 70/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.004)	Loss 1.4632e-01 (1.5856e-01)	Acc@1  93.75 ( 94.84)	Acc@5 100.00 ( 99.65)
Epoch: [85][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.1630e-01 (1.5817e-01)	Acc@1  97.66 ( 94.89)	Acc@5 100.00 ( 99.63)
Epoch: [85][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.1383e-01 (1.5768e-01)	Acc@1  97.66 ( 94.89)	Acc@5 100.00 ( 99.62)
Epoch: [85][100/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0675e-01 (1.5716e-01)	Acc@1  92.19 ( 94.83)	Acc@5  99.22 ( 99.64)
Epoch: [85][110/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2946e-01 (1.5582e-01)	Acc@1  96.88 ( 94.93)	Acc@5  99.22 ( 99.66)
Epoch: [85][120/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.4980e-01 (1.5612e-01)	Acc@1  93.75 ( 94.93)	Acc@5 100.00 ( 99.67)
Epoch: [85][130/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2266e-01 (1.5745e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.66)
Epoch: [85][140/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3184e-01 (1.5746e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.66)
Epoch: [85][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0243e-01 (1.5757e-01)	Acc@1  97.66 ( 94.89)	Acc@5  99.22 ( 99.65)
Epoch: [85][160/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6795e-01 (1.5814e-01)	Acc@1  94.53 ( 94.84)	Acc@5  99.22 ( 99.65)
Epoch: [85][170/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3901e-01 (1.5786e-01)	Acc@1  95.31 ( 94.87)	Acc@5 100.00 ( 99.65)
Epoch: [85][180/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9765e-01 (1.5806e-01)	Acc@1  92.97 ( 94.86)	Acc@5  99.22 ( 99.65)
Epoch: [85][190/391]	Time  0.035 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1232e-01 (1.5727e-01)	Acc@1  95.31 ( 94.89)	Acc@5 100.00 ( 99.65)
Epoch: [85][200/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6179e-01 (1.5603e-01)	Acc@1  96.88 ( 94.95)	Acc@5  99.22 ( 99.66)
Epoch: [85][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2438e-01 (1.5559e-01)	Acc@1  96.09 ( 94.97)	Acc@5 100.00 ( 99.66)
Epoch: [85][220/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.4766e-02 (1.5526e-01)	Acc@1  98.44 ( 94.97)	Acc@5 100.00 ( 99.68)
Epoch: [85][230/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5142e-01 (1.5592e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.67)
Epoch: [85][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4521e-01 (1.5576e-01)	Acc@1  96.09 ( 94.98)	Acc@5 100.00 ( 99.67)
Epoch: [85][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0432e-01 (1.5555e-01)	Acc@1  92.19 ( 94.99)	Acc@5  98.44 ( 99.67)
Epoch: [85][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.3477e-02 (1.5545e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.68)
Epoch: [85][270/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.9313e-01 (1.5712e-01)	Acc@1  91.41 ( 94.96)	Acc@5  99.22 ( 99.67)
Epoch: [85][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2498e-01 (1.5606e-01)	Acc@1  96.09 ( 94.98)	Acc@5 100.00 ( 99.68)
Epoch: [85][290/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.2571e-01 (1.5632e-01)	Acc@1  92.97 ( 94.98)	Acc@5 100.00 ( 99.68)
Epoch: [85][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.2463e-01 (1.5533e-01)	Acc@1  92.97 ( 95.02)	Acc@5  99.22 ( 99.69)
Epoch: [85][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1877e-01 (1.5602e-01)	Acc@1  94.53 ( 94.99)	Acc@5 100.00 ( 99.69)
Epoch: [85][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3415e-01 (1.5599e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.69)
Epoch: [85][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7429e-01 (1.5589e-01)	Acc@1  94.53 ( 94.99)	Acc@5 100.00 ( 99.70)
Epoch: [85][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0810e-01 (1.5615e-01)	Acc@1  96.88 ( 94.99)	Acc@5 100.00 ( 99.70)
Epoch: [85][350/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3870e-01 (1.5678e-01)	Acc@1  94.53 ( 94.97)	Acc@5 100.00 ( 99.70)
Epoch: [85][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.3991e-01 (1.5738e-01)	Acc@1  92.97 ( 94.97)	Acc@5 100.00 ( 99.69)
Epoch: [85][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 7.9316e-02 (1.5684e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.70)
Epoch: [85][380/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.2440e-02 (1.5667e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.70)
Epoch: [85][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.2658e-01 (1.5637e-01)	Acc@1  96.25 ( 94.98)	Acc@5 100.00 ( 99.71)
## e[85] optimizer.zero_grad (sum) time: 0.1169891357421875
## e[85]       loss.backward (sum) time: 2.147171974182129
## e[85]      optimizer.step (sum) time: 0.7360966205596924
## epoch[85] training(only) time: 12.993436098098755
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.6580e+00 (1.6580e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.1757e+00 (1.8253e+00)	Acc@1  66.00 ( 66.36)	Acc@5  89.00 ( 87.73)
Test: [ 20/100]	Time  0.017 ( 0.023)	Loss 1.8418e+00 (1.7815e+00)	Acc@1  61.00 ( 66.00)	Acc@5  89.00 ( 87.81)
Test: [ 30/100]	Time  0.017 ( 0.021)	Loss 1.9133e+00 (1.7771e+00)	Acc@1  61.00 ( 65.48)	Acc@5  85.00 ( 87.71)
Test: [ 40/100]	Time  0.018 ( 0.020)	Loss 1.4483e+00 (1.7637e+00)	Acc@1  69.00 ( 65.88)	Acc@5  93.00 ( 88.12)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5861e+00 (1.7797e+00)	Acc@1  68.00 ( 65.80)	Acc@5  90.00 ( 88.04)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1704e+00 (1.7529e+00)	Acc@1  63.00 ( 66.25)	Acc@5  84.00 ( 88.20)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.8000e+00 (1.7516e+00)	Acc@1  65.00 ( 66.23)	Acc@5  88.00 ( 88.37)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.7051e+00 (1.7502e+00)	Acc@1  68.00 ( 66.25)	Acc@5  86.00 ( 88.32)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.4003e+00 (1.7319e+00)	Acc@1  58.00 ( 66.41)	Acc@5  85.00 ( 88.44)
 * Acc@1 66.430 Acc@5 88.600
### epoch[85] execution time: 14.932780742645264
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.179 ( 0.179)	Data  0.153 ( 0.153)	Loss 1.8578e-01 (1.8578e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [86][ 10/391]	Time  0.032 ( 0.045)	Data  0.001 ( 0.015)	Loss 8.2159e-02 (1.4676e-01)	Acc@1  98.44 ( 95.53)	Acc@5 100.00 ( 99.64)
Epoch: [86][ 20/391]	Time  0.030 ( 0.039)	Data  0.001 ( 0.009)	Loss 1.2310e-01 (1.5133e-01)	Acc@1  96.88 ( 95.28)	Acc@5 100.00 ( 99.70)
Epoch: [86][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.0563e-01 (1.4464e-01)	Acc@1  96.88 ( 95.26)	Acc@5 100.00 ( 99.70)
Epoch: [86][ 40/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.006)	Loss 7.8507e-02 (1.5105e-01)	Acc@1  98.44 ( 95.20)	Acc@5 100.00 ( 99.58)
Epoch: [86][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.7077e-01 (1.5062e-01)	Acc@1  92.19 ( 95.13)	Acc@5 100.00 ( 99.66)
Epoch: [86][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.9457e-01 (1.4997e-01)	Acc@1  94.53 ( 95.17)	Acc@5  99.22 ( 99.69)
Epoch: [86][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.4033e-01 (1.4899e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.71)
Epoch: [86][ 80/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.6584e-01 (1.5004e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.73)
Epoch: [86][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.2176e-01 (1.4858e-01)	Acc@1  96.09 ( 95.28)	Acc@5 100.00 ( 99.74)
Epoch: [86][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1868e-01 (1.5131e-01)	Acc@1  92.97 ( 95.20)	Acc@5  99.22 ( 99.74)
Epoch: [86][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.0658e-01 (1.5206e-01)	Acc@1  97.66 ( 95.19)	Acc@5  99.22 ( 99.75)
Epoch: [86][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1561e-01 (1.5228e-01)	Acc@1  96.88 ( 95.20)	Acc@5 100.00 ( 99.74)
Epoch: [86][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.0744e-01 (1.5332e-01)	Acc@1  94.53 ( 95.14)	Acc@5  99.22 ( 99.74)
Epoch: [86][140/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.5994e-01 (1.5339e-01)	Acc@1  91.41 ( 95.09)	Acc@5  99.22 ( 99.75)
Epoch: [86][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5486e-01 (1.5136e-01)	Acc@1  95.31 ( 95.15)	Acc@5  99.22 ( 99.75)
Epoch: [86][160/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4547e-01 (1.5015e-01)	Acc@1  94.53 ( 95.19)	Acc@5 100.00 ( 99.76)
Epoch: [86][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2990e-01 (1.5084e-01)	Acc@1  96.09 ( 95.17)	Acc@5 100.00 ( 99.75)
Epoch: [86][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4018e-01 (1.5129e-01)	Acc@1  96.88 ( 95.20)	Acc@5  99.22 ( 99.74)
Epoch: [86][190/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8382e-01 (1.5135e-01)	Acc@1  96.88 ( 95.17)	Acc@5 100.00 ( 99.74)
Epoch: [86][200/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7260e-01 (1.5093e-01)	Acc@1  94.53 ( 95.17)	Acc@5 100.00 ( 99.75)
Epoch: [86][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.3316e-01 (1.5183e-01)	Acc@1  94.53 ( 95.16)	Acc@5  99.22 ( 99.75)
Epoch: [86][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0701e-01 (1.5094e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.75)
Epoch: [86][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1746e-01 (1.5026e-01)	Acc@1  96.88 ( 95.21)	Acc@5 100.00 ( 99.76)
Epoch: [86][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1037e-01 (1.4959e-01)	Acc@1  97.66 ( 95.23)	Acc@5  99.22 ( 99.75)
Epoch: [86][250/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3680e-01 (1.4964e-01)	Acc@1  95.31 ( 95.22)	Acc@5 100.00 ( 99.75)
Epoch: [86][260/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2948e-01 (1.5092e-01)	Acc@1  96.09 ( 95.19)	Acc@5 100.00 ( 99.73)
Epoch: [86][270/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.0387e-01 (1.5106e-01)	Acc@1  96.09 ( 95.19)	Acc@5 100.00 ( 99.73)
Epoch: [86][280/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2136e-01 (1.5103e-01)	Acc@1  93.75 ( 95.19)	Acc@5 100.00 ( 99.73)
Epoch: [86][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.1937e-01 (1.5072e-01)	Acc@1  91.41 ( 95.19)	Acc@5 100.00 ( 99.73)
Epoch: [86][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4128e-01 (1.5010e-01)	Acc@1  95.31 ( 95.20)	Acc@5 100.00 ( 99.73)
Epoch: [86][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3405e-01 (1.5042e-01)	Acc@1  96.09 ( 95.19)	Acc@5 100.00 ( 99.73)
Epoch: [86][320/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4951e-01 (1.5127e-01)	Acc@1  96.09 ( 95.17)	Acc@5  99.22 ( 99.72)
Epoch: [86][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.9605e-02 (1.5085e-01)	Acc@1  99.22 ( 95.20)	Acc@5 100.00 ( 99.71)
Epoch: [86][340/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.7309e-01 (1.5151e-01)	Acc@1  92.19 ( 95.19)	Acc@5  99.22 ( 99.71)
Epoch: [86][350/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4910e-01 (1.5211e-01)	Acc@1  95.31 ( 95.17)	Acc@5 100.00 ( 99.71)
Epoch: [86][360/391]	Time  0.034 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.4057e-01 (1.5228e-01)	Acc@1  95.31 ( 95.17)	Acc@5  99.22 ( 99.70)
Epoch: [86][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0694e-01 (1.5201e-01)	Acc@1  97.66 ( 95.18)	Acc@5 100.00 ( 99.70)
Epoch: [86][380/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.4220e-01 (1.5179e-01)	Acc@1  94.53 ( 95.20)	Acc@5  98.44 ( 99.70)
Epoch: [86][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6604e-01 (1.5233e-01)	Acc@1  95.00 ( 95.17)	Acc@5 100.00 ( 99.70)
## e[86] optimizer.zero_grad (sum) time: 0.11707067489624023
## e[86]       loss.backward (sum) time: 2.138869285583496
## e[86]      optimizer.step (sum) time: 0.7330961227416992
## epoch[86] training(only) time: 13.015862226486206
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.6762e+00 (1.6762e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 2.2497e+00 (1.8400e+00)	Acc@1  65.00 ( 65.64)	Acc@5  89.00 ( 88.36)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.8793e+00 (1.7879e+00)	Acc@1  61.00 ( 65.71)	Acc@5  88.00 ( 88.48)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8462e+00 (1.7832e+00)	Acc@1  60.00 ( 65.35)	Acc@5  86.00 ( 88.35)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.4723e+00 (1.7661e+00)	Acc@1  68.00 ( 65.51)	Acc@5  92.00 ( 88.61)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6119e+00 (1.7843e+00)	Acc@1  66.00 ( 65.73)	Acc@5  89.00 ( 88.47)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1770e+00 (1.7614e+00)	Acc@1  63.00 ( 66.11)	Acc@5  86.00 ( 88.57)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7793e+00 (1.7615e+00)	Acc@1  64.00 ( 66.01)	Acc@5  87.00 ( 88.66)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6777e+00 (1.7597e+00)	Acc@1  71.00 ( 66.10)	Acc@5  87.00 ( 88.58)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.4589e+00 (1.7426e+00)	Acc@1  60.00 ( 66.32)	Acc@5  86.00 ( 88.71)
 * Acc@1 66.330 Acc@5 88.850
### epoch[86] execution time: 14.987169981002808
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.182 ( 0.182)	Data  0.155 ( 0.155)	Loss 1.6663e-01 (1.6663e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [87][ 10/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.016)	Loss 8.7895e-02 (1.5038e-01)	Acc@1  96.88 ( 95.17)	Acc@5 100.00 ( 99.64)
Epoch: [87][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.5131e-01 (1.4338e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.70)
Epoch: [87][ 30/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.007)	Loss 1.6995e-01 (1.4179e-01)	Acc@1  92.97 ( 95.19)	Acc@5  99.22 ( 99.72)
Epoch: [87][ 40/391]	Time  0.032 ( 0.036)	Data  0.002 ( 0.006)	Loss 2.0896e-01 (1.4231e-01)	Acc@1  90.62 ( 95.03)	Acc@5 100.00 ( 99.77)
Epoch: [87][ 50/391]	Time  0.032 ( 0.036)	Data  0.002 ( 0.005)	Loss 1.4993e-01 (1.4258e-01)	Acc@1  96.88 ( 95.13)	Acc@5 100.00 ( 99.79)
Epoch: [87][ 60/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.005)	Loss 2.2630e-01 (1.4491e-01)	Acc@1  90.62 ( 95.09)	Acc@5 100.00 ( 99.78)
Epoch: [87][ 70/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.8459e-01 (1.4323e-01)	Acc@1  92.19 ( 95.20)	Acc@5  99.22 ( 99.79)
Epoch: [87][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.1559e-01 (1.4483e-01)	Acc@1  92.19 ( 95.20)	Acc@5 100.00 ( 99.80)
Epoch: [87][ 90/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 2.4910e-01 (1.4578e-01)	Acc@1  92.19 ( 95.18)	Acc@5  99.22 ( 99.80)
Epoch: [87][100/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.3597e-01 (1.4598e-01)	Acc@1  95.31 ( 95.14)	Acc@5 100.00 ( 99.81)
Epoch: [87][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7458e-01 (1.4614e-01)	Acc@1  92.97 ( 95.11)	Acc@5 100.00 ( 99.83)
Epoch: [87][120/391]	Time  0.030 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.8290e-01 (1.4643e-01)	Acc@1  94.53 ( 95.14)	Acc@5  98.44 ( 99.81)
Epoch: [87][130/391]	Time  0.033 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5595e-01 (1.4528e-01)	Acc@1  93.75 ( 95.22)	Acc@5 100.00 ( 99.81)
Epoch: [87][140/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.2942e-01 (1.4463e-01)	Acc@1  96.88 ( 95.22)	Acc@5  99.22 ( 99.81)
Epoch: [87][150/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 8.2710e-02 (1.4523e-01)	Acc@1  98.44 ( 95.23)	Acc@5 100.00 ( 99.81)
Epoch: [87][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8840e-01 (1.4536e-01)	Acc@1  95.31 ( 95.23)	Acc@5 100.00 ( 99.81)
Epoch: [87][170/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2651e-01 (1.4545e-01)	Acc@1  93.75 ( 95.19)	Acc@5 100.00 ( 99.80)
Epoch: [87][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5235e-01 (1.4668e-01)	Acc@1  89.84 ( 95.14)	Acc@5 100.00 ( 99.80)
Epoch: [87][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 3.1244e-01 (1.4745e-01)	Acc@1  91.41 ( 95.16)	Acc@5 100.00 ( 99.80)
Epoch: [87][200/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4200e-01 (1.4706e-01)	Acc@1  92.97 ( 95.16)	Acc@5 100.00 ( 99.80)
Epoch: [87][210/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6964e-01 (1.4768e-01)	Acc@1  94.53 ( 95.16)	Acc@5 100.00 ( 99.79)
Epoch: [87][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.0637e-01 (1.4794e-01)	Acc@1  92.97 ( 95.14)	Acc@5 100.00 ( 99.79)
Epoch: [87][230/391]	Time  0.033 ( 0.033)	Data  0.002 ( 0.003)	Loss 8.5637e-02 (1.4742e-01)	Acc@1  96.88 ( 95.15)	Acc@5 100.00 ( 99.78)
Epoch: [87][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.3139e-02 (1.4719e-01)	Acc@1  96.88 ( 95.16)	Acc@5 100.00 ( 99.78)
Epoch: [87][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2959e-01 (1.4671e-01)	Acc@1  96.88 ( 95.19)	Acc@5 100.00 ( 99.78)
Epoch: [87][260/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1340e-01 (1.4609e-01)	Acc@1  95.31 ( 95.19)	Acc@5 100.00 ( 99.78)
Epoch: [87][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3000e-01 (1.4628e-01)	Acc@1  96.88 ( 95.21)	Acc@5 100.00 ( 99.79)
Epoch: [87][280/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4678e-01 (1.4696e-01)	Acc@1  93.75 ( 95.18)	Acc@5 100.00 ( 99.79)
Epoch: [87][290/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3527e-01 (1.4736e-01)	Acc@1  94.53 ( 95.15)	Acc@5 100.00 ( 99.78)
Epoch: [87][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7986e-01 (1.4799e-01)	Acc@1  94.53 ( 95.16)	Acc@5  98.44 ( 99.76)
Epoch: [87][310/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.5770e-01 (1.4802e-01)	Acc@1  95.31 ( 95.18)	Acc@5 100.00 ( 99.76)
Epoch: [87][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3882e-01 (1.4821e-01)	Acc@1  94.53 ( 95.17)	Acc@5 100.00 ( 99.77)
Epoch: [87][330/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2876e-01 (1.4833e-01)	Acc@1  94.53 ( 95.17)	Acc@5 100.00 ( 99.77)
Epoch: [87][340/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5913e-01 (1.4826e-01)	Acc@1  94.53 ( 95.16)	Acc@5 100.00 ( 99.77)
Epoch: [87][350/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3352e-01 (1.4797e-01)	Acc@1  96.09 ( 95.18)	Acc@5 100.00 ( 99.77)
Epoch: [87][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6278e-01 (1.4769e-01)	Acc@1  93.75 ( 95.19)	Acc@5 100.00 ( 99.77)
Epoch: [87][370/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1784e-01 (1.4698e-01)	Acc@1  96.88 ( 95.22)	Acc@5  99.22 ( 99.77)
Epoch: [87][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.6871e-01 (1.4678e-01)	Acc@1  94.53 ( 95.25)	Acc@5 100.00 ( 99.77)
Epoch: [87][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.5379e-01 (1.4713e-01)	Acc@1  91.25 ( 95.24)	Acc@5  98.75 ( 99.76)
## e[87] optimizer.zero_grad (sum) time: 0.11617636680603027
## e[87]       loss.backward (sum) time: 2.0770316123962402
## e[87]      optimizer.step (sum) time: 0.7475709915161133
## epoch[87] training(only) time: 13.029541015625
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.6316e+00 (1.6316e+00)	Acc@1  70.00 ( 70.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.2150e+00 (1.8418e+00)	Acc@1  65.00 ( 66.09)	Acc@5  88.00 ( 88.18)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.9324e+00 (1.8019e+00)	Acc@1  61.00 ( 65.86)	Acc@5  88.00 ( 88.19)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.8945e+00 (1.7936e+00)	Acc@1  56.00 ( 65.32)	Acc@5  85.00 ( 87.97)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4710e+00 (1.7733e+00)	Acc@1  67.00 ( 65.63)	Acc@5  91.00 ( 88.32)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5902e+00 (1.7909e+00)	Acc@1  68.00 ( 65.65)	Acc@5  89.00 ( 88.18)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1863e+00 (1.7660e+00)	Acc@1  64.00 ( 66.18)	Acc@5  86.00 ( 88.28)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7953e+00 (1.7641e+00)	Acc@1  65.00 ( 66.14)	Acc@5  88.00 ( 88.41)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6931e+00 (1.7632e+00)	Acc@1  70.00 ( 66.27)	Acc@5  86.00 ( 88.35)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.4271e+00 (1.7449e+00)	Acc@1  61.00 ( 66.48)	Acc@5  87.00 ( 88.54)
 * Acc@1 66.530 Acc@5 88.640
### epoch[87] execution time: 14.967883110046387
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.179 ( 0.179)	Data  0.154 ( 0.154)	Loss 1.7887e-01 (1.7887e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.033 ( 0.046)	Data  0.001 ( 0.016)	Loss 1.9180e-01 (1.7928e-01)	Acc@1  92.97 ( 93.96)	Acc@5 100.00 ( 99.79)
Epoch: [88][ 20/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.8042e-01 (1.7097e-01)	Acc@1  96.09 ( 94.35)	Acc@5 100.00 ( 99.78)
Epoch: [88][ 30/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.4100e-01 (1.5351e-01)	Acc@1  96.88 ( 95.09)	Acc@5 100.00 ( 99.80)
Epoch: [88][ 40/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.006)	Loss 2.5435e-01 (1.5331e-01)	Acc@1  89.84 ( 95.01)	Acc@5 100.00 ( 99.81)
Epoch: [88][ 50/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.005)	Loss 1.4616e-01 (1.5069e-01)	Acc@1  96.09 ( 95.17)	Acc@5 100.00 ( 99.79)
Epoch: [88][ 60/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.005)	Loss 8.7425e-02 (1.4735e-01)	Acc@1  97.66 ( 95.31)	Acc@5 100.00 ( 99.80)
Epoch: [88][ 70/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.004)	Loss 1.3844e-01 (1.4616e-01)	Acc@1  96.88 ( 95.38)	Acc@5  99.22 ( 99.81)
Epoch: [88][ 80/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.7548e-01 (1.4651e-01)	Acc@1  94.53 ( 95.30)	Acc@5 100.00 ( 99.82)
Epoch: [88][ 90/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.004)	Loss 7.8375e-02 (1.4714e-01)	Acc@1  98.44 ( 95.24)	Acc@5 100.00 ( 99.81)
Epoch: [88][100/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.4471e-01 (1.4928e-01)	Acc@1  94.53 ( 95.18)	Acc@5 100.00 ( 99.80)
Epoch: [88][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.6859e-01 (1.4868e-01)	Acc@1  92.97 ( 95.17)	Acc@5 100.00 ( 99.77)
Epoch: [88][120/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.3268e-01 (1.4981e-01)	Acc@1  95.31 ( 95.13)	Acc@5 100.00 ( 99.77)
Epoch: [88][130/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5018e-01 (1.5097e-01)	Acc@1  93.75 ( 95.03)	Acc@5 100.00 ( 99.77)
Epoch: [88][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.8788e-01 (1.5154e-01)	Acc@1  93.75 ( 95.03)	Acc@5 100.00 ( 99.78)
Epoch: [88][150/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6779e-01 (1.5242e-01)	Acc@1  92.97 ( 95.01)	Acc@5 100.00 ( 99.77)
Epoch: [88][160/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1369e-01 (1.5129e-01)	Acc@1  96.88 ( 95.05)	Acc@5 100.00 ( 99.78)
Epoch: [88][170/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2976e-01 (1.5016e-01)	Acc@1  94.53 ( 95.07)	Acc@5 100.00 ( 99.78)
Epoch: [88][180/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 6.8729e-02 (1.4920e-01)	Acc@1  97.66 ( 95.12)	Acc@5 100.00 ( 99.78)
Epoch: [88][190/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1880e-01 (1.5000e-01)	Acc@1  95.31 ( 95.08)	Acc@5 100.00 ( 99.78)
Epoch: [88][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3090e-01 (1.4937e-01)	Acc@1  94.53 ( 95.09)	Acc@5 100.00 ( 99.78)
Epoch: [88][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4453e-01 (1.4990e-01)	Acc@1  95.31 ( 95.08)	Acc@5 100.00 ( 99.78)
Epoch: [88][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.0606e-02 (1.5065e-01)	Acc@1  96.88 ( 95.07)	Acc@5 100.00 ( 99.78)
Epoch: [88][230/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.5080e-02 (1.5015e-01)	Acc@1  97.66 ( 95.08)	Acc@5 100.00 ( 99.78)
Epoch: [88][240/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1938e-01 (1.5017e-01)	Acc@1  94.53 ( 95.07)	Acc@5 100.00 ( 99.78)
Epoch: [88][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6400e-01 (1.5104e-01)	Acc@1  96.09 ( 95.06)	Acc@5  99.22 ( 99.77)
Epoch: [88][260/391]	Time  0.030 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7140e-01 (1.5129e-01)	Acc@1  93.75 ( 95.05)	Acc@5 100.00 ( 99.75)
Epoch: [88][270/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5373e-01 (1.5139e-01)	Acc@1  92.97 ( 95.04)	Acc@5 100.00 ( 99.75)
Epoch: [88][280/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 8.0823e-02 (1.5123e-01)	Acc@1  97.66 ( 95.07)	Acc@5 100.00 ( 99.75)
Epoch: [88][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4197e-01 (1.5098e-01)	Acc@1  95.31 ( 95.09)	Acc@5  99.22 ( 99.75)
Epoch: [88][300/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8662e-01 (1.5230e-01)	Acc@1  92.97 ( 95.05)	Acc@5 100.00 ( 99.75)
Epoch: [88][310/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1645e-01 (1.5203e-01)	Acc@1  96.88 ( 95.07)	Acc@5 100.00 ( 99.75)
Epoch: [88][320/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.5260e-02 (1.5182e-01)	Acc@1  97.66 ( 95.08)	Acc@5 100.00 ( 99.75)
Epoch: [88][330/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4618e-01 (1.5142e-01)	Acc@1  96.09 ( 95.10)	Acc@5 100.00 ( 99.75)
Epoch: [88][340/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.6076e-01 (1.5091e-01)	Acc@1  92.97 ( 95.10)	Acc@5  99.22 ( 99.76)
Epoch: [88][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.8834e-01 (1.5173e-01)	Acc@1  93.75 ( 95.07)	Acc@5 100.00 ( 99.75)
Epoch: [88][360/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.7910e-01 (1.5161e-01)	Acc@1  92.19 ( 95.08)	Acc@5 100.00 ( 99.76)
Epoch: [88][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.0589e-01 (1.5160e-01)	Acc@1  96.09 ( 95.07)	Acc@5 100.00 ( 99.76)
Epoch: [88][380/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 9.5333e-02 (1.5165e-01)	Acc@1  94.53 ( 95.06)	Acc@5 100.00 ( 99.76)
Epoch: [88][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.7781e-01 (1.5227e-01)	Acc@1  91.25 ( 95.04)	Acc@5 100.00 ( 99.76)
## e[88] optimizer.zero_grad (sum) time: 0.11641049385070801
## e[88]       loss.backward (sum) time: 2.1208248138427734
## e[88]      optimizer.step (sum) time: 0.7314496040344238
## epoch[88] training(only) time: 12.998653650283813
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.6495e+00 (1.6495e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.1767e+00 (1.8198e+00)	Acc@1  66.00 ( 66.45)	Acc@5  89.00 ( 87.91)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 1.8714e+00 (1.7878e+00)	Acc@1  61.00 ( 66.10)	Acc@5  88.00 ( 88.10)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.9366e+00 (1.7796e+00)	Acc@1  59.00 ( 65.77)	Acc@5  83.00 ( 87.94)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 1.4538e+00 (1.7605e+00)	Acc@1  68.00 ( 66.10)	Acc@5  90.00 ( 88.20)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5900e+00 (1.7764e+00)	Acc@1  67.00 ( 66.06)	Acc@5  89.00 ( 88.08)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 2.1296e+00 (1.7483e+00)	Acc@1  64.00 ( 66.64)	Acc@5  86.00 ( 88.26)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7692e+00 (1.7454e+00)	Acc@1  66.00 ( 66.42)	Acc@5  87.00 ( 88.44)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.6760e+00 (1.7429e+00)	Acc@1  69.00 ( 66.51)	Acc@5  86.00 ( 88.41)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.4438e+00 (1.7272e+00)	Acc@1  59.00 ( 66.70)	Acc@5  84.00 ( 88.55)
 * Acc@1 66.710 Acc@5 88.700
### epoch[88] execution time: 14.958935260772705
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.186 ( 0.186)	Data  0.155 ( 0.155)	Loss 7.8129e-02 (7.8129e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.029 ( 0.046)	Data  0.002 ( 0.016)	Loss 1.6650e-01 (1.4788e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 ( 99.86)
Epoch: [89][ 20/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.009)	Loss 1.8729e-01 (1.3840e-01)	Acc@1  95.31 ( 95.54)	Acc@5  98.44 ( 99.78)
Epoch: [89][ 30/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.007)	Loss 1.1658e-01 (1.3966e-01)	Acc@1  94.53 ( 95.54)	Acc@5 100.00 ( 99.72)
Epoch: [89][ 40/391]	Time  0.032 ( 0.036)	Data  0.002 ( 0.006)	Loss 1.2624e-01 (1.4031e-01)	Acc@1  96.09 ( 95.41)	Acc@5 100.00 ( 99.75)
Epoch: [89][ 50/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.005)	Loss 1.8150e-01 (1.4438e-01)	Acc@1  93.75 ( 95.31)	Acc@5 100.00 ( 99.72)
Epoch: [89][ 60/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.005)	Loss 2.0801e-01 (1.4261e-01)	Acc@1  94.53 ( 95.26)	Acc@5  99.22 ( 99.73)
Epoch: [89][ 70/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.004)	Loss 3.8329e-01 (1.4718e-01)	Acc@1  91.41 ( 95.21)	Acc@5  97.66 ( 99.68)
Epoch: [89][ 80/391]	Time  0.034 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.3090e-01 (1.4337e-01)	Acc@1  96.88 ( 95.40)	Acc@5 100.00 ( 99.69)
Epoch: [89][ 90/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.8948e-01 (1.4348e-01)	Acc@1  92.97 ( 95.43)	Acc@5 100.00 ( 99.72)
Epoch: [89][100/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.004)	Loss 1.5148e-01 (1.4472e-01)	Acc@1  92.97 ( 95.40)	Acc@5  99.22 ( 99.71)
Epoch: [89][110/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.1612e-01 (1.4269e-01)	Acc@1  97.66 ( 95.43)	Acc@5 100.00 ( 99.73)
Epoch: [89][120/391]	Time  0.032 ( 0.034)	Data  0.002 ( 0.003)	Loss 1.3767e-01 (1.4228e-01)	Acc@1  96.09 ( 95.41)	Acc@5 100.00 ( 99.74)
Epoch: [89][130/391]	Time  0.030 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.7736e-01 (1.4198e-01)	Acc@1  92.97 ( 95.40)	Acc@5  99.22 ( 99.74)
Epoch: [89][140/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5077e-01 (1.4377e-01)	Acc@1  96.09 ( 95.41)	Acc@5  99.22 ( 99.73)
Epoch: [89][150/391]	Time  0.031 ( 0.034)	Data  0.001 ( 0.003)	Loss 1.5768e-01 (1.4353e-01)	Acc@1  92.19 ( 95.41)	Acc@5 100.00 ( 99.72)
Epoch: [89][160/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.003)	Loss 2.4272e-01 (1.4583e-01)	Acc@1  90.62 ( 95.30)	Acc@5  99.22 ( 99.71)
Epoch: [89][170/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9730e-01 (1.4695e-01)	Acc@1  93.75 ( 95.28)	Acc@5  99.22 ( 99.72)
Epoch: [89][180/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3465e-01 (1.4647e-01)	Acc@1  94.53 ( 95.27)	Acc@5  99.22 ( 99.72)
Epoch: [89][190/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.2249e-01 (1.4537e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 ( 99.73)
Epoch: [89][200/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.3514e-01 (1.4569e-01)	Acc@1  95.31 ( 95.29)	Acc@5 100.00 ( 99.74)
Epoch: [89][210/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 2.5921e-01 (1.4651e-01)	Acc@1  92.19 ( 95.25)	Acc@5  99.22 ( 99.74)
Epoch: [89][220/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 9.6384e-02 (1.4708e-01)	Acc@1  98.44 ( 95.23)	Acc@5 100.00 ( 99.74)
Epoch: [89][230/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 7.0832e-02 (1.4602e-01)	Acc@1  99.22 ( 95.28)	Acc@5 100.00 ( 99.74)
Epoch: [89][240/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.5917e-01 (1.4653e-01)	Acc@1  94.53 ( 95.26)	Acc@5 100.00 ( 99.75)
Epoch: [89][250/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.7827e-01 (1.4681e-01)	Acc@1  92.97 ( 95.23)	Acc@5 100.00 ( 99.74)
Epoch: [89][260/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.8586e-01 (1.4667e-01)	Acc@1  92.19 ( 95.23)	Acc@5  99.22 ( 99.75)
Epoch: [89][270/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.9746e-01 (1.4714e-01)	Acc@1  92.19 ( 95.22)	Acc@5  99.22 ( 99.75)
Epoch: [89][280/391]	Time  0.032 ( 0.033)	Data  0.002 ( 0.003)	Loss 1.5804e-01 (1.4766e-01)	Acc@1  93.75 ( 95.19)	Acc@5  99.22 ( 99.75)
Epoch: [89][290/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.4389e-01 (1.4866e-01)	Acc@1  96.09 ( 95.16)	Acc@5 100.00 ( 99.73)
Epoch: [89][300/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6244e-01 (1.4870e-01)	Acc@1  93.75 ( 95.17)	Acc@5 100.00 ( 99.73)
Epoch: [89][310/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1157e-01 (1.4849e-01)	Acc@1  96.09 ( 95.17)	Acc@5  99.22 ( 99.73)
Epoch: [89][320/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.6569e-01 (1.4833e-01)	Acc@1  96.88 ( 95.20)	Acc@5  99.22 ( 99.73)
Epoch: [89][330/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.003)	Loss 1.1879e-01 (1.4911e-01)	Acc@1  96.88 ( 95.17)	Acc@5 100.00 ( 99.72)
Epoch: [89][340/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 2.1286e-01 (1.4945e-01)	Acc@1  96.09 ( 95.18)	Acc@5  99.22 ( 99.72)
Epoch: [89][350/391]	Time  0.033 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.1774e-01 (1.4931e-01)	Acc@1  96.09 ( 95.18)	Acc@5 100.00 ( 99.72)
Epoch: [89][360/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.5268e-01 (1.4898e-01)	Acc@1  95.31 ( 95.21)	Acc@5 100.00 ( 99.72)
Epoch: [89][370/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 1.3494e-01 (1.4873e-01)	Acc@1  96.09 ( 95.23)	Acc@5 100.00 ( 99.72)
Epoch: [89][380/391]	Time  0.034 ( 0.033)	Data  0.002 ( 0.002)	Loss 1.6622e-01 (1.4865e-01)	Acc@1  92.19 ( 95.21)	Acc@5  99.22 ( 99.72)
Epoch: [89][390/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.002)	Loss 8.3557e-02 (1.4867e-01)	Acc@1  97.50 ( 95.20)	Acc@5 100.00 ( 99.72)
## e[89] optimizer.zero_grad (sum) time: 0.11801862716674805
## e[89]       loss.backward (sum) time: 2.1522088050842285
## e[89]      optimizer.step (sum) time: 0.7350645065307617
## epoch[89] training(only) time: 13.019258737564087
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.5971e+00 (1.5971e+00)	Acc@1  71.00 ( 71.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 2.1976e+00 (1.8381e+00)	Acc@1  66.00 ( 66.36)	Acc@5  89.00 ( 88.00)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.8707e+00 (1.7934e+00)	Acc@1  61.00 ( 65.95)	Acc@5  88.00 ( 88.24)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.9047e+00 (1.7907e+00)	Acc@1  62.00 ( 65.52)	Acc@5  86.00 ( 88.03)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.4483e+00 (1.7714e+00)	Acc@1  68.00 ( 65.68)	Acc@5  92.00 ( 88.32)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.6710e+00 (1.7886e+00)	Acc@1  66.00 ( 65.75)	Acc@5  88.00 ( 88.12)
Test: [ 60/100]	Time  0.017 ( 0.019)	Loss 2.1409e+00 (1.7631e+00)	Acc@1  65.00 ( 66.34)	Acc@5  85.00 ( 88.33)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.7829e+00 (1.7616e+00)	Acc@1  67.00 ( 66.14)	Acc@5  88.00 ( 88.55)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.7116e+00 (1.7614e+00)	Acc@1  69.00 ( 66.21)	Acc@5  86.00 ( 88.41)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 2.4321e+00 (1.7443e+00)	Acc@1  59.00 ( 66.40)	Acc@5  86.00 ( 88.54)
 * Acc@1 66.480 Acc@5 88.650
### epoch[89] execution time: 14.964808464050293
### Training complete:
#### total training(only) time: 1172.5673627853394
##### Total run time: 1351.6953489780426
