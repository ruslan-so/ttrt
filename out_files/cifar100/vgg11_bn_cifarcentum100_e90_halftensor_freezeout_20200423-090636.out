# Model: vgg11_bn
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: True
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.vgg
<function vgg11_bn at 0x7f139e3390d0>
# model requested: 'vgg11_bn'
# printing out the model
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(inplace=True)
    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU(inplace=True)
    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (27): ReLU(inplace=True)
    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=512, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=100, bias=True)
  )
)
# model is low precision
PARAM TOTAL COUNT: 38
PARAM TO FREEZE COUNT: 32

features.module.0.weight
[0.1, 0.10088384267912825, 0.10053591041744064, 0.0999578198101117, 0.09915225683538814, 0.09812296437474749, 0.09687472482237458, 0.09541333786475781, 0.09374559353364734, 0.0918792406575792, 0.0898229508585488, 0.08758627826111579, 0.08517961510114358, 0.08261414344042829, 0.07990178321156519, 0.0770551368344512, 0.07408743066175172, 0.07101245352539297, 0.06784449266961103, 0.06459826736823124, 0.06128886053461177, 0.05793164864201136, 0.054542230279991735, 0.05113635367880273, 0.04772984353849288, 0.044338527502719036, 0.040978162618878995, 0.03766436212625511, 0.034412522912332426, 0.031237753974350746, 0.028154806218479014, 0.025178003922785554, 0.022321178182447728, 0.019597602646433312, 0.017019931844240544, 0.01460014238924829, 0.01234947733186318, 0.010278393921015021, 0.0083965150167161, 0.006712584379435493, 0.005234426044027662, 0.003968907966975204, 0.002921910115851916, 0.0020982971492716097, 0.001501895814259612, 0.0011354771660654897, 0.0010007436930289206, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.0.bias
[0.1, 0.10088832241345322, 0.10055378852914634, 0.09999789274455206, 0.09922311830365418, 0.09823292620403981, 0.09703173973626507, 0.09562492472455848, 0.09401876555712907, 0.09222043711315396, 0.09023797271185083, 0.08808022822680978, 0.08575684252588976, 0.0832781944133986, 0.0806553562669004, 0.07790004457575932, 0.07502456760236934, 0.07204177039987285, 0.0689649774319792, 0.0658079330512067, 0.062584740101438, 0.05930979691905728, 0.055997733014092, 0.05266334371867796, 0.04932152409477991, 0.04598720239640876, 0.04267527338356627, 0.03940053178581119, 0.03617760621267156, 0.03302089380613254, 0.02994449592711351, 0.026962155163229907, 0.024087193939232573, 0.021332455004357948, 0.018710244062438604, 0.016232274801050932, 0.013909616565260862, 0.011752644909714265, 0.009770995249960796, 0.007973519820055723, 0.0063682481287146205, 0.004962351090667143, 0.00376210899343895, 0.002772883442657691, 0.0019990934112058037, 0.0014441954992115583, 0.0011106684930586189, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.1.weight
[0.1, 0.10089260192924465, 0.10057086909080265, 0.10003618362411991, 0.09929084249669973, 0.09833804763650769, 0.09718189217673621, 0.0958273428720202, 0.09428021876164241, 0.09254716617138983, 0.09063563016145088, 0.08855382254301139, 0.08631068660094757, 0.0839158586741642, 0.08137962675862638, 0.0787128863109228, 0.07592709344222497, 0.0730342157037178, 0.07004668067492317, 0.06697732257577735, 0.06383932713181253, 0.06064617492929756, 0.057411583503679614, 0.05414944841011035, 0.050873783529213526, 0.0475986608645351, 0.04433815009030201, 0.04110625810918748, 0.03791686887973791, 0.034783683771958576, 0.03172016270728527, 0.028739466335800542, 0.025854399499096587, 0.023077356221663927, 0.020420266467118565, 0.01789454488799898, 0.015511041789299674, 0.013279996516398239, 0.011210993467617773, 0.009312920920391314, 0.007593932847907897, 0.006061413890272965, 0.004721947630664352, 0.003581288312767044, 0.0026443361209862563, 0.0019151161296334976, 0.0013967610115178655, 0.0010914975802254303, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.1.bias
[0.1, 0.10089669138466442, 0.10058719244545776, 0.10007278213865434, 0.09935558618491452, 0.09843856828507457, 0.09732551787311323, 0.09602103445690491, 0.09453050861146783, 0.09286009970325058, 0.09101671043750724, 0.08900795833394054, 0.08684214424848535, 0.08452821807131139, 0.08207574174279275, 0.07949484974027515, 0.07679620719892302, 0.07399096583970564, 0.07109071788664319, 0.06810744816374203, 0.06505348456957204, 0.06194144713414112, 0.05878419586858239, 0.05559477762315627, 0.05238637217316924, 0.0491722377556002, 0.04596565628149644, 0.04277987845053997, 0.039628068994590065, 0.036523252276474606, 0.03347825846883436, 0.030505670535428204, 0.0276177722339907, 0.0248264973555119, 0.022143380409700476, 0.01957950896041465, 0.017145477808027377, 0.014851345208060102, 0.012706591307004828, 0.010720078967091668, 0.008900017141887956, 0.007253926954073197, 0.00578861061556819, 0.004510123318451336, 0.0034237482128182, 0.0025339745749846923, 0.0018444792562503022, 0.001358111488881363, 0.0010768811121015178, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.4.weight
[0.1, 0.10090060036203999, 0.10060279665968103, 0.100107772956131, 0.09941749745846651, 0.09853471469206997, 0.09746293458843644, 0.09620641852973691, 0.09477016240562441, 0.0931598767496491, 0.09138196403425965, 0.0894434932146654, 0.08735217162277292, 0.08511631432294589, 0.08274481105142963, 0.08024709087088853, 0.07763308468058944, 0.07491318573128997, 0.0720982083018241, 0.06919934470168597, 0.06622812077056954, 0.06319635005179688, 0.06011608682184097, 0.05699957816269691, 0.05385921526766146, 0.05070748417412779, 0.04755691611928169, 0.044420037716083986, 0.04130932114763874, 0.038237134577973945, 0.03521569297640044, 0.03225700955097073, 0.02937284798413789, 0.02657467566052435, 0.023873618072766806, 0.021280414586718557, 0.01880537574188636, 0.01645834225687477, 0.014248645902831404, 0.012185072400459969, 0.010275826488122069, 0.008528499299916231, 0.006950038183438644, 0.005546719077229817, 0.004324121557733701, 0.0032871066549826516, 0.0024397975252123828, 0.0017855630572524586, 0.001327004477873133, 0.0010659450093453528, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.4.bias
[0.1, 0.10090433790388006, 0.1006177176649856, 0.10014123603103034, 0.09947671625128894, 0.09862670109996315, 0.09759444314629735, 0.09638389230867489, 0.09499968074031925, 0.09344710510443453, 0.09173210630660925, 0.08986124676203716, 0.08784168528454142, 0.08568114969348928, 0.08338790724341599, 0.08097073298950895, 0.07843887621000083, 0.0758020250139557, 0.07307026926987645, 0.07025406199698565, 0.06736417936691666, 0.06441167946886679, 0.06140785999599772, 0.05836421501499503, 0.05529239098420783, 0.05220414218866402, 0.04911128576248844, 0.04602565647083015, 0.04295906142432469, 0.03992323489937607, 0.036929793437137606, 0.033990191393004895, 0.03111567710670995, 0.028317249860730934, 0.025605617791715392, 0.022991156915968487, 0.0204838714257941, 0.018093355408614665, 0.01582875613535068, 0.013698739058536219, 0.011711454654104836, 0.009874507233725148, 0.00819492584702492, 0.006679137385046446, 0.005332941987851585, 0.004161490850379633, 0.003169266511483126, 0.002360065701565244, 0.0017369848144523463, 0.0013024080590929735, 0.0010579983364208225, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.5.weight
[0.1, 0.10090791254643015, 0.10063198938968479, 0.10017324689219934, 0.0995333748311113, 0.09871473017396933, 0.09772032839684762, 0.09655383237684538, 0.09521953889988587, 0.09372236283351341, 0.09206781902298794, 0.09026200197736267, 0.0883115634203714, 0.08622368778881645, 0.08400606576870902, 0.08166686596664163, 0.07921470482074078, 0.07665861486203324, 0.07400801144313472, 0.07127265805681604, 0.06846263037219556, 0.06558827912103081, 0.0626601919708172, 0.05968915452513443, 0.05668611059489588, 0.05366212188684131, 0.05062832725776066, 0.0475959016845358, 0.04457601510113382, 0.04157979125417616, 0.03861826672863849, 0.03570235029461042, 0.03284278272486129, 0.030050097231223444, 0.027334580665526104, 0.02470623562799575, 0.022174743622696738, 0.01974942939572909, 0.01743922658754354, 0.015252644825893502, 0.013197738380637081, 0.011282076495849037, 0.009512715508524144, 0.007896172856573198, 0.006438403071852388, 0.005144775846656346, 0.004020056254466558, 0.0030683871978120273, 0.0022932741478954846, 0.0016975722321967582, 0.001283475717615983, 0.00105250992789555, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.5.bias
[0.1, 0.1009113323509501, 0.1006456438818799, 0.10020387691166668, 0.09958759825785753, 0.09879899367962815, 0.09784086012553782, 0.0967165958135752, 0.09543018817867839, 0.09398619973047564, 0.09238975187140526, 0.09064650673260702, 0.08876264709200792, 0.0867448544458268, 0.08460028531127202, 0.08233654584447896, 0.07996166486371091, 0.07748406537350139, 0.07491253469073376, 0.07225619327861209, 0.06952446239905982, 0.066727030698274, 0.06387381984394613, 0.06097494933602408, 0.05804070061582133, 0.05508148060076782, 0.05210778477413378, 0.049130159960636736, 0.046159166919955266, 0.04320534289081982, 0.04027916421852523, 0.03739100919841437, 0.03455112126711625, 0.03176957267208819, 0.029056228748315784, 0.026420712928870427, 0.02387237261342183, 0.021420246015760043, 0.01907303010790856, 0.016839049774521813, 0.014726228286967229, 0.012742059201811356, 0.01089357978337811, 0.009187346044641508, 0.007629409494975253, 0.006225295677228476, 0.00497998457024989, 0.003897892926367152, 0.0029828586064650766, 0.002238126968221459, 0.001666339355777762, 0.0012695237316680311, 0.0010490874842321077, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.8.weight
[0.1, 0.10091460493288093, 0.10065871142422322, 0.1002331935557608, 0.09963950481257114, 0.09887967311825714, 0.09795629390796023, 0.096872521262866, 0.0956320571364852, 0.0942391387095111, 0.0926985239164462, 0.09101547519343688, 0.08919574150282972, 0.08724553869585068, 0.08517152828048367, 0.08298079466707374, 0.08068082096937915, 0.07827946344373138, 0.07578492465361363, 0.07320572545132241, 0.07055067587241744, 0.06782884504237856, 0.06504953019826246, 0.06222222493117544, 0.05935658675803952, 0.05646240413342002, 0.05354956301409634, 0.05062801309058413, 0.047707733800955804, 0.04479870024304911, 0.04191084910150126, 0.03905404470599471, 0.03623804533665346, 0.03347246989168364, 0.030766765031115056, 0.028130172908874648, 0.02557169960341218, 0.02310008435471332, 0.020723769712780307, 0.018450872699546872, 0.016289157082733013, 0.014246006856346367, 0.012328401018415557, 0.01054288973211014, 0.008895571951675535, 0.007392074589608787, 0.006037533296235824, 0.004836574917343139, 0.0037933016897856546, 0.0029112772290549975, 0.0021935143566721655, 0.0016424648089836615, 0.0012600108625139207, 0.0010474589044800855, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.8.bias
[0.1, 0.1009177374890553, 0.10067122064104952, 0.10026126061977891, 0.09968920639887294, 0.09895694032299596, 0.0980668719139912, 0.09702192994234815, 0.09582555279008198, 0.09448167713673598, 0.0929947250057358, 0.09136958921371949, 0.08961161727072212, 0.08772659378419174, 0.08572072142473605, 0.08360060051623196, 0.08137320731745706, 0.07904587106670707, 0.07662624986493392, 0.07412230547676138, 0.07154227713229487, 0.06889465441593139, 0.06618814933137827, 0.06343166763480101, 0.06063427953042905, 0.057805189825044415, 0.054953707639561256, 0.052089215777359255, 0.04922113985016499, 0.04635891726307252, 0.043511966160757726, 0.04068965443706912, 0.03790126890996908, 0.035155984763255245, 0.03246283535561424, 0.029830682496350926, 0.02726818728560103, 0.024783781614977365, 0.02238564042242675, 0.02008165479259315, 0.017879405991200192, 0.015786140518892978, 0.013808746266624518, 0.011953729851047618, 0.010227195204489836, 0.00863482348996148, 0.007181854407286364, 0.005873068951867116, 0.004712773682817761, 0.0037047865522290718, 0.002852424342195519, 0.002158491750942403, 0.0016252721639650018, 0.001254520140547726, 0.001047455640385935, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.9.weight
[0.1, 0.1009207368230938, 0.10068319859842367, 0.10028813844736262, 0.09973680891881632, 0.09903095801798296, 0.09817282366425545, 0.09716512659583727, 0.09601106174356858, 0.0947142881013112, 0.09327891712500969, 0.09170949969720886, 0.09001101169835748, 0.08818883823064463, 0.0862487565443873, 0.08419691772110188, 0.0820398271713337, 0.07978432400907674, 0.0774375593681775, 0.07500697372947152, 0.07250027333053746, 0.06992540573286218, 0.06729053462388189, 0.06460401393379, 0.06187436134917538, 0.05911023130746627, 0.056320387557802214, 0.05351367537533001, 0.05069899351701941, 0.047885266007913235, 0.04508141384726429, 0.04229632672426569, 0.03953883483305063, 0.03681768087632274, 0.03414149234638047, 0.031518754171419146, 0.0289577818138364, 0.026466694905833445, 0.024053391505901425, 0.021725523057813, 0.019490470131512807, 0.017355319022820528, 0.015326839286138105, 0.013411462271394382, 0.01161526073327578, 0.009943929577393311, 0.008402767804430104, 0.006996661709516177, 0.005730069390097569, 0.004607006611417546, 0.0036310340744242437, 0.002805246126471871, 0.00213226095060866, 0.001614212264556596, 0.0012527425557016182, 0.0010489978735428941, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.9.bias
[0.1, 0.10092360936911773, 0.10069467089761037, 0.10031388413564882, 0.09978241262487215, 0.09910188034304057, 0.09827436674176784, 0.09730240039249656, 0.09618895126013144, 0.09493742162793953, 0.09355163570144767, 0.09203582792310357, 0.09039463003340659, 0.08863305691804456, 0.08675649128428263, 0.08477066721342699, 0.0826816526396214, 0.08049583080851444, 0.07821988077245312, 0.0758607569818019, 0.07342566803474863, 0.07092205465053014, 0.06835756693338267, 0.06574004099669027, 0.06307747501875918, 0.06037800480338216, 0.05764987891987126, 0.05490143349852117, 0.052141066758519185, 0.049377213346134714, 0.046618318561601306, 0.04387281255344405, 0.041149084559105154, 0.03845545727057816, 0.03580016140338038, 0.03319131054657078, 0.030636876370662543, 0.028144664269185438, 0.025722289508328472, 0.02337715395754024, 0.02111642347218988, 0.01894700599739866, 0.016875530459948395, 0.014908326512765645, 0.013051405193874367, 0.011310440558916501, 0.0096907523433644, 0.008197289707402777, 0.006834616113150077, 0.005606895380428339, 0.004517878963690352, 0.0035708944889807684, 0.002768835585957923, 0.0021141530460460902, 0.0016088473337353242, 0.0012544624739116985, 0.0010520813338958982, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.11.weight
[0.1, 0.10092636121389996, 0.10070566176243254, 0.10033855172718571, 0.0998261124496539, 0.09916985334609102, 0.09837170746144719, 0.09743402577548546, 0.09635957027784992, 0.09515150583248314, 0.09381339085535663, 0.0923491668329735, 0.09076314671251723, 0.08906000219784384, 0.08724474998873788, 0.08532273700396509, 0.08329962463164818, 0.08118137205335679, 0.07897421869103168, 0.07668466582844637, 0.07431945746134137, 0.07188556043263776, 0.06939014391124303, 0.06684055827489511, 0.06424431345924679, 0.061609056836963765, 0.05894255069199511, 0.05625264935536661, 0.053547276069844565, 0.05083439965161672, 0.048122011017734195, 0.045418099648454176, 0.042730630053814944, 0.04006751831376168, 0.037436608760925846, 0.034845650874739835, 0.032302276454946634, 0.029813977141741128, 0.027388082348758416, 0.02503173767390898, 0.022751883851652378, 0.020555236308706884, 0.018448265383414576, 0.016437177267026915, 0.014527895723049154, 0.012726044638490159, 0.011036931458413925, 0.00946553155258709, 0.008016473560271426, 0.006694025756328581, 0.005502083478797103, 0.004444157654973734, 0.003523364459796479, 0.0027424161369910428, 0.0021036130100173293, 0.0016088367063484424, 0.0012595446150401083, 0.0010567655939162637, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.11.bias
[0.1, 0.10092899811756442, 0.10071619412095008, 0.10036219238953092, 0.09986799831487964, 0.09923501544537972, 0.0984650415000426, 0.09756026326285001, 0.09652325037212311, 0.09535694802255593, 0.0940646686006408, 0.09265008227724131, 0.09111720658403055, 0.08947039500339891, 0.08771432460423599, 0.08585398275870301, 0.08389465297772024, 0.08184189990539813, 0.07970155351502911, 0.07747969255152438, 0.07518262726732071, 0.07281688150078834, 0.07038917414803879, 0.06790640008075394, 0.0653756105642314, 0.06280399323126, 0.060198851668701564, 0.057567584674754195, 0.0549176652458083, 0.05225661935257423, 0.04959200456575866, 0.046931388591994735, 0.044282327780985516, 0.041652345664901703, 0.03904891159098385, 0.03647941950803376, 0.03395116696704316, 0.031471334395599335, 0.029046964704930223, 0.02668494328750608, 0.024391978462006515, 0.022174582421190345, 0.020039052736777613, 0.017991454473872015, 0.016037602965719017, 0.014183047297721761, 0.012433054547620677, 0.01079259482659592, 0.009266327163777642, 0.007858586274252271, 0.006573370248145276, 0.005414329195744293, 0.00438475488091094, 0.0034875713722236296, 0.002725326738402644, 0.0021001858116030826, 0.0016139240391285661, 0.0012679224410271851, 0.0010631636878905607, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.12.weight
[0.1, 0.1009315255329381, 0.10072628968185798, 0.10038485458338063, 0.09990815542096221, 0.09929749786344655, 0.09855455448886599, 0.09768136020328577, 0.0966803066672398, 0.09555413574502325, 0.09430593199478497, 0.09293911421998823, 0.09145742610538098, 0.08986492596312326, 0.08816597561715654, 0.086365228456261, 0.08446761668852287, 0.0824783378321216, 0.08040284047943837, 0.07824680937347772, 0.07601614983747762, 0.07371697160035505, 0.0713555720622887, 0.06893841904627411, 0.06647213308289449, 0.06396346927682837, 0.06141929880476208, 0.05884659009538321, 0.056252389743002965, 0.05364380320708456, 0.051027975350541314, 0.04841207087010928, 0.04580325467239578, 0.04320867224935283, 0.0406354301069271, 0.038090576300491535, 0.03558108113037197, 0.033113818050343136, 0.030695544841385314, 0.028332885102266232, 0.026032310107645253, 0.02380012108339012, 0.021642431947653498, 0.019565152564981602, 0.01757397255932111, 0.01567434573026048, 0.013871475115189203, 0.012170298738289183, 0.01057547608539191, 0.009091375341745995, 0.007722061427650911, 0.006471284864727011, 0.005342471503316659, 0.0043387131391532715, 0.003462759044998636, 0.002717008440443474, 0.0021035039204961073, 0.0016239258609582743, 0.001279587815911625, 0.001071432919921116, 0.0010000313048095168, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.12.bias
[0.1, 0.1009339486236499, 0.10073596900597234, 0.10040658422001676, 0.09994666451852163, 0.09935742503465872, 0.0986404225715844, 0.09779755148927992, 0.0968310386995482, 0.0957434377823901, 0.09453762223930554, 0.0932167779013443, 0.09178439451196507, 0.09024425650694104, 0.0886004330156722, 0.08685726711032159, 0.08501936433117939, 0.083091580518572, 0.08107900898346455, 0.07898696705065292, 0.07682098201009886, 0.0745867765135257, 0.07229025345485732, 0.06993748037444751, 0.06753467342830428, 0.06508818096466351, 0.06260446675130332, 0.06009009289791385, 0.057551702518642084, 0.05499600218061859, 0.052429744184838006, 0.04985970872620793, 0.0472926859799008, 0.044735458161337276, 0.04219478160719958, 0.03967736892481821, 0.037189871257093644, 0.03473886070981071, 0.03233081298777328, 0.02997209028563564, 0.027668924478633974, 0.02542740065762887, 0.02325344105196029, 0.02115278938259161, 0.019130995686882552, 0.017193401655085035, 0.015345126517303364, 0.013591053518206525, 0.011935817015226863, 0.010383790234332658, 0.00893907371572428, 0.007605484479981383, 0.006386545943284172, 0.005285478608354087, 0.004305191555708154, 0.003448274757708197, 0.0027169922357112345, 0.0021132760784004214, 0.00163872133710034, 0.0012945818115636712, 0.001081766737363126, 0.0010008383836410664, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.15.weight
[0.1, 0.10093627228106483, 0.10074525157314573, 0.1004274248088021, 0.09998360216302227, 0.09941491498799933, 0.09872281292919138, 0.09790906023001751, 0.09697573123460958, 0.0959252051000833, 0.0947601597318081, 0.09348356495713502, 0.0920986749549838, 0.09060901996058704, 0.08901839726653643, 0.0873308615430711, 0.08555071450228219, 0.08368249393258083, 0.08173096213138203, 0.07970109376549057, 0.07759806319013435, 0.07542723125897009, 0.07319413165868438, 0.07090445680302447, 0.06856404332221654, 0.06617885718476046, 0.06375497848952728, 0.061298585966925924, 0.05881594122864734, 0.056313372806135444, 0.053797260018472084, 0.05127401671079887, 0.04875007490472847, 0.04623186840242195, 0.043725816386127284, 0.04123830705498574, 0.038775681340817436, 0.03634421674439659, 0.033950111333419276, 0.03159946794295474, 0.029298278618655194, 0.027052409342379624, 0.024867585079168204, 0.022749375183683685, 0.02070317920332064, 0.018734213114172005, 0.01684749602493867, 0.015047837382675538, 0.013339824712987954, 0.011727811925929994, 0.010215908217414166, 0.008807967594424225, 0.007507579050732144, 0.006318057418162676, 0.005242434916726534, 0.004283453425161786, 0.0034435574915867733, 0.00272488810208109, 0.0021292772230791838, 0.001658243131488756, 0.0013129865444376988, 0.001094387558515613, 0.0010030034063118801, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.15.bias
[0.1, 0.10093850114013428, 0.10075415584492767, 0.10044741759539925, 0.10001904095365396, 0.09947007970669422, 0.09880188427415196, 0.09801609838631793, 0.09711465504064015, 0.09609977174663835, 0.0949739450709316, 0.09373994449579844, 0.09240080560637663, 0.0909598226232622, 0.08942054029887708, 0.0877867451975397, 0.08606245638068953, 0.08425191552017917, 0.08235957646395511, 0.08039009427979482, 0.07834831380405238, 0.07623925772358198, 0.07406811422015726, 0.07184022420778084, 0.06956106819427943, 0.0672362527995045, 0.0648714969633031, 0.062472617877186755, 0.060045516674305514, 0.0575961639129296, 0.055130584889148404, 0.05265484481491675, 0.05017503389791022, 0.04769725235989193, 0.045227595430445286, 0.0427721383529871, 0.04033692143994588, 0.037927935213868745, 0.03555110567100914, 0.033212279703646284, 0.030917210716996893, 0.028671544476100642, 0.02648080521749569, 0.02435038205984873, 0.022285515746968854, 0.020291285755816704, 0.018372597801222985, 0.016534171768053448, 0.014780530100507665, 0.013115986677112269, 0.011544636198776704, 0.010070344116015654, 0.008696737120117086, 0.007427194221647049, 0.006264838438237807, 0.0052125291121068555, 0.004272854876205561, 0.003448127286300017, 0.0027403751346493912, 0.002151339459269457, 0.0016824692610584438, 0.0013349179393209612, 0.0011095404544582735, 0.0010068912248047172, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.16.weight
[0.1, 0.10094063959423796, 0.10076269932326272, 0.10046660169134174, 0.10005304975749806, 0.09952302546615553, 0.0988777873155941, 0.09811886736975095, 0.09724806762046316, 0.09626745570878892, 0.09517936001556711, 0.09398636413287205, 0.09269130072949049, 0.09129724482498693, 0.08980750648832724, 0.08822562297839712, 0.08655535034507729, 0.08480065451081766, 0.08296570185388663, 0.081054849315655, 0.07907263405540359, 0.07702376267721865, 0.07491310005455472, 0.0727456577790001, 0.07052658226067254, 0.06826114250849939, 0.06595471761939742, 0.06361278400605767, 0.06124090239366238, 0.05884470461640878, 0.05642988024519068, 0.054002163078188795, 0.05156731752644751, 0.04913112492676359, 0.04669936981438631, 0.044277826188123065, 0.04187224380046284, 0.039488334505270686, 0.0371317586954691, 0.03480811186290921, 0.032522911312344194, 0.0302815830610512, 0.02808944895520792, 0.02595171403361468, 0.02387345416876578, 0.021859604014615466, 0.019914945289655518, 0.018044095423125208, 0.01625149659131247, 0.014541405169978254, 0.01291788162794899, 0.011384780885873486, 0.009945743163036619, 0.008604185333963826, 0.007363292815338573, 0.006226012002497344, 0.005195043273460786, 0.004272834577112255, 0.0034615756207482134, 0.0027631926708018054, 0.0021793439790844795, 0.0017114158454059718, 0.0013605193259215989, 0.0011274875950224464, 0.0010128739670325784, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.16.bias
[0.1, 0.10094269180908678, 0.10077089860549697, 0.1004850141955389, 0.10008569391994637, 0.09957385315162298, 0.09895066519730744, 0.09821755860797017, 0.09737621390410643, 0.09642855972343274, 0.09537676839981735, 0.09422325098357923, 0.0929706517145708, 0.09162184196071398, 0.09017991363588389, 0.08864817211222946, 0.08703012864317738, 0.08532949231448886, 0.08355016154181982, 0.08169621513427504, 0.0797719029444411, 0.0777816361263321, 0.07572997702357925, 0.07362162871104454, 0.0714614242138322, 0.06925431542841176, 0.06700536177124886, 0.0647197185809651, 0.06240262530061278, 0.06005939346715488, 0.057695394535682065, 0.05531604756627761, 0.052926806801756425, 0.0505331491647533, 0.04814056170282196, 0.0457545290103242, 0.04338052065594267, 0.04102397864463749, 0.03869030494278781, 0.036384849095115, 0.03411289596177352, 0.03187965360371978, 0.02969024134413023, 0.027549678033235315, 0.025462870543470312, 0.02343460252131608, 0.021469523421614175, 0.019572137849492743, 0.017746795234335932, 0.015997679859466624, 0.01432880127039806, 0.012743985083642016, 0.011246864217142171, 0.009840870562436072, 0.008529227117635157, 0.007314940599257202, 0.006200794549846982, 0.0051893429571846774, 0.004282904399709141, 0.0034835567315765195, 0.002793132319537649, 0.002213213842553168, 0.0017451306637748347, 0.0013899557832095335, 0.0011485033780517897, 0.0010213269363228575, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.18.weight
[0.1, 0.1009446617357511, 0.10077876943594398, 0.1005026903082556, 0.10011703546227482, 0.09962265855679267, 0.0990206539102025, 0.09831235407819282, 0.09749932690409446, 0.0965833720484115, 0.09556651700521769, 0.09445101261423636, 0.09323932807853785, 0.0919341454988831, 0.09053835393681153, 0.08905504301961507, 0.0874874961013537, 0.08583918299505083, 0.0841137522921561, 0.08231502328627673, 0.08044697751905429, 0.07851374996690061, 0.07651961988810108, 0.07446900135054556, 0.07236643346105424, 0.07021657031792597, 0.06802417070894924, 0.06579408757768011, 0.06353125728130318, 0.061240688663854084, 0.058927451968990144, 0.056596667616851, 0.05425349486985205, 0.05190312041249948, 0.049550746870505515, 0.047201581294617295, 0.04486082363465092, 0.0425336552292436, 0.04022522733680214, 0.037940649733034994, 0.035684979400307536, 0.03346320933385702, 0.03128025748964493, 0.029140955898311185, 0.027050039969326534, 0.02501213800901901, 0.023031760975676347, 0.02111329249440183, 0.019260979153826335, 0.017478921106154323, 0.015771062991351715, 0.01414118520556485, 0.012592895533097809, 0.011129621160471774, 0.009754601090242752, 0.008470878971370527, 0.007281296362008549, 0.006188486439628115, 0.005194868172399574, 0.004302640964732312, 0.0035137797888258266, 0.0028300308130084543, 0.002252907536540343, 0.0017836874394365756, 0.00142340915472614, 0.0011728701694059364, 0.0010326250591789312, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.18.bias
[0.1, 0.100946553122874, 0.10078632675424296, 0.10051966343806835, 0.10014713326720992, 0.09966953266463836, 0.09908788268078253, 0.09840342681065112, 0.09761762833539567, 0.09673216719399831, 0.09574893639177201, 0.0946700379533517, 0.09349777842882814, 0.09223466396263204, 0.09088339493571022, 0.08944686019244852, 0.08792813086468337, 0.08633045380600549, 0.0846572446503926, 0.08291208051001052, 0.08109869232779428, 0.07922095690115817, 0.0772828885938871, 0.07528863075392823, 0.07324244685543081, 0.07114871138397083, 0.0690119004844474, 0.06683658239164404, 0.06462740766391341, 0.06238909924086439, 0.06012644234630729, 0.05784427425804296, 0.05554747396636718, 0.05324095174339882, 0.05092963864553149, 0.048618475971450824, 0.046312404698255205, 0.04401635491826395, 0.0417352352990957, 0.039473922589550285, 0.03723725119372889, 0.03503000283568211, 0.032856896336681075, 0.030722577526967165, 0.028631609313547197, 0.026588461925268203, 0.02459750335602673, 0.022662990026543248, 0.020789057684666552, 0.018979712563661412, 0.01723882281738224, 0.01557011025064341, 0.013977142362465451, 0.012463324719207785, 0.0110318936738932, 0.009685909447289441, 0.008428249585539432, 0.007261602808327033, 0.006188463260730309, 0.005211125181050851, 0.004331677996018834, 0.00355200185385977, 0.002873763604772383, 0.0022984132374110947, 0.0018271807789914875, 0.0014610736656456617, 0.001200874588649691, 0.0010471398211274398, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.19.weight
[0.1, 0.10094836952812497, 0.10079358474072486, 0.10053596530226423, 0.10017604325326993, 0.09971456191155105, 0.09915247433708502, 0.09849094136373998, 0.09773132920189893, 0.09687520661693622, 0.09592434168937358, 0.09488069816340705, 0.09374643139134518, 0.09252388388233493, 0.09121558046456751, 0.0898242230709555, 0.08835268515905007, 0.08680400577672218, 0.08518138328586351, 0.08348816875706923, 0.0817278590489436, 0.07990408958632168, 0.07802062685232136, 0.07608136060973125, 0.07409029586779918, 0.07205154461101158, 0.06996931730694536, 0.06784791421073094, 0.06569171648408412, 0.06350517714724817, 0.061292811882532494, 0.05905918970844018, 0.05680892354364475, 0.05454666068030356, 0.052277073186382174, 0.05000484825681152, 0.047734678533404386, 0.04547125241352301, 0.043219244367512365, 0.04098330528489569, 0.03876805286926979, 0.0365780621017363, 0.03441785579256438, 0.03229189524059769, 0.030204571019695627, 0.028160193911237426, 0.02616298600141513, 0.024217071961701392, 0.022326470530500173, 0.020495086213572294, 0.018726701220376413, 0.01702496765297881, 0.015393399963663061, 0.013835367696816998, 0.012354088530085988, 0.010952621629164262, 0.00963386132994835, 0.008400531161099761, 0.007255178219362306, 0.006200167909249837, 0.0052376790579680855, 0.004369699415659711, 0.003598021550265117, 0.0029242391454773467, 0.002349743709436413, 0.0018757217009604246, 0.0015031520792484535, 0.0012328042821155118, 0.001065236636935051, 0.0010007952075707473, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.19.bias
[0.1, 0.10095011432894495, 0.10080055685898683, 0.10055162602111566, 0.10020381853860717, 0.09975782843584789, 0.09921454565346022, 0.09857505427249105, 0.0978406303512073, 0.09701273937881508, 0.09609303335118309, 0.09508334747440583, 0.09398569650278399, 0.09280227071852974, 0.09153543156121874, 0.0901877069157102, 0.08876178606793768, 0.08726051433863571, 0.08568688740571036, 0.08404404532658319, 0.0823352662724364, 0.08056395998686204, 0.0787336609819681, 0.07684802148551817, 0.07491080415317823, 0.07292587456041241, 0.07089719348900995, 0.06882880902363504, 0.06672484847416987, 0.06458951013996977, 0.06242705493246401, 0.06024179787281867, 0.058038099481627685, 0.05582035707781299, 0.05359299600409622, 0.051360460796551155, 0.04912720631585708, 0.04689768885795026, 0.044676357261811364, 0.04246764403213274, 0.0402759564945796, 0.03810566800129416, 0.035961109204191176, 0.033846559413459126, 0.031766238058509574, 0.029724296268414604, 0.02772480858863238, 0.02577176485054979, 0.02386906221006583, 0.022020497371102035, 0.02022975900955759, 0.01850042041282654, 0.01683593234956401, 0.01523961618393, 0.013714657248050016, 0.012264098485917994, 0.010890834381424103, 0.009597605182623852, 0.008386991433773649, 0.0072614088260435075, 0.006223103377181998, 0.005274146949752172, 0.004416433116881424, 0.003651673383774772, 0.0029813937725315146, 0.002406931777079701, 0.001929433694304871, 0.0015498523366985033, 0.0012689451310903591, 0.0010872726072586815, 0.0010051972794340104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.22.weight
[0.1, 0.10095179073263055, 0.10080725589586059, 0.10056667420643373, 0.10023050959502956, 0.09979941031163335, 0.09927420767546971, 0.09865591447191106, 0.09794572299945192, 0.09714500277051545, 0.09625529787052586, 0.0952783239803393, 0.09421596506777513, 0.09307026975462772, 0.09184344736616407, 0.09053786367072578, 0.08915603631765073, 0.08770062998231182, 0.08617445122763512, 0.08458044309200614, 0.08292167941400089, 0.08120135890488554, 0.07942279898031555, 0.07758942936312835, 0.07570478546956641, 0.07377250159168416, 0.07179630388908573, 0.06978000320350788, 0.06772748771010453, 0.06564271541960377, 0.06352970654579586, 0.06139253575307088, 0.05923532429895523, 0.05706223208679946, 0.05487744964394259, 0.052685190040821867, 0.05048968076661127, 0.048295155577055326, 0.04610584633021854, 0.043925974825894276, 0.041759744664409904, 0.03961133314052695, 0.03748488318806836, 0.03538449539080645, 0.03331422007501707, 0.031278049498949315, 0.029279910154271668, 0.02732365519434045, 0.025413057003891966, 0.023551799924486125, 0.021743473149730293, 0.01999156380398355, 0.01829945021788807, 0.01667039541369558, 0.015107540812950647, 0.013613900178665522, 0.012192353803667927, 0.010845642956328665, 0.009576364594380077, 0.008386966357018872, 0.007279741844950388, 0.006256826197476239, 0.005320191975154253, 0.0044716453559704656, 0.0037128226523584934, 0.0030451871557824854, 0.0024700263149688368, 0.0019884492532277745, 0.0016013846296524183, 0.0013095788483198693, 0.0011135946189474101, 0.0010138098717795965, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.22.bias
[0.1, 0.10095340178580126, 0.10081369399894771, 0.10058113704477442, 0.10025616439283197, 0.09983938176893006, 0.0993315660261081, 0.09873366369663629, 0.09804678922774746, 0.0972722229043867, 0.09641140846285196, 0.09546595039977245, 0.0944376109814418, 0.09332830695907934, 0.09214010599614293, 0.09087522281435184, 0.08953601506560371, 0.08812497893747989, 0.08664474450052995, 0.08509807080600823, 0.08348784074319922, 0.08181705566591797, 0.08008882979820092, 0.0783063844296148, 0.07647304191100288, 0.07459221946186047, 0.07266742280088177, 0.07070223961155067, 0.068700332854955, 0.06666543394228877, 0.06460133577976819, 0.06251188569892542, 0.06040097828545746, 0.05827254811999654, 0.05613056244433299, 0.05397901376675979, 0.051821912420322414, 0.04966327908784445, 0.04750713730766204, 0.04535750597403579, 0.04321839184621901, 0.04109378208014468, 0.038987636796651466, 0.03690388170010157, 0.034846400761148075, 0.032819028977291054, 0.030825545224716346, 0.028869665214740072, 0.026955034567988354, 0.025085222019220716, 0.023263712765463507, 0.021493901969851625, 0.01977908843328684, 0.01812246844570869, 0.016527129828438247, 0.01499604617869978, 0.013532071327047968, 0.012137934018031583, 0.010816232824008516, 0.009569431301592629, 0.008399853399760092, 0.007309679128174573, 0.00630094049380494, 0.005375517713409371, 0.004535135708945697, 0.003781360892440016, 0.0031155982463066103, 0.0025390887045610927, 0.0020529068398080046, 0.0016579588603142896, 0.0013549809209017763, 0.0011445377508071887, 0.001027021601067212, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.23.weight
[0.1, 0.10095495038329078, 0.10081988271188166, 0.10059504037564578, 0.1002808285370258, 0.09987781340093756, 0.09938672119447739, 0.09880843685827208, 0.09814400245182947, 0.09739461527576306, 0.09656162571427483, 0.09564653480178348, 0.09465099151808345, 0.09357678981690874, 0.09242586539325555, 0.09120029219528976, 0.08990227868712408, 0.08853416386919956, 0.08709841306344267, 0.08559761347079298, 0.08403446950910674, 0.08241179793983736, 0.08073252279227448, 0.07899967009448797, 0.07721636242047177, 0.07538581326331344, 0.07351132124452892, 0.07159626416999702, 0.06964409294320509, 0.06765832534677384, 0.06564253970346699, 0.0636003684281085, 0.06153549148202673, 0.059451629741820625, 0.057352538294397015, 0.0552419996703617, 0.053123817027957126, 0.05100180729982976, 0.04887979431497583, 0.046761601908260085, 0.044651047029924104, 0.04255193286750041, 0.04046804199252682, 0.03840312954441063, 0.03636091646372485, 0.0343450827871304, 0.03235926101600652, 0.03040702957073887, 0.028491906342460946, 0.026617342353868152, 0.024786715540527755, 0.02300332466389098, 0.021270383366975162, 0.019591014383428595, 0.017968243910412112, 0.0164049961554384, 0.014904088066994774, 0.013468224258445249, 0.012099992134358688, 0.01080185722804548, 0.009576158758704278, 0.008425105416184846, 0.007350771380962928, 0.006355092586498656, 0.005439863230714314, 0.004606732542877039, 0.0038572018117130785, 0.003192621680108453, 0.0026141897112712245, 0.0021229482307409276, 0.0017197824481339488, 0.0014054188620093492, 0.0011804239507296247, 0.001045203151675366, 0.0010000001306533048, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.23.bias
[0.1, 0.10095643927650017, 0.10082583300746593, 0.10060840876504021, 0.10030454539551524, 0.09991477235921992, 0.0994397688079727, 0.09888036240170786, 0.09823752786633694, 0.09751238529535794, 0.09670619819817218, 0.09582037129850884, 0.09485644808679376, 0.0938161081307269, 0.09270116414875518, 0.09151355885153926, 0.09025536155691828, 0.08892876458427063, 0.08753607943455313, 0.08607973276267498, 0.0845622621492239, 0.08298631167891235, 0.08135462733344785, 0.0796700522068548, 0.07793552155158541, 0.07615405766405073, 0.07432876461848392, 0.07246282285831161, 0.070559483654457, 0.06862206344023151, 0.0666539380326854, 0.06465853675048626, 0.0626393364385751, 0.060599855410011054, 0.058543647315560846, 0.05647429495171491, 0.054395404017919036, 0.05231059683389938, 0.05022350602802767, 0.04813776820772447, 0.046057017622929075, 0.04398487983367738, 0.041924965392821006, 0.03988086355489564, 0.03785613602209995, 0.03585431073828268, 0.03387887574175119, 0.03193327308761248, 0.030020892850236888, 0.028145067216294263, 0.026309064678655555, 0.024516084341276285, 0.0227692503449851, 0.021071606423890544, 0.019426110601890513, 0.017835630038525885, 0.0163029360331588, 0.014830699196180335, 0.01342148479566177, 0.012077748287557238, 0.010801831037246173, 0.00959595623987055, 0.008462225046575345, 0.0074026129034019, 0.0064189661092136805, 0.005512998598651597, 0.0046862889557247375, 0.003940277663239921, 0.003276264592862731, 0.002695406740183536, 0.0021987162087348513, 0.0017870584464729049, 0.001461150737796029, 0.0012215609537275954, 0.0010687065624410844, 0.0010028539018514795, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.25.weight
[0.1, 0.10095787108124868, 0.10083155531882651, 0.10062126557459318, 0.10032735621973066, 0.09995032253757219, 0.09949079988897225, 0.09894956264162394, 0.09832752286512801, 0.09762572879401261, 0.09684536306129386, 0.09598774070555374, 0.09505430695489402, 0.09404663479150034, 0.09296642230092071, 0.0918154898105251, 0.09059577682196857, 0.0893093387428269, 0.0879583434229127, 0.0865450675011086, 0.08507189256887392, 0.0835413011568897, 0.08195587255160547, 0.0803182784487371, 0.0786312784510409, 0.07689771541795035, 0.0751205106749123, 0.07330265909049567, 0.07144722402956859, 0.0695573321910482, 0.06763616833892312, 0.06568696993542666, 0.06371302168540573, 0.06171765000107803, 0.05970421739650631, 0.05767611682123497, 0.05563676594263816, 0.0535896013866147, 0.05153807294633465, 0.04948563776879705, 0.047435754528995386, 0.045391877601508056, 0.043357451239335626, 0.041335903769794874, 0.039330641817249615, 0.03734504456241477, 0.03538245804790727, 0.03344618953963956, 0.03153950195355829, 0.02966560835711939, 0.02782766655476572, 0.026028773766531812, 0.024271961408742306, 0.022560189985600533, 0.020896344100274772, 0.01928322759388969, 0.01772355882061466, 0.016219966066810827, 0.014774983121956565, 0.013391045008815116, 0.012070483880039524, 0.010815525088129763, 0.009628283435365075, 0.008510759610030566, 0.007464836814943842, 0.006492277593963069, 0.005594720861824314, 0.004773679142313308, 0.004030536019425728, 0.0033665438058114, 0.002782821432431059, 0.0022803525629823624, 0.0018599839362723628, 0.0015224239393298693, 0.0012682414136622679, 0.0010978646966684482, 0.0010115808998230882, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.25.bias
[0.1, 0.10095924828515429, 0.10083705956870764, 0.10063363302664925, 0.10034930025819651, 0.09998452474526542, 0.09953990109696348, 0.0990161540803358, 0.09841413743894532, 0.09773483250121212, 0.09697934658078085, 0.09614891117152348, 0.09524487994011983, 0.09426872651948807, 0.09322204210666192, 0.09210653286903037, 0.09092401716316784, 0.08967642257078812, 0.08836578275665413, 0.08699423415356466, 0.08556401247982255, 0.08407744909486037, 0.0825369671989647, 0.08094507788329337, 0.07930437603662473, 0.07761753611551087, 0.07588730778473, 0.07411651143514429, 0.07230803358626928, 0.07046482218104902, 0.06858988178050698, 0.06668626866610537, 0.06475708585779678, 0.06280547805588879, 0.06083462651496674, 0.05884774385823064, 0.05684806884069881, 0.0548388610698152, 0.05282339569206562, 0.050804958054264285, 0.048786838348213166, 0.04677232624746369, 0.04476470554492318, 0.042767248800047175, 0.04078321200434301, 0.03881582927388036, 0.036868307577460235, 0.034943821509035716, 0.03304550811290624, 0.031176461770120085, 0.029339729154421002, 0.027538304265961145, 0.02577512355087529, 0.02405306111467237, 0.02237492403724627, 0.020743447797142996, 0.019161291812543165, 0.01763103510622779, 0.01615517210159431, 0.014736108556575415, 0.013376157642088448, 0.012077536171408283, 0.01084236098660956, 0.009672645507968869, 0.00857029645195168, 0.0075371107231336385, 0.006574772485122864, 0.005684850415257797, 0.004868795147555476, 0.004127936908078621, 0.0034634833465758677, 0.0028765175679298805, 0.0023679963666220927, 0.0019387486670920375, 0.001589474172533705, 0.0013207422243312996, 0.001132990873993738, 0.0010265261691006547, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.26.weight
[0.1, 0.10096057325457508, 0.1008423551970305, 0.1006455322654966, 0.10037041486347804, 0.10001743687032567, 0.09958715495697856, 0.09908024770805501, 0.09849751455167738, 0.09783987449871788, 0.09710836469345453, 0.09630413877792204, 0.09542846507253784, 0.09448272457587222, 0.09346840878671721, 0.09238711735188876, 0.09124055554347213, 0.09003053156948869, 0.08875895372222566, 0.08742782736872591, 0.08603925178818417, 0.08459541686123712, 0.08309859961636877, 0.08155116063887757, 0.07995554034806857, 0.07831425514854179, 0.07662989346164649, 0.07490511164335996, 0.07314262979502875, 0.07134522747357884, 0.06951573930796044, 0.06765705052874005, 0.0657720924178906, 0.06386383768595498, 0.06193529578387411, 0.059989508156872554, 0.05802954344788713, 0.05605849265810279, 0.05407946427222785, 0.052095579356196665, 0.05010996663503069, 0.04812575755862073, 0.046146081363211786, 0.0441740601363791, 0.042212803893277966, 0.040265405671932764, 0.038334936655300085, 0.036424441327798665, 0.03453693267394508, 0.032675387426666584, 0.030842741372785643, 0.029041884723079114, 0.02727565755421395, 0.02554684532974813, 0.02385817450726015, 0.022212308238534956, 0.02061184216958778, 0.019059300347149116, 0.01755713123806669, 0.01610770386790255, 0.014713304084814045, 0.013376130954611938, 0.012098293292679974, 0.010881806338225768, 0.009728588576108026, 0.00864045871125177, 0.007619132800423822, 0.006666221545891509, 0.0057832277552325945, 0.0049715439713029375, 0.00423245027609913, 0.003567112271979777, 0.002976579243429322, 0.0024617825022629925, 0.0020235339188831906, 0.001662524641903106, 0.0013793240081571114, 0.0011743786448168473, 0.0010480117650289713, 0.0010004226581854616, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
features.module.26.bias
[0.1, 0.10096184824113959, 0.10084745118682648, 0.10065698341501392, 0.10039073559292194, 0.10004911403345873, 0.09963264007515868, 0.09914194928658289, 0.09857779049639699, 0.09794102465060589, 0.09723262349868955, 0.09645366811064494, 0.09560534722719717, 0.0946889554456975, 0.09370589124447672, 0.09265765484866878, 0.09154584594076176, 0.09037216121936978, 0.0891383918099516, 0.08784642053142713, 0.08649821902286325, 0.08509584473461398, 0.08364143778850644, 0.0821372177118643, 0.08058548005035304, 0.07898859286481577, 0.07734899311744566, 0.07566918295281026, 0.07395172587940291, 0.0721992428575483, 0.07041440829963269, 0.06859994598876247, 0.06675862492207944, 0.06489325508507647, 0.0630066831633618, 0.06110178819841669, 0.05918147719397533, 0.057248680679732926, 0.05530634823915096, 0.05335744400818543, 0.0514049421518068, 0.04945182232521507, 0.04750106512667618, 0.04555564754891951, 0.043618538436037434, 0.041692693952820374, 0.03978105307344142, 0.037886533096374846, 0.03601202519239353, 0.03416038999243886, 0.03233445322209665, 0.030537001389341192, 0.028770777532127922, 0.027038477032324627, 0.025342743502368995, 0.02368616475093003, 0.022071268833730135, 0.020500520195553963, 0.018976315909332474, 0.017500982018041008, 0.016076769984993862, 0.014705853257953087, 0.013390323952294243, 0.012132189658290986, 0.010933370377391331, 0.009795695592160158, 0.00872090147436013, 0.007710628235431577, 0.006766417623414239, 0.005889710570131484, 0.005081844992226824, 0.004344053749409027, 0.0036774627630215377, 0.003083089297807195, 0.0025618404094907, 0.002114511560547804, 0.001741785406273543, 0.0014442307530022726, 0.0012223016900691766, 0.0010763368968379696, 0.0010065591258523955, 0, 0, 0, 0, 0, 0, 0, 0, 0]
classifier.0.weight
[0.1, 0.10096307538789243, 0.10085235608864884, 0.10066800563295637, 0.10041029630357774, 0.10007960873319388, 0.09967643134221543, 0.09920135961739328, 0.0986550952322939, 0.09803844501093843, 0.09735231973613606, 0.09659773280427193, 0.0957757987285365, 0.09488773149280672, 0.09393484275861082, 0.09291853992782444, 0.09184032406395999, 0.09070178767511919, 0.08950461236188312, 0.08825056633361444, 0.08694150179683949, 0.08557935221956804, 0.08416612947559106, 0.08270392087297422, 0.08119488607113641, 0.07964125389106623, 0.07804531902338796, 0.07640943863913913, 0.0747360289082654, 0.0730275614309752, 0.07128655958722444, 0.06951559480972341, 0.0677172827859708, 0.0658942795949236, 0.0640492777840098, 0.06218500239227768, 0.06030420692555525, 0.05840966928956505, 0.05650418768700034, 0.054590576484623225, 0.05267166205648863, 0.0507502786094336, 0.0488292639969978, 0.046911455527958, 0.04499968577566678, 0.04309677839438583, 0.041205543948792396, 0.03932877576281876, 0.037469245793956295, 0.03562970053911708, 0.03381285697810066, 0.0320213985606571, 0.030257971243073223, 0.028525179580136308, 0.026825582878247024, 0.025161691415363653, 0.02353596273336042, 0.021950798008276363, 0.020408538503815462, 0.01891146211333658, 0.017461779995440296, 0.016061633308121864, 0.01471309004631427, 0.013418141987491548, 0.012178701749844624, 0.010996599967373667, 0.009873582586070285, 0.008811308285182267, 0.007811346027370069, 0.0068751727413734086, 0.0060041711406102925, 0.005199627680930862, 0.004462730660542013, 0.0037945684649093917, 0.0031961279592289385, 0.002668293030842316, 0.002211843283749048, 0.0018274528871434206, 0.00151568957967717, 0.001277013830918177, 0.0011117781612441527, 0.0010202266211754454, 0.0010024944309162556, 0.0010586077806364772, 0.0011884837917891958, 0.0013919306395210709, 0.0016686478359946126, 0.0020182266742040872, 0.002440150831629428, 0.002933797132836568]
classifier.0.bias
[0.1, 0.10096425673507994, 0.10085707804355926, 0.1006786171620925, 0.1004291292416623, 0.10010897098278196, 0.09971860012551007, 0.09925857479500536, 0.09872955270355814, 0.09813229021023895, 0.09746764123950881, 0.09673655606033732, 0.09594007992757397, 0.09507935158751543, 0.09415560164980502, 0.09317015082799246, 0.09212440805126933, 0.09101986845007992, 0.08985811121848743, 0.08864079735635191, 0.08736966729454798, 0.0860465384066175, 0.08467330241041507, 0.0832519226634611, 0.08178443135586946, 0.08027292660486317, 0.07871956945503196, 0.0771265807886207, 0.07549623815026611, 0.07383087249072154, 0.07213286483422528, 0.07040464287427743, 0.06864867750269221, 0.0668674792768883, 0.06506359483046809, 0.06323960323221771, 0.061398112298733536, 0.05954175486594685, 0.05767318502487795, 0.055795074327000864, 0.053910107964644495, 0.05202098093189109, 0.05013039417146061, 0.04824105071309036, 0.046355651808930615, 0.04447689307148184, 0.04260746061959501, 0.04075002723804519, 0.03890724855616954, 0.03708175925103265, 0.03527616928054822, 0.03349306015194236, 0.03173498123089361, 0.030004446096627058, 0.028303928948172952, 0.02663586106692886, 0.025002627340582114, 0.02340656285336301, 0.021849949547503408, 0.02033501296067407, 0.018863919044065407, 0.017438771065660764, 0.016061606603129837, 0.014734394630641533, 0.013459032703761371, 0.012237344246458191, 0.01107107594409918, 0.009961895246160285, 0.00891138798222274, 0.007921056094663963, 0.006992315491284573, 0.00612649402094173, 0.005324829575082939, 0.004588468317894822, 0.00391846304759707, 0.003315771691224655, 0.0027812559350502133, 0.0023156799926048067, 0.0019197095120585088, 0.0015939106245227588, 0.0013387491346354909, 0.0011545898545859182, 0.0010416960825314061, 0.0010002292261520598, 0.0010302485718811862, 0.0011317112001417097, 0.0013044720467095697, 0.0015482841101164982, 0.0018627988047955782, 0.0022475664594646828]
classifier.3.weight
[0.1, 0.10096539422559865, 0.1008616248047795, 0.1006888353783892, 0.10044726512690408, 0.10013724843934917, 0.09975921445042644, 0.0993136864464934, 0.09880128114121377, 0.09822270782188341, 0.09757876736761284, 0.09687035114072565, 0.09609843975290698, 0.09526410170781065, 0.09436849192200317, 0.09341285012629252, 0.09239849914965431, 0.09132684308813104, 0.0901993653612387, 0.08901762665857167, 0.08778326277944781, 0.0864979823685843, 0.08516356455093882, 0.08378185646898949, 0.08235477072586322, 0.08088428273785116, 0.07937242799997622, 0.07782129926839786, 0.07623304366355417, 0.07460985969805126, 0.07295399423341402, 0.0712677393699108, 0.06955342927375716, 0.06781343694609075, 0.06605017093818964, 0.06426607201748101, 0.06246360978895518, 0.06064527927666209, 0.05881359747002157, 0.05697109983972864, 0.055120336828076426, 0.053263870318555034, 0.0514042700896129, 0.04954411025749005, 0.04768596571304651, 0.045832408557518654, 0.04398600454213709, 0.04214930951653418, 0.04032486589085797, 0.03851519911648915, 0.036722814190233155, 0.034950192186825914, 0.03319978682455337, 0.03147402106873866, 0.029775283777798367, 0.02810592639651078, 0.026468259701073228, 0.024864550600453946, 0.023297018998466604, 0.02176783472091061, 0.020279114512031057, 0.01883291910445575, 0.017431250366665287, 0.016076048531944682, 0.014769189512652353, 0.013512482303524153, 0.012307666477606775, 0.011156409778286903, 0.01006030581074912, 0.009020871836058268, 0.00803954667091969, 0.007117688696024515, 0.006256573975737162, 0.0054573944917273515, 0.004721256492992287, 0.004049178964552485, 0.0034420922169411, 0.0029008365984393375, 0.0024261613318402923, 0.002018723477351552, 0.0016790870230721168, 0.0014077221043025992, 0.001205004352769378, 0.0010712143766635142, 0.001006537372214227, 0.0010110628673345049, 0.0010847845976938266, 0.00122760051538942, 0.0014393129302041332, 0.0017196287832553715]
classifier.3.bias
[0.1, 0.10096648971012784, 0.10086600375809239, 0.10069867683642893, 0.10046473323208337, 0.10016448652577553, 0.09979833917167187, 0.09936678195793128, 0.0988703933488469, 0.09830983870946591, 0.09768586941372652, 0.09699932183730758, 0.09625111623654099, 0.09544225551488934, 0.09457382387864262, 0.09364698538363539, 0.09266298237493295, 0.09162313382157758, 0.09052883354862712, 0.08938154836885573, 0.08818281611662099, 0.08693424358653297, 0.08563750437968809, 0.08429433666035471, 0.08290654082611772, 0.08147597709460466, 0.08000456301002848, 0.07849427087288904, 0.0769471250962786, 0.07536519949233511, 0.0737506144924804, 0.07210553430516912, 0.07043216401495893, 0.06873274662678933, 0.067009560059432, 0.06526491409214195, 0.06350114726860275, 0.06172062376231529, 0.059925730207632436, 0.05811887250068642, 0.056302472574497955, 0.05447896515258907, 0.05265079448545144, 0.050820411074244745, 0.04899026838611645, 0.047162819565546, 0.04534051414612158, 0.04352579476715677, 0.04172109389954866, 0.03992883058526545, 0.03815140719483471, 0.036391206207178006, 0.03465058701610849, 0.032931882767772216, 0.03123739723327178, 0.029569401720664995, 0.027930132030477218, 0.026321785458808308, 0.024746517852051498, 0.02320644071717198, 0.021703618391418364, 0.02024006527526109, 0.01881774313226684, 0.01743855845952772, 0.016104359932170586, 0.014816935925371377, 0.01357801211719633, 0.01238924917548313, 0.011252240531862405, 0.010168510245903557, 0.009139510962247608, 0.008166621963465497, 0.007251147321251743, 0.006394314148431595, 0.005597270954124766, 0.004861086104270567, 0.0041867463895776825, 0.0035751557028185937, 0.003027133827241168, 0.002543415337721665, 0.0021246486161321182, 0.0017713949822416798, 0.001484127941317164, 0.0012632325494310473, 0.0011090048973278746, 0.0010216517135407768, 0.0010012900872901566, 0.001047947311535923, 0.0011615608463937212, 0.0013419784029640883]
classifier.6.weight
[0.1, 0.10096754495196603, 0.10087022194106982, 0.10070815731223111, 0.10048156145806247, 0.1001907285457371, 0.09983603613510061, 0.099417944688522, 0.09893699697312047, 0.09839381735614428, 0.09778911099441578, 0.09712366291889568, 0.09639833701555411, 0.09561407490387248, 0.094771894714431, 0.0938728897671697, 0.09291822715203846, 0.09190914621387847, 0.09084695694350248, 0.0897330382770621, 0.08856883630591039, 0.08735586239928311, 0.08609569124223644, 0.08478995879138766, 0.08344036015111327, 0.08204864737296112, 0.08061662718113378, 0.07914615862699576, 0.07763915067564925, 0.07609755972771226, 0.07452338707951535, 0.07291867632501493, 0.07128551070279572, 0.06962601039160642, 0.06794232975793944, 0.06623665455922843, 0.06451119910629356, 0.06276820338871901, 0.0610099301668943, 0.059238662034494266, 0.0574566984552116, 0.05566635277758876, 0.05386994923182429, 0.0520698199124529, 0.05026830175081565, 0.04846773348125131, 0.04667045260494704, 0.04487879235538961, 0.04309507866935748, 0.04132162716738498, 0.03956074014761939, 0.0378147035969731, 0.0360857842234509, 0.03437622651350538, 0.03268824981824027, 0.031024045472244556, 0.029385773948797756, 0.027775562055139327, 0.026195500171443387, 0.024647639537083238, 0.023133989587708383, 0.02165651534659137, 0.0202171348736308, 0.01881771677532218, 0.01746007777892959, 0.016145980374006625, 0.01487713052432919, 0.013655175453210072, 0.012481701505070414, 0.011358232086044369, 0.010286225686290267, 0.009267073986576002, 0.00830210005159631, 0.007392556612367617, 0.006539624439930302, 0.005744410812469424, 0.00500794807784402, 0.004331192313391201, 0.0037150220847446784, 0.003160237305279156, 0.002667558197661273, 0.0022376243588551606, 0.0018709939297964153, 0.001568142870812455, 0.0013294643437299517, 0.0011552682014713536, 0.001045780585803229, 0.0010011436337585991, 0.0010214152931143797, 0.0011065692471634405]
classifier.6.bias
[0.1, 0.10096856163159025, 0.10087428606120136, 0.10071729184363783, 0.10049777640458171, 0.10021601579232275, 0.09987236433061825, 0.09946725417311848, 0.09900119475991859, 0.09847477217691988, 0.0978886484188065, 0.0972435605565641, 0.09654031981058747, 0.09577981053054271, 0.09496298908326684, 0.09409088265010324, 0.09316458793518544, 0.09218526978629356, 0.09115415973001784, 0.09007255442307108, 0.0889418140216978, 0.08776336047123061, 0.08653867571794442, 0.08526929984545761, 0.08395682913802338, 0.08260291407314682, 0.08120925724605214, 0.07977761122861002, 0.07830977636541758, 0.0768075985098026, 0.07527296670259874, 0.07370781079661111, 0.07211409902975939, 0.07049383554995015, 0.06884905789479139, 0.0671818344293181, 0.06549426174495143, 0.06378846202296216, 0.062066580365754014, 0.06033078209932287, 0.05858325005028414, 0.05682618180089245, 0.05506178692550562, 0.05329228421196816, 0.05151989887140847, 0.04974685973995857, 0.047975396475915234, 0.04620773675586724, 0.04444610347331468, 0.04269271194330313, 0.04094976711658806, 0.0392194608068327, 0.037503968934326136, 0.035805448789688075, 0.03412603632100084, 0.032467843447780496, 0.03083295540516482, 0.029223428121657458, 0.027641285633726583, 0.02608851754050874, 0.02456707650181866, 0.023078875782611877, 0.021625786846987516, 0.020209637004757187, 0.018832207113539435, 0.017495229339269416, 0.01620038497794001, 0.01494930234131387, 0.01374355470926459, 0.012584658351322848, 0.011474070619914588, 0.010413188117689882, 0.009403344941246495, 0.008445811003457126, 0.00754179043650981, 0.0066924200776698915, 0.005898768039667684, 0.005161832367509555, 0.004482539783401762, 0.003861744521364962, 0.003300227253005377, 0.0027986941057930362, 0.0023577757750820154, 0.0019780267309890956, 0.0016599245211283355, 0.0014038691700783525, 0.0012101826763375391, 0.001079108607399736, 0.0010108117934596696, 0.0010053781201332648]
*** Model named parameters and requires_grad:
name:       features.module.0.weight  req_grad:  True 
name:         features.module.0.bias  req_grad:  True 
name:       features.module.1.weight  req_grad:  True 
name:         features.module.1.bias  req_grad:  True 
name:       features.module.4.weight  req_grad:  True 
name:         features.module.4.bias  req_grad:  True 
name:       features.module.5.weight  req_grad:  True 
name:         features.module.5.bias  req_grad:  True 
name:       features.module.8.weight  req_grad:  True 
name:         features.module.8.bias  req_grad:  True 
name:       features.module.9.weight  req_grad:  True 
name:         features.module.9.bias  req_grad:  True 
name:      features.module.11.weight  req_grad:  True 
name:        features.module.11.bias  req_grad:  True 
name:      features.module.12.weight  req_grad:  True 
name:        features.module.12.bias  req_grad:  True 
name:      features.module.15.weight  req_grad:  True 
name:        features.module.15.bias  req_grad:  True 
name:      features.module.16.weight  req_grad:  True 
name:        features.module.16.bias  req_grad:  True 
name:      features.module.18.weight  req_grad:  True 
name:        features.module.18.bias  req_grad:  True 
name:      features.module.19.weight  req_grad:  True 
name:        features.module.19.bias  req_grad:  True 
name:      features.module.22.weight  req_grad:  True 
name:        features.module.22.bias  req_grad:  True 
name:      features.module.23.weight  req_grad:  True 
name:        features.module.23.bias  req_grad:  True 
name:      features.module.25.weight  req_grad:  True 
name:        features.module.25.bias  req_grad:  True 
name:      features.module.26.weight  req_grad:  True 
name:        features.module.26.bias  req_grad:  True 
name:            classifier.0.weight  req_grad:  True 
name:              classifier.0.bias  req_grad:  True 
name:            classifier.3.weight  req_grad:  True 
name:              classifier.3.bias  req_grad:  True 
name:            classifier.6.weight  req_grad:  True 
name:              classifier.6.bias  req_grad:  True 


*** Optimizer groups, parameters and req_grads
#        requires_grad:                            True
#           param_name:        features.module.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:          features.module.0.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:        features.module.1.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:          features.module.1.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:        features.module.4.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:          features.module.4.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:        features.module.5.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:          features.module.5.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:        features.module.8.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:          features.module.8.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:        features.module.9.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:          features.module.9.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.11.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.11.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.12.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.12.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.15.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.15.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.16.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.16.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.18.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.18.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.19.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.19.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.22.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.22.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.23.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.23.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.25.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.25.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:       features.module.26.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:         features.module.26.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:             classifier.0.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:               classifier.0.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:             classifier.3.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:               classifier.3.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:             classifier.6.weight
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:               classifier.6.bias
#                   lr:                             0.1
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False



*** Optimizer group lrs
# group:  0,   name:       features.module.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  1,   name:         features.module.0.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  2,   name:       features.module.1.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  3,   name:         features.module.1.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  4,   name:       features.module.4.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  5,   name:         features.module.4.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  6,   name:       features.module.5.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  7,   name:         features.module.5.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  8,   name:       features.module.8.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  9,   name:         features.module.8.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 10,   name:       features.module.9.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 11,   name:         features.module.9.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 12,   name:      features.module.11.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 13,   name:        features.module.11.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 14,   name:      features.module.12.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 15,   name:        features.module.12.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 16,   name:      features.module.15.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 17,   name:        features.module.15.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 18,   name:      features.module.16.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 19,   name:        features.module.16.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 20,   name:      features.module.18.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 21,   name:        features.module.18.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 22,   name:      features.module.19.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 23,   name:        features.module.19.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 24,   name:      features.module.22.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 25,   name:        features.module.22.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 26,   name:      features.module.23.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 27,   name:        features.module.23.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 28,   name:      features.module.25.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 29,   name:        features.module.25.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 30,   name:      features.module.26.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 31,   name:        features.module.26.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 32,   name:            classifier.0.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 33,   name:              classifier.0.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 34,   name:            classifier.3.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 35,   name:              classifier.3.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 36,   name:            classifier.6.weight,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group: 37,   name:              classifier.6.bias,   req_grad:   True   lr: 0.100000000000000006,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
---------------
# Model: vgg11_bn
# Dataset: cifarcentum
# Freezeout: True
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	param_name features.module.0.weight
#	lr 0.1
#	momentum 0.9
#	weight_decay 0.0001
#	dampening 0
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
i:   0, name:       features.module.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   1, name:         features.module.0.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   2, name:       features.module.1.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   3, name:         features.module.1.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   4, name:       features.module.4.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   5, name:         features.module.4.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   6, name:       features.module.5.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   7, name:         features.module.5.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   8, name:       features.module.8.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:   9, name:         features.module.8.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  10, name:       features.module.9.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  11, name:         features.module.9.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  12, name:      features.module.11.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  13, name:        features.module.11.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  14, name:      features.module.12.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  15, name:        features.module.12.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  16, name:      features.module.15.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  17, name:        features.module.15.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  18, name:      features.module.16.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  19, name:        features.module.16.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  20, name:      features.module.18.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  21, name:        features.module.18.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  22, name:      features.module.19.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  23, name:        features.module.19.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  24, name:      features.module.22.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  25, name:        features.module.22.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  26, name:      features.module.23.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  27, name:        features.module.23.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  28, name:      features.module.25.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  29, name:        features.module.25.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  30, name:      features.module.26.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  31, name:        features.module.26.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  32, name:            classifier.0.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  33, name:              classifier.0.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  34, name:            classifier.3.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  35, name:              classifier.3.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  36, name:            classifier.6.weight  changing lr from: 0.100000000000000006   to: 0.100000000000000006
i:  37, name:              classifier.6.bias  changing lr from: 0.100000000000000006   to: 0.100000000000000006



# Switched to train mode...
Epoch: [0][  0/391]	Time  3.181 ( 3.181)	Data  0.103 ( 0.103)	Loss 4.6133e+00 (4.6133e+00)	Acc@1   0.00 (  0.00)	Acc@5   3.91 (  3.91)
Epoch: [0][ 10/391]	Time  0.020 ( 0.312)	Data  0.001 ( 0.011)	Loss 4.6211e+00 (4.7163e+00)	Acc@1   3.12 (  1.21)	Acc@5   6.25 (  5.68)
Epoch: [0][ 20/391]	Time  0.037 ( 0.174)	Data  0.001 ( 0.006)	Loss 4.5938e+00 (4.6579e+00)	Acc@1   0.78 (  1.64)	Acc@5   6.25 (  6.32)
Epoch: [0][ 30/391]	Time  0.038 ( 0.126)	Data  0.002 ( 0.005)	Loss 4.5195e+00 (4.6174e+00)	Acc@1   2.34 (  1.84)	Acc@5  10.94 (  7.11)
Epoch: [0][ 40/391]	Time  0.035 ( 0.102)	Data  0.001 ( 0.004)	Loss 4.4844e+00 (4.5959e+00)	Acc@1   0.78 (  1.70)	Acc@5   7.81 (  7.24)
Epoch: [0][ 50/391]	Time  0.026 ( 0.087)	Data  0.001 ( 0.004)	Loss 4.4648e+00 (4.5786e+00)	Acc@1   0.78 (  1.73)	Acc@5   8.59 (  7.15)
Epoch: [0][ 60/391]	Time  0.021 ( 0.076)	Data  0.000 ( 0.003)	Loss 4.4453e+00 (4.5595e+00)	Acc@1   1.56 (  1.64)	Acc@5   7.03 (  7.33)
Epoch: [0][ 70/391]	Time  0.020 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.4297e+00 (4.5391e+00)	Acc@1   4.69 (  1.79)	Acc@5  12.50 (  8.04)
Epoch: [0][ 80/391]	Time  0.020 ( 0.064)	Data  0.001 ( 0.003)	Loss 4.3672e+00 (4.5228e+00)	Acc@1   3.91 (  1.94)	Acc@5  17.19 (  8.68)
Epoch: [0][ 90/391]	Time  0.019 ( 0.059)	Data  0.001 ( 0.003)	Loss 4.4336e+00 (4.5109e+00)	Acc@1   2.34 (  2.07)	Acc@5  15.62 (  9.15)
Epoch: [0][100/391]	Time  0.019 ( 0.056)	Data  0.001 ( 0.003)	Loss 4.4023e+00 (4.4969e+00)	Acc@1   3.91 (  2.15)	Acc@5  15.62 (  9.61)
Epoch: [0][110/391]	Time  0.030 ( 0.053)	Data  0.001 ( 0.003)	Loss 4.4297e+00 (4.4798e+00)	Acc@1   0.78 (  2.25)	Acc@5   9.38 ( 10.08)
Epoch: [0][120/391]	Time  0.020 ( 0.051)	Data  0.001 ( 0.003)	Loss 4.2109e+00 (4.4698e+00)	Acc@1   3.12 (  2.27)	Acc@5  17.97 ( 10.25)
Epoch: [0][130/391]	Time  0.019 ( 0.049)	Data  0.001 ( 0.003)	Loss 4.2617e+00 (4.4585e+00)	Acc@1   1.56 (  2.30)	Acc@5  11.72 ( 10.47)
Epoch: [0][140/391]	Time  0.019 ( 0.047)	Data  0.001 ( 0.003)	Loss 4.2422e+00 (4.4459e+00)	Acc@1   3.12 (  2.35)	Acc@5  14.06 ( 10.69)
Epoch: [0][150/391]	Time  0.032 ( 0.046)	Data  0.004 ( 0.003)	Loss 4.2695e+00 (4.4331e+00)	Acc@1   5.47 (  2.44)	Acc@5  19.53 ( 11.04)
Epoch: [0][160/391]	Time  0.020 ( 0.044)	Data  0.001 ( 0.002)	Loss 4.4219e+00 (4.4246e+00)	Acc@1   2.34 (  2.48)	Acc@5  10.94 ( 11.22)
Epoch: [0][170/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.002)	Loss 4.2695e+00 (4.4176e+00)	Acc@1   0.78 (  2.50)	Acc@5  14.06 ( 11.36)
Epoch: [0][180/391]	Time  0.021 ( 0.042)	Data  0.001 ( 0.002)	Loss 4.2617e+00 (4.4075e+00)	Acc@1   3.12 (  2.59)	Acc@5  17.97 ( 11.71)
Epoch: [0][190/391]	Time  0.026 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.1875e+00 (4.4004e+00)	Acc@1   3.91 (  2.66)	Acc@5  17.19 ( 11.91)
Epoch: [0][200/391]	Time  0.020 ( 0.040)	Data  0.000 ( 0.002)	Loss 4.3984e+00 (4.3931e+00)	Acc@1   3.12 (  2.75)	Acc@5  10.16 ( 12.14)
Epoch: [0][210/391]	Time  0.022 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.0820e+00 (4.3852e+00)	Acc@1   2.34 (  2.78)	Acc@5  19.53 ( 12.41)
Epoch: [0][220/391]	Time  0.019 ( 0.039)	Data  0.001 ( 0.002)	Loss 4.1094e+00 (4.3770e+00)	Acc@1   5.47 (  2.87)	Acc@5  20.31 ( 12.64)
Epoch: [0][230/391]	Time  0.027 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.2305e+00 (4.3691e+00)	Acc@1   1.56 (  2.89)	Acc@5  13.28 ( 12.89)
Epoch: [0][240/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.2578e+00 (4.3609e+00)	Acc@1   2.34 (  2.97)	Acc@5  14.06 ( 13.16)
Epoch: [0][250/391]	Time  0.020 ( 0.037)	Data  0.001 ( 0.002)	Loss 4.2266e+00 (4.3562e+00)	Acc@1   4.69 (  2.97)	Acc@5  14.84 ( 13.27)
Epoch: [0][260/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.002)	Loss 4.1602e+00 (4.3489e+00)	Acc@1   7.03 (  3.06)	Acc@5  18.75 ( 13.48)
Epoch: [0][270/391]	Time  0.021 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2773e+00 (4.3415e+00)	Acc@1   3.12 (  3.12)	Acc@5  17.97 ( 13.73)
Epoch: [0][280/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3086e+00 (4.3356e+00)	Acc@1   3.91 (  3.19)	Acc@5  17.19 ( 13.93)
Epoch: [0][290/391]	Time  0.024 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1680e+00 (4.3303e+00)	Acc@1   7.03 (  3.25)	Acc@5  24.22 ( 14.11)
Epoch: [0][300/391]	Time  0.021 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2188e+00 (4.3250e+00)	Acc@1   1.56 (  3.26)	Acc@5  16.41 ( 14.25)
Epoch: [0][310/391]	Time  0.033 ( 0.035)	Data  0.000 ( 0.002)	Loss 4.2109e+00 (4.3192e+00)	Acc@1   6.25 (  3.30)	Acc@5  18.75 ( 14.46)
Epoch: [0][320/391]	Time  0.023 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0781e+00 (4.3123e+00)	Acc@1   4.69 (  3.35)	Acc@5  21.09 ( 14.66)
Epoch: [0][330/391]	Time  0.020 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0703e+00 (4.3052e+00)	Acc@1   5.47 (  3.42)	Acc@5  20.31 ( 14.87)
Epoch: [0][340/391]	Time  0.028 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2852e+00 (4.2995e+00)	Acc@1   4.69 (  3.49)	Acc@5  15.62 ( 15.02)
Epoch: [0][350/391]	Time  0.021 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2227e+00 (4.2955e+00)	Acc@1   2.34 (  3.50)	Acc@5  16.41 ( 15.12)
Epoch: [0][360/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1250e+00 (4.2910e+00)	Acc@1   3.91 (  3.55)	Acc@5  19.53 ( 15.25)
Epoch: [0][370/391]	Time  0.020 ( 0.033)	Data  0.000 ( 0.002)	Loss 3.9102e+00 (4.2850e+00)	Acc@1   5.47 (  3.60)	Acc@5  21.09 ( 15.45)
Epoch: [0][380/391]	Time  0.020 ( 0.033)	Data  0.001 ( 0.002)	Loss 4.0195e+00 (4.2797e+00)	Acc@1   3.91 (  3.66)	Acc@5  21.09 ( 15.61)
Epoch: [0][390/391]	Time  0.307 ( 0.034)	Data  0.001 ( 0.002)	Loss 3.9609e+00 (4.2745e+00)	Acc@1   7.50 (  3.70)	Acc@5  23.75 ( 15.79)
## e[0] optimizer.zero_grad (sum) time: 0.11667013168334961
## e[0]       loss.backward (sum) time: 2.5878002643585205
## e[0]      optimizer.step (sum) time: 0.9388928413391113
## epoch[0] training(only) time: 13.165128946304321
# Switched to evaluate mode...
Test: [  0/100]	Time  0.247 ( 0.247)	Loss 3.9570e+00 (3.9570e+00)	Acc@1   6.00 (  6.00)	Acc@5  27.00 ( 27.00)
Test: [ 10/100]	Time  0.019 ( 0.036)	Loss 4.0938e+00 (4.0337e+00)	Acc@1   4.00 (  6.00)	Acc@5  24.00 ( 25.55)
Test: [ 20/100]	Time  0.009 ( 0.024)	Loss 3.9277e+00 (4.0360e+00)	Acc@1   4.00 (  5.67)	Acc@5  27.00 ( 24.95)
Test: [ 30/100]	Time  0.015 ( 0.021)	Loss 4.1562e+00 (4.0194e+00)	Acc@1   3.00 (  5.81)	Acc@5  25.00 ( 25.48)
Test: [ 40/100]	Time  0.015 ( 0.019)	Loss 4.0977e+00 (4.0262e+00)	Acc@1   6.00 (  5.80)	Acc@5  22.00 ( 24.88)
Test: [ 50/100]	Time  0.033 ( 0.018)	Loss 4.0234e+00 (4.0262e+00)	Acc@1   8.00 (  5.86)	Acc@5  28.00 ( 24.92)
Test: [ 60/100]	Time  0.019 ( 0.018)	Loss 4.1797e+00 (4.0285e+00)	Acc@1   4.00 (  6.20)	Acc@5  19.00 ( 24.79)
Test: [ 70/100]	Time  0.010 ( 0.017)	Loss 4.0156e+00 (4.0256e+00)	Acc@1   6.00 (  6.37)	Acc@5  27.00 ( 24.90)
Test: [ 80/100]	Time  0.009 ( 0.017)	Loss 4.1484e+00 (4.0296e+00)	Acc@1   3.00 (  6.28)	Acc@5  18.00 ( 24.74)
Test: [ 90/100]	Time  0.009 ( 0.016)	Loss 3.7695e+00 (4.0231e+00)	Acc@1   9.00 (  6.33)	Acc@5  32.00 ( 24.92)
 * Acc@1 6.340 Acc@5 24.950
### epoch[0] execution time: 14.851097345352173
EPOCH 1
i:   0, name:       features.module.0.weight  changing lr from: 0.100000000000000006   to: 0.100883842679128255
i:   1, name:         features.module.0.bias  changing lr from: 0.100000000000000006   to: 0.100888322413453219
i:   2, name:       features.module.1.weight  changing lr from: 0.100000000000000006   to: 0.100892601929244652
i:   3, name:         features.module.1.bias  changing lr from: 0.100000000000000006   to: 0.100896691384664422
i:   4, name:       features.module.4.weight  changing lr from: 0.100000000000000006   to: 0.100900600362039988
i:   5, name:         features.module.4.bias  changing lr from: 0.100000000000000006   to: 0.100904337903880065
i:   6, name:       features.module.5.weight  changing lr from: 0.100000000000000006   to: 0.100907912546430154
i:   7, name:         features.module.5.bias  changing lr from: 0.100000000000000006   to: 0.100911332350950095
i:   8, name:       features.module.8.weight  changing lr from: 0.100000000000000006   to: 0.100914604932880930
i:   9, name:         features.module.8.bias  changing lr from: 0.100000000000000006   to: 0.100917737489055304
i:  10, name:       features.module.9.weight  changing lr from: 0.100000000000000006   to: 0.100920736823093798
i:  11, name:         features.module.9.bias  changing lr from: 0.100000000000000006   to: 0.100923609369117734
i:  12, name:      features.module.11.weight  changing lr from: 0.100000000000000006   to: 0.100926361213899959
i:  13, name:        features.module.11.bias  changing lr from: 0.100000000000000006   to: 0.100928998117564422
i:  14, name:      features.module.12.weight  changing lr from: 0.100000000000000006   to: 0.100931525532938104
i:  15, name:        features.module.12.bias  changing lr from: 0.100000000000000006   to: 0.100933948623649905
i:  16, name:      features.module.15.weight  changing lr from: 0.100000000000000006   to: 0.100936272281064832
i:  17, name:        features.module.15.bias  changing lr from: 0.100000000000000006   to: 0.100938501140134279
i:  18, name:      features.module.16.weight  changing lr from: 0.100000000000000006   to: 0.100940639594237963
i:  19, name:        features.module.16.bias  changing lr from: 0.100000000000000006   to: 0.100942691809086785
i:  20, name:      features.module.18.weight  changing lr from: 0.100000000000000006   to: 0.100944661735751104
i:  21, name:        features.module.18.bias  changing lr from: 0.100000000000000006   to: 0.100946553122873997
i:  22, name:      features.module.19.weight  changing lr from: 0.100000000000000006   to: 0.100948369528124970
i:  23, name:        features.module.19.bias  changing lr from: 0.100000000000000006   to: 0.100950114328944948
i:  24, name:      features.module.22.weight  changing lr from: 0.100000000000000006   to: 0.100951790732630553
i:  25, name:        features.module.22.bias  changing lr from: 0.100000000000000006   to: 0.100953401785801256
i:  26, name:      features.module.23.weight  changing lr from: 0.100000000000000006   to: 0.100954950383290776
i:  27, name:        features.module.23.bias  changing lr from: 0.100000000000000006   to: 0.100956439276500165
i:  28, name:      features.module.25.weight  changing lr from: 0.100000000000000006   to: 0.100957871081248679
i:  29, name:        features.module.25.bias  changing lr from: 0.100000000000000006   to: 0.100959248285154291
i:  30, name:      features.module.26.weight  changing lr from: 0.100000000000000006   to: 0.100960573254575081
i:  31, name:        features.module.26.bias  changing lr from: 0.100000000000000006   to: 0.100961848241139587
i:  32, name:            classifier.0.weight  changing lr from: 0.100000000000000006   to: 0.100963075387892426
i:  33, name:              classifier.0.bias  changing lr from: 0.100000000000000006   to: 0.100964256735079944
i:  34, name:            classifier.3.weight  changing lr from: 0.100000000000000006   to: 0.100965394225598651
i:  35, name:              classifier.3.bias  changing lr from: 0.100000000000000006   to: 0.100966489710127838
i:  36, name:            classifier.6.weight  changing lr from: 0.100000000000000006   to: 0.100967544951966026
i:  37, name:              classifier.6.bias  changing lr from: 0.100000000000000006   to: 0.100968561631590251



# Switched to train mode...
Epoch: [1][  0/391]	Time  0.182 ( 0.182)	Data  0.146 ( 0.146)	Loss 4.1055e+00 (4.1055e+00)	Acc@1   5.47 (  5.47)	Acc@5  24.22 ( 24.22)
Epoch: [1][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 4.1562e+00 (4.1275e+00)	Acc@1   0.78 (  4.26)	Acc@5  19.53 ( 20.53)
Epoch: [1][ 20/391]	Time  0.019 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.9551e+00 (4.0871e+00)	Acc@1   7.03 (  5.10)	Acc@5  21.88 ( 21.28)
Epoch: [1][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.007)	Loss 4.0547e+00 (4.0837e+00)	Acc@1   8.59 (  5.34)	Acc@5  23.44 ( 21.14)
Epoch: [1][ 40/391]	Time  0.036 ( 0.028)	Data  0.002 ( 0.006)	Loss 4.2344e+00 (4.0710e+00)	Acc@1   7.03 (  5.60)	Acc@5  17.19 ( 21.49)
Epoch: [1][ 50/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.0547e+00 (4.0738e+00)	Acc@1   5.47 (  5.47)	Acc@5  22.66 ( 21.46)
Epoch: [1][ 60/391]	Time  0.035 ( 0.027)	Data  0.003 ( 0.004)	Loss 3.9375e+00 (4.0689e+00)	Acc@1   7.81 (  5.74)	Acc@5  27.34 ( 21.64)
Epoch: [1][ 70/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.0859e+00 (4.0677e+00)	Acc@1   3.91 (  5.68)	Acc@5  21.09 ( 21.69)
Epoch: [1][ 80/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 4.0273e+00 (4.0633e+00)	Acc@1   3.91 (  5.78)	Acc@5  22.66 ( 21.77)
Epoch: [1][ 90/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.004)	Loss 4.0977e+00 (4.0621e+00)	Acc@1   5.47 (  5.84)	Acc@5  20.31 ( 22.06)
Epoch: [1][100/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1797e+00 (4.0624e+00)	Acc@1   7.03 (  5.78)	Acc@5  19.53 ( 22.00)
Epoch: [1][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9824e+00 (4.0608e+00)	Acc@1   6.25 (  5.88)	Acc@5  25.00 ( 22.21)
Epoch: [1][120/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9375e+00 (4.0537e+00)	Acc@1   8.59 (  5.92)	Acc@5  25.78 ( 22.44)
Epoch: [1][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8867e+00 (4.0491e+00)	Acc@1  11.72 (  6.00)	Acc@5  29.69 ( 22.61)
Epoch: [1][140/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9844e+00 (4.0448e+00)	Acc@1   6.25 (  6.05)	Acc@5  17.97 ( 22.72)
Epoch: [1][150/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9766e+00 (4.0393e+00)	Acc@1   7.03 (  6.07)	Acc@5  26.56 ( 23.02)
Epoch: [1][160/391]	Time  0.041 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8574e+00 (4.0329e+00)	Acc@1   7.03 (  6.13)	Acc@5  29.69 ( 23.24)
Epoch: [1][170/391]	Time  0.028 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.0078e+00 (4.0307e+00)	Acc@1   3.91 (  6.10)	Acc@5  22.66 ( 23.24)
Epoch: [1][180/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.9609e+00 (4.0295e+00)	Acc@1   7.03 (  6.09)	Acc@5  25.78 ( 23.28)
Epoch: [1][190/391]	Time  0.034 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.9727e+00 (4.0244e+00)	Acc@1   7.03 (  6.17)	Acc@5  28.12 ( 23.46)
Epoch: [1][200/391]	Time  0.036 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.1367e+00 (4.0216e+00)	Acc@1   2.34 (  6.22)	Acc@5  15.62 ( 23.55)
Epoch: [1][210/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0469e+00 (4.0188e+00)	Acc@1   9.38 (  6.25)	Acc@5  32.03 ( 23.66)
Epoch: [1][220/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.0586e+00 (4.0149e+00)	Acc@1   4.69 (  6.33)	Acc@5  23.44 ( 23.83)
Epoch: [1][230/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7637e+00 (4.0119e+00)	Acc@1  10.16 (  6.39)	Acc@5  28.91 ( 23.93)
Epoch: [1][240/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9277e+00 (4.0094e+00)	Acc@1   8.59 (  6.43)	Acc@5  29.69 ( 24.02)
Epoch: [1][250/391]	Time  0.041 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.7383e+00 (4.0047e+00)	Acc@1   7.81 (  6.45)	Acc@5  39.06 ( 24.23)
Epoch: [1][260/391]	Time  0.040 ( 0.025)	Data  0.003 ( 0.003)	Loss 4.0117e+00 (4.0016e+00)	Acc@1   3.12 (  6.45)	Acc@5  25.78 ( 24.35)
Epoch: [1][270/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.8828e+00 (3.9975e+00)	Acc@1   6.25 (  6.54)	Acc@5  31.25 ( 24.57)
Epoch: [1][280/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.9648e+00 (3.9944e+00)	Acc@1   7.03 (  6.59)	Acc@5  21.09 ( 24.75)
Epoch: [1][290/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8203e+00 (3.9900e+00)	Acc@1   7.81 (  6.62)	Acc@5  30.47 ( 24.87)
Epoch: [1][300/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.9062e+00 (3.9860e+00)	Acc@1   8.59 (  6.67)	Acc@5  23.44 ( 24.98)
Epoch: [1][310/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8906e+00 (3.9853e+00)	Acc@1   5.47 (  6.67)	Acc@5  21.88 ( 24.95)
Epoch: [1][320/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.9102e+00 (3.9815e+00)	Acc@1   8.59 (  6.73)	Acc@5  25.00 ( 25.11)
Epoch: [1][330/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.7891e+00 (3.9785e+00)	Acc@1  11.72 (  6.79)	Acc@5  23.44 ( 25.17)
Epoch: [1][340/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.9551e+00 (3.9761e+00)	Acc@1   7.03 (  6.77)	Acc@5  22.66 ( 25.21)
Epoch: [1][350/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.8281e+00 (3.9728e+00)	Acc@1   4.69 (  6.83)	Acc@5  31.25 ( 25.32)
Epoch: [1][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8145e+00 (3.9707e+00)	Acc@1   8.59 (  6.86)	Acc@5  30.47 ( 25.40)
Epoch: [1][370/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7207e+00 (3.9669e+00)	Acc@1  14.06 (  6.94)	Acc@5  35.94 ( 25.56)
Epoch: [1][380/391]	Time  0.030 ( 0.025)	Data  0.003 ( 0.002)	Loss 3.9590e+00 (3.9634e+00)	Acc@1   7.81 (  6.94)	Acc@5  28.12 ( 25.66)
Epoch: [1][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7871e+00 (3.9591e+00)	Acc@1   8.75 (  7.01)	Acc@5  31.25 ( 25.83)
## e[1] optimizer.zero_grad (sum) time: 0.11353635787963867
## e[1]       loss.backward (sum) time: 2.208956003189087
## e[1]      optimizer.step (sum) time: 0.9629089832305908
## epoch[1] training(only) time: 9.95888376235962
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 3.9023e+00 (3.9023e+00)	Acc@1  12.00 ( 12.00)	Acc@5  28.00 ( 28.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 4.0938e+00 (3.8908e+00)	Acc@1   8.00 (  8.91)	Acc@5  33.00 ( 30.00)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 3.8691e+00 (3.8733e+00)	Acc@1   8.00 (  9.29)	Acc@5  30.00 ( 30.90)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 3.9004e+00 (3.8718e+00)	Acc@1   7.00 (  8.84)	Acc@5  29.00 ( 30.97)
Test: [ 40/100]	Time  0.009 ( 0.016)	Loss 3.8945e+00 (3.8671e+00)	Acc@1  10.00 (  8.85)	Acc@5  32.00 ( 30.98)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 3.9238e+00 (3.8629e+00)	Acc@1  11.00 (  9.10)	Acc@5  26.00 ( 31.08)
Test: [ 60/100]	Time  0.016 ( 0.015)	Loss 3.8105e+00 (3.8660e+00)	Acc@1   5.00 (  9.10)	Acc@5  31.00 ( 31.08)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 3.9805e+00 (3.8690e+00)	Acc@1   6.00 (  9.01)	Acc@5  30.00 ( 30.82)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 3.9629e+00 (3.8672e+00)	Acc@1   6.00 (  8.94)	Acc@5  23.00 ( 30.81)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 3.7246e+00 (3.8580e+00)	Acc@1   8.00 (  8.99)	Acc@5  33.00 ( 30.98)
 * Acc@1 8.930 Acc@5 30.980
### epoch[1] execution time: 11.532325983047485
EPOCH 2
i:   0, name:       features.module.0.weight  changing lr from: 0.100883842679128255   to: 0.100535910417440644
i:   1, name:         features.module.0.bias  changing lr from: 0.100888322413453219   to: 0.100553788529146343
i:   2, name:       features.module.1.weight  changing lr from: 0.100892601929244652   to: 0.100570869090802650
i:   3, name:         features.module.1.bias  changing lr from: 0.100896691384664422   to: 0.100587192445457760
i:   4, name:       features.module.4.weight  changing lr from: 0.100900600362039988   to: 0.100602796659681026
i:   5, name:         features.module.4.bias  changing lr from: 0.100904337903880065   to: 0.100617717664985604
i:   6, name:       features.module.5.weight  changing lr from: 0.100907912546430154   to: 0.100631989389684789
i:   7, name:         features.module.5.bias  changing lr from: 0.100911332350950095   to: 0.100645643881879895
i:   8, name:       features.module.8.weight  changing lr from: 0.100914604932880930   to: 0.100658711424223224
i:   9, name:         features.module.8.bias  changing lr from: 0.100917737489055304   to: 0.100671220641049525
i:  10, name:       features.module.9.weight  changing lr from: 0.100920736823093798   to: 0.100683198598423673
i:  11, name:         features.module.9.bias  changing lr from: 0.100923609369117734   to: 0.100694670897610375
i:  12, name:      features.module.11.weight  changing lr from: 0.100926361213899959   to: 0.100705661762432536
i:  13, name:        features.module.11.bias  changing lr from: 0.100928998117564422   to: 0.100716194120950078
i:  14, name:      features.module.12.weight  changing lr from: 0.100931525532938104   to: 0.100726289681857983
i:  15, name:        features.module.12.bias  changing lr from: 0.100933948623649905   to: 0.100735969005972342
i:  16, name:      features.module.15.weight  changing lr from: 0.100936272281064832   to: 0.100745251573145733
i:  17, name:        features.module.15.bias  changing lr from: 0.100938501140134279   to: 0.100754155844927668
i:  18, name:      features.module.16.weight  changing lr from: 0.100940639594237963   to: 0.100762699323262719
i:  19, name:        features.module.16.bias  changing lr from: 0.100942691809086785   to: 0.100770898605496970
i:  20, name:      features.module.18.weight  changing lr from: 0.100944661735751104   to: 0.100778769435943980
i:  21, name:        features.module.18.bias  changing lr from: 0.100946553122873997   to: 0.100786326754242955
i:  22, name:      features.module.19.weight  changing lr from: 0.100948369528124970   to: 0.100793584740724862
i:  23, name:        features.module.19.bias  changing lr from: 0.100950114328944948   to: 0.100800556858986826
i:  24, name:      features.module.22.weight  changing lr from: 0.100951790732630553   to: 0.100807255895860592
i:  25, name:        features.module.22.bias  changing lr from: 0.100953401785801256   to: 0.100813693998947712
i:  26, name:      features.module.23.weight  changing lr from: 0.100954950383290776   to: 0.100819882711881664
i:  27, name:        features.module.23.bias  changing lr from: 0.100956439276500165   to: 0.100825833007465934
i:  28, name:      features.module.25.weight  changing lr from: 0.100957871081248679   to: 0.100831555318826510
i:  29, name:        features.module.25.bias  changing lr from: 0.100959248285154291   to: 0.100837059568707643
i:  30, name:      features.module.26.weight  changing lr from: 0.100960573254575081   to: 0.100842355197030498
i:  31, name:        features.module.26.bias  changing lr from: 0.100961848241139587   to: 0.100847451186826476
i:  32, name:            classifier.0.weight  changing lr from: 0.100963075387892426   to: 0.100852356088648842
i:  33, name:              classifier.0.bias  changing lr from: 0.100964256735079944   to: 0.100857078043559256
i:  34, name:            classifier.3.weight  changing lr from: 0.100965394225598651   to: 0.100861624804779504
i:  35, name:              classifier.3.bias  changing lr from: 0.100966489710127838   to: 0.100866003758092390
i:  36, name:            classifier.6.weight  changing lr from: 0.100967544951966026   to: 0.100870221941069815
i:  37, name:              classifier.6.bias  changing lr from: 0.100968561631590251   to: 0.100874286061201357



# Switched to train mode...
Epoch: [2][  0/391]	Time  0.169 ( 0.169)	Data  0.142 ( 0.142)	Loss 3.6895e+00 (3.6895e+00)	Acc@1  10.94 ( 10.94)	Acc@5  32.81 ( 32.81)
Epoch: [2][ 10/391]	Time  0.023 ( 0.038)	Data  0.001 ( 0.014)	Loss 3.8809e+00 (3.8622e+00)	Acc@1   8.59 (  7.95)	Acc@5  28.91 ( 28.27)
Epoch: [2][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.8164e+00 (3.8397e+00)	Acc@1   8.59 (  8.37)	Acc@5  33.59 ( 29.87)
Epoch: [2][ 30/391]	Time  0.028 ( 0.029)	Data  0.000 ( 0.006)	Loss 3.8594e+00 (3.8415e+00)	Acc@1   4.69 (  8.09)	Acc@5  29.69 ( 30.22)
Epoch: [2][ 40/391]	Time  0.019 ( 0.028)	Data  0.002 ( 0.005)	Loss 3.8828e+00 (3.8535e+00)	Acc@1   7.81 (  7.87)	Acc@5  28.12 ( 29.86)
Epoch: [2][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.8418e+00 (3.8392e+00)	Acc@1  12.50 (  8.06)	Acc@5  28.12 ( 30.21)
Epoch: [2][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.6855e+00 (3.8251e+00)	Acc@1   8.59 (  8.21)	Acc@5  32.03 ( 30.51)
Epoch: [2][ 70/391]	Time  0.033 ( 0.027)	Data  0.003 ( 0.004)	Loss 3.8770e+00 (3.8242e+00)	Acc@1   3.91 (  8.29)	Acc@5  25.00 ( 30.63)
Epoch: [2][ 80/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.6074e+00 (3.8207e+00)	Acc@1  10.16 (  8.50)	Acc@5  38.28 ( 30.59)
Epoch: [2][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7012e+00 (3.8086e+00)	Acc@1   8.59 (  8.74)	Acc@5  33.59 ( 31.07)
Epoch: [2][100/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.9297e+00 (3.8083e+00)	Acc@1   5.47 (  8.82)	Acc@5  28.12 ( 31.05)
Epoch: [2][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6445e+00 (3.8045e+00)	Acc@1  11.72 (  8.88)	Acc@5  37.50 ( 31.15)
Epoch: [2][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4902e+00 (3.7957e+00)	Acc@1  17.19 (  9.05)	Acc@5  39.06 ( 31.42)
Epoch: [2][130/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7910e+00 (3.7918e+00)	Acc@1   7.81 (  9.11)	Acc@5  29.69 ( 31.53)
Epoch: [2][140/391]	Time  0.038 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.8477e+00 (3.7873e+00)	Acc@1  10.16 (  9.19)	Acc@5  35.94 ( 31.75)
Epoch: [2][150/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7773e+00 (3.7781e+00)	Acc@1   6.25 (  9.30)	Acc@5  30.47 ( 32.01)
Epoch: [2][160/391]	Time  0.042 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6582e+00 (3.7704e+00)	Acc@1  11.72 (  9.41)	Acc@5  40.62 ( 32.31)
Epoch: [2][170/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6445e+00 (3.7621e+00)	Acc@1  11.72 (  9.55)	Acc@5  33.59 ( 32.52)
Epoch: [2][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6660e+00 (3.7593e+00)	Acc@1   7.03 (  9.66)	Acc@5  36.72 ( 32.69)
Epoch: [2][190/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7773e+00 (3.7574e+00)	Acc@1  13.28 (  9.73)	Acc@5  33.59 ( 32.69)
Epoch: [2][200/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.7188e+00 (3.7582e+00)	Acc@1  12.50 (  9.67)	Acc@5  34.38 ( 32.64)
Epoch: [2][210/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7910e+00 (3.7556e+00)	Acc@1   9.38 (  9.71)	Acc@5  32.81 ( 32.76)
Epoch: [2][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4883e+00 (3.7503e+00)	Acc@1  14.84 (  9.78)	Acc@5  39.06 ( 32.92)
Epoch: [2][230/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.6816e+00 (3.7488e+00)	Acc@1  10.16 (  9.81)	Acc@5  33.59 ( 32.89)
Epoch: [2][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6719e+00 (3.7451e+00)	Acc@1   8.59 (  9.86)	Acc@5  40.62 ( 33.04)
Epoch: [2][250/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4316e+00 (3.7410e+00)	Acc@1  11.72 (  9.92)	Acc@5  41.41 ( 33.14)
Epoch: [2][260/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6602e+00 (3.7387e+00)	Acc@1   7.81 (  9.97)	Acc@5  29.69 ( 33.22)
Epoch: [2][270/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7539e+00 (3.7352e+00)	Acc@1   4.69 ( 10.00)	Acc@5  33.59 ( 33.41)
Epoch: [2][280/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.7793e+00 (3.7298e+00)	Acc@1   8.59 ( 10.07)	Acc@5  39.06 ( 33.64)
Epoch: [2][290/391]	Time  0.029 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.5312e+00 (3.7247e+00)	Acc@1  11.72 ( 10.16)	Acc@5  40.62 ( 33.77)
Epoch: [2][300/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4414e+00 (3.7184e+00)	Acc@1  16.41 ( 10.28)	Acc@5  41.41 ( 33.91)
Epoch: [2][310/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.5039e+00 (3.7134e+00)	Acc@1  11.72 ( 10.35)	Acc@5  42.19 ( 34.01)
Epoch: [2][320/391]	Time  0.036 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.6973e+00 (3.7102e+00)	Acc@1  10.94 ( 10.37)	Acc@5  32.81 ( 34.10)
Epoch: [2][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6113e+00 (3.7057e+00)	Acc@1  16.41 ( 10.45)	Acc@5  38.28 ( 34.20)
Epoch: [2][340/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6973e+00 (3.7032e+00)	Acc@1  12.50 ( 10.49)	Acc@5  36.72 ( 34.31)
Epoch: [2][350/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.3809e+00 (3.6986e+00)	Acc@1  15.62 ( 10.55)	Acc@5  46.88 ( 34.47)
Epoch: [2][360/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.5117e+00 (3.6948e+00)	Acc@1  12.50 ( 10.63)	Acc@5  43.75 ( 34.60)
Epoch: [2][370/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.5273e+00 (3.6909e+00)	Acc@1  12.50 ( 10.67)	Acc@5  41.41 ( 34.75)
Epoch: [2][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8359e+00 (3.6865e+00)	Acc@1   9.38 ( 10.73)	Acc@5  29.69 ( 34.91)
Epoch: [2][390/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8828e+00 (3.6830e+00)	Acc@1   5.00 ( 10.75)	Acc@5  25.00 ( 35.03)
## e[2] optimizer.zero_grad (sum) time: 0.11501884460449219
## e[2]       loss.backward (sum) time: 2.1531243324279785
## e[2]      optimizer.step (sum) time: 0.951073408126831
## epoch[2] training(only) time: 9.964533805847168
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 3.4844e+00 (3.4844e+00)	Acc@1  12.00 ( 12.00)	Acc@5  43.00 ( 43.00)
Test: [ 10/100]	Time  0.017 ( 0.024)	Loss 3.5273e+00 (3.4906e+00)	Acc@1  16.00 ( 13.45)	Acc@5  38.00 ( 41.36)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 3.3594e+00 (3.4785e+00)	Acc@1  17.00 ( 14.29)	Acc@5  45.00 ( 41.48)
Test: [ 30/100]	Time  0.016 ( 0.017)	Loss 3.6484e+00 (3.4640e+00)	Acc@1  18.00 ( 14.77)	Acc@5  41.00 ( 42.45)
Test: [ 40/100]	Time  0.016 ( 0.016)	Loss 3.5078e+00 (3.4694e+00)	Acc@1  15.00 ( 14.78)	Acc@5  42.00 ( 42.10)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 3.3477e+00 (3.4578e+00)	Acc@1  19.00 ( 14.80)	Acc@5  48.00 ( 42.29)
Test: [ 60/100]	Time  0.019 ( 0.016)	Loss 3.3887e+00 (3.4567e+00)	Acc@1  18.00 ( 14.90)	Acc@5  42.00 ( 42.00)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 3.6211e+00 (3.4596e+00)	Acc@1  11.00 ( 15.03)	Acc@5  34.00 ( 41.94)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 3.5098e+00 (3.4622e+00)	Acc@1   7.00 ( 14.79)	Acc@5  40.00 ( 41.90)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 3.3320e+00 (3.4552e+00)	Acc@1  13.00 ( 14.91)	Acc@5  40.00 ( 42.11)
 * Acc@1 14.810 Acc@5 42.260
### epoch[2] execution time: 11.49596905708313
EPOCH 3
i:   0, name:       features.module.0.weight  changing lr from: 0.100535910417440644   to: 0.099957819810111703
i:   1, name:         features.module.0.bias  changing lr from: 0.100553788529146343   to: 0.099997892744552064
i:   2, name:       features.module.1.weight  changing lr from: 0.100570869090802650   to: 0.100036183624119909
i:   3, name:         features.module.1.bias  changing lr from: 0.100587192445457760   to: 0.100072782138654337
i:   4, name:       features.module.4.weight  changing lr from: 0.100602796659681026   to: 0.100107772956131005
i:   5, name:         features.module.4.bias  changing lr from: 0.100617717664985604   to: 0.100141236031030340
i:   6, name:       features.module.5.weight  changing lr from: 0.100631989389684789   to: 0.100173246892199336
i:   7, name:         features.module.5.bias  changing lr from: 0.100645643881879895   to: 0.100203876911666684
i:   8, name:       features.module.8.weight  changing lr from: 0.100658711424223224   to: 0.100233193555760797
i:   9, name:         features.module.8.bias  changing lr from: 0.100671220641049525   to: 0.100261260619778914
i:  10, name:       features.module.9.weight  changing lr from: 0.100683198598423673   to: 0.100288138447362618
i:  11, name:         features.module.9.bias  changing lr from: 0.100694670897610375   to: 0.100313884135648818
i:  12, name:      features.module.11.weight  changing lr from: 0.100705661762432536   to: 0.100338551727185712
i:  13, name:        features.module.11.bias  changing lr from: 0.100716194120950078   to: 0.100362192389530924
i:  14, name:      features.module.12.weight  changing lr from: 0.100726289681857983   to: 0.100384854583380631
i:  15, name:        features.module.12.bias  changing lr from: 0.100735969005972342   to: 0.100406584220016762
i:  16, name:      features.module.15.weight  changing lr from: 0.100745251573145733   to: 0.100427424808802096
i:  17, name:        features.module.15.bias  changing lr from: 0.100754155844927668   to: 0.100447417595399255
i:  18, name:      features.module.16.weight  changing lr from: 0.100762699323262719   to: 0.100466601691341736
i:  19, name:        features.module.16.bias  changing lr from: 0.100770898605496970   to: 0.100485014195538902
i:  20, name:      features.module.18.weight  changing lr from: 0.100778769435943980   to: 0.100502690308255593
i:  21, name:        features.module.18.bias  changing lr from: 0.100786326754242955   to: 0.100519663438068355
i:  22, name:      features.module.19.weight  changing lr from: 0.100793584740724862   to: 0.100535965302264232
i:  23, name:        features.module.19.bias  changing lr from: 0.100800556858986826   to: 0.100551626021115656
i:  24, name:      features.module.22.weight  changing lr from: 0.100807255895860592   to: 0.100566674206433726
i:  25, name:        features.module.22.bias  changing lr from: 0.100813693998947712   to: 0.100581137044774421
i:  26, name:      features.module.23.weight  changing lr from: 0.100819882711881664   to: 0.100595040375645775
i:  27, name:        features.module.23.bias  changing lr from: 0.100825833007465934   to: 0.100608408765040208
i:  28, name:      features.module.25.weight  changing lr from: 0.100831555318826510   to: 0.100621265574593180
i:  29, name:        features.module.25.bias  changing lr from: 0.100837059568707643   to: 0.100633633026649252
i:  30, name:      features.module.26.weight  changing lr from: 0.100842355197030498   to: 0.100645532265496607
i:  31, name:        features.module.26.bias  changing lr from: 0.100847451186826476   to: 0.100656983415013918
i:  32, name:            classifier.0.weight  changing lr from: 0.100852356088648842   to: 0.100668005632956373
i:  33, name:              classifier.0.bias  changing lr from: 0.100857078043559256   to: 0.100678617162092504
i:  34, name:            classifier.3.weight  changing lr from: 0.100861624804779504   to: 0.100688835378389199
i:  35, name:              classifier.3.bias  changing lr from: 0.100866003758092390   to: 0.100698676836428927
i:  36, name:            classifier.6.weight  changing lr from: 0.100870221941069815   to: 0.100708157312231109
i:  37, name:              classifier.6.bias  changing lr from: 0.100874286061201357   to: 0.100717291843637827



# Switched to train mode...
Epoch: [3][  0/391]	Time  0.167 ( 0.167)	Data  0.141 ( 0.141)	Loss 3.5254e+00 (3.5254e+00)	Acc@1  12.50 ( 12.50)	Acc@5  41.41 ( 41.41)
Epoch: [3][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.014)	Loss 3.5254e+00 (3.6294e+00)	Acc@1  12.50 ( 11.72)	Acc@5  41.41 ( 36.29)
Epoch: [3][ 20/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.4199e+00 (3.5411e+00)	Acc@1  17.19 ( 12.65)	Acc@5  46.88 ( 39.55)
Epoch: [3][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.5059e+00 (3.5377e+00)	Acc@1  16.41 ( 12.90)	Acc@5  42.97 ( 39.69)
Epoch: [3][ 40/391]	Time  0.021 ( 0.028)	Data  0.000 ( 0.005)	Loss 3.4141e+00 (3.5305e+00)	Acc@1  14.84 ( 13.30)	Acc@5  43.75 ( 39.73)
Epoch: [3][ 50/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 3.4121e+00 (3.5180e+00)	Acc@1  17.19 ( 13.68)	Acc@5  43.75 ( 40.01)
Epoch: [3][ 60/391]	Time  0.026 ( 0.027)	Data  0.004 ( 0.004)	Loss 3.5156e+00 (3.5056e+00)	Acc@1  12.50 ( 13.95)	Acc@5  36.72 ( 40.46)
Epoch: [3][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.5195e+00 (3.4968e+00)	Acc@1  17.19 ( 13.96)	Acc@5  41.41 ( 40.76)
Epoch: [3][ 80/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.4688e+00 (3.4955e+00)	Acc@1  12.50 ( 14.00)	Acc@5  41.41 ( 41.01)
Epoch: [3][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4824e+00 (3.4903e+00)	Acc@1  12.50 ( 13.99)	Acc@5  44.53 ( 41.21)
Epoch: [3][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5488e+00 (3.4919e+00)	Acc@1  11.72 ( 13.88)	Acc@5  42.19 ( 41.14)
Epoch: [3][110/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5098e+00 (3.4954e+00)	Acc@1  10.94 ( 13.89)	Acc@5  44.53 ( 41.15)
Epoch: [3][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5391e+00 (3.4919e+00)	Acc@1  14.06 ( 14.01)	Acc@5  43.75 ( 41.28)
Epoch: [3][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5000e+00 (3.4877e+00)	Acc@1  13.28 ( 14.09)	Acc@5  39.06 ( 41.38)
Epoch: [3][140/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.6758e+00 (3.4762e+00)	Acc@1   5.47 ( 14.22)	Acc@5  38.28 ( 41.84)
Epoch: [3][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3418e+00 (3.4729e+00)	Acc@1  13.28 ( 14.25)	Acc@5  47.66 ( 41.90)
Epoch: [3][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1816e+00 (3.4658e+00)	Acc@1  19.53 ( 14.41)	Acc@5  50.00 ( 42.07)
Epoch: [3][170/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3320e+00 (3.4643e+00)	Acc@1  19.53 ( 14.32)	Acc@5  46.88 ( 41.99)
Epoch: [3][180/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3438e+00 (3.4599e+00)	Acc@1  19.53 ( 14.36)	Acc@5  49.22 ( 42.11)
Epoch: [3][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2539e+00 (3.4540e+00)	Acc@1  21.88 ( 14.48)	Acc@5  46.88 ( 42.28)
Epoch: [3][200/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4395e+00 (3.4490e+00)	Acc@1  14.84 ( 14.60)	Acc@5  43.75 ( 42.37)
Epoch: [3][210/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.5527e+00 (3.4471e+00)	Acc@1   9.38 ( 14.64)	Acc@5  42.97 ( 42.47)
Epoch: [3][220/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.4453e+00 (3.4437e+00)	Acc@1  15.62 ( 14.60)	Acc@5  39.06 ( 42.59)
Epoch: [3][230/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.4512e+00 (3.4418e+00)	Acc@1  13.28 ( 14.68)	Acc@5  40.62 ( 42.67)
Epoch: [3][240/391]	Time  0.035 ( 0.025)	Data  0.003 ( 0.002)	Loss 3.3398e+00 (3.4365e+00)	Acc@1  14.84 ( 14.74)	Acc@5  47.66 ( 42.84)
Epoch: [3][250/391]	Time  0.034 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.1758e+00 (3.4304e+00)	Acc@1  19.53 ( 14.84)	Acc@5  50.78 ( 42.99)
Epoch: [3][260/391]	Time  0.041 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.3027e+00 (3.4263e+00)	Acc@1  17.19 ( 14.92)	Acc@5  43.75 ( 43.09)
Epoch: [3][270/391]	Time  0.041 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3281e+00 (3.4214e+00)	Acc@1  13.28 ( 15.03)	Acc@5  46.88 ( 43.26)
Epoch: [3][280/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.5098e+00 (3.4175e+00)	Acc@1  16.41 ( 15.05)	Acc@5  36.72 ( 43.35)
Epoch: [3][290/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3125e+00 (3.4126e+00)	Acc@1  14.06 ( 15.14)	Acc@5  46.88 ( 43.50)
Epoch: [3][300/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6309e+00 (3.4093e+00)	Acc@1  14.06 ( 15.26)	Acc@5  36.72 ( 43.67)
Epoch: [3][310/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.2031e+00 (3.4039e+00)	Acc@1  15.62 ( 15.35)	Acc@5  51.56 ( 43.80)
Epoch: [3][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3887e+00 (3.4014e+00)	Acc@1  16.41 ( 15.38)	Acc@5  47.66 ( 43.89)
Epoch: [3][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1582e+00 (3.3971e+00)	Acc@1  18.75 ( 15.47)	Acc@5  46.09 ( 44.05)
Epoch: [3][340/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3945e+00 (3.3928e+00)	Acc@1  16.41 ( 15.54)	Acc@5  42.19 ( 44.17)
Epoch: [3][350/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1738e+00 (3.3889e+00)	Acc@1  22.66 ( 15.63)	Acc@5  54.69 ( 44.32)
Epoch: [3][360/391]	Time  0.025 ( 0.025)	Data  0.004 ( 0.002)	Loss 3.2441e+00 (3.3871e+00)	Acc@1  17.97 ( 15.64)	Acc@5  46.09 ( 44.34)
Epoch: [3][370/391]	Time  0.032 ( 0.025)	Data  0.004 ( 0.002)	Loss 3.0625e+00 (3.3822e+00)	Acc@1  18.75 ( 15.71)	Acc@5  52.34 ( 44.49)
Epoch: [3][380/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.4414e+00 (3.3789e+00)	Acc@1  16.41 ( 15.75)	Acc@5  44.53 ( 44.63)
Epoch: [3][390/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2715e+00 (3.3742e+00)	Acc@1  20.00 ( 15.85)	Acc@5  46.25 ( 44.75)
## e[3] optimizer.zero_grad (sum) time: 0.11635136604309082
## e[3]       loss.backward (sum) time: 2.241753101348877
## e[3]      optimizer.step (sum) time: 0.9532284736633301
## epoch[3] training(only) time: 9.970341205596924
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 3.1250e+00 (3.1250e+00)	Acc@1  22.00 ( 22.00)	Acc@5  53.00 ( 53.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 3.2910e+00 (3.1383e+00)	Acc@1  18.00 ( 20.55)	Acc@5  45.00 ( 51.18)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 3.1328e+00 (3.1474e+00)	Acc@1  22.00 ( 20.52)	Acc@5  49.00 ( 50.95)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 3.3320e+00 (3.1708e+00)	Acc@1  20.00 ( 19.94)	Acc@5  49.00 ( 50.84)
Test: [ 40/100]	Time  0.012 ( 0.016)	Loss 3.1719e+00 (3.1637e+00)	Acc@1  19.00 ( 19.71)	Acc@5  46.00 ( 50.59)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 3.0684e+00 (3.1577e+00)	Acc@1  23.00 ( 20.06)	Acc@5  54.00 ( 50.67)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 3.0781e+00 (3.1458e+00)	Acc@1  21.00 ( 20.10)	Acc@5  56.00 ( 50.80)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 3.3379e+00 (3.1459e+00)	Acc@1  12.00 ( 20.06)	Acc@5  47.00 ( 50.90)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 3.0996e+00 (3.1474e+00)	Acc@1  19.00 ( 20.07)	Acc@5  52.00 ( 51.05)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 3.1523e+00 (3.1424e+00)	Acc@1  24.00 ( 20.11)	Acc@5  50.00 ( 51.15)
 * Acc@1 20.180 Acc@5 50.950
### epoch[3] execution time: 11.5194091796875
EPOCH 4
i:   0, name:       features.module.0.weight  changing lr from: 0.099957819810111703   to: 0.099152256835388142
i:   1, name:         features.module.0.bias  changing lr from: 0.099997892744552064   to: 0.099223118303654184
i:   2, name:       features.module.1.weight  changing lr from: 0.100036183624119909   to: 0.099290842496699727
i:   3, name:         features.module.1.bias  changing lr from: 0.100072782138654337   to: 0.099355586184914524
i:   4, name:       features.module.4.weight  changing lr from: 0.100107772956131005   to: 0.099417497458466508
i:   5, name:         features.module.4.bias  changing lr from: 0.100141236031030340   to: 0.099476716251288941
i:   6, name:       features.module.5.weight  changing lr from: 0.100173246892199336   to: 0.099533374831111299
i:   7, name:         features.module.5.bias  changing lr from: 0.100203876911666684   to: 0.099587598257857526
i:   8, name:       features.module.8.weight  changing lr from: 0.100233193555760797   to: 0.099639504812571139
i:   9, name:         features.module.8.bias  changing lr from: 0.100261260619778914   to: 0.099689206398872937
i:  10, name:       features.module.9.weight  changing lr from: 0.100288138447362618   to: 0.099736808918816325
i:  11, name:         features.module.9.bias  changing lr from: 0.100313884135648818   to: 0.099782412624872155
i:  12, name:      features.module.11.weight  changing lr from: 0.100338551727185712   to: 0.099826112449653906
i:  13, name:        features.module.11.bias  changing lr from: 0.100362192389530924   to: 0.099867998314879636
i:  14, name:      features.module.12.weight  changing lr from: 0.100384854583380631   to: 0.099908155420962208
i:  15, name:        features.module.12.bias  changing lr from: 0.100406584220016762   to: 0.099946664518521633
i:  16, name:      features.module.15.weight  changing lr from: 0.100427424808802096   to: 0.099983602163022267
i:  17, name:        features.module.15.bias  changing lr from: 0.100447417595399255   to: 0.100019040953653965
i:  18, name:      features.module.16.weight  changing lr from: 0.100466601691341736   to: 0.100053049757498055
i:  19, name:        features.module.16.bias  changing lr from: 0.100485014195538902   to: 0.100085693919946372
i:  20, name:      features.module.18.weight  changing lr from: 0.100502690308255593   to: 0.100117035462274823
i:  21, name:        features.module.18.bias  changing lr from: 0.100519663438068355   to: 0.100147133267209920
i:  22, name:      features.module.19.weight  changing lr from: 0.100535965302264232   to: 0.100176043253269925
i:  23, name:        features.module.19.bias  changing lr from: 0.100551626021115656   to: 0.100203818538607173
i:  24, name:      features.module.22.weight  changing lr from: 0.100566674206433726   to: 0.100230509595029563
i:  25, name:        features.module.22.bias  changing lr from: 0.100581137044774421   to: 0.100256164392831970
i:  26, name:      features.module.23.weight  changing lr from: 0.100595040375645775   to: 0.100280828537025801
i:  27, name:        features.module.23.bias  changing lr from: 0.100608408765040208   to: 0.100304545395515241
i:  28, name:      features.module.25.weight  changing lr from: 0.100621265574593180   to: 0.100327356219730657
i:  29, name:        features.module.25.bias  changing lr from: 0.100633633026649252   to: 0.100349300258196508
i:  30, name:      features.module.26.weight  changing lr from: 0.100645532265496607   to: 0.100370414863478044
i:  31, name:        features.module.26.bias  changing lr from: 0.100656983415013918   to: 0.100390735592921942
i:  32, name:            classifier.0.weight  changing lr from: 0.100668005632956373   to: 0.100410296303577742
i:  33, name:              classifier.0.bias  changing lr from: 0.100678617162092504   to: 0.100429129241662293
i:  34, name:            classifier.3.weight  changing lr from: 0.100688835378389199   to: 0.100447265126904078
i:  35, name:              classifier.3.bias  changing lr from: 0.100698676836428927   to: 0.100464733232083367
i:  36, name:            classifier.6.weight  changing lr from: 0.100708157312231109   to: 0.100481561458062474
i:  37, name:              classifier.6.bias  changing lr from: 0.100717291843637827   to: 0.100497776404581712



# Switched to train mode...
Epoch: [4][  0/391]	Time  0.177 ( 0.177)	Data  0.148 ( 0.148)	Loss 3.1797e+00 (3.1797e+00)	Acc@1  18.75 ( 18.75)	Acc@5  48.44 ( 48.44)
Epoch: [4][ 10/391]	Time  0.040 ( 0.039)	Data  0.002 ( 0.015)	Loss 3.1484e+00 (3.1799e+00)	Acc@1  19.53 ( 19.67)	Acc@5  46.88 ( 49.64)
Epoch: [4][ 20/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.1055e+00 (3.2141e+00)	Acc@1  16.41 ( 18.38)	Acc@5  57.03 ( 48.74)
Epoch: [4][ 30/391]	Time  0.020 ( 0.029)	Data  0.002 ( 0.006)	Loss 3.1914e+00 (3.2259e+00)	Acc@1  15.62 ( 18.02)	Acc@5  50.78 ( 48.46)
Epoch: [4][ 40/391]	Time  0.036 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.3652e+00 (3.2166e+00)	Acc@1  17.97 ( 18.14)	Acc@5  46.09 ( 48.82)
Epoch: [4][ 50/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.2266e+00 (3.2287e+00)	Acc@1  20.31 ( 18.12)	Acc@5  48.44 ( 48.58)
Epoch: [4][ 60/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.1953e+00 (3.2313e+00)	Acc@1  25.78 ( 18.40)	Acc@5  48.44 ( 48.69)
Epoch: [4][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.2188e+00 (3.2114e+00)	Acc@1  16.41 ( 18.71)	Acc@5  52.34 ( 49.33)
Epoch: [4][ 80/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 3.0898e+00 (3.2124e+00)	Acc@1  21.09 ( 18.79)	Acc@5  55.47 ( 49.35)
Epoch: [4][ 90/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3398e+00 (3.2095e+00)	Acc@1  14.84 ( 18.82)	Acc@5  42.19 ( 49.54)
Epoch: [4][100/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.9512e+00 (3.2026e+00)	Acc@1  25.78 ( 19.19)	Acc@5  57.03 ( 49.74)
Epoch: [4][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0938e+00 (3.1981e+00)	Acc@1  22.66 ( 19.16)	Acc@5  53.12 ( 49.85)
Epoch: [4][120/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.2266e+00 (3.2004e+00)	Acc@1  17.19 ( 19.05)	Acc@5  47.66 ( 49.63)
Epoch: [4][130/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3066e+00 (3.1991e+00)	Acc@1  12.50 ( 18.99)	Acc@5  47.66 ( 49.83)
Epoch: [4][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2793e+00 (3.1954e+00)	Acc@1  19.53 ( 19.10)	Acc@5  50.00 ( 49.91)
Epoch: [4][150/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0137e+00 (3.1917e+00)	Acc@1  21.09 ( 19.15)	Acc@5  53.12 ( 49.97)
Epoch: [4][160/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2207e+00 (3.1888e+00)	Acc@1  18.75 ( 19.28)	Acc@5  46.88 ( 50.02)
Epoch: [4][170/391]	Time  0.039 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.1719e+00 (3.1901e+00)	Acc@1  20.31 ( 19.20)	Acc@5  53.91 ( 50.01)
Epoch: [4][180/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.1172e+00 (3.1877e+00)	Acc@1  22.66 ( 19.25)	Acc@5  50.00 ( 50.08)
Epoch: [4][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2012e+00 (3.1817e+00)	Acc@1  21.88 ( 19.39)	Acc@5  50.00 ( 50.25)
Epoch: [4][200/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1797e+00 (3.1782e+00)	Acc@1  24.22 ( 19.56)	Acc@5  52.34 ( 50.41)
Epoch: [4][210/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.0488e+00 (3.1737e+00)	Acc@1  22.66 ( 19.64)	Acc@5  50.78 ( 50.50)
Epoch: [4][220/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0293e+00 (3.1690e+00)	Acc@1  17.19 ( 19.77)	Acc@5  58.59 ( 50.72)
Epoch: [4][230/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2617e+00 (3.1671e+00)	Acc@1  15.62 ( 19.83)	Acc@5  50.78 ( 50.84)
Epoch: [4][240/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.3516e+00 (3.1652e+00)	Acc@1  13.28 ( 19.90)	Acc@5  51.56 ( 50.95)
Epoch: [4][250/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.0449e+00 (3.1594e+00)	Acc@1  22.66 ( 20.05)	Acc@5  51.56 ( 51.11)
Epoch: [4][260/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.3516e+00 (3.1612e+00)	Acc@1  15.62 ( 20.00)	Acc@5  45.31 ( 51.11)
Epoch: [4][270/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.1367e+00 (3.1614e+00)	Acc@1  18.75 ( 19.97)	Acc@5  56.25 ( 51.12)
Epoch: [4][280/391]	Time  0.029 ( 0.025)	Data  0.003 ( 0.002)	Loss 3.1367e+00 (3.1607e+00)	Acc@1  20.31 ( 20.04)	Acc@5  53.91 ( 51.14)
Epoch: [4][290/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2070e+00 (3.1586e+00)	Acc@1  19.53 ( 20.12)	Acc@5  47.66 ( 51.24)
Epoch: [4][300/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0488e+00 (3.1570e+00)	Acc@1  24.22 ( 20.14)	Acc@5  52.34 ( 51.27)
Epoch: [4][310/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2734e+00 (3.1566e+00)	Acc@1  19.53 ( 20.17)	Acc@5  52.34 ( 51.30)
Epoch: [4][320/391]	Time  0.035 ( 0.025)	Data  0.005 ( 0.002)	Loss 3.0488e+00 (3.1564e+00)	Acc@1  24.22 ( 20.21)	Acc@5  56.25 ( 51.28)
Epoch: [4][330/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.2129e+00 (3.1546e+00)	Acc@1  20.31 ( 20.24)	Acc@5  46.88 ( 51.32)
Epoch: [4][340/391]	Time  0.047 ( 0.025)	Data  0.006 ( 0.002)	Loss 2.9082e+00 (3.1497e+00)	Acc@1  26.56 ( 20.34)	Acc@5  57.03 ( 51.47)
Epoch: [4][350/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1523e+00 (3.1465e+00)	Acc@1  19.53 ( 20.36)	Acc@5  52.34 ( 51.58)
Epoch: [4][360/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9688e+00 (3.1425e+00)	Acc@1  21.88 ( 20.47)	Acc@5  55.47 ( 51.69)
Epoch: [4][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0742e+00 (3.1402e+00)	Acc@1  19.53 ( 20.52)	Acc@5  53.91 ( 51.77)
Epoch: [4][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7832e+00 (3.1379e+00)	Acc@1  27.34 ( 20.58)	Acc@5  64.84 ( 51.81)
Epoch: [4][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8848e+00 (3.1354e+00)	Acc@1  28.75 ( 20.63)	Acc@5  63.75 ( 51.90)
## e[4] optimizer.zero_grad (sum) time: 0.1135413646697998
## e[4]       loss.backward (sum) time: 2.2682247161865234
## e[4]      optimizer.step (sum) time: 0.9561314582824707
## epoch[4] training(only) time: 9.978352785110474
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 3.2500e+00 (3.2500e+00)	Acc@1  25.00 ( 25.00)	Acc@5  51.00 ( 51.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 2.8887e+00 (2.9529e+00)	Acc@1  23.00 ( 25.09)	Acc@5  60.00 ( 56.82)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 3.0234e+00 (2.9632e+00)	Acc@1  22.00 ( 23.81)	Acc@5  52.00 ( 56.43)
Test: [ 30/100]	Time  0.009 ( 0.018)	Loss 3.2617e+00 (2.9747e+00)	Acc@1  24.00 ( 23.29)	Acc@5  52.00 ( 56.26)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 3.0254e+00 (2.9828e+00)	Acc@1  26.00 ( 23.07)	Acc@5  52.00 ( 56.63)
Test: [ 50/100]	Time  0.014 ( 0.017)	Loss 2.6426e+00 (2.9822e+00)	Acc@1  33.00 ( 23.37)	Acc@5  61.00 ( 56.39)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 2.7500e+00 (2.9747e+00)	Acc@1  25.00 ( 23.69)	Acc@5  69.00 ( 56.51)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 3.2578e+00 (2.9817e+00)	Acc@1  22.00 ( 23.49)	Acc@5  50.00 ( 56.41)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 2.9180e+00 (2.9838e+00)	Acc@1  24.00 ( 23.25)	Acc@5  60.00 ( 56.35)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.7637e+00 (2.9764e+00)	Acc@1  28.00 ( 23.56)	Acc@5  59.00 ( 56.62)
 * Acc@1 23.440 Acc@5 56.410
### epoch[4] execution time: 11.562774419784546
EPOCH 5
i:   0, name:       features.module.0.weight  changing lr from: 0.099152256835388142   to: 0.098122964374747490
i:   1, name:         features.module.0.bias  changing lr from: 0.099223118303654184   to: 0.098232926204039811
i:   2, name:       features.module.1.weight  changing lr from: 0.099290842496699727   to: 0.098338047636507692
i:   3, name:         features.module.1.bias  changing lr from: 0.099355586184914524   to: 0.098438568285074565
i:   4, name:       features.module.4.weight  changing lr from: 0.099417497458466508   to: 0.098534714692069966
i:   5, name:         features.module.4.bias  changing lr from: 0.099476716251288941   to: 0.098626701099963146
i:   6, name:       features.module.5.weight  changing lr from: 0.099533374831111299   to: 0.098714730173969326
i:   7, name:         features.module.5.bias  changing lr from: 0.099587598257857526   to: 0.098798993679628147
i:   8, name:       features.module.8.weight  changing lr from: 0.099639504812571139   to: 0.098879673118257141
i:   9, name:         features.module.8.bias  changing lr from: 0.099689206398872937   to: 0.098956940322995957
i:  10, name:       features.module.9.weight  changing lr from: 0.099736808918816325   to: 0.099030958017982965
i:  11, name:         features.module.9.bias  changing lr from: 0.099782412624872155   to: 0.099101880343040571
i:  12, name:      features.module.11.weight  changing lr from: 0.099826112449653906   to: 0.099169853346091019
i:  13, name:        features.module.11.bias  changing lr from: 0.099867998314879636   to: 0.099235015445379715
i:  14, name:      features.module.12.weight  changing lr from: 0.099908155420962208   to: 0.099297497863446549
i:  15, name:        features.module.12.bias  changing lr from: 0.099946664518521633   to: 0.099357425034658720
i:  16, name:      features.module.15.weight  changing lr from: 0.099983602163022267   to: 0.099414914987999328
i:  17, name:        features.module.15.bias  changing lr from: 0.100019040953653965   to: 0.099470079706694220
i:  18, name:      features.module.16.weight  changing lr from: 0.100053049757498055   to: 0.099523025466155532
i:  19, name:        features.module.16.bias  changing lr from: 0.100085693919946372   to: 0.099573853151622982
i:  20, name:      features.module.18.weight  changing lr from: 0.100117035462274823   to: 0.099622658556792670
i:  21, name:        features.module.18.bias  changing lr from: 0.100147133267209920   to: 0.099669532664638363
i:  22, name:      features.module.19.weight  changing lr from: 0.100176043253269925   to: 0.099714561911551050
i:  23, name:        features.module.19.bias  changing lr from: 0.100203818538607173   to: 0.099757828435847892
i:  24, name:      features.module.22.weight  changing lr from: 0.100230509595029563   to: 0.099799410311633349
i:  25, name:        features.module.22.bias  changing lr from: 0.100256164392831970   to: 0.099839381768930063
i:  26, name:      features.module.23.weight  changing lr from: 0.100280828537025801   to: 0.099877813400937557
i:  27, name:        features.module.23.bias  changing lr from: 0.100304545395515241   to: 0.099914772359219917
i:  28, name:      features.module.25.weight  changing lr from: 0.100327356219730657   to: 0.099950322537572189
i:  29, name:        features.module.25.bias  changing lr from: 0.100349300258196508   to: 0.099984524745265424
i:  30, name:      features.module.26.weight  changing lr from: 0.100370414863478044   to: 0.100017436870325674
i:  31, name:        features.module.26.bias  changing lr from: 0.100390735592921942   to: 0.100049114033458730
i:  32, name:            classifier.0.weight  changing lr from: 0.100410296303577742   to: 0.100079608733193881
i:  33, name:              classifier.0.bias  changing lr from: 0.100429129241662293   to: 0.100108970982781958
i:  34, name:            classifier.3.weight  changing lr from: 0.100447265126904078   to: 0.100137248439349166
i:  35, name:              classifier.3.bias  changing lr from: 0.100464733232083367   to: 0.100164486525775531
i:  36, name:            classifier.6.weight  changing lr from: 0.100481561458062474   to: 0.100190728545737101
i:  37, name:              classifier.6.bias  changing lr from: 0.100497776404581712   to: 0.100216015792322755



# Switched to train mode...
Epoch: [5][  0/391]	Time  0.168 ( 0.168)	Data  0.139 ( 0.139)	Loss 2.9141e+00 (2.9141e+00)	Acc@1  25.00 ( 25.00)	Acc@5  54.69 ( 54.69)
Epoch: [5][ 10/391]	Time  0.021 ( 0.037)	Data  0.002 ( 0.014)	Loss 2.7871e+00 (3.0169e+00)	Acc@1  26.56 ( 21.73)	Acc@5  64.06 ( 55.82)
Epoch: [5][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 3.0020e+00 (2.9923e+00)	Acc@1  22.66 ( 22.88)	Acc@5  53.91 ( 56.32)
Epoch: [5][ 30/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.9492e+00 (2.9686e+00)	Acc@1  25.00 ( 23.84)	Acc@5  59.38 ( 56.38)
Epoch: [5][ 40/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.8203e+00 (2.9605e+00)	Acc@1  28.12 ( 24.10)	Acc@5  57.81 ( 56.31)
Epoch: [5][ 50/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.3750e+00 (2.9545e+00)	Acc@1  20.31 ( 24.19)	Acc@5  48.44 ( 56.54)
Epoch: [5][ 60/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.0469e+00 (2.9534e+00)	Acc@1  24.22 ( 24.26)	Acc@5  53.12 ( 56.40)
Epoch: [5][ 70/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.8633e+00 (2.9522e+00)	Acc@1  25.00 ( 23.89)	Acc@5  62.50 ( 56.60)
Epoch: [5][ 80/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8574e+00 (2.9549e+00)	Acc@1  27.34 ( 23.95)	Acc@5  60.94 ( 56.56)
Epoch: [5][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7012e+00 (2.9512e+00)	Acc@1  28.91 ( 24.23)	Acc@5  63.28 ( 56.63)
Epoch: [5][100/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.0547e+00 (2.9526e+00)	Acc@1  26.56 ( 24.18)	Acc@5  58.59 ( 56.77)
Epoch: [5][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0605e+00 (2.9534e+00)	Acc@1  23.44 ( 24.16)	Acc@5  45.31 ( 56.61)
Epoch: [5][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9297e+00 (2.9471e+00)	Acc@1  24.22 ( 24.37)	Acc@5  58.59 ( 56.79)
Epoch: [5][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7715e+00 (2.9429e+00)	Acc@1  24.22 ( 24.45)	Acc@5  59.38 ( 56.92)
Epoch: [5][140/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.8086e+00 (2.9441e+00)	Acc@1  24.22 ( 24.31)	Acc@5  60.16 ( 56.95)
Epoch: [5][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0195e+00 (2.9410e+00)	Acc@1  25.00 ( 24.32)	Acc@5  53.91 ( 57.00)
Epoch: [5][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0898e+00 (2.9391e+00)	Acc@1  20.31 ( 24.33)	Acc@5  57.03 ( 57.08)
Epoch: [5][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0703e+00 (2.9372e+00)	Acc@1  19.53 ( 24.43)	Acc@5  49.22 ( 57.08)
Epoch: [5][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8672e+00 (2.9328e+00)	Acc@1  22.66 ( 24.46)	Acc@5  58.59 ( 57.26)
Epoch: [5][190/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.8027e+00 (2.9327e+00)	Acc@1  25.00 ( 24.46)	Acc@5  61.72 ( 57.31)
Epoch: [5][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1016e+00 (2.9339e+00)	Acc@1  19.53 ( 24.39)	Acc@5  52.34 ( 57.26)
Epoch: [5][210/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.8066e+00 (2.9309e+00)	Acc@1  29.69 ( 24.50)	Acc@5  61.72 ( 57.34)
Epoch: [5][220/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5938e+00 (2.9246e+00)	Acc@1  26.56 ( 24.59)	Acc@5  64.84 ( 57.43)
Epoch: [5][230/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7734e+00 (2.9235e+00)	Acc@1  25.78 ( 24.67)	Acc@5  62.50 ( 57.53)
Epoch: [5][240/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8730e+00 (2.9215e+00)	Acc@1  25.78 ( 24.65)	Acc@5  60.94 ( 57.55)
Epoch: [5][250/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0898e+00 (2.9200e+00)	Acc@1  21.09 ( 24.65)	Acc@5  52.34 ( 57.55)
Epoch: [5][260/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7754e+00 (2.9165e+00)	Acc@1  32.81 ( 24.67)	Acc@5  57.81 ( 57.57)
Epoch: [5][270/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7070e+00 (2.9190e+00)	Acc@1  22.66 ( 24.70)	Acc@5  64.84 ( 57.56)
Epoch: [5][280/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6582e+00 (2.9159e+00)	Acc@1  27.34 ( 24.77)	Acc@5  63.28 ( 57.66)
Epoch: [5][290/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0254e+00 (2.9157e+00)	Acc@1  18.75 ( 24.79)	Acc@5  51.56 ( 57.69)
Epoch: [5][300/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9727e+00 (2.9143e+00)	Acc@1  24.22 ( 24.82)	Acc@5  54.69 ( 57.77)
Epoch: [5][310/391]	Time  0.026 ( 0.025)	Data  0.003 ( 0.002)	Loss 2.7598e+00 (2.9140e+00)	Acc@1  26.56 ( 24.82)	Acc@5  64.84 ( 57.75)
Epoch: [5][320/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1699e+00 (2.9131e+00)	Acc@1  20.31 ( 24.84)	Acc@5  51.56 ( 57.80)
Epoch: [5][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6836e+00 (2.9132e+00)	Acc@1  25.78 ( 24.84)	Acc@5  56.25 ( 57.81)
Epoch: [5][340/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.6074e+00 (2.9104e+00)	Acc@1  32.03 ( 24.91)	Acc@5  64.06 ( 57.83)
Epoch: [5][350/391]	Time  0.044 ( 0.025)	Data  0.003 ( 0.002)	Loss 2.7598e+00 (2.9079e+00)	Acc@1  31.25 ( 24.92)	Acc@5  57.81 ( 57.91)
Epoch: [5][360/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8672e+00 (2.9030e+00)	Acc@1  30.47 ( 25.05)	Acc@5  60.16 ( 58.02)
Epoch: [5][370/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7910e+00 (2.8990e+00)	Acc@1  27.34 ( 25.09)	Acc@5  60.94 ( 58.11)
Epoch: [5][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9023e+00 (2.8950e+00)	Acc@1  26.56 ( 25.19)	Acc@5  60.94 ( 58.22)
Epoch: [5][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7852e+00 (2.8939e+00)	Acc@1  27.50 ( 25.23)	Acc@5  61.25 ( 58.24)
## e[5] optimizer.zero_grad (sum) time: 0.11334633827209473
## e[5]       loss.backward (sum) time: 2.2774927616119385
## e[5]      optimizer.step (sum) time: 0.945260763168335
## epoch[5] training(only) time: 9.99014163017273
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 2.7637e+00 (2.7637e+00)	Acc@1  27.00 ( 27.00)	Acc@5  60.00 ( 60.00)
Test: [ 10/100]	Time  0.015 ( 0.026)	Loss 2.8965e+00 (2.8423e+00)	Acc@1  20.00 ( 26.55)	Acc@5  57.00 ( 60.55)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 2.8105e+00 (2.8876e+00)	Acc@1  29.00 ( 26.76)	Acc@5  64.00 ( 59.86)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.7949e+00 (2.8848e+00)	Acc@1  31.00 ( 26.19)	Acc@5  59.00 ( 59.81)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 2.8906e+00 (2.8784e+00)	Acc@1  25.00 ( 26.10)	Acc@5  59.00 ( 60.05)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 2.7695e+00 (2.8769e+00)	Acc@1  31.00 ( 25.96)	Acc@5  58.00 ( 59.69)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 2.6758e+00 (2.8608e+00)	Acc@1  29.00 ( 26.23)	Acc@5  62.00 ( 60.03)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 3.1250e+00 (2.8726e+00)	Acc@1  23.00 ( 26.17)	Acc@5  55.00 ( 59.73)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 2.8906e+00 (2.8861e+00)	Acc@1  23.00 ( 25.85)	Acc@5  57.00 ( 59.52)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 2.6621e+00 (2.8778e+00)	Acc@1  41.00 ( 26.30)	Acc@5  61.00 ( 59.87)
 * Acc@1 26.150 Acc@5 59.590
### epoch[5] execution time: 11.602168083190918
EPOCH 6
i:   0, name:       features.module.0.weight  changing lr from: 0.098122964374747490   to: 0.096874724822374580
i:   1, name:         features.module.0.bias  changing lr from: 0.098232926204039811   to: 0.097031739736265069
i:   2, name:       features.module.1.weight  changing lr from: 0.098338047636507692   to: 0.097181892176736215
i:   3, name:         features.module.1.bias  changing lr from: 0.098438568285074565   to: 0.097325517873113235
i:   4, name:       features.module.4.weight  changing lr from: 0.098534714692069966   to: 0.097462934588436440
i:   5, name:         features.module.4.bias  changing lr from: 0.098626701099963146   to: 0.097594443146297352
i:   6, name:       features.module.5.weight  changing lr from: 0.098714730173969326   to: 0.097720328396847619
i:   7, name:         features.module.5.bias  changing lr from: 0.098798993679628147   to: 0.097840860125537818
i:   8, name:       features.module.8.weight  changing lr from: 0.098879673118257141   to: 0.097956293907960235
i:   9, name:         features.module.8.bias  changing lr from: 0.098956940322995957   to: 0.098066871913991202
i:  10, name:       features.module.9.weight  changing lr from: 0.099030958017982965   to: 0.098172823664255449
i:  11, name:         features.module.9.bias  changing lr from: 0.099101880343040571   to: 0.098274366741767843
i:  12, name:      features.module.11.weight  changing lr from: 0.099169853346091019   to: 0.098371707461447189
i:  13, name:        features.module.11.bias  changing lr from: 0.099235015445379715   to: 0.098465041500042594
i:  14, name:      features.module.12.weight  changing lr from: 0.099297497863446549   to: 0.098554554488865992
i:  15, name:        features.module.12.bias  changing lr from: 0.099357425034658720   to: 0.098640422571584396
i:  16, name:      features.module.15.weight  changing lr from: 0.099414914987999328   to: 0.098722812929191384
i:  17, name:        features.module.15.bias  changing lr from: 0.099470079706694220   to: 0.098801884274151958
i:  18, name:      features.module.16.weight  changing lr from: 0.099523025466155532   to: 0.098877787315594101
i:  19, name:        features.module.16.bias  changing lr from: 0.099573853151622982   to: 0.098950665197307441
i:  20, name:      features.module.18.weight  changing lr from: 0.099622658556792670   to: 0.099020653910202497
i:  21, name:        features.module.18.bias  changing lr from: 0.099669532664638363   to: 0.099087882680782530
i:  22, name:      features.module.19.weight  changing lr from: 0.099714561911551050   to: 0.099152474337085017
i:  23, name:        features.module.19.bias  changing lr from: 0.099757828435847892   to: 0.099214545653460218
i:  24, name:      features.module.22.weight  changing lr from: 0.099799410311633349   to: 0.099274207675469711
i:  25, name:        features.module.22.bias  changing lr from: 0.099839381768930063   to: 0.099331566026108098
i:  26, name:      features.module.23.weight  changing lr from: 0.099877813400937557   to: 0.099386721194477390
i:  27, name:        features.module.23.bias  changing lr from: 0.099914772359219917   to: 0.099439768807972700
i:  28, name:      features.module.25.weight  changing lr from: 0.099950322537572189   to: 0.099490799888972248
i:  29, name:        features.module.25.bias  changing lr from: 0.099984524745265424   to: 0.099539901096963479
i:  30, name:      features.module.26.weight  changing lr from: 0.100017436870325674   to: 0.099587154956978563
i:  31, name:        features.module.26.bias  changing lr from: 0.100049114033458730   to: 0.099632640075158679
i:  32, name:            classifier.0.weight  changing lr from: 0.100079608733193881   to: 0.099676431342215432
i:  33, name:              classifier.0.bias  changing lr from: 0.100108970982781958   to: 0.099718600125510065
i:  34, name:            classifier.3.weight  changing lr from: 0.100137248439349166   to: 0.099759214450426445
i:  35, name:              classifier.3.bias  changing lr from: 0.100164486525775531   to: 0.099798339171671871
i:  36, name:            classifier.6.weight  changing lr from: 0.100190728545737101   to: 0.099836036135100614
i:  37, name:              classifier.6.bias  changing lr from: 0.100216015792322755   to: 0.099872364330618246



# Switched to train mode...
Epoch: [6][  0/391]	Time  0.176 ( 0.176)	Data  0.149 ( 0.149)	Loss 2.7910e+00 (2.7910e+00)	Acc@1  31.25 ( 31.25)	Acc@5  61.72 ( 61.72)
Epoch: [6][ 10/391]	Time  0.023 ( 0.039)	Data  0.001 ( 0.015)	Loss 2.8711e+00 (2.7264e+00)	Acc@1  29.69 ( 27.70)	Acc@5  56.25 ( 62.14)
Epoch: [6][ 20/391]	Time  0.036 ( 0.032)	Data  0.003 ( 0.009)	Loss 2.6836e+00 (2.7648e+00)	Acc@1  28.91 ( 27.94)	Acc@5  66.41 ( 61.57)
Epoch: [6][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.6816e+00 (2.7684e+00)	Acc@1  29.69 ( 27.97)	Acc@5  65.62 ( 61.27)
Epoch: [6][ 40/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.8281e+00 (2.7640e+00)	Acc@1  28.12 ( 27.34)	Acc@5  64.84 ( 61.43)
Epoch: [6][ 50/391]	Time  0.032 ( 0.028)	Data  0.003 ( 0.005)	Loss 2.5234e+00 (2.7566e+00)	Acc@1  31.25 ( 27.71)	Acc@5  65.62 ( 61.61)
Epoch: [6][ 60/391]	Time  0.025 ( 0.027)	Data  0.004 ( 0.004)	Loss 2.7871e+00 (2.7436e+00)	Acc@1  26.56 ( 28.29)	Acc@5  62.50 ( 62.00)
Epoch: [6][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5898e+00 (2.7353e+00)	Acc@1  29.69 ( 28.27)	Acc@5  67.19 ( 62.30)
Epoch: [6][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.6348e+00 (2.7473e+00)	Acc@1  27.34 ( 27.93)	Acc@5  64.06 ( 62.03)
Epoch: [6][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.9160e+00 (2.7485e+00)	Acc@1  22.66 ( 27.95)	Acc@5  55.47 ( 61.97)
Epoch: [6][100/391]	Time  0.029 ( 0.026)	Data  0.006 ( 0.003)	Loss 2.6172e+00 (2.7433e+00)	Acc@1  35.16 ( 28.11)	Acc@5  64.84 ( 62.12)
Epoch: [6][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6094e+00 (2.7471e+00)	Acc@1  32.03 ( 28.01)	Acc@5  64.06 ( 62.10)
Epoch: [6][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8418e+00 (2.7453e+00)	Acc@1  28.12 ( 28.11)	Acc@5  61.72 ( 62.19)
Epoch: [6][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5020e+00 (2.7407e+00)	Acc@1  32.03 ( 28.18)	Acc@5  65.62 ( 62.24)
Epoch: [6][140/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8223e+00 (2.7406e+00)	Acc@1  31.25 ( 28.15)	Acc@5  59.38 ( 62.25)
Epoch: [6][150/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7441e+00 (2.7446e+00)	Acc@1  27.34 ( 28.02)	Acc@5  65.62 ( 62.10)
Epoch: [6][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6016e+00 (2.7434e+00)	Acc@1  29.69 ( 28.11)	Acc@5  69.53 ( 62.14)
Epoch: [6][170/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.6035e+00 (2.7403e+00)	Acc@1  34.38 ( 28.20)	Acc@5  63.28 ( 62.20)
Epoch: [6][180/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.7910e+00 (2.7409e+00)	Acc@1  31.25 ( 28.20)	Acc@5  59.38 ( 62.18)
Epoch: [6][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8613e+00 (2.7363e+00)	Acc@1  23.44 ( 28.32)	Acc@5  60.16 ( 62.34)
Epoch: [6][200/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7051e+00 (2.7327e+00)	Acc@1  29.69 ( 28.38)	Acc@5  60.94 ( 62.48)
Epoch: [6][210/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6914e+00 (2.7352e+00)	Acc@1  29.69 ( 28.42)	Acc@5  64.06 ( 62.39)
Epoch: [6][220/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5234e+00 (2.7319e+00)	Acc@1  35.16 ( 28.54)	Acc@5  63.28 ( 62.47)
Epoch: [6][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7793e+00 (2.7317e+00)	Acc@1  28.91 ( 28.56)	Acc@5  60.94 ( 62.49)
Epoch: [6][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7109e+00 (2.7237e+00)	Acc@1  28.12 ( 28.79)	Acc@5  60.16 ( 62.67)
Epoch: [6][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7422e+00 (2.7225e+00)	Acc@1  30.47 ( 28.83)	Acc@5  61.72 ( 62.71)
Epoch: [6][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3730e+00 (2.7196e+00)	Acc@1  37.50 ( 28.92)	Acc@5  74.22 ( 62.77)
Epoch: [6][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8047e+00 (2.7188e+00)	Acc@1  31.25 ( 28.96)	Acc@5  60.94 ( 62.81)
Epoch: [6][280/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8867e+00 (2.7166e+00)	Acc@1  24.22 ( 29.04)	Acc@5  60.16 ( 62.84)
Epoch: [6][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7949e+00 (2.7184e+00)	Acc@1  27.34 ( 29.01)	Acc@5  60.16 ( 62.78)
Epoch: [6][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7793e+00 (2.7174e+00)	Acc@1  28.12 ( 29.05)	Acc@5  60.16 ( 62.80)
Epoch: [6][310/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.7715e+00 (2.7177e+00)	Acc@1  32.03 ( 29.09)	Acc@5  60.94 ( 62.80)
Epoch: [6][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7246e+00 (2.7187e+00)	Acc@1  30.47 ( 29.13)	Acc@5  64.06 ( 62.78)
Epoch: [6][330/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.8281e+00 (2.7166e+00)	Acc@1  27.34 ( 29.19)	Acc@5  60.16 ( 62.81)
Epoch: [6][340/391]	Time  0.040 ( 0.025)	Data  0.003 ( 0.002)	Loss 2.7090e+00 (2.7136e+00)	Acc@1  27.34 ( 29.26)	Acc@5  58.59 ( 62.88)
Epoch: [6][350/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3770e+00 (2.7102e+00)	Acc@1  41.41 ( 29.38)	Acc@5  69.53 ( 62.94)
Epoch: [6][360/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.5078e+00 (2.7094e+00)	Acc@1  28.91 ( 29.40)	Acc@5  64.84 ( 62.96)
Epoch: [6][370/391]	Time  0.028 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.6055e+00 (2.7074e+00)	Acc@1  26.56 ( 29.39)	Acc@5  67.19 ( 63.01)
Epoch: [6][380/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6816e+00 (2.7076e+00)	Acc@1  28.12 ( 29.38)	Acc@5  62.50 ( 62.99)
Epoch: [6][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7676e+00 (2.7079e+00)	Acc@1  30.00 ( 29.37)	Acc@5  63.75 ( 62.98)
## e[6] optimizer.zero_grad (sum) time: 0.11232638359069824
## e[6]       loss.backward (sum) time: 2.269374370574951
## e[6]      optimizer.step (sum) time: 0.9520859718322754
## epoch[6] training(only) time: 9.996461391448975
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 2.5547e+00 (2.5547e+00)	Acc@1  27.00 ( 27.00)	Acc@5  64.00 ( 64.00)
Test: [ 10/100]	Time  0.009 ( 0.024)	Loss 2.6016e+00 (2.5618e+00)	Acc@1  30.00 ( 30.64)	Acc@5  58.00 ( 66.45)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 2.4004e+00 (2.5428e+00)	Acc@1  38.00 ( 32.14)	Acc@5  74.00 ( 67.48)
Test: [ 30/100]	Time  0.013 ( 0.017)	Loss 2.7988e+00 (2.5498e+00)	Acc@1  31.00 ( 32.29)	Acc@5  62.00 ( 66.77)
Test: [ 40/100]	Time  0.010 ( 0.016)	Loss 2.5547e+00 (2.5497e+00)	Acc@1  28.00 ( 32.02)	Acc@5  74.00 ( 66.66)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 2.4902e+00 (2.5589e+00)	Acc@1  37.00 ( 32.08)	Acc@5  62.00 ( 66.18)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 2.4844e+00 (2.5520e+00)	Acc@1  34.00 ( 32.23)	Acc@5  69.00 ( 66.33)
Test: [ 70/100]	Time  0.014 ( 0.015)	Loss 2.6582e+00 (2.5645e+00)	Acc@1  35.00 ( 32.06)	Acc@5  61.00 ( 66.23)
Test: [ 80/100]	Time  0.011 ( 0.015)	Loss 2.6934e+00 (2.5701e+00)	Acc@1  33.00 ( 31.98)	Acc@5  59.00 ( 66.06)
Test: [ 90/100]	Time  0.032 ( 0.015)	Loss 2.4414e+00 (2.5609e+00)	Acc@1  40.00 ( 32.46)	Acc@5  63.00 ( 66.21)
 * Acc@1 32.540 Acc@5 66.010
### epoch[6] execution time: 11.543767929077148
EPOCH 7
i:   0, name:       features.module.0.weight  changing lr from: 0.096874724822374580   to: 0.095413337864757808
i:   1, name:         features.module.0.bias  changing lr from: 0.097031739736265069   to: 0.095624924724558483
i:   2, name:       features.module.1.weight  changing lr from: 0.097181892176736215   to: 0.095827342872020202
i:   3, name:         features.module.1.bias  changing lr from: 0.097325517873113235   to: 0.096021034456904908
i:   4, name:       features.module.4.weight  changing lr from: 0.097462934588436440   to: 0.096206418529736909
i:   5, name:         features.module.4.bias  changing lr from: 0.097594443146297352   to: 0.096383892308674890
i:   6, name:       features.module.5.weight  changing lr from: 0.097720328396847619   to: 0.096553832376845380
i:   7, name:         features.module.5.bias  changing lr from: 0.097840860125537818   to: 0.096716595813575196
i:   8, name:       features.module.8.weight  changing lr from: 0.097956293907960235   to: 0.096872521262865996
i:   9, name:         features.module.8.bias  changing lr from: 0.098066871913991202   to: 0.097021929942348151
i:  10, name:       features.module.9.weight  changing lr from: 0.098172823664255449   to: 0.097165126595837273
i:  11, name:         features.module.9.bias  changing lr from: 0.098274366741767843   to: 0.097302400392496560
i:  12, name:      features.module.11.weight  changing lr from: 0.098371707461447189   to: 0.097434025775485461
i:  13, name:        features.module.11.bias  changing lr from: 0.098465041500042594   to: 0.097560263262850014
i:  14, name:      features.module.12.weight  changing lr from: 0.098554554488865992   to: 0.097681360203285772
i:  15, name:        features.module.12.bias  changing lr from: 0.098640422571584396   to: 0.097797551489279919
i:  16, name:      features.module.15.weight  changing lr from: 0.098722812929191384   to: 0.097909060230017511
i:  17, name:        features.module.15.bias  changing lr from: 0.098801884274151958   to: 0.098016098386317932
i:  18, name:      features.module.16.weight  changing lr from: 0.098877787315594101   to: 0.098118867369750951
i:  19, name:        features.module.16.bias  changing lr from: 0.098950665197307441   to: 0.098217558607970171
i:  20, name:      features.module.18.weight  changing lr from: 0.099020653910202497   to: 0.098312354078192818
i:  21, name:        features.module.18.bias  changing lr from: 0.099087882680782530   to: 0.098403426810651118
i:  22, name:      features.module.19.weight  changing lr from: 0.099152474337085017   to: 0.098490941363739981
i:  23, name:        features.module.19.bias  changing lr from: 0.099214545653460218   to: 0.098575054272491053
i:  24, name:      features.module.22.weight  changing lr from: 0.099274207675469711   to: 0.098655914471911063
i:  25, name:        features.module.22.bias  changing lr from: 0.099331566026108098   to: 0.098733663696636290
i:  26, name:      features.module.23.weight  changing lr from: 0.099386721194477390   to: 0.098808436858272078
i:  27, name:        features.module.23.bias  changing lr from: 0.099439768807972700   to: 0.098880362401707861
i:  28, name:      features.module.25.weight  changing lr from: 0.099490799888972248   to: 0.098949562641623939
i:  29, name:        features.module.25.bias  changing lr from: 0.099539901096963479   to: 0.099016154080335803
i:  30, name:      features.module.26.weight  changing lr from: 0.099587154956978563   to: 0.099080247708055014
i:  31, name:        features.module.26.bias  changing lr from: 0.099632640075158679   to: 0.099141949286582889
i:  32, name:            classifier.0.weight  changing lr from: 0.099676431342215432   to: 0.099201359617393278
i:  33, name:              classifier.0.bias  changing lr from: 0.099718600125510065   to: 0.099258574795005355
i:  34, name:            classifier.3.weight  changing lr from: 0.099759214450426445   to: 0.099313686446493396
i:  35, name:              classifier.3.bias  changing lr from: 0.099798339171671871   to: 0.099366781957931277
i:  36, name:            classifier.6.weight  changing lr from: 0.099836036135100614   to: 0.099417944688521998
i:  37, name:              classifier.6.bias  changing lr from: 0.099872364330618246   to: 0.099467254173118483



# Switched to train mode...
Epoch: [7][  0/391]	Time  0.175 ( 0.175)	Data  0.146 ( 0.146)	Loss 2.3301e+00 (2.3301e+00)	Acc@1  37.50 ( 37.50)	Acc@5  70.31 ( 70.31)
Epoch: [7][ 10/391]	Time  0.030 ( 0.039)	Data  0.002 ( 0.015)	Loss 2.7031e+00 (2.5717e+00)	Acc@1  25.00 ( 31.25)	Acc@5  65.62 ( 66.62)
Epoch: [7][ 20/391]	Time  0.020 ( 0.032)	Data  0.000 ( 0.009)	Loss 2.3789e+00 (2.5490e+00)	Acc@1  37.50 ( 32.96)	Acc@5  67.19 ( 65.96)
Epoch: [7][ 30/391]	Time  0.020 ( 0.029)	Data  0.002 ( 0.006)	Loss 2.7188e+00 (2.5505e+00)	Acc@1  25.78 ( 32.91)	Acc@5  64.06 ( 66.18)
Epoch: [7][ 40/391]	Time  0.036 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.6797e+00 (2.5613e+00)	Acc@1  29.69 ( 32.76)	Acc@5  64.06 ( 66.27)
Epoch: [7][ 50/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.5586e+00 (2.5752e+00)	Acc@1  36.72 ( 32.54)	Acc@5  64.84 ( 66.13)
Epoch: [7][ 60/391]	Time  0.038 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.1172e+00 (2.5736e+00)	Acc@1  23.44 ( 32.58)	Acc@5  54.69 ( 66.19)
Epoch: [7][ 70/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.6875e+00 (2.5600e+00)	Acc@1  26.56 ( 33.05)	Acc@5  67.19 ( 66.43)
Epoch: [7][ 80/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5703e+00 (2.5592e+00)	Acc@1  32.03 ( 32.92)	Acc@5  67.97 ( 66.48)
Epoch: [7][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8008e+00 (2.5664e+00)	Acc@1  29.69 ( 32.82)	Acc@5  64.84 ( 66.35)
Epoch: [7][100/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7188e+00 (2.5612e+00)	Acc@1  25.78 ( 32.84)	Acc@5  65.62 ( 66.62)
Epoch: [7][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4316e+00 (2.5620e+00)	Acc@1  30.47 ( 32.83)	Acc@5  70.31 ( 66.60)
Epoch: [7][120/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7930e+00 (2.5672e+00)	Acc@1  27.34 ( 32.64)	Acc@5  61.72 ( 66.27)
Epoch: [7][130/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3613e+00 (2.5645e+00)	Acc@1  29.69 ( 32.73)	Acc@5  69.53 ( 66.39)
Epoch: [7][140/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7422e+00 (2.5620e+00)	Acc@1  29.69 ( 32.72)	Acc@5  61.72 ( 66.41)
Epoch: [7][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2344e+00 (2.5670e+00)	Acc@1  35.94 ( 32.63)	Acc@5  75.00 ( 66.30)
Epoch: [7][160/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.8164e+00 (2.5690e+00)	Acc@1  22.66 ( 32.55)	Acc@5  54.69 ( 66.30)
Epoch: [7][170/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5195e+00 (2.5716e+00)	Acc@1  33.59 ( 32.50)	Acc@5  65.62 ( 66.25)
Epoch: [7][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7422e+00 (2.5694e+00)	Acc@1  33.59 ( 32.54)	Acc@5  64.84 ( 66.35)
Epoch: [7][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5684e+00 (2.5690e+00)	Acc@1  33.59 ( 32.50)	Acc@5  68.75 ( 66.34)
Epoch: [7][200/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5254e+00 (2.5693e+00)	Acc@1  31.25 ( 32.42)	Acc@5  67.97 ( 66.37)
Epoch: [7][210/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3555e+00 (2.5702e+00)	Acc@1  35.16 ( 32.37)	Acc@5  71.88 ( 66.33)
Epoch: [7][220/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.7227e+00 (2.5702e+00)	Acc@1  31.25 ( 32.40)	Acc@5  63.28 ( 66.28)
Epoch: [7][230/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.5254e+00 (2.5713e+00)	Acc@1  35.16 ( 32.37)	Acc@5  67.19 ( 66.28)
Epoch: [7][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4668e+00 (2.5678e+00)	Acc@1  28.91 ( 32.47)	Acc@5  64.84 ( 66.41)
Epoch: [7][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3770e+00 (2.5667e+00)	Acc@1  34.38 ( 32.49)	Acc@5  69.53 ( 66.42)
Epoch: [7][260/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.4551e+00 (2.5627e+00)	Acc@1  33.59 ( 32.56)	Acc@5  66.41 ( 66.46)
Epoch: [7][270/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.002)	Loss 2.1973e+00 (2.5605e+00)	Acc@1  37.50 ( 32.59)	Acc@5  75.00 ( 66.54)
Epoch: [7][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6934e+00 (2.5607e+00)	Acc@1  26.56 ( 32.55)	Acc@5  64.84 ( 66.53)
Epoch: [7][290/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5000e+00 (2.5595e+00)	Acc@1  35.16 ( 32.59)	Acc@5  63.28 ( 66.52)
Epoch: [7][300/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5840e+00 (2.5592e+00)	Acc@1  32.03 ( 32.59)	Acc@5  64.84 ( 66.54)
Epoch: [7][310/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5117e+00 (2.5572e+00)	Acc@1  32.81 ( 32.66)	Acc@5  71.88 ( 66.61)
Epoch: [7][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5879e+00 (2.5538e+00)	Acc@1  29.69 ( 32.72)	Acc@5  63.28 ( 66.71)
Epoch: [7][330/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5000e+00 (2.5529e+00)	Acc@1  30.47 ( 32.76)	Acc@5  68.75 ( 66.71)
Epoch: [7][340/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5332e+00 (2.5505e+00)	Acc@1  33.59 ( 32.81)	Acc@5  70.31 ( 66.76)
Epoch: [7][350/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5820e+00 (2.5516e+00)	Acc@1  35.94 ( 32.79)	Acc@5  63.28 ( 66.75)
Epoch: [7][360/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4453e+00 (2.5527e+00)	Acc@1  34.38 ( 32.85)	Acc@5  66.41 ( 66.74)
Epoch: [7][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3457e+00 (2.5512e+00)	Acc@1  37.50 ( 32.90)	Acc@5  73.44 ( 66.75)
Epoch: [7][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4688e+00 (2.5500e+00)	Acc@1  32.03 ( 32.93)	Acc@5  66.41 ( 66.78)
Epoch: [7][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1738e+00 (2.5475e+00)	Acc@1  43.75 ( 32.99)	Acc@5  75.00 ( 66.84)
## e[7] optimizer.zero_grad (sum) time: 0.1120152473449707
## e[7]       loss.backward (sum) time: 2.2521822452545166
## e[7]      optimizer.step (sum) time: 0.9487981796264648
## epoch[7] training(only) time: 10.007266998291016
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 2.5176e+00 (2.5176e+00)	Acc@1  37.00 ( 37.00)	Acc@5  68.00 ( 68.00)
Test: [ 10/100]	Time  0.017 ( 0.026)	Loss 2.3984e+00 (2.3809e+00)	Acc@1  37.00 ( 37.91)	Acc@5  71.00 ( 69.82)
Test: [ 20/100]	Time  0.009 ( 0.020)	Loss 2.1406e+00 (2.3703e+00)	Acc@1  39.00 ( 37.52)	Acc@5  76.00 ( 70.76)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.3340e+00 (2.3570e+00)	Acc@1  40.00 ( 37.65)	Acc@5  69.00 ( 70.65)
Test: [ 40/100]	Time  0.009 ( 0.017)	Loss 2.4238e+00 (2.3634e+00)	Acc@1  30.00 ( 36.93)	Acc@5  76.00 ( 70.73)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 2.3750e+00 (2.3761e+00)	Acc@1  37.00 ( 36.78)	Acc@5  64.00 ( 70.14)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 2.1543e+00 (2.3614e+00)	Acc@1  42.00 ( 37.03)	Acc@5  73.00 ( 70.49)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 2.6172e+00 (2.3698e+00)	Acc@1  32.00 ( 36.90)	Acc@5  64.00 ( 70.54)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 2.5508e+00 (2.3825e+00)	Acc@1  37.00 ( 36.70)	Acc@5  69.00 ( 70.15)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.5098e+00 (2.3798e+00)	Acc@1  36.00 ( 36.86)	Acc@5  66.00 ( 70.16)
 * Acc@1 36.950 Acc@5 70.120
### epoch[7] execution time: 11.597169876098633
EPOCH 8
i:   0, name:       features.module.0.weight  changing lr from: 0.095413337864757808   to: 0.093745593533647337
i:   1, name:         features.module.0.bias  changing lr from: 0.095624924724558483   to: 0.094018765557129069
i:   2, name:       features.module.1.weight  changing lr from: 0.095827342872020202   to: 0.094280218761642415
i:   3, name:         features.module.1.bias  changing lr from: 0.096021034456904908   to: 0.094530508611467834
i:   4, name:       features.module.4.weight  changing lr from: 0.096206418529736909   to: 0.094770162405624414
i:   5, name:         features.module.4.bias  changing lr from: 0.096383892308674890   to: 0.094999680740319253
i:   6, name:       features.module.5.weight  changing lr from: 0.096553832376845380   to: 0.095219538899885867
i:   7, name:         features.module.5.bias  changing lr from: 0.096716595813575196   to: 0.095430188178678391
i:   8, name:       features.module.8.weight  changing lr from: 0.096872521262865996   to: 0.095632057136485205
i:   9, name:         features.module.8.bias  changing lr from: 0.097021929942348151   to: 0.095825552790081980
i:  10, name:       features.module.9.weight  changing lr from: 0.097165126595837273   to: 0.096011061743568576
i:  11, name:         features.module.9.bias  changing lr from: 0.097302400392496560   to: 0.096188951260131439
i:  12, name:      features.module.11.weight  changing lr from: 0.097434025775485461   to: 0.096359570277849915
i:  13, name:        features.module.11.bias  changing lr from: 0.097560263262850014   to: 0.096523250372123115
i:  14, name:      features.module.12.weight  changing lr from: 0.097681360203285772   to: 0.096680306667239799
i:  15, name:        features.module.12.bias  changing lr from: 0.097797551489279919   to: 0.096831038699548205
i:  16, name:      features.module.15.weight  changing lr from: 0.097909060230017511   to: 0.096975731234609577
i:  17, name:        features.module.15.bias  changing lr from: 0.098016098386317932   to: 0.097114655040640152
i:  18, name:      features.module.16.weight  changing lr from: 0.098118867369750951   to: 0.097248067620463161
i:  19, name:        features.module.16.bias  changing lr from: 0.098217558607970171   to: 0.097376213904106432
i:  20, name:      features.module.18.weight  changing lr from: 0.098312354078192818   to: 0.097499326904094463
i:  21, name:        features.module.18.bias  changing lr from: 0.098403426810651118   to: 0.097617628335395668
i:  22, name:      features.module.19.weight  changing lr from: 0.098490941363739981   to: 0.097731329201898934
i:  23, name:        features.module.19.bias  changing lr from: 0.098575054272491053   to: 0.097840630351207297
i:  24, name:      features.module.22.weight  changing lr from: 0.098655914471911063   to: 0.097945722999451923
i:  25, name:        features.module.22.bias  changing lr from: 0.098733663696636290   to: 0.098046789227747455
i:  26, name:      features.module.23.weight  changing lr from: 0.098808436858272078   to: 0.098144002451829471
i:  27, name:        features.module.23.bias  changing lr from: 0.098880362401707861   to: 0.098237527866336938
i:  28, name:      features.module.25.weight  changing lr from: 0.098949562641623939   to: 0.098327522865128011
i:  29, name:        features.module.25.bias  changing lr from: 0.099016154080335803   to: 0.098414137438945323
i:  30, name:      features.module.26.weight  changing lr from: 0.099080247708055014   to: 0.098497514551677379
i:  31, name:        features.module.26.bias  changing lr from: 0.099141949286582889   to: 0.098577790496396991
i:  32, name:            classifier.0.weight  changing lr from: 0.099201359617393278   to: 0.098655095232293905
i:  33, name:              classifier.0.bias  changing lr from: 0.099258574795005355   to: 0.098729552703558141
i:  34, name:            classifier.3.weight  changing lr from: 0.099313686446493396   to: 0.098801281141213768
i:  35, name:              classifier.3.bias  changing lr from: 0.099366781957931277   to: 0.098870393348846897
i:  36, name:            classifier.6.weight  changing lr from: 0.099417944688521998   to: 0.098936996973120472
i:  37, name:              classifier.6.bias  changing lr from: 0.099467254173118483   to: 0.099001194759918587



# Switched to train mode...
Epoch: [8][  0/391]	Time  0.147 ( 0.147)	Data  0.118 ( 0.118)	Loss 2.4004e+00 (2.4004e+00)	Acc@1  40.62 ( 40.62)	Acc@5  71.88 ( 71.88)
Epoch: [8][ 10/391]	Time  0.030 ( 0.038)	Data  0.001 ( 0.012)	Loss 2.1387e+00 (2.3869e+00)	Acc@1  42.97 ( 35.72)	Acc@5  75.78 ( 70.53)
Epoch: [8][ 20/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.007)	Loss 2.5332e+00 (2.4147e+00)	Acc@1  36.72 ( 35.31)	Acc@5  65.62 ( 69.64)
Epoch: [8][ 30/391]	Time  0.032 ( 0.030)	Data  0.002 ( 0.006)	Loss 2.4121e+00 (2.4265e+00)	Acc@1  37.50 ( 35.58)	Acc@5  68.75 ( 68.83)
Epoch: [8][ 40/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.1875e+00 (2.4120e+00)	Acc@1  40.62 ( 35.94)	Acc@5  75.00 ( 69.15)
Epoch: [8][ 50/391]	Time  0.021 ( 0.028)	Data  0.000 ( 0.004)	Loss 2.5410e+00 (2.4218e+00)	Acc@1  28.12 ( 36.04)	Acc@5  64.06 ( 69.16)
Epoch: [8][ 60/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.2695e+00 (2.4202e+00)	Acc@1  38.28 ( 36.24)	Acc@5  76.56 ( 69.28)
Epoch: [8][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.6855e+00 (2.4231e+00)	Acc@1  35.16 ( 36.50)	Acc@5  62.50 ( 69.17)
Epoch: [8][ 80/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.4453e+00 (2.4268e+00)	Acc@1  30.47 ( 36.18)	Acc@5  70.31 ( 69.10)
Epoch: [8][ 90/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6016e+00 (2.4278e+00)	Acc@1  27.34 ( 36.00)	Acc@5  67.97 ( 69.12)
Epoch: [8][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5625e+00 (2.4296e+00)	Acc@1  37.50 ( 35.99)	Acc@5  67.19 ( 69.14)
Epoch: [8][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4395e+00 (2.4317e+00)	Acc@1  33.59 ( 35.75)	Acc@5  69.53 ( 69.14)
Epoch: [8][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5312e+00 (2.4245e+00)	Acc@1  39.84 ( 35.83)	Acc@5  66.41 ( 69.24)
Epoch: [8][130/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1836e+00 (2.4198e+00)	Acc@1  36.72 ( 35.99)	Acc@5  73.44 ( 69.32)
Epoch: [8][140/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4277e+00 (2.4150e+00)	Acc@1  40.62 ( 36.08)	Acc@5  70.31 ( 69.42)
Epoch: [8][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4512e+00 (2.4174e+00)	Acc@1  36.72 ( 36.08)	Acc@5  73.44 ( 69.50)
Epoch: [8][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4414e+00 (2.4221e+00)	Acc@1  32.03 ( 35.98)	Acc@5  68.75 ( 69.44)
Epoch: [8][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1562e+00 (2.4222e+00)	Acc@1  40.62 ( 36.04)	Acc@5  71.88 ( 69.43)
Epoch: [8][180/391]	Time  0.048 ( 0.026)	Data  0.013 ( 0.003)	Loss 2.4883e+00 (2.4217e+00)	Acc@1  39.06 ( 36.02)	Acc@5  65.62 ( 69.49)
Epoch: [8][190/391]	Time  0.039 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.4297e+00 (2.4209e+00)	Acc@1  39.06 ( 36.07)	Acc@5  68.75 ( 69.49)
Epoch: [8][200/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2637e+00 (2.4226e+00)	Acc@1  36.72 ( 35.99)	Acc@5  75.78 ( 69.47)
Epoch: [8][210/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4199e+00 (2.4187e+00)	Acc@1  32.81 ( 36.08)	Acc@5  71.09 ( 69.58)
Epoch: [8][220/391]	Time  0.034 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4453e+00 (2.4204e+00)	Acc@1  28.91 ( 36.06)	Acc@5  71.09 ( 69.53)
Epoch: [8][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3008e+00 (2.4196e+00)	Acc@1  37.50 ( 36.10)	Acc@5  71.09 ( 69.60)
Epoch: [8][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1934e+00 (2.4175e+00)	Acc@1  38.28 ( 36.20)	Acc@5  75.00 ( 69.66)
Epoch: [8][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3496e+00 (2.4160e+00)	Acc@1  35.94 ( 36.23)	Acc@5  74.22 ( 69.71)
Epoch: [8][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3359e+00 (2.4159e+00)	Acc@1  40.62 ( 36.22)	Acc@5  68.75 ( 69.73)
Epoch: [8][270/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.5625e+00 (2.4191e+00)	Acc@1  31.25 ( 36.16)	Acc@5  67.19 ( 69.66)
Epoch: [8][280/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2793e+00 (2.4159e+00)	Acc@1  39.06 ( 36.22)	Acc@5  71.88 ( 69.69)
Epoch: [8][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7969e+00 (2.4137e+00)	Acc@1  24.22 ( 36.20)	Acc@5  58.59 ( 69.75)
Epoch: [8][300/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2285e+00 (2.4114e+00)	Acc@1  42.19 ( 36.24)	Acc@5  75.00 ( 69.82)
Epoch: [8][310/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2871e+00 (2.4121e+00)	Acc@1  34.38 ( 36.19)	Acc@5  71.09 ( 69.81)
Epoch: [8][320/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4492e+00 (2.4128e+00)	Acc@1  35.16 ( 36.14)	Acc@5  71.88 ( 69.81)
Epoch: [8][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2832e+00 (2.4093e+00)	Acc@1  39.06 ( 36.22)	Acc@5  73.44 ( 69.90)
Epoch: [8][340/391]	Time  0.022 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.2129e+00 (2.4060e+00)	Acc@1  34.38 ( 36.25)	Acc@5  75.00 ( 69.98)
Epoch: [8][350/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6387e+00 (2.4033e+00)	Acc@1  34.38 ( 36.29)	Acc@5  64.84 ( 70.00)
Epoch: [8][360/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4863e+00 (2.4030e+00)	Acc@1  37.50 ( 36.32)	Acc@5  69.53 ( 69.99)
Epoch: [8][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1113e+00 (2.4019e+00)	Acc@1  41.41 ( 36.38)	Acc@5  76.56 ( 70.03)
Epoch: [8][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6191e+00 (2.4009e+00)	Acc@1  28.91 ( 36.39)	Acc@5  64.84 ( 70.02)
Epoch: [8][390/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.3477e+00 (2.3977e+00)	Acc@1  36.25 ( 36.45)	Acc@5  72.50 ( 70.07)
## e[8] optimizer.zero_grad (sum) time: 0.11299872398376465
## e[8]       loss.backward (sum) time: 2.259674549102783
## e[8]      optimizer.step (sum) time: 0.9630811214447021
## epoch[8] training(only) time: 10.001696825027466
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 2.2871e+00 (2.2871e+00)	Acc@1  40.00 ( 40.00)	Acc@5  70.00 ( 70.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 2.1914e+00 (2.2349e+00)	Acc@1  36.00 ( 41.45)	Acc@5  74.00 ( 73.09)
Test: [ 20/100]	Time  0.014 ( 0.019)	Loss 2.1270e+00 (2.2257e+00)	Acc@1  41.00 ( 41.43)	Acc@5  76.00 ( 73.33)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.3555e+00 (2.2347e+00)	Acc@1  38.00 ( 41.39)	Acc@5  69.00 ( 73.42)
Test: [ 40/100]	Time  0.009 ( 0.016)	Loss 2.2168e+00 (2.2311e+00)	Acc@1  38.00 ( 41.20)	Acc@5  80.00 ( 73.56)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 2.1445e+00 (2.2461e+00)	Acc@1  44.00 ( 40.96)	Acc@5  73.00 ( 72.86)
Test: [ 60/100]	Time  0.019 ( 0.016)	Loss 2.2207e+00 (2.2239e+00)	Acc@1  42.00 ( 41.11)	Acc@5  74.00 ( 73.30)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 2.3086e+00 (2.2298e+00)	Acc@1  45.00 ( 40.97)	Acc@5  73.00 ( 73.20)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 2.3379e+00 (2.2420e+00)	Acc@1  42.00 ( 40.75)	Acc@5  67.00 ( 72.89)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 2.2363e+00 (2.2364e+00)	Acc@1  41.00 ( 40.78)	Acc@5  69.00 ( 72.93)
 * Acc@1 40.820 Acc@5 72.870
### epoch[8] execution time: 11.547666311264038
EPOCH 9
i:   0, name:       features.module.0.weight  changing lr from: 0.093745593533647337   to: 0.091879240657579200
i:   1, name:         features.module.0.bias  changing lr from: 0.094018765557129069   to: 0.092220437113153961
i:   2, name:       features.module.1.weight  changing lr from: 0.094280218761642415   to: 0.092547166171389833
i:   3, name:         features.module.1.bias  changing lr from: 0.094530508611467834   to: 0.092860099703250584
i:   4, name:       features.module.4.weight  changing lr from: 0.094770162405624414   to: 0.093159876749649095
i:   5, name:         features.module.4.bias  changing lr from: 0.094999680740319253   to: 0.093447105104434533
i:   6, name:       features.module.5.weight  changing lr from: 0.095219538899885867   to: 0.093722362833513415
i:   7, name:         features.module.5.bias  changing lr from: 0.095430188178678391   to: 0.093986199730475645
i:   8, name:       features.module.8.weight  changing lr from: 0.095632057136485205   to: 0.094239138709511103
i:   9, name:         features.module.8.bias  changing lr from: 0.095825552790081980   to: 0.094481677136735984
i:  10, name:       features.module.9.weight  changing lr from: 0.096011061743568576   to: 0.094714288101311203
i:  11, name:         features.module.9.bias  changing lr from: 0.096188951260131439   to: 0.094937421627939528
i:  12, name:      features.module.11.weight  changing lr from: 0.096359570277849915   to: 0.095151505832483135
i:  13, name:        features.module.11.bias  changing lr from: 0.096523250372123115   to: 0.095356948022555926
i:  14, name:      features.module.12.weight  changing lr from: 0.096680306667239799   to: 0.095554135745023250
i:  15, name:        features.module.12.bias  changing lr from: 0.096831038699548205   to: 0.095743437782390098
i:  16, name:      features.module.15.weight  changing lr from: 0.096975731234609577   to: 0.095925205100083302
i:  17, name:        features.module.15.bias  changing lr from: 0.097114655040640152   to: 0.096099771746638349
i:  18, name:      features.module.16.weight  changing lr from: 0.097248067620463161   to: 0.096267455708788918
i:  19, name:        features.module.16.bias  changing lr from: 0.097376213904106432   to: 0.096428559723432741
i:  20, name:      features.module.18.weight  changing lr from: 0.097499326904094463   to: 0.096583372048411498
i:  21, name:        features.module.18.bias  changing lr from: 0.097617628335395668   to: 0.096732167193998311
i:  22, name:      features.module.19.weight  changing lr from: 0.097731329201898934   to: 0.096875206616936216
i:  23, name:        features.module.19.bias  changing lr from: 0.097840630351207297   to: 0.097012739378815077
i:  24, name:      features.module.22.weight  changing lr from: 0.097945722999451923   to: 0.097145002770515451
i:  25, name:        features.module.22.bias  changing lr from: 0.098046789227747455   to: 0.097272222904386696
i:  26, name:      features.module.23.weight  changing lr from: 0.098144002451829471   to: 0.097394615275763055
i:  27, name:        features.module.23.bias  changing lr from: 0.098237527866336938   to: 0.097512385295357940
i:  28, name:      features.module.25.weight  changing lr from: 0.098327522865128011   to: 0.097625728794012614
i:  29, name:        features.module.25.bias  changing lr from: 0.098414137438945323   to: 0.097734832501212121
i:  30, name:      features.module.26.weight  changing lr from: 0.098497514551677379   to: 0.097839874498717880
i:  31, name:        features.module.26.bias  changing lr from: 0.098577790496396991   to: 0.097941024650605890
i:  32, name:            classifier.0.weight  changing lr from: 0.098655095232293905   to: 0.098038445010938433
i:  33, name:              classifier.0.bias  changing lr from: 0.098729552703558141   to: 0.098132290210238948
i:  34, name:            classifier.3.weight  changing lr from: 0.098801281141213768   to: 0.098222707821883415
i:  35, name:              classifier.3.bias  changing lr from: 0.098870393348846897   to: 0.098309838709465913
i:  36, name:            classifier.6.weight  changing lr from: 0.098936996973120472   to: 0.098393817356144275
i:  37, name:              classifier.6.bias  changing lr from: 0.099001194759918587   to: 0.098474772176919884



# Switched to train mode...
Epoch: [9][  0/391]	Time  0.178 ( 0.178)	Data  0.147 ( 0.147)	Loss 2.2832e+00 (2.2832e+00)	Acc@1  41.41 ( 41.41)	Acc@5  75.00 ( 75.00)
Epoch: [9][ 10/391]	Time  0.029 ( 0.039)	Data  0.000 ( 0.015)	Loss 2.2969e+00 (2.2255e+00)	Acc@1  39.84 ( 40.84)	Acc@5  71.09 ( 73.79)
Epoch: [9][ 20/391]	Time  0.022 ( 0.032)	Data  0.001 ( 0.009)	Loss 2.3594e+00 (2.2909e+00)	Acc@1  40.62 ( 38.99)	Acc@5  71.09 ( 72.21)
Epoch: [9][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.3066e+00 (2.2913e+00)	Acc@1  39.84 ( 39.19)	Acc@5  67.97 ( 72.23)
Epoch: [9][ 40/391]	Time  0.026 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.6504e+00 (2.2847e+00)	Acc@1  28.12 ( 39.01)	Acc@5  64.84 ( 72.16)
Epoch: [9][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.1738e+00 (2.2854e+00)	Acc@1  42.97 ( 38.99)	Acc@5  71.09 ( 71.92)
Epoch: [9][ 60/391]	Time  0.031 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.4062e+00 (2.2759e+00)	Acc@1  38.28 ( 39.19)	Acc@5  69.53 ( 72.32)
Epoch: [9][ 70/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4258e+00 (2.2761e+00)	Acc@1  38.28 ( 39.10)	Acc@5  71.88 ( 72.52)
Epoch: [9][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.3242e+00 (2.2824e+00)	Acc@1  39.06 ( 38.80)	Acc@5  71.88 ( 72.27)
Epoch: [9][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1406e+00 (2.2794e+00)	Acc@1  42.97 ( 38.92)	Acc@5  77.34 ( 72.39)
Epoch: [9][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3320e+00 (2.2811e+00)	Acc@1  38.28 ( 38.98)	Acc@5  72.66 ( 72.39)
Epoch: [9][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2832e+00 (2.2743e+00)	Acc@1  38.28 ( 39.26)	Acc@5  68.75 ( 72.53)
Epoch: [9][120/391]	Time  0.041 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3105e+00 (2.2738e+00)	Acc@1  40.62 ( 39.35)	Acc@5  71.09 ( 72.59)
Epoch: [9][130/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1719e+00 (2.2700e+00)	Acc@1  44.53 ( 39.29)	Acc@5  72.66 ( 72.75)
Epoch: [9][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2168e+00 (2.2787e+00)	Acc@1  42.19 ( 39.23)	Acc@5  72.66 ( 72.54)
Epoch: [9][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2266e+00 (2.2784e+00)	Acc@1  40.62 ( 39.17)	Acc@5  75.00 ( 72.64)
Epoch: [9][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2520e+00 (2.2822e+00)	Acc@1  37.50 ( 39.00)	Acc@5  75.78 ( 72.58)
Epoch: [9][170/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3066e+00 (2.2833e+00)	Acc@1  35.16 ( 38.96)	Acc@5  74.22 ( 72.58)
Epoch: [9][180/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2168e+00 (2.2813e+00)	Acc@1  39.84 ( 38.99)	Acc@5  76.56 ( 72.60)
Epoch: [9][190/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0703e+00 (2.2781e+00)	Acc@1  40.62 ( 39.13)	Acc@5  75.78 ( 72.65)
Epoch: [9][200/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1797e+00 (2.2789e+00)	Acc@1  41.41 ( 39.06)	Acc@5  67.97 ( 72.68)
Epoch: [9][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3516e+00 (2.2826e+00)	Acc@1  34.38 ( 39.07)	Acc@5  75.00 ( 72.55)
Epoch: [9][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3242e+00 (2.2794e+00)	Acc@1  35.94 ( 39.19)	Acc@5  74.22 ( 72.61)
Epoch: [9][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2070e+00 (2.2800e+00)	Acc@1  36.72 ( 39.19)	Acc@5  75.78 ( 72.63)
Epoch: [9][240/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2500e+00 (2.2814e+00)	Acc@1  38.28 ( 39.16)	Acc@5  72.66 ( 72.64)
Epoch: [9][250/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.2285e+00 (2.2801e+00)	Acc@1  41.41 ( 39.28)	Acc@5  75.00 ( 72.63)
Epoch: [9][260/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3965e+00 (2.2829e+00)	Acc@1  32.81 ( 39.25)	Acc@5  69.53 ( 72.61)
Epoch: [9][270/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1230e+00 (2.2789e+00)	Acc@1  42.19 ( 39.29)	Acc@5  77.34 ( 72.70)
Epoch: [9][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3066e+00 (2.2744e+00)	Acc@1  38.28 ( 39.42)	Acc@5  73.44 ( 72.74)
Epoch: [9][290/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1113e+00 (2.2735e+00)	Acc@1  45.31 ( 39.47)	Acc@5  75.78 ( 72.76)
Epoch: [9][300/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9961e+00 (2.2741e+00)	Acc@1  43.75 ( 39.45)	Acc@5  77.34 ( 72.72)
Epoch: [9][310/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3730e+00 (2.2739e+00)	Acc@1  35.94 ( 39.46)	Acc@5  73.44 ( 72.70)
Epoch: [9][320/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2070e+00 (2.2748e+00)	Acc@1  44.53 ( 39.42)	Acc@5  67.97 ( 72.68)
Epoch: [9][330/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4277e+00 (2.2739e+00)	Acc@1  35.16 ( 39.44)	Acc@5  64.84 ( 72.69)
Epoch: [9][340/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.2363e+00 (2.2717e+00)	Acc@1  39.84 ( 39.50)	Acc@5  73.44 ( 72.75)
Epoch: [9][350/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3340e+00 (2.2755e+00)	Acc@1  36.72 ( 39.42)	Acc@5  75.00 ( 72.75)
Epoch: [9][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5762e+00 (2.2738e+00)	Acc@1  29.69 ( 39.43)	Acc@5  63.28 ( 72.72)
Epoch: [9][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4141e+00 (2.2747e+00)	Acc@1  33.59 ( 39.41)	Acc@5  74.22 ( 72.70)
Epoch: [9][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3223e+00 (2.2739e+00)	Acc@1  38.28 ( 39.41)	Acc@5  72.66 ( 72.71)
Epoch: [9][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0488e+00 (2.2749e+00)	Acc@1  42.50 ( 39.38)	Acc@5  80.00 ( 72.71)
## e[9] optimizer.zero_grad (sum) time: 0.11256909370422363
## e[9]       loss.backward (sum) time: 2.2629756927490234
## e[9]      optimizer.step (sum) time: 0.95973801612854
## epoch[9] training(only) time: 10.010704517364502
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 2.2695e+00 (2.2695e+00)	Acc@1  42.00 ( 42.00)	Acc@5  72.00 ( 72.00)
Test: [ 10/100]	Time  0.015 ( 0.025)	Loss 2.3379e+00 (2.2431e+00)	Acc@1  39.00 ( 41.09)	Acc@5  71.00 ( 74.55)
Test: [ 20/100]	Time  0.013 ( 0.019)	Loss 1.9023e+00 (2.2235e+00)	Acc@1  46.00 ( 41.81)	Acc@5  78.00 ( 73.76)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 2.3223e+00 (2.2347e+00)	Acc@1  37.00 ( 41.00)	Acc@5  75.00 ( 73.90)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 2.3613e+00 (2.2321e+00)	Acc@1  37.00 ( 40.68)	Acc@5  70.00 ( 73.59)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 2.0215e+00 (2.2346e+00)	Acc@1  49.00 ( 41.10)	Acc@5  72.00 ( 73.00)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 2.0684e+00 (2.2102e+00)	Acc@1  46.00 ( 41.33)	Acc@5  81.00 ( 73.48)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 2.4844e+00 (2.2102e+00)	Acc@1  38.00 ( 41.27)	Acc@5  67.00 ( 73.52)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 2.3633e+00 (2.2255e+00)	Acc@1  42.00 ( 40.94)	Acc@5  66.00 ( 73.21)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.3652e+00 (2.2244e+00)	Acc@1  39.00 ( 40.88)	Acc@5  71.00 ( 73.26)
 * Acc@1 41.020 Acc@5 73.330
### epoch[9] execution time: 11.591291189193726
EPOCH 10
i:   0, name:       features.module.0.weight  changing lr from: 0.091879240657579200   to: 0.089822950858548800
i:   1, name:         features.module.0.bias  changing lr from: 0.092220437113153961   to: 0.090237972711850828
i:   2, name:       features.module.1.weight  changing lr from: 0.092547166171389833   to: 0.090635630161450875
i:   3, name:         features.module.1.bias  changing lr from: 0.092860099703250584   to: 0.091016710437507239
i:   4, name:       features.module.4.weight  changing lr from: 0.093159876749649095   to: 0.091381964034259647
i:   5, name:         features.module.4.bias  changing lr from: 0.093447105104434533   to: 0.091732106306609246
i:   6, name:       features.module.5.weight  changing lr from: 0.093722362833513415   to: 0.092067819022987940
i:   7, name:         features.module.5.bias  changing lr from: 0.093986199730475645   to: 0.092389751871405260
i:   8, name:       features.module.8.weight  changing lr from: 0.094239138709511103   to: 0.092698523916446202
i:   9, name:         features.module.8.bias  changing lr from: 0.094481677136735984   to: 0.092994725005735801
i:  10, name:       features.module.9.weight  changing lr from: 0.094714288101311203   to: 0.093278917125009686
i:  11, name:         features.module.9.bias  changing lr from: 0.094937421627939528   to: 0.093551635701447672
i:  12, name:      features.module.11.weight  changing lr from: 0.095151505832483135   to: 0.093813390855356629
i:  13, name:        features.module.11.bias  changing lr from: 0.095356948022555926   to: 0.094064668600640805
i:  14, name:      features.module.12.weight  changing lr from: 0.095554135745023250   to: 0.094305931994784972
i:  15, name:        features.module.12.bias  changing lr from: 0.095743437782390098   to: 0.094537622239305535
i:  16, name:      features.module.15.weight  changing lr from: 0.095925205100083302   to: 0.094760159731808097
i:  17, name:        features.module.15.bias  changing lr from: 0.096099771746638349   to: 0.094973945070931598
i:  18, name:      features.module.16.weight  changing lr from: 0.096267455708788918   to: 0.095179360015567108
i:  19, name:        features.module.16.bias  changing lr from: 0.096428559723432741   to: 0.095376768399817347
i:  20, name:      features.module.18.weight  changing lr from: 0.096583372048411498   to: 0.095566517005217691
i:  21, name:        features.module.18.bias  changing lr from: 0.096732167193998311   to: 0.095748936391772010
i:  22, name:      features.module.19.weight  changing lr from: 0.096875206616936216   to: 0.095924341689373582
i:  23, name:        features.module.19.bias  changing lr from: 0.097012739378815077   to: 0.096093033351183091
i:  24, name:      features.module.22.weight  changing lr from: 0.097145002770515451   to: 0.096255297870525863
i:  25, name:        features.module.22.bias  changing lr from: 0.097272222904386696   to: 0.096411408462851955
i:  26, name:      features.module.23.weight  changing lr from: 0.097394615275763055   to: 0.096561625714274832
i:  27, name:        features.module.23.bias  changing lr from: 0.097512385295357940   to: 0.096706198198172180
i:  28, name:      features.module.25.weight  changing lr from: 0.097625728794012614   to: 0.096845363061293863
i:  29, name:        features.module.25.bias  changing lr from: 0.097734832501212121   to: 0.096979346580780848
i:  30, name:      features.module.26.weight  changing lr from: 0.097839874498717880   to: 0.097108364693454532
i:  31, name:        features.module.26.bias  changing lr from: 0.097941024650605890   to: 0.097232623498689552
i:  32, name:            classifier.0.weight  changing lr from: 0.098038445010938433   to: 0.097352319736136061
i:  33, name:              classifier.0.bias  changing lr from: 0.098132290210238948   to: 0.097467641239508809
i:  34, name:            classifier.3.weight  changing lr from: 0.098222707821883415   to: 0.097578767367612840
i:  35, name:              classifier.3.bias  changing lr from: 0.098309838709465913   to: 0.097685869413726520
i:  36, name:            classifier.6.weight  changing lr from: 0.098393817356144275   to: 0.097789110994415784
i:  37, name:              classifier.6.bias  changing lr from: 0.098474772176919884   to: 0.097888648418806498



# Switched to train mode...
Epoch: [10][  0/391]	Time  0.175 ( 0.175)	Data  0.146 ( 0.146)	Loss 2.0508e+00 (2.0508e+00)	Acc@1  43.75 ( 43.75)	Acc@5  75.78 ( 75.78)
Epoch: [10][ 10/391]	Time  0.035 ( 0.038)	Data  0.005 ( 0.015)	Loss 2.1523e+00 (2.2073e+00)	Acc@1  42.97 ( 40.13)	Acc@5  82.03 ( 74.43)
Epoch: [10][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 2.7344e+00 (2.1957e+00)	Acc@1  31.25 ( 41.03)	Acc@5  69.53 ( 75.15)
Epoch: [10][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.9424e+00 (2.1803e+00)	Acc@1  44.53 ( 41.08)	Acc@5  82.81 ( 75.13)
Epoch: [10][ 40/391]	Time  0.031 ( 0.028)	Data  0.003 ( 0.006)	Loss 1.9785e+00 (2.1779e+00)	Acc@1  41.41 ( 40.95)	Acc@5  77.34 ( 75.25)
Epoch: [10][ 50/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.5762e+00 (2.1711e+00)	Acc@1  30.47 ( 41.15)	Acc@5  67.19 ( 75.32)
Epoch: [10][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9473e+00 (2.1674e+00)	Acc@1  47.66 ( 41.51)	Acc@5  80.47 ( 75.37)
Epoch: [10][ 70/391]	Time  0.033 ( 0.027)	Data  0.003 ( 0.004)	Loss 2.4219e+00 (2.1763e+00)	Acc@1  38.28 ( 41.53)	Acc@5  71.09 ( 75.23)
Epoch: [10][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0352e+00 (2.1675e+00)	Acc@1  43.75 ( 41.62)	Acc@5  75.78 ( 75.28)
Epoch: [10][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3184e+00 (2.1703e+00)	Acc@1  35.94 ( 41.62)	Acc@5  69.53 ( 75.16)
Epoch: [10][100/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1621e+00 (2.1719e+00)	Acc@1  40.62 ( 41.55)	Acc@5  72.66 ( 75.12)
Epoch: [10][110/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0703e+00 (2.1736e+00)	Acc@1  42.19 ( 41.55)	Acc@5  78.91 ( 75.06)
Epoch: [10][120/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.0449e+00 (2.1764e+00)	Acc@1  49.22 ( 41.59)	Acc@5  74.22 ( 74.95)
Epoch: [10][130/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8984e+00 (2.1783e+00)	Acc@1  46.88 ( 41.57)	Acc@5  81.25 ( 74.99)
Epoch: [10][140/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3906e+00 (2.1815e+00)	Acc@1  40.62 ( 41.53)	Acc@5  69.53 ( 74.96)
Epoch: [10][150/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.2227e+00 (2.1772e+00)	Acc@1  41.41 ( 41.58)	Acc@5  72.66 ( 74.99)
Epoch: [10][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2402e+00 (2.1717e+00)	Acc@1  40.62 ( 41.85)	Acc@5  71.09 ( 75.03)
Epoch: [10][170/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1484e+00 (2.1764e+00)	Acc@1  46.09 ( 41.79)	Acc@5  75.78 ( 74.89)
Epoch: [10][180/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1641e+00 (2.1750e+00)	Acc@1  46.09 ( 41.83)	Acc@5  74.22 ( 74.91)
Epoch: [10][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4316e+00 (2.1787e+00)	Acc@1  41.41 ( 41.71)	Acc@5  68.75 ( 74.84)
Epoch: [10][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2598e+00 (2.1781e+00)	Acc@1  41.41 ( 41.74)	Acc@5  75.78 ( 74.84)
Epoch: [10][210/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.3750e+00 (2.1801e+00)	Acc@1  35.94 ( 41.66)	Acc@5  71.09 ( 74.81)
Epoch: [10][220/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2031e+00 (2.1826e+00)	Acc@1  35.94 ( 41.62)	Acc@5  77.34 ( 74.79)
Epoch: [10][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1582e+00 (2.1841e+00)	Acc@1  42.97 ( 41.58)	Acc@5  75.78 ( 74.75)
Epoch: [10][240/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1016e+00 (2.1846e+00)	Acc@1  48.44 ( 41.61)	Acc@5  71.88 ( 74.73)
Epoch: [10][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0352e+00 (2.1836e+00)	Acc@1  42.97 ( 41.59)	Acc@5  78.12 ( 74.72)
Epoch: [10][260/391]	Time  0.038 ( 0.026)	Data  0.004 ( 0.002)	Loss 2.0293e+00 (2.1806e+00)	Acc@1  44.53 ( 41.62)	Acc@5  78.91 ( 74.80)
Epoch: [10][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1230e+00 (2.1799e+00)	Acc@1  43.75 ( 41.59)	Acc@5  82.03 ( 74.84)
Epoch: [10][280/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1797e+00 (2.1772e+00)	Acc@1  42.97 ( 41.61)	Acc@5  78.12 ( 74.91)
Epoch: [10][290/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.3613e+00 (2.1769e+00)	Acc@1  36.72 ( 41.61)	Acc@5  72.66 ( 74.92)
Epoch: [10][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1465e+00 (2.1781e+00)	Acc@1  44.53 ( 41.61)	Acc@5  71.88 ( 74.90)
Epoch: [10][310/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4883e+00 (2.1797e+00)	Acc@1  45.31 ( 41.61)	Acc@5  67.97 ( 74.86)
Epoch: [10][320/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1758e+00 (2.1793e+00)	Acc@1  37.50 ( 41.60)	Acc@5  75.00 ( 74.86)
Epoch: [10][330/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1641e+00 (2.1740e+00)	Acc@1  46.09 ( 41.74)	Acc@5  76.56 ( 74.97)
Epoch: [10][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9932e+00 (2.1726e+00)	Acc@1  39.84 ( 41.75)	Acc@5  79.69 ( 74.98)
Epoch: [10][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0469e+00 (2.1724e+00)	Acc@1  46.09 ( 41.79)	Acc@5  78.91 ( 74.96)
Epoch: [10][360/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1855e+00 (2.1722e+00)	Acc@1  40.62 ( 41.79)	Acc@5  75.78 ( 74.97)
Epoch: [10][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0703e+00 (2.1719e+00)	Acc@1  48.44 ( 41.78)	Acc@5  78.91 ( 74.97)
Epoch: [10][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9824e+00 (2.1720e+00)	Acc@1  45.31 ( 41.81)	Acc@5  75.78 ( 74.93)
Epoch: [10][390/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2305e+00 (2.1722e+00)	Acc@1  41.25 ( 41.82)	Acc@5  73.75 ( 74.92)
## e[10] optimizer.zero_grad (sum) time: 0.11379551887512207
## e[10]       loss.backward (sum) time: 2.2412149906158447
## e[10]      optimizer.step (sum) time: 0.9461781978607178
## epoch[10] training(only) time: 10.033576011657715
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 2.0820e+00 (2.0820e+00)	Acc@1  44.00 ( 44.00)	Acc@5  74.00 ( 74.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 2.3359e+00 (2.1727e+00)	Acc@1  40.00 ( 43.00)	Acc@5  78.00 ( 75.36)
Test: [ 20/100]	Time  0.020 ( 0.020)	Loss 2.0391e+00 (2.1875e+00)	Acc@1  49.00 ( 42.48)	Acc@5  74.00 ( 74.57)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 2.1719e+00 (2.1759e+00)	Acc@1  46.00 ( 41.87)	Acc@5  71.00 ( 74.77)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 2.2734e+00 (2.1774e+00)	Acc@1  42.00 ( 41.66)	Acc@5  72.00 ( 75.02)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 2.0215e+00 (2.1824e+00)	Acc@1  48.00 ( 41.98)	Acc@5  78.00 ( 74.69)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 2.0039e+00 (2.1627e+00)	Acc@1  46.00 ( 42.33)	Acc@5  80.00 ( 75.07)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 2.3516e+00 (2.1688e+00)	Acc@1  41.00 ( 42.24)	Acc@5  74.00 ( 75.25)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 2.4629e+00 (2.1848e+00)	Acc@1  40.00 ( 42.20)	Acc@5  67.00 ( 74.81)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.3398e+00 (2.1829e+00)	Acc@1  39.00 ( 42.41)	Acc@5  72.00 ( 74.88)
 * Acc@1 42.210 Acc@5 74.930
### epoch[10] execution time: 11.587471961975098
EPOCH 11
i:   0, name:       features.module.0.weight  changing lr from: 0.089822950858548800   to: 0.087586278261115785
i:   1, name:         features.module.0.bias  changing lr from: 0.090237972711850828   to: 0.088080228226809776
i:   2, name:       features.module.1.weight  changing lr from: 0.090635630161450875   to: 0.088553822543011390
i:   3, name:         features.module.1.bias  changing lr from: 0.091016710437507239   to: 0.089007958333940540
i:   4, name:       features.module.4.weight  changing lr from: 0.091381964034259647   to: 0.089443493214665395
i:   5, name:         features.module.4.bias  changing lr from: 0.091732106306609246   to: 0.089861246762037161
i:   6, name:       features.module.5.weight  changing lr from: 0.092067819022987940   to: 0.090262001977362671
i:   7, name:         features.module.5.bias  changing lr from: 0.092389751871405260   to: 0.090646506732607018
i:   8, name:       features.module.8.weight  changing lr from: 0.092698523916446202   to: 0.091015475193436876
i:   9, name:         features.module.8.bias  changing lr from: 0.092994725005735801   to: 0.091369589213719488
i:  10, name:       features.module.9.weight  changing lr from: 0.093278917125009686   to: 0.091709499697208860
i:  11, name:         features.module.9.bias  changing lr from: 0.093551635701447672   to: 0.092035827923103566
i:  12, name:      features.module.11.weight  changing lr from: 0.093813390855356629   to: 0.092349166832973506
i:  13, name:        features.module.11.bias  changing lr from: 0.094064668600640805   to: 0.092650082277241308
i:  14, name:      features.module.12.weight  changing lr from: 0.094305931994784972   to: 0.092939114219988225
i:  15, name:        features.module.12.bias  changing lr from: 0.094537622239305535   to: 0.093216777901344305
i:  16, name:      features.module.15.weight  changing lr from: 0.094760159731808097   to: 0.093483564957135015
i:  17, name:        features.module.15.bias  changing lr from: 0.094973945070931598   to: 0.093739944495798436
i:  18, name:      features.module.16.weight  changing lr from: 0.095179360015567108   to: 0.093986364132872049
i:  19, name:        features.module.16.bias  changing lr from: 0.095376768399817347   to: 0.094223250983579226
i:  20, name:      features.module.18.weight  changing lr from: 0.095566517005217691   to: 0.094451012614236357
i:  21, name:        features.module.18.bias  changing lr from: 0.095748936391772010   to: 0.094670037953351702
i:  22, name:      features.module.19.weight  changing lr from: 0.095924341689373582   to: 0.094880698163407051
i:  23, name:        features.module.19.bias  changing lr from: 0.096093033351183091   to: 0.095083347474405833
i:  24, name:      features.module.22.weight  changing lr from: 0.096255297870525863   to: 0.095278323980339294
i:  25, name:        features.module.22.bias  changing lr from: 0.096411408462851955   to: 0.095465950399772445
i:  26, name:      features.module.23.weight  changing lr from: 0.096561625714274832   to: 0.095646534801783478
i:  27, name:        features.module.23.bias  changing lr from: 0.096706198198172180   to: 0.095820371298508844
i:  28, name:      features.module.25.weight  changing lr from: 0.096845363061293863   to: 0.095987740705553742
i:  29, name:        features.module.25.bias  changing lr from: 0.096979346580780848   to: 0.096148911171523480
i:  30, name:      features.module.26.weight  changing lr from: 0.097108364693454532   to: 0.096304138777922044
i:  31, name:        features.module.26.bias  changing lr from: 0.097232623498689552   to: 0.096453668110644944
i:  32, name:            classifier.0.weight  changing lr from: 0.097352319736136061   to: 0.096597732804271932
i:  33, name:              classifier.0.bias  changing lr from: 0.097467641239508809   to: 0.096736556060337320
i:  34, name:            classifier.3.weight  changing lr from: 0.097578767367612840   to: 0.096870351140725647
i:  35, name:              classifier.3.bias  changing lr from: 0.097685869413726520   to: 0.096999321837307584
i:  36, name:            classifier.6.weight  changing lr from: 0.097789110994415784   to: 0.097123662918895681
i:  37, name:              classifier.6.bias  changing lr from: 0.097888648418806498   to: 0.097243560556564093



# Switched to train mode...
Epoch: [11][  0/391]	Time  0.172 ( 0.172)	Data  0.144 ( 0.144)	Loss 1.9824e+00 (1.9824e+00)	Acc@1  42.97 ( 42.97)	Acc@5  78.12 ( 78.12)
Epoch: [11][ 10/391]	Time  0.029 ( 0.039)	Data  0.001 ( 0.015)	Loss 2.0430e+00 (2.1206e+00)	Acc@1  44.53 ( 42.40)	Acc@5  83.59 ( 76.63)
Epoch: [11][ 20/391]	Time  0.025 ( 0.032)	Data  0.001 ( 0.009)	Loss 2.0547e+00 (2.0863e+00)	Acc@1  42.97 ( 43.08)	Acc@5  75.78 ( 76.79)
Epoch: [11][ 30/391]	Time  0.043 ( 0.030)	Data  0.003 ( 0.006)	Loss 2.1582e+00 (2.0955e+00)	Acc@1  42.97 ( 42.82)	Acc@5  78.12 ( 76.64)
Epoch: [11][ 40/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.0820e+00 (2.0893e+00)	Acc@1  46.88 ( 43.33)	Acc@5  76.56 ( 76.49)
Epoch: [11][ 50/391]	Time  0.036 ( 0.028)	Data  0.003 ( 0.005)	Loss 1.9150e+00 (2.0850e+00)	Acc@1  42.97 ( 43.46)	Acc@5  84.38 ( 76.75)
Epoch: [11][ 60/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.2637e+00 (2.0882e+00)	Acc@1  42.97 ( 43.55)	Acc@5  78.12 ( 76.65)
Epoch: [11][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0996e+00 (2.0837e+00)	Acc@1  46.09 ( 43.81)	Acc@5  78.12 ( 76.87)
Epoch: [11][ 80/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.004)	Loss 2.1152e+00 (2.0816e+00)	Acc@1  39.06 ( 43.82)	Acc@5  71.09 ( 76.98)
Epoch: [11][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7471e+00 (2.0796e+00)	Acc@1  51.56 ( 43.98)	Acc@5  84.38 ( 76.96)
Epoch: [11][100/391]	Time  0.037 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.9502e+00 (2.0642e+00)	Acc@1  43.75 ( 44.31)	Acc@5  80.47 ( 77.22)
Epoch: [11][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0430e+00 (2.0669e+00)	Acc@1  45.31 ( 44.34)	Acc@5  79.69 ( 77.22)
Epoch: [11][120/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.0996e+00 (2.0660e+00)	Acc@1  47.66 ( 44.44)	Acc@5  75.00 ( 77.18)
Epoch: [11][130/391]	Time  0.034 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.4023e+00 (2.0672e+00)	Acc@1  37.50 ( 44.47)	Acc@5  71.09 ( 77.19)
Epoch: [11][140/391]	Time  0.036 ( 0.026)	Data  0.008 ( 0.003)	Loss 1.7021e+00 (2.0636e+00)	Acc@1  54.69 ( 44.60)	Acc@5  83.59 ( 77.31)
Epoch: [11][150/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0664e+00 (2.0653e+00)	Acc@1  38.28 ( 44.48)	Acc@5  79.69 ( 77.34)
Epoch: [11][160/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.1992e+00 (2.0629e+00)	Acc@1  40.62 ( 44.42)	Acc@5  72.66 ( 77.32)
Epoch: [11][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5488e+00 (2.0659e+00)	Acc@1  34.38 ( 44.23)	Acc@5  67.19 ( 77.26)
Epoch: [11][180/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9854e+00 (2.0706e+00)	Acc@1  42.97 ( 44.13)	Acc@5  78.12 ( 77.16)
Epoch: [11][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9385e+00 (2.0650e+00)	Acc@1  50.00 ( 44.20)	Acc@5  79.69 ( 77.27)
Epoch: [11][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0762e+00 (2.0686e+00)	Acc@1  45.31 ( 44.15)	Acc@5  82.03 ( 77.19)
Epoch: [11][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1738e+00 (2.0661e+00)	Acc@1  39.84 ( 44.22)	Acc@5  75.00 ( 77.23)
Epoch: [11][220/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3164e+00 (2.0718e+00)	Acc@1  43.75 ( 44.09)	Acc@5  68.75 ( 77.07)
Epoch: [11][230/391]	Time  0.028 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.9961e+00 (2.0730e+00)	Acc@1  42.97 ( 44.09)	Acc@5  77.34 ( 77.01)
Epoch: [11][240/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4434e+00 (2.0745e+00)	Acc@1  36.72 ( 44.04)	Acc@5  71.88 ( 77.03)
Epoch: [11][250/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7891e+00 (2.0757e+00)	Acc@1  51.56 ( 43.98)	Acc@5  80.47 ( 76.97)
Epoch: [11][260/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.1094e+00 (2.0775e+00)	Acc@1  42.19 ( 43.88)	Acc@5  76.56 ( 76.95)
Epoch: [11][270/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1230e+00 (2.0807e+00)	Acc@1  45.31 ( 43.80)	Acc@5  74.22 ( 76.87)
Epoch: [11][280/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7393e+00 (2.0797e+00)	Acc@1  52.34 ( 43.83)	Acc@5  83.59 ( 76.88)
Epoch: [11][290/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9834e+00 (2.0780e+00)	Acc@1  50.78 ( 43.90)	Acc@5  75.78 ( 76.86)
Epoch: [11][300/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7686e+00 (2.0743e+00)	Acc@1  50.78 ( 44.02)	Acc@5  82.81 ( 76.87)
Epoch: [11][310/391]	Time  0.038 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.9854e+00 (2.0732e+00)	Acc@1  47.66 ( 44.11)	Acc@5  77.34 ( 76.88)
Epoch: [11][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0684e+00 (2.0736e+00)	Acc@1  43.75 ( 44.11)	Acc@5  74.22 ( 76.86)
Epoch: [11][330/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.1660e+00 (2.0737e+00)	Acc@1  43.75 ( 44.11)	Acc@5  67.97 ( 76.85)
Epoch: [11][340/391]	Time  0.023 ( 0.025)	Data  0.003 ( 0.002)	Loss 2.0195e+00 (2.0772e+00)	Acc@1  42.97 ( 44.02)	Acc@5  82.81 ( 76.79)
Epoch: [11][350/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2305e+00 (2.0802e+00)	Acc@1  40.62 ( 43.98)	Acc@5  71.88 ( 76.72)
Epoch: [11][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9668e+00 (2.0794e+00)	Acc@1  45.31 ( 43.99)	Acc@5  78.12 ( 76.74)
Epoch: [11][370/391]	Time  0.045 ( 0.025)	Data  0.010 ( 0.002)	Loss 2.2070e+00 (2.0786e+00)	Acc@1  42.97 ( 44.01)	Acc@5  75.78 ( 76.76)
Epoch: [11][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9131e+00 (2.0802e+00)	Acc@1  45.31 ( 44.01)	Acc@5  75.00 ( 76.73)
Epoch: [11][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3770e+00 (2.0791e+00)	Acc@1  38.75 ( 44.04)	Acc@5  77.50 ( 76.74)
## e[11] optimizer.zero_grad (sum) time: 0.11409187316894531
## e[11]       loss.backward (sum) time: 2.2808761596679688
## e[11]      optimizer.step (sum) time: 0.9685239791870117
## epoch[11] training(only) time: 9.972378730773926
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 2.2227e+00 (2.2227e+00)	Acc@1  41.00 ( 41.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.013 ( 0.025)	Loss 2.1758e+00 (2.0975e+00)	Acc@1  46.00 ( 44.18)	Acc@5  73.00 ( 76.55)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.8770e+00 (2.1185e+00)	Acc@1  48.00 ( 43.67)	Acc@5  79.00 ( 76.33)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 2.2539e+00 (2.1282e+00)	Acc@1  38.00 ( 43.16)	Acc@5  71.00 ( 75.94)
Test: [ 40/100]	Time  0.011 ( 0.016)	Loss 2.2070e+00 (2.1509e+00)	Acc@1  39.00 ( 42.88)	Acc@5  75.00 ( 75.44)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 2.0137e+00 (2.1609e+00)	Acc@1  51.00 ( 43.22)	Acc@5  79.00 ( 74.84)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 2.1465e+00 (2.1565e+00)	Acc@1  47.00 ( 43.03)	Acc@5  74.00 ( 75.07)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 2.4414e+00 (2.1661e+00)	Acc@1  40.00 ( 43.03)	Acc@5  71.00 ( 74.79)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 2.3691e+00 (2.1779e+00)	Acc@1  39.00 ( 42.58)	Acc@5  65.00 ( 74.36)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 2.1641e+00 (2.1720e+00)	Acc@1  47.00 ( 42.79)	Acc@5  74.00 ( 74.54)
 * Acc@1 42.760 Acc@5 74.570
### epoch[11] execution time: 11.49252700805664
EPOCH 12
i:   0, name:       features.module.0.weight  changing lr from: 0.087586278261115785   to: 0.085179615101143580
i:   1, name:         features.module.0.bias  changing lr from: 0.088080228226809776   to: 0.085756842525889762
i:   2, name:       features.module.1.weight  changing lr from: 0.088553822543011390   to: 0.086310686600947567
i:   3, name:         features.module.1.bias  changing lr from: 0.089007958333940540   to: 0.086842144248485353
i:   4, name:       features.module.4.weight  changing lr from: 0.089443493214665395   to: 0.087352171622772923
i:   5, name:         features.module.4.bias  changing lr from: 0.089861246762037161   to: 0.087841685284541421
i:   6, name:       features.module.5.weight  changing lr from: 0.090262001977362671   to: 0.088311563420371400
i:   7, name:         features.module.5.bias  changing lr from: 0.090646506732607018   to: 0.088762647092007915
i:   8, name:       features.module.8.weight  changing lr from: 0.091015475193436876   to: 0.089195741502829717
i:   9, name:         features.module.8.bias  changing lr from: 0.091369589213719488   to: 0.089611617270722116
i:  10, name:       features.module.9.weight  changing lr from: 0.091709499697208860   to: 0.090011011698357485
i:  11, name:         features.module.9.bias  changing lr from: 0.092035827923103566   to: 0.090394630033406587
i:  12, name:      features.module.11.weight  changing lr from: 0.092349166832973506   to: 0.090763146712517229
i:  13, name:        features.module.11.bias  changing lr from: 0.092650082277241308   to: 0.091117206584030552
i:  14, name:      features.module.12.weight  changing lr from: 0.092939114219988225   to: 0.091457426105380976
i:  15, name:        features.module.12.bias  changing lr from: 0.093216777901344305   to: 0.091784394511965073
i:  16, name:      features.module.15.weight  changing lr from: 0.093483564957135015   to: 0.092098674954983806
i:  17, name:        features.module.15.bias  changing lr from: 0.093739944495798436   to: 0.092400805606376632
i:  18, name:      features.module.16.weight  changing lr from: 0.093986364132872049   to: 0.092691300729490486
i:  19, name:        features.module.16.bias  changing lr from: 0.094223250983579226   to: 0.092970651714570807
i:  20, name:      features.module.18.weight  changing lr from: 0.094451012614236357   to: 0.093239328078537853
i:  21, name:        features.module.18.bias  changing lr from: 0.094670037953351702   to: 0.093497778428828143
i:  22, name:      features.module.19.weight  changing lr from: 0.094880698163407051   to: 0.093746431391345184
i:  23, name:        features.module.19.bias  changing lr from: 0.095083347474405833   to: 0.093985696502783989
i:  24, name:      features.module.22.weight  changing lr from: 0.095278323980339294   to: 0.094215965067775131
i:  25, name:        features.module.22.bias  changing lr from: 0.095465950399772445   to: 0.094437610981441794
i:  26, name:      features.module.23.weight  changing lr from: 0.095646534801783478   to: 0.094650991518083455
i:  27, name:        features.module.23.bias  changing lr from: 0.095820371298508844   to: 0.094856448086793763
i:  28, name:      features.module.25.weight  changing lr from: 0.095987740705553742   to: 0.095054306954894016
i:  29, name:        features.module.25.bias  changing lr from: 0.096148911171523480   to: 0.095244879940119834
i:  30, name:      features.module.26.weight  changing lr from: 0.096304138777922044   to: 0.095428465072537841
i:  31, name:        features.module.26.bias  changing lr from: 0.096453668110644944   to: 0.095605347227197171
i:  32, name:            classifier.0.weight  changing lr from: 0.096597732804271932   to: 0.095775798728536504
i:  33, name:              classifier.0.bias  changing lr from: 0.096736556060337320   to: 0.095940079927573974
i:  34, name:            classifier.3.weight  changing lr from: 0.096870351140725647   to: 0.096098439752906981
i:  35, name:              classifier.3.bias  changing lr from: 0.096999321837307584   to: 0.096251116236540993
i:  36, name:            classifier.6.weight  changing lr from: 0.097123662918895681   to: 0.096398337015554114
i:  37, name:              classifier.6.bias  changing lr from: 0.097243560556564093   to: 0.096540319810587472



# Switched to train mode...
Epoch: [12][  0/391]	Time  0.175 ( 0.175)	Data  0.140 ( 0.140)	Loss 2.1699e+00 (2.1699e+00)	Acc@1  45.31 ( 45.31)	Acc@5  71.09 ( 71.09)
Epoch: [12][ 10/391]	Time  0.032 ( 0.039)	Data  0.003 ( 0.014)	Loss 1.9141e+00 (1.8988e+00)	Acc@1  43.75 ( 47.30)	Acc@5  81.25 ( 78.55)
Epoch: [12][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 2.0879e+00 (1.9022e+00)	Acc@1  40.62 ( 47.73)	Acc@5  76.56 ( 79.28)
Epoch: [12][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.0605e+00 (1.9127e+00)	Acc@1  45.31 ( 48.44)	Acc@5  74.22 ( 79.26)
Epoch: [12][ 40/391]	Time  0.040 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.9502e+00 (1.9490e+00)	Acc@1  46.88 ( 47.45)	Acc@5  81.25 ( 78.75)
Epoch: [12][ 50/391]	Time  0.030 ( 0.028)	Data  0.000 ( 0.004)	Loss 1.9990e+00 (1.9498e+00)	Acc@1  39.84 ( 47.10)	Acc@5  78.12 ( 79.07)
Epoch: [12][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0039e+00 (1.9432e+00)	Acc@1  53.12 ( 47.16)	Acc@5  79.69 ( 79.02)
Epoch: [12][ 70/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.6729e+00 (1.9569e+00)	Acc@1  57.81 ( 47.03)	Acc@5  87.50 ( 78.88)
Epoch: [12][ 80/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.2363e+00 (1.9707e+00)	Acc@1  41.41 ( 46.66)	Acc@5  75.00 ( 78.51)
Epoch: [12][ 90/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9111e+00 (1.9682e+00)	Acc@1  43.75 ( 46.88)	Acc@5  80.47 ( 78.49)
Epoch: [12][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9092e+00 (1.9714e+00)	Acc@1  50.78 ( 46.80)	Acc@5  77.34 ( 78.49)
Epoch: [12][110/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9746e+00 (1.9710e+00)	Acc@1  50.00 ( 46.80)	Acc@5  77.34 ( 78.47)
Epoch: [12][120/391]	Time  0.026 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.0879e+00 (1.9704e+00)	Acc@1  47.66 ( 46.85)	Acc@5  75.78 ( 78.44)
Epoch: [12][130/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8184e+00 (1.9752e+00)	Acc@1  53.12 ( 46.60)	Acc@5  84.38 ( 78.45)
Epoch: [12][140/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0273e+00 (1.9782e+00)	Acc@1  39.84 ( 46.53)	Acc@5  78.12 ( 78.40)
Epoch: [12][150/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9209e+00 (1.9709e+00)	Acc@1  50.78 ( 46.70)	Acc@5  78.12 ( 78.54)
Epoch: [12][160/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9570e+00 (1.9706e+00)	Acc@1  49.22 ( 46.71)	Acc@5  82.81 ( 78.50)
Epoch: [12][170/391]	Time  0.023 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.0781e+00 (1.9749e+00)	Acc@1  47.66 ( 46.68)	Acc@5  78.12 ( 78.54)
Epoch: [12][180/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9658e+00 (1.9809e+00)	Acc@1  44.53 ( 46.45)	Acc@5  78.12 ( 78.43)
Epoch: [12][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9004e+00 (1.9843e+00)	Acc@1  44.53 ( 46.33)	Acc@5  79.69 ( 78.35)
Epoch: [12][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0371e+00 (1.9849e+00)	Acc@1  51.56 ( 46.42)	Acc@5  75.00 ( 78.37)
Epoch: [12][210/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0195e+00 (1.9860e+00)	Acc@1  49.22 ( 46.42)	Acc@5  76.56 ( 78.31)
Epoch: [12][220/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.1875e+00 (1.9907e+00)	Acc@1  40.62 ( 46.28)	Acc@5  75.78 ( 78.25)
Epoch: [12][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9551e+00 (1.9890e+00)	Acc@1  42.97 ( 46.28)	Acc@5  82.81 ( 78.33)
Epoch: [12][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0371e+00 (1.9880e+00)	Acc@1  49.22 ( 46.31)	Acc@5  77.34 ( 78.31)
Epoch: [12][250/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3984e+00 (1.9878e+00)	Acc@1  41.41 ( 46.36)	Acc@5  64.84 ( 78.27)
Epoch: [12][260/391]	Time  0.029 ( 0.026)	Data  0.005 ( 0.002)	Loss 2.0312e+00 (1.9888e+00)	Acc@1  44.53 ( 46.38)	Acc@5  75.78 ( 78.21)
Epoch: [12][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3438e+00 (1.9883e+00)	Acc@1  42.97 ( 46.44)	Acc@5  75.00 ( 78.18)
Epoch: [12][280/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9912e+00 (1.9899e+00)	Acc@1  48.44 ( 46.40)	Acc@5  78.12 ( 78.16)
Epoch: [12][290/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0137e+00 (1.9897e+00)	Acc@1  46.88 ( 46.34)	Acc@5  75.00 ( 78.14)
Epoch: [12][300/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.1406e+00 (1.9921e+00)	Acc@1  39.84 ( 46.26)	Acc@5  75.78 ( 78.12)
Epoch: [12][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2598e+00 (1.9926e+00)	Acc@1  40.62 ( 46.28)	Acc@5  69.53 ( 78.11)
Epoch: [12][320/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1367e+00 (1.9935e+00)	Acc@1  44.53 ( 46.28)	Acc@5  73.44 ( 78.08)
Epoch: [12][330/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2871e+00 (1.9951e+00)	Acc@1  42.97 ( 46.26)	Acc@5  72.66 ( 78.04)
Epoch: [12][340/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1855e+00 (1.9952e+00)	Acc@1  39.06 ( 46.27)	Acc@5  80.47 ( 78.03)
Epoch: [12][350/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0703e+00 (1.9946e+00)	Acc@1  48.44 ( 46.28)	Acc@5  74.22 ( 78.05)
Epoch: [12][360/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.7988e+00 (1.9926e+00)	Acc@1  49.22 ( 46.33)	Acc@5  81.25 ( 78.11)
Epoch: [12][370/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.3223e+00 (1.9927e+00)	Acc@1  35.94 ( 46.26)	Acc@5  71.09 ( 78.11)
Epoch: [12][380/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6602e+00 (1.9927e+00)	Acc@1  53.12 ( 46.22)	Acc@5  80.47 ( 78.11)
Epoch: [12][390/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2344e+00 (1.9953e+00)	Acc@1  40.00 ( 46.16)	Acc@5  73.75 ( 78.08)
## e[12] optimizer.zero_grad (sum) time: 0.1128702163696289
## e[12]       loss.backward (sum) time: 2.2465660572052
## e[12]      optimizer.step (sum) time: 0.9650356769561768
## epoch[12] training(only) time: 10.000059127807617
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 2.1660e+00 (2.1660e+00)	Acc@1  44.00 ( 44.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 2.2305e+00 (2.0746e+00)	Acc@1  38.00 ( 45.27)	Acc@5  76.00 ( 76.45)
Test: [ 20/100]	Time  0.009 ( 0.019)	Loss 2.0742e+00 (2.1070e+00)	Acc@1  47.00 ( 45.05)	Acc@5  76.00 ( 75.57)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 2.1562e+00 (2.1053e+00)	Acc@1  45.00 ( 45.29)	Acc@5  78.00 ( 75.90)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 2.0957e+00 (2.1073e+00)	Acc@1  44.00 ( 45.17)	Acc@5  78.00 ( 76.00)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 2.0371e+00 (2.1270e+00)	Acc@1  50.00 ( 44.88)	Acc@5  69.00 ( 75.18)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 2.2344e+00 (2.1164e+00)	Acc@1  43.00 ( 44.82)	Acc@5  72.00 ( 75.25)
Test: [ 70/100]	Time  0.014 ( 0.015)	Loss 2.1367e+00 (2.1154e+00)	Acc@1  43.00 ( 44.92)	Acc@5  76.00 ( 75.45)
Test: [ 80/100]	Time  0.020 ( 0.015)	Loss 2.1992e+00 (2.1248e+00)	Acc@1  44.00 ( 44.46)	Acc@5  72.00 ( 75.05)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 2.0312e+00 (2.1160e+00)	Acc@1  44.00 ( 44.52)	Acc@5  77.00 ( 75.18)
 * Acc@1 44.420 Acc@5 75.250
### epoch[12] execution time: 11.576170921325684
EPOCH 13
i:   0, name:       features.module.0.weight  changing lr from: 0.085179615101143580   to: 0.082614143440428292
i:   1, name:         features.module.0.bias  changing lr from: 0.085756842525889762   to: 0.083278194413398607
i:   2, name:       features.module.1.weight  changing lr from: 0.086310686600947567   to: 0.083915858674164207
i:   3, name:         features.module.1.bias  changing lr from: 0.086842144248485353   to: 0.084528218071311390
i:   4, name:       features.module.4.weight  changing lr from: 0.087352171622772923   to: 0.085116314322945891
i:   5, name:         features.module.4.bias  changing lr from: 0.087841685284541421   to: 0.085681149693489278
i:   6, name:       features.module.5.weight  changing lr from: 0.088311563420371400   to: 0.086223687788816450
i:   7, name:         features.module.5.bias  changing lr from: 0.088762647092007915   to: 0.086744854445826802
i:   8, name:       features.module.8.weight  changing lr from: 0.089195741502829717   to: 0.087245538695850680
i:   9, name:         features.module.8.bias  changing lr from: 0.089611617270722116   to: 0.087726593784191745
i:  10, name:       features.module.9.weight  changing lr from: 0.090011011698357485   to: 0.088188838230644634
i:  11, name:         features.module.9.bias  changing lr from: 0.090394630033406587   to: 0.088633056918044559
i:  12, name:      features.module.11.weight  changing lr from: 0.090763146712517229   to: 0.089060002197843838
i:  13, name:        features.module.11.bias  changing lr from: 0.091117206584030552   to: 0.089470395003398914
i:  14, name:      features.module.12.weight  changing lr from: 0.091457426105380976   to: 0.089864925963123260
i:  15, name:        features.module.12.bias  changing lr from: 0.091784394511965073   to: 0.090244256506941040
i:  16, name:      features.module.15.weight  changing lr from: 0.092098674954983806   to: 0.090609019960587039
i:  17, name:        features.module.15.bias  changing lr from: 0.092400805606376632   to: 0.090959822623262201
i:  18, name:      features.module.16.weight  changing lr from: 0.092691300729490486   to: 0.091297244824986926
i:  19, name:        features.module.16.bias  changing lr from: 0.092970651714570807   to: 0.091621841960713976
i:  20, name:      features.module.18.weight  changing lr from: 0.093239328078537853   to: 0.091934145498883096
i:  21, name:        features.module.18.bias  changing lr from: 0.093497778428828143   to: 0.092234663962632038
i:  22, name:      features.module.19.weight  changing lr from: 0.093746431391345184   to: 0.092523883882334929
i:  23, name:        features.module.19.bias  changing lr from: 0.093985696502783989   to: 0.092802270718529742
i:  24, name:      features.module.22.weight  changing lr from: 0.094215965067775131   to: 0.093070269754627719
i:  25, name:        features.module.22.bias  changing lr from: 0.094437610981441794   to: 0.093328306959079343
i:  26, name:      features.module.23.weight  changing lr from: 0.094650991518083455   to: 0.093576789816908743
i:  27, name:        features.module.23.bias  changing lr from: 0.094856448086793763   to: 0.093816108130726897
i:  28, name:      features.module.25.weight  changing lr from: 0.095054306954894016   to: 0.094046634791500341
i:  29, name:        features.module.25.bias  changing lr from: 0.095244879940119834   to: 0.094268726519488075
i:  30, name:      features.module.26.weight  changing lr from: 0.095428465072537841   to: 0.094482724575872215
i:  31, name:        features.module.26.bias  changing lr from: 0.095605347227197171   to: 0.094688955445697498
i:  32, name:            classifier.0.weight  changing lr from: 0.095775798728536504   to: 0.094887731492806715
i:  33, name:              classifier.0.bias  changing lr from: 0.095940079927573974   to: 0.095079351587515432
i:  34, name:            classifier.3.weight  changing lr from: 0.096098439752906981   to: 0.095264101707810647
i:  35, name:              classifier.3.bias  changing lr from: 0.096251116236540993   to: 0.095442255514889343
i:  36, name:            classifier.6.weight  changing lr from: 0.096398337015554114   to: 0.095614074903872484
i:  37, name:              classifier.6.bias  changing lr from: 0.096540319810587472   to: 0.095779810530542714



# Switched to train mode...
Epoch: [13][  0/391]	Time  0.182 ( 0.182)	Data  0.150 ( 0.150)	Loss 1.7500e+00 (1.7500e+00)	Acc@1  53.12 ( 53.12)	Acc@5  83.59 ( 83.59)
Epoch: [13][ 10/391]	Time  0.026 ( 0.039)	Data  0.002 ( 0.016)	Loss 1.9893e+00 (1.9227e+00)	Acc@1  51.56 ( 46.88)	Acc@5  81.25 ( 80.18)
Epoch: [13][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.8340e+00 (1.8898e+00)	Acc@1  46.09 ( 47.92)	Acc@5  84.38 ( 80.84)
Epoch: [13][ 30/391]	Time  0.021 ( 0.029)	Data  0.000 ( 0.007)	Loss 1.8184e+00 (1.8871e+00)	Acc@1  47.66 ( 48.69)	Acc@5  80.47 ( 80.47)
Epoch: [13][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.8662e+00 (1.8913e+00)	Acc@1  50.00 ( 48.38)	Acc@5  79.69 ( 80.11)
Epoch: [13][ 50/391]	Time  0.039 ( 0.028)	Data  0.003 ( 0.005)	Loss 1.9268e+00 (1.9173e+00)	Acc@1  48.44 ( 48.38)	Acc@5  73.44 ( 79.60)
Epoch: [13][ 60/391]	Time  0.025 ( 0.027)	Data  0.005 ( 0.005)	Loss 1.7148e+00 (1.9210e+00)	Acc@1  56.25 ( 48.34)	Acc@5  83.59 ( 79.39)
Epoch: [13][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8955e+00 (1.9287e+00)	Acc@1  53.12 ( 48.14)	Acc@5  78.12 ( 79.40)
Epoch: [13][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9990e+00 (1.9439e+00)	Acc@1  49.22 ( 47.80)	Acc@5  81.25 ( 79.16)
Epoch: [13][ 90/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1113e+00 (1.9342e+00)	Acc@1  42.97 ( 47.98)	Acc@5  75.78 ( 79.30)
Epoch: [13][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6328e+00 (1.9250e+00)	Acc@1  51.56 ( 48.14)	Acc@5  84.38 ( 79.52)
Epoch: [13][110/391]	Time  0.037 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.9717e+00 (1.9272e+00)	Acc@1  43.75 ( 47.94)	Acc@5  78.91 ( 79.60)
Epoch: [13][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7656e+00 (1.9242e+00)	Acc@1  52.34 ( 47.96)	Acc@5  85.16 ( 79.66)
Epoch: [13][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7490e+00 (1.9257e+00)	Acc@1  50.78 ( 47.83)	Acc@5  82.03 ( 79.65)
Epoch: [13][140/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9775e+00 (1.9254e+00)	Acc@1  45.31 ( 47.86)	Acc@5  75.00 ( 79.58)
Epoch: [13][150/391]	Time  0.036 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.0039e+00 (1.9245e+00)	Acc@1  45.31 ( 47.95)	Acc@5  78.91 ( 79.55)
Epoch: [13][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8330e+00 (1.9265e+00)	Acc@1  47.66 ( 47.92)	Acc@5  82.03 ( 79.55)
Epoch: [13][170/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7607e+00 (1.9225e+00)	Acc@1  50.78 ( 48.00)	Acc@5  81.25 ( 79.63)
Epoch: [13][180/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9180e+00 (1.9222e+00)	Acc@1  41.41 ( 47.87)	Acc@5  79.69 ( 79.63)
Epoch: [13][190/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.0996e+00 (1.9188e+00)	Acc@1  43.75 ( 47.96)	Acc@5  75.78 ( 79.67)
Epoch: [13][200/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1973e+00 (1.9193e+00)	Acc@1  42.97 ( 47.97)	Acc@5  73.44 ( 79.69)
Epoch: [13][210/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8105e+00 (1.9175e+00)	Acc@1  51.56 ( 48.06)	Acc@5  82.81 ( 79.67)
Epoch: [13][220/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8730e+00 (1.9142e+00)	Acc@1  52.34 ( 48.10)	Acc@5  80.47 ( 79.72)
Epoch: [13][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0312e+00 (1.9166e+00)	Acc@1  46.88 ( 48.01)	Acc@5  78.12 ( 79.67)
Epoch: [13][240/391]	Time  0.035 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.7275e+00 (1.9184e+00)	Acc@1  57.03 ( 48.03)	Acc@5  80.47 ( 79.61)
Epoch: [13][250/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9600e+00 (1.9188e+00)	Acc@1  48.44 ( 48.01)	Acc@5  78.91 ( 79.63)
Epoch: [13][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8535e+00 (1.9196e+00)	Acc@1  53.12 ( 47.97)	Acc@5  78.91 ( 79.61)
Epoch: [13][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0254e+00 (1.9213e+00)	Acc@1  46.09 ( 47.94)	Acc@5  77.34 ( 79.53)
Epoch: [13][280/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9775e+00 (1.9207e+00)	Acc@1  47.66 ( 48.02)	Acc@5  81.25 ( 79.52)
Epoch: [13][290/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7432e+00 (1.9193e+00)	Acc@1  53.12 ( 48.07)	Acc@5  82.81 ( 79.53)
Epoch: [13][300/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3008e+00 (1.9201e+00)	Acc@1  37.50 ( 48.06)	Acc@5  72.66 ( 79.52)
Epoch: [13][310/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0410e+00 (1.9184e+00)	Acc@1  46.09 ( 48.14)	Acc@5  76.56 ( 79.55)
Epoch: [13][320/391]	Time  0.023 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.8232e+00 (1.9207e+00)	Acc@1  48.44 ( 48.09)	Acc@5  85.16 ( 79.55)
Epoch: [13][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5186e+00 (1.9163e+00)	Acc@1  56.25 ( 48.21)	Acc@5  85.94 ( 79.64)
Epoch: [13][340/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9619e+00 (1.9187e+00)	Acc@1  50.00 ( 48.20)	Acc@5  78.91 ( 79.60)
Epoch: [13][350/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2910e+00 (1.9203e+00)	Acc@1  39.06 ( 48.18)	Acc@5  70.31 ( 79.58)
Epoch: [13][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0996e+00 (1.9192e+00)	Acc@1  42.97 ( 48.20)	Acc@5  77.34 ( 79.63)
Epoch: [13][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1445e+00 (1.9194e+00)	Acc@1  42.19 ( 48.18)	Acc@5  75.78 ( 79.63)
Epoch: [13][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6152e+00 (1.9198e+00)	Acc@1  57.81 ( 48.19)	Acc@5  85.94 ( 79.64)
Epoch: [13][390/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.0840e+00 (1.9216e+00)	Acc@1  51.25 ( 48.19)	Acc@5  75.00 ( 79.58)
## e[13] optimizer.zero_grad (sum) time: 0.11473369598388672
## e[13]       loss.backward (sum) time: 2.286893844604492
## e[13]      optimizer.step (sum) time: 0.9748573303222656
## epoch[13] training(only) time: 9.972069263458252
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.9668e+00 (1.9668e+00)	Acc@1  47.00 ( 47.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.9551e+00 (1.9196e+00)	Acc@1  46.00 ( 49.09)	Acc@5  82.00 ( 79.55)
Test: [ 20/100]	Time  0.013 ( 0.019)	Loss 1.8340e+00 (1.9395e+00)	Acc@1  56.00 ( 49.38)	Acc@5  78.00 ( 78.62)
Test: [ 30/100]	Time  0.013 ( 0.017)	Loss 1.8506e+00 (1.9408e+00)	Acc@1  47.00 ( 49.06)	Acc@5  85.00 ( 78.65)
Test: [ 40/100]	Time  0.012 ( 0.016)	Loss 1.8721e+00 (1.9405e+00)	Acc@1  49.00 ( 48.85)	Acc@5  81.00 ( 78.68)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.7939e+00 (1.9418e+00)	Acc@1  56.00 ( 48.86)	Acc@5  77.00 ( 78.20)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.8447e+00 (1.9258e+00)	Acc@1  48.00 ( 49.10)	Acc@5  82.00 ( 78.51)
Test: [ 70/100]	Time  0.019 ( 0.015)	Loss 2.0332e+00 (1.9254e+00)	Acc@1  47.00 ( 48.90)	Acc@5  73.00 ( 78.52)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.9268e+00 (1.9290e+00)	Acc@1  52.00 ( 48.96)	Acc@5  74.00 ( 78.52)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 1.7490e+00 (1.9188e+00)	Acc@1  57.00 ( 49.26)	Acc@5  85.00 ( 78.73)
 * Acc@1 49.240 Acc@5 78.840
### epoch[13] execution time: 11.503278493881226
EPOCH 14
i:   0, name:       features.module.0.weight  changing lr from: 0.082614143440428292   to: 0.079901783211565186
i:   1, name:         features.module.0.bias  changing lr from: 0.083278194413398607   to: 0.080655356266900402
i:   2, name:       features.module.1.weight  changing lr from: 0.083915858674164207   to: 0.081379626758626381
i:   3, name:         features.module.1.bias  changing lr from: 0.084528218071311390   to: 0.082075741742792752
i:   4, name:       features.module.4.weight  changing lr from: 0.085116314322945891   to: 0.082744811051429626
i:   5, name:         features.module.4.bias  changing lr from: 0.085681149693489278   to: 0.083387907243415993
i:   6, name:       features.module.5.weight  changing lr from: 0.086223687788816450   to: 0.084006065768709018
i:   7, name:         features.module.5.bias  changing lr from: 0.086744854445826802   to: 0.084600285311272019
i:   8, name:       features.module.8.weight  changing lr from: 0.087245538695850680   to: 0.085171528280483674
i:   9, name:         features.module.8.bias  changing lr from: 0.087726593784191745   to: 0.085720721424736052
i:  10, name:       features.module.9.weight  changing lr from: 0.088188838230644634   to: 0.086248756544387295
i:  11, name:         features.module.9.bias  changing lr from: 0.088633056918044559   to: 0.086756491284282630
i:  12, name:      features.module.11.weight  changing lr from: 0.089060002197843838   to: 0.087244749988737880
i:  13, name:        features.module.11.bias  changing lr from: 0.089470395003398914   to: 0.087714324604235991
i:  14, name:      features.module.12.weight  changing lr from: 0.089864925963123260   to: 0.088165975617156542
i:  15, name:        features.module.12.bias  changing lr from: 0.090244256506941040   to: 0.088600433015672206
i:  16, name:      features.module.15.weight  changing lr from: 0.090609019960587039   to: 0.089018397266536431
i:  17, name:        features.module.15.bias  changing lr from: 0.090959822623262201   to: 0.089420540298877083
i:  18, name:      features.module.16.weight  changing lr from: 0.091297244824986926   to: 0.089807506488327235
i:  19, name:        features.module.16.bias  changing lr from: 0.091621841960713976   to: 0.090179913635883888
i:  20, name:      features.module.18.weight  changing lr from: 0.091934145498883096   to: 0.090538353936811533
i:  21, name:        features.module.18.bias  changing lr from: 0.092234663962632038   to: 0.090883394935710224
i:  22, name:      features.module.19.weight  changing lr from: 0.092523883882334929   to: 0.091215580464567514
i:  23, name:        features.module.19.bias  changing lr from: 0.092802270718529742   to: 0.091535431561218739
i:  24, name:      features.module.22.weight  changing lr from: 0.093070269754627719   to: 0.091843447366164066
i:  25, name:        features.module.22.bias  changing lr from: 0.093328306959079343   to: 0.092140105996142932
i:  26, name:      features.module.23.weight  changing lr from: 0.093576789816908743   to: 0.092425865393255549
i:  27, name:        features.module.23.bias  changing lr from: 0.093816108130726897   to: 0.092701164148755177
i:  28, name:      features.module.25.weight  changing lr from: 0.094046634791500341   to: 0.092966422300920712
i:  29, name:        features.module.25.bias  changing lr from: 0.094268726519488075   to: 0.093222042106661920
i:  30, name:      features.module.26.weight  changing lr from: 0.094482724575872215   to: 0.093468408786717208
i:  31, name:        features.module.26.bias  changing lr from: 0.094688955445697498   to: 0.093705891244476716
i:  32, name:            classifier.0.weight  changing lr from: 0.094887731492806715   to: 0.093934842758610818
i:  33, name:              classifier.0.bias  changing lr from: 0.095079351587515432   to: 0.094155601649805021
i:  34, name:            classifier.3.weight  changing lr from: 0.095264101707810647   to: 0.094368491922003167
i:  35, name:              classifier.3.bias  changing lr from: 0.095442255514889343   to: 0.094573823878642618
i:  36, name:            classifier.6.weight  changing lr from: 0.095614074903872484   to: 0.094771894714430996
i:  37, name:              classifier.6.bias  changing lr from: 0.095779810530542714   to: 0.094962989083266836



# Switched to train mode...
Epoch: [14][  0/391]	Time  0.171 ( 0.171)	Data  0.143 ( 0.143)	Loss 1.8154e+00 (1.8154e+00)	Acc@1  51.56 ( 51.56)	Acc@5  82.03 ( 82.03)
Epoch: [14][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.015)	Loss 1.7266e+00 (1.8605e+00)	Acc@1  53.91 ( 50.50)	Acc@5  86.72 ( 80.54)
Epoch: [14][ 20/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.009)	Loss 1.9531e+00 (1.8527e+00)	Acc@1  46.88 ( 50.56)	Acc@5  82.03 ( 80.13)
Epoch: [14][ 30/391]	Time  0.030 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.8330e+00 (1.8557e+00)	Acc@1  50.78 ( 49.87)	Acc@5  80.47 ( 80.07)
Epoch: [14][ 40/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.6514e+00 (1.8716e+00)	Acc@1  52.34 ( 49.26)	Acc@5  83.59 ( 79.65)
Epoch: [14][ 50/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.9355e+00 (1.8728e+00)	Acc@1  44.53 ( 49.19)	Acc@5  80.47 ( 79.63)
Epoch: [14][ 60/391]	Time  0.035 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.5596e+00 (1.8643e+00)	Acc@1  61.72 ( 49.62)	Acc@5  79.69 ( 79.71)
Epoch: [14][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5352e+00 (1.8520e+00)	Acc@1  57.81 ( 49.87)	Acc@5  88.28 ( 80.13)
Epoch: [14][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6270e+00 (1.8479e+00)	Acc@1  57.03 ( 50.14)	Acc@5  83.59 ( 80.26)
Epoch: [14][ 90/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6914e+00 (1.8423e+00)	Acc@1  52.34 ( 50.15)	Acc@5  82.81 ( 80.54)
Epoch: [14][100/391]	Time  0.032 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.2461e+00 (1.8450e+00)	Acc@1  42.97 ( 50.17)	Acc@5  75.00 ( 80.55)
Epoch: [14][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5400e+00 (1.8416e+00)	Acc@1  54.69 ( 50.23)	Acc@5  85.16 ( 80.62)
Epoch: [14][120/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9453e+00 (1.8420e+00)	Acc@1  46.09 ( 50.16)	Acc@5  81.25 ( 80.58)
Epoch: [14][130/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5664e+00 (1.8324e+00)	Acc@1  60.16 ( 50.48)	Acc@5  86.72 ( 80.74)
Epoch: [14][140/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9121e+00 (1.8340e+00)	Acc@1  48.44 ( 50.44)	Acc@5  77.34 ( 80.78)
Epoch: [14][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9932e+00 (1.8424e+00)	Acc@1  46.88 ( 50.19)	Acc@5  83.59 ( 80.74)
Epoch: [14][160/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9736e+00 (1.8480e+00)	Acc@1  46.09 ( 50.09)	Acc@5  79.69 ( 80.66)
Epoch: [14][170/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8291e+00 (1.8510e+00)	Acc@1  49.22 ( 49.92)	Acc@5  84.38 ( 80.57)
Epoch: [14][180/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7422e+00 (1.8519e+00)	Acc@1  53.91 ( 50.00)	Acc@5  83.59 ( 80.55)
Epoch: [14][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9375e+00 (1.8529e+00)	Acc@1  52.34 ( 49.96)	Acc@5  79.69 ( 80.55)
Epoch: [14][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7129e+00 (1.8530e+00)	Acc@1  57.03 ( 49.96)	Acc@5  82.81 ( 80.62)
Epoch: [14][210/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8262e+00 (1.8510e+00)	Acc@1  51.56 ( 49.93)	Acc@5  83.59 ( 80.67)
Epoch: [14][220/391]	Time  0.043 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9365e+00 (1.8525e+00)	Acc@1  50.00 ( 49.83)	Acc@5  79.69 ( 80.65)
Epoch: [14][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8242e+00 (1.8506e+00)	Acc@1  49.22 ( 49.82)	Acc@5  83.59 ( 80.70)
Epoch: [14][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9980e+00 (1.8480e+00)	Acc@1  42.97 ( 49.82)	Acc@5  81.25 ( 80.75)
Epoch: [14][250/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7822e+00 (1.8467e+00)	Acc@1  52.34 ( 49.86)	Acc@5  82.03 ( 80.76)
Epoch: [14][260/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.9512e+00 (1.8455e+00)	Acc@1  49.22 ( 49.90)	Acc@5  78.12 ( 80.79)
Epoch: [14][270/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.7148e+00 (1.8434e+00)	Acc@1  51.56 ( 49.97)	Acc@5  81.25 ( 80.81)
Epoch: [14][280/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9473e+00 (1.8408e+00)	Acc@1  46.88 ( 50.00)	Acc@5  79.69 ( 80.84)
Epoch: [14][290/391]	Time  0.024 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.8799e+00 (1.8427e+00)	Acc@1  45.31 ( 49.95)	Acc@5  79.69 ( 80.78)
Epoch: [14][300/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7314e+00 (1.8413e+00)	Acc@1  57.03 ( 49.97)	Acc@5  82.81 ( 80.77)
Epoch: [14][310/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6240e+00 (1.8385e+00)	Acc@1  56.25 ( 50.07)	Acc@5  79.69 ( 80.79)
Epoch: [14][320/391]	Time  0.036 ( 0.025)	Data  0.004 ( 0.002)	Loss 1.7285e+00 (1.8384e+00)	Acc@1  54.69 ( 50.08)	Acc@5  82.03 ( 80.78)
Epoch: [14][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7676e+00 (1.8368e+00)	Acc@1  56.25 ( 50.13)	Acc@5  81.25 ( 80.81)
Epoch: [14][340/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8994e+00 (1.8377e+00)	Acc@1  54.69 ( 50.15)	Acc@5  78.12 ( 80.80)
Epoch: [14][350/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9229e+00 (1.8402e+00)	Acc@1  50.78 ( 50.10)	Acc@5  72.66 ( 80.71)
Epoch: [14][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6514e+00 (1.8403e+00)	Acc@1  51.56 ( 50.13)	Acc@5  82.03 ( 80.72)
Epoch: [14][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9170e+00 (1.8407e+00)	Acc@1  46.88 ( 50.09)	Acc@5  81.25 ( 80.75)
Epoch: [14][380/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9502e+00 (1.8403e+00)	Acc@1  49.22 ( 50.07)	Acc@5  77.34 ( 80.76)
Epoch: [14][390/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.1270e+00 (1.8426e+00)	Acc@1  43.75 ( 50.03)	Acc@5  78.75 ( 80.73)
## e[14] optimizer.zero_grad (sum) time: 0.11336159706115723
## e[14]       loss.backward (sum) time: 2.288706064224243
## e[14]      optimizer.step (sum) time: 0.9728412628173828
## epoch[14] training(only) time: 9.96541690826416
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 2.0527e+00 (2.0527e+00)	Acc@1  48.00 ( 48.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.014 ( 0.025)	Loss 1.9160e+00 (1.9348e+00)	Acc@1  45.00 ( 49.64)	Acc@5  79.00 ( 79.09)
Test: [ 20/100]	Time  0.013 ( 0.019)	Loss 1.8926e+00 (1.9663e+00)	Acc@1  53.00 ( 49.00)	Acc@5  81.00 ( 78.48)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.9258e+00 (1.9556e+00)	Acc@1  48.00 ( 48.48)	Acc@5  81.00 ( 78.35)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 1.9023e+00 (1.9542e+00)	Acc@1  51.00 ( 48.24)	Acc@5  82.00 ( 78.68)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 2.1738e+00 (1.9774e+00)	Acc@1  41.00 ( 47.69)	Acc@5  73.00 ( 78.16)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 2.0703e+00 (1.9768e+00)	Acc@1  50.00 ( 47.41)	Acc@5  72.00 ( 77.98)
Test: [ 70/100]	Time  0.017 ( 0.015)	Loss 2.1738e+00 (1.9853e+00)	Acc@1  42.00 ( 47.17)	Acc@5  74.00 ( 77.90)
Test: [ 80/100]	Time  0.019 ( 0.015)	Loss 2.2344e+00 (2.0025e+00)	Acc@1  42.00 ( 46.79)	Acc@5  71.00 ( 77.60)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.0488e+00 (1.9989e+00)	Acc@1  51.00 ( 47.05)	Acc@5  74.00 ( 77.65)
 * Acc@1 47.020 Acc@5 77.660
### epoch[14] execution time: 11.504609823226929
EPOCH 15
i:   0, name:       features.module.0.weight  changing lr from: 0.079901783211565186   to: 0.077055136834451199
i:   1, name:         features.module.0.bias  changing lr from: 0.080655356266900402   to: 0.077900044575759317
i:   2, name:       features.module.1.weight  changing lr from: 0.081379626758626381   to: 0.078712886310922797
i:   3, name:         features.module.1.bias  changing lr from: 0.082075741742792752   to: 0.079494849740275153
i:   4, name:       features.module.4.weight  changing lr from: 0.082744811051429626   to: 0.080247090870888535
i:   5, name:         features.module.4.bias  changing lr from: 0.083387907243415993   to: 0.080970732989508951
i:   6, name:       features.module.5.weight  changing lr from: 0.084006065768709018   to: 0.081666865966641633
i:   7, name:         features.module.5.bias  changing lr from: 0.084600285311272019   to: 0.082336545844478956
i:   8, name:       features.module.8.weight  changing lr from: 0.085171528280483674   to: 0.082980794667073740
i:   9, name:         features.module.8.bias  changing lr from: 0.085720721424736052   to: 0.083600600516231957
i:  10, name:       features.module.9.weight  changing lr from: 0.086248756544387295   to: 0.084196917721101883
i:  11, name:         features.module.9.bias  changing lr from: 0.086756491284282630   to: 0.084770667213426992
i:  12, name:      features.module.11.weight  changing lr from: 0.087244749988737880   to: 0.085322737003965088
i:  13, name:        features.module.11.bias  changing lr from: 0.087714324604235991   to: 0.085853982758703012
i:  14, name:      features.module.12.weight  changing lr from: 0.088165975617156542   to: 0.086365228456261003
i:  15, name:        features.module.12.bias  changing lr from: 0.088600433015672206   to: 0.086857267110321587
i:  16, name:      features.module.15.weight  changing lr from: 0.089018397266536431   to: 0.087330861543071098
i:  17, name:        features.module.15.bias  changing lr from: 0.089420540298877083   to: 0.087786745197539706
i:  18, name:      features.module.16.weight  changing lr from: 0.089807506488327235   to: 0.088225622978397117
i:  19, name:        features.module.16.bias  changing lr from: 0.090179913635883888   to: 0.088648172112229459
i:  20, name:      features.module.18.weight  changing lr from: 0.090538353936811533   to: 0.089055043019615074
i:  21, name:        features.module.18.bias  changing lr from: 0.090883394935710224   to: 0.089446860192448518
i:  22, name:      features.module.19.weight  changing lr from: 0.091215580464567514   to: 0.089824223070955500
i:  23, name:        features.module.19.bias  changing lr from: 0.091535431561218739   to: 0.090187706915710195
i:  24, name:      features.module.22.weight  changing lr from: 0.091843447366164066   to: 0.090537863670725777
i:  25, name:        features.module.22.bias  changing lr from: 0.092140105996142932   to: 0.090875222814351841
i:  26, name:      features.module.23.weight  changing lr from: 0.092425865393255549   to: 0.091200292195289759
i:  27, name:        features.module.23.bias  changing lr from: 0.092701164148755177   to: 0.091513558851539265
i:  28, name:      features.module.25.weight  changing lr from: 0.092966422300920712   to: 0.091815489810525105
i:  29, name:        features.module.25.bias  changing lr from: 0.093222042106661920   to: 0.092106532869030366
i:  30, name:      features.module.26.weight  changing lr from: 0.093468408786717208   to: 0.092387117351888759
i:  31, name:        features.module.26.bias  changing lr from: 0.093705891244476716   to: 0.092657654848668780
i:  32, name:            classifier.0.weight  changing lr from: 0.093934842758610818   to: 0.092918539927824437
i:  33, name:              classifier.0.bias  changing lr from: 0.094155601649805021   to: 0.093170150827992460
i:  34, name:            classifier.3.weight  changing lr from: 0.094368491922003167   to: 0.093412850126292524
i:  35, name:              classifier.3.bias  changing lr from: 0.094573823878642618   to: 0.093646985383635389
i:  36, name:            classifier.6.weight  changing lr from: 0.094771894714430996   to: 0.093872889767169704
i:  37, name:              classifier.6.bias  changing lr from: 0.094962989083266836   to: 0.094090882650103236



# Switched to train mode...
Epoch: [15][  0/391]	Time  0.172 ( 0.172)	Data  0.147 ( 0.147)	Loss 1.6514e+00 (1.6514e+00)	Acc@1  53.12 ( 53.12)	Acc@5  89.06 ( 89.06)
Epoch: [15][ 10/391]	Time  0.030 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.7607e+00 (1.7479e+00)	Acc@1  50.00 ( 53.12)	Acc@5  79.69 ( 82.53)
Epoch: [15][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.8525e+00 (1.7866e+00)	Acc@1  47.66 ( 51.41)	Acc@5  80.47 ( 81.62)
Epoch: [15][ 30/391]	Time  0.032 ( 0.030)	Data  0.003 ( 0.006)	Loss 1.6543e+00 (1.7639e+00)	Acc@1  54.69 ( 51.59)	Acc@5  84.38 ( 82.36)
Epoch: [15][ 40/391]	Time  0.020 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.5430e+00 (1.7619e+00)	Acc@1  60.94 ( 51.66)	Acc@5  83.59 ( 82.22)
Epoch: [15][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.8750e+00 (1.7695e+00)	Acc@1  52.34 ( 51.41)	Acc@5  76.56 ( 81.92)
Epoch: [15][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8145e+00 (1.7636e+00)	Acc@1  46.88 ( 51.43)	Acc@5  80.47 ( 82.16)
Epoch: [15][ 70/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0508e+00 (1.7578e+00)	Acc@1  42.97 ( 51.47)	Acc@5  75.78 ( 82.21)
Epoch: [15][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6230e+00 (1.7634e+00)	Acc@1  52.34 ( 51.31)	Acc@5  87.50 ( 82.28)
Epoch: [15][ 90/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6387e+00 (1.7693e+00)	Acc@1  53.91 ( 51.17)	Acc@5  85.16 ( 82.18)
Epoch: [15][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9971e+00 (1.7721e+00)	Acc@1  52.34 ( 51.31)	Acc@5  78.91 ( 82.17)
Epoch: [15][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8096e+00 (1.7746e+00)	Acc@1  50.78 ( 51.29)	Acc@5  82.81 ( 82.05)
Epoch: [15][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7520e+00 (1.7771e+00)	Acc@1  55.47 ( 51.18)	Acc@5  82.81 ( 82.07)
Epoch: [15][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9395e+00 (1.7831e+00)	Acc@1  46.88 ( 51.00)	Acc@5  81.25 ( 82.04)
Epoch: [15][140/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4951e+00 (1.7830e+00)	Acc@1  57.81 ( 51.01)	Acc@5  85.16 ( 81.96)
Epoch: [15][150/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6230e+00 (1.7815e+00)	Acc@1  53.91 ( 51.01)	Acc@5  85.16 ( 82.00)
Epoch: [15][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5430e+00 (1.7834e+00)	Acc@1  52.34 ( 51.03)	Acc@5  88.28 ( 81.99)
Epoch: [15][170/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0234e+00 (1.7785e+00)	Acc@1  40.62 ( 51.12)	Acc@5  77.34 ( 82.02)
Epoch: [15][180/391]	Time  0.042 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6514e+00 (1.7789e+00)	Acc@1  64.84 ( 51.20)	Acc@5  82.81 ( 82.03)
Epoch: [15][190/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.7676e+00 (1.7803e+00)	Acc@1  53.12 ( 51.16)	Acc@5  78.12 ( 82.06)
Epoch: [15][200/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8672e+00 (1.7836e+00)	Acc@1  48.44 ( 51.06)	Acc@5  82.03 ( 81.94)
Epoch: [15][210/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5605e+00 (1.7834e+00)	Acc@1  59.38 ( 51.10)	Acc@5  85.16 ( 81.96)
Epoch: [15][220/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7617e+00 (1.7871e+00)	Acc@1  50.78 ( 51.04)	Acc@5  82.81 ( 81.94)
Epoch: [15][230/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.7861e+00 (1.7873e+00)	Acc@1  51.56 ( 51.05)	Acc@5  84.38 ( 81.96)
Epoch: [15][240/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6172e+00 (1.7886e+00)	Acc@1  52.34 ( 51.04)	Acc@5  86.72 ( 81.90)
Epoch: [15][250/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.5723e+00 (1.7883e+00)	Acc@1  52.34 ( 51.09)	Acc@5  85.16 ( 81.91)
Epoch: [15][260/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9541e+00 (1.7855e+00)	Acc@1  51.56 ( 51.16)	Acc@5  77.34 ( 81.94)
Epoch: [15][270/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8721e+00 (1.7824e+00)	Acc@1  49.22 ( 51.24)	Acc@5  78.12 ( 81.98)
Epoch: [15][280/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9316e+00 (1.7831e+00)	Acc@1  49.22 ( 51.23)	Acc@5  81.25 ( 81.96)
Epoch: [15][290/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7422e+00 (1.7834e+00)	Acc@1  55.47 ( 51.30)	Acc@5  85.16 ( 81.96)
Epoch: [15][300/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7480e+00 (1.7816e+00)	Acc@1  51.56 ( 51.35)	Acc@5  80.47 ( 81.97)
Epoch: [15][310/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8760e+00 (1.7811e+00)	Acc@1  50.00 ( 51.36)	Acc@5  81.25 ( 81.96)
Epoch: [15][320/391]	Time  0.038 ( 0.025)	Data  0.003 ( 0.002)	Loss 2.2363e+00 (1.7840e+00)	Acc@1  41.41 ( 51.28)	Acc@5  75.78 ( 81.90)
Epoch: [15][330/391]	Time  0.040 ( 0.025)	Data  0.004 ( 0.002)	Loss 1.8037e+00 (1.7880e+00)	Acc@1  50.00 ( 51.16)	Acc@5  81.25 ( 81.82)
Epoch: [15][340/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5518e+00 (1.7859e+00)	Acc@1  58.59 ( 51.23)	Acc@5  87.50 ( 81.87)
Epoch: [15][350/391]	Time  0.040 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.7178e+00 (1.7847e+00)	Acc@1  57.03 ( 51.27)	Acc@5  81.25 ( 81.84)
Epoch: [15][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6484e+00 (1.7823e+00)	Acc@1  55.47 ( 51.36)	Acc@5  86.72 ( 81.89)
Epoch: [15][370/391]	Time  0.043 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8027e+00 (1.7828e+00)	Acc@1  53.91 ( 51.37)	Acc@5  78.12 ( 81.91)
Epoch: [15][380/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8848e+00 (1.7841e+00)	Acc@1  55.47 ( 51.35)	Acc@5  75.78 ( 81.88)
Epoch: [15][390/391]	Time  0.018 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.7607e+00 (1.7844e+00)	Acc@1  53.75 ( 51.41)	Acc@5  78.75 ( 81.86)
## e[15] optimizer.zero_grad (sum) time: 0.11379551887512207
## e[15]       loss.backward (sum) time: 2.291193723678589
## e[15]      optimizer.step (sum) time: 0.9811089038848877
## epoch[15] training(only) time: 9.972069025039673
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 1.9932e+00 (1.9932e+00)	Acc@1  50.00 ( 50.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 1.7871e+00 (1.7787e+00)	Acc@1  49.00 ( 52.27)	Acc@5  84.00 ( 82.18)
Test: [ 20/100]	Time  0.012 ( 0.019)	Loss 1.6543e+00 (1.7737e+00)	Acc@1  56.00 ( 52.43)	Acc@5  79.00 ( 81.33)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 1.8486e+00 (1.7725e+00)	Acc@1  52.00 ( 52.19)	Acc@5  81.00 ( 81.39)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.9082e+00 (1.7897e+00)	Acc@1  47.00 ( 51.76)	Acc@5  81.00 ( 81.27)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.9150e+00 (1.8115e+00)	Acc@1  50.00 ( 51.24)	Acc@5  77.00 ( 80.67)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.7734e+00 (1.8024e+00)	Acc@1  50.00 ( 51.21)	Acc@5  82.00 ( 80.79)
Test: [ 70/100]	Time  0.015 ( 0.015)	Loss 1.7812e+00 (1.8005e+00)	Acc@1  53.00 ( 51.24)	Acc@5  80.00 ( 80.86)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.8223e+00 (1.8066e+00)	Acc@1  56.00 ( 51.14)	Acc@5  78.00 ( 80.81)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.9834e+00 (1.8037e+00)	Acc@1  46.00 ( 51.23)	Acc@5  83.00 ( 80.97)
 * Acc@1 51.410 Acc@5 80.820
### epoch[15] execution time: 11.531561374664307
EPOCH 16
i:   0, name:       features.module.0.weight  changing lr from: 0.077055136834451199   to: 0.074087430661751719
i:   1, name:         features.module.0.bias  changing lr from: 0.077900044575759317   to: 0.075024567602369335
i:   2, name:       features.module.1.weight  changing lr from: 0.078712886310922797   to: 0.075927093442224966
i:   3, name:         features.module.1.bias  changing lr from: 0.079494849740275153   to: 0.076796207198923022
i:   4, name:       features.module.4.weight  changing lr from: 0.080247090870888535   to: 0.077633084680589437
i:   5, name:         features.module.4.bias  changing lr from: 0.080970732989508951   to: 0.078438876210000827
i:   6, name:       features.module.5.weight  changing lr from: 0.081666865966641633   to: 0.079214704820740778
i:   7, name:         features.module.5.bias  changing lr from: 0.082336545844478956   to: 0.079961664863710910
i:   8, name:       features.module.8.weight  changing lr from: 0.082980794667073740   to: 0.080680820969379147
i:   9, name:         features.module.8.bias  changing lr from: 0.083600600516231957   to: 0.081373207317457064
i:  10, name:       features.module.9.weight  changing lr from: 0.084196917721101883   to: 0.082039827171333707
i:  11, name:         features.module.9.bias  changing lr from: 0.084770667213426992   to: 0.082681652639621406
i:  12, name:      features.module.11.weight  changing lr from: 0.085322737003965088   to: 0.083299624631648181
i:  13, name:        features.module.11.bias  changing lr from: 0.085853982758703012   to: 0.083894652977720238
i:  14, name:      features.module.12.weight  changing lr from: 0.086365228456261003   to: 0.084467616688522873
i:  15, name:        features.module.12.bias  changing lr from: 0.086857267110321587   to: 0.085019364331179390
i:  16, name:      features.module.15.weight  changing lr from: 0.087330861543071098   to: 0.085550714502282188
i:  17, name:        features.module.15.bias  changing lr from: 0.087786745197539706   to: 0.086062456380689531
i:  18, name:      features.module.16.weight  changing lr from: 0.088225622978397117   to: 0.086555350345077289
i:  19, name:        features.module.16.bias  changing lr from: 0.088648172112229459   to: 0.087030128643177385
i:  20, name:      features.module.18.weight  changing lr from: 0.089055043019615074   to: 0.087487496101353701
i:  21, name:        features.module.18.bias  changing lr from: 0.089446860192448518   to: 0.087928130864683374
i:  22, name:      features.module.19.weight  changing lr from: 0.089824223070955500   to: 0.088352685159050065
i:  23, name:        features.module.19.bias  changing lr from: 0.090187706915710195   to: 0.088761786067937679
i:  24, name:      features.module.22.weight  changing lr from: 0.090537863670725777   to: 0.089156036317650733
i:  25, name:        features.module.22.bias  changing lr from: 0.090875222814351841   to: 0.089536015065603713
i:  26, name:      features.module.23.weight  changing lr from: 0.091200292195289759   to: 0.089902278687124082
i:  27, name:        features.module.23.bias  changing lr from: 0.091513558851539265   to: 0.090255361556918282
i:  28, name:      features.module.25.weight  changing lr from: 0.091815489810525105   to: 0.090595776821968566
i:  29, name:        features.module.25.bias  changing lr from: 0.092106532869030366   to: 0.090924017163167836
i:  30, name:      features.module.26.weight  changing lr from: 0.092387117351888759   to: 0.091240555543472129
i:  31, name:        features.module.26.bias  changing lr from: 0.092657654848668780   to: 0.091545845940761761
i:  32, name:            classifier.0.weight  changing lr from: 0.092918539927824437   to: 0.091840324063959991
i:  33, name:              classifier.0.bias  changing lr from: 0.093170150827992460   to: 0.092124408051269327
i:  34, name:            classifier.3.weight  changing lr from: 0.093412850126292524   to: 0.092398499149654312
i:  35, name:              classifier.3.bias  changing lr from: 0.093646985383635389   to: 0.092662982374932951
i:  36, name:            classifier.6.weight  changing lr from: 0.093872889767169704   to: 0.092918227152038457
i:  37, name:              classifier.6.bias  changing lr from: 0.094090882650103236   to: 0.093164587935185436



# Switched to train mode...
Epoch: [16][  0/391]	Time  0.179 ( 0.179)	Data  0.147 ( 0.147)	Loss 1.6318e+00 (1.6318e+00)	Acc@1  57.03 ( 57.03)	Acc@5  82.81 ( 82.81)
Epoch: [16][ 10/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.4385e+00 (1.6276e+00)	Acc@1  61.72 ( 56.32)	Acc@5  86.72 ( 84.52)
Epoch: [16][ 20/391]	Time  0.028 ( 0.032)	Data  0.001 ( 0.008)	Loss 1.5391e+00 (1.6013e+00)	Acc@1  59.38 ( 55.99)	Acc@5  84.38 ( 84.86)
Epoch: [16][ 30/391]	Time  0.022 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.6611e+00 (1.6211e+00)	Acc@1  50.00 ( 55.19)	Acc@5  84.38 ( 84.60)
Epoch: [16][ 40/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.6699e+00 (1.6451e+00)	Acc@1  51.56 ( 54.36)	Acc@5  80.47 ( 84.07)
Epoch: [16][ 50/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.7500e+00 (1.6532e+00)	Acc@1  52.34 ( 54.37)	Acc@5  85.16 ( 84.02)
Epoch: [16][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.5391e+00 (1.6430e+00)	Acc@1  58.59 ( 54.57)	Acc@5  86.72 ( 84.29)
Epoch: [16][ 70/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7031e+00 (1.6548e+00)	Acc@1  54.69 ( 54.48)	Acc@5  84.38 ( 84.13)
Epoch: [16][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3301e+00 (1.6626e+00)	Acc@1  60.94 ( 54.07)	Acc@5  90.62 ( 84.08)
Epoch: [16][ 90/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9209e+00 (1.6667e+00)	Acc@1  50.00 ( 54.20)	Acc@5  78.12 ( 83.98)
Epoch: [16][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6865e+00 (1.6733e+00)	Acc@1  54.69 ( 53.96)	Acc@5  82.03 ( 83.71)
Epoch: [16][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9619e+00 (1.6785e+00)	Acc@1  48.44 ( 53.79)	Acc@5  82.03 ( 83.73)
Epoch: [16][120/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7920e+00 (1.6771e+00)	Acc@1  48.44 ( 53.85)	Acc@5  83.59 ( 83.79)
Epoch: [16][130/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9004e+00 (1.6850e+00)	Acc@1  53.12 ( 53.75)	Acc@5  78.12 ( 83.58)
Epoch: [16][140/391]	Time  0.024 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.6738e+00 (1.6898e+00)	Acc@1  55.47 ( 53.82)	Acc@5  83.59 ( 83.48)
Epoch: [16][150/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6768e+00 (1.6855e+00)	Acc@1  54.69 ( 53.86)	Acc@5  85.16 ( 83.54)
Epoch: [16][160/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9385e+00 (1.6867e+00)	Acc@1  46.88 ( 53.86)	Acc@5  79.69 ( 83.55)
Epoch: [16][170/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6602e+00 (1.6863e+00)	Acc@1  54.69 ( 53.96)	Acc@5  81.25 ( 83.55)
Epoch: [16][180/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7275e+00 (1.6911e+00)	Acc@1  50.78 ( 53.85)	Acc@5  89.06 ( 83.46)
Epoch: [16][190/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.7070e+00 (1.6883e+00)	Acc@1  58.59 ( 53.96)	Acc@5  79.69 ( 83.51)
Epoch: [16][200/391]	Time  0.034 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9121e+00 (1.6871e+00)	Acc@1  50.78 ( 54.07)	Acc@5  81.25 ( 83.50)
Epoch: [16][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6270e+00 (1.6868e+00)	Acc@1  51.56 ( 54.04)	Acc@5  84.38 ( 83.57)
Epoch: [16][220/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7285e+00 (1.6864e+00)	Acc@1  54.69 ( 54.02)	Acc@5  84.38 ( 83.57)
Epoch: [16][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6943e+00 (1.6861e+00)	Acc@1  51.56 ( 54.10)	Acc@5  82.81 ( 83.55)
Epoch: [16][240/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.7568e+00 (1.6847e+00)	Acc@1  49.22 ( 54.12)	Acc@5  88.28 ( 83.59)
Epoch: [16][250/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6914e+00 (1.6860e+00)	Acc@1  56.25 ( 54.06)	Acc@5  78.91 ( 83.55)
Epoch: [16][260/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7197e+00 (1.6853e+00)	Acc@1  53.91 ( 54.02)	Acc@5  79.69 ( 83.55)
Epoch: [16][270/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0430e+00 (1.6920e+00)	Acc@1  42.97 ( 53.87)	Acc@5  75.78 ( 83.42)
Epoch: [16][280/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.7324e+00 (1.6925e+00)	Acc@1  48.44 ( 53.81)	Acc@5  85.94 ( 83.47)
Epoch: [16][290/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7266e+00 (1.6927e+00)	Acc@1  51.56 ( 53.85)	Acc@5  82.03 ( 83.43)
Epoch: [16][300/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9639e+00 (1.6936e+00)	Acc@1  50.00 ( 53.81)	Acc@5  81.25 ( 83.42)
Epoch: [16][310/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5068e+00 (1.6913e+00)	Acc@1  57.81 ( 53.79)	Acc@5  85.16 ( 83.47)
Epoch: [16][320/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6738e+00 (1.6924e+00)	Acc@1  51.56 ( 53.69)	Acc@5  85.94 ( 83.45)
Epoch: [16][330/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4629e+00 (1.6923e+00)	Acc@1  54.69 ( 53.67)	Acc@5  85.94 ( 83.43)
Epoch: [16][340/391]	Time  0.044 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5596e+00 (1.6936e+00)	Acc@1  53.12 ( 53.60)	Acc@5  87.50 ( 83.42)
Epoch: [16][350/391]	Time  0.039 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7529e+00 (1.6965e+00)	Acc@1  51.56 ( 53.54)	Acc@5  82.81 ( 83.37)
Epoch: [16][360/391]	Time  0.024 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.6406e+00 (1.6987e+00)	Acc@1  55.47 ( 53.48)	Acc@5  84.38 ( 83.37)
Epoch: [16][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8389e+00 (1.7021e+00)	Acc@1  50.00 ( 53.42)	Acc@5  83.59 ( 83.28)
Epoch: [16][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7451e+00 (1.7049e+00)	Acc@1  49.22 ( 53.36)	Acc@5  80.47 ( 83.22)
Epoch: [16][390/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.7959e+00 (1.7049e+00)	Acc@1  48.75 ( 53.36)	Acc@5  85.00 ( 83.21)
## e[16] optimizer.zero_grad (sum) time: 0.11542534828186035
## e[16]       loss.backward (sum) time: 2.291266918182373
## e[16]      optimizer.step (sum) time: 0.9700326919555664
## epoch[16] training(only) time: 9.973116159439087
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.9277e+00 (1.9277e+00)	Acc@1  51.00 ( 51.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.016 ( 0.025)	Loss 2.0918e+00 (1.8365e+00)	Acc@1  41.00 ( 52.91)	Acc@5  78.00 ( 80.27)
Test: [ 20/100]	Time  0.020 ( 0.020)	Loss 1.8105e+00 (1.8503e+00)	Acc@1  53.00 ( 51.71)	Acc@5  75.00 ( 79.71)
Test: [ 30/100]	Time  0.020 ( 0.018)	Loss 2.0605e+00 (1.8673e+00)	Acc@1  49.00 ( 51.58)	Acc@5  77.00 ( 79.65)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.9609e+00 (1.8915e+00)	Acc@1  49.00 ( 50.80)	Acc@5  79.00 ( 79.37)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.8467e+00 (1.9203e+00)	Acc@1  56.00 ( 50.24)	Acc@5  74.00 ( 78.61)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.8838e+00 (1.9114e+00)	Acc@1  48.00 ( 50.28)	Acc@5  80.00 ( 78.82)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 2.0664e+00 (1.9172e+00)	Acc@1  49.00 ( 50.45)	Acc@5  74.00 ( 78.69)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 2.0098e+00 (1.9229e+00)	Acc@1  53.00 ( 50.49)	Acc@5  73.00 ( 78.69)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.0762e+00 (1.9152e+00)	Acc@1  45.00 ( 50.51)	Acc@5  77.00 ( 78.88)
 * Acc@1 50.470 Acc@5 78.920
### epoch[16] execution time: 11.553921699523926
EPOCH 17
i:   0, name:       features.module.0.weight  changing lr from: 0.074087430661751719   to: 0.071012453525392968
i:   1, name:         features.module.0.bias  changing lr from: 0.075024567602369335   to: 0.072041770399872848
i:   2, name:       features.module.1.weight  changing lr from: 0.075927093442224966   to: 0.073034215703717806
i:   3, name:         features.module.1.bias  changing lr from: 0.076796207198923022   to: 0.073990965839705644
i:   4, name:       features.module.4.weight  changing lr from: 0.077633084680589437   to: 0.074913185731289966
i:   5, name:         features.module.4.bias  changing lr from: 0.078438876210000827   to: 0.075802025013955707
i:   6, name:       features.module.5.weight  changing lr from: 0.079214704820740778   to: 0.076658614862033236
i:   7, name:         features.module.5.bias  changing lr from: 0.079961664863710910   to: 0.077484065373501387
i:   8, name:       features.module.8.weight  changing lr from: 0.080680820969379147   to: 0.078279463443731379
i:   9, name:         features.module.8.bias  changing lr from: 0.081373207317457064   to: 0.079045871066707074
i:  10, name:       features.module.9.weight  changing lr from: 0.082039827171333707   to: 0.079784324009076743
i:  11, name:         features.module.9.bias  changing lr from: 0.082681652639621406   to: 0.080495830808514440
i:  12, name:      features.module.11.weight  changing lr from: 0.083299624631648181   to: 0.081181372053356793
i:  13, name:        features.module.11.bias  changing lr from: 0.083894652977720238   to: 0.081841899905398127
i:  14, name:      features.module.12.weight  changing lr from: 0.084467616688522873   to: 0.082478337832121607
i:  15, name:        features.module.12.bias  changing lr from: 0.085019364331179390   to: 0.083091580518572003
i:  16, name:      features.module.15.weight  changing lr from: 0.085550714502282188   to: 0.083682493932580829
i:  17, name:        features.module.15.bias  changing lr from: 0.086062456380689531   to: 0.084251915520179174
i:  18, name:      features.module.16.weight  changing lr from: 0.086555350345077289   to: 0.084800654510817655
i:  19, name:        features.module.16.bias  changing lr from: 0.087030128643177385   to: 0.085329492314488858
i:  20, name:      features.module.18.weight  changing lr from: 0.087487496101353701   to: 0.085839182995050833
i:  21, name:        features.module.18.bias  changing lr from: 0.087928130864683374   to: 0.086330453806005486
i:  22, name:      features.module.19.weight  changing lr from: 0.088352685159050065   to: 0.086804005776722176
i:  23, name:        features.module.19.bias  changing lr from: 0.088761786067937679   to: 0.087260514338635714
i:  24, name:      features.module.22.weight  changing lr from: 0.089156036317650733   to: 0.087700629982311817
i:  25, name:        features.module.22.bias  changing lr from: 0.089536015065603713   to: 0.088124978937479892
i:  26, name:      features.module.23.weight  changing lr from: 0.089902278687124082   to: 0.088534163869199559
i:  27, name:        features.module.23.bias  changing lr from: 0.090255361556918282   to: 0.088928764584270625
i:  28, name:      features.module.25.weight  changing lr from: 0.090595776821968566   to: 0.089309338742826899
i:  29, name:        features.module.25.bias  changing lr from: 0.090924017163167836   to: 0.089676422570788117
i:  30, name:      features.module.26.weight  changing lr from: 0.091240555543472129   to: 0.090030531569488689
i:  31, name:        features.module.26.bias  changing lr from: 0.091545845940761761   to: 0.090372161219369779
i:  32, name:            classifier.0.weight  changing lr from: 0.091840324063959991   to: 0.090701787675119189
i:  33, name:              classifier.0.bias  changing lr from: 0.092124408051269327   to: 0.091019868450079922
i:  34, name:            classifier.3.weight  changing lr from: 0.092398499149654312   to: 0.091326843088131035
i:  35, name:              classifier.3.bias  changing lr from: 0.092662982374932951   to: 0.091623133821577579
i:  36, name:            classifier.6.weight  changing lr from: 0.092918227152038457   to: 0.091909146213878468
i:  37, name:              classifier.6.bias  changing lr from: 0.093164587935185436   to: 0.092185269786293558



# Switched to train mode...
Epoch: [17][  0/391]	Time  0.173 ( 0.173)	Data  0.141 ( 0.141)	Loss 1.5947e+00 (1.5947e+00)	Acc@1  51.56 ( 51.56)	Acc@5  85.94 ( 85.94)
Epoch: [17][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.014)	Loss 1.7520e+00 (1.6058e+00)	Acc@1  47.66 ( 56.61)	Acc@5  82.03 ( 84.16)
Epoch: [17][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 1.5869e+00 (1.6201e+00)	Acc@1  51.56 ( 55.58)	Acc@5  84.38 ( 84.15)
Epoch: [17][ 30/391]	Time  0.026 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.5625e+00 (1.6367e+00)	Acc@1  56.25 ( 55.42)	Acc@5  86.72 ( 84.35)
Epoch: [17][ 40/391]	Time  0.020 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.2529e+00 (1.6294e+00)	Acc@1  60.94 ( 55.41)	Acc@5  92.19 ( 84.22)
Epoch: [17][ 50/391]	Time  0.041 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.7646e+00 (1.6237e+00)	Acc@1  52.34 ( 55.25)	Acc@5  79.69 ( 84.59)
Epoch: [17][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9463e+00 (1.6269e+00)	Acc@1  46.09 ( 55.15)	Acc@5  78.91 ( 84.52)
Epoch: [17][ 70/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.6494e+00 (1.6334e+00)	Acc@1  57.81 ( 55.00)	Acc@5  85.16 ( 84.45)
Epoch: [17][ 80/391]	Time  0.039 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6152e+00 (1.6344e+00)	Acc@1  51.56 ( 54.82)	Acc@5  82.81 ( 84.36)
Epoch: [17][ 90/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6094e+00 (1.6343e+00)	Acc@1  54.69 ( 54.77)	Acc@5  85.16 ( 84.49)
Epoch: [17][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6387e+00 (1.6279e+00)	Acc@1  58.59 ( 55.00)	Acc@5  81.25 ( 84.61)
Epoch: [17][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7617e+00 (1.6253e+00)	Acc@1  52.34 ( 55.07)	Acc@5  85.16 ( 84.66)
Epoch: [17][120/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6250e+00 (1.6260e+00)	Acc@1  52.34 ( 55.07)	Acc@5  84.38 ( 84.61)
Epoch: [17][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6553e+00 (1.6306e+00)	Acc@1  51.56 ( 55.02)	Acc@5  83.59 ( 84.45)
Epoch: [17][140/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7197e+00 (1.6271e+00)	Acc@1  54.69 ( 55.03)	Acc@5  85.16 ( 84.55)
Epoch: [17][150/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8799e+00 (1.6286e+00)	Acc@1  49.22 ( 55.11)	Acc@5  77.34 ( 84.46)
Epoch: [17][160/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6611e+00 (1.6307e+00)	Acc@1  53.91 ( 55.05)	Acc@5  84.38 ( 84.41)
Epoch: [17][170/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8057e+00 (1.6308e+00)	Acc@1  46.09 ( 55.06)	Acc@5  79.69 ( 84.43)
Epoch: [17][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8535e+00 (1.6328e+00)	Acc@1  50.00 ( 55.05)	Acc@5  82.81 ( 84.41)
Epoch: [17][190/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.4268e+00 (1.6310e+00)	Acc@1  64.84 ( 55.13)	Acc@5  85.16 ( 84.44)
Epoch: [17][200/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6357e+00 (1.6328e+00)	Acc@1  53.91 ( 55.09)	Acc@5  81.25 ( 84.39)
Epoch: [17][210/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6328e+00 (1.6328e+00)	Acc@1  57.81 ( 55.13)	Acc@5  82.81 ( 84.36)
Epoch: [17][220/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3594e+00 (1.6306e+00)	Acc@1  61.72 ( 55.18)	Acc@5  82.81 ( 84.34)
Epoch: [17][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9209e+00 (1.6350e+00)	Acc@1  47.66 ( 55.11)	Acc@5  82.03 ( 84.27)
Epoch: [17][240/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.4961e+00 (1.6371e+00)	Acc@1  56.25 ( 55.06)	Acc@5  84.38 ( 84.20)
Epoch: [17][250/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6260e+00 (1.6405e+00)	Acc@1  57.81 ( 55.02)	Acc@5  84.38 ( 84.16)
Epoch: [17][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6611e+00 (1.6426e+00)	Acc@1  56.25 ( 54.98)	Acc@5  86.72 ( 84.11)
Epoch: [17][270/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4570e+00 (1.6418e+00)	Acc@1  57.03 ( 55.02)	Acc@5  88.28 ( 84.16)
Epoch: [17][280/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8857e+00 (1.6444e+00)	Acc@1  48.44 ( 55.00)	Acc@5  81.25 ( 84.08)
Epoch: [17][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6084e+00 (1.6505e+00)	Acc@1  53.12 ( 54.92)	Acc@5  85.94 ( 83.95)
Epoch: [17][300/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.7148e+00 (1.6522e+00)	Acc@1  50.78 ( 54.88)	Acc@5  86.72 ( 83.95)
Epoch: [17][310/391]	Time  0.023 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.6377e+00 (1.6521e+00)	Acc@1  47.66 ( 54.86)	Acc@5  85.94 ( 83.98)
Epoch: [17][320/391]	Time  0.042 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.6943e+00 (1.6490e+00)	Acc@1  51.56 ( 54.93)	Acc@5  77.34 ( 84.00)
Epoch: [17][330/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3398e+00 (1.6490e+00)	Acc@1  55.47 ( 54.90)	Acc@5  92.19 ( 84.03)
Epoch: [17][340/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.8105e+00 (1.6510e+00)	Acc@1  52.34 ( 54.82)	Acc@5  79.69 ( 84.00)
Epoch: [17][350/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8066e+00 (1.6522e+00)	Acc@1  51.56 ( 54.79)	Acc@5  80.47 ( 84.00)
Epoch: [17][360/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6914e+00 (1.6539e+00)	Acc@1  54.69 ( 54.76)	Acc@5  81.25 ( 83.95)
Epoch: [17][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7412e+00 (1.6543e+00)	Acc@1  51.56 ( 54.75)	Acc@5  80.47 ( 83.91)
Epoch: [17][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7412e+00 (1.6543e+00)	Acc@1  50.00 ( 54.77)	Acc@5  82.03 ( 83.90)
Epoch: [17][390/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9541e+00 (1.6555e+00)	Acc@1  45.00 ( 54.72)	Acc@5  78.75 ( 83.89)
## e[17] optimizer.zero_grad (sum) time: 0.11471843719482422
## e[17]       loss.backward (sum) time: 2.2916555404663086
## e[17]      optimizer.step (sum) time: 0.9717812538146973
## epoch[17] training(only) time: 10.037268161773682
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.8779e+00 (1.8779e+00)	Acc@1  50.00 ( 50.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.7949e+00 (1.8274e+00)	Acc@1  49.00 ( 50.91)	Acc@5  84.00 ( 80.18)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 1.7686e+00 (1.8361e+00)	Acc@1  56.00 ( 51.62)	Acc@5  77.00 ( 79.95)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 2.0020e+00 (1.8342e+00)	Acc@1  43.00 ( 51.42)	Acc@5  82.00 ( 80.19)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.7402e+00 (1.8454e+00)	Acc@1  52.00 ( 51.24)	Acc@5  83.00 ( 80.37)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.8574e+00 (1.8651e+00)	Acc@1  53.00 ( 50.86)	Acc@5  74.00 ( 80.08)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.9355e+00 (1.8521e+00)	Acc@1  50.00 ( 51.08)	Acc@5  81.00 ( 80.26)
Test: [ 70/100]	Time  0.017 ( 0.016)	Loss 2.0273e+00 (1.8504e+00)	Acc@1  48.00 ( 51.28)	Acc@5  78.00 ( 80.30)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.7109e+00 (1.8550e+00)	Acc@1  54.00 ( 51.19)	Acc@5  79.00 ( 80.12)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 1.9082e+00 (1.8437e+00)	Acc@1  50.00 ( 51.53)	Acc@5  82.00 ( 80.30)
 * Acc@1 51.730 Acc@5 80.370
### epoch[17] execution time: 11.619869232177734
EPOCH 18
i:   0, name:       features.module.0.weight  changing lr from: 0.071012453525392968   to: 0.067844492669611026
i:   1, name:         features.module.0.bias  changing lr from: 0.072041770399872848   to: 0.068964977431979199
i:   2, name:       features.module.1.weight  changing lr from: 0.073034215703717806   to: 0.070046680674923173
i:   3, name:         features.module.1.bias  changing lr from: 0.073990965839705644   to: 0.071090717886643187
i:   4, name:       features.module.4.weight  changing lr from: 0.074913185731289966   to: 0.072098208301824104
i:   5, name:         features.module.4.bias  changing lr from: 0.075802025013955707   to: 0.073070269269876448
i:   6, name:       features.module.5.weight  changing lr from: 0.076658614862033236   to: 0.074008011443134722
i:   7, name:         features.module.5.bias  changing lr from: 0.077484065373501387   to: 0.074912534690733765
i:   8, name:       features.module.8.weight  changing lr from: 0.078279463443731379   to: 0.075784924653613625
i:   9, name:         features.module.8.bias  changing lr from: 0.079045871066707074   to: 0.076626249864933924
i:  10, name:       features.module.9.weight  changing lr from: 0.079784324009076743   to: 0.077437559368177505
i:  11, name:         features.module.9.bias  changing lr from: 0.080495830808514440   to: 0.078219880772453121
i:  12, name:      features.module.11.weight  changing lr from: 0.081181372053356793   to: 0.078974218691031675
i:  13, name:        features.module.11.bias  changing lr from: 0.081841899905398127   to: 0.079701553515029111
i:  14, name:      features.module.12.weight  changing lr from: 0.082478337832121607   to: 0.080402840479438367
i:  15, name:        features.module.12.bias  changing lr from: 0.083091580518572003   to: 0.081079008983464554
i:  16, name:      features.module.15.weight  changing lr from: 0.083682493932580829   to: 0.081730962131382034
i:  17, name:        features.module.15.bias  changing lr from: 0.084251915520179174   to: 0.082359576463955114
i:  18, name:      features.module.16.weight  changing lr from: 0.084800654510817655   to: 0.082965701853886631
i:  19, name:        features.module.16.bias  changing lr from: 0.085329492314488858   to: 0.083550161541819823
i:  20, name:      features.module.18.weight  changing lr from: 0.085839182995050833   to: 0.084113752292156105
i:  21, name:        features.module.18.bias  changing lr from: 0.086330453806005486   to: 0.084657244650392593
i:  22, name:      features.module.19.weight  changing lr from: 0.086804005776722176   to: 0.085181383285863513
i:  23, name:        features.module.19.bias  changing lr from: 0.087260514338635714   to: 0.085686887405710357
i:  24, name:      features.module.22.weight  changing lr from: 0.087700629982311817   to: 0.086174451227635118
i:  25, name:        features.module.22.bias  changing lr from: 0.088124978937479892   to: 0.086644744500529947
i:  26, name:      features.module.23.weight  changing lr from: 0.088534163869199559   to: 0.087098413063442667
i:  27, name:        features.module.23.bias  changing lr from: 0.088928764584270625   to: 0.087536079434553132
i:  28, name:      features.module.25.weight  changing lr from: 0.089309338742826899   to: 0.087958343422912699
i:  29, name:        features.module.25.bias  changing lr from: 0.089676422570788117   to: 0.088365782756654132
i:  30, name:      features.module.26.weight  changing lr from: 0.090030531569488689   to: 0.088758953722225656
i:  31, name:        features.module.26.bias  changing lr from: 0.090372161219369779   to: 0.089138391809951603
i:  32, name:            classifier.0.weight  changing lr from: 0.090701787675119189   to: 0.089504612361883118
i:  33, name:              classifier.0.bias  changing lr from: 0.091019868450079922   to: 0.089858111218487427
i:  34, name:            classifier.3.weight  changing lr from: 0.091326843088131035   to: 0.090199365361238695
i:  35, name:              classifier.3.bias  changing lr from: 0.091623133821577579   to: 0.090528833548627119
i:  36, name:            classifier.6.weight  changing lr from: 0.091909146213878468   to: 0.090846956943502477
i:  37, name:              classifier.6.bias  changing lr from: 0.092185269786293558   to: 0.091154159730017836



# Switched to train mode...
Epoch: [18][  0/391]	Time  0.173 ( 0.173)	Data  0.145 ( 0.145)	Loss 1.2822e+00 (1.2822e+00)	Acc@1  64.06 ( 64.06)	Acc@5  92.19 ( 92.19)
Epoch: [18][ 10/391]	Time  0.022 ( 0.038)	Data  0.001 ( 0.015)	Loss 1.5488e+00 (1.5177e+00)	Acc@1  57.03 ( 57.74)	Acc@5  86.72 ( 86.79)
Epoch: [18][ 20/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.009)	Loss 1.6445e+00 (1.5351e+00)	Acc@1  56.25 ( 57.59)	Acc@5  82.81 ( 85.94)
Epoch: [18][ 30/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.9258e+00 (1.5569e+00)	Acc@1  50.00 ( 57.89)	Acc@5  77.34 ( 85.13)
Epoch: [18][ 40/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.6738e+00 (1.5567e+00)	Acc@1  50.00 ( 57.68)	Acc@5  85.94 ( 85.06)
Epoch: [18][ 50/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.4365e+00 (1.5490e+00)	Acc@1  64.84 ( 57.72)	Acc@5  85.16 ( 85.46)
Epoch: [18][ 60/391]	Time  0.020 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.8145e+00 (1.5566e+00)	Acc@1  51.56 ( 57.48)	Acc@5  78.91 ( 85.37)
Epoch: [18][ 70/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4678e+00 (1.5501e+00)	Acc@1  59.38 ( 57.72)	Acc@5  86.72 ( 85.38)
Epoch: [18][ 80/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.3848e+00 (1.5435e+00)	Acc@1  60.16 ( 57.81)	Acc@5  89.06 ( 85.45)
Epoch: [18][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8564e+00 (1.5492e+00)	Acc@1  48.44 ( 57.68)	Acc@5  82.81 ( 85.35)
Epoch: [18][100/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6426e+00 (1.5525e+00)	Acc@1  56.25 ( 57.61)	Acc@5  86.72 ( 85.28)
Epoch: [18][110/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6895e+00 (1.5540e+00)	Acc@1  57.03 ( 57.64)	Acc@5  85.16 ( 85.41)
Epoch: [18][120/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4795e+00 (1.5551e+00)	Acc@1  56.25 ( 57.46)	Acc@5  85.94 ( 85.43)
Epoch: [18][130/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6172e+00 (1.5566e+00)	Acc@1  58.59 ( 57.44)	Acc@5  82.81 ( 85.41)
Epoch: [18][140/391]	Time  0.048 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3398e+00 (1.5534e+00)	Acc@1  59.38 ( 57.45)	Acc@5  87.50 ( 85.45)
Epoch: [18][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4736e+00 (1.5531e+00)	Acc@1  59.38 ( 57.46)	Acc@5  83.59 ( 85.43)
Epoch: [18][160/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4834e+00 (1.5587e+00)	Acc@1  57.03 ( 57.36)	Acc@5  89.06 ( 85.36)
Epoch: [18][170/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.4912e+00 (1.5599e+00)	Acc@1  59.38 ( 57.27)	Acc@5  89.06 ( 85.30)
Epoch: [18][180/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6934e+00 (1.5626e+00)	Acc@1  52.34 ( 57.14)	Acc@5  83.59 ( 85.21)
Epoch: [18][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8779e+00 (1.5652e+00)	Acc@1  53.12 ( 57.16)	Acc@5  75.00 ( 85.06)
Epoch: [18][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4131e+00 (1.5647e+00)	Acc@1  61.72 ( 57.21)	Acc@5  86.72 ( 85.03)
Epoch: [18][210/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.6553e+00 (1.5643e+00)	Acc@1  59.38 ( 57.22)	Acc@5  85.94 ( 85.11)
Epoch: [18][220/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3682e+00 (1.5645e+00)	Acc@1  60.94 ( 57.24)	Acc@5  85.16 ( 85.14)
Epoch: [18][230/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0410e+00 (1.5712e+00)	Acc@1  46.09 ( 57.05)	Acc@5  76.56 ( 85.08)
Epoch: [18][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4209e+00 (1.5742e+00)	Acc@1  60.16 ( 56.95)	Acc@5  88.28 ( 85.04)
Epoch: [18][250/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.4414e+00 (1.5756e+00)	Acc@1  57.81 ( 56.85)	Acc@5  91.41 ( 85.03)
Epoch: [18][260/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.4727e+00 (1.5764e+00)	Acc@1  57.81 ( 56.81)	Acc@5  88.28 ( 85.05)
Epoch: [18][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6240e+00 (1.5775e+00)	Acc@1  58.59 ( 56.82)	Acc@5  82.81 ( 84.99)
Epoch: [18][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6260e+00 (1.5810e+00)	Acc@1  51.56 ( 56.71)	Acc@5  86.72 ( 84.96)
Epoch: [18][290/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.4160e+00 (1.5844e+00)	Acc@1  62.50 ( 56.67)	Acc@5  86.72 ( 84.91)
Epoch: [18][300/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.6396e+00 (1.5862e+00)	Acc@1  56.25 ( 56.62)	Acc@5  85.16 ( 84.89)
Epoch: [18][310/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5684e+00 (1.5898e+00)	Acc@1  56.25 ( 56.55)	Acc@5  84.38 ( 84.81)
Epoch: [18][320/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2930e+00 (1.5903e+00)	Acc@1  64.84 ( 56.50)	Acc@5  86.72 ( 84.77)
Epoch: [18][330/391]	Time  0.043 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.8584e+00 (1.5920e+00)	Acc@1  46.88 ( 56.43)	Acc@5  84.38 ( 84.76)
Epoch: [18][340/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.6592e+00 (1.5941e+00)	Acc@1  53.12 ( 56.37)	Acc@5  84.38 ( 84.74)
Epoch: [18][350/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5410e+00 (1.5945e+00)	Acc@1  52.34 ( 56.32)	Acc@5  89.06 ( 84.76)
Epoch: [18][360/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.6289e+00 (1.5955e+00)	Acc@1  57.03 ( 56.32)	Acc@5  83.59 ( 84.74)
Epoch: [18][370/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5059e+00 (1.5950e+00)	Acc@1  60.94 ( 56.34)	Acc@5  88.28 ( 84.75)
Epoch: [18][380/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8008e+00 (1.5977e+00)	Acc@1  55.47 ( 56.30)	Acc@5  79.69 ( 84.72)
Epoch: [18][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5479e+00 (1.5971e+00)	Acc@1  60.00 ( 56.29)	Acc@5  82.50 ( 84.72)
## e[18] optimizer.zero_grad (sum) time: 0.11383771896362305
## e[18]       loss.backward (sum) time: 2.254729986190796
## e[18]      optimizer.step (sum) time: 0.970029354095459
## epoch[18] training(only) time: 10.02814793586731
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.8594e+00 (1.8594e+00)	Acc@1  53.00 ( 53.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 1.8389e+00 (1.7568e+00)	Acc@1  46.00 ( 52.45)	Acc@5  86.00 ( 82.73)
Test: [ 20/100]	Time  0.030 ( 0.019)	Loss 1.8564e+00 (1.7758e+00)	Acc@1  51.00 ( 51.52)	Acc@5  83.00 ( 81.90)
Test: [ 30/100]	Time  0.014 ( 0.018)	Loss 1.8604e+00 (1.7717e+00)	Acc@1  50.00 ( 51.94)	Acc@5  80.00 ( 82.00)
Test: [ 40/100]	Time  0.019 ( 0.017)	Loss 1.6758e+00 (1.7526e+00)	Acc@1  57.00 ( 52.37)	Acc@5  83.00 ( 82.24)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.8281e+00 (1.7788e+00)	Acc@1  53.00 ( 51.94)	Acc@5  77.00 ( 81.69)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.8242e+00 (1.7711e+00)	Acc@1  54.00 ( 52.02)	Acc@5  87.00 ( 81.93)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 1.8604e+00 (1.7705e+00)	Acc@1  49.00 ( 52.10)	Acc@5  83.00 ( 82.07)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.8447e+00 (1.7777e+00)	Acc@1  51.00 ( 51.95)	Acc@5  76.00 ( 81.72)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.7949e+00 (1.7660e+00)	Acc@1  51.00 ( 52.23)	Acc@5  82.00 ( 81.86)
 * Acc@1 52.510 Acc@5 81.790
### epoch[18] execution time: 11.629389524459839
EPOCH 19
i:   0, name:       features.module.0.weight  changing lr from: 0.067844492669611026   to: 0.064598267368231238
i:   1, name:         features.module.0.bias  changing lr from: 0.068964977431979199   to: 0.065807933051206705
i:   2, name:       features.module.1.weight  changing lr from: 0.070046680674923173   to: 0.066977322575777348
i:   3, name:         features.module.1.bias  changing lr from: 0.071090717886643187   to: 0.068107448163742035
i:   4, name:       features.module.4.weight  changing lr from: 0.072098208301824104   to: 0.069199344701685969
i:   5, name:         features.module.4.bias  changing lr from: 0.073070269269876448   to: 0.070254061996985653
i:   6, name:       features.module.5.weight  changing lr from: 0.074008011443134722   to: 0.071272658056816041
i:   7, name:         features.module.5.bias  changing lr from: 0.074912534690733765   to: 0.072256193278612088
i:   8, name:       features.module.8.weight  changing lr from: 0.075784924653613625   to: 0.073205725451322409
i:   9, name:         features.module.8.bias  changing lr from: 0.076626249864933924   to: 0.074122305476761380
i:  10, name:       features.module.9.weight  changing lr from: 0.077437559368177505   to: 0.075006973729471516
i:  11, name:         features.module.9.bias  changing lr from: 0.078219880772453121   to: 0.075860756981801905
i:  12, name:      features.module.11.weight  changing lr from: 0.078974218691031675   to: 0.076684665828446366
i:  13, name:        features.module.11.bias  changing lr from: 0.079701553515029111   to: 0.077479692551524385
i:  14, name:      features.module.12.weight  changing lr from: 0.080402840479438367   to: 0.078246809373477721
i:  15, name:        features.module.12.bias  changing lr from: 0.081079008983464554   to: 0.078986967050652918
i:  16, name:      features.module.15.weight  changing lr from: 0.081730962131382034   to: 0.079701093765490574
i:  17, name:        features.module.15.bias  changing lr from: 0.082359576463955114   to: 0.080390094279794821
i:  18, name:      features.module.16.weight  changing lr from: 0.082965701853886631   to: 0.081054849315655000
i:  19, name:        features.module.16.bias  changing lr from: 0.083550161541819823   to: 0.081696215134275038
i:  20, name:      features.module.18.weight  changing lr from: 0.084113752292156105   to: 0.082315023286276726
i:  21, name:        features.module.18.bias  changing lr from: 0.084657244650392593   to: 0.082912080510010519
i:  22, name:      features.module.19.weight  changing lr from: 0.085181383285863513   to: 0.083488168757069228
i:  23, name:        features.module.19.bias  changing lr from: 0.085686887405710357   to: 0.084044045326583189
i:  24, name:      features.module.22.weight  changing lr from: 0.086174451227635118   to: 0.084580443092006144
i:  25, name:        features.module.22.bias  changing lr from: 0.086644744500529947   to: 0.085098070806008230
i:  26, name:      features.module.23.weight  changing lr from: 0.087098413063442667   to: 0.085597613470792977
i:  27, name:        features.module.23.bias  changing lr from: 0.087536079434553132   to: 0.086079732762674985
i:  28, name:      features.module.25.weight  changing lr from: 0.087958343422912699   to: 0.086545067501108602
i:  29, name:        features.module.25.bias  changing lr from: 0.088365782756654132   to: 0.086994234153564665
i:  30, name:      features.module.26.weight  changing lr from: 0.088758953722225656   to: 0.087427827368725911
i:  31, name:        features.module.26.bias  changing lr from: 0.089138391809951603   to: 0.087846420531427133
i:  32, name:            classifier.0.weight  changing lr from: 0.089504612361883118   to: 0.088250566333614444
i:  33, name:              classifier.0.bias  changing lr from: 0.089858111218487427   to: 0.088640797356351914
i:  34, name:            classifier.3.weight  changing lr from: 0.090199365361238695   to: 0.089017626658571672
i:  35, name:              classifier.3.bias  changing lr from: 0.090528833548627119   to: 0.089381548368855729
i:  36, name:            classifier.6.weight  changing lr from: 0.090846956943502477   to: 0.089733038277062097
i:  37, name:              classifier.6.bias  changing lr from: 0.091154159730017836   to: 0.090072554423071077



# Switched to train mode...
Epoch: [19][  0/391]	Time  0.173 ( 0.173)	Data  0.139 ( 0.139)	Loss 1.5947e+00 (1.5947e+00)	Acc@1  56.25 ( 56.25)	Acc@5  84.38 ( 84.38)
Epoch: [19][ 10/391]	Time  0.022 ( 0.038)	Data  0.001 ( 0.014)	Loss 1.2705e+00 (1.4378e+00)	Acc@1  64.84 ( 58.66)	Acc@5  89.06 ( 86.72)
Epoch: [19][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 1.2031e+00 (1.4566e+00)	Acc@1  64.06 ( 58.52)	Acc@5  92.19 ( 86.94)
Epoch: [19][ 30/391]	Time  0.021 ( 0.029)	Data  0.003 ( 0.006)	Loss 1.6094e+00 (1.4707e+00)	Acc@1  53.12 ( 58.19)	Acc@5  87.50 ( 86.72)
Epoch: [19][ 40/391]	Time  0.036 ( 0.028)	Data  0.003 ( 0.005)	Loss 1.5391e+00 (1.4538e+00)	Acc@1  59.38 ( 58.92)	Acc@5  88.28 ( 86.89)
Epoch: [19][ 50/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.6387e+00 (1.4759e+00)	Acc@1  57.81 ( 58.64)	Acc@5  84.38 ( 86.76)
Epoch: [19][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6211e+00 (1.4817e+00)	Acc@1  53.91 ( 58.54)	Acc@5  82.03 ( 86.57)
Epoch: [19][ 70/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5547e+00 (1.4827e+00)	Acc@1  60.16 ( 58.58)	Acc@5  83.59 ( 86.53)
Epoch: [19][ 80/391]	Time  0.028 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.4834e+00 (1.4914e+00)	Acc@1  53.12 ( 58.27)	Acc@5  90.62 ( 86.45)
Epoch: [19][ 90/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2939e+00 (1.4996e+00)	Acc@1  62.50 ( 58.18)	Acc@5  87.50 ( 86.39)
Epoch: [19][100/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3564e+00 (1.4994e+00)	Acc@1  60.94 ( 58.28)	Acc@5  86.72 ( 86.32)
Epoch: [19][110/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7285e+00 (1.5050e+00)	Acc@1  58.59 ( 58.27)	Acc@5  84.38 ( 86.28)
Epoch: [19][120/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4307e+00 (1.5050e+00)	Acc@1  57.03 ( 58.26)	Acc@5  89.84 ( 86.17)
Epoch: [19][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5283e+00 (1.5094e+00)	Acc@1  57.81 ( 58.15)	Acc@5  87.50 ( 86.10)
Epoch: [19][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6631e+00 (1.5211e+00)	Acc@1  53.12 ( 57.98)	Acc@5  89.84 ( 85.95)
Epoch: [19][150/391]	Time  0.038 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.6445e+00 (1.5247e+00)	Acc@1  56.25 ( 57.97)	Acc@5  84.38 ( 85.89)
Epoch: [19][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5137e+00 (1.5247e+00)	Acc@1  58.59 ( 57.98)	Acc@5  86.72 ( 85.93)
Epoch: [19][170/391]	Time  0.035 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.4766e+00 (1.5242e+00)	Acc@1  57.81 ( 57.94)	Acc@5  86.72 ( 86.01)
Epoch: [19][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5762e+00 (1.5242e+00)	Acc@1  60.16 ( 57.88)	Acc@5  87.50 ( 86.02)
Epoch: [19][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7275e+00 (1.5253e+00)	Acc@1  55.47 ( 57.86)	Acc@5  82.81 ( 85.99)
Epoch: [19][200/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4287e+00 (1.5231e+00)	Acc@1  60.94 ( 57.89)	Acc@5  84.38 ( 85.99)
Epoch: [19][210/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6631e+00 (1.5286e+00)	Acc@1  53.12 ( 57.76)	Acc@5  85.94 ( 85.91)
Epoch: [19][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5479e+00 (1.5304e+00)	Acc@1  59.38 ( 57.73)	Acc@5  85.94 ( 85.88)
Epoch: [19][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2783e+00 (1.5255e+00)	Acc@1  60.94 ( 57.91)	Acc@5  88.28 ( 85.92)
Epoch: [19][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5439e+00 (1.5242e+00)	Acc@1  57.81 ( 57.96)	Acc@5  82.81 ( 85.90)
Epoch: [19][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5752e+00 (1.5224e+00)	Acc@1  55.47 ( 57.97)	Acc@5  82.81 ( 85.97)
Epoch: [19][260/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6494e+00 (1.5232e+00)	Acc@1  53.91 ( 57.94)	Acc@5  87.50 ( 86.00)
Epoch: [19][270/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6436e+00 (1.5273e+00)	Acc@1  57.81 ( 57.92)	Acc@5  81.25 ( 85.89)
Epoch: [19][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7227e+00 (1.5309e+00)	Acc@1  53.12 ( 57.89)	Acc@5  88.28 ( 85.84)
Epoch: [19][290/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7354e+00 (1.5335e+00)	Acc@1  53.91 ( 57.77)	Acc@5  80.47 ( 85.81)
Epoch: [19][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5176e+00 (1.5372e+00)	Acc@1  57.81 ( 57.72)	Acc@5  85.94 ( 85.71)
Epoch: [19][310/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6719e+00 (1.5372e+00)	Acc@1  55.47 ( 57.73)	Acc@5  86.72 ( 85.72)
Epoch: [19][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4385e+00 (1.5383e+00)	Acc@1  61.72 ( 57.72)	Acc@5  87.50 ( 85.70)
Epoch: [19][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4482e+00 (1.5417e+00)	Acc@1  59.38 ( 57.65)	Acc@5  88.28 ( 85.67)
Epoch: [19][340/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5957e+00 (1.5411e+00)	Acc@1  53.12 ( 57.65)	Acc@5  88.28 ( 85.69)
Epoch: [19][350/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4668e+00 (1.5419e+00)	Acc@1  60.94 ( 57.61)	Acc@5  89.84 ( 85.69)
Epoch: [19][360/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5459e+00 (1.5419e+00)	Acc@1  53.12 ( 57.56)	Acc@5  85.16 ( 85.69)
Epoch: [19][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3203e+00 (1.5409e+00)	Acc@1  61.72 ( 57.58)	Acc@5  89.84 ( 85.71)
Epoch: [19][380/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.5684e+00 (1.5388e+00)	Acc@1  57.81 ( 57.63)	Acc@5  82.03 ( 85.71)
Epoch: [19][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5010e+00 (1.5396e+00)	Acc@1  61.25 ( 57.60)	Acc@5  88.75 ( 85.71)
## e[19] optimizer.zero_grad (sum) time: 0.11386942863464355
## e[19]       loss.backward (sum) time: 2.2552528381347656
## e[19]      optimizer.step (sum) time: 0.9713475704193115
## epoch[19] training(only) time: 10.018348932266235
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.6719e+00 (1.6719e+00)	Acc@1  58.00 ( 58.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 1.8535e+00 (1.8087e+00)	Acc@1  48.00 ( 52.73)	Acc@5  84.00 ( 81.09)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.7520e+00 (1.8195e+00)	Acc@1  56.00 ( 52.90)	Acc@5  78.00 ( 80.62)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.7510e+00 (1.8115e+00)	Acc@1  52.00 ( 52.19)	Acc@5  84.00 ( 80.84)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.6914e+00 (1.8019e+00)	Acc@1  52.00 ( 52.20)	Acc@5  81.00 ( 80.61)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 1.8076e+00 (1.8206e+00)	Acc@1  53.00 ( 52.12)	Acc@5  79.00 ( 80.37)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 1.7783e+00 (1.8062e+00)	Acc@1  54.00 ( 52.23)	Acc@5  76.00 ( 80.49)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 1.8936e+00 (1.8105e+00)	Acc@1  59.00 ( 52.38)	Acc@5  81.00 ( 80.65)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8887e+00 (1.8217e+00)	Acc@1  50.00 ( 52.05)	Acc@5  73.00 ( 80.58)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 2.1289e+00 (1.8164e+00)	Acc@1  48.00 ( 52.18)	Acc@5  71.00 ( 80.66)
 * Acc@1 52.300 Acc@5 80.870
### epoch[19] execution time: 11.567310333251953
EPOCH 20
i:   0, name:       features.module.0.weight  changing lr from: 0.064598267368231238   to: 0.061288860534611772
i:   1, name:         features.module.0.bias  changing lr from: 0.065807933051206705   to: 0.062584740101437997
i:   2, name:       features.module.1.weight  changing lr from: 0.066977322575777348   to: 0.063839327131812534
i:   3, name:         features.module.1.bias  changing lr from: 0.068107448163742035   to: 0.065053484569572037
i:   4, name:       features.module.4.weight  changing lr from: 0.069199344701685969   to: 0.066228120770569540
i:   5, name:         features.module.4.bias  changing lr from: 0.070254061996985653   to: 0.067364179366916660
i:   6, name:       features.module.5.weight  changing lr from: 0.071272658056816041   to: 0.068462630372195563
i:   7, name:         features.module.5.bias  changing lr from: 0.072256193278612088   to: 0.069524462399059822
i:   8, name:       features.module.8.weight  changing lr from: 0.073205725451322409   to: 0.070550675872417437
i:   9, name:         features.module.8.bias  changing lr from: 0.074122305476761380   to: 0.071542277132294871
i:  10, name:       features.module.9.weight  changing lr from: 0.075006973729471516   to: 0.072500273330537462
i:  11, name:         features.module.9.bias  changing lr from: 0.075860756981801905   to: 0.073425668034748634
i:  12, name:      features.module.11.weight  changing lr from: 0.076684665828446366   to: 0.074319457461341373
i:  13, name:        features.module.11.bias  changing lr from: 0.077479692551524385   to: 0.075182627267320709
i:  14, name:      features.module.12.weight  changing lr from: 0.078246809373477721   to: 0.076016149837477620
i:  15, name:        features.module.12.bias  changing lr from: 0.078986967050652918   to: 0.076820982010098862
i:  16, name:      features.module.15.weight  changing lr from: 0.079701093765490574   to: 0.077598063190134348
i:  17, name:        features.module.15.bias  changing lr from: 0.080390094279794821   to: 0.078348313804052383
i:  18, name:      features.module.16.weight  changing lr from: 0.081054849315655000   to: 0.079072634055403587
i:  19, name:        features.module.16.bias  changing lr from: 0.081696215134275038   to: 0.079771902944441100
i:  20, name:      features.module.18.weight  changing lr from: 0.082315023286276726   to: 0.080446977519054289
i:  21, name:        features.module.18.bias  changing lr from: 0.082912080510010519   to: 0.081098692327794278
i:  22, name:      features.module.19.weight  changing lr from: 0.083488168757069228   to: 0.081727859048943599
i:  23, name:        features.module.19.bias  changing lr from: 0.084044045326583189   to: 0.082335266272436397
i:  24, name:      features.module.22.weight  changing lr from: 0.084580443092006144   to: 0.082921679414000887
i:  25, name:        features.module.22.bias  changing lr from: 0.085098070806008230   to: 0.083487840743199218
i:  26, name:      features.module.23.weight  changing lr from: 0.085597613470792977   to: 0.084034469509106743
i:  27, name:        features.module.23.bias  changing lr from: 0.086079732762674985   to: 0.084562262149223896
i:  28, name:      features.module.25.weight  changing lr from: 0.086545067501108602   to: 0.085071892568873916
i:  29, name:        features.module.25.bias  changing lr from: 0.086994234153564665   to: 0.085564012479822552
i:  30, name:      features.module.26.weight  changing lr from: 0.087427827368725911   to: 0.086039251788184168
i:  31, name:        features.module.26.bias  changing lr from: 0.087846420531427133   to: 0.086498219022863254
i:  32, name:            classifier.0.weight  changing lr from: 0.088250566333614444   to: 0.086941501796839488
i:  33, name:              classifier.0.bias  changing lr from: 0.088640797356351914   to: 0.087369667294547981
i:  34, name:            classifier.3.weight  changing lr from: 0.089017626658571672   to: 0.087783262779447813
i:  35, name:              classifier.3.bias  changing lr from: 0.089381548368855729   to: 0.088182816116620988
i:  36, name:            classifier.6.weight  changing lr from: 0.089733038277062097   to: 0.088568836305910392
i:  37, name:              classifier.6.bias  changing lr from: 0.090072554423071077   to: 0.088941814021697804



# Switched to train mode...
Epoch: [20][  0/391]	Time  0.181 ( 0.181)	Data  0.145 ( 0.145)	Loss 1.5264e+00 (1.5264e+00)	Acc@1  62.50 ( 62.50)	Acc@5  86.72 ( 86.72)
Epoch: [20][ 10/391]	Time  0.020 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.1113e+00 (1.4246e+00)	Acc@1  66.41 ( 60.16)	Acc@5  92.19 ( 88.21)
Epoch: [20][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.6992e+00 (1.4635e+00)	Acc@1  54.69 ( 59.30)	Acc@5  84.38 ( 87.76)
Epoch: [20][ 30/391]	Time  0.021 ( 0.029)	Data  0.000 ( 0.006)	Loss 1.5957e+00 (1.4810e+00)	Acc@1  52.34 ( 58.57)	Acc@5  86.72 ( 87.53)
Epoch: [20][ 40/391]	Time  0.033 ( 0.029)	Data  0.000 ( 0.005)	Loss 1.5420e+00 (1.4545e+00)	Acc@1  57.81 ( 59.36)	Acc@5  87.50 ( 87.63)
Epoch: [20][ 50/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.3369e+00 (1.4518e+00)	Acc@1  57.81 ( 59.64)	Acc@5  88.28 ( 87.52)
Epoch: [20][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5449e+00 (1.4604e+00)	Acc@1  58.59 ( 59.35)	Acc@5  87.50 ( 87.27)
Epoch: [20][ 70/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4375e+00 (1.4552e+00)	Acc@1  59.38 ( 59.74)	Acc@5  92.19 ( 87.41)
Epoch: [20][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4766e+00 (1.4423e+00)	Acc@1  60.94 ( 60.12)	Acc@5  83.59 ( 87.58)
Epoch: [20][ 90/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.3398e+00 (1.4356e+00)	Acc@1  60.94 ( 60.24)	Acc@5  87.50 ( 87.61)
Epoch: [20][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3447e+00 (1.4345e+00)	Acc@1  60.94 ( 60.18)	Acc@5  91.41 ( 87.68)
Epoch: [20][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2646e+00 (1.4356e+00)	Acc@1  64.06 ( 60.15)	Acc@5  87.50 ( 87.53)
Epoch: [20][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5586e+00 (1.4422e+00)	Acc@1  56.25 ( 59.91)	Acc@5  86.72 ( 87.49)
Epoch: [20][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4062e+00 (1.4520e+00)	Acc@1  59.38 ( 59.61)	Acc@5  85.16 ( 87.27)
Epoch: [20][140/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7803e+00 (1.4494e+00)	Acc@1  52.34 ( 59.71)	Acc@5  87.50 ( 87.25)
Epoch: [20][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3145e+00 (1.4540e+00)	Acc@1  60.16 ( 59.61)	Acc@5  89.06 ( 87.11)
Epoch: [20][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5117e+00 (1.4586e+00)	Acc@1  56.25 ( 59.56)	Acc@5  89.84 ( 87.13)
Epoch: [20][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2021e+00 (1.4627e+00)	Acc@1  58.59 ( 59.47)	Acc@5  91.41 ( 87.01)
Epoch: [20][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3926e+00 (1.4597e+00)	Acc@1  58.59 ( 59.55)	Acc@5  88.28 ( 87.05)
Epoch: [20][190/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.5361e+00 (1.4606e+00)	Acc@1  57.81 ( 59.58)	Acc@5  89.06 ( 87.02)
Epoch: [20][200/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2598e+00 (1.4572e+00)	Acc@1  61.72 ( 59.62)	Acc@5  90.62 ( 87.05)
Epoch: [20][210/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6094e+00 (1.4605e+00)	Acc@1  57.03 ( 59.55)	Acc@5  86.72 ( 87.04)
Epoch: [20][220/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4062e+00 (1.4608e+00)	Acc@1  57.81 ( 59.54)	Acc@5  92.97 ( 87.05)
Epoch: [20][230/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6719e+00 (1.4634e+00)	Acc@1  57.03 ( 59.51)	Acc@5  84.38 ( 87.00)
Epoch: [20][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4795e+00 (1.4617e+00)	Acc@1  55.47 ( 59.54)	Acc@5  85.16 ( 87.02)
Epoch: [20][250/391]	Time  0.037 ( 0.026)	Data  0.006 ( 0.002)	Loss 1.5039e+00 (1.4622e+00)	Acc@1  55.47 ( 59.48)	Acc@5  85.94 ( 87.03)
Epoch: [20][260/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5205e+00 (1.4625e+00)	Acc@1  57.81 ( 59.45)	Acc@5  85.94 ( 87.05)
Epoch: [20][270/391]	Time  0.025 ( 0.025)	Data  0.004 ( 0.002)	Loss 1.4092e+00 (1.4623e+00)	Acc@1  57.03 ( 59.46)	Acc@5  89.06 ( 87.04)
Epoch: [20][280/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5928e+00 (1.4641e+00)	Acc@1  58.59 ( 59.44)	Acc@5  83.59 ( 87.01)
Epoch: [20][290/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7021e+00 (1.4697e+00)	Acc@1  54.69 ( 59.29)	Acc@5  82.81 ( 86.91)
Epoch: [20][300/391]	Time  0.042 ( 0.025)	Data  0.006 ( 0.002)	Loss 1.4375e+00 (1.4708e+00)	Acc@1  60.94 ( 59.29)	Acc@5  87.50 ( 86.90)
Epoch: [20][310/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5234e+00 (1.4740e+00)	Acc@1  61.72 ( 59.20)	Acc@5  81.25 ( 86.86)
Epoch: [20][320/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.4717e+00 (1.4765e+00)	Acc@1  59.38 ( 59.16)	Acc@5  83.59 ( 86.77)
Epoch: [20][330/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7227e+00 (1.4759e+00)	Acc@1  52.34 ( 59.16)	Acc@5  82.03 ( 86.76)
Epoch: [20][340/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5986e+00 (1.4751e+00)	Acc@1  60.94 ( 59.18)	Acc@5  81.25 ( 86.76)
Epoch: [20][350/391]	Time  0.028 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.2803e+00 (1.4747e+00)	Acc@1  61.72 ( 59.15)	Acc@5  90.62 ( 86.76)
Epoch: [20][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4404e+00 (1.4751e+00)	Acc@1  55.47 ( 59.13)	Acc@5  87.50 ( 86.73)
Epoch: [20][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2510e+00 (1.4770e+00)	Acc@1  64.06 ( 59.11)	Acc@5  89.84 ( 86.67)
Epoch: [20][380/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3369e+00 (1.4768e+00)	Acc@1  62.50 ( 59.13)	Acc@5  90.62 ( 86.70)
Epoch: [20][390/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.4648e+00 (1.4791e+00)	Acc@1  55.00 ( 59.05)	Acc@5  87.50 ( 86.67)
## e[20] optimizer.zero_grad (sum) time: 0.11512303352355957
## e[20]       loss.backward (sum) time: 2.275233745574951
## e[20]      optimizer.step (sum) time: 0.9734060764312744
## epoch[20] training(only) time: 9.992908477783203
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.7471e+00 (1.7471e+00)	Acc@1  60.00 ( 60.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.027 ( 0.026)	Loss 1.8271e+00 (1.7087e+00)	Acc@1  50.00 ( 55.27)	Acc@5  86.00 ( 82.45)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 1.7061e+00 (1.6990e+00)	Acc@1  57.00 ( 54.81)	Acc@5  76.00 ( 82.29)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.8164e+00 (1.7124e+00)	Acc@1  54.00 ( 54.23)	Acc@5  79.00 ( 82.19)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.5703e+00 (1.7054e+00)	Acc@1  56.00 ( 54.22)	Acc@5  87.00 ( 82.51)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.8242e+00 (1.7247e+00)	Acc@1  55.00 ( 53.96)	Acc@5  80.00 ( 82.10)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.8252e+00 (1.7171e+00)	Acc@1  53.00 ( 53.98)	Acc@5  81.00 ( 82.21)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 1.7363e+00 (1.7257e+00)	Acc@1  52.00 ( 53.79)	Acc@5  82.00 ( 82.20)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 2.0840e+00 (1.7335e+00)	Acc@1  50.00 ( 53.68)	Acc@5  72.00 ( 82.05)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.8398e+00 (1.7257e+00)	Acc@1  47.00 ( 53.98)	Acc@5  79.00 ( 82.15)
 * Acc@1 54.160 Acc@5 82.220
### epoch[20] execution time: 11.549638271331787
EPOCH 21
i:   0, name:       features.module.0.weight  changing lr from: 0.061288860534611772   to: 0.057931648642011363
i:   1, name:         features.module.0.bias  changing lr from: 0.062584740101437997   to: 0.059309796919057280
i:   2, name:       features.module.1.weight  changing lr from: 0.063839327131812534   to: 0.060646174929297560
i:   3, name:         features.module.1.bias  changing lr from: 0.065053484569572037   to: 0.061941447134141119
i:   4, name:       features.module.4.weight  changing lr from: 0.066228120770569540   to: 0.063196350051796885
i:   5, name:         features.module.4.bias  changing lr from: 0.067364179366916660   to: 0.064411679468866792
i:   6, name:       features.module.5.weight  changing lr from: 0.068462630372195563   to: 0.065588279121030810
i:   7, name:         features.module.5.bias  changing lr from: 0.069524462399059822   to: 0.066727030698274001
i:   8, name:       features.module.8.weight  changing lr from: 0.070550675872417437   to: 0.067828845042378558
i:   9, name:         features.module.8.bias  changing lr from: 0.071542277132294871   to: 0.068894654415931386
i:  10, name:       features.module.9.weight  changing lr from: 0.072500273330537462   to: 0.069925405732862184
i:  11, name:         features.module.9.bias  changing lr from: 0.073425668034748634   to: 0.070922054650530142
i:  12, name:      features.module.11.weight  changing lr from: 0.074319457461341373   to: 0.071885560432637760
i:  13, name:        features.module.11.bias  changing lr from: 0.075182627267320709   to: 0.072816881500788341
i:  14, name:      features.module.12.weight  changing lr from: 0.076016149837477620   to: 0.073716971600355050
i:  15, name:        features.module.12.bias  changing lr from: 0.076820982010098862   to: 0.074586776513525702
i:  16, name:      features.module.15.weight  changing lr from: 0.077598063190134348   to: 0.075427231258970093
i:  17, name:        features.module.15.bias  changing lr from: 0.078348313804052383   to: 0.076239257723581977
i:  18, name:      features.module.16.weight  changing lr from: 0.079072634055403587   to: 0.077023762677218652
i:  19, name:        features.module.16.bias  changing lr from: 0.079771902944441100   to: 0.077781636126332099
i:  20, name:      features.module.18.weight  changing lr from: 0.080446977519054289   to: 0.078513749966900609
i:  21, name:        features.module.18.bias  changing lr from: 0.081098692327794278   to: 0.079220956901158171
i:  22, name:      features.module.19.weight  changing lr from: 0.081727859048943599   to: 0.079904089586321683
i:  23, name:        features.module.19.bias  changing lr from: 0.082335266272436397   to: 0.080563959986862041
i:  24, name:      features.module.22.weight  changing lr from: 0.082921679414000887   to: 0.081201358904885543
i:  25, name:        features.module.22.bias  changing lr from: 0.083487840743199218   to: 0.081817055665917973
i:  26, name:      features.module.23.weight  changing lr from: 0.084034469509106743   to: 0.082411797939837356
i:  27, name:        features.module.23.bias  changing lr from: 0.084562262149223896   to: 0.082986311678912350
i:  28, name:      features.module.25.weight  changing lr from: 0.085071892568873916   to: 0.083541301156889694
i:  29, name:        features.module.25.bias  changing lr from: 0.085564012479822552   to: 0.084077449094860368
i:  30, name:      features.module.26.weight  changing lr from: 0.086039251788184168   to: 0.084595416861237116
i:  31, name:        features.module.26.bias  changing lr from: 0.086498219022863254   to: 0.085095844734613985
i:  32, name:            classifier.0.weight  changing lr from: 0.086941501796839488   to: 0.085579352219568042
i:  33, name:              classifier.0.bias  changing lr from: 0.087369667294547981   to: 0.086046538406617495
i:  34, name:            classifier.3.weight  changing lr from: 0.087783262779447813   to: 0.086497982368584303
i:  35, name:              classifier.3.bias  changing lr from: 0.088182816116620988   to: 0.086934243586532969
i:  36, name:            classifier.6.weight  changing lr from: 0.088568836305910392   to: 0.087355862399283113
i:  37, name:              classifier.6.bias  changing lr from: 0.088941814021697804   to: 0.087763360471230609



# Switched to train mode...
Epoch: [21][  0/391]	Time  0.175 ( 0.175)	Data  0.147 ( 0.147)	Loss 1.4980e+00 (1.4980e+00)	Acc@1  59.38 ( 59.38)	Acc@5  83.59 ( 83.59)
Epoch: [21][ 10/391]	Time  0.028 ( 0.039)	Data  0.003 ( 0.015)	Loss 1.4141e+00 (1.4567e+00)	Acc@1  61.72 ( 59.66)	Acc@5  88.28 ( 85.87)
Epoch: [21][ 20/391]	Time  0.029 ( 0.032)	Data  0.000 ( 0.009)	Loss 1.4541e+00 (1.4639e+00)	Acc@1  57.03 ( 58.74)	Acc@5  85.16 ( 86.09)
Epoch: [21][ 30/391]	Time  0.021 ( 0.029)	Data  0.000 ( 0.007)	Loss 1.2910e+00 (1.4481e+00)	Acc@1  59.38 ( 58.85)	Acc@5  89.84 ( 86.72)
Epoch: [21][ 40/391]	Time  0.026 ( 0.029)	Data  0.005 ( 0.006)	Loss 1.3516e+00 (1.4168e+00)	Acc@1  60.94 ( 59.62)	Acc@5  87.50 ( 87.40)
Epoch: [21][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.5381e+00 (1.4097e+00)	Acc@1  58.59 ( 60.19)	Acc@5  88.28 ( 87.53)
Epoch: [21][ 60/391]	Time  0.045 ( 0.027)	Data  0.006 ( 0.005)	Loss 1.3057e+00 (1.4019e+00)	Acc@1  65.62 ( 60.19)	Acc@5  89.06 ( 87.65)
Epoch: [21][ 70/391]	Time  0.025 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.5322e+00 (1.3945e+00)	Acc@1  59.38 ( 60.51)	Acc@5  86.72 ( 87.86)
Epoch: [21][ 80/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2998e+00 (1.3998e+00)	Acc@1  62.50 ( 60.47)	Acc@5  90.62 ( 87.71)
Epoch: [21][ 90/391]	Time  0.040 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.2158e+00 (1.3992e+00)	Acc@1  62.50 ( 60.43)	Acc@5  89.84 ( 87.81)
Epoch: [21][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3867e+00 (1.4009e+00)	Acc@1  60.16 ( 60.48)	Acc@5  91.41 ( 87.79)
Epoch: [21][110/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3320e+00 (1.4094e+00)	Acc@1  63.28 ( 60.40)	Acc@5  88.28 ( 87.72)
Epoch: [21][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2324e+00 (1.4079e+00)	Acc@1  61.72 ( 60.45)	Acc@5  88.28 ( 87.77)
Epoch: [21][130/391]	Time  0.039 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.4395e+00 (1.4113e+00)	Acc@1  62.50 ( 60.47)	Acc@5  86.72 ( 87.80)
Epoch: [21][140/391]	Time  0.041 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6230e+00 (1.4153e+00)	Acc@1  60.94 ( 60.39)	Acc@5  84.38 ( 87.65)
Epoch: [21][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4697e+00 (1.4184e+00)	Acc@1  56.25 ( 60.34)	Acc@5  87.50 ( 87.66)
Epoch: [21][160/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3379e+00 (1.4218e+00)	Acc@1  60.16 ( 60.24)	Acc@5  90.62 ( 87.61)
Epoch: [21][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5020e+00 (1.4192e+00)	Acc@1  64.06 ( 60.39)	Acc@5  85.94 ( 87.66)
Epoch: [21][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2861e+00 (1.4167e+00)	Acc@1  61.72 ( 60.50)	Acc@5  91.41 ( 87.65)
Epoch: [21][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6475e+00 (1.4187e+00)	Acc@1  56.25 ( 60.39)	Acc@5  83.59 ( 87.66)
Epoch: [21][200/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4619e+00 (1.4178e+00)	Acc@1  58.59 ( 60.40)	Acc@5  86.72 ( 87.64)
Epoch: [21][210/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7207e+00 (1.4246e+00)	Acc@1  47.66 ( 60.24)	Acc@5  82.03 ( 87.54)
Epoch: [21][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6553e+00 (1.4284e+00)	Acc@1  54.69 ( 60.14)	Acc@5  85.16 ( 87.47)
Epoch: [21][230/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4980e+00 (1.4273e+00)	Acc@1  55.47 ( 60.18)	Acc@5  89.06 ( 87.47)
Epoch: [21][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4727e+00 (1.4277e+00)	Acc@1  60.16 ( 60.21)	Acc@5  85.16 ( 87.45)
Epoch: [21][250/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.1055e+00 (1.4252e+00)	Acc@1  66.41 ( 60.23)	Acc@5  94.53 ( 87.52)
Epoch: [21][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2891e+00 (1.4248e+00)	Acc@1  67.97 ( 60.28)	Acc@5  87.50 ( 87.50)
Epoch: [21][270/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4229e+00 (1.4252e+00)	Acc@1  60.16 ( 60.26)	Acc@5  89.06 ( 87.52)
Epoch: [21][280/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4883e+00 (1.4261e+00)	Acc@1  57.81 ( 60.22)	Acc@5  85.16 ( 87.54)
Epoch: [21][290/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3311e+00 (1.4254e+00)	Acc@1  58.59 ( 60.24)	Acc@5  86.72 ( 87.52)
Epoch: [21][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7021e+00 (1.4247e+00)	Acc@1  57.81 ( 60.30)	Acc@5  84.38 ( 87.54)
Epoch: [21][310/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.5986e+00 (1.4266e+00)	Acc@1  53.91 ( 60.23)	Acc@5  87.50 ( 87.50)
Epoch: [21][320/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5332e+00 (1.4275e+00)	Acc@1  56.25 ( 60.18)	Acc@5  83.59 ( 87.47)
Epoch: [21][330/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.5322e+00 (1.4307e+00)	Acc@1  60.16 ( 60.11)	Acc@5  84.38 ( 87.41)
Epoch: [21][340/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3105e+00 (1.4324e+00)	Acc@1  65.62 ( 60.07)	Acc@5  89.84 ( 87.40)
Epoch: [21][350/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4092e+00 (1.4345e+00)	Acc@1  54.69 ( 59.99)	Acc@5  89.06 ( 87.38)
Epoch: [21][360/391]	Time  0.022 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.6484e+00 (1.4342e+00)	Acc@1  53.91 ( 60.03)	Acc@5  82.03 ( 87.36)
Epoch: [21][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4658e+00 (1.4334e+00)	Acc@1  59.38 ( 60.06)	Acc@5  90.62 ( 87.39)
Epoch: [21][380/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5947e+00 (1.4334e+00)	Acc@1  60.94 ( 60.10)	Acc@5  87.50 ( 87.43)
Epoch: [21][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1143e+00 (1.4337e+00)	Acc@1  73.75 ( 60.11)	Acc@5  92.50 ( 87.43)
## e[21] optimizer.zero_grad (sum) time: 0.11394238471984863
## e[21]       loss.backward (sum) time: 2.273308277130127
## e[21]      optimizer.step (sum) time: 0.9666407108306885
## epoch[21] training(only) time: 10.018702030181885
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.7510e+00 (1.7510e+00)	Acc@1  57.00 ( 57.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 1.8584e+00 (1.7498e+00)	Acc@1  50.00 ( 55.64)	Acc@5  88.00 ( 82.27)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 1.7773e+00 (1.7358e+00)	Acc@1  57.00 ( 54.76)	Acc@5  80.00 ( 82.86)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 1.7871e+00 (1.7365e+00)	Acc@1  52.00 ( 54.26)	Acc@5  83.00 ( 82.58)
Test: [ 40/100]	Time  0.010 ( 0.016)	Loss 1.6084e+00 (1.7196e+00)	Acc@1  59.00 ( 54.32)	Acc@5  83.00 ( 82.90)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.4668e+00 (1.7292e+00)	Acc@1  62.00 ( 54.06)	Acc@5  82.00 ( 82.51)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6172e+00 (1.7181e+00)	Acc@1  54.00 ( 54.05)	Acc@5  84.00 ( 82.87)
Test: [ 70/100]	Time  0.021 ( 0.015)	Loss 1.7793e+00 (1.7201e+00)	Acc@1  49.00 ( 53.77)	Acc@5  82.00 ( 82.99)
Test: [ 80/100]	Time  0.011 ( 0.015)	Loss 2.0410e+00 (1.7332e+00)	Acc@1  46.00 ( 53.51)	Acc@5  75.00 ( 82.53)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.9570e+00 (1.7262e+00)	Acc@1  45.00 ( 53.59)	Acc@5  85.00 ( 82.63)
 * Acc@1 53.650 Acc@5 82.840
### epoch[21] execution time: 11.587580680847168
EPOCH 22
i:   0, name:       features.module.0.weight  changing lr from: 0.057931648642011363   to: 0.054542230279991735
i:   1, name:         features.module.0.bias  changing lr from: 0.059309796919057280   to: 0.055997733014091999
i:   2, name:       features.module.1.weight  changing lr from: 0.060646174929297560   to: 0.057411583503679614
i:   3, name:         features.module.1.bias  changing lr from: 0.061941447134141119   to: 0.058784195868582391
i:   4, name:       features.module.4.weight  changing lr from: 0.063196350051796885   to: 0.060116086821840969
i:   5, name:         features.module.4.bias  changing lr from: 0.064411679468866792   to: 0.061407859995997717
i:   6, name:       features.module.5.weight  changing lr from: 0.065588279121030810   to: 0.062660191970817194
i:   7, name:         features.module.5.bias  changing lr from: 0.066727030698274001   to: 0.063873819843946134
i:   8, name:       features.module.8.weight  changing lr from: 0.067828845042378558   to: 0.065049530198262456
i:   9, name:         features.module.8.bias  changing lr from: 0.068894654415931386   to: 0.066188149331378274
i:  10, name:       features.module.9.weight  changing lr from: 0.069925405732862184   to: 0.067290534623881890
i:  11, name:         features.module.9.bias  changing lr from: 0.070922054650530142   to: 0.068357566933382671
i:  12, name:      features.module.11.weight  changing lr from: 0.071885560432637760   to: 0.069390143911243027
i:  13, name:        features.module.11.bias  changing lr from: 0.072816881500788341   to: 0.070389174148038788
i:  14, name:      features.module.12.weight  changing lr from: 0.073716971600355050   to: 0.071355572062288694
i:  15, name:        features.module.12.bias  changing lr from: 0.074586776513525702   to: 0.072290253454857317
i:  16, name:      features.module.15.weight  changing lr from: 0.075427231258970093   to: 0.073194131658684380
i:  17, name:        features.module.15.bias  changing lr from: 0.076239257723581977   to: 0.074068114220157258
i:  18, name:      features.module.16.weight  changing lr from: 0.077023762677218652   to: 0.074913100054554718
i:  19, name:        features.module.16.bias  changing lr from: 0.077781636126332099   to: 0.075729977023579254
i:  20, name:      features.module.18.weight  changing lr from: 0.078513749966900609   to: 0.076519619888101076
i:  21, name:        features.module.18.bias  changing lr from: 0.079220956901158171   to: 0.077282888593887100
i:  22, name:      features.module.19.weight  changing lr from: 0.079904089586321683   to: 0.078020626852321362
i:  23, name:        features.module.19.bias  changing lr from: 0.080563959986862041   to: 0.078733660981968101
i:  24, name:      features.module.22.weight  changing lr from: 0.081201358904885543   to: 0.079422798980315554
i:  25, name:        features.module.22.bias  changing lr from: 0.081817055665917973   to: 0.080088829798200922
i:  26, name:      features.module.23.weight  changing lr from: 0.082411797939837356   to: 0.080732522792274483
i:  27, name:        features.module.23.bias  changing lr from: 0.082986311678912350   to: 0.081354627333447851
i:  28, name:      features.module.25.weight  changing lr from: 0.083541301156889694   to: 0.081955872551605469
i:  29, name:        features.module.25.bias  changing lr from: 0.084077449094860368   to: 0.082536967198964695
i:  30, name:      features.module.26.weight  changing lr from: 0.084595416861237116   to: 0.083098599616368771
i:  31, name:        features.module.26.bias  changing lr from: 0.085095844734613985   to: 0.083641437788506440
i:  32, name:            classifier.0.weight  changing lr from: 0.085579352219568042   to: 0.084166129475591059
i:  33, name:              classifier.0.bias  changing lr from: 0.086046538406617495   to: 0.084673302410415069
i:  34, name:            classifier.3.weight  changing lr from: 0.086497982368584303   to: 0.085163564550938819
i:  35, name:              classifier.3.bias  changing lr from: 0.086934243586532969   to: 0.085637504379688090
i:  36, name:            classifier.6.weight  changing lr from: 0.087355862399283113   to: 0.086095691242236441
i:  37, name:              classifier.6.bias  changing lr from: 0.087763360471230609   to: 0.086538675717944422



# Switched to train mode...
Epoch: [22][  0/391]	Time  0.176 ( 0.176)	Data  0.143 ( 0.143)	Loss 1.1934e+00 (1.1934e+00)	Acc@1  68.75 ( 68.75)	Acc@5  89.84 ( 89.84)
Epoch: [22][ 10/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.014)	Loss 1.2520e+00 (1.2417e+00)	Acc@1  59.38 ( 63.35)	Acc@5  90.62 ( 90.13)
Epoch: [22][ 20/391]	Time  0.025 ( 0.033)	Data  0.003 ( 0.008)	Loss 1.6328e+00 (1.3010e+00)	Acc@1  53.12 ( 62.76)	Acc@5  89.06 ( 89.55)
Epoch: [22][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.3184e+00 (1.2846e+00)	Acc@1  64.06 ( 63.48)	Acc@5  90.62 ( 89.64)
Epoch: [22][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.1836e+00 (1.3052e+00)	Acc@1  66.41 ( 62.84)	Acc@5  90.62 ( 89.39)
Epoch: [22][ 50/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.5020e+00 (1.3091e+00)	Acc@1  55.47 ( 62.73)	Acc@5  85.16 ( 89.40)
Epoch: [22][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.2627e+00 (1.3117e+00)	Acc@1  65.62 ( 62.77)	Acc@5  89.06 ( 89.34)
Epoch: [22][ 70/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.6445e+00 (1.3256e+00)	Acc@1  57.81 ( 62.47)	Acc@5  83.59 ( 89.18)
Epoch: [22][ 80/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.1328e+00 (1.3358e+00)	Acc@1  66.41 ( 62.44)	Acc@5  92.19 ( 88.98)
Epoch: [22][ 90/391]	Time  0.039 ( 0.027)	Data  0.006 ( 0.003)	Loss 1.3311e+00 (1.3366e+00)	Acc@1  60.16 ( 62.23)	Acc@5  87.50 ( 88.95)
Epoch: [22][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5830e+00 (1.3388e+00)	Acc@1  58.59 ( 62.12)	Acc@5  82.81 ( 88.96)
Epoch: [22][110/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1582e+00 (1.3423e+00)	Acc@1  66.41 ( 61.99)	Acc@5  89.84 ( 88.89)
Epoch: [22][120/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3271e+00 (1.3434e+00)	Acc@1  63.28 ( 61.87)	Acc@5  88.28 ( 88.82)
Epoch: [22][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1465e+00 (1.3472e+00)	Acc@1  66.41 ( 61.84)	Acc@5  92.97 ( 88.83)
Epoch: [22][140/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5410e+00 (1.3458e+00)	Acc@1  56.25 ( 61.87)	Acc@5  85.16 ( 88.76)
Epoch: [22][150/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.1016e+00 (1.3431e+00)	Acc@1  67.97 ( 62.02)	Acc@5  92.97 ( 88.79)
Epoch: [22][160/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2842e+00 (1.3417e+00)	Acc@1  64.06 ( 62.14)	Acc@5  86.72 ( 88.80)
Epoch: [22][170/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4170e+00 (1.3412e+00)	Acc@1  56.25 ( 62.18)	Acc@5  88.28 ( 88.74)
Epoch: [22][180/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5234e+00 (1.3429e+00)	Acc@1  60.16 ( 62.25)	Acc@5  84.38 ( 88.72)
Epoch: [22][190/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5811e+00 (1.3463e+00)	Acc@1  57.03 ( 62.13)	Acc@5  89.06 ( 88.61)
Epoch: [22][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6055e+00 (1.3470e+00)	Acc@1  51.56 ( 62.09)	Acc@5  84.38 ( 88.58)
Epoch: [22][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6104e+00 (1.3480e+00)	Acc@1  55.47 ( 62.06)	Acc@5  83.59 ( 88.51)
Epoch: [22][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3281e+00 (1.3485e+00)	Acc@1  60.16 ( 61.99)	Acc@5  89.06 ( 88.53)
Epoch: [22][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4639e+00 (1.3507e+00)	Acc@1  57.81 ( 61.95)	Acc@5  89.06 ( 88.53)
Epoch: [22][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1484e+00 (1.3539e+00)	Acc@1  66.41 ( 61.86)	Acc@5  89.84 ( 88.45)
Epoch: [22][250/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2832e+00 (1.3561e+00)	Acc@1  62.50 ( 61.81)	Acc@5  89.84 ( 88.40)
Epoch: [22][260/391]	Time  0.038 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.2910e+00 (1.3576e+00)	Acc@1  66.41 ( 61.78)	Acc@5  88.28 ( 88.40)
Epoch: [22][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5283e+00 (1.3614e+00)	Acc@1  62.50 ( 61.77)	Acc@5  85.16 ( 88.35)
Epoch: [22][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5713e+00 (1.3620e+00)	Acc@1  56.25 ( 61.74)	Acc@5  85.16 ( 88.34)
Epoch: [22][290/391]	Time  0.040 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.3838e+00 (1.3614e+00)	Acc@1  62.50 ( 61.73)	Acc@5  85.16 ( 88.35)
Epoch: [22][300/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3633e+00 (1.3630e+00)	Acc@1  64.06 ( 61.66)	Acc@5  85.94 ( 88.34)
Epoch: [22][310/391]	Time  0.025 ( 0.025)	Data  0.004 ( 0.002)	Loss 1.4795e+00 (1.3640e+00)	Acc@1  61.72 ( 61.65)	Acc@5  88.28 ( 88.34)
Epoch: [22][320/391]	Time  0.034 ( 0.025)	Data  0.006 ( 0.002)	Loss 1.5576e+00 (1.3638e+00)	Acc@1  60.16 ( 61.64)	Acc@5  83.59 ( 88.33)
Epoch: [22][330/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.5039e+00 (1.3645e+00)	Acc@1  61.72 ( 61.63)	Acc@5  85.94 ( 88.30)
Epoch: [22][340/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3613e+00 (1.3659e+00)	Acc@1  65.62 ( 61.65)	Acc@5  92.19 ( 88.29)
Epoch: [22][350/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6943e+00 (1.3690e+00)	Acc@1  55.47 ( 61.54)	Acc@5  82.81 ( 88.23)
Epoch: [22][360/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1377e+00 (1.3679e+00)	Acc@1  66.41 ( 61.63)	Acc@5  91.41 ( 88.26)
Epoch: [22][370/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.3896e+00 (1.3682e+00)	Acc@1  62.50 ( 61.63)	Acc@5  85.16 ( 88.23)
Epoch: [22][380/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2949e+00 (1.3689e+00)	Acc@1  62.50 ( 61.62)	Acc@5  90.62 ( 88.24)
Epoch: [22][390/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.8848e+00 (1.3699e+00)	Acc@1  58.75 ( 61.61)	Acc@5  76.25 ( 88.20)
## e[22] optimizer.zero_grad (sum) time: 0.11455726623535156
## e[22]       loss.backward (sum) time: 2.294407844543457
## e[22]      optimizer.step (sum) time: 0.9689731597900391
## epoch[22] training(only) time: 10.02258014678955
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.9023e+00 (1.9023e+00)	Acc@1  49.00 ( 49.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.023 ( 0.026)	Loss 1.7188e+00 (1.6512e+00)	Acc@1  54.00 ( 56.18)	Acc@5  87.00 ( 82.73)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.4941e+00 (1.6513e+00)	Acc@1  61.00 ( 56.10)	Acc@5  84.00 ( 83.05)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.8135e+00 (1.6659e+00)	Acc@1  50.00 ( 55.65)	Acc@5  86.00 ( 82.97)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 1.5850e+00 (1.6545e+00)	Acc@1  59.00 ( 55.71)	Acc@5  86.00 ( 83.22)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.7100e+00 (1.6723e+00)	Acc@1  58.00 ( 55.76)	Acc@5  80.00 ( 82.86)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.8291e+00 (1.6698e+00)	Acc@1  54.00 ( 55.77)	Acc@5  87.00 ( 82.98)
Test: [ 70/100]	Time  0.019 ( 0.016)	Loss 1.6943e+00 (1.6748e+00)	Acc@1  59.00 ( 55.79)	Acc@5  79.00 ( 82.90)
Test: [ 80/100]	Time  0.009 ( 0.015)	Loss 1.8496e+00 (1.6847e+00)	Acc@1  56.00 ( 55.65)	Acc@5  78.00 ( 82.78)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.8877e+00 (1.6757e+00)	Acc@1  53.00 ( 55.75)	Acc@5  83.00 ( 83.03)
 * Acc@1 56.000 Acc@5 83.250
### epoch[22] execution time: 11.601545810699463
EPOCH 23
i:   0, name:       features.module.0.weight  changing lr from: 0.054542230279991735   to: 0.051136353678802732
i:   1, name:         features.module.0.bias  changing lr from: 0.055997733014091999   to: 0.052663343718677959
i:   2, name:       features.module.1.weight  changing lr from: 0.057411583503679614   to: 0.054149448410110351
i:   3, name:         features.module.1.bias  changing lr from: 0.058784195868582391   to: 0.055594777623156268
i:   4, name:       features.module.4.weight  changing lr from: 0.060116086821840969   to: 0.056999578162696907
i:   5, name:         features.module.4.bias  changing lr from: 0.061407859995997717   to: 0.058364215014995030
i:   6, name:       features.module.5.weight  changing lr from: 0.062660191970817194   to: 0.059689154525134430
i:   7, name:         features.module.5.bias  changing lr from: 0.063873819843946134   to: 0.060974949336024077
i:   8, name:       features.module.8.weight  changing lr from: 0.065049530198262456   to: 0.062222224931175442
i:   9, name:         features.module.8.bias  changing lr from: 0.066188149331378274   to: 0.063431667634801014
i:  10, name:       features.module.9.weight  changing lr from: 0.067290534623881890   to: 0.064604013933790005
i:  11, name:         features.module.9.bias  changing lr from: 0.068357566933382671   to: 0.065740040996690272
i:  12, name:      features.module.11.weight  changing lr from: 0.069390143911243027   to: 0.066840558274895115
i:  13, name:        features.module.11.bias  changing lr from: 0.070389174148038788   to: 0.067906400080753937
i:  14, name:      features.module.12.weight  changing lr from: 0.071355572062288694   to: 0.068938419046274110
i:  15, name:        features.module.12.bias  changing lr from: 0.072290253454857317   to: 0.069937480374447511
i:  16, name:      features.module.15.weight  changing lr from: 0.073194131658684380   to: 0.070904456803024471
i:  17, name:        features.module.15.bias  changing lr from: 0.074068114220157258   to: 0.071840224207780842
i:  18, name:      features.module.16.weight  changing lr from: 0.074913100054554718   to: 0.072745657779000106
i:  19, name:        features.module.16.bias  changing lr from: 0.075729977023579254   to: 0.073621628711044537
i:  20, name:      features.module.18.weight  changing lr from: 0.076519619888101076   to: 0.074469001350545558
i:  21, name:        features.module.18.bias  changing lr from: 0.077282888593887100   to: 0.075288630753928235
i:  22, name:      features.module.19.weight  changing lr from: 0.078020626852321362   to: 0.076081360609731252
i:  23, name:        features.module.19.bias  changing lr from: 0.078733660981968101   to: 0.076848021485518170
i:  24, name:      features.module.22.weight  changing lr from: 0.079422798980315554   to: 0.077589429363128348
i:  25, name:        features.module.22.bias  changing lr from: 0.080088829798200922   to: 0.078306384429614795
i:  26, name:      features.module.23.weight  changing lr from: 0.080732522792274483   to: 0.078999670094487967
i:  27, name:        features.module.23.bias  changing lr from: 0.081354627333447851   to: 0.079670052206854802
i:  28, name:      features.module.25.weight  changing lr from: 0.081955872551605469   to: 0.080318278448737096
i:  29, name:        features.module.25.bias  changing lr from: 0.082536967198964695   to: 0.080945077883293373
i:  30, name:      features.module.26.weight  changing lr from: 0.083098599616368771   to: 0.081551160638877573
i:  31, name:        features.module.26.bias  changing lr from: 0.083641437788506440   to: 0.082137217711864302
i:  32, name:            classifier.0.weight  changing lr from: 0.084166129475591059   to: 0.082703920872974221
i:  33, name:              classifier.0.bias  changing lr from: 0.084673302410415069   to: 0.083251922663461098
i:  34, name:            classifier.3.weight  changing lr from: 0.085163564550938819   to: 0.083781856468989488
i:  35, name:              classifier.3.bias  changing lr from: 0.085637504379688090   to: 0.084294336660354713
i:  36, name:            classifier.6.weight  changing lr from: 0.086095691242236441   to: 0.084789958791387662
i:  37, name:              classifier.6.bias  changing lr from: 0.086538675717944422   to: 0.085269299845457613



# Switched to train mode...
Epoch: [23][  0/391]	Time  0.170 ( 0.170)	Data  0.139 ( 0.139)	Loss 1.1113e+00 (1.1113e+00)	Acc@1  71.88 ( 71.88)	Acc@5  93.75 ( 93.75)
Epoch: [23][ 10/391]	Time  0.030 ( 0.038)	Data  0.002 ( 0.015)	Loss 1.3721e+00 (1.2465e+00)	Acc@1  63.28 ( 65.13)	Acc@5  86.72 ( 90.34)
Epoch: [23][ 20/391]	Time  0.022 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.2695e+00 (1.2609e+00)	Acc@1  62.50 ( 63.84)	Acc@5  89.84 ( 89.99)
Epoch: [23][ 30/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.4766e+00 (1.2814e+00)	Acc@1  55.47 ( 63.53)	Acc@5  88.28 ( 89.84)
Epoch: [23][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.1943e+00 (1.2764e+00)	Acc@1  65.62 ( 63.76)	Acc@5  89.84 ( 89.92)
Epoch: [23][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.2891e+00 (1.2708e+00)	Acc@1  62.50 ( 63.85)	Acc@5  88.28 ( 89.89)
Epoch: [23][ 60/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2822e+00 (1.2737e+00)	Acc@1  62.50 ( 63.68)	Acc@5  93.75 ( 89.89)
Epoch: [23][ 70/391]	Time  0.027 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.3994e+00 (1.2657e+00)	Acc@1  58.59 ( 63.89)	Acc@5  91.41 ( 90.13)
Epoch: [23][ 80/391]	Time  0.020 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.4854e+00 (1.2768e+00)	Acc@1  57.03 ( 63.64)	Acc@5  89.84 ( 89.91)
Epoch: [23][ 90/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.0791e+00 (1.2785e+00)	Acc@1  69.53 ( 63.72)	Acc@5  90.62 ( 89.91)
Epoch: [23][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2217e+00 (1.2835e+00)	Acc@1  60.94 ( 63.58)	Acc@5  90.62 ( 89.75)
Epoch: [23][110/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.4805e+00 (1.2792e+00)	Acc@1  59.38 ( 63.55)	Acc@5  86.72 ( 89.85)
Epoch: [23][120/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3926e+00 (1.2785e+00)	Acc@1  61.72 ( 63.54)	Acc@5  90.62 ( 89.93)
Epoch: [23][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2412e+00 (1.2842e+00)	Acc@1  63.28 ( 63.45)	Acc@5  90.62 ( 89.83)
Epoch: [23][140/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2344e+00 (1.2886e+00)	Acc@1  63.28 ( 63.40)	Acc@5  87.50 ( 89.73)
Epoch: [23][150/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3398e+00 (1.2913e+00)	Acc@1  61.72 ( 63.42)	Acc@5  89.84 ( 89.64)
Epoch: [23][160/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2227e+00 (1.2885e+00)	Acc@1  66.41 ( 63.42)	Acc@5  89.06 ( 89.67)
Epoch: [23][170/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1914e+00 (1.2931e+00)	Acc@1  62.50 ( 63.32)	Acc@5  88.28 ( 89.58)
Epoch: [23][180/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3906e+00 (1.2943e+00)	Acc@1  63.28 ( 63.26)	Acc@5  86.72 ( 89.51)
Epoch: [23][190/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.2285e+00 (1.2968e+00)	Acc@1  68.75 ( 63.25)	Acc@5  89.84 ( 89.48)
Epoch: [23][200/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3867e+00 (1.2991e+00)	Acc@1  60.94 ( 63.18)	Acc@5  87.50 ( 89.43)
Epoch: [23][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2861e+00 (1.3025e+00)	Acc@1  64.84 ( 63.12)	Acc@5  85.16 ( 89.37)
Epoch: [23][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5557e+00 (1.3059e+00)	Acc@1  57.81 ( 63.10)	Acc@5  81.25 ( 89.26)
Epoch: [23][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3184e+00 (1.3057e+00)	Acc@1  61.72 ( 63.12)	Acc@5  89.84 ( 89.24)
Epoch: [23][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5244e+00 (1.3068e+00)	Acc@1  55.47 ( 63.11)	Acc@5  84.38 ( 89.21)
Epoch: [23][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2490e+00 (1.3087e+00)	Acc@1  61.72 ( 63.06)	Acc@5  91.41 ( 89.19)
Epoch: [23][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4297e+00 (1.3136e+00)	Acc@1  61.72 ( 62.98)	Acc@5  86.72 ( 89.07)
Epoch: [23][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2373e+00 (1.3151e+00)	Acc@1  64.06 ( 62.92)	Acc@5  90.62 ( 89.07)
Epoch: [23][280/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.4609e+00 (1.3190e+00)	Acc@1  60.16 ( 62.83)	Acc@5  84.38 ( 89.02)
Epoch: [23][290/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3398e+00 (1.3181e+00)	Acc@1  64.84 ( 62.88)	Acc@5  85.94 ( 89.03)
Epoch: [23][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2490e+00 (1.3208e+00)	Acc@1  64.06 ( 62.81)	Acc@5  90.62 ( 88.98)
Epoch: [23][310/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3350e+00 (1.3211e+00)	Acc@1  60.94 ( 62.81)	Acc@5  89.06 ( 88.92)
Epoch: [23][320/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1641e+00 (1.3178e+00)	Acc@1  62.50 ( 62.86)	Acc@5  90.62 ( 88.99)
Epoch: [23][330/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4570e+00 (1.3217e+00)	Acc@1  60.16 ( 62.75)	Acc@5  87.50 ( 88.93)
Epoch: [23][340/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4482e+00 (1.3252e+00)	Acc@1  62.50 ( 62.66)	Acc@5  88.28 ( 88.88)
Epoch: [23][350/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2734e+00 (1.3287e+00)	Acc@1  63.28 ( 62.58)	Acc@5  90.62 ( 88.85)
Epoch: [23][360/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3750e+00 (1.3299e+00)	Acc@1  64.06 ( 62.53)	Acc@5  86.72 ( 88.85)
Epoch: [23][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1660e+00 (1.3300e+00)	Acc@1  60.94 ( 62.56)	Acc@5  89.84 ( 88.82)
Epoch: [23][380/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5996e+00 (1.3318e+00)	Acc@1  57.03 ( 62.51)	Acc@5  85.94 ( 88.81)
Epoch: [23][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4121e+00 (1.3345e+00)	Acc@1  60.00 ( 62.42)	Acc@5  87.50 ( 88.80)
## e[23] optimizer.zero_grad (sum) time: 0.11288690567016602
## e[23]       loss.backward (sum) time: 2.2624855041503906
## e[23]      optimizer.step (sum) time: 0.9569437503814697
## epoch[23] training(only) time: 10.011889696121216
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.9014e+00 (1.9014e+00)	Acc@1  57.00 ( 57.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.012 ( 0.026)	Loss 1.7646e+00 (1.6105e+00)	Acc@1  55.00 ( 58.82)	Acc@5  84.00 ( 83.73)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.4199e+00 (1.6218e+00)	Acc@1  59.00 ( 57.62)	Acc@5  85.00 ( 83.62)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.7051e+00 (1.6254e+00)	Acc@1  52.00 ( 56.97)	Acc@5  81.00 ( 83.58)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.4727e+00 (1.6158e+00)	Acc@1  59.00 ( 56.90)	Acc@5  86.00 ( 83.95)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6270e+00 (1.6353e+00)	Acc@1  59.00 ( 56.65)	Acc@5  84.00 ( 83.63)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6299e+00 (1.6291e+00)	Acc@1  53.00 ( 56.61)	Acc@5  88.00 ( 83.84)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 1.6338e+00 (1.6239e+00)	Acc@1  55.00 ( 56.66)	Acc@5  88.00 ( 83.96)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8213e+00 (1.6267e+00)	Acc@1  56.00 ( 56.65)	Acc@5  80.00 ( 83.99)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 1.9609e+00 (1.6233e+00)	Acc@1  50.00 ( 56.78)	Acc@5  82.00 ( 84.12)
 * Acc@1 56.790 Acc@5 84.020
### epoch[23] execution time: 11.562082052230835
EPOCH 24
i:   0, name:       features.module.0.weight  changing lr from: 0.051136353678802732   to: 0.047729843538492883
i:   1, name:         features.module.0.bias  changing lr from: 0.052663343718677959   to: 0.049321524094779912
i:   2, name:       features.module.1.weight  changing lr from: 0.054149448410110351   to: 0.050873783529213526
i:   3, name:         features.module.1.bias  changing lr from: 0.055594777623156268   to: 0.052386372173169243
i:   4, name:       features.module.4.weight  changing lr from: 0.056999578162696907   to: 0.053859215267661457
i:   5, name:         features.module.4.bias  changing lr from: 0.058364215014995030   to: 0.055292390984207830
i:   6, name:       features.module.5.weight  changing lr from: 0.059689154525134430   to: 0.056686110594895880
i:   7, name:         features.module.5.bias  changing lr from: 0.060974949336024077   to: 0.058040700615821332
i:   8, name:       features.module.8.weight  changing lr from: 0.062222224931175442   to: 0.059356586758039523
i:   9, name:         features.module.8.bias  changing lr from: 0.063431667634801014   to: 0.060634279530429047
i:  10, name:       features.module.9.weight  changing lr from: 0.064604013933790005   to: 0.061874361349175379
i:  11, name:         features.module.9.bias  changing lr from: 0.065740040996690272   to: 0.063077475018759183
i:  12, name:      features.module.11.weight  changing lr from: 0.066840558274895115   to: 0.064244313459246791
i:  13, name:        features.module.11.bias  changing lr from: 0.067906400080753937   to: 0.065375610564231398
i:  14, name:      features.module.12.weight  changing lr from: 0.068938419046274110   to: 0.066472133082894486
i:  15, name:        features.module.12.bias  changing lr from: 0.069937480374447511   to: 0.067534673428304284
i:  16, name:      features.module.15.weight  changing lr from: 0.070904456803024471   to: 0.068564043322216536
i:  17, name:        features.module.15.bias  changing lr from: 0.071840224207780842   to: 0.069561068194279430
i:  18, name:      features.module.16.weight  changing lr from: 0.072745657779000106   to: 0.070526582260672543
i:  19, name:        features.module.16.bias  changing lr from: 0.073621628711044537   to: 0.071461424213832198
i:  20, name:      features.module.18.weight  changing lr from: 0.074469001350545558   to: 0.072366433461054244
i:  21, name:        features.module.18.bias  changing lr from: 0.075288630753928235   to: 0.073242446855430812
i:  22, name:      features.module.19.weight  changing lr from: 0.076081360609731252   to: 0.074090295867799180
i:  23, name:        features.module.19.bias  changing lr from: 0.076848021485518170   to: 0.074910804153178231
i:  24, name:      features.module.22.weight  changing lr from: 0.077589429363128348   to: 0.075704785469566410
i:  25, name:        features.module.22.bias  changing lr from: 0.078306384429614795   to: 0.076473041911002876
i:  26, name:      features.module.23.weight  changing lr from: 0.078999670094487967   to: 0.077216362420471765
i:  27, name:        features.module.23.bias  changing lr from: 0.079670052206854802   to: 0.077935521551585410
i:  28, name:      features.module.25.weight  changing lr from: 0.080318278448737096   to: 0.078631278451040898
i:  29, name:        features.module.25.bias  changing lr from: 0.080945077883293373   to: 0.079304376036624727
i:  30, name:      features.module.26.weight  changing lr from: 0.081551160638877573   to: 0.079955540348068566
i:  31, name:        features.module.26.bias  changing lr from: 0.082137217711864302   to: 0.080585480050353042
i:  32, name:            classifier.0.weight  changing lr from: 0.082703920872974221   to: 0.081194886071136410
i:  33, name:              classifier.0.bias  changing lr from: 0.083251922663461098   to: 0.081784431355869464
i:  34, name:            classifier.3.weight  changing lr from: 0.083781856468989488   to: 0.082354770725863224
i:  35, name:              classifier.3.bias  changing lr from: 0.084294336660354713   to: 0.082906540826117717
i:  36, name:            classifier.6.weight  changing lr from: 0.084789958791387662   to: 0.083440360151113266
i:  37, name:              classifier.6.bias  changing lr from: 0.085269299845457613   to: 0.083956829138023381



# Switched to train mode...
Epoch: [24][  0/391]	Time  0.172 ( 0.172)	Data  0.143 ( 0.143)	Loss 1.2549e+00 (1.2549e+00)	Acc@1  62.50 ( 62.50)	Acc@5  90.62 ( 90.62)
Epoch: [24][ 10/391]	Time  0.034 ( 0.039)	Data  0.003 ( 0.015)	Loss 1.3662e+00 (1.2741e+00)	Acc@1  61.72 ( 64.63)	Acc@5  89.84 ( 89.63)
Epoch: [24][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 1.1240e+00 (1.2760e+00)	Acc@1  66.41 ( 64.73)	Acc@5  91.41 ( 89.66)
Epoch: [24][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.2881e+00 (1.2900e+00)	Acc@1  61.72 ( 64.52)	Acc@5  91.41 ( 89.14)
Epoch: [24][ 40/391]	Time  0.021 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.0400e+00 (1.2634e+00)	Acc@1  69.53 ( 65.28)	Acc@5  92.97 ( 89.77)
Epoch: [24][ 50/391]	Time  0.032 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.2441e+00 (1.2534e+00)	Acc@1  65.62 ( 65.32)	Acc@5  89.84 ( 89.80)
Epoch: [24][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1289e+00 (1.2451e+00)	Acc@1  64.84 ( 65.30)	Acc@5  93.75 ( 89.86)
Epoch: [24][ 70/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2178e+00 (1.2497e+00)	Acc@1  64.06 ( 65.22)	Acc@5  89.84 ( 89.87)
Epoch: [24][ 80/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.0615e+00 (1.2436e+00)	Acc@1  67.97 ( 65.33)	Acc@5  92.97 ( 90.00)
Epoch: [24][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2842e+00 (1.2448e+00)	Acc@1  61.72 ( 65.33)	Acc@5  89.84 ( 89.98)
Epoch: [24][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3389e+00 (1.2493e+00)	Acc@1  64.84 ( 65.22)	Acc@5  89.84 ( 89.87)
Epoch: [24][110/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1611e+00 (1.2543e+00)	Acc@1  64.84 ( 64.99)	Acc@5  94.53 ( 89.86)
Epoch: [24][120/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6416e+00 (1.2589e+00)	Acc@1  52.34 ( 64.83)	Acc@5  85.16 ( 89.79)
Epoch: [24][130/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.1689e+00 (1.2622e+00)	Acc@1  65.62 ( 64.68)	Acc@5  88.28 ( 89.74)
Epoch: [24][140/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.2080e+00 (1.2664e+00)	Acc@1  65.62 ( 64.48)	Acc@5  89.84 ( 89.73)
Epoch: [24][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1738e+00 (1.2615e+00)	Acc@1  69.53 ( 64.61)	Acc@5  92.19 ( 89.82)
Epoch: [24][160/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5098e+00 (1.2641e+00)	Acc@1  60.94 ( 64.55)	Acc@5  85.16 ( 89.73)
Epoch: [24][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2598e+00 (1.2619e+00)	Acc@1  64.06 ( 64.57)	Acc@5  90.62 ( 89.79)
Epoch: [24][180/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3701e+00 (1.2638e+00)	Acc@1  60.94 ( 64.59)	Acc@5  88.28 ( 89.74)
Epoch: [24][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.2275e+00 (1.2667e+00)	Acc@1  64.06 ( 64.52)	Acc@5  93.75 ( 89.74)
Epoch: [24][200/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1855e+00 (1.2669e+00)	Acc@1  67.97 ( 64.47)	Acc@5  88.28 ( 89.73)
Epoch: [24][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3223e+00 (1.2679e+00)	Acc@1  64.06 ( 64.37)	Acc@5  89.06 ( 89.75)
Epoch: [24][220/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3916e+00 (1.2634e+00)	Acc@1  63.28 ( 64.50)	Acc@5  89.06 ( 89.79)
Epoch: [24][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5430e+00 (1.2652e+00)	Acc@1  53.91 ( 64.44)	Acc@5  85.16 ( 89.72)
Epoch: [24][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3545e+00 (1.2664e+00)	Acc@1  55.47 ( 64.41)	Acc@5  90.62 ( 89.72)
Epoch: [24][250/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.6309e+00 (1.2693e+00)	Acc@1  54.69 ( 64.34)	Acc@5  83.59 ( 89.68)
Epoch: [24][260/391]	Time  0.020 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.2080e+00 (1.2710e+00)	Acc@1  64.84 ( 64.27)	Acc@5  91.41 ( 89.67)
Epoch: [24][270/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.3564e+00 (1.2722e+00)	Acc@1  62.50 ( 64.24)	Acc@5  88.28 ( 89.68)
Epoch: [24][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2900e+00 (1.2734e+00)	Acc@1  60.16 ( 64.18)	Acc@5  89.06 ( 89.67)
Epoch: [24][290/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.2373e+00 (1.2735e+00)	Acc@1  66.41 ( 64.16)	Acc@5  89.84 ( 89.66)
Epoch: [24][300/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1777e+00 (1.2733e+00)	Acc@1  64.06 ( 64.16)	Acc@5  90.62 ( 89.66)
Epoch: [24][310/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0117e+00 (1.2739e+00)	Acc@1  73.44 ( 64.16)	Acc@5  95.31 ( 89.65)
Epoch: [24][320/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.5000e+00 (1.2766e+00)	Acc@1  57.81 ( 64.10)	Acc@5  85.94 ( 89.60)
Epoch: [24][330/391]	Time  0.022 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.1953e+00 (1.2768e+00)	Acc@1  64.06 ( 64.11)	Acc@5  90.62 ( 89.56)
Epoch: [24][340/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2471e+00 (1.2760e+00)	Acc@1  60.16 ( 64.11)	Acc@5  92.19 ( 89.57)
Epoch: [24][350/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2480e+00 (1.2757e+00)	Acc@1  67.97 ( 64.12)	Acc@5  89.84 ( 89.58)
Epoch: [24][360/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1201e+00 (1.2759e+00)	Acc@1  67.97 ( 64.14)	Acc@5  94.53 ( 89.58)
Epoch: [24][370/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2715e+00 (1.2763e+00)	Acc@1  62.50 ( 64.10)	Acc@5  89.84 ( 89.58)
Epoch: [24][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2832e+00 (1.2779e+00)	Acc@1  65.62 ( 64.07)	Acc@5  89.06 ( 89.56)
Epoch: [24][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4414e+00 (1.2782e+00)	Acc@1  58.75 ( 64.09)	Acc@5  88.75 ( 89.55)
## e[24] optimizer.zero_grad (sum) time: 0.11366915702819824
## e[24]       loss.backward (sum) time: 2.265453577041626
## e[24]      optimizer.step (sum) time: 0.9696683883666992
## epoch[24] training(only) time: 9.985594749450684
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.8193e+00 (1.8193e+00)	Acc@1  57.00 ( 57.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 1.8164e+00 (1.6730e+00)	Acc@1  55.00 ( 57.45)	Acc@5  85.00 ( 82.91)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 1.6787e+00 (1.6733e+00)	Acc@1  58.00 ( 57.38)	Acc@5  81.00 ( 82.86)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.7314e+00 (1.6568e+00)	Acc@1  50.00 ( 57.13)	Acc@5  83.00 ( 83.19)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.3105e+00 (1.6366e+00)	Acc@1  62.00 ( 57.24)	Acc@5  85.00 ( 83.71)
Test: [ 50/100]	Time  0.021 ( 0.016)	Loss 1.5771e+00 (1.6482e+00)	Acc@1  59.00 ( 56.88)	Acc@5  83.00 ( 83.51)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6836e+00 (1.6355e+00)	Acc@1  55.00 ( 57.08)	Acc@5  83.00 ( 83.84)
Test: [ 70/100]	Time  0.018 ( 0.015)	Loss 1.8867e+00 (1.6336e+00)	Acc@1  53.00 ( 57.10)	Acc@5  81.00 ( 84.03)
Test: [ 80/100]	Time  0.036 ( 0.015)	Loss 1.8154e+00 (1.6409e+00)	Acc@1  54.00 ( 56.88)	Acc@5  78.00 ( 83.93)
Test: [ 90/100]	Time  0.020 ( 0.015)	Loss 1.8662e+00 (1.6379e+00)	Acc@1  50.00 ( 56.91)	Acc@5  82.00 ( 83.88)
 * Acc@1 57.060 Acc@5 84.110
### epoch[24] execution time: 11.549314260482788
EPOCH 25
i:   0, name:       features.module.0.weight  changing lr from: 0.047729843538492883   to: 0.044338527502719036
i:   1, name:         features.module.0.bias  changing lr from: 0.049321524094779912   to: 0.045987202396408758
i:   2, name:       features.module.1.weight  changing lr from: 0.050873783529213526   to: 0.047598660864535097
i:   3, name:         features.module.1.bias  changing lr from: 0.052386372173169243   to: 0.049172237755600201
i:   4, name:       features.module.4.weight  changing lr from: 0.053859215267661457   to: 0.050707484174127793
i:   5, name:         features.module.4.bias  changing lr from: 0.055292390984207830   to: 0.052204142188664017
i:   6, name:       features.module.5.weight  changing lr from: 0.056686110594895880   to: 0.053662121886841309
i:   7, name:         features.module.5.bias  changing lr from: 0.058040700615821332   to: 0.055081480600767820
i:   8, name:       features.module.8.weight  changing lr from: 0.059356586758039523   to: 0.056462404133420019
i:   9, name:         features.module.8.bias  changing lr from: 0.060634279530429047   to: 0.057805189825044415
i:  10, name:       features.module.9.weight  changing lr from: 0.061874361349175379   to: 0.059110231307466267
i:  11, name:         features.module.9.bias  changing lr from: 0.063077475018759183   to: 0.060378004803382158
i:  12, name:      features.module.11.weight  changing lr from: 0.064244313459246791   to: 0.061609056836963765
i:  13, name:        features.module.11.bias  changing lr from: 0.065375610564231398   to: 0.062803993231260000
i:  14, name:      features.module.12.weight  changing lr from: 0.066472133082894486   to: 0.063963469276828375
i:  15, name:        features.module.12.bias  changing lr from: 0.067534673428304284   to: 0.065088180964663511
i:  16, name:      features.module.15.weight  changing lr from: 0.068564043322216536   to: 0.066178857184760465
i:  17, name:        features.module.15.bias  changing lr from: 0.069561068194279430   to: 0.067236252799504501
i:  18, name:      features.module.16.weight  changing lr from: 0.070526582260672543   to: 0.068261142508499392
i:  19, name:        features.module.16.bias  changing lr from: 0.071461424213832198   to: 0.069254315428411761
i:  20, name:      features.module.18.weight  changing lr from: 0.072366433461054244   to: 0.070216570317925969
i:  21, name:        features.module.18.bias  changing lr from: 0.073242446855430812   to: 0.071148711383970831
i:  22, name:      features.module.19.weight  changing lr from: 0.074090295867799180   to: 0.072051544611011584
i:  23, name:        features.module.19.bias  changing lr from: 0.074910804153178231   to: 0.072925874560412413
i:  24, name:      features.module.22.weight  changing lr from: 0.075704785469566410   to: 0.073772501591684164
i:  25, name:        features.module.22.bias  changing lr from: 0.076473041911002876   to: 0.074592219461860468
i:  26, name:      features.module.23.weight  changing lr from: 0.077216362420471765   to: 0.075385813263313436
i:  27, name:        features.module.23.bias  changing lr from: 0.077935521551585410   to: 0.076154057664050728
i:  28, name:      features.module.25.weight  changing lr from: 0.078631278451040898   to: 0.076897715417950346
i:  29, name:        features.module.25.bias  changing lr from: 0.079304376036624727   to: 0.077617536115510866
i:  30, name:      features.module.26.weight  changing lr from: 0.079955540348068566   to: 0.078314255148541789
i:  31, name:        features.module.26.bias  changing lr from: 0.080585480050353042   to: 0.078988592864815771
i:  32, name:            classifier.0.weight  changing lr from: 0.081194886071136410   to: 0.079641253891066230
i:  33, name:              classifier.0.bias  changing lr from: 0.081784431355869464   to: 0.080272926604863168
i:  34, name:            classifier.3.weight  changing lr from: 0.082354770725863224   to: 0.080884282737851157
i:  35, name:              classifier.3.bias  changing lr from: 0.082906540826117717   to: 0.081475977094604662
i:  36, name:            classifier.6.weight  changing lr from: 0.083440360151113266   to: 0.082048647372961123
i:  37, name:              classifier.6.bias  changing lr from: 0.083956829138023381   to: 0.082602914073146821



# Switched to train mode...
Epoch: [25][  0/391]	Time  0.173 ( 0.173)	Data  0.147 ( 0.147)	Loss 1.1895e+00 (1.1895e+00)	Acc@1  66.41 ( 66.41)	Acc@5  89.84 ( 89.84)
Epoch: [25][ 10/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.2451e+00 (1.2108e+00)	Acc@1  63.28 ( 66.97)	Acc@5  88.28 ( 89.06)
Epoch: [25][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.1660e+00 (1.1943e+00)	Acc@1  66.41 ( 67.00)	Acc@5  90.62 ( 89.77)
Epoch: [25][ 30/391]	Time  0.033 ( 0.030)	Data  0.001 ( 0.007)	Loss 8.7207e-01 (1.1798e+00)	Acc@1  73.44 ( 66.89)	Acc@5  96.09 ( 90.27)
Epoch: [25][ 40/391]	Time  0.033 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.1650e+00 (1.1843e+00)	Acc@1  67.19 ( 66.56)	Acc@5  92.19 ( 90.40)
Epoch: [25][ 50/391]	Time  0.020 ( 0.028)	Data  0.000 ( 0.005)	Loss 1.5400e+00 (1.1946e+00)	Acc@1  60.16 ( 66.47)	Acc@5  84.38 ( 90.23)
Epoch: [25][ 60/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.1006e+00 (1.1990e+00)	Acc@1  67.97 ( 66.07)	Acc@5  91.41 ( 90.18)
Epoch: [25][ 70/391]	Time  0.024 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.2744e+00 (1.1946e+00)	Acc@1  62.50 ( 66.22)	Acc@5  92.19 ( 90.39)
Epoch: [25][ 80/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2686e+00 (1.1955e+00)	Acc@1  65.62 ( 66.21)	Acc@5  89.06 ( 90.33)
Epoch: [25][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1670e+00 (1.1926e+00)	Acc@1  62.50 ( 66.23)	Acc@5  88.28 ( 90.42)
Epoch: [25][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4795e+00 (1.1936e+00)	Acc@1  57.81 ( 66.11)	Acc@5  90.62 ( 90.54)
Epoch: [25][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2871e+00 (1.1981e+00)	Acc@1  62.50 ( 66.09)	Acc@5  91.41 ( 90.53)
Epoch: [25][120/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3184e+00 (1.2007e+00)	Acc@1  62.50 ( 65.96)	Acc@5  89.06 ( 90.59)
Epoch: [25][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3262e+00 (1.2049e+00)	Acc@1  65.62 ( 65.90)	Acc@5  89.06 ( 90.57)
Epoch: [25][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1895e+00 (1.2072e+00)	Acc@1  63.28 ( 65.87)	Acc@5  92.19 ( 90.58)
Epoch: [25][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4131e+00 (1.2073e+00)	Acc@1  58.59 ( 65.85)	Acc@5  86.72 ( 90.54)
Epoch: [25][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3672e+00 (1.2089e+00)	Acc@1  61.72 ( 65.79)	Acc@5  86.72 ( 90.51)
Epoch: [25][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3330e+00 (1.2093e+00)	Acc@1  64.06 ( 65.82)	Acc@5  87.50 ( 90.48)
Epoch: [25][180/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0664e+00 (1.2094e+00)	Acc@1  71.09 ( 65.84)	Acc@5  92.97 ( 90.48)
Epoch: [25][190/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.4248e+00 (1.2113e+00)	Acc@1  59.38 ( 65.83)	Acc@5  84.38 ( 90.45)
Epoch: [25][200/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1172e+00 (1.2124e+00)	Acc@1  65.62 ( 65.78)	Acc@5  92.19 ( 90.45)
Epoch: [25][210/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4580e+00 (1.2132e+00)	Acc@1  58.59 ( 65.70)	Acc@5  86.72 ( 90.45)
Epoch: [25][220/391]	Time  0.041 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1611e+00 (1.2153e+00)	Acc@1  66.41 ( 65.66)	Acc@5  89.06 ( 90.42)
Epoch: [25][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1758e+00 (1.2131e+00)	Acc@1  66.41 ( 65.73)	Acc@5  90.62 ( 90.42)
Epoch: [25][240/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3779e+00 (1.2159e+00)	Acc@1  63.28 ( 65.67)	Acc@5  89.84 ( 90.42)
Epoch: [25][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4111e+00 (1.2190e+00)	Acc@1  62.50 ( 65.62)	Acc@5  84.38 ( 90.34)
Epoch: [25][260/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.4541e+00 (1.2226e+00)	Acc@1  64.84 ( 65.61)	Acc@5  89.06 ( 90.25)
Epoch: [25][270/391]	Time  0.038 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.2207e+00 (1.2243e+00)	Acc@1  72.66 ( 65.59)	Acc@5  89.06 ( 90.20)
Epoch: [25][280/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1768e+00 (1.2259e+00)	Acc@1  67.19 ( 65.58)	Acc@5  88.28 ( 90.16)
Epoch: [25][290/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0967e+00 (1.2263e+00)	Acc@1  69.53 ( 65.58)	Acc@5  92.97 ( 90.17)
Epoch: [25][300/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.3477e+00 (1.2276e+00)	Acc@1  60.16 ( 65.56)	Acc@5  89.84 ( 90.17)
Epoch: [25][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2305e+00 (1.2278e+00)	Acc@1  67.19 ( 65.50)	Acc@5  88.28 ( 90.17)
Epoch: [25][320/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.2900e+00 (1.2304e+00)	Acc@1  60.94 ( 65.44)	Acc@5  92.97 ( 90.14)
Epoch: [25][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0283e+00 (1.2294e+00)	Acc@1  69.53 ( 65.39)	Acc@5  92.97 ( 90.17)
Epoch: [25][340/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4297e+00 (1.2322e+00)	Acc@1  64.06 ( 65.32)	Acc@5  91.41 ( 90.14)
Epoch: [25][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2979e+00 (1.2329e+00)	Acc@1  65.62 ( 65.30)	Acc@5  88.28 ( 90.15)
Epoch: [25][360/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 9.8291e-01 (1.2309e+00)	Acc@1  68.75 ( 65.32)	Acc@5  96.09 ( 90.20)
Epoch: [25][370/391]	Time  0.037 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.2500e+00 (1.2303e+00)	Acc@1  67.97 ( 65.35)	Acc@5  87.50 ( 90.21)
Epoch: [25][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4248e+00 (1.2314e+00)	Acc@1  62.50 ( 65.32)	Acc@5  88.28 ( 90.18)
Epoch: [25][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1992e+00 (1.2319e+00)	Acc@1  62.50 ( 65.33)	Acc@5  88.75 ( 90.16)
## e[25] optimizer.zero_grad (sum) time: 0.11198234558105469
## e[25]       loss.backward (sum) time: 2.264636754989624
## e[25]      optimizer.step (sum) time: 0.9552357196807861
## epoch[25] training(only) time: 10.051215648651123
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.7979e+00 (1.7979e+00)	Acc@1  57.00 ( 57.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.010 ( 0.027)	Loss 1.7646e+00 (1.6185e+00)	Acc@1  54.00 ( 58.09)	Acc@5  84.00 ( 83.91)
Test: [ 20/100]	Time  0.010 ( 0.021)	Loss 1.4434e+00 (1.6380e+00)	Acc@1  69.00 ( 58.14)	Acc@5  87.00 ( 84.14)
Test: [ 30/100]	Time  0.010 ( 0.019)	Loss 1.6699e+00 (1.6316e+00)	Acc@1  58.00 ( 57.77)	Acc@5  87.00 ( 84.06)
Test: [ 40/100]	Time  0.021 ( 0.018)	Loss 1.4072e+00 (1.6336e+00)	Acc@1  58.00 ( 57.34)	Acc@5  89.00 ( 84.12)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 1.5547e+00 (1.6486e+00)	Acc@1  60.00 ( 57.04)	Acc@5  82.00 ( 83.75)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.5088e+00 (1.6352e+00)	Acc@1  58.00 ( 57.34)	Acc@5  87.00 ( 83.87)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.7881e+00 (1.6341e+00)	Acc@1  53.00 ( 57.41)	Acc@5  81.00 ( 83.77)
Test: [ 80/100]	Time  0.015 ( 0.016)	Loss 1.8047e+00 (1.6469e+00)	Acc@1  56.00 ( 57.43)	Acc@5  75.00 ( 83.53)
Test: [ 90/100]	Time  0.015 ( 0.016)	Loss 1.7979e+00 (1.6369e+00)	Acc@1  46.00 ( 57.46)	Acc@5  86.00 ( 83.70)
 * Acc@1 57.570 Acc@5 83.670
### epoch[25] execution time: 11.658518314361572
EPOCH 26
i:   0, name:       features.module.0.weight  changing lr from: 0.044338527502719036   to: 0.040978162618878995
i:   1, name:         features.module.0.bias  changing lr from: 0.045987202396408758   to: 0.042675273383566269
i:   2, name:       features.module.1.weight  changing lr from: 0.047598660864535097   to: 0.044338150090302009
i:   3, name:         features.module.1.bias  changing lr from: 0.049172237755600201   to: 0.045965656281496438
i:   4, name:       features.module.4.weight  changing lr from: 0.050707484174127793   to: 0.047556916119281693
i:   5, name:         features.module.4.bias  changing lr from: 0.052204142188664017   to: 0.049111285762488438
i:   6, name:       features.module.5.weight  changing lr from: 0.053662121886841309   to: 0.050628327257760661
i:   7, name:         features.module.5.bias  changing lr from: 0.055081480600767820   to: 0.052107784774133782
i:   8, name:       features.module.8.weight  changing lr from: 0.056462404133420019   to: 0.053549563014096337
i:   9, name:         features.module.8.bias  changing lr from: 0.057805189825044415   to: 0.054953707639561256
i:  10, name:       features.module.9.weight  changing lr from: 0.059110231307466267   to: 0.056320387557802214
i:  11, name:         features.module.9.bias  changing lr from: 0.060378004803382158   to: 0.057649878919871257
i:  12, name:      features.module.11.weight  changing lr from: 0.061609056836963765   to: 0.058942550691995110
i:  13, name:        features.module.11.bias  changing lr from: 0.062803993231260000   to: 0.060198851668701564
i:  14, name:      features.module.12.weight  changing lr from: 0.063963469276828375   to: 0.061419298804762081
i:  15, name:        features.module.12.bias  changing lr from: 0.065088180964663511   to: 0.062604466751303323
i:  16, name:      features.module.15.weight  changing lr from: 0.066178857184760465   to: 0.063754978489527284
i:  17, name:        features.module.15.bias  changing lr from: 0.067236252799504501   to: 0.064871496963303105
i:  18, name:      features.module.16.weight  changing lr from: 0.068261142508499392   to: 0.065954717619397418
i:  19, name:        features.module.16.bias  changing lr from: 0.069254315428411761   to: 0.067005361771248856
i:  20, name:      features.module.18.weight  changing lr from: 0.070216570317925969   to: 0.068024170708949241
i:  21, name:        features.module.18.bias  changing lr from: 0.071148711383970831   to: 0.069011900484447400
i:  22, name:      features.module.19.weight  changing lr from: 0.072051544611011584   to: 0.069969317306945361
i:  23, name:        features.module.19.bias  changing lr from: 0.072925874560412413   to: 0.070897193489009955
i:  24, name:      features.module.22.weight  changing lr from: 0.073772501591684164   to: 0.071796303889085727
i:  25, name:        features.module.22.bias  changing lr from: 0.074592219461860468   to: 0.072667422800881773
i:  26, name:      features.module.23.weight  changing lr from: 0.075385813263313436   to: 0.073511321244528915
i:  27, name:        features.module.23.bias  changing lr from: 0.076154057664050728   to: 0.074328764618483922
i:  28, name:      features.module.25.weight  changing lr from: 0.076897715417950346   to: 0.075120510674912294
i:  29, name:        features.module.25.bias  changing lr from: 0.077617536115510866   to: 0.075887307784730004
i:  30, name:      features.module.26.weight  changing lr from: 0.078314255148541789   to: 0.076629893461646489
i:  31, name:        features.module.26.bias  changing lr from: 0.078988592864815771   to: 0.077348993117445655
i:  32, name:            classifier.0.weight  changing lr from: 0.079641253891066230   to: 0.078045319023387960
i:  33, name:              classifier.0.bias  changing lr from: 0.080272926604863168   to: 0.078719569455031960
i:  34, name:            classifier.3.weight  changing lr from: 0.080884282737851157   to: 0.079372427999976222
i:  35, name:              classifier.3.bias  changing lr from: 0.081475977094604662   to: 0.080004563010028479
i:  36, name:            classifier.6.weight  changing lr from: 0.082048647372961123   to: 0.080616627181133785
i:  37, name:              classifier.6.bias  changing lr from: 0.082602914073146821   to: 0.081209257246052144



# Switched to train mode...
Epoch: [26][  0/391]	Time  0.182 ( 0.182)	Data  0.152 ( 0.152)	Loss 1.0166e+00 (1.0166e+00)	Acc@1  73.44 ( 73.44)	Acc@5  94.53 ( 94.53)
Epoch: [26][ 10/391]	Time  0.023 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.0527e+00 (1.1236e+00)	Acc@1  70.31 ( 66.97)	Acc@5  93.75 ( 92.68)
Epoch: [26][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.1924e+00 (1.0995e+00)	Acc@1  68.75 ( 68.49)	Acc@5  92.97 ( 92.41)
Epoch: [26][ 30/391]	Time  0.037 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.0908e+00 (1.1365e+00)	Acc@1  69.53 ( 67.67)	Acc@5  89.84 ( 91.66)
Epoch: [26][ 40/391]	Time  0.032 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.1230e+00 (1.1373e+00)	Acc@1  64.84 ( 67.49)	Acc@5  91.41 ( 91.60)
Epoch: [26][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.2725e+00 (1.1422e+00)	Acc@1  60.16 ( 67.46)	Acc@5  92.19 ( 91.33)
Epoch: [26][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0068e+00 (1.1322e+00)	Acc@1  71.09 ( 67.64)	Acc@5  92.97 ( 91.65)
Epoch: [26][ 70/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3330e+00 (1.1467e+00)	Acc@1  62.50 ( 67.34)	Acc@5  88.28 ( 91.26)
Epoch: [26][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.7852e-01 (1.1499e+00)	Acc@1  71.09 ( 67.26)	Acc@5  92.97 ( 91.26)
Epoch: [26][ 90/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1885e+00 (1.1499e+00)	Acc@1  66.41 ( 67.19)	Acc@5  88.28 ( 91.23)
Epoch: [26][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1172e+00 (1.1526e+00)	Acc@1  67.97 ( 67.13)	Acc@5  92.97 ( 91.21)
Epoch: [26][110/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2471e+00 (1.1532e+00)	Acc@1  64.84 ( 67.08)	Acc@5  89.06 ( 91.24)
Epoch: [26][120/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.4092e-01 (1.1579e+00)	Acc@1  77.34 ( 67.10)	Acc@5  92.97 ( 91.22)
Epoch: [26][130/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.2529e+00 (1.1565e+00)	Acc@1  66.41 ( 67.13)	Acc@5  89.06 ( 91.23)
Epoch: [26][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1035e+00 (1.1550e+00)	Acc@1  67.19 ( 67.12)	Acc@5  90.62 ( 91.21)
Epoch: [26][150/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3066e+00 (1.1595e+00)	Acc@1  66.41 ( 67.04)	Acc@5  87.50 ( 91.12)
Epoch: [26][160/391]	Time  0.030 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.1572e+00 (1.1627e+00)	Acc@1  65.62 ( 66.96)	Acc@5  92.97 ( 91.13)
Epoch: [26][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3496e+00 (1.1679e+00)	Acc@1  65.62 ( 66.90)	Acc@5  86.72 ( 91.08)
Epoch: [26][180/391]	Time  0.045 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.1504e+00 (1.1673e+00)	Acc@1  64.84 ( 66.93)	Acc@5  91.41 ( 91.07)
Epoch: [26][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.9609e-01 (1.1714e+00)	Acc@1  69.53 ( 66.82)	Acc@5  92.97 ( 91.01)
Epoch: [26][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3506e+00 (1.1779e+00)	Acc@1  58.59 ( 66.60)	Acc@5  88.28 ( 90.93)
Epoch: [26][210/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2441e+00 (1.1837e+00)	Acc@1  64.06 ( 66.50)	Acc@5  90.62 ( 90.88)
Epoch: [26][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1348e+00 (1.1838e+00)	Acc@1  70.31 ( 66.52)	Acc@5  89.06 ( 90.87)
Epoch: [26][230/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.1113e+00 (1.1831e+00)	Acc@1  67.97 ( 66.52)	Acc@5  94.53 ( 90.86)
Epoch: [26][240/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.1406e+00 (1.1850e+00)	Acc@1  62.50 ( 66.45)	Acc@5  91.41 ( 90.84)
Epoch: [26][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2793e+00 (1.1856e+00)	Acc@1  62.50 ( 66.44)	Acc@5  90.62 ( 90.81)
Epoch: [26][260/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.2061e+00 (1.1837e+00)	Acc@1  72.66 ( 66.53)	Acc@5  92.19 ( 90.84)
Epoch: [26][270/391]	Time  0.042 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6357e+00 (1.1884e+00)	Acc@1  57.81 ( 66.40)	Acc@5  85.16 ( 90.80)
Epoch: [26][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0459e+00 (1.1859e+00)	Acc@1  68.75 ( 66.46)	Acc@5  93.75 ( 90.83)
Epoch: [26][290/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.1133e+00 (1.1861e+00)	Acc@1  67.19 ( 66.49)	Acc@5  90.62 ( 90.81)
Epoch: [26][300/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.1533e+00 (1.1854e+00)	Acc@1  68.75 ( 66.47)	Acc@5  93.75 ( 90.84)
Epoch: [26][310/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.1875e+00 (1.1841e+00)	Acc@1  70.31 ( 66.51)	Acc@5  93.75 ( 90.86)
Epoch: [26][320/391]	Time  0.038 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.1914e+00 (1.1874e+00)	Acc@1  66.41 ( 66.44)	Acc@5  90.62 ( 90.81)
Epoch: [26][330/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.0850e+00 (1.1876e+00)	Acc@1  67.19 ( 66.39)	Acc@5  92.97 ( 90.80)
Epoch: [26][340/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0889e+00 (1.1891e+00)	Acc@1  67.97 ( 66.38)	Acc@5  93.75 ( 90.82)
Epoch: [26][350/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0986e+00 (1.1914e+00)	Acc@1  67.19 ( 66.27)	Acc@5  93.75 ( 90.77)
Epoch: [26][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1289e+00 (1.1914e+00)	Acc@1  66.41 ( 66.29)	Acc@5  91.41 ( 90.76)
Epoch: [26][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2949e+00 (1.1922e+00)	Acc@1  67.19 ( 66.25)	Acc@5  88.28 ( 90.76)
Epoch: [26][380/391]	Time  0.020 ( 0.025)	Data  0.004 ( 0.002)	Loss 9.9316e-01 (1.1904e+00)	Acc@1  70.31 ( 66.32)	Acc@5  93.75 ( 90.78)
Epoch: [26][390/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4707e+00 (1.1917e+00)	Acc@1  60.00 ( 66.27)	Acc@5  82.50 ( 90.75)
## e[26] optimizer.zero_grad (sum) time: 0.11431574821472168
## e[26]       loss.backward (sum) time: 2.2204947471618652
## e[26]      optimizer.step (sum) time: 0.977074146270752
## epoch[26] training(only) time: 10.020979404449463
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 1.5518e+00 (1.5518e+00)	Acc@1  59.00 ( 59.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.8906e+00 (1.6527e+00)	Acc@1  51.00 ( 57.55)	Acc@5  85.00 ( 84.36)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.5869e+00 (1.6353e+00)	Acc@1  61.00 ( 58.00)	Acc@5  80.00 ( 83.71)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.6807e+00 (1.6337e+00)	Acc@1  59.00 ( 57.77)	Acc@5  85.00 ( 83.74)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.7305e+00 (1.6257e+00)	Acc@1  53.00 ( 57.61)	Acc@5  87.00 ( 83.93)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.5547e+00 (1.6452e+00)	Acc@1  62.00 ( 57.39)	Acc@5  86.00 ( 83.65)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.6543e+00 (1.6256e+00)	Acc@1  59.00 ( 57.61)	Acc@5  81.00 ( 83.95)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.7285e+00 (1.6294e+00)	Acc@1  52.00 ( 57.42)	Acc@5  80.00 ( 83.93)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.8965e+00 (1.6356e+00)	Acc@1  61.00 ( 57.37)	Acc@5  78.00 ( 83.90)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 1.7314e+00 (1.6277e+00)	Acc@1  50.00 ( 57.62)	Acc@5  86.00 ( 83.98)
 * Acc@1 57.800 Acc@5 84.010
### epoch[26] execution time: 11.564266920089722
EPOCH 27
i:   0, name:       features.module.0.weight  changing lr from: 0.040978162618878995   to: 0.037664362126255110
i:   1, name:         features.module.0.bias  changing lr from: 0.042675273383566269   to: 0.039400531785811189
i:   2, name:       features.module.1.weight  changing lr from: 0.044338150090302009   to: 0.041106258109187478
i:   3, name:         features.module.1.bias  changing lr from: 0.045965656281496438   to: 0.042779878450539967
i:   4, name:       features.module.4.weight  changing lr from: 0.047556916119281693   to: 0.044420037716083986
i:   5, name:         features.module.4.bias  changing lr from: 0.049111285762488438   to: 0.046025656470830151
i:   6, name:       features.module.5.weight  changing lr from: 0.050628327257760661   to: 0.047595901684535799
i:   7, name:         features.module.5.bias  changing lr from: 0.052107784774133782   to: 0.049130159960636736
i:   8, name:       features.module.8.weight  changing lr from: 0.053549563014096337   to: 0.050628013090584130
i:   9, name:         features.module.8.bias  changing lr from: 0.054953707639561256   to: 0.052089215777359255
i:  10, name:       features.module.9.weight  changing lr from: 0.056320387557802214   to: 0.053513675375330007
i:  11, name:         features.module.9.bias  changing lr from: 0.057649878919871257   to: 0.054901433498521168
i:  12, name:      features.module.11.weight  changing lr from: 0.058942550691995110   to: 0.056252649355366613
i:  13, name:        features.module.11.bias  changing lr from: 0.060198851668701564   to: 0.057567584674754195
i:  14, name:      features.module.12.weight  changing lr from: 0.061419298804762081   to: 0.058846590095383212
i:  15, name:        features.module.12.bias  changing lr from: 0.062604466751303323   to: 0.060090092897913852
i:  16, name:      features.module.15.weight  changing lr from: 0.063754978489527284   to: 0.061298585966925924
i:  17, name:        features.module.15.bias  changing lr from: 0.064871496963303105   to: 0.062472617877186755
i:  18, name:      features.module.16.weight  changing lr from: 0.065954717619397418   to: 0.063612784006057674
i:  19, name:        features.module.16.bias  changing lr from: 0.067005361771248856   to: 0.064719718580965105
i:  20, name:      features.module.18.weight  changing lr from: 0.068024170708949241   to: 0.065794087577680113
i:  21, name:        features.module.18.bias  changing lr from: 0.069011900484447400   to: 0.066836582391644042
i:  22, name:      features.module.19.weight  changing lr from: 0.069969317306945361   to: 0.067847914210730942
i:  23, name:        features.module.19.bias  changing lr from: 0.070897193489009955   to: 0.068828809023635043
i:  24, name:      features.module.22.weight  changing lr from: 0.071796303889085727   to: 0.069780003203507884
i:  25, name:        features.module.22.bias  changing lr from: 0.072667422800881773   to: 0.070702239611550671
i:  26, name:      features.module.23.weight  changing lr from: 0.073511321244528915   to: 0.071596264169997023
i:  27, name:        features.module.23.bias  changing lr from: 0.074328764618483922   to: 0.072462822858311612
i:  28, name:      features.module.25.weight  changing lr from: 0.075120510674912294   to: 0.073302659090495673
i:  29, name:        features.module.25.bias  changing lr from: 0.075887307784730004   to: 0.074116511435144292
i:  30, name:      features.module.26.weight  changing lr from: 0.076629893461646489   to: 0.074905111643359959
i:  31, name:        features.module.26.bias  changing lr from: 0.077348993117445655   to: 0.075669182952810263
i:  32, name:            classifier.0.weight  changing lr from: 0.078045319023387960   to: 0.076409438639139127
i:  33, name:              classifier.0.bias  changing lr from: 0.078719569455031960   to: 0.077126580788620697
i:  34, name:            classifier.3.weight  changing lr from: 0.079372427999976222   to: 0.077821299268397856
i:  35, name:              classifier.3.bias  changing lr from: 0.080004563010028479   to: 0.078494270872889044
i:  36, name:            classifier.6.weight  changing lr from: 0.080616627181133785   to: 0.079146158626995755
i:  37, name:              classifier.6.bias  changing lr from: 0.081209257246052144   to: 0.079777611228610018



# Switched to train mode...
Epoch: [27][  0/391]	Time  0.180 ( 0.180)	Data  0.144 ( 0.144)	Loss 1.1729e+00 (1.1729e+00)	Acc@1  68.75 ( 68.75)	Acc@5  89.84 ( 89.84)
Epoch: [27][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.015)	Loss 9.5752e-01 (1.0931e+00)	Acc@1  75.00 ( 69.39)	Acc@5  96.88 ( 92.47)
Epoch: [27][ 20/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.009)	Loss 1.0889e+00 (1.0955e+00)	Acc@1  71.09 ( 69.90)	Acc@5  89.06 ( 91.67)
Epoch: [27][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.1768e+00 (1.0995e+00)	Acc@1  64.84 ( 69.15)	Acc@5  92.19 ( 91.78)
Epoch: [27][ 40/391]	Time  0.037 ( 0.029)	Data  0.005 ( 0.005)	Loss 1.1494e+00 (1.1073e+00)	Acc@1  64.06 ( 68.50)	Acc@5  90.62 ( 91.94)
Epoch: [27][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.1436e+00 (1.0941e+00)	Acc@1  66.41 ( 69.09)	Acc@5  92.97 ( 91.94)
Epoch: [27][ 60/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 9.8828e-01 (1.0884e+00)	Acc@1  74.22 ( 69.02)	Acc@5  96.09 ( 92.20)
Epoch: [27][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0088e+00 (1.0947e+00)	Acc@1  71.88 ( 68.71)	Acc@5  92.19 ( 92.22)
Epoch: [27][ 80/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0586e+00 (1.0941e+00)	Acc@1  71.88 ( 68.89)	Acc@5  92.97 ( 92.14)
Epoch: [27][ 90/391]	Time  0.034 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.1738e+00 (1.1036e+00)	Acc@1  69.53 ( 68.52)	Acc@5  89.84 ( 91.96)
Epoch: [27][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1699e+00 (1.1056e+00)	Acc@1  64.84 ( 68.49)	Acc@5  95.31 ( 92.00)
Epoch: [27][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2676e+00 (1.1046e+00)	Acc@1  60.16 ( 68.43)	Acc@5  91.41 ( 92.05)
Epoch: [27][120/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3535e+00 (1.1067e+00)	Acc@1  62.50 ( 68.34)	Acc@5  90.62 ( 92.04)
Epoch: [27][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4756e+00 (1.1105e+00)	Acc@1  60.16 ( 68.16)	Acc@5  86.72 ( 92.01)
Epoch: [27][140/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0986e+00 (1.1091e+00)	Acc@1  68.75 ( 68.27)	Acc@5  89.06 ( 91.95)
Epoch: [27][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0137e+00 (1.1107e+00)	Acc@1  72.66 ( 68.34)	Acc@5  92.19 ( 91.86)
Epoch: [27][160/391]	Time  0.028 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.2295e+00 (1.1116e+00)	Acc@1  61.72 ( 68.28)	Acc@5  92.19 ( 91.78)
Epoch: [27][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2275e+00 (1.1138e+00)	Acc@1  65.62 ( 68.23)	Acc@5  88.28 ( 91.71)
Epoch: [27][180/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2627e-01 (1.1156e+00)	Acc@1  72.66 ( 68.22)	Acc@5  95.31 ( 91.72)
Epoch: [27][190/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2197e+00 (1.1173e+00)	Acc@1  71.09 ( 68.19)	Acc@5  88.28 ( 91.72)
Epoch: [27][200/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.1191e+00 (1.1178e+00)	Acc@1  67.97 ( 68.18)	Acc@5  89.84 ( 91.69)
Epoch: [27][210/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.7012e-01 (1.1211e+00)	Acc@1  71.09 ( 68.10)	Acc@5  98.44 ( 91.65)
Epoch: [27][220/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.3457e-01 (1.1212e+00)	Acc@1  73.44 ( 68.14)	Acc@5  93.75 ( 91.66)
Epoch: [27][230/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2939e+00 (1.1225e+00)	Acc@1  65.62 ( 68.13)	Acc@5  88.28 ( 91.64)
Epoch: [27][240/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0742e+00 (1.1228e+00)	Acc@1  72.66 ( 68.09)	Acc@5  90.62 ( 91.63)
Epoch: [27][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2861e+00 (1.1246e+00)	Acc@1  62.50 ( 68.04)	Acc@5  93.75 ( 91.62)
Epoch: [27][260/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.1113e+00 (1.1272e+00)	Acc@1  67.97 ( 68.01)	Acc@5  92.97 ( 91.58)
Epoch: [27][270/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.5107e-01 (1.1271e+00)	Acc@1  76.56 ( 68.03)	Acc@5  96.09 ( 91.58)
Epoch: [27][280/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.2500e+00 (1.1288e+00)	Acc@1  61.72 ( 67.99)	Acc@5  89.84 ( 91.54)
Epoch: [27][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3760e+00 (1.1307e+00)	Acc@1  61.72 ( 67.89)	Acc@5  90.62 ( 91.53)
Epoch: [27][300/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.0859e+00 (1.1329e+00)	Acc@1  68.75 ( 67.86)	Acc@5  89.84 ( 91.51)
Epoch: [27][310/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1152e+00 (1.1332e+00)	Acc@1  69.53 ( 67.87)	Acc@5  89.06 ( 91.49)
Epoch: [27][320/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2061e+00 (1.1325e+00)	Acc@1  69.53 ( 67.87)	Acc@5  89.06 ( 91.49)
Epoch: [27][330/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2939e+00 (1.1328e+00)	Acc@1  59.38 ( 67.86)	Acc@5  92.97 ( 91.48)
Epoch: [27][340/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.2109e+00 (1.1327e+00)	Acc@1  63.28 ( 67.84)	Acc@5  93.75 ( 91.50)
Epoch: [27][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0762e+00 (1.1335e+00)	Acc@1  70.31 ( 67.79)	Acc@5  92.19 ( 91.49)
Epoch: [27][360/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1592e+00 (1.1344e+00)	Acc@1  69.53 ( 67.79)	Acc@5  89.06 ( 91.47)
Epoch: [27][370/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2754e+00 (1.1347e+00)	Acc@1  64.06 ( 67.76)	Acc@5  89.84 ( 91.44)
Epoch: [27][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3486e+00 (1.1378e+00)	Acc@1  65.62 ( 67.71)	Acc@5  88.28 ( 91.41)
Epoch: [27][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4883e+00 (1.1394e+00)	Acc@1  60.00 ( 67.66)	Acc@5  86.25 ( 91.40)
## e[27] optimizer.zero_grad (sum) time: 0.11407208442687988
## e[27]       loss.backward (sum) time: 2.274437665939331
## e[27]      optimizer.step (sum) time: 0.9714694023132324
## epoch[27] training(only) time: 10.0775306224823
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.6162e+00 (1.6162e+00)	Acc@1  60.00 ( 60.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.7021e+00 (1.5464e+00)	Acc@1  59.00 ( 60.36)	Acc@5  89.00 ( 85.73)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.5488e+00 (1.5528e+00)	Acc@1  61.00 ( 59.95)	Acc@5  82.00 ( 85.90)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.5664e+00 (1.5511e+00)	Acc@1  59.00 ( 59.48)	Acc@5  89.00 ( 85.77)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.4951e+00 (1.5370e+00)	Acc@1  58.00 ( 59.59)	Acc@5  86.00 ( 85.73)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.4229e+00 (1.5554e+00)	Acc@1  62.00 ( 59.20)	Acc@5  86.00 ( 85.41)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.5879e+00 (1.5374e+00)	Acc@1  58.00 ( 59.28)	Acc@5  84.00 ( 85.66)
Test: [ 70/100]	Time  0.020 ( 0.015)	Loss 1.7637e+00 (1.5397e+00)	Acc@1  59.00 ( 59.21)	Acc@5  83.00 ( 85.66)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.7812e+00 (1.5502e+00)	Acc@1  63.00 ( 59.26)	Acc@5  78.00 ( 85.48)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 1.7119e+00 (1.5463e+00)	Acc@1  55.00 ( 59.29)	Acc@5  87.00 ( 85.59)
 * Acc@1 59.450 Acc@5 85.680
### epoch[27] execution time: 11.663222551345825
EPOCH 28
i:   0, name:       features.module.0.weight  changing lr from: 0.037664362126255110   to: 0.034412522912332426
i:   1, name:         features.module.0.bias  changing lr from: 0.039400531785811189   to: 0.036177606212671562
i:   2, name:       features.module.1.weight  changing lr from: 0.041106258109187478   to: 0.037916868879737907
i:   3, name:         features.module.1.bias  changing lr from: 0.042779878450539967   to: 0.039628068994590065
i:   4, name:       features.module.4.weight  changing lr from: 0.044420037716083986   to: 0.041309321147638738
i:   5, name:         features.module.4.bias  changing lr from: 0.046025656470830151   to: 0.042959061424324693
i:   6, name:       features.module.5.weight  changing lr from: 0.047595901684535799   to: 0.044576015101133820
i:   7, name:         features.module.5.bias  changing lr from: 0.049130159960636736   to: 0.046159166919955266
i:   8, name:       features.module.8.weight  changing lr from: 0.050628013090584130   to: 0.047707733800955804
i:   9, name:         features.module.8.bias  changing lr from: 0.052089215777359255   to: 0.049221139850164991
i:  10, name:       features.module.9.weight  changing lr from: 0.053513675375330007   to: 0.050698993517019411
i:  11, name:         features.module.9.bias  changing lr from: 0.054901433498521168   to: 0.052141066758519185
i:  12, name:      features.module.11.weight  changing lr from: 0.056252649355366613   to: 0.053547276069844565
i:  13, name:        features.module.11.bias  changing lr from: 0.057567584674754195   to: 0.054917665245808302
i:  14, name:      features.module.12.weight  changing lr from: 0.058846590095383212   to: 0.056252389743002965
i:  15, name:        features.module.12.bias  changing lr from: 0.060090092897913852   to: 0.057551702518642084
i:  16, name:      features.module.15.weight  changing lr from: 0.061298585966925924   to: 0.058815941228647341
i:  17, name:        features.module.15.bias  changing lr from: 0.062472617877186755   to: 0.060045516674305514
i:  18, name:      features.module.16.weight  changing lr from: 0.063612784006057674   to: 0.061240902393662380
i:  19, name:        features.module.16.bias  changing lr from: 0.064719718580965105   to: 0.062402625300612781
i:  20, name:      features.module.18.weight  changing lr from: 0.065794087577680113   to: 0.063531257281303183
i:  21, name:        features.module.18.bias  changing lr from: 0.066836582391644042   to: 0.064627407663913408
i:  22, name:      features.module.19.weight  changing lr from: 0.067847914210730942   to: 0.065691716484084117
i:  23, name:        features.module.19.bias  changing lr from: 0.068828809023635043   to: 0.066724848474169873
i:  24, name:      features.module.22.weight  changing lr from: 0.069780003203507884   to: 0.067727487710104534
i:  25, name:        features.module.22.bias  changing lr from: 0.070702239611550671   to: 0.068700332854954999
i:  26, name:      features.module.23.weight  changing lr from: 0.071596264169997023   to: 0.069644092943205088
i:  27, name:        features.module.23.bias  changing lr from: 0.072462822858311612   to: 0.070559483654456995
i:  28, name:      features.module.25.weight  changing lr from: 0.073302659090495673   to: 0.071447224029568587
i:  29, name:        features.module.25.bias  changing lr from: 0.074116511435144292   to: 0.072308033586269277
i:  30, name:      features.module.26.weight  changing lr from: 0.074905111643359959   to: 0.073142629795028752
i:  31, name:        features.module.26.bias  changing lr from: 0.075669182952810263   to: 0.073951725879402908
i:  32, name:            classifier.0.weight  changing lr from: 0.076409438639139127   to: 0.074736028908265403
i:  33, name:              classifier.0.bias  changing lr from: 0.077126580788620697   to: 0.075496238150266112
i:  34, name:            classifier.3.weight  changing lr from: 0.077821299268397856   to: 0.076233043663554170
i:  35, name:              classifier.3.bias  changing lr from: 0.078494270872889044   to: 0.076947125096278601
i:  36, name:            classifier.6.weight  changing lr from: 0.079146158626995755   to: 0.077639150675649249
i:  37, name:              classifier.6.bias  changing lr from: 0.079777611228610018   to: 0.078309776365417583



# Switched to train mode...
Epoch: [28][  0/391]	Time  0.172 ( 0.172)	Data  0.144 ( 0.144)	Loss 8.7500e-01 (8.7500e-01)	Acc@1  72.66 ( 72.66)	Acc@5  98.44 ( 98.44)
Epoch: [28][ 10/391]	Time  0.028 ( 0.039)	Data  0.001 ( 0.014)	Loss 1.0283e+00 (1.1256e+00)	Acc@1  69.53 ( 66.97)	Acc@5  93.75 ( 91.90)
Epoch: [28][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.2334e+00 (1.0926e+00)	Acc@1  64.84 ( 68.38)	Acc@5  90.62 ( 92.26)
Epoch: [28][ 30/391]	Time  0.029 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.2588e+00 (1.0753e+00)	Acc@1  67.19 ( 68.80)	Acc@5  90.62 ( 92.54)
Epoch: [28][ 40/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.0049e+00 (1.0687e+00)	Acc@1  66.41 ( 69.19)	Acc@5  94.53 ( 92.55)
Epoch: [28][ 50/391]	Time  0.020 ( 0.028)	Data  0.000 ( 0.005)	Loss 1.2070e+00 (1.0676e+00)	Acc@1  68.75 ( 69.09)	Acc@5  86.72 ( 92.19)
Epoch: [28][ 60/391]	Time  0.042 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1621e+00 (1.0598e+00)	Acc@1  67.19 ( 69.42)	Acc@5  89.06 ( 92.14)
Epoch: [28][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.7012e-01 (1.0585e+00)	Acc@1  71.88 ( 69.39)	Acc@5  93.75 ( 91.96)
Epoch: [28][ 80/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0479e+00 (1.0643e+00)	Acc@1  68.75 ( 69.46)	Acc@5  93.75 ( 91.92)
Epoch: [28][ 90/391]	Time  0.020 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.1543e+00 (1.0659e+00)	Acc@1  67.19 ( 69.39)	Acc@5  91.41 ( 91.96)
Epoch: [28][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2881e+00 (1.0636e+00)	Acc@1  69.53 ( 69.44)	Acc@5  91.41 ( 92.11)
Epoch: [28][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1836e+00 (1.0685e+00)	Acc@1  68.75 ( 69.28)	Acc@5  89.84 ( 92.10)
Epoch: [28][120/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6191e-01 (1.0637e+00)	Acc@1  67.19 ( 69.36)	Acc@5  93.75 ( 92.13)
Epoch: [28][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4014e+00 (1.0678e+00)	Acc@1  60.94 ( 69.26)	Acc@5  87.50 ( 92.10)
Epoch: [28][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2900e+00 (1.0646e+00)	Acc@1  65.62 ( 69.29)	Acc@5  90.62 ( 92.22)
Epoch: [28][150/391]	Time  0.043 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.0586e+00 (1.0652e+00)	Acc@1  71.88 ( 69.29)	Acc@5  90.62 ( 92.27)
Epoch: [28][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1221e+00 (1.0641e+00)	Acc@1  67.97 ( 69.26)	Acc@5  94.53 ( 92.27)
Epoch: [28][170/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0381e+00 (1.0698e+00)	Acc@1  68.75 ( 69.07)	Acc@5  91.41 ( 92.22)
Epoch: [28][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1172e+00 (1.0747e+00)	Acc@1  68.75 ( 69.00)	Acc@5  89.84 ( 92.11)
Epoch: [28][190/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3369e+00 (1.0755e+00)	Acc@1  63.28 ( 69.05)	Acc@5  88.28 ( 92.11)
Epoch: [28][200/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.4434e-01 (1.0725e+00)	Acc@1  71.88 ( 69.06)	Acc@5  92.97 ( 92.14)
Epoch: [28][210/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0684e+00 (1.0726e+00)	Acc@1  71.09 ( 69.06)	Acc@5  89.06 ( 92.14)
Epoch: [28][220/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1748e+00 (1.0724e+00)	Acc@1  66.41 ( 69.08)	Acc@5  92.97 ( 92.16)
Epoch: [28][230/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.7646e-01 (1.0700e+00)	Acc@1  75.00 ( 69.21)	Acc@5  95.31 ( 92.17)
Epoch: [28][240/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0635e+00 (1.0711e+00)	Acc@1  67.19 ( 69.19)	Acc@5  91.41 ( 92.16)
Epoch: [28][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0664e+00 (1.0736e+00)	Acc@1  70.31 ( 69.14)	Acc@5  90.62 ( 92.13)
Epoch: [28][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2207e+00 (1.0760e+00)	Acc@1  66.41 ( 69.06)	Acc@5  90.62 ( 92.08)
Epoch: [28][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.3154e-01 (1.0747e+00)	Acc@1  75.00 ( 69.09)	Acc@5  96.88 ( 92.09)
Epoch: [28][280/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.2900e+00 (1.0756e+00)	Acc@1  64.06 ( 69.10)	Acc@5  90.62 ( 92.08)
Epoch: [28][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4355e+00 (1.0789e+00)	Acc@1  58.59 ( 69.00)	Acc@5  86.72 ( 92.02)
Epoch: [28][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.6631e-01 (1.0808e+00)	Acc@1  69.53 ( 68.91)	Acc@5  94.53 ( 92.01)
Epoch: [28][310/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 9.5361e-01 (1.0795e+00)	Acc@1  69.53 ( 68.91)	Acc@5  93.75 ( 92.05)
Epoch: [28][320/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2324e+00 (1.0813e+00)	Acc@1  61.72 ( 68.87)	Acc@5  92.19 ( 92.02)
Epoch: [28][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0586e+00 (1.0844e+00)	Acc@1  70.31 ( 68.78)	Acc@5  92.19 ( 91.97)
Epoch: [28][340/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0762e+00 (1.0847e+00)	Acc@1  65.62 ( 68.75)	Acc@5  92.97 ( 91.99)
Epoch: [28][350/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0967e+00 (1.0867e+00)	Acc@1  74.22 ( 68.71)	Acc@5  91.41 ( 91.97)
Epoch: [28][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0811e+00 (1.0872e+00)	Acc@1  68.75 ( 68.69)	Acc@5  92.19 ( 91.99)
Epoch: [28][370/391]	Time  0.037 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.2695e+00 (1.0879e+00)	Acc@1  64.84 ( 68.68)	Acc@5  88.28 ( 91.98)
Epoch: [28][380/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.1973e+00 (1.0906e+00)	Acc@1  67.97 ( 68.66)	Acc@5  87.50 ( 91.92)
Epoch: [28][390/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4180e+00 (1.0912e+00)	Acc@1  62.50 ( 68.63)	Acc@5  83.75 ( 91.92)
## e[28] optimizer.zero_grad (sum) time: 0.11312508583068848
## e[28]       loss.backward (sum) time: 2.313339948654175
## e[28]      optimizer.step (sum) time: 0.9669315814971924
## epoch[28] training(only) time: 10.028753280639648
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.6875e+00 (1.6875e+00)	Acc@1  57.00 ( 57.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 1.8164e+00 (1.6341e+00)	Acc@1  54.00 ( 59.55)	Acc@5  89.00 ( 85.27)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 1.3740e+00 (1.6248e+00)	Acc@1  60.00 ( 59.14)	Acc@5  86.00 ( 84.95)
Test: [ 30/100]	Time  0.009 ( 0.018)	Loss 1.9160e+00 (1.6369e+00)	Acc@1  52.00 ( 58.35)	Acc@5  85.00 ( 84.52)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.3359e+00 (1.6167e+00)	Acc@1  63.00 ( 58.61)	Acc@5  87.00 ( 84.71)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.5771e+00 (1.6252e+00)	Acc@1  63.00 ( 58.39)	Acc@5  81.00 ( 84.51)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 1.6514e+00 (1.6063e+00)	Acc@1  53.00 ( 58.54)	Acc@5  86.00 ( 84.84)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.5859e+00 (1.6037e+00)	Acc@1  56.00 ( 58.37)	Acc@5  83.00 ( 84.94)
Test: [ 80/100]	Time  0.009 ( 0.015)	Loss 1.7217e+00 (1.6064e+00)	Acc@1  63.00 ( 58.40)	Acc@5  80.00 ( 84.78)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.6035e+00 (1.5956e+00)	Acc@1  55.00 ( 58.62)	Acc@5  86.00 ( 84.99)
 * Acc@1 58.680 Acc@5 85.110
### epoch[28] execution time: 11.60257625579834
EPOCH 29
i:   0, name:       features.module.0.weight  changing lr from: 0.034412522912332426   to: 0.031237753974350746
i:   1, name:         features.module.0.bias  changing lr from: 0.036177606212671562   to: 0.033020893806132538
i:   2, name:       features.module.1.weight  changing lr from: 0.037916868879737907   to: 0.034783683771958576
i:   3, name:         features.module.1.bias  changing lr from: 0.039628068994590065   to: 0.036523252276474606
i:   4, name:       features.module.4.weight  changing lr from: 0.041309321147638738   to: 0.038237134577973945
i:   5, name:         features.module.4.bias  changing lr from: 0.042959061424324693   to: 0.039923234899376069
i:   6, name:       features.module.5.weight  changing lr from: 0.044576015101133820   to: 0.041579791254176157
i:   7, name:         features.module.5.bias  changing lr from: 0.046159166919955266   to: 0.043205342890819821
i:   8, name:       features.module.8.weight  changing lr from: 0.047707733800955804   to: 0.044798700243049112
i:   9, name:         features.module.8.bias  changing lr from: 0.049221139850164991   to: 0.046358917263072522
i:  10, name:       features.module.9.weight  changing lr from: 0.050698993517019411   to: 0.047885266007913235
i:  11, name:         features.module.9.bias  changing lr from: 0.052141066758519185   to: 0.049377213346134714
i:  12, name:      features.module.11.weight  changing lr from: 0.053547276069844565   to: 0.050834399651616721
i:  13, name:        features.module.11.bias  changing lr from: 0.054917665245808302   to: 0.052256619352574231
i:  14, name:      features.module.12.weight  changing lr from: 0.056252389743002965   to: 0.053643803207084562
i:  15, name:        features.module.12.bias  changing lr from: 0.057551702518642084   to: 0.054996002180618592
i:  16, name:      features.module.15.weight  changing lr from: 0.058815941228647341   to: 0.056313372806135444
i:  17, name:        features.module.15.bias  changing lr from: 0.060045516674305514   to: 0.057596163912929599
i:  18, name:      features.module.16.weight  changing lr from: 0.061240902393662380   to: 0.058844704616408783
i:  19, name:        features.module.16.bias  changing lr from: 0.062402625300612781   to: 0.060059393467154880
i:  20, name:      features.module.18.weight  changing lr from: 0.063531257281303183   to: 0.061240688663854084
i:  21, name:        features.module.18.bias  changing lr from: 0.064627407663913408   to: 0.062389099240864387
i:  22, name:      features.module.19.weight  changing lr from: 0.065691716484084117   to: 0.063505177147248168
i:  23, name:        features.module.19.bias  changing lr from: 0.066724848474169873   to: 0.064589510139969775
i:  24, name:      features.module.22.weight  changing lr from: 0.067727487710104534   to: 0.065642715419603773
i:  25, name:        features.module.22.bias  changing lr from: 0.068700332854954999   to: 0.066665433942288771
i:  26, name:      features.module.23.weight  changing lr from: 0.069644092943205088   to: 0.067658325346773837
i:  27, name:        features.module.23.bias  changing lr from: 0.070559483654456995   to: 0.068622063440231515
i:  28, name:      features.module.25.weight  changing lr from: 0.071447224029568587   to: 0.069557332191048205
i:  29, name:        features.module.25.bias  changing lr from: 0.072308033586269277   to: 0.070464822181049019
i:  30, name:      features.module.26.weight  changing lr from: 0.073142629795028752   to: 0.071345227473578840
i:  31, name:        features.module.26.bias  changing lr from: 0.073951725879402908   to: 0.072199242857548293
i:  32, name:            classifier.0.weight  changing lr from: 0.074736028908265403   to: 0.073027561430975205
i:  33, name:              classifier.0.bias  changing lr from: 0.075496238150266112   to: 0.073830872490721539
i:  34, name:            classifier.3.weight  changing lr from: 0.076233043663554170   to: 0.074609859698051256
i:  35, name:              classifier.3.bias  changing lr from: 0.076947125096278601   to: 0.075365199492335111
i:  36, name:            classifier.6.weight  changing lr from: 0.077639150675649249   to: 0.076097559727712261
i:  37, name:              classifier.6.bias  changing lr from: 0.078309776365417583   to: 0.076807598509802594



# Switched to train mode...
Epoch: [29][  0/391]	Time  0.179 ( 0.179)	Data  0.142 ( 0.142)	Loss 9.5898e-01 (9.5898e-01)	Acc@1  75.78 ( 75.78)	Acc@5  95.31 ( 95.31)
Epoch: [29][ 10/391]	Time  0.032 ( 0.039)	Data  0.003 ( 0.015)	Loss 8.3203e-01 (9.5703e-01)	Acc@1  71.88 ( 73.51)	Acc@5  97.66 ( 94.18)
Epoch: [29][ 20/391]	Time  0.021 ( 0.032)	Data  0.002 ( 0.009)	Loss 1.0273e+00 (9.7982e-01)	Acc@1  65.62 ( 71.61)	Acc@5  93.75 ( 93.90)
Epoch: [29][ 30/391]	Time  0.022 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.0674e+00 (9.9401e-01)	Acc@1  71.88 ( 71.27)	Acc@5  91.41 ( 93.65)
Epoch: [29][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 9.7559e-01 (9.9515e-01)	Acc@1  73.44 ( 71.15)	Acc@5  92.97 ( 93.58)
Epoch: [29][ 50/391]	Time  0.021 ( 0.028)	Data  0.000 ( 0.005)	Loss 9.5605e-01 (9.9165e-01)	Acc@1  72.66 ( 71.37)	Acc@5  96.09 ( 93.58)
Epoch: [29][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0508e+00 (9.9618e-01)	Acc@1  72.66 ( 71.30)	Acc@5  91.41 ( 93.61)
Epoch: [29][ 70/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.8613e-01 (9.9346e-01)	Acc@1  75.78 ( 71.32)	Acc@5  97.66 ( 93.61)
Epoch: [29][ 80/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0186e+00 (1.0002e+00)	Acc@1  72.66 ( 71.38)	Acc@5  93.75 ( 93.45)
Epoch: [29][ 90/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1699e+00 (9.9605e-01)	Acc@1  61.72 ( 71.44)	Acc@5  89.84 ( 93.43)
Epoch: [29][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9199e-01 (9.9529e-01)	Acc@1  75.00 ( 71.45)	Acc@5  96.09 ( 93.53)
Epoch: [29][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.3701e-01 (9.9903e-01)	Acc@1  71.09 ( 71.35)	Acc@5  96.88 ( 93.40)
Epoch: [29][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6484e-01 (1.0013e+00)	Acc@1  78.12 ( 71.29)	Acc@5  94.53 ( 93.39)
Epoch: [29][130/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.6045e-01 (1.0023e+00)	Acc@1  73.44 ( 71.24)	Acc@5  92.19 ( 93.30)
Epoch: [29][140/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5977e-01 (9.9717e-01)	Acc@1  82.03 ( 71.34)	Acc@5  95.31 ( 93.34)
Epoch: [29][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0596e+00 (1.0003e+00)	Acc@1  71.09 ( 71.30)	Acc@5  91.41 ( 93.32)
Epoch: [29][160/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.4727e-01 (1.0092e+00)	Acc@1  72.66 ( 71.11)	Acc@5  94.53 ( 93.23)
Epoch: [29][170/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.9648e-01 (1.0067e+00)	Acc@1  77.34 ( 71.23)	Acc@5  94.53 ( 93.21)
Epoch: [29][180/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.2559e+00 (1.0065e+00)	Acc@1  67.97 ( 71.25)	Acc@5  89.84 ( 93.21)
Epoch: [29][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0293e+00 (1.0103e+00)	Acc@1  68.75 ( 71.14)	Acc@5  92.97 ( 93.15)
Epoch: [29][200/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.4912e-01 (1.0111e+00)	Acc@1  75.00 ( 71.10)	Acc@5  93.75 ( 93.12)
Epoch: [29][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.4629e-01 (1.0128e+00)	Acc@1  71.88 ( 71.07)	Acc@5  92.19 ( 93.06)
Epoch: [29][220/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0840e+00 (1.0139e+00)	Acc@1  67.97 ( 71.02)	Acc@5  91.41 ( 93.03)
Epoch: [29][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3320e+00 (1.0185e+00)	Acc@1  60.94 ( 70.92)	Acc@5  94.53 ( 93.03)
Epoch: [29][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1455e+00 (1.0206e+00)	Acc@1  63.28 ( 70.84)	Acc@5  92.97 ( 92.98)
Epoch: [29][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1143e+00 (1.0219e+00)	Acc@1  67.19 ( 70.80)	Acc@5  89.84 ( 92.94)
Epoch: [29][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0635e+00 (1.0209e+00)	Acc@1  68.75 ( 70.79)	Acc@5  91.41 ( 92.95)
Epoch: [29][270/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.002)	Loss 9.2529e-01 (1.0263e+00)	Acc@1  76.56 ( 70.64)	Acc@5  92.97 ( 92.88)
Epoch: [29][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0811e+00 (1.0282e+00)	Acc@1  67.19 ( 70.58)	Acc@5  93.75 ( 92.87)
Epoch: [29][290/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.0029e+00 (1.0306e+00)	Acc@1  67.97 ( 70.56)	Acc@5  93.75 ( 92.84)
Epoch: [29][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.5166e-01 (1.0321e+00)	Acc@1  69.53 ( 70.48)	Acc@5  92.97 ( 92.81)
Epoch: [29][310/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0928e+00 (1.0324e+00)	Acc@1  63.28 ( 70.47)	Acc@5  91.41 ( 92.80)
Epoch: [29][320/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.4473e-01 (1.0342e+00)	Acc@1  73.44 ( 70.43)	Acc@5  94.53 ( 92.76)
Epoch: [29][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0039e+00 (1.0337e+00)	Acc@1  69.53 ( 70.43)	Acc@5  91.41 ( 92.75)
Epoch: [29][340/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.0430e+00 (1.0356e+00)	Acc@1  69.53 ( 70.39)	Acc@5  94.53 ( 92.71)
Epoch: [29][350/391]	Time  0.029 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.3613e+00 (1.0379e+00)	Acc@1  64.06 ( 70.33)	Acc@5  87.50 ( 92.69)
Epoch: [29][360/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.1084e+00 (1.0409e+00)	Acc@1  67.19 ( 70.28)	Acc@5  89.06 ( 92.63)
Epoch: [29][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0645e+00 (1.0417e+00)	Acc@1  65.62 ( 70.25)	Acc@5  91.41 ( 92.61)
Epoch: [29][380/391]	Time  0.022 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.2295e+00 (1.0432e+00)	Acc@1  69.53 ( 70.21)	Acc@5  88.28 ( 92.55)
Epoch: [29][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4355e+00 (1.0467e+00)	Acc@1  58.75 ( 70.10)	Acc@5  86.25 ( 92.50)
## e[29] optimizer.zero_grad (sum) time: 0.11292147636413574
## e[29]       loss.backward (sum) time: 2.295958995819092
## e[29]      optimizer.step (sum) time: 0.9673826694488525
## epoch[29] training(only) time: 10.011558294296265
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.5254e+00 (1.5254e+00)	Acc@1  61.00 ( 61.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.016 ( 0.026)	Loss 1.5684e+00 (1.6258e+00)	Acc@1  60.00 ( 59.27)	Acc@5  89.00 ( 85.36)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 1.5459e+00 (1.5675e+00)	Acc@1  64.00 ( 59.57)	Acc@5  82.00 ( 85.62)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.7666e+00 (1.5751e+00)	Acc@1  57.00 ( 59.48)	Acc@5  85.00 ( 85.26)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 1.4873e+00 (1.5798e+00)	Acc@1  58.00 ( 59.29)	Acc@5  84.00 ( 85.12)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.4092e+00 (1.6126e+00)	Acc@1  64.00 ( 58.67)	Acc@5  87.00 ( 84.49)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.6094e+00 (1.6007e+00)	Acc@1  56.00 ( 58.75)	Acc@5  86.00 ( 84.66)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 1.6260e+00 (1.5989e+00)	Acc@1  62.00 ( 58.94)	Acc@5  77.00 ( 84.65)
Test: [ 80/100]	Time  0.018 ( 0.015)	Loss 1.6611e+00 (1.6088e+00)	Acc@1  60.00 ( 58.86)	Acc@5  81.00 ( 84.47)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.9229e+00 (1.5985e+00)	Acc@1  55.00 ( 59.11)	Acc@5  82.00 ( 84.63)
 * Acc@1 59.230 Acc@5 84.660
### epoch[29] execution time: 11.555163621902466
EPOCH 30
i:   0, name:       features.module.0.weight  changing lr from: 0.031237753974350746   to: 0.028154806218479014
i:   1, name:         features.module.0.bias  changing lr from: 0.033020893806132538   to: 0.029944495927113509
i:   2, name:       features.module.1.weight  changing lr from: 0.034783683771958576   to: 0.031720162707285268
i:   3, name:         features.module.1.bias  changing lr from: 0.036523252276474606   to: 0.033478258468834357
i:   4, name:       features.module.4.weight  changing lr from: 0.038237134577973945   to: 0.035215692976400439
i:   5, name:         features.module.4.bias  changing lr from: 0.039923234899376069   to: 0.036929793437137606
i:   6, name:       features.module.5.weight  changing lr from: 0.041579791254176157   to: 0.038618266728638488
i:   7, name:         features.module.5.bias  changing lr from: 0.043205342890819821   to: 0.040279164218525232
i:   8, name:       features.module.8.weight  changing lr from: 0.044798700243049112   to: 0.041910849101501259
i:   9, name:         features.module.8.bias  changing lr from: 0.046358917263072522   to: 0.043511966160757726
i:  10, name:       features.module.9.weight  changing lr from: 0.047885266007913235   to: 0.045081413847264287
i:  11, name:         features.module.9.bias  changing lr from: 0.049377213346134714   to: 0.046618318561601306
i:  12, name:      features.module.11.weight  changing lr from: 0.050834399651616721   to: 0.048122011017734195
i:  13, name:        features.module.11.bias  changing lr from: 0.052256619352574231   to: 0.049592004565758657
i:  14, name:      features.module.12.weight  changing lr from: 0.053643803207084562   to: 0.051027975350541314
i:  15, name:        features.module.12.bias  changing lr from: 0.054996002180618592   to: 0.052429744184838006
i:  16, name:      features.module.15.weight  changing lr from: 0.056313372806135444   to: 0.053797260018472084
i:  17, name:        features.module.15.bias  changing lr from: 0.057596163912929599   to: 0.055130584889148404
i:  18, name:      features.module.16.weight  changing lr from: 0.058844704616408783   to: 0.056429880245190682
i:  19, name:        features.module.16.bias  changing lr from: 0.060059393467154880   to: 0.057695394535682065
i:  20, name:      features.module.18.weight  changing lr from: 0.061240688663854084   to: 0.058927451968990144
i:  21, name:        features.module.18.bias  changing lr from: 0.062389099240864387   to: 0.060126442346307291
i:  22, name:      features.module.19.weight  changing lr from: 0.063505177147248168   to: 0.061292811882532494
i:  23, name:        features.module.19.bias  changing lr from: 0.064589510139969775   to: 0.062427054932464011
i:  24, name:      features.module.22.weight  changing lr from: 0.065642715419603773   to: 0.063529706545795858
i:  25, name:        features.module.22.bias  changing lr from: 0.066665433942288771   to: 0.064601335779768190
i:  26, name:      features.module.23.weight  changing lr from: 0.067658325346773837   to: 0.065642539703466993
i:  27, name:        features.module.23.bias  changing lr from: 0.068622063440231515   to: 0.066653938032685406
i:  28, name:      features.module.25.weight  changing lr from: 0.069557332191048205   to: 0.067636168338923122
i:  29, name:        features.module.25.bias  changing lr from: 0.070464822181049019   to: 0.068589881780506984
i:  30, name:      features.module.26.weight  changing lr from: 0.071345227473578840   to: 0.069515739307960442
i:  31, name:        features.module.26.bias  changing lr from: 0.072199242857548293   to: 0.070414408299632691
i:  32, name:            classifier.0.weight  changing lr from: 0.073027561430975205   to: 0.071286559587224435
i:  33, name:              classifier.0.bias  changing lr from: 0.073830872490721539   to: 0.072132864834225277
i:  34, name:            classifier.3.weight  changing lr from: 0.074609859698051256   to: 0.072953994233414018
i:  35, name:              classifier.3.bias  changing lr from: 0.075365199492335111   to: 0.073750614492480396
i:  36, name:            classifier.6.weight  changing lr from: 0.076097559727712261   to: 0.074523387079515349
i:  37, name:              classifier.6.bias  changing lr from: 0.076807598509802594   to: 0.075272966702598740



# Switched to train mode...
Epoch: [30][  0/391]	Time  0.189 ( 0.189)	Data  0.156 ( 0.156)	Loss 1.2441e+00 (1.2441e+00)	Acc@1  65.62 ( 65.62)	Acc@5  89.84 ( 89.84)
Epoch: [30][ 10/391]	Time  0.021 ( 0.040)	Data  0.001 ( 0.016)	Loss 8.1250e-01 (9.8553e-01)	Acc@1  75.00 ( 71.31)	Acc@5  96.88 ( 92.76)
Epoch: [30][ 20/391]	Time  0.021 ( 0.033)	Data  0.001 ( 0.009)	Loss 9.6143e-01 (9.7970e-01)	Acc@1  75.00 ( 72.21)	Acc@5  93.75 ( 92.93)
Epoch: [30][ 30/391]	Time  0.037 ( 0.030)	Data  0.010 ( 0.007)	Loss 8.6475e-01 (9.7907e-01)	Acc@1  75.78 ( 72.00)	Acc@5  94.53 ( 93.02)
Epoch: [30][ 40/391]	Time  0.031 ( 0.029)	Data  0.001 ( 0.006)	Loss 8.0713e-01 (9.6004e-01)	Acc@1  76.56 ( 72.64)	Acc@5  93.75 ( 93.08)
Epoch: [30][ 50/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 9.3457e-01 (9.5093e-01)	Acc@1  75.78 ( 73.02)	Acc@5  92.97 ( 93.32)
Epoch: [30][ 60/391]	Time  0.030 ( 0.028)	Data  0.003 ( 0.004)	Loss 9.7949e-01 (9.5446e-01)	Acc@1  70.31 ( 72.73)	Acc@5  92.97 ( 93.30)
Epoch: [30][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1025e+00 (9.6208e-01)	Acc@1  69.53 ( 72.38)	Acc@5  93.75 ( 93.20)
Epoch: [30][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.4385e-01 (9.7024e-01)	Acc@1  69.53 ( 72.16)	Acc@5  95.31 ( 93.19)
Epoch: [30][ 90/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.1436e+00 (9.6973e-01)	Acc@1  70.31 ( 72.32)	Acc@5  85.94 ( 93.10)
Epoch: [30][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0811e+00 (9.6746e-01)	Acc@1  67.97 ( 72.25)	Acc@5  91.41 ( 93.19)
Epoch: [30][110/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.003)	Loss 8.5596e-01 (9.6770e-01)	Acc@1  77.34 ( 72.13)	Acc@5  96.88 ( 93.28)
Epoch: [30][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0156e+00 (9.6448e-01)	Acc@1  71.88 ( 72.11)	Acc@5  91.41 ( 93.36)
Epoch: [30][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.4629e-01 (9.6563e-01)	Acc@1  74.22 ( 72.04)	Acc@5  93.75 ( 93.40)
Epoch: [30][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.3594e-01 (9.6600e-01)	Acc@1  72.66 ( 72.00)	Acc@5  95.31 ( 93.43)
Epoch: [30][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0146e+00 (9.6809e-01)	Acc@1  70.31 ( 71.95)	Acc@5  92.19 ( 93.42)
Epoch: [30][160/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.9463e-01 (9.6969e-01)	Acc@1  72.66 ( 71.85)	Acc@5  90.62 ( 93.39)
Epoch: [30][170/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2764e-01 (9.7058e-01)	Acc@1  78.12 ( 71.95)	Acc@5  96.88 ( 93.38)
Epoch: [30][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.8389e-01 (9.7627e-01)	Acc@1  71.09 ( 71.75)	Acc@5  94.53 ( 93.33)
Epoch: [30][190/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.0527e-01 (9.7830e-01)	Acc@1  72.66 ( 71.70)	Acc@5  95.31 ( 93.28)
Epoch: [30][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0000e+00 (9.8008e-01)	Acc@1  69.53 ( 71.68)	Acc@5  93.75 ( 93.24)
Epoch: [30][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.8242e-01 (9.8112e-01)	Acc@1  73.44 ( 71.65)	Acc@5  96.09 ( 93.23)
Epoch: [30][220/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.2129e+00 (9.8264e-01)	Acc@1  65.62 ( 71.65)	Acc@5  89.06 ( 93.20)
Epoch: [30][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0557e+00 (9.8380e-01)	Acc@1  67.97 ( 71.66)	Acc@5  91.41 ( 93.14)
Epoch: [30][240/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.3838e-01 (9.8359e-01)	Acc@1  72.66 ( 71.66)	Acc@5  96.09 ( 93.16)
Epoch: [30][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1309e+00 (9.8420e-01)	Acc@1  69.53 ( 71.65)	Acc@5  90.62 ( 93.17)
Epoch: [30][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.9805e-01 (9.8645e-01)	Acc@1  78.91 ( 71.62)	Acc@5  92.19 ( 93.12)
Epoch: [30][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1377e+00 (9.8592e-01)	Acc@1  71.09 ( 71.63)	Acc@5  89.06 ( 93.10)
Epoch: [30][280/391]	Time  0.047 ( 0.026)	Data  0.006 ( 0.002)	Loss 1.1875e+00 (9.8962e-01)	Acc@1  66.41 ( 71.47)	Acc@5  89.84 ( 93.05)
Epoch: [30][290/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.002)	Loss 9.0332e-01 (9.8961e-01)	Acc@1  73.44 ( 71.42)	Acc@5  93.75 ( 93.08)
Epoch: [30][300/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1660e+00 (9.9208e-01)	Acc@1  65.62 ( 71.32)	Acc@5  92.97 ( 93.09)
Epoch: [30][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.4336e-01 (9.9226e-01)	Acc@1  75.00 ( 71.35)	Acc@5  93.75 ( 93.10)
Epoch: [30][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.7480e-01 (9.9194e-01)	Acc@1  79.69 ( 71.38)	Acc@5  97.66 ( 93.06)
Epoch: [30][330/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 8.9746e-01 (9.9240e-01)	Acc@1  71.88 ( 71.39)	Acc@5  93.75 ( 93.05)
Epoch: [30][340/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0586e+00 (9.9529e-01)	Acc@1  69.53 ( 71.31)	Acc@5  92.97 ( 93.04)
Epoch: [30][350/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (9.9739e-01)	Acc@1  68.75 ( 71.26)	Acc@5  92.97 ( 93.03)
Epoch: [30][360/391]	Time  0.048 ( 0.025)	Data  0.006 ( 0.002)	Loss 1.0615e+00 (9.9908e-01)	Acc@1  67.97 ( 71.17)	Acc@5  90.62 ( 93.02)
Epoch: [30][370/391]	Time  0.037 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.1631e+00 (9.9896e-01)	Acc@1  69.53 ( 71.16)	Acc@5  89.84 ( 93.03)
Epoch: [30][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 8.7744e-01 (9.9963e-01)	Acc@1  74.22 ( 71.17)	Acc@5  94.53 ( 93.03)
Epoch: [30][390/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0830e+00 (9.9974e-01)	Acc@1  65.00 ( 71.17)	Acc@5  97.50 ( 93.03)
## e[30] optimizer.zero_grad (sum) time: 0.11463069915771484
## e[30]       loss.backward (sum) time: 2.2831835746765137
## e[30]      optimizer.step (sum) time: 0.974337100982666
## epoch[30] training(only) time: 10.045100212097168
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.6201e+00 (1.6201e+00)	Acc@1  59.00 ( 59.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.6641e+00 (1.6515e+00)	Acc@1  58.00 ( 58.73)	Acc@5  82.00 ( 83.73)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.5713e+00 (1.6382e+00)	Acc@1  66.00 ( 59.52)	Acc@5  84.00 ( 84.62)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.6543e+00 (1.6157e+00)	Acc@1  56.00 ( 59.29)	Acc@5  85.00 ( 85.10)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.4248e+00 (1.6080e+00)	Acc@1  59.00 ( 59.12)	Acc@5  90.00 ( 85.20)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.3857e+00 (1.6240e+00)	Acc@1  65.00 ( 58.98)	Acc@5  87.00 ( 84.80)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.7139e+00 (1.6038e+00)	Acc@1  57.00 ( 59.36)	Acc@5  80.00 ( 85.08)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 1.7295e+00 (1.6104e+00)	Acc@1  60.00 ( 59.24)	Acc@5  82.00 ( 85.01)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8242e+00 (1.6124e+00)	Acc@1  59.00 ( 59.16)	Acc@5  78.00 ( 84.91)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 1.8516e+00 (1.6115e+00)	Acc@1  52.00 ( 59.21)	Acc@5  84.00 ( 84.92)
 * Acc@1 59.300 Acc@5 84.830
### epoch[30] execution time: 11.5907142162323
EPOCH 31
i:   0, name:       features.module.0.weight  changing lr from: 0.028154806218479014   to: 0.025178003922785554
i:   1, name:         features.module.0.bias  changing lr from: 0.029944495927113509   to: 0.026962155163229907
i:   2, name:       features.module.1.weight  changing lr from: 0.031720162707285268   to: 0.028739466335800542
i:   3, name:         features.module.1.bias  changing lr from: 0.033478258468834357   to: 0.030505670535428204
i:   4, name:       features.module.4.weight  changing lr from: 0.035215692976400439   to: 0.032257009550970731
i:   5, name:         features.module.4.bias  changing lr from: 0.036929793437137606   to: 0.033990191393004895
i:   6, name:       features.module.5.weight  changing lr from: 0.038618266728638488   to: 0.035702350294610420
i:   7, name:         features.module.5.bias  changing lr from: 0.040279164218525232   to: 0.037391009198414371
i:   8, name:       features.module.8.weight  changing lr from: 0.041910849101501259   to: 0.039054044705994710
i:   9, name:         features.module.8.bias  changing lr from: 0.043511966160757726   to: 0.040689654437069123
i:  10, name:       features.module.9.weight  changing lr from: 0.045081413847264287   to: 0.042296326724265691
i:  11, name:         features.module.9.bias  changing lr from: 0.046618318561601306   to: 0.043872812553444052
i:  12, name:      features.module.11.weight  changing lr from: 0.048122011017734195   to: 0.045418099648454176
i:  13, name:        features.module.11.bias  changing lr from: 0.049592004565758657   to: 0.046931388591994735
i:  14, name:      features.module.12.weight  changing lr from: 0.051027975350541314   to: 0.048412070870109278
i:  15, name:        features.module.12.bias  changing lr from: 0.052429744184838006   to: 0.049859708726207930
i:  16, name:      features.module.15.weight  changing lr from: 0.053797260018472084   to: 0.051274016710798870
i:  17, name:        features.module.15.bias  changing lr from: 0.055130584889148404   to: 0.052654844814916751
i:  18, name:      features.module.16.weight  changing lr from: 0.056429880245190682   to: 0.054002163078188795
i:  19, name:        features.module.16.bias  changing lr from: 0.057695394535682065   to: 0.055316047566277608
i:  20, name:      features.module.18.weight  changing lr from: 0.058927451968990144   to: 0.056596667616850997
i:  21, name:        features.module.18.bias  changing lr from: 0.060126442346307291   to: 0.057844274258042963
i:  22, name:      features.module.19.weight  changing lr from: 0.061292811882532494   to: 0.059059189708440180
i:  23, name:        features.module.19.bias  changing lr from: 0.062427054932464011   to: 0.060241797872818671
i:  24, name:      features.module.22.weight  changing lr from: 0.063529706545795858   to: 0.061392535753070882
i:  25, name:        features.module.22.bias  changing lr from: 0.064601335779768190   to: 0.062511885698925421
i:  26, name:      features.module.23.weight  changing lr from: 0.065642539703466993   to: 0.063600368428108497
i:  27, name:        features.module.23.bias  changing lr from: 0.066653938032685406   to: 0.064658536750486259
i:  28, name:      features.module.25.weight  changing lr from: 0.067636168338923122   to: 0.065686969935426665
i:  29, name:        features.module.25.bias  changing lr from: 0.068589881780506984   to: 0.066686268666105372
i:  30, name:      features.module.26.weight  changing lr from: 0.069515739307960442   to: 0.067657050528740048
i:  31, name:        features.module.26.bias  changing lr from: 0.070414408299632691   to: 0.068599945988762470
i:  32, name:            classifier.0.weight  changing lr from: 0.071286559587224435   to: 0.069515594809723413
i:  33, name:              classifier.0.bias  changing lr from: 0.072132864834225277   to: 0.070404642874277432
i:  34, name:            classifier.3.weight  changing lr from: 0.072953994233414018   to: 0.071267739369910807
i:  35, name:              classifier.3.bias  changing lr from: 0.073750614492480396   to: 0.072105534305169125
i:  36, name:            classifier.6.weight  changing lr from: 0.074523387079515349   to: 0.072918676325014933
i:  37, name:              classifier.6.bias  changing lr from: 0.075272966702598740   to: 0.073707810796611109



# Switched to train mode...
Epoch: [31][  0/391]	Time  0.175 ( 0.175)	Data  0.149 ( 0.149)	Loss 7.7734e-01 (7.7734e-01)	Acc@1  78.91 ( 78.91)	Acc@5  94.53 ( 94.53)
Epoch: [31][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.015)	Loss 9.9854e-01 (8.8104e-01)	Acc@1  73.44 ( 74.36)	Acc@5  92.97 ( 94.39)
Epoch: [31][ 20/391]	Time  0.029 ( 0.032)	Data  0.002 ( 0.009)	Loss 7.8369e-01 (9.0851e-01)	Acc@1  75.78 ( 73.14)	Acc@5  94.53 ( 93.94)
Epoch: [31][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.1953e+00 (9.0713e-01)	Acc@1  67.97 ( 73.66)	Acc@5  93.75 ( 94.00)
Epoch: [31][ 40/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.005)	Loss 8.1982e-01 (9.0082e-01)	Acc@1  77.34 ( 73.74)	Acc@5  93.75 ( 94.09)
Epoch: [31][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.0498e+00 (8.9982e-01)	Acc@1  69.53 ( 73.56)	Acc@5  96.09 ( 94.06)
Epoch: [31][ 60/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.2070e-01 (8.9765e-01)	Acc@1  72.66 ( 73.64)	Acc@5  96.88 ( 94.04)
Epoch: [31][ 70/391]	Time  0.039 ( 0.027)	Data  0.002 ( 0.004)	Loss 8.8232e-01 (9.0194e-01)	Acc@1  71.88 ( 73.65)	Acc@5  94.53 ( 94.03)
Epoch: [31][ 80/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.5303e-01 (9.0809e-01)	Acc@1  75.00 ( 73.45)	Acc@5  93.75 ( 93.97)
Epoch: [31][ 90/391]	Time  0.037 ( 0.027)	Data  0.003 ( 0.003)	Loss 8.6328e-01 (9.1337e-01)	Acc@1  75.78 ( 73.47)	Acc@5  94.53 ( 93.84)
Epoch: [31][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2500e+00 (9.1625e-01)	Acc@1  64.06 ( 73.45)	Acc@5  96.88 ( 93.89)
Epoch: [31][110/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.0381e+00 (9.1733e-01)	Acc@1  70.31 ( 73.46)	Acc@5  93.75 ( 93.84)
Epoch: [31][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1055e-01 (9.1788e-01)	Acc@1  78.91 ( 73.48)	Acc@5  95.31 ( 93.89)
Epoch: [31][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0166e+00 (9.1952e-01)	Acc@1  64.84 ( 73.37)	Acc@5  91.41 ( 93.88)
Epoch: [31][140/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.7354e-01 (9.2943e-01)	Acc@1  77.34 ( 73.16)	Acc@5  92.19 ( 93.74)
Epoch: [31][150/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.3252e-01 (9.2998e-01)	Acc@1  77.34 ( 73.11)	Acc@5  94.53 ( 93.77)
Epoch: [31][160/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0049e+00 (9.2943e-01)	Acc@1  67.19 ( 73.07)	Acc@5  92.97 ( 93.78)
Epoch: [31][170/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.1846e-01 (9.3367e-01)	Acc@1  74.22 ( 72.92)	Acc@5  95.31 ( 93.75)
Epoch: [31][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.1455e-01 (9.3185e-01)	Acc@1  74.22 ( 72.98)	Acc@5  96.09 ( 93.76)
Epoch: [31][190/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0166e+00 (9.3363e-01)	Acc@1  69.53 ( 72.90)	Acc@5  94.53 ( 93.74)
Epoch: [31][200/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5898e-01 (9.3624e-01)	Acc@1  72.66 ( 72.88)	Acc@5  93.75 ( 93.71)
Epoch: [31][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5850e-01 (9.3703e-01)	Acc@1  71.88 ( 72.89)	Acc@5  92.97 ( 93.67)
Epoch: [31][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1299e+00 (9.3714e-01)	Acc@1  68.75 ( 72.89)	Acc@5  92.19 ( 93.68)
Epoch: [31][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1924e+00 (9.4025e-01)	Acc@1  65.62 ( 72.84)	Acc@5  92.97 ( 93.66)
Epoch: [31][240/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.003)	Loss 7.9297e-01 (9.4070e-01)	Acc@1  72.66 ( 72.79)	Acc@5  95.31 ( 93.63)
Epoch: [31][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.1455e-01 (9.4251e-01)	Acc@1  70.31 ( 72.73)	Acc@5  93.75 ( 93.62)
Epoch: [31][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.3945e-01 (9.4320e-01)	Acc@1  71.09 ( 72.70)	Acc@5  96.88 ( 93.61)
Epoch: [31][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.1992e-01 (9.4630e-01)	Acc@1  73.44 ( 72.63)	Acc@5  92.97 ( 93.54)
Epoch: [31][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.2236e-01 (9.4540e-01)	Acc@1  75.00 ( 72.66)	Acc@5  92.97 ( 93.56)
Epoch: [31][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.4912e-01 (9.4895e-01)	Acc@1  76.56 ( 72.58)	Acc@5  95.31 ( 93.53)
Epoch: [31][300/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.1113e-01 (9.5003e-01)	Acc@1  69.53 ( 72.57)	Acc@5  93.75 ( 93.53)
Epoch: [31][310/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.2061e+00 (9.5415e-01)	Acc@1  69.53 ( 72.49)	Acc@5  91.41 ( 93.47)
Epoch: [31][320/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1250e+00 (9.5606e-01)	Acc@1  67.19 ( 72.45)	Acc@5  89.06 ( 93.45)
Epoch: [31][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.9219e-01 (9.5564e-01)	Acc@1  73.44 ( 72.46)	Acc@5  92.19 ( 93.46)
Epoch: [31][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.4092e-01 (9.5582e-01)	Acc@1  71.09 ( 72.45)	Acc@5  94.53 ( 93.47)
Epoch: [31][350/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.0801e+00 (9.5472e-01)	Acc@1  67.97 ( 72.47)	Acc@5  90.62 ( 93.48)
Epoch: [31][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0020e+00 (9.5476e-01)	Acc@1  73.44 ( 72.47)	Acc@5  92.19 ( 93.47)
Epoch: [31][370/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.2256e+00 (9.5690e-01)	Acc@1  62.50 ( 72.42)	Acc@5  92.19 ( 93.46)
Epoch: [31][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.2578e-01 (9.5702e-01)	Acc@1  75.78 ( 72.39)	Acc@5  96.09 ( 93.49)
Epoch: [31][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3301e+00 (9.5731e-01)	Acc@1  66.25 ( 72.38)	Acc@5  90.00 ( 93.49)
## e[31] optimizer.zero_grad (sum) time: 0.11368560791015625
## e[31]       loss.backward (sum) time: 2.293996572494507
## e[31]      optimizer.step (sum) time: 0.9578077793121338
## epoch[31] training(only) time: 10.051195859909058
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.5117e+00 (1.5117e+00)	Acc@1  58.00 ( 58.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.014 ( 0.026)	Loss 1.5859e+00 (1.5384e+00)	Acc@1  56.00 ( 60.09)	Acc@5  87.00 ( 86.36)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.6650e+00 (1.5133e+00)	Acc@1  66.00 ( 61.14)	Acc@5  82.00 ( 86.10)
Test: [ 30/100]	Time  0.018 ( 0.018)	Loss 1.6455e+00 (1.5132e+00)	Acc@1  55.00 ( 61.00)	Acc@5  87.00 ( 86.06)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 1.5986e+00 (1.5156e+00)	Acc@1  63.00 ( 60.44)	Acc@5  84.00 ( 86.17)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.3613e+00 (1.5319e+00)	Acc@1  68.00 ( 60.14)	Acc@5  86.00 ( 85.78)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 1.6367e+00 (1.5142e+00)	Acc@1  58.00 ( 60.54)	Acc@5  85.00 ( 86.02)
Test: [ 70/100]	Time  0.014 ( 0.015)	Loss 1.7520e+00 (1.5219e+00)	Acc@1  64.00 ( 60.62)	Acc@5  84.00 ( 86.08)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.9268e+00 (1.5313e+00)	Acc@1  58.00 ( 60.52)	Acc@5  76.00 ( 85.95)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.8154e+00 (1.5266e+00)	Acc@1  55.00 ( 60.56)	Acc@5  83.00 ( 86.03)
 * Acc@1 60.440 Acc@5 86.150
### epoch[31] execution time: 11.601655721664429
EPOCH 32
i:   0, name:       features.module.0.weight  changing lr from: 0.025178003922785554   to: 0.022321178182447728
i:   1, name:         features.module.0.bias  changing lr from: 0.026962155163229907   to: 0.024087193939232573
i:   2, name:       features.module.1.weight  changing lr from: 0.028739466335800542   to: 0.025854399499096587
i:   3, name:         features.module.1.bias  changing lr from: 0.030505670535428204   to: 0.027617772233990701
i:   4, name:       features.module.4.weight  changing lr from: 0.032257009550970731   to: 0.029372847984137890
i:   5, name:         features.module.4.bias  changing lr from: 0.033990191393004895   to: 0.031115677106709951
i:   6, name:       features.module.5.weight  changing lr from: 0.035702350294610420   to: 0.032842782724861290
i:   7, name:         features.module.5.bias  changing lr from: 0.037391009198414371   to: 0.034551121267116250
i:   8, name:       features.module.8.weight  changing lr from: 0.039054044705994710   to: 0.036238045336653459
i:   9, name:         features.module.8.bias  changing lr from: 0.040689654437069123   to: 0.037901268909969081
i:  10, name:       features.module.9.weight  changing lr from: 0.042296326724265691   to: 0.039538834833050630
i:  11, name:         features.module.9.bias  changing lr from: 0.043872812553444052   to: 0.041149084559105154
i:  12, name:      features.module.11.weight  changing lr from: 0.045418099648454176   to: 0.042730630053814944
i:  13, name:        features.module.11.bias  changing lr from: 0.046931388591994735   to: 0.044282327780985516
i:  14, name:      features.module.12.weight  changing lr from: 0.048412070870109278   to: 0.045803254672395780
i:  15, name:        features.module.12.bias  changing lr from: 0.049859708726207930   to: 0.047292685979900800
i:  16, name:      features.module.15.weight  changing lr from: 0.051274016710798870   to: 0.048750074904728470
i:  17, name:        features.module.15.bias  changing lr from: 0.052654844814916751   to: 0.050175033897910219
i:  18, name:      features.module.16.weight  changing lr from: 0.054002163078188795   to: 0.051567317526447512
i:  19, name:        features.module.16.bias  changing lr from: 0.055316047566277608   to: 0.052926806801756425
i:  20, name:      features.module.18.weight  changing lr from: 0.056596667616850997   to: 0.054253494869852051
i:  21, name:        features.module.18.bias  changing lr from: 0.057844274258042963   to: 0.055547473966367181
i:  22, name:      features.module.19.weight  changing lr from: 0.059059189708440180   to: 0.056808923543644751
i:  23, name:        features.module.19.bias  changing lr from: 0.060241797872818671   to: 0.058038099481627685
i:  24, name:      features.module.22.weight  changing lr from: 0.061392535753070882   to: 0.059235324298955228
i:  25, name:        features.module.22.bias  changing lr from: 0.062511885698925421   to: 0.060400978285457457
i:  26, name:      features.module.23.weight  changing lr from: 0.063600368428108497   to: 0.061535491482026729
i:  27, name:        features.module.23.bias  changing lr from: 0.064658536750486259   to: 0.062639336438575097
i:  28, name:      features.module.25.weight  changing lr from: 0.065686969935426665   to: 0.063713021685405727
i:  29, name:        features.module.25.bias  changing lr from: 0.066686268666105372   to: 0.064757085857796784
i:  30, name:      features.module.26.weight  changing lr from: 0.067657050528740048   to: 0.065772092417890601
i:  31, name:        features.module.26.bias  changing lr from: 0.068599945988762470   to: 0.066758624922079443
i:  32, name:            classifier.0.weight  changing lr from: 0.069515594809723413   to: 0.067717282785970806
i:  33, name:              classifier.0.bias  changing lr from: 0.070404642874277432   to: 0.068648677502692212
i:  34, name:            classifier.3.weight  changing lr from: 0.071267739369910807   to: 0.069553429273757156
i:  35, name:              classifier.3.bias  changing lr from: 0.072105534305169125   to: 0.070432164014958928
i:  36, name:            classifier.6.weight  changing lr from: 0.072918676325014933   to: 0.071285510702795721
i:  37, name:              classifier.6.bias  changing lr from: 0.073707810796611109   to: 0.072114099029759390



# Switched to train mode...
Epoch: [32][  0/391]	Time  0.179 ( 0.179)	Data  0.139 ( 0.139)	Loss 6.2988e-01 (6.2988e-01)	Acc@1  79.69 ( 79.69)	Acc@5  97.66 ( 97.66)
Epoch: [32][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.014)	Loss 8.5742e-01 (9.0958e-01)	Acc@1  77.34 ( 74.79)	Acc@5  93.75 ( 93.96)
Epoch: [32][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 7.5830e-01 (9.0416e-01)	Acc@1  78.91 ( 74.55)	Acc@5  93.75 ( 94.05)
Epoch: [32][ 30/391]	Time  0.034 ( 0.030)	Data  0.001 ( 0.006)	Loss 8.0371e-01 (8.6687e-01)	Acc@1  75.00 ( 74.97)	Acc@5  92.19 ( 94.46)
Epoch: [32][ 40/391]	Time  0.044 ( 0.029)	Data  0.005 ( 0.005)	Loss 7.4170e-01 (8.7193e-01)	Acc@1  77.34 ( 74.87)	Acc@5  96.88 ( 94.28)
Epoch: [32][ 50/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.1221e+00 (8.7922e-01)	Acc@1  62.50 ( 74.51)	Acc@5  95.31 ( 94.29)
Epoch: [32][ 60/391]	Time  0.030 ( 0.028)	Data  0.000 ( 0.004)	Loss 8.5059e-01 (8.8089e-01)	Acc@1  72.66 ( 74.36)	Acc@5  94.53 ( 94.33)
Epoch: [32][ 70/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.9805e-01 (8.8153e-01)	Acc@1  73.44 ( 74.32)	Acc@5  90.62 ( 94.31)
Epoch: [32][ 80/391]	Time  0.020 ( 0.027)	Data  0.000 ( 0.003)	Loss 7.9541e-01 (8.7704e-01)	Acc@1  76.56 ( 74.42)	Acc@5  96.09 ( 94.28)
Epoch: [32][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.4287e-01 (8.7593e-01)	Acc@1  72.66 ( 74.44)	Acc@5  94.53 ( 94.36)
Epoch: [32][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.1699e-01 (8.7099e-01)	Acc@1  71.09 ( 74.57)	Acc@5  93.75 ( 94.46)
Epoch: [32][110/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2715e-01 (8.6936e-01)	Acc@1  75.00 ( 74.62)	Acc@5  96.09 ( 94.47)
Epoch: [32][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0684e+00 (8.6915e-01)	Acc@1  70.31 ( 74.67)	Acc@5  92.97 ( 94.40)
Epoch: [32][130/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.0176e+00 (8.7250e-01)	Acc@1  73.44 ( 74.62)	Acc@5  92.19 ( 94.34)
Epoch: [32][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9297e-01 (8.7800e-01)	Acc@1  75.78 ( 74.49)	Acc@5  94.53 ( 94.28)
Epoch: [32][150/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.9307e-01 (8.8584e-01)	Acc@1  73.44 ( 74.29)	Acc@5  93.75 ( 94.21)
Epoch: [32][160/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.3555e-01 (8.8473e-01)	Acc@1  71.88 ( 74.31)	Acc@5  96.88 ( 94.26)
Epoch: [32][170/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.7803e-01 (8.8373e-01)	Acc@1  72.66 ( 74.32)	Acc@5  93.75 ( 94.27)
Epoch: [32][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6953e-01 (8.8311e-01)	Acc@1  79.69 ( 74.40)	Acc@5  96.09 ( 94.31)
Epoch: [32][190/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0781e+00 (8.8771e-01)	Acc@1  72.66 ( 74.23)	Acc@5  88.28 ( 94.29)
Epoch: [32][200/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.0781e+00 (8.8985e-01)	Acc@1  68.75 ( 74.16)	Acc@5  92.97 ( 94.26)
Epoch: [32][210/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.7803e-01 (8.9254e-01)	Acc@1  71.09 ( 74.03)	Acc@5  91.41 ( 94.21)
Epoch: [32][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0078e+00 (8.9022e-01)	Acc@1  71.09 ( 74.06)	Acc@5  92.97 ( 94.21)
Epoch: [32][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.9678e-01 (8.9002e-01)	Acc@1  78.12 ( 74.07)	Acc@5  94.53 ( 94.22)
Epoch: [32][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.7607e-01 (8.9335e-01)	Acc@1  71.88 ( 74.04)	Acc@5  93.75 ( 94.18)
Epoch: [32][250/391]	Time  0.041 ( 0.026)	Data  0.006 ( 0.002)	Loss 1.2617e+00 (8.9511e-01)	Acc@1  66.41 ( 74.02)	Acc@5  89.06 ( 94.15)
Epoch: [32][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.7988e-01 (8.9769e-01)	Acc@1  75.00 ( 73.91)	Acc@5  94.53 ( 94.12)
Epoch: [32][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.5830e-01 (9.0240e-01)	Acc@1  71.88 ( 73.76)	Acc@5  97.66 ( 94.06)
Epoch: [32][280/391]	Time  0.032 ( 0.026)	Data  0.004 ( 0.002)	Loss 9.4189e-01 (9.0417e-01)	Acc@1  73.44 ( 73.72)	Acc@5  96.09 ( 94.07)
Epoch: [32][290/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.0078e-01 (9.0770e-01)	Acc@1  73.44 ( 73.65)	Acc@5  96.88 ( 94.03)
Epoch: [32][300/391]	Time  0.038 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.0137e+00 (9.0947e-01)	Acc@1  72.66 ( 73.63)	Acc@5  92.19 ( 94.01)
Epoch: [32][310/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 7.9980e-01 (9.1006e-01)	Acc@1  75.00 ( 73.60)	Acc@5  94.53 ( 94.00)
Epoch: [32][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.1406e-01 (9.0987e-01)	Acc@1  73.44 ( 73.58)	Acc@5  94.53 ( 94.00)
Epoch: [32][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0449e+00 (9.0956e-01)	Acc@1  72.66 ( 73.58)	Acc@5  92.97 ( 94.01)
Epoch: [32][340/391]	Time  0.037 ( 0.026)	Data  0.005 ( 0.002)	Loss 9.0283e-01 (9.1100e-01)	Acc@1  73.44 ( 73.53)	Acc@5  92.19 ( 94.00)
Epoch: [32][350/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.002)	Loss 9.4336e-01 (9.1298e-01)	Acc@1  73.44 ( 73.51)	Acc@5  92.97 ( 93.95)
Epoch: [32][360/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.8340e-01 (9.1335e-01)	Acc@1  68.75 ( 73.49)	Acc@5  92.97 ( 93.97)
Epoch: [32][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.1016e-01 (9.1502e-01)	Acc@1  72.66 ( 73.45)	Acc@5  92.19 ( 93.94)
Epoch: [32][380/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.002)	Loss 8.8721e-01 (9.1677e-01)	Acc@1  76.56 ( 73.41)	Acc@5  89.84 ( 93.91)
Epoch: [32][390/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.0635e+00 (9.1728e-01)	Acc@1  72.50 ( 73.38)	Acc@5  88.75 ( 93.91)
## e[32] optimizer.zero_grad (sum) time: 0.11497330665588379
## e[32]       loss.backward (sum) time: 2.2910168170928955
## e[32]      optimizer.step (sum) time: 0.9768292903900146
## epoch[32] training(only) time: 10.059005737304688
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.5312e+00 (1.5312e+00)	Acc@1  62.00 ( 62.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.5225e+00 (1.6456e+00)	Acc@1  64.00 ( 60.91)	Acc@5  90.00 ( 85.45)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.5957e+00 (1.5970e+00)	Acc@1  64.00 ( 61.57)	Acc@5  85.00 ( 85.67)
Test: [ 30/100]	Time  0.016 ( 0.017)	Loss 1.6934e+00 (1.5894e+00)	Acc@1  59.00 ( 61.10)	Acc@5  87.00 ( 85.58)
Test: [ 40/100]	Time  0.016 ( 0.016)	Loss 1.3916e+00 (1.5697e+00)	Acc@1  64.00 ( 60.90)	Acc@5  87.00 ( 85.68)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.4365e+00 (1.5964e+00)	Acc@1  64.00 ( 60.55)	Acc@5  87.00 ( 85.35)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.6973e+00 (1.5697e+00)	Acc@1  58.00 ( 60.98)	Acc@5  86.00 ( 85.77)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 1.5059e+00 (1.5652e+00)	Acc@1  60.00 ( 60.92)	Acc@5  86.00 ( 85.86)
Test: [ 80/100]	Time  0.019 ( 0.015)	Loss 1.8242e+00 (1.5787e+00)	Acc@1  57.00 ( 60.67)	Acc@5  75.00 ( 85.65)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.8457e+00 (1.5665e+00)	Acc@1  55.00 ( 60.89)	Acc@5  80.00 ( 85.73)
 * Acc@1 60.970 Acc@5 85.750
### epoch[32] execution time: 11.5940842628479
EPOCH 33
i:   0, name:       features.module.0.weight  changing lr from: 0.022321178182447728   to: 0.019597602646433312
i:   1, name:         features.module.0.bias  changing lr from: 0.024087193939232573   to: 0.021332455004357948
i:   2, name:       features.module.1.weight  changing lr from: 0.025854399499096587   to: 0.023077356221663927
i:   3, name:         features.module.1.bias  changing lr from: 0.027617772233990701   to: 0.024826497355511901
i:   4, name:       features.module.4.weight  changing lr from: 0.029372847984137890   to: 0.026574675660524349
i:   5, name:         features.module.4.bias  changing lr from: 0.031115677106709951   to: 0.028317249860730934
i:   6, name:       features.module.5.weight  changing lr from: 0.032842782724861290   to: 0.030050097231223444
i:   7, name:         features.module.5.bias  changing lr from: 0.034551121267116250   to: 0.031769572672088187
i:   8, name:       features.module.8.weight  changing lr from: 0.036238045336653459   to: 0.033472469891683641
i:   9, name:         features.module.8.bias  changing lr from: 0.037901268909969081   to: 0.035155984763255245
i:  10, name:       features.module.9.weight  changing lr from: 0.039538834833050630   to: 0.036817680876322742
i:  11, name:         features.module.9.bias  changing lr from: 0.041149084559105154   to: 0.038455457270578162
i:  12, name:      features.module.11.weight  changing lr from: 0.042730630053814944   to: 0.040067518313761680
i:  13, name:        features.module.11.bias  changing lr from: 0.044282327780985516   to: 0.041652345664901703
i:  14, name:      features.module.12.weight  changing lr from: 0.045803254672395780   to: 0.043208672249352827
i:  15, name:        features.module.12.bias  changing lr from: 0.047292685979900800   to: 0.044735458161337276
i:  16, name:      features.module.15.weight  changing lr from: 0.048750074904728470   to: 0.046231868402421952
i:  17, name:        features.module.15.bias  changing lr from: 0.050175033897910219   to: 0.047697252359891931
i:  18, name:      features.module.16.weight  changing lr from: 0.051567317526447512   to: 0.049131124926763588
i:  19, name:        features.module.16.bias  changing lr from: 0.052926806801756425   to: 0.050533149164753301
i:  20, name:      features.module.18.weight  changing lr from: 0.054253494869852051   to: 0.051903120412499483
i:  21, name:        features.module.18.bias  changing lr from: 0.055547473966367181   to: 0.053240951743398823
i:  22, name:      features.module.19.weight  changing lr from: 0.056808923543644751   to: 0.054546660680303560
i:  23, name:        features.module.19.bias  changing lr from: 0.058038099481627685   to: 0.055820357077812993
i:  24, name:      features.module.22.weight  changing lr from: 0.059235324298955228   to: 0.057062232086799461
i:  25, name:        features.module.22.bias  changing lr from: 0.060400978285457457   to: 0.058272548119996541
i:  26, name:      features.module.23.weight  changing lr from: 0.061535491482026729   to: 0.059451629741820625
i:  27, name:        features.module.23.bias  changing lr from: 0.062639336438575097   to: 0.060599855410011054
i:  28, name:      features.module.25.weight  changing lr from: 0.063713021685405727   to: 0.061717650001078032
i:  29, name:        features.module.25.bias  changing lr from: 0.064757085857796784   to: 0.062805478055888792
i:  30, name:      features.module.26.weight  changing lr from: 0.065772092417890601   to: 0.063863837685954980
i:  31, name:        features.module.26.bias  changing lr from: 0.066758624922079443   to: 0.064893255085076471
i:  32, name:            classifier.0.weight  changing lr from: 0.067717282785970806   to: 0.065894279594923597
i:  33, name:              classifier.0.bias  changing lr from: 0.068648677502692212   to: 0.066867479276888298
i:  34, name:            classifier.3.weight  changing lr from: 0.069553429273757156   to: 0.067813436946090752
i:  35, name:              classifier.3.bias  changing lr from: 0.070432164014958928   to: 0.068732746626789326
i:  36, name:            classifier.6.weight  changing lr from: 0.071285510702795721   to: 0.069626010391606416
i:  37, name:              classifier.6.bias  changing lr from: 0.072114099029759390   to: 0.070493835549950146



# Switched to train mode...
Epoch: [33][  0/391]	Time  0.179 ( 0.179)	Data  0.148 ( 0.148)	Loss 7.7979e-01 (7.7979e-01)	Acc@1  78.12 ( 78.12)	Acc@5  95.31 ( 95.31)
Epoch: [33][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.0293e+00 (7.8809e-01)	Acc@1  69.53 ( 77.41)	Acc@5  90.62 ( 94.53)
Epoch: [33][ 20/391]	Time  0.025 ( 0.033)	Data  0.001 ( 0.009)	Loss 8.7158e-01 (7.8016e-01)	Acc@1  76.56 ( 77.57)	Acc@5  95.31 ( 94.79)
Epoch: [33][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.006)	Loss 7.5293e-01 (7.8530e-01)	Acc@1  78.91 ( 77.82)	Acc@5  96.09 ( 94.83)
Epoch: [33][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 9.8389e-01 (7.9470e-01)	Acc@1  71.88 ( 77.44)	Acc@5  92.97 ( 94.86)
Epoch: [33][ 50/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 9.6680e-01 (8.0447e-01)	Acc@1  75.00 ( 77.18)	Acc@5  93.75 ( 94.68)
Epoch: [33][ 60/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.004)	Loss 8.4277e-01 (8.0852e-01)	Acc@1  68.75 ( 76.92)	Acc@5  96.88 ( 94.76)
Epoch: [33][ 70/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.6709e-01 (8.1933e-01)	Acc@1  78.91 ( 76.64)	Acc@5  92.97 ( 94.62)
Epoch: [33][ 80/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.8281e-01 (8.2879e-01)	Acc@1  74.22 ( 76.53)	Acc@5  94.53 ( 94.49)
Epoch: [33][ 90/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.003)	Loss 8.8184e-01 (8.2408e-01)	Acc@1  75.78 ( 76.55)	Acc@5  95.31 ( 94.64)
Epoch: [33][100/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 9.4238e-01 (8.2208e-01)	Acc@1  69.53 ( 76.46)	Acc@5  94.53 ( 94.71)
Epoch: [33][110/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 8.3887e-01 (8.2517e-01)	Acc@1  74.22 ( 76.40)	Acc@5  94.53 ( 94.71)
Epoch: [33][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.2266e-01 (8.2587e-01)	Acc@1  77.34 ( 76.36)	Acc@5  95.31 ( 94.72)
Epoch: [33][130/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.8037e-01 (8.2480e-01)	Acc@1  71.88 ( 76.31)	Acc@5  94.53 ( 94.76)
Epoch: [33][140/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1475e-01 (8.2263e-01)	Acc@1  78.91 ( 76.38)	Acc@5  97.66 ( 94.76)
Epoch: [33][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9248e-01 (8.2512e-01)	Acc@1  79.69 ( 76.35)	Acc@5  95.31 ( 94.75)
Epoch: [33][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.7461e-01 (8.2571e-01)	Acc@1  75.00 ( 76.35)	Acc@5  91.41 ( 94.75)
Epoch: [33][170/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5391e-01 (8.2777e-01)	Acc@1  82.81 ( 76.36)	Acc@5  93.75 ( 94.73)
Epoch: [33][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0117e+00 (8.2929e-01)	Acc@1  67.19 ( 76.24)	Acc@5  96.09 ( 94.73)
Epoch: [33][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.5264e-01 (8.3125e-01)	Acc@1  70.31 ( 76.18)	Acc@5  94.53 ( 94.71)
Epoch: [33][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9727e-01 (8.2966e-01)	Acc@1  78.12 ( 76.15)	Acc@5  95.31 ( 94.74)
Epoch: [33][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1299e-01 (8.3218e-01)	Acc@1  68.75 ( 75.98)	Acc@5  93.75 ( 94.77)
Epoch: [33][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2236e-01 (8.3882e-01)	Acc@1  74.22 ( 75.82)	Acc@5  91.41 ( 94.68)
Epoch: [33][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.8467e-01 (8.4279e-01)	Acc@1  77.34 ( 75.70)	Acc@5  96.88 ( 94.64)
Epoch: [33][240/391]	Time  0.031 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.5713e-01 (8.4242e-01)	Acc@1  85.16 ( 75.68)	Acc@5  96.88 ( 94.65)
Epoch: [33][250/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6436e-01 (8.4212e-01)	Acc@1  70.31 ( 75.67)	Acc@5  90.62 ( 94.65)
Epoch: [33][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.8311e-01 (8.4185e-01)	Acc@1  78.12 ( 75.68)	Acc@5  96.88 ( 94.66)
Epoch: [33][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3730e-01 (8.4168e-01)	Acc@1  77.34 ( 75.66)	Acc@5  95.31 ( 94.65)
Epoch: [33][280/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.0879e+00 (8.4470e-01)	Acc@1  67.97 ( 75.59)	Acc@5  96.88 ( 94.64)
Epoch: [33][290/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 7.2607e-01 (8.4500e-01)	Acc@1  79.69 ( 75.57)	Acc@5  93.75 ( 94.65)
Epoch: [33][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.8320e-01 (8.4652e-01)	Acc@1  75.00 ( 75.49)	Acc@5  96.88 ( 94.67)
Epoch: [33][310/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.9062e-01 (8.4826e-01)	Acc@1  74.22 ( 75.42)	Acc@5  94.53 ( 94.65)
Epoch: [33][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.6631e-01 (8.4944e-01)	Acc@1  73.44 ( 75.39)	Acc@5  92.19 ( 94.63)
Epoch: [33][330/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 7.1484e-01 (8.4854e-01)	Acc@1  78.12 ( 75.38)	Acc@5  97.66 ( 94.64)
Epoch: [33][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0449e+00 (8.4979e-01)	Acc@1  68.75 ( 75.31)	Acc@5  94.53 ( 94.62)
Epoch: [33][350/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.002)	Loss 8.4961e-01 (8.5133e-01)	Acc@1  75.78 ( 75.27)	Acc@5  91.41 ( 94.59)
Epoch: [33][360/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.3750e-01 (8.5140e-01)	Acc@1  74.22 ( 75.26)	Acc@5  89.06 ( 94.60)
Epoch: [33][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3926e-01 (8.5235e-01)	Acc@1  78.91 ( 75.21)	Acc@5  95.31 ( 94.60)
Epoch: [33][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2754e+00 (8.5431e-01)	Acc@1  62.50 ( 75.15)	Acc@5  89.06 ( 94.59)
Epoch: [33][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.6289e-01 (8.5478e-01)	Acc@1  72.50 ( 75.16)	Acc@5  92.50 ( 94.58)
## e[33] optimizer.zero_grad (sum) time: 0.11499881744384766
## e[33]       loss.backward (sum) time: 2.2562904357910156
## e[33]      optimizer.step (sum) time: 0.9712767601013184
## epoch[33] training(only) time: 10.087557315826416
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.4023e+00 (1.4023e+00)	Acc@1  61.00 ( 61.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 1.5137e+00 (1.5724e+00)	Acc@1  66.00 ( 61.82)	Acc@5  87.00 ( 85.09)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.4385e+00 (1.5259e+00)	Acc@1  67.00 ( 62.71)	Acc@5  85.00 ( 86.10)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.4668e+00 (1.5307e+00)	Acc@1  61.00 ( 62.42)	Acc@5  89.00 ( 85.71)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.4580e+00 (1.5244e+00)	Acc@1  64.00 ( 62.56)	Acc@5  90.00 ( 86.07)
Test: [ 50/100]	Time  0.018 ( 0.016)	Loss 1.3223e+00 (1.5548e+00)	Acc@1  68.00 ( 61.82)	Acc@5  87.00 ( 85.57)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6582e+00 (1.5455e+00)	Acc@1  61.00 ( 61.70)	Acc@5  86.00 ( 85.72)
Test: [ 70/100]	Time  0.018 ( 0.016)	Loss 1.6768e+00 (1.5430e+00)	Acc@1  59.00 ( 61.58)	Acc@5  88.00 ( 85.85)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.7217e+00 (1.5520e+00)	Acc@1  64.00 ( 61.47)	Acc@5  78.00 ( 85.64)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.8135e+00 (1.5436e+00)	Acc@1  57.00 ( 61.59)	Acc@5  84.00 ( 85.77)
 * Acc@1 61.640 Acc@5 85.730
### epoch[33] execution time: 11.680915594100952
EPOCH 34
i:   0, name:       features.module.0.weight  changing lr from: 0.019597602646433312   to: 0.017019931844240544
i:   1, name:         features.module.0.bias  changing lr from: 0.021332455004357948   to: 0.018710244062438604
i:   2, name:       features.module.1.weight  changing lr from: 0.023077356221663927   to: 0.020420266467118565
i:   3, name:         features.module.1.bias  changing lr from: 0.024826497355511901   to: 0.022143380409700476
i:   4, name:       features.module.4.weight  changing lr from: 0.026574675660524349   to: 0.023873618072766806
i:   5, name:         features.module.4.bias  changing lr from: 0.028317249860730934   to: 0.025605617791715392
i:   6, name:       features.module.5.weight  changing lr from: 0.030050097231223444   to: 0.027334580665526104
i:   7, name:         features.module.5.bias  changing lr from: 0.031769572672088187   to: 0.029056228748315784
i:   8, name:       features.module.8.weight  changing lr from: 0.033472469891683641   to: 0.030766765031115056
i:   9, name:         features.module.8.bias  changing lr from: 0.035155984763255245   to: 0.032462835355614242
i:  10, name:       features.module.9.weight  changing lr from: 0.036817680876322742   to: 0.034141492346380468
i:  11, name:         features.module.9.bias  changing lr from: 0.038455457270578162   to: 0.035800161403380382
i:  12, name:      features.module.11.weight  changing lr from: 0.040067518313761680   to: 0.037436608760925846
i:  13, name:        features.module.11.bias  changing lr from: 0.041652345664901703   to: 0.039048911590983851
i:  14, name:      features.module.12.weight  changing lr from: 0.043208672249352827   to: 0.040635430106927099
i:  15, name:        features.module.12.bias  changing lr from: 0.044735458161337276   to: 0.042194781607199583
i:  16, name:      features.module.15.weight  changing lr from: 0.046231868402421952   to: 0.043725816386127284
i:  17, name:        features.module.15.bias  changing lr from: 0.047697252359891931   to: 0.045227595430445286
i:  18, name:      features.module.16.weight  changing lr from: 0.049131124926763588   to: 0.046699369814386313
i:  19, name:        features.module.16.bias  changing lr from: 0.050533149164753301   to: 0.048140561702821959
i:  20, name:      features.module.18.weight  changing lr from: 0.051903120412499483   to: 0.049550746870505515
i:  21, name:        features.module.18.bias  changing lr from: 0.053240951743398823   to: 0.050929638645531489
i:  22, name:      features.module.19.weight  changing lr from: 0.054546660680303560   to: 0.052277073186382174
i:  23, name:        features.module.19.bias  changing lr from: 0.055820357077812993   to: 0.053592996004096220
i:  24, name:      features.module.22.weight  changing lr from: 0.057062232086799461   to: 0.054877449643942589
i:  25, name:        features.module.22.bias  changing lr from: 0.058272548119996541   to: 0.056130562444332989
i:  26, name:      features.module.23.weight  changing lr from: 0.059451629741820625   to: 0.057352538294397015
i:  27, name:        features.module.23.bias  changing lr from: 0.060599855410011054   to: 0.058543647315560846
i:  28, name:      features.module.25.weight  changing lr from: 0.061717650001078032   to: 0.059704217396506311
i:  29, name:        features.module.25.bias  changing lr from: 0.062805478055888792   to: 0.060834626514966739
i:  30, name:      features.module.26.weight  changing lr from: 0.063863837685954980   to: 0.061935295783874113
i:  31, name:        features.module.26.bias  changing lr from: 0.064893255085076471   to: 0.063006683163361804
i:  32, name:            classifier.0.weight  changing lr from: 0.065894279594923597   to: 0.064049277784009803
i:  33, name:              classifier.0.bias  changing lr from: 0.066867479276888298   to: 0.065063594830468086
i:  34, name:            classifier.3.weight  changing lr from: 0.067813436946090752   to: 0.066050170938189642
i:  35, name:              classifier.3.bias  changing lr from: 0.068732746626789326   to: 0.067009560059431997
i:  36, name:            classifier.6.weight  changing lr from: 0.069626010391606416   to: 0.067942329757939443
i:  37, name:              classifier.6.bias  changing lr from: 0.070493835549950146   to: 0.068849057894791385



# Switched to train mode...
Epoch: [34][  0/391]	Time  0.171 ( 0.171)	Data  0.142 ( 0.142)	Loss 9.0918e-01 (9.0918e-01)	Acc@1  71.88 ( 71.88)	Acc@5  93.75 ( 93.75)
Epoch: [34][ 10/391]	Time  0.021 ( 0.037)	Data  0.001 ( 0.015)	Loss 9.5166e-01 (7.9519e-01)	Acc@1  73.44 ( 76.63)	Acc@5  93.75 ( 95.60)
Epoch: [34][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.0127e+00 (7.9753e-01)	Acc@1  74.22 ( 76.34)	Acc@5  94.53 ( 95.65)
Epoch: [34][ 30/391]	Time  0.042 ( 0.029)	Data  0.005 ( 0.007)	Loss 6.3623e-01 (7.8720e-01)	Acc@1  82.03 ( 76.99)	Acc@5  96.88 ( 95.74)
Epoch: [34][ 40/391]	Time  0.020 ( 0.028)	Data  0.002 ( 0.005)	Loss 6.9629e-01 (7.7796e-01)	Acc@1  80.47 ( 77.31)	Acc@5  96.88 ( 95.69)
Epoch: [34][ 50/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.1572e-01 (7.7214e-01)	Acc@1  81.25 ( 77.53)	Acc@5  97.66 ( 95.74)
Epoch: [34][ 60/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.004)	Loss 8.8232e-01 (7.7595e-01)	Acc@1  75.78 ( 77.38)	Acc@5  95.31 ( 95.85)
Epoch: [34][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.2119e-01 (7.8867e-01)	Acc@1  77.34 ( 76.83)	Acc@5  95.31 ( 95.72)
Epoch: [34][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.6221e-01 (7.9334e-01)	Acc@1  78.91 ( 76.77)	Acc@5  95.31 ( 95.67)
Epoch: [34][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.7295e-01 (7.9683e-01)	Acc@1  78.12 ( 76.82)	Acc@5  97.66 ( 95.60)
Epoch: [34][100/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 7.2070e-01 (7.9712e-01)	Acc@1  79.69 ( 77.00)	Acc@5  96.09 ( 95.54)
Epoch: [34][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2041e-01 (7.9651e-01)	Acc@1  71.09 ( 76.92)	Acc@5  94.53 ( 95.52)
Epoch: [34][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3486e-01 (7.9718e-01)	Acc@1  79.69 ( 76.87)	Acc@5  93.75 ( 95.51)
Epoch: [34][130/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7441e-01 (7.9815e-01)	Acc@1  78.12 ( 76.89)	Acc@5  95.31 ( 95.50)
Epoch: [34][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.6523e-01 (8.0116e-01)	Acc@1  76.56 ( 76.75)	Acc@5  94.53 ( 95.51)
Epoch: [34][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.6963e-01 (8.0024e-01)	Acc@1  80.47 ( 76.76)	Acc@5  92.19 ( 95.49)
Epoch: [34][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5283e-01 (7.9863e-01)	Acc@1  78.91 ( 76.72)	Acc@5  96.09 ( 95.55)
Epoch: [34][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4512e-01 (7.9965e-01)	Acc@1  75.00 ( 76.71)	Acc@5  99.22 ( 95.50)
Epoch: [34][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6436e-01 (8.0483e-01)	Acc@1  67.97 ( 76.56)	Acc@5  93.75 ( 95.45)
Epoch: [34][190/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1445e-01 (8.0395e-01)	Acc@1  76.56 ( 76.60)	Acc@5  97.66 ( 95.46)
Epoch: [34][200/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6289e-01 (8.0269e-01)	Acc@1  70.31 ( 76.61)	Acc@5  94.53 ( 95.49)
Epoch: [34][210/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8740e-01 (7.9895e-01)	Acc@1  78.12 ( 76.74)	Acc@5  96.88 ( 95.50)
Epoch: [34][220/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2129e-01 (7.9983e-01)	Acc@1  77.34 ( 76.68)	Acc@5  93.75 ( 95.48)
Epoch: [34][230/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0420e+00 (8.0423e-01)	Acc@1  73.44 ( 76.59)	Acc@5  88.28 ( 95.40)
Epoch: [34][240/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8311e-01 (8.0370e-01)	Acc@1  82.03 ( 76.61)	Acc@5  94.53 ( 95.38)
Epoch: [34][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.2822e-01 (8.0485e-01)	Acc@1  72.66 ( 76.57)	Acc@5  95.31 ( 95.39)
Epoch: [34][260/391]	Time  0.021 ( 0.025)	Data  0.003 ( 0.002)	Loss 8.1543e-01 (8.0641e-01)	Acc@1  78.12 ( 76.54)	Acc@5  92.97 ( 95.35)
Epoch: [34][270/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.8135e-01 (8.0865e-01)	Acc@1  74.22 ( 76.46)	Acc@5  93.75 ( 95.32)
Epoch: [34][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.2617e-01 (8.0864e-01)	Acc@1  72.66 ( 76.44)	Acc@5  95.31 ( 95.30)
Epoch: [34][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.1396e-01 (8.0980e-01)	Acc@1  74.22 ( 76.40)	Acc@5  95.31 ( 95.29)
Epoch: [34][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3730e-01 (8.0916e-01)	Acc@1  78.91 ( 76.41)	Acc@5  96.88 ( 95.30)
Epoch: [34][310/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.0049e+00 (8.1193e-01)	Acc@1  74.22 ( 76.32)	Acc@5  92.19 ( 95.27)
Epoch: [34][320/391]	Time  0.024 ( 0.025)	Data  0.003 ( 0.002)	Loss 9.2725e-01 (8.1323e-01)	Acc@1  75.00 ( 76.26)	Acc@5  92.19 ( 95.24)
Epoch: [34][330/391]	Time  0.022 ( 0.025)	Data  0.002 ( 0.002)	Loss 8.2812e-01 (8.1371e-01)	Acc@1  75.00 ( 76.24)	Acc@5  96.09 ( 95.25)
Epoch: [34][340/391]	Time  0.043 ( 0.025)	Data  0.011 ( 0.002)	Loss 7.1338e-01 (8.1432e-01)	Acc@1  78.91 ( 76.21)	Acc@5  93.75 ( 95.25)
Epoch: [34][350/391]	Time  0.035 ( 0.025)	Data  0.002 ( 0.002)	Loss 9.8291e-01 (8.1554e-01)	Acc@1  67.97 ( 76.18)	Acc@5  92.97 ( 95.21)
Epoch: [34][360/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.3389e-01 (8.1591e-01)	Acc@1  80.47 ( 76.19)	Acc@5  96.88 ( 95.21)
Epoch: [34][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.8047e-01 (8.1857e-01)	Acc@1  73.44 ( 76.12)	Acc@5  91.41 ( 95.18)
Epoch: [34][380/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.9189e-01 (8.1718e-01)	Acc@1  76.56 ( 76.15)	Acc@5  97.66 ( 95.19)
Epoch: [34][390/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.9688e-01 (8.1810e-01)	Acc@1  78.75 ( 76.12)	Acc@5  91.25 ( 95.15)
## e[34] optimizer.zero_grad (sum) time: 0.11432719230651855
## e[34]       loss.backward (sum) time: 2.2957353591918945
## e[34]      optimizer.step (sum) time: 0.9730343818664551
## epoch[34] training(only) time: 10.0190110206604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.5977e+00 (1.5977e+00)	Acc@1  63.00 ( 63.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 1.4355e+00 (1.5337e+00)	Acc@1  61.00 ( 61.36)	Acc@5  88.00 ( 86.09)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.5049e+00 (1.4990e+00)	Acc@1  66.00 ( 62.38)	Acc@5  84.00 ( 86.43)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 1.6377e+00 (1.4997e+00)	Acc@1  56.00 ( 62.06)	Acc@5  85.00 ( 86.58)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.5469e+00 (1.4970e+00)	Acc@1  62.00 ( 62.05)	Acc@5  89.00 ( 86.80)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.2461e+00 (1.5027e+00)	Acc@1  68.00 ( 61.92)	Acc@5  87.00 ( 86.63)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.6875e+00 (1.4955e+00)	Acc@1  61.00 ( 62.10)	Acc@5  84.00 ( 86.87)
Test: [ 70/100]	Time  0.024 ( 0.015)	Loss 1.6133e+00 (1.5020e+00)	Acc@1  60.00 ( 61.89)	Acc@5  85.00 ( 86.83)
Test: [ 80/100]	Time  0.020 ( 0.015)	Loss 1.6260e+00 (1.5028e+00)	Acc@1  63.00 ( 61.91)	Acc@5  80.00 ( 86.74)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.8545e+00 (1.4997e+00)	Acc@1  53.00 ( 61.92)	Acc@5  82.00 ( 86.80)
 * Acc@1 61.970 Acc@5 86.840
### epoch[34] execution time: 11.59191346168518
EPOCH 35
i:   0, name:       features.module.0.weight  changing lr from: 0.017019931844240544   to: 0.014600142389248290
i:   1, name:         features.module.0.bias  changing lr from: 0.018710244062438604   to: 0.016232274801050932
i:   2, name:       features.module.1.weight  changing lr from: 0.020420266467118565   to: 0.017894544887998982
i:   3, name:         features.module.1.bias  changing lr from: 0.022143380409700476   to: 0.019579508960414650
i:   4, name:       features.module.4.weight  changing lr from: 0.023873618072766806   to: 0.021280414586718557
i:   5, name:         features.module.4.bias  changing lr from: 0.025605617791715392   to: 0.022991156915968487
i:   6, name:       features.module.5.weight  changing lr from: 0.027334580665526104   to: 0.024706235627995749
i:   7, name:         features.module.5.bias  changing lr from: 0.029056228748315784   to: 0.026420712928870427
i:   8, name:       features.module.8.weight  changing lr from: 0.030766765031115056   to: 0.028130172908874648
i:   9, name:         features.module.8.bias  changing lr from: 0.032462835355614242   to: 0.029830682496350926
i:  10, name:       features.module.9.weight  changing lr from: 0.034141492346380468   to: 0.031518754171419146
i:  11, name:         features.module.9.bias  changing lr from: 0.035800161403380382   to: 0.033191310546570783
i:  12, name:      features.module.11.weight  changing lr from: 0.037436608760925846   to: 0.034845650874739835
i:  13, name:        features.module.11.bias  changing lr from: 0.039048911590983851   to: 0.036479419508033760
i:  14, name:      features.module.12.weight  changing lr from: 0.040635430106927099   to: 0.038090576300491535
i:  15, name:        features.module.12.bias  changing lr from: 0.042194781607199583   to: 0.039677368924818213
i:  16, name:      features.module.15.weight  changing lr from: 0.043725816386127284   to: 0.041238307054985740
i:  17, name:        features.module.15.bias  changing lr from: 0.045227595430445286   to: 0.042772138352987099
i:  18, name:      features.module.16.weight  changing lr from: 0.046699369814386313   to: 0.044277826188123065
i:  19, name:        features.module.16.bias  changing lr from: 0.048140561702821959   to: 0.045754529010324202
i:  20, name:      features.module.18.weight  changing lr from: 0.049550746870505515   to: 0.047201581294617295
i:  21, name:        features.module.18.bias  changing lr from: 0.050929638645531489   to: 0.048618475971450824
i:  22, name:      features.module.19.weight  changing lr from: 0.052277073186382174   to: 0.050004848256811521
i:  23, name:        features.module.19.bias  changing lr from: 0.053592996004096220   to: 0.051360460796551155
i:  24, name:      features.module.22.weight  changing lr from: 0.054877449643942589   to: 0.052685190040821867
i:  25, name:        features.module.22.bias  changing lr from: 0.056130562444332989   to: 0.053979013766759787
i:  26, name:      features.module.23.weight  changing lr from: 0.057352538294397015   to: 0.055241999670361697
i:  27, name:        features.module.23.bias  changing lr from: 0.058543647315560846   to: 0.056474294951714910
i:  28, name:      features.module.25.weight  changing lr from: 0.059704217396506311   to: 0.057676116821234968
i:  29, name:        features.module.25.bias  changing lr from: 0.060834626514966739   to: 0.058847743858230640
i:  30, name:      features.module.26.weight  changing lr from: 0.061935295783874113   to: 0.059989508156872554
i:  31, name:        features.module.26.bias  changing lr from: 0.063006683163361804   to: 0.061101788198416690
i:  32, name:            classifier.0.weight  changing lr from: 0.064049277784009803   to: 0.062185002392277679
i:  33, name:              classifier.0.bias  changing lr from: 0.065063594830468086   to: 0.063239603232217709
i:  34, name:            classifier.3.weight  changing lr from: 0.066050170938189642   to: 0.064266072017481010
i:  35, name:              classifier.3.bias  changing lr from: 0.067009560059431997   to: 0.065264914092141954
i:  36, name:            classifier.6.weight  changing lr from: 0.067942329757939443   to: 0.066236654559228433
i:  37, name:              classifier.6.bias  changing lr from: 0.068849057894791385   to: 0.067181834429318105



# Switched to train mode...
Epoch: [35][  0/391]	Time  0.180 ( 0.180)	Data  0.147 ( 0.147)	Loss 8.3643e-01 (8.3643e-01)	Acc@1  78.12 ( 78.12)	Acc@5  92.97 ( 92.97)
Epoch: [35][ 10/391]	Time  0.022 ( 0.040)	Data  0.001 ( 0.015)	Loss 6.4404e-01 (7.4259e-01)	Acc@1  77.34 ( 78.41)	Acc@5  96.88 ( 95.45)
Epoch: [35][ 20/391]	Time  0.035 ( 0.033)	Data  0.002 ( 0.009)	Loss 1.0234e+00 (7.2986e-01)	Acc@1  72.66 ( 78.94)	Acc@5  92.19 ( 95.87)
Epoch: [35][ 30/391]	Time  0.020 ( 0.030)	Data  0.002 ( 0.007)	Loss 6.8262e-01 (7.4091e-01)	Acc@1  81.25 ( 78.63)	Acc@5  96.88 ( 95.69)
Epoch: [35][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.005)	Loss 6.0547e-01 (7.1565e-01)	Acc@1  78.12 ( 79.04)	Acc@5  98.44 ( 96.13)
Epoch: [35][ 50/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.0586e-01 (7.2277e-01)	Acc@1  81.25 ( 78.63)	Acc@5  99.22 ( 96.05)
Epoch: [35][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.2363e-01 (7.3347e-01)	Acc@1  79.69 ( 78.25)	Acc@5  94.53 ( 95.85)
Epoch: [35][ 70/391]	Time  0.023 ( 0.027)	Data  0.005 ( 0.004)	Loss 7.6025e-01 (7.4086e-01)	Acc@1  77.34 ( 78.07)	Acc@5  96.09 ( 95.94)
Epoch: [35][ 80/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.4609e-01 (7.4258e-01)	Acc@1  75.00 ( 77.98)	Acc@5  97.66 ( 95.94)
Epoch: [35][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.6992e-01 (7.3657e-01)	Acc@1  78.12 ( 78.25)	Acc@5  94.53 ( 95.94)
Epoch: [35][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.5977e-01 (7.3874e-01)	Acc@1  76.56 ( 78.20)	Acc@5  96.09 ( 96.07)
Epoch: [35][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.9863e-01 (7.3977e-01)	Acc@1  79.69 ( 78.00)	Acc@5  97.66 ( 96.07)
Epoch: [35][120/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.7100e-01 (7.4114e-01)	Acc@1  74.22 ( 77.94)	Acc@5  95.31 ( 96.00)
Epoch: [35][130/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.7861e-01 (7.3903e-01)	Acc@1  82.81 ( 78.02)	Acc@5  99.22 ( 96.03)
Epoch: [35][140/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.4600e-01 (7.3989e-01)	Acc@1  79.69 ( 78.05)	Acc@5  97.66 ( 95.97)
Epoch: [35][150/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1006e-01 (7.4414e-01)	Acc@1  76.56 ( 77.95)	Acc@5  95.31 ( 95.91)
Epoch: [35][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1045e-01 (7.4536e-01)	Acc@1  78.91 ( 77.95)	Acc@5  95.31 ( 95.90)
Epoch: [35][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9590e-01 (7.4773e-01)	Acc@1  77.34 ( 77.91)	Acc@5  96.09 ( 95.89)
Epoch: [35][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5234e-01 (7.4925e-01)	Acc@1  81.25 ( 77.95)	Acc@5  98.44 ( 95.87)
Epoch: [35][190/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.1445e-01 (7.5218e-01)	Acc@1  78.12 ( 77.89)	Acc@5  94.53 ( 95.86)
Epoch: [35][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.7539e-01 (7.5585e-01)	Acc@1  76.56 ( 77.77)	Acc@5  96.09 ( 95.81)
Epoch: [35][210/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4463e-01 (7.5571e-01)	Acc@1  75.78 ( 77.74)	Acc@5  96.88 ( 95.83)
Epoch: [35][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8066e-01 (7.5550e-01)	Acc@1  82.03 ( 77.79)	Acc@5  95.31 ( 95.83)
Epoch: [35][230/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8496e-01 (7.5361e-01)	Acc@1  81.25 ( 77.84)	Acc@5  96.88 ( 95.82)
Epoch: [35][240/391]	Time  0.028 ( 0.026)	Data  0.004 ( 0.003)	Loss 8.8428e-01 (7.5678e-01)	Acc@1  75.00 ( 77.83)	Acc@5  96.09 ( 95.79)
Epoch: [35][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0732e+00 (7.5776e-01)	Acc@1  74.22 ( 77.80)	Acc@5  91.41 ( 95.75)
Epoch: [35][260/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.2598e-01 (7.5749e-01)	Acc@1  82.03 ( 77.78)	Acc@5  97.66 ( 95.78)
Epoch: [35][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3975e-01 (7.5922e-01)	Acc@1  75.78 ( 77.72)	Acc@5  96.09 ( 95.76)
Epoch: [35][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.6807e-01 (7.6053e-01)	Acc@1  74.22 ( 77.66)	Acc@5  96.09 ( 95.75)
Epoch: [35][290/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 7.7734e-01 (7.6259e-01)	Acc@1  77.34 ( 77.57)	Acc@5  95.31 ( 95.74)
Epoch: [35][300/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.002)	Loss 9.2334e-01 (7.6454e-01)	Acc@1  71.88 ( 77.52)	Acc@5  95.31 ( 95.71)
Epoch: [35][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.0859e-01 (7.6643e-01)	Acc@1  78.91 ( 77.48)	Acc@5  96.88 ( 95.70)
Epoch: [35][320/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 7.4170e-01 (7.6605e-01)	Acc@1  78.12 ( 77.46)	Acc@5  94.53 ( 95.68)
Epoch: [35][330/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 5.5811e-01 (7.6448e-01)	Acc@1  80.47 ( 77.48)	Acc@5  99.22 ( 95.70)
Epoch: [35][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.9248e-01 (7.6765e-01)	Acc@1  76.56 ( 77.42)	Acc@5  97.66 ( 95.67)
Epoch: [35][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.0361e-01 (7.6879e-01)	Acc@1  78.91 ( 77.38)	Acc@5  94.53 ( 95.64)
Epoch: [35][360/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0430e+00 (7.7102e-01)	Acc@1  69.53 ( 77.32)	Acc@5  93.75 ( 95.60)
Epoch: [35][370/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.002)	Loss 9.8584e-01 (7.7044e-01)	Acc@1  75.00 ( 77.35)	Acc@5  89.84 ( 95.59)
Epoch: [35][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.4824e-01 (7.6939e-01)	Acc@1  87.50 ( 77.43)	Acc@5  96.88 ( 95.58)
Epoch: [35][390/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0322e+00 (7.6871e-01)	Acc@1  67.50 ( 77.42)	Acc@5  93.75 ( 95.59)
## e[35] optimizer.zero_grad (sum) time: 0.11458611488342285
## e[35]       loss.backward (sum) time: 2.281308650970459
## e[35]      optimizer.step (sum) time: 0.9715044498443604
## epoch[35] training(only) time: 10.099302530288696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.131 ( 0.131)	Loss 1.5645e+00 (1.5645e+00)	Acc@1  61.00 ( 61.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.5859e+00 (1.5676e+00)	Acc@1  63.00 ( 62.91)	Acc@5  89.00 ( 87.09)
Test: [ 20/100]	Time  0.024 ( 0.020)	Loss 1.5156e+00 (1.5188e+00)	Acc@1  64.00 ( 63.24)	Acc@5  87.00 ( 87.81)
Test: [ 30/100]	Time  0.009 ( 0.018)	Loss 1.7412e+00 (1.4948e+00)	Acc@1  57.00 ( 63.13)	Acc@5  86.00 ( 87.97)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.1699e+00 (1.4686e+00)	Acc@1  66.00 ( 63.37)	Acc@5  93.00 ( 87.98)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.4502e+00 (1.4940e+00)	Acc@1  66.00 ( 63.12)	Acc@5  87.00 ( 87.41)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 1.5850e+00 (1.4803e+00)	Acc@1  64.00 ( 63.38)	Acc@5  87.00 ( 87.54)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.7275e+00 (1.4844e+00)	Acc@1  62.00 ( 63.41)	Acc@5  84.00 ( 87.46)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.7510e+00 (1.4994e+00)	Acc@1  62.00 ( 63.36)	Acc@5  84.00 ( 87.17)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 1.6855e+00 (1.4852e+00)	Acc@1  57.00 ( 63.45)	Acc@5  85.00 ( 87.37)
 * Acc@1 63.410 Acc@5 87.290
### epoch[35] execution time: 11.67147159576416
EPOCH 36
i:   0, name:       features.module.0.weight  changing lr from: 0.014600142389248290   to: 0.012349477331863180
i:   1, name:         features.module.0.bias  changing lr from: 0.016232274801050932   to: 0.013909616565260862
i:   2, name:       features.module.1.weight  changing lr from: 0.017894544887998982   to: 0.015511041789299674
i:   3, name:         features.module.1.bias  changing lr from: 0.019579508960414650   to: 0.017145477808027377
i:   4, name:       features.module.4.weight  changing lr from: 0.021280414586718557   to: 0.018805375741886361
i:   5, name:         features.module.4.bias  changing lr from: 0.022991156915968487   to: 0.020483871425794099
i:   6, name:       features.module.5.weight  changing lr from: 0.024706235627995749   to: 0.022174743622696738
i:   7, name:         features.module.5.bias  changing lr from: 0.026420712928870427   to: 0.023872372613421830
i:   8, name:       features.module.8.weight  changing lr from: 0.028130172908874648   to: 0.025571699603412181
i:   9, name:         features.module.8.bias  changing lr from: 0.029830682496350926   to: 0.027268187285601032
i:  10, name:       features.module.9.weight  changing lr from: 0.031518754171419146   to: 0.028957781813836399
i:  11, name:         features.module.9.bias  changing lr from: 0.033191310546570783   to: 0.030636876370662543
i:  12, name:      features.module.11.weight  changing lr from: 0.034845650874739835   to: 0.032302276454946634
i:  13, name:        features.module.11.bias  changing lr from: 0.036479419508033760   to: 0.033951166967043159
i:  14, name:      features.module.12.weight  changing lr from: 0.038090576300491535   to: 0.035581081130371971
i:  15, name:        features.module.12.bias  changing lr from: 0.039677368924818213   to: 0.037189871257093644
i:  16, name:      features.module.15.weight  changing lr from: 0.041238307054985740   to: 0.038775681340817436
i:  17, name:        features.module.15.bias  changing lr from: 0.042772138352987099   to: 0.040336921439945883
i:  18, name:      features.module.16.weight  changing lr from: 0.044277826188123065   to: 0.041872243800462838
i:  19, name:        features.module.16.bias  changing lr from: 0.045754529010324202   to: 0.043380520655942667
i:  20, name:      features.module.18.weight  changing lr from: 0.047201581294617295   to: 0.044860823634650923
i:  21, name:        features.module.18.bias  changing lr from: 0.048618475971450824   to: 0.046312404698255205
i:  22, name:      features.module.19.weight  changing lr from: 0.050004848256811521   to: 0.047734678533404386
i:  23, name:        features.module.19.bias  changing lr from: 0.051360460796551155   to: 0.049127206315857083
i:  24, name:      features.module.22.weight  changing lr from: 0.052685190040821867   to: 0.050489680766611270
i:  25, name:        features.module.22.bias  changing lr from: 0.053979013766759787   to: 0.051821912420322414
i:  26, name:      features.module.23.weight  changing lr from: 0.055241999670361697   to: 0.053123817027957126
i:  27, name:        features.module.23.bias  changing lr from: 0.056474294951714910   to: 0.054395404017919036
i:  28, name:      features.module.25.weight  changing lr from: 0.057676116821234968   to: 0.055636765942638160
i:  29, name:        features.module.25.bias  changing lr from: 0.058847743858230640   to: 0.056848068840698811
i:  30, name:      features.module.26.weight  changing lr from: 0.059989508156872554   to: 0.058029543447887133
i:  31, name:        features.module.26.bias  changing lr from: 0.061101788198416690   to: 0.059181477193975332
i:  32, name:            classifier.0.weight  changing lr from: 0.062185002392277679   to: 0.060304206925555250
i:  33, name:              classifier.0.bias  changing lr from: 0.063239603232217709   to: 0.061398112298733536
i:  34, name:            classifier.3.weight  changing lr from: 0.064266072017481010   to: 0.062463609788955182
i:  35, name:              classifier.3.bias  changing lr from: 0.065264914092141954   to: 0.063501147268602745
i:  36, name:            classifier.6.weight  changing lr from: 0.066236654559228433   to: 0.064511199106293562
i:  37, name:              classifier.6.bias  changing lr from: 0.067181834429318105   to: 0.065494261744951432



# Switched to train mode...
Epoch: [36][  0/391]	Time  0.195 ( 0.195)	Data  0.160 ( 0.160)	Loss 5.9766e-01 (5.9766e-01)	Acc@1  79.69 ( 79.69)	Acc@5  96.88 ( 96.88)
Epoch: [36][ 10/391]	Time  0.033 ( 0.041)	Data  0.001 ( 0.016)	Loss 7.3389e-01 (6.7765e-01)	Acc@1  79.69 ( 79.69)	Acc@5  97.66 ( 96.38)
Epoch: [36][ 20/391]	Time  0.020 ( 0.033)	Data  0.001 ( 0.009)	Loss 9.0283e-01 (6.8362e-01)	Acc@1  75.00 ( 79.76)	Acc@5  94.53 ( 96.09)
Epoch: [36][ 30/391]	Time  0.030 ( 0.031)	Data  0.002 ( 0.007)	Loss 7.1436e-01 (6.8659e-01)	Acc@1  79.69 ( 79.39)	Acc@5  95.31 ( 96.24)
Epoch: [36][ 40/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 7.5244e-01 (6.9428e-01)	Acc@1  76.56 ( 78.93)	Acc@5  96.09 ( 96.21)
Epoch: [36][ 50/391]	Time  0.045 ( 0.028)	Data  0.005 ( 0.005)	Loss 5.2148e-01 (6.9339e-01)	Acc@1  80.47 ( 79.07)	Acc@5  99.22 ( 96.25)
Epoch: [36][ 60/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.0894e-01 (6.8698e-01)	Acc@1  85.94 ( 79.24)	Acc@5  99.22 ( 96.26)
Epoch: [36][ 70/391]	Time  0.028 ( 0.027)	Data  0.000 ( 0.004)	Loss 7.6709e-01 (6.9914e-01)	Acc@1  77.34 ( 78.99)	Acc@5  95.31 ( 96.18)
Epoch: [36][ 80/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.3701e-01 (6.9345e-01)	Acc@1  74.22 ( 79.22)	Acc@5  92.97 ( 96.27)
Epoch: [36][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.2646e-01 (6.9885e-01)	Acc@1  82.81 ( 79.13)	Acc@5  95.31 ( 96.27)
Epoch: [36][100/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.1426e-01 (7.0364e-01)	Acc@1  81.25 ( 79.05)	Acc@5  97.66 ( 96.19)
Epoch: [36][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2158e-01 (7.0048e-01)	Acc@1  82.81 ( 79.19)	Acc@5  95.31 ( 96.23)
Epoch: [36][120/391]	Time  0.041 ( 0.026)	Data  0.005 ( 0.003)	Loss 9.0088e-01 (7.0142e-01)	Acc@1  77.34 ( 79.17)	Acc@5  93.75 ( 96.20)
Epoch: [36][130/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.3579e-01 (6.9865e-01)	Acc@1  86.72 ( 79.34)	Acc@5  97.66 ( 96.20)
Epoch: [36][140/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.2520e-01 (6.9786e-01)	Acc@1  77.34 ( 79.43)	Acc@5  96.09 ( 96.23)
Epoch: [36][150/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.2021e-01 (7.0018e-01)	Acc@1  81.25 ( 79.45)	Acc@5  96.09 ( 96.21)
Epoch: [36][160/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 7.9688e-01 (7.0216e-01)	Acc@1  75.78 ( 79.41)	Acc@5  96.09 ( 96.19)
Epoch: [36][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.6035e-01 (7.0711e-01)	Acc@1  71.09 ( 79.30)	Acc@5  95.31 ( 96.12)
Epoch: [36][180/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5918e-01 (7.0834e-01)	Acc@1  82.03 ( 79.20)	Acc@5  95.31 ( 96.15)
Epoch: [36][190/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 8.0859e-01 (7.0763e-01)	Acc@1  70.31 ( 79.16)	Acc@5  96.88 ( 96.16)
Epoch: [36][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1533e-01 (7.1012e-01)	Acc@1  77.34 ( 79.13)	Acc@5  95.31 ( 96.14)
Epoch: [36][210/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1426e-01 (7.0976e-01)	Acc@1  77.34 ( 79.11)	Acc@5  96.88 ( 96.12)
Epoch: [36][220/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.4609e-01 (7.1223e-01)	Acc@1  80.47 ( 79.06)	Acc@5  96.88 ( 96.08)
Epoch: [36][230/391]	Time  0.024 ( 0.026)	Data  0.006 ( 0.003)	Loss 7.3242e-01 (7.1368e-01)	Acc@1  76.56 ( 79.00)	Acc@5  96.88 ( 96.06)
Epoch: [36][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8115e-01 (7.1402e-01)	Acc@1  80.47 ( 78.96)	Acc@5  95.31 ( 96.04)
Epoch: [36][250/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.7148e-01 (7.1594e-01)	Acc@1  77.34 ( 78.95)	Acc@5  95.31 ( 96.02)
Epoch: [36][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.3057e-01 (7.1779e-01)	Acc@1  77.34 ( 78.93)	Acc@5  96.09 ( 96.01)
Epoch: [36][270/391]	Time  0.034 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.0205e-01 (7.1646e-01)	Acc@1  85.94 ( 78.96)	Acc@5  95.31 ( 95.98)
Epoch: [36][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.5986e-01 (7.1846e-01)	Acc@1  79.69 ( 78.91)	Acc@5  92.97 ( 95.98)
Epoch: [36][290/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.4209e-01 (7.1894e-01)	Acc@1  78.91 ( 78.87)	Acc@5  97.66 ( 96.00)
Epoch: [36][300/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.6865e-01 (7.2251e-01)	Acc@1  78.12 ( 78.80)	Acc@5  92.19 ( 95.96)
Epoch: [36][310/391]	Time  0.039 ( 0.026)	Data  0.004 ( 0.002)	Loss 8.7402e-01 (7.2370e-01)	Acc@1  73.44 ( 78.78)	Acc@5  94.53 ( 95.93)
Epoch: [36][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.4170e-01 (7.2336e-01)	Acc@1  82.03 ( 78.81)	Acc@5  92.97 ( 95.91)
Epoch: [36][330/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.3867e-01 (7.2537e-01)	Acc@1  81.25 ( 78.78)	Acc@5  94.53 ( 95.87)
Epoch: [36][340/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 8.4229e-01 (7.2657e-01)	Acc@1  80.47 ( 78.75)	Acc@5  95.31 ( 95.86)
Epoch: [36][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.9492e-01 (7.2727e-01)	Acc@1  76.56 ( 78.71)	Acc@5  96.88 ( 95.86)
Epoch: [36][360/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 6.8945e-01 (7.2749e-01)	Acc@1  78.12 ( 78.71)	Acc@5  94.53 ( 95.87)
Epoch: [36][370/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.9727e-01 (7.2943e-01)	Acc@1  74.22 ( 78.64)	Acc@5  96.09 ( 95.85)
Epoch: [36][380/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.002)	Loss 5.0342e-01 (7.2856e-01)	Acc@1  88.28 ( 78.67)	Acc@5  98.44 ( 95.85)
Epoch: [36][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.5479e-01 (7.3024e-01)	Acc@1  81.25 ( 78.63)	Acc@5  97.50 ( 95.82)
## e[36] optimizer.zero_grad (sum) time: 0.11508750915527344
## e[36]       loss.backward (sum) time: 2.269874095916748
## e[36]      optimizer.step (sum) time: 0.9671781063079834
## epoch[36] training(only) time: 10.100612878799438
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.4453e+00 (1.4453e+00)	Acc@1  64.00 ( 64.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.020 ( 0.027)	Loss 1.3750e+00 (1.5539e+00)	Acc@1  65.00 ( 62.45)	Acc@5  90.00 ( 86.18)
Test: [ 20/100]	Time  0.014 ( 0.021)	Loss 1.5029e+00 (1.5571e+00)	Acc@1  65.00 ( 62.33)	Acc@5  83.00 ( 86.43)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.7354e+00 (1.5260e+00)	Acc@1  59.00 ( 62.23)	Acc@5  88.00 ( 86.52)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.3086e+00 (1.5031e+00)	Acc@1  61.00 ( 62.44)	Acc@5  92.00 ( 86.88)
Test: [ 50/100]	Time  0.019 ( 0.017)	Loss 1.3574e+00 (1.5250e+00)	Acc@1  69.00 ( 61.98)	Acc@5  88.00 ( 86.71)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 1.6807e+00 (1.5110e+00)	Acc@1  57.00 ( 62.18)	Acc@5  88.00 ( 86.84)
Test: [ 70/100]	Time  0.019 ( 0.016)	Loss 1.6221e+00 (1.5151e+00)	Acc@1  61.00 ( 62.07)	Acc@5  87.00 ( 86.89)
Test: [ 80/100]	Time  0.015 ( 0.016)	Loss 1.8125e+00 (1.5224e+00)	Acc@1  59.00 ( 61.79)	Acc@5  80.00 ( 86.84)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.7969e+00 (1.5179e+00)	Acc@1  58.00 ( 62.02)	Acc@5  85.00 ( 86.92)
 * Acc@1 62.230 Acc@5 86.980
### epoch[36] execution time: 11.692116498947144
EPOCH 37
i:   0, name:       features.module.0.weight  changing lr from: 0.012349477331863180   to: 0.010278393921015021
i:   1, name:         features.module.0.bias  changing lr from: 0.013909616565260862   to: 0.011752644909714265
i:   2, name:       features.module.1.weight  changing lr from: 0.015511041789299674   to: 0.013279996516398239
i:   3, name:         features.module.1.bias  changing lr from: 0.017145477808027377   to: 0.014851345208060102
i:   4, name:       features.module.4.weight  changing lr from: 0.018805375741886361   to: 0.016458342256874769
i:   5, name:         features.module.4.bias  changing lr from: 0.020483871425794099   to: 0.018093355408614665
i:   6, name:       features.module.5.weight  changing lr from: 0.022174743622696738   to: 0.019749429395729091
i:   7, name:         features.module.5.bias  changing lr from: 0.023872372613421830   to: 0.021420246015760043
i:   8, name:       features.module.8.weight  changing lr from: 0.025571699603412181   to: 0.023100084354713321
i:   9, name:         features.module.8.bias  changing lr from: 0.027268187285601032   to: 0.024783781614977365
i:  10, name:       features.module.9.weight  changing lr from: 0.028957781813836399   to: 0.026466694905833445
i:  11, name:         features.module.9.bias  changing lr from: 0.030636876370662543   to: 0.028144664269185438
i:  12, name:      features.module.11.weight  changing lr from: 0.032302276454946634   to: 0.029813977141741128
i:  13, name:        features.module.11.bias  changing lr from: 0.033951166967043159   to: 0.031471334395599335
i:  14, name:      features.module.12.weight  changing lr from: 0.035581081130371971   to: 0.033113818050343136
i:  15, name:        features.module.12.bias  changing lr from: 0.037189871257093644   to: 0.034738860709810707
i:  16, name:      features.module.15.weight  changing lr from: 0.038775681340817436   to: 0.036344216744396592
i:  17, name:        features.module.15.bias  changing lr from: 0.040336921439945883   to: 0.037927935213868745
i:  18, name:      features.module.16.weight  changing lr from: 0.041872243800462838   to: 0.039488334505270686
i:  19, name:        features.module.16.bias  changing lr from: 0.043380520655942667   to: 0.041023978644637492
i:  20, name:      features.module.18.weight  changing lr from: 0.044860823634650923   to: 0.042533655229243600
i:  21, name:        features.module.18.bias  changing lr from: 0.046312404698255205   to: 0.044016354918263952
i:  22, name:      features.module.19.weight  changing lr from: 0.047734678533404386   to: 0.045471252413523011
i:  23, name:        features.module.19.bias  changing lr from: 0.049127206315857083   to: 0.046897688857950259
i:  24, name:      features.module.22.weight  changing lr from: 0.050489680766611270   to: 0.048295155577055326
i:  25, name:        features.module.22.bias  changing lr from: 0.051821912420322414   to: 0.049663279087844452
i:  26, name:      features.module.23.weight  changing lr from: 0.053123817027957126   to: 0.051001807299829761
i:  27, name:        features.module.23.bias  changing lr from: 0.054395404017919036   to: 0.052310596833899381
i:  28, name:      features.module.25.weight  changing lr from: 0.055636765942638160   to: 0.053589601386614699
i:  29, name:        features.module.25.bias  changing lr from: 0.056848068840698811   to: 0.054838861069815198
i:  30, name:      features.module.26.weight  changing lr from: 0.058029543447887133   to: 0.056058492658102789
i:  31, name:        features.module.26.bias  changing lr from: 0.059181477193975332   to: 0.057248680679732926
i:  32, name:            classifier.0.weight  changing lr from: 0.060304206925555250   to: 0.058409669289565050
i:  33, name:              classifier.0.bias  changing lr from: 0.061398112298733536   to: 0.059541754865946851
i:  34, name:            classifier.3.weight  changing lr from: 0.062463609788955182   to: 0.060645279276662092
i:  35, name:              classifier.3.bias  changing lr from: 0.063501147268602745   to: 0.061720623762315290
i:  36, name:            classifier.6.weight  changing lr from: 0.064511199106293562   to: 0.062768203388719010
i:  37, name:              classifier.6.bias  changing lr from: 0.065494261744951432   to: 0.063788462022962164



# Switched to train mode...
Epoch: [37][  0/391]	Time  0.151 ( 0.151)	Data  0.121 ( 0.121)	Loss 6.3428e-01 (6.3428e-01)	Acc@1  82.81 ( 82.81)	Acc@5  94.53 ( 94.53)
Epoch: [37][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.013)	Loss 6.7822e-01 (6.3215e-01)	Acc@1  80.47 ( 81.39)	Acc@5  96.09 ( 96.16)
Epoch: [37][ 20/391]	Time  0.028 ( 0.032)	Data  0.001 ( 0.007)	Loss 5.7959e-01 (6.2974e-01)	Acc@1  82.81 ( 81.14)	Acc@5  96.88 ( 96.17)
Epoch: [37][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.006)	Loss 9.0088e-01 (6.4441e-01)	Acc@1  71.09 ( 80.67)	Acc@5  94.53 ( 96.19)
Epoch: [37][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.2559e-01 (6.4529e-01)	Acc@1  77.34 ( 80.83)	Acc@5  95.31 ( 96.40)
Epoch: [37][ 50/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.6157e-01 (6.4583e-01)	Acc@1  89.06 ( 80.78)	Acc@5  98.44 ( 96.42)
Epoch: [37][ 60/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 7.5098e-01 (6.4902e-01)	Acc@1  76.56 ( 80.67)	Acc@5  95.31 ( 96.49)
Epoch: [37][ 70/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 5.2246e-01 (6.4167e-01)	Acc@1  84.38 ( 80.83)	Acc@5  96.88 ( 96.49)
Epoch: [37][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9463e-01 (6.4352e-01)	Acc@1  83.59 ( 80.70)	Acc@5  98.44 ( 96.50)
Epoch: [37][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.6572e-01 (6.4379e-01)	Acc@1  71.88 ( 80.60)	Acc@5  95.31 ( 96.57)
Epoch: [37][100/391]	Time  0.029 ( 0.027)	Data  0.002 ( 0.003)	Loss 6.8457e-01 (6.4888e-01)	Acc@1  78.12 ( 80.53)	Acc@5  95.31 ( 96.50)
Epoch: [37][110/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.4023e-01 (6.5416e-01)	Acc@1  78.12 ( 80.37)	Acc@5  96.88 ( 96.52)
Epoch: [37][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4707e-01 (6.5869e-01)	Acc@1  78.91 ( 80.35)	Acc@5  96.88 ( 96.52)
Epoch: [37][130/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.6592e-01 (6.5526e-01)	Acc@1  82.81 ( 80.47)	Acc@5  98.44 ( 96.56)
Epoch: [37][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3633e-01 (6.5427e-01)	Acc@1  78.12 ( 80.52)	Acc@5  96.09 ( 96.54)
Epoch: [37][150/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3096e-01 (6.5898e-01)	Acc@1  78.91 ( 80.38)	Acc@5  95.31 ( 96.53)
Epoch: [37][160/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1094e-01 (6.5955e-01)	Acc@1  77.34 ( 80.34)	Acc@5  96.09 ( 96.58)
Epoch: [37][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9717e-01 (6.5972e-01)	Acc@1  79.69 ( 80.26)	Acc@5  98.44 ( 96.63)
Epoch: [37][180/391]	Time  0.034 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.6494e-01 (6.5721e-01)	Acc@1  82.03 ( 80.30)	Acc@5  98.44 ( 96.66)
Epoch: [37][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.6416e-01 (6.5702e-01)	Acc@1  83.59 ( 80.36)	Acc@5  94.53 ( 96.65)
Epoch: [37][200/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.003)	Loss 6.2695e-01 (6.5884e-01)	Acc@1  83.59 ( 80.33)	Acc@5  97.66 ( 96.61)
Epoch: [37][210/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.1436e-01 (6.6102e-01)	Acc@1  78.91 ( 80.24)	Acc@5  97.66 ( 96.59)
Epoch: [37][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3340e-01 (6.6349e-01)	Acc@1  81.25 ( 80.20)	Acc@5  96.09 ( 96.59)
Epoch: [37][230/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3047e-01 (6.6680e-01)	Acc@1  75.00 ( 80.11)	Acc@5  95.31 ( 96.56)
Epoch: [37][240/391]	Time  0.041 ( 0.026)	Data  0.004 ( 0.002)	Loss 5.5615e-01 (6.6705e-01)	Acc@1  79.69 ( 80.09)	Acc@5  98.44 ( 96.54)
Epoch: [37][250/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.002)	Loss 5.8936e-01 (6.6622e-01)	Acc@1  83.59 ( 80.13)	Acc@5  95.31 ( 96.51)
Epoch: [37][260/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7285e-01 (6.6689e-01)	Acc@1  79.69 ( 80.10)	Acc@5  95.31 ( 96.50)
Epoch: [37][270/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.0557e-01 (6.6614e-01)	Acc@1  79.69 ( 80.17)	Acc@5  94.53 ( 96.50)
Epoch: [37][280/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3145e-01 (6.6776e-01)	Acc@1  77.34 ( 80.14)	Acc@5  97.66 ( 96.48)
Epoch: [37][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.2607e-01 (6.6910e-01)	Acc@1  78.91 ( 80.07)	Acc@5  95.31 ( 96.48)
Epoch: [37][300/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.8154e-01 (6.6989e-01)	Acc@1  79.69 ( 80.01)	Acc@5  96.09 ( 96.48)
Epoch: [37][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.6416e-01 (6.7246e-01)	Acc@1  82.03 ( 79.98)	Acc@5  89.84 ( 96.45)
Epoch: [37][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.9922e-01 (6.7384e-01)	Acc@1  78.91 ( 79.92)	Acc@5  95.31 ( 96.43)
Epoch: [37][330/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.2275e-01 (6.7488e-01)	Acc@1  78.12 ( 79.90)	Acc@5  92.97 ( 96.41)
Epoch: [37][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.7598e-01 (6.7552e-01)	Acc@1  71.88 ( 79.86)	Acc@5  96.88 ( 96.41)
Epoch: [37][350/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7871e-01 (6.7739e-01)	Acc@1  75.78 ( 79.82)	Acc@5  96.88 ( 96.37)
Epoch: [37][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7139e-01 (6.7692e-01)	Acc@1  80.47 ( 79.85)	Acc@5  94.53 ( 96.38)
Epoch: [37][370/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.4375e-01 (6.7891e-01)	Acc@1  75.78 ( 79.77)	Acc@5  96.88 ( 96.36)
Epoch: [37][380/391]	Time  0.025 ( 0.026)	Data  0.004 ( 0.002)	Loss 6.2402e-01 (6.7985e-01)	Acc@1  80.47 ( 79.75)	Acc@5  97.66 ( 96.36)
Epoch: [37][390/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.8066e-01 (6.8147e-01)	Acc@1  76.25 ( 79.72)	Acc@5  95.00 ( 96.35)
## e[37] optimizer.zero_grad (sum) time: 0.11446714401245117
## e[37]       loss.backward (sum) time: 2.2853848934173584
## e[37]      optimizer.step (sum) time: 0.9690124988555908
## epoch[37] training(only) time: 10.054545402526855
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.6699e+00 (1.6699e+00)	Acc@1  61.00 ( 61.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.016 ( 0.026)	Loss 1.6338e+00 (1.5816e+00)	Acc@1  59.00 ( 62.64)	Acc@5  91.00 ( 86.91)
Test: [ 20/100]	Time  0.019 ( 0.020)	Loss 1.5205e+00 (1.5327e+00)	Acc@1  67.00 ( 63.38)	Acc@5  84.00 ( 86.90)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.7090e+00 (1.5265e+00)	Acc@1  53.00 ( 62.55)	Acc@5  88.00 ( 87.10)
Test: [ 40/100]	Time  0.009 ( 0.017)	Loss 1.3643e+00 (1.5172e+00)	Acc@1  68.00 ( 62.63)	Acc@5  91.00 ( 87.51)
Test: [ 50/100]	Time  0.018 ( 0.016)	Loss 1.2480e+00 (1.5354e+00)	Acc@1  67.00 ( 62.47)	Acc@5  90.00 ( 87.14)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.5527e+00 (1.5277e+00)	Acc@1  61.00 ( 62.67)	Acc@5  85.00 ( 87.30)
Test: [ 70/100]	Time  0.014 ( 0.015)	Loss 1.6367e+00 (1.5253e+00)	Acc@1  64.00 ( 62.66)	Acc@5  89.00 ( 87.49)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.9990e+00 (1.5281e+00)	Acc@1  58.00 ( 62.52)	Acc@5  76.00 ( 87.35)
Test: [ 90/100]	Time  0.021 ( 0.015)	Loss 1.7637e+00 (1.5207e+00)	Acc@1  54.00 ( 62.51)	Acc@5  86.00 ( 87.41)
 * Acc@1 62.770 Acc@5 87.360
### epoch[37] execution time: 11.624800205230713
EPOCH 38
i:   0, name:       features.module.0.weight  changing lr from: 0.010278393921015021   to: 0.008396515016716099
i:   1, name:         features.module.0.bias  changing lr from: 0.011752644909714265   to: 0.009770995249960796
i:   2, name:       features.module.1.weight  changing lr from: 0.013279996516398239   to: 0.011210993467617773
i:   3, name:         features.module.1.bias  changing lr from: 0.014851345208060102   to: 0.012706591307004828
i:   4, name:       features.module.4.weight  changing lr from: 0.016458342256874769   to: 0.014248645902831404
i:   5, name:         features.module.4.bias  changing lr from: 0.018093355408614665   to: 0.015828756135350681
i:   6, name:       features.module.5.weight  changing lr from: 0.019749429395729091   to: 0.017439226587543540
i:   7, name:         features.module.5.bias  changing lr from: 0.021420246015760043   to: 0.019073030107908558
i:   8, name:       features.module.8.weight  changing lr from: 0.023100084354713321   to: 0.020723769712780307
i:   9, name:         features.module.8.bias  changing lr from: 0.024783781614977365   to: 0.022385640422426752
i:  10, name:       features.module.9.weight  changing lr from: 0.026466694905833445   to: 0.024053391505901425
i:  11, name:         features.module.9.bias  changing lr from: 0.028144664269185438   to: 0.025722289508328472
i:  12, name:      features.module.11.weight  changing lr from: 0.029813977141741128   to: 0.027388082348758416
i:  13, name:        features.module.11.bias  changing lr from: 0.031471334395599335   to: 0.029046964704930223
i:  14, name:      features.module.12.weight  changing lr from: 0.033113818050343136   to: 0.030695544841385314
i:  15, name:        features.module.12.bias  changing lr from: 0.034738860709810707   to: 0.032330812987773280
i:  16, name:      features.module.15.weight  changing lr from: 0.036344216744396592   to: 0.033950111333419276
i:  17, name:        features.module.15.bias  changing lr from: 0.037927935213868745   to: 0.035551105671009137
i:  18, name:      features.module.16.weight  changing lr from: 0.039488334505270686   to: 0.037131758695469100
i:  19, name:        features.module.16.bias  changing lr from: 0.041023978644637492   to: 0.038690304942787808
i:  20, name:      features.module.18.weight  changing lr from: 0.042533655229243600   to: 0.040225227336802137
i:  21, name:        features.module.18.bias  changing lr from: 0.044016354918263952   to: 0.041735235299095702
i:  22, name:      features.module.19.weight  changing lr from: 0.045471252413523011   to: 0.043219244367512365
i:  23, name:        features.module.19.bias  changing lr from: 0.046897688857950259   to: 0.044676357261811364
i:  24, name:      features.module.22.weight  changing lr from: 0.048295155577055326   to: 0.046105846330218539
i:  25, name:        features.module.22.bias  changing lr from: 0.049663279087844452   to: 0.047507137307662040
i:  26, name:      features.module.23.weight  changing lr from: 0.051001807299829761   to: 0.048879794314975827
i:  27, name:        features.module.23.bias  changing lr from: 0.052310596833899381   to: 0.050223506028027672
i:  28, name:      features.module.25.weight  changing lr from: 0.053589601386614699   to: 0.051538072946334647
i:  29, name:        features.module.25.bias  changing lr from: 0.054838861069815198   to: 0.052823395692065622
i:  30, name:      features.module.26.weight  changing lr from: 0.056058492658102789   to: 0.054079464272227852
i:  31, name:        features.module.26.bias  changing lr from: 0.057248680679732926   to: 0.055306348239150961
i:  32, name:            classifier.0.weight  changing lr from: 0.058409669289565050   to: 0.056504187687000343
i:  33, name:              classifier.0.bias  changing lr from: 0.059541754865946851   to: 0.057673185024877949
i:  34, name:            classifier.3.weight  changing lr from: 0.060645279276662092   to: 0.058813597470021572
i:  35, name:              classifier.3.bias  changing lr from: 0.061720623762315290   to: 0.059925730207632436
i:  36, name:            classifier.6.weight  changing lr from: 0.062768203388719010   to: 0.061009930166894301
i:  37, name:              classifier.6.bias  changing lr from: 0.063788462022962164   to: 0.062066580365754014



# Switched to train mode...
Epoch: [38][  0/391]	Time  0.181 ( 0.181)	Data  0.150 ( 0.150)	Loss 7.4756e-01 (7.4756e-01)	Acc@1  75.00 ( 75.00)	Acc@5  96.09 ( 96.09)
Epoch: [38][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.015)	Loss 4.2749e-01 (6.3778e-01)	Acc@1  86.72 ( 81.32)	Acc@5  99.22 ( 96.66)
Epoch: [38][ 20/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.009)	Loss 6.8311e-01 (6.3123e-01)	Acc@1  78.91 ( 81.47)	Acc@5  96.09 ( 96.69)
Epoch: [38][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 5.1562e-01 (6.2566e-01)	Acc@1  84.38 ( 81.58)	Acc@5  98.44 ( 96.85)
Epoch: [38][ 40/391]	Time  0.019 ( 0.029)	Data  0.001 ( 0.005)	Loss 7.3340e-01 (6.0342e-01)	Acc@1  78.12 ( 82.26)	Acc@5  95.31 ( 97.07)
Epoch: [38][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 8.4082e-01 (6.0822e-01)	Acc@1  74.22 ( 82.20)	Acc@5  94.53 ( 96.97)
Epoch: [38][ 60/391]	Time  0.030 ( 0.028)	Data  0.002 ( 0.004)	Loss 6.2793e-01 (6.0954e-01)	Acc@1  81.25 ( 81.98)	Acc@5  96.09 ( 96.93)
Epoch: [38][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.9365e-01 (6.1408e-01)	Acc@1  84.38 ( 81.82)	Acc@5  97.66 ( 96.83)
Epoch: [38][ 80/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.7695e-01 (6.2012e-01)	Acc@1  75.78 ( 81.65)	Acc@5  94.53 ( 96.84)
Epoch: [38][ 90/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7988e-01 (6.1470e-01)	Acc@1  89.06 ( 81.71)	Acc@5  98.44 ( 96.87)
Epoch: [38][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6362e-01 (6.1141e-01)	Acc@1  85.16 ( 81.77)	Acc@5  98.44 ( 96.87)
Epoch: [38][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5420e-01 (6.1344e-01)	Acc@1  86.72 ( 81.81)	Acc@5  94.53 ( 96.81)
Epoch: [38][120/391]	Time  0.037 ( 0.026)	Data  0.004 ( 0.003)	Loss 6.7871e-01 (6.1102e-01)	Acc@1  77.34 ( 81.76)	Acc@5  96.09 ( 96.82)
Epoch: [38][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9863e-01 (6.1291e-01)	Acc@1  85.94 ( 81.80)	Acc@5  98.44 ( 96.83)
Epoch: [38][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0068e-01 (6.1203e-01)	Acc@1  79.69 ( 81.78)	Acc@5  94.53 ( 96.85)
Epoch: [38][150/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0264e-01 (6.1280e-01)	Acc@1  78.91 ( 81.74)	Acc@5  93.75 ( 96.82)
Epoch: [38][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8359e-01 (6.1557e-01)	Acc@1  76.56 ( 81.60)	Acc@5  96.09 ( 96.82)
Epoch: [38][170/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5234e-01 (6.1660e-01)	Acc@1  82.81 ( 81.59)	Acc@5  98.44 ( 96.84)
Epoch: [38][180/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1436e-01 (6.1554e-01)	Acc@1  79.69 ( 81.65)	Acc@5  96.09 ( 96.86)
Epoch: [38][190/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5586e-01 (6.1643e-01)	Acc@1  75.00 ( 81.56)	Acc@5  96.09 ( 96.86)
Epoch: [38][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0488e-01 (6.1446e-01)	Acc@1  84.38 ( 81.59)	Acc@5  99.22 ( 96.90)
Epoch: [38][210/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.2295e-01 (6.1595e-01)	Acc@1  83.59 ( 81.56)	Acc@5  96.88 ( 96.88)
Epoch: [38][220/391]	Time  0.035 ( 0.026)	Data  0.004 ( 0.003)	Loss 5.4297e-01 (6.1610e-01)	Acc@1  83.59 ( 81.54)	Acc@5  97.66 ( 96.91)
Epoch: [38][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2402e-01 (6.1552e-01)	Acc@1  82.03 ( 81.59)	Acc@5  95.31 ( 96.89)
Epoch: [38][240/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0693e-01 (6.1444e-01)	Acc@1  79.69 ( 81.57)	Acc@5  96.09 ( 96.88)
Epoch: [38][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6641e-01 (6.1688e-01)	Acc@1  81.25 ( 81.52)	Acc@5  99.22 ( 96.83)
Epoch: [38][260/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.7881e-01 (6.1994e-01)	Acc@1  77.34 ( 81.42)	Acc@5  96.09 ( 96.84)
Epoch: [38][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.6201e-01 (6.1956e-01)	Acc@1  82.81 ( 81.41)	Acc@5  97.66 ( 96.84)
Epoch: [38][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.5586e-01 (6.2009e-01)	Acc@1  76.56 ( 81.40)	Acc@5  96.88 ( 96.87)
Epoch: [38][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.3467e-01 (6.2273e-01)	Acc@1  82.81 ( 81.36)	Acc@5  98.44 ( 96.84)
Epoch: [38][300/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 6.3867e-01 (6.2453e-01)	Acc@1  78.91 ( 81.31)	Acc@5  97.66 ( 96.84)
Epoch: [38][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.2441e-01 (6.2490e-01)	Acc@1  85.16 ( 81.30)	Acc@5  96.88 ( 96.84)
Epoch: [38][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.4209e-01 (6.2583e-01)	Acc@1  78.91 ( 81.31)	Acc@5  96.88 ( 96.82)
Epoch: [38][330/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (6.2630e-01)	Acc@1  85.16 ( 81.30)	Acc@5  98.44 ( 96.82)
Epoch: [38][340/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.002)	Loss 6.2891e-01 (6.2770e-01)	Acc@1  80.47 ( 81.24)	Acc@5  96.09 ( 96.81)
Epoch: [38][350/391]	Time  0.049 ( 0.025)	Data  0.010 ( 0.002)	Loss 7.2363e-01 (6.2864e-01)	Acc@1  80.47 ( 81.22)	Acc@5  96.09 ( 96.82)
Epoch: [38][360/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.8984e-01 (6.3063e-01)	Acc@1  84.38 ( 81.16)	Acc@5  98.44 ( 96.83)
Epoch: [38][370/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 7.7393e-01 (6.3274e-01)	Acc@1  75.78 ( 81.09)	Acc@5  95.31 ( 96.81)
Epoch: [38][380/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.1230e-01 (6.3462e-01)	Acc@1  82.81 ( 81.04)	Acc@5  96.88 ( 96.80)
Epoch: [38][390/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.3018e-01 (6.3686e-01)	Acc@1  72.50 ( 80.97)	Acc@5  92.50 ( 96.76)
## e[38] optimizer.zero_grad (sum) time: 0.1139974594116211
## e[38]       loss.backward (sum) time: 2.313737630844116
## e[38]      optimizer.step (sum) time: 0.9716455936431885
## epoch[38] training(only) time: 10.035204648971558
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.6816e+00 (1.6816e+00)	Acc@1  59.00 ( 59.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.020 ( 0.026)	Loss 1.5234e+00 (1.5601e+00)	Acc@1  61.00 ( 63.64)	Acc@5  94.00 ( 87.09)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 1.5693e+00 (1.5006e+00)	Acc@1  65.00 ( 64.19)	Acc@5  84.00 ( 87.57)
Test: [ 30/100]	Time  0.014 ( 0.018)	Loss 1.4873e+00 (1.4759e+00)	Acc@1  62.00 ( 63.65)	Acc@5  87.00 ( 87.45)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.3350e+00 (1.4810e+00)	Acc@1  64.00 ( 63.56)	Acc@5  92.00 ( 87.24)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.2324e+00 (1.5028e+00)	Acc@1  65.00 ( 63.31)	Acc@5  89.00 ( 86.92)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.7285e+00 (1.4975e+00)	Acc@1  56.00 ( 63.18)	Acc@5  85.00 ( 86.98)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.4814e+00 (1.4896e+00)	Acc@1  65.00 ( 63.30)	Acc@5  89.00 ( 87.15)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8115e+00 (1.5000e+00)	Acc@1  59.00 ( 63.11)	Acc@5  85.00 ( 87.06)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.6514e+00 (1.4908e+00)	Acc@1  63.00 ( 63.60)	Acc@5  85.00 ( 87.15)
 * Acc@1 63.770 Acc@5 87.310
### epoch[38] execution time: 11.592027187347412
EPOCH 39
i:   0, name:       features.module.0.weight  changing lr from: 0.008396515016716099   to: 0.006712584379435493
i:   1, name:         features.module.0.bias  changing lr from: 0.009770995249960796   to: 0.007973519820055723
i:   2, name:       features.module.1.weight  changing lr from: 0.011210993467617773   to: 0.009312920920391314
i:   3, name:         features.module.1.bias  changing lr from: 0.012706591307004828   to: 0.010720078967091668
i:   4, name:       features.module.4.weight  changing lr from: 0.014248645902831404   to: 0.012185072400459969
i:   5, name:         features.module.4.bias  changing lr from: 0.015828756135350681   to: 0.013698739058536219
i:   6, name:       features.module.5.weight  changing lr from: 0.017439226587543540   to: 0.015252644825893502
i:   7, name:         features.module.5.bias  changing lr from: 0.019073030107908558   to: 0.016839049774521813
i:   8, name:       features.module.8.weight  changing lr from: 0.020723769712780307   to: 0.018450872699546872
i:   9, name:         features.module.8.bias  changing lr from: 0.022385640422426752   to: 0.020081654792593152
i:  10, name:       features.module.9.weight  changing lr from: 0.024053391505901425   to: 0.021725523057813001
i:  11, name:         features.module.9.bias  changing lr from: 0.025722289508328472   to: 0.023377153957540239
i:  12, name:      features.module.11.weight  changing lr from: 0.027388082348758416   to: 0.025031737673908979
i:  13, name:        features.module.11.bias  changing lr from: 0.029046964704930223   to: 0.026684943287506081
i:  14, name:      features.module.12.weight  changing lr from: 0.030695544841385314   to: 0.028332885102266232
i:  15, name:        features.module.12.bias  changing lr from: 0.032330812987773280   to: 0.029972090285635640
i:  16, name:      features.module.15.weight  changing lr from: 0.033950111333419276   to: 0.031599467942954741
i:  17, name:        features.module.15.bias  changing lr from: 0.035551105671009137   to: 0.033212279703646284
i:  18, name:      features.module.16.weight  changing lr from: 0.037131758695469100   to: 0.034808111862909212
i:  19, name:        features.module.16.bias  changing lr from: 0.038690304942787808   to: 0.036384849095115003
i:  20, name:      features.module.18.weight  changing lr from: 0.040225227336802137   to: 0.037940649733034994
i:  21, name:        features.module.18.bias  changing lr from: 0.041735235299095702   to: 0.039473922589550285
i:  22, name:      features.module.19.weight  changing lr from: 0.043219244367512365   to: 0.040983305284895688
i:  23, name:        features.module.19.bias  changing lr from: 0.044676357261811364   to: 0.042467644032132740
i:  24, name:      features.module.22.weight  changing lr from: 0.046105846330218539   to: 0.043925974825894276
i:  25, name:        features.module.22.bias  changing lr from: 0.047507137307662040   to: 0.045357505974035789
i:  26, name:      features.module.23.weight  changing lr from: 0.048879794314975827   to: 0.046761601908260085
i:  27, name:        features.module.23.bias  changing lr from: 0.050223506028027672   to: 0.048137768207724467
i:  28, name:      features.module.25.weight  changing lr from: 0.051538072946334647   to: 0.049485637768797053
i:  29, name:        features.module.25.bias  changing lr from: 0.052823395692065622   to: 0.050804958054264285
i:  30, name:      features.module.26.weight  changing lr from: 0.054079464272227852   to: 0.052095579356196665
i:  31, name:        features.module.26.bias  changing lr from: 0.055306348239150961   to: 0.053357444008185428
i:  32, name:            classifier.0.weight  changing lr from: 0.056504187687000343   to: 0.054590576484623225
i:  33, name:              classifier.0.bias  changing lr from: 0.057673185024877949   to: 0.055795074327000864
i:  34, name:            classifier.3.weight  changing lr from: 0.058813597470021572   to: 0.056971099839728638
i:  35, name:              classifier.3.bias  changing lr from: 0.059925730207632436   to: 0.058118872500686418
i:  36, name:            classifier.6.weight  changing lr from: 0.061009930166894301   to: 0.059238662034494266
i:  37, name:              classifier.6.bias  changing lr from: 0.062066580365754014   to: 0.060330782099322869



# Switched to train mode...
Epoch: [39][  0/391]	Time  0.173 ( 0.173)	Data  0.145 ( 0.145)	Loss 5.9668e-01 (5.9668e-01)	Acc@1  82.03 ( 82.03)	Acc@5  97.66 ( 97.66)
Epoch: [39][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.015)	Loss 4.0820e-01 (5.7273e-01)	Acc@1  88.28 ( 82.74)	Acc@5 100.00 ( 98.08)
Epoch: [39][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 6.2012e-01 (5.8816e-01)	Acc@1  82.03 ( 82.37)	Acc@5  96.88 ( 97.77)
Epoch: [39][ 30/391]	Time  0.037 ( 0.030)	Data  0.002 ( 0.006)	Loss 5.9473e-01 (5.6508e-01)	Acc@1  83.59 ( 82.94)	Acc@5  96.09 ( 97.66)
Epoch: [39][ 40/391]	Time  0.020 ( 0.029)	Data  0.002 ( 0.005)	Loss 4.6313e-01 (5.4987e-01)	Acc@1  83.59 ( 83.31)	Acc@5  97.66 ( 97.73)
Epoch: [39][ 50/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.6641e-01 (5.5767e-01)	Acc@1  85.94 ( 83.07)	Acc@5  96.88 ( 97.55)
Epoch: [39][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.9268e-01 (5.5963e-01)	Acc@1  84.38 ( 83.09)	Acc@5  98.44 ( 97.59)
Epoch: [39][ 70/391]	Time  0.027 ( 0.027)	Data  0.005 ( 0.004)	Loss 5.3369e-01 (5.5948e-01)	Acc@1  88.28 ( 83.25)	Acc@5  97.66 ( 97.58)
Epoch: [39][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.1123e-01 (5.6072e-01)	Acc@1  84.38 ( 83.16)	Acc@5  97.66 ( 97.50)
Epoch: [39][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.9668e-01 (5.6732e-01)	Acc@1  83.59 ( 83.08)	Acc@5  98.44 ( 97.44)
Epoch: [39][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.3027e-01 (5.6733e-01)	Acc@1  85.94 ( 83.08)	Acc@5  96.09 ( 97.42)
Epoch: [39][110/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9927e-01 (5.6846e-01)	Acc@1  85.16 ( 83.01)	Acc@5  96.88 ( 97.41)
Epoch: [39][120/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2344e-01 (5.7377e-01)	Acc@1  86.72 ( 82.97)	Acc@5  96.09 ( 97.30)
Epoch: [39][130/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.3428e-01 (5.7170e-01)	Acc@1  80.47 ( 82.94)	Acc@5  97.66 ( 97.27)
Epoch: [39][140/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6934e-01 (5.7309e-01)	Acc@1  80.47 ( 82.82)	Acc@5  96.88 ( 97.28)
Epoch: [39][150/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.9600e-01 (5.7018e-01)	Acc@1  89.06 ( 82.94)	Acc@5 100.00 ( 97.33)
Epoch: [39][160/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1191e-01 (5.7400e-01)	Acc@1  78.91 ( 82.79)	Acc@5  96.88 ( 97.30)
Epoch: [39][170/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0352e-01 (5.7687e-01)	Acc@1  80.47 ( 82.64)	Acc@5  98.44 ( 97.31)
Epoch: [39][180/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.3760e-01 (5.7924e-01)	Acc@1  82.03 ( 82.57)	Acc@5  98.44 ( 97.30)
Epoch: [39][190/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9341e-01 (5.7920e-01)	Acc@1  86.72 ( 82.64)	Acc@5  99.22 ( 97.29)
Epoch: [39][200/391]	Time  0.026 ( 0.026)	Data  0.004 ( 0.003)	Loss 7.6953e-01 (5.8304e-01)	Acc@1  77.34 ( 82.54)	Acc@5  97.66 ( 97.30)
Epoch: [39][210/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.4951e-01 (5.8586e-01)	Acc@1  79.69 ( 82.47)	Acc@5  96.09 ( 97.29)
Epoch: [39][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1660e-01 (5.8627e-01)	Acc@1  87.50 ( 82.44)	Acc@5  97.66 ( 97.30)
Epoch: [39][230/391]	Time  0.042 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0605e-01 (5.8743e-01)	Acc@1  78.91 ( 82.39)	Acc@5  97.66 ( 97.29)
Epoch: [39][240/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9912e-01 (5.8623e-01)	Acc@1  82.81 ( 82.39)	Acc@5  97.66 ( 97.28)
Epoch: [39][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4150e-01 (5.8491e-01)	Acc@1  82.81 ( 82.43)	Acc@5  98.44 ( 97.29)
Epoch: [39][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1230e-01 (5.8461e-01)	Acc@1  79.69 ( 82.37)	Acc@5  96.88 ( 97.30)
Epoch: [39][270/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.6309e-01 (5.8376e-01)	Acc@1  81.25 ( 82.41)	Acc@5  95.31 ( 97.28)
Epoch: [39][280/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.3574e-01 (5.8663e-01)	Acc@1  81.25 ( 82.37)	Acc@5  96.88 ( 97.26)
Epoch: [39][290/391]	Time  0.039 ( 0.026)	Data  0.006 ( 0.002)	Loss 6.7334e-01 (5.8553e-01)	Acc@1  81.25 ( 82.42)	Acc@5  96.88 ( 97.25)
Epoch: [39][300/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.0400e-01 (5.8620e-01)	Acc@1  78.12 ( 82.37)	Acc@5  97.66 ( 97.26)
Epoch: [39][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.6758e-01 (5.8681e-01)	Acc@1  81.25 ( 82.37)	Acc@5  95.31 ( 97.24)
Epoch: [39][320/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.002)	Loss 6.2744e-01 (5.8792e-01)	Acc@1  82.81 ( 82.39)	Acc@5  95.31 ( 97.24)
Epoch: [39][330/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.002)	Loss 5.5762e-01 (5.8850e-01)	Acc@1  82.81 ( 82.37)	Acc@5  96.09 ( 97.23)
Epoch: [39][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.8125e-01 (5.9087e-01)	Acc@1  78.12 ( 82.31)	Acc@5  96.09 ( 97.23)
Epoch: [39][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.3643e-01 (5.9527e-01)	Acc@1  78.91 ( 82.21)	Acc@5  93.75 ( 97.20)
Epoch: [39][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.7148e-01 (5.9555e-01)	Acc@1  77.34 ( 82.18)	Acc@5  96.88 ( 97.21)
Epoch: [39][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.6738e-01 (5.9631e-01)	Acc@1  82.81 ( 82.16)	Acc@5  96.88 ( 97.23)
Epoch: [39][380/391]	Time  0.036 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.0244e-01 (5.9720e-01)	Acc@1  86.72 ( 82.17)	Acc@5  98.44 ( 97.19)
Epoch: [39][390/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.6982e-01 (5.9748e-01)	Acc@1  81.25 ( 82.16)	Acc@5  97.50 ( 97.18)
## e[39] optimizer.zero_grad (sum) time: 0.11269664764404297
## e[39]       loss.backward (sum) time: 2.2899889945983887
## e[39]      optimizer.step (sum) time: 0.9660992622375488
## epoch[39] training(only) time: 10.102600812911987
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.6133e+00 (1.6133e+00)	Acc@1  66.00 ( 66.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.6309e+00 (1.5796e+00)	Acc@1  60.00 ( 64.36)	Acc@5  92.00 ( 87.36)
Test: [ 20/100]	Time  0.032 ( 0.020)	Loss 1.7861e+00 (1.5346e+00)	Acc@1  65.00 ( 64.86)	Acc@5  85.00 ( 87.52)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.8262e+00 (1.5204e+00)	Acc@1  58.00 ( 64.71)	Acc@5  86.00 ( 87.58)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.3799e+00 (1.5076e+00)	Acc@1  60.00 ( 64.24)	Acc@5  91.00 ( 87.78)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.3613e+00 (1.5218e+00)	Acc@1  66.00 ( 64.25)	Acc@5  89.00 ( 87.57)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 1.5205e+00 (1.5042e+00)	Acc@1  64.00 ( 64.38)	Acc@5  85.00 ( 87.79)
Test: [ 70/100]	Time  0.018 ( 0.016)	Loss 1.6826e+00 (1.5077e+00)	Acc@1  62.00 ( 64.45)	Acc@5  84.00 ( 87.69)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.6611e+00 (1.5130e+00)	Acc@1  63.00 ( 64.31)	Acc@5  82.00 ( 87.60)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.8320e+00 (1.5096e+00)	Acc@1  58.00 ( 64.33)	Acc@5  83.00 ( 87.56)
 * Acc@1 64.390 Acc@5 87.570
### epoch[39] execution time: 11.679831981658936
EPOCH 40
i:   0, name:       features.module.0.weight  changing lr from: 0.006712584379435493   to: 0.005234426044027662
i:   1, name:         features.module.0.bias  changing lr from: 0.007973519820055723   to: 0.006368248128714621
i:   2, name:       features.module.1.weight  changing lr from: 0.009312920920391314   to: 0.007593932847907897
i:   3, name:         features.module.1.bias  changing lr from: 0.010720078967091668   to: 0.008900017141887956
i:   4, name:       features.module.4.weight  changing lr from: 0.012185072400459969   to: 0.010275826488122069
i:   5, name:         features.module.4.bias  changing lr from: 0.013698739058536219   to: 0.011711454654104836
i:   6, name:       features.module.5.weight  changing lr from: 0.015252644825893502   to: 0.013197738380637081
i:   7, name:         features.module.5.bias  changing lr from: 0.016839049774521813   to: 0.014726228286967229
i:   8, name:       features.module.8.weight  changing lr from: 0.018450872699546872   to: 0.016289157082733013
i:   9, name:         features.module.8.bias  changing lr from: 0.020081654792593152   to: 0.017879405991200192
i:  10, name:       features.module.9.weight  changing lr from: 0.021725523057813001   to: 0.019490470131512807
i:  11, name:         features.module.9.bias  changing lr from: 0.023377153957540239   to: 0.021116423472189880
i:  12, name:      features.module.11.weight  changing lr from: 0.025031737673908979   to: 0.022751883851652378
i:  13, name:        features.module.11.bias  changing lr from: 0.026684943287506081   to: 0.024391978462006515
i:  14, name:      features.module.12.weight  changing lr from: 0.028332885102266232   to: 0.026032310107645253
i:  15, name:        features.module.12.bias  changing lr from: 0.029972090285635640   to: 0.027668924478633974
i:  16, name:      features.module.15.weight  changing lr from: 0.031599467942954741   to: 0.029298278618655194
i:  17, name:        features.module.15.bias  changing lr from: 0.033212279703646284   to: 0.030917210716996893
i:  18, name:      features.module.16.weight  changing lr from: 0.034808111862909212   to: 0.032522911312344194
i:  19, name:        features.module.16.bias  changing lr from: 0.036384849095115003   to: 0.034112895961773519
i:  20, name:      features.module.18.weight  changing lr from: 0.037940649733034994   to: 0.035684979400307536
i:  21, name:        features.module.18.bias  changing lr from: 0.039473922589550285   to: 0.037237251193728893
i:  22, name:      features.module.19.weight  changing lr from: 0.040983305284895688   to: 0.038768052869269792
i:  23, name:        features.module.19.bias  changing lr from: 0.042467644032132740   to: 0.040275956494579601
i:  24, name:      features.module.22.weight  changing lr from: 0.043925974825894276   to: 0.041759744664409904
i:  25, name:        features.module.22.bias  changing lr from: 0.045357505974035789   to: 0.043218391846219013
i:  26, name:      features.module.23.weight  changing lr from: 0.046761601908260085   to: 0.044651047029924104
i:  27, name:        features.module.23.bias  changing lr from: 0.048137768207724467   to: 0.046057017622929075
i:  28, name:      features.module.25.weight  changing lr from: 0.049485637768797053   to: 0.047435754528995386
i:  29, name:        features.module.25.bias  changing lr from: 0.050804958054264285   to: 0.048786838348213166
i:  30, name:      features.module.26.weight  changing lr from: 0.052095579356196665   to: 0.050109966635030687
i:  31, name:        features.module.26.bias  changing lr from: 0.053357444008185428   to: 0.051404942151806798
i:  32, name:            classifier.0.weight  changing lr from: 0.054590576484623225   to: 0.052671662056488627
i:  33, name:              classifier.0.bias  changing lr from: 0.055795074327000864   to: 0.053910107964644495
i:  34, name:            classifier.3.weight  changing lr from: 0.056971099839728638   to: 0.055120336828076426
i:  35, name:              classifier.3.bias  changing lr from: 0.058118872500686418   to: 0.056302472574497955
i:  36, name:            classifier.6.weight  changing lr from: 0.059238662034494266   to: 0.057456698455211598
i:  37, name:              classifier.6.bias  changing lr from: 0.060330782099322869   to: 0.058583250050284141



# Switched to train mode...
Epoch: [40][  0/391]	Time  0.166 ( 0.166)	Data  0.138 ( 0.138)	Loss 6.3135e-01 (6.3135e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.88 ( 96.88)
Epoch: [40][ 10/391]	Time  0.032 ( 0.038)	Data  0.004 ( 0.014)	Loss 5.6738e-01 (5.7675e-01)	Acc@1  82.03 ( 82.60)	Acc@5  96.09 ( 97.16)
Epoch: [40][ 20/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.008)	Loss 3.6914e-01 (5.3223e-01)	Acc@1  91.41 ( 84.08)	Acc@5  98.44 ( 97.28)
Epoch: [40][ 30/391]	Time  0.033 ( 0.030)	Data  0.001 ( 0.006)	Loss 5.9082e-01 (5.4160e-01)	Acc@1  83.59 ( 83.64)	Acc@5  96.88 ( 97.38)
Epoch: [40][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.6494e-01 (5.2739e-01)	Acc@1  78.91 ( 83.96)	Acc@5  95.31 ( 97.50)
Epoch: [40][ 50/391]	Time  0.033 ( 0.028)	Data  0.003 ( 0.004)	Loss 5.7764e-01 (5.2424e-01)	Acc@1  85.16 ( 84.19)	Acc@5  96.88 ( 97.58)
Epoch: [40][ 60/391]	Time  0.036 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.1270e-01 (5.2525e-01)	Acc@1  83.59 ( 84.17)	Acc@5  96.88 ( 97.55)
Epoch: [40][ 70/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.004)	Loss 5.9814e-01 (5.2919e-01)	Acc@1  82.03 ( 83.96)	Acc@5  94.53 ( 97.47)
Epoch: [40][ 80/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.6890e-01 (5.3047e-01)	Acc@1  87.50 ( 83.95)	Acc@5 100.00 ( 97.46)
Epoch: [40][ 90/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.8706e-01 (5.3238e-01)	Acc@1  85.16 ( 83.87)	Acc@5  96.88 ( 97.42)
Epoch: [40][100/391]	Time  0.043 ( 0.027)	Data  0.003 ( 0.003)	Loss 6.4893e-01 (5.2941e-01)	Acc@1  78.12 ( 83.97)	Acc@5  96.88 ( 97.45)
Epoch: [40][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7568e-01 (5.3605e-01)	Acc@1  84.38 ( 83.82)	Acc@5  97.66 ( 97.40)
Epoch: [40][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.6650e-01 (5.3765e-01)	Acc@1  80.47 ( 83.85)	Acc@5  95.31 ( 97.38)
Epoch: [40][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0254e-01 (5.4234e-01)	Acc@1  80.47 ( 83.69)	Acc@5  94.53 ( 97.33)
Epoch: [40][140/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2891e-01 (5.4444e-01)	Acc@1  83.59 ( 83.66)	Acc@5  96.09 ( 97.29)
Epoch: [40][150/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9951e-01 (5.4300e-01)	Acc@1  84.38 ( 83.70)	Acc@5  98.44 ( 97.33)
Epoch: [40][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9316e-01 (5.3831e-01)	Acc@1  85.94 ( 83.82)	Acc@5  97.66 ( 97.37)
Epoch: [40][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2354e-01 (5.3804e-01)	Acc@1  83.59 ( 83.79)	Acc@5  97.66 ( 97.41)
Epoch: [40][180/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1064e-01 (5.3522e-01)	Acc@1  89.06 ( 83.83)	Acc@5  98.44 ( 97.45)
Epoch: [40][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7803e-01 (5.3485e-01)	Acc@1  82.03 ( 83.84)	Acc@5 100.00 ( 97.46)
Epoch: [40][200/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.0400e-01 (5.3717e-01)	Acc@1  81.25 ( 83.80)	Acc@5  97.66 ( 97.45)
Epoch: [40][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.8740e-01 (5.3961e-01)	Acc@1  82.03 ( 83.71)	Acc@5  96.88 ( 97.44)
Epoch: [40][220/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.002)	Loss 6.9092e-01 (5.4186e-01)	Acc@1  78.91 ( 83.61)	Acc@5  96.09 ( 97.44)
Epoch: [40][230/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.002)	Loss 5.9375e-01 (5.4285e-01)	Acc@1  80.47 ( 83.61)	Acc@5  95.31 ( 97.41)
Epoch: [40][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.7178e-01 (5.4174e-01)	Acc@1  80.47 ( 83.60)	Acc@5  97.66 ( 97.43)
Epoch: [40][250/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9575e-01 (5.4209e-01)	Acc@1  85.94 ( 83.54)	Acc@5  98.44 ( 97.43)
Epoch: [40][260/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 7.7051e-01 (5.4382e-01)	Acc@1  75.78 ( 83.51)	Acc@5  96.88 ( 97.41)
Epoch: [40][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.2803e-01 (5.4522e-01)	Acc@1  81.25 ( 83.46)	Acc@5  95.31 ( 97.40)
Epoch: [40][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.6802e-01 (5.4724e-01)	Acc@1  85.16 ( 83.40)	Acc@5  97.66 ( 97.39)
Epoch: [40][290/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.9131e-01 (5.4829e-01)	Acc@1  81.25 ( 83.37)	Acc@5  96.88 ( 97.40)
Epoch: [40][300/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.5088e-01 (5.5276e-01)	Acc@1  82.03 ( 83.25)	Acc@5  96.09 ( 97.35)
Epoch: [40][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.8018e-01 (5.5275e-01)	Acc@1  79.69 ( 83.25)	Acc@5  93.75 ( 97.35)
Epoch: [40][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.8438e-01 (5.5300e-01)	Acc@1  82.81 ( 83.26)	Acc@5  96.88 ( 97.35)
Epoch: [40][330/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 6.7676e-01 (5.5504e-01)	Acc@1  81.25 ( 83.21)	Acc@5  98.44 ( 97.35)
Epoch: [40][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.8838e-01 (5.5556e-01)	Acc@1  85.16 ( 83.22)	Acc@5  96.09 ( 97.33)
Epoch: [40][350/391]	Time  0.039 ( 0.026)	Data  0.004 ( 0.002)	Loss 8.3252e-01 (5.5514e-01)	Acc@1  77.34 ( 83.24)	Acc@5  95.31 ( 97.34)
Epoch: [40][360/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 5.0830e-01 (5.5620e-01)	Acc@1  83.59 ( 83.21)	Acc@5  97.66 ( 97.36)
Epoch: [40][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.0596e-01 (5.5714e-01)	Acc@1  80.47 ( 83.16)	Acc@5  96.09 ( 97.37)
Epoch: [40][380/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.0781e-01 (5.5748e-01)	Acc@1  83.59 ( 83.15)	Acc@5  97.66 ( 97.37)
Epoch: [40][390/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.7910e-01 (5.5777e-01)	Acc@1  83.75 ( 83.14)	Acc@5  97.50 ( 97.38)
## e[40] optimizer.zero_grad (sum) time: 0.11583828926086426
## e[40]       loss.backward (sum) time: 2.2839534282684326
## e[40]      optimizer.step (sum) time: 0.9674224853515625
## epoch[40] training(only) time: 10.100440979003906
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.5752e+00 (1.5752e+00)	Acc@1  65.00 ( 65.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.021 ( 0.025)	Loss 1.6436e+00 (1.5109e+00)	Acc@1  63.00 ( 65.27)	Acc@5  89.00 ( 88.00)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 1.5068e+00 (1.4807e+00)	Acc@1  71.00 ( 65.71)	Acc@5  83.00 ( 87.71)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.8984e+00 (1.4930e+00)	Acc@1  54.00 ( 64.26)	Acc@5  86.00 ( 87.45)
Test: [ 40/100]	Time  0.019 ( 0.017)	Loss 1.4268e+00 (1.4963e+00)	Acc@1  65.00 ( 63.95)	Acc@5  89.00 ( 87.63)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.2090e+00 (1.5075e+00)	Acc@1  68.00 ( 64.04)	Acc@5  91.00 ( 87.65)
Test: [ 60/100]	Time  0.019 ( 0.016)	Loss 1.6094e+00 (1.4912e+00)	Acc@1  62.00 ( 64.21)	Acc@5  89.00 ( 88.03)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 1.6006e+00 (1.4895e+00)	Acc@1  59.00 ( 64.01)	Acc@5  89.00 ( 88.08)
Test: [ 80/100]	Time  0.011 ( 0.015)	Loss 1.6641e+00 (1.5001e+00)	Acc@1  66.00 ( 63.89)	Acc@5  81.00 ( 87.77)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 1.7764e+00 (1.4991e+00)	Acc@1  57.00 ( 64.00)	Acc@5  86.00 ( 87.80)
 * Acc@1 64.230 Acc@5 87.880
### epoch[40] execution time: 11.704915761947632
EPOCH 41
i:   0, name:       features.module.0.weight  changing lr from: 0.005234426044027662   to: 0.003968907966975204
i:   1, name:         features.module.0.bias  changing lr from: 0.006368248128714621   to: 0.004962351090667143
i:   2, name:       features.module.1.weight  changing lr from: 0.007593932847907897   to: 0.006061413890272965
i:   3, name:         features.module.1.bias  changing lr from: 0.008900017141887956   to: 0.007253926954073197
i:   4, name:       features.module.4.weight  changing lr from: 0.010275826488122069   to: 0.008528499299916231
i:   5, name:         features.module.4.bias  changing lr from: 0.011711454654104836   to: 0.009874507233725148
i:   6, name:       features.module.5.weight  changing lr from: 0.013197738380637081   to: 0.011282076495849037
i:   7, name:         features.module.5.bias  changing lr from: 0.014726228286967229   to: 0.012742059201811356
i:   8, name:       features.module.8.weight  changing lr from: 0.016289157082733013   to: 0.014246006856346367
i:   9, name:         features.module.8.bias  changing lr from: 0.017879405991200192   to: 0.015786140518892978
i:  10, name:       features.module.9.weight  changing lr from: 0.019490470131512807   to: 0.017355319022820528
i:  11, name:         features.module.9.bias  changing lr from: 0.021116423472189880   to: 0.018947005997398660
i:  12, name:      features.module.11.weight  changing lr from: 0.022751883851652378   to: 0.020555236308706884
i:  13, name:        features.module.11.bias  changing lr from: 0.024391978462006515   to: 0.022174582421190345
i:  14, name:      features.module.12.weight  changing lr from: 0.026032310107645253   to: 0.023800121083390122
i:  15, name:        features.module.12.bias  changing lr from: 0.027668924478633974   to: 0.025427400657628871
i:  16, name:      features.module.15.weight  changing lr from: 0.029298278618655194   to: 0.027052409342379624
i:  17, name:        features.module.15.bias  changing lr from: 0.030917210716996893   to: 0.028671544476100642
i:  18, name:      features.module.16.weight  changing lr from: 0.032522911312344194   to: 0.030281583061051200
i:  19, name:        features.module.16.bias  changing lr from: 0.034112895961773519   to: 0.031879653603719783
i:  20, name:      features.module.18.weight  changing lr from: 0.035684979400307536   to: 0.033463209333857019
i:  21, name:        features.module.18.bias  changing lr from: 0.037237251193728893   to: 0.035030002835682111
i:  22, name:      features.module.19.weight  changing lr from: 0.038768052869269792   to: 0.036578062101736301
i:  23, name:        features.module.19.bias  changing lr from: 0.040275956494579601   to: 0.038105668001294161
i:  24, name:      features.module.22.weight  changing lr from: 0.041759744664409904   to: 0.039611333140526950
i:  25, name:        features.module.22.bias  changing lr from: 0.043218391846219013   to: 0.041093782080144678
i:  26, name:      features.module.23.weight  changing lr from: 0.044651047029924104   to: 0.042551932867500412
i:  27, name:        features.module.23.bias  changing lr from: 0.046057017622929075   to: 0.043984879833677382
i:  28, name:      features.module.25.weight  changing lr from: 0.047435754528995386   to: 0.045391877601508056
i:  29, name:        features.module.25.bias  changing lr from: 0.048786838348213166   to: 0.046772326247463691
i:  30, name:      features.module.26.weight  changing lr from: 0.050109966635030687   to: 0.048125757558620731
i:  31, name:        features.module.26.bias  changing lr from: 0.051404942151806798   to: 0.049451822325215070
i:  32, name:            classifier.0.weight  changing lr from: 0.052671662056488627   to: 0.050750278609433597
i:  33, name:              classifier.0.bias  changing lr from: 0.053910107964644495   to: 0.052020980931891093
i:  34, name:            classifier.3.weight  changing lr from: 0.055120336828076426   to: 0.053263870318555034
i:  35, name:              classifier.3.bias  changing lr from: 0.056302472574497955   to: 0.054478965152589071
i:  36, name:            classifier.6.weight  changing lr from: 0.057456698455211598   to: 0.055666352777588760
i:  37, name:              classifier.6.bias  changing lr from: 0.058583250050284141   to: 0.056826181800892453



# Switched to train mode...
Epoch: [41][  0/391]	Time  0.177 ( 0.177)	Data  0.148 ( 0.148)	Loss 3.9844e-01 (3.9844e-01)	Acc@1  85.94 ( 85.94)	Acc@5  99.22 ( 99.22)
Epoch: [41][ 10/391]	Time  0.033 ( 0.039)	Data  0.005 ( 0.015)	Loss 6.2988e-01 (4.8156e-01)	Acc@1  81.25 ( 85.23)	Acc@5 100.00 ( 98.44)
Epoch: [41][ 20/391]	Time  0.021 ( 0.032)	Data  0.002 ( 0.009)	Loss 5.5273e-01 (4.7233e-01)	Acc@1  87.50 ( 85.38)	Acc@5  96.88 ( 98.62)
Epoch: [41][ 30/391]	Time  0.023 ( 0.030)	Data  0.001 ( 0.007)	Loss 4.4434e-01 (4.7837e-01)	Acc@1  84.38 ( 85.08)	Acc@5  97.66 ( 98.36)
Epoch: [41][ 40/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.6240e-01 (4.7232e-01)	Acc@1  85.94 ( 85.18)	Acc@5  97.66 ( 98.40)
Epoch: [41][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.6899e-01 (4.7514e-01)	Acc@1  83.59 ( 85.25)	Acc@5  98.44 ( 98.31)
Epoch: [41][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.9082e-01 (4.8511e-01)	Acc@1  83.59 ( 85.05)	Acc@5  95.31 ( 98.12)
Epoch: [41][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.2441e-01 (4.9125e-01)	Acc@1  83.59 ( 85.06)	Acc@5  96.88 ( 98.01)
Epoch: [41][ 80/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.0146e-01 (4.8967e-01)	Acc@1  82.03 ( 85.00)	Acc@5  98.44 ( 98.02)
Epoch: [41][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.1431e-01 (4.8878e-01)	Acc@1  89.06 ( 85.06)	Acc@5  97.66 ( 98.05)
Epoch: [41][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8770e-01 (4.8765e-01)	Acc@1  89.84 ( 85.15)	Acc@5  99.22 ( 98.10)
Epoch: [41][110/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.8936e-01 (4.8836e-01)	Acc@1  81.25 ( 85.14)	Acc@5  99.22 ( 98.09)
Epoch: [41][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.3857e-01 (4.8910e-01)	Acc@1  87.50 ( 85.12)	Acc@5  96.88 ( 98.11)
Epoch: [41][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3818e-01 (4.9020e-01)	Acc@1  80.47 ( 85.14)	Acc@5  96.09 ( 98.09)
Epoch: [41][140/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1323e-01 (4.8908e-01)	Acc@1  89.06 ( 85.15)	Acc@5  99.22 ( 98.12)
Epoch: [41][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6265e-01 (4.9138e-01)	Acc@1  87.50 ( 85.11)	Acc@5  99.22 ( 98.08)
Epoch: [41][160/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.9502e-01 (4.9111e-01)	Acc@1  86.72 ( 85.05)	Acc@5 100.00 ( 98.09)
Epoch: [41][170/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.8267e-01 (4.9477e-01)	Acc@1  85.94 ( 84.96)	Acc@5  97.66 ( 98.04)
Epoch: [41][180/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3677e-01 (4.9582e-01)	Acc@1  87.50 ( 84.92)	Acc@5  96.09 ( 98.02)
Epoch: [41][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0244e-01 (4.9736e-01)	Acc@1  81.25 ( 84.86)	Acc@5  97.66 ( 97.99)
Epoch: [41][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2407e-01 (5.0060e-01)	Acc@1  85.16 ( 84.76)	Acc@5 100.00 ( 97.96)
Epoch: [41][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6348e-01 (5.0330e-01)	Acc@1  83.59 ( 84.68)	Acc@5  97.66 ( 97.92)
Epoch: [41][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1035e-01 (5.0208e-01)	Acc@1  82.03 ( 84.69)	Acc@5  97.66 ( 97.91)
Epoch: [41][230/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.0928e-01 (5.0300e-01)	Acc@1  86.72 ( 84.68)	Acc@5  98.44 ( 97.91)
Epoch: [41][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8867e-01 (5.0271e-01)	Acc@1  85.16 ( 84.67)	Acc@5  99.22 ( 97.91)
Epoch: [41][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3438e-01 (5.0486e-01)	Acc@1  82.81 ( 84.61)	Acc@5  95.31 ( 97.89)
Epoch: [41][260/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.5215e-01 (5.0314e-01)	Acc@1  88.28 ( 84.68)	Acc@5  97.66 ( 97.90)
Epoch: [41][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5332e-01 (5.0467e-01)	Acc@1  79.69 ( 84.70)	Acc@5  97.66 ( 97.88)
Epoch: [41][280/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2930e-01 (5.0518e-01)	Acc@1  82.81 ( 84.69)	Acc@5  98.44 ( 97.88)
Epoch: [41][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0645e-01 (5.0570e-01)	Acc@1  82.81 ( 84.67)	Acc@5  96.09 ( 97.86)
Epoch: [41][300/391]	Time  0.036 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.2202e-01 (5.0637e-01)	Acc@1  89.84 ( 84.65)	Acc@5  97.66 ( 97.84)
Epoch: [41][310/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9082e-01 (5.0692e-01)	Acc@1  81.25 ( 84.62)	Acc@5  98.44 ( 97.84)
Epoch: [41][320/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.9634e-01 (5.0690e-01)	Acc@1  82.03 ( 84.65)	Acc@5  96.88 ( 97.83)
Epoch: [41][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.1133e-01 (5.0792e-01)	Acc@1  81.25 ( 84.60)	Acc@5  96.88 ( 97.82)
Epoch: [41][340/391]	Time  0.038 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.4863e-01 (5.0947e-01)	Acc@1  88.28 ( 84.57)	Acc@5  99.22 ( 97.81)
Epoch: [41][350/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.9180e-01 (5.1100e-01)	Acc@1  80.47 ( 84.52)	Acc@5  97.66 ( 97.78)
Epoch: [41][360/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (5.1198e-01)	Acc@1  85.94 ( 84.46)	Acc@5  99.22 ( 97.77)
Epoch: [41][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.7461e-01 (5.1324e-01)	Acc@1  85.94 ( 84.44)	Acc@5  97.66 ( 97.76)
Epoch: [41][380/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.002)	Loss 4.0820e-01 (5.1283e-01)	Acc@1  85.94 ( 84.45)	Acc@5  97.66 ( 97.76)
Epoch: [41][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.1377e-01 (5.1421e-01)	Acc@1  81.25 ( 84.42)	Acc@5  98.75 ( 97.75)
## e[41] optimizer.zero_grad (sum) time: 0.11380290985107422
## e[41]       loss.backward (sum) time: 2.293985605239868
## e[41]      optimizer.step (sum) time: 0.9623701572418213
## epoch[41] training(only) time: 10.078437805175781
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.5332e+00 (1.5332e+00)	Acc@1  66.00 ( 66.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.6768e+00 (1.5653e+00)	Acc@1  62.00 ( 64.64)	Acc@5  89.00 ( 86.18)
Test: [ 20/100]	Time  0.021 ( 0.020)	Loss 1.5215e+00 (1.5419e+00)	Acc@1  64.00 ( 64.10)	Acc@5  85.00 ( 86.38)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.7559e+00 (1.5272e+00)	Acc@1  61.00 ( 64.19)	Acc@5  87.00 ( 86.90)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4375e+00 (1.5152e+00)	Acc@1  68.00 ( 64.17)	Acc@5  89.00 ( 87.39)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.3037e+00 (1.5289e+00)	Acc@1  70.00 ( 64.14)	Acc@5  90.00 ( 87.37)
Test: [ 60/100]	Time  0.018 ( 0.016)	Loss 1.5293e+00 (1.5197e+00)	Acc@1  63.00 ( 64.30)	Acc@5  88.00 ( 87.43)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 1.7344e+00 (1.5127e+00)	Acc@1  61.00 ( 64.30)	Acc@5  90.00 ( 87.62)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.6084e+00 (1.5192e+00)	Acc@1  60.00 ( 64.21)	Acc@5  84.00 ( 87.52)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 1.5986e+00 (1.5154e+00)	Acc@1  62.00 ( 64.33)	Acc@5  86.00 ( 87.44)
 * Acc@1 64.430 Acc@5 87.460
### epoch[41] execution time: 11.63607144355774
EPOCH 42
i:   0, name:       features.module.0.weight  changing lr from: 0.003968907966975204   to: 0.002921910115851916
i:   1, name:         features.module.0.bias  changing lr from: 0.004962351090667143   to: 0.003762108993438950
i:   2, name:       features.module.1.weight  changing lr from: 0.006061413890272965   to: 0.004721947630664352
i:   3, name:         features.module.1.bias  changing lr from: 0.007253926954073197   to: 0.005788610615568190
i:   4, name:       features.module.4.weight  changing lr from: 0.008528499299916231   to: 0.006950038183438644
i:   5, name:         features.module.4.bias  changing lr from: 0.009874507233725148   to: 0.008194925847024919
i:   6, name:       features.module.5.weight  changing lr from: 0.011282076495849037   to: 0.009512715508524144
i:   7, name:         features.module.5.bias  changing lr from: 0.012742059201811356   to: 0.010893579783378109
i:   8, name:       features.module.8.weight  changing lr from: 0.014246006856346367   to: 0.012328401018415557
i:   9, name:         features.module.8.bias  changing lr from: 0.015786140518892978   to: 0.013808746266624518
i:  10, name:       features.module.9.weight  changing lr from: 0.017355319022820528   to: 0.015326839286138105
i:  11, name:         features.module.9.bias  changing lr from: 0.018947005997398660   to: 0.016875530459948395
i:  12, name:      features.module.11.weight  changing lr from: 0.020555236308706884   to: 0.018448265383414576
i:  13, name:        features.module.11.bias  changing lr from: 0.022174582421190345   to: 0.020039052736777613
i:  14, name:      features.module.12.weight  changing lr from: 0.023800121083390122   to: 0.021642431947653498
i:  15, name:        features.module.12.bias  changing lr from: 0.025427400657628871   to: 0.023253441051960290
i:  16, name:      features.module.15.weight  changing lr from: 0.027052409342379624   to: 0.024867585079168204
i:  17, name:        features.module.15.bias  changing lr from: 0.028671544476100642   to: 0.026480805217495691
i:  18, name:      features.module.16.weight  changing lr from: 0.030281583061051200   to: 0.028089448955207919
i:  19, name:        features.module.16.bias  changing lr from: 0.031879653603719783   to: 0.029690241344130230
i:  20, name:      features.module.18.weight  changing lr from: 0.033463209333857019   to: 0.031280257489644928
i:  21, name:        features.module.18.bias  changing lr from: 0.035030002835682111   to: 0.032856896336681075
i:  22, name:      features.module.19.weight  changing lr from: 0.036578062101736301   to: 0.034417855792564379
i:  23, name:        features.module.19.bias  changing lr from: 0.038105668001294161   to: 0.035961109204191176
i:  24, name:      features.module.22.weight  changing lr from: 0.039611333140526950   to: 0.037484883188068363
i:  25, name:        features.module.22.bias  changing lr from: 0.041093782080144678   to: 0.038987636796651466
i:  26, name:      features.module.23.weight  changing lr from: 0.042551932867500412   to: 0.040468041992526821
i:  27, name:        features.module.23.bias  changing lr from: 0.043984879833677382   to: 0.041924965392821006
i:  28, name:      features.module.25.weight  changing lr from: 0.045391877601508056   to: 0.043357451239335626
i:  29, name:        features.module.25.bias  changing lr from: 0.046772326247463691   to: 0.044764705544923178
i:  30, name:      features.module.26.weight  changing lr from: 0.048125757558620731   to: 0.046146081363211786
i:  31, name:        features.module.26.bias  changing lr from: 0.049451822325215070   to: 0.047501065126676179
i:  32, name:            classifier.0.weight  changing lr from: 0.050750278609433597   to: 0.048829263996997802
i:  33, name:              classifier.0.bias  changing lr from: 0.052020980931891093   to: 0.050130394171460611
i:  34, name:            classifier.3.weight  changing lr from: 0.053263870318555034   to: 0.051404270089612902
i:  35, name:              classifier.3.bias  changing lr from: 0.054478965152589071   to: 0.052650794485451437
i:  36, name:            classifier.6.weight  changing lr from: 0.055666352777588760   to: 0.053869949231824289
i:  37, name:              classifier.6.bias  changing lr from: 0.056826181800892453   to: 0.055061786925505618



# Switched to train mode...
Epoch: [42][  0/391]	Time  0.183 ( 0.183)	Data  0.149 ( 0.149)	Loss 4.0112e-01 (4.0112e-01)	Acc@1  87.50 ( 87.50)	Acc@5  97.66 ( 97.66)
Epoch: [42][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 3.4082e-01 (4.5710e-01)	Acc@1  90.62 ( 85.37)	Acc@5  97.66 ( 98.51)
Epoch: [42][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 4.7949e-01 (4.6273e-01)	Acc@1  86.72 ( 85.83)	Acc@5  99.22 ( 98.18)
Epoch: [42][ 30/391]	Time  0.037 ( 0.030)	Data  0.001 ( 0.007)	Loss 5.0195e-01 (4.7533e-01)	Acc@1  85.16 ( 85.23)	Acc@5  98.44 ( 98.26)
Epoch: [42][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 5.3906e-01 (4.7626e-01)	Acc@1  83.59 ( 85.35)	Acc@5  98.44 ( 98.27)
Epoch: [42][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.1650e-01 (4.8101e-01)	Acc@1  86.72 ( 85.06)	Acc@5  97.66 ( 98.19)
Epoch: [42][ 60/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 4.1431e-01 (4.8032e-01)	Acc@1  89.84 ( 85.00)	Acc@5  97.66 ( 98.14)
Epoch: [42][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.2979e-01 (4.8015e-01)	Acc@1  84.38 ( 85.01)	Acc@5  97.66 ( 98.11)
Epoch: [42][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.6006e-01 (4.7075e-01)	Acc@1  82.81 ( 85.37)	Acc@5  97.66 ( 98.11)
Epoch: [42][ 90/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.4019e-01 (4.6423e-01)	Acc@1  83.59 ( 85.57)	Acc@5  99.22 ( 98.16)
Epoch: [42][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.8740e-01 (4.6507e-01)	Acc@1  85.94 ( 85.64)	Acc@5  96.88 ( 98.18)
Epoch: [42][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3423e-01 (4.6575e-01)	Acc@1  88.28 ( 85.67)	Acc@5  99.22 ( 98.16)
Epoch: [42][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8740e-01 (4.6555e-01)	Acc@1  82.81 ( 85.69)	Acc@5  96.09 ( 98.13)
Epoch: [42][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9600e-01 (4.6699e-01)	Acc@1  85.16 ( 85.68)	Acc@5  98.44 ( 98.08)
Epoch: [42][140/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.1318e-01 (4.6499e-01)	Acc@1  85.94 ( 85.79)	Acc@5  95.31 ( 98.09)
Epoch: [42][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6533e-01 (4.6299e-01)	Acc@1  87.50 ( 85.86)	Acc@5  97.66 ( 98.12)
Epoch: [42][160/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.6631e-01 (4.6389e-01)	Acc@1  85.94 ( 85.83)	Acc@5 100.00 ( 98.11)
Epoch: [42][170/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4502e-01 (4.6652e-01)	Acc@1  78.91 ( 85.73)	Acc@5  99.22 ( 98.10)
Epoch: [42][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9487e-01 (4.6733e-01)	Acc@1  87.50 ( 85.75)	Acc@5  96.09 ( 98.11)
Epoch: [42][190/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7119e-01 (4.7069e-01)	Acc@1  82.03 ( 85.63)	Acc@5  99.22 ( 98.11)
Epoch: [42][200/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2061e-01 (4.7282e-01)	Acc@1  80.47 ( 85.55)	Acc@5  97.66 ( 98.10)
Epoch: [42][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7329e-01 (4.7465e-01)	Acc@1  87.50 ( 85.54)	Acc@5  99.22 ( 98.07)
Epoch: [42][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0391e-01 (4.7252e-01)	Acc@1  87.50 ( 85.61)	Acc@5  97.66 ( 98.08)
Epoch: [42][230/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.7461e-01 (4.7080e-01)	Acc@1  85.94 ( 85.70)	Acc@5  98.44 ( 98.10)
Epoch: [42][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1211e-01 (4.7040e-01)	Acc@1  87.50 ( 85.71)	Acc@5  98.44 ( 98.09)
Epoch: [42][250/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.2065e-01 (4.7017e-01)	Acc@1  87.50 ( 85.73)	Acc@5 100.00 ( 98.09)
Epoch: [42][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7275e-01 (4.7140e-01)	Acc@1  82.81 ( 85.72)	Acc@5  95.31 ( 98.06)
Epoch: [42][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6304e-01 (4.7164e-01)	Acc@1  88.28 ( 85.72)	Acc@5  99.22 ( 98.07)
Epoch: [42][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4067e-01 (4.7075e-01)	Acc@1  86.72 ( 85.77)	Acc@5  98.44 ( 98.08)
Epoch: [42][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.2051e-01 (4.7274e-01)	Acc@1  88.28 ( 85.74)	Acc@5  96.09 ( 98.06)
Epoch: [42][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.8438e-01 (4.7253e-01)	Acc@1  85.16 ( 85.74)	Acc@5  99.22 ( 98.06)
Epoch: [42][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2139e-01 (4.7368e-01)	Acc@1  87.50 ( 85.70)	Acc@5  99.22 ( 98.05)
Epoch: [42][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3057e-01 (4.7482e-01)	Acc@1  89.06 ( 85.65)	Acc@5  98.44 ( 98.02)
Epoch: [42][330/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.2861e-01 (4.7374e-01)	Acc@1  92.19 ( 85.68)	Acc@5  98.44 ( 98.05)
Epoch: [42][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.1123e-01 (4.7470e-01)	Acc@1  87.50 ( 85.65)	Acc@5  98.44 ( 98.04)
Epoch: [42][350/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6392e-01 (4.7381e-01)	Acc@1  91.41 ( 85.66)	Acc@5 100.00 ( 98.05)
Epoch: [42][360/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.3984e-01 (4.7425e-01)	Acc@1  89.06 ( 85.63)	Acc@5 100.00 ( 98.05)
Epoch: [42][370/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4399e-01 (4.7443e-01)	Acc@1  88.28 ( 85.63)	Acc@5  99.22 ( 98.04)
Epoch: [42][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.8535e-01 (4.7608e-01)	Acc@1  82.03 ( 85.58)	Acc@5  96.88 ( 98.03)
Epoch: [42][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.1523e-01 (4.7686e-01)	Acc@1  81.25 ( 85.54)	Acc@5  98.75 ( 98.03)
## e[42] optimizer.zero_grad (sum) time: 0.11436653137207031
## e[42]       loss.backward (sum) time: 2.271949291229248
## e[42]      optimizer.step (sum) time: 0.9662485122680664
## epoch[42] training(only) time: 10.102135181427002
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.5234e+00 (1.5234e+00)	Acc@1  64.00 ( 64.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 1.7041e+00 (1.5367e+00)	Acc@1  64.00 ( 65.55)	Acc@5  89.00 ( 87.55)
Test: [ 20/100]	Time  0.009 ( 0.020)	Loss 1.6924e+00 (1.5682e+00)	Acc@1  67.00 ( 64.86)	Acc@5  85.00 ( 87.19)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.7891e+00 (1.5526e+00)	Acc@1  59.00 ( 64.65)	Acc@5  85.00 ( 87.32)
Test: [ 40/100]	Time  0.019 ( 0.017)	Loss 1.3555e+00 (1.5388e+00)	Acc@1  66.00 ( 64.61)	Acc@5  92.00 ( 87.85)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.2949e+00 (1.5464e+00)	Acc@1  67.00 ( 64.27)	Acc@5  91.00 ( 87.59)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.5029e+00 (1.5306e+00)	Acc@1  63.00 ( 64.44)	Acc@5  86.00 ( 87.72)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 1.6895e+00 (1.5369e+00)	Acc@1  60.00 ( 64.27)	Acc@5  90.00 ( 87.83)
Test: [ 80/100]	Time  0.017 ( 0.015)	Loss 1.7949e+00 (1.5445e+00)	Acc@1  61.00 ( 64.12)	Acc@5  82.00 ( 87.63)
Test: [ 90/100]	Time  0.022 ( 0.015)	Loss 2.0156e+00 (1.5372e+00)	Acc@1  56.00 ( 64.34)	Acc@5  84.00 ( 87.76)
 * Acc@1 64.290 Acc@5 87.880
### epoch[42] execution time: 11.665751934051514
EPOCH 43
i:   0, name:       features.module.0.weight  changing lr from: 0.002921910115851916   to: 0.002098297149271610
i:   1, name:         features.module.0.bias  changing lr from: 0.003762108993438950   to: 0.002772883442657691
i:   2, name:       features.module.1.weight  changing lr from: 0.004721947630664352   to: 0.003581288312767044
i:   3, name:         features.module.1.bias  changing lr from: 0.005788610615568190   to: 0.004510123318451336
i:   4, name:       features.module.4.weight  changing lr from: 0.006950038183438644   to: 0.005546719077229817
i:   5, name:         features.module.4.bias  changing lr from: 0.008194925847024919   to: 0.006679137385046446
i:   6, name:       features.module.5.weight  changing lr from: 0.009512715508524144   to: 0.007896172856573198
i:   7, name:         features.module.5.bias  changing lr from: 0.010893579783378109   to: 0.009187346044641508
i:   8, name:       features.module.8.weight  changing lr from: 0.012328401018415557   to: 0.010542889732110140
i:   9, name:         features.module.8.bias  changing lr from: 0.013808746266624518   to: 0.011953729851047618
i:  10, name:       features.module.9.weight  changing lr from: 0.015326839286138105   to: 0.013411462271394382
i:  11, name:         features.module.9.bias  changing lr from: 0.016875530459948395   to: 0.014908326512765645
i:  12, name:      features.module.11.weight  changing lr from: 0.018448265383414576   to: 0.016437177267026915
i:  13, name:        features.module.11.bias  changing lr from: 0.020039052736777613   to: 0.017991454473872015
i:  14, name:      features.module.12.weight  changing lr from: 0.021642431947653498   to: 0.019565152564981602
i:  15, name:        features.module.12.bias  changing lr from: 0.023253441051960290   to: 0.021152789382591609
i:  16, name:      features.module.15.weight  changing lr from: 0.024867585079168204   to: 0.022749375183683685
i:  17, name:        features.module.15.bias  changing lr from: 0.026480805217495691   to: 0.024350382059848730
i:  18, name:      features.module.16.weight  changing lr from: 0.028089448955207919   to: 0.025951714033614681
i:  19, name:        features.module.16.bias  changing lr from: 0.029690241344130230   to: 0.027549678033235315
i:  20, name:      features.module.18.weight  changing lr from: 0.031280257489644928   to: 0.029140955898311185
i:  21, name:        features.module.18.bias  changing lr from: 0.032856896336681075   to: 0.030722577526967165
i:  22, name:      features.module.19.weight  changing lr from: 0.034417855792564379   to: 0.032291895240597689
i:  23, name:        features.module.19.bias  changing lr from: 0.035961109204191176   to: 0.033846559413459126
i:  24, name:      features.module.22.weight  changing lr from: 0.037484883188068363   to: 0.035384495390806452
i:  25, name:        features.module.22.bias  changing lr from: 0.038987636796651466   to: 0.036903881700101573
i:  26, name:      features.module.23.weight  changing lr from: 0.040468041992526821   to: 0.038403129544410629
i:  27, name:        features.module.23.bias  changing lr from: 0.041924965392821006   to: 0.039880863554895639
i:  28, name:      features.module.25.weight  changing lr from: 0.043357451239335626   to: 0.041335903769794874
i:  29, name:        features.module.25.bias  changing lr from: 0.044764705544923178   to: 0.042767248800047175
i:  30, name:      features.module.26.weight  changing lr from: 0.046146081363211786   to: 0.044174060136379097
i:  31, name:        features.module.26.bias  changing lr from: 0.047501065126676179   to: 0.045555647548919512
i:  32, name:            classifier.0.weight  changing lr from: 0.048829263996997802   to: 0.046911455527958001
i:  33, name:              classifier.0.bias  changing lr from: 0.050130394171460611   to: 0.048241050713090358
i:  34, name:            classifier.3.weight  changing lr from: 0.051404270089612902   to: 0.049544110257490052
i:  35, name:              classifier.3.bias  changing lr from: 0.052650794485451437   to: 0.050820411074244745
i:  36, name:            classifier.6.weight  changing lr from: 0.053869949231824289   to: 0.052069819912452901
i:  37, name:              classifier.6.bias  changing lr from: 0.055061786925505618   to: 0.053292284211968161



# Switched to train mode...
Epoch: [43][  0/391]	Time  0.180 ( 0.180)	Data  0.151 ( 0.151)	Loss 3.5205e-01 (3.5205e-01)	Acc@1  87.50 ( 87.50)	Acc@5  98.44 ( 98.44)
Epoch: [43][ 10/391]	Time  0.030 ( 0.040)	Data  0.000 ( 0.015)	Loss 5.4199e-01 (4.3541e-01)	Acc@1  82.81 ( 86.72)	Acc@5  98.44 ( 98.22)
Epoch: [43][ 20/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.009)	Loss 3.6792e-01 (4.1233e-01)	Acc@1  89.84 ( 87.43)	Acc@5  98.44 ( 98.40)
Epoch: [43][ 30/391]	Time  0.043 ( 0.030)	Data  0.001 ( 0.007)	Loss 4.9854e-01 (4.1556e-01)	Acc@1  82.81 ( 87.35)	Acc@5  99.22 ( 98.29)
Epoch: [43][ 40/391]	Time  0.020 ( 0.029)	Data  0.000 ( 0.006)	Loss 3.7915e-01 (4.1086e-01)	Acc@1  88.28 ( 87.50)	Acc@5  99.22 ( 98.42)
Epoch: [43][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.2549e-01 (4.1851e-01)	Acc@1  79.69 ( 87.38)	Acc@5  96.09 ( 98.36)
Epoch: [43][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.4131e-01 (4.1777e-01)	Acc@1  88.28 ( 87.31)	Acc@5  99.22 ( 98.34)
Epoch: [43][ 70/391]	Time  0.026 ( 0.027)	Data  0.004 ( 0.004)	Loss 4.8315e-01 (4.2475e-01)	Acc@1  85.16 ( 87.08)	Acc@5  96.09 ( 98.28)
Epoch: [43][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.3188e-01 (4.2347e-01)	Acc@1  86.72 ( 87.02)	Acc@5  98.44 ( 98.34)
Epoch: [43][ 90/391]	Time  0.029 ( 0.027)	Data  0.003 ( 0.004)	Loss 3.5913e-01 (4.2041e-01)	Acc@1  89.06 ( 87.15)	Acc@5  98.44 ( 98.34)
Epoch: [43][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.5703e-01 (4.1615e-01)	Acc@1  85.16 ( 87.28)	Acc@5  98.44 ( 98.38)
Epoch: [43][110/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.3384e-01 (4.2024e-01)	Acc@1  89.84 ( 87.30)	Acc@5  97.66 ( 98.37)
Epoch: [43][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0488e-01 (4.2353e-01)	Acc@1  83.59 ( 87.13)	Acc@5 100.00 ( 98.37)
Epoch: [43][130/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0645e-01 (4.2071e-01)	Acc@1  82.81 ( 87.22)	Acc@5  96.09 ( 98.35)
Epoch: [43][140/391]	Time  0.035 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.1333e-01 (4.2214e-01)	Acc@1  86.72 ( 87.15)	Acc@5  98.44 ( 98.35)
Epoch: [43][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0156e-01 (4.2144e-01)	Acc@1  86.72 ( 87.16)	Acc@5  98.44 ( 98.38)
Epoch: [43][160/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3286e-01 (4.2122e-01)	Acc@1  88.28 ( 87.19)	Acc@5  96.88 ( 98.37)
Epoch: [43][170/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3018e-01 (4.1952e-01)	Acc@1  86.72 ( 87.24)	Acc@5  98.44 ( 98.37)
Epoch: [43][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3203e-01 (4.2020e-01)	Acc@1  89.06 ( 87.22)	Acc@5  99.22 ( 98.36)
Epoch: [43][190/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4082e-01 (4.2248e-01)	Acc@1  89.84 ( 87.17)	Acc@5  97.66 ( 98.33)
Epoch: [43][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7314e-01 (4.2093e-01)	Acc@1  85.94 ( 87.18)	Acc@5  97.66 ( 98.35)
Epoch: [43][210/391]	Time  0.029 ( 0.026)	Data  0.005 ( 0.003)	Loss 5.1855e-01 (4.2149e-01)	Acc@1  83.59 ( 87.16)	Acc@5  96.88 ( 98.36)
Epoch: [43][220/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.2153e-01 (4.2268e-01)	Acc@1  90.62 ( 87.11)	Acc@5 100.00 ( 98.36)
Epoch: [43][230/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.7500e-01 (4.2373e-01)	Acc@1  89.06 ( 87.08)	Acc@5  98.44 ( 98.33)
Epoch: [43][240/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0786e-01 (4.2394e-01)	Acc@1  92.19 ( 87.14)	Acc@5  97.66 ( 98.30)
Epoch: [43][250/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.9414e-01 (4.2568e-01)	Acc@1  86.72 ( 87.07)	Acc@5  98.44 ( 98.29)
Epoch: [43][260/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.002)	Loss 5.4346e-01 (4.2673e-01)	Acc@1  83.59 ( 87.00)	Acc@5  97.66 ( 98.30)
Epoch: [43][270/391]	Time  0.031 ( 0.026)	Data  0.010 ( 0.002)	Loss 4.5288e-01 (4.2907e-01)	Acc@1  83.59 ( 86.96)	Acc@5  97.66 ( 98.28)
Epoch: [43][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.6924e-01 (4.2990e-01)	Acc@1  86.72 ( 86.90)	Acc@5  95.31 ( 98.28)
Epoch: [43][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.0503e-01 (4.3029e-01)	Acc@1  87.50 ( 86.87)	Acc@5  98.44 ( 98.28)
Epoch: [43][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.7471e-01 (4.3269e-01)	Acc@1  78.91 ( 86.75)	Acc@5  97.66 ( 98.27)
Epoch: [43][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3716e-01 (4.3479e-01)	Acc@1  88.28 ( 86.69)	Acc@5  99.22 ( 98.25)
Epoch: [43][320/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.3960e-01 (4.3521e-01)	Acc@1  93.75 ( 86.72)	Acc@5  98.44 ( 98.26)
Epoch: [43][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7061e-01 (4.3691e-01)	Acc@1  91.41 ( 86.67)	Acc@5  99.22 ( 98.26)
Epoch: [43][340/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.8755e-01 (4.3750e-01)	Acc@1  88.28 ( 86.65)	Acc@5  97.66 ( 98.27)
Epoch: [43][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.8242e-01 (4.3669e-01)	Acc@1  87.50 ( 86.67)	Acc@5  98.44 ( 98.27)
Epoch: [43][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.6240e-01 (4.3771e-01)	Acc@1  87.50 ( 86.65)	Acc@5  97.66 ( 98.27)
Epoch: [43][370/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2676e-01 (4.3731e-01)	Acc@1  85.16 ( 86.66)	Acc@5  97.66 ( 98.26)
Epoch: [43][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5620e-01 (4.3832e-01)	Acc@1  89.84 ( 86.63)	Acc@5  99.22 ( 98.27)
Epoch: [43][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.6406e-01 (4.3741e-01)	Acc@1  78.75 ( 86.65)	Acc@5  95.00 ( 98.27)
## e[43] optimizer.zero_grad (sum) time: 0.11542773246765137
## e[43]       loss.backward (sum) time: 2.2742459774017334
## e[43]      optimizer.step (sum) time: 0.9578485488891602
## epoch[43] training(only) time: 10.116183519363403
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.4951e+00 (1.4951e+00)	Acc@1  63.00 ( 63.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.5547e+00 (1.6100e+00)	Acc@1  60.00 ( 64.73)	Acc@5  90.00 ( 87.00)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.9072e+00 (1.6154e+00)	Acc@1  65.00 ( 65.05)	Acc@5  82.00 ( 87.24)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.8887e+00 (1.5854e+00)	Acc@1  54.00 ( 65.06)	Acc@5  86.00 ( 87.32)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.3398e+00 (1.5673e+00)	Acc@1  65.00 ( 65.05)	Acc@5  93.00 ( 87.61)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.5205e+00 (1.5830e+00)	Acc@1  65.00 ( 64.86)	Acc@5  88.00 ( 87.63)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6260e+00 (1.5693e+00)	Acc@1  62.00 ( 64.90)	Acc@5  84.00 ( 87.62)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 1.7324e+00 (1.5615e+00)	Acc@1  63.00 ( 64.96)	Acc@5  89.00 ( 87.75)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.7812e+00 (1.5729e+00)	Acc@1  65.00 ( 64.64)	Acc@5  83.00 ( 87.51)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.0156e+00 (1.5666e+00)	Acc@1  54.00 ( 64.75)	Acc@5  79.00 ( 87.56)
 * Acc@1 64.770 Acc@5 87.710
### epoch[43] execution time: 11.695317506790161
EPOCH 44
i:   0, name:       features.module.0.weight  changing lr from: 0.002098297149271610   to: 0.001501895814259612
i:   1, name:         features.module.0.bias  changing lr from: 0.002772883442657691   to: 0.001999093411205804
i:   2, name:       features.module.1.weight  changing lr from: 0.003581288312767044   to: 0.002644336120986256
i:   3, name:         features.module.1.bias  changing lr from: 0.004510123318451336   to: 0.003423748212818200
i:   4, name:       features.module.4.weight  changing lr from: 0.005546719077229817   to: 0.004324121557733701
i:   5, name:         features.module.4.bias  changing lr from: 0.006679137385046446   to: 0.005332941987851585
i:   6, name:       features.module.5.weight  changing lr from: 0.007896172856573198   to: 0.006438403071852388
i:   7, name:         features.module.5.bias  changing lr from: 0.009187346044641508   to: 0.007629409494975253
i:   8, name:       features.module.8.weight  changing lr from: 0.010542889732110140   to: 0.008895571951675535
i:   9, name:         features.module.8.bias  changing lr from: 0.011953729851047618   to: 0.010227195204489836
i:  10, name:       features.module.9.weight  changing lr from: 0.013411462271394382   to: 0.011615260733275779
i:  11, name:         features.module.9.bias  changing lr from: 0.014908326512765645   to: 0.013051405193874367
i:  12, name:      features.module.11.weight  changing lr from: 0.016437177267026915   to: 0.014527895723049154
i:  13, name:        features.module.11.bias  changing lr from: 0.017991454473872015   to: 0.016037602965719017
i:  14, name:      features.module.12.weight  changing lr from: 0.019565152564981602   to: 0.017573972559321110
i:  15, name:        features.module.12.bias  changing lr from: 0.021152789382591609   to: 0.019130995686882552
i:  16, name:      features.module.15.weight  changing lr from: 0.022749375183683685   to: 0.020703179203320640
i:  17, name:        features.module.15.bias  changing lr from: 0.024350382059848730   to: 0.022285515746968854
i:  18, name:      features.module.16.weight  changing lr from: 0.025951714033614681   to: 0.023873454168765781
i:  19, name:        features.module.16.bias  changing lr from: 0.027549678033235315   to: 0.025462870543470312
i:  20, name:      features.module.18.weight  changing lr from: 0.029140955898311185   to: 0.027050039969326534
i:  21, name:        features.module.18.bias  changing lr from: 0.030722577526967165   to: 0.028631609313547197
i:  22, name:      features.module.19.weight  changing lr from: 0.032291895240597689   to: 0.030204571019695627
i:  23, name:        features.module.19.bias  changing lr from: 0.033846559413459126   to: 0.031766238058509574
i:  24, name:      features.module.22.weight  changing lr from: 0.035384495390806452   to: 0.033314220075017068
i:  25, name:        features.module.22.bias  changing lr from: 0.036903881700101573   to: 0.034846400761148075
i:  26, name:      features.module.23.weight  changing lr from: 0.038403129544410629   to: 0.036360916463724850
i:  27, name:        features.module.23.bias  changing lr from: 0.039880863554895639   to: 0.037856136022099952
i:  28, name:      features.module.25.weight  changing lr from: 0.041335903769794874   to: 0.039330641817249615
i:  29, name:        features.module.25.bias  changing lr from: 0.042767248800047175   to: 0.040783212004343011
i:  30, name:      features.module.26.weight  changing lr from: 0.044174060136379097   to: 0.042212803893277966
i:  31, name:        features.module.26.bias  changing lr from: 0.045555647548919512   to: 0.043618538436037434
i:  32, name:            classifier.0.weight  changing lr from: 0.046911455527958001   to: 0.044999685775666778
i:  33, name:              classifier.0.bias  changing lr from: 0.048241050713090358   to: 0.046355651808930615
i:  34, name:            classifier.3.weight  changing lr from: 0.049544110257490052   to: 0.047685965713046508
i:  35, name:              classifier.3.bias  changing lr from: 0.050820411074244745   to: 0.048990268386116448
i:  36, name:            classifier.6.weight  changing lr from: 0.052069819912452901   to: 0.050268301750815647
i:  37, name:              classifier.6.bias  changing lr from: 0.053292284211968161   to: 0.051519898871408468



# Switched to train mode...
Epoch: [44][  0/391]	Time  0.175 ( 0.175)	Data  0.149 ( 0.149)	Loss 3.9624e-01 (3.9624e-01)	Acc@1  85.94 ( 85.94)	Acc@5  98.44 ( 98.44)
Epoch: [44][ 10/391]	Time  0.021 ( 0.038)	Data  0.002 ( 0.015)	Loss 3.9771e-01 (4.2529e-01)	Acc@1  88.28 ( 86.22)	Acc@5  98.44 ( 98.65)
Epoch: [44][ 20/391]	Time  0.042 ( 0.033)	Data  0.001 ( 0.009)	Loss 3.0200e-01 (4.0380e-01)	Acc@1  89.06 ( 87.24)	Acc@5  99.22 ( 98.85)
Epoch: [44][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 3.6255e-01 (3.8360e-01)	Acc@1  87.50 ( 87.85)	Acc@5  99.22 ( 98.77)
Epoch: [44][ 40/391]	Time  0.021 ( 0.029)	Data  0.000 ( 0.005)	Loss 2.7197e-01 (3.8036e-01)	Acc@1  91.41 ( 88.21)	Acc@5  99.22 ( 98.84)
Epoch: [44][ 50/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.2637e-01 (3.7527e-01)	Acc@1  87.50 ( 88.31)	Acc@5  95.31 ( 98.77)
Epoch: [44][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.6582e-01 (3.7591e-01)	Acc@1  88.28 ( 88.37)	Acc@5  96.88 ( 98.77)
Epoch: [44][ 70/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.2056e-01 (3.7781e-01)	Acc@1  92.19 ( 88.30)	Acc@5  98.44 ( 98.72)
Epoch: [44][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.4932e-01 (3.9005e-01)	Acc@1  83.59 ( 88.00)	Acc@5  96.88 ( 98.60)
Epoch: [44][ 90/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.004)	Loss 4.0845e-01 (3.9715e-01)	Acc@1  87.50 ( 87.74)	Acc@5  98.44 ( 98.54)
Epoch: [44][100/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8735e-01 (3.9787e-01)	Acc@1  90.62 ( 87.73)	Acc@5  99.22 ( 98.53)
Epoch: [44][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7485e-01 (3.9505e-01)	Acc@1  88.28 ( 87.84)	Acc@5  98.44 ( 98.51)
Epoch: [44][120/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9370e-01 (3.9214e-01)	Acc@1  91.41 ( 87.92)	Acc@5  99.22 ( 98.54)
Epoch: [44][130/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1299e-01 (3.9216e-01)	Acc@1  90.62 ( 87.92)	Acc@5 100.00 ( 98.54)
Epoch: [44][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9932e-01 (3.9117e-01)	Acc@1  92.19 ( 87.98)	Acc@5  99.22 ( 98.56)
Epoch: [44][150/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.6631e-01 (3.9115e-01)	Acc@1  85.16 ( 87.97)	Acc@5  99.22 ( 98.58)
Epoch: [44][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8647e-01 (3.8978e-01)	Acc@1  89.84 ( 88.01)	Acc@5  98.44 ( 98.60)
Epoch: [44][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0542e-01 (3.8757e-01)	Acc@1  89.84 ( 88.08)	Acc@5 100.00 ( 98.63)
Epoch: [44][180/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4834e-01 (3.8759e-01)	Acc@1  85.16 ( 88.08)	Acc@5  98.44 ( 98.65)
Epoch: [44][190/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5288e-01 (3.8640e-01)	Acc@1  86.72 ( 88.15)	Acc@5  97.66 ( 98.65)
Epoch: [44][200/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.7461e-01 (3.8632e-01)	Acc@1  88.28 ( 88.14)	Acc@5  96.88 ( 98.67)
Epoch: [44][210/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.3506e-01 (3.8925e-01)	Acc@1  87.50 ( 88.03)	Acc@5  96.09 ( 98.63)
Epoch: [44][220/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.4448e-01 (3.8882e-01)	Acc@1  89.06 ( 88.03)	Acc@5 100.00 ( 98.62)
Epoch: [44][230/391]	Time  0.039 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.4937e-01 (3.9037e-01)	Acc@1  87.50 ( 88.00)	Acc@5  99.22 ( 98.62)
Epoch: [44][240/391]	Time  0.030 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.9575e-01 (3.9100e-01)	Acc@1  86.72 ( 87.98)	Acc@5  99.22 ( 98.63)
Epoch: [44][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4375e-01 (3.9244e-01)	Acc@1  90.62 ( 87.96)	Acc@5  98.44 ( 98.64)
Epoch: [44][260/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.6797e-01 (3.9237e-01)	Acc@1  78.91 ( 87.96)	Acc@5  95.31 ( 98.63)
Epoch: [44][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6401e-01 (3.9309e-01)	Acc@1  89.84 ( 87.92)	Acc@5  98.44 ( 98.62)
Epoch: [44][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7842e-01 (3.9415e-01)	Acc@1  85.16 ( 87.88)	Acc@5 100.00 ( 98.61)
Epoch: [44][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7964e-01 (3.9417e-01)	Acc@1  86.72 ( 87.90)	Acc@5  99.22 ( 98.60)
Epoch: [44][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4790e-01 (3.9449e-01)	Acc@1  88.28 ( 87.87)	Acc@5  99.22 ( 98.59)
Epoch: [44][310/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0371e-01 (3.9362e-01)	Acc@1  90.62 ( 87.89)	Acc@5 100.00 ( 98.61)
Epoch: [44][320/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.8232e-01 (3.9363e-01)	Acc@1  89.84 ( 87.89)	Acc@5  98.44 ( 98.60)
Epoch: [44][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (3.9508e-01)	Acc@1  83.59 ( 87.83)	Acc@5  98.44 ( 98.60)
Epoch: [44][340/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.1064e-01 (3.9594e-01)	Acc@1  88.28 ( 87.80)	Acc@5  96.88 ( 98.59)
Epoch: [44][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.9668e-01 (3.9671e-01)	Acc@1  85.16 ( 87.76)	Acc@5  96.09 ( 98.59)
Epoch: [44][360/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.002)	Loss 3.8379e-01 (3.9615e-01)	Acc@1  90.62 ( 87.79)	Acc@5  97.66 ( 98.59)
Epoch: [44][370/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.1270e-01 (3.9780e-01)	Acc@1  83.59 ( 87.77)	Acc@5  97.66 ( 98.57)
Epoch: [44][380/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.4297e-01 (3.9896e-01)	Acc@1  76.56 ( 87.72)	Acc@5  97.66 ( 98.56)
Epoch: [44][390/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.1375e-01 (3.9904e-01)	Acc@1  92.50 ( 87.72)	Acc@5 100.00 ( 98.56)
## e[44] optimizer.zero_grad (sum) time: 0.11439895629882812
## e[44]       loss.backward (sum) time: 2.2533137798309326
## e[44]      optimizer.step (sum) time: 0.9552476406097412
## epoch[44] training(only) time: 10.128923177719116
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.7344e+00 (1.7344e+00)	Acc@1  66.00 ( 66.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.6484e+00 (1.5980e+00)	Acc@1  63.00 ( 64.55)	Acc@5  88.00 ( 87.00)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 1.5811e+00 (1.5570e+00)	Acc@1  67.00 ( 65.33)	Acc@5  86.00 ( 87.33)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.8389e+00 (1.5738e+00)	Acc@1  60.00 ( 64.87)	Acc@5  85.00 ( 87.29)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4580e+00 (1.5594e+00)	Acc@1  67.00 ( 65.05)	Acc@5  93.00 ( 87.73)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.3896e+00 (1.5876e+00)	Acc@1  70.00 ( 64.98)	Acc@5  89.00 ( 87.61)
Test: [ 60/100]	Time  0.017 ( 0.016)	Loss 1.4092e+00 (1.5692e+00)	Acc@1  66.00 ( 65.13)	Acc@5  88.00 ( 87.77)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.7031e+00 (1.5688e+00)	Acc@1  62.00 ( 65.14)	Acc@5  90.00 ( 87.83)
Test: [ 80/100]	Time  0.029 ( 0.015)	Loss 1.5664e+00 (1.5805e+00)	Acc@1  68.00 ( 64.99)	Acc@5  85.00 ( 87.63)
Test: [ 90/100]	Time  0.022 ( 0.015)	Loss 1.7656e+00 (1.5687e+00)	Acc@1  60.00 ( 65.14)	Acc@5  88.00 ( 87.82)
 * Acc@1 65.020 Acc@5 87.930
### epoch[44] execution time: 11.719177007675171
EPOCH 45
i:   0, name:       features.module.0.weight  changing lr from: 0.001501895814259612   to: 0.001135477166065490
i:   1, name:         features.module.0.bias  changing lr from: 0.001999093411205804   to: 0.001444195499211558
i:   2, name:       features.module.1.weight  changing lr from: 0.002644336120986256   to: 0.001915116129633498
i:   3, name:         features.module.1.bias  changing lr from: 0.003423748212818200   to: 0.002533974574984692
i:   4, name:       features.module.4.weight  changing lr from: 0.004324121557733701   to: 0.003287106654982652
i:   5, name:         features.module.4.bias  changing lr from: 0.005332941987851585   to: 0.004161490850379633
i:   6, name:       features.module.5.weight  changing lr from: 0.006438403071852388   to: 0.005144775846656346
i:   7, name:         features.module.5.bias  changing lr from: 0.007629409494975253   to: 0.006225295677228476
i:   8, name:       features.module.8.weight  changing lr from: 0.008895571951675535   to: 0.007392074589608787
i:   9, name:         features.module.8.bias  changing lr from: 0.010227195204489836   to: 0.008634823489961479
i:  10, name:       features.module.9.weight  changing lr from: 0.011615260733275779   to: 0.009943929577393311
i:  11, name:         features.module.9.bias  changing lr from: 0.013051405193874367   to: 0.011310440558916501
i:  12, name:      features.module.11.weight  changing lr from: 0.014527895723049154   to: 0.012726044638490159
i:  13, name:        features.module.11.bias  changing lr from: 0.016037602965719017   to: 0.014183047297721761
i:  14, name:      features.module.12.weight  changing lr from: 0.017573972559321110   to: 0.015674345730260480
i:  15, name:        features.module.12.bias  changing lr from: 0.019130995686882552   to: 0.017193401655085035
i:  16, name:      features.module.15.weight  changing lr from: 0.020703179203320640   to: 0.018734213114172005
i:  17, name:        features.module.15.bias  changing lr from: 0.022285515746968854   to: 0.020291285755816704
i:  18, name:      features.module.16.weight  changing lr from: 0.023873454168765781   to: 0.021859604014615466
i:  19, name:        features.module.16.bias  changing lr from: 0.025462870543470312   to: 0.023434602521316079
i:  20, name:      features.module.18.weight  changing lr from: 0.027050039969326534   to: 0.025012138009019011
i:  21, name:        features.module.18.bias  changing lr from: 0.028631609313547197   to: 0.026588461925268203
i:  22, name:      features.module.19.weight  changing lr from: 0.030204571019695627   to: 0.028160193911237426
i:  23, name:        features.module.19.bias  changing lr from: 0.031766238058509574   to: 0.029724296268414604
i:  24, name:      features.module.22.weight  changing lr from: 0.033314220075017068   to: 0.031278049498949315
i:  25, name:        features.module.22.bias  changing lr from: 0.034846400761148075   to: 0.032819028977291054
i:  26, name:      features.module.23.weight  changing lr from: 0.036360916463724850   to: 0.034345082787130402
i:  27, name:        features.module.23.bias  changing lr from: 0.037856136022099952   to: 0.035854310738282678
i:  28, name:      features.module.25.weight  changing lr from: 0.039330641817249615   to: 0.037345044562414768
i:  29, name:        features.module.25.bias  changing lr from: 0.040783212004343011   to: 0.038815829273880359
i:  30, name:      features.module.26.weight  changing lr from: 0.042212803893277966   to: 0.040265405671932764
i:  31, name:        features.module.26.bias  changing lr from: 0.043618538436037434   to: 0.041692693952820374
i:  32, name:            classifier.0.weight  changing lr from: 0.044999685775666778   to: 0.043096778394385832
i:  33, name:              classifier.0.bias  changing lr from: 0.046355651808930615   to: 0.044476893071481838
i:  34, name:            classifier.3.weight  changing lr from: 0.047685965713046508   to: 0.045832408557518654
i:  35, name:              classifier.3.bias  changing lr from: 0.048990268386116448   to: 0.047162819565546001
i:  36, name:            classifier.6.weight  changing lr from: 0.050268301750815647   to: 0.048467733481251309
i:  37, name:              classifier.6.bias  changing lr from: 0.051519898871408468   to: 0.049746859739958571



# Switched to train mode...
Epoch: [45][  0/391]	Time  0.180 ( 0.180)	Data  0.155 ( 0.155)	Loss 5.7324e-01 (5.7324e-01)	Acc@1  87.50 ( 87.50)	Acc@5  97.66 ( 97.66)
Epoch: [45][ 10/391]	Time  0.024 ( 0.041)	Data  0.001 ( 0.016)	Loss 3.6035e-01 (3.7563e-01)	Acc@1  90.62 ( 89.13)	Acc@5  99.22 ( 99.01)
Epoch: [45][ 20/391]	Time  0.022 ( 0.033)	Data  0.001 ( 0.009)	Loss 3.4326e-01 (3.7331e-01)	Acc@1  89.06 ( 89.03)	Acc@5  99.22 ( 98.96)
Epoch: [45][ 30/391]	Time  0.033 ( 0.031)	Data  0.001 ( 0.007)	Loss 4.2896e-01 (3.6536e-01)	Acc@1  89.06 ( 89.34)	Acc@5  98.44 ( 98.69)
Epoch: [45][ 40/391]	Time  0.034 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.2446e-01 (3.6049e-01)	Acc@1  89.84 ( 89.54)	Acc@5  98.44 ( 98.70)
Epoch: [45][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.6172e-01 (3.5370e-01)	Acc@1  91.41 ( 89.69)	Acc@5 100.00 ( 98.77)
Epoch: [45][ 60/391]	Time  0.032 ( 0.028)	Data  0.002 ( 0.005)	Loss 4.9707e-01 (3.5378e-01)	Acc@1  84.38 ( 89.61)	Acc@5  97.66 ( 98.78)
Epoch: [45][ 70/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.7329e-01 (3.5317e-01)	Acc@1  88.28 ( 89.57)	Acc@5  99.22 ( 98.79)
Epoch: [45][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.8086e-01 (3.5420e-01)	Acc@1  86.72 ( 89.44)	Acc@5 100.00 ( 98.82)
Epoch: [45][ 90/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.5205e-01 (3.5706e-01)	Acc@1  87.50 ( 89.23)	Acc@5  99.22 ( 98.82)
Epoch: [45][100/391]	Time  0.036 ( 0.027)	Data  0.002 ( 0.003)	Loss 3.1445e-01 (3.5166e-01)	Acc@1  92.19 ( 89.37)	Acc@5 100.00 ( 98.87)
Epoch: [45][110/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8086e-01 (3.4736e-01)	Acc@1  87.50 ( 89.50)	Acc@5  98.44 ( 98.87)
Epoch: [45][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3572e-01 (3.4822e-01)	Acc@1  94.53 ( 89.48)	Acc@5 100.00 ( 98.85)
Epoch: [45][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2881e-01 (3.4900e-01)	Acc@1  83.59 ( 89.49)	Acc@5  98.44 ( 98.83)
Epoch: [45][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9209e-01 (3.4957e-01)	Acc@1  89.06 ( 89.44)	Acc@5  97.66 ( 98.82)
Epoch: [45][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7368e-01 (3.5050e-01)	Acc@1  93.75 ( 89.38)	Acc@5  98.44 ( 98.80)
Epoch: [45][160/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5728e-01 (3.5028e-01)	Acc@1  86.72 ( 89.33)	Acc@5  96.88 ( 98.80)
Epoch: [45][170/391]	Time  0.026 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.3057e-01 (3.4842e-01)	Acc@1  89.06 ( 89.41)	Acc@5  98.44 ( 98.82)
Epoch: [45][180/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6831e-01 (3.4865e-01)	Acc@1  92.19 ( 89.39)	Acc@5 100.00 ( 98.82)
Epoch: [45][190/391]	Time  0.029 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.6367e-01 (3.4943e-01)	Acc@1  92.19 ( 89.34)	Acc@5 100.00 ( 98.82)
Epoch: [45][200/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.5654e-01 (3.5073e-01)	Acc@1  88.28 ( 89.26)	Acc@5  98.44 ( 98.81)
Epoch: [45][210/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3936e-01 (3.5073e-01)	Acc@1  88.28 ( 89.22)	Acc@5  98.44 ( 98.82)
Epoch: [45][220/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.2300e-01 (3.4892e-01)	Acc@1  89.84 ( 89.24)	Acc@5  98.44 ( 98.84)
Epoch: [45][230/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5098e-01 (3.4883e-01)	Acc@1  92.97 ( 89.23)	Acc@5  99.22 ( 98.84)
Epoch: [45][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8599e-01 (3.4981e-01)	Acc@1  89.06 ( 89.20)	Acc@5 100.00 ( 98.83)
Epoch: [45][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5083e-01 (3.4985e-01)	Acc@1  89.06 ( 89.20)	Acc@5 100.00 ( 98.83)
Epoch: [45][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1396e-01 (3.5232e-01)	Acc@1  89.06 ( 89.11)	Acc@5  99.22 ( 98.83)
Epoch: [45][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5522e-01 (3.5449e-01)	Acc@1  87.50 ( 89.06)	Acc@5 100.00 ( 98.83)
Epoch: [45][280/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.002)	Loss 4.7534e-01 (3.5571e-01)	Acc@1  84.38 ( 89.02)	Acc@5  97.66 ( 98.81)
Epoch: [45][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4375e-01 (3.5679e-01)	Acc@1  87.50 ( 88.96)	Acc@5  98.44 ( 98.80)
Epoch: [45][300/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.5581e-01 (3.5809e-01)	Acc@1  90.62 ( 88.90)	Acc@5  98.44 ( 98.79)
Epoch: [45][310/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.6270e-01 (3.5899e-01)	Acc@1  91.41 ( 88.88)	Acc@5  99.22 ( 98.79)
Epoch: [45][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6377e-01 (3.5973e-01)	Acc@1  88.28 ( 88.88)	Acc@5  99.22 ( 98.79)
Epoch: [45][330/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.2251e-01 (3.6102e-01)	Acc@1  92.19 ( 88.85)	Acc@5  98.44 ( 98.78)
Epoch: [45][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8687e-01 (3.5954e-01)	Acc@1  90.62 ( 88.89)	Acc@5  99.22 ( 98.80)
Epoch: [45][350/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.4263e-01 (3.5944e-01)	Acc@1  85.16 ( 88.87)	Acc@5  98.44 ( 98.81)
Epoch: [45][360/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0493e-01 (3.5952e-01)	Acc@1  89.06 ( 88.87)	Acc@5  99.22 ( 98.82)
Epoch: [45][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.1211e-01 (3.5981e-01)	Acc@1  85.16 ( 88.85)	Acc@5 100.00 ( 98.82)
Epoch: [45][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4473e-01 (3.6061e-01)	Acc@1  90.62 ( 88.82)	Acc@5  98.44 ( 98.81)
Epoch: [45][390/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 5.3320e-01 (3.6253e-01)	Acc@1  81.25 ( 88.78)	Acc@5  97.50 ( 98.79)
## e[45] optimizer.zero_grad (sum) time: 0.11627721786499023
## e[45]       loss.backward (sum) time: 2.2862203121185303
## e[45]      optimizer.step (sum) time: 0.9726607799530029
## epoch[45] training(only) time: 10.0814950466156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.4336e+00 (1.4336e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 1.4941e+00 (1.5384e+00)	Acc@1  68.00 ( 66.45)	Acc@5  90.00 ( 87.73)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.7803e+00 (1.5661e+00)	Acc@1  68.00 ( 66.52)	Acc@5  86.00 ( 87.95)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.8643e+00 (1.5574e+00)	Acc@1  58.00 ( 65.77)	Acc@5  87.00 ( 88.16)
Test: [ 40/100]	Time  0.023 ( 0.017)	Loss 1.2803e+00 (1.5495e+00)	Acc@1  69.00 ( 65.71)	Acc@5  94.00 ( 88.34)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5615e+00 (1.5757e+00)	Acc@1  67.00 ( 65.45)	Acc@5  87.00 ( 88.10)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.5869e+00 (1.5601e+00)	Acc@1  64.00 ( 65.39)	Acc@5  86.00 ( 88.16)
Test: [ 70/100]	Time  0.020 ( 0.016)	Loss 1.7373e+00 (1.5554e+00)	Acc@1  63.00 ( 65.51)	Acc@5  91.00 ( 88.27)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.5264e+00 (1.5662e+00)	Acc@1  65.00 ( 65.15)	Acc@5  85.00 ( 88.01)
Test: [ 90/100]	Time  0.019 ( 0.015)	Loss 1.8252e+00 (1.5633e+00)	Acc@1  63.00 ( 65.38)	Acc@5  86.00 ( 88.20)
 * Acc@1 65.460 Acc@5 88.300
### epoch[45] execution time: 11.665753841400146
EPOCH 46
i:   0, name:       features.module.0.weight  changing lr from: 0.001135477166065490   to: 0.001000743693028921
i:   1, name:         features.module.0.bias  changing lr from: 0.001444195499211558   to: 0.001110668493058619
i:   2, name:       features.module.1.weight  changing lr from: 0.001915116129633498   to: 0.001396761011517865
i:   3, name:         features.module.1.bias  changing lr from: 0.002533974574984692   to: 0.001844479256250302
i:   4, name:       features.module.4.weight  changing lr from: 0.003287106654982652   to: 0.002439797525212383
i:   5, name:         features.module.4.bias  changing lr from: 0.004161490850379633   to: 0.003169266511483126
i:   6, name:       features.module.5.weight  changing lr from: 0.005144775846656346   to: 0.004020056254466558
i:   7, name:         features.module.5.bias  changing lr from: 0.006225295677228476   to: 0.004979984570249890
i:   8, name:       features.module.8.weight  changing lr from: 0.007392074589608787   to: 0.006037533296235824
i:   9, name:         features.module.8.bias  changing lr from: 0.008634823489961479   to: 0.007181854407286364
i:  10, name:       features.module.9.weight  changing lr from: 0.009943929577393311   to: 0.008402767804430104
i:  11, name:         features.module.9.bias  changing lr from: 0.011310440558916501   to: 0.009690752343364400
i:  12, name:      features.module.11.weight  changing lr from: 0.012726044638490159   to: 0.011036931458413925
i:  13, name:        features.module.11.bias  changing lr from: 0.014183047297721761   to: 0.012433054547620677
i:  14, name:      features.module.12.weight  changing lr from: 0.015674345730260480   to: 0.013871475115189203
i:  15, name:        features.module.12.bias  changing lr from: 0.017193401655085035   to: 0.015345126517303364
i:  16, name:      features.module.15.weight  changing lr from: 0.018734213114172005   to: 0.016847496024938669
i:  17, name:        features.module.15.bias  changing lr from: 0.020291285755816704   to: 0.018372597801222985
i:  18, name:      features.module.16.weight  changing lr from: 0.021859604014615466   to: 0.019914945289655518
i:  19, name:        features.module.16.bias  changing lr from: 0.023434602521316079   to: 0.021469523421614175
i:  20, name:      features.module.18.weight  changing lr from: 0.025012138009019011   to: 0.023031760975676347
i:  21, name:        features.module.18.bias  changing lr from: 0.026588461925268203   to: 0.024597503356026729
i:  22, name:      features.module.19.weight  changing lr from: 0.028160193911237426   to: 0.026162986001415131
i:  23, name:        features.module.19.bias  changing lr from: 0.029724296268414604   to: 0.027724808588632379
i:  24, name:      features.module.22.weight  changing lr from: 0.031278049498949315   to: 0.029279910154271668
i:  25, name:        features.module.22.bias  changing lr from: 0.032819028977291054   to: 0.030825545224716346
i:  26, name:      features.module.23.weight  changing lr from: 0.034345082787130402   to: 0.032359261016006523
i:  27, name:        features.module.23.bias  changing lr from: 0.035854310738282678   to: 0.033878875741751191
i:  28, name:      features.module.25.weight  changing lr from: 0.037345044562414768   to: 0.035382458047907267
i:  29, name:        features.module.25.bias  changing lr from: 0.038815829273880359   to: 0.036868307577460235
i:  30, name:      features.module.26.weight  changing lr from: 0.040265405671932764   to: 0.038334936655300085
i:  31, name:        features.module.26.bias  changing lr from: 0.041692693952820374   to: 0.039781053073441423
i:  32, name:            classifier.0.weight  changing lr from: 0.043096778394385832   to: 0.041205543948792396
i:  33, name:              classifier.0.bias  changing lr from: 0.044476893071481838   to: 0.042607460619595007
i:  34, name:            classifier.3.weight  changing lr from: 0.045832408557518654   to: 0.043986004542137087
i:  35, name:              classifier.3.bias  changing lr from: 0.047162819565546001   to: 0.045340514146121579
i:  36, name:            classifier.6.weight  changing lr from: 0.048467733481251309   to: 0.046670452604947038
i:  37, name:              classifier.6.bias  changing lr from: 0.049746859739958571   to: 0.047975396475915234



# Switched to train mode...
Epoch: [46][  0/391]	Time  0.181 ( 0.181)	Data  0.153 ( 0.153)	Loss 3.0029e-01 (3.0029e-01)	Acc@1  94.53 ( 94.53)	Acc@5  98.44 ( 98.44)
Epoch: [46][ 10/391]	Time  0.029 ( 0.039)	Data  0.001 ( 0.015)	Loss 3.0615e-01 (3.1137e-01)	Acc@1  90.62 ( 90.77)	Acc@5  99.22 ( 98.65)
Epoch: [46][ 20/391]	Time  0.026 ( 0.032)	Data  0.001 ( 0.009)	Loss 2.2778e-01 (3.2116e-01)	Acc@1  92.19 ( 90.14)	Acc@5 100.00 ( 98.81)
Epoch: [46][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.8979e-01 (3.1861e-01)	Acc@1  89.06 ( 90.12)	Acc@5  99.22 ( 98.92)
Epoch: [46][ 40/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.8613e-01 (3.2671e-01)	Acc@1  88.28 ( 89.88)	Acc@5 100.00 ( 98.86)
Epoch: [46][ 50/391]	Time  0.034 ( 0.028)	Data  0.002 ( 0.005)	Loss 2.5537e-01 (3.2463e-01)	Acc@1  93.75 ( 90.20)	Acc@5 100.00 ( 98.84)
Epoch: [46][ 60/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.1763e-01 (3.3152e-01)	Acc@1  88.28 ( 89.93)	Acc@5 100.00 ( 98.87)
Epoch: [46][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.8135e-01 (3.3241e-01)	Acc@1  88.28 ( 89.96)	Acc@5  99.22 ( 98.90)
Epoch: [46][ 80/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8223e-01 (3.3051e-01)	Acc@1  89.06 ( 89.95)	Acc@5  99.22 ( 98.93)
Epoch: [46][ 90/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9785e-01 (3.3148e-01)	Acc@1  91.41 ( 89.90)	Acc@5  99.22 ( 98.94)
Epoch: [46][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3325e-01 (3.3077e-01)	Acc@1  89.84 ( 89.91)	Acc@5  99.22 ( 98.97)
Epoch: [46][110/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.7915e-01 (3.2807e-01)	Acc@1  89.84 ( 90.02)	Acc@5  99.22 ( 98.99)
Epoch: [46][120/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.6963e-01 (3.2511e-01)	Acc@1  89.84 ( 90.09)	Acc@5  97.66 ( 99.03)
Epoch: [46][130/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3267e-01 (3.2277e-01)	Acc@1  90.62 ( 90.12)	Acc@5 100.00 ( 99.01)
Epoch: [46][140/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.3130e-01 (3.2441e-01)	Acc@1  87.50 ( 90.09)	Acc@5 100.00 ( 98.99)
Epoch: [46][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6816e-01 (3.2707e-01)	Acc@1  89.06 ( 90.03)	Acc@5  99.22 ( 99.01)
Epoch: [46][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6240e-01 (3.2795e-01)	Acc@1  89.84 ( 90.03)	Acc@5  97.66 ( 99.00)
Epoch: [46][170/391]	Time  0.034 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.2324e-01 (3.2812e-01)	Acc@1  89.06 ( 90.04)	Acc@5  99.22 ( 98.98)
Epoch: [46][180/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.3521e-01 (3.2817e-01)	Acc@1  89.84 ( 90.04)	Acc@5  99.22 ( 98.97)
Epoch: [46][190/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3057e-01 (3.2933e-01)	Acc@1  88.28 ( 89.97)	Acc@5  99.22 ( 98.97)
Epoch: [46][200/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.7925e-01 (3.3183e-01)	Acc@1  85.94 ( 89.88)	Acc@5  97.66 ( 98.98)
Epoch: [46][210/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8877e-01 (3.3317e-01)	Acc@1  87.50 ( 89.86)	Acc@5  97.66 ( 98.97)
Epoch: [46][220/391]	Time  0.045 ( 0.026)	Data  0.005 ( 0.003)	Loss 3.3716e-01 (3.3291e-01)	Acc@1  90.62 ( 89.86)	Acc@5  98.44 ( 98.97)
Epoch: [46][230/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7012e-01 (3.3075e-01)	Acc@1  89.06 ( 89.91)	Acc@5  99.22 ( 99.00)
Epoch: [46][240/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1372e-01 (3.2990e-01)	Acc@1  89.06 ( 89.89)	Acc@5  99.22 ( 99.00)
Epoch: [46][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0396e-01 (3.3079e-01)	Acc@1  90.62 ( 89.85)	Acc@5  99.22 ( 99.00)
Epoch: [46][260/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2139e-01 (3.3131e-01)	Acc@1  85.16 ( 89.83)	Acc@5  97.66 ( 98.99)
Epoch: [46][270/391]	Time  0.031 ( 0.026)	Data  0.004 ( 0.002)	Loss 3.5132e-01 (3.3054e-01)	Acc@1  89.84 ( 89.84)	Acc@5  98.44 ( 98.99)
Epoch: [46][280/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0762e-01 (3.3216e-01)	Acc@1  89.84 ( 89.81)	Acc@5  99.22 ( 98.99)
Epoch: [46][290/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.2651e-01 (3.3286e-01)	Acc@1  87.50 ( 89.77)	Acc@5  96.88 ( 98.98)
Epoch: [46][300/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.8369e-01 (3.3190e-01)	Acc@1  92.19 ( 89.80)	Acc@5  97.66 ( 98.99)
Epoch: [46][310/391]	Time  0.044 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.3091e-01 (3.3345e-01)	Acc@1  85.16 ( 89.74)	Acc@5  99.22 ( 98.98)
Epoch: [46][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0764e-01 (3.3313e-01)	Acc@1  93.75 ( 89.76)	Acc@5  99.22 ( 98.99)
Epoch: [46][330/391]	Time  0.024 ( 0.025)	Data  0.006 ( 0.002)	Loss 2.5146e-01 (3.3315e-01)	Acc@1  92.19 ( 89.77)	Acc@5 100.00 ( 98.99)
Epoch: [46][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5938e-01 (3.3304e-01)	Acc@1  89.06 ( 89.78)	Acc@5 100.00 ( 98.99)
Epoch: [46][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5708e-01 (3.3291e-01)	Acc@1  91.41 ( 89.77)	Acc@5  99.22 ( 99.00)
Epoch: [46][360/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.5415e-01 (3.3289e-01)	Acc@1  92.97 ( 89.76)	Acc@5  99.22 ( 98.98)
Epoch: [46][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.1260e-01 (3.3255e-01)	Acc@1  89.06 ( 89.76)	Acc@5  98.44 ( 98.99)
Epoch: [46][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7368e-01 (3.3275e-01)	Acc@1  92.19 ( 89.76)	Acc@5  99.22 ( 98.99)
Epoch: [46][390/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.5864e-01 (3.3333e-01)	Acc@1  95.00 ( 89.74)	Acc@5  98.75 ( 98.99)
## e[46] optimizer.zero_grad (sum) time: 0.1140739917755127
## e[46]       loss.backward (sum) time: 2.286245822906494
## e[46]      optimizer.step (sum) time: 0.9674081802368164
## epoch[46] training(only) time: 10.053831577301025
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.5283e+00 (1.5283e+00)	Acc@1  62.00 ( 62.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.012 ( 0.025)	Loss 1.7129e+00 (1.5970e+00)	Acc@1  61.00 ( 64.55)	Acc@5  89.00 ( 87.27)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.6982e+00 (1.5885e+00)	Acc@1  69.00 ( 65.33)	Acc@5  84.00 ( 87.52)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 2.0234e+00 (1.5855e+00)	Acc@1  55.00 ( 65.19)	Acc@5  86.00 ( 87.84)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.4814e+00 (1.5878e+00)	Acc@1  63.00 ( 65.27)	Acc@5  92.00 ( 88.07)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5186e+00 (1.6154e+00)	Acc@1  67.00 ( 64.94)	Acc@5  86.00 ( 87.71)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.7109e+00 (1.6097e+00)	Acc@1  68.00 ( 65.10)	Acc@5  83.00 ( 87.77)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.7188e+00 (1.6040e+00)	Acc@1  66.00 ( 65.30)	Acc@5  86.00 ( 87.93)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.7910e+00 (1.6098e+00)	Acc@1  63.00 ( 65.17)	Acc@5  86.00 ( 87.78)
Test: [ 90/100]	Time  0.023 ( 0.015)	Loss 1.6406e+00 (1.5938e+00)	Acc@1  65.00 ( 65.44)	Acc@5  88.00 ( 87.95)
 * Acc@1 65.600 Acc@5 88.020
### epoch[46] execution time: 11.625168561935425
EPOCH 47
REMOVING: features.module.0.weight
REMOVING: features.module.0.bias
i:   0, name:       features.module.1.weight  changing lr from: 0.001396761011517865   to: 0.001091497580225430
i:   1, name:         features.module.1.bias  changing lr from: 0.001844479256250302   to: 0.001358111488881363
i:   2, name:       features.module.4.weight  changing lr from: 0.002439797525212383   to: 0.001785563057252459
i:   3, name:         features.module.4.bias  changing lr from: 0.003169266511483126   to: 0.002360065701565244
i:   4, name:       features.module.5.weight  changing lr from: 0.004020056254466558   to: 0.003068387197812027
i:   5, name:         features.module.5.bias  changing lr from: 0.004979984570249890   to: 0.003897892926367152
i:   6, name:       features.module.8.weight  changing lr from: 0.006037533296235824   to: 0.004836574917343139
i:   7, name:         features.module.8.bias  changing lr from: 0.007181854407286364   to: 0.005873068951867116
i:   8, name:       features.module.9.weight  changing lr from: 0.008402767804430104   to: 0.006996661709516177
i:   9, name:         features.module.9.bias  changing lr from: 0.009690752343364400   to: 0.008197289707402777
i:  10, name:      features.module.11.weight  changing lr from: 0.011036931458413925   to: 0.009465531552587090
i:  11, name:        features.module.11.bias  changing lr from: 0.012433054547620677   to: 0.010792594826595919
i:  12, name:      features.module.12.weight  changing lr from: 0.013871475115189203   to: 0.012170298738289183
i:  13, name:        features.module.12.bias  changing lr from: 0.015345126517303364   to: 0.013591053518206525
i:  14, name:      features.module.15.weight  changing lr from: 0.016847496024938669   to: 0.015047837382675538
i:  15, name:        features.module.15.bias  changing lr from: 0.018372597801222985   to: 0.016534171768053448
i:  16, name:      features.module.16.weight  changing lr from: 0.019914945289655518   to: 0.018044095423125208
i:  17, name:        features.module.16.bias  changing lr from: 0.021469523421614175   to: 0.019572137849492743
i:  18, name:      features.module.18.weight  changing lr from: 0.023031760975676347   to: 0.021113292494401831
i:  19, name:        features.module.18.bias  changing lr from: 0.024597503356026729   to: 0.022662990026543248
i:  20, name:      features.module.19.weight  changing lr from: 0.026162986001415131   to: 0.024217071961701392
i:  21, name:        features.module.19.bias  changing lr from: 0.027724808588632379   to: 0.025771764850549789
i:  22, name:      features.module.22.weight  changing lr from: 0.029279910154271668   to: 0.027323655194340449
i:  23, name:        features.module.22.bias  changing lr from: 0.030825545224716346   to: 0.028869665214740072
i:  24, name:      features.module.23.weight  changing lr from: 0.032359261016006523   to: 0.030407029570738871
i:  25, name:        features.module.23.bias  changing lr from: 0.033878875741751191   to: 0.031933273087612478
i:  26, name:      features.module.25.weight  changing lr from: 0.035382458047907267   to: 0.033446189539639563
i:  27, name:        features.module.25.bias  changing lr from: 0.036868307577460235   to: 0.034943821509035716
i:  28, name:      features.module.26.weight  changing lr from: 0.038334936655300085   to: 0.036424441327798665
i:  29, name:        features.module.26.bias  changing lr from: 0.039781053073441423   to: 0.037886533096374846
i:  30, name:            classifier.0.weight  changing lr from: 0.041205543948792396   to: 0.039328775762818763
i:  31, name:              classifier.0.bias  changing lr from: 0.042607460619595007   to: 0.040750027238045189
i:  32, name:            classifier.3.weight  changing lr from: 0.043986004542137087   to: 0.042149309516534179
i:  33, name:              classifier.3.bias  changing lr from: 0.045340514146121579   to: 0.043525794767156768
i:  34, name:            classifier.6.weight  changing lr from: 0.046670452604947038   to: 0.044878792355389607
i:  35, name:              classifier.6.bias  changing lr from: 0.047975396475915234   to: 0.046207736755867243



# Switched to train mode...
Epoch: [47][  0/391]	Time  0.174 ( 0.174)	Data  0.127 ( 0.127)	Loss 2.5854e-01 (2.5854e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.013)	Loss 2.9272e-01 (3.1562e-01)	Acc@1  88.28 ( 89.56)	Acc@5  99.22 ( 98.93)
Epoch: [47][ 20/391]	Time  0.042 ( 0.031)	Data  0.007 ( 0.008)	Loss 2.5366e-01 (2.9395e-01)	Acc@1  89.84 ( 90.44)	Acc@5  99.22 ( 99.22)
Epoch: [47][ 30/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.2852e-01 (2.8889e-01)	Acc@1  96.88 ( 90.78)	Acc@5  98.44 ( 99.22)
Epoch: [47][ 40/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.5586e-01 (2.8944e-01)	Acc@1  95.31 ( 91.01)	Acc@5  99.22 ( 99.16)
Epoch: [47][ 50/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.2456e-01 (2.9088e-01)	Acc@1  88.28 ( 90.99)	Acc@5  98.44 ( 99.14)
Epoch: [47][ 60/391]	Time  0.034 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.8052e-01 (2.9105e-01)	Acc@1  89.06 ( 90.96)	Acc@5  99.22 ( 99.12)
Epoch: [47][ 70/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.9404e-01 (2.9089e-01)	Acc@1  85.94 ( 90.92)	Acc@5  99.22 ( 99.11)
Epoch: [47][ 80/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.004)	Loss 3.4009e-01 (2.8808e-01)	Acc@1  90.62 ( 91.05)	Acc@5  99.22 ( 99.13)
Epoch: [47][ 90/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5986e-01 (2.8922e-01)	Acc@1  88.28 ( 91.04)	Acc@5  98.44 ( 99.12)
Epoch: [47][100/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0654e-01 (2.8981e-01)	Acc@1  95.31 ( 90.97)	Acc@5  99.22 ( 99.15)
Epoch: [47][110/391]	Time  0.038 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.4336e-01 (2.9124e-01)	Acc@1  88.28 ( 90.86)	Acc@5  99.22 ( 99.17)
Epoch: [47][120/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.2334e-01 (2.9064e-01)	Acc@1  85.94 ( 90.88)	Acc@5  98.44 ( 99.17)
Epoch: [47][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8428e-01 (2.9144e-01)	Acc@1  88.28 ( 90.86)	Acc@5  99.22 ( 99.19)
Epoch: [47][140/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.1992e-01 (2.9538e-01)	Acc@1  88.28 ( 90.75)	Acc@5  96.88 ( 99.16)
Epoch: [47][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8091e-01 (2.9590e-01)	Acc@1  93.75 ( 90.74)	Acc@5 100.00 ( 99.17)
Epoch: [47][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5928e-01 (2.9550e-01)	Acc@1  92.19 ( 90.75)	Acc@5  99.22 ( 99.18)
Epoch: [47][170/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.8882e-01 (2.9759e-01)	Acc@1  89.84 ( 90.73)	Acc@5  99.22 ( 99.14)
Epoch: [47][180/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9421e-01 (2.9764e-01)	Acc@1  92.19 ( 90.78)	Acc@5  99.22 ( 99.12)
Epoch: [47][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3179e-01 (2.9948e-01)	Acc@1  89.84 ( 90.68)	Acc@5  99.22 ( 99.14)
Epoch: [47][200/391]	Time  0.035 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.4521e-01 (2.9781e-01)	Acc@1  90.62 ( 90.73)	Acc@5  99.22 ( 99.15)
Epoch: [47][210/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2522e-01 (2.9747e-01)	Acc@1  92.97 ( 90.73)	Acc@5 100.00 ( 99.17)
Epoch: [47][220/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.9917e-01 (2.9608e-01)	Acc@1  87.50 ( 90.78)	Acc@5  99.22 ( 99.18)
Epoch: [47][230/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6980e-01 (2.9607e-01)	Acc@1  92.97 ( 90.75)	Acc@5 100.00 ( 99.19)
Epoch: [47][240/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7588e-01 (2.9529e-01)	Acc@1  90.62 ( 90.76)	Acc@5  98.44 ( 99.18)
Epoch: [47][250/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9517e-01 (2.9476e-01)	Acc@1  89.84 ( 90.77)	Acc@5  99.22 ( 99.19)
Epoch: [47][260/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.9771e-01 (2.9451e-01)	Acc@1  88.28 ( 90.78)	Acc@5  99.22 ( 99.20)
Epoch: [47][270/391]	Time  0.043 ( 0.025)	Data  0.009 ( 0.002)	Loss 3.0298e-01 (2.9421e-01)	Acc@1  90.62 ( 90.78)	Acc@5  97.66 ( 99.20)
Epoch: [47][280/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8442e-01 (2.9441e-01)	Acc@1  92.19 ( 90.75)	Acc@5  99.22 ( 99.20)
Epoch: [47][290/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2959e-01 (2.9489e-01)	Acc@1  88.28 ( 90.72)	Acc@5  99.22 ( 99.20)
Epoch: [47][300/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8198e-01 (2.9486e-01)	Acc@1  90.62 ( 90.70)	Acc@5 100.00 ( 99.21)
Epoch: [47][310/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9419e-01 (2.9507e-01)	Acc@1  91.41 ( 90.71)	Acc@5 100.00 ( 99.22)
Epoch: [47][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.9404e-01 (2.9495e-01)	Acc@1  89.06 ( 90.73)	Acc@5  98.44 ( 99.22)
Epoch: [47][330/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.5693e-01 (2.9416e-01)	Acc@1  86.72 ( 90.74)	Acc@5  97.66 ( 99.22)
Epoch: [47][340/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2422e-01 (2.9379e-01)	Acc@1  91.41 ( 90.76)	Acc@5  98.44 ( 99.22)
Epoch: [47][350/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.1948e-01 (2.9361e-01)	Acc@1  92.97 ( 90.76)	Acc@5 100.00 ( 99.22)
Epoch: [47][360/391]	Time  0.024 ( 0.025)	Data  0.003 ( 0.002)	Loss 3.3228e-01 (2.9553e-01)	Acc@1  92.19 ( 90.73)	Acc@5  97.66 ( 99.21)
Epoch: [47][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 4.9805e-01 (2.9676e-01)	Acc@1  87.50 ( 90.68)	Acc@5  98.44 ( 99.20)
Epoch: [47][380/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5122e-01 (2.9707e-01)	Acc@1  89.84 ( 90.66)	Acc@5 100.00 ( 99.19)
Epoch: [47][390/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8433e-01 (2.9789e-01)	Acc@1  93.75 ( 90.63)	Acc@5  98.75 ( 99.19)
## e[47] optimizer.zero_grad (sum) time: 0.11078023910522461
## e[47]       loss.backward (sum) time: 2.187093734741211
## e[47]      optimizer.step (sum) time: 0.9225876331329346
## epoch[47] training(only) time: 9.967179775238037
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.5312e+00 (1.5312e+00)	Acc@1  65.00 ( 65.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.015 ( 0.025)	Loss 1.7178e+00 (1.5825e+00)	Acc@1  66.00 ( 66.27)	Acc@5  87.00 ( 88.09)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 1.7617e+00 (1.5691e+00)	Acc@1  65.00 ( 67.52)	Acc@5  85.00 ( 88.24)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.7900e+00 (1.5742e+00)	Acc@1  60.00 ( 66.77)	Acc@5  84.00 ( 87.87)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.4219e+00 (1.5715e+00)	Acc@1  65.00 ( 66.51)	Acc@5  90.00 ( 87.93)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.5078e+00 (1.5979e+00)	Acc@1  71.00 ( 66.27)	Acc@5  88.00 ( 87.71)
Test: [ 60/100]	Time  0.017 ( 0.016)	Loss 1.4756e+00 (1.5813e+00)	Acc@1  61.00 ( 66.31)	Acc@5  89.00 ( 87.87)
Test: [ 70/100]	Time  0.019 ( 0.016)	Loss 1.5947e+00 (1.5860e+00)	Acc@1  69.00 ( 66.14)	Acc@5  90.00 ( 87.89)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.5352e+00 (1.5995e+00)	Acc@1  64.00 ( 65.86)	Acc@5  87.00 ( 87.79)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.7637e+00 (1.5924e+00)	Acc@1  63.00 ( 66.05)	Acc@5  87.00 ( 87.82)
 * Acc@1 66.230 Acc@5 87.920
### epoch[47] execution time: 11.542125225067139
EPOCH 48
REMOVING: features.module.1.weight
i:   0, name:         features.module.1.bias  changing lr from: 0.001358111488881363   to: 0.001076881112101518
i:   1, name:       features.module.4.weight  changing lr from: 0.001785563057252459   to: 0.001327004477873133
i:   2, name:         features.module.4.bias  changing lr from: 0.002360065701565244   to: 0.001736984814452346
i:   3, name:       features.module.5.weight  changing lr from: 0.003068387197812027   to: 0.002293274147895485
i:   4, name:         features.module.5.bias  changing lr from: 0.003897892926367152   to: 0.002982858606465077
i:   5, name:       features.module.8.weight  changing lr from: 0.004836574917343139   to: 0.003793301689785655
i:   6, name:         features.module.8.bias  changing lr from: 0.005873068951867116   to: 0.004712773682817761
i:   7, name:       features.module.9.weight  changing lr from: 0.006996661709516177   to: 0.005730069390097569
i:   8, name:         features.module.9.bias  changing lr from: 0.008197289707402777   to: 0.006834616113150077
i:   9, name:      features.module.11.weight  changing lr from: 0.009465531552587090   to: 0.008016473560271426
i:  10, name:        features.module.11.bias  changing lr from: 0.010792594826595919   to: 0.009266327163777642
i:  11, name:      features.module.12.weight  changing lr from: 0.012170298738289183   to: 0.010575476085391909
i:  12, name:        features.module.12.bias  changing lr from: 0.013591053518206525   to: 0.011935817015226863
i:  13, name:      features.module.15.weight  changing lr from: 0.015047837382675538   to: 0.013339824712987954
i:  14, name:        features.module.15.bias  changing lr from: 0.016534171768053448   to: 0.014780530100507665
i:  15, name:      features.module.16.weight  changing lr from: 0.018044095423125208   to: 0.016251496591312468
i:  16, name:        features.module.16.bias  changing lr from: 0.019572137849492743   to: 0.017746795234335932
i:  17, name:      features.module.18.weight  changing lr from: 0.021113292494401831   to: 0.019260979153826335
i:  18, name:        features.module.18.bias  changing lr from: 0.022662990026543248   to: 0.020789057684666552
i:  19, name:      features.module.19.weight  changing lr from: 0.024217071961701392   to: 0.022326470530500173
i:  20, name:        features.module.19.bias  changing lr from: 0.025771764850549789   to: 0.023869062210065829
i:  21, name:      features.module.22.weight  changing lr from: 0.027323655194340449   to: 0.025413057003891966
i:  22, name:        features.module.22.bias  changing lr from: 0.028869665214740072   to: 0.026955034567988354
i:  23, name:      features.module.23.weight  changing lr from: 0.030407029570738871   to: 0.028491906342460946
i:  24, name:        features.module.23.bias  changing lr from: 0.031933273087612478   to: 0.030020892850236888
i:  25, name:      features.module.25.weight  changing lr from: 0.033446189539639563   to: 0.031539501953558292
i:  26, name:        features.module.25.bias  changing lr from: 0.034943821509035716   to: 0.033045508112906241
i:  27, name:      features.module.26.weight  changing lr from: 0.036424441327798665   to: 0.034536932673945080
i:  28, name:        features.module.26.bias  changing lr from: 0.037886533096374846   to: 0.036012025192393528
i:  29, name:            classifier.0.weight  changing lr from: 0.039328775762818763   to: 0.037469245793956295
i:  30, name:              classifier.0.bias  changing lr from: 0.040750027238045189   to: 0.038907248556169539
i:  31, name:            classifier.3.weight  changing lr from: 0.042149309516534179   to: 0.040324865890857972
i:  32, name:              classifier.3.bias  changing lr from: 0.043525794767156768   to: 0.041721093899548659
i:  33, name:            classifier.6.weight  changing lr from: 0.044878792355389607   to: 0.043095078669357481
i:  34, name:              classifier.6.bias  changing lr from: 0.046207736755867243   to: 0.044446103473314680



# Switched to train mode...
Epoch: [48][  0/391]	Time  0.178 ( 0.178)	Data  0.147 ( 0.147)	Loss 3.1909e-01 (3.1909e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 2.2644e-01 (2.8195e-01)	Acc@1  92.97 ( 90.48)	Acc@5 100.00 ( 99.57)
Epoch: [48][ 20/391]	Time  0.035 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.3569e-01 (2.7551e-01)	Acc@1  90.62 ( 90.96)	Acc@5  98.44 ( 99.33)
Epoch: [48][ 30/391]	Time  0.020 ( 0.029)	Data  0.000 ( 0.006)	Loss 3.5913e-01 (2.7174e-01)	Acc@1  91.41 ( 91.15)	Acc@5  99.22 ( 99.37)
Epoch: [48][ 40/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.6440e-01 (2.6844e-01)	Acc@1  90.62 ( 91.27)	Acc@5 100.00 ( 99.29)
Epoch: [48][ 50/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.1472e-01 (2.6585e-01)	Acc@1  92.19 ( 91.39)	Acc@5 100.00 ( 99.33)
Epoch: [48][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4866e-01 (2.5899e-01)	Acc@1  92.97 ( 91.65)	Acc@5  99.22 ( 99.37)
Epoch: [48][ 70/391]	Time  0.032 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.6660e-01 (2.6036e-01)	Acc@1  91.41 ( 91.64)	Acc@5 100.00 ( 99.38)
Epoch: [48][ 80/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.9712e-01 (2.6455e-01)	Acc@1  93.75 ( 91.66)	Acc@5  97.66 ( 99.35)
Epoch: [48][ 90/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8933e-01 (2.6253e-01)	Acc@1  93.75 ( 91.70)	Acc@5 100.00 ( 99.40)
Epoch: [48][100/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3291e-01 (2.6835e-01)	Acc@1  92.97 ( 91.51)	Acc@5  99.22 ( 99.40)
Epoch: [48][110/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.1997e-01 (2.6516e-01)	Acc@1  91.41 ( 91.53)	Acc@5  99.22 ( 99.42)
Epoch: [48][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2764e-01 (2.6885e-01)	Acc@1  92.19 ( 91.44)	Acc@5  97.66 ( 99.38)
Epoch: [48][130/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4158e-01 (2.6552e-01)	Acc@1  91.41 ( 91.53)	Acc@5 100.00 ( 99.39)
Epoch: [48][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5073e-01 (2.6406e-01)	Acc@1  92.97 ( 91.61)	Acc@5 100.00 ( 99.39)
Epoch: [48][150/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3569e-01 (2.6544e-01)	Acc@1  86.72 ( 91.51)	Acc@5 100.00 ( 99.39)
Epoch: [48][160/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.4595e-01 (2.6654e-01)	Acc@1  87.50 ( 91.44)	Acc@5  99.22 ( 99.40)
Epoch: [48][170/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6562e-01 (2.6642e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 ( 99.41)
Epoch: [48][180/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.4988e-01 (2.6383e-01)	Acc@1  92.19 ( 91.51)	Acc@5 100.00 ( 99.42)
Epoch: [48][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6418e-01 (2.6368e-01)	Acc@1  94.53 ( 91.51)	Acc@5 100.00 ( 99.41)
Epoch: [48][200/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.8174e-01 (2.6382e-01)	Acc@1  92.97 ( 91.56)	Acc@5  98.44 ( 99.40)
Epoch: [48][210/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4744e-01 (2.6752e-01)	Acc@1  92.19 ( 91.46)	Acc@5 100.00 ( 99.39)
Epoch: [48][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7930e-01 (2.6779e-01)	Acc@1  92.97 ( 91.47)	Acc@5  99.22 ( 99.38)
Epoch: [48][230/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.2104e-01 (2.6836e-01)	Acc@1  90.62 ( 91.47)	Acc@5  97.66 ( 99.37)
Epoch: [48][240/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0615e-01 (2.6944e-01)	Acc@1  92.19 ( 91.44)	Acc@5  98.44 ( 99.37)
Epoch: [48][250/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0801e-01 (2.6869e-01)	Acc@1  94.53 ( 91.46)	Acc@5  99.22 ( 99.35)
Epoch: [48][260/391]	Time  0.033 ( 0.025)	Data  0.004 ( 0.002)	Loss 2.4194e-01 (2.6846e-01)	Acc@1  92.97 ( 91.48)	Acc@5  99.22 ( 99.35)
Epoch: [48][270/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7808e-01 (2.6872e-01)	Acc@1  89.06 ( 91.47)	Acc@5  99.22 ( 99.34)
Epoch: [48][280/391]	Time  0.035 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.1179e-01 (2.6820e-01)	Acc@1  92.97 ( 91.51)	Acc@5 100.00 ( 99.35)
Epoch: [48][290/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9712e-01 (2.6834e-01)	Acc@1  91.41 ( 91.52)	Acc@5  97.66 ( 99.34)
Epoch: [48][300/391]	Time  0.022 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.8518e-01 (2.6940e-01)	Acc@1  94.53 ( 91.49)	Acc@5 100.00 ( 99.33)
Epoch: [48][310/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.0493e-01 (2.7057e-01)	Acc@1  91.41 ( 91.46)	Acc@5  99.22 ( 99.32)
Epoch: [48][320/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.8467e-01 (2.7150e-01)	Acc@1  92.97 ( 91.45)	Acc@5  99.22 ( 99.31)
Epoch: [48][330/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2788e-01 (2.7219e-01)	Acc@1  87.50 ( 91.42)	Acc@5  98.44 ( 99.31)
Epoch: [48][340/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.8638e-01 (2.7261e-01)	Acc@1  92.19 ( 91.41)	Acc@5 100.00 ( 99.30)
Epoch: [48][350/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6516e-01 (2.7255e-01)	Acc@1  92.97 ( 91.40)	Acc@5 100.00 ( 99.30)
Epoch: [48][360/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.002)	Loss 3.7476e-01 (2.7368e-01)	Acc@1  90.62 ( 91.38)	Acc@5  98.44 ( 99.29)
Epoch: [48][370/391]	Time  0.037 ( 0.025)	Data  0.002 ( 0.002)	Loss 3.1299e-01 (2.7451e-01)	Acc@1  89.06 ( 91.33)	Acc@5 100.00 ( 99.29)
Epoch: [48][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.6255e-01 (2.7502e-01)	Acc@1  90.62 ( 91.34)	Acc@5 100.00 ( 99.28)
Epoch: [48][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.5718e-01 (2.7414e-01)	Acc@1  91.25 ( 91.39)	Acc@5  97.50 ( 99.28)
## e[48] optimizer.zero_grad (sum) time: 0.10909843444824219
## e[48]       loss.backward (sum) time: 2.1834239959716797
## e[48]      optimizer.step (sum) time: 0.90494704246521
## epoch[48] training(only) time: 9.910385370254517
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.7363e+00 (1.7363e+00)	Acc@1  65.00 ( 65.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.014 ( 0.025)	Loss 1.7803e+00 (1.6672e+00)	Acc@1  64.00 ( 65.18)	Acc@5  90.00 ( 88.18)
Test: [ 20/100]	Time  0.017 ( 0.020)	Loss 1.6396e+00 (1.6383e+00)	Acc@1  70.00 ( 65.71)	Acc@5  85.00 ( 88.38)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 2.0723e+00 (1.6407e+00)	Acc@1  58.00 ( 65.29)	Acc@5  85.00 ( 88.23)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4668e+00 (1.6250e+00)	Acc@1  65.00 ( 65.49)	Acc@5  90.00 ( 88.27)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.3516e+00 (1.6415e+00)	Acc@1  71.00 ( 65.67)	Acc@5  87.00 ( 87.92)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.7246e+00 (1.6200e+00)	Acc@1  64.00 ( 66.03)	Acc@5  89.00 ( 88.07)
Test: [ 70/100]	Time  0.015 ( 0.016)	Loss 1.5293e+00 (1.6054e+00)	Acc@1  66.00 ( 66.21)	Acc@5  93.00 ( 88.25)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.6221e+00 (1.6142e+00)	Acc@1  65.00 ( 65.99)	Acc@5  84.00 ( 88.14)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.8369e+00 (1.6072e+00)	Acc@1  61.00 ( 66.04)	Acc@5  86.00 ( 88.19)
 * Acc@1 66.330 Acc@5 88.230
### epoch[48] execution time: 11.507990837097168
EPOCH 49
REMOVING: features.module.1.bias
i:   0, name:       features.module.4.weight  changing lr from: 0.001327004477873133   to: 0.001065945009345353
i:   1, name:         features.module.4.bias  changing lr from: 0.001736984814452346   to: 0.001302408059092973
i:   2, name:       features.module.5.weight  changing lr from: 0.002293274147895485   to: 0.001697572232196758
i:   3, name:         features.module.5.bias  changing lr from: 0.002982858606465077   to: 0.002238126968221459
i:   4, name:       features.module.8.weight  changing lr from: 0.003793301689785655   to: 0.002911277229054998
i:   5, name:         features.module.8.bias  changing lr from: 0.004712773682817761   to: 0.003704786552229072
i:   6, name:       features.module.9.weight  changing lr from: 0.005730069390097569   to: 0.004607006611417546
i:   7, name:         features.module.9.bias  changing lr from: 0.006834616113150077   to: 0.005606895380428339
i:   8, name:      features.module.11.weight  changing lr from: 0.008016473560271426   to: 0.006694025756328581
i:   9, name:        features.module.11.bias  changing lr from: 0.009266327163777642   to: 0.007858586274252271
i:  10, name:      features.module.12.weight  changing lr from: 0.010575476085391909   to: 0.009091375341745995
i:  11, name:        features.module.12.bias  changing lr from: 0.011935817015226863   to: 0.010383790234332658
i:  12, name:      features.module.15.weight  changing lr from: 0.013339824712987954   to: 0.011727811925929994
i:  13, name:        features.module.15.bias  changing lr from: 0.014780530100507665   to: 0.013115986677112269
i:  14, name:      features.module.16.weight  changing lr from: 0.016251496591312468   to: 0.014541405169978254
i:  15, name:        features.module.16.bias  changing lr from: 0.017746795234335932   to: 0.015997679859466624
i:  16, name:      features.module.18.weight  changing lr from: 0.019260979153826335   to: 0.017478921106154323
i:  17, name:        features.module.18.bias  changing lr from: 0.020789057684666552   to: 0.018979712563661412
i:  18, name:      features.module.19.weight  changing lr from: 0.022326470530500173   to: 0.020495086213572294
i:  19, name:        features.module.19.bias  changing lr from: 0.023869062210065829   to: 0.022020497371102035
i:  20, name:      features.module.22.weight  changing lr from: 0.025413057003891966   to: 0.023551799924486125
i:  21, name:        features.module.22.bias  changing lr from: 0.026955034567988354   to: 0.025085222019220716
i:  22, name:      features.module.23.weight  changing lr from: 0.028491906342460946   to: 0.026617342353868152
i:  23, name:        features.module.23.bias  changing lr from: 0.030020892850236888   to: 0.028145067216294263
i:  24, name:      features.module.25.weight  changing lr from: 0.031539501953558292   to: 0.029665608357119389
i:  25, name:        features.module.25.bias  changing lr from: 0.033045508112906241   to: 0.031176461770120085
i:  26, name:      features.module.26.weight  changing lr from: 0.034536932673945080   to: 0.032675387426666584
i:  27, name:        features.module.26.bias  changing lr from: 0.036012025192393528   to: 0.034160389992438860
i:  28, name:            classifier.0.weight  changing lr from: 0.037469245793956295   to: 0.035629700539117078
i:  29, name:              classifier.0.bias  changing lr from: 0.038907248556169539   to: 0.037081759251032652
i:  30, name:            classifier.3.weight  changing lr from: 0.040324865890857972   to: 0.038515199116489152
i:  31, name:              classifier.3.bias  changing lr from: 0.041721093899548659   to: 0.039928830585265453
i:  32, name:            classifier.6.weight  changing lr from: 0.043095078669357481   to: 0.041321627167384981
i:  33, name:              classifier.6.bias  changing lr from: 0.044446103473314680   to: 0.042692711943303131



# Switched to train mode...
Epoch: [49][  0/391]	Time  0.177 ( 0.177)	Data  0.144 ( 0.144)	Loss 2.8296e-01 (2.8296e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [49][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.015)	Loss 2.2559e-01 (2.3406e-01)	Acc@1  93.75 ( 93.04)	Acc@5  99.22 ( 99.50)
Epoch: [49][ 20/391]	Time  0.024 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.2593e-01 (2.4365e-01)	Acc@1  88.28 ( 92.52)	Acc@5 100.00 ( 99.44)
Epoch: [49][ 30/391]	Time  0.040 ( 0.029)	Data  0.002 ( 0.006)	Loss 2.6953e-01 (2.3803e-01)	Acc@1  88.28 ( 92.39)	Acc@5 100.00 ( 99.40)
Epoch: [49][ 40/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.1421e-01 (2.3692e-01)	Acc@1  89.06 ( 92.28)	Acc@5  97.66 ( 99.37)
Epoch: [49][ 50/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.7539e-01 (2.3632e-01)	Acc@1  91.41 ( 92.33)	Acc@5  98.44 ( 99.31)
Epoch: [49][ 60/391]	Time  0.022 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.9666e-01 (2.3044e-01)	Acc@1  93.75 ( 92.51)	Acc@5  99.22 ( 99.32)
Epoch: [49][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9482e-01 (2.3047e-01)	Acc@1  91.41 ( 92.48)	Acc@5 100.00 ( 99.37)
Epoch: [49][ 80/391]	Time  0.021 ( 0.026)	Data  0.005 ( 0.004)	Loss 1.7395e-01 (2.2914e-01)	Acc@1  94.53 ( 92.59)	Acc@5 100.00 ( 99.39)
Epoch: [49][ 90/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.2375e-01 (2.2738e-01)	Acc@1  92.97 ( 92.69)	Acc@5 100.00 ( 99.42)
Epoch: [49][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1301e-01 (2.2635e-01)	Acc@1  91.41 ( 92.74)	Acc@5  99.22 ( 99.44)
Epoch: [49][110/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9263e-01 (2.2490e-01)	Acc@1  92.19 ( 92.76)	Acc@5 100.00 ( 99.44)
Epoch: [49][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4453e-01 (2.2459e-01)	Acc@1  93.75 ( 92.81)	Acc@5 100.00 ( 99.46)
Epoch: [49][130/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8860e-01 (2.2703e-01)	Acc@1  92.19 ( 92.77)	Acc@5 100.00 ( 99.45)
Epoch: [49][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4805e-01 (2.2782e-01)	Acc@1  92.97 ( 92.78)	Acc@5  99.22 ( 99.46)
Epoch: [49][150/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6833e-01 (2.3151e-01)	Acc@1  94.53 ( 92.70)	Acc@5 100.00 ( 99.44)
Epoch: [49][160/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3035e-01 (2.3308e-01)	Acc@1  92.19 ( 92.64)	Acc@5 100.00 ( 99.45)
Epoch: [49][170/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.9373e-01 (2.3418e-01)	Acc@1  92.19 ( 92.62)	Acc@5 100.00 ( 99.43)
Epoch: [49][180/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.0444e-01 (2.3336e-01)	Acc@1  89.06 ( 92.63)	Acc@5 100.00 ( 99.46)
Epoch: [49][190/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.6489e-01 (2.3482e-01)	Acc@1  91.41 ( 92.54)	Acc@5 100.00 ( 99.47)
Epoch: [49][200/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3157e-01 (2.3694e-01)	Acc@1  91.41 ( 92.45)	Acc@5 100.00 ( 99.46)
Epoch: [49][210/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.5244e-01 (2.3923e-01)	Acc@1  90.62 ( 92.40)	Acc@5  99.22 ( 99.46)
Epoch: [49][220/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.5781e-01 (2.4066e-01)	Acc@1  89.84 ( 92.34)	Acc@5  99.22 ( 99.44)
Epoch: [49][230/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3423e-01 (2.4146e-01)	Acc@1  89.84 ( 92.28)	Acc@5  98.44 ( 99.44)
Epoch: [49][240/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7979e-01 (2.4253e-01)	Acc@1  92.97 ( 92.23)	Acc@5  99.22 ( 99.43)
Epoch: [49][250/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4277e-01 (2.4224e-01)	Acc@1  89.84 ( 92.23)	Acc@5  99.22 ( 99.43)
Epoch: [49][260/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8882e-01 (2.4317e-01)	Acc@1  89.06 ( 92.19)	Acc@5 100.00 ( 99.43)
Epoch: [49][270/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5049e-01 (2.4329e-01)	Acc@1  93.75 ( 92.22)	Acc@5 100.00 ( 99.44)
Epoch: [49][280/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.2427e-01 (2.4294e-01)	Acc@1  96.09 ( 92.22)	Acc@5 100.00 ( 99.43)
Epoch: [49][290/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.1240e-01 (2.4285e-01)	Acc@1  92.19 ( 92.21)	Acc@5 100.00 ( 99.43)
Epoch: [49][300/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.5562e-01 (2.4280e-01)	Acc@1  90.62 ( 92.21)	Acc@5  99.22 ( 99.42)
Epoch: [49][310/391]	Time  0.038 ( 0.025)	Data  0.008 ( 0.002)	Loss 3.1982e-01 (2.4419e-01)	Acc@1  90.62 ( 92.16)	Acc@5 100.00 ( 99.42)
Epoch: [49][320/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9189e-01 (2.4396e-01)	Acc@1  92.97 ( 92.17)	Acc@5 100.00 ( 99.42)
Epoch: [49][330/391]	Time  0.039 ( 0.025)	Data  0.003 ( 0.002)	Loss 1.6980e-01 (2.4417e-01)	Acc@1  93.75 ( 92.17)	Acc@5 100.00 ( 99.42)
Epoch: [49][340/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8457e-01 (2.4491e-01)	Acc@1  92.97 ( 92.14)	Acc@5 100.00 ( 99.42)
Epoch: [49][350/391]	Time  0.020 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.7612e-01 (2.4411e-01)	Acc@1  88.28 ( 92.16)	Acc@5 100.00 ( 99.42)
Epoch: [49][360/391]	Time  0.028 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3291e-01 (2.4428e-01)	Acc@1  92.19 ( 92.16)	Acc@5  99.22 ( 99.42)
Epoch: [49][370/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6577e-01 (2.4433e-01)	Acc@1  93.75 ( 92.15)	Acc@5 100.00 ( 99.43)
Epoch: [49][380/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.2471e-01 (2.4449e-01)	Acc@1  90.62 ( 92.14)	Acc@5  99.22 ( 99.44)
Epoch: [49][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2803e-01 (2.4463e-01)	Acc@1  92.50 ( 92.14)	Acc@5 100.00 ( 99.43)
## e[49] optimizer.zero_grad (sum) time: 0.10480380058288574
## e[49]       loss.backward (sum) time: 2.114563465118408
## e[49]      optimizer.step (sum) time: 0.8694593906402588
## epoch[49] training(only) time: 9.861181259155273
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.8301e+00 (1.8301e+00)	Acc@1  64.00 ( 64.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.012 ( 0.025)	Loss 1.7812e+00 (1.7234e+00)	Acc@1  63.00 ( 65.09)	Acc@5  90.00 ( 86.82)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.7432e+00 (1.6751e+00)	Acc@1  68.00 ( 65.76)	Acc@5  87.00 ( 87.52)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.9287e+00 (1.6571e+00)	Acc@1  60.00 ( 65.84)	Acc@5  84.00 ( 87.45)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.5898e+00 (1.6445e+00)	Acc@1  62.00 ( 65.73)	Acc@5  91.00 ( 87.78)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.4648e+00 (1.6632e+00)	Acc@1  73.00 ( 65.92)	Acc@5  88.00 ( 87.61)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6348e+00 (1.6434e+00)	Acc@1  62.00 ( 66.15)	Acc@5  89.00 ( 87.87)
Test: [ 70/100]	Time  0.014 ( 0.015)	Loss 1.6748e+00 (1.6367e+00)	Acc@1  65.00 ( 66.24)	Acc@5  87.00 ( 88.11)
Test: [ 80/100]	Time  0.011 ( 0.015)	Loss 1.6396e+00 (1.6365e+00)	Acc@1  61.00 ( 66.12)	Acc@5  84.00 ( 88.10)
Test: [ 90/100]	Time  0.032 ( 0.015)	Loss 1.8789e+00 (1.6288e+00)	Acc@1  63.00 ( 66.35)	Acc@5  88.00 ( 88.20)
 * Acc@1 66.460 Acc@5 88.240
### epoch[49] execution time: 11.405535221099854
EPOCH 50
REMOVING: features.module.4.weight
i:   0, name:         features.module.4.bias  changing lr from: 0.001302408059092973   to: 0.001057998336420823
i:   1, name:       features.module.5.weight  changing lr from: 0.001697572232196758   to: 0.001283475717615983
i:   2, name:         features.module.5.bias  changing lr from: 0.002238126968221459   to: 0.001666339355777762
i:   3, name:       features.module.8.weight  changing lr from: 0.002911277229054998   to: 0.002193514356672165
i:   4, name:         features.module.8.bias  changing lr from: 0.003704786552229072   to: 0.002852424342195519
i:   5, name:       features.module.9.weight  changing lr from: 0.004607006611417546   to: 0.003631034074424244
i:   6, name:         features.module.9.bias  changing lr from: 0.005606895380428339   to: 0.004517878963690352
i:   7, name:      features.module.11.weight  changing lr from: 0.006694025756328581   to: 0.005502083478797103
i:   8, name:        features.module.11.bias  changing lr from: 0.007858586274252271   to: 0.006573370248145276
i:   9, name:      features.module.12.weight  changing lr from: 0.009091375341745995   to: 0.007722061427650911
i:  10, name:        features.module.12.bias  changing lr from: 0.010383790234332658   to: 0.008939073715724281
i:  11, name:      features.module.15.weight  changing lr from: 0.011727811925929994   to: 0.010215908217414166
i:  12, name:        features.module.15.bias  changing lr from: 0.013115986677112269   to: 0.011544636198776704
i:  13, name:      features.module.16.weight  changing lr from: 0.014541405169978254   to: 0.012917881627948991
i:  14, name:        features.module.16.bias  changing lr from: 0.015997679859466624   to: 0.014328801270398060
i:  15, name:      features.module.18.weight  changing lr from: 0.017478921106154323   to: 0.015771062991351715
i:  16, name:        features.module.18.bias  changing lr from: 0.018979712563661412   to: 0.017238822817382239
i:  17, name:      features.module.19.weight  changing lr from: 0.020495086213572294   to: 0.018726701220376413
i:  18, name:        features.module.19.bias  changing lr from: 0.022020497371102035   to: 0.020229759009557591
i:  19, name:      features.module.22.weight  changing lr from: 0.023551799924486125   to: 0.021743473149730293
i:  20, name:        features.module.22.bias  changing lr from: 0.025085222019220716   to: 0.023263712765463507
i:  21, name:      features.module.23.weight  changing lr from: 0.026617342353868152   to: 0.024786715540527755
i:  22, name:        features.module.23.bias  changing lr from: 0.028145067216294263   to: 0.026309064678655555
i:  23, name:      features.module.25.weight  changing lr from: 0.029665608357119389   to: 0.027827666554765720
i:  24, name:        features.module.25.bias  changing lr from: 0.031176461770120085   to: 0.029339729154421002
i:  25, name:      features.module.26.weight  changing lr from: 0.032675387426666584   to: 0.030842741372785643
i:  26, name:        features.module.26.bias  changing lr from: 0.034160389992438860   to: 0.032334453222096653
i:  27, name:            classifier.0.weight  changing lr from: 0.035629700539117078   to: 0.033812856978100658
i:  28, name:              classifier.0.bias  changing lr from: 0.037081759251032652   to: 0.035276169280548220
i:  29, name:            classifier.3.weight  changing lr from: 0.038515199116489152   to: 0.036722814190233155
i:  30, name:              classifier.3.bias  changing lr from: 0.039928830585265453   to: 0.038151407194834709
i:  31, name:            classifier.6.weight  changing lr from: 0.041321627167384981   to: 0.039560740147619393
i:  32, name:              classifier.6.bias  changing lr from: 0.042692711943303131   to: 0.040949767116588057



# Switched to train mode...
Epoch: [50][  0/391]	Time  0.176 ( 0.176)	Data  0.149 ( 0.149)	Loss 1.5747e-01 (1.5747e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.030 ( 0.039)	Data  0.000 ( 0.015)	Loss 2.0251e-01 (2.1288e-01)	Acc@1  93.75 ( 93.47)	Acc@5  99.22 ( 99.64)
Epoch: [50][ 20/391]	Time  0.020 ( 0.031)	Data  0.002 ( 0.009)	Loss 2.9102e-01 (2.1640e-01)	Acc@1  90.62 ( 93.23)	Acc@5  98.44 ( 99.52)
Epoch: [50][ 30/391]	Time  0.025 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.2927e-01 (2.1727e-01)	Acc@1  94.53 ( 93.12)	Acc@5 100.00 ( 99.50)
Epoch: [50][ 40/391]	Time  0.019 ( 0.028)	Data  0.002 ( 0.006)	Loss 2.0691e-01 (2.1912e-01)	Acc@1  94.53 ( 93.14)	Acc@5 100.00 ( 99.47)
Epoch: [50][ 50/391]	Time  0.031 ( 0.027)	Data  0.000 ( 0.005)	Loss 4.2334e-01 (2.2010e-01)	Acc@1  91.41 ( 93.23)	Acc@5  98.44 ( 99.49)
Epoch: [50][ 60/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7417e-01 (2.2096e-01)	Acc@1  92.97 ( 93.17)	Acc@5  98.44 ( 99.47)
Epoch: [50][ 70/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6223e-01 (2.1892e-01)	Acc@1  96.09 ( 93.34)	Acc@5  99.22 ( 99.52)
Epoch: [50][ 80/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.3706e-01 (2.2000e-01)	Acc@1  95.31 ( 93.33)	Acc@5 100.00 ( 99.52)
Epoch: [50][ 90/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.6001e-01 (2.1922e-01)	Acc@1  93.75 ( 93.29)	Acc@5 100.00 ( 99.52)
Epoch: [50][100/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.7114e-01 (2.1807e-01)	Acc@1  96.09 ( 93.33)	Acc@5 100.00 ( 99.55)
Epoch: [50][110/391]	Time  0.034 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.7393e-01 (2.2165e-01)	Acc@1  92.19 ( 93.19)	Acc@5  99.22 ( 99.50)
Epoch: [50][120/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2104e-01 (2.2231e-01)	Acc@1  88.28 ( 93.18)	Acc@5 100.00 ( 99.51)
Epoch: [50][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1533e-01 (2.2403e-01)	Acc@1  92.19 ( 93.08)	Acc@5  99.22 ( 99.52)
Epoch: [50][140/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6626e-01 (2.2347e-01)	Acc@1  94.53 ( 93.10)	Acc@5  99.22 ( 99.51)
Epoch: [50][150/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3965e-01 (2.2241e-01)	Acc@1  96.09 ( 93.14)	Acc@5 100.00 ( 99.52)
Epoch: [50][160/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.1958e-01 (2.2255e-01)	Acc@1  90.62 ( 93.14)	Acc@5  98.44 ( 99.52)
Epoch: [50][170/391]	Time  0.031 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1118e-01 (2.2144e-01)	Acc@1  92.19 ( 93.15)	Acc@5 100.00 ( 99.53)
Epoch: [50][180/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1558e-01 (2.2042e-01)	Acc@1  94.53 ( 93.20)	Acc@5  99.22 ( 99.53)
Epoch: [50][190/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.5229e-01 (2.2091e-01)	Acc@1  89.84 ( 93.19)	Acc@5  98.44 ( 99.52)
Epoch: [50][200/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.3984e-01 (2.2177e-01)	Acc@1  91.41 ( 93.17)	Acc@5  96.88 ( 99.51)
Epoch: [50][210/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0663e-01 (2.2151e-01)	Acc@1  96.88 ( 93.15)	Acc@5 100.00 ( 99.51)
Epoch: [50][220/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.5767e-01 (2.2172e-01)	Acc@1  88.28 ( 93.14)	Acc@5  98.44 ( 99.51)
Epoch: [50][230/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8809e-01 (2.2280e-01)	Acc@1  92.19 ( 93.14)	Acc@5 100.00 ( 99.51)
Epoch: [50][240/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8457e-01 (2.2416e-01)	Acc@1  93.75 ( 93.07)	Acc@5 100.00 ( 99.50)
Epoch: [50][250/391]	Time  0.038 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.9861e-01 (2.2441e-01)	Acc@1  92.19 ( 93.07)	Acc@5 100.00 ( 99.50)
Epoch: [50][260/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.003)	Loss 3.3911e-01 (2.2437e-01)	Acc@1  88.28 ( 93.04)	Acc@5  98.44 ( 99.50)
Epoch: [50][270/391]	Time  0.033 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.9443e-01 (2.2412e-01)	Acc@1  91.41 ( 93.06)	Acc@5  98.44 ( 99.50)
Epoch: [50][280/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8687e-01 (2.2318e-01)	Acc@1  89.06 ( 93.07)	Acc@5 100.00 ( 99.51)
Epoch: [50][290/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (2.2311e-01)	Acc@1  95.31 ( 93.07)	Acc@5 100.00 ( 99.51)
Epoch: [50][300/391]	Time  0.035 ( 0.025)	Data  0.004 ( 0.002)	Loss 2.1875e-01 (2.2193e-01)	Acc@1  93.75 ( 93.09)	Acc@5  99.22 ( 99.52)
Epoch: [50][310/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4841e-01 (2.2249e-01)	Acc@1  92.19 ( 93.05)	Acc@5 100.00 ( 99.52)
Epoch: [50][320/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0679e-01 (2.2327e-01)	Acc@1  94.53 ( 93.03)	Acc@5  99.22 ( 99.51)
Epoch: [50][330/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.2925e-01 (2.2436e-01)	Acc@1  92.97 ( 92.99)	Acc@5  99.22 ( 99.51)
Epoch: [50][340/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.1030e-01 (2.2536e-01)	Acc@1  90.62 ( 92.95)	Acc@5 100.00 ( 99.51)
Epoch: [50][350/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5220e-01 (2.2526e-01)	Acc@1  89.06 ( 92.95)	Acc@5  98.44 ( 99.50)
Epoch: [50][360/391]	Time  0.035 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.5171e-01 (2.2576e-01)	Acc@1  91.41 ( 92.93)	Acc@5  99.22 ( 99.50)
Epoch: [50][370/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.7700e-01 (2.2554e-01)	Acc@1  94.53 ( 92.93)	Acc@5 100.00 ( 99.50)
Epoch: [50][380/391]	Time  0.037 ( 0.025)	Data  0.005 ( 0.002)	Loss 1.4502e-01 (2.2503e-01)	Acc@1  94.53 ( 92.94)	Acc@5 100.00 ( 99.51)
Epoch: [50][390/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.0789e-01 (2.2583e-01)	Acc@1  91.25 ( 92.90)	Acc@5 100.00 ( 99.51)
## e[50] optimizer.zero_grad (sum) time: 0.1024174690246582
## e[50]       loss.backward (sum) time: 2.0473146438598633
## e[50]      optimizer.step (sum) time: 0.8509256839752197
## epoch[50] training(only) time: 9.848355293273926
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.6748e+00 (1.6748e+00)	Acc@1  68.00 ( 68.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.8027e+00 (1.7010e+00)	Acc@1  65.00 ( 66.82)	Acc@5  87.00 ( 87.64)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.6533e+00 (1.6475e+00)	Acc@1  69.00 ( 67.67)	Acc@5  87.00 ( 88.10)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.9316e+00 (1.6431e+00)	Acc@1  58.00 ( 67.35)	Acc@5  84.00 ( 87.94)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 1.3398e+00 (1.6361e+00)	Acc@1  69.00 ( 67.27)	Acc@5  90.00 ( 88.10)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.5107e+00 (1.6620e+00)	Acc@1  70.00 ( 67.14)	Acc@5  87.00 ( 87.96)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.6094e+00 (1.6472e+00)	Acc@1  68.00 ( 67.25)	Acc@5  87.00 ( 87.98)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.7158e+00 (1.6447e+00)	Acc@1  65.00 ( 67.14)	Acc@5  89.00 ( 88.14)
Test: [ 80/100]	Time  0.011 ( 0.015)	Loss 1.7861e+00 (1.6553e+00)	Acc@1  62.00 ( 66.95)	Acc@5  83.00 ( 87.98)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.9766e+00 (1.6491e+00)	Acc@1  60.00 ( 67.01)	Acc@5  86.00 ( 88.08)
 * Acc@1 67.110 Acc@5 88.170
### epoch[50] execution time: 11.41144609451294
EPOCH 51
REMOVING: features.module.4.bias
i:   0, name:       features.module.5.weight  changing lr from: 0.001283475717615983   to: 0.001052509927895550
i:   1, name:         features.module.5.bias  changing lr from: 0.001666339355777762   to: 0.001269523731668031
i:   2, name:       features.module.8.weight  changing lr from: 0.002193514356672165   to: 0.001642464808983661
i:   3, name:         features.module.8.bias  changing lr from: 0.002852424342195519   to: 0.002158491750942403
i:   4, name:       features.module.9.weight  changing lr from: 0.003631034074424244   to: 0.002805246126471871
i:   5, name:         features.module.9.bias  changing lr from: 0.004517878963690352   to: 0.003570894488980768
i:   6, name:      features.module.11.weight  changing lr from: 0.005502083478797103   to: 0.004444157654973734
i:   7, name:        features.module.11.bias  changing lr from: 0.006573370248145276   to: 0.005414329195744293
i:   8, name:      features.module.12.weight  changing lr from: 0.007722061427650911   to: 0.006471284864727011
i:   9, name:        features.module.12.bias  changing lr from: 0.008939073715724281   to: 0.007605484479981383
i:  10, name:      features.module.15.weight  changing lr from: 0.010215908217414166   to: 0.008807967594424225
i:  11, name:        features.module.15.bias  changing lr from: 0.011544636198776704   to: 0.010070344116015654
i:  12, name:      features.module.16.weight  changing lr from: 0.012917881627948991   to: 0.011384780885873486
i:  13, name:        features.module.16.bias  changing lr from: 0.014328801270398060   to: 0.012743985083642016
i:  14, name:      features.module.18.weight  changing lr from: 0.015771062991351715   to: 0.014141185205564850
i:  15, name:        features.module.18.bias  changing lr from: 0.017238822817382239   to: 0.015570110250643410
i:  16, name:      features.module.19.weight  changing lr from: 0.018726701220376413   to: 0.017024967652978808
i:  17, name:        features.module.19.bias  changing lr from: 0.020229759009557591   to: 0.018500420412826540
i:  18, name:      features.module.22.weight  changing lr from: 0.021743473149730293   to: 0.019991563803983550
i:  19, name:        features.module.22.bias  changing lr from: 0.023263712765463507   to: 0.021493901969851625
i:  20, name:      features.module.23.weight  changing lr from: 0.024786715540527755   to: 0.023003324663890979
i:  21, name:        features.module.23.bias  changing lr from: 0.026309064678655555   to: 0.024516084341276285
i:  22, name:      features.module.25.weight  changing lr from: 0.027827666554765720   to: 0.026028773766531812
i:  23, name:        features.module.25.bias  changing lr from: 0.029339729154421002   to: 0.027538304265961145
i:  24, name:      features.module.26.weight  changing lr from: 0.030842741372785643   to: 0.029041884723079114
i:  25, name:        features.module.26.bias  changing lr from: 0.032334453222096653   to: 0.030537001389341192
i:  26, name:            classifier.0.weight  changing lr from: 0.033812856978100658   to: 0.032021398560657097
i:  27, name:              classifier.0.bias  changing lr from: 0.035276169280548220   to: 0.033493060151942360
i:  28, name:            classifier.3.weight  changing lr from: 0.036722814190233155   to: 0.034950192186825914
i:  29, name:              classifier.3.bias  changing lr from: 0.038151407194834709   to: 0.036391206207178006
i:  30, name:            classifier.6.weight  changing lr from: 0.039560740147619393   to: 0.037814703596973101
i:  31, name:              classifier.6.bias  changing lr from: 0.040949767116588057   to: 0.039219460806832702



# Switched to train mode...
Epoch: [51][  0/391]	Time  0.170 ( 0.170)	Data  0.146 ( 0.146)	Loss 1.3342e-01 (1.3342e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.025 ( 0.038)	Data  0.005 ( 0.015)	Loss 1.0339e-01 (1.7182e-01)	Acc@1  96.88 ( 94.67)	Acc@5 100.00 ( 99.86)
Epoch: [51][ 20/391]	Time  0.029 ( 0.031)	Data  0.009 ( 0.009)	Loss 2.8271e-01 (1.8937e-01)	Acc@1  90.62 ( 94.05)	Acc@5 100.00 ( 99.59)
Epoch: [51][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.3757e-01 (1.8447e-01)	Acc@1  96.09 ( 94.41)	Acc@5  99.22 ( 99.60)
Epoch: [51][ 40/391]	Time  0.038 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.3831e-01 (1.8081e-01)	Acc@1  93.75 ( 94.36)	Acc@5 100.00 ( 99.56)
Epoch: [51][ 50/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.2107e-01 (1.8161e-01)	Acc@1  93.75 ( 94.44)	Acc@5 100.00 ( 99.49)
Epoch: [51][ 60/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.8567e-01 (1.8045e-01)	Acc@1  92.97 ( 94.44)	Acc@5 100.00 ( 99.55)
Epoch: [51][ 70/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.004)	Loss 1.6064e-01 (1.8346e-01)	Acc@1  95.31 ( 94.33)	Acc@5 100.00 ( 99.55)
Epoch: [51][ 80/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.0288e-01 (1.8315e-01)	Acc@1  92.97 ( 94.34)	Acc@5  99.22 ( 99.55)
Epoch: [51][ 90/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4636e-01 (1.8648e-01)	Acc@1  96.09 ( 94.25)	Acc@5 100.00 ( 99.54)
Epoch: [51][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8271e-01 (1.9004e-01)	Acc@1  90.62 ( 94.11)	Acc@5  98.44 ( 99.51)
Epoch: [51][110/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.5586e-01 (1.9008e-01)	Acc@1  92.97 ( 94.08)	Acc@5 100.00 ( 99.54)
Epoch: [51][120/391]	Time  0.026 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.2988e-01 (1.9246e-01)	Acc@1  95.31 ( 94.00)	Acc@5 100.00 ( 99.54)
Epoch: [51][130/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.3645e-01 (1.9440e-01)	Acc@1  92.97 ( 93.95)	Acc@5 100.00 ( 99.55)
Epoch: [51][140/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7883e-01 (1.9382e-01)	Acc@1  92.19 ( 93.98)	Acc@5 100.00 ( 99.56)
Epoch: [51][150/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0886e-01 (1.9343e-01)	Acc@1  94.53 ( 93.97)	Acc@5  99.22 ( 99.59)
Epoch: [51][160/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 3.7378e-01 (1.9604e-01)	Acc@1  89.06 ( 93.89)	Acc@5  99.22 ( 99.57)
Epoch: [51][170/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.0056e-01 (1.9586e-01)	Acc@1  92.97 ( 93.85)	Acc@5  99.22 ( 99.59)
Epoch: [51][180/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.7832e-01 (1.9679e-01)	Acc@1  91.41 ( 93.78)	Acc@5 100.00 ( 99.59)
Epoch: [51][190/391]	Time  0.036 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.2827e-01 (1.9671e-01)	Acc@1  92.97 ( 93.77)	Acc@5 100.00 ( 99.61)
Epoch: [51][200/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.8867e-02 (1.9491e-01)	Acc@1  96.88 ( 93.79)	Acc@5 100.00 ( 99.62)
Epoch: [51][210/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.5781e-01 (1.9728e-01)	Acc@1  90.62 ( 93.74)	Acc@5 100.00 ( 99.61)
Epoch: [51][220/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9836e-01 (1.9626e-01)	Acc@1  90.62 ( 93.77)	Acc@5 100.00 ( 99.62)
Epoch: [51][230/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.8188e-01 (1.9707e-01)	Acc@1  95.31 ( 93.78)	Acc@5 100.00 ( 99.62)
Epoch: [51][240/391]	Time  0.029 ( 0.025)	Data  0.003 ( 0.003)	Loss 2.5952e-01 (1.9904e-01)	Acc@1  90.62 ( 93.72)	Acc@5 100.00 ( 99.62)
Epoch: [51][250/391]	Time  0.036 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9165e-01 (1.9914e-01)	Acc@1  95.31 ( 93.73)	Acc@5  99.22 ( 99.62)
Epoch: [51][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4570e-01 (1.9995e-01)	Acc@1  86.72 ( 93.70)	Acc@5  98.44 ( 99.60)
Epoch: [51][270/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5320e-01 (1.9896e-01)	Acc@1  93.75 ( 93.73)	Acc@5 100.00 ( 99.61)
Epoch: [51][280/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1533e-01 (1.9876e-01)	Acc@1  93.75 ( 93.73)	Acc@5  99.22 ( 99.62)
Epoch: [51][290/391]	Time  0.042 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3401e-01 (1.9958e-01)	Acc@1  92.97 ( 93.69)	Acc@5 100.00 ( 99.62)
Epoch: [51][300/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4307e-01 (1.9940e-01)	Acc@1  94.53 ( 93.68)	Acc@5 100.00 ( 99.63)
Epoch: [51][310/391]	Time  0.020 ( 0.025)	Data  0.003 ( 0.002)	Loss 2.9053e-01 (1.9950e-01)	Acc@1  92.19 ( 93.68)	Acc@5  98.44 ( 99.62)
Epoch: [51][320/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.9639e-01 (1.9942e-01)	Acc@1  89.06 ( 93.67)	Acc@5  98.44 ( 99.61)
Epoch: [51][330/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4075e-01 (1.9956e-01)	Acc@1  96.09 ( 93.65)	Acc@5 100.00 ( 99.61)
Epoch: [51][340/391]	Time  0.028 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.2494e-01 (2.0012e-01)	Acc@1  96.09 ( 93.64)	Acc@5 100.00 ( 99.61)
Epoch: [51][350/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0032e-01 (2.0022e-01)	Acc@1  94.53 ( 93.63)	Acc@5 100.00 ( 99.61)
Epoch: [51][360/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.0728e-01 (2.0062e-01)	Acc@1  92.97 ( 93.63)	Acc@5 100.00 ( 99.61)
Epoch: [51][370/391]	Time  0.028 ( 0.025)	Data  0.006 ( 0.002)	Loss 1.7395e-01 (2.0143e-01)	Acc@1  95.31 ( 93.62)	Acc@5 100.00 ( 99.61)
Epoch: [51][380/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4075e-01 (2.0143e-01)	Acc@1  93.75 ( 93.61)	Acc@5 100.00 ( 99.60)
Epoch: [51][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.8013e-01 (2.0207e-01)	Acc@1  85.00 ( 93.59)	Acc@5  98.75 ( 99.60)
## e[51] optimizer.zero_grad (sum) time: 0.09841465950012207
## e[51]       loss.backward (sum) time: 2.042151927947998
## e[51]      optimizer.step (sum) time: 0.811680793762207
## epoch[51] training(only) time: 9.807704210281372
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.8555e+00 (1.8555e+00)	Acc@1  67.00 ( 67.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.020 ( 0.027)	Loss 1.6562e+00 (1.7338e+00)	Acc@1  64.00 ( 66.09)	Acc@5  89.00 ( 88.00)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.8008e+00 (1.7027e+00)	Acc@1  71.00 ( 66.90)	Acc@5  85.00 ( 88.19)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.9082e+00 (1.7011e+00)	Acc@1  61.00 ( 66.23)	Acc@5  85.00 ( 88.16)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.3828e+00 (1.6796e+00)	Acc@1  70.00 ( 66.02)	Acc@5  92.00 ( 88.61)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 1.6123e+00 (1.6986e+00)	Acc@1  67.00 ( 66.08)	Acc@5  88.00 ( 88.35)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6338e+00 (1.6669e+00)	Acc@1  67.00 ( 66.51)	Acc@5  86.00 ( 88.44)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.6494e+00 (1.6535e+00)	Acc@1  68.00 ( 66.59)	Acc@5  90.00 ( 88.65)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 1.6318e+00 (1.6591e+00)	Acc@1  67.00 ( 66.56)	Acc@5  86.00 ( 88.57)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.9512e+00 (1.6574e+00)	Acc@1  61.00 ( 66.63)	Acc@5  85.00 ( 88.57)
 * Acc@1 66.820 Acc@5 88.610
### epoch[51] execution time: 11.39197325706482
EPOCH 52
REMOVING: features.module.5.weight
i:   0, name:         features.module.5.bias  changing lr from: 0.001269523731668031   to: 0.001049087484232108
i:   1, name:       features.module.8.weight  changing lr from: 0.001642464808983661   to: 0.001260010862513921
i:   2, name:         features.module.8.bias  changing lr from: 0.002158491750942403   to: 0.001625272163965002
i:   3, name:       features.module.9.weight  changing lr from: 0.002805246126471871   to: 0.002132260950608660
i:   4, name:         features.module.9.bias  changing lr from: 0.003570894488980768   to: 0.002768835585957923
i:   5, name:      features.module.11.weight  changing lr from: 0.004444157654973734   to: 0.003523364459796479
i:   6, name:        features.module.11.bias  changing lr from: 0.005414329195744293   to: 0.004384754880910940
i:   7, name:      features.module.12.weight  changing lr from: 0.006471284864727011   to: 0.005342471503316659
i:   8, name:        features.module.12.bias  changing lr from: 0.007605484479981383   to: 0.006386545943284172
i:   9, name:      features.module.15.weight  changing lr from: 0.008807967594424225   to: 0.007507579050732144
i:  10, name:        features.module.15.bias  changing lr from: 0.010070344116015654   to: 0.008696737120117086
i:  11, name:      features.module.16.weight  changing lr from: 0.011384780885873486   to: 0.009945743163036619
i:  12, name:        features.module.16.bias  changing lr from: 0.012743985083642016   to: 0.011246864217142171
i:  13, name:      features.module.18.weight  changing lr from: 0.014141185205564850   to: 0.012592895533097809
i:  14, name:        features.module.18.bias  changing lr from: 0.015570110250643410   to: 0.013977142362465451
i:  15, name:      features.module.19.weight  changing lr from: 0.017024967652978808   to: 0.015393399963663061
i:  16, name:        features.module.19.bias  changing lr from: 0.018500420412826540   to: 0.016835932349564011
i:  17, name:      features.module.22.weight  changing lr from: 0.019991563803983550   to: 0.018299450217888071
i:  18, name:        features.module.22.bias  changing lr from: 0.021493901969851625   to: 0.019779088433286841
i:  19, name:      features.module.23.weight  changing lr from: 0.023003324663890979   to: 0.021270383366975162
i:  20, name:        features.module.23.bias  changing lr from: 0.024516084341276285   to: 0.022769250344985101
i:  21, name:      features.module.25.weight  changing lr from: 0.026028773766531812   to: 0.024271961408742306
i:  22, name:        features.module.25.bias  changing lr from: 0.027538304265961145   to: 0.025775123550875290
i:  23, name:      features.module.26.weight  changing lr from: 0.029041884723079114   to: 0.027275657554213951
i:  24, name:        features.module.26.bias  changing lr from: 0.030537001389341192   to: 0.028770777532127922
i:  25, name:            classifier.0.weight  changing lr from: 0.032021398560657097   to: 0.030257971243073223
i:  26, name:              classifier.0.bias  changing lr from: 0.033493060151942360   to: 0.031734981230893612
i:  27, name:            classifier.3.weight  changing lr from: 0.034950192186825914   to: 0.033199786824553372
i:  28, name:              classifier.3.bias  changing lr from: 0.036391206207178006   to: 0.034650587016108493
i:  29, name:            classifier.6.weight  changing lr from: 0.037814703596973101   to: 0.036085784223450899
i:  30, name:              classifier.6.bias  changing lr from: 0.039219460806832702   to: 0.037503968934326136



# Switched to train mode...
Epoch: [52][  0/391]	Time  0.172 ( 0.172)	Data  0.147 ( 0.147)	Loss 2.7417e-01 (2.7417e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.015)	Loss 1.6956e-01 (1.9364e-01)	Acc@1  93.75 ( 93.32)	Acc@5 100.00 ( 99.79)
Epoch: [52][ 20/391]	Time  0.019 ( 0.030)	Data  0.002 ( 0.009)	Loss 8.5205e-02 (1.8973e-01)	Acc@1  97.66 ( 93.82)	Acc@5 100.00 ( 99.81)
Epoch: [52][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.9678e-01 (1.9213e-01)	Acc@1  93.75 ( 93.95)	Acc@5  99.22 ( 99.77)
Epoch: [52][ 40/391]	Time  0.018 ( 0.028)	Data  0.000 ( 0.005)	Loss 3.1592e-01 (1.8831e-01)	Acc@1  90.62 ( 94.04)	Acc@5 100.00 ( 99.75)
Epoch: [52][ 50/391]	Time  0.031 ( 0.027)	Data  0.003 ( 0.005)	Loss 2.4036e-01 (1.8372e-01)	Acc@1  92.19 ( 94.13)	Acc@5  98.44 ( 99.74)
Epoch: [52][ 60/391]	Time  0.041 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.5027e-01 (1.8182e-01)	Acc@1  93.75 ( 94.11)	Acc@5 100.00 ( 99.76)
Epoch: [52][ 70/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4941e-01 (1.8016e-01)	Acc@1  94.53 ( 94.21)	Acc@5 100.00 ( 99.77)
Epoch: [52][ 80/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.004)	Loss 2.4561e-01 (1.8246e-01)	Acc@1  93.75 ( 94.17)	Acc@5 100.00 ( 99.75)
Epoch: [52][ 90/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.5808e-01 (1.7956e-01)	Acc@1  96.09 ( 94.28)	Acc@5 100.00 ( 99.73)
Epoch: [52][100/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3342e-01 (1.7913e-01)	Acc@1  96.09 ( 94.27)	Acc@5 100.00 ( 99.74)
Epoch: [52][110/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2522e-01 (1.8210e-01)	Acc@1  90.62 ( 94.16)	Acc@5  99.22 ( 99.73)
Epoch: [52][120/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7896e-01 (1.7904e-01)	Acc@1  94.53 ( 94.25)	Acc@5 100.00 ( 99.74)
Epoch: [52][130/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5833e-01 (1.7823e-01)	Acc@1  93.75 ( 94.28)	Acc@5 100.00 ( 99.74)
Epoch: [52][140/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9263e-01 (1.7936e-01)	Acc@1  94.53 ( 94.27)	Acc@5 100.00 ( 99.74)
Epoch: [52][150/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.0520e-01 (1.8017e-01)	Acc@1  91.41 ( 94.21)	Acc@5 100.00 ( 99.73)
Epoch: [52][160/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.4072e-01 (1.8109e-01)	Acc@1  92.97 ( 94.15)	Acc@5  98.44 ( 99.72)
Epoch: [52][170/391]	Time  0.044 ( 0.025)	Data  0.003 ( 0.003)	Loss 2.1814e-01 (1.8137e-01)	Acc@1  93.75 ( 94.15)	Acc@5 100.00 ( 99.72)
Epoch: [52][180/391]	Time  0.038 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.2363e-01 (1.8114e-01)	Acc@1  92.19 ( 94.12)	Acc@5  99.22 ( 99.71)
Epoch: [52][190/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4233e-01 (1.8251e-01)	Acc@1  95.31 ( 94.08)	Acc@5 100.00 ( 99.70)
Epoch: [52][200/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1985e-01 (1.8231e-01)	Acc@1  92.97 ( 94.10)	Acc@5 100.00 ( 99.70)
Epoch: [52][210/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5295e-01 (1.8091e-01)	Acc@1  96.88 ( 94.16)	Acc@5  99.22 ( 99.71)
Epoch: [52][220/391]	Time  0.047 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.8152e-01 (1.8067e-01)	Acc@1  95.31 ( 94.18)	Acc@5  99.22 ( 99.71)
Epoch: [52][230/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2079e-01 (1.8162e-01)	Acc@1  95.31 ( 94.17)	Acc@5 100.00 ( 99.72)
Epoch: [52][240/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4429e-01 (1.8046e-01)	Acc@1  95.31 ( 94.23)	Acc@5 100.00 ( 99.71)
Epoch: [52][250/391]	Time  0.039 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.3999e-01 (1.8015e-01)	Acc@1  91.41 ( 94.24)	Acc@5 100.00 ( 99.71)
Epoch: [52][260/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8848e-01 (1.7949e-01)	Acc@1  93.75 ( 94.26)	Acc@5 100.00 ( 99.71)
Epoch: [52][270/391]	Time  0.032 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1118e-01 (1.7972e-01)	Acc@1  91.41 ( 94.28)	Acc@5 100.00 ( 99.71)
Epoch: [52][280/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.1798e-01 (1.8000e-01)	Acc@1  95.31 ( 94.28)	Acc@5  99.22 ( 99.71)
Epoch: [52][290/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.4009e-01 (1.8079e-01)	Acc@1  89.84 ( 94.25)	Acc@5 100.00 ( 99.71)
Epoch: [52][300/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.8567e-01 (1.8094e-01)	Acc@1  95.31 ( 94.26)	Acc@5 100.00 ( 99.71)
Epoch: [52][310/391]	Time  0.034 ( 0.025)	Data  0.004 ( 0.002)	Loss 2.4377e-01 (1.8167e-01)	Acc@1  92.97 ( 94.26)	Acc@5 100.00 ( 99.71)
Epoch: [52][320/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 2.0569e-01 (1.8153e-01)	Acc@1  93.75 ( 94.27)	Acc@5  99.22 ( 99.70)
Epoch: [52][330/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.6807e-01 (1.8172e-01)	Acc@1  89.06 ( 94.26)	Acc@5 100.00 ( 99.70)
Epoch: [52][340/391]	Time  0.029 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.2482e-01 (1.8196e-01)	Acc@1  97.66 ( 94.27)	Acc@5 100.00 ( 99.69)
Epoch: [52][350/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3376e-01 (1.8294e-01)	Acc@1  91.41 ( 94.21)	Acc@5 100.00 ( 99.68)
Epoch: [52][360/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5540e-01 (1.8276e-01)	Acc@1  95.31 ( 94.21)	Acc@5  99.22 ( 99.68)
Epoch: [52][370/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2439e-01 (1.8309e-01)	Acc@1  96.09 ( 94.20)	Acc@5 100.00 ( 99.68)
Epoch: [52][380/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9409e-01 (1.8289e-01)	Acc@1  93.75 ( 94.22)	Acc@5  99.22 ( 99.68)
Epoch: [52][390/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.7493e-01 (1.8291e-01)	Acc@1  92.50 ( 94.22)	Acc@5 100.00 ( 99.68)
## e[52] optimizer.zero_grad (sum) time: 0.0972757339477539
## e[52]       loss.backward (sum) time: 2.0444912910461426
## e[52]      optimizer.step (sum) time: 0.8031213283538818
## epoch[52] training(only) time: 9.746476173400879
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.9248e+00 (1.9248e+00)	Acc@1  66.00 ( 66.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.016 ( 0.026)	Loss 1.8203e+00 (1.7556e+00)	Acc@1  66.00 ( 65.55)	Acc@5  86.00 ( 87.18)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.7744e+00 (1.7041e+00)	Acc@1  70.00 ( 66.43)	Acc@5  85.00 ( 87.71)
Test: [ 30/100]	Time  0.027 ( 0.018)	Loss 1.8711e+00 (1.7013e+00)	Acc@1  63.00 ( 65.84)	Acc@5  86.00 ( 87.90)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.3750e+00 (1.6791e+00)	Acc@1  68.00 ( 66.02)	Acc@5  95.00 ( 88.39)
Test: [ 50/100]	Time  0.038 ( 0.016)	Loss 1.5225e+00 (1.6964e+00)	Acc@1  70.00 ( 66.27)	Acc@5  91.00 ( 88.20)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 1.5283e+00 (1.6717e+00)	Acc@1  70.00 ( 66.57)	Acc@5  89.00 ( 88.49)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 1.7920e+00 (1.6663e+00)	Acc@1  67.00 ( 66.58)	Acc@5  90.00 ( 88.72)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.5322e+00 (1.6736e+00)	Acc@1  66.00 ( 66.47)	Acc@5  87.00 ( 88.54)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 1.9297e+00 (1.6673e+00)	Acc@1  64.00 ( 66.74)	Acc@5  86.00 ( 88.59)
 * Acc@1 66.770 Acc@5 88.700
### epoch[52] execution time: 11.329032897949219
EPOCH 53
REMOVING: features.module.5.bias
i:   0, name:       features.module.8.weight  changing lr from: 0.001260010862513921   to: 0.001047458904480085
i:   1, name:         features.module.8.bias  changing lr from: 0.001625272163965002   to: 0.001254520140547726
i:   2, name:       features.module.9.weight  changing lr from: 0.002132260950608660   to: 0.001614212264556596
i:   3, name:         features.module.9.bias  changing lr from: 0.002768835585957923   to: 0.002114153046046090
i:   4, name:      features.module.11.weight  changing lr from: 0.003523364459796479   to: 0.002742416136991043
i:   5, name:        features.module.11.bias  changing lr from: 0.004384754880910940   to: 0.003487571372223630
i:   6, name:      features.module.12.weight  changing lr from: 0.005342471503316659   to: 0.004338713139153271
i:   7, name:        features.module.12.bias  changing lr from: 0.006386545943284172   to: 0.005285478608354087
i:   8, name:      features.module.15.weight  changing lr from: 0.007507579050732144   to: 0.006318057418162676
i:   9, name:        features.module.15.bias  changing lr from: 0.008696737120117086   to: 0.007427194221647049
i:  10, name:      features.module.16.weight  changing lr from: 0.009945743163036619   to: 0.008604185333963826
i:  11, name:        features.module.16.bias  changing lr from: 0.011246864217142171   to: 0.009840870562436072
i:  12, name:      features.module.18.weight  changing lr from: 0.012592895533097809   to: 0.011129621160471774
i:  13, name:        features.module.18.bias  changing lr from: 0.013977142362465451   to: 0.012463324719207785
i:  14, name:      features.module.19.weight  changing lr from: 0.015393399963663061   to: 0.013835367696816998
i:  15, name:        features.module.19.bias  changing lr from: 0.016835932349564011   to: 0.015239616183930001
i:  16, name:      features.module.22.weight  changing lr from: 0.018299450217888071   to: 0.016670395413695579
i:  17, name:        features.module.22.bias  changing lr from: 0.019779088433286841   to: 0.018122468445708691
i:  18, name:      features.module.23.weight  changing lr from: 0.021270383366975162   to: 0.019591014383428595
i:  19, name:        features.module.23.bias  changing lr from: 0.022769250344985101   to: 0.021071606423890544
i:  20, name:      features.module.25.weight  changing lr from: 0.024271961408742306   to: 0.022560189985600533
i:  21, name:        features.module.25.bias  changing lr from: 0.025775123550875290   to: 0.024053061114672369
i:  22, name:      features.module.26.weight  changing lr from: 0.027275657554213951   to: 0.025546845329748130
i:  23, name:        features.module.26.bias  changing lr from: 0.028770777532127922   to: 0.027038477032324627
i:  24, name:            classifier.0.weight  changing lr from: 0.030257971243073223   to: 0.028525179580136308
i:  25, name:              classifier.0.bias  changing lr from: 0.031734981230893612   to: 0.030004446096627058
i:  26, name:            classifier.3.weight  changing lr from: 0.033199786824553372   to: 0.031474021068738658
i:  27, name:              classifier.3.bias  changing lr from: 0.034650587016108493   to: 0.032931882767772216
i:  28, name:            classifier.6.weight  changing lr from: 0.036085784223450899   to: 0.034376226513505380
i:  29, name:              classifier.6.bias  changing lr from: 0.037503968934326136   to: 0.035805448789688075



# Switched to train mode...
Epoch: [53][  0/391]	Time  0.174 ( 0.174)	Data  0.148 ( 0.148)	Loss 1.6589e-01 (1.6589e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.019 ( 0.038)	Data  0.001 ( 0.015)	Loss 9.1003e-02 (1.5983e-01)	Acc@1  97.66 ( 94.96)	Acc@5 100.00 ( 99.79)
Epoch: [53][ 20/391]	Time  0.019 ( 0.031)	Data  0.001 ( 0.009)	Loss 1.1353e-01 (1.5922e-01)	Acc@1  96.88 ( 95.13)	Acc@5  99.22 ( 99.70)
Epoch: [53][ 30/391]	Time  0.043 ( 0.029)	Data  0.007 ( 0.007)	Loss 1.2903e-01 (1.5361e-01)	Acc@1  95.31 ( 95.29)	Acc@5 100.00 ( 99.72)
Epoch: [53][ 40/391]	Time  0.038 ( 0.028)	Data  0.000 ( 0.006)	Loss 1.5002e-01 (1.6017e-01)	Acc@1  94.53 ( 95.12)	Acc@5 100.00 ( 99.73)
Epoch: [53][ 50/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.005)	Loss 1.8628e-01 (1.5786e-01)	Acc@1  93.75 ( 95.14)	Acc@5 100.00 ( 99.72)
Epoch: [53][ 60/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.005)	Loss 7.6050e-02 (1.5268e-01)	Acc@1  99.22 ( 95.27)	Acc@5 100.00 ( 99.76)
Epoch: [53][ 70/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.0217e-01 (1.5426e-01)	Acc@1  96.09 ( 95.09)	Acc@5 100.00 ( 99.75)
Epoch: [53][ 80/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.0559e-01 (1.5604e-01)	Acc@1  94.53 ( 95.00)	Acc@5 100.00 ( 99.72)
Epoch: [53][ 90/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.3806e-01 (1.5616e-01)	Acc@1  94.53 ( 94.95)	Acc@5  99.22 ( 99.70)
Epoch: [53][100/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2073e-01 (1.5612e-01)	Acc@1  96.88 ( 94.99)	Acc@5 100.00 ( 99.71)
Epoch: [53][110/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1456e-01 (1.5668e-01)	Acc@1  95.31 ( 94.97)	Acc@5 100.00 ( 99.72)
Epoch: [53][120/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4258e-01 (1.5547e-01)	Acc@1  95.31 ( 95.02)	Acc@5 100.00 ( 99.73)
Epoch: [53][130/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4355e-01 (1.5587e-01)	Acc@1  96.09 ( 95.04)	Acc@5  99.22 ( 99.72)
Epoch: [53][140/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6077e-01 (1.5568e-01)	Acc@1  94.53 ( 95.01)	Acc@5  98.44 ( 99.71)
Epoch: [53][150/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3684e-01 (1.5522e-01)	Acc@1  95.31 ( 95.02)	Acc@5 100.00 ( 99.72)
Epoch: [53][160/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9348e-01 (1.5531e-01)	Acc@1  92.97 ( 95.02)	Acc@5  99.22 ( 99.71)
Epoch: [53][170/391]	Time  0.029 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1517e-01 (1.5460e-01)	Acc@1  96.88 ( 95.04)	Acc@5 100.00 ( 99.72)
Epoch: [53][180/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4294e-01 (1.5489e-01)	Acc@1  94.53 ( 95.01)	Acc@5 100.00 ( 99.72)
Epoch: [53][190/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1737e-01 (1.5518e-01)	Acc@1  96.09 ( 94.99)	Acc@5 100.00 ( 99.73)
Epoch: [53][200/391]	Time  0.028 ( 0.025)	Data  0.000 ( 0.003)	Loss 1.3879e-01 (1.5446e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.74)
Epoch: [53][210/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.003)	Loss 2.3083e-01 (1.5511e-01)	Acc@1  91.41 ( 94.98)	Acc@5 100.00 ( 99.72)
Epoch: [53][220/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 6.7017e-02 (1.5523e-01)	Acc@1  97.66 ( 94.98)	Acc@5 100.00 ( 99.72)
Epoch: [53][230/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4014e-01 (1.5562e-01)	Acc@1  94.53 ( 94.96)	Acc@5  99.22 ( 99.72)
Epoch: [53][240/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.5547e-01 (1.5684e-01)	Acc@1  89.06 ( 94.94)	Acc@5  98.44 ( 99.71)
Epoch: [53][250/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2549e-01 (1.5690e-01)	Acc@1  95.31 ( 94.94)	Acc@5  99.22 ( 99.71)
Epoch: [53][260/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4185e-01 (1.5826e-01)	Acc@1  96.09 ( 94.90)	Acc@5 100.00 ( 99.72)
Epoch: [53][270/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.002)	Loss 7.0007e-02 (1.5705e-01)	Acc@1  97.66 ( 94.96)	Acc@5 100.00 ( 99.72)
Epoch: [53][280/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 1.6736e-01 (1.5717e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.72)
Epoch: [53][290/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8481e-01 (1.5695e-01)	Acc@1  94.53 ( 94.94)	Acc@5 100.00 ( 99.73)
Epoch: [53][300/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.7383e-02 (1.5723e-01)	Acc@1  96.88 ( 94.94)	Acc@5 100.00 ( 99.74)
Epoch: [53][310/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4648e-01 (1.5823e-01)	Acc@1  95.31 ( 94.92)	Acc@5 100.00 ( 99.74)
Epoch: [53][320/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3574e-01 (1.5883e-01)	Acc@1  96.88 ( 94.90)	Acc@5 100.00 ( 99.74)
Epoch: [53][330/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3147e-01 (1.5874e-01)	Acc@1  96.88 ( 94.91)	Acc@5  99.22 ( 99.74)
Epoch: [53][340/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.9824e-02 (1.5926e-01)	Acc@1  99.22 ( 94.89)	Acc@5 100.00 ( 99.74)
Epoch: [53][350/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.3159e-01 (1.5918e-01)	Acc@1  94.53 ( 94.88)	Acc@5 100.00 ( 99.74)
Epoch: [53][360/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.7344e-01 (1.6001e-01)	Acc@1  92.19 ( 94.85)	Acc@5  99.22 ( 99.74)
Epoch: [53][370/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5747e-01 (1.5939e-01)	Acc@1  92.19 ( 94.89)	Acc@5 100.00 ( 99.75)
Epoch: [53][380/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.9731e-02 (1.5990e-01)	Acc@1  96.88 ( 94.88)	Acc@5 100.00 ( 99.74)
Epoch: [53][390/391]	Time  0.016 ( 0.025)	Data  0.000 ( 0.002)	Loss 2.6807e-01 (1.5975e-01)	Acc@1  95.00 ( 94.90)	Acc@5 100.00 ( 99.74)
## e[53] optimizer.zero_grad (sum) time: 0.09401392936706543
## e[53]       loss.backward (sum) time: 1.9316315650939941
## e[53]      optimizer.step (sum) time: 0.7732267379760742
## epoch[53] training(only) time: 9.702229976654053
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.8018e+00 (1.8018e+00)	Acc@1  66.00 ( 66.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.7275e+00 (1.8037e+00)	Acc@1  65.00 ( 65.91)	Acc@5  89.00 ( 88.00)
Test: [ 20/100]	Time  0.022 ( 0.020)	Loss 1.9072e+00 (1.7923e+00)	Acc@1  68.00 ( 66.57)	Acc@5  84.00 ( 88.14)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.9629e+00 (1.7831e+00)	Acc@1  62.00 ( 66.52)	Acc@5  84.00 ( 88.00)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 1.3477e+00 (1.7457e+00)	Acc@1  69.00 ( 66.66)	Acc@5  93.00 ( 88.20)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.7598e+00 (1.7711e+00)	Acc@1  70.00 ( 66.45)	Acc@5  91.00 ( 87.98)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 1.5947e+00 (1.7413e+00)	Acc@1  64.00 ( 66.56)	Acc@5  89.00 ( 88.28)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.7920e+00 (1.7426e+00)	Acc@1  65.00 ( 66.51)	Acc@5  88.00 ( 88.44)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.8135e+00 (1.7516e+00)	Acc@1  64.00 ( 66.54)	Acc@5  87.00 ( 88.36)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 2.0234e+00 (1.7412e+00)	Acc@1  60.00 ( 66.78)	Acc@5  86.00 ( 88.58)
 * Acc@1 66.790 Acc@5 88.630
### epoch[53] execution time: 11.294842720031738
EPOCH 54
REMOVING: features.module.8.weight
i:   0, name:         features.module.8.bias  changing lr from: 0.001254520140547726   to: 0.001047455640385935
i:   1, name:       features.module.9.weight  changing lr from: 0.001614212264556596   to: 0.001252742555701618
i:   2, name:         features.module.9.bias  changing lr from: 0.002114153046046090   to: 0.001608847333735324
i:   3, name:      features.module.11.weight  changing lr from: 0.002742416136991043   to: 0.002103613010017329
i:   4, name:        features.module.11.bias  changing lr from: 0.003487571372223630   to: 0.002725326738402644
i:   5, name:      features.module.12.weight  changing lr from: 0.004338713139153271   to: 0.003462759044998636
i:   6, name:        features.module.12.bias  changing lr from: 0.005285478608354087   to: 0.004305191555708154
i:   7, name:      features.module.15.weight  changing lr from: 0.006318057418162676   to: 0.005242434916726534
i:   8, name:        features.module.15.bias  changing lr from: 0.007427194221647049   to: 0.006264838438237807
i:   9, name:      features.module.16.weight  changing lr from: 0.008604185333963826   to: 0.007363292815338573
i:  10, name:        features.module.16.bias  changing lr from: 0.009840870562436072   to: 0.008529227117635157
i:  11, name:      features.module.18.weight  changing lr from: 0.011129621160471774   to: 0.009754601090242752
i:  12, name:        features.module.18.bias  changing lr from: 0.012463324719207785   to: 0.011031893673893199
i:  13, name:      features.module.19.weight  changing lr from: 0.013835367696816998   to: 0.012354088530085988
i:  14, name:        features.module.19.bias  changing lr from: 0.015239616183930001   to: 0.013714657248050016
i:  15, name:      features.module.22.weight  changing lr from: 0.016670395413695579   to: 0.015107540812950647
i:  16, name:        features.module.22.bias  changing lr from: 0.018122468445708691   to: 0.016527129828438247
i:  17, name:      features.module.23.weight  changing lr from: 0.019591014383428595   to: 0.017968243910412112
i:  18, name:        features.module.23.bias  changing lr from: 0.021071606423890544   to: 0.019426110601890513
i:  19, name:      features.module.25.weight  changing lr from: 0.022560189985600533   to: 0.020896344100274772
i:  20, name:        features.module.25.bias  changing lr from: 0.024053061114672369   to: 0.022374924037246269
i:  21, name:      features.module.26.weight  changing lr from: 0.025546845329748130   to: 0.023858174507260150
i:  22, name:        features.module.26.bias  changing lr from: 0.027038477032324627   to: 0.025342743502368995
i:  23, name:            classifier.0.weight  changing lr from: 0.028525179580136308   to: 0.026825582878247024
i:  24, name:              classifier.0.bias  changing lr from: 0.030004446096627058   to: 0.028303928948172952
i:  25, name:            classifier.3.weight  changing lr from: 0.031474021068738658   to: 0.029775283777798367
i:  26, name:              classifier.3.bias  changing lr from: 0.032931882767772216   to: 0.031237397233271780
i:  27, name:            classifier.6.weight  changing lr from: 0.034376226513505380   to: 0.032688249818240270
i:  28, name:              classifier.6.bias  changing lr from: 0.035805448789688075   to: 0.034126036321000842



# Switched to train mode...
Epoch: [54][  0/391]	Time  0.176 ( 0.176)	Data  0.145 ( 0.145)	Loss 1.5637e-01 (1.5637e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.019 ( 0.038)	Data  0.001 ( 0.015)	Loss 1.4819e-01 (1.6338e-01)	Acc@1  95.31 ( 94.96)	Acc@5  99.22 ( 99.57)
Epoch: [54][ 20/391]	Time  0.022 ( 0.031)	Data  0.002 ( 0.009)	Loss 2.0032e-01 (1.5050e-01)	Acc@1  94.53 ( 95.42)	Acc@5 100.00 ( 99.67)
Epoch: [54][ 30/391]	Time  0.024 ( 0.029)	Data  0.003 ( 0.006)	Loss 1.5039e-01 (1.5170e-01)	Acc@1  96.09 ( 95.31)	Acc@5  99.22 ( 99.65)
Epoch: [54][ 40/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.0923e-01 (1.5904e-01)	Acc@1  92.19 ( 95.14)	Acc@5 100.00 ( 99.68)
Epoch: [54][ 50/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.1469e-01 (1.4887e-01)	Acc@1  97.66 ( 95.50)	Acc@5  99.22 ( 99.71)
Epoch: [54][ 60/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7773e-01 (1.4583e-01)	Acc@1  94.53 ( 95.58)	Acc@5  99.22 ( 99.72)
Epoch: [54][ 70/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.5466e-01 (1.4478e-01)	Acc@1  94.53 ( 95.66)	Acc@5  99.22 ( 99.74)
Epoch: [54][ 80/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4478e-01 (1.4574e-01)	Acc@1  96.09 ( 95.61)	Acc@5 100.00 ( 99.74)
Epoch: [54][ 90/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3611e-01 (1.4872e-01)	Acc@1  96.09 ( 95.52)	Acc@5  99.22 ( 99.73)
Epoch: [54][100/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7065e-01 (1.4901e-01)	Acc@1  93.75 ( 95.47)	Acc@5 100.00 ( 99.72)
Epoch: [54][110/391]	Time  0.042 ( 0.025)	Data  0.004 ( 0.003)	Loss 1.5674e-01 (1.4770e-01)	Acc@1  95.31 ( 95.48)	Acc@5 100.00 ( 99.73)
Epoch: [54][120/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.3801e-02 (1.4827e-01)	Acc@1  95.31 ( 95.42)	Acc@5 100.00 ( 99.73)
Epoch: [54][130/391]	Time  0.030 ( 0.025)	Data  0.002 ( 0.003)	Loss 9.5093e-02 (1.4902e-01)	Acc@1  96.88 ( 95.40)	Acc@5 100.00 ( 99.73)
Epoch: [54][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2720e-01 (1.4641e-01)	Acc@1  96.09 ( 95.47)	Acc@5 100.00 ( 99.73)
Epoch: [54][150/391]	Time  0.033 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0339e-01 (1.4591e-01)	Acc@1  96.09 ( 95.44)	Acc@5 100.00 ( 99.74)
Epoch: [54][160/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2817e-01 (1.4611e-01)	Acc@1  96.09 ( 95.46)	Acc@5 100.00 ( 99.73)
Epoch: [54][170/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0742e-01 (1.4549e-01)	Acc@1  96.09 ( 95.48)	Acc@5 100.00 ( 99.74)
Epoch: [54][180/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4893e-01 (1.4594e-01)	Acc@1  93.75 ( 95.47)	Acc@5 100.00 ( 99.72)
Epoch: [54][190/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1707e-01 (1.4584e-01)	Acc@1  94.53 ( 95.47)	Acc@5 100.00 ( 99.73)
Epoch: [54][200/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.5271e-01 (1.4478e-01)	Acc@1  96.09 ( 95.48)	Acc@5 100.00 ( 99.74)
Epoch: [54][210/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3574e-01 (1.4516e-01)	Acc@1  95.31 ( 95.48)	Acc@5 100.00 ( 99.73)
Epoch: [54][220/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2205e-01 (1.4565e-01)	Acc@1  92.97 ( 95.44)	Acc@5 100.00 ( 99.73)
Epoch: [54][230/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.8564e-01 (1.4695e-01)	Acc@1  91.41 ( 95.41)	Acc@5  99.22 ( 99.73)
Epoch: [54][240/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5881e-01 (1.4622e-01)	Acc@1  93.75 ( 95.43)	Acc@5  99.22 ( 99.73)
Epoch: [54][250/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.0837e-01 (1.4708e-01)	Acc@1  93.75 ( 95.40)	Acc@5  98.44 ( 99.72)
Epoch: [54][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3730e-01 (1.4826e-01)	Acc@1  90.62 ( 95.36)	Acc@5 100.00 ( 99.72)
Epoch: [54][270/391]	Time  0.022 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.1328e-01 (1.4739e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.73)
Epoch: [54][280/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.4001e-01 (1.4753e-01)	Acc@1  96.09 ( 95.38)	Acc@5 100.00 ( 99.72)
Epoch: [54][290/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.8726e-01 (1.4675e-01)	Acc@1  94.53 ( 95.41)	Acc@5 100.00 ( 99.73)
Epoch: [54][300/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6968e-01 (1.4748e-01)	Acc@1  94.53 ( 95.38)	Acc@5 100.00 ( 99.73)
Epoch: [54][310/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6833e-01 (1.4744e-01)	Acc@1  95.31 ( 95.38)	Acc@5 100.00 ( 99.74)
Epoch: [54][320/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.3462e-01 (1.4772e-01)	Acc@1  94.53 ( 95.38)	Acc@5  99.22 ( 99.74)
Epoch: [54][330/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.002)	Loss 6.2622e-02 (1.4730e-01)	Acc@1  97.66 ( 95.40)	Acc@5 100.00 ( 99.74)
Epoch: [54][340/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.2732e-01 (1.4723e-01)	Acc@1  95.31 ( 95.39)	Acc@5 100.00 ( 99.75)
Epoch: [54][350/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 9.5276e-02 (1.4732e-01)	Acc@1  96.88 ( 95.38)	Acc@5 100.00 ( 99.75)
Epoch: [54][360/391]	Time  0.023 ( 0.025)	Data  0.005 ( 0.002)	Loss 1.4453e-01 (1.4783e-01)	Acc@1  95.31 ( 95.36)	Acc@5 100.00 ( 99.74)
Epoch: [54][370/391]	Time  0.042 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.2048e-01 (1.4845e-01)	Acc@1  94.53 ( 95.35)	Acc@5 100.00 ( 99.74)
Epoch: [54][380/391]	Time  0.026 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.7029e-01 (1.4915e-01)	Acc@1  92.19 ( 95.31)	Acc@5 100.00 ( 99.73)
Epoch: [54][390/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0386e-01 (1.4900e-01)	Acc@1  92.50 ( 95.32)	Acc@5 100.00 ( 99.73)
## e[54] optimizer.zero_grad (sum) time: 0.09067988395690918
## e[54]       loss.backward (sum) time: 1.8978540897369385
## e[54]      optimizer.step (sum) time: 0.7444684505462646
## epoch[54] training(only) time: 9.650696754455566
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.7617e+00 (1.7617e+00)	Acc@1  66.00 ( 66.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 1.8682e+00 (1.7982e+00)	Acc@1  65.00 ( 65.91)	Acc@5  88.00 ( 87.91)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.8672e+00 (1.7648e+00)	Acc@1  69.00 ( 66.95)	Acc@5  84.00 ( 88.05)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.0195e+00 (1.7590e+00)	Acc@1  59.00 ( 66.52)	Acc@5  81.00 ( 87.81)
Test: [ 40/100]	Time  0.023 ( 0.017)	Loss 1.4209e+00 (1.7354e+00)	Acc@1  65.00 ( 66.68)	Acc@5  92.00 ( 88.27)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6562e+00 (1.7476e+00)	Acc@1  67.00 ( 66.71)	Acc@5  89.00 ( 88.06)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6650e+00 (1.7236e+00)	Acc@1  66.00 ( 66.85)	Acc@5  88.00 ( 88.31)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.8838e+00 (1.7192e+00)	Acc@1  69.00 ( 66.90)	Acc@5  89.00 ( 88.44)
Test: [ 80/100]	Time  0.019 ( 0.016)	Loss 1.8457e+00 (1.7320e+00)	Acc@1  67.00 ( 66.77)	Acc@5  87.00 ( 88.35)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 2.1875e+00 (1.7203e+00)	Acc@1  59.00 ( 66.95)	Acc@5  84.00 ( 88.53)
 * Acc@1 67.040 Acc@5 88.630
### epoch[54] execution time: 11.233673572540283
EPOCH 55
REMOVING: features.module.8.bias
i:   0, name:       features.module.9.weight  changing lr from: 0.001252742555701618   to: 0.001048997873542894
i:   1, name:         features.module.9.bias  changing lr from: 0.001608847333735324   to: 0.001254462473911699
i:   2, name:      features.module.11.weight  changing lr from: 0.002103613010017329   to: 0.001608836706348442
i:   3, name:        features.module.11.bias  changing lr from: 0.002725326738402644   to: 0.002100185811603083
i:   4, name:      features.module.12.weight  changing lr from: 0.003462759044998636   to: 0.002717008440443474
i:   5, name:        features.module.12.bias  changing lr from: 0.004305191555708154   to: 0.003448274757708197
i:   6, name:      features.module.15.weight  changing lr from: 0.005242434916726534   to: 0.004283453425161786
i:   7, name:        features.module.15.bias  changing lr from: 0.006264838438237807   to: 0.005212529112106856
i:   8, name:      features.module.16.weight  changing lr from: 0.007363292815338573   to: 0.006226012002497344
i:   9, name:        features.module.16.bias  changing lr from: 0.008529227117635157   to: 0.007314940599257202
i:  10, name:      features.module.18.weight  changing lr from: 0.009754601090242752   to: 0.008470878971370527
i:  11, name:        features.module.18.bias  changing lr from: 0.011031893673893199   to: 0.009685909447289441
i:  12, name:      features.module.19.weight  changing lr from: 0.012354088530085988   to: 0.010952621629164262
i:  13, name:        features.module.19.bias  changing lr from: 0.013714657248050016   to: 0.012264098485917994
i:  14, name:      features.module.22.weight  changing lr from: 0.015107540812950647   to: 0.013613900178665522
i:  15, name:        features.module.22.bias  changing lr from: 0.016527129828438247   to: 0.014996046178699780
i:  16, name:      features.module.23.weight  changing lr from: 0.017968243910412112   to: 0.016404996155438400
i:  17, name:        features.module.23.bias  changing lr from: 0.019426110601890513   to: 0.017835630038525885
i:  18, name:      features.module.25.weight  changing lr from: 0.020896344100274772   to: 0.019283227593889691
i:  19, name:        features.module.25.bias  changing lr from: 0.022374924037246269   to: 0.020743447797142996
i:  20, name:      features.module.26.weight  changing lr from: 0.023858174507260150   to: 0.022212308238534956
i:  21, name:        features.module.26.bias  changing lr from: 0.025342743502368995   to: 0.023686164750930029
i:  22, name:            classifier.0.weight  changing lr from: 0.026825582878247024   to: 0.025161691415363653
i:  23, name:              classifier.0.bias  changing lr from: 0.028303928948172952   to: 0.026635861066928859
i:  24, name:            classifier.3.weight  changing lr from: 0.029775283777798367   to: 0.028105926396510779
i:  25, name:              classifier.3.bias  changing lr from: 0.031237397233271780   to: 0.029569401720664995
i:  26, name:            classifier.6.weight  changing lr from: 0.032688249818240270   to: 0.031024045472244556
i:  27, name:              classifier.6.bias  changing lr from: 0.034126036321000842   to: 0.032467843447780496



# Switched to train mode...
Epoch: [55][  0/391]	Time  0.173 ( 0.173)	Data  0.148 ( 0.148)	Loss 9.0332e-02 (9.0332e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.020 ( 0.037)	Data  0.001 ( 0.015)	Loss 1.0791e-01 (1.2544e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.86)
Epoch: [55][ 20/391]	Time  0.030 ( 0.031)	Data  0.002 ( 0.009)	Loss 2.0740e-01 (1.3533e-01)	Acc@1  93.75 ( 95.68)	Acc@5  99.22 ( 99.78)
Epoch: [55][ 30/391]	Time  0.044 ( 0.029)	Data  0.001 ( 0.006)	Loss 9.8145e-02 (1.3754e-01)	Acc@1  96.88 ( 95.67)	Acc@5 100.00 ( 99.80)
Epoch: [55][ 40/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.5112e-01 (1.3069e-01)	Acc@1  96.09 ( 95.73)	Acc@5  98.44 ( 99.81)
Epoch: [55][ 50/391]	Time  0.027 ( 0.027)	Data  0.002 ( 0.005)	Loss 1.1517e-01 (1.2718e-01)	Acc@1  95.31 ( 95.85)	Acc@5 100.00 ( 99.83)
Epoch: [55][ 60/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6797e-01 (1.3155e-01)	Acc@1  94.53 ( 95.72)	Acc@5 100.00 ( 99.78)
Epoch: [55][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.5845e-01 (1.3101e-01)	Acc@1  92.97 ( 95.75)	Acc@5 100.00 ( 99.78)
Epoch: [55][ 80/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.5039e-01 (1.3032e-01)	Acc@1  93.75 ( 95.69)	Acc@5 100.00 ( 99.80)
Epoch: [55][ 90/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1530e-01 (1.3076e-01)	Acc@1  95.31 ( 95.71)	Acc@5 100.00 ( 99.80)
Epoch: [55][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.1655e-01 (1.3030e-01)	Acc@1  93.75 ( 95.77)	Acc@5 100.00 ( 99.82)
Epoch: [55][110/391]	Time  0.034 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.1779e-02 (1.2700e-01)	Acc@1  99.22 ( 95.88)	Acc@5 100.00 ( 99.84)
Epoch: [55][120/391]	Time  0.032 ( 0.025)	Data  0.000 ( 0.003)	Loss 2.5977e-01 (1.2986e-01)	Acc@1  92.19 ( 95.81)	Acc@5  99.22 ( 99.82)
Epoch: [55][130/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 5.9113e-02 (1.3004e-01)	Acc@1  98.44 ( 95.79)	Acc@5 100.00 ( 99.82)
Epoch: [55][140/391]	Time  0.042 ( 0.025)	Data  0.000 ( 0.003)	Loss 4.1290e-02 (1.2978e-01)	Acc@1  99.22 ( 95.78)	Acc@5 100.00 ( 99.82)
Epoch: [55][150/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.7334e-01 (1.3030e-01)	Acc@1  93.75 ( 95.79)	Acc@5 100.00 ( 99.81)
Epoch: [55][160/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.8928e-02 (1.2890e-01)	Acc@1  96.09 ( 95.81)	Acc@5 100.00 ( 99.82)
Epoch: [55][170/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5295e-01 (1.2876e-01)	Acc@1  93.75 ( 95.86)	Acc@5 100.00 ( 99.83)
Epoch: [55][180/391]	Time  0.035 ( 0.025)	Data  0.005 ( 0.003)	Loss 1.6882e-01 (1.3071e-01)	Acc@1  95.31 ( 95.81)	Acc@5  99.22 ( 99.82)
Epoch: [55][190/391]	Time  0.019 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.5210e-01 (1.3002e-01)	Acc@1  92.97 ( 95.81)	Acc@5 100.00 ( 99.83)
Epoch: [55][200/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1279e-01 (1.3083e-01)	Acc@1  95.31 ( 95.78)	Acc@5 100.00 ( 99.83)
Epoch: [55][210/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.7290e-02 (1.3062e-01)	Acc@1  96.09 ( 95.81)	Acc@5 100.00 ( 99.83)
Epoch: [55][220/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.9182e-02 (1.2954e-01)	Acc@1  96.88 ( 95.85)	Acc@5 100.00 ( 99.83)
Epoch: [55][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.2485e-01 (1.3020e-01)	Acc@1  92.97 ( 95.83)	Acc@5  98.44 ( 99.82)
Epoch: [55][240/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.6069e-02 (1.3094e-01)	Acc@1  96.09 ( 95.81)	Acc@5 100.00 ( 99.82)
Epoch: [55][250/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4954e-01 (1.3031e-01)	Acc@1  94.53 ( 95.82)	Acc@5  99.22 ( 99.82)
Epoch: [55][260/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4600e-01 (1.3093e-01)	Acc@1  94.53 ( 95.81)	Acc@5 100.00 ( 99.82)
Epoch: [55][270/391]	Time  0.024 ( 0.024)	Data  0.007 ( 0.003)	Loss 1.7493e-01 (1.3184e-01)	Acc@1  93.75 ( 95.78)	Acc@5 100.00 ( 99.83)
Epoch: [55][280/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.5991e-01 (1.3205e-01)	Acc@1  91.41 ( 95.76)	Acc@5 100.00 ( 99.82)
Epoch: [55][290/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 6.4514e-02 (1.3150e-01)	Acc@1  98.44 ( 95.77)	Acc@5 100.00 ( 99.83)
Epoch: [55][300/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1169e-01 (1.3078e-01)	Acc@1  96.88 ( 95.81)	Acc@5  99.22 ( 99.82)
Epoch: [55][310/391]	Time  0.038 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.4209e-01 (1.3085e-01)	Acc@1  96.88 ( 95.81)	Acc@5 100.00 ( 99.82)
Epoch: [55][320/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4294e-01 (1.3082e-01)	Acc@1  97.66 ( 95.82)	Acc@5 100.00 ( 99.82)
Epoch: [55][330/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.2266e-01 (1.3149e-01)	Acc@1  94.53 ( 95.80)	Acc@5  99.22 ( 99.83)
Epoch: [55][340/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.8787e-01 (1.3141e-01)	Acc@1  94.53 ( 95.80)	Acc@5 100.00 ( 99.83)
Epoch: [55][350/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5308e-01 (1.3141e-01)	Acc@1  95.31 ( 95.81)	Acc@5 100.00 ( 99.83)
Epoch: [55][360/391]	Time  0.038 ( 0.024)	Data  0.005 ( 0.002)	Loss 9.6191e-02 (1.3083e-01)	Acc@1  96.09 ( 95.83)	Acc@5 100.00 ( 99.83)
Epoch: [55][370/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2415e-01 (1.3069e-01)	Acc@1  96.09 ( 95.82)	Acc@5 100.00 ( 99.83)
Epoch: [55][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0459e-01 (1.3065e-01)	Acc@1  93.75 ( 95.81)	Acc@5 100.00 ( 99.82)
Epoch: [55][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5393e-01 (1.2996e-01)	Acc@1  93.75 ( 95.83)	Acc@5 100.00 ( 99.82)
## e[55] optimizer.zero_grad (sum) time: 0.08964657783508301
## e[55]       loss.backward (sum) time: 1.886253833770752
## e[55]      optimizer.step (sum) time: 0.7153303623199463
## epoch[55] training(only) time: 9.629151582717896
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.9414e+00 (1.9414e+00)	Acc@1  63.00 ( 63.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.9355e+00 (1.8322e+00)	Acc@1  65.00 ( 65.64)	Acc@5  88.00 ( 87.64)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.9404e+00 (1.7948e+00)	Acc@1  73.00 ( 67.00)	Acc@5  83.00 ( 87.76)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 2.0703e+00 (1.8031e+00)	Acc@1  61.00 ( 66.35)	Acc@5  84.00 ( 87.77)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.4551e+00 (1.7799e+00)	Acc@1  68.00 ( 66.41)	Acc@5  93.00 ( 88.27)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5752e+00 (1.7968e+00)	Acc@1  69.00 ( 66.47)	Acc@5  92.00 ( 88.24)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.5293e+00 (1.7699e+00)	Acc@1  69.00 ( 66.57)	Acc@5  90.00 ( 88.43)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.8340e+00 (1.7657e+00)	Acc@1  71.00 ( 66.77)	Acc@5  89.00 ( 88.58)
Test: [ 80/100]	Time  0.018 ( 0.015)	Loss 1.6777e+00 (1.7727e+00)	Acc@1  67.00 ( 66.77)	Acc@5  85.00 ( 88.40)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.0215e+00 (1.7658e+00)	Acc@1  62.00 ( 66.87)	Acc@5  85.00 ( 88.52)
 * Acc@1 66.910 Acc@5 88.570
### epoch[55] execution time: 11.211214065551758
EPOCH 56
REMOVING: features.module.9.weight
i:   0, name:         features.module.9.bias  changing lr from: 0.001254462473911699   to: 0.001052081333895898
i:   1, name:      features.module.11.weight  changing lr from: 0.001608836706348442   to: 0.001259544615040108
i:   2, name:        features.module.11.bias  changing lr from: 0.002100185811603083   to: 0.001613924039128566
i:   3, name:      features.module.12.weight  changing lr from: 0.002717008440443474   to: 0.002103503920496107
i:   4, name:        features.module.12.bias  changing lr from: 0.003448274757708197   to: 0.002716992235711234
i:   5, name:      features.module.15.weight  changing lr from: 0.004283453425161786   to: 0.003443557491586773
i:   6, name:        features.module.15.bias  changing lr from: 0.005212529112106856   to: 0.004272854876205561
i:   7, name:      features.module.16.weight  changing lr from: 0.006226012002497344   to: 0.005195043273460786
i:   8, name:        features.module.16.bias  changing lr from: 0.007314940599257202   to: 0.006200794549846982
i:   9, name:      features.module.18.weight  changing lr from: 0.008470878971370527   to: 0.007281296362008549
i:  10, name:        features.module.18.bias  changing lr from: 0.009685909447289441   to: 0.008428249585539432
i:  11, name:      features.module.19.weight  changing lr from: 0.010952621629164262   to: 0.009633861329948350
i:  12, name:        features.module.19.bias  changing lr from: 0.012264098485917994   to: 0.010890834381424103
i:  13, name:      features.module.22.weight  changing lr from: 0.013613900178665522   to: 0.012192353803667927
i:  14, name:        features.module.22.bias  changing lr from: 0.014996046178699780   to: 0.013532071327047968
i:  15, name:      features.module.23.weight  changing lr from: 0.016404996155438400   to: 0.014904088066994774
i:  16, name:        features.module.23.bias  changing lr from: 0.017835630038525885   to: 0.016302936033158799
i:  17, name:      features.module.25.weight  changing lr from: 0.019283227593889691   to: 0.017723558820614660
i:  18, name:        features.module.25.bias  changing lr from: 0.020743447797142996   to: 0.019161291812543165
i:  19, name:      features.module.26.weight  changing lr from: 0.022212308238534956   to: 0.020611842169587780
i:  20, name:        features.module.26.bias  changing lr from: 0.023686164750930029   to: 0.022071268833730135
i:  21, name:            classifier.0.weight  changing lr from: 0.025161691415363653   to: 0.023535962733360422
i:  22, name:              classifier.0.bias  changing lr from: 0.026635861066928859   to: 0.025002627340582114
i:  23, name:            classifier.3.weight  changing lr from: 0.028105926396510779   to: 0.026468259701073228
i:  24, name:              classifier.3.bias  changing lr from: 0.029569401720664995   to: 0.027930132030477218
i:  25, name:            classifier.6.weight  changing lr from: 0.031024045472244556   to: 0.029385773948797756
i:  26, name:              classifier.6.bias  changing lr from: 0.032467843447780496   to: 0.030832955405164821



# Switched to train mode...
Epoch: [56][  0/391]	Time  0.172 ( 0.172)	Data  0.147 ( 0.147)	Loss 5.4688e-02 (5.4688e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.022 ( 0.038)	Data  0.001 ( 0.015)	Loss 1.7212e-01 (1.1685e-01)	Acc@1  93.75 ( 96.16)	Acc@5  99.22 ( 99.86)
Epoch: [56][ 20/391]	Time  0.040 ( 0.032)	Data  0.003 ( 0.009)	Loss 1.1536e-01 (1.3529e-01)	Acc@1  96.09 ( 95.80)	Acc@5 100.00 ( 99.85)
Epoch: [56][ 30/391]	Time  0.029 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.0889e-01 (1.2735e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 ( 99.87)
Epoch: [56][ 40/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.006)	Loss 8.5022e-02 (1.2796e-01)	Acc@1  96.88 ( 95.96)	Acc@5 100.00 ( 99.89)
Epoch: [56][ 50/391]	Time  0.019 ( 0.027)	Data  0.002 ( 0.005)	Loss 1.0785e-01 (1.2501e-01)	Acc@1  97.66 ( 96.16)	Acc@5 100.00 ( 99.89)
Epoch: [56][ 60/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.004)	Loss 9.8999e-02 (1.2602e-01)	Acc@1  96.09 ( 95.98)	Acc@5 100.00 ( 99.91)
Epoch: [56][ 70/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.1157e-01 (1.2598e-01)	Acc@1  96.88 ( 96.09)	Acc@5 100.00 ( 99.90)
Epoch: [56][ 80/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.2976e-01 (1.2804e-01)	Acc@1  95.31 ( 96.02)	Acc@5 100.00 ( 99.89)
Epoch: [56][ 90/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.004)	Loss 1.2402e-01 (1.2792e-01)	Acc@1  94.53 ( 95.98)	Acc@5 100.00 ( 99.90)
Epoch: [56][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.3140e-02 (1.2431e-01)	Acc@1  96.09 ( 96.10)	Acc@5 100.00 ( 99.90)
Epoch: [56][110/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.5049e-01 (1.2668e-01)	Acc@1  92.97 ( 96.04)	Acc@5 100.00 ( 99.88)
Epoch: [56][120/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1963e-01 (1.2456e-01)	Acc@1  97.66 ( 96.14)	Acc@5 100.00 ( 99.88)
Epoch: [56][130/391]	Time  0.024 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.3953e-01 (1.2410e-01)	Acc@1  94.53 ( 96.14)	Acc@5 100.00 ( 99.89)
Epoch: [56][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2524e-01 (1.2371e-01)	Acc@1  96.09 ( 96.13)	Acc@5 100.00 ( 99.89)
Epoch: [56][150/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.3914e-02 (1.2198e-01)	Acc@1  97.66 ( 96.14)	Acc@5 100.00 ( 99.89)
Epoch: [56][160/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0052e-01 (1.2224e-01)	Acc@1  96.09 ( 96.08)	Acc@5 100.00 ( 99.89)
Epoch: [56][170/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.4167e-02 (1.2078e-01)	Acc@1  97.66 ( 96.12)	Acc@5 100.00 ( 99.89)
Epoch: [56][180/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1047e-01 (1.2103e-01)	Acc@1  97.66 ( 96.13)	Acc@5 100.00 ( 99.89)
Epoch: [56][190/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.6785e-01 (1.2084e-01)	Acc@1  95.31 ( 96.13)	Acc@5 100.00 ( 99.89)
Epoch: [56][200/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.3062e-01 (1.1970e-01)	Acc@1  95.31 ( 96.18)	Acc@5  99.22 ( 99.89)
Epoch: [56][210/391]	Time  0.029 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.3367e-01 (1.1901e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 ( 99.89)
Epoch: [56][220/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.4539e-01 (1.1986e-01)	Acc@1  95.31 ( 96.14)	Acc@5  99.22 ( 99.88)
Epoch: [56][230/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2354e-01 (1.2101e-01)	Acc@1  96.09 ( 96.10)	Acc@5 100.00 ( 99.87)
Epoch: [56][240/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.7412e-02 (1.2098e-01)	Acc@1  94.53 ( 96.10)	Acc@5 100.00 ( 99.88)
Epoch: [56][250/391]	Time  0.021 ( 0.025)	Data  0.002 ( 0.003)	Loss 9.2346e-02 (1.2097e-01)	Acc@1  98.44 ( 96.12)	Acc@5 100.00 ( 99.88)
Epoch: [56][260/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4878e-01 (1.2134e-01)	Acc@1  91.41 ( 96.09)	Acc@5 100.00 ( 99.88)
Epoch: [56][270/391]	Time  0.030 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.6553e-01 (1.2182e-01)	Acc@1  92.97 ( 96.08)	Acc@5 100.00 ( 99.87)
Epoch: [56][280/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.5754e-02 (1.2146e-01)	Acc@1  96.88 ( 96.08)	Acc@5 100.00 ( 99.87)
Epoch: [56][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5076e-01 (1.2118e-01)	Acc@1  93.75 ( 96.09)	Acc@5 100.00 ( 99.87)
Epoch: [56][300/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7944e-01 (1.2156e-01)	Acc@1  93.75 ( 96.06)	Acc@5  99.22 ( 99.87)
Epoch: [56][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1041e-01 (1.2057e-01)	Acc@1  96.09 ( 96.08)	Acc@5 100.00 ( 99.87)
Epoch: [56][320/391]	Time  0.036 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.0254e-01 (1.2147e-01)	Acc@1  98.44 ( 96.04)	Acc@5  99.22 ( 99.87)
Epoch: [56][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5247e-01 (1.2162e-01)	Acc@1  96.88 ( 96.04)	Acc@5  99.22 ( 99.87)
Epoch: [56][340/391]	Time  0.034 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.2937e-02 (1.2127e-01)	Acc@1  96.88 ( 96.05)	Acc@5 100.00 ( 99.87)
Epoch: [56][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.7290e-02 (1.2111e-01)	Acc@1  96.88 ( 96.06)	Acc@5 100.00 ( 99.87)
Epoch: [56][360/391]	Time  0.034 ( 0.024)	Data  0.004 ( 0.002)	Loss 1.6992e-01 (1.2121e-01)	Acc@1  95.31 ( 96.07)	Acc@5 100.00 ( 99.87)
Epoch: [56][370/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.9922e-01 (1.2136e-01)	Acc@1  92.97 ( 96.07)	Acc@5 100.00 ( 99.88)
Epoch: [56][380/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1792e-01 (1.2174e-01)	Acc@1  96.09 ( 96.07)	Acc@5 100.00 ( 99.87)
Epoch: [56][390/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.7271e-01 (1.2198e-01)	Acc@1  92.50 ( 96.08)	Acc@5  98.75 ( 99.87)
## e[56] optimizer.zero_grad (sum) time: 0.08605432510375977
## e[56]       loss.backward (sum) time: 1.894059658050537
## e[56]      optimizer.step (sum) time: 0.6919171810150146
## epoch[56] training(only) time: 9.646715641021729
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.9678e+00 (1.9678e+00)	Acc@1  66.00 ( 66.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 2.0254e+00 (1.8704e+00)	Acc@1  62.00 ( 65.27)	Acc@5  89.00 ( 87.55)
Test: [ 20/100]	Time  0.013 ( 0.019)	Loss 1.9033e+00 (1.8324e+00)	Acc@1  68.00 ( 66.67)	Acc@5  86.00 ( 88.19)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 2.1016e+00 (1.8283e+00)	Acc@1  62.00 ( 66.45)	Acc@5  84.00 ( 88.16)
Test: [ 40/100]	Time  0.010 ( 0.016)	Loss 1.3477e+00 (1.7904e+00)	Acc@1  71.00 ( 66.61)	Acc@5  94.00 ( 88.12)
Test: [ 50/100]	Time  0.026 ( 0.016)	Loss 1.7842e+00 (1.8071e+00)	Acc@1  69.00 ( 66.78)	Acc@5  90.00 ( 88.00)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6172e+00 (1.7782e+00)	Acc@1  72.00 ( 67.02)	Acc@5  89.00 ( 88.36)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.9570e+00 (1.7688e+00)	Acc@1  69.00 ( 67.14)	Acc@5  86.00 ( 88.55)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.8506e+00 (1.7778e+00)	Acc@1  65.00 ( 66.81)	Acc@5  86.00 ( 88.44)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.9980e+00 (1.7674e+00)	Acc@1  60.00 ( 66.96)	Acc@5  85.00 ( 88.58)
 * Acc@1 67.040 Acc@5 88.690
### epoch[56] execution time: 11.176100015640259
EPOCH 57
REMOVING: features.module.9.bias
i:   0, name:      features.module.11.weight  changing lr from: 0.001259544615040108   to: 0.001056765593916264
i:   1, name:        features.module.11.bias  changing lr from: 0.001613924039128566   to: 0.001267922441027185
i:   2, name:      features.module.12.weight  changing lr from: 0.002103503920496107   to: 0.001623925860958274
i:   3, name:        features.module.12.bias  changing lr from: 0.002716992235711234   to: 0.002113276078400421
i:   4, name:      features.module.15.weight  changing lr from: 0.003443557491586773   to: 0.002724888102081090
i:   5, name:        features.module.15.bias  changing lr from: 0.004272854876205561   to: 0.003448127286300017
i:   6, name:      features.module.16.weight  changing lr from: 0.005195043273460786   to: 0.004272834577112255
i:   7, name:        features.module.16.bias  changing lr from: 0.006200794549846982   to: 0.005189342957184677
i:   8, name:      features.module.18.weight  changing lr from: 0.007281296362008549   to: 0.006188486439628115
i:   9, name:        features.module.18.bias  changing lr from: 0.008428249585539432   to: 0.007261602808327033
i:  10, name:      features.module.19.weight  changing lr from: 0.009633861329948350   to: 0.008400531161099761
i:  11, name:        features.module.19.bias  changing lr from: 0.010890834381424103   to: 0.009597605182623852
i:  12, name:      features.module.22.weight  changing lr from: 0.012192353803667927   to: 0.010845642956328665
i:  13, name:        features.module.22.bias  changing lr from: 0.013532071327047968   to: 0.012137934018031583
i:  14, name:      features.module.23.weight  changing lr from: 0.014904088066994774   to: 0.013468224258445249
i:  15, name:        features.module.23.bias  changing lr from: 0.016302936033158799   to: 0.014830699196180335
i:  16, name:      features.module.25.weight  changing lr from: 0.017723558820614660   to: 0.016219966066810827
i:  17, name:        features.module.25.bias  changing lr from: 0.019161291812543165   to: 0.017631035106227790
i:  18, name:      features.module.26.weight  changing lr from: 0.020611842169587780   to: 0.019059300347149116
i:  19, name:        features.module.26.bias  changing lr from: 0.022071268833730135   to: 0.020500520195553963
i:  20, name:            classifier.0.weight  changing lr from: 0.023535962733360422   to: 0.021950798008276363
i:  21, name:              classifier.0.bias  changing lr from: 0.025002627340582114   to: 0.023406562853363011
i:  22, name:            classifier.3.weight  changing lr from: 0.026468259701073228   to: 0.024864550600453946
i:  23, name:              classifier.3.bias  changing lr from: 0.027930132030477218   to: 0.026321785458808308
i:  24, name:            classifier.6.weight  changing lr from: 0.029385773948797756   to: 0.027775562055139327
i:  25, name:              classifier.6.bias  changing lr from: 0.030832955405164821   to: 0.029223428121657458



# Switched to train mode...
Epoch: [57][  0/391]	Time  0.175 ( 0.175)	Data  0.147 ( 0.147)	Loss 2.1814e-01 (2.1814e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [57][ 10/391]	Time  0.019 ( 0.037)	Data  0.002 ( 0.015)	Loss 6.3721e-02 (1.1411e-01)	Acc@1  98.44 ( 96.24)	Acc@5  99.22 ( 99.72)
Epoch: [57][ 20/391]	Time  0.040 ( 0.030)	Data  0.008 ( 0.009)	Loss 9.7900e-02 (1.1614e-01)	Acc@1  96.09 ( 96.35)	Acc@5 100.00 ( 99.81)
Epoch: [57][ 30/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.007)	Loss 1.2622e-01 (1.2402e-01)	Acc@1  96.09 ( 96.17)	Acc@5 100.00 ( 99.77)
Epoch: [57][ 40/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.006)	Loss 1.3879e-01 (1.2027e-01)	Acc@1  94.53 ( 96.25)	Acc@5 100.00 ( 99.83)
Epoch: [57][ 50/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.005)	Loss 1.2537e-01 (1.1395e-01)	Acc@1  94.53 ( 96.37)	Acc@5 100.00 ( 99.86)
Epoch: [57][ 60/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.005)	Loss 7.4646e-02 (1.0968e-01)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.86)
Epoch: [57][ 70/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.004)	Loss 5.6458e-02 (1.0759e-01)	Acc@1  98.44 ( 96.62)	Acc@5 100.00 ( 99.88)
Epoch: [57][ 80/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 1.0638e-01 (1.0763e-01)	Acc@1  96.09 ( 96.62)	Acc@5 100.00 ( 99.87)
Epoch: [57][ 90/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.004)	Loss 5.9692e-02 (1.0823e-01)	Acc@1  99.22 ( 96.65)	Acc@5 100.00 ( 99.87)
Epoch: [57][100/391]	Time  0.041 ( 0.025)	Data  0.008 ( 0.004)	Loss 7.9163e-02 (1.0718e-01)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.86)
Epoch: [57][110/391]	Time  0.037 ( 0.025)	Data  0.005 ( 0.003)	Loss 8.0750e-02 (1.0714e-01)	Acc@1  96.88 ( 96.58)	Acc@5 100.00 ( 99.86)
Epoch: [57][120/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.7607e-02 (1.0710e-01)	Acc@1  98.44 ( 96.53)	Acc@5 100.00 ( 99.86)
Epoch: [57][130/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.6609e-02 (1.0624e-01)	Acc@1  97.66 ( 96.57)	Acc@5 100.00 ( 99.85)
Epoch: [57][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.9019e-01 (1.0653e-01)	Acc@1  92.19 ( 96.55)	Acc@5  99.22 ( 99.86)
Epoch: [57][150/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.4890e-02 (1.0687e-01)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.86)
Epoch: [57][160/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2219e-01 (1.0793e-01)	Acc@1  96.09 ( 96.51)	Acc@5  99.22 ( 99.85)
Epoch: [57][170/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.0718e-01 (1.0775e-01)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.85)
Epoch: [57][180/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.7993e-02 (1.0793e-01)	Acc@1  98.44 ( 96.53)	Acc@5 100.00 ( 99.86)
Epoch: [57][190/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.5356e-01 (1.0842e-01)	Acc@1  95.31 ( 96.50)	Acc@5 100.00 ( 99.87)
Epoch: [57][200/391]	Time  0.033 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.4951e-02 (1.0891e-01)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.86)
Epoch: [57][210/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.5503e-01 (1.0982e-01)	Acc@1  94.53 ( 96.46)	Acc@5 100.00 ( 99.86)
Epoch: [57][220/391]	Time  0.030 ( 0.024)	Data  0.002 ( 0.003)	Loss 1.9250e-01 (1.1018e-01)	Acc@1  93.75 ( 96.44)	Acc@5 100.00 ( 99.86)
Epoch: [57][230/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.0638e-01 (1.1070e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.86)
Epoch: [57][240/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.003)	Loss 9.2712e-02 (1.1023e-01)	Acc@1  96.09 ( 96.45)	Acc@5 100.00 ( 99.86)
Epoch: [57][250/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5198e-01 (1.1140e-01)	Acc@1  95.31 ( 96.42)	Acc@5 100.00 ( 99.86)
Epoch: [57][260/391]	Time  0.019 ( 0.024)	Data  0.000 ( 0.003)	Loss 5.8533e-02 (1.1200e-01)	Acc@1  98.44 ( 96.40)	Acc@5 100.00 ( 99.86)
Epoch: [57][270/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.1228e-02 (1.1081e-01)	Acc@1  98.44 ( 96.44)	Acc@5 100.00 ( 99.86)
Epoch: [57][280/391]	Time  0.037 ( 0.024)	Data  0.005 ( 0.002)	Loss 6.8542e-02 (1.1119e-01)	Acc@1  97.66 ( 96.42)	Acc@5 100.00 ( 99.86)
Epoch: [57][290/391]	Time  0.027 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7078e-01 (1.1126e-01)	Acc@1  96.88 ( 96.42)	Acc@5 100.00 ( 99.87)
Epoch: [57][300/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0248e-01 (1.1127e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.87)
Epoch: [57][310/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.6335e-02 (1.1172e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.87)
Epoch: [57][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.3843e-01 (1.1209e-01)	Acc@1  97.66 ( 96.39)	Acc@5 100.00 ( 99.87)
Epoch: [57][330/391]	Time  0.018 ( 0.024)	Data  0.000 ( 0.002)	Loss 9.7961e-02 (1.1243e-01)	Acc@1  96.88 ( 96.36)	Acc@5 100.00 ( 99.87)
Epoch: [57][340/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4148e-01 (1.1229e-01)	Acc@1  96.09 ( 96.36)	Acc@5  99.22 ( 99.87)
Epoch: [57][350/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4099e-01 (1.1249e-01)	Acc@1  94.53 ( 96.36)	Acc@5 100.00 ( 99.87)
Epoch: [57][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.3811e-02 (1.1281e-01)	Acc@1  94.53 ( 96.35)	Acc@5 100.00 ( 99.87)
Epoch: [57][370/391]	Time  0.033 ( 0.024)	Data  0.003 ( 0.002)	Loss 5.7434e-02 (1.1283e-01)	Acc@1  97.66 ( 96.36)	Acc@5 100.00 ( 99.87)
Epoch: [57][380/391]	Time  0.023 ( 0.024)	Data  0.005 ( 0.002)	Loss 9.2407e-02 (1.1262e-01)	Acc@1  98.44 ( 96.37)	Acc@5 100.00 ( 99.87)
Epoch: [57][390/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.0895e-01 (1.1340e-01)	Acc@1  96.25 ( 96.36)	Acc@5 100.00 ( 99.87)
## e[57] optimizer.zero_grad (sum) time: 0.0829155445098877
## e[57]       loss.backward (sum) time: 1.830488920211792
## e[57]      optimizer.step (sum) time: 0.6804935932159424
## epoch[57] training(only) time: 9.495423555374146
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.6406e+00 (1.6406e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.013 ( 0.025)	Loss 1.8262e+00 (1.8045e+00)	Acc@1  66.00 ( 66.64)	Acc@5  90.00 ( 88.18)
Test: [ 20/100]	Time  0.016 ( 0.019)	Loss 1.8154e+00 (1.7985e+00)	Acc@1  69.00 ( 67.14)	Acc@5  86.00 ( 88.24)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.9697e+00 (1.7973e+00)	Acc@1  66.00 ( 66.71)	Acc@5  85.00 ( 88.32)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.3389e+00 (1.7672e+00)	Acc@1  72.00 ( 66.85)	Acc@5  93.00 ( 88.59)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.7432e+00 (1.7861e+00)	Acc@1  72.00 ( 66.92)	Acc@5  92.00 ( 88.35)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.5312e+00 (1.7585e+00)	Acc@1  74.00 ( 67.38)	Acc@5  89.00 ( 88.46)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.9160e+00 (1.7548e+00)	Acc@1  67.00 ( 67.34)	Acc@5  88.00 ( 88.48)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.8027e+00 (1.7676e+00)	Acc@1  66.00 ( 67.15)	Acc@5  83.00 ( 88.33)
Test: [ 90/100]	Time  0.023 ( 0.015)	Loss 2.0234e+00 (1.7578e+00)	Acc@1  65.00 ( 67.29)	Acc@5  87.00 ( 88.51)
 * Acc@1 67.260 Acc@5 88.530
### epoch[57] execution time: 11.051116228103638
EPOCH 58
REMOVING: features.module.11.weight
i:   0, name:        features.module.11.bias  changing lr from: 0.001267922441027185   to: 0.001063163687890561
i:   1, name:      features.module.12.weight  changing lr from: 0.001623925860958274   to: 0.001279587815911625
i:   2, name:        features.module.12.bias  changing lr from: 0.002113276078400421   to: 0.001638721337100340
i:   3, name:      features.module.15.weight  changing lr from: 0.002724888102081090   to: 0.002129277223079184
i:   4, name:        features.module.15.bias  changing lr from: 0.003448127286300017   to: 0.002740375134649391
i:   5, name:      features.module.16.weight  changing lr from: 0.004272834577112255   to: 0.003461575620748213
i:   6, name:        features.module.16.bias  changing lr from: 0.005189342957184677   to: 0.004282904399709141
i:   7, name:      features.module.18.weight  changing lr from: 0.006188486439628115   to: 0.005194868172399574
i:   8, name:        features.module.18.bias  changing lr from: 0.007261602808327033   to: 0.006188463260730309
i:   9, name:      features.module.19.weight  changing lr from: 0.008400531161099761   to: 0.007255178219362306
i:  10, name:        features.module.19.bias  changing lr from: 0.009597605182623852   to: 0.008386991433773649
i:  11, name:      features.module.22.weight  changing lr from: 0.010845642956328665   to: 0.009576364594380077
i:  12, name:        features.module.22.bias  changing lr from: 0.012137934018031583   to: 0.010816232824008516
i:  13, name:      features.module.23.weight  changing lr from: 0.013468224258445249   to: 0.012099992134358688
i:  14, name:        features.module.23.bias  changing lr from: 0.014830699196180335   to: 0.013421484795661770
i:  15, name:      features.module.25.weight  changing lr from: 0.016219966066810827   to: 0.014774983121956565
i:  16, name:        features.module.25.bias  changing lr from: 0.017631035106227790   to: 0.016155172101594312
i:  17, name:      features.module.26.weight  changing lr from: 0.019059300347149116   to: 0.017557131238066691
i:  18, name:        features.module.26.bias  changing lr from: 0.020500520195553963   to: 0.018976315909332474
i:  19, name:            classifier.0.weight  changing lr from: 0.021950798008276363   to: 0.020408538503815462
i:  20, name:              classifier.0.bias  changing lr from: 0.023406562853363011   to: 0.021849949547503408
i:  21, name:            classifier.3.weight  changing lr from: 0.024864550600453946   to: 0.023297018998466604
i:  22, name:              classifier.3.bias  changing lr from: 0.026321785458808308   to: 0.024746517852051498
i:  23, name:            classifier.6.weight  changing lr from: 0.027775562055139327   to: 0.026195500171443387
i:  24, name:              classifier.6.bias  changing lr from: 0.029223428121657458   to: 0.027641285633726583



# Switched to train mode...
Epoch: [58][  0/391]	Time  0.173 ( 0.173)	Data  0.148 ( 0.148)	Loss 4.2450e-02 (4.2450e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.018 ( 0.035)	Data  0.001 ( 0.015)	Loss 7.2449e-02 (9.1986e-02)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.93)
Epoch: [58][ 20/391]	Time  0.017 ( 0.030)	Data  0.001 ( 0.009)	Loss 1.6370e-01 (9.7749e-02)	Acc@1  95.31 ( 96.91)	Acc@5 100.00 ( 99.93)
Epoch: [58][ 30/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.007)	Loss 1.2781e-01 (9.9666e-02)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.90)
Epoch: [58][ 40/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.006)	Loss 1.0596e-01 (9.9312e-02)	Acc@1  96.09 ( 96.76)	Acc@5 100.00 ( 99.90)
Epoch: [58][ 50/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.005)	Loss 1.3806e-01 (9.9672e-02)	Acc@1  96.09 ( 96.71)	Acc@5 100.00 ( 99.92)
Epoch: [58][ 60/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.004)	Loss 9.8267e-02 (9.8034e-02)	Acc@1  96.88 ( 96.84)	Acc@5  99.22 ( 99.92)
Epoch: [58][ 70/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.004)	Loss 6.9580e-02 (9.7383e-02)	Acc@1  96.88 ( 96.89)	Acc@5 100.00 ( 99.91)
Epoch: [58][ 80/391]	Time  0.031 ( 0.025)	Data  0.000 ( 0.004)	Loss 1.0315e-01 (9.8365e-02)	Acc@1  96.09 ( 96.84)	Acc@5 100.00 ( 99.90)
Epoch: [58][ 90/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 6.0486e-02 (9.5106e-02)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.90)
Epoch: [58][100/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 4.9683e-02 (9.5908e-02)	Acc@1 100.00 ( 96.91)	Acc@5 100.00 ( 99.90)
Epoch: [58][110/391]	Time  0.026 ( 0.025)	Data  0.003 ( 0.003)	Loss 7.2327e-02 (9.6507e-02)	Acc@1  97.66 ( 96.88)	Acc@5 100.00 ( 99.89)
Epoch: [58][120/391]	Time  0.040 ( 0.025)	Data  0.008 ( 0.003)	Loss 8.4900e-02 (9.6764e-02)	Acc@1  96.09 ( 96.91)	Acc@5 100.00 ( 99.88)
Epoch: [58][130/391]	Time  0.018 ( 0.025)	Data  0.003 ( 0.003)	Loss 1.3806e-01 (9.6607e-02)	Acc@1  92.97 ( 96.90)	Acc@5  99.22 ( 99.88)
Epoch: [58][140/391]	Time  0.040 ( 0.025)	Data  0.002 ( 0.003)	Loss 6.3599e-02 (9.7480e-02)	Acc@1  98.44 ( 96.87)	Acc@5 100.00 ( 99.87)
Epoch: [58][150/391]	Time  0.019 ( 0.024)	Data  0.003 ( 0.003)	Loss 1.1053e-01 (9.7613e-02)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.88)
Epoch: [58][160/391]	Time  0.034 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.5222e-01 (9.8075e-02)	Acc@1  96.09 ( 96.83)	Acc@5  98.44 ( 99.87)
Epoch: [58][170/391]	Time  0.027 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.8796e-02 (9.6395e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.88)
Epoch: [58][180/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.7993e-02 (9.6663e-02)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.87)
Epoch: [58][190/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.3047e-01 (9.7311e-02)	Acc@1  91.41 ( 96.86)	Acc@5 100.00 ( 99.88)
Epoch: [58][200/391]	Time  0.034 ( 0.024)	Data  0.000 ( 0.003)	Loss 8.1421e-02 (9.8145e-02)	Acc@1  96.88 ( 96.84)	Acc@5 100.00 ( 99.88)
Epoch: [58][210/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.7892e-02 (9.7370e-02)	Acc@1  98.44 ( 96.85)	Acc@5 100.00 ( 99.89)
Epoch: [58][220/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.4319e-01 (9.7003e-02)	Acc@1  95.31 ( 96.84)	Acc@5 100.00 ( 99.89)
Epoch: [58][230/391]	Time  0.027 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.2793e-01 (9.6630e-02)	Acc@1  96.09 ( 96.87)	Acc@5 100.00 ( 99.89)
Epoch: [58][240/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.3818e-01 (9.6796e-02)	Acc@1  94.53 ( 96.85)	Acc@5 100.00 ( 99.88)
Epoch: [58][250/391]	Time  0.033 ( 0.024)	Data  0.005 ( 0.003)	Loss 1.0968e-01 (9.6496e-02)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.89)
Epoch: [58][260/391]	Time  0.018 ( 0.024)	Data  0.000 ( 0.003)	Loss 2.2571e-01 (9.7740e-02)	Acc@1  94.53 ( 96.84)	Acc@5 100.00 ( 99.89)
Epoch: [58][270/391]	Time  0.032 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.1665e-02 (9.8738e-02)	Acc@1  96.09 ( 96.81)	Acc@5 100.00 ( 99.89)
Epoch: [58][280/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.5796e-02 (9.8604e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.90)
Epoch: [58][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.0120e-02 (9.8832e-02)	Acc@1  96.88 ( 96.82)	Acc@5 100.00 ( 99.90)
Epoch: [58][300/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0071e-01 (9.8976e-02)	Acc@1  96.88 ( 96.81)	Acc@5 100.00 ( 99.90)
Epoch: [58][310/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1041e-01 (9.9540e-02)	Acc@1  96.88 ( 96.79)	Acc@5 100.00 ( 99.89)
Epoch: [58][320/391]	Time  0.032 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.0862e-02 (9.9223e-02)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.89)
Epoch: [58][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0071e-01 (9.8647e-02)	Acc@1  96.09 ( 96.82)	Acc@5 100.00 ( 99.89)
Epoch: [58][340/391]	Time  0.033 ( 0.024)	Data  0.002 ( 0.002)	Loss 8.6365e-02 (9.9043e-02)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.89)
Epoch: [58][350/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5845e-01 (1.0020e-01)	Acc@1  94.53 ( 96.80)	Acc@5  99.22 ( 99.89)
Epoch: [58][360/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2097e-01 (1.0053e-01)	Acc@1  94.53 ( 96.77)	Acc@5 100.00 ( 99.90)
Epoch: [58][370/391]	Time  0.026 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.2494e-01 (1.0012e-01)	Acc@1  95.31 ( 96.78)	Acc@5 100.00 ( 99.90)
Epoch: [58][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.8867e-02 (1.0050e-01)	Acc@1  98.44 ( 96.78)	Acc@5 100.00 ( 99.90)
Epoch: [58][390/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.7810e-02 (1.0039e-01)	Acc@1  97.50 ( 96.78)	Acc@5 100.00 ( 99.90)
## e[58] optimizer.zero_grad (sum) time: 0.08015942573547363
## e[58]       loss.backward (sum) time: 1.7636148929595947
## e[58]      optimizer.step (sum) time: 0.6480944156646729
## epoch[58] training(only) time: 9.486512899398804
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.7002e+00 (1.7002e+00)	Acc@1  69.00 ( 69.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.009 ( 0.026)	Loss 1.8281e+00 (1.8241e+00)	Acc@1  66.00 ( 66.91)	Acc@5  90.00 ( 88.18)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.8965e+00 (1.8149e+00)	Acc@1  67.00 ( 67.52)	Acc@5  85.00 ( 88.14)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.0410e+00 (1.8177e+00)	Acc@1  66.00 ( 67.32)	Acc@5  85.00 ( 87.94)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.2422e+00 (1.7886e+00)	Acc@1  68.00 ( 67.17)	Acc@5  96.00 ( 88.49)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.6660e+00 (1.8052e+00)	Acc@1  70.00 ( 67.16)	Acc@5  91.00 ( 88.35)
Test: [ 60/100]	Time  0.029 ( 0.016)	Loss 1.6865e+00 (1.7719e+00)	Acc@1  71.00 ( 67.52)	Acc@5  90.00 ( 88.54)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 2.0332e+00 (1.7744e+00)	Acc@1  68.00 ( 67.61)	Acc@5  88.00 ( 88.59)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8809e+00 (1.7865e+00)	Acc@1  66.00 ( 67.41)	Acc@5  84.00 ( 88.43)
Test: [ 90/100]	Time  0.021 ( 0.015)	Loss 1.9902e+00 (1.7808e+00)	Acc@1  64.00 ( 67.67)	Acc@5  85.00 ( 88.56)
 * Acc@1 67.660 Acc@5 88.620
### epoch[58] execution time: 11.082932710647583
EPOCH 59
REMOVING: features.module.11.bias
i:   0, name:      features.module.12.weight  changing lr from: 0.001279587815911625   to: 0.001071432919921116
i:   1, name:        features.module.12.bias  changing lr from: 0.001638721337100340   to: 0.001294581811563671
i:   2, name:      features.module.15.weight  changing lr from: 0.002129277223079184   to: 0.001658243131488756
i:   3, name:        features.module.15.bias  changing lr from: 0.002740375134649391   to: 0.002151339459269457
i:   4, name:      features.module.16.weight  changing lr from: 0.003461575620748213   to: 0.002763192670801805
i:   5, name:        features.module.16.bias  changing lr from: 0.004282904399709141   to: 0.003483556731576519
i:   6, name:      features.module.18.weight  changing lr from: 0.005194868172399574   to: 0.004302640964732312
i:   7, name:        features.module.18.bias  changing lr from: 0.006188463260730309   to: 0.005211125181050851
i:   8, name:      features.module.19.weight  changing lr from: 0.007255178219362306   to: 0.006200167909249837
i:   9, name:        features.module.19.bias  changing lr from: 0.008386991433773649   to: 0.007261408826043507
i:  10, name:      features.module.22.weight  changing lr from: 0.009576364594380077   to: 0.008386966357018872
i:  11, name:        features.module.22.bias  changing lr from: 0.010816232824008516   to: 0.009569431301592629
i:  12, name:      features.module.23.weight  changing lr from: 0.012099992134358688   to: 0.010801857228045480
i:  13, name:        features.module.23.bias  changing lr from: 0.013421484795661770   to: 0.012077748287557238
i:  14, name:      features.module.25.weight  changing lr from: 0.014774983121956565   to: 0.013391045008815116
i:  15, name:        features.module.25.bias  changing lr from: 0.016155172101594312   to: 0.014736108556575415
i:  16, name:      features.module.26.weight  changing lr from: 0.017557131238066691   to: 0.016107703867902549
i:  17, name:        features.module.26.bias  changing lr from: 0.018976315909332474   to: 0.017500982018041008
i:  18, name:            classifier.0.weight  changing lr from: 0.020408538503815462   to: 0.018911462113336580
i:  19, name:              classifier.0.bias  changing lr from: 0.021849949547503408   to: 0.020335012960674071
i:  20, name:            classifier.3.weight  changing lr from: 0.023297018998466604   to: 0.021767834720910609
i:  21, name:              classifier.3.bias  changing lr from: 0.024746517852051498   to: 0.023206440717171980
i:  22, name:            classifier.6.weight  changing lr from: 0.026195500171443387   to: 0.024647639537083238
i:  23, name:              classifier.6.bias  changing lr from: 0.027641285633726583   to: 0.026088517540508740



# Switched to train mode...
Epoch: [59][  0/391]	Time  0.177 ( 0.177)	Data  0.152 ( 0.152)	Loss 6.5186e-02 (6.5186e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.019 ( 0.037)	Data  0.002 ( 0.015)	Loss 5.7983e-02 (6.6295e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.86)
Epoch: [59][ 20/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.009)	Loss 7.9041e-02 (7.0539e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 ( 99.93)
Epoch: [59][ 30/391]	Time  0.018 ( 0.028)	Data  0.001 ( 0.007)	Loss 1.0657e-01 (7.8323e-02)	Acc@1  96.88 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [59][ 40/391]	Time  0.026 ( 0.027)	Data  0.002 ( 0.005)	Loss 7.9651e-02 (7.6367e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.94)
Epoch: [59][ 50/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.0571e-01 (8.4211e-02)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.95)
Epoch: [59][ 60/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.004)	Loss 7.4402e-02 (8.5710e-02)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.94)
Epoch: [59][ 70/391]	Time  0.020 ( 0.025)	Data  0.005 ( 0.004)	Loss 1.2573e-01 (8.5101e-02)	Acc@1  94.53 ( 97.46)	Acc@5 100.00 ( 99.93)
Epoch: [59][ 80/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.004)	Loss 9.6863e-02 (8.6017e-02)	Acc@1  96.09 ( 97.42)	Acc@5 100.00 ( 99.94)
Epoch: [59][ 90/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.004)	Loss 1.3086e-01 (8.7840e-02)	Acc@1  96.09 ( 97.36)	Acc@5 100.00 ( 99.94)
Epoch: [59][100/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.004)	Loss 7.6111e-02 (8.9834e-02)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 ( 99.91)
Epoch: [59][110/391]	Time  0.020 ( 0.025)	Data  0.004 ( 0.004)	Loss 1.5442e-01 (9.0072e-02)	Acc@1  94.53 ( 97.26)	Acc@5 100.00 ( 99.91)
Epoch: [59][120/391]	Time  0.025 ( 0.025)	Data  0.002 ( 0.004)	Loss 7.3059e-02 (9.1169e-02)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.91)
Epoch: [59][130/391]	Time  0.024 ( 0.025)	Data  0.001 ( 0.004)	Loss 1.2659e-01 (9.0724e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.91)
Epoch: [59][140/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.003)	Loss 9.0759e-02 (9.0853e-02)	Acc@1  97.66 ( 97.16)	Acc@5  99.22 ( 99.89)
Epoch: [59][150/391]	Time  0.018 ( 0.025)	Data  0.000 ( 0.003)	Loss 5.5420e-02 (9.0525e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.89)
Epoch: [59][160/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.4038e-01 (9.1113e-02)	Acc@1  96.09 ( 97.12)	Acc@5 100.00 ( 99.88)
Epoch: [59][170/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.0383e-02 (9.0631e-02)	Acc@1  95.31 ( 97.12)	Acc@5 100.00 ( 99.88)
Epoch: [59][180/391]	Time  0.034 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.5449e-02 (9.0362e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.88)
Epoch: [59][190/391]	Time  0.032 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.6915e-02 (9.0630e-02)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.88)
Epoch: [59][200/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.4050e-01 (9.2001e-02)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.88)
Epoch: [59][210/391]	Time  0.039 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.3060e-02 (9.2446e-02)	Acc@1 100.00 ( 97.09)	Acc@5 100.00 ( 99.87)
Epoch: [59][220/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.0017e-02 (9.2453e-02)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.87)
Epoch: [59][230/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.2842e-01 (9.2842e-02)	Acc@1  94.53 ( 97.04)	Acc@5 100.00 ( 99.87)
Epoch: [59][240/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.3611e-01 (9.3000e-02)	Acc@1  95.31 ( 97.03)	Acc@5 100.00 ( 99.87)
Epoch: [59][250/391]	Time  0.018 ( 0.024)	Data  0.000 ( 0.003)	Loss 8.8440e-02 (9.3100e-02)	Acc@1  97.66 ( 97.05)	Acc@5 100.00 ( 99.88)
Epoch: [59][260/391]	Time  0.017 ( 0.024)	Data  0.002 ( 0.003)	Loss 1.1053e-01 (9.3721e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.87)
Epoch: [59][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.6060e-02 (9.4415e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.88)
Epoch: [59][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.6797e-01 (9.4389e-02)	Acc@1  93.75 ( 96.99)	Acc@5 100.00 ( 99.88)
Epoch: [59][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.5454e-01 (9.4345e-02)	Acc@1  96.88 ( 97.00)	Acc@5  99.22 ( 99.88)
Epoch: [59][300/391]	Time  0.018 ( 0.024)	Data  0.000 ( 0.003)	Loss 8.9722e-02 (9.4348e-02)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.88)
Epoch: [59][310/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.003)	Loss 1.0028e-01 (9.4115e-02)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.89)
Epoch: [59][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.5248e-02 (9.3899e-02)	Acc@1  99.22 ( 97.02)	Acc@5 100.00 ( 99.89)
Epoch: [59][330/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.003)	Loss 7.3547e-02 (9.3928e-02)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 ( 99.89)
Epoch: [59][340/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.002)	Loss 9.6069e-02 (9.4148e-02)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.89)
Epoch: [59][350/391]	Time  0.033 ( 0.024)	Data  0.004 ( 0.002)	Loss 1.2286e-01 (9.4678e-02)	Acc@1  96.88 ( 96.98)	Acc@5 100.00 ( 99.89)
Epoch: [59][360/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.0383e-02 (9.5113e-02)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.89)
Epoch: [59][370/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4368e-01 (9.5233e-02)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.89)
Epoch: [59][380/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.4412e-02 (9.5530e-02)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.89)
Epoch: [59][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2305e-01 (9.5701e-02)	Acc@1  93.75 ( 96.97)	Acc@5 100.00 ( 99.89)
## e[59] optimizer.zero_grad (sum) time: 0.07724308967590332
## e[59]       loss.backward (sum) time: 1.7426540851593018
## e[59]      optimizer.step (sum) time: 0.6213610172271729
## epoch[59] training(only) time: 9.486156702041626
# Switched to evaluate mode...
Test: [  0/100]	Time  0.135 ( 0.135)	Loss 1.9297e+00 (1.9297e+00)	Acc@1  65.00 ( 65.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.013 ( 0.024)	Loss 2.0020e+00 (1.8717e+00)	Acc@1  64.00 ( 66.36)	Acc@5  88.00 ( 87.91)
Test: [ 20/100]	Time  0.014 ( 0.019)	Loss 1.8809e+00 (1.8396e+00)	Acc@1  69.00 ( 66.90)	Acc@5  85.00 ( 88.10)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.9111e+00 (1.8274e+00)	Acc@1  65.00 ( 66.65)	Acc@5  85.00 ( 88.19)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.4453e+00 (1.7940e+00)	Acc@1  71.00 ( 66.68)	Acc@5  92.00 ( 88.63)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.7051e+00 (1.8098e+00)	Acc@1  70.00 ( 66.96)	Acc@5  92.00 ( 88.45)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.6562e+00 (1.7710e+00)	Acc@1  73.00 ( 67.54)	Acc@5  89.00 ( 88.67)
Test: [ 70/100]	Time  0.020 ( 0.015)	Loss 2.0195e+00 (1.7745e+00)	Acc@1  67.00 ( 67.61)	Acc@5  87.00 ( 88.72)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.7490e+00 (1.7840e+00)	Acc@1  67.00 ( 67.46)	Acc@5  87.00 ( 88.67)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.1230e+00 (1.7772e+00)	Acc@1  65.00 ( 67.70)	Acc@5  86.00 ( 88.86)
 * Acc@1 67.810 Acc@5 88.960
### epoch[59] execution time: 11.03338885307312
EPOCH 60
i:   0, name:      features.module.12.weight  changing lr from: 0.001071432919921116   to: 0.001000031304809517
i:   1, name:        features.module.12.bias  changing lr from: 0.001294581811563671   to: 0.001081766737363126
i:   2, name:      features.module.15.weight  changing lr from: 0.001658243131488756   to: 0.001312986544437699
i:   3, name:        features.module.15.bias  changing lr from: 0.002151339459269457   to: 0.001682469261058444
i:   4, name:      features.module.16.weight  changing lr from: 0.002763192670801805   to: 0.002179343979084480
i:   5, name:        features.module.16.bias  changing lr from: 0.003483556731576519   to: 0.002793132319537649
i:   6, name:      features.module.18.weight  changing lr from: 0.004302640964732312   to: 0.003513779788825827
i:   7, name:        features.module.18.bias  changing lr from: 0.005211125181050851   to: 0.004331677996018834
i:   8, name:      features.module.19.weight  changing lr from: 0.006200167909249837   to: 0.005237679057968085
i:   9, name:        features.module.19.bias  changing lr from: 0.007261408826043507   to: 0.006223103377181998
i:  10, name:      features.module.22.weight  changing lr from: 0.008386966357018872   to: 0.007279741844950388
i:  11, name:        features.module.22.bias  changing lr from: 0.009569431301592629   to: 0.008399853399760092
i:  12, name:      features.module.23.weight  changing lr from: 0.010801857228045480   to: 0.009576158758704278
i:  13, name:        features.module.23.bias  changing lr from: 0.012077748287557238   to: 0.010801831037246173
i:  14, name:      features.module.25.weight  changing lr from: 0.013391045008815116   to: 0.012070483880039524
i:  15, name:        features.module.25.bias  changing lr from: 0.014736108556575415   to: 0.013376157642088448
i:  16, name:      features.module.26.weight  changing lr from: 0.016107703867902549   to: 0.014713304084814045
i:  17, name:        features.module.26.bias  changing lr from: 0.017500982018041008   to: 0.016076769984993862
i:  18, name:            classifier.0.weight  changing lr from: 0.018911462113336580   to: 0.017461779995440296
i:  19, name:              classifier.0.bias  changing lr from: 0.020335012960674071   to: 0.018863919044065407
i:  20, name:            classifier.3.weight  changing lr from: 0.021767834720910609   to: 0.020279114512031057
i:  21, name:              classifier.3.bias  changing lr from: 0.023206440717171980   to: 0.021703618391418364
i:  22, name:            classifier.6.weight  changing lr from: 0.024647639537083238   to: 0.023133989587708383
i:  23, name:              classifier.6.bias  changing lr from: 0.026088517540508740   to: 0.024567076501818660



# Switched to train mode...
Epoch: [60][  0/391]	Time  0.168 ( 0.168)	Data  0.142 ( 0.142)	Loss 1.3330e-01 (1.3330e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.22 ( 99.22)
Epoch: [60][ 10/391]	Time  0.019 ( 0.035)	Data  0.001 ( 0.015)	Loss 6.8176e-02 (7.8161e-02)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.93)
Epoch: [60][ 20/391]	Time  0.040 ( 0.030)	Data  0.001 ( 0.009)	Loss 9.7778e-02 (8.4221e-02)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.93)
Epoch: [60][ 30/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.007)	Loss 9.1309e-02 (8.6241e-02)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.90)
Epoch: [60][ 40/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.005)	Loss 4.8981e-02 (9.0839e-02)	Acc@1  99.22 ( 97.03)	Acc@5 100.00 ( 99.89)
Epoch: [60][ 50/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.005)	Loss 8.3191e-02 (9.2880e-02)	Acc@1  99.22 ( 96.97)	Acc@5  99.22 ( 99.85)
Epoch: [60][ 60/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.004)	Loss 7.5317e-02 (9.1033e-02)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.87)
Epoch: [60][ 70/391]	Time  0.033 ( 0.025)	Data  0.000 ( 0.004)	Loss 7.4158e-02 (9.1916e-02)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.88)
Epoch: [60][ 80/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.004)	Loss 6.2988e-02 (8.9564e-02)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.89)
Epoch: [60][ 90/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2915e-01 (9.1038e-02)	Acc@1  96.09 ( 97.06)	Acc@5 100.00 ( 99.91)
Epoch: [60][100/391]	Time  0.027 ( 0.025)	Data  0.005 ( 0.003)	Loss 8.1421e-02 (8.9205e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.91)
Epoch: [60][110/391]	Time  0.027 ( 0.025)	Data  0.002 ( 0.003)	Loss 1.1462e-01 (8.8445e-02)	Acc@1  97.66 ( 97.11)	Acc@5  99.22 ( 99.91)
Epoch: [60][120/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.6284e-02 (8.8025e-02)	Acc@1  99.22 ( 97.15)	Acc@5 100.00 ( 99.91)
Epoch: [60][130/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.003)	Loss 5.6641e-02 (8.8479e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.90)
Epoch: [60][140/391]	Time  0.029 ( 0.024)	Data  0.000 ( 0.003)	Loss 9.2041e-02 (8.7883e-02)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.89)
Epoch: [60][150/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.9529e-02 (8.7610e-02)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.90)
Epoch: [60][160/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.3196e-01 (8.8666e-02)	Acc@1  96.88 ( 97.07)	Acc@5  99.22 ( 99.89)
Epoch: [60][170/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 9.1736e-02 (8.9557e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.90)
Epoch: [60][180/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.0425e-01 (8.9921e-02)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.90)
Epoch: [60][190/391]	Time  0.037 ( 0.024)	Data  0.005 ( 0.003)	Loss 1.1731e-01 (8.9707e-02)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.91)
Epoch: [60][200/391]	Time  0.023 ( 0.024)	Data  0.000 ( 0.003)	Loss 7.0496e-02 (8.9473e-02)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.90)
Epoch: [60][210/391]	Time  0.038 ( 0.024)	Data  0.000 ( 0.002)	Loss 8.7097e-02 (8.8561e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.90)
Epoch: [60][220/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.5825e-02 (8.7970e-02)	Acc@1  94.53 ( 97.15)	Acc@5 100.00 ( 99.90)
Epoch: [60][230/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.0383e-02 (8.7760e-02)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.91)
Epoch: [60][240/391]	Time  0.038 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.9631e-02 (8.8991e-02)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.90)
Epoch: [60][250/391]	Time  0.025 ( 0.024)	Data  0.003 ( 0.002)	Loss 5.8594e-02 (8.8756e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.90)
Epoch: [60][260/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5161e-01 (8.7986e-02)	Acc@1  94.53 ( 97.18)	Acc@5  99.22 ( 99.90)
Epoch: [60][270/391]	Time  0.023 ( 0.024)	Data  0.000 ( 0.002)	Loss 5.9052e-02 (8.7744e-02)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.90)
Epoch: [60][280/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.9224e-02 (8.7788e-02)	Acc@1  98.44 ( 97.20)	Acc@5 100.00 ( 99.90)
Epoch: [60][290/391]	Time  0.019 ( 0.024)	Data  0.003 ( 0.002)	Loss 3.7872e-02 (8.7747e-02)	Acc@1 100.00 ( 97.22)	Acc@5 100.00 ( 99.90)
Epoch: [60][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.6305e-02 (8.7130e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.90)
Epoch: [60][310/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.0642e-01 (8.7410e-02)	Acc@1  93.75 ( 97.22)	Acc@5 100.00 ( 99.90)
Epoch: [60][320/391]	Time  0.025 ( 0.024)	Data  0.002 ( 0.002)	Loss 7.8369e-02 (8.7548e-02)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.90)
Epoch: [60][330/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1902e-01 (8.7467e-02)	Acc@1  95.31 ( 97.24)	Acc@5  99.22 ( 99.90)
Epoch: [60][340/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.0364e-02 (8.7426e-02)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.90)
Epoch: [60][350/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.7900e-02 (8.7057e-02)	Acc@1  96.09 ( 97.26)	Acc@5  99.22 ( 99.90)
Epoch: [60][360/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.7017e-02 (8.6472e-02)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.90)
Epoch: [60][370/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.7881e-02 (8.6063e-02)	Acc@1  97.66 ( 97.29)	Acc@5 100.00 ( 99.91)
Epoch: [60][380/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.6356e-02 (8.5481e-02)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.91)
Epoch: [60][390/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.2185e-02 (8.6615e-02)	Acc@1  98.75 ( 97.29)	Acc@5 100.00 ( 99.90)
## e[60] optimizer.zero_grad (sum) time: 0.07781982421875
## e[60]       loss.backward (sum) time: 1.7470383644104004
## e[60]      optimizer.step (sum) time: 0.6248018741607666
## epoch[60] training(only) time: 9.50421142578125
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.7520e+00 (1.7520e+00)	Acc@1  64.00 ( 64.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.9688e+00 (1.8588e+00)	Acc@1  62.00 ( 65.91)	Acc@5  89.00 ( 88.09)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.8906e+00 (1.8390e+00)	Acc@1  71.00 ( 67.24)	Acc@5  85.00 ( 88.38)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 2.1133e+00 (1.8473e+00)	Acc@1  61.00 ( 66.61)	Acc@5  85.00 ( 88.00)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.4385e+00 (1.8147e+00)	Acc@1  68.00 ( 66.88)	Acc@5  94.00 ( 88.41)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.7158e+00 (1.8370e+00)	Acc@1  73.00 ( 67.00)	Acc@5  92.00 ( 88.29)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.6123e+00 (1.8056e+00)	Acc@1  71.00 ( 67.30)	Acc@5  89.00 ( 88.49)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 2.0098e+00 (1.8040e+00)	Acc@1  68.00 ( 67.37)	Acc@5  88.00 ( 88.58)
Test: [ 80/100]	Time  0.022 ( 0.016)	Loss 1.8848e+00 (1.8154e+00)	Acc@1  63.00 ( 67.19)	Acc@5  84.00 ( 88.43)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 2.1953e+00 (1.8121e+00)	Acc@1  61.00 ( 67.38)	Acc@5  85.00 ( 88.53)
 * Acc@1 67.500 Acc@5 88.610
### epoch[60] execution time: 11.096564531326294
EPOCH 61
REMOVING: features.module.12.weight
i:   0, name:        features.module.12.bias  changing lr from: 0.001081766737363126   to: 0.001000838383641066
i:   1, name:      features.module.15.weight  changing lr from: 0.001312986544437699   to: 0.001094387558515613
i:   2, name:        features.module.15.bias  changing lr from: 0.001682469261058444   to: 0.001334917939320961
i:   3, name:      features.module.16.weight  changing lr from: 0.002179343979084480   to: 0.001711415845405972
i:   4, name:        features.module.16.bias  changing lr from: 0.002793132319537649   to: 0.002213213842553168
i:   5, name:      features.module.18.weight  changing lr from: 0.003513779788825827   to: 0.002830030813008454
i:   6, name:        features.module.18.bias  changing lr from: 0.004331677996018834   to: 0.003552001853859770
i:   7, name:      features.module.19.weight  changing lr from: 0.005237679057968085   to: 0.004369699415659711
i:   8, name:        features.module.19.bias  changing lr from: 0.006223103377181998   to: 0.005274146949752172
i:   9, name:      features.module.22.weight  changing lr from: 0.007279741844950388   to: 0.006256826197476239
i:  10, name:        features.module.22.bias  changing lr from: 0.008399853399760092   to: 0.007309679128174573
i:  11, name:      features.module.23.weight  changing lr from: 0.009576158758704278   to: 0.008425105416184846
i:  12, name:        features.module.23.bias  changing lr from: 0.010801831037246173   to: 0.009595956239870550
i:  13, name:      features.module.25.weight  changing lr from: 0.012070483880039524   to: 0.010815525088129763
i:  14, name:        features.module.25.bias  changing lr from: 0.013376157642088448   to: 0.012077536171408283
i:  15, name:      features.module.26.weight  changing lr from: 0.014713304084814045   to: 0.013376130954611938
i:  16, name:        features.module.26.bias  changing lr from: 0.016076769984993862   to: 0.014705853257953087
i:  17, name:            classifier.0.weight  changing lr from: 0.017461779995440296   to: 0.016061633308121864
i:  18, name:              classifier.0.bias  changing lr from: 0.018863919044065407   to: 0.017438771065660764
i:  19, name:            classifier.3.weight  changing lr from: 0.020279114512031057   to: 0.018832919104455750
i:  20, name:              classifier.3.bias  changing lr from: 0.021703618391418364   to: 0.020240065275261090
i:  21, name:            classifier.6.weight  changing lr from: 0.023133989587708383   to: 0.021656515346591371
i:  22, name:              classifier.6.bias  changing lr from: 0.024567076501818660   to: 0.023078875782611877



# Switched to train mode...
Epoch: [61][  0/391]	Time  0.170 ( 0.170)	Data  0.138 ( 0.138)	Loss 5.7526e-02 (5.7526e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.027 ( 0.037)	Data  0.001 ( 0.014)	Loss 8.8257e-02 (9.1261e-02)	Acc@1  98.44 ( 96.73)	Acc@5 100.00 ( 99.93)
Epoch: [61][ 20/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.008)	Loss 1.5686e-01 (8.8491e-02)	Acc@1  95.31 ( 97.10)	Acc@5  99.22 ( 99.93)
Epoch: [61][ 30/391]	Time  0.027 ( 0.028)	Data  0.002 ( 0.006)	Loss 5.7220e-02 (8.7505e-02)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 ( 99.92)
Epoch: [61][ 40/391]	Time  0.024 ( 0.027)	Data  0.004 ( 0.005)	Loss 5.7404e-02 (8.3649e-02)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.94)
Epoch: [61][ 50/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.005)	Loss 6.9153e-02 (8.6429e-02)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.91)
Epoch: [61][ 60/391]	Time  0.034 ( 0.026)	Data  0.005 ( 0.004)	Loss 1.0162e-01 (8.5185e-02)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 ( 99.91)
Epoch: [61][ 70/391]	Time  0.023 ( 0.025)	Data  0.005 ( 0.004)	Loss 3.8513e-02 (8.3890e-02)	Acc@1  99.22 ( 97.41)	Acc@5 100.00 ( 99.92)
Epoch: [61][ 80/391]	Time  0.018 ( 0.025)	Data  0.002 ( 0.004)	Loss 1.0535e-01 (8.4426e-02)	Acc@1  96.09 ( 97.35)	Acc@5 100.00 ( 99.92)
Epoch: [61][ 90/391]	Time  0.031 ( 0.025)	Data  0.003 ( 0.003)	Loss 7.4463e-02 (8.3112e-02)	Acc@1  96.88 ( 97.39)	Acc@5 100.00 ( 99.93)
Epoch: [61][100/391]	Time  0.043 ( 0.025)	Data  0.001 ( 0.003)	Loss 8.1055e-02 (8.3520e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.94)
Epoch: [61][110/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.8857e-02 (8.2327e-02)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.94)
Epoch: [61][120/391]	Time  0.017 ( 0.025)	Data  0.002 ( 0.003)	Loss 3.9368e-02 (8.0761e-02)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.94)
Epoch: [61][130/391]	Time  0.035 ( 0.025)	Data  0.001 ( 0.003)	Loss 2.8427e-02 (8.1066e-02)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.93)
Epoch: [61][140/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.003)	Loss 4.0955e-02 (8.0951e-02)	Acc@1 100.00 ( 97.53)	Acc@5 100.00 ( 99.93)
Epoch: [61][150/391]	Time  0.049 ( 0.024)	Data  0.012 ( 0.003)	Loss 7.1045e-02 (8.1546e-02)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.93)
Epoch: [61][160/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.7953e-02 (8.2255e-02)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.93)
Epoch: [61][170/391]	Time  0.027 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.5894e-01 (8.2750e-02)	Acc@1  95.31 ( 97.46)	Acc@5  99.22 ( 99.93)
Epoch: [61][180/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.2205e-02 (8.3024e-02)	Acc@1  97.66 ( 97.43)	Acc@5  99.22 ( 99.93)
Epoch: [61][190/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.2928e-02 (8.3043e-02)	Acc@1  99.22 ( 97.43)	Acc@5 100.00 ( 99.93)
Epoch: [61][200/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.0132e-01 (8.3814e-02)	Acc@1  95.31 ( 97.39)	Acc@5 100.00 ( 99.93)
Epoch: [61][210/391]	Time  0.040 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.5248e-02 (8.2482e-02)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 ( 99.93)
Epoch: [61][220/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.1029e-01 (8.3130e-02)	Acc@1  93.75 ( 97.43)	Acc@5 100.00 ( 99.93)
Epoch: [61][230/391]	Time  0.034 ( 0.024)	Data  0.004 ( 0.002)	Loss 6.4026e-02 (8.3296e-02)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.93)
Epoch: [61][240/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4648e-01 (8.3344e-02)	Acc@1  95.31 ( 97.44)	Acc@5  99.22 ( 99.93)
Epoch: [61][250/391]	Time  0.027 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.3035e-01 (8.3703e-02)	Acc@1  92.97 ( 97.41)	Acc@5 100.00 ( 99.93)
Epoch: [61][260/391]	Time  0.040 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.7952e-02 (8.3643e-02)	Acc@1  98.44 ( 97.41)	Acc@5 100.00 ( 99.93)
Epoch: [61][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.6641e-02 (8.2631e-02)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.93)
Epoch: [61][280/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5552e-01 (8.3241e-02)	Acc@1  94.53 ( 97.41)	Acc@5 100.00 ( 99.93)
Epoch: [61][290/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.9041e-02 (8.2998e-02)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.93)
Epoch: [61][300/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.5686e-01 (8.3482e-02)	Acc@1  95.31 ( 97.39)	Acc@5  99.22 ( 99.92)
Epoch: [61][310/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.2927e-02 (8.3260e-02)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.92)
Epoch: [61][320/391]	Time  0.039 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.4307e-01 (8.3473e-02)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.92)
Epoch: [61][330/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.2458e-02 (8.3864e-02)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.92)
Epoch: [61][340/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.7786e-01 (8.4101e-02)	Acc@1  95.31 ( 97.37)	Acc@5 100.00 ( 99.92)
Epoch: [61][350/391]	Time  0.034 ( 0.024)	Data  0.002 ( 0.002)	Loss 1.2384e-01 (8.3995e-02)	Acc@1  96.09 ( 97.36)	Acc@5 100.00 ( 99.92)
Epoch: [61][360/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.6069e-02 (8.3982e-02)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.92)
Epoch: [61][370/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0059e-01 (8.4631e-02)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.92)
Epoch: [61][380/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.0618e-02 (8.4147e-02)	Acc@1  97.66 ( 97.37)	Acc@5  99.22 ( 99.92)
Epoch: [61][390/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.1910e-02 (8.4547e-02)	Acc@1  97.50 ( 97.36)	Acc@5 100.00 ( 99.92)
## e[61] optimizer.zero_grad (sum) time: 0.07501339912414551
## e[61]       loss.backward (sum) time: 1.744879961013794
## e[61]      optimizer.step (sum) time: 0.6019902229309082
## epoch[61] training(only) time: 9.474601984024048
# Switched to evaluate mode...
Test: [  0/100]	Time  0.134 ( 0.134)	Loss 2.0117e+00 (2.0117e+00)	Acc@1  65.00 ( 65.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.009 ( 0.025)	Loss 1.8965e+00 (1.8786e+00)	Acc@1  65.00 ( 66.45)	Acc@5  90.00 ( 88.09)
Test: [ 20/100]	Time  0.020 ( 0.020)	Loss 1.9893e+00 (1.8437e+00)	Acc@1  70.00 ( 67.71)	Acc@5  85.00 ( 88.33)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.0449e+00 (1.8437e+00)	Acc@1  63.00 ( 66.97)	Acc@5  84.00 ( 87.90)
Test: [ 40/100]	Time  0.021 ( 0.017)	Loss 1.4014e+00 (1.8043e+00)	Acc@1  69.00 ( 67.05)	Acc@5  94.00 ( 88.41)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.7344e+00 (1.8241e+00)	Acc@1  71.00 ( 67.12)	Acc@5  91.00 ( 88.29)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.5273e+00 (1.7880e+00)	Acc@1  70.00 ( 67.48)	Acc@5  88.00 ( 88.59)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.9258e+00 (1.7816e+00)	Acc@1  68.00 ( 67.49)	Acc@5  89.00 ( 88.77)
Test: [ 80/100]	Time  0.021 ( 0.015)	Loss 1.8848e+00 (1.7933e+00)	Acc@1  68.00 ( 67.36)	Acc@5  85.00 ( 88.63)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 2.1660e+00 (1.7884e+00)	Acc@1  63.00 ( 67.51)	Acc@5  86.00 ( 88.77)
 * Acc@1 67.600 Acc@5 88.790
### epoch[61] execution time: 11.04065203666687
EPOCH 62
REMOVING: features.module.12.bias
i:   0, name:      features.module.15.weight  changing lr from: 0.001094387558515613   to: 0.001003003406311880
i:   1, name:        features.module.15.bias  changing lr from: 0.001334917939320961   to: 0.001109540454458274
i:   2, name:      features.module.16.weight  changing lr from: 0.001711415845405972   to: 0.001360519325921599
i:   3, name:        features.module.16.bias  changing lr from: 0.002213213842553168   to: 0.001745130663774835
i:   4, name:      features.module.18.weight  changing lr from: 0.002830030813008454   to: 0.002252907536540343
i:   5, name:        features.module.18.bias  changing lr from: 0.003552001853859770   to: 0.002873763604772383
i:   6, name:      features.module.19.weight  changing lr from: 0.004369699415659711   to: 0.003598021550265117
i:   7, name:        features.module.19.bias  changing lr from: 0.005274146949752172   to: 0.004416433116881424
i:   8, name:      features.module.22.weight  changing lr from: 0.006256826197476239   to: 0.005320191975154253
i:   9, name:        features.module.22.bias  changing lr from: 0.007309679128174573   to: 0.006300940493804940
i:  10, name:      features.module.23.weight  changing lr from: 0.008425105416184846   to: 0.007350771380962928
i:  11, name:        features.module.23.bias  changing lr from: 0.009595956239870550   to: 0.008462225046575345
i:  12, name:      features.module.25.weight  changing lr from: 0.010815525088129763   to: 0.009628283435365075
i:  13, name:        features.module.25.bias  changing lr from: 0.012077536171408283   to: 0.010842360986609560
i:  14, name:      features.module.26.weight  changing lr from: 0.013376130954611938   to: 0.012098293292679974
i:  15, name:        features.module.26.bias  changing lr from: 0.014705853257953087   to: 0.013390323952294243
i:  16, name:            classifier.0.weight  changing lr from: 0.016061633308121864   to: 0.014713090046314270
i:  17, name:              classifier.0.bias  changing lr from: 0.017438771065660764   to: 0.016061606603129837
i:  18, name:            classifier.3.weight  changing lr from: 0.018832919104455750   to: 0.017431250366665287
i:  19, name:              classifier.3.bias  changing lr from: 0.020240065275261090   to: 0.018817743132266841
i:  20, name:            classifier.6.weight  changing lr from: 0.021656515346591371   to: 0.020217134873630801
i:  21, name:              classifier.6.bias  changing lr from: 0.023078875782611877   to: 0.021625786846987516



# Switched to train mode...
Epoch: [62][  0/391]	Time  0.173 ( 0.173)	Data  0.148 ( 0.148)	Loss 4.5044e-02 (4.5044e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.018 ( 0.036)	Data  0.002 ( 0.015)	Loss 7.0068e-02 (7.5464e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.009)	Loss 1.2018e-01 (7.1854e-02)	Acc@1  96.09 ( 97.73)	Acc@5  98.44 ( 99.89)
Epoch: [62][ 30/391]	Time  0.017 ( 0.028)	Data  0.001 ( 0.006)	Loss 7.7820e-02 (7.4136e-02)	Acc@1  99.22 ( 97.76)	Acc@5  99.22 ( 99.87)
Epoch: [62][ 40/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.005)	Loss 1.4221e-01 (7.7279e-02)	Acc@1  96.09 ( 97.50)	Acc@5 100.00 ( 99.90)
Epoch: [62][ 50/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.005)	Loss 3.3478e-02 (7.4149e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.92)
Epoch: [62][ 60/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.004)	Loss 4.3671e-02 (7.6278e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.92)
Epoch: [62][ 70/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.004)	Loss 3.6041e-02 (7.5482e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.92)
Epoch: [62][ 80/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.004)	Loss 9.7473e-02 (7.5811e-02)	Acc@1  95.31 ( 97.56)	Acc@5 100.00 ( 99.92)
Epoch: [62][ 90/391]	Time  0.017 ( 0.025)	Data  0.002 ( 0.003)	Loss 5.1544e-02 (7.3836e-02)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.93)
Epoch: [62][100/391]	Time  0.018 ( 0.024)	Data  0.003 ( 0.003)	Loss 8.5632e-02 (7.3914e-02)	Acc@1  96.09 ( 97.65)	Acc@5 100.00 ( 99.94)
Epoch: [62][110/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.2177e-01 (7.4521e-02)	Acc@1  96.88 ( 97.66)	Acc@5  99.22 ( 99.94)
Epoch: [62][120/391]	Time  0.042 ( 0.024)	Data  0.004 ( 0.003)	Loss 9.6741e-02 (7.5718e-02)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.94)
Epoch: [62][130/391]	Time  0.018 ( 0.024)	Data  0.002 ( 0.003)	Loss 7.5745e-02 (7.5254e-02)	Acc@1  96.09 ( 97.64)	Acc@5 100.00 ( 99.94)
Epoch: [62][140/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.4087e-02 (7.5875e-02)	Acc@1  99.22 ( 97.65)	Acc@5 100.00 ( 99.94)
Epoch: [62][150/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.9233e-02 (7.5547e-02)	Acc@1  96.09 ( 97.64)	Acc@5 100.00 ( 99.94)
Epoch: [62][160/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 9.1614e-02 (7.5752e-02)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [62][170/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.4697e-01 (7.6215e-02)	Acc@1  96.88 ( 97.64)	Acc@5 100.00 ( 99.94)
Epoch: [62][180/391]	Time  0.017 ( 0.024)	Data  0.002 ( 0.003)	Loss 6.9275e-02 (7.7208e-02)	Acc@1  98.44 ( 97.62)	Acc@5  99.22 ( 99.94)
Epoch: [62][190/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.4697e-02 (7.6955e-02)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.94)
Epoch: [62][200/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.2399e-02 (7.6335e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.93)
Epoch: [62][210/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.6895e-01 (7.6404e-02)	Acc@1  96.88 ( 97.64)	Acc@5 100.00 ( 99.94)
Epoch: [62][220/391]	Time  0.033 ( 0.024)	Data  0.005 ( 0.003)	Loss 5.5878e-02 (7.6533e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.94)
Epoch: [62][230/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.0547e-02 (7.6709e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.94)
Epoch: [62][240/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.1338e-01 (7.7525e-02)	Acc@1  93.75 ( 97.61)	Acc@5 100.00 ( 99.94)
Epoch: [62][250/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 2.5375e-02 (7.6852e-02)	Acc@1  99.22 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [62][260/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.2815e-02 (7.6116e-02)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.95)
Epoch: [62][270/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.0942e-02 (7.5575e-02)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.95)
Epoch: [62][280/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.3086e-01 (7.5641e-02)	Acc@1  95.31 ( 97.69)	Acc@5 100.00 ( 99.95)
Epoch: [62][290/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.6853e-02 (7.6129e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.95)
Epoch: [62][300/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0510e-01 (7.6548e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [62][310/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.6504e-01 (7.6456e-02)	Acc@1  95.31 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [62][320/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.9275e-02 (7.6392e-02)	Acc@1  96.09 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [62][330/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 8.9966e-02 (7.6434e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [62][340/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.4260e-02 (7.6849e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.95)
Epoch: [62][350/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.3374e-02 (7.6535e-02)	Acc@1  97.66 ( 97.64)	Acc@5 100.00 ( 99.95)
Epoch: [62][360/391]	Time  0.027 ( 0.024)	Data  0.002 ( 0.002)	Loss 6.7444e-02 (7.6233e-02)	Acc@1  98.44 ( 97.65)	Acc@5 100.00 ( 99.95)
Epoch: [62][370/391]	Time  0.038 ( 0.024)	Data  0.004 ( 0.002)	Loss 5.6671e-02 (7.5654e-02)	Acc@1  97.66 ( 97.67)	Acc@5 100.00 ( 99.95)
Epoch: [62][380/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 9.4604e-02 (7.5788e-02)	Acc@1  96.09 ( 97.66)	Acc@5  99.22 ( 99.95)
Epoch: [62][390/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 7.2876e-02 (7.5639e-02)	Acc@1  96.25 ( 97.67)	Acc@5 100.00 ( 99.95)
## e[62] optimizer.zero_grad (sum) time: 0.07205367088317871
## e[62]       loss.backward (sum) time: 1.6467502117156982
## e[62]      optimizer.step (sum) time: 0.5730834007263184
## epoch[62] training(only) time: 9.37905740737915
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.9473e+00 (1.9473e+00)	Acc@1  66.00 ( 66.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.016 ( 0.026)	Loss 1.9170e+00 (1.9031e+00)	Acc@1  64.00 ( 65.73)	Acc@5  88.00 ( 87.82)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.9404e+00 (1.8629e+00)	Acc@1  70.00 ( 67.10)	Acc@5  86.00 ( 88.24)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 2.1504e+00 (1.8778e+00)	Acc@1  61.00 ( 66.61)	Acc@5  84.00 ( 87.97)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.5176e+00 (1.8403e+00)	Acc@1  66.00 ( 66.78)	Acc@5  94.00 ( 88.56)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.8018e+00 (1.8666e+00)	Acc@1  71.00 ( 66.88)	Acc@5  90.00 ( 88.37)
Test: [ 60/100]	Time  0.019 ( 0.016)	Loss 1.5840e+00 (1.8324e+00)	Acc@1  70.00 ( 67.34)	Acc@5  88.00 ( 88.62)
Test: [ 70/100]	Time  0.025 ( 0.015)	Loss 1.9678e+00 (1.8253e+00)	Acc@1  68.00 ( 67.49)	Acc@5  89.00 ( 88.73)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8633e+00 (1.8366e+00)	Acc@1  67.00 ( 67.27)	Acc@5  87.00 ( 88.59)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.2129e+00 (1.8298e+00)	Acc@1  60.00 ( 67.46)	Acc@5  89.00 ( 88.76)
 * Acc@1 67.610 Acc@5 88.790
### epoch[62] execution time: 10.951310873031616
EPOCH 63
REMOVING: features.module.15.weight
i:   0, name:        features.module.15.bias  changing lr from: 0.001109540454458274   to: 0.001006891224804717
i:   1, name:      features.module.16.weight  changing lr from: 0.001360519325921599   to: 0.001127487595022446
i:   2, name:        features.module.16.bias  changing lr from: 0.001745130663774835   to: 0.001389955783209533
i:   3, name:      features.module.18.weight  changing lr from: 0.002252907536540343   to: 0.001783687439436576
i:   4, name:        features.module.18.bias  changing lr from: 0.002873763604772383   to: 0.002298413237411095
i:   5, name:      features.module.19.weight  changing lr from: 0.003598021550265117   to: 0.002924239145477347
i:   6, name:        features.module.19.bias  changing lr from: 0.004416433116881424   to: 0.003651673383774772
i:   7, name:      features.module.22.weight  changing lr from: 0.005320191975154253   to: 0.004471645355970466
i:   8, name:        features.module.22.bias  changing lr from: 0.006300940493804940   to: 0.005375517713409371
i:   9, name:      features.module.23.weight  changing lr from: 0.007350771380962928   to: 0.006355092586498656
i:  10, name:        features.module.23.bias  changing lr from: 0.008462225046575345   to: 0.007402612903401900
i:  11, name:      features.module.25.weight  changing lr from: 0.009628283435365075   to: 0.008510759610030566
i:  12, name:        features.module.25.bias  changing lr from: 0.010842360986609560   to: 0.009672645507968869
i:  13, name:      features.module.26.weight  changing lr from: 0.012098293292679974   to: 0.010881806338225768
i:  14, name:        features.module.26.bias  changing lr from: 0.013390323952294243   to: 0.012132189658290986
i:  15, name:            classifier.0.weight  changing lr from: 0.014713090046314270   to: 0.013418141987491548
i:  16, name:              classifier.0.bias  changing lr from: 0.016061606603129837   to: 0.014734394630641533
i:  17, name:            classifier.3.weight  changing lr from: 0.017431250366665287   to: 0.016076048531944682
i:  18, name:              classifier.3.bias  changing lr from: 0.018817743132266841   to: 0.017438558459527721
i:  19, name:            classifier.6.weight  changing lr from: 0.020217134873630801   to: 0.018817716775322180
i:  20, name:              classifier.6.bias  changing lr from: 0.021625786846987516   to: 0.020209637004757187



# Switched to train mode...
Epoch: [63][  0/391]	Time  0.170 ( 0.170)	Data  0.147 ( 0.147)	Loss 6.6589e-02 (6.6589e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.018 ( 0.036)	Data  0.001 ( 0.015)	Loss 1.5955e-01 (9.1661e-02)	Acc@1  96.09 ( 97.30)	Acc@5 100.00 ( 99.86)
Epoch: [63][ 20/391]	Time  0.018 ( 0.030)	Data  0.001 ( 0.009)	Loss 3.3966e-02 (8.1939e-02)	Acc@1 100.00 ( 97.62)	Acc@5 100.00 ( 99.93)
Epoch: [63][ 30/391]	Time  0.017 ( 0.028)	Data  0.002 ( 0.006)	Loss 9.1492e-02 (8.2021e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.95)
Epoch: [63][ 40/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.005)	Loss 5.5389e-02 (8.0716e-02)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.94)
Epoch: [63][ 50/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.005)	Loss 7.4097e-02 (7.7499e-02)	Acc@1  96.88 ( 97.67)	Acc@5 100.00 ( 99.95)
Epoch: [63][ 60/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.004)	Loss 4.6539e-02 (7.6649e-02)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 ( 99.94)
Epoch: [63][ 70/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.004)	Loss 3.2074e-02 (7.3933e-02)	Acc@1  99.22 ( 97.78)	Acc@5 100.00 ( 99.94)
Epoch: [63][ 80/391]	Time  0.027 ( 0.025)	Data  0.001 ( 0.004)	Loss 7.4768e-02 (7.3474e-02)	Acc@1  96.88 ( 97.73)	Acc@5 100.00 ( 99.95)
Epoch: [63][ 90/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.003)	Loss 1.1310e-01 (7.3627e-02)	Acc@1  97.66 ( 97.77)	Acc@5 100.00 ( 99.96)
Epoch: [63][100/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.003)	Loss 1.0895e-01 (7.3897e-02)	Acc@1  96.09 ( 97.75)	Acc@5  99.22 ( 99.95)
Epoch: [63][110/391]	Time  0.037 ( 0.025)	Data  0.001 ( 0.003)	Loss 7.3242e-02 (7.3290e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.95)
Epoch: [63][120/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.6091e-02 (7.2439e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.95)
Epoch: [63][130/391]	Time  0.028 ( 0.024)	Data  0.002 ( 0.003)	Loss 5.9479e-02 (7.3711e-02)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 ( 99.95)
Epoch: [63][140/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.5195e-02 (7.3883e-02)	Acc@1  98.44 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [63][150/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.6779e-02 (7.3103e-02)	Acc@1 100.00 ( 97.75)	Acc@5 100.00 ( 99.95)
Epoch: [63][160/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.3672e-01 (7.2883e-02)	Acc@1  96.09 ( 97.77)	Acc@5  99.22 ( 99.95)
Epoch: [63][170/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.4413e-02 (7.2697e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.95)
Epoch: [63][180/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.003)	Loss 9.8877e-02 (7.1982e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.95)
Epoch: [63][190/391]	Time  0.034 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.1408e-02 (7.1306e-02)	Acc@1 100.00 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [63][200/391]	Time  0.041 ( 0.024)	Data  0.003 ( 0.003)	Loss 5.6915e-02 (7.1102e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [63][210/391]	Time  0.018 ( 0.024)	Data  0.003 ( 0.003)	Loss 6.8176e-02 (7.0793e-02)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.96)
Epoch: [63][220/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.003)	Loss 1.8188e-02 (7.0399e-02)	Acc@1 100.00 ( 97.79)	Acc@5 100.00 ( 99.96)
Epoch: [63][230/391]	Time  0.036 ( 0.024)	Data  0.000 ( 0.002)	Loss 9.6985e-02 (7.0700e-02)	Acc@1  96.09 ( 97.76)	Acc@5 100.00 ( 99.96)
Epoch: [63][240/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.4474e-02 (7.0761e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [63][250/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.7158e-02 (7.0982e-02)	Acc@1  96.09 ( 97.74)	Acc@5 100.00 ( 99.97)
Epoch: [63][260/391]	Time  0.033 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.7609e-02 (7.0835e-02)	Acc@1 100.00 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [63][270/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.6406e-02 (7.0739e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [63][280/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.0175e-01 (7.0649e-02)	Acc@1  96.09 ( 97.76)	Acc@5 100.00 ( 99.96)
Epoch: [63][290/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.0779e-01 (7.1195e-02)	Acc@1  95.31 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [63][300/391]	Time  0.032 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.6382e-02 (7.0973e-02)	Acc@1  99.22 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [63][310/391]	Time  0.017 ( 0.024)	Data  0.002 ( 0.002)	Loss 9.6252e-02 (7.0696e-02)	Acc@1  96.88 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [63][320/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.2671e-01 (7.0857e-02)	Acc@1  96.88 ( 97.73)	Acc@5  99.22 ( 99.95)
Epoch: [63][330/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2267e-02 (7.0847e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.95)
Epoch: [63][340/391]	Time  0.043 ( 0.024)	Data  0.005 ( 0.002)	Loss 5.3986e-02 (7.0585e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.95)
Epoch: [63][350/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.1981e-02 (7.0864e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.95)
Epoch: [63][360/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.7637e-02 (7.0673e-02)	Acc@1  97.66 ( 97.74)	Acc@5 100.00 ( 99.95)
Epoch: [63][370/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 7.6599e-02 (7.0656e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.95)
Epoch: [63][380/391]	Time  0.019 ( 0.024)	Data  0.000 ( 0.002)	Loss 9.1736e-02 (7.0780e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.95)
Epoch: [63][390/391]	Time  0.018 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.3672e-01 (7.0460e-02)	Acc@1  95.00 ( 97.76)	Acc@5 100.00 ( 99.95)
## e[63] optimizer.zero_grad (sum) time: 0.06874966621398926
## e[63]       loss.backward (sum) time: 1.5870201587677002
## e[63]      optimizer.step (sum) time: 0.5507297515869141
## epoch[63] training(only) time: 9.345717430114746
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 2.0059e+00 (2.0059e+00)	Acc@1  68.00 ( 68.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.9561e+00 (1.9240e+00)	Acc@1  63.00 ( 66.73)	Acc@5  89.00 ( 87.82)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.9639e+00 (1.8864e+00)	Acc@1  72.00 ( 67.52)	Acc@5  85.00 ( 88.29)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 2.1582e+00 (1.8944e+00)	Acc@1  63.00 ( 67.10)	Acc@5  84.00 ( 88.06)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.4443e+00 (1.8587e+00)	Acc@1  68.00 ( 67.49)	Acc@5  94.00 ( 88.56)
Test: [ 50/100]	Time  0.009 ( 0.017)	Loss 1.7588e+00 (1.8835e+00)	Acc@1  71.00 ( 67.59)	Acc@5  91.00 ( 88.25)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 1.6797e+00 (1.8519e+00)	Acc@1  72.00 ( 67.92)	Acc@5  89.00 ( 88.44)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 2.0820e+00 (1.8463e+00)	Acc@1  67.00 ( 67.96)	Acc@5  88.00 ( 88.66)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.9395e+00 (1.8571e+00)	Acc@1  69.00 ( 67.75)	Acc@5  85.00 ( 88.52)
Test: [ 90/100]	Time  0.017 ( 0.015)	Loss 2.2637e+00 (1.8506e+00)	Acc@1  61.00 ( 67.89)	Acc@5  89.00 ( 88.71)
 * Acc@1 67.890 Acc@5 88.720
### epoch[63] execution time: 10.940003633499146
EPOCH 64
REMOVING: features.module.15.bias
i:   0, name:      features.module.16.weight  changing lr from: 0.001127487595022446   to: 0.001012873967032578
i:   1, name:        features.module.16.bias  changing lr from: 0.001389955783209533   to: 0.001148503378051790
i:   2, name:      features.module.18.weight  changing lr from: 0.001783687439436576   to: 0.001423409154726140
i:   3, name:        features.module.18.bias  changing lr from: 0.002298413237411095   to: 0.001827180778991487
i:   4, name:      features.module.19.weight  changing lr from: 0.002924239145477347   to: 0.002349743709436413
i:   5, name:        features.module.19.bias  changing lr from: 0.003651673383774772   to: 0.002981393772531515
i:   6, name:      features.module.22.weight  changing lr from: 0.004471645355970466   to: 0.003712822652358493
i:   7, name:        features.module.22.bias  changing lr from: 0.005375517713409371   to: 0.004535135708945697
i:   8, name:      features.module.23.weight  changing lr from: 0.006355092586498656   to: 0.005439863230714314
i:   9, name:        features.module.23.bias  changing lr from: 0.007402612903401900   to: 0.006418966109213681
i:  10, name:      features.module.25.weight  changing lr from: 0.008510759610030566   to: 0.007464836814943842
i:  11, name:        features.module.25.bias  changing lr from: 0.009672645507968869   to: 0.008570296451951681
i:  12, name:      features.module.26.weight  changing lr from: 0.010881806338225768   to: 0.009728588576108026
i:  13, name:        features.module.26.bias  changing lr from: 0.012132189658290986   to: 0.010933370377391331
i:  14, name:            classifier.0.weight  changing lr from: 0.013418141987491548   to: 0.012178701749844624
i:  15, name:              classifier.0.bias  changing lr from: 0.014734394630641533   to: 0.013459032703761371
i:  16, name:            classifier.3.weight  changing lr from: 0.016076048531944682   to: 0.014769189512652353
i:  17, name:              classifier.3.bias  changing lr from: 0.017438558459527721   to: 0.016104359932170586
i:  18, name:            classifier.6.weight  changing lr from: 0.018817716775322180   to: 0.017460077778929588
i:  19, name:              classifier.6.bias  changing lr from: 0.020209637004757187   to: 0.018832207113539435



# Switched to train mode...
Epoch: [64][  0/391]	Time  0.172 ( 0.172)	Data  0.146 ( 0.146)	Loss 6.7200e-02 (6.7200e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.015)	Loss 2.3911e-02 (5.2804e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.017 ( 0.029)	Data  0.001 ( 0.009)	Loss 4.6265e-02 (5.8490e-02)	Acc@1  97.66 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.017 ( 0.027)	Data  0.001 ( 0.007)	Loss 8.3435e-02 (6.3128e-02)	Acc@1  96.88 ( 98.03)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 40/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.005)	Loss 5.5939e-02 (6.5081e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 50/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.005)	Loss 5.0476e-02 (6.7372e-02)	Acc@1  99.22 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 60/391]	Time  0.017 ( 0.025)	Data  0.002 ( 0.004)	Loss 1.4917e-01 (6.9997e-02)	Acc@1  94.53 ( 97.77)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 70/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.004)	Loss 1.3618e-02 (6.8899e-02)	Acc@1 100.00 ( 97.89)	Acc@5 100.00 ( 99.96)
Epoch: [64][ 80/391]	Time  0.036 ( 0.025)	Data  0.005 ( 0.004)	Loss 1.0968e-01 (6.9692e-02)	Acc@1  96.88 ( 97.86)	Acc@5 100.00 ( 99.95)
Epoch: [64][ 90/391]	Time  0.017 ( 0.025)	Data  0.000 ( 0.004)	Loss 4.7913e-02 (7.0567e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.95)
Epoch: [64][100/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.004)	Loss 7.2266e-02 (7.0943e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.95)
Epoch: [64][110/391]	Time  0.025 ( 0.024)	Data  0.004 ( 0.003)	Loss 6.6040e-02 (7.1122e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.95)
Epoch: [64][120/391]	Time  0.032 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.0242e-01 (7.0270e-02)	Acc@1  95.31 ( 97.80)	Acc@5 100.00 ( 99.95)
Epoch: [64][130/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.6040e-02 (7.0693e-02)	Acc@1  97.66 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [64][140/391]	Time  0.041 ( 0.024)	Data  0.005 ( 0.003)	Loss 6.2225e-02 (6.9522e-02)	Acc@1  98.44 ( 97.79)	Acc@5  99.22 ( 99.94)
Epoch: [64][150/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.1208e-02 (6.8249e-02)	Acc@1  99.22 ( 97.84)	Acc@5 100.00 ( 99.94)
Epoch: [64][160/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.5156e-02 (6.8165e-02)	Acc@1  99.22 ( 97.86)	Acc@5 100.00 ( 99.95)
Epoch: [64][170/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.0598e-02 (6.8752e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.95)
Epoch: [64][180/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.003)	Loss 2.4429e-02 (6.8195e-02)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 ( 99.95)
Epoch: [64][190/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.1371e-02 (6.7864e-02)	Acc@1  97.66 ( 97.84)	Acc@5 100.00 ( 99.95)
Epoch: [64][200/391]	Time  0.021 ( 0.024)	Data  0.000 ( 0.003)	Loss 5.4993e-02 (6.7444e-02)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 ( 99.95)
Epoch: [64][210/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.1016e-02 (6.6833e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 ( 99.95)
Epoch: [64][220/391]	Time  0.017 ( 0.024)	Data  0.002 ( 0.003)	Loss 6.0211e-02 (6.6906e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 ( 99.95)
Epoch: [64][230/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.9347e-02 (6.7862e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.95)
Epoch: [64][240/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.003)	Loss 9.6680e-02 (6.7924e-02)	Acc@1  97.66 ( 97.83)	Acc@5 100.00 ( 99.95)
Epoch: [64][250/391]	Time  0.036 ( 0.024)	Data  0.003 ( 0.003)	Loss 2.4768e-01 (6.7926e-02)	Acc@1  93.75 ( 97.84)	Acc@5  99.22 ( 99.94)
Epoch: [64][260/391]	Time  0.039 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.1046e-02 (6.8547e-02)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.94)
Epoch: [64][270/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.4617e-02 (6.7950e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.95)
Epoch: [64][280/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.8641e-02 (6.8196e-02)	Acc@1 100.00 ( 97.83)	Acc@5 100.00 ( 99.94)
Epoch: [64][290/391]	Time  0.023 ( 0.024)	Data  0.000 ( 0.003)	Loss 5.4199e-02 (6.8269e-02)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.94)
Epoch: [64][300/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.0516e-02 (6.8135e-02)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.94)
Epoch: [64][310/391]	Time  0.022 ( 0.024)	Data  0.004 ( 0.003)	Loss 6.5613e-02 (6.8232e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.94)
Epoch: [64][320/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1121e-01 (6.8315e-02)	Acc@1  95.31 ( 97.82)	Acc@5 100.00 ( 99.94)
Epoch: [64][330/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1115e-01 (6.8777e-02)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.94)
Epoch: [64][340/391]	Time  0.038 ( 0.024)	Data  0.002 ( 0.002)	Loss 4.4128e-02 (6.8632e-02)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 ( 99.94)
Epoch: [64][350/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.9712e-02 (6.8735e-02)	Acc@1  96.09 ( 97.82)	Acc@5 100.00 ( 99.94)
Epoch: [64][360/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.4749e-02 (6.8756e-02)	Acc@1  98.44 ( 97.80)	Acc@5 100.00 ( 99.95)
Epoch: [64][370/391]	Time  0.046 ( 0.024)	Data  0.005 ( 0.002)	Loss 8.3313e-02 (6.8892e-02)	Acc@1  96.09 ( 97.79)	Acc@5 100.00 ( 99.95)
Epoch: [64][380/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 2.9648e-02 (6.8735e-02)	Acc@1  98.44 ( 97.80)	Acc@5 100.00 ( 99.94)
Epoch: [64][390/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.4983e-02 (6.8935e-02)	Acc@1 100.00 ( 97.79)	Acc@5 100.00 ( 99.94)
## e[64] optimizer.zero_grad (sum) time: 0.06609869003295898
## e[64]       loss.backward (sum) time: 1.5779328346252441
## e[64]      optimizer.step (sum) time: 0.5249319076538086
## epoch[64] training(only) time: 9.3015296459198
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.9102e+00 (1.9102e+00)	Acc@1  68.00 ( 68.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.016 ( 0.026)	Loss 1.9150e+00 (1.8933e+00)	Acc@1  63.00 ( 66.55)	Acc@5  89.00 ( 88.18)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 1.9561e+00 (1.8755e+00)	Acc@1  68.00 ( 66.81)	Acc@5  86.00 ( 88.52)
Test: [ 30/100]	Time  0.014 ( 0.018)	Loss 2.0625e+00 (1.8825e+00)	Acc@1  62.00 ( 66.58)	Acc@5  84.00 ( 88.32)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.4326e+00 (1.8465e+00)	Acc@1  67.00 ( 66.93)	Acc@5  94.00 ( 88.76)
Test: [ 50/100]	Time  0.014 ( 0.016)	Loss 1.8008e+00 (1.8723e+00)	Acc@1  68.00 ( 67.08)	Acc@5  91.00 ( 88.37)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6943e+00 (1.8402e+00)	Acc@1  68.00 ( 67.46)	Acc@5  88.00 ( 88.59)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.9834e+00 (1.8381e+00)	Acc@1  69.00 ( 67.61)	Acc@5  87.00 ( 88.65)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.8145e+00 (1.8479e+00)	Acc@1  66.00 ( 67.42)	Acc@5  85.00 ( 88.48)
Test: [ 90/100]	Time  0.024 ( 0.015)	Loss 2.1465e+00 (1.8385e+00)	Acc@1  64.00 ( 67.64)	Acc@5  87.00 ( 88.66)
 * Acc@1 67.640 Acc@5 88.740
### epoch[64] execution time: 10.913435220718384
EPOCH 65
REMOVING: features.module.16.weight
i:   0, name:        features.module.16.bias  changing lr from: 0.001148503378051790   to: 0.001021326936322857
i:   1, name:      features.module.18.weight  changing lr from: 0.001423409154726140   to: 0.001172870169405936
i:   2, name:        features.module.18.bias  changing lr from: 0.001827180778991487   to: 0.001461073665645662
i:   3, name:      features.module.19.weight  changing lr from: 0.002349743709436413   to: 0.001875721700960425
i:   4, name:        features.module.19.bias  changing lr from: 0.002981393772531515   to: 0.002406931777079701
i:   5, name:      features.module.22.weight  changing lr from: 0.003712822652358493   to: 0.003045187155782485
i:   6, name:        features.module.22.bias  changing lr from: 0.004535135708945697   to: 0.003781360892440016
i:   7, name:      features.module.23.weight  changing lr from: 0.005439863230714314   to: 0.004606732542877039
i:   8, name:        features.module.23.bias  changing lr from: 0.006418966109213681   to: 0.005512998598651597
i:   9, name:      features.module.25.weight  changing lr from: 0.007464836814943842   to: 0.006492277593963069
i:  10, name:        features.module.25.bias  changing lr from: 0.008570296451951681   to: 0.007537110723133639
i:  11, name:      features.module.26.weight  changing lr from: 0.009728588576108026   to: 0.008640458711251769
i:  12, name:        features.module.26.bias  changing lr from: 0.010933370377391331   to: 0.009795695592160158
i:  13, name:            classifier.0.weight  changing lr from: 0.012178701749844624   to: 0.010996599967373667
i:  14, name:              classifier.0.bias  changing lr from: 0.013459032703761371   to: 0.012237344246458191
i:  15, name:            classifier.3.weight  changing lr from: 0.014769189512652353   to: 0.013512482303524153
i:  16, name:              classifier.3.bias  changing lr from: 0.016104359932170586   to: 0.014816935925371377
i:  17, name:            classifier.6.weight  changing lr from: 0.017460077778929588   to: 0.016145980374006625
i:  18, name:              classifier.6.bias  changing lr from: 0.018832207113539435   to: 0.017495229339269416



# Switched to train mode...
Epoch: [65][  0/391]	Time  0.175 ( 0.175)	Data  0.150 ( 0.150)	Loss 2.7939e-02 (2.7939e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.027 ( 0.037)	Data  0.001 ( 0.015)	Loss 6.6650e-02 (6.2912e-02)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.93)
Epoch: [65][ 20/391]	Time  0.017 ( 0.030)	Data  0.001 ( 0.009)	Loss 9.6375e-02 (6.4133e-02)	Acc@1  96.09 ( 97.88)	Acc@5 100.00 ( 99.96)
Epoch: [65][ 30/391]	Time  0.018 ( 0.028)	Data  0.000 ( 0.007)	Loss 6.4575e-02 (6.6492e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.95)
Epoch: [65][ 40/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.005)	Loss 4.9896e-02 (6.4778e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 ( 99.94)
Epoch: [65][ 50/391]	Time  0.037 ( 0.026)	Data  0.011 ( 0.005)	Loss 9.9121e-02 (6.3223e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 ( 99.95)
Epoch: [65][ 60/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.004)	Loss 4.1931e-02 (6.2770e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 ( 99.96)
Epoch: [65][ 70/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.004)	Loss 5.5664e-02 (6.2479e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 ( 99.97)
Epoch: [65][ 80/391]	Time  0.025 ( 0.025)	Data  0.004 ( 0.004)	Loss 3.7140e-02 (6.3612e-02)	Acc@1 100.00 ( 98.03)	Acc@5 100.00 ( 99.97)
Epoch: [65][ 90/391]	Time  0.017 ( 0.025)	Data  0.002 ( 0.004)	Loss 7.1045e-02 (6.3760e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 ( 99.97)
Epoch: [65][100/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.9417e-02 (6.5885e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.95)
Epoch: [65][110/391]	Time  0.024 ( 0.024)	Data  0.004 ( 0.003)	Loss 1.0883e-01 (6.5954e-02)	Acc@1  96.09 ( 97.90)	Acc@5 100.00 ( 99.96)
Epoch: [65][120/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.1443e-02 (6.5105e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.95)
Epoch: [65][130/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.0901e-01 (6.6294e-02)	Acc@1  96.09 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [65][140/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.7495e-02 (6.7174e-02)	Acc@1  97.66 ( 97.84)	Acc@5 100.00 ( 99.94)
Epoch: [65][150/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.7176e-02 (6.6897e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 ( 99.94)
Epoch: [65][160/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.8976e-02 (6.6562e-02)	Acc@1  99.22 ( 97.89)	Acc@5 100.00 ( 99.94)
Epoch: [65][170/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.0468e-01 (6.6989e-02)	Acc@1  97.66 ( 97.89)	Acc@5  99.22 ( 99.94)
Epoch: [65][180/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.4067e-02 (6.7662e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.93)
Epoch: [65][190/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.1086e-02 (6.8285e-02)	Acc@1  99.22 ( 97.84)	Acc@5  99.22 ( 99.93)
Epoch: [65][200/391]	Time  0.027 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.0505e-02 (6.8227e-02)	Acc@1  96.09 ( 97.84)	Acc@5 100.00 ( 99.93)
Epoch: [65][210/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.003)	Loss 8.2779e-03 (6.6799e-02)	Acc@1 100.00 ( 97.90)	Acc@5 100.00 ( 99.93)
Epoch: [65][220/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.9163e-02 (6.6842e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.93)
Epoch: [65][230/391]	Time  0.017 ( 0.024)	Data  0.002 ( 0.002)	Loss 5.0201e-02 (6.6704e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.94)
Epoch: [65][240/391]	Time  0.038 ( 0.024)	Data  0.000 ( 0.003)	Loss 5.5573e-02 (6.6633e-02)	Acc@1  99.22 ( 97.89)	Acc@5 100.00 ( 99.94)
Epoch: [65][250/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 6.1066e-02 (6.6575e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.93)
Epoch: [65][260/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.002)	Loss 8.1665e-02 (6.6323e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.94)
Epoch: [65][270/391]	Time  0.033 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.1533e-02 (6.5811e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.94)
Epoch: [65][280/391]	Time  0.023 ( 0.024)	Data  0.000 ( 0.002)	Loss 1.3196e-01 (6.6317e-02)	Acc@1  96.88 ( 97.93)	Acc@5 100.00 ( 99.94)
Epoch: [65][290/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.4088e-02 (6.5674e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.94)
Epoch: [65][300/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 7.2327e-02 (6.5566e-02)	Acc@1  96.09 ( 97.94)	Acc@5 100.00 ( 99.95)
Epoch: [65][310/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.2572e-02 (6.4921e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.94)
Epoch: [65][320/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.3496e-02 (6.5181e-02)	Acc@1  96.09 ( 97.94)	Acc@5 100.00 ( 99.94)
Epoch: [65][330/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.9551e-02 (6.5420e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [65][340/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 8.8989e-02 (6.5680e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [65][350/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 4.5380e-02 (6.5575e-02)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 ( 99.94)
Epoch: [65][360/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.002)	Loss 5.8899e-02 (6.5242e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.95)
Epoch: [65][370/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.7231e-02 (6.4779e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.95)
Epoch: [65][380/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.002)	Loss 1.1780e-01 (6.4931e-02)	Acc@1  96.88 ( 97.97)	Acc@5 100.00 ( 99.95)
Epoch: [65][390/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 7.3425e-02 (6.5038e-02)	Acc@1  97.50 ( 97.96)	Acc@5 100.00 ( 99.95)
## e[65] optimizer.zero_grad (sum) time: 0.0640099048614502
## e[65]       loss.backward (sum) time: 1.5774178504943848
## e[65]      optimizer.step (sum) time: 0.49819183349609375
## epoch[65] training(only) time: 9.273873567581177
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.9512e+00 (1.9512e+00)	Acc@1  66.00 ( 66.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 1.9375e+00 (1.8990e+00)	Acc@1  64.00 ( 66.55)	Acc@5  89.00 ( 88.36)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 2.0078e+00 (1.8951e+00)	Acc@1  69.00 ( 67.14)	Acc@5  85.00 ( 88.24)
Test: [ 30/100]	Time  0.012 ( 0.019)	Loss 2.1035e+00 (1.9127e+00)	Acc@1  63.00 ( 66.58)	Acc@5  86.00 ( 88.23)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4199e+00 (1.8775e+00)	Acc@1  69.00 ( 66.71)	Acc@5  93.00 ( 88.76)
Test: [ 50/100]	Time  0.016 ( 0.017)	Loss 1.8018e+00 (1.9032e+00)	Acc@1  68.00 ( 66.94)	Acc@5  91.00 ( 88.41)
Test: [ 60/100]	Time  0.023 ( 0.016)	Loss 1.6494e+00 (1.8723e+00)	Acc@1  72.00 ( 67.38)	Acc@5  89.00 ( 88.59)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 2.0215e+00 (1.8680e+00)	Acc@1  71.00 ( 67.48)	Acc@5  89.00 ( 88.73)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 1.9111e+00 (1.8762e+00)	Acc@1  69.00 ( 67.43)	Acc@5  85.00 ( 88.59)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.2129e+00 (1.8667e+00)	Acc@1  62.00 ( 67.59)	Acc@5  84.00 ( 88.76)
 * Acc@1 67.630 Acc@5 88.800
### epoch[65] execution time: 10.85776400566101
EPOCH 66
REMOVING: features.module.16.bias
i:   0, name:      features.module.18.weight  changing lr from: 0.001172870169405936   to: 0.001032625059178931
i:   1, name:        features.module.18.bias  changing lr from: 0.001461073665645662   to: 0.001200874588649691
i:   2, name:      features.module.19.weight  changing lr from: 0.001875721700960425   to: 0.001503152079248454
i:   3, name:        features.module.19.bias  changing lr from: 0.002406931777079701   to: 0.001929433694304871
i:   4, name:      features.module.22.weight  changing lr from: 0.003045187155782485   to: 0.002470026314968837
i:   5, name:        features.module.22.bias  changing lr from: 0.003781360892440016   to: 0.003115598246306610
i:   6, name:      features.module.23.weight  changing lr from: 0.004606732542877039   to: 0.003857201811713078
i:   7, name:        features.module.23.bias  changing lr from: 0.005512998598651597   to: 0.004686288955724737
i:   8, name:      features.module.25.weight  changing lr from: 0.006492277593963069   to: 0.005594720861824314
i:   9, name:        features.module.25.bias  changing lr from: 0.007537110723133639   to: 0.006574772485122864
i:  10, name:      features.module.26.weight  changing lr from: 0.008640458711251769   to: 0.007619132800423822
i:  11, name:        features.module.26.bias  changing lr from: 0.009795695592160158   to: 0.008720901474360131
i:  12, name:            classifier.0.weight  changing lr from: 0.010996599967373667   to: 0.009873582586070285
i:  13, name:              classifier.0.bias  changing lr from: 0.012237344246458191   to: 0.011071075944099180
i:  14, name:            classifier.3.weight  changing lr from: 0.013512482303524153   to: 0.012307666477606775
i:  15, name:              classifier.3.bias  changing lr from: 0.014816935925371377   to: 0.013578012117196329
i:  16, name:            classifier.6.weight  changing lr from: 0.016145980374006625   to: 0.014877130524329190
i:  17, name:              classifier.6.bias  changing lr from: 0.017495229339269416   to: 0.016200384977940008



# Switched to train mode...
Epoch: [66][  0/391]	Time  0.173 ( 0.173)	Data  0.149 ( 0.149)	Loss 1.1700e-01 (1.1700e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.024 ( 0.036)	Data  0.001 ( 0.015)	Loss 8.2947e-02 (8.4151e-02)	Acc@1  95.31 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.032 ( 0.030)	Data  0.002 ( 0.008)	Loss 2.0370e-02 (7.0250e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.96)
Epoch: [66][ 30/391]	Time  0.017 ( 0.028)	Data  0.001 ( 0.006)	Loss 7.5378e-02 (7.1848e-02)	Acc@1  97.66 ( 97.73)	Acc@5 100.00 ( 99.97)
Epoch: [66][ 40/391]	Time  0.016 ( 0.026)	Data  0.002 ( 0.005)	Loss 4.5410e-02 (7.4260e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.96)
Epoch: [66][ 50/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.005)	Loss 4.8889e-02 (6.8970e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.95)
Epoch: [66][ 60/391]	Time  0.017 ( 0.025)	Data  0.002 ( 0.004)	Loss 1.4612e-01 (6.8368e-02)	Acc@1  95.31 ( 97.91)	Acc@5 100.00 ( 99.96)
Epoch: [66][ 70/391]	Time  0.038 ( 0.025)	Data  0.001 ( 0.004)	Loss 4.7363e-02 (6.7226e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.96)
Epoch: [66][ 80/391]	Time  0.030 ( 0.025)	Data  0.001 ( 0.004)	Loss 7.3120e-02 (6.7237e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 ( 99.93)
Epoch: [66][ 90/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.9478e-02 (6.8444e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.94)
Epoch: [66][100/391]	Time  0.029 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.0629e-02 (6.7973e-02)	Acc@1  99.22 ( 97.83)	Acc@5 100.00 ( 99.95)
Epoch: [66][110/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.5767e-02 (6.6092e-02)	Acc@1 100.00 ( 97.90)	Acc@5 100.00 ( 99.95)
Epoch: [66][120/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.2603e-02 (6.4703e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [66][130/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.0852e-02 (6.4820e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 ( 99.95)
Epoch: [66][140/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.003)	Loss 7.1228e-02 (6.3729e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.96)
Epoch: [66][150/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.9662e-02 (6.3845e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.96)
Epoch: [66][160/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.0955e-02 (6.4822e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.96)
Epoch: [66][170/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.0461e-01 (6.5902e-02)	Acc@1  97.66 ( 97.84)	Acc@5  99.22 ( 99.95)
Epoch: [66][180/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.8879e-02 (6.5090e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 ( 99.94)
Epoch: [66][190/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.2659e-02 (6.5897e-02)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 ( 99.94)
Epoch: [66][200/391]	Time  0.025 ( 0.024)	Data  0.003 ( 0.003)	Loss 1.6138e-01 (6.5816e-02)	Acc@1  96.88 ( 97.87)	Acc@5 100.00 ( 99.94)
Epoch: [66][210/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.9417e-02 (6.4943e-02)	Acc@1  95.31 ( 97.89)	Acc@5 100.00 ( 99.94)
Epoch: [66][220/391]	Time  0.031 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.8481e-02 (6.5396e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [66][230/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.002)	Loss 3.4058e-02 (6.5409e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [66][240/391]	Time  0.019 ( 0.024)	Data  0.002 ( 0.002)	Loss 3.7292e-02 (6.4787e-02)	Acc@1  99.22 ( 97.91)	Acc@5 100.00 ( 99.95)
Epoch: [66][250/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.2216e-02 (6.4844e-02)	Acc@1  96.88 ( 97.90)	Acc@5 100.00 ( 99.95)
Epoch: [66][260/391]	Time  0.038 ( 0.023)	Data  0.002 ( 0.002)	Loss 9.7778e-02 (6.4606e-02)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 ( 99.95)
Epoch: [66][270/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.4685e-01 (6.4632e-02)	Acc@1  96.09 ( 97.90)	Acc@5 100.00 ( 99.95)
Epoch: [66][280/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.3843e-01 (6.5248e-02)	Acc@1  96.09 ( 97.88)	Acc@5  99.22 ( 99.95)
Epoch: [66][290/391]	Time  0.033 ( 0.023)	Data  0.000 ( 0.002)	Loss 3.1647e-02 (6.5719e-02)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 ( 99.95)
Epoch: [66][300/391]	Time  0.021 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.9785e-02 (6.5245e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.95)
Epoch: [66][310/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.9276e-02 (6.5068e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.95)
Epoch: [66][320/391]	Time  0.016 ( 0.023)	Data  0.000 ( 0.002)	Loss 5.4840e-02 (6.5034e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [66][330/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.2542e-02 (6.5752e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.95)
Epoch: [66][340/391]	Time  0.028 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.3701e-02 (6.5124e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.95)
Epoch: [66][350/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (6.4712e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [66][360/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.2927e-02 (6.4827e-02)	Acc@1  96.88 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [66][370/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.0894e-02 (6.4970e-02)	Acc@1  99.22 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [66][380/391]	Time  0.029 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.1484e-02 (6.4831e-02)	Acc@1 100.00 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [66][390/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.3464e-01 (6.4737e-02)	Acc@1  93.75 ( 97.89)	Acc@5 100.00 ( 99.95)
## e[66] optimizer.zero_grad (sum) time: 0.06102585792541504
## e[66]       loss.backward (sum) time: 1.4779870510101318
## e[66]      optimizer.step (sum) time: 0.47151899337768555
## epoch[66] training(only) time: 9.21999478340149
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.9053e+00 (1.9053e+00)	Acc@1  68.00 ( 68.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 1.9922e+00 (1.9110e+00)	Acc@1  62.00 ( 66.36)	Acc@5  90.00 ( 87.82)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 2.0293e+00 (1.8826e+00)	Acc@1  70.00 ( 67.24)	Acc@5  85.00 ( 88.19)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.1328e+00 (1.8929e+00)	Acc@1  63.00 ( 67.00)	Acc@5  84.00 ( 88.19)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.5137e+00 (1.8620e+00)	Acc@1  67.00 ( 67.17)	Acc@5  93.00 ( 88.76)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.8594e+00 (1.8860e+00)	Acc@1  71.00 ( 67.31)	Acc@5  90.00 ( 88.35)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.6094e+00 (1.8523e+00)	Acc@1  74.00 ( 67.75)	Acc@5  88.00 ( 88.54)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 2.0801e+00 (1.8504e+00)	Acc@1  67.00 ( 67.77)	Acc@5  89.00 ( 88.65)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.7920e+00 (1.8598e+00)	Acc@1  69.00 ( 67.72)	Acc@5  86.00 ( 88.48)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 2.2051e+00 (1.8541e+00)	Acc@1  62.00 ( 67.85)	Acc@5  88.00 ( 88.65)
 * Acc@1 67.890 Acc@5 88.690
### epoch[66] execution time: 10.798731327056885
EPOCH 67
REMOVING: features.module.18.weight
i:   0, name:        features.module.18.bias  changing lr from: 0.001200874588649691   to: 0.001047139821127440
i:   1, name:      features.module.19.weight  changing lr from: 0.001503152079248454   to: 0.001232804282115512
i:   2, name:        features.module.19.bias  changing lr from: 0.001929433694304871   to: 0.001549852336698503
i:   3, name:      features.module.22.weight  changing lr from: 0.002470026314968837   to: 0.001988449253227774
i:   4, name:        features.module.22.bias  changing lr from: 0.003115598246306610   to: 0.002539088704561093
i:   5, name:      features.module.23.weight  changing lr from: 0.003857201811713078   to: 0.003192621680108453
i:   6, name:        features.module.23.bias  changing lr from: 0.004686288955724737   to: 0.003940277663239921
i:   7, name:      features.module.25.weight  changing lr from: 0.005594720861824314   to: 0.004773679142313308
i:   8, name:        features.module.25.bias  changing lr from: 0.006574772485122864   to: 0.005684850415257797
i:   9, name:      features.module.26.weight  changing lr from: 0.007619132800423822   to: 0.006666221545891509
i:  10, name:        features.module.26.bias  changing lr from: 0.008720901474360131   to: 0.007710628235431577
i:  11, name:            classifier.0.weight  changing lr from: 0.009873582586070285   to: 0.008811308285182267
i:  12, name:              classifier.0.bias  changing lr from: 0.011071075944099180   to: 0.009961895246160285
i:  13, name:            classifier.3.weight  changing lr from: 0.012307666477606775   to: 0.011156409778286903
i:  14, name:              classifier.3.bias  changing lr from: 0.013578012117196329   to: 0.012389249175483130
i:  15, name:            classifier.6.weight  changing lr from: 0.014877130524329190   to: 0.013655175453210072
i:  16, name:              classifier.6.bias  changing lr from: 0.016200384977940008   to: 0.014949302341313871



# Switched to train mode...
Epoch: [67][  0/391]	Time  0.194 ( 0.194)	Data  0.168 ( 0.168)	Loss 8.3130e-02 (8.3130e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.017 ( 0.037)	Data  0.001 ( 0.017)	Loss 7.7820e-02 (5.0648e-02)	Acc@1  96.88 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.016 ( 0.031)	Data  0.001 ( 0.010)	Loss 7.4890e-02 (6.3495e-02)	Acc@1  99.22 ( 98.14)	Acc@5  99.22 ( 99.89)
Epoch: [67][ 30/391]	Time  0.035 ( 0.028)	Data  0.002 ( 0.007)	Loss 4.7272e-02 (6.0938e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.92)
Epoch: [67][ 40/391]	Time  0.039 ( 0.027)	Data  0.001 ( 0.006)	Loss 4.7821e-02 (6.3548e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.94)
Epoch: [67][ 50/391]	Time  0.016 ( 0.026)	Data  0.001 ( 0.005)	Loss 9.3140e-02 (6.6579e-02)	Acc@1  96.09 ( 97.90)	Acc@5 100.00 ( 99.95)
Epoch: [67][ 60/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.005)	Loss 8.9844e-02 (7.3052e-02)	Acc@1  96.09 ( 97.68)	Acc@5 100.00 ( 99.96)
Epoch: [67][ 70/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.004)	Loss 8.0444e-02 (7.3455e-02)	Acc@1  97.66 ( 97.69)	Acc@5 100.00 ( 99.97)
Epoch: [67][ 80/391]	Time  0.017 ( 0.024)	Data  0.000 ( 0.004)	Loss 7.7637e-02 (7.4335e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.97)
Epoch: [67][ 90/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.004)	Loss 2.4796e-02 (7.1079e-02)	Acc@1  99.22 ( 97.76)	Acc@5 100.00 ( 99.97)
Epoch: [67][100/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.1230e-01 (7.0589e-02)	Acc@1  96.09 ( 97.77)	Acc@5 100.00 ( 99.96)
Epoch: [67][110/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.6642e-02 (7.0068e-02)	Acc@1  99.22 ( 97.83)	Acc@5 100.00 ( 99.95)
Epoch: [67][120/391]	Time  0.026 ( 0.024)	Data  0.004 ( 0.003)	Loss 3.9337e-02 (7.0727e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.95)
Epoch: [67][130/391]	Time  0.016 ( 0.024)	Data  0.002 ( 0.003)	Loss 3.1738e-02 (6.8546e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.96)
Epoch: [67][140/391]	Time  0.025 ( 0.024)	Data  0.003 ( 0.003)	Loss 5.0354e-02 (6.8993e-02)	Acc@1  99.22 ( 97.84)	Acc@5  99.22 ( 99.96)
Epoch: [67][150/391]	Time  0.025 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.6917e-02 (6.8427e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.96)
Epoch: [67][160/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.2815e-02 (6.7775e-02)	Acc@1  98.44 ( 97.85)	Acc@5  99.22 ( 99.95)
Epoch: [67][170/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.0322e-02 (6.7217e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [67][180/391]	Time  0.028 ( 0.024)	Data  0.005 ( 0.003)	Loss 1.1230e-01 (6.6702e-02)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 ( 99.95)
Epoch: [67][190/391]	Time  0.032 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.4551e-01 (6.7319e-02)	Acc@1  96.09 ( 97.90)	Acc@5  99.22 ( 99.94)
Epoch: [67][200/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.9082e-02 (6.6832e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.95)
Epoch: [67][210/391]	Time  0.024 ( 0.024)	Data  0.005 ( 0.003)	Loss 2.9373e-02 (6.7245e-02)	Acc@1  99.22 ( 97.91)	Acc@5 100.00 ( 99.94)
Epoch: [67][220/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.1788e-02 (6.6859e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 ( 99.94)
Epoch: [67][230/391]	Time  0.030 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.6488e-02 (6.6333e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.94)
Epoch: [67][240/391]	Time  0.016 ( 0.023)	Data  0.000 ( 0.003)	Loss 3.7781e-02 (6.6281e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.94)
Epoch: [67][250/391]	Time  0.024 ( 0.023)	Data  0.001 ( 0.003)	Loss 9.3384e-02 (6.6356e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 ( 99.94)
Epoch: [67][260/391]	Time  0.019 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.9540e-02 (6.6832e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.94)
Epoch: [67][270/391]	Time  0.040 ( 0.023)	Data  0.002 ( 0.003)	Loss 3.9856e-02 (6.6562e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.95)
Epoch: [67][280/391]	Time  0.029 ( 0.023)	Data  0.005 ( 0.002)	Loss 6.5430e-02 (6.6993e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.94)
Epoch: [67][290/391]	Time  0.027 ( 0.023)	Data  0.003 ( 0.002)	Loss 4.2114e-02 (6.7037e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.94)
Epoch: [67][300/391]	Time  0.037 ( 0.023)	Data  0.002 ( 0.002)	Loss 5.9326e-02 (6.6520e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.94)
Epoch: [67][310/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 7.8857e-02 (6.6380e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 ( 99.94)
Epoch: [67][320/391]	Time  0.052 ( 0.023)	Data  0.013 ( 0.002)	Loss 4.4617e-02 (6.6227e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [67][330/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.4973e-02 (6.6135e-02)	Acc@1  99.22 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [67][340/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.4780e-02 (6.5955e-02)	Acc@1 100.00 ( 97.94)	Acc@5 100.00 ( 99.95)
Epoch: [67][350/391]	Time  0.042 ( 0.023)	Data  0.007 ( 0.002)	Loss 2.4826e-02 (6.5707e-02)	Acc@1  99.22 ( 97.95)	Acc@5 100.00 ( 99.95)
Epoch: [67][360/391]	Time  0.020 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.9791e-02 (6.5613e-02)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 ( 99.95)
Epoch: [67][370/391]	Time  0.027 ( 0.023)	Data  0.001 ( 0.002)	Loss 7.8735e-02 (6.5483e-02)	Acc@1  96.88 ( 97.94)	Acc@5 100.00 ( 99.95)
Epoch: [67][380/391]	Time  0.027 ( 0.023)	Data  0.002 ( 0.002)	Loss 6.7261e-02 (6.5493e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [67][390/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.7129e-02 (6.5761e-02)	Acc@1  98.75 ( 97.93)	Acc@5 100.00 ( 99.95)
## e[67] optimizer.zero_grad (sum) time: 0.056899070739746094
## e[67]       loss.backward (sum) time: 1.4353573322296143
## e[67]      optimizer.step (sum) time: 0.4387810230255127
## epoch[67] training(only) time: 9.14060640335083
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.9121e+00 (1.9121e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 2.0020e+00 (1.9328e+00)	Acc@1  64.00 ( 66.36)	Acc@5  91.00 ( 88.45)
Test: [ 20/100]	Time  0.012 ( 0.019)	Loss 2.0215e+00 (1.9012e+00)	Acc@1  69.00 ( 67.24)	Acc@5  85.00 ( 88.62)
Test: [ 30/100]	Time  0.019 ( 0.017)	Loss 2.0918e+00 (1.9038e+00)	Acc@1  63.00 ( 66.74)	Acc@5  86.00 ( 88.26)
Test: [ 40/100]	Time  0.010 ( 0.016)	Loss 1.4697e+00 (1.8629e+00)	Acc@1  70.00 ( 67.05)	Acc@5  94.00 ( 88.73)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.8721e+00 (1.8883e+00)	Acc@1  71.00 ( 67.20)	Acc@5  90.00 ( 88.33)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 1.6777e+00 (1.8558e+00)	Acc@1  70.00 ( 67.61)	Acc@5  89.00 ( 88.64)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 1.9834e+00 (1.8553e+00)	Acc@1  67.00 ( 67.65)	Acc@5  89.00 ( 88.76)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8623e+00 (1.8618e+00)	Acc@1  66.00 ( 67.54)	Acc@5  86.00 ( 88.62)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 2.1504e+00 (1.8551e+00)	Acc@1  63.00 ( 67.74)	Acc@5  88.00 ( 88.78)
 * Acc@1 67.850 Acc@5 88.750
### epoch[67] execution time: 10.714114665985107
EPOCH 68
REMOVING: features.module.18.bias
i:   0, name:      features.module.19.weight  changing lr from: 0.001232804282115512   to: 0.001065236636935051
i:   1, name:        features.module.19.bias  changing lr from: 0.001549852336698503   to: 0.001268945131090359
i:   2, name:      features.module.22.weight  changing lr from: 0.001988449253227774   to: 0.001601384629652418
i:   3, name:        features.module.22.bias  changing lr from: 0.002539088704561093   to: 0.002052906839808005
i:   4, name:      features.module.23.weight  changing lr from: 0.003192621680108453   to: 0.002614189711271224
i:   5, name:        features.module.23.bias  changing lr from: 0.003940277663239921   to: 0.003276264592862731
i:   6, name:      features.module.25.weight  changing lr from: 0.004773679142313308   to: 0.004030536019425728
i:   7, name:        features.module.25.bias  changing lr from: 0.005684850415257797   to: 0.004868795147555476
i:   8, name:      features.module.26.weight  changing lr from: 0.006666221545891509   to: 0.005783227755232595
i:   9, name:        features.module.26.bias  changing lr from: 0.007710628235431577   to: 0.006766417623414239
i:  10, name:            classifier.0.weight  changing lr from: 0.008811308285182267   to: 0.007811346027370069
i:  11, name:              classifier.0.bias  changing lr from: 0.009961895246160285   to: 0.008911387982222740
i:  12, name:            classifier.3.weight  changing lr from: 0.011156409778286903   to: 0.010060305810749119
i:  13, name:              classifier.3.bias  changing lr from: 0.012389249175483130   to: 0.011252240531862405
i:  14, name:            classifier.6.weight  changing lr from: 0.013655175453210072   to: 0.012481701505070414
i:  15, name:              classifier.6.bias  changing lr from: 0.014949302341313871   to: 0.013743554709264590



# Switched to train mode...
Epoch: [68][  0/391]	Time  0.181 ( 0.181)	Data  0.157 ( 0.157)	Loss 1.4946e-02 (1.4946e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.017 ( 0.035)	Data  0.001 ( 0.016)	Loss 4.1138e-02 (5.9653e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.93)
Epoch: [68][ 20/391]	Time  0.025 ( 0.030)	Data  0.002 ( 0.010)	Loss 2.4826e-02 (5.7227e-02)	Acc@1 100.00 ( 98.36)	Acc@5 100.00 ( 99.96)
Epoch: [68][ 30/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.007)	Loss 1.2024e-01 (5.7141e-02)	Acc@1  95.31 ( 98.34)	Acc@5 100.00 ( 99.95)
Epoch: [68][ 40/391]	Time  0.042 ( 0.026)	Data  0.007 ( 0.006)	Loss 3.7048e-02 (5.6481e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.96)
Epoch: [68][ 50/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.005)	Loss 6.5613e-02 (5.9079e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [68][ 60/391]	Time  0.016 ( 0.025)	Data  0.000 ( 0.005)	Loss 7.4829e-02 (5.9686e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [68][ 70/391]	Time  0.016 ( 0.025)	Data  0.000 ( 0.004)	Loss 7.8247e-02 (6.1018e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [68][ 80/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.004)	Loss 4.9316e-02 (6.2158e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.96)
Epoch: [68][ 90/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.004)	Loss 7.9956e-02 (6.1836e-02)	Acc@1  96.88 ( 98.05)	Acc@5 100.00 ( 99.96)
Epoch: [68][100/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.004)	Loss 5.4810e-02 (6.1660e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 ( 99.95)
Epoch: [68][110/391]	Time  0.027 ( 0.024)	Data  0.000 ( 0.003)	Loss 4.5013e-02 (6.0789e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.96)
Epoch: [68][120/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.3544e-02 (6.1430e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.95)
Epoch: [68][130/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.1909e-02 (6.1504e-02)	Acc@1  96.09 ( 98.04)	Acc@5 100.00 ( 99.95)
Epoch: [68][140/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.1279e-02 (6.2197e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 ( 99.95)
Epoch: [68][150/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.003)	Loss 9.4421e-02 (6.2362e-02)	Acc@1  98.44 ( 98.07)	Acc@5  98.44 ( 99.94)
Epoch: [68][160/391]	Time  0.016 ( 0.024)	Data  0.002 ( 0.003)	Loss 5.3314e-02 (6.2106e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [68][170/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.0852e-02 (6.2354e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 ( 99.94)
Epoch: [68][180/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.8350e-02 (6.1359e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.94)
Epoch: [68][190/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.9816e-02 (6.1586e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.94)
Epoch: [68][200/391]	Time  0.032 ( 0.024)	Data  0.000 ( 0.003)	Loss 4.8889e-02 (6.1407e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.94)
Epoch: [68][210/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.5350e-02 (6.1325e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 ( 99.94)
Epoch: [68][220/391]	Time  0.016 ( 0.023)	Data  0.002 ( 0.003)	Loss 1.7349e-02 (6.0986e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 ( 99.94)
Epoch: [68][230/391]	Time  0.040 ( 0.023)	Data  0.017 ( 0.003)	Loss 4.2908e-02 (6.1501e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.94)
Epoch: [68][240/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 7.2327e-02 (6.1457e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 ( 99.94)
Epoch: [68][250/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.1758e-02 (6.0817e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.94)
Epoch: [68][260/391]	Time  0.037 ( 0.023)	Data  0.002 ( 0.003)	Loss 3.4729e-02 (6.0877e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.94)
Epoch: [68][270/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.2023e-02 (6.1002e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [68][280/391]	Time  0.036 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.3112e-02 (6.1303e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [68][290/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.1219e-02 (6.1147e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [68][300/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.9835e-02 (6.1464e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [68][310/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.9570e-02 (6.1865e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 ( 99.94)
Epoch: [68][320/391]	Time  0.027 ( 0.023)	Data  0.002 ( 0.002)	Loss 4.3304e-02 (6.1402e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [68][330/391]	Time  0.020 ( 0.023)	Data  0.002 ( 0.002)	Loss 4.8187e-02 (6.1154e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.94)
Epoch: [68][340/391]	Time  0.016 ( 0.023)	Data  0.002 ( 0.002)	Loss 8.0872e-02 (6.1216e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.94)
Epoch: [68][350/391]	Time  0.021 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.2581e-02 (6.1061e-02)	Acc@1 100.00 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [68][360/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.6312e-02 (6.1360e-02)	Acc@1 100.00 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [68][370/391]	Time  0.028 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.0016e-01 (6.1734e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [68][380/391]	Time  0.018 ( 0.023)	Data  0.000 ( 0.002)	Loss 3.1342e-02 (6.1746e-02)	Acc@1 100.00 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [68][390/391]	Time  0.016 ( 0.023)	Data  0.000 ( 0.002)	Loss 5.0079e-02 (6.1575e-02)	Acc@1  98.75 ( 98.10)	Acc@5 100.00 ( 99.94)
## e[68] optimizer.zero_grad (sum) time: 0.05494046211242676
## e[68]       loss.backward (sum) time: 1.4247665405273438
## e[68]      optimizer.step (sum) time: 0.42453527450561523
## epoch[68] training(only) time: 9.085553169250488
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.8809e+00 (1.8809e+00)	Acc@1  67.00 ( 67.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.016 ( 0.025)	Loss 2.0000e+00 (1.9005e+00)	Acc@1  63.00 ( 66.55)	Acc@5  89.00 ( 88.36)
Test: [ 20/100]	Time  0.011 ( 0.019)	Loss 1.9561e+00 (1.8770e+00)	Acc@1  69.00 ( 66.95)	Acc@5  87.00 ( 88.57)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 2.0762e+00 (1.8880e+00)	Acc@1  62.00 ( 66.45)	Acc@5  86.00 ( 88.19)
Test: [ 40/100]	Time  0.017 ( 0.017)	Loss 1.4092e+00 (1.8512e+00)	Acc@1  71.00 ( 66.88)	Acc@5  93.00 ( 88.71)
Test: [ 50/100]	Time  0.018 ( 0.016)	Loss 1.8369e+00 (1.8769e+00)	Acc@1  71.00 ( 67.04)	Acc@5  92.00 ( 88.43)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.6729e+00 (1.8451e+00)	Acc@1  74.00 ( 67.57)	Acc@5  89.00 ( 88.64)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 2.0215e+00 (1.8433e+00)	Acc@1  66.00 ( 67.62)	Acc@5  89.00 ( 88.75)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8447e+00 (1.8519e+00)	Acc@1  69.00 ( 67.42)	Acc@5  85.00 ( 88.57)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.2285e+00 (1.8454e+00)	Acc@1  63.00 ( 67.60)	Acc@5  87.00 ( 88.69)
 * Acc@1 67.650 Acc@5 88.720
### epoch[68] execution time: 10.643882274627686
EPOCH 69
i:   0, name:      features.module.19.weight  changing lr from: 0.001065236636935051   to: 0.001000795207570747
i:   1, name:        features.module.19.bias  changing lr from: 0.001268945131090359   to: 0.001087272607258682
i:   2, name:      features.module.22.weight  changing lr from: 0.001601384629652418   to: 0.001309578848319869
i:   3, name:        features.module.22.bias  changing lr from: 0.002052906839808005   to: 0.001657958860314290
i:   4, name:      features.module.23.weight  changing lr from: 0.002614189711271224   to: 0.002122948230740928
i:   5, name:        features.module.23.bias  changing lr from: 0.003276264592862731   to: 0.002695406740183536
i:   6, name:      features.module.25.weight  changing lr from: 0.004030536019425728   to: 0.003366543805811400
i:   7, name:        features.module.25.bias  changing lr from: 0.004868795147555476   to: 0.004127936908078621
i:   8, name:      features.module.26.weight  changing lr from: 0.005783227755232595   to: 0.004971543971302938
i:   9, name:        features.module.26.bias  changing lr from: 0.006766417623414239   to: 0.005889710570131484
i:  10, name:            classifier.0.weight  changing lr from: 0.007811346027370069   to: 0.006875172741373409
i:  11, name:              classifier.0.bias  changing lr from: 0.008911387982222740   to: 0.007921056094663963
i:  12, name:            classifier.3.weight  changing lr from: 0.010060305810749119   to: 0.009020871836058268
i:  13, name:              classifier.3.bias  changing lr from: 0.011252240531862405   to: 0.010168510245903557
i:  14, name:            classifier.6.weight  changing lr from: 0.012481701505070414   to: 0.011358232086044369
i:  15, name:              classifier.6.bias  changing lr from: 0.013743554709264590   to: 0.012584658351322848



# Switched to train mode...
Epoch: [69][  0/391]	Time  0.172 ( 0.172)	Data  0.143 ( 0.143)	Loss 5.0201e-02 (5.0201e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.016 ( 0.035)	Data  0.001 ( 0.015)	Loss 7.3486e-02 (5.2668e-02)	Acc@1  97.66 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.016 ( 0.029)	Data  0.001 ( 0.009)	Loss 8.8623e-02 (6.3845e-02)	Acc@1  96.09 ( 97.95)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.006)	Loss 5.6763e-02 (6.5369e-02)	Acc@1  98.44 ( 98.01)	Acc@5  99.22 ( 99.95)
Epoch: [69][ 40/391]	Time  0.016 ( 0.026)	Data  0.001 ( 0.005)	Loss 8.3862e-02 (6.5829e-02)	Acc@1  97.66 ( 97.98)	Acc@5 100.00 ( 99.94)
Epoch: [69][ 50/391]	Time  0.016 ( 0.025)	Data  0.000 ( 0.005)	Loss 9.7839e-02 (6.5998e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.95)
Epoch: [69][ 60/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.004)	Loss 5.3558e-02 (6.6866e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [69][ 70/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.004)	Loss 7.6599e-02 (6.5733e-02)	Acc@1  96.09 ( 97.92)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 80/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.004)	Loss 5.6763e-02 (6.5135e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 90/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 4.4830e-02 (6.4006e-02)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 ( 99.96)
Epoch: [69][100/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.3721e-02 (6.2082e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 ( 99.95)
Epoch: [69][110/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.4119e-02 (6.1952e-02)	Acc@1  98.44 ( 98.03)	Acc@5 100.00 ( 99.95)
Epoch: [69][120/391]	Time  0.016 ( 0.024)	Data  0.002 ( 0.003)	Loss 6.5796e-02 (6.1108e-02)	Acc@1  97.66 ( 98.04)	Acc@5 100.00 ( 99.95)
Epoch: [69][130/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.7578e-02 (6.1106e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 ( 99.95)
Epoch: [69][140/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.6779e-02 (5.9963e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 ( 99.96)
Epoch: [69][150/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.4241e-02 (6.1160e-02)	Acc@1 100.00 ( 98.06)	Acc@5 100.00 ( 99.95)
Epoch: [69][160/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.4992e-02 (6.0220e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [69][170/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.3965e-02 (5.9624e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [69][180/391]	Time  0.019 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.2128e-01 (6.0738e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.96)
Epoch: [69][190/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.9622e-02 (6.0097e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [69][200/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.1992e-02 (6.0348e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [69][210/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.8946e-02 (6.0130e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [69][220/391]	Time  0.019 ( 0.023)	Data  0.002 ( 0.003)	Loss 7.3914e-02 (5.9210e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [69][230/391]	Time  0.025 ( 0.023)	Data  0.004 ( 0.003)	Loss 4.2053e-02 (5.8414e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [69][240/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.6337e-02 (5.8953e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [69][250/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.7708e-02 (5.8924e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [69][260/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.003)	Loss 7.8857e-02 (5.9468e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [69][270/391]	Time  0.022 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.5369e-02 (5.9210e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [69][280/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.2948e-02 (5.9125e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [69][290/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 8.4381e-03 (5.8898e-02)	Acc@1 100.00 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [69][300/391]	Time  0.030 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.1575e-02 (5.9022e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [69][310/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.5381e-01 (5.9333e-02)	Acc@1  96.09 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [69][320/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.6713e-02 (5.9429e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [69][330/391]	Time  0.040 ( 0.023)	Data  0.001 ( 0.002)	Loss 7.3975e-02 (5.9614e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [69][340/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 9.6863e-02 (5.9667e-02)	Acc@1  97.66 ( 98.14)	Acc@5  99.22 ( 99.97)
Epoch: [69][350/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.002)	Loss 9.1125e-02 (6.0062e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [69][360/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.1025e-02 (5.9850e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [69][370/391]	Time  0.021 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.8427e-02 (6.0529e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [69][380/391]	Time  0.016 ( 0.023)	Data  0.000 ( 0.002)	Loss 2.6260e-02 (6.0274e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [69][390/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.002)	Loss 7.3486e-02 (6.0430e-02)	Acc@1  98.75 ( 98.12)	Acc@5 100.00 ( 99.96)
## e[69] optimizer.zero_grad (sum) time: 0.05531787872314453
## e[69]       loss.backward (sum) time: 1.4067819118499756
## e[69]      optimizer.step (sum) time: 0.42262911796569824
## epoch[69] training(only) time: 9.065484285354614
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.9492e+00 (1.9492e+00)	Acc@1  66.00 ( 66.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 2.0527e+00 (1.9314e+00)	Acc@1  64.00 ( 66.00)	Acc@5  89.00 ( 88.45)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.9736e+00 (1.8937e+00)	Acc@1  69.00 ( 66.71)	Acc@5  84.00 ( 88.48)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 2.0508e+00 (1.9070e+00)	Acc@1  63.00 ( 66.58)	Acc@5  85.00 ( 87.94)
Test: [ 40/100]	Time  0.010 ( 0.016)	Loss 1.4482e+00 (1.8692e+00)	Acc@1  69.00 ( 67.05)	Acc@5  94.00 ( 88.54)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.8779e+00 (1.8955e+00)	Acc@1  70.00 ( 67.14)	Acc@5  89.00 ( 88.22)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.6504e+00 (1.8596e+00)	Acc@1  72.00 ( 67.57)	Acc@5  90.00 ( 88.48)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 2.0723e+00 (1.8560e+00)	Acc@1  67.00 ( 67.65)	Acc@5  89.00 ( 88.66)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.8223e+00 (1.8624e+00)	Acc@1  67.00 ( 67.57)	Acc@5  86.00 ( 88.56)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.1348e+00 (1.8549e+00)	Acc@1  62.00 ( 67.76)	Acc@5  87.00 ( 88.70)
 * Acc@1 67.870 Acc@5 88.790
### epoch[69] execution time: 10.623578786849976
EPOCH 70
REMOVING: features.module.19.weight
i:   0, name:        features.module.19.bias  changing lr from: 0.001087272607258682   to: 0.001005197279434010
i:   1, name:      features.module.22.weight  changing lr from: 0.001309578848319869   to: 0.001113594618947410
i:   2, name:        features.module.22.bias  changing lr from: 0.001657958860314290   to: 0.001354980920901776
i:   3, name:      features.module.23.weight  changing lr from: 0.002122948230740928   to: 0.001719782448133949
i:   4, name:        features.module.23.bias  changing lr from: 0.002695406740183536   to: 0.002198716208734851
i:   5, name:      features.module.25.weight  changing lr from: 0.003366543805811400   to: 0.002782821432431059
i:   6, name:        features.module.25.bias  changing lr from: 0.004127936908078621   to: 0.003463483346575868
i:   7, name:      features.module.26.weight  changing lr from: 0.004971543971302938   to: 0.004232450276099130
i:   8, name:        features.module.26.bias  changing lr from: 0.005889710570131484   to: 0.005081844992226824
i:   9, name:            classifier.0.weight  changing lr from: 0.006875172741373409   to: 0.006004171140610292
i:  10, name:              classifier.0.bias  changing lr from: 0.007921056094663963   to: 0.006992315491284573
i:  11, name:            classifier.3.weight  changing lr from: 0.009020871836058268   to: 0.008039546670919689
i:  12, name:              classifier.3.bias  changing lr from: 0.010168510245903557   to: 0.009139510962247608
i:  13, name:            classifier.6.weight  changing lr from: 0.011358232086044369   to: 0.010286225686290267
i:  14, name:              classifier.6.bias  changing lr from: 0.012584658351322848   to: 0.011474070619914588



# Switched to train mode...
Epoch: [70][  0/391]	Time  0.166 ( 0.166)	Data  0.136 ( 0.136)	Loss 9.2957e-02 (9.2957e-02)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.017 ( 0.035)	Data  0.001 ( 0.014)	Loss 7.8735e-02 (6.3740e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.016 ( 0.029)	Data  0.001 ( 0.008)	Loss 7.3975e-02 (7.0308e-02)	Acc@1  97.66 ( 97.77)	Acc@5 100.00 ( 99.96)
Epoch: [70][ 30/391]	Time  0.016 ( 0.026)	Data  0.001 ( 0.007)	Loss 1.9379e-02 (6.6134e-02)	Acc@1 100.00 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [70][ 40/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.005)	Loss 6.3171e-02 (6.2978e-02)	Acc@1  96.88 ( 98.02)	Acc@5 100.00 ( 99.96)
Epoch: [70][ 50/391]	Time  0.016 ( 0.025)	Data  0.000 ( 0.005)	Loss 7.8552e-02 (6.4108e-02)	Acc@1  96.09 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [70][ 60/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.004)	Loss 9.5154e-02 (6.4087e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [70][ 70/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.004)	Loss 5.0842e-02 (6.6864e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.97)
Epoch: [70][ 80/391]	Time  0.026 ( 0.024)	Data  0.001 ( 0.004)	Loss 6.0944e-02 (6.4873e-02)	Acc@1  96.88 ( 97.96)	Acc@5 100.00 ( 99.96)
Epoch: [70][ 90/391]	Time  0.028 ( 0.024)	Data  0.004 ( 0.003)	Loss 7.4158e-02 (6.3651e-02)	Acc@1  97.66 ( 97.99)	Acc@5 100.00 ( 99.97)
Epoch: [70][100/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.1586e-02 (6.3357e-02)	Acc@1 100.00 ( 98.02)	Acc@5 100.00 ( 99.97)
Epoch: [70][110/391]	Time  0.050 ( 0.024)	Data  0.013 ( 0.003)	Loss 2.4506e-02 (6.2985e-02)	Acc@1 100.00 ( 98.04)	Acc@5 100.00 ( 99.97)
Epoch: [70][120/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.0510e-01 (6.3449e-02)	Acc@1  96.09 ( 98.02)	Acc@5 100.00 ( 99.97)
Epoch: [70][130/391]	Time  0.044 ( 0.024)	Data  0.013 ( 0.003)	Loss 6.3171e-02 (6.3951e-02)	Acc@1  96.88 ( 98.02)	Acc@5 100.00 ( 99.96)
Epoch: [70][140/391]	Time  0.016 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.3966e-02 (6.3217e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 ( 99.96)
Epoch: [70][150/391]	Time  0.035 ( 0.024)	Data  0.003 ( 0.003)	Loss 6.1676e-02 (6.4086e-02)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 ( 99.95)
Epoch: [70][160/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.003)	Loss 3.3447e-02 (6.4395e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.96)
Epoch: [70][170/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.1102e-01 (6.5139e-02)	Acc@1  95.31 ( 97.97)	Acc@5 100.00 ( 99.95)
Epoch: [70][180/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.4067e-02 (6.5134e-02)	Acc@1  98.44 ( 97.98)	Acc@5 100.00 ( 99.94)
Epoch: [70][190/391]	Time  0.027 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.0293e-02 (6.5901e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 ( 99.95)
Epoch: [70][200/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.1743e-01 (6.5326e-02)	Acc@1  96.88 ( 97.96)	Acc@5  99.22 ( 99.95)
Epoch: [70][210/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.9885e-02 (6.5759e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.94)
Epoch: [70][220/391]	Time  0.030 ( 0.023)	Data  0.001 ( 0.003)	Loss 7.7148e-02 (6.5298e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.94)
Epoch: [70][230/391]	Time  0.025 ( 0.023)	Data  0.000 ( 0.003)	Loss 3.9917e-02 (6.5063e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.94)
Epoch: [70][240/391]	Time  0.030 ( 0.023)	Data  0.001 ( 0.003)	Loss 7.2937e-02 (6.5169e-02)	Acc@1  96.88 ( 97.96)	Acc@5 100.00 ( 99.94)
Epoch: [70][250/391]	Time  0.018 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.5776e-02 (6.5330e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.94)
Epoch: [70][260/391]	Time  0.023 ( 0.023)	Data  0.000 ( 0.002)	Loss 5.1514e-02 (6.5058e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.94)
Epoch: [70][270/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.4673e-02 (6.4379e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.95)
Epoch: [70][280/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.3497e-02 (6.4492e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.94)
Epoch: [70][290/391]	Time  0.039 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.6345e-02 (6.3883e-02)	Acc@1  96.09 ( 97.96)	Acc@5 100.00 ( 99.95)
Epoch: [70][300/391]	Time  0.024 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.2694e-02 (6.3542e-02)	Acc@1  98.44 ( 97.97)	Acc@5 100.00 ( 99.95)
Epoch: [70][310/391]	Time  0.016 ( 0.023)	Data  0.002 ( 0.002)	Loss 6.5369e-02 (6.3181e-02)	Acc@1  97.66 ( 97.98)	Acc@5 100.00 ( 99.95)
Epoch: [70][320/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 7.5989e-02 (6.3067e-02)	Acc@1  98.44 ( 97.99)	Acc@5  99.22 ( 99.95)
Epoch: [70][330/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.002)	Loss 5.5054e-02 (6.2922e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 ( 99.95)
Epoch: [70][340/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.2521e-02 (6.2826e-02)	Acc@1  98.44 ( 98.00)	Acc@5 100.00 ( 99.95)
Epoch: [70][350/391]	Time  0.030 ( 0.023)	Data  0.000 ( 0.002)	Loss 4.0070e-02 (6.3179e-02)	Acc@1  99.22 ( 97.98)	Acc@5 100.00 ( 99.95)
Epoch: [70][360/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (6.2929e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 ( 99.95)
Epoch: [70][370/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.2463e-01 (6.3460e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 ( 99.95)
Epoch: [70][380/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.0217e-01 (6.3239e-02)	Acc@1  95.31 ( 97.99)	Acc@5 100.00 ( 99.95)
Epoch: [70][390/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.002)	Loss 7.4768e-02 (6.3273e-02)	Acc@1  96.25 ( 97.98)	Acc@5 100.00 ( 99.95)
## e[70] optimizer.zero_grad (sum) time: 0.052301645278930664
## e[70]       loss.backward (sum) time: 1.3950507640838623
## e[70]      optimizer.step (sum) time: 0.39040064811706543
## epoch[70] training(only) time: 9.139978408813477
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.9062e+00 (1.9062e+00)	Acc@1  66.00 ( 66.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.012 ( 0.025)	Loss 2.0332e+00 (1.9092e+00)	Acc@1  64.00 ( 65.73)	Acc@5  89.00 ( 88.73)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.9600e+00 (1.8835e+00)	Acc@1  71.00 ( 66.86)	Acc@5  86.00 ( 88.62)
Test: [ 30/100]	Time  0.025 ( 0.018)	Loss 2.0586e+00 (1.8931e+00)	Acc@1  62.00 ( 66.84)	Acc@5  85.00 ( 88.29)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.4355e+00 (1.8593e+00)	Acc@1  70.00 ( 67.22)	Acc@5  94.00 ( 88.78)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.8486e+00 (1.8825e+00)	Acc@1  71.00 ( 67.41)	Acc@5  90.00 ( 88.49)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.7002e+00 (1.8496e+00)	Acc@1  70.00 ( 67.69)	Acc@5  88.00 ( 88.70)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 1.9893e+00 (1.8477e+00)	Acc@1  69.00 ( 67.72)	Acc@5  90.00 ( 88.82)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.7842e+00 (1.8560e+00)	Acc@1  68.00 ( 67.49)	Acc@5  85.00 ( 88.64)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.2051e+00 (1.8510e+00)	Acc@1  61.00 ( 67.60)	Acc@5  88.00 ( 88.79)
 * Acc@1 67.720 Acc@5 88.840
### epoch[70] execution time: 10.70175576210022
EPOCH 71
REMOVING: features.module.19.bias
i:   0, name:      features.module.22.weight  changing lr from: 0.001113594618947410   to: 0.001013809871779596
i:   1, name:        features.module.22.bias  changing lr from: 0.001354980920901776   to: 0.001144537750807189
i:   2, name:      features.module.23.weight  changing lr from: 0.001719782448133949   to: 0.001405418862009349
i:   3, name:        features.module.23.bias  changing lr from: 0.002198716208734851   to: 0.001787058446472905
i:   4, name:      features.module.25.weight  changing lr from: 0.002782821432431059   to: 0.002280352562982362
i:   5, name:        features.module.25.bias  changing lr from: 0.003463483346575868   to: 0.002876517567929881
i:   6, name:      features.module.26.weight  changing lr from: 0.004232450276099130   to: 0.003567112271979777
i:   7, name:        features.module.26.bias  changing lr from: 0.005081844992226824   to: 0.004344053749409027
i:   8, name:            classifier.0.weight  changing lr from: 0.006004171140610292   to: 0.005199627680930862
i:   9, name:              classifier.0.bias  changing lr from: 0.006992315491284573   to: 0.006126494020941730
i:  10, name:            classifier.3.weight  changing lr from: 0.008039546670919689   to: 0.007117688696024515
i:  11, name:              classifier.3.bias  changing lr from: 0.009139510962247608   to: 0.008166621963465497
i:  12, name:            classifier.6.weight  changing lr from: 0.010286225686290267   to: 0.009267073986576002
i:  13, name:              classifier.6.bias  changing lr from: 0.011474070619914588   to: 0.010413188117689882



# Switched to train mode...
Epoch: [71][  0/391]	Time  0.178 ( 0.178)	Data  0.152 ( 0.152)	Loss 2.8473e-02 (2.8473e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.016 ( 0.036)	Data  0.001 ( 0.016)	Loss 3.7384e-02 (5.2909e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.016 ( 0.029)	Data  0.001 ( 0.009)	Loss 1.0828e-01 (6.8054e-02)	Acc@1  96.88 ( 98.03)	Acc@5 100.00 ( 99.96)
Epoch: [71][ 30/391]	Time  0.031 ( 0.027)	Data  0.005 ( 0.007)	Loss 8.4351e-02 (6.3737e-02)	Acc@1  96.88 ( 98.03)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 40/391]	Time  0.016 ( 0.026)	Data  0.001 ( 0.005)	Loss 4.0009e-02 (6.0028e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [71][ 50/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.005)	Loss 5.7617e-02 (6.0495e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 60/391]	Time  0.026 ( 0.025)	Data  0.003 ( 0.004)	Loss 8.0200e-02 (6.2996e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [71][ 70/391]	Time  0.026 ( 0.025)	Data  0.001 ( 0.004)	Loss 1.4819e-01 (6.1943e-02)	Acc@1  93.75 ( 97.92)	Acc@5 100.00 ( 99.96)
Epoch: [71][ 80/391]	Time  0.054 ( 0.024)	Data  0.020 ( 0.004)	Loss 2.1103e-02 (6.1166e-02)	Acc@1 100.00 ( 97.97)	Acc@5 100.00 ( 99.96)
Epoch: [71][ 90/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.004)	Loss 8.7219e-02 (6.0863e-02)	Acc@1  96.88 ( 97.98)	Acc@5  99.22 ( 99.96)
Epoch: [71][100/391]	Time  0.033 ( 0.024)	Data  0.001 ( 0.003)	Loss 2.8671e-02 (6.1262e-02)	Acc@1 100.00 ( 97.98)	Acc@5 100.00 ( 99.96)
Epoch: [71][110/391]	Time  0.030 ( 0.024)	Data  0.004 ( 0.003)	Loss 8.5144e-02 (6.2070e-02)	Acc@1  96.88 ( 97.98)	Acc@5 100.00 ( 99.96)
Epoch: [71][120/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.003)	Loss 7.9285e-02 (6.3207e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.95)
Epoch: [71][130/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.003)	Loss 5.9753e-02 (6.2192e-02)	Acc@1  97.66 ( 97.96)	Acc@5 100.00 ( 99.96)
Epoch: [71][140/391]	Time  0.016 ( 0.024)	Data  0.000 ( 0.003)	Loss 7.7454e-02 (6.2913e-02)	Acc@1  96.88 ( 97.89)	Acc@5 100.00 ( 99.96)
Epoch: [71][150/391]	Time  0.024 ( 0.024)	Data  0.000 ( 0.003)	Loss 3.3661e-02 (6.1627e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [71][160/391]	Time  0.027 ( 0.023)	Data  0.002 ( 0.003)	Loss 4.5349e-02 (6.1745e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [71][170/391]	Time  0.023 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.0444e-02 (6.3053e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 ( 99.95)
Epoch: [71][180/391]	Time  0.035 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.1870e-02 (6.4061e-02)	Acc@1  98.44 ( 97.89)	Acc@5 100.00 ( 99.95)
Epoch: [71][190/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.6284e-01 (6.3644e-02)	Acc@1  93.75 ( 97.89)	Acc@5 100.00 ( 99.95)
Epoch: [71][200/391]	Time  0.026 ( 0.023)	Data  0.005 ( 0.003)	Loss 7.5989e-02 (6.3568e-02)	Acc@1  97.66 ( 97.90)	Acc@5  99.22 ( 99.95)
Epoch: [71][210/391]	Time  0.032 ( 0.023)	Data  0.004 ( 0.003)	Loss 8.1848e-02 (6.3319e-02)	Acc@1  97.66 ( 97.91)	Acc@5 100.00 ( 99.95)
Epoch: [71][220/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.4901e-02 (6.3021e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [71][230/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.2512e-02 (6.2590e-02)	Acc@1 100.00 ( 97.95)	Acc@5 100.00 ( 99.95)
Epoch: [71][240/391]	Time  0.029 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.3916e-01 (6.3097e-02)	Acc@1  93.75 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [71][250/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.0803e-01 (6.3557e-02)	Acc@1  94.53 ( 97.91)	Acc@5 100.00 ( 99.95)
Epoch: [71][260/391]	Time  0.038 ( 0.023)	Data  0.004 ( 0.003)	Loss 6.4331e-02 (6.3187e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 ( 99.95)
Epoch: [71][270/391]	Time  0.020 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.3880e-02 (6.3672e-02)	Acc@1  99.22 ( 97.92)	Acc@5 100.00 ( 99.95)
Epoch: [71][280/391]	Time  0.016 ( 0.023)	Data  0.002 ( 0.003)	Loss 1.2732e-01 (6.3664e-02)	Acc@1  96.88 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [71][290/391]	Time  0.040 ( 0.023)	Data  0.004 ( 0.003)	Loss 4.3884e-02 (6.2904e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.95)
Epoch: [71][300/391]	Time  0.018 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.6570e-02 (6.2695e-02)	Acc@1  99.22 ( 97.97)	Acc@5 100.00 ( 99.95)
Epoch: [71][310/391]	Time  0.018 ( 0.023)	Data  0.002 ( 0.003)	Loss 2.9938e-02 (6.2084e-02)	Acc@1  99.22 ( 98.00)	Acc@5 100.00 ( 99.95)
Epoch: [71][320/391]	Time  0.036 ( 0.023)	Data  0.001 ( 0.003)	Loss 7.1594e-02 (6.1897e-02)	Acc@1  98.44 ( 98.01)	Acc@5 100.00 ( 99.96)
Epoch: [71][330/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.3293e-02 (6.1749e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 ( 99.96)
Epoch: [71][340/391]	Time  0.016 ( 0.023)	Data  0.000 ( 0.002)	Loss 4.1138e-02 (6.1311e-02)	Acc@1 100.00 ( 98.03)	Acc@5 100.00 ( 99.96)
Epoch: [71][350/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.5430e-02 (6.0859e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 ( 99.96)
Epoch: [71][360/391]	Time  0.036 ( 0.023)	Data  0.000 ( 0.002)	Loss 7.2998e-02 (6.1220e-02)	Acc@1  97.66 ( 98.03)	Acc@5 100.00 ( 99.96)
Epoch: [71][370/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.7892e-02 (6.1572e-02)	Acc@1  97.66 ( 98.02)	Acc@5 100.00 ( 99.96)
Epoch: [71][380/391]	Time  0.037 ( 0.023)	Data  0.008 ( 0.002)	Loss 8.4595e-02 (6.1858e-02)	Acc@1  96.88 ( 98.01)	Acc@5 100.00 ( 99.95)
Epoch: [71][390/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 7.3059e-02 (6.1931e-02)	Acc@1  97.50 ( 98.01)	Acc@5 100.00 ( 99.95)
## e[71] optimizer.zero_grad (sum) time: 0.0498046875
## e[71]       loss.backward (sum) time: 1.3097782135009766
## e[71]      optimizer.step (sum) time: 0.37116456031799316
## epoch[71] training(only) time: 9.062405109405518
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.8711e+00 (1.8711e+00)	Acc@1  66.00 ( 66.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.016 ( 0.025)	Loss 1.9854e+00 (1.8968e+00)	Acc@1  63.00 ( 66.09)	Acc@5  90.00 ( 88.55)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.9863e+00 (1.8770e+00)	Acc@1  69.00 ( 67.10)	Acc@5  87.00 ( 88.62)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 1.9893e+00 (1.8779e+00)	Acc@1  64.00 ( 66.94)	Acc@5  85.00 ( 88.23)
Test: [ 40/100]	Time  0.009 ( 0.017)	Loss 1.4521e+00 (1.8465e+00)	Acc@1  69.00 ( 67.37)	Acc@5  93.00 ( 88.71)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.8896e+00 (1.8709e+00)	Acc@1  72.00 ( 67.49)	Acc@5  90.00 ( 88.22)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.6299e+00 (1.8382e+00)	Acc@1  72.00 ( 67.87)	Acc@5  90.00 ( 88.51)
Test: [ 70/100]	Time  0.012 ( 0.015)	Loss 1.9746e+00 (1.8378e+00)	Acc@1  69.00 ( 67.89)	Acc@5  88.00 ( 88.59)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8652e+00 (1.8492e+00)	Acc@1  69.00 ( 67.68)	Acc@5  87.00 ( 88.48)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 2.2090e+00 (1.8433e+00)	Acc@1  62.00 ( 67.78)	Acc@5  88.00 ( 88.65)
 * Acc@1 67.830 Acc@5 88.670
### epoch[71] execution time: 10.623416423797607
EPOCH 72
REMOVING: features.module.22.weight
i:   0, name:        features.module.22.bias  changing lr from: 0.001144537750807189   to: 0.001027021601067212
i:   1, name:      features.module.23.weight  changing lr from: 0.001405418862009349   to: 0.001180423950729625
i:   2, name:        features.module.23.bias  changing lr from: 0.001787058446472905   to: 0.001461150737796029
i:   3, name:      features.module.25.weight  changing lr from: 0.002280352562982362   to: 0.001859983936272363
i:   4, name:        features.module.25.bias  changing lr from: 0.002876517567929881   to: 0.002367996366622093
i:   5, name:      features.module.26.weight  changing lr from: 0.003567112271979777   to: 0.002976579243429322
i:   6, name:        features.module.26.bias  changing lr from: 0.004344053749409027   to: 0.003677462763021538
i:   7, name:            classifier.0.weight  changing lr from: 0.005199627680930862   to: 0.004462730660542013
i:   8, name:              classifier.0.bias  changing lr from: 0.006126494020941730   to: 0.005324829575082939
i:   9, name:            classifier.3.weight  changing lr from: 0.007117688696024515   to: 0.006256573975737162
i:  10, name:              classifier.3.bias  changing lr from: 0.008166621963465497   to: 0.007251147321251743
i:  11, name:            classifier.6.weight  changing lr from: 0.009267073986576002   to: 0.008302100051596310
i:  12, name:              classifier.6.bias  changing lr from: 0.010413188117689882   to: 0.009403344941246495



# Switched to train mode...
Epoch: [72][  0/391]	Time  0.180 ( 0.180)	Data  0.147 ( 0.147)	Loss 2.9572e-02 (2.9572e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.035 ( 0.035)	Data  0.012 ( 0.016)	Loss 9.3567e-02 (5.9924e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.050 ( 0.029)	Data  0.012 ( 0.010)	Loss 7.1960e-02 (5.8834e-02)	Acc@1  97.66 ( 98.07)	Acc@5 100.00 ( 99.96)
Epoch: [72][ 30/391]	Time  0.025 ( 0.027)	Data  0.005 ( 0.007)	Loss 1.0718e-01 (5.9453e-02)	Acc@1  96.88 ( 98.06)	Acc@5 100.00 ( 99.92)
Epoch: [72][ 40/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.006)	Loss 6.5735e-02 (5.6172e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.94)
Epoch: [72][ 50/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.005)	Loss 1.2158e-01 (5.7518e-02)	Acc@1  96.09 ( 98.18)	Acc@5 100.00 ( 99.95)
Epoch: [72][ 60/391]	Time  0.015 ( 0.025)	Data  0.001 ( 0.005)	Loss 1.8356e-02 (5.4214e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.96)
Epoch: [72][ 70/391]	Time  0.024 ( 0.024)	Data  0.001 ( 0.004)	Loss 5.5054e-02 (5.4976e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.96)
Epoch: [72][ 80/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.004)	Loss 2.6825e-02 (5.4667e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.96)
Epoch: [72][ 90/391]	Time  0.030 ( 0.024)	Data  0.001 ( 0.004)	Loss 8.3252e-02 (5.5196e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [72][100/391]	Time  0.037 ( 0.024)	Data  0.001 ( 0.004)	Loss 4.4464e-02 (5.5309e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [72][110/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.003)	Loss 9.8267e-02 (5.5199e-02)	Acc@1  97.66 ( 98.32)	Acc@5  99.22 ( 99.96)
Epoch: [72][120/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.003)	Loss 6.3599e-02 (5.5336e-02)	Acc@1  96.09 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [72][130/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.3569e-02 (5.4826e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [72][140/391]	Time  0.036 ( 0.023)	Data  0.002 ( 0.003)	Loss 5.0140e-02 (5.4916e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [72][150/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 7.8003e-02 (5.6038e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [72][160/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.6505e-02 (5.6402e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [72][170/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 9.1980e-02 (5.6023e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [72][180/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 9.2712e-02 (5.5686e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [72][190/391]	Time  0.033 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.0638e-02 (5.5730e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [72][200/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.1116e-02 (5.6345e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [72][210/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.8441e-02 (5.6453e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [72][220/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.0291e-01 (5.7155e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [72][230/391]	Time  0.025 ( 0.023)	Data  0.005 ( 0.003)	Loss 4.3030e-02 (5.6981e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [72][240/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.2389e-02 (5.7150e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [72][250/391]	Time  0.021 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.8305e-02 (5.7064e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [72][260/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.3621e-02 (5.6835e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [72][270/391]	Time  0.032 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.5319e-02 (5.6914e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [72][280/391]	Time  0.038 ( 0.023)	Data  0.003 ( 0.002)	Loss 4.8096e-02 (5.6854e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [72][290/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 7.7576e-02 (5.6887e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [72][300/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.0192e-02 (5.7585e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [72][310/391]	Time  0.029 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.1317e-02 (5.7725e-02)	Acc@1 100.00 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [72][320/391]	Time  0.038 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.7638e-02 (5.7612e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.97)
Epoch: [72][330/391]	Time  0.032 ( 0.023)	Data  0.000 ( 0.002)	Loss 7.2876e-02 (5.7555e-02)	Acc@1  97.66 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [72][340/391]	Time  0.016 ( 0.023)	Data  0.002 ( 0.002)	Loss 7.3975e-02 (5.8322e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.97)
Epoch: [72][350/391]	Time  0.039 ( 0.023)	Data  0.002 ( 0.002)	Loss 4.2877e-02 (5.8564e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [72][360/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.3640e-02 (5.8325e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.97)
Epoch: [72][370/391]	Time  0.032 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.0436e-02 (5.8555e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.97)
Epoch: [72][380/391]	Time  0.027 ( 0.023)	Data  0.001 ( 0.002)	Loss 9.6497e-02 (5.8609e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.97)
Epoch: [72][390/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.9692e-02 (5.8457e-02)	Acc@1  98.75 ( 98.22)	Acc@5 100.00 ( 99.96)
## e[72] optimizer.zero_grad (sum) time: 0.04543256759643555
## e[72]       loss.backward (sum) time: 1.2495079040527344
## e[72]      optimizer.step (sum) time: 0.33666086196899414
## epoch[72] training(only) time: 9.017237424850464
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.9658e+00 (1.9658e+00)	Acc@1  66.00 ( 66.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.016 ( 0.026)	Loss 1.9941e+00 (1.9246e+00)	Acc@1  61.00 ( 66.18)	Acc@5  90.00 ( 88.45)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.9922e+00 (1.8942e+00)	Acc@1  72.00 ( 67.33)	Acc@5  87.00 ( 88.62)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 2.0625e+00 (1.9012e+00)	Acc@1  62.00 ( 66.97)	Acc@5  85.00 ( 88.29)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4834e+00 (1.8649e+00)	Acc@1  69.00 ( 67.39)	Acc@5  95.00 ( 88.78)
Test: [ 50/100]	Time  0.020 ( 0.016)	Loss 1.8662e+00 (1.8909e+00)	Acc@1  71.00 ( 67.49)	Acc@5  91.00 ( 88.39)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 1.6953e+00 (1.8588e+00)	Acc@1  70.00 ( 67.75)	Acc@5  87.00 ( 88.64)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.9795e+00 (1.8567e+00)	Acc@1  69.00 ( 67.82)	Acc@5  87.00 ( 88.76)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.8535e+00 (1.8637e+00)	Acc@1  67.00 ( 67.67)	Acc@5  85.00 ( 88.60)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 2.1777e+00 (1.8575e+00)	Acc@1  62.00 ( 67.82)	Acc@5  88.00 ( 88.77)
 * Acc@1 67.890 Acc@5 88.790
### epoch[72] execution time: 10.611600875854492
EPOCH 73
REMOVING: features.module.22.bias
i:   0, name:      features.module.23.weight  changing lr from: 0.001180423950729625   to: 0.001045203151675366
i:   1, name:        features.module.23.bias  changing lr from: 0.001461150737796029   to: 0.001221560953727595
i:   2, name:      features.module.25.weight  changing lr from: 0.001859983936272363   to: 0.001522423939329869
i:   3, name:        features.module.25.bias  changing lr from: 0.002367996366622093   to: 0.001938748667092038
i:   4, name:      features.module.26.weight  changing lr from: 0.002976579243429322   to: 0.002461782502262993
i:   5, name:        features.module.26.bias  changing lr from: 0.003677462763021538   to: 0.003083089297807195
i:   6, name:            classifier.0.weight  changing lr from: 0.004462730660542013   to: 0.003794568464909392
i:   7, name:              classifier.0.bias  changing lr from: 0.005324829575082939   to: 0.004588468317894822
i:   8, name:            classifier.3.weight  changing lr from: 0.006256573975737162   to: 0.005457394491727352
i:   9, name:              classifier.3.bias  changing lr from: 0.007251147321251743   to: 0.006394314148431595
i:  10, name:            classifier.6.weight  changing lr from: 0.008302100051596310   to: 0.007392556612367617
i:  11, name:              classifier.6.bias  changing lr from: 0.009403344941246495   to: 0.008445811003457126



# Switched to train mode...
Epoch: [73][  0/391]	Time  0.165 ( 0.165)	Data  0.141 ( 0.141)	Loss 3.5828e-02 (3.5828e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.018 ( 0.033)	Data  0.004 ( 0.015)	Loss 1.3416e-01 (5.8742e-02)	Acc@1  95.31 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.032 ( 0.029)	Data  0.006 ( 0.009)	Loss 2.3804e-02 (4.9588e-02)	Acc@1 100.00 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.006)	Loss 6.8176e-02 (5.5533e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.015 ( 0.025)	Data  0.001 ( 0.005)	Loss 6.3660e-02 (5.4508e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.005)	Loss 6.9092e-02 (5.4462e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.004)	Loss 1.2134e-01 (5.5042e-02)	Acc@1  96.09 ( 98.26)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.015 ( 0.024)	Data  0.002 ( 0.004)	Loss 4.0833e-02 (5.4806e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.004)	Loss 1.6312e-02 (5.3932e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.99)
Epoch: [73][ 90/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.003)	Loss 1.3525e-01 (5.4674e-02)	Acc@1  94.53 ( 98.28)	Acc@5  99.22 ( 99.97)
Epoch: [73][100/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.2214e-02 (5.4329e-02)	Acc@1  98.44 ( 98.31)	Acc@5  99.22 ( 99.96)
Epoch: [73][110/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.9326e-02 (5.4582e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.96)
Epoch: [73][120/391]	Time  0.032 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.6224e-02 (5.4289e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [73][130/391]	Time  0.032 ( 0.023)	Data  0.000 ( 0.003)	Loss 9.0820e-02 (5.5425e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [73][140/391]	Time  0.020 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.1177e-03 (5.5535e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [73][150/391]	Time  0.016 ( 0.023)	Data  0.002 ( 0.003)	Loss 1.2292e-01 (5.6198e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [73][160/391]	Time  0.019 ( 0.023)	Data  0.002 ( 0.003)	Loss 1.9861e-01 (5.7081e-02)	Acc@1  92.97 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [73][170/391]	Time  0.023 ( 0.023)	Data  0.002 ( 0.003)	Loss 3.8513e-02 (5.8068e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [73][180/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.003)	Loss 9.2407e-02 (5.8799e-02)	Acc@1  96.09 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [73][190/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 4.9164e-02 (5.8650e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [73][200/391]	Time  0.024 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.7506e-02 (5.8261e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [73][210/391]	Time  0.018 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.4443e-02 (5.8115e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [73][220/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.5247e-02 (5.7951e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [73][230/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.1697e-02 (5.7931e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [73][240/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 3.6682e-02 (5.8167e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [73][250/391]	Time  0.024 ( 0.023)	Data  0.000 ( 0.003)	Loss 5.4138e-02 (5.8269e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [73][260/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.9103e-02 (5.8228e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [73][270/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 6.1462e-02 (5.8046e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [73][280/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.7170e-02 (5.7780e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [73][290/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.4108e-02 (5.8549e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [73][300/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.7261e-02 (5.8603e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [73][310/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.2171e-02 (5.8739e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [73][320/391]	Time  0.028 ( 0.023)	Data  0.000 ( 0.003)	Loss 1.7807e-02 (5.8279e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [73][330/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.1229e-02 (5.7945e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [73][340/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.5898e-02 (5.8000e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [73][350/391]	Time  0.023 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.5481e-02 (5.8400e-02)	Acc@1  97.66 ( 98.14)	Acc@5  99.22 ( 99.96)
Epoch: [73][360/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.7943e-02 (5.8596e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [73][370/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.1176e-01 (5.8420e-02)	Acc@1  97.66 ( 98.14)	Acc@5  99.22 ( 99.96)
Epoch: [73][380/391]	Time  0.036 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.8229e-02 (5.8543e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [73][390/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.002)	Loss 6.1035e-02 (5.8210e-02)	Acc@1  98.75 ( 98.14)	Acc@5 100.00 ( 99.96)
## e[73] optimizer.zero_grad (sum) time: 0.043286800384521484
## e[73]       loss.backward (sum) time: 1.2355186939239502
## e[73]      optimizer.step (sum) time: 0.3222789764404297
## epoch[73] training(only) time: 8.929867267608643
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.9443e+00 (1.9443e+00)	Acc@1  65.00 ( 65.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.011 ( 0.024)	Loss 2.0684e+00 (1.9405e+00)	Acc@1  63.00 ( 66.18)	Acc@5  90.00 ( 87.91)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.9688e+00 (1.8985e+00)	Acc@1  70.00 ( 67.24)	Acc@5  88.00 ( 88.43)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.0605e+00 (1.9050e+00)	Acc@1  62.00 ( 66.58)	Acc@5  86.00 ( 88.00)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4395e+00 (1.8691e+00)	Acc@1  66.00 ( 66.85)	Acc@5  94.00 ( 88.59)
Test: [ 50/100]	Time  0.017 ( 0.016)	Loss 1.8457e+00 (1.8934e+00)	Acc@1  71.00 ( 67.02)	Acc@5  90.00 ( 88.22)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.6953e+00 (1.8594e+00)	Acc@1  71.00 ( 67.39)	Acc@5  87.00 ( 88.43)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 2.0410e+00 (1.8564e+00)	Acc@1  67.00 ( 67.48)	Acc@5  89.00 ( 88.54)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.8164e+00 (1.8636e+00)	Acc@1  67.00 ( 67.30)	Acc@5  87.00 ( 88.46)
Test: [ 90/100]	Time  0.036 ( 0.015)	Loss 2.1309e+00 (1.8561e+00)	Acc@1  63.00 ( 67.53)	Acc@5  88.00 ( 88.65)
 * Acc@1 67.640 Acc@5 88.720
### epoch[73] execution time: 10.514292478561401
EPOCH 74
i:   0, name:      features.module.23.weight  changing lr from: 0.001045203151675366   to: 0.001000000130653305
i:   1, name:        features.module.23.bias  changing lr from: 0.001221560953727595   to: 0.001068706562441084
i:   2, name:      features.module.25.weight  changing lr from: 0.001522423939329869   to: 0.001268241413662268
i:   3, name:        features.module.25.bias  changing lr from: 0.001938748667092038   to: 0.001589474172533705
i:   4, name:      features.module.26.weight  changing lr from: 0.002461782502262993   to: 0.002023533918883191
i:   5, name:        features.module.26.bias  changing lr from: 0.003083089297807195   to: 0.002561840409490700
i:   6, name:            classifier.0.weight  changing lr from: 0.003794568464909392   to: 0.003196127959228938
i:   7, name:              classifier.0.bias  changing lr from: 0.004588468317894822   to: 0.003918463047597070
i:   8, name:            classifier.3.weight  changing lr from: 0.005457394491727352   to: 0.004721256492992287
i:   9, name:              classifier.3.bias  changing lr from: 0.006394314148431595   to: 0.005597270954124766
i:  10, name:            classifier.6.weight  changing lr from: 0.007392556612367617   to: 0.006539624439930302
i:  11, name:              classifier.6.bias  changing lr from: 0.008445811003457126   to: 0.007541790436509810



# Switched to train mode...
Epoch: [74][  0/391]	Time  0.177 ( 0.177)	Data  0.151 ( 0.151)	Loss 3.7537e-02 (3.7537e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.025 ( 0.036)	Data  0.003 ( 0.015)	Loss 9.1858e-02 (5.8592e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.93)
Epoch: [74][ 20/391]	Time  0.016 ( 0.028)	Data  0.002 ( 0.009)	Loss 5.0781e-02 (5.5998e-02)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 ( 99.96)
Epoch: [74][ 30/391]	Time  0.015 ( 0.027)	Data  0.001 ( 0.007)	Loss 3.0319e-02 (5.6174e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.92)
Epoch: [74][ 40/391]	Time  0.015 ( 0.025)	Data  0.001 ( 0.006)	Loss 4.3182e-02 (5.8565e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.92)
Epoch: [74][ 50/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.005)	Loss 4.7516e-02 (5.5838e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 ( 99.94)
Epoch: [74][ 60/391]	Time  0.018 ( 0.024)	Data  0.001 ( 0.004)	Loss 1.0931e-01 (5.5827e-02)	Acc@1  95.31 ( 98.25)	Acc@5 100.00 ( 99.94)
Epoch: [74][ 70/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.004)	Loss 4.5349e-02 (5.5622e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.93)
Epoch: [74][ 80/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.004)	Loss 4.7058e-02 (5.4132e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.94)
Epoch: [74][ 90/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.004)	Loss 6.7261e-02 (5.4359e-02)	Acc@1  97.66 ( 98.28)	Acc@5 100.00 ( 99.95)
Epoch: [74][100/391]	Time  0.033 ( 0.024)	Data  0.001 ( 0.003)	Loss 5.8685e-02 (5.4087e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.95)
Epoch: [74][110/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.9938e-02 (5.3241e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.96)
Epoch: [74][120/391]	Time  0.024 ( 0.023)	Data  0.000 ( 0.003)	Loss 7.9285e-02 (5.4283e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.96)
Epoch: [74][130/391]	Time  0.019 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.1178e-02 (5.4683e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.96)
Epoch: [74][140/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.4697e-02 (5.4806e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.96)
Epoch: [74][150/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.4465e-01 (5.4407e-02)	Acc@1  94.53 ( 98.28)	Acc@5 100.00 ( 99.96)
Epoch: [74][160/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.8116e-02 (5.4463e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.95)
Epoch: [74][170/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 1.6388e-02 (5.3837e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 ( 99.95)
Epoch: [74][180/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.0944e-02 (5.3891e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.94)
Epoch: [74][190/391]	Time  0.029 ( 0.023)	Data  0.003 ( 0.003)	Loss 2.4551e-02 (5.3183e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.95)
Epoch: [74][200/391]	Time  0.033 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.4800e-02 (5.2491e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.95)
Epoch: [74][210/391]	Time  0.017 ( 0.023)	Data  0.002 ( 0.003)	Loss 1.0211e-01 (5.2430e-02)	Acc@1  96.09 ( 98.33)	Acc@5 100.00 ( 99.95)
Epoch: [74][220/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.3427e-02 (5.3575e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.95)
Epoch: [74][230/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.0394e-01 (5.4631e-02)	Acc@1  96.09 ( 98.26)	Acc@5 100.00 ( 99.95)
Epoch: [74][240/391]	Time  0.031 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.9276e-02 (5.4064e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.95)
Epoch: [74][250/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.002)	Loss 5.8655e-02 (5.3938e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.95)
Epoch: [74][260/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 7.9285e-02 (5.4471e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.95)
Epoch: [74][270/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.7903e-02 (5.4933e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 ( 99.95)
Epoch: [74][280/391]	Time  0.024 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.7943e-02 (5.4591e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [74][290/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.3040e-02 (5.4449e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [74][300/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.002)	Loss 9.8206e-02 (5.4401e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.95)
Epoch: [74][310/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.2805e-02 (5.4027e-02)	Acc@1  96.88 ( 98.27)	Acc@5 100.00 ( 99.95)
Epoch: [74][320/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.002)	Loss 5.1758e-02 (5.4064e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.95)
Epoch: [74][330/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.5797e-02 (5.4311e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.96)
Epoch: [74][340/391]	Time  0.021 ( 0.023)	Data  0.001 ( 0.002)	Loss 7.9651e-02 (5.4513e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.96)
Epoch: [74][350/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.002)	Loss 1.0101e-01 (5.4963e-02)	Acc@1  97.66 ( 98.24)	Acc@5  99.22 ( 99.96)
Epoch: [74][360/391]	Time  0.031 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.8539e-02 (5.4515e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.96)
Epoch: [74][370/391]	Time  0.032 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.9724e-02 (5.4361e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.96)
Epoch: [74][380/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.1881e-02 (5.4226e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.96)
Epoch: [74][390/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.002)	Loss 5.8441e-02 (5.4738e-02)	Acc@1  98.75 ( 98.25)	Acc@5 100.00 ( 99.96)
## e[74] optimizer.zero_grad (sum) time: 0.043152570724487305
## e[74]       loss.backward (sum) time: 1.223357915878296
## e[74]      optimizer.step (sum) time: 0.3193473815917969
## epoch[74] training(only) time: 8.95372462272644
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.9238e+00 (1.9238e+00)	Acc@1  66.00 ( 66.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.013 ( 0.025)	Loss 2.0059e+00 (1.9263e+00)	Acc@1  62.00 ( 66.55)	Acc@5  89.00 ( 88.18)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 2.0117e+00 (1.8930e+00)	Acc@1  71.00 ( 67.38)	Acc@5  86.00 ( 88.67)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 2.0840e+00 (1.8992e+00)	Acc@1  61.00 ( 66.90)	Acc@5  86.00 ( 88.16)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 1.4805e+00 (1.8625e+00)	Acc@1  67.00 ( 67.22)	Acc@5  94.00 ( 88.78)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.8623e+00 (1.8879e+00)	Acc@1  70.00 ( 67.27)	Acc@5  91.00 ( 88.43)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 1.6426e+00 (1.8542e+00)	Acc@1  71.00 ( 67.62)	Acc@5  89.00 ( 88.66)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 2.0391e+00 (1.8505e+00)	Acc@1  68.00 ( 67.80)	Acc@5  88.00 ( 88.79)
Test: [ 80/100]	Time  0.011 ( 0.015)	Loss 1.8242e+00 (1.8579e+00)	Acc@1  69.00 ( 67.64)	Acc@5  85.00 ( 88.65)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.1504e+00 (1.8512e+00)	Acc@1  63.00 ( 67.86)	Acc@5  87.00 ( 88.84)
 * Acc@1 67.900 Acc@5 88.880
### epoch[74] execution time: 10.532089233398438
EPOCH 75
REMOVING: features.module.23.weight
i:   0, name:        features.module.23.bias  changing lr from: 0.001068706562441084   to: 0.001002853901851480
i:   1, name:      features.module.25.weight  changing lr from: 0.001268241413662268   to: 0.001097864696668448
i:   2, name:        features.module.25.bias  changing lr from: 0.001589474172533705   to: 0.001320742224331300
i:   3, name:      features.module.26.weight  changing lr from: 0.002023533918883191   to: 0.001662524641903106
i:   4, name:        features.module.26.bias  changing lr from: 0.002561840409490700   to: 0.002114511560547804
i:   5, name:            classifier.0.weight  changing lr from: 0.003196127959228938   to: 0.002668293030842316
i:   6, name:              classifier.0.bias  changing lr from: 0.003918463047597070   to: 0.003315771691224655
i:   7, name:            classifier.3.weight  changing lr from: 0.004721256492992287   to: 0.004049178964552485
i:   8, name:              classifier.3.bias  changing lr from: 0.005597270954124766   to: 0.004861086104270567
i:   9, name:            classifier.6.weight  changing lr from: 0.006539624439930302   to: 0.005744410812469424
i:  10, name:              classifier.6.bias  changing lr from: 0.007541790436509810   to: 0.006692420077669891



# Switched to train mode...
Epoch: [75][  0/391]	Time  0.168 ( 0.168)	Data  0.146 ( 0.146)	Loss 7.6294e-02 (7.6294e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.027 ( 0.035)	Data  0.002 ( 0.016)	Loss 7.2449e-02 (6.3210e-02)	Acc@1  97.66 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.016 ( 0.029)	Data  0.001 ( 0.009)	Loss 2.4063e-02 (5.1779e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.016 ( 0.027)	Data  0.002 ( 0.007)	Loss 5.4810e-02 (5.4498e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 40/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.006)	Loss 9.1736e-02 (5.4549e-02)	Acc@1  96.88 ( 98.48)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 50/391]	Time  0.040 ( 0.025)	Data  0.001 ( 0.005)	Loss 7.0129e-02 (5.7511e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 60/391]	Time  0.030 ( 0.025)	Data  0.003 ( 0.004)	Loss 8.3008e-02 (5.4894e-02)	Acc@1  96.88 ( 98.44)	Acc@5  99.22 ( 99.96)
Epoch: [75][ 70/391]	Time  0.025 ( 0.024)	Data  0.005 ( 0.004)	Loss 6.4148e-02 (5.4542e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 80/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.004)	Loss 2.7267e-02 (5.6169e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 90/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.004)	Loss 3.4576e-02 (5.4714e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 ( 99.97)
Epoch: [75][100/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.003)	Loss 8.6121e-02 (5.6319e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [75][110/391]	Time  0.037 ( 0.024)	Data  0.002 ( 0.003)	Loss 4.1138e-02 (5.6013e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [75][120/391]	Time  0.037 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.7150e-02 (5.5045e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [75][130/391]	Time  0.033 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.5144e-02 (5.5285e-02)	Acc@1  96.88 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [75][140/391]	Time  0.025 ( 0.023)	Data  0.003 ( 0.003)	Loss 1.2854e-01 (5.6400e-02)	Acc@1  96.88 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [75][150/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 2.8290e-02 (5.6008e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [75][160/391]	Time  0.016 ( 0.023)	Data  0.000 ( 0.003)	Loss 6.5125e-02 (5.6257e-02)	Acc@1  97.66 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [75][170/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.4006e-02 (5.6502e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [75][180/391]	Time  0.020 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.8244e-02 (5.5915e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [75][190/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.003)	Loss 9.6924e-02 (5.5779e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [75][200/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.1067e-02 (5.6267e-02)	Acc@1  97.66 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [75][210/391]	Time  0.021 ( 0.023)	Data  0.002 ( 0.003)	Loss 1.9547e-02 (5.6458e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [75][220/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.003)	Loss 5.8594e-02 (5.6001e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [75][230/391]	Time  0.038 ( 0.023)	Data  0.006 ( 0.003)	Loss 9.5703e-02 (5.6652e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [75][240/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.4656e-02 (5.7414e-02)	Acc@1  97.66 ( 98.26)	Acc@5  99.22 ( 99.98)
Epoch: [75][250/391]	Time  0.023 ( 0.023)	Data  0.004 ( 0.003)	Loss 9.0149e-02 (5.8097e-02)	Acc@1  96.09 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [75][260/391]	Time  0.021 ( 0.023)	Data  0.002 ( 0.002)	Loss 3.2623e-02 (5.7877e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [75][270/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.6642e-02 (5.7994e-02)	Acc@1  99.22 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [75][280/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.0228e-02 (5.7576e-02)	Acc@1 100.00 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [75][290/391]	Time  0.021 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.9652e-02 (5.7659e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 ( 99.98)
Epoch: [75][300/391]	Time  0.024 ( 0.023)	Data  0.003 ( 0.002)	Loss 1.1542e-01 (5.8401e-02)	Acc@1  96.09 ( 98.20)	Acc@5  99.22 ( 99.97)
Epoch: [75][310/391]	Time  0.035 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.3478e-02 (5.7750e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.97)
Epoch: [75][320/391]	Time  0.030 ( 0.023)	Data  0.000 ( 0.002)	Loss 6.8054e-02 (5.7445e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [75][330/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.002)	Loss 2.5558e-02 (5.7386e-02)	Acc@1 100.00 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [75][340/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.002)	Loss 8.6365e-02 (5.7548e-02)	Acc@1  97.66 ( 98.24)	Acc@5  99.22 ( 99.97)
Epoch: [75][350/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.6101e-02 (5.7773e-02)	Acc@1  96.88 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [75][360/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.3365e-02 (5.8271e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.97)
Epoch: [75][370/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 9.8572e-02 (5.8634e-02)	Acc@1  96.88 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [75][380/391]	Time  0.019 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.5817e-02 (5.8408e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.97)
Epoch: [75][390/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.002)	Loss 4.1534e-02 (5.8319e-02)	Acc@1  98.75 ( 98.21)	Acc@5 100.00 ( 99.97)
## e[75] optimizer.zero_grad (sum) time: 0.040952444076538086
## e[75]       loss.backward (sum) time: 1.2210171222686768
## e[75]      optimizer.step (sum) time: 0.2937431335449219
## epoch[75] training(only) time: 8.940666675567627
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.9277e+00 (1.9277e+00)	Acc@1  65.00 ( 65.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.026 ( 0.025)	Loss 1.9961e+00 (1.9223e+00)	Acc@1  62.00 ( 66.09)	Acc@5  89.00 ( 87.82)
Test: [ 20/100]	Time  0.009 ( 0.019)	Loss 1.9951e+00 (1.8935e+00)	Acc@1  71.00 ( 67.05)	Acc@5  87.00 ( 88.29)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.1270e+00 (1.9028e+00)	Acc@1  61.00 ( 66.90)	Acc@5  85.00 ( 87.97)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4541e+00 (1.8635e+00)	Acc@1  68.00 ( 67.17)	Acc@5  94.00 ( 88.59)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.8750e+00 (1.8889e+00)	Acc@1  71.00 ( 67.33)	Acc@5  91.00 ( 88.27)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 1.6416e+00 (1.8558e+00)	Acc@1  71.00 ( 67.69)	Acc@5  88.00 ( 88.57)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 2.0332e+00 (1.8512e+00)	Acc@1  68.00 ( 67.82)	Acc@5  89.00 ( 88.70)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.8428e+00 (1.8593e+00)	Acc@1  68.00 ( 67.63)	Acc@5  86.00 ( 88.57)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 2.1953e+00 (1.8520e+00)	Acc@1  62.00 ( 67.78)	Acc@5  86.00 ( 88.74)
 * Acc@1 67.880 Acc@5 88.800
### epoch[75] execution time: 10.50540018081665
EPOCH 76
REMOVING: features.module.23.bias
i:   0, name:      features.module.25.weight  changing lr from: 0.001097864696668448   to: 0.001011580899823088
i:   1, name:        features.module.25.bias  changing lr from: 0.001320742224331300   to: 0.001132990873993738
i:   2, name:      features.module.26.weight  changing lr from: 0.001662524641903106   to: 0.001379324008157111
i:   3, name:        features.module.26.bias  changing lr from: 0.002114511560547804   to: 0.001741785406273543
i:   4, name:            classifier.0.weight  changing lr from: 0.002668293030842316   to: 0.002211843283749048
i:   5, name:              classifier.0.bias  changing lr from: 0.003315771691224655   to: 0.002781255935050213
i:   6, name:            classifier.3.weight  changing lr from: 0.004049178964552485   to: 0.003442092216941100
i:   7, name:              classifier.3.bias  changing lr from: 0.004861086104270567   to: 0.004186746389577682
i:   8, name:            classifier.6.weight  changing lr from: 0.005744410812469424   to: 0.005007948077844020
i:   9, name:              classifier.6.bias  changing lr from: 0.006692420077669891   to: 0.005898768039667684



# Switched to train mode...
Epoch: [76][  0/391]	Time  0.171 ( 0.171)	Data  0.148 ( 0.148)	Loss 4.3030e-02 (4.3030e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.019 ( 0.035)	Data  0.001 ( 0.015)	Loss 6.7993e-02 (5.6682e-02)	Acc@1  97.66 ( 98.30)	Acc@5  99.22 ( 99.86)
Epoch: [76][ 20/391]	Time  0.016 ( 0.029)	Data  0.001 ( 0.008)	Loss 1.2250e-01 (5.5023e-02)	Acc@1  97.66 ( 98.40)	Acc@5 100.00 ( 99.93)
Epoch: [76][ 30/391]	Time  0.032 ( 0.026)	Data  0.008 ( 0.007)	Loss 3.3203e-02 (5.4970e-02)	Acc@1 100.00 ( 98.56)	Acc@5 100.00 ( 99.92)
Epoch: [76][ 40/391]	Time  0.019 ( 0.025)	Data  0.002 ( 0.006)	Loss 5.9204e-02 (5.8789e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.92)
Epoch: [76][ 50/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.005)	Loss 3.5522e-02 (5.7887e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.91)
Epoch: [76][ 60/391]	Time  0.017 ( 0.024)	Data  0.001 ( 0.004)	Loss 4.0558e-02 (5.7227e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.92)
Epoch: [76][ 70/391]	Time  0.023 ( 0.024)	Data  0.001 ( 0.004)	Loss 7.9773e-02 (5.7568e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.93)
Epoch: [76][ 80/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.004)	Loss 2.1500e-02 (5.8923e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.94)
Epoch: [76][ 90/391]	Time  0.022 ( 0.024)	Data  0.001 ( 0.003)	Loss 6.9702e-02 (5.8169e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.95)
Epoch: [76][100/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.003)	Loss 6.3293e-02 (5.8218e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 ( 99.95)
Epoch: [76][110/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.8440e-02 (5.8648e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [76][120/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.5589e-02 (5.7513e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.95)
Epoch: [76][130/391]	Time  0.031 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.3783e-02 (5.7344e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.96)
Epoch: [76][140/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.4993e-02 (5.8193e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.95)
Epoch: [76][150/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.4502e-01 (5.8740e-02)	Acc@1  94.53 ( 98.28)	Acc@5 100.00 ( 99.95)
Epoch: [76][160/391]	Time  0.030 ( 0.023)	Data  0.000 ( 0.003)	Loss 7.9163e-02 (5.7964e-02)	Acc@1  96.88 ( 98.29)	Acc@5  99.22 ( 99.95)
Epoch: [76][170/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 5.0659e-02 (5.8553e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.95)
Epoch: [76][180/391]	Time  0.029 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.0314e-02 (5.9642e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [76][190/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.5587e-02 (5.8322e-02)	Acc@1 100.00 ( 98.28)	Acc@5 100.00 ( 99.95)
Epoch: [76][200/391]	Time  0.025 ( 0.023)	Data  0.003 ( 0.003)	Loss 5.4169e-02 (5.7786e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.95)
Epoch: [76][210/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.6711e-02 (5.7452e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 ( 99.96)
Epoch: [76][220/391]	Time  0.026 ( 0.023)	Data  0.002 ( 0.003)	Loss 1.9165e-02 (5.7700e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.95)
Epoch: [76][230/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.2122e-01 (5.7375e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 ( 99.95)
Epoch: [76][240/391]	Time  0.029 ( 0.023)	Data  0.000 ( 0.002)	Loss 6.0883e-02 (5.6981e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.95)
Epoch: [76][250/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.1340e-02 (5.6842e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.95)
Epoch: [76][260/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.4413e-02 (5.6520e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.96)
Epoch: [76][270/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.1432e-02 (5.6684e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.95)
Epoch: [76][280/391]	Time  0.031 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.9509e-02 (5.7196e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.96)
Epoch: [76][290/391]	Time  0.023 ( 0.023)	Data  0.001 ( 0.002)	Loss 8.4167e-02 (5.7298e-02)	Acc@1  97.66 ( 98.30)	Acc@5  99.22 ( 99.95)
Epoch: [76][300/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.5308e-02 (5.7264e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.95)
Epoch: [76][310/391]	Time  0.028 ( 0.023)	Data  0.001 ( 0.002)	Loss 3.4332e-02 (5.6975e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.95)
Epoch: [76][320/391]	Time  0.024 ( 0.023)	Data  0.001 ( 0.002)	Loss 1.5747e-02 (5.6519e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.95)
Epoch: [76][330/391]	Time  0.040 ( 0.023)	Data  0.007 ( 0.002)	Loss 5.3345e-02 (5.6645e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.95)
Epoch: [76][340/391]	Time  0.028 ( 0.023)	Data  0.001 ( 0.002)	Loss 9.5337e-02 (5.6752e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.95)
Epoch: [76][350/391]	Time  0.024 ( 0.023)	Data  0.001 ( 0.002)	Loss 8.8562e-02 (5.7314e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.95)
Epoch: [76][360/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.002)	Loss 4.7211e-02 (5.7218e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.95)
Epoch: [76][370/391]	Time  0.019 ( 0.023)	Data  0.000 ( 0.002)	Loss 6.4697e-02 (5.7565e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.96)
Epoch: [76][380/391]	Time  0.037 ( 0.023)	Data  0.001 ( 0.002)	Loss 8.3435e-02 (5.7738e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.96)
Epoch: [76][390/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 6.2927e-02 (5.7940e-02)	Acc@1  97.50 ( 98.26)	Acc@5 100.00 ( 99.96)
## e[76] optimizer.zero_grad (sum) time: 0.0380709171295166
## e[76]       loss.backward (sum) time: 1.132735013961792
## e[76]      optimizer.step (sum) time: 0.2689521312713623
## epoch[76] training(only) time: 8.929934978485107
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.9365e+00 (1.9365e+00)	Acc@1  64.00 ( 64.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.012 ( 0.026)	Loss 2.0410e+00 (1.9190e+00)	Acc@1  62.00 ( 65.91)	Acc@5  90.00 ( 87.91)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.9629e+00 (1.8806e+00)	Acc@1  73.00 ( 67.38)	Acc@5  88.00 ( 88.57)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.0742e+00 (1.8864e+00)	Acc@1  61.00 ( 67.03)	Acc@5  85.00 ( 88.06)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 1.5000e+00 (1.8505e+00)	Acc@1  66.00 ( 67.20)	Acc@5  94.00 ( 88.63)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.8467e+00 (1.8746e+00)	Acc@1  70.00 ( 67.31)	Acc@5  90.00 ( 88.27)
Test: [ 60/100]	Time  0.030 ( 0.016)	Loss 1.6553e+00 (1.8425e+00)	Acc@1  72.00 ( 67.66)	Acc@5  87.00 ( 88.51)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 2.0195e+00 (1.8394e+00)	Acc@1  68.00 ( 67.77)	Acc@5  89.00 ( 88.63)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.8145e+00 (1.8467e+00)	Acc@1  68.00 ( 67.63)	Acc@5  86.00 ( 88.52)
Test: [ 90/100]	Time  0.019 ( 0.015)	Loss 2.1562e+00 (1.8395e+00)	Acc@1  63.00 ( 67.81)	Acc@5  87.00 ( 88.71)
 * Acc@1 67.920 Acc@5 88.760
### epoch[76] execution time: 10.495355606079102
EPOCH 77
REMOVING: features.module.25.weight
i:   0, name:        features.module.25.bias  changing lr from: 0.001132990873993738   to: 0.001026526169100655
i:   1, name:      features.module.26.weight  changing lr from: 0.001379324008157111   to: 0.001174378644816847
i:   2, name:        features.module.26.bias  changing lr from: 0.001741785406273543   to: 0.001444230753002273
i:   3, name:            classifier.0.weight  changing lr from: 0.002211843283749048   to: 0.001827452887143421
i:   4, name:              classifier.0.bias  changing lr from: 0.002781255935050213   to: 0.002315679992604807
i:   5, name:            classifier.3.weight  changing lr from: 0.003442092216941100   to: 0.002900836598439338
i:   6, name:              classifier.3.bias  changing lr from: 0.004186746389577682   to: 0.003575155702818594
i:   7, name:            classifier.6.weight  changing lr from: 0.005007948077844020   to: 0.004331192313391201
i:   8, name:              classifier.6.bias  changing lr from: 0.005898768039667684   to: 0.005161832367509555



# Switched to train mode...
Epoch: [77][  0/391]	Time  0.170 ( 0.170)	Data  0.149 ( 0.149)	Loss 5.5145e-02 (5.5145e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.015 ( 0.034)	Data  0.001 ( 0.015)	Loss 6.6345e-02 (5.7172e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.009)	Loss 9.2407e-02 (5.3950e-02)	Acc@1  96.88 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.007)	Loss 5.3101e-02 (5.2922e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [77][ 40/391]	Time  0.022 ( 0.025)	Data  0.002 ( 0.005)	Loss 5.1117e-02 (5.2162e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [77][ 50/391]	Time  0.015 ( 0.024)	Data  0.000 ( 0.005)	Loss 5.4199e-02 (5.1306e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [77][ 60/391]	Time  0.024 ( 0.024)	Data  0.005 ( 0.004)	Loss 4.6783e-02 (5.2389e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 70/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.004)	Loss 5.1453e-02 (5.3484e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 80/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.004)	Loss 3.7567e-02 (5.1988e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [77][ 90/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 7.1716e-02 (5.2191e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.99)
Epoch: [77][100/391]	Time  0.044 ( 0.023)	Data  0.012 ( 0.003)	Loss 3.8544e-02 (5.3834e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [77][110/391]	Time  0.035 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.0498e-01 (5.5241e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [77][120/391]	Time  0.027 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.7939e-02 (5.4942e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.99)
Epoch: [77][130/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.3557e-02 (5.5828e-02)	Acc@1  96.88 ( 98.06)	Acc@5 100.00 ( 99.99)
Epoch: [77][140/391]	Time  0.022 ( 0.023)	Data  0.000 ( 0.003)	Loss 1.0852e-01 (5.6622e-02)	Acc@1  96.09 ( 98.06)	Acc@5 100.00 ( 99.99)
Epoch: [77][150/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.3040e-02 (5.6007e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [77][160/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.003)	Loss 1.0480e-01 (5.5990e-02)	Acc@1  96.88 ( 98.10)	Acc@5  99.22 ( 99.98)
Epoch: [77][170/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 9.5520e-02 (5.5864e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [77][180/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.003)	Loss 6.5186e-02 (5.6459e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [77][190/391]	Time  0.034 ( 0.023)	Data  0.003 ( 0.003)	Loss 3.7811e-02 (5.6410e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [77][200/391]	Time  0.029 ( 0.023)	Data  0.001 ( 0.003)	Loss 7.9590e-02 (5.6300e-02)	Acc@1  96.88 ( 98.13)	Acc@5  99.22 ( 99.97)
Epoch: [77][210/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 7.6904e-02 (5.6562e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [77][220/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.6040e-02 (5.6054e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [77][230/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.5511e-02 (5.6447e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [77][240/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.002)	Loss 5.8685e-02 (5.6614e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [77][250/391]	Time  0.020 ( 0.022)	Data  0.001 ( 0.002)	Loss 4.2480e-02 (5.6084e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [77][260/391]	Time  0.034 ( 0.022)	Data  0.001 ( 0.002)	Loss 6.1066e-02 (5.6769e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [77][270/391]	Time  0.024 ( 0.022)	Data  0.001 ( 0.002)	Loss 1.1969e-01 (5.7063e-02)	Acc@1  99.22 ( 98.13)	Acc@5  99.22 ( 99.96)
Epoch: [77][280/391]	Time  0.027 ( 0.022)	Data  0.000 ( 0.002)	Loss 3.6865e-02 (5.6820e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [77][290/391]	Time  0.016 ( 0.022)	Data  0.001 ( 0.002)	Loss 3.6591e-02 (5.6693e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [77][300/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 3.4210e-02 (5.7362e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [77][310/391]	Time  0.032 ( 0.022)	Data  0.001 ( 0.002)	Loss 1.0706e-01 (5.7929e-02)	Acc@1  96.09 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [77][320/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 8.2275e-02 (5.7758e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [77][330/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 3.6591e-02 (5.7444e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [77][340/391]	Time  0.029 ( 0.022)	Data  0.001 ( 0.002)	Loss 3.6133e-02 (5.7432e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [77][350/391]	Time  0.026 ( 0.022)	Data  0.001 ( 0.002)	Loss 7.3303e-02 (5.7370e-02)	Acc@1  96.09 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [77][360/391]	Time  0.032 ( 0.022)	Data  0.002 ( 0.002)	Loss 5.8502e-02 (5.7327e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [77][370/391]	Time  0.037 ( 0.022)	Data  0.001 ( 0.002)	Loss 1.1993e-02 (5.7547e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [77][380/391]	Time  0.028 ( 0.022)	Data  0.001 ( 0.002)	Loss 2.9755e-02 (5.7408e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [77][390/391]	Time  0.014 ( 0.022)	Data  0.000 ( 0.002)	Loss 7.5684e-02 (5.7392e-02)	Acc@1  97.50 ( 98.11)	Acc@5 100.00 ( 99.96)
## e[77] optimizer.zero_grad (sum) time: 0.03461313247680664
## e[77]       loss.backward (sum) time: 1.100212574005127
## e[77]      optimizer.step (sum) time: 0.2412889003753662
## epoch[77] training(only) time: 8.825288534164429
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.8682e+00 (1.8682e+00)	Acc@1  66.00 ( 66.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 2.0293e+00 (1.9055e+00)	Acc@1  62.00 ( 66.18)	Acc@5  89.00 ( 88.45)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 2.0039e+00 (1.8841e+00)	Acc@1  69.00 ( 67.00)	Acc@5  86.00 ( 88.52)
Test: [ 30/100]	Time  0.015 ( 0.018)	Loss 2.0703e+00 (1.8918e+00)	Acc@1  61.00 ( 66.61)	Acc@5  85.00 ( 88.10)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 1.4863e+00 (1.8526e+00)	Acc@1  67.00 ( 67.02)	Acc@5  94.00 ( 88.59)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.8652e+00 (1.8765e+00)	Acc@1  71.00 ( 67.22)	Acc@5  91.00 ( 88.29)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.6338e+00 (1.8441e+00)	Acc@1  72.00 ( 67.64)	Acc@5  89.00 ( 88.59)
Test: [ 70/100]	Time  0.017 ( 0.015)	Loss 2.0215e+00 (1.8412e+00)	Acc@1  67.00 ( 67.68)	Acc@5  87.00 ( 88.72)
Test: [ 80/100]	Time  0.019 ( 0.015)	Loss 1.8242e+00 (1.8498e+00)	Acc@1  69.00 ( 67.59)	Acc@5  86.00 ( 88.54)
Test: [ 90/100]	Time  0.020 ( 0.015)	Loss 2.1562e+00 (1.8437e+00)	Acc@1  62.00 ( 67.79)	Acc@5  86.00 ( 88.74)
 * Acc@1 67.850 Acc@5 88.800
### epoch[77] execution time: 10.39209771156311
EPOCH 78
REMOVING: features.module.25.bias
i:   0, name:      features.module.26.weight  changing lr from: 0.001174378644816847   to: 0.001048011765028971
i:   1, name:        features.module.26.bias  changing lr from: 0.001444230753002273   to: 0.001222301690069177
i:   2, name:            classifier.0.weight  changing lr from: 0.001827452887143421   to: 0.001515689579677170
i:   3, name:              classifier.0.bias  changing lr from: 0.002315679992604807   to: 0.001919709512058509
i:   4, name:            classifier.3.weight  changing lr from: 0.002900836598439338   to: 0.002426161331840292
i:   5, name:              classifier.3.bias  changing lr from: 0.003575155702818594   to: 0.003027133827241168
i:   6, name:            classifier.6.weight  changing lr from: 0.004331192313391201   to: 0.003715022084744678
i:   7, name:              classifier.6.bias  changing lr from: 0.005161832367509555   to: 0.004482539783401762



# Switched to train mode...
Epoch: [78][  0/391]	Time  0.175 ( 0.175)	Data  0.155 ( 0.155)	Loss 5.3497e-02 (5.3497e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.015 ( 0.035)	Data  0.001 ( 0.016)	Loss 5.9296e-02 (5.5075e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.016 ( 0.028)	Data  0.001 ( 0.009)	Loss 4.6600e-02 (5.4579e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.023 ( 0.026)	Data  0.004 ( 0.007)	Loss 4.1260e-02 (5.2081e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 ( 99.97)
Epoch: [78][ 40/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.006)	Loss 2.7695e-02 (5.2583e-02)	Acc@1  99.22 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [78][ 50/391]	Time  0.015 ( 0.025)	Data  0.001 ( 0.005)	Loss 6.7566e-02 (5.5335e-02)	Acc@1  96.09 ( 98.25)	Acc@5 100.00 ( 99.97)
Epoch: [78][ 60/391]	Time  0.015 ( 0.024)	Data  0.001 ( 0.004)	Loss 6.5247e-02 (5.3417e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.97)
Epoch: [78][ 70/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.004)	Loss 5.5176e-02 (5.3217e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [78][ 80/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.004)	Loss 2.6718e-02 (5.2575e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [78][ 90/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.004)	Loss 3.8452e-02 (5.2073e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [78][100/391]	Time  0.023 ( 0.023)	Data  0.002 ( 0.003)	Loss 9.1248e-02 (5.2666e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [78][110/391]	Time  0.029 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.1239e-02 (5.2838e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [78][120/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 4.2206e-02 (5.3735e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [78][130/391]	Time  0.032 ( 0.023)	Data  0.002 ( 0.003)	Loss 5.9570e-02 (5.3572e-02)	Acc@1  96.88 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [78][140/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.0386e-02 (5.2887e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [78][150/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.9032e-02 (5.3473e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [78][160/391]	Time  0.023 ( 0.023)	Data  0.002 ( 0.003)	Loss 6.3477e-02 (5.4758e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [78][170/391]	Time  0.019 ( 0.023)	Data  0.002 ( 0.003)	Loss 3.8025e-02 (5.4559e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [78][180/391]	Time  0.023 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.5726e-02 (5.3769e-02)	Acc@1 100.00 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [78][190/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.4717e-02 (5.3752e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.98)
Epoch: [78][200/391]	Time  0.027 ( 0.023)	Data  0.014 ( 0.003)	Loss 4.9896e-02 (5.4301e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 ( 99.98)
Epoch: [78][210/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.6251e-02 (5.4248e-02)	Acc@1 100.00 ( 98.25)	Acc@5 100.00 ( 99.97)
Epoch: [78][220/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.5247e-01 (5.5149e-02)	Acc@1  95.31 ( 98.21)	Acc@5  99.22 ( 99.97)
Epoch: [78][230/391]	Time  0.023 ( 0.023)	Data  0.004 ( 0.003)	Loss 1.6006e-02 (5.4886e-02)	Acc@1 100.00 ( 98.22)	Acc@5 100.00 ( 99.97)
Epoch: [78][240/391]	Time  0.024 ( 0.023)	Data  0.004 ( 0.003)	Loss 4.1443e-02 (5.5644e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [78][250/391]	Time  0.025 ( 0.023)	Data  0.003 ( 0.003)	Loss 6.0760e-02 (5.6128e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [78][260/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.1250e-02 (5.6073e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [78][270/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 5.4413e-02 (5.6080e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [78][280/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 2.9099e-02 (5.5813e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.97)
Epoch: [78][290/391]	Time  0.021 ( 0.022)	Data  0.001 ( 0.003)	Loss 2.7054e-02 (5.6181e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [78][300/391]	Time  0.019 ( 0.022)	Data  0.002 ( 0.003)	Loss 6.7505e-02 (5.5967e-02)	Acc@1  96.88 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [78][310/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 8.7708e-02 (5.6055e-02)	Acc@1  96.88 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [78][320/391]	Time  0.018 ( 0.022)	Data  0.001 ( 0.003)	Loss 7.6660e-02 (5.5915e-02)	Acc@1  96.88 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [78][330/391]	Time  0.022 ( 0.022)	Data  0.001 ( 0.002)	Loss 3.1677e-02 (5.5793e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [78][340/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 5.5054e-02 (5.6186e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [78][350/391]	Time  0.023 ( 0.022)	Data  0.001 ( 0.002)	Loss 4.9255e-02 (5.6454e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [78][360/391]	Time  0.024 ( 0.022)	Data  0.002 ( 0.002)	Loss 1.3542e-02 (5.6320e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [78][370/391]	Time  0.036 ( 0.022)	Data  0.001 ( 0.002)	Loss 2.7039e-02 (5.6175e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.98)
Epoch: [78][380/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 4.9103e-02 (5.6795e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [78][390/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 6.5918e-02 (5.6963e-02)	Acc@1  97.50 ( 98.20)	Acc@5 100.00 ( 99.98)
## e[78] optimizer.zero_grad (sum) time: 0.03221535682678223
## e[78]       loss.backward (sum) time: 1.0749475955963135
## e[78]      optimizer.step (sum) time: 0.21412396430969238
## epoch[78] training(only) time: 8.8475980758667
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.8691e+00 (1.8691e+00)	Acc@1  65.00 ( 65.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.014 ( 0.026)	Loss 2.0508e+00 (1.9244e+00)	Acc@1  62.00 ( 65.91)	Acc@5  90.00 ( 88.27)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.9814e+00 (1.8963e+00)	Acc@1  69.00 ( 67.00)	Acc@5  86.00 ( 88.48)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 2.0820e+00 (1.9011e+00)	Acc@1  62.00 ( 66.81)	Acc@5  85.00 ( 88.13)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4756e+00 (1.8654e+00)	Acc@1  66.00 ( 67.00)	Acc@5  94.00 ( 88.63)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 1.8320e+00 (1.8869e+00)	Acc@1  71.00 ( 67.16)	Acc@5  90.00 ( 88.39)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6562e+00 (1.8528e+00)	Acc@1  72.00 ( 67.59)	Acc@5  88.00 ( 88.66)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 2.0293e+00 (1.8498e+00)	Acc@1  69.00 ( 67.72)	Acc@5  88.00 ( 88.79)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.7822e+00 (1.8582e+00)	Acc@1  69.00 ( 67.54)	Acc@5  86.00 ( 88.62)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 2.2129e+00 (1.8509e+00)	Acc@1  62.00 ( 67.69)	Acc@5  87.00 ( 88.73)
 * Acc@1 67.810 Acc@5 88.760
### epoch[78] execution time: 10.443148136138916
EPOCH 79
i:   0, name:      features.module.26.weight  changing lr from: 0.001048011765028971   to: 0.001000422658185462
i:   1, name:        features.module.26.bias  changing lr from: 0.001222301690069177   to: 0.001076336896837970
i:   2, name:            classifier.0.weight  changing lr from: 0.001515689579677170   to: 0.001277013830918177
i:   3, name:              classifier.0.bias  changing lr from: 0.001919709512058509   to: 0.001593910624522759
i:   4, name:            classifier.3.weight  changing lr from: 0.002426161331840292   to: 0.002018723477351552
i:   5, name:              classifier.3.bias  changing lr from: 0.003027133827241168   to: 0.002543415337721665
i:   6, name:            classifier.6.weight  changing lr from: 0.003715022084744678   to: 0.003160237305279156
i:   7, name:              classifier.6.bias  changing lr from: 0.004482539783401762   to: 0.003861744521364962



# Switched to train mode...
Epoch: [79][  0/391]	Time  0.175 ( 0.175)	Data  0.137 ( 0.137)	Loss 7.5745e-02 (7.5745e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.015 ( 0.035)	Data  0.001 ( 0.014)	Loss 4.1809e-02 (5.6768e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 ( 99.93)
Epoch: [79][ 20/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.008)	Loss 1.2042e-01 (6.2460e-02)	Acc@1  98.44 ( 98.07)	Acc@5  99.22 ( 99.93)
Epoch: [79][ 30/391]	Time  0.016 ( 0.026)	Data  0.001 ( 0.006)	Loss 3.8025e-02 (6.1846e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.95)
Epoch: [79][ 40/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.005)	Loss 6.8176e-02 (5.8694e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 50/391]	Time  0.042 ( 0.024)	Data  0.011 ( 0.005)	Loss 1.9348e-02 (5.8215e-02)	Acc@1 100.00 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [79][ 60/391]	Time  0.036 ( 0.024)	Data  0.001 ( 0.004)	Loss 1.1145e-01 (5.9034e-02)	Acc@1  96.88 ( 98.21)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 70/391]	Time  0.052 ( 0.024)	Data  0.014 ( 0.004)	Loss 7.0496e-02 (5.8961e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 80/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.004)	Loss 2.4338e-02 (5.9317e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.95)
Epoch: [79][ 90/391]	Time  0.028 ( 0.023)	Data  0.001 ( 0.004)	Loss 8.7769e-02 (5.9801e-02)	Acc@1  96.09 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [79][100/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.7150e-02 (5.9695e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [79][110/391]	Time  0.033 ( 0.023)	Data  0.004 ( 0.003)	Loss 1.1864e-02 (5.8502e-02)	Acc@1 100.00 ( 98.22)	Acc@5 100.00 ( 99.96)
Epoch: [79][120/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.003)	Loss 2.0874e-02 (5.7684e-02)	Acc@1 100.00 ( 98.24)	Acc@5 100.00 ( 99.96)
Epoch: [79][130/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 7.2632e-02 (5.7036e-02)	Acc@1  96.88 ( 98.24)	Acc@5 100.00 ( 99.96)
Epoch: [79][140/391]	Time  0.023 ( 0.023)	Data  0.003 ( 0.003)	Loss 3.9856e-02 (5.8231e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.96)
Epoch: [79][150/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 8.1848e-02 (5.7349e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.96)
Epoch: [79][160/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.3375e-02 (5.7042e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.96)
Epoch: [79][170/391]	Time  0.036 ( 0.023)	Data  0.000 ( 0.003)	Loss 7.7759e-02 (5.6607e-02)	Acc@1  98.44 ( 98.28)	Acc@5  99.22 ( 99.96)
Epoch: [79][180/391]	Time  0.014 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.5717e-02 (5.6518e-02)	Acc@1 100.00 ( 98.29)	Acc@5 100.00 ( 99.96)
Epoch: [79][190/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.9663e-02 (5.6898e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.95)
Epoch: [79][200/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.8624e-02 (5.6783e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.95)
Epoch: [79][210/391]	Time  0.019 ( 0.023)	Data  0.001 ( 0.003)	Loss 2.3132e-02 (5.6237e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.95)
Epoch: [79][220/391]	Time  0.015 ( 0.023)	Data  0.002 ( 0.003)	Loss 4.6112e-02 (5.6285e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.95)
Epoch: [79][230/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.7129e-02 (5.6233e-02)	Acc@1  96.88 ( 98.29)	Acc@5 100.00 ( 99.95)
Epoch: [79][240/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 5.3680e-02 (5.6771e-02)	Acc@1  99.22 ( 98.26)	Acc@5 100.00 ( 99.95)
Epoch: [79][250/391]	Time  0.036 ( 0.023)	Data  0.004 ( 0.003)	Loss 2.2736e-02 (5.6524e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.95)
Epoch: [79][260/391]	Time  0.015 ( 0.022)	Data  0.000 ( 0.003)	Loss 9.3689e-02 (5.6618e-02)	Acc@1  96.09 ( 98.26)	Acc@5 100.00 ( 99.95)
Epoch: [79][270/391]	Time  0.033 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.1372e-02 (5.6655e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.95)
Epoch: [79][280/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 3.3539e-02 (5.6788e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [79][290/391]	Time  0.014 ( 0.022)	Data  0.001 ( 0.003)	Loss 7.9422e-03 (5.6682e-02)	Acc@1 100.00 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [79][300/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 6.7505e-02 (5.6382e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.95)
Epoch: [79][310/391]	Time  0.024 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.4821e-02 (5.6463e-02)	Acc@1 100.00 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [79][320/391]	Time  0.020 ( 0.022)	Data  0.001 ( 0.003)	Loss 8.3618e-02 (5.6439e-02)	Acc@1  97.66 ( 98.25)	Acc@5 100.00 ( 99.95)
Epoch: [79][330/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 9.7900e-02 (5.7051e-02)	Acc@1  96.88 ( 98.23)	Acc@5  99.22 ( 99.95)
Epoch: [79][340/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 4.7211e-02 (5.7049e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 ( 99.95)
Epoch: [79][350/391]	Time  0.025 ( 0.022)	Data  0.002 ( 0.003)	Loss 2.3529e-02 (5.6666e-02)	Acc@1  99.22 ( 98.23)	Acc@5 100.00 ( 99.95)
Epoch: [79][360/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 4.3793e-02 (5.6420e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.95)
Epoch: [79][370/391]	Time  0.022 ( 0.022)	Data  0.003 ( 0.002)	Loss 3.0930e-02 (5.6808e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.96)
Epoch: [79][380/391]	Time  0.036 ( 0.022)	Data  0.001 ( 0.002)	Loss 3.4302e-02 (5.6848e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.96)
Epoch: [79][390/391]	Time  0.015 ( 0.022)	Data  0.000 ( 0.002)	Loss 1.0913e-01 (5.7248e-02)	Acc@1  97.50 ( 98.18)	Acc@5 100.00 ( 99.96)
## e[79] optimizer.zero_grad (sum) time: 0.03198695182800293
## e[79]       loss.backward (sum) time: 1.0773534774780273
## e[79]      optimizer.step (sum) time: 0.21853899955749512
## epoch[79] training(only) time: 8.840644121170044
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.9043e+00 (1.9043e+00)	Acc@1  66.00 ( 66.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.012 ( 0.026)	Loss 1.9688e+00 (1.9140e+00)	Acc@1  62.00 ( 65.91)	Acc@5  92.00 ( 88.91)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.9912e+00 (1.8828e+00)	Acc@1  72.00 ( 66.95)	Acc@5  84.00 ( 88.81)
Test: [ 30/100]	Time  0.019 ( 0.018)	Loss 2.0508e+00 (1.8855e+00)	Acc@1  62.00 ( 66.97)	Acc@5  85.00 ( 88.32)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.4492e+00 (1.8526e+00)	Acc@1  69.00 ( 67.29)	Acc@5  93.00 ( 88.68)
Test: [ 50/100]	Time  0.023 ( 0.017)	Loss 1.8818e+00 (1.8802e+00)	Acc@1  68.00 ( 67.39)	Acc@5  90.00 ( 88.35)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.6465e+00 (1.8454e+00)	Acc@1  71.00 ( 67.80)	Acc@5  89.00 ( 88.61)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 1.9844e+00 (1.8451e+00)	Acc@1  69.00 ( 67.90)	Acc@5  89.00 ( 88.69)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 1.8193e+00 (1.8523e+00)	Acc@1  69.00 ( 67.67)	Acc@5  87.00 ( 88.52)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.1406e+00 (1.8448e+00)	Acc@1  64.00 ( 67.90)	Acc@5  89.00 ( 88.78)
 * Acc@1 68.000 Acc@5 88.820
### epoch[79] execution time: 10.467748641967773
EPOCH 80
REMOVING: features.module.26.weight
i:   0, name:        features.module.26.bias  changing lr from: 0.001076336896837970   to: 0.001006559125852396
i:   1, name:            classifier.0.weight  changing lr from: 0.001277013830918177   to: 0.001111778161244153
i:   2, name:              classifier.0.bias  changing lr from: 0.001593910624522759   to: 0.001338749134635491
i:   3, name:            classifier.3.weight  changing lr from: 0.002018723477351552   to: 0.001679087023072117
i:   4, name:              classifier.3.bias  changing lr from: 0.002543415337721665   to: 0.002124648616132118
i:   5, name:            classifier.6.weight  changing lr from: 0.003160237305279156   to: 0.002667558197661273
i:   6, name:              classifier.6.bias  changing lr from: 0.003861744521364962   to: 0.003300227253005377



# Switched to train mode...
Epoch: [80][  0/391]	Time  0.170 ( 0.170)	Data  0.144 ( 0.144)	Loss 1.2598e-01 (1.2598e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.016)	Loss 7.2388e-02 (5.7900e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.015 ( 0.028)	Data  0.001 ( 0.009)	Loss 3.6987e-02 (5.4658e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.007)	Loss 3.3661e-02 (5.4764e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [80][ 40/391]	Time  0.015 ( 0.024)	Data  0.002 ( 0.005)	Loss 2.5681e-02 (5.3737e-02)	Acc@1 100.00 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [80][ 50/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.005)	Loss 6.6345e-02 (5.4379e-02)	Acc@1  96.88 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [80][ 60/391]	Time  0.020 ( 0.024)	Data  0.001 ( 0.004)	Loss 2.0447e-02 (5.5395e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [80][ 70/391]	Time  0.015 ( 0.024)	Data  0.002 ( 0.004)	Loss 2.2903e-02 (5.6483e-02)	Acc@1 100.00 ( 98.23)	Acc@5 100.00 ( 99.99)
Epoch: [80][ 80/391]	Time  0.022 ( 0.023)	Data  0.004 ( 0.004)	Loss 8.8928e-02 (5.6840e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.99)
Epoch: [80][ 90/391]	Time  0.016 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.0293e-02 (5.5989e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 ( 99.99)
Epoch: [80][100/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.4087e-02 (5.6374e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [80][110/391]	Time  0.026 ( 0.023)	Data  0.001 ( 0.003)	Loss 1.7639e-02 (5.6322e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.99)
Epoch: [80][120/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.4434e-02 (5.7339e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [80][130/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.9591e-02 (5.7618e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [80][140/391]	Time  0.015 ( 0.023)	Data  0.000 ( 0.003)	Loss 9.2957e-02 (5.8046e-02)	Acc@1  97.66 ( 98.12)	Acc@5  99.22 ( 99.97)
Epoch: [80][150/391]	Time  0.036 ( 0.023)	Data  0.000 ( 0.003)	Loss 5.2551e-02 (5.7810e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [80][160/391]	Time  0.033 ( 0.023)	Data  0.001 ( 0.003)	Loss 6.3171e-02 (5.7527e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [80][170/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.8300e-02 (5.7585e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [80][180/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 5.6061e-02 (5.7808e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [80][190/391]	Time  0.025 ( 0.023)	Data  0.001 ( 0.003)	Loss 3.7170e-02 (5.7219e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [80][200/391]	Time  0.015 ( 0.023)	Data  0.001 ( 0.003)	Loss 4.0497e-02 (5.6902e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [80][210/391]	Time  0.031 ( 0.023)	Data  0.000 ( 0.003)	Loss 8.6609e-02 (5.7846e-02)	Acc@1  96.09 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [80][220/391]	Time  0.041 ( 0.023)	Data  0.003 ( 0.003)	Loss 3.3997e-02 (5.7525e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [80][230/391]	Time  0.019 ( 0.022)	Data  0.001 ( 0.003)	Loss 3.8452e-02 (5.7214e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [80][240/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 6.1646e-02 (5.7442e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [80][250/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 4.7180e-02 (5.8136e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [80][260/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.003)	Loss 3.0640e-02 (5.8144e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [80][270/391]	Time  0.031 ( 0.022)	Data  0.001 ( 0.002)	Loss 4.7852e-02 (5.8021e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [80][280/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 1.4130e-02 (5.7748e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [80][290/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 2.0676e-02 (5.7585e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [80][300/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 5.1086e-02 (5.7623e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [80][310/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 8.7952e-02 (5.7437e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 ( 99.96)
Epoch: [80][320/391]	Time  0.020 ( 0.022)	Data  0.001 ( 0.002)	Loss 4.2358e-02 (5.7239e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.96)
Epoch: [80][330/391]	Time  0.016 ( 0.022)	Data  0.001 ( 0.002)	Loss 4.5532e-02 (5.7306e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.96)
Epoch: [80][340/391]	Time  0.024 ( 0.022)	Data  0.001 ( 0.002)	Loss 9.2468e-02 (5.7508e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [80][350/391]	Time  0.050 ( 0.022)	Data  0.014 ( 0.002)	Loss 4.2084e-02 (5.7490e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [80][360/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.002)	Loss 4.9248e-03 (5.7333e-02)	Acc@1 100.00 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [80][370/391]	Time  0.030 ( 0.022)	Data  0.000 ( 0.002)	Loss 4.5288e-02 (5.7351e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [80][380/391]	Time  0.030 ( 0.022)	Data  0.000 ( 0.002)	Loss 6.1981e-02 (5.7606e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [80][390/391]	Time  0.013 ( 0.022)	Data  0.001 ( 0.002)	Loss 8.1482e-02 (5.7257e-02)	Acc@1  98.75 ( 98.18)	Acc@5 100.00 ( 99.96)
## e[80] optimizer.zero_grad (sum) time: 0.029433012008666992
## e[80]       loss.backward (sum) time: 1.060396671295166
## e[80]      optimizer.step (sum) time: 0.19190096855163574
## epoch[80] training(only) time: 8.805237770080566
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.9355e+00 (1.9355e+00)	Acc@1  67.00 ( 67.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 2.0000e+00 (1.9214e+00)	Acc@1  63.00 ( 66.09)	Acc@5  90.00 ( 88.00)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.9443e+00 (1.8878e+00)	Acc@1  70.00 ( 67.33)	Acc@5  87.00 ( 88.24)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 2.0762e+00 (1.8910e+00)	Acc@1  62.00 ( 67.13)	Acc@5  85.00 ( 87.97)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.4414e+00 (1.8576e+00)	Acc@1  70.00 ( 67.49)	Acc@5  94.00 ( 88.54)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.8691e+00 (1.8859e+00)	Acc@1  70.00 ( 67.51)	Acc@5  90.00 ( 88.20)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 1.6797e+00 (1.8536e+00)	Acc@1  70.00 ( 67.77)	Acc@5  89.00 ( 88.52)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 2.0352e+00 (1.8520e+00)	Acc@1  69.00 ( 67.97)	Acc@5  88.00 ( 88.63)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.8584e+00 (1.8591e+00)	Acc@1  68.00 ( 67.77)	Acc@5  86.00 ( 88.52)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 2.1953e+00 (1.8534e+00)	Acc@1  63.00 ( 68.01)	Acc@5  88.00 ( 88.70)
 * Acc@1 68.080 Acc@5 88.790
### epoch[80] execution time: 10.398212194442749
EPOCH 81
REMOVING: features.module.26.bias
i:   0, name:            classifier.0.weight  changing lr from: 0.001111778161244153   to: 0.001020226621175445
i:   1, name:              classifier.0.bias  changing lr from: 0.001338749134635491   to: 0.001154589854585918
i:   2, name:            classifier.3.weight  changing lr from: 0.001679087023072117   to: 0.001407722104302599
i:   3, name:              classifier.3.bias  changing lr from: 0.002124648616132118   to: 0.001771394982241680
i:   4, name:            classifier.6.weight  changing lr from: 0.002667558197661273   to: 0.002237624358855161
i:   5, name:              classifier.6.bias  changing lr from: 0.003300227253005377   to: 0.002798694105793036



# Switched to train mode...
Epoch: [81][  0/391]	Time  0.169 ( 0.169)	Data  0.151 ( 0.151)	Loss 7.6843e-02 (7.6843e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.027 ( 0.034)	Data  0.001 ( 0.015)	Loss 4.8065e-02 (5.4269e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.93)
Epoch: [81][ 20/391]	Time  0.014 ( 0.027)	Data  0.001 ( 0.008)	Loss 1.5778e-02 (5.6218e-02)	Acc@1 100.00 ( 98.33)	Acc@5 100.00 ( 99.96)
Epoch: [81][ 30/391]	Time  0.015 ( 0.025)	Data  0.002 ( 0.006)	Loss 5.8319e-02 (5.4913e-02)	Acc@1  97.66 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [81][ 40/391]	Time  0.031 ( 0.024)	Data  0.001 ( 0.005)	Loss 5.1636e-02 (5.9490e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.96)
Epoch: [81][ 50/391]	Time  0.012 ( 0.023)	Data  0.001 ( 0.004)	Loss 2.3315e-02 (5.8386e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [81][ 60/391]	Time  0.032 ( 0.023)	Data  0.001 ( 0.004)	Loss 4.0680e-02 (5.6619e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.96)
Epoch: [81][ 70/391]	Time  0.034 ( 0.022)	Data  0.000 ( 0.004)	Loss 2.7969e-02 (5.5648e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.96)
Epoch: [81][ 80/391]	Time  0.022 ( 0.022)	Data  0.000 ( 0.004)	Loss 2.9785e-02 (5.6272e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.96)
Epoch: [81][ 90/391]	Time  0.020 ( 0.022)	Data  0.000 ( 0.003)	Loss 8.5083e-02 (5.5942e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [81][100/391]	Time  0.020 ( 0.022)	Data  0.001 ( 0.003)	Loss 2.9343e-02 (5.5290e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.97)
Epoch: [81][110/391]	Time  0.035 ( 0.021)	Data  0.001 ( 0.003)	Loss 6.0272e-02 (5.4230e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.97)
Epoch: [81][120/391]	Time  0.032 ( 0.021)	Data  0.001 ( 0.003)	Loss 1.2779e-02 (5.2921e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.97)
Epoch: [81][130/391]	Time  0.027 ( 0.021)	Data  0.001 ( 0.003)	Loss 3.6926e-02 (5.3745e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [81][140/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.003)	Loss 5.3558e-02 (5.2758e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 ( 99.97)
Epoch: [81][150/391]	Time  0.019 ( 0.021)	Data  0.001 ( 0.003)	Loss 7.0038e-03 (5.2322e-02)	Acc@1 100.00 ( 98.39)	Acc@5 100.00 ( 99.97)
Epoch: [81][160/391]	Time  0.023 ( 0.021)	Data  0.004 ( 0.002)	Loss 6.7139e-02 (5.3424e-02)	Acc@1  96.88 ( 98.36)	Acc@5 100.00 ( 99.97)
Epoch: [81][170/391]	Time  0.017 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.6661e-02 (5.4347e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [81][180/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.2837e-02 (5.4367e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [81][190/391]	Time  0.034 ( 0.021)	Data  0.006 ( 0.002)	Loss 1.7761e-02 (5.4216e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [81][200/391]	Time  0.034 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.3867e-01 (5.5375e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [81][210/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.5552e-02 (5.5105e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [81][220/391]	Time  0.030 ( 0.021)	Data  0.000 ( 0.002)	Loss 9.5703e-02 (5.5298e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [81][230/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 9.4116e-02 (5.5305e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [81][240/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.0862e-02 (5.5466e-02)	Acc@1  96.88 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [81][250/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.4969e-02 (5.5784e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [81][260/391]	Time  0.028 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.9307e-02 (5.5392e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [81][270/391]	Time  0.017 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.3427e-02 (5.5176e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [81][280/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.6152e-02 (5.5190e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [81][290/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.1909e-02 (5.5097e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [81][300/391]	Time  0.013 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.0262e-02 (5.5602e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [81][310/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.1910e-02 (5.5188e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [81][320/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.4006e-02 (5.5710e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [81][330/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.9398e-02 (5.5632e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [81][340/391]	Time  0.027 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.6589e-02 (5.5419e-02)	Acc@1  96.09 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [81][350/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.9336e-02 (5.5979e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [81][360/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.7781e-02 (5.5914e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [81][370/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.2390e-01 (5.6254e-02)	Acc@1  96.09 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [81][380/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.7638e-02 (5.6220e-02)	Acc@1  97.66 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [81][390/391]	Time  0.011 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.4149e-02 (5.6016e-02)	Acc@1  98.75 ( 98.28)	Acc@5 100.00 ( 99.98)
## e[81] optimizer.zero_grad (sum) time: 0.02594137191772461
## e[81]       loss.backward (sum) time: 0.2329246997833252
## e[81]      optimizer.step (sum) time: 0.16245412826538086
## epoch[81] training(only) time: 8.1563880443573
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.9297e+00 (1.9297e+00)	Acc@1  64.00 ( 64.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 2.0605e+00 (1.9434e+00)	Acc@1  62.00 ( 66.00)	Acc@5  89.00 ( 87.82)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 2.0293e+00 (1.9055e+00)	Acc@1  70.00 ( 66.71)	Acc@5  87.00 ( 88.33)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 2.1152e+00 (1.9101e+00)	Acc@1  61.00 ( 66.42)	Acc@5  85.00 ( 88.03)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.4932e+00 (1.8730e+00)	Acc@1  67.00 ( 66.73)	Acc@5  94.00 ( 88.59)
Test: [ 50/100]	Time  0.019 ( 0.016)	Loss 1.8154e+00 (1.8961e+00)	Acc@1  72.00 ( 67.02)	Acc@5  90.00 ( 88.29)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.6729e+00 (1.8617e+00)	Acc@1  70.00 ( 67.44)	Acc@5  88.00 ( 88.61)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 2.0137e+00 (1.8579e+00)	Acc@1  69.00 ( 67.56)	Acc@5  88.00 ( 88.69)
Test: [ 80/100]	Time  0.026 ( 0.015)	Loss 1.8018e+00 (1.8651e+00)	Acc@1  68.00 ( 67.43)	Acc@5  86.00 ( 88.58)
Test: [ 90/100]	Time  0.019 ( 0.015)	Loss 2.1543e+00 (1.8583e+00)	Acc@1  64.00 ( 67.65)	Acc@5  85.00 ( 88.74)
 * Acc@1 67.670 Acc@5 88.770
### epoch[81] execution time: 9.76408338546753
EPOCH 82
i:   0, name:            classifier.0.weight  changing lr from: 0.001020226621175445   to: 0.001002494430916256
i:   1, name:              classifier.0.bias  changing lr from: 0.001154589854585918   to: 0.001041696082531406
i:   2, name:            classifier.3.weight  changing lr from: 0.001407722104302599   to: 0.001205004352769378
i:   3, name:              classifier.3.bias  changing lr from: 0.001771394982241680   to: 0.001484127941317164
i:   4, name:            classifier.6.weight  changing lr from: 0.002237624358855161   to: 0.001870993929796415
i:   5, name:              classifier.6.bias  changing lr from: 0.002798694105793036   to: 0.002357775775082015



# Switched to train mode...
Epoch: [82][  0/391]	Time  0.170 ( 0.170)	Data  0.146 ( 0.146)	Loss 1.0596e-01 (1.0596e-01)	Acc@1  97.66 ( 97.66)	Acc@5  99.22 ( 99.22)
Epoch: [82][ 10/391]	Time  0.012 ( 0.033)	Data  0.001 ( 0.014)	Loss 3.0533e-02 (6.2104e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.93)
Epoch: [82][ 20/391]	Time  0.016 ( 0.027)	Data  0.001 ( 0.008)	Loss 2.0752e-02 (6.2074e-02)	Acc@1 100.00 ( 98.10)	Acc@5 100.00 ( 99.96)
Epoch: [82][ 30/391]	Time  0.012 ( 0.024)	Data  0.001 ( 0.006)	Loss 4.5563e-02 (6.0982e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.90)
Epoch: [82][ 40/391]	Time  0.021 ( 0.024)	Data  0.001 ( 0.005)	Loss 4.7180e-02 (6.1324e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 ( 99.90)
Epoch: [82][ 50/391]	Time  0.012 ( 0.023)	Data  0.001 ( 0.004)	Loss 3.7872e-02 (5.7250e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.92)
Epoch: [82][ 60/391]	Time  0.014 ( 0.022)	Data  0.001 ( 0.004)	Loss 7.3120e-02 (5.7294e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.94)
Epoch: [82][ 70/391]	Time  0.013 ( 0.022)	Data  0.002 ( 0.003)	Loss 6.4514e-02 (5.6348e-02)	Acc@1  97.66 ( 98.22)	Acc@5 100.00 ( 99.94)
Epoch: [82][ 80/391]	Time  0.013 ( 0.022)	Data  0.002 ( 0.003)	Loss 4.6936e-02 (5.6461e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.93)
Epoch: [82][ 90/391]	Time  0.019 ( 0.022)	Data  0.001 ( 0.003)	Loss 4.6631e-02 (5.5991e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.94)
Epoch: [82][100/391]	Time  0.013 ( 0.021)	Data  0.001 ( 0.003)	Loss 7.7637e-02 (5.8418e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.95)
Epoch: [82][110/391]	Time  0.018 ( 0.021)	Data  0.001 ( 0.003)	Loss 4.1351e-02 (5.8697e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.94)
Epoch: [82][120/391]	Time  0.029 ( 0.021)	Data  0.000 ( 0.003)	Loss 6.5430e-02 (5.8511e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 ( 99.95)
Epoch: [82][130/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 4.2664e-02 (5.7861e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.95)
Epoch: [82][140/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.003)	Loss 2.9831e-02 (5.6523e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.96)
Epoch: [82][150/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.8269e-02 (5.7475e-02)	Acc@1  99.22 ( 98.24)	Acc@5 100.00 ( 99.95)
Epoch: [82][160/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.1250e-02 (5.7265e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.96)
Epoch: [82][170/391]	Time  0.017 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.2632e-02 (5.7889e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [82][180/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.4506e-02 (5.8003e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.95)
Epoch: [82][190/391]	Time  0.030 ( 0.021)	Data  0.003 ( 0.002)	Loss 9.2834e-02 (5.8110e-02)	Acc@1  96.09 ( 98.15)	Acc@5  99.22 ( 99.95)
Epoch: [82][200/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.3254e-02 (5.7970e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.95)
Epoch: [82][210/391]	Time  0.018 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.4281e-02 (5.8579e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.95)
Epoch: [82][220/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.002)	Loss 2.9160e-02 (5.8949e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [82][230/391]	Time  0.017 ( 0.021)	Data  0.002 ( 0.002)	Loss 5.9906e-02 (5.8767e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [82][240/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.1993e-01 (5.8912e-02)	Acc@1  96.88 ( 98.16)	Acc@5  99.22 ( 99.95)
Epoch: [82][250/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.8491e-02 (5.8890e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [82][260/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.4790e-02 (5.8492e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.95)
Epoch: [82][270/391]	Time  0.011 ( 0.021)	Data  0.000 ( 0.002)	Loss 3.6926e-02 (5.8235e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.95)
Epoch: [82][280/391]	Time  0.029 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.8143e-02 (5.7590e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 ( 99.95)
Epoch: [82][290/391]	Time  0.011 ( 0.021)	Data  0.000 ( 0.002)	Loss 7.9285e-02 (5.7685e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.95)
Epoch: [82][300/391]	Time  0.036 ( 0.021)	Data  0.000 ( 0.002)	Loss 2.9022e-02 (5.8268e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.95)
Epoch: [82][310/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.5635e-02 (5.8790e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [82][320/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.8868e-02 (5.8861e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.95)
Epoch: [82][330/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.002)	Loss 7.8430e-02 (5.9549e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.95)
Epoch: [82][340/391]	Time  0.024 ( 0.021)	Data  0.004 ( 0.002)	Loss 2.4536e-02 (5.9344e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.95)
Epoch: [82][350/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.0272e-02 (5.9246e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 ( 99.95)
Epoch: [82][360/391]	Time  0.018 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.2816e-02 (5.9248e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.95)
Epoch: [82][370/391]	Time  0.017 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.5205e-02 (5.8956e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.95)
Epoch: [82][380/391]	Time  0.013 ( 0.021)	Data  0.001 ( 0.002)	Loss 9.3445e-02 (5.8902e-02)	Acc@1  96.09 ( 98.11)	Acc@5 100.00 ( 99.95)
Epoch: [82][390/391]	Time  0.011 ( 0.020)	Data  0.001 ( 0.002)	Loss 3.8788e-02 (5.8473e-02)	Acc@1  98.75 ( 98.13)	Acc@5 100.00 ( 99.95)
## e[82] optimizer.zero_grad (sum) time: 0.02604842185974121
## e[82]       loss.backward (sum) time: 0.23521828651428223
## e[82]      optimizer.step (sum) time: 0.16221261024475098
## epoch[82] training(only) time: 8.100355863571167
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.8906e+00 (1.8906e+00)	Acc@1  65.00 ( 65.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.023 ( 0.026)	Loss 2.0176e+00 (1.9158e+00)	Acc@1  62.00 ( 66.09)	Acc@5  90.00 ( 88.45)
Test: [ 20/100]	Time  0.017 ( 0.021)	Loss 1.9551e+00 (1.8863e+00)	Acc@1  72.00 ( 67.24)	Acc@5  87.00 ( 88.86)
Test: [ 30/100]	Time  0.015 ( 0.018)	Loss 2.1465e+00 (1.8947e+00)	Acc@1  61.00 ( 66.77)	Acc@5  86.00 ( 88.35)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4688e+00 (1.8572e+00)	Acc@1  67.00 ( 67.12)	Acc@5  94.00 ( 88.93)
Test: [ 50/100]	Time  0.016 ( 0.017)	Loss 1.8896e+00 (1.8812e+00)	Acc@1  70.00 ( 67.22)	Acc@5  90.00 ( 88.61)
Test: [ 60/100]	Time  0.018 ( 0.016)	Loss 1.6758e+00 (1.8479e+00)	Acc@1  70.00 ( 67.61)	Acc@5  89.00 ( 88.82)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 2.0430e+00 (1.8452e+00)	Acc@1  68.00 ( 67.75)	Acc@5  88.00 ( 88.89)
Test: [ 80/100]	Time  0.022 ( 0.016)	Loss 1.8447e+00 (1.8542e+00)	Acc@1  68.00 ( 67.58)	Acc@5  86.00 ( 88.72)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 2.1758e+00 (1.8472e+00)	Acc@1  62.00 ( 67.75)	Acc@5  86.00 ( 88.88)
 * Acc@1 67.890 Acc@5 88.930
### epoch[82] execution time: 9.691593408584595
EPOCH 83
i:   0, name:            classifier.0.weight  changing lr from: 0.001002494430916256   to: 0.001058607780636477
i:   1, name:              classifier.0.bias  changing lr from: 0.001041696082531406   to: 0.001000229226152060
i:   2, name:            classifier.3.weight  changing lr from: 0.001205004352769378   to: 0.001071214376663514
i:   3, name:              classifier.3.bias  changing lr from: 0.001484127941317164   to: 0.001263232549431047
i:   4, name:            classifier.6.weight  changing lr from: 0.001870993929796415   to: 0.001568142870812455
i:   5, name:              classifier.6.bias  changing lr from: 0.002357775775082015   to: 0.001978026730989096



# Switched to train mode...
Epoch: [83][  0/391]	Time  0.174 ( 0.174)	Data  0.156 ( 0.156)	Loss 1.0559e-01 (1.0559e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.016 ( 0.034)	Data  0.001 ( 0.016)	Loss 9.2957e-02 (6.8422e-02)	Acc@1  96.88 ( 97.73)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.012 ( 0.027)	Data  0.001 ( 0.009)	Loss 1.3391e-01 (6.7937e-02)	Acc@1  96.88 ( 97.77)	Acc@5 100.00 ( 99.96)
Epoch: [83][ 30/391]	Time  0.036 ( 0.025)	Data  0.000 ( 0.007)	Loss 7.5195e-02 (6.3587e-02)	Acc@1  96.09 ( 97.91)	Acc@5 100.00 ( 99.97)
Epoch: [83][ 40/391]	Time  0.011 ( 0.023)	Data  0.000 ( 0.006)	Loss 2.1835e-02 (6.2948e-02)	Acc@1  99.22 ( 97.98)	Acc@5 100.00 ( 99.96)
Epoch: [83][ 50/391]	Time  0.029 ( 0.023)	Data  0.001 ( 0.005)	Loss 5.9570e-02 (6.3032e-02)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 ( 99.97)
Epoch: [83][ 60/391]	Time  0.020 ( 0.023)	Data  0.000 ( 0.004)	Loss 7.1838e-02 (6.1871e-02)	Acc@1  96.09 ( 98.07)	Acc@5 100.00 ( 99.96)
Epoch: [83][ 70/391]	Time  0.012 ( 0.022)	Data  0.001 ( 0.004)	Loss 9.3750e-02 (6.0909e-02)	Acc@1  96.09 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [83][ 80/391]	Time  0.012 ( 0.022)	Data  0.001 ( 0.004)	Loss 3.7384e-02 (5.8530e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [83][ 90/391]	Time  0.030 ( 0.022)	Data  0.000 ( 0.003)	Loss 7.3547e-02 (5.7894e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [83][100/391]	Time  0.028 ( 0.022)	Data  0.003 ( 0.003)	Loss 7.2388e-02 (5.8197e-02)	Acc@1  99.22 ( 98.21)	Acc@5  99.22 ( 99.96)
Epoch: [83][110/391]	Time  0.023 ( 0.022)	Data  0.001 ( 0.003)	Loss 7.1838e-02 (5.9336e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [83][120/391]	Time  0.026 ( 0.022)	Data  0.000 ( 0.003)	Loss 2.6169e-02 (5.9453e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [83][130/391]	Time  0.027 ( 0.021)	Data  0.001 ( 0.003)	Loss 3.3691e-02 (5.9172e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [83][140/391]	Time  0.016 ( 0.021)	Data  0.000 ( 0.003)	Loss 6.0608e-02 (5.9562e-02)	Acc@1  96.88 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [83][150/391]	Time  0.023 ( 0.021)	Data  0.001 ( 0.003)	Loss 6.5674e-02 (5.8718e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [83][160/391]	Time  0.035 ( 0.021)	Data  0.001 ( 0.003)	Loss 5.4474e-02 (5.8472e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [83][170/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 9.4604e-02 (5.9143e-02)	Acc@1  98.44 ( 98.17)	Acc@5  99.22 ( 99.97)
Epoch: [83][180/391]	Time  0.015 ( 0.021)	Data  0.004 ( 0.002)	Loss 4.0192e-02 (5.8992e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [83][190/391]	Time  0.025 ( 0.021)	Data  0.002 ( 0.002)	Loss 8.1543e-02 (5.9294e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [83][200/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.9286e-02 (5.9260e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [83][210/391]	Time  0.029 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.0029e-02 (5.8999e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [83][220/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 9.2896e-02 (5.8323e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [83][230/391]	Time  0.035 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.8494e-02 (5.8197e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [83][240/391]	Time  0.011 ( 0.021)	Data  0.000 ( 0.002)	Loss 7.2571e-02 (5.8346e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [83][250/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.9469e-02 (5.8864e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [83][260/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.4167e-02 (5.9233e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [83][270/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.1304e-01 (5.9153e-02)	Acc@1  96.09 ( 98.14)	Acc@5  99.22 ( 99.96)
Epoch: [83][280/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.8845e-02 (5.9052e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [83][290/391]	Time  0.026 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.4585e-02 (5.8927e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [83][300/391]	Time  0.031 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.6233e-02 (5.8915e-02)	Acc@1  96.88 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [83][310/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.4412e-02 (5.9322e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [83][320/391]	Time  0.015 ( 0.021)	Data  0.001 ( 0.002)	Loss 9.1125e-02 (5.9572e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [83][330/391]	Time  0.019 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.7914e-02 (5.9598e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [83][340/391]	Time  0.019 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.5918e-02 (5.9552e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [83][350/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.8745e-02 (5.9312e-02)	Acc@1  98.44 ( 98.12)	Acc@5  99.22 ( 99.96)
Epoch: [83][360/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.2275e-02 (5.9354e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [83][370/391]	Time  0.029 ( 0.021)	Data  0.005 ( 0.002)	Loss 3.3936e-02 (5.9360e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [83][380/391]	Time  0.024 ( 0.021)	Data  0.003 ( 0.002)	Loss 5.1208e-02 (5.9216e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [83][390/391]	Time  0.011 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.9225e-02 (5.9265e-02)	Acc@1  96.25 ( 98.13)	Acc@5 100.00 ( 99.96)
## e[83] optimizer.zero_grad (sum) time: 0.026818513870239258
## e[83]       loss.backward (sum) time: 0.23477935791015625
## e[83]      optimizer.step (sum) time: 0.16495418548583984
## epoch[83] training(only) time: 8.119580030441284
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.8838e+00 (1.8838e+00)	Acc@1  67.00 ( 67.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 2.0254e+00 (1.9225e+00)	Acc@1  62.00 ( 66.00)	Acc@5  89.00 ( 88.09)
Test: [ 20/100]	Time  0.019 ( 0.020)	Loss 1.9482e+00 (1.8871e+00)	Acc@1  71.00 ( 67.24)	Acc@5  88.00 ( 88.62)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.1016e+00 (1.8961e+00)	Acc@1  61.00 ( 67.16)	Acc@5  85.00 ( 88.13)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4395e+00 (1.8598e+00)	Acc@1  66.00 ( 67.29)	Acc@5  94.00 ( 88.73)
Test: [ 50/100]	Time  0.016 ( 0.017)	Loss 1.8584e+00 (1.8848e+00)	Acc@1  70.00 ( 67.43)	Acc@5  90.00 ( 88.41)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.6113e+00 (1.8504e+00)	Acc@1  71.00 ( 67.90)	Acc@5  89.00 ( 88.69)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 2.0488e+00 (1.8490e+00)	Acc@1  68.00 ( 67.96)	Acc@5  89.00 ( 88.80)
Test: [ 80/100]	Time  0.021 ( 0.016)	Loss 1.8232e+00 (1.8589e+00)	Acc@1  68.00 ( 67.75)	Acc@5  86.00 ( 88.64)
Test: [ 90/100]	Time  0.019 ( 0.015)	Loss 2.1836e+00 (1.8519e+00)	Acc@1  62.00 ( 67.93)	Acc@5  87.00 ( 88.86)
 * Acc@1 68.030 Acc@5 88.890
### epoch[83] execution time: 9.735241889953613
EPOCH 84
i:   0, name:            classifier.0.weight  changing lr from: 0.001058607780636477   to: 0.001188483791789196
i:   1, name:              classifier.0.bias  changing lr from: 0.001000229226152060   to: 0.001030248571881186
i:   2, name:            classifier.3.weight  changing lr from: 0.001071214376663514   to: 0.001006537372214227
i:   3, name:              classifier.3.bias  changing lr from: 0.001263232549431047   to: 0.001109004897327875
i:   4, name:            classifier.6.weight  changing lr from: 0.001568142870812455   to: 0.001329464343729952
i:   5, name:              classifier.6.bias  changing lr from: 0.001978026730989096   to: 0.001659924521128335



# Switched to train mode...
Epoch: [84][  0/391]	Time  0.172 ( 0.172)	Data  0.151 ( 0.151)	Loss 6.0455e-02 (6.0455e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.034 ( 0.033)	Data  0.006 ( 0.016)	Loss 5.6885e-02 (7.0307e-02)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 ( 99.93)
Epoch: [84][ 20/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.009)	Loss 6.0150e-02 (6.4418e-02)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.96)
Epoch: [84][ 30/391]	Time  0.012 ( 0.025)	Data  0.001 ( 0.006)	Loss 7.6355e-02 (6.2476e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [84][ 40/391]	Time  0.019 ( 0.024)	Data  0.001 ( 0.005)	Loss 1.1774e-01 (6.5663e-02)	Acc@1  95.31 ( 97.75)	Acc@5  99.22 ( 99.94)
Epoch: [84][ 50/391]	Time  0.019 ( 0.023)	Data  0.004 ( 0.004)	Loss 4.4434e-02 (6.3531e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.94)
Epoch: [84][ 60/391]	Time  0.015 ( 0.022)	Data  0.001 ( 0.004)	Loss 5.5145e-02 (6.2878e-02)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 ( 99.95)
Epoch: [84][ 70/391]	Time  0.039 ( 0.022)	Data  0.001 ( 0.004)	Loss 3.8055e-02 (6.1483e-02)	Acc@1  98.44 ( 97.90)	Acc@5 100.00 ( 99.94)
Epoch: [84][ 80/391]	Time  0.013 ( 0.022)	Data  0.001 ( 0.003)	Loss 6.1981e-02 (6.1585e-02)	Acc@1  97.66 ( 97.96)	Acc@5  99.22 ( 99.93)
Epoch: [84][ 90/391]	Time  0.035 ( 0.022)	Data  0.001 ( 0.003)	Loss 6.6528e-02 (6.2284e-02)	Acc@1  97.66 ( 97.90)	Acc@5 100.00 ( 99.93)
Epoch: [84][100/391]	Time  0.016 ( 0.022)	Data  0.001 ( 0.003)	Loss 7.2815e-02 (6.1856e-02)	Acc@1  96.88 ( 97.93)	Acc@5 100.00 ( 99.93)
Epoch: [84][110/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.003)	Loss 5.2826e-02 (6.0325e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.94)
Epoch: [84][120/391]	Time  0.011 ( 0.021)	Data  0.000 ( 0.003)	Loss 3.8391e-02 (6.0354e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 ( 99.94)
Epoch: [84][130/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.003)	Loss 2.5574e-02 (5.9602e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [84][140/391]	Time  0.026 ( 0.021)	Data  0.000 ( 0.003)	Loss 7.7454e-02 (5.8800e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.94)
Epoch: [84][150/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.003)	Loss 5.0812e-02 (5.9585e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [84][160/391]	Time  0.029 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.2654e-02 (5.8943e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.94)
Epoch: [84][170/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.0974e-02 (5.8967e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.95)
Epoch: [84][180/391]	Time  0.013 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.8247e-02 (5.9405e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.95)
Epoch: [84][190/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.1060e-01 (6.0139e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.95)
Epoch: [84][200/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 9.1553e-02 (6.0446e-02)	Acc@1  95.31 ( 98.07)	Acc@5 100.00 ( 99.95)
Epoch: [84][210/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.002)	Loss 2.8519e-02 (6.0791e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 ( 99.95)
Epoch: [84][220/391]	Time  0.019 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.0679e-02 (6.0758e-02)	Acc@1  97.66 ( 98.07)	Acc@5 100.00 ( 99.95)
Epoch: [84][230/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.6906e-02 (6.0614e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.95)
Epoch: [84][240/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.5787e-02 (6.0277e-02)	Acc@1 100.00 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [84][250/391]	Time  0.011 ( 0.021)	Data  0.000 ( 0.002)	Loss 3.2288e-02 (6.0219e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.95)
Epoch: [84][260/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.3087e-02 (5.9711e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 ( 99.95)
Epoch: [84][270/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.8361e-02 (5.9321e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.95)
Epoch: [84][280/391]	Time  0.036 ( 0.021)	Data  0.005 ( 0.002)	Loss 5.1910e-02 (5.8922e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.95)
Epoch: [84][290/391]	Time  0.026 ( 0.021)	Data  0.001 ( 0.002)	Loss 9.3323e-02 (5.8688e-02)	Acc@1  97.66 ( 98.16)	Acc@5  99.22 ( 99.95)
Epoch: [84][300/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.5126e-02 (5.8128e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.95)
Epoch: [84][310/391]	Time  0.017 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.9510e-02 (5.7612e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.95)
Epoch: [84][320/391]	Time  0.026 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.8091e-01 (5.8612e-02)	Acc@1  92.97 ( 98.15)	Acc@5 100.00 ( 99.95)
Epoch: [84][330/391]	Time  0.027 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.3669e-02 (5.8208e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.95)
Epoch: [84][340/391]	Time  0.030 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.7354e-02 (5.8197e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.95)
Epoch: [84][350/391]	Time  0.026 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.5553e-02 (5.8029e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [84][360/391]	Time  0.025 ( 0.021)	Data  0.002 ( 0.002)	Loss 2.6398e-02 (5.7892e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 ( 99.95)
Epoch: [84][370/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.5522e-02 (5.8051e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [84][380/391]	Time  0.018 ( 0.021)	Data  0.000 ( 0.002)	Loss 8.4167e-02 (5.8283e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [84][390/391]	Time  0.011 ( 0.021)	Data  0.000 ( 0.002)	Loss 8.9966e-02 (5.8633e-02)	Acc@1  98.75 ( 98.15)	Acc@5 100.00 ( 99.95)
## e[84] optimizer.zero_grad (sum) time: 0.026119470596313477
## e[84]       loss.backward (sum) time: 0.23571205139160156
## e[84]      optimizer.step (sum) time: 0.16371488571166992
## epoch[84] training(only) time: 8.109081029891968
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.8887e+00 (1.8887e+00)	Acc@1  66.00 ( 66.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.018 ( 0.026)	Loss 2.0117e+00 (1.9197e+00)	Acc@1  62.00 ( 66.55)	Acc@5  89.00 ( 88.09)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 2.0195e+00 (1.8895e+00)	Acc@1  70.00 ( 67.24)	Acc@5  87.00 ( 88.52)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.0176e+00 (1.8906e+00)	Acc@1  62.00 ( 67.16)	Acc@5  85.00 ( 88.23)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 1.4766e+00 (1.8558e+00)	Acc@1  69.00 ( 67.44)	Acc@5  94.00 ( 88.73)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.8604e+00 (1.8799e+00)	Acc@1  71.00 ( 67.59)	Acc@5  91.00 ( 88.31)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.6758e+00 (1.8483e+00)	Acc@1  71.00 ( 67.90)	Acc@5  89.00 ( 88.61)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 2.0156e+00 (1.8467e+00)	Acc@1  67.00 ( 67.99)	Acc@5  87.00 ( 88.66)
Test: [ 80/100]	Time  0.014 ( 0.016)	Loss 1.8018e+00 (1.8562e+00)	Acc@1  68.00 ( 67.75)	Acc@5  88.00 ( 88.51)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.2031e+00 (1.8494e+00)	Acc@1  64.00 ( 67.93)	Acc@5  86.00 ( 88.66)
 * Acc@1 68.030 Acc@5 88.720
### epoch[84] execution time: 9.69640326499939
EPOCH 85
i:   0, name:            classifier.0.weight  changing lr from: 0.001188483791789196   to: 0.001391930639521071
i:   1, name:              classifier.0.bias  changing lr from: 0.001030248571881186   to: 0.001131711200141710
i:   2, name:            classifier.3.weight  changing lr from: 0.001006537372214227   to: 0.001011062867334505
i:   3, name:              classifier.3.bias  changing lr from: 0.001109004897327875   to: 0.001021651713540777
i:   4, name:            classifier.6.weight  changing lr from: 0.001329464343729952   to: 0.001155268201471354
i:   5, name:              classifier.6.bias  changing lr from: 0.001659924521128335   to: 0.001403869170078353



# Switched to train mode...
Epoch: [85][  0/391]	Time  0.174 ( 0.174)	Data  0.156 ( 0.156)	Loss 8.6304e-02 (8.6304e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.012 ( 0.034)	Data  0.001 ( 0.017)	Loss 4.7394e-02 (7.4245e-02)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.93)
Epoch: [85][ 20/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.010)	Loss 4.7729e-02 (6.7199e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.89)
Epoch: [85][ 30/391]	Time  0.025 ( 0.025)	Data  0.001 ( 0.007)	Loss 6.1768e-02 (6.2629e-02)	Acc@1  97.66 ( 97.98)	Acc@5 100.00 ( 99.90)
Epoch: [85][ 40/391]	Time  0.028 ( 0.024)	Data  0.001 ( 0.006)	Loss 5.2277e-02 (5.9078e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 ( 99.92)
Epoch: [85][ 50/391]	Time  0.014 ( 0.023)	Data  0.001 ( 0.005)	Loss 3.9673e-02 (5.7893e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [85][ 60/391]	Time  0.012 ( 0.022)	Data  0.001 ( 0.004)	Loss 1.3696e-01 (6.0627e-02)	Acc@1  96.09 ( 98.00)	Acc@5 100.00 ( 99.95)
Epoch: [85][ 70/391]	Time  0.018 ( 0.022)	Data  0.001 ( 0.004)	Loss 6.7261e-02 (6.0204e-02)	Acc@1  96.09 ( 97.98)	Acc@5 100.00 ( 99.96)
Epoch: [85][ 80/391]	Time  0.024 ( 0.022)	Data  0.001 ( 0.004)	Loss 8.0566e-02 (6.1540e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.96)
Epoch: [85][ 90/391]	Time  0.024 ( 0.022)	Data  0.002 ( 0.003)	Loss 7.6904e-02 (6.0893e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.96)
Epoch: [85][100/391]	Time  0.016 ( 0.022)	Data  0.002 ( 0.003)	Loss 4.4922e-02 (6.0036e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 ( 99.96)
Epoch: [85][110/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 4.5593e-02 (5.9112e-02)	Acc@1  97.66 ( 98.04)	Acc@5 100.00 ( 99.96)
Epoch: [85][120/391]	Time  0.033 ( 0.021)	Data  0.000 ( 0.003)	Loss 5.8655e-02 (5.8072e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [85][130/391]	Time  0.020 ( 0.021)	Data  0.000 ( 0.003)	Loss 4.8767e-02 (5.8163e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [85][140/391]	Time  0.017 ( 0.021)	Data  0.001 ( 0.003)	Loss 3.5400e-02 (5.8382e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [85][150/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.003)	Loss 7.7393e-02 (5.7532e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.96)
Epoch: [85][160/391]	Time  0.016 ( 0.021)	Data  0.001 ( 0.003)	Loss 1.2000e-01 (5.8197e-02)	Acc@1  96.09 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [85][170/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.003)	Loss 5.0293e-02 (5.8413e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [85][180/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.002)	Loss 5.3711e-02 (5.8672e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.95)
Epoch: [85][190/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.003)	Loss 9.6130e-02 (5.8059e-02)	Acc@1  96.09 ( 98.14)	Acc@5 100.00 ( 99.95)
Epoch: [85][200/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.1401e-02 (5.8500e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.95)
Epoch: [85][210/391]	Time  0.015 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.4200e-02 (5.8029e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [85][220/391]	Time  0.031 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.2998e-02 (5.8093e-02)	Acc@1  97.66 ( 98.11)	Acc@5  99.22 ( 99.95)
Epoch: [85][230/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.3905e-02 (5.7584e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.95)
Epoch: [85][240/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 9.4788e-02 (5.7671e-02)	Acc@1  95.31 ( 98.14)	Acc@5 100.00 ( 99.95)
Epoch: [85][250/391]	Time  0.036 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.9368e-02 (5.8003e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [85][260/391]	Time  0.020 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.7283e-02 (5.8212e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [85][270/391]	Time  0.015 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.6904e-02 (5.8057e-02)	Acc@1  98.44 ( 98.13)	Acc@5  99.22 ( 99.96)
Epoch: [85][280/391]	Time  0.016 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.7322e-02 (5.7969e-02)	Acc@1  96.88 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [85][290/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.6509e-02 (5.8279e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [85][300/391]	Time  0.032 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.0106e-02 (5.8557e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.95)
Epoch: [85][310/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.0577e-02 (5.8560e-02)	Acc@1  96.88 ( 98.13)	Acc@5 100.00 ( 99.95)
Epoch: [85][320/391]	Time  0.023 ( 0.021)	Data  0.002 ( 0.002)	Loss 2.6169e-02 (5.8239e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.95)
Epoch: [85][330/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.5319e-02 (5.8194e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [85][340/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.4979e-02 (5.7793e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [85][350/391]	Time  0.020 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.4399e-02 (5.7450e-02)	Acc@1 100.00 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [85][360/391]	Time  0.020 ( 0.021)	Data  0.000 ( 0.002)	Loss 1.0004e-01 (5.8014e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [85][370/391]	Time  0.013 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.6596e-02 (5.7951e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [85][380/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.3508e-02 (5.7827e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [85][390/391]	Time  0.011 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.8279e-02 (5.7942e-02)	Acc@1  98.75 ( 98.15)	Acc@5 100.00 ( 99.96)
## e[85] optimizer.zero_grad (sum) time: 0.025890111923217773
## e[85]       loss.backward (sum) time: 0.23582243919372559
## e[85]      optimizer.step (sum) time: 0.16191720962524414
## epoch[85] training(only) time: 8.132009267807007
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.9043e+00 (1.9043e+00)	Acc@1  67.00 ( 67.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.014 ( 0.027)	Loss 2.0039e+00 (1.9134e+00)	Acc@1  62.00 ( 66.55)	Acc@5  90.00 ( 88.36)
Test: [ 20/100]	Time  0.011 ( 0.021)	Loss 1.9658e+00 (1.8786e+00)	Acc@1  71.00 ( 67.62)	Acc@5  85.00 ( 88.52)
Test: [ 30/100]	Time  0.010 ( 0.019)	Loss 2.0645e+00 (1.8820e+00)	Acc@1  62.00 ( 67.10)	Acc@5  85.00 ( 88.06)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4580e+00 (1.8473e+00)	Acc@1  66.00 ( 67.22)	Acc@5  94.00 ( 88.59)
Test: [ 50/100]	Time  0.014 ( 0.017)	Loss 1.8340e+00 (1.8734e+00)	Acc@1  70.00 ( 67.25)	Acc@5  90.00 ( 88.24)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 1.6377e+00 (1.8391e+00)	Acc@1  70.00 ( 67.66)	Acc@5  88.00 ( 88.51)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 2.0078e+00 (1.8360e+00)	Acc@1  66.00 ( 67.79)	Acc@5  88.00 ( 88.65)
Test: [ 80/100]	Time  0.016 ( 0.016)	Loss 1.8135e+00 (1.8432e+00)	Acc@1  67.00 ( 67.64)	Acc@5  87.00 ( 88.56)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.1270e+00 (1.8373e+00)	Acc@1  64.00 ( 67.86)	Acc@5  87.00 ( 88.77)
 * Acc@1 67.970 Acc@5 88.820
### epoch[85] execution time: 9.740683555603027
EPOCH 86
i:   0, name:            classifier.0.weight  changing lr from: 0.001391930639521071   to: 0.001668647835994613
i:   1, name:              classifier.0.bias  changing lr from: 0.001131711200141710   to: 0.001304472046709570
i:   2, name:            classifier.3.weight  changing lr from: 0.001011062867334505   to: 0.001084784597693827
i:   3, name:              classifier.3.bias  changing lr from: 0.001021651713540777   to: 0.001001290087290157
i:   4, name:            classifier.6.weight  changing lr from: 0.001155268201471354   to: 0.001045780585803229
i:   5, name:              classifier.6.bias  changing lr from: 0.001403869170078353   to: 0.001210182676337539



# Switched to train mode...
Epoch: [86][  0/391]	Time  0.178 ( 0.178)	Data  0.157 ( 0.157)	Loss 2.0081e-02 (2.0081e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.020 ( 0.033)	Data  0.001 ( 0.016)	Loss 3.8483e-02 (6.4359e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.93)
Epoch: [86][ 20/391]	Time  0.012 ( 0.027)	Data  0.001 ( 0.009)	Loss 2.1515e-02 (5.3106e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.96)
Epoch: [86][ 30/391]	Time  0.023 ( 0.025)	Data  0.001 ( 0.006)	Loss 4.4373e-02 (5.7041e-02)	Acc@1  98.44 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [86][ 40/391]	Time  0.012 ( 0.024)	Data  0.001 ( 0.005)	Loss 4.7577e-02 (5.6795e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.98)
Epoch: [86][ 50/391]	Time  0.023 ( 0.023)	Data  0.001 ( 0.004)	Loss 8.1848e-02 (6.0330e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.95)
Epoch: [86][ 60/391]	Time  0.014 ( 0.022)	Data  0.001 ( 0.004)	Loss 2.4216e-02 (6.2536e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 ( 99.95)
Epoch: [86][ 70/391]	Time  0.026 ( 0.022)	Data  0.001 ( 0.004)	Loss 7.2876e-02 (6.3392e-02)	Acc@1  96.88 ( 98.06)	Acc@5 100.00 ( 99.96)
Epoch: [86][ 80/391]	Time  0.012 ( 0.022)	Data  0.001 ( 0.003)	Loss 2.2095e-02 (6.1326e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.94)
Epoch: [86][ 90/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.003)	Loss 2.4048e-02 (6.0635e-02)	Acc@1 100.00 ( 98.18)	Acc@5 100.00 ( 99.95)
Epoch: [86][100/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.003)	Loss 5.4962e-02 (6.1584e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.94)
Epoch: [86][110/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.003)	Loss 6.2134e-02 (6.1697e-02)	Acc@1  97.66 ( 98.13)	Acc@5  99.22 ( 99.92)
Epoch: [86][120/391]	Time  0.013 ( 0.021)	Data  0.001 ( 0.003)	Loss 5.6122e-02 (6.1395e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.92)
Epoch: [86][130/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.003)	Loss 6.0211e-02 (6.0629e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.93)
Epoch: [86][140/391]	Time  0.040 ( 0.021)	Data  0.011 ( 0.003)	Loss 5.3131e-02 (6.1642e-02)	Acc@1  98.44 ( 98.07)	Acc@5 100.00 ( 99.93)
Epoch: [86][150/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 2.5116e-02 (6.2190e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 ( 99.91)
Epoch: [86][160/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 6.3904e-02 (6.1441e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.92)
Epoch: [86][170/391]	Time  0.036 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.4063e-02 (6.1706e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.92)
Epoch: [86][180/391]	Time  0.026 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.1157e-01 (6.2117e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.93)
Epoch: [86][190/391]	Time  0.018 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.0801e-02 (6.1925e-02)	Acc@1  98.44 ( 98.11)	Acc@5  99.22 ( 99.93)
Epoch: [86][200/391]	Time  0.028 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.2458e-02 (6.2441e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 ( 99.93)
Epoch: [86][210/391]	Time  0.033 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.3416e-02 (6.1892e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.93)
Epoch: [86][220/391]	Time  0.022 ( 0.021)	Data  0.004 ( 0.002)	Loss 7.4219e-02 (6.1144e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.93)
Epoch: [86][230/391]	Time  0.016 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.0559e-01 (6.1259e-02)	Acc@1  94.53 ( 98.13)	Acc@5 100.00 ( 99.93)
Epoch: [86][240/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.6274e-02 (6.1189e-02)	Acc@1  99.22 ( 98.12)	Acc@5  99.22 ( 99.93)
Epoch: [86][250/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.3232e-02 (6.1218e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.93)
Epoch: [86][260/391]	Time  0.026 ( 0.021)	Data  0.004 ( 0.002)	Loss 3.6560e-02 (6.1338e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.93)
Epoch: [86][270/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.5980e-02 (6.1070e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.93)
Epoch: [86][280/391]	Time  0.027 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.9224e-02 (6.1178e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [86][290/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.9449e-02 (6.0989e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [86][300/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.4830e-02 (6.1027e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [86][310/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.5023e-02 (6.1495e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 ( 99.93)
Epoch: [86][320/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.7566e-02 (6.1325e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 ( 99.93)
Epoch: [86][330/391]	Time  0.025 ( 0.021)	Data  0.004 ( 0.002)	Loss 1.0791e-01 (6.1002e-02)	Acc@1  96.09 ( 98.09)	Acc@5  99.22 ( 99.93)
Epoch: [86][340/391]	Time  0.030 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.6406e-02 (6.0813e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [86][350/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.6548e-02 (6.1006e-02)	Acc@1  96.09 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [86][360/391]	Time  0.026 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.3020e-02 (6.0803e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [86][370/391]	Time  0.027 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.8777e-02 (6.0960e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.94)
Epoch: [86][380/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.3135e-01 (6.0877e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 ( 99.94)
Epoch: [86][390/391]	Time  0.011 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.8542e-02 (6.1101e-02)	Acc@1  97.50 ( 98.09)	Acc@5 100.00 ( 99.94)
## e[86] optimizer.zero_grad (sum) time: 0.025715351104736328
## e[86]       loss.backward (sum) time: 0.23077774047851562
## e[86]      optimizer.step (sum) time: 0.16059255599975586
## epoch[86] training(only) time: 8.11262559890747
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.9336e+00 (1.9336e+00)	Acc@1  66.00 ( 66.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.022 ( 0.025)	Loss 2.0020e+00 (1.9202e+00)	Acc@1  62.00 ( 65.73)	Acc@5  90.00 ( 88.27)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.9932e+00 (1.8923e+00)	Acc@1  70.00 ( 66.81)	Acc@5  87.00 ( 88.43)
Test: [ 30/100]	Time  0.018 ( 0.018)	Loss 2.1816e+00 (1.9069e+00)	Acc@1  62.00 ( 66.45)	Acc@5  85.00 ( 88.10)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.5273e+00 (1.8684e+00)	Acc@1  67.00 ( 66.68)	Acc@5  94.00 ( 88.66)
Test: [ 50/100]	Time  0.018 ( 0.016)	Loss 1.8271e+00 (1.8910e+00)	Acc@1  70.00 ( 67.04)	Acc@5  91.00 ( 88.33)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6055e+00 (1.8556e+00)	Acc@1  72.00 ( 67.49)	Acc@5  88.00 ( 88.64)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 2.0098e+00 (1.8511e+00)	Acc@1  70.00 ( 67.68)	Acc@5  88.00 ( 88.77)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8311e+00 (1.8600e+00)	Acc@1  68.00 ( 67.59)	Acc@5  88.00 ( 88.64)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.1172e+00 (1.8537e+00)	Acc@1  63.00 ( 67.76)	Acc@5  87.00 ( 88.82)
 * Acc@1 67.780 Acc@5 88.860
### epoch[86] execution time: 9.702263116836548
EPOCH 87
i:   0, name:            classifier.0.weight  changing lr from: 0.001668647835994613   to: 0.002018226674204087
i:   1, name:              classifier.0.bias  changing lr from: 0.001304472046709570   to: 0.001548284110116498
i:   2, name:            classifier.3.weight  changing lr from: 0.001084784597693827   to: 0.001227600515389420
i:   3, name:              classifier.3.bias  changing lr from: 0.001001290087290157   to: 0.001047947311535923
i:   4, name:            classifier.6.weight  changing lr from: 0.001045780585803229   to: 0.001001143633758599
i:   5, name:              classifier.6.bias  changing lr from: 0.001210182676337539   to: 0.001079108607399736



# Switched to train mode...
Epoch: [87][  0/391]	Time  0.168 ( 0.168)	Data  0.146 ( 0.146)	Loss 1.0248e-01 (1.0248e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.012 ( 0.031)	Data  0.001 ( 0.015)	Loss 2.1393e-02 (5.7397e-02)	Acc@1  98.44 ( 98.22)	Acc@5 100.00 ( 99.93)
Epoch: [87][ 20/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.009)	Loss 5.4138e-02 (5.4829e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.93)
Epoch: [87][ 30/391]	Time  0.027 ( 0.024)	Data  0.002 ( 0.006)	Loss 4.8035e-02 (5.6781e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.95)
Epoch: [87][ 40/391]	Time  0.017 ( 0.023)	Data  0.001 ( 0.005)	Loss 7.6050e-02 (5.7436e-02)	Acc@1  96.09 ( 98.11)	Acc@5 100.00 ( 99.94)
Epoch: [87][ 50/391]	Time  0.011 ( 0.022)	Data  0.000 ( 0.004)	Loss 5.4413e-02 (5.5764e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.95)
Epoch: [87][ 60/391]	Time  0.025 ( 0.022)	Data  0.001 ( 0.004)	Loss 2.6276e-02 (5.6026e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.96)
Epoch: [87][ 70/391]	Time  0.012 ( 0.022)	Data  0.001 ( 0.004)	Loss 3.2349e-02 (5.6089e-02)	Acc@1  99.22 ( 98.15)	Acc@5 100.00 ( 99.94)
Epoch: [87][ 80/391]	Time  0.029 ( 0.022)	Data  0.000 ( 0.004)	Loss 6.7261e-02 (5.6646e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.95)
Epoch: [87][ 90/391]	Time  0.012 ( 0.022)	Data  0.001 ( 0.003)	Loss 1.6586e-02 (5.5055e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 ( 99.96)
Epoch: [87][100/391]	Time  0.015 ( 0.021)	Data  0.001 ( 0.003)	Loss 3.4973e-02 (5.5610e-02)	Acc@1  98.44 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [87][110/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 7.3120e-02 (5.7123e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [87][120/391]	Time  0.019 ( 0.021)	Data  0.001 ( 0.003)	Loss 3.9062e-02 (5.6192e-02)	Acc@1  98.44 ( 98.19)	Acc@5 100.00 ( 99.96)
Epoch: [87][130/391]	Time  0.017 ( 0.021)	Data  0.001 ( 0.003)	Loss 5.1331e-02 (5.6090e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 ( 99.96)
Epoch: [87][140/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 3.1403e-02 (5.6601e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.96)
Epoch: [87][150/391]	Time  0.032 ( 0.021)	Data  0.000 ( 0.003)	Loss 8.4473e-02 (5.6792e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [87][160/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 6.4880e-02 (5.7391e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [87][170/391]	Time  0.023 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.5686e-02 (5.7668e-02)	Acc@1 100.00 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [87][180/391]	Time  0.016 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.5735e-02 (5.8174e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.96)
Epoch: [87][190/391]	Time  0.027 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.9877e-02 (5.7905e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [87][200/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.8336e-02 (5.8146e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 ( 99.96)
Epoch: [87][210/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.6591e-02 (5.7654e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [87][220/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.1228e-02 (5.7168e-02)	Acc@1  96.09 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [87][230/391]	Time  0.034 ( 0.021)	Data  0.000 ( 0.002)	Loss 5.6305e-02 (5.6985e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [87][240/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.8849e-02 (5.7369e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [87][250/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.6387e-02 (5.7451e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [87][260/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.6520e-02 (5.7037e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [87][270/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.4961e-02 (5.7380e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [87][280/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.8799e-01 (5.7967e-02)	Acc@1  95.31 ( 98.10)	Acc@5  99.22 ( 99.96)
Epoch: [87][290/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 8.6243e-02 (5.8045e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.96)
Epoch: [87][300/391]	Time  0.018 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.2266e-02 (5.8396e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.96)
Epoch: [87][310/391]	Time  0.016 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.4829e-02 (5.7826e-02)	Acc@1  99.22 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [87][320/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.5674e-02 (5.7815e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [87][330/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.8025e-02 (5.7558e-02)	Acc@1  98.44 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [87][340/391]	Time  0.028 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.6213e-02 (5.7514e-02)	Acc@1  98.44 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [87][350/391]	Time  0.016 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.6352e-02 (5.7155e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [87][360/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.8369e-02 (5.6956e-02)	Acc@1  96.88 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [87][370/391]	Time  0.015 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.6396e-02 (5.6425e-02)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 ( 99.96)
Epoch: [87][380/391]	Time  0.022 ( 0.021)	Data  0.000 ( 0.002)	Loss 5.9814e-02 (5.6787e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.96)
Epoch: [87][390/391]	Time  0.012 ( 0.020)	Data  0.001 ( 0.002)	Loss 6.8420e-02 (5.7061e-02)	Acc@1  97.50 ( 98.13)	Acc@5 100.00 ( 99.96)
## e[87] optimizer.zero_grad (sum) time: 0.025865554809570312
## e[87]       loss.backward (sum) time: 0.23275089263916016
## e[87]      optimizer.step (sum) time: 0.16235041618347168
## epoch[87] training(only) time: 8.103002309799194
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.9219e+00 (1.9219e+00)	Acc@1  67.00 ( 67.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 2.0098e+00 (1.9161e+00)	Acc@1  62.00 ( 66.18)	Acc@5  91.00 ( 88.64)
Test: [ 20/100]	Time  0.020 ( 0.020)	Loss 1.9648e+00 (1.8834e+00)	Acc@1  71.00 ( 67.29)	Acc@5  86.00 ( 88.86)
Test: [ 30/100]	Time  0.020 ( 0.018)	Loss 2.1016e+00 (1.8917e+00)	Acc@1  63.00 ( 67.10)	Acc@5  85.00 ( 88.42)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.4805e+00 (1.8594e+00)	Acc@1  70.00 ( 67.41)	Acc@5  94.00 ( 88.76)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.8311e+00 (1.8837e+00)	Acc@1  70.00 ( 67.61)	Acc@5  92.00 ( 88.43)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.6807e+00 (1.8520e+00)	Acc@1  71.00 ( 67.84)	Acc@5  89.00 ( 88.66)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 2.0430e+00 (1.8526e+00)	Acc@1  71.00 ( 67.87)	Acc@5  88.00 ( 88.75)
Test: [ 80/100]	Time  0.014 ( 0.016)	Loss 1.8242e+00 (1.8631e+00)	Acc@1  68.00 ( 67.70)	Acc@5  86.00 ( 88.63)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.1855e+00 (1.8562e+00)	Acc@1  63.00 ( 67.87)	Acc@5  88.00 ( 88.76)
 * Acc@1 67.980 Acc@5 88.790
### epoch[87] execution time: 9.69326400756836
EPOCH 88
i:   0, name:            classifier.0.weight  changing lr from: 0.002018226674204087   to: 0.002440150831629428
i:   1, name:              classifier.0.bias  changing lr from: 0.001548284110116498   to: 0.001862798804795578
i:   2, name:            classifier.3.weight  changing lr from: 0.001227600515389420   to: 0.001439312930204133
i:   3, name:              classifier.3.bias  changing lr from: 0.001047947311535923   to: 0.001161560846393721
i:   4, name:            classifier.6.weight  changing lr from: 0.001001143633758599   to: 0.001021415293114380
i:   5, name:              classifier.6.bias  changing lr from: 0.001079108607399736   to: 0.001010811793459670



# Switched to train mode...
Epoch: [88][  0/391]	Time  0.174 ( 0.174)	Data  0.153 ( 0.153)	Loss 4.2877e-02 (4.2877e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.012 ( 0.033)	Data  0.001 ( 0.016)	Loss 6.4575e-02 (5.0472e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.011 ( 0.027)	Data  0.000 ( 0.009)	Loss 2.4841e-02 (5.2504e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.034 ( 0.025)	Data  0.001 ( 0.007)	Loss 5.5298e-02 (5.3089e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.012 ( 0.024)	Data  0.001 ( 0.005)	Loss 4.3762e-02 (5.3408e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.018 ( 0.023)	Data  0.002 ( 0.005)	Loss 9.5886e-02 (5.3820e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [88][ 60/391]	Time  0.021 ( 0.023)	Data  0.001 ( 0.004)	Loss 8.6426e-02 (5.6033e-02)	Acc@1  96.88 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [88][ 70/391]	Time  0.012 ( 0.022)	Data  0.001 ( 0.004)	Loss 6.7688e-02 (5.8401e-02)	Acc@1  95.31 ( 98.07)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 80/391]	Time  0.025 ( 0.022)	Data  0.002 ( 0.003)	Loss 5.4260e-02 (5.7868e-02)	Acc@1  96.09 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 90/391]	Time  0.012 ( 0.022)	Data  0.001 ( 0.003)	Loss 1.0126e-01 (5.8148e-02)	Acc@1  95.31 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [88][100/391]	Time  0.011 ( 0.021)	Data  0.000 ( 0.003)	Loss 3.0914e-02 (5.8415e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [88][110/391]	Time  0.029 ( 0.021)	Data  0.001 ( 0.003)	Loss 6.8237e-02 (5.7924e-02)	Acc@1  96.09 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [88][120/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.003)	Loss 8.5205e-02 (5.7385e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [88][130/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 7.2510e-02 (5.8001e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [88][140/391]	Time  0.032 ( 0.021)	Data  0.001 ( 0.003)	Loss 9.9854e-02 (5.8820e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [88][150/391]	Time  0.026 ( 0.021)	Data  0.003 ( 0.002)	Loss 1.4053e-02 (5.8352e-02)	Acc@1 100.00 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [88][160/391]	Time  0.024 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.3721e-02 (5.9108e-02)	Acc@1  96.88 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [88][170/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.1199e-02 (5.8301e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [88][180/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.9326e-02 (5.7824e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [88][190/391]	Time  0.024 ( 0.021)	Data  0.000 ( 0.002)	Loss 3.1799e-02 (5.8259e-02)	Acc@1 100.00 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [88][200/391]	Time  0.017 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.1021e-02 (5.8334e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [88][210/391]	Time  0.029 ( 0.021)	Data  0.000 ( 0.002)	Loss 8.7219e-02 (5.8304e-02)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 ( 99.97)
Epoch: [88][220/391]	Time  0.028 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.8666e-02 (5.8373e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [88][230/391]	Time  0.012 ( 0.021)	Data  0.002 ( 0.002)	Loss 1.5247e-01 (5.9398e-02)	Acc@1  96.88 ( 98.11)	Acc@5  99.22 ( 99.96)
Epoch: [88][240/391]	Time  0.011 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.5634e-02 (6.0092e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.96)
Epoch: [88][250/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.8726e-02 (5.9677e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.96)
Epoch: [88][260/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.3660e-01 (5.9882e-02)	Acc@1  95.31 ( 98.10)	Acc@5 100.00 ( 99.96)
Epoch: [88][270/391]	Time  0.029 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.9519e-02 (5.9867e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.96)
Epoch: [88][280/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.9424e-02 (5.9702e-02)	Acc@1 100.00 ( 98.09)	Acc@5 100.00 ( 99.96)
Epoch: [88][290/391]	Time  0.031 ( 0.021)	Data  0.003 ( 0.002)	Loss 5.8044e-02 (6.0171e-02)	Acc@1  98.44 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [88][300/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.9368e-02 (5.9989e-02)	Acc@1  97.66 ( 98.08)	Acc@5 100.00 ( 99.97)
Epoch: [88][310/391]	Time  0.026 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.1969e-01 (6.0246e-02)	Acc@1  96.88 ( 98.07)	Acc@5  99.22 ( 99.96)
Epoch: [88][320/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.8091e-02 (6.0141e-02)	Acc@1 100.00 ( 98.08)	Acc@5 100.00 ( 99.96)
Epoch: [88][330/391]	Time  0.011 ( 0.021)	Data  0.000 ( 0.002)	Loss 6.0730e-02 (5.9794e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.96)
Epoch: [88][340/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.4281e-02 (5.9592e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.96)
Epoch: [88][350/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.6956e-02 (5.9316e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [88][360/391]	Time  0.028 ( 0.021)	Data  0.000 ( 0.002)	Loss 4.5349e-02 (5.9175e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [88][370/391]	Time  0.032 ( 0.021)	Data  0.002 ( 0.002)	Loss 3.9246e-02 (5.9172e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.96)
Epoch: [88][380/391]	Time  0.031 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.5063e-02 (5.8895e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [88][390/391]	Time  0.011 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.1261e-02 (5.8608e-02)	Acc@1 100.00 ( 98.14)	Acc@5 100.00 ( 99.97)
## e[88] optimizer.zero_grad (sum) time: 0.026272296905517578
## e[88]       loss.backward (sum) time: 0.23607444763183594
## e[88]      optimizer.step (sum) time: 0.16420292854309082
## epoch[88] training(only) time: 8.109500646591187
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.9053e+00 (1.9053e+00)	Acc@1  66.00 ( 66.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.9785e+00 (1.9187e+00)	Acc@1  61.00 ( 65.91)	Acc@5  89.00 ( 88.73)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 1.9688e+00 (1.8797e+00)	Acc@1  71.00 ( 67.24)	Acc@5  86.00 ( 88.90)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 2.0430e+00 (1.8851e+00)	Acc@1  62.00 ( 67.16)	Acc@5  85.00 ( 88.42)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.4404e+00 (1.8528e+00)	Acc@1  70.00 ( 67.39)	Acc@5  94.00 ( 88.90)
Test: [ 50/100]	Time  0.017 ( 0.016)	Loss 1.8467e+00 (1.8799e+00)	Acc@1  69.00 ( 67.45)	Acc@5  91.00 ( 88.57)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6533e+00 (1.8474e+00)	Acc@1  69.00 ( 67.72)	Acc@5  90.00 ( 88.82)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 2.0215e+00 (1.8465e+00)	Acc@1  70.00 ( 67.83)	Acc@5  89.00 ( 88.90)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.8154e+00 (1.8545e+00)	Acc@1  68.00 ( 67.68)	Acc@5  86.00 ( 88.72)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 2.1914e+00 (1.8487e+00)	Acc@1  63.00 ( 67.85)	Acc@5  88.00 ( 88.88)
 * Acc@1 67.970 Acc@5 88.920
### epoch[88] execution time: 9.697451829910278
EPOCH 89
i:   0, name:            classifier.0.weight  changing lr from: 0.002440150831629428   to: 0.002933797132836568
i:   1, name:              classifier.0.bias  changing lr from: 0.001862798804795578   to: 0.002247566459464683
i:   2, name:            classifier.3.weight  changing lr from: 0.001439312930204133   to: 0.001719628783255372
i:   3, name:              classifier.3.bias  changing lr from: 0.001161560846393721   to: 0.001341978402964088
i:   4, name:            classifier.6.weight  changing lr from: 0.001021415293114380   to: 0.001106569247163440
i:   5, name:              classifier.6.bias  changing lr from: 0.001010811793459670   to: 0.001005378120133265



# Switched to train mode...
Epoch: [89][  0/391]	Time  0.173 ( 0.173)	Data  0.150 ( 0.150)	Loss 4.2450e-02 (4.2450e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.015)	Loss 2.9541e-02 (4.1771e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.93)
Epoch: [89][ 20/391]	Time  0.012 ( 0.027)	Data  0.001 ( 0.009)	Loss 1.2231e-01 (4.6432e-02)	Acc@1  96.09 ( 98.40)	Acc@5 100.00 ( 99.96)
Epoch: [89][ 30/391]	Time  0.013 ( 0.024)	Data  0.002 ( 0.006)	Loss 7.2144e-02 (4.8694e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.95)
Epoch: [89][ 40/391]	Time  0.013 ( 0.024)	Data  0.001 ( 0.006)	Loss 1.1969e-01 (5.3343e-02)	Acc@1  94.53 ( 98.19)	Acc@5 100.00 ( 99.96)
Epoch: [89][ 50/391]	Time  0.039 ( 0.023)	Data  0.012 ( 0.005)	Loss 5.0171e-02 (5.7459e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [89][ 60/391]	Time  0.021 ( 0.023)	Data  0.001 ( 0.004)	Loss 3.5583e-02 (5.5026e-02)	Acc@1  99.22 ( 98.25)	Acc@5 100.00 ( 99.97)
Epoch: [89][ 70/391]	Time  0.022 ( 0.022)	Data  0.001 ( 0.004)	Loss 3.7445e-02 (5.3885e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 80/391]	Time  0.013 ( 0.022)	Data  0.001 ( 0.004)	Loss 6.5735e-02 (5.3476e-02)	Acc@1  96.88 ( 98.28)	Acc@5  99.22 ( 99.97)
Epoch: [89][ 90/391]	Time  0.018 ( 0.022)	Data  0.001 ( 0.003)	Loss 5.4230e-02 (5.3443e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [89][100/391]	Time  0.032 ( 0.021)	Data  0.003 ( 0.003)	Loss 3.2867e-02 (5.2664e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [89][110/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.003)	Loss 1.9714e-02 (5.2415e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.97)
Epoch: [89][120/391]	Time  0.023 ( 0.021)	Data  0.001 ( 0.003)	Loss 6.0547e-02 (5.3074e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [89][130/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 5.3680e-02 (5.2701e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [89][140/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.003)	Loss 4.2419e-02 (5.2728e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [89][150/391]	Time  0.029 ( 0.021)	Data  0.001 ( 0.003)	Loss 5.9723e-02 (5.2998e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [89][160/391]	Time  0.016 ( 0.021)	Data  0.001 ( 0.003)	Loss 5.4688e-02 (5.3639e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [89][170/391]	Time  0.044 ( 0.021)	Data  0.014 ( 0.003)	Loss 9.2224e-02 (5.4080e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [89][180/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.003)	Loss 3.8574e-02 (5.3861e-02)	Acc@1  99.22 ( 98.27)	Acc@5 100.00 ( 99.98)
Epoch: [89][190/391]	Time  0.018 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.8990e-02 (5.4805e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.98)
Epoch: [89][200/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.9805e-02 (5.5762e-02)	Acc@1  97.66 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [89][210/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.4475e-02 (5.5854e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [89][220/391]	Time  0.016 ( 0.021)	Data  0.001 ( 0.002)	Loss 9.5215e-02 (5.5750e-02)	Acc@1  97.66 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [89][230/391]	Time  0.028 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.8899e-02 (5.5781e-02)	Acc@1  96.09 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [89][240/391]	Time  0.030 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.3904e-02 (5.5986e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [89][250/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 6.1401e-02 (5.5901e-02)	Acc@1  98.44 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [89][260/391]	Time  0.028 ( 0.021)	Data  0.001 ( 0.002)	Loss 7.1655e-02 (5.6188e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [89][270/391]	Time  0.028 ( 0.021)	Data  0.002 ( 0.002)	Loss 6.5369e-02 (5.6174e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [89][280/391]	Time  0.013 ( 0.021)	Data  0.002 ( 0.002)	Loss 6.6650e-02 (5.5998e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [89][290/391]	Time  0.020 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.0325e-02 (5.6283e-02)	Acc@1 100.00 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [89][300/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.1840e-02 (5.6382e-02)	Acc@1  98.44 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [89][310/391]	Time  0.021 ( 0.021)	Data  0.001 ( 0.002)	Loss 1.0742e-01 (5.6471e-02)	Acc@1  96.09 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [89][320/391]	Time  0.012 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.0853e-02 (5.6156e-02)	Acc@1  99.22 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [89][330/391]	Time  0.022 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.3270e-02 (5.5731e-02)	Acc@1 100.00 ( 98.21)	Acc@5 100.00 ( 99.97)
Epoch: [89][340/391]	Time  0.014 ( 0.021)	Data  0.001 ( 0.002)	Loss 2.5650e-02 (5.5867e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.97)
Epoch: [89][350/391]	Time  0.017 ( 0.021)	Data  0.001 ( 0.002)	Loss 5.0720e-02 (5.5814e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [89][360/391]	Time  0.025 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.4393e-02 (5.5554e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [89][370/391]	Time  0.031 ( 0.021)	Data  0.001 ( 0.002)	Loss 3.2928e-02 (5.5341e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [89][380/391]	Time  0.020 ( 0.021)	Data  0.001 ( 0.002)	Loss 4.6631e-02 (5.5387e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [89][390/391]	Time  0.011 ( 0.020)	Data  0.001 ( 0.002)	Loss 3.6163e-02 (5.5303e-02)	Acc@1  97.50 ( 98.23)	Acc@5 100.00 ( 99.97)
## e[89] optimizer.zero_grad (sum) time: 0.02552652359008789
## e[89]       loss.backward (sum) time: 0.23075580596923828
## e[89]      optimizer.step (sum) time: 0.15866708755493164
## epoch[89] training(only) time: 8.105515241622925
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.9082e+00 (1.9082e+00)	Acc@1  67.00 ( 67.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.016 ( 0.025)	Loss 2.0059e+00 (1.9116e+00)	Acc@1  63.00 ( 66.27)	Acc@5  89.00 ( 87.91)
Test: [ 20/100]	Time  0.018 ( 0.020)	Loss 2.0078e+00 (1.8858e+00)	Acc@1  69.00 ( 67.38)	Acc@5  86.00 ( 88.52)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.1348e+00 (1.8962e+00)	Acc@1  61.00 ( 66.87)	Acc@5  85.00 ( 88.06)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4893e+00 (1.8604e+00)	Acc@1  67.00 ( 67.20)	Acc@5  94.00 ( 88.61)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.8672e+00 (1.8823e+00)	Acc@1  69.00 ( 67.29)	Acc@5  90.00 ( 88.29)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.6641e+00 (1.8497e+00)	Acc@1  71.00 ( 67.67)	Acc@5  90.00 ( 88.59)
Test: [ 70/100]	Time  0.015 ( 0.016)	Loss 2.0391e+00 (1.8479e+00)	Acc@1  67.00 ( 67.73)	Acc@5  89.00 ( 88.72)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.8154e+00 (1.8561e+00)	Acc@1  68.00 ( 67.57)	Acc@5  86.00 ( 88.56)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.1406e+00 (1.8480e+00)	Acc@1  63.00 ( 67.77)	Acc@5  88.00 ( 88.77)
 * Acc@1 67.860 Acc@5 88.850
### epoch[89] execution time: 9.703134775161743
### Training complete:
*** Model named parameters and requires_grad:
name:       features.module.0.weight  req_grad: False 
name:         features.module.0.bias  req_grad: False 
name:       features.module.1.weight  req_grad: False 
name:         features.module.1.bias  req_grad: False 
name:       features.module.4.weight  req_grad: False 
name:         features.module.4.bias  req_grad: False 
name:       features.module.5.weight  req_grad: False 
name:         features.module.5.bias  req_grad: False 
name:       features.module.8.weight  req_grad: False 
name:         features.module.8.bias  req_grad: False 
name:       features.module.9.weight  req_grad: False 
name:         features.module.9.bias  req_grad: False 
name:      features.module.11.weight  req_grad: False 
name:        features.module.11.bias  req_grad: False 
name:      features.module.12.weight  req_grad: False 
name:        features.module.12.bias  req_grad: False 
name:      features.module.15.weight  req_grad: False 
name:        features.module.15.bias  req_grad: False 
name:      features.module.16.weight  req_grad: False 
name:        features.module.16.bias  req_grad: False 
name:      features.module.18.weight  req_grad: False 
name:        features.module.18.bias  req_grad: False 
name:      features.module.19.weight  req_grad: False 
name:        features.module.19.bias  req_grad: False 
name:      features.module.22.weight  req_grad: False 
name:        features.module.22.bias  req_grad: False 
name:      features.module.23.weight  req_grad: False 
name:        features.module.23.bias  req_grad: False 
name:      features.module.25.weight  req_grad: False 
name:        features.module.25.bias  req_grad: False 
name:      features.module.26.weight  req_grad: False 
name:        features.module.26.bias  req_grad: False 
name:            classifier.0.weight  req_grad:  True 
name:              classifier.0.bias  req_grad:  True 
name:            classifier.3.weight  req_grad:  True 
name:              classifier.3.bias  req_grad:  True 
name:            classifier.6.weight  req_grad:  True 
name:              classifier.6.bias  req_grad:  True 


*** Optimizer groups, parameters and req_grads
#        requires_grad:                            True
#           param_name:             classifier.0.weight
#                   lr:            0.002933797132836568
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:               classifier.0.bias
#                   lr:           0.0022475664594646828
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:             classifier.3.weight
#                   lr:           0.0017196287832553715
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:               classifier.3.bias
#                   lr:           0.0013419784029640883
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:             classifier.6.weight
#                   lr:           0.0011065692471634405
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False

#        requires_grad:                            True
#           param_name:               classifier.6.bias
#                   lr:           0.0010053781201332648
#             momentum:                             0.9
#         weight_decay:                          0.0001
#            dampening:                               0
#             nesterov:                           False



*** Optimizer group lrs
# group:  0,   name:            classifier.0.weight,   req_grad:   True   lr: 0.002933797132836568,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  1,   name:              classifier.0.bias,   req_grad:   True   lr: 0.002247566459464683,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  2,   name:            classifier.3.weight,   req_grad:   True   lr: 0.001719628783255372,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  3,   name:              classifier.3.bias,   req_grad:   True   lr: 0.001341978402964088,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  4,   name:            classifier.6.weight,   req_grad:   True   lr: 0.001106569247163440,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
# group:  5,   name:              classifier.6.bias,   req_grad:   True   lr: 0.001005378120133265,   mmm: 0.90000,   weight_decay:    0.0,   damp:    0.0,   nesterov:  False
---------------
#### total training(only) time: 864.9226984977722
##### Total run time: 1010.6016223430634
