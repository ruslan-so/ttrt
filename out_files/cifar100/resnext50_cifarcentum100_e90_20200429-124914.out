# Model: resnext50
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.resnext
<function resnext50 at 0x7f68b267ff28>
# model requested: 'resnext50'
# printing out the model
ResNext(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (conv2): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv3): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (3): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv4): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (3): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (4): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (5): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv5): Sequential(
    (0): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (2): ResNextBottleNeckC(
      (split_transforms): Sequential(
        (0): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (avg): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=100, bias=True)
)
# model is full precision
# Model: resnext50
# Dataset: cifarcentum
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  5.419 ( 5.419)	Data  0.168 ( 0.168)	Loss 4.7444e+00 (4.7444e+00)	Acc@1   1.56 (  1.56)	Acc@5   5.47 (  5.47)
Epoch: [0][ 10/391]	Time  0.151 ( 0.630)	Data  0.001 ( 0.017)	Loss 1.1744e+01 (1.3131e+01)	Acc@1   0.78 (  1.28)	Acc@5   4.69 (  5.61)
Epoch: [0][ 20/391]	Time  0.151 ( 0.402)	Data  0.001 ( 0.011)	Loss 8.0672e+00 (1.4169e+01)	Acc@1   1.56 (  1.23)	Acc@5   7.81 (  5.65)
Epoch: [0][ 30/391]	Time  0.148 ( 0.321)	Data  0.001 ( 0.009)	Loss 4.9001e+00 (1.1338e+01)	Acc@1   1.56 (  1.26)	Acc@5   7.81 (  6.00)
Epoch: [0][ 40/391]	Time  0.149 ( 0.279)	Data  0.001 ( 0.008)	Loss 4.6072e+00 (9.7514e+00)	Acc@1   0.78 (  1.07)	Acc@5  10.16 (  5.85)
Epoch: [0][ 50/391]	Time  0.153 ( 0.254)	Data  0.001 ( 0.007)	Loss 4.6117e+00 (8.7538e+00)	Acc@1   3.12 (  1.13)	Acc@5   7.03 (  6.07)
Epoch: [0][ 60/391]	Time  0.150 ( 0.237)	Data  0.001 ( 0.007)	Loss 4.6009e+00 (8.0789e+00)	Acc@1   2.34 (  1.14)	Acc@5   8.59 (  6.07)
Epoch: [0][ 70/391]	Time  0.150 ( 0.225)	Data  0.001 ( 0.006)	Loss 4.6151e+00 (7.5927e+00)	Acc@1   1.56 (  1.18)	Acc@5   5.47 (  6.08)
Epoch: [0][ 80/391]	Time  0.152 ( 0.216)	Data  0.001 ( 0.006)	Loss 4.6168e+00 (7.2228e+00)	Acc@1   0.00 (  1.22)	Acc@5   2.34 (  6.03)
Epoch: [0][ 90/391]	Time  0.151 ( 0.209)	Data  0.001 ( 0.006)	Loss 4.5690e+00 (6.9342e+00)	Acc@1   3.91 (  1.22)	Acc@5   7.03 (  6.16)
Epoch: [0][100/391]	Time  0.201 ( 0.205)	Data  0.001 ( 0.006)	Loss 4.5581e+00 (6.7019e+00)	Acc@1   2.34 (  1.21)	Acc@5   8.59 (  6.15)
Epoch: [0][110/391]	Time  0.199 ( 0.205)	Data  0.001 ( 0.006)	Loss 4.5965e+00 (6.5116e+00)	Acc@1   0.78 (  1.23)	Acc@5   5.47 (  6.22)
Epoch: [0][120/391]	Time  0.206 ( 0.205)	Data  0.001 ( 0.006)	Loss 4.5852e+00 (6.3524e+00)	Acc@1   3.12 (  1.28)	Acc@5   7.03 (  6.19)
Epoch: [0][130/391]	Time  0.206 ( 0.205)	Data  0.001 ( 0.006)	Loss 4.6027e+00 (6.2188e+00)	Acc@1   0.78 (  1.26)	Acc@5   5.47 (  6.17)
Epoch: [0][140/391]	Time  0.229 ( 0.206)	Data  0.001 ( 0.006)	Loss 4.5875e+00 (6.1035e+00)	Acc@1   0.78 (  1.25)	Acc@5   6.25 (  6.09)
Epoch: [0][150/391]	Time  0.224 ( 0.207)	Data  0.001 ( 0.006)	Loss 4.5868e+00 (6.0028e+00)	Acc@1   2.34 (  1.28)	Acc@5   7.81 (  6.17)
Epoch: [0][160/391]	Time  0.171 ( 0.207)	Data  0.001 ( 0.006)	Loss 4.5770e+00 (5.9144e+00)	Acc@1   0.00 (  1.29)	Acc@5   3.12 (  6.16)
Epoch: [0][170/391]	Time  0.175 ( 0.205)	Data  0.001 ( 0.006)	Loss 4.5664e+00 (5.8360e+00)	Acc@1   1.56 (  1.30)	Acc@5   4.69 (  6.10)
Epoch: [0][180/391]	Time  0.176 ( 0.203)	Data  0.001 ( 0.006)	Loss 4.6064e+00 (5.7668e+00)	Acc@1   0.78 (  1.33)	Acc@5   5.47 (  6.18)
Epoch: [0][190/391]	Time  0.222 ( 0.203)	Data  0.002 ( 0.006)	Loss 4.5632e+00 (5.7046e+00)	Acc@1   0.78 (  1.33)	Acc@5   7.03 (  6.23)
Epoch: [0][200/391]	Time  0.220 ( 0.204)	Data  0.001 ( 0.006)	Loss 4.6223e+00 (5.6492e+00)	Acc@1   0.00 (  1.33)	Acc@5   0.78 (  6.22)
Epoch: [0][210/391]	Time  0.184 ( 0.204)	Data  0.001 ( 0.006)	Loss 4.5890e+00 (5.5987e+00)	Acc@1   0.00 (  1.32)	Acc@5   3.12 (  6.22)
Epoch: [0][220/391]	Time  0.175 ( 0.203)	Data  0.001 ( 0.006)	Loss 4.5617e+00 (5.5518e+00)	Acc@1   2.34 (  1.33)	Acc@5   7.81 (  6.25)
Epoch: [0][230/391]	Time  0.173 ( 0.202)	Data  0.001 ( 0.006)	Loss 4.5699e+00 (5.5092e+00)	Acc@1   0.78 (  1.36)	Acc@5   7.03 (  6.31)
Epoch: [0][240/391]	Time  0.222 ( 0.202)	Data  0.001 ( 0.006)	Loss 4.5362e+00 (5.4699e+00)	Acc@1   3.91 (  1.37)	Acc@5  10.94 (  6.39)
Epoch: [0][250/391]	Time  0.225 ( 0.202)	Data  0.001 ( 0.006)	Loss 4.5478e+00 (5.4339e+00)	Acc@1   1.56 (  1.37)	Acc@5  10.16 (  6.43)
Epoch: [0][260/391]	Time  0.203 ( 0.203)	Data  0.001 ( 0.006)	Loss 4.5543e+00 (5.4006e+00)	Acc@1   0.78 (  1.38)	Acc@5   7.03 (  6.46)
Epoch: [0][270/391]	Time  0.176 ( 0.202)	Data  0.001 ( 0.006)	Loss 4.6092e+00 (5.3694e+00)	Acc@1   0.78 (  1.40)	Acc@5   4.69 (  6.49)
Epoch: [0][280/391]	Time  0.175 ( 0.201)	Data  0.001 ( 0.006)	Loss 4.5678e+00 (5.3406e+00)	Acc@1   1.56 (  1.41)	Acc@5   7.03 (  6.55)
Epoch: [0][290/391]	Time  0.162 ( 0.200)	Data  0.001 ( 0.006)	Loss 4.5603e+00 (5.3136e+00)	Acc@1   0.00 (  1.41)	Acc@5   8.59 (  6.59)
Epoch: [0][300/391]	Time  0.165 ( 0.199)	Data  0.001 ( 0.006)	Loss 4.5450e+00 (5.2880e+00)	Acc@1   0.78 (  1.41)	Acc@5   8.59 (  6.64)
Epoch: [0][310/391]	Time  0.175 ( 0.199)	Data  0.002 ( 0.006)	Loss 4.5335e+00 (5.2635e+00)	Acc@1   1.56 (  1.42)	Acc@5   8.59 (  6.74)
Epoch: [0][320/391]	Time  0.173 ( 0.198)	Data  0.001 ( 0.006)	Loss 4.5672e+00 (5.2414e+00)	Acc@1   1.56 (  1.44)	Acc@5   7.03 (  6.78)
Epoch: [0][330/391]	Time  0.174 ( 0.198)	Data  0.001 ( 0.006)	Loss 4.5292e+00 (5.2199e+00)	Acc@1   2.34 (  1.44)	Acc@5   8.59 (  6.82)
Epoch: [0][340/391]	Time  0.183 ( 0.197)	Data  0.001 ( 0.006)	Loss 4.5285e+00 (5.1996e+00)	Acc@1   0.78 (  1.45)	Acc@5   5.47 (  6.84)
Epoch: [0][350/391]	Time  0.172 ( 0.197)	Data  0.001 ( 0.006)	Loss 4.4605e+00 (5.1805e+00)	Acc@1   3.12 (  1.48)	Acc@5  14.06 (  6.93)
Epoch: [0][360/391]	Time  0.170 ( 0.196)	Data  0.001 ( 0.006)	Loss 4.5167e+00 (5.1620e+00)	Acc@1   3.12 (  1.47)	Acc@5   7.81 (  6.99)
Epoch: [0][370/391]	Time  0.196 ( 0.196)	Data  0.001 ( 0.006)	Loss 4.4462e+00 (5.1439e+00)	Acc@1   4.69 (  1.50)	Acc@5  14.06 (  7.08)
Epoch: [0][380/391]	Time  0.173 ( 0.195)	Data  0.001 ( 0.006)	Loss 4.3875e+00 (5.1259e+00)	Acc@1   5.47 (  1.52)	Acc@5  15.62 (  7.19)
Epoch: [0][390/391]	Time  1.160 ( 0.197)	Data  0.001 ( 0.006)	Loss 4.5111e+00 (5.1090e+00)	Acc@1   1.25 (  1.54)	Acc@5  10.00 (  7.28)
## e[0] optimizer.zero_grad (sum) time: 0.35569095611572266
## e[0]       loss.backward (sum) time: 25.062546730041504
## e[0]      optimizer.step (sum) time: 3.5460667610168457
## epoch[0] training(only) time: 77.2048864364624
# Switched to evaluate mode...
Test: [  0/100]	Time  0.631 ( 0.631)	Loss 4.4183e+00 (4.4183e+00)	Acc@1   5.00 (  5.00)	Acc@5  13.00 ( 13.00)
Test: [ 10/100]	Time  0.067 ( 0.122)	Loss 4.4211e+00 (4.4301e+00)	Acc@1   4.00 (  2.55)	Acc@5  13.00 ( 10.55)
Test: [ 20/100]	Time  0.064 ( 0.095)	Loss 4.4995e+00 (4.4295e+00)	Acc@1   1.00 (  2.14)	Acc@5  11.00 ( 11.24)
Test: [ 30/100]	Time  0.065 ( 0.086)	Loss 4.4836e+00 (4.4230e+00)	Acc@1   3.00 (  2.16)	Acc@5   9.00 ( 11.29)
Test: [ 40/100]	Time  0.058 ( 0.080)	Loss 4.3747e+00 (4.4195e+00)	Acc@1   2.00 (  2.37)	Acc@5  13.00 ( 11.10)
Test: [ 50/100]	Time  0.058 ( 0.075)	Loss 4.4396e+00 (4.4242e+00)	Acc@1   4.00 (  2.33)	Acc@5  14.00 ( 11.39)
Test: [ 60/100]	Time  0.056 ( 0.072)	Loss 4.4330e+00 (4.4194e+00)	Acc@1   2.00 (  2.51)	Acc@5  14.00 ( 11.66)
Test: [ 70/100]	Time  0.058 ( 0.070)	Loss 4.4630e+00 (4.4200e+00)	Acc@1   2.00 (  2.52)	Acc@5  12.00 ( 11.46)
Test: [ 80/100]	Time  0.056 ( 0.069)	Loss 4.4492e+00 (4.4248e+00)	Acc@1   3.00 (  2.48)	Acc@5  12.00 ( 11.38)
Test: [ 90/100]	Time  0.057 ( 0.067)	Loss 4.3122e+00 (4.4189e+00)	Acc@1   2.00 (  2.55)	Acc@5  15.00 ( 11.46)
 * Acc@1 2.550 Acc@5 11.550
### epoch[0] execution time: 83.95739650726318
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.425 ( 0.425)	Data  0.227 ( 0.227)	Loss 4.3861e+00 (4.3861e+00)	Acc@1   3.91 (  3.91)	Acc@5  10.94 ( 10.94)
Epoch: [1][ 10/391]	Time  0.218 ( 0.235)	Data  0.002 ( 0.024)	Loss 4.3425e+00 (4.4115e+00)	Acc@1   2.34 (  2.77)	Acc@5  10.94 ( 12.07)
Epoch: [1][ 20/391]	Time  0.224 ( 0.228)	Data  0.001 ( 0.016)	Loss 4.3426e+00 (4.3938e+00)	Acc@1   3.91 (  2.60)	Acc@5  17.19 ( 11.38)
Epoch: [1][ 30/391]	Time  0.174 ( 0.218)	Data  0.002 ( 0.013)	Loss 4.4337e+00 (4.3974e+00)	Acc@1   0.78 (  2.34)	Acc@5   4.69 ( 11.32)
Epoch: [1][ 40/391]	Time  0.176 ( 0.208)	Data  0.001 ( 0.011)	Loss 4.4774e+00 (4.3949e+00)	Acc@1   2.34 (  2.44)	Acc@5  14.84 ( 11.45)
Epoch: [1][ 50/391]	Time  0.159 ( 0.201)	Data  0.001 ( 0.010)	Loss 4.4225e+00 (4.3866e+00)	Acc@1   4.69 (  2.62)	Acc@5  11.72 ( 11.87)
Epoch: [1][ 60/391]	Time  0.219 ( 0.202)	Data  0.001 ( 0.009)	Loss 4.3365e+00 (4.3787e+00)	Acc@1   3.12 (  2.72)	Acc@5  12.50 ( 12.06)
Epoch: [1][ 70/391]	Time  0.219 ( 0.205)	Data  0.001 ( 0.009)	Loss 4.3999e+00 (4.3745e+00)	Acc@1   3.12 (  2.86)	Acc@5  10.94 ( 12.21)
Epoch: [1][ 80/391]	Time  0.175 ( 0.205)	Data  0.001 ( 0.009)	Loss 4.3331e+00 (4.3708e+00)	Acc@1   5.47 (  2.95)	Acc@5  10.94 ( 12.45)
Epoch: [1][ 90/391]	Time  0.175 ( 0.202)	Data  0.001 ( 0.008)	Loss 4.3880e+00 (4.3680e+00)	Acc@1   2.34 (  2.94)	Acc@5  10.16 ( 12.53)
Epoch: [1][100/391]	Time  0.176 ( 0.199)	Data  0.001 ( 0.008)	Loss 4.3165e+00 (4.3594e+00)	Acc@1   1.56 (  2.99)	Acc@5  14.06 ( 12.97)
Epoch: [1][110/391]	Time  0.191 ( 0.197)	Data  0.001 ( 0.008)	Loss 4.3748e+00 (4.3515e+00)	Acc@1   2.34 (  2.99)	Acc@5   9.38 ( 13.27)
Epoch: [1][120/391]	Time  0.173 ( 0.196)	Data  0.001 ( 0.007)	Loss 4.2207e+00 (4.3451e+00)	Acc@1   3.91 (  3.03)	Acc@5  12.50 ( 13.37)
Epoch: [1][130/391]	Time  0.174 ( 0.194)	Data  0.001 ( 0.007)	Loss 4.2136e+00 (4.3412e+00)	Acc@1   4.69 (  3.09)	Acc@5  17.97 ( 13.52)
Epoch: [1][140/391]	Time  0.191 ( 0.193)	Data  0.002 ( 0.007)	Loss 4.3440e+00 (4.3404e+00)	Acc@1   3.12 (  3.14)	Acc@5  14.84 ( 13.54)
Epoch: [1][150/391]	Time  0.168 ( 0.192)	Data  0.001 ( 0.007)	Loss 4.2482e+00 (4.3332e+00)	Acc@1   1.56 (  3.19)	Acc@5   9.38 ( 13.76)
Epoch: [1][160/391]	Time  0.174 ( 0.191)	Data  0.001 ( 0.007)	Loss 4.2918e+00 (4.3285e+00)	Acc@1   0.78 (  3.20)	Acc@5  11.72 ( 13.78)
Epoch: [1][170/391]	Time  0.175 ( 0.190)	Data  0.001 ( 0.007)	Loss 4.2603e+00 (4.3253e+00)	Acc@1   2.34 (  3.19)	Acc@5  14.84 ( 13.89)
Epoch: [1][180/391]	Time  0.169 ( 0.190)	Data  0.001 ( 0.006)	Loss 4.2388e+00 (4.3191e+00)	Acc@1   3.91 (  3.18)	Acc@5  14.06 ( 14.02)
Epoch: [1][190/391]	Time  0.176 ( 0.189)	Data  0.001 ( 0.006)	Loss 4.2951e+00 (4.3156e+00)	Acc@1   0.78 (  3.19)	Acc@5  16.41 ( 14.14)
Epoch: [1][200/391]	Time  0.178 ( 0.189)	Data  0.001 ( 0.006)	Loss 4.2114e+00 (4.3119e+00)	Acc@1   3.91 (  3.21)	Acc@5  16.41 ( 14.16)
Epoch: [1][210/391]	Time  0.225 ( 0.189)	Data  0.001 ( 0.006)	Loss 4.2238e+00 (4.3077e+00)	Acc@1   6.25 (  3.22)	Acc@5  20.31 ( 14.27)
Epoch: [1][220/391]	Time  0.217 ( 0.191)	Data  0.002 ( 0.006)	Loss 4.3316e+00 (4.3040e+00)	Acc@1   2.34 (  3.23)	Acc@5  14.84 ( 14.36)
Epoch: [1][230/391]	Time  0.174 ( 0.192)	Data  0.001 ( 0.006)	Loss 4.2993e+00 (4.2994e+00)	Acc@1   7.81 (  3.30)	Acc@5  15.62 ( 14.52)
Epoch: [1][240/391]	Time  0.163 ( 0.191)	Data  0.002 ( 0.006)	Loss 4.2226e+00 (4.2980e+00)	Acc@1   0.78 (  3.29)	Acc@5  12.50 ( 14.52)
Epoch: [1][250/391]	Time  0.173 ( 0.190)	Data  0.001 ( 0.006)	Loss 4.1751e+00 (4.2935e+00)	Acc@1   7.03 (  3.33)	Acc@5  21.88 ( 14.62)
Epoch: [1][260/391]	Time  0.220 ( 0.190)	Data  0.001 ( 0.006)	Loss 4.1567e+00 (4.2895e+00)	Acc@1   3.91 (  3.33)	Acc@5  19.53 ( 14.72)
Epoch: [1][270/391]	Time  0.222 ( 0.192)	Data  0.001 ( 0.006)	Loss 4.1031e+00 (4.2837e+00)	Acc@1   3.12 (  3.37)	Acc@5  18.75 ( 14.85)
Epoch: [1][280/391]	Time  0.205 ( 0.193)	Data  0.001 ( 0.006)	Loss 4.1184e+00 (4.2799e+00)	Acc@1   7.81 (  3.41)	Acc@5  22.66 ( 14.98)
Epoch: [1][290/391]	Time  0.170 ( 0.192)	Data  0.001 ( 0.006)	Loss 4.1909e+00 (4.2756e+00)	Acc@1   2.34 (  3.45)	Acc@5  17.19 ( 15.14)
Epoch: [1][300/391]	Time  0.165 ( 0.191)	Data  0.001 ( 0.006)	Loss 4.0961e+00 (4.2726e+00)	Acc@1   3.12 (  3.47)	Acc@5  20.31 ( 15.21)
Epoch: [1][310/391]	Time  0.209 ( 0.191)	Data  0.001 ( 0.006)	Loss 4.2604e+00 (4.2692e+00)	Acc@1   5.47 (  3.52)	Acc@5  14.84 ( 15.33)
Epoch: [1][320/391]	Time  0.219 ( 0.192)	Data  0.001 ( 0.006)	Loss 4.0901e+00 (4.2648e+00)	Acc@1   3.91 (  3.56)	Acc@5  19.53 ( 15.49)
Epoch: [1][330/391]	Time  0.223 ( 0.192)	Data  0.001 ( 0.006)	Loss 4.2211e+00 (4.2611e+00)	Acc@1   3.12 (  3.60)	Acc@5  16.41 ( 15.59)
Epoch: [1][340/391]	Time  0.174 ( 0.192)	Data  0.001 ( 0.006)	Loss 4.1676e+00 (4.2579e+00)	Acc@1   4.69 (  3.65)	Acc@5  18.75 ( 15.72)
Epoch: [1][350/391]	Time  0.176 ( 0.192)	Data  0.001 ( 0.006)	Loss 4.1003e+00 (4.2539e+00)	Acc@1   4.69 (  3.69)	Acc@5  18.75 ( 15.86)
Epoch: [1][360/391]	Time  0.163 ( 0.191)	Data  0.001 ( 0.006)	Loss 4.1662e+00 (4.2506e+00)	Acc@1   7.03 (  3.73)	Acc@5  21.88 ( 15.97)
Epoch: [1][370/391]	Time  0.176 ( 0.191)	Data  0.001 ( 0.006)	Loss 4.1288e+00 (4.2476e+00)	Acc@1   5.47 (  3.73)	Acc@5  23.44 ( 16.07)
Epoch: [1][380/391]	Time  0.203 ( 0.190)	Data  0.001 ( 0.006)	Loss 4.1068e+00 (4.2432e+00)	Acc@1   5.47 (  3.75)	Acc@5  19.53 ( 16.25)
Epoch: [1][390/391]	Time  0.169 ( 0.190)	Data  0.001 ( 0.006)	Loss 4.2833e+00 (4.2416e+00)	Acc@1   6.25 (  3.77)	Acc@5  13.75 ( 16.33)
## e[1] optimizer.zero_grad (sum) time: 0.3878481388092041
## e[1]       loss.backward (sum) time: 25.87405014038086
## e[1]      optimizer.step (sum) time: 3.792067289352417
## epoch[1] training(only) time: 74.44542145729065
# Switched to evaluate mode...
Test: [  0/100]	Time  0.253 ( 0.253)	Loss 4.1600e+00 (4.1600e+00)	Acc@1   3.00 (  3.00)	Acc@5  19.00 ( 19.00)
Test: [ 10/100]	Time  0.059 ( 0.078)	Loss 4.0823e+00 (4.1058e+00)	Acc@1   7.00 (  5.18)	Acc@5  21.00 ( 21.09)
Test: [ 20/100]	Time  0.060 ( 0.069)	Loss 4.1383e+00 (4.0989e+00)	Acc@1   6.00 (  5.76)	Acc@5  16.00 ( 22.00)
Test: [ 30/100]	Time  0.057 ( 0.066)	Loss 4.1482e+00 (4.1000e+00)	Acc@1   7.00 (  5.84)	Acc@5  23.00 ( 22.13)
Test: [ 40/100]	Time  0.058 ( 0.064)	Loss 4.1972e+00 (4.1035e+00)	Acc@1   4.00 (  5.59)	Acc@5  16.00 ( 21.51)
Test: [ 50/100]	Time  0.061 ( 0.063)	Loss 4.0388e+00 (4.0970e+00)	Acc@1   7.00 (  5.75)	Acc@5  26.00 ( 22.08)
Test: [ 60/100]	Time  0.057 ( 0.062)	Loss 4.0308e+00 (4.0962e+00)	Acc@1   3.00 (  5.84)	Acc@5  25.00 ( 21.98)
Test: [ 70/100]	Time  0.057 ( 0.062)	Loss 4.1304e+00 (4.0943e+00)	Acc@1   5.00 (  5.61)	Acc@5  18.00 ( 21.80)
Test: [ 80/100]	Time  0.058 ( 0.061)	Loss 4.2532e+00 (4.1039e+00)	Acc@1   5.00 (  5.58)	Acc@5  20.00 ( 21.70)
Test: [ 90/100]	Time  0.057 ( 0.061)	Loss 4.0295e+00 (4.0984e+00)	Acc@1   5.00 (  5.66)	Acc@5  26.00 ( 21.84)
 * Acc@1 5.730 Acc@5 21.820
### epoch[1] execution time: 80.61962676048279
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.393 ( 0.393)	Data  0.234 ( 0.234)	Loss 4.0048e+00 (4.0048e+00)	Acc@1   4.69 (  4.69)	Acc@5  24.22 ( 24.22)
Epoch: [2][ 10/391]	Time  0.175 ( 0.194)	Data  0.001 ( 0.025)	Loss 4.0820e+00 (4.0822e+00)	Acc@1   3.12 (  4.83)	Acc@5  22.66 ( 22.44)
Epoch: [2][ 20/391]	Time  0.173 ( 0.187)	Data  0.001 ( 0.016)	Loss 3.9160e+00 (4.0646e+00)	Acc@1   7.81 (  5.73)	Acc@5  29.69 ( 23.33)
Epoch: [2][ 30/391]	Time  0.228 ( 0.187)	Data  0.001 ( 0.012)	Loss 3.9901e+00 (4.0654e+00)	Acc@1   7.81 (  5.59)	Acc@5  26.56 ( 23.06)
Epoch: [2][ 40/391]	Time  0.222 ( 0.195)	Data  0.002 ( 0.011)	Loss 4.0311e+00 (4.0641e+00)	Acc@1   6.25 (  5.70)	Acc@5  26.56 ( 23.11)
Epoch: [2][ 50/391]	Time  0.224 ( 0.200)	Data  0.001 ( 0.010)	Loss 4.0432e+00 (4.0459e+00)	Acc@1   6.25 (  5.79)	Acc@5  21.88 ( 23.70)
Epoch: [2][ 60/391]	Time  0.166 ( 0.196)	Data  0.001 ( 0.010)	Loss 4.0456e+00 (4.0376e+00)	Acc@1   1.56 (  5.87)	Acc@5  19.53 ( 23.89)
Epoch: [2][ 70/391]	Time  0.170 ( 0.193)	Data  0.002 ( 0.009)	Loss 4.0076e+00 (4.0386e+00)	Acc@1   6.25 (  5.82)	Acc@5  21.09 ( 23.80)
Epoch: [2][ 80/391]	Time  0.167 ( 0.190)	Data  0.002 ( 0.008)	Loss 4.1231e+00 (4.0386e+00)	Acc@1   9.38 (  5.82)	Acc@5  28.12 ( 23.60)
Epoch: [2][ 90/391]	Time  0.227 ( 0.193)	Data  0.002 ( 0.008)	Loss 3.9678e+00 (4.0366e+00)	Acc@1   6.25 (  5.76)	Acc@5  23.44 ( 23.63)
Epoch: [2][100/391]	Time  0.217 ( 0.196)	Data  0.001 ( 0.008)	Loss 3.9298e+00 (4.0317e+00)	Acc@1   7.03 (  5.84)	Acc@5  23.44 ( 23.73)
Epoch: [2][110/391]	Time  0.170 ( 0.196)	Data  0.001 ( 0.008)	Loss 3.8826e+00 (4.0220e+00)	Acc@1   6.25 (  5.89)	Acc@5  29.69 ( 23.99)
Epoch: [2][120/391]	Time  0.178 ( 0.194)	Data  0.002 ( 0.007)	Loss 3.9649e+00 (4.0149e+00)	Acc@1   4.69 (  6.03)	Acc@5  22.66 ( 24.26)
Epoch: [2][130/391]	Time  0.175 ( 0.193)	Data  0.002 ( 0.007)	Loss 3.9638e+00 (4.0091e+00)	Acc@1   3.12 (  6.08)	Acc@5  21.88 ( 24.31)
Epoch: [2][140/391]	Time  0.217 ( 0.193)	Data  0.001 ( 0.007)	Loss 3.9194e+00 (4.0060e+00)	Acc@1   6.25 (  6.12)	Acc@5  31.25 ( 24.53)
Epoch: [2][150/391]	Time  0.218 ( 0.195)	Data  0.001 ( 0.007)	Loss 3.8357e+00 (3.9985e+00)	Acc@1   6.25 (  6.27)	Acc@5  29.69 ( 24.69)
Epoch: [2][160/391]	Time  0.178 ( 0.196)	Data  0.001 ( 0.007)	Loss 3.9846e+00 (3.9951e+00)	Acc@1   7.03 (  6.35)	Acc@5  25.00 ( 24.90)
Epoch: [2][170/391]	Time  0.177 ( 0.195)	Data  0.001 ( 0.007)	Loss 4.1207e+00 (3.9895e+00)	Acc@1   4.69 (  6.40)	Acc@5  22.66 ( 25.06)
Epoch: [2][180/391]	Time  0.169 ( 0.193)	Data  0.001 ( 0.007)	Loss 3.9715e+00 (3.9838e+00)	Acc@1  10.94 (  6.49)	Acc@5  25.00 ( 25.25)
Epoch: [2][190/391]	Time  0.178 ( 0.193)	Data  0.001 ( 0.007)	Loss 3.8230e+00 (3.9780e+00)	Acc@1  10.16 (  6.57)	Acc@5  34.38 ( 25.54)
Epoch: [2][200/391]	Time  0.168 ( 0.192)	Data  0.001 ( 0.007)	Loss 3.9233e+00 (3.9752e+00)	Acc@1   7.03 (  6.70)	Acc@5  28.12 ( 25.66)
Epoch: [2][210/391]	Time  0.177 ( 0.191)	Data  0.001 ( 0.006)	Loss 3.9965e+00 (3.9723e+00)	Acc@1   7.03 (  6.70)	Acc@5  28.91 ( 25.73)
Epoch: [2][220/391]	Time  0.203 ( 0.191)	Data  0.001 ( 0.006)	Loss 3.8264e+00 (3.9695e+00)	Acc@1   6.25 (  6.74)	Acc@5  31.25 ( 25.82)
Epoch: [2][230/391]	Time  0.174 ( 0.190)	Data  0.001 ( 0.006)	Loss 3.8667e+00 (3.9627e+00)	Acc@1   8.59 (  6.84)	Acc@5  27.34 ( 25.99)
Epoch: [2][240/391]	Time  0.162 ( 0.190)	Data  0.001 ( 0.006)	Loss 3.9918e+00 (3.9607e+00)	Acc@1   5.47 (  6.91)	Acc@5  23.44 ( 26.14)
Epoch: [2][250/391]	Time  0.171 ( 0.189)	Data  0.002 ( 0.006)	Loss 3.7904e+00 (3.9557e+00)	Acc@1  10.16 (  6.94)	Acc@5  32.81 ( 26.29)
Epoch: [2][260/391]	Time  0.175 ( 0.189)	Data  0.001 ( 0.006)	Loss 3.6976e+00 (3.9496e+00)	Acc@1  11.72 (  7.04)	Acc@5  35.94 ( 26.53)
Epoch: [2][270/391]	Time  0.176 ( 0.189)	Data  0.001 ( 0.006)	Loss 3.7677e+00 (3.9460e+00)	Acc@1   7.81 (  7.10)	Acc@5  35.16 ( 26.68)
Epoch: [2][280/391]	Time  0.162 ( 0.188)	Data  0.001 ( 0.006)	Loss 3.7935e+00 (3.9427e+00)	Acc@1  11.72 (  7.15)	Acc@5  28.91 ( 26.76)
Epoch: [2][290/391]	Time  0.221 ( 0.189)	Data  0.001 ( 0.006)	Loss 4.0612e+00 (3.9390e+00)	Acc@1   6.25 (  7.25)	Acc@5  26.56 ( 26.92)
Epoch: [2][300/391]	Time  0.224 ( 0.190)	Data  0.001 ( 0.006)	Loss 3.8389e+00 (3.9335e+00)	Acc@1   9.38 (  7.34)	Acc@5  28.12 ( 27.09)
Epoch: [2][310/391]	Time  0.168 ( 0.190)	Data  0.001 ( 0.006)	Loss 3.6434e+00 (3.9275e+00)	Acc@1  13.28 (  7.43)	Acc@5  39.06 ( 27.32)
Epoch: [2][320/391]	Time  0.179 ( 0.190)	Data  0.001 ( 0.006)	Loss 3.7772e+00 (3.9215e+00)	Acc@1   9.38 (  7.54)	Acc@5  35.94 ( 27.57)
Epoch: [2][330/391]	Time  0.172 ( 0.189)	Data  0.001 ( 0.006)	Loss 3.6297e+00 (3.9152e+00)	Acc@1  10.94 (  7.66)	Acc@5  38.28 ( 27.77)
Epoch: [2][340/391]	Time  0.219 ( 0.190)	Data  0.001 ( 0.006)	Loss 3.6876e+00 (3.9097e+00)	Acc@1   9.38 (  7.78)	Acc@5  35.16 ( 27.97)
Epoch: [2][350/391]	Time  0.208 ( 0.191)	Data  0.002 ( 0.006)	Loss 3.6183e+00 (3.9049e+00)	Acc@1  17.97 (  7.87)	Acc@5  40.62 ( 28.14)
Epoch: [2][360/391]	Time  0.175 ( 0.191)	Data  0.001 ( 0.006)	Loss 3.7378e+00 (3.9004e+00)	Acc@1  10.16 (  7.92)	Acc@5  32.81 ( 28.31)
Epoch: [2][370/391]	Time  0.173 ( 0.191)	Data  0.001 ( 0.006)	Loss 3.7701e+00 (3.8967e+00)	Acc@1   6.25 (  7.96)	Acc@5  37.50 ( 28.45)
Epoch: [2][380/391]	Time  0.174 ( 0.190)	Data  0.001 ( 0.006)	Loss 3.8225e+00 (3.8908e+00)	Acc@1   8.59 (  8.02)	Acc@5  36.72 ( 28.64)
Epoch: [2][390/391]	Time  0.197 ( 0.190)	Data  0.001 ( 0.006)	Loss 3.8975e+00 (3.8870e+00)	Acc@1   6.25 (  8.08)	Acc@5  32.50 ( 28.82)
## e[2] optimizer.zero_grad (sum) time: 0.39006853103637695
## e[2]       loss.backward (sum) time: 25.967464923858643
## e[2]      optimizer.step (sum) time: 3.802645683288574
## epoch[2] training(only) time: 74.4888048171997
# Switched to evaluate mode...
Test: [  0/100]	Time  0.250 ( 0.250)	Loss 3.6804e+00 (3.6804e+00)	Acc@1  10.00 ( 10.00)	Acc@5  36.00 ( 36.00)
Test: [ 10/100]	Time  0.066 ( 0.082)	Loss 3.8734e+00 (3.7073e+00)	Acc@1   9.00 ( 10.45)	Acc@5  30.00 ( 35.18)
Test: [ 20/100]	Time  0.064 ( 0.073)	Loss 3.7252e+00 (3.7275e+00)	Acc@1  10.00 ( 11.00)	Acc@5  38.00 ( 35.00)
Test: [ 30/100]	Time  0.062 ( 0.070)	Loss 3.8352e+00 (3.7177e+00)	Acc@1   9.00 ( 10.77)	Acc@5  29.00 ( 35.35)
Test: [ 40/100]	Time  0.064 ( 0.068)	Loss 3.7030e+00 (3.7140e+00)	Acc@1  12.00 ( 10.95)	Acc@5  35.00 ( 35.12)
Test: [ 50/100]	Time  0.063 ( 0.067)	Loss 3.6122e+00 (3.7014e+00)	Acc@1  18.00 ( 11.29)	Acc@5  44.00 ( 35.69)
Test: [ 60/100]	Time  0.057 ( 0.066)	Loss 3.7158e+00 (3.7018e+00)	Acc@1   9.00 ( 11.26)	Acc@5  41.00 ( 35.93)
Test: [ 70/100]	Time  0.055 ( 0.065)	Loss 3.7803e+00 (3.7025e+00)	Acc@1  13.00 ( 11.18)	Acc@5  29.00 ( 35.75)
Test: [ 80/100]	Time  0.055 ( 0.064)	Loss 3.8274e+00 (3.7083e+00)	Acc@1   7.00 ( 11.01)	Acc@5  31.00 ( 35.60)
Test: [ 90/100]	Time  0.057 ( 0.063)	Loss 3.4675e+00 (3.7033e+00)	Acc@1  13.00 ( 11.22)	Acc@5  43.00 ( 35.74)
 * Acc@1 11.220 Acc@5 35.620
### epoch[2] execution time: 80.81460404396057
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.406 ( 0.406)	Data  0.231 ( 0.231)	Loss 3.6716e+00 (3.6716e+00)	Acc@1   7.81 (  7.81)	Acc@5  32.03 ( 32.03)
Epoch: [3][ 10/391]	Time  0.173 ( 0.191)	Data  0.001 ( 0.025)	Loss 3.6674e+00 (3.6792e+00)	Acc@1  10.94 ( 10.65)	Acc@5  35.94 ( 35.72)
Epoch: [3][ 20/391]	Time  0.180 ( 0.185)	Data  0.001 ( 0.015)	Loss 3.4290e+00 (3.6459e+00)	Acc@1  17.19 ( 11.79)	Acc@5  44.53 ( 37.39)
Epoch: [3][ 30/391]	Time  0.209 ( 0.184)	Data  0.001 ( 0.012)	Loss 3.6483e+00 (3.6697e+00)	Acc@1  10.94 ( 11.39)	Acc@5  32.81 ( 36.77)
Epoch: [3][ 40/391]	Time  0.172 ( 0.182)	Data  0.002 ( 0.010)	Loss 3.4896e+00 (3.6637e+00)	Acc@1  20.31 ( 11.76)	Acc@5  46.88 ( 37.12)
Epoch: [3][ 50/391]	Time  0.170 ( 0.182)	Data  0.002 ( 0.009)	Loss 3.6327e+00 (3.6502e+00)	Acc@1  10.16 ( 11.98)	Acc@5  37.50 ( 37.55)
Epoch: [3][ 60/391]	Time  0.185 ( 0.181)	Data  0.001 ( 0.008)	Loss 3.5572e+00 (3.6393e+00)	Acc@1  10.94 ( 12.24)	Acc@5  39.84 ( 37.81)
Epoch: [3][ 70/391]	Time  0.172 ( 0.180)	Data  0.001 ( 0.008)	Loss 3.6690e+00 (3.6394e+00)	Acc@1  11.72 ( 12.24)	Acc@5  35.16 ( 37.76)
Epoch: [3][ 80/391]	Time  0.170 ( 0.180)	Data  0.001 ( 0.008)	Loss 3.6773e+00 (3.6339e+00)	Acc@1  10.16 ( 12.22)	Acc@5  35.16 ( 37.80)
Epoch: [3][ 90/391]	Time  0.174 ( 0.179)	Data  0.001 ( 0.007)	Loss 3.3731e+00 (3.6258e+00)	Acc@1  15.62 ( 12.34)	Acc@5  45.31 ( 38.00)
Epoch: [3][100/391]	Time  0.170 ( 0.179)	Data  0.001 ( 0.007)	Loss 3.6076e+00 (3.6292e+00)	Acc@1  13.28 ( 12.28)	Acc@5  34.38 ( 37.84)
Epoch: [3][110/391]	Time  0.223 ( 0.180)	Data  0.001 ( 0.007)	Loss 3.6416e+00 (3.6280e+00)	Acc@1  12.50 ( 12.29)	Acc@5  30.47 ( 37.73)
Epoch: [3][120/391]	Time  0.223 ( 0.183)	Data  0.001 ( 0.007)	Loss 3.5991e+00 (3.6289e+00)	Acc@1  10.16 ( 12.22)	Acc@5  44.53 ( 37.84)
Epoch: [3][130/391]	Time  0.228 ( 0.186)	Data  0.001 ( 0.007)	Loss 3.4637e+00 (3.6272e+00)	Acc@1  10.94 ( 12.16)	Acc@5  42.97 ( 37.92)
Epoch: [3][140/391]	Time  0.165 ( 0.186)	Data  0.001 ( 0.007)	Loss 3.7555e+00 (3.6238e+00)	Acc@1  11.72 ( 12.20)	Acc@5  32.03 ( 37.93)
Epoch: [3][150/391]	Time  0.165 ( 0.185)	Data  0.001 ( 0.007)	Loss 3.4330e+00 (3.6198e+00)	Acc@1  12.50 ( 12.28)	Acc@5  44.53 ( 38.06)
Epoch: [3][160/391]	Time  0.203 ( 0.185)	Data  0.001 ( 0.007)	Loss 3.4944e+00 (3.6201e+00)	Acc@1  10.94 ( 12.33)	Acc@5  44.53 ( 38.10)
Epoch: [3][170/391]	Time  0.220 ( 0.187)	Data  0.001 ( 0.007)	Loss 3.4344e+00 (3.6095e+00)	Acc@1  14.84 ( 12.55)	Acc@5  46.09 ( 38.42)
Epoch: [3][180/391]	Time  0.225 ( 0.189)	Data  0.001 ( 0.007)	Loss 3.2650e+00 (3.6047e+00)	Acc@1  23.44 ( 12.58)	Acc@5  52.34 ( 38.47)
Epoch: [3][190/391]	Time  0.161 ( 0.189)	Data  0.001 ( 0.007)	Loss 3.5659e+00 (3.5988e+00)	Acc@1  13.28 ( 12.62)	Acc@5  43.75 ( 38.60)
Epoch: [3][200/391]	Time  0.173 ( 0.188)	Data  0.001 ( 0.007)	Loss 3.8511e+00 (3.5979e+00)	Acc@1  10.94 ( 12.63)	Acc@5  32.03 ( 38.68)
Epoch: [3][210/391]	Time  0.165 ( 0.187)	Data  0.002 ( 0.007)	Loss 3.4920e+00 (3.5940e+00)	Acc@1  15.62 ( 12.67)	Acc@5  47.66 ( 38.83)
Epoch: [3][220/391]	Time  0.207 ( 0.188)	Data  0.001 ( 0.007)	Loss 3.2469e+00 (3.5868e+00)	Acc@1  19.53 ( 12.81)	Acc@5  52.34 ( 38.99)
Epoch: [3][230/391]	Time  0.203 ( 0.189)	Data  0.001 ( 0.007)	Loss 3.4861e+00 (3.5818e+00)	Acc@1  14.06 ( 12.85)	Acc@5  46.88 ( 39.13)
Epoch: [3][240/391]	Time  0.169 ( 0.189)	Data  0.001 ( 0.007)	Loss 3.3637e+00 (3.5780e+00)	Acc@1  12.50 ( 12.97)	Acc@5  43.75 ( 39.23)
Epoch: [3][250/391]	Time  0.175 ( 0.188)	Data  0.001 ( 0.007)	Loss 3.5045e+00 (3.5725e+00)	Acc@1  12.50 ( 13.09)	Acc@5  40.62 ( 39.40)
Epoch: [3][260/391]	Time  0.163 ( 0.187)	Data  0.001 ( 0.006)	Loss 3.4352e+00 (3.5679e+00)	Acc@1  14.84 ( 13.24)	Acc@5  41.41 ( 39.57)
Epoch: [3][270/391]	Time  0.178 ( 0.187)	Data  0.001 ( 0.006)	Loss 3.4114e+00 (3.5662e+00)	Acc@1   8.59 ( 13.21)	Acc@5  46.09 ( 39.64)
Epoch: [3][280/391]	Time  0.164 ( 0.186)	Data  0.001 ( 0.006)	Loss 3.4953e+00 (3.5649e+00)	Acc@1  17.19 ( 13.29)	Acc@5  44.53 ( 39.64)
Epoch: [3][290/391]	Time  0.164 ( 0.186)	Data  0.001 ( 0.006)	Loss 3.4703e+00 (3.5607e+00)	Acc@1  14.06 ( 13.38)	Acc@5  42.19 ( 39.74)
Epoch: [3][300/391]	Time  0.177 ( 0.186)	Data  0.001 ( 0.006)	Loss 3.4755e+00 (3.5595e+00)	Acc@1  20.31 ( 13.43)	Acc@5  37.50 ( 39.76)
Epoch: [3][310/391]	Time  0.174 ( 0.186)	Data  0.002 ( 0.006)	Loss 3.6041e+00 (3.5599e+00)	Acc@1  12.50 ( 13.41)	Acc@5  34.38 ( 39.70)
Epoch: [3][320/391]	Time  0.167 ( 0.185)	Data  0.001 ( 0.006)	Loss 3.4020e+00 (3.5557e+00)	Acc@1  13.28 ( 13.49)	Acc@5  42.19 ( 39.81)
Epoch: [3][330/391]	Time  0.187 ( 0.185)	Data  0.002 ( 0.006)	Loss 3.3866e+00 (3.5518e+00)	Acc@1  14.84 ( 13.57)	Acc@5  50.78 ( 39.99)
Epoch: [3][340/391]	Time  0.173 ( 0.185)	Data  0.001 ( 0.006)	Loss 3.4148e+00 (3.5473e+00)	Acc@1  10.94 ( 13.65)	Acc@5  36.72 ( 40.10)
Epoch: [3][350/391]	Time  0.171 ( 0.185)	Data  0.001 ( 0.006)	Loss 3.3543e+00 (3.5426e+00)	Acc@1  21.09 ( 13.76)	Acc@5  45.31 ( 40.22)
Epoch: [3][360/391]	Time  0.163 ( 0.184)	Data  0.001 ( 0.006)	Loss 3.4194e+00 (3.5409e+00)	Acc@1  17.19 ( 13.79)	Acc@5  44.53 ( 40.24)
Epoch: [3][370/391]	Time  0.221 ( 0.185)	Data  0.001 ( 0.006)	Loss 3.2154e+00 (3.5351e+00)	Acc@1  21.09 ( 13.89)	Acc@5  40.62 ( 40.39)
Epoch: [3][380/391]	Time  0.224 ( 0.186)	Data  0.001 ( 0.006)	Loss 3.2827e+00 (3.5315e+00)	Acc@1  17.97 ( 13.95)	Acc@5  46.88 ( 40.48)
Epoch: [3][390/391]	Time  0.172 ( 0.186)	Data  0.001 ( 0.006)	Loss 3.2839e+00 (3.5268e+00)	Acc@1  18.75 ( 14.06)	Acc@5  52.50 ( 40.58)
## e[3] optimizer.zero_grad (sum) time: 0.3800184726715088
## e[3]       loss.backward (sum) time: 25.30486512184143
## e[3]      optimizer.step (sum) time: 3.7330026626586914
## epoch[3] training(only) time: 72.9961199760437
# Switched to evaluate mode...
Test: [  0/100]	Time  0.277 ( 0.277)	Loss 3.3854e+00 (3.3854e+00)	Acc@1  18.00 ( 18.00)	Acc@5  44.00 ( 44.00)
Test: [ 10/100]	Time  0.058 ( 0.078)	Loss 3.2728e+00 (3.2811e+00)	Acc@1  16.00 ( 19.09)	Acc@5  53.00 ( 48.55)
Test: [ 20/100]	Time  0.056 ( 0.068)	Loss 3.0843e+00 (3.2960e+00)	Acc@1  19.00 ( 18.57)	Acc@5  50.00 ( 47.57)
Test: [ 30/100]	Time  0.056 ( 0.065)	Loss 3.5640e+00 (3.2895e+00)	Acc@1  13.00 ( 18.65)	Acc@5  33.00 ( 47.71)
Test: [ 40/100]	Time  0.057 ( 0.063)	Loss 3.3791e+00 (3.2900e+00)	Acc@1  19.00 ( 18.80)	Acc@5  43.00 ( 47.83)
Test: [ 50/100]	Time  0.057 ( 0.062)	Loss 3.1119e+00 (3.2791e+00)	Acc@1  28.00 ( 19.25)	Acc@5  49.00 ( 48.04)
Test: [ 60/100]	Time  0.066 ( 0.063)	Loss 3.2978e+00 (3.2764e+00)	Acc@1  17.00 ( 19.20)	Acc@5  47.00 ( 48.10)
Test: [ 70/100]	Time  0.067 ( 0.063)	Loss 3.5537e+00 (3.2842e+00)	Acc@1  15.00 ( 19.13)	Acc@5  37.00 ( 47.65)
Test: [ 80/100]	Time  0.067 ( 0.064)	Loss 3.3928e+00 (3.2891e+00)	Acc@1  10.00 ( 19.27)	Acc@5  43.00 ( 47.60)
Test: [ 90/100]	Time  0.066 ( 0.064)	Loss 3.1515e+00 (3.2847e+00)	Acc@1  18.00 ( 19.14)	Acc@5  51.00 ( 47.70)
 * Acc@1 19.030 Acc@5 47.650
### epoch[3] execution time: 79.5470323562622
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.418 ( 0.418)	Data  0.231 ( 0.231)	Loss 3.5490e+00 (3.5490e+00)	Acc@1  17.19 ( 17.19)	Acc@5  41.41 ( 41.41)
Epoch: [4][ 10/391]	Time  0.169 ( 0.222)	Data  0.001 ( 0.027)	Loss 3.3699e+00 (3.2404e+00)	Acc@1  17.19 ( 20.17)	Acc@5  48.44 ( 49.01)
Epoch: [4][ 20/391]	Time  0.172 ( 0.199)	Data  0.001 ( 0.016)	Loss 3.1232e+00 (3.2669e+00)	Acc@1  16.41 ( 19.01)	Acc@5  53.91 ( 48.36)
Epoch: [4][ 30/391]	Time  0.168 ( 0.191)	Data  0.001 ( 0.012)	Loss 3.3028e+00 (3.2984e+00)	Acc@1  14.84 ( 18.60)	Acc@5  46.09 ( 47.58)
Epoch: [4][ 40/391]	Time  0.203 ( 0.191)	Data  0.001 ( 0.010)	Loss 3.2008e+00 (3.3015e+00)	Acc@1  21.09 ( 18.33)	Acc@5  50.78 ( 47.58)
Epoch: [4][ 50/391]	Time  0.203 ( 0.194)	Data  0.002 ( 0.010)	Loss 3.2306e+00 (3.3093e+00)	Acc@1  21.09 ( 18.41)	Acc@5  47.66 ( 47.33)
Epoch: [4][ 60/391]	Time  0.183 ( 0.195)	Data  0.001 ( 0.009)	Loss 3.3148e+00 (3.2972e+00)	Acc@1  16.41 ( 18.62)	Acc@5  42.97 ( 47.57)
Epoch: [4][ 70/391]	Time  0.172 ( 0.192)	Data  0.001 ( 0.008)	Loss 3.3348e+00 (3.2911e+00)	Acc@1  18.75 ( 18.58)	Acc@5  46.09 ( 47.67)
Epoch: [4][ 80/391]	Time  0.172 ( 0.189)	Data  0.001 ( 0.008)	Loss 3.3032e+00 (3.2843e+00)	Acc@1  19.53 ( 18.71)	Acc@5  51.56 ( 47.85)
Epoch: [4][ 90/391]	Time  0.203 ( 0.188)	Data  0.002 ( 0.008)	Loss 3.0518e+00 (3.2788e+00)	Acc@1  20.31 ( 18.78)	Acc@5  48.44 ( 47.73)
Epoch: [4][100/391]	Time  0.173 ( 0.187)	Data  0.001 ( 0.007)	Loss 3.0650e+00 (3.2726e+00)	Acc@1  21.88 ( 18.92)	Acc@5  57.81 ( 48.01)
Epoch: [4][110/391]	Time  0.182 ( 0.186)	Data  0.001 ( 0.007)	Loss 3.2683e+00 (3.2709e+00)	Acc@1  23.44 ( 19.00)	Acc@5  51.56 ( 48.11)
Epoch: [4][120/391]	Time  0.209 ( 0.186)	Data  0.001 ( 0.007)	Loss 3.1664e+00 (3.2677e+00)	Acc@1  25.00 ( 19.06)	Acc@5  53.12 ( 48.19)
Epoch: [4][130/391]	Time  0.165 ( 0.185)	Data  0.001 ( 0.007)	Loss 3.3408e+00 (3.2615e+00)	Acc@1  18.75 ( 19.31)	Acc@5  43.75 ( 48.35)
Epoch: [4][140/391]	Time  0.165 ( 0.184)	Data  0.001 ( 0.007)	Loss 3.0642e+00 (3.2557e+00)	Acc@1  28.91 ( 19.49)	Acc@5  53.91 ( 48.54)
Epoch: [4][150/391]	Time  0.166 ( 0.184)	Data  0.001 ( 0.006)	Loss 3.3821e+00 (3.2482e+00)	Acc@1  21.88 ( 19.64)	Acc@5  42.19 ( 48.77)
Epoch: [4][160/391]	Time  0.173 ( 0.183)	Data  0.001 ( 0.006)	Loss 3.2809e+00 (3.2447e+00)	Acc@1  17.97 ( 19.75)	Acc@5  42.19 ( 48.87)
Epoch: [4][170/391]	Time  0.176 ( 0.182)	Data  0.001 ( 0.006)	Loss 3.1521e+00 (3.2405e+00)	Acc@1  17.97 ( 19.81)	Acc@5  49.22 ( 48.98)
Epoch: [4][180/391]	Time  0.173 ( 0.182)	Data  0.001 ( 0.006)	Loss 3.1160e+00 (3.2393e+00)	Acc@1  17.97 ( 19.73)	Acc@5  51.56 ( 49.01)
Epoch: [4][190/391]	Time  0.219 ( 0.183)	Data  0.001 ( 0.006)	Loss 2.9947e+00 (3.2356e+00)	Acc@1  21.88 ( 19.71)	Acc@5  57.03 ( 49.11)
Epoch: [4][200/391]	Time  0.212 ( 0.185)	Data  0.001 ( 0.006)	Loss 3.0446e+00 (3.2291e+00)	Acc@1  18.75 ( 19.75)	Acc@5  55.47 ( 49.18)
Epoch: [4][210/391]	Time  0.174 ( 0.186)	Data  0.001 ( 0.006)	Loss 3.1034e+00 (3.2240e+00)	Acc@1  19.53 ( 19.85)	Acc@5  51.56 ( 49.35)
Epoch: [4][220/391]	Time  0.167 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.9547e+00 (3.2222e+00)	Acc@1  29.69 ( 19.92)	Acc@5  58.59 ( 49.41)
Epoch: [4][230/391]	Time  0.164 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.9175e+00 (3.2161e+00)	Acc@1  25.00 ( 19.95)	Acc@5  60.16 ( 49.61)
Epoch: [4][240/391]	Time  0.226 ( 0.185)	Data  0.001 ( 0.006)	Loss 3.0692e+00 (3.2142e+00)	Acc@1  21.88 ( 20.01)	Acc@5  52.34 ( 49.69)
Epoch: [4][250/391]	Time  0.220 ( 0.186)	Data  0.002 ( 0.006)	Loss 3.1586e+00 (3.2100e+00)	Acc@1  19.53 ( 20.13)	Acc@5  51.56 ( 49.82)
Epoch: [4][260/391]	Time  0.213 ( 0.188)	Data  0.001 ( 0.006)	Loss 3.0189e+00 (3.2066e+00)	Acc@1  21.88 ( 20.16)	Acc@5  54.69 ( 49.90)
Epoch: [4][270/391]	Time  0.181 ( 0.187)	Data  0.001 ( 0.006)	Loss 3.1927e+00 (3.2032e+00)	Acc@1  17.97 ( 20.24)	Acc@5  48.44 ( 50.03)
Epoch: [4][280/391]	Time  0.168 ( 0.187)	Data  0.001 ( 0.006)	Loss 3.0354e+00 (3.1976e+00)	Acc@1  22.66 ( 20.37)	Acc@5  53.91 ( 50.20)
Epoch: [4][290/391]	Time  0.191 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.8990e+00 (3.1918e+00)	Acc@1  25.78 ( 20.47)	Acc@5  54.69 ( 50.35)
Epoch: [4][300/391]	Time  0.198 ( 0.187)	Data  0.001 ( 0.006)	Loss 3.2290e+00 (3.1895e+00)	Acc@1  23.44 ( 20.61)	Acc@5  50.00 ( 50.43)
Epoch: [4][310/391]	Time  0.198 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.9800e+00 (3.1857e+00)	Acc@1  19.53 ( 20.63)	Acc@5  59.38 ( 50.55)
Epoch: [4][320/391]	Time  0.166 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.9806e+00 (3.1813e+00)	Acc@1  23.44 ( 20.73)	Acc@5  58.59 ( 50.66)
Epoch: [4][330/391]	Time  0.166 ( 0.186)	Data  0.001 ( 0.006)	Loss 3.0339e+00 (3.1764e+00)	Acc@1  25.78 ( 20.81)	Acc@5  51.56 ( 50.76)
Epoch: [4][340/391]	Time  0.151 ( 0.186)	Data  0.001 ( 0.006)	Loss 2.9496e+00 (3.1723e+00)	Acc@1  22.66 ( 20.88)	Acc@5  54.69 ( 50.84)
Epoch: [4][350/391]	Time  0.162 ( 0.186)	Data  0.001 ( 0.006)	Loss 2.8359e+00 (3.1668e+00)	Acc@1  28.91 ( 21.01)	Acc@5  55.47 ( 50.97)
Epoch: [4][360/391]	Time  0.182 ( 0.185)	Data  0.001 ( 0.006)	Loss 3.1406e+00 (3.1618e+00)	Acc@1  16.41 ( 21.08)	Acc@5  46.88 ( 51.05)
Epoch: [4][370/391]	Time  0.176 ( 0.185)	Data  0.002 ( 0.006)	Loss 2.7511e+00 (3.1557e+00)	Acc@1  25.78 ( 21.22)	Acc@5  58.59 ( 51.20)
Epoch: [4][380/391]	Time  0.172 ( 0.185)	Data  0.001 ( 0.006)	Loss 3.0470e+00 (3.1495e+00)	Acc@1  25.78 ( 21.36)	Acc@5  53.12 ( 51.38)
Epoch: [4][390/391]	Time  0.190 ( 0.185)	Data  0.001 ( 0.006)	Loss 3.0521e+00 (3.1447e+00)	Acc@1  18.75 ( 21.49)	Acc@5  58.75 ( 51.51)
## e[4] optimizer.zero_grad (sum) time: 0.3851292133331299
## e[4]       loss.backward (sum) time: 25.705409049987793
## e[4]      optimizer.step (sum) time: 3.7790417671203613
## epoch[4] training(only) time: 72.40060520172119
# Switched to evaluate mode...
Test: [  0/100]	Time  0.287 ( 0.287)	Loss 2.9821e+00 (2.9821e+00)	Acc@1  29.00 ( 29.00)	Acc@5  58.00 ( 58.00)
Test: [ 10/100]	Time  0.059 ( 0.080)	Loss 2.9334e+00 (2.9037e+00)	Acc@1  20.00 ( 27.18)	Acc@5  57.00 ( 58.73)
Test: [ 20/100]	Time  0.060 ( 0.070)	Loss 2.5941e+00 (2.8658e+00)	Acc@1  35.00 ( 27.43)	Acc@5  62.00 ( 59.00)
Test: [ 30/100]	Time  0.057 ( 0.066)	Loss 3.0600e+00 (2.8727e+00)	Acc@1  23.00 ( 26.58)	Acc@5  50.00 ( 58.42)
Test: [ 40/100]	Time  0.057 ( 0.065)	Loss 2.8576e+00 (2.8744e+00)	Acc@1  26.00 ( 26.78)	Acc@5  61.00 ( 58.61)
Test: [ 50/100]	Time  0.054 ( 0.063)	Loss 2.7964e+00 (2.8704e+00)	Acc@1  34.00 ( 26.92)	Acc@5  54.00 ( 58.69)
Test: [ 60/100]	Time  0.055 ( 0.061)	Loss 2.5857e+00 (2.8639e+00)	Acc@1  32.00 ( 26.90)	Acc@5  67.00 ( 58.84)
Test: [ 70/100]	Time  0.054 ( 0.060)	Loss 3.1357e+00 (2.8704e+00)	Acc@1  19.00 ( 26.82)	Acc@5  58.00 ( 58.94)
Test: [ 80/100]	Time  0.057 ( 0.060)	Loss 2.9851e+00 (2.8789e+00)	Acc@1  27.00 ( 26.81)	Acc@5  55.00 ( 58.73)
Test: [ 90/100]	Time  0.054 ( 0.059)	Loss 2.7303e+00 (2.8740e+00)	Acc@1  32.00 ( 26.92)	Acc@5  57.00 ( 58.87)
 * Acc@1 26.870 Acc@5 58.810
### epoch[4] execution time: 78.39407229423523
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.340 ( 0.340)	Data  0.164 ( 0.164)	Loss 2.7440e+00 (2.7440e+00)	Acc@1  27.34 ( 27.34)	Acc@5  60.94 ( 60.94)
Epoch: [5][ 10/391]	Time  0.221 ( 0.204)	Data  0.001 ( 0.019)	Loss 2.9029e+00 (2.8683e+00)	Acc@1  27.34 ( 26.70)	Acc@5  57.81 ( 59.23)
Epoch: [5][ 20/391]	Time  0.222 ( 0.212)	Data  0.001 ( 0.014)	Loss 2.9269e+00 (2.8918e+00)	Acc@1  24.22 ( 25.78)	Acc@5  59.38 ( 59.11)
Epoch: [5][ 30/391]	Time  0.199 ( 0.214)	Data  0.002 ( 0.012)	Loss 2.9228e+00 (2.8995e+00)	Acc@1  25.00 ( 26.03)	Acc@5  53.12 ( 58.80)
Epoch: [5][ 40/391]	Time  0.167 ( 0.204)	Data  0.001 ( 0.010)	Loss 2.6651e+00 (2.8941e+00)	Acc@1  31.25 ( 25.91)	Acc@5  63.28 ( 58.86)
Epoch: [5][ 50/391]	Time  0.174 ( 0.198)	Data  0.002 ( 0.009)	Loss 2.9792e+00 (2.8949e+00)	Acc@1  28.12 ( 25.98)	Acc@5  60.16 ( 58.78)
Epoch: [5][ 60/391]	Time  0.214 ( 0.195)	Data  0.001 ( 0.008)	Loss 3.1310e+00 (2.8975e+00)	Acc@1  23.44 ( 26.05)	Acc@5  52.34 ( 58.64)
Epoch: [5][ 70/391]	Time  0.221 ( 0.199)	Data  0.001 ( 0.008)	Loss 2.8403e+00 (2.8905e+00)	Acc@1  25.78 ( 26.29)	Acc@5  62.50 ( 59.01)
Epoch: [5][ 80/391]	Time  0.214 ( 0.202)	Data  0.001 ( 0.008)	Loss 2.8910e+00 (2.8928e+00)	Acc@1  29.69 ( 26.23)	Acc@5  57.03 ( 58.72)
Epoch: [5][ 90/391]	Time  0.181 ( 0.200)	Data  0.001 ( 0.008)	Loss 2.6809e+00 (2.8775e+00)	Acc@1  29.69 ( 26.47)	Acc@5  62.50 ( 59.11)
Epoch: [5][100/391]	Time  0.175 ( 0.198)	Data  0.001 ( 0.007)	Loss 3.1131e+00 (2.8742e+00)	Acc@1  21.88 ( 26.53)	Acc@5  57.81 ( 59.17)
Epoch: [5][110/391]	Time  0.175 ( 0.196)	Data  0.001 ( 0.007)	Loss 2.7318e+00 (2.8724e+00)	Acc@1  28.91 ( 26.66)	Acc@5  62.50 ( 59.21)
Epoch: [5][120/391]	Time  0.199 ( 0.195)	Data  0.001 ( 0.007)	Loss 3.0673e+00 (2.8672e+00)	Acc@1  23.44 ( 26.92)	Acc@5  53.91 ( 59.20)
Epoch: [5][130/391]	Time  0.197 ( 0.195)	Data  0.001 ( 0.007)	Loss 3.0916e+00 (2.8648e+00)	Acc@1  22.66 ( 26.99)	Acc@5  54.69 ( 59.30)
Epoch: [5][140/391]	Time  0.170 ( 0.195)	Data  0.001 ( 0.007)	Loss 2.5866e+00 (2.8570e+00)	Acc@1  37.50 ( 27.14)	Acc@5  61.72 ( 59.42)
Epoch: [5][150/391]	Time  0.172 ( 0.193)	Data  0.001 ( 0.007)	Loss 2.8562e+00 (2.8600e+00)	Acc@1  28.91 ( 27.00)	Acc@5  61.72 ( 59.35)
Epoch: [5][160/391]	Time  0.164 ( 0.192)	Data  0.001 ( 0.007)	Loss 2.5989e+00 (2.8558e+00)	Acc@1  32.03 ( 27.10)	Acc@5  63.28 ( 59.43)
Epoch: [5][170/391]	Time  0.177 ( 0.191)	Data  0.001 ( 0.006)	Loss 2.8022e+00 (2.8508e+00)	Acc@1  25.78 ( 27.17)	Acc@5  55.47 ( 59.47)
Epoch: [5][180/391]	Time  0.177 ( 0.190)	Data  0.001 ( 0.006)	Loss 2.7403e+00 (2.8480e+00)	Acc@1  25.00 ( 27.10)	Acc@5  58.59 ( 59.42)
Epoch: [5][190/391]	Time  0.168 ( 0.189)	Data  0.001 ( 0.006)	Loss 2.7366e+00 (2.8443e+00)	Acc@1  27.34 ( 27.12)	Acc@5  64.84 ( 59.52)
Epoch: [5][200/391]	Time  0.200 ( 0.189)	Data  0.001 ( 0.006)	Loss 3.0275e+00 (2.8417e+00)	Acc@1  25.78 ( 27.17)	Acc@5  53.91 ( 59.56)
Epoch: [5][210/391]	Time  0.173 ( 0.188)	Data  0.001 ( 0.006)	Loss 2.6619e+00 (2.8351e+00)	Acc@1  28.12 ( 27.26)	Acc@5  63.28 ( 59.73)
Epoch: [5][220/391]	Time  0.172 ( 0.188)	Data  0.002 ( 0.006)	Loss 2.6588e+00 (2.8310e+00)	Acc@1  28.91 ( 27.39)	Acc@5  60.94 ( 59.77)
Epoch: [5][230/391]	Time  0.174 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.5772e+00 (2.8295e+00)	Acc@1  32.81 ( 27.54)	Acc@5  67.19 ( 59.81)
Epoch: [5][240/391]	Time  0.168 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.7489e+00 (2.8246e+00)	Acc@1  28.91 ( 27.64)	Acc@5  66.41 ( 59.96)
Epoch: [5][250/391]	Time  0.182 ( 0.186)	Data  0.001 ( 0.006)	Loss 2.6458e+00 (2.8215e+00)	Acc@1  32.81 ( 27.82)	Acc@5  65.62 ( 60.07)
Epoch: [5][260/391]	Time  0.165 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.6259e+00 (2.8180e+00)	Acc@1  32.81 ( 27.82)	Acc@5  67.97 ( 60.18)
Epoch: [5][270/391]	Time  0.221 ( 0.186)	Data  0.001 ( 0.006)	Loss 2.9539e+00 (2.8147e+00)	Acc@1  29.69 ( 27.89)	Acc@5  53.91 ( 60.19)
Epoch: [5][280/391]	Time  0.222 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.6104e+00 (2.8100e+00)	Acc@1  32.03 ( 27.90)	Acc@5  61.72 ( 60.31)
Epoch: [5][290/391]	Time  0.179 ( 0.188)	Data  0.001 ( 0.006)	Loss 2.7703e+00 (2.8061e+00)	Acc@1  30.47 ( 27.99)	Acc@5  61.72 ( 60.43)
Epoch: [5][300/391]	Time  0.178 ( 0.188)	Data  0.001 ( 0.006)	Loss 2.7573e+00 (2.8000e+00)	Acc@1  32.81 ( 28.12)	Acc@5  60.16 ( 60.56)
Epoch: [5][310/391]	Time  0.175 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.6138e+00 (2.7963e+00)	Acc@1  29.69 ( 28.14)	Acc@5  65.62 ( 60.65)
Epoch: [5][320/391]	Time  0.217 ( 0.188)	Data  0.001 ( 0.006)	Loss 2.6189e+00 (2.7910e+00)	Acc@1  34.38 ( 28.22)	Acc@5  67.97 ( 60.82)
Epoch: [5][330/391]	Time  0.221 ( 0.189)	Data  0.001 ( 0.006)	Loss 2.5961e+00 (2.7846e+00)	Acc@1  29.69 ( 28.35)	Acc@5  70.31 ( 60.98)
Epoch: [5][340/391]	Time  0.207 ( 0.190)	Data  0.001 ( 0.006)	Loss 2.6785e+00 (2.7786e+00)	Acc@1  34.38 ( 28.50)	Acc@5  64.84 ( 61.09)
Epoch: [5][350/391]	Time  0.170 ( 0.189)	Data  0.001 ( 0.006)	Loss 2.3128e+00 (2.7730e+00)	Acc@1  37.50 ( 28.62)	Acc@5  66.41 ( 61.21)
Epoch: [5][360/391]	Time  0.169 ( 0.189)	Data  0.001 ( 0.006)	Loss 2.4475e+00 (2.7688e+00)	Acc@1  29.69 ( 28.68)	Acc@5  67.19 ( 61.30)
Epoch: [5][370/391]	Time  0.188 ( 0.188)	Data  0.001 ( 0.006)	Loss 2.4904e+00 (2.7636e+00)	Acc@1  28.91 ( 28.79)	Acc@5  73.44 ( 61.44)
Epoch: [5][380/391]	Time  0.197 ( 0.188)	Data  0.001 ( 0.006)	Loss 2.5357e+00 (2.7591e+00)	Acc@1  33.59 ( 28.88)	Acc@5  63.28 ( 61.54)
Epoch: [5][390/391]	Time  0.189 ( 0.189)	Data  0.001 ( 0.006)	Loss 2.3290e+00 (2.7555e+00)	Acc@1  38.75 ( 28.95)	Acc@5  70.00 ( 61.62)
## e[5] optimizer.zero_grad (sum) time: 0.38823723793029785
## e[5]       loss.backward (sum) time: 25.763380765914917
## e[5]      optimizer.step (sum) time: 3.811460256576538
## epoch[5] training(only) time: 73.84065556526184
# Switched to evaluate mode...
Test: [  0/100]	Time  0.220 ( 0.220)	Loss 2.5975e+00 (2.5975e+00)	Acc@1  38.00 ( 38.00)	Acc@5  65.00 ( 65.00)
Test: [ 10/100]	Time  0.056 ( 0.071)	Loss 2.6978e+00 (2.6663e+00)	Acc@1  26.00 ( 32.64)	Acc@5  66.00 ( 65.64)
Test: [ 20/100]	Time  0.054 ( 0.064)	Loss 2.4891e+00 (2.6446e+00)	Acc@1  38.00 ( 32.33)	Acc@5  69.00 ( 65.05)
Test: [ 30/100]	Time  0.055 ( 0.061)	Loss 2.8298e+00 (2.6634e+00)	Acc@1  29.00 ( 32.03)	Acc@5  59.00 ( 64.58)
Test: [ 40/100]	Time  0.054 ( 0.059)	Loss 2.8938e+00 (2.6798e+00)	Acc@1  27.00 ( 31.88)	Acc@5  62.00 ( 64.37)
Test: [ 50/100]	Time  0.055 ( 0.058)	Loss 2.5069e+00 (2.6925e+00)	Acc@1  40.00 ( 31.90)	Acc@5  63.00 ( 63.82)
Test: [ 60/100]	Time  0.055 ( 0.058)	Loss 2.5470e+00 (2.6774e+00)	Acc@1  35.00 ( 32.33)	Acc@5  65.00 ( 64.03)
Test: [ 70/100]	Time  0.054 ( 0.057)	Loss 2.9730e+00 (2.6935e+00)	Acc@1  23.00 ( 31.94)	Acc@5  57.00 ( 63.70)
Test: [ 80/100]	Time  0.050 ( 0.057)	Loss 2.6671e+00 (2.7039e+00)	Acc@1  32.00 ( 31.69)	Acc@5  64.00 ( 63.37)
Test: [ 90/100]	Time  0.057 ( 0.057)	Loss 2.4922e+00 (2.6966e+00)	Acc@1  39.00 ( 32.00)	Acc@5  66.00 ( 63.58)
 * Acc@1 31.790 Acc@5 63.560
### epoch[5] execution time: 79.66279816627502
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.443 ( 0.443)	Data  0.260 ( 0.260)	Loss 2.3725e+00 (2.3725e+00)	Acc@1  37.50 ( 37.50)	Acc@5  75.78 ( 75.78)
Epoch: [6][ 10/391]	Time  0.174 ( 0.201)	Data  0.002 ( 0.027)	Loss 2.4532e+00 (2.5113e+00)	Acc@1  36.72 ( 34.16)	Acc@5  65.62 ( 68.25)
Epoch: [6][ 20/391]	Time  0.169 ( 0.188)	Data  0.001 ( 0.016)	Loss 2.4869e+00 (2.4897e+00)	Acc@1  33.59 ( 34.45)	Acc@5  70.31 ( 68.82)
Epoch: [6][ 30/391]	Time  0.174 ( 0.186)	Data  0.001 ( 0.012)	Loss 2.4668e+00 (2.5280e+00)	Acc@1  37.50 ( 33.62)	Acc@5  66.41 ( 67.99)
Epoch: [6][ 40/391]	Time  0.203 ( 0.184)	Data  0.001 ( 0.010)	Loss 2.7879e+00 (2.5340e+00)	Acc@1  25.00 ( 33.25)	Acc@5  61.72 ( 67.28)
Epoch: [6][ 50/391]	Time  0.174 ( 0.183)	Data  0.001 ( 0.010)	Loss 2.6565e+00 (2.5378e+00)	Acc@1  30.47 ( 32.86)	Acc@5  64.06 ( 67.25)
Epoch: [6][ 60/391]	Time  0.156 ( 0.182)	Data  0.001 ( 0.009)	Loss 2.6531e+00 (2.5485e+00)	Acc@1  31.25 ( 32.85)	Acc@5  65.62 ( 66.87)
Epoch: [6][ 70/391]	Time  0.171 ( 0.180)	Data  0.001 ( 0.008)	Loss 2.8243e+00 (2.5495e+00)	Acc@1  28.12 ( 32.75)	Acc@5  63.28 ( 67.01)
Epoch: [6][ 80/391]	Time  0.162 ( 0.179)	Data  0.002 ( 0.008)	Loss 2.4775e+00 (2.5541e+00)	Acc@1  35.16 ( 32.72)	Acc@5  69.53 ( 66.68)
Epoch: [6][ 90/391]	Time  0.217 ( 0.179)	Data  0.001 ( 0.007)	Loss 2.4757e+00 (2.5495e+00)	Acc@1  37.50 ( 32.79)	Acc@5  71.88 ( 66.83)
Epoch: [6][100/391]	Time  0.225 ( 0.184)	Data  0.001 ( 0.007)	Loss 2.2494e+00 (2.5395e+00)	Acc@1  39.84 ( 33.16)	Acc@5  72.66 ( 67.03)
Epoch: [6][110/391]	Time  0.215 ( 0.187)	Data  0.001 ( 0.008)	Loss 2.7814e+00 (2.5472e+00)	Acc@1  27.34 ( 32.99)	Acc@5  61.72 ( 66.98)
Epoch: [6][120/391]	Time  0.169 ( 0.186)	Data  0.002 ( 0.007)	Loss 2.5592e+00 (2.5396e+00)	Acc@1  35.16 ( 33.18)	Acc@5  65.62 ( 67.14)
Epoch: [6][130/391]	Time  0.167 ( 0.185)	Data  0.001 ( 0.007)	Loss 2.3149e+00 (2.5293e+00)	Acc@1  35.94 ( 33.33)	Acc@5  68.75 ( 67.34)
Epoch: [6][140/391]	Time  0.173 ( 0.184)	Data  0.001 ( 0.007)	Loss 2.4165e+00 (2.5236e+00)	Acc@1  35.16 ( 33.56)	Acc@5  68.75 ( 67.24)
Epoch: [6][150/391]	Time  0.219 ( 0.186)	Data  0.002 ( 0.007)	Loss 2.4491e+00 (2.5247e+00)	Acc@1  33.59 ( 33.59)	Acc@5  70.31 ( 67.21)
Epoch: [6][160/391]	Time  0.217 ( 0.188)	Data  0.001 ( 0.007)	Loss 2.2694e+00 (2.5220e+00)	Acc@1  39.84 ( 33.65)	Acc@5  73.44 ( 67.34)
Epoch: [6][170/391]	Time  0.174 ( 0.189)	Data  0.001 ( 0.007)	Loss 2.4553e+00 (2.5158e+00)	Acc@1  34.38 ( 33.75)	Acc@5  69.53 ( 67.48)
Epoch: [6][180/391]	Time  0.169 ( 0.188)	Data  0.001 ( 0.007)	Loss 2.4347e+00 (2.5131e+00)	Acc@1  37.50 ( 33.77)	Acc@5  67.97 ( 67.53)
Epoch: [6][190/391]	Time  0.169 ( 0.187)	Data  0.002 ( 0.007)	Loss 2.5864e+00 (2.5103e+00)	Acc@1  28.12 ( 33.76)	Acc@5  68.75 ( 67.61)
Epoch: [6][200/391]	Time  0.189 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.5719e+00 (2.5051e+00)	Acc@1  32.81 ( 33.90)	Acc@5  63.28 ( 67.62)
Epoch: [6][210/391]	Time  0.193 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.4186e+00 (2.5010e+00)	Acc@1  28.12 ( 33.88)	Acc@5  71.88 ( 67.75)
Epoch: [6][220/391]	Time  0.167 ( 0.187)	Data  0.001 ( 0.006)	Loss 2.3921e+00 (2.4991e+00)	Acc@1  39.06 ( 33.98)	Acc@5  69.53 ( 67.79)
Epoch: [6][230/391]	Time  0.167 ( 0.186)	Data  0.001 ( 0.006)	Loss 2.5267e+00 (2.4976e+00)	Acc@1  29.69 ( 34.06)	Acc@5  66.41 ( 67.82)
Epoch: [6][240/391]	Time  0.167 ( 0.186)	Data  0.001 ( 0.006)	Loss 2.4369e+00 (2.4909e+00)	Acc@1  37.50 ( 34.18)	Acc@5  68.75 ( 67.96)
Epoch: [6][250/391]	Time  0.176 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.8798e+00 (2.4897e+00)	Acc@1  28.12 ( 34.16)	Acc@5  64.06 ( 67.98)
Epoch: [6][260/391]	Time  0.167 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.2637e+00 (2.4838e+00)	Acc@1  44.53 ( 34.36)	Acc@5  70.31 ( 68.06)
Epoch: [6][270/391]	Time  0.160 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.6171e+00 (2.4809e+00)	Acc@1  30.47 ( 34.44)	Acc@5  67.19 ( 68.12)
Epoch: [6][280/391]	Time  0.197 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.4106e+00 (2.4782e+00)	Acc@1  33.59 ( 34.50)	Acc@5  71.09 ( 68.16)
Epoch: [6][290/391]	Time  0.162 ( 0.184)	Data  0.001 ( 0.006)	Loss 2.2902e+00 (2.4732e+00)	Acc@1  35.16 ( 34.55)	Acc@5  72.66 ( 68.25)
Epoch: [6][300/391]	Time  0.166 ( 0.184)	Data  0.001 ( 0.006)	Loss 2.1333e+00 (2.4691e+00)	Acc@1  46.09 ( 34.59)	Acc@5  75.00 ( 68.29)
Epoch: [6][310/391]	Time  0.183 ( 0.184)	Data  0.001 ( 0.006)	Loss 2.3875e+00 (2.4645e+00)	Acc@1  35.94 ( 34.71)	Acc@5  71.09 ( 68.40)
Epoch: [6][320/391]	Time  0.166 ( 0.183)	Data  0.001 ( 0.006)	Loss 2.3399e+00 (2.4616e+00)	Acc@1  41.41 ( 34.78)	Acc@5  75.00 ( 68.46)
Epoch: [6][330/391]	Time  0.163 ( 0.183)	Data  0.001 ( 0.006)	Loss 2.5056e+00 (2.4585e+00)	Acc@1  39.06 ( 34.88)	Acc@5  69.53 ( 68.60)
Epoch: [6][340/391]	Time  0.175 ( 0.182)	Data  0.001 ( 0.006)	Loss 2.2591e+00 (2.4560e+00)	Acc@1  35.94 ( 34.90)	Acc@5  72.66 ( 68.68)
Epoch: [6][350/391]	Time  0.220 ( 0.183)	Data  0.001 ( 0.006)	Loss 2.3674e+00 (2.4527e+00)	Acc@1  36.72 ( 34.99)	Acc@5  69.53 ( 68.76)
Epoch: [6][360/391]	Time  0.218 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.3087e+00 (2.4482e+00)	Acc@1  36.72 ( 35.07)	Acc@5  71.09 ( 68.82)
Epoch: [6][370/391]	Time  0.172 ( 0.185)	Data  0.002 ( 0.006)	Loss 2.2323e+00 (2.4454e+00)	Acc@1  39.84 ( 35.17)	Acc@5  75.00 ( 68.89)
Epoch: [6][380/391]	Time  0.173 ( 0.184)	Data  0.002 ( 0.006)	Loss 2.2634e+00 (2.4417e+00)	Acc@1  42.19 ( 35.29)	Acc@5  71.09 ( 68.96)
Epoch: [6][390/391]	Time  0.171 ( 0.184)	Data  0.001 ( 0.006)	Loss 2.5558e+00 (2.4391e+00)	Acc@1  33.75 ( 35.36)	Acc@5  63.75 ( 69.03)
## e[6] optimizer.zero_grad (sum) time: 0.38713717460632324
## e[6]       loss.backward (sum) time: 25.509597539901733
## e[6]      optimizer.step (sum) time: 3.745424747467041
## epoch[6] training(only) time: 72.1644926071167
# Switched to evaluate mode...
Test: [  0/100]	Time  0.286 ( 0.286)	Loss 2.1914e+00 (2.1914e+00)	Acc@1  41.00 ( 41.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.066 ( 0.088)	Loss 2.2485e+00 (2.2316e+00)	Acc@1  36.00 ( 40.27)	Acc@5  75.00 ( 74.55)
Test: [ 20/100]	Time  0.066 ( 0.078)	Loss 2.1152e+00 (2.2358e+00)	Acc@1  43.00 ( 40.48)	Acc@5  78.00 ( 74.10)
Test: [ 30/100]	Time  0.069 ( 0.075)	Loss 2.4810e+00 (2.2272e+00)	Acc@1  32.00 ( 39.87)	Acc@5  68.00 ( 74.10)
Test: [ 40/100]	Time  0.068 ( 0.073)	Loss 2.3502e+00 (2.2392e+00)	Acc@1  42.00 ( 39.61)	Acc@5  76.00 ( 73.98)
Test: [ 50/100]	Time  0.067 ( 0.072)	Loss 2.0786e+00 (2.2555e+00)	Acc@1  49.00 ( 39.73)	Acc@5  75.00 ( 73.53)
Test: [ 60/100]	Time  0.069 ( 0.071)	Loss 2.1556e+00 (2.2468e+00)	Acc@1  41.00 ( 39.69)	Acc@5  75.00 ( 73.43)
Test: [ 70/100]	Time  0.065 ( 0.071)	Loss 2.5401e+00 (2.2587e+00)	Acc@1  36.00 ( 39.59)	Acc@5  70.00 ( 72.97)
Test: [ 80/100]	Time  0.060 ( 0.069)	Loss 2.5100e+00 (2.2751e+00)	Acc@1  37.00 ( 39.26)	Acc@5  67.00 ( 72.72)
Test: [ 90/100]	Time  0.063 ( 0.068)	Loss 2.2247e+00 (2.2698e+00)	Acc@1  42.00 ( 39.42)	Acc@5  71.00 ( 72.84)
 * Acc@1 39.290 Acc@5 72.720
### epoch[6] execution time: 79.03082942962646
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.379 ( 0.379)	Data  0.198 ( 0.198)	Loss 2.5476e+00 (2.5476e+00)	Acc@1  32.81 ( 32.81)	Acc@5  63.28 ( 63.28)
Epoch: [7][ 10/391]	Time  0.172 ( 0.193)	Data  0.001 ( 0.021)	Loss 2.3778e+00 (2.2124e+00)	Acc@1  35.94 ( 40.91)	Acc@5  67.19 ( 72.59)
Epoch: [7][ 20/391]	Time  0.179 ( 0.186)	Data  0.001 ( 0.013)	Loss 2.0950e+00 (2.1827e+00)	Acc@1  40.62 ( 41.33)	Acc@5  75.00 ( 73.51)
Epoch: [7][ 30/391]	Time  0.178 ( 0.185)	Data  0.001 ( 0.011)	Loss 2.0883e+00 (2.1740e+00)	Acc@1  37.50 ( 41.05)	Acc@5  79.69 ( 74.22)
Epoch: [7][ 40/391]	Time  0.179 ( 0.184)	Data  0.001 ( 0.010)	Loss 2.0830e+00 (2.1869e+00)	Acc@1  39.84 ( 40.51)	Acc@5  76.56 ( 73.84)
Epoch: [7][ 50/391]	Time  0.165 ( 0.181)	Data  0.001 ( 0.009)	Loss 2.2240e+00 (2.1996e+00)	Acc@1  44.53 ( 40.56)	Acc@5  72.66 ( 73.54)
Epoch: [7][ 60/391]	Time  0.168 ( 0.178)	Data  0.001 ( 0.008)	Loss 2.4773e+00 (2.2064e+00)	Acc@1  27.34 ( 40.34)	Acc@5  69.53 ( 73.60)
Epoch: [7][ 70/391]	Time  0.197 ( 0.177)	Data  0.001 ( 0.008)	Loss 2.3918e+00 (2.2110e+00)	Acc@1  38.28 ( 40.49)	Acc@5  70.31 ( 73.50)
Epoch: [7][ 80/391]	Time  0.178 ( 0.176)	Data  0.001 ( 0.007)	Loss 2.4896e+00 (2.2125e+00)	Acc@1  28.91 ( 40.45)	Acc@5  69.53 ( 73.54)
Epoch: [7][ 90/391]	Time  0.166 ( 0.177)	Data  0.001 ( 0.007)	Loss 2.2696e+00 (2.2107e+00)	Acc@1  43.75 ( 40.61)	Acc@5  68.75 ( 73.57)
Epoch: [7][100/391]	Time  0.180 ( 0.177)	Data  0.001 ( 0.007)	Loss 2.2126e+00 (2.2084e+00)	Acc@1  41.41 ( 40.53)	Acc@5  73.44 ( 73.65)
Epoch: [7][110/391]	Time  0.177 ( 0.177)	Data  0.001 ( 0.007)	Loss 2.1915e+00 (2.2084e+00)	Acc@1  39.06 ( 40.51)	Acc@5  73.44 ( 73.60)
Epoch: [7][120/391]	Time  0.175 ( 0.178)	Data  0.001 ( 0.006)	Loss 2.0939e+00 (2.2164e+00)	Acc@1  42.97 ( 40.26)	Acc@5  76.56 ( 73.50)
Epoch: [7][130/391]	Time  0.175 ( 0.178)	Data  0.001 ( 0.006)	Loss 2.1556e+00 (2.2149e+00)	Acc@1  46.09 ( 40.25)	Acc@5  75.00 ( 73.56)
Epoch: [7][140/391]	Time  0.163 ( 0.177)	Data  0.001 ( 0.006)	Loss 2.3512e+00 (2.2166e+00)	Acc@1  36.72 ( 40.26)	Acc@5  64.84 ( 73.53)
Epoch: [7][150/391]	Time  0.162 ( 0.177)	Data  0.002 ( 0.006)	Loss 2.1513e+00 (2.2132e+00)	Acc@1  39.06 ( 40.37)	Acc@5  76.56 ( 73.57)
Epoch: [7][160/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.006)	Loss 2.1289e+00 (2.2120e+00)	Acc@1  40.62 ( 40.30)	Acc@5  73.44 ( 73.51)
Epoch: [7][170/391]	Time  0.224 ( 0.178)	Data  0.001 ( 0.006)	Loss 2.1125e+00 (2.2062e+00)	Acc@1  39.84 ( 40.39)	Acc@5  77.34 ( 73.66)
Epoch: [7][180/391]	Time  0.215 ( 0.180)	Data  0.002 ( 0.006)	Loss 2.1883e+00 (2.2043e+00)	Acc@1  41.41 ( 40.39)	Acc@5  71.88 ( 73.66)
Epoch: [7][190/391]	Time  0.175 ( 0.181)	Data  0.001 ( 0.006)	Loss 2.1537e+00 (2.2031e+00)	Acc@1  44.53 ( 40.47)	Acc@5  75.78 ( 73.78)
Epoch: [7][200/391]	Time  0.169 ( 0.181)	Data  0.002 ( 0.006)	Loss 2.2758e+00 (2.2048e+00)	Acc@1  35.94 ( 40.41)	Acc@5  66.41 ( 73.74)
Epoch: [7][210/391]	Time  0.166 ( 0.181)	Data  0.001 ( 0.006)	Loss 2.1053e+00 (2.2031e+00)	Acc@1  42.19 ( 40.44)	Acc@5  75.00 ( 73.81)
Epoch: [7][220/391]	Time  0.222 ( 0.181)	Data  0.002 ( 0.006)	Loss 2.2828e+00 (2.2004e+00)	Acc@1  43.75 ( 40.53)	Acc@5  71.88 ( 73.89)
Epoch: [7][230/391]	Time  0.216 ( 0.183)	Data  0.002 ( 0.006)	Loss 2.1558e+00 (2.1964e+00)	Acc@1  41.41 ( 40.62)	Acc@5  75.78 ( 73.92)
Epoch: [7][240/391]	Time  0.223 ( 0.184)	Data  0.001 ( 0.006)	Loss 2.4789e+00 (2.1932e+00)	Acc@1  36.72 ( 40.77)	Acc@5  66.41 ( 73.95)
Epoch: [7][250/391]	Time  0.171 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.8603e+00 (2.1884e+00)	Acc@1  48.44 ( 40.85)	Acc@5  82.03 ( 74.10)
Epoch: [7][260/391]	Time  0.168 ( 0.184)	Data  0.002 ( 0.006)	Loss 2.0303e+00 (2.1861e+00)	Acc@1  44.53 ( 40.89)	Acc@5  78.12 ( 74.17)
Epoch: [7][270/391]	Time  0.170 ( 0.183)	Data  0.002 ( 0.006)	Loss 2.3638e+00 (2.1845e+00)	Acc@1  35.94 ( 40.92)	Acc@5  72.66 ( 74.25)
Epoch: [7][280/391]	Time  0.185 ( 0.183)	Data  0.001 ( 0.006)	Loss 2.0272e+00 (2.1821e+00)	Acc@1  50.00 ( 41.03)	Acc@5  74.22 ( 74.27)
Epoch: [7][290/391]	Time  0.181 ( 0.183)	Data  0.001 ( 0.006)	Loss 2.0322e+00 (2.1797e+00)	Acc@1  42.19 ( 41.03)	Acc@5  82.03 ( 74.30)
Epoch: [7][300/391]	Time  0.165 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.9387e+00 (2.1748e+00)	Acc@1  46.88 ( 41.16)	Acc@5  82.03 ( 74.39)
Epoch: [7][310/391]	Time  0.164 ( 0.182)	Data  0.001 ( 0.006)	Loss 2.3756e+00 (2.1711e+00)	Acc@1  42.19 ( 41.33)	Acc@5  71.88 ( 74.46)
Epoch: [7][320/391]	Time  0.169 ( 0.182)	Data  0.001 ( 0.006)	Loss 2.3155e+00 (2.1685e+00)	Acc@1  42.97 ( 41.43)	Acc@5  67.97 ( 74.45)
Epoch: [7][330/391]	Time  0.175 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.9995e+00 (2.1646e+00)	Acc@1  47.66 ( 41.52)	Acc@5  78.91 ( 74.54)
Epoch: [7][340/391]	Time  0.180 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.7221e+00 (2.1593e+00)	Acc@1  53.91 ( 41.68)	Acc@5  79.69 ( 74.61)
Epoch: [7][350/391]	Time  0.159 ( 0.181)	Data  0.001 ( 0.006)	Loss 2.4182e+00 (2.1572e+00)	Acc@1  36.72 ( 41.74)	Acc@5  70.31 ( 74.66)
Epoch: [7][360/391]	Time  0.171 ( 0.181)	Data  0.002 ( 0.006)	Loss 1.9726e+00 (2.1547e+00)	Acc@1  46.09 ( 41.77)	Acc@5  76.56 ( 74.73)
Epoch: [7][370/391]	Time  0.169 ( 0.181)	Data  0.001 ( 0.005)	Loss 1.9435e+00 (2.1504e+00)	Acc@1  50.78 ( 41.86)	Acc@5  80.47 ( 74.83)
Epoch: [7][380/391]	Time  0.173 ( 0.181)	Data  0.001 ( 0.005)	Loss 2.2291e+00 (2.1476e+00)	Acc@1  36.72 ( 41.89)	Acc@5  72.66 ( 74.88)
Epoch: [7][390/391]	Time  0.176 ( 0.181)	Data  0.001 ( 0.005)	Loss 1.9035e+00 (2.1434e+00)	Acc@1  42.50 ( 42.00)	Acc@5  81.25 ( 74.94)
## e[7] optimizer.zero_grad (sum) time: 0.3826260566711426
## e[7]       loss.backward (sum) time: 25.44305682182312
## e[7]      optimizer.step (sum) time: 3.724790096282959
## epoch[7] training(only) time: 70.90108442306519
# Switched to evaluate mode...
Test: [  0/100]	Time  0.263 ( 0.263)	Loss 2.2814e+00 (2.2814e+00)	Acc@1  37.00 ( 37.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.053 ( 0.073)	Loss 2.3120e+00 (2.1775e+00)	Acc@1  34.00 ( 43.09)	Acc@5  75.00 ( 74.27)
Test: [ 20/100]	Time  0.052 ( 0.063)	Loss 2.0064e+00 (2.1357e+00)	Acc@1  48.00 ( 42.86)	Acc@5  77.00 ( 75.71)
Test: [ 30/100]	Time  0.053 ( 0.060)	Loss 2.5288e+00 (2.1529e+00)	Acc@1  33.00 ( 42.06)	Acc@5  67.00 ( 75.61)
Test: [ 40/100]	Time  0.053 ( 0.058)	Loss 2.2930e+00 (2.1529e+00)	Acc@1  46.00 ( 42.56)	Acc@5  71.00 ( 75.37)
Test: [ 50/100]	Time  0.052 ( 0.057)	Loss 2.0426e+00 (2.1652e+00)	Acc@1  51.00 ( 42.78)	Acc@5  74.00 ( 74.69)
Test: [ 60/100]	Time  0.053 ( 0.057)	Loss 1.9536e+00 (2.1462e+00)	Acc@1  45.00 ( 43.16)	Acc@5  77.00 ( 74.93)
Test: [ 70/100]	Time  0.052 ( 0.056)	Loss 2.4974e+00 (2.1477e+00)	Acc@1  38.00 ( 43.27)	Acc@5  71.00 ( 74.87)
Test: [ 80/100]	Time  0.066 ( 0.056)	Loss 2.2984e+00 (2.1624e+00)	Acc@1  39.00 ( 42.78)	Acc@5  75.00 ( 74.70)
Test: [ 90/100]	Time  0.067 ( 0.057)	Loss 2.2716e+00 (2.1562e+00)	Acc@1  43.00 ( 42.95)	Acc@5  72.00 ( 74.76)
 * Acc@1 42.640 Acc@5 74.620
### epoch[7] execution time: 76.82954907417297
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.384 ( 0.384)	Data  0.194 ( 0.194)	Loss 1.8790e+00 (1.8790e+00)	Acc@1  46.88 ( 46.88)	Acc@5  79.69 ( 79.69)
Epoch: [8][ 10/391]	Time  0.225 ( 0.238)	Data  0.001 ( 0.025)	Loss 2.2341e+00 (1.9589e+00)	Acc@1  38.28 ( 47.30)	Acc@5  75.00 ( 79.40)
Epoch: [8][ 20/391]	Time  0.175 ( 0.217)	Data  0.001 ( 0.017)	Loss 1.9024e+00 (1.9554e+00)	Acc@1  44.53 ( 46.65)	Acc@5  77.34 ( 79.02)
Epoch: [8][ 30/391]	Time  0.178 ( 0.204)	Data  0.001 ( 0.013)	Loss 1.5809e+00 (1.9460e+00)	Acc@1  54.69 ( 46.98)	Acc@5  83.59 ( 79.31)
Epoch: [8][ 40/391]	Time  0.169 ( 0.196)	Data  0.001 ( 0.011)	Loss 2.0912e+00 (1.9636e+00)	Acc@1  38.28 ( 46.04)	Acc@5  79.69 ( 78.83)
Epoch: [8][ 50/391]	Time  0.223 ( 0.200)	Data  0.001 ( 0.010)	Loss 2.0002e+00 (1.9777e+00)	Acc@1  40.62 ( 45.47)	Acc@5  78.12 ( 78.52)
Epoch: [8][ 60/391]	Time  0.226 ( 0.204)	Data  0.001 ( 0.009)	Loss 1.7666e+00 (1.9688e+00)	Acc@1  48.44 ( 45.74)	Acc@5  84.38 ( 78.65)
Epoch: [8][ 70/391]	Time  0.176 ( 0.203)	Data  0.001 ( 0.009)	Loss 1.8471e+00 (1.9745e+00)	Acc@1  50.00 ( 45.80)	Acc@5  81.25 ( 78.44)
Epoch: [8][ 80/391]	Time  0.177 ( 0.199)	Data  0.002 ( 0.008)	Loss 2.0855e+00 (1.9719e+00)	Acc@1  38.28 ( 45.81)	Acc@5  82.03 ( 78.61)
Epoch: [8][ 90/391]	Time  0.169 ( 0.197)	Data  0.001 ( 0.008)	Loss 2.2072e+00 (1.9798e+00)	Acc@1  37.50 ( 45.52)	Acc@5  72.66 ( 78.39)
Epoch: [8][100/391]	Time  0.183 ( 0.194)	Data  0.001 ( 0.008)	Loss 1.9726e+00 (1.9890e+00)	Acc@1  39.06 ( 45.30)	Acc@5  77.34 ( 78.24)
Epoch: [8][110/391]	Time  0.180 ( 0.193)	Data  0.001 ( 0.007)	Loss 1.9740e+00 (1.9859e+00)	Acc@1  49.22 ( 45.39)	Acc@5  78.12 ( 78.33)
Epoch: [8][120/391]	Time  0.165 ( 0.192)	Data  0.001 ( 0.007)	Loss 1.8090e+00 (1.9801e+00)	Acc@1  53.12 ( 45.56)	Acc@5  79.69 ( 78.41)
Epoch: [8][130/391]	Time  0.166 ( 0.190)	Data  0.001 ( 0.007)	Loss 1.8863e+00 (1.9763e+00)	Acc@1  44.53 ( 45.68)	Acc@5  74.22 ( 78.44)
Epoch: [8][140/391]	Time  0.161 ( 0.188)	Data  0.001 ( 0.007)	Loss 1.9359e+00 (1.9747e+00)	Acc@1  46.88 ( 45.64)	Acc@5  75.00 ( 78.41)
Epoch: [8][150/391]	Time  0.171 ( 0.187)	Data  0.001 ( 0.007)	Loss 1.8766e+00 (1.9723e+00)	Acc@1  50.00 ( 45.67)	Acc@5  79.69 ( 78.43)
Epoch: [8][160/391]	Time  0.181 ( 0.187)	Data  0.001 ( 0.007)	Loss 1.8658e+00 (1.9708e+00)	Acc@1  49.22 ( 45.80)	Acc@5  80.47 ( 78.43)
Epoch: [8][170/391]	Time  0.171 ( 0.186)	Data  0.001 ( 0.007)	Loss 1.8168e+00 (1.9712e+00)	Acc@1  50.78 ( 45.84)	Acc@5  76.56 ( 78.33)
Epoch: [8][180/391]	Time  0.175 ( 0.186)	Data  0.002 ( 0.007)	Loss 1.7676e+00 (1.9655e+00)	Acc@1  48.44 ( 45.88)	Acc@5  84.38 ( 78.47)
Epoch: [8][190/391]	Time  0.166 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.1421e+00 (1.9650e+00)	Acc@1  37.50 ( 45.82)	Acc@5  80.47 ( 78.55)
Epoch: [8][200/391]	Time  0.175 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.0214e+00 (1.9632e+00)	Acc@1  42.19 ( 45.81)	Acc@5  78.12 ( 78.58)
Epoch: [8][210/391]	Time  0.161 ( 0.184)	Data  0.001 ( 0.006)	Loss 2.0789e+00 (1.9637e+00)	Acc@1  38.28 ( 45.80)	Acc@5  76.56 ( 78.57)
Epoch: [8][220/391]	Time  0.171 ( 0.183)	Data  0.001 ( 0.006)	Loss 2.0159e+00 (1.9629e+00)	Acc@1  35.16 ( 45.82)	Acc@5  78.91 ( 78.62)
Epoch: [8][230/391]	Time  0.169 ( 0.183)	Data  0.001 ( 0.006)	Loss 2.0058e+00 (1.9593e+00)	Acc@1  43.75 ( 45.89)	Acc@5  78.91 ( 78.70)
Epoch: [8][240/391]	Time  0.163 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.7359e+00 (1.9536e+00)	Acc@1  46.88 ( 46.02)	Acc@5  85.94 ( 78.85)
Epoch: [8][250/391]	Time  0.218 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.7427e+00 (1.9516e+00)	Acc@1  49.22 ( 46.03)	Acc@5  80.47 ( 78.85)
Epoch: [8][260/391]	Time  0.218 ( 0.185)	Data  0.001 ( 0.006)	Loss 2.0975e+00 (1.9497e+00)	Acc@1  36.72 ( 45.99)	Acc@5  75.78 ( 78.90)
Epoch: [8][270/391]	Time  0.177 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.8783e+00 (1.9466e+00)	Acc@1  48.44 ( 46.10)	Acc@5  82.03 ( 78.96)
Epoch: [8][280/391]	Time  0.176 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.9683e+00 (1.9431e+00)	Acc@1  45.31 ( 46.23)	Acc@5  75.78 ( 79.01)
Epoch: [8][290/391]	Time  0.174 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.8729e+00 (1.9387e+00)	Acc@1  46.09 ( 46.33)	Acc@5  82.03 ( 79.09)
Epoch: [8][300/391]	Time  0.221 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.9249e+00 (1.9406e+00)	Acc@1  44.53 ( 46.22)	Acc@5  78.12 ( 79.05)
Epoch: [8][310/391]	Time  0.216 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.9463e+00 (1.9418e+00)	Acc@1  42.19 ( 46.18)	Acc@5  78.12 ( 79.00)
Epoch: [8][320/391]	Time  0.175 ( 0.187)	Data  0.002 ( 0.006)	Loss 2.0511e+00 (1.9404e+00)	Acc@1  47.66 ( 46.23)	Acc@5  79.69 ( 79.01)
Epoch: [8][330/391]	Time  0.168 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.7913e+00 (1.9356e+00)	Acc@1  50.00 ( 46.35)	Acc@5  82.03 ( 79.07)
Epoch: [8][340/391]	Time  0.167 ( 0.187)	Data  0.002 ( 0.006)	Loss 2.0996e+00 (1.9335e+00)	Acc@1  46.88 ( 46.41)	Acc@5  78.91 ( 79.10)
Epoch: [8][350/391]	Time  0.180 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.5532e+00 (1.9310e+00)	Acc@1  54.69 ( 46.48)	Acc@5  82.81 ( 79.10)
Epoch: [8][360/391]	Time  0.178 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.8045e+00 (1.9278e+00)	Acc@1  50.00 ( 46.62)	Acc@5  81.25 ( 79.17)
Epoch: [8][370/391]	Time  0.173 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.8259e+00 (1.9266e+00)	Acc@1  49.22 ( 46.66)	Acc@5  81.25 ( 79.16)
Epoch: [8][380/391]	Time  0.165 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.8184e+00 (1.9264e+00)	Acc@1  49.22 ( 46.65)	Acc@5  78.91 ( 79.13)
Epoch: [8][390/391]	Time  0.161 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.9945e+00 (1.9264e+00)	Acc@1  48.75 ( 46.67)	Acc@5  73.75 ( 79.12)
## e[8] optimizer.zero_grad (sum) time: 0.37839388847351074
## e[8]       loss.backward (sum) time: 24.962971687316895
## e[8]      optimizer.step (sum) time: 3.7198333740234375
## epoch[8] training(only) time: 72.39279007911682
# Switched to evaluate mode...
Test: [  0/100]	Time  0.242 ( 0.242)	Loss 1.8804e+00 (1.8804e+00)	Acc@1  53.00 ( 53.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.049 ( 0.069)	Loss 1.9688e+00 (1.8965e+00)	Acc@1  52.00 ( 49.91)	Acc@5  85.00 ( 80.18)
Test: [ 20/100]	Time  0.057 ( 0.064)	Loss 1.6322e+00 (1.8789e+00)	Acc@1  49.00 ( 48.67)	Acc@5  88.00 ( 80.95)
Test: [ 30/100]	Time  0.057 ( 0.062)	Loss 1.9106e+00 (1.8665e+00)	Acc@1  43.00 ( 48.61)	Acc@5  82.00 ( 81.00)
Test: [ 40/100]	Time  0.058 ( 0.061)	Loss 1.9195e+00 (1.8760e+00)	Acc@1  51.00 ( 48.61)	Acc@5  78.00 ( 80.76)
Test: [ 50/100]	Time  0.057 ( 0.060)	Loss 1.9287e+00 (1.8974e+00)	Acc@1  51.00 ( 48.59)	Acc@5  75.00 ( 80.10)
Test: [ 60/100]	Time  0.058 ( 0.060)	Loss 1.7724e+00 (1.8873e+00)	Acc@1  52.00 ( 48.69)	Acc@5  78.00 ( 80.15)
Test: [ 70/100]	Time  0.057 ( 0.060)	Loss 2.1951e+00 (1.8922e+00)	Acc@1  42.00 ( 48.80)	Acc@5  79.00 ( 79.90)
Test: [ 80/100]	Time  0.057 ( 0.060)	Loss 1.8560e+00 (1.8954e+00)	Acc@1  46.00 ( 48.53)	Acc@5  78.00 ( 79.79)
Test: [ 90/100]	Time  0.056 ( 0.059)	Loss 1.9023e+00 (1.8849e+00)	Acc@1  53.00 ( 48.84)	Acc@5  80.00 ( 80.10)
 * Acc@1 48.950 Acc@5 80.050
### epoch[8] execution time: 78.42978405952454
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.423 ( 0.423)	Data  0.236 ( 0.236)	Loss 2.0837e+00 (2.0837e+00)	Acc@1  51.56 ( 51.56)	Acc@5  74.22 ( 74.22)
Epoch: [9][ 10/391]	Time  0.171 ( 0.196)	Data  0.002 ( 0.025)	Loss 1.7706e+00 (1.8565e+00)	Acc@1  50.78 ( 49.01)	Acc@5  80.47 ( 79.69)
Epoch: [9][ 20/391]	Time  0.174 ( 0.189)	Data  0.001 ( 0.015)	Loss 1.7606e+00 (1.8215e+00)	Acc@1  53.91 ( 49.59)	Acc@5  80.47 ( 80.58)
Epoch: [9][ 30/391]	Time  0.173 ( 0.184)	Data  0.001 ( 0.012)	Loss 1.8131e+00 (1.8103e+00)	Acc@1  49.22 ( 49.42)	Acc@5  76.56 ( 81.10)
Epoch: [9][ 40/391]	Time  0.161 ( 0.179)	Data  0.001 ( 0.010)	Loss 1.9494e+00 (1.7784e+00)	Acc@1  49.22 ( 50.25)	Acc@5  77.34 ( 81.76)
Epoch: [9][ 50/391]	Time  0.158 ( 0.175)	Data  0.001 ( 0.009)	Loss 1.7239e+00 (1.7827e+00)	Acc@1  50.78 ( 49.97)	Acc@5  83.59 ( 81.85)
Epoch: [9][ 60/391]	Time  0.180 ( 0.173)	Data  0.001 ( 0.008)	Loss 1.8211e+00 (1.7812e+00)	Acc@1  52.34 ( 49.90)	Acc@5  81.25 ( 81.78)
Epoch: [9][ 70/391]	Time  0.227 ( 0.180)	Data  0.001 ( 0.008)	Loss 1.8060e+00 (1.7872e+00)	Acc@1  55.47 ( 49.82)	Acc@5  78.12 ( 81.75)
Epoch: [9][ 80/391]	Time  0.219 ( 0.185)	Data  0.001 ( 0.008)	Loss 1.9766e+00 (1.8002e+00)	Acc@1  43.75 ( 49.47)	Acc@5  78.12 ( 81.53)
Epoch: [9][ 90/391]	Time  0.175 ( 0.186)	Data  0.001 ( 0.008)	Loss 1.5484e+00 (1.7944e+00)	Acc@1  57.03 ( 49.54)	Acc@5  83.59 ( 81.62)
Epoch: [9][100/391]	Time  0.176 ( 0.185)	Data  0.001 ( 0.008)	Loss 1.6504e+00 (1.7864e+00)	Acc@1  56.25 ( 49.95)	Acc@5  83.59 ( 81.78)
Epoch: [9][110/391]	Time  0.177 ( 0.184)	Data  0.001 ( 0.007)	Loss 1.7972e+00 (1.7832e+00)	Acc@1  54.69 ( 50.11)	Acc@5  78.12 ( 81.87)
Epoch: [9][120/391]	Time  0.212 ( 0.187)	Data  0.001 ( 0.007)	Loss 1.7504e+00 (1.7790e+00)	Acc@1  45.31 ( 50.17)	Acc@5  83.59 ( 81.95)
Epoch: [9][130/391]	Time  0.219 ( 0.189)	Data  0.002 ( 0.007)	Loss 1.5554e+00 (1.7768e+00)	Acc@1  57.81 ( 50.41)	Acc@5  86.72 ( 81.92)
Epoch: [9][140/391]	Time  0.175 ( 0.190)	Data  0.001 ( 0.007)	Loss 1.7059e+00 (1.7744e+00)	Acc@1  53.91 ( 50.44)	Acc@5  79.69 ( 81.87)
Epoch: [9][150/391]	Time  0.180 ( 0.189)	Data  0.001 ( 0.007)	Loss 1.6546e+00 (1.7670e+00)	Acc@1  55.47 ( 50.47)	Acc@5  83.59 ( 82.08)
Epoch: [9][160/391]	Time  0.182 ( 0.189)	Data  0.001 ( 0.007)	Loss 1.7955e+00 (1.7633e+00)	Acc@1  49.22 ( 50.66)	Acc@5  82.03 ( 82.18)
Epoch: [9][170/391]	Time  0.169 ( 0.187)	Data  0.001 ( 0.007)	Loss 1.8616e+00 (1.7648e+00)	Acc@1  44.53 ( 50.65)	Acc@5  82.03 ( 82.19)
Epoch: [9][180/391]	Time  0.165 ( 0.186)	Data  0.001 ( 0.007)	Loss 1.6734e+00 (1.7652e+00)	Acc@1  49.22 ( 50.66)	Acc@5  85.94 ( 82.19)
Epoch: [9][190/391]	Time  0.156 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.8285e+00 (1.7673e+00)	Acc@1  52.34 ( 50.64)	Acc@5  83.59 ( 82.19)
Epoch: [9][200/391]	Time  0.156 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.8364e+00 (1.7665e+00)	Acc@1  50.78 ( 50.67)	Acc@5  83.59 ( 82.20)
Epoch: [9][210/391]	Time  0.162 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.5668e+00 (1.7671e+00)	Acc@1  56.25 ( 50.69)	Acc@5  85.94 ( 82.18)
Epoch: [9][220/391]	Time  0.164 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.9764e+00 (1.7685e+00)	Acc@1  46.09 ( 50.70)	Acc@5  79.69 ( 82.16)
Epoch: [9][230/391]	Time  0.171 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.9005e+00 (1.7661e+00)	Acc@1  47.66 ( 50.74)	Acc@5  76.56 ( 82.22)
Epoch: [9][240/391]	Time  0.172 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.8529e+00 (1.7626e+00)	Acc@1  50.00 ( 50.87)	Acc@5  79.69 ( 82.25)
Epoch: [9][250/391]	Time  0.207 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.8366e+00 (1.7608e+00)	Acc@1  44.53 ( 50.85)	Acc@5  80.47 ( 82.27)
Epoch: [9][260/391]	Time  0.173 ( 0.182)	Data  0.002 ( 0.006)	Loss 1.5844e+00 (1.7595e+00)	Acc@1  56.25 ( 50.88)	Acc@5  87.50 ( 82.29)
Epoch: [9][270/391]	Time  0.174 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.5632e+00 (1.7572e+00)	Acc@1  56.25 ( 50.93)	Acc@5  85.94 ( 82.35)
Epoch: [9][280/391]	Time  0.175 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.6524e+00 (1.7562e+00)	Acc@1  51.56 ( 50.91)	Acc@5  85.94 ( 82.40)
Epoch: [9][290/391]	Time  0.162 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.8707e+00 (1.7565e+00)	Acc@1  48.44 ( 50.94)	Acc@5  82.81 ( 82.40)
Epoch: [9][300/391]	Time  0.157 ( 0.180)	Data  0.001 ( 0.006)	Loss 2.2284e+00 (1.7554e+00)	Acc@1  42.19 ( 50.99)	Acc@5  74.22 ( 82.42)
Epoch: [9][310/391]	Time  0.158 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.7298e+00 (1.7547e+00)	Acc@1  53.91 ( 50.97)	Acc@5  82.03 ( 82.43)
Epoch: [9][320/391]	Time  0.229 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.6584e+00 (1.7524e+00)	Acc@1  53.91 ( 51.04)	Acc@5  85.16 ( 82.47)
Epoch: [9][330/391]	Time  0.223 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.5656e+00 (1.7524e+00)	Acc@1  52.34 ( 51.02)	Acc@5  85.94 ( 82.46)
Epoch: [9][340/391]	Time  0.179 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.6986e+00 (1.7499e+00)	Acc@1  50.78 ( 51.07)	Acc@5  85.94 ( 82.49)
Epoch: [9][350/391]	Time  0.175 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.6471e+00 (1.7479e+00)	Acc@1  55.47 ( 51.13)	Acc@5  83.59 ( 82.54)
Epoch: [9][360/391]	Time  0.175 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.7625e+00 (1.7441e+00)	Acc@1  53.12 ( 51.23)	Acc@5  81.25 ( 82.64)
Epoch: [9][370/391]	Time  0.221 ( 0.183)	Data  0.002 ( 0.006)	Loss 1.6158e+00 (1.7414e+00)	Acc@1  53.91 ( 51.28)	Acc@5  83.59 ( 82.67)
Epoch: [9][380/391]	Time  0.218 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.6710e+00 (1.7383e+00)	Acc@1  53.91 ( 51.37)	Acc@5  85.94 ( 82.71)
Epoch: [9][390/391]	Time  0.175 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.7205e+00 (1.7384e+00)	Acc@1  52.50 ( 51.41)	Acc@5  78.75 ( 82.69)
## e[9] optimizer.zero_grad (sum) time: 0.37360429763793945
## e[9]       loss.backward (sum) time: 24.87966775894165
## e[9]      optimizer.step (sum) time: 3.6804473400115967
## epoch[9] training(only) time: 72.18620777130127
# Switched to evaluate mode...
Test: [  0/100]	Time  0.271 ( 0.271)	Loss 1.9801e+00 (1.9801e+00)	Acc@1  54.00 ( 54.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.061 ( 0.080)	Loss 2.0369e+00 (1.9975e+00)	Acc@1  44.00 ( 48.73)	Acc@5  78.00 ( 79.64)
Test: [ 20/100]	Time  0.061 ( 0.070)	Loss 1.7860e+00 (2.0003e+00)	Acc@1  51.00 ( 47.95)	Acc@5  81.00 ( 79.24)
Test: [ 30/100]	Time  0.059 ( 0.067)	Loss 1.7416e+00 (1.9623e+00)	Acc@1  44.00 ( 48.03)	Acc@5  82.00 ( 79.68)
Test: [ 40/100]	Time  0.059 ( 0.065)	Loss 2.0486e+00 (1.9606e+00)	Acc@1  46.00 ( 47.71)	Acc@5  81.00 ( 79.93)
Test: [ 50/100]	Time  0.059 ( 0.064)	Loss 2.0522e+00 (1.9844e+00)	Acc@1  45.00 ( 47.49)	Acc@5  75.00 ( 79.53)
Test: [ 60/100]	Time  0.057 ( 0.063)	Loss 2.0781e+00 (1.9714e+00)	Acc@1  50.00 ( 47.75)	Acc@5  76.00 ( 79.61)
Test: [ 70/100]	Time  0.054 ( 0.062)	Loss 2.0835e+00 (1.9713e+00)	Acc@1  47.00 ( 47.97)	Acc@5  78.00 ( 79.51)
Test: [ 80/100]	Time  0.054 ( 0.061)	Loss 2.2393e+00 (1.9750e+00)	Acc@1  38.00 ( 47.81)	Acc@5  73.00 ( 79.35)
Test: [ 90/100]	Time  0.054 ( 0.060)	Loss 2.1630e+00 (1.9669e+00)	Acc@1  49.00 ( 48.04)	Acc@5  78.00 ( 79.43)
 * Acc@1 48.070 Acc@5 79.540
### epoch[9] execution time: 78.24437713623047
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.369 ( 0.369)	Data  0.231 ( 0.231)	Loss 1.5955e+00 (1.5955e+00)	Acc@1  58.59 ( 58.59)	Acc@5  82.03 ( 82.03)
Epoch: [10][ 10/391]	Time  0.159 ( 0.185)	Data  0.001 ( 0.026)	Loss 1.6999e+00 (1.6489e+00)	Acc@1  53.91 ( 52.98)	Acc@5  80.47 ( 83.52)
Epoch: [10][ 20/391]	Time  0.161 ( 0.173)	Data  0.001 ( 0.016)	Loss 1.5727e+00 (1.6531e+00)	Acc@1  53.91 ( 53.05)	Acc@5  85.94 ( 83.63)
Epoch: [10][ 30/391]	Time  0.158 ( 0.169)	Data  0.001 ( 0.012)	Loss 1.5782e+00 (1.6597e+00)	Acc@1  56.25 ( 53.40)	Acc@5  85.16 ( 83.47)
Epoch: [10][ 40/391]	Time  0.178 ( 0.170)	Data  0.001 ( 0.010)	Loss 1.6142e+00 (1.6670e+00)	Acc@1  52.34 ( 53.03)	Acc@5  83.59 ( 83.54)
Epoch: [10][ 50/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.009)	Loss 1.6281e+00 (1.6577e+00)	Acc@1  50.78 ( 52.99)	Acc@5  82.81 ( 83.79)
Epoch: [10][ 60/391]	Time  0.163 ( 0.171)	Data  0.002 ( 0.009)	Loss 1.5722e+00 (1.6491e+00)	Acc@1  53.12 ( 53.39)	Acc@5  85.94 ( 83.98)
Epoch: [10][ 70/391]	Time  0.193 ( 0.172)	Data  0.001 ( 0.008)	Loss 1.4880e+00 (1.6460e+00)	Acc@1  56.25 ( 53.58)	Acc@5  85.16 ( 83.93)
Epoch: [10][ 80/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.008)	Loss 1.6597e+00 (1.6469e+00)	Acc@1  54.69 ( 53.52)	Acc@5  85.16 ( 83.98)
Epoch: [10][ 90/391]	Time  0.170 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.5900e+00 (1.6416e+00)	Acc@1  55.47 ( 53.51)	Acc@5  85.94 ( 84.16)
Epoch: [10][100/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.6087e+00 (1.6380e+00)	Acc@1  53.91 ( 53.81)	Acc@5  81.25 ( 84.14)
Epoch: [10][110/391]	Time  0.160 ( 0.173)	Data  0.001 ( 0.007)	Loss 1.6015e+00 (1.6417e+00)	Acc@1  53.91 ( 53.70)	Acc@5  83.59 ( 84.02)
Epoch: [10][120/391]	Time  0.158 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.5936e+00 (1.6381e+00)	Acc@1  50.78 ( 53.71)	Acc@5  85.16 ( 84.15)
Epoch: [10][130/391]	Time  0.179 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.5444e+00 (1.6335e+00)	Acc@1  60.16 ( 53.87)	Acc@5  86.72 ( 84.20)
Epoch: [10][140/391]	Time  0.218 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.7847e+00 (1.6349e+00)	Acc@1  52.34 ( 53.78)	Acc@5  82.03 ( 84.19)
Epoch: [10][150/391]	Time  0.215 ( 0.177)	Data  0.001 ( 0.007)	Loss 1.6370e+00 (1.6320e+00)	Acc@1  53.12 ( 53.89)	Acc@5  82.81 ( 84.21)
Epoch: [10][160/391]	Time  0.177 ( 0.178)	Data  0.001 ( 0.007)	Loss 1.8962e+00 (1.6303e+00)	Acc@1  46.09 ( 53.95)	Acc@5  81.25 ( 84.22)
Epoch: [10][170/391]	Time  0.174 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.6722e+00 (1.6265e+00)	Acc@1  51.56 ( 54.01)	Acc@5  80.47 ( 84.24)
Epoch: [10][180/391]	Time  0.165 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.6249e+00 (1.6259e+00)	Acc@1  54.69 ( 53.97)	Acc@5  85.94 ( 84.28)
Epoch: [10][190/391]	Time  0.226 ( 0.180)	Data  0.001 ( 0.006)	Loss 1.9908e+00 (1.6249e+00)	Acc@1  47.66 ( 54.04)	Acc@5  78.91 ( 84.36)
Epoch: [10][200/391]	Time  0.218 ( 0.181)	Data  0.002 ( 0.006)	Loss 1.5827e+00 (1.6206e+00)	Acc@1  57.03 ( 54.11)	Acc@5  82.81 ( 84.45)
Epoch: [10][210/391]	Time  0.171 ( 0.183)	Data  0.002 ( 0.006)	Loss 1.7764e+00 (1.6240e+00)	Acc@1  55.47 ( 54.08)	Acc@5  82.03 ( 84.41)
Epoch: [10][220/391]	Time  0.169 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.4962e+00 (1.6243e+00)	Acc@1  52.34 ( 54.03)	Acc@5  89.06 ( 84.42)
Epoch: [10][230/391]	Time  0.177 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.4505e+00 (1.6232e+00)	Acc@1  60.94 ( 54.04)	Acc@5  82.81 ( 84.41)
Epoch: [10][240/391]	Time  0.162 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.6380e+00 (1.6232e+00)	Acc@1  60.16 ( 54.07)	Acc@5  82.81 ( 84.38)
Epoch: [10][250/391]	Time  0.167 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.6846e+00 (1.6238e+00)	Acc@1  54.69 ( 54.14)	Acc@5  81.25 ( 84.37)
Epoch: [10][260/391]	Time  0.164 ( 0.180)	Data  0.001 ( 0.006)	Loss 1.6150e+00 (1.6218e+00)	Acc@1  60.94 ( 54.26)	Acc@5  83.59 ( 84.41)
Epoch: [10][270/391]	Time  0.158 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.4404e+00 (1.6222e+00)	Acc@1  61.72 ( 54.21)	Acc@5  86.72 ( 84.41)
Epoch: [10][280/391]	Time  0.161 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.7929e+00 (1.6243e+00)	Acc@1  50.78 ( 54.18)	Acc@5  81.25 ( 84.41)
Epoch: [10][290/391]	Time  0.175 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.4657e+00 (1.6201e+00)	Acc@1  64.84 ( 54.30)	Acc@5  83.59 ( 84.49)
Epoch: [10][300/391]	Time  0.178 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.6488e+00 (1.6183e+00)	Acc@1  52.34 ( 54.33)	Acc@5  88.28 ( 84.58)
Epoch: [10][310/391]	Time  0.177 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.8618e+00 (1.6160e+00)	Acc@1  47.66 ( 54.43)	Acc@5  82.03 ( 84.64)
Epoch: [10][320/391]	Time  0.168 ( 0.179)	Data  0.002 ( 0.006)	Loss 1.6553e+00 (1.6126e+00)	Acc@1  53.91 ( 54.51)	Acc@5  81.25 ( 84.67)
Epoch: [10][330/391]	Time  0.173 ( 0.178)	Data  0.002 ( 0.006)	Loss 1.6258e+00 (1.6113e+00)	Acc@1  57.03 ( 54.55)	Acc@5  83.59 ( 84.72)
Epoch: [10][340/391]	Time  0.166 ( 0.179)	Data  0.002 ( 0.006)	Loss 1.7519e+00 (1.6105e+00)	Acc@1  50.00 ( 54.53)	Acc@5  85.16 ( 84.70)
Epoch: [10][350/391]	Time  0.163 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.5853e+00 (1.6064e+00)	Acc@1  58.59 ( 54.68)	Acc@5  84.38 ( 84.75)
Epoch: [10][360/391]	Time  0.157 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.4177e+00 (1.6044e+00)	Acc@1  64.06 ( 54.72)	Acc@5  85.16 ( 84.81)
Epoch: [10][370/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.006)	Loss 1.6228e+00 (1.6062e+00)	Acc@1  53.91 ( 54.69)	Acc@5  84.38 ( 84.78)
Epoch: [10][380/391]	Time  0.171 ( 0.177)	Data  0.001 ( 0.006)	Loss 1.8137e+00 (1.6055e+00)	Acc@1  50.78 ( 54.72)	Acc@5  76.56 ( 84.79)
Epoch: [10][390/391]	Time  0.220 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.9416e+00 (1.6060e+00)	Acc@1  51.25 ( 54.70)	Acc@5  80.00 ( 84.77)
## e[10] optimizer.zero_grad (sum) time: 0.37449049949645996
## e[10]       loss.backward (sum) time: 24.64238929748535
## e[10]      optimizer.step (sum) time: 3.6402406692504883
## epoch[10] training(only) time: 69.72194981575012
# Switched to evaluate mode...
Test: [  0/100]	Time  0.272 ( 0.272)	Loss 1.6912e+00 (1.6912e+00)	Acc@1  57.00 ( 57.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.065 ( 0.087)	Loss 1.6749e+00 (1.6960e+00)	Acc@1  53.00 ( 55.18)	Acc@5  87.00 ( 82.82)
Test: [ 20/100]	Time  0.065 ( 0.077)	Loss 1.5152e+00 (1.6923e+00)	Acc@1  59.00 ( 54.29)	Acc@5  85.00 ( 83.14)
Test: [ 30/100]	Time  0.065 ( 0.073)	Loss 1.7164e+00 (1.6738e+00)	Acc@1  54.00 ( 54.35)	Acc@5  84.00 ( 83.26)
Test: [ 40/100]	Time  0.057 ( 0.070)	Loss 1.8097e+00 (1.6917e+00)	Acc@1  50.00 ( 54.24)	Acc@5  87.00 ( 83.24)
Test: [ 50/100]	Time  0.057 ( 0.068)	Loss 1.6699e+00 (1.7178e+00)	Acc@1  57.00 ( 53.86)	Acc@5  81.00 ( 82.82)
Test: [ 60/100]	Time  0.062 ( 0.067)	Loss 1.6619e+00 (1.7065e+00)	Acc@1  52.00 ( 54.26)	Acc@5  83.00 ( 82.89)
Test: [ 70/100]	Time  0.061 ( 0.066)	Loss 1.8119e+00 (1.7070e+00)	Acc@1  52.00 ( 54.27)	Acc@5  83.00 ( 82.90)
Test: [ 80/100]	Time  0.061 ( 0.065)	Loss 1.7661e+00 (1.7167e+00)	Acc@1  55.00 ( 53.95)	Acc@5  81.00 ( 82.86)
Test: [ 90/100]	Time  0.063 ( 0.065)	Loss 1.8705e+00 (1.7039e+00)	Acc@1  50.00 ( 54.32)	Acc@5  79.00 ( 83.10)
 * Acc@1 54.490 Acc@5 83.150
### epoch[10] execution time: 76.27916884422302
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.432 ( 0.432)	Data  0.237 ( 0.237)	Loss 1.4938e+00 (1.4938e+00)	Acc@1  63.28 ( 63.28)	Acc@5  81.25 ( 81.25)
Epoch: [11][ 10/391]	Time  0.225 ( 0.232)	Data  0.002 ( 0.025)	Loss 1.5419e+00 (1.5234e+00)	Acc@1  55.47 ( 58.03)	Acc@5  80.47 ( 85.23)
Epoch: [11][ 20/391]	Time  0.218 ( 0.227)	Data  0.001 ( 0.016)	Loss 1.4418e+00 (1.4496e+00)	Acc@1  61.72 ( 59.30)	Acc@5  87.50 ( 86.53)
Epoch: [11][ 30/391]	Time  0.168 ( 0.219)	Data  0.001 ( 0.013)	Loss 1.4205e+00 (1.4553e+00)	Acc@1  57.81 ( 58.97)	Acc@5  89.06 ( 86.54)
Epoch: [11][ 40/391]	Time  0.173 ( 0.207)	Data  0.002 ( 0.011)	Loss 1.5565e+00 (1.4667e+00)	Acc@1  53.91 ( 59.01)	Acc@5  85.16 ( 86.53)
Epoch: [11][ 50/391]	Time  0.176 ( 0.201)	Data  0.001 ( 0.010)	Loss 1.2993e+00 (1.4556e+00)	Acc@1  64.84 ( 59.13)	Acc@5  91.41 ( 86.84)
Epoch: [11][ 60/391]	Time  0.165 ( 0.196)	Data  0.001 ( 0.009)	Loss 1.3344e+00 (1.4555e+00)	Acc@1  64.06 ( 58.94)	Acc@5  88.28 ( 86.90)
Epoch: [11][ 70/391]	Time  0.164 ( 0.191)	Data  0.001 ( 0.008)	Loss 1.4822e+00 (1.4502e+00)	Acc@1  53.91 ( 58.93)	Acc@5  87.50 ( 87.05)
Epoch: [11][ 80/391]	Time  0.163 ( 0.188)	Data  0.001 ( 0.008)	Loss 1.5958e+00 (1.4487e+00)	Acc@1  55.47 ( 59.11)	Acc@5  85.94 ( 87.02)
Epoch: [11][ 90/391]	Time  0.160 ( 0.185)	Data  0.001 ( 0.008)	Loss 1.5181e+00 (1.4557e+00)	Acc@1  53.91 ( 59.05)	Acc@5  82.81 ( 86.84)
Epoch: [11][100/391]	Time  0.161 ( 0.182)	Data  0.001 ( 0.007)	Loss 1.4588e+00 (1.4615e+00)	Acc@1  57.81 ( 58.77)	Acc@5  85.16 ( 86.63)
Epoch: [11][110/391]	Time  0.178 ( 0.182)	Data  0.001 ( 0.007)	Loss 1.7331e+00 (1.4662e+00)	Acc@1  54.69 ( 58.43)	Acc@5  82.81 ( 86.68)
Epoch: [11][120/391]	Time  0.178 ( 0.181)	Data  0.001 ( 0.007)	Loss 1.1729e+00 (1.4679e+00)	Acc@1  64.84 ( 58.25)	Acc@5  92.19 ( 86.73)
Epoch: [11][130/391]	Time  0.176 ( 0.181)	Data  0.001 ( 0.007)	Loss 1.4762e+00 (1.4657e+00)	Acc@1  56.25 ( 58.27)	Acc@5  85.16 ( 86.82)
Epoch: [11][140/391]	Time  0.175 ( 0.181)	Data  0.001 ( 0.007)	Loss 1.4516e+00 (1.4717e+00)	Acc@1  56.25 ( 58.08)	Acc@5  90.62 ( 86.75)
Epoch: [11][150/391]	Time  0.177 ( 0.181)	Data  0.001 ( 0.007)	Loss 1.2851e+00 (1.4740e+00)	Acc@1  67.97 ( 57.96)	Acc@5  89.84 ( 86.76)
Epoch: [11][160/391]	Time  0.167 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.4825e+00 (1.4799e+00)	Acc@1  54.69 ( 57.69)	Acc@5  86.72 ( 86.76)
Epoch: [11][170/391]	Time  0.178 ( 0.180)	Data  0.001 ( 0.006)	Loss 1.5761e+00 (1.4806e+00)	Acc@1  58.59 ( 57.63)	Acc@5  83.59 ( 86.81)
Epoch: [11][180/391]	Time  0.215 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.4002e+00 (1.4806e+00)	Acc@1  57.81 ( 57.58)	Acc@5  91.41 ( 86.78)
Epoch: [11][190/391]	Time  0.228 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.2069e+00 (1.4779e+00)	Acc@1  62.50 ( 57.66)	Acc@5  95.31 ( 86.85)
Epoch: [11][200/391]	Time  0.167 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.7233e+00 (1.4837e+00)	Acc@1  55.47 ( 57.58)	Acc@5  81.25 ( 86.74)
Epoch: [11][210/391]	Time  0.170 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.7178e+00 (1.4839e+00)	Acc@1  54.69 ( 57.54)	Acc@5  83.59 ( 86.77)
Epoch: [11][220/391]	Time  0.178 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.3878e+00 (1.4847e+00)	Acc@1  57.03 ( 57.56)	Acc@5  88.28 ( 86.74)
Epoch: [11][230/391]	Time  0.219 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.4731e+00 (1.4861e+00)	Acc@1  53.12 ( 57.53)	Acc@5  88.28 ( 86.70)
Epoch: [11][240/391]	Time  0.220 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.4562e+00 (1.4838e+00)	Acc@1  59.38 ( 57.57)	Acc@5  89.06 ( 86.79)
Epoch: [11][250/391]	Time  0.169 ( 0.188)	Data  0.002 ( 0.006)	Loss 1.5824e+00 (1.4834e+00)	Acc@1  54.69 ( 57.56)	Acc@5  84.38 ( 86.78)
Epoch: [11][260/391]	Time  0.178 ( 0.187)	Data  0.002 ( 0.006)	Loss 1.6752e+00 (1.4814e+00)	Acc@1  57.03 ( 57.64)	Acc@5  85.94 ( 86.83)
Epoch: [11][270/391]	Time  0.175 ( 0.187)	Data  0.002 ( 0.006)	Loss 1.4838e+00 (1.4839e+00)	Acc@1  57.81 ( 57.60)	Acc@5  89.06 ( 86.78)
Epoch: [11][280/391]	Time  0.194 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.4336e+00 (1.4815e+00)	Acc@1  62.50 ( 57.69)	Acc@5  87.50 ( 86.81)
Epoch: [11][290/391]	Time  0.169 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.4293e+00 (1.4813e+00)	Acc@1  55.47 ( 57.67)	Acc@5  84.38 ( 86.86)
Epoch: [11][300/391]	Time  0.177 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.3967e+00 (1.4811e+00)	Acc@1  53.91 ( 57.65)	Acc@5  87.50 ( 86.86)
Epoch: [11][310/391]	Time  0.165 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.2374e+00 (1.4809e+00)	Acc@1  64.84 ( 57.62)	Acc@5  94.53 ( 86.86)
Epoch: [11][320/391]	Time  0.166 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.7722e+00 (1.4827e+00)	Acc@1  53.12 ( 57.57)	Acc@5  82.03 ( 86.87)
Epoch: [11][330/391]	Time  0.180 ( 0.185)	Data  0.002 ( 0.006)	Loss 1.1932e+00 (1.4827e+00)	Acc@1  68.75 ( 57.58)	Acc@5  89.06 ( 86.86)
Epoch: [11][340/391]	Time  0.170 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.6493e+00 (1.4853e+00)	Acc@1  53.12 ( 57.48)	Acc@5  85.94 ( 86.85)
Epoch: [11][350/391]	Time  0.225 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.5012e+00 (1.4858e+00)	Acc@1  60.16 ( 57.48)	Acc@5  88.28 ( 86.84)
Epoch: [11][360/391]	Time  0.223 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.2410e+00 (1.4843e+00)	Acc@1  62.50 ( 57.54)	Acc@5  88.28 ( 86.88)
Epoch: [11][370/391]	Time  0.217 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.4166e+00 (1.4843e+00)	Acc@1  64.84 ( 57.52)	Acc@5  88.28 ( 86.88)
Epoch: [11][380/391]	Time  0.166 ( 0.186)	Data  0.002 ( 0.006)	Loss 1.3651e+00 (1.4841e+00)	Acc@1  60.94 ( 57.52)	Acc@5  89.84 ( 86.87)
Epoch: [11][390/391]	Time  0.172 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.3332e+00 (1.4845e+00)	Acc@1  61.25 ( 57.53)	Acc@5  88.75 ( 86.88)
## e[11] optimizer.zero_grad (sum) time: 0.39417052268981934
## e[11]       loss.backward (sum) time: 26.185210943222046
## e[11]      optimizer.step (sum) time: 3.8419058322906494
## epoch[11] training(only) time: 72.77265167236328
# Switched to evaluate mode...
Test: [  0/100]	Time  0.283 ( 0.283)	Loss 1.6289e+00 (1.6289e+00)	Acc@1  55.00 ( 55.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.058 ( 0.079)	Loss 1.5009e+00 (1.5445e+00)	Acc@1  55.00 ( 57.73)	Acc@5  85.00 ( 84.82)
Test: [ 20/100]	Time  0.058 ( 0.069)	Loss 1.3142e+00 (1.5260e+00)	Acc@1  65.00 ( 57.67)	Acc@5  88.00 ( 85.52)
Test: [ 30/100]	Time  0.072 ( 0.068)	Loss 1.5700e+00 (1.5187e+00)	Acc@1  54.00 ( 57.55)	Acc@5  87.00 ( 85.52)
Test: [ 40/100]	Time  0.070 ( 0.069)	Loss 1.6943e+00 (1.5341e+00)	Acc@1  59.00 ( 57.44)	Acc@5  82.00 ( 85.29)
Test: [ 50/100]	Time  0.066 ( 0.069)	Loss 1.6294e+00 (1.5560e+00)	Acc@1  57.00 ( 56.76)	Acc@5  83.00 ( 84.84)
Test: [ 60/100]	Time  0.067 ( 0.068)	Loss 1.4027e+00 (1.5463e+00)	Acc@1  60.00 ( 57.08)	Acc@5  89.00 ( 85.26)
Test: [ 70/100]	Time  0.068 ( 0.068)	Loss 1.5716e+00 (1.5543e+00)	Acc@1  58.00 ( 56.99)	Acc@5  88.00 ( 85.18)
Test: [ 80/100]	Time  0.066 ( 0.068)	Loss 1.6611e+00 (1.5596e+00)	Acc@1  52.00 ( 56.94)	Acc@5  83.00 ( 85.17)
Test: [ 90/100]	Time  0.065 ( 0.068)	Loss 1.7464e+00 (1.5515e+00)	Acc@1  52.00 ( 56.96)	Acc@5  87.00 ( 85.35)
 * Acc@1 56.860 Acc@5 85.440
### epoch[11] execution time: 79.69019842147827
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.412 ( 0.412)	Data  0.234 ( 0.234)	Loss 1.2839e+00 (1.2839e+00)	Acc@1  60.16 ( 60.16)	Acc@5  89.84 ( 89.84)
Epoch: [12][ 10/391]	Time  0.178 ( 0.198)	Data  0.001 ( 0.025)	Loss 1.3974e+00 (1.3120e+00)	Acc@1  60.16 ( 62.00)	Acc@5  89.06 ( 88.92)
Epoch: [12][ 20/391]	Time  0.173 ( 0.185)	Data  0.001 ( 0.015)	Loss 1.6496e+00 (1.3509e+00)	Acc@1  49.22 ( 60.64)	Acc@5  85.16 ( 88.47)
Epoch: [12][ 30/391]	Time  0.220 ( 0.182)	Data  0.002 ( 0.012)	Loss 1.5184e+00 (1.3730e+00)	Acc@1  52.34 ( 60.01)	Acc@5  87.50 ( 88.18)
Epoch: [12][ 40/391]	Time  0.176 ( 0.180)	Data  0.001 ( 0.010)	Loss 1.4890e+00 (1.3453e+00)	Acc@1  63.28 ( 60.90)	Acc@5  86.72 ( 88.74)
Epoch: [12][ 50/391]	Time  0.179 ( 0.181)	Data  0.001 ( 0.009)	Loss 1.2035e+00 (1.3417e+00)	Acc@1  66.41 ( 60.98)	Acc@5  89.84 ( 88.85)
Epoch: [12][ 60/391]	Time  0.179 ( 0.180)	Data  0.001 ( 0.008)	Loss 1.2344e+00 (1.3485e+00)	Acc@1  65.62 ( 60.85)	Acc@5  92.97 ( 88.74)
Epoch: [12][ 70/391]	Time  0.178 ( 0.180)	Data  0.001 ( 0.008)	Loss 1.4709e+00 (1.3498e+00)	Acc@1  62.50 ( 61.06)	Acc@5  85.16 ( 88.59)
Epoch: [12][ 80/391]	Time  0.186 ( 0.179)	Data  0.001 ( 0.007)	Loss 1.2720e+00 (1.3430e+00)	Acc@1  62.50 ( 61.28)	Acc@5  89.84 ( 88.73)
Epoch: [12][ 90/391]	Time  0.172 ( 0.178)	Data  0.001 ( 0.007)	Loss 1.4717e+00 (1.3423e+00)	Acc@1  54.69 ( 61.28)	Acc@5  89.06 ( 88.68)
Epoch: [12][100/391]	Time  0.227 ( 0.179)	Data  0.001 ( 0.007)	Loss 1.4107e+00 (1.3365e+00)	Acc@1  58.59 ( 61.56)	Acc@5  85.94 ( 88.81)
Epoch: [12][110/391]	Time  0.221 ( 0.183)	Data  0.001 ( 0.007)	Loss 1.5385e+00 (1.3413e+00)	Acc@1  56.25 ( 61.36)	Acc@5  87.50 ( 88.74)
Epoch: [12][120/391]	Time  0.225 ( 0.186)	Data  0.002 ( 0.007)	Loss 1.2729e+00 (1.3415e+00)	Acc@1  61.72 ( 61.34)	Acc@5  92.19 ( 88.82)
Epoch: [12][130/391]	Time  0.179 ( 0.186)	Data  0.001 ( 0.007)	Loss 1.4364e+00 (1.3465e+00)	Acc@1  54.69 ( 61.22)	Acc@5  84.38 ( 88.69)
Epoch: [12][140/391]	Time  0.177 ( 0.185)	Data  0.002 ( 0.006)	Loss 1.6365e+00 (1.3472e+00)	Acc@1  57.03 ( 61.22)	Acc@5  84.38 ( 88.67)
Epoch: [12][150/391]	Time  0.217 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.4821e+00 (1.3483e+00)	Acc@1  62.50 ( 61.22)	Acc@5  85.94 ( 88.67)
Epoch: [12][160/391]	Time  0.206 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.5076e+00 (1.3538e+00)	Acc@1  59.38 ( 61.07)	Acc@5  89.84 ( 88.63)
Epoch: [12][170/391]	Time  0.221 ( 0.189)	Data  0.001 ( 0.006)	Loss 1.2858e+00 (1.3572e+00)	Acc@1  61.72 ( 60.97)	Acc@5  86.72 ( 88.60)
Epoch: [12][180/391]	Time  0.168 ( 0.189)	Data  0.001 ( 0.006)	Loss 1.4002e+00 (1.3581e+00)	Acc@1  55.47 ( 60.94)	Acc@5  92.97 ( 88.60)
Epoch: [12][190/391]	Time  0.180 ( 0.188)	Data  0.001 ( 0.006)	Loss 1.2990e+00 (1.3587e+00)	Acc@1  67.19 ( 60.97)	Acc@5  86.72 ( 88.58)
Epoch: [12][200/391]	Time  0.156 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.5822e+00 (1.3637e+00)	Acc@1  56.25 ( 60.79)	Acc@5  84.38 ( 88.59)
Epoch: [12][210/391]	Time  0.173 ( 0.186)	Data  0.002 ( 0.006)	Loss 1.6336e+00 (1.3716e+00)	Acc@1  57.81 ( 60.56)	Acc@5  83.59 ( 88.47)
Epoch: [12][220/391]	Time  0.191 ( 0.186)	Data  0.002 ( 0.006)	Loss 1.4042e+00 (1.3729e+00)	Acc@1  58.59 ( 60.53)	Acc@5  86.72 ( 88.39)
Epoch: [12][230/391]	Time  0.176 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.3228e+00 (1.3736e+00)	Acc@1  62.50 ( 60.49)	Acc@5  88.28 ( 88.38)
Epoch: [12][240/391]	Time  0.173 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.3314e+00 (1.3721e+00)	Acc@1  64.84 ( 60.51)	Acc@5  89.84 ( 88.46)
Epoch: [12][250/391]	Time  0.173 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.3810e+00 (1.3693e+00)	Acc@1  58.59 ( 60.57)	Acc@5  88.28 ( 88.49)
Epoch: [12][260/391]	Time  0.164 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.5604e+00 (1.3671e+00)	Acc@1  58.59 ( 60.59)	Acc@5  86.72 ( 88.49)
Epoch: [12][270/391]	Time  0.177 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.1716e+00 (1.3658e+00)	Acc@1  64.06 ( 60.57)	Acc@5  90.62 ( 88.48)
Epoch: [12][280/391]	Time  0.222 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.2937e+00 (1.3680e+00)	Acc@1  59.38 ( 60.50)	Acc@5  92.97 ( 88.45)
Epoch: [12][290/391]	Time  0.224 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.3752e+00 (1.3687e+00)	Acc@1  60.16 ( 60.48)	Acc@5  88.28 ( 88.44)
Epoch: [12][300/391]	Time  0.177 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.5451e+00 (1.3707e+00)	Acc@1  57.81 ( 60.50)	Acc@5  85.94 ( 88.40)
Epoch: [12][310/391]	Time  0.175 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.5428e+00 (1.3738e+00)	Acc@1  54.69 ( 60.41)	Acc@5  87.50 ( 88.37)
Epoch: [12][320/391]	Time  0.174 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.3504e+00 (1.3746e+00)	Acc@1  63.28 ( 60.33)	Acc@5  85.16 ( 88.35)
Epoch: [12][330/391]	Time  0.209 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.4707e+00 (1.3738e+00)	Acc@1  55.47 ( 60.35)	Acc@5  89.84 ( 88.34)
Epoch: [12][340/391]	Time  0.217 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.5094e+00 (1.3743e+00)	Acc@1  57.81 ( 60.32)	Acc@5  86.72 ( 88.34)
Epoch: [12][350/391]	Time  0.177 ( 0.188)	Data  0.001 ( 0.006)	Loss 1.5187e+00 (1.3741e+00)	Acc@1  57.81 ( 60.32)	Acc@5  83.59 ( 88.33)
Epoch: [12][360/391]	Time  0.174 ( 0.188)	Data  0.001 ( 0.006)	Loss 1.2341e+00 (1.3735e+00)	Acc@1  64.84 ( 60.38)	Acc@5  92.97 ( 88.36)
Epoch: [12][370/391]	Time  0.175 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.2739e+00 (1.3735e+00)	Acc@1  64.84 ( 60.39)	Acc@5  87.50 ( 88.33)
Epoch: [12][380/391]	Time  0.178 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.6885e+00 (1.3724e+00)	Acc@1  53.91 ( 60.42)	Acc@5  82.03 ( 88.36)
Epoch: [12][390/391]	Time  0.174 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.4915e+00 (1.3718e+00)	Acc@1  53.75 ( 60.43)	Acc@5  85.00 ( 88.36)
## e[12] optimizer.zero_grad (sum) time: 0.39980435371398926
## e[12]       loss.backward (sum) time: 26.65814995765686
## e[12]      optimizer.step (sum) time: 3.895500659942627
## epoch[12] training(only) time: 73.14510464668274
# Switched to evaluate mode...
Test: [  0/100]	Time  0.272 ( 0.272)	Loss 1.6551e+00 (1.6551e+00)	Acc@1  59.00 ( 59.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.058 ( 0.081)	Loss 1.6017e+00 (1.5766e+00)	Acc@1  57.00 ( 58.91)	Acc@5  87.00 ( 85.45)
Test: [ 20/100]	Time  0.058 ( 0.070)	Loss 1.3731e+00 (1.5440e+00)	Acc@1  60.00 ( 58.57)	Acc@5  86.00 ( 86.57)
Test: [ 30/100]	Time  0.057 ( 0.066)	Loss 1.4792e+00 (1.5330e+00)	Acc@1  56.00 ( 58.39)	Acc@5  84.00 ( 86.32)
Test: [ 40/100]	Time  0.057 ( 0.064)	Loss 1.6956e+00 (1.5482e+00)	Acc@1  58.00 ( 57.83)	Acc@5  84.00 ( 86.41)
Test: [ 50/100]	Time  0.060 ( 0.063)	Loss 1.5134e+00 (1.5828e+00)	Acc@1  59.00 ( 56.90)	Acc@5  85.00 ( 85.53)
Test: [ 60/100]	Time  0.058 ( 0.062)	Loss 1.4989e+00 (1.5560e+00)	Acc@1  58.00 ( 57.15)	Acc@5  86.00 ( 85.95)
Test: [ 70/100]	Time  0.058 ( 0.062)	Loss 1.5865e+00 (1.5602e+00)	Acc@1  57.00 ( 57.30)	Acc@5  86.00 ( 86.00)
Test: [ 80/100]	Time  0.059 ( 0.062)	Loss 1.8539e+00 (1.5728e+00)	Acc@1  53.00 ( 57.14)	Acc@5  82.00 ( 85.88)
Test: [ 90/100]	Time  0.068 ( 0.061)	Loss 1.7991e+00 (1.5638e+00)	Acc@1  48.00 ( 57.34)	Acc@5  82.00 ( 86.03)
 * Acc@1 57.490 Acc@5 85.960
### epoch[12] execution time: 79.40058255195618
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.420 ( 0.420)	Data  0.233 ( 0.233)	Loss 1.2864e+00 (1.2864e+00)	Acc@1  58.59 ( 58.59)	Acc@5  89.06 ( 89.06)
Epoch: [13][ 10/391]	Time  0.172 ( 0.195)	Data  0.001 ( 0.025)	Loss 1.7588e+00 (1.3209e+00)	Acc@1  51.56 ( 60.58)	Acc@5  79.69 ( 88.71)
Epoch: [13][ 20/391]	Time  0.214 ( 0.201)	Data  0.002 ( 0.015)	Loss 1.2545e+00 (1.2807e+00)	Acc@1  66.41 ( 62.13)	Acc@5  90.62 ( 89.40)
Epoch: [13][ 30/391]	Time  0.225 ( 0.207)	Data  0.001 ( 0.013)	Loss 1.2049e+00 (1.2409e+00)	Acc@1  68.75 ( 63.58)	Acc@5  91.41 ( 90.12)
Epoch: [13][ 40/391]	Time  0.168 ( 0.206)	Data  0.001 ( 0.011)	Loss 1.3425e+00 (1.2526e+00)	Acc@1  62.50 ( 63.43)	Acc@5  86.72 ( 90.02)
Epoch: [13][ 50/391]	Time  0.168 ( 0.199)	Data  0.002 ( 0.010)	Loss 1.2682e+00 (1.2718e+00)	Acc@1  63.28 ( 63.04)	Acc@5  92.97 ( 89.95)
Epoch: [13][ 60/391]	Time  0.166 ( 0.195)	Data  0.001 ( 0.009)	Loss 1.5041e+00 (1.2815e+00)	Acc@1  60.94 ( 63.10)	Acc@5  86.72 ( 89.70)
Epoch: [13][ 70/391]	Time  0.221 ( 0.194)	Data  0.001 ( 0.008)	Loss 1.4704e+00 (1.2865e+00)	Acc@1  59.38 ( 62.93)	Acc@5  83.59 ( 89.63)
Epoch: [13][ 80/391]	Time  0.224 ( 0.197)	Data  0.001 ( 0.008)	Loss 1.2131e+00 (1.2937e+00)	Acc@1  66.41 ( 62.67)	Acc@5  90.62 ( 89.53)
Epoch: [13][ 90/391]	Time  0.199 ( 0.199)	Data  0.001 ( 0.008)	Loss 1.1395e+00 (1.2904e+00)	Acc@1  67.97 ( 62.68)	Acc@5  93.75 ( 89.71)
Epoch: [13][100/391]	Time  0.172 ( 0.197)	Data  0.001 ( 0.008)	Loss 1.2313e+00 (1.2915e+00)	Acc@1  64.06 ( 62.65)	Acc@5  89.06 ( 89.61)
Epoch: [13][110/391]	Time  0.176 ( 0.195)	Data  0.001 ( 0.008)	Loss 1.2814e+00 (1.2951e+00)	Acc@1  57.81 ( 62.47)	Acc@5  87.50 ( 89.56)
Epoch: [13][120/391]	Time  0.179 ( 0.194)	Data  0.002 ( 0.007)	Loss 1.2267e+00 (1.2860e+00)	Acc@1  65.62 ( 62.64)	Acc@5  89.06 ( 89.67)
Epoch: [13][130/391]	Time  0.167 ( 0.192)	Data  0.002 ( 0.007)	Loss 1.3396e+00 (1.2830e+00)	Acc@1  58.59 ( 62.62)	Acc@5  89.06 ( 89.75)
Epoch: [13][140/391]	Time  0.174 ( 0.191)	Data  0.001 ( 0.007)	Loss 1.4529e+00 (1.2836e+00)	Acc@1  53.12 ( 62.57)	Acc@5  91.41 ( 89.76)
Epoch: [13][150/391]	Time  0.177 ( 0.190)	Data  0.001 ( 0.007)	Loss 1.3062e+00 (1.2875e+00)	Acc@1  66.41 ( 62.57)	Acc@5  88.28 ( 89.63)
Epoch: [13][160/391]	Time  0.164 ( 0.189)	Data  0.001 ( 0.007)	Loss 1.4570e+00 (1.2881e+00)	Acc@1  57.81 ( 62.49)	Acc@5  87.50 ( 89.68)
Epoch: [13][170/391]	Time  0.194 ( 0.188)	Data  0.002 ( 0.006)	Loss 1.4462e+00 (1.2871e+00)	Acc@1  57.81 ( 62.52)	Acc@5  87.50 ( 89.70)
Epoch: [13][180/391]	Time  0.175 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.0533e+00 (1.2865e+00)	Acc@1  65.62 ( 62.48)	Acc@5  93.75 ( 89.67)
Epoch: [13][190/391]	Time  0.222 ( 0.188)	Data  0.001 ( 0.006)	Loss 1.1698e+00 (1.2879e+00)	Acc@1  69.53 ( 62.46)	Acc@5  88.28 ( 89.66)
Epoch: [13][200/391]	Time  0.224 ( 0.189)	Data  0.001 ( 0.006)	Loss 1.1463e+00 (1.2852e+00)	Acc@1  67.19 ( 62.53)	Acc@5  93.75 ( 89.73)
Epoch: [13][210/391]	Time  0.208 ( 0.191)	Data  0.001 ( 0.006)	Loss 1.5583e+00 (1.2880e+00)	Acc@1  55.47 ( 62.44)	Acc@5  82.03 ( 89.64)
Epoch: [13][220/391]	Time  0.173 ( 0.190)	Data  0.001 ( 0.006)	Loss 1.2043e+00 (1.2898e+00)	Acc@1  68.75 ( 62.41)	Acc@5  87.50 ( 89.59)
Epoch: [13][230/391]	Time  0.168 ( 0.189)	Data  0.001 ( 0.006)	Loss 1.2580e+00 (1.2910e+00)	Acc@1  60.94 ( 62.38)	Acc@5  90.62 ( 89.59)
Epoch: [13][240/391]	Time  0.198 ( 0.189)	Data  0.002 ( 0.006)	Loss 1.1738e+00 (1.2915e+00)	Acc@1  66.41 ( 62.40)	Acc@5  92.19 ( 89.57)
Epoch: [13][250/391]	Time  0.206 ( 0.190)	Data  0.001 ( 0.006)	Loss 1.2976e+00 (1.2899e+00)	Acc@1  61.72 ( 62.40)	Acc@5  89.84 ( 89.60)
Epoch: [13][260/391]	Time  0.209 ( 0.190)	Data  0.001 ( 0.006)	Loss 1.4171e+00 (1.2905e+00)	Acc@1  58.59 ( 62.44)	Acc@5  86.72 ( 89.59)
Epoch: [13][270/391]	Time  0.172 ( 0.190)	Data  0.001 ( 0.006)	Loss 1.0562e+00 (1.2914e+00)	Acc@1  71.09 ( 62.41)	Acc@5  94.53 ( 89.52)
Epoch: [13][280/391]	Time  0.174 ( 0.190)	Data  0.001 ( 0.006)	Loss 1.2975e+00 (1.2952e+00)	Acc@1  64.84 ( 62.33)	Acc@5  86.72 ( 89.49)
Epoch: [13][290/391]	Time  0.172 ( 0.189)	Data  0.001 ( 0.006)	Loss 1.4688e+00 (1.2983e+00)	Acc@1  60.16 ( 62.24)	Acc@5  87.50 ( 89.47)
Epoch: [13][300/391]	Time  0.175 ( 0.189)	Data  0.001 ( 0.006)	Loss 1.2018e+00 (1.2976e+00)	Acc@1  63.28 ( 62.30)	Acc@5  93.75 ( 89.46)
Epoch: [13][310/391]	Time  0.190 ( 0.188)	Data  0.001 ( 0.006)	Loss 1.5109e+00 (1.2995e+00)	Acc@1  59.38 ( 62.28)	Acc@5  83.59 ( 89.41)
Epoch: [13][320/391]	Time  0.169 ( 0.188)	Data  0.001 ( 0.006)	Loss 1.2626e+00 (1.2970e+00)	Acc@1  67.19 ( 62.37)	Acc@5  87.50 ( 89.46)
Epoch: [13][330/391]	Time  0.166 ( 0.187)	Data  0.002 ( 0.006)	Loss 1.2970e+00 (1.2952e+00)	Acc@1  62.50 ( 62.44)	Acc@5  87.50 ( 89.47)
Epoch: [13][340/391]	Time  0.164 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.4714e+00 (1.2957e+00)	Acc@1  60.94 ( 62.43)	Acc@5  85.16 ( 89.45)
Epoch: [13][350/391]	Time  0.163 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.1429e+00 (1.2934e+00)	Acc@1  68.75 ( 62.51)	Acc@5  92.97 ( 89.44)
Epoch: [13][360/391]	Time  0.157 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.4709e+00 (1.2948e+00)	Acc@1  59.38 ( 62.48)	Acc@5  88.28 ( 89.44)
Epoch: [13][370/391]	Time  0.229 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.2223e+00 (1.2955e+00)	Acc@1  66.41 ( 62.49)	Acc@5  92.97 ( 89.44)
Epoch: [13][380/391]	Time  0.220 ( 0.188)	Data  0.001 ( 0.006)	Loss 1.2087e+00 (1.2949e+00)	Acc@1  63.28 ( 62.47)	Acc@5  90.62 ( 89.49)
Epoch: [13][390/391]	Time  0.176 ( 0.188)	Data  0.001 ( 0.006)	Loss 1.4391e+00 (1.2947e+00)	Acc@1  60.00 ( 62.49)	Acc@5  86.25 ( 89.47)
## e[13] optimizer.zero_grad (sum) time: 0.4053194522857666
## e[13]       loss.backward (sum) time: 26.723908185958862
## e[13]      optimizer.step (sum) time: 3.906813144683838
## epoch[13] training(only) time: 73.6698386669159
# Switched to evaluate mode...
Test: [  0/100]	Time  0.287 ( 0.287)	Loss 1.4900e+00 (1.4900e+00)	Acc@1  58.00 ( 58.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.062 ( 0.081)	Loss 1.7379e+00 (1.5134e+00)	Acc@1  56.00 ( 59.91)	Acc@5  87.00 ( 87.09)
Test: [ 20/100]	Time  0.060 ( 0.072)	Loss 1.2619e+00 (1.4674e+00)	Acc@1  65.00 ( 60.67)	Acc@5  88.00 ( 87.29)
Test: [ 30/100]	Time  0.059 ( 0.068)	Loss 1.5956e+00 (1.4729e+00)	Acc@1  57.00 ( 59.81)	Acc@5  88.00 ( 87.32)
Test: [ 40/100]	Time  0.060 ( 0.066)	Loss 1.5941e+00 (1.4879e+00)	Acc@1  56.00 ( 59.34)	Acc@5  89.00 ( 87.54)
Test: [ 50/100]	Time  0.061 ( 0.064)	Loss 1.2448e+00 (1.5093e+00)	Acc@1  65.00 ( 59.06)	Acc@5  86.00 ( 87.00)
Test: [ 60/100]	Time  0.062 ( 0.064)	Loss 1.3251e+00 (1.4948e+00)	Acc@1  63.00 ( 59.34)	Acc@5  86.00 ( 86.98)
Test: [ 70/100]	Time  0.064 ( 0.064)	Loss 1.7207e+00 (1.5086e+00)	Acc@1  54.00 ( 59.21)	Acc@5  83.00 ( 86.85)
Test: [ 80/100]	Time  0.063 ( 0.064)	Loss 1.5722e+00 (1.5137e+00)	Acc@1  60.00 ( 59.27)	Acc@5  82.00 ( 86.73)
Test: [ 90/100]	Time  0.063 ( 0.064)	Loss 1.7971e+00 (1.5096e+00)	Acc@1  54.00 ( 59.24)	Acc@5  86.00 ( 86.77)
 * Acc@1 59.260 Acc@5 86.820
### epoch[13] execution time: 80.17731785774231
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.421 ( 0.421)	Data  0.232 ( 0.232)	Loss 9.7840e-01 (9.7840e-01)	Acc@1  68.75 ( 68.75)	Acc@5  94.53 ( 94.53)
Epoch: [14][ 10/391]	Time  0.181 ( 0.222)	Data  0.001 ( 0.026)	Loss 1.4282e+00 (1.2007e+00)	Acc@1  60.94 ( 64.77)	Acc@5  85.16 ( 91.05)
Epoch: [14][ 20/391]	Time  0.163 ( 0.196)	Data  0.001 ( 0.015)	Loss 1.5938e+00 (1.1916e+00)	Acc@1  63.28 ( 65.33)	Acc@5  85.16 ( 90.85)
Epoch: [14][ 30/391]	Time  0.167 ( 0.188)	Data  0.001 ( 0.012)	Loss 1.2903e+00 (1.1859e+00)	Acc@1  64.84 ( 65.80)	Acc@5  89.84 ( 91.00)
Epoch: [14][ 40/391]	Time  0.160 ( 0.182)	Data  0.001 ( 0.010)	Loss 1.2286e+00 (1.1748e+00)	Acc@1  66.41 ( 65.76)	Acc@5  90.62 ( 91.20)
Epoch: [14][ 50/391]	Time  0.175 ( 0.182)	Data  0.001 ( 0.009)	Loss 1.0169e+00 (1.1810e+00)	Acc@1  68.75 ( 65.70)	Acc@5  93.75 ( 90.95)
Epoch: [14][ 60/391]	Time  0.173 ( 0.182)	Data  0.001 ( 0.009)	Loss 1.0786e+00 (1.1833e+00)	Acc@1  71.09 ( 65.48)	Acc@5  89.06 ( 90.98)
Epoch: [14][ 70/391]	Time  0.168 ( 0.181)	Data  0.001 ( 0.008)	Loss 9.3071e-01 (1.1847e+00)	Acc@1  64.06 ( 65.26)	Acc@5  96.88 ( 90.97)
Epoch: [14][ 80/391]	Time  0.174 ( 0.180)	Data  0.001 ( 0.008)	Loss 1.1636e+00 (1.1755e+00)	Acc@1  63.28 ( 65.44)	Acc@5  90.62 ( 91.03)
Epoch: [14][ 90/391]	Time  0.185 ( 0.180)	Data  0.001 ( 0.007)	Loss 1.0804e+00 (1.1774e+00)	Acc@1  64.06 ( 65.33)	Acc@5  92.19 ( 90.95)
Epoch: [14][100/391]	Time  0.169 ( 0.179)	Data  0.001 ( 0.007)	Loss 1.3351e+00 (1.1832e+00)	Acc@1  62.50 ( 65.32)	Acc@5  92.19 ( 90.81)
Epoch: [14][110/391]	Time  0.215 ( 0.181)	Data  0.001 ( 0.007)	Loss 1.1913e+00 (1.1905e+00)	Acc@1  63.28 ( 65.18)	Acc@5  93.75 ( 90.78)
Epoch: [14][120/391]	Time  0.223 ( 0.185)	Data  0.002 ( 0.007)	Loss 1.3839e+00 (1.1862e+00)	Acc@1  60.94 ( 65.35)	Acc@5  88.28 ( 90.89)
Epoch: [14][130/391]	Time  0.167 ( 0.187)	Data  0.001 ( 0.007)	Loss 1.1502e+00 (1.1827e+00)	Acc@1  66.41 ( 65.49)	Acc@5  90.62 ( 90.92)
Epoch: [14][140/391]	Time  0.166 ( 0.186)	Data  0.001 ( 0.007)	Loss 1.2025e+00 (1.1879e+00)	Acc@1  67.19 ( 65.40)	Acc@5  92.19 ( 90.84)
Epoch: [14][150/391]	Time  0.168 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.1800e+00 (1.1879e+00)	Acc@1  61.72 ( 65.37)	Acc@5  89.84 ( 90.82)
Epoch: [14][160/391]	Time  0.206 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.1174e+00 (1.1886e+00)	Acc@1  58.59 ( 65.28)	Acc@5  92.97 ( 90.81)
Epoch: [14][170/391]	Time  0.203 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.2089e+00 (1.1882e+00)	Acc@1  67.19 ( 65.27)	Acc@5  92.97 ( 90.86)
Epoch: [14][180/391]	Time  0.206 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.3796e+00 (1.1917e+00)	Acc@1  59.38 ( 65.17)	Acc@5  88.28 ( 90.76)
Epoch: [14][190/391]	Time  0.161 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.3069e+00 (1.1926e+00)	Acc@1  64.84 ( 65.16)	Acc@5  86.72 ( 90.74)
Epoch: [14][200/391]	Time  0.164 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.2278e+00 (1.1944e+00)	Acc@1  60.16 ( 65.12)	Acc@5  92.97 ( 90.70)
Epoch: [14][210/391]	Time  0.160 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.0987e+00 (1.1955e+00)	Acc@1  69.53 ( 65.12)	Acc@5  91.41 ( 90.68)
Epoch: [14][220/391]	Time  0.175 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.1819e+00 (1.1974e+00)	Acc@1  67.19 ( 65.03)	Acc@5  88.28 ( 90.66)
Epoch: [14][230/391]	Time  0.177 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.0493e+00 (1.1961e+00)	Acc@1  67.19 ( 65.04)	Acc@5  93.75 ( 90.67)
Epoch: [14][240/391]	Time  0.177 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.4678e+00 (1.1965e+00)	Acc@1  56.25 ( 65.00)	Acc@5  87.50 ( 90.66)
Epoch: [14][250/391]	Time  0.167 ( 0.183)	Data  0.002 ( 0.006)	Loss 1.1252e+00 (1.1987e+00)	Acc@1  67.97 ( 64.97)	Acc@5  91.41 ( 90.62)
Epoch: [14][260/391]	Time  0.181 ( 0.183)	Data  0.002 ( 0.006)	Loss 1.4058e+00 (1.2006e+00)	Acc@1  60.16 ( 64.89)	Acc@5  86.72 ( 90.61)
Epoch: [14][270/391]	Time  0.164 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.3028e+00 (1.2042e+00)	Acc@1  62.50 ( 64.77)	Acc@5  88.28 ( 90.60)
Epoch: [14][280/391]	Time  0.227 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.1397e+00 (1.2026e+00)	Acc@1  67.19 ( 64.83)	Acc@5  92.19 ( 90.64)
Epoch: [14][290/391]	Time  0.224 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.1851e+00 (1.2058e+00)	Acc@1  62.50 ( 64.76)	Acc@5  90.62 ( 90.59)
Epoch: [14][300/391]	Time  0.220 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.2209e+00 (1.2072e+00)	Acc@1  64.06 ( 64.76)	Acc@5  90.62 ( 90.55)
Epoch: [14][310/391]	Time  0.168 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.2782e+00 (1.2069e+00)	Acc@1  60.16 ( 64.75)	Acc@5  92.19 ( 90.57)
Epoch: [14][320/391]	Time  0.179 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.1439e+00 (1.2049e+00)	Acc@1  71.88 ( 64.80)	Acc@5  91.41 ( 90.59)
Epoch: [14][330/391]	Time  0.168 ( 0.184)	Data  0.002 ( 0.006)	Loss 1.1815e+00 (1.2074e+00)	Acc@1  61.72 ( 64.73)	Acc@5  92.19 ( 90.56)
Epoch: [14][340/391]	Time  0.208 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.2762e+00 (1.2063e+00)	Acc@1  60.94 ( 64.75)	Acc@5  92.19 ( 90.60)
Epoch: [14][350/391]	Time  0.207 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.1407e+00 (1.2095e+00)	Acc@1  65.62 ( 64.63)	Acc@5  95.31 ( 90.58)
Epoch: [14][360/391]	Time  0.171 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.3465e+00 (1.2096e+00)	Acc@1  60.16 ( 64.61)	Acc@5  89.84 ( 90.57)
Epoch: [14][370/391]	Time  0.168 ( 0.185)	Data  0.002 ( 0.006)	Loss 1.3498e+00 (1.2106e+00)	Acc@1  64.06 ( 64.57)	Acc@5  88.28 ( 90.55)
Epoch: [14][380/391]	Time  0.171 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.2185e+00 (1.2101e+00)	Acc@1  62.50 ( 64.59)	Acc@5  90.62 ( 90.57)
Epoch: [14][390/391]	Time  0.175 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.0990e+00 (1.2107e+00)	Acc@1  66.25 ( 64.58)	Acc@5  95.00 ( 90.58)
## e[14] optimizer.zero_grad (sum) time: 0.39898204803466797
## e[14]       loss.backward (sum) time: 26.17329740524292
## e[14]      optimizer.step (sum) time: 3.8493854999542236
## epoch[14] training(only) time: 72.09498572349548
# Switched to evaluate mode...
Test: [  0/100]	Time  0.267 ( 0.267)	Loss 1.4771e+00 (1.4771e+00)	Acc@1  62.00 ( 62.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.058 ( 0.078)	Loss 1.4474e+00 (1.4920e+00)	Acc@1  55.00 ( 59.55)	Acc@5  90.00 ( 86.45)
Test: [ 20/100]	Time  0.065 ( 0.069)	Loss 1.3196e+00 (1.4363e+00)	Acc@1  68.00 ( 61.33)	Acc@5  88.00 ( 86.95)
Test: [ 30/100]	Time  0.059 ( 0.067)	Loss 1.7736e+00 (1.4620e+00)	Acc@1  58.00 ( 60.61)	Acc@5  84.00 ( 86.81)
Test: [ 40/100]	Time  0.059 ( 0.065)	Loss 1.4392e+00 (1.4600e+00)	Acc@1  62.00 ( 60.32)	Acc@5  91.00 ( 87.27)
Test: [ 50/100]	Time  0.058 ( 0.063)	Loss 1.6215e+00 (1.4871e+00)	Acc@1  59.00 ( 59.84)	Acc@5  82.00 ( 86.75)
Test: [ 60/100]	Time  0.059 ( 0.062)	Loss 1.2461e+00 (1.4585e+00)	Acc@1  66.00 ( 60.34)	Acc@5  85.00 ( 86.97)
Test: [ 70/100]	Time  0.060 ( 0.062)	Loss 1.5924e+00 (1.4635e+00)	Acc@1  53.00 ( 60.25)	Acc@5  82.00 ( 87.06)
Test: [ 80/100]	Time  0.059 ( 0.062)	Loss 1.6473e+00 (1.4743e+00)	Acc@1  57.00 ( 59.88)	Acc@5  87.00 ( 87.11)
Test: [ 90/100]	Time  0.061 ( 0.062)	Loss 1.7345e+00 (1.4601e+00)	Acc@1  54.00 ( 60.07)	Acc@5  84.00 ( 87.27)
 * Acc@1 60.110 Acc@5 87.330
### epoch[14] execution time: 78.3560380935669
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.379 ( 0.379)	Data  0.189 ( 0.189)	Loss 1.1587e+00 (1.1587e+00)	Acc@1  64.84 ( 64.84)	Acc@5  92.97 ( 92.97)
Epoch: [15][ 10/391]	Time  0.171 ( 0.190)	Data  0.001 ( 0.021)	Loss 1.1097e+00 (1.0833e+00)	Acc@1  67.97 ( 68.39)	Acc@5  90.62 ( 91.76)
Epoch: [15][ 20/391]	Time  0.213 ( 0.180)	Data  0.001 ( 0.013)	Loss 1.2745e+00 (1.1037e+00)	Acc@1  61.72 ( 67.63)	Acc@5  92.97 ( 92.04)
Epoch: [15][ 30/391]	Time  0.223 ( 0.193)	Data  0.001 ( 0.011)	Loss 8.9936e-01 (1.1066e+00)	Acc@1  74.22 ( 67.41)	Acc@5  92.97 ( 91.78)
Epoch: [15][ 40/391]	Time  0.225 ( 0.200)	Data  0.001 ( 0.010)	Loss 1.1271e+00 (1.0960e+00)	Acc@1  66.41 ( 67.66)	Acc@5  91.41 ( 92.00)
Epoch: [15][ 50/391]	Time  0.171 ( 0.197)	Data  0.001 ( 0.010)	Loss 8.9136e-01 (1.0941e+00)	Acc@1  73.44 ( 67.98)	Acc@5  92.97 ( 91.87)
Epoch: [15][ 60/391]	Time  0.181 ( 0.194)	Data  0.001 ( 0.009)	Loss 1.0044e+00 (1.0972e+00)	Acc@1  71.09 ( 67.52)	Acc@5  92.19 ( 92.05)
Epoch: [15][ 70/391]	Time  0.172 ( 0.191)	Data  0.002 ( 0.008)	Loss 1.1512e+00 (1.1008e+00)	Acc@1  64.06 ( 67.58)	Acc@5  91.41 ( 92.03)
Epoch: [15][ 80/391]	Time  0.207 ( 0.192)	Data  0.001 ( 0.008)	Loss 1.0779e+00 (1.1073e+00)	Acc@1  67.97 ( 67.40)	Acc@5  93.75 ( 92.02)
Epoch: [15][ 90/391]	Time  0.208 ( 0.193)	Data  0.001 ( 0.008)	Loss 1.0697e+00 (1.1084e+00)	Acc@1  67.97 ( 67.42)	Acc@5  92.97 ( 91.96)
Epoch: [15][100/391]	Time  0.173 ( 0.193)	Data  0.001 ( 0.007)	Loss 1.1623e+00 (1.1209e+00)	Acc@1  64.06 ( 67.00)	Acc@5  92.19 ( 91.85)
Epoch: [15][110/391]	Time  0.173 ( 0.191)	Data  0.002 ( 0.007)	Loss 1.1801e+00 (1.1252e+00)	Acc@1  64.06 ( 66.88)	Acc@5  91.41 ( 91.78)
Epoch: [15][120/391]	Time  0.175 ( 0.190)	Data  0.001 ( 0.007)	Loss 9.1112e-01 (1.1222e+00)	Acc@1  74.22 ( 66.87)	Acc@5  92.97 ( 91.85)
Epoch: [15][130/391]	Time  0.163 ( 0.189)	Data  0.002 ( 0.007)	Loss 9.5402e-01 (1.1250e+00)	Acc@1  71.88 ( 66.91)	Acc@5  89.84 ( 91.71)
Epoch: [15][140/391]	Time  0.179 ( 0.188)	Data  0.001 ( 0.007)	Loss 1.2794e+00 (1.1285e+00)	Acc@1  68.75 ( 66.90)	Acc@5  91.41 ( 91.68)
Epoch: [15][150/391]	Time  0.168 ( 0.187)	Data  0.001 ( 0.007)	Loss 1.2785e+00 (1.1292e+00)	Acc@1  66.41 ( 66.93)	Acc@5  87.50 ( 91.63)
Epoch: [15][160/391]	Time  0.165 ( 0.187)	Data  0.002 ( 0.006)	Loss 1.3428e+00 (1.1305e+00)	Acc@1  67.19 ( 66.99)	Acc@5  86.72 ( 91.63)
Epoch: [15][170/391]	Time  0.170 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.1149e+00 (1.1322e+00)	Acc@1  66.41 ( 66.91)	Acc@5  92.97 ( 91.60)
Epoch: [15][180/391]	Time  0.166 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.1883e+00 (1.1337e+00)	Acc@1  63.28 ( 66.87)	Acc@5  92.97 ( 91.67)
Epoch: [15][190/391]	Time  0.161 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.0681e+00 (1.1342e+00)	Acc@1  66.41 ( 66.93)	Acc@5  90.62 ( 91.64)
Epoch: [15][200/391]	Time  0.219 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.2283e+00 (1.1348e+00)	Acc@1  65.62 ( 66.93)	Acc@5  89.06 ( 91.59)
Epoch: [15][210/391]	Time  0.219 ( 0.187)	Data  0.002 ( 0.006)	Loss 1.1049e+00 (1.1362e+00)	Acc@1  65.62 ( 66.81)	Acc@5  90.62 ( 91.58)
Epoch: [15][220/391]	Time  0.177 ( 0.188)	Data  0.001 ( 0.006)	Loss 9.6199e-01 (1.1349e+00)	Acc@1  69.53 ( 66.89)	Acc@5  93.75 ( 91.57)
Epoch: [15][230/391]	Time  0.168 ( 0.187)	Data  0.002 ( 0.006)	Loss 1.1538e+00 (1.1340e+00)	Acc@1  65.62 ( 66.90)	Acc@5  88.28 ( 91.54)
Epoch: [15][240/391]	Time  0.169 ( 0.187)	Data  0.002 ( 0.006)	Loss 1.2743e+00 (1.1338e+00)	Acc@1  64.84 ( 66.92)	Acc@5  91.41 ( 91.56)
Epoch: [15][250/391]	Time  0.196 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.0940e+00 (1.1340e+00)	Acc@1  71.09 ( 66.92)	Acc@5  89.84 ( 91.54)
Epoch: [15][260/391]	Time  0.199 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.1294e+00 (1.1348e+00)	Acc@1  67.97 ( 66.86)	Acc@5  89.84 ( 91.49)
Epoch: [15][270/391]	Time  0.166 ( 0.187)	Data  0.002 ( 0.006)	Loss 1.3728e+00 (1.1361e+00)	Acc@1  62.50 ( 66.77)	Acc@5  90.62 ( 91.50)
Epoch: [15][280/391]	Time  0.160 ( 0.187)	Data  0.001 ( 0.006)	Loss 1.2776e+00 (1.1385e+00)	Acc@1  64.06 ( 66.72)	Acc@5  90.62 ( 91.47)
Epoch: [15][290/391]	Time  0.167 ( 0.186)	Data  0.002 ( 0.006)	Loss 1.2064e+00 (1.1395e+00)	Acc@1  64.84 ( 66.67)	Acc@5  89.84 ( 91.45)
Epoch: [15][300/391]	Time  0.177 ( 0.186)	Data  0.001 ( 0.006)	Loss 9.6206e-01 (1.1376e+00)	Acc@1  68.75 ( 66.66)	Acc@5  98.44 ( 91.52)
Epoch: [15][310/391]	Time  0.174 ( 0.185)	Data  0.002 ( 0.006)	Loss 1.2283e+00 (1.1385e+00)	Acc@1  64.06 ( 66.67)	Acc@5  90.62 ( 91.51)
Epoch: [15][320/391]	Time  0.174 ( 0.185)	Data  0.002 ( 0.006)	Loss 1.0061e+00 (1.1391e+00)	Acc@1  75.00 ( 66.61)	Acc@5  90.62 ( 91.54)
Epoch: [15][330/391]	Time  0.180 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.0778e+00 (1.1387e+00)	Acc@1  67.19 ( 66.60)	Acc@5  91.41 ( 91.57)
Epoch: [15][340/391]	Time  0.166 ( 0.185)	Data  0.001 ( 0.006)	Loss 9.8467e-01 (1.1397e+00)	Acc@1  73.44 ( 66.55)	Acc@5  92.97 ( 91.56)
Epoch: [15][350/391]	Time  0.168 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.2415e+00 (1.1397e+00)	Acc@1  60.16 ( 66.54)	Acc@5  91.41 ( 91.57)
Epoch: [15][360/391]	Time  0.165 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.2976e+00 (1.1410e+00)	Acc@1  64.84 ( 66.54)	Acc@5  92.19 ( 91.57)
Epoch: [15][370/391]	Time  0.223 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.1603e+00 (1.1385e+00)	Acc@1  64.84 ( 66.58)	Acc@5  91.41 ( 91.60)
Epoch: [15][380/391]	Time  0.230 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.0099e+00 (1.1363e+00)	Acc@1  70.31 ( 66.62)	Acc@5  92.97 ( 91.65)
Epoch: [15][390/391]	Time  0.177 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.0187e+00 (1.1364e+00)	Acc@1  66.25 ( 66.62)	Acc@5  96.25 ( 91.63)
## e[15] optimizer.zero_grad (sum) time: 0.3855578899383545
## e[15]       loss.backward (sum) time: 25.668652534484863
## e[15]      optimizer.step (sum) time: 3.758296251296997
## epoch[15] training(only) time: 72.81942009925842
# Switched to evaluate mode...
Test: [  0/100]	Time  0.279 ( 0.279)	Loss 1.3694e+00 (1.3694e+00)	Acc@1  62.00 ( 62.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.059 ( 0.079)	Loss 1.5843e+00 (1.5269e+00)	Acc@1  60.00 ( 60.55)	Acc@5  91.00 ( 87.91)
Test: [ 20/100]	Time  0.059 ( 0.069)	Loss 1.3457e+00 (1.4641e+00)	Acc@1  65.00 ( 60.86)	Acc@5  90.00 ( 88.67)
Test: [ 30/100]	Time  0.057 ( 0.065)	Loss 1.6512e+00 (1.4690e+00)	Acc@1  54.00 ( 60.48)	Acc@5  88.00 ( 87.97)
Test: [ 40/100]	Time  0.055 ( 0.063)	Loss 1.3692e+00 (1.4703e+00)	Acc@1  63.00 ( 60.44)	Acc@5  91.00 ( 87.98)
Test: [ 50/100]	Time  0.059 ( 0.062)	Loss 1.3688e+00 (1.4987e+00)	Acc@1  58.00 ( 59.69)	Acc@5  87.00 ( 87.39)
Test: [ 60/100]	Time  0.060 ( 0.062)	Loss 1.2344e+00 (1.4737e+00)	Acc@1  63.00 ( 59.92)	Acc@5  88.00 ( 87.67)
Test: [ 70/100]	Time  0.060 ( 0.061)	Loss 1.6428e+00 (1.4815e+00)	Acc@1  60.00 ( 60.21)	Acc@5  85.00 ( 87.59)
Test: [ 80/100]	Time  0.060 ( 0.061)	Loss 1.6576e+00 (1.4949e+00)	Acc@1  61.00 ( 59.99)	Acc@5  83.00 ( 87.43)
Test: [ 90/100]	Time  0.060 ( 0.061)	Loss 1.8065e+00 (1.4799e+00)	Acc@1  51.00 ( 60.20)	Acc@5  86.00 ( 87.56)
 * Acc@1 60.390 Acc@5 87.580
### epoch[15] execution time: 79.00709629058838
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.337 ( 0.337)	Data  0.181 ( 0.181)	Loss 9.3401e-01 (9.3401e-01)	Acc@1  73.44 ( 73.44)	Acc@5  95.31 ( 95.31)
Epoch: [16][ 10/391]	Time  0.169 ( 0.201)	Data  0.001 ( 0.022)	Loss 8.2886e-01 (1.0068e+00)	Acc@1  78.91 ( 71.24)	Acc@5  94.53 ( 93.39)
Epoch: [16][ 20/391]	Time  0.171 ( 0.187)	Data  0.002 ( 0.014)	Loss 9.8055e-01 (1.0108e+00)	Acc@1  71.88 ( 70.72)	Acc@5  91.41 ( 92.97)
Epoch: [16][ 30/391]	Time  0.167 ( 0.181)	Data  0.001 ( 0.011)	Loss 1.1026e+00 (1.0293e+00)	Acc@1  67.97 ( 70.04)	Acc@5  94.53 ( 92.97)
Epoch: [16][ 40/391]	Time  0.167 ( 0.180)	Data  0.001 ( 0.009)	Loss 1.2497e+00 (1.0212e+00)	Acc@1  65.62 ( 69.91)	Acc@5  87.50 ( 93.18)
Epoch: [16][ 50/391]	Time  0.175 ( 0.179)	Data  0.001 ( 0.008)	Loss 1.2897e+00 (1.0280e+00)	Acc@1  67.19 ( 69.79)	Acc@5  89.06 ( 93.12)
Epoch: [16][ 60/391]	Time  0.176 ( 0.179)	Data  0.001 ( 0.008)	Loss 1.0251e+00 (1.0265e+00)	Acc@1  66.41 ( 69.57)	Acc@5  94.53 ( 93.19)
Epoch: [16][ 70/391]	Time  0.157 ( 0.178)	Data  0.001 ( 0.007)	Loss 1.2372e+00 (1.0348e+00)	Acc@1  67.19 ( 69.51)	Acc@5  89.06 ( 93.00)
Epoch: [16][ 80/391]	Time  0.167 ( 0.177)	Data  0.001 ( 0.007)	Loss 9.1317e-01 (1.0362e+00)	Acc@1  70.31 ( 69.43)	Acc@5  93.75 ( 92.91)
Epoch: [16][ 90/391]	Time  0.169 ( 0.176)	Data  0.001 ( 0.007)	Loss 9.4094e-01 (1.0420e+00)	Acc@1  75.00 ( 69.31)	Acc@5  92.97 ( 92.89)
Epoch: [16][100/391]	Time  0.229 ( 0.176)	Data  0.001 ( 0.007)	Loss 1.0929e+00 (1.0431e+00)	Acc@1  68.75 ( 69.08)	Acc@5  92.97 ( 92.92)
Epoch: [16][110/391]	Time  0.221 ( 0.180)	Data  0.002 ( 0.007)	Loss 1.0484e+00 (1.0414e+00)	Acc@1  68.75 ( 69.14)	Acc@5  91.41 ( 92.93)
Epoch: [16][120/391]	Time  0.224 ( 0.184)	Data  0.001 ( 0.007)	Loss 9.2405e-01 (1.0461e+00)	Acc@1  73.44 ( 69.00)	Acc@5  92.97 ( 92.89)
Epoch: [16][130/391]	Time  0.169 ( 0.184)	Data  0.002 ( 0.007)	Loss 1.0931e+00 (1.0498e+00)	Acc@1  67.97 ( 68.92)	Acc@5  93.75 ( 92.89)
Epoch: [16][140/391]	Time  0.167 ( 0.183)	Data  0.001 ( 0.007)	Loss 9.5127e-01 (1.0516e+00)	Acc@1  71.09 ( 68.81)	Acc@5  92.97 ( 92.93)
Epoch: [16][150/391]	Time  0.166 ( 0.182)	Data  0.002 ( 0.006)	Loss 1.0555e+00 (1.0568e+00)	Acc@1  68.75 ( 68.58)	Acc@5  92.97 ( 92.91)
Epoch: [16][160/391]	Time  0.195 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.1625e+00 (1.0598e+00)	Acc@1  64.84 ( 68.47)	Acc@5  89.84 ( 92.86)
Epoch: [16][170/391]	Time  0.196 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.0132e+00 (1.0611e+00)	Acc@1  68.75 ( 68.48)	Acc@5  96.09 ( 92.82)
Epoch: [16][180/391]	Time  0.169 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.0099e+00 (1.0595e+00)	Acc@1  67.19 ( 68.42)	Acc@5  94.53 ( 92.84)
Epoch: [16][190/391]	Time  0.170 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.1944e+00 (1.0613e+00)	Acc@1  68.75 ( 68.37)	Acc@5  87.50 ( 92.79)
Epoch: [16][200/391]	Time  0.167 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.1302e+00 (1.0659e+00)	Acc@1  66.41 ( 68.26)	Acc@5  92.19 ( 92.69)
Epoch: [16][210/391]	Time  0.167 ( 0.182)	Data  0.002 ( 0.006)	Loss 1.0731e+00 (1.0639e+00)	Acc@1  67.97 ( 68.34)	Acc@5  91.41 ( 92.70)
Epoch: [16][220/391]	Time  0.191 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.0399e+00 (1.0635e+00)	Acc@1  61.72 ( 68.34)	Acc@5  95.31 ( 92.71)
Epoch: [16][230/391]	Time  0.163 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.3376e+00 (1.0644e+00)	Acc@1  63.28 ( 68.32)	Acc@5  87.50 ( 92.67)
Epoch: [16][240/391]	Time  0.166 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.2446e+00 (1.0638e+00)	Acc@1  63.28 ( 68.34)	Acc@5  89.06 ( 92.69)
Epoch: [16][250/391]	Time  0.166 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.0044e+00 (1.0655e+00)	Acc@1  71.88 ( 68.30)	Acc@5  92.97 ( 92.63)
Epoch: [16][260/391]	Time  0.165 ( 0.180)	Data  0.001 ( 0.006)	Loss 1.0071e+00 (1.0681e+00)	Acc@1  68.75 ( 68.23)	Acc@5  92.97 ( 92.57)
Epoch: [16][270/391]	Time  0.221 ( 0.180)	Data  0.001 ( 0.006)	Loss 1.3621e+00 (1.0714e+00)	Acc@1  62.50 ( 68.18)	Acc@5  85.16 ( 92.53)
Epoch: [16][280/391]	Time  0.232 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.0492e+00 (1.0730e+00)	Acc@1  67.19 ( 68.14)	Acc@5  94.53 ( 92.52)
Epoch: [16][290/391]	Time  0.224 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.1443e+00 (1.0718e+00)	Acc@1  68.75 ( 68.18)	Acc@5  90.62 ( 92.52)
Epoch: [16][300/391]	Time  0.177 ( 0.183)	Data  0.001 ( 0.006)	Loss 8.3176e-01 (1.0734e+00)	Acc@1  76.56 ( 68.12)	Acc@5  93.75 ( 92.51)
Epoch: [16][310/391]	Time  0.177 ( 0.183)	Data  0.001 ( 0.006)	Loss 9.4708e-01 (1.0719e+00)	Acc@1  69.53 ( 68.15)	Acc@5  94.53 ( 92.51)
Epoch: [16][320/391]	Time  0.161 ( 0.183)	Data  0.001 ( 0.006)	Loss 8.3386e-01 (1.0724e+00)	Acc@1  76.56 ( 68.15)	Acc@5  92.97 ( 92.48)
Epoch: [16][330/391]	Time  0.196 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.1559e+00 (1.0727e+00)	Acc@1  63.28 ( 68.12)	Acc@5  90.62 ( 92.47)
Epoch: [16][340/391]	Time  0.194 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.0899e+00 (1.0737e+00)	Acc@1  67.97 ( 68.13)	Acc@5  92.19 ( 92.46)
Epoch: [16][350/391]	Time  0.169 ( 0.183)	Data  0.001 ( 0.006)	Loss 9.9729e-01 (1.0736e+00)	Acc@1  71.09 ( 68.13)	Acc@5  92.19 ( 92.46)
Epoch: [16][360/391]	Time  0.167 ( 0.183)	Data  0.001 ( 0.006)	Loss 9.2856e-01 (1.0736e+00)	Acc@1  69.53 ( 68.15)	Acc@5  95.31 ( 92.46)
Epoch: [16][370/391]	Time  0.166 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.0349e+00 (1.0735e+00)	Acc@1  67.19 ( 68.17)	Acc@5  92.97 ( 92.44)
Epoch: [16][380/391]	Time  0.168 ( 0.182)	Data  0.002 ( 0.006)	Loss 9.1778e-01 (1.0733e+00)	Acc@1  69.53 ( 68.16)	Acc@5  95.31 ( 92.45)
Epoch: [16][390/391]	Time  0.203 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.0636e+00 (1.0733e+00)	Acc@1  75.00 ( 68.19)	Acc@5  90.00 ( 92.44)
## e[16] optimizer.zero_grad (sum) time: 0.37669873237609863
## e[16]       loss.backward (sum) time: 24.942155838012695
## e[16]      optimizer.step (sum) time: 3.6971476078033447
## epoch[16] training(only) time: 71.27646207809448
# Switched to evaluate mode...
Test: [  0/100]	Time  0.241 ( 0.241)	Loss 1.3587e+00 (1.3587e+00)	Acc@1  63.00 ( 63.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.066 ( 0.076)	Loss 1.6770e+00 (1.4801e+00)	Acc@1  58.00 ( 61.55)	Acc@5  90.00 ( 87.00)
Test: [ 20/100]	Time  0.058 ( 0.067)	Loss 1.1307e+00 (1.4246e+00)	Acc@1  69.00 ( 62.14)	Acc@5  89.00 ( 88.00)
Test: [ 30/100]	Time  0.059 ( 0.064)	Loss 1.4822e+00 (1.4186e+00)	Acc@1  61.00 ( 61.68)	Acc@5  91.00 ( 88.00)
Test: [ 40/100]	Time  0.056 ( 0.062)	Loss 1.3265e+00 (1.4324e+00)	Acc@1  60.00 ( 61.29)	Acc@5  87.00 ( 87.98)
Test: [ 50/100]	Time  0.053 ( 0.061)	Loss 1.2304e+00 (1.4407e+00)	Acc@1  62.00 ( 60.80)	Acc@5  88.00 ( 87.61)
Test: [ 60/100]	Time  0.056 ( 0.060)	Loss 1.3303e+00 (1.4228e+00)	Acc@1  61.00 ( 61.08)	Acc@5  88.00 ( 87.92)
Test: [ 70/100]	Time  0.054 ( 0.059)	Loss 1.5435e+00 (1.4269e+00)	Acc@1  60.00 ( 61.01)	Acc@5  88.00 ( 87.96)
Test: [ 80/100]	Time  0.059 ( 0.059)	Loss 1.4536e+00 (1.4290e+00)	Acc@1  58.00 ( 61.15)	Acc@5  90.00 ( 87.86)
Test: [ 90/100]	Time  0.056 ( 0.058)	Loss 1.7208e+00 (1.4208e+00)	Acc@1  57.00 ( 61.19)	Acc@5  86.00 ( 88.09)
 * Acc@1 61.540 Acc@5 88.180
### epoch[16] execution time: 77.1608738899231
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.332 ( 0.332)	Data  0.182 ( 0.182)	Loss 1.0623e+00 (1.0623e+00)	Acc@1  67.97 ( 67.97)	Acc@5  90.62 ( 90.62)
Epoch: [17][ 10/391]	Time  0.223 ( 0.207)	Data  0.001 ( 0.020)	Loss 9.3085e-01 (1.0005e+00)	Acc@1  70.31 ( 69.82)	Acc@5  96.09 ( 93.47)
Epoch: [17][ 20/391]	Time  0.221 ( 0.214)	Data  0.001 ( 0.014)	Loss 8.2277e-01 (9.6977e-01)	Acc@1  74.22 ( 71.02)	Acc@5  95.31 ( 93.75)
Epoch: [17][ 30/391]	Time  0.180 ( 0.215)	Data  0.001 ( 0.012)	Loss 9.7237e-01 (9.7820e-01)	Acc@1  70.31 ( 70.46)	Acc@5  94.53 ( 93.85)
Epoch: [17][ 40/391]	Time  0.169 ( 0.204)	Data  0.001 ( 0.010)	Loss 1.0358e+00 (9.7844e-01)	Acc@1  68.75 ( 70.50)	Acc@5  94.53 ( 93.83)
Epoch: [17][ 50/391]	Time  0.166 ( 0.198)	Data  0.002 ( 0.009)	Loss 6.7957e-01 (9.6220e-01)	Acc@1  81.25 ( 70.71)	Acc@5  96.88 ( 93.98)
Epoch: [17][ 60/391]	Time  0.188 ( 0.194)	Data  0.001 ( 0.008)	Loss 1.1595e+00 (9.6214e-01)	Acc@1  64.84 ( 70.71)	Acc@5  91.41 ( 93.93)
Epoch: [17][ 70/391]	Time  0.195 ( 0.194)	Data  0.001 ( 0.008)	Loss 9.0979e-01 (9.6088e-01)	Acc@1  73.44 ( 70.86)	Acc@5  95.31 ( 93.90)
Epoch: [17][ 80/391]	Time  0.192 ( 0.194)	Data  0.001 ( 0.008)	Loss 8.2330e-01 (9.6417e-01)	Acc@1  75.00 ( 70.88)	Acc@5  97.66 ( 93.85)
Epoch: [17][ 90/391]	Time  0.170 ( 0.192)	Data  0.001 ( 0.008)	Loss 1.0739e+00 (9.6248e-01)	Acc@1  68.75 ( 70.96)	Acc@5  92.97 ( 93.88)
Epoch: [17][100/391]	Time  0.171 ( 0.190)	Data  0.002 ( 0.007)	Loss 9.2583e-01 (9.5867e-01)	Acc@1  75.00 ( 71.15)	Acc@5  94.53 ( 93.92)
Epoch: [17][110/391]	Time  0.173 ( 0.188)	Data  0.002 ( 0.007)	Loss 9.3733e-01 (9.6743e-01)	Acc@1  71.88 ( 70.90)	Acc@5  93.75 ( 93.81)
Epoch: [17][120/391]	Time  0.172 ( 0.187)	Data  0.002 ( 0.007)	Loss 1.1905e+00 (9.7422e-01)	Acc@1  68.75 ( 70.75)	Acc@5  87.50 ( 93.69)
Epoch: [17][130/391]	Time  0.168 ( 0.186)	Data  0.001 ( 0.007)	Loss 1.0213e+00 (9.7835e-01)	Acc@1  70.31 ( 70.55)	Acc@5  93.75 ( 93.66)
Epoch: [17][140/391]	Time  0.169 ( 0.185)	Data  0.001 ( 0.007)	Loss 9.9538e-01 (9.7886e-01)	Acc@1  72.66 ( 70.57)	Acc@5  91.41 ( 93.63)
Epoch: [17][150/391]	Time  0.164 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.1338e+00 (9.8206e-01)	Acc@1  63.28 ( 70.48)	Acc@5  93.75 ( 93.61)
Epoch: [17][160/391]	Time  0.189 ( 0.183)	Data  0.001 ( 0.006)	Loss 9.5696e-01 (9.8038e-01)	Acc@1  72.66 ( 70.59)	Acc@5  92.97 ( 93.59)
Epoch: [17][170/391]	Time  0.163 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.0427e+00 (9.8267e-01)	Acc@1  66.41 ( 70.52)	Acc@5  95.31 ( 93.58)
Epoch: [17][180/391]	Time  0.220 ( 0.183)	Data  0.001 ( 0.006)	Loss 9.9239e-01 (9.8462e-01)	Acc@1  70.31 ( 70.48)	Acc@5  95.31 ( 93.56)
Epoch: [17][190/391]	Time  0.230 ( 0.185)	Data  0.001 ( 0.006)	Loss 8.1870e-01 (9.8251e-01)	Acc@1  69.53 ( 70.50)	Acc@5  96.09 ( 93.57)
Epoch: [17][200/391]	Time  0.168 ( 0.186)	Data  0.001 ( 0.006)	Loss 8.5213e-01 (9.8615e-01)	Acc@1  74.22 ( 70.39)	Acc@5  94.53 ( 93.56)
Epoch: [17][210/391]	Time  0.166 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.0056e+00 (9.8802e-01)	Acc@1  65.62 ( 70.33)	Acc@5  94.53 ( 93.53)
Epoch: [17][220/391]	Time  0.177 ( 0.185)	Data  0.001 ( 0.006)	Loss 9.8151e-01 (9.8778e-01)	Acc@1  71.09 ( 70.34)	Acc@5  92.19 ( 93.54)
Epoch: [17][230/391]	Time  0.190 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.3133e+00 (9.9389e-01)	Acc@1  61.72 ( 70.24)	Acc@5  87.50 ( 93.45)
Epoch: [17][240/391]	Time  0.192 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.1070e+00 (9.9750e-01)	Acc@1  67.97 ( 70.18)	Acc@5  90.62 ( 93.39)
Epoch: [17][250/391]	Time  0.199 ( 0.186)	Data  0.001 ( 0.006)	Loss 1.2059e+00 (1.0005e+00)	Acc@1  67.19 ( 70.13)	Acc@5  91.41 ( 93.35)
Epoch: [17][260/391]	Time  0.170 ( 0.185)	Data  0.001 ( 0.006)	Loss 1.2376e+00 (1.0019e+00)	Acc@1  65.62 ( 70.07)	Acc@5  88.28 ( 93.33)
Epoch: [17][270/391]	Time  0.166 ( 0.184)	Data  0.001 ( 0.006)	Loss 9.8913e-01 (1.0013e+00)	Acc@1  71.09 ( 70.08)	Acc@5  93.75 ( 93.36)
Epoch: [17][280/391]	Time  0.170 ( 0.184)	Data  0.002 ( 0.006)	Loss 8.6357e-01 (1.0012e+00)	Acc@1  72.66 ( 70.08)	Acc@5  95.31 ( 93.37)
Epoch: [17][290/391]	Time  0.178 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.5106e+00 (1.0042e+00)	Acc@1  61.72 ( 70.05)	Acc@5  89.06 ( 93.32)
Epoch: [17][300/391]	Time  0.174 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.0741e+00 (1.0063e+00)	Acc@1  67.19 ( 70.00)	Acc@5  91.41 ( 93.31)
Epoch: [17][310/391]	Time  0.174 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.0459e+00 (1.0071e+00)	Acc@1  69.53 ( 70.01)	Acc@5  92.97 ( 93.29)
Epoch: [17][320/391]	Time  0.167 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.1848e+00 (1.0090e+00)	Acc@1  69.53 ( 69.98)	Acc@5  90.62 ( 93.25)
Epoch: [17][330/391]	Time  0.166 ( 0.182)	Data  0.001 ( 0.006)	Loss 9.0903e-01 (1.0099e+00)	Acc@1  72.66 ( 69.98)	Acc@5  92.97 ( 93.23)
Epoch: [17][340/391]	Time  0.221 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.0089e+00 (1.0106e+00)	Acc@1  69.53 ( 69.99)	Acc@5  95.31 ( 93.23)
Epoch: [17][350/391]	Time  0.224 ( 0.183)	Data  0.001 ( 0.006)	Loss 1.0308e+00 (1.0119e+00)	Acc@1  70.31 ( 69.93)	Acc@5  90.62 ( 93.20)
Epoch: [17][360/391]	Time  0.222 ( 0.184)	Data  0.001 ( 0.006)	Loss 9.3157e-01 (1.0136e+00)	Acc@1  68.75 ( 69.88)	Acc@5  96.88 ( 93.19)
Epoch: [17][370/391]	Time  0.175 ( 0.184)	Data  0.002 ( 0.006)	Loss 1.1083e+00 (1.0143e+00)	Acc@1  64.84 ( 69.84)	Acc@5  92.97 ( 93.20)
Epoch: [17][380/391]	Time  0.172 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.1920e+00 (1.0151e+00)	Acc@1  67.19 ( 69.82)	Acc@5  88.28 ( 93.17)
Epoch: [17][390/391]	Time  0.169 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.0455e+00 (1.0165e+00)	Acc@1  70.00 ( 69.76)	Acc@5  91.25 ( 93.16)
## e[17] optimizer.zero_grad (sum) time: 0.3795771598815918
## e[17]       loss.backward (sum) time: 25.012531995773315
## e[17]      optimizer.step (sum) time: 3.690077304840088
## epoch[17] training(only) time: 71.99614548683167
# Switched to evaluate mode...
Test: [  0/100]	Time  0.238 ( 0.238)	Loss 1.2998e+00 (1.2998e+00)	Acc@1  61.00 ( 61.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.057 ( 0.073)	Loss 1.4315e+00 (1.3795e+00)	Acc@1  64.00 ( 63.64)	Acc@5  87.00 ( 88.00)
Test: [ 20/100]	Time  0.056 ( 0.065)	Loss 1.2680e+00 (1.3378e+00)	Acc@1  67.00 ( 64.52)	Acc@5  91.00 ( 88.38)
Test: [ 30/100]	Time  0.057 ( 0.063)	Loss 1.4784e+00 (1.3474e+00)	Acc@1  62.00 ( 64.26)	Acc@5  87.00 ( 88.16)
Test: [ 40/100]	Time  0.056 ( 0.061)	Loss 1.3366e+00 (1.3561e+00)	Acc@1  60.00 ( 63.71)	Acc@5  92.00 ( 88.44)
Test: [ 50/100]	Time  0.057 ( 0.060)	Loss 1.2430e+00 (1.3725e+00)	Acc@1  62.00 ( 63.16)	Acc@5  89.00 ( 88.29)
Test: [ 60/100]	Time  0.057 ( 0.060)	Loss 1.5683e+00 (1.3602e+00)	Acc@1  62.00 ( 63.34)	Acc@5  85.00 ( 88.52)
Test: [ 70/100]	Time  0.053 ( 0.059)	Loss 1.3879e+00 (1.3614e+00)	Acc@1  66.00 ( 63.28)	Acc@5  88.00 ( 88.55)
Test: [ 80/100]	Time  0.054 ( 0.058)	Loss 1.6189e+00 (1.3716e+00)	Acc@1  61.00 ( 62.85)	Acc@5  86.00 ( 88.43)
Test: [ 90/100]	Time  0.053 ( 0.058)	Loss 1.8362e+00 (1.3653e+00)	Acc@1  53.00 ( 62.89)	Acc@5  83.00 ( 88.54)
 * Acc@1 63.070 Acc@5 88.530
### epoch[17] execution time: 77.8822615146637
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.398 ( 0.398)	Data  0.238 ( 0.238)	Loss 8.2943e-01 (8.2943e-01)	Acc@1  69.53 ( 69.53)	Acc@5  96.09 ( 96.09)
Epoch: [18][ 10/391]	Time  0.161 ( 0.184)	Data  0.002 ( 0.025)	Loss 1.0365e+00 (9.7305e-01)	Acc@1  67.19 ( 68.82)	Acc@5  92.19 ( 94.18)
Epoch: [18][ 20/391]	Time  0.178 ( 0.181)	Data  0.001 ( 0.016)	Loss 9.8239e-01 (9.1966e-01)	Acc@1  67.97 ( 71.13)	Acc@5  96.09 ( 94.23)
Epoch: [18][ 30/391]	Time  0.195 ( 0.182)	Data  0.001 ( 0.012)	Loss 9.5380e-01 (8.8756e-01)	Acc@1  71.09 ( 72.03)	Acc@5  95.31 ( 94.93)
Epoch: [18][ 40/391]	Time  0.175 ( 0.181)	Data  0.001 ( 0.010)	Loss 1.0773e+00 (8.8705e-01)	Acc@1  60.94 ( 72.07)	Acc@5  94.53 ( 94.89)
Epoch: [18][ 50/391]	Time  0.163 ( 0.179)	Data  0.001 ( 0.009)	Loss 8.8559e-01 (8.8610e-01)	Acc@1  75.78 ( 72.17)	Acc@5  96.09 ( 94.96)
Epoch: [18][ 60/391]	Time  0.179 ( 0.177)	Data  0.001 ( 0.008)	Loss 9.1546e-01 (8.9309e-01)	Acc@1  72.66 ( 72.07)	Acc@5  93.75 ( 94.81)
Epoch: [18][ 70/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.008)	Loss 8.3499e-01 (9.0602e-01)	Acc@1  72.66 ( 71.86)	Acc@5  96.88 ( 94.69)
Epoch: [18][ 80/391]	Time  0.215 ( 0.178)	Data  0.001 ( 0.007)	Loss 9.0019e-01 (9.0996e-01)	Acc@1  74.22 ( 71.95)	Acc@5  95.31 ( 94.54)
Epoch: [18][ 90/391]	Time  0.228 ( 0.183)	Data  0.001 ( 0.007)	Loss 8.3864e-01 (9.0437e-01)	Acc@1  74.22 ( 72.12)	Acc@5  95.31 ( 94.63)
Epoch: [18][100/391]	Time  0.174 ( 0.185)	Data  0.001 ( 0.007)	Loss 5.8471e-01 (8.9929e-01)	Acc@1  82.81 ( 72.32)	Acc@5  97.66 ( 94.64)
Epoch: [18][110/391]	Time  0.175 ( 0.185)	Data  0.001 ( 0.007)	Loss 8.7614e-01 (9.0241e-01)	Acc@1  72.66 ( 72.35)	Acc@5  93.75 ( 94.56)
Epoch: [18][120/391]	Time  0.175 ( 0.184)	Data  0.001 ( 0.007)	Loss 8.3066e-01 (9.0423e-01)	Acc@1  74.22 ( 72.27)	Acc@5  96.88 ( 94.60)
Epoch: [18][130/391]	Time  0.186 ( 0.184)	Data  0.001 ( 0.007)	Loss 1.0552e+00 (9.1085e-01)	Acc@1  67.97 ( 72.09)	Acc@5  92.19 ( 94.47)
Epoch: [18][140/391]	Time  0.179 ( 0.183)	Data  0.001 ( 0.007)	Loss 9.0223e-01 (9.1275e-01)	Acc@1  71.88 ( 72.04)	Acc@5  93.75 ( 94.42)
Epoch: [18][150/391]	Time  0.186 ( 0.183)	Data  0.001 ( 0.007)	Loss 1.0381e+00 (9.1767e-01)	Acc@1  69.53 ( 71.93)	Acc@5  90.62 ( 94.34)
Epoch: [18][160/391]	Time  0.163 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.0443e+00 (9.1966e-01)	Acc@1  70.31 ( 71.82)	Acc@5  92.97 ( 94.38)
Epoch: [18][170/391]	Time  0.162 ( 0.181)	Data  0.001 ( 0.006)	Loss 9.8818e-01 (9.2427e-01)	Acc@1  69.53 ( 71.71)	Acc@5  95.31 ( 94.35)
Epoch: [18][180/391]	Time  0.182 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.0142e+00 (9.2683e-01)	Acc@1  67.97 ( 71.67)	Acc@5  94.53 ( 94.36)
Epoch: [18][190/391]	Time  0.178 ( 0.180)	Data  0.001 ( 0.006)	Loss 8.5343e-01 (9.3182e-01)	Acc@1  74.22 ( 71.55)	Acc@5  96.88 ( 94.33)
Epoch: [18][200/391]	Time  0.180 ( 0.181)	Data  0.001 ( 0.006)	Loss 8.4505e-01 (9.3328e-01)	Acc@1  77.34 ( 71.55)	Acc@5  93.75 ( 94.29)
Epoch: [18][210/391]	Time  0.170 ( 0.180)	Data  0.001 ( 0.006)	Loss 1.2173e+00 (9.3521e-01)	Acc@1  69.53 ( 71.56)	Acc@5  91.41 ( 94.29)
Epoch: [18][220/391]	Time  0.168 ( 0.180)	Data  0.001 ( 0.006)	Loss 8.9941e-01 (9.3564e-01)	Acc@1  74.22 ( 71.63)	Acc@5  93.75 ( 94.27)
Epoch: [18][230/391]	Time  0.161 ( 0.179)	Data  0.001 ( 0.006)	Loss 9.1908e-01 (9.3680e-01)	Acc@1  69.53 ( 71.65)	Acc@5  92.97 ( 94.23)
Epoch: [18][240/391]	Time  0.218 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.0191e+00 (9.3934e-01)	Acc@1  69.53 ( 71.57)	Acc@5  94.53 ( 94.18)
Epoch: [18][250/391]	Time  0.223 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.2776e+00 (9.4279e-01)	Acc@1  60.94 ( 71.48)	Acc@5  92.19 ( 94.13)
Epoch: [18][260/391]	Time  0.223 ( 0.183)	Data  0.001 ( 0.006)	Loss 9.7783e-01 (9.4574e-01)	Acc@1  68.75 ( 71.38)	Acc@5  95.31 ( 94.08)
Epoch: [18][270/391]	Time  0.173 ( 0.182)	Data  0.001 ( 0.006)	Loss 1.0721e+00 (9.4828e-01)	Acc@1  67.19 ( 71.34)	Acc@5  92.19 ( 94.08)
Epoch: [18][280/391]	Time  0.172 ( 0.182)	Data  0.001 ( 0.006)	Loss 8.4151e-01 (9.5039e-01)	Acc@1  72.66 ( 71.29)	Acc@5  97.66 ( 94.04)
Epoch: [18][290/391]	Time  0.187 ( 0.182)	Data  0.001 ( 0.006)	Loss 9.3719e-01 (9.5098e-01)	Acc@1  73.44 ( 71.28)	Acc@5  95.31 ( 94.07)
Epoch: [18][300/391]	Time  0.180 ( 0.182)	Data  0.001 ( 0.006)	Loss 9.3701e-01 (9.5072e-01)	Acc@1  72.66 ( 71.31)	Acc@5  96.09 ( 94.06)
Epoch: [18][310/391]	Time  0.179 ( 0.182)	Data  0.002 ( 0.006)	Loss 8.9209e-01 (9.4993e-01)	Acc@1  73.44 ( 71.31)	Acc@5  92.19 ( 94.08)
Epoch: [18][320/391]	Time  0.166 ( 0.182)	Data  0.001 ( 0.006)	Loss 8.6894e-01 (9.4876e-01)	Acc@1  76.56 ( 71.36)	Acc@5  94.53 ( 94.09)
Epoch: [18][330/391]	Time  0.162 ( 0.181)	Data  0.001 ( 0.006)	Loss 9.2004e-01 (9.5065e-01)	Acc@1  71.09 ( 71.29)	Acc@5  96.09 ( 94.07)
Epoch: [18][340/391]	Time  0.165 ( 0.181)	Data  0.001 ( 0.006)	Loss 9.6461e-01 (9.5047e-01)	Acc@1  67.19 ( 71.29)	Acc@5  96.09 ( 94.09)
Epoch: [18][350/391]	Time  0.177 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.1740e+00 (9.5300e-01)	Acc@1  66.41 ( 71.21)	Acc@5  93.75 ( 94.08)
Epoch: [18][360/391]	Time  0.181 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.0531e+00 (9.5341e-01)	Acc@1  70.31 ( 71.20)	Acc@5  93.75 ( 94.06)
Epoch: [18][370/391]	Time  0.177 ( 0.181)	Data  0.001 ( 0.006)	Loss 1.0289e+00 (9.5525e-01)	Acc@1  71.88 ( 71.17)	Acc@5  92.97 ( 94.03)
Epoch: [18][380/391]	Time  0.165 ( 0.180)	Data  0.001 ( 0.006)	Loss 9.8417e-01 (9.5690e-01)	Acc@1  72.66 ( 71.14)	Acc@5  94.53 ( 94.01)
Epoch: [18][390/391]	Time  0.162 ( 0.180)	Data  0.001 ( 0.006)	Loss 1.1503e+00 (9.5615e-01)	Acc@1  63.75 ( 71.17)	Acc@5  91.25 ( 94.02)
## e[18] optimizer.zero_grad (sum) time: 0.3616495132446289
## e[18]       loss.backward (sum) time: 24.193348169326782
## e[18]      optimizer.step (sum) time: 3.58144474029541
## epoch[18] training(only) time: 70.48047256469727
# Switched to evaluate mode...
Test: [  0/100]	Time  0.252 ( 0.252)	Loss 1.3123e+00 (1.3123e+00)	Acc@1  65.00 ( 65.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.058 ( 0.072)	Loss 1.5414e+00 (1.3699e+00)	Acc@1  59.00 ( 63.82)	Acc@5  91.00 ( 89.45)
Test: [ 20/100]	Time  0.056 ( 0.063)	Loss 1.0952e+00 (1.3118e+00)	Acc@1  72.00 ( 64.86)	Acc@5  93.00 ( 90.10)
Test: [ 30/100]	Time  0.067 ( 0.064)	Loss 1.6834e+00 (1.3315e+00)	Acc@1  55.00 ( 64.45)	Acc@5  82.00 ( 89.35)
Test: [ 40/100]	Time  0.071 ( 0.065)	Loss 1.4196e+00 (1.3526e+00)	Acc@1  63.00 ( 63.76)	Acc@5  89.00 ( 89.02)
Test: [ 50/100]	Time  0.066 ( 0.065)	Loss 1.4173e+00 (1.3546e+00)	Acc@1  66.00 ( 63.41)	Acc@5  88.00 ( 89.12)
Test: [ 60/100]	Time  0.068 ( 0.065)	Loss 1.1374e+00 (1.3339e+00)	Acc@1  67.00 ( 63.54)	Acc@5  92.00 ( 89.38)
Test: [ 70/100]	Time  0.068 ( 0.066)	Loss 1.6043e+00 (1.3426e+00)	Acc@1  59.00 ( 63.21)	Acc@5  90.00 ( 89.30)
Test: [ 80/100]	Time  0.067 ( 0.066)	Loss 1.6339e+00 (1.3490e+00)	Acc@1  62.00 ( 63.26)	Acc@5  82.00 ( 89.19)
Test: [ 90/100]	Time  0.066 ( 0.066)	Loss 1.5575e+00 (1.3397e+00)	Acc@1  55.00 ( 63.45)	Acc@5  90.00 ( 89.38)
 * Acc@1 63.650 Acc@5 89.460
### epoch[18] execution time: 77.13659119606018
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.380 ( 0.380)	Data  0.181 ( 0.181)	Loss 8.6144e-01 (8.6144e-01)	Acc@1  74.22 ( 74.22)	Acc@5  94.53 ( 94.53)
Epoch: [19][ 10/391]	Time  0.175 ( 0.194)	Data  0.001 ( 0.020)	Loss 9.0503e-01 (8.4461e-01)	Acc@1  69.53 ( 73.15)	Acc@5  96.88 ( 95.67)
Epoch: [19][ 20/391]	Time  0.178 ( 0.186)	Data  0.001 ( 0.013)	Loss 9.3297e-01 (8.7914e-01)	Acc@1  72.66 ( 73.07)	Acc@5  92.97 ( 94.98)
Epoch: [19][ 30/391]	Time  0.179 ( 0.182)	Data  0.001 ( 0.010)	Loss 7.7995e-01 (8.7926e-01)	Acc@1  76.56 ( 73.19)	Acc@5  95.31 ( 94.91)
Epoch: [19][ 40/391]	Time  0.180 ( 0.182)	Data  0.001 ( 0.009)	Loss 7.9600e-01 (8.6654e-01)	Acc@1  76.56 ( 73.21)	Acc@5  96.88 ( 95.08)
Epoch: [19][ 50/391]	Time  0.163 ( 0.181)	Data  0.001 ( 0.009)	Loss 8.1429e-01 (8.6509e-01)	Acc@1  75.00 ( 73.38)	Acc@5  96.88 ( 95.01)
Epoch: [19][ 60/391]	Time  0.165 ( 0.178)	Data  0.001 ( 0.008)	Loss 8.0059e-01 (8.6551e-01)	Acc@1  76.56 ( 73.50)	Acc@5  93.75 ( 95.03)
Epoch: [19][ 70/391]	Time  0.165 ( 0.176)	Data  0.001 ( 0.008)	Loss 7.9599e-01 (8.7058e-01)	Acc@1  78.12 ( 73.22)	Acc@5  96.09 ( 95.03)
Epoch: [19][ 80/391]	Time  0.167 ( 0.176)	Data  0.001 ( 0.007)	Loss 1.0253e+00 (8.7098e-01)	Acc@1  71.09 ( 73.35)	Acc@5  90.62 ( 94.91)
Epoch: [19][ 90/391]	Time  0.210 ( 0.177)	Data  0.001 ( 0.007)	Loss 9.2073e-01 (8.7359e-01)	Acc@1  71.88 ( 73.26)	Acc@5  94.53 ( 94.99)
Epoch: [19][100/391]	Time  0.174 ( 0.177)	Data  0.001 ( 0.007)	Loss 8.5755e-01 (8.8039e-01)	Acc@1  74.22 ( 73.22)	Acc@5  92.97 ( 94.80)
Epoch: [19][110/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.007)	Loss 8.1386e-01 (8.7474e-01)	Acc@1  78.12 ( 73.40)	Acc@5  92.97 ( 94.90)
Epoch: [19][120/391]	Time  0.171 ( 0.175)	Data  0.001 ( 0.006)	Loss 8.9038e-01 (8.8042e-01)	Acc@1  74.22 ( 73.34)	Acc@5  92.97 ( 94.80)
Epoch: [19][130/391]	Time  0.167 ( 0.175)	Data  0.001 ( 0.006)	Loss 9.0407e-01 (8.8253e-01)	Acc@1  75.00 ( 73.31)	Acc@5  93.75 ( 94.82)
Epoch: [19][140/391]	Time  0.225 ( 0.176)	Data  0.001 ( 0.006)	Loss 9.0415e-01 (8.8235e-01)	Acc@1  72.66 ( 73.32)	Acc@5  93.75 ( 94.82)
Epoch: [19][150/391]	Time  0.220 ( 0.179)	Data  0.001 ( 0.006)	Loss 8.6644e-01 (8.8015e-01)	Acc@1  69.53 ( 73.28)	Acc@5  94.53 ( 94.86)
Epoch: [19][160/391]	Time  0.175 ( 0.181)	Data  0.002 ( 0.006)	Loss 1.0510e+00 (8.8318e-01)	Acc@1  67.97 ( 73.19)	Acc@5  93.75 ( 94.85)
Epoch: [19][170/391]	Time  0.166 ( 0.181)	Data  0.002 ( 0.006)	Loss 9.3582e-01 (8.8675e-01)	Acc@1  70.31 ( 73.14)	Acc@5  96.09 ( 94.82)
Epoch: [19][180/391]	Time  0.168 ( 0.180)	Data  0.002 ( 0.006)	Loss 9.7414e-01 (8.8582e-01)	Acc@1  71.88 ( 73.14)	Acc@5  93.75 ( 94.88)
Epoch: [19][190/391]	Time  0.171 ( 0.180)	Data  0.002 ( 0.006)	Loss 6.7398e-01 (8.8939e-01)	Acc@1  82.03 ( 72.98)	Acc@5  96.09 ( 94.83)
Epoch: [19][200/391]	Time  0.174 ( 0.180)	Data  0.001 ( 0.006)	Loss 9.8537e-01 (8.9048e-01)	Acc@1  69.53 ( 72.94)	Acc@5  92.97 ( 94.79)
Epoch: [19][210/391]	Time  0.177 ( 0.180)	Data  0.001 ( 0.006)	Loss 8.9122e-01 (8.9059e-01)	Acc@1  71.88 ( 72.95)	Acc@5  94.53 ( 94.76)
Epoch: [19][220/391]	Time  0.160 ( 0.179)	Data  0.001 ( 0.006)	Loss 8.2909e-01 (8.9233e-01)	Acc@1  75.78 ( 72.86)	Acc@5  94.53 ( 94.79)
Epoch: [19][230/391]	Time  0.164 ( 0.178)	Data  0.001 ( 0.006)	Loss 8.2612e-01 (8.9449e-01)	Acc@1  71.88 ( 72.81)	Acc@5  94.53 ( 94.74)
Epoch: [19][240/391]	Time  0.192 ( 0.178)	Data  0.001 ( 0.006)	Loss 7.2113e-01 (8.9590e-01)	Acc@1  77.34 ( 72.80)	Acc@5  96.88 ( 94.72)
Epoch: [19][250/391]	Time  0.172 ( 0.178)	Data  0.001 ( 0.006)	Loss 8.8623e-01 (8.9749e-01)	Acc@1  73.44 ( 72.74)	Acc@5  93.75 ( 94.72)
Epoch: [19][260/391]	Time  0.166 ( 0.178)	Data  0.001 ( 0.006)	Loss 9.6796e-01 (8.9753e-01)	Acc@1  74.22 ( 72.72)	Acc@5  90.62 ( 94.70)
Epoch: [19][270/391]	Time  0.178 ( 0.178)	Data  0.001 ( 0.006)	Loss 7.9830e-01 (8.9908e-01)	Acc@1  70.31 ( 72.66)	Acc@5  95.31 ( 94.71)
Epoch: [19][280/391]	Time  0.163 ( 0.177)	Data  0.001 ( 0.006)	Loss 1.0522e+00 (9.0092e-01)	Acc@1  66.41 ( 72.61)	Acc@5  93.75 ( 94.66)
Epoch: [19][290/391]	Time  0.166 ( 0.177)	Data  0.001 ( 0.006)	Loss 9.2813e-01 (9.0097e-01)	Acc@1  76.56 ( 72.65)	Acc@5  95.31 ( 94.66)
Epoch: [19][300/391]	Time  0.163 ( 0.177)	Data  0.001 ( 0.006)	Loss 7.4457e-01 (9.0136e-01)	Acc@1  74.22 ( 72.62)	Acc@5  97.66 ( 94.67)
Epoch: [19][310/391]	Time  0.220 ( 0.178)	Data  0.001 ( 0.006)	Loss 8.6605e-01 (8.9969e-01)	Acc@1  71.88 ( 72.65)	Acc@5  93.75 ( 94.68)
Epoch: [19][320/391]	Time  0.226 ( 0.179)	Data  0.001 ( 0.006)	Loss 9.2873e-01 (8.9923e-01)	Acc@1  70.31 ( 72.65)	Acc@5  96.09 ( 94.70)
Epoch: [19][330/391]	Time  0.170 ( 0.180)	Data  0.001 ( 0.006)	Loss 8.1061e-01 (9.0114e-01)	Acc@1  77.34 ( 72.62)	Acc@5  94.53 ( 94.66)
Epoch: [19][340/391]	Time  0.181 ( 0.180)	Data  0.001 ( 0.006)	Loss 9.2649e-01 (9.0258e-01)	Acc@1  71.09 ( 72.60)	Acc@5  95.31 ( 94.64)
Epoch: [19][350/391]	Time  0.168 ( 0.179)	Data  0.001 ( 0.006)	Loss 8.2405e-01 (9.0377e-01)	Acc@1  76.56 ( 72.58)	Acc@5  96.88 ( 94.62)
Epoch: [19][360/391]	Time  0.180 ( 0.179)	Data  0.001 ( 0.006)	Loss 8.3237e-01 (9.0486e-01)	Acc@1  71.88 ( 72.52)	Acc@5  94.53 ( 94.59)
Epoch: [19][370/391]	Time  0.180 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.0436e+00 (9.0632e-01)	Acc@1  71.88 ( 72.46)	Acc@5  93.75 ( 94.57)
Epoch: [19][380/391]	Time  0.161 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.3813e+00 (9.0835e-01)	Acc@1  63.28 ( 72.39)	Acc@5  89.06 ( 94.56)
Epoch: [19][390/391]	Time  0.160 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.0204e+00 (9.1077e-01)	Acc@1  71.25 ( 72.36)	Acc@5  93.75 ( 94.52)
## e[19] optimizer.zero_grad (sum) time: 0.3583090305328369
## e[19]       loss.backward (sum) time: 23.815041542053223
## e[19]      optimizer.step (sum) time: 3.5052571296691895
## epoch[19] training(only) time: 70.05714130401611
# Switched to evaluate mode...
Test: [  0/100]	Time  0.243 ( 0.243)	Loss 1.2191e+00 (1.2191e+00)	Acc@1  68.00 ( 68.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.053 ( 0.070)	Loss 1.4894e+00 (1.4007e+00)	Acc@1  62.00 ( 64.00)	Acc@5  92.00 ( 88.73)
Test: [ 20/100]	Time  0.052 ( 0.062)	Loss 1.2204e+00 (1.3437e+00)	Acc@1  68.00 ( 64.48)	Acc@5  91.00 ( 89.76)
Test: [ 30/100]	Time  0.058 ( 0.059)	Loss 1.6149e+00 (1.3644e+00)	Acc@1  55.00 ( 63.45)	Acc@5  91.00 ( 89.48)
Test: [ 40/100]	Time  0.057 ( 0.059)	Loss 1.2591e+00 (1.3464e+00)	Acc@1  62.00 ( 63.56)	Acc@5  94.00 ( 90.05)
Test: [ 50/100]	Time  0.059 ( 0.059)	Loss 1.5185e+00 (1.3687e+00)	Acc@1  66.00 ( 63.57)	Acc@5  85.00 ( 89.39)
Test: [ 60/100]	Time  0.058 ( 0.059)	Loss 1.2392e+00 (1.3513e+00)	Acc@1  64.00 ( 63.52)	Acc@5  91.00 ( 89.66)
Test: [ 70/100]	Time  0.059 ( 0.059)	Loss 1.3972e+00 (1.3622e+00)	Acc@1  61.00 ( 63.28)	Acc@5  92.00 ( 89.49)
Test: [ 80/100]	Time  0.059 ( 0.059)	Loss 1.5409e+00 (1.3710e+00)	Acc@1  59.00 ( 63.06)	Acc@5  87.00 ( 89.37)
Test: [ 90/100]	Time  0.058 ( 0.059)	Loss 1.7391e+00 (1.3617e+00)	Acc@1  54.00 ( 63.33)	Acc@5  87.00 ( 89.36)
 * Acc@1 63.560 Acc@5 89.460
### epoch[19] execution time: 76.05471754074097
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.387 ( 0.387)	Data  0.214 ( 0.214)	Loss 9.2871e-01 (9.2871e-01)	Acc@1  69.53 ( 69.53)	Acc@5  93.75 ( 93.75)
Epoch: [20][ 10/391]	Time  0.161 ( 0.186)	Data  0.001 ( 0.023)	Loss 7.4326e-01 (8.6667e-01)	Acc@1  78.12 ( 73.65)	Acc@5  96.09 ( 94.96)
Epoch: [20][ 20/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.014)	Loss 9.5190e-01 (8.3503e-01)	Acc@1  72.66 ( 74.78)	Acc@5  97.66 ( 95.57)
Epoch: [20][ 30/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.011)	Loss 6.1091e-01 (8.1363e-01)	Acc@1  85.94 ( 75.63)	Acc@5  98.44 ( 95.82)
Epoch: [20][ 40/391]	Time  0.217 ( 0.177)	Data  0.002 ( 0.010)	Loss 1.0547e+00 (8.1522e-01)	Acc@1  69.53 ( 75.21)	Acc@5  92.19 ( 95.73)
Epoch: [20][ 50/391]	Time  0.228 ( 0.186)	Data  0.001 ( 0.009)	Loss 6.5605e-01 (8.1718e-01)	Acc@1  77.34 ( 74.97)	Acc@5  98.44 ( 95.66)
Epoch: [20][ 60/391]	Time  0.177 ( 0.190)	Data  0.001 ( 0.009)	Loss 9.1296e-01 (8.1735e-01)	Acc@1  69.53 ( 74.86)	Acc@5  96.88 ( 95.65)
Epoch: [20][ 70/391]	Time  0.177 ( 0.188)	Data  0.001 ( 0.008)	Loss 8.5330e-01 (8.0702e-01)	Acc@1  77.34 ( 75.02)	Acc@5  95.31 ( 95.79)
Epoch: [20][ 80/391]	Time  0.173 ( 0.186)	Data  0.001 ( 0.008)	Loss 7.2314e-01 (8.0954e-01)	Acc@1  78.12 ( 75.08)	Acc@5  96.88 ( 95.77)
Epoch: [20][ 90/391]	Time  0.168 ( 0.184)	Data  0.001 ( 0.007)	Loss 7.4601e-01 (8.1660e-01)	Acc@1  76.56 ( 75.03)	Acc@5  96.88 ( 95.62)
Epoch: [20][100/391]	Time  0.166 ( 0.183)	Data  0.001 ( 0.007)	Loss 8.7154e-01 (8.2286e-01)	Acc@1  69.53 ( 74.82)	Acc@5  97.66 ( 95.53)
Epoch: [20][110/391]	Time  0.163 ( 0.181)	Data  0.001 ( 0.007)	Loss 9.3667e-01 (8.2541e-01)	Acc@1  70.31 ( 74.76)	Acc@5  95.31 ( 95.54)
Epoch: [20][120/391]	Time  0.153 ( 0.179)	Data  0.001 ( 0.007)	Loss 7.8059e-01 (8.2826e-01)	Acc@1  75.00 ( 74.70)	Acc@5  95.31 ( 95.49)
Epoch: [20][130/391]	Time  0.162 ( 0.178)	Data  0.002 ( 0.007)	Loss 1.0352e+00 (8.3045e-01)	Acc@1  65.62 ( 74.53)	Acc@5  95.31 ( 95.48)
Epoch: [20][140/391]	Time  0.175 ( 0.178)	Data  0.001 ( 0.007)	Loss 9.0280e-01 (8.2969e-01)	Acc@1  70.31 ( 74.61)	Acc@5  92.97 ( 95.42)
Epoch: [20][150/391]	Time  0.178 ( 0.178)	Data  0.002 ( 0.006)	Loss 1.0627e+00 (8.3830e-01)	Acc@1  65.62 ( 74.38)	Acc@5  94.53 ( 95.33)
Epoch: [20][160/391]	Time  0.164 ( 0.178)	Data  0.001 ( 0.007)	Loss 9.3982e-01 (8.4178e-01)	Acc@1  75.78 ( 74.34)	Acc@5  91.41 ( 95.30)
Epoch: [20][170/391]	Time  0.152 ( 0.177)	Data  0.001 ( 0.006)	Loss 9.7296e-01 (8.4305e-01)	Acc@1  71.88 ( 74.29)	Acc@5  94.53 ( 95.28)
Epoch: [20][180/391]	Time  0.161 ( 0.176)	Data  0.002 ( 0.006)	Loss 7.2016e-01 (8.4311e-01)	Acc@1  77.34 ( 74.31)	Acc@5  97.66 ( 95.29)
Epoch: [20][190/391]	Time  0.159 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.0784e+00 (8.4149e-01)	Acc@1  67.97 ( 74.36)	Acc@5  92.19 ( 95.30)
Epoch: [20][200/391]	Time  0.222 ( 0.176)	Data  0.001 ( 0.006)	Loss 7.9034e-01 (8.3992e-01)	Acc@1  79.69 ( 74.46)	Acc@5  94.53 ( 95.29)
Epoch: [20][210/391]	Time  0.228 ( 0.178)	Data  0.001 ( 0.006)	Loss 8.7548e-01 (8.3831e-01)	Acc@1  72.66 ( 74.43)	Acc@5  95.31 ( 95.31)
Epoch: [20][220/391]	Time  0.184 ( 0.180)	Data  0.001 ( 0.006)	Loss 8.9299e-01 (8.4024e-01)	Acc@1  71.09 ( 74.37)	Acc@5  96.09 ( 95.29)
Epoch: [20][230/391]	Time  0.179 ( 0.180)	Data  0.001 ( 0.006)	Loss 8.9039e-01 (8.4353e-01)	Acc@1  73.44 ( 74.29)	Acc@5  95.31 ( 95.27)
Epoch: [20][240/391]	Time  0.179 ( 0.180)	Data  0.001 ( 0.006)	Loss 7.8933e-01 (8.4596e-01)	Acc@1  78.91 ( 74.26)	Acc@5  97.66 ( 95.26)
Epoch: [20][250/391]	Time  0.167 ( 0.180)	Data  0.001 ( 0.006)	Loss 8.5297e-01 (8.4558e-01)	Acc@1  72.66 ( 74.25)	Acc@5  93.75 ( 95.26)
Epoch: [20][260/391]	Time  0.166 ( 0.179)	Data  0.001 ( 0.006)	Loss 8.6789e-01 (8.4860e-01)	Acc@1  74.22 ( 74.14)	Acc@5  96.88 ( 95.24)
Epoch: [20][270/391]	Time  0.172 ( 0.179)	Data  0.001 ( 0.006)	Loss 8.2641e-01 (8.4979e-01)	Acc@1  78.91 ( 74.16)	Acc@5  95.31 ( 95.22)
Epoch: [20][280/391]	Time  0.161 ( 0.178)	Data  0.001 ( 0.006)	Loss 8.7278e-01 (8.5035e-01)	Acc@1  75.00 ( 74.17)	Acc@5  96.09 ( 95.20)
Epoch: [20][290/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.006)	Loss 7.7616e-01 (8.5116e-01)	Acc@1  75.78 ( 74.13)	Acc@5  96.88 ( 95.17)
Epoch: [20][300/391]	Time  0.183 ( 0.177)	Data  0.001 ( 0.006)	Loss 6.6885e-01 (8.5083e-01)	Acc@1  77.34 ( 74.15)	Acc@5  98.44 ( 95.16)
Epoch: [20][310/391]	Time  0.181 ( 0.177)	Data  0.001 ( 0.006)	Loss 9.3591e-01 (8.5102e-01)	Acc@1  75.78 ( 74.17)	Acc@5  92.19 ( 95.17)
Epoch: [20][320/391]	Time  0.175 ( 0.177)	Data  0.002 ( 0.006)	Loss 8.7779e-01 (8.5534e-01)	Acc@1  71.88 ( 74.08)	Acc@5  98.44 ( 95.09)
Epoch: [20][330/391]	Time  0.174 ( 0.177)	Data  0.001 ( 0.006)	Loss 8.4403e-01 (8.5736e-01)	Acc@1  74.22 ( 74.02)	Acc@5  95.31 ( 95.06)
Epoch: [20][340/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.006)	Loss 8.4129e-01 (8.5890e-01)	Acc@1  75.78 ( 73.97)	Acc@5  92.97 ( 95.03)
Epoch: [20][350/391]	Time  0.158 ( 0.176)	Data  0.001 ( 0.006)	Loss 9.1188e-01 (8.6077e-01)	Acc@1  75.00 ( 73.93)	Acc@5  93.75 ( 95.02)
Epoch: [20][360/391]	Time  0.213 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.0389e+00 (8.6381e-01)	Acc@1  69.53 ( 73.87)	Acc@5  94.53 ( 95.00)
Epoch: [20][370/391]	Time  0.225 ( 0.177)	Data  0.001 ( 0.006)	Loss 9.7199e-01 (8.6394e-01)	Acc@1  72.66 ( 73.88)	Acc@5  92.97 ( 94.99)
Epoch: [20][380/391]	Time  0.221 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.0057e+00 (8.6427e-01)	Acc@1  72.66 ( 73.87)	Acc@5  89.06 ( 94.99)
Epoch: [20][390/391]	Time  0.175 ( 0.179)	Data  0.001 ( 0.006)	Loss 8.4884e-01 (8.6536e-01)	Acc@1  71.25 ( 73.84)	Acc@5  93.75 ( 94.99)
## e[20] optimizer.zero_grad (sum) time: 0.34977221488952637
## e[20]       loss.backward (sum) time: 23.263795852661133
## e[20]      optimizer.step (sum) time: 3.4637675285339355
## epoch[20] training(only) time: 69.9448573589325
# Switched to evaluate mode...
Test: [  0/100]	Time  0.268 ( 0.268)	Loss 1.5046e+00 (1.5046e+00)	Acc@1  62.00 ( 62.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.058 ( 0.078)	Loss 1.4948e+00 (1.4323e+00)	Acc@1  62.00 ( 64.82)	Acc@5  88.00 ( 87.91)
Test: [ 20/100]	Time  0.059 ( 0.069)	Loss 1.1783e+00 (1.3424e+00)	Acc@1  71.00 ( 66.43)	Acc@5  90.00 ( 89.19)
Test: [ 30/100]	Time  0.059 ( 0.066)	Loss 1.7243e+00 (1.3928e+00)	Acc@1  58.00 ( 64.19)	Acc@5  86.00 ( 88.90)
Test: [ 40/100]	Time  0.058 ( 0.064)	Loss 1.3033e+00 (1.3856e+00)	Acc@1  67.00 ( 64.02)	Acc@5  90.00 ( 89.12)
Test: [ 50/100]	Time  0.059 ( 0.063)	Loss 1.5263e+00 (1.4033e+00)	Acc@1  57.00 ( 63.35)	Acc@5  90.00 ( 88.90)
Test: [ 60/100]	Time  0.054 ( 0.061)	Loss 1.3259e+00 (1.3740e+00)	Acc@1  65.00 ( 63.56)	Acc@5  86.00 ( 89.18)
Test: [ 70/100]	Time  0.054 ( 0.060)	Loss 1.4185e+00 (1.3838e+00)	Acc@1  66.00 ( 63.52)	Acc@5  91.00 ( 89.20)
Test: [ 80/100]	Time  0.054 ( 0.060)	Loss 1.5033e+00 (1.3946e+00)	Acc@1  65.00 ( 63.52)	Acc@5  87.00 ( 89.04)
Test: [ 90/100]	Time  0.054 ( 0.059)	Loss 1.5647e+00 (1.3875e+00)	Acc@1  57.00 ( 63.62)	Acc@5  88.00 ( 89.13)
 * Acc@1 63.680 Acc@5 89.240
### epoch[20] execution time: 75.9108533859253
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.322 ( 0.322)	Data  0.182 ( 0.182)	Loss 6.7222e-01 (6.7222e-01)	Acc@1  80.47 ( 80.47)	Acc@5  99.22 ( 99.22)
Epoch: [21][ 10/391]	Time  0.160 ( 0.178)	Data  0.001 ( 0.021)	Loss 7.5727e-01 (7.5892e-01)	Acc@1  78.91 ( 76.28)	Acc@5  92.97 ( 96.45)
Epoch: [21][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.013)	Loss 6.8614e-01 (7.4892e-01)	Acc@1  79.69 ( 77.01)	Acc@5  96.09 ( 96.21)
Epoch: [21][ 30/391]	Time  0.157 ( 0.166)	Data  0.001 ( 0.010)	Loss 7.0174e-01 (7.4691e-01)	Acc@1  76.56 ( 77.12)	Acc@5  99.22 ( 96.12)
Epoch: [21][ 40/391]	Time  0.184 ( 0.170)	Data  0.002 ( 0.009)	Loss 7.7974e-01 (7.4754e-01)	Acc@1  75.00 ( 76.91)	Acc@5  95.31 ( 96.23)
Epoch: [21][ 50/391]	Time  0.182 ( 0.172)	Data  0.001 ( 0.008)	Loss 7.7201e-01 (7.5237e-01)	Acc@1  75.78 ( 77.04)	Acc@5  96.88 ( 96.17)
Epoch: [21][ 60/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.008)	Loss 8.1178e-01 (7.6330e-01)	Acc@1  74.22 ( 76.65)	Acc@5  96.88 ( 96.04)
Epoch: [21][ 70/391]	Time  0.158 ( 0.172)	Data  0.001 ( 0.007)	Loss 7.8387e-01 (7.6104e-01)	Acc@1  75.78 ( 76.79)	Acc@5  96.09 ( 96.16)
Epoch: [21][ 80/391]	Time  0.158 ( 0.170)	Data  0.001 ( 0.007)	Loss 7.0250e-01 (7.6073e-01)	Acc@1  77.34 ( 76.80)	Acc@5  96.09 ( 96.15)
Epoch: [21][ 90/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.007)	Loss 9.5422e-01 (7.6731e-01)	Acc@1  72.66 ( 76.60)	Acc@5  93.75 ( 96.11)
Epoch: [21][100/391]	Time  0.221 ( 0.173)	Data  0.001 ( 0.006)	Loss 7.4032e-01 (7.6925e-01)	Acc@1  79.69 ( 76.45)	Acc@5  96.88 ( 96.14)
Epoch: [21][110/391]	Time  0.227 ( 0.177)	Data  0.001 ( 0.007)	Loss 6.6476e-01 (7.7028e-01)	Acc@1  80.47 ( 76.37)	Acc@5  97.66 ( 96.11)
Epoch: [21][120/391]	Time  0.178 ( 0.179)	Data  0.002 ( 0.007)	Loss 6.0312e-01 (7.7026e-01)	Acc@1  83.59 ( 76.32)	Acc@5  99.22 ( 96.09)
Epoch: [21][130/391]	Time  0.177 ( 0.179)	Data  0.001 ( 0.006)	Loss 5.8889e-01 (7.6791e-01)	Acc@1  82.81 ( 76.44)	Acc@5  96.88 ( 96.09)
Epoch: [21][140/391]	Time  0.180 ( 0.179)	Data  0.001 ( 0.006)	Loss 7.1415e-01 (7.7069e-01)	Acc@1  77.34 ( 76.41)	Acc@5  96.88 ( 96.09)
Epoch: [21][150/391]	Time  0.168 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.9733e-01 (7.7079e-01)	Acc@1  77.34 ( 76.41)	Acc@5  96.09 ( 96.10)
Epoch: [21][160/391]	Time  0.169 ( 0.178)	Data  0.001 ( 0.006)	Loss 9.9644e-01 (7.7635e-01)	Acc@1  72.66 ( 76.31)	Acc@5  92.97 ( 96.02)
Epoch: [21][170/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.006)	Loss 9.1262e-01 (7.7710e-01)	Acc@1  71.09 ( 76.28)	Acc@5  94.53 ( 96.02)
Epoch: [21][180/391]	Time  0.158 ( 0.176)	Data  0.001 ( 0.006)	Loss 9.0197e-01 (7.7749e-01)	Acc@1  75.00 ( 76.30)	Acc@5  94.53 ( 96.00)
Epoch: [21][190/391]	Time  0.157 ( 0.175)	Data  0.001 ( 0.006)	Loss 8.0596e-01 (7.7941e-01)	Acc@1  74.22 ( 76.28)	Acc@5  96.09 ( 95.98)
Epoch: [21][200/391]	Time  0.164 ( 0.175)	Data  0.002 ( 0.006)	Loss 8.1904e-01 (7.8376e-01)	Acc@1  78.91 ( 76.14)	Acc@5  92.97 ( 95.91)
Epoch: [21][210/391]	Time  0.174 ( 0.175)	Data  0.001 ( 0.006)	Loss 8.9938e-01 (7.8732e-01)	Acc@1  74.22 ( 76.02)	Acc@5  93.75 ( 95.88)
Epoch: [21][220/391]	Time  0.169 ( 0.175)	Data  0.001 ( 0.006)	Loss 8.1866e-01 (7.9134e-01)	Acc@1  75.00 ( 75.95)	Acc@5  94.53 ( 95.81)
Epoch: [21][230/391]	Time  0.163 ( 0.175)	Data  0.001 ( 0.006)	Loss 7.9985e-01 (7.9318e-01)	Acc@1  73.44 ( 75.88)	Acc@5  96.88 ( 95.81)
Epoch: [21][240/391]	Time  0.158 ( 0.174)	Data  0.001 ( 0.006)	Loss 9.8265e-01 (7.9711e-01)	Acc@1  69.53 ( 75.72)	Acc@5  96.09 ( 95.80)
Epoch: [21][250/391]	Time  0.161 ( 0.174)	Data  0.001 ( 0.006)	Loss 9.7627e-01 (7.9861e-01)	Acc@1  67.97 ( 75.68)	Acc@5  93.75 ( 95.80)
Epoch: [21][260/391]	Time  0.227 ( 0.174)	Data  0.002 ( 0.006)	Loss 9.3871e-01 (8.0120e-01)	Acc@1  76.56 ( 75.59)	Acc@5  95.31 ( 95.82)
Epoch: [21][270/391]	Time  0.224 ( 0.176)	Data  0.001 ( 0.006)	Loss 9.7440e-01 (8.0389e-01)	Acc@1  67.97 ( 75.54)	Acc@5  95.31 ( 95.77)
Epoch: [21][280/391]	Time  0.211 ( 0.178)	Data  0.001 ( 0.006)	Loss 8.0963e-01 (8.0510e-01)	Acc@1  78.91 ( 75.50)	Acc@5  95.31 ( 95.74)
Epoch: [21][290/391]	Time  0.165 ( 0.178)	Data  0.001 ( 0.006)	Loss 9.0563e-01 (8.0673e-01)	Acc@1  74.22 ( 75.45)	Acc@5  91.41 ( 95.73)
Epoch: [21][300/391]	Time  0.175 ( 0.178)	Data  0.001 ( 0.006)	Loss 8.9903e-01 (8.1016e-01)	Acc@1  72.66 ( 75.36)	Acc@5  94.53 ( 95.68)
Epoch: [21][310/391]	Time  0.153 ( 0.177)	Data  0.001 ( 0.006)	Loss 9.5667e-01 (8.1141e-01)	Acc@1  68.75 ( 75.34)	Acc@5  93.75 ( 95.68)
Epoch: [21][320/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.006)	Loss 8.8798e-01 (8.1327e-01)	Acc@1  71.88 ( 75.26)	Acc@5  93.75 ( 95.66)
Epoch: [21][330/391]	Time  0.164 ( 0.177)	Data  0.001 ( 0.006)	Loss 6.2068e-01 (8.1377e-01)	Acc@1  81.25 ( 75.25)	Acc@5  98.44 ( 95.64)
Epoch: [21][340/391]	Time  0.154 ( 0.176)	Data  0.001 ( 0.006)	Loss 8.4104e-01 (8.1711e-01)	Acc@1  72.66 ( 75.18)	Acc@5  95.31 ( 95.59)
Epoch: [21][350/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.006)	Loss 8.9498e-01 (8.1854e-01)	Acc@1  74.22 ( 75.14)	Acc@5  93.75 ( 95.57)
Epoch: [21][360/391]	Time  0.158 ( 0.175)	Data  0.001 ( 0.006)	Loss 8.3103e-01 (8.2056e-01)	Acc@1  73.44 ( 75.09)	Acc@5  95.31 ( 95.53)
Epoch: [21][370/391]	Time  0.173 ( 0.175)	Data  0.001 ( 0.006)	Loss 6.9883e-01 (8.2205e-01)	Acc@1  78.12 ( 75.06)	Acc@5  99.22 ( 95.50)
Epoch: [21][380/391]	Time  0.178 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.0452e+00 (8.2293e-01)	Acc@1  66.41 ( 75.01)	Acc@5  92.97 ( 95.50)
Epoch: [21][390/391]	Time  0.170 ( 0.176)	Data  0.001 ( 0.006)	Loss 8.1503e-01 (8.2405e-01)	Acc@1  72.50 ( 74.97)	Acc@5  96.25 ( 95.50)
## e[21] optimizer.zero_grad (sum) time: 0.34635162353515625
## e[21]       loss.backward (sum) time: 23.09317398071289
## e[21]      optimizer.step (sum) time: 3.4270236492156982
## epoch[21] training(only) time: 68.78408074378967
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 1.4606e+00 (1.4606e+00)	Acc@1  59.00 ( 59.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.051 ( 0.067)	Loss 1.5788e+00 (1.4854e+00)	Acc@1  57.00 ( 62.73)	Acc@5  87.00 ( 87.55)
Test: [ 20/100]	Time  0.052 ( 0.059)	Loss 1.1676e+00 (1.3667e+00)	Acc@1  73.00 ( 65.14)	Acc@5  91.00 ( 88.90)
Test: [ 30/100]	Time  0.051 ( 0.057)	Loss 1.5864e+00 (1.3990e+00)	Acc@1  57.00 ( 63.71)	Acc@5  85.00 ( 88.81)
Test: [ 40/100]	Time  0.051 ( 0.056)	Loss 1.3742e+00 (1.3893e+00)	Acc@1  66.00 ( 63.71)	Acc@5  91.00 ( 89.12)
Test: [ 50/100]	Time  0.051 ( 0.055)	Loss 1.4755e+00 (1.4133e+00)	Acc@1  63.00 ( 63.33)	Acc@5  89.00 ( 88.86)
Test: [ 60/100]	Time  0.051 ( 0.054)	Loss 1.5619e+00 (1.3957e+00)	Acc@1  62.00 ( 63.43)	Acc@5  88.00 ( 89.20)
Test: [ 70/100]	Time  0.051 ( 0.054)	Loss 1.6957e+00 (1.4130e+00)	Acc@1  63.00 ( 63.14)	Acc@5  88.00 ( 88.96)
Test: [ 80/100]	Time  0.065 ( 0.055)	Loss 1.4944e+00 (1.4198e+00)	Acc@1  60.00 ( 63.00)	Acc@5  87.00 ( 88.89)
Test: [ 90/100]	Time  0.065 ( 0.056)	Loss 1.8964e+00 (1.4089e+00)	Acc@1  52.00 ( 63.26)	Acc@5  83.00 ( 89.08)
 * Acc@1 63.330 Acc@5 89.160
### epoch[21] execution time: 74.62170767784119
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.397 ( 0.397)	Data  0.215 ( 0.215)	Loss 7.9777e-01 (7.9777e-01)	Acc@1  75.00 ( 75.00)	Acc@5  96.09 ( 96.09)
Epoch: [22][ 10/391]	Time  0.220 ( 0.236)	Data  0.002 ( 0.026)	Loss 8.7343e-01 (7.7090e-01)	Acc@1  73.44 ( 76.70)	Acc@5  96.09 ( 96.59)
Epoch: [22][ 20/391]	Time  0.176 ( 0.213)	Data  0.001 ( 0.017)	Loss 8.1694e-01 (7.3801e-01)	Acc@1  71.88 ( 77.49)	Acc@5  96.09 ( 96.35)
Epoch: [22][ 30/391]	Time  0.177 ( 0.202)	Data  0.001 ( 0.013)	Loss 7.7143e-01 (7.2988e-01)	Acc@1  75.78 ( 77.39)	Acc@5  92.97 ( 96.60)
Epoch: [22][ 40/391]	Time  0.171 ( 0.195)	Data  0.001 ( 0.011)	Loss 5.1579e-01 (7.2848e-01)	Acc@1  83.59 ( 77.32)	Acc@5 100.00 ( 96.78)
Epoch: [22][ 50/391]	Time  0.167 ( 0.189)	Data  0.002 ( 0.010)	Loss 5.6651e-01 (7.1828e-01)	Acc@1  81.25 ( 77.68)	Acc@5  98.44 ( 96.78)
Epoch: [22][ 60/391]	Time  0.166 ( 0.185)	Data  0.001 ( 0.009)	Loss 7.5778e-01 (7.2144e-01)	Acc@1  74.22 ( 77.78)	Acc@5  96.09 ( 96.64)
Epoch: [22][ 70/391]	Time  0.161 ( 0.182)	Data  0.001 ( 0.008)	Loss 6.7940e-01 (7.1908e-01)	Acc@1  79.69 ( 77.82)	Acc@5  94.53 ( 96.59)
Epoch: [22][ 80/391]	Time  0.159 ( 0.179)	Data  0.001 ( 0.008)	Loss 7.8539e-01 (7.2320e-01)	Acc@1  76.56 ( 77.73)	Acc@5  96.09 ( 96.57)
Epoch: [22][ 90/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.008)	Loss 6.4349e-01 (7.2541e-01)	Acc@1  78.12 ( 77.60)	Acc@5  96.09 ( 96.55)
Epoch: [22][100/391]	Time  0.182 ( 0.177)	Data  0.001 ( 0.007)	Loss 7.0192e-01 (7.3237e-01)	Acc@1  79.69 ( 77.33)	Acc@5  95.31 ( 96.43)
Epoch: [22][110/391]	Time  0.185 ( 0.178)	Data  0.001 ( 0.007)	Loss 6.5681e-01 (7.3171e-01)	Acc@1  76.56 ( 77.34)	Acc@5  98.44 ( 96.47)
Epoch: [22][120/391]	Time  0.171 ( 0.178)	Data  0.001 ( 0.007)	Loss 8.6103e-01 (7.3400e-01)	Acc@1  70.31 ( 77.23)	Acc@5  94.53 ( 96.44)
Epoch: [22][130/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.007)	Loss 8.6883e-01 (7.3827e-01)	Acc@1  73.44 ( 77.11)	Acc@5  96.88 ( 96.43)
Epoch: [22][140/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.007)	Loss 6.5867e-01 (7.3913e-01)	Acc@1  80.47 ( 77.11)	Acc@5  98.44 ( 96.44)
Epoch: [22][150/391]	Time  0.160 ( 0.174)	Data  0.001 ( 0.007)	Loss 7.4585e-01 (7.4033e-01)	Acc@1  81.25 ( 77.16)	Acc@5  96.88 ( 96.41)
Epoch: [22][160/391]	Time  0.231 ( 0.177)	Data  0.002 ( 0.006)	Loss 6.9091e-01 (7.4102e-01)	Acc@1  78.91 ( 77.10)	Acc@5  96.09 ( 96.38)
Epoch: [22][170/391]	Time  0.227 ( 0.180)	Data  0.001 ( 0.007)	Loss 7.0751e-01 (7.4600e-01)	Acc@1  79.69 ( 76.99)	Acc@5  92.97 ( 96.31)
Epoch: [22][180/391]	Time  0.177 ( 0.181)	Data  0.001 ( 0.007)	Loss 6.4946e-01 (7.4728e-01)	Acc@1  84.38 ( 76.92)	Acc@5  97.66 ( 96.33)
Epoch: [22][190/391]	Time  0.176 ( 0.180)	Data  0.001 ( 0.007)	Loss 9.2526e-01 (7.5291e-01)	Acc@1  75.78 ( 76.71)	Acc@5  91.41 ( 96.25)
Epoch: [22][200/391]	Time  0.173 ( 0.180)	Data  0.001 ( 0.006)	Loss 8.5053e-01 (7.5586e-01)	Acc@1  76.56 ( 76.64)	Acc@5  92.19 ( 96.19)
Epoch: [22][210/391]	Time  0.166 ( 0.179)	Data  0.001 ( 0.006)	Loss 8.5188e-01 (7.5887e-01)	Acc@1  74.22 ( 76.48)	Acc@5  92.97 ( 96.17)
Epoch: [22][220/391]	Time  0.167 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.0711e+00 (7.6001e-01)	Acc@1  67.97 ( 76.44)	Acc@5  93.75 ( 96.16)
Epoch: [22][230/391]	Time  0.155 ( 0.178)	Data  0.001 ( 0.006)	Loss 9.4562e-01 (7.6302e-01)	Acc@1  64.84 ( 76.33)	Acc@5  93.75 ( 96.14)
Epoch: [22][240/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.006)	Loss 8.5068e-01 (7.6611e-01)	Acc@1  75.78 ( 76.28)	Acc@5  93.75 ( 96.07)
Epoch: [22][250/391]	Time  0.203 ( 0.177)	Data  0.002 ( 0.006)	Loss 1.0005e+00 (7.6756e-01)	Acc@1  73.44 ( 76.23)	Acc@5  94.53 ( 96.08)
Epoch: [22][260/391]	Time  0.176 ( 0.177)	Data  0.001 ( 0.006)	Loss 6.5253e-01 (7.6742e-01)	Acc@1  79.69 ( 76.26)	Acc@5  97.66 ( 96.07)
Epoch: [22][270/391]	Time  0.180 ( 0.177)	Data  0.001 ( 0.006)	Loss 7.4617e-01 (7.6900e-01)	Acc@1  76.56 ( 76.20)	Acc@5  94.53 ( 96.03)
Epoch: [22][280/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.006)	Loss 7.2959e-01 (7.7223e-01)	Acc@1  77.34 ( 76.14)	Acc@5  96.09 ( 96.00)
Epoch: [22][290/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.006)	Loss 7.3006e-01 (7.7292e-01)	Acc@1  74.22 ( 76.11)	Acc@5  96.09 ( 95.99)
Epoch: [22][300/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.006)	Loss 9.1370e-01 (7.7433e-01)	Acc@1  73.44 ( 76.07)	Acc@5  91.41 ( 95.96)
Epoch: [22][310/391]	Time  0.221 ( 0.176)	Data  0.001 ( 0.006)	Loss 9.0562e-01 (7.7506e-01)	Acc@1  71.09 ( 76.03)	Acc@5  92.97 ( 95.93)
Epoch: [22][320/391]	Time  0.222 ( 0.178)	Data  0.001 ( 0.006)	Loss 9.1153e-01 (7.7658e-01)	Acc@1  75.78 ( 76.00)	Acc@5  96.88 ( 95.92)
Epoch: [22][330/391]	Time  0.221 ( 0.179)	Data  0.002 ( 0.006)	Loss 9.1775e-01 (7.7839e-01)	Acc@1  69.53 ( 75.91)	Acc@5  93.75 ( 95.90)
Epoch: [22][340/391]	Time  0.171 ( 0.179)	Data  0.002 ( 0.006)	Loss 6.1713e-01 (7.8105e-01)	Acc@1  78.91 ( 75.83)	Acc@5  98.44 ( 95.87)
Epoch: [22][350/391]	Time  0.172 ( 0.179)	Data  0.002 ( 0.006)	Loss 9.0781e-01 (7.8128e-01)	Acc@1  70.31 ( 75.83)	Acc@5  93.75 ( 95.88)
Epoch: [22][360/391]	Time  0.161 ( 0.179)	Data  0.002 ( 0.006)	Loss 6.0363e-01 (7.8403e-01)	Acc@1  78.12 ( 75.76)	Acc@5  96.88 ( 95.84)
Epoch: [22][370/391]	Time  0.166 ( 0.178)	Data  0.001 ( 0.006)	Loss 7.7087e-01 (7.8346e-01)	Acc@1  77.34 ( 75.77)	Acc@5  97.66 ( 95.86)
Epoch: [22][380/391]	Time  0.166 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.8540e-01 (7.8346e-01)	Acc@1  80.47 ( 75.76)	Acc@5  97.66 ( 95.87)
Epoch: [22][390/391]	Time  0.156 ( 0.177)	Data  0.001 ( 0.006)	Loss 6.1370e-01 (7.8491e-01)	Acc@1  80.00 ( 75.72)	Acc@5  98.75 ( 95.86)
## e[22] optimizer.zero_grad (sum) time: 0.3442380428314209
## e[22]       loss.backward (sum) time: 22.7773118019104
## e[22]      optimizer.step (sum) time: 3.4029438495635986
## epoch[22] training(only) time: 69.51873707771301
# Switched to evaluate mode...
Test: [  0/100]	Time  0.203 ( 0.203)	Loss 1.3654e+00 (1.3654e+00)	Acc@1  65.00 ( 65.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.051 ( 0.065)	Loss 1.4996e+00 (1.4906e+00)	Acc@1  60.00 ( 62.27)	Acc@5  92.00 ( 88.27)
Test: [ 20/100]	Time  0.052 ( 0.059)	Loss 1.3265e+00 (1.4186e+00)	Acc@1  63.00 ( 63.24)	Acc@5  91.00 ( 89.05)
Test: [ 30/100]	Time  0.051 ( 0.057)	Loss 1.7035e+00 (1.4204e+00)	Acc@1  61.00 ( 62.77)	Acc@5  88.00 ( 88.74)
Test: [ 40/100]	Time  0.052 ( 0.056)	Loss 1.4759e+00 (1.4044e+00)	Acc@1  65.00 ( 63.00)	Acc@5  85.00 ( 89.05)
Test: [ 50/100]	Time  0.058 ( 0.056)	Loss 1.5130e+00 (1.4193e+00)	Acc@1  57.00 ( 62.53)	Acc@5  89.00 ( 88.75)
Test: [ 60/100]	Time  0.056 ( 0.056)	Loss 1.4770e+00 (1.3990e+00)	Acc@1  63.00 ( 62.69)	Acc@5  88.00 ( 89.02)
Test: [ 70/100]	Time  0.058 ( 0.056)	Loss 1.5972e+00 (1.4041e+00)	Acc@1  62.00 ( 62.63)	Acc@5  86.00 ( 88.99)
Test: [ 80/100]	Time  0.060 ( 0.057)	Loss 1.5087e+00 (1.4137e+00)	Acc@1  66.00 ( 62.67)	Acc@5  89.00 ( 88.94)
Test: [ 90/100]	Time  0.058 ( 0.057)	Loss 1.5386e+00 (1.4025e+00)	Acc@1  58.00 ( 62.77)	Acc@5  89.00 ( 89.10)
 * Acc@1 62.840 Acc@5 89.150
### epoch[22] execution time: 75.35599708557129
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.379 ( 0.379)	Data  0.199 ( 0.199)	Loss 6.4135e-01 (6.4135e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.88 ( 96.88)
Epoch: [23][ 10/391]	Time  0.209 ( 0.196)	Data  0.001 ( 0.022)	Loss 6.4849e-01 (6.4446e-01)	Acc@1  80.47 ( 80.33)	Acc@5  95.31 ( 97.30)
Epoch: [23][ 20/391]	Time  0.230 ( 0.208)	Data  0.001 ( 0.015)	Loss 7.0865e-01 (6.5515e-01)	Acc@1  75.78 ( 80.06)	Acc@5  96.09 ( 96.99)
Epoch: [23][ 30/391]	Time  0.229 ( 0.213)	Data  0.001 ( 0.012)	Loss 6.1025e-01 (6.6402e-01)	Acc@1  78.91 ( 79.49)	Acc@5  99.22 ( 96.95)
Epoch: [23][ 40/391]	Time  0.177 ( 0.207)	Data  0.002 ( 0.011)	Loss 9.1496e-01 (6.7093e-01)	Acc@1  71.09 ( 79.00)	Acc@5  93.75 ( 96.82)
Epoch: [23][ 50/391]	Time  0.173 ( 0.201)	Data  0.001 ( 0.010)	Loss 7.2042e-01 (6.7187e-01)	Acc@1  75.00 ( 78.91)	Acc@5  97.66 ( 96.92)
Epoch: [23][ 60/391]	Time  0.180 ( 0.197)	Data  0.001 ( 0.009)	Loss 7.4697e-01 (6.8034e-01)	Acc@1  75.78 ( 78.68)	Acc@5  95.31 ( 96.89)
Epoch: [23][ 70/391]	Time  0.172 ( 0.194)	Data  0.002 ( 0.008)	Loss 7.1801e-01 (6.8641e-01)	Acc@1  79.69 ( 78.64)	Acc@5  96.09 ( 96.89)
Epoch: [23][ 80/391]	Time  0.187 ( 0.191)	Data  0.002 ( 0.008)	Loss 5.4710e-01 (6.8668e-01)	Acc@1  84.38 ( 78.49)	Acc@5  99.22 ( 96.93)
Epoch: [23][ 90/391]	Time  0.174 ( 0.190)	Data  0.001 ( 0.008)	Loss 5.8900e-01 (6.8892e-01)	Acc@1  78.12 ( 78.38)	Acc@5  98.44 ( 96.98)
Epoch: [23][100/391]	Time  0.219 ( 0.189)	Data  0.001 ( 0.007)	Loss 6.2928e-01 (6.8761e-01)	Acc@1  82.81 ( 78.62)	Acc@5  98.44 ( 96.94)
Epoch: [23][110/391]	Time  0.211 ( 0.192)	Data  0.001 ( 0.007)	Loss 5.9249e-01 (6.9189e-01)	Acc@1  81.25 ( 78.48)	Acc@5  96.88 ( 96.90)
Epoch: [23][120/391]	Time  0.225 ( 0.194)	Data  0.001 ( 0.007)	Loss 6.3543e-01 (6.9260e-01)	Acc@1  78.12 ( 78.36)	Acc@5  98.44 ( 96.97)
Epoch: [23][130/391]	Time  0.178 ( 0.193)	Data  0.001 ( 0.007)	Loss 6.7342e-01 (6.9509e-01)	Acc@1  75.00 ( 78.27)	Acc@5  96.09 ( 96.96)
Epoch: [23][140/391]	Time  0.170 ( 0.192)	Data  0.001 ( 0.007)	Loss 6.3860e-01 (6.9898e-01)	Acc@1  82.81 ( 78.17)	Acc@5  96.09 ( 96.93)
Epoch: [23][150/391]	Time  0.161 ( 0.191)	Data  0.001 ( 0.007)	Loss 8.6118e-01 (7.0147e-01)	Acc@1  75.00 ( 78.17)	Acc@5  94.53 ( 96.92)
Epoch: [23][160/391]	Time  0.173 ( 0.190)	Data  0.001 ( 0.007)	Loss 7.1214e-01 (7.0467e-01)	Acc@1  75.00 ( 78.09)	Acc@5  96.88 ( 96.92)
Epoch: [23][170/391]	Time  0.185 ( 0.190)	Data  0.002 ( 0.007)	Loss 7.3733e-01 (7.0688e-01)	Acc@1  76.56 ( 77.98)	Acc@5  97.66 ( 96.89)
Epoch: [23][180/391]	Time  0.173 ( 0.189)	Data  0.001 ( 0.006)	Loss 6.5544e-01 (7.0969e-01)	Acc@1  78.91 ( 77.87)	Acc@5 100.00 ( 96.86)
Epoch: [23][190/391]	Time  0.220 ( 0.189)	Data  0.002 ( 0.006)	Loss 8.1438e-01 (7.1215e-01)	Acc@1  73.44 ( 77.82)	Acc@5  96.09 ( 96.82)
Epoch: [23][200/391]	Time  0.225 ( 0.191)	Data  0.001 ( 0.006)	Loss 7.3360e-01 (7.1210e-01)	Acc@1  76.56 ( 77.80)	Acc@5  96.88 ( 96.79)
Epoch: [23][210/391]	Time  0.219 ( 0.192)	Data  0.001 ( 0.006)	Loss 7.5548e-01 (7.1579e-01)	Acc@1  71.09 ( 77.70)	Acc@5  95.31 ( 96.72)
Epoch: [23][220/391]	Time  0.166 ( 0.191)	Data  0.001 ( 0.006)	Loss 5.9936e-01 (7.1654e-01)	Acc@1  82.03 ( 77.64)	Acc@5  96.88 ( 96.69)
Epoch: [23][230/391]	Time  0.175 ( 0.190)	Data  0.001 ( 0.006)	Loss 6.8802e-01 (7.2054e-01)	Acc@1  76.56 ( 77.54)	Acc@5  96.09 ( 96.65)
Epoch: [23][240/391]	Time  0.153 ( 0.190)	Data  0.001 ( 0.006)	Loss 9.2283e-01 (7.2391e-01)	Acc@1  71.09 ( 77.41)	Acc@5  92.97 ( 96.59)
Epoch: [23][250/391]	Time  0.170 ( 0.189)	Data  0.001 ( 0.006)	Loss 8.3644e-01 (7.2574e-01)	Acc@1  78.12 ( 77.42)	Acc@5  94.53 ( 96.54)
Epoch: [23][260/391]	Time  0.189 ( 0.189)	Data  0.001 ( 0.006)	Loss 9.1942e-01 (7.2890e-01)	Acc@1  72.66 ( 77.35)	Acc@5  93.75 ( 96.50)
Epoch: [23][270/391]	Time  0.179 ( 0.188)	Data  0.001 ( 0.006)	Loss 7.6674e-01 (7.3100e-01)	Acc@1  77.34 ( 77.29)	Acc@5  96.88 ( 96.47)
Epoch: [23][280/391]	Time  0.222 ( 0.188)	Data  0.001 ( 0.006)	Loss 6.7567e-01 (7.3361e-01)	Acc@1  78.91 ( 77.27)	Acc@5  96.88 ( 96.40)
Epoch: [23][290/391]	Time  0.216 ( 0.189)	Data  0.002 ( 0.006)	Loss 7.7123e-01 (7.3315e-01)	Acc@1  75.00 ( 77.26)	Acc@5  95.31 ( 96.42)
Epoch: [23][300/391]	Time  0.194 ( 0.190)	Data  0.001 ( 0.006)	Loss 6.8646e-01 (7.3248e-01)	Acc@1  78.91 ( 77.30)	Acc@5  97.66 ( 96.43)
Epoch: [23][310/391]	Time  0.171 ( 0.190)	Data  0.002 ( 0.006)	Loss 7.5507e-01 (7.3350e-01)	Acc@1  78.91 ( 77.29)	Acc@5  96.88 ( 96.42)
Epoch: [23][320/391]	Time  0.172 ( 0.189)	Data  0.001 ( 0.006)	Loss 7.7617e-01 (7.3555e-01)	Acc@1  77.34 ( 77.24)	Acc@5  96.09 ( 96.38)
Epoch: [23][330/391]	Time  0.216 ( 0.189)	Data  0.001 ( 0.006)	Loss 7.0008e-01 (7.3891e-01)	Acc@1  78.12 ( 77.14)	Acc@5  96.09 ( 96.34)
Epoch: [23][340/391]	Time  0.176 ( 0.189)	Data  0.002 ( 0.006)	Loss 6.4522e-01 (7.4017e-01)	Acc@1  82.03 ( 77.09)	Acc@5  96.88 ( 96.35)
Epoch: [23][350/391]	Time  0.172 ( 0.188)	Data  0.001 ( 0.006)	Loss 7.2395e-01 (7.4080e-01)	Acc@1  76.56 ( 77.07)	Acc@5  96.09 ( 96.35)
Epoch: [23][360/391]	Time  0.178 ( 0.188)	Data  0.001 ( 0.006)	Loss 7.0791e-01 (7.4177e-01)	Acc@1  78.12 ( 77.05)	Acc@5  96.09 ( 96.33)
Epoch: [23][370/391]	Time  0.221 ( 0.189)	Data  0.001 ( 0.006)	Loss 7.6896e-01 (7.4171e-01)	Acc@1  79.69 ( 77.06)	Acc@5  95.31 ( 96.32)
Epoch: [23][380/391]	Time  0.224 ( 0.190)	Data  0.001 ( 0.006)	Loss 8.0272e-01 (7.4371e-01)	Acc@1  73.44 ( 77.03)	Acc@5  96.09 ( 96.30)
Epoch: [23][390/391]	Time  0.181 ( 0.190)	Data  0.001 ( 0.006)	Loss 7.5726e-01 (7.4617e-01)	Acc@1  78.75 ( 76.97)	Acc@5  97.50 ( 96.28)
## e[23] optimizer.zero_grad (sum) time: 0.40363073348999023
## e[23]       loss.backward (sum) time: 26.75775933265686
## e[23]      optimizer.step (sum) time: 3.8988962173461914
## epoch[23] training(only) time: 74.39038324356079
# Switched to evaluate mode...
Test: [  0/100]	Time  0.283 ( 0.283)	Loss 1.5533e+00 (1.5533e+00)	Acc@1  61.00 ( 61.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.060 ( 0.081)	Loss 1.3577e+00 (1.4470e+00)	Acc@1  60.00 ( 64.00)	Acc@5  90.00 ( 88.45)
Test: [ 20/100]	Time  0.058 ( 0.070)	Loss 1.2978e+00 (1.3662e+00)	Acc@1  62.00 ( 64.81)	Acc@5  91.00 ( 89.29)
Test: [ 30/100]	Time  0.059 ( 0.066)	Loss 1.7305e+00 (1.3679e+00)	Acc@1  57.00 ( 64.97)	Acc@5  83.00 ( 89.03)
Test: [ 40/100]	Time  0.059 ( 0.065)	Loss 1.4484e+00 (1.3709e+00)	Acc@1  66.00 ( 64.46)	Acc@5  91.00 ( 89.17)
Test: [ 50/100]	Time  0.058 ( 0.063)	Loss 1.4899e+00 (1.3851e+00)	Acc@1  62.00 ( 64.24)	Acc@5  85.00 ( 88.94)
Test: [ 60/100]	Time  0.055 ( 0.062)	Loss 1.3428e+00 (1.3672e+00)	Acc@1  63.00 ( 64.13)	Acc@5  91.00 ( 89.28)
Test: [ 70/100]	Time  0.068 ( 0.062)	Loss 1.5012e+00 (1.3766e+00)	Acc@1  64.00 ( 64.04)	Acc@5  89.00 ( 89.14)
Test: [ 80/100]	Time  0.061 ( 0.062)	Loss 1.6620e+00 (1.3860e+00)	Acc@1  65.00 ( 64.01)	Acc@5  85.00 ( 89.05)
Test: [ 90/100]	Time  0.058 ( 0.062)	Loss 1.7008e+00 (1.3726e+00)	Acc@1  52.00 ( 64.41)	Acc@5  87.00 ( 89.18)
 * Acc@1 64.670 Acc@5 89.230
### epoch[23] execution time: 80.64241766929626
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.407 ( 0.407)	Data  0.225 ( 0.225)	Loss 8.5934e-01 (8.5934e-01)	Acc@1  75.00 ( 75.00)	Acc@5  97.66 ( 97.66)
Epoch: [24][ 10/391]	Time  0.172 ( 0.198)	Data  0.001 ( 0.026)	Loss 7.0340e-01 (6.9266e-01)	Acc@1  79.69 ( 78.20)	Acc@5  96.09 ( 97.02)
Epoch: [24][ 20/391]	Time  0.218 ( 0.191)	Data  0.001 ( 0.015)	Loss 7.4149e-01 (6.9368e-01)	Acc@1  79.69 ( 78.16)	Acc@5  95.31 ( 96.95)
Epoch: [24][ 30/391]	Time  0.220 ( 0.200)	Data  0.001 ( 0.013)	Loss 7.2682e-01 (6.7025e-01)	Acc@1  80.47 ( 78.91)	Acc@5  95.31 ( 97.03)
Epoch: [24][ 40/391]	Time  0.220 ( 0.204)	Data  0.001 ( 0.011)	Loss 6.5567e-01 (6.6764e-01)	Acc@1  77.34 ( 79.13)	Acc@5  99.22 ( 97.07)
Epoch: [24][ 50/391]	Time  0.175 ( 0.200)	Data  0.001 ( 0.010)	Loss 6.7164e-01 (6.5185e-01)	Acc@1  85.16 ( 79.60)	Acc@5  96.88 ( 97.26)
Epoch: [24][ 60/391]	Time  0.173 ( 0.195)	Data  0.001 ( 0.009)	Loss 6.5464e-01 (6.6017e-01)	Acc@1  75.00 ( 79.37)	Acc@5  97.66 ( 97.04)
Epoch: [24][ 70/391]	Time  0.159 ( 0.192)	Data  0.001 ( 0.009)	Loss 6.3149e-01 (6.5501e-01)	Acc@1  82.03 ( 79.65)	Acc@5  95.31 ( 97.01)
Epoch: [24][ 80/391]	Time  0.173 ( 0.190)	Data  0.001 ( 0.008)	Loss 8.7276e-01 (6.5386e-01)	Acc@1  74.22 ( 79.57)	Acc@5  96.09 ( 97.12)
Epoch: [24][ 90/391]	Time  0.169 ( 0.189)	Data  0.001 ( 0.008)	Loss 6.7710e-01 (6.5236e-01)	Acc@1  76.56 ( 79.69)	Acc@5  97.66 ( 97.12)
Epoch: [24][100/391]	Time  0.168 ( 0.188)	Data  0.002 ( 0.008)	Loss 5.3313e-01 (6.5145e-01)	Acc@1  81.25 ( 79.62)	Acc@5  98.44 ( 97.15)
Epoch: [24][110/391]	Time  0.225 ( 0.189)	Data  0.001 ( 0.007)	Loss 4.7395e-01 (6.5211e-01)	Acc@1  84.38 ( 79.62)	Acc@5  98.44 ( 97.09)
Epoch: [24][120/391]	Time  0.224 ( 0.192)	Data  0.001 ( 0.007)	Loss 5.9634e-01 (6.6063e-01)	Acc@1  85.16 ( 79.44)	Acc@5  96.88 ( 97.02)
Epoch: [24][130/391]	Time  0.181 ( 0.193)	Data  0.001 ( 0.007)	Loss 7.1364e-01 (6.6423e-01)	Acc@1  78.12 ( 79.33)	Acc@5  97.66 ( 97.02)
Epoch: [24][140/391]	Time  0.177 ( 0.192)	Data  0.001 ( 0.007)	Loss 6.7086e-01 (6.6337e-01)	Acc@1  77.34 ( 79.36)	Acc@5  98.44 ( 97.04)
Epoch: [24][150/391]	Time  0.168 ( 0.190)	Data  0.002 ( 0.007)	Loss 6.3324e-01 (6.6837e-01)	Acc@1  79.69 ( 79.18)	Acc@5  98.44 ( 96.99)
Epoch: [24][160/391]	Time  0.169 ( 0.190)	Data  0.002 ( 0.007)	Loss 8.2486e-01 (6.7506e-01)	Acc@1  76.56 ( 79.03)	Acc@5  97.66 ( 96.96)
Epoch: [24][170/391]	Time  0.174 ( 0.188)	Data  0.001 ( 0.007)	Loss 6.1694e-01 (6.7662e-01)	Acc@1  78.12 ( 79.01)	Acc@5  97.66 ( 96.90)
Epoch: [24][180/391]	Time  0.174 ( 0.188)	Data  0.001 ( 0.007)	Loss 5.7338e-01 (6.7969e-01)	Acc@1  84.38 ( 78.90)	Acc@5  96.88 ( 96.85)
Epoch: [24][190/391]	Time  0.228 ( 0.187)	Data  0.001 ( 0.006)	Loss 6.5379e-01 (6.7852e-01)	Acc@1  83.59 ( 78.98)	Acc@5  96.88 ( 96.86)
Epoch: [24][200/391]	Time  0.220 ( 0.189)	Data  0.001 ( 0.006)	Loss 6.9751e-01 (6.8332e-01)	Acc@1  80.47 ( 78.90)	Acc@5  95.31 ( 96.80)
Epoch: [24][210/391]	Time  0.217 ( 0.190)	Data  0.001 ( 0.006)	Loss 7.5633e-01 (6.8474e-01)	Acc@1  77.34 ( 78.87)	Acc@5  96.88 ( 96.82)
Epoch: [24][220/391]	Time  0.178 ( 0.190)	Data  0.001 ( 0.007)	Loss 8.3480e-01 (6.8525e-01)	Acc@1  72.66 ( 78.82)	Acc@5  98.44 ( 96.80)
Epoch: [24][230/391]	Time  0.167 ( 0.189)	Data  0.001 ( 0.006)	Loss 8.5732e-01 (6.8801e-01)	Acc@1  72.66 ( 78.73)	Acc@5  96.09 ( 96.80)
Epoch: [24][240/391]	Time  0.174 ( 0.189)	Data  0.001 ( 0.006)	Loss 5.7020e-01 (6.8941e-01)	Acc@1  78.91 ( 78.71)	Acc@5  96.88 ( 96.77)
Epoch: [24][250/391]	Time  0.173 ( 0.188)	Data  0.002 ( 0.006)	Loss 8.1619e-01 (6.9307e-01)	Acc@1  71.09 ( 78.62)	Acc@5  95.31 ( 96.71)
Epoch: [24][260/391]	Time  0.211 ( 0.188)	Data  0.001 ( 0.006)	Loss 5.5695e-01 (6.9402e-01)	Acc@1  80.47 ( 78.59)	Acc@5  99.22 ( 96.73)
Epoch: [24][270/391]	Time  0.177 ( 0.187)	Data  0.001 ( 0.006)	Loss 6.9700e-01 (6.9532e-01)	Acc@1  79.69 ( 78.53)	Acc@5  93.75 ( 96.72)
Epoch: [24][280/391]	Time  0.207 ( 0.187)	Data  0.002 ( 0.006)	Loss 8.8288e-01 (6.9841e-01)	Acc@1  78.91 ( 78.45)	Acc@5  94.53 ( 96.69)
Epoch: [24][290/391]	Time  0.221 ( 0.188)	Data  0.001 ( 0.006)	Loss 6.1586e-01 (6.9856e-01)	Acc@1  78.91 ( 78.40)	Acc@5  97.66 ( 96.70)
Epoch: [24][300/391]	Time  0.212 ( 0.189)	Data  0.001 ( 0.006)	Loss 7.2519e-01 (7.0083e-01)	Acc@1  82.03 ( 78.31)	Acc@5  94.53 ( 96.69)
Epoch: [24][310/391]	Time  0.171 ( 0.189)	Data  0.001 ( 0.006)	Loss 8.6552e-01 (7.0169e-01)	Acc@1  68.75 ( 78.29)	Acc@5  96.88 ( 96.70)
Epoch: [24][320/391]	Time  0.164 ( 0.188)	Data  0.001 ( 0.006)	Loss 7.5235e-01 (7.0271e-01)	Acc@1  75.78 ( 78.27)	Acc@5  96.09 ( 96.71)
Epoch: [24][330/391]	Time  0.148 ( 0.188)	Data  0.001 ( 0.006)	Loss 7.8660e-01 (7.0383e-01)	Acc@1  77.34 ( 78.25)	Acc@5  95.31 ( 96.70)
Epoch: [24][340/391]	Time  0.170 ( 0.188)	Data  0.001 ( 0.006)	Loss 6.1834e-01 (7.0715e-01)	Acc@1  75.00 ( 78.13)	Acc@5  98.44 ( 96.65)
Epoch: [24][350/391]	Time  0.176 ( 0.187)	Data  0.001 ( 0.006)	Loss 8.4903e-01 (7.0965e-01)	Acc@1  71.88 ( 78.01)	Acc@5  92.97 ( 96.64)
Epoch: [24][360/391]	Time  0.169 ( 0.187)	Data  0.001 ( 0.006)	Loss 7.7835e-01 (7.1137e-01)	Acc@1  78.12 ( 77.95)	Acc@5  94.53 ( 96.64)
Epoch: [24][370/391]	Time  0.217 ( 0.188)	Data  0.002 ( 0.006)	Loss 7.6232e-01 (7.1241e-01)	Acc@1  74.22 ( 77.90)	Acc@5  96.09 ( 96.62)
Epoch: [24][380/391]	Time  0.221 ( 0.188)	Data  0.001 ( 0.006)	Loss 7.1063e-01 (7.1192e-01)	Acc@1  77.34 ( 77.93)	Acc@5  97.66 ( 96.63)
Epoch: [24][390/391]	Time  0.174 ( 0.189)	Data  0.001 ( 0.006)	Loss 6.6357e-01 (7.1293e-01)	Acc@1  80.00 ( 77.89)	Acc@5  97.50 ( 96.64)
## e[24] optimizer.zero_grad (sum) time: 0.3903770446777344
## e[24]       loss.backward (sum) time: 26.08075523376465
## e[24]      optimizer.step (sum) time: 3.8175806999206543
## epoch[24] training(only) time: 73.93384861946106
# Switched to evaluate mode...
Test: [  0/100]	Time  0.283 ( 0.283)	Loss 1.3619e+00 (1.3619e+00)	Acc@1  63.00 ( 63.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.059 ( 0.079)	Loss 1.6095e+00 (1.3247e+00)	Acc@1  62.00 ( 66.64)	Acc@5  86.00 ( 88.36)
Test: [ 20/100]	Time  0.058 ( 0.069)	Loss 1.2106e+00 (1.2399e+00)	Acc@1  69.00 ( 68.10)	Acc@5  91.00 ( 89.86)
Test: [ 30/100]	Time  0.055 ( 0.065)	Loss 1.7415e+00 (1.2620e+00)	Acc@1  56.00 ( 67.29)	Acc@5  88.00 ( 89.68)
Test: [ 40/100]	Time  0.058 ( 0.064)	Loss 1.1486e+00 (1.2525e+00)	Acc@1  68.00 ( 67.39)	Acc@5  92.00 ( 90.15)
Test: [ 50/100]	Time  0.055 ( 0.062)	Loss 1.5480e+00 (1.2786e+00)	Acc@1  64.00 ( 66.69)	Acc@5  84.00 ( 90.00)
Test: [ 60/100]	Time  0.059 ( 0.061)	Loss 1.2659e+00 (1.2632e+00)	Acc@1  67.00 ( 66.82)	Acc@5  89.00 ( 90.28)
Test: [ 70/100]	Time  0.057 ( 0.061)	Loss 1.4884e+00 (1.2681e+00)	Acc@1  64.00 ( 66.87)	Acc@5  90.00 ( 90.28)
Test: [ 80/100]	Time  0.059 ( 0.061)	Loss 1.5036e+00 (1.2693e+00)	Acc@1  64.00 ( 66.65)	Acc@5  90.00 ( 90.40)
Test: [ 90/100]	Time  0.055 ( 0.060)	Loss 1.6459e+00 (1.2653e+00)	Acc@1  61.00 ( 66.84)	Acc@5  91.00 ( 90.59)
 * Acc@1 67.070 Acc@5 90.650
### epoch[24] execution time: 80.03128027915955
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.396 ( 0.396)	Data  0.232 ( 0.232)	Loss 8.1615e-01 (8.1615e-01)	Acc@1  74.22 ( 74.22)	Acc@5  92.97 ( 92.97)
Epoch: [25][ 10/391]	Time  0.172 ( 0.191)	Data  0.001 ( 0.025)	Loss 4.5874e-01 (6.1341e-01)	Acc@1  85.94 ( 80.04)	Acc@5  97.66 ( 97.51)
Epoch: [25][ 20/391]	Time  0.211 ( 0.192)	Data  0.001 ( 0.015)	Loss 4.2126e-01 (6.0820e-01)	Acc@1  90.62 ( 80.88)	Acc@5  98.44 ( 97.43)
Epoch: [25][ 30/391]	Time  0.212 ( 0.197)	Data  0.001 ( 0.012)	Loss 5.4384e-01 (5.9861e-01)	Acc@1  82.81 ( 81.25)	Acc@5  98.44 ( 97.53)
Epoch: [25][ 40/391]	Time  0.173 ( 0.199)	Data  0.001 ( 0.011)	Loss 4.4915e-01 (6.0066e-01)	Acc@1  86.72 ( 81.55)	Acc@5  98.44 ( 97.60)
Epoch: [25][ 50/391]	Time  0.173 ( 0.194)	Data  0.002 ( 0.010)	Loss 6.0034e-01 (6.0960e-01)	Acc@1  88.28 ( 81.50)	Acc@5  96.09 ( 97.49)
Epoch: [25][ 60/391]	Time  0.172 ( 0.190)	Data  0.001 ( 0.009)	Loss 4.9600e-01 (6.1420e-01)	Acc@1  82.03 ( 81.33)	Acc@5  99.22 ( 97.32)
Epoch: [25][ 70/391]	Time  0.161 ( 0.188)	Data  0.001 ( 0.008)	Loss 5.3080e-01 (6.1332e-01)	Acc@1  82.03 ( 81.39)	Acc@5  96.88 ( 97.33)
Epoch: [25][ 80/391]	Time  0.171 ( 0.186)	Data  0.001 ( 0.008)	Loss 6.8902e-01 (6.2328e-01)	Acc@1  78.12 ( 80.96)	Acc@5  96.88 ( 97.23)
Epoch: [25][ 90/391]	Time  0.170 ( 0.185)	Data  0.001 ( 0.008)	Loss 5.2408e-01 (6.2588e-01)	Acc@1  82.03 ( 80.79)	Acc@5  98.44 ( 97.24)
Epoch: [25][100/391]	Time  0.201 ( 0.185)	Data  0.002 ( 0.007)	Loss 6.9907e-01 (6.3138e-01)	Acc@1  74.22 ( 80.52)	Acc@5  96.88 ( 97.20)
Epoch: [25][110/391]	Time  0.206 ( 0.187)	Data  0.001 ( 0.007)	Loss 6.5399e-01 (6.3205e-01)	Acc@1  79.69 ( 80.37)	Acc@5  96.09 ( 97.21)
Epoch: [25][120/391]	Time  0.214 ( 0.188)	Data  0.001 ( 0.007)	Loss 7.3101e-01 (6.3106e-01)	Acc@1  73.44 ( 80.29)	Acc@5  97.66 ( 97.31)
Epoch: [25][130/391]	Time  0.160 ( 0.187)	Data  0.001 ( 0.007)	Loss 8.6219e-01 (6.3296e-01)	Acc@1  72.66 ( 80.25)	Acc@5  96.09 ( 97.33)
Epoch: [25][140/391]	Time  0.172 ( 0.186)	Data  0.001 ( 0.007)	Loss 6.5482e-01 (6.3101e-01)	Acc@1  82.81 ( 80.40)	Acc@5  96.09 ( 97.32)
Epoch: [25][150/391]	Time  0.163 ( 0.185)	Data  0.001 ( 0.007)	Loss 5.2777e-01 (6.2872e-01)	Acc@1  84.38 ( 80.51)	Acc@5  97.66 ( 97.32)
Epoch: [25][160/391]	Time  0.174 ( 0.185)	Data  0.001 ( 0.007)	Loss 5.3458e-01 (6.3284e-01)	Acc@1  82.03 ( 80.34)	Acc@5  98.44 ( 97.25)
Epoch: [25][170/391]	Time  0.170 ( 0.184)	Data  0.001 ( 0.007)	Loss 5.7436e-01 (6.3137e-01)	Acc@1  82.81 ( 80.41)	Acc@5  99.22 ( 97.30)
Epoch: [25][180/391]	Time  0.175 ( 0.184)	Data  0.001 ( 0.006)	Loss 6.3340e-01 (6.3535e-01)	Acc@1  78.91 ( 80.33)	Acc@5  96.09 ( 97.25)
Epoch: [25][190/391]	Time  0.207 ( 0.184)	Data  0.001 ( 0.006)	Loss 6.7082e-01 (6.3844e-01)	Acc@1  78.12 ( 80.26)	Acc@5  96.88 ( 97.21)
Epoch: [25][200/391]	Time  0.205 ( 0.186)	Data  0.002 ( 0.006)	Loss 7.3259e-01 (6.4226e-01)	Acc@1  77.34 ( 80.15)	Acc@5  96.09 ( 97.15)
Epoch: [25][210/391]	Time  0.173 ( 0.186)	Data  0.001 ( 0.006)	Loss 8.0823e-01 (6.4605e-01)	Acc@1  72.66 ( 80.02)	Acc@5  96.09 ( 97.11)
Epoch: [25][220/391]	Time  0.169 ( 0.186)	Data  0.002 ( 0.006)	Loss 6.5602e-01 (6.4910e-01)	Acc@1  78.12 ( 79.91)	Acc@5  97.66 ( 97.12)
Epoch: [25][230/391]	Time  0.173 ( 0.185)	Data  0.001 ( 0.006)	Loss 5.7258e-01 (6.4974e-01)	Acc@1  84.38 ( 79.86)	Acc@5  97.66 ( 97.12)
Epoch: [25][240/391]	Time  0.167 ( 0.185)	Data  0.001 ( 0.006)	Loss 7.5045e-01 (6.5219e-01)	Acc@1  78.12 ( 79.78)	Acc@5  94.53 ( 97.10)
Epoch: [25][250/391]	Time  0.173 ( 0.184)	Data  0.001 ( 0.006)	Loss 6.8892e-01 (6.5527e-01)	Acc@1  81.25 ( 79.71)	Acc@5  96.88 ( 97.07)
Epoch: [25][260/391]	Time  0.169 ( 0.184)	Data  0.002 ( 0.006)	Loss 8.6913e-01 (6.5675e-01)	Acc@1  69.53 ( 79.68)	Acc@5  93.75 ( 97.05)
Epoch: [25][270/391]	Time  0.208 ( 0.183)	Data  0.001 ( 0.006)	Loss 5.5484e-01 (6.5611e-01)	Acc@1  79.69 ( 79.68)	Acc@5  98.44 ( 97.06)
Epoch: [25][280/391]	Time  0.208 ( 0.184)	Data  0.002 ( 0.006)	Loss 7.0694e-01 (6.5855e-01)	Acc@1  81.25 ( 79.61)	Acc@5  96.09 ( 97.03)
Epoch: [25][290/391]	Time  0.210 ( 0.185)	Data  0.001 ( 0.006)	Loss 6.5049e-01 (6.5974e-01)	Acc@1  80.47 ( 79.58)	Acc@5  97.66 ( 97.02)
Epoch: [25][300/391]	Time  0.177 ( 0.185)	Data  0.001 ( 0.006)	Loss 9.4015e-01 (6.6401e-01)	Acc@1  70.31 ( 79.45)	Acc@5  91.41 ( 96.98)
Epoch: [25][310/391]	Time  0.174 ( 0.185)	Data  0.001 ( 0.006)	Loss 7.1229e-01 (6.6348e-01)	Acc@1  75.78 ( 79.47)	Acc@5  96.09 ( 96.99)
Epoch: [25][320/391]	Time  0.211 ( 0.184)	Data  0.001 ( 0.006)	Loss 9.0236e-01 (6.6491e-01)	Acc@1  71.88 ( 79.41)	Acc@5  91.41 ( 96.98)
Epoch: [25][330/391]	Time  0.165 ( 0.184)	Data  0.001 ( 0.006)	Loss 7.7433e-01 (6.6703e-01)	Acc@1  78.91 ( 79.32)	Acc@5  96.88 ( 96.98)
Epoch: [25][340/391]	Time  0.160 ( 0.184)	Data  0.001 ( 0.006)	Loss 7.9047e-01 (6.7091e-01)	Acc@1  72.66 ( 79.23)	Acc@5  96.88 ( 96.94)
Epoch: [25][350/391]	Time  0.173 ( 0.183)	Data  0.001 ( 0.006)	Loss 6.5163e-01 (6.7191e-01)	Acc@1  81.25 ( 79.21)	Acc@5  96.88 ( 96.93)
Epoch: [25][360/391]	Time  0.214 ( 0.184)	Data  0.001 ( 0.006)	Loss 6.6174e-01 (6.7309e-01)	Acc@1  77.34 ( 79.17)	Acc@5  97.66 ( 96.93)
Epoch: [25][370/391]	Time  0.208 ( 0.185)	Data  0.001 ( 0.006)	Loss 6.1354e-01 (6.7473e-01)	Acc@1  76.56 ( 79.11)	Acc@5  98.44 ( 96.90)
Epoch: [25][380/391]	Time  0.170 ( 0.185)	Data  0.001 ( 0.006)	Loss 6.0187e-01 (6.7602e-01)	Acc@1  80.47 ( 79.09)	Acc@5  98.44 ( 96.89)
Epoch: [25][390/391]	Time  0.168 ( 0.184)	Data  0.001 ( 0.006)	Loss 1.0582e+00 (6.7848e-01)	Acc@1  61.25 ( 79.01)	Acc@5  96.25 ( 96.86)
## e[25] optimizer.zero_grad (sum) time: 0.37548398971557617
## e[25]       loss.backward (sum) time: 25.16485071182251
## e[25]      optimizer.step (sum) time: 3.6754074096679688
## epoch[25] training(only) time: 72.24259638786316
# Switched to evaluate mode...
Test: [  0/100]	Time  0.261 ( 0.261)	Loss 1.4559e+00 (1.4559e+00)	Acc@1  62.00 ( 62.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.059 ( 0.076)	Loss 1.5721e+00 (1.4201e+00)	Acc@1  63.00 ( 65.09)	Acc@5  87.00 ( 88.82)
Test: [ 20/100]	Time  0.056 ( 0.067)	Loss 1.1349e+00 (1.3170e+00)	Acc@1  69.00 ( 65.86)	Acc@5  92.00 ( 90.24)
Test: [ 30/100]	Time  0.055 ( 0.063)	Loss 1.6675e+00 (1.3484e+00)	Acc@1  58.00 ( 64.90)	Acc@5  88.00 ( 89.94)
Test: [ 40/100]	Time  0.063 ( 0.062)	Loss 1.1882e+00 (1.3335e+00)	Acc@1  68.00 ( 64.95)	Acc@5  96.00 ( 90.22)
Test: [ 50/100]	Time  0.060 ( 0.062)	Loss 1.4426e+00 (1.3423e+00)	Acc@1  62.00 ( 64.76)	Acc@5  88.00 ( 90.24)
Test: [ 60/100]	Time  0.060 ( 0.061)	Loss 1.1820e+00 (1.3149e+00)	Acc@1  66.00 ( 65.30)	Acc@5  92.00 ( 90.41)
Test: [ 70/100]	Time  0.060 ( 0.061)	Loss 1.4925e+00 (1.3187e+00)	Acc@1  65.00 ( 65.03)	Acc@5  86.00 ( 90.35)
Test: [ 80/100]	Time  0.064 ( 0.061)	Loss 1.6306e+00 (1.3200e+00)	Acc@1  63.00 ( 64.98)	Acc@5  87.00 ( 90.26)
Test: [ 90/100]	Time  0.059 ( 0.061)	Loss 1.6069e+00 (1.3067e+00)	Acc@1  59.00 ( 65.29)	Acc@5  89.00 ( 90.36)
 * Acc@1 65.500 Acc@5 90.340
### epoch[25] execution time: 78.41866636276245
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.410 ( 0.410)	Data  0.230 ( 0.230)	Loss 5.8626e-01 (5.8626e-01)	Acc@1  83.59 ( 83.59)	Acc@5  96.88 ( 96.88)
Epoch: [26][ 10/391]	Time  0.207 ( 0.205)	Data  0.001 ( 0.024)	Loss 7.2327e-01 (5.8264e-01)	Acc@1  78.12 ( 82.17)	Acc@5  96.88 ( 97.37)
Epoch: [26][ 20/391]	Time  0.208 ( 0.207)	Data  0.001 ( 0.016)	Loss 5.9909e-01 (5.7842e-01)	Acc@1  82.81 ( 82.03)	Acc@5  96.88 ( 97.69)
Epoch: [26][ 30/391]	Time  0.197 ( 0.207)	Data  0.001 ( 0.013)	Loss 4.6382e-01 (5.7088e-01)	Acc@1  85.16 ( 82.36)	Acc@5  98.44 ( 97.81)
Epoch: [26][ 40/391]	Time  0.172 ( 0.198)	Data  0.001 ( 0.011)	Loss 5.5412e-01 (5.6773e-01)	Acc@1  82.81 ( 82.34)	Acc@5  96.88 ( 97.90)
Epoch: [26][ 50/391]	Time  0.174 ( 0.193)	Data  0.001 ( 0.010)	Loss 5.4869e-01 (5.6205e-01)	Acc@1  81.25 ( 82.44)	Acc@5  96.88 ( 97.89)
Epoch: [26][ 60/391]	Time  0.195 ( 0.190)	Data  0.001 ( 0.009)	Loss 6.6668e-01 (5.6629e-01)	Acc@1  78.12 ( 82.35)	Acc@5  98.44 ( 97.91)
Epoch: [26][ 70/391]	Time  0.171 ( 0.187)	Data  0.001 ( 0.008)	Loss 7.0374e-01 (5.7370e-01)	Acc@1  80.47 ( 82.09)	Acc@5  95.31 ( 97.88)
Epoch: [26][ 80/391]	Time  0.175 ( 0.186)	Data  0.001 ( 0.008)	Loss 6.5671e-01 (5.7569e-01)	Acc@1  80.47 ( 82.12)	Acc@5  94.53 ( 97.88)
Epoch: [26][ 90/391]	Time  0.164 ( 0.184)	Data  0.001 ( 0.007)	Loss 6.5871e-01 (5.7701e-01)	Acc@1  75.78 ( 82.08)	Acc@5  99.22 ( 97.87)
Epoch: [26][100/391]	Time  0.205 ( 0.186)	Data  0.001 ( 0.007)	Loss 6.6462e-01 (5.8372e-01)	Acc@1  81.25 ( 81.82)	Acc@5  99.22 ( 97.87)
Epoch: [26][110/391]	Time  0.204 ( 0.188)	Data  0.001 ( 0.007)	Loss 6.9322e-01 (5.8635e-01)	Acc@1  78.91 ( 81.73)	Acc@5  96.88 ( 97.86)
Epoch: [26][120/391]	Time  0.172 ( 0.188)	Data  0.001 ( 0.007)	Loss 5.2095e-01 (5.9176e-01)	Acc@1  82.03 ( 81.57)	Acc@5  99.22 ( 97.84)
Epoch: [26][130/391]	Time  0.169 ( 0.187)	Data  0.001 ( 0.007)	Loss 6.0516e-01 (5.9438e-01)	Acc@1  80.47 ( 81.50)	Acc@5  99.22 ( 97.79)
Epoch: [26][140/391]	Time  0.172 ( 0.186)	Data  0.001 ( 0.007)	Loss 5.1771e-01 (5.9683e-01)	Acc@1  81.25 ( 81.41)	Acc@5 100.00 ( 97.81)
Epoch: [26][150/391]	Time  0.169 ( 0.185)	Data  0.002 ( 0.007)	Loss 1.0461e+00 (6.0298e-01)	Acc@1  67.97 ( 81.21)	Acc@5  91.41 ( 97.72)
Epoch: [26][160/391]	Time  0.172 ( 0.184)	Data  0.001 ( 0.006)	Loss 6.6170e-01 (6.0710e-01)	Acc@1  78.91 ( 81.14)	Acc@5  96.88 ( 97.67)
Epoch: [26][170/391]	Time  0.176 ( 0.184)	Data  0.001 ( 0.006)	Loss 5.8383e-01 (6.1009e-01)	Acc@1  78.12 ( 80.99)	Acc@5  98.44 ( 97.64)
Epoch: [26][180/391]	Time  0.207 ( 0.184)	Data  0.001 ( 0.006)	Loss 8.2024e-01 (6.1393e-01)	Acc@1  75.78 ( 80.86)	Acc@5  95.31 ( 97.61)
Epoch: [26][190/391]	Time  0.201 ( 0.185)	Data  0.001 ( 0.006)	Loss 7.4277e-01 (6.1624e-01)	Acc@1  77.34 ( 80.83)	Acc@5  95.31 ( 97.57)
Epoch: [26][200/391]	Time  0.173 ( 0.186)	Data  0.001 ( 0.006)	Loss 6.2607e-01 (6.1648e-01)	Acc@1  83.59 ( 80.86)	Acc@5  95.31 ( 97.57)
Epoch: [26][210/391]	Time  0.167 ( 0.185)	Data  0.001 ( 0.006)	Loss 7.2014e-01 (6.1876e-01)	Acc@1  78.12 ( 80.81)	Acc@5  96.88 ( 97.54)
Epoch: [26][220/391]	Time  0.171 ( 0.185)	Data  0.001 ( 0.006)	Loss 9.5113e-01 (6.2179e-01)	Acc@1  73.44 ( 80.72)	Acc@5  92.19 ( 97.51)
Epoch: [26][230/391]	Time  0.168 ( 0.184)	Data  0.001 ( 0.006)	Loss 9.0423e-01 (6.2430e-01)	Acc@1  67.19 ( 80.56)	Acc@5  94.53 ( 97.47)
Epoch: [26][240/391]	Time  0.174 ( 0.183)	Data  0.001 ( 0.006)	Loss 7.5195e-01 (6.2785e-01)	Acc@1  78.91 ( 80.44)	Acc@5  95.31 ( 97.43)
Epoch: [26][250/391]	Time  0.171 ( 0.183)	Data  0.001 ( 0.006)	Loss 7.1347e-01 (6.3187e-01)	Acc@1  77.34 ( 80.34)	Acc@5  98.44 ( 97.38)
Epoch: [26][260/391]	Time  0.203 ( 0.183)	Data  0.001 ( 0.006)	Loss 6.7474e-01 (6.3533e-01)	Acc@1  77.34 ( 80.24)	Acc@5  98.44 ( 97.34)
Epoch: [26][270/391]	Time  0.206 ( 0.184)	Data  0.001 ( 0.006)	Loss 6.2297e-01 (6.3635e-01)	Acc@1  81.25 ( 80.17)	Acc@5  96.88 ( 97.33)
Epoch: [26][280/391]	Time  0.198 ( 0.185)	Data  0.001 ( 0.006)	Loss 6.1787e-01 (6.3664e-01)	Acc@1  79.69 ( 80.16)	Acc@5  99.22 ( 97.34)
Epoch: [26][290/391]	Time  0.173 ( 0.184)	Data  0.001 ( 0.006)	Loss 6.0029e-01 (6.3917e-01)	Acc@1  79.69 ( 80.06)	Acc@5  96.88 ( 97.33)
Epoch: [26][300/391]	Time  0.169 ( 0.184)	Data  0.001 ( 0.006)	Loss 7.1979e-01 (6.3988e-01)	Acc@1  79.69 ( 80.11)	Acc@5  97.66 ( 97.30)
Epoch: [26][310/391]	Time  0.169 ( 0.184)	Data  0.001 ( 0.006)	Loss 7.7467e-01 (6.4145e-01)	Acc@1  71.88 ( 80.07)	Acc@5  96.88 ( 97.29)
Epoch: [26][320/391]	Time  0.172 ( 0.183)	Data  0.001 ( 0.006)	Loss 9.2989e-01 (6.4214e-01)	Acc@1  72.66 ( 80.05)	Acc@5  93.75 ( 97.27)
Epoch: [26][330/391]	Time  0.170 ( 0.183)	Data  0.001 ( 0.006)	Loss 6.7975e-01 (6.4374e-01)	Acc@1  75.78 ( 80.01)	Acc@5  97.66 ( 97.24)
Epoch: [26][340/391]	Time  0.199 ( 0.183)	Data  0.001 ( 0.006)	Loss 7.1082e-01 (6.4431e-01)	Acc@1  78.91 ( 79.96)	Acc@5  95.31 ( 97.23)
Epoch: [26][350/391]	Time  0.209 ( 0.184)	Data  0.001 ( 0.006)	Loss 7.0661e-01 (6.4526e-01)	Acc@1  76.56 ( 79.93)	Acc@5  98.44 ( 97.23)
Epoch: [26][360/391]	Time  0.207 ( 0.184)	Data  0.001 ( 0.006)	Loss 4.8178e-01 (6.4629e-01)	Acc@1  88.28 ( 79.91)	Acc@5  98.44 ( 97.23)
Epoch: [26][370/391]	Time  0.170 ( 0.184)	Data  0.001 ( 0.006)	Loss 5.2036e-01 (6.4783e-01)	Acc@1  85.16 ( 79.83)	Acc@5  96.88 ( 97.21)
Epoch: [26][380/391]	Time  0.171 ( 0.184)	Data  0.001 ( 0.006)	Loss 5.8344e-01 (6.4898e-01)	Acc@1  81.25 ( 79.84)	Acc@5  98.44 ( 97.20)
Epoch: [26][390/391]	Time  0.181 ( 0.184)	Data  0.001 ( 0.006)	Loss 8.9291e-01 (6.5045e-01)	Acc@1  68.75 ( 79.82)	Acc@5  98.75 ( 97.19)
## e[26] optimizer.zero_grad (sum) time: 0.3680427074432373
## e[26]       loss.backward (sum) time: 24.455736875534058
## e[26]      optimizer.step (sum) time: 3.6276822090148926
## epoch[26] training(only) time: 71.89730787277222
# Switched to evaluate mode...
Test: [  0/100]	Time  0.301 ( 0.301)	Loss 1.5371e+00 (1.5371e+00)	Acc@1  65.00 ( 65.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.057 ( 0.080)	Loss 1.4824e+00 (1.4506e+00)	Acc@1  65.00 ( 66.18)	Acc@5  90.00 ( 87.45)
Test: [ 20/100]	Time  0.055 ( 0.068)	Loss 1.4918e+00 (1.3870e+00)	Acc@1  63.00 ( 66.48)	Acc@5  90.00 ( 88.71)
Test: [ 30/100]	Time  0.060 ( 0.064)	Loss 1.9478e+00 (1.4064e+00)	Acc@1  48.00 ( 65.52)	Acc@5  88.00 ( 88.65)
Test: [ 40/100]	Time  0.055 ( 0.062)	Loss 1.3843e+00 (1.3980e+00)	Acc@1  70.00 ( 65.24)	Acc@5  89.00 ( 89.22)
Test: [ 50/100]	Time  0.055 ( 0.061)	Loss 1.6544e+00 (1.3932e+00)	Acc@1  61.00 ( 65.33)	Acc@5  91.00 ( 89.25)
Test: [ 60/100]	Time  0.062 ( 0.060)	Loss 1.1127e+00 (1.3634e+00)	Acc@1  68.00 ( 65.49)	Acc@5  92.00 ( 89.54)
Test: [ 70/100]	Time  0.055 ( 0.059)	Loss 1.5009e+00 (1.3753e+00)	Acc@1  61.00 ( 65.28)	Acc@5  88.00 ( 89.39)
Test: [ 80/100]	Time  0.063 ( 0.059)	Loss 1.8032e+00 (1.3782e+00)	Acc@1  54.00 ( 65.12)	Acc@5  85.00 ( 89.32)
Test: [ 90/100]	Time  0.060 ( 0.060)	Loss 1.7618e+00 (1.3623e+00)	Acc@1  59.00 ( 65.30)	Acc@5  86.00 ( 89.49)
 * Acc@1 65.450 Acc@5 89.510
### epoch[26] execution time: 77.98461580276489
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.384 ( 0.384)	Data  0.205 ( 0.205)	Loss 6.3920e-01 (6.3920e-01)	Acc@1  82.81 ( 82.81)	Acc@5  96.88 ( 96.88)
Epoch: [27][ 10/391]	Time  0.202 ( 0.222)	Data  0.002 ( 0.024)	Loss 5.2073e-01 (5.5303e-01)	Acc@1  80.47 ( 81.82)	Acc@5  99.22 ( 98.30)
Epoch: [27][ 20/391]	Time  0.176 ( 0.201)	Data  0.001 ( 0.016)	Loss 4.3447e-01 (5.5661e-01)	Acc@1  83.59 ( 81.77)	Acc@5  99.22 ( 98.03)
Epoch: [27][ 30/391]	Time  0.171 ( 0.192)	Data  0.001 ( 0.012)	Loss 3.9740e-01 (5.7734e-01)	Acc@1  85.94 ( 81.38)	Acc@5 100.00 ( 97.83)
Epoch: [27][ 40/391]	Time  0.147 ( 0.186)	Data  0.001 ( 0.011)	Loss 5.6563e-01 (5.6827e-01)	Acc@1  84.38 ( 81.88)	Acc@5  97.66 ( 97.71)
Epoch: [27][ 50/391]	Time  0.171 ( 0.183)	Data  0.001 ( 0.009)	Loss 4.5803e-01 (5.6698e-01)	Acc@1  85.16 ( 81.69)	Acc@5  98.44 ( 97.67)
Epoch: [27][ 60/391]	Time  0.175 ( 0.182)	Data  0.001 ( 0.009)	Loss 5.4828e-01 (5.6019e-01)	Acc@1  84.38 ( 82.10)	Acc@5  98.44 ( 97.76)
Epoch: [27][ 70/391]	Time  0.173 ( 0.180)	Data  0.001 ( 0.008)	Loss 4.9213e-01 (5.6125e-01)	Acc@1  82.03 ( 82.09)	Acc@5  99.22 ( 97.78)
Epoch: [27][ 80/391]	Time  0.204 ( 0.181)	Data  0.002 ( 0.008)	Loss 5.3650e-01 (5.5984e-01)	Acc@1  81.25 ( 82.07)	Acc@5  99.22 ( 97.74)
Epoch: [27][ 90/391]	Time  0.206 ( 0.184)	Data  0.001 ( 0.007)	Loss 6.8084e-01 (5.6589e-01)	Acc@1  82.81 ( 82.02)	Acc@5  95.31 ( 97.71)
Epoch: [27][100/391]	Time  0.169 ( 0.186)	Data  0.002 ( 0.007)	Loss 7.4654e-01 (5.6073e-01)	Acc@1  75.78 ( 82.28)	Acc@5  99.22 ( 97.78)
Epoch: [27][110/391]	Time  0.174 ( 0.184)	Data  0.001 ( 0.007)	Loss 6.3132e-01 (5.6390e-01)	Acc@1  82.81 ( 82.19)	Acc@5  97.66 ( 97.74)
Epoch: [27][120/391]	Time  0.170 ( 0.183)	Data  0.001 ( 0.007)	Loss 8.1410e-01 (5.6840e-01)	Acc@1  73.44 ( 82.08)	Acc@5  98.44 ( 97.76)
Epoch: [27][130/391]	Time  0.157 ( 0.182)	Data  0.001 ( 0.007)	Loss 7.4930e-01 (5.7264e-01)	Acc@1  77.34 ( 82.00)	Acc@5  94.53 ( 97.74)
Epoch: [27][140/391]	Time  0.175 ( 0.182)	Data  0.001 ( 0.007)	Loss 8.1283e-01 (5.7620e-01)	Acc@1  71.09 ( 81.85)	Acc@5  96.09 ( 97.71)
Epoch: [27][150/391]	Time  0.168 ( 0.181)	Data  0.001 ( 0.007)	Loss 6.7108e-01 (5.8150e-01)	Acc@1  80.47 ( 81.71)	Acc@5  98.44 ( 97.65)
Epoch: [27][160/391]	Time  0.188 ( 0.181)	Data  0.001 ( 0.007)	Loss 5.2642e-01 (5.7989e-01)	Acc@1  82.03 ( 81.73)	Acc@5  98.44 ( 97.65)
Epoch: [27][170/391]	Time  0.196 ( 0.182)	Data  0.001 ( 0.007)	Loss 7.2398e-01 (5.8301e-01)	Acc@1  74.22 ( 81.59)	Acc@5  96.09 ( 97.66)
Epoch: [27][180/391]	Time  0.169 ( 0.183)	Data  0.001 ( 0.007)	Loss 6.5113e-01 (5.8520e-01)	Acc@1  77.34 ( 81.47)	Acc@5  96.09 ( 97.66)
Epoch: [27][190/391]	Time  0.171 ( 0.182)	Data  0.001 ( 0.007)	Loss 5.3212e-01 (5.8756e-01)	Acc@1  81.25 ( 81.41)	Acc@5  99.22 ( 97.67)
Epoch: [27][200/391]	Time  0.171 ( 0.181)	Data  0.001 ( 0.006)	Loss 6.8940e-01 (5.9242e-01)	Acc@1  75.78 ( 81.30)	Acc@5  96.88 ( 97.63)
Epoch: [27][210/391]	Time  0.159 ( 0.181)	Data  0.001 ( 0.006)	Loss 5.4308e-01 (5.9565e-01)	Acc@1  82.81 ( 81.25)	Acc@5  98.44 ( 97.62)
Epoch: [27][220/391]	Time  0.169 ( 0.180)	Data  0.001 ( 0.006)	Loss 4.9045e-01 (5.9750e-01)	Acc@1  85.16 ( 81.25)	Acc@5  96.88 ( 97.58)
Epoch: [27][230/391]	Time  0.168 ( 0.180)	Data  0.001 ( 0.006)	Loss 7.0099e-01 (5.9943e-01)	Acc@1  76.56 ( 81.16)	Acc@5  96.09 ( 97.56)
Epoch: [27][240/391]	Time  0.192 ( 0.180)	Data  0.001 ( 0.006)	Loss 7.8552e-01 (6.0029e-01)	Acc@1  80.47 ( 81.16)	Acc@5  95.31 ( 97.56)
Epoch: [27][250/391]	Time  0.200 ( 0.180)	Data  0.001 ( 0.006)	Loss 6.1524e-01 (6.0174e-01)	Acc@1  79.69 ( 81.12)	Acc@5  96.88 ( 97.56)
Epoch: [27][260/391]	Time  0.202 ( 0.181)	Data  0.001 ( 0.006)	Loss 6.4019e-01 (6.0337e-01)	Acc@1  80.47 ( 81.07)	Acc@5  96.09 ( 97.55)
Epoch: [27][270/391]	Time  0.174 ( 0.181)	Data  0.001 ( 0.006)	Loss 4.6301e-01 (6.0428e-01)	Acc@1  86.72 ( 81.04)	Acc@5  99.22 ( 97.52)
Epoch: [27][280/391]	Time  0.168 ( 0.180)	Data  0.001 ( 0.006)	Loss 6.1933e-01 (6.0786e-01)	Acc@1  82.81 ( 80.94)	Acc@5  96.88 ( 97.49)
Epoch: [27][290/391]	Time  0.172 ( 0.180)	Data  0.001 ( 0.006)	Loss 6.9054e-01 (6.1063e-01)	Acc@1  82.81 ( 80.88)	Acc@5  96.09 ( 97.48)
Epoch: [27][300/391]	Time  0.167 ( 0.179)	Data  0.001 ( 0.006)	Loss 7.5291e-01 (6.1425e-01)	Acc@1  75.00 ( 80.75)	Acc@5  96.88 ( 97.45)
Epoch: [27][310/391]	Time  0.164 ( 0.179)	Data  0.001 ( 0.006)	Loss 6.5402e-01 (6.1696e-01)	Acc@1  77.34 ( 80.65)	Acc@5  97.66 ( 97.44)
Epoch: [27][320/391]	Time  0.193 ( 0.179)	Data  0.001 ( 0.006)	Loss 5.7624e-01 (6.1865e-01)	Acc@1  83.59 ( 80.60)	Acc@5  97.66 ( 97.41)
Epoch: [27][330/391]	Time  0.197 ( 0.180)	Data  0.001 ( 0.006)	Loss 5.9237e-01 (6.2141e-01)	Acc@1  79.69 ( 80.50)	Acc@5  99.22 ( 97.41)
Epoch: [27][340/391]	Time  0.198 ( 0.180)	Data  0.001 ( 0.006)	Loss 7.0045e-01 (6.2336e-01)	Acc@1  75.00 ( 80.45)	Acc@5  97.66 ( 97.37)
Epoch: [27][350/391]	Time  0.164 ( 0.180)	Data  0.001 ( 0.006)	Loss 5.0167e-01 (6.2434e-01)	Acc@1  79.69 ( 80.38)	Acc@5  99.22 ( 97.37)
Epoch: [27][360/391]	Time  0.168 ( 0.180)	Data  0.001 ( 0.006)	Loss 6.9133e-01 (6.2665e-01)	Acc@1  78.12 ( 80.34)	Acc@5  98.44 ( 97.36)
Epoch: [27][370/391]	Time  0.179 ( 0.179)	Data  0.002 ( 0.006)	Loss 5.7446e-01 (6.2721e-01)	Acc@1  84.38 ( 80.33)	Acc@5  96.88 ( 97.37)
Epoch: [27][380/391]	Time  0.170 ( 0.179)	Data  0.001 ( 0.006)	Loss 8.5882e-01 (6.3086e-01)	Acc@1  75.00 ( 80.22)	Acc@5  94.53 ( 97.33)
Epoch: [27][390/391]	Time  0.167 ( 0.179)	Data  0.001 ( 0.006)	Loss 6.9002e-01 (6.3319e-01)	Acc@1  76.25 ( 80.15)	Acc@5  97.50 ( 97.30)
## e[27] optimizer.zero_grad (sum) time: 0.35631489753723145
## e[27]       loss.backward (sum) time: 23.631266355514526
## e[27]      optimizer.step (sum) time: 3.522526979446411
## epoch[27] training(only) time: 70.14392375946045
# Switched to evaluate mode...
Test: [  0/100]	Time  0.265 ( 0.265)	Loss 1.4112e+00 (1.4112e+00)	Acc@1  69.00 ( 69.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.055 ( 0.073)	Loss 1.5819e+00 (1.4689e+00)	Acc@1  63.00 ( 64.18)	Acc@5  90.00 ( 88.27)
Test: [ 20/100]	Time  0.060 ( 0.066)	Loss 1.4311e+00 (1.3982e+00)	Acc@1  68.00 ( 65.81)	Acc@5  87.00 ( 89.19)
Test: [ 30/100]	Time  0.060 ( 0.064)	Loss 1.7676e+00 (1.4030e+00)	Acc@1  59.00 ( 65.77)	Acc@5  85.00 ( 89.13)
Test: [ 40/100]	Time  0.059 ( 0.063)	Loss 1.3873e+00 (1.4034e+00)	Acc@1  66.00 ( 65.51)	Acc@5  90.00 ( 89.20)
Test: [ 50/100]	Time  0.060 ( 0.062)	Loss 1.4527e+00 (1.3977e+00)	Acc@1  63.00 ( 65.35)	Acc@5  87.00 ( 89.02)
Test: [ 60/100]	Time  0.059 ( 0.062)	Loss 1.2082e+00 (1.3768e+00)	Acc@1  66.00 ( 65.34)	Acc@5  93.00 ( 89.31)
Test: [ 70/100]	Time  0.059 ( 0.061)	Loss 1.4785e+00 (1.3981e+00)	Acc@1  68.00 ( 65.17)	Acc@5  89.00 ( 89.18)
Test: [ 80/100]	Time  0.059 ( 0.061)	Loss 1.5305e+00 (1.4075e+00)	Acc@1  58.00 ( 64.93)	Acc@5  88.00 ( 89.12)
Test: [ 90/100]	Time  0.054 ( 0.061)	Loss 2.2135e+00 (1.3943e+00)	Acc@1  55.00 ( 65.01)	Acc@5  82.00 ( 89.24)
 * Acc@1 65.180 Acc@5 89.260
### epoch[27] execution time: 76.27209496498108
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.365 ( 0.365)	Data  0.201 ( 0.201)	Loss 4.6459e-01 (4.6459e-01)	Acc@1  83.59 ( 83.59)	Acc@5 100.00 (100.00)
Epoch: [28][ 10/391]	Time  0.171 ( 0.189)	Data  0.001 ( 0.022)	Loss 6.3450e-01 (5.8198e-01)	Acc@1  80.47 ( 80.75)	Acc@5  96.09 ( 98.22)
Epoch: [28][ 20/391]	Time  0.171 ( 0.181)	Data  0.001 ( 0.014)	Loss 5.2681e-01 (5.7660e-01)	Acc@1  83.59 ( 81.25)	Acc@5  99.22 ( 98.33)
Epoch: [28][ 30/391]	Time  0.169 ( 0.177)	Data  0.001 ( 0.011)	Loss 4.7223e-01 (5.7845e-01)	Acc@1  86.72 ( 81.40)	Acc@5  98.44 ( 98.11)
Epoch: [28][ 40/391]	Time  0.167 ( 0.176)	Data  0.001 ( 0.009)	Loss 5.7304e-01 (5.6709e-01)	Acc@1  78.12 ( 81.59)	Acc@5  97.66 ( 98.15)
Epoch: [28][ 50/391]	Time  0.192 ( 0.175)	Data  0.002 ( 0.008)	Loss 5.0467e-01 (5.6831e-01)	Acc@1  82.03 ( 81.72)	Acc@5  99.22 ( 98.10)
Epoch: [28][ 60/391]	Time  0.189 ( 0.178)	Data  0.001 ( 0.008)	Loss 5.3594e-01 (5.6326e-01)	Acc@1  82.81 ( 81.88)	Acc@5  97.66 ( 98.13)
Epoch: [28][ 70/391]	Time  0.200 ( 0.181)	Data  0.001 ( 0.008)	Loss 6.9161e-01 (5.6107e-01)	Acc@1  76.56 ( 82.06)	Acc@5  95.31 ( 98.16)
Epoch: [28][ 80/391]	Time  0.167 ( 0.180)	Data  0.001 ( 0.008)	Loss 5.3821e-01 (5.5922e-01)	Acc@1  84.38 ( 82.13)	Acc@5 100.00 ( 98.17)
Epoch: [28][ 90/391]	Time  0.166 ( 0.179)	Data  0.001 ( 0.007)	Loss 7.7747e-01 (5.5960e-01)	Acc@1  77.34 ( 82.09)	Acc@5  96.88 ( 98.17)
Epoch: [28][100/391]	Time  0.195 ( 0.179)	Data  0.001 ( 0.007)	Loss 5.8478e-01 (5.6329e-01)	Acc@1  82.03 ( 81.91)	Acc@5  98.44 ( 98.17)
Epoch: [28][110/391]	Time  0.163 ( 0.178)	Data  0.001 ( 0.007)	Loss 4.7186e-01 (5.6421e-01)	Acc@1  85.16 ( 81.90)	Acc@5  98.44 ( 98.17)
Epoch: [28][120/391]	Time  0.169 ( 0.178)	Data  0.001 ( 0.007)	Loss 6.2925e-01 (5.6491e-01)	Acc@1  78.91 ( 81.80)	Acc@5  97.66 ( 98.16)
Epoch: [28][130/391]	Time  0.196 ( 0.177)	Data  0.001 ( 0.007)	Loss 6.9092e-01 (5.6963e-01)	Acc@1  77.34 ( 81.69)	Acc@5  96.88 ( 98.08)
Epoch: [28][140/391]	Time  0.195 ( 0.178)	Data  0.001 ( 0.006)	Loss 7.0252e-01 (5.7368e-01)	Acc@1  81.25 ( 81.56)	Acc@5  97.66 ( 98.06)
Epoch: [28][150/391]	Time  0.195 ( 0.179)	Data  0.001 ( 0.007)	Loss 5.2093e-01 (5.7615e-01)	Acc@1  84.38 ( 81.50)	Acc@5  98.44 ( 98.06)
Epoch: [28][160/391]	Time  0.172 ( 0.179)	Data  0.001 ( 0.007)	Loss 6.2399e-01 (5.7685e-01)	Acc@1  78.12 ( 81.50)	Acc@5  96.88 ( 98.04)
Epoch: [28][170/391]	Time  0.163 ( 0.179)	Data  0.001 ( 0.006)	Loss 6.6393e-01 (5.7783e-01)	Acc@1  75.00 ( 81.47)	Acc@5  99.22 ( 98.06)
Epoch: [28][180/391]	Time  0.189 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.0057e-01 (5.7846e-01)	Acc@1  78.91 ( 81.44)	Acc@5  95.31 ( 98.04)
Epoch: [28][190/391]	Time  0.168 ( 0.178)	Data  0.001 ( 0.006)	Loss 4.5174e-01 (5.8037e-01)	Acc@1  84.38 ( 81.41)	Acc@5  99.22 ( 98.03)
Epoch: [28][200/391]	Time  0.168 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.0942e-01 (5.8156e-01)	Acc@1  82.81 ( 81.38)	Acc@5  96.88 ( 97.97)
Epoch: [28][210/391]	Time  0.182 ( 0.177)	Data  0.001 ( 0.006)	Loss 4.9932e-01 (5.8291e-01)	Acc@1  85.16 ( 81.38)	Acc@5  98.44 ( 97.95)
Epoch: [28][220/391]	Time  0.198 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.2869e-01 (5.8394e-01)	Acc@1  82.03 ( 81.41)	Acc@5  96.09 ( 97.94)
Epoch: [28][230/391]	Time  0.197 ( 0.179)	Data  0.001 ( 0.006)	Loss 6.3496e-01 (5.8705e-01)	Acc@1  81.25 ( 81.34)	Acc@5  97.66 ( 97.88)
Epoch: [28][240/391]	Time  0.167 ( 0.179)	Data  0.001 ( 0.006)	Loss 7.5321e-01 (5.8926e-01)	Acc@1  77.34 ( 81.28)	Acc@5  94.53 ( 97.87)
Epoch: [28][250/391]	Time  0.168 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.3171e-01 (5.9263e-01)	Acc@1  82.03 ( 81.18)	Acc@5  96.88 ( 97.86)
Epoch: [28][260/391]	Time  0.163 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.3130e-01 (5.9279e-01)	Acc@1  75.78 ( 81.16)	Acc@5 100.00 ( 97.87)
Epoch: [28][270/391]	Time  0.169 ( 0.178)	Data  0.001 ( 0.006)	Loss 4.8674e-01 (5.9225e-01)	Acc@1  85.94 ( 81.23)	Acc@5  96.88 ( 97.85)
Epoch: [28][280/391]	Time  0.176 ( 0.178)	Data  0.001 ( 0.006)	Loss 4.6918e-01 (5.9316e-01)	Acc@1  82.81 ( 81.16)	Acc@5  99.22 ( 97.83)
Epoch: [28][290/391]	Time  0.168 ( 0.177)	Data  0.001 ( 0.006)	Loss 8.9014e-01 (5.9592e-01)	Acc@1  71.09 ( 81.08)	Acc@5  93.75 ( 97.79)
Epoch: [28][300/391]	Time  0.199 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.6756e-01 (5.9863e-01)	Acc@1  80.47 ( 81.02)	Acc@5  97.66 ( 97.75)
Epoch: [28][310/391]	Time  0.198 ( 0.178)	Data  0.001 ( 0.006)	Loss 7.4033e-01 (5.9975e-01)	Acc@1  79.69 ( 81.01)	Acc@5  94.53 ( 97.72)
Epoch: [28][320/391]	Time  0.169 ( 0.179)	Data  0.001 ( 0.006)	Loss 6.4371e-01 (6.0034e-01)	Acc@1  81.25 ( 81.04)	Acc@5  98.44 ( 97.73)
Epoch: [28][330/391]	Time  0.171 ( 0.178)	Data  0.001 ( 0.006)	Loss 9.1467e-01 (6.0199e-01)	Acc@1  70.31 ( 80.98)	Acc@5  92.97 ( 97.69)
Epoch: [28][340/391]	Time  0.171 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.4145e-01 (6.0435e-01)	Acc@1  78.91 ( 80.88)	Acc@5  97.66 ( 97.68)
Epoch: [28][350/391]	Time  0.162 ( 0.178)	Data  0.001 ( 0.006)	Loss 7.3575e-01 (6.0650e-01)	Acc@1  78.91 ( 80.82)	Acc@5  94.53 ( 97.65)
Epoch: [28][360/391]	Time  0.167 ( 0.178)	Data  0.001 ( 0.006)	Loss 7.1265e-01 (6.0611e-01)	Acc@1  75.78 ( 80.79)	Acc@5  96.88 ( 97.65)
Epoch: [28][370/391]	Time  0.167 ( 0.177)	Data  0.001 ( 0.006)	Loss 5.6230e-01 (6.0791e-01)	Acc@1  82.03 ( 80.73)	Acc@5  95.31 ( 97.62)
Epoch: [28][380/391]	Time  0.192 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.7355e-01 (6.0822e-01)	Acc@1  80.47 ( 80.75)	Acc@5  96.88 ( 97.63)
Epoch: [28][390/391]	Time  0.189 ( 0.178)	Data  0.001 ( 0.006)	Loss 8.6663e-01 (6.1003e-01)	Acc@1  72.50 ( 80.67)	Acc@5  93.75 ( 97.61)
## e[28] optimizer.zero_grad (sum) time: 0.34349870681762695
## e[28]       loss.backward (sum) time: 22.925566911697388
## e[28]      optimizer.step (sum) time: 3.4231019020080566
## epoch[28] training(only) time: 69.69637751579285
# Switched to evaluate mode...
Test: [  0/100]	Time  0.227 ( 0.227)	Loss 1.3686e+00 (1.3686e+00)	Acc@1  70.00 ( 70.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.060 ( 0.075)	Loss 1.6236e+00 (1.4984e+00)	Acc@1  67.00 ( 66.00)	Acc@5  90.00 ( 88.00)
Test: [ 20/100]	Time  0.053 ( 0.066)	Loss 1.0523e+00 (1.3818e+00)	Acc@1  73.00 ( 67.05)	Acc@5  89.00 ( 89.81)
Test: [ 30/100]	Time  0.054 ( 0.062)	Loss 1.6490e+00 (1.3964e+00)	Acc@1  53.00 ( 66.42)	Acc@5  89.00 ( 90.10)
Test: [ 40/100]	Time  0.064 ( 0.061)	Loss 1.4215e+00 (1.4115e+00)	Acc@1  64.00 ( 65.78)	Acc@5  91.00 ( 90.34)
Test: [ 50/100]	Time  0.055 ( 0.060)	Loss 1.9110e+00 (1.4330e+00)	Acc@1  58.00 ( 65.37)	Acc@5  88.00 ( 90.04)
Test: [ 60/100]	Time  0.055 ( 0.059)	Loss 1.3079e+00 (1.3946e+00)	Acc@1  67.00 ( 65.70)	Acc@5  87.00 ( 90.23)
Test: [ 70/100]	Time  0.054 ( 0.058)	Loss 1.5599e+00 (1.4015e+00)	Acc@1  65.00 ( 65.55)	Acc@5  88.00 ( 90.14)
Test: [ 80/100]	Time  0.055 ( 0.058)	Loss 1.6104e+00 (1.4128e+00)	Acc@1  61.00 ( 65.37)	Acc@5  87.00 ( 89.88)
Test: [ 90/100]	Time  0.059 ( 0.058)	Loss 1.9918e+00 (1.4005e+00)	Acc@1  56.00 ( 65.58)	Acc@5  82.00 ( 89.95)
 * Acc@1 66.000 Acc@5 90.090
### epoch[28] execution time: 75.5237889289856
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.371 ( 0.371)	Data  0.207 ( 0.207)	Loss 6.1754e-01 (6.1754e-01)	Acc@1  82.03 ( 82.03)	Acc@5  96.88 ( 96.88)
Epoch: [29][ 10/391]	Time  0.176 ( 0.188)	Data  0.001 ( 0.023)	Loss 3.2987e-01 (5.0721e-01)	Acc@1  90.62 ( 84.09)	Acc@5 100.00 ( 98.37)
Epoch: [29][ 20/391]	Time  0.166 ( 0.179)	Data  0.001 ( 0.015)	Loss 3.9518e-01 (4.8824e-01)	Acc@1  86.72 ( 84.45)	Acc@5  99.22 ( 98.66)
Epoch: [29][ 30/391]	Time  0.196 ( 0.180)	Data  0.001 ( 0.012)	Loss 5.8763e-01 (4.9585e-01)	Acc@1  84.38 ( 84.43)	Acc@5  98.44 ( 98.64)
Epoch: [29][ 40/391]	Time  0.194 ( 0.183)	Data  0.001 ( 0.011)	Loss 3.7883e-01 (5.0899e-01)	Acc@1  89.06 ( 84.07)	Acc@5  99.22 ( 98.34)
Epoch: [29][ 50/391]	Time  0.163 ( 0.184)	Data  0.001 ( 0.010)	Loss 6.2601e-01 (5.2327e-01)	Acc@1  80.47 ( 83.66)	Acc@5  96.88 ( 98.15)
Epoch: [29][ 60/391]	Time  0.171 ( 0.181)	Data  0.001 ( 0.009)	Loss 5.6288e-01 (5.2287e-01)	Acc@1  83.59 ( 83.61)	Acc@5  96.88 ( 98.19)
Epoch: [29][ 70/391]	Time  0.169 ( 0.179)	Data  0.001 ( 0.008)	Loss 4.8197e-01 (5.1959e-01)	Acc@1  82.81 ( 83.65)	Acc@5  99.22 ( 98.21)
Epoch: [29][ 80/391]	Time  0.169 ( 0.178)	Data  0.001 ( 0.008)	Loss 6.6085e-01 (5.2565e-01)	Acc@1  78.91 ( 83.56)	Acc@5  96.88 ( 98.08)
Epoch: [29][ 90/391]	Time  0.188 ( 0.177)	Data  0.001 ( 0.008)	Loss 6.1120e-01 (5.2629e-01)	Acc@1  81.25 ( 83.47)	Acc@5  99.22 ( 98.11)
Epoch: [29][100/391]	Time  0.168 ( 0.176)	Data  0.001 ( 0.007)	Loss 7.1649e-01 (5.3014e-01)	Acc@1  76.56 ( 83.24)	Acc@5  97.66 ( 98.09)
Epoch: [29][110/391]	Time  0.193 ( 0.177)	Data  0.001 ( 0.007)	Loss 3.9566e-01 (5.3468e-01)	Acc@1  88.28 ( 83.09)	Acc@5  99.22 ( 98.12)
Epoch: [29][120/391]	Time  0.193 ( 0.178)	Data  0.001 ( 0.007)	Loss 5.6530e-01 (5.3709e-01)	Acc@1  84.38 ( 83.06)	Acc@5  96.88 ( 98.08)
Epoch: [29][130/391]	Time  0.174 ( 0.179)	Data  0.001 ( 0.007)	Loss 6.6965e-01 (5.3985e-01)	Acc@1  82.03 ( 82.95)	Acc@5  96.88 ( 98.02)
Epoch: [29][140/391]	Time  0.164 ( 0.178)	Data  0.001 ( 0.007)	Loss 6.3275e-01 (5.4006e-01)	Acc@1  77.34 ( 82.96)	Acc@5  96.88 ( 98.02)
Epoch: [29][150/391]	Time  0.169 ( 0.178)	Data  0.001 ( 0.007)	Loss 5.5777e-01 (5.4291e-01)	Acc@1  82.03 ( 82.84)	Acc@5  99.22 ( 98.02)
Epoch: [29][160/391]	Time  0.175 ( 0.177)	Data  0.001 ( 0.007)	Loss 5.6153e-01 (5.4437e-01)	Acc@1  85.94 ( 82.75)	Acc@5  96.88 ( 98.04)
Epoch: [29][170/391]	Time  0.174 ( 0.177)	Data  0.001 ( 0.007)	Loss 6.8204e-01 (5.4748e-01)	Acc@1  77.34 ( 82.67)	Acc@5  95.31 ( 97.98)
Epoch: [29][180/391]	Time  0.168 ( 0.176)	Data  0.001 ( 0.007)	Loss 5.4935e-01 (5.4881e-01)	Acc@1  83.59 ( 82.58)	Acc@5  99.22 ( 97.99)
Epoch: [29][190/391]	Time  0.191 ( 0.176)	Data  0.002 ( 0.006)	Loss 4.2470e-01 (5.5055e-01)	Acc@1  85.16 ( 82.54)	Acc@5  98.44 ( 97.99)
Epoch: [29][200/391]	Time  0.193 ( 0.177)	Data  0.001 ( 0.006)	Loss 4.9277e-01 (5.5376e-01)	Acc@1  85.16 ( 82.42)	Acc@5  97.66 ( 97.97)
Epoch: [29][210/391]	Time  0.168 ( 0.178)	Data  0.001 ( 0.007)	Loss 5.8818e-01 (5.5677e-01)	Acc@1  81.25 ( 82.35)	Acc@5  97.66 ( 97.93)
Epoch: [29][220/391]	Time  0.167 ( 0.178)	Data  0.001 ( 0.006)	Loss 7.1848e-01 (5.6077e-01)	Acc@1  72.66 ( 82.21)	Acc@5  96.88 ( 97.88)
Epoch: [29][230/391]	Time  0.164 ( 0.177)	Data  0.001 ( 0.006)	Loss 6.3941e-01 (5.6119e-01)	Acc@1  74.22 ( 82.21)	Acc@5  99.22 ( 97.88)
Epoch: [29][240/391]	Time  0.165 ( 0.177)	Data  0.002 ( 0.006)	Loss 5.6069e-01 (5.6299e-01)	Acc@1  82.03 ( 82.13)	Acc@5  97.66 ( 97.87)
Epoch: [29][250/391]	Time  0.169 ( 0.176)	Data  0.001 ( 0.006)	Loss 6.1297e-01 (5.6412e-01)	Acc@1  83.59 ( 82.11)	Acc@5  96.88 ( 97.86)
Epoch: [29][260/391]	Time  0.169 ( 0.176)	Data  0.001 ( 0.006)	Loss 6.4839e-01 (5.6573e-01)	Acc@1  78.91 ( 82.03)	Acc@5  97.66 ( 97.86)
Epoch: [29][270/391]	Time  0.185 ( 0.176)	Data  0.001 ( 0.006)	Loss 7.8502e-01 (5.6663e-01)	Acc@1  75.00 ( 82.00)	Acc@5  96.09 ( 97.85)
Epoch: [29][280/391]	Time  0.196 ( 0.177)	Data  0.001 ( 0.006)	Loss 7.2989e-01 (5.7028e-01)	Acc@1  80.47 ( 81.91)	Acc@5  96.88 ( 97.82)
Epoch: [29][290/391]	Time  0.188 ( 0.178)	Data  0.001 ( 0.006)	Loss 6.5239e-01 (5.7384e-01)	Acc@1  78.91 ( 81.81)	Acc@5  97.66 ( 97.80)
Epoch: [29][300/391]	Time  0.170 ( 0.177)	Data  0.001 ( 0.006)	Loss 6.5008e-01 (5.7587e-01)	Acc@1  75.00 ( 81.72)	Acc@5  96.88 ( 97.78)
Epoch: [29][310/391]	Time  0.163 ( 0.177)	Data  0.001 ( 0.006)	Loss 7.2324e-01 (5.7803e-01)	Acc@1  75.00 ( 81.65)	Acc@5  95.31 ( 97.77)
Epoch: [29][320/391]	Time  0.174 ( 0.177)	Data  0.001 ( 0.006)	Loss 6.3737e-01 (5.7856e-01)	Acc@1  76.56 ( 81.59)	Acc@5  98.44 ( 97.77)
Epoch: [29][330/391]	Time  0.171 ( 0.177)	Data  0.002 ( 0.006)	Loss 6.6530e-01 (5.7951e-01)	Acc@1  78.12 ( 81.55)	Acc@5  96.88 ( 97.78)
Epoch: [29][340/391]	Time  0.170 ( 0.176)	Data  0.001 ( 0.006)	Loss 6.6731e-01 (5.8098e-01)	Acc@1  82.81 ( 81.54)	Acc@5  96.88 ( 97.76)
Epoch: [29][350/391]	Time  0.195 ( 0.176)	Data  0.001 ( 0.006)	Loss 4.3146e-01 (5.8210e-01)	Acc@1  85.94 ( 81.49)	Acc@5  99.22 ( 97.76)
Epoch: [29][360/391]	Time  0.197 ( 0.177)	Data  0.001 ( 0.006)	Loss 4.7042e-01 (5.8280e-01)	Acc@1  89.84 ( 81.51)	Acc@5  96.88 ( 97.74)
Epoch: [29][370/391]	Time  0.190 ( 0.177)	Data  0.001 ( 0.006)	Loss 5.8854e-01 (5.8522e-01)	Acc@1  79.69 ( 81.43)	Acc@5  97.66 ( 97.73)
Epoch: [29][380/391]	Time  0.167 ( 0.177)	Data  0.001 ( 0.006)	Loss 6.6967e-01 (5.8635e-01)	Acc@1  77.34 ( 81.42)	Acc@5  96.88 ( 97.74)
Epoch: [29][390/391]	Time  0.168 ( 0.177)	Data  0.001 ( 0.006)	Loss 6.9562e-01 (5.8740e-01)	Acc@1  76.25 ( 81.38)	Acc@5  96.25 ( 97.73)
## e[29] optimizer.zero_grad (sum) time: 0.34108400344848633
## e[29]       loss.backward (sum) time: 22.475593328475952
## e[29]      optimizer.step (sum) time: 3.408745050430298
## epoch[29] training(only) time: 69.25996375083923
# Switched to evaluate mode...
Test: [  0/100]	Time  0.220 ( 0.220)	Loss 1.4169e+00 (1.4169e+00)	Acc@1  67.00 ( 67.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.052 ( 0.070)	Loss 1.7331e+00 (1.4653e+00)	Acc@1  54.00 ( 65.00)	Acc@5  88.00 ( 89.55)
Test: [ 20/100]	Time  0.055 ( 0.063)	Loss 1.2248e+00 (1.3443e+00)	Acc@1  69.00 ( 66.43)	Acc@5  93.00 ( 90.48)
Test: [ 30/100]	Time  0.054 ( 0.060)	Loss 1.6740e+00 (1.3632e+00)	Acc@1  53.00 ( 65.68)	Acc@5  92.00 ( 90.03)
Test: [ 40/100]	Time  0.054 ( 0.058)	Loss 1.4108e+00 (1.3494e+00)	Acc@1  66.00 ( 65.95)	Acc@5  92.00 ( 90.37)
Test: [ 50/100]	Time  0.056 ( 0.058)	Loss 1.3924e+00 (1.3713e+00)	Acc@1  64.00 ( 65.69)	Acc@5  88.00 ( 90.08)
Test: [ 60/100]	Time  0.054 ( 0.057)	Loss 1.3490e+00 (1.3444e+00)	Acc@1  64.00 ( 66.21)	Acc@5  92.00 ( 90.31)
Test: [ 70/100]	Time  0.053 ( 0.057)	Loss 1.7217e+00 (1.3536e+00)	Acc@1  64.00 ( 65.97)	Acc@5  86.00 ( 90.30)
Test: [ 80/100]	Time  0.054 ( 0.056)	Loss 1.5824e+00 (1.3655e+00)	Acc@1  60.00 ( 65.75)	Acc@5  91.00 ( 90.27)
Test: [ 90/100]	Time  0.054 ( 0.056)	Loss 1.6970e+00 (1.3381e+00)	Acc@1  56.00 ( 66.29)	Acc@5  89.00 ( 90.45)
 * Acc@1 66.460 Acc@5 90.570
### epoch[29] execution time: 74.97828674316406
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.355 ( 0.355)	Data  0.190 ( 0.190)	Loss 3.9102e-01 (3.9102e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [30][ 10/391]	Time  0.191 ( 0.207)	Data  0.001 ( 0.023)	Loss 4.8386e-01 (4.8123e-01)	Acc@1  83.59 ( 85.01)	Acc@5  98.44 ( 98.65)
Epoch: [30][ 20/391]	Time  0.183 ( 0.200)	Data  0.001 ( 0.015)	Loss 3.4842e-01 (4.7461e-01)	Acc@1  91.41 ( 85.71)	Acc@5 100.00 ( 98.47)
Epoch: [30][ 30/391]	Time  0.171 ( 0.190)	Data  0.001 ( 0.012)	Loss 3.6334e-01 (4.5182e-01)	Acc@1  89.06 ( 86.16)	Acc@5  99.22 ( 98.71)
Epoch: [30][ 40/391]	Time  0.170 ( 0.185)	Data  0.001 ( 0.010)	Loss 4.4062e-01 (4.5183e-01)	Acc@1  85.94 ( 86.07)	Acc@5  98.44 ( 98.65)
Epoch: [30][ 50/391]	Time  0.171 ( 0.182)	Data  0.001 ( 0.009)	Loss 4.2797e-01 (4.4096e-01)	Acc@1  86.72 ( 86.50)	Acc@5 100.00 ( 98.76)
Epoch: [30][ 60/391]	Time  0.167 ( 0.180)	Data  0.001 ( 0.008)	Loss 3.0352e-01 (4.2994e-01)	Acc@1  92.97 ( 86.90)	Acc@5  99.22 ( 98.78)
Epoch: [30][ 70/391]	Time  0.158 ( 0.178)	Data  0.001 ( 0.008)	Loss 4.0323e-01 (4.1700e-01)	Acc@1  87.50 ( 87.36)	Acc@5  99.22 ( 98.88)
Epoch: [30][ 80/391]	Time  0.192 ( 0.178)	Data  0.001 ( 0.008)	Loss 3.5197e-01 (4.0420e-01)	Acc@1  86.72 ( 87.79)	Acc@5 100.00 ( 98.92)
Epoch: [30][ 90/391]	Time  0.195 ( 0.180)	Data  0.001 ( 0.008)	Loss 2.9839e-01 (4.0088e-01)	Acc@1  89.84 ( 87.86)	Acc@5 100.00 ( 98.94)
Epoch: [30][100/391]	Time  0.185 ( 0.181)	Data  0.001 ( 0.007)	Loss 2.6413e-01 (3.9560e-01)	Acc@1  93.75 ( 88.00)	Acc@5 100.00 ( 98.98)
Epoch: [30][110/391]	Time  0.163 ( 0.180)	Data  0.001 ( 0.007)	Loss 2.9929e-01 (3.9027e-01)	Acc@1  92.19 ( 88.18)	Acc@5  99.22 ( 98.99)
Epoch: [30][120/391]	Time  0.171 ( 0.179)	Data  0.001 ( 0.007)	Loss 4.0707e-01 (3.8322e-01)	Acc@1  85.16 ( 88.42)	Acc@5  99.22 ( 99.04)
Epoch: [30][130/391]	Time  0.161 ( 0.178)	Data  0.001 ( 0.007)	Loss 3.0954e-01 (3.7754e-01)	Acc@1  92.97 ( 88.66)	Acc@5  99.22 ( 99.05)
Epoch: [30][140/391]	Time  0.168 ( 0.178)	Data  0.001 ( 0.007)	Loss 4.2860e-01 (3.7429e-01)	Acc@1  86.72 ( 88.69)	Acc@5  99.22 ( 99.07)
Epoch: [30][150/391]	Time  0.171 ( 0.177)	Data  0.001 ( 0.007)	Loss 2.4078e-01 (3.7151e-01)	Acc@1  93.75 ( 88.78)	Acc@5 100.00 ( 99.07)
Epoch: [30][160/391]	Time  0.191 ( 0.177)	Data  0.001 ( 0.006)	Loss 4.2910e-01 (3.6788e-01)	Acc@1  84.38 ( 88.82)	Acc@5  98.44 ( 99.09)
Epoch: [30][170/391]	Time  0.195 ( 0.178)	Data  0.001 ( 0.007)	Loss 2.5279e-01 (3.6542e-01)	Acc@1  92.19 ( 88.91)	Acc@5  99.22 ( 99.10)
Epoch: [30][180/391]	Time  0.194 ( 0.179)	Data  0.001 ( 0.007)	Loss 1.7207e-01 (3.6093e-01)	Acc@1  96.88 ( 89.07)	Acc@5 100.00 ( 99.14)
Epoch: [30][190/391]	Time  0.171 ( 0.179)	Data  0.001 ( 0.006)	Loss 2.8615e-01 (3.5826e-01)	Acc@1  92.19 ( 89.14)	Acc@5  99.22 ( 99.15)
Epoch: [30][200/391]	Time  0.166 ( 0.178)	Data  0.001 ( 0.006)	Loss 2.6166e-01 (3.5466e-01)	Acc@1  94.53 ( 89.23)	Acc@5  99.22 ( 99.17)
Epoch: [30][210/391]	Time  0.162 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.9625e-01 (3.5118e-01)	Acc@1  96.09 ( 89.34)	Acc@5 100.00 ( 99.18)
Epoch: [30][220/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.006)	Loss 3.1011e-01 (3.4758e-01)	Acc@1  92.19 ( 89.49)	Acc@5  97.66 ( 99.17)
Epoch: [30][230/391]	Time  0.167 ( 0.177)	Data  0.001 ( 0.006)	Loss 2.9932e-01 (3.4569e-01)	Acc@1  90.62 ( 89.57)	Acc@5 100.00 ( 99.18)
Epoch: [30][240/391]	Time  0.179 ( 0.176)	Data  0.001 ( 0.006)	Loss 3.0177e-01 (3.4251e-01)	Acc@1  92.19 ( 89.66)	Acc@5  99.22 ( 99.20)
Epoch: [30][250/391]	Time  0.182 ( 0.177)	Data  0.001 ( 0.006)	Loss 3.2168e-01 (3.3989e-01)	Acc@1  94.53 ( 89.75)	Acc@5 100.00 ( 99.22)
Epoch: [30][260/391]	Time  0.185 ( 0.177)	Data  0.001 ( 0.006)	Loss 3.1222e-01 (3.3801e-01)	Acc@1  89.06 ( 89.78)	Acc@5 100.00 ( 99.23)
Epoch: [30][270/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.006)	Loss 4.1213e-01 (3.3788e-01)	Acc@1  88.28 ( 89.79)	Acc@5  97.66 ( 99.21)
Epoch: [30][280/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.006)	Loss 3.5713e-01 (3.3564e-01)	Acc@1  85.16 ( 89.82)	Acc@5 100.00 ( 99.22)
Epoch: [30][290/391]	Time  0.165 ( 0.176)	Data  0.001 ( 0.006)	Loss 3.0444e-01 (3.3334e-01)	Acc@1  91.41 ( 89.90)	Acc@5  98.44 ( 99.23)
Epoch: [30][300/391]	Time  0.165 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.2384e-01 (3.3126e-01)	Acc@1  90.62 ( 89.96)	Acc@5 100.00 ( 99.23)
Epoch: [30][310/391]	Time  0.166 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.3312e-01 (3.2945e-01)	Acc@1  92.97 ( 90.00)	Acc@5 100.00 ( 99.25)
Epoch: [30][320/391]	Time  0.187 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.7910e-01 (3.2809e-01)	Acc@1  90.62 ( 90.05)	Acc@5 100.00 ( 99.26)
Epoch: [30][330/391]	Time  0.182 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.4938e-01 (3.2545e-01)	Acc@1  90.62 ( 90.15)	Acc@5 100.00 ( 99.27)
Epoch: [30][340/391]	Time  0.182 ( 0.175)	Data  0.002 ( 0.006)	Loss 3.9140e-01 (3.2462e-01)	Acc@1  87.50 ( 90.18)	Acc@5  99.22 ( 99.27)
Epoch: [30][350/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.2232e-01 (3.2273e-01)	Acc@1  92.19 ( 90.24)	Acc@5 100.00 ( 99.27)
Epoch: [30][360/391]	Time  0.165 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.5187e-01 (3.2165e-01)	Acc@1  94.53 ( 90.28)	Acc@5 100.00 ( 99.28)
Epoch: [30][370/391]	Time  0.180 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.3846e-01 (3.2005e-01)	Acc@1  91.41 ( 90.32)	Acc@5  99.22 ( 99.29)
Epoch: [30][380/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.006)	Loss 2.6994e-01 (3.1824e-01)	Acc@1  92.97 ( 90.39)	Acc@5  99.22 ( 99.28)
Epoch: [30][390/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.006)	Loss 2.5364e-01 (3.1674e-01)	Acc@1  96.25 ( 90.42)	Acc@5 100.00 ( 99.29)
## e[30] optimizer.zero_grad (sum) time: 0.3340175151824951
## e[30]       loss.backward (sum) time: 22.06545352935791
## e[30]      optimizer.step (sum) time: 3.3076136112213135
## epoch[30] training(only) time: 68.22719407081604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.253 ( 0.253)	Loss 1.1278e+00 (1.1278e+00)	Acc@1  70.00 ( 70.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.053 ( 0.072)	Loss 1.2466e+00 (1.1714e+00)	Acc@1  69.00 ( 72.09)	Acc@5  92.00 ( 91.36)
Test: [ 20/100]	Time  0.058 ( 0.064)	Loss 9.7340e-01 (1.0563e+00)	Acc@1  74.00 ( 74.14)	Acc@5  94.00 ( 92.76)
Test: [ 30/100]	Time  0.057 ( 0.062)	Loss 1.5021e+00 (1.0864e+00)	Acc@1  57.00 ( 73.35)	Acc@5  92.00 ( 92.81)
Test: [ 40/100]	Time  0.056 ( 0.060)	Loss 1.1146e+00 (1.0762e+00)	Acc@1  73.00 ( 73.17)	Acc@5  97.00 ( 93.22)
Test: [ 50/100]	Time  0.057 ( 0.060)	Loss 1.2398e+00 (1.0863e+00)	Acc@1  68.00 ( 72.92)	Acc@5  91.00 ( 92.98)
Test: [ 60/100]	Time  0.056 ( 0.059)	Loss 1.2024e+00 (1.0567e+00)	Acc@1  68.00 ( 73.33)	Acc@5  92.00 ( 93.11)
Test: [ 70/100]	Time  0.057 ( 0.059)	Loss 1.5172e+00 (1.0652e+00)	Acc@1  69.00 ( 73.28)	Acc@5  90.00 ( 93.07)
Test: [ 80/100]	Time  0.056 ( 0.059)	Loss 1.3050e+00 (1.0703e+00)	Acc@1  66.00 ( 73.25)	Acc@5  91.00 ( 92.99)
Test: [ 90/100]	Time  0.053 ( 0.058)	Loss 1.4334e+00 (1.0521e+00)	Acc@1  65.00 ( 73.38)	Acc@5  91.00 ( 93.12)
 * Acc@1 73.620 Acc@5 93.160
### epoch[30] execution time: 74.11442399024963
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.388 ( 0.388)	Data  0.243 ( 0.243)	Loss 2.5706e-01 (2.5706e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.164 ( 0.185)	Data  0.001 ( 0.026)	Loss 2.6289e-01 (2.4924e-01)	Acc@1  90.62 ( 92.19)	Acc@5 100.00 ( 99.79)
Epoch: [31][ 20/391]	Time  0.165 ( 0.177)	Data  0.001 ( 0.016)	Loss 2.5949e-01 (2.2935e-01)	Acc@1  94.53 ( 93.15)	Acc@5  99.22 ( 99.81)
Epoch: [31][ 30/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.013)	Loss 2.6327e-01 (2.2728e-01)	Acc@1  89.06 ( 93.12)	Acc@5 100.00 ( 99.87)
Epoch: [31][ 40/391]	Time  0.166 ( 0.172)	Data  0.002 ( 0.011)	Loss 2.3562e-01 (2.3255e-01)	Acc@1  92.97 ( 93.04)	Acc@5  99.22 ( 99.79)
Epoch: [31][ 50/391]	Time  0.179 ( 0.171)	Data  0.001 ( 0.010)	Loss 2.0442e-01 (2.2755e-01)	Acc@1  95.31 ( 93.34)	Acc@5 100.00 ( 99.80)
Epoch: [31][ 60/391]	Time  0.187 ( 0.173)	Data  0.001 ( 0.009)	Loss 1.7118e-01 (2.3234e-01)	Acc@1  96.09 ( 93.17)	Acc@5 100.00 ( 99.80)
Epoch: [31][ 70/391]	Time  0.184 ( 0.174)	Data  0.001 ( 0.008)	Loss 2.4741e-01 (2.3646e-01)	Acc@1  92.19 ( 92.95)	Acc@5  99.22 ( 99.74)
Epoch: [31][ 80/391]	Time  0.162 ( 0.174)	Data  0.001 ( 0.008)	Loss 1.8956e-01 (2.3230e-01)	Acc@1  94.53 ( 93.03)	Acc@5 100.00 ( 99.76)
Epoch: [31][ 90/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.008)	Loss 2.2740e-01 (2.3238e-01)	Acc@1  92.19 ( 92.97)	Acc@5 100.00 ( 99.73)
Epoch: [31][100/391]	Time  0.188 ( 0.172)	Data  0.001 ( 0.007)	Loss 2.6968e-01 (2.3339e-01)	Acc@1  87.50 ( 92.84)	Acc@5 100.00 ( 99.71)
Epoch: [31][110/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.007)	Loss 2.7641e-01 (2.3366e-01)	Acc@1  91.41 ( 92.79)	Acc@5 100.00 ( 99.69)
Epoch: [31][120/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.007)	Loss 1.9281e-01 (2.3238e-01)	Acc@1  96.09 ( 92.87)	Acc@5 100.00 ( 99.69)
Epoch: [31][130/391]	Time  0.184 ( 0.171)	Data  0.001 ( 0.007)	Loss 1.7146e-01 (2.3353e-01)	Acc@1  96.88 ( 92.88)	Acc@5 100.00 ( 99.64)
Epoch: [31][140/391]	Time  0.184 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.7052e-01 (2.3418e-01)	Acc@1  96.88 ( 92.84)	Acc@5 100.00 ( 99.65)
Epoch: [31][150/391]	Time  0.182 ( 0.173)	Data  0.001 ( 0.007)	Loss 1.6953e-01 (2.3426e-01)	Acc@1  95.31 ( 92.86)	Acc@5 100.00 ( 99.65)
Epoch: [31][160/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.007)	Loss 2.2583e-01 (2.3467e-01)	Acc@1  91.41 ( 92.82)	Acc@5 100.00 ( 99.65)
Epoch: [31][170/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.8172e-01 (2.3350e-01)	Acc@1  94.53 ( 92.83)	Acc@5 100.00 ( 99.66)
Epoch: [31][180/391]	Time  0.155 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.6663e-01 (2.3262e-01)	Acc@1  95.31 ( 92.90)	Acc@5 100.00 ( 99.65)
Epoch: [31][190/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.8481e-01 (2.3326e-01)	Acc@1  89.06 ( 92.91)	Acc@5  99.22 ( 99.65)
Epoch: [31][200/391]	Time  0.234 ( 0.174)	Data  0.002 ( 0.006)	Loss 2.4647e-01 (2.3243e-01)	Acc@1  93.75 ( 92.97)	Acc@5 100.00 ( 99.66)
Epoch: [31][210/391]	Time  0.149 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.5269e-01 (2.3130e-01)	Acc@1  92.97 ( 93.00)	Acc@5 100.00 ( 99.67)
Epoch: [31][220/391]	Time  0.151 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.7111e-01 (2.3052e-01)	Acc@1  96.09 ( 93.05)	Acc@5 100.00 ( 99.68)
Epoch: [31][230/391]	Time  0.153 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.5681e-01 (2.3045e-01)	Acc@1  91.41 ( 93.01)	Acc@5  99.22 ( 99.68)
Epoch: [31][240/391]	Time  0.151 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.9690e-01 (2.2971e-01)	Acc@1  93.75 ( 93.05)	Acc@5 100.00 ( 99.70)
Epoch: [31][250/391]	Time  0.152 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.4327e-01 (2.2968e-01)	Acc@1  94.53 ( 93.07)	Acc@5  98.44 ( 99.69)
Epoch: [31][260/391]	Time  0.151 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.5258e-01 (2.2900e-01)	Acc@1  92.97 ( 93.09)	Acc@5 100.00 ( 99.69)
Epoch: [31][270/391]	Time  0.151 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.7179e-01 (2.2783e-01)	Acc@1  96.09 ( 93.16)	Acc@5 100.00 ( 99.69)
Epoch: [31][280/391]	Time  0.155 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.1098e-01 (2.2756e-01)	Acc@1  94.53 ( 93.17)	Acc@5 100.00 ( 99.70)
Epoch: [31][290/391]	Time  0.151 ( 0.168)	Data  0.001 ( 0.006)	Loss 2.3368e-01 (2.2701e-01)	Acc@1  94.53 ( 93.17)	Acc@5 100.00 ( 99.70)
Epoch: [31][300/391]	Time  0.149 ( 0.168)	Data  0.001 ( 0.006)	Loss 2.0799e-01 (2.2635e-01)	Acc@1  92.19 ( 93.20)	Acc@5 100.00 ( 99.70)
Epoch: [31][310/391]	Time  0.153 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.2990e-01 (2.2610e-01)	Acc@1  96.88 ( 93.24)	Acc@5 100.00 ( 99.69)
Epoch: [31][320/391]	Time  0.214 ( 0.167)	Data  0.001 ( 0.005)	Loss 2.7516e-01 (2.2602e-01)	Acc@1  90.62 ( 93.22)	Acc@5 100.00 ( 99.70)
Epoch: [31][330/391]	Time  0.215 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.8919e-01 (2.2517e-01)	Acc@1  93.75 ( 93.25)	Acc@5 100.00 ( 99.70)
Epoch: [31][340/391]	Time  0.209 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.3047e-01 (2.2460e-01)	Acc@1  92.97 ( 93.25)	Acc@5  99.22 ( 99.70)
Epoch: [31][350/391]	Time  0.205 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.8419e-01 (2.2455e-01)	Acc@1  89.84 ( 93.27)	Acc@5  99.22 ( 99.70)
Epoch: [31][360/391]	Time  0.186 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.6418e-01 (2.2439e-01)	Acc@1  95.31 ( 93.27)	Acc@5 100.00 ( 99.69)
Epoch: [31][370/391]	Time  0.176 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.9070e-01 (2.2421e-01)	Acc@1  94.53 ( 93.29)	Acc@5 100.00 ( 99.69)
Epoch: [31][380/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.2406e-01 (2.2374e-01)	Acc@1  91.41 ( 93.32)	Acc@5  99.22 ( 99.68)
Epoch: [31][390/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.1413e-01 (2.2393e-01)	Acc@1  92.50 ( 93.29)	Acc@5 100.00 ( 99.68)
## e[31] optimizer.zero_grad (sum) time: 0.34102344512939453
## e[31]       loss.backward (sum) time: 22.33224129676819
## e[31]      optimizer.step (sum) time: 3.3486998081207275
## epoch[31] training(only) time: 67.2766363620758
# Switched to evaluate mode...
Test: [  0/100]	Time  0.240 ( 0.240)	Loss 1.1656e+00 (1.1656e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.059 ( 0.075)	Loss 1.3018e+00 (1.1681e+00)	Acc@1  67.00 ( 72.09)	Acc@5  92.00 ( 91.73)
Test: [ 20/100]	Time  0.057 ( 0.066)	Loss 8.8514e-01 (1.0566e+00)	Acc@1  77.00 ( 74.05)	Acc@5  95.00 ( 93.10)
Test: [ 30/100]	Time  0.058 ( 0.064)	Loss 1.6132e+00 (1.0991e+00)	Acc@1  62.00 ( 73.45)	Acc@5  92.00 ( 93.19)
Test: [ 40/100]	Time  0.058 ( 0.062)	Loss 1.0987e+00 (1.0854e+00)	Acc@1  73.00 ( 73.63)	Acc@5  96.00 ( 93.39)
Test: [ 50/100]	Time  0.060 ( 0.062)	Loss 1.2862e+00 (1.0935e+00)	Acc@1  68.00 ( 73.39)	Acc@5  91.00 ( 93.27)
Test: [ 60/100]	Time  0.058 ( 0.061)	Loss 1.1961e+00 (1.0639e+00)	Acc@1  70.00 ( 73.62)	Acc@5  94.00 ( 93.51)
Test: [ 70/100]	Time  0.058 ( 0.061)	Loss 1.5067e+00 (1.0724e+00)	Acc@1  70.00 ( 73.58)	Acc@5  91.00 ( 93.52)
Test: [ 80/100]	Time  0.058 ( 0.060)	Loss 1.3817e+00 (1.0780e+00)	Acc@1  66.00 ( 73.52)	Acc@5  91.00 ( 93.44)
Test: [ 90/100]	Time  0.058 ( 0.060)	Loss 1.4862e+00 (1.0578e+00)	Acc@1  63.00 ( 73.73)	Acc@5  90.00 ( 93.59)
 * Acc@1 73.940 Acc@5 93.600
### epoch[31] execution time: 73.38467001914978
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.377 ( 0.377)	Data  0.202 ( 0.202)	Loss 1.4338e-01 (1.4338e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.170 ( 0.192)	Data  0.001 ( 0.022)	Loss 1.3323e-01 (1.7982e-01)	Acc@1  97.66 ( 95.45)	Acc@5  99.22 ( 99.79)
Epoch: [32][ 20/391]	Time  0.171 ( 0.184)	Data  0.001 ( 0.014)	Loss 1.1738e-01 (1.8557e-01)	Acc@1  96.88 ( 94.98)	Acc@5 100.00 ( 99.85)
Epoch: [32][ 30/391]	Time  0.177 ( 0.181)	Data  0.002 ( 0.011)	Loss 2.0450e-01 (1.8648e-01)	Acc@1  95.31 ( 95.01)	Acc@5 100.00 ( 99.82)
Epoch: [32][ 40/391]	Time  0.179 ( 0.180)	Data  0.002 ( 0.009)	Loss 2.0369e-01 (1.8875e-01)	Acc@1  95.31 ( 94.91)	Acc@5  99.22 ( 99.81)
Epoch: [32][ 50/391]	Time  0.185 ( 0.180)	Data  0.001 ( 0.008)	Loss 1.7855e-01 (1.8765e-01)	Acc@1  93.75 ( 94.90)	Acc@5 100.00 ( 99.79)
Epoch: [32][ 60/391]	Time  0.169 ( 0.179)	Data  0.002 ( 0.008)	Loss 2.2879e-01 (1.8779e-01)	Acc@1  92.19 ( 94.75)	Acc@5  99.22 ( 99.74)
Epoch: [32][ 70/391]	Time  0.175 ( 0.178)	Data  0.001 ( 0.007)	Loss 2.2897e-01 (1.8663e-01)	Acc@1  94.53 ( 94.83)	Acc@5 100.00 ( 99.77)
Epoch: [32][ 80/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.007)	Loss 1.7829e-01 (1.8647e-01)	Acc@1  96.88 ( 94.83)	Acc@5 100.00 ( 99.79)
Epoch: [32][ 90/391]	Time  0.169 ( 0.176)	Data  0.002 ( 0.007)	Loss 1.5399e-01 (1.8398e-01)	Acc@1  96.09 ( 94.91)	Acc@5 100.00 ( 99.79)
Epoch: [32][100/391]	Time  0.175 ( 0.176)	Data  0.002 ( 0.006)	Loss 2.1241e-01 (1.8524e-01)	Acc@1  91.41 ( 94.78)	Acc@5 100.00 ( 99.77)
Epoch: [32][110/391]	Time  0.177 ( 0.176)	Data  0.001 ( 0.006)	Loss 2.1449e-01 (1.8489e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.77)
Epoch: [32][120/391]	Time  0.175 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.4756e-01 (1.8397e-01)	Acc@1  96.09 ( 94.88)	Acc@5 100.00 ( 99.77)
Epoch: [32][130/391]	Time  0.173 ( 0.175)	Data  0.001 ( 0.006)	Loss 3.2044e-01 (1.8512e-01)	Acc@1  89.84 ( 94.81)	Acc@5  98.44 ( 99.76)
Epoch: [32][140/391]	Time  0.174 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.4769e-01 (1.8637e-01)	Acc@1  90.62 ( 94.75)	Acc@5  99.22 ( 99.75)
Epoch: [32][150/391]	Time  0.166 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.3675e-01 (1.8472e-01)	Acc@1  96.09 ( 94.78)	Acc@5 100.00 ( 99.76)
Epoch: [32][160/391]	Time  0.174 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.7317e-01 (1.8494e-01)	Acc@1  93.75 ( 94.78)	Acc@5 100.00 ( 99.76)
Epoch: [32][170/391]	Time  0.171 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.8168e-01 (1.8532e-01)	Acc@1  92.97 ( 94.76)	Acc@5 100.00 ( 99.75)
Epoch: [32][180/391]	Time  0.183 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.8708e-01 (1.8513e-01)	Acc@1  94.53 ( 94.76)	Acc@5 100.00 ( 99.75)
Epoch: [32][190/391]	Time  0.185 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.4908e-01 (1.8441e-01)	Acc@1  94.53 ( 94.77)	Acc@5 100.00 ( 99.76)
Epoch: [32][200/391]	Time  0.183 ( 0.176)	Data  0.001 ( 0.005)	Loss 1.8243e-01 (1.8434e-01)	Acc@1  96.09 ( 94.78)	Acc@5  99.22 ( 99.75)
Epoch: [32][210/391]	Time  0.169 ( 0.176)	Data  0.001 ( 0.005)	Loss 1.3253e-01 (1.8296e-01)	Acc@1  95.31 ( 94.80)	Acc@5 100.00 ( 99.76)
Epoch: [32][220/391]	Time  0.176 ( 0.176)	Data  0.001 ( 0.005)	Loss 1.1625e-01 (1.8414e-01)	Acc@1  96.09 ( 94.79)	Acc@5 100.00 ( 99.76)
Epoch: [32][230/391]	Time  0.173 ( 0.176)	Data  0.002 ( 0.005)	Loss 2.0381e-01 (1.8442e-01)	Acc@1  94.53 ( 94.77)	Acc@5  99.22 ( 99.76)
Epoch: [32][240/391]	Time  0.172 ( 0.176)	Data  0.002 ( 0.005)	Loss 2.1317e-01 (1.8525e-01)	Acc@1  95.31 ( 94.75)	Acc@5  99.22 ( 99.74)
Epoch: [32][250/391]	Time  0.175 ( 0.176)	Data  0.001 ( 0.005)	Loss 2.5709e-01 (1.8498e-01)	Acc@1  92.19 ( 94.76)	Acc@5 100.00 ( 99.75)
Epoch: [32][260/391]	Time  0.170 ( 0.176)	Data  0.001 ( 0.005)	Loss 1.3557e-01 (1.8459e-01)	Acc@1  96.88 ( 94.77)	Acc@5 100.00 ( 99.75)
Epoch: [32][270/391]	Time  0.165 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.5664e-01 (1.8476e-01)	Acc@1  96.09 ( 94.76)	Acc@5 100.00 ( 99.75)
Epoch: [32][280/391]	Time  0.167 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.4604e-01 (1.8436e-01)	Acc@1  92.19 ( 94.77)	Acc@5  98.44 ( 99.75)
Epoch: [32][290/391]	Time  0.180 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.8045e-01 (1.8370e-01)	Acc@1  94.53 ( 94.76)	Acc@5  99.22 ( 99.75)
Epoch: [32][300/391]	Time  0.177 ( 0.176)	Data  0.002 ( 0.005)	Loss 1.0021e-01 (1.8415e-01)	Acc@1  98.44 ( 94.79)	Acc@5 100.00 ( 99.74)
Epoch: [32][310/391]	Time  0.182 ( 0.176)	Data  0.001 ( 0.005)	Loss 1.5268e-01 (1.8362e-01)	Acc@1  97.66 ( 94.82)	Acc@5 100.00 ( 99.74)
Epoch: [32][320/391]	Time  0.169 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.5653e-01 (1.8328e-01)	Acc@1  92.97 ( 94.82)	Acc@5  99.22 ( 99.74)
Epoch: [32][330/391]	Time  0.165 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.6054e-01 (1.8327e-01)	Acc@1  94.53 ( 94.81)	Acc@5 100.00 ( 99.75)
Epoch: [32][340/391]	Time  0.157 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.0199e-01 (1.8318e-01)	Acc@1  91.41 ( 94.80)	Acc@5  99.22 ( 99.75)
Epoch: [32][350/391]	Time  0.171 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.1928e-01 (1.8327e-01)	Acc@1  97.66 ( 94.78)	Acc@5 100.00 ( 99.74)
Epoch: [32][360/391]	Time  0.170 ( 0.175)	Data  0.002 ( 0.005)	Loss 2.1008e-01 (1.8366e-01)	Acc@1  94.53 ( 94.77)	Acc@5 100.00 ( 99.74)
Epoch: [32][370/391]	Time  0.168 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.3052e-01 (1.8289e-01)	Acc@1  95.31 ( 94.79)	Acc@5 100.00 ( 99.74)
Epoch: [32][380/391]	Time  0.165 ( 0.175)	Data  0.002 ( 0.005)	Loss 2.2124e-01 (1.8337e-01)	Acc@1  91.41 ( 94.75)	Acc@5  99.22 ( 99.74)
Epoch: [32][390/391]	Time  0.177 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.5539e-01 (1.8298e-01)	Acc@1  92.50 ( 94.78)	Acc@5 100.00 ( 99.75)
## e[32] optimizer.zero_grad (sum) time: 0.40767359733581543
## e[32]       loss.backward (sum) time: 26.67641520500183
## e[32]      optimizer.step (sum) time: 3.93603777885437
## epoch[32] training(only) time: 68.40128922462463
# Switched to evaluate mode...
Test: [  0/100]	Time  0.254 ( 0.254)	Loss 1.1432e+00 (1.1432e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.059 ( 0.078)	Loss 1.3765e+00 (1.1706e+00)	Acc@1  68.00 ( 72.73)	Acc@5  92.00 ( 92.55)
Test: [ 20/100]	Time  0.060 ( 0.069)	Loss 9.0778e-01 (1.0687e+00)	Acc@1  78.00 ( 74.62)	Acc@5  95.00 ( 93.29)
Test: [ 30/100]	Time  0.060 ( 0.066)	Loss 1.6656e+00 (1.1186e+00)	Acc@1  61.00 ( 73.68)	Acc@5  91.00 ( 93.13)
Test: [ 40/100]	Time  0.055 ( 0.064)	Loss 1.1016e+00 (1.1052e+00)	Acc@1  76.00 ( 73.80)	Acc@5  97.00 ( 93.51)
Test: [ 50/100]	Time  0.056 ( 0.063)	Loss 1.3129e+00 (1.1121e+00)	Acc@1  69.00 ( 73.49)	Acc@5  92.00 ( 93.35)
Test: [ 60/100]	Time  0.056 ( 0.062)	Loss 1.2236e+00 (1.0776e+00)	Acc@1  70.00 ( 73.98)	Acc@5  92.00 ( 93.52)
Test: [ 70/100]	Time  0.056 ( 0.061)	Loss 1.6083e+00 (1.0881e+00)	Acc@1  67.00 ( 73.83)	Acc@5  88.00 ( 93.42)
Test: [ 80/100]	Time  0.057 ( 0.061)	Loss 1.3858e+00 (1.0936e+00)	Acc@1  66.00 ( 73.63)	Acc@5  92.00 ( 93.42)
Test: [ 90/100]	Time  0.056 ( 0.060)	Loss 1.4936e+00 (1.0768e+00)	Acc@1  64.00 ( 73.82)	Acc@5  90.00 ( 93.55)
 * Acc@1 74.070 Acc@5 93.540
### epoch[32] execution time: 74.52176427841187
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.372 ( 0.372)	Data  0.202 ( 0.202)	Loss 1.7689e-01 (1.7689e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.176 ( 0.194)	Data  0.001 ( 0.022)	Loss 1.1206e-01 (1.5714e-01)	Acc@1  97.66 ( 95.95)	Acc@5 100.00 ( 99.86)
Epoch: [33][ 20/391]	Time  0.181 ( 0.187)	Data  0.001 ( 0.014)	Loss 2.4788e-01 (1.6792e-01)	Acc@1  92.19 ( 95.24)	Acc@5 100.00 ( 99.78)
Epoch: [33][ 30/391]	Time  0.181 ( 0.184)	Data  0.001 ( 0.011)	Loss 1.0198e-01 (1.6469e-01)	Acc@1  96.88 ( 95.16)	Acc@5 100.00 ( 99.80)
Epoch: [33][ 40/391]	Time  0.167 ( 0.181)	Data  0.001 ( 0.009)	Loss 1.2399e-01 (1.6098e-01)	Acc@1  96.09 ( 95.31)	Acc@5 100.00 ( 99.83)
Epoch: [33][ 50/391]	Time  0.167 ( 0.179)	Data  0.001 ( 0.008)	Loss 2.0242e-01 (1.5660e-01)	Acc@1  92.19 ( 95.40)	Acc@5 100.00 ( 99.85)
Epoch: [33][ 60/391]	Time  0.175 ( 0.177)	Data  0.001 ( 0.008)	Loss 1.6136e-01 (1.5832e-01)	Acc@1  96.88 ( 95.33)	Acc@5 100.00 ( 99.85)
Epoch: [33][ 70/391]	Time  0.180 ( 0.178)	Data  0.001 ( 0.007)	Loss 2.0772e-01 (1.5877e-01)	Acc@1  94.53 ( 95.28)	Acc@5 100.00 ( 99.86)
Epoch: [33][ 80/391]	Time  0.180 ( 0.178)	Data  0.001 ( 0.007)	Loss 1.7462e-01 (1.5724e-01)	Acc@1  96.09 ( 95.36)	Acc@5 100.00 ( 99.85)
Epoch: [33][ 90/391]	Time  0.177 ( 0.178)	Data  0.001 ( 0.007)	Loss 2.1094e-01 (1.5803e-01)	Acc@1  92.97 ( 95.32)	Acc@5  99.22 ( 99.83)
Epoch: [33][100/391]	Time  0.176 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.5063e-01 (1.5657e-01)	Acc@1  96.09 ( 95.37)	Acc@5 100.00 ( 99.84)
Epoch: [33][110/391]	Time  0.181 ( 0.177)	Data  0.001 ( 0.006)	Loss 1.4093e-01 (1.5651e-01)	Acc@1  96.88 ( 95.40)	Acc@5  99.22 ( 99.82)
Epoch: [33][120/391]	Time  0.182 ( 0.178)	Data  0.002 ( 0.006)	Loss 2.2308e-01 (1.5674e-01)	Acc@1  92.97 ( 95.37)	Acc@5  99.22 ( 99.83)
Epoch: [33][130/391]	Time  0.181 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.5288e-01 (1.5664e-01)	Acc@1  93.75 ( 95.37)	Acc@5 100.00 ( 99.83)
Epoch: [33][140/391]	Time  0.167 ( 0.177)	Data  0.002 ( 0.006)	Loss 1.2428e-01 (1.5667e-01)	Acc@1  97.66 ( 95.37)	Acc@5  99.22 ( 99.82)
Epoch: [33][150/391]	Time  0.167 ( 0.177)	Data  0.001 ( 0.006)	Loss 2.0580e-01 (1.5691e-01)	Acc@1  92.97 ( 95.35)	Acc@5  99.22 ( 99.82)
Epoch: [33][160/391]	Time  0.168 ( 0.176)	Data  0.001 ( 0.006)	Loss 2.1483e-01 (1.5767e-01)	Acc@1  93.75 ( 95.35)	Acc@5 100.00 ( 99.82)
Epoch: [33][170/391]	Time  0.173 ( 0.176)	Data  0.002 ( 0.005)	Loss 1.1818e-01 (1.5775e-01)	Acc@1  96.88 ( 95.37)	Acc@5 100.00 ( 99.81)
Epoch: [33][180/391]	Time  0.171 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.7063e-01 (1.5846e-01)	Acc@1  94.53 ( 95.36)	Acc@5  99.22 ( 99.81)
Epoch: [33][190/391]	Time  0.176 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.4872e-01 (1.5857e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.81)
Epoch: [33][200/391]	Time  0.190 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.7130e-01 (1.5812e-01)	Acc@1  97.66 ( 95.44)	Acc@5 100.00 ( 99.82)
Epoch: [33][210/391]	Time  0.163 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.5774e-01 (1.5806e-01)	Acc@1  96.09 ( 95.45)	Acc@5 100.00 ( 99.83)
Epoch: [33][220/391]	Time  0.166 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.2353e-01 (1.5796e-01)	Acc@1  96.88 ( 95.46)	Acc@5 100.00 ( 99.82)
Epoch: [33][230/391]	Time  0.176 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.2323e-01 (1.5837e-01)	Acc@1  89.06 ( 95.44)	Acc@5 100.00 ( 99.82)
Epoch: [33][240/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.4847e-01 (1.5886e-01)	Acc@1  92.97 ( 95.44)	Acc@5  99.22 ( 99.81)
Epoch: [33][250/391]	Time  0.176 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.7420e-01 (1.5933e-01)	Acc@1  93.75 ( 95.42)	Acc@5 100.00 ( 99.81)
Epoch: [33][260/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.4793e-01 (1.5984e-01)	Acc@1  93.75 ( 95.41)	Acc@5 100.00 ( 99.81)
Epoch: [33][270/391]	Time  0.181 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.2557e-01 (1.5982e-01)	Acc@1  96.88 ( 95.42)	Acc@5 100.00 ( 99.82)
Epoch: [33][280/391]	Time  0.187 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.8778e-01 (1.6002e-01)	Acc@1  94.53 ( 95.42)	Acc@5 100.00 ( 99.82)
Epoch: [33][290/391]	Time  0.164 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.7080e-01 (1.5975e-01)	Acc@1  94.53 ( 95.43)	Acc@5 100.00 ( 99.82)
Epoch: [33][300/391]	Time  0.169 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.2979e-01 (1.5967e-01)	Acc@1  97.66 ( 95.42)	Acc@5 100.00 ( 99.82)
Epoch: [33][310/391]	Time  0.161 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.3122e-01 (1.5926e-01)	Acc@1  96.88 ( 95.42)	Acc@5 100.00 ( 99.83)
Epoch: [33][320/391]	Time  0.186 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.2567e-01 (1.5975e-01)	Acc@1  95.31 ( 95.42)	Acc@5 100.00 ( 99.82)
Epoch: [33][330/391]	Time  0.180 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.2463e-01 (1.5982e-01)	Acc@1  95.31 ( 95.42)	Acc@5 100.00 ( 99.83)
Epoch: [33][340/391]	Time  0.168 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.2390e-01 (1.5965e-01)	Acc@1  97.66 ( 95.42)	Acc@5 100.00 ( 99.83)
Epoch: [33][350/391]	Time  0.167 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.0529e-01 (1.6021e-01)	Acc@1  97.66 ( 95.39)	Acc@5 100.00 ( 99.82)
Epoch: [33][360/391]	Time  0.168 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.4649e-01 (1.6000e-01)	Acc@1  96.09 ( 95.39)	Acc@5 100.00 ( 99.82)
Epoch: [33][370/391]	Time  0.176 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.4579e-01 (1.6036e-01)	Acc@1  95.31 ( 95.37)	Acc@5 100.00 ( 99.82)
Epoch: [33][380/391]	Time  0.174 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.0305e-01 (1.6084e-01)	Acc@1  95.31 ( 95.33)	Acc@5  99.22 ( 99.82)
Epoch: [33][390/391]	Time  0.175 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.5928e-01 (1.6088e-01)	Acc@1  97.50 ( 95.33)	Acc@5 100.00 ( 99.82)
## e[33] optimizer.zero_grad (sum) time: 0.4106025695800781
## e[33]       loss.backward (sum) time: 26.820018768310547
## e[33]      optimizer.step (sum) time: 3.969099283218384
## epoch[33] training(only) time: 68.49268555641174
# Switched to evaluate mode...
Test: [  0/100]	Time  0.305 ( 0.305)	Loss 1.1976e+00 (1.1976e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.060 ( 0.083)	Loss 1.3932e+00 (1.1967e+00)	Acc@1  69.00 ( 72.45)	Acc@5  91.00 ( 92.09)
Test: [ 20/100]	Time  0.062 ( 0.072)	Loss 8.9290e-01 (1.0805e+00)	Acc@1  77.00 ( 74.29)	Acc@5  96.00 ( 93.52)
Test: [ 30/100]	Time  0.059 ( 0.068)	Loss 1.6688e+00 (1.1260e+00)	Acc@1  61.00 ( 73.35)	Acc@5  92.00 ( 93.39)
Test: [ 40/100]	Time  0.060 ( 0.066)	Loss 1.0989e+00 (1.1129e+00)	Acc@1  75.00 ( 73.54)	Acc@5  97.00 ( 93.73)
Test: [ 50/100]	Time  0.058 ( 0.065)	Loss 1.2696e+00 (1.1190e+00)	Acc@1  68.00 ( 73.18)	Acc@5  92.00 ( 93.47)
Test: [ 60/100]	Time  0.059 ( 0.064)	Loss 1.2162e+00 (1.0842e+00)	Acc@1  68.00 ( 73.64)	Acc@5  94.00 ( 93.72)
Test: [ 70/100]	Time  0.059 ( 0.063)	Loss 1.5760e+00 (1.0952e+00)	Acc@1  69.00 ( 73.61)	Acc@5  89.00 ( 93.52)
Test: [ 80/100]	Time  0.054 ( 0.062)	Loss 1.3916e+00 (1.1001e+00)	Acc@1  66.00 ( 73.49)	Acc@5  92.00 ( 93.42)
Test: [ 90/100]	Time  0.060 ( 0.061)	Loss 1.5396e+00 (1.0836e+00)	Acc@1  64.00 ( 73.67)	Acc@5  91.00 ( 93.55)
 * Acc@1 73.870 Acc@5 93.510
### epoch[33] execution time: 74.71159243583679
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.416 ( 0.416)	Data  0.232 ( 0.232)	Loss 1.8314e-01 (1.8314e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [34][ 10/391]	Time  0.175 ( 0.196)	Data  0.001 ( 0.025)	Loss 1.7760e-01 (1.4361e-01)	Acc@1  95.31 ( 95.95)	Acc@5  99.22 ( 99.86)
Epoch: [34][ 20/391]	Time  0.166 ( 0.183)	Data  0.001 ( 0.015)	Loss 9.5018e-02 (1.4396e-01)	Acc@1 100.00 ( 96.09)	Acc@5 100.00 ( 99.81)
Epoch: [34][ 30/391]	Time  0.177 ( 0.180)	Data  0.001 ( 0.011)	Loss 1.4019e-01 (1.3979e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.85)
Epoch: [34][ 40/391]	Time  0.182 ( 0.178)	Data  0.001 ( 0.010)	Loss 1.5206e-01 (1.3890e-01)	Acc@1  94.53 ( 96.38)	Acc@5 100.00 ( 99.85)
Epoch: [34][ 50/391]	Time  0.174 ( 0.177)	Data  0.001 ( 0.009)	Loss 9.6485e-02 (1.3559e-01)	Acc@1  98.44 ( 96.46)	Acc@5 100.00 ( 99.86)
Epoch: [34][ 60/391]	Time  0.164 ( 0.176)	Data  0.002 ( 0.008)	Loss 1.6124e-01 (1.3873e-01)	Acc@1  96.88 ( 96.38)	Acc@5 100.00 ( 99.87)
Epoch: [34][ 70/391]	Time  0.172 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.1364e-01 (1.3867e-01)	Acc@1  98.44 ( 96.35)	Acc@5 100.00 ( 99.88)
Epoch: [34][ 80/391]	Time  0.168 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.5228e-01 (1.4083e-01)	Acc@1  97.66 ( 96.25)	Acc@5  99.22 ( 99.86)
Epoch: [34][ 90/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.007)	Loss 1.1115e-01 (1.3791e-01)	Acc@1  98.44 ( 96.33)	Acc@5 100.00 ( 99.87)
Epoch: [34][100/391]	Time  0.184 ( 0.173)	Data  0.001 ( 0.006)	Loss 9.9362e-02 (1.3710e-01)	Acc@1  98.44 ( 96.36)	Acc@5 100.00 ( 99.88)
Epoch: [34][110/391]	Time  0.180 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.4019e-01 (1.3903e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.89)
Epoch: [34][120/391]	Time  0.172 ( 0.174)	Data  0.002 ( 0.006)	Loss 1.0573e-01 (1.3969e-01)	Acc@1  97.66 ( 96.26)	Acc@5 100.00 ( 99.88)
Epoch: [34][130/391]	Time  0.168 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.9316e-01 (1.3881e-01)	Acc@1  95.31 ( 96.28)	Acc@5 100.00 ( 99.89)
Epoch: [34][140/391]	Time  0.167 ( 0.174)	Data  0.002 ( 0.006)	Loss 9.2797e-02 (1.3922e-01)	Acc@1  98.44 ( 96.29)	Acc@5 100.00 ( 99.89)
Epoch: [34][150/391]	Time  0.173 ( 0.174)	Data  0.002 ( 0.006)	Loss 1.4676e-01 (1.4092e-01)	Acc@1  95.31 ( 96.22)	Acc@5 100.00 ( 99.88)
Epoch: [34][160/391]	Time  0.169 ( 0.174)	Data  0.002 ( 0.006)	Loss 1.5769e-01 (1.4170e-01)	Acc@1  96.09 ( 96.19)	Acc@5  99.22 ( 99.86)
Epoch: [34][170/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.006)	Loss 9.6850e-02 (1.4162e-01)	Acc@1  98.44 ( 96.17)	Acc@5 100.00 ( 99.86)
Epoch: [34][180/391]	Time  0.166 ( 0.174)	Data  0.002 ( 0.006)	Loss 1.2404e-01 (1.4182e-01)	Acc@1  96.88 ( 96.16)	Acc@5 100.00 ( 99.86)
Epoch: [34][190/391]	Time  0.170 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.4558e-01 (1.4191e-01)	Acc@1  95.31 ( 96.15)	Acc@5 100.00 ( 99.86)
Epoch: [34][200/391]	Time  0.173 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.5156e-01 (1.4256e-01)	Acc@1  95.31 ( 96.11)	Acc@5 100.00 ( 99.85)
Epoch: [34][210/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.7001e-01 (1.4373e-01)	Acc@1  94.53 ( 96.08)	Acc@5  99.22 ( 99.84)
Epoch: [34][220/391]	Time  0.179 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.4188e-01 (1.4409e-01)	Acc@1  96.09 ( 96.06)	Acc@5 100.00 ( 99.85)
Epoch: [34][230/391]	Time  0.163 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.5023e-01 (1.4420e-01)	Acc@1  96.88 ( 96.06)	Acc@5  99.22 ( 99.85)
Epoch: [34][240/391]	Time  0.168 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.3221e-01 (1.4279e-01)	Acc@1  98.44 ( 96.14)	Acc@5  99.22 ( 99.85)
Epoch: [34][250/391]	Time  0.161 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.5017e-01 (1.4272e-01)	Acc@1  96.09 ( 96.14)	Acc@5 100.00 ( 99.85)
Epoch: [34][260/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.1847e-01 (1.4239e-01)	Acc@1  96.88 ( 96.13)	Acc@5 100.00 ( 99.86)
Epoch: [34][270/391]	Time  0.177 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.1561e-01 (1.4250e-01)	Acc@1  96.88 ( 96.14)	Acc@5 100.00 ( 99.85)
Epoch: [34][280/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.7199e-01 (1.4207e-01)	Acc@1  92.97 ( 96.15)	Acc@5 100.00 ( 99.85)
Epoch: [34][290/391]	Time  0.162 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.0143e-01 (1.4201e-01)	Acc@1  95.31 ( 96.15)	Acc@5  99.22 ( 99.85)
Epoch: [34][300/391]	Time  0.183 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.4483e-01 (1.4229e-01)	Acc@1  96.09 ( 96.12)	Acc@5 100.00 ( 99.85)
Epoch: [34][310/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.1564e-01 (1.4235e-01)	Acc@1  96.88 ( 96.11)	Acc@5 100.00 ( 99.84)
Epoch: [34][320/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.6591e-01 (1.4220e-01)	Acc@1  95.31 ( 96.13)	Acc@5 100.00 ( 99.85)
Epoch: [34][330/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 8.8554e-02 (1.4236e-01)	Acc@1  96.88 ( 96.12)	Acc@5 100.00 ( 99.84)
Epoch: [34][340/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.9506e-01 (1.4216e-01)	Acc@1  93.75 ( 96.13)	Acc@5 100.00 ( 99.84)
Epoch: [34][350/391]	Time  0.184 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0205e-01 (1.4204e-01)	Acc@1  97.66 ( 96.12)	Acc@5 100.00 ( 99.84)
Epoch: [34][360/391]	Time  0.174 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.4118e-01 (1.4174e-01)	Acc@1  96.88 ( 96.13)	Acc@5  99.22 ( 99.85)
Epoch: [34][370/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.1952e-01 (1.4212e-01)	Acc@1  98.44 ( 96.12)	Acc@5 100.00 ( 99.85)
Epoch: [34][380/391]	Time  0.167 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.0689e-01 (1.4193e-01)	Acc@1  96.09 ( 96.11)	Acc@5 100.00 ( 99.85)
Epoch: [34][390/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5392e-01 (1.4260e-01)	Acc@1  96.25 ( 96.09)	Acc@5 100.00 ( 99.84)
## e[34] optimizer.zero_grad (sum) time: 0.40906643867492676
## e[34]       loss.backward (sum) time: 26.76205849647522
## e[34]      optimizer.step (sum) time: 3.902444362640381
## epoch[34] training(only) time: 67.77779197692871
# Switched to evaluate mode...
Test: [  0/100]	Time  0.248 ( 0.248)	Loss 1.2249e+00 (1.2249e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.060 ( 0.078)	Loss 1.3450e+00 (1.2191e+00)	Acc@1  70.00 ( 72.73)	Acc@5  92.00 ( 92.09)
Test: [ 20/100]	Time  0.062 ( 0.069)	Loss 9.2365e-01 (1.0963e+00)	Acc@1  78.00 ( 74.43)	Acc@5  97.00 ( 93.43)
Test: [ 30/100]	Time  0.060 ( 0.066)	Loss 1.7098e+00 (1.1448e+00)	Acc@1  58.00 ( 73.71)	Acc@5  92.00 ( 93.32)
Test: [ 40/100]	Time  0.058 ( 0.064)	Loss 1.1756e+00 (1.1333e+00)	Acc@1  72.00 ( 73.85)	Acc@5  97.00 ( 93.61)
Test: [ 50/100]	Time  0.058 ( 0.063)	Loss 1.3682e+00 (1.1406e+00)	Acc@1  69.00 ( 73.61)	Acc@5  93.00 ( 93.49)
Test: [ 60/100]	Time  0.059 ( 0.063)	Loss 1.2777e+00 (1.1073e+00)	Acc@1  67.00 ( 73.93)	Acc@5  92.00 ( 93.62)
Test: [ 70/100]	Time  0.059 ( 0.062)	Loss 1.6242e+00 (1.1184e+00)	Acc@1  67.00 ( 73.79)	Acc@5  89.00 ( 93.51)
Test: [ 80/100]	Time  0.060 ( 0.062)	Loss 1.4670e+00 (1.1245e+00)	Acc@1  67.00 ( 73.70)	Acc@5  90.00 ( 93.47)
Test: [ 90/100]	Time  0.059 ( 0.062)	Loss 1.5904e+00 (1.1069e+00)	Acc@1  62.00 ( 73.80)	Acc@5  90.00 ( 93.57)
 * Acc@1 74.010 Acc@5 93.590
### epoch[34] execution time: 74.02193832397461
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.414 ( 0.414)	Data  0.232 ( 0.232)	Loss 2.0312e-01 (2.0312e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.164 ( 0.192)	Data  0.001 ( 0.024)	Loss 1.1362e-01 (1.4157e-01)	Acc@1  96.09 ( 95.45)	Acc@5 100.00 ( 99.79)
Epoch: [35][ 20/391]	Time  0.176 ( 0.183)	Data  0.001 ( 0.015)	Loss 1.0949e-01 (1.3189e-01)	Acc@1  97.66 ( 95.98)	Acc@5 100.00 ( 99.85)
Epoch: [35][ 30/391]	Time  0.172 ( 0.181)	Data  0.001 ( 0.012)	Loss 1.0541e-01 (1.2756e-01)	Acc@1  96.88 ( 96.40)	Acc@5 100.00 ( 99.85)
Epoch: [35][ 40/391]	Time  0.162 ( 0.179)	Data  0.001 ( 0.010)	Loss 1.3385e-01 (1.2624e-01)	Acc@1  96.09 ( 96.42)	Acc@5 100.00 ( 99.85)
Epoch: [35][ 50/391]	Time  0.170 ( 0.178)	Data  0.001 ( 0.009)	Loss 1.8516e-01 (1.2835e-01)	Acc@1  91.41 ( 96.38)	Acc@5  99.22 ( 99.86)
Epoch: [35][ 60/391]	Time  0.164 ( 0.176)	Data  0.001 ( 0.008)	Loss 1.8450e-01 (1.2593e-01)	Acc@1  96.09 ( 96.57)	Acc@5  99.22 ( 99.86)
Epoch: [35][ 70/391]	Time  0.152 ( 0.174)	Data  0.001 ( 0.007)	Loss 9.6797e-02 (1.2505e-01)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.87)
Epoch: [35][ 80/391]	Time  0.166 ( 0.173)	Data  0.002 ( 0.007)	Loss 1.2500e-01 (1.2665e-01)	Acc@1  96.88 ( 96.62)	Acc@5 100.00 ( 99.87)
Epoch: [35][ 90/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.007)	Loss 1.8255e-01 (1.2828e-01)	Acc@1  92.97 ( 96.52)	Acc@5 100.00 ( 99.86)
Epoch: [35][100/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.2857e-01 (1.2635e-01)	Acc@1  96.09 ( 96.60)	Acc@5 100.00 ( 99.88)
Epoch: [35][110/391]	Time  0.172 ( 0.173)	Data  0.002 ( 0.006)	Loss 2.2391e-01 (1.2775e-01)	Acc@1  93.75 ( 96.54)	Acc@5  98.44 ( 99.87)
Epoch: [35][120/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.006)	Loss 7.4270e-02 (1.2772e-01)	Acc@1  98.44 ( 96.55)	Acc@5 100.00 ( 99.88)
Epoch: [35][130/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.006)	Loss 9.6554e-02 (1.2852e-01)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.89)
Epoch: [35][140/391]	Time  0.177 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.2546e-01 (1.2782e-01)	Acc@1  97.66 ( 96.51)	Acc@5  99.22 ( 99.89)
Epoch: [35][150/391]	Time  0.158 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.0237e-01 (1.2805e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.89)
Epoch: [35][160/391]	Time  0.169 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.7621e-01 (1.2818e-01)	Acc@1  93.75 ( 96.51)	Acc@5 100.00 ( 99.89)
Epoch: [35][170/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2683e-01 (1.2834e-01)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 ( 99.90)
Epoch: [35][180/391]	Time  0.186 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.4022e-02 (1.2780e-01)	Acc@1  97.66 ( 96.55)	Acc@5 100.00 ( 99.91)
Epoch: [35][190/391]	Time  0.176 ( 0.172)	Data  0.001 ( 0.005)	Loss 7.7432e-02 (1.2812e-01)	Acc@1  99.22 ( 96.54)	Acc@5 100.00 ( 99.90)
Epoch: [35][200/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5038e-01 (1.2839e-01)	Acc@1  93.75 ( 96.53)	Acc@5 100.00 ( 99.90)
Epoch: [35][210/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.5434e-01 (1.2855e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.89)
Epoch: [35][220/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.5748e-01 (1.2863e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.89)
Epoch: [35][230/391]	Time  0.182 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1192e-01 (1.2838e-01)	Acc@1  96.88 ( 96.51)	Acc@5 100.00 ( 99.90)
Epoch: [35][240/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5867e-01 (1.2894e-01)	Acc@1  94.53 ( 96.49)	Acc@5 100.00 ( 99.90)
Epoch: [35][250/391]	Time  0.181 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0142e-01 (1.2794e-01)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.90)
Epoch: [35][260/391]	Time  0.164 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.0471e-01 (1.2762e-01)	Acc@1  97.66 ( 96.53)	Acc@5 100.00 ( 99.90)
Epoch: [35][270/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5865e-01 (1.2759e-01)	Acc@1  93.75 ( 96.54)	Acc@5 100.00 ( 99.90)
Epoch: [35][280/391]	Time  0.172 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.7979e-01 (1.2800e-01)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.90)
Epoch: [35][290/391]	Time  0.182 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0390e-01 (1.2805e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.90)
Epoch: [35][300/391]	Time  0.181 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0120e-01 (1.2796e-01)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.90)
Epoch: [35][310/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.7135e-01 (1.2814e-01)	Acc@1  94.53 ( 96.49)	Acc@5 100.00 ( 99.90)
Epoch: [35][320/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5133e-01 (1.2760e-01)	Acc@1  94.53 ( 96.50)	Acc@5 100.00 ( 99.91)
Epoch: [35][330/391]	Time  0.164 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.2640e-01 (1.2755e-01)	Acc@1  96.88 ( 96.50)	Acc@5 100.00 ( 99.91)
Epoch: [35][340/391]	Time  0.168 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.2107e-01 (1.2757e-01)	Acc@1  94.53 ( 96.48)	Acc@5 100.00 ( 99.90)
Epoch: [35][350/391]	Time  0.182 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0062e-01 (1.2725e-01)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.90)
Epoch: [35][360/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2385e-01 (1.2658e-01)	Acc@1  94.53 ( 96.51)	Acc@5 100.00 ( 99.90)
Epoch: [35][370/391]	Time  0.171 ( 0.173)	Data  0.002 ( 0.005)	Loss 9.8504e-02 (1.2647e-01)	Acc@1  98.44 ( 96.53)	Acc@5 100.00 ( 99.90)
Epoch: [35][380/391]	Time  0.180 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2545e-01 (1.2654e-01)	Acc@1  97.66 ( 96.53)	Acc@5  99.22 ( 99.89)
Epoch: [35][390/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.1057e-01 (1.2674e-01)	Acc@1  96.25 ( 96.51)	Acc@5 100.00 ( 99.89)
## e[35] optimizer.zero_grad (sum) time: 0.4031968116760254
## e[35]       loss.backward (sum) time: 26.236366033554077
## e[35]      optimizer.step (sum) time: 3.851945400238037
## epoch[35] training(only) time: 67.69867610931396
# Switched to evaluate mode...
Test: [  0/100]	Time  0.238 ( 0.238)	Loss 1.2207e+00 (1.2207e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.056 ( 0.073)	Loss 1.4042e+00 (1.2260e+00)	Acc@1  71.00 ( 72.82)	Acc@5  92.00 ( 91.91)
Test: [ 20/100]	Time  0.056 ( 0.066)	Loss 9.0967e-01 (1.1113e+00)	Acc@1  75.00 ( 74.95)	Acc@5  95.00 ( 93.19)
Test: [ 30/100]	Time  0.055 ( 0.062)	Loss 1.7892e+00 (1.1642e+00)	Acc@1  61.00 ( 74.13)	Acc@5  93.00 ( 93.29)
Test: [ 40/100]	Time  0.056 ( 0.060)	Loss 1.1423e+00 (1.1474e+00)	Acc@1  75.00 ( 74.12)	Acc@5  96.00 ( 93.54)
Test: [ 50/100]	Time  0.056 ( 0.059)	Loss 1.2980e+00 (1.1537e+00)	Acc@1  70.00 ( 73.78)	Acc@5  92.00 ( 93.31)
Test: [ 60/100]	Time  0.055 ( 0.059)	Loss 1.2861e+00 (1.1199e+00)	Acc@1  69.00 ( 74.03)	Acc@5  92.00 ( 93.49)
Test: [ 70/100]	Time  0.054 ( 0.058)	Loss 1.6055e+00 (1.1308e+00)	Acc@1  70.00 ( 73.93)	Acc@5  88.00 ( 93.46)
Test: [ 80/100]	Time  0.053 ( 0.058)	Loss 1.4640e+00 (1.1353e+00)	Acc@1  66.00 ( 73.84)	Acc@5  91.00 ( 93.35)
Test: [ 90/100]	Time  0.059 ( 0.057)	Loss 1.5802e+00 (1.1168e+00)	Acc@1  65.00 ( 73.99)	Acc@5  91.00 ( 93.46)
 * Acc@1 74.210 Acc@5 93.460
### epoch[35] execution time: 73.56919527053833
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.378 ( 0.378)	Data  0.207 ( 0.207)	Loss 8.7011e-02 (8.7011e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.179 ( 0.200)	Data  0.001 ( 0.023)	Loss 1.0142e-01 (1.0106e-01)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 20/391]	Time  0.168 ( 0.189)	Data  0.001 ( 0.014)	Loss 9.7396e-02 (1.1287e-01)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 30/391]	Time  0.163 ( 0.182)	Data  0.001 ( 0.011)	Loss 7.2679e-02 (1.1201e-01)	Acc@1  99.22 ( 96.80)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 40/391]	Time  0.172 ( 0.180)	Data  0.002 ( 0.009)	Loss 6.7580e-02 (1.1430e-01)	Acc@1  98.44 ( 96.72)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 50/391]	Time  0.185 ( 0.179)	Data  0.001 ( 0.008)	Loss 1.4807e-01 (1.1741e-01)	Acc@1  94.53 ( 96.60)	Acc@5 100.00 ( 99.97)
Epoch: [36][ 60/391]	Time  0.182 ( 0.179)	Data  0.001 ( 0.008)	Loss 1.5647e-01 (1.1780e-01)	Acc@1  95.31 ( 96.54)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 70/391]	Time  0.183 ( 0.180)	Data  0.002 ( 0.007)	Loss 1.4988e-01 (1.1757e-01)	Acc@1  96.88 ( 96.62)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 80/391]	Time  0.165 ( 0.178)	Data  0.001 ( 0.007)	Loss 1.3175e-01 (1.1762e-01)	Acc@1  93.75 ( 96.60)	Acc@5 100.00 ( 99.92)
Epoch: [36][ 90/391]	Time  0.168 ( 0.177)	Data  0.002 ( 0.007)	Loss 7.7286e-02 (1.1525e-01)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.93)
Epoch: [36][100/391]	Time  0.163 ( 0.177)	Data  0.002 ( 0.006)	Loss 1.3752e-01 (1.1477e-01)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.94)
Epoch: [36][110/391]	Time  0.174 ( 0.177)	Data  0.001 ( 0.006)	Loss 1.0761e-01 (1.1444e-01)	Acc@1  96.88 ( 96.77)	Acc@5 100.00 ( 99.92)
Epoch: [36][120/391]	Time  0.171 ( 0.176)	Data  0.001 ( 0.006)	Loss 8.5030e-02 (1.1457e-01)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.92)
Epoch: [36][130/391]	Time  0.168 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.0918e-01 (1.1473e-01)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.92)
Epoch: [36][140/391]	Time  0.164 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.1442e-01 (1.1467e-01)	Acc@1  96.09 ( 96.74)	Acc@5 100.00 ( 99.92)
Epoch: [36][150/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.4402e-01 (1.1411e-01)	Acc@1  95.31 ( 96.77)	Acc@5 100.00 ( 99.93)
Epoch: [36][160/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.0945e-01 (1.1551e-01)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.91)
Epoch: [36][170/391]	Time  0.179 ( 0.174)	Data  0.002 ( 0.006)	Loss 7.6782e-02 (1.1524e-01)	Acc@1  99.22 ( 96.73)	Acc@5 100.00 ( 99.92)
Epoch: [36][180/391]	Time  0.163 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.4690e-01 (1.1502e-01)	Acc@1  95.31 ( 96.72)	Acc@5  99.22 ( 99.92)
Epoch: [36][190/391]	Time  0.162 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.1451e-01 (1.1507e-01)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.92)
Epoch: [36][200/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.3528e-01 (1.1534e-01)	Acc@1  96.09 ( 96.73)	Acc@5 100.00 ( 99.92)
Epoch: [36][210/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.4482e-01 (1.1611e-01)	Acc@1  95.31 ( 96.72)	Acc@5  99.22 ( 99.91)
Epoch: [36][220/391]	Time  0.159 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.4922e-01 (1.1638e-01)	Acc@1  96.09 ( 96.71)	Acc@5  99.22 ( 99.91)
Epoch: [36][230/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.005)	Loss 9.6579e-02 (1.1689e-01)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 ( 99.91)
Epoch: [36][240/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0229e-01 (1.1646e-01)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.91)
Epoch: [36][250/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0529e-01 (1.1666e-01)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 ( 99.92)
Epoch: [36][260/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.3573e-01 (1.1685e-01)	Acc@1  94.53 ( 96.72)	Acc@5 100.00 ( 99.92)
Epoch: [36][270/391]	Time  0.177 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.2007e-01 (1.1733e-01)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.92)
Epoch: [36][280/391]	Time  0.167 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.7220e-01 (1.1802e-01)	Acc@1  95.31 ( 96.67)	Acc@5 100.00 ( 99.92)
Epoch: [36][290/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5005e-01 (1.1832e-01)	Acc@1  96.09 ( 96.65)	Acc@5 100.00 ( 99.92)
Epoch: [36][300/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.005)	Loss 7.6202e-02 (1.1789e-01)	Acc@1  98.44 ( 96.68)	Acc@5 100.00 ( 99.92)
Epoch: [36][310/391]	Time  0.183 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0181e-01 (1.1739e-01)	Acc@1  97.66 ( 96.72)	Acc@5 100.00 ( 99.92)
Epoch: [36][320/391]	Time  0.179 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.6086e-01 (1.1719e-01)	Acc@1  93.75 ( 96.72)	Acc@5 100.00 ( 99.92)
Epoch: [36][330/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.3332e-01 (1.1703e-01)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.92)
Epoch: [36][340/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.005)	Loss 7.6919e-02 (1.1680e-01)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.92)
Epoch: [36][350/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.005)	Loss 7.8913e-02 (1.1669e-01)	Acc@1  99.22 ( 96.75)	Acc@5 100.00 ( 99.92)
Epoch: [36][360/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.005)	Loss 9.0572e-02 (1.1708e-01)	Acc@1  98.44 ( 96.75)	Acc@5 100.00 ( 99.92)
Epoch: [36][370/391]	Time  0.184 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2394e-01 (1.1697e-01)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.92)
Epoch: [36][380/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.1207e-01 (1.1737e-01)	Acc@1  96.88 ( 96.76)	Acc@5 100.00 ( 99.92)
Epoch: [36][390/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.7340e-01 (1.1732e-01)	Acc@1  92.50 ( 96.77)	Acc@5 100.00 ( 99.92)
## e[36] optimizer.zero_grad (sum) time: 0.39742159843444824
## e[36]       loss.backward (sum) time: 25.928877592086792
## e[36]      optimizer.step (sum) time: 3.8180041313171387
## epoch[36] training(only) time: 67.88187575340271
# Switched to evaluate mode...
Test: [  0/100]	Time  0.269 ( 0.269)	Loss 1.2995e+00 (1.2995e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.055 ( 0.075)	Loss 1.3963e+00 (1.2389e+00)	Acc@1  69.00 ( 73.00)	Acc@5  92.00 ( 92.18)
Test: [ 20/100]	Time  0.055 ( 0.066)	Loss 8.9289e-01 (1.1162e+00)	Acc@1  77.00 ( 74.95)	Acc@5  96.00 ( 93.29)
Test: [ 30/100]	Time  0.053 ( 0.062)	Loss 1.7644e+00 (1.1715e+00)	Acc@1  63.00 ( 74.10)	Acc@5  92.00 ( 93.29)
Test: [ 40/100]	Time  0.058 ( 0.060)	Loss 1.1512e+00 (1.1616e+00)	Acc@1  74.00 ( 73.83)	Acc@5  96.00 ( 93.61)
Test: [ 50/100]	Time  0.058 ( 0.060)	Loss 1.4157e+00 (1.1713e+00)	Acc@1  68.00 ( 73.51)	Acc@5  94.00 ( 93.37)
Test: [ 60/100]	Time  0.058 ( 0.059)	Loss 1.2677e+00 (1.1336e+00)	Acc@1  72.00 ( 73.92)	Acc@5  92.00 ( 93.57)
Test: [ 70/100]	Time  0.058 ( 0.059)	Loss 1.6383e+00 (1.1439e+00)	Acc@1  69.00 ( 73.92)	Acc@5  89.00 ( 93.54)
Test: [ 80/100]	Time  0.059 ( 0.059)	Loss 1.4629e+00 (1.1497e+00)	Acc@1  65.00 ( 73.73)	Acc@5  91.00 ( 93.44)
Test: [ 90/100]	Time  0.065 ( 0.059)	Loss 1.7353e+00 (1.1328e+00)	Acc@1  63.00 ( 73.88)	Acc@5  91.00 ( 93.55)
 * Acc@1 74.140 Acc@5 93.550
### epoch[36] execution time: 73.89339423179626
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.377 ( 0.377)	Data  0.190 ( 0.190)	Loss 1.2214e-01 (1.2214e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.173 ( 0.190)	Data  0.001 ( 0.021)	Loss 5.5709e-02 (1.0277e-01)	Acc@1 100.00 ( 97.66)	Acc@5 100.00 ( 99.93)
Epoch: [37][ 20/391]	Time  0.168 ( 0.182)	Data  0.001 ( 0.013)	Loss 1.0825e-01 (9.8942e-02)	Acc@1  96.09 ( 97.77)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 30/391]	Time  0.167 ( 0.179)	Data  0.001 ( 0.010)	Loss 1.1024e-01 (1.0301e-01)	Acc@1  98.44 ( 97.56)	Acc@5  99.22 ( 99.92)
Epoch: [37][ 40/391]	Time  0.163 ( 0.176)	Data  0.002 ( 0.009)	Loss 1.1689e-01 (1.0393e-01)	Acc@1  95.31 ( 97.41)	Acc@5 100.00 ( 99.92)
Epoch: [37][ 50/391]	Time  0.157 ( 0.173)	Data  0.001 ( 0.008)	Loss 1.0744e-01 (1.0329e-01)	Acc@1  95.31 ( 97.32)	Acc@5 100.00 ( 99.92)
Epoch: [37][ 60/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.1187e-01 (1.0474e-01)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.94)
Epoch: [37][ 70/391]	Time  0.165 ( 0.171)	Data  0.001 ( 0.007)	Loss 1.1142e-01 (1.0400e-01)	Acc@1  96.88 ( 97.30)	Acc@5  99.22 ( 99.92)
Epoch: [37][ 80/391]	Time  0.183 ( 0.173)	Data  0.001 ( 0.007)	Loss 1.1430e-01 (1.0593e-01)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.93)
Epoch: [37][ 90/391]	Time  0.179 ( 0.174)	Data  0.001 ( 0.006)	Loss 9.0926e-02 (1.0378e-01)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.93)
Epoch: [37][100/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.1028e-01 (1.0451e-01)	Acc@1  96.09 ( 97.21)	Acc@5 100.00 ( 99.94)
Epoch: [37][110/391]	Time  0.177 ( 0.174)	Data  0.001 ( 0.006)	Loss 9.9954e-02 (1.0376e-01)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.94)
Epoch: [37][120/391]	Time  0.162 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.1387e-01 (1.0467e-01)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.94)
Epoch: [37][130/391]	Time  0.180 ( 0.173)	Data  0.002 ( 0.006)	Loss 8.5509e-02 (1.0473e-01)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.94)
Epoch: [37][140/391]	Time  0.180 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.2657e-01 (1.0460e-01)	Acc@1  96.09 ( 97.27)	Acc@5 100.00 ( 99.94)
Epoch: [37][150/391]	Time  0.176 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.0555e-01 (1.0482e-01)	Acc@1  98.44 ( 97.27)	Acc@5 100.00 ( 99.94)
Epoch: [37][160/391]	Time  0.166 ( 0.174)	Data  0.002 ( 0.006)	Loss 1.3009e-01 (1.0482e-01)	Acc@1  96.09 ( 97.26)	Acc@5 100.00 ( 99.95)
Epoch: [37][170/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.4820e-01 (1.0562e-01)	Acc@1  95.31 ( 97.21)	Acc@5 100.00 ( 99.95)
Epoch: [37][180/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.005)	Loss 9.6333e-02 (1.0511e-01)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.94)
Epoch: [37][190/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.4369e-01 (1.0582e-01)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.93)
Epoch: [37][200/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0311e-01 (1.0554e-01)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.94)
Epoch: [37][210/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2785e-01 (1.0524e-01)	Acc@1  95.31 ( 97.29)	Acc@5 100.00 ( 99.94)
Epoch: [37][220/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0380e-01 (1.0567e-01)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.94)
Epoch: [37][230/391]	Time  0.187 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.5711e-02 (1.0471e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.94)
Epoch: [37][240/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0285e-01 (1.0515e-01)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.94)
Epoch: [37][250/391]	Time  0.174 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2122e-01 (1.0540e-01)	Acc@1  95.31 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [37][260/391]	Time  0.188 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0762e-01 (1.0533e-01)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [37][270/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.3027e-01 (1.0567e-01)	Acc@1  95.31 ( 97.26)	Acc@5 100.00 ( 99.95)
Epoch: [37][280/391]	Time  0.163 ( 0.172)	Data  0.002 ( 0.005)	Loss 7.5231e-02 (1.0520e-01)	Acc@1  98.44 ( 97.27)	Acc@5 100.00 ( 99.95)
Epoch: [37][290/391]	Time  0.163 ( 0.172)	Data  0.002 ( 0.005)	Loss 8.8133e-02 (1.0576e-01)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.94)
Epoch: [37][300/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0636e-01 (1.0573e-01)	Acc@1  96.88 ( 97.26)	Acc@5 100.00 ( 99.94)
Epoch: [37][310/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0494e-01 (1.0546e-01)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.94)
Epoch: [37][320/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.7635e-02 (1.0552e-01)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.94)
Epoch: [37][330/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 6.7575e-02 (1.0498e-01)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.94)
Epoch: [37][340/391]	Time  0.183 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2400e-01 (1.0523e-01)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.95)
Epoch: [37][350/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2529e-01 (1.0550e-01)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [37][360/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.005)	Loss 6.4660e-02 (1.0536e-01)	Acc@1  97.66 ( 97.24)	Acc@5 100.00 ( 99.95)
Epoch: [37][370/391]	Time  0.172 ( 0.173)	Data  0.002 ( 0.005)	Loss 7.4850e-02 (1.0557e-01)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.94)
Epoch: [37][380/391]	Time  0.182 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0190e-01 (1.0564e-01)	Acc@1  94.53 ( 97.24)	Acc@5 100.00 ( 99.94)
Epoch: [37][390/391]	Time  0.181 ( 0.173)	Data  0.001 ( 0.005)	Loss 6.5668e-02 (1.0571e-01)	Acc@1  98.75 ( 97.22)	Acc@5 100.00 ( 99.94)
## e[37] optimizer.zero_grad (sum) time: 0.39105987548828125
## e[37]       loss.backward (sum) time: 25.882474660873413
## e[37]      optimizer.step (sum) time: 3.8017120361328125
## epoch[37] training(only) time: 67.72248196601868
# Switched to evaluate mode...
Test: [  0/100]	Time  0.282 ( 0.282)	Loss 1.2833e+00 (1.2833e+00)	Acc@1  76.00 ( 76.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.060 ( 0.080)	Loss 1.4597e+00 (1.2482e+00)	Acc@1  69.00 ( 73.55)	Acc@5  90.00 ( 92.09)
Test: [ 20/100]	Time  0.058 ( 0.070)	Loss 9.0137e-01 (1.1270e+00)	Acc@1  77.00 ( 74.81)	Acc@5  95.00 ( 93.14)
Test: [ 30/100]	Time  0.058 ( 0.066)	Loss 1.8401e+00 (1.1839e+00)	Acc@1  62.00 ( 74.06)	Acc@5  92.00 ( 93.13)
Test: [ 40/100]	Time  0.058 ( 0.064)	Loss 1.1362e+00 (1.1728e+00)	Acc@1  74.00 ( 73.95)	Acc@5  96.00 ( 93.41)
Test: [ 50/100]	Time  0.058 ( 0.063)	Loss 1.4236e+00 (1.1846e+00)	Acc@1  68.00 ( 73.57)	Acc@5  94.00 ( 93.10)
Test: [ 60/100]	Time  0.058 ( 0.062)	Loss 1.2617e+00 (1.1471e+00)	Acc@1  68.00 ( 73.89)	Acc@5  95.00 ( 93.39)
Test: [ 70/100]	Time  0.057 ( 0.062)	Loss 1.6292e+00 (1.1583e+00)	Acc@1  69.00 ( 73.76)	Acc@5  89.00 ( 93.28)
Test: [ 80/100]	Time  0.056 ( 0.061)	Loss 1.4773e+00 (1.1632e+00)	Acc@1  68.00 ( 73.65)	Acc@5  90.00 ( 93.21)
Test: [ 90/100]	Time  0.059 ( 0.061)	Loss 1.6723e+00 (1.1449e+00)	Acc@1  67.00 ( 73.86)	Acc@5  90.00 ( 93.32)
 * Acc@1 74.060 Acc@5 93.310
### epoch[37] execution time: 73.91723442077637
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.412 ( 0.412)	Data  0.245 ( 0.245)	Loss 7.5121e-02 (7.5121e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.169 ( 0.192)	Data  0.001 ( 0.026)	Loss 5.0866e-02 (8.5395e-02)	Acc@1  99.22 ( 97.80)	Acc@5 100.00 (100.00)
Epoch: [38][ 20/391]	Time  0.165 ( 0.181)	Data  0.001 ( 0.016)	Loss 7.3802e-02 (8.2768e-02)	Acc@1  98.44 ( 98.07)	Acc@5 100.00 (100.00)
Epoch: [38][ 30/391]	Time  0.168 ( 0.177)	Data  0.001 ( 0.012)	Loss 1.4090e-01 (8.2696e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 40/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.010)	Loss 8.4762e-02 (8.5617e-02)	Acc@1  96.88 ( 98.00)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 50/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.009)	Loss 6.0962e-02 (8.6168e-02)	Acc@1  98.44 ( 97.92)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 60/391]	Time  0.183 ( 0.173)	Data  0.001 ( 0.008)	Loss 4.7073e-02 (8.6609e-02)	Acc@1 100.00 ( 97.95)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 70/391]	Time  0.173 ( 0.173)	Data  0.002 ( 0.008)	Loss 1.2583e-01 (8.7529e-02)	Acc@1  96.09 ( 97.90)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 80/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.007)	Loss 9.1989e-02 (8.7513e-02)	Acc@1  97.66 ( 97.86)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 90/391]	Time  0.164 ( 0.173)	Data  0.002 ( 0.007)	Loss 6.1726e-02 (8.9643e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.97)
Epoch: [38][100/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.007)	Loss 5.0050e-02 (9.0196e-02)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [38][110/391]	Time  0.162 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.3997e-01 (9.1480e-02)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.98)
Epoch: [38][120/391]	Time  0.160 ( 0.172)	Data  0.001 ( 0.006)	Loss 8.1341e-02 (9.2536e-02)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [38][130/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.1915e-01 (9.2349e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [38][140/391]	Time  0.182 ( 0.171)	Data  0.001 ( 0.006)	Loss 8.4974e-02 (9.1726e-02)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.98)
Epoch: [38][150/391]	Time  0.186 ( 0.172)	Data  0.001 ( 0.006)	Loss 9.4223e-02 (9.2284e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [38][160/391]	Time  0.187 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.2094e-01 (9.1900e-02)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [38][170/391]	Time  0.167 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.3413e-01 (9.2616e-02)	Acc@1  96.09 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [38][180/391]	Time  0.167 ( 0.173)	Data  0.002 ( 0.006)	Loss 8.7996e-02 (9.3054e-02)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [38][190/391]	Time  0.179 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.0630e-01 (9.3553e-02)	Acc@1  96.09 ( 97.62)	Acc@5 100.00 ( 99.98)
Epoch: [38][200/391]	Time  0.185 ( 0.173)	Data  0.002 ( 0.006)	Loss 8.2219e-02 (9.3706e-02)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [38][210/391]	Time  0.182 ( 0.174)	Data  0.001 ( 0.006)	Loss 8.0765e-02 (9.4122e-02)	Acc@1  99.22 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [38][220/391]	Time  0.164 ( 0.174)	Data  0.002 ( 0.006)	Loss 1.1597e-01 (9.4423e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [38][230/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.0182e-01 (9.4808e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [38][240/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.5765e-02 (9.4609e-02)	Acc@1  99.22 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [38][250/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 7.7814e-02 (9.4595e-02)	Acc@1  97.66 ( 97.57)	Acc@5 100.00 ( 99.98)
Epoch: [38][260/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.005)	Loss 8.8705e-02 (9.4626e-02)	Acc@1  97.66 ( 97.56)	Acc@5 100.00 ( 99.98)
Epoch: [38][270/391]	Time  0.160 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.4579e-01 (9.4894e-02)	Acc@1  96.09 ( 97.55)	Acc@5  99.22 ( 99.97)
Epoch: [38][280/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1671e-01 (9.4924e-02)	Acc@1  95.31 ( 97.54)	Acc@5 100.00 ( 99.97)
Epoch: [38][290/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 7.1253e-02 (9.5071e-02)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.98)
Epoch: [38][300/391]	Time  0.174 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.6646e-01 (9.5156e-02)	Acc@1  93.75 ( 97.54)	Acc@5 100.00 ( 99.97)
Epoch: [38][310/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2446e-01 (9.5672e-02)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.97)
Epoch: [38][320/391]	Time  0.172 ( 0.172)	Data  0.002 ( 0.005)	Loss 8.6756e-02 (9.5603e-02)	Acc@1  96.09 ( 97.53)	Acc@5 100.00 ( 99.97)
Epoch: [38][330/391]	Time  0.170 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.2840e-01 (9.6222e-02)	Acc@1  94.53 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [38][340/391]	Time  0.174 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.1756e-01 (9.6449e-02)	Acc@1  96.09 ( 97.47)	Acc@5 100.00 ( 99.97)
Epoch: [38][350/391]	Time  0.171 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.2318e-01 (9.6354e-02)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [38][360/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 9.9922e-02 (9.6483e-02)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [38][370/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.5688e-02 (9.6543e-02)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [38][380/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.3200e-01 (9.6786e-02)	Acc@1  96.09 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [38][390/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.3827e-01 (9.6981e-02)	Acc@1  96.25 ( 97.43)	Acc@5 100.00 ( 99.96)
## e[38] optimizer.zero_grad (sum) time: 0.3841855525970459
## e[38]       loss.backward (sum) time: 25.26216697692871
## e[38]      optimizer.step (sum) time: 3.711557626724243
## epoch[38] training(only) time: 67.24418187141418
# Switched to evaluate mode...
Test: [  0/100]	Time  0.291 ( 0.291)	Loss 1.3625e+00 (1.3625e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.061 ( 0.081)	Loss 1.4729e+00 (1.2492e+00)	Acc@1  68.00 ( 73.36)	Acc@5  91.00 ( 92.18)
Test: [ 20/100]	Time  0.060 ( 0.071)	Loss 9.8218e-01 (1.1378e+00)	Acc@1  74.00 ( 74.52)	Acc@5  97.00 ( 93.48)
Test: [ 30/100]	Time  0.061 ( 0.068)	Loss 1.7286e+00 (1.1907e+00)	Acc@1  61.00 ( 73.87)	Acc@5  91.00 ( 93.42)
Test: [ 40/100]	Time  0.061 ( 0.066)	Loss 1.2208e+00 (1.1760e+00)	Acc@1  75.00 ( 73.78)	Acc@5  97.00 ( 93.68)
Test: [ 50/100]	Time  0.058 ( 0.065)	Loss 1.4224e+00 (1.1820e+00)	Acc@1  69.00 ( 73.61)	Acc@5  94.00 ( 93.41)
Test: [ 60/100]	Time  0.057 ( 0.064)	Loss 1.3048e+00 (1.1472e+00)	Acc@1  68.00 ( 73.93)	Acc@5  92.00 ( 93.59)
Test: [ 70/100]	Time  0.060 ( 0.063)	Loss 1.6686e+00 (1.1573e+00)	Acc@1  69.00 ( 73.93)	Acc@5  88.00 ( 93.61)
Test: [ 80/100]	Time  0.061 ( 0.063)	Loss 1.5773e+00 (1.1667e+00)	Acc@1  65.00 ( 73.75)	Acc@5  90.00 ( 93.52)
Test: [ 90/100]	Time  0.059 ( 0.062)	Loss 1.7362e+00 (1.1493e+00)	Acc@1  66.00 ( 73.96)	Acc@5  88.00 ( 93.65)
 * Acc@1 74.200 Acc@5 93.620
### epoch[38] execution time: 73.5628719329834
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.392 ( 0.392)	Data  0.216 ( 0.216)	Loss 7.0357e-02 (7.0357e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.172 ( 0.193)	Data  0.002 ( 0.023)	Loss 1.0550e-01 (7.7358e-02)	Acc@1  96.88 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [39][ 20/391]	Time  0.175 ( 0.186)	Data  0.002 ( 0.014)	Loss 7.8655e-02 (8.6164e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 30/391]	Time  0.175 ( 0.183)	Data  0.002 ( 0.011)	Loss 1.1878e-01 (8.7931e-02)	Acc@1  96.88 ( 97.88)	Acc@5 100.00 ( 99.92)
Epoch: [39][ 40/391]	Time  0.168 ( 0.180)	Data  0.002 ( 0.009)	Loss 5.0129e-02 (8.8607e-02)	Acc@1 100.00 ( 97.85)	Acc@5 100.00 ( 99.94)
Epoch: [39][ 50/391]	Time  0.169 ( 0.178)	Data  0.001 ( 0.008)	Loss 9.8280e-02 (8.8210e-02)	Acc@1  97.66 ( 97.92)	Acc@5 100.00 ( 99.95)
Epoch: [39][ 60/391]	Time  0.164 ( 0.177)	Data  0.002 ( 0.007)	Loss 5.9164e-02 (8.7426e-02)	Acc@1 100.00 ( 97.94)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 70/391]	Time  0.167 ( 0.175)	Data  0.001 ( 0.007)	Loss 7.3434e-02 (8.8856e-02)	Acc@1  96.09 ( 97.84)	Acc@5 100.00 ( 99.92)
Epoch: [39][ 80/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.007)	Loss 7.3963e-02 (8.7899e-02)	Acc@1  99.22 ( 97.87)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 90/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.007)	Loss 3.7177e-02 (8.5898e-02)	Acc@1 100.00 ( 97.94)	Acc@5 100.00 ( 99.94)
Epoch: [39][100/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.006)	Loss 9.7882e-02 (8.6064e-02)	Acc@1  96.09 ( 97.90)	Acc@5 100.00 ( 99.95)
Epoch: [39][110/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.0762e-01 (8.6906e-02)	Acc@1  97.66 ( 97.87)	Acc@5 100.00 ( 99.95)
Epoch: [39][120/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.006)	Loss 7.8093e-02 (8.6978e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 ( 99.95)
Epoch: [39][130/391]	Time  0.172 ( 0.172)	Data  0.002 ( 0.006)	Loss 5.4640e-02 (8.6105e-02)	Acc@1  98.44 ( 97.91)	Acc@5 100.00 ( 99.95)
Epoch: [39][140/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.0557e-01 (8.6762e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.96)
Epoch: [39][150/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.006)	Loss 6.6482e-02 (8.7065e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.96)
Epoch: [39][160/391]	Time  0.173 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.0367e-01 (8.7560e-02)	Acc@1  97.66 ( 97.81)	Acc@5  99.22 ( 99.95)
Epoch: [39][170/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.006)	Loss 9.0636e-02 (8.7730e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.95)
Epoch: [39][180/391]	Time  0.160 ( 0.171)	Data  0.001 ( 0.006)	Loss 6.6195e-02 (8.7178e-02)	Acc@1 100.00 ( 97.83)	Acc@5 100.00 ( 99.95)
Epoch: [39][190/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.006)	Loss 7.2094e-02 (8.7344e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.95)
Epoch: [39][200/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.006)	Loss 7.6770e-02 (8.7565e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.95)
Epoch: [39][210/391]	Time  0.184 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.0140e-01 (8.7810e-02)	Acc@1  96.88 ( 97.83)	Acc@5 100.00 ( 99.96)
Epoch: [39][220/391]	Time  0.186 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.0519e-02 (8.7383e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.96)
Epoch: [39][230/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0313e-01 (8.7176e-02)	Acc@1  95.31 ( 97.82)	Acc@5 100.00 ( 99.96)
Epoch: [39][240/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.005)	Loss 9.7275e-02 (8.7425e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.96)
Epoch: [39][250/391]	Time  0.180 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1503e-01 (8.7690e-02)	Acc@1  96.09 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [39][260/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1517e-01 (8.7527e-02)	Acc@1  95.31 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [39][270/391]	Time  0.182 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.8699e-02 (8.7606e-02)	Acc@1  99.22 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [39][280/391]	Time  0.173 ( 0.172)	Data  0.001 ( 0.005)	Loss 7.1242e-02 (8.8082e-02)	Acc@1  97.66 ( 97.79)	Acc@5 100.00 ( 99.96)
Epoch: [39][290/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2502e-01 (8.8127e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.96)
Epoch: [39][300/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 7.5419e-02 (8.8337e-02)	Acc@1  99.22 ( 97.78)	Acc@5 100.00 ( 99.96)
Epoch: [39][310/391]	Time  0.157 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0710e-01 (8.8558e-02)	Acc@1  96.88 ( 97.76)	Acc@5 100.00 ( 99.96)
Epoch: [39][320/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0204e-01 (8.8574e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.95)
Epoch: [39][330/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.2512e-02 (8.8731e-02)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.95)
Epoch: [39][340/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.005)	Loss 6.4782e-02 (8.8525e-02)	Acc@1  99.22 ( 97.77)	Acc@5 100.00 ( 99.95)
Epoch: [39][350/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.005)	Loss 7.4142e-02 (8.8669e-02)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.96)
Epoch: [39][360/391]	Time  0.173 ( 0.171)	Data  0.002 ( 0.005)	Loss 6.5170e-02 (8.8804e-02)	Acc@1  99.22 ( 97.76)	Acc@5 100.00 ( 99.96)
Epoch: [39][370/391]	Time  0.176 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.4443e-02 (8.8950e-02)	Acc@1  96.88 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [39][380/391]	Time  0.172 ( 0.171)	Data  0.001 ( 0.005)	Loss 8.7715e-02 (8.9323e-02)	Acc@1  96.88 ( 97.72)	Acc@5 100.00 ( 99.96)
Epoch: [39][390/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.4189e-01 (8.9032e-02)	Acc@1  97.50 ( 97.73)	Acc@5 100.00 ( 99.96)
## e[39] optimizer.zero_grad (sum) time: 0.3863821029663086
## e[39]       loss.backward (sum) time: 25.221367597579956
## e[39]      optimizer.step (sum) time: 3.7329518795013428
## epoch[39] training(only) time: 67.20292639732361
# Switched to evaluate mode...
Test: [  0/100]	Time  0.241 ( 0.241)	Loss 1.3794e+00 (1.3794e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.058 ( 0.075)	Loss 1.4723e+00 (1.2842e+00)	Acc@1  68.00 ( 72.91)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.058 ( 0.066)	Loss 9.7419e-01 (1.1545e+00)	Acc@1  76.00 ( 74.38)	Acc@5  95.00 ( 93.62)
Test: [ 30/100]	Time  0.058 ( 0.064)	Loss 1.8265e+00 (1.2074e+00)	Acc@1  61.00 ( 73.55)	Acc@5  91.00 ( 93.48)
Test: [ 40/100]	Time  0.057 ( 0.063)	Loss 1.2240e+00 (1.1913e+00)	Acc@1  74.00 ( 73.68)	Acc@5  96.00 ( 93.73)
Test: [ 50/100]	Time  0.056 ( 0.061)	Loss 1.4086e+00 (1.1963e+00)	Acc@1  65.00 ( 73.37)	Acc@5  91.00 ( 93.43)
Test: [ 60/100]	Time  0.058 ( 0.061)	Loss 1.3090e+00 (1.1592e+00)	Acc@1  68.00 ( 73.84)	Acc@5  92.00 ( 93.62)
Test: [ 70/100]	Time  0.053 ( 0.060)	Loss 1.6478e+00 (1.1722e+00)	Acc@1  69.00 ( 73.73)	Acc@5  90.00 ( 93.58)
Test: [ 80/100]	Time  0.052 ( 0.059)	Loss 1.4549e+00 (1.1800e+00)	Acc@1  66.00 ( 73.56)	Acc@5  92.00 ( 93.58)
Test: [ 90/100]	Time  0.052 ( 0.058)	Loss 1.7406e+00 (1.1629e+00)	Acc@1  67.00 ( 73.76)	Acc@5  89.00 ( 93.68)
 * Acc@1 74.070 Acc@5 93.700
### epoch[39] execution time: 73.08563995361328
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.359 ( 0.359)	Data  0.218 ( 0.218)	Loss 1.1226e-01 (1.1226e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.161 ( 0.180)	Data  0.001 ( 0.024)	Loss 6.5229e-02 (7.3325e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.93)
Epoch: [40][ 20/391]	Time  0.179 ( 0.178)	Data  0.001 ( 0.015)	Loss 4.7729e-02 (7.7813e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.182 ( 0.180)	Data  0.001 ( 0.012)	Loss 8.8092e-02 (7.8093e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.165 ( 0.179)	Data  0.002 ( 0.010)	Loss 1.1737e-01 (7.8752e-02)	Acc@1  95.31 ( 98.00)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 50/391]	Time  0.164 ( 0.177)	Data  0.002 ( 0.009)	Loss 1.4637e-01 (8.1671e-02)	Acc@1  96.09 ( 97.93)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 60/391]	Time  0.168 ( 0.175)	Data  0.001 ( 0.008)	Loss 6.7900e-02 (8.2150e-02)	Acc@1  98.44 ( 97.81)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 70/391]	Time  0.173 ( 0.174)	Data  0.001 ( 0.007)	Loss 7.1568e-02 (8.1627e-02)	Acc@1  98.44 ( 97.80)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 80/391]	Time  0.174 ( 0.175)	Data  0.002 ( 0.007)	Loss 8.0107e-02 (8.1152e-02)	Acc@1  97.66 ( 97.85)	Acc@5 100.00 ( 99.99)
Epoch: [40][ 90/391]	Time  0.177 ( 0.176)	Data  0.002 ( 0.007)	Loss 5.9774e-02 (8.0318e-02)	Acc@1 100.00 ( 97.91)	Acc@5 100.00 ( 99.99)
Epoch: [40][100/391]	Time  0.166 ( 0.175)	Data  0.002 ( 0.007)	Loss 9.6045e-02 (8.1514e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.99)
Epoch: [40][110/391]	Time  0.164 ( 0.175)	Data  0.002 ( 0.006)	Loss 7.7036e-02 (7.9813e-02)	Acc@1  96.88 ( 97.99)	Acc@5 100.00 ( 99.99)
Epoch: [40][120/391]	Time  0.168 ( 0.175)	Data  0.002 ( 0.006)	Loss 7.5201e-02 (7.9200e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 ( 99.99)
Epoch: [40][130/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.006)	Loss 7.8486e-02 (7.9233e-02)	Acc@1  98.44 ( 98.07)	Acc@5 100.00 ( 99.99)
Epoch: [40][140/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.0084e-01 (7.9650e-02)	Acc@1  96.09 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [40][150/391]	Time  0.162 ( 0.173)	Data  0.001 ( 0.006)	Loss 9.2108e-02 (7.9397e-02)	Acc@1  98.44 ( 98.05)	Acc@5 100.00 ( 99.98)
Epoch: [40][160/391]	Time  0.159 ( 0.172)	Data  0.001 ( 0.006)	Loss 4.9570e-02 (7.9017e-02)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.99)
Epoch: [40][170/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.006)	Loss 8.2936e-02 (7.8942e-02)	Acc@1  97.66 ( 98.04)	Acc@5 100.00 ( 99.99)
Epoch: [40][180/391]	Time  0.172 ( 0.172)	Data  0.001 ( 0.006)	Loss 7.8007e-02 (7.9550e-02)	Acc@1  97.66 ( 98.03)	Acc@5 100.00 ( 99.98)
Epoch: [40][190/391]	Time  0.173 ( 0.172)	Data  0.001 ( 0.006)	Loss 5.2739e-02 (7.9616e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.98)
Epoch: [40][200/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 9.1917e-02 (8.0354e-02)	Acc@1  99.22 ( 97.97)	Acc@5  99.22 ( 99.98)
Epoch: [40][210/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.3127e-01 (8.0669e-02)	Acc@1  95.31 ( 97.97)	Acc@5 100.00 ( 99.97)
Epoch: [40][220/391]	Time  0.178 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0930e-01 (8.1286e-02)	Acc@1  96.09 ( 97.95)	Acc@5 100.00 ( 99.98)
Epoch: [40][230/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0052e-01 (8.1258e-02)	Acc@1  96.09 ( 97.92)	Acc@5 100.00 ( 99.98)
Epoch: [40][240/391]	Time  0.156 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.7538e-02 (8.1397e-02)	Acc@1  98.44 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [40][250/391]	Time  0.156 ( 0.171)	Data  0.001 ( 0.005)	Loss 4.7337e-02 (8.0700e-02)	Acc@1  98.44 ( 97.96)	Acc@5 100.00 ( 99.98)
Epoch: [40][260/391]	Time  0.154 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.2116e-01 (8.0992e-02)	Acc@1  96.09 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [40][270/391]	Time  0.181 ( 0.171)	Data  0.001 ( 0.005)	Loss 6.2703e-02 (8.1525e-02)	Acc@1  99.22 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [40][280/391]	Time  0.182 ( 0.171)	Data  0.002 ( 0.005)	Loss 5.2995e-02 (8.1234e-02)	Acc@1  98.44 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [40][290/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.4605e-01 (8.1592e-02)	Acc@1  94.53 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [40][300/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.0458e-01 (8.1613e-02)	Acc@1  96.88 ( 97.95)	Acc@5  99.22 ( 99.97)
Epoch: [40][310/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.6478e-02 (8.1371e-02)	Acc@1  96.88 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [40][320/391]	Time  0.174 ( 0.172)	Data  0.002 ( 0.005)	Loss 6.7635e-02 (8.1251e-02)	Acc@1  97.66 ( 97.95)	Acc@5 100.00 ( 99.97)
Epoch: [40][330/391]	Time  0.180 ( 0.172)	Data  0.002 ( 0.005)	Loss 6.6490e-02 (8.1411e-02)	Acc@1  99.22 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [40][340/391]	Time  0.170 ( 0.172)	Data  0.002 ( 0.005)	Loss 9.3288e-02 (8.1431e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [40][350/391]	Time  0.166 ( 0.172)	Data  0.002 ( 0.005)	Loss 4.9338e-02 (8.1344e-02)	Acc@1  99.22 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [40][360/391]	Time  0.165 ( 0.172)	Data  0.002 ( 0.005)	Loss 4.7286e-02 (8.0928e-02)	Acc@1 100.00 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [40][370/391]	Time  0.157 ( 0.172)	Data  0.002 ( 0.005)	Loss 4.9157e-02 (8.0755e-02)	Acc@1  97.66 ( 97.97)	Acc@5 100.00 ( 99.97)
Epoch: [40][380/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.4716e-02 (8.0693e-02)	Acc@1 100.00 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [40][390/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.1920e-01 (8.0805e-02)	Acc@1  97.50 ( 97.97)	Acc@5 100.00 ( 99.97)
## e[40] optimizer.zero_grad (sum) time: 0.3945331573486328
## e[40]       loss.backward (sum) time: 25.58950686454773
## e[40]      optimizer.step (sum) time: 3.7853713035583496
## epoch[40] training(only) time: 67.1024227142334
# Switched to evaluate mode...
Test: [  0/100]	Time  0.252 ( 0.252)	Loss 1.3895e+00 (1.3895e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.052 ( 0.071)	Loss 1.5320e+00 (1.3091e+00)	Acc@1  69.00 ( 73.64)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.052 ( 0.062)	Loss 9.9534e-01 (1.1874e+00)	Acc@1  79.00 ( 75.19)	Acc@5  96.00 ( 93.76)
Test: [ 30/100]	Time  0.052 ( 0.059)	Loss 1.8470e+00 (1.2442e+00)	Acc@1  60.00 ( 73.90)	Acc@5  90.00 ( 93.23)
Test: [ 40/100]	Time  0.051 ( 0.057)	Loss 1.1848e+00 (1.2258e+00)	Acc@1  75.00 ( 73.95)	Acc@5  97.00 ( 93.51)
Test: [ 50/100]	Time  0.051 ( 0.056)	Loss 1.4629e+00 (1.2322e+00)	Acc@1  67.00 ( 73.73)	Acc@5  92.00 ( 93.22)
Test: [ 60/100]	Time  0.052 ( 0.055)	Loss 1.2742e+00 (1.1904e+00)	Acc@1  70.00 ( 74.20)	Acc@5  93.00 ( 93.56)
Test: [ 70/100]	Time  0.050 ( 0.055)	Loss 1.6717e+00 (1.2018e+00)	Acc@1  70.00 ( 74.15)	Acc@5  90.00 ( 93.55)
Test: [ 80/100]	Time  0.057 ( 0.055)	Loss 1.5090e+00 (1.2062e+00)	Acc@1  67.00 ( 73.99)	Acc@5  92.00 ( 93.46)
Test: [ 90/100]	Time  0.056 ( 0.055)	Loss 1.7169e+00 (1.1884e+00)	Acc@1  67.00 ( 74.10)	Acc@5  90.00 ( 93.55)
 * Acc@1 74.300 Acc@5 93.560
### epoch[40] execution time: 72.71762728691101
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.398 ( 0.398)	Data  0.230 ( 0.230)	Loss 1.0170e-01 (1.0170e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.176 ( 0.196)	Data  0.001 ( 0.025)	Loss 6.4547e-02 (9.0318e-02)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 ( 99.93)
Epoch: [41][ 20/391]	Time  0.167 ( 0.186)	Data  0.001 ( 0.015)	Loss 7.2986e-02 (7.7870e-02)	Acc@1  98.44 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 30/391]	Time  0.163 ( 0.181)	Data  0.002 ( 0.012)	Loss 9.6917e-02 (7.8328e-02)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.95)
Epoch: [41][ 40/391]	Time  0.165 ( 0.178)	Data  0.001 ( 0.010)	Loss 6.0456e-02 (7.6146e-02)	Acc@1  98.44 ( 98.04)	Acc@5 100.00 ( 99.96)
Epoch: [41][ 50/391]	Time  0.154 ( 0.176)	Data  0.001 ( 0.009)	Loss 5.1888e-02 (7.4323e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 60/391]	Time  0.152 ( 0.173)	Data  0.001 ( 0.008)	Loss 7.3755e-02 (7.4716e-02)	Acc@1  99.22 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 70/391]	Time  0.157 ( 0.171)	Data  0.001 ( 0.007)	Loss 9.2168e-02 (7.4027e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 80/391]	Time  0.189 ( 0.171)	Data  0.001 ( 0.007)	Loss 7.3494e-02 (7.3811e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 90/391]	Time  0.184 ( 0.172)	Data  0.001 ( 0.007)	Loss 9.8154e-02 (7.4877e-02)	Acc@1  97.66 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [41][100/391]	Time  0.184 ( 0.173)	Data  0.001 ( 0.007)	Loss 7.3629e-02 (7.5305e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [41][110/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.007)	Loss 5.8471e-02 (7.5545e-02)	Acc@1  97.66 ( 98.13)	Acc@5 100.00 ( 99.99)
Epoch: [41][120/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.006)	Loss 9.5916e-02 (7.6298e-02)	Acc@1  96.09 ( 98.06)	Acc@5 100.00 ( 99.99)
Epoch: [41][130/391]	Time  0.170 ( 0.173)	Data  0.002 ( 0.006)	Loss 6.1496e-02 (7.5581e-02)	Acc@1  98.44 ( 98.11)	Acc@5 100.00 ( 99.99)
Epoch: [41][140/391]	Time  0.182 ( 0.174)	Data  0.001 ( 0.006)	Loss 8.1139e-02 (7.5605e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.99)
Epoch: [41][150/391]	Time  0.180 ( 0.175)	Data  0.001 ( 0.006)	Loss 7.7128e-02 (7.5150e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [41][160/391]	Time  0.168 ( 0.174)	Data  0.001 ( 0.006)	Loss 7.6655e-02 (7.5023e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [41][170/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.006)	Loss 7.1830e-02 (7.5388e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [41][180/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.006)	Loss 8.6222e-02 (7.5323e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [41][190/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.006)	Loss 8.4215e-02 (7.5130e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [41][200/391]	Time  0.158 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.5448e-02 (7.5507e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [41][210/391]	Time  0.158 ( 0.172)	Data  0.001 ( 0.006)	Loss 6.6766e-02 (7.5444e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.99)
Epoch: [41][220/391]	Time  0.159 ( 0.171)	Data  0.001 ( 0.006)	Loss 9.1925e-02 (7.5141e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.99)
Epoch: [41][230/391]	Time  0.156 ( 0.171)	Data  0.001 ( 0.005)	Loss 6.4906e-02 (7.5652e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [41][240/391]	Time  0.172 ( 0.171)	Data  0.001 ( 0.005)	Loss 7.8908e-02 (7.5310e-02)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [41][250/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 6.2491e-02 (7.5504e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [41][260/391]	Time  0.162 ( 0.171)	Data  0.001 ( 0.005)	Loss 8.1680e-02 (7.5795e-02)	Acc@1  97.66 ( 98.12)	Acc@5  99.22 ( 99.98)
Epoch: [41][270/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.005)	Loss 8.2319e-02 (7.6391e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [41][280/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.005)	Loss 7.3788e-02 (7.6214e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [41][290/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.2726e-01 (7.6037e-02)	Acc@1  94.53 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [41][300/391]	Time  0.160 ( 0.171)	Data  0.001 ( 0.005)	Loss 8.9550e-02 (7.6017e-02)	Acc@1  96.88 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [41][310/391]	Time  0.160 ( 0.171)	Data  0.001 ( 0.005)	Loss 7.2862e-02 (7.6093e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [41][320/391]	Time  0.157 ( 0.170)	Data  0.001 ( 0.005)	Loss 8.0775e-02 (7.6249e-02)	Acc@1  96.09 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [41][330/391]	Time  0.185 ( 0.170)	Data  0.001 ( 0.005)	Loss 6.4836e-02 (7.6265e-02)	Acc@1  97.66 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [41][340/391]	Time  0.183 ( 0.171)	Data  0.001 ( 0.005)	Loss 8.2491e-02 (7.6301e-02)	Acc@1  96.88 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [41][350/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.7305e-02 (7.6547e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [41][360/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 7.5903e-02 (7.6431e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [41][370/391]	Time  0.165 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.3105e-01 (7.6423e-02)	Acc@1  96.09 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [41][380/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.0129e-01 (7.6223e-02)	Acc@1  97.66 ( 98.12)	Acc@5 100.00 ( 99.98)
Epoch: [41][390/391]	Time  0.180 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.7657e-01 (7.6486e-02)	Acc@1  95.00 ( 98.10)	Acc@5 100.00 ( 99.98)
## e[41] optimizer.zero_grad (sum) time: 0.38115644454956055
## e[41]       loss.backward (sum) time: 24.967074871063232
## e[41]      optimizer.step (sum) time: 3.671431064605713
## epoch[41] training(only) time: 67.08502125740051
# Switched to evaluate mode...
Test: [  0/100]	Time  0.294 ( 0.294)	Loss 1.3925e+00 (1.3925e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.059 ( 0.082)	Loss 1.4556e+00 (1.2993e+00)	Acc@1  69.00 ( 73.36)	Acc@5  90.00 ( 92.09)
Test: [ 20/100]	Time  0.058 ( 0.071)	Loss 9.2443e-01 (1.1795e+00)	Acc@1  80.00 ( 75.00)	Acc@5  96.00 ( 93.38)
Test: [ 30/100]	Time  0.057 ( 0.066)	Loss 1.8809e+00 (1.2449e+00)	Acc@1  63.00 ( 74.19)	Acc@5  91.00 ( 93.26)
Test: [ 40/100]	Time  0.057 ( 0.064)	Loss 1.1413e+00 (1.2333e+00)	Acc@1  74.00 ( 74.02)	Acc@5  95.00 ( 93.44)
Test: [ 50/100]	Time  0.056 ( 0.063)	Loss 1.4719e+00 (1.2372e+00)	Acc@1  69.00 ( 73.92)	Acc@5  93.00 ( 93.22)
Test: [ 60/100]	Time  0.063 ( 0.062)	Loss 1.2935e+00 (1.1958e+00)	Acc@1  68.00 ( 74.28)	Acc@5  94.00 ( 93.44)
Test: [ 70/100]	Time  0.063 ( 0.062)	Loss 1.7107e+00 (1.2058e+00)	Acc@1  69.00 ( 74.18)	Acc@5  90.00 ( 93.46)
Test: [ 80/100]	Time  0.063 ( 0.063)	Loss 1.5460e+00 (1.2136e+00)	Acc@1  67.00 ( 73.99)	Acc@5  90.00 ( 93.37)
Test: [ 90/100]	Time  0.060 ( 0.063)	Loss 1.8316e+00 (1.1980e+00)	Acc@1  64.00 ( 74.03)	Acc@5  87.00 ( 93.41)
 * Acc@1 74.190 Acc@5 93.460
### epoch[41] execution time: 73.4252302646637
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.378 ( 0.378)	Data  0.213 ( 0.213)	Loss 8.8035e-02 (8.8035e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.157 ( 0.177)	Data  0.001 ( 0.023)	Loss 5.5906e-02 (7.2670e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.156 ( 0.168)	Data  0.001 ( 0.014)	Loss 4.0879e-02 (6.9640e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.158 ( 0.164)	Data  0.001 ( 0.011)	Loss 7.3560e-02 (6.8077e-02)	Acc@1  99.22 ( 98.54)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.155 ( 0.163)	Data  0.001 ( 0.009)	Loss 8.9347e-02 (6.9988e-02)	Acc@1  96.88 ( 98.30)	Acc@5  99.22 ( 99.98)
Epoch: [42][ 50/391]	Time  0.167 ( 0.165)	Data  0.001 ( 0.008)	Loss 9.1871e-02 (7.0300e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [42][ 60/391]	Time  0.173 ( 0.166)	Data  0.001 ( 0.008)	Loss 7.4630e-02 (7.0874e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 70/391]	Time  0.176 ( 0.168)	Data  0.001 ( 0.007)	Loss 6.8764e-02 (6.9664e-02)	Acc@1  96.88 ( 98.36)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 80/391]	Time  0.166 ( 0.168)	Data  0.001 ( 0.007)	Loss 5.5521e-02 (6.9663e-02)	Acc@1  99.22 ( 98.40)	Acc@5 100.00 ( 99.99)
Epoch: [42][ 90/391]	Time  0.174 ( 0.169)	Data  0.001 ( 0.007)	Loss 1.3084e-01 (7.0840e-02)	Acc@1  96.09 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [42][100/391]	Time  0.165 ( 0.169)	Data  0.002 ( 0.006)	Loss 7.4198e-02 (6.9665e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [42][110/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.006)	Loss 6.0888e-02 (6.8824e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [42][120/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.006)	Loss 4.4652e-02 (6.9290e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [42][130/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 7.8755e-02 (6.9782e-02)	Acc@1  97.66 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [42][140/391]	Time  0.185 ( 0.168)	Data  0.001 ( 0.006)	Loss 4.6994e-02 (6.9847e-02)	Acc@1 100.00 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [42][150/391]	Time  0.185 ( 0.169)	Data  0.001 ( 0.006)	Loss 5.1402e-02 (6.9590e-02)	Acc@1  99.22 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [42][160/391]	Time  0.177 ( 0.170)	Data  0.001 ( 0.006)	Loss 9.4712e-02 (6.9308e-02)	Acc@1  98.44 ( 98.38)	Acc@5 100.00 ( 99.98)
Epoch: [42][170/391]	Time  0.177 ( 0.170)	Data  0.001 ( 0.006)	Loss 7.3815e-02 (6.9532e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.97)
Epoch: [42][180/391]	Time  0.177 ( 0.170)	Data  0.001 ( 0.006)	Loss 6.6985e-02 (6.9569e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [42][190/391]	Time  0.185 ( 0.170)	Data  0.001 ( 0.006)	Loss 9.4760e-02 (6.9794e-02)	Acc@1  96.09 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [42][200/391]	Time  0.183 ( 0.171)	Data  0.001 ( 0.006)	Loss 4.6630e-02 (6.9778e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [42][210/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.006)	Loss 7.8775e-02 (7.0069e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [42][220/391]	Time  0.168 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.2345e-01 (7.0349e-02)	Acc@1  96.88 ( 98.29)	Acc@5  99.22 ( 99.98)
Epoch: [42][230/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.1936e-01 (7.0251e-02)	Acc@1  98.44 ( 98.31)	Acc@5  99.22 ( 99.97)
Epoch: [42][240/391]	Time  0.159 ( 0.171)	Data  0.001 ( 0.005)	Loss 5.1645e-02 (7.0255e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [42][250/391]	Time  0.156 ( 0.171)	Data  0.001 ( 0.005)	Loss 7.6350e-02 (7.0459e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [42][260/391]	Time  0.157 ( 0.170)	Data  0.001 ( 0.005)	Loss 6.3656e-02 (7.0308e-02)	Acc@1  99.22 ( 98.28)	Acc@5 100.00 ( 99.98)
Epoch: [42][270/391]	Time  0.155 ( 0.170)	Data  0.001 ( 0.005)	Loss 6.2596e-02 (7.0016e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [42][280/391]	Time  0.158 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.9313e-02 (6.9937e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [42][290/391]	Time  0.175 ( 0.169)	Data  0.001 ( 0.005)	Loss 6.5732e-02 (6.9774e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [42][300/391]	Time  0.167 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.3099e-01 (6.9686e-02)	Acc@1  95.31 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [42][310/391]	Time  0.169 ( 0.169)	Data  0.002 ( 0.005)	Loss 9.8455e-02 (6.9833e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [42][320/391]	Time  0.172 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.9717e-02 (6.9612e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [42][330/391]	Time  0.180 ( 0.170)	Data  0.001 ( 0.005)	Loss 6.1127e-02 (6.9795e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [42][340/391]	Time  0.178 ( 0.170)	Data  0.001 ( 0.005)	Loss 6.3593e-02 (7.0084e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [42][350/391]	Time  0.183 ( 0.170)	Data  0.001 ( 0.005)	Loss 6.3334e-02 (6.9866e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [42][360/391]	Time  0.180 ( 0.170)	Data  0.001 ( 0.005)	Loss 5.8608e-02 (6.9980e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [42][370/391]	Time  0.163 ( 0.171)	Data  0.002 ( 0.005)	Loss 6.6369e-02 (6.9969e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [42][380/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 5.2460e-02 (7.0043e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.98)
Epoch: [42][390/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.005)	Loss 7.1840e-02 (6.9932e-02)	Acc@1  96.25 ( 98.31)	Acc@5 100.00 ( 99.98)
## e[42] optimizer.zero_grad (sum) time: 0.3731067180633545
## e[42]       loss.backward (sum) time: 24.81843113899231
## e[42]      optimizer.step (sum) time: 3.664637565612793
## epoch[42] training(only) time: 66.93786883354187
# Switched to evaluate mode...
Test: [  0/100]	Time  0.281 ( 0.281)	Loss 1.3776e+00 (1.3776e+00)	Acc@1  70.00 ( 70.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.061 ( 0.080)	Loss 1.4953e+00 (1.3211e+00)	Acc@1  69.00 ( 73.36)	Acc@5  89.00 ( 91.91)
Test: [ 20/100]	Time  0.060 ( 0.070)	Loss 9.2237e-01 (1.1984e+00)	Acc@1  78.00 ( 74.81)	Acc@5  96.00 ( 93.38)
Test: [ 30/100]	Time  0.060 ( 0.067)	Loss 1.8772e+00 (1.2594e+00)	Acc@1  64.00 ( 73.84)	Acc@5  92.00 ( 93.29)
Test: [ 40/100]	Time  0.065 ( 0.067)	Loss 1.1970e+00 (1.2406e+00)	Acc@1  74.00 ( 73.80)	Acc@5  96.00 ( 93.56)
Test: [ 50/100]	Time  0.065 ( 0.066)	Loss 1.4270e+00 (1.2448e+00)	Acc@1  69.00 ( 73.55)	Acc@5  93.00 ( 93.24)
Test: [ 60/100]	Time  0.061 ( 0.066)	Loss 1.3756e+00 (1.2078e+00)	Acc@1  68.00 ( 73.98)	Acc@5  93.00 ( 93.39)
Test: [ 70/100]	Time  0.059 ( 0.065)	Loss 1.7053e+00 (1.2170e+00)	Acc@1  72.00 ( 73.87)	Acc@5  88.00 ( 93.42)
Test: [ 80/100]	Time  0.061 ( 0.065)	Loss 1.5185e+00 (1.2238e+00)	Acc@1  70.00 ( 73.73)	Acc@5  89.00 ( 93.30)
Test: [ 90/100]	Time  0.060 ( 0.065)	Loss 1.8254e+00 (1.2057e+00)	Acc@1  67.00 ( 73.80)	Acc@5  91.00 ( 93.41)
 * Acc@1 73.990 Acc@5 93.420
### epoch[42] execution time: 73.47999882698059
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.393 ( 0.393)	Data  0.200 ( 0.200)	Loss 3.4938e-02 (3.4938e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.166 ( 0.189)	Data  0.002 ( 0.021)	Loss 6.9343e-02 (6.9223e-02)	Acc@1  98.44 ( 98.51)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 20/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.013)	Loss 1.0971e-01 (7.0476e-02)	Acc@1  96.09 ( 98.40)	Acc@5 100.00 ( 99.93)
Epoch: [43][ 30/391]	Time  0.174 ( 0.176)	Data  0.001 ( 0.011)	Loss 5.7656e-02 (6.8159e-02)	Acc@1  97.66 ( 98.36)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 40/391]	Time  0.170 ( 0.176)	Data  0.001 ( 0.009)	Loss 1.2704e-01 (6.9169e-02)	Acc@1  96.09 ( 98.38)	Acc@5  99.22 ( 99.94)
Epoch: [43][ 50/391]	Time  0.175 ( 0.175)	Data  0.001 ( 0.008)	Loss 2.3389e-02 (7.0015e-02)	Acc@1 100.00 ( 98.35)	Acc@5 100.00 ( 99.95)
Epoch: [43][ 60/391]	Time  0.165 ( 0.175)	Data  0.002 ( 0.008)	Loss 9.4549e-02 (7.1434e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.96)
Epoch: [43][ 70/391]	Time  0.174 ( 0.175)	Data  0.001 ( 0.007)	Loss 6.4486e-02 (7.0350e-02)	Acc@1  99.22 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 80/391]	Time  0.170 ( 0.175)	Data  0.001 ( 0.007)	Loss 8.2738e-02 (6.9532e-02)	Acc@1  98.44 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [43][ 90/391]	Time  0.181 ( 0.175)	Data  0.001 ( 0.006)	Loss 4.6988e-02 (6.8841e-02)	Acc@1 100.00 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [43][100/391]	Time  0.177 ( 0.176)	Data  0.002 ( 0.006)	Loss 6.2013e-02 (6.8299e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [43][110/391]	Time  0.164 ( 0.176)	Data  0.001 ( 0.006)	Loss 8.2145e-02 (6.9216e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [43][120/391]	Time  0.173 ( 0.176)	Data  0.001 ( 0.006)	Loss 8.5628e-02 (6.9008e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [43][130/391]	Time  0.163 ( 0.175)	Data  0.001 ( 0.006)	Loss 4.9536e-02 (6.8415e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [43][140/391]	Time  0.185 ( 0.175)	Data  0.001 ( 0.006)	Loss 5.9807e-02 (6.9087e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [43][150/391]	Time  0.180 ( 0.176)	Data  0.001 ( 0.006)	Loss 5.8854e-02 (6.8866e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [43][160/391]	Time  0.178 ( 0.176)	Data  0.001 ( 0.006)	Loss 3.3822e-02 (6.8770e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [43][170/391]	Time  0.171 ( 0.176)	Data  0.002 ( 0.006)	Loss 5.4480e-02 (6.8608e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [43][180/391]	Time  0.171 ( 0.176)	Data  0.001 ( 0.006)	Loss 4.7556e-02 (6.7944e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [43][190/391]	Time  0.154 ( 0.175)	Data  0.001 ( 0.006)	Loss 3.8827e-02 (6.7963e-02)	Acc@1 100.00 ( 98.37)	Acc@5 100.00 ( 99.98)
Epoch: [43][200/391]	Time  0.167 ( 0.175)	Data  0.002 ( 0.006)	Loss 6.8895e-02 (6.8047e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [43][210/391]	Time  0.177 ( 0.175)	Data  0.001 ( 0.005)	Loss 5.8923e-02 (6.8465e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [43][220/391]	Time  0.177 ( 0.175)	Data  0.002 ( 0.005)	Loss 6.9852e-02 (6.8914e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [43][230/391]	Time  0.169 ( 0.175)	Data  0.001 ( 0.005)	Loss 7.0572e-02 (6.8689e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [43][240/391]	Time  0.177 ( 0.175)	Data  0.001 ( 0.005)	Loss 4.6580e-02 (6.8836e-02)	Acc@1 100.00 ( 98.32)	Acc@5 100.00 ( 99.99)
Epoch: [43][250/391]	Time  0.165 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.2150e-01 (6.9305e-02)	Acc@1  94.53 ( 98.29)	Acc@5 100.00 ( 99.99)
Epoch: [43][260/391]	Time  0.176 ( 0.175)	Data  0.002 ( 0.005)	Loss 8.4481e-02 (6.9157e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.99)
Epoch: [43][270/391]	Time  0.178 ( 0.175)	Data  0.001 ( 0.005)	Loss 6.7568e-02 (6.8767e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [43][280/391]	Time  0.176 ( 0.175)	Data  0.001 ( 0.005)	Loss 9.5585e-02 (6.8671e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [43][290/391]	Time  0.177 ( 0.175)	Data  0.002 ( 0.005)	Loss 5.5921e-02 (6.8548e-02)	Acc@1 100.00 ( 98.34)	Acc@5 100.00 ( 99.99)
Epoch: [43][300/391]	Time  0.176 ( 0.175)	Data  0.001 ( 0.005)	Loss 6.1492e-02 (6.8564e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.99)
Epoch: [43][310/391]	Time  0.184 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.0062e-01 (6.8654e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [43][320/391]	Time  0.173 ( 0.175)	Data  0.002 ( 0.005)	Loss 5.5945e-02 (6.8760e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [43][330/391]	Time  0.170 ( 0.175)	Data  0.002 ( 0.005)	Loss 5.0901e-02 (6.8710e-02)	Acc@1  98.44 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [43][340/391]	Time  0.165 ( 0.175)	Data  0.001 ( 0.005)	Loss 4.3900e-02 (6.8877e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [43][350/391]	Time  0.163 ( 0.175)	Data  0.002 ( 0.005)	Loss 5.6898e-02 (6.9042e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [43][360/391]	Time  0.155 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.0330e-01 (6.9142e-02)	Acc@1  96.88 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [43][370/391]	Time  0.167 ( 0.175)	Data  0.001 ( 0.005)	Loss 5.5133e-02 (6.8933e-02)	Acc@1  98.44 ( 98.33)	Acc@5 100.00 ( 99.98)
Epoch: [43][380/391]	Time  0.179 ( 0.175)	Data  0.001 ( 0.005)	Loss 6.1817e-02 (6.8532e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [43][390/391]	Time  0.171 ( 0.175)	Data  0.001 ( 0.005)	Loss 9.5028e-02 (6.8526e-02)	Acc@1  96.25 ( 98.35)	Acc@5 100.00 ( 99.98)
## e[43] optimizer.zero_grad (sum) time: 0.41100144386291504
## e[43]       loss.backward (sum) time: 26.723630666732788
## e[43]      optimizer.step (sum) time: 3.9374122619628906
## epoch[43] training(only) time: 68.42646551132202
# Switched to evaluate mode...
Test: [  0/100]	Time  0.249 ( 0.249)	Loss 1.4105e+00 (1.4105e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.058 ( 0.076)	Loss 1.6059e+00 (1.3450e+00)	Acc@1  68.00 ( 73.36)	Acc@5  89.00 ( 91.64)
Test: [ 20/100]	Time  0.057 ( 0.068)	Loss 9.6393e-01 (1.2193e+00)	Acc@1  76.00 ( 74.67)	Acc@5  96.00 ( 93.19)
Test: [ 30/100]	Time  0.059 ( 0.065)	Loss 1.8808e+00 (1.2751e+00)	Acc@1  63.00 ( 74.03)	Acc@5  91.00 ( 93.10)
Test: [ 40/100]	Time  0.058 ( 0.063)	Loss 1.1955e+00 (1.2554e+00)	Acc@1  75.00 ( 74.07)	Acc@5  97.00 ( 93.37)
Test: [ 50/100]	Time  0.059 ( 0.062)	Loss 1.5758e+00 (1.2629e+00)	Acc@1  66.00 ( 73.84)	Acc@5  92.00 ( 93.10)
Test: [ 60/100]	Time  0.058 ( 0.061)	Loss 1.3673e+00 (1.2237e+00)	Acc@1  67.00 ( 74.23)	Acc@5  92.00 ( 93.34)
Test: [ 70/100]	Time  0.057 ( 0.061)	Loss 1.7445e+00 (1.2322e+00)	Acc@1  72.00 ( 74.24)	Acc@5  90.00 ( 93.35)
Test: [ 80/100]	Time  0.058 ( 0.061)	Loss 1.6269e+00 (1.2420e+00)	Acc@1  68.00 ( 73.96)	Acc@5  90.00 ( 93.19)
Test: [ 90/100]	Time  0.054 ( 0.060)	Loss 1.7876e+00 (1.2222e+00)	Acc@1  67.00 ( 74.18)	Acc@5  88.00 ( 93.32)
 * Acc@1 74.360 Acc@5 93.300
### epoch[43] execution time: 74.51056671142578
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.452 ( 0.452)	Data  0.274 ( 0.274)	Loss 7.3134e-02 (7.3134e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.175 ( 0.202)	Data  0.002 ( 0.029)	Loss 5.0747e-02 (5.7868e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [44][ 20/391]	Time  0.172 ( 0.191)	Data  0.001 ( 0.017)	Loss 4.3539e-02 (5.4580e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [44][ 30/391]	Time  0.167 ( 0.184)	Data  0.002 ( 0.013)	Loss 5.2531e-02 (5.4071e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [44][ 40/391]	Time  0.166 ( 0.181)	Data  0.002 ( 0.011)	Loss 2.6845e-02 (5.3973e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [44][ 50/391]	Time  0.176 ( 0.178)	Data  0.002 ( 0.009)	Loss 5.1331e-02 (5.4495e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [44][ 60/391]	Time  0.169 ( 0.178)	Data  0.001 ( 0.009)	Loss 4.6226e-02 (5.6812e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 70/391]	Time  0.179 ( 0.179)	Data  0.001 ( 0.008)	Loss 3.1366e-02 (5.6077e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 80/391]	Time  0.167 ( 0.178)	Data  0.001 ( 0.008)	Loss 7.3705e-02 (5.5220e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 90/391]	Time  0.175 ( 0.177)	Data  0.001 ( 0.007)	Loss 6.5352e-02 (5.5047e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][100/391]	Time  0.164 ( 0.176)	Data  0.001 ( 0.007)	Loss 7.8649e-02 (5.4892e-02)	Acc@1  99.22 ( 98.83)	Acc@5  99.22 ( 99.98)
Epoch: [44][110/391]	Time  0.173 ( 0.176)	Data  0.001 ( 0.007)	Loss 8.2154e-02 (5.5336e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [44][120/391]	Time  0.168 ( 0.175)	Data  0.002 ( 0.006)	Loss 6.5031e-02 (5.6574e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [44][130/391]	Time  0.165 ( 0.175)	Data  0.002 ( 0.006)	Loss 3.6934e-02 (5.6437e-02)	Acc@1 100.00 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [44][140/391]	Time  0.170 ( 0.175)	Data  0.002 ( 0.006)	Loss 5.0894e-02 (5.7247e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [44][150/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.006)	Loss 7.3468e-02 (5.7665e-02)	Acc@1  97.66 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [44][160/391]	Time  0.168 ( 0.174)	Data  0.001 ( 0.006)	Loss 3.5131e-02 (5.7251e-02)	Acc@1 100.00 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [44][170/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.006)	Loss 5.3478e-02 (5.7397e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [44][180/391]	Time  0.179 ( 0.174)	Data  0.002 ( 0.006)	Loss 4.3607e-02 (5.7342e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [44][190/391]	Time  0.185 ( 0.175)	Data  0.001 ( 0.006)	Loss 4.0203e-02 (5.7697e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [44][200/391]	Time  0.166 ( 0.175)	Data  0.001 ( 0.006)	Loss 4.1748e-02 (5.7912e-02)	Acc@1 100.00 ( 98.76)	Acc@5 100.00 ( 99.98)
Epoch: [44][210/391]	Time  0.174 ( 0.175)	Data  0.001 ( 0.005)	Loss 7.8194e-02 (5.7671e-02)	Acc@1  97.66 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [44][220/391]	Time  0.166 ( 0.175)	Data  0.001 ( 0.005)	Loss 6.2309e-02 (5.8161e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [44][230/391]	Time  0.179 ( 0.175)	Data  0.002 ( 0.005)	Loss 6.1886e-02 (5.8409e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [44][240/391]	Time  0.180 ( 0.175)	Data  0.001 ( 0.005)	Loss 3.9779e-02 (5.8114e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 ( 99.99)
Epoch: [44][250/391]	Time  0.174 ( 0.175)	Data  0.002 ( 0.005)	Loss 4.5509e-02 (5.8046e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [44][260/391]	Time  0.173 ( 0.175)	Data  0.001 ( 0.005)	Loss 6.0456e-02 (5.7971e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [44][270/391]	Time  0.167 ( 0.175)	Data  0.001 ( 0.005)	Loss 8.1083e-02 (5.8130e-02)	Acc@1  96.88 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [44][280/391]	Time  0.187 ( 0.175)	Data  0.002 ( 0.005)	Loss 7.1301e-02 (5.8131e-02)	Acc@1  96.88 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [44][290/391]	Time  0.164 ( 0.175)	Data  0.002 ( 0.005)	Loss 3.6364e-02 (5.8251e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [44][300/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.005)	Loss 6.4692e-02 (5.8535e-02)	Acc@1  98.44 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [44][310/391]	Time  0.157 ( 0.174)	Data  0.001 ( 0.005)	Loss 6.9652e-02 (5.8793e-02)	Acc@1  98.44 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [44][320/391]	Time  0.162 ( 0.174)	Data  0.001 ( 0.005)	Loss 5.1398e-02 (5.8908e-02)	Acc@1  98.44 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [44][330/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 7.7796e-02 (5.8980e-02)	Acc@1  96.09 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [44][340/391]	Time  0.168 ( 0.174)	Data  0.001 ( 0.005)	Loss 6.2137e-02 (5.9217e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [44][350/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 5.7435e-02 (5.9124e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [44][360/391]	Time  0.180 ( 0.174)	Data  0.001 ( 0.005)	Loss 5.3336e-02 (5.9013e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [44][370/391]	Time  0.169 ( 0.174)	Data  0.002 ( 0.005)	Loss 7.5123e-02 (5.8964e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [44][380/391]	Time  0.167 ( 0.174)	Data  0.002 ( 0.005)	Loss 3.2379e-02 (5.9171e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [44][390/391]	Time  0.170 ( 0.174)	Data  0.001 ( 0.005)	Loss 8.8898e-02 (5.9264e-02)	Acc@1  96.25 ( 98.65)	Acc@5 100.00 ( 99.99)
## e[44] optimizer.zero_grad (sum) time: 0.42064380645751953
## e[44]       loss.backward (sum) time: 27.20708966255188
## e[44]      optimizer.step (sum) time: 3.9996211528778076
## epoch[44] training(only) time: 68.13358187675476
# Switched to evaluate mode...
Test: [  0/100]	Time  0.294 ( 0.294)	Loss 1.3726e+00 (1.3726e+00)	Acc@1  74.00 ( 74.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.062 ( 0.082)	Loss 1.6302e+00 (1.3391e+00)	Acc@1  68.00 ( 73.36)	Acc@5  91.00 ( 91.91)
Test: [ 20/100]	Time  0.059 ( 0.071)	Loss 1.0055e+00 (1.2183e+00)	Acc@1  77.00 ( 74.52)	Acc@5  96.00 ( 93.24)
Test: [ 30/100]	Time  0.058 ( 0.067)	Loss 1.8468e+00 (1.2729e+00)	Acc@1  65.00 ( 74.00)	Acc@5  90.00 ( 93.19)
Test: [ 40/100]	Time  0.059 ( 0.065)	Loss 1.2287e+00 (1.2603e+00)	Acc@1  74.00 ( 73.83)	Acc@5  96.00 ( 93.34)
Test: [ 50/100]	Time  0.058 ( 0.063)	Loss 1.5030e+00 (1.2628e+00)	Acc@1  68.00 ( 73.69)	Acc@5  94.00 ( 93.14)
Test: [ 60/100]	Time  0.059 ( 0.063)	Loss 1.3890e+00 (1.2256e+00)	Acc@1  65.00 ( 74.00)	Acc@5  93.00 ( 93.38)
Test: [ 70/100]	Time  0.057 ( 0.062)	Loss 1.6835e+00 (1.2348e+00)	Acc@1  72.00 ( 73.96)	Acc@5  89.00 ( 93.38)
Test: [ 80/100]	Time  0.058 ( 0.062)	Loss 1.6801e+00 (1.2462e+00)	Acc@1  65.00 ( 73.68)	Acc@5  91.00 ( 93.28)
Test: [ 90/100]	Time  0.061 ( 0.061)	Loss 1.8308e+00 (1.2250e+00)	Acc@1  66.00 ( 73.89)	Acc@5  89.00 ( 93.42)
 * Acc@1 74.100 Acc@5 93.440
### epoch[44] execution time: 74.34596276283264
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.383 ( 0.383)	Data  0.196 ( 0.196)	Loss 6.4277e-02 (6.4277e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.165 ( 0.186)	Data  0.002 ( 0.021)	Loss 6.2987e-02 (5.6852e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.154 ( 0.176)	Data  0.001 ( 0.013)	Loss 5.8706e-02 (5.5642e-02)	Acc@1  97.66 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.010)	Loss 6.8156e-02 (5.2499e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.179 ( 0.174)	Data  0.002 ( 0.009)	Loss 7.0285e-02 (5.2733e-02)	Acc@1  96.88 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.170 ( 0.173)	Data  0.002 ( 0.008)	Loss 1.1226e-01 (5.6083e-02)	Acc@1  94.53 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.007)	Loss 5.0344e-02 (5.6659e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 (100.00)
Epoch: [45][ 70/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.007)	Loss 3.8554e-02 (5.4890e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [45][ 80/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.4709e-02 (5.4837e-02)	Acc@1 100.00 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [45][ 90/391]	Time  0.180 ( 0.173)	Data  0.001 ( 0.006)	Loss 7.3842e-02 (5.4694e-02)	Acc@1  96.88 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [45][100/391]	Time  0.170 ( 0.174)	Data  0.002 ( 0.006)	Loss 4.5162e-02 (5.4330e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [45][110/391]	Time  0.179 ( 0.174)	Data  0.001 ( 0.006)	Loss 4.0556e-02 (5.4854e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 (100.00)
Epoch: [45][120/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.006)	Loss 3.7198e-02 (5.4332e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [45][130/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.006)	Loss 3.2996e-02 (5.4207e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [45][140/391]	Time  0.166 ( 0.173)	Data  0.002 ( 0.006)	Loss 4.2467e-02 (5.4377e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [45][150/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5976e-01 (5.5507e-02)	Acc@1  92.97 ( 98.70)	Acc@5 100.00 ( 99.98)
Epoch: [45][160/391]	Time  0.176 ( 0.174)	Data  0.001 ( 0.005)	Loss 6.0009e-02 (5.6511e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 ( 99.98)
Epoch: [45][170/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.005)	Loss 6.8883e-02 (5.6463e-02)	Acc@1  97.66 ( 98.67)	Acc@5 100.00 ( 99.98)
Epoch: [45][180/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.005)	Loss 5.9075e-02 (5.6298e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 ( 99.98)
Epoch: [45][190/391]	Time  0.151 ( 0.173)	Data  0.001 ( 0.005)	Loss 7.4415e-02 (5.6448e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 ( 99.98)
Epoch: [45][200/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.005)	Loss 3.7112e-02 (5.6722e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 ( 99.98)
Epoch: [45][210/391]	Time  0.182 ( 0.173)	Data  0.001 ( 0.005)	Loss 5.6066e-02 (5.6327e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [45][220/391]	Time  0.161 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.1580e-02 (5.6802e-02)	Acc@1 100.00 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [45][230/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.005)	Loss 6.3867e-02 (5.6783e-02)	Acc@1  99.22 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [45][240/391]	Time  0.167 ( 0.172)	Data  0.002 ( 0.005)	Loss 5.9075e-02 (5.6873e-02)	Acc@1  97.66 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [45][250/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.2912e-02 (5.6571e-02)	Acc@1 100.00 ( 98.66)	Acc@5 100.00 ( 99.98)
Epoch: [45][260/391]	Time  0.174 ( 0.172)	Data  0.002 ( 0.005)	Loss 4.8484e-02 (5.6544e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.98)
Epoch: [45][270/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.9554e-02 (5.6328e-02)	Acc@1  98.44 ( 98.67)	Acc@5 100.00 ( 99.98)
Epoch: [45][280/391]	Time  0.183 ( 0.173)	Data  0.001 ( 0.005)	Loss 6.4635e-02 (5.6347e-02)	Acc@1  99.22 ( 98.67)	Acc@5 100.00 ( 99.98)
Epoch: [45][290/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.005)	Loss 3.9663e-02 (5.6081e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.98)
Epoch: [45][300/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.7879e-02 (5.5887e-02)	Acc@1  97.66 ( 98.70)	Acc@5 100.00 ( 99.98)
Epoch: [45][310/391]	Time  0.173 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.9809e-02 (5.5883e-02)	Acc@1 100.00 ( 98.71)	Acc@5 100.00 ( 99.98)
Epoch: [45][320/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.005)	Loss 7.3751e-02 (5.6296e-02)	Acc@1  96.88 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [45][330/391]	Time  0.178 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.6168e-02 (5.6397e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [45][340/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.6609e-02 (5.6322e-02)	Acc@1 100.00 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [45][350/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.4387e-02 (5.6373e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [45][360/391]	Time  0.153 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.9762e-02 (5.6312e-02)	Acc@1 100.00 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [45][370/391]	Time  0.172 ( 0.172)	Data  0.002 ( 0.005)	Loss 4.2867e-02 (5.6216e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [45][380/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.9831e-02 (5.6518e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [45][390/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.0063e-02 (5.6579e-02)	Acc@1  96.25 ( 98.67)	Acc@5 100.00 ( 99.99)
## e[45] optimizer.zero_grad (sum) time: 0.4067370891571045
## e[45]       loss.backward (sum) time: 26.509385585784912
## e[45]      optimizer.step (sum) time: 3.8869733810424805
## epoch[45] training(only) time: 67.40057420730591
# Switched to evaluate mode...
Test: [  0/100]	Time  0.316 ( 0.316)	Loss 1.4274e+00 (1.4274e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.056 ( 0.081)	Loss 1.5338e+00 (1.3331e+00)	Acc@1  68.00 ( 72.91)	Acc@5  91.00 ( 91.91)
Test: [ 20/100]	Time  0.055 ( 0.069)	Loss 9.1968e-01 (1.2128e+00)	Acc@1  80.00 ( 74.90)	Acc@5  97.00 ( 93.38)
Test: [ 30/100]	Time  0.054 ( 0.064)	Loss 1.9100e+00 (1.2771e+00)	Acc@1  62.00 ( 73.87)	Acc@5  91.00 ( 93.23)
Test: [ 40/100]	Time  0.055 ( 0.062)	Loss 1.2247e+00 (1.2658e+00)	Acc@1  73.00 ( 73.68)	Acc@5  96.00 ( 93.46)
Test: [ 50/100]	Time  0.056 ( 0.061)	Loss 1.5666e+00 (1.2731e+00)	Acc@1  66.00 ( 73.47)	Acc@5  92.00 ( 93.12)
Test: [ 60/100]	Time  0.054 ( 0.060)	Loss 1.3288e+00 (1.2309e+00)	Acc@1  70.00 ( 73.87)	Acc@5  95.00 ( 93.41)
Test: [ 70/100]	Time  0.054 ( 0.059)	Loss 1.7205e+00 (1.2439e+00)	Acc@1  69.00 ( 73.73)	Acc@5  88.00 ( 93.39)
Test: [ 80/100]	Time  0.057 ( 0.059)	Loss 1.5932e+00 (1.2525e+00)	Acc@1  69.00 ( 73.48)	Acc@5  89.00 ( 93.23)
Test: [ 90/100]	Time  0.058 ( 0.059)	Loss 1.7444e+00 (1.2322e+00)	Acc@1  66.00 ( 73.68)	Acc@5  91.00 ( 93.41)
 * Acc@1 73.930 Acc@5 93.410
### epoch[45] execution time: 73.3706066608429
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.442 ( 0.442)	Data  0.250 ( 0.250)	Loss 3.1160e-02 (3.1160e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.178 ( 0.202)	Data  0.001 ( 0.026)	Loss 3.1769e-02 (4.7377e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.174 ( 0.191)	Data  0.001 ( 0.016)	Loss 4.9385e-02 (5.5377e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.168 ( 0.184)	Data  0.002 ( 0.012)	Loss 4.5005e-02 (4.9929e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [46][ 40/391]	Time  0.161 ( 0.180)	Data  0.001 ( 0.010)	Loss 2.7021e-02 (4.8818e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [46][ 50/391]	Time  0.178 ( 0.178)	Data  0.001 ( 0.009)	Loss 8.7516e-02 (4.8681e-02)	Acc@1  96.88 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [46][ 60/391]	Time  0.174 ( 0.177)	Data  0.001 ( 0.008)	Loss 3.9876e-02 (4.8467e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [46][ 70/391]	Time  0.169 ( 0.177)	Data  0.001 ( 0.008)	Loss 8.6077e-02 (4.9497e-02)	Acc@1  96.09 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [46][ 80/391]	Time  0.167 ( 0.176)	Data  0.001 ( 0.007)	Loss 3.3558e-02 (5.0122e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [46][ 90/391]	Time  0.169 ( 0.176)	Data  0.001 ( 0.007)	Loss 4.2754e-02 (5.0387e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [46][100/391]	Time  0.163 ( 0.175)	Data  0.001 ( 0.007)	Loss 4.4994e-02 (5.1159e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [46][110/391]	Time  0.165 ( 0.174)	Data  0.002 ( 0.007)	Loss 3.6939e-02 (5.0660e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [46][120/391]	Time  0.166 ( 0.174)	Data  0.002 ( 0.006)	Loss 3.9311e-02 (5.0216e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [46][130/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.006)	Loss 4.7894e-02 (5.0605e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [46][140/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.6918e-02 (5.0626e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [46][150/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.8371e-02 (5.0560e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [46][160/391]	Time  0.164 ( 0.173)	Data  0.001 ( 0.006)	Loss 6.7130e-02 (5.0667e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.179 ( 0.173)	Data  0.002 ( 0.006)	Loss 3.3152e-02 (5.0899e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.184 ( 0.173)	Data  0.002 ( 0.006)	Loss 6.5193e-02 (5.1088e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.182 ( 0.174)	Data  0.001 ( 0.006)	Loss 7.9350e-02 (5.0861e-02)	Acc@1  97.66 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.163 ( 0.173)	Data  0.002 ( 0.006)	Loss 4.9064e-02 (5.1264e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.006)	Loss 3.4507e-02 (5.0903e-02)	Acc@1  99.22 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.180 ( 0.173)	Data  0.001 ( 0.005)	Loss 5.9868e-02 (5.0950e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.0131e-02 (5.0795e-02)	Acc@1 100.00 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.4473e-02 (5.0846e-02)	Acc@1 100.00 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [46][250/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.3086e-02 (5.1092e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [46][260/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 6.0138e-02 (5.1217e-02)	Acc@1  98.44 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [46][270/391]	Time  0.159 ( 0.173)	Data  0.002 ( 0.005)	Loss 3.3462e-02 (5.1091e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [46][280/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 8.2555e-02 (5.0838e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 (100.00)
Epoch: [46][290/391]	Time  0.177 ( 0.173)	Data  0.002 ( 0.005)	Loss 3.9871e-02 (5.0434e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 (100.00)
Epoch: [46][300/391]	Time  0.175 ( 0.173)	Data  0.002 ( 0.005)	Loss 8.6663e-02 (5.0548e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 (100.00)
Epoch: [46][310/391]	Time  0.161 ( 0.173)	Data  0.001 ( 0.005)	Loss 5.4423e-02 (5.1021e-02)	Acc@1  99.22 ( 98.89)	Acc@5 100.00 (100.00)
Epoch: [46][320/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.005)	Loss 5.5014e-02 (5.1058e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [46][330/391]	Time  0.157 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.8200e-02 (5.1308e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [46][340/391]	Time  0.182 ( 0.172)	Data  0.001 ( 0.005)	Loss 6.2674e-02 (5.1597e-02)	Acc@1  98.44 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [46][350/391]	Time  0.180 ( 0.173)	Data  0.001 ( 0.005)	Loss 6.5957e-02 (5.1518e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [46][360/391]	Time  0.171 ( 0.173)	Data  0.002 ( 0.005)	Loss 6.6008e-02 (5.1552e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [46][370/391]	Time  0.167 ( 0.173)	Data  0.002 ( 0.005)	Loss 5.3184e-02 (5.1550e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [46][380/391]	Time  0.169 ( 0.173)	Data  0.002 ( 0.005)	Loss 5.2956e-02 (5.1551e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [46][390/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.1840e-01 (5.1978e-02)	Acc@1  96.25 ( 98.86)	Acc@5 100.00 ( 99.99)
## e[46] optimizer.zero_grad (sum) time: 0.40006136894226074
## e[46]       loss.backward (sum) time: 26.255066633224487
## e[46]      optimizer.step (sum) time: 3.8248207569122314
## epoch[46] training(only) time: 67.62650799751282
# Switched to evaluate mode...
Test: [  0/100]	Time  0.222 ( 0.222)	Loss 1.4238e+00 (1.4238e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.056 ( 0.070)	Loss 1.6010e+00 (1.3494e+00)	Acc@1  67.00 ( 73.18)	Acc@5  91.00 ( 92.09)
Test: [ 20/100]	Time  0.054 ( 0.063)	Loss 9.1262e-01 (1.2255e+00)	Acc@1  80.00 ( 75.43)	Acc@5  95.00 ( 92.95)
Test: [ 30/100]	Time  0.055 ( 0.061)	Loss 1.9197e+00 (1.2856e+00)	Acc@1  63.00 ( 74.45)	Acc@5  90.00 ( 92.84)
Test: [ 40/100]	Time  0.055 ( 0.059)	Loss 1.2495e+00 (1.2724e+00)	Acc@1  74.00 ( 74.27)	Acc@5  96.00 ( 93.12)
Test: [ 50/100]	Time  0.054 ( 0.058)	Loss 1.5563e+00 (1.2788e+00)	Acc@1  63.00 ( 73.86)	Acc@5  92.00 ( 92.92)
Test: [ 60/100]	Time  0.053 ( 0.058)	Loss 1.3576e+00 (1.2383e+00)	Acc@1  70.00 ( 74.21)	Acc@5  93.00 ( 93.16)
Test: [ 70/100]	Time  0.053 ( 0.057)	Loss 1.7775e+00 (1.2513e+00)	Acc@1  70.00 ( 74.11)	Acc@5  89.00 ( 93.21)
Test: [ 80/100]	Time  0.053 ( 0.057)	Loss 1.6091e+00 (1.2607e+00)	Acc@1  71.00 ( 73.85)	Acc@5  90.00 ( 93.14)
Test: [ 90/100]	Time  0.055 ( 0.056)	Loss 1.7510e+00 (1.2419e+00)	Acc@1  67.00 ( 73.96)	Acc@5  91.00 ( 93.25)
 * Acc@1 74.190 Acc@5 93.270
### epoch[46] execution time: 73.34984731674194
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.365 ( 0.365)	Data  0.216 ( 0.216)	Loss 4.5036e-02 (4.5036e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.174 ( 0.188)	Data  0.002 ( 0.023)	Loss 4.5228e-02 (4.5065e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [47][ 20/391]	Time  0.163 ( 0.180)	Data  0.001 ( 0.014)	Loss 3.8338e-02 (4.5836e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [47][ 30/391]	Time  0.163 ( 0.178)	Data  0.001 ( 0.011)	Loss 3.9066e-02 (4.6112e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [47][ 40/391]	Time  0.167 ( 0.176)	Data  0.002 ( 0.009)	Loss 4.4717e-02 (4.3570e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [47][ 50/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.008)	Loss 5.1449e-02 (4.4529e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [47][ 60/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.008)	Loss 4.7288e-02 (4.5674e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [47][ 70/391]	Time  0.182 ( 0.172)	Data  0.001 ( 0.007)	Loss 3.9357e-02 (4.6898e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][ 80/391]	Time  0.184 ( 0.173)	Data  0.002 ( 0.007)	Loss 3.1275e-02 (4.5879e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [47][ 90/391]	Time  0.180 ( 0.174)	Data  0.001 ( 0.007)	Loss 4.2525e-02 (4.6322e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [47][100/391]	Time  0.168 ( 0.174)	Data  0.002 ( 0.006)	Loss 2.9520e-02 (4.7271e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][110/391]	Time  0.177 ( 0.174)	Data  0.001 ( 0.006)	Loss 3.7749e-02 (4.7347e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][120/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.6540e-02 (4.7329e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][130/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.006)	Loss 8.0202e-02 (4.7586e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][140/391]	Time  0.173 ( 0.173)	Data  0.002 ( 0.006)	Loss 2.6665e-02 (4.7766e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [47][150/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.3016e-02 (4.7778e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][160/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.5737e-02 (4.7393e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][170/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.0002e-02 (4.7161e-02)	Acc@1  98.44 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [47][180/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.5271e-02 (4.7248e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [47][190/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.006)	Loss 7.1407e-02 (4.7680e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [47][200/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.9490e-02 (4.8303e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][210/391]	Time  0.162 ( 0.172)	Data  0.002 ( 0.005)	Loss 4.9421e-02 (4.8429e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][220/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.5734e-02 (4.8235e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [47][230/391]	Time  0.158 ( 0.172)	Data  0.001 ( 0.005)	Loss 7.0805e-02 (4.8137e-02)	Acc@1  97.66 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][240/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 6.1601e-02 (4.8251e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][250/391]	Time  0.172 ( 0.172)	Data  0.002 ( 0.005)	Loss 7.2494e-02 (4.8415e-02)	Acc@1  96.88 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [47][260/391]	Time  0.181 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.5358e-02 (4.8186e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [47][270/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.8399e-02 (4.8042e-02)	Acc@1 100.00 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [47][280/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.3043e-02 (4.8336e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [47][290/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.7746e-02 (4.8356e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][300/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.9189e-02 (4.8339e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][310/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.6760e-02 (4.8253e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [47][320/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.4625e-02 (4.8292e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][330/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.7800e-02 (4.8317e-02)	Acc@1  99.22 ( 98.98)	Acc@5 100.00 (100.00)
Epoch: [47][340/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.1698e-02 (4.8588e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [47][350/391]	Time  0.172 ( 0.172)	Data  0.002 ( 0.005)	Loss 6.2095e-02 (4.8851e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [47][360/391]	Time  0.180 ( 0.172)	Data  0.002 ( 0.005)	Loss 7.5149e-02 (4.9036e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][370/391]	Time  0.167 ( 0.172)	Data  0.002 ( 0.005)	Loss 2.5045e-02 (4.8963e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [47][380/391]	Time  0.156 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.2584e-02 (4.8879e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [47][390/391]	Time  0.162 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.3284e-01 (4.9265e-02)	Acc@1  96.25 ( 98.93)	Acc@5 100.00 (100.00)
## e[47] optimizer.zero_grad (sum) time: 0.3944244384765625
## e[47]       loss.backward (sum) time: 25.886067152023315
## e[47]      optimizer.step (sum) time: 3.8101987838745117
## epoch[47] training(only) time: 67.1078417301178
# Switched to evaluate mode...
Test: [  0/100]	Time  0.247 ( 0.247)	Loss 1.4111e+00 (1.4111e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.055 ( 0.072)	Loss 1.7300e+00 (1.3691e+00)	Acc@1  70.00 ( 73.64)	Acc@5  89.00 ( 92.00)
Test: [ 20/100]	Time  0.054 ( 0.064)	Loss 9.1881e-01 (1.2400e+00)	Acc@1  78.00 ( 74.71)	Acc@5  97.00 ( 93.52)
Test: [ 30/100]	Time  0.054 ( 0.061)	Loss 1.9295e+00 (1.2972e+00)	Acc@1  63.00 ( 73.71)	Acc@5  91.00 ( 93.35)
Test: [ 40/100]	Time  0.053 ( 0.059)	Loss 1.2352e+00 (1.2828e+00)	Acc@1  77.00 ( 73.83)	Acc@5  97.00 ( 93.54)
Test: [ 50/100]	Time  0.057 ( 0.058)	Loss 1.6439e+00 (1.2926e+00)	Acc@1  65.00 ( 73.61)	Acc@5  92.00 ( 93.18)
Test: [ 60/100]	Time  0.057 ( 0.058)	Loss 1.2940e+00 (1.2502e+00)	Acc@1  70.00 ( 73.93)	Acc@5  92.00 ( 93.33)
Test: [ 70/100]	Time  0.057 ( 0.058)	Loss 1.7955e+00 (1.2607e+00)	Acc@1  66.00 ( 73.90)	Acc@5  90.00 ( 93.34)
Test: [ 80/100]	Time  0.059 ( 0.058)	Loss 1.6387e+00 (1.2706e+00)	Acc@1  69.00 ( 73.73)	Acc@5  92.00 ( 93.23)
Test: [ 90/100]	Time  0.060 ( 0.058)	Loss 1.7572e+00 (1.2518e+00)	Acc@1  65.00 ( 73.92)	Acc@5  91.00 ( 93.36)
 * Acc@1 74.180 Acc@5 93.370
### epoch[47] execution time: 73.0764479637146
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.378 ( 0.378)	Data  0.191 ( 0.191)	Loss 3.1310e-02 (3.1310e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.162 ( 0.188)	Data  0.001 ( 0.021)	Loss 4.4363e-02 (4.5065e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.171 ( 0.182)	Data  0.002 ( 0.013)	Loss 4.1016e-02 (4.2608e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.163 ( 0.178)	Data  0.002 ( 0.010)	Loss 1.8102e-02 (4.0511e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.173 ( 0.177)	Data  0.001 ( 0.008)	Loss 8.1612e-02 (4.1870e-02)	Acc@1  97.66 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.173 ( 0.176)	Data  0.001 ( 0.008)	Loss 3.0641e-02 (4.0564e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.163 ( 0.174)	Data  0.001 ( 0.007)	Loss 3.4288e-02 (4.0896e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.164 ( 0.173)	Data  0.001 ( 0.007)	Loss 5.5312e-02 (4.1633e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.158 ( 0.172)	Data  0.001 ( 0.007)	Loss 5.6184e-02 (4.2433e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.007)	Loss 2.5435e-02 (4.2385e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.165 ( 0.173)	Data  0.002 ( 0.006)	Loss 2.3293e-02 (4.2509e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.006)	Loss 5.6449e-02 (4.3111e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [48][120/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.7973e-02 (4.2926e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [48][130/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.006)	Loss 4.9759e-02 (4.2393e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [48][140/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.006)	Loss 4.4049e-02 (4.2250e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 ( 99.99)
Epoch: [48][150/391]	Time  0.178 ( 0.172)	Data  0.001 ( 0.006)	Loss 9.1669e-02 (4.3488e-02)	Acc@1  96.88 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [48][160/391]	Time  0.180 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.5651e-02 (4.3510e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.3750e-02 (4.3480e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.6658e-02 (4.3379e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.2973e-02 (4.3970e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.172 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.2345e-02 (4.3780e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 6.9318e-02 (4.3836e-02)	Acc@1  97.66 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.6280e-02 (4.4132e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.8201e-02 (4.4051e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.162 ( 0.171)	Data  0.001 ( 0.005)	Loss 6.1764e-02 (4.4322e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.175 ( 0.171)	Data  0.002 ( 0.005)	Loss 5.3936e-02 (4.4442e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 8.9834e-02 (4.4588e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.4409e-02 (4.5133e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [48][280/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.7367e-02 (4.5079e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 ( 99.99)
Epoch: [48][290/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.4720e-02 (4.5153e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [48][300/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.0842e-02 (4.5176e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 ( 99.99)
Epoch: [48][310/391]	Time  0.177 ( 0.171)	Data  0.002 ( 0.005)	Loss 4.1485e-02 (4.4989e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [48][320/391]	Time  0.181 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.9858e-02 (4.4959e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.187 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.9070e-02 (4.5268e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.3109e-02 (4.5295e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.177 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.4399e-02 (4.5558e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.1562e-02 (4.5890e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [48][370/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.7638e-02 (4.6126e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [48][380/391]	Time  0.172 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.4615e-02 (4.6082e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [48][390/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.5409e-02 (4.6098e-02)	Acc@1  97.50 ( 99.07)	Acc@5 100.00 ( 99.99)
## e[48] optimizer.zero_grad (sum) time: 0.3870584964752197
## e[48]       loss.backward (sum) time: 25.32676124572754
## e[48]      optimizer.step (sum) time: 3.731128692626953
## epoch[48] training(only) time: 67.2621054649353
# Switched to evaluate mode...
Test: [  0/100]	Time  0.229 ( 0.229)	Loss 1.4040e+00 (1.4040e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.056 ( 0.072)	Loss 1.6522e+00 (1.3613e+00)	Acc@1  70.00 ( 73.64)	Acc@5  90.00 ( 92.36)
Test: [ 20/100]	Time  0.056 ( 0.064)	Loss 9.2950e-01 (1.2454e+00)	Acc@1  79.00 ( 74.29)	Acc@5  97.00 ( 93.29)
Test: [ 30/100]	Time  0.055 ( 0.061)	Loss 1.9665e+00 (1.3012e+00)	Acc@1  63.00 ( 73.52)	Acc@5  92.00 ( 93.26)
Test: [ 40/100]	Time  0.054 ( 0.060)	Loss 1.2705e+00 (1.2914e+00)	Acc@1  74.00 ( 73.54)	Acc@5  97.00 ( 93.59)
Test: [ 50/100]	Time  0.058 ( 0.058)	Loss 1.5974e+00 (1.3001e+00)	Acc@1  67.00 ( 73.39)	Acc@5  93.00 ( 93.27)
Test: [ 60/100]	Time  0.060 ( 0.059)	Loss 1.4205e+00 (1.2599e+00)	Acc@1  69.00 ( 73.82)	Acc@5  93.00 ( 93.41)
Test: [ 70/100]	Time  0.061 ( 0.059)	Loss 1.9354e+00 (1.2723e+00)	Acc@1  67.00 ( 73.65)	Acc@5  90.00 ( 93.51)
Test: [ 80/100]	Time  0.063 ( 0.059)	Loss 1.7146e+00 (1.2853e+00)	Acc@1  69.00 ( 73.38)	Acc@5  91.00 ( 93.35)
Test: [ 90/100]	Time  0.060 ( 0.059)	Loss 1.7843e+00 (1.2640e+00)	Acc@1  64.00 ( 73.53)	Acc@5  91.00 ( 93.54)
 * Acc@1 73.750 Acc@5 93.540
### epoch[48] execution time: 73.30280756950378
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.376 ( 0.376)	Data  0.209 ( 0.209)	Loss 2.9277e-02 (2.9277e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.163 ( 0.191)	Data  0.001 ( 0.023)	Loss 4.2084e-02 (4.0294e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.164 ( 0.179)	Data  0.001 ( 0.014)	Loss 7.4078e-02 (4.5156e-02)	Acc@1  98.44 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.011)	Loss 3.8516e-02 (4.4401e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.185 ( 0.173)	Data  0.001 ( 0.009)	Loss 6.6819e-02 (4.5578e-02)	Acc@1  97.66 ( 98.95)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.008)	Loss 3.2183e-02 (4.3437e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.180 ( 0.175)	Data  0.001 ( 0.008)	Loss 5.4442e-02 (4.3109e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.168 ( 0.175)	Data  0.001 ( 0.007)	Loss 4.1085e-02 (4.2722e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.007)	Loss 3.6713e-02 (4.2386e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.162 ( 0.173)	Data  0.002 ( 0.007)	Loss 2.6644e-02 (4.1654e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.9645e-02 (4.1620e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.160 ( 0.172)	Data  0.001 ( 0.006)	Loss 4.8005e-02 (4.1734e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.155 ( 0.172)	Data  0.001 ( 0.006)	Loss 4.6575e-02 (4.1747e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.006)	Loss 3.4785e-02 (4.1836e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.006)	Loss 5.9746e-02 (4.2074e-02)	Acc@1  97.66 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.167 ( 0.170)	Data  0.001 ( 0.006)	Loss 6.4955e-02 (4.2354e-02)	Acc@1  97.66 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.187 ( 0.171)	Data  0.001 ( 0.006)	Loss 4.3643e-02 (4.2017e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.177 ( 0.171)	Data  0.001 ( 0.006)	Loss 4.0289e-02 (4.1726e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.165 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.6139e-02 (4.1932e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.168 ( 0.170)	Data  0.001 ( 0.005)	Loss 5.8319e-02 (4.1779e-02)	Acc@1  98.44 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.166 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.6586e-02 (4.2054e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.180 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.2077e-02 (4.1789e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.180 ( 0.171)	Data  0.001 ( 0.005)	Loss 4.8610e-02 (4.2265e-02)	Acc@1  97.66 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.177 ( 0.171)	Data  0.002 ( 0.005)	Loss 5.8565e-02 (4.2429e-02)	Acc@1  97.66 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.7296e-02 (4.2319e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.005)	Loss 4.3512e-02 (4.2541e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.155 ( 0.171)	Data  0.002 ( 0.005)	Loss 6.8158e-02 (4.2829e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 ( 99.99)
Epoch: [49][270/391]	Time  0.170 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.3868e-02 (4.3019e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [49][280/391]	Time  0.170 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.0736e-02 (4.3002e-02)	Acc@1  98.44 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [49][290/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.3412e-02 (4.3155e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 ( 99.99)
Epoch: [49][300/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.1296e-02 (4.3231e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [49][310/391]	Time  0.193 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.5274e-02 (4.3352e-02)	Acc@1  98.44 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [49][320/391]	Time  0.174 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.7963e-02 (4.3336e-02)	Acc@1 100.00 ( 99.06)	Acc@5 100.00 ( 99.99)
Epoch: [49][330/391]	Time  0.172 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.7787e-02 (4.3583e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [49][340/391]	Time  0.152 ( 0.170)	Data  0.001 ( 0.005)	Loss 5.1812e-02 (4.3670e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [49][350/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.4154e-02 (4.3775e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [49][360/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.005)	Loss 5.8294e-02 (4.3885e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [49][370/391]	Time  0.182 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.6607e-02 (4.3921e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [49][380/391]	Time  0.172 ( 0.170)	Data  0.002 ( 0.005)	Loss 8.5179e-02 (4.4185e-02)	Acc@1  98.44 ( 99.02)	Acc@5 100.00 ( 99.99)
Epoch: [49][390/391]	Time  0.176 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.1138e-02 (4.4010e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
## e[49] optimizer.zero_grad (sum) time: 0.38187098503112793
## e[49]       loss.backward (sum) time: 24.969828605651855
## e[49]      optimizer.step (sum) time: 3.683548927307129
## epoch[49] training(only) time: 66.68762183189392
# Switched to evaluate mode...
Test: [  0/100]	Time  0.288 ( 0.288)	Loss 1.4241e+00 (1.4241e+00)	Acc@1  74.00 ( 74.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.060 ( 0.081)	Loss 1.7109e+00 (1.3846e+00)	Acc@1  69.00 ( 73.64)	Acc@5  91.00 ( 92.64)
Test: [ 20/100]	Time  0.059 ( 0.071)	Loss 9.2598e-01 (1.2553e+00)	Acc@1  79.00 ( 74.86)	Acc@5  96.00 ( 93.52)
Test: [ 30/100]	Time  0.059 ( 0.068)	Loss 1.9676e+00 (1.3156e+00)	Acc@1  63.00 ( 73.68)	Acc@5  91.00 ( 93.32)
Test: [ 40/100]	Time  0.059 ( 0.066)	Loss 1.2969e+00 (1.2997e+00)	Acc@1  76.00 ( 73.85)	Acc@5  96.00 ( 93.59)
Test: [ 50/100]	Time  0.059 ( 0.065)	Loss 1.6556e+00 (1.3042e+00)	Acc@1  66.00 ( 73.57)	Acc@5  91.00 ( 93.25)
Test: [ 60/100]	Time  0.058 ( 0.064)	Loss 1.3869e+00 (1.2666e+00)	Acc@1  70.00 ( 73.85)	Acc@5  93.00 ( 93.38)
Test: [ 70/100]	Time  0.060 ( 0.063)	Loss 1.8431e+00 (1.2776e+00)	Acc@1  70.00 ( 73.85)	Acc@5  91.00 ( 93.45)
Test: [ 80/100]	Time  0.059 ( 0.063)	Loss 1.7566e+00 (1.2880e+00)	Acc@1  68.00 ( 73.58)	Acc@5  91.00 ( 93.37)
Test: [ 90/100]	Time  0.053 ( 0.062)	Loss 1.8933e+00 (1.2696e+00)	Acc@1  64.00 ( 73.67)	Acc@5  89.00 ( 93.49)
 * Acc@1 73.910 Acc@5 93.510
### epoch[49] execution time: 72.89224648475647
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.354 ( 0.354)	Data  0.204 ( 0.204)	Loss 3.0815e-02 (3.0815e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.170 ( 0.185)	Data  0.001 ( 0.023)	Loss 3.5537e-02 (4.9303e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.164 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.8661e-02 (4.5547e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.011)	Loss 7.3764e-02 (4.5647e-02)	Acc@1  97.66 ( 98.94)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.202 ( 0.170)	Data  0.001 ( 0.010)	Loss 2.1388e-02 (4.3265e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.009)	Loss 5.7970e-02 (4.2273e-02)	Acc@1  97.66 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.008)	Loss 3.1391e-02 (4.3096e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.007)	Loss 3.0858e-02 (4.3517e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.007)	Loss 3.0938e-02 (4.1842e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.007)	Loss 3.3395e-02 (4.1237e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [50][100/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.007)	Loss 2.7056e-02 (4.0758e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][110/391]	Time  0.183 ( 0.169)	Data  0.001 ( 0.006)	Loss 4.8923e-02 (4.1073e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][120/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.006)	Loss 5.3874e-02 (4.1378e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][130/391]	Time  0.175 ( 0.171)	Data  0.002 ( 0.006)	Loss 2.3948e-02 (4.1054e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [50][140/391]	Time  0.172 ( 0.171)	Data  0.002 ( 0.006)	Loss 4.2233e-02 (4.1482e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [50][150/391]	Time  0.170 ( 0.171)	Data  0.002 ( 0.006)	Loss 3.2901e-02 (4.1658e-02)	Acc@1  99.22 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [50][160/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.006)	Loss 6.3975e-02 (4.1702e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.169 ( 0.171)	Data  0.002 ( 0.006)	Loss 4.1412e-02 (4.1704e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.006)	Loss 3.3439e-02 (4.1803e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.7533e-02 (4.1461e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.166 ( 0.170)	Data  0.002 ( 0.006)	Loss 8.4969e-02 (4.1759e-02)	Acc@1  96.09 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.172 ( 0.170)	Data  0.002 ( 0.006)	Loss 2.6964e-02 (4.1514e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.177 ( 0.170)	Data  0.001 ( 0.006)	Loss 4.3896e-02 (4.1417e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.8191e-02 (4.1298e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.166 ( 0.170)	Data  0.001 ( 0.005)	Loss 5.0452e-02 (4.1173e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.2790e-02 (4.0965e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.005)	Loss 6.1817e-02 (4.0936e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][270/391]	Time  0.183 ( 0.170)	Data  0.001 ( 0.005)	Loss 5.0450e-02 (4.1040e-02)	Acc@1  96.88 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][280/391]	Time  0.178 ( 0.170)	Data  0.002 ( 0.005)	Loss 4.9645e-02 (4.1072e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][290/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.5027e-02 (4.1070e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [50][300/391]	Time  0.169 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.9268e-02 (4.1060e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [50][310/391]	Time  0.169 ( 0.170)	Data  0.002 ( 0.005)	Loss 4.0318e-02 (4.1270e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [50][320/391]	Time  0.170 ( 0.170)	Data  0.001 ( 0.005)	Loss 5.9503e-02 (4.1424e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [50][330/391]	Time  0.167 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.7241e-02 (4.1514e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][340/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.1108e-02 (4.1714e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [50][350/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.3275e-02 (4.1490e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][360/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.3077e-02 (4.1564e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [50][370/391]	Time  0.172 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.0435e-02 (4.1416e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][380/391]	Time  0.180 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.0603e-02 (4.1382e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [50][390/391]	Time  0.172 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.1030e-02 (4.1418e-02)	Acc@1  98.75 ( 99.16)	Acc@5 100.00 (100.00)
## e[50] optimizer.zero_grad (sum) time: 0.3660416603088379
## e[50]       loss.backward (sum) time: 24.080426931381226
## e[50]      optimizer.step (sum) time: 3.5883681774139404
## epoch[50] training(only) time: 66.60307574272156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.287 ( 0.287)	Loss 1.4109e+00 (1.4109e+00)	Acc@1  74.00 ( 74.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.053 ( 0.076)	Loss 1.6434e+00 (1.3914e+00)	Acc@1  68.00 ( 73.55)	Acc@5  92.00 ( 92.73)
Test: [ 20/100]	Time  0.051 ( 0.065)	Loss 9.4856e-01 (1.2611e+00)	Acc@1  80.00 ( 74.90)	Acc@5  97.00 ( 93.62)
Test: [ 30/100]	Time  0.051 ( 0.061)	Loss 1.9686e+00 (1.3202e+00)	Acc@1  64.00 ( 73.74)	Acc@5  90.00 ( 93.32)
Test: [ 40/100]	Time  0.052 ( 0.058)	Loss 1.2681e+00 (1.3056e+00)	Acc@1  74.00 ( 73.78)	Acc@5  93.00 ( 93.51)
Test: [ 50/100]	Time  0.053 ( 0.057)	Loss 1.6834e+00 (1.3098e+00)	Acc@1  65.00 ( 73.55)	Acc@5  92.00 ( 93.35)
Test: [ 60/100]	Time  0.052 ( 0.057)	Loss 1.3846e+00 (1.2701e+00)	Acc@1  71.00 ( 73.92)	Acc@5  93.00 ( 93.52)
Test: [ 70/100]	Time  0.052 ( 0.056)	Loss 1.8660e+00 (1.2809e+00)	Acc@1  67.00 ( 73.94)	Acc@5  91.00 ( 93.45)
Test: [ 80/100]	Time  0.052 ( 0.055)	Loss 1.6810e+00 (1.2919e+00)	Acc@1  69.00 ( 73.77)	Acc@5  91.00 ( 93.27)
Test: [ 90/100]	Time  0.057 ( 0.056)	Loss 1.7931e+00 (1.2742e+00)	Acc@1  67.00 ( 73.89)	Acc@5  89.00 ( 93.35)
 * Acc@1 74.080 Acc@5 93.340
### epoch[50] execution time: 72.28594160079956
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.385 ( 0.385)	Data  0.215 ( 0.215)	Loss 1.9272e-02 (1.9272e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.176 ( 0.202)	Data  0.002 ( 0.024)	Loss 3.5111e-02 (3.6404e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.173 ( 0.190)	Data  0.002 ( 0.014)	Loss 4.9135e-02 (3.8592e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.171 ( 0.184)	Data  0.002 ( 0.011)	Loss 3.0126e-02 (3.8072e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.173 ( 0.181)	Data  0.002 ( 0.009)	Loss 3.9789e-02 (3.9755e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.166 ( 0.178)	Data  0.001 ( 0.008)	Loss 5.1830e-02 (3.9164e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.167 ( 0.176)	Data  0.001 ( 0.008)	Loss 4.8418e-02 (3.8424e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.162 ( 0.175)	Data  0.001 ( 0.007)	Loss 3.1458e-02 (3.7877e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.007)	Loss 5.0653e-02 (3.7751e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.159 ( 0.172)	Data  0.001 ( 0.007)	Loss 2.0904e-02 (3.7836e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.007)	Loss 3.5849e-02 (3.7367e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.3956e-02 (3.7503e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.3451e-02 (3.7160e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.006)	Loss 5.9464e-02 (3.7622e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.006)	Loss 3.8608e-02 (3.7767e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.159 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.0711e-02 (3.7673e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.183 ( 0.170)	Data  0.002 ( 0.006)	Loss 3.2140e-02 (3.7415e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.006)	Loss 4.8018e-02 (3.7365e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.168 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.9503e-02 (3.7896e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.006)	Loss 4.9097e-02 (3.8089e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.166 ( 0.171)	Data  0.002 ( 0.006)	Loss 4.2504e-02 (3.8466e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.159 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.1216e-02 (3.8225e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.005)	Loss 6.1025e-02 (3.8222e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.1556e-02 (3.8122e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.9648e-02 (3.8076e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.0798e-02 (3.8110e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.175 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.8759e-02 (3.8287e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.171 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.1719e-02 (3.8142e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.174 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.9045e-02 (3.8016e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.156 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.0680e-02 (3.8040e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.0647e-02 (3.8169e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.4149e-02 (3.8341e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.185 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.9202e-02 (3.8454e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.183 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.8829e-02 (3.8583e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.9467e-02 (3.8976e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.164 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.9665e-02 (3.8865e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.168 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.4121e-02 (3.8747e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.1559e-02 (3.8626e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.154 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.8656e-02 (3.8654e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.157 ( 0.169)	Data  0.001 ( 0.005)	Loss 7.3896e-02 (3.8812e-02)	Acc@1  95.00 ( 99.24)	Acc@5 100.00 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.3740856647491455
## e[51]       loss.backward (sum) time: 24.37406015396118
## e[51]      optimizer.step (sum) time: 3.629347324371338
## epoch[51] training(only) time: 66.21929621696472
# Switched to evaluate mode...
Test: [  0/100]	Time  0.241 ( 0.241)	Loss 1.4199e+00 (1.4199e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.052 ( 0.069)	Loss 1.6565e+00 (1.3817e+00)	Acc@1  69.00 ( 73.64)	Acc@5  89.00 ( 92.09)
Test: [ 20/100]	Time  0.051 ( 0.061)	Loss 8.4026e-01 (1.2519e+00)	Acc@1  79.00 ( 74.67)	Acc@5  98.00 ( 93.62)
Test: [ 30/100]	Time  0.051 ( 0.058)	Loss 1.9066e+00 (1.3210e+00)	Acc@1  61.00 ( 73.84)	Acc@5  90.00 ( 93.32)
Test: [ 40/100]	Time  0.051 ( 0.057)	Loss 1.2686e+00 (1.3046e+00)	Acc@1  76.00 ( 73.71)	Acc@5  95.00 ( 93.54)
Test: [ 50/100]	Time  0.051 ( 0.055)	Loss 1.6489e+00 (1.3108e+00)	Acc@1  64.00 ( 73.41)	Acc@5  93.00 ( 93.22)
Test: [ 60/100]	Time  0.052 ( 0.055)	Loss 1.3678e+00 (1.2740e+00)	Acc@1  70.00 ( 73.82)	Acc@5  93.00 ( 93.31)
Test: [ 70/100]	Time  0.056 ( 0.055)	Loss 1.9639e+00 (1.2885e+00)	Acc@1  69.00 ( 73.76)	Acc@5  88.00 ( 93.35)
Test: [ 80/100]	Time  0.056 ( 0.055)	Loss 1.6398e+00 (1.2997e+00)	Acc@1  70.00 ( 73.58)	Acc@5  92.00 ( 93.25)
Test: [ 90/100]	Time  0.054 ( 0.055)	Loss 1.8130e+00 (1.2798e+00)	Acc@1  65.00 ( 73.75)	Acc@5  91.00 ( 93.40)
 * Acc@1 74.030 Acc@5 93.400
### epoch[51] execution time: 71.85976529121399
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.368 ( 0.368)	Data  0.181 ( 0.181)	Loss 4.2200e-02 (4.2200e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.178 ( 0.188)	Data  0.001 ( 0.020)	Loss 2.7083e-02 (3.5926e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.157 ( 0.175)	Data  0.001 ( 0.012)	Loss 3.4075e-02 (3.7318e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.158 ( 0.170)	Data  0.001 ( 0.010)	Loss 2.2393e-02 (3.6122e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.156 ( 0.168)	Data  0.001 ( 0.009)	Loss 2.1274e-02 (3.4943e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.98)
Epoch: [52][ 50/391]	Time  0.182 ( 0.169)	Data  0.001 ( 0.008)	Loss 6.3589e-02 (3.4761e-02)	Acc@1  96.88 ( 99.40)	Acc@5 100.00 ( 99.98)
Epoch: [52][ 60/391]	Time  0.182 ( 0.171)	Data  0.001 ( 0.007)	Loss 2.0929e-02 (3.5245e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 70/391]	Time  0.178 ( 0.172)	Data  0.001 ( 0.007)	Loss 4.6165e-02 (3.5277e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 80/391]	Time  0.167 ( 0.172)	Data  0.002 ( 0.007)	Loss 5.3072e-02 (3.5688e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 90/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.5011e-02 (3.6095e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [52][100/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.4766e-02 (3.5963e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 ( 99.99)
Epoch: [52][110/391]	Time  0.151 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.3624e-02 (3.5915e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [52][120/391]	Time  0.158 ( 0.170)	Data  0.001 ( 0.006)	Loss 3.1897e-02 (3.6550e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 ( 99.99)
Epoch: [52][130/391]	Time  0.157 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.4651e-02 (3.6495e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [52][140/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.006)	Loss 4.8500e-02 (3.6114e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [52][150/391]	Time  0.173 ( 0.168)	Data  0.001 ( 0.006)	Loss 3.3448e-02 (3.6124e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [52][160/391]	Time  0.183 ( 0.168)	Data  0.001 ( 0.006)	Loss 4.6977e-02 (3.6345e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.174 ( 0.169)	Data  0.001 ( 0.006)	Loss 4.1884e-02 (3.6347e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.158 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.9557e-02 (3.6102e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.162 ( 0.168)	Data  0.001 ( 0.005)	Loss 3.8852e-02 (3.6114e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][200/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.8176e-02 (3.6231e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][210/391]	Time  0.185 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.1657e-02 (3.6045e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][220/391]	Time  0.174 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.3892e-02 (3.6057e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][230/391]	Time  0.175 ( 0.169)	Data  0.001 ( 0.005)	Loss 8.8737e-02 (3.6128e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][240/391]	Time  0.172 ( 0.169)	Data  0.001 ( 0.005)	Loss 2.8971e-02 (3.5920e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [52][250/391]	Time  0.169 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.0071e-02 (3.5986e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][260/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.4564e-02 (3.6207e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][270/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.0129e-02 (3.6098e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][280/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.2948e-02 (3.5872e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][290/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.005)	Loss 3.2142e-02 (3.5981e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][300/391]	Time  0.156 ( 0.168)	Data  0.001 ( 0.005)	Loss 3.5360e-02 (3.6256e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [52][310/391]	Time  0.173 ( 0.168)	Data  0.001 ( 0.005)	Loss 3.5859e-02 (3.6275e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [52][320/391]	Time  0.170 ( 0.168)	Data  0.001 ( 0.005)	Loss 4.1407e-02 (3.6620e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.172 ( 0.168)	Data  0.001 ( 0.005)	Loss 5.0350e-02 (3.6607e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.005)	Loss 4.6807e-02 (3.6630e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.005)	Loss 4.7410e-02 (3.6771e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.005)	Loss 4.4476e-02 (3.6772e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.185 ( 0.168)	Data  0.001 ( 0.005)	Loss 6.1999e-02 (3.6822e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.179 ( 0.168)	Data  0.001 ( 0.005)	Loss 5.7877e-02 (3.6789e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.169 ( 0.169)	Data  0.001 ( 0.005)	Loss 5.5242e-02 (3.6893e-02)	Acc@1  98.75 ( 99.27)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.3607504367828369
## e[52]       loss.backward (sum) time: 23.97973871231079
## e[52]      optimizer.step (sum) time: 3.5714540481567383
## epoch[52] training(only) time: 66.01410341262817
# Switched to evaluate mode...
Test: [  0/100]	Time  0.287 ( 0.287)	Loss 1.4459e+00 (1.4459e+00)	Acc@1  73.00 ( 73.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.058 ( 0.080)	Loss 1.6551e+00 (1.3881e+00)	Acc@1  70.00 ( 73.73)	Acc@5  91.00 ( 92.45)
Test: [ 20/100]	Time  0.059 ( 0.070)	Loss 9.7982e-01 (1.2703e+00)	Acc@1  79.00 ( 74.67)	Acc@5  96.00 ( 93.38)
Test: [ 30/100]	Time  0.058 ( 0.066)	Loss 1.9084e+00 (1.3303e+00)	Acc@1  63.00 ( 73.84)	Acc@5  93.00 ( 93.32)
Test: [ 40/100]	Time  0.059 ( 0.065)	Loss 1.3387e+00 (1.3149e+00)	Acc@1  75.00 ( 73.93)	Acc@5  95.00 ( 93.66)
Test: [ 50/100]	Time  0.059 ( 0.063)	Loss 1.6689e+00 (1.3255e+00)	Acc@1  66.00 ( 73.59)	Acc@5  92.00 ( 93.29)
Test: [ 60/100]	Time  0.058 ( 0.063)	Loss 1.3204e+00 (1.2901e+00)	Acc@1  70.00 ( 73.93)	Acc@5  93.00 ( 93.33)
Test: [ 70/100]	Time  0.052 ( 0.062)	Loss 1.9828e+00 (1.3082e+00)	Acc@1  69.00 ( 73.83)	Acc@5  89.00 ( 93.32)
Test: [ 80/100]	Time  0.053 ( 0.061)	Loss 1.7097e+00 (1.3180e+00)	Acc@1  69.00 ( 73.59)	Acc@5  90.00 ( 93.21)
Test: [ 90/100]	Time  0.052 ( 0.060)	Loss 1.7879e+00 (1.2957e+00)	Acc@1  66.00 ( 73.77)	Acc@5  91.00 ( 93.37)
 * Acc@1 74.000 Acc@5 93.340
### epoch[52] execution time: 72.02497291564941
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.323 ( 0.323)	Data  0.181 ( 0.181)	Loss 1.6980e-02 (1.6980e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.158 ( 0.173)	Data  0.001 ( 0.020)	Loss 4.4390e-02 (3.2385e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.157 ( 0.166)	Data  0.001 ( 0.013)	Loss 7.5533e-02 (3.5684e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.158 ( 0.163)	Data  0.001 ( 0.010)	Loss 4.8956e-02 (3.5707e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.177 ( 0.166)	Data  0.001 ( 0.009)	Loss 5.3131e-02 (3.5302e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.170 ( 0.168)	Data  0.001 ( 0.008)	Loss 2.8156e-02 (3.4640e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.166 ( 0.168)	Data  0.002 ( 0.007)	Loss 3.0197e-02 (3.4222e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.158 ( 0.167)	Data  0.001 ( 0.007)	Loss 4.2450e-02 (3.4934e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.158 ( 0.166)	Data  0.001 ( 0.007)	Loss 2.1882e-02 (3.4511e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.150 ( 0.165)	Data  0.001 ( 0.006)	Loss 4.8452e-02 (3.4616e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.188 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.0521e-02 (3.4491e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.177 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.7933e-02 (3.4873e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.171 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.5872e-02 (3.4645e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.169 ( 0.169)	Data  0.001 ( 0.006)	Loss 3.5248e-02 (3.5020e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.172 ( 0.170)	Data  0.001 ( 0.006)	Loss 5.6862e-02 (3.4931e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][150/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.9637e-02 (3.4766e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [53][160/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.006)	Loss 3.9782e-02 (3.4462e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.155 ( 0.168)	Data  0.001 ( 0.006)	Loss 3.6160e-02 (3.4649e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.157 ( 0.167)	Data  0.001 ( 0.006)	Loss 5.5403e-02 (3.4564e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.187 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.5740e-02 (3.4757e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.175 ( 0.167)	Data  0.001 ( 0.005)	Loss 3.5480e-02 (3.5151e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.174 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.0882e-02 (3.5032e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.152 ( 0.168)	Data  0.001 ( 0.005)	Loss 5.2544e-02 (3.5258e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.158 ( 0.167)	Data  0.001 ( 0.005)	Loss 3.5494e-02 (3.5366e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.158 ( 0.167)	Data  0.001 ( 0.005)	Loss 7.9516e-02 (3.5597e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.184 ( 0.167)	Data  0.001 ( 0.005)	Loss 4.7870e-02 (3.5620e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.187 ( 0.167)	Data  0.001 ( 0.005)	Loss 2.3201e-02 (3.5464e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.178 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.9356e-02 (3.5702e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.168 ( 0.168)	Data  0.002 ( 0.005)	Loss 2.8929e-02 (3.5490e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.173 ( 0.169)	Data  0.001 ( 0.005)	Loss 4.8468e-02 (3.5383e-02)	Acc@1  97.66 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.7978e-02 (3.5425e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.5507e-02 (3.5493e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.6553e-02 (3.5422e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.005)	Loss 4.2797e-02 (3.5453e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.159 ( 0.167)	Data  0.001 ( 0.005)	Loss 3.4937e-02 (3.5505e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.175 ( 0.167)	Data  0.001 ( 0.005)	Loss 4.3748e-02 (3.5512e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.172 ( 0.168)	Data  0.001 ( 0.005)	Loss 4.4173e-02 (3.5746e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][370/391]	Time  0.170 ( 0.168)	Data  0.001 ( 0.005)	Loss 3.9636e-02 (3.5885e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][380/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.2217e-02 (3.5757e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][390/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.005)	Loss 5.3846e-02 (3.5721e-02)	Acc@1  98.75 ( 99.24)	Acc@5 100.00 (100.00)
## e[53] optimizer.zero_grad (sum) time: 0.3488171100616455
## e[53]       loss.backward (sum) time: 23.147711038589478
## e[53]      optimizer.step (sum) time: 3.432131052017212
## epoch[53] training(only) time: 65.60170865058899
# Switched to evaluate mode...
Test: [  0/100]	Time  0.212 ( 0.212)	Loss 1.4550e+00 (1.4550e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.052 ( 0.066)	Loss 1.6509e+00 (1.3731e+00)	Acc@1  69.00 ( 73.45)	Acc@5  90.00 ( 92.36)
Test: [ 20/100]	Time  0.052 ( 0.059)	Loss 9.1192e-01 (1.2562e+00)	Acc@1  78.00 ( 74.90)	Acc@5  97.00 ( 93.76)
Test: [ 30/100]	Time  0.056 ( 0.057)	Loss 1.9492e+00 (1.3274e+00)	Acc@1  65.00 ( 74.13)	Acc@5  92.00 ( 93.45)
Test: [ 40/100]	Time  0.057 ( 0.057)	Loss 1.3423e+00 (1.3061e+00)	Acc@1  75.00 ( 74.34)	Acc@5  96.00 ( 93.66)
Test: [ 50/100]	Time  0.064 ( 0.058)	Loss 1.6205e+00 (1.3082e+00)	Acc@1  65.00 ( 74.24)	Acc@5  92.00 ( 93.43)
Test: [ 60/100]	Time  0.058 ( 0.058)	Loss 1.3491e+00 (1.2721e+00)	Acc@1  69.00 ( 74.49)	Acc@5  92.00 ( 93.56)
Test: [ 70/100]	Time  0.059 ( 0.058)	Loss 1.8353e+00 (1.2895e+00)	Acc@1  70.00 ( 74.42)	Acc@5  90.00 ( 93.48)
Test: [ 80/100]	Time  0.060 ( 0.058)	Loss 1.5982e+00 (1.3006e+00)	Acc@1  68.00 ( 74.05)	Acc@5  90.00 ( 93.40)
Test: [ 90/100]	Time  0.057 ( 0.058)	Loss 1.8274e+00 (1.2805e+00)	Acc@1  65.00 ( 74.18)	Acc@5  88.00 ( 93.57)
 * Acc@1 74.440 Acc@5 93.540
### epoch[53] execution time: 71.52649188041687
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.383 ( 0.383)	Data  0.196 ( 0.196)	Loss 4.6740e-02 (4.6740e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.163 ( 0.188)	Data  0.002 ( 0.021)	Loss 1.5537e-02 (2.8884e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.176 ( 0.180)	Data  0.001 ( 0.013)	Loss 3.6521e-02 (3.0464e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.165 ( 0.175)	Data  0.001 ( 0.010)	Loss 9.7451e-02 (3.2115e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.009)	Loss 2.1603e-02 (3.1501e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.4284e-02 (2.9965e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.159 ( 0.167)	Data  0.001 ( 0.007)	Loss 4.0149e-02 (3.0570e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.156 ( 0.166)	Data  0.001 ( 0.007)	Loss 3.8752e-02 (3.0837e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.175 ( 0.166)	Data  0.001 ( 0.007)	Loss 1.7766e-02 (3.1223e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.180 ( 0.167)	Data  0.002 ( 0.007)	Loss 4.2115e-02 (3.1683e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 ( 99.99)
Epoch: [54][100/391]	Time  0.176 ( 0.168)	Data  0.001 ( 0.006)	Loss 2.7622e-02 (3.2408e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 ( 99.99)
Epoch: [54][110/391]	Time  0.182 ( 0.169)	Data  0.001 ( 0.006)	Loss 5.1671e-02 (3.2250e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [54][120/391]	Time  0.181 ( 0.170)	Data  0.001 ( 0.006)	Loss 4.2315e-02 (3.2038e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 ( 99.99)
Epoch: [54][130/391]	Time  0.165 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.6925e-02 (3.1685e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 ( 99.99)
Epoch: [54][140/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.5809e-02 (3.1899e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 ( 99.99)
Epoch: [54][150/391]	Time  0.165 ( 0.171)	Data  0.001 ( 0.006)	Loss 4.9526e-02 (3.2437e-02)	Acc@1  97.66 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [54][160/391]	Time  0.170 ( 0.171)	Data  0.002 ( 0.006)	Loss 3.4541e-02 (3.2304e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.4848e-02 (3.2173e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.006)	Loss 5.8820e-02 (3.2848e-02)	Acc@1  97.66 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.148 ( 0.171)	Data  0.001 ( 0.005)	Loss 4.1906e-02 (3.2835e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.175 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.2089e-02 (3.2465e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.181 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.1048e-02 (3.2661e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.9852e-02 (3.2789e-02)	Acc@1  97.66 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.6851e-02 (3.2997e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.168 ( 0.171)	Data  0.002 ( 0.005)	Loss 2.0957e-02 (3.3055e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.171 ( 0.171)	Data  0.002 ( 0.005)	Loss 3.7584e-02 (3.3195e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.170 ( 0.171)	Data  0.002 ( 0.005)	Loss 4.2290e-02 (3.3336e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.9624e-02 (3.3389e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.161 ( 0.171)	Data  0.002 ( 0.005)	Loss 4.8991e-02 (3.3513e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.181 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.7573e-02 (3.3376e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.6230e-02 (3.3287e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.166 ( 0.172)	Data  0.002 ( 0.005)	Loss 5.3586e-02 (3.3435e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.0074e-02 (3.3181e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.6924e-02 (3.3465e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.2167e-02 (3.3471e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.9675e-02 (3.3475e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 6.3003e-02 (3.3544e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.8754e-03 (3.3714e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.1937e-02 (3.3825e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.178 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0792e-01 (3.3930e-02)	Acc@1  95.00 ( 99.38)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.40956926345825195
## e[54]       loss.backward (sum) time: 26.32378363609314
## e[54]      optimizer.step (sum) time: 3.8586015701293945
## epoch[54] training(only) time: 67.37137389183044
# Switched to evaluate mode...
Test: [  0/100]	Time  0.284 ( 0.284)	Loss 1.4899e+00 (1.4899e+00)	Acc@1  76.00 ( 76.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.058 ( 0.080)	Loss 1.5858e+00 (1.3954e+00)	Acc@1  68.00 ( 73.91)	Acc@5  90.00 ( 91.91)
Test: [ 20/100]	Time  0.058 ( 0.070)	Loss 9.1252e-01 (1.2820e+00)	Acc@1  78.00 ( 74.95)	Acc@5  97.00 ( 93.10)
Test: [ 30/100]	Time  0.057 ( 0.066)	Loss 1.9272e+00 (1.3425e+00)	Acc@1  63.00 ( 74.19)	Acc@5  91.00 ( 92.97)
Test: [ 40/100]	Time  0.058 ( 0.064)	Loss 1.3156e+00 (1.3207e+00)	Acc@1  77.00 ( 74.10)	Acc@5  95.00 ( 93.27)
Test: [ 50/100]	Time  0.060 ( 0.063)	Loss 1.7350e+00 (1.3260e+00)	Acc@1  67.00 ( 73.86)	Acc@5  91.00 ( 93.00)
Test: [ 60/100]	Time  0.057 ( 0.062)	Loss 1.3083e+00 (1.2866e+00)	Acc@1  69.00 ( 74.10)	Acc@5  92.00 ( 93.15)
Test: [ 70/100]	Time  0.057 ( 0.061)	Loss 1.9247e+00 (1.3016e+00)	Acc@1  69.00 ( 74.07)	Acc@5  90.00 ( 93.14)
Test: [ 80/100]	Time  0.057 ( 0.061)	Loss 1.6310e+00 (1.3138e+00)	Acc@1  72.00 ( 73.86)	Acc@5  91.00 ( 93.04)
Test: [ 90/100]	Time  0.053 ( 0.061)	Loss 1.8698e+00 (1.2919e+00)	Acc@1  67.00 ( 74.01)	Acc@5  89.00 ( 93.20)
 * Acc@1 74.260 Acc@5 93.200
### epoch[54] execution time: 73.54504728317261
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.382 ( 0.382)	Data  0.215 ( 0.215)	Loss 2.2202e-02 (2.2202e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.171 ( 0.189)	Data  0.001 ( 0.023)	Loss 5.3929e-02 (3.5659e-02)	Acc@1  96.88 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.177 ( 0.182)	Data  0.001 ( 0.014)	Loss 3.0991e-02 (3.4846e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.169 ( 0.178)	Data  0.002 ( 0.011)	Loss 1.6935e-02 (3.3220e-02)	Acc@1 100.00 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.170 ( 0.178)	Data  0.001 ( 0.009)	Loss 3.8872e-02 (3.4377e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.171 ( 0.178)	Data  0.002 ( 0.008)	Loss 2.4052e-02 (3.3811e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.165 ( 0.177)	Data  0.002 ( 0.008)	Loss 2.3031e-02 (3.4185e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.165 ( 0.176)	Data  0.001 ( 0.007)	Loss 6.9118e-02 (3.3980e-02)	Acc@1  96.88 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.163 ( 0.175)	Data  0.002 ( 0.007)	Loss 2.8552e-02 (3.4035e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.9061e-02 (3.3739e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.006)	Loss 3.1892e-02 (3.3736e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.9135e-02 (3.3403e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.182 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.0496e-02 (3.3304e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.006)	Loss 4.2071e-02 (3.3485e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.179 ( 0.174)	Data  0.001 ( 0.006)	Loss 2.9775e-02 (3.3465e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.0099e-02 (3.2945e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.163 ( 0.174)	Data  0.001 ( 0.006)	Loss 4.0530e-02 (3.2769e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.3659e-02 (3.3023e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.161 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.7259e-02 (3.2776e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.5875e-02 (3.2646e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.4103e-02 (3.2392e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.5160e-02 (3.2112e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 3.2150e-02 (3.1970e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.185 ( 0.174)	Data  0.001 ( 0.005)	Loss 5.2392e-02 (3.1983e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.7279e-02 (3.2023e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.179 ( 0.173)	Data  0.001 ( 0.005)	Loss 3.6079e-02 (3.2207e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.166 ( 0.173)	Data  0.002 ( 0.005)	Loss 2.6023e-02 (3.2233e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.166 ( 0.173)	Data  0.002 ( 0.005)	Loss 5.1016e-02 (3.2816e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.5495e-02 (3.3094e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.161 ( 0.173)	Data  0.002 ( 0.005)	Loss 3.1875e-02 (3.3014e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.183 ( 0.173)	Data  0.002 ( 0.005)	Loss 3.5219e-02 (3.3098e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.170 ( 0.173)	Data  0.002 ( 0.005)	Loss 3.1343e-02 (3.3032e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.164 ( 0.173)	Data  0.002 ( 0.005)	Loss 5.9974e-02 (3.2940e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.167 ( 0.173)	Data  0.002 ( 0.005)	Loss 2.7889e-02 (3.3087e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.161 ( 0.173)	Data  0.002 ( 0.005)	Loss 2.6934e-02 (3.3152e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 5.4080e-02 (3.3254e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.2730e-02 (3.3208e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.6125e-02 (3.3230e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.3526e-02 (3.3340e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.005)	Loss 7.0626e-02 (3.3372e-02)	Acc@1  97.50 ( 99.33)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.41500186920166016
## e[55]       loss.backward (sum) time: 26.886082887649536
## e[55]      optimizer.step (sum) time: 3.951035261154175
## epoch[55] training(only) time: 67.62409687042236
# Switched to evaluate mode...
Test: [  0/100]	Time  0.246 ( 0.246)	Loss 1.4493e+00 (1.4493e+00)	Acc@1  76.00 ( 76.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.059 ( 0.077)	Loss 1.7041e+00 (1.3981e+00)	Acc@1  69.00 ( 73.82)	Acc@5  91.00 ( 92.00)
Test: [ 20/100]	Time  0.060 ( 0.068)	Loss 9.8693e-01 (1.2818e+00)	Acc@1  77.00 ( 75.33)	Acc@5  97.00 ( 93.05)
Test: [ 30/100]	Time  0.059 ( 0.065)	Loss 1.9086e+00 (1.3462e+00)	Acc@1  65.00 ( 74.32)	Acc@5  89.00 ( 92.81)
Test: [ 40/100]	Time  0.057 ( 0.063)	Loss 1.2932e+00 (1.3301e+00)	Acc@1  75.00 ( 74.12)	Acc@5  97.00 ( 93.17)
Test: [ 50/100]	Time  0.065 ( 0.062)	Loss 1.7280e+00 (1.3364e+00)	Acc@1  65.00 ( 73.88)	Acc@5  92.00 ( 93.02)
Test: [ 60/100]	Time  0.055 ( 0.061)	Loss 1.3147e+00 (1.2992e+00)	Acc@1  71.00 ( 74.15)	Acc@5  92.00 ( 93.20)
Test: [ 70/100]	Time  0.058 ( 0.061)	Loss 1.9819e+00 (1.3124e+00)	Acc@1  70.00 ( 74.00)	Acc@5  89.00 ( 93.17)
Test: [ 80/100]	Time  0.056 ( 0.060)	Loss 1.7018e+00 (1.3249e+00)	Acc@1  70.00 ( 73.77)	Acc@5  91.00 ( 93.15)
Test: [ 90/100]	Time  0.058 ( 0.060)	Loss 1.8537e+00 (1.3019e+00)	Acc@1  63.00 ( 73.92)	Acc@5  90.00 ( 93.32)
 * Acc@1 74.010 Acc@5 93.300
### epoch[55] execution time: 73.70357728004456
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.380 ( 0.380)	Data  0.219 ( 0.219)	Loss 1.5698e-02 (1.5698e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.166 ( 0.188)	Data  0.001 ( 0.023)	Loss 2.0923e-02 (2.6509e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.177 ( 0.180)	Data  0.001 ( 0.014)	Loss 2.0269e-02 (2.5830e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.170 ( 0.177)	Data  0.001 ( 0.011)	Loss 4.5489e-02 (2.7997e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.166 ( 0.175)	Data  0.002 ( 0.009)	Loss 3.0062e-02 (2.7906e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.182 ( 0.176)	Data  0.001 ( 0.008)	Loss 3.6874e-02 (2.7765e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.176 ( 0.176)	Data  0.001 ( 0.008)	Loss 2.4082e-02 (2.7641e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.169 ( 0.175)	Data  0.002 ( 0.007)	Loss 2.5175e-02 (2.7877e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.007)	Loss 2.6473e-02 (2.8087e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.145 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.7576e-02 (2.8804e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.168 ( 0.173)	Data  0.002 ( 0.006)	Loss 5.2747e-02 (2.9085e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.006)	Loss 3.2386e-02 (2.8955e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.4035e-02 (2.8877e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.7530e-02 (2.8755e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.2120e-02 (2.8903e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.165 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.4629e-02 (2.8965e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.179 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.4480e-02 (2.9209e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.9533e-02 (2.9252e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.3659e-02 (2.9212e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.7194e-02 (2.9399e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.0918e-02 (2.9711e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [56][210/391]	Time  0.173 ( 0.172)	Data  0.001 ( 0.005)	Loss 6.1236e-02 (2.9937e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 ( 99.99)
Epoch: [56][220/391]	Time  0.178 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.7141e-02 (3.0099e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [56][230/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.0778e-02 (3.0256e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [56][240/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.5964e-02 (3.0305e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [56][250/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.0373e-02 (3.0532e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [56][260/391]	Time  0.152 ( 0.172)	Data  0.001 ( 0.005)	Loss 7.2756e-02 (3.0848e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [56][270/391]	Time  0.173 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1977e-02 (3.0636e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [56][280/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.8457e-02 (3.0678e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [56][290/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.7573e-02 (3.0604e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [56][300/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.5272e-02 (3.0527e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [56][310/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.8891e-02 (3.0537e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [56][320/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.7893e-02 (3.0479e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.0835e-02 (3.0707e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.2567e-02 (3.0880e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 5.5413e-02 (3.0953e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.6540e-02 (3.0996e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 7.1559e-02 (3.0955e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.174 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.5551e-02 (3.1000e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.177 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0069e-01 (3.1070e-02)	Acc@1  95.00 ( 99.44)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.3976626396179199
## e[56]       loss.backward (sum) time: 26.082751035690308
## e[56]      optimizer.step (sum) time: 3.807180881500244
## epoch[56] training(only) time: 67.33292484283447
# Switched to evaluate mode...
Test: [  0/100]	Time  0.231 ( 0.231)	Loss 1.4548e+00 (1.4548e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.056 ( 0.073)	Loss 1.6714e+00 (1.4053e+00)	Acc@1  67.00 ( 74.55)	Acc@5  92.00 ( 92.27)
Test: [ 20/100]	Time  0.057 ( 0.066)	Loss 9.6414e-01 (1.2978e+00)	Acc@1  78.00 ( 75.48)	Acc@5  97.00 ( 93.19)
Test: [ 30/100]	Time  0.057 ( 0.063)	Loss 1.8960e+00 (1.3525e+00)	Acc@1  60.00 ( 74.42)	Acc@5  91.00 ( 93.00)
Test: [ 40/100]	Time  0.056 ( 0.061)	Loss 1.3398e+00 (1.3368e+00)	Acc@1  76.00 ( 74.39)	Acc@5  94.00 ( 93.29)
Test: [ 50/100]	Time  0.055 ( 0.060)	Loss 1.7351e+00 (1.3505e+00)	Acc@1  63.00 ( 74.04)	Acc@5  93.00 ( 92.92)
Test: [ 60/100]	Time  0.056 ( 0.059)	Loss 1.3781e+00 (1.3119e+00)	Acc@1  72.00 ( 74.33)	Acc@5  92.00 ( 93.15)
Test: [ 70/100]	Time  0.057 ( 0.059)	Loss 1.8716e+00 (1.3266e+00)	Acc@1  69.00 ( 74.27)	Acc@5  91.00 ( 93.14)
Test: [ 80/100]	Time  0.058 ( 0.059)	Loss 1.7558e+00 (1.3371e+00)	Acc@1  66.00 ( 73.98)	Acc@5  91.00 ( 92.98)
Test: [ 90/100]	Time  0.053 ( 0.058)	Loss 1.9794e+00 (1.3170e+00)	Acc@1  64.00 ( 74.16)	Acc@5  89.00 ( 93.15)
 * Acc@1 74.400 Acc@5 93.160
### epoch[56] execution time: 73.2699716091156
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.441 ( 0.441)	Data  0.260 ( 0.260)	Loss 3.5245e-02 (3.5245e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.175 ( 0.193)	Data  0.001 ( 0.027)	Loss 2.5627e-02 (2.5614e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.169 ( 0.182)	Data  0.001 ( 0.016)	Loss 4.3767e-02 (2.8241e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.173 ( 0.179)	Data  0.002 ( 0.012)	Loss 4.4438e-02 (3.1509e-02)	Acc@1  97.66 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.173 ( 0.178)	Data  0.001 ( 0.010)	Loss 1.4083e-02 (2.9681e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.172 ( 0.178)	Data  0.001 ( 0.009)	Loss 2.5106e-02 (2.8882e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.174 ( 0.177)	Data  0.001 ( 0.009)	Loss 3.3775e-02 (2.9649e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.168 ( 0.176)	Data  0.001 ( 0.008)	Loss 2.5144e-02 (3.0088e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [57][ 80/391]	Time  0.175 ( 0.175)	Data  0.002 ( 0.007)	Loss 3.3319e-02 (2.9965e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [57][ 90/391]	Time  0.167 ( 0.175)	Data  0.001 ( 0.007)	Loss 2.5220e-02 (2.9879e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.98)
Epoch: [57][100/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.007)	Loss 3.1463e-02 (3.0010e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.98)
Epoch: [57][110/391]	Time  0.156 ( 0.173)	Data  0.002 ( 0.007)	Loss 4.6102e-02 (2.9790e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [57][120/391]	Time  0.178 ( 0.174)	Data  0.002 ( 0.006)	Loss 1.9107e-02 (2.9562e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [57][130/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.006)	Loss 2.5181e-02 (2.9336e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 ( 99.99)
Epoch: [57][140/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.2701e-02 (2.9453e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [57][150/391]	Time  0.169 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.3108e-02 (2.9054e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [57][160/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.4241e-02 (2.9211e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 ( 99.99)
Epoch: [57][170/391]	Time  0.159 ( 0.173)	Data  0.001 ( 0.006)	Loss 6.2028e-02 (2.9879e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 ( 99.99)
Epoch: [57][180/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.3829e-02 (3.0260e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][190/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.7643e-02 (3.0115e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [57][200/391]	Time  0.174 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.3576e-02 (2.9955e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [57][210/391]	Time  0.168 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.9176e-02 (2.9991e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][220/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.005)	Loss 5.1960e-02 (2.9987e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][230/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.1932e-02 (3.0234e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [57][240/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.0222e-02 (3.0111e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [57][250/391]	Time  0.191 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.1219e-02 (2.9894e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][260/391]	Time  0.172 ( 0.172)	Data  0.002 ( 0.005)	Loss 3.3235e-02 (2.9844e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][270/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.4893e-02 (3.0010e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [57][280/391]	Time  0.182 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.7804e-02 (2.9792e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [57][290/391]	Time  0.178 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.2924e-02 (2.9941e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [57][300/391]	Time  0.172 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.7518e-02 (2.9959e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [57][310/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.8160e-02 (2.9849e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [57][320/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.1731e-02 (2.9981e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.150 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.3866e-02 (2.9887e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.2885e-02 (3.0026e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.2348e-02 (2.9995e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.005)	Loss 7.7002e-02 (3.0189e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.178 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.3012e-02 (3.0213e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.174 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.0168e-02 (3.0176e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.4313e-02 (2.9944e-02)	Acc@1  98.75 ( 99.45)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.3845851421356201
## e[57]       loss.backward (sum) time: 25.506454467773438
## e[57]      optimizer.step (sum) time: 3.7274720668792725
## epoch[57] training(only) time: 67.2433443069458
# Switched to evaluate mode...
Test: [  0/100]	Time  0.263 ( 0.263)	Loss 1.5146e+00 (1.5146e+00)	Acc@1  76.00 ( 76.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.057 ( 0.076)	Loss 1.6554e+00 (1.3921e+00)	Acc@1  68.00 ( 74.45)	Acc@5  91.00 ( 92.09)
Test: [ 20/100]	Time  0.057 ( 0.067)	Loss 9.5513e-01 (1.2701e+00)	Acc@1  78.00 ( 75.43)	Acc@5  97.00 ( 93.29)
Test: [ 30/100]	Time  0.065 ( 0.064)	Loss 1.9327e+00 (1.3352e+00)	Acc@1  65.00 ( 74.42)	Acc@5  92.00 ( 93.03)
Test: [ 40/100]	Time  0.056 ( 0.062)	Loss 1.3833e+00 (1.3250e+00)	Acc@1  74.00 ( 74.34)	Acc@5  96.00 ( 93.29)
Test: [ 50/100]	Time  0.055 ( 0.061)	Loss 1.6733e+00 (1.3344e+00)	Acc@1  65.00 ( 74.02)	Acc@5  92.00 ( 93.00)
Test: [ 60/100]	Time  0.055 ( 0.060)	Loss 1.3419e+00 (1.3003e+00)	Acc@1  70.00 ( 74.23)	Acc@5  91.00 ( 93.16)
Test: [ 70/100]	Time  0.056 ( 0.059)	Loss 1.9111e+00 (1.3176e+00)	Acc@1  70.00 ( 74.10)	Acc@5  91.00 ( 93.17)
Test: [ 80/100]	Time  0.057 ( 0.059)	Loss 1.7782e+00 (1.3310e+00)	Acc@1  71.00 ( 73.86)	Acc@5  88.00 ( 93.02)
Test: [ 90/100]	Time  0.056 ( 0.058)	Loss 1.8528e+00 (1.3113e+00)	Acc@1  67.00 ( 74.12)	Acc@5  90.00 ( 93.18)
 * Acc@1 74.420 Acc@5 93.160
### epoch[57] execution time: 73.1365487575531
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.355 ( 0.355)	Data  0.188 ( 0.188)	Loss 2.4792e-02 (2.4792e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.174 ( 0.185)	Data  0.001 ( 0.020)	Loss 1.2315e-02 (2.1322e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.179 ( 0.180)	Data  0.001 ( 0.013)	Loss 2.1097e-02 (2.1924e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.173 ( 0.178)	Data  0.002 ( 0.010)	Loss 1.8528e-02 (2.2387e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.157 ( 0.176)	Data  0.001 ( 0.009)	Loss 1.4174e-02 (2.3446e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.170 ( 0.174)	Data  0.001 ( 0.008)	Loss 7.1619e-02 (2.5127e-02)	Acc@1  98.44 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.007)	Loss 3.8311e-02 (2.5098e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.007)	Loss 5.2571e-02 (2.5690e-02)	Acc@1  98.44 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.007)	Loss 2.4683e-02 (2.5589e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.006)	Loss 3.1033e-02 (2.5746e-02)	Acc@1  99.22 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.006)	Loss 3.7949e-02 (2.6042e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.181 ( 0.172)	Data  0.002 ( 0.006)	Loss 3.3890e-02 (2.6445e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.006)	Loss 5.7310e-02 (2.6512e-02)	Acc@1  96.88 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.006)	Loss 4.4044e-02 (2.6914e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.1628e-02 (2.6626e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.160 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.6069e-02 (2.6319e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.006)	Loss 5.2176e-02 (2.6648e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.006)	Loss 4.8398e-02 (2.6582e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.006)	Loss 3.0971e-02 (2.6716e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.177 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.6199e-02 (2.6969e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1600e-02 (2.6990e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.3352e-02 (2.7297e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.5369e-02 (2.7361e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.005)	Loss 4.2263e-02 (2.7334e-02)	Acc@1  97.66 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.2667e-02 (2.7308e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.9769e-02 (2.7282e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.2188e-02 (2.7297e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.172 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.1084e-02 (2.7274e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.0269e-02 (2.7241e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.9120e-02 (2.7336e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.4693e-02 (2.7373e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.5783e-02 (2.7329e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.4575e-02 (2.7318e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.7265e-02 (2.7387e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.162 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.2478e-02 (2.7298e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.4094e-02 (2.7385e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.005)	Loss 4.8820e-02 (2.7499e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.4723e-02 (2.7544e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.8616e-02 (2.7712e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.165 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.3674e-02 (2.7835e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.3677828311920166
## e[58]       loss.backward (sum) time: 24.29584574699402
## e[58]      optimizer.step (sum) time: 3.6240060329437256
## epoch[58] training(only) time: 66.78199982643127
# Switched to evaluate mode...
Test: [  0/100]	Time  0.253 ( 0.253)	Loss 1.5338e+00 (1.5338e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.054 ( 0.073)	Loss 1.6530e+00 (1.4030e+00)	Acc@1  69.00 ( 73.45)	Acc@5  89.00 ( 92.36)
Test: [ 20/100]	Time  0.055 ( 0.064)	Loss 9.6350e-01 (1.2848e+00)	Acc@1  78.00 ( 74.57)	Acc@5  98.00 ( 93.52)
Test: [ 30/100]	Time  0.054 ( 0.061)	Loss 1.9792e+00 (1.3464e+00)	Acc@1  61.00 ( 73.84)	Acc@5  89.00 ( 93.29)
Test: [ 40/100]	Time  0.053 ( 0.059)	Loss 1.4499e+00 (1.3385e+00)	Acc@1  74.00 ( 73.85)	Acc@5  95.00 ( 93.54)
Test: [ 50/100]	Time  0.054 ( 0.058)	Loss 1.7106e+00 (1.3513e+00)	Acc@1  67.00 ( 73.71)	Acc@5  92.00 ( 93.22)
Test: [ 60/100]	Time  0.055 ( 0.057)	Loss 1.2691e+00 (1.3114e+00)	Acc@1  74.00 ( 74.08)	Acc@5  93.00 ( 93.34)
Test: [ 70/100]	Time  0.055 ( 0.057)	Loss 1.9178e+00 (1.3285e+00)	Acc@1  67.00 ( 73.85)	Acc@5  90.00 ( 93.35)
Test: [ 80/100]	Time  0.057 ( 0.057)	Loss 1.8064e+00 (1.3409e+00)	Acc@1  70.00 ( 73.72)	Acc@5  89.00 ( 93.22)
Test: [ 90/100]	Time  0.055 ( 0.057)	Loss 1.8953e+00 (1.3204e+00)	Acc@1  64.00 ( 73.81)	Acc@5  88.00 ( 93.40)
 * Acc@1 74.100 Acc@5 93.360
### epoch[58] execution time: 72.58314895629883
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.383 ( 0.383)	Data  0.217 ( 0.217)	Loss 1.4464e-02 (1.4464e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.170 ( 0.189)	Data  0.001 ( 0.023)	Loss 4.3198e-02 (2.5895e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.169 ( 0.179)	Data  0.001 ( 0.014)	Loss 1.9612e-02 (2.6267e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.168 ( 0.175)	Data  0.001 ( 0.011)	Loss 1.7500e-02 (2.5467e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.009)	Loss 2.2540e-02 (2.6507e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.008)	Loss 1.2938e-02 (2.7142e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.172 ( 0.172)	Data  0.001 ( 0.008)	Loss 1.8296e-02 (2.6963e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.6204e-02 (2.6574e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.173 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.8682e-02 (2.6841e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.9359e-02 (2.7064e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.007)	Loss 2.0768e-02 (2.6805e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.007)	Loss 1.8504e-02 (2.6937e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.7861e-02 (2.6375e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.7558e-02 (2.6409e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.0172e-02 (2.6591e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.168 ( 0.171)	Data  0.002 ( 0.006)	Loss 2.9257e-02 (2.6715e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.165 ( 0.171)	Data  0.001 ( 0.006)	Loss 4.2485e-02 (2.6830e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.4961e-02 (2.6777e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.7129e-02 (2.6751e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.159 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.7084e-02 (2.6332e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.167 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.4013e-02 (2.6215e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.168 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.4707e-02 (2.6044e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.183 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.1839e-02 (2.5845e-02)	Acc@1 100.00 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.174 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.2020e-02 (2.5784e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.006)	Loss 3.8019e-02 (2.6241e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.166 ( 0.170)	Data  0.001 ( 0.006)	Loss 3.5701e-02 (2.6362e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.164 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.3670e-02 (2.6470e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.191 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.0176e-02 (2.6406e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.165 ( 0.170)	Data  0.002 ( 0.005)	Loss 2.3942e-02 (2.6549e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.164 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.0422e-02 (2.6655e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.171 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.9777e-02 (2.6647e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.172 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.8298e-02 (2.6695e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.170 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.5468e-02 (2.6701e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.171 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.8494e-02 (2.6745e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.170 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.3091e-02 (2.6957e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.170 ( 0.170)	Data  0.002 ( 0.005)	Loss 2.4749e-02 (2.6856e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.167 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.1814e-02 (2.6764e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.9897e-02 (2.7030e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.163 ( 0.170)	Data  0.002 ( 0.005)	Loss 4.2928e-02 (2.7135e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.167 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.8388e-02 (2.7078e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.35126471519470215
## e[59]       loss.backward (sum) time: 23.319608688354492
## e[59]      optimizer.step (sum) time: 3.4362785816192627
## epoch[59] training(only) time: 66.55074381828308
# Switched to evaluate mode...
Test: [  0/100]	Time  0.220 ( 0.220)	Loss 1.5136e+00 (1.5136e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.055 ( 0.070)	Loss 1.6931e+00 (1.4225e+00)	Acc@1  70.00 ( 74.00)	Acc@5  90.00 ( 92.09)
Test: [ 20/100]	Time  0.055 ( 0.063)	Loss 8.7248e-01 (1.2879e+00)	Acc@1  79.00 ( 74.90)	Acc@5  97.00 ( 93.14)
Test: [ 30/100]	Time  0.054 ( 0.060)	Loss 2.0361e+00 (1.3620e+00)	Acc@1  62.00 ( 74.06)	Acc@5  89.00 ( 92.74)
Test: [ 40/100]	Time  0.053 ( 0.059)	Loss 1.4374e+00 (1.3460e+00)	Acc@1  74.00 ( 74.17)	Acc@5  95.00 ( 93.17)
Test: [ 50/100]	Time  0.054 ( 0.058)	Loss 1.7315e+00 (1.3546e+00)	Acc@1  67.00 ( 74.00)	Acc@5  91.00 ( 92.86)
Test: [ 60/100]	Time  0.053 ( 0.057)	Loss 1.3052e+00 (1.3215e+00)	Acc@1  69.00 ( 74.11)	Acc@5  92.00 ( 93.00)
Test: [ 70/100]	Time  0.054 ( 0.057)	Loss 1.8915e+00 (1.3419e+00)	Acc@1  69.00 ( 73.97)	Acc@5  89.00 ( 93.00)
Test: [ 80/100]	Time  0.054 ( 0.056)	Loss 1.7453e+00 (1.3523e+00)	Acc@1  70.00 ( 73.83)	Acc@5  88.00 ( 92.89)
Test: [ 90/100]	Time  0.052 ( 0.056)	Loss 1.8467e+00 (1.3309e+00)	Acc@1  66.00 ( 74.01)	Acc@5  90.00 ( 93.05)
 * Acc@1 74.230 Acc@5 93.020
### epoch[59] execution time: 72.23709416389465
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.402 ( 0.402)	Data  0.228 ( 0.228)	Loss 2.6462e-02 (2.6462e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.164 ( 0.186)	Data  0.001 ( 0.024)	Loss 2.5223e-02 (3.0817e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.164 ( 0.176)	Data  0.001 ( 0.015)	Loss 4.4270e-02 (2.9614e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.011)	Loss 4.2140e-02 (2.8870e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.010)	Loss 5.0337e-02 (2.8867e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.009)	Loss 2.5051e-02 (2.9192e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.008)	Loss 1.6776e-02 (2.9031e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.008)	Loss 1.9181e-02 (2.8296e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.157 ( 0.171)	Data  0.001 ( 0.007)	Loss 1.3759e-02 (2.7469e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.175 ( 0.170)	Data  0.001 ( 0.007)	Loss 2.7953e-02 (2.7476e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.007)	Loss 4.1594e-02 (2.7302e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.168 ( 0.170)	Data  0.001 ( 0.007)	Loss 1.3811e-02 (2.6774e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.176 ( 0.170)	Data  0.001 ( 0.006)	Loss 3.4980e-02 (2.6632e-02)	Acc@1  98.44 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.2062e-02 (2.6109e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.167 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.5053e-02 (2.6116e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.8482e-02 (2.5848e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.171 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.5702e-02 (2.5617e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.167 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.7307e-02 (2.5620e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.9556e-02 (2.5787e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.176 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.0082e-02 (2.5764e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.168 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.9808e-02 (2.5529e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.6978e-02 (2.5475e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.165 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.9936e-02 (2.5175e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.163 ( 0.169)	Data  0.002 ( 0.006)	Loss 4.7669e-02 (2.5094e-02)	Acc@1  98.44 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.006)	Loss 7.0053e-03 (2.5008e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.166 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.6676e-02 (2.5008e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.166 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.9027e-02 (2.4804e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.172 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.3103e-02 (2.4769e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.176 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.4320e-02 (2.4757e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.166 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.5535e-02 (2.4568e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.1475e-02 (2.4440e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.172 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.4251e-02 (2.4454e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.167 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.7934e-02 (2.4404e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.163 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.7534e-02 (2.4390e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.168 ( 0.169)	Data  0.002 ( 0.005)	Loss 1.9036e-02 (2.4299e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.163 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.3990e-02 (2.4225e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.173 ( 0.169)	Data  0.001 ( 0.005)	Loss 2.8127e-02 (2.4132e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.5297e-02 (2.4009e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.2713e-02 (2.3978e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.164 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.2835e-02 (2.3942e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.3418400287628174
## e[60]       loss.backward (sum) time: 22.81092929840088
## e[60]      optimizer.step (sum) time: 3.403407096862793
## epoch[60] training(only) time: 65.99322819709778
# Switched to evaluate mode...
Test: [  0/100]	Time  0.261 ( 0.261)	Loss 1.4920e+00 (1.4920e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.053 ( 0.073)	Loss 1.7282e+00 (1.4089e+00)	Acc@1  69.00 ( 74.82)	Acc@5  88.00 ( 92.09)
Test: [ 20/100]	Time  0.052 ( 0.064)	Loss 8.7941e-01 (1.2792e+00)	Acc@1  75.00 ( 75.67)	Acc@5  97.00 ( 93.38)
Test: [ 30/100]	Time  0.054 ( 0.060)	Loss 1.9781e+00 (1.3473e+00)	Acc@1  62.00 ( 74.52)	Acc@5  91.00 ( 93.06)
Test: [ 40/100]	Time  0.054 ( 0.059)	Loss 1.3845e+00 (1.3341e+00)	Acc@1  75.00 ( 74.41)	Acc@5  95.00 ( 93.44)
Test: [ 50/100]	Time  0.053 ( 0.058)	Loss 1.7244e+00 (1.3406e+00)	Acc@1  66.00 ( 74.25)	Acc@5  93.00 ( 93.24)
Test: [ 60/100]	Time  0.053 ( 0.057)	Loss 1.2633e+00 (1.3061e+00)	Acc@1  72.00 ( 74.33)	Acc@5  93.00 ( 93.31)
Test: [ 70/100]	Time  0.055 ( 0.057)	Loss 1.8993e+00 (1.3241e+00)	Acc@1  70.00 ( 74.18)	Acc@5  89.00 ( 93.27)
Test: [ 80/100]	Time  0.056 ( 0.056)	Loss 1.7435e+00 (1.3356e+00)	Acc@1  68.00 ( 73.95)	Acc@5  88.00 ( 93.14)
Test: [ 90/100]	Time  0.055 ( 0.056)	Loss 1.8897e+00 (1.3139e+00)	Acc@1  66.00 ( 74.14)	Acc@5  91.00 ( 93.32)
 * Acc@1 74.370 Acc@5 93.260
### epoch[60] execution time: 71.72270250320435
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.366 ( 0.366)	Data  0.218 ( 0.218)	Loss 1.6155e-02 (1.6155e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.165 ( 0.190)	Data  0.002 ( 0.024)	Loss 1.6889e-02 (2.1578e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.166 ( 0.179)	Data  0.001 ( 0.015)	Loss 3.2956e-02 (2.2625e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.165 ( 0.175)	Data  0.001 ( 0.012)	Loss 1.5694e-02 (2.1934e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.010)	Loss 5.5769e-02 (2.2651e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.173 ( 0.171)	Data  0.002 ( 0.009)	Loss 2.2483e-02 (2.2242e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.008)	Loss 2.5413e-02 (2.2798e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.172 ( 0.170)	Data  0.001 ( 0.008)	Loss 1.7112e-02 (2.2730e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.171 ( 0.170)	Data  0.002 ( 0.007)	Loss 1.2224e-02 (2.2559e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.166 ( 0.170)	Data  0.001 ( 0.007)	Loss 1.5595e-02 (2.2741e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.007)	Loss 2.7302e-02 (2.2827e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [61][110/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.007)	Loss 9.8987e-03 (2.2342e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][120/391]	Time  0.168 ( 0.169)	Data  0.001 ( 0.007)	Loss 3.8832e-02 (2.2469e-02)	Acc@1  98.44 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][130/391]	Time  0.163 ( 0.169)	Data  0.001 ( 0.007)	Loss 4.0773e-02 (2.2971e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][140/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.0010e-02 (2.3239e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [61][150/391]	Time  0.170 ( 0.169)	Data  0.001 ( 0.006)	Loss 3.6789e-02 (2.2929e-02)	Acc@1  99.22 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [61][160/391]	Time  0.169 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.3640e-02 (2.2798e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.0907e-02 (2.2771e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.167 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.5636e-02 (2.2611e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.3951e-02 (2.2451e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.168 ( 0.169)	Data  0.001 ( 0.006)	Loss 3.5341e-02 (2.2581e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.165 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.7843e-02 (2.2601e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.172 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.6811e-02 (2.2352e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.169 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.2987e-02 (2.2212e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.175 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.4571e-02 (2.2111e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.166 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.2753e-02 (2.2183e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.163 ( 0.169)	Data  0.001 ( 0.006)	Loss 3.9243e-02 (2.2276e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.183 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.6578e-02 (2.2219e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.006)	Loss 3.8159e-02 (2.2329e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.161 ( 0.168)	Data  0.002 ( 0.006)	Loss 1.1466e-02 (2.2288e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.167 ( 0.168)	Data  0.001 ( 0.006)	Loss 4.3741e-02 (2.2411e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.168 ( 0.168)	Data  0.002 ( 0.006)	Loss 2.7196e-02 (2.2513e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.171 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.3216e-02 (2.2439e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.164 ( 0.168)	Data  0.001 ( 0.006)	Loss 8.5435e-03 (2.2317e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.7321e-02 (2.2286e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.005)	Loss 3.1154e-02 (2.2305e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.166 ( 0.167)	Data  0.001 ( 0.005)	Loss 4.3644e-02 (2.2464e-02)	Acc@1  98.44 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.164 ( 0.167)	Data  0.001 ( 0.005)	Loss 1.1833e-02 (2.2513e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.168 ( 0.167)	Data  0.001 ( 0.005)	Loss 2.2141e-02 (2.2673e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.166 ( 0.167)	Data  0.001 ( 0.005)	Loss 1.6231e-02 (2.2708e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.3433976173400879
## e[61]       loss.backward (sum) time: 22.649702310562134
## e[61]      optimizer.step (sum) time: 3.3832669258117676
## epoch[61] training(only) time: 65.51549053192139
# Switched to evaluate mode...
Test: [  0/100]	Time  0.249 ( 0.249)	Loss 1.4840e+00 (1.4840e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.054 ( 0.072)	Loss 1.7419e+00 (1.4212e+00)	Acc@1  68.00 ( 74.18)	Acc@5  90.00 ( 91.73)
Test: [ 20/100]	Time  0.052 ( 0.063)	Loss 8.6563e-01 (1.2875e+00)	Acc@1  76.00 ( 75.33)	Acc@5  98.00 ( 93.05)
Test: [ 30/100]	Time  0.053 ( 0.060)	Loss 2.0086e+00 (1.3568e+00)	Acc@1  62.00 ( 74.26)	Acc@5  90.00 ( 92.97)
Test: [ 40/100]	Time  0.053 ( 0.058)	Loss 1.4246e+00 (1.3409e+00)	Acc@1  74.00 ( 74.37)	Acc@5  95.00 ( 93.34)
Test: [ 50/100]	Time  0.053 ( 0.057)	Loss 1.7442e+00 (1.3460e+00)	Acc@1  65.00 ( 74.12)	Acc@5  91.00 ( 93.12)
Test: [ 60/100]	Time  0.054 ( 0.056)	Loss 1.2823e+00 (1.3098e+00)	Acc@1  72.00 ( 74.30)	Acc@5  93.00 ( 93.21)
Test: [ 70/100]	Time  0.053 ( 0.056)	Loss 1.8810e+00 (1.3260e+00)	Acc@1  72.00 ( 74.25)	Acc@5  89.00 ( 93.20)
Test: [ 80/100]	Time  0.053 ( 0.055)	Loss 1.7379e+00 (1.3368e+00)	Acc@1  70.00 ( 74.10)	Acc@5  89.00 ( 93.06)
Test: [ 90/100]	Time  0.053 ( 0.055)	Loss 1.8588e+00 (1.3126e+00)	Acc@1  65.00 ( 74.26)	Acc@5  90.00 ( 93.26)
 * Acc@1 74.540 Acc@5 93.220
### epoch[61] execution time: 71.13499283790588
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.386 ( 0.386)	Data  0.214 ( 0.214)	Loss 1.1724e-02 (1.1724e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.164 ( 0.183)	Data  0.001 ( 0.023)	Loss 2.9632e-02 (2.5021e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.161 ( 0.173)	Data  0.001 ( 0.014)	Loss 9.9567e-03 (2.2932e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.011)	Loss 1.3616e-02 (2.2864e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.010)	Loss 2.1133e-02 (2.2853e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.009)	Loss 1.6204e-02 (2.2087e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.9667e-02 (2.1835e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.2727e-02 (2.1284e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.165 ( 0.168)	Data  0.001 ( 0.007)	Loss 1.4006e-02 (2.1596e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.166 ( 0.168)	Data  0.001 ( 0.007)	Loss 1.4806e-02 (2.1884e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.172 ( 0.167)	Data  0.001 ( 0.007)	Loss 1.1071e-02 (2.2288e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.007)	Loss 1.8678e-02 (2.2532e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.166 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.7789e-02 (2.2581e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][130/391]	Time  0.163 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.0699e-02 (2.2835e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [62][140/391]	Time  0.166 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.3047e-02 (2.3099e-02)	Acc@1  99.22 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [62][150/391]	Time  0.158 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.6561e-02 (2.2936e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [62][160/391]	Time  0.164 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.1563e-02 (2.3077e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.165 ( 0.166)	Data  0.001 ( 0.006)	Loss 5.0065e-02 (2.3070e-02)	Acc@1  97.66 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.165 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.0128e-02 (2.3013e-02)	Acc@1  99.22 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.163 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.5708e-02 (2.2822e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.169 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.3538e-02 (2.3017e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.165 ( 0.167)	Data  0.002 ( 0.006)	Loss 2.3093e-02 (2.3045e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.164 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.6954e-02 (2.3025e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.154 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.0760e-02 (2.2667e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.185 ( 0.166)	Data  0.002 ( 0.006)	Loss 2.0997e-02 (2.2414e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.144 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.0036e-02 (2.2381e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.150 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.0195e-02 (2.2666e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.153 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.5528e-02 (2.2719e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.151 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.5561e-02 (2.2726e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.152 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.7205e-02 (2.2662e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.152 ( 0.165)	Data  0.001 ( 0.005)	Loss 7.7661e-03 (2.2604e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.151 ( 0.164)	Data  0.001 ( 0.005)	Loss 2.1652e-02 (2.2554e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.154 ( 0.164)	Data  0.001 ( 0.005)	Loss 8.0152e-03 (2.2397e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.154 ( 0.164)	Data  0.001 ( 0.005)	Loss 1.4264e-02 (2.2412e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.152 ( 0.163)	Data  0.001 ( 0.005)	Loss 1.3646e-02 (2.2492e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.153 ( 0.163)	Data  0.001 ( 0.005)	Loss 2.8774e-02 (2.2606e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.151 ( 0.163)	Data  0.001 ( 0.005)	Loss 3.0925e-02 (2.2679e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.142 ( 0.162)	Data  0.001 ( 0.005)	Loss 2.1184e-02 (2.2728e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.152 ( 0.162)	Data  0.001 ( 0.005)	Loss 1.2348e-02 (2.2728e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.148 ( 0.162)	Data  0.001 ( 0.005)	Loss 7.0578e-02 (2.2843e-02)	Acc@1  97.50 ( 99.61)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.3292667865753174
## e[62]       loss.backward (sum) time: 21.662885427474976
## e[62]      optimizer.step (sum) time: 3.270169973373413
## epoch[62] training(only) time: 63.4217255115509
# Switched to evaluate mode...
Test: [  0/100]	Time  0.210 ( 0.210)	Loss 1.5239e+00 (1.5239e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.051 ( 0.065)	Loss 1.7086e+00 (1.4142e+00)	Acc@1  69.00 ( 74.00)	Acc@5  89.00 ( 92.09)
Test: [ 20/100]	Time  0.050 ( 0.058)	Loss 9.0036e-01 (1.2862e+00)	Acc@1  78.00 ( 75.33)	Acc@5  96.00 ( 93.24)
Test: [ 30/100]	Time  0.049 ( 0.055)	Loss 1.9719e+00 (1.3557e+00)	Acc@1  63.00 ( 74.35)	Acc@5  91.00 ( 93.03)
Test: [ 40/100]	Time  0.049 ( 0.054)	Loss 1.4329e+00 (1.3422e+00)	Acc@1  75.00 ( 74.27)	Acc@5  96.00 ( 93.46)
Test: [ 50/100]	Time  0.049 ( 0.053)	Loss 1.7490e+00 (1.3496e+00)	Acc@1  66.00 ( 74.08)	Acc@5  92.00 ( 93.16)
Test: [ 60/100]	Time  0.050 ( 0.053)	Loss 1.2746e+00 (1.3122e+00)	Acc@1  72.00 ( 74.25)	Acc@5  94.00 ( 93.30)
Test: [ 70/100]	Time  0.050 ( 0.052)	Loss 1.9414e+00 (1.3288e+00)	Acc@1  70.00 ( 74.14)	Acc@5  87.00 ( 93.30)
Test: [ 80/100]	Time  0.050 ( 0.052)	Loss 1.7517e+00 (1.3402e+00)	Acc@1  71.00 ( 73.99)	Acc@5  91.00 ( 93.19)
Test: [ 90/100]	Time  0.051 ( 0.052)	Loss 1.8071e+00 (1.3184e+00)	Acc@1  66.00 ( 74.15)	Acc@5  89.00 ( 93.35)
 * Acc@1 74.390 Acc@5 93.320
### epoch[62] execution time: 68.88539958000183
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.462 ( 0.462)	Data  0.307 ( 0.307)	Loss 1.7012e-02 (1.7012e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.208 ( 0.230)	Data  0.001 ( 0.034)	Loss 1.4215e-02 (1.8680e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.205 ( 0.218)	Data  0.001 ( 0.021)	Loss 3.0802e-02 (2.0792e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.181 ( 0.213)	Data  0.001 ( 0.017)	Loss 2.1134e-02 (2.1324e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.182 ( 0.205)	Data  0.001 ( 0.014)	Loss 1.7741e-02 (2.0622e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.181 ( 0.200)	Data  0.001 ( 0.012)	Loss 7.5480e-03 (2.0495e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.168 ( 0.196)	Data  0.001 ( 0.011)	Loss 1.4009e-02 (2.0365e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.161 ( 0.192)	Data  0.001 ( 0.010)	Loss 1.6642e-02 (2.0207e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.170 ( 0.189)	Data  0.001 ( 0.009)	Loss 1.6947e-02 (2.0139e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.183 ( 0.188)	Data  0.001 ( 0.009)	Loss 1.7932e-02 (2.0030e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.185 ( 0.188)	Data  0.001 ( 0.008)	Loss 2.4868e-02 (2.0262e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.174 ( 0.187)	Data  0.001 ( 0.008)	Loss 2.5585e-02 (2.1368e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.171 ( 0.186)	Data  0.001 ( 0.008)	Loss 3.6021e-02 (2.1813e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.172 ( 0.185)	Data  0.001 ( 0.007)	Loss 4.0650e-02 (2.1777e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.182 ( 0.185)	Data  0.001 ( 0.007)	Loss 1.0190e-02 (2.1612e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.187 ( 0.184)	Data  0.001 ( 0.007)	Loss 1.9494e-02 (2.1310e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.168 ( 0.184)	Data  0.001 ( 0.007)	Loss 1.9665e-02 (2.1632e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.176 ( 0.183)	Data  0.001 ( 0.007)	Loss 1.8198e-02 (2.1713e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.165 ( 0.183)	Data  0.001 ( 0.007)	Loss 1.6186e-02 (2.1541e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.179 ( 0.182)	Data  0.002 ( 0.007)	Loss 4.5337e-02 (2.1510e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.170 ( 0.181)	Data  0.001 ( 0.006)	Loss 4.8507e-02 (2.1726e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.176 ( 0.181)	Data  0.001 ( 0.006)	Loss 2.1826e-02 (2.1778e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.190 ( 0.181)	Data  0.001 ( 0.006)	Loss 2.5081e-02 (2.1625e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.169 ( 0.180)	Data  0.001 ( 0.006)	Loss 1.9721e-02 (2.1908e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.171 ( 0.180)	Data  0.001 ( 0.006)	Loss 2.6413e-02 (2.2065e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.178 ( 0.180)	Data  0.001 ( 0.006)	Loss 2.0067e-02 (2.2005e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.177 ( 0.180)	Data  0.001 ( 0.006)	Loss 1.6097e-02 (2.1938e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.168 ( 0.179)	Data  0.001 ( 0.006)	Loss 2.6115e-02 (2.1858e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.155 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.9368e-02 (2.1925e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.186 ( 0.179)	Data  0.001 ( 0.006)	Loss 1.3885e-02 (2.1749e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.170 ( 0.179)	Data  0.001 ( 0.006)	Loss 5.0951e-02 (2.2031e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.164 ( 0.179)	Data  0.001 ( 0.006)	Loss 4.2870e-02 (2.2141e-02)	Acc@1  97.66 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.168 ( 0.179)	Data  0.001 ( 0.006)	Loss 2.7395e-02 (2.2188e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.167 ( 0.178)	Data  0.001 ( 0.006)	Loss 2.1710e-02 (2.2158e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.190 ( 0.178)	Data  0.001 ( 0.006)	Loss 2.2370e-02 (2.2108e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.180 ( 0.178)	Data  0.002 ( 0.006)	Loss 3.6002e-02 (2.2156e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.181 ( 0.179)	Data  0.001 ( 0.006)	Loss 2.1545e-02 (2.2353e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.165 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.3235e-02 (2.2352e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.161 ( 0.178)	Data  0.001 ( 0.006)	Loss 9.1528e-03 (2.2232e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.173 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.7113e-02 (2.2269e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.39162325859069824
## e[63]       loss.backward (sum) time: 26.057682514190674
## e[63]      optimizer.step (sum) time: 3.7990493774414062
## epoch[63] training(only) time: 69.72797012329102
# Switched to evaluate mode...
Test: [  0/100]	Time  0.268 ( 0.268)	Loss 1.4757e+00 (1.4757e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.060 ( 0.079)	Loss 1.7023e+00 (1.4068e+00)	Acc@1  70.00 ( 74.27)	Acc@5  91.00 ( 91.91)
Test: [ 20/100]	Time  0.059 ( 0.070)	Loss 9.1000e-01 (1.2840e+00)	Acc@1  77.00 ( 75.14)	Acc@5  97.00 ( 93.24)
Test: [ 30/100]	Time  0.061 ( 0.067)	Loss 1.9779e+00 (1.3518e+00)	Acc@1  62.00 ( 74.23)	Acc@5  91.00 ( 93.03)
Test: [ 40/100]	Time  0.061 ( 0.066)	Loss 1.4243e+00 (1.3368e+00)	Acc@1  74.00 ( 74.29)	Acc@5  96.00 ( 93.37)
Test: [ 50/100]	Time  0.061 ( 0.065)	Loss 1.7493e+00 (1.3437e+00)	Acc@1  64.00 ( 74.04)	Acc@5  92.00 ( 93.14)
Test: [ 60/100]	Time  0.059 ( 0.064)	Loss 1.2874e+00 (1.3056e+00)	Acc@1  73.00 ( 74.31)	Acc@5  94.00 ( 93.36)
Test: [ 70/100]	Time  0.061 ( 0.063)	Loss 1.9368e+00 (1.3221e+00)	Acc@1  69.00 ( 74.14)	Acc@5  90.00 ( 93.38)
Test: [ 80/100]	Time  0.062 ( 0.063)	Loss 1.7536e+00 (1.3339e+00)	Acc@1  71.00 ( 73.98)	Acc@5  89.00 ( 93.17)
Test: [ 90/100]	Time  0.060 ( 0.063)	Loss 1.8486e+00 (1.3116e+00)	Acc@1  66.00 ( 74.18)	Acc@5  90.00 ( 93.37)
 * Acc@1 74.420 Acc@5 93.340
### epoch[63] execution time: 76.08798694610596
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.450 ( 0.450)	Data  0.240 ( 0.240)	Loss 2.1448e-02 (2.1448e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.163 ( 0.192)	Data  0.001 ( 0.025)	Loss 3.0372e-02 (2.0191e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.163 ( 0.182)	Data  0.002 ( 0.015)	Loss 2.0588e-02 (2.1377e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.181 ( 0.180)	Data  0.001 ( 0.012)	Loss 1.0108e-02 (2.3059e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.172 ( 0.178)	Data  0.001 ( 0.010)	Loss 1.7979e-02 (2.2510e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.166 ( 0.176)	Data  0.001 ( 0.009)	Loss 1.9737e-02 (2.2275e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.181 ( 0.175)	Data  0.001 ( 0.008)	Loss 2.4473e-02 (2.1631e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.169 ( 0.175)	Data  0.001 ( 0.008)	Loss 1.4985e-02 (2.1278e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.173 ( 0.175)	Data  0.001 ( 0.007)	Loss 1.3556e-02 (2.1326e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.176 ( 0.175)	Data  0.001 ( 0.007)	Loss 1.4266e-02 (2.0941e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [64][100/391]	Time  0.181 ( 0.174)	Data  0.001 ( 0.007)	Loss 2.0573e-02 (2.1219e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [64][110/391]	Time  0.156 ( 0.174)	Data  0.001 ( 0.006)	Loss 3.2703e-02 (2.1196e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [64][120/391]	Time  0.180 ( 0.174)	Data  0.002 ( 0.006)	Loss 1.0004e-02 (2.1029e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][130/391]	Time  0.180 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.9311e-02 (2.0584e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [64][140/391]	Time  0.163 ( 0.175)	Data  0.002 ( 0.006)	Loss 2.3912e-02 (2.0710e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][150/391]	Time  0.173 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.8908e-02 (2.0610e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [64][160/391]	Time  0.158 ( 0.174)	Data  0.001 ( 0.006)	Loss 2.9335e-02 (2.0778e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][170/391]	Time  0.169 ( 0.175)	Data  0.002 ( 0.006)	Loss 1.4151e-02 (2.1045e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][180/391]	Time  0.183 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.5182e-02 (2.1109e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][190/391]	Time  0.168 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.3590e-02 (2.0988e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [64][200/391]	Time  0.176 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.4547e-02 (2.0911e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][210/391]	Time  0.164 ( 0.174)	Data  0.002 ( 0.006)	Loss 2.9970e-02 (2.0938e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][220/391]	Time  0.172 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.6068e-02 (2.0843e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][230/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.7765e-02 (2.0902e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [64][240/391]	Time  0.178 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.0591e-02 (2.0950e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][250/391]	Time  0.168 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.4873e-02 (2.0870e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [64][260/391]	Time  0.171 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.3678e-02 (2.0946e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][270/391]	Time  0.157 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.4077e-02 (2.0934e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][280/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.5937e-02 (2.0973e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][290/391]	Time  0.183 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.2213e-02 (2.0910e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [64][300/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.8054e-02 (2.0920e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [64][310/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.3523e-02 (2.1163e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [64][320/391]	Time  0.183 ( 0.174)	Data  0.001 ( 0.005)	Loss 3.7906e-02 (2.1296e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.163 ( 0.174)	Data  0.002 ( 0.005)	Loss 4.8788e-02 (2.1519e-02)	Acc@1  97.66 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.170 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.3570e-02 (2.1364e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.170 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.5512e-02 (2.1309e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.4410e-02 (2.1283e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.155 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.1363e-02 (2.1387e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.176 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.1430e-02 (2.1570e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.176 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.0465e-02 (2.1598e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.40239810943603516
## e[64]       loss.backward (sum) time: 26.56109356880188
## e[64]      optimizer.step (sum) time: 3.8894989490509033
## epoch[64] training(only) time: 68.1581962108612
# Switched to evaluate mode...
Test: [  0/100]	Time  0.250 ( 0.250)	Loss 1.5180e+00 (1.5180e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.060 ( 0.077)	Loss 1.6594e+00 (1.3963e+00)	Acc@1  71.00 ( 75.18)	Acc@5  89.00 ( 92.18)
Test: [ 20/100]	Time  0.059 ( 0.068)	Loss 8.7949e-01 (1.2739e+00)	Acc@1  78.00 ( 75.95)	Acc@5  97.00 ( 93.33)
Test: [ 30/100]	Time  0.061 ( 0.065)	Loss 1.9985e+00 (1.3401e+00)	Acc@1  63.00 ( 74.84)	Acc@5  90.00 ( 92.94)
Test: [ 40/100]	Time  0.060 ( 0.064)	Loss 1.4537e+00 (1.3291e+00)	Acc@1  74.00 ( 74.71)	Acc@5  96.00 ( 93.37)
Test: [ 50/100]	Time  0.059 ( 0.063)	Loss 1.6979e+00 (1.3380e+00)	Acc@1  65.00 ( 74.41)	Acc@5  93.00 ( 93.12)
Test: [ 60/100]	Time  0.059 ( 0.062)	Loss 1.2741e+00 (1.3026e+00)	Acc@1  71.00 ( 74.54)	Acc@5  92.00 ( 93.25)
Test: [ 70/100]	Time  0.058 ( 0.062)	Loss 1.9571e+00 (1.3221e+00)	Acc@1  68.00 ( 74.37)	Acc@5  90.00 ( 93.23)
Test: [ 80/100]	Time  0.060 ( 0.062)	Loss 1.7456e+00 (1.3344e+00)	Acc@1  71.00 ( 74.15)	Acc@5  91.00 ( 93.12)
Test: [ 90/100]	Time  0.060 ( 0.061)	Loss 1.8705e+00 (1.3144e+00)	Acc@1  66.00 ( 74.25)	Acc@5  90.00 ( 93.30)
 * Acc@1 74.480 Acc@5 93.250
### epoch[64] execution time: 74.37306022644043
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.381 ( 0.381)	Data  0.210 ( 0.210)	Loss 1.7976e-02 (1.7976e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.175 ( 0.198)	Data  0.001 ( 0.023)	Loss 1.5749e-02 (2.2564e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.167 ( 0.187)	Data  0.002 ( 0.014)	Loss 2.4140e-02 (2.0425e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.168 ( 0.181)	Data  0.001 ( 0.011)	Loss 2.4898e-02 (2.0622e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.167 ( 0.178)	Data  0.001 ( 0.009)	Loss 1.4535e-02 (2.2084e-02)	Acc@1 100.00 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.181 ( 0.177)	Data  0.001 ( 0.008)	Loss 1.4691e-02 (2.1584e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.184 ( 0.178)	Data  0.001 ( 0.007)	Loss 1.5761e-02 (2.1073e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.178 ( 0.178)	Data  0.001 ( 0.007)	Loss 1.6475e-02 (2.1127e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.178 ( 0.178)	Data  0.001 ( 0.007)	Loss 4.7090e-02 (2.1419e-02)	Acc@1  96.88 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.172 ( 0.177)	Data  0.001 ( 0.006)	Loss 1.2803e-02 (2.1422e-02)	Acc@1 100.00 ( 99.61)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.184 ( 0.176)	Data  0.001 ( 0.006)	Loss 4.1752e-02 (2.1489e-02)	Acc@1  97.66 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.167 ( 0.176)	Data  0.001 ( 0.006)	Loss 4.1171e-02 (2.1189e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.172 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.3720e-02 (2.1020e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.181 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.6813e-02 (2.1295e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.174 ( 0.176)	Data  0.001 ( 0.006)	Loss 3.6252e-02 (2.1377e-02)	Acc@1  98.44 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.177 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.2991e-02 (2.1254e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.181 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.3187e-02 (2.1406e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.162 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.9296e-02 (2.1582e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.6376e-02 (2.1484e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.173 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.5369e-02 (2.1333e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.175 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.3236e-02 (2.1465e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.170 ( 0.175)	Data  0.002 ( 0.005)	Loss 2.1467e-02 (2.1470e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.172 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.9951e-02 (2.1514e-02)	Acc@1  99.22 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.175 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.3489e-02 (2.1614e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.174 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.9652e-02 (2.1772e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.169 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.8825e-02 (2.1742e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.182 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.8197e-02 (2.1725e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.168 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.4027e-02 (2.1655e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.165 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.1769e-02 (2.1531e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.164 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.1705e-02 (2.1438e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.177 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.8177e-02 (2.1433e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.177 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.8925e-02 (2.1523e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.178 ( 0.175)	Data  0.001 ( 0.005)	Loss 3.3223e-02 (2.1563e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.170 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.9493e-02 (2.1497e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.167 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.1977e-02 (2.1663e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.6981e-02 (2.1601e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.1879e-02 (2.1673e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.181 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.3973e-02 (2.1673e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.168 ( 0.174)	Data  0.001 ( 0.005)	Loss 8.8420e-03 (2.1582e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.170 ( 0.174)	Data  0.001 ( 0.005)	Loss 4.5748e-02 (2.1591e-02)	Acc@1  98.75 ( 99.65)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.402313232421875
## e[65]       loss.backward (sum) time: 26.38425850868225
## e[65]      optimizer.step (sum) time: 3.8506641387939453
## epoch[65] training(only) time: 68.18016648292542
# Switched to evaluate mode...
Test: [  0/100]	Time  0.246 ( 0.246)	Loss 1.5443e+00 (1.5443e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.059 ( 0.077)	Loss 1.7281e+00 (1.4061e+00)	Acc@1  70.00 ( 74.55)	Acc@5  89.00 ( 92.09)
Test: [ 20/100]	Time  0.058 ( 0.068)	Loss 8.6324e-01 (1.2797e+00)	Acc@1  78.00 ( 75.52)	Acc@5  97.00 ( 93.38)
Test: [ 30/100]	Time  0.058 ( 0.065)	Loss 1.9863e+00 (1.3492e+00)	Acc@1  60.00 ( 74.35)	Acc@5  91.00 ( 93.10)
Test: [ 40/100]	Time  0.058 ( 0.064)	Loss 1.4466e+00 (1.3366e+00)	Acc@1  74.00 ( 74.41)	Acc@5  94.00 ( 93.44)
Test: [ 50/100]	Time  0.057 ( 0.062)	Loss 1.7345e+00 (1.3414e+00)	Acc@1  64.00 ( 74.18)	Acc@5  92.00 ( 93.18)
Test: [ 60/100]	Time  0.058 ( 0.062)	Loss 1.2785e+00 (1.3040e+00)	Acc@1  71.00 ( 74.48)	Acc@5  93.00 ( 93.38)
Test: [ 70/100]	Time  0.055 ( 0.061)	Loss 1.9452e+00 (1.3212e+00)	Acc@1  70.00 ( 74.32)	Acc@5  88.00 ( 93.37)
Test: [ 80/100]	Time  0.060 ( 0.061)	Loss 1.7708e+00 (1.3332e+00)	Acc@1  71.00 ( 74.15)	Acc@5  90.00 ( 93.20)
Test: [ 90/100]	Time  0.057 ( 0.060)	Loss 1.8933e+00 (1.3109e+00)	Acc@1  66.00 ( 74.26)	Acc@5  90.00 ( 93.37)
 * Acc@1 74.490 Acc@5 93.320
### epoch[65] execution time: 74.23203825950623
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.377 ( 0.377)	Data  0.194 ( 0.194)	Loss 2.6532e-02 (2.6532e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.166 ( 0.188)	Data  0.001 ( 0.021)	Loss 1.5308e-02 (2.1810e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.182 ( 0.181)	Data  0.001 ( 0.013)	Loss 2.5439e-02 (2.3668e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.185 ( 0.182)	Data  0.001 ( 0.011)	Loss 3.4134e-02 (2.2699e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.178 ( 0.181)	Data  0.002 ( 0.009)	Loss 4.8619e-02 (2.3471e-02)	Acc@1  98.44 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.172 ( 0.179)	Data  0.001 ( 0.008)	Loss 1.8801e-02 (2.2132e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.165 ( 0.178)	Data  0.001 ( 0.008)	Loss 1.4104e-02 (2.1074e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.174 ( 0.177)	Data  0.001 ( 0.007)	Loss 2.0799e-02 (2.0318e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.179 ( 0.178)	Data  0.002 ( 0.007)	Loss 1.7018e-02 (2.0504e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.176 ( 0.178)	Data  0.001 ( 0.007)	Loss 1.1037e-02 (2.0596e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.165 ( 0.178)	Data  0.001 ( 0.006)	Loss 1.8936e-02 (2.0521e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.170 ( 0.177)	Data  0.002 ( 0.006)	Loss 1.4947e-02 (2.0524e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.164 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.7317e-02 (2.0469e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.176 ( 0.176)	Data  0.001 ( 0.006)	Loss 3.1756e-02 (2.0162e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.174 ( 0.176)	Data  0.001 ( 0.006)	Loss 2.1286e-02 (2.0311e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.168 ( 0.176)	Data  0.001 ( 0.006)	Loss 2.4801e-02 (2.0104e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.170 ( 0.176)	Data  0.001 ( 0.006)	Loss 3.7469e-02 (2.0518e-02)	Acc@1  97.66 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.5971e-02 (2.0467e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.175 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.4403e-02 (2.0252e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.173 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.5856e-02 (2.0350e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.170 ( 0.175)	Data  0.001 ( 0.005)	Loss 3.0908e-02 (2.0291e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.168 ( 0.174)	Data  0.002 ( 0.005)	Loss 2.9776e-02 (2.0117e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.5946e-02 (2.0291e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.173 ( 0.174)	Data  0.001 ( 0.005)	Loss 3.2787e-02 (2.0346e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.167 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.7033e-02 (2.0261e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.4746e-02 (2.0219e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.9573e-02 (2.0188e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.9058e-02 (2.0389e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.176 ( 0.174)	Data  0.002 ( 0.005)	Loss 9.3650e-03 (2.0490e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.180 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.2829e-02 (2.0461e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.4251e-02 (2.0415e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.005)	Loss 3.5213e-02 (2.0522e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.168 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.6962e-02 (2.0738e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.183 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.8118e-02 (2.0737e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.172 ( 0.174)	Data  0.002 ( 0.005)	Loss 8.9456e-03 (2.0675e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.187 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.4130e-02 (2.0729e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.174 ( 0.174)	Data  0.002 ( 0.005)	Loss 9.3611e-03 (2.0675e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.4529e-02 (2.0602e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.167 ( 0.174)	Data  0.002 ( 0.005)	Loss 9.5345e-03 (2.0581e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.005)	Loss 3.6197e-02 (2.0570e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.4059324264526367
## e[66]       loss.backward (sum) time: 26.56619668006897
## e[66]      optimizer.step (sum) time: 3.88151216506958
## epoch[66] training(only) time: 67.99808859825134
# Switched to evaluate mode...
Test: [  0/100]	Time  0.251 ( 0.251)	Loss 1.5277e+00 (1.5277e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.056 ( 0.074)	Loss 1.6990e+00 (1.4213e+00)	Acc@1  69.00 ( 74.27)	Acc@5  90.00 ( 92.09)
Test: [ 20/100]	Time  0.055 ( 0.066)	Loss 8.7221e-01 (1.2789e+00)	Acc@1  80.00 ( 75.62)	Acc@5  97.00 ( 93.43)
Test: [ 30/100]	Time  0.055 ( 0.062)	Loss 2.0046e+00 (1.3432e+00)	Acc@1  62.00 ( 74.55)	Acc@5  90.00 ( 93.13)
Test: [ 40/100]	Time  0.054 ( 0.060)	Loss 1.4457e+00 (1.3335e+00)	Acc@1  73.00 ( 74.39)	Acc@5  95.00 ( 93.46)
Test: [ 50/100]	Time  0.055 ( 0.060)	Loss 1.7893e+00 (1.3421e+00)	Acc@1  65.00 ( 74.18)	Acc@5  92.00 ( 93.20)
Test: [ 60/100]	Time  0.055 ( 0.059)	Loss 1.3044e+00 (1.3050e+00)	Acc@1  71.00 ( 74.41)	Acc@5  92.00 ( 93.38)
Test: [ 70/100]	Time  0.058 ( 0.058)	Loss 1.9351e+00 (1.3235e+00)	Acc@1  69.00 ( 74.15)	Acc@5  89.00 ( 93.38)
Test: [ 80/100]	Time  0.058 ( 0.058)	Loss 1.7623e+00 (1.3350e+00)	Acc@1  71.00 ( 74.02)	Acc@5  89.00 ( 93.20)
Test: [ 90/100]	Time  0.058 ( 0.058)	Loss 1.8673e+00 (1.3124e+00)	Acc@1  67.00 ( 74.20)	Acc@5  90.00 ( 93.38)
 * Acc@1 74.440 Acc@5 93.320
### epoch[66] execution time: 73.91444373130798
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.337 ( 0.337)	Data  0.190 ( 0.190)	Loss 2.0823e-02 (2.0823e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.165 ( 0.184)	Data  0.002 ( 0.021)	Loss 8.3056e-03 (2.1707e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.183 ( 0.178)	Data  0.001 ( 0.013)	Loss 1.1458e-02 (2.0965e-02)	Acc@1 100.00 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.166 ( 0.175)	Data  0.001 ( 0.010)	Loss 1.3622e-02 (2.0638e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.168 ( 0.175)	Data  0.001 ( 0.009)	Loss 1.3054e-02 (2.0153e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.164 ( 0.174)	Data  0.002 ( 0.008)	Loss 2.3058e-02 (2.1190e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.164 ( 0.173)	Data  0.001 ( 0.007)	Loss 3.0499e-02 (2.1498e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.163 ( 0.172)	Data  0.002 ( 0.007)	Loss 1.8986e-02 (2.1502e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.007)	Loss 3.2582e-02 (2.1656e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.5528e-02 (2.1690e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.177 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.0052e-02 (2.1540e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.184 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.2939e-02 (2.1829e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.180 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.7300e-02 (2.1948e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.2602e-02 (2.2255e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.7695e-02 (2.2468e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.1551e-02 (2.2337e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.174 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.1396e-02 (2.2161e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.171 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.1441e-02 (2.2147e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.170 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.3839e-02 (2.1831e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.5980e-02 (2.1723e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.164 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.5811e-02 (2.1586e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.005)	Loss 7.7184e-03 (2.1421e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.0151e-02 (2.1347e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.9131e-02 (2.1192e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.9769e-02 (2.1450e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.3453e-02 (2.1440e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.175 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.4857e-02 (2.1419e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.5160e-02 (2.1361e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.1121e-02 (2.1451e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.3512e-02 (2.1428e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2339e-02 (2.1385e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.4425e-02 (2.1244e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.173 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.9161e-02 (2.1221e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.8589e-02 (2.1184e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.8982e-02 (2.1094e-02)	Acc@1  98.44 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.163 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.5451e-02 (2.1070e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.165 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.1603e-02 (2.1142e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.182 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.4490e-02 (2.1071e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.9749e-02 (2.1071e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.1630e-02 (2.1059e-02)	Acc@1  98.75 ( 99.68)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.39670801162719727
## e[67]       loss.backward (sum) time: 25.910770177841187
## e[67]      optimizer.step (sum) time: 3.8386964797973633
## epoch[67] training(only) time: 67.54979276657104
# Switched to evaluate mode...
Test: [  0/100]	Time  0.248 ( 0.248)	Loss 1.4971e+00 (1.4971e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.057 ( 0.075)	Loss 1.7234e+00 (1.4142e+00)	Acc@1  68.00 ( 74.09)	Acc@5  89.00 ( 92.18)
Test: [ 20/100]	Time  0.058 ( 0.067)	Loss 9.3618e-01 (1.2893e+00)	Acc@1  75.00 ( 75.19)	Acc@5  96.00 ( 93.52)
Test: [ 30/100]	Time  0.057 ( 0.064)	Loss 1.9994e+00 (1.3555e+00)	Acc@1  62.00 ( 74.32)	Acc@5  91.00 ( 93.16)
Test: [ 40/100]	Time  0.062 ( 0.063)	Loss 1.4419e+00 (1.3413e+00)	Acc@1  74.00 ( 74.32)	Acc@5  96.00 ( 93.51)
Test: [ 50/100]	Time  0.060 ( 0.063)	Loss 1.7069e+00 (1.3492e+00)	Acc@1  64.00 ( 74.04)	Acc@5  92.00 ( 93.20)
Test: [ 60/100]	Time  0.059 ( 0.062)	Loss 1.2753e+00 (1.3107e+00)	Acc@1  71.00 ( 74.28)	Acc@5  93.00 ( 93.38)
Test: [ 70/100]	Time  0.059 ( 0.062)	Loss 1.9317e+00 (1.3289e+00)	Acc@1  69.00 ( 74.11)	Acc@5  88.00 ( 93.35)
Test: [ 80/100]	Time  0.057 ( 0.062)	Loss 1.7422e+00 (1.3399e+00)	Acc@1  69.00 ( 73.93)	Acc@5  92.00 ( 93.25)
Test: [ 90/100]	Time  0.062 ( 0.061)	Loss 1.8836e+00 (1.3189e+00)	Acc@1  65.00 ( 74.11)	Acc@5  90.00 ( 93.42)
 * Acc@1 74.390 Acc@5 93.360
### epoch[67] execution time: 73.77778697013855
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.392 ( 0.392)	Data  0.195 ( 0.195)	Loss 2.0608e-02 (2.0608e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.166 ( 0.188)	Data  0.001 ( 0.021)	Loss 2.7323e-02 (1.8913e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.173 ( 0.180)	Data  0.001 ( 0.013)	Loss 4.0526e-02 (2.0187e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.175 ( 0.176)	Data  0.001 ( 0.010)	Loss 1.8413e-02 (2.0002e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.171 ( 0.175)	Data  0.001 ( 0.009)	Loss 4.0161e-02 (2.0292e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.174 ( 0.175)	Data  0.001 ( 0.008)	Loss 1.1122e-02 (2.0146e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.007)	Loss 3.8276e-02 (2.0223e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.007)	Loss 2.0376e-02 (2.0778e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.5635e-02 (2.0532e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.173 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.0202e-02 (2.0600e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.7134e-02 (2.0862e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.177 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.6665e-02 (2.0744e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.168 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.3644e-02 (2.0649e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.165 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.6110e-02 (2.1016e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.4717e-02 (2.0857e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.8625e-02 (2.1489e-02)	Acc@1  98.44 ( 99.66)	Acc@5 100.00 ( 99.99)
Epoch: [68][160/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.2908e-02 (2.1367e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [68][170/391]	Time  0.160 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.1907e-02 (2.1079e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [68][180/391]	Time  0.180 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.3647e-02 (2.1295e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [68][190/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.2267e-02 (2.1381e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [68][200/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.7995e-02 (2.1108e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [68][210/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.2358e-02 (2.1108e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [68][220/391]	Time  0.164 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.0965e-02 (2.1187e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [68][230/391]	Time  0.180 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.6742e-02 (2.1284e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [68][240/391]	Time  0.182 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.1279e-02 (2.1181e-02)	Acc@1  98.44 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [68][250/391]	Time  0.181 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.8876e-02 (2.1105e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [68][260/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.6529e-02 (2.0898e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [68][270/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.7297e-02 (2.1007e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [68][280/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.8465e-02 (2.1051e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [68][290/391]	Time  0.174 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.2209e-02 (2.1030e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [68][300/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.3741e-02 (2.1044e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [68][310/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.3889e-02 (2.0969e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [68][320/391]	Time  0.159 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.5305e-02 (2.0916e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.174 ( 0.172)	Data  0.002 ( 0.005)	Loss 9.7171e-03 (2.0837e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.3266e-02 (2.0719e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.7566e-02 (2.0734e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.168 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.7760e-02 (2.0965e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.1864e-02 (2.1020e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.173 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.5102e-02 (2.1051e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.5938e-02 (2.1134e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.39254307746887207
## e[68]       loss.backward (sum) time: 25.85320496559143
## e[68]      optimizer.step (sum) time: 3.797595500946045
## epoch[68] training(only) time: 67.31818222999573
# Switched to evaluate mode...
Test: [  0/100]	Time  0.222 ( 0.222)	Loss 1.5373e+00 (1.5373e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.054 ( 0.069)	Loss 1.6850e+00 (1.4185e+00)	Acc@1  69.00 ( 74.09)	Acc@5  91.00 ( 92.27)
Test: [ 20/100]	Time  0.056 ( 0.062)	Loss 9.3669e-01 (1.2908e+00)	Acc@1  79.00 ( 75.14)	Acc@5  96.00 ( 93.33)
Test: [ 30/100]	Time  0.055 ( 0.060)	Loss 1.9929e+00 (1.3541e+00)	Acc@1  61.00 ( 74.23)	Acc@5  91.00 ( 93.19)
Test: [ 40/100]	Time  0.054 ( 0.059)	Loss 1.4782e+00 (1.3400e+00)	Acc@1  73.00 ( 74.20)	Acc@5  95.00 ( 93.49)
Test: [ 50/100]	Time  0.054 ( 0.058)	Loss 1.6924e+00 (1.3468e+00)	Acc@1  67.00 ( 74.06)	Acc@5  92.00 ( 93.22)
Test: [ 60/100]	Time  0.054 ( 0.057)	Loss 1.2939e+00 (1.3073e+00)	Acc@1  73.00 ( 74.38)	Acc@5  94.00 ( 93.38)
Test: [ 70/100]	Time  0.054 ( 0.057)	Loss 1.9273e+00 (1.3249e+00)	Acc@1  68.00 ( 74.14)	Acc@5  90.00 ( 93.39)
Test: [ 80/100]	Time  0.057 ( 0.056)	Loss 1.7582e+00 (1.3370e+00)	Acc@1  71.00 ( 73.91)	Acc@5  88.00 ( 93.23)
Test: [ 90/100]	Time  0.059 ( 0.056)	Loss 1.8711e+00 (1.3143e+00)	Acc@1  66.00 ( 74.15)	Acc@5  89.00 ( 93.40)
 * Acc@1 74.330 Acc@5 93.340
### epoch[68] execution time: 73.12080407142639
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.378 ( 0.378)	Data  0.202 ( 0.202)	Loss 1.1612e-02 (1.1612e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.177 ( 0.199)	Data  0.001 ( 0.022)	Loss 5.2331e-02 (2.4504e-02)	Acc@1  97.66 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.178 ( 0.188)	Data  0.001 ( 0.014)	Loss 2.9202e-02 (2.3116e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.177 ( 0.182)	Data  0.002 ( 0.011)	Loss 1.2194e-02 (2.0938e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.175 ( 0.179)	Data  0.001 ( 0.009)	Loss 2.2993e-02 (2.0511e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.178 ( 0.179)	Data  0.002 ( 0.008)	Loss 2.6151e-02 (2.0367e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.175 ( 0.179)	Data  0.002 ( 0.007)	Loss 2.3587e-02 (2.1090e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.179 ( 0.179)	Data  0.001 ( 0.007)	Loss 3.6559e-02 (2.1132e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.171 ( 0.178)	Data  0.001 ( 0.007)	Loss 1.5165e-02 (2.1213e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.164 ( 0.177)	Data  0.001 ( 0.006)	Loss 1.4985e-02 (2.0760e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.165 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.6850e-02 (2.0659e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.170 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.8060e-02 (2.0726e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.165 ( 0.176)	Data  0.001 ( 0.006)	Loss 2.6963e-02 (2.0611e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.2278e-02 (2.0552e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.160 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.9113e-02 (2.0382e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [69][150/391]	Time  0.180 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.1300e-02 (2.0202e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [69][160/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.5820e-02 (2.0181e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.6245e-02 (2.0133e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.183 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5722e-02 (1.9979e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.2149e-02 (2.0106e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.005)	Loss 8.1234e-03 (1.9985e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.4965e-02 (2.0000e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.156 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0400e-02 (1.9840e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.6166e-02 (1.9859e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.4090e-02 (1.9810e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.185 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.0641e-02 (1.9903e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0201e-02 (1.9853e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0276e-02 (1.9709e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 7.1926e-03 (1.9616e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.3778e-02 (1.9658e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.183 ( 0.173)	Data  0.001 ( 0.005)	Loss 9.0370e-03 (1.9768e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.176 ( 0.173)	Data  0.002 ( 0.005)	Loss 9.5228e-03 (1.9696e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.177 ( 0.173)	Data  0.002 ( 0.005)	Loss 2.5087e-02 (1.9731e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.0975e-02 (1.9664e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.175 ( 0.173)	Data  0.002 ( 0.005)	Loss 8.1098e-03 (1.9638e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.159 ( 0.173)	Data  0.001 ( 0.005)	Loss 8.6004e-03 (1.9582e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.2348e-02 (1.9577e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.7968e-02 (1.9546e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.1981e-02 (1.9528e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.160 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.9107e-02 (1.9482e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.3860960006713867
## e[69]       loss.backward (sum) time: 25.548738718032837
## e[69]      optimizer.step (sum) time: 3.742915391921997
## epoch[69] training(only) time: 67.42962884902954
# Switched to evaluate mode...
Test: [  0/100]	Time  0.252 ( 0.252)	Loss 1.5374e+00 (1.5374e+00)	Acc@1  77.00 ( 77.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.051 ( 0.070)	Loss 1.7237e+00 (1.4132e+00)	Acc@1  69.00 ( 74.18)	Acc@5  89.00 ( 91.73)
Test: [ 20/100]	Time  0.059 ( 0.064)	Loss 9.0741e-01 (1.2870e+00)	Acc@1  79.00 ( 75.29)	Acc@5  97.00 ( 93.10)
Test: [ 30/100]	Time  0.064 ( 0.063)	Loss 1.9884e+00 (1.3508e+00)	Acc@1  62.00 ( 74.16)	Acc@5  91.00 ( 93.03)
Test: [ 40/100]	Time  0.064 ( 0.063)	Loss 1.4371e+00 (1.3386e+00)	Acc@1  73.00 ( 74.27)	Acc@5  95.00 ( 93.34)
Test: [ 50/100]	Time  0.062 ( 0.063)	Loss 1.7223e+00 (1.3432e+00)	Acc@1  66.00 ( 74.14)	Acc@5  93.00 ( 93.14)
Test: [ 60/100]	Time  0.059 ( 0.063)	Loss 1.2891e+00 (1.3058e+00)	Acc@1  71.00 ( 74.33)	Acc@5  92.00 ( 93.34)
Test: [ 70/100]	Time  0.058 ( 0.062)	Loss 1.9293e+00 (1.3234e+00)	Acc@1  69.00 ( 74.13)	Acc@5  89.00 ( 93.31)
Test: [ 80/100]	Time  0.059 ( 0.062)	Loss 1.7254e+00 (1.3346e+00)	Acc@1  73.00 ( 73.99)	Acc@5  89.00 ( 93.16)
Test: [ 90/100]	Time  0.059 ( 0.062)	Loss 1.8587e+00 (1.3123e+00)	Acc@1  66.00 ( 74.19)	Acc@5  90.00 ( 93.32)
 * Acc@1 74.410 Acc@5 93.280
### epoch[69] execution time: 73.68251776695251
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.409 ( 0.409)	Data  0.224 ( 0.224)	Loss 1.5262e-02 (1.5262e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.164 ( 0.190)	Data  0.001 ( 0.024)	Loss 1.2334e-02 (2.1079e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.161 ( 0.181)	Data  0.001 ( 0.014)	Loss 1.3947e-02 (2.0512e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.169 ( 0.178)	Data  0.001 ( 0.011)	Loss 2.0643e-02 (2.0711e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.159 ( 0.174)	Data  0.001 ( 0.009)	Loss 2.7908e-02 (2.1478e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.154 ( 0.172)	Data  0.001 ( 0.009)	Loss 1.1935e-02 (2.0850e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.190 ( 0.171)	Data  0.002 ( 0.008)	Loss 1.2922e-02 (2.0382e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.179 ( 0.173)	Data  0.001 ( 0.007)	Loss 2.3171e-02 (2.0290e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.184 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.0538e-02 (1.9880e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.7380e-02 (2.0017e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.174 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.9674e-02 (2.0659e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.173 ( 0.174)	Data  0.002 ( 0.006)	Loss 1.4668e-02 (2.0517e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.180 ( 0.175)	Data  0.002 ( 0.006)	Loss 2.1896e-02 (2.0131e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.181 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.8861e-02 (1.9890e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][140/391]	Time  0.171 ( 0.175)	Data  0.002 ( 0.006)	Loss 1.1791e-02 (1.9798e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][150/391]	Time  0.164 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.2346e-02 (1.9957e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][160/391]	Time  0.164 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.5596e-02 (1.9897e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.006)	Loss 5.3622e-02 (2.0169e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.169 ( 0.174)	Data  0.001 ( 0.006)	Loss 2.5178e-02 (2.0280e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.163 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.3171e-02 (2.0263e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.7504e-02 (2.0012e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.3450e-02 (1.9923e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.1623e-02 (2.0075e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.7717e-02 (1.9937e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.166 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.2705e-02 (2.0027e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.163 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.3627e-02 (2.0216e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.1941e-02 (2.0171e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2688e-02 (2.0057e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.158 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.8080e-02 (2.0175e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.005)	Loss 3.2811e-02 (2.0092e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2944e-02 (2.0153e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.187 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.5179e-02 (2.0074e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.173 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0649e-02 (2.0109e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.172 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.5523e-02 (2.0203e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.5582e-02 (2.0410e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.9185e-02 (2.0414e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.7341e-02 (2.0337e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.179 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.2548e-02 (2.0391e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.7307e-02 (2.0424e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.5616e-02 (2.0350e-02)	Acc@1  98.75 ( 99.71)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.39240360260009766
## e[70]       loss.backward (sum) time: 25.616880178451538
## e[70]      optimizer.step (sum) time: 3.7667016983032227
## epoch[70] training(only) time: 67.51306366920471
# Switched to evaluate mode...
Test: [  0/100]	Time  0.288 ( 0.288)	Loss 1.5023e+00 (1.5023e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.059 ( 0.080)	Loss 1.6946e+00 (1.4081e+00)	Acc@1  68.00 ( 73.91)	Acc@5  89.00 ( 92.00)
Test: [ 20/100]	Time  0.059 ( 0.070)	Loss 9.0772e-01 (1.2812e+00)	Acc@1  78.00 ( 75.14)	Acc@5  98.00 ( 93.29)
Test: [ 30/100]	Time  0.056 ( 0.066)	Loss 1.9969e+00 (1.3453e+00)	Acc@1  62.00 ( 74.48)	Acc@5  91.00 ( 93.00)
Test: [ 40/100]	Time  0.058 ( 0.064)	Loss 1.4065e+00 (1.3327e+00)	Acc@1  74.00 ( 74.49)	Acc@5  95.00 ( 93.39)
Test: [ 50/100]	Time  0.058 ( 0.063)	Loss 1.7094e+00 (1.3377e+00)	Acc@1  66.00 ( 74.25)	Acc@5  93.00 ( 93.14)
Test: [ 60/100]	Time  0.058 ( 0.062)	Loss 1.3226e+00 (1.2999e+00)	Acc@1  71.00 ( 74.49)	Acc@5  91.00 ( 93.26)
Test: [ 70/100]	Time  0.054 ( 0.061)	Loss 1.9273e+00 (1.3178e+00)	Acc@1  70.00 ( 74.31)	Acc@5  89.00 ( 93.24)
Test: [ 80/100]	Time  0.054 ( 0.060)	Loss 1.7771e+00 (1.3294e+00)	Acc@1  72.00 ( 74.16)	Acc@5  88.00 ( 93.07)
Test: [ 90/100]	Time  0.053 ( 0.059)	Loss 1.8787e+00 (1.3081e+00)	Acc@1  65.00 ( 74.36)	Acc@5  91.00 ( 93.29)
 * Acc@1 74.580 Acc@5 93.250
### epoch[70] execution time: 73.49648451805115
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.357 ( 0.357)	Data  0.210 ( 0.210)	Loss 1.4011e-02 (1.4011e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.162 ( 0.181)	Data  0.001 ( 0.023)	Loss 1.1207e-02 (2.1545e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.015)	Loss 1.0193e-02 (2.0043e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.011)	Loss 2.7293e-02 (2.0108e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.174 ( 0.170)	Data  0.002 ( 0.010)	Loss 7.6636e-03 (1.9894e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.172 ( 0.170)	Data  0.002 ( 0.009)	Loss 1.5033e-02 (2.0320e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.166 ( 0.170)	Data  0.001 ( 0.008)	Loss 1.4540e-02 (2.0419e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.173 ( 0.170)	Data  0.001 ( 0.007)	Loss 2.4735e-02 (2.0455e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.179 ( 0.171)	Data  0.001 ( 0.007)	Loss 4.3829e-02 (2.0957e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.174 ( 0.171)	Data  0.002 ( 0.007)	Loss 1.8828e-02 (2.0354e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.165 ( 0.171)	Data  0.001 ( 0.007)	Loss 2.1395e-02 (2.0744e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.7393e-02 (2.0581e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.157 ( 0.169)	Data  0.001 ( 0.006)	Loss 7.3275e-03 (2.0264e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.177 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.7191e-02 (2.0560e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.179 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.7441e-02 (2.0355e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.178 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.7108e-02 (2.0242e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.9413e-02 (2.0225e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.5486e-02 (2.0312e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.174 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.4125e-02 (2.0286e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.184 ( 0.172)	Data  0.001 ( 0.006)	Loss 5.5890e-02 (2.0288e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.181 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.4110e-02 (2.0507e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.4410e-02 (2.0665e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.9180e-02 (2.0509e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.171 ( 0.172)	Data  0.001 ( 0.005)	Loss 9.9488e-03 (2.0350e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.164 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.3156e-02 (2.0346e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.0149e-02 (2.0217e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.2236e-02 (2.0369e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.4692e-02 (2.0279e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.157 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.1291e-02 (2.0174e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.1453e-02 (2.0024e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.172 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.0137e-02 (2.0028e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.3464e-02 (2.0236e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.0745e-02 (2.0220e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.9955e-02 (2.0114e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.5055e-03 (2.0144e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.7578e-02 (2.0116e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.160 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.7709e-02 (2.0040e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.0787e-02 (1.9932e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.182 ( 0.171)	Data  0.001 ( 0.005)	Loss 4.2776e-02 (1.9928e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.178 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.6675e-02 (1.9892e-02)	Acc@1  98.75 ( 99.73)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.3726367950439453
## e[71]       loss.backward (sum) time: 24.622398614883423
## e[71]      optimizer.step (sum) time: 3.6373367309570312
## epoch[71] training(only) time: 66.93262004852295
# Switched to evaluate mode...
Test: [  0/100]	Time  0.293 ( 0.293)	Loss 1.5193e+00 (1.5193e+00)	Acc@1  77.00 ( 77.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.059 ( 0.082)	Loss 1.7165e+00 (1.4167e+00)	Acc@1  69.00 ( 74.45)	Acc@5  89.00 ( 92.00)
Test: [ 20/100]	Time  0.060 ( 0.071)	Loss 9.0283e-01 (1.2889e+00)	Acc@1  79.00 ( 75.33)	Acc@5  98.00 ( 93.19)
Test: [ 30/100]	Time  0.058 ( 0.067)	Loss 2.0085e+00 (1.3501e+00)	Acc@1  60.00 ( 74.39)	Acc@5  91.00 ( 93.03)
Test: [ 40/100]	Time  0.059 ( 0.065)	Loss 1.4344e+00 (1.3382e+00)	Acc@1  73.00 ( 74.29)	Acc@5  94.00 ( 93.39)
Test: [ 50/100]	Time  0.058 ( 0.064)	Loss 1.7534e+00 (1.3452e+00)	Acc@1  66.00 ( 74.18)	Acc@5  92.00 ( 93.10)
Test: [ 60/100]	Time  0.059 ( 0.063)	Loss 1.2897e+00 (1.3073e+00)	Acc@1  73.00 ( 74.44)	Acc@5  92.00 ( 93.28)
Test: [ 70/100]	Time  0.059 ( 0.063)	Loss 1.9539e+00 (1.3260e+00)	Acc@1  70.00 ( 74.30)	Acc@5  87.00 ( 93.25)
Test: [ 80/100]	Time  0.059 ( 0.062)	Loss 1.7656e+00 (1.3374e+00)	Acc@1  74.00 ( 74.16)	Acc@5  90.00 ( 93.14)
Test: [ 90/100]	Time  0.062 ( 0.062)	Loss 1.8524e+00 (1.3147e+00)	Acc@1  66.00 ( 74.36)	Acc@5  91.00 ( 93.31)
 * Acc@1 74.620 Acc@5 93.290
### epoch[71] execution time: 73.22223591804504
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.407 ( 0.407)	Data  0.238 ( 0.238)	Loss 1.1037e-02 (1.1037e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.170 ( 0.198)	Data  0.002 ( 0.025)	Loss 9.8282e-03 (1.7640e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.164 ( 0.187)	Data  0.001 ( 0.015)	Loss 1.9363e-02 (1.9467e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.164 ( 0.181)	Data  0.001 ( 0.012)	Loss 2.2669e-02 (1.9556e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.166 ( 0.178)	Data  0.001 ( 0.010)	Loss 1.7605e-02 (2.0453e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.151 ( 0.175)	Data  0.001 ( 0.009)	Loss 2.0896e-02 (2.0480e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.157 ( 0.173)	Data  0.001 ( 0.008)	Loss 1.5177e-02 (2.0556e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.007)	Loss 1.1440e-02 (2.0345e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.156 ( 0.169)	Data  0.001 ( 0.007)	Loss 1.3227e-02 (2.0179e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.155 ( 0.168)	Data  0.001 ( 0.007)	Loss 1.1725e-02 (2.0459e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.170 ( 0.168)	Data  0.001 ( 0.006)	Loss 2.9344e-02 (2.0546e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.177 ( 0.168)	Data  0.002 ( 0.006)	Loss 3.4127e-02 (2.0635e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.175 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.3681e-02 (2.0364e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.181 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.4823e-02 (2.0033e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.168 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.5739e-02 (1.9929e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.168 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.5911e-02 (1.9709e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.153 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.3082e-02 (1.9561e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.163 ( 0.169)	Data  0.001 ( 0.006)	Loss 4.3456e-02 (1.9758e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.006)	Loss 2.7445e-02 (1.9583e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.183 ( 0.168)	Data  0.001 ( 0.006)	Loss 2.1071e-02 (1.9602e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.183 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.4630e-02 (1.9735e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.182 ( 0.170)	Data  0.002 ( 0.006)	Loss 3.0827e-02 (2.0185e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.173 ( 0.170)	Data  0.002 ( 0.005)	Loss 3.6778e-02 (2.0179e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.164 ( 0.170)	Data  0.001 ( 0.005)	Loss 9.1586e-03 (2.0109e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.180 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.8198e-02 (2.0091e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [72][250/391]	Time  0.183 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.0340e-02 (2.0287e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][260/391]	Time  0.182 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.2167e-02 (2.0182e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][270/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.4094e-02 (2.0198e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][280/391]	Time  0.176 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.1285e-02 (2.0098e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][290/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.8597e-02 (2.0155e-02)	Acc@1  99.22 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][300/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.9513e-02 (2.0169e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][310/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.4126e-02 (2.0012e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][320/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.0857e-02 (1.9981e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.156 ( 0.170)	Data  0.001 ( 0.005)	Loss 9.6011e-03 (2.0106e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.184 ( 0.169)	Data  0.001 ( 0.005)	Loss 9.1142e-03 (2.0137e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.173 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.2422e-02 (2.0149e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.173 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.0671e-02 (2.0128e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.183 ( 0.170)	Data  0.002 ( 0.005)	Loss 1.5051e-02 (2.0185e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.168 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.1814e-02 (2.0106e-02)	Acc@1  98.44 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.172 ( 0.170)	Data  0.001 ( 0.005)	Loss 5.1392e-02 (2.0251e-02)	Acc@1  98.75 ( 99.69)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.376434326171875
## e[72]       loss.backward (sum) time: 24.699620485305786
## e[72]      optimizer.step (sum) time: 3.6599643230438232
## epoch[72] training(only) time: 66.55182957649231
# Switched to evaluate mode...
Test: [  0/100]	Time  0.285 ( 0.285)	Loss 1.5179e+00 (1.5179e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.058 ( 0.080)	Loss 1.7003e+00 (1.4168e+00)	Acc@1  70.00 ( 74.55)	Acc@5  89.00 ( 92.00)
Test: [ 20/100]	Time  0.057 ( 0.069)	Loss 9.5890e-01 (1.2886e+00)	Acc@1  78.00 ( 75.38)	Acc@5  97.00 ( 93.24)
Test: [ 30/100]	Time  0.052 ( 0.064)	Loss 1.9580e+00 (1.3486e+00)	Acc@1  62.00 ( 74.45)	Acc@5  91.00 ( 93.10)
Test: [ 40/100]	Time  0.051 ( 0.061)	Loss 1.4141e+00 (1.3374e+00)	Acc@1  74.00 ( 74.39)	Acc@5  95.00 ( 93.39)
Test: [ 50/100]	Time  0.051 ( 0.059)	Loss 1.7617e+00 (1.3430e+00)	Acc@1  66.00 ( 74.22)	Acc@5  92.00 ( 93.12)
Test: [ 60/100]	Time  0.051 ( 0.058)	Loss 1.2913e+00 (1.3065e+00)	Acc@1  70.00 ( 74.44)	Acc@5  93.00 ( 93.28)
Test: [ 70/100]	Time  0.051 ( 0.057)	Loss 1.9109e+00 (1.3239e+00)	Acc@1  67.00 ( 74.23)	Acc@5  89.00 ( 93.24)
Test: [ 80/100]	Time  0.052 ( 0.056)	Loss 1.7675e+00 (1.3354e+00)	Acc@1  71.00 ( 74.09)	Acc@5  89.00 ( 93.07)
Test: [ 90/100]	Time  0.051 ( 0.056)	Loss 1.8680e+00 (1.3127e+00)	Acc@1  67.00 ( 74.26)	Acc@5  92.00 ( 93.31)
 * Acc@1 74.480 Acc@5 93.280
### epoch[72] execution time: 72.21245336532593
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.437 ( 0.437)	Data  0.268 ( 0.268)	Loss 2.7870e-02 (2.7870e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.177 ( 0.202)	Data  0.002 ( 0.028)	Loss 2.2344e-02 (1.7628e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.184 ( 0.192)	Data  0.001 ( 0.017)	Loss 2.0409e-02 (1.8506e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.171 ( 0.186)	Data  0.001 ( 0.013)	Loss 1.2188e-02 (1.8660e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.174 ( 0.183)	Data  0.001 ( 0.011)	Loss 3.2624e-02 (1.8780e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.183 ( 0.182)	Data  0.002 ( 0.010)	Loss 1.5074e-02 (1.8933e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.177 ( 0.182)	Data  0.002 ( 0.009)	Loss 1.6977e-02 (1.9322e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.182 ( 0.181)	Data  0.001 ( 0.008)	Loss 8.4287e-03 (1.8725e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [73][ 80/391]	Time  0.169 ( 0.180)	Data  0.001 ( 0.008)	Loss 1.9744e-02 (1.8571e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [73][ 90/391]	Time  0.173 ( 0.179)	Data  0.001 ( 0.007)	Loss 2.1947e-02 (1.8426e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [73][100/391]	Time  0.176 ( 0.178)	Data  0.001 ( 0.007)	Loss 2.4368e-02 (1.8629e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [73][110/391]	Time  0.158 ( 0.177)	Data  0.001 ( 0.007)	Loss 2.3696e-02 (1.9164e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][120/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.007)	Loss 1.6805e-02 (1.8738e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [73][130/391]	Time  0.154 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.9621e-02 (1.8970e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][140/391]	Time  0.157 ( 0.173)	Data  0.001 ( 0.006)	Loss 3.2464e-02 (1.9576e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][150/391]	Time  0.175 ( 0.172)	Data  0.002 ( 0.006)	Loss 2.0644e-02 (1.9557e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][160/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.006)	Loss 3.0044e-02 (1.9625e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.170 ( 0.173)	Data  0.002 ( 0.006)	Loss 5.4498e-03 (1.9476e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.6781e-02 (1.9431e-02)	Acc@1  99.22 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.0362e-02 (1.9372e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.170 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.2435e-02 (1.9329e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.160 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.2238e-02 (1.9059e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.149 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.8242e-02 (1.8915e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.157 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.1349e-02 (1.9000e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.7819e-02 (1.8818e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.177 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.4550e-02 (1.9170e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.6892e-02 (1.9083e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.167 ( 0.172)	Data  0.002 ( 0.005)	Loss 3.2696e-02 (1.9212e-02)	Acc@1  98.44 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.166 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.6110e-02 (1.9183e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.165 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.6205e-02 (1.9130e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.183 ( 0.172)	Data  0.001 ( 0.005)	Loss 4.4813e-02 (1.9128e-02)	Acc@1  98.44 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.177 ( 0.172)	Data  0.001 ( 0.005)	Loss 9.5820e-03 (1.8995e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.165 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.7208e-02 (1.8993e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.3017e-02 (1.9005e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.6615e-02 (1.8967e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1105e-02 (1.8865e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.160 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.8944e-02 (1.8911e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.7747e-02 (1.8858e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.4582e-02 (1.8934e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.157 ( 0.171)	Data  0.001 ( 0.005)	Loss 5.3498e-02 (1.8949e-02)	Acc@1  96.25 ( 99.70)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.3798182010650635
## e[73]       loss.backward (sum) time: 24.922229528427124
## e[73]      optimizer.step (sum) time: 3.6731035709381104
## epoch[73] training(only) time: 66.85170364379883
# Switched to evaluate mode...
Test: [  0/100]	Time  0.304 ( 0.304)	Loss 1.5172e+00 (1.5172e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.057 ( 0.080)	Loss 1.7430e+00 (1.4183e+00)	Acc@1  68.00 ( 74.18)	Acc@5  89.00 ( 91.91)
Test: [ 20/100]	Time  0.057 ( 0.069)	Loss 9.1544e-01 (1.2880e+00)	Acc@1  77.00 ( 75.52)	Acc@5  96.00 ( 93.10)
Test: [ 30/100]	Time  0.057 ( 0.066)	Loss 1.9718e+00 (1.3503e+00)	Acc@1  60.00 ( 74.48)	Acc@5  90.00 ( 93.00)
Test: [ 40/100]	Time  0.056 ( 0.064)	Loss 1.4356e+00 (1.3403e+00)	Acc@1  73.00 ( 74.44)	Acc@5  95.00 ( 93.41)
Test: [ 50/100]	Time  0.057 ( 0.062)	Loss 1.7049e+00 (1.3437e+00)	Acc@1  66.00 ( 74.31)	Acc@5  91.00 ( 93.14)
Test: [ 60/100]	Time  0.057 ( 0.061)	Loss 1.3318e+00 (1.3059e+00)	Acc@1  71.00 ( 74.49)	Acc@5  91.00 ( 93.30)
Test: [ 70/100]	Time  0.058 ( 0.061)	Loss 1.8965e+00 (1.3241e+00)	Acc@1  70.00 ( 74.25)	Acc@5  88.00 ( 93.25)
Test: [ 80/100]	Time  0.059 ( 0.060)	Loss 1.7194e+00 (1.3346e+00)	Acc@1  71.00 ( 74.07)	Acc@5  89.00 ( 93.10)
Test: [ 90/100]	Time  0.057 ( 0.060)	Loss 1.9038e+00 (1.3125e+00)	Acc@1  65.00 ( 74.24)	Acc@5  91.00 ( 93.32)
 * Acc@1 74.490 Acc@5 93.270
### epoch[73] execution time: 72.9397120475769
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.373 ( 0.373)	Data  0.197 ( 0.197)	Loss 2.0581e-02 (2.0581e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.164 ( 0.192)	Data  0.001 ( 0.021)	Loss 1.4607e-02 (1.8902e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.161 ( 0.182)	Data  0.001 ( 0.013)	Loss 2.9157e-02 (1.9346e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.173 ( 0.180)	Data  0.002 ( 0.010)	Loss 1.9089e-02 (1.8256e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.179 ( 0.181)	Data  0.001 ( 0.009)	Loss 1.4916e-02 (1.9126e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.178 ( 0.180)	Data  0.001 ( 0.008)	Loss 2.1694e-02 (1.9405e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.163 ( 0.178)	Data  0.001 ( 0.008)	Loss 1.4880e-02 (1.9279e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.165 ( 0.177)	Data  0.001 ( 0.007)	Loss 1.1398e-02 (1.9081e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.181 ( 0.177)	Data  0.002 ( 0.007)	Loss 1.7395e-02 (1.9100e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.183 ( 0.177)	Data  0.001 ( 0.006)	Loss 2.3823e-02 (1.8885e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.181 ( 0.177)	Data  0.001 ( 0.006)	Loss 1.7315e-02 (1.9221e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.167 ( 0.177)	Data  0.002 ( 0.006)	Loss 2.8520e-02 (1.9749e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.167 ( 0.176)	Data  0.002 ( 0.006)	Loss 3.7050e-02 (1.9789e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.162 ( 0.176)	Data  0.002 ( 0.006)	Loss 2.6511e-02 (1.9807e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.168 ( 0.175)	Data  0.002 ( 0.006)	Loss 1.2906e-02 (1.9777e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.180 ( 0.175)	Data  0.002 ( 0.005)	Loss 1.8386e-02 (1.9429e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.164 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.9901e-02 (1.9295e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.168 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.7373e-02 (1.9522e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.6522e-02 (1.9228e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.164 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.8827e-02 (1.9416e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.172 ( 0.174)	Data  0.002 ( 0.005)	Loss 4.4480e-02 (1.9312e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.173 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.0461e-02 (1.9231e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.175 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.4134e-02 (1.9209e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.167 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.6954e-02 (1.9197e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.173 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.5005e-02 (1.9462e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.7302e-02 (1.9507e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.191 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.2735e-02 (1.9305e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.179 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.6636e-02 (1.9302e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.179 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.6318e-02 (1.9241e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.0151e-02 (1.9452e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.176 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.3802e-02 (1.9535e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.169 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.3441e-02 (1.9440e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.171 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.3411e-02 (1.9411e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.183 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.4085e-02 (1.9421e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.174 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.9112e-02 (1.9475e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.170 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.3819e-02 (1.9404e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.177 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.7598e-02 (1.9351e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.8712e-02 (1.9368e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.181 ( 0.174)	Data  0.002 ( 0.005)	Loss 2.9968e-02 (1.9290e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.181 ( 0.174)	Data  0.001 ( 0.005)	Loss 4.3357e-02 (1.9317e-02)	Acc@1  98.75 ( 99.76)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.4243350028991699
## e[74]       loss.backward (sum) time: 27.419456243515015
## e[74]      optimizer.step (sum) time: 4.022744178771973
## epoch[74] training(only) time: 68.070809841156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.251 ( 0.251)	Loss 1.5181e+00 (1.5181e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.063 ( 0.078)	Loss 1.7256e+00 (1.4286e+00)	Acc@1  69.00 ( 74.27)	Acc@5  89.00 ( 91.91)
Test: [ 20/100]	Time  0.059 ( 0.070)	Loss 9.1523e-01 (1.2936e+00)	Acc@1  78.00 ( 75.43)	Acc@5  96.00 ( 93.10)
Test: [ 30/100]	Time  0.059 ( 0.067)	Loss 1.9927e+00 (1.3567e+00)	Acc@1  61.00 ( 74.45)	Acc@5  91.00 ( 92.97)
Test: [ 40/100]	Time  0.059 ( 0.065)	Loss 1.4326e+00 (1.3459e+00)	Acc@1  74.00 ( 74.44)	Acc@5  95.00 ( 93.34)
Test: [ 50/100]	Time  0.060 ( 0.064)	Loss 1.7003e+00 (1.3484e+00)	Acc@1  65.00 ( 74.18)	Acc@5  92.00 ( 93.10)
Test: [ 60/100]	Time  0.060 ( 0.063)	Loss 1.2987e+00 (1.3101e+00)	Acc@1  70.00 ( 74.33)	Acc@5  92.00 ( 93.26)
Test: [ 70/100]	Time  0.057 ( 0.063)	Loss 1.9374e+00 (1.3272e+00)	Acc@1  70.00 ( 74.13)	Acc@5  88.00 ( 93.18)
Test: [ 80/100]	Time  0.058 ( 0.062)	Loss 1.7871e+00 (1.3389e+00)	Acc@1  73.00 ( 74.05)	Acc@5  90.00 ( 93.07)
Test: [ 90/100]	Time  0.062 ( 0.062)	Loss 1.8432e+00 (1.3151e+00)	Acc@1  68.00 ( 74.27)	Acc@5  90.00 ( 93.26)
 * Acc@1 74.510 Acc@5 93.240
### epoch[74] execution time: 74.35578799247742
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.395 ( 0.395)	Data  0.220 ( 0.220)	Loss 1.8366e-02 (1.8366e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.183 ( 0.201)	Data  0.001 ( 0.024)	Loss 3.4046e-02 (2.1793e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.179 ( 0.191)	Data  0.001 ( 0.015)	Loss 1.3813e-02 (1.9501e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.174 ( 0.185)	Data  0.001 ( 0.011)	Loss 2.8090e-02 (1.9625e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.164 ( 0.181)	Data  0.001 ( 0.010)	Loss 1.3963e-02 (1.8495e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.171 ( 0.178)	Data  0.001 ( 0.009)	Loss 1.5672e-02 (1.9439e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.173 ( 0.177)	Data  0.001 ( 0.008)	Loss 3.1971e-02 (1.9484e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.177 ( 0.177)	Data  0.001 ( 0.007)	Loss 1.6748e-02 (1.9238e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.162 ( 0.176)	Data  0.001 ( 0.007)	Loss 1.7308e-02 (1.8818e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.176 ( 0.176)	Data  0.001 ( 0.007)	Loss 1.8566e-02 (1.8748e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.176 ( 0.175)	Data  0.001 ( 0.007)	Loss 7.0908e-03 (1.8673e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.171 ( 0.175)	Data  0.001 ( 0.006)	Loss 3.0142e-02 (1.8703e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.183 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.2258e-02 (1.8727e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.180 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.9957e-02 (1.8744e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.180 ( 0.176)	Data  0.001 ( 0.006)	Loss 4.2180e-02 (1.9027e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.164 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.3324e-02 (1.8938e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.162 ( 0.175)	Data  0.002 ( 0.006)	Loss 7.4929e-03 (1.8658e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.171 ( 0.175)	Data  0.002 ( 0.006)	Loss 2.5034e-02 (1.8588e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.177 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.5607e-02 (1.8502e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.182 ( 0.175)	Data  0.001 ( 0.006)	Loss 2.2880e-02 (1.8349e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.162 ( 0.175)	Data  0.001 ( 0.006)	Loss 8.0312e-03 (1.8484e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.175 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.5418e-02 (1.8552e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.174 ( 0.175)	Data  0.002 ( 0.005)	Loss 9.5874e-03 (1.8603e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.4322e-02 (1.8568e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.173 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.9868e-02 (1.8831e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.163 ( 0.174)	Data  0.002 ( 0.005)	Loss 3.0255e-02 (1.9045e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.5861e-02 (1.9080e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.161 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.1385e-02 (1.9073e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.161 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.0743e-02 (1.9217e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.7746e-02 (1.9267e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.168 ( 0.174)	Data  0.001 ( 0.005)	Loss 8.8611e-03 (1.9149e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.179 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.2335e-02 (1.9157e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.005)	Loss 8.5510e-03 (1.9187e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.1345e-02 (1.9203e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.166 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.1202e-02 (1.9070e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.182 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.5958e-02 (1.9192e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.181 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.0834e-02 (1.9200e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.8439e-02 (1.9102e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.166 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.8553e-02 (1.9074e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.174 ( 0.174)	Data  0.001 ( 0.005)	Loss 3.0658e-02 (1.9151e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.40273451805114746
## e[75]       loss.backward (sum) time: 26.69786286354065
## e[75]      optimizer.step (sum) time: 3.870666742324829
## epoch[75] training(only) time: 68.10211491584778
# Switched to evaluate mode...
Test: [  0/100]	Time  0.280 ( 0.280)	Loss 1.5770e+00 (1.5770e+00)	Acc@1  77.00 ( 77.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.056 ( 0.078)	Loss 1.7003e+00 (1.4204e+00)	Acc@1  68.00 ( 74.09)	Acc@5  89.00 ( 92.18)
Test: [ 20/100]	Time  0.050 ( 0.066)	Loss 9.0829e-01 (1.2878e+00)	Acc@1  80.00 ( 75.48)	Acc@5  98.00 ( 93.38)
Test: [ 30/100]	Time  0.066 ( 0.065)	Loss 1.9742e+00 (1.3506e+00)	Acc@1  62.00 ( 74.29)	Acc@5  90.00 ( 93.16)
Test: [ 40/100]	Time  0.059 ( 0.064)	Loss 1.4114e+00 (1.3384e+00)	Acc@1  74.00 ( 74.27)	Acc@5  95.00 ( 93.54)
Test: [ 50/100]	Time  0.060 ( 0.063)	Loss 1.7374e+00 (1.3446e+00)	Acc@1  66.00 ( 74.06)	Acc@5  91.00 ( 93.25)
Test: [ 60/100]	Time  0.064 ( 0.063)	Loss 1.3208e+00 (1.3064e+00)	Acc@1  72.00 ( 74.34)	Acc@5  90.00 ( 93.39)
Test: [ 70/100]	Time  0.060 ( 0.063)	Loss 1.9627e+00 (1.3250e+00)	Acc@1  70.00 ( 74.15)	Acc@5  88.00 ( 93.35)
Test: [ 80/100]	Time  0.059 ( 0.063)	Loss 1.7785e+00 (1.3372e+00)	Acc@1  73.00 ( 74.05)	Acc@5  89.00 ( 93.19)
Test: [ 90/100]	Time  0.063 ( 0.063)	Loss 1.8804e+00 (1.3147e+00)	Acc@1  66.00 ( 74.24)	Acc@5  89.00 ( 93.34)
 * Acc@1 74.450 Acc@5 93.280
### epoch[75] execution time: 74.47924947738647
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.430 ( 0.430)	Data  0.250 ( 0.250)	Loss 2.5458e-02 (2.5458e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.161 ( 0.193)	Data  0.001 ( 0.026)	Loss 1.3760e-02 (1.6716e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.163 ( 0.181)	Data  0.002 ( 0.015)	Loss 1.8787e-02 (1.9235e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.162 ( 0.175)	Data  0.001 ( 0.012)	Loss 1.6578e-02 (1.8309e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.180 ( 0.176)	Data  0.001 ( 0.010)	Loss 2.7309e-02 (1.9352e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.185 ( 0.177)	Data  0.002 ( 0.009)	Loss 1.3896e-02 (2.0138e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.173 ( 0.177)	Data  0.001 ( 0.008)	Loss 2.0639e-02 (2.0076e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.164 ( 0.176)	Data  0.001 ( 0.007)	Loss 8.4713e-03 (1.9552e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.176 ( 0.176)	Data  0.002 ( 0.007)	Loss 1.6213e-02 (1.9414e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.178 ( 0.176)	Data  0.002 ( 0.007)	Loss 4.1352e-02 (1.9652e-02)	Acc@1  98.44 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.175 ( 0.175)	Data  0.001 ( 0.007)	Loss 2.5855e-02 (1.9671e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.169 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.9600e-02 (1.9445e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.173 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.6641e-02 (1.9223e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.172 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.1297e-02 (1.9135e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.151 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.4973e-02 (1.8962e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.174 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.1778e-02 (1.8866e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.9511e-02 (1.9158e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.176 ( 0.174)	Data  0.002 ( 0.006)	Loss 2.0986e-02 (1.9105e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.159 ( 0.174)	Data  0.001 ( 0.006)	Loss 2.1166e-02 (1.9150e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.0008e-02 (1.9043e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.4981e-02 (1.9116e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.183 ( 0.173)	Data  0.002 ( 0.005)	Loss 2.9373e-02 (1.9146e-02)	Acc@1  99.22 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.005)	Loss 9.6522e-03 (1.9066e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.162 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.4726e-02 (1.9117e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.169 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.6479e-02 (1.8983e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.8437e-02 (1.9003e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.170 ( 0.173)	Data  0.002 ( 0.005)	Loss 2.2350e-02 (1.8935e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.005)	Loss 3.1352e-02 (1.8926e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5079e-02 (1.8830e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.161 ( 0.173)	Data  0.002 ( 0.005)	Loss 2.2370e-02 (1.9017e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.6725e-02 (1.9034e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.160 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2929e-02 (1.8972e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.171 ( 0.173)	Data  0.002 ( 0.005)	Loss 9.4564e-03 (1.8843e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.9614e-02 (1.8763e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.164 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.0834e-02 (1.8826e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.9768e-02 (1.8946e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.6761e-02 (1.8977e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2193e-02 (1.8975e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.6793e-02 (1.9020e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.6988e-02 (1.9073e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.3992323875427246
## e[76]       loss.backward (sum) time: 26.340351343154907
## e[76]      optimizer.step (sum) time: 3.87260365486145
## epoch[76] training(only) time: 67.6639153957367
# Switched to evaluate mode...
Test: [  0/100]	Time  0.277 ( 0.277)	Loss 1.5652e+00 (1.5652e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.060 ( 0.080)	Loss 1.7252e+00 (1.4139e+00)	Acc@1  67.00 ( 74.27)	Acc@5  90.00 ( 92.09)
Test: [ 20/100]	Time  0.061 ( 0.071)	Loss 9.1686e-01 (1.2840e+00)	Acc@1  78.00 ( 75.24)	Acc@5  96.00 ( 93.19)
Test: [ 30/100]	Time  0.057 ( 0.067)	Loss 1.9909e+00 (1.3468e+00)	Acc@1  60.00 ( 74.13)	Acc@5  90.00 ( 92.97)
Test: [ 40/100]	Time  0.058 ( 0.065)	Loss 1.4073e+00 (1.3331e+00)	Acc@1  73.00 ( 74.17)	Acc@5  95.00 ( 93.27)
Test: [ 50/100]	Time  0.059 ( 0.064)	Loss 1.6951e+00 (1.3389e+00)	Acc@1  67.00 ( 74.14)	Acc@5  92.00 ( 93.02)
Test: [ 60/100]	Time  0.058 ( 0.063)	Loss 1.2863e+00 (1.3008e+00)	Acc@1  73.00 ( 74.46)	Acc@5  92.00 ( 93.23)
Test: [ 70/100]	Time  0.060 ( 0.062)	Loss 1.9192e+00 (1.3175e+00)	Acc@1  70.00 ( 74.32)	Acc@5  88.00 ( 93.17)
Test: [ 80/100]	Time  0.059 ( 0.062)	Loss 1.7778e+00 (1.3312e+00)	Acc@1  72.00 ( 74.20)	Acc@5  88.00 ( 93.00)
Test: [ 90/100]	Time  0.057 ( 0.061)	Loss 1.8946e+00 (1.3093e+00)	Acc@1  64.00 ( 74.33)	Acc@5  91.00 ( 93.18)
 * Acc@1 74.520 Acc@5 93.150
### epoch[76] execution time: 73.87414574623108
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.413 ( 0.413)	Data  0.243 ( 0.243)	Loss 4.0152e-02 (4.0152e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.161 ( 0.197)	Data  0.001 ( 0.026)	Loss 1.0107e-02 (2.0234e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.178 ( 0.187)	Data  0.001 ( 0.016)	Loss 2.2559e-02 (2.1554e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.169 ( 0.182)	Data  0.001 ( 0.012)	Loss 7.8071e-03 (1.9369e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.170 ( 0.178)	Data  0.001 ( 0.010)	Loss 1.7956e-02 (1.9030e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.154 ( 0.176)	Data  0.001 ( 0.009)	Loss 1.2154e-02 (1.9403e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.173 ( 0.175)	Data  0.002 ( 0.008)	Loss 2.6578e-02 (1.9585e-02)	Acc@1  99.22 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.177 ( 0.174)	Data  0.002 ( 0.008)	Loss 3.7470e-02 (2.0055e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.165 ( 0.174)	Data  0.002 ( 0.007)	Loss 2.3738e-02 (2.0066e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.007)	Loss 2.6414e-02 (2.0272e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.6923e-02 (2.0661e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.7778e-02 (2.0611e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.184 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.9162e-02 (2.0941e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.180 ( 0.173)	Data  0.001 ( 0.006)	Loss 4.0292e-02 (2.1081e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.5924e-02 (2.0794e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.006)	Loss 8.2705e-03 (2.0751e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.6698e-02 (2.0352e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.1346e-02 (2.0229e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.3075e-02 (2.0030e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.1743e-02 (1.9967e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.006)	Loss 9.9514e-03 (1.9659e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.006)	Loss 3.1578e-02 (1.9709e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.151 ( 0.173)	Data  0.001 ( 0.006)	Loss 3.5409e-02 (1.9804e-02)	Acc@1  97.66 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.2650e-02 (1.9938e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.3812e-02 (1.9962e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.3504e-02 (1.9805e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.4550e-02 (1.9674e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.180 ( 0.173)	Data  0.001 ( 0.005)	Loss 3.5513e-02 (1.9603e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1571e-02 (1.9612e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.171 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.1643e-02 (1.9596e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.184 ( 0.173)	Data  0.002 ( 0.005)	Loss 9.3753e-03 (1.9453e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2246e-02 (1.9394e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.162 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.6346e-02 (1.9406e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.164 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.6457e-02 (1.9482e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.169 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.6947e-02 (1.9307e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.4917e-02 (1.9238e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2100e-02 (1.9277e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 3.5061e-02 (1.9345e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.164 ( 0.173)	Data  0.002 ( 0.005)	Loss 3.5153e-02 (1.9301e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.155 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.7279e-03 (1.9173e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.3935697078704834
## e[77]       loss.backward (sum) time: 25.937713861465454
## e[77]      optimizer.step (sum) time: 3.7945199012756348
## epoch[77] training(only) time: 67.51959228515625
# Switched to evaluate mode...
Test: [  0/100]	Time  0.291 ( 0.291)	Loss 1.5626e+00 (1.5626e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.058 ( 0.082)	Loss 1.6899e+00 (1.4138e+00)	Acc@1  69.00 ( 73.64)	Acc@5  89.00 ( 92.18)
Test: [ 20/100]	Time  0.056 ( 0.071)	Loss 8.8482e-01 (1.2837e+00)	Acc@1  79.00 ( 75.05)	Acc@5  96.00 ( 93.48)
Test: [ 30/100]	Time  0.056 ( 0.066)	Loss 2.0047e+00 (1.3480e+00)	Acc@1  61.00 ( 74.13)	Acc@5  90.00 ( 93.29)
Test: [ 40/100]	Time  0.056 ( 0.064)	Loss 1.4195e+00 (1.3369e+00)	Acc@1  73.00 ( 74.15)	Acc@5  95.00 ( 93.56)
Test: [ 50/100]	Time  0.057 ( 0.063)	Loss 1.6965e+00 (1.3411e+00)	Acc@1  66.00 ( 73.96)	Acc@5  91.00 ( 93.22)
Test: [ 60/100]	Time  0.057 ( 0.062)	Loss 1.2995e+00 (1.3028e+00)	Acc@1  72.00 ( 74.36)	Acc@5  93.00 ( 93.46)
Test: [ 70/100]	Time  0.058 ( 0.061)	Loss 1.9541e+00 (1.3207e+00)	Acc@1  67.00 ( 74.21)	Acc@5  88.00 ( 93.37)
Test: [ 80/100]	Time  0.059 ( 0.061)	Loss 1.7315e+00 (1.3344e+00)	Acc@1  73.00 ( 74.02)	Acc@5  89.00 ( 93.21)
Test: [ 90/100]	Time  0.056 ( 0.060)	Loss 1.9061e+00 (1.3154e+00)	Acc@1  66.00 ( 74.25)	Acc@5  90.00 ( 93.31)
 * Acc@1 74.420 Acc@5 93.230
### epoch[77] execution time: 73.61947822570801
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.369 ( 0.369)	Data  0.204 ( 0.204)	Loss 3.1311e-02 (3.1311e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.170 ( 0.186)	Data  0.001 ( 0.022)	Loss 1.4639e-02 (2.0920e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.014)	Loss 1.3305e-02 (1.8175e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.182 ( 0.178)	Data  0.001 ( 0.011)	Loss 7.2806e-03 (1.6563e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.183 ( 0.179)	Data  0.001 ( 0.009)	Loss 1.2477e-02 (1.7456e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.160 ( 0.178)	Data  0.002 ( 0.008)	Loss 3.7226e-02 (1.8015e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.177 ( 0.177)	Data  0.001 ( 0.008)	Loss 1.2015e-02 (1.8670e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 70/391]	Time  0.165 ( 0.175)	Data  0.002 ( 0.007)	Loss 1.0980e-02 (1.8377e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 80/391]	Time  0.176 ( 0.175)	Data  0.001 ( 0.007)	Loss 9.8699e-03 (1.7953e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 ( 99.99)
Epoch: [78][ 90/391]	Time  0.174 ( 0.175)	Data  0.002 ( 0.006)	Loss 2.1320e-02 (1.7981e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 ( 99.99)
Epoch: [78][100/391]	Time  0.158 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.4513e-02 (1.7902e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 ( 99.99)
Epoch: [78][110/391]	Time  0.168 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.4666e-02 (1.8047e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 ( 99.99)
Epoch: [78][120/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.4924e-02 (1.8257e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 ( 99.99)
Epoch: [78][130/391]	Time  0.175 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.9695e-02 (1.8362e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 ( 99.99)
Epoch: [78][140/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.006)	Loss 9.3556e-03 (1.8304e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 ( 99.99)
Epoch: [78][150/391]	Time  0.161 ( 0.173)	Data  0.002 ( 0.006)	Loss 9.4243e-03 (1.8509e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 ( 99.99)
Epoch: [78][160/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.006)	Loss 2.4016e-02 (1.8656e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.4166e-02 (1.8544e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.3689e-02 (1.8619e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.182 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1378e-02 (1.8803e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.192 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.0122e-02 (1.8646e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.181 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2970e-02 (1.8440e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.7367e-02 (1.8511e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.2526e-02 (1.8434e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.005)	Loss 3.5161e-02 (1.8600e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.3462e-02 (1.8628e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.4291e-02 (1.8542e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.4355e-02 (1.8477e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.0508e-02 (1.8828e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.201 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.5305e-02 (1.8760e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.165 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.6842e-02 (1.8650e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.005)	Loss 4.6478e-02 (1.8723e-02)	Acc@1  97.66 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.1134e-02 (1.8688e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1960e-02 (1.8704e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.005)	Loss 2.5947e-02 (1.8810e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.1104e-02 (1.8751e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.174 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.4755e-02 (1.8783e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.177 ( 0.172)	Data  0.002 ( 0.005)	Loss 1.6784e-02 (1.8793e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.167 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.9467e-02 (1.8803e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.5962e-02 (1.8676e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.3872826099395752
## e[78]       loss.backward (sum) time: 25.437525987625122
## e[78]      optimizer.step (sum) time: 3.7214179039001465
## epoch[78] training(only) time: 67.55030035972595
# Switched to evaluate mode...
Test: [  0/100]	Time  0.258 ( 0.258)	Loss 1.5476e+00 (1.5476e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.060 ( 0.079)	Loss 1.7577e+00 (1.4321e+00)	Acc@1  68.00 ( 73.91)	Acc@5  89.00 ( 92.00)
Test: [ 20/100]	Time  0.062 ( 0.070)	Loss 9.3346e-01 (1.3008e+00)	Acc@1  77.00 ( 75.29)	Acc@5  97.00 ( 93.24)
Test: [ 30/100]	Time  0.058 ( 0.067)	Loss 2.0061e+00 (1.3663e+00)	Acc@1  62.00 ( 74.26)	Acc@5  91.00 ( 93.03)
Test: [ 40/100]	Time  0.057 ( 0.065)	Loss 1.4511e+00 (1.3528e+00)	Acc@1  73.00 ( 74.15)	Acc@5  96.00 ( 93.39)
Test: [ 50/100]	Time  0.058 ( 0.063)	Loss 1.6471e+00 (1.3528e+00)	Acc@1  65.00 ( 74.00)	Acc@5  92.00 ( 93.10)
Test: [ 60/100]	Time  0.055 ( 0.062)	Loss 1.2836e+00 (1.3124e+00)	Acc@1  71.00 ( 74.25)	Acc@5  92.00 ( 93.25)
Test: [ 70/100]	Time  0.056 ( 0.061)	Loss 1.9372e+00 (1.3283e+00)	Acc@1  70.00 ( 74.11)	Acc@5  88.00 ( 93.18)
Test: [ 80/100]	Time  0.055 ( 0.061)	Loss 1.7532e+00 (1.3400e+00)	Acc@1  71.00 ( 73.96)	Acc@5  89.00 ( 93.05)
Test: [ 90/100]	Time  0.055 ( 0.060)	Loss 1.8673e+00 (1.3166e+00)	Acc@1  66.00 ( 74.18)	Acc@5  91.00 ( 93.25)
 * Acc@1 74.410 Acc@5 93.240
### epoch[78] execution time: 73.64098477363586
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.394 ( 0.394)	Data  0.223 ( 0.223)	Loss 2.2159e-02 (2.2159e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.158 ( 0.185)	Data  0.001 ( 0.024)	Loss 2.1139e-02 (1.4195e-02)	Acc@1  99.22 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.164 ( 0.176)	Data  0.002 ( 0.014)	Loss 2.3155e-02 (1.6345e-02)	Acc@1 100.00 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.011)	Loss 2.0201e-02 (1.8336e-02)	Acc@1  99.22 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.178 ( 0.172)	Data  0.001 ( 0.009)	Loss 1.2875e-02 (1.8553e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.008)	Loss 2.1701e-02 (1.9024e-02)	Acc@1  99.22 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.161 ( 0.172)	Data  0.002 ( 0.008)	Loss 1.8684e-02 (1.9298e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.155 ( 0.170)	Data  0.001 ( 0.007)	Loss 1.6401e-02 (1.8931e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.164 ( 0.170)	Data  0.001 ( 0.007)	Loss 1.5011e-02 (1.8665e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.3179e-02 (1.8635e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.185 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.5144e-02 (1.9530e-02)	Acc@1  99.22 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.186 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.2923e-02 (1.9783e-02)	Acc@1 100.00 ( 99.65)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.2866e-02 (1.9779e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.167 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.3522e-02 (1.9489e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.1080e-02 (1.9490e-02)	Acc@1 100.00 ( 99.68)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.5633e-02 (1.9221e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.5901e-02 (1.9129e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.163 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.8312e-02 (1.9130e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.2592e-02 (1.8942e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.162 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.4807e-02 (1.9039e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.3888e-02 (1.9060e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.181 ( 0.171)	Data  0.001 ( 0.005)	Loss 8.0451e-03 (1.9094e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.9025e-02 (1.9170e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.0889e-02 (1.9143e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.5134e-02 (1.9157e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.168 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.3944e-02 (1.9060e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.176 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.1349e-02 (1.8984e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.183 ( 0.171)	Data  0.002 ( 0.005)	Loss 2.2270e-02 (1.9078e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.180 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.7177e-03 (1.8925e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.165 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.0802e-02 (1.9068e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.0291e-02 (1.9057e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.151 ( 0.171)	Data  0.002 ( 0.005)	Loss 3.1590e-02 (1.8986e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.2491e-02 (1.8861e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.1075e-02 (1.8882e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.3174e-02 (1.8951e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.8114e-02 (1.9069e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.171 ( 0.171)	Data  0.002 ( 0.005)	Loss 2.4397e-02 (1.9016e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.177 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.0163e-02 (1.8963e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.3926e-02 (1.8929e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.005)	Loss 5.6857e-02 (1.8922e-02)	Acc@1  98.75 ( 99.73)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.38519287109375
## e[79]       loss.backward (sum) time: 25.157236576080322
## e[79]      optimizer.step (sum) time: 3.728300094604492
## epoch[79] training(only) time: 66.94436430931091
# Switched to evaluate mode...
Test: [  0/100]	Time  0.279 ( 0.279)	Loss 1.5496e+00 (1.5496e+00)	Acc@1  77.00 ( 77.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.055 ( 0.076)	Loss 1.7294e+00 (1.4237e+00)	Acc@1  69.00 ( 74.64)	Acc@5  89.00 ( 91.91)
Test: [ 20/100]	Time  0.054 ( 0.066)	Loss 9.2805e-01 (1.2940e+00)	Acc@1  77.00 ( 75.43)	Acc@5  96.00 ( 93.10)
Test: [ 30/100]	Time  0.055 ( 0.063)	Loss 1.9909e+00 (1.3567e+00)	Acc@1  62.00 ( 74.39)	Acc@5  89.00 ( 92.77)
Test: [ 40/100]	Time  0.054 ( 0.061)	Loss 1.4122e+00 (1.3425e+00)	Acc@1  73.00 ( 74.37)	Acc@5  96.00 ( 93.20)
Test: [ 50/100]	Time  0.054 ( 0.060)	Loss 1.6778e+00 (1.3457e+00)	Acc@1  67.00 ( 74.29)	Acc@5  92.00 ( 92.96)
Test: [ 60/100]	Time  0.055 ( 0.059)	Loss 1.2923e+00 (1.3065e+00)	Acc@1  71.00 ( 74.67)	Acc@5  92.00 ( 93.13)
Test: [ 70/100]	Time  0.054 ( 0.058)	Loss 1.9187e+00 (1.3238e+00)	Acc@1  70.00 ( 74.51)	Acc@5  88.00 ( 93.08)
Test: [ 80/100]	Time  0.063 ( 0.058)	Loss 1.7643e+00 (1.3360e+00)	Acc@1  73.00 ( 74.32)	Acc@5  88.00 ( 92.96)
Test: [ 90/100]	Time  0.061 ( 0.058)	Loss 1.8765e+00 (1.3138e+00)	Acc@1  66.00 ( 74.48)	Acc@5  91.00 ( 93.16)
 * Acc@1 74.680 Acc@5 93.130
### epoch[79] execution time: 72.8541431427002
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.403 ( 0.403)	Data  0.243 ( 0.243)	Loss 1.9380e-02 (1.9380e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.180 ( 0.203)	Data  0.001 ( 0.026)	Loss 9.3749e-03 (2.1642e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.165 ( 0.188)	Data  0.001 ( 0.016)	Loss 2.2522e-02 (1.9434e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.165 ( 0.183)	Data  0.001 ( 0.012)	Loss 2.0956e-02 (1.8591e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.176 ( 0.179)	Data  0.002 ( 0.010)	Loss 1.3979e-02 (1.8081e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.173 ( 0.178)	Data  0.001 ( 0.009)	Loss 1.3291e-02 (1.8253e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.170 ( 0.177)	Data  0.001 ( 0.008)	Loss 8.2678e-03 (1.7927e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.169 ( 0.176)	Data  0.001 ( 0.008)	Loss 1.1126e-02 (1.7598e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.171 ( 0.175)	Data  0.001 ( 0.008)	Loss 3.2493e-02 (1.8390e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.173 ( 0.174)	Data  0.001 ( 0.007)	Loss 2.0402e-02 (1.8248e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.5868e-02 (1.8047e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.167 ( 0.174)	Data  0.001 ( 0.007)	Loss 8.6385e-03 (1.7984e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.007)	Loss 4.2103e-02 (1.8044e-02)	Acc@1  98.44 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.160 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.2612e-02 (1.8093e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.165 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.8504e-02 (1.7958e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.1108e-02 (1.7750e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.179 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.9602e-02 (1.7890e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.184 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.4834e-02 (1.7753e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.172 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.5160e-02 (1.7734e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.169 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.1286e-02 (1.7721e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.171 ( 0.173)	Data  0.002 ( 0.006)	Loss 2.9818e-02 (1.7681e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.1883e-02 (1.7549e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.172 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.1280e-02 (1.7747e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.6217e-02 (1.7882e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.157 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.7640e-02 (1.7917e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.7226e-02 (1.7870e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.8078e-02 (1.7920e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.1984e-02 (1.7950e-02)	Acc@1  97.66 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.3263e-02 (1.8038e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.155 ( 0.171)	Data  0.002 ( 0.005)	Loss 9.8105e-03 (1.7950e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.9123e-02 (1.7931e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.5710e-02 (1.7952e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][320/391]	Time  0.198 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.5914e-02 (1.7986e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.187 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.8123e-02 (1.8022e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.185 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.4567e-02 (1.8038e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.169 ( 0.171)	Data  0.002 ( 0.005)	Loss 2.3776e-02 (1.8069e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.9722e-02 (1.8014e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.6708e-02 (1.8012e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.1907e-02 (1.8070e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.005)	Loss 4.3392e-02 (1.8097e-02)	Acc@1  98.75 ( 99.78)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.37760329246520996
## e[80]       loss.backward (sum) time: 24.83973240852356
## e[80]      optimizer.step (sum) time: 3.6665194034576416
## epoch[80] training(only) time: 67.02631855010986
# Switched to evaluate mode...
Test: [  0/100]	Time  0.240 ( 0.240)	Loss 1.5543e+00 (1.5543e+00)	Acc@1  76.00 ( 76.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.053 ( 0.070)	Loss 1.6905e+00 (1.4135e+00)	Acc@1  70.00 ( 74.36)	Acc@5  89.00 ( 92.00)
Test: [ 20/100]	Time  0.053 ( 0.062)	Loss 9.1605e-01 (1.2885e+00)	Acc@1  78.00 ( 75.33)	Acc@5  97.00 ( 93.14)
Test: [ 30/100]	Time  0.053 ( 0.059)	Loss 1.9632e+00 (1.3504e+00)	Acc@1  61.00 ( 74.35)	Acc@5  91.00 ( 93.06)
Test: [ 40/100]	Time  0.053 ( 0.058)	Loss 1.4300e+00 (1.3356e+00)	Acc@1  74.00 ( 74.32)	Acc@5  95.00 ( 93.39)
Test: [ 50/100]	Time  0.053 ( 0.057)	Loss 1.6873e+00 (1.3408e+00)	Acc@1  66.00 ( 74.16)	Acc@5  92.00 ( 93.08)
Test: [ 60/100]	Time  0.052 ( 0.056)	Loss 1.2979e+00 (1.3025e+00)	Acc@1  72.00 ( 74.44)	Acc@5  92.00 ( 93.30)
Test: [ 70/100]	Time  0.058 ( 0.056)	Loss 1.9646e+00 (1.3202e+00)	Acc@1  69.00 ( 74.34)	Acc@5  88.00 ( 93.24)
Test: [ 80/100]	Time  0.058 ( 0.056)	Loss 1.7788e+00 (1.3337e+00)	Acc@1  72.00 ( 74.15)	Acc@5  89.00 ( 93.12)
Test: [ 90/100]	Time  0.061 ( 0.056)	Loss 1.8347e+00 (1.3109e+00)	Acc@1  66.00 ( 74.38)	Acc@5  89.00 ( 93.26)
 * Acc@1 74.590 Acc@5 93.230
### epoch[80] execution time: 72.77067947387695
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.409 ( 0.409)	Data  0.238 ( 0.238)	Loss 1.0860e-02 (1.0860e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.172 ( 0.194)	Data  0.001 ( 0.025)	Loss 1.5887e-02 (1.4474e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.160 ( 0.182)	Data  0.001 ( 0.015)	Loss 1.7692e-02 (1.7315e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.156 ( 0.175)	Data  0.001 ( 0.012)	Loss 9.9256e-03 (1.7280e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.157 ( 0.172)	Data  0.001 ( 0.010)	Loss 2.2128e-02 (1.7243e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.183 ( 0.171)	Data  0.001 ( 0.009)	Loss 1.4269e-02 (1.7162e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.179 ( 0.173)	Data  0.001 ( 0.008)	Loss 8.5204e-03 (1.6549e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.185 ( 0.175)	Data  0.001 ( 0.008)	Loss 1.0860e-02 (1.6882e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.164 ( 0.175)	Data  0.001 ( 0.007)	Loss 8.2327e-03 (1.6555e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.172 ( 0.174)	Data  0.002 ( 0.007)	Loss 2.0352e-02 (1.6937e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.170 ( 0.174)	Data  0.002 ( 0.007)	Loss 1.6122e-02 (1.6773e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.8478e-02 (1.7194e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.172 ( 0.173)	Data  0.001 ( 0.006)	Loss 8.1334e-03 (1.7475e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.5726e-02 (1.7647e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.169 ( 0.172)	Data  0.001 ( 0.006)	Loss 1.3958e-02 (1.8000e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.163 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.1623e-02 (1.8090e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.5417e-02 (1.8393e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.175 ( 0.172)	Data  0.001 ( 0.006)	Loss 3.3273e-02 (1.8815e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.155 ( 0.172)	Data  0.002 ( 0.006)	Loss 2.1457e-02 (1.8740e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.167 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.3766e-02 (1.8685e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.157 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.3956e-02 (1.8679e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.194 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.3620e-02 (1.8646e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.185 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.0281e-02 (1.8545e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.180 ( 0.171)	Data  0.001 ( 0.006)	Loss 3.4396e-02 (1.8643e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.170 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.4858e-02 (1.8621e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.167 ( 0.172)	Data  0.001 ( 0.005)	Loss 8.5482e-03 (1.8511e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.1658e-02 (1.8394e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.2404e-02 (1.8485e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.163 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.2792e-02 (1.8649e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.162 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.2452e-02 (1.8611e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.163 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.0757e-02 (1.8530e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.152 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.4406e-02 (1.8509e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.166 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.9781e-02 (1.8641e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.164 ( 0.170)	Data  0.002 ( 0.005)	Loss 2.9851e-02 (1.8710e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.2150e-02 (1.8696e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.158 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.7727e-02 (1.8617e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.169 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.4541e-02 (1.8597e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.7334e-02 (1.8561e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.179 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.5146e-02 (1.8743e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.177 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.6213e-02 (1.8721e-02)	Acc@1  98.75 ( 99.76)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.37222790718078613
## e[81]       loss.backward (sum) time: 24.532028675079346
## e[81]      optimizer.step (sum) time: 3.623927593231201
## epoch[81] training(only) time: 66.67124962806702
# Switched to evaluate mode...
Test: [  0/100]	Time  0.274 ( 0.274)	Loss 1.5551e+00 (1.5551e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.059 ( 0.078)	Loss 1.6890e+00 (1.4150e+00)	Acc@1  70.00 ( 73.73)	Acc@5  89.00 ( 92.27)
Test: [ 20/100]	Time  0.058 ( 0.069)	Loss 9.2041e-01 (1.2890e+00)	Acc@1  78.00 ( 75.19)	Acc@5  97.00 ( 93.38)
Test: [ 30/100]	Time  0.057 ( 0.065)	Loss 1.9782e+00 (1.3574e+00)	Acc@1  63.00 ( 74.23)	Acc@5  91.00 ( 93.06)
Test: [ 40/100]	Time  0.061 ( 0.063)	Loss 1.3992e+00 (1.3447e+00)	Acc@1  73.00 ( 74.27)	Acc@5  96.00 ( 93.44)
Test: [ 50/100]	Time  0.061 ( 0.063)	Loss 1.7025e+00 (1.3487e+00)	Acc@1  65.00 ( 74.08)	Acc@5  93.00 ( 93.18)
Test: [ 60/100]	Time  0.060 ( 0.063)	Loss 1.3070e+00 (1.3110e+00)	Acc@1  72.00 ( 74.34)	Acc@5  92.00 ( 93.34)
Test: [ 70/100]	Time  0.062 ( 0.062)	Loss 1.9923e+00 (1.3269e+00)	Acc@1  68.00 ( 74.24)	Acc@5  89.00 ( 93.31)
Test: [ 80/100]	Time  0.062 ( 0.062)	Loss 1.7445e+00 (1.3403e+00)	Acc@1  73.00 ( 74.02)	Acc@5  91.00 ( 93.16)
Test: [ 90/100]	Time  0.063 ( 0.062)	Loss 1.8620e+00 (1.3193e+00)	Acc@1  67.00 ( 74.29)	Acc@5  89.00 ( 93.30)
 * Acc@1 74.550 Acc@5 93.210
### epoch[81] execution time: 72.94519782066345
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.390 ( 0.390)	Data  0.218 ( 0.218)	Loss 1.0712e-02 (1.0712e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.165 ( 0.187)	Data  0.001 ( 0.023)	Loss 2.2018e-02 (1.8454e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.6421e-02 (1.7566e-02)	Acc@1  99.22 ( 99.63)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.163 ( 0.172)	Data  0.001 ( 0.011)	Loss 1.1762e-02 (1.8579e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.156 ( 0.170)	Data  0.001 ( 0.010)	Loss 1.6491e-02 (1.7886e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.157 ( 0.170)	Data  0.002 ( 0.009)	Loss 1.8206e-02 (1.8065e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.008)	Loss 1.4661e-02 (1.8391e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.008)	Loss 2.0261e-02 (1.8379e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.154 ( 0.170)	Data  0.001 ( 0.007)	Loss 2.4773e-02 (1.8504e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.007)	Loss 1.2467e-02 (1.8335e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.007)	Loss 2.0125e-02 (1.8427e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.182 ( 0.170)	Data  0.001 ( 0.007)	Loss 2.1659e-02 (1.8580e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.182 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.2600e-02 (1.8436e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.176 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.2389e-02 (1.8337e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.0034e-02 (1.8318e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.164 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.9350e-02 (1.8183e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.163 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.3101e-02 (1.7920e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.006)	Loss 6.0366e-03 (1.8202e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.5962e-02 (1.8632e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.006)	Loss 8.5625e-03 (1.8481e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.165 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.7171e-02 (1.8585e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.177 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.8137e-02 (1.8701e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.174 ( 0.170)	Data  0.002 ( 0.006)	Loss 1.0346e-02 (1.8725e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.175 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.7733e-02 (1.8622e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.156 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.0278e-02 (1.8805e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.006)	Loss 8.1763e-03 (1.8653e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.157 ( 0.169)	Data  0.001 ( 0.006)	Loss 5.5260e-03 (1.8434e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.177 ( 0.169)	Data  0.001 ( 0.005)	Loss 2.2341e-02 (1.8573e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.181 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.1178e-02 (1.8371e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.6873e-02 (1.8342e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.1608e-02 (1.8309e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.168 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.2101e-02 (1.8187e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.162 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.4790e-02 (1.8221e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.005)	Loss 2.0599e-02 (1.8262e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.5625e-02 (1.8276e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.6785e-02 (1.8256e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.005)	Loss 2.2454e-02 (1.8318e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.170 ( 0.169)	Data  0.002 ( 0.005)	Loss 2.8662e-02 (1.8377e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.183 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.3462e-02 (1.8346e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.173 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.4060e-02 (1.8405e-02)	Acc@1  98.75 ( 99.75)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.3605508804321289
## e[82]       loss.backward (sum) time: 23.71863079071045
## e[82]      optimizer.step (sum) time: 3.553738832473755
## epoch[82] training(only) time: 66.13617205619812
# Switched to evaluate mode...
Test: [  0/100]	Time  0.270 ( 0.270)	Loss 1.5342e+00 (1.5342e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.051 ( 0.074)	Loss 1.7076e+00 (1.4195e+00)	Acc@1  68.00 ( 74.18)	Acc@5  89.00 ( 92.00)
Test: [ 20/100]	Time  0.052 ( 0.063)	Loss 9.4038e-01 (1.2908e+00)	Acc@1  78.00 ( 75.48)	Acc@5  97.00 ( 93.19)
Test: [ 30/100]	Time  0.051 ( 0.060)	Loss 1.9829e+00 (1.3543e+00)	Acc@1  62.00 ( 74.42)	Acc@5  91.00 ( 93.10)
Test: [ 40/100]	Time  0.057 ( 0.058)	Loss 1.3880e+00 (1.3425e+00)	Acc@1  74.00 ( 74.41)	Acc@5  96.00 ( 93.56)
Test: [ 50/100]	Time  0.051 ( 0.057)	Loss 1.6929e+00 (1.3461e+00)	Acc@1  67.00 ( 74.20)	Acc@5  93.00 ( 93.25)
Test: [ 60/100]	Time  0.052 ( 0.056)	Loss 1.2967e+00 (1.3065e+00)	Acc@1  73.00 ( 74.61)	Acc@5  91.00 ( 93.41)
Test: [ 70/100]	Time  0.052 ( 0.055)	Loss 2.0024e+00 (1.3240e+00)	Acc@1  68.00 ( 74.45)	Acc@5  89.00 ( 93.39)
Test: [ 80/100]	Time  0.050 ( 0.055)	Loss 1.7560e+00 (1.3373e+00)	Acc@1  72.00 ( 74.20)	Acc@5  89.00 ( 93.21)
Test: [ 90/100]	Time  0.060 ( 0.055)	Loss 1.8703e+00 (1.3167e+00)	Acc@1  67.00 ( 74.34)	Acc@5  92.00 ( 93.36)
 * Acc@1 74.510 Acc@5 93.280
### epoch[82] execution time: 71.76286244392395
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.370 ( 0.370)	Data  0.195 ( 0.195)	Loss 8.5848e-03 (8.5848e-03)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.181 ( 0.197)	Data  0.001 ( 0.022)	Loss 1.9534e-02 (1.5409e-02)	Acc@1 100.00 ( 99.86)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.164 ( 0.186)	Data  0.002 ( 0.013)	Loss 1.8143e-02 (1.7029e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.175 ( 0.181)	Data  0.002 ( 0.010)	Loss 1.7548e-02 (1.6597e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.171 ( 0.179)	Data  0.001 ( 0.009)	Loss 7.5839e-03 (1.5819e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [83][ 50/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.008)	Loss 1.4030e-02 (1.6054e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [83][ 60/391]	Time  0.163 ( 0.173)	Data  0.001 ( 0.007)	Loss 2.6312e-02 (1.6082e-02)	Acc@1  98.44 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [83][ 70/391]	Time  0.159 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.6013e-02 (1.5767e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][ 80/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.007)	Loss 1.5981e-02 (1.5718e-02)	Acc@1  99.22 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][ 90/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.007)	Loss 1.7690e-02 (1.6001e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][100/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.2892e-02 (1.6100e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [83][110/391]	Time  0.174 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.6845e-02 (1.6434e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [83][120/391]	Time  0.166 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.9062e-02 (1.7090e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [83][130/391]	Time  0.163 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.7185e-02 (1.7754e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][140/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.6327e-02 (1.7793e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][150/391]	Time  0.162 ( 0.168)	Data  0.001 ( 0.006)	Loss 9.7036e-03 (1.8073e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][160/391]	Time  0.180 ( 0.168)	Data  0.002 ( 0.006)	Loss 1.3536e-02 (1.7993e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.183 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.4557e-02 (1.8020e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.177 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.4080e-02 (1.7853e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.165 ( 0.169)	Data  0.001 ( 0.005)	Loss 2.8650e-02 (1.7825e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.170 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.9597e-02 (1.7752e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.7151e-02 (1.7912e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.005)	Loss 8.0374e-03 (1.7777e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.158 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.3568e-02 (1.7833e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.1670e-02 (1.7821e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.156 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.2161e-02 (1.7926e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.176 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.6837e-02 (1.7793e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.172 ( 0.168)	Data  0.002 ( 0.005)	Loss 2.0971e-02 (1.7867e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.163 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.5120e-02 (1.7893e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.142 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.1363e-02 (1.7963e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.162 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.3736e-02 (1.7849e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.151 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.1167e-02 (1.7903e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.182 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.7555e-02 (1.7877e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.179 ( 0.168)	Data  0.001 ( 0.005)	Loss 3.2206e-02 (1.8002e-02)	Acc@1  98.44 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.186 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.3524e-02 (1.7915e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.173 ( 0.169)	Data  0.001 ( 0.005)	Loss 2.2135e-02 (1.7845e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.175 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.3620e-02 (1.7831e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.156 ( 0.169)	Data  0.001 ( 0.005)	Loss 2.5092e-02 (1.7919e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.156 ( 0.169)	Data  0.001 ( 0.005)	Loss 1.4173e-02 (1.7871e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.162 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.4035e-02 (1.7893e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.36161112785339355
## e[83]       loss.backward (sum) time: 23.95731210708618
## e[83]      optimizer.step (sum) time: 3.5317325592041016
## epoch[83] training(only) time: 65.94016623497009
# Switched to evaluate mode...
Test: [  0/100]	Time  0.245 ( 0.245)	Loss 1.5433e+00 (1.5433e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.052 ( 0.070)	Loss 1.6836e+00 (1.4226e+00)	Acc@1  67.00 ( 73.64)	Acc@5  90.00 ( 92.27)
Test: [ 20/100]	Time  0.052 ( 0.061)	Loss 9.3333e-01 (1.2915e+00)	Acc@1  80.00 ( 75.14)	Acc@5  97.00 ( 93.29)
Test: [ 30/100]	Time  0.051 ( 0.058)	Loss 1.9720e+00 (1.3602e+00)	Acc@1  62.00 ( 74.06)	Acc@5  90.00 ( 93.06)
Test: [ 40/100]	Time  0.051 ( 0.056)	Loss 1.4004e+00 (1.3441e+00)	Acc@1  74.00 ( 74.15)	Acc@5  96.00 ( 93.41)
Test: [ 50/100]	Time  0.051 ( 0.055)	Loss 1.7156e+00 (1.3490e+00)	Acc@1  65.00 ( 73.90)	Acc@5  93.00 ( 93.20)
Test: [ 60/100]	Time  0.052 ( 0.055)	Loss 1.3293e+00 (1.3115e+00)	Acc@1  71.00 ( 74.23)	Acc@5  90.00 ( 93.30)
Test: [ 70/100]	Time  0.058 ( 0.055)	Loss 1.9731e+00 (1.3274e+00)	Acc@1  69.00 ( 74.18)	Acc@5  89.00 ( 93.30)
Test: [ 80/100]	Time  0.059 ( 0.055)	Loss 1.7481e+00 (1.3399e+00)	Acc@1  72.00 ( 74.04)	Acc@5  90.00 ( 93.17)
Test: [ 90/100]	Time  0.058 ( 0.056)	Loss 1.8503e+00 (1.3179e+00)	Acc@1  68.00 ( 74.23)	Acc@5  90.00 ( 93.35)
 * Acc@1 74.470 Acc@5 93.320
### epoch[83] execution time: 71.65719175338745
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.434 ( 0.434)	Data  0.245 ( 0.245)	Loss 1.4576e-02 (1.4576e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.174 ( 0.193)	Data  0.001 ( 0.026)	Loss 2.8224e-02 (1.8291e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.157 ( 0.178)	Data  0.001 ( 0.016)	Loss 8.7290e-03 (1.6844e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.159 ( 0.172)	Data  0.001 ( 0.012)	Loss 1.9154e-02 (1.7042e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.157 ( 0.168)	Data  0.001 ( 0.010)	Loss 2.4267e-02 (1.8256e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.184 ( 0.169)	Data  0.001 ( 0.009)	Loss 1.0458e-02 (1.8144e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.184 ( 0.172)	Data  0.001 ( 0.009)	Loss 6.7220e-03 (1.7267e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.008)	Loss 1.8296e-02 (1.7313e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.162 ( 0.173)	Data  0.002 ( 0.008)	Loss 1.8004e-02 (1.6807e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.4947e-02 (1.6491e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.160 ( 0.172)	Data  0.001 ( 0.007)	Loss 3.1723e-02 (1.7222e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.007)	Loss 2.2478e-02 (1.7410e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.159 ( 0.170)	Data  0.001 ( 0.007)	Loss 9.0066e-03 (1.7461e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.157 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.0667e-02 (1.7292e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.006)	Loss 2.0744e-02 (1.7569e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.174 ( 0.168)	Data  0.001 ( 0.006)	Loss 9.4102e-03 (1.7580e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.182 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.8965e-02 (1.7790e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.172 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.4326e-02 (1.7586e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.156 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.1723e-02 (1.7663e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.162 ( 0.168)	Data  0.001 ( 0.006)	Loss 3.3384e-02 (1.7879e-02)	Acc@1  99.22 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.8455e-02 (1.7743e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.184 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.5143e-02 (1.7649e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.181 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.2213e-02 (1.7544e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.0624e-02 (1.7594e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.174 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.3643e-02 (1.7641e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.172 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.7391e-02 (1.7695e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.152 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.5582e-02 (1.7600e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.005)	Loss 2.4937e-02 (1.7535e-02)	Acc@1  98.44 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.156 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.0943e-02 (1.7468e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.3235e-02 (1.7402e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.5614e-02 (1.7468e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.175 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.8778e-02 (1.7568e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.174 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.0940e-02 (1.7610e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.170 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.9465e-02 (1.7691e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.7934e-02 (1.7773e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.005)	Loss 1.6883e-02 (1.7813e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.159 ( 0.167)	Data  0.001 ( 0.005)	Loss 2.1407e-02 (1.7770e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.184 ( 0.168)	Data  0.001 ( 0.005)	Loss 8.8255e-03 (1.7724e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.184 ( 0.168)	Data  0.002 ( 0.005)	Loss 1.1638e-02 (1.7718e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.171 ( 0.168)	Data  0.001 ( 0.005)	Loss 2.7501e-02 (1.7671e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.3560318946838379
## e[84]       loss.backward (sum) time: 23.492175340652466
## e[84]      optimizer.step (sum) time: 3.509005308151245
## epoch[84] training(only) time: 66.00380229949951
# Switched to evaluate mode...
Test: [  0/100]	Time  0.234 ( 0.234)	Loss 1.5686e+00 (1.5686e+00)	Acc@1  76.00 ( 76.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.059 ( 0.075)	Loss 1.7213e+00 (1.4269e+00)	Acc@1  67.00 ( 73.82)	Acc@5  89.00 ( 91.82)
Test: [ 20/100]	Time  0.059 ( 0.067)	Loss 9.1462e-01 (1.2927e+00)	Acc@1  79.00 ( 75.29)	Acc@5  97.00 ( 93.19)
Test: [ 30/100]	Time  0.058 ( 0.064)	Loss 1.9590e+00 (1.3541e+00)	Acc@1  60.00 ( 74.42)	Acc@5  91.00 ( 92.97)
Test: [ 40/100]	Time  0.069 ( 0.063)	Loss 1.3994e+00 (1.3418e+00)	Acc@1  73.00 ( 74.27)	Acc@5  95.00 ( 93.32)
Test: [ 50/100]	Time  0.060 ( 0.063)	Loss 1.7433e+00 (1.3474e+00)	Acc@1  66.00 ( 74.20)	Acc@5  92.00 ( 93.04)
Test: [ 60/100]	Time  0.052 ( 0.062)	Loss 1.3346e+00 (1.3104e+00)	Acc@1  71.00 ( 74.48)	Acc@5  93.00 ( 93.20)
Test: [ 70/100]	Time  0.051 ( 0.060)	Loss 1.9291e+00 (1.3280e+00)	Acc@1  68.00 ( 74.34)	Acc@5  88.00 ( 93.15)
Test: [ 80/100]	Time  0.051 ( 0.059)	Loss 1.7642e+00 (1.3402e+00)	Acc@1  71.00 ( 74.09)	Acc@5  90.00 ( 93.06)
Test: [ 90/100]	Time  0.051 ( 0.058)	Loss 1.8938e+00 (1.3184e+00)	Acc@1  66.00 ( 74.22)	Acc@5  90.00 ( 93.23)
 * Acc@1 74.380 Acc@5 93.200
### epoch[84] execution time: 71.90153574943542
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.359 ( 0.359)	Data  0.216 ( 0.216)	Loss 2.5368e-02 (2.5368e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.159 ( 0.176)	Data  0.001 ( 0.023)	Loss 2.7487e-02 (1.9283e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.157 ( 0.167)	Data  0.001 ( 0.014)	Loss 3.9794e-02 (2.1737e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.176 ( 0.166)	Data  0.001 ( 0.011)	Loss 1.9480e-02 (2.2554e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.171 ( 0.168)	Data  0.001 ( 0.010)	Loss 1.5854e-02 (2.1569e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.171 ( 0.170)	Data  0.001 ( 0.009)	Loss 1.2428e-02 (2.0593e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.157 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.7296e-02 (2.0928e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.154 ( 0.168)	Data  0.002 ( 0.008)	Loss 1.5660e-02 (2.0185e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.007)	Loss 2.5631e-02 (1.9554e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.182 ( 0.167)	Data  0.001 ( 0.007)	Loss 8.8758e-03 (1.9450e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.184 ( 0.168)	Data  0.001 ( 0.007)	Loss 1.1860e-02 (1.9248e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.185 ( 0.170)	Data  0.001 ( 0.007)	Loss 8.4821e-03 (1.8916e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.169 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.9090e-02 (1.8885e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.164 ( 0.170)	Data  0.001 ( 0.006)	Loss 2.0556e-02 (1.8371e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.1612e-02 (1.8380e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.006)	Loss 2.0456e-02 (1.8260e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.5922e-02 (1.8469e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.4438e-02 (1.8388e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.158 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.5347e-02 (1.8446e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.174 ( 0.168)	Data  0.001 ( 0.006)	Loss 4.1209e-02 (1.8673e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.181 ( 0.168)	Data  0.001 ( 0.006)	Loss 1.3821e-02 (1.8545e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.172 ( 0.168)	Data  0.001 ( 0.006)	Loss 9.3941e-03 (1.8331e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.172 ( 0.169)	Data  0.002 ( 0.006)	Loss 2.4484e-02 (1.8298e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.181 ( 0.169)	Data  0.001 ( 0.006)	Loss 4.7202e-02 (1.8613e-02)	Acc@1  98.44 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.179 ( 0.170)	Data  0.001 ( 0.006)	Loss 1.4538e-02 (1.8575e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.168 ( 0.169)	Data  0.001 ( 0.006)	Loss 1.7183e-02 (1.8607e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.168 ( 0.169)	Data  0.001 ( 0.005)	Loss 3.0590e-02 (1.8694e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.156 ( 0.169)	Data  0.002 ( 0.005)	Loss 7.9907e-03 (1.8573e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.170 ( 0.169)	Data  0.001 ( 0.005)	Loss 5.7064e-03 (1.8471e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.169 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.0828e-02 (1.8394e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.173 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.8190e-02 (1.8497e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.179 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.9485e-02 (1.8319e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.182 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.4146e-02 (1.8337e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.165 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.6010e-02 (1.8398e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.2075e-02 (1.8422e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.163 ( 0.170)	Data  0.002 ( 0.005)	Loss 4.3031e-02 (1.8490e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.200 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.1321e-02 (1.8402e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.176 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.2996e-02 (1.8407e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.178 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.5018e-02 (1.8280e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.169 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.2560e-02 (1.8303e-02)	Acc@1  98.75 ( 99.76)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.3759586811065674
## e[85]       loss.backward (sum) time: 24.6943256855011
## e[85]      optimizer.step (sum) time: 3.673966646194458
## epoch[85] training(only) time: 66.80785465240479
# Switched to evaluate mode...
Test: [  0/100]	Time  0.249 ( 0.249)	Loss 1.5722e+00 (1.5722e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.063 ( 0.082)	Loss 1.6662e+00 (1.4190e+00)	Acc@1  68.00 ( 74.18)	Acc@5  89.00 ( 92.09)
Test: [ 20/100]	Time  0.063 ( 0.073)	Loss 9.5798e-01 (1.2883e+00)	Acc@1  79.00 ( 75.57)	Acc@5  97.00 ( 93.24)
Test: [ 30/100]	Time  0.061 ( 0.070)	Loss 1.9876e+00 (1.3505e+00)	Acc@1  62.00 ( 74.55)	Acc@5  90.00 ( 93.10)
Test: [ 40/100]	Time  0.061 ( 0.067)	Loss 1.3981e+00 (1.3395e+00)	Acc@1  73.00 ( 74.37)	Acc@5  96.00 ( 93.49)
Test: [ 50/100]	Time  0.059 ( 0.066)	Loss 1.7470e+00 (1.3455e+00)	Acc@1  65.00 ( 74.12)	Acc@5  92.00 ( 93.18)
Test: [ 60/100]	Time  0.062 ( 0.065)	Loss 1.3106e+00 (1.3077e+00)	Acc@1  72.00 ( 74.31)	Acc@5  92.00 ( 93.38)
Test: [ 70/100]	Time  0.061 ( 0.065)	Loss 1.9725e+00 (1.3237e+00)	Acc@1  67.00 ( 74.20)	Acc@5  89.00 ( 93.35)
Test: [ 80/100]	Time  0.062 ( 0.065)	Loss 1.7391e+00 (1.3355e+00)	Acc@1  70.00 ( 73.96)	Acc@5  90.00 ( 93.20)
Test: [ 90/100]	Time  0.062 ( 0.065)	Loss 1.8981e+00 (1.3136e+00)	Acc@1  68.00 ( 74.19)	Acc@5  90.00 ( 93.34)
 * Acc@1 74.410 Acc@5 93.290
### epoch[85] execution time: 73.34545040130615
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.407 ( 0.407)	Data  0.210 ( 0.210)	Loss 1.4111e-02 (1.4111e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.166 ( 0.192)	Data  0.002 ( 0.022)	Loss 1.1195e-02 (1.6334e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.166 ( 0.181)	Data  0.002 ( 0.013)	Loss 1.5020e-02 (1.8074e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.173 ( 0.177)	Data  0.001 ( 0.010)	Loss 1.6083e-02 (1.7714e-02)	Acc@1  99.22 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.177 ( 0.177)	Data  0.001 ( 0.009)	Loss 1.6400e-02 (1.9677e-02)	Acc@1 100.00 ( 99.66)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.172 ( 0.176)	Data  0.001 ( 0.008)	Loss 1.1257e-02 (1.8992e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.184 ( 0.176)	Data  0.001 ( 0.007)	Loss 1.2757e-02 (1.8851e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.182 ( 0.177)	Data  0.001 ( 0.007)	Loss 9.7344e-03 (1.8743e-02)	Acc@1 100.00 ( 99.67)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.177 ( 0.177)	Data  0.001 ( 0.007)	Loss 1.5222e-02 (1.8394e-02)	Acc@1 100.00 ( 99.69)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.171 ( 0.177)	Data  0.001 ( 0.006)	Loss 1.5238e-02 (1.8238e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.164 ( 0.176)	Data  0.002 ( 0.006)	Loss 4.4059e-03 (1.8344e-02)	Acc@1 100.00 ( 99.71)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.174 ( 0.176)	Data  0.002 ( 0.006)	Loss 8.6812e-03 (1.8005e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.179 ( 0.176)	Data  0.001 ( 0.006)	Loss 1.6835e-02 (1.7786e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.164 ( 0.175)	Data  0.001 ( 0.006)	Loss 7.7841e-03 (1.7970e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.177 ( 0.175)	Data  0.001 ( 0.006)	Loss 1.0322e-02 (1.8351e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.175 ( 0.175)	Data  0.001 ( 0.006)	Loss 3.0112e-02 (1.8294e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.184 ( 0.176)	Data  0.001 ( 0.006)	Loss 4.6354e-02 (1.8544e-02)	Acc@1  98.44 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.7077e-02 (1.8556e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.174 ( 0.175)	Data  0.001 ( 0.005)	Loss 2.1309e-02 (1.8294e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.150 ( 0.175)	Data  0.001 ( 0.005)	Loss 8.0487e-03 (1.8410e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.162 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.4094e-02 (1.8309e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.168 ( 0.174)	Data  0.002 ( 0.005)	Loss 2.4535e-02 (1.8137e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.161 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.1874e-02 (1.7958e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.182 ( 0.174)	Data  0.001 ( 0.005)	Loss 7.4855e-03 (1.8035e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.181 ( 0.175)	Data  0.002 ( 0.005)	Loss 3.2356e-02 (1.8169e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.169 ( 0.175)	Data  0.001 ( 0.005)	Loss 3.2360e-02 (1.8350e-02)	Acc@1  99.22 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.173 ( 0.175)	Data  0.001 ( 0.005)	Loss 1.8484e-02 (1.8374e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.171 ( 0.175)	Data  0.001 ( 0.005)	Loss 9.1641e-03 (1.8460e-02)	Acc@1 100.00 ( 99.73)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.176 ( 0.174)	Data  0.002 ( 0.005)	Loss 1.9069e-02 (1.8422e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.5327e-02 (1.8394e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.166 ( 0.174)	Data  0.002 ( 0.005)	Loss 2.4654e-02 (1.8334e-02)	Acc@1  98.44 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.171 ( 0.174)	Data  0.002 ( 0.005)	Loss 2.3908e-02 (1.8294e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.182 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.2538e-02 (1.8277e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.005)	Loss 9.8993e-03 (1.8247e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.174 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.0909e-02 (1.8244e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.165 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.3335e-02 (1.8162e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.162 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.3252e-02 (1.8180e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.162 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.5479e-02 (1.8275e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.2193e-02 (1.8215e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.172 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.6648e-02 (1.8145e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.40831780433654785
## e[86]       loss.backward (sum) time: 26.65568256378174
## e[86]      optimizer.step (sum) time: 3.9222211837768555
## epoch[86] training(only) time: 68.03823518753052
# Switched to evaluate mode...
Test: [  0/100]	Time  0.250 ( 0.250)	Loss 1.5957e+00 (1.5957e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.051 ( 0.074)	Loss 1.7508e+00 (1.4346e+00)	Acc@1  67.00 ( 74.00)	Acc@5  89.00 ( 92.09)
Test: [ 20/100]	Time  0.070 ( 0.068)	Loss 9.2901e-01 (1.2972e+00)	Acc@1  78.00 ( 75.67)	Acc@5  96.00 ( 93.24)
Test: [ 30/100]	Time  0.062 ( 0.066)	Loss 1.9588e+00 (1.3603e+00)	Acc@1  62.00 ( 74.35)	Acc@5  91.00 ( 93.03)
Test: [ 40/100]	Time  0.059 ( 0.064)	Loss 1.3892e+00 (1.3473e+00)	Acc@1  73.00 ( 74.22)	Acc@5  96.00 ( 93.41)
Test: [ 50/100]	Time  0.060 ( 0.063)	Loss 1.7134e+00 (1.3531e+00)	Acc@1  67.00 ( 74.08)	Acc@5  93.00 ( 93.20)
Test: [ 60/100]	Time  0.059 ( 0.062)	Loss 1.2999e+00 (1.3127e+00)	Acc@1  72.00 ( 74.44)	Acc@5  92.00 ( 93.41)
Test: [ 70/100]	Time  0.058 ( 0.062)	Loss 1.9443e+00 (1.3292e+00)	Acc@1  67.00 ( 74.18)	Acc@5  89.00 ( 93.39)
Test: [ 80/100]	Time  0.058 ( 0.061)	Loss 1.7673e+00 (1.3416e+00)	Acc@1  72.00 ( 73.98)	Acc@5  89.00 ( 93.27)
Test: [ 90/100]	Time  0.058 ( 0.061)	Loss 1.8859e+00 (1.3203e+00)	Acc@1  67.00 ( 74.13)	Acc@5  90.00 ( 93.42)
 * Acc@1 74.360 Acc@5 93.350
### epoch[86] execution time: 74.21451878547668
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.379 ( 0.379)	Data  0.216 ( 0.216)	Loss 2.3447e-02 (2.3447e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.164 ( 0.187)	Data  0.001 ( 0.023)	Loss 8.5802e-03 (1.5476e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.174 ( 0.180)	Data  0.002 ( 0.014)	Loss 2.8773e-02 (1.6825e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.163 ( 0.176)	Data  0.001 ( 0.011)	Loss 1.4187e-02 (1.6400e-02)	Acc@1 100.00 ( 99.82)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.168 ( 0.175)	Data  0.001 ( 0.009)	Loss 2.5661e-02 (1.6109e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.169 ( 0.174)	Data  0.001 ( 0.008)	Loss 1.2802e-02 (1.7342e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.172 ( 0.174)	Data  0.001 ( 0.008)	Loss 1.2654e-02 (1.7016e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.178 ( 0.175)	Data  0.001 ( 0.007)	Loss 1.8656e-02 (1.7393e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.007)	Loss 1.2834e-02 (1.7491e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.164 ( 0.174)	Data  0.001 ( 0.007)	Loss 1.5542e-02 (1.8036e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.175 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.2726e-02 (1.7837e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.170 ( 0.173)	Data  0.002 ( 0.006)	Loss 1.7330e-02 (1.8040e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.173 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.4876e-02 (1.7954e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.176 ( 0.173)	Data  0.001 ( 0.006)	Loss 8.4528e-03 (1.7714e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.176 ( 0.173)	Data  0.002 ( 0.006)	Loss 7.7202e-03 (1.7691e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.180 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.1161e-02 (1.7642e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.181 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.4133e-02 (1.7511e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.171 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.7743e-02 (1.7460e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.170 ( 0.174)	Data  0.001 ( 0.005)	Loss 1.4425e-02 (1.7468e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.152 ( 0.174)	Data  0.001 ( 0.005)	Loss 2.3113e-02 (1.7707e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.159 ( 0.173)	Data  0.001 ( 0.005)	Loss 3.0495e-02 (1.7885e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.005)	Loss 7.5673e-03 (1.7814e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.157 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.0495e-02 (1.7708e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][230/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.005)	Loss 8.5304e-03 (1.7726e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.175 ( 0.173)	Data  0.001 ( 0.005)	Loss 9.0453e-03 (1.7542e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.171 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.2338e-02 (1.7605e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.161 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.0878e-02 (1.7693e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.3474e-02 (1.7680e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.167 ( 0.173)	Data  0.002 ( 0.005)	Loss 1.7335e-02 (1.7639e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.166 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.3395e-02 (1.7600e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2719e-02 (1.7600e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.167 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.9571e-02 (1.7547e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.179 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.2819e-02 (1.7486e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.178 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.8505e-02 (1.7515e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.005)	Loss 9.0944e-03 (1.7572e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.005)	Loss 8.9147e-03 (1.7639e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.005)	Loss 2.1333e-02 (1.7664e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.161 ( 0.173)	Data  0.001 ( 0.005)	Loss 1.1040e-02 (1.7691e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.157 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.5557e-02 (1.7727e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.005)	Loss 7.8075e-02 (1.7728e-02)	Acc@1  97.50 ( 99.75)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.39659881591796875
## e[87]       loss.backward (sum) time: 26.143813848495483
## e[87]      optimizer.step (sum) time: 3.811314821243286
## epoch[87] training(only) time: 67.49626803398132
# Switched to evaluate mode...
Test: [  0/100]	Time  0.292 ( 0.292)	Loss 1.5549e+00 (1.5549e+00)	Acc@1  76.00 ( 76.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.059 ( 0.082)	Loss 1.6827e+00 (1.4107e+00)	Acc@1  70.00 ( 74.55)	Acc@5  89.00 ( 91.91)
Test: [ 20/100]	Time  0.059 ( 0.071)	Loss 9.3262e-01 (1.2857e+00)	Acc@1  78.00 ( 75.62)	Acc@5  97.00 ( 93.10)
Test: [ 30/100]	Time  0.057 ( 0.066)	Loss 2.0063e+00 (1.3464e+00)	Acc@1  63.00 ( 74.48)	Acc@5  91.00 ( 93.06)
Test: [ 40/100]	Time  0.058 ( 0.064)	Loss 1.3842e+00 (1.3357e+00)	Acc@1  74.00 ( 74.49)	Acc@5  95.00 ( 93.37)
Test: [ 50/100]	Time  0.056 ( 0.063)	Loss 1.7020e+00 (1.3428e+00)	Acc@1  67.00 ( 74.25)	Acc@5  92.00 ( 93.04)
Test: [ 60/100]	Time  0.056 ( 0.062)	Loss 1.3110e+00 (1.3035e+00)	Acc@1  72.00 ( 74.49)	Acc@5  91.00 ( 93.25)
Test: [ 70/100]	Time  0.055 ( 0.061)	Loss 1.9791e+00 (1.3227e+00)	Acc@1  69.00 ( 74.41)	Acc@5  90.00 ( 93.28)
Test: [ 80/100]	Time  0.056 ( 0.061)	Loss 1.7703e+00 (1.3371e+00)	Acc@1  72.00 ( 74.19)	Acc@5  89.00 ( 93.15)
Test: [ 90/100]	Time  0.056 ( 0.060)	Loss 1.9219e+00 (1.3165e+00)	Acc@1  63.00 ( 74.31)	Acc@5  92.00 ( 93.31)
 * Acc@1 74.530 Acc@5 93.330
### epoch[87] execution time: 73.60088324546814
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.365 ( 0.365)	Data  0.181 ( 0.181)	Loss 1.4988e-02 (1.4988e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.170 ( 0.187)	Data  0.001 ( 0.020)	Loss 1.9823e-02 (1.3660e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.163 ( 0.178)	Data  0.002 ( 0.013)	Loss 1.3645e-02 (1.3702e-02)	Acc@1 100.00 ( 99.96)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.176 ( 0.175)	Data  0.001 ( 0.010)	Loss 7.7497e-03 (1.5121e-02)	Acc@1 100.00 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.170 ( 0.174)	Data  0.002 ( 0.008)	Loss 1.8138e-02 (1.6027e-02)	Acc@1  99.22 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.178 ( 0.174)	Data  0.001 ( 0.008)	Loss 8.4640e-03 (1.6242e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.174 ( 0.174)	Data  0.001 ( 0.007)	Loss 2.6245e-02 (1.6865e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.007)	Loss 1.3937e-02 (1.7191e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.159 ( 0.174)	Data  0.001 ( 0.006)	Loss 1.5871e-02 (1.6978e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.168 ( 0.173)	Data  0.001 ( 0.006)	Loss 1.0797e-02 (1.7308e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.168 ( 0.172)	Data  0.002 ( 0.006)	Loss 1.8331e-02 (1.7342e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.162 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.3841e-02 (1.7492e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.160 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.6277e-02 (1.7637e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.006)	Loss 3.6522e-02 (1.8139e-02)	Acc@1  98.44 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.182 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.8647e-02 (1.8456e-02)	Acc@1  99.22 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.005)	Loss 1.2948e-02 (1.8596e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.162 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.5258e-02 (1.8646e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.164 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.5226e-02 (1.8515e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.7383e-02 (1.8340e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.1206e-02 (1.8151e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.171 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.5548e-02 (1.8524e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.0292e-02 (1.8689e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.178 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.7587e-03 (1.8529e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.4499e-02 (1.8698e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.177 ( 0.171)	Data  0.001 ( 0.005)	Loss 8.7029e-03 (1.8587e-02)	Acc@1 100.00 ( 99.74)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.4926e-02 (1.8504e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.166 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.5928e-02 (1.8410e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.174 ( 0.171)	Data  0.002 ( 0.005)	Loss 7.7729e-03 (1.8386e-02)	Acc@1 100.00 ( 99.75)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.158 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.8495e-02 (1.8454e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.1200e-02 (1.8402e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.189 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.3644e-02 (1.8262e-02)	Acc@1 100.00 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.175 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.9713e-02 (1.8221e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.176 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.8727e-02 (1.8252e-02)	Acc@1  99.22 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.164 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.1281e-02 (1.8195e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.162 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.8270e-02 (1.8249e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.159 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.9021e-02 (1.8285e-02)	Acc@1  99.22 ( 99.76)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.165 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.4123e-03 (1.8157e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.173 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.3435e-02 (1.8105e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.162 ( 0.171)	Data  0.002 ( 0.005)	Loss 1.4003e-02 (1.8181e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 2.9509e-02 (1.8201e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.39147520065307617
## e[88]       loss.backward (sum) time: 25.776620864868164
## e[88]      optimizer.step (sum) time: 3.7645392417907715
## epoch[88] training(only) time: 66.9568681716919
# Switched to evaluate mode...
Test: [  0/100]	Time  0.222 ( 0.222)	Loss 1.5483e+00 (1.5483e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.057 ( 0.074)	Loss 1.7150e+00 (1.4357e+00)	Acc@1  69.00 ( 73.73)	Acc@5  90.00 ( 91.91)
Test: [ 20/100]	Time  0.057 ( 0.066)	Loss 9.0697e-01 (1.3000e+00)	Acc@1  77.00 ( 75.19)	Acc@5  96.00 ( 93.00)
Test: [ 30/100]	Time  0.057 ( 0.063)	Loss 1.9929e+00 (1.3653e+00)	Acc@1  61.00 ( 74.29)	Acc@5  91.00 ( 92.81)
Test: [ 40/100]	Time  0.055 ( 0.061)	Loss 1.4487e+00 (1.3494e+00)	Acc@1  74.00 ( 74.44)	Acc@5  95.00 ( 93.24)
Test: [ 50/100]	Time  0.056 ( 0.060)	Loss 1.6818e+00 (1.3529e+00)	Acc@1  67.00 ( 74.33)	Acc@5  91.00 ( 93.02)
Test: [ 60/100]	Time  0.056 ( 0.059)	Loss 1.3492e+00 (1.3134e+00)	Acc@1  70.00 ( 74.56)	Acc@5  92.00 ( 93.18)
Test: [ 70/100]	Time  0.054 ( 0.059)	Loss 1.9026e+00 (1.3305e+00)	Acc@1  70.00 ( 74.45)	Acc@5  89.00 ( 93.17)
Test: [ 80/100]	Time  0.056 ( 0.058)	Loss 1.7640e+00 (1.3431e+00)	Acc@1  71.00 ( 74.20)	Acc@5  89.00 ( 93.04)
Test: [ 90/100]	Time  0.057 ( 0.058)	Loss 1.8661e+00 (1.3203e+00)	Acc@1  65.00 ( 74.36)	Acc@5  91.00 ( 93.22)
 * Acc@1 74.530 Acc@5 93.180
### epoch[88] execution time: 72.83925175666809
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.391 ( 0.391)	Data  0.215 ( 0.215)	Loss 1.3552e-02 (1.3552e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.162 ( 0.185)	Data  0.001 ( 0.023)	Loss 8.6508e-03 (1.4062e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.014)	Loss 1.5738e-02 (1.5224e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.171 ( 0.173)	Data  0.001 ( 0.011)	Loss 1.5613e-02 (1.5170e-02)	Acc@1 100.00 ( 99.95)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.009)	Loss 2.0642e-02 (1.5097e-02)	Acc@1  99.22 ( 99.94)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.177 ( 0.173)	Data  0.001 ( 0.008)	Loss 1.8976e-02 (1.5439e-02)	Acc@1 100.00 ( 99.92)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.169 ( 0.173)	Data  0.001 ( 0.008)	Loss 1.0165e-02 (1.5765e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.174 ( 0.173)	Data  0.001 ( 0.007)	Loss 1.2735e-02 (1.6123e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.166 ( 0.172)	Data  0.001 ( 0.007)	Loss 1.1293e-02 (1.6024e-02)	Acc@1 100.00 ( 99.88)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.007)	Loss 2.0739e-02 (1.6540e-02)	Acc@1  99.22 ( 99.87)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.170 ( 0.172)	Data  0.001 ( 0.006)	Loss 6.0444e-03 (1.6405e-02)	Acc@1 100.00 ( 99.85)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.0265e-02 (1.6344e-02)	Acc@1 100.00 ( 99.83)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.176 ( 0.172)	Data  0.001 ( 0.006)	Loss 5.2074e-02 (1.6603e-02)	Acc@1  97.66 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.168 ( 0.172)	Data  0.001 ( 0.006)	Loss 2.0444e-02 (1.6698e-02)	Acc@1  99.22 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.160 ( 0.172)	Data  0.001 ( 0.006)	Loss 4.5039e-02 (1.6928e-02)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.169 ( 0.171)	Data  0.002 ( 0.006)	Loss 1.6816e-02 (1.6739e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.160 ( 0.171)	Data  0.001 ( 0.006)	Loss 1.0901e-02 (1.6926e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.172 ( 0.171)	Data  0.001 ( 0.006)	Loss 2.4588e-02 (1.7126e-02)	Acc@1  99.22 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.170 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.6762e-02 (1.7191e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.0761e-02 (1.7066e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.177 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.4143e-02 (1.7141e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.2298e-02 (1.7047e-02)	Acc@1 100.00 ( 99.80)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.6724e-02 (1.7152e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.168 ( 0.171)	Data  0.001 ( 0.005)	Loss 3.5081e-02 (1.7118e-02)	Acc@1  98.44 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.171 ( 0.171)	Data  0.001 ( 0.005)	Loss 8.9275e-03 (1.7104e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.182 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.8167e-02 (1.7241e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.169 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.4093e-02 (1.7269e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.2036e-02 (1.7302e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.179 ( 0.171)	Data  0.001 ( 0.005)	Loss 9.4890e-03 (1.7205e-02)	Acc@1 100.00 ( 99.77)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.165 ( 0.171)	Data  0.002 ( 0.005)	Loss 2.1672e-02 (1.7235e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.174 ( 0.171)	Data  0.001 ( 0.005)	Loss 1.4192e-02 (1.7122e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.175 ( 0.171)	Data  0.001 ( 0.005)	Loss 7.7377e-03 (1.7069e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.005)	Loss 5.7377e-03 (1.7005e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.163 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.9669e-02 (1.7002e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.164 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.1697e-02 (1.7040e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.167 ( 0.170)	Data  0.001 ( 0.005)	Loss 2.7333e-02 (1.6990e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.158 ( 0.170)	Data  0.001 ( 0.005)	Loss 6.6062e-03 (1.6983e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.171 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.6975e-02 (1.6935e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.176 ( 0.170)	Data  0.001 ( 0.005)	Loss 1.8430e-02 (1.7003e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.167 ( 0.170)	Data  0.001 ( 0.005)	Loss 3.2961e-02 (1.6992e-02)	Acc@1 100.00 ( 99.78)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.3765249252319336
## e[89]       loss.backward (sum) time: 24.80617666244507
## e[89]      optimizer.step (sum) time: 3.678403377532959
## epoch[89] training(only) time: 66.75943565368652
# Switched to evaluate mode...
Test: [  0/100]	Time  0.223 ( 0.223)	Loss 1.5861e+00 (1.5861e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.053 ( 0.070)	Loss 1.7159e+00 (1.4341e+00)	Acc@1  69.00 ( 74.00)	Acc@5  89.00 ( 92.00)
Test: [ 20/100]	Time  0.054 ( 0.062)	Loss 9.2113e-01 (1.2918e+00)	Acc@1  80.00 ( 75.48)	Acc@5  97.00 ( 93.33)
Test: [ 30/100]	Time  0.054 ( 0.060)	Loss 1.9954e+00 (1.3570e+00)	Acc@1  61.00 ( 74.26)	Acc@5  91.00 ( 93.13)
Test: [ 40/100]	Time  0.054 ( 0.058)	Loss 1.4275e+00 (1.3483e+00)	Acc@1  74.00 ( 74.37)	Acc@5  95.00 ( 93.46)
Test: [ 50/100]	Time  0.055 ( 0.057)	Loss 1.7052e+00 (1.3506e+00)	Acc@1  66.00 ( 74.20)	Acc@5  93.00 ( 93.22)
Test: [ 60/100]	Time  0.057 ( 0.057)	Loss 1.3481e+00 (1.3123e+00)	Acc@1  70.00 ( 74.46)	Acc@5  92.00 ( 93.39)
Test: [ 70/100]	Time  0.055 ( 0.057)	Loss 1.9798e+00 (1.3300e+00)	Acc@1  68.00 ( 74.28)	Acc@5  89.00 ( 93.35)
Test: [ 80/100]	Time  0.054 ( 0.057)	Loss 1.7514e+00 (1.3425e+00)	Acc@1  72.00 ( 74.02)	Acc@5  87.00 ( 93.12)
Test: [ 90/100]	Time  0.055 ( 0.056)	Loss 1.8768e+00 (1.3206e+00)	Acc@1  67.00 ( 74.15)	Acc@5  91.00 ( 93.30)
 * Acc@1 74.440 Acc@5 93.250
### epoch[89] execution time: 72.49331951141357
### Training complete:
#### total training(only) time: 6192.7348890304565
##### Total run time: 6743.369947433472
