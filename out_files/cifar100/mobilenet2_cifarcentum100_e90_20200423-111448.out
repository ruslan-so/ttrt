# Model: mobilenet2
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: False
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.mobilenet
<function mobilenet2 at 0x7fa8e1589f28>
# model requested: 'mobilenet2'
# printing out the model
MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)
      (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
# model is full precision
# Model: mobilenet2
# Dataset: cifarcentum
# Freezeout: False
# Low precision: False
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / full precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.331 ( 3.331)	Data  0.117 ( 0.117)	Loss 4.6257e+00 (4.6257e+00)	Acc@1   0.00 (  0.00)	Acc@5   1.56 (  1.56)
Epoch: [0][ 10/391]	Time  0.065 ( 0.362)	Data  0.001 ( 0.011)	Loss 4.7634e+00 (4.7320e+00)	Acc@1   1.56 (  0.78)	Acc@5   6.25 (  5.82)
Epoch: [0][ 20/391]	Time  0.064 ( 0.220)	Data  0.001 ( 0.007)	Loss 4.9214e+00 (4.7770e+00)	Acc@1   1.56 (  1.67)	Acc@5  10.94 (  7.40)
Epoch: [0][ 30/391]	Time  0.066 ( 0.170)	Data  0.001 ( 0.005)	Loss 4.5889e+00 (4.8048e+00)	Acc@1   4.69 (  1.86)	Acc@5   9.38 (  7.94)
Epoch: [0][ 40/391]	Time  0.064 ( 0.145)	Data  0.001 ( 0.004)	Loss 4.5306e+00 (4.7704e+00)	Acc@1   3.12 (  2.04)	Acc@5  12.50 (  8.56)
Epoch: [0][ 50/391]	Time  0.064 ( 0.129)	Data  0.001 ( 0.003)	Loss 4.2929e+00 (4.7069e+00)	Acc@1   2.34 (  2.08)	Acc@5  15.62 (  9.76)
Epoch: [0][ 60/391]	Time  0.063 ( 0.118)	Data  0.001 ( 0.003)	Loss 4.1959e+00 (4.6518e+00)	Acc@1   4.69 (  2.29)	Acc@5  15.62 ( 10.64)
Epoch: [0][ 70/391]	Time  0.063 ( 0.111)	Data  0.001 ( 0.003)	Loss 4.2802e+00 (4.6057e+00)	Acc@1   1.56 (  2.39)	Acc@5  13.28 ( 11.37)
Epoch: [0][ 80/391]	Time  0.065 ( 0.105)	Data  0.001 ( 0.003)	Loss 4.0810e+00 (4.5642e+00)	Acc@1   6.25 (  2.62)	Acc@5  25.00 ( 12.00)
Epoch: [0][ 90/391]	Time  0.066 ( 0.101)	Data  0.001 ( 0.002)	Loss 4.1563e+00 (4.5211e+00)	Acc@1   6.25 (  2.76)	Acc@5  21.09 ( 12.91)
Epoch: [0][100/391]	Time  0.067 ( 0.097)	Data  0.001 ( 0.002)	Loss 4.1429e+00 (4.4848e+00)	Acc@1   5.47 (  2.89)	Acc@5  20.31 ( 13.60)
Epoch: [0][110/391]	Time  0.070 ( 0.094)	Data  0.001 ( 0.002)	Loss 4.0738e+00 (4.4538e+00)	Acc@1   5.47 (  3.06)	Acc@5  21.88 ( 14.14)
Epoch: [0][120/391]	Time  0.063 ( 0.092)	Data  0.001 ( 0.002)	Loss 4.1345e+00 (4.4280e+00)	Acc@1   7.03 (  3.18)	Acc@5  21.88 ( 14.71)
Epoch: [0][130/391]	Time  0.063 ( 0.090)	Data  0.001 ( 0.002)	Loss 4.1547e+00 (4.4016e+00)	Acc@1   4.69 (  3.28)	Acc@5  21.88 ( 15.32)
Epoch: [0][140/391]	Time  0.066 ( 0.088)	Data  0.001 ( 0.002)	Loss 4.0429e+00 (4.3822e+00)	Acc@1   4.69 (  3.45)	Acc@5  22.66 ( 15.71)
Epoch: [0][150/391]	Time  0.066 ( 0.086)	Data  0.001 ( 0.002)	Loss 3.9512e+00 (4.3621e+00)	Acc@1   7.81 (  3.57)	Acc@5  22.66 ( 16.07)
Epoch: [0][160/391]	Time  0.064 ( 0.085)	Data  0.001 ( 0.002)	Loss 4.0768e+00 (4.3439e+00)	Acc@1   5.47 (  3.64)	Acc@5  26.56 ( 16.54)
Epoch: [0][170/391]	Time  0.060 ( 0.084)	Data  0.001 ( 0.002)	Loss 4.1859e+00 (4.3250e+00)	Acc@1   5.47 (  3.77)	Acc@5  21.88 ( 17.02)
Epoch: [0][180/391]	Time  0.066 ( 0.083)	Data  0.001 ( 0.002)	Loss 3.9477e+00 (4.3081e+00)	Acc@1   3.91 (  3.85)	Acc@5  26.56 ( 17.39)
Epoch: [0][190/391]	Time  0.066 ( 0.082)	Data  0.001 ( 0.002)	Loss 4.0674e+00 (4.2951e+00)	Acc@1   3.12 (  3.96)	Acc@5  25.00 ( 17.70)
Epoch: [0][200/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.002)	Loss 4.0568e+00 (4.2821e+00)	Acc@1   0.00 (  4.05)	Acc@5  20.31 ( 17.95)
Epoch: [0][210/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.002)	Loss 3.9115e+00 (4.2664e+00)	Acc@1   6.25 (  4.19)	Acc@5  28.12 ( 18.37)
Epoch: [0][220/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.002)	Loss 4.0002e+00 (4.2531e+00)	Acc@1   6.25 (  4.30)	Acc@5  26.56 ( 18.67)
Epoch: [0][230/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.002)	Loss 3.9231e+00 (4.2414e+00)	Acc@1   7.03 (  4.40)	Acc@5  25.78 ( 18.93)
Epoch: [0][240/391]	Time  0.064 ( 0.078)	Data  0.001 ( 0.002)	Loss 3.9519e+00 (4.2299e+00)	Acc@1   7.03 (  4.51)	Acc@5  26.56 ( 19.17)
Epoch: [0][250/391]	Time  0.065 ( 0.078)	Data  0.001 ( 0.002)	Loss 3.9518e+00 (4.2201e+00)	Acc@1   5.47 (  4.61)	Acc@5  28.91 ( 19.44)
Epoch: [0][260/391]	Time  0.066 ( 0.077)	Data  0.001 ( 0.002)	Loss 3.9233e+00 (4.2116e+00)	Acc@1   9.38 (  4.65)	Acc@5  25.78 ( 19.61)
Epoch: [0][270/391]	Time  0.063 ( 0.077)	Data  0.001 ( 0.002)	Loss 3.7046e+00 (4.1991e+00)	Acc@1  11.72 (  4.75)	Acc@5  35.94 ( 19.96)
Epoch: [0][280/391]	Time  0.065 ( 0.076)	Data  0.001 ( 0.002)	Loss 3.9628e+00 (4.1876e+00)	Acc@1  11.72 (  4.88)	Acc@5  25.00 ( 20.25)
Epoch: [0][290/391]	Time  0.067 ( 0.076)	Data  0.001 ( 0.002)	Loss 3.8604e+00 (4.1764e+00)	Acc@1   3.12 (  4.97)	Acc@5  25.78 ( 20.56)
Epoch: [0][300/391]	Time  0.060 ( 0.076)	Data  0.001 ( 0.001)	Loss 3.9317e+00 (4.1670e+00)	Acc@1   8.59 (  5.09)	Acc@5  26.56 ( 20.78)
Epoch: [0][310/391]	Time  0.067 ( 0.075)	Data  0.001 ( 0.001)	Loss 3.9034e+00 (4.1576e+00)	Acc@1   6.25 (  5.18)	Acc@5  29.69 ( 21.00)
Epoch: [0][320/391]	Time  0.066 ( 0.075)	Data  0.001 ( 0.001)	Loss 4.0190e+00 (4.1482e+00)	Acc@1   7.81 (  5.28)	Acc@5  25.00 ( 21.28)
Epoch: [0][330/391]	Time  0.067 ( 0.075)	Data  0.001 ( 0.001)	Loss 3.7868e+00 (4.1404e+00)	Acc@1  10.16 (  5.36)	Acc@5  32.03 ( 21.49)
Epoch: [0][340/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.001)	Loss 3.7637e+00 (4.1306e+00)	Acc@1  10.94 (  5.45)	Acc@5  30.47 ( 21.73)
Epoch: [0][350/391]	Time  0.064 ( 0.074)	Data  0.001 ( 0.001)	Loss 3.8276e+00 (4.1233e+00)	Acc@1  11.72 (  5.50)	Acc@5  32.81 ( 21.95)
Epoch: [0][360/391]	Time  0.061 ( 0.074)	Data  0.001 ( 0.001)	Loss 3.7631e+00 (4.1150e+00)	Acc@1  12.50 (  5.61)	Acc@5  30.47 ( 22.15)
Epoch: [0][370/391]	Time  0.063 ( 0.074)	Data  0.001 ( 0.001)	Loss 3.7952e+00 (4.1069e+00)	Acc@1   7.03 (  5.66)	Acc@5  32.81 ( 22.43)
Epoch: [0][380/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.001)	Loss 3.9778e+00 (4.0999e+00)	Acc@1   3.91 (  5.74)	Acc@5  28.12 ( 22.62)
Epoch: [0][390/391]	Time  0.416 ( 0.074)	Data  0.001 ( 0.001)	Loss 3.8274e+00 (4.0934e+00)	Acc@1  11.25 (  5.82)	Acc@5  32.50 ( 22.82)
## e[0] optimizer.zero_grad (sum) time: 0.4182260036468506
## e[0]       loss.backward (sum) time: 7.462890625
## e[0]      optimizer.step (sum) time: 3.4021692276000977
## epoch[0] training(only) time: 28.963165521621704
# Switched to evaluate mode...
Test: [  0/100]	Time  0.291 ( 0.291)	Loss 3.7181e+00 (3.7181e+00)	Acc@1  11.00 ( 11.00)	Acc@5  39.00 ( 39.00)
Test: [ 10/100]	Time  0.026 ( 0.052)	Loss 3.9572e+00 (3.8723e+00)	Acc@1   8.00 (  8.64)	Acc@5  29.00 ( 29.91)
Test: [ 20/100]	Time  0.026 ( 0.040)	Loss 3.9197e+00 (3.8838e+00)	Acc@1  11.00 (  8.67)	Acc@5  28.00 ( 30.52)
Test: [ 30/100]	Time  0.028 ( 0.036)	Loss 4.1154e+00 (3.8774e+00)	Acc@1   9.00 (  8.84)	Acc@5  23.00 ( 30.48)
Test: [ 40/100]	Time  0.026 ( 0.034)	Loss 3.8129e+00 (3.8710e+00)	Acc@1  10.00 (  9.10)	Acc@5  30.00 ( 30.49)
Test: [ 50/100]	Time  0.025 ( 0.032)	Loss 3.6834e+00 (3.8602e+00)	Acc@1  16.00 (  9.25)	Acc@5  33.00 ( 30.73)
Test: [ 60/100]	Time  0.026 ( 0.031)	Loss 3.9113e+00 (3.8584e+00)	Acc@1   5.00 (  9.13)	Acc@5  32.00 ( 30.64)
Test: [ 70/100]	Time  0.029 ( 0.031)	Loss 3.9990e+00 (3.8590e+00)	Acc@1   7.00 (  9.10)	Acc@5  26.00 ( 30.34)
Test: [ 80/100]	Time  0.025 ( 0.030)	Loss 4.0744e+00 (3.8735e+00)	Acc@1   7.00 (  8.90)	Acc@5  28.00 ( 30.00)
Test: [ 90/100]	Time  0.027 ( 0.030)	Loss 3.8038e+00 (3.8738e+00)	Acc@1   8.00 (  8.99)	Acc@5  31.00 ( 30.18)
 * Acc@1 8.890 Acc@5 30.270
### epoch[0] execution time: 31.97773575782776
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.237 ( 0.237)	Data  0.154 ( 0.154)	Loss 3.7947e+00 (3.7947e+00)	Acc@1   7.03 (  7.03)	Acc@5  32.03 ( 32.03)
Epoch: [1][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.015)	Loss 3.8603e+00 (3.7314e+00)	Acc@1  10.94 ( 10.51)	Acc@5  31.25 ( 33.81)
Epoch: [1][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.008)	Loss 3.7789e+00 (3.7290e+00)	Acc@1   8.59 ( 11.09)	Acc@5  31.25 ( 34.26)
Epoch: [1][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.7297e+00 (3.7444e+00)	Acc@1   9.38 ( 10.13)	Acc@5  29.69 ( 33.09)
Epoch: [1][ 40/391]	Time  0.073 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.9181e+00 (3.7499e+00)	Acc@1   7.03 (  9.91)	Acc@5  26.56 ( 32.79)
Epoch: [1][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.8683e+00 (3.7472e+00)	Acc@1  11.72 ( 10.08)	Acc@5  33.59 ( 32.92)
Epoch: [1][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.6942e+00 (3.7375e+00)	Acc@1  13.28 ( 10.40)	Acc@5  32.03 ( 33.24)
Epoch: [1][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.7799e+00 (3.7392e+00)	Acc@1   9.38 ( 10.35)	Acc@5  31.25 ( 33.21)
Epoch: [1][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.8957e+00 (3.7402e+00)	Acc@1   9.38 ( 10.48)	Acc@5  28.91 ( 33.16)
Epoch: [1][ 90/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5420e+00 (3.7310e+00)	Acc@1  14.84 ( 10.62)	Acc@5  37.50 ( 33.35)
Epoch: [1][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.7455e+00 (3.7226e+00)	Acc@1   9.38 ( 10.76)	Acc@5  36.72 ( 33.62)
Epoch: [1][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5380e+00 (3.7165e+00)	Acc@1  14.84 ( 10.89)	Acc@5  39.84 ( 33.82)
Epoch: [1][120/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6761e+00 (3.7124e+00)	Acc@1  10.94 ( 10.98)	Acc@5  41.41 ( 34.01)
Epoch: [1][130/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6706e+00 (3.7068e+00)	Acc@1  11.72 ( 11.22)	Acc@5  39.06 ( 34.30)
Epoch: [1][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5634e+00 (3.7047e+00)	Acc@1   9.38 ( 11.23)	Acc@5  34.38 ( 34.43)
Epoch: [1][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4744e+00 (3.6968e+00)	Acc@1  22.66 ( 11.32)	Acc@5  45.31 ( 34.62)
Epoch: [1][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6237e+00 (3.6928e+00)	Acc@1  16.41 ( 11.44)	Acc@5  39.06 ( 34.82)
Epoch: [1][170/391]	Time  0.072 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5920e+00 (3.6898e+00)	Acc@1  10.16 ( 11.41)	Acc@5  39.06 ( 35.02)
Epoch: [1][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8686e+00 (3.6870e+00)	Acc@1   6.25 ( 11.46)	Acc@5  29.69 ( 35.16)
Epoch: [1][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5686e+00 (3.6842e+00)	Acc@1   9.38 ( 11.42)	Acc@5  39.06 ( 35.23)
Epoch: [1][200/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6470e+00 (3.6786e+00)	Acc@1  11.72 ( 11.53)	Acc@5  36.72 ( 35.40)
Epoch: [1][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4363e+00 (3.6750e+00)	Acc@1  10.94 ( 11.53)	Acc@5  44.53 ( 35.51)
Epoch: [1][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7568e+00 (3.6716e+00)	Acc@1   7.03 ( 11.61)	Acc@5  37.50 ( 35.61)
Epoch: [1][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5033e+00 (3.6678e+00)	Acc@1  11.72 ( 11.65)	Acc@5  39.84 ( 35.75)
Epoch: [1][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4818e+00 (3.6625e+00)	Acc@1  17.19 ( 11.78)	Acc@5  40.62 ( 35.88)
Epoch: [1][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3520e+00 (3.6573e+00)	Acc@1  12.50 ( 11.91)	Acc@5  46.09 ( 36.01)
Epoch: [1][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3172e+00 (3.6498e+00)	Acc@1  13.28 ( 12.02)	Acc@5  41.41 ( 36.21)
Epoch: [1][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4908e+00 (3.6448e+00)	Acc@1  12.50 ( 12.08)	Acc@5  40.62 ( 36.34)
Epoch: [1][280/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7121e+00 (3.6405e+00)	Acc@1   9.38 ( 12.17)	Acc@5  36.72 ( 36.50)
Epoch: [1][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7388e+00 (3.6373e+00)	Acc@1  10.16 ( 12.18)	Acc@5  35.94 ( 36.60)
Epoch: [1][300/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5327e+00 (3.6322e+00)	Acc@1  10.16 ( 12.22)	Acc@5  39.06 ( 36.75)
Epoch: [1][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6027e+00 (3.6266e+00)	Acc@1  17.19 ( 12.32)	Acc@5  42.97 ( 36.97)
Epoch: [1][320/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5138e+00 (3.6226e+00)	Acc@1  19.53 ( 12.41)	Acc@5  41.41 ( 37.08)
Epoch: [1][330/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3631e+00 (3.6170e+00)	Acc@1  16.41 ( 12.49)	Acc@5  46.09 ( 37.27)
Epoch: [1][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6196e+00 (3.6130e+00)	Acc@1  13.28 ( 12.57)	Acc@5  42.97 ( 37.43)
Epoch: [1][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4147e+00 (3.6075e+00)	Acc@1  18.75 ( 12.69)	Acc@5  42.19 ( 37.62)
Epoch: [1][360/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4789e+00 (3.6025e+00)	Acc@1  17.97 ( 12.79)	Acc@5  38.28 ( 37.71)
Epoch: [1][370/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4056e+00 (3.5995e+00)	Acc@1  19.53 ( 12.83)	Acc@5  41.41 ( 37.81)
Epoch: [1][380/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3410e+00 (3.5959e+00)	Acc@1  19.53 ( 12.87)	Acc@5  48.44 ( 37.93)
Epoch: [1][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3293e+00 (3.5923e+00)	Acc@1  13.75 ( 12.96)	Acc@5  46.25 ( 38.05)
## e[1] optimizer.zero_grad (sum) time: 0.4112131595611572
## e[1]       loss.backward (sum) time: 6.964248180389404
## e[1]      optimizer.step (sum) time: 3.527853488922119
## epoch[1] training(only) time: 25.667388439178467
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 3.5631e+00 (3.5631e+00)	Acc@1  15.00 ( 15.00)	Acc@5  46.00 ( 46.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 3.4457e+00 (3.4160e+00)	Acc@1  14.00 ( 14.27)	Acc@5  40.00 ( 43.55)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 3.3605e+00 (3.4054e+00)	Acc@1  11.00 ( 15.29)	Acc@5  49.00 ( 43.95)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 3.7712e+00 (3.4245e+00)	Acc@1  12.00 ( 15.19)	Acc@5  35.00 ( 43.45)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 3.4375e+00 (3.4082e+00)	Acc@1  17.00 ( 15.71)	Acc@5  44.00 ( 43.73)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 3.3961e+00 (3.4059e+00)	Acc@1  17.00 ( 15.75)	Acc@5  40.00 ( 43.78)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 3.1962e+00 (3.3969e+00)	Acc@1  17.00 ( 15.85)	Acc@5  49.00 ( 43.85)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 3.6087e+00 (3.3907e+00)	Acc@1  16.00 ( 16.00)	Acc@5  42.00 ( 44.17)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 3.4820e+00 (3.3979e+00)	Acc@1  17.00 ( 15.91)	Acc@5  41.00 ( 43.85)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 3.2341e+00 (3.3938e+00)	Acc@1  18.00 ( 16.10)	Acc@5  50.00 ( 44.11)
 * Acc@1 15.960 Acc@5 44.120
### epoch[1] execution time: 28.50664448738098
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.210 ( 0.210)	Data  0.142 ( 0.142)	Loss 3.2045e+00 (3.2045e+00)	Acc@1  20.31 ( 20.31)	Acc@5  50.78 ( 50.78)
Epoch: [2][ 10/391]	Time  0.063 ( 0.078)	Data  0.001 ( 0.014)	Loss 3.3181e+00 (3.3471e+00)	Acc@1  18.75 ( 16.62)	Acc@5  39.84 ( 44.46)
Epoch: [2][ 20/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.008)	Loss 3.5978e+00 (3.3478e+00)	Acc@1  14.84 ( 17.08)	Acc@5  39.84 ( 44.90)
Epoch: [2][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 3.3441e+00 (3.3682e+00)	Acc@1  17.97 ( 16.68)	Acc@5  41.41 ( 44.58)
Epoch: [2][ 40/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.2169e+00 (3.3677e+00)	Acc@1  22.66 ( 16.96)	Acc@5  45.31 ( 44.82)
Epoch: [2][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.4788e+00 (3.3650e+00)	Acc@1  17.97 ( 16.99)	Acc@5  40.62 ( 44.82)
Epoch: [2][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.3480e+00 (3.3518e+00)	Acc@1  11.72 ( 17.10)	Acc@5  41.41 ( 45.11)
Epoch: [2][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5422e+00 (3.3507e+00)	Acc@1  10.94 ( 17.04)	Acc@5  39.84 ( 45.03)
Epoch: [2][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5225e+00 (3.3466e+00)	Acc@1  11.72 ( 16.98)	Acc@5  41.41 ( 45.24)
Epoch: [2][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.2000e+00 (3.3416e+00)	Acc@1  21.09 ( 17.08)	Acc@5  53.12 ( 45.42)
Epoch: [2][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.3152e+00 (3.3387e+00)	Acc@1  23.44 ( 17.04)	Acc@5  50.78 ( 45.56)
Epoch: [2][110/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2862e+00 (3.3381e+00)	Acc@1  20.31 ( 16.96)	Acc@5  45.31 ( 45.39)
Epoch: [2][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3943e+00 (3.3368e+00)	Acc@1  18.75 ( 16.97)	Acc@5  46.09 ( 45.53)
Epoch: [2][130/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5894e+00 (3.3359e+00)	Acc@1  15.62 ( 17.13)	Acc@5  38.28 ( 45.62)
Epoch: [2][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2935e+00 (3.3373e+00)	Acc@1  16.41 ( 17.15)	Acc@5  46.09 ( 45.63)
Epoch: [2][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1277e+00 (3.3363e+00)	Acc@1  21.88 ( 17.14)	Acc@5  49.22 ( 45.66)
Epoch: [2][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3026e+00 (3.3352e+00)	Acc@1  14.06 ( 17.12)	Acc@5  42.97 ( 45.71)
Epoch: [2][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3734e+00 (3.3323e+00)	Acc@1  17.19 ( 17.21)	Acc@5  48.44 ( 45.79)
Epoch: [2][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4606e+00 (3.3271e+00)	Acc@1  15.62 ( 17.28)	Acc@5  39.84 ( 45.85)
Epoch: [2][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2312e+00 (3.3228e+00)	Acc@1  23.44 ( 17.40)	Acc@5  49.22 ( 45.94)
Epoch: [2][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0843e+00 (3.3166e+00)	Acc@1  20.31 ( 17.53)	Acc@5  56.25 ( 46.07)
Epoch: [2][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1937e+00 (3.3134e+00)	Acc@1  15.62 ( 17.54)	Acc@5  45.31 ( 46.23)
Epoch: [2][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0015e+00 (3.3109e+00)	Acc@1  20.31 ( 17.62)	Acc@5  52.34 ( 46.41)
Epoch: [2][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1394e+00 (3.3081e+00)	Acc@1  19.53 ( 17.65)	Acc@5  48.44 ( 46.53)
Epoch: [2][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2276e+00 (3.3020e+00)	Acc@1  16.41 ( 17.70)	Acc@5  46.09 ( 46.69)
Epoch: [2][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2100e+00 (3.2998e+00)	Acc@1  20.31 ( 17.71)	Acc@5  46.88 ( 46.78)
Epoch: [2][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3201e+00 (3.2964e+00)	Acc@1  16.41 ( 17.79)	Acc@5  47.66 ( 46.85)
Epoch: [2][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1724e+00 (3.2920e+00)	Acc@1  21.88 ( 17.90)	Acc@5  57.03 ( 47.00)
Epoch: [2][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0349e+00 (3.2860e+00)	Acc@1  25.00 ( 18.00)	Acc@5  61.72 ( 47.19)
Epoch: [2][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2227e+00 (3.2802e+00)	Acc@1  17.19 ( 18.16)	Acc@5  52.34 ( 47.36)
Epoch: [2][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1349e+00 (3.2738e+00)	Acc@1  18.75 ( 18.27)	Acc@5  51.56 ( 47.49)
Epoch: [2][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9856e+00 (3.2696e+00)	Acc@1  28.91 ( 18.38)	Acc@5  55.47 ( 47.60)
Epoch: [2][320/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0213e+00 (3.2650e+00)	Acc@1  19.53 ( 18.45)	Acc@5  56.25 ( 47.70)
Epoch: [2][330/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1472e+00 (3.2624e+00)	Acc@1  21.88 ( 18.51)	Acc@5  47.66 ( 47.80)
Epoch: [2][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0415e+00 (3.2573e+00)	Acc@1  22.66 ( 18.64)	Acc@5  55.47 ( 47.97)
Epoch: [2][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4373e+00 (3.2551e+00)	Acc@1  17.19 ( 18.69)	Acc@5  42.97 ( 47.99)
Epoch: [2][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0818e+00 (3.2509e+00)	Acc@1  21.09 ( 18.73)	Acc@5  53.91 ( 48.08)
Epoch: [2][370/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3427e+00 (3.2478e+00)	Acc@1  24.22 ( 18.76)	Acc@5  46.09 ( 48.18)
Epoch: [2][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2607e+00 (3.2442e+00)	Acc@1  16.41 ( 18.80)	Acc@5  47.66 ( 48.28)
Epoch: [2][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1117e+00 (3.2405e+00)	Acc@1  21.25 ( 18.84)	Acc@5  52.50 ( 48.38)
## e[2] optimizer.zero_grad (sum) time: 0.40789246559143066
## e[2]       loss.backward (sum) time: 6.937885522842407
## e[2]      optimizer.step (sum) time: 3.551741123199463
## epoch[2] training(only) time: 25.612296104431152
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 3.3271e+00 (3.3271e+00)	Acc@1  17.00 ( 17.00)	Acc@5  45.00 ( 45.00)
Test: [ 10/100]	Time  0.028 ( 0.040)	Loss 3.1980e+00 (3.2707e+00)	Acc@1  17.00 ( 18.64)	Acc@5  50.00 ( 47.91)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 3.1139e+00 (3.2199e+00)	Acc@1  15.00 ( 18.67)	Acc@5  52.00 ( 48.71)
Test: [ 30/100]	Time  0.025 ( 0.032)	Loss 3.3899e+00 (3.2222e+00)	Acc@1  23.00 ( 19.00)	Acc@5  48.00 ( 49.10)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 3.2280e+00 (3.2123e+00)	Acc@1  19.00 ( 19.15)	Acc@5  51.00 ( 49.32)
Test: [ 50/100]	Time  0.027 ( 0.030)	Loss 3.2136e+00 (3.2294e+00)	Acc@1  20.00 ( 19.43)	Acc@5  45.00 ( 49.14)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 3.1494e+00 (3.2268e+00)	Acc@1  27.00 ( 19.72)	Acc@5  54.00 ( 49.16)
Test: [ 70/100]	Time  0.027 ( 0.029)	Loss 3.4057e+00 (3.2338e+00)	Acc@1  16.00 ( 19.68)	Acc@5  48.00 ( 49.08)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 3.3684e+00 (3.2375e+00)	Acc@1  10.00 ( 19.30)	Acc@5  44.00 ( 48.96)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.9816e+00 (3.2278e+00)	Acc@1  27.00 ( 19.63)	Acc@5  55.00 ( 49.34)
 * Acc@1 19.450 Acc@5 49.200
### epoch[2] execution time: 28.467472314834595
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.221 ( 0.221)	Data  0.153 ( 0.153)	Loss 3.2051e+00 (3.2051e+00)	Acc@1  17.19 ( 17.19)	Acc@5  51.56 ( 51.56)
Epoch: [3][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.015)	Loss 2.9962e+00 (2.9978e+00)	Acc@1  25.00 ( 22.73)	Acc@5  54.69 ( 55.33)
Epoch: [3][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 3.0257e+00 (2.9967e+00)	Acc@1  23.44 ( 22.84)	Acc@5  54.69 ( 55.13)
Epoch: [3][ 30/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.1277e+00 (3.0124e+00)	Acc@1  17.97 ( 22.51)	Acc@5  54.69 ( 54.66)
Epoch: [3][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.7993e+00 (3.0214e+00)	Acc@1  25.00 ( 22.52)	Acc@5  61.72 ( 54.36)
Epoch: [3][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.0994e+00 (3.0169e+00)	Acc@1  25.00 ( 22.73)	Acc@5  53.91 ( 54.92)
Epoch: [3][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.8385e+00 (3.0070e+00)	Acc@1  20.31 ( 22.86)	Acc@5  64.06 ( 55.01)
Epoch: [3][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8518e+00 (3.0148e+00)	Acc@1  21.09 ( 22.94)	Acc@5  57.81 ( 54.67)
Epoch: [3][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8286e+00 (3.0104e+00)	Acc@1  27.34 ( 23.10)	Acc@5  60.16 ( 54.70)
Epoch: [3][ 90/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.1447e+00 (3.0146e+00)	Acc@1  26.56 ( 23.08)	Acc@5  55.47 ( 54.42)
Epoch: [3][100/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8587e+00 (3.0079e+00)	Acc@1  28.91 ( 23.31)	Acc@5  59.38 ( 54.64)
Epoch: [3][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.9694e+00 (3.0044e+00)	Acc@1  14.84 ( 23.23)	Acc@5  58.59 ( 54.92)
Epoch: [3][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9423e+00 (3.0034e+00)	Acc@1  21.88 ( 23.17)	Acc@5  57.03 ( 54.76)
Epoch: [3][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1750e+00 (3.0037e+00)	Acc@1  23.44 ( 23.16)	Acc@5  50.78 ( 54.74)
Epoch: [3][140/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9002e+00 (2.9996e+00)	Acc@1  24.22 ( 23.20)	Acc@5  55.47 ( 54.85)
Epoch: [3][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0268e+00 (2.9943e+00)	Acc@1  24.22 ( 23.25)	Acc@5  52.34 ( 54.99)
Epoch: [3][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1050e+00 (2.9943e+00)	Acc@1  21.09 ( 23.28)	Acc@5  53.91 ( 55.05)
Epoch: [3][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1003e+00 (2.9919e+00)	Acc@1  18.75 ( 23.38)	Acc@5  49.22 ( 55.06)
Epoch: [3][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0061e+00 (2.9953e+00)	Acc@1  22.66 ( 23.40)	Acc@5  53.91 ( 55.00)
Epoch: [3][190/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6062e+00 (2.9889e+00)	Acc@1  32.81 ( 23.54)	Acc@5  59.38 ( 55.16)
Epoch: [3][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7714e+00 (2.9880e+00)	Acc@1  29.69 ( 23.54)	Acc@5  59.38 ( 55.19)
Epoch: [3][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0480e+00 (2.9847e+00)	Acc@1  19.53 ( 23.53)	Acc@5  50.00 ( 55.26)
Epoch: [3][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8575e+00 (2.9813e+00)	Acc@1  18.75 ( 23.58)	Acc@5  62.50 ( 55.30)
Epoch: [3][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0865e+00 (2.9804e+00)	Acc@1  24.22 ( 23.69)	Acc@5  51.56 ( 55.34)
Epoch: [3][240/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9542e+00 (2.9805e+00)	Acc@1  21.88 ( 23.70)	Acc@5  55.47 ( 55.37)
Epoch: [3][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6448e+00 (2.9736e+00)	Acc@1  32.03 ( 23.84)	Acc@5  66.41 ( 55.51)
Epoch: [3][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8749e+00 (2.9666e+00)	Acc@1  27.34 ( 24.00)	Acc@5  60.16 ( 55.65)
Epoch: [3][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9155e+00 (2.9675e+00)	Acc@1  22.66 ( 24.02)	Acc@5  57.03 ( 55.61)
Epoch: [3][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0423e+00 (2.9648e+00)	Acc@1  20.31 ( 24.07)	Acc@5  53.12 ( 55.65)
Epoch: [3][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0489e+00 (2.9642e+00)	Acc@1  21.88 ( 24.09)	Acc@5  50.78 ( 55.63)
Epoch: [3][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5952e+00 (2.9590e+00)	Acc@1  28.91 ( 24.18)	Acc@5  63.28 ( 55.70)
Epoch: [3][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6789e+00 (2.9542e+00)	Acc@1  27.34 ( 24.25)	Acc@5  65.62 ( 55.83)
Epoch: [3][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8839e+00 (2.9525e+00)	Acc@1  31.25 ( 24.29)	Acc@5  62.50 ( 55.88)
Epoch: [3][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1285e+00 (2.9500e+00)	Acc@1  23.44 ( 24.36)	Acc@5  50.00 ( 55.94)
Epoch: [3][340/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7416e+00 (2.9482e+00)	Acc@1  24.22 ( 24.40)	Acc@5  60.16 ( 56.00)
Epoch: [3][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5916e+00 (2.9435e+00)	Acc@1  32.03 ( 24.53)	Acc@5  67.97 ( 56.14)
Epoch: [3][360/391]	Time  0.067 ( 0.065)	Data  0.002 ( 0.002)	Loss 2.6648e+00 (2.9379e+00)	Acc@1  27.34 ( 24.65)	Acc@5  54.69 ( 56.24)
Epoch: [3][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7391e+00 (2.9341e+00)	Acc@1  26.56 ( 24.73)	Acc@5  61.72 ( 56.34)
Epoch: [3][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9784e+00 (2.9286e+00)	Acc@1  24.22 ( 24.81)	Acc@5  57.81 ( 56.49)
Epoch: [3][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8893e+00 (2.9261e+00)	Acc@1  28.75 ( 24.84)	Acc@5  56.25 ( 56.57)
## e[3] optimizer.zero_grad (sum) time: 0.4098796844482422
## e[3]       loss.backward (sum) time: 6.986597776412964
## e[3]      optimizer.step (sum) time: 3.4467124938964844
## epoch[3] training(only) time: 25.604508638381958
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 2.8698e+00 (2.8698e+00)	Acc@1  27.00 ( 27.00)	Acc@5  55.00 ( 55.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 2.8020e+00 (2.8630e+00)	Acc@1  20.00 ( 24.91)	Acc@5  59.00 ( 59.36)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 2.7476e+00 (2.8364e+00)	Acc@1  30.00 ( 25.57)	Acc@5  57.00 ( 59.81)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 3.0012e+00 (2.8463e+00)	Acc@1  24.00 ( 25.94)	Acc@5  54.00 ( 59.81)
Test: [ 40/100]	Time  0.029 ( 0.030)	Loss 2.9197e+00 (2.8402e+00)	Acc@1  26.00 ( 26.17)	Acc@5  54.00 ( 59.46)
Test: [ 50/100]	Time  0.028 ( 0.030)	Loss 2.9959e+00 (2.8700e+00)	Acc@1  26.00 ( 25.94)	Acc@5  55.00 ( 58.71)
Test: [ 60/100]	Time  0.029 ( 0.029)	Loss 2.7640e+00 (2.8738e+00)	Acc@1  28.00 ( 26.03)	Acc@5  65.00 ( 58.74)
Test: [ 70/100]	Time  0.026 ( 0.029)	Loss 3.1036e+00 (2.8829e+00)	Acc@1  20.00 ( 25.97)	Acc@5  53.00 ( 58.44)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 3.0443e+00 (2.8877e+00)	Acc@1  21.00 ( 25.83)	Acc@5  54.00 ( 58.27)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.6788e+00 (2.8760e+00)	Acc@1  36.00 ( 26.01)	Acc@5  60.00 ( 58.54)
 * Acc@1 25.870 Acc@5 58.410
### epoch[3] execution time: 28.48761510848999
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.219 ( 0.219)	Data  0.152 ( 0.152)	Loss 2.5896e+00 (2.5896e+00)	Acc@1  33.59 ( 33.59)	Acc@5  63.28 ( 63.28)
Epoch: [4][ 10/391]	Time  0.064 ( 0.078)	Data  0.001 ( 0.015)	Loss 2.6093e+00 (2.7075e+00)	Acc@1  30.47 ( 28.55)	Acc@5  66.41 ( 61.93)
Epoch: [4][ 20/391]	Time  0.072 ( 0.072)	Data  0.001 ( 0.008)	Loss 2.7815e+00 (2.7385e+00)	Acc@1  26.56 ( 28.12)	Acc@5  59.38 ( 60.97)
Epoch: [4][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.8076e+00 (2.7585e+00)	Acc@1  27.34 ( 27.90)	Acc@5  53.12 ( 60.26)
Epoch: [4][ 40/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.7688e+00 (2.7577e+00)	Acc@1  31.25 ( 27.74)	Acc@5  60.16 ( 60.50)
Epoch: [4][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.5616e+00 (2.7492e+00)	Acc@1  32.03 ( 28.22)	Acc@5  65.62 ( 60.98)
Epoch: [4][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.7823e+00 (2.7463e+00)	Acc@1  29.69 ( 28.41)	Acc@5  63.28 ( 61.23)
Epoch: [4][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7576e+00 (2.7419e+00)	Acc@1  24.22 ( 28.22)	Acc@5  57.81 ( 61.26)
Epoch: [4][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7295e+00 (2.7367e+00)	Acc@1  30.47 ( 28.26)	Acc@5  61.72 ( 61.22)
Epoch: [4][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7622e+00 (2.7371e+00)	Acc@1  29.69 ( 28.23)	Acc@5  61.72 ( 61.16)
Epoch: [4][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7372e+00 (2.7387e+00)	Acc@1  28.91 ( 28.48)	Acc@5  64.06 ( 61.29)
Epoch: [4][110/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6451e+00 (2.7366e+00)	Acc@1  30.47 ( 28.44)	Acc@5  66.41 ( 61.39)
Epoch: [4][120/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8256e+00 (2.7354e+00)	Acc@1  29.69 ( 28.60)	Acc@5  60.94 ( 61.46)
Epoch: [4][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6800e+00 (2.7324e+00)	Acc@1  28.91 ( 28.80)	Acc@5  67.19 ( 61.67)
Epoch: [4][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6871e+00 (2.7341e+00)	Acc@1  33.59 ( 28.83)	Acc@5  60.94 ( 61.65)
Epoch: [4][150/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5388e+00 (2.7320e+00)	Acc@1  34.38 ( 28.81)	Acc@5  65.62 ( 61.65)
Epoch: [4][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7550e+00 (2.7332e+00)	Acc@1  28.91 ( 28.85)	Acc@5  60.16 ( 61.61)
Epoch: [4][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6131e+00 (2.7319e+00)	Acc@1  31.25 ( 28.88)	Acc@5  66.41 ( 61.67)
Epoch: [4][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7823e+00 (2.7308e+00)	Acc@1  30.47 ( 28.91)	Acc@5  60.94 ( 61.69)
Epoch: [4][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0174e+00 (2.7312e+00)	Acc@1  24.22 ( 28.93)	Acc@5  54.69 ( 61.75)
Epoch: [4][200/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4966e+00 (2.7300e+00)	Acc@1  32.81 ( 28.86)	Acc@5  70.31 ( 61.75)
Epoch: [4][210/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8513e+00 (2.7274e+00)	Acc@1  23.44 ( 28.86)	Acc@5  60.16 ( 61.83)
Epoch: [4][220/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7201e+00 (2.7264e+00)	Acc@1  26.56 ( 28.96)	Acc@5  62.50 ( 61.77)
Epoch: [4][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7192e+00 (2.7271e+00)	Acc@1  28.12 ( 28.96)	Acc@5  61.72 ( 61.78)
Epoch: [4][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7434e+00 (2.7263e+00)	Acc@1  25.00 ( 28.90)	Acc@5  62.50 ( 61.81)
Epoch: [4][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5445e+00 (2.7245e+00)	Acc@1  35.16 ( 28.96)	Acc@5  66.41 ( 61.83)
Epoch: [4][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5217e+00 (2.7202e+00)	Acc@1  36.72 ( 29.03)	Acc@5  64.84 ( 61.95)
Epoch: [4][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6258e+00 (2.7191e+00)	Acc@1  25.78 ( 29.05)	Acc@5  60.94 ( 61.98)
Epoch: [4][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5555e+00 (2.7149e+00)	Acc@1  31.25 ( 29.14)	Acc@5  66.41 ( 62.08)
Epoch: [4][290/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8218e+00 (2.7112e+00)	Acc@1  33.59 ( 29.29)	Acc@5  57.81 ( 62.19)
Epoch: [4][300/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5667e+00 (2.7110e+00)	Acc@1  30.47 ( 29.31)	Acc@5  64.06 ( 62.19)
Epoch: [4][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4730e+00 (2.7069e+00)	Acc@1  32.03 ( 29.36)	Acc@5  71.09 ( 62.28)
Epoch: [4][320/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6265e+00 (2.7062e+00)	Acc@1  29.69 ( 29.36)	Acc@5  66.41 ( 62.31)
Epoch: [4][330/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4877e+00 (2.7012e+00)	Acc@1  31.25 ( 29.46)	Acc@5  69.53 ( 62.46)
Epoch: [4][340/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7238e+00 (2.7006e+00)	Acc@1  27.34 ( 29.43)	Acc@5  60.94 ( 62.43)
Epoch: [4][350/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4873e+00 (2.6976e+00)	Acc@1  35.16 ( 29.48)	Acc@5  69.53 ( 62.53)
Epoch: [4][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6857e+00 (2.6925e+00)	Acc@1  25.00 ( 29.54)	Acc@5  60.16 ( 62.63)
Epoch: [4][370/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5433e+00 (2.6887e+00)	Acc@1  32.81 ( 29.63)	Acc@5  63.28 ( 62.71)
Epoch: [4][380/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7327e+00 (2.6873e+00)	Acc@1  25.78 ( 29.66)	Acc@5  60.94 ( 62.73)
Epoch: [4][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7002e+00 (2.6861e+00)	Acc@1  35.00 ( 29.70)	Acc@5  65.00 ( 62.77)
## e[4] optimizer.zero_grad (sum) time: 0.40976810455322266
## e[4]       loss.backward (sum) time: 6.981008052825928
## e[4]      optimizer.step (sum) time: 3.505985975265503
## epoch[4] training(only) time: 25.69271492958069
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 2.5893e+00 (2.5893e+00)	Acc@1  32.00 ( 32.00)	Acc@5  67.00 ( 67.00)
Test: [ 10/100]	Time  0.029 ( 0.039)	Loss 2.7198e+00 (2.6706e+00)	Acc@1  27.00 ( 30.64)	Acc@5  63.00 ( 64.09)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 2.1941e+00 (2.6146e+00)	Acc@1  41.00 ( 32.43)	Acc@5  73.00 ( 65.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.9395e+00 (2.6156e+00)	Acc@1  25.00 ( 32.32)	Acc@5  57.00 ( 65.06)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 2.6693e+00 (2.6161e+00)	Acc@1  38.00 ( 32.10)	Acc@5  63.00 ( 65.02)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 2.7470e+00 (2.6356e+00)	Acc@1  35.00 ( 31.98)	Acc@5  63.00 ( 64.65)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.8410e+00 (2.6418e+00)	Acc@1  29.00 ( 31.74)	Acc@5  57.00 ( 64.56)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 2.8164e+00 (2.6533e+00)	Acc@1  24.00 ( 31.35)	Acc@5  63.00 ( 64.35)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.8341e+00 (2.6600e+00)	Acc@1  27.00 ( 31.11)	Acc@5  57.00 ( 64.10)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.2919e+00 (2.6435e+00)	Acc@1  37.00 ( 31.37)	Acc@5  70.00 ( 64.35)
 * Acc@1 31.330 Acc@5 64.350
### epoch[4] execution time: 28.549938440322876
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.212 ( 0.212)	Data  0.143 ( 0.143)	Loss 2.5998e+00 (2.5998e+00)	Acc@1  34.38 ( 34.38)	Acc@5  64.06 ( 64.06)
Epoch: [5][ 10/391]	Time  0.074 ( 0.079)	Data  0.001 ( 0.014)	Loss 2.5115e+00 (2.5565e+00)	Acc@1  30.47 ( 32.53)	Acc@5  66.41 ( 67.05)
Epoch: [5][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 2.6685e+00 (2.5232e+00)	Acc@1  26.56 ( 33.41)	Acc@5  60.16 ( 67.19)
Epoch: [5][ 30/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.7153e+00 (2.5402e+00)	Acc@1  32.81 ( 32.89)	Acc@5  64.06 ( 66.78)
Epoch: [5][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.2385e+00 (2.5325e+00)	Acc@1  41.41 ( 33.10)	Acc@5  73.44 ( 67.11)
Epoch: [5][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.2549e+00 (2.5267e+00)	Acc@1  38.28 ( 32.77)	Acc@5  70.31 ( 67.10)
Epoch: [5][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.6776e+00 (2.5232e+00)	Acc@1  31.25 ( 32.76)	Acc@5  66.41 ( 67.14)
Epoch: [5][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6467e+00 (2.5279e+00)	Acc@1  28.12 ( 32.59)	Acc@5  60.94 ( 66.78)
Epoch: [5][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7401e+00 (2.5373e+00)	Acc@1  29.69 ( 32.50)	Acc@5  60.94 ( 66.47)
Epoch: [5][ 90/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.4825e+00 (2.5397e+00)	Acc@1  29.69 ( 32.41)	Acc@5  65.62 ( 66.42)
Epoch: [5][100/391]	Time  0.066 ( 0.067)	Data  0.002 ( 0.003)	Loss 2.6075e+00 (2.5425e+00)	Acc@1  31.25 ( 32.38)	Acc@5  64.84 ( 66.33)
Epoch: [5][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3901e+00 (2.5457e+00)	Acc@1  39.84 ( 32.36)	Acc@5  67.19 ( 66.25)
Epoch: [5][120/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2023e+00 (2.5438e+00)	Acc@1  40.62 ( 32.37)	Acc@5  73.44 ( 66.27)
Epoch: [5][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4752e+00 (2.5362e+00)	Acc@1  32.81 ( 32.56)	Acc@5  65.62 ( 66.54)
Epoch: [5][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4237e+00 (2.5371e+00)	Acc@1  37.50 ( 32.50)	Acc@5  65.62 ( 66.51)
Epoch: [5][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4036e+00 (2.5373e+00)	Acc@1  31.25 ( 32.41)	Acc@5  75.00 ( 66.54)
Epoch: [5][160/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3130e+00 (2.5357e+00)	Acc@1  39.06 ( 32.49)	Acc@5  71.09 ( 66.62)
Epoch: [5][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4014e+00 (2.5313e+00)	Acc@1  31.25 ( 32.53)	Acc@5  69.53 ( 66.65)
Epoch: [5][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3836e+00 (2.5271e+00)	Acc@1  35.94 ( 32.69)	Acc@5  71.09 ( 66.71)
Epoch: [5][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4594e+00 (2.5289e+00)	Acc@1  42.19 ( 32.69)	Acc@5  67.97 ( 66.56)
Epoch: [5][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6699e+00 (2.5292e+00)	Acc@1  33.59 ( 32.77)	Acc@5  57.81 ( 66.51)
Epoch: [5][210/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6227e+00 (2.5257e+00)	Acc@1  32.81 ( 32.84)	Acc@5  67.97 ( 66.63)
Epoch: [5][220/391]	Time  0.072 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3767e+00 (2.5259e+00)	Acc@1  32.81 ( 32.84)	Acc@5  74.22 ( 66.62)
Epoch: [5][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5412e+00 (2.5222e+00)	Acc@1  33.59 ( 32.91)	Acc@5  69.53 ( 66.77)
Epoch: [5][240/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3974e+00 (2.5219e+00)	Acc@1  32.03 ( 32.88)	Acc@5  67.97 ( 66.81)
Epoch: [5][250/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3856e+00 (2.5218e+00)	Acc@1  32.03 ( 32.92)	Acc@5  68.75 ( 66.77)
Epoch: [5][260/391]	Time  0.065 ( 0.065)	Data  0.002 ( 0.002)	Loss 2.4353e+00 (2.5225e+00)	Acc@1  36.72 ( 32.93)	Acc@5  67.19 ( 66.74)
Epoch: [5][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4935e+00 (2.5212e+00)	Acc@1  33.59 ( 33.00)	Acc@5  65.62 ( 66.76)
Epoch: [5][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5209e+00 (2.5209e+00)	Acc@1  33.59 ( 33.02)	Acc@5  68.75 ( 66.77)
Epoch: [5][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6514e+00 (2.5221e+00)	Acc@1  28.91 ( 33.00)	Acc@5  60.16 ( 66.72)
Epoch: [5][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2661e+00 (2.5199e+00)	Acc@1  40.62 ( 33.05)	Acc@5  71.09 ( 66.84)
Epoch: [5][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4631e+00 (2.5180e+00)	Acc@1  35.94 ( 33.10)	Acc@5  63.28 ( 66.84)
Epoch: [5][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4779e+00 (2.5151e+00)	Acc@1  36.72 ( 33.13)	Acc@5  67.97 ( 66.91)
Epoch: [5][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4638e+00 (2.5134e+00)	Acc@1  34.38 ( 33.21)	Acc@5  69.53 ( 66.96)
Epoch: [5][340/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3772e+00 (2.5110e+00)	Acc@1  40.62 ( 33.28)	Acc@5  66.41 ( 66.97)
Epoch: [5][350/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5839e+00 (2.5117e+00)	Acc@1  29.69 ( 33.24)	Acc@5  64.84 ( 66.97)
Epoch: [5][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3936e+00 (2.5114e+00)	Acc@1  43.75 ( 33.26)	Acc@5  70.31 ( 66.97)
Epoch: [5][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5345e+00 (2.5103e+00)	Acc@1  28.91 ( 33.28)	Acc@5  64.84 ( 66.96)
Epoch: [5][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4328e+00 (2.5099e+00)	Acc@1  38.28 ( 33.33)	Acc@5  70.31 ( 66.96)
Epoch: [5][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.001)	Loss 2.2403e+00 (2.5067e+00)	Acc@1  38.75 ( 33.44)	Acc@5  76.25 ( 67.05)
## e[5] optimizer.zero_grad (sum) time: 0.41262149810791016
## e[5]       loss.backward (sum) time: 6.969502210617065
## e[5]      optimizer.step (sum) time: 3.402214765548706
## epoch[5] training(only) time: 25.565686225891113
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 2.4964e+00 (2.4964e+00)	Acc@1  33.00 ( 33.00)	Acc@5  69.00 ( 69.00)
Test: [ 10/100]	Time  0.029 ( 0.039)	Loss 2.6485e+00 (2.5716e+00)	Acc@1  29.00 ( 32.64)	Acc@5  67.00 ( 66.36)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 2.3314e+00 (2.5926e+00)	Acc@1  38.00 ( 32.81)	Acc@5  73.00 ( 66.14)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 2.6425e+00 (2.5704e+00)	Acc@1  36.00 ( 33.06)	Acc@5  61.00 ( 66.29)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.5421e+00 (2.5472e+00)	Acc@1  37.00 ( 33.20)	Acc@5  65.00 ( 66.63)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 2.4382e+00 (2.5541e+00)	Acc@1  30.00 ( 33.10)	Acc@5  66.00 ( 66.27)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 2.5804e+00 (2.5568e+00)	Acc@1  26.00 ( 33.03)	Acc@5  67.00 ( 66.11)
Test: [ 70/100]	Time  0.027 ( 0.029)	Loss 2.6962e+00 (2.5644e+00)	Acc@1  29.00 ( 32.82)	Acc@5  64.00 ( 66.08)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.9896e+00 (2.5699e+00)	Acc@1  27.00 ( 32.57)	Acc@5  55.00 ( 65.93)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.3814e+00 (2.5591e+00)	Acc@1  38.00 ( 32.77)	Acc@5  67.00 ( 66.20)
 * Acc@1 32.580 Acc@5 66.050
### epoch[5] execution time: 28.444505214691162
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.215 ( 0.215)	Data  0.148 ( 0.148)	Loss 2.5751e+00 (2.5751e+00)	Acc@1  30.47 ( 30.47)	Acc@5  63.28 ( 63.28)
Epoch: [6][ 10/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.015)	Loss 2.6487e+00 (2.4219e+00)	Acc@1  29.69 ( 34.87)	Acc@5  62.50 ( 69.25)
Epoch: [6][ 20/391]	Time  0.062 ( 0.072)	Data  0.001 ( 0.008)	Loss 2.5005e+00 (2.4407e+00)	Acc@1  36.72 ( 34.82)	Acc@5  61.72 ( 68.34)
Epoch: [6][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.2720e+00 (2.3790e+00)	Acc@1  43.75 ( 36.01)	Acc@5  62.50 ( 69.46)
Epoch: [6][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.1537e+00 (2.3671e+00)	Acc@1  42.97 ( 36.24)	Acc@5  75.00 ( 70.01)
Epoch: [6][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.3214e+00 (2.3700e+00)	Acc@1  37.50 ( 36.35)	Acc@5  69.53 ( 70.01)
Epoch: [6][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.2403e+00 (2.3653e+00)	Acc@1  35.94 ( 36.23)	Acc@5  78.12 ( 70.30)
Epoch: [6][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.4811e+00 (2.3688e+00)	Acc@1  35.16 ( 36.00)	Acc@5  65.62 ( 70.24)
Epoch: [6][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.2090e+00 (2.3745e+00)	Acc@1  42.19 ( 36.01)	Acc@5  75.00 ( 70.08)
Epoch: [6][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.2315e+00 (2.3766e+00)	Acc@1  38.28 ( 35.77)	Acc@5  68.75 ( 70.10)
Epoch: [6][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.4641e+00 (2.3793e+00)	Acc@1  35.94 ( 35.80)	Acc@5  66.41 ( 69.93)
Epoch: [6][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2087e+00 (2.3752e+00)	Acc@1  35.16 ( 35.86)	Acc@5  75.00 ( 69.97)
Epoch: [6][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1317e+00 (2.3708e+00)	Acc@1  40.62 ( 35.98)	Acc@5  75.78 ( 70.09)
Epoch: [6][130/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1761e+00 (2.3735e+00)	Acc@1  39.06 ( 35.98)	Acc@5  74.22 ( 69.92)
Epoch: [6][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3283e+00 (2.3755e+00)	Acc@1  36.72 ( 35.89)	Acc@5  70.31 ( 69.91)
Epoch: [6][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2970e+00 (2.3773e+00)	Acc@1  35.94 ( 35.80)	Acc@5  77.34 ( 69.93)
Epoch: [6][160/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1409e+00 (2.3789e+00)	Acc@1  45.31 ( 35.75)	Acc@5  75.78 ( 69.92)
Epoch: [6][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1408e+00 (2.3727e+00)	Acc@1  42.19 ( 35.89)	Acc@5  73.44 ( 70.07)
Epoch: [6][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3439e+00 (2.3750e+00)	Acc@1  35.16 ( 35.88)	Acc@5  72.66 ( 69.99)
Epoch: [6][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1994e+00 (2.3724e+00)	Acc@1  39.84 ( 35.87)	Acc@5  72.66 ( 70.08)
Epoch: [6][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6398e+00 (2.3742e+00)	Acc@1  31.25 ( 35.78)	Acc@5  67.19 ( 70.07)
Epoch: [6][210/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2569e+00 (2.3699e+00)	Acc@1  35.16 ( 35.86)	Acc@5  73.44 ( 70.13)
Epoch: [6][220/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2361e+00 (2.3691e+00)	Acc@1  42.19 ( 35.89)	Acc@5  73.44 ( 70.10)
Epoch: [6][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3788e+00 (2.3692e+00)	Acc@1  35.94 ( 35.91)	Acc@5  69.53 ( 70.10)
Epoch: [6][240/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4171e+00 (2.3692e+00)	Acc@1  32.81 ( 35.91)	Acc@5  67.19 ( 70.05)
Epoch: [6][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3806e+00 (2.3695e+00)	Acc@1  39.06 ( 35.93)	Acc@5  71.09 ( 70.04)
Epoch: [6][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6271e+00 (2.3696e+00)	Acc@1  34.38 ( 35.90)	Acc@5  66.41 ( 70.08)
Epoch: [6][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3677e+00 (2.3665e+00)	Acc@1  36.72 ( 35.93)	Acc@5  71.09 ( 70.13)
Epoch: [6][280/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3200e+00 (2.3644e+00)	Acc@1  35.94 ( 35.95)	Acc@5  66.41 ( 70.18)
Epoch: [6][290/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1180e+00 (2.3621e+00)	Acc@1  39.06 ( 35.99)	Acc@5  78.91 ( 70.25)
Epoch: [6][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4026e+00 (2.3609e+00)	Acc@1  35.16 ( 36.00)	Acc@5  67.97 ( 70.26)
Epoch: [6][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5257e+00 (2.3592e+00)	Acc@1  35.94 ( 36.06)	Acc@5  71.09 ( 70.28)
Epoch: [6][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3815e+00 (2.3594e+00)	Acc@1  35.94 ( 36.09)	Acc@5  70.31 ( 70.26)
Epoch: [6][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3488e+00 (2.3591e+00)	Acc@1  37.50 ( 36.11)	Acc@5  73.44 ( 70.27)
Epoch: [6][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4147e+00 (2.3579e+00)	Acc@1  30.47 ( 36.13)	Acc@5  71.09 ( 70.28)
Epoch: [6][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3412e+00 (2.3560e+00)	Acc@1  38.28 ( 36.22)	Acc@5  70.31 ( 70.31)
Epoch: [6][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5217e+00 (2.3546e+00)	Acc@1  28.12 ( 36.23)	Acc@5  67.19 ( 70.36)
Epoch: [6][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1792e+00 (2.3545e+00)	Acc@1  35.16 ( 36.25)	Acc@5  72.66 ( 70.34)
Epoch: [6][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3412e+00 (2.3540e+00)	Acc@1  36.72 ( 36.28)	Acc@5  70.31 ( 70.32)
Epoch: [6][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4574e+00 (2.3533e+00)	Acc@1  33.75 ( 36.30)	Acc@5  67.50 ( 70.31)
## e[6] optimizer.zero_grad (sum) time: 0.41477108001708984
## e[6]       loss.backward (sum) time: 6.990387439727783
## e[6]      optimizer.step (sum) time: 3.4149117469787598
## epoch[6] training(only) time: 25.569622039794922
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 2.2693e+00 (2.2693e+00)	Acc@1  44.00 ( 44.00)	Acc@5  68.00 ( 68.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 2.4840e+00 (2.3892e+00)	Acc@1  31.00 ( 36.91)	Acc@5  71.00 ( 70.18)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 2.0518e+00 (2.3482e+00)	Acc@1  41.00 ( 37.43)	Acc@5  78.00 ( 70.90)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.5780e+00 (2.3503e+00)	Acc@1  35.00 ( 36.84)	Acc@5  71.00 ( 70.94)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.4762e+00 (2.3525e+00)	Acc@1  34.00 ( 37.00)	Acc@5  70.00 ( 70.93)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 2.2524e+00 (2.3448e+00)	Acc@1  40.00 ( 37.37)	Acc@5  65.00 ( 70.57)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.2850e+00 (2.3380e+00)	Acc@1  31.00 ( 37.34)	Acc@5  71.00 ( 70.66)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 2.5698e+00 (2.3477e+00)	Acc@1  35.00 ( 37.15)	Acc@5  60.00 ( 70.55)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.5803e+00 (2.3554e+00)	Acc@1  30.00 ( 36.96)	Acc@5  64.00 ( 70.41)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.3723e+00 (2.3521e+00)	Acc@1  36.00 ( 37.09)	Acc@5  70.00 ( 70.55)
 * Acc@1 37.030 Acc@5 70.600
### epoch[6] execution time: 28.391742706298828
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.219 ( 0.219)	Data  0.150 ( 0.150)	Loss 2.3071e+00 (2.3071e+00)	Acc@1  35.94 ( 35.94)	Acc@5  71.88 ( 71.88)
Epoch: [7][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.015)	Loss 2.4623e+00 (2.3107e+00)	Acc@1  36.72 ( 37.86)	Acc@5  64.84 ( 70.74)
Epoch: [7][ 20/391]	Time  0.069 ( 0.073)	Data  0.001 ( 0.008)	Loss 2.3096e+00 (2.2834e+00)	Acc@1  39.06 ( 38.21)	Acc@5  71.09 ( 71.84)
Epoch: [7][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.5791e+00 (2.2867e+00)	Acc@1  34.38 ( 38.03)	Acc@5  65.62 ( 72.03)
Epoch: [7][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.2419e+00 (2.2708e+00)	Acc@1  37.50 ( 38.22)	Acc@5  67.19 ( 71.93)
Epoch: [7][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.1414e+00 (2.2798e+00)	Acc@1  41.41 ( 38.19)	Acc@5  73.44 ( 71.78)
Epoch: [7][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.1643e+00 (2.2608e+00)	Acc@1  37.50 ( 38.47)	Acc@5  77.34 ( 72.49)
Epoch: [7][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3678e+00 (2.2685e+00)	Acc@1  35.16 ( 38.16)	Acc@5  71.88 ( 72.49)
Epoch: [7][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3462e+00 (2.2647e+00)	Acc@1  35.16 ( 38.19)	Acc@5  71.09 ( 72.57)
Epoch: [7][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3424e+00 (2.2580e+00)	Acc@1  36.72 ( 38.59)	Acc@5  69.53 ( 72.62)
Epoch: [7][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.5459e+00 (2.2593e+00)	Acc@1  37.50 ( 38.58)	Acc@5  66.41 ( 72.54)
Epoch: [7][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2089e+00 (2.2503e+00)	Acc@1  44.53 ( 38.89)	Acc@5  75.00 ( 72.68)
Epoch: [7][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1944e+00 (2.2512e+00)	Acc@1  42.97 ( 38.95)	Acc@5  71.88 ( 72.67)
Epoch: [7][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3346e+00 (2.2451e+00)	Acc@1  35.16 ( 39.05)	Acc@5  70.31 ( 72.76)
Epoch: [7][140/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1393e+00 (2.2411e+00)	Acc@1  37.50 ( 39.00)	Acc@5  72.66 ( 72.90)
Epoch: [7][150/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3070e+00 (2.2447e+00)	Acc@1  38.28 ( 38.98)	Acc@5  71.09 ( 72.82)
Epoch: [7][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3335e+00 (2.2490e+00)	Acc@1  39.84 ( 38.88)	Acc@5  72.66 ( 72.72)
Epoch: [7][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9923e+00 (2.2484e+00)	Acc@1  50.00 ( 38.87)	Acc@5  78.91 ( 72.75)
Epoch: [7][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3748e+00 (2.2467e+00)	Acc@1  40.62 ( 38.98)	Acc@5  69.53 ( 72.75)
Epoch: [7][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9867e+00 (2.2440e+00)	Acc@1  47.66 ( 39.00)	Acc@5  78.12 ( 72.85)
Epoch: [7][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1180e+00 (2.2391e+00)	Acc@1  42.19 ( 39.07)	Acc@5  68.75 ( 72.92)
Epoch: [7][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3613e+00 (2.2418e+00)	Acc@1  32.81 ( 38.95)	Acc@5  68.75 ( 72.89)
Epoch: [7][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0303e+00 (2.2423e+00)	Acc@1  42.97 ( 38.97)	Acc@5  75.78 ( 72.84)
Epoch: [7][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2992e+00 (2.2411e+00)	Acc@1  42.19 ( 39.07)	Acc@5  71.88 ( 72.88)
Epoch: [7][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1261e+00 (2.2411e+00)	Acc@1  39.06 ( 39.07)	Acc@5  74.22 ( 72.87)
Epoch: [7][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1813e+00 (2.2415e+00)	Acc@1  36.72 ( 39.08)	Acc@5  75.78 ( 72.86)
Epoch: [7][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4482e+00 (2.2407e+00)	Acc@1  35.16 ( 39.16)	Acc@5  67.19 ( 72.84)
Epoch: [7][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2479e+00 (2.2396e+00)	Acc@1  39.84 ( 39.23)	Acc@5  74.22 ( 72.83)
Epoch: [7][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1942e+00 (2.2382e+00)	Acc@1  39.06 ( 39.26)	Acc@5  71.88 ( 72.90)
Epoch: [7][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2596e+00 (2.2391e+00)	Acc@1  37.50 ( 39.25)	Acc@5  72.66 ( 72.86)
Epoch: [7][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1591e+00 (2.2355e+00)	Acc@1  41.41 ( 39.34)	Acc@5  71.09 ( 72.93)
Epoch: [7][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2662e+00 (2.2313e+00)	Acc@1  42.19 ( 39.42)	Acc@5  73.44 ( 73.03)
Epoch: [7][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0972e+00 (2.2273e+00)	Acc@1  45.31 ( 39.46)	Acc@5  78.12 ( 73.12)
Epoch: [7][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2847e+00 (2.2258e+00)	Acc@1  34.38 ( 39.45)	Acc@5  71.88 ( 73.18)
Epoch: [7][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1598e+00 (2.2264e+00)	Acc@1  40.62 ( 39.45)	Acc@5  74.22 ( 73.23)
Epoch: [7][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2303e+00 (2.2258e+00)	Acc@1  34.38 ( 39.45)	Acc@5  70.31 ( 73.23)
Epoch: [7][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2310e+00 (2.2277e+00)	Acc@1  45.31 ( 39.48)	Acc@5  78.12 ( 73.19)
Epoch: [7][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4079e+00 (2.2280e+00)	Acc@1  33.59 ( 39.47)	Acc@5  67.97 ( 73.18)
Epoch: [7][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9489e+00 (2.2255e+00)	Acc@1  46.88 ( 39.52)	Acc@5  78.91 ( 73.24)
Epoch: [7][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8893e+00 (2.2245e+00)	Acc@1  51.25 ( 39.55)	Acc@5  81.25 ( 73.29)
## e[7] optimizer.zero_grad (sum) time: 0.4103720188140869
## e[7]       loss.backward (sum) time: 6.982037544250488
## e[7]      optimizer.step (sum) time: 3.471674919128418
## epoch[7] training(only) time: 25.573577165603638
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 2.3295e+00 (2.3295e+00)	Acc@1  42.00 ( 42.00)	Acc@5  69.00 ( 69.00)
Test: [ 10/100]	Time  0.028 ( 0.040)	Loss 2.2387e+00 (2.3403e+00)	Acc@1  42.00 ( 39.64)	Acc@5  73.00 ( 71.36)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 2.0957e+00 (2.3169e+00)	Acc@1  42.00 ( 39.24)	Acc@5  78.00 ( 72.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.4619e+00 (2.2920e+00)	Acc@1  33.00 ( 39.39)	Acc@5  69.00 ( 72.26)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 2.2619e+00 (2.2716e+00)	Acc@1  37.00 ( 39.20)	Acc@5  75.00 ( 72.98)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 2.2828e+00 (2.2845e+00)	Acc@1  35.00 ( 39.04)	Acc@5  71.00 ( 72.37)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.1142e+00 (2.2786e+00)	Acc@1  42.00 ( 38.93)	Acc@5  74.00 ( 72.48)
Test: [ 70/100]	Time  0.028 ( 0.029)	Loss 2.4008e+00 (2.2825e+00)	Acc@1  33.00 ( 38.83)	Acc@5  73.00 ( 72.49)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.6124e+00 (2.2946e+00)	Acc@1  34.00 ( 38.49)	Acc@5  63.00 ( 72.17)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.3434e+00 (2.2854e+00)	Acc@1  39.00 ( 38.84)	Acc@5  73.00 ( 72.42)
 * Acc@1 38.810 Acc@5 72.500
### epoch[7] execution time: 28.428352117538452
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.215 ( 0.215)	Data  0.147 ( 0.147)	Loss 2.2579e+00 (2.2579e+00)	Acc@1  45.31 ( 45.31)	Acc@5  69.53 ( 69.53)
Epoch: [8][ 10/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.014)	Loss 2.0971e+00 (2.1345e+00)	Acc@1  41.41 ( 42.26)	Acc@5  72.66 ( 74.22)
Epoch: [8][ 20/391]	Time  0.067 ( 0.072)	Data  0.001 ( 0.008)	Loss 2.1926e+00 (2.1188e+00)	Acc@1  33.59 ( 41.44)	Acc@5  75.00 ( 75.00)
Epoch: [8][ 30/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.9926e+00 (2.1215e+00)	Acc@1  41.41 ( 41.36)	Acc@5  75.00 ( 75.30)
Epoch: [8][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.1010e+00 (2.1237e+00)	Acc@1  46.09 ( 41.52)	Acc@5  73.44 ( 74.94)
Epoch: [8][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.0166e+00 (2.1276e+00)	Acc@1  43.75 ( 41.67)	Acc@5  76.56 ( 74.94)
Epoch: [8][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.3405e+00 (2.1336e+00)	Acc@1  33.59 ( 41.06)	Acc@5  71.09 ( 74.92)
Epoch: [8][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.1703e+00 (2.1273e+00)	Acc@1  39.06 ( 41.08)	Acc@5  72.66 ( 74.97)
Epoch: [8][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.2182e+00 (2.1327e+00)	Acc@1  39.84 ( 40.96)	Acc@5  71.09 ( 74.86)
Epoch: [8][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3185e+00 (2.1407e+00)	Acc@1  37.50 ( 40.74)	Acc@5  75.00 ( 74.75)
Epoch: [8][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9346e+00 (2.1391e+00)	Acc@1  43.75 ( 40.66)	Acc@5  81.25 ( 74.95)
Epoch: [8][110/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.4240e+00 (2.1373e+00)	Acc@1  35.16 ( 40.86)	Acc@5  65.62 ( 74.86)
Epoch: [8][120/391]	Time  0.076 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0578e+00 (2.1270e+00)	Acc@1  48.44 ( 41.30)	Acc@5  75.00 ( 75.10)
Epoch: [8][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9826e+00 (2.1261e+00)	Acc@1  39.06 ( 41.30)	Acc@5  75.00 ( 75.12)
Epoch: [8][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2335e+00 (2.1244e+00)	Acc@1  37.50 ( 41.27)	Acc@5  74.22 ( 75.16)
Epoch: [8][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3675e+00 (2.1288e+00)	Acc@1  35.16 ( 41.30)	Acc@5  75.78 ( 75.10)
Epoch: [8][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2401e+00 (2.1276e+00)	Acc@1  38.28 ( 41.28)	Acc@5  73.44 ( 75.15)
Epoch: [8][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0077e+00 (2.1205e+00)	Acc@1  48.44 ( 41.53)	Acc@5  75.78 ( 75.31)
Epoch: [8][180/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9303e+00 (2.1197e+00)	Acc@1  46.09 ( 41.57)	Acc@5  74.22 ( 75.30)
Epoch: [8][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1685e+00 (2.1182e+00)	Acc@1  43.75 ( 41.63)	Acc@5  75.78 ( 75.36)
Epoch: [8][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2280e+00 (2.1235e+00)	Acc@1  35.16 ( 41.48)	Acc@5  73.44 ( 75.30)
Epoch: [8][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1735e+00 (2.1232e+00)	Acc@1  38.28 ( 41.40)	Acc@5  75.78 ( 75.29)
Epoch: [8][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4455e+00 (2.1257e+00)	Acc@1  31.25 ( 41.34)	Acc@5  73.44 ( 75.27)
Epoch: [8][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0743e+00 (2.1233e+00)	Acc@1  45.31 ( 41.48)	Acc@5  73.44 ( 75.28)
Epoch: [8][240/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9023e+00 (2.1208e+00)	Acc@1  48.44 ( 41.54)	Acc@5  81.25 ( 75.32)
Epoch: [8][250/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9128e+00 (2.1185e+00)	Acc@1  49.22 ( 41.65)	Acc@5  80.47 ( 75.39)
Epoch: [8][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8847e+00 (2.1149e+00)	Acc@1  44.53 ( 41.72)	Acc@5  83.59 ( 75.45)
Epoch: [8][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2504e+00 (2.1158e+00)	Acc@1  39.84 ( 41.78)	Acc@5  76.56 ( 75.40)
Epoch: [8][280/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0985e+00 (2.1169e+00)	Acc@1  39.06 ( 41.76)	Acc@5  82.03 ( 75.44)
Epoch: [8][290/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0344e+00 (2.1187e+00)	Acc@1  42.97 ( 41.77)	Acc@5  77.34 ( 75.42)
Epoch: [8][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8932e+00 (2.1183e+00)	Acc@1  49.22 ( 41.82)	Acc@5  77.34 ( 75.42)
Epoch: [8][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0588e+00 (2.1175e+00)	Acc@1  44.53 ( 41.82)	Acc@5  75.78 ( 75.46)
Epoch: [8][320/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1094e+00 (2.1188e+00)	Acc@1  39.06 ( 41.80)	Acc@5  78.12 ( 75.41)
Epoch: [8][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9665e+00 (2.1187e+00)	Acc@1  46.88 ( 41.87)	Acc@5  77.34 ( 75.41)
Epoch: [8][340/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1531e+00 (2.1201e+00)	Acc@1  38.28 ( 41.82)	Acc@5  75.78 ( 75.42)
Epoch: [8][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2745e+00 (2.1194e+00)	Acc@1  35.94 ( 41.83)	Acc@5  75.78 ( 75.45)
Epoch: [8][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1407e+00 (2.1174e+00)	Acc@1  35.94 ( 41.83)	Acc@5  75.78 ( 75.50)
Epoch: [8][370/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1177e+00 (2.1150e+00)	Acc@1  43.75 ( 41.93)	Acc@5  71.88 ( 75.52)
Epoch: [8][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9846e+00 (2.1144e+00)	Acc@1  43.75 ( 41.95)	Acc@5  75.78 ( 75.53)
Epoch: [8][390/391]	Time  0.054 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1194e+00 (2.1136e+00)	Acc@1  32.50 ( 41.95)	Acc@5  76.25 ( 75.51)
## e[8] optimizer.zero_grad (sum) time: 0.406862735748291
## e[8]       loss.backward (sum) time: 6.977033853530884
## e[8]      optimizer.step (sum) time: 3.5133402347564697
## epoch[8] training(only) time: 25.621336936950684
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 2.2276e+00 (2.2276e+00)	Acc@1  43.00 ( 43.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 2.3693e+00 (2.2110e+00)	Acc@1  31.00 ( 40.45)	Acc@5  71.00 ( 74.82)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 2.0170e+00 (2.1850e+00)	Acc@1  42.00 ( 40.43)	Acc@5  77.00 ( 74.57)
Test: [ 30/100]	Time  0.033 ( 0.031)	Loss 2.3886e+00 (2.1934e+00)	Acc@1  32.00 ( 40.23)	Acc@5  73.00 ( 74.13)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 2.1047e+00 (2.1912e+00)	Acc@1  37.00 ( 40.24)	Acc@5  80.00 ( 74.10)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 2.0746e+00 (2.1958e+00)	Acc@1  40.00 ( 40.47)	Acc@5  72.00 ( 73.51)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.2709e+00 (2.1814e+00)	Acc@1  37.00 ( 40.79)	Acc@5  76.00 ( 74.15)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 2.4525e+00 (2.1840e+00)	Acc@1  37.00 ( 40.80)	Acc@5  72.00 ( 74.03)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 2.2502e+00 (2.1976e+00)	Acc@1  40.00 ( 40.63)	Acc@5  67.00 ( 73.69)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.4368e+00 (2.1919e+00)	Acc@1  37.00 ( 40.87)	Acc@5  67.00 ( 73.79)
 * Acc@1 41.010 Acc@5 73.840
### epoch[8] execution time: 28.475260257720947
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.214 ( 0.214)	Data  0.144 ( 0.144)	Loss 2.2122e+00 (2.2122e+00)	Acc@1  45.31 ( 45.31)	Acc@5  74.22 ( 74.22)
Epoch: [9][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.014)	Loss 1.7553e+00 (2.0246e+00)	Acc@1  52.34 ( 45.81)	Acc@5  81.25 ( 77.98)
Epoch: [9][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.008)	Loss 1.8733e+00 (2.0193e+00)	Acc@1  47.66 ( 44.98)	Acc@5  82.03 ( 78.12)
Epoch: [9][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 2.1298e+00 (2.0084e+00)	Acc@1  45.31 ( 45.09)	Acc@5  75.78 ( 78.00)
Epoch: [9][ 40/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.0436e+00 (2.0030e+00)	Acc@1  43.75 ( 44.93)	Acc@5  78.91 ( 77.92)
Epoch: [9][ 50/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.9390e+00 (2.0173e+00)	Acc@1  44.53 ( 44.55)	Acc@5  82.03 ( 77.77)
Epoch: [9][ 60/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8971e+00 (2.0173e+00)	Acc@1  47.66 ( 44.35)	Acc@5  81.25 ( 77.57)
Epoch: [9][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.2549e+00 (2.0271e+00)	Acc@1  42.97 ( 44.27)	Acc@5  68.75 ( 77.33)
Epoch: [9][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.9216e+00 (2.0221e+00)	Acc@1  45.31 ( 44.47)	Acc@5  78.91 ( 77.29)
Epoch: [9][ 90/391]	Time  0.066 ( 0.066)	Data  0.002 ( 0.003)	Loss 2.0719e+00 (2.0233e+00)	Acc@1  39.84 ( 44.54)	Acc@5  74.22 ( 77.18)
Epoch: [9][100/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.8445e+00 (2.0203e+00)	Acc@1  52.34 ( 44.70)	Acc@5  80.47 ( 77.28)
Epoch: [9][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0343e+00 (2.0203e+00)	Acc@1  47.66 ( 44.66)	Acc@5  75.78 ( 77.30)
Epoch: [9][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9517e+00 (2.0251e+00)	Acc@1  42.19 ( 44.55)	Acc@5  74.22 ( 77.13)
Epoch: [9][130/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1041e+00 (2.0267e+00)	Acc@1  40.62 ( 44.36)	Acc@5  75.00 ( 77.14)
Epoch: [9][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1192e+00 (2.0244e+00)	Acc@1  48.44 ( 44.52)	Acc@5  75.00 ( 77.24)
Epoch: [9][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0279e+00 (2.0251e+00)	Acc@1  44.53 ( 44.47)	Acc@5  78.91 ( 77.25)
Epoch: [9][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8994e+00 (2.0248e+00)	Acc@1  46.88 ( 44.43)	Acc@5  81.25 ( 77.25)
Epoch: [9][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9127e+00 (2.0219e+00)	Acc@1  46.88 ( 44.40)	Acc@5  79.69 ( 77.25)
Epoch: [9][180/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2857e+00 (2.0187e+00)	Acc@1  39.84 ( 44.50)	Acc@5  73.44 ( 77.33)
Epoch: [9][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9905e+00 (2.0227e+00)	Acc@1  47.66 ( 44.42)	Acc@5  78.12 ( 77.30)
Epoch: [9][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2062e+00 (2.0247e+00)	Acc@1  42.97 ( 44.39)	Acc@5  71.88 ( 77.25)
Epoch: [9][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1101e+00 (2.0267e+00)	Acc@1  39.06 ( 44.29)	Acc@5  78.91 ( 77.20)
Epoch: [9][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9144e+00 (2.0269e+00)	Acc@1  40.62 ( 44.26)	Acc@5  82.03 ( 77.20)
Epoch: [9][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7097e+00 (2.0228e+00)	Acc@1  50.78 ( 44.34)	Acc@5  82.03 ( 77.28)
Epoch: [9][240/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0368e+00 (2.0224e+00)	Acc@1  42.19 ( 44.39)	Acc@5  75.78 ( 77.28)
Epoch: [9][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0057e+00 (2.0222e+00)	Acc@1  48.44 ( 44.38)	Acc@5  72.66 ( 77.27)
Epoch: [9][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9960e+00 (2.0208e+00)	Acc@1  42.19 ( 44.45)	Acc@5  76.56 ( 77.26)
Epoch: [9][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9951e+00 (2.0185e+00)	Acc@1  40.62 ( 44.42)	Acc@5  75.00 ( 77.32)
Epoch: [9][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9469e+00 (2.0170e+00)	Acc@1  42.97 ( 44.42)	Acc@5  76.56 ( 77.34)
Epoch: [9][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8692e+00 (2.0188e+00)	Acc@1  44.53 ( 44.35)	Acc@5  77.34 ( 77.31)
Epoch: [9][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1180e+00 (2.0205e+00)	Acc@1  39.06 ( 44.39)	Acc@5  75.00 ( 77.25)
Epoch: [9][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0266e+00 (2.0209e+00)	Acc@1  42.19 ( 44.37)	Acc@5  77.34 ( 77.27)
Epoch: [9][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9621e+00 (2.0213e+00)	Acc@1  53.91 ( 44.41)	Acc@5  75.78 ( 77.22)
Epoch: [9][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1675e+00 (2.0233e+00)	Acc@1  38.28 ( 44.34)	Acc@5  75.00 ( 77.21)
Epoch: [9][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1676e+00 (2.0227e+00)	Acc@1  40.62 ( 44.36)	Acc@5  70.31 ( 77.21)
Epoch: [9][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1758e+00 (2.0221e+00)	Acc@1  34.38 ( 44.40)	Acc@5  71.09 ( 77.21)
Epoch: [9][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9841e+00 (2.0193e+00)	Acc@1  41.41 ( 44.47)	Acc@5  80.47 ( 77.27)
Epoch: [9][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3287e+00 (2.0204e+00)	Acc@1  39.84 ( 44.47)	Acc@5  65.62 ( 77.22)
Epoch: [9][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0788e+00 (2.0214e+00)	Acc@1  36.72 ( 44.44)	Acc@5  76.56 ( 77.20)
Epoch: [9][390/391]	Time  0.053 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8880e+00 (2.0202e+00)	Acc@1  47.50 ( 44.48)	Acc@5  82.50 ( 77.22)
## e[9] optimizer.zero_grad (sum) time: 0.41711878776550293
## e[9]       loss.backward (sum) time: 6.969026803970337
## e[9]      optimizer.step (sum) time: 3.4809207916259766
## epoch[9] training(only) time: 25.57347321510315
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 2.0237e+00 (2.0237e+00)	Acc@1  45.00 ( 45.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 2.0559e+00 (2.1099e+00)	Acc@1  45.00 ( 43.64)	Acc@5  76.00 ( 75.64)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 2.0037e+00 (2.0956e+00)	Acc@1  41.00 ( 43.29)	Acc@5  78.00 ( 76.24)
Test: [ 30/100]	Time  0.029 ( 0.031)	Loss 2.2075e+00 (2.0841e+00)	Acc@1  40.00 ( 43.23)	Acc@5  74.00 ( 76.19)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.9024e+00 (2.0723e+00)	Acc@1  49.00 ( 43.46)	Acc@5  83.00 ( 76.44)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.9487e+00 (2.0648e+00)	Acc@1  45.00 ( 43.73)	Acc@5  80.00 ( 76.35)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.8936e+00 (2.0556e+00)	Acc@1  48.00 ( 43.97)	Acc@5  81.00 ( 76.69)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 2.1661e+00 (2.0684e+00)	Acc@1  41.00 ( 43.68)	Acc@5  74.00 ( 76.62)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.2760e+00 (2.0827e+00)	Acc@1  39.00 ( 43.49)	Acc@5  72.00 ( 76.16)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.3244e+00 (2.0767e+00)	Acc@1  37.00 ( 43.70)	Acc@5  75.00 ( 76.22)
 * Acc@1 43.940 Acc@5 76.290
### epoch[9] execution time: 28.388729333877563
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.216 ( 0.216)	Data  0.148 ( 0.148)	Loss 1.6898e+00 (1.6898e+00)	Acc@1  56.25 ( 56.25)	Acc@5  85.94 ( 85.94)
Epoch: [10][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 2.0898e+00 (1.9369e+00)	Acc@1  38.28 ( 47.37)	Acc@5  78.12 ( 79.40)
Epoch: [10][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 1.9151e+00 (1.9122e+00)	Acc@1  40.62 ( 46.88)	Acc@5  80.47 ( 79.91)
Epoch: [10][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.6582e+00 (1.8972e+00)	Acc@1  50.78 ( 46.93)	Acc@5  84.38 ( 80.27)
Epoch: [10][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.0054e+00 (1.9194e+00)	Acc@1  42.19 ( 46.47)	Acc@5  76.56 ( 79.31)
Epoch: [10][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.7918e+00 (1.9296e+00)	Acc@1  49.22 ( 46.14)	Acc@5  80.47 ( 79.20)
Epoch: [10][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.7973e+00 (1.9274e+00)	Acc@1  49.22 ( 46.17)	Acc@5  80.47 ( 79.15)
Epoch: [10][ 70/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9363e+00 (1.9244e+00)	Acc@1  43.75 ( 46.34)	Acc@5  82.81 ( 79.24)
Epoch: [10][ 80/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.0171e+00 (1.9328e+00)	Acc@1  49.22 ( 46.23)	Acc@5  75.78 ( 79.03)
Epoch: [10][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7653e+00 (1.9293e+00)	Acc@1  53.91 ( 46.49)	Acc@5  80.47 ( 78.97)
Epoch: [10][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.0324e+00 (1.9422e+00)	Acc@1  45.31 ( 46.09)	Acc@5  77.34 ( 78.71)
Epoch: [10][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8698e+00 (1.9456e+00)	Acc@1  50.78 ( 46.16)	Acc@5  82.03 ( 78.65)
Epoch: [10][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9554e+00 (1.9408e+00)	Acc@1  45.31 ( 46.29)	Acc@5  78.12 ( 78.71)
Epoch: [10][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8741e+00 (1.9369e+00)	Acc@1  50.78 ( 46.25)	Acc@5  77.34 ( 78.72)
Epoch: [10][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1074e+00 (1.9345e+00)	Acc@1  39.84 ( 46.31)	Acc@5  72.66 ( 78.78)
Epoch: [10][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9493e+00 (1.9364e+00)	Acc@1  41.41 ( 46.18)	Acc@5  79.69 ( 78.69)
Epoch: [10][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0621e+00 (1.9352e+00)	Acc@1  42.19 ( 46.24)	Acc@5  76.56 ( 78.70)
Epoch: [10][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9283e+00 (1.9338e+00)	Acc@1  46.88 ( 46.24)	Acc@5  77.34 ( 78.77)
Epoch: [10][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0115e+00 (1.9326e+00)	Acc@1  43.75 ( 46.25)	Acc@5  78.91 ( 78.87)
Epoch: [10][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0177e+00 (1.9303e+00)	Acc@1  45.31 ( 46.25)	Acc@5  75.00 ( 78.90)
Epoch: [10][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7569e+00 (1.9335e+00)	Acc@1  48.44 ( 46.21)	Acc@5  84.38 ( 78.88)
Epoch: [10][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2267e+00 (1.9336e+00)	Acc@1  41.41 ( 46.21)	Acc@5  75.00 ( 78.97)
Epoch: [10][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9579e+00 (1.9344e+00)	Acc@1  48.44 ( 46.22)	Acc@5  78.12 ( 78.94)
Epoch: [10][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1525e+00 (1.9354e+00)	Acc@1  46.88 ( 46.19)	Acc@5  70.31 ( 78.90)
Epoch: [10][240/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0693e+00 (1.9381e+00)	Acc@1  43.75 ( 46.16)	Acc@5  74.22 ( 78.85)
Epoch: [10][250/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8273e+00 (1.9408e+00)	Acc@1  46.09 ( 46.12)	Acc@5  78.12 ( 78.79)
Epoch: [10][260/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7690e+00 (1.9405e+00)	Acc@1  48.44 ( 46.11)	Acc@5  80.47 ( 78.79)
Epoch: [10][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8917e+00 (1.9397e+00)	Acc@1  50.00 ( 46.13)	Acc@5  78.91 ( 78.82)
Epoch: [10][280/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9559e+00 (1.9367e+00)	Acc@1  46.88 ( 46.19)	Acc@5  81.25 ( 78.86)
Epoch: [10][290/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0527e+00 (1.9365e+00)	Acc@1  41.41 ( 46.25)	Acc@5  73.44 ( 78.84)
Epoch: [10][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8088e+00 (1.9345e+00)	Acc@1  54.69 ( 46.33)	Acc@5  81.25 ( 78.91)
Epoch: [10][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1822e+00 (1.9322e+00)	Acc@1  43.75 ( 46.46)	Acc@5  74.22 ( 78.95)
Epoch: [10][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8893e+00 (1.9315e+00)	Acc@1  48.44 ( 46.47)	Acc@5  83.59 ( 78.97)
Epoch: [10][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0090e+00 (1.9304e+00)	Acc@1  45.31 ( 46.52)	Acc@5  83.59 ( 79.03)
Epoch: [10][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8955e+00 (1.9329e+00)	Acc@1  46.88 ( 46.50)	Acc@5  76.56 ( 78.96)
Epoch: [10][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8814e+00 (1.9332e+00)	Acc@1  49.22 ( 46.47)	Acc@5  80.47 ( 78.94)
Epoch: [10][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8091e+00 (1.9331e+00)	Acc@1  47.66 ( 46.42)	Acc@5  82.81 ( 78.93)
Epoch: [10][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8289e+00 (1.9324e+00)	Acc@1  45.31 ( 46.49)	Acc@5  78.91 ( 78.96)
Epoch: [10][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0287e+00 (1.9322e+00)	Acc@1  46.09 ( 46.47)	Acc@5  76.56 ( 78.96)
Epoch: [10][390/391]	Time  0.058 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3730e+00 (1.9322e+00)	Acc@1  37.50 ( 46.50)	Acc@5  71.25 ( 78.97)
## e[10] optimizer.zero_grad (sum) time: 0.4067966938018799
## e[10]       loss.backward (sum) time: 6.979773283004761
## e[10]      optimizer.step (sum) time: 3.5260610580444336
## epoch[10] training(only) time: 25.644227743148804
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.9653e+00 (1.9653e+00)	Acc@1  46.00 ( 46.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.030 ( 0.039)	Loss 2.4646e+00 (2.2080e+00)	Acc@1  37.00 ( 41.00)	Acc@5  74.00 ( 76.73)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 2.0203e+00 (2.1926e+00)	Acc@1  46.00 ( 40.95)	Acc@5  77.00 ( 76.29)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 2.3394e+00 (2.2017e+00)	Acc@1  39.00 ( 41.13)	Acc@5  72.00 ( 75.71)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 2.1199e+00 (2.1798e+00)	Acc@1  45.00 ( 41.66)	Acc@5  77.00 ( 76.17)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 2.1071e+00 (2.1869e+00)	Acc@1  41.00 ( 41.86)	Acc@5  72.00 ( 75.82)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.2105e+00 (2.1808e+00)	Acc@1  42.00 ( 41.66)	Acc@5  77.00 ( 75.95)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 2.2817e+00 (2.1957e+00)	Acc@1  44.00 ( 41.51)	Acc@5  74.00 ( 75.68)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.2431e+00 (2.2079e+00)	Acc@1  45.00 ( 41.44)	Acc@5  72.00 ( 75.44)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.0667e+00 (2.2021e+00)	Acc@1  45.00 ( 41.63)	Acc@5  78.00 ( 75.74)
 * Acc@1 41.780 Acc@5 75.650
### epoch[10] execution time: 28.485782384872437
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.214 ( 0.214)	Data  0.147 ( 0.147)	Loss 1.7106e+00 (1.7106e+00)	Acc@1  53.12 ( 53.12)	Acc@5  81.25 ( 81.25)
Epoch: [11][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.014)	Loss 2.0465e+00 (1.8736e+00)	Acc@1  40.62 ( 46.95)	Acc@5  74.22 ( 79.76)
Epoch: [11][ 20/391]	Time  0.061 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.7660e+00 (1.8557e+00)	Acc@1  52.34 ( 48.92)	Acc@5  83.59 ( 80.43)
Epoch: [11][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.5821e+00 (1.8382e+00)	Acc@1  53.12 ( 49.62)	Acc@5  85.16 ( 81.00)
Epoch: [11][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.0127e+00 (1.8372e+00)	Acc@1  42.19 ( 49.29)	Acc@5  77.34 ( 81.08)
Epoch: [11][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.9503e+00 (1.8334e+00)	Acc@1  42.19 ( 49.25)	Acc@5  78.12 ( 81.14)
Epoch: [11][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.6727e+00 (1.8303e+00)	Acc@1  49.22 ( 49.14)	Acc@5  81.25 ( 81.21)
Epoch: [11][ 70/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.0131e+00 (1.8377e+00)	Acc@1  46.09 ( 48.78)	Acc@5  82.03 ( 81.13)
Epoch: [11][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9413e+00 (1.8425e+00)	Acc@1  43.75 ( 48.61)	Acc@5  82.81 ( 81.05)
Epoch: [11][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9611e+00 (1.8364e+00)	Acc@1  50.78 ( 48.71)	Acc@5  78.12 ( 81.05)
Epoch: [11][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8957e+00 (1.8422e+00)	Acc@1  42.19 ( 48.52)	Acc@5  78.91 ( 80.97)
Epoch: [11][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.0187e+00 (1.8465e+00)	Acc@1  43.75 ( 48.39)	Acc@5  78.91 ( 80.88)
Epoch: [11][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0235e+00 (1.8464e+00)	Acc@1  43.75 ( 48.30)	Acc@5  78.12 ( 80.86)
Epoch: [11][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7343e+00 (1.8438e+00)	Acc@1  50.78 ( 48.35)	Acc@5  81.25 ( 80.88)
Epoch: [11][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5458e+00 (1.8392e+00)	Acc@1  54.69 ( 48.40)	Acc@5  83.59 ( 80.98)
Epoch: [11][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0829e+00 (1.8421e+00)	Acc@1  41.41 ( 48.38)	Acc@5  74.22 ( 80.96)
Epoch: [11][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9127e+00 (1.8508e+00)	Acc@1  45.31 ( 48.18)	Acc@5  82.81 ( 80.81)
Epoch: [11][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9809e+00 (1.8527e+00)	Acc@1  43.75 ( 48.13)	Acc@5  80.47 ( 80.70)
Epoch: [11][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8313e+00 (1.8543e+00)	Acc@1  53.12 ( 48.17)	Acc@5  78.91 ( 80.66)
Epoch: [11][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8857e+00 (1.8551e+00)	Acc@1  47.66 ( 48.13)	Acc@5  78.91 ( 80.69)
Epoch: [11][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9520e+00 (1.8579e+00)	Acc@1  48.44 ( 48.09)	Acc@5  81.25 ( 80.65)
Epoch: [11][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7529e+00 (1.8599e+00)	Acc@1  60.16 ( 48.09)	Acc@5  79.69 ( 80.59)
Epoch: [11][220/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7580e+00 (1.8564e+00)	Acc@1  51.56 ( 48.21)	Acc@5  81.25 ( 80.64)
Epoch: [11][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7365e+00 (1.8563e+00)	Acc@1  51.56 ( 48.26)	Acc@5  82.03 ( 80.66)
Epoch: [11][240/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8830e+00 (1.8595e+00)	Acc@1  46.88 ( 48.17)	Acc@5  82.03 ( 80.61)
Epoch: [11][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8358e+00 (1.8610e+00)	Acc@1  48.44 ( 48.09)	Acc@5  85.16 ( 80.60)
Epoch: [11][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8950e+00 (1.8603e+00)	Acc@1  48.44 ( 48.14)	Acc@5  83.59 ( 80.60)
Epoch: [11][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9863e+00 (1.8616e+00)	Acc@1  44.53 ( 48.13)	Acc@5  78.12 ( 80.59)
Epoch: [11][280/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8365e+00 (1.8622e+00)	Acc@1  46.09 ( 48.11)	Acc@5  82.03 ( 80.56)
Epoch: [11][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6377e+00 (1.8599e+00)	Acc@1  56.25 ( 48.17)	Acc@5  85.16 ( 80.58)
Epoch: [11][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6781e+00 (1.8588e+00)	Acc@1  50.78 ( 48.21)	Acc@5  85.94 ( 80.59)
Epoch: [11][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7899e+00 (1.8587e+00)	Acc@1  49.22 ( 48.19)	Acc@5  85.16 ( 80.62)
Epoch: [11][320/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8573e+00 (1.8616e+00)	Acc@1  46.09 ( 48.11)	Acc@5  85.16 ( 80.58)
Epoch: [11][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9398e+00 (1.8614e+00)	Acc@1  43.75 ( 48.10)	Acc@5  76.56 ( 80.58)
Epoch: [11][340/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7942e+00 (1.8629e+00)	Acc@1  56.25 ( 48.10)	Acc@5  81.25 ( 80.52)
Epoch: [11][350/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9248e+00 (1.8632e+00)	Acc@1  46.09 ( 48.11)	Acc@5  79.69 ( 80.50)
Epoch: [11][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0068e+00 (1.8632e+00)	Acc@1  42.19 ( 48.10)	Acc@5  75.00 ( 80.46)
Epoch: [11][370/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9772e+00 (1.8629e+00)	Acc@1  46.88 ( 48.12)	Acc@5  77.34 ( 80.45)
Epoch: [11][380/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1294e+00 (1.8646e+00)	Acc@1  45.31 ( 48.08)	Acc@5  75.00 ( 80.40)
Epoch: [11][390/391]	Time  0.052 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7896e+00 (1.8663e+00)	Acc@1  46.25 ( 48.06)	Acc@5  86.25 ( 80.33)
## e[11] optimizer.zero_grad (sum) time: 0.4096181392669678
## e[11]       loss.backward (sum) time: 6.959580659866333
## e[11]      optimizer.step (sum) time: 3.542203426361084
## epoch[11] training(only) time: 25.673107385635376
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.8403e+00 (1.8403e+00)	Acc@1  51.00 ( 51.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 2.0232e+00 (1.9549e+00)	Acc@1  42.00 ( 46.27)	Acc@5  81.00 ( 78.73)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.8026e+00 (1.9403e+00)	Acc@1  44.00 ( 46.95)	Acc@5  82.00 ( 78.67)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 2.0239e+00 (1.9361e+00)	Acc@1  39.00 ( 46.87)	Acc@5  82.00 ( 78.68)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 2.0357e+00 (1.9507e+00)	Acc@1  48.00 ( 46.88)	Acc@5  74.00 ( 78.32)
Test: [ 50/100]	Time  0.027 ( 0.030)	Loss 1.7120e+00 (1.9497e+00)	Acc@1  55.00 ( 46.88)	Acc@5  79.00 ( 77.92)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.9249e+00 (1.9319e+00)	Acc@1  51.00 ( 47.08)	Acc@5  77.00 ( 78.38)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 2.1595e+00 (1.9395e+00)	Acc@1  47.00 ( 46.97)	Acc@5  75.00 ( 78.28)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.1313e+00 (1.9529e+00)	Acc@1  44.00 ( 46.68)	Acc@5  73.00 ( 78.06)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.0674e+00 (1.9456e+00)	Acc@1  41.00 ( 46.82)	Acc@5  83.00 ( 78.29)
 * Acc@1 46.970 Acc@5 78.350
### epoch[11] execution time: 28.527339696884155
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.220 ( 0.220)	Data  0.148 ( 0.148)	Loss 1.7492e+00 (1.7492e+00)	Acc@1  49.22 ( 49.22)	Acc@5  84.38 ( 84.38)
Epoch: [12][ 10/391]	Time  0.069 ( 0.079)	Data  0.001 ( 0.014)	Loss 1.9271e+00 (1.8043e+00)	Acc@1  48.44 ( 51.14)	Acc@5  83.59 ( 81.89)
Epoch: [12][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 2.0039e+00 (1.7909e+00)	Acc@1  49.22 ( 50.67)	Acc@5  77.34 ( 81.81)
Epoch: [12][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.6061e+00 (1.7862e+00)	Acc@1  50.78 ( 50.38)	Acc@5  86.72 ( 81.75)
Epoch: [12][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.7284e+00 (1.7758e+00)	Acc@1  58.59 ( 50.74)	Acc@5  81.25 ( 81.80)
Epoch: [12][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.0579e+00 (1.7765e+00)	Acc@1  40.62 ( 50.64)	Acc@5  76.56 ( 81.83)
Epoch: [12][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.9375e+00 (1.7853e+00)	Acc@1  45.31 ( 50.53)	Acc@5  78.91 ( 81.72)
Epoch: [12][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6996e+00 (1.7939e+00)	Acc@1  49.22 ( 50.09)	Acc@5  83.59 ( 81.60)
Epoch: [12][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.9693e+00 (1.8028e+00)	Acc@1  46.88 ( 49.96)	Acc@5  79.69 ( 81.36)
Epoch: [12][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.0254e+00 (1.8042e+00)	Acc@1  43.75 ( 50.10)	Acc@5  80.47 ( 81.44)
Epoch: [12][100/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7424e+00 (1.7997e+00)	Acc@1  48.44 ( 50.05)	Acc@5  84.38 ( 81.47)
Epoch: [12][110/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.7930e+00 (1.8018e+00)	Acc@1  50.00 ( 49.89)	Acc@5  83.59 ( 81.48)
Epoch: [12][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8206e+00 (1.7986e+00)	Acc@1  45.31 ( 49.97)	Acc@5  79.69 ( 81.56)
Epoch: [12][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7820e+00 (1.7941e+00)	Acc@1  49.22 ( 50.04)	Acc@5  84.38 ( 81.55)
Epoch: [12][140/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7075e+00 (1.7951e+00)	Acc@1  56.25 ( 50.03)	Acc@5  80.47 ( 81.54)
Epoch: [12][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8449e+00 (1.7928e+00)	Acc@1  49.22 ( 50.06)	Acc@5  83.59 ( 81.58)
Epoch: [12][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9721e+00 (1.7945e+00)	Acc@1  41.41 ( 49.96)	Acc@5  76.56 ( 81.59)
Epoch: [12][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5183e+00 (1.7871e+00)	Acc@1  53.91 ( 50.07)	Acc@5  87.50 ( 81.77)
Epoch: [12][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7325e+00 (1.7892e+00)	Acc@1  52.34 ( 50.01)	Acc@5  80.47 ( 81.70)
Epoch: [12][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5700e+00 (1.7882e+00)	Acc@1  56.25 ( 50.03)	Acc@5  90.62 ( 81.77)
Epoch: [12][200/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6527e+00 (1.7892e+00)	Acc@1  51.56 ( 49.93)	Acc@5  85.16 ( 81.80)
Epoch: [12][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7404e+00 (1.7887e+00)	Acc@1  48.44 ( 49.93)	Acc@5  81.25 ( 81.80)
Epoch: [12][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6539e+00 (1.7921e+00)	Acc@1  50.00 ( 49.85)	Acc@5  82.03 ( 81.65)
Epoch: [12][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0211e+00 (1.7909e+00)	Acc@1  39.06 ( 49.82)	Acc@5  78.91 ( 81.65)
Epoch: [12][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1004e+00 (1.7945e+00)	Acc@1  41.41 ( 49.78)	Acc@5  72.66 ( 81.60)
Epoch: [12][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6143e+00 (1.7938e+00)	Acc@1  53.12 ( 49.79)	Acc@5  87.50 ( 81.65)
Epoch: [12][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8076e+00 (1.7944e+00)	Acc@1  55.47 ( 49.83)	Acc@5  81.25 ( 81.58)
Epoch: [12][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6810e+00 (1.7953e+00)	Acc@1  51.56 ( 49.83)	Acc@5  85.94 ( 81.58)
Epoch: [12][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7434e+00 (1.7947e+00)	Acc@1  50.78 ( 49.85)	Acc@5  86.72 ( 81.62)
Epoch: [12][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8368e+00 (1.7951e+00)	Acc@1  48.44 ( 49.80)	Acc@5  76.56 ( 81.66)
Epoch: [12][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5787e+00 (1.7947e+00)	Acc@1  53.12 ( 49.77)	Acc@5  82.81 ( 81.63)
Epoch: [12][310/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8552e+00 (1.7956e+00)	Acc@1  46.88 ( 49.72)	Acc@5  82.03 ( 81.63)
Epoch: [12][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7241e+00 (1.7963e+00)	Acc@1  58.59 ( 49.73)	Acc@5  85.16 ( 81.63)
Epoch: [12][330/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9897e+00 (1.8006e+00)	Acc@1  46.88 ( 49.66)	Acc@5  78.91 ( 81.51)
Epoch: [12][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9501e+00 (1.8018e+00)	Acc@1  46.09 ( 49.65)	Acc@5  76.56 ( 81.44)
Epoch: [12][350/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7685e+00 (1.8001e+00)	Acc@1  49.22 ( 49.71)	Acc@5  83.59 ( 81.47)
Epoch: [12][360/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6195e+00 (1.8006e+00)	Acc@1  53.12 ( 49.72)	Acc@5  84.38 ( 81.44)
Epoch: [12][370/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9556e+00 (1.8021e+00)	Acc@1  45.31 ( 49.67)	Acc@5  78.12 ( 81.41)
Epoch: [12][380/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6692e+00 (1.8003e+00)	Acc@1  52.34 ( 49.74)	Acc@5  87.50 ( 81.46)
Epoch: [12][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6557e+00 (1.8016e+00)	Acc@1  48.75 ( 49.71)	Acc@5  90.00 ( 81.47)
## e[12] optimizer.zero_grad (sum) time: 0.424177885055542
## e[12]       loss.backward (sum) time: 7.008268594741821
## e[12]      optimizer.step (sum) time: 3.4584603309631348
## epoch[12] training(only) time: 25.664822101593018
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.8370e+00 (1.8370e+00)	Acc@1  50.00 ( 50.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.025 ( 0.040)	Loss 1.9626e+00 (1.9368e+00)	Acc@1  44.00 ( 46.91)	Acc@5  78.00 ( 80.00)
Test: [ 20/100]	Time  0.026 ( 0.034)	Loss 1.7892e+00 (1.9170e+00)	Acc@1  51.00 ( 47.57)	Acc@5  86.00 ( 80.00)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.0071e+00 (1.9280e+00)	Acc@1  44.00 ( 46.87)	Acc@5  75.00 ( 79.71)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.8799e+00 (1.9181e+00)	Acc@1  46.00 ( 47.10)	Acc@5  79.00 ( 79.73)
Test: [ 50/100]	Time  0.026 ( 0.030)	Loss 2.1448e+00 (1.9360e+00)	Acc@1  36.00 ( 46.53)	Acc@5  72.00 ( 79.10)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 2.0435e+00 (1.9408e+00)	Acc@1  43.00 ( 46.33)	Acc@5  73.00 ( 78.93)
Test: [ 70/100]	Time  0.027 ( 0.029)	Loss 2.1571e+00 (1.9551e+00)	Acc@1  44.00 ( 46.01)	Acc@5  75.00 ( 78.69)
Test: [ 80/100]	Time  0.031 ( 0.028)	Loss 2.0966e+00 (1.9642e+00)	Acc@1  41.00 ( 45.91)	Acc@5  76.00 ( 78.46)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9067e+00 (1.9518e+00)	Acc@1  48.00 ( 46.19)	Acc@5  80.00 ( 78.81)
 * Acc@1 46.140 Acc@5 78.820
### epoch[12] execution time: 28.52167320251465
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.223 ( 0.223)	Data  0.152 ( 0.152)	Loss 1.6543e+00 (1.6543e+00)	Acc@1  56.25 ( 56.25)	Acc@5  83.59 ( 83.59)
Epoch: [13][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.015)	Loss 1.7919e+00 (1.6945e+00)	Acc@1  53.12 ( 52.34)	Acc@5  79.69 ( 82.88)
Epoch: [13][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.8935e+00 (1.7080e+00)	Acc@1  43.75 ( 51.60)	Acc@5  80.47 ( 83.04)
Epoch: [13][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.7962e+00 (1.7383e+00)	Acc@1  51.56 ( 50.76)	Acc@5  85.16 ( 82.69)
Epoch: [13][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.7643e+00 (1.7454e+00)	Acc@1  49.22 ( 50.53)	Acc@5  81.25 ( 82.58)
Epoch: [13][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.8164e+00 (1.7361e+00)	Acc@1  53.12 ( 50.78)	Acc@5  84.38 ( 82.71)
Epoch: [13][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.7586e+00 (1.7428e+00)	Acc@1  50.00 ( 51.00)	Acc@5  78.12 ( 82.45)
Epoch: [13][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4526e+00 (1.7428e+00)	Acc@1  60.16 ( 50.86)	Acc@5  88.28 ( 82.54)
Epoch: [13][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5041e+00 (1.7365e+00)	Acc@1  53.91 ( 51.01)	Acc@5  85.16 ( 82.65)
Epoch: [13][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6398e+00 (1.7333e+00)	Acc@1  57.03 ( 51.21)	Acc@5  85.94 ( 82.68)
Epoch: [13][100/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5672e+00 (1.7287e+00)	Acc@1  52.34 ( 51.38)	Acc@5  87.50 ( 82.70)
Epoch: [13][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.7353e+00 (1.7253e+00)	Acc@1  53.91 ( 51.45)	Acc@5  85.94 ( 82.87)
Epoch: [13][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7671e+00 (1.7286e+00)	Acc@1  54.69 ( 51.44)	Acc@5  77.34 ( 82.76)
Epoch: [13][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7301e+00 (1.7244e+00)	Acc@1  50.00 ( 51.59)	Acc@5  79.69 ( 82.81)
Epoch: [13][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6064e+00 (1.7261e+00)	Acc@1  52.34 ( 51.51)	Acc@5  84.38 ( 82.78)
Epoch: [13][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7251e+00 (1.7243e+00)	Acc@1  55.47 ( 51.59)	Acc@5  82.03 ( 82.87)
Epoch: [13][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7504e+00 (1.7243e+00)	Acc@1  51.56 ( 51.55)	Acc@5  78.91 ( 82.83)
Epoch: [13][170/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9304e+00 (1.7315e+00)	Acc@1  43.75 ( 51.38)	Acc@5  80.47 ( 82.68)
Epoch: [13][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5975e+00 (1.7315e+00)	Acc@1  54.69 ( 51.30)	Acc@5  85.94 ( 82.67)
Epoch: [13][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8895e+00 (1.7359e+00)	Acc@1  46.88 ( 51.24)	Acc@5  81.25 ( 82.59)
Epoch: [13][200/391]	Time  0.066 ( 0.066)	Data  0.002 ( 0.002)	Loss 1.7622e+00 (1.7350e+00)	Acc@1  43.75 ( 51.23)	Acc@5  84.38 ( 82.56)
Epoch: [13][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6413e+00 (1.7371e+00)	Acc@1  52.34 ( 51.17)	Acc@5  82.03 ( 82.53)
Epoch: [13][220/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6462e+00 (1.7372e+00)	Acc@1  54.69 ( 51.22)	Acc@5  84.38 ( 82.57)
Epoch: [13][230/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9032e+00 (1.7400e+00)	Acc@1  46.88 ( 51.16)	Acc@5  82.03 ( 82.50)
Epoch: [13][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6451e+00 (1.7367e+00)	Acc@1  54.69 ( 51.26)	Acc@5  82.03 ( 82.57)
Epoch: [13][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9699e+00 (1.7346e+00)	Acc@1  45.31 ( 51.38)	Acc@5  78.91 ( 82.57)
Epoch: [13][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.9580e+00 (1.7365e+00)	Acc@1  47.66 ( 51.31)	Acc@5  78.91 ( 82.53)
Epoch: [13][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8489e+00 (1.7359e+00)	Acc@1  46.88 ( 51.28)	Acc@5  82.03 ( 82.56)
Epoch: [13][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7488e+00 (1.7366e+00)	Acc@1  50.00 ( 51.27)	Acc@5  82.81 ( 82.54)
Epoch: [13][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0687e+00 (1.7407e+00)	Acc@1  40.62 ( 51.14)	Acc@5  81.25 ( 82.46)
Epoch: [13][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6091e+00 (1.7390e+00)	Acc@1  55.47 ( 51.18)	Acc@5  84.38 ( 82.50)
Epoch: [13][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6777e+00 (1.7384e+00)	Acc@1  53.91 ( 51.15)	Acc@5  85.94 ( 82.52)
Epoch: [13][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7732e+00 (1.7385e+00)	Acc@1  53.12 ( 51.20)	Acc@5  81.25 ( 82.48)
Epoch: [13][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6591e+00 (1.7400e+00)	Acc@1  53.12 ( 51.16)	Acc@5  83.59 ( 82.45)
Epoch: [13][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7428e+00 (1.7415e+00)	Acc@1  49.22 ( 51.12)	Acc@5  85.16 ( 82.45)
Epoch: [13][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8321e+00 (1.7421e+00)	Acc@1  50.78 ( 51.08)	Acc@5  78.12 ( 82.47)
Epoch: [13][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7564e+00 (1.7418e+00)	Acc@1  48.44 ( 51.09)	Acc@5  82.03 ( 82.46)
Epoch: [13][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7915e+00 (1.7416e+00)	Acc@1  47.66 ( 51.08)	Acc@5  83.59 ( 82.48)
Epoch: [13][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7411e+00 (1.7416e+00)	Acc@1  54.69 ( 51.07)	Acc@5  82.81 ( 82.53)
Epoch: [13][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5630e+00 (1.7410e+00)	Acc@1  56.25 ( 51.08)	Acc@5  82.50 ( 82.54)
## e[13] optimizer.zero_grad (sum) time: 0.4069502353668213
## e[13]       loss.backward (sum) time: 6.975897789001465
## e[13]      optimizer.step (sum) time: 3.4272091388702393
## epoch[13] training(only) time: 25.519083738327026
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.8998e+00 (1.8998e+00)	Acc@1  51.00 ( 51.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.9874e+00 (1.9536e+00)	Acc@1  43.00 ( 47.18)	Acc@5  79.00 ( 78.91)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.8429e+00 (1.9379e+00)	Acc@1  49.00 ( 47.90)	Acc@5  83.00 ( 79.38)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 2.1186e+00 (1.9381e+00)	Acc@1  45.00 ( 47.87)	Acc@5  79.00 ( 79.10)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.9729e+00 (1.9370e+00)	Acc@1  43.00 ( 47.41)	Acc@5  82.00 ( 79.49)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.8046e+00 (1.9410e+00)	Acc@1  52.00 ( 47.08)	Acc@5  79.00 ( 79.27)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.8230e+00 (1.9187e+00)	Acc@1  50.00 ( 47.34)	Acc@5  79.00 ( 79.67)
Test: [ 70/100]	Time  0.030 ( 0.028)	Loss 2.0133e+00 (1.9326e+00)	Acc@1  45.00 ( 47.17)	Acc@5  79.00 ( 79.42)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 2.0979e+00 (1.9454e+00)	Acc@1  38.00 ( 46.84)	Acc@5  80.00 ( 79.35)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 2.0437e+00 (1.9379e+00)	Acc@1  46.00 ( 47.13)	Acc@5  80.00 ( 79.45)
 * Acc@1 47.220 Acc@5 79.550
### epoch[13] execution time: 28.36312222480774
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.221 ( 0.221)	Data  0.153 ( 0.153)	Loss 1.7696e+00 (1.7696e+00)	Acc@1  50.78 ( 50.78)	Acc@5  81.25 ( 81.25)
Epoch: [14][ 10/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.015)	Loss 1.7170e+00 (1.7349e+00)	Acc@1  48.44 ( 50.07)	Acc@5  85.94 ( 82.03)
Epoch: [14][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 1.5555e+00 (1.6990e+00)	Acc@1  60.16 ( 51.71)	Acc@5  82.81 ( 82.89)
Epoch: [14][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.5369e+00 (1.6837e+00)	Acc@1  53.12 ( 52.12)	Acc@5  86.72 ( 83.32)
Epoch: [14][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.7915e+00 (1.6917e+00)	Acc@1  46.09 ( 51.79)	Acc@5  81.25 ( 83.42)
Epoch: [14][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3990e+00 (1.6702e+00)	Acc@1  57.81 ( 52.39)	Acc@5  87.50 ( 83.85)
Epoch: [14][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.7520e+00 (1.6670e+00)	Acc@1  55.47 ( 52.39)	Acc@5  82.03 ( 84.04)
Epoch: [14][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.8090e+00 (1.6687e+00)	Acc@1  50.00 ( 52.35)	Acc@5  83.59 ( 83.98)
Epoch: [14][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6511e+00 (1.6728e+00)	Acc@1  50.78 ( 52.28)	Acc@5  82.81 ( 83.86)
Epoch: [14][ 90/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7047e+00 (1.6772e+00)	Acc@1  52.34 ( 52.16)	Acc@5  85.16 ( 83.81)
Epoch: [14][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5825e+00 (1.6751e+00)	Acc@1  56.25 ( 52.39)	Acc@5  82.81 ( 83.86)
Epoch: [14][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5109e+00 (1.6717e+00)	Acc@1  57.81 ( 52.68)	Acc@5  86.72 ( 83.95)
Epoch: [14][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7554e+00 (1.6739e+00)	Acc@1  50.00 ( 52.47)	Acc@5  83.59 ( 83.90)
Epoch: [14][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3531e+00 (1.6662e+00)	Acc@1  60.94 ( 52.76)	Acc@5  89.84 ( 84.07)
Epoch: [14][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6736e+00 (1.6656e+00)	Acc@1  52.34 ( 52.84)	Acc@5  87.50 ( 84.11)
Epoch: [14][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8169e+00 (1.6698e+00)	Acc@1  54.69 ( 52.74)	Acc@5  79.69 ( 84.05)
Epoch: [14][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6019e+00 (1.6759e+00)	Acc@1  50.00 ( 52.63)	Acc@5  89.84 ( 83.97)
Epoch: [14][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7320e+00 (1.6794e+00)	Acc@1  45.31 ( 52.53)	Acc@5  82.81 ( 83.85)
Epoch: [14][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6832e+00 (1.6843e+00)	Acc@1  55.47 ( 52.52)	Acc@5  79.69 ( 83.74)
Epoch: [14][190/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5753e+00 (1.6824e+00)	Acc@1  57.03 ( 52.49)	Acc@5  88.28 ( 83.84)
Epoch: [14][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7874e+00 (1.6834e+00)	Acc@1  48.44 ( 52.40)	Acc@5  82.03 ( 83.83)
Epoch: [14][210/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7238e+00 (1.6876e+00)	Acc@1  52.34 ( 52.35)	Acc@5  85.16 ( 83.68)
Epoch: [14][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8480e+00 (1.6868e+00)	Acc@1  49.22 ( 52.39)	Acc@5  81.25 ( 83.69)
Epoch: [14][230/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4096e+00 (1.6849e+00)	Acc@1  55.47 ( 52.40)	Acc@5  91.41 ( 83.72)
Epoch: [14][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6513e+00 (1.6826e+00)	Acc@1  52.34 ( 52.48)	Acc@5  84.38 ( 83.75)
Epoch: [14][250/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7949e+00 (1.6826e+00)	Acc@1  53.12 ( 52.55)	Acc@5  82.81 ( 83.73)
Epoch: [14][260/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6975e+00 (1.6843e+00)	Acc@1  53.12 ( 52.52)	Acc@5  82.81 ( 83.67)
Epoch: [14][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8334e+00 (1.6863e+00)	Acc@1  50.00 ( 52.50)	Acc@5  83.59 ( 83.64)
Epoch: [14][280/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9670e+00 (1.6880e+00)	Acc@1  42.97 ( 52.40)	Acc@5  81.25 ( 83.61)
Epoch: [14][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5967e+00 (1.6892e+00)	Acc@1  55.47 ( 52.35)	Acc@5  83.59 ( 83.54)
Epoch: [14][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7294e+00 (1.6895e+00)	Acc@1  47.66 ( 52.33)	Acc@5  81.25 ( 83.53)
Epoch: [14][310/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7787e+00 (1.6894e+00)	Acc@1  46.88 ( 52.34)	Acc@5  80.47 ( 83.55)
Epoch: [14][320/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8287e+00 (1.6895e+00)	Acc@1  50.00 ( 52.40)	Acc@5  80.47 ( 83.50)
Epoch: [14][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5994e+00 (1.6865e+00)	Acc@1  51.56 ( 52.48)	Acc@5  85.16 ( 83.50)
Epoch: [14][340/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6742e+00 (1.6890e+00)	Acc@1  51.56 ( 52.45)	Acc@5  79.69 ( 83.44)
Epoch: [14][350/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8156e+00 (1.6903e+00)	Acc@1  50.78 ( 52.40)	Acc@5  78.91 ( 83.39)
Epoch: [14][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0124e+00 (1.6929e+00)	Acc@1  43.75 ( 52.35)	Acc@5  77.34 ( 83.36)
Epoch: [14][370/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7640e+00 (1.6945e+00)	Acc@1  53.12 ( 52.33)	Acc@5  78.12 ( 83.33)
Epoch: [14][380/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8582e+00 (1.6968e+00)	Acc@1  46.09 ( 52.25)	Acc@5  79.69 ( 83.28)
Epoch: [14][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6380e+00 (1.6981e+00)	Acc@1  52.50 ( 52.24)	Acc@5  83.75 ( 83.23)
## e[14] optimizer.zero_grad (sum) time: 0.41111111640930176
## e[14]       loss.backward (sum) time: 6.99363112449646
## e[14]      optimizer.step (sum) time: 3.4692018032073975
## epoch[14] training(only) time: 25.65407633781433
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.8195e+00 (1.8195e+00)	Acc@1  53.00 ( 53.00)	Acc@5  73.00 ( 73.00)
Test: [ 10/100]	Time  0.025 ( 0.040)	Loss 1.8906e+00 (1.8596e+00)	Acc@1  51.00 ( 50.09)	Acc@5  79.00 ( 80.00)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.6917e+00 (1.8452e+00)	Acc@1  54.00 ( 49.52)	Acc@5  82.00 ( 80.67)
Test: [ 30/100]	Time  0.029 ( 0.032)	Loss 1.8725e+00 (1.8473e+00)	Acc@1  49.00 ( 49.45)	Acc@5  79.00 ( 80.58)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.8641e+00 (1.8400e+00)	Acc@1  50.00 ( 49.46)	Acc@5  77.00 ( 80.68)
Test: [ 50/100]	Time  0.025 ( 0.030)	Loss 1.8137e+00 (1.8556e+00)	Acc@1  46.00 ( 49.12)	Acc@5  80.00 ( 80.22)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.7274e+00 (1.8448e+00)	Acc@1  53.00 ( 49.15)	Acc@5  81.00 ( 80.36)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 2.0140e+00 (1.8552e+00)	Acc@1  39.00 ( 48.86)	Acc@5  79.00 ( 80.34)
Test: [ 80/100]	Time  0.025 ( 0.029)	Loss 1.9710e+00 (1.8623e+00)	Acc@1  41.00 ( 48.62)	Acc@5  77.00 ( 80.11)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 2.1302e+00 (1.8550e+00)	Acc@1  45.00 ( 48.88)	Acc@5  77.00 ( 80.37)
 * Acc@1 48.960 Acc@5 80.280
### epoch[14] execution time: 28.54843235015869
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.217 ( 0.217)	Data  0.150 ( 0.150)	Loss 1.6547e+00 (1.6547e+00)	Acc@1  51.56 ( 51.56)	Acc@5  87.50 ( 87.50)
Epoch: [15][ 10/391]	Time  0.067 ( 0.079)	Data  0.001 ( 0.015)	Loss 1.4528e+00 (1.5508e+00)	Acc@1  60.94 ( 56.18)	Acc@5  85.94 ( 86.43)
Epoch: [15][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.5427e+00 (1.5896e+00)	Acc@1  57.81 ( 54.54)	Acc@5  83.59 ( 85.04)
Epoch: [15][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.7000e+00 (1.5894e+00)	Acc@1  56.25 ( 54.59)	Acc@5  81.25 ( 84.88)
Epoch: [15][ 40/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.7113e+00 (1.5875e+00)	Acc@1  47.66 ( 54.48)	Acc@5  85.16 ( 84.81)
Epoch: [15][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4567e+00 (1.5785e+00)	Acc@1  52.34 ( 54.61)	Acc@5  87.50 ( 84.96)
Epoch: [15][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.6695e+00 (1.5818e+00)	Acc@1  56.25 ( 54.46)	Acc@5  82.81 ( 84.90)
Epoch: [15][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.0139e+00 (1.5933e+00)	Acc@1  52.34 ( 54.30)	Acc@5  78.91 ( 84.71)
Epoch: [15][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7851e+00 (1.5957e+00)	Acc@1  48.44 ( 54.18)	Acc@5  79.69 ( 84.73)
Epoch: [15][ 90/391]	Time  0.069 ( 0.067)	Data  0.004 ( 0.003)	Loss 1.7389e+00 (1.5940e+00)	Acc@1  57.03 ( 54.34)	Acc@5  83.59 ( 84.81)
Epoch: [15][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5037e+00 (1.5933e+00)	Acc@1  58.59 ( 54.39)	Acc@5  85.94 ( 84.86)
Epoch: [15][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6652e+00 (1.6006e+00)	Acc@1  58.59 ( 54.39)	Acc@5  86.72 ( 84.83)
Epoch: [15][120/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5679e+00 (1.5976e+00)	Acc@1  55.47 ( 54.37)	Acc@5  86.72 ( 84.98)
Epoch: [15][130/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7851e+00 (1.6036e+00)	Acc@1  51.56 ( 54.24)	Acc@5  77.34 ( 84.82)
Epoch: [15][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9499e+00 (1.6108e+00)	Acc@1  47.66 ( 54.12)	Acc@5  79.69 ( 84.66)
Epoch: [15][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6994e+00 (1.6148e+00)	Acc@1  53.91 ( 54.04)	Acc@5  83.59 ( 84.57)
Epoch: [15][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7631e+00 (1.6225e+00)	Acc@1  52.34 ( 53.90)	Acc@5  80.47 ( 84.43)
Epoch: [15][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4980e+00 (1.6209e+00)	Acc@1  58.59 ( 54.05)	Acc@5  85.94 ( 84.45)
Epoch: [15][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9520e+00 (1.6262e+00)	Acc@1  50.78 ( 53.93)	Acc@5  79.69 ( 84.36)
Epoch: [15][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8547e+00 (1.6288e+00)	Acc@1  48.44 ( 53.91)	Acc@5  78.12 ( 84.33)
Epoch: [15][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6431e+00 (1.6312e+00)	Acc@1  54.69 ( 53.82)	Acc@5  85.16 ( 84.27)
Epoch: [15][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8175e+00 (1.6321e+00)	Acc@1  50.78 ( 53.79)	Acc@5  77.34 ( 84.28)
Epoch: [15][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7409e+00 (1.6331e+00)	Acc@1  51.56 ( 53.78)	Acc@5  84.38 ( 84.25)
Epoch: [15][230/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6399e+00 (1.6316e+00)	Acc@1  53.12 ( 53.77)	Acc@5  81.25 ( 84.23)
Epoch: [15][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6808e+00 (1.6302e+00)	Acc@1  57.81 ( 53.85)	Acc@5  84.38 ( 84.26)
Epoch: [15][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7267e+00 (1.6303e+00)	Acc@1  53.91 ( 53.83)	Acc@5  82.81 ( 84.27)
Epoch: [15][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6561e+00 (1.6321e+00)	Acc@1  53.12 ( 53.82)	Acc@5  80.47 ( 84.21)
Epoch: [15][270/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8266e+00 (1.6350e+00)	Acc@1  52.34 ( 53.72)	Acc@5  78.91 ( 84.17)
Epoch: [15][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7895e+00 (1.6401e+00)	Acc@1  51.56 ( 53.61)	Acc@5  78.12 ( 84.06)
Epoch: [15][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6464e+00 (1.6422e+00)	Acc@1  52.34 ( 53.58)	Acc@5  84.38 ( 84.03)
Epoch: [15][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4250e+00 (1.6409e+00)	Acc@1  58.59 ( 53.63)	Acc@5  88.28 ( 84.07)
Epoch: [15][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4376e+00 (1.6407e+00)	Acc@1  57.03 ( 53.63)	Acc@5  91.41 ( 84.06)
Epoch: [15][320/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7730e+00 (1.6413e+00)	Acc@1  48.44 ( 53.63)	Acc@5  79.69 ( 84.04)
Epoch: [15][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8205e+00 (1.6435e+00)	Acc@1  48.44 ( 53.59)	Acc@5  80.47 ( 84.04)
Epoch: [15][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5473e+00 (1.6418e+00)	Acc@1  57.81 ( 53.64)	Acc@5  85.16 ( 84.06)
Epoch: [15][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7435e+00 (1.6372e+00)	Acc@1  55.47 ( 53.76)	Acc@5  82.81 ( 84.13)
Epoch: [15][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4947e+00 (1.6377e+00)	Acc@1  57.81 ( 53.75)	Acc@5  89.06 ( 84.16)
Epoch: [15][370/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9077e+00 (1.6384e+00)	Acc@1  42.19 ( 53.68)	Acc@5  81.25 ( 84.18)
Epoch: [15][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6877e+00 (1.6400e+00)	Acc@1  47.66 ( 53.64)	Acc@5  80.47 ( 84.13)
Epoch: [15][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5433e+00 (1.6432e+00)	Acc@1  55.00 ( 53.60)	Acc@5  85.00 ( 84.08)
## e[15] optimizer.zero_grad (sum) time: 0.41447925567626953
## e[15]       loss.backward (sum) time: 7.021003723144531
## e[15]      optimizer.step (sum) time: 3.468657970428467
## epoch[15] training(only) time: 25.700031757354736
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.6041e+00 (1.6041e+00)	Acc@1  58.00 ( 58.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.8144e+00 (1.7390e+00)	Acc@1  46.00 ( 52.91)	Acc@5  84.00 ( 81.82)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.7952e+00 (1.7302e+00)	Acc@1  52.00 ( 52.00)	Acc@5  81.00 ( 82.81)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.8412e+00 (1.7361e+00)	Acc@1  53.00 ( 51.42)	Acc@5  87.00 ( 82.29)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.6456e+00 (1.7274e+00)	Acc@1  50.00 ( 51.41)	Acc@5  87.00 ( 82.46)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.6344e+00 (1.7430e+00)	Acc@1  57.00 ( 51.73)	Acc@5  83.00 ( 81.96)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.5671e+00 (1.7360e+00)	Acc@1  61.00 ( 52.03)	Acc@5  83.00 ( 82.03)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 2.0168e+00 (1.7513e+00)	Acc@1  48.00 ( 51.73)	Acc@5  78.00 ( 81.89)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 2.0434e+00 (1.7654e+00)	Acc@1  47.00 ( 51.28)	Acc@5  76.00 ( 81.58)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.8344e+00 (1.7575e+00)	Acc@1  48.00 ( 51.52)	Acc@5  85.00 ( 81.85)
 * Acc@1 51.800 Acc@5 81.970
### epoch[15] execution time: 28.52694296836853
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.216 ( 0.216)	Data  0.147 ( 0.147)	Loss 1.6962e+00 (1.6962e+00)	Acc@1  56.25 ( 56.25)	Acc@5  82.03 ( 82.03)
Epoch: [16][ 10/391]	Time  0.067 ( 0.079)	Data  0.001 ( 0.014)	Loss 1.5737e+00 (1.5868e+00)	Acc@1  55.47 ( 55.26)	Acc@5  87.50 ( 85.65)
Epoch: [16][ 20/391]	Time  0.064 ( 0.072)	Data  0.002 ( 0.008)	Loss 1.5287e+00 (1.5763e+00)	Acc@1  56.25 ( 55.73)	Acc@5  84.38 ( 85.12)
Epoch: [16][ 30/391]	Time  0.071 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.5536e+00 (1.5794e+00)	Acc@1  56.25 ( 55.09)	Acc@5  89.06 ( 85.41)
Epoch: [16][ 40/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.6806e+00 (1.5900e+00)	Acc@1  49.22 ( 55.05)	Acc@5  85.94 ( 85.21)
Epoch: [16][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4521e+00 (1.5908e+00)	Acc@1  58.59 ( 54.96)	Acc@5  89.06 ( 85.13)
Epoch: [16][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.8476e+00 (1.6001e+00)	Acc@1  42.97 ( 54.55)	Acc@5  82.03 ( 84.85)
Epoch: [16][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3541e+00 (1.5942e+00)	Acc@1  56.25 ( 54.63)	Acc@5  90.62 ( 85.05)
Epoch: [16][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4730e+00 (1.5966e+00)	Acc@1  56.25 ( 54.62)	Acc@5  87.50 ( 84.89)
Epoch: [16][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.6793e+00 (1.5871e+00)	Acc@1  53.12 ( 54.86)	Acc@5  84.38 ( 85.03)
Epoch: [16][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5043e+00 (1.5899e+00)	Acc@1  59.38 ( 54.79)	Acc@5  87.50 ( 85.08)
Epoch: [16][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2189e+00 (1.5877e+00)	Acc@1  66.41 ( 54.89)	Acc@5  89.84 ( 85.08)
Epoch: [16][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6558e+00 (1.5932e+00)	Acc@1  57.03 ( 54.80)	Acc@5  82.03 ( 84.90)
Epoch: [16][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7393e+00 (1.5983e+00)	Acc@1  54.69 ( 54.74)	Acc@5  79.69 ( 84.77)
Epoch: [16][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3016e+00 (1.5996e+00)	Acc@1  60.94 ( 54.62)	Acc@5  89.06 ( 84.77)
Epoch: [16][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6793e+00 (1.5968e+00)	Acc@1  54.69 ( 54.68)	Acc@5  81.25 ( 84.76)
Epoch: [16][160/391]	Time  0.072 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5591e+00 (1.5990e+00)	Acc@1  55.47 ( 54.63)	Acc@5  85.94 ( 84.70)
Epoch: [16][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7434e+00 (1.5996e+00)	Acc@1  47.66 ( 54.59)	Acc@5  80.47 ( 84.70)
Epoch: [16][180/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4670e+00 (1.6013e+00)	Acc@1  56.25 ( 54.55)	Acc@5  85.94 ( 84.68)
Epoch: [16][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5908e+00 (1.5983e+00)	Acc@1  52.34 ( 54.66)	Acc@5  82.81 ( 84.76)
Epoch: [16][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5551e+00 (1.5982e+00)	Acc@1  64.84 ( 54.64)	Acc@5  82.03 ( 84.79)
Epoch: [16][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4627e+00 (1.5978e+00)	Acc@1  53.12 ( 54.57)	Acc@5  88.28 ( 84.84)
Epoch: [16][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7723e+00 (1.6005e+00)	Acc@1  51.56 ( 54.51)	Acc@5  82.81 ( 84.79)
Epoch: [16][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7514e+00 (1.6025e+00)	Acc@1  53.12 ( 54.46)	Acc@5  81.25 ( 84.76)
Epoch: [16][240/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5222e+00 (1.6023e+00)	Acc@1  59.38 ( 54.42)	Acc@5  89.84 ( 84.77)
Epoch: [16][250/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5616e+00 (1.5991e+00)	Acc@1  56.25 ( 54.50)	Acc@5  82.81 ( 84.83)
Epoch: [16][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6778e+00 (1.5998e+00)	Acc@1  53.12 ( 54.50)	Acc@5  81.25 ( 84.80)
Epoch: [16][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5895e+00 (1.6013e+00)	Acc@1  58.59 ( 54.48)	Acc@5  83.59 ( 84.76)
Epoch: [16][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7301e+00 (1.6006e+00)	Acc@1  53.12 ( 54.57)	Acc@5  85.94 ( 84.79)
Epoch: [16][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5975e+00 (1.6010e+00)	Acc@1  53.12 ( 54.51)	Acc@5  82.81 ( 84.78)
Epoch: [16][300/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5446e+00 (1.6032e+00)	Acc@1  57.81 ( 54.41)	Acc@5  88.28 ( 84.78)
Epoch: [16][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7077e+00 (1.6075e+00)	Acc@1  49.22 ( 54.32)	Acc@5  83.59 ( 84.68)
Epoch: [16][320/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6146e+00 (1.6061e+00)	Acc@1  55.47 ( 54.33)	Acc@5  85.16 ( 84.74)
Epoch: [16][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5446e+00 (1.6026e+00)	Acc@1  49.22 ( 54.39)	Acc@5  84.38 ( 84.82)
Epoch: [16][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3103e+00 (1.6030e+00)	Acc@1  64.06 ( 54.33)	Acc@5  89.06 ( 84.82)
Epoch: [16][350/391]	Time  0.073 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4972e+00 (1.6028e+00)	Acc@1  55.47 ( 54.36)	Acc@5  88.28 ( 84.80)
Epoch: [16][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8091e+00 (1.6068e+00)	Acc@1  44.53 ( 54.30)	Acc@5  83.59 ( 84.75)
Epoch: [16][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4682e+00 (1.6093e+00)	Acc@1  54.69 ( 54.19)	Acc@5  89.06 ( 84.70)
Epoch: [16][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6165e+00 (1.6112e+00)	Acc@1  57.81 ( 54.16)	Acc@5  82.81 ( 84.67)
Epoch: [16][390/391]	Time  0.052 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8929e+00 (1.6112e+00)	Acc@1  46.25 ( 54.16)	Acc@5  78.75 ( 84.69)
## e[16] optimizer.zero_grad (sum) time: 0.4127538204193115
## e[16]       loss.backward (sum) time: 7.002708673477173
## e[16]      optimizer.step (sum) time: 3.3923001289367676
## epoch[16] training(only) time: 25.532267570495605
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 1.8106e+00 (1.8106e+00)	Acc@1  55.00 ( 55.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.025 ( 0.041)	Loss 1.6888e+00 (1.7846e+00)	Acc@1  45.00 ( 50.09)	Acc@5  87.00 ( 82.82)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 1.5968e+00 (1.7632e+00)	Acc@1  54.00 ( 51.00)	Acc@5  86.00 ( 83.24)
Test: [ 30/100]	Time  0.029 ( 0.032)	Loss 1.8478e+00 (1.7506e+00)	Acc@1  50.00 ( 51.13)	Acc@5  83.00 ( 83.42)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.9089e+00 (1.7572e+00)	Acc@1  49.00 ( 50.51)	Acc@5  83.00 ( 83.37)
Test: [ 50/100]	Time  0.027 ( 0.030)	Loss 1.6450e+00 (1.7671e+00)	Acc@1  50.00 ( 50.25)	Acc@5  82.00 ( 82.86)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.6290e+00 (1.7514e+00)	Acc@1  52.00 ( 50.56)	Acc@5  83.00 ( 82.90)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.7811e+00 (1.7524e+00)	Acc@1  57.00 ( 50.68)	Acc@5  83.00 ( 82.75)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 2.0081e+00 (1.7691e+00)	Acc@1  48.00 ( 50.41)	Acc@5  77.00 ( 82.36)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0321e+00 (1.7597e+00)	Acc@1  47.00 ( 50.68)	Acc@5  77.00 ( 82.37)
 * Acc@1 50.750 Acc@5 82.580
### epoch[16] execution time: 28.39055299758911
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.223 ( 0.223)	Data  0.153 ( 0.153)	Loss 1.6229e+00 (1.6229e+00)	Acc@1  53.91 ( 53.91)	Acc@5  85.16 ( 85.16)
Epoch: [17][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.015)	Loss 1.5120e+00 (1.4491e+00)	Acc@1  57.81 ( 58.24)	Acc@5  85.16 ( 86.86)
Epoch: [17][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.5432e+00 (1.4572e+00)	Acc@1  56.25 ( 57.78)	Acc@5  89.06 ( 87.28)
Epoch: [17][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.6129e+00 (1.4827e+00)	Acc@1  55.47 ( 57.26)	Acc@5  83.59 ( 86.84)
Epoch: [17][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.3634e+00 (1.5080e+00)	Acc@1  59.38 ( 56.86)	Acc@5  93.75 ( 86.38)
Epoch: [17][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3952e+00 (1.5079e+00)	Acc@1  62.50 ( 57.15)	Acc@5  86.72 ( 86.31)
Epoch: [17][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5296e+00 (1.5232e+00)	Acc@1  56.25 ( 56.54)	Acc@5  82.81 ( 86.04)
Epoch: [17][ 70/391]	Time  0.073 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4732e+00 (1.5222e+00)	Acc@1  60.16 ( 56.67)	Acc@5  84.38 ( 86.09)
Epoch: [17][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.7522e+00 (1.5237e+00)	Acc@1  53.12 ( 56.59)	Acc@5  82.81 ( 86.12)
Epoch: [17][ 90/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5434e+00 (1.5306e+00)	Acc@1  60.94 ( 56.40)	Acc@5  85.16 ( 86.13)
Epoch: [17][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6406e+00 (1.5371e+00)	Acc@1  50.78 ( 56.28)	Acc@5  82.03 ( 86.00)
Epoch: [17][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5199e+00 (1.5391e+00)	Acc@1  58.59 ( 56.31)	Acc@5  85.16 ( 85.92)
Epoch: [17][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5285e+00 (1.5414e+00)	Acc@1  53.12 ( 56.27)	Acc@5  87.50 ( 85.89)
Epoch: [17][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5190e+00 (1.5418e+00)	Acc@1  53.12 ( 56.09)	Acc@5  86.72 ( 85.90)
Epoch: [17][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3375e+00 (1.5470e+00)	Acc@1  55.47 ( 55.90)	Acc@5  92.19 ( 85.77)
Epoch: [17][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6117e+00 (1.5536e+00)	Acc@1  57.03 ( 55.75)	Acc@5  82.03 ( 85.64)
Epoch: [17][160/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3556e+00 (1.5581e+00)	Acc@1  57.03 ( 55.56)	Acc@5  89.84 ( 85.53)
Epoch: [17][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3410e+00 (1.5595e+00)	Acc@1  62.50 ( 55.50)	Acc@5  83.59 ( 85.54)
Epoch: [17][180/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5078e+00 (1.5583e+00)	Acc@1  53.91 ( 55.48)	Acc@5  85.16 ( 85.61)
Epoch: [17][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4280e+00 (1.5549e+00)	Acc@1  59.38 ( 55.53)	Acc@5  87.50 ( 85.64)
Epoch: [17][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8439e+00 (1.5559e+00)	Acc@1  52.34 ( 55.57)	Acc@5  81.25 ( 85.67)
Epoch: [17][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2700e+00 (1.5559e+00)	Acc@1  61.72 ( 55.52)	Acc@5  87.50 ( 85.65)
Epoch: [17][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4835e+00 (1.5605e+00)	Acc@1  55.47 ( 55.42)	Acc@5  89.06 ( 85.52)
Epoch: [17][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6969e+00 (1.5612e+00)	Acc@1  51.56 ( 55.42)	Acc@5  86.72 ( 85.54)
Epoch: [17][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6997e+00 (1.5597e+00)	Acc@1  49.22 ( 55.40)	Acc@5  83.59 ( 85.57)
Epoch: [17][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3868e+00 (1.5603e+00)	Acc@1  60.94 ( 55.40)	Acc@5  89.06 ( 85.59)
Epoch: [17][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8050e+00 (1.5632e+00)	Acc@1  48.44 ( 55.28)	Acc@5  82.81 ( 85.55)
Epoch: [17][270/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3770e+00 (1.5627e+00)	Acc@1  64.06 ( 55.34)	Acc@5  87.50 ( 85.53)
Epoch: [17][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5110e+00 (1.5610e+00)	Acc@1  51.56 ( 55.37)	Acc@5  88.28 ( 85.58)
Epoch: [17][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6271e+00 (1.5637e+00)	Acc@1  56.25 ( 55.29)	Acc@5  85.94 ( 85.54)
Epoch: [17][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8771e+00 (1.5641e+00)	Acc@1  47.66 ( 55.33)	Acc@5  77.34 ( 85.50)
Epoch: [17][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5168e+00 (1.5644e+00)	Acc@1  53.91 ( 55.36)	Acc@5  87.50 ( 85.50)
Epoch: [17][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7141e+00 (1.5668e+00)	Acc@1  50.78 ( 55.28)	Acc@5  83.59 ( 85.48)
Epoch: [17][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7153e+00 (1.5690e+00)	Acc@1  54.69 ( 55.24)	Acc@5  82.81 ( 85.44)
Epoch: [17][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5817e+00 (1.5675e+00)	Acc@1  57.81 ( 55.27)	Acc@5  85.94 ( 85.48)
Epoch: [17][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5421e+00 (1.5677e+00)	Acc@1  55.47 ( 55.25)	Acc@5  85.94 ( 85.46)
Epoch: [17][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3172e+00 (1.5683e+00)	Acc@1  60.16 ( 55.22)	Acc@5  89.84 ( 85.46)
Epoch: [17][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.8500e+00 (1.5700e+00)	Acc@1  44.53 ( 55.19)	Acc@5  82.03 ( 85.41)
Epoch: [17][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3935e+00 (1.5717e+00)	Acc@1  60.16 ( 55.16)	Acc@5  89.84 ( 85.41)
Epoch: [17][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5977e+00 (1.5706e+00)	Acc@1  55.00 ( 55.20)	Acc@5  86.25 ( 85.43)
## e[17] optimizer.zero_grad (sum) time: 0.4084744453430176
## e[17]       loss.backward (sum) time: 6.939913034439087
## e[17]      optimizer.step (sum) time: 3.4616689682006836
## epoch[17] training(only) time: 25.561426162719727
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.7060e+00 (1.7060e+00)	Acc@1  53.00 ( 53.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.8793e+00 (1.6855e+00)	Acc@1  47.00 ( 54.82)	Acc@5  81.00 ( 83.55)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.5358e+00 (1.7001e+00)	Acc@1  54.00 ( 53.33)	Acc@5  85.00 ( 83.81)
Test: [ 30/100]	Time  0.030 ( 0.031)	Loss 1.6255e+00 (1.7040e+00)	Acc@1  50.00 ( 53.06)	Acc@5  91.00 ( 83.81)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.7706e+00 (1.6957e+00)	Acc@1  51.00 ( 52.68)	Acc@5  83.00 ( 83.76)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.5968e+00 (1.7104e+00)	Acc@1  54.00 ( 52.37)	Acc@5  80.00 ( 83.10)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 1.6806e+00 (1.7010e+00)	Acc@1  56.00 ( 52.39)	Acc@5  78.00 ( 83.02)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.6926e+00 (1.7183e+00)	Acc@1  54.00 ( 52.27)	Acc@5  84.00 ( 82.97)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.7765e+00 (1.7321e+00)	Acc@1  58.00 ( 52.00)	Acc@5  80.00 ( 82.80)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0308e+00 (1.7279e+00)	Acc@1  47.00 ( 52.16)	Acc@5  79.00 ( 82.79)
 * Acc@1 52.260 Acc@5 82.790
### epoch[17] execution time: 28.405068397521973
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.222 ( 0.222)	Data  0.152 ( 0.152)	Loss 1.6910e+00 (1.6910e+00)	Acc@1  46.88 ( 46.88)	Acc@5  79.69 ( 79.69)
Epoch: [18][ 10/391]	Time  0.063 ( 0.079)	Data  0.001 ( 0.015)	Loss 1.2886e+00 (1.5822e+00)	Acc@1  64.06 ( 55.54)	Acc@5  89.84 ( 84.66)
Epoch: [18][ 20/391]	Time  0.067 ( 0.072)	Data  0.001 ( 0.008)	Loss 1.5315e+00 (1.5707e+00)	Acc@1  55.47 ( 55.28)	Acc@5  87.50 ( 85.08)
Epoch: [18][ 30/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.5618e+00 (1.5540e+00)	Acc@1  55.47 ( 56.02)	Acc@5  89.06 ( 85.89)
Epoch: [18][ 40/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.6371e+00 (1.5422e+00)	Acc@1  51.56 ( 56.48)	Acc@5  82.81 ( 86.03)
Epoch: [18][ 50/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.7394e+00 (1.5483e+00)	Acc@1  58.59 ( 55.84)	Acc@5  80.47 ( 85.89)
Epoch: [18][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4124e+00 (1.5460e+00)	Acc@1  59.38 ( 55.96)	Acc@5  89.06 ( 85.82)
Epoch: [18][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.0464e+00 (1.5524e+00)	Acc@1  49.22 ( 55.86)	Acc@5  74.22 ( 85.85)
Epoch: [18][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5289e+00 (1.5526e+00)	Acc@1  57.03 ( 55.94)	Acc@5  84.38 ( 85.78)
Epoch: [18][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5311e+00 (1.5478e+00)	Acc@1  54.69 ( 56.04)	Acc@5  85.94 ( 85.79)
Epoch: [18][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5036e+00 (1.5375e+00)	Acc@1  58.59 ( 56.30)	Acc@5  82.81 ( 85.86)
Epoch: [18][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3665e+00 (1.5392e+00)	Acc@1  57.03 ( 56.07)	Acc@5  88.28 ( 85.75)
Epoch: [18][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2981e+00 (1.5380e+00)	Acc@1  53.12 ( 56.06)	Acc@5  89.84 ( 85.81)
Epoch: [18][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3282e+00 (1.5346e+00)	Acc@1  60.16 ( 56.15)	Acc@5  85.16 ( 85.78)
Epoch: [18][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4661e+00 (1.5314e+00)	Acc@1  60.16 ( 56.41)	Acc@5  85.94 ( 85.74)
Epoch: [18][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4406e+00 (1.5289e+00)	Acc@1  60.16 ( 56.52)	Acc@5  85.16 ( 85.73)
Epoch: [18][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6557e+00 (1.5309e+00)	Acc@1  56.25 ( 56.45)	Acc@5  84.38 ( 85.83)
Epoch: [18][170/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2982e+00 (1.5294e+00)	Acc@1  62.50 ( 56.50)	Acc@5  93.75 ( 85.88)
Epoch: [18][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6230e+00 (1.5319e+00)	Acc@1  48.44 ( 56.37)	Acc@5  83.59 ( 85.79)
Epoch: [18][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6388e+00 (1.5353e+00)	Acc@1  52.34 ( 56.28)	Acc@5  84.38 ( 85.73)
Epoch: [18][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5333e+00 (1.5349e+00)	Acc@1  49.22 ( 56.18)	Acc@5  85.16 ( 85.79)
Epoch: [18][210/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6001e+00 (1.5358e+00)	Acc@1  53.91 ( 56.08)	Acc@5  85.16 ( 85.83)
Epoch: [18][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7777e+00 (1.5340e+00)	Acc@1  53.12 ( 56.16)	Acc@5  81.25 ( 85.86)
Epoch: [18][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3135e+00 (1.5314e+00)	Acc@1  65.62 ( 56.23)	Acc@5  89.06 ( 85.87)
Epoch: [18][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8574e+00 (1.5333e+00)	Acc@1  49.22 ( 56.24)	Acc@5  81.25 ( 85.87)
Epoch: [18][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5513e+00 (1.5327e+00)	Acc@1  59.38 ( 56.31)	Acc@5  82.03 ( 85.87)
Epoch: [18][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3079e+00 (1.5326e+00)	Acc@1  63.28 ( 56.27)	Acc@5  89.06 ( 85.86)
Epoch: [18][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2773e+00 (1.5297e+00)	Acc@1  67.19 ( 56.34)	Acc@5  90.62 ( 85.91)
Epoch: [18][280/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6917e+00 (1.5306e+00)	Acc@1  56.25 ( 56.31)	Acc@5  82.03 ( 85.92)
Epoch: [18][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8723e+00 (1.5311e+00)	Acc@1  44.53 ( 56.25)	Acc@5  84.38 ( 85.94)
Epoch: [18][300/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6147e+00 (1.5325e+00)	Acc@1  53.91 ( 56.21)	Acc@5  89.84 ( 85.90)
Epoch: [18][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7817e+00 (1.5341e+00)	Acc@1  50.78 ( 56.18)	Acc@5  82.03 ( 85.90)
Epoch: [18][320/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6638e+00 (1.5346e+00)	Acc@1  53.91 ( 56.19)	Acc@5  82.03 ( 85.92)
Epoch: [18][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5610e+00 (1.5374e+00)	Acc@1  58.59 ( 56.11)	Acc@5  87.50 ( 85.89)
Epoch: [18][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6828e+00 (1.5377e+00)	Acc@1  52.34 ( 56.09)	Acc@5  82.81 ( 85.88)
Epoch: [18][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5849e+00 (1.5372e+00)	Acc@1  53.12 ( 56.08)	Acc@5  87.50 ( 85.87)
Epoch: [18][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2507e+00 (1.5370e+00)	Acc@1  67.97 ( 56.11)	Acc@5  89.84 ( 85.86)
Epoch: [18][370/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4799e+00 (1.5391e+00)	Acc@1  57.03 ( 56.02)	Acc@5  89.06 ( 85.84)
Epoch: [18][380/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6070e+00 (1.5376e+00)	Acc@1  58.59 ( 56.08)	Acc@5  85.16 ( 85.87)
Epoch: [18][390/391]	Time  0.057 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7824e+00 (1.5378e+00)	Acc@1  51.25 ( 56.10)	Acc@5  78.75 ( 85.86)
## e[18] optimizer.zero_grad (sum) time: 0.41158509254455566
## e[18]       loss.backward (sum) time: 7.001440763473511
## e[18]      optimizer.step (sum) time: 3.4499869346618652
## epoch[18] training(only) time: 25.654090881347656
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.8822e+00 (1.8822e+00)	Acc@1  51.00 ( 51.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.6988e+00 (1.7267e+00)	Acc@1  55.00 ( 52.00)	Acc@5  84.00 ( 83.09)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.8042e+00 (1.7170e+00)	Acc@1  47.00 ( 51.95)	Acc@5  83.00 ( 83.14)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.6191e+00 (1.7070e+00)	Acc@1  49.00 ( 52.32)	Acc@5  87.00 ( 83.19)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.6501e+00 (1.6990e+00)	Acc@1  50.00 ( 52.32)	Acc@5  81.00 ( 83.37)
Test: [ 50/100]	Time  0.029 ( 0.030)	Loss 1.7001e+00 (1.7150e+00)	Acc@1  52.00 ( 51.98)	Acc@5  81.00 ( 83.22)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 1.7230e+00 (1.7077e+00)	Acc@1  47.00 ( 52.00)	Acc@5  81.00 ( 83.16)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.7746e+00 (1.7071e+00)	Acc@1  51.00 ( 51.83)	Acc@5  82.00 ( 83.07)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.6400e+00 (1.7161e+00)	Acc@1  54.00 ( 51.62)	Acc@5  85.00 ( 82.89)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.1457e+00 (1.7148e+00)	Acc@1  51.00 ( 52.03)	Acc@5  74.00 ( 82.95)
 * Acc@1 52.060 Acc@5 82.890
### epoch[18] execution time: 28.49482011795044
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.219 ( 0.219)	Data  0.151 ( 0.151)	Loss 1.4138e+00 (1.4138e+00)	Acc@1  60.16 ( 60.16)	Acc@5  88.28 ( 88.28)
Epoch: [19][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.015)	Loss 1.4614e+00 (1.5235e+00)	Acc@1  57.81 ( 55.68)	Acc@5  87.50 ( 86.22)
Epoch: [19][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.3122e+00 (1.4839e+00)	Acc@1  62.50 ( 57.07)	Acc@5  89.84 ( 86.79)
Epoch: [19][ 30/391]	Time  0.061 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.4010e+00 (1.4756e+00)	Acc@1  60.16 ( 57.28)	Acc@5  88.28 ( 87.05)
Epoch: [19][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.5720e+00 (1.4602e+00)	Acc@1  54.69 ( 57.91)	Acc@5  84.38 ( 87.35)
Epoch: [19][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5127e+00 (1.4736e+00)	Acc@1  54.69 ( 57.66)	Acc@5  85.16 ( 87.07)
Epoch: [19][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3896e+00 (1.4705e+00)	Acc@1  60.16 ( 57.79)	Acc@5  85.94 ( 87.04)
Epoch: [19][ 70/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5570e+00 (1.4741e+00)	Acc@1  53.91 ( 57.59)	Acc@5  86.72 ( 87.06)
Epoch: [19][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5555e+00 (1.4720e+00)	Acc@1  53.91 ( 57.64)	Acc@5  87.50 ( 86.95)
Epoch: [19][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5839e+00 (1.4685e+00)	Acc@1  51.56 ( 57.71)	Acc@5  87.50 ( 87.01)
Epoch: [19][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6549e+00 (1.4730e+00)	Acc@1  51.56 ( 57.70)	Acc@5  86.72 ( 86.90)
Epoch: [19][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.5809e+00 (1.4782e+00)	Acc@1  50.78 ( 57.53)	Acc@5  87.50 ( 86.83)
Epoch: [19][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4747e+00 (1.4769e+00)	Acc@1  59.38 ( 57.61)	Acc@5  84.38 ( 86.82)
Epoch: [19][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4408e+00 (1.4813e+00)	Acc@1  57.81 ( 57.61)	Acc@5  85.16 ( 86.67)
Epoch: [19][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7488e+00 (1.4834e+00)	Acc@1  51.56 ( 57.54)	Acc@5  84.38 ( 86.69)
Epoch: [19][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7300e+00 (1.4894e+00)	Acc@1  51.56 ( 57.40)	Acc@5  84.38 ( 86.67)
Epoch: [19][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6464e+00 (1.4865e+00)	Acc@1  52.34 ( 57.48)	Acc@5  80.47 ( 86.74)
Epoch: [19][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7159e+00 (1.4848e+00)	Acc@1  50.00 ( 57.55)	Acc@5  83.59 ( 86.80)
Epoch: [19][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7368e+00 (1.4877e+00)	Acc@1  50.78 ( 57.42)	Acc@5  82.81 ( 86.82)
Epoch: [19][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4866e+00 (1.4878e+00)	Acc@1  58.59 ( 57.43)	Acc@5  86.72 ( 86.85)
Epoch: [19][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7449e+00 (1.4880e+00)	Acc@1  49.22 ( 57.44)	Acc@5  86.72 ( 86.84)
Epoch: [19][210/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5227e+00 (1.4897e+00)	Acc@1  57.03 ( 57.37)	Acc@5  87.50 ( 86.80)
Epoch: [19][220/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4364e+00 (1.4960e+00)	Acc@1  59.38 ( 57.13)	Acc@5  88.28 ( 86.75)
Epoch: [19][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4755e+00 (1.4994e+00)	Acc@1  56.25 ( 57.08)	Acc@5  83.59 ( 86.67)
Epoch: [19][240/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5634e+00 (1.5011e+00)	Acc@1  55.47 ( 57.02)	Acc@5  82.81 ( 86.64)
Epoch: [19][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2814e+00 (1.4985e+00)	Acc@1  64.06 ( 57.11)	Acc@5  93.75 ( 86.70)
Epoch: [19][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6207e+00 (1.4981e+00)	Acc@1  57.81 ( 57.13)	Acc@5  82.81 ( 86.72)
Epoch: [19][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4514e+00 (1.5010e+00)	Acc@1  58.59 ( 57.09)	Acc@5  86.72 ( 86.69)
Epoch: [19][280/391]	Time  0.076 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6035e+00 (1.5027e+00)	Acc@1  57.03 ( 57.13)	Acc@5  85.94 ( 86.59)
Epoch: [19][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6157e+00 (1.5038e+00)	Acc@1  52.34 ( 57.11)	Acc@5  88.28 ( 86.58)
Epoch: [19][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3820e+00 (1.5026e+00)	Acc@1  63.28 ( 57.18)	Acc@5  85.94 ( 86.61)
Epoch: [19][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4024e+00 (1.5022e+00)	Acc@1  57.81 ( 57.21)	Acc@5  85.16 ( 86.61)
Epoch: [19][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5533e+00 (1.5031e+00)	Acc@1  57.03 ( 57.17)	Acc@5  87.50 ( 86.58)
Epoch: [19][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5285e+00 (1.5036e+00)	Acc@1  56.25 ( 57.13)	Acc@5  85.16 ( 86.60)
Epoch: [19][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5576e+00 (1.5070e+00)	Acc@1  53.91 ( 57.02)	Acc@5  86.72 ( 86.54)
Epoch: [19][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4169e+00 (1.5059e+00)	Acc@1  53.12 ( 57.02)	Acc@5  86.72 ( 86.53)
Epoch: [19][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5882e+00 (1.5053e+00)	Acc@1  55.47 ( 57.01)	Acc@5  85.16 ( 86.53)
Epoch: [19][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4779e+00 (1.5068e+00)	Acc@1  53.12 ( 56.99)	Acc@5  83.59 ( 86.47)
Epoch: [19][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7361e+00 (1.5067e+00)	Acc@1  54.69 ( 56.98)	Acc@5  81.25 ( 86.44)
Epoch: [19][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6835e+00 (1.5075e+00)	Acc@1  55.00 ( 56.96)	Acc@5  81.25 ( 86.42)
## e[19] optimizer.zero_grad (sum) time: 0.4060189723968506
## e[19]       loss.backward (sum) time: 7.006718635559082
## e[19]      optimizer.step (sum) time: 3.4656410217285156
## epoch[19] training(only) time: 25.615859746932983
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.9559e+00 (1.9559e+00)	Acc@1  51.00 ( 51.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.034 ( 0.039)	Loss 1.8633e+00 (1.6780e+00)	Acc@1  52.00 ( 55.09)	Acc@5  80.00 ( 83.45)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.4233e+00 (1.6627e+00)	Acc@1  64.00 ( 54.95)	Acc@5  90.00 ( 83.76)
Test: [ 30/100]	Time  0.029 ( 0.031)	Loss 1.8185e+00 (1.6646e+00)	Acc@1  52.00 ( 54.94)	Acc@5  79.00 ( 83.35)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.5491e+00 (1.6613e+00)	Acc@1  54.00 ( 54.73)	Acc@5  83.00 ( 83.61)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.6123e+00 (1.6707e+00)	Acc@1  54.00 ( 54.33)	Acc@5  82.00 ( 83.29)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.7424e+00 (1.6678e+00)	Acc@1  50.00 ( 54.03)	Acc@5  80.00 ( 83.59)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.5958e+00 (1.6779e+00)	Acc@1  50.00 ( 53.56)	Acc@5  85.00 ( 83.41)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.6608e+00 (1.6850e+00)	Acc@1  52.00 ( 53.41)	Acc@5  84.00 ( 83.17)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.9697e+00 (1.6789e+00)	Acc@1  50.00 ( 53.54)	Acc@5  79.00 ( 83.22)
 * Acc@1 53.580 Acc@5 83.310
### epoch[19] execution time: 28.471542358398438
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.210 ( 0.210)	Data  0.143 ( 0.143)	Loss 1.2007e+00 (1.2007e+00)	Acc@1  64.84 ( 64.84)	Acc@5  94.53 ( 94.53)
Epoch: [20][ 10/391]	Time  0.065 ( 0.078)	Data  0.001 ( 0.014)	Loss 1.3646e+00 (1.4250e+00)	Acc@1  55.47 ( 58.88)	Acc@5  89.84 ( 88.57)
Epoch: [20][ 20/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.008)	Loss 1.5930e+00 (1.4248e+00)	Acc@1  53.12 ( 58.41)	Acc@5  85.16 ( 87.72)
Epoch: [20][ 30/391]	Time  0.065 ( 0.069)	Data  0.002 ( 0.006)	Loss 1.3621e+00 (1.4109e+00)	Acc@1  59.38 ( 59.30)	Acc@5  88.28 ( 88.23)
Epoch: [20][ 40/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.4517e+00 (1.4411e+00)	Acc@1  58.59 ( 58.73)	Acc@5  83.59 ( 87.63)
Epoch: [20][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 1.5650e+00 (1.4505e+00)	Acc@1  51.56 ( 58.39)	Acc@5  83.59 ( 87.16)
Epoch: [20][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3416e+00 (1.4465e+00)	Acc@1  60.16 ( 58.52)	Acc@5  88.28 ( 87.27)
Epoch: [20][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5559e+00 (1.4580e+00)	Acc@1  60.94 ( 58.44)	Acc@5  85.16 ( 87.09)
Epoch: [20][ 80/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.2426e+00 (1.4580e+00)	Acc@1  63.28 ( 58.25)	Acc@5  91.41 ( 87.19)
Epoch: [20][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.4640e+00 (1.4557e+00)	Acc@1  58.59 ( 58.19)	Acc@5  85.16 ( 87.19)
Epoch: [20][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3633e+00 (1.4555e+00)	Acc@1  60.16 ( 58.31)	Acc@5  90.62 ( 87.25)
Epoch: [20][110/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1764e+00 (1.4506e+00)	Acc@1  66.41 ( 58.53)	Acc@5  91.41 ( 87.25)
Epoch: [20][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4928e+00 (1.4484e+00)	Acc@1  53.12 ( 58.59)	Acc@5  87.50 ( 87.31)
Epoch: [20][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5739e+00 (1.4466e+00)	Acc@1  56.25 ( 58.56)	Acc@5  83.59 ( 87.37)
Epoch: [20][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3685e+00 (1.4480e+00)	Acc@1  59.38 ( 58.49)	Acc@5  92.97 ( 87.40)
Epoch: [20][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5667e+00 (1.4536e+00)	Acc@1  58.59 ( 58.31)	Acc@5  85.16 ( 87.30)
Epoch: [20][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5152e+00 (1.4551e+00)	Acc@1  53.91 ( 58.30)	Acc@5  86.72 ( 87.31)
Epoch: [20][170/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4580e+00 (1.4571e+00)	Acc@1  58.59 ( 58.15)	Acc@5  89.84 ( 87.34)
Epoch: [20][180/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5478e+00 (1.4597e+00)	Acc@1  57.81 ( 58.04)	Acc@5  84.38 ( 87.31)
Epoch: [20][190/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7725e+00 (1.4584e+00)	Acc@1  48.44 ( 58.05)	Acc@5  84.38 ( 87.32)
Epoch: [20][200/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5937e+00 (1.4596e+00)	Acc@1  58.59 ( 57.94)	Acc@5  83.59 ( 87.34)
Epoch: [20][210/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4826e+00 (1.4624e+00)	Acc@1  60.94 ( 57.86)	Acc@5  85.94 ( 87.24)
Epoch: [20][220/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6062e+00 (1.4643e+00)	Acc@1  53.91 ( 57.78)	Acc@5  82.03 ( 87.19)
Epoch: [20][230/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5398e+00 (1.4637e+00)	Acc@1  51.56 ( 57.84)	Acc@5  87.50 ( 87.18)
Epoch: [20][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3481e+00 (1.4661e+00)	Acc@1  57.81 ( 57.74)	Acc@5  89.84 ( 87.13)
Epoch: [20][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7061e+00 (1.4699e+00)	Acc@1  48.44 ( 57.60)	Acc@5  85.16 ( 87.04)
Epoch: [20][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5089e+00 (1.4705e+00)	Acc@1  57.81 ( 57.58)	Acc@5  82.81 ( 86.97)
Epoch: [20][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5855e+00 (1.4724e+00)	Acc@1  56.25 ( 57.50)	Acc@5  81.25 ( 86.93)
Epoch: [20][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5116e+00 (1.4750e+00)	Acc@1  57.81 ( 57.42)	Acc@5  85.94 ( 86.88)
Epoch: [20][290/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5191e+00 (1.4772e+00)	Acc@1  56.25 ( 57.35)	Acc@5  85.16 ( 86.85)
Epoch: [20][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5040e+00 (1.4792e+00)	Acc@1  57.81 ( 57.33)	Acc@5  87.50 ( 86.86)
Epoch: [20][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3901e+00 (1.4768e+00)	Acc@1  57.03 ( 57.43)	Acc@5  90.62 ( 86.89)
Epoch: [20][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6772e+00 (1.4783e+00)	Acc@1  48.44 ( 57.40)	Acc@5  84.38 ( 86.86)
Epoch: [20][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4973e+00 (1.4784e+00)	Acc@1  59.38 ( 57.43)	Acc@5  82.81 ( 86.81)
Epoch: [20][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3738e+00 (1.4779e+00)	Acc@1  58.59 ( 57.43)	Acc@5  87.50 ( 86.79)
Epoch: [20][350/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4747e+00 (1.4766e+00)	Acc@1  56.25 ( 57.53)	Acc@5  89.84 ( 86.81)
Epoch: [20][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4014e+00 (1.4761e+00)	Acc@1  58.59 ( 57.49)	Acc@5  86.72 ( 86.84)
Epoch: [20][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6419e+00 (1.4775e+00)	Acc@1  55.47 ( 57.46)	Acc@5  85.16 ( 86.82)
Epoch: [20][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.6767e+00 (1.4785e+00)	Acc@1  50.78 ( 57.46)	Acc@5  82.03 ( 86.80)
Epoch: [20][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.2856e+00 (1.4778e+00)	Acc@1  61.25 ( 57.53)	Acc@5  91.25 ( 86.80)
## e[20] optimizer.zero_grad (sum) time: 0.411557674407959
## e[20]       loss.backward (sum) time: 6.9717793464660645
## e[20]      optimizer.step (sum) time: 3.3613901138305664
## epoch[20] training(only) time: 25.453885316848755
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.7020e+00 (1.7020e+00)	Acc@1  51.00 ( 51.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.6613e+00 (1.6138e+00)	Acc@1  55.00 ( 55.00)	Acc@5  85.00 ( 84.45)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.5183e+00 (1.5968e+00)	Acc@1  56.00 ( 55.33)	Acc@5  87.00 ( 85.29)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 1.9262e+00 (1.6189e+00)	Acc@1  43.00 ( 54.03)	Acc@5  83.00 ( 84.55)
Test: [ 40/100]	Time  0.029 ( 0.030)	Loss 1.6330e+00 (1.6203e+00)	Acc@1  57.00 ( 54.15)	Acc@5  81.00 ( 84.27)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.5534e+00 (1.6259e+00)	Acc@1  53.00 ( 54.08)	Acc@5  85.00 ( 84.00)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.7021e+00 (1.6268e+00)	Acc@1  56.00 ( 54.02)	Acc@5  84.00 ( 83.97)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.5871e+00 (1.6323e+00)	Acc@1  60.00 ( 53.90)	Acc@5  84.00 ( 83.85)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.7172e+00 (1.6473e+00)	Acc@1  58.00 ( 53.79)	Acc@5  81.00 ( 83.51)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8775e+00 (1.6418e+00)	Acc@1  50.00 ( 54.05)	Acc@5  82.00 ( 83.71)
 * Acc@1 54.140 Acc@5 83.770
### epoch[20] execution time: 28.28494381904602
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.218 ( 0.218)	Data  0.146 ( 0.146)	Loss 1.3341e+00 (1.3341e+00)	Acc@1  61.72 ( 61.72)	Acc@5  92.97 ( 92.97)
Epoch: [21][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 1.3592e+00 (1.3496e+00)	Acc@1  63.28 ( 62.64)	Acc@5  87.50 ( 89.35)
Epoch: [21][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 1.2393e+00 (1.3494e+00)	Acc@1  64.06 ( 61.90)	Acc@5  88.28 ( 89.43)
Epoch: [21][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.3616e+00 (1.3719e+00)	Acc@1  65.62 ( 61.44)	Acc@5  87.50 ( 88.61)
Epoch: [21][ 40/391]	Time  0.071 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.2546e+00 (1.3874e+00)	Acc@1  63.28 ( 60.71)	Acc@5  90.62 ( 88.17)
Epoch: [21][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4193e+00 (1.4025e+00)	Acc@1  62.50 ( 60.31)	Acc@5  84.38 ( 87.97)
Epoch: [21][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4822e+00 (1.4094e+00)	Acc@1  57.81 ( 60.21)	Acc@5  83.59 ( 87.85)
Epoch: [21][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2993e+00 (1.4084e+00)	Acc@1  65.62 ( 59.95)	Acc@5  90.62 ( 87.89)
Epoch: [21][ 80/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5176e+00 (1.4141e+00)	Acc@1  54.69 ( 59.82)	Acc@5  87.50 ( 87.71)
Epoch: [21][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3034e+00 (1.4099e+00)	Acc@1  65.62 ( 59.99)	Acc@5  89.06 ( 87.73)
Epoch: [21][100/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2708e+00 (1.4085e+00)	Acc@1  60.16 ( 59.85)	Acc@5  92.19 ( 87.65)
Epoch: [21][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4814e+00 (1.4102e+00)	Acc@1  55.47 ( 59.74)	Acc@5  90.62 ( 87.63)
Epoch: [21][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3976e+00 (1.4150e+00)	Acc@1  62.50 ( 59.62)	Acc@5  88.28 ( 87.70)
Epoch: [21][130/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2590e+00 (1.4190e+00)	Acc@1  65.62 ( 59.58)	Acc@5  90.62 ( 87.67)
Epoch: [21][140/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4207e+00 (1.4186e+00)	Acc@1  60.16 ( 59.61)	Acc@5  85.94 ( 87.68)
Epoch: [21][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4685e+00 (1.4209e+00)	Acc@1  56.25 ( 59.49)	Acc@5  87.50 ( 87.63)
Epoch: [21][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3396e+00 (1.4205e+00)	Acc@1  63.28 ( 59.54)	Acc@5  85.94 ( 87.65)
Epoch: [21][170/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4952e+00 (1.4231e+00)	Acc@1  55.47 ( 59.46)	Acc@5  88.28 ( 87.55)
Epoch: [21][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4060e+00 (1.4228e+00)	Acc@1  60.94 ( 59.46)	Acc@5  86.72 ( 87.56)
Epoch: [21][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4686e+00 (1.4248e+00)	Acc@1  60.94 ( 59.41)	Acc@5  87.50 ( 87.54)
Epoch: [21][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5925e+00 (1.4259e+00)	Acc@1  57.81 ( 59.34)	Acc@5  83.59 ( 87.55)
Epoch: [21][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5470e+00 (1.4306e+00)	Acc@1  56.25 ( 59.23)	Acc@5  84.38 ( 87.50)
Epoch: [21][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6406e+00 (1.4380e+00)	Acc@1  50.00 ( 59.03)	Acc@5  88.28 ( 87.40)
Epoch: [21][230/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4230e+00 (1.4377e+00)	Acc@1  56.25 ( 58.99)	Acc@5  88.28 ( 87.39)
Epoch: [21][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3986e+00 (1.4367e+00)	Acc@1  64.06 ( 59.03)	Acc@5  85.16 ( 87.41)
Epoch: [21][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3795e+00 (1.4375e+00)	Acc@1  60.94 ( 58.99)	Acc@5  92.19 ( 87.44)
Epoch: [21][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4068e+00 (1.4376e+00)	Acc@1  64.84 ( 58.97)	Acc@5  83.59 ( 87.42)
Epoch: [21][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5342e+00 (1.4362e+00)	Acc@1  53.12 ( 58.99)	Acc@5  89.06 ( 87.46)
Epoch: [21][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5058e+00 (1.4394e+00)	Acc@1  60.94 ( 58.97)	Acc@5  84.38 ( 87.38)
Epoch: [21][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3747e+00 (1.4440e+00)	Acc@1  55.47 ( 58.84)	Acc@5  90.62 ( 87.31)
Epoch: [21][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3662e+00 (1.4459e+00)	Acc@1  57.81 ( 58.76)	Acc@5  91.41 ( 87.29)
Epoch: [21][310/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1972e+00 (1.4439e+00)	Acc@1  64.06 ( 58.79)	Acc@5  92.19 ( 87.35)
Epoch: [21][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7585e+00 (1.4460e+00)	Acc@1  54.69 ( 58.77)	Acc@5  81.25 ( 87.32)
Epoch: [21][330/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2925e+00 (1.4471e+00)	Acc@1  60.94 ( 58.74)	Acc@5  92.97 ( 87.31)
Epoch: [21][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5081e+00 (1.4499e+00)	Acc@1  54.69 ( 58.66)	Acc@5  87.50 ( 87.28)
Epoch: [21][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4059e+00 (1.4512e+00)	Acc@1  63.28 ( 58.62)	Acc@5  88.28 ( 87.27)
Epoch: [21][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4382e+00 (1.4505e+00)	Acc@1  57.03 ( 58.62)	Acc@5  88.28 ( 87.26)
Epoch: [21][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2807e+00 (1.4513e+00)	Acc@1  66.41 ( 58.61)	Acc@5  87.50 ( 87.23)
Epoch: [21][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3464e+00 (1.4525e+00)	Acc@1  64.06 ( 58.62)	Acc@5  91.41 ( 87.20)
Epoch: [21][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4191e+00 (1.4521e+00)	Acc@1  60.00 ( 58.58)	Acc@5  86.25 ( 87.21)
## e[21] optimizer.zero_grad (sum) time: 0.40248894691467285
## e[21]       loss.backward (sum) time: 6.942897796630859
## e[21]      optimizer.step (sum) time: 3.450279712677002
## epoch[21] training(only) time: 25.563316345214844
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.8410e+00 (1.8410e+00)	Acc@1  53.00 ( 53.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.7867e+00 (1.7314e+00)	Acc@1  54.00 ( 54.27)	Acc@5  84.00 ( 82.27)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.7742e+00 (1.7035e+00)	Acc@1  53.00 ( 54.05)	Acc@5  77.00 ( 83.05)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.7254e+00 (1.7110e+00)	Acc@1  52.00 ( 53.48)	Acc@5  82.00 ( 83.10)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.6232e+00 (1.6997e+00)	Acc@1  50.00 ( 53.22)	Acc@5  86.00 ( 83.59)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.5754e+00 (1.7099e+00)	Acc@1  55.00 ( 53.18)	Acc@5  85.00 ( 83.49)
Test: [ 60/100]	Time  0.028 ( 0.029)	Loss 1.7869e+00 (1.7006e+00)	Acc@1  48.00 ( 53.38)	Acc@5  80.00 ( 83.57)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.7146e+00 (1.7121e+00)	Acc@1  49.00 ( 52.92)	Acc@5  82.00 ( 83.35)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.7530e+00 (1.7252e+00)	Acc@1  51.00 ( 52.56)	Acc@5  81.00 ( 83.07)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0150e+00 (1.7253e+00)	Acc@1  48.00 ( 52.45)	Acc@5  80.00 ( 83.19)
 * Acc@1 52.500 Acc@5 83.160
### epoch[21] execution time: 28.410671710968018
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.211 ( 0.211)	Data  0.140 ( 0.140)	Loss 1.4210e+00 (1.4210e+00)	Acc@1  57.81 ( 57.81)	Acc@5  88.28 ( 88.28)
Epoch: [22][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.014)	Loss 1.2811e+00 (1.4220e+00)	Acc@1  64.06 ( 59.16)	Acc@5  89.06 ( 88.28)
Epoch: [22][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.4341e+00 (1.4266e+00)	Acc@1  59.38 ( 58.67)	Acc@5  85.94 ( 87.61)
Epoch: [22][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.5017e+00 (1.4251e+00)	Acc@1  53.91 ( 58.74)	Acc@5  88.28 ( 88.00)
Epoch: [22][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.5434e+00 (1.4091e+00)	Acc@1  50.78 ( 58.86)	Acc@5  88.28 ( 88.38)
Epoch: [22][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.5036e+00 (1.3949e+00)	Acc@1  57.03 ( 59.02)	Acc@5  86.72 ( 88.56)
Epoch: [22][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3647e+00 (1.3896e+00)	Acc@1  60.16 ( 59.20)	Acc@5  89.06 ( 88.65)
Epoch: [22][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2533e+00 (1.3835e+00)	Acc@1  61.72 ( 59.33)	Acc@5  95.31 ( 88.92)
Epoch: [22][ 80/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4146e+00 (1.3824e+00)	Acc@1  59.38 ( 59.45)	Acc@5  86.72 ( 88.90)
Epoch: [22][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2174e+00 (1.3815e+00)	Acc@1  58.59 ( 59.38)	Acc@5  94.53 ( 88.98)
Epoch: [22][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4241e+00 (1.3824e+00)	Acc@1  60.16 ( 59.39)	Acc@5  87.50 ( 88.81)
Epoch: [22][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4422e+00 (1.3842e+00)	Acc@1  53.91 ( 59.35)	Acc@5  85.16 ( 88.68)
Epoch: [22][120/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6018e+00 (1.3869e+00)	Acc@1  55.47 ( 59.36)	Acc@5  86.72 ( 88.57)
Epoch: [22][130/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2961e+00 (1.3929e+00)	Acc@1  61.72 ( 59.27)	Acc@5  90.62 ( 88.38)
Epoch: [22][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0727e+00 (1.3955e+00)	Acc@1  67.19 ( 59.25)	Acc@5  90.62 ( 88.34)
Epoch: [22][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4284e+00 (1.3932e+00)	Acc@1  56.25 ( 59.32)	Acc@5  89.84 ( 88.45)
Epoch: [22][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6162e+00 (1.3905e+00)	Acc@1  53.12 ( 59.42)	Acc@5  83.59 ( 88.47)
Epoch: [22][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5343e+00 (1.3910e+00)	Acc@1  53.91 ( 59.40)	Acc@5  90.62 ( 88.46)
Epoch: [22][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2098e+00 (1.3883e+00)	Acc@1  67.19 ( 59.47)	Acc@5  92.19 ( 88.46)
Epoch: [22][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2849e+00 (1.3890e+00)	Acc@1  71.09 ( 59.47)	Acc@5  88.28 ( 88.44)
Epoch: [22][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6446e+00 (1.3936e+00)	Acc@1  56.25 ( 59.43)	Acc@5  86.72 ( 88.34)
Epoch: [22][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5247e+00 (1.3958e+00)	Acc@1  53.12 ( 59.38)	Acc@5  89.84 ( 88.29)
Epoch: [22][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3409e+00 (1.3986e+00)	Acc@1  61.72 ( 59.32)	Acc@5  87.50 ( 88.21)
Epoch: [22][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3042e+00 (1.4031e+00)	Acc@1  59.38 ( 59.16)	Acc@5  90.62 ( 88.13)
Epoch: [22][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5266e+00 (1.4039e+00)	Acc@1  54.69 ( 59.14)	Acc@5  84.38 ( 88.12)
Epoch: [22][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5624e+00 (1.4049e+00)	Acc@1  57.81 ( 59.06)	Acc@5  85.16 ( 88.12)
Epoch: [22][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4411e+00 (1.4072e+00)	Acc@1  62.50 ( 59.00)	Acc@5  85.16 ( 88.06)
Epoch: [22][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6591e+00 (1.4104e+00)	Acc@1  58.59 ( 58.94)	Acc@5  81.25 ( 88.01)
Epoch: [22][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5788e+00 (1.4108e+00)	Acc@1  50.78 ( 58.91)	Acc@5  88.28 ( 87.99)
Epoch: [22][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3848e+00 (1.4109e+00)	Acc@1  62.50 ( 58.92)	Acc@5  89.06 ( 87.98)
Epoch: [22][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5589e+00 (1.4112e+00)	Acc@1  57.03 ( 58.90)	Acc@5  85.94 ( 87.99)
Epoch: [22][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3396e+00 (1.4127e+00)	Acc@1  62.50 ( 58.96)	Acc@5  88.28 ( 87.96)
Epoch: [22][320/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3072e+00 (1.4132e+00)	Acc@1  60.16 ( 58.98)	Acc@5  90.62 ( 87.94)
Epoch: [22][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3725e+00 (1.4157e+00)	Acc@1  58.59 ( 58.88)	Acc@5  89.06 ( 87.88)
Epoch: [22][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4922e+00 (1.4179e+00)	Acc@1  56.25 ( 58.88)	Acc@5  88.28 ( 87.83)
Epoch: [22][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3686e+00 (1.4186e+00)	Acc@1  59.38 ( 58.83)	Acc@5  92.19 ( 87.85)
Epoch: [22][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5673e+00 (1.4199e+00)	Acc@1  53.12 ( 58.78)	Acc@5  85.94 ( 87.81)
Epoch: [22][370/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2367e+00 (1.4201e+00)	Acc@1  67.19 ( 58.78)	Acc@5  91.41 ( 87.81)
Epoch: [22][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5155e+00 (1.4227e+00)	Acc@1  60.16 ( 58.73)	Acc@5  87.50 ( 87.77)
Epoch: [22][390/391]	Time  0.047 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9334e+00 (1.4244e+00)	Acc@1  42.50 ( 58.71)	Acc@5  82.50 ( 87.75)
## e[22] optimizer.zero_grad (sum) time: 0.41385912895202637
## e[22]       loss.backward (sum) time: 7.00922155380249
## e[22]      optimizer.step (sum) time: 3.5714523792266846
## epoch[22] training(only) time: 25.74793577194214
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.7022e+00 (1.7022e+00)	Acc@1  56.00 ( 56.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.7767e+00 (1.6964e+00)	Acc@1  50.00 ( 53.18)	Acc@5  88.00 ( 84.27)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6159e+00 (1.6862e+00)	Acc@1  57.00 ( 53.52)	Acc@5  82.00 ( 83.86)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5977e+00 (1.7094e+00)	Acc@1  57.00 ( 53.03)	Acc@5  85.00 ( 83.84)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.6970e+00 (1.7096e+00)	Acc@1  56.00 ( 52.80)	Acc@5  83.00 ( 83.61)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.6574e+00 (1.7264e+00)	Acc@1  53.00 ( 52.76)	Acc@5  84.00 ( 83.45)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.8307e+00 (1.7209e+00)	Acc@1  52.00 ( 52.95)	Acc@5  79.00 ( 83.31)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.5180e+00 (1.7131e+00)	Acc@1  61.00 ( 53.21)	Acc@5  82.00 ( 83.39)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.7941e+00 (1.7275e+00)	Acc@1  51.00 ( 52.94)	Acc@5  80.00 ( 83.15)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 2.1989e+00 (1.7161e+00)	Acc@1  40.00 ( 53.18)	Acc@5  76.00 ( 83.12)
 * Acc@1 53.120 Acc@5 83.120
### epoch[22] execution time: 28.585389852523804
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.215 ( 0.215)	Data  0.147 ( 0.147)	Loss 1.2884e+00 (1.2884e+00)	Acc@1  60.16 ( 60.16)	Acc@5  91.41 ( 91.41)
Epoch: [23][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.014)	Loss 1.4476e+00 (1.4750e+00)	Acc@1  59.38 ( 58.24)	Acc@5  85.94 ( 86.65)
Epoch: [23][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.008)	Loss 1.2990e+00 (1.4347e+00)	Acc@1  62.50 ( 57.92)	Acc@5  89.06 ( 87.61)
Epoch: [23][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 1.5894e+00 (1.4056e+00)	Acc@1  52.34 ( 58.67)	Acc@5  83.59 ( 87.83)
Epoch: [23][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 1.2704e+00 (1.3999e+00)	Acc@1  63.28 ( 58.82)	Acc@5  92.19 ( 87.96)
Epoch: [23][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.1647e+00 (1.3788e+00)	Acc@1  70.31 ( 59.54)	Acc@5  88.28 ( 88.28)
Epoch: [23][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2354e+00 (1.3609e+00)	Acc@1  62.50 ( 59.93)	Acc@5  86.72 ( 88.54)
Epoch: [23][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1808e+00 (1.3547e+00)	Acc@1  71.88 ( 60.45)	Acc@5  88.28 ( 88.42)
Epoch: [23][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2310e+00 (1.3550e+00)	Acc@1  66.41 ( 60.39)	Acc@5  92.19 ( 88.46)
Epoch: [23][ 90/391]	Time  0.068 ( 0.067)	Data  0.002 ( 0.003)	Loss 1.4224e+00 (1.3529e+00)	Acc@1  57.03 ( 60.31)	Acc@5  88.28 ( 88.58)
Epoch: [23][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3702e+00 (1.3523e+00)	Acc@1  59.38 ( 60.45)	Acc@5  89.06 ( 88.60)
Epoch: [23][110/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6216e+00 (1.3566e+00)	Acc@1  53.12 ( 60.40)	Acc@5  82.03 ( 88.54)
Epoch: [23][120/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4191e+00 (1.3631e+00)	Acc@1  54.69 ( 60.25)	Acc@5  89.06 ( 88.53)
Epoch: [23][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3269e+00 (1.3693e+00)	Acc@1  56.25 ( 60.11)	Acc@5  88.28 ( 88.31)
Epoch: [23][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6387e+00 (1.3730e+00)	Acc@1  53.91 ( 59.98)	Acc@5  85.94 ( 88.28)
Epoch: [23][150/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5161e+00 (1.3755e+00)	Acc@1  58.59 ( 59.94)	Acc@5  85.16 ( 88.22)
Epoch: [23][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.7059e+00 (1.3792e+00)	Acc@1  47.66 ( 59.87)	Acc@5  81.25 ( 88.13)
Epoch: [23][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3875e+00 (1.3795e+00)	Acc@1  60.16 ( 59.90)	Acc@5  88.28 ( 88.10)
Epoch: [23][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.8051e+00 (1.3794e+00)	Acc@1  50.78 ( 59.90)	Acc@5  78.91 ( 88.11)
Epoch: [23][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6918e+00 (1.3796e+00)	Acc@1  49.22 ( 59.88)	Acc@5  82.03 ( 88.08)
Epoch: [23][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5674e+00 (1.3796e+00)	Acc@1  54.69 ( 59.88)	Acc@5  84.38 ( 88.12)
Epoch: [23][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3825e+00 (1.3813e+00)	Acc@1  58.59 ( 59.87)	Acc@5  87.50 ( 88.10)
Epoch: [23][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3664e+00 (1.3828e+00)	Acc@1  57.81 ( 59.78)	Acc@5  90.62 ( 88.11)
Epoch: [23][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4129e+00 (1.3860e+00)	Acc@1  60.16 ( 59.70)	Acc@5  87.50 ( 88.10)
Epoch: [23][240/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3927e+00 (1.3857e+00)	Acc@1  63.28 ( 59.72)	Acc@5  89.84 ( 88.16)
Epoch: [23][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4980e+00 (1.3888e+00)	Acc@1  53.12 ( 59.63)	Acc@5  86.72 ( 88.09)
Epoch: [23][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5139e+00 (1.3904e+00)	Acc@1  57.03 ( 59.61)	Acc@5  83.59 ( 88.08)
Epoch: [23][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3979e+00 (1.3905e+00)	Acc@1  57.81 ( 59.59)	Acc@5  85.16 ( 88.08)
Epoch: [23][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5525e+00 (1.3924e+00)	Acc@1  53.12 ( 59.53)	Acc@5  88.28 ( 88.09)
Epoch: [23][290/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5072e+00 (1.3926e+00)	Acc@1  57.81 ( 59.54)	Acc@5  82.81 ( 88.02)
Epoch: [23][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1132e+00 (1.3916e+00)	Acc@1  70.31 ( 59.59)	Acc@5  92.97 ( 88.05)
Epoch: [23][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3480e+00 (1.3947e+00)	Acc@1  57.03 ( 59.51)	Acc@5  91.41 ( 87.99)
Epoch: [23][320/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3894e+00 (1.3969e+00)	Acc@1  63.28 ( 59.51)	Acc@5  88.28 ( 87.95)
Epoch: [23][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4279e+00 (1.3967e+00)	Acc@1  60.94 ( 59.54)	Acc@5  86.72 ( 87.95)
Epoch: [23][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4565e+00 (1.3974e+00)	Acc@1  50.78 ( 59.48)	Acc@5  89.84 ( 87.94)
Epoch: [23][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4015e+00 (1.4005e+00)	Acc@1  60.16 ( 59.40)	Acc@5  91.41 ( 87.93)
Epoch: [23][360/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6886e+00 (1.4049e+00)	Acc@1  48.44 ( 59.29)	Acc@5  83.59 ( 87.85)
Epoch: [23][370/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4059e+00 (1.4061e+00)	Acc@1  58.59 ( 59.25)	Acc@5  87.50 ( 87.83)
Epoch: [23][380/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3328e+00 (1.4092e+00)	Acc@1  63.28 ( 59.20)	Acc@5  88.28 ( 87.80)
Epoch: [23][390/391]	Time  0.054 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3448e+00 (1.4087e+00)	Acc@1  61.25 ( 59.18)	Acc@5  88.75 ( 87.82)
## e[23] optimizer.zero_grad (sum) time: 0.41466188430786133
## e[23]       loss.backward (sum) time: 6.971198797225952
## e[23]      optimizer.step (sum) time: 3.4880850315093994
## epoch[23] training(only) time: 25.676474809646606
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.6969e+00 (1.6969e+00)	Acc@1  55.00 ( 55.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.7078e+00 (1.6264e+00)	Acc@1  51.00 ( 54.55)	Acc@5  84.00 ( 84.00)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.6887e+00 (1.6064e+00)	Acc@1  60.00 ( 54.95)	Acc@5  84.00 ( 84.67)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6295e+00 (1.6031e+00)	Acc@1  55.00 ( 54.87)	Acc@5  86.00 ( 84.94)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.4897e+00 (1.5984e+00)	Acc@1  53.00 ( 55.17)	Acc@5  85.00 ( 84.90)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.6192e+00 (1.6229e+00)	Acc@1  55.00 ( 54.69)	Acc@5  86.00 ( 84.37)
Test: [ 60/100]	Time  0.027 ( 0.028)	Loss 1.6448e+00 (1.6164e+00)	Acc@1  57.00 ( 54.82)	Acc@5  81.00 ( 84.41)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.6494e+00 (1.6266e+00)	Acc@1  54.00 ( 54.76)	Acc@5  85.00 ( 84.25)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.7174e+00 (1.6337e+00)	Acc@1  52.00 ( 54.79)	Acc@5  83.00 ( 84.06)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8750e+00 (1.6248e+00)	Acc@1  51.00 ( 54.91)	Acc@5  82.00 ( 84.07)
 * Acc@1 54.870 Acc@5 84.190
### epoch[23] execution time: 28.47997498512268
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.216 ( 0.216)	Data  0.148 ( 0.148)	Loss 1.3221e+00 (1.3221e+00)	Acc@1  62.50 ( 62.50)	Acc@5  86.72 ( 86.72)
Epoch: [24][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 1.3206e+00 (1.3264e+00)	Acc@1  62.50 ( 60.94)	Acc@5  89.06 ( 88.49)
Epoch: [24][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.2637e+00 (1.3209e+00)	Acc@1  60.94 ( 61.64)	Acc@5  89.84 ( 88.65)
Epoch: [24][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.1194e+00 (1.3140e+00)	Acc@1  67.97 ( 61.97)	Acc@5  92.19 ( 89.14)
Epoch: [24][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.0936e+00 (1.3105e+00)	Acc@1  73.44 ( 62.50)	Acc@5  91.41 ( 89.01)
Epoch: [24][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1411e+00 (1.3136e+00)	Acc@1  70.31 ( 62.64)	Acc@5  91.41 ( 88.82)
Epoch: [24][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2987e+00 (1.3128e+00)	Acc@1  64.06 ( 62.32)	Acc@5  89.06 ( 88.91)
Epoch: [24][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.2249e+00 (1.3082e+00)	Acc@1  67.19 ( 62.24)	Acc@5  89.84 ( 89.05)
Epoch: [24][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2773e+00 (1.3147e+00)	Acc@1  57.81 ( 61.90)	Acc@5  89.84 ( 88.99)
Epoch: [24][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3761e+00 (1.3118e+00)	Acc@1  62.50 ( 62.03)	Acc@5  86.72 ( 89.14)
Epoch: [24][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5217e+00 (1.3157e+00)	Acc@1  54.69 ( 61.90)	Acc@5  86.72 ( 89.13)
Epoch: [24][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5679e+00 (1.3241e+00)	Acc@1  57.03 ( 61.62)	Acc@5  83.59 ( 89.08)
Epoch: [24][120/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3300e+00 (1.3308e+00)	Acc@1  61.72 ( 61.40)	Acc@5  89.84 ( 89.02)
Epoch: [24][130/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4836e+00 (1.3344e+00)	Acc@1  54.69 ( 61.19)	Acc@5  86.72 ( 89.01)
Epoch: [24][140/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.5154e+00 (1.3368e+00)	Acc@1  56.25 ( 61.13)	Acc@5  85.94 ( 88.97)
Epoch: [24][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4761e+00 (1.3424e+00)	Acc@1  57.03 ( 61.02)	Acc@5  87.50 ( 88.92)
Epoch: [24][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2550e+00 (1.3449e+00)	Acc@1  64.06 ( 61.01)	Acc@5  89.06 ( 88.95)
Epoch: [24][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2315e+00 (1.3479e+00)	Acc@1  62.50 ( 60.93)	Acc@5  89.06 ( 88.95)
Epoch: [24][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5553e+00 (1.3547e+00)	Acc@1  57.03 ( 60.72)	Acc@5  85.94 ( 88.82)
Epoch: [24][190/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5076e+00 (1.3560e+00)	Acc@1  56.25 ( 60.63)	Acc@5  89.84 ( 88.86)
Epoch: [24][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1172e+00 (1.3597e+00)	Acc@1  64.84 ( 60.47)	Acc@5  89.84 ( 88.83)
Epoch: [24][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1302e+00 (1.3589e+00)	Acc@1  67.97 ( 60.48)	Acc@5  89.06 ( 88.83)
Epoch: [24][220/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1781e+00 (1.3605e+00)	Acc@1  67.97 ( 60.44)	Acc@5  91.41 ( 88.83)
Epoch: [24][230/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2671e+00 (1.3621e+00)	Acc@1  60.16 ( 60.35)	Acc@5  91.41 ( 88.77)
Epoch: [24][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2431e+00 (1.3619e+00)	Acc@1  60.94 ( 60.36)	Acc@5  88.28 ( 88.73)
Epoch: [24][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4235e+00 (1.3635e+00)	Acc@1  60.16 ( 60.33)	Acc@5  87.50 ( 88.74)
Epoch: [24][260/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3663e+00 (1.3659e+00)	Acc@1  60.16 ( 60.33)	Acc@5  91.41 ( 88.72)
Epoch: [24][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3688e+00 (1.3653e+00)	Acc@1  57.03 ( 60.27)	Acc@5  88.28 ( 88.74)
Epoch: [24][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2230e+00 (1.3662e+00)	Acc@1  59.38 ( 60.20)	Acc@5  91.41 ( 88.69)
Epoch: [24][290/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4788e+00 (1.3662e+00)	Acc@1  58.59 ( 60.23)	Acc@5  87.50 ( 88.69)
Epoch: [24][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1994e+00 (1.3664e+00)	Acc@1  63.28 ( 60.21)	Acc@5  92.97 ( 88.68)
Epoch: [24][310/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3184e+00 (1.3666e+00)	Acc@1  57.03 ( 60.19)	Acc@5  86.72 ( 88.68)
Epoch: [24][320/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4592e+00 (1.3716e+00)	Acc@1  60.94 ( 60.05)	Acc@5  86.72 ( 88.60)
Epoch: [24][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5561e+00 (1.3725e+00)	Acc@1  57.03 ( 60.05)	Acc@5  89.06 ( 88.60)
Epoch: [24][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4547e+00 (1.3749e+00)	Acc@1  60.16 ( 59.99)	Acc@5  88.28 ( 88.56)
Epoch: [24][350/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3955e+00 (1.3753e+00)	Acc@1  58.59 ( 59.96)	Acc@5  89.06 ( 88.56)
Epoch: [24][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4891e+00 (1.3783e+00)	Acc@1  64.84 ( 59.93)	Acc@5  84.38 ( 88.48)
Epoch: [24][370/391]	Time  0.066 ( 0.066)	Data  0.002 ( 0.002)	Loss 1.5312e+00 (1.3792e+00)	Acc@1  59.38 ( 59.91)	Acc@5  88.28 ( 88.48)
Epoch: [24][380/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2158e+00 (1.3805e+00)	Acc@1  63.28 ( 59.85)	Acc@5  89.84 ( 88.47)
Epoch: [24][390/391]	Time  0.050 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5303e+00 (1.3812e+00)	Acc@1  52.50 ( 59.83)	Acc@5  87.50 ( 88.45)
## e[24] optimizer.zero_grad (sum) time: 0.41898036003112793
## e[24]       loss.backward (sum) time: 6.99306058883667
## e[24]      optimizer.step (sum) time: 3.509505033493042
## epoch[24] training(only) time: 25.705506324768066
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.8214e+00 (1.8214e+00)	Acc@1  48.00 ( 48.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.025 ( 0.038)	Loss 1.7789e+00 (1.8268e+00)	Acc@1  55.00 ( 49.73)	Acc@5  85.00 ( 81.45)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.6489e+00 (1.8059e+00)	Acc@1  55.00 ( 50.67)	Acc@5  82.00 ( 81.71)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.8817e+00 (1.7868e+00)	Acc@1  46.00 ( 51.13)	Acc@5  82.00 ( 82.13)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.8307e+00 (1.7600e+00)	Acc@1  54.00 ( 51.49)	Acc@5  82.00 ( 82.49)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.5067e+00 (1.7603e+00)	Acc@1  55.00 ( 51.84)	Acc@5  85.00 ( 82.43)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.6503e+00 (1.7527e+00)	Acc@1  55.00 ( 52.25)	Acc@5  84.00 ( 82.34)
Test: [ 70/100]	Time  0.028 ( 0.029)	Loss 1.5244e+00 (1.7520e+00)	Acc@1  59.00 ( 52.24)	Acc@5  86.00 ( 82.21)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.8014e+00 (1.7606e+00)	Acc@1  48.00 ( 52.05)	Acc@5  79.00 ( 82.01)
Test: [ 90/100]	Time  0.029 ( 0.028)	Loss 2.0650e+00 (1.7485e+00)	Acc@1  49.00 ( 52.41)	Acc@5  71.00 ( 82.16)
 * Acc@1 52.600 Acc@5 82.240
### epoch[24] execution time: 28.565073251724243
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.219 ( 0.219)	Data  0.152 ( 0.152)	Loss 1.3603e+00 (1.3603e+00)	Acc@1  59.38 ( 59.38)	Acc@5  88.28 ( 88.28)
Epoch: [25][ 10/391]	Time  0.069 ( 0.080)	Data  0.001 ( 0.015)	Loss 1.2141e+00 (1.3077e+00)	Acc@1  65.62 ( 62.43)	Acc@5  90.62 ( 88.92)
Epoch: [25][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.2566e+00 (1.3004e+00)	Acc@1  61.72 ( 62.02)	Acc@5  90.62 ( 89.03)
Epoch: [25][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.2904e+00 (1.3034e+00)	Acc@1  64.06 ( 61.77)	Acc@5  89.84 ( 89.04)
Epoch: [25][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.3433e+00 (1.3023e+00)	Acc@1  58.59 ( 61.97)	Acc@5  89.84 ( 89.18)
Epoch: [25][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3611e+00 (1.3067e+00)	Acc@1  56.25 ( 61.89)	Acc@5  90.62 ( 89.25)
Epoch: [25][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.4503e+00 (1.3111e+00)	Acc@1  60.94 ( 61.85)	Acc@5  85.94 ( 89.09)
Epoch: [25][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5590e+00 (1.3147e+00)	Acc@1  54.69 ( 61.53)	Acc@5  85.94 ( 89.22)
Epoch: [25][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2947e+00 (1.3171e+00)	Acc@1  64.06 ( 61.46)	Acc@5  85.94 ( 89.14)
Epoch: [25][ 90/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3286e+00 (1.3257e+00)	Acc@1  58.59 ( 61.18)	Acc@5  92.19 ( 89.05)
Epoch: [25][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2789e+00 (1.3261e+00)	Acc@1  66.41 ( 61.13)	Acc@5  89.06 ( 89.03)
Epoch: [25][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2889e+00 (1.3314e+00)	Acc@1  64.06 ( 61.10)	Acc@5  88.28 ( 88.92)
Epoch: [25][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3724e+00 (1.3340e+00)	Acc@1  61.72 ( 61.00)	Acc@5  88.28 ( 88.88)
Epoch: [25][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5353e+00 (1.3359e+00)	Acc@1  54.69 ( 61.03)	Acc@5  85.94 ( 88.88)
Epoch: [25][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5164e+00 (1.3377e+00)	Acc@1  56.25 ( 61.08)	Acc@5  85.94 ( 88.87)
Epoch: [25][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4689e+00 (1.3359e+00)	Acc@1  60.94 ( 61.10)	Acc@5  87.50 ( 88.90)
Epoch: [25][160/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1565e+00 (1.3333e+00)	Acc@1  60.94 ( 61.18)	Acc@5  92.19 ( 88.95)
Epoch: [25][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2892e+00 (1.3340e+00)	Acc@1  61.72 ( 61.16)	Acc@5  89.06 ( 88.99)
Epoch: [25][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4202e+00 (1.3323e+00)	Acc@1  54.69 ( 61.24)	Acc@5  89.84 ( 89.07)
Epoch: [25][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3756e+00 (1.3340e+00)	Acc@1  57.81 ( 61.16)	Acc@5  89.06 ( 89.05)
Epoch: [25][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2709e+00 (1.3332e+00)	Acc@1  65.62 ( 61.15)	Acc@5  90.62 ( 89.07)
Epoch: [25][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3521e+00 (1.3319e+00)	Acc@1  55.47 ( 61.18)	Acc@5  90.62 ( 89.14)
Epoch: [25][220/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4224e+00 (1.3375e+00)	Acc@1  61.72 ( 61.04)	Acc@5  88.28 ( 89.08)
Epoch: [25][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4279e+00 (1.3387e+00)	Acc@1  60.16 ( 61.05)	Acc@5  87.50 ( 89.02)
Epoch: [25][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3924e+00 (1.3429e+00)	Acc@1  60.16 ( 60.99)	Acc@5  89.06 ( 88.92)
Epoch: [25][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5227e+00 (1.3435e+00)	Acc@1  53.12 ( 61.01)	Acc@5  91.41 ( 88.97)
Epoch: [25][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4782e+00 (1.3474e+00)	Acc@1  57.81 ( 60.95)	Acc@5  88.28 ( 88.89)
Epoch: [25][270/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4078e+00 (1.3497e+00)	Acc@1  64.84 ( 60.88)	Acc@5  89.06 ( 88.87)
Epoch: [25][280/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3035e+00 (1.3526e+00)	Acc@1  60.94 ( 60.81)	Acc@5  90.62 ( 88.82)
Epoch: [25][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1868e+00 (1.3489e+00)	Acc@1  64.06 ( 60.88)	Acc@5  91.41 ( 88.88)
Epoch: [25][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5821e+00 (1.3516e+00)	Acc@1  54.69 ( 60.80)	Acc@5  82.81 ( 88.83)
Epoch: [25][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5191e+00 (1.3508e+00)	Acc@1  57.81 ( 60.81)	Acc@5  85.94 ( 88.86)
Epoch: [25][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.3617e+00 (1.3519e+00)	Acc@1  60.94 ( 60.76)	Acc@5  89.84 ( 88.88)
Epoch: [25][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4886e+00 (1.3530e+00)	Acc@1  55.47 ( 60.75)	Acc@5  85.94 ( 88.83)
Epoch: [25][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2771e+00 (1.3555e+00)	Acc@1  63.28 ( 60.69)	Acc@5  91.41 ( 88.79)
Epoch: [25][350/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4268e+00 (1.3572e+00)	Acc@1  59.38 ( 60.64)	Acc@5  89.84 ( 88.73)
Epoch: [25][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2788e+00 (1.3586e+00)	Acc@1  59.38 ( 60.60)	Acc@5  90.62 ( 88.69)
Epoch: [25][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5441e+00 (1.3592e+00)	Acc@1  56.25 ( 60.54)	Acc@5  90.62 ( 88.70)
Epoch: [25][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2111e+00 (1.3598e+00)	Acc@1  59.38 ( 60.50)	Acc@5  90.62 ( 88.72)
Epoch: [25][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.7057e+00 (1.3586e+00)	Acc@1  51.25 ( 60.53)	Acc@5  82.50 ( 88.76)
## e[25] optimizer.zero_grad (sum) time: 0.41112852096557617
## e[25]       loss.backward (sum) time: 7.001464128494263
## e[25]      optimizer.step (sum) time: 3.438847064971924
## epoch[25] training(only) time: 25.581143379211426
# Switched to evaluate mode...
Test: [  0/100]	Time  0.167 ( 0.167)	Loss 1.6225e+00 (1.6225e+00)	Acc@1  58.00 ( 58.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.026 ( 0.040)	Loss 1.5763e+00 (1.4810e+00)	Acc@1  55.00 ( 58.00)	Acc@5  88.00 ( 86.82)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 1.3868e+00 (1.4836e+00)	Acc@1  58.00 ( 57.95)	Acc@5  89.00 ( 87.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5318e+00 (1.5211e+00)	Acc@1  52.00 ( 57.00)	Acc@5  88.00 ( 86.42)
Test: [ 40/100]	Time  0.029 ( 0.030)	Loss 1.5341e+00 (1.5357e+00)	Acc@1  55.00 ( 56.49)	Acc@5  88.00 ( 86.24)
Test: [ 50/100]	Time  0.028 ( 0.030)	Loss 1.4846e+00 (1.5377e+00)	Acc@1  59.00 ( 56.63)	Acc@5  85.00 ( 86.18)
Test: [ 60/100]	Time  0.029 ( 0.029)	Loss 1.5734e+00 (1.5347e+00)	Acc@1  55.00 ( 56.77)	Acc@5  85.00 ( 86.10)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.5123e+00 (1.5427e+00)	Acc@1  53.00 ( 56.65)	Acc@5  86.00 ( 85.92)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.7306e+00 (1.5536e+00)	Acc@1  57.00 ( 56.56)	Acc@5  79.00 ( 85.58)
Test: [ 90/100]	Time  0.029 ( 0.028)	Loss 1.7293e+00 (1.5407e+00)	Acc@1  53.00 ( 56.90)	Acc@5  85.00 ( 85.76)
 * Acc@1 56.990 Acc@5 85.750
### epoch[25] execution time: 28.42831778526306
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.224 ( 0.224)	Data  0.153 ( 0.153)	Loss 1.3313e+00 (1.3313e+00)	Acc@1  59.38 ( 59.38)	Acc@5  91.41 ( 91.41)
Epoch: [26][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.015)	Loss 1.2174e+00 (1.2688e+00)	Acc@1  60.94 ( 61.43)	Acc@5  92.97 ( 91.48)
Epoch: [26][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.3777e+00 (1.2971e+00)	Acc@1  61.72 ( 61.20)	Acc@5  87.50 ( 90.74)
Epoch: [26][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.3391e+00 (1.3217e+00)	Acc@1  64.06 ( 60.69)	Acc@5  87.50 ( 90.02)
Epoch: [26][ 40/391]	Time  0.064 ( 0.069)	Data  0.002 ( 0.005)	Loss 1.0744e+00 (1.3050e+00)	Acc@1  67.97 ( 61.19)	Acc@5  92.97 ( 90.02)
Epoch: [26][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.2251e+00 (1.2926e+00)	Acc@1  65.62 ( 61.55)	Acc@5  89.06 ( 90.17)
Epoch: [26][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.0662e+00 (1.2821e+00)	Acc@1  69.53 ( 61.87)	Acc@5  93.75 ( 90.15)
Epoch: [26][ 70/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.4594e+00 (1.3020e+00)	Acc@1  57.81 ( 61.52)	Acc@5  84.38 ( 89.90)
Epoch: [26][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4235e+00 (1.3050e+00)	Acc@1  58.59 ( 61.82)	Acc@5  87.50 ( 89.71)
Epoch: [26][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.4168e+00 (1.3115e+00)	Acc@1  58.59 ( 61.56)	Acc@5  88.28 ( 89.61)
Epoch: [26][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.6384e+00 (1.3116e+00)	Acc@1  56.25 ( 61.62)	Acc@5  81.25 ( 89.53)
Epoch: [26][110/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2140e+00 (1.3150e+00)	Acc@1  67.97 ( 61.54)	Acc@5  91.41 ( 89.46)
Epoch: [26][120/391]	Time  0.063 ( 0.067)	Data  0.002 ( 0.002)	Loss 1.4459e+00 (1.3226e+00)	Acc@1  63.28 ( 61.48)	Acc@5  84.38 ( 89.30)
Epoch: [26][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2830e+00 (1.3175e+00)	Acc@1  64.84 ( 61.74)	Acc@5  94.53 ( 89.40)
Epoch: [26][140/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4034e+00 (1.3179e+00)	Acc@1  57.03 ( 61.66)	Acc@5  92.97 ( 89.43)
Epoch: [26][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3205e+00 (1.3173e+00)	Acc@1  59.38 ( 61.64)	Acc@5  89.06 ( 89.44)
Epoch: [26][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1183e+00 (1.3174e+00)	Acc@1  66.41 ( 61.58)	Acc@5  91.41 ( 89.48)
Epoch: [26][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4316e+00 (1.3207e+00)	Acc@1  57.03 ( 61.38)	Acc@5  89.06 ( 89.41)
Epoch: [26][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1327e+00 (1.3152e+00)	Acc@1  74.22 ( 61.65)	Acc@5  89.84 ( 89.44)
Epoch: [26][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1786e+00 (1.3168e+00)	Acc@1  65.62 ( 61.58)	Acc@5  91.41 ( 89.39)
Epoch: [26][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1749e+00 (1.3184e+00)	Acc@1  64.84 ( 61.56)	Acc@5  89.84 ( 89.39)
Epoch: [26][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2281e+00 (1.3200e+00)	Acc@1  63.28 ( 61.54)	Acc@5  92.19 ( 89.36)
Epoch: [26][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4847e+00 (1.3231e+00)	Acc@1  56.25 ( 61.41)	Acc@5  88.28 ( 89.40)
Epoch: [26][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2862e+00 (1.3226e+00)	Acc@1  63.28 ( 61.50)	Acc@5  92.19 ( 89.40)
Epoch: [26][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1786e+00 (1.3251e+00)	Acc@1  61.72 ( 61.38)	Acc@5  87.50 ( 89.37)
Epoch: [26][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4076e+00 (1.3314e+00)	Acc@1  58.59 ( 61.24)	Acc@5  90.62 ( 89.26)
Epoch: [26][260/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5831e+00 (1.3331e+00)	Acc@1  55.47 ( 61.17)	Acc@5  85.16 ( 89.25)
Epoch: [26][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2185e+00 (1.3343e+00)	Acc@1  65.62 ( 61.18)	Acc@5  87.50 ( 89.22)
Epoch: [26][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5394e+00 (1.3345e+00)	Acc@1  58.59 ( 61.14)	Acc@5  80.47 ( 89.20)
Epoch: [26][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4870e+00 (1.3357e+00)	Acc@1  59.38 ( 61.15)	Acc@5  86.72 ( 89.13)
Epoch: [26][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4588e+00 (1.3365e+00)	Acc@1  57.81 ( 61.13)	Acc@5  82.81 ( 89.14)
Epoch: [26][310/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3261e+00 (1.3350e+00)	Acc@1  60.16 ( 61.18)	Acc@5  89.84 ( 89.16)
Epoch: [26][320/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5466e+00 (1.3355e+00)	Acc@1  55.47 ( 61.16)	Acc@5  87.50 ( 89.17)
Epoch: [26][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3203e+00 (1.3354e+00)	Acc@1  60.94 ( 61.17)	Acc@5  91.41 ( 89.15)
Epoch: [26][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3185e+00 (1.3396e+00)	Acc@1  63.28 ( 61.03)	Acc@5  90.62 ( 89.04)
Epoch: [26][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3916e+00 (1.3415e+00)	Acc@1  58.59 ( 60.99)	Acc@5  89.06 ( 89.02)
Epoch: [26][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3507e+00 (1.3414e+00)	Acc@1  60.94 ( 61.00)	Acc@5  89.84 ( 89.04)
Epoch: [26][370/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6924e+00 (1.3441e+00)	Acc@1  52.34 ( 60.96)	Acc@5  85.94 ( 89.02)
Epoch: [26][380/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4065e+00 (1.3437e+00)	Acc@1  56.25 ( 60.99)	Acc@5  89.06 ( 89.00)
Epoch: [26][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2378e+00 (1.3441e+00)	Acc@1  66.25 ( 61.01)	Acc@5  91.25 ( 88.97)
## e[26] optimizer.zero_grad (sum) time: 0.40906310081481934
## e[26]       loss.backward (sum) time: 6.97981071472168
## e[26]      optimizer.step (sum) time: 3.5309345722198486
## epoch[26] training(only) time: 25.66686749458313
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.5344e+00 (1.5344e+00)	Acc@1  61.00 ( 61.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 1.8270e+00 (1.5908e+00)	Acc@1  49.00 ( 56.18)	Acc@5  81.00 ( 84.73)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.7586e+00 (1.5687e+00)	Acc@1  55.00 ( 56.62)	Acc@5  84.00 ( 85.52)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.5200e+00 (1.5753e+00)	Acc@1  57.00 ( 56.42)	Acc@5  89.00 ( 85.58)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.4865e+00 (1.5644e+00)	Acc@1  56.00 ( 56.49)	Acc@5  89.00 ( 85.83)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.4042e+00 (1.5698e+00)	Acc@1  63.00 ( 56.18)	Acc@5  89.00 ( 85.55)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.5653e+00 (1.5615e+00)	Acc@1  53.00 ( 56.25)	Acc@5  84.00 ( 85.57)
Test: [ 70/100]	Time  0.029 ( 0.028)	Loss 1.5135e+00 (1.5696e+00)	Acc@1  56.00 ( 55.87)	Acc@5  85.00 ( 85.41)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.5616e+00 (1.5775e+00)	Acc@1  55.00 ( 55.63)	Acc@5  88.00 ( 85.41)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9537e+00 (1.5687e+00)	Acc@1  52.00 ( 55.87)	Acc@5  80.00 ( 85.47)
 * Acc@1 55.900 Acc@5 85.460
### epoch[26] execution time: 28.527263879776
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.221 ( 0.221)	Data  0.150 ( 0.150)	Loss 1.5167e+00 (1.5167e+00)	Acc@1  54.69 ( 54.69)	Acc@5  90.62 ( 90.62)
Epoch: [27][ 10/391]	Time  0.071 ( 0.080)	Data  0.002 ( 0.015)	Loss 1.2858e+00 (1.2769e+00)	Acc@1  57.03 ( 61.93)	Acc@5  90.62 ( 90.91)
Epoch: [27][ 20/391]	Time  0.067 ( 0.074)	Data  0.001 ( 0.008)	Loss 1.5074e+00 (1.2842e+00)	Acc@1  57.81 ( 62.54)	Acc@5  84.38 ( 90.51)
Epoch: [27][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.1858e+00 (1.2781e+00)	Acc@1  65.62 ( 62.78)	Acc@5  92.19 ( 90.20)
Epoch: [27][ 40/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.005)	Loss 1.2722e+00 (1.2725e+00)	Acc@1  60.16 ( 62.63)	Acc@5  92.19 ( 90.28)
Epoch: [27][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.4258e+00 (1.2633e+00)	Acc@1  59.38 ( 62.97)	Acc@5  88.28 ( 90.15)
Epoch: [27][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2613e+00 (1.2594e+00)	Acc@1  64.84 ( 63.08)	Acc@5  90.62 ( 90.16)
Epoch: [27][ 70/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.0755e+00 (1.2638e+00)	Acc@1  64.84 ( 62.72)	Acc@5  92.19 ( 90.09)
Epoch: [27][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0694e+00 (1.2637e+00)	Acc@1  63.28 ( 62.90)	Acc@5  96.09 ( 89.97)
Epoch: [27][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1361e+00 (1.2658e+00)	Acc@1  64.84 ( 62.69)	Acc@5  92.19 ( 89.90)
Epoch: [27][100/391]	Time  0.064 ( 0.067)	Data  0.002 ( 0.003)	Loss 1.2991e+00 (1.2657e+00)	Acc@1  58.59 ( 62.69)	Acc@5  89.06 ( 89.92)
Epoch: [27][110/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1441e+00 (1.2662e+00)	Acc@1  63.28 ( 62.68)	Acc@5  91.41 ( 89.91)
Epoch: [27][120/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.4009e+00 (1.2744e+00)	Acc@1  61.72 ( 62.47)	Acc@5  85.94 ( 89.77)
Epoch: [27][130/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.3495e+00 (1.2780e+00)	Acc@1  61.72 ( 62.42)	Acc@5  90.62 ( 89.73)
Epoch: [27][140/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0675e+00 (1.2779e+00)	Acc@1  70.31 ( 62.51)	Acc@5  94.53 ( 89.69)
Epoch: [27][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3095e+00 (1.2824e+00)	Acc@1  61.72 ( 62.40)	Acc@5  89.06 ( 89.65)
Epoch: [27][160/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3288e+00 (1.2830e+00)	Acc@1  57.03 ( 62.32)	Acc@5  89.84 ( 89.70)
Epoch: [27][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3073e+00 (1.2867e+00)	Acc@1  62.50 ( 62.22)	Acc@5  91.41 ( 89.65)
Epoch: [27][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4073e+00 (1.2870e+00)	Acc@1  57.03 ( 62.21)	Acc@5  91.41 ( 89.65)
Epoch: [27][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3707e+00 (1.2875e+00)	Acc@1  60.16 ( 62.27)	Acc@5  85.94 ( 89.62)
Epoch: [27][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3783e+00 (1.2923e+00)	Acc@1  58.59 ( 62.08)	Acc@5  86.72 ( 89.54)
Epoch: [27][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2695e+00 (1.2963e+00)	Acc@1  63.28 ( 62.00)	Acc@5  89.06 ( 89.52)
Epoch: [27][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3333e+00 (1.3007e+00)	Acc@1  63.28 ( 61.96)	Acc@5  89.84 ( 89.43)
Epoch: [27][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3125e+00 (1.3030e+00)	Acc@1  56.25 ( 61.87)	Acc@5  89.84 ( 89.45)
Epoch: [27][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3886e+00 (1.3057e+00)	Acc@1  59.38 ( 61.80)	Acc@5  88.28 ( 89.39)
Epoch: [27][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4706e+00 (1.3089e+00)	Acc@1  53.12 ( 61.67)	Acc@5  89.84 ( 89.37)
Epoch: [27][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3248e+00 (1.3105e+00)	Acc@1  63.28 ( 61.69)	Acc@5  86.72 ( 89.30)
Epoch: [27][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1324e+00 (1.3094e+00)	Acc@1  60.94 ( 61.69)	Acc@5  91.41 ( 89.29)
Epoch: [27][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2084e+00 (1.3119e+00)	Acc@1  63.28 ( 61.64)	Acc@5  88.28 ( 89.27)
Epoch: [27][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3036e+00 (1.3119e+00)	Acc@1  63.28 ( 61.67)	Acc@5  89.84 ( 89.24)
Epoch: [27][300/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3357e+00 (1.3121e+00)	Acc@1  56.25 ( 61.65)	Acc@5  94.53 ( 89.25)
Epoch: [27][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4090e+00 (1.3145e+00)	Acc@1  57.81 ( 61.65)	Acc@5  86.72 ( 89.22)
Epoch: [27][320/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2738e+00 (1.3138e+00)	Acc@1  61.72 ( 61.64)	Acc@5  88.28 ( 89.22)
Epoch: [27][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4677e+00 (1.3169e+00)	Acc@1  58.59 ( 61.56)	Acc@5  82.81 ( 89.17)
Epoch: [27][340/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3597e+00 (1.3199e+00)	Acc@1  58.59 ( 61.50)	Acc@5  85.94 ( 89.14)
Epoch: [27][350/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3314e+00 (1.3224e+00)	Acc@1  60.94 ( 61.45)	Acc@5  92.97 ( 89.11)
Epoch: [27][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5356e+00 (1.3257e+00)	Acc@1  51.56 ( 61.34)	Acc@5  87.50 ( 89.08)
Epoch: [27][370/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3979e+00 (1.3268e+00)	Acc@1  64.84 ( 61.30)	Acc@5  88.28 ( 89.07)
Epoch: [27][380/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5072e+00 (1.3275e+00)	Acc@1  54.69 ( 61.27)	Acc@5  89.06 ( 89.07)
Epoch: [27][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.4791e+00 (1.3285e+00)	Acc@1  60.00 ( 61.26)	Acc@5  83.75 ( 89.06)
## e[27] optimizer.zero_grad (sum) time: 0.41245317459106445
## e[27]       loss.backward (sum) time: 7.0081703662872314
## e[27]      optimizer.step (sum) time: 3.4755799770355225
## epoch[27] training(only) time: 25.681543111801147
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.4653e+00 (1.4653e+00)	Acc@1  60.00 ( 60.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.5976e+00 (1.5359e+00)	Acc@1  58.00 ( 57.45)	Acc@5  87.00 ( 86.00)
Test: [ 20/100]	Time  0.026 ( 0.034)	Loss 1.4564e+00 (1.5618e+00)	Acc@1  57.00 ( 56.71)	Acc@5  87.00 ( 85.10)
Test: [ 30/100]	Time  0.027 ( 0.032)	Loss 1.5630e+00 (1.5611e+00)	Acc@1  57.00 ( 56.65)	Acc@5  87.00 ( 85.42)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.5255e+00 (1.5451e+00)	Acc@1  57.00 ( 56.80)	Acc@5  85.00 ( 85.78)
Test: [ 50/100]	Time  0.025 ( 0.030)	Loss 1.5149e+00 (1.5736e+00)	Acc@1  60.00 ( 56.45)	Acc@5  82.00 ( 85.33)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.4654e+00 (1.5708e+00)	Acc@1  56.00 ( 56.66)	Acc@5  87.00 ( 85.41)
Test: [ 70/100]	Time  0.026 ( 0.029)	Loss 1.4353e+00 (1.5820e+00)	Acc@1  60.00 ( 56.56)	Acc@5  81.00 ( 85.08)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.7287e+00 (1.5937e+00)	Acc@1  54.00 ( 56.28)	Acc@5  81.00 ( 84.79)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.9943e+00 (1.5884e+00)	Acc@1  48.00 ( 56.36)	Acc@5  81.00 ( 84.84)
 * Acc@1 56.210 Acc@5 84.770
### epoch[27] execution time: 28.574605226516724
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.220 ( 0.220)	Data  0.151 ( 0.151)	Loss 1.2180e+00 (1.2180e+00)	Acc@1  66.41 ( 66.41)	Acc@5  89.06 ( 89.06)
Epoch: [28][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.015)	Loss 1.3205e+00 (1.2730e+00)	Acc@1  60.94 ( 61.51)	Acc@5  90.62 ( 90.27)
Epoch: [28][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.4566e+00 (1.2696e+00)	Acc@1  60.94 ( 62.54)	Acc@5  90.62 ( 90.22)
Epoch: [28][ 30/391]	Time  0.067 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.2469e+00 (1.2743e+00)	Acc@1  64.06 ( 62.58)	Acc@5  89.84 ( 90.25)
Epoch: [28][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.2284e+00 (1.2736e+00)	Acc@1  64.06 ( 62.65)	Acc@5  90.62 ( 90.21)
Epoch: [28][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.1941e+00 (1.2589e+00)	Acc@1  62.50 ( 62.76)	Acc@5  92.97 ( 90.32)
Epoch: [28][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.2911e+00 (1.2579e+00)	Acc@1  57.81 ( 62.76)	Acc@5  92.97 ( 90.47)
Epoch: [28][ 70/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1956e+00 (1.2529e+00)	Acc@1  67.97 ( 63.07)	Acc@5  90.62 ( 90.40)
Epoch: [28][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1104e+00 (1.2610e+00)	Acc@1  65.62 ( 62.89)	Acc@5  92.19 ( 90.36)
Epoch: [28][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2943e+00 (1.2673e+00)	Acc@1  64.84 ( 62.82)	Acc@5  89.84 ( 90.20)
Epoch: [28][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 1.3154e+00 (1.2728e+00)	Acc@1  61.72 ( 62.78)	Acc@5  92.97 ( 90.00)
Epoch: [28][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2169e+00 (1.2746e+00)	Acc@1  60.16 ( 62.83)	Acc@5  88.28 ( 89.98)
Epoch: [28][120/391]	Time  0.077 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4008e+00 (1.2760e+00)	Acc@1  60.94 ( 62.70)	Acc@5  87.50 ( 89.99)
Epoch: [28][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2949e+00 (1.2727e+00)	Acc@1  63.28 ( 62.85)	Acc@5  90.62 ( 90.00)
Epoch: [28][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2701e+00 (1.2685e+00)	Acc@1  60.16 ( 62.94)	Acc@5  92.97 ( 90.12)
Epoch: [28][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2119e+00 (1.2718e+00)	Acc@1  64.06 ( 62.91)	Acc@5  89.06 ( 90.04)
Epoch: [28][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1253e+00 (1.2717e+00)	Acc@1  64.06 ( 62.83)	Acc@5  92.97 ( 90.00)
Epoch: [28][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5194e+00 (1.2790e+00)	Acc@1  55.47 ( 62.63)	Acc@5  84.38 ( 89.90)
Epoch: [28][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4298e+00 (1.2832e+00)	Acc@1  60.94 ( 62.50)	Acc@5  84.38 ( 89.82)
Epoch: [28][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1943e+00 (1.2871e+00)	Acc@1  69.53 ( 62.41)	Acc@5  90.62 ( 89.75)
Epoch: [28][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5471e+00 (1.2885e+00)	Acc@1  50.78 ( 62.32)	Acc@5  86.72 ( 89.72)
Epoch: [28][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1624e+00 (1.2888e+00)	Acc@1  63.28 ( 62.33)	Acc@5  89.84 ( 89.70)
Epoch: [28][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3519e+00 (1.2908e+00)	Acc@1  63.28 ( 62.28)	Acc@5  89.84 ( 89.65)
Epoch: [28][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3505e+00 (1.2925e+00)	Acc@1  58.59 ( 62.21)	Acc@5  85.94 ( 89.61)
Epoch: [28][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3197e+00 (1.2939e+00)	Acc@1  62.50 ( 62.20)	Acc@5  87.50 ( 89.55)
Epoch: [28][250/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5529e+00 (1.2962e+00)	Acc@1  51.56 ( 62.17)	Acc@5  87.50 ( 89.50)
Epoch: [28][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4768e+00 (1.2949e+00)	Acc@1  57.03 ( 62.17)	Acc@5  85.16 ( 89.52)
Epoch: [28][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4143e+00 (1.2948e+00)	Acc@1  59.38 ( 62.23)	Acc@5  88.28 ( 89.49)
Epoch: [28][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1906e+00 (1.2950e+00)	Acc@1  65.62 ( 62.22)	Acc@5  90.62 ( 89.54)
Epoch: [28][290/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2856e+00 (1.2946e+00)	Acc@1  61.72 ( 62.21)	Acc@5  92.19 ( 89.54)
Epoch: [28][300/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2322e+00 (1.2932e+00)	Acc@1  60.16 ( 62.23)	Acc@5  90.62 ( 89.53)
Epoch: [28][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3144e+00 (1.2952e+00)	Acc@1  64.06 ( 62.20)	Acc@5  91.41 ( 89.51)
Epoch: [28][320/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1722e+00 (1.2950e+00)	Acc@1  64.06 ( 62.22)	Acc@5  92.97 ( 89.54)
Epoch: [28][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2493e+00 (1.2949e+00)	Acc@1  66.41 ( 62.22)	Acc@5  93.75 ( 89.56)
Epoch: [28][340/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1315e+00 (1.2958e+00)	Acc@1  66.41 ( 62.20)	Acc@5  92.97 ( 89.53)
Epoch: [28][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.6370e+00 (1.2983e+00)	Acc@1  51.56 ( 62.12)	Acc@5  83.59 ( 89.50)
Epoch: [28][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3757e+00 (1.2990e+00)	Acc@1  62.50 ( 62.16)	Acc@5  87.50 ( 89.50)
Epoch: [28][370/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2481e+00 (1.3009e+00)	Acc@1  61.72 ( 62.11)	Acc@5  90.62 ( 89.48)
Epoch: [28][380/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2275e+00 (1.3011e+00)	Acc@1  63.28 ( 62.09)	Acc@5  91.41 ( 89.46)
Epoch: [28][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.5144e+00 (1.3012e+00)	Acc@1  56.25 ( 62.08)	Acc@5  85.00 ( 89.45)
## e[28] optimizer.zero_grad (sum) time: 0.4051060676574707
## e[28]       loss.backward (sum) time: 6.947105407714844
## e[28]      optimizer.step (sum) time: 3.4971365928649902
## epoch[28] training(only) time: 25.618592500686646
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.4777e+00 (1.4777e+00)	Acc@1  61.00 ( 61.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.4696e+00 (1.5173e+00)	Acc@1  58.00 ( 58.36)	Acc@5  87.00 ( 85.91)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3663e+00 (1.5181e+00)	Acc@1  61.00 ( 58.43)	Acc@5  86.00 ( 85.81)
Test: [ 30/100]	Time  0.033 ( 0.031)	Loss 1.6777e+00 (1.5274e+00)	Acc@1  57.00 ( 58.32)	Acc@5  81.00 ( 85.87)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.4858e+00 (1.5218e+00)	Acc@1  58.00 ( 57.80)	Acc@5  85.00 ( 86.05)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.4034e+00 (1.5336e+00)	Acc@1  59.00 ( 57.49)	Acc@5  85.00 ( 85.51)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.7994e+00 (1.5422e+00)	Acc@1  54.00 ( 57.18)	Acc@5  80.00 ( 85.26)
Test: [ 70/100]	Time  0.029 ( 0.028)	Loss 1.5293e+00 (1.5499e+00)	Acc@1  53.00 ( 56.96)	Acc@5  87.00 ( 85.30)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 1.5856e+00 (1.5627e+00)	Acc@1  56.00 ( 56.84)	Acc@5  87.00 ( 85.11)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9668e+00 (1.5569e+00)	Acc@1  47.00 ( 56.90)	Acc@5  77.00 ( 85.18)
 * Acc@1 56.980 Acc@5 85.220
### epoch[28] execution time: 28.50206470489502
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.215 ( 0.215)	Data  0.144 ( 0.144)	Loss 1.1401e+00 (1.1401e+00)	Acc@1  60.16 ( 60.16)	Acc@5  94.53 ( 94.53)
Epoch: [29][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.014)	Loss 1.1976e+00 (1.1683e+00)	Acc@1  65.62 ( 65.70)	Acc@5  92.97 ( 91.69)
Epoch: [29][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.1287e+00 (1.1986e+00)	Acc@1  69.53 ( 64.32)	Acc@5  91.41 ( 91.44)
Epoch: [29][ 30/391]	Time  0.070 ( 0.070)	Data  0.001 ( 0.006)	Loss 1.2342e+00 (1.2142e+00)	Acc@1  68.75 ( 64.14)	Acc@5  88.28 ( 90.90)
Epoch: [29][ 40/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.005)	Loss 1.4427e+00 (1.2183e+00)	Acc@1  57.03 ( 64.29)	Acc@5  85.94 ( 90.66)
Epoch: [29][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 9.8208e-01 (1.2219e+00)	Acc@1  75.00 ( 64.32)	Acc@5  92.97 ( 90.69)
Epoch: [29][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.3597e+00 (1.2235e+00)	Acc@1  63.28 ( 64.37)	Acc@5  87.50 ( 90.65)
Epoch: [29][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.2624e+00 (1.2212e+00)	Acc@1  63.28 ( 64.48)	Acc@5  86.72 ( 90.58)
Epoch: [29][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.5299e+00 (1.2238e+00)	Acc@1  54.69 ( 64.43)	Acc@5  86.72 ( 90.64)
Epoch: [29][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.1385e+00 (1.2211e+00)	Acc@1  67.19 ( 64.32)	Acc@5  92.97 ( 90.66)
Epoch: [29][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.3384e+00 (1.2227e+00)	Acc@1  64.06 ( 64.35)	Acc@5  89.84 ( 90.63)
Epoch: [29][110/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.5119e-01 (1.2309e+00)	Acc@1  74.22 ( 64.22)	Acc@5  93.75 ( 90.46)
Epoch: [29][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1800e+00 (1.2353e+00)	Acc@1  67.19 ( 64.06)	Acc@5  90.62 ( 90.42)
Epoch: [29][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5256e+00 (1.2482e+00)	Acc@1  54.69 ( 63.67)	Acc@5  85.94 ( 90.17)
Epoch: [29][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3120e+00 (1.2514e+00)	Acc@1  59.38 ( 63.54)	Acc@5  91.41 ( 90.19)
Epoch: [29][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3204e+00 (1.2525e+00)	Acc@1  60.94 ( 63.45)	Acc@5  85.94 ( 90.17)
Epoch: [29][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4446e+00 (1.2561e+00)	Acc@1  64.84 ( 63.37)	Acc@5  84.38 ( 90.11)
Epoch: [29][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5929e+00 (1.2603e+00)	Acc@1  53.91 ( 63.15)	Acc@5  85.94 ( 90.05)
Epoch: [29][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3924e+00 (1.2598e+00)	Acc@1  60.16 ( 63.20)	Acc@5  89.06 ( 90.09)
Epoch: [29][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1560e+00 (1.2657e+00)	Acc@1  67.97 ( 63.08)	Acc@5  89.84 ( 90.02)
Epoch: [29][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4042e+00 (1.2699e+00)	Acc@1  57.81 ( 62.95)	Acc@5  86.72 ( 89.94)
Epoch: [29][210/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3139e+00 (1.2737e+00)	Acc@1  64.84 ( 62.90)	Acc@5  87.50 ( 89.90)
Epoch: [29][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2018e+00 (1.2739e+00)	Acc@1  62.50 ( 62.83)	Acc@5  90.62 ( 89.93)
Epoch: [29][230/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3410e+00 (1.2765e+00)	Acc@1  60.16 ( 62.77)	Acc@5  89.06 ( 89.90)
Epoch: [29][240/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3584e+00 (1.2770e+00)	Acc@1  62.50 ( 62.75)	Acc@5  88.28 ( 89.90)
Epoch: [29][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.5088e+00 (1.2814e+00)	Acc@1  57.03 ( 62.66)	Acc@5  83.59 ( 89.80)
Epoch: [29][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2221e+00 (1.2817e+00)	Acc@1  63.28 ( 62.64)	Acc@5  92.97 ( 89.78)
Epoch: [29][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3539e+00 (1.2820e+00)	Acc@1  63.28 ( 62.59)	Acc@5  89.84 ( 89.75)
Epoch: [29][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4679e+00 (1.2843e+00)	Acc@1  55.47 ( 62.56)	Acc@5  83.59 ( 89.71)
Epoch: [29][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2182e+00 (1.2888e+00)	Acc@1  63.28 ( 62.44)	Acc@5  91.41 ( 89.67)
Epoch: [29][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2660e+00 (1.2884e+00)	Acc@1  59.38 ( 62.45)	Acc@5  93.75 ( 89.72)
Epoch: [29][310/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1540e+00 (1.2893e+00)	Acc@1  62.50 ( 62.39)	Acc@5  93.75 ( 89.72)
Epoch: [29][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3402e+00 (1.2893e+00)	Acc@1  60.16 ( 62.37)	Acc@5  88.28 ( 89.69)
Epoch: [29][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2109e+00 (1.2913e+00)	Acc@1  63.28 ( 62.29)	Acc@5  89.84 ( 89.66)
Epoch: [29][340/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.4484e+00 (1.2928e+00)	Acc@1  59.38 ( 62.23)	Acc@5  89.84 ( 89.64)
Epoch: [29][350/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2497e+00 (1.2938e+00)	Acc@1  64.84 ( 62.21)	Acc@5  89.06 ( 89.62)
Epoch: [29][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2056e+00 (1.2925e+00)	Acc@1  63.28 ( 62.23)	Acc@5  89.06 ( 89.65)
Epoch: [29][370/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.2070e+00 (1.2924e+00)	Acc@1  64.06 ( 62.25)	Acc@5  89.06 ( 89.64)
Epoch: [29][380/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3131e+00 (1.2926e+00)	Acc@1  57.81 ( 62.26)	Acc@5  88.28 ( 89.66)
Epoch: [29][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.2406e+00 (1.2932e+00)	Acc@1  70.00 ( 62.23)	Acc@5  83.75 ( 89.63)
## e[29] optimizer.zero_grad (sum) time: 0.41771960258483887
## e[29]       loss.backward (sum) time: 6.999296188354492
## e[29]      optimizer.step (sum) time: 3.54067063331604
## epoch[29] training(only) time: 25.67955780029297
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.4906e+00 (1.4906e+00)	Acc@1  63.00 ( 63.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.027 ( 0.038)	Loss 1.7868e+00 (1.5814e+00)	Acc@1  47.00 ( 58.27)	Acc@5  89.00 ( 85.45)
Test: [ 20/100]	Time  0.025 ( 0.032)	Loss 1.2553e+00 (1.5236e+00)	Acc@1  65.00 ( 58.71)	Acc@5  88.00 ( 86.24)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 1.6619e+00 (1.5207e+00)	Acc@1  49.00 ( 58.35)	Acc@5  87.00 ( 86.42)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.5079e+00 (1.5048e+00)	Acc@1  59.00 ( 58.20)	Acc@5  86.00 ( 86.71)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.5613e+00 (1.5246e+00)	Acc@1  58.00 ( 57.90)	Acc@5  82.00 ( 86.29)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.6115e+00 (1.5177e+00)	Acc@1  57.00 ( 58.03)	Acc@5  85.00 ( 86.30)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.5574e+00 (1.5305e+00)	Acc@1  60.00 ( 57.75)	Acc@5  85.00 ( 86.01)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.6361e+00 (1.5432e+00)	Acc@1  52.00 ( 57.43)	Acc@5  86.00 ( 85.86)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8664e+00 (1.5414e+00)	Acc@1  55.00 ( 57.33)	Acc@5  79.00 ( 85.93)
 * Acc@1 57.480 Acc@5 86.080
### epoch[29] execution time: 28.496573209762573
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.225 ( 0.225)	Data  0.154 ( 0.154)	Loss 9.1324e-01 (9.1324e-01)	Acc@1  72.66 ( 72.66)	Acc@5  97.66 ( 97.66)
Epoch: [30][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.015)	Loss 1.1742e+00 (1.1785e+00)	Acc@1  65.62 ( 65.06)	Acc@5  92.19 ( 91.48)
Epoch: [30][ 20/391]	Time  0.070 ( 0.073)	Data  0.001 ( 0.008)	Loss 1.0602e+00 (1.1398e+00)	Acc@1  62.50 ( 65.85)	Acc@5  93.75 ( 91.59)
Epoch: [30][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.006)	Loss 1.1597e+00 (1.1295e+00)	Acc@1  64.84 ( 66.51)	Acc@5  91.41 ( 91.66)
Epoch: [30][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 9.4642e-01 (1.1133e+00)	Acc@1  70.31 ( 67.40)	Acc@5  92.97 ( 91.62)
Epoch: [30][ 50/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 8.8297e-01 (1.0938e+00)	Acc@1  74.22 ( 67.92)	Acc@5  96.88 ( 92.16)
Epoch: [30][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 1.0200e+00 (1.0879e+00)	Acc@1  73.44 ( 68.31)	Acc@5  92.97 ( 92.21)
Epoch: [30][ 70/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.003)	Loss 1.1711e+00 (1.0856e+00)	Acc@1  64.84 ( 68.32)	Acc@5  92.19 ( 92.35)
Epoch: [30][ 80/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.2919e-01 (1.0810e+00)	Acc@1  78.91 ( 68.68)	Acc@5  96.88 ( 92.34)
Epoch: [30][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.4593e-01 (1.0713e+00)	Acc@1  67.19 ( 68.98)	Acc@5  96.88 ( 92.49)
Epoch: [30][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 1.0101e+00 (1.0708e+00)	Acc@1  67.19 ( 68.88)	Acc@5  94.53 ( 92.46)
Epoch: [30][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.9845e-01 (1.0681e+00)	Acc@1  71.88 ( 68.83)	Acc@5  90.62 ( 92.47)
Epoch: [30][120/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0046e+00 (1.0593e+00)	Acc@1  76.56 ( 69.13)	Acc@5  90.62 ( 92.61)
Epoch: [30][130/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.2382e-01 (1.0540e+00)	Acc@1  74.22 ( 69.30)	Acc@5  90.62 ( 92.60)
Epoch: [30][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5211e-01 (1.0502e+00)	Acc@1  76.56 ( 69.45)	Acc@5  95.31 ( 92.64)
Epoch: [30][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0191e+00 (1.0450e+00)	Acc@1  70.31 ( 69.55)	Acc@5  95.31 ( 92.74)
Epoch: [30][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1064e+00 (1.0419e+00)	Acc@1  67.19 ( 69.60)	Acc@5  91.41 ( 92.78)
Epoch: [30][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0014e+00 (1.0399e+00)	Acc@1  66.41 ( 69.66)	Acc@5  96.88 ( 92.83)
Epoch: [30][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6248e-01 (1.0354e+00)	Acc@1  67.97 ( 69.72)	Acc@5  94.53 ( 92.91)
Epoch: [30][190/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9650e-01 (1.0314e+00)	Acc@1  67.19 ( 69.85)	Acc@5  94.53 ( 92.98)
Epoch: [30][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.3324e+00 (1.0301e+00)	Acc@1  64.84 ( 69.90)	Acc@5  85.94 ( 93.00)
Epoch: [30][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0905e+00 (1.0300e+00)	Acc@1  68.75 ( 69.89)	Acc@5  91.41 ( 93.01)
Epoch: [30][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0071e+00 (1.0281e+00)	Acc@1  68.75 ( 69.95)	Acc@5  92.97 ( 93.01)
Epoch: [30][230/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2896e-01 (1.0215e+00)	Acc@1  75.00 ( 70.11)	Acc@5  94.53 ( 93.07)
Epoch: [30][240/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5658e-01 (1.0164e+00)	Acc@1  78.12 ( 70.29)	Acc@5  93.75 ( 93.11)
Epoch: [30][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1268e-01 (1.0133e+00)	Acc@1  73.44 ( 70.39)	Acc@5  94.53 ( 93.12)
Epoch: [30][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8192e-01 (1.0095e+00)	Acc@1  69.53 ( 70.48)	Acc@5  92.19 ( 93.18)
Epoch: [30][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9793e-01 (1.0078e+00)	Acc@1  75.78 ( 70.51)	Acc@5  94.53 ( 93.20)
Epoch: [30][280/391]	Time  0.065 ( 0.066)	Data  0.002 ( 0.002)	Loss 1.0861e+00 (1.0072e+00)	Acc@1  67.97 ( 70.54)	Acc@5  89.06 ( 93.20)
Epoch: [30][290/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1304e+00 (1.0051e+00)	Acc@1  67.19 ( 70.59)	Acc@5  92.19 ( 93.25)
Epoch: [30][300/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1126e-01 (1.0027e+00)	Acc@1  67.19 ( 70.63)	Acc@5  96.09 ( 93.29)
Epoch: [30][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5669e-01 (1.0014e+00)	Acc@1  73.44 ( 70.62)	Acc@5  95.31 ( 93.30)
Epoch: [30][320/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6468e-01 (1.0008e+00)	Acc@1  76.56 ( 70.60)	Acc@5  95.31 ( 93.29)
Epoch: [30][330/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9483e-01 (9.9768e-01)	Acc@1  79.69 ( 70.69)	Acc@5  95.31 ( 93.34)
Epoch: [30][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9180e-01 (9.9732e-01)	Acc@1  73.44 ( 70.69)	Acc@5  94.53 ( 93.34)
Epoch: [30][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0623e-01 (9.9452e-01)	Acc@1  78.12 ( 70.80)	Acc@5  96.88 ( 93.35)
Epoch: [30][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5880e-01 (9.9145e-01)	Acc@1  74.22 ( 70.92)	Acc@5  95.31 ( 93.38)
Epoch: [30][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8321e-01 (9.8967e-01)	Acc@1  69.53 ( 70.95)	Acc@5  93.75 ( 93.38)
Epoch: [30][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0914e+00 (9.8994e-01)	Acc@1  68.75 ( 70.92)	Acc@5  91.41 ( 93.37)
Epoch: [30][390/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7224e-01 (9.8941e-01)	Acc@1  76.25 ( 70.92)	Acc@5  93.75 ( 93.39)
## e[30] optimizer.zero_grad (sum) time: 0.407318115234375
## e[30]       loss.backward (sum) time: 7.014099597930908
## e[30]      optimizer.step (sum) time: 3.5399115085601807
## epoch[30] training(only) time: 25.699673175811768
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2957e+00 (1.2957e+00)	Acc@1  68.00 ( 68.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3221e+00 (1.2216e+00)	Acc@1  63.00 ( 66.64)	Acc@5  89.00 ( 88.64)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.1245e+00 (1.1938e+00)	Acc@1  68.00 ( 66.86)	Acc@5  91.00 ( 89.86)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.3267e+00 (1.2036e+00)	Acc@1  59.00 ( 66.03)	Acc@5  89.00 ( 89.71)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1346e+00 (1.1942e+00)	Acc@1  64.00 ( 65.76)	Acc@5  94.00 ( 90.24)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.1384e+00 (1.2038e+00)	Acc@1  69.00 ( 65.57)	Acc@5  91.00 ( 89.96)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2715e+00 (1.1976e+00)	Acc@1  62.00 ( 65.52)	Acc@5  90.00 ( 90.02)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.2177e+00 (1.2060e+00)	Acc@1  69.00 ( 65.58)	Acc@5  91.00 ( 89.92)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3273e+00 (1.2151e+00)	Acc@1  66.00 ( 65.33)	Acc@5  90.00 ( 89.77)
Test: [ 90/100]	Time  0.029 ( 0.028)	Loss 1.6802e+00 (1.2103e+00)	Acc@1  59.00 ( 65.48)	Acc@5  86.00 ( 89.78)
 * Acc@1 65.710 Acc@5 89.950
### epoch[30] execution time: 28.53770923614502
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.222 ( 0.222)	Data  0.129 ( 0.129)	Loss 8.0244e-01 (8.0244e-01)	Acc@1  74.22 ( 74.22)	Acc@5  94.53 ( 94.53)
Epoch: [31][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.013)	Loss 9.7509e-01 (9.1665e-01)	Acc@1  69.53 ( 72.94)	Acc@5  93.75 ( 93.89)
Epoch: [31][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.007)	Loss 8.0946e-01 (9.2060e-01)	Acc@1  74.22 ( 72.88)	Acc@5  95.31 ( 93.90)
Epoch: [31][ 30/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.005)	Loss 8.5697e-01 (9.1238e-01)	Acc@1  75.78 ( 73.46)	Acc@5  93.75 ( 93.75)
Epoch: [31][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 7.6747e-01 (9.0261e-01)	Acc@1  78.12 ( 73.57)	Acc@5  96.09 ( 93.88)
Epoch: [31][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.5369e-01 (9.0599e-01)	Acc@1  73.44 ( 73.13)	Acc@5  92.97 ( 94.07)
Epoch: [31][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 8.5779e-01 (9.0563e-01)	Acc@1  75.00 ( 73.07)	Acc@5  93.75 ( 94.13)
Epoch: [31][ 70/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.1696e-01 (8.9736e-01)	Acc@1  77.34 ( 73.33)	Acc@5  96.09 ( 94.28)
Epoch: [31][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.6455e-01 (8.8831e-01)	Acc@1  81.25 ( 73.69)	Acc@5  96.88 ( 94.38)
Epoch: [31][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.0241e-01 (8.8790e-01)	Acc@1  71.88 ( 73.74)	Acc@5  96.09 ( 94.45)
Epoch: [31][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.1526e+00 (8.9608e-01)	Acc@1  66.41 ( 73.48)	Acc@5  90.62 ( 94.43)
Epoch: [31][110/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 9.1064e-01 (8.9685e-01)	Acc@1  73.44 ( 73.35)	Acc@5  93.75 ( 94.41)
Epoch: [31][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2500e-01 (8.9383e-01)	Acc@1  76.56 ( 73.44)	Acc@5  96.88 ( 94.47)
Epoch: [31][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2615e-01 (8.9678e-01)	Acc@1  78.12 ( 73.29)	Acc@5  94.53 ( 94.48)
Epoch: [31][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8918e-01 (8.9391e-01)	Acc@1  67.97 ( 73.36)	Acc@5  93.75 ( 94.50)
Epoch: [31][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8308e-01 (8.9208e-01)	Acc@1  71.88 ( 73.41)	Acc@5  97.66 ( 94.50)
Epoch: [31][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9708e-01 (8.9237e-01)	Acc@1  67.19 ( 73.33)	Acc@5  93.75 ( 94.50)
Epoch: [31][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8247e-01 (8.9471e-01)	Acc@1  71.88 ( 73.27)	Acc@5  92.19 ( 94.45)
Epoch: [31][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0902e+00 (8.9554e-01)	Acc@1  68.75 ( 73.26)	Acc@5  91.41 ( 94.40)
Epoch: [31][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8999e-01 (8.9833e-01)	Acc@1  71.09 ( 73.14)	Acc@5  96.88 ( 94.40)
Epoch: [31][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1297e-01 (8.9660e-01)	Acc@1  74.22 ( 73.20)	Acc@5  95.31 ( 94.41)
Epoch: [31][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1792e+00 (8.9691e-01)	Acc@1  66.41 ( 73.13)	Acc@5  92.19 ( 94.43)
Epoch: [31][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7474e-01 (8.9793e-01)	Acc@1  68.75 ( 73.11)	Acc@5  96.88 ( 94.44)
Epoch: [31][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8358e-01 (8.9619e-01)	Acc@1  72.66 ( 73.20)	Acc@5  92.19 ( 94.44)
Epoch: [31][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9793e-01 (8.9854e-01)	Acc@1  67.97 ( 73.17)	Acc@5  92.97 ( 94.42)
Epoch: [31][250/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5012e-01 (8.9909e-01)	Acc@1  76.56 ( 73.14)	Acc@5  94.53 ( 94.43)
Epoch: [31][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2015e-01 (9.0038e-01)	Acc@1  79.69 ( 73.10)	Acc@5  94.53 ( 94.38)
Epoch: [31][270/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0831e-01 (9.0073e-01)	Acc@1  75.78 ( 73.08)	Acc@5  96.09 ( 94.36)
Epoch: [31][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6883e-01 (8.9943e-01)	Acc@1  72.66 ( 73.10)	Acc@5  95.31 ( 94.38)
Epoch: [31][290/391]	Time  0.064 ( 0.066)	Data  0.002 ( 0.002)	Loss 8.6363e-01 (8.9824e-01)	Acc@1  72.66 ( 73.12)	Acc@5  94.53 ( 94.42)
Epoch: [31][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3424e-01 (8.9740e-01)	Acc@1  74.22 ( 73.16)	Acc@5  93.75 ( 94.41)
Epoch: [31][310/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5190e-01 (8.9782e-01)	Acc@1  78.91 ( 73.13)	Acc@5  93.75 ( 94.44)
Epoch: [31][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6058e-01 (8.9669e-01)	Acc@1  75.00 ( 73.21)	Acc@5  89.84 ( 94.42)
Epoch: [31][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0058e+00 (8.9588e-01)	Acc@1  71.09 ( 73.23)	Acc@5  94.53 ( 94.46)
Epoch: [31][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8809e-01 (8.9620e-01)	Acc@1  76.56 ( 73.25)	Acc@5  96.09 ( 94.43)
Epoch: [31][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.4366e-01 (8.9622e-01)	Acc@1  73.44 ( 73.24)	Acc@5  94.53 ( 94.43)
Epoch: [31][360/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3111e-01 (8.9561e-01)	Acc@1  78.12 ( 73.26)	Acc@5  96.88 ( 94.45)
Epoch: [31][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5738e-01 (8.9577e-01)	Acc@1  75.00 ( 73.24)	Acc@5  93.75 ( 94.46)
Epoch: [31][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5942e-01 (8.9562e-01)	Acc@1  77.34 ( 73.25)	Acc@5  96.88 ( 94.49)
Epoch: [31][390/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.1356e+00 (8.9521e-01)	Acc@1  68.75 ( 73.30)	Acc@5  92.50 ( 94.49)
## e[31] optimizer.zero_grad (sum) time: 0.41185426712036133
## e[31]       loss.backward (sum) time: 6.962064266204834
## e[31]      optimizer.step (sum) time: 3.5333259105682373
## epoch[31] training(only) time: 25.668528079986572
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2491e+00 (1.2491e+00)	Acc@1  66.00 ( 66.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.029 ( 0.040)	Loss 1.2849e+00 (1.2099e+00)	Acc@1  66.00 ( 67.45)	Acc@5  91.00 ( 89.27)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.1345e+00 (1.1783e+00)	Acc@1  71.00 ( 67.52)	Acc@5  88.00 ( 90.38)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 1.3533e+00 (1.1867e+00)	Acc@1  60.00 ( 66.90)	Acc@5  88.00 ( 90.26)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.1383e+00 (1.1753e+00)	Acc@1  67.00 ( 66.68)	Acc@5  93.00 ( 90.68)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.1294e+00 (1.1875e+00)	Acc@1  70.00 ( 66.29)	Acc@5  89.00 ( 90.47)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2494e+00 (1.1820e+00)	Acc@1  62.00 ( 66.36)	Acc@5  91.00 ( 90.57)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1875e+00 (1.1902e+00)	Acc@1  65.00 ( 66.18)	Acc@5  90.00 ( 90.45)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.3070e+00 (1.1971e+00)	Acc@1  66.00 ( 65.98)	Acc@5  89.00 ( 90.19)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.7185e+00 (1.1930e+00)	Acc@1  60.00 ( 66.12)	Acc@5  86.00 ( 90.20)
 * Acc@1 66.270 Acc@5 90.300
### epoch[31] execution time: 28.515564441680908
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.218 ( 0.218)	Data  0.144 ( 0.144)	Loss 8.8661e-01 (8.8661e-01)	Acc@1  74.22 ( 74.22)	Acc@5  93.75 ( 93.75)
Epoch: [32][ 10/391]	Time  0.070 ( 0.079)	Data  0.001 ( 0.014)	Loss 8.9507e-01 (8.8113e-01)	Acc@1  74.22 ( 73.37)	Acc@5  96.09 ( 94.96)
Epoch: [32][ 20/391]	Time  0.067 ( 0.072)	Data  0.001 ( 0.008)	Loss 7.8554e-01 (8.9239e-01)	Acc@1  77.34 ( 73.33)	Acc@5  95.31 ( 94.57)
Epoch: [32][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 7.6480e-01 (8.7634e-01)	Acc@1  76.56 ( 73.49)	Acc@5  95.31 ( 94.86)
Epoch: [32][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 8.7718e-01 (8.7164e-01)	Acc@1  71.09 ( 73.23)	Acc@5  94.53 ( 95.06)
Epoch: [32][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.8091e-01 (8.7244e-01)	Acc@1  73.44 ( 73.30)	Acc@5  93.75 ( 95.02)
Epoch: [32][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 9.9273e-01 (8.8517e-01)	Acc@1  70.31 ( 72.96)	Acc@5  91.41 ( 94.86)
Epoch: [32][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.3827e-01 (8.7814e-01)	Acc@1  78.12 ( 73.25)	Acc@5  96.09 ( 94.96)
Epoch: [32][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.4632e-01 (8.7622e-01)	Acc@1  70.31 ( 73.33)	Acc@5  96.09 ( 94.88)
Epoch: [32][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.4307e-01 (8.7852e-01)	Acc@1  75.78 ( 73.44)	Acc@5  92.97 ( 94.75)
Epoch: [32][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.3954e-01 (8.7143e-01)	Acc@1  74.22 ( 73.61)	Acc@5  93.75 ( 94.86)
Epoch: [32][110/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 8.0253e-01 (8.6948e-01)	Acc@1  75.00 ( 73.81)	Acc@5  94.53 ( 94.80)
Epoch: [32][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.2991e-01 (8.6492e-01)	Acc@1  74.22 ( 74.02)	Acc@5  92.19 ( 94.87)
Epoch: [32][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1409e-01 (8.6272e-01)	Acc@1  69.53 ( 74.06)	Acc@5  93.75 ( 94.93)
Epoch: [32][140/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7854e-01 (8.6044e-01)	Acc@1  76.56 ( 74.13)	Acc@5  94.53 ( 94.94)
Epoch: [32][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2872e-01 (8.6243e-01)	Acc@1  74.22 ( 74.09)	Acc@5  96.09 ( 94.93)
Epoch: [32][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7961e-01 (8.6173e-01)	Acc@1  72.66 ( 74.15)	Acc@5  92.97 ( 94.90)
Epoch: [32][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9456e-01 (8.6611e-01)	Acc@1  72.66 ( 73.99)	Acc@5  95.31 ( 94.83)
Epoch: [32][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0534e-01 (8.6424e-01)	Acc@1  74.22 ( 73.97)	Acc@5  89.84 ( 94.84)
Epoch: [32][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7650e-01 (8.6501e-01)	Acc@1  78.12 ( 73.94)	Acc@5  91.41 ( 94.77)
Epoch: [32][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7413e-01 (8.6395e-01)	Acc@1  74.22 ( 73.96)	Acc@5  96.09 ( 94.79)
Epoch: [32][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3296e-01 (8.6493e-01)	Acc@1  78.12 ( 73.94)	Acc@5  92.97 ( 94.77)
Epoch: [32][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9755e-01 (8.6603e-01)	Acc@1  78.91 ( 73.94)	Acc@5  94.53 ( 94.74)
Epoch: [32][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5752e-01 (8.6514e-01)	Acc@1  78.91 ( 73.89)	Acc@5  92.19 ( 94.78)
Epoch: [32][240/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5810e-01 (8.6367e-01)	Acc@1  77.34 ( 73.98)	Acc@5  96.88 ( 94.76)
Epoch: [32][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1518e-01 (8.6401e-01)	Acc@1  80.47 ( 73.97)	Acc@5  98.44 ( 94.76)
Epoch: [32][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9033e-01 (8.6376e-01)	Acc@1  76.56 ( 73.98)	Acc@5  96.88 ( 94.78)
Epoch: [32][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9477e-01 (8.6386e-01)	Acc@1  69.53 ( 73.98)	Acc@5  94.53 ( 94.78)
Epoch: [32][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9799e-01 (8.6463e-01)	Acc@1  75.00 ( 73.97)	Acc@5  91.41 ( 94.77)
Epoch: [32][290/391]	Time  0.068 ( 0.066)	Data  0.002 ( 0.002)	Loss 8.5287e-01 (8.6162e-01)	Acc@1  74.22 ( 74.02)	Acc@5  96.09 ( 94.84)
Epoch: [32][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2177e-01 (8.6186e-01)	Acc@1  77.34 ( 74.04)	Acc@5  93.75 ( 94.83)
Epoch: [32][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2812e-01 (8.6141e-01)	Acc@1  74.22 ( 73.99)	Acc@5  95.31 ( 94.85)
Epoch: [32][320/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3798e-01 (8.6033e-01)	Acc@1  81.25 ( 74.08)	Acc@5  92.97 ( 94.84)
Epoch: [32][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4924e-01 (8.6230e-01)	Acc@1  77.34 ( 74.04)	Acc@5  96.88 ( 94.81)
Epoch: [32][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2527e-01 (8.6211e-01)	Acc@1  73.44 ( 74.02)	Acc@5  94.53 ( 94.83)
Epoch: [32][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1803e-01 (8.6266e-01)	Acc@1  73.44 ( 74.00)	Acc@5  93.75 ( 94.82)
Epoch: [32][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4842e-01 (8.6359e-01)	Acc@1  74.22 ( 73.98)	Acc@5  97.66 ( 94.83)
Epoch: [32][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0754e-01 (8.6310e-01)	Acc@1  75.00 ( 74.04)	Acc@5  93.75 ( 94.83)
Epoch: [32][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3914e-01 (8.6184e-01)	Acc@1  69.53 ( 74.04)	Acc@5  99.22 ( 94.85)
Epoch: [32][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.0661e-01 (8.6147e-01)	Acc@1  72.50 ( 74.06)	Acc@5  92.50 ( 94.86)
## e[32] optimizer.zero_grad (sum) time: 0.41913819313049316
## e[32]       loss.backward (sum) time: 7.033982276916504
## e[32]      optimizer.step (sum) time: 3.415734052658081
## epoch[32] training(only) time: 25.595043420791626
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.2163e+00 (1.2163e+00)	Acc@1  72.00 ( 72.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.040)	Loss 1.2794e+00 (1.2006e+00)	Acc@1  64.00 ( 66.73)	Acc@5  90.00 ( 89.64)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.1661e+00 (1.1722e+00)	Acc@1  68.00 ( 66.81)	Acc@5  91.00 ( 90.71)
Test: [ 30/100]	Time  0.028 ( 0.032)	Loss 1.3426e+00 (1.1771e+00)	Acc@1  60.00 ( 66.87)	Acc@5  89.00 ( 90.52)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1465e+00 (1.1680e+00)	Acc@1  65.00 ( 66.78)	Acc@5  91.00 ( 90.90)
Test: [ 50/100]	Time  0.026 ( 0.030)	Loss 1.1045e+00 (1.1793e+00)	Acc@1  69.00 ( 66.31)	Acc@5  89.00 ( 90.71)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3051e+00 (1.1739e+00)	Acc@1  62.00 ( 66.34)	Acc@5  91.00 ( 90.70)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.1995e+00 (1.1849e+00)	Acc@1  70.00 ( 66.37)	Acc@5  90.00 ( 90.62)
Test: [ 80/100]	Time  0.031 ( 0.028)	Loss 1.2692e+00 (1.1919e+00)	Acc@1  66.00 ( 66.11)	Acc@5  91.00 ( 90.46)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.7373e+00 (1.1873e+00)	Acc@1  60.00 ( 66.35)	Acc@5  85.00 ( 90.38)
 * Acc@1 66.580 Acc@5 90.450
### epoch[32] execution time: 28.445467472076416
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.216 ( 0.216)	Data  0.147 ( 0.147)	Loss 6.6606e-01 (6.6606e-01)	Acc@1  80.47 ( 80.47)	Acc@5  96.88 ( 96.88)
Epoch: [33][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.014)	Loss 7.7831e-01 (8.1339e-01)	Acc@1  78.12 ( 76.35)	Acc@5  93.75 ( 95.03)
Epoch: [33][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.008)	Loss 7.1687e-01 (8.3343e-01)	Acc@1  79.69 ( 75.63)	Acc@5  94.53 ( 94.79)
Epoch: [33][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 7.4672e-01 (8.3184e-01)	Acc@1  77.34 ( 75.73)	Acc@5  96.88 ( 94.81)
Epoch: [33][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 8.3233e-01 (8.3251e-01)	Acc@1  74.22 ( 75.42)	Acc@5  96.09 ( 94.99)
Epoch: [33][ 50/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.2562e-01 (8.2976e-01)	Acc@1  75.00 ( 75.51)	Acc@5  96.88 ( 95.05)
Epoch: [33][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.2990e-01 (8.2589e-01)	Acc@1  78.91 ( 75.45)	Acc@5  96.88 ( 95.08)
Epoch: [33][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.4750e-01 (8.2537e-01)	Acc@1  75.78 ( 75.48)	Acc@5  94.53 ( 95.09)
Epoch: [33][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.6251e-01 (8.2529e-01)	Acc@1  77.34 ( 75.38)	Acc@5  96.09 ( 95.15)
Epoch: [33][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.4851e-01 (8.2321e-01)	Acc@1  76.56 ( 75.39)	Acc@5  96.88 ( 95.18)
Epoch: [33][100/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.9853e-01 (8.1919e-01)	Acc@1  75.00 ( 75.48)	Acc@5  94.53 ( 95.31)
Epoch: [33][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0473e-01 (8.1742e-01)	Acc@1  78.12 ( 75.47)	Acc@5  97.66 ( 95.34)
Epoch: [33][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.4804e-01 (8.1602e-01)	Acc@1  74.22 ( 75.48)	Acc@5  94.53 ( 95.33)
Epoch: [33][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9998e-01 (8.1913e-01)	Acc@1  78.91 ( 75.35)	Acc@5  94.53 ( 95.29)
Epoch: [33][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7731e-01 (8.1669e-01)	Acc@1  74.22 ( 75.35)	Acc@5  90.62 ( 95.35)
Epoch: [33][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2664e-01 (8.2132e-01)	Acc@1  77.34 ( 75.21)	Acc@5  93.75 ( 95.26)
Epoch: [33][160/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8259e-01 (8.2288e-01)	Acc@1  75.78 ( 75.10)	Acc@5  96.88 ( 95.29)
Epoch: [33][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3773e-01 (8.2409e-01)	Acc@1  78.91 ( 75.08)	Acc@5  92.97 ( 95.27)
Epoch: [33][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6478e-01 (8.2496e-01)	Acc@1  75.78 ( 75.13)	Acc@5  92.19 ( 95.25)
Epoch: [33][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0602e-01 (8.2706e-01)	Acc@1  75.78 ( 75.03)	Acc@5  95.31 ( 95.21)
Epoch: [33][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6795e-01 (8.2817e-01)	Acc@1  74.22 ( 74.98)	Acc@5  96.09 ( 95.22)
Epoch: [33][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5724e-01 (8.3062e-01)	Acc@1  79.69 ( 74.92)	Acc@5  96.09 ( 95.22)
Epoch: [33][220/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0516e-01 (8.3058e-01)	Acc@1  73.44 ( 74.88)	Acc@5  94.53 ( 95.22)
Epoch: [33][230/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8204e-01 (8.2947e-01)	Acc@1  69.53 ( 74.97)	Acc@5  95.31 ( 95.26)
Epoch: [33][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2047e-01 (8.2713e-01)	Acc@1  80.47 ( 75.06)	Acc@5  97.66 ( 95.24)
Epoch: [33][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0467e+00 (8.3149e-01)	Acc@1  70.31 ( 74.96)	Acc@5  92.97 ( 95.18)
Epoch: [33][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3291e-01 (8.3219e-01)	Acc@1  78.12 ( 74.93)	Acc@5  95.31 ( 95.17)
Epoch: [33][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7942e-01 (8.3114e-01)	Acc@1  81.25 ( 74.92)	Acc@5  98.44 ( 95.18)
Epoch: [33][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.4736e-01 (8.3014e-01)	Acc@1  72.66 ( 74.94)	Acc@5  94.53 ( 95.21)
Epoch: [33][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0252e+00 (8.3027e-01)	Acc@1  69.53 ( 74.95)	Acc@5  91.41 ( 95.21)
Epoch: [33][300/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4408e-01 (8.3040e-01)	Acc@1  78.12 ( 74.98)	Acc@5  94.53 ( 95.20)
Epoch: [33][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9876e-01 (8.3126e-01)	Acc@1  68.75 ( 74.96)	Acc@5  96.09 ( 95.20)
Epoch: [33][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8290e-01 (8.3284e-01)	Acc@1  75.00 ( 74.93)	Acc@5  92.97 ( 95.15)
Epoch: [33][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1653e-01 (8.3323e-01)	Acc@1  78.12 ( 74.93)	Acc@5  92.19 ( 95.13)
Epoch: [33][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3401e-01 (8.3256e-01)	Acc@1  75.78 ( 74.97)	Acc@5  93.75 ( 95.12)
Epoch: [33][350/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9474e-01 (8.3200e-01)	Acc@1  81.25 ( 74.99)	Acc@5  95.31 ( 95.12)
Epoch: [33][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0515e-01 (8.3432e-01)	Acc@1  74.22 ( 74.93)	Acc@5  94.53 ( 95.07)
Epoch: [33][370/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.4811e-01 (8.3391e-01)	Acc@1  69.53 ( 74.94)	Acc@5  95.31 ( 95.07)
Epoch: [33][380/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0193e-01 (8.3324e-01)	Acc@1  76.56 ( 74.93)	Acc@5  96.88 ( 95.08)
Epoch: [33][390/391]	Time  0.054 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3229e-01 (8.3385e-01)	Acc@1  73.75 ( 74.92)	Acc@5  95.00 ( 95.07)
## e[33] optimizer.zero_grad (sum) time: 0.4085869789123535
## e[33]       loss.backward (sum) time: 7.037497282028198
## e[33]      optimizer.step (sum) time: 3.459324836730957
## epoch[33] training(only) time: 25.64436149597168
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2615e+00 (1.2615e+00)	Acc@1  70.00 ( 70.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.3244e+00 (1.2079e+00)	Acc@1  64.00 ( 66.73)	Acc@5  90.00 ( 89.18)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.1648e+00 (1.1681e+00)	Acc@1  70.00 ( 66.95)	Acc@5  93.00 ( 90.67)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.3198e+00 (1.1733e+00)	Acc@1  59.00 ( 66.84)	Acc@5  89.00 ( 90.42)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.1007e+00 (1.1654e+00)	Acc@1  63.00 ( 66.54)	Acc@5  92.00 ( 90.76)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.0795e+00 (1.1771e+00)	Acc@1  73.00 ( 66.16)	Acc@5  90.00 ( 90.65)
Test: [ 60/100]	Time  0.026 ( 0.028)	Loss 1.2562e+00 (1.1749e+00)	Acc@1  62.00 ( 66.28)	Acc@5  90.00 ( 90.64)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.2312e+00 (1.1858e+00)	Acc@1  68.00 ( 66.31)	Acc@5  88.00 ( 90.51)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.2850e+00 (1.1923e+00)	Acc@1  67.00 ( 66.23)	Acc@5  90.00 ( 90.36)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7307e+00 (1.1872e+00)	Acc@1  59.00 ( 66.41)	Acc@5  85.00 ( 90.32)
 * Acc@1 66.590 Acc@5 90.420
### epoch[33] execution time: 28.482263803482056
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.209 ( 0.209)	Data  0.120 ( 0.120)	Loss 9.4795e-01 (9.4795e-01)	Acc@1  71.88 ( 71.88)	Acc@5  93.75 ( 93.75)
Epoch: [34][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.012)	Loss 8.0942e-01 (7.7792e-01)	Acc@1  76.56 ( 77.13)	Acc@5  95.31 ( 95.24)
Epoch: [34][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.007)	Loss 8.5135e-01 (7.8840e-01)	Acc@1  75.78 ( 76.38)	Acc@5  96.88 ( 95.65)
Epoch: [34][ 30/391]	Time  0.066 ( 0.071)	Data  0.001 ( 0.005)	Loss 7.6415e-01 (7.8258e-01)	Acc@1  77.34 ( 76.74)	Acc@5  96.88 ( 95.69)
Epoch: [34][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 6.6163e-01 (7.7924e-01)	Acc@1  80.47 ( 76.87)	Acc@5  97.66 ( 95.67)
Epoch: [34][ 50/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 1.0642e+00 (7.8483e-01)	Acc@1  70.31 ( 76.75)	Acc@5  91.41 ( 95.56)
Epoch: [34][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.7260e-01 (7.9084e-01)	Acc@1  76.56 ( 76.47)	Acc@5  94.53 ( 95.49)
Epoch: [34][ 70/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 7.3734e-01 (7.9935e-01)	Acc@1  75.00 ( 76.12)	Acc@5  96.88 ( 95.37)
Epoch: [34][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.8992e-01 (7.9700e-01)	Acc@1  75.78 ( 76.15)	Acc@5  96.09 ( 95.37)
Epoch: [34][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.2953e-01 (7.9870e-01)	Acc@1  75.78 ( 76.00)	Acc@5  94.53 ( 95.34)
Epoch: [34][100/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.4884e-01 (7.9758e-01)	Acc@1  79.69 ( 75.90)	Acc@5  99.22 ( 95.42)
Epoch: [34][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 1.0197e+00 (8.0066e-01)	Acc@1  67.97 ( 75.85)	Acc@5  92.19 ( 95.38)
Epoch: [34][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6411e-01 (8.0367e-01)	Acc@1  73.44 ( 75.74)	Acc@5  97.66 ( 95.38)
Epoch: [34][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3598e-01 (7.9999e-01)	Acc@1  75.00 ( 75.79)	Acc@5  95.31 ( 95.42)
Epoch: [34][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4678e-01 (8.0143e-01)	Acc@1  76.56 ( 75.79)	Acc@5  96.09 ( 95.42)
Epoch: [34][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1274e-01 (7.9783e-01)	Acc@1  74.22 ( 75.84)	Acc@5  96.88 ( 95.49)
Epoch: [34][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7076e-01 (8.0245e-01)	Acc@1  69.53 ( 75.64)	Acc@5  93.75 ( 95.43)
Epoch: [34][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0152e+00 (8.0228e-01)	Acc@1  73.44 ( 75.58)	Acc@5  89.84 ( 95.42)
Epoch: [34][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1582e-01 (8.0132e-01)	Acc@1  75.78 ( 75.63)	Acc@5  95.31 ( 95.41)
Epoch: [34][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6210e-01 (8.0376e-01)	Acc@1  69.53 ( 75.51)	Acc@5  91.41 ( 95.36)
Epoch: [34][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5616e-01 (8.0568e-01)	Acc@1  82.81 ( 75.48)	Acc@5  96.09 ( 95.31)
Epoch: [34][210/391]	Time  0.076 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3592e-01 (8.0710e-01)	Acc@1  80.47 ( 75.47)	Acc@5  98.44 ( 95.34)
Epoch: [34][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1661e-01 (8.0847e-01)	Acc@1  67.19 ( 75.41)	Acc@5  97.66 ( 95.33)
Epoch: [34][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1234e-01 (8.0916e-01)	Acc@1  74.22 ( 75.43)	Acc@5  96.09 ( 95.32)
Epoch: [34][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8095e-01 (8.1040e-01)	Acc@1  81.25 ( 75.41)	Acc@5  96.09 ( 95.30)
Epoch: [34][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8144e-01 (8.1128e-01)	Acc@1  75.78 ( 75.41)	Acc@5  97.66 ( 95.31)
Epoch: [34][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2221e-01 (8.1022e-01)	Acc@1  75.00 ( 75.45)	Acc@5  96.09 ( 95.31)
Epoch: [34][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2757e-01 (8.0951e-01)	Acc@1  78.12 ( 75.46)	Acc@5  92.97 ( 95.32)
Epoch: [34][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0381e-01 (8.1116e-01)	Acc@1  76.56 ( 75.47)	Acc@5  93.75 ( 95.30)
Epoch: [34][290/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7309e-01 (8.0964e-01)	Acc@1  75.00 ( 75.51)	Acc@5  94.53 ( 95.33)
Epoch: [34][300/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3411e-01 (8.1126e-01)	Acc@1  75.78 ( 75.49)	Acc@5  93.75 ( 95.28)
Epoch: [34][310/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5173e-01 (8.1034e-01)	Acc@1  67.19 ( 75.49)	Acc@5  96.88 ( 95.29)
Epoch: [34][320/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.6816e-01 (8.0991e-01)	Acc@1  75.78 ( 75.50)	Acc@5  99.22 ( 95.31)
Epoch: [34][330/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.4920e-01 (8.1009e-01)	Acc@1  71.09 ( 75.51)	Acc@5  92.97 ( 95.31)
Epoch: [34][340/391]	Time  0.071 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1451e-01 (8.1064e-01)	Acc@1  78.91 ( 75.49)	Acc@5  96.09 ( 95.30)
Epoch: [34][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0261e+00 (8.0951e-01)	Acc@1  66.41 ( 75.51)	Acc@5  92.97 ( 95.31)
Epoch: [34][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3448e-01 (8.0897e-01)	Acc@1  75.00 ( 75.55)	Acc@5  93.75 ( 95.31)
Epoch: [34][370/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.1931e-01 (8.0995e-01)	Acc@1  72.66 ( 75.55)	Acc@5  93.75 ( 95.29)
Epoch: [34][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.001)	Loss 7.6399e-01 (8.0959e-01)	Acc@1  74.22 ( 75.54)	Acc@5  96.09 ( 95.30)
Epoch: [34][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.001)	Loss 1.2998e+00 (8.1082e-01)	Acc@1  63.75 ( 75.51)	Acc@5  90.00 ( 95.28)
## e[34] optimizer.zero_grad (sum) time: 0.40817689895629883
## e[34]       loss.backward (sum) time: 7.011378288269043
## e[34]      optimizer.step (sum) time: 3.456429958343506
## epoch[34] training(only) time: 25.591851472854614
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2048e+00 (1.2048e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.2954e+00 (1.2017e+00)	Acc@1  66.00 ( 67.27)	Acc@5  89.00 ( 88.91)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.1641e+00 (1.1576e+00)	Acc@1  69.00 ( 67.14)	Acc@5  94.00 ( 90.62)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 1.3863e+00 (1.1757e+00)	Acc@1  62.00 ( 67.03)	Acc@5  90.00 ( 90.42)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.0968e+00 (1.1715e+00)	Acc@1  65.00 ( 66.63)	Acc@5  93.00 ( 90.73)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.1287e+00 (1.1814e+00)	Acc@1  71.00 ( 66.41)	Acc@5  89.00 ( 90.51)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2281e+00 (1.1746e+00)	Acc@1  65.00 ( 66.41)	Acc@5  90.00 ( 90.67)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1742e+00 (1.1844e+00)	Acc@1  67.00 ( 66.52)	Acc@5  92.00 ( 90.52)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.3461e+00 (1.1907e+00)	Acc@1  61.00 ( 66.19)	Acc@5  91.00 ( 90.38)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.7440e+00 (1.1865e+00)	Acc@1  58.00 ( 66.35)	Acc@5  84.00 ( 90.35)
 * Acc@1 66.530 Acc@5 90.400
### epoch[34] execution time: 28.44626522064209
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.210 ( 0.210)	Data  0.141 ( 0.141)	Loss 8.7970e-01 (8.7970e-01)	Acc@1  72.66 ( 72.66)	Acc@5  94.53 ( 94.53)
Epoch: [35][ 10/391]	Time  0.068 ( 0.080)	Data  0.001 ( 0.014)	Loss 6.3268e-01 (7.8407e-01)	Acc@1  82.81 ( 76.63)	Acc@5  97.66 ( 95.45)
Epoch: [35][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 7.1694e-01 (7.8494e-01)	Acc@1  76.56 ( 77.23)	Acc@5  96.88 ( 95.46)
Epoch: [35][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 6.1003e-01 (7.7893e-01)	Acc@1  82.03 ( 77.07)	Acc@5  98.44 ( 95.36)
Epoch: [35][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 7.1923e-01 (7.8207e-01)	Acc@1  80.47 ( 76.94)	Acc@5  96.09 ( 95.39)
Epoch: [35][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.6478e-01 (7.8425e-01)	Acc@1  78.12 ( 76.79)	Acc@5  96.09 ( 95.40)
Epoch: [35][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.9737e-01 (7.9078e-01)	Acc@1  75.00 ( 76.38)	Acc@5  95.31 ( 95.36)
Epoch: [35][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.7678e-01 (7.8789e-01)	Acc@1  75.78 ( 76.39)	Acc@5  93.75 ( 95.36)
Epoch: [35][ 80/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.7339e-01 (7.8541e-01)	Acc@1  78.91 ( 76.42)	Acc@5  95.31 ( 95.45)
Epoch: [35][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.0883e-01 (7.8621e-01)	Acc@1  71.09 ( 76.31)	Acc@5  96.88 ( 95.51)
Epoch: [35][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.1858e-01 (7.8826e-01)	Acc@1  73.44 ( 76.21)	Acc@5  95.31 ( 95.45)
Epoch: [35][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0394e+00 (7.8827e-01)	Acc@1  67.97 ( 76.25)	Acc@5  93.75 ( 95.45)
Epoch: [35][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4653e-01 (7.9058e-01)	Acc@1  78.12 ( 76.21)	Acc@5  95.31 ( 95.38)
Epoch: [35][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1231e-01 (7.9344e-01)	Acc@1  73.44 ( 76.10)	Acc@5  93.75 ( 95.31)
Epoch: [35][140/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3624e-01 (7.8673e-01)	Acc@1  79.69 ( 76.27)	Acc@5  93.75 ( 95.37)
Epoch: [35][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9897e-01 (7.8345e-01)	Acc@1  73.44 ( 76.38)	Acc@5  90.62 ( 95.41)
Epoch: [35][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9437e-01 (7.8293e-01)	Acc@1  74.22 ( 76.50)	Acc@5  96.88 ( 95.41)
Epoch: [35][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1918e-01 (7.8555e-01)	Acc@1  79.69 ( 76.46)	Acc@5  96.88 ( 95.37)
Epoch: [35][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6092e-01 (7.8641e-01)	Acc@1  74.22 ( 76.46)	Acc@5  92.97 ( 95.39)
Epoch: [35][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.4525e-01 (7.8730e-01)	Acc@1  75.00 ( 76.42)	Acc@5  95.31 ( 95.39)
Epoch: [35][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7202e-01 (7.8724e-01)	Acc@1  74.22 ( 76.40)	Acc@5  96.09 ( 95.43)
Epoch: [35][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7265e-01 (7.8445e-01)	Acc@1  77.34 ( 76.52)	Acc@5  95.31 ( 95.47)
Epoch: [35][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9692e-01 (7.8419e-01)	Acc@1  75.78 ( 76.48)	Acc@5  95.31 ( 95.48)
Epoch: [35][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7203e-01 (7.8442e-01)	Acc@1  71.09 ( 76.41)	Acc@5  96.09 ( 95.51)
Epoch: [35][240/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.9676e-01 (7.8728e-01)	Acc@1  69.53 ( 76.35)	Acc@5  90.62 ( 95.46)
Epoch: [35][250/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5727e-01 (7.8646e-01)	Acc@1  70.31 ( 76.39)	Acc@5  92.19 ( 95.47)
Epoch: [35][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5505e-01 (7.8490e-01)	Acc@1  75.00 ( 76.39)	Acc@5  93.75 ( 95.47)
Epoch: [35][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7692e-01 (7.8475e-01)	Acc@1  77.34 ( 76.43)	Acc@5  97.66 ( 95.47)
Epoch: [35][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7905e-01 (7.8466e-01)	Acc@1  77.34 ( 76.49)	Acc@5  95.31 ( 95.47)
Epoch: [35][290/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1058e+00 (7.8618e-01)	Acc@1  68.75 ( 76.44)	Acc@5  92.97 ( 95.47)
Epoch: [35][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9567e-01 (7.8541e-01)	Acc@1  78.91 ( 76.43)	Acc@5  94.53 ( 95.47)
Epoch: [35][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6046e-01 (7.8501e-01)	Acc@1  78.12 ( 76.46)	Acc@5  96.09 ( 95.48)
Epoch: [35][320/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.7567e-01 (7.8686e-01)	Acc@1  77.34 ( 76.42)	Acc@5  95.31 ( 95.46)
Epoch: [35][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9726e-01 (7.8600e-01)	Acc@1  79.69 ( 76.44)	Acc@5  96.09 ( 95.47)
Epoch: [35][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1814e-01 (7.8685e-01)	Acc@1  77.34 ( 76.41)	Acc@5  97.66 ( 95.48)
Epoch: [35][350/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7543e-01 (7.8692e-01)	Acc@1  70.31 ( 76.39)	Acc@5  95.31 ( 95.48)
Epoch: [35][360/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0314e-01 (7.8681e-01)	Acc@1  71.88 ( 76.40)	Acc@5  96.09 ( 95.49)
Epoch: [35][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5110e-01 (7.8713e-01)	Acc@1  79.69 ( 76.40)	Acc@5  97.66 ( 95.48)
Epoch: [35][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4302e-01 (7.8662e-01)	Acc@1  75.00 ( 76.44)	Acc@5  97.66 ( 95.48)
Epoch: [35][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6506e-01 (7.8650e-01)	Acc@1  80.00 ( 76.43)	Acc@5  97.50 ( 95.50)
## e[35] optimizer.zero_grad (sum) time: 0.4167060852050781
## e[35]       loss.backward (sum) time: 7.014033079147339
## e[35]      optimizer.step (sum) time: 3.4586048126220703
## epoch[35] training(only) time: 25.654062271118164
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.2096e+00 (1.2096e+00)	Acc@1  65.00 ( 65.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.029 ( 0.040)	Loss 1.3331e+00 (1.2192e+00)	Acc@1  67.00 ( 66.45)	Acc@5  88.00 ( 89.45)
Test: [ 20/100]	Time  0.026 ( 0.034)	Loss 1.1561e+00 (1.1725e+00)	Acc@1  71.00 ( 67.29)	Acc@5  93.00 ( 90.76)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.3450e+00 (1.1769e+00)	Acc@1  63.00 ( 67.55)	Acc@5  88.00 ( 90.68)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.0804e+00 (1.1685e+00)	Acc@1  64.00 ( 67.12)	Acc@5  94.00 ( 91.10)
Test: [ 50/100]	Time  0.025 ( 0.030)	Loss 1.0568e+00 (1.1792e+00)	Acc@1  71.00 ( 66.65)	Acc@5  91.00 ( 90.98)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2913e+00 (1.1742e+00)	Acc@1  63.00 ( 66.72)	Acc@5  89.00 ( 90.90)
Test: [ 70/100]	Time  0.029 ( 0.029)	Loss 1.1824e+00 (1.1861e+00)	Acc@1  71.00 ( 66.70)	Acc@5  91.00 ( 90.83)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 1.2917e+00 (1.1917e+00)	Acc@1  61.00 ( 66.53)	Acc@5  92.00 ( 90.69)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.7868e+00 (1.1876e+00)	Acc@1  58.00 ( 66.64)	Acc@5  85.00 ( 90.66)
 * Acc@1 66.730 Acc@5 90.710
### epoch[35] execution time: 28.54998517036438
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.210 ( 0.210)	Data  0.143 ( 0.143)	Loss 7.8833e-01 (7.8833e-01)	Acc@1  73.44 ( 73.44)	Acc@5  96.09 ( 96.09)
Epoch: [36][ 10/391]	Time  0.063 ( 0.078)	Data  0.001 ( 0.014)	Loss 5.6616e-01 (7.0397e-01)	Acc@1  83.59 ( 78.62)	Acc@5  98.44 ( 96.52)
Epoch: [36][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.008)	Loss 6.1643e-01 (7.2974e-01)	Acc@1  81.25 ( 77.94)	Acc@5  96.09 ( 95.83)
Epoch: [36][ 30/391]	Time  0.071 ( 0.069)	Data  0.001 ( 0.006)	Loss 8.8321e-01 (7.5775e-01)	Acc@1  75.78 ( 77.29)	Acc@5  93.75 ( 95.67)
Epoch: [36][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.4348e-01 (7.5542e-01)	Acc@1  85.16 ( 77.29)	Acc@5  99.22 ( 95.77)
Epoch: [36][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.7175e-01 (7.6164e-01)	Acc@1  81.25 ( 77.19)	Acc@5  92.97 ( 95.62)
Epoch: [36][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.0428e-01 (7.6431e-01)	Acc@1  72.66 ( 77.05)	Acc@5  96.09 ( 95.68)
Epoch: [36][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.7638e-01 (7.5965e-01)	Acc@1  74.22 ( 77.07)	Acc@5  94.53 ( 95.77)
Epoch: [36][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.5511e-01 (7.5443e-01)	Acc@1  73.44 ( 77.22)	Acc@5  94.53 ( 95.85)
Epoch: [36][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.4297e-01 (7.5258e-01)	Acc@1  78.12 ( 77.24)	Acc@5  96.09 ( 95.86)
Epoch: [36][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.5181e-01 (7.5770e-01)	Acc@1  82.81 ( 77.10)	Acc@5  98.44 ( 95.85)
Epoch: [36][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8212e-01 (7.5756e-01)	Acc@1  78.12 ( 77.17)	Acc@5  96.09 ( 95.84)
Epoch: [36][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9677e-01 (7.6523e-01)	Acc@1  71.88 ( 76.83)	Acc@5  92.97 ( 95.77)
Epoch: [36][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9615e-01 (7.6398e-01)	Acc@1  78.12 ( 76.83)	Acc@5  96.88 ( 95.80)
Epoch: [36][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0466e-01 (7.6686e-01)	Acc@1  74.22 ( 76.84)	Acc@5  95.31 ( 95.78)
Epoch: [36][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2959e-01 (7.6755e-01)	Acc@1  81.25 ( 76.84)	Acc@5  94.53 ( 95.81)
Epoch: [36][160/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3816e-01 (7.6566e-01)	Acc@1  80.47 ( 76.84)	Acc@5  96.09 ( 95.79)
Epoch: [36][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3707e-01 (7.6451e-01)	Acc@1  71.88 ( 76.91)	Acc@5  96.88 ( 95.79)
Epoch: [36][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3177e-01 (7.6518e-01)	Acc@1  71.09 ( 76.91)	Acc@5  94.53 ( 95.80)
Epoch: [36][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.1107e-01 (7.6438e-01)	Acc@1  76.56 ( 76.90)	Acc@5  93.75 ( 95.81)
Epoch: [36][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0386e-01 (7.6336e-01)	Acc@1  83.59 ( 76.94)	Acc@5  99.22 ( 95.84)
Epoch: [36][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1712e-01 (7.6197e-01)	Acc@1  81.25 ( 77.01)	Acc@5  96.88 ( 95.81)
Epoch: [36][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3283e-01 (7.6507e-01)	Acc@1  77.34 ( 76.90)	Acc@5  97.66 ( 95.76)
Epoch: [36][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.1161e+00 (7.6628e-01)	Acc@1  63.28 ( 76.84)	Acc@5  94.53 ( 95.76)
Epoch: [36][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.2547e-01 (7.6800e-01)	Acc@1  72.66 ( 76.83)	Acc@5  96.09 ( 95.76)
Epoch: [36][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8744e-01 (7.7016e-01)	Acc@1  78.91 ( 76.77)	Acc@5  95.31 ( 95.73)
Epoch: [36][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8406e-01 (7.6984e-01)	Acc@1  75.78 ( 76.79)	Acc@5  95.31 ( 95.73)
Epoch: [36][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3927e-01 (7.6890e-01)	Acc@1  74.22 ( 76.81)	Acc@5  97.66 ( 95.74)
Epoch: [36][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8886e-01 (7.6810e-01)	Acc@1  79.69 ( 76.87)	Acc@5  96.88 ( 95.76)
Epoch: [36][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1451e-01 (7.6745e-01)	Acc@1  75.00 ( 76.87)	Acc@5  96.09 ( 95.76)
Epoch: [36][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.6088e-01 (7.6894e-01)	Acc@1  71.09 ( 76.82)	Acc@5  92.97 ( 95.77)
Epoch: [36][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4294e-01 (7.6837e-01)	Acc@1  78.12 ( 76.78)	Acc@5  99.22 ( 95.79)
Epoch: [36][320/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2458e-01 (7.6689e-01)	Acc@1  82.03 ( 76.80)	Acc@5  98.44 ( 95.81)
Epoch: [36][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3498e-01 (7.6894e-01)	Acc@1  75.78 ( 76.75)	Acc@5  96.88 ( 95.79)
Epoch: [36][340/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0976e-01 (7.6888e-01)	Acc@1  80.47 ( 76.76)	Acc@5  96.88 ( 95.80)
Epoch: [36][350/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5893e-01 (7.6820e-01)	Acc@1  84.38 ( 76.80)	Acc@5  94.53 ( 95.78)
Epoch: [36][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3510e-01 (7.6777e-01)	Acc@1  79.69 ( 76.84)	Acc@5  98.44 ( 95.78)
Epoch: [36][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.8910e-01 (7.6818e-01)	Acc@1  77.34 ( 76.82)	Acc@5  95.31 ( 95.78)
Epoch: [36][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0594e-01 (7.6848e-01)	Acc@1  75.00 ( 76.83)	Acc@5  95.31 ( 95.78)
Epoch: [36][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0148e-01 (7.6784e-01)	Acc@1  82.50 ( 76.84)	Acc@5  96.25 ( 95.79)
## e[36] optimizer.zero_grad (sum) time: 0.40502285957336426
## e[36]       loss.backward (sum) time: 6.980988502502441
## e[36]      optimizer.step (sum) time: 3.503966808319092
## epoch[36] training(only) time: 25.60800313949585
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2246e+00 (1.2246e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.3041e+00 (1.2052e+00)	Acc@1  67.00 ( 67.73)	Acc@5  89.00 ( 89.27)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.1720e+00 (1.1618e+00)	Acc@1  66.00 ( 67.67)	Acc@5  90.00 ( 90.76)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.3922e+00 (1.1762e+00)	Acc@1  62.00 ( 67.00)	Acc@5  87.00 ( 90.65)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.0602e+00 (1.1631e+00)	Acc@1  67.00 ( 67.05)	Acc@5  94.00 ( 91.22)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.0331e+00 (1.1729e+00)	Acc@1  72.00 ( 66.73)	Acc@5  90.00 ( 91.02)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2747e+00 (1.1670e+00)	Acc@1  64.00 ( 66.97)	Acc@5  90.00 ( 91.05)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1427e+00 (1.1759e+00)	Acc@1  72.00 ( 67.11)	Acc@5  91.00 ( 90.90)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 1.2835e+00 (1.1837e+00)	Acc@1  66.00 ( 66.95)	Acc@5  93.00 ( 90.70)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.7060e+00 (1.1792e+00)	Acc@1  57.00 ( 67.03)	Acc@5  86.00 ( 90.70)
 * Acc@1 67.290 Acc@5 90.790
### epoch[36] execution time: 28.410123586654663
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.217 ( 0.217)	Data  0.145 ( 0.145)	Loss 7.9596e-01 (7.9596e-01)	Acc@1  76.56 ( 76.56)	Acc@5  95.31 ( 95.31)
Epoch: [37][ 10/391]	Time  0.063 ( 0.078)	Data  0.001 ( 0.014)	Loss 5.6513e-01 (7.5770e-01)	Acc@1  82.03 ( 77.49)	Acc@5  97.66 ( 96.09)
Epoch: [37][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 6.4296e-01 (7.5014e-01)	Acc@1  82.03 ( 77.31)	Acc@5  96.09 ( 96.06)
Epoch: [37][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 7.1074e-01 (7.5223e-01)	Acc@1  72.66 ( 77.07)	Acc@5  97.66 ( 95.87)
Epoch: [37][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 8.3367e-01 (7.5103e-01)	Acc@1  71.09 ( 77.08)	Acc@5  94.53 ( 95.94)
Epoch: [37][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.3363e-01 (7.4077e-01)	Acc@1  73.44 ( 77.68)	Acc@5  95.31 ( 96.00)
Epoch: [37][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.3314e-01 (7.3733e-01)	Acc@1  85.16 ( 77.83)	Acc@5  97.66 ( 96.07)
Epoch: [37][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.7508e-01 (7.4343e-01)	Acc@1  78.12 ( 77.55)	Acc@5  95.31 ( 95.87)
Epoch: [37][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.7732e-01 (7.4742e-01)	Acc@1  68.75 ( 77.34)	Acc@5  90.62 ( 95.93)
Epoch: [37][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.3928e-01 (7.4407e-01)	Acc@1  82.03 ( 77.41)	Acc@5  96.88 ( 95.96)
Epoch: [37][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 9.0335e-01 (7.4426e-01)	Acc@1  71.88 ( 77.49)	Acc@5  92.97 ( 95.95)
Epoch: [37][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5576e-01 (7.4250e-01)	Acc@1  80.47 ( 77.56)	Acc@5  99.22 ( 96.05)
Epoch: [37][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5725e-01 (7.4333e-01)	Acc@1  85.94 ( 77.58)	Acc@5  96.88 ( 96.04)
Epoch: [37][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6610e-01 (7.4528e-01)	Acc@1  83.59 ( 77.53)	Acc@5  99.22 ( 96.04)
Epoch: [37][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6544e-01 (7.4701e-01)	Acc@1  75.78 ( 77.29)	Acc@5  97.66 ( 96.05)
Epoch: [37][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0461e-01 (7.5095e-01)	Acc@1  76.56 ( 77.16)	Acc@5  96.09 ( 96.04)
Epoch: [37][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5018e-01 (7.4704e-01)	Acc@1  79.69 ( 77.30)	Acc@5  98.44 ( 96.02)
Epoch: [37][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.0069e+00 (7.4845e-01)	Acc@1  67.19 ( 77.28)	Acc@5  93.75 ( 96.01)
Epoch: [37][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2674e-01 (7.4683e-01)	Acc@1  78.12 ( 77.35)	Acc@5  98.44 ( 96.04)
Epoch: [37][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3749e-01 (7.4834e-01)	Acc@1  74.22 ( 77.30)	Acc@5  97.66 ( 96.04)
Epoch: [37][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9348e-01 (7.4834e-01)	Acc@1  74.22 ( 77.27)	Acc@5  93.75 ( 96.02)
Epoch: [37][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.3081e-01 (7.5004e-01)	Acc@1  71.09 ( 77.19)	Acc@5  94.53 ( 96.03)
Epoch: [37][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7752e-01 (7.5158e-01)	Acc@1  72.66 ( 77.17)	Acc@5  91.41 ( 95.99)
Epoch: [37][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0625e-01 (7.5098e-01)	Acc@1  77.34 ( 77.16)	Acc@5  93.75 ( 96.02)
Epoch: [37][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7412e-01 (7.5197e-01)	Acc@1  78.12 ( 77.13)	Acc@5  97.66 ( 96.02)
Epoch: [37][250/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1415e-01 (7.5258e-01)	Acc@1  74.22 ( 77.10)	Acc@5  94.53 ( 96.00)
Epoch: [37][260/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.4323e-01 (7.5584e-01)	Acc@1  76.56 ( 77.04)	Acc@5  95.31 ( 95.95)
Epoch: [37][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4851e-01 (7.5438e-01)	Acc@1  85.94 ( 77.10)	Acc@5  95.31 ( 95.95)
Epoch: [37][280/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5709e-01 (7.5388e-01)	Acc@1  81.25 ( 77.13)	Acc@5  95.31 ( 95.96)
Epoch: [37][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0597e-01 (7.5380e-01)	Acc@1  77.34 ( 77.12)	Acc@5  93.75 ( 95.95)
Epoch: [37][300/391]	Time  0.064 ( 0.065)	Data  0.002 ( 0.002)	Loss 7.7071e-01 (7.5286e-01)	Acc@1  78.91 ( 77.15)	Acc@5  95.31 ( 95.95)
Epoch: [37][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.1361e-01 (7.5270e-01)	Acc@1  75.78 ( 77.14)	Acc@5  97.66 ( 95.98)
Epoch: [37][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7212e-01 (7.5410e-01)	Acc@1  72.66 ( 77.11)	Acc@5  94.53 ( 95.98)
Epoch: [37][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6051e-01 (7.5432e-01)	Acc@1  81.25 ( 77.14)	Acc@5  96.88 ( 95.97)
Epoch: [37][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5345e-01 (7.5370e-01)	Acc@1  77.34 ( 77.11)	Acc@5  97.66 ( 95.99)
Epoch: [37][350/391]	Time  0.067 ( 0.065)	Data  0.002 ( 0.002)	Loss 8.4188e-01 (7.5375e-01)	Acc@1  77.34 ( 77.09)	Acc@5  95.31 ( 96.01)
Epoch: [37][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7697e-01 (7.5339e-01)	Acc@1  71.88 ( 77.06)	Acc@5  98.44 ( 96.02)
Epoch: [37][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5247e-01 (7.5406e-01)	Acc@1  78.12 ( 77.07)	Acc@5  94.53 ( 95.99)
Epoch: [37][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5118e-01 (7.5528e-01)	Acc@1  78.12 ( 77.05)	Acc@5  94.53 ( 95.97)
Epoch: [37][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3169e-01 (7.5474e-01)	Acc@1  80.00 ( 77.05)	Acc@5  95.00 ( 95.96)
## e[37] optimizer.zero_grad (sum) time: 0.40619850158691406
## e[37]       loss.backward (sum) time: 6.973085165023804
## e[37]      optimizer.step (sum) time: 3.455148458480835
## epoch[37] training(only) time: 25.591015815734863
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2432e+00 (1.2432e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.3427e+00 (1.1963e+00)	Acc@1  69.00 ( 67.91)	Acc@5  90.00 ( 89.36)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.1981e+00 (1.1590e+00)	Acc@1  68.00 ( 67.90)	Acc@5  89.00 ( 90.90)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.3699e+00 (1.1755e+00)	Acc@1  61.00 ( 67.19)	Acc@5  88.00 ( 90.71)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.0892e+00 (1.1669e+00)	Acc@1  62.00 ( 66.90)	Acc@5  93.00 ( 91.02)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.0774e+00 (1.1795e+00)	Acc@1  70.00 ( 66.43)	Acc@5  92.00 ( 90.82)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2677e+00 (1.1741e+00)	Acc@1  65.00 ( 66.49)	Acc@5  89.00 ( 90.87)
Test: [ 70/100]	Time  0.029 ( 0.028)	Loss 1.1548e+00 (1.1821e+00)	Acc@1  69.00 ( 66.68)	Acc@5  92.00 ( 90.93)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3079e+00 (1.1895e+00)	Acc@1  65.00 ( 66.57)	Acc@5  91.00 ( 90.84)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7490e+00 (1.1858e+00)	Acc@1  61.00 ( 66.64)	Acc@5  86.00 ( 90.80)
 * Acc@1 66.840 Acc@5 90.880
### epoch[37] execution time: 28.4427649974823
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.218 ( 0.218)	Data  0.149 ( 0.149)	Loss 7.9646e-01 (7.9646e-01)	Acc@1  75.00 ( 75.00)	Acc@5  96.09 ( 96.09)
Epoch: [38][ 10/391]	Time  0.063 ( 0.079)	Data  0.001 ( 0.015)	Loss 6.6691e-01 (7.3262e-01)	Acc@1  78.12 ( 76.99)	Acc@5  96.88 ( 96.09)
Epoch: [38][ 20/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.008)	Loss 7.1861e-01 (7.3255e-01)	Acc@1  77.34 ( 77.19)	Acc@5  99.22 ( 96.54)
Epoch: [38][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 6.6341e-01 (7.3799e-01)	Acc@1  82.81 ( 77.17)	Acc@5  96.09 ( 96.30)
Epoch: [38][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 6.6917e-01 (7.2302e-01)	Acc@1  80.47 ( 77.69)	Acc@5  97.66 ( 96.27)
Epoch: [38][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.9739e-01 (7.2240e-01)	Acc@1  76.56 ( 77.85)	Acc@5  93.75 ( 96.25)
Epoch: [38][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 7.2923e-01 (7.2698e-01)	Acc@1  80.47 ( 77.92)	Acc@5  96.09 ( 96.16)
Epoch: [38][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.9820e-01 (7.2809e-01)	Acc@1  83.59 ( 77.95)	Acc@5  98.44 ( 96.18)
Epoch: [38][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.2368e-01 (7.2666e-01)	Acc@1  77.34 ( 78.10)	Acc@5  95.31 ( 96.17)
Epoch: [38][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 8.4402e-01 (7.2675e-01)	Acc@1  78.12 ( 78.15)	Acc@5  96.09 ( 96.15)
Epoch: [38][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 7.1956e-01 (7.3058e-01)	Acc@1  75.00 ( 78.07)	Acc@5  95.31 ( 96.04)
Epoch: [38][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5350e-01 (7.3454e-01)	Acc@1  72.66 ( 77.98)	Acc@5  94.53 ( 96.00)
Epoch: [38][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8797e-01 (7.3276e-01)	Acc@1  77.34 ( 78.08)	Acc@5  97.66 ( 96.01)
Epoch: [38][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3309e-01 (7.2924e-01)	Acc@1  71.88 ( 78.10)	Acc@5  91.41 ( 96.04)
Epoch: [38][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2016e-01 (7.3114e-01)	Acc@1  73.44 ( 77.96)	Acc@5  92.97 ( 96.01)
Epoch: [38][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5073e-01 (7.3176e-01)	Acc@1  77.34 ( 77.96)	Acc@5  94.53 ( 95.96)
Epoch: [38][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0199e-01 (7.3199e-01)	Acc@1  80.47 ( 77.89)	Acc@5  99.22 ( 96.02)
Epoch: [38][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6417e-01 (7.3039e-01)	Acc@1  80.47 ( 77.95)	Acc@5  95.31 ( 96.07)
Epoch: [38][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2223e-01 (7.2829e-01)	Acc@1  76.56 ( 78.05)	Acc@5  96.88 ( 96.11)
Epoch: [38][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2842e-01 (7.2790e-01)	Acc@1  79.69 ( 78.05)	Acc@5  98.44 ( 96.13)
Epoch: [38][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4664e-01 (7.2544e-01)	Acc@1  75.78 ( 78.05)	Acc@5  96.88 ( 96.18)
Epoch: [38][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0191e-01 (7.2523e-01)	Acc@1  78.91 ( 78.01)	Acc@5  98.44 ( 96.18)
Epoch: [38][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8403e-01 (7.2713e-01)	Acc@1  77.34 ( 78.02)	Acc@5  94.53 ( 96.13)
Epoch: [38][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5993e-01 (7.2619e-01)	Acc@1  75.78 ( 78.05)	Acc@5  96.88 ( 96.14)
Epoch: [38][240/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.8143e-01 (7.2692e-01)	Acc@1  72.66 ( 78.01)	Acc@5  95.31 ( 96.15)
Epoch: [38][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4262e-01 (7.3068e-01)	Acc@1  75.78 ( 77.87)	Acc@5  95.31 ( 96.11)
Epoch: [38][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8316e-01 (7.3275e-01)	Acc@1  73.44 ( 77.79)	Acc@5  97.66 ( 96.06)
Epoch: [38][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9971e-01 (7.3330e-01)	Acc@1  71.09 ( 77.76)	Acc@5  96.09 ( 96.06)
Epoch: [38][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1358e-01 (7.3468e-01)	Acc@1  78.12 ( 77.75)	Acc@5  97.66 ( 96.04)
Epoch: [38][290/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7979e-01 (7.3588e-01)	Acc@1  72.66 ( 77.70)	Acc@5  97.66 ( 96.02)
Epoch: [38][300/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3665e-01 (7.3440e-01)	Acc@1  81.25 ( 77.72)	Acc@5  96.88 ( 96.04)
Epoch: [38][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9813e-01 (7.3597e-01)	Acc@1  74.22 ( 77.69)	Acc@5  92.97 ( 96.02)
Epoch: [38][320/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2622e-01 (7.3549e-01)	Acc@1  82.03 ( 77.72)	Acc@5  97.66 ( 96.03)
Epoch: [38][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6957e-01 (7.3494e-01)	Acc@1  81.25 ( 77.76)	Acc@5  96.09 ( 96.02)
Epoch: [38][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3946e-01 (7.3617e-01)	Acc@1  78.12 ( 77.72)	Acc@5  94.53 ( 96.03)
Epoch: [38][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0443e-01 (7.3602e-01)	Acc@1  80.47 ( 77.67)	Acc@5  97.66 ( 96.05)
Epoch: [38][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5984e-01 (7.3528e-01)	Acc@1  78.12 ( 77.69)	Acc@5  97.66 ( 96.08)
Epoch: [38][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6098e-01 (7.3631e-01)	Acc@1  78.91 ( 77.69)	Acc@5  95.31 ( 96.03)
Epoch: [38][380/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1041e-01 (7.3807e-01)	Acc@1  76.56 ( 77.62)	Acc@5  95.31 ( 96.04)
Epoch: [38][390/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.2133e-01 (7.3715e-01)	Acc@1  75.00 ( 77.64)	Acc@5  92.50 ( 96.05)
## e[38] optimizer.zero_grad (sum) time: 0.41759777069091797
## e[38]       loss.backward (sum) time: 7.027418375015259
## e[38]      optimizer.step (sum) time: 3.395606756210327
## epoch[38] training(only) time: 25.623682975769043
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2935e+00 (1.2935e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.028 ( 0.040)	Loss 1.2686e+00 (1.1913e+00)	Acc@1  69.00 ( 67.91)	Acc@5  90.00 ( 89.45)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.1689e+00 (1.1485e+00)	Acc@1  70.00 ( 68.29)	Acc@5  91.00 ( 90.95)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.4550e+00 (1.1723e+00)	Acc@1  59.00 ( 67.23)	Acc@5  86.00 ( 90.71)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.0940e+00 (1.1611e+00)	Acc@1  65.00 ( 66.80)	Acc@5  94.00 ( 91.20)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.0917e+00 (1.1749e+00)	Acc@1  72.00 ( 66.35)	Acc@5  89.00 ( 90.94)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2012e+00 (1.1693e+00)	Acc@1  61.00 ( 66.41)	Acc@5  90.00 ( 90.97)
Test: [ 70/100]	Time  0.029 ( 0.028)	Loss 1.0987e+00 (1.1789e+00)	Acc@1  71.00 ( 66.61)	Acc@5  91.00 ( 90.85)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2556e+00 (1.1850e+00)	Acc@1  68.00 ( 66.60)	Acc@5  91.00 ( 90.70)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.7454e+00 (1.1807e+00)	Acc@1  61.00 ( 66.67)	Acc@5  85.00 ( 90.71)
 * Acc@1 66.900 Acc@5 90.750
### epoch[38] execution time: 28.483278036117554
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.218 ( 0.218)	Data  0.151 ( 0.151)	Loss 6.1354e-01 (6.1354e-01)	Acc@1  82.81 ( 82.81)	Acc@5  96.88 ( 96.88)
Epoch: [39][ 10/391]	Time  0.066 ( 0.081)	Data  0.001 ( 0.015)	Loss 7.7346e-01 (7.1180e-01)	Acc@1  78.12 ( 79.05)	Acc@5  95.31 ( 96.52)
Epoch: [39][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.008)	Loss 5.8929e-01 (6.9208e-01)	Acc@1  85.16 ( 79.58)	Acc@5  97.66 ( 96.80)
Epoch: [39][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 8.2215e-01 (6.9394e-01)	Acc@1  70.31 ( 79.36)	Acc@5  96.88 ( 96.88)
Epoch: [39][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.1670e-01 (6.9828e-01)	Acc@1  75.00 ( 78.89)	Acc@5  98.44 ( 96.68)
Epoch: [39][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.0629e-01 (6.9648e-01)	Acc@1  83.59 ( 78.91)	Acc@5  96.88 ( 96.65)
Epoch: [39][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.2931e-01 (6.9764e-01)	Acc@1  67.97 ( 78.66)	Acc@5  97.66 ( 96.70)
Epoch: [39][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.8602e-01 (6.9288e-01)	Acc@1  84.38 ( 78.86)	Acc@5  95.31 ( 96.70)
Epoch: [39][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.7653e-01 (7.0089e-01)	Acc@1  80.47 ( 78.76)	Acc@5  94.53 ( 96.60)
Epoch: [39][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.7977e-01 (7.0846e-01)	Acc@1  71.88 ( 78.66)	Acc@5  94.53 ( 96.46)
Epoch: [39][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.3662e-01 (7.0832e-01)	Acc@1  83.59 ( 78.74)	Acc@5  99.22 ( 96.37)
Epoch: [39][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9060e-01 (7.1317e-01)	Acc@1  76.56 ( 78.55)	Acc@5  94.53 ( 96.23)
Epoch: [39][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1411e-01 (7.1682e-01)	Acc@1  77.34 ( 78.40)	Acc@5  96.09 ( 96.16)
Epoch: [39][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5783e-01 (7.1226e-01)	Acc@1  75.78 ( 78.45)	Acc@5  96.88 ( 96.25)
Epoch: [39][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3206e-01 (7.1106e-01)	Acc@1  75.78 ( 78.50)	Acc@5  95.31 ( 96.28)
Epoch: [39][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0185e-01 (7.1257e-01)	Acc@1  73.44 ( 78.46)	Acc@5  96.88 ( 96.32)
Epoch: [39][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6050e-01 (7.1253e-01)	Acc@1  78.91 ( 78.47)	Acc@5  96.88 ( 96.34)
Epoch: [39][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1557e-01 (7.1077e-01)	Acc@1  76.56 ( 78.48)	Acc@5  94.53 ( 96.31)
Epoch: [39][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8032e-01 (7.1063e-01)	Acc@1  78.91 ( 78.42)	Acc@5  96.88 ( 96.32)
Epoch: [39][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9347e-01 (7.1323e-01)	Acc@1  75.00 ( 78.40)	Acc@5  96.09 ( 96.27)
Epoch: [39][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6804e-01 (7.1420e-01)	Acc@1  77.34 ( 78.37)	Acc@5  95.31 ( 96.23)
Epoch: [39][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0303e-01 (7.1546e-01)	Acc@1  78.91 ( 78.32)	Acc@5  92.97 ( 96.22)
Epoch: [39][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4674e-01 (7.1469e-01)	Acc@1  75.00 ( 78.35)	Acc@5  96.09 ( 96.19)
Epoch: [39][230/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1937e-01 (7.1505e-01)	Acc@1  80.47 ( 78.40)	Acc@5  97.66 ( 96.18)
Epoch: [39][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0896e-01 (7.1586e-01)	Acc@1  68.75 ( 78.35)	Acc@5  94.53 ( 96.18)
Epoch: [39][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5812e-01 (7.1774e-01)	Acc@1  78.12 ( 78.36)	Acc@5  92.97 ( 96.14)
Epoch: [39][260/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8785e-01 (7.1766e-01)	Acc@1  82.03 ( 78.32)	Acc@5  96.09 ( 96.15)
Epoch: [39][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7295e-01 (7.1682e-01)	Acc@1  78.91 ( 78.32)	Acc@5  98.44 ( 96.18)
Epoch: [39][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6799e-01 (7.1893e-01)	Acc@1  78.91 ( 78.24)	Acc@5  96.09 ( 96.17)
Epoch: [39][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7604e-01 (7.1723e-01)	Acc@1  80.47 ( 78.31)	Acc@5  96.09 ( 96.20)
Epoch: [39][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7171e-01 (7.1904e-01)	Acc@1  78.12 ( 78.26)	Acc@5  94.53 ( 96.19)
Epoch: [39][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.6242e-01 (7.1877e-01)	Acc@1  81.25 ( 78.28)	Acc@5  95.31 ( 96.18)
Epoch: [39][320/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5676e-01 (7.1874e-01)	Acc@1  71.88 ( 78.30)	Acc@5  94.53 ( 96.17)
Epoch: [39][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9391e-01 (7.2027e-01)	Acc@1  71.09 ( 78.28)	Acc@5  95.31 ( 96.16)
Epoch: [39][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3672e-01 (7.2064e-01)	Acc@1  74.22 ( 78.20)	Acc@5  96.88 ( 96.15)
Epoch: [39][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9796e-01 (7.2052e-01)	Acc@1  79.69 ( 78.17)	Acc@5  96.09 ( 96.16)
Epoch: [39][360/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5276e-01 (7.1974e-01)	Acc@1  77.34 ( 78.20)	Acc@5  99.22 ( 96.20)
Epoch: [39][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2493e-01 (7.1890e-01)	Acc@1  83.59 ( 78.22)	Acc@5  96.88 ( 96.21)
Epoch: [39][380/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7890e-01 (7.2018e-01)	Acc@1  75.00 ( 78.18)	Acc@5  96.88 ( 96.20)
Epoch: [39][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4364e-01 (7.2077e-01)	Acc@1  78.75 ( 78.15)	Acc@5  96.25 ( 96.19)
## e[39] optimizer.zero_grad (sum) time: 0.4146847724914551
## e[39]       loss.backward (sum) time: 7.022362947463989
## e[39]      optimizer.step (sum) time: 3.422752618789673
## epoch[39] training(only) time: 25.633454084396362
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.1842e+00 (1.1842e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.034 ( 0.039)	Loss 1.2779e+00 (1.1915e+00)	Acc@1  66.00 ( 67.45)	Acc@5  89.00 ( 89.09)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.2391e+00 (1.1534e+00)	Acc@1  70.00 ( 68.19)	Acc@5  91.00 ( 90.43)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.3910e+00 (1.1696e+00)	Acc@1  64.00 ( 67.55)	Acc@5  89.00 ( 90.39)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.0899e+00 (1.1620e+00)	Acc@1  64.00 ( 66.95)	Acc@5  92.00 ( 90.83)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.0577e+00 (1.1756e+00)	Acc@1  74.00 ( 66.71)	Acc@5  90.00 ( 90.61)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3510e+00 (1.1740e+00)	Acc@1  58.00 ( 66.69)	Acc@5  92.00 ( 90.72)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.1551e+00 (1.1862e+00)	Acc@1  64.00 ( 66.68)	Acc@5  92.00 ( 90.63)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3046e+00 (1.1933e+00)	Acc@1  63.00 ( 66.58)	Acc@5  92.00 ( 90.52)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.8380e+00 (1.1890e+00)	Acc@1  57.00 ( 66.71)	Acc@5  85.00 ( 90.58)
 * Acc@1 66.830 Acc@5 90.620
### epoch[39] execution time: 28.487521171569824
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.219 ( 0.219)	Data  0.148 ( 0.148)	Loss 7.1471e-01 (7.1471e-01)	Acc@1  80.47 ( 80.47)	Acc@5  96.88 ( 96.88)
Epoch: [40][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.014)	Loss 7.0789e-01 (6.8153e-01)	Acc@1  81.25 ( 80.61)	Acc@5  96.88 ( 96.88)
Epoch: [40][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.008)	Loss 6.5168e-01 (6.8396e-01)	Acc@1  81.25 ( 79.69)	Acc@5  97.66 ( 96.95)
Epoch: [40][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.006)	Loss 7.6877e-01 (6.9587e-01)	Acc@1  76.56 ( 79.39)	Acc@5  95.31 ( 96.52)
Epoch: [40][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.1438e-01 (6.9134e-01)	Acc@1  84.38 ( 79.08)	Acc@5  96.88 ( 96.68)
Epoch: [40][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.5568e-01 (6.9922e-01)	Acc@1  80.47 ( 78.97)	Acc@5  94.53 ( 96.57)
Epoch: [40][ 60/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.5871e-01 (6.9676e-01)	Acc@1  78.91 ( 78.98)	Acc@5  95.31 ( 96.64)
Epoch: [40][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.4946e-01 (7.0197e-01)	Acc@1  74.22 ( 78.73)	Acc@5  94.53 ( 96.63)
Epoch: [40][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.9117e-01 (6.9743e-01)	Acc@1  78.12 ( 78.80)	Acc@5  96.88 ( 96.61)
Epoch: [40][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.1764e-01 (6.9640e-01)	Acc@1  78.91 ( 78.71)	Acc@5  96.88 ( 96.60)
Epoch: [40][100/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.3080e-01 (6.9648e-01)	Acc@1  82.03 ( 78.78)	Acc@5  96.88 ( 96.60)
Epoch: [40][110/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.1199e-01 (7.0051e-01)	Acc@1  76.56 ( 78.60)	Acc@5  98.44 ( 96.50)
Epoch: [40][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2064e-01 (6.9698e-01)	Acc@1  80.47 ( 78.64)	Acc@5  96.09 ( 96.55)
Epoch: [40][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1200e-01 (6.9878e-01)	Acc@1  82.03 ( 78.55)	Acc@5  98.44 ( 96.55)
Epoch: [40][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7798e-01 (6.9829e-01)	Acc@1  77.34 ( 78.46)	Acc@5  96.88 ( 96.56)
Epoch: [40][150/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.7304e-01 (6.9884e-01)	Acc@1  67.97 ( 78.43)	Acc@5  94.53 ( 96.56)
Epoch: [40][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9139e-01 (6.9644e-01)	Acc@1  82.03 ( 78.47)	Acc@5  95.31 ( 96.55)
Epoch: [40][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7446e-01 (7.0001e-01)	Acc@1  81.25 ( 78.44)	Acc@5  97.66 ( 96.53)
Epoch: [40][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5171e-01 (7.0382e-01)	Acc@1  73.44 ( 78.32)	Acc@5  96.88 ( 96.51)
Epoch: [40][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7232e-01 (7.0139e-01)	Acc@1  81.25 ( 78.39)	Acc@5  96.88 ( 96.53)
Epoch: [40][200/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1293e-01 (7.0057e-01)	Acc@1  78.91 ( 78.45)	Acc@5  96.88 ( 96.50)
Epoch: [40][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4706e-01 (7.0015e-01)	Acc@1  77.34 ( 78.47)	Acc@5  95.31 ( 96.50)
Epoch: [40][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2692e-01 (7.0112e-01)	Acc@1  76.56 ( 78.40)	Acc@5  96.88 ( 96.51)
Epoch: [40][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0276e-01 (7.0225e-01)	Acc@1  78.12 ( 78.38)	Acc@5  94.53 ( 96.51)
Epoch: [40][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3545e-01 (7.0406e-01)	Acc@1  80.47 ( 78.32)	Acc@5  96.09 ( 96.50)
Epoch: [40][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7997e-01 (7.0238e-01)	Acc@1  85.16 ( 78.36)	Acc@5  96.88 ( 96.52)
Epoch: [40][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4136e-01 (7.0382e-01)	Acc@1  75.78 ( 78.30)	Acc@5  95.31 ( 96.53)
Epoch: [40][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0687e-01 (7.0708e-01)	Acc@1  79.69 ( 78.22)	Acc@5  96.09 ( 96.48)
Epoch: [40][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.8199e-01 (7.0689e-01)	Acc@1  69.53 ( 78.20)	Acc@5  94.53 ( 96.51)
Epoch: [40][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2712e-01 (7.0674e-01)	Acc@1  77.34 ( 78.22)	Acc@5  94.53 ( 96.50)
Epoch: [40][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1759e-01 (7.0592e-01)	Acc@1  79.69 ( 78.25)	Acc@5  97.66 ( 96.51)
Epoch: [40][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1727e-01 (7.0503e-01)	Acc@1  78.91 ( 78.26)	Acc@5  98.44 ( 96.49)
Epoch: [40][320/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6683e-01 (7.0732e-01)	Acc@1  83.59 ( 78.21)	Acc@5  99.22 ( 96.45)
Epoch: [40][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6931e-01 (7.0658e-01)	Acc@1  76.56 ( 78.26)	Acc@5  95.31 ( 96.46)
Epoch: [40][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7423e-01 (7.0788e-01)	Acc@1  75.78 ( 78.21)	Acc@5  95.31 ( 96.43)
Epoch: [40][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8398e-01 (7.0901e-01)	Acc@1  82.81 ( 78.19)	Acc@5  95.31 ( 96.41)
Epoch: [40][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3280e-01 (7.0868e-01)	Acc@1  79.69 ( 78.20)	Acc@5  96.88 ( 96.41)
Epoch: [40][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4919e-01 (7.0940e-01)	Acc@1  75.78 ( 78.17)	Acc@5  96.88 ( 96.41)
Epoch: [40][380/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8343e-01 (7.1093e-01)	Acc@1  78.91 ( 78.12)	Acc@5  96.09 ( 96.41)
Epoch: [40][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3413e-01 (7.1088e-01)	Acc@1  72.50 ( 78.13)	Acc@5  96.25 ( 96.41)
## e[40] optimizer.zero_grad (sum) time: 0.4101874828338623
## e[40]       loss.backward (sum) time: 6.974309682846069
## e[40]      optimizer.step (sum) time: 3.4654133319854736
## epoch[40] training(only) time: 25.674747943878174
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2823e+00 (1.2823e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.026 ( 0.040)	Loss 1.3324e+00 (1.2281e+00)	Acc@1  65.00 ( 66.73)	Acc@5  90.00 ( 89.09)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.2328e+00 (1.1806e+00)	Acc@1  66.00 ( 67.43)	Acc@5  90.00 ( 90.38)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.3777e+00 (1.1921e+00)	Acc@1  61.00 ( 67.03)	Acc@5  89.00 ( 90.45)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.0794e+00 (1.1744e+00)	Acc@1  66.00 ( 66.83)	Acc@5  93.00 ( 91.02)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1286e+00 (1.1874e+00)	Acc@1  72.00 ( 66.67)	Acc@5  92.00 ( 90.78)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2516e+00 (1.1824e+00)	Acc@1  59.00 ( 66.74)	Acc@5  91.00 ( 90.84)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.1237e+00 (1.1918e+00)	Acc@1  70.00 ( 66.73)	Acc@5  93.00 ( 90.80)
Test: [ 80/100]	Time  0.030 ( 0.028)	Loss 1.2752e+00 (1.1959e+00)	Acc@1  67.00 ( 66.80)	Acc@5  92.00 ( 90.73)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.7962e+00 (1.1910e+00)	Acc@1  57.00 ( 66.89)	Acc@5  85.00 ( 90.78)
 * Acc@1 67.060 Acc@5 90.860
### epoch[40] execution time: 28.524665594100952
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.220 ( 0.220)	Data  0.151 ( 0.151)	Loss 6.6196e-01 (6.6196e-01)	Acc@1  78.12 ( 78.12)	Acc@5  96.09 ( 96.09)
Epoch: [41][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.015)	Loss 5.0414e-01 (7.4573e-01)	Acc@1  83.59 ( 77.49)	Acc@5  98.44 ( 95.95)
Epoch: [41][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.008)	Loss 5.5702e-01 (7.0838e-01)	Acc@1  83.59 ( 78.50)	Acc@5  99.22 ( 96.69)
Epoch: [41][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.9002e-01 (6.9051e-01)	Acc@1  83.59 ( 79.21)	Acc@5  99.22 ( 96.88)
Epoch: [41][ 40/391]	Time  0.069 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.6019e-01 (6.9154e-01)	Acc@1  75.78 ( 79.29)	Acc@5  94.53 ( 96.82)
Epoch: [41][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.2766e-01 (6.8511e-01)	Acc@1  82.03 ( 79.73)	Acc@5  96.88 ( 96.66)
Epoch: [41][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.7964e-01 (6.8219e-01)	Acc@1  82.81 ( 79.79)	Acc@5  97.66 ( 96.75)
Epoch: [41][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 9.1889e-01 (6.8202e-01)	Acc@1  75.00 ( 79.67)	Acc@5  93.75 ( 96.69)
Epoch: [41][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.4558e-01 (6.8445e-01)	Acc@1  78.12 ( 79.55)	Acc@5  97.66 ( 96.72)
Epoch: [41][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.9517e-01 (6.8654e-01)	Acc@1  73.44 ( 79.45)	Acc@5  94.53 ( 96.76)
Epoch: [41][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.8404e-01 (6.8870e-01)	Acc@1  78.12 ( 79.36)	Acc@5  96.09 ( 96.74)
Epoch: [41][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.0937e-01 (6.8718e-01)	Acc@1  75.00 ( 79.39)	Acc@5  95.31 ( 96.69)
Epoch: [41][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7023e-01 (6.9547e-01)	Acc@1  75.00 ( 79.22)	Acc@5  95.31 ( 96.59)
Epoch: [41][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2894e-01 (6.9444e-01)	Acc@1  78.12 ( 79.17)	Acc@5  93.75 ( 96.56)
Epoch: [41][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0901e-01 (6.9119e-01)	Acc@1  78.12 ( 79.18)	Acc@5  98.44 ( 96.58)
Epoch: [41][150/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1777e-01 (6.8889e-01)	Acc@1  80.47 ( 79.19)	Acc@5  98.44 ( 96.61)
Epoch: [41][160/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0535e-01 (6.8588e-01)	Acc@1  82.03 ( 79.23)	Acc@5  95.31 ( 96.66)
Epoch: [41][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0034e-01 (6.8396e-01)	Acc@1  78.91 ( 79.28)	Acc@5  96.88 ( 96.67)
Epoch: [41][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8082e-01 (6.8403e-01)	Acc@1  78.12 ( 79.26)	Acc@5  97.66 ( 96.70)
Epoch: [41][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0103e-01 (6.8586e-01)	Acc@1  79.69 ( 79.23)	Acc@5  96.09 ( 96.63)
Epoch: [41][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2810e-01 (6.8622e-01)	Acc@1  82.81 ( 79.23)	Acc@5  96.09 ( 96.61)
Epoch: [41][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5348e-01 (6.8694e-01)	Acc@1  75.00 ( 79.21)	Acc@5 100.00 ( 96.63)
Epoch: [41][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8285e-01 (6.8715e-01)	Acc@1  82.81 ( 79.18)	Acc@5  95.31 ( 96.61)
Epoch: [41][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2656e-01 (6.8754e-01)	Acc@1  82.81 ( 79.16)	Acc@5  96.88 ( 96.61)
Epoch: [41][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5937e-01 (6.8884e-01)	Acc@1  82.03 ( 79.14)	Acc@5  96.09 ( 96.58)
Epoch: [41][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0618e-01 (6.8820e-01)	Acc@1  75.78 ( 79.11)	Acc@5  96.88 ( 96.59)
Epoch: [41][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2898e-01 (6.8916e-01)	Acc@1  79.69 ( 79.08)	Acc@5  96.88 ( 96.58)
Epoch: [41][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5755e-01 (6.8991e-01)	Acc@1  79.69 ( 79.07)	Acc@5  97.66 ( 96.57)
Epoch: [41][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1955e-01 (6.8867e-01)	Acc@1  75.00 ( 79.08)	Acc@5  93.75 ( 96.58)
Epoch: [41][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2028e-01 (6.8963e-01)	Acc@1  73.44 ( 79.06)	Acc@5  98.44 ( 96.57)
Epoch: [41][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.9944e-01 (6.8983e-01)	Acc@1  75.00 ( 79.04)	Acc@5  97.66 ( 96.57)
Epoch: [41][310/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.7592e-01 (6.8938e-01)	Acc@1  78.12 ( 79.04)	Acc@5  96.09 ( 96.57)
Epoch: [41][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7838e-01 (6.8987e-01)	Acc@1  81.25 ( 79.00)	Acc@5  96.88 ( 96.58)
Epoch: [41][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5936e-01 (6.8988e-01)	Acc@1  77.34 ( 78.99)	Acc@5  93.75 ( 96.58)
Epoch: [41][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3571e-01 (6.8983e-01)	Acc@1  75.00 ( 79.01)	Acc@5  96.88 ( 96.59)
Epoch: [41][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2357e-01 (6.9125e-01)	Acc@1  74.22 ( 78.97)	Acc@5  98.44 ( 96.57)
Epoch: [41][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0051e-01 (6.9110e-01)	Acc@1  74.22 ( 78.94)	Acc@5  94.53 ( 96.56)
Epoch: [41][370/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6271e-01 (6.9167e-01)	Acc@1  73.44 ( 78.94)	Acc@5  93.75 ( 96.54)
Epoch: [41][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 9.3820e-01 (6.9284e-01)	Acc@1  69.53 ( 78.90)	Acc@5  93.75 ( 96.53)
Epoch: [41][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.4400e-01 (6.9256e-01)	Acc@1  80.00 ( 78.93)	Acc@5  98.75 ( 96.54)
## e[41] optimizer.zero_grad (sum) time: 0.41527223587036133
## e[41]       loss.backward (sum) time: 7.011885166168213
## e[41]      optimizer.step (sum) time: 3.455817937850952
## epoch[41] training(only) time: 25.602527856826782
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.3093e+00 (1.3093e+00)	Acc@1  63.00 ( 63.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3830e+00 (1.2193e+00)	Acc@1  65.00 ( 66.55)	Acc@5  89.00 ( 89.73)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.2710e+00 (1.1852e+00)	Acc@1  70.00 ( 67.86)	Acc@5  90.00 ( 90.95)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.4745e+00 (1.2107e+00)	Acc@1  60.00 ( 67.16)	Acc@5  89.00 ( 90.48)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.0764e+00 (1.1881e+00)	Acc@1  65.00 ( 66.88)	Acc@5  90.00 ( 91.00)
Test: [ 50/100]	Time  0.025 ( 0.030)	Loss 1.1583e+00 (1.2011e+00)	Acc@1  71.00 ( 66.39)	Acc@5  90.00 ( 90.86)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3564e+00 (1.1939e+00)	Acc@1  61.00 ( 66.79)	Acc@5  87.00 ( 90.80)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.2252e+00 (1.2043e+00)	Acc@1  65.00 ( 66.69)	Acc@5  91.00 ( 90.75)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.2900e+00 (1.2099e+00)	Acc@1  68.00 ( 66.58)	Acc@5  93.00 ( 90.58)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8005e+00 (1.2049e+00)	Acc@1  56.00 ( 66.73)	Acc@5  86.00 ( 90.63)
 * Acc@1 66.870 Acc@5 90.650
### epoch[41] execution time: 28.45913791656494
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.218 ( 0.218)	Data  0.149 ( 0.149)	Loss 7.2176e-01 (7.2176e-01)	Acc@1  78.91 ( 78.91)	Acc@5  95.31 ( 95.31)
Epoch: [42][ 10/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.015)	Loss 7.1759e-01 (6.8171e-01)	Acc@1  78.91 ( 78.84)	Acc@5  96.88 ( 96.52)
Epoch: [42][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 5.5765e-01 (6.9717e-01)	Acc@1  88.28 ( 78.50)	Acc@5  96.88 ( 96.69)
Epoch: [42][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 5.7031e-01 (6.7209e-01)	Acc@1  82.81 ( 79.31)	Acc@5  98.44 ( 96.75)
Epoch: [42][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.1132e-01 (6.8177e-01)	Acc@1  79.69 ( 79.06)	Acc@5  95.31 ( 96.53)
Epoch: [42][ 50/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.9275e-01 (6.8136e-01)	Acc@1  85.94 ( 79.17)	Acc@5  95.31 ( 96.40)
Epoch: [42][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.6484e-01 (6.9017e-01)	Acc@1  72.66 ( 78.92)	Acc@5  95.31 ( 96.35)
Epoch: [42][ 70/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 6.8181e-01 (6.8478e-01)	Acc@1  79.69 ( 79.20)	Acc@5  96.09 ( 96.30)
Epoch: [42][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.9348e-01 (6.8634e-01)	Acc@1  72.66 ( 79.07)	Acc@5  92.97 ( 96.28)
Epoch: [42][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.7411e-01 (6.8613e-01)	Acc@1  78.91 ( 79.03)	Acc@5  96.09 ( 96.27)
Epoch: [42][100/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.1910e-01 (6.8700e-01)	Acc@1  77.34 ( 78.93)	Acc@5  96.09 ( 96.31)
Epoch: [42][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.8100e-01 (6.8992e-01)	Acc@1  65.62 ( 78.66)	Acc@5  94.53 ( 96.33)
Epoch: [42][120/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8457e-01 (6.9088e-01)	Acc@1  78.91 ( 78.75)	Acc@5  96.09 ( 96.27)
Epoch: [42][130/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.002)	Loss 7.3921e-01 (6.9339e-01)	Acc@1  73.44 ( 78.60)	Acc@5  96.09 ( 96.27)
Epoch: [42][140/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 6.8930e-01 (6.9120e-01)	Acc@1  78.12 ( 78.68)	Acc@5  95.31 ( 96.34)
Epoch: [42][150/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5262e-01 (6.8814e-01)	Acc@1  78.12 ( 78.79)	Acc@5  96.88 ( 96.36)
Epoch: [42][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3449e-01 (6.8412e-01)	Acc@1  79.69 ( 78.91)	Acc@5  96.88 ( 96.43)
Epoch: [42][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1822e-01 (6.8248e-01)	Acc@1  76.56 ( 78.91)	Acc@5  98.44 ( 96.49)
Epoch: [42][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6226e-01 (6.8161e-01)	Acc@1  82.81 ( 78.92)	Acc@5  98.44 ( 96.55)
Epoch: [42][190/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3831e-01 (6.8217e-01)	Acc@1  75.00 ( 78.93)	Acc@5  96.88 ( 96.55)
Epoch: [42][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9270e-01 (6.8370e-01)	Acc@1  78.12 ( 78.88)	Acc@5  96.88 ( 96.54)
Epoch: [42][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5378e-01 (6.8396e-01)	Acc@1  74.22 ( 78.88)	Acc@5  93.75 ( 96.53)
Epoch: [42][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0792e-01 (6.8647e-01)	Acc@1  80.47 ( 78.83)	Acc@5  96.09 ( 96.51)
Epoch: [42][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8688e-01 (6.8579e-01)	Acc@1  71.88 ( 78.81)	Acc@5  98.44 ( 96.54)
Epoch: [42][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2518e-01 (6.8499e-01)	Acc@1  79.69 ( 78.87)	Acc@5  96.88 ( 96.55)
Epoch: [42][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7981e-01 (6.8371e-01)	Acc@1  75.78 ( 78.90)	Acc@5  96.09 ( 96.56)
Epoch: [42][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3058e-01 (6.8378e-01)	Acc@1  79.69 ( 78.90)	Acc@5  96.88 ( 96.57)
Epoch: [42][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5484e-01 (6.8460e-01)	Acc@1  84.38 ( 78.90)	Acc@5  96.88 ( 96.56)
Epoch: [42][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1052e-01 (6.8266e-01)	Acc@1  78.91 ( 78.94)	Acc@5  96.09 ( 96.57)
Epoch: [42][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6104e-01 (6.8148e-01)	Acc@1  79.69 ( 78.97)	Acc@5  94.53 ( 96.58)
Epoch: [42][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9495e-01 (6.7856e-01)	Acc@1  85.16 ( 79.06)	Acc@5  97.66 ( 96.59)
Epoch: [42][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9079e-01 (6.7995e-01)	Acc@1  75.78 ( 79.02)	Acc@5  96.09 ( 96.58)
Epoch: [42][320/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5296e-01 (6.8036e-01)	Acc@1  87.50 ( 79.02)	Acc@5  98.44 ( 96.59)
Epoch: [42][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3382e-01 (6.8250e-01)	Acc@1  80.47 ( 78.94)	Acc@5  96.09 ( 96.55)
Epoch: [42][340/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1365e-01 (6.8350e-01)	Acc@1  78.91 ( 78.93)	Acc@5  96.09 ( 96.51)
Epoch: [42][350/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0967e-01 (6.8465e-01)	Acc@1  80.47 ( 78.91)	Acc@5  96.09 ( 96.48)
Epoch: [42][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.2317e-01 (6.8452e-01)	Acc@1  76.56 ( 78.94)	Acc@5  94.53 ( 96.47)
Epoch: [42][370/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1724e-01 (6.8364e-01)	Acc@1  81.25 ( 79.00)	Acc@5  96.88 ( 96.50)
Epoch: [42][380/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5269e-01 (6.8353e-01)	Acc@1  75.78 ( 79.01)	Acc@5  97.66 ( 96.51)
Epoch: [42][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2373e-01 (6.8304e-01)	Acc@1  77.50 ( 79.01)	Acc@5  97.50 ( 96.51)
## e[42] optimizer.zero_grad (sum) time: 0.40957093238830566
## e[42]       loss.backward (sum) time: 6.944184303283691
## e[42]      optimizer.step (sum) time: 3.586390495300293
## epoch[42] training(only) time: 25.70900297164917
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.3034e+00 (1.3034e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.028 ( 0.038)	Loss 1.3178e+00 (1.1966e+00)	Acc@1  64.00 ( 66.91)	Acc@5  89.00 ( 90.09)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.2195e+00 (1.1685e+00)	Acc@1  67.00 ( 67.81)	Acc@5  91.00 ( 90.95)
Test: [ 30/100]	Time  0.029 ( 0.031)	Loss 1.4484e+00 (1.1903e+00)	Acc@1  62.00 ( 67.52)	Acc@5  87.00 ( 90.65)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.0725e+00 (1.1769e+00)	Acc@1  67.00 ( 67.20)	Acc@5  91.00 ( 91.12)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1417e+00 (1.1935e+00)	Acc@1  72.00 ( 66.82)	Acc@5  91.00 ( 90.94)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2884e+00 (1.1869e+00)	Acc@1  61.00 ( 66.92)	Acc@5  91.00 ( 90.95)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1403e+00 (1.1967e+00)	Acc@1  68.00 ( 66.92)	Acc@5  91.00 ( 90.90)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3267e+00 (1.2019e+00)	Acc@1  63.00 ( 66.80)	Acc@5  94.00 ( 90.75)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7936e+00 (1.1981e+00)	Acc@1  57.00 ( 67.00)	Acc@5  85.00 ( 90.74)
 * Acc@1 67.190 Acc@5 90.800
### epoch[42] execution time: 28.531978130340576
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.216 ( 0.216)	Data  0.144 ( 0.144)	Loss 6.9317e-01 (6.9317e-01)	Acc@1  79.69 ( 79.69)	Acc@5  96.88 ( 96.88)
Epoch: [43][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.014)	Loss 8.1662e-01 (6.5154e-01)	Acc@1  70.31 ( 79.05)	Acc@5  96.88 ( 97.30)
Epoch: [43][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.6544e-01 (6.3699e-01)	Acc@1  85.16 ( 79.50)	Acc@5 100.00 ( 97.40)
Epoch: [43][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 5.9611e-01 (6.3734e-01)	Acc@1  84.38 ( 79.79)	Acc@5  96.88 ( 97.15)
Epoch: [43][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.9864e-01 (6.4549e-01)	Acc@1  75.00 ( 79.52)	Acc@5  94.53 ( 97.08)
Epoch: [43][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.9891e-01 (6.3648e-01)	Acc@1  79.69 ( 79.79)	Acc@5  96.88 ( 97.10)
Epoch: [43][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.1994e-01 (6.3939e-01)	Acc@1  81.25 ( 79.82)	Acc@5  96.09 ( 97.04)
Epoch: [43][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.0911e-01 (6.3562e-01)	Acc@1  82.81 ( 79.96)	Acc@5  96.09 ( 97.08)
Epoch: [43][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.7627e-01 (6.4340e-01)	Acc@1  80.47 ( 79.68)	Acc@5  95.31 ( 97.04)
Epoch: [43][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.9152e-01 (6.4619e-01)	Acc@1  82.03 ( 79.55)	Acc@5  96.88 ( 97.04)
Epoch: [43][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.6618e-01 (6.4627e-01)	Acc@1  79.69 ( 79.50)	Acc@5  96.88 ( 96.99)
Epoch: [43][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.7933e-01 (6.4855e-01)	Acc@1  75.00 ( 79.54)	Acc@5  98.44 ( 97.02)
Epoch: [43][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7748e-01 (6.5238e-01)	Acc@1  76.56 ( 79.44)	Acc@5  95.31 ( 96.94)
Epoch: [43][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3484e-01 (6.5129e-01)	Acc@1  80.47 ( 79.44)	Acc@5  97.66 ( 96.99)
Epoch: [43][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9766e-01 (6.5166e-01)	Acc@1  81.25 ( 79.50)	Acc@5  99.22 ( 96.95)
Epoch: [43][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2128e-01 (6.5267e-01)	Acc@1  75.78 ( 79.52)	Acc@5  95.31 ( 96.93)
Epoch: [43][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9675e-01 (6.5257e-01)	Acc@1  79.69 ( 79.59)	Acc@5  99.22 ( 96.92)
Epoch: [43][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9735e-01 (6.5205e-01)	Acc@1  87.50 ( 79.62)	Acc@5  98.44 ( 96.94)
Epoch: [43][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7741e-01 (6.5383e-01)	Acc@1  79.69 ( 79.61)	Acc@5  98.44 ( 96.95)
Epoch: [43][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4623e-01 (6.5477e-01)	Acc@1  80.47 ( 79.65)	Acc@5  93.75 ( 96.94)
Epoch: [43][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8943e-01 (6.5799e-01)	Acc@1  78.12 ( 79.60)	Acc@5  96.88 ( 96.88)
Epoch: [43][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0757e-01 (6.6064e-01)	Acc@1  78.91 ( 79.54)	Acc@5  97.66 ( 96.83)
Epoch: [43][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1907e-01 (6.5942e-01)	Acc@1  83.59 ( 79.56)	Acc@5  96.88 ( 96.84)
Epoch: [43][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3568e-01 (6.6081e-01)	Acc@1  80.47 ( 79.49)	Acc@5  94.53 ( 96.82)
Epoch: [43][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5835e-01 (6.6201e-01)	Acc@1  84.38 ( 79.47)	Acc@5  96.09 ( 96.79)
Epoch: [43][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0530e-01 (6.6176e-01)	Acc@1  78.91 ( 79.49)	Acc@5  97.66 ( 96.79)
Epoch: [43][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3251e-01 (6.6342e-01)	Acc@1  82.81 ( 79.45)	Acc@5  96.88 ( 96.77)
Epoch: [43][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8869e-01 (6.6391e-01)	Acc@1  85.16 ( 79.44)	Acc@5  96.88 ( 96.77)
Epoch: [43][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0187e-01 (6.6298e-01)	Acc@1  78.91 ( 79.45)	Acc@5  98.44 ( 96.79)
Epoch: [43][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6828e-01 (6.6312e-01)	Acc@1  80.47 ( 79.45)	Acc@5  96.88 ( 96.80)
Epoch: [43][300/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1268e-01 (6.6601e-01)	Acc@1  80.47 ( 79.39)	Acc@5  98.44 ( 96.77)
Epoch: [43][310/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.1704e-01 (6.6464e-01)	Acc@1  79.69 ( 79.46)	Acc@5  95.31 ( 96.79)
Epoch: [43][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.9152e-01 (6.6658e-01)	Acc@1  71.88 ( 79.38)	Acc@5  96.09 ( 96.81)
Epoch: [43][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0886e-01 (6.6578e-01)	Acc@1  80.47 ( 79.41)	Acc@5  94.53 ( 96.82)
Epoch: [43][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8374e-01 (6.6499e-01)	Acc@1  80.47 ( 79.43)	Acc@5  97.66 ( 96.83)
Epoch: [43][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7322e-01 (6.6536e-01)	Acc@1  77.34 ( 79.41)	Acc@5  96.09 ( 96.82)
Epoch: [43][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9541e-01 (6.6617e-01)	Acc@1  85.16 ( 79.40)	Acc@5 100.00 ( 96.81)
Epoch: [43][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6920e-01 (6.6612e-01)	Acc@1  78.12 ( 79.43)	Acc@5  96.09 ( 96.81)
Epoch: [43][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7719e-01 (6.6594e-01)	Acc@1  78.91 ( 79.45)	Acc@5  95.31 ( 96.80)
Epoch: [43][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9455e-01 (6.6575e-01)	Acc@1  80.00 ( 79.48)	Acc@5  95.00 ( 96.79)
## e[43] optimizer.zero_grad (sum) time: 0.4091932773590088
## e[43]       loss.backward (sum) time: 6.951597213745117
## e[43]      optimizer.step (sum) time: 3.5436127185821533
## epoch[43] training(only) time: 25.66704535484314
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2546e+00 (1.2546e+00)	Acc@1  68.00 ( 68.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.029 ( 0.039)	Loss 1.3532e+00 (1.2044e+00)	Acc@1  65.00 ( 68.91)	Acc@5  90.00 ( 89.18)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.2518e+00 (1.1673e+00)	Acc@1  70.00 ( 68.48)	Acc@5  92.00 ( 90.90)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5114e+00 (1.1939e+00)	Acc@1  59.00 ( 67.77)	Acc@5  88.00 ( 90.55)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.0571e+00 (1.1827e+00)	Acc@1  63.00 ( 67.24)	Acc@5  92.00 ( 91.00)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1188e+00 (1.1996e+00)	Acc@1  73.00 ( 66.80)	Acc@5  90.00 ( 90.78)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2957e+00 (1.1954e+00)	Acc@1  65.00 ( 66.79)	Acc@5  88.00 ( 90.70)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.1434e+00 (1.2039e+00)	Acc@1  68.00 ( 66.89)	Acc@5  93.00 ( 90.66)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.3775e+00 (1.2096e+00)	Acc@1  66.00 ( 66.78)	Acc@5  91.00 ( 90.60)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.7950e+00 (1.2034e+00)	Acc@1  57.00 ( 66.99)	Acc@5  87.00 ( 90.75)
 * Acc@1 67.180 Acc@5 90.760
### epoch[43] execution time: 28.521098613739014
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.218 ( 0.218)	Data  0.150 ( 0.150)	Loss 5.9558e-01 (5.9558e-01)	Acc@1  82.03 ( 82.03)	Acc@5  98.44 ( 98.44)
Epoch: [44][ 10/391]	Time  0.067 ( 0.079)	Data  0.001 ( 0.015)	Loss 5.4790e-01 (6.1756e-01)	Acc@1  84.38 ( 81.75)	Acc@5  96.88 ( 96.16)
Epoch: [44][ 20/391]	Time  0.068 ( 0.074)	Data  0.001 ( 0.008)	Loss 6.9981e-01 (6.2121e-01)	Acc@1  78.91 ( 81.21)	Acc@5  96.09 ( 96.58)
Epoch: [44][ 30/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.006)	Loss 8.3222e-01 (6.3056e-01)	Acc@1  71.09 ( 80.80)	Acc@5  96.88 ( 96.60)
Epoch: [44][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 7.4378e-01 (6.2280e-01)	Acc@1  79.69 ( 81.06)	Acc@5  96.09 ( 96.93)
Epoch: [44][ 50/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.004)	Loss 6.3823e-01 (6.1830e-01)	Acc@1  82.03 ( 80.96)	Acc@5  96.88 ( 97.03)
Epoch: [44][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.1229e-01 (6.2196e-01)	Acc@1  82.03 ( 81.06)	Acc@5  96.88 ( 96.95)
Epoch: [44][ 70/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.003)	Loss 8.0307e-01 (6.2371e-01)	Acc@1  76.56 ( 80.79)	Acc@5  96.88 ( 97.03)
Epoch: [44][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.7072e-01 (6.3205e-01)	Acc@1  78.12 ( 80.50)	Acc@5  96.09 ( 96.92)
Epoch: [44][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.7650e-01 (6.2980e-01)	Acc@1  80.47 ( 80.55)	Acc@5  97.66 ( 96.94)
Epoch: [44][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.3857e-01 (6.2782e-01)	Acc@1  80.47 ( 80.71)	Acc@5  97.66 ( 96.97)
Epoch: [44][110/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.9461e-01 (6.3082e-01)	Acc@1  79.69 ( 80.50)	Acc@5  96.09 ( 96.93)
Epoch: [44][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3673e-01 (6.3493e-01)	Acc@1  75.78 ( 80.34)	Acc@5  94.53 ( 96.94)
Epoch: [44][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4268e-01 (6.3400e-01)	Acc@1  79.69 ( 80.44)	Acc@5  99.22 ( 96.98)
Epoch: [44][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5179e-01 (6.3931e-01)	Acc@1  78.12 ( 80.23)	Acc@5  97.66 ( 96.97)
Epoch: [44][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0755e-01 (6.3758e-01)	Acc@1  79.69 ( 80.28)	Acc@5  95.31 ( 96.98)
Epoch: [44][160/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1411e-01 (6.3964e-01)	Acc@1  77.34 ( 80.21)	Acc@5  96.09 ( 96.94)
Epoch: [44][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6912e-01 (6.3989e-01)	Acc@1  80.47 ( 80.16)	Acc@5  98.44 ( 96.93)
Epoch: [44][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9133e-01 (6.3944e-01)	Acc@1  78.12 ( 80.22)	Acc@5  95.31 ( 96.92)
Epoch: [44][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0535e-01 (6.4114e-01)	Acc@1  77.34 ( 80.19)	Acc@5  96.88 ( 96.92)
Epoch: [44][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8025e-01 (6.4098e-01)	Acc@1  81.25 ( 80.19)	Acc@5  96.88 ( 96.97)
Epoch: [44][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4137e-01 (6.4166e-01)	Acc@1  78.91 ( 80.17)	Acc@5  94.53 ( 96.97)
Epoch: [44][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4117e-01 (6.3958e-01)	Acc@1  79.69 ( 80.18)	Acc@5  99.22 ( 97.01)
Epoch: [44][230/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0333e-01 (6.4203e-01)	Acc@1  82.81 ( 80.16)	Acc@5  97.66 ( 96.97)
Epoch: [44][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2925e-01 (6.4217e-01)	Acc@1  81.25 ( 80.17)	Acc@5  96.09 ( 96.97)
Epoch: [44][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9542e-01 (6.4298e-01)	Acc@1  73.44 ( 80.15)	Acc@5  96.88 ( 96.95)
Epoch: [44][260/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5310e-01 (6.4425e-01)	Acc@1  76.56 ( 80.06)	Acc@5  97.66 ( 96.95)
Epoch: [44][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.5550e-01 (6.4636e-01)	Acc@1  77.34 ( 80.01)	Acc@5  96.88 ( 96.92)
Epoch: [44][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.6654e-01 (6.4779e-01)	Acc@1  75.78 ( 80.02)	Acc@5  94.53 ( 96.88)
Epoch: [44][290/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1756e-01 (6.4759e-01)	Acc@1  77.34 ( 80.04)	Acc@5  96.88 ( 96.90)
Epoch: [44][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8495e-01 (6.4833e-01)	Acc@1  80.47 ( 80.01)	Acc@5  96.88 ( 96.92)
Epoch: [44][310/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7355e-01 (6.4845e-01)	Acc@1  83.59 ( 80.03)	Acc@5  97.66 ( 96.90)
Epoch: [44][320/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8542e-01 (6.4731e-01)	Acc@1  82.81 ( 80.07)	Acc@5  99.22 ( 96.92)
Epoch: [44][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6877e-01 (6.4898e-01)	Acc@1  77.34 ( 80.02)	Acc@5  96.88 ( 96.90)
Epoch: [44][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8569e-01 (6.5070e-01)	Acc@1  78.12 ( 79.97)	Acc@5  96.09 ( 96.88)
Epoch: [44][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6628e-01 (6.5047e-01)	Acc@1  82.03 ( 80.01)	Acc@5  96.88 ( 96.88)
Epoch: [44][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7297e-01 (6.5022e-01)	Acc@1  85.16 ( 80.04)	Acc@5  96.88 ( 96.87)
Epoch: [44][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0855e-01 (6.5040e-01)	Acc@1  80.47 ( 80.03)	Acc@5  96.88 ( 96.87)
Epoch: [44][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8164e-01 (6.5020e-01)	Acc@1  79.69 ( 80.04)	Acc@5  96.88 ( 96.87)
Epoch: [44][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 1.0570e+00 (6.5133e-01)	Acc@1  75.00 ( 79.98)	Acc@5  95.00 ( 96.87)
## e[44] optimizer.zero_grad (sum) time: 0.40569329261779785
## e[44]       loss.backward (sum) time: 6.991333484649658
## e[44]      optimizer.step (sum) time: 3.5104238986968994
## epoch[44] training(only) time: 25.600781679153442
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.3362e+00 (1.3362e+00)	Acc@1  63.00 ( 63.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.026 ( 0.038)	Loss 1.2752e+00 (1.2173e+00)	Acc@1  66.00 ( 67.27)	Acc@5  91.00 ( 89.36)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.1999e+00 (1.1695e+00)	Acc@1  70.00 ( 68.33)	Acc@5  91.00 ( 90.43)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.4948e+00 (1.2034e+00)	Acc@1  59.00 ( 67.61)	Acc@5  90.00 ( 90.29)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.0758e+00 (1.1887e+00)	Acc@1  65.00 ( 67.37)	Acc@5  94.00 ( 90.98)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.1941e+00 (1.2039e+00)	Acc@1  70.00 ( 67.04)	Acc@5  90.00 ( 90.67)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3194e+00 (1.2002e+00)	Acc@1  60.00 ( 66.95)	Acc@5  90.00 ( 90.75)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0923e+00 (1.2073e+00)	Acc@1  73.00 ( 67.08)	Acc@5  93.00 ( 90.66)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2646e+00 (1.2111e+00)	Acc@1  66.00 ( 67.07)	Acc@5  91.00 ( 90.49)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.7763e+00 (1.2052e+00)	Acc@1  54.00 ( 67.19)	Acc@5  87.00 ( 90.49)
 * Acc@1 67.210 Acc@5 90.590
### epoch[44] execution time: 28.44052767753601
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.222 ( 0.222)	Data  0.151 ( 0.151)	Loss 8.7161e-01 (8.7161e-01)	Acc@1  71.09 ( 71.09)	Acc@5  94.53 ( 94.53)
Epoch: [45][ 10/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.015)	Loss 6.8975e-01 (6.6504e-01)	Acc@1  79.69 ( 78.69)	Acc@5  98.44 ( 97.30)
Epoch: [45][ 20/391]	Time  0.064 ( 0.072)	Data  0.002 ( 0.008)	Loss 6.7820e-01 (6.5224e-01)	Acc@1  78.91 ( 79.35)	Acc@5  96.88 ( 96.95)
Epoch: [45][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 6.2254e-01 (6.4430e-01)	Acc@1  76.56 ( 79.84)	Acc@5  97.66 ( 97.05)
Epoch: [45][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 6.9582e-01 (6.3722e-01)	Acc@1  80.47 ( 80.26)	Acc@5  95.31 ( 97.12)
Epoch: [45][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 8.4938e-01 (6.4509e-01)	Acc@1  78.12 ( 80.15)	Acc@5  95.31 ( 97.00)
Epoch: [45][ 60/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.3802e-01 (6.3649e-01)	Acc@1  89.06 ( 80.34)	Acc@5  99.22 ( 97.12)
Epoch: [45][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.3557e-01 (6.3913e-01)	Acc@1  75.78 ( 80.37)	Acc@5  96.09 ( 97.00)
Epoch: [45][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.3876e-01 (6.4033e-01)	Acc@1  85.16 ( 80.35)	Acc@5  97.66 ( 97.01)
Epoch: [45][ 90/391]	Time  0.074 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.7863e-01 (6.3808e-01)	Acc@1  82.81 ( 80.40)	Acc@5  98.44 ( 97.01)
Epoch: [45][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.2961e-01 (6.3334e-01)	Acc@1  75.78 ( 80.55)	Acc@5  96.88 ( 97.08)
Epoch: [45][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.9047e-01 (6.3620e-01)	Acc@1  82.81 ( 80.55)	Acc@5  94.53 ( 96.99)
Epoch: [45][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6846e-01 (6.3402e-01)	Acc@1  82.03 ( 80.57)	Acc@5  97.66 ( 96.99)
Epoch: [45][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0576e-01 (6.3050e-01)	Acc@1  79.69 ( 80.55)	Acc@5  98.44 ( 97.03)
Epoch: [45][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2924e-01 (6.2756e-01)	Acc@1  76.56 ( 80.62)	Acc@5  96.09 ( 97.07)
Epoch: [45][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4951e-01 (6.2802e-01)	Acc@1  80.47 ( 80.65)	Acc@5  95.31 ( 97.07)
Epoch: [45][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0302e-01 (6.2678e-01)	Acc@1  84.38 ( 80.74)	Acc@5  96.09 ( 97.08)
Epoch: [45][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.5348e-01 (6.2908e-01)	Acc@1  68.75 ( 80.65)	Acc@5  91.41 ( 97.07)
Epoch: [45][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8307e-01 (6.3004e-01)	Acc@1  82.03 ( 80.68)	Acc@5  96.88 ( 97.07)
Epoch: [45][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7241e-01 (6.3053e-01)	Acc@1  78.12 ( 80.60)	Acc@5  97.66 ( 97.09)
Epoch: [45][200/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8080e-01 (6.3036e-01)	Acc@1  76.56 ( 80.61)	Acc@5  96.09 ( 97.12)
Epoch: [45][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0624e-01 (6.3119e-01)	Acc@1  78.12 ( 80.58)	Acc@5  97.66 ( 97.12)
Epoch: [45][220/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9685e-01 (6.3235e-01)	Acc@1  77.34 ( 80.56)	Acc@5  98.44 ( 97.12)
Epoch: [45][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9469e-01 (6.3054e-01)	Acc@1  82.81 ( 80.63)	Acc@5  97.66 ( 97.15)
Epoch: [45][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0233e-01 (6.2967e-01)	Acc@1  82.03 ( 80.71)	Acc@5 100.00 ( 97.17)
Epoch: [45][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9120e-01 (6.2972e-01)	Acc@1  80.47 ( 80.70)	Acc@5  96.88 ( 97.16)
Epoch: [45][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0865e-01 (6.2972e-01)	Acc@1  82.03 ( 80.69)	Acc@5  97.66 ( 97.14)
Epoch: [45][270/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9472e-01 (6.2965e-01)	Acc@1  82.81 ( 80.68)	Acc@5  96.88 ( 97.15)
Epoch: [45][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7751e-01 (6.3023e-01)	Acc@1  77.34 ( 80.69)	Acc@5  92.97 ( 97.10)
Epoch: [45][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3332e-01 (6.3037e-01)	Acc@1  76.56 ( 80.67)	Acc@5  96.88 ( 97.12)
Epoch: [45][300/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2040e-01 (6.3173e-01)	Acc@1  87.50 ( 80.62)	Acc@5  97.66 ( 97.10)
Epoch: [45][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7761e-01 (6.3140e-01)	Acc@1  78.12 ( 80.63)	Acc@5  98.44 ( 97.09)
Epoch: [45][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3054e-01 (6.3352e-01)	Acc@1  75.78 ( 80.59)	Acc@5  96.09 ( 97.07)
Epoch: [45][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6196e-01 (6.3274e-01)	Acc@1  80.47 ( 80.58)	Acc@5  98.44 ( 97.09)
Epoch: [45][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3735e-01 (6.3449e-01)	Acc@1  80.47 ( 80.54)	Acc@5  98.44 ( 97.07)
Epoch: [45][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3967e-01 (6.3552e-01)	Acc@1  82.81 ( 80.52)	Acc@5  96.88 ( 97.05)
Epoch: [45][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.7725e-01 (6.3683e-01)	Acc@1  76.56 ( 80.50)	Acc@5  94.53 ( 97.03)
Epoch: [45][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9641e-01 (6.3663e-01)	Acc@1  81.25 ( 80.52)	Acc@5  96.88 ( 97.04)
Epoch: [45][380/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9744e-01 (6.3660e-01)	Acc@1  71.09 ( 80.50)	Acc@5  98.44 ( 97.04)
Epoch: [45][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0102e-01 (6.3763e-01)	Acc@1  71.25 ( 80.49)	Acc@5  98.75 ( 97.02)
## e[45] optimizer.zero_grad (sum) time: 0.41469788551330566
## e[45]       loss.backward (sum) time: 6.966365098953247
## e[45]      optimizer.step (sum) time: 3.4758784770965576
## epoch[45] training(only) time: 25.581485509872437
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.3469e+00 (1.3469e+00)	Acc@1  64.00 ( 64.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3173e+00 (1.2336e+00)	Acc@1  66.00 ( 66.00)	Acc@5  90.00 ( 90.00)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.3008e+00 (1.1876e+00)	Acc@1  68.00 ( 66.90)	Acc@5  90.00 ( 91.00)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5957e+00 (1.2225e+00)	Acc@1  58.00 ( 66.13)	Acc@5  85.00 ( 90.65)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1217e+00 (1.2043e+00)	Acc@1  66.00 ( 66.27)	Acc@5  92.00 ( 91.17)
Test: [ 50/100]	Time  0.027 ( 0.030)	Loss 1.0961e+00 (1.2179e+00)	Acc@1  74.00 ( 66.10)	Acc@5  91.00 ( 90.73)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2742e+00 (1.2064e+00)	Acc@1  63.00 ( 66.49)	Acc@5  90.00 ( 90.79)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.1092e+00 (1.2146e+00)	Acc@1  70.00 ( 66.61)	Acc@5  93.00 ( 90.80)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.2833e+00 (1.2168e+00)	Acc@1  65.00 ( 66.59)	Acc@5  94.00 ( 90.68)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7452e+00 (1.2103e+00)	Acc@1  58.00 ( 66.78)	Acc@5  86.00 ( 90.67)
 * Acc@1 66.820 Acc@5 90.680
### epoch[45] execution time: 28.458364248275757
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.217 ( 0.217)	Data  0.146 ( 0.146)	Loss 5.8938e-01 (5.8938e-01)	Acc@1  81.25 ( 81.25)	Acc@5  99.22 ( 99.22)
Epoch: [46][ 10/391]	Time  0.063 ( 0.079)	Data  0.001 ( 0.014)	Loss 5.3542e-01 (5.9956e-01)	Acc@1  81.25 ( 81.39)	Acc@5  97.66 ( 97.73)
Epoch: [46][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 6.3910e-01 (6.1472e-01)	Acc@1  79.69 ( 80.51)	Acc@5  96.88 ( 97.36)
Epoch: [46][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 6.6657e-01 (6.1313e-01)	Acc@1  80.47 ( 80.82)	Acc@5  95.31 ( 97.25)
Epoch: [46][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 6.5503e-01 (6.1425e-01)	Acc@1  79.69 ( 80.75)	Acc@5  96.88 ( 97.35)
Epoch: [46][ 50/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.2958e-01 (6.0847e-01)	Acc@1  82.03 ( 80.94)	Acc@5  97.66 ( 97.49)
Epoch: [46][ 60/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.4502e-01 (5.9999e-01)	Acc@1  81.25 ( 81.34)	Acc@5  98.44 ( 97.55)
Epoch: [46][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.3607e-01 (6.0478e-01)	Acc@1  79.69 ( 81.26)	Acc@5  92.19 ( 97.41)
Epoch: [46][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.5508e-01 (6.0671e-01)	Acc@1  81.25 ( 81.27)	Acc@5  96.09 ( 97.37)
Epoch: [46][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.2835e-01 (6.0187e-01)	Acc@1  82.03 ( 81.40)	Acc@5  96.88 ( 97.47)
Epoch: [46][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.1139e-01 (6.0429e-01)	Acc@1  82.03 ( 81.27)	Acc@5  97.66 ( 97.44)
Epoch: [46][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.4133e-01 (6.0632e-01)	Acc@1  82.81 ( 81.26)	Acc@5  98.44 ( 97.39)
Epoch: [46][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4786e-01 (6.0813e-01)	Acc@1  75.00 ( 81.18)	Acc@5  96.09 ( 97.35)
Epoch: [46][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8522e-01 (6.1188e-01)	Acc@1  81.25 ( 81.02)	Acc@5  99.22 ( 97.37)
Epoch: [46][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1963e-01 (6.1538e-01)	Acc@1  82.81 ( 80.97)	Acc@5  93.75 ( 97.31)
Epoch: [46][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5526e-01 (6.1613e-01)	Acc@1  84.38 ( 80.99)	Acc@5  99.22 ( 97.30)
Epoch: [46][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.9233e-01 (6.1882e-01)	Acc@1  76.56 ( 80.92)	Acc@5  92.97 ( 97.28)
Epoch: [46][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2075e-01 (6.2038e-01)	Acc@1  79.69 ( 80.92)	Acc@5  98.44 ( 97.29)
Epoch: [46][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7792e-01 (6.2126e-01)	Acc@1  77.34 ( 80.85)	Acc@5  96.88 ( 97.28)
Epoch: [46][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7052e-01 (6.2028e-01)	Acc@1  84.38 ( 80.85)	Acc@5  96.88 ( 97.29)
Epoch: [46][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0858e-01 (6.2021e-01)	Acc@1  79.69 ( 80.80)	Acc@5  97.66 ( 97.29)
Epoch: [46][210/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8175e-01 (6.2151e-01)	Acc@1  79.69 ( 80.75)	Acc@5  98.44 ( 97.27)
Epoch: [46][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0686e-01 (6.2387e-01)	Acc@1  75.78 ( 80.70)	Acc@5  96.09 ( 97.25)
Epoch: [46][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3508e-01 (6.2669e-01)	Acc@1  75.78 ( 80.63)	Acc@5  97.66 ( 97.24)
Epoch: [46][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1253e-01 (6.2749e-01)	Acc@1  81.25 ( 80.59)	Acc@5  98.44 ( 97.23)
Epoch: [46][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5601e-01 (6.2890e-01)	Acc@1  77.34 ( 80.55)	Acc@5  97.66 ( 97.22)
Epoch: [46][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2673e-01 (6.3017e-01)	Acc@1  82.03 ( 80.55)	Acc@5  98.44 ( 97.20)
Epoch: [46][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2014e-01 (6.3060e-01)	Acc@1  81.25 ( 80.52)	Acc@5  98.44 ( 97.19)
Epoch: [46][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9567e-01 (6.2980e-01)	Acc@1  85.16 ( 80.58)	Acc@5  97.66 ( 97.21)
Epoch: [46][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5011e-01 (6.3030e-01)	Acc@1  79.69 ( 80.56)	Acc@5  97.66 ( 97.22)
Epoch: [46][300/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1690e-01 (6.3347e-01)	Acc@1  77.34 ( 80.43)	Acc@5  94.53 ( 97.17)
Epoch: [46][310/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.9999e-01 (6.3359e-01)	Acc@1  73.44 ( 80.43)	Acc@5  97.66 ( 97.17)
Epoch: [46][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8334e-01 (6.3422e-01)	Acc@1  85.16 ( 80.41)	Acc@5  99.22 ( 97.16)
Epoch: [46][330/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3547e-01 (6.3530e-01)	Acc@1  82.03 ( 80.39)	Acc@5  92.97 ( 97.12)
Epoch: [46][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8448e-01 (6.3439e-01)	Acc@1  76.56 ( 80.41)	Acc@5  93.75 ( 97.12)
Epoch: [46][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4034e-01 (6.3360e-01)	Acc@1  87.50 ( 80.40)	Acc@5  98.44 ( 97.15)
Epoch: [46][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4151e-01 (6.3516e-01)	Acc@1  85.94 ( 80.37)	Acc@5  99.22 ( 97.12)
Epoch: [46][370/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4307e-01 (6.3516e-01)	Acc@1  82.03 ( 80.40)	Acc@5  96.88 ( 97.11)
Epoch: [46][380/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8639e-01 (6.3419e-01)	Acc@1  82.03 ( 80.43)	Acc@5  97.66 ( 97.12)
Epoch: [46][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.6677e-01 (6.3427e-01)	Acc@1  76.25 ( 80.45)	Acc@5  96.25 ( 97.11)
## e[46] optimizer.zero_grad (sum) time: 0.41095614433288574
## e[46]       loss.backward (sum) time: 6.960206508636475
## e[46]      optimizer.step (sum) time: 3.51743483543396
## epoch[46] training(only) time: 25.639049530029297
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2795e+00 (1.2795e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3857e+00 (1.2367e+00)	Acc@1  67.00 ( 67.55)	Acc@5  91.00 ( 89.73)
Test: [ 20/100]	Time  0.028 ( 0.033)	Loss 1.2420e+00 (1.1974e+00)	Acc@1  72.00 ( 68.33)	Acc@5  91.00 ( 90.67)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.4819e+00 (1.2266e+00)	Acc@1  61.00 ( 67.55)	Acc@5  90.00 ( 90.45)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.0701e+00 (1.2028e+00)	Acc@1  68.00 ( 67.34)	Acc@5  93.00 ( 91.17)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.1969e+00 (1.2183e+00)	Acc@1  73.00 ( 66.90)	Acc@5  89.00 ( 90.94)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.3584e+00 (1.2117e+00)	Acc@1  61.00 ( 67.03)	Acc@5  88.00 ( 90.84)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.0604e+00 (1.2194e+00)	Acc@1  69.00 ( 67.06)	Acc@5  93.00 ( 90.70)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.3509e+00 (1.2226e+00)	Acc@1  63.00 ( 66.98)	Acc@5  92.00 ( 90.51)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.7316e+00 (1.2168e+00)	Acc@1  59.00 ( 67.23)	Acc@5  86.00 ( 90.54)
 * Acc@1 67.220 Acc@5 90.570
### epoch[46] execution time: 28.491247177124023
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.218 ( 0.218)	Data  0.148 ( 0.148)	Loss 5.5481e-01 (5.5481e-01)	Acc@1  83.59 ( 83.59)	Acc@5  96.88 ( 96.88)
Epoch: [47][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 6.2495e-01 (6.3024e-01)	Acc@1  81.25 ( 80.61)	Acc@5  97.66 ( 97.30)
Epoch: [47][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 4.3471e-01 (6.0688e-01)	Acc@1  85.16 ( 81.06)	Acc@5  98.44 ( 97.43)
Epoch: [47][ 30/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.1460e-01 (5.8958e-01)	Acc@1  88.28 ( 81.60)	Acc@5  97.66 ( 97.45)
Epoch: [47][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.8412e-01 (5.8749e-01)	Acc@1  85.16 ( 81.71)	Acc@5  97.66 ( 97.62)
Epoch: [47][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.2288e-01 (5.8868e-01)	Acc@1  82.03 ( 81.85)	Acc@5  98.44 ( 97.56)
Epoch: [47][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 6.3539e-01 (5.9076e-01)	Acc@1  80.47 ( 81.90)	Acc@5  97.66 ( 97.48)
Epoch: [47][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.2179e-01 (5.9036e-01)	Acc@1  82.03 ( 81.78)	Acc@5  98.44 ( 97.54)
Epoch: [47][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.1265e-01 (5.9496e-01)	Acc@1  89.06 ( 81.66)	Acc@5  96.88 ( 97.43)
Epoch: [47][ 90/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9472e-01 (6.0010e-01)	Acc@1  82.81 ( 81.53)	Acc@5 100.00 ( 97.41)
Epoch: [47][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.1513e-01 (5.9928e-01)	Acc@1  84.38 ( 81.64)	Acc@5  97.66 ( 97.36)
Epoch: [47][110/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.5059e-01 (5.9681e-01)	Acc@1  78.12 ( 81.72)	Acc@5  96.88 ( 97.37)
Epoch: [47][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7956e-01 (5.9534e-01)	Acc@1  83.59 ( 81.74)	Acc@5  99.22 ( 97.43)
Epoch: [47][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8023e-01 (5.9963e-01)	Acc@1  81.25 ( 81.63)	Acc@5  96.09 ( 97.40)
Epoch: [47][140/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.7078e-01 (6.0722e-01)	Acc@1  78.91 ( 81.41)	Acc@5  92.97 ( 97.30)
Epoch: [47][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0824e-01 (6.0843e-01)	Acc@1  83.59 ( 81.50)	Acc@5  92.97 ( 97.25)
Epoch: [47][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8621e-01 (6.1011e-01)	Acc@1  84.38 ( 81.44)	Acc@5  98.44 ( 97.25)
Epoch: [47][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3068e-01 (6.1050e-01)	Acc@1  82.81 ( 81.44)	Acc@5  95.31 ( 97.23)
Epoch: [47][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4717e-01 (6.1218e-01)	Acc@1  75.78 ( 81.36)	Acc@5  95.31 ( 97.22)
Epoch: [47][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0859e-01 (6.1161e-01)	Acc@1  81.25 ( 81.38)	Acc@5  96.09 ( 97.26)
Epoch: [47][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4374e-01 (6.1186e-01)	Acc@1  80.47 ( 81.41)	Acc@5  96.88 ( 97.27)
Epoch: [47][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3736e-01 (6.1259e-01)	Acc@1  81.25 ( 81.37)	Acc@5  96.09 ( 97.27)
Epoch: [47][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4767e-01 (6.1158e-01)	Acc@1  85.16 ( 81.39)	Acc@5  98.44 ( 97.28)
Epoch: [47][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6402e-01 (6.1247e-01)	Acc@1  84.38 ( 81.34)	Acc@5  98.44 ( 97.26)
Epoch: [47][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1368e-01 (6.1286e-01)	Acc@1  85.16 ( 81.31)	Acc@5  98.44 ( 97.26)
Epoch: [47][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8564e-01 (6.1410e-01)	Acc@1  75.78 ( 81.27)	Acc@5  96.09 ( 97.25)
Epoch: [47][260/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4347e-01 (6.1369e-01)	Acc@1  83.59 ( 81.27)	Acc@5  99.22 ( 97.25)
Epoch: [47][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1469e-01 (6.1346e-01)	Acc@1  88.28 ( 81.30)	Acc@5  97.66 ( 97.25)
Epoch: [47][280/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1044e-01 (6.1488e-01)	Acc@1  79.69 ( 81.26)	Acc@5  97.66 ( 97.23)
Epoch: [47][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0350e-01 (6.1573e-01)	Acc@1  81.25 ( 81.25)	Acc@5  97.66 ( 97.22)
Epoch: [47][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5269e-01 (6.1507e-01)	Acc@1  74.22 ( 81.23)	Acc@5  98.44 ( 97.23)
Epoch: [47][310/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9515e-01 (6.1446e-01)	Acc@1  83.59 ( 81.23)	Acc@5  98.44 ( 97.22)
Epoch: [47][320/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8209e-01 (6.1505e-01)	Acc@1  83.59 ( 81.19)	Acc@5  98.44 ( 97.23)
Epoch: [47][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7376e-01 (6.1639e-01)	Acc@1  78.12 ( 81.14)	Acc@5  96.88 ( 97.23)
Epoch: [47][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8258e-01 (6.1702e-01)	Acc@1  76.56 ( 81.11)	Acc@5  96.88 ( 97.23)
Epoch: [47][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9747e-01 (6.1657e-01)	Acc@1  84.38 ( 81.10)	Acc@5  98.44 ( 97.24)
Epoch: [47][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7097e-01 (6.1750e-01)	Acc@1  75.78 ( 81.06)	Acc@5  96.09 ( 97.24)
Epoch: [47][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5851e-01 (6.1838e-01)	Acc@1  83.59 ( 81.02)	Acc@5  99.22 ( 97.23)
Epoch: [47][380/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2461e-01 (6.1870e-01)	Acc@1  89.06 ( 81.01)	Acc@5  97.66 ( 97.24)
Epoch: [47][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8410e-01 (6.1897e-01)	Acc@1  83.75 ( 81.00)	Acc@5  96.25 ( 97.23)
## e[47] optimizer.zero_grad (sum) time: 0.4127347469329834
## e[47]       loss.backward (sum) time: 7.011215448379517
## e[47]      optimizer.step (sum) time: 3.458993911743164
## epoch[47] training(only) time: 25.655890941619873
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2888e+00 (1.2888e+00)	Acc@1  66.00 ( 66.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.3059e+00 (1.2360e+00)	Acc@1  67.00 ( 67.91)	Acc@5  90.00 ( 89.27)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.3323e+00 (1.2027e+00)	Acc@1  67.00 ( 67.95)	Acc@5  93.00 ( 90.48)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5403e+00 (1.2293e+00)	Acc@1  62.00 ( 67.29)	Acc@5  86.00 ( 90.19)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1306e+00 (1.2198e+00)	Acc@1  68.00 ( 67.00)	Acc@5  92.00 ( 90.44)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.1914e+00 (1.2345e+00)	Acc@1  70.00 ( 66.27)	Acc@5  90.00 ( 90.41)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.3501e+00 (1.2258e+00)	Acc@1  65.00 ( 66.44)	Acc@5  90.00 ( 90.54)
Test: [ 70/100]	Time  0.028 ( 0.029)	Loss 1.1415e+00 (1.2335e+00)	Acc@1  71.00 ( 66.70)	Acc@5  93.00 ( 90.54)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.3171e+00 (1.2369e+00)	Acc@1  64.00 ( 66.65)	Acc@5  91.00 ( 90.49)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.8654e+00 (1.2303e+00)	Acc@1  59.00 ( 66.91)	Acc@5  85.00 ( 90.57)
 * Acc@1 66.920 Acc@5 90.630
### epoch[47] execution time: 28.51818084716797
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.218 ( 0.218)	Data  0.146 ( 0.146)	Loss 5.1360e-01 (5.1360e-01)	Acc@1  85.16 ( 85.16)	Acc@5  96.88 ( 96.88)
Epoch: [48][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.014)	Loss 5.9806e-01 (5.6412e-01)	Acc@1  81.25 ( 83.59)	Acc@5  97.66 ( 97.30)
Epoch: [48][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.008)	Loss 6.2970e-01 (5.6578e-01)	Acc@1  79.69 ( 83.00)	Acc@5  96.88 ( 97.66)
Epoch: [48][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 6.8856e-01 (5.6750e-01)	Acc@1  81.25 ( 83.06)	Acc@5  95.31 ( 97.58)
Epoch: [48][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 6.9412e-01 (5.6857e-01)	Acc@1  72.66 ( 82.79)	Acc@5  98.44 ( 97.69)
Epoch: [48][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.6814e-01 (5.7490e-01)	Acc@1  87.50 ( 82.61)	Acc@5  98.44 ( 97.63)
Epoch: [48][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.2422e-01 (5.7885e-01)	Acc@1  81.25 ( 82.35)	Acc@5  98.44 ( 97.58)
Epoch: [48][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.9636e-01 (5.7248e-01)	Acc@1  83.59 ( 82.60)	Acc@5  96.09 ( 97.57)
Epoch: [48][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.2596e-01 (5.7832e-01)	Acc@1  75.78 ( 82.40)	Acc@5  98.44 ( 97.52)
Epoch: [48][ 90/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.3095e-01 (5.8383e-01)	Acc@1  85.16 ( 82.34)	Acc@5  96.88 ( 97.54)
Epoch: [48][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.9057e-01 (5.8968e-01)	Acc@1  85.94 ( 82.12)	Acc@5  98.44 ( 97.46)
Epoch: [48][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9931e-01 (5.8670e-01)	Acc@1  81.25 ( 82.06)	Acc@5  99.22 ( 97.49)
Epoch: [48][120/391]	Time  0.073 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3833e-01 (5.8335e-01)	Acc@1  78.12 ( 82.13)	Acc@5  98.44 ( 97.55)
Epoch: [48][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1305e-01 (5.8113e-01)	Acc@1  80.47 ( 82.20)	Acc@5  97.66 ( 97.58)
Epoch: [48][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3634e-01 (5.8445e-01)	Acc@1  85.94 ( 82.12)	Acc@5  97.66 ( 97.56)
Epoch: [48][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2044e-01 (5.8382e-01)	Acc@1  78.91 ( 82.15)	Acc@5  98.44 ( 97.60)
Epoch: [48][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1273e-01 (5.8311e-01)	Acc@1  82.03 ( 82.16)	Acc@5  96.88 ( 97.64)
Epoch: [48][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6049e-01 (5.8480e-01)	Acc@1  70.31 ( 82.08)	Acc@5  96.88 ( 97.59)
Epoch: [48][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5483e-01 (5.8636e-01)	Acc@1  81.25 ( 81.98)	Acc@5  98.44 ( 97.56)
Epoch: [48][190/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3773e-01 (5.8873e-01)	Acc@1  82.81 ( 81.92)	Acc@5  99.22 ( 97.55)
Epoch: [48][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9059e-01 (5.8736e-01)	Acc@1  83.59 ( 81.93)	Acc@5  97.66 ( 97.56)
Epoch: [48][210/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7592e-01 (5.8802e-01)	Acc@1  76.56 ( 81.85)	Acc@5  96.09 ( 97.53)
Epoch: [48][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7914e-01 (5.8973e-01)	Acc@1  78.12 ( 81.83)	Acc@5  96.09 ( 97.49)
Epoch: [48][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9018e-01 (5.8861e-01)	Acc@1  82.03 ( 81.87)	Acc@5  98.44 ( 97.49)
Epoch: [48][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5361e-01 (5.9010e-01)	Acc@1  85.16 ( 81.86)	Acc@5  99.22 ( 97.46)
Epoch: [48][250/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0420e-01 (5.9232e-01)	Acc@1  81.25 ( 81.79)	Acc@5  98.44 ( 97.42)
Epoch: [48][260/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0637e-01 (5.9354e-01)	Acc@1  78.12 ( 81.73)	Acc@5  97.66 ( 97.39)
Epoch: [48][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1631e-01 (5.9382e-01)	Acc@1  75.78 ( 81.68)	Acc@5  96.88 ( 97.39)
Epoch: [48][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8783e-01 (5.9405e-01)	Acc@1  84.38 ( 81.65)	Acc@5  98.44 ( 97.38)
Epoch: [48][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6293e-01 (5.9491e-01)	Acc@1  85.94 ( 81.63)	Acc@5  98.44 ( 97.36)
Epoch: [48][300/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0540e-01 (5.9654e-01)	Acc@1  83.59 ( 81.64)	Acc@5  95.31 ( 97.35)
Epoch: [48][310/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9559e-01 (5.9707e-01)	Acc@1  81.25 ( 81.62)	Acc@5  98.44 ( 97.34)
Epoch: [48][320/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3805e-01 (5.9912e-01)	Acc@1  79.69 ( 81.56)	Acc@5  96.88 ( 97.33)
Epoch: [48][330/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 9.3669e-01 (6.0084e-01)	Acc@1  72.66 ( 81.50)	Acc@5  93.75 ( 97.32)
Epoch: [48][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.6783e-01 (6.0171e-01)	Acc@1  71.88 ( 81.45)	Acc@5  96.88 ( 97.31)
Epoch: [48][350/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4487e-01 (6.0282e-01)	Acc@1  73.44 ( 81.40)	Acc@5  98.44 ( 97.32)
Epoch: [48][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9129e-01 (6.0279e-01)	Acc@1  76.56 ( 81.37)	Acc@5  99.22 ( 97.32)
Epoch: [48][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.0812e-01 (6.0438e-01)	Acc@1  73.44 ( 81.29)	Acc@5  96.09 ( 97.31)
Epoch: [48][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.6419e-01 (6.0590e-01)	Acc@1  79.69 ( 81.25)	Acc@5  93.75 ( 97.29)
Epoch: [48][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.5582e-01 (6.0679e-01)	Acc@1  73.75 ( 81.21)	Acc@5  96.25 ( 97.29)
## e[48] optimizer.zero_grad (sum) time: 0.41513895988464355
## e[48]       loss.backward (sum) time: 6.996452331542969
## e[48]      optimizer.step (sum) time: 3.377251625061035
## epoch[48] training(only) time: 25.61900305747986
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.3686e+00 (1.3686e+00)	Acc@1  64.00 ( 64.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.030 ( 0.039)	Loss 1.2664e+00 (1.2583e+00)	Acc@1  65.00 ( 67.00)	Acc@5  90.00 ( 89.27)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.2593e+00 (1.2100e+00)	Acc@1  69.00 ( 67.48)	Acc@5  92.00 ( 90.05)
Test: [ 30/100]	Time  0.033 ( 0.031)	Loss 1.5804e+00 (1.2348e+00)	Acc@1  62.00 ( 67.23)	Acc@5  89.00 ( 90.03)
Test: [ 40/100]	Time  0.029 ( 0.030)	Loss 1.1533e+00 (1.2185e+00)	Acc@1  66.00 ( 66.95)	Acc@5  93.00 ( 90.78)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.1540e+00 (1.2377e+00)	Acc@1  73.00 ( 66.41)	Acc@5  88.00 ( 90.63)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3764e+00 (1.2308e+00)	Acc@1  61.00 ( 66.44)	Acc@5  90.00 ( 90.72)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.2601e+00 (1.2378e+00)	Acc@1  63.00 ( 66.61)	Acc@5  93.00 ( 90.66)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.2990e+00 (1.2432e+00)	Acc@1  64.00 ( 66.43)	Acc@5  92.00 ( 90.57)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7195e+00 (1.2347e+00)	Acc@1  63.00 ( 66.55)	Acc@5  82.00 ( 90.55)
 * Acc@1 66.680 Acc@5 90.610
### epoch[48] execution time: 28.478583335876465
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.219 ( 0.219)	Data  0.146 ( 0.146)	Loss 6.0101e-01 (6.0101e-01)	Acc@1  80.47 ( 80.47)	Acc@5  96.88 ( 96.88)
Epoch: [49][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 5.6194e-01 (5.5454e-01)	Acc@1  87.50 ( 83.10)	Acc@5  96.09 ( 97.80)
Epoch: [49][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 5.8266e-01 (5.8427e-01)	Acc@1  86.72 ( 81.96)	Acc@5  95.31 ( 97.32)
Epoch: [49][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 6.3643e-01 (5.9376e-01)	Acc@1  77.34 ( 81.48)	Acc@5  98.44 ( 97.43)
Epoch: [49][ 40/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.005)	Loss 5.8063e-01 (5.9103e-01)	Acc@1  85.94 ( 81.54)	Acc@5  98.44 ( 97.47)
Epoch: [49][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.9222e-01 (5.8225e-01)	Acc@1  82.03 ( 81.76)	Acc@5  99.22 ( 97.47)
Epoch: [49][ 60/391]	Time  0.064 ( 0.067)	Data  0.002 ( 0.004)	Loss 7.0009e-01 (5.8765e-01)	Acc@1  79.69 ( 81.70)	Acc@5  96.09 ( 97.41)
Epoch: [49][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.1861e-01 (5.7755e-01)	Acc@1  85.94 ( 82.10)	Acc@5  97.66 ( 97.54)
Epoch: [49][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.9207e-01 (5.7683e-01)	Acc@1  80.47 ( 82.02)	Acc@5  97.66 ( 97.56)
Epoch: [49][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.5349e-01 (5.7575e-01)	Acc@1  84.38 ( 82.03)	Acc@5  97.66 ( 97.63)
Epoch: [49][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 6.5821e-01 (5.7536e-01)	Acc@1  79.69 ( 82.09)	Acc@5  96.88 ( 97.66)
Epoch: [49][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4060e-01 (5.7571e-01)	Acc@1  83.59 ( 82.09)	Acc@5  98.44 ( 97.64)
Epoch: [49][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9060e-01 (5.7643e-01)	Acc@1  85.16 ( 82.06)	Acc@5  98.44 ( 97.66)
Epoch: [49][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0578e-01 (5.7326e-01)	Acc@1  85.94 ( 82.19)	Acc@5  98.44 ( 97.68)
Epoch: [49][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4891e-01 (5.7085e-01)	Acc@1  87.50 ( 82.24)	Acc@5  98.44 ( 97.73)
Epoch: [49][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7498e-01 (5.7116e-01)	Acc@1  79.69 ( 82.20)	Acc@5  97.66 ( 97.70)
Epoch: [49][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0098e-01 (5.7454e-01)	Acc@1  77.34 ( 82.07)	Acc@5  95.31 ( 97.65)
Epoch: [49][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5322e-01 (5.7712e-01)	Acc@1  83.59 ( 81.99)	Acc@5  96.88 ( 97.65)
Epoch: [49][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5290e-01 (5.8069e-01)	Acc@1  85.16 ( 81.92)	Acc@5  96.88 ( 97.58)
Epoch: [49][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6945e-01 (5.8443e-01)	Acc@1  82.03 ( 81.77)	Acc@5  95.31 ( 97.56)
Epoch: [49][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2551e-01 (5.8305e-01)	Acc@1  83.59 ( 81.84)	Acc@5  97.66 ( 97.57)
Epoch: [49][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1262e-01 (5.8581e-01)	Acc@1  78.91 ( 81.79)	Acc@5  96.09 ( 97.55)
Epoch: [49][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5403e-01 (5.8760e-01)	Acc@1  82.81 ( 81.79)	Acc@5  96.09 ( 97.54)
Epoch: [49][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4276e-01 (5.8777e-01)	Acc@1  83.59 ( 81.81)	Acc@5  96.88 ( 97.53)
Epoch: [49][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9577e-01 (5.9056e-01)	Acc@1  82.81 ( 81.72)	Acc@5  98.44 ( 97.48)
Epoch: [49][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6594e-01 (5.9017e-01)	Acc@1  79.69 ( 81.79)	Acc@5  97.66 ( 97.48)
Epoch: [49][260/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1535e-01 (5.9100e-01)	Acc@1  81.25 ( 81.77)	Acc@5  96.88 ( 97.45)
Epoch: [49][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7781e-01 (5.9146e-01)	Acc@1  74.22 ( 81.69)	Acc@5  96.09 ( 97.44)
Epoch: [49][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8727e-01 (5.9113e-01)	Acc@1  83.59 ( 81.70)	Acc@5  97.66 ( 97.46)
Epoch: [49][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3774e-01 (5.9161e-01)	Acc@1  83.59 ( 81.67)	Acc@5  97.66 ( 97.47)
Epoch: [49][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6755e-01 (5.9320e-01)	Acc@1  82.03 ( 81.63)	Acc@5  95.31 ( 97.47)
Epoch: [49][310/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.7586e-01 (5.9480e-01)	Acc@1  75.78 ( 81.55)	Acc@5  95.31 ( 97.46)
Epoch: [49][320/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9385e-01 (5.9619e-01)	Acc@1  84.38 ( 81.53)	Acc@5  96.88 ( 97.46)
Epoch: [49][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.5459e-01 (5.9525e-01)	Acc@1  75.00 ( 81.57)	Acc@5  96.88 ( 97.45)
Epoch: [49][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5447e-01 (5.9537e-01)	Acc@1  86.72 ( 81.57)	Acc@5  97.66 ( 97.47)
Epoch: [49][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7072e-01 (5.9576e-01)	Acc@1  82.03 ( 81.57)	Acc@5  98.44 ( 97.46)
Epoch: [49][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7753e-01 (5.9720e-01)	Acc@1  81.25 ( 81.54)	Acc@5  98.44 ( 97.45)
Epoch: [49][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4809e-01 (5.9760e-01)	Acc@1  80.47 ( 81.53)	Acc@5 100.00 ( 97.44)
Epoch: [49][380/391]	Time  0.059 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8120e-01 (5.9870e-01)	Acc@1  78.12 ( 81.48)	Acc@5  98.44 ( 97.43)
Epoch: [49][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6270e-01 (5.9883e-01)	Acc@1  76.25 ( 81.47)	Acc@5  96.25 ( 97.44)
## e[49] optimizer.zero_grad (sum) time: 0.40467262268066406
## e[49]       loss.backward (sum) time: 7.007558822631836
## e[49]      optimizer.step (sum) time: 3.4696033000946045
## epoch[49] training(only) time: 25.590956211090088
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.4303e+00 (1.4303e+00)	Acc@1  65.00 ( 65.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.027 ( 0.038)	Loss 1.4021e+00 (1.2568e+00)	Acc@1  63.00 ( 66.91)	Acc@5  89.00 ( 89.73)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.1464e+00 (1.2068e+00)	Acc@1  70.00 ( 67.24)	Acc@5  92.00 ( 90.71)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6060e+00 (1.2430e+00)	Acc@1  62.00 ( 66.81)	Acc@5  85.00 ( 90.23)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.0802e+00 (1.2349e+00)	Acc@1  69.00 ( 66.27)	Acc@5  94.00 ( 90.85)
Test: [ 50/100]	Time  0.030 ( 0.029)	Loss 1.2109e+00 (1.2487e+00)	Acc@1  74.00 ( 65.94)	Acc@5  89.00 ( 90.55)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3617e+00 (1.2400e+00)	Acc@1  62.00 ( 66.15)	Acc@5  89.00 ( 90.64)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.2146e+00 (1.2528e+00)	Acc@1  63.00 ( 66.25)	Acc@5  93.00 ( 90.51)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 1.3659e+00 (1.2549e+00)	Acc@1  62.00 ( 66.17)	Acc@5  91.00 ( 90.38)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.9227e+00 (1.2485e+00)	Acc@1  54.00 ( 66.44)	Acc@5  86.00 ( 90.41)
 * Acc@1 66.610 Acc@5 90.500
### epoch[49] execution time: 28.456315279006958
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.219 ( 0.219)	Data  0.149 ( 0.149)	Loss 4.9027e-01 (4.9027e-01)	Acc@1  87.50 ( 87.50)	Acc@5  98.44 ( 98.44)
Epoch: [50][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.015)	Loss 4.0759e-01 (5.6555e-01)	Acc@1  91.41 ( 83.24)	Acc@5  98.44 ( 97.66)
Epoch: [50][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 5.4709e-01 (5.5120e-01)	Acc@1  80.47 ( 82.74)	Acc@5  96.88 ( 97.99)
Epoch: [50][ 30/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.006)	Loss 6.3331e-01 (5.4971e-01)	Acc@1  82.81 ( 82.79)	Acc@5  96.88 ( 98.03)
Epoch: [50][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 6.1036e-01 (5.6648e-01)	Acc@1  81.25 ( 82.15)	Acc@5  96.88 ( 97.75)
Epoch: [50][ 50/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.6744e-01 (5.6361e-01)	Acc@1  82.81 ( 82.31)	Acc@5  96.88 ( 97.76)
Epoch: [50][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.9386e-01 (5.6453e-01)	Acc@1  83.59 ( 82.36)	Acc@5  98.44 ( 97.77)
Epoch: [50][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.7727e-01 (5.6869e-01)	Acc@1  78.91 ( 82.24)	Acc@5  98.44 ( 97.76)
Epoch: [50][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.5775e-01 (5.6541e-01)	Acc@1  84.38 ( 82.32)	Acc@5  98.44 ( 97.73)
Epoch: [50][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.8481e-01 (5.6446e-01)	Acc@1  86.72 ( 82.39)	Acc@5  98.44 ( 97.73)
Epoch: [50][100/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.1593e-01 (5.6539e-01)	Acc@1  82.81 ( 82.41)	Acc@5  97.66 ( 97.69)
Epoch: [50][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2443e-01 (5.6539e-01)	Acc@1  81.25 ( 82.33)	Acc@5  99.22 ( 97.74)
Epoch: [50][120/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0173e-01 (5.6788e-01)	Acc@1  75.78 ( 82.20)	Acc@5  99.22 ( 97.71)
Epoch: [50][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4637e-01 (5.6986e-01)	Acc@1  85.94 ( 82.17)	Acc@5  99.22 ( 97.72)
Epoch: [50][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7724e-01 (5.7172e-01)	Acc@1  78.91 ( 82.15)	Acc@5  98.44 ( 97.63)
Epoch: [50][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3079e-01 (5.7176e-01)	Acc@1  87.50 ( 82.17)	Acc@5  98.44 ( 97.62)
Epoch: [50][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6731e-01 (5.7508e-01)	Acc@1  79.69 ( 82.12)	Acc@5  97.66 ( 97.59)
Epoch: [50][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1059e-01 (5.7638e-01)	Acc@1  81.25 ( 82.10)	Acc@5  97.66 ( 97.54)
Epoch: [50][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3954e-01 (5.7690e-01)	Acc@1  82.81 ( 82.06)	Acc@5  96.88 ( 97.54)
Epoch: [50][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6980e-01 (5.7588e-01)	Acc@1  84.38 ( 82.04)	Acc@5  99.22 ( 97.57)
Epoch: [50][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0970e-01 (5.7551e-01)	Acc@1  82.81 ( 82.08)	Acc@5  96.09 ( 97.60)
Epoch: [50][210/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4803e-01 (5.7563e-01)	Acc@1  80.47 ( 82.05)	Acc@5  98.44 ( 97.64)
Epoch: [50][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5965e-01 (5.7638e-01)	Acc@1  80.47 ( 82.01)	Acc@5  94.53 ( 97.64)
Epoch: [50][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0171e-01 (5.8030e-01)	Acc@1  82.81 ( 81.88)	Acc@5  97.66 ( 97.64)
Epoch: [50][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6408e-01 (5.7919e-01)	Acc@1  85.16 ( 81.92)	Acc@5  97.66 ( 97.65)
Epoch: [50][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5389e-01 (5.8081e-01)	Acc@1  87.50 ( 81.90)	Acc@5  99.22 ( 97.63)
Epoch: [50][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6002e-01 (5.8299e-01)	Acc@1  78.91 ( 81.81)	Acc@5  98.44 ( 97.61)
Epoch: [50][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8818e-01 (5.8298e-01)	Acc@1  83.59 ( 81.83)	Acc@5  96.88 ( 97.59)
Epoch: [50][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1100e-01 (5.8512e-01)	Acc@1  78.12 ( 81.76)	Acc@5  98.44 ( 97.60)
Epoch: [50][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5381e-01 (5.8655e-01)	Acc@1  82.81 ( 81.70)	Acc@5  96.88 ( 97.59)
Epoch: [50][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9367e-01 (5.8813e-01)	Acc@1  85.16 ( 81.69)	Acc@5  99.22 ( 97.54)
Epoch: [50][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4988e-01 (5.8626e-01)	Acc@1  87.50 ( 81.76)	Acc@5  98.44 ( 97.56)
Epoch: [50][320/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5572e-01 (5.8832e-01)	Acc@1  82.03 ( 81.72)	Acc@5  96.88 ( 97.53)
Epoch: [50][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2670e-01 (5.8974e-01)	Acc@1  75.78 ( 81.69)	Acc@5  94.53 ( 97.52)
Epoch: [50][340/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7625e-01 (5.9056e-01)	Acc@1  82.81 ( 81.66)	Acc@5  96.09 ( 97.49)
Epoch: [50][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6489e-01 (5.9211e-01)	Acc@1  83.59 ( 81.62)	Acc@5  99.22 ( 97.49)
Epoch: [50][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.1240e-01 (5.9264e-01)	Acc@1  78.12 ( 81.61)	Acc@5  96.88 ( 97.49)
Epoch: [50][370/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2455e-01 (5.9248e-01)	Acc@1  82.03 ( 81.60)	Acc@5  97.66 ( 97.51)
Epoch: [50][380/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0254e-01 (5.9282e-01)	Acc@1  78.91 ( 81.58)	Acc@5  96.09 ( 97.50)
Epoch: [50][390/391]	Time  0.057 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1012e-01 (5.9228e-01)	Acc@1  86.25 ( 81.60)	Acc@5  97.50 ( 97.52)
## e[50] optimizer.zero_grad (sum) time: 0.4156162738800049
## e[50]       loss.backward (sum) time: 7.007958650588989
## e[50]      optimizer.step (sum) time: 3.503211259841919
## epoch[50] training(only) time: 25.6987624168396
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3974e+00 (1.3974e+00)	Acc@1  68.00 ( 68.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.2846e+00 (1.2566e+00)	Acc@1  68.00 ( 67.45)	Acc@5  91.00 ( 89.55)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.2867e+00 (1.2161e+00)	Acc@1  66.00 ( 67.62)	Acc@5  91.00 ( 90.48)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6172e+00 (1.2499e+00)	Acc@1  61.00 ( 66.90)	Acc@5  88.00 ( 90.32)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.0991e+00 (1.2348e+00)	Acc@1  66.00 ( 66.80)	Acc@5  95.00 ( 90.85)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.2214e+00 (1.2521e+00)	Acc@1  73.00 ( 66.33)	Acc@5  89.00 ( 90.55)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.4058e+00 (1.2413e+00)	Acc@1  62.00 ( 66.48)	Acc@5  86.00 ( 90.69)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.1603e+00 (1.2513e+00)	Acc@1  70.00 ( 66.62)	Acc@5  92.00 ( 90.61)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3865e+00 (1.2580e+00)	Acc@1  63.00 ( 66.53)	Acc@5  91.00 ( 90.49)
Test: [ 90/100]	Time  0.029 ( 0.028)	Loss 1.7940e+00 (1.2516e+00)	Acc@1  59.00 ( 66.68)	Acc@5  87.00 ( 90.49)
 * Acc@1 66.780 Acc@5 90.550
### epoch[50] execution time: 28.520493268966675
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.219 ( 0.219)	Data  0.149 ( 0.149)	Loss 5.7461e-01 (5.7461e-01)	Acc@1  84.38 ( 84.38)	Acc@5  96.09 ( 96.09)
Epoch: [51][ 10/391]	Time  0.065 ( 0.079)	Data  0.001 ( 0.015)	Loss 6.4615e-01 (5.4255e-01)	Acc@1  78.91 ( 83.52)	Acc@5  96.88 ( 98.08)
Epoch: [51][ 20/391]	Time  0.071 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.9913e-01 (5.5375e-01)	Acc@1  83.59 ( 82.74)	Acc@5  97.66 ( 97.84)
Epoch: [51][ 30/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.006)	Loss 6.2065e-01 (5.6759e-01)	Acc@1  82.81 ( 82.33)	Acc@5  95.31 ( 97.68)
Epoch: [51][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.1236e-01 (5.6728e-01)	Acc@1  87.50 ( 82.55)	Acc@5  99.22 ( 97.83)
Epoch: [51][ 50/391]	Time  0.071 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.8290e-01 (5.6135e-01)	Acc@1  87.50 ( 82.86)	Acc@5  97.66 ( 97.86)
Epoch: [51][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.6633e-01 (5.6285e-01)	Acc@1  85.16 ( 82.71)	Acc@5  97.66 ( 97.82)
Epoch: [51][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.1945e-01 (5.5694e-01)	Acc@1  84.38 ( 82.94)	Acc@5  98.44 ( 97.88)
Epoch: [51][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.4540e-01 (5.5711e-01)	Acc@1  82.81 ( 83.02)	Acc@5  98.44 ( 97.85)
Epoch: [51][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.3320e-01 (5.5525e-01)	Acc@1  88.28 ( 83.06)	Acc@5  97.66 ( 97.79)
Epoch: [51][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.6108e-01 (5.5914e-01)	Acc@1  85.16 ( 82.94)	Acc@5  96.88 ( 97.76)
Epoch: [51][110/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2827e-01 (5.5775e-01)	Acc@1  79.69 ( 82.88)	Acc@5  97.66 ( 97.77)
Epoch: [51][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6404e-01 (5.5965e-01)	Acc@1  80.47 ( 82.73)	Acc@5  99.22 ( 97.79)
Epoch: [51][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7134e-01 (5.5764e-01)	Acc@1  82.03 ( 82.76)	Acc@5  99.22 ( 97.82)
Epoch: [51][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2516e-01 (5.5746e-01)	Acc@1  79.69 ( 82.71)	Acc@5  96.88 ( 97.81)
Epoch: [51][150/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2840e-01 (5.6131e-01)	Acc@1  77.34 ( 82.52)	Acc@5  93.75 ( 97.79)
Epoch: [51][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5832e-01 (5.6003e-01)	Acc@1  86.72 ( 82.62)	Acc@5  97.66 ( 97.78)
Epoch: [51][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2066e-01 (5.6179e-01)	Acc@1  79.69 ( 82.57)	Acc@5  97.66 ( 97.77)
Epoch: [51][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8954e-01 (5.6135e-01)	Acc@1  84.38 ( 82.63)	Acc@5  96.88 ( 97.79)
Epoch: [51][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.0368e-01 (5.6293e-01)	Acc@1  75.00 ( 82.55)	Acc@5  94.53 ( 97.78)
Epoch: [51][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4852e-01 (5.6358e-01)	Acc@1  83.59 ( 82.52)	Acc@5  98.44 ( 97.78)
Epoch: [51][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4519e-01 (5.6700e-01)	Acc@1  82.81 ( 82.36)	Acc@5  97.66 ( 97.77)
Epoch: [51][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9944e-01 (5.6521e-01)	Acc@1  82.81 ( 82.44)	Acc@5  99.22 ( 97.81)
Epoch: [51][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3834e-01 (5.6641e-01)	Acc@1  89.06 ( 82.38)	Acc@5  98.44 ( 97.79)
Epoch: [51][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7296e-01 (5.6596e-01)	Acc@1  78.91 ( 82.40)	Acc@5  96.88 ( 97.78)
Epoch: [51][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3591e-01 (5.6660e-01)	Acc@1  85.94 ( 82.41)	Acc@5  97.66 ( 97.77)
Epoch: [51][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2517e-01 (5.6729e-01)	Acc@1  78.12 ( 82.42)	Acc@5  96.09 ( 97.74)
Epoch: [51][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1359e-01 (5.6787e-01)	Acc@1  76.56 ( 82.38)	Acc@5  99.22 ( 97.73)
Epoch: [51][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1797e-01 (5.6958e-01)	Acc@1  83.59 ( 82.35)	Acc@5  98.44 ( 97.73)
Epoch: [51][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.0495e-01 (5.7076e-01)	Acc@1  75.78 ( 82.32)	Acc@5  94.53 ( 97.70)
Epoch: [51][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6204e-01 (5.7058e-01)	Acc@1  82.03 ( 82.30)	Acc@5  98.44 ( 97.70)
Epoch: [51][310/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2531e-01 (5.7023e-01)	Acc@1  78.91 ( 82.32)	Acc@5  98.44 ( 97.69)
Epoch: [51][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4213e-01 (5.6975e-01)	Acc@1  81.25 ( 82.32)	Acc@5  98.44 ( 97.70)
Epoch: [51][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1451e-01 (5.7042e-01)	Acc@1  81.25 ( 82.30)	Acc@5  96.88 ( 97.70)
Epoch: [51][340/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.9952e-01 (5.7174e-01)	Acc@1  78.91 ( 82.25)	Acc@5  96.88 ( 97.69)
Epoch: [51][350/391]	Time  0.066 ( 0.065)	Data  0.002 ( 0.002)	Loss 6.1466e-01 (5.7199e-01)	Acc@1  80.47 ( 82.24)	Acc@5  96.88 ( 97.67)
Epoch: [51][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6417e-01 (5.7349e-01)	Acc@1  81.25 ( 82.18)	Acc@5  99.22 ( 97.66)
Epoch: [51][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6056e-01 (5.7297e-01)	Acc@1  85.94 ( 82.18)	Acc@5  99.22 ( 97.68)
Epoch: [51][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1709e-01 (5.7437e-01)	Acc@1  82.81 ( 82.14)	Acc@5  98.44 ( 97.67)
Epoch: [51][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8419e-01 (5.7512e-01)	Acc@1  86.25 ( 82.11)	Acc@5  97.50 ( 97.66)
## e[51] optimizer.zero_grad (sum) time: 0.4151942729949951
## e[51]       loss.backward (sum) time: 6.9691667556762695
## e[51]      optimizer.step (sum) time: 3.4609546661376953
## epoch[51] training(only) time: 25.554513454437256
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.3979e+00 (1.3979e+00)	Acc@1  70.00 ( 70.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.028 ( 0.040)	Loss 1.3355e+00 (1.2614e+00)	Acc@1  64.00 ( 67.82)	Acc@5  91.00 ( 89.82)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.1637e+00 (1.2261e+00)	Acc@1  69.00 ( 67.81)	Acc@5  91.00 ( 90.52)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5465e+00 (1.2463e+00)	Acc@1  61.00 ( 67.26)	Acc@5  89.00 ( 90.29)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1403e+00 (1.2278e+00)	Acc@1  65.00 ( 67.10)	Acc@5  92.00 ( 91.05)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2863e+00 (1.2520e+00)	Acc@1  73.00 ( 66.63)	Acc@5  89.00 ( 90.69)
Test: [ 60/100]	Time  0.029 ( 0.029)	Loss 1.3478e+00 (1.2411e+00)	Acc@1  58.00 ( 66.75)	Acc@5  88.00 ( 90.87)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.1506e+00 (1.2463e+00)	Acc@1  68.00 ( 67.01)	Acc@5  93.00 ( 90.76)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.4287e+00 (1.2546e+00)	Acc@1  62.00 ( 66.69)	Acc@5  89.00 ( 90.67)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.7765e+00 (1.2467e+00)	Acc@1  60.00 ( 66.93)	Acc@5  85.00 ( 90.76)
 * Acc@1 67.010 Acc@5 90.770
### epoch[51] execution time: 28.39123511314392
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.228 ( 0.228)	Data  0.159 ( 0.159)	Loss 5.8104e-01 (5.8104e-01)	Acc@1  83.59 ( 83.59)	Acc@5  96.09 ( 96.09)
Epoch: [52][ 10/391]	Time  0.072 ( 0.080)	Data  0.001 ( 0.015)	Loss 4.9235e-01 (5.3661e-01)	Acc@1  86.72 ( 84.59)	Acc@5  96.88 ( 97.66)
Epoch: [52][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.009)	Loss 5.4351e-01 (5.2266e-01)	Acc@1  85.94 ( 84.45)	Acc@5  96.88 ( 97.81)
Epoch: [52][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 5.6005e-01 (5.2297e-01)	Acc@1  85.16 ( 83.90)	Acc@5  97.66 ( 97.96)
Epoch: [52][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.3931e-01 (5.2450e-01)	Acc@1  85.16 ( 83.73)	Acc@5  97.66 ( 97.98)
Epoch: [52][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.8052e-01 (5.2820e-01)	Acc@1  84.38 ( 83.82)	Acc@5  98.44 ( 97.96)
Epoch: [52][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.0511e-01 (5.3981e-01)	Acc@1  79.69 ( 83.38)	Acc@5  99.22 ( 97.96)
Epoch: [52][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.9931e-01 (5.4295e-01)	Acc@1  85.94 ( 83.26)	Acc@5  98.44 ( 97.99)
Epoch: [52][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4320e-01 (5.4461e-01)	Acc@1  88.28 ( 83.27)	Acc@5  98.44 ( 97.98)
Epoch: [52][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.5655e-01 (5.4243e-01)	Acc@1  76.56 ( 83.33)	Acc@5  95.31 ( 98.02)
Epoch: [52][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.3933e-01 (5.4527e-01)	Acc@1  80.47 ( 83.25)	Acc@5  98.44 ( 98.00)
Epoch: [52][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.3750e-01 (5.4380e-01)	Acc@1  80.47 ( 83.25)	Acc@5  98.44 ( 98.00)
Epoch: [52][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6523e-01 (5.4563e-01)	Acc@1  83.59 ( 83.13)	Acc@5  96.88 ( 97.97)
Epoch: [52][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3657e-01 (5.4518e-01)	Acc@1  85.16 ( 83.12)	Acc@5  97.66 ( 98.00)
Epoch: [52][140/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4401e-01 (5.4478e-01)	Acc@1  86.72 ( 83.14)	Acc@5  99.22 ( 98.02)
Epoch: [52][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2008e-01 (5.4602e-01)	Acc@1  83.59 ( 83.12)	Acc@5  96.88 ( 97.96)
Epoch: [52][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3529e-01 (5.4520e-01)	Acc@1  85.16 ( 83.06)	Acc@5  99.22 ( 97.97)
Epoch: [52][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3211e-01 (5.4506e-01)	Acc@1  85.16 ( 83.13)	Acc@5  96.88 ( 97.96)
Epoch: [52][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.6630e-01 (5.5203e-01)	Acc@1  75.78 ( 82.94)	Acc@5  96.09 ( 97.92)
Epoch: [52][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8787e-01 (5.5188e-01)	Acc@1  82.03 ( 82.89)	Acc@5  99.22 ( 97.92)
Epoch: [52][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6100e-01 (5.5150e-01)	Acc@1  85.94 ( 82.90)	Acc@5  97.66 ( 97.93)
Epoch: [52][210/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3925e-01 (5.5113e-01)	Acc@1  87.50 ( 82.95)	Acc@5  99.22 ( 97.92)
Epoch: [52][220/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6147e-01 (5.5379e-01)	Acc@1  75.00 ( 82.90)	Acc@5  98.44 ( 97.88)
Epoch: [52][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0213e-01 (5.5328e-01)	Acc@1  85.16 ( 82.90)	Acc@5  96.09 ( 97.89)
Epoch: [52][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9013e-01 (5.5254e-01)	Acc@1  83.59 ( 82.94)	Acc@5  96.88 ( 97.89)
Epoch: [52][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1937e-01 (5.5307e-01)	Acc@1  81.25 ( 82.92)	Acc@5  96.88 ( 97.87)
Epoch: [52][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3980e-01 (5.5448e-01)	Acc@1  77.34 ( 82.90)	Acc@5  96.88 ( 97.86)
Epoch: [52][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9828e-01 (5.5432e-01)	Acc@1  85.16 ( 82.90)	Acc@5  97.66 ( 97.85)
Epoch: [52][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6339e-01 (5.5462e-01)	Acc@1  82.81 ( 82.87)	Acc@5  96.88 ( 97.85)
Epoch: [52][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9680e-01 (5.5552e-01)	Acc@1  80.47 ( 82.84)	Acc@5  96.88 ( 97.82)
Epoch: [52][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5203e-01 (5.5665e-01)	Acc@1  82.03 ( 82.80)	Acc@5  96.09 ( 97.78)
Epoch: [52][310/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.1858e-01 (5.5712e-01)	Acc@1  82.03 ( 82.79)	Acc@5  97.66 ( 97.77)
Epoch: [52][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.1718e-01 (5.5841e-01)	Acc@1  77.34 ( 82.78)	Acc@5  93.75 ( 97.76)
Epoch: [52][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2560e-01 (5.5950e-01)	Acc@1  81.25 ( 82.73)	Acc@5 100.00 ( 97.77)
Epoch: [52][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6618e-01 (5.6026e-01)	Acc@1  76.56 ( 82.74)	Acc@5  95.31 ( 97.75)
Epoch: [52][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.3969e-01 (5.6010e-01)	Acc@1  78.12 ( 82.74)	Acc@5  97.66 ( 97.76)
Epoch: [52][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8716e-01 (5.5978e-01)	Acc@1  82.03 ( 82.72)	Acc@5  96.09 ( 97.77)
Epoch: [52][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6338e-01 (5.6124e-01)	Acc@1  83.59 ( 82.67)	Acc@5  96.88 ( 97.75)
Epoch: [52][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0589e-01 (5.6180e-01)	Acc@1  78.12 ( 82.65)	Acc@5  99.22 ( 97.74)
Epoch: [52][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.8442e-01 (5.6376e-01)	Acc@1  76.25 ( 82.58)	Acc@5  96.25 ( 97.72)
## e[52] optimizer.zero_grad (sum) time: 0.411618709564209
## e[52]       loss.backward (sum) time: 6.960448503494263
## e[52]      optimizer.step (sum) time: 3.4588067531585693
## epoch[52] training(only) time: 25.5998432636261
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.3597e+00 (1.3597e+00)	Acc@1  69.00 ( 69.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.2255e+00 (1.2530e+00)	Acc@1  69.00 ( 67.18)	Acc@5  91.00 ( 89.73)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.1504e+00 (1.2096e+00)	Acc@1  71.00 ( 68.05)	Acc@5  91.00 ( 90.67)
Test: [ 30/100]	Time  0.029 ( 0.031)	Loss 1.5008e+00 (1.2477e+00)	Acc@1  61.00 ( 67.52)	Acc@5  88.00 ( 90.45)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1464e+00 (1.2237e+00)	Acc@1  67.00 ( 67.39)	Acc@5  95.00 ( 91.12)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2398e+00 (1.2483e+00)	Acc@1  72.00 ( 66.90)	Acc@5  90.00 ( 90.90)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2363e+00 (1.2383e+00)	Acc@1  63.00 ( 66.84)	Acc@5  89.00 ( 90.84)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.1304e+00 (1.2479e+00)	Acc@1  65.00 ( 66.85)	Acc@5  93.00 ( 90.69)
Test: [ 80/100]	Time  0.031 ( 0.028)	Loss 1.3933e+00 (1.2544e+00)	Acc@1  62.00 ( 66.68)	Acc@5  90.00 ( 90.57)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.8215e+00 (1.2470e+00)	Acc@1  57.00 ( 66.80)	Acc@5  85.00 ( 90.56)
 * Acc@1 66.920 Acc@5 90.560
### epoch[52] execution time: 28.467106103897095
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.222 ( 0.222)	Data  0.153 ( 0.153)	Loss 5.7561e-01 (5.7561e-01)	Acc@1  83.59 ( 83.59)	Acc@5  97.66 ( 97.66)
Epoch: [53][ 10/391]	Time  0.067 ( 0.079)	Data  0.001 ( 0.015)	Loss 3.5725e-01 (5.1612e-01)	Acc@1  92.19 ( 84.52)	Acc@5 100.00 ( 97.30)
Epoch: [53][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.008)	Loss 5.8241e-01 (5.2548e-01)	Acc@1  82.03 ( 84.00)	Acc@5  97.66 ( 97.36)
Epoch: [53][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.7271e-01 (5.2362e-01)	Acc@1  84.38 ( 84.10)	Acc@5  98.44 ( 97.51)
Epoch: [53][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 6.0920e-01 (5.3109e-01)	Acc@1  85.16 ( 83.99)	Acc@5  96.88 ( 97.60)
Epoch: [53][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.4650e-01 (5.1832e-01)	Acc@1  82.03 ( 83.99)	Acc@5  98.44 ( 97.86)
Epoch: [53][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.5620e-01 (5.1934e-01)	Acc@1  83.59 ( 84.04)	Acc@5  98.44 ( 98.04)
Epoch: [53][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.6804e-01 (5.1875e-01)	Acc@1  80.47 ( 84.14)	Acc@5  98.44 ( 98.00)
Epoch: [53][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.2698e-01 (5.1895e-01)	Acc@1  85.16 ( 84.01)	Acc@5  97.66 ( 98.06)
Epoch: [53][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.1517e-01 (5.2446e-01)	Acc@1  85.94 ( 83.89)	Acc@5  97.66 ( 98.03)
Epoch: [53][100/391]	Time  0.071 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.1820e-01 (5.2908e-01)	Acc@1  82.03 ( 83.72)	Acc@5  96.09 ( 98.03)
Epoch: [53][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 5.0541e-01 (5.2978e-01)	Acc@1  84.38 ( 83.64)	Acc@5  98.44 ( 98.01)
Epoch: [53][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7286e-01 (5.3143e-01)	Acc@1  86.72 ( 83.59)	Acc@5  98.44 ( 97.97)
Epoch: [53][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4096e-01 (5.3346e-01)	Acc@1  83.59 ( 83.51)	Acc@5  96.09 ( 97.95)
Epoch: [53][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4855e-01 (5.3793e-01)	Acc@1  81.25 ( 83.38)	Acc@5  97.66 ( 97.92)
Epoch: [53][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0283e-01 (5.3726e-01)	Acc@1  85.94 ( 83.40)	Acc@5  97.66 ( 97.92)
Epoch: [53][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1644e-01 (5.3768e-01)	Acc@1  82.81 ( 83.30)	Acc@5  97.66 ( 97.91)
Epoch: [53][170/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7384e-01 (5.3781e-01)	Acc@1  84.38 ( 83.27)	Acc@5  99.22 ( 97.92)
Epoch: [53][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7359e-01 (5.3729e-01)	Acc@1  86.72 ( 83.27)	Acc@5  97.66 ( 97.95)
Epoch: [53][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6998e-01 (5.3977e-01)	Acc@1  82.03 ( 83.12)	Acc@5  99.22 ( 97.97)
Epoch: [53][200/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2945e-01 (5.3892e-01)	Acc@1  85.94 ( 83.18)	Acc@5  98.44 ( 97.94)
Epoch: [53][210/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9472e-01 (5.3889e-01)	Acc@1  80.47 ( 83.22)	Acc@5  99.22 ( 97.94)
Epoch: [53][220/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7199e-01 (5.3882e-01)	Acc@1  78.91 ( 83.16)	Acc@5  97.66 ( 97.92)
Epoch: [53][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5628e-01 (5.4038e-01)	Acc@1  84.38 ( 83.14)	Acc@5  97.66 ( 97.89)
Epoch: [53][240/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4370e-01 (5.4053e-01)	Acc@1  81.25 ( 83.15)	Acc@5  97.66 ( 97.85)
Epoch: [53][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5962e-01 (5.4034e-01)	Acc@1  85.16 ( 83.17)	Acc@5  98.44 ( 97.86)
Epoch: [53][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6676e-01 (5.4107e-01)	Acc@1  85.94 ( 83.15)	Acc@5  97.66 ( 97.85)
Epoch: [53][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7895e-01 (5.4249e-01)	Acc@1  80.47 ( 83.14)	Acc@5  96.09 ( 97.86)
Epoch: [53][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.3384e-01 (5.4439e-01)	Acc@1  75.00 ( 83.09)	Acc@5  96.09 ( 97.82)
Epoch: [53][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6670e-01 (5.4654e-01)	Acc@1  82.03 ( 83.06)	Acc@5  97.66 ( 97.82)
Epoch: [53][300/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8777e-01 (5.4726e-01)	Acc@1  84.38 ( 83.01)	Acc@5  98.44 ( 97.82)
Epoch: [53][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6256e-01 (5.4806e-01)	Acc@1  87.50 ( 83.02)	Acc@5 100.00 ( 97.80)
Epoch: [53][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7246e-01 (5.4842e-01)	Acc@1  82.81 ( 83.03)	Acc@5  96.88 ( 97.81)
Epoch: [53][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.7533e-01 (5.4786e-01)	Acc@1  81.25 ( 83.03)	Acc@5  98.44 ( 97.82)
Epoch: [53][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6585e-01 (5.4767e-01)	Acc@1  86.72 ( 83.03)	Acc@5 100.00 ( 97.81)
Epoch: [53][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.6658e-01 (5.4909e-01)	Acc@1  73.44 ( 82.96)	Acc@5  96.09 ( 97.80)
Epoch: [53][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4775e-01 (5.5131e-01)	Acc@1  79.69 ( 82.88)	Acc@5  97.66 ( 97.77)
Epoch: [53][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4343e-01 (5.5136e-01)	Acc@1  85.94 ( 82.91)	Acc@5  96.09 ( 97.77)
Epoch: [53][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4229e-01 (5.5189e-01)	Acc@1  87.50 ( 82.86)	Acc@5  96.09 ( 97.76)
Epoch: [53][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.9543e-01 (5.5315e-01)	Acc@1  76.25 ( 82.79)	Acc@5  93.75 ( 97.74)
## e[53] optimizer.zero_grad (sum) time: 0.4121732711791992
## e[53]       loss.backward (sum) time: 6.985876560211182
## e[53]      optimizer.step (sum) time: 3.4278106689453125
## epoch[53] training(only) time: 25.57025933265686
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3805e+00 (1.3805e+00)	Acc@1  70.00 ( 70.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.030 ( 0.039)	Loss 1.3345e+00 (1.2592e+00)	Acc@1  68.00 ( 67.82)	Acc@5  90.00 ( 89.64)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.2054e+00 (1.2218e+00)	Acc@1  68.00 ( 68.38)	Acc@5  91.00 ( 90.29)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.5512e+00 (1.2597e+00)	Acc@1  64.00 ( 67.26)	Acc@5  88.00 ( 90.06)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1607e+00 (1.2400e+00)	Acc@1  71.00 ( 67.24)	Acc@5  92.00 ( 90.63)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2420e+00 (1.2552e+00)	Acc@1  67.00 ( 66.80)	Acc@5  89.00 ( 90.43)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.3462e+00 (1.2513e+00)	Acc@1  59.00 ( 66.72)	Acc@5  88.00 ( 90.48)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.1182e+00 (1.2613e+00)	Acc@1  67.00 ( 66.82)	Acc@5  94.00 ( 90.56)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3743e+00 (1.2729e+00)	Acc@1  63.00 ( 66.57)	Acc@5  91.00 ( 90.37)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8738e+00 (1.2661e+00)	Acc@1  56.00 ( 66.64)	Acc@5  86.00 ( 90.44)
 * Acc@1 66.730 Acc@5 90.530
### epoch[53] execution time: 28.413253784179688
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.215 ( 0.215)	Data  0.147 ( 0.147)	Loss 6.1865e-01 (6.1865e-01)	Acc@1  77.34 ( 77.34)	Acc@5  98.44 ( 98.44)
Epoch: [54][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.014)	Loss 5.1688e-01 (5.3436e-01)	Acc@1  84.38 ( 83.59)	Acc@5  98.44 ( 97.94)
Epoch: [54][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.008)	Loss 5.4065e-01 (5.1290e-01)	Acc@1  83.59 ( 84.30)	Acc@5  98.44 ( 98.33)
Epoch: [54][ 30/391]	Time  0.064 ( 0.071)	Data  0.001 ( 0.006)	Loss 6.3946e-01 (5.1629e-01)	Acc@1  76.56 ( 83.82)	Acc@5  96.88 ( 98.26)
Epoch: [54][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 6.5408e-01 (5.2863e-01)	Acc@1  80.47 ( 83.42)	Acc@5 100.00 ( 98.29)
Epoch: [54][ 50/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.004)	Loss 6.1251e-01 (5.2641e-01)	Acc@1  77.34 ( 83.29)	Acc@5  98.44 ( 98.28)
Epoch: [54][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.6116e-01 (5.1589e-01)	Acc@1  87.50 ( 83.82)	Acc@5  97.66 ( 98.36)
Epoch: [54][ 70/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 5.7288e-01 (5.2040e-01)	Acc@1  81.25 ( 83.59)	Acc@5  99.22 ( 98.33)
Epoch: [54][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.3145e-01 (5.1648e-01)	Acc@1  86.72 ( 83.85)	Acc@5 100.00 ( 98.34)
Epoch: [54][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.5316e-01 (5.1787e-01)	Acc@1  81.25 ( 83.80)	Acc@5 100.00 ( 98.32)
Epoch: [54][100/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.5851e-01 (5.1943e-01)	Acc@1  83.59 ( 83.66)	Acc@5  96.88 ( 98.30)
Epoch: [54][110/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.6331e-01 (5.2045e-01)	Acc@1  82.81 ( 83.63)	Acc@5  97.66 ( 98.29)
Epoch: [54][120/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0226e-01 (5.2382e-01)	Acc@1  82.03 ( 83.54)	Acc@5  97.66 ( 98.26)
Epoch: [54][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0688e-01 (5.2319e-01)	Acc@1  85.16 ( 83.59)	Acc@5  97.66 ( 98.23)
Epoch: [54][140/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9774e-01 (5.2528e-01)	Acc@1  84.38 ( 83.54)	Acc@5  97.66 ( 98.22)
Epoch: [54][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3116e-01 (5.2669e-01)	Acc@1  85.16 ( 83.50)	Acc@5  96.88 ( 98.22)
Epoch: [54][160/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4593e-01 (5.2470e-01)	Acc@1  85.16 ( 83.55)	Acc@5  96.09 ( 98.22)
Epoch: [54][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5019e-01 (5.2570e-01)	Acc@1  82.81 ( 83.52)	Acc@5  96.09 ( 98.20)
Epoch: [54][180/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3432e-01 (5.2806e-01)	Acc@1  85.94 ( 83.37)	Acc@5  97.66 ( 98.16)
Epoch: [54][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6042e-01 (5.2804e-01)	Acc@1  84.38 ( 83.40)	Acc@5  99.22 ( 98.14)
Epoch: [54][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9754e-01 (5.2649e-01)	Acc@1  86.72 ( 83.45)	Acc@5  96.88 ( 98.12)
Epoch: [54][210/391]	Time  0.077 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7021e-01 (5.2662e-01)	Acc@1  82.81 ( 83.47)	Acc@5  98.44 ( 98.12)
Epoch: [54][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4840e-01 (5.2774e-01)	Acc@1  78.12 ( 83.42)	Acc@5  98.44 ( 98.12)
Epoch: [54][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2956e-01 (5.2929e-01)	Acc@1  80.47 ( 83.34)	Acc@5  98.44 ( 98.10)
Epoch: [54][240/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.4395e-01 (5.3064e-01)	Acc@1  75.00 ( 83.30)	Acc@5  97.66 ( 98.10)
Epoch: [54][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1385e-01 (5.3193e-01)	Acc@1  82.81 ( 83.27)	Acc@5  96.88 ( 98.09)
Epoch: [54][260/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8221e-01 (5.3279e-01)	Acc@1  82.81 ( 83.23)	Acc@5  98.44 ( 98.09)
Epoch: [54][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8585e-01 (5.3252e-01)	Acc@1  87.50 ( 83.27)	Acc@5 100.00 ( 98.11)
Epoch: [54][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6595e-01 (5.3463e-01)	Acc@1  78.91 ( 83.22)	Acc@5  95.31 ( 98.08)
Epoch: [54][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.8460e-01 (5.3492e-01)	Acc@1  69.53 ( 83.13)	Acc@5  95.31 ( 98.08)
Epoch: [54][300/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7107e-01 (5.3785e-01)	Acc@1  88.28 ( 83.05)	Acc@5  97.66 ( 98.06)
Epoch: [54][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9135e-01 (5.3939e-01)	Acc@1  82.03 ( 82.99)	Acc@5  96.09 ( 98.03)
Epoch: [54][320/391]	Time  0.072 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1497e-01 (5.4168e-01)	Acc@1  83.59 ( 82.89)	Acc@5  97.66 ( 98.02)
Epoch: [54][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1102e-01 (5.4363e-01)	Acc@1  84.38 ( 82.85)	Acc@5  96.09 ( 97.99)
Epoch: [54][340/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0726e-01 (5.4299e-01)	Acc@1  83.59 ( 82.88)	Acc@5  94.53 ( 97.99)
Epoch: [54][350/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.0921e-01 (5.4356e-01)	Acc@1  75.78 ( 82.89)	Acc@5  95.31 ( 97.97)
Epoch: [54][360/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8191e-01 (5.4396e-01)	Acc@1  82.81 ( 82.85)	Acc@5 100.00 ( 97.99)
Epoch: [54][370/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3322e-01 (5.4485e-01)	Acc@1  80.47 ( 82.81)	Acc@5  96.88 ( 97.98)
Epoch: [54][380/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.5410e-01 (5.4655e-01)	Acc@1  78.91 ( 82.76)	Acc@5  96.09 ( 97.95)
Epoch: [54][390/391]	Time  0.047 ( 0.066)	Data  0.001 ( 0.002)	Loss 8.5762e-01 (5.4639e-01)	Acc@1  75.00 ( 82.78)	Acc@5  95.00 ( 97.96)
## e[54] optimizer.zero_grad (sum) time: 0.4130892753601074
## e[54]       loss.backward (sum) time: 7.011812210083008
## e[54]      optimizer.step (sum) time: 3.5676498413085938
## epoch[54] training(only) time: 25.76372718811035
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.3578e+00 (1.3578e+00)	Acc@1  69.00 ( 69.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.3231e+00 (1.3092e+00)	Acc@1  65.00 ( 66.55)	Acc@5  92.00 ( 88.91)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.2407e+00 (1.2501e+00)	Acc@1  67.00 ( 67.57)	Acc@5  90.00 ( 90.38)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.5846e+00 (1.2724e+00)	Acc@1  62.00 ( 66.77)	Acc@5  88.00 ( 90.35)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.1281e+00 (1.2422e+00)	Acc@1  72.00 ( 66.98)	Acc@5  93.00 ( 91.07)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2490e+00 (1.2625e+00)	Acc@1  68.00 ( 66.51)	Acc@5  89.00 ( 90.76)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3340e+00 (1.2525e+00)	Acc@1  61.00 ( 66.57)	Acc@5  88.00 ( 90.90)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.0143e+00 (1.2562e+00)	Acc@1  75.00 ( 66.73)	Acc@5  94.00 ( 90.83)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.4178e+00 (1.2685e+00)	Acc@1  67.00 ( 66.47)	Acc@5  90.00 ( 90.64)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.8894e+00 (1.2597e+00)	Acc@1  56.00 ( 66.68)	Acc@5  86.00 ( 90.67)
 * Acc@1 66.740 Acc@5 90.630
### epoch[54] execution time: 28.60327386856079
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.219 ( 0.219)	Data  0.148 ( 0.148)	Loss 5.2523e-01 (5.2523e-01)	Acc@1  82.03 ( 82.03)	Acc@5  99.22 ( 99.22)
Epoch: [55][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.015)	Loss 5.0719e-01 (5.2681e-01)	Acc@1  84.38 ( 83.24)	Acc@5  97.66 ( 98.30)
Epoch: [55][ 20/391]	Time  0.061 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.7299e-01 (5.1323e-01)	Acc@1  86.72 ( 83.52)	Acc@5 100.00 ( 98.36)
Epoch: [55][ 30/391]	Time  0.065 ( 0.070)	Data  0.002 ( 0.006)	Loss 5.1527e-01 (5.0303e-01)	Acc@1  84.38 ( 83.85)	Acc@5  97.66 ( 98.26)
Epoch: [55][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 5.3288e-01 (5.1136e-01)	Acc@1  79.69 ( 83.38)	Acc@5  98.44 ( 98.25)
Epoch: [55][ 50/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.004)	Loss 7.3008e-01 (5.1477e-01)	Acc@1  76.56 ( 83.53)	Acc@5  94.53 ( 98.19)
Epoch: [55][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.6801e-01 (5.1525e-01)	Acc@1  85.94 ( 83.50)	Acc@5  97.66 ( 98.18)
Epoch: [55][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.7564e-01 (5.1359e-01)	Acc@1  83.59 ( 83.65)	Acc@5  97.66 ( 98.16)
Epoch: [55][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.6774e-01 (5.0764e-01)	Acc@1  85.94 ( 83.88)	Acc@5  99.22 ( 98.22)
Epoch: [55][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.0072e-01 (5.0362e-01)	Acc@1  87.50 ( 84.09)	Acc@5  98.44 ( 98.21)
Epoch: [55][100/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 7.1843e-01 (5.0882e-01)	Acc@1  72.66 ( 83.91)	Acc@5  96.88 ( 98.21)
Epoch: [55][110/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.8154e-01 (5.1112e-01)	Acc@1  82.81 ( 83.91)	Acc@5  98.44 ( 98.21)
Epoch: [55][120/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.3670e-01 (5.0859e-01)	Acc@1  83.59 ( 84.00)	Acc@5  98.44 ( 98.26)
Epoch: [55][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 7.2669e-01 (5.1258e-01)	Acc@1  81.25 ( 83.91)	Acc@5  96.09 ( 98.21)
Epoch: [55][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.8364e-01 (5.1478e-01)	Acc@1  79.69 ( 83.94)	Acc@5  97.66 ( 98.15)
Epoch: [55][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6536e-01 (5.1701e-01)	Acc@1  87.50 ( 83.93)	Acc@5  98.44 ( 98.14)
Epoch: [55][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5009e-01 (5.1721e-01)	Acc@1  88.28 ( 83.94)	Acc@5  99.22 ( 98.16)
Epoch: [55][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8308e-01 (5.1991e-01)	Acc@1  85.16 ( 83.86)	Acc@5  98.44 ( 98.13)
Epoch: [55][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4133e-01 (5.2170e-01)	Acc@1  85.16 ( 83.74)	Acc@5  98.44 ( 98.11)
Epoch: [55][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8001e-01 (5.2094e-01)	Acc@1  86.72 ( 83.76)	Acc@5  99.22 ( 98.12)
Epoch: [55][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6760e-01 (5.2327e-01)	Acc@1  82.03 ( 83.63)	Acc@5  96.88 ( 98.11)
Epoch: [55][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7064e-01 (5.2422e-01)	Acc@1  83.59 ( 83.60)	Acc@5  96.09 ( 98.12)
Epoch: [55][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4853e-01 (5.2262e-01)	Acc@1  85.16 ( 83.70)	Acc@5  96.88 ( 98.11)
Epoch: [55][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2967e-01 (5.2365e-01)	Acc@1  81.25 ( 83.66)	Acc@5  97.66 ( 98.07)
Epoch: [55][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9142e-01 (5.2503e-01)	Acc@1  81.25 ( 83.63)	Acc@5  96.88 ( 98.07)
Epoch: [55][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5865e-01 (5.2488e-01)	Acc@1  83.59 ( 83.66)	Acc@5  96.88 ( 98.06)
Epoch: [55][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3618e-01 (5.2493e-01)	Acc@1  82.81 ( 83.69)	Acc@5  97.66 ( 98.04)
Epoch: [55][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9696e-01 (5.2714e-01)	Acc@1  81.25 ( 83.62)	Acc@5  97.66 ( 98.02)
Epoch: [55][280/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5822e-01 (5.2930e-01)	Acc@1  79.69 ( 83.56)	Acc@5  98.44 ( 98.01)
Epoch: [55][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5984e-01 (5.3018e-01)	Acc@1  82.81 ( 83.49)	Acc@5  96.88 ( 97.99)
Epoch: [55][300/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4518e-01 (5.2974e-01)	Acc@1  82.03 ( 83.47)	Acc@5  98.44 ( 98.00)
Epoch: [55][310/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7900e-01 (5.2891e-01)	Acc@1  85.94 ( 83.46)	Acc@5 100.00 ( 98.00)
Epoch: [55][320/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1829e-01 (5.3027e-01)	Acc@1  78.91 ( 83.41)	Acc@5  97.66 ( 97.99)
Epoch: [55][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3659e-01 (5.3084e-01)	Acc@1  85.16 ( 83.40)	Acc@5  97.66 ( 97.99)
Epoch: [55][340/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9992e-01 (5.3235e-01)	Acc@1  81.25 ( 83.33)	Acc@5  96.88 ( 97.99)
Epoch: [55][350/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5737e-01 (5.3280e-01)	Acc@1  82.81 ( 83.34)	Acc@5  99.22 ( 97.99)
Epoch: [55][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6783e-01 (5.3310e-01)	Acc@1  82.81 ( 83.35)	Acc@5  97.66 ( 97.99)
Epoch: [55][370/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5493e-01 (5.3445e-01)	Acc@1  87.50 ( 83.28)	Acc@5  99.22 ( 97.99)
Epoch: [55][380/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7271e-01 (5.3583e-01)	Acc@1  85.94 ( 83.24)	Acc@5  98.44 ( 97.97)
Epoch: [55][390/391]	Time  0.051 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.0403e-01 (5.3651e-01)	Acc@1  72.50 ( 83.21)	Acc@5  98.75 ( 97.97)
## e[55] optimizer.zero_grad (sum) time: 0.4129345417022705
## e[55]       loss.backward (sum) time: 7.025153160095215
## e[55]      optimizer.step (sum) time: 3.4864718914031982
## epoch[55] training(only) time: 25.653343677520752
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.4643e+00 (1.4643e+00)	Acc@1  66.00 ( 66.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.2445e+00 (1.3148e+00)	Acc@1  67.00 ( 65.91)	Acc@5  93.00 ( 89.27)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.2744e+00 (1.2479e+00)	Acc@1  68.00 ( 67.00)	Acc@5  91.00 ( 90.62)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6658e+00 (1.2801e+00)	Acc@1  57.00 ( 66.42)	Acc@5  87.00 ( 90.19)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1002e+00 (1.2549e+00)	Acc@1  69.00 ( 66.20)	Acc@5  95.00 ( 90.88)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.3074e+00 (1.2787e+00)	Acc@1  71.00 ( 65.78)	Acc@5  89.00 ( 90.65)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.1419e+00 (1.2637e+00)	Acc@1  66.00 ( 66.08)	Acc@5  88.00 ( 90.62)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.0375e+00 (1.2679e+00)	Acc@1  68.00 ( 66.31)	Acc@5  94.00 ( 90.65)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4276e+00 (1.2787e+00)	Acc@1  66.00 ( 66.12)	Acc@5  89.00 ( 90.52)
Test: [ 90/100]	Time  0.030 ( 0.028)	Loss 1.7994e+00 (1.2677e+00)	Acc@1  58.00 ( 66.40)	Acc@5  88.00 ( 90.57)
 * Acc@1 66.600 Acc@5 90.570
### epoch[55] execution time: 28.500872135162354
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.219 ( 0.219)	Data  0.148 ( 0.148)	Loss 4.2272e-01 (4.2272e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Epoch: [56][ 10/391]	Time  0.063 ( 0.079)	Data  0.001 ( 0.014)	Loss 5.2448e-01 (5.1719e-01)	Acc@1  84.38 ( 84.52)	Acc@5  96.88 ( 97.73)
Epoch: [56][ 20/391]	Time  0.062 ( 0.072)	Data  0.001 ( 0.008)	Loss 4.7886e-01 (5.0881e-01)	Acc@1  85.94 ( 84.19)	Acc@5  97.66 ( 97.95)
Epoch: [56][ 30/391]	Time  0.070 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.2981e-01 (5.0700e-01)	Acc@1  86.72 ( 84.32)	Acc@5  97.66 ( 98.06)
Epoch: [56][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 6.9080e-01 (5.1383e-01)	Acc@1  76.56 ( 84.07)	Acc@5  97.66 ( 97.96)
Epoch: [56][ 50/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.5995e-01 (5.1449e-01)	Acc@1  83.59 ( 84.21)	Acc@5  97.66 ( 97.95)
Epoch: [56][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.0568e-01 (5.1796e-01)	Acc@1  82.03 ( 84.14)	Acc@5  95.31 ( 97.93)
Epoch: [56][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.3176e-01 (5.1300e-01)	Acc@1  85.94 ( 84.28)	Acc@5  99.22 ( 97.96)
Epoch: [56][ 80/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 6.5697e-01 (5.1531e-01)	Acc@1  78.91 ( 84.31)	Acc@5  96.88 ( 97.93)
Epoch: [56][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.3542e-01 (5.1222e-01)	Acc@1  90.62 ( 84.33)	Acc@5  99.22 ( 97.97)
Epoch: [56][100/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.7658e-01 (5.1054e-01)	Acc@1  85.94 ( 84.37)	Acc@5 100.00 ( 98.04)
Epoch: [56][110/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.0712e-01 (5.0916e-01)	Acc@1  85.94 ( 84.39)	Acc@5  97.66 ( 98.07)
Epoch: [56][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6741e-01 (5.1308e-01)	Acc@1  78.91 ( 84.28)	Acc@5  99.22 ( 98.05)
Epoch: [56][130/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9166e-01 (5.1150e-01)	Acc@1  89.84 ( 84.31)	Acc@5  99.22 ( 98.06)
Epoch: [56][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2450e-01 (5.0924e-01)	Acc@1  85.94 ( 84.41)	Acc@5  96.88 ( 98.06)
Epoch: [56][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0942e-01 (5.0851e-01)	Acc@1  88.28 ( 84.38)	Acc@5  98.44 ( 98.08)
Epoch: [56][160/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4223e-01 (5.0730e-01)	Acc@1  88.28 ( 84.42)	Acc@5  99.22 ( 98.11)
Epoch: [56][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7375e-01 (5.0682e-01)	Acc@1  82.03 ( 84.32)	Acc@5  99.22 ( 98.15)
Epoch: [56][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2538e-01 (5.0868e-01)	Acc@1  87.50 ( 84.28)	Acc@5  97.66 ( 98.14)
Epoch: [56][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0226e-01 (5.0950e-01)	Acc@1  80.47 ( 84.23)	Acc@5  97.66 ( 98.14)
Epoch: [56][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3630e-01 (5.0947e-01)	Acc@1  81.25 ( 84.24)	Acc@5  97.66 ( 98.14)
Epoch: [56][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7396e-01 (5.1083e-01)	Acc@1  82.03 ( 84.22)	Acc@5  97.66 ( 98.12)
Epoch: [56][220/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4399e-01 (5.1372e-01)	Acc@1  84.38 ( 84.16)	Acc@5  99.22 ( 98.09)
Epoch: [56][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5439e-01 (5.1614e-01)	Acc@1  86.72 ( 84.10)	Acc@5  98.44 ( 98.07)
Epoch: [56][240/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4820e-01 (5.1767e-01)	Acc@1  82.03 ( 84.07)	Acc@5  96.88 ( 98.07)
Epoch: [56][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2363e-01 (5.1816e-01)	Acc@1  83.59 ( 84.05)	Acc@5 100.00 ( 98.06)
Epoch: [56][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6673e-01 (5.1669e-01)	Acc@1  83.59 ( 84.08)	Acc@5  99.22 ( 98.06)
Epoch: [56][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2122e-01 (5.1778e-01)	Acc@1  85.16 ( 84.05)	Acc@5  96.09 ( 98.05)
Epoch: [56][280/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1343e-01 (5.1915e-01)	Acc@1  85.16 ( 83.99)	Acc@5  98.44 ( 98.06)
Epoch: [56][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2058e-01 (5.2014e-01)	Acc@1  80.47 ( 83.90)	Acc@5  97.66 ( 98.06)
Epoch: [56][300/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0583e-01 (5.2165e-01)	Acc@1  79.69 ( 83.84)	Acc@5  96.09 ( 98.05)
Epoch: [56][310/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0909e-01 (5.2268e-01)	Acc@1  82.03 ( 83.82)	Acc@5  96.88 ( 98.05)
Epoch: [56][320/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.7637e-01 (5.2309e-01)	Acc@1  78.91 ( 83.82)	Acc@5  97.66 ( 98.05)
Epoch: [56][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9307e-01 (5.2484e-01)	Acc@1  82.03 ( 83.78)	Acc@5  96.88 ( 98.05)
Epoch: [56][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2200e-01 (5.2695e-01)	Acc@1  82.81 ( 83.69)	Acc@5  98.44 ( 98.05)
Epoch: [56][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6160e-01 (5.2637e-01)	Acc@1  82.03 ( 83.70)	Acc@5  98.44 ( 98.05)
Epoch: [56][360/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8798e-01 (5.2746e-01)	Acc@1  82.81 ( 83.65)	Acc@5  96.88 ( 98.04)
Epoch: [56][370/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4331e-01 (5.2872e-01)	Acc@1  84.38 ( 83.58)	Acc@5  97.66 ( 98.03)
Epoch: [56][380/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3043e-01 (5.2852e-01)	Acc@1  79.69 ( 83.56)	Acc@5  96.09 ( 98.03)
Epoch: [56][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 8.3681e-01 (5.2957e-01)	Acc@1  73.75 ( 83.55)	Acc@5  95.00 ( 98.02)
## e[56] optimizer.zero_grad (sum) time: 0.4147183895111084
## e[56]       loss.backward (sum) time: 7.010203838348389
## e[56]      optimizer.step (sum) time: 3.4706227779388428
## epoch[56] training(only) time: 25.64822506904602
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.3951e+00 (1.3951e+00)	Acc@1  71.00 ( 71.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.5080e+00 (1.3599e+00)	Acc@1  63.00 ( 66.09)	Acc@5  91.00 ( 88.91)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.4020e+00 (1.2822e+00)	Acc@1  67.00 ( 66.24)	Acc@5  91.00 ( 90.62)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6164e+00 (1.3013e+00)	Acc@1  62.00 ( 66.03)	Acc@5  86.00 ( 90.32)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.1223e+00 (1.2741e+00)	Acc@1  71.00 ( 66.10)	Acc@5  94.00 ( 91.05)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.2583e+00 (1.2939e+00)	Acc@1  70.00 ( 66.00)	Acc@5  89.00 ( 90.71)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3597e+00 (1.2827e+00)	Acc@1  60.00 ( 65.97)	Acc@5  91.00 ( 90.90)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.2766e+00 (1.2889e+00)	Acc@1  64.00 ( 66.04)	Acc@5  94.00 ( 90.87)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4236e+00 (1.2989e+00)	Acc@1  65.00 ( 65.86)	Acc@5  91.00 ( 90.62)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9342e+00 (1.2891e+00)	Acc@1  59.00 ( 66.11)	Acc@5  85.00 ( 90.70)
 * Acc@1 66.360 Acc@5 90.690
### epoch[56] execution time: 28.507110595703125
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.216 ( 0.216)	Data  0.144 ( 0.144)	Loss 5.2222e-01 (5.2222e-01)	Acc@1  82.81 ( 82.81)	Acc@5  98.44 ( 98.44)
Epoch: [57][ 10/391]	Time  0.065 ( 0.078)	Data  0.001 ( 0.014)	Loss 5.0650e-01 (4.8126e-01)	Acc@1  86.72 ( 84.66)	Acc@5  97.66 ( 98.30)
Epoch: [57][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.008)	Loss 3.9490e-01 (4.8330e-01)	Acc@1  89.06 ( 84.78)	Acc@5  99.22 ( 98.33)
Epoch: [57][ 30/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.1749e-01 (4.8467e-01)	Acc@1  82.03 ( 84.95)	Acc@5 100.00 ( 98.24)
Epoch: [57][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.6858e-01 (4.8393e-01)	Acc@1  85.94 ( 84.81)	Acc@5  98.44 ( 98.17)
Epoch: [57][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.1117e-01 (4.9035e-01)	Acc@1  84.38 ( 84.59)	Acc@5  97.66 ( 98.10)
Epoch: [57][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.2853e-01 (4.8542e-01)	Acc@1  91.41 ( 84.71)	Acc@5 100.00 ( 98.19)
Epoch: [57][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.7701e-01 (4.8986e-01)	Acc@1  77.34 ( 84.58)	Acc@5  99.22 ( 98.22)
Epoch: [57][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.8953e-01 (4.9612e-01)	Acc@1  82.81 ( 84.38)	Acc@5  95.31 ( 98.17)
Epoch: [57][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.3909e-01 (4.9581e-01)	Acc@1  78.12 ( 84.30)	Acc@5  98.44 ( 98.21)
Epoch: [57][100/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.6059e-01 (4.9825e-01)	Acc@1  82.81 ( 84.30)	Acc@5  96.88 ( 98.21)
Epoch: [57][110/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 5.9105e-01 (5.0107e-01)	Acc@1  82.81 ( 84.18)	Acc@5  98.44 ( 98.24)
Epoch: [57][120/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7355e-01 (4.9962e-01)	Acc@1  82.81 ( 84.16)	Acc@5  95.31 ( 98.26)
Epoch: [57][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6977e-01 (5.0322e-01)	Acc@1  81.25 ( 84.09)	Acc@5  97.66 ( 98.22)
Epoch: [57][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4170e-01 (5.0371e-01)	Acc@1  85.94 ( 84.06)	Acc@5 100.00 ( 98.25)
Epoch: [57][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2646e-01 (5.0134e-01)	Acc@1  86.72 ( 84.23)	Acc@5  99.22 ( 98.25)
Epoch: [57][160/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3936e-01 (5.0016e-01)	Acc@1  87.50 ( 84.32)	Acc@5  99.22 ( 98.26)
Epoch: [57][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4956e-01 (5.0093e-01)	Acc@1  85.16 ( 84.27)	Acc@5  98.44 ( 98.27)
Epoch: [57][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5765e-01 (5.0193e-01)	Acc@1  82.81 ( 84.20)	Acc@5  96.88 ( 98.28)
Epoch: [57][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3320e-01 (5.0221e-01)	Acc@1  85.16 ( 84.20)	Acc@5  99.22 ( 98.25)
Epoch: [57][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4652e-01 (5.0173e-01)	Acc@1  85.16 ( 84.26)	Acc@5  97.66 ( 98.25)
Epoch: [57][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8786e-01 (5.0312e-01)	Acc@1  83.59 ( 84.25)	Acc@5  98.44 ( 98.23)
Epoch: [57][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7586e-01 (5.0367e-01)	Acc@1  85.16 ( 84.23)	Acc@5  99.22 ( 98.24)
Epoch: [57][230/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5407e-01 (5.0678e-01)	Acc@1  80.47 ( 84.10)	Acc@5  98.44 ( 98.21)
Epoch: [57][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8766e-01 (5.0721e-01)	Acc@1  88.28 ( 84.11)	Acc@5  97.66 ( 98.21)
Epoch: [57][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2366e-01 (5.0530e-01)	Acc@1  82.81 ( 84.14)	Acc@5 100.00 ( 98.24)
Epoch: [57][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1519e-01 (5.0578e-01)	Acc@1  82.03 ( 84.11)	Acc@5  98.44 ( 98.25)
Epoch: [57][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4859e-01 (5.0801e-01)	Acc@1  82.03 ( 84.03)	Acc@5  98.44 ( 98.24)
Epoch: [57][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2799e-01 (5.0824e-01)	Acc@1  80.47 ( 84.03)	Acc@5  99.22 ( 98.23)
Epoch: [57][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2414e-01 (5.1057e-01)	Acc@1  85.94 ( 83.96)	Acc@5  98.44 ( 98.20)
Epoch: [57][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6265e-01 (5.0881e-01)	Acc@1  82.81 ( 83.99)	Acc@5  99.22 ( 98.23)
Epoch: [57][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5978e-01 (5.0961e-01)	Acc@1  89.06 ( 83.98)	Acc@5  97.66 ( 98.23)
Epoch: [57][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8203e-01 (5.0952e-01)	Acc@1  83.59 ( 83.99)	Acc@5  99.22 ( 98.23)
Epoch: [57][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5233e-01 (5.0983e-01)	Acc@1  85.94 ( 83.99)	Acc@5  99.22 ( 98.23)
Epoch: [57][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.2102e-01 (5.1308e-01)	Acc@1  78.91 ( 83.86)	Acc@5  96.88 ( 98.20)
Epoch: [57][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8858e-01 (5.1428e-01)	Acc@1  81.25 ( 83.78)	Acc@5  98.44 ( 98.20)
Epoch: [57][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6940e-01 (5.1472e-01)	Acc@1  85.16 ( 83.73)	Acc@5  98.44 ( 98.21)
Epoch: [57][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7969e-01 (5.1352e-01)	Acc@1  85.16 ( 83.77)	Acc@5  98.44 ( 98.21)
Epoch: [57][380/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 7.2015e-01 (5.1512e-01)	Acc@1  79.69 ( 83.75)	Acc@5  96.88 ( 98.19)
Epoch: [57][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.8850e-01 (5.1660e-01)	Acc@1  82.50 ( 83.68)	Acc@5  96.25 ( 98.18)
## e[57] optimizer.zero_grad (sum) time: 0.41153407096862793
## e[57]       loss.backward (sum) time: 6.994509935379028
## e[57]      optimizer.step (sum) time: 3.475939989089966
## epoch[57] training(only) time: 25.605685472488403
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3495e+00 (1.3495e+00)	Acc@1  71.00 ( 71.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.4228e+00 (1.3492e+00)	Acc@1  67.00 ( 67.45)	Acc@5  90.00 ( 89.73)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3188e+00 (1.2832e+00)	Acc@1  68.00 ( 67.71)	Acc@5  92.00 ( 90.19)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 1.4910e+00 (1.3049e+00)	Acc@1  65.00 ( 67.06)	Acc@5  88.00 ( 90.03)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1584e+00 (1.2863e+00)	Acc@1  69.00 ( 66.68)	Acc@5  92.00 ( 90.51)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.3254e+00 (1.3063e+00)	Acc@1  72.00 ( 66.41)	Acc@5  87.00 ( 90.35)
Test: [ 60/100]	Time  0.029 ( 0.029)	Loss 1.3478e+00 (1.2977e+00)	Acc@1  65.00 ( 66.51)	Acc@5  89.00 ( 90.39)
Test: [ 70/100]	Time  0.028 ( 0.029)	Loss 1.2737e+00 (1.3067e+00)	Acc@1  67.00 ( 66.51)	Acc@5  92.00 ( 90.28)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.5395e+00 (1.3177e+00)	Acc@1  62.00 ( 66.36)	Acc@5  90.00 ( 90.10)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8983e+00 (1.3089e+00)	Acc@1  59.00 ( 66.56)	Acc@5  85.00 ( 90.24)
 * Acc@1 66.580 Acc@5 90.310
### epoch[57] execution time: 28.47228169441223
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.217 ( 0.217)	Data  0.146 ( 0.146)	Loss 4.6371e-01 (4.6371e-01)	Acc@1  88.28 ( 88.28)	Acc@5  96.88 ( 96.88)
Epoch: [58][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 4.5699e-01 (5.1710e-01)	Acc@1  85.94 ( 83.59)	Acc@5  98.44 ( 98.22)
Epoch: [58][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.008)	Loss 4.8038e-01 (4.8739e-01)	Acc@1  82.81 ( 84.56)	Acc@5  98.44 ( 98.29)
Epoch: [58][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.6657e-01 (4.8064e-01)	Acc@1  85.94 ( 85.03)	Acc@5  96.09 ( 98.26)
Epoch: [58][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.5902e-01 (4.8658e-01)	Acc@1  88.28 ( 85.02)	Acc@5  97.66 ( 98.15)
Epoch: [58][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.1702e-01 (4.8416e-01)	Acc@1  82.81 ( 85.09)	Acc@5  96.88 ( 98.21)
Epoch: [58][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.4098e-01 (4.8031e-01)	Acc@1  85.94 ( 85.12)	Acc@5  99.22 ( 98.30)
Epoch: [58][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.6222e-01 (4.8270e-01)	Acc@1  85.16 ( 85.12)	Acc@5  98.44 ( 98.28)
Epoch: [58][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.7969e-01 (4.8139e-01)	Acc@1  86.72 ( 85.08)	Acc@5  99.22 ( 98.34)
Epoch: [58][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.8479e-01 (4.8125e-01)	Acc@1  82.03 ( 84.99)	Acc@5  96.88 ( 98.36)
Epoch: [58][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.1922e-01 (4.8639e-01)	Acc@1  85.94 ( 84.82)	Acc@5  97.66 ( 98.34)
Epoch: [58][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1003e-01 (4.9148e-01)	Acc@1  86.72 ( 84.59)	Acc@5  99.22 ( 98.30)
Epoch: [58][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7906e-01 (4.9364e-01)	Acc@1  89.06 ( 84.48)	Acc@5 100.00 ( 98.31)
Epoch: [58][130/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.9155e-01 (4.9592e-01)	Acc@1  82.03 ( 84.37)	Acc@5  96.09 ( 98.27)
Epoch: [58][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4536e-01 (4.9751e-01)	Acc@1  84.38 ( 84.25)	Acc@5 100.00 ( 98.28)
Epoch: [58][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8308e-01 (4.9810e-01)	Acc@1  83.59 ( 84.27)	Acc@5  98.44 ( 98.25)
Epoch: [58][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6588e-01 (4.9524e-01)	Acc@1  88.28 ( 84.35)	Acc@5 100.00 ( 98.30)
Epoch: [58][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9460e-01 (4.9643e-01)	Acc@1  82.81 ( 84.28)	Acc@5  97.66 ( 98.31)
Epoch: [58][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4612e-01 (4.9537e-01)	Acc@1  83.59 ( 84.30)	Acc@5  97.66 ( 98.32)
Epoch: [58][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2973e-01 (4.9226e-01)	Acc@1  85.16 ( 84.37)	Acc@5 100.00 ( 98.35)
Epoch: [58][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1075e-01 (4.9255e-01)	Acc@1  87.50 ( 84.42)	Acc@5  99.22 ( 98.36)
Epoch: [58][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1266e-01 (4.9311e-01)	Acc@1  85.16 ( 84.46)	Acc@5  98.44 ( 98.37)
Epoch: [58][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4473e-01 (4.9448e-01)	Acc@1  81.25 ( 84.47)	Acc@5  96.88 ( 98.34)
Epoch: [58][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4062e-01 (4.9605e-01)	Acc@1  83.59 ( 84.44)	Acc@5  94.53 ( 98.30)
Epoch: [58][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2929e-01 (4.9631e-01)	Acc@1  85.16 ( 84.46)	Acc@5  98.44 ( 98.29)
Epoch: [58][250/391]	Time  0.064 ( 0.066)	Data  0.002 ( 0.002)	Loss 5.4135e-01 (4.9687e-01)	Acc@1  82.81 ( 84.41)	Acc@5  98.44 ( 98.31)
Epoch: [58][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.1321e-01 (5.0051e-01)	Acc@1  79.69 ( 84.31)	Acc@5  98.44 ( 98.29)
Epoch: [58][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3896e-01 (5.0165e-01)	Acc@1  86.72 ( 84.26)	Acc@5  99.22 ( 98.29)
Epoch: [58][280/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0115e-01 (5.0345e-01)	Acc@1  79.69 ( 84.19)	Acc@5  98.44 ( 98.27)
Epoch: [58][290/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3562e-01 (5.0414e-01)	Acc@1  78.91 ( 84.16)	Acc@5  97.66 ( 98.27)
Epoch: [58][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.4235e-01 (5.0569e-01)	Acc@1  80.47 ( 84.10)	Acc@5  97.66 ( 98.25)
Epoch: [58][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8030e-01 (5.0605e-01)	Acc@1  85.94 ( 84.08)	Acc@5  97.66 ( 98.25)
Epoch: [58][320/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2764e-01 (5.0640e-01)	Acc@1  85.94 ( 84.08)	Acc@5  96.88 ( 98.25)
Epoch: [58][330/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6479e-01 (5.0758e-01)	Acc@1  85.16 ( 84.07)	Acc@5 100.00 ( 98.25)
Epoch: [58][340/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0031e-01 (5.0840e-01)	Acc@1  83.59 ( 84.06)	Acc@5  99.22 ( 98.23)
Epoch: [58][350/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.6007e-01 (5.0948e-01)	Acc@1  80.47 ( 84.03)	Acc@5  96.88 ( 98.21)
Epoch: [58][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4661e-01 (5.0999e-01)	Acc@1  85.94 ( 84.04)	Acc@5  98.44 ( 98.21)
Epoch: [58][370/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5212e-01 (5.1117e-01)	Acc@1  79.69 ( 83.96)	Acc@5 100.00 ( 98.20)
Epoch: [58][380/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.3079e-01 (5.1157e-01)	Acc@1  84.38 ( 83.95)	Acc@5  96.88 ( 98.20)
Epoch: [58][390/391]	Time  0.057 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7010e-01 (5.1194e-01)	Acc@1  87.50 ( 83.92)	Acc@5 100.00 ( 98.20)
## e[58] optimizer.zero_grad (sum) time: 0.41249632835388184
## e[58]       loss.backward (sum) time: 6.99910569190979
## e[58]      optimizer.step (sum) time: 3.457895278930664
## epoch[58] training(only) time: 25.660115242004395
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.4008e+00 (1.4008e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.026 ( 0.039)	Loss 1.3597e+00 (1.3426e+00)	Acc@1  67.00 ( 66.91)	Acc@5  93.00 ( 89.64)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.2439e+00 (1.2621e+00)	Acc@1  68.00 ( 67.38)	Acc@5  92.00 ( 90.76)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6237e+00 (1.3024e+00)	Acc@1  62.00 ( 66.74)	Acc@5  87.00 ( 90.23)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1715e+00 (1.2810e+00)	Acc@1  64.00 ( 66.05)	Acc@5  93.00 ( 90.88)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2339e+00 (1.2975e+00)	Acc@1  71.00 ( 65.82)	Acc@5  88.00 ( 90.55)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.3641e+00 (1.2894e+00)	Acc@1  57.00 ( 65.98)	Acc@5  89.00 ( 90.48)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.0972e+00 (1.2948e+00)	Acc@1  71.00 ( 66.27)	Acc@5  93.00 ( 90.51)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4033e+00 (1.3009e+00)	Acc@1  64.00 ( 66.17)	Acc@5  93.00 ( 90.42)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.8252e+00 (1.2916e+00)	Acc@1  57.00 ( 66.43)	Acc@5  86.00 ( 90.43)
 * Acc@1 66.500 Acc@5 90.460
### epoch[58] execution time: 28.50479769706726
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.214 ( 0.214)	Data  0.121 ( 0.121)	Loss 4.7098e-01 (4.7098e-01)	Acc@1  87.50 ( 87.50)	Acc@5  96.09 ( 96.09)
Epoch: [59][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.012)	Loss 3.8011e-01 (4.4381e-01)	Acc@1  91.41 ( 85.94)	Acc@5 100.00 ( 98.58)
Epoch: [59][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.007)	Loss 4.2882e-01 (4.4900e-01)	Acc@1  84.38 ( 85.71)	Acc@5  99.22 ( 98.44)
Epoch: [59][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.005)	Loss 4.8584e-01 (4.7116e-01)	Acc@1  85.94 ( 85.08)	Acc@5  96.88 ( 98.11)
Epoch: [59][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 5.1275e-01 (4.6256e-01)	Acc@1  81.25 ( 85.18)	Acc@5  98.44 ( 98.30)
Epoch: [59][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 6.2569e-01 (4.6377e-01)	Acc@1  81.25 ( 85.28)	Acc@5  97.66 ( 98.35)
Epoch: [59][ 60/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.6977e-01 (4.7068e-01)	Acc@1  81.25 ( 85.08)	Acc@5  98.44 ( 98.34)
Epoch: [59][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.5614e-01 (4.7138e-01)	Acc@1  83.59 ( 85.01)	Acc@5  96.88 ( 98.37)
Epoch: [59][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4933e-01 (4.7370e-01)	Acc@1  88.28 ( 84.91)	Acc@5  97.66 ( 98.36)
Epoch: [59][ 90/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5724e-01 (4.7190e-01)	Acc@1  85.94 ( 84.96)	Acc@5  98.44 ( 98.36)
Epoch: [59][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5700e-01 (4.7585e-01)	Acc@1  84.38 ( 84.81)	Acc@5  96.88 ( 98.34)
Epoch: [59][110/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8319e-01 (4.7916e-01)	Acc@1  82.81 ( 84.79)	Acc@5  96.88 ( 98.34)
Epoch: [59][120/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6255e-01 (4.8123e-01)	Acc@1  83.59 ( 84.79)	Acc@5  98.44 ( 98.35)
Epoch: [59][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7110e-01 (4.8075e-01)	Acc@1  81.25 ( 84.90)	Acc@5  98.44 ( 98.32)
Epoch: [59][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4834e-01 (4.8267e-01)	Acc@1  78.91 ( 84.74)	Acc@5  99.22 ( 98.33)
Epoch: [59][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2500e-01 (4.8357e-01)	Acc@1  88.28 ( 84.68)	Acc@5  99.22 ( 98.33)
Epoch: [59][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8930e-01 (4.8492e-01)	Acc@1  81.25 ( 84.60)	Acc@5  99.22 ( 98.33)
Epoch: [59][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7485e-01 (4.8557e-01)	Acc@1  86.72 ( 84.60)	Acc@5  99.22 ( 98.32)
Epoch: [59][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9456e-01 (4.8614e-01)	Acc@1  79.69 ( 84.58)	Acc@5 100.00 ( 98.33)
Epoch: [59][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4459e-01 (4.8916e-01)	Acc@1  80.47 ( 84.47)	Acc@5  97.66 ( 98.29)
Epoch: [59][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1424e-01 (4.8923e-01)	Acc@1  85.94 ( 84.44)	Acc@5  98.44 ( 98.31)
Epoch: [59][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6181e-01 (4.8910e-01)	Acc@1  85.16 ( 84.42)	Acc@5  99.22 ( 98.33)
Epoch: [59][220/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.7295e-01 (4.9087e-01)	Acc@1  81.25 ( 84.41)	Acc@5  97.66 ( 98.30)
Epoch: [59][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3253e-01 (4.8950e-01)	Acc@1  86.72 ( 84.48)	Acc@5  98.44 ( 98.32)
Epoch: [59][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1013e-01 (4.9011e-01)	Acc@1  82.81 ( 84.44)	Acc@5  99.22 ( 98.32)
Epoch: [59][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4917e-01 (4.9179e-01)	Acc@1  87.50 ( 84.38)	Acc@5  99.22 ( 98.31)
Epoch: [59][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.8678e-01 (4.9287e-01)	Acc@1  82.03 ( 84.39)	Acc@5  97.66 ( 98.31)
Epoch: [59][270/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.0836e-01 (4.9446e-01)	Acc@1  79.69 ( 84.33)	Acc@5  96.88 ( 98.31)
Epoch: [59][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5729e-01 (4.9490e-01)	Acc@1  87.50 ( 84.36)	Acc@5  97.66 ( 98.31)
Epoch: [59][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8272e-01 (4.9567e-01)	Acc@1  89.06 ( 84.36)	Acc@5 100.00 ( 98.30)
Epoch: [59][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4544e-01 (4.9638e-01)	Acc@1  82.81 ( 84.35)	Acc@5  97.66 ( 98.29)
Epoch: [59][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.5296e-01 (4.9555e-01)	Acc@1  80.47 ( 84.34)	Acc@5  97.66 ( 98.31)
Epoch: [59][320/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4276e-01 (4.9688e-01)	Acc@1  82.81 ( 84.30)	Acc@5  96.88 ( 98.31)
Epoch: [59][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.4843e-01 (4.9698e-01)	Acc@1  81.25 ( 84.30)	Acc@5  97.66 ( 98.32)
Epoch: [59][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.6503e-01 (4.9806e-01)	Acc@1  82.03 ( 84.27)	Acc@5  96.88 ( 98.31)
Epoch: [59][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7279e-01 (4.9844e-01)	Acc@1  85.94 ( 84.26)	Acc@5  96.09 ( 98.31)
Epoch: [59][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.001)	Loss 5.0539e-01 (5.0008e-01)	Acc@1  88.28 ( 84.20)	Acc@5  97.66 ( 98.29)
Epoch: [59][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.001)	Loss 5.8706e-01 (5.0079e-01)	Acc@1  80.47 ( 84.15)	Acc@5 100.00 ( 98.29)
Epoch: [59][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.001)	Loss 5.2740e-01 (5.0169e-01)	Acc@1  85.16 ( 84.13)	Acc@5  98.44 ( 98.27)
Epoch: [59][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.001)	Loss 5.7177e-01 (5.0271e-01)	Acc@1  82.50 ( 84.11)	Acc@5  98.75 ( 98.26)
## e[59] optimizer.zero_grad (sum) time: 0.40793514251708984
## e[59]       loss.backward (sum) time: 7.007144927978516
## e[59]      optimizer.step (sum) time: 3.4343552589416504
## epoch[59] training(only) time: 25.55860948562622
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.4289e+00 (1.4289e+00)	Acc@1  67.00 ( 67.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.036 ( 0.040)	Loss 1.3932e+00 (1.3299e+00)	Acc@1  65.00 ( 67.45)	Acc@5  91.00 ( 89.64)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 1.3392e+00 (1.2635e+00)	Acc@1  66.00 ( 67.90)	Acc@5  92.00 ( 91.05)
Test: [ 30/100]	Time  0.027 ( 0.032)	Loss 1.5825e+00 (1.2863e+00)	Acc@1  59.00 ( 66.94)	Acc@5  87.00 ( 90.42)
Test: [ 40/100]	Time  0.028 ( 0.030)	Loss 1.1981e+00 (1.2715e+00)	Acc@1  65.00 ( 66.54)	Acc@5  94.00 ( 90.80)
Test: [ 50/100]	Time  0.028 ( 0.030)	Loss 1.3127e+00 (1.2917e+00)	Acc@1  71.00 ( 66.22)	Acc@5  88.00 ( 90.53)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3723e+00 (1.2818e+00)	Acc@1  62.00 ( 66.54)	Acc@5  89.00 ( 90.59)
Test: [ 70/100]	Time  0.028 ( 0.029)	Loss 1.2375e+00 (1.2927e+00)	Acc@1  64.00 ( 66.49)	Acc@5  91.00 ( 90.55)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.5534e+00 (1.3054e+00)	Acc@1  62.00 ( 66.20)	Acc@5  89.00 ( 90.36)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.9344e+00 (1.3003e+00)	Acc@1  57.00 ( 66.34)	Acc@5  84.00 ( 90.37)
 * Acc@1 66.350 Acc@5 90.390
### epoch[59] execution time: 28.426302909851074
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.221 ( 0.221)	Data  0.151 ( 0.151)	Loss 4.8354e-01 (4.8354e-01)	Acc@1  82.81 ( 82.81)	Acc@5  96.88 ( 96.88)
Epoch: [60][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.015)	Loss 4.1266e-01 (4.7364e-01)	Acc@1  87.50 ( 84.59)	Acc@5  99.22 ( 98.44)
Epoch: [60][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.008)	Loss 3.8149e-01 (4.4061e-01)	Acc@1  88.28 ( 85.75)	Acc@5 100.00 ( 98.66)
Epoch: [60][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.5640e-01 (4.5045e-01)	Acc@1  88.28 ( 85.56)	Acc@5  99.22 ( 98.69)
Epoch: [60][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.2981e-01 (4.5050e-01)	Acc@1  85.16 ( 85.80)	Acc@5  99.22 ( 98.63)
Epoch: [60][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.9076e-01 (4.5663e-01)	Acc@1  83.59 ( 85.88)	Acc@5  98.44 ( 98.56)
Epoch: [60][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.5564e-01 (4.5400e-01)	Acc@1  85.16 ( 85.73)	Acc@5  97.66 ( 98.60)
Epoch: [60][ 70/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.2501e-01 (4.4385e-01)	Acc@1  88.28 ( 86.07)	Acc@5 100.00 ( 98.66)
Epoch: [60][ 80/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4675e-01 (4.4525e-01)	Acc@1  87.50 ( 86.01)	Acc@5  97.66 ( 98.62)
Epoch: [60][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.2627e-01 (4.4004e-01)	Acc@1  89.06 ( 86.28)	Acc@5  98.44 ( 98.64)
Epoch: [60][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4585e-01 (4.4357e-01)	Acc@1  87.50 ( 86.16)	Acc@5  99.22 ( 98.59)
Epoch: [60][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.5071e-01 (4.4249e-01)	Acc@1  82.81 ( 86.23)	Acc@5  97.66 ( 98.60)
Epoch: [60][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9086e-01 (4.4173e-01)	Acc@1  87.50 ( 86.30)	Acc@5  99.22 ( 98.61)
Epoch: [60][130/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9737e-01 (4.4085e-01)	Acc@1  87.50 ( 86.38)	Acc@5 100.00 ( 98.62)
Epoch: [60][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3456e-01 (4.4112e-01)	Acc@1  89.06 ( 86.40)	Acc@5  98.44 ( 98.65)
Epoch: [60][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5642e-01 (4.4032e-01)	Acc@1  89.06 ( 86.40)	Acc@5  99.22 ( 98.65)
Epoch: [60][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0818e-01 (4.3829e-01)	Acc@1  82.81 ( 86.51)	Acc@5  99.22 ( 98.68)
Epoch: [60][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0952e-01 (4.3648e-01)	Acc@1  91.41 ( 86.62)	Acc@5  99.22 ( 98.69)
Epoch: [60][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1320e-01 (4.3704e-01)	Acc@1  85.16 ( 86.60)	Acc@5  96.88 ( 98.69)
Epoch: [60][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4528e-01 (4.3539e-01)	Acc@1  87.50 ( 86.69)	Acc@5  99.22 ( 98.70)
Epoch: [60][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2138e-01 (4.3637e-01)	Acc@1  82.81 ( 86.62)	Acc@5  98.44 ( 98.69)
Epoch: [60][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9119e-01 (4.3684e-01)	Acc@1  86.72 ( 86.57)	Acc@5 100.00 ( 98.68)
Epoch: [60][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6865e-01 (4.3615e-01)	Acc@1  89.06 ( 86.58)	Acc@5 100.00 ( 98.69)
Epoch: [60][230/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6938e-01 (4.3614e-01)	Acc@1  87.50 ( 86.57)	Acc@5  99.22 ( 98.68)
Epoch: [60][240/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1646e-01 (4.3529e-01)	Acc@1  88.28 ( 86.61)	Acc@5  97.66 ( 98.67)
Epoch: [60][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9586e-01 (4.3508e-01)	Acc@1  90.62 ( 86.58)	Acc@5 100.00 ( 98.66)
Epoch: [60][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8037e-01 (4.3455e-01)	Acc@1  94.53 ( 86.61)	Acc@5 100.00 ( 98.64)
Epoch: [60][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4502e-01 (4.3361e-01)	Acc@1  85.94 ( 86.60)	Acc@5 100.00 ( 98.66)
Epoch: [60][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9154e-01 (4.3192e-01)	Acc@1  85.16 ( 86.66)	Acc@5  99.22 ( 98.67)
Epoch: [60][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8304e-01 (4.3250e-01)	Acc@1  89.84 ( 86.68)	Acc@5  99.22 ( 98.66)
Epoch: [60][300/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2650e-01 (4.3240e-01)	Acc@1  86.72 ( 86.68)	Acc@5  97.66 ( 98.68)
Epoch: [60][310/391]	Time  0.066 ( 0.066)	Data  0.002 ( 0.002)	Loss 3.0939e-01 (4.3258e-01)	Acc@1  89.84 ( 86.67)	Acc@5  99.22 ( 98.65)
Epoch: [60][320/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7081e-01 (4.3260e-01)	Acc@1  88.28 ( 86.65)	Acc@5  99.22 ( 98.66)
Epoch: [60][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6587e-01 (4.3352e-01)	Acc@1  85.94 ( 86.63)	Acc@5  99.22 ( 98.65)
Epoch: [60][340/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6449e-01 (4.3343e-01)	Acc@1  90.62 ( 86.62)	Acc@5  99.22 ( 98.65)
Epoch: [60][350/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9287e-01 (4.3220e-01)	Acc@1  89.84 ( 86.70)	Acc@5  97.66 ( 98.66)
Epoch: [60][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1950e-01 (4.3179e-01)	Acc@1  89.06 ( 86.71)	Acc@5 100.00 ( 98.66)
Epoch: [60][370/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2148e-01 (4.3115e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 ( 98.67)
Epoch: [60][380/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2036e-01 (4.3052e-01)	Acc@1  84.38 ( 86.76)	Acc@5  99.22 ( 98.68)
Epoch: [60][390/391]	Time  0.057 ( 0.065)	Data  0.001 ( 0.002)	Loss 6.3736e-01 (4.3008e-01)	Acc@1  83.75 ( 86.78)	Acc@5  97.50 ( 98.68)
## e[60] optimizer.zero_grad (sum) time: 0.4087510108947754
## e[60]       loss.backward (sum) time: 6.994891405105591
## e[60]      optimizer.step (sum) time: 3.4756462574005127
## epoch[60] training(only) time: 25.66461157798767
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.3120e+00 (1.3120e+00)	Acc@1  68.00 ( 68.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 1.3763e+00 (1.2718e+00)	Acc@1  66.00 ( 67.18)	Acc@5  91.00 ( 89.73)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 1.2586e+00 (1.2188e+00)	Acc@1  70.00 ( 67.76)	Acc@5  92.00 ( 91.19)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.5770e+00 (1.2475e+00)	Acc@1  60.00 ( 67.23)	Acc@5  89.00 ( 90.61)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1484e+00 (1.2319e+00)	Acc@1  72.00 ( 67.00)	Acc@5  94.00 ( 91.12)
Test: [ 50/100]	Time  0.025 ( 0.030)	Loss 1.3032e+00 (1.2507e+00)	Acc@1  71.00 ( 66.75)	Acc@5  88.00 ( 90.92)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.3032e+00 (1.2387e+00)	Acc@1  62.00 ( 67.20)	Acc@5  91.00 ( 90.97)
Test: [ 70/100]	Time  0.028 ( 0.029)	Loss 1.1653e+00 (1.2462e+00)	Acc@1  68.00 ( 67.13)	Acc@5  94.00 ( 90.96)
Test: [ 80/100]	Time  0.027 ( 0.029)	Loss 1.4409e+00 (1.2567e+00)	Acc@1  65.00 ( 67.10)	Acc@5  91.00 ( 90.74)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9034e+00 (1.2522e+00)	Acc@1  60.00 ( 67.23)	Acc@5  84.00 ( 90.74)
 * Acc@1 67.280 Acc@5 90.710
### epoch[60] execution time: 28.562111616134644
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.224 ( 0.224)	Data  0.154 ( 0.154)	Loss 4.3899e-01 (4.3899e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [61][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.015)	Loss 3.8869e-01 (3.8528e-01)	Acc@1  86.72 ( 88.78)	Acc@5  99.22 ( 99.08)
Epoch: [61][ 20/391]	Time  0.067 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.5847e-01 (3.9199e-01)	Acc@1  87.50 ( 88.58)	Acc@5  98.44 ( 99.03)
Epoch: [61][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.5920e-01 (3.9479e-01)	Acc@1  88.28 ( 88.43)	Acc@5 100.00 ( 99.09)
Epoch: [61][ 40/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.3088e-01 (4.0044e-01)	Acc@1  92.19 ( 88.36)	Acc@5  97.66 ( 98.88)
Epoch: [61][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.1597e-01 (4.0460e-01)	Acc@1  88.28 ( 88.30)	Acc@5  98.44 ( 98.74)
Epoch: [61][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.9011e-01 (4.0231e-01)	Acc@1  87.50 ( 88.33)	Acc@5  98.44 ( 98.71)
Epoch: [61][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.3181e-01 (4.0052e-01)	Acc@1  87.50 ( 88.49)	Acc@5  97.66 ( 98.67)
Epoch: [61][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.3250e-01 (3.9841e-01)	Acc@1  85.94 ( 88.45)	Acc@5  98.44 ( 98.71)
Epoch: [61][ 90/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.3327e-01 (4.0121e-01)	Acc@1  85.16 ( 88.22)	Acc@5  97.66 ( 98.73)
Epoch: [61][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.8040e-01 (4.0323e-01)	Acc@1  84.38 ( 88.08)	Acc@5  98.44 ( 98.71)
Epoch: [61][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.1621e-01 (4.0494e-01)	Acc@1  86.72 ( 88.01)	Acc@5  96.88 ( 98.70)
Epoch: [61][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3091e-01 (4.0249e-01)	Acc@1  90.62 ( 88.11)	Acc@5  99.22 ( 98.72)
Epoch: [61][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8447e-01 (4.0042e-01)	Acc@1  89.84 ( 88.20)	Acc@5 100.00 ( 98.73)
Epoch: [61][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4714e-01 (3.9936e-01)	Acc@1  87.50 ( 88.25)	Acc@5  98.44 ( 98.74)
Epoch: [61][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8658e-01 (4.0019e-01)	Acc@1  85.94 ( 88.17)	Acc@5  98.44 ( 98.73)
Epoch: [61][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1386e-01 (3.9933e-01)	Acc@1  89.06 ( 88.20)	Acc@5  98.44 ( 98.75)
Epoch: [61][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7287e-01 (4.0163e-01)	Acc@1  85.94 ( 88.07)	Acc@5  98.44 ( 98.76)
Epoch: [61][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5819e-01 (4.0219e-01)	Acc@1  83.59 ( 88.01)	Acc@5  96.09 ( 98.76)
Epoch: [61][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2665e-01 (4.0304e-01)	Acc@1  81.25 ( 87.92)	Acc@5 100.00 ( 98.74)
Epoch: [61][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0567e-01 (4.0429e-01)	Acc@1  84.38 ( 87.85)	Acc@5  97.66 ( 98.73)
Epoch: [61][210/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4425e-01 (4.0617e-01)	Acc@1  86.72 ( 87.80)	Acc@5  99.22 ( 98.72)
Epoch: [61][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3606e-01 (4.0726e-01)	Acc@1  89.06 ( 87.77)	Acc@5  98.44 ( 98.73)
Epoch: [61][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6845e-01 (4.0733e-01)	Acc@1  82.81 ( 87.74)	Acc@5  99.22 ( 98.74)
Epoch: [61][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2445e-01 (4.0685e-01)	Acc@1  89.06 ( 87.72)	Acc@5 100.00 ( 98.76)
Epoch: [61][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1779e-01 (4.0963e-01)	Acc@1  88.28 ( 87.63)	Acc@5  97.66 ( 98.73)
Epoch: [61][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0883e-01 (4.0886e-01)	Acc@1  81.25 ( 87.62)	Acc@5  99.22 ( 98.76)
Epoch: [61][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4853e-01 (4.0791e-01)	Acc@1  88.28 ( 87.64)	Acc@5  99.22 ( 98.77)
Epoch: [61][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6706e-01 (4.0812e-01)	Acc@1  87.50 ( 87.62)	Acc@5 100.00 ( 98.77)
Epoch: [61][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4024e-01 (4.0844e-01)	Acc@1  92.19 ( 87.62)	Acc@5  99.22 ( 98.77)
Epoch: [61][300/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1906e-01 (4.0835e-01)	Acc@1  89.84 ( 87.63)	Acc@5 100.00 ( 98.77)
Epoch: [61][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7338e-01 (4.0767e-01)	Acc@1  89.06 ( 87.67)	Acc@5 100.00 ( 98.79)
Epoch: [61][320/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.1926e-01 (4.0723e-01)	Acc@1  83.59 ( 87.67)	Acc@5  97.66 ( 98.79)
Epoch: [61][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8493e-01 (4.0796e-01)	Acc@1  86.72 ( 87.65)	Acc@5  99.22 ( 98.77)
Epoch: [61][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4724e-01 (4.0848e-01)	Acc@1  92.19 ( 87.64)	Acc@5  98.44 ( 98.75)
Epoch: [61][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8052e-01 (4.0806e-01)	Acc@1  86.72 ( 87.66)	Acc@5 100.00 ( 98.75)
Epoch: [61][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4147e-01 (4.0745e-01)	Acc@1  85.16 ( 87.67)	Acc@5  98.44 ( 98.76)
Epoch: [61][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.2451e-01 (4.0699e-01)	Acc@1  82.03 ( 87.70)	Acc@5  98.44 ( 98.77)
Epoch: [61][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9729e-01 (4.0630e-01)	Acc@1  87.50 ( 87.69)	Acc@5  96.88 ( 98.78)
Epoch: [61][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9446e-01 (4.0532e-01)	Acc@1  91.25 ( 87.72)	Acc@5 100.00 ( 98.79)
## e[61] optimizer.zero_grad (sum) time: 0.4133727550506592
## e[61]       loss.backward (sum) time: 6.981083869934082
## e[61]      optimizer.step (sum) time: 3.464890718460083
## epoch[61] training(only) time: 25.626212120056152
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.3022e+00 (1.3022e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.028 ( 0.040)	Loss 1.3687e+00 (1.2724e+00)	Acc@1  66.00 ( 67.27)	Acc@5  92.00 ( 90.45)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 1.2759e+00 (1.2098e+00)	Acc@1  67.00 ( 68.24)	Acc@5  92.00 ( 91.52)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.6046e+00 (1.2419e+00)	Acc@1  60.00 ( 67.71)	Acc@5  87.00 ( 90.90)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1257e+00 (1.2267e+00)	Acc@1  70.00 ( 67.46)	Acc@5  94.00 ( 91.44)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2680e+00 (1.2447e+00)	Acc@1  71.00 ( 67.18)	Acc@5  88.00 ( 91.20)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.3002e+00 (1.2329e+00)	Acc@1  63.00 ( 67.51)	Acc@5  90.00 ( 91.20)
Test: [ 70/100]	Time  0.026 ( 0.029)	Loss 1.1304e+00 (1.2408e+00)	Acc@1  67.00 ( 67.54)	Acc@5  94.00 ( 91.07)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3967e+00 (1.2500e+00)	Acc@1  65.00 ( 67.37)	Acc@5  91.00 ( 90.89)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8620e+00 (1.2431e+00)	Acc@1  58.00 ( 67.58)	Acc@5  85.00 ( 90.87)
 * Acc@1 67.660 Acc@5 90.910
### epoch[61] execution time: 28.475899696350098
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.220 ( 0.220)	Data  0.148 ( 0.148)	Loss 4.4000e-01 (4.4000e-01)	Acc@1  84.38 ( 84.38)	Acc@5  98.44 ( 98.44)
Epoch: [62][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.015)	Loss 3.4712e-01 (4.5241e-01)	Acc@1  88.28 ( 85.72)	Acc@5 100.00 ( 98.72)
Epoch: [62][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.4037e-01 (4.3124e-01)	Acc@1  85.94 ( 86.27)	Acc@5  98.44 ( 98.77)
Epoch: [62][ 30/391]	Time  0.070 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.8841e-01 (4.2226e-01)	Acc@1  87.50 ( 86.67)	Acc@5  99.22 ( 98.99)
Epoch: [62][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.6416e-01 (4.1894e-01)	Acc@1  88.28 ( 86.85)	Acc@5 100.00 ( 99.01)
Epoch: [62][ 50/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.8299e-01 (4.0665e-01)	Acc@1  84.38 ( 87.38)	Acc@5  98.44 ( 99.02)
Epoch: [62][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.5107e-01 (4.0660e-01)	Acc@1  86.72 ( 87.38)	Acc@5  97.66 ( 98.99)
Epoch: [62][ 70/391]	Time  0.059 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.0102e-01 (4.0844e-01)	Acc@1  86.72 ( 87.21)	Acc@5  98.44 ( 98.93)
Epoch: [62][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.7137e-01 (4.0368e-01)	Acc@1  89.06 ( 87.42)	Acc@5 100.00 ( 98.98)
Epoch: [62][ 90/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.1878e-01 (3.9960e-01)	Acc@1  90.62 ( 87.67)	Acc@5 100.00 ( 99.01)
Epoch: [62][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.3086e-01 (3.9820e-01)	Acc@1  89.06 ( 87.78)	Acc@5  99.22 ( 99.01)
Epoch: [62][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.3433e-01 (3.9718e-01)	Acc@1  89.84 ( 87.92)	Acc@5  99.22 ( 98.99)
Epoch: [62][120/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.5731e-01 (3.9286e-01)	Acc@1  92.97 ( 88.06)	Acc@5  99.22 ( 99.03)
Epoch: [62][130/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8636e-01 (3.9451e-01)	Acc@1  91.41 ( 88.01)	Acc@5  97.66 ( 98.99)
Epoch: [62][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7617e-01 (3.9436e-01)	Acc@1  89.06 ( 88.06)	Acc@5  98.44 ( 98.99)
Epoch: [62][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0816e-01 (3.9504e-01)	Acc@1  88.28 ( 88.01)	Acc@5  99.22 ( 98.99)
Epoch: [62][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7881e-01 (3.9391e-01)	Acc@1  93.75 ( 88.08)	Acc@5 100.00 ( 98.99)
Epoch: [62][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5583e-01 (3.9413e-01)	Acc@1  83.59 ( 88.05)	Acc@5  97.66 ( 98.96)
Epoch: [62][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1035e-01 (3.9417e-01)	Acc@1  88.28 ( 88.09)	Acc@5  98.44 ( 98.95)
Epoch: [62][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5108e-01 (3.9402e-01)	Acc@1  89.84 ( 88.08)	Acc@5  99.22 ( 98.96)
Epoch: [62][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7030e-01 (3.9523e-01)	Acc@1  86.72 ( 88.03)	Acc@5  99.22 ( 98.95)
Epoch: [62][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6372e-01 (3.9556e-01)	Acc@1  89.06 ( 88.07)	Acc@5  98.44 ( 98.95)
Epoch: [62][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2178e-01 (3.9471e-01)	Acc@1  89.06 ( 88.09)	Acc@5  99.22 ( 98.96)
Epoch: [62][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8534e-01 (3.9485e-01)	Acc@1  87.50 ( 88.08)	Acc@5  96.88 ( 98.96)
Epoch: [62][240/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0379e-01 (3.9410e-01)	Acc@1  87.50 ( 88.10)	Acc@5  99.22 ( 98.96)
Epoch: [62][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7669e-01 (3.9402e-01)	Acc@1  88.28 ( 88.07)	Acc@5  98.44 ( 98.97)
Epoch: [62][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5071e-01 (3.9358e-01)	Acc@1  88.28 ( 88.07)	Acc@5  99.22 ( 98.97)
Epoch: [62][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9904e-01 (3.9354e-01)	Acc@1  85.16 ( 88.09)	Acc@5  98.44 ( 98.98)
Epoch: [62][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9201e-01 (3.9399e-01)	Acc@1  88.28 ( 88.10)	Acc@5  98.44 ( 98.97)
Epoch: [62][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8085e-01 (3.9470e-01)	Acc@1  90.62 ( 88.11)	Acc@5  97.66 ( 98.96)
Epoch: [62][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8062e-01 (3.9486e-01)	Acc@1  91.41 ( 88.12)	Acc@5 100.00 ( 98.97)
Epoch: [62][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4753e-01 (3.9587e-01)	Acc@1  85.16 ( 88.07)	Acc@5  94.53 ( 98.95)
Epoch: [62][320/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6979e-01 (3.9567e-01)	Acc@1  87.50 ( 88.08)	Acc@5 100.00 ( 98.94)
Epoch: [62][330/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0921e-01 (3.9590e-01)	Acc@1  92.97 ( 88.09)	Acc@5  97.66 ( 98.92)
Epoch: [62][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6401e-01 (3.9582e-01)	Acc@1  90.62 ( 88.08)	Acc@5  98.44 ( 98.92)
Epoch: [62][350/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5971e-01 (3.9588e-01)	Acc@1  89.06 ( 88.12)	Acc@5  98.44 ( 98.91)
Epoch: [62][360/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7083e-01 (3.9632e-01)	Acc@1  89.06 ( 88.08)	Acc@5  96.88 ( 98.91)
Epoch: [62][370/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4648e-01 (3.9595e-01)	Acc@1  89.84 ( 88.07)	Acc@5  99.22 ( 98.92)
Epoch: [62][380/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3788e-01 (3.9607e-01)	Acc@1  89.06 ( 88.05)	Acc@5  97.66 ( 98.92)
Epoch: [62][390/391]	Time  0.047 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8016e-01 (3.9578e-01)	Acc@1  92.50 ( 88.06)	Acc@5 100.00 ( 98.93)
## e[62] optimizer.zero_grad (sum) time: 0.41496992111206055
## e[62]       loss.backward (sum) time: 7.027421951293945
## e[62]      optimizer.step (sum) time: 3.4815237522125244
## epoch[62] training(only) time: 25.713865995407104
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2975e+00 (1.2975e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.029 ( 0.039)	Loss 1.3237e+00 (1.2693e+00)	Acc@1  67.00 ( 67.27)	Acc@5  91.00 ( 90.18)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.2697e+00 (1.2152e+00)	Acc@1  67.00 ( 68.38)	Acc@5  92.00 ( 91.29)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6008e+00 (1.2511e+00)	Acc@1  58.00 ( 67.68)	Acc@5  88.00 ( 90.68)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.1161e+00 (1.2326e+00)	Acc@1  71.00 ( 67.59)	Acc@5  95.00 ( 91.22)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2661e+00 (1.2527e+00)	Acc@1  73.00 ( 67.35)	Acc@5  88.00 ( 90.98)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2981e+00 (1.2404e+00)	Acc@1  63.00 ( 67.57)	Acc@5  90.00 ( 91.08)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.1514e+00 (1.2489e+00)	Acc@1  67.00 ( 67.70)	Acc@5  93.00 ( 90.94)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 1.4001e+00 (1.2587e+00)	Acc@1  65.00 ( 67.53)	Acc@5  92.00 ( 90.75)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8977e+00 (1.2528e+00)	Acc@1  60.00 ( 67.77)	Acc@5  84.00 ( 90.76)
 * Acc@1 67.680 Acc@5 90.810
### epoch[62] execution time: 28.541457653045654
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.226 ( 0.226)	Data  0.157 ( 0.157)	Loss 2.9083e-01 (2.9083e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [63][ 10/391]	Time  0.067 ( 0.081)	Data  0.001 ( 0.015)	Loss 3.5743e-01 (3.7097e-01)	Acc@1  88.28 ( 88.78)	Acc@5 100.00 ( 99.08)
Epoch: [63][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.009)	Loss 3.4419e-01 (3.7232e-01)	Acc@1  91.41 ( 88.95)	Acc@5  99.22 ( 99.07)
Epoch: [63][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.5796e-01 (3.8511e-01)	Acc@1  85.94 ( 88.48)	Acc@5  98.44 ( 99.02)
Epoch: [63][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.9872e-01 (3.8394e-01)	Acc@1  90.62 ( 88.41)	Acc@5 100.00 ( 98.97)
Epoch: [63][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.4117e-01 (3.8802e-01)	Acc@1  91.41 ( 88.45)	Acc@5  99.22 ( 98.96)
Epoch: [63][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.2132e-01 (3.8685e-01)	Acc@1  90.62 ( 88.60)	Acc@5 100.00 ( 98.99)
Epoch: [63][ 70/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.003)	Loss 2.6702e-01 (3.8351e-01)	Acc@1  90.62 ( 88.64)	Acc@5 100.00 ( 99.03)
Epoch: [63][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.9103e-01 (3.8532e-01)	Acc@1  84.38 ( 88.54)	Acc@5  96.09 ( 98.96)
Epoch: [63][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5518e-01 (3.8676e-01)	Acc@1  88.28 ( 88.43)	Acc@5  98.44 ( 98.96)
Epoch: [63][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.9633e-01 (3.8703e-01)	Acc@1  83.59 ( 88.44)	Acc@5  98.44 ( 98.93)
Epoch: [63][110/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9013e-01 (3.8536e-01)	Acc@1  88.28 ( 88.53)	Acc@5  99.22 ( 98.93)
Epoch: [63][120/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.6395e-01 (3.8583e-01)	Acc@1  89.06 ( 88.46)	Acc@5 100.00 ( 98.95)
Epoch: [63][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3406e-01 (3.8573e-01)	Acc@1  89.06 ( 88.46)	Acc@5 100.00 ( 98.95)
Epoch: [63][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1965e-01 (3.8588e-01)	Acc@1  86.72 ( 88.44)	Acc@5  99.22 ( 98.95)
Epoch: [63][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6665e-01 (3.8659e-01)	Acc@1  88.28 ( 88.38)	Acc@5  98.44 ( 98.95)
Epoch: [63][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2402e-01 (3.8925e-01)	Acc@1  85.94 ( 88.28)	Acc@5 100.00 ( 98.94)
Epoch: [63][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3835e-01 (3.8857e-01)	Acc@1  89.84 ( 88.35)	Acc@5  99.22 ( 98.93)
Epoch: [63][180/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9196e-01 (3.8808e-01)	Acc@1  85.94 ( 88.31)	Acc@5  99.22 ( 98.93)
Epoch: [63][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2256e-01 (3.8807e-01)	Acc@1  89.84 ( 88.28)	Acc@5 100.00 ( 98.94)
Epoch: [63][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0521e-01 (3.8752e-01)	Acc@1  85.16 ( 88.26)	Acc@5 100.00 ( 98.94)
Epoch: [63][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5520e-01 (3.8771e-01)	Acc@1  89.84 ( 88.25)	Acc@5  99.22 ( 98.95)
Epoch: [63][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2820e-01 (3.8891e-01)	Acc@1  89.06 ( 88.21)	Acc@5  99.22 ( 98.93)
Epoch: [63][230/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.5311e-01 (3.8929e-01)	Acc@1  84.38 ( 88.22)	Acc@5  99.22 ( 98.92)
Epoch: [63][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6874e-01 (3.8957e-01)	Acc@1  83.59 ( 88.19)	Acc@5  97.66 ( 98.93)
Epoch: [63][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1157e-01 (3.9022e-01)	Acc@1  87.50 ( 88.17)	Acc@5  98.44 ( 98.92)
Epoch: [63][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9093e-01 (3.9044e-01)	Acc@1  88.28 ( 88.20)	Acc@5  98.44 ( 98.90)
Epoch: [63][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7226e-01 (3.8987e-01)	Acc@1  91.41 ( 88.22)	Acc@5 100.00 ( 98.92)
Epoch: [63][280/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1554e-01 (3.8950e-01)	Acc@1  88.28 ( 88.24)	Acc@5  97.66 ( 98.92)
Epoch: [63][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5814e-01 (3.8892e-01)	Acc@1  87.50 ( 88.26)	Acc@5  98.44 ( 98.91)
Epoch: [63][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2689e-01 (3.9058e-01)	Acc@1  82.03 ( 88.19)	Acc@5  96.88 ( 98.91)
Epoch: [63][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9882e-01 (3.9099e-01)	Acc@1  84.38 ( 88.16)	Acc@5  98.44 ( 98.90)
Epoch: [63][320/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4230e-01 (3.9212e-01)	Acc@1  93.75 ( 88.10)	Acc@5  98.44 ( 98.88)
Epoch: [63][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8155e-01 (3.9194e-01)	Acc@1  89.06 ( 88.12)	Acc@5  98.44 ( 98.88)
Epoch: [63][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8679e-01 (3.9215e-01)	Acc@1  82.81 ( 88.11)	Acc@5  99.22 ( 98.87)
Epoch: [63][350/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8297e-01 (3.9228e-01)	Acc@1  89.06 ( 88.10)	Acc@5  99.22 ( 98.87)
Epoch: [63][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8989e-01 (3.9208e-01)	Acc@1  89.84 ( 88.10)	Acc@5  97.66 ( 98.87)
Epoch: [63][370/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8287e-01 (3.9114e-01)	Acc@1  93.75 ( 88.15)	Acc@5 100.00 ( 98.88)
Epoch: [63][380/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7162e-01 (3.9121e-01)	Acc@1  85.94 ( 88.14)	Acc@5  99.22 ( 98.88)
Epoch: [63][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4825e-01 (3.9041e-01)	Acc@1  87.50 ( 88.16)	Acc@5  97.50 ( 98.89)
## e[63] optimizer.zero_grad (sum) time: 0.4153175354003906
## e[63]       loss.backward (sum) time: 6.960049390792847
## e[63]      optimizer.step (sum) time: 3.5021286010742188
## epoch[63] training(only) time: 25.61979627609253
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.3021e+00 (1.3021e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.030 ( 0.039)	Loss 1.3489e+00 (1.2704e+00)	Acc@1  66.00 ( 67.55)	Acc@5  91.00 ( 90.09)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 1.3035e+00 (1.2152e+00)	Acc@1  67.00 ( 68.38)	Acc@5  93.00 ( 91.14)
Test: [ 30/100]	Time  0.025 ( 0.032)	Loss 1.6058e+00 (1.2485e+00)	Acc@1  59.00 ( 67.68)	Acc@5  88.00 ( 90.68)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.0900e+00 (1.2328e+00)	Acc@1  70.00 ( 67.63)	Acc@5  95.00 ( 91.24)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2520e+00 (1.2526e+00)	Acc@1  69.00 ( 67.24)	Acc@5  89.00 ( 91.04)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2683e+00 (1.2406e+00)	Acc@1  62.00 ( 67.44)	Acc@5  90.00 ( 91.11)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1481e+00 (1.2477e+00)	Acc@1  69.00 ( 67.49)	Acc@5  93.00 ( 91.10)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3907e+00 (1.2578e+00)	Acc@1  64.00 ( 67.37)	Acc@5  92.00 ( 90.91)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.9035e+00 (1.2518e+00)	Acc@1  60.00 ( 67.57)	Acc@5  84.00 ( 90.91)
 * Acc@1 67.510 Acc@5 90.930
### epoch[63] execution time: 28.47412109375
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.213 ( 0.213)	Data  0.144 ( 0.144)	Loss 4.7175e-01 (4.7175e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [64][ 10/391]	Time  0.063 ( 0.078)	Data  0.001 ( 0.014)	Loss 3.6768e-01 (3.8187e-01)	Acc@1  90.62 ( 88.28)	Acc@5 100.00 ( 99.15)
Epoch: [64][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 3.5101e-01 (3.7420e-01)	Acc@1  88.28 ( 88.39)	Acc@5 100.00 ( 99.11)
Epoch: [64][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.3147e-01 (3.7652e-01)	Acc@1  89.06 ( 88.28)	Acc@5 100.00 ( 99.09)
Epoch: [64][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.1442e-01 (3.7519e-01)	Acc@1  86.72 ( 88.41)	Acc@5  99.22 ( 98.97)
Epoch: [64][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.7850e-01 (3.7749e-01)	Acc@1  88.28 ( 88.45)	Acc@5  99.22 ( 98.93)
Epoch: [64][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.004)	Loss 5.0478e-01 (3.7851e-01)	Acc@1  82.03 ( 88.47)	Acc@5  97.66 ( 98.94)
Epoch: [64][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.4953e-01 (3.7387e-01)	Acc@1  91.41 ( 88.75)	Acc@5  99.22 ( 98.98)
Epoch: [64][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4633e-01 (3.7348e-01)	Acc@1  87.50 ( 88.87)	Acc@5  97.66 ( 98.99)
Epoch: [64][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4031e-01 (3.7650e-01)	Acc@1  86.72 ( 88.70)	Acc@5  98.44 ( 98.98)
Epoch: [64][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.8590e-01 (3.8059e-01)	Acc@1  90.62 ( 88.54)	Acc@5  98.44 ( 98.97)
Epoch: [64][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5860e-01 (3.8366e-01)	Acc@1  92.97 ( 88.45)	Acc@5 100.00 ( 98.95)
Epoch: [64][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6212e-01 (3.8384e-01)	Acc@1  90.62 ( 88.37)	Acc@5  99.22 ( 98.95)
Epoch: [64][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6195e-01 (3.8470e-01)	Acc@1  89.06 ( 88.26)	Acc@5  99.22 ( 98.97)
Epoch: [64][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2969e-01 (3.8357e-01)	Acc@1  90.62 ( 88.26)	Acc@5  98.44 ( 98.99)
Epoch: [64][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8603e-01 (3.8225e-01)	Acc@1  92.97 ( 88.32)	Acc@5  99.22 ( 99.00)
Epoch: [64][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0161e-01 (3.8355e-01)	Acc@1  84.38 ( 88.19)	Acc@5 100.00 ( 99.01)
Epoch: [64][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9628e-01 (3.8282e-01)	Acc@1  85.16 ( 88.26)	Acc@5 100.00 ( 99.01)
Epoch: [64][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9773e-01 (3.8330e-01)	Acc@1  86.72 ( 88.26)	Acc@5  99.22 ( 99.00)
Epoch: [64][190/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1821e-01 (3.8276e-01)	Acc@1  87.50 ( 88.27)	Acc@5  97.66 ( 99.00)
Epoch: [64][200/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3685e-01 (3.8322e-01)	Acc@1  91.41 ( 88.27)	Acc@5  98.44 ( 99.00)
Epoch: [64][210/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4134e-01 (3.8368e-01)	Acc@1  89.06 ( 88.19)	Acc@5 100.00 ( 99.02)
Epoch: [64][220/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5856e-01 (3.8249e-01)	Acc@1  87.50 ( 88.25)	Acc@5 100.00 ( 99.03)
Epoch: [64][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3016e-01 (3.8472e-01)	Acc@1  87.50 ( 88.18)	Acc@5  98.44 ( 99.00)
Epoch: [64][240/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3837e-01 (3.8374e-01)	Acc@1  87.50 ( 88.21)	Acc@5 100.00 ( 99.02)
Epoch: [64][250/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5436e-01 (3.8271e-01)	Acc@1  90.62 ( 88.25)	Acc@5 100.00 ( 99.04)
Epoch: [64][260/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4568e-01 (3.8209e-01)	Acc@1  82.81 ( 88.29)	Acc@5  96.09 ( 99.04)
Epoch: [64][270/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0030e-01 (3.8162e-01)	Acc@1  93.75 ( 88.29)	Acc@5  99.22 ( 99.05)
Epoch: [64][280/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3399e-01 (3.8185e-01)	Acc@1  84.38 ( 88.30)	Acc@5  98.44 ( 99.03)
Epoch: [64][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2268e-01 (3.8097e-01)	Acc@1  92.19 ( 88.34)	Acc@5 100.00 ( 99.05)
Epoch: [64][300/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8516e-01 (3.8053e-01)	Acc@1  88.28 ( 88.33)	Acc@5 100.00 ( 99.06)
Epoch: [64][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1928e-01 (3.8003e-01)	Acc@1  89.84 ( 88.35)	Acc@5  98.44 ( 99.05)
Epoch: [64][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2014e-01 (3.8032e-01)	Acc@1  85.94 ( 88.33)	Acc@5  99.22 ( 99.04)
Epoch: [64][330/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7028e-01 (3.7939e-01)	Acc@1  93.75 ( 88.37)	Acc@5  98.44 ( 99.04)
Epoch: [64][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4518e-01 (3.8035e-01)	Acc@1  84.38 ( 88.33)	Acc@5  98.44 ( 99.04)
Epoch: [64][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0100e-01 (3.8065e-01)	Acc@1  91.41 ( 88.33)	Acc@5  97.66 ( 99.03)
Epoch: [64][360/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3061e-01 (3.8066e-01)	Acc@1  92.19 ( 88.31)	Acc@5 100.00 ( 99.04)
Epoch: [64][370/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1405e-01 (3.8046e-01)	Acc@1  85.94 ( 88.32)	Acc@5  97.66 ( 99.03)
Epoch: [64][380/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0332e-01 (3.7984e-01)	Acc@1  86.72 ( 88.35)	Acc@5  99.22 ( 99.03)
Epoch: [64][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2100e-01 (3.7966e-01)	Acc@1  86.25 ( 88.36)	Acc@5  98.75 ( 99.02)
## e[64] optimizer.zero_grad (sum) time: 0.40993285179138184
## e[64]       loss.backward (sum) time: 6.96743631362915
## e[64]      optimizer.step (sum) time: 3.473675489425659
## epoch[64] training(only) time: 25.5249605178833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.2903e+00 (1.2903e+00)	Acc@1  68.00 ( 68.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.3242e+00 (1.2816e+00)	Acc@1  66.00 ( 67.00)	Acc@5  92.00 ( 90.00)
Test: [ 20/100]	Time  0.028 ( 0.034)	Loss 1.3069e+00 (1.2226e+00)	Acc@1  66.00 ( 67.57)	Acc@5  92.00 ( 90.86)
Test: [ 30/100]	Time  0.025 ( 0.032)	Loss 1.5972e+00 (1.2561e+00)	Acc@1  64.00 ( 67.35)	Acc@5  87.00 ( 90.58)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1484e+00 (1.2399e+00)	Acc@1  69.00 ( 67.32)	Acc@5  95.00 ( 91.12)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2531e+00 (1.2607e+00)	Acc@1  71.00 ( 67.10)	Acc@5  89.00 ( 90.88)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.2673e+00 (1.2489e+00)	Acc@1  62.00 ( 67.34)	Acc@5  91.00 ( 90.98)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.1720e+00 (1.2576e+00)	Acc@1  65.00 ( 67.41)	Acc@5  94.00 ( 90.97)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4056e+00 (1.2679e+00)	Acc@1  66.00 ( 67.35)	Acc@5  93.00 ( 90.77)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.8953e+00 (1.2601e+00)	Acc@1  58.00 ( 67.58)	Acc@5  84.00 ( 90.74)
 * Acc@1 67.480 Acc@5 90.830
### epoch[64] execution time: 28.4042809009552
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.218 ( 0.218)	Data  0.150 ( 0.150)	Loss 3.8435e-01 (3.8435e-01)	Acc@1  88.28 ( 88.28)	Acc@5  98.44 ( 98.44)
Epoch: [65][ 10/391]	Time  0.065 ( 0.078)	Data  0.001 ( 0.015)	Loss 4.9617e-01 (3.7790e-01)	Acc@1  82.81 ( 88.00)	Acc@5  98.44 ( 99.01)
Epoch: [65][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.008)	Loss 2.7640e-01 (3.7886e-01)	Acc@1  91.41 ( 88.06)	Acc@5  99.22 ( 98.81)
Epoch: [65][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.8291e-01 (3.9112e-01)	Acc@1  89.06 ( 88.05)	Acc@5  98.44 ( 98.71)
Epoch: [65][ 40/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.9639e-01 (3.8938e-01)	Acc@1  84.38 ( 88.07)	Acc@5 100.00 ( 98.82)
Epoch: [65][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.6338e-01 (3.8444e-01)	Acc@1  88.28 ( 88.04)	Acc@5 100.00 ( 98.93)
Epoch: [65][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.9565e-01 (3.7842e-01)	Acc@1  89.84 ( 88.29)	Acc@5  98.44 ( 98.96)
Epoch: [65][ 70/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.0004e-01 (3.7539e-01)	Acc@1  91.41 ( 88.53)	Acc@5  99.22 ( 98.98)
Epoch: [65][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.6897e-01 (3.7981e-01)	Acc@1  89.06 ( 88.31)	Acc@5  99.22 ( 98.97)
Epoch: [65][ 90/391]	Time  0.075 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8099e-01 (3.7966e-01)	Acc@1  91.41 ( 88.37)	Acc@5 100.00 ( 98.97)
Epoch: [65][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.6404e-01 (3.8082e-01)	Acc@1  89.06 ( 88.27)	Acc@5 100.00 ( 99.00)
Epoch: [65][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.2704e-01 (3.8250e-01)	Acc@1  89.84 ( 88.23)	Acc@5 100.00 ( 98.99)
Epoch: [65][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3389e-01 (3.8334e-01)	Acc@1  89.06 ( 88.19)	Acc@5 100.00 ( 98.99)
Epoch: [65][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4175e-01 (3.8030e-01)	Acc@1  85.16 ( 88.28)	Acc@5 100.00 ( 99.00)
Epoch: [65][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1538e-01 (3.7929e-01)	Acc@1  86.72 ( 88.33)	Acc@5  99.22 ( 99.00)
Epoch: [65][150/391]	Time  0.069 ( 0.066)	Data  0.002 ( 0.002)	Loss 3.7700e-01 (3.7999e-01)	Acc@1  86.72 ( 88.21)	Acc@5 100.00 ( 99.02)
Epoch: [65][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7543e-01 (3.8126e-01)	Acc@1  84.38 ( 88.19)	Acc@5  98.44 ( 99.02)
Epoch: [65][170/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7204e-01 (3.8116e-01)	Acc@1  87.50 ( 88.23)	Acc@5  99.22 ( 99.01)
Epoch: [65][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4717e-01 (3.8127e-01)	Acc@1  86.72 ( 88.22)	Acc@5  99.22 ( 99.02)
Epoch: [65][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7287e-01 (3.7991e-01)	Acc@1  92.19 ( 88.27)	Acc@5 100.00 ( 99.06)
Epoch: [65][200/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0545e-01 (3.8005e-01)	Acc@1  91.41 ( 88.29)	Acc@5 100.00 ( 99.06)
Epoch: [65][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4050e-01 (3.7949e-01)	Acc@1  89.84 ( 88.37)	Acc@5  99.22 ( 99.06)
Epoch: [65][220/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5766e-01 (3.8048e-01)	Acc@1  86.72 ( 88.36)	Acc@5  98.44 ( 99.03)
Epoch: [65][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2366e-01 (3.8041e-01)	Acc@1  90.62 ( 88.37)	Acc@5  98.44 ( 99.03)
Epoch: [65][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1064e-01 (3.7918e-01)	Acc@1  80.47 ( 88.38)	Acc@5  98.44 ( 99.05)
Epoch: [65][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6407e-01 (3.7941e-01)	Acc@1  87.50 ( 88.35)	Acc@5  99.22 ( 99.06)
Epoch: [65][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8987e-01 (3.7877e-01)	Acc@1  85.16 ( 88.38)	Acc@5 100.00 ( 99.05)
Epoch: [65][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2283e-01 (3.7905e-01)	Acc@1  89.06 ( 88.38)	Acc@5 100.00 ( 99.03)
Epoch: [65][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1497e-01 (3.7909e-01)	Acc@1  89.84 ( 88.36)	Acc@5 100.00 ( 99.05)
Epoch: [65][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.2755e-01 (3.7943e-01)	Acc@1  80.47 ( 88.36)	Acc@5  98.44 ( 99.04)
Epoch: [65][300/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4702e-01 (3.8000e-01)	Acc@1  91.41 ( 88.38)	Acc@5  98.44 ( 99.01)
Epoch: [65][310/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7401e-01 (3.8029e-01)	Acc@1  86.72 ( 88.36)	Acc@5  99.22 ( 99.01)
Epoch: [65][320/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4354e-01 (3.7989e-01)	Acc@1  90.62 ( 88.38)	Acc@5 100.00 ( 99.00)
Epoch: [65][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6037e-01 (3.8084e-01)	Acc@1  91.41 ( 88.38)	Acc@5 100.00 ( 99.00)
Epoch: [65][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5368e-01 (3.8155e-01)	Acc@1  92.97 ( 88.36)	Acc@5  98.44 ( 98.99)
Epoch: [65][350/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3366e-01 (3.8164e-01)	Acc@1  85.94 ( 88.35)	Acc@5  98.44 ( 98.99)
Epoch: [65][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5211e-01 (3.8139e-01)	Acc@1  89.84 ( 88.36)	Acc@5 100.00 ( 98.98)
Epoch: [65][370/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0239e-01 (3.8204e-01)	Acc@1  85.16 ( 88.34)	Acc@5  99.22 ( 98.96)
Epoch: [65][380/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1418e-01 (3.8223e-01)	Acc@1  86.72 ( 88.33)	Acc@5  97.66 ( 98.96)
Epoch: [65][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4128e-01 (3.8276e-01)	Acc@1  85.00 ( 88.32)	Acc@5 100.00 ( 98.95)
## e[65] optimizer.zero_grad (sum) time: 0.4043726921081543
## e[65]       loss.backward (sum) time: 6.96467399597168
## e[65]      optimizer.step (sum) time: 3.5485188961029053
## epoch[65] training(only) time: 25.699036359786987
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 1.2856e+00 (1.2856e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.025 ( 0.040)	Loss 1.3275e+00 (1.2741e+00)	Acc@1  64.00 ( 66.64)	Acc@5  91.00 ( 90.09)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.3696e+00 (1.2200e+00)	Acc@1  67.00 ( 67.86)	Acc@5  92.00 ( 91.19)
Test: [ 30/100]	Time  0.030 ( 0.031)	Loss 1.6218e+00 (1.2543e+00)	Acc@1  60.00 ( 67.55)	Acc@5  87.00 ( 90.55)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1412e+00 (1.2403e+00)	Acc@1  69.00 ( 67.27)	Acc@5  95.00 ( 91.17)
Test: [ 50/100]	Time  0.028 ( 0.030)	Loss 1.2974e+00 (1.2616e+00)	Acc@1  71.00 ( 67.00)	Acc@5  88.00 ( 90.94)
Test: [ 60/100]	Time  0.029 ( 0.029)	Loss 1.2552e+00 (1.2488e+00)	Acc@1  63.00 ( 67.34)	Acc@5  91.00 ( 90.97)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.1356e+00 (1.2566e+00)	Acc@1  68.00 ( 67.42)	Acc@5  94.00 ( 91.00)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.3948e+00 (1.2663e+00)	Acc@1  66.00 ( 67.23)	Acc@5  92.00 ( 90.83)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.9153e+00 (1.2601e+00)	Acc@1  60.00 ( 67.42)	Acc@5  84.00 ( 90.79)
 * Acc@1 67.370 Acc@5 90.820
### epoch[65] execution time: 28.57917809486389
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.215 ( 0.215)	Data  0.143 ( 0.143)	Loss 5.5102e-01 (5.5102e-01)	Acc@1  82.03 ( 82.03)	Acc@5  97.66 ( 97.66)
Epoch: [66][ 10/391]	Time  0.062 ( 0.078)	Data  0.001 ( 0.014)	Loss 3.3760e-01 (3.7594e-01)	Acc@1  91.41 ( 88.92)	Acc@5  98.44 ( 99.15)
Epoch: [66][ 20/391]	Time  0.065 ( 0.071)	Data  0.001 ( 0.008)	Loss 3.4908e-01 (3.8271e-01)	Acc@1  92.19 ( 88.91)	Acc@5  99.22 ( 98.88)
Epoch: [66][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.2385e-01 (3.7312e-01)	Acc@1  88.28 ( 89.01)	Acc@5 100.00 ( 99.02)
Epoch: [66][ 40/391]	Time  0.061 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.4157e-01 (3.7648e-01)	Acc@1  87.50 ( 88.81)	Acc@5 100.00 ( 98.99)
Epoch: [66][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.3935e-01 (3.7030e-01)	Acc@1  88.28 ( 89.09)	Acc@5  97.66 ( 98.99)
Epoch: [66][ 60/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.0356e-01 (3.6573e-01)	Acc@1  87.50 ( 89.25)	Acc@5  98.44 ( 99.01)
Epoch: [66][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.4702e-01 (3.6639e-01)	Acc@1  85.94 ( 89.18)	Acc@5  99.22 ( 99.06)
Epoch: [66][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.8564e-01 (3.6861e-01)	Acc@1  88.28 ( 89.14)	Acc@5  96.09 ( 99.02)
Epoch: [66][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9916e-01 (3.7303e-01)	Acc@1  86.72 ( 88.81)	Acc@5  98.44 ( 98.97)
Epoch: [66][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.8504e-01 (3.7355e-01)	Acc@1  82.81 ( 88.73)	Acc@5  98.44 ( 98.99)
Epoch: [66][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8904e-01 (3.7146e-01)	Acc@1  88.28 ( 88.81)	Acc@5  99.22 ( 99.01)
Epoch: [66][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8762e-01 (3.7197e-01)	Acc@1  87.50 ( 88.85)	Acc@5  98.44 ( 98.98)
Epoch: [66][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6875e-01 (3.7213e-01)	Acc@1  86.72 ( 88.84)	Acc@5  99.22 ( 98.96)
Epoch: [66][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0720e-01 (3.7369e-01)	Acc@1  89.06 ( 88.86)	Acc@5  99.22 ( 98.95)
Epoch: [66][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1059e-01 (3.7244e-01)	Acc@1  92.19 ( 88.80)	Acc@5  99.22 ( 98.99)
Epoch: [66][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0878e-01 (3.7333e-01)	Acc@1  87.50 ( 88.75)	Acc@5  99.22 ( 99.00)
Epoch: [66][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0142e-01 (3.7346e-01)	Acc@1  89.06 ( 88.73)	Acc@5  99.22 ( 99.01)
Epoch: [66][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7501e-01 (3.7316e-01)	Acc@1  92.19 ( 88.75)	Acc@5  98.44 ( 99.02)
Epoch: [66][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6831e-01 (3.7333e-01)	Acc@1  85.94 ( 88.71)	Acc@5 100.00 ( 99.01)
Epoch: [66][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5355e-01 (3.7607e-01)	Acc@1  85.16 ( 88.61)	Acc@5  98.44 ( 98.99)
Epoch: [66][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3618e-01 (3.7714e-01)	Acc@1  89.84 ( 88.56)	Acc@5  99.22 ( 98.96)
Epoch: [66][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8289e-01 (3.7690e-01)	Acc@1  87.50 ( 88.57)	Acc@5  99.22 ( 98.96)
Epoch: [66][230/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6470e-01 (3.7701e-01)	Acc@1  85.94 ( 88.56)	Acc@5 100.00 ( 98.97)
Epoch: [66][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0718e-01 (3.7750e-01)	Acc@1  89.84 ( 88.55)	Acc@5  99.22 ( 98.98)
Epoch: [66][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9616e-01 (3.7559e-01)	Acc@1  92.19 ( 88.63)	Acc@5 100.00 ( 98.99)
Epoch: [66][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3104e-01 (3.7645e-01)	Acc@1  87.50 ( 88.60)	Acc@5 100.00 ( 98.99)
Epoch: [66][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9988e-01 (3.7645e-01)	Acc@1  89.06 ( 88.62)	Acc@5  98.44 ( 98.97)
Epoch: [66][280/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1947e-01 (3.7525e-01)	Acc@1  90.62 ( 88.66)	Acc@5 100.00 ( 98.99)
Epoch: [66][290/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5893e-01 (3.7501e-01)	Acc@1  87.50 ( 88.66)	Acc@5 100.00 ( 98.99)
Epoch: [66][300/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3122e-01 (3.7446e-01)	Acc@1  86.72 ( 88.70)	Acc@5  97.66 ( 98.98)
Epoch: [66][310/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7219e-01 (3.7477e-01)	Acc@1  93.75 ( 88.70)	Acc@5 100.00 ( 98.98)
Epoch: [66][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5777e-01 (3.7459e-01)	Acc@1  90.62 ( 88.71)	Acc@5  98.44 ( 98.97)
Epoch: [66][330/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1163e-01 (3.7452e-01)	Acc@1  89.84 ( 88.71)	Acc@5 100.00 ( 98.97)
Epoch: [66][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9993e-01 (3.7491e-01)	Acc@1  90.62 ( 88.69)	Acc@5  98.44 ( 98.98)
Epoch: [66][350/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3938e-01 (3.7453e-01)	Acc@1  83.59 ( 88.73)	Acc@5  99.22 ( 98.98)
Epoch: [66][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4045e-01 (3.7454e-01)	Acc@1  88.28 ( 88.73)	Acc@5 100.00 ( 98.98)
Epoch: [66][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2223e-01 (3.7431e-01)	Acc@1  89.84 ( 88.75)	Acc@5 100.00 ( 98.99)
Epoch: [66][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1539e-01 (3.7373e-01)	Acc@1  87.50 ( 88.77)	Acc@5 100.00 ( 98.99)
Epoch: [66][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8912e-01 (3.7468e-01)	Acc@1  83.75 ( 88.73)	Acc@5 100.00 ( 99.00)
## e[66] optimizer.zero_grad (sum) time: 0.40123438835144043
## e[66]       loss.backward (sum) time: 6.983966827392578
## e[66]      optimizer.step (sum) time: 3.4829022884368896
## epoch[66] training(only) time: 25.57574152946472
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2966e+00 (1.2966e+00)	Acc@1  69.00 ( 69.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 1.3598e+00 (1.2890e+00)	Acc@1  64.00 ( 67.27)	Acc@5  91.00 ( 89.91)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3252e+00 (1.2248e+00)	Acc@1  67.00 ( 68.52)	Acc@5  93.00 ( 91.19)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6359e+00 (1.2595e+00)	Acc@1  60.00 ( 67.77)	Acc@5  88.00 ( 90.81)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1390e+00 (1.2394e+00)	Acc@1  71.00 ( 67.63)	Acc@5  94.00 ( 91.32)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2864e+00 (1.2609e+00)	Acc@1  70.00 ( 67.35)	Acc@5  90.00 ( 91.10)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2858e+00 (1.2494e+00)	Acc@1  62.00 ( 67.56)	Acc@5  91.00 ( 91.13)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1318e+00 (1.2577e+00)	Acc@1  66.00 ( 67.52)	Acc@5  94.00 ( 91.06)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.3964e+00 (1.2677e+00)	Acc@1  65.00 ( 67.27)	Acc@5  93.00 ( 90.91)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.8724e+00 (1.2608e+00)	Acc@1  61.00 ( 67.48)	Acc@5  86.00 ( 90.93)
 * Acc@1 67.440 Acc@5 90.930
### epoch[66] execution time: 28.447726249694824
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.223 ( 0.223)	Data  0.153 ( 0.153)	Loss 3.3801e-01 (3.3801e-01)	Acc@1  89.84 ( 89.84)	Acc@5  98.44 ( 98.44)
Epoch: [67][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.015)	Loss 4.4669e-01 (3.6528e-01)	Acc@1  85.94 ( 88.07)	Acc@5  99.22 ( 99.29)
Epoch: [67][ 20/391]	Time  0.065 ( 0.072)	Data  0.001 ( 0.008)	Loss 4.3934e-01 (3.7861e-01)	Acc@1  87.50 ( 87.61)	Acc@5  99.22 ( 99.29)
Epoch: [67][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.6377e-01 (3.7357e-01)	Acc@1  88.28 ( 88.21)	Acc@5  99.22 ( 99.24)
Epoch: [67][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 4.5271e-01 (3.7661e-01)	Acc@1  89.06 ( 88.36)	Acc@5  96.09 ( 99.09)
Epoch: [67][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.1331e-01 (3.7836e-01)	Acc@1  91.41 ( 88.45)	Acc@5 100.00 ( 99.11)
Epoch: [67][ 60/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.4019e-01 (3.7715e-01)	Acc@1  88.28 ( 88.52)	Acc@5  96.88 ( 99.05)
Epoch: [67][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.1847e-01 (3.7584e-01)	Acc@1  85.94 ( 88.59)	Acc@5  99.22 ( 99.06)
Epoch: [67][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.0896e-01 (3.7038e-01)	Acc@1  86.72 ( 88.76)	Acc@5  97.66 ( 99.11)
Epoch: [67][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.0021e-01 (3.7045e-01)	Acc@1  92.19 ( 88.89)	Acc@5 100.00 ( 99.09)
Epoch: [67][100/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.5516e-01 (3.7432e-01)	Acc@1  87.50 ( 88.74)	Acc@5  98.44 ( 99.07)
Epoch: [67][110/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.9887e-01 (3.7945e-01)	Acc@1  84.38 ( 88.48)	Acc@5  97.66 ( 98.99)
Epoch: [67][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5821e-01 (3.7658e-01)	Acc@1  87.50 ( 88.58)	Acc@5  97.66 ( 99.01)
Epoch: [67][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9290e-01 (3.7751e-01)	Acc@1  87.50 ( 88.54)	Acc@5  99.22 ( 98.99)
Epoch: [67][140/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0064e-01 (3.7960e-01)	Acc@1  88.28 ( 88.47)	Acc@5  98.44 ( 98.99)
Epoch: [67][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6322e-01 (3.7769e-01)	Acc@1  87.50 ( 88.51)	Acc@5  99.22 ( 99.01)
Epoch: [67][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5030e-01 (3.7771e-01)	Acc@1  89.06 ( 88.55)	Acc@5  98.44 ( 98.97)
Epoch: [67][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8820e-01 (3.7766e-01)	Acc@1  91.41 ( 88.61)	Acc@5 100.00 ( 98.97)
Epoch: [67][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6264e-01 (3.7640e-01)	Acc@1  85.16 ( 88.64)	Acc@5  97.66 ( 98.99)
Epoch: [67][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0500e-01 (3.7504e-01)	Acc@1  89.84 ( 88.71)	Acc@5 100.00 ( 99.01)
Epoch: [67][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5494e-01 (3.7381e-01)	Acc@1  90.62 ( 88.75)	Acc@5  98.44 ( 99.02)
Epoch: [67][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4155e-01 (3.7325e-01)	Acc@1  88.28 ( 88.76)	Acc@5  99.22 ( 99.00)
Epoch: [67][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1304e-01 (3.7256e-01)	Acc@1  87.50 ( 88.76)	Acc@5  98.44 ( 99.01)
Epoch: [67][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1639e-01 (3.7140e-01)	Acc@1  91.41 ( 88.80)	Acc@5 100.00 ( 99.03)
Epoch: [67][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0415e-01 (3.7119e-01)	Acc@1  90.62 ( 88.80)	Acc@5 100.00 ( 99.03)
Epoch: [67][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7634e-01 (3.7107e-01)	Acc@1  89.06 ( 88.81)	Acc@5  98.44 ( 99.03)
Epoch: [67][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7712e-01 (3.7137e-01)	Acc@1  89.06 ( 88.80)	Acc@5  97.66 ( 99.03)
Epoch: [67][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8308e-01 (3.7081e-01)	Acc@1  90.62 ( 88.78)	Acc@5 100.00 ( 99.03)
Epoch: [67][280/391]	Time  0.062 ( 0.066)	Data  0.002 ( 0.002)	Loss 3.2941e-01 (3.7122e-01)	Acc@1  89.06 ( 88.78)	Acc@5 100.00 ( 99.04)
Epoch: [67][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4059e-01 (3.7144e-01)	Acc@1  90.62 ( 88.76)	Acc@5  99.22 ( 99.03)
Epoch: [67][300/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1880e-01 (3.7212e-01)	Acc@1  92.97 ( 88.73)	Acc@5  98.44 ( 99.01)
Epoch: [67][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2041e-01 (3.7331e-01)	Acc@1  92.19 ( 88.69)	Acc@5  99.22 ( 99.01)
Epoch: [67][320/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3379e-01 (3.7302e-01)	Acc@1  88.28 ( 88.69)	Acc@5 100.00 ( 99.02)
Epoch: [67][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9226e-01 (3.7259e-01)	Acc@1  91.41 ( 88.70)	Acc@5 100.00 ( 99.02)
Epoch: [67][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6299e-01 (3.7245e-01)	Acc@1  89.84 ( 88.71)	Acc@5  99.22 ( 99.01)
Epoch: [67][350/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9185e-01 (3.7209e-01)	Acc@1  90.62 ( 88.74)	Acc@5  98.44 ( 99.01)
Epoch: [67][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4252e-01 (3.7234e-01)	Acc@1  87.50 ( 88.73)	Acc@5 100.00 ( 99.00)
Epoch: [67][370/391]	Time  0.066 ( 0.066)	Data  0.002 ( 0.002)	Loss 3.2716e-01 (3.7116e-01)	Acc@1  89.84 ( 88.77)	Acc@5  99.22 ( 99.02)
Epoch: [67][380/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1774e-01 (3.7134e-01)	Acc@1  92.97 ( 88.78)	Acc@5 100.00 ( 99.03)
Epoch: [67][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5943e-01 (3.7201e-01)	Acc@1  83.75 ( 88.77)	Acc@5  98.75 ( 99.02)
## e[67] optimizer.zero_grad (sum) time: 0.4044945240020752
## e[67]       loss.backward (sum) time: 7.020619869232178
## e[67]      optimizer.step (sum) time: 3.4993600845336914
## epoch[67] training(only) time: 25.68296480178833
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2595e+00 (1.2595e+00)	Acc@1  66.00 ( 66.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.3446e+00 (1.3012e+00)	Acc@1  64.00 ( 67.18)	Acc@5  90.00 ( 89.55)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3748e+00 (1.2412e+00)	Acc@1  67.00 ( 68.48)	Acc@5  92.00 ( 90.86)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6043e+00 (1.2703e+00)	Acc@1  61.00 ( 67.74)	Acc@5  87.00 ( 90.48)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1422e+00 (1.2509e+00)	Acc@1  73.00 ( 67.66)	Acc@5  95.00 ( 91.12)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.3110e+00 (1.2701e+00)	Acc@1  73.00 ( 67.33)	Acc@5  89.00 ( 91.00)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.2885e+00 (1.2581e+00)	Acc@1  61.00 ( 67.51)	Acc@5  91.00 ( 91.08)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.1479e+00 (1.2662e+00)	Acc@1  69.00 ( 67.55)	Acc@5  94.00 ( 91.01)
Test: [ 80/100]	Time  0.029 ( 0.028)	Loss 1.4255e+00 (1.2748e+00)	Acc@1  66.00 ( 67.43)	Acc@5  90.00 ( 90.81)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.9441e+00 (1.2685e+00)	Acc@1  59.00 ( 67.59)	Acc@5  84.00 ( 90.82)
 * Acc@1 67.510 Acc@5 90.780
### epoch[67] execution time: 28.52921199798584
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.228 ( 0.228)	Data  0.156 ( 0.156)	Loss 2.5955e-01 (2.5955e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.065 ( 0.081)	Data  0.001 ( 0.015)	Loss 4.1631e-01 (3.5606e-01)	Acc@1  85.94 ( 88.78)	Acc@5 100.00 ( 99.22)
Epoch: [68][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.009)	Loss 4.0491e-01 (3.5862e-01)	Acc@1  88.28 ( 88.58)	Acc@5  98.44 ( 99.22)
Epoch: [68][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.3064e-01 (3.6520e-01)	Acc@1  85.16 ( 88.48)	Acc@5  99.22 ( 99.24)
Epoch: [68][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.5788e-01 (3.6486e-01)	Acc@1  90.62 ( 88.45)	Acc@5 100.00 ( 99.20)
Epoch: [68][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.3406e-01 (3.7179e-01)	Acc@1  85.16 ( 88.28)	Acc@5  99.22 ( 99.20)
Epoch: [68][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.1806e-01 (3.6566e-01)	Acc@1  91.41 ( 88.68)	Acc@5  99.22 ( 99.22)
Epoch: [68][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.3133e-01 (3.7201e-01)	Acc@1  86.72 ( 88.59)	Acc@5  98.44 ( 99.21)
Epoch: [68][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.8761e-01 (3.7215e-01)	Acc@1  89.06 ( 88.66)	Acc@5  98.44 ( 99.20)
Epoch: [68][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.1719e-01 (3.7137e-01)	Acc@1  87.50 ( 88.68)	Acc@5  97.66 ( 99.22)
Epoch: [68][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.9678e-01 (3.7249e-01)	Acc@1  87.50 ( 88.67)	Acc@5 100.00 ( 99.20)
Epoch: [68][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0997e-01 (3.7480e-01)	Acc@1  86.72 ( 88.57)	Acc@5  98.44 ( 99.18)
Epoch: [68][120/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2881e-01 (3.7294e-01)	Acc@1  89.06 ( 88.65)	Acc@5  99.22 ( 99.15)
Epoch: [68][130/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0062e-01 (3.7168e-01)	Acc@1  93.75 ( 88.72)	Acc@5 100.00 ( 99.18)
Epoch: [68][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8041e-01 (3.7551e-01)	Acc@1  92.97 ( 88.64)	Acc@5 100.00 ( 99.15)
Epoch: [68][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5754e-01 (3.7537e-01)	Acc@1  89.84 ( 88.63)	Acc@5  99.22 ( 99.14)
Epoch: [68][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5376e-01 (3.7496e-01)	Acc@1  89.84 ( 88.65)	Acc@5  98.44 ( 99.14)
Epoch: [68][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5280e-01 (3.7347e-01)	Acc@1  89.84 ( 88.68)	Acc@5  99.22 ( 99.10)
Epoch: [68][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5353e-01 (3.7165e-01)	Acc@1  92.19 ( 88.75)	Acc@5 100.00 ( 99.11)
Epoch: [68][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9027e-01 (3.7073e-01)	Acc@1  88.28 ( 88.81)	Acc@5 100.00 ( 99.11)
Epoch: [68][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8906e-01 (3.7081e-01)	Acc@1  88.28 ( 88.83)	Acc@5  99.22 ( 99.10)
Epoch: [68][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0070e-01 (3.7248e-01)	Acc@1  84.38 ( 88.78)	Acc@5  99.22 ( 99.10)
Epoch: [68][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9525e-01 (3.7309e-01)	Acc@1  90.62 ( 88.77)	Acc@5  98.44 ( 99.07)
Epoch: [68][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3613e-01 (3.7251e-01)	Acc@1  88.28 ( 88.76)	Acc@5 100.00 ( 99.07)
Epoch: [68][240/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2863e-01 (3.7241e-01)	Acc@1  89.06 ( 88.77)	Acc@5  96.88 ( 99.08)
Epoch: [68][250/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9833e-01 (3.7043e-01)	Acc@1  88.28 ( 88.80)	Acc@5  99.22 ( 99.10)
Epoch: [68][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8139e-01 (3.7075e-01)	Acc@1  87.50 ( 88.81)	Acc@5 100.00 ( 99.10)
Epoch: [68][270/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9235e-01 (3.6976e-01)	Acc@1  89.84 ( 88.86)	Acc@5  99.22 ( 99.10)
Epoch: [68][280/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9663e-01 (3.7094e-01)	Acc@1  88.28 ( 88.82)	Acc@5  98.44 ( 99.10)
Epoch: [68][290/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9550e-01 (3.7232e-01)	Acc@1  82.03 ( 88.77)	Acc@5  98.44 ( 99.08)
Epoch: [68][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6261e-01 (3.7152e-01)	Acc@1  90.62 ( 88.79)	Acc@5 100.00 ( 99.10)
Epoch: [68][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5184e-01 (3.7098e-01)	Acc@1  91.41 ( 88.78)	Acc@5 100.00 ( 99.10)
Epoch: [68][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.6124e-01 (3.7123e-01)	Acc@1  87.50 ( 88.79)	Acc@5  96.88 ( 99.09)
Epoch: [68][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8118e-01 (3.7137e-01)	Acc@1  84.38 ( 88.79)	Acc@5 100.00 ( 99.10)
Epoch: [68][340/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9036e-01 (3.7100e-01)	Acc@1  84.38 ( 88.78)	Acc@5  96.88 ( 99.10)
Epoch: [68][350/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3971e-01 (3.7057e-01)	Acc@1  88.28 ( 88.77)	Acc@5 100.00 ( 99.10)
Epoch: [68][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6167e-01 (3.7063e-01)	Acc@1  89.84 ( 88.75)	Acc@5  99.22 ( 99.10)
Epoch: [68][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2486e-01 (3.7097e-01)	Acc@1  91.41 ( 88.75)	Acc@5  99.22 ( 99.10)
Epoch: [68][380/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2407e-01 (3.7123e-01)	Acc@1  87.50 ( 88.75)	Acc@5  96.88 ( 99.08)
Epoch: [68][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4422e-01 (3.7169e-01)	Acc@1  86.25 ( 88.75)	Acc@5 100.00 ( 99.07)
## e[68] optimizer.zero_grad (sum) time: 0.40207719802856445
## e[68]       loss.backward (sum) time: 6.960430383682251
## e[68]      optimizer.step (sum) time: 3.4928343296051025
## epoch[68] training(only) time: 25.565653085708618
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2711e+00 (1.2711e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.029 ( 0.039)	Loss 1.3305e+00 (1.2872e+00)	Acc@1  63.00 ( 67.36)	Acc@5  91.00 ( 89.73)
Test: [ 20/100]	Time  0.029 ( 0.033)	Loss 1.3247e+00 (1.2281e+00)	Acc@1  66.00 ( 68.57)	Acc@5  93.00 ( 91.14)
Test: [ 30/100]	Time  0.033 ( 0.031)	Loss 1.6170e+00 (1.2622e+00)	Acc@1  61.00 ( 67.77)	Acc@5  88.00 ( 90.61)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1556e+00 (1.2413e+00)	Acc@1  70.00 ( 67.78)	Acc@5  95.00 ( 91.24)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2887e+00 (1.2618e+00)	Acc@1  72.00 ( 67.39)	Acc@5  90.00 ( 91.04)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2857e+00 (1.2518e+00)	Acc@1  61.00 ( 67.61)	Acc@5  91.00 ( 91.13)
Test: [ 70/100]	Time  0.029 ( 0.028)	Loss 1.1170e+00 (1.2585e+00)	Acc@1  68.00 ( 67.70)	Acc@5  95.00 ( 91.08)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3836e+00 (1.2679e+00)	Acc@1  65.00 ( 67.53)	Acc@5  91.00 ( 90.90)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.8881e+00 (1.2621e+00)	Acc@1  61.00 ( 67.74)	Acc@5  84.00 ( 90.90)
 * Acc@1 67.710 Acc@5 90.860
### epoch[68] execution time: 28.38759183883667
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.222 ( 0.222)	Data  0.150 ( 0.150)	Loss 3.6576e-01 (3.6576e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [69][ 10/391]	Time  0.061 ( 0.080)	Data  0.001 ( 0.015)	Loss 3.5863e-01 (3.4924e-01)	Acc@1  92.19 ( 90.20)	Acc@5  97.66 ( 99.29)
Epoch: [69][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.008)	Loss 3.1952e-01 (3.6563e-01)	Acc@1  91.41 ( 89.58)	Acc@5 100.00 ( 99.11)
Epoch: [69][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.8205e-01 (3.5293e-01)	Acc@1  91.41 ( 89.77)	Acc@5  99.22 ( 99.19)
Epoch: [69][ 40/391]	Time  0.068 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.9998e-01 (3.5355e-01)	Acc@1  89.06 ( 89.73)	Acc@5  98.44 ( 99.14)
Epoch: [69][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.1105e-01 (3.6120e-01)	Acc@1  89.84 ( 89.35)	Acc@5  97.66 ( 99.05)
Epoch: [69][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.7034e-01 (3.6243e-01)	Acc@1  87.50 ( 89.04)	Acc@5  99.22 ( 99.12)
Epoch: [69][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.2839e-01 (3.6926e-01)	Acc@1  82.03 ( 88.74)	Acc@5 100.00 ( 99.05)
Epoch: [69][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5745e-01 (3.6983e-01)	Acc@1  88.28 ( 88.59)	Acc@5  99.22 ( 99.05)
Epoch: [69][ 90/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9690e-01 (3.6584e-01)	Acc@1  88.28 ( 88.74)	Acc@5  98.44 ( 99.08)
Epoch: [69][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.6655e-01 (3.6629e-01)	Acc@1  84.38 ( 88.76)	Acc@5  98.44 ( 99.09)
Epoch: [69][110/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.1335e-01 (3.6333e-01)	Acc@1  91.41 ( 88.94)	Acc@5 100.00 ( 99.08)
Epoch: [69][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5246e-01 (3.6151e-01)	Acc@1  86.72 ( 88.99)	Acc@5  99.22 ( 99.09)
Epoch: [69][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6171e-01 (3.6057e-01)	Acc@1  88.28 ( 89.02)	Acc@5  99.22 ( 99.12)
Epoch: [69][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2456e-01 (3.6358e-01)	Acc@1  86.72 ( 88.97)	Acc@5  98.44 ( 99.07)
Epoch: [69][150/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9891e-01 (3.6380e-01)	Acc@1  89.84 ( 89.04)	Acc@5 100.00 ( 99.05)
Epoch: [69][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8533e-01 (3.6256e-01)	Acc@1  90.62 ( 89.03)	Acc@5 100.00 ( 99.08)
Epoch: [69][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1427e-01 (3.6395e-01)	Acc@1  82.03 ( 88.93)	Acc@5  98.44 ( 99.08)
Epoch: [69][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9722e-01 (3.6415e-01)	Acc@1  93.75 ( 88.90)	Acc@5  99.22 ( 99.05)
Epoch: [69][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8961e-01 (3.6494e-01)	Acc@1  89.84 ( 88.88)	Acc@5  98.44 ( 99.06)
Epoch: [69][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8076e-01 (3.6575e-01)	Acc@1  89.06 ( 88.86)	Acc@5  99.22 ( 99.07)
Epoch: [69][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9065e-01 (3.6524e-01)	Acc@1  85.94 ( 88.87)	Acc@5  99.22 ( 99.07)
Epoch: [69][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5947e-01 (3.6566e-01)	Acc@1  85.16 ( 88.87)	Acc@5  99.22 ( 99.06)
Epoch: [69][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2655e-01 (3.6529e-01)	Acc@1  91.41 ( 88.87)	Acc@5 100.00 ( 99.07)
Epoch: [69][240/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5414e-01 (3.6611e-01)	Acc@1  89.06 ( 88.86)	Acc@5 100.00 ( 99.07)
Epoch: [69][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2379e-01 (3.6635e-01)	Acc@1  87.50 ( 88.83)	Acc@5  98.44 ( 99.07)
Epoch: [69][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7983e-01 (3.6647e-01)	Acc@1  88.28 ( 88.83)	Acc@5  98.44 ( 99.07)
Epoch: [69][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7631e-01 (3.6677e-01)	Acc@1  90.62 ( 88.81)	Acc@5  99.22 ( 99.07)
Epoch: [69][280/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6755e-01 (3.6773e-01)	Acc@1  83.59 ( 88.76)	Acc@5  98.44 ( 99.06)
Epoch: [69][290/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3817e-01 (3.6726e-01)	Acc@1  92.97 ( 88.83)	Acc@5 100.00 ( 99.05)
Epoch: [69][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4038e-01 (3.6659e-01)	Acc@1  90.62 ( 88.87)	Acc@5  99.22 ( 99.06)
Epoch: [69][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6509e-01 (3.6590e-01)	Acc@1  92.19 ( 88.88)	Acc@5 100.00 ( 99.08)
Epoch: [69][320/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1369e-01 (3.6610e-01)	Acc@1  89.06 ( 88.89)	Acc@5  97.66 ( 99.07)
Epoch: [69][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4114e-01 (3.6629e-01)	Acc@1  85.94 ( 88.89)	Acc@5  98.44 ( 99.07)
Epoch: [69][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4361e-01 (3.6622e-01)	Acc@1  84.38 ( 88.90)	Acc@5  99.22 ( 99.07)
Epoch: [69][350/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4349e-01 (3.6688e-01)	Acc@1  78.91 ( 88.86)	Acc@5  98.44 ( 99.07)
Epoch: [69][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8177e-01 (3.6751e-01)	Acc@1  89.06 ( 88.83)	Acc@5  99.22 ( 99.07)
Epoch: [69][370/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9233e-01 (3.6670e-01)	Acc@1  86.72 ( 88.85)	Acc@5  99.22 ( 99.07)
Epoch: [69][380/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5632e-01 (3.6694e-01)	Acc@1  89.06 ( 88.85)	Acc@5  97.66 ( 99.06)
Epoch: [69][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1508e-01 (3.6590e-01)	Acc@1  91.25 ( 88.90)	Acc@5  98.75 ( 99.07)
## e[69] optimizer.zero_grad (sum) time: 0.4108457565307617
## e[69]       loss.backward (sum) time: 6.994890928268433
## e[69]      optimizer.step (sum) time: 3.4829866886138916
## epoch[69] training(only) time: 25.671414136886597
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.2954e+00 (1.2954e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 1.3167e+00 (1.2962e+00)	Acc@1  66.00 ( 67.55)	Acc@5  91.00 ( 90.09)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.2595e+00 (1.2341e+00)	Acc@1  69.00 ( 68.52)	Acc@5  93.00 ( 91.05)
Test: [ 30/100]	Time  0.028 ( 0.032)	Loss 1.6490e+00 (1.2676e+00)	Acc@1  59.00 ( 68.03)	Acc@5  85.00 ( 90.52)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1356e+00 (1.2474e+00)	Acc@1  70.00 ( 67.80)	Acc@5  95.00 ( 91.15)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2637e+00 (1.2662e+00)	Acc@1  70.00 ( 67.59)	Acc@5  90.00 ( 90.96)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2942e+00 (1.2562e+00)	Acc@1  61.00 ( 67.77)	Acc@5  92.00 ( 91.00)
Test: [ 70/100]	Time  0.027 ( 0.029)	Loss 1.1722e+00 (1.2643e+00)	Acc@1  69.00 ( 67.83)	Acc@5  94.00 ( 90.97)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.4098e+00 (1.2744e+00)	Acc@1  65.00 ( 67.64)	Acc@5  92.00 ( 90.81)
Test: [ 90/100]	Time  0.029 ( 0.028)	Loss 1.9265e+00 (1.2686e+00)	Acc@1  59.00 ( 67.85)	Acc@5  86.00 ( 90.81)
 * Acc@1 67.750 Acc@5 90.800
### epoch[69] execution time: 28.559175729751587
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.222 ( 0.222)	Data  0.153 ( 0.153)	Loss 3.6182e-01 (3.6182e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Epoch: [70][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.015)	Loss 3.4564e-01 (3.6422e-01)	Acc@1  91.41 ( 88.42)	Acc@5  99.22 ( 98.93)
Epoch: [70][ 20/391]	Time  0.065 ( 0.073)	Data  0.001 ( 0.008)	Loss 4.0081e-01 (3.5198e-01)	Acc@1  89.06 ( 89.21)	Acc@5  97.66 ( 99.07)
Epoch: [70][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.3282e-01 (3.3923e-01)	Acc@1  96.09 ( 89.62)	Acc@5 100.00 ( 99.14)
Epoch: [70][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.2284e-01 (3.4139e-01)	Acc@1  89.84 ( 89.50)	Acc@5 100.00 ( 99.16)
Epoch: [70][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.2792e-01 (3.4680e-01)	Acc@1  92.19 ( 89.32)	Acc@5 100.00 ( 99.13)
Epoch: [70][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.3496e-01 (3.4825e-01)	Acc@1  89.84 ( 89.40)	Acc@5 100.00 ( 99.15)
Epoch: [70][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.8941e-01 (3.5020e-01)	Acc@1  85.16 ( 89.34)	Acc@5  99.22 ( 99.17)
Epoch: [70][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.4568e-01 (3.5396e-01)	Acc@1  88.28 ( 89.24)	Acc@5 100.00 ( 99.15)
Epoch: [70][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.8090e-01 (3.5636e-01)	Acc@1  86.72 ( 89.16)	Acc@5  99.22 ( 99.15)
Epoch: [70][100/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.5911e-01 (3.5920e-01)	Acc@1  89.06 ( 88.95)	Acc@5  99.22 ( 99.15)
Epoch: [70][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.4584e-01 (3.6291e-01)	Acc@1  85.16 ( 88.82)	Acc@5  99.22 ( 99.13)
Epoch: [70][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7671e-01 (3.6164e-01)	Acc@1  89.06 ( 88.91)	Acc@5  99.22 ( 99.15)
Epoch: [70][130/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3526e-01 (3.6056e-01)	Acc@1  85.94 ( 89.00)	Acc@5  98.44 ( 99.14)
Epoch: [70][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0554e-01 (3.6115e-01)	Acc@1  84.38 ( 89.05)	Acc@5  97.66 ( 99.11)
Epoch: [70][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5103e-01 (3.6161e-01)	Acc@1  89.06 ( 89.08)	Acc@5  99.22 ( 99.09)
Epoch: [70][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5368e-01 (3.6146e-01)	Acc@1  85.94 ( 89.12)	Acc@5 100.00 ( 99.11)
Epoch: [70][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9858e-01 (3.6257e-01)	Acc@1  89.84 ( 89.16)	Acc@5  99.22 ( 99.09)
Epoch: [70][180/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.1592e-01 (3.6279e-01)	Acc@1  83.59 ( 89.14)	Acc@5  99.22 ( 99.10)
Epoch: [70][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9666e-01 (3.6254e-01)	Acc@1  89.84 ( 89.19)	Acc@5  99.22 ( 99.11)
Epoch: [70][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7259e-01 (3.6250e-01)	Acc@1  87.50 ( 89.21)	Acc@5 100.00 ( 99.13)
Epoch: [70][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3344e-01 (3.6315e-01)	Acc@1  91.41 ( 89.22)	Acc@5  97.66 ( 99.12)
Epoch: [70][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8452e-01 (3.6303e-01)	Acc@1  90.62 ( 89.20)	Acc@5 100.00 ( 99.13)
Epoch: [70][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2089e-01 (3.6180e-01)	Acc@1  89.06 ( 89.22)	Acc@5  99.22 ( 99.14)
Epoch: [70][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9046e-01 (3.6188e-01)	Acc@1  88.28 ( 89.16)	Acc@5  99.22 ( 99.15)
Epoch: [70][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9972e-01 (3.6214e-01)	Acc@1  85.94 ( 89.14)	Acc@5  99.22 ( 99.17)
Epoch: [70][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1190e-01 (3.6273e-01)	Acc@1  89.84 ( 89.10)	Acc@5  99.22 ( 99.18)
Epoch: [70][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6346e-01 (3.6106e-01)	Acc@1  87.50 ( 89.18)	Acc@5  98.44 ( 99.17)
Epoch: [70][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5996e-01 (3.6169e-01)	Acc@1  89.84 ( 89.15)	Acc@5 100.00 ( 99.17)
Epoch: [70][290/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2445e-01 (3.6245e-01)	Acc@1  84.38 ( 89.11)	Acc@5  97.66 ( 99.17)
Epoch: [70][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1064e-01 (3.6353e-01)	Acc@1  90.62 ( 89.05)	Acc@5  99.22 ( 99.16)
Epoch: [70][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1134e-01 (3.6402e-01)	Acc@1  89.06 ( 89.02)	Acc@5  98.44 ( 99.16)
Epoch: [70][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3031e-01 (3.6363e-01)	Acc@1  89.84 ( 89.05)	Acc@5  98.44 ( 99.16)
Epoch: [70][330/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5562e-01 (3.6270e-01)	Acc@1  92.19 ( 89.06)	Acc@5  99.22 ( 99.17)
Epoch: [70][340/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6533e-01 (3.6329e-01)	Acc@1  86.72 ( 89.00)	Acc@5  99.22 ( 99.17)
Epoch: [70][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0674e-01 (3.6359e-01)	Acc@1  89.06 ( 88.98)	Acc@5 100.00 ( 99.17)
Epoch: [70][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4951e-01 (3.6324e-01)	Acc@1  85.16 ( 88.98)	Acc@5  99.22 ( 99.18)
Epoch: [70][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3460e-01 (3.6275e-01)	Acc@1  91.41 ( 89.00)	Acc@5 100.00 ( 99.19)
Epoch: [70][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1390e-01 (3.6290e-01)	Acc@1  86.72 ( 88.98)	Acc@5 100.00 ( 99.18)
Epoch: [70][390/391]	Time  0.060 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4502e-01 (3.6348e-01)	Acc@1  91.25 ( 88.98)	Acc@5 100.00 ( 99.18)
## e[70] optimizer.zero_grad (sum) time: 0.411210298538208
## e[70]       loss.backward (sum) time: 6.987787246704102
## e[70]      optimizer.step (sum) time: 3.438873529434204
## epoch[70] training(only) time: 25.558547973632812
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.3215e+00 (1.3215e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.3846e+00 (1.3029e+00)	Acc@1  65.00 ( 67.00)	Acc@5  91.00 ( 89.82)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3441e+00 (1.2468e+00)	Acc@1  70.00 ( 68.05)	Acc@5  91.00 ( 90.90)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6767e+00 (1.2790e+00)	Acc@1  59.00 ( 67.52)	Acc@5  86.00 ( 90.39)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1859e+00 (1.2609e+00)	Acc@1  68.00 ( 67.29)	Acc@5  93.00 ( 90.93)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.2918e+00 (1.2779e+00)	Acc@1  70.00 ( 67.02)	Acc@5  89.00 ( 90.75)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2926e+00 (1.2662e+00)	Acc@1  61.00 ( 67.26)	Acc@5  92.00 ( 90.79)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1517e+00 (1.2739e+00)	Acc@1  69.00 ( 67.28)	Acc@5  94.00 ( 90.77)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4099e+00 (1.2835e+00)	Acc@1  65.00 ( 67.17)	Acc@5  92.00 ( 90.60)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.9041e+00 (1.2772e+00)	Acc@1  59.00 ( 67.40)	Acc@5  84.00 ( 90.59)
 * Acc@1 67.370 Acc@5 90.580
### epoch[70] execution time: 28.387782096862793
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.219 ( 0.219)	Data  0.147 ( 0.147)	Loss 3.9937e-01 (3.9937e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [71][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 2.5073e-01 (3.3438e-01)	Acc@1  93.75 ( 90.84)	Acc@5 100.00 ( 99.50)
Epoch: [71][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.008)	Loss 2.9823e-01 (3.5225e-01)	Acc@1  89.84 ( 89.96)	Acc@5  98.44 ( 99.29)
Epoch: [71][ 30/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.006)	Loss 3.3466e-01 (3.4798e-01)	Acc@1  87.50 ( 89.89)	Acc@5 100.00 ( 99.32)
Epoch: [71][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.5944e-01 (3.6021e-01)	Acc@1  89.84 ( 89.65)	Acc@5 100.00 ( 99.12)
Epoch: [71][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.4492e-01 (3.5316e-01)	Acc@1  91.41 ( 89.72)	Acc@5 100.00 ( 99.17)
Epoch: [71][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.8213e-01 (3.5479e-01)	Acc@1  93.75 ( 89.68)	Acc@5  97.66 ( 99.09)
Epoch: [71][ 70/391]	Time  0.062 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.4728e-01 (3.5613e-01)	Acc@1  90.62 ( 89.56)	Acc@5 100.00 ( 99.09)
Epoch: [71][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.4366e-01 (3.5374e-01)	Acc@1  91.41 ( 89.62)	Acc@5  98.44 ( 99.09)
Epoch: [71][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.2391e-01 (3.5537e-01)	Acc@1  85.94 ( 89.54)	Acc@5 100.00 ( 99.15)
Epoch: [71][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.5527e-01 (3.5546e-01)	Acc@1  92.97 ( 89.62)	Acc@5 100.00 ( 99.13)
Epoch: [71][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0455e-01 (3.5385e-01)	Acc@1  89.84 ( 89.59)	Acc@5  98.44 ( 99.13)
Epoch: [71][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7818e-01 (3.5283e-01)	Acc@1  91.41 ( 89.61)	Acc@5 100.00 ( 99.14)
Epoch: [71][130/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2365e-01 (3.5282e-01)	Acc@1  86.72 ( 89.66)	Acc@5  98.44 ( 99.13)
Epoch: [71][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9901e-01 (3.5278e-01)	Acc@1  89.06 ( 89.69)	Acc@5  96.88 ( 99.10)
Epoch: [71][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8275e-01 (3.5225e-01)	Acc@1  89.06 ( 89.72)	Acc@5  98.44 ( 99.10)
Epoch: [71][160/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1703e-01 (3.5165e-01)	Acc@1  86.72 ( 89.76)	Acc@5  98.44 ( 99.10)
Epoch: [71][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5609e-01 (3.5091e-01)	Acc@1  86.72 ( 89.72)	Acc@5  99.22 ( 99.10)
Epoch: [71][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8473e-01 (3.5110e-01)	Acc@1  87.50 ( 89.74)	Acc@5  98.44 ( 99.10)
Epoch: [71][190/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7481e-01 (3.5206e-01)	Acc@1  88.28 ( 89.69)	Acc@5  98.44 ( 99.12)
Epoch: [71][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0748e-01 (3.5069e-01)	Acc@1  93.75 ( 89.76)	Acc@5 100.00 ( 99.12)
Epoch: [71][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6864e-01 (3.5000e-01)	Acc@1  91.41 ( 89.78)	Acc@5 100.00 ( 99.13)
Epoch: [71][220/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4316e-01 (3.5019e-01)	Acc@1  86.72 ( 89.75)	Acc@5  99.22 ( 99.14)
Epoch: [71][230/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3155e-01 (3.5106e-01)	Acc@1  88.28 ( 89.74)	Acc@5  99.22 ( 99.15)
Epoch: [71][240/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7638e-01 (3.5058e-01)	Acc@1  92.19 ( 89.75)	Acc@5  99.22 ( 99.16)
Epoch: [71][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5041e-01 (3.5112e-01)	Acc@1  91.41 ( 89.72)	Acc@5  97.66 ( 99.15)
Epoch: [71][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9025e-01 (3.5266e-01)	Acc@1  86.72 ( 89.65)	Acc@5  98.44 ( 99.14)
Epoch: [71][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7549e-01 (3.5402e-01)	Acc@1  85.94 ( 89.60)	Acc@5  99.22 ( 99.15)
Epoch: [71][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0359e-01 (3.5358e-01)	Acc@1  89.84 ( 89.64)	Acc@5  98.44 ( 99.14)
Epoch: [71][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8485e-01 (3.5416e-01)	Acc@1  92.19 ( 89.64)	Acc@5  99.22 ( 99.14)
Epoch: [71][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3293e-01 (3.5374e-01)	Acc@1  89.06 ( 89.61)	Acc@5  99.22 ( 99.14)
Epoch: [71][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7676e-01 (3.5387e-01)	Acc@1  89.06 ( 89.62)	Acc@5  99.22 ( 99.13)
Epoch: [71][320/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5022e-01 (3.5391e-01)	Acc@1  88.28 ( 89.62)	Acc@5  99.22 ( 99.13)
Epoch: [71][330/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4970e-01 (3.5386e-01)	Acc@1  85.94 ( 89.60)	Acc@5  99.22 ( 99.13)
Epoch: [71][340/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0469e-01 (3.5385e-01)	Acc@1  89.84 ( 89.58)	Acc@5 100.00 ( 99.15)
Epoch: [71][350/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9921e-01 (3.5350e-01)	Acc@1  89.84 ( 89.56)	Acc@5 100.00 ( 99.15)
Epoch: [71][360/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6814e-01 (3.5382e-01)	Acc@1  85.94 ( 89.55)	Acc@5  96.88 ( 99.14)
Epoch: [71][370/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8815e-01 (3.5520e-01)	Acc@1  85.94 ( 89.48)	Acc@5  97.66 ( 99.13)
Epoch: [71][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2295e-01 (3.5482e-01)	Acc@1  85.16 ( 89.47)	Acc@5  98.44 ( 99.13)
Epoch: [71][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9594e-01 (3.5547e-01)	Acc@1  80.00 ( 89.44)	Acc@5  98.75 ( 99.13)
## e[71] optimizer.zero_grad (sum) time: 0.41748905181884766
## e[71]       loss.backward (sum) time: 6.99673056602478
## e[71]      optimizer.step (sum) time: 3.4790616035461426
## epoch[71] training(only) time: 25.636466026306152
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.3144e+00 (1.3144e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.025 ( 0.038)	Loss 1.3085e+00 (1.2929e+00)	Acc@1  65.00 ( 67.18)	Acc@5  91.00 ( 89.91)
Test: [ 20/100]	Time  0.027 ( 0.033)	Loss 1.3486e+00 (1.2393e+00)	Acc@1  68.00 ( 68.10)	Acc@5  92.00 ( 90.81)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6599e+00 (1.2753e+00)	Acc@1  57.00 ( 67.45)	Acc@5  85.00 ( 90.39)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1516e+00 (1.2571e+00)	Acc@1  71.00 ( 67.37)	Acc@5  93.00 ( 91.02)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2669e+00 (1.2739e+00)	Acc@1  71.00 ( 67.25)	Acc@5  90.00 ( 90.86)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3045e+00 (1.2614e+00)	Acc@1  63.00 ( 67.43)	Acc@5  92.00 ( 90.98)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.1474e+00 (1.2666e+00)	Acc@1  68.00 ( 67.51)	Acc@5  93.00 ( 90.93)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.3818e+00 (1.2751e+00)	Acc@1  65.00 ( 67.37)	Acc@5  91.00 ( 90.72)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9507e+00 (1.2686e+00)	Acc@1  59.00 ( 67.62)	Acc@5  84.00 ( 90.77)
 * Acc@1 67.560 Acc@5 90.780
### epoch[71] execution time: 28.48248791694641
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.213 ( 0.213)	Data  0.137 ( 0.137)	Loss 3.2996e-01 (3.2996e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.070 ( 0.079)	Data  0.001 ( 0.013)	Loss 3.5282e-01 (3.3831e-01)	Acc@1  87.50 ( 90.20)	Acc@5 100.00 ( 99.08)
Epoch: [72][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 2.1277e-01 (3.2300e-01)	Acc@1  94.53 ( 90.62)	Acc@5  99.22 ( 99.33)
Epoch: [72][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.6166e-01 (3.2790e-01)	Acc@1  85.94 ( 90.42)	Acc@5  99.22 ( 99.34)
Epoch: [72][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.004)	Loss 3.0465e-01 (3.3570e-01)	Acc@1  90.62 ( 90.34)	Acc@5 100.00 ( 99.28)
Epoch: [72][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.4857e-01 (3.3472e-01)	Acc@1  93.75 ( 90.33)	Acc@5 100.00 ( 99.26)
Epoch: [72][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.3146e-01 (3.3528e-01)	Acc@1  92.97 ( 90.23)	Acc@5  99.22 ( 99.30)
Epoch: [72][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6609e-01 (3.2875e-01)	Acc@1  91.41 ( 90.44)	Acc@5  99.22 ( 99.31)
Epoch: [72][ 80/391]	Time  0.060 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.4111e-01 (3.3384e-01)	Acc@1  87.50 ( 90.26)	Acc@5  99.22 ( 99.25)
Epoch: [72][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 5.6725e-01 (3.3965e-01)	Acc@1  80.47 ( 90.07)	Acc@5  97.66 ( 99.18)
Epoch: [72][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5738e-01 (3.4169e-01)	Acc@1  91.41 ( 90.06)	Acc@5  99.22 ( 99.16)
Epoch: [72][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.8842e-01 (3.4256e-01)	Acc@1  92.19 ( 89.94)	Acc@5  99.22 ( 99.18)
Epoch: [72][120/391]	Time  0.071 ( 0.066)	Data  0.002 ( 0.002)	Loss 3.2136e-01 (3.4058e-01)	Acc@1  89.06 ( 89.96)	Acc@5  99.22 ( 99.19)
Epoch: [72][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4770e-01 (3.4163e-01)	Acc@1  93.75 ( 89.98)	Acc@5 100.00 ( 99.18)
Epoch: [72][140/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8383e-01 (3.4326e-01)	Acc@1  89.06 ( 89.91)	Acc@5  99.22 ( 99.19)
Epoch: [72][150/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.4282e-01 (3.4455e-01)	Acc@1  78.91 ( 89.79)	Acc@5  99.22 ( 99.21)
Epoch: [72][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9633e-01 (3.4692e-01)	Acc@1  85.94 ( 89.69)	Acc@5  98.44 ( 99.17)
Epoch: [72][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8366e-01 (3.4784e-01)	Acc@1  86.72 ( 89.58)	Acc@5  99.22 ( 99.18)
Epoch: [72][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9712e-01 (3.4574e-01)	Acc@1  96.09 ( 89.69)	Acc@5 100.00 ( 99.19)
Epoch: [72][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7463e-01 (3.4559e-01)	Acc@1  92.97 ( 89.69)	Acc@5 100.00 ( 99.19)
Epoch: [72][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4925e-01 (3.4639e-01)	Acc@1  91.41 ( 89.63)	Acc@5 100.00 ( 99.18)
Epoch: [72][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9302e-01 (3.4728e-01)	Acc@1  91.41 ( 89.65)	Acc@5  99.22 ( 99.16)
Epoch: [72][220/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5648e-01 (3.4720e-01)	Acc@1  89.84 ( 89.65)	Acc@5  98.44 ( 99.16)
Epoch: [72][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4878e-01 (3.4685e-01)	Acc@1  89.06 ( 89.67)	Acc@5 100.00 ( 99.16)
Epoch: [72][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6976e-01 (3.4886e-01)	Acc@1  90.62 ( 89.60)	Acc@5  98.44 ( 99.12)
Epoch: [72][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3778e-01 (3.4935e-01)	Acc@1  91.41 ( 89.58)	Acc@5  99.22 ( 99.13)
Epoch: [72][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.5704e-01 (3.5016e-01)	Acc@1  83.59 ( 89.56)	Acc@5  99.22 ( 99.13)
Epoch: [72][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8291e-01 (3.5108e-01)	Acc@1  89.06 ( 89.54)	Acc@5  98.44 ( 99.12)
Epoch: [72][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7126e-01 (3.5132e-01)	Acc@1  92.97 ( 89.54)	Acc@5  99.22 ( 99.12)
Epoch: [72][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8908e-01 (3.5312e-01)	Acc@1  88.28 ( 89.46)	Acc@5  98.44 ( 99.12)
Epoch: [72][300/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3540e-01 (3.5343e-01)	Acc@1  91.41 ( 89.44)	Acc@5  97.66 ( 99.12)
Epoch: [72][310/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3806e-01 (3.5450e-01)	Acc@1  89.06 ( 89.41)	Acc@5 100.00 ( 99.12)
Epoch: [72][320/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8878e-01 (3.5462e-01)	Acc@1  85.94 ( 89.37)	Acc@5 100.00 ( 99.13)
Epoch: [72][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8611e-01 (3.5534e-01)	Acc@1  88.28 ( 89.36)	Acc@5  99.22 ( 99.13)
Epoch: [72][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2568e-01 (3.5503e-01)	Acc@1  85.16 ( 89.36)	Acc@5  99.22 ( 99.15)
Epoch: [72][350/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3814e-01 (3.5472e-01)	Acc@1  93.75 ( 89.40)	Acc@5  98.44 ( 99.14)
Epoch: [72][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3666e-01 (3.5574e-01)	Acc@1  84.38 ( 89.34)	Acc@5 100.00 ( 99.13)
Epoch: [72][370/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4944e-01 (3.5492e-01)	Acc@1  90.62 ( 89.37)	Acc@5 100.00 ( 99.13)
Epoch: [72][380/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8439e-01 (3.5476e-01)	Acc@1  92.19 ( 89.38)	Acc@5 100.00 ( 99.13)
Epoch: [72][390/391]	Time  0.050 ( 0.066)	Data  0.001 ( 0.002)	Loss 6.2047e-01 (3.5517e-01)	Acc@1  80.00 ( 89.37)	Acc@5  96.25 ( 99.13)
## e[72] optimizer.zero_grad (sum) time: 0.41576075553894043
## e[72]       loss.backward (sum) time: 7.0456342697143555
## e[72]      optimizer.step (sum) time: 3.5121703147888184
## epoch[72] training(only) time: 25.75908899307251
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3209e+00 (1.3209e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.029 ( 0.039)	Loss 1.3343e+00 (1.3002e+00)	Acc@1  67.00 ( 66.73)	Acc@5  91.00 ( 90.00)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.3462e+00 (1.2399e+00)	Acc@1  67.00 ( 67.71)	Acc@5  93.00 ( 91.10)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.6333e+00 (1.2745e+00)	Acc@1  62.00 ( 67.39)	Acc@5  87.00 ( 90.68)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1704e+00 (1.2572e+00)	Acc@1  69.00 ( 67.10)	Acc@5  94.00 ( 91.27)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2661e+00 (1.2777e+00)	Acc@1  71.00 ( 66.98)	Acc@5  90.00 ( 90.94)
Test: [ 60/100]	Time  0.029 ( 0.029)	Loss 1.2620e+00 (1.2642e+00)	Acc@1  63.00 ( 67.31)	Acc@5  92.00 ( 90.98)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.1389e+00 (1.2709e+00)	Acc@1  68.00 ( 67.30)	Acc@5  94.00 ( 90.99)
Test: [ 80/100]	Time  0.031 ( 0.028)	Loss 1.4304e+00 (1.2808e+00)	Acc@1  65.00 ( 67.22)	Acc@5  93.00 ( 90.85)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.9300e+00 (1.2731e+00)	Acc@1  59.00 ( 67.47)	Acc@5  86.00 ( 90.90)
 * Acc@1 67.460 Acc@5 90.880
### epoch[72] execution time: 28.624131441116333
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.218 ( 0.218)	Data  0.148 ( 0.148)	Loss 3.3869e-01 (3.3869e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
Epoch: [73][ 10/391]	Time  0.064 ( 0.078)	Data  0.001 ( 0.015)	Loss 2.9652e-01 (3.3889e-01)	Acc@1  90.62 ( 89.77)	Acc@5 100.00 ( 99.29)
Epoch: [73][ 20/391]	Time  0.066 ( 0.072)	Data  0.001 ( 0.008)	Loss 3.8174e-01 (3.3926e-01)	Acc@1  89.06 ( 90.07)	Acc@5  97.66 ( 99.22)
Epoch: [73][ 30/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.006)	Loss 3.2322e-01 (3.4629e-01)	Acc@1  89.06 ( 89.52)	Acc@5 100.00 ( 99.17)
Epoch: [73][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.6299e-01 (3.4570e-01)	Acc@1  85.94 ( 89.50)	Acc@5 100.00 ( 99.24)
Epoch: [73][ 50/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.5832e-01 (3.4253e-01)	Acc@1  92.19 ( 89.84)	Acc@5  98.44 ( 99.25)
Epoch: [73][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.7383e-01 (3.4471e-01)	Acc@1  85.94 ( 89.83)	Acc@5  99.22 ( 99.24)
Epoch: [73][ 70/391]	Time  0.064 ( 0.067)	Data  0.002 ( 0.003)	Loss 2.4498e-01 (3.4388e-01)	Acc@1  92.19 ( 89.70)	Acc@5 100.00 ( 99.21)
Epoch: [73][ 80/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5277e-01 (3.4631e-01)	Acc@1  89.06 ( 89.69)	Acc@5  99.22 ( 99.19)
Epoch: [73][ 90/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.7709e-01 (3.4502e-01)	Acc@1  89.06 ( 89.72)	Acc@5 100.00 ( 99.18)
Epoch: [73][100/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.4323e-01 (3.4623e-01)	Acc@1  90.62 ( 89.64)	Acc@5  98.44 ( 99.18)
Epoch: [73][110/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0141e-01 (3.4626e-01)	Acc@1  91.41 ( 89.64)	Acc@5  99.22 ( 99.18)
Epoch: [73][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2286e-01 (3.4763e-01)	Acc@1  90.62 ( 89.65)	Acc@5  98.44 ( 99.13)
Epoch: [73][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2191e-01 (3.4546e-01)	Acc@1  90.62 ( 89.72)	Acc@5 100.00 ( 99.14)
Epoch: [73][140/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6142e-01 (3.4548e-01)	Acc@1  92.97 ( 89.70)	Acc@5 100.00 ( 99.14)
Epoch: [73][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1647e-01 (3.4811e-01)	Acc@1  90.62 ( 89.63)	Acc@5 100.00 ( 99.13)
Epoch: [73][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9138e-01 (3.4709e-01)	Acc@1  91.41 ( 89.62)	Acc@5  99.22 ( 99.16)
Epoch: [73][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.4979e-01 (3.5106e-01)	Acc@1  85.16 ( 89.51)	Acc@5  98.44 ( 99.12)
Epoch: [73][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3299e-01 (3.5050e-01)	Acc@1  90.62 ( 89.52)	Acc@5  98.44 ( 99.10)
Epoch: [73][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3419e-01 (3.5172e-01)	Acc@1  89.84 ( 89.44)	Acc@5 100.00 ( 99.12)
Epoch: [73][200/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4885e-01 (3.5180e-01)	Acc@1  89.06 ( 89.46)	Acc@5 100.00 ( 99.13)
Epoch: [73][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3843e-01 (3.5199e-01)	Acc@1  91.41 ( 89.44)	Acc@5 100.00 ( 99.13)
Epoch: [73][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1658e-01 (3.5229e-01)	Acc@1  89.84 ( 89.42)	Acc@5  99.22 ( 99.12)
Epoch: [73][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0573e-01 (3.5343e-01)	Acc@1  89.06 ( 89.38)	Acc@5  99.22 ( 99.12)
Epoch: [73][240/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8434e-01 (3.5186e-01)	Acc@1  90.62 ( 89.45)	Acc@5  99.22 ( 99.14)
Epoch: [73][250/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3266e-01 (3.5139e-01)	Acc@1  90.62 ( 89.44)	Acc@5 100.00 ( 99.15)
Epoch: [73][260/391]	Time  0.074 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4570e-01 (3.5145e-01)	Acc@1  91.41 ( 89.46)	Acc@5  96.88 ( 99.12)
Epoch: [73][270/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3470e-01 (3.5154e-01)	Acc@1  89.84 ( 89.47)	Acc@5  98.44 ( 99.11)
Epoch: [73][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7107e-01 (3.5095e-01)	Acc@1  93.75 ( 89.49)	Acc@5 100.00 ( 99.13)
Epoch: [73][290/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.8619e-01 (3.5161e-01)	Acc@1  82.03 ( 89.46)	Acc@5  98.44 ( 99.11)
Epoch: [73][300/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4562e-01 (3.5214e-01)	Acc@1  89.06 ( 89.43)	Acc@5 100.00 ( 99.11)
Epoch: [73][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3875e-01 (3.5171e-01)	Acc@1  86.72 ( 89.43)	Acc@5 100.00 ( 99.12)
Epoch: [73][320/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1201e-01 (3.5129e-01)	Acc@1  88.28 ( 89.45)	Acc@5  98.44 ( 99.13)
Epoch: [73][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7394e-01 (3.5152e-01)	Acc@1  86.72 ( 89.43)	Acc@5  99.22 ( 99.12)
Epoch: [73][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5101e-01 (3.5114e-01)	Acc@1  90.62 ( 89.42)	Acc@5 100.00 ( 99.13)
Epoch: [73][350/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7164e-01 (3.5039e-01)	Acc@1  91.41 ( 89.43)	Acc@5 100.00 ( 99.15)
Epoch: [73][360/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3947e-01 (3.5121e-01)	Acc@1  87.50 ( 89.42)	Acc@5  99.22 ( 99.14)
Epoch: [73][370/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9359e-01 (3.5056e-01)	Acc@1  96.09 ( 89.44)	Acc@5 100.00 ( 99.15)
Epoch: [73][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5494e-01 (3.4988e-01)	Acc@1  92.19 ( 89.47)	Acc@5 100.00 ( 99.16)
Epoch: [73][390/391]	Time  0.051 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9018e-01 (3.5002e-01)	Acc@1  92.50 ( 89.47)	Acc@5 100.00 ( 99.16)
## e[73] optimizer.zero_grad (sum) time: 0.41300320625305176
## e[73]       loss.backward (sum) time: 6.987316131591797
## e[73]      optimizer.step (sum) time: 3.4735617637634277
## epoch[73] training(only) time: 25.662718534469604
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2835e+00 (1.2835e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.3080e+00 (1.2972e+00)	Acc@1  66.00 ( 66.91)	Acc@5  91.00 ( 89.36)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3480e+00 (1.2409e+00)	Acc@1  69.00 ( 67.71)	Acc@5  92.00 ( 90.71)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.5935e+00 (1.2738e+00)	Acc@1  62.00 ( 67.48)	Acc@5  89.00 ( 90.29)
Test: [ 40/100]	Time  0.025 ( 0.029)	Loss 1.1585e+00 (1.2549e+00)	Acc@1  70.00 ( 67.39)	Acc@5  94.00 ( 90.95)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.2923e+00 (1.2778e+00)	Acc@1  72.00 ( 67.22)	Acc@5  89.00 ( 90.75)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.2657e+00 (1.2643e+00)	Acc@1  63.00 ( 67.39)	Acc@5  92.00 ( 90.85)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.1495e+00 (1.2717e+00)	Acc@1  68.00 ( 67.51)	Acc@5  94.00 ( 90.87)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4345e+00 (1.2824e+00)	Acc@1  65.00 ( 67.32)	Acc@5  92.00 ( 90.70)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.9572e+00 (1.2756e+00)	Acc@1  57.00 ( 67.48)	Acc@5  84.00 ( 90.71)
 * Acc@1 67.450 Acc@5 90.680
### epoch[73] execution time: 28.48341417312622
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.221 ( 0.221)	Data  0.151 ( 0.151)	Loss 4.1792e-01 (4.1792e-01)	Acc@1  85.16 ( 85.16)	Acc@5  98.44 ( 98.44)
Epoch: [74][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.015)	Loss 3.6774e-01 (3.4264e-01)	Acc@1  89.06 ( 90.41)	Acc@5  99.22 ( 99.22)
Epoch: [74][ 20/391]	Time  0.067 ( 0.072)	Data  0.001 ( 0.008)	Loss 4.4848e-01 (3.4417e-01)	Acc@1  85.94 ( 90.14)	Acc@5  98.44 ( 99.22)
Epoch: [74][ 30/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.5613e-01 (3.4373e-01)	Acc@1  93.75 ( 90.07)	Acc@5 100.00 ( 99.22)
Epoch: [74][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.6895e-01 (3.4464e-01)	Acc@1  96.09 ( 90.11)	Acc@5  98.44 ( 99.22)
Epoch: [74][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 5.1131e-01 (3.4890e-01)	Acc@1  85.16 ( 89.78)	Acc@5  98.44 ( 99.25)
Epoch: [74][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.0563e-01 (3.4533e-01)	Acc@1  90.62 ( 89.74)	Acc@5 100.00 ( 99.30)
Epoch: [74][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.9035e-01 (3.4728e-01)	Acc@1  92.19 ( 89.77)	Acc@5 100.00 ( 99.22)
Epoch: [74][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5736e-01 (3.4594e-01)	Acc@1  89.84 ( 89.72)	Acc@5  98.44 ( 99.19)
Epoch: [74][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.2414e-01 (3.4412e-01)	Acc@1  87.50 ( 89.87)	Acc@5  98.44 ( 99.21)
Epoch: [74][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.7130e-01 (3.4388e-01)	Acc@1  88.28 ( 89.83)	Acc@5  98.44 ( 99.18)
Epoch: [74][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.3762e-01 (3.4450e-01)	Acc@1  90.62 ( 89.81)	Acc@5  96.88 ( 99.16)
Epoch: [74][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1859e-01 (3.4708e-01)	Acc@1  87.50 ( 89.70)	Acc@5 100.00 ( 99.13)
Epoch: [74][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2470e-01 (3.4682e-01)	Acc@1  86.72 ( 89.65)	Acc@5  98.44 ( 99.14)
Epoch: [74][140/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3123e-01 (3.4701e-01)	Acc@1  92.97 ( 89.67)	Acc@5  98.44 ( 99.12)
Epoch: [74][150/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8874e-01 (3.4586e-01)	Acc@1  86.72 ( 89.71)	Acc@5 100.00 ( 99.12)
Epoch: [74][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5546e-01 (3.4522e-01)	Acc@1  88.28 ( 89.70)	Acc@5 100.00 ( 99.15)
Epoch: [74][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9876e-01 (3.4581e-01)	Acc@1  89.84 ( 89.65)	Acc@5 100.00 ( 99.15)
Epoch: [74][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9838e-01 (3.4500e-01)	Acc@1  87.50 ( 89.63)	Acc@5  97.66 ( 99.16)
Epoch: [74][190/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0382e-01 (3.4549e-01)	Acc@1  85.94 ( 89.57)	Acc@5 100.00 ( 99.17)
Epoch: [74][200/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1705e-01 (3.4593e-01)	Acc@1  85.94 ( 89.55)	Acc@5  98.44 ( 99.18)
Epoch: [74][210/391]	Time  0.076 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9688e-01 (3.4513e-01)	Acc@1  91.41 ( 89.60)	Acc@5  99.22 ( 99.19)
Epoch: [74][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5572e-01 (3.4586e-01)	Acc@1  89.06 ( 89.55)	Acc@5 100.00 ( 99.20)
Epoch: [74][230/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5778e-01 (3.4573e-01)	Acc@1  89.06 ( 89.55)	Acc@5 100.00 ( 99.21)
Epoch: [74][240/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4917e-01 (3.4570e-01)	Acc@1  92.19 ( 89.55)	Acc@5  99.22 ( 99.22)
Epoch: [74][250/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6885e-01 (3.4556e-01)	Acc@1  92.19 ( 89.55)	Acc@5 100.00 ( 99.22)
Epoch: [74][260/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0394e-01 (3.4721e-01)	Acc@1  87.50 ( 89.50)	Acc@5  98.44 ( 99.21)
Epoch: [74][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5552e-01 (3.4777e-01)	Acc@1  88.28 ( 89.47)	Acc@5  98.44 ( 99.20)
Epoch: [74][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3692e-01 (3.4743e-01)	Acc@1  92.19 ( 89.48)	Acc@5  99.22 ( 99.20)
Epoch: [74][290/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8942e-01 (3.4642e-01)	Acc@1  88.28 ( 89.50)	Acc@5 100.00 ( 99.20)
Epoch: [74][300/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7707e-01 (3.4557e-01)	Acc@1  85.16 ( 89.53)	Acc@5  99.22 ( 99.20)
Epoch: [74][310/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8932e-01 (3.4715e-01)	Acc@1  88.28 ( 89.51)	Acc@5  96.88 ( 99.18)
Epoch: [74][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3729e-01 (3.4799e-01)	Acc@1  87.50 ( 89.48)	Acc@5  98.44 ( 99.17)
Epoch: [74][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1186e-01 (3.4848e-01)	Acc@1  89.06 ( 89.48)	Acc@5  98.44 ( 99.17)
Epoch: [74][340/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9154e-01 (3.4933e-01)	Acc@1  89.84 ( 89.45)	Acc@5  99.22 ( 99.18)
Epoch: [74][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0686e-01 (3.4892e-01)	Acc@1  92.97 ( 89.44)	Acc@5 100.00 ( 99.19)
Epoch: [74][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2675e-01 (3.4906e-01)	Acc@1  90.62 ( 89.42)	Acc@5 100.00 ( 99.19)
Epoch: [74][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.5611e-01 (3.4998e-01)	Acc@1  85.16 ( 89.37)	Acc@5  98.44 ( 99.20)
Epoch: [74][380/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9785e-01 (3.4961e-01)	Acc@1  88.28 ( 89.38)	Acc@5  98.44 ( 99.20)
Epoch: [74][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4263e-01 (3.4913e-01)	Acc@1  90.00 ( 89.39)	Acc@5 100.00 ( 99.21)
## e[74] optimizer.zero_grad (sum) time: 0.4166531562805176
## e[74]       loss.backward (sum) time: 7.004236459732056
## e[74]      optimizer.step (sum) time: 3.3588783740997314
## epoch[74] training(only) time: 25.548084497451782
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.3310e+00 (1.3310e+00)	Acc@1  65.00 ( 65.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.028 ( 0.040)	Loss 1.2983e+00 (1.3005e+00)	Acc@1  66.00 ( 67.09)	Acc@5  91.00 ( 89.55)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.3673e+00 (1.2516e+00)	Acc@1  68.00 ( 68.00)	Acc@5  93.00 ( 90.86)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6353e+00 (1.2877e+00)	Acc@1  64.00 ( 67.65)	Acc@5  87.00 ( 90.42)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1937e+00 (1.2690e+00)	Acc@1  71.00 ( 67.44)	Acc@5  92.00 ( 90.98)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.3118e+00 (1.2881e+00)	Acc@1  72.00 ( 67.14)	Acc@5  88.00 ( 90.75)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.2618e+00 (1.2730e+00)	Acc@1  64.00 ( 67.44)	Acc@5  93.00 ( 90.87)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1762e+00 (1.2816e+00)	Acc@1  67.00 ( 67.52)	Acc@5  94.00 ( 90.90)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4480e+00 (1.2933e+00)	Acc@1  67.00 ( 67.36)	Acc@5  93.00 ( 90.73)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.9267e+00 (1.2867e+00)	Acc@1  56.00 ( 67.45)	Acc@5  84.00 ( 90.77)
 * Acc@1 67.370 Acc@5 90.730
### epoch[74] execution time: 28.398467540740967
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.216 ( 0.216)	Data  0.146 ( 0.146)	Loss 2.5949e-01 (2.5949e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.066 ( 0.079)	Data  0.001 ( 0.014)	Loss 3.2982e-01 (3.3466e-01)	Acc@1  89.06 ( 90.06)	Acc@5  99.22 ( 99.29)
Epoch: [75][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 2.9378e-01 (3.2922e-01)	Acc@1  91.41 ( 90.25)	Acc@5  99.22 ( 99.37)
Epoch: [75][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.3098e-01 (3.3834e-01)	Acc@1  89.06 ( 90.02)	Acc@5  99.22 ( 99.34)
Epoch: [75][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.4666e-01 (3.4315e-01)	Acc@1  89.06 ( 89.69)	Acc@5  99.22 ( 99.24)
Epoch: [75][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.4877e-01 (3.4442e-01)	Acc@1  89.06 ( 89.77)	Acc@5 100.00 ( 99.23)
Epoch: [75][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.4977e-01 (3.4308e-01)	Acc@1  90.62 ( 89.77)	Acc@5  99.22 ( 99.22)
Epoch: [75][ 70/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.003)	Loss 3.3882e-01 (3.4178e-01)	Acc@1  90.62 ( 89.77)	Acc@5  98.44 ( 99.20)
Epoch: [75][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.7482e-01 (3.4249e-01)	Acc@1  88.28 ( 89.69)	Acc@5  96.88 ( 99.18)
Epoch: [75][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5042e-01 (3.3969e-01)	Acc@1  88.28 ( 89.70)	Acc@5  99.22 ( 99.18)
Epoch: [75][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.1458e-01 (3.4018e-01)	Acc@1  87.50 ( 89.63)	Acc@5 100.00 ( 99.21)
Epoch: [75][110/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8822e-01 (3.4037e-01)	Acc@1  90.62 ( 89.68)	Acc@5 100.00 ( 99.20)
Epoch: [75][120/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.7591e-01 (3.4014e-01)	Acc@1  90.62 ( 89.74)	Acc@5 100.00 ( 99.21)
Epoch: [75][130/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.5531e-01 (3.4030e-01)	Acc@1  83.59 ( 89.67)	Acc@5  98.44 ( 99.23)
Epoch: [75][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5938e-01 (3.4095e-01)	Acc@1  90.62 ( 89.66)	Acc@5  98.44 ( 99.22)
Epoch: [75][150/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7712e-01 (3.4005e-01)	Acc@1  92.19 ( 89.69)	Acc@5 100.00 ( 99.23)
Epoch: [75][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4621e-01 (3.3978e-01)	Acc@1  90.62 ( 89.73)	Acc@5 100.00 ( 99.24)
Epoch: [75][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4874e-01 (3.4107e-01)	Acc@1  88.28 ( 89.65)	Acc@5  99.22 ( 99.24)
Epoch: [75][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1787e-01 (3.4089e-01)	Acc@1  93.75 ( 89.67)	Acc@5 100.00 ( 99.24)
Epoch: [75][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7502e-01 (3.4254e-01)	Acc@1  91.41 ( 89.61)	Acc@5  99.22 ( 99.21)
Epoch: [75][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1352e-01 (3.4188e-01)	Acc@1  92.19 ( 89.64)	Acc@5  99.22 ( 99.21)
Epoch: [75][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6284e-01 (3.4452e-01)	Acc@1  87.50 ( 89.54)	Acc@5 100.00 ( 99.21)
Epoch: [75][220/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9845e-01 (3.4413e-01)	Acc@1  82.81 ( 89.55)	Acc@5  96.88 ( 99.21)
Epoch: [75][230/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8170e-01 (3.4428e-01)	Acc@1  90.62 ( 89.53)	Acc@5 100.00 ( 99.23)
Epoch: [75][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1491e-01 (3.4415e-01)	Acc@1  92.97 ( 89.57)	Acc@5  98.44 ( 99.23)
Epoch: [75][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4984e-01 (3.4347e-01)	Acc@1  93.75 ( 89.56)	Acc@5  99.22 ( 99.23)
Epoch: [75][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6395e-01 (3.4444e-01)	Acc@1  87.50 ( 89.52)	Acc@5 100.00 ( 99.24)
Epoch: [75][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0446e-01 (3.4524e-01)	Acc@1  90.62 ( 89.49)	Acc@5  99.22 ( 99.24)
Epoch: [75][280/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9506e-01 (3.4493e-01)	Acc@1  90.62 ( 89.50)	Acc@5 100.00 ( 99.25)
Epoch: [75][290/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5799e-01 (3.4476e-01)	Acc@1  90.62 ( 89.53)	Acc@5  98.44 ( 99.25)
Epoch: [75][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5953e-01 (3.4438e-01)	Acc@1  86.72 ( 89.53)	Acc@5 100.00 ( 99.25)
Epoch: [75][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1856e-01 (3.4471e-01)	Acc@1  85.16 ( 89.51)	Acc@5  99.22 ( 99.24)
Epoch: [75][320/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1611e-01 (3.4593e-01)	Acc@1  89.84 ( 89.48)	Acc@5 100.00 ( 99.22)
Epoch: [75][330/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0911e-01 (3.4643e-01)	Acc@1  87.50 ( 89.44)	Acc@5 100.00 ( 99.23)
Epoch: [75][340/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4384e-01 (3.4795e-01)	Acc@1  86.72 ( 89.42)	Acc@5 100.00 ( 99.21)
Epoch: [75][350/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4228e-01 (3.4810e-01)	Acc@1  93.75 ( 89.39)	Acc@5 100.00 ( 99.20)
Epoch: [75][360/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0274e-01 (3.4703e-01)	Acc@1  89.06 ( 89.43)	Acc@5  99.22 ( 99.22)
Epoch: [75][370/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8620e-01 (3.4731e-01)	Acc@1  92.19 ( 89.43)	Acc@5 100.00 ( 99.22)
Epoch: [75][380/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3690e-01 (3.4706e-01)	Acc@1  89.84 ( 89.43)	Acc@5  99.22 ( 99.22)
Epoch: [75][390/391]	Time  0.053 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9729e-01 (3.4803e-01)	Acc@1  86.25 ( 89.40)	Acc@5  98.75 ( 99.20)
## e[75] optimizer.zero_grad (sum) time: 0.412203311920166
## e[75]       loss.backward (sum) time: 6.985019207000732
## e[75]      optimizer.step (sum) time: 3.517244815826416
## epoch[75] training(only) time: 25.70296835899353
# Switched to evaluate mode...
Test: [  0/100]	Time  0.163 ( 0.163)	Loss 1.3428e+00 (1.3428e+00)	Acc@1  67.00 ( 67.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.3080e+00 (1.3191e+00)	Acc@1  63.00 ( 66.64)	Acc@5  91.00 ( 89.45)
Test: [ 20/100]	Time  0.029 ( 0.033)	Loss 1.3535e+00 (1.2597e+00)	Acc@1  68.00 ( 67.52)	Acc@5  92.00 ( 90.38)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6525e+00 (1.2961e+00)	Acc@1  62.00 ( 67.32)	Acc@5  86.00 ( 90.03)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2151e+00 (1.2802e+00)	Acc@1  68.00 ( 67.07)	Acc@5  93.00 ( 90.78)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.3235e+00 (1.2971e+00)	Acc@1  71.00 ( 66.92)	Acc@5  89.00 ( 90.59)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.2991e+00 (1.2845e+00)	Acc@1  63.00 ( 67.11)	Acc@5  90.00 ( 90.62)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1874e+00 (1.2921e+00)	Acc@1  64.00 ( 67.13)	Acc@5  94.00 ( 90.70)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.4614e+00 (1.3016e+00)	Acc@1  64.00 ( 66.95)	Acc@5  90.00 ( 90.46)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9738e+00 (1.2950e+00)	Acc@1  57.00 ( 67.12)	Acc@5  84.00 ( 90.46)
 * Acc@1 67.130 Acc@5 90.500
### epoch[75] execution time: 28.546091556549072
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.223 ( 0.223)	Data  0.154 ( 0.154)	Loss 3.9817e-01 (3.9817e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 97.66)
Epoch: [76][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.015)	Loss 2.2514e-01 (3.4102e-01)	Acc@1  94.53 ( 89.77)	Acc@5 100.00 ( 99.22)
Epoch: [76][ 20/391]	Time  0.063 ( 0.073)	Data  0.001 ( 0.008)	Loss 2.0868e-01 (3.1861e-01)	Acc@1  93.75 ( 90.62)	Acc@5 100.00 ( 99.37)
Epoch: [76][ 30/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.8923e-01 (3.2981e-01)	Acc@1  88.28 ( 90.37)	Acc@5  96.88 ( 99.17)
Epoch: [76][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.8761e-01 (3.3168e-01)	Acc@1  86.72 ( 90.28)	Acc@5  98.44 ( 99.16)
Epoch: [76][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 4.4624e-01 (3.3364e-01)	Acc@1  87.50 ( 90.18)	Acc@5  97.66 ( 99.16)
Epoch: [76][ 60/391]	Time  0.061 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.8841e-01 (3.3292e-01)	Acc@1  91.41 ( 90.22)	Acc@5  99.22 ( 99.13)
Epoch: [76][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.9068e-01 (3.3307e-01)	Acc@1  91.41 ( 90.03)	Acc@5  99.22 ( 99.13)
Epoch: [76][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.5971e-01 (3.3759e-01)	Acc@1  86.72 ( 89.92)	Acc@5  96.88 ( 99.10)
Epoch: [76][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8933e-01 (3.3697e-01)	Acc@1  89.84 ( 89.90)	Acc@5 100.00 ( 99.12)
Epoch: [76][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.7323e-01 (3.3798e-01)	Acc@1  89.06 ( 89.75)	Acc@5  98.44 ( 99.15)
Epoch: [76][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 4.0442e-01 (3.4006e-01)	Acc@1  89.06 ( 89.67)	Acc@5  97.66 ( 99.14)
Epoch: [76][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2566e-01 (3.3968e-01)	Acc@1  85.16 ( 89.65)	Acc@5  99.22 ( 99.13)
Epoch: [76][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9946e-01 (3.3853e-01)	Acc@1  88.28 ( 89.73)	Acc@5 100.00 ( 99.15)
Epoch: [76][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5276e-01 (3.3967e-01)	Acc@1  90.62 ( 89.71)	Acc@5  97.66 ( 99.17)
Epoch: [76][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0780e-01 (3.3943e-01)	Acc@1  88.28 ( 89.69)	Acc@5 100.00 ( 99.17)
Epoch: [76][160/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6381e-01 (3.3737e-01)	Acc@1  92.97 ( 89.80)	Acc@5 100.00 ( 99.18)
Epoch: [76][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8833e-01 (3.3780e-01)	Acc@1  90.62 ( 89.76)	Acc@5 100.00 ( 99.17)
Epoch: [76][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7826e-01 (3.3918e-01)	Acc@1  88.28 ( 89.73)	Acc@5  96.09 ( 99.17)
Epoch: [76][190/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2335e-01 (3.4014e-01)	Acc@1  87.50 ( 89.64)	Acc@5  99.22 ( 99.17)
Epoch: [76][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5376e-01 (3.3905e-01)	Acc@1  92.19 ( 89.67)	Acc@5 100.00 ( 99.18)
Epoch: [76][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7026e-01 (3.4081e-01)	Acc@1  86.72 ( 89.59)	Acc@5  98.44 ( 99.16)
Epoch: [76][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9482e-01 (3.4309e-01)	Acc@1  82.81 ( 89.49)	Acc@5  98.44 ( 99.16)
Epoch: [76][230/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3662e-01 (3.4303e-01)	Acc@1  89.84 ( 89.51)	Acc@5 100.00 ( 99.18)
Epoch: [76][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8000e-01 (3.4396e-01)	Acc@1  89.84 ( 89.50)	Acc@5  97.66 ( 99.17)
Epoch: [76][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7540e-01 (3.4442e-01)	Acc@1  85.94 ( 89.50)	Acc@5  99.22 ( 99.16)
Epoch: [76][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0583e-01 (3.4528e-01)	Acc@1  89.84 ( 89.49)	Acc@5  98.44 ( 99.16)
Epoch: [76][270/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5544e-01 (3.4517e-01)	Acc@1  92.19 ( 89.48)	Acc@5  99.22 ( 99.18)
Epoch: [76][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1879e-01 (3.4467e-01)	Acc@1  90.62 ( 89.52)	Acc@5  99.22 ( 99.18)
Epoch: [76][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5383e-01 (3.4457e-01)	Acc@1  89.84 ( 89.54)	Acc@5  98.44 ( 99.17)
Epoch: [76][300/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9422e-01 (3.4484e-01)	Acc@1  90.62 ( 89.53)	Acc@5  99.22 ( 99.17)
Epoch: [76][310/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5556e-01 (3.4441e-01)	Acc@1  88.28 ( 89.54)	Acc@5 100.00 ( 99.19)
Epoch: [76][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3506e-01 (3.4484e-01)	Acc@1  89.06 ( 89.54)	Acc@5 100.00 ( 99.19)
Epoch: [76][330/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2322e-01 (3.4576e-01)	Acc@1  88.28 ( 89.50)	Acc@5  98.44 ( 99.19)
Epoch: [76][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5492e-01 (3.4464e-01)	Acc@1  89.06 ( 89.54)	Acc@5  99.22 ( 99.19)
Epoch: [76][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7139e-01 (3.4549e-01)	Acc@1  89.06 ( 89.50)	Acc@5  99.22 ( 99.19)
Epoch: [76][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4127e-01 (3.4493e-01)	Acc@1  91.41 ( 89.52)	Acc@5 100.00 ( 99.19)
Epoch: [76][370/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3447e-01 (3.4505e-01)	Acc@1  88.28 ( 89.53)	Acc@5  99.22 ( 99.20)
Epoch: [76][380/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1052e-01 (3.4528e-01)	Acc@1  88.28 ( 89.52)	Acc@5  99.22 ( 99.19)
Epoch: [76][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4646e-01 (3.4527e-01)	Acc@1  86.25 ( 89.51)	Acc@5 100.00 ( 99.19)
## e[76] optimizer.zero_grad (sum) time: 0.4013247489929199
## e[76]       loss.backward (sum) time: 6.976643323898315
## e[76]      optimizer.step (sum) time: 3.4821548461914062
## epoch[76] training(only) time: 25.577955961227417
# Switched to evaluate mode...
Test: [  0/100]	Time  0.166 ( 0.166)	Loss 1.3194e+00 (1.3194e+00)	Acc@1  67.00 ( 67.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.025 ( 0.040)	Loss 1.3091e+00 (1.3003e+00)	Acc@1  65.00 ( 66.64)	Acc@5  91.00 ( 89.82)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.3256e+00 (1.2443e+00)	Acc@1  69.00 ( 68.14)	Acc@5  93.00 ( 90.95)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6307e+00 (1.2841e+00)	Acc@1  61.00 ( 67.61)	Acc@5  88.00 ( 90.55)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1609e+00 (1.2681e+00)	Acc@1  68.00 ( 67.39)	Acc@5  95.00 ( 91.29)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.2953e+00 (1.2902e+00)	Acc@1  71.00 ( 67.16)	Acc@5  89.00 ( 90.92)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.2736e+00 (1.2781e+00)	Acc@1  61.00 ( 67.39)	Acc@5  91.00 ( 90.93)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1782e+00 (1.2856e+00)	Acc@1  65.00 ( 67.38)	Acc@5  94.00 ( 90.90)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4382e+00 (1.2953e+00)	Acc@1  65.00 ( 67.22)	Acc@5  92.00 ( 90.72)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.9494e+00 (1.2877e+00)	Acc@1  56.00 ( 67.45)	Acc@5  84.00 ( 90.75)
 * Acc@1 67.370 Acc@5 90.710
### epoch[76] execution time: 28.41787624359131
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.215 ( 0.215)	Data  0.143 ( 0.143)	Loss 2.7624e-01 (2.7624e-01)	Acc@1  93.75 ( 93.75)	Acc@5  98.44 ( 98.44)
Epoch: [77][ 10/391]	Time  0.063 ( 0.079)	Data  0.001 ( 0.014)	Loss 2.9749e-01 (3.0665e-01)	Acc@1  91.41 ( 91.48)	Acc@5 100.00 ( 99.43)
Epoch: [77][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 3.1520e-01 (3.2360e-01)	Acc@1  92.19 ( 90.22)	Acc@5  99.22 ( 99.40)
Epoch: [77][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.9310e-01 (3.2506e-01)	Acc@1  90.62 ( 90.15)	Acc@5 100.00 ( 99.37)
Epoch: [77][ 40/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.005)	Loss 3.9977e-01 (3.3227e-01)	Acc@1  89.06 ( 89.82)	Acc@5  99.22 ( 99.31)
Epoch: [77][ 50/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.9244e-01 (3.4048e-01)	Acc@1  89.06 ( 89.52)	Acc@5 100.00 ( 99.26)
Epoch: [77][ 60/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.7583e-01 (3.3951e-01)	Acc@1  87.50 ( 89.50)	Acc@5 100.00 ( 99.24)
Epoch: [77][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.0235e-01 (3.4070e-01)	Acc@1  91.41 ( 89.61)	Acc@5  99.22 ( 99.20)
Epoch: [77][ 80/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9013e-01 (3.4177e-01)	Acc@1  89.06 ( 89.63)	Acc@5  97.66 ( 99.18)
Epoch: [77][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.5344e-01 (3.3746e-01)	Acc@1  89.84 ( 89.90)	Acc@5  99.22 ( 99.23)
Epoch: [77][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.2317e-01 (3.3894e-01)	Acc@1  86.72 ( 89.87)	Acc@5 100.00 ( 99.20)
Epoch: [77][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7784e-01 (3.4002e-01)	Acc@1  92.97 ( 89.77)	Acc@5  99.22 ( 99.20)
Epoch: [77][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6033e-01 (3.3934e-01)	Acc@1  91.41 ( 89.81)	Acc@5  99.22 ( 99.20)
Epoch: [77][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6281e-01 (3.3904e-01)	Acc@1  92.19 ( 89.83)	Acc@5  99.22 ( 99.21)
Epoch: [77][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6757e-01 (3.4010e-01)	Acc@1  93.75 ( 89.79)	Acc@5 100.00 ( 99.21)
Epoch: [77][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8805e-01 (3.4071e-01)	Acc@1  89.84 ( 89.76)	Acc@5  97.66 ( 99.20)
Epoch: [77][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2071e-01 (3.3978e-01)	Acc@1  91.41 ( 89.77)	Acc@5  99.22 ( 99.21)
Epoch: [77][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2757e-01 (3.3831e-01)	Acc@1  90.62 ( 89.82)	Acc@5  99.22 ( 99.22)
Epoch: [77][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0915e-01 (3.3856e-01)	Acc@1  92.19 ( 89.83)	Acc@5  99.22 ( 99.22)
Epoch: [77][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4273e-01 (3.3862e-01)	Acc@1  89.06 ( 89.80)	Acc@5 100.00 ( 99.23)
Epoch: [77][200/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9129e-01 (3.3885e-01)	Acc@1  86.72 ( 89.79)	Acc@5  99.22 ( 99.23)
Epoch: [77][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0290e-01 (3.3871e-01)	Acc@1  87.50 ( 89.82)	Acc@5  98.44 ( 99.22)
Epoch: [77][220/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2062e-01 (3.4076e-01)	Acc@1  86.72 ( 89.73)	Acc@5  98.44 ( 99.18)
Epoch: [77][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4028e-01 (3.3985e-01)	Acc@1  90.62 ( 89.76)	Acc@5  98.44 ( 99.19)
Epoch: [77][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4619e-01 (3.4022e-01)	Acc@1  86.72 ( 89.70)	Acc@5 100.00 ( 99.21)
Epoch: [77][250/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8472e-01 (3.3976e-01)	Acc@1  90.62 ( 89.75)	Acc@5  98.44 ( 99.19)
Epoch: [77][260/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1900e-01 (3.4021e-01)	Acc@1  87.50 ( 89.71)	Acc@5  99.22 ( 99.19)
Epoch: [77][270/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0110e-01 (3.4150e-01)	Acc@1  88.28 ( 89.67)	Acc@5  98.44 ( 99.17)
Epoch: [77][280/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4789e-01 (3.4177e-01)	Acc@1  89.06 ( 89.68)	Acc@5 100.00 ( 99.17)
Epoch: [77][290/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9123e-01 (3.4165e-01)	Acc@1  89.84 ( 89.70)	Acc@5  99.22 ( 99.17)
Epoch: [77][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1490e-01 (3.4092e-01)	Acc@1  89.84 ( 89.71)	Acc@5  99.22 ( 99.17)
Epoch: [77][310/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9591e-01 (3.4116e-01)	Acc@1  89.84 ( 89.73)	Acc@5  96.88 ( 99.17)
Epoch: [77][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6263e-01 (3.4159e-01)	Acc@1  90.62 ( 89.72)	Acc@5  99.22 ( 99.16)
Epoch: [77][330/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.4035e-01 (3.4234e-01)	Acc@1  82.81 ( 89.68)	Acc@5  98.44 ( 99.15)
Epoch: [77][340/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.9532e-01 (3.4246e-01)	Acc@1  88.28 ( 89.67)	Acc@5 100.00 ( 99.15)
Epoch: [77][350/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3920e-01 (3.4303e-01)	Acc@1  90.62 ( 89.64)	Acc@5 100.00 ( 99.16)
Epoch: [77][360/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5547e-01 (3.4252e-01)	Acc@1  89.06 ( 89.68)	Acc@5  99.22 ( 99.17)
Epoch: [77][370/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3891e-01 (3.4192e-01)	Acc@1  92.19 ( 89.70)	Acc@5  98.44 ( 99.16)
Epoch: [77][380/391]	Time  0.067 ( 0.065)	Data  0.002 ( 0.002)	Loss 3.0182e-01 (3.4182e-01)	Acc@1  92.19 ( 89.70)	Acc@5  99.22 ( 99.17)
Epoch: [77][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0583e-01 (3.4083e-01)	Acc@1  90.00 ( 89.75)	Acc@5 100.00 ( 99.17)
## e[77] optimizer.zero_grad (sum) time: 0.4064161777496338
## e[77]       loss.backward (sum) time: 7.0092997550964355
## e[77]      optimizer.step (sum) time: 3.3991310596466064
## epoch[77] training(only) time: 25.520169734954834
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.3042e+00 (1.3042e+00)	Acc@1  70.00 ( 70.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3797e+00 (1.3029e+00)	Acc@1  64.00 ( 67.18)	Acc@5  91.00 ( 90.00)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.3494e+00 (1.2560e+00)	Acc@1  67.00 ( 67.86)	Acc@5  93.00 ( 91.00)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 1.6567e+00 (1.2937e+00)	Acc@1  61.00 ( 67.42)	Acc@5  88.00 ( 90.65)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.1809e+00 (1.2772e+00)	Acc@1  69.00 ( 67.07)	Acc@5  93.00 ( 91.20)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.2894e+00 (1.2977e+00)	Acc@1  70.00 ( 66.80)	Acc@5  89.00 ( 90.86)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3417e+00 (1.2860e+00)	Acc@1  61.00 ( 67.11)	Acc@5  91.00 ( 90.80)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1531e+00 (1.2933e+00)	Acc@1  67.00 ( 67.06)	Acc@5  94.00 ( 90.76)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4506e+00 (1.3028e+00)	Acc@1  65.00 ( 66.86)	Acc@5  92.00 ( 90.59)
Test: [ 90/100]	Time  0.029 ( 0.028)	Loss 1.9833e+00 (1.2947e+00)	Acc@1  56.00 ( 67.10)	Acc@5  85.00 ( 90.70)
 * Acc@1 67.070 Acc@5 90.730
### epoch[77] execution time: 28.384883880615234
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.219 ( 0.219)	Data  0.142 ( 0.142)	Loss 4.0533e-01 (4.0533e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [78][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 3.7412e-01 (3.7986e-01)	Acc@1  89.06 ( 89.28)	Acc@5  99.22 ( 98.44)
Epoch: [78][ 20/391]	Time  0.068 ( 0.072)	Data  0.001 ( 0.008)	Loss 4.6017e-01 (3.6368e-01)	Acc@1  89.06 ( 89.69)	Acc@5  97.66 ( 98.66)
Epoch: [78][ 30/391]	Time  0.065 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.9841e-01 (3.5096e-01)	Acc@1  92.19 ( 89.89)	Acc@5 100.00 ( 98.87)
Epoch: [78][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.9931e-01 (3.5090e-01)	Acc@1  87.50 ( 89.60)	Acc@5  99.22 ( 98.97)
Epoch: [78][ 50/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.2320e-01 (3.4808e-01)	Acc@1  90.62 ( 89.60)	Acc@5  99.22 ( 99.03)
Epoch: [78][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7083e-01 (3.4136e-01)	Acc@1  92.19 ( 89.88)	Acc@5 100.00 ( 99.15)
Epoch: [78][ 70/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.6536e-01 (3.3864e-01)	Acc@1  89.06 ( 90.03)	Acc@5  96.88 ( 99.15)
Epoch: [78][ 80/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.2659e-01 (3.3917e-01)	Acc@1  85.94 ( 90.08)	Acc@5  97.66 ( 99.14)
Epoch: [78][ 90/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8389e-01 (3.3779e-01)	Acc@1  92.19 ( 90.14)	Acc@5  99.22 ( 99.14)
Epoch: [78][100/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.9492e-01 (3.3817e-01)	Acc@1  92.97 ( 90.04)	Acc@5 100.00 ( 99.16)
Epoch: [78][110/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1856e-01 (3.3408e-01)	Acc@1  95.31 ( 90.25)	Acc@5  99.22 ( 99.18)
Epoch: [78][120/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.6041e-01 (3.3636e-01)	Acc@1  83.59 ( 90.23)	Acc@5  98.44 ( 99.15)
Epoch: [78][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7112e-01 (3.3599e-01)	Acc@1  89.84 ( 90.20)	Acc@5  98.44 ( 99.16)
Epoch: [78][140/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1350e-01 (3.3446e-01)	Acc@1  89.84 ( 90.28)	Acc@5  99.22 ( 99.19)
Epoch: [78][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.9957e-01 (3.3555e-01)	Acc@1  81.25 ( 90.22)	Acc@5  98.44 ( 99.18)
Epoch: [78][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7687e-01 (3.3364e-01)	Acc@1  90.62 ( 90.28)	Acc@5 100.00 ( 99.19)
Epoch: [78][170/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3733e-01 (3.3454e-01)	Acc@1  89.06 ( 90.22)	Acc@5 100.00 ( 99.20)
Epoch: [78][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4859e-01 (3.3527e-01)	Acc@1  86.72 ( 90.15)	Acc@5  99.22 ( 99.21)
Epoch: [78][190/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3931e-01 (3.3525e-01)	Acc@1  89.84 ( 90.12)	Acc@5 100.00 ( 99.22)
Epoch: [78][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0765e-01 (3.3497e-01)	Acc@1  89.06 ( 90.11)	Acc@5 100.00 ( 99.22)
Epoch: [78][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0154e-01 (3.3558e-01)	Acc@1  91.41 ( 90.10)	Acc@5  99.22 ( 99.22)
Epoch: [78][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8861e-01 (3.3472e-01)	Acc@1  90.62 ( 90.16)	Acc@5  97.66 ( 99.23)
Epoch: [78][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7991e-01 (3.3478e-01)	Acc@1  89.84 ( 90.21)	Acc@5  99.22 ( 99.23)
Epoch: [78][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4764e-01 (3.3373e-01)	Acc@1  90.62 ( 90.28)	Acc@5 100.00 ( 99.23)
Epoch: [78][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.7077e-01 (3.3446e-01)	Acc@1  85.94 ( 90.22)	Acc@5  97.66 ( 99.23)
Epoch: [78][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6121e-01 (3.3626e-01)	Acc@1  89.06 ( 90.14)	Acc@5  99.22 ( 99.22)
Epoch: [78][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5531e-01 (3.3584e-01)	Acc@1  89.84 ( 90.15)	Acc@5  99.22 ( 99.23)
Epoch: [78][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3452e-01 (3.3640e-01)	Acc@1  89.84 ( 90.13)	Acc@5  98.44 ( 99.23)
Epoch: [78][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5812e-01 (3.3725e-01)	Acc@1  89.84 ( 90.09)	Acc@5  99.22 ( 99.22)
Epoch: [78][300/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3696e-01 (3.3659e-01)	Acc@1  92.97 ( 90.10)	Acc@5 100.00 ( 99.23)
Epoch: [78][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0249e-01 (3.3671e-01)	Acc@1  86.72 ( 90.03)	Acc@5 100.00 ( 99.23)
Epoch: [78][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4593e-01 (3.3647e-01)	Acc@1  87.50 ( 90.03)	Acc@5 100.00 ( 99.23)
Epoch: [78][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1258e-01 (3.3632e-01)	Acc@1  91.41 ( 90.03)	Acc@5  99.22 ( 99.24)
Epoch: [78][340/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9850e-01 (3.3604e-01)	Acc@1  90.62 ( 90.07)	Acc@5  98.44 ( 99.22)
Epoch: [78][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5497e-01 (3.3630e-01)	Acc@1  89.84 ( 90.05)	Acc@5 100.00 ( 99.22)
Epoch: [78][360/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0998e-01 (3.3663e-01)	Acc@1  89.84 ( 90.03)	Acc@5 100.00 ( 99.22)
Epoch: [78][370/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3894e-01 (3.3695e-01)	Acc@1  89.84 ( 90.01)	Acc@5  99.22 ( 99.22)
Epoch: [78][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1165e-01 (3.3763e-01)	Acc@1  89.84 ( 89.96)	Acc@5  98.44 ( 99.22)
Epoch: [78][390/391]	Time  0.057 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3078e-01 (3.3752e-01)	Acc@1  91.25 ( 89.98)	Acc@5  98.75 ( 99.22)
## e[78] optimizer.zero_grad (sum) time: 0.41066884994506836
## e[78]       loss.backward (sum) time: 7.008848190307617
## e[78]      optimizer.step (sum) time: 3.4537715911865234
## epoch[78] training(only) time: 25.597410202026367
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.2920e+00 (1.2920e+00)	Acc@1  69.00 ( 69.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.3407e+00 (1.2984e+00)	Acc@1  67.00 ( 67.45)	Acc@5  91.00 ( 89.64)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3609e+00 (1.2516e+00)	Acc@1  68.00 ( 68.19)	Acc@5  92.00 ( 90.52)
Test: [ 30/100]	Time  0.025 ( 0.030)	Loss 1.6166e+00 (1.2878e+00)	Acc@1  62.00 ( 67.77)	Acc@5  87.00 ( 90.13)
Test: [ 40/100]	Time  0.028 ( 0.029)	Loss 1.1921e+00 (1.2724e+00)	Acc@1  69.00 ( 67.37)	Acc@5  93.00 ( 90.71)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.3004e+00 (1.2920e+00)	Acc@1  71.00 ( 67.12)	Acc@5  88.00 ( 90.49)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3202e+00 (1.2789e+00)	Acc@1  63.00 ( 67.31)	Acc@5  91.00 ( 90.57)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1451e+00 (1.2857e+00)	Acc@1  69.00 ( 67.38)	Acc@5  93.00 ( 90.61)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.4248e+00 (1.2949e+00)	Acc@1  65.00 ( 67.27)	Acc@5  93.00 ( 90.46)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.9617e+00 (1.2887e+00)	Acc@1  58.00 ( 67.44)	Acc@5  85.00 ( 90.53)
 * Acc@1 67.470 Acc@5 90.550
### epoch[78] execution time: 28.43635892868042
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.219 ( 0.219)	Data  0.150 ( 0.150)	Loss 3.2639e-01 (3.2639e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.065 ( 0.080)	Data  0.001 ( 0.015)	Loss 4.1257e-01 (3.4038e-01)	Acc@1  86.72 ( 89.35)	Acc@5  99.22 ( 99.15)
Epoch: [79][ 20/391]	Time  0.060 ( 0.072)	Data  0.001 ( 0.008)	Loss 3.0625e-01 (3.3624e-01)	Acc@1  90.62 ( 89.96)	Acc@5  99.22 ( 99.14)
Epoch: [79][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.0210e-01 (3.3992e-01)	Acc@1  86.72 ( 89.67)	Acc@5  99.22 ( 99.24)
Epoch: [79][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.6467e-01 (3.3818e-01)	Acc@1  92.97 ( 89.71)	Acc@5 100.00 ( 99.24)
Epoch: [79][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.9542e-01 (3.3242e-01)	Acc@1  90.62 ( 89.80)	Acc@5 100.00 ( 99.26)
Epoch: [79][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.9328e-01 (3.3697e-01)	Acc@1  85.94 ( 89.75)	Acc@5  99.22 ( 99.21)
Epoch: [79][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.7734e-01 (3.4086e-01)	Acc@1  91.41 ( 89.80)	Acc@5  98.44 ( 99.13)
Epoch: [79][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.0278e-01 (3.4504e-01)	Acc@1  89.84 ( 89.70)	Acc@5  99.22 ( 99.09)
Epoch: [79][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.0899e-01 (3.4490e-01)	Acc@1  89.06 ( 89.77)	Acc@5  99.22 ( 99.09)
Epoch: [79][100/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.2175e-01 (3.4210e-01)	Acc@1  91.41 ( 89.85)	Acc@5  99.22 ( 99.10)
Epoch: [79][110/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.0466e-01 (3.3903e-01)	Acc@1  91.41 ( 89.98)	Acc@5  99.22 ( 99.12)
Epoch: [79][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6455e-01 (3.4078e-01)	Acc@1  89.06 ( 89.97)	Acc@5  99.22 ( 99.10)
Epoch: [79][130/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2175e-01 (3.4058e-01)	Acc@1  86.72 ( 89.97)	Acc@5  99.22 ( 99.13)
Epoch: [79][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2292e-01 (3.4037e-01)	Acc@1  89.06 ( 89.92)	Acc@5  99.22 ( 99.16)
Epoch: [79][150/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4860e-01 (3.3903e-01)	Acc@1  91.41 ( 89.96)	Acc@5  99.22 ( 99.19)
Epoch: [79][160/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6235e-01 (3.3878e-01)	Acc@1  89.06 ( 89.93)	Acc@5  99.22 ( 99.21)
Epoch: [79][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6109e-01 (3.3770e-01)	Acc@1  94.53 ( 89.99)	Acc@5  99.22 ( 99.21)
Epoch: [79][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8197e-01 (3.3748e-01)	Acc@1  92.97 ( 89.94)	Acc@5 100.00 ( 99.23)
Epoch: [79][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6849e-01 (3.3646e-01)	Acc@1  85.16 ( 89.96)	Acc@5  99.22 ( 99.24)
Epoch: [79][200/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2805e-01 (3.3515e-01)	Acc@1  89.06 ( 90.03)	Acc@5  99.22 ( 99.23)
Epoch: [79][210/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2208e-01 (3.3398e-01)	Acc@1  88.28 ( 90.03)	Acc@5  99.22 ( 99.24)
Epoch: [79][220/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4991e-01 (3.3329e-01)	Acc@1  89.84 ( 90.01)	Acc@5 100.00 ( 99.25)
Epoch: [79][230/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5451e-01 (3.3301e-01)	Acc@1  93.75 ( 89.98)	Acc@5 100.00 ( 99.26)
Epoch: [79][240/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6182e-01 (3.3348e-01)	Acc@1  94.53 ( 89.96)	Acc@5 100.00 ( 99.25)
Epoch: [79][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6707e-01 (3.3413e-01)	Acc@1  91.41 ( 89.98)	Acc@5  99.22 ( 99.24)
Epoch: [79][260/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9513e-01 (3.3332e-01)	Acc@1  87.50 ( 90.00)	Acc@5  99.22 ( 99.25)
Epoch: [79][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2108e-01 (3.3374e-01)	Acc@1  89.84 ( 89.98)	Acc@5 100.00 ( 99.26)
Epoch: [79][280/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6533e-01 (3.3298e-01)	Acc@1  91.41 ( 90.01)	Acc@5 100.00 ( 99.27)
Epoch: [79][290/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4193e-01 (3.3245e-01)	Acc@1  89.84 ( 90.03)	Acc@5  98.44 ( 99.28)
Epoch: [79][300/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4915e-01 (3.3252e-01)	Acc@1  88.28 ( 90.04)	Acc@5  98.44 ( 99.27)
Epoch: [79][310/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4445e-01 (3.3254e-01)	Acc@1  89.06 ( 90.03)	Acc@5  99.22 ( 99.26)
Epoch: [79][320/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7039e-01 (3.3317e-01)	Acc@1  92.97 ( 90.02)	Acc@5 100.00 ( 99.26)
Epoch: [79][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6382e-01 (3.3396e-01)	Acc@1  94.53 ( 90.00)	Acc@5  99.22 ( 99.25)
Epoch: [79][340/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8755e-01 (3.3477e-01)	Acc@1  86.72 ( 89.99)	Acc@5 100.00 ( 99.26)
Epoch: [79][350/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6866e-01 (3.3449e-01)	Acc@1  89.84 ( 90.02)	Acc@5  99.22 ( 99.25)
Epoch: [79][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0447e-01 (3.3477e-01)	Acc@1  90.62 ( 90.01)	Acc@5 100.00 ( 99.26)
Epoch: [79][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6904e-01 (3.3575e-01)	Acc@1  89.84 ( 89.98)	Acc@5  97.66 ( 99.25)
Epoch: [79][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2040e-01 (3.3596e-01)	Acc@1  89.06 ( 89.98)	Acc@5 100.00 ( 99.25)
Epoch: [79][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.4766e-01 (3.3640e-01)	Acc@1  83.75 ( 89.95)	Acc@5  98.75 ( 99.25)
## e[79] optimizer.zero_grad (sum) time: 0.4095311164855957
## e[79]       loss.backward (sum) time: 6.994468927383423
## e[79]      optimizer.step (sum) time: 3.4910097122192383
## epoch[79] training(only) time: 25.614440202713013
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.2979e+00 (1.2979e+00)	Acc@1  69.00 ( 69.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.035 ( 0.040)	Loss 1.3363e+00 (1.3139e+00)	Acc@1  65.00 ( 66.91)	Acc@5  91.00 ( 89.73)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 1.3429e+00 (1.2580e+00)	Acc@1  67.00 ( 67.86)	Acc@5  93.00 ( 90.95)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6555e+00 (1.2954e+00)	Acc@1  60.00 ( 67.48)	Acc@5  87.00 ( 90.58)
Test: [ 40/100]	Time  0.027 ( 0.030)	Loss 1.1930e+00 (1.2804e+00)	Acc@1  69.00 ( 67.24)	Acc@5  94.00 ( 91.12)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.3270e+00 (1.2997e+00)	Acc@1  73.00 ( 67.02)	Acc@5  89.00 ( 90.80)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3641e+00 (1.2877e+00)	Acc@1  64.00 ( 67.26)	Acc@5  91.00 ( 90.87)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.1752e+00 (1.2918e+00)	Acc@1  65.00 ( 67.25)	Acc@5  94.00 ( 90.89)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4203e+00 (1.3020e+00)	Acc@1  65.00 ( 67.12)	Acc@5  91.00 ( 90.68)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9814e+00 (1.2950e+00)	Acc@1  59.00 ( 67.42)	Acc@5  84.00 ( 90.70)
 * Acc@1 67.420 Acc@5 90.700
### epoch[79] execution time: 28.471275329589844
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.222 ( 0.222)	Data  0.149 ( 0.149)	Loss 2.8245e-01 (2.8245e-01)	Acc@1  92.19 ( 92.19)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.015)	Loss 3.5020e-01 (3.0069e-01)	Acc@1  90.62 ( 91.12)	Acc@5  98.44 ( 99.36)
Epoch: [80][ 20/391]	Time  0.064 ( 0.073)	Data  0.001 ( 0.008)	Loss 3.2507e-01 (3.0202e-01)	Acc@1  89.06 ( 90.81)	Acc@5 100.00 ( 99.37)
Epoch: [80][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.5749e-01 (3.2166e-01)	Acc@1  88.28 ( 90.35)	Acc@5  98.44 ( 99.22)
Epoch: [80][ 40/391]	Time  0.063 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.6134e-01 (3.1988e-01)	Acc@1  89.84 ( 90.34)	Acc@5  98.44 ( 99.29)
Epoch: [80][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.9063e-01 (3.2224e-01)	Acc@1  93.75 ( 90.44)	Acc@5  99.22 ( 99.30)
Epoch: [80][ 60/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.7025e-01 (3.2570e-01)	Acc@1  93.75 ( 90.39)	Acc@5 100.00 ( 99.32)
Epoch: [80][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.0730e-01 (3.3020e-01)	Acc@1  91.41 ( 90.27)	Acc@5  96.88 ( 99.26)
Epoch: [80][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8267e-01 (3.2857e-01)	Acc@1  92.19 ( 90.42)	Acc@5 100.00 ( 99.28)
Epoch: [80][ 90/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7798e-01 (3.3158e-01)	Acc@1  91.41 ( 90.34)	Acc@5 100.00 ( 99.26)
Epoch: [80][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6305e-01 (3.3191e-01)	Acc@1  92.97 ( 90.22)	Acc@5  99.22 ( 99.28)
Epoch: [80][110/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.6039e-01 (3.3306e-01)	Acc@1  89.06 ( 90.16)	Acc@5  98.44 ( 99.25)
Epoch: [80][120/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5559e-01 (3.3078e-01)	Acc@1  87.50 ( 90.22)	Acc@5  99.22 ( 99.25)
Epoch: [80][130/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.5241e-01 (3.2933e-01)	Acc@1  91.41 ( 90.30)	Acc@5  98.44 ( 99.25)
Epoch: [80][140/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.4942e-01 (3.3183e-01)	Acc@1  89.84 ( 90.14)	Acc@5  99.22 ( 99.24)
Epoch: [80][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0630e-01 (3.3291e-01)	Acc@1  89.84 ( 90.12)	Acc@5  99.22 ( 99.22)
Epoch: [80][160/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3662e-01 (3.3556e-01)	Acc@1  91.41 ( 90.03)	Acc@5  98.44 ( 99.22)
Epoch: [80][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5171e-01 (3.3464e-01)	Acc@1  88.28 ( 90.07)	Acc@5 100.00 ( 99.21)
Epoch: [80][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3970e-01 (3.3491e-01)	Acc@1  90.62 ( 90.09)	Acc@5  98.44 ( 99.21)
Epoch: [80][190/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6437e-01 (3.3377e-01)	Acc@1  92.97 ( 90.15)	Acc@5  99.22 ( 99.23)
Epoch: [80][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3720e-01 (3.3371e-01)	Acc@1  91.41 ( 90.20)	Acc@5  99.22 ( 99.23)
Epoch: [80][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1198e-01 (3.3319e-01)	Acc@1  90.62 ( 90.23)	Acc@5  99.22 ( 99.21)
Epoch: [80][220/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6234e-01 (3.3166e-01)	Acc@1  95.31 ( 90.29)	Acc@5 100.00 ( 99.24)
Epoch: [80][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4400e-01 (3.3152e-01)	Acc@1  89.84 ( 90.26)	Acc@5 100.00 ( 99.25)
Epoch: [80][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4111e-01 (3.3108e-01)	Acc@1  92.19 ( 90.27)	Acc@5 100.00 ( 99.25)
Epoch: [80][250/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6023e-01 (3.3055e-01)	Acc@1  94.53 ( 90.27)	Acc@5 100.00 ( 99.26)
Epoch: [80][260/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6524e-01 (3.3109e-01)	Acc@1  86.72 ( 90.24)	Acc@5  99.22 ( 99.26)
Epoch: [80][270/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7137e-01 (3.3171e-01)	Acc@1  93.75 ( 90.22)	Acc@5 100.00 ( 99.26)
Epoch: [80][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0958e-01 (3.3121e-01)	Acc@1  86.72 ( 90.20)	Acc@5 100.00 ( 99.26)
Epoch: [80][290/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0373e-01 (3.3185e-01)	Acc@1  86.72 ( 90.15)	Acc@5  98.44 ( 99.26)
Epoch: [80][300/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7286e-01 (3.3255e-01)	Acc@1  90.62 ( 90.13)	Acc@5  97.66 ( 99.26)
Epoch: [80][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0403e-01 (3.3247e-01)	Acc@1  90.62 ( 90.12)	Acc@5 100.00 ( 99.26)
Epoch: [80][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6036e-01 (3.3181e-01)	Acc@1  89.84 ( 90.15)	Acc@5  98.44 ( 99.26)
Epoch: [80][330/391]	Time  0.063 ( 0.066)	Data  0.002 ( 0.002)	Loss 2.9448e-01 (3.3234e-01)	Acc@1  91.41 ( 90.16)	Acc@5 100.00 ( 99.26)
Epoch: [80][340/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2813e-01 (3.3312e-01)	Acc@1  84.38 ( 90.11)	Acc@5  98.44 ( 99.26)
Epoch: [80][350/391]	Time  0.061 ( 0.066)	Data  0.002 ( 0.002)	Loss 3.2675e-01 (3.3323e-01)	Acc@1  90.62 ( 90.10)	Acc@5 100.00 ( 99.27)
Epoch: [80][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0845e-01 (3.3294e-01)	Acc@1  89.84 ( 90.11)	Acc@5 100.00 ( 99.28)
Epoch: [80][370/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4392e-01 (3.3279e-01)	Acc@1  89.84 ( 90.12)	Acc@5 100.00 ( 99.29)
Epoch: [80][380/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6860e-01 (3.3277e-01)	Acc@1  85.94 ( 90.10)	Acc@5 100.00 ( 99.29)
Epoch: [80][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7861e-01 (3.3338e-01)	Acc@1  85.00 ( 90.06)	Acc@5 100.00 ( 99.28)
## e[80] optimizer.zero_grad (sum) time: 0.409623384475708
## e[80]       loss.backward (sum) time: 6.983098268508911
## e[80]      optimizer.step (sum) time: 3.532689332962036
## epoch[80] training(only) time: 25.68505620956421
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.3073e+00 (1.3073e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3637e+00 (1.3067e+00)	Acc@1  66.00 ( 67.73)	Acc@5  91.00 ( 89.73)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3568e+00 (1.2582e+00)	Acc@1  69.00 ( 68.29)	Acc@5  92.00 ( 90.76)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.6634e+00 (1.2985e+00)	Acc@1  64.00 ( 67.61)	Acc@5  86.00 ( 90.35)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1701e+00 (1.2829e+00)	Acc@1  68.00 ( 67.24)	Acc@5  93.00 ( 91.00)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.3134e+00 (1.3046e+00)	Acc@1  73.00 ( 67.02)	Acc@5  89.00 ( 90.71)
Test: [ 60/100]	Time  0.029 ( 0.028)	Loss 1.3032e+00 (1.2901e+00)	Acc@1  64.00 ( 67.26)	Acc@5  91.00 ( 90.74)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1530e+00 (1.2951e+00)	Acc@1  70.00 ( 67.31)	Acc@5  94.00 ( 90.76)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4157e+00 (1.3057e+00)	Acc@1  67.00 ( 67.15)	Acc@5  91.00 ( 90.56)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 1.9724e+00 (1.2986e+00)	Acc@1  56.00 ( 67.34)	Acc@5  84.00 ( 90.57)
 * Acc@1 67.210 Acc@5 90.510
### epoch[80] execution time: 28.499940156936646
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.210 ( 0.210)	Data  0.142 ( 0.142)	Loss 3.2561e-01 (3.2561e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [81][ 10/391]	Time  0.064 ( 0.078)	Data  0.001 ( 0.014)	Loss 4.0024e-01 (3.3443e-01)	Acc@1  89.06 ( 89.28)	Acc@5  98.44 ( 99.43)
Epoch: [81][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 3.4296e-01 (3.3305e-01)	Acc@1  89.84 ( 89.92)	Acc@5 100.00 ( 99.52)
Epoch: [81][ 30/391]	Time  0.066 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.1400e-01 (3.3523e-01)	Acc@1  91.41 ( 89.99)	Acc@5  97.66 ( 99.29)
Epoch: [81][ 40/391]	Time  0.069 ( 0.068)	Data  0.001 ( 0.005)	Loss 4.3198e-01 (3.3876e-01)	Acc@1  88.28 ( 90.07)	Acc@5  97.66 ( 99.20)
Epoch: [81][ 50/391]	Time  0.062 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.8005e-01 (3.3557e-01)	Acc@1  87.50 ( 90.00)	Acc@5  99.22 ( 99.26)
Epoch: [81][ 60/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.3689e-01 (3.3602e-01)	Acc@1  92.19 ( 90.05)	Acc@5  99.22 ( 99.27)
Epoch: [81][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.1294e-01 (3.3485e-01)	Acc@1  91.41 ( 90.15)	Acc@5  99.22 ( 99.27)
Epoch: [81][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.1572e-01 (3.3373e-01)	Acc@1  89.84 ( 90.08)	Acc@5 100.00 ( 99.31)
Epoch: [81][ 90/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.4910e-01 (3.3036e-01)	Acc@1  89.06 ( 90.12)	Acc@5  99.22 ( 99.32)
Epoch: [81][100/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.3694e-01 (3.3025e-01)	Acc@1  93.75 ( 90.07)	Acc@5 100.00 ( 99.33)
Epoch: [81][110/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4076e-01 (3.3362e-01)	Acc@1  89.84 ( 89.93)	Acc@5  97.66 ( 99.30)
Epoch: [81][120/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0373e-01 (3.3524e-01)	Acc@1  92.19 ( 89.95)	Acc@5  97.66 ( 99.24)
Epoch: [81][130/391]	Time  0.060 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3249e-01 (3.3529e-01)	Acc@1  87.50 ( 89.93)	Acc@5  99.22 ( 99.26)
Epoch: [81][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9773e-01 (3.3408e-01)	Acc@1  92.97 ( 89.96)	Acc@5  99.22 ( 99.26)
Epoch: [81][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9562e-01 (3.3342e-01)	Acc@1  95.31 ( 90.01)	Acc@5  99.22 ( 99.28)
Epoch: [81][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5832e-01 (3.3262e-01)	Acc@1  89.84 ( 90.05)	Acc@5  96.88 ( 99.27)
Epoch: [81][170/391]	Time  0.070 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1068e-01 (3.3359e-01)	Acc@1  89.84 ( 90.01)	Acc@5  97.66 ( 99.26)
Epoch: [81][180/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5559e-01 (3.3313e-01)	Acc@1  89.84 ( 90.03)	Acc@5  98.44 ( 99.27)
Epoch: [81][190/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9829e-01 (3.3127e-01)	Acc@1  92.19 ( 90.12)	Acc@5 100.00 ( 99.28)
Epoch: [81][200/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.0666e-01 (3.3143e-01)	Acc@1  85.94 ( 90.13)	Acc@5  96.09 ( 99.26)
Epoch: [81][210/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.3733e-01 (3.3231e-01)	Acc@1  89.06 ( 90.09)	Acc@5  99.22 ( 99.25)
Epoch: [81][220/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8562e-01 (3.3266e-01)	Acc@1  90.62 ( 90.08)	Acc@5  97.66 ( 99.25)
Epoch: [81][230/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2625e-01 (3.3160e-01)	Acc@1  85.94 ( 90.14)	Acc@5  98.44 ( 99.27)
Epoch: [81][240/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7766e-01 (3.3157e-01)	Acc@1  89.84 ( 90.11)	Acc@5  98.44 ( 99.27)
Epoch: [81][250/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0924e-01 (3.3059e-01)	Acc@1  87.50 ( 90.13)	Acc@5  99.22 ( 99.27)
Epoch: [81][260/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.7302e-01 (3.2979e-01)	Acc@1  90.62 ( 90.16)	Acc@5 100.00 ( 99.28)
Epoch: [81][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8415e-01 (3.3031e-01)	Acc@1  86.72 ( 90.11)	Acc@5  99.22 ( 99.26)
Epoch: [81][280/391]	Time  0.070 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4637e-01 (3.3010e-01)	Acc@1  92.97 ( 90.09)	Acc@5 100.00 ( 99.28)
Epoch: [81][290/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2247e-01 (3.2954e-01)	Acc@1  90.62 ( 90.13)	Acc@5  98.44 ( 99.27)
Epoch: [81][300/391]	Time  0.064 ( 0.065)	Data  0.002 ( 0.002)	Loss 4.2118e-01 (3.3000e-01)	Acc@1  90.62 ( 90.12)	Acc@5  98.44 ( 99.27)
Epoch: [81][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2775e-01 (3.3021e-01)	Acc@1  89.84 ( 90.10)	Acc@5  98.44 ( 99.27)
Epoch: [81][320/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.0724e-01 (3.2928e-01)	Acc@1  94.53 ( 90.10)	Acc@5 100.00 ( 99.28)
Epoch: [81][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4574e-01 (3.3037e-01)	Acc@1  94.53 ( 90.05)	Acc@5 100.00 ( 99.28)
Epoch: [81][340/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9602e-01 (3.3079e-01)	Acc@1  89.84 ( 90.07)	Acc@5 100.00 ( 99.27)
Epoch: [81][350/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9551e-01 (3.3057e-01)	Acc@1  92.19 ( 90.09)	Acc@5  99.22 ( 99.27)
Epoch: [81][360/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1352e-01 (3.3113e-01)	Acc@1  91.41 ( 90.08)	Acc@5  99.22 ( 99.26)
Epoch: [81][370/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8721e-01 (3.3136e-01)	Acc@1  92.19 ( 90.08)	Acc@5  99.22 ( 99.25)
Epoch: [81][380/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0713e-01 (3.3188e-01)	Acc@1  89.06 ( 90.07)	Acc@5  99.22 ( 99.25)
Epoch: [81][390/391]	Time  0.048 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.9235e-01 (3.3222e-01)	Acc@1  88.75 ( 90.06)	Acc@5  98.75 ( 99.25)
## e[81] optimizer.zero_grad (sum) time: 0.41022467613220215
## e[81]       loss.backward (sum) time: 6.988233804702759
## e[81]      optimizer.step (sum) time: 3.375960111618042
## epoch[81] training(only) time: 25.505406141281128
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.3205e+00 (1.3205e+00)	Acc@1  68.00 ( 68.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.027 ( 0.039)	Loss 1.3718e+00 (1.3106e+00)	Acc@1  67.00 ( 68.00)	Acc@5  90.00 ( 89.55)
Test: [ 20/100]	Time  0.027 ( 0.034)	Loss 1.3585e+00 (1.2569e+00)	Acc@1  67.00 ( 68.43)	Acc@5  92.00 ( 90.71)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 1.6671e+00 (1.2979e+00)	Acc@1  61.00 ( 67.87)	Acc@5  88.00 ( 90.42)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.1726e+00 (1.2820e+00)	Acc@1  70.00 ( 67.56)	Acc@5  94.00 ( 90.98)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.3146e+00 (1.3045e+00)	Acc@1  72.00 ( 67.39)	Acc@5  89.00 ( 90.69)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3376e+00 (1.2908e+00)	Acc@1  64.00 ( 67.61)	Acc@5  91.00 ( 90.75)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1655e+00 (1.2976e+00)	Acc@1  68.00 ( 67.58)	Acc@5  95.00 ( 90.80)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.4514e+00 (1.3069e+00)	Acc@1  65.00 ( 67.37)	Acc@5  91.00 ( 90.60)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0145e+00 (1.3001e+00)	Acc@1  58.00 ( 67.47)	Acc@5  85.00 ( 90.66)
 * Acc@1 67.460 Acc@5 90.720
### epoch[81] execution time: 28.34352421760559
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.226 ( 0.226)	Data  0.155 ( 0.155)	Loss 3.7223e-01 (3.7223e-01)	Acc@1  92.97 ( 92.97)	Acc@5  98.44 ( 98.44)
Epoch: [82][ 10/391]	Time  0.066 ( 0.080)	Data  0.001 ( 0.015)	Loss 3.0467e-01 (3.5365e-01)	Acc@1  90.62 ( 89.70)	Acc@5  99.22 ( 98.93)
Epoch: [82][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.009)	Loss 4.1476e-01 (3.5699e-01)	Acc@1  88.28 ( 89.58)	Acc@5  99.22 ( 98.88)
Epoch: [82][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.4065e-01 (3.4598e-01)	Acc@1  95.31 ( 89.77)	Acc@5 100.00 ( 99.07)
Epoch: [82][ 40/391]	Time  0.065 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.0304e-01 (3.4142e-01)	Acc@1  92.19 ( 90.11)	Acc@5 100.00 ( 99.09)
Epoch: [82][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.9530e-01 (3.3264e-01)	Acc@1  89.84 ( 90.21)	Acc@5 100.00 ( 99.22)
Epoch: [82][ 60/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.0821e-01 (3.2732e-01)	Acc@1  96.09 ( 90.41)	Acc@5 100.00 ( 99.31)
Epoch: [82][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3714e-01 (3.2421e-01)	Acc@1  92.19 ( 90.45)	Acc@5 100.00 ( 99.34)
Epoch: [82][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.0171e-01 (3.2377e-01)	Acc@1  89.84 ( 90.38)	Acc@5  99.22 ( 99.32)
Epoch: [82][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6633e-01 (3.2793e-01)	Acc@1  91.41 ( 90.32)	Acc@5 100.00 ( 99.25)
Epoch: [82][100/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.9735e-01 (3.2831e-01)	Acc@1  88.28 ( 90.32)	Acc@5 100.00 ( 99.29)
Epoch: [82][110/391]	Time  0.068 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.8425e-01 (3.3020e-01)	Acc@1  86.72 ( 90.20)	Acc@5  98.44 ( 99.28)
Epoch: [82][120/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.002)	Loss 4.1932e-01 (3.3060e-01)	Acc@1  87.50 ( 90.17)	Acc@5  99.22 ( 99.26)
Epoch: [82][130/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.002)	Loss 3.3693e-01 (3.3137e-01)	Acc@1  91.41 ( 90.19)	Acc@5  98.44 ( 99.25)
Epoch: [82][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5955e-01 (3.3107e-01)	Acc@1  93.75 ( 90.18)	Acc@5  99.22 ( 99.27)
Epoch: [82][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2591e-01 (3.3279e-01)	Acc@1  92.19 ( 90.15)	Acc@5 100.00 ( 99.26)
Epoch: [82][160/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4224e-01 (3.3321e-01)	Acc@1  94.53 ( 90.14)	Acc@5 100.00 ( 99.26)
Epoch: [82][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4876e-01 (3.3381e-01)	Acc@1  89.06 ( 90.03)	Acc@5 100.00 ( 99.26)
Epoch: [82][180/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7307e-01 (3.3357e-01)	Acc@1  93.75 ( 90.07)	Acc@5 100.00 ( 99.27)
Epoch: [82][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1001e-01 (3.3436e-01)	Acc@1  88.28 ( 90.05)	Acc@5  99.22 ( 99.25)
Epoch: [82][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4965e-01 (3.3387e-01)	Acc@1  88.28 ( 90.07)	Acc@5  98.44 ( 99.25)
Epoch: [82][210/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9173e-01 (3.3392e-01)	Acc@1  90.62 ( 90.04)	Acc@5  99.22 ( 99.25)
Epoch: [82][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2281e-01 (3.3407e-01)	Acc@1  89.84 ( 90.04)	Acc@5  98.44 ( 99.24)
Epoch: [82][230/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6048e-01 (3.3552e-01)	Acc@1  87.50 ( 90.02)	Acc@5 100.00 ( 99.23)
Epoch: [82][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3199e-01 (3.3530e-01)	Acc@1  85.94 ( 90.04)	Acc@5  98.44 ( 99.23)
Epoch: [82][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9009e-01 (3.3541e-01)	Acc@1  91.41 ( 90.04)	Acc@5 100.00 ( 99.22)
Epoch: [82][260/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6827e-01 (3.3461e-01)	Acc@1  92.97 ( 90.06)	Acc@5  99.22 ( 99.21)
Epoch: [82][270/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9868e-01 (3.3368e-01)	Acc@1  93.75 ( 90.12)	Acc@5  99.22 ( 99.21)
Epoch: [82][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3092e-01 (3.3331e-01)	Acc@1  87.50 ( 90.12)	Acc@5  99.22 ( 99.21)
Epoch: [82][290/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3161e-01 (3.3381e-01)	Acc@1  94.53 ( 90.11)	Acc@5 100.00 ( 99.21)
Epoch: [82][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9714e-01 (3.3403e-01)	Acc@1  86.72 ( 90.07)	Acc@5 100.00 ( 99.21)
Epoch: [82][310/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1016e-01 (3.3279e-01)	Acc@1  95.31 ( 90.08)	Acc@5  99.22 ( 99.23)
Epoch: [82][320/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3941e-01 (3.3216e-01)	Acc@1  92.97 ( 90.11)	Acc@5 100.00 ( 99.23)
Epoch: [82][330/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2904e-01 (3.3274e-01)	Acc@1  84.38 ( 90.08)	Acc@5  98.44 ( 99.22)
Epoch: [82][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1472e-01 (3.3201e-01)	Acc@1  92.19 ( 90.10)	Acc@5  98.44 ( 99.22)
Epoch: [82][350/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1442e-01 (3.3172e-01)	Acc@1  88.28 ( 90.11)	Acc@5  97.66 ( 99.23)
Epoch: [82][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8774e-01 (3.3198e-01)	Acc@1  89.06 ( 90.12)	Acc@5  99.22 ( 99.22)
Epoch: [82][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4882e-01 (3.3204e-01)	Acc@1  93.75 ( 90.14)	Acc@5 100.00 ( 99.23)
Epoch: [82][380/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5486e-01 (3.3174e-01)	Acc@1  90.62 ( 90.15)	Acc@5 100.00 ( 99.23)
Epoch: [82][390/391]	Time  0.049 ( 0.065)	Data  0.001 ( 0.002)	Loss 5.3437e-01 (3.3161e-01)	Acc@1  82.50 ( 90.16)	Acc@5  96.25 ( 99.24)
## e[82] optimizer.zero_grad (sum) time: 0.4123821258544922
## e[82]       loss.backward (sum) time: 6.948525667190552
## e[82]      optimizer.step (sum) time: 3.568448781967163
## epoch[82] training(only) time: 25.65915584564209
# Switched to evaluate mode...
Test: [  0/100]	Time  0.160 ( 0.160)	Loss 1.3442e+00 (1.3442e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3450e+00 (1.3187e+00)	Acc@1  65.00 ( 66.73)	Acc@5  91.00 ( 89.73)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.3409e+00 (1.2611e+00)	Acc@1  66.00 ( 67.71)	Acc@5  92.00 ( 90.81)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.6511e+00 (1.2970e+00)	Acc@1  62.00 ( 67.35)	Acc@5  87.00 ( 90.42)
Test: [ 40/100]	Time  0.026 ( 0.030)	Loss 1.1800e+00 (1.2852e+00)	Acc@1  69.00 ( 67.07)	Acc@5  93.00 ( 90.95)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.3343e+00 (1.3106e+00)	Acc@1  70.00 ( 66.98)	Acc@5  90.00 ( 90.73)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3135e+00 (1.2997e+00)	Acc@1  65.00 ( 67.26)	Acc@5  91.00 ( 90.70)
Test: [ 70/100]	Time  0.025 ( 0.029)	Loss 1.1565e+00 (1.3055e+00)	Acc@1  67.00 ( 67.20)	Acc@5  95.00 ( 90.73)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.4488e+00 (1.3170e+00)	Acc@1  65.00 ( 67.01)	Acc@5  91.00 ( 90.52)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.9936e+00 (1.3080e+00)	Acc@1  58.00 ( 67.21)	Acc@5  85.00 ( 90.54)
 * Acc@1 67.220 Acc@5 90.540
### epoch[82] execution time: 28.546392679214478
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.218 ( 0.218)	Data  0.147 ( 0.147)	Loss 3.1094e-01 (3.1094e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.063 ( 0.080)	Data  0.001 ( 0.014)	Loss 3.6839e-01 (3.1903e-01)	Acc@1  89.84 ( 90.91)	Acc@5  98.44 ( 99.43)
Epoch: [83][ 20/391]	Time  0.063 ( 0.072)	Data  0.001 ( 0.008)	Loss 4.3577e-01 (3.1838e-01)	Acc@1  86.72 ( 90.81)	Acc@5  97.66 ( 99.29)
Epoch: [83][ 30/391]	Time  0.068 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.6974e-01 (3.2853e-01)	Acc@1  94.53 ( 90.05)	Acc@5  97.66 ( 99.27)
Epoch: [83][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.1946e-01 (3.2037e-01)	Acc@1  89.06 ( 90.26)	Acc@5 100.00 ( 99.35)
Epoch: [83][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.6505e-01 (3.1830e-01)	Acc@1  92.19 ( 90.33)	Acc@5 100.00 ( 99.37)
Epoch: [83][ 60/391]	Time  0.070 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.8996e-01 (3.1497e-01)	Acc@1  90.62 ( 90.51)	Acc@5 100.00 ( 99.44)
Epoch: [83][ 70/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.3076e-01 (3.1772e-01)	Acc@1  89.84 ( 90.45)	Acc@5 100.00 ( 99.38)
Epoch: [83][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.2015e-01 (3.1614e-01)	Acc@1  89.84 ( 90.56)	Acc@5 100.00 ( 99.33)
Epoch: [83][ 90/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6557e-01 (3.1773e-01)	Acc@1  91.41 ( 90.47)	Acc@5  99.22 ( 99.31)
Epoch: [83][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6264e-01 (3.1651e-01)	Acc@1  93.75 ( 90.52)	Acc@5 100.00 ( 99.34)
Epoch: [83][110/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.5383e-01 (3.1542e-01)	Acc@1  88.28 ( 90.55)	Acc@5 100.00 ( 99.35)
Epoch: [83][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8947e-01 (3.1741e-01)	Acc@1  86.72 ( 90.50)	Acc@5 100.00 ( 99.34)
Epoch: [83][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.0063e-01 (3.1829e-01)	Acc@1  82.03 ( 90.42)	Acc@5  97.66 ( 99.33)
Epoch: [83][140/391]	Time  0.064 ( 0.066)	Data  0.002 ( 0.002)	Loss 3.2054e-01 (3.1967e-01)	Acc@1  89.84 ( 90.36)	Acc@5  99.22 ( 99.33)
Epoch: [83][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7130e-01 (3.2192e-01)	Acc@1  87.50 ( 90.30)	Acc@5 100.00 ( 99.34)
Epoch: [83][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2932e-01 (3.2131e-01)	Acc@1  92.97 ( 90.37)	Acc@5  99.22 ( 99.31)
Epoch: [83][170/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0993e-01 (3.2019e-01)	Acc@1  91.41 ( 90.38)	Acc@5  99.22 ( 99.34)
Epoch: [83][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7602e-01 (3.2027e-01)	Acc@1  91.41 ( 90.38)	Acc@5 100.00 ( 99.34)
Epoch: [83][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7533e-01 (3.2234e-01)	Acc@1  88.28 ( 90.30)	Acc@5  98.44 ( 99.33)
Epoch: [83][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7059e-01 (3.2256e-01)	Acc@1  89.06 ( 90.26)	Acc@5 100.00 ( 99.34)
Epoch: [83][210/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0948e-01 (3.2368e-01)	Acc@1  90.62 ( 90.21)	Acc@5 100.00 ( 99.32)
Epoch: [83][220/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0134e-01 (3.2444e-01)	Acc@1  89.06 ( 90.20)	Acc@5  99.22 ( 99.32)
Epoch: [83][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7377e-01 (3.2731e-01)	Acc@1  86.72 ( 90.10)	Acc@5 100.00 ( 99.30)
Epoch: [83][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2426e-01 (3.2719e-01)	Acc@1  92.19 ( 90.09)	Acc@5 100.00 ( 99.29)
Epoch: [83][250/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3247e-01 (3.2676e-01)	Acc@1  89.84 ( 90.14)	Acc@5 100.00 ( 99.30)
Epoch: [83][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4136e-01 (3.2679e-01)	Acc@1  89.06 ( 90.13)	Acc@5  98.44 ( 99.29)
Epoch: [83][270/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6894e-01 (3.2728e-01)	Acc@1  85.94 ( 90.11)	Acc@5  98.44 ( 99.27)
Epoch: [83][280/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3021e-01 (3.2761e-01)	Acc@1  88.28 ( 90.12)	Acc@5 100.00 ( 99.25)
Epoch: [83][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8062e-01 (3.2785e-01)	Acc@1  89.06 ( 90.12)	Acc@5  98.44 ( 99.25)
Epoch: [83][300/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0729e-01 (3.2790e-01)	Acc@1  92.97 ( 90.12)	Acc@5 100.00 ( 99.26)
Epoch: [83][310/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0953e-01 (3.2818e-01)	Acc@1  90.62 ( 90.10)	Acc@5 100.00 ( 99.26)
Epoch: [83][320/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6681e-01 (3.2855e-01)	Acc@1  88.28 ( 90.08)	Acc@5  99.22 ( 99.25)
Epoch: [83][330/391]	Time  0.064 ( 0.066)	Data  0.002 ( 0.002)	Loss 3.0997e-01 (3.2872e-01)	Acc@1  92.19 ( 90.10)	Acc@5  98.44 ( 99.24)
Epoch: [83][340/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5184e-01 (3.2869e-01)	Acc@1  92.97 ( 90.08)	Acc@5  99.22 ( 99.24)
Epoch: [83][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8289e-01 (3.2822e-01)	Acc@1  87.50 ( 90.07)	Acc@5 100.00 ( 99.25)
Epoch: [83][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0243e-01 (3.2841e-01)	Acc@1  90.62 ( 90.06)	Acc@5 100.00 ( 99.25)
Epoch: [83][370/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0896e-01 (3.2841e-01)	Acc@1  90.62 ( 90.06)	Acc@5  98.44 ( 99.25)
Epoch: [83][380/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4979e-01 (3.2847e-01)	Acc@1  90.62 ( 90.05)	Acc@5 100.00 ( 99.25)
Epoch: [83][390/391]	Time  0.050 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.8440e-01 (3.2894e-01)	Acc@1  85.00 ( 90.03)	Acc@5 100.00 ( 99.25)
## e[83] optimizer.zero_grad (sum) time: 0.40723371505737305
## e[83]       loss.backward (sum) time: 6.974326848983765
## e[83]      optimizer.step (sum) time: 3.5480618476867676
## epoch[83] training(only) time: 25.634274005889893
# Switched to evaluate mode...
Test: [  0/100]	Time  0.161 ( 0.161)	Loss 1.2974e+00 (1.2974e+00)	Acc@1  68.00 ( 68.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.027 ( 0.040)	Loss 1.3247e+00 (1.3145e+00)	Acc@1  67.00 ( 67.73)	Acc@5  91.00 ( 89.45)
Test: [ 20/100]	Time  0.025 ( 0.034)	Loss 1.3690e+00 (1.2644e+00)	Acc@1  68.00 ( 67.81)	Acc@5  93.00 ( 90.71)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6458e+00 (1.3024e+00)	Acc@1  61.00 ( 66.97)	Acc@5  86.00 ( 90.26)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1880e+00 (1.2875e+00)	Acc@1  67.00 ( 66.80)	Acc@5  93.00 ( 90.88)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.3114e+00 (1.3085e+00)	Acc@1  72.00 ( 66.78)	Acc@5  90.00 ( 90.63)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3323e+00 (1.2973e+00)	Acc@1  65.00 ( 67.03)	Acc@5  90.00 ( 90.61)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.1675e+00 (1.3038e+00)	Acc@1  68.00 ( 67.15)	Acc@5  94.00 ( 90.65)
Test: [ 80/100]	Time  0.028 ( 0.028)	Loss 1.4558e+00 (1.3145e+00)	Acc@1  65.00 ( 67.00)	Acc@5  91.00 ( 90.47)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0386e+00 (1.3060e+00)	Acc@1  57.00 ( 67.19)	Acc@5  85.00 ( 90.49)
 * Acc@1 67.160 Acc@5 90.540
### epoch[83] execution time: 28.47755217552185
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.217 ( 0.217)	Data  0.148 ( 0.148)	Loss 3.5326e-01 (3.5326e-01)	Acc@1  90.62 ( 90.62)	Acc@5  99.22 ( 99.22)
Epoch: [84][ 10/391]	Time  0.067 ( 0.080)	Data  0.001 ( 0.014)	Loss 3.5068e-01 (3.4852e-01)	Acc@1  90.62 ( 89.91)	Acc@5  98.44 ( 99.01)
Epoch: [84][ 20/391]	Time  0.066 ( 0.073)	Data  0.001 ( 0.008)	Loss 3.1147e-01 (3.3003e-01)	Acc@1  89.84 ( 90.48)	Acc@5  99.22 ( 99.03)
Epoch: [84][ 30/391]	Time  0.063 ( 0.070)	Data  0.001 ( 0.006)	Loss 2.3743e-01 (3.2853e-01)	Acc@1  92.97 ( 90.68)	Acc@5 100.00 ( 98.99)
Epoch: [84][ 40/391]	Time  0.064 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.8407e-01 (3.2064e-01)	Acc@1  89.06 ( 90.95)	Acc@5  99.22 ( 99.10)
Epoch: [84][ 50/391]	Time  0.060 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.9110e-01 (3.2100e-01)	Acc@1  91.41 ( 90.87)	Acc@5 100.00 ( 99.23)
Epoch: [84][ 60/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.5651e-01 (3.1762e-01)	Acc@1  95.31 ( 90.82)	Acc@5 100.00 ( 99.32)
Epoch: [84][ 70/391]	Time  0.070 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.1880e-01 (3.2178e-01)	Acc@1  87.50 ( 90.57)	Acc@5  99.22 ( 99.26)
Epoch: [84][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7095e-01 (3.1899e-01)	Acc@1  92.97 ( 90.66)	Acc@5 100.00 ( 99.27)
Epoch: [84][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.0397e-01 (3.2019e-01)	Acc@1  95.31 ( 90.70)	Acc@5 100.00 ( 99.24)
Epoch: [84][100/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.8584e-01 (3.2135e-01)	Acc@1  91.41 ( 90.62)	Acc@5  99.22 ( 99.25)
Epoch: [84][110/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.6642e-01 (3.2029e-01)	Acc@1  93.75 ( 90.66)	Acc@5 100.00 ( 99.25)
Epoch: [84][120/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5266e-01 (3.1946e-01)	Acc@1  90.62 ( 90.76)	Acc@5  99.22 ( 99.26)
Epoch: [84][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2129e-01 (3.2010e-01)	Acc@1  85.16 ( 90.67)	Acc@5  98.44 ( 99.27)
Epoch: [84][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5307e-01 (3.1977e-01)	Acc@1  92.19 ( 90.64)	Acc@5 100.00 ( 99.27)
Epoch: [84][150/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3324e-01 (3.2060e-01)	Acc@1  89.06 ( 90.61)	Acc@5 100.00 ( 99.28)
Epoch: [84][160/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0101e-01 (3.1797e-01)	Acc@1  87.50 ( 90.67)	Acc@5  99.22 ( 99.29)
Epoch: [84][170/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5629e-01 (3.1806e-01)	Acc@1  88.28 ( 90.65)	Acc@5  98.44 ( 99.28)
Epoch: [84][180/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7698e-01 (3.1800e-01)	Acc@1  89.06 ( 90.66)	Acc@5 100.00 ( 99.27)
Epoch: [84][190/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5191e-01 (3.1912e-01)	Acc@1  85.16 ( 90.60)	Acc@5 100.00 ( 99.29)
Epoch: [84][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8155e-01 (3.1953e-01)	Acc@1  87.50 ( 90.52)	Acc@5  99.22 ( 99.30)
Epoch: [84][210/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8192e-01 (3.1890e-01)	Acc@1  94.53 ( 90.55)	Acc@5 100.00 ( 99.32)
Epoch: [84][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0964e-01 (3.1909e-01)	Acc@1  85.16 ( 90.55)	Acc@5  98.44 ( 99.31)
Epoch: [84][230/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2423e-01 (3.1859e-01)	Acc@1  90.62 ( 90.58)	Acc@5 100.00 ( 99.32)
Epoch: [84][240/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5311e-01 (3.1954e-01)	Acc@1  89.84 ( 90.54)	Acc@5  99.22 ( 99.33)
Epoch: [84][250/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9347e-01 (3.2062e-01)	Acc@1  89.84 ( 90.51)	Acc@5  99.22 ( 99.32)
Epoch: [84][260/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9170e-01 (3.1951e-01)	Acc@1  89.84 ( 90.53)	Acc@5 100.00 ( 99.32)
Epoch: [84][270/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3021e-01 (3.1879e-01)	Acc@1  89.84 ( 90.55)	Acc@5  98.44 ( 99.33)
Epoch: [84][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5967e-01 (3.1941e-01)	Acc@1  86.72 ( 90.49)	Acc@5  99.22 ( 99.33)
Epoch: [84][290/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2400e-01 (3.1912e-01)	Acc@1  88.28 ( 90.51)	Acc@5  99.22 ( 99.33)
Epoch: [84][300/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8812e-01 (3.1944e-01)	Acc@1  90.62 ( 90.48)	Acc@5 100.00 ( 99.33)
Epoch: [84][310/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4571e-01 (3.2017e-01)	Acc@1  94.53 ( 90.48)	Acc@5  98.44 ( 99.32)
Epoch: [84][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3688e-01 (3.2017e-01)	Acc@1  92.97 ( 90.46)	Acc@5  99.22 ( 99.33)
Epoch: [84][330/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2620e-01 (3.1991e-01)	Acc@1  91.41 ( 90.47)	Acc@5  97.66 ( 99.33)
Epoch: [84][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4843e-01 (3.1995e-01)	Acc@1  94.53 ( 90.45)	Acc@5  99.22 ( 99.33)
Epoch: [84][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5666e-01 (3.2048e-01)	Acc@1  84.38 ( 90.41)	Acc@5 100.00 ( 99.33)
Epoch: [84][360/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1869e-01 (3.2085e-01)	Acc@1  84.38 ( 90.39)	Acc@5 100.00 ( 99.32)
Epoch: [84][370/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1577e-01 (3.2136e-01)	Acc@1  91.41 ( 90.37)	Acc@5  99.22 ( 99.31)
Epoch: [84][380/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3671e-01 (3.2251e-01)	Acc@1  87.50 ( 90.32)	Acc@5  99.22 ( 99.31)
Epoch: [84][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.7515e-01 (3.2292e-01)	Acc@1  82.50 ( 90.31)	Acc@5  98.75 ( 99.31)
## e[84] optimizer.zero_grad (sum) time: 0.4099907875061035
## e[84]       loss.backward (sum) time: 6.9712255001068115
## e[84]      optimizer.step (sum) time: 3.488976240158081
## epoch[84] training(only) time: 25.636019706726074
# Switched to evaluate mode...
Test: [  0/100]	Time  0.164 ( 0.164)	Loss 1.3111e+00 (1.3111e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3502e+00 (1.3261e+00)	Acc@1  66.00 ( 68.00)	Acc@5  91.00 ( 89.64)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.4537e+00 (1.2741e+00)	Acc@1  67.00 ( 67.95)	Acc@5  93.00 ( 90.95)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6763e+00 (1.3071e+00)	Acc@1  60.00 ( 67.26)	Acc@5  87.00 ( 90.42)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1987e+00 (1.2921e+00)	Acc@1  66.00 ( 66.85)	Acc@5  94.00 ( 91.05)
Test: [ 50/100]	Time  0.029 ( 0.029)	Loss 1.3135e+00 (1.3149e+00)	Acc@1  71.00 ( 66.69)	Acc@5  89.00 ( 90.75)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3357e+00 (1.3044e+00)	Acc@1  64.00 ( 67.13)	Acc@5  91.00 ( 90.75)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1999e+00 (1.3104e+00)	Acc@1  69.00 ( 67.23)	Acc@5  94.00 ( 90.66)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.4453e+00 (1.3204e+00)	Acc@1  64.00 ( 67.01)	Acc@5  90.00 ( 90.44)
Test: [ 90/100]	Time  0.025 ( 0.028)	Loss 2.0016e+00 (1.3111e+00)	Acc@1  57.00 ( 67.31)	Acc@5  85.00 ( 90.53)
 * Acc@1 67.320 Acc@5 90.600
### epoch[84] execution time: 28.469433546066284
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.220 ( 0.220)	Data  0.151 ( 0.151)	Loss 3.3408e-01 (3.3408e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.063 ( 0.079)	Data  0.001 ( 0.015)	Loss 3.9206e-01 (3.2148e-01)	Acc@1  89.84 ( 90.48)	Acc@5  97.66 ( 99.29)
Epoch: [85][ 20/391]	Time  0.062 ( 0.073)	Data  0.001 ( 0.008)	Loss 3.8330e-01 (3.3117e-01)	Acc@1  86.72 ( 90.07)	Acc@5 100.00 ( 99.26)
Epoch: [85][ 30/391]	Time  0.064 ( 0.070)	Data  0.001 ( 0.006)	Loss 4.2428e-01 (3.2557e-01)	Acc@1  87.50 ( 90.50)	Acc@5  97.66 ( 99.14)
Epoch: [85][ 40/391]	Time  0.067 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.7313e-01 (3.2670e-01)	Acc@1  88.28 ( 90.51)	Acc@5  99.22 ( 99.16)
Epoch: [85][ 50/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.1376e-01 (3.2769e-01)	Acc@1  92.19 ( 90.47)	Acc@5 100.00 ( 99.19)
Epoch: [85][ 60/391]	Time  0.065 ( 0.068)	Data  0.001 ( 0.004)	Loss 3.4963e-01 (3.2672e-01)	Acc@1  92.19 ( 90.54)	Acc@5 100.00 ( 99.24)
Epoch: [85][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.1163e-01 (3.1884e-01)	Acc@1  91.41 ( 90.70)	Acc@5 100.00 ( 99.28)
Epoch: [85][ 80/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.3178e-01 (3.1921e-01)	Acc@1  89.06 ( 90.62)	Acc@5  98.44 ( 99.30)
Epoch: [85][ 90/391]	Time  0.072 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.9874e-01 (3.1742e-01)	Acc@1  92.19 ( 90.78)	Acc@5  99.22 ( 99.28)
Epoch: [85][100/391]	Time  0.070 ( 0.067)	Data  0.002 ( 0.003)	Loss 3.2038e-01 (3.1755e-01)	Acc@1  89.84 ( 90.83)	Acc@5  99.22 ( 99.28)
Epoch: [85][110/391]	Time  0.069 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.3100e-01 (3.1985e-01)	Acc@1  96.09 ( 90.82)	Acc@5 100.00 ( 99.24)
Epoch: [85][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9725e-01 (3.2047e-01)	Acc@1  89.06 ( 90.75)	Acc@5  98.44 ( 99.24)
Epoch: [85][130/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0191e-01 (3.1966e-01)	Acc@1  91.41 ( 90.72)	Acc@5 100.00 ( 99.26)
Epoch: [85][140/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9386e-01 (3.2004e-01)	Acc@1  92.19 ( 90.71)	Acc@5 100.00 ( 99.27)
Epoch: [85][150/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4428e-01 (3.2182e-01)	Acc@1  88.28 ( 90.67)	Acc@5 100.00 ( 99.28)
Epoch: [85][160/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4168e-01 (3.2118e-01)	Acc@1  86.72 ( 90.64)	Acc@5 100.00 ( 99.31)
Epoch: [85][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2301e-01 (3.2184e-01)	Acc@1  91.41 ( 90.68)	Acc@5  98.44 ( 99.30)
Epoch: [85][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1738e-01 (3.2136e-01)	Acc@1  90.62 ( 90.71)	Acc@5  99.22 ( 99.30)
Epoch: [85][190/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8952e-01 (3.2111e-01)	Acc@1  92.97 ( 90.71)	Acc@5 100.00 ( 99.30)
Epoch: [85][200/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5521e-01 (3.2217e-01)	Acc@1  86.72 ( 90.62)	Acc@5 100.00 ( 99.30)
Epoch: [85][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0681e-01 (3.2163e-01)	Acc@1  91.41 ( 90.61)	Acc@5  98.44 ( 99.30)
Epoch: [85][220/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3857e-01 (3.2227e-01)	Acc@1  89.84 ( 90.57)	Acc@5 100.00 ( 99.30)
Epoch: [85][230/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2061e-01 (3.2115e-01)	Acc@1  92.19 ( 90.61)	Acc@5 100.00 ( 99.31)
Epoch: [85][240/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4082e-01 (3.2257e-01)	Acc@1  92.19 ( 90.54)	Acc@5 100.00 ( 99.31)
Epoch: [85][250/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5464e-01 (3.2152e-01)	Acc@1  91.41 ( 90.55)	Acc@5 100.00 ( 99.33)
Epoch: [85][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4133e-01 (3.2122e-01)	Acc@1  89.84 ( 90.59)	Acc@5  99.22 ( 99.34)
Epoch: [85][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5822e-01 (3.1965e-01)	Acc@1  92.97 ( 90.62)	Acc@5 100.00 ( 99.35)
Epoch: [85][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.3383e-01 (3.2085e-01)	Acc@1  83.59 ( 90.54)	Acc@5  97.66 ( 99.34)
Epoch: [85][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.0878e-01 (3.2021e-01)	Acc@1  93.75 ( 90.55)	Acc@5 100.00 ( 99.35)
Epoch: [85][300/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1687e-01 (3.2007e-01)	Acc@1  87.50 ( 90.55)	Acc@5  99.22 ( 99.35)
Epoch: [85][310/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8029e-01 (3.1953e-01)	Acc@1  89.84 ( 90.57)	Acc@5  99.22 ( 99.34)
Epoch: [85][320/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0189e-01 (3.1924e-01)	Acc@1  90.62 ( 90.59)	Acc@5 100.00 ( 99.35)
Epoch: [85][330/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9943e-01 (3.1914e-01)	Acc@1  85.94 ( 90.59)	Acc@5  98.44 ( 99.34)
Epoch: [85][340/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1490e-01 (3.1876e-01)	Acc@1  88.28 ( 90.59)	Acc@5  99.22 ( 99.34)
Epoch: [85][350/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5995e-01 (3.1829e-01)	Acc@1  93.75 ( 90.61)	Acc@5  99.22 ( 99.34)
Epoch: [85][360/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.6214e-01 (3.1863e-01)	Acc@1  88.28 ( 90.59)	Acc@5 100.00 ( 99.35)
Epoch: [85][370/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9628e-01 (3.1890e-01)	Acc@1  92.19 ( 90.57)	Acc@5 100.00 ( 99.35)
Epoch: [85][380/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1903e-01 (3.1922e-01)	Acc@1  86.72 ( 90.55)	Acc@5 100.00 ( 99.35)
Epoch: [85][390/391]	Time  0.053 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2187e-01 (3.1979e-01)	Acc@1  88.75 ( 90.51)	Acc@5 100.00 ( 99.34)
## e[85] optimizer.zero_grad (sum) time: 0.4084916114807129
## e[85]       loss.backward (sum) time: 7.029035329818726
## e[85]      optimizer.step (sum) time: 3.4586338996887207
## epoch[85] training(only) time: 25.67957878112793
# Switched to evaluate mode...
Test: [  0/100]	Time  0.162 ( 0.162)	Loss 1.3017e+00 (1.3017e+00)	Acc@1  70.00 ( 70.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.3531e+00 (1.3180e+00)	Acc@1  65.00 ( 67.73)	Acc@5  91.00 ( 89.18)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.4056e+00 (1.2654e+00)	Acc@1  69.00 ( 68.24)	Acc@5  93.00 ( 90.71)
Test: [ 30/100]	Time  0.028 ( 0.031)	Loss 1.6761e+00 (1.2982e+00)	Acc@1  60.00 ( 67.55)	Acc@5  88.00 ( 90.29)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2033e+00 (1.2846e+00)	Acc@1  69.00 ( 67.32)	Acc@5  92.00 ( 90.76)
Test: [ 50/100]	Time  0.026 ( 0.029)	Loss 1.3111e+00 (1.3094e+00)	Acc@1  72.00 ( 66.98)	Acc@5  89.00 ( 90.39)
Test: [ 60/100]	Time  0.026 ( 0.029)	Loss 1.3142e+00 (1.2982e+00)	Acc@1  64.00 ( 67.23)	Acc@5  91.00 ( 90.46)
Test: [ 70/100]	Time  0.026 ( 0.028)	Loss 1.1474e+00 (1.3056e+00)	Acc@1  67.00 ( 67.21)	Acc@5  94.00 ( 90.48)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.4660e+00 (1.3159e+00)	Acc@1  65.00 ( 67.02)	Acc@5  91.00 ( 90.30)
Test: [ 90/100]	Time  0.027 ( 0.028)	Loss 1.9804e+00 (1.3068e+00)	Acc@1  55.00 ( 67.26)	Acc@5  85.00 ( 90.40)
 * Acc@1 67.260 Acc@5 90.460
### epoch[85] execution time: 28.517536401748657
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.220 ( 0.220)	Data  0.150 ( 0.150)	Loss 3.0558e-01 (3.0558e-01)	Acc@1  91.41 ( 91.41)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.065 ( 0.078)	Data  0.001 ( 0.015)	Loss 2.6691e-01 (2.6883e-01)	Acc@1  94.53 ( 92.97)	Acc@5  99.22 ( 99.72)
Epoch: [86][ 20/391]	Time  0.068 ( 0.072)	Data  0.001 ( 0.008)	Loss 1.6564e-01 (2.7846e-01)	Acc@1  96.88 ( 92.26)	Acc@5 100.00 ( 99.59)
Epoch: [86][ 30/391]	Time  0.067 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.1326e-01 (2.8418e-01)	Acc@1  89.84 ( 91.78)	Acc@5  99.22 ( 99.55)
Epoch: [86][ 40/391]	Time  0.062 ( 0.069)	Data  0.001 ( 0.005)	Loss 2.7069e-01 (2.9125e-01)	Acc@1  92.19 ( 91.46)	Acc@5 100.00 ( 99.56)
Epoch: [86][ 50/391]	Time  0.068 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.7316e-01 (2.9662e-01)	Acc@1  90.62 ( 91.30)	Acc@5 100.00 ( 99.56)
Epoch: [86][ 60/391]	Time  0.064 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.8689e-01 (3.0683e-01)	Acc@1  92.19 ( 90.87)	Acc@5  99.22 ( 99.51)
Epoch: [86][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.1416e-01 (3.0782e-01)	Acc@1  95.31 ( 90.88)	Acc@5 100.00 ( 99.50)
Epoch: [86][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.2444e-01 (3.0708e-01)	Acc@1  92.19 ( 90.90)	Acc@5 100.00 ( 99.49)
Epoch: [86][ 90/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.9662e-01 (3.1027e-01)	Acc@1  82.81 ( 90.76)	Acc@5  98.44 ( 99.42)
Epoch: [86][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.6893e-01 (3.0975e-01)	Acc@1  91.41 ( 90.74)	Acc@5 100.00 ( 99.43)
Epoch: [86][110/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2182e-01 (3.0929e-01)	Acc@1  85.94 ( 90.79)	Acc@5  98.44 ( 99.40)
Epoch: [86][120/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2172e-01 (3.0838e-01)	Acc@1  90.62 ( 90.84)	Acc@5 100.00 ( 99.41)
Epoch: [86][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4767e-01 (3.0984e-01)	Acc@1  87.50 ( 90.76)	Acc@5  99.22 ( 99.41)
Epoch: [86][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.1575e-01 (3.1037e-01)	Acc@1  94.53 ( 90.75)	Acc@5 100.00 ( 99.40)
Epoch: [86][150/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2376e-01 (3.1091e-01)	Acc@1  91.41 ( 90.76)	Acc@5  99.22 ( 99.38)
Epoch: [86][160/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0107e-01 (3.1138e-01)	Acc@1  90.62 ( 90.78)	Acc@5 100.00 ( 99.40)
Epoch: [86][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0432e-01 (3.1284e-01)	Acc@1  84.38 ( 90.70)	Acc@5 100.00 ( 99.39)
Epoch: [86][180/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.9906e-01 (3.1498e-01)	Acc@1  88.28 ( 90.59)	Acc@5  98.44 ( 99.40)
Epoch: [86][190/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3329e-01 (3.1484e-01)	Acc@1  95.31 ( 90.62)	Acc@5  99.22 ( 99.41)
Epoch: [86][200/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3932e-01 (3.1400e-01)	Acc@1  92.97 ( 90.64)	Acc@5 100.00 ( 99.41)
Epoch: [86][210/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 5.3616e-01 (3.1680e-01)	Acc@1  82.81 ( 90.55)	Acc@5  96.88 ( 99.37)
Epoch: [86][220/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6273e-01 (3.1795e-01)	Acc@1  86.72 ( 90.53)	Acc@5  98.44 ( 99.34)
Epoch: [86][230/391]	Time  0.059 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.2366e-01 (3.1904e-01)	Acc@1  88.28 ( 90.46)	Acc@5  97.66 ( 99.34)
Epoch: [86][240/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0420e-01 (3.1851e-01)	Acc@1  91.41 ( 90.47)	Acc@5 100.00 ( 99.35)
Epoch: [86][250/391]	Time  0.061 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7609e-01 (3.1797e-01)	Acc@1  87.50 ( 90.48)	Acc@5  99.22 ( 99.36)
Epoch: [86][260/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5652e-01 (3.1802e-01)	Acc@1  85.94 ( 90.48)	Acc@5  99.22 ( 99.36)
Epoch: [86][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1785e-01 (3.1845e-01)	Acc@1  89.06 ( 90.45)	Acc@5  99.22 ( 99.34)
Epoch: [86][280/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3642e-01 (3.1764e-01)	Acc@1  88.28 ( 90.47)	Acc@5  98.44 ( 99.33)
Epoch: [86][290/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4396e-01 (3.1898e-01)	Acc@1  89.84 ( 90.45)	Acc@5  99.22 ( 99.32)
Epoch: [86][300/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0816e-01 (3.1957e-01)	Acc@1  89.84 ( 90.42)	Acc@5  99.22 ( 99.31)
Epoch: [86][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7205e-01 (3.1914e-01)	Acc@1  90.62 ( 90.42)	Acc@5  99.22 ( 99.32)
Epoch: [86][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3058e-01 (3.1892e-01)	Acc@1  95.31 ( 90.44)	Acc@5 100.00 ( 99.32)
Epoch: [86][330/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5755e-01 (3.1904e-01)	Acc@1  85.16 ( 90.41)	Acc@5 100.00 ( 99.31)
Epoch: [86][340/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0630e-01 (3.1904e-01)	Acc@1  91.41 ( 90.42)	Acc@5  99.22 ( 99.32)
Epoch: [86][350/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9431e-01 (3.1901e-01)	Acc@1  91.41 ( 90.41)	Acc@5  99.22 ( 99.32)
Epoch: [86][360/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8533e-01 (3.1902e-01)	Acc@1  89.06 ( 90.41)	Acc@5 100.00 ( 99.33)
Epoch: [86][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2218e-01 (3.1937e-01)	Acc@1  86.72 ( 90.39)	Acc@5 100.00 ( 99.33)
Epoch: [86][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.0783e-01 (3.1906e-01)	Acc@1  90.62 ( 90.42)	Acc@5 100.00 ( 99.33)
Epoch: [86][390/391]	Time  0.046 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.0966e-01 (3.1875e-01)	Acc@1  86.25 ( 90.44)	Acc@5 100.00 ( 99.34)
## e[86] optimizer.zero_grad (sum) time: 0.41348886489868164
## e[86]       loss.backward (sum) time: 7.0208899974823
## e[86]      optimizer.step (sum) time: 3.3813841342926025
## epoch[86] training(only) time: 25.586263179779053
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2753e+00 (1.2753e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.028 ( 0.039)	Loss 1.3194e+00 (1.3146e+00)	Acc@1  66.00 ( 67.36)	Acc@5  92.00 ( 89.45)
Test: [ 20/100]	Time  0.026 ( 0.033)	Loss 1.4300e+00 (1.2668e+00)	Acc@1  66.00 ( 67.71)	Acc@5  92.00 ( 90.62)
Test: [ 30/100]	Time  0.026 ( 0.031)	Loss 1.6582e+00 (1.3013e+00)	Acc@1  60.00 ( 67.16)	Acc@5  85.00 ( 90.26)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.1821e+00 (1.2820e+00)	Acc@1  68.00 ( 67.15)	Acc@5  93.00 ( 90.88)
Test: [ 50/100]	Time  0.025 ( 0.029)	Loss 1.2983e+00 (1.3032e+00)	Acc@1  72.00 ( 67.04)	Acc@5  90.00 ( 90.51)
Test: [ 60/100]	Time  0.025 ( 0.028)	Loss 1.3195e+00 (1.2952e+00)	Acc@1  64.00 ( 67.03)	Acc@5  89.00 ( 90.56)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.1921e+00 (1.3032e+00)	Acc@1  66.00 ( 67.03)	Acc@5  94.00 ( 90.63)
Test: [ 80/100]	Time  0.026 ( 0.028)	Loss 1.4965e+00 (1.3152e+00)	Acc@1  64.00 ( 66.89)	Acc@5  92.00 ( 90.41)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.9890e+00 (1.3059e+00)	Acc@1  57.00 ( 67.12)	Acc@5  85.00 ( 90.47)
 * Acc@1 67.120 Acc@5 90.460
### epoch[86] execution time: 28.44522190093994
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.217 ( 0.217)	Data  0.146 ( 0.146)	Loss 3.5121e-01 (3.5121e-01)	Acc@1  89.06 ( 89.06)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.014)	Loss 3.3304e-01 (3.2087e-01)	Acc@1  89.84 ( 90.48)	Acc@5  99.22 ( 99.36)
Epoch: [87][ 20/391]	Time  0.064 ( 0.072)	Data  0.001 ( 0.008)	Loss 3.3571e-01 (3.2627e-01)	Acc@1  86.72 ( 90.14)	Acc@5 100.00 ( 99.52)
Epoch: [87][ 30/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.006)	Loss 3.5580e-01 (3.2237e-01)	Acc@1  89.84 ( 90.37)	Acc@5  99.22 ( 99.62)
Epoch: [87][ 40/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.7974e-01 (3.2734e-01)	Acc@1  92.97 ( 90.38)	Acc@5  99.22 ( 99.47)
Epoch: [87][ 50/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.004)	Loss 4.1218e-01 (3.2665e-01)	Acc@1  88.28 ( 90.36)	Acc@5  99.22 ( 99.43)
Epoch: [87][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 2.6882e-01 (3.2370e-01)	Acc@1  93.75 ( 90.45)	Acc@5  97.66 ( 99.41)
Epoch: [87][ 70/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.3591e-01 (3.2035e-01)	Acc@1  89.06 ( 90.53)	Acc@5 100.00 ( 99.38)
Epoch: [87][ 80/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.3146e-01 (3.2209e-01)	Acc@1  89.84 ( 90.44)	Acc@5  99.22 ( 99.33)
Epoch: [87][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.2260e-01 (3.2298e-01)	Acc@1  90.62 ( 90.38)	Acc@5  99.22 ( 99.34)
Epoch: [87][100/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.9621e-01 (3.2315e-01)	Acc@1  87.50 ( 90.41)	Acc@5  99.22 ( 99.35)
Epoch: [87][110/391]	Time  0.069 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2960e-01 (3.2043e-01)	Acc@1  94.53 ( 90.57)	Acc@5 100.00 ( 99.36)
Epoch: [87][120/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9358e-01 (3.2251e-01)	Acc@1  89.06 ( 90.47)	Acc@5 100.00 ( 99.35)
Epoch: [87][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5968e-01 (3.2206e-01)	Acc@1  89.84 ( 90.51)	Acc@5  99.22 ( 99.35)
Epoch: [87][140/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1314e-01 (3.2393e-01)	Acc@1  88.28 ( 90.45)	Acc@5 100.00 ( 99.31)
Epoch: [87][150/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.0964e-01 (3.2333e-01)	Acc@1  88.28 ( 90.52)	Acc@5  98.44 ( 99.31)
Epoch: [87][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.1469e-01 (3.2442e-01)	Acc@1  85.94 ( 90.45)	Acc@5  98.44 ( 99.31)
Epoch: [87][170/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8921e-01 (3.2292e-01)	Acc@1  90.62 ( 90.52)	Acc@5 100.00 ( 99.32)
Epoch: [87][180/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9014e-01 (3.2401e-01)	Acc@1  92.19 ( 90.44)	Acc@5  99.22 ( 99.30)
Epoch: [87][190/391]	Time  0.069 ( 0.065)	Data  0.002 ( 0.002)	Loss 2.4881e-01 (3.2444e-01)	Acc@1  91.41 ( 90.38)	Acc@5 100.00 ( 99.31)
Epoch: [87][200/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0715e-01 (3.2426e-01)	Acc@1  89.84 ( 90.36)	Acc@5 100.00 ( 99.31)
Epoch: [87][210/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8520e-01 (3.2419e-01)	Acc@1  90.62 ( 90.37)	Acc@5 100.00 ( 99.32)
Epoch: [87][220/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1628e-01 (3.2304e-01)	Acc@1  92.19 ( 90.43)	Acc@5  98.44 ( 99.32)
Epoch: [87][230/391]	Time  0.063 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8876e-01 (3.2249e-01)	Acc@1  91.41 ( 90.43)	Acc@5  98.44 ( 99.32)
Epoch: [87][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4097e-01 (3.2076e-01)	Acc@1  88.28 ( 90.48)	Acc@5 100.00 ( 99.34)
Epoch: [87][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4169e-01 (3.2041e-01)	Acc@1  89.06 ( 90.51)	Acc@5  98.44 ( 99.32)
Epoch: [87][260/391]	Time  0.062 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.1362e-01 (3.1988e-01)	Acc@1  86.72 ( 90.51)	Acc@5  97.66 ( 99.33)
Epoch: [87][270/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3760e-01 (3.1870e-01)	Acc@1  92.19 ( 90.52)	Acc@5 100.00 ( 99.35)
Epoch: [87][280/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1339e-01 (3.1907e-01)	Acc@1  91.41 ( 90.51)	Acc@5 100.00 ( 99.34)
Epoch: [87][290/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4882e-01 (3.1963e-01)	Acc@1  91.41 ( 90.52)	Acc@5  98.44 ( 99.32)
Epoch: [87][300/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7350e-01 (3.1905e-01)	Acc@1  93.75 ( 90.55)	Acc@5 100.00 ( 99.32)
Epoch: [87][310/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.2702e-01 (3.1944e-01)	Acc@1  88.28 ( 90.53)	Acc@5  99.22 ( 99.32)
Epoch: [87][320/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9876e-01 (3.1937e-01)	Acc@1  92.19 ( 90.54)	Acc@5  99.22 ( 99.32)
Epoch: [87][330/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.4430e-01 (3.1931e-01)	Acc@1  93.75 ( 90.55)	Acc@5  99.22 ( 99.33)
Epoch: [87][340/391]	Time  0.069 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5275e-01 (3.2010e-01)	Acc@1  88.28 ( 90.53)	Acc@5  99.22 ( 99.33)
Epoch: [87][350/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8011e-01 (3.1850e-01)	Acc@1  90.62 ( 90.57)	Acc@5 100.00 ( 99.33)
Epoch: [87][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1957e-01 (3.1809e-01)	Acc@1  91.41 ( 90.60)	Acc@5  99.22 ( 99.34)
Epoch: [87][370/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.8307e-01 (3.1837e-01)	Acc@1  94.53 ( 90.59)	Acc@5  99.22 ( 99.33)
Epoch: [87][380/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.9305e-01 (3.1850e-01)	Acc@1  91.41 ( 90.58)	Acc@5  99.22 ( 99.33)
Epoch: [87][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.4102e-01 (3.1864e-01)	Acc@1  87.50 ( 90.57)	Acc@5  98.75 ( 99.32)
## e[87] optimizer.zero_grad (sum) time: 0.4089369773864746
## e[87]       loss.backward (sum) time: 6.935291290283203
## e[87]      optimizer.step (sum) time: 3.4746196269989014
## epoch[87] training(only) time: 25.52993130683899
# Switched to evaluate mode...
Test: [  0/100]	Time  0.156 ( 0.156)	Loss 1.3277e+00 (1.3277e+00)	Acc@1  66.00 ( 66.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3223e+00 (1.3250e+00)	Acc@1  69.00 ( 67.64)	Acc@5  92.00 ( 89.45)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3883e+00 (1.2680e+00)	Acc@1  66.00 ( 68.14)	Acc@5  93.00 ( 90.67)
Test: [ 30/100]	Time  0.025 ( 0.031)	Loss 1.6622e+00 (1.3047e+00)	Acc@1  60.00 ( 67.48)	Acc@5  86.00 ( 90.35)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2102e+00 (1.2892e+00)	Acc@1  66.00 ( 67.32)	Acc@5  94.00 ( 90.90)
Test: [ 50/100]	Time  0.027 ( 0.029)	Loss 1.3295e+00 (1.3144e+00)	Acc@1  71.00 ( 67.04)	Acc@5  89.00 ( 90.65)
Test: [ 60/100]	Time  0.025 ( 0.029)	Loss 1.3465e+00 (1.3030e+00)	Acc@1  65.00 ( 67.30)	Acc@5  90.00 ( 90.69)
Test: [ 70/100]	Time  0.027 ( 0.028)	Loss 1.1688e+00 (1.3095e+00)	Acc@1  68.00 ( 67.32)	Acc@5  95.00 ( 90.68)
Test: [ 80/100]	Time  0.027 ( 0.028)	Loss 1.4253e+00 (1.3188e+00)	Acc@1  65.00 ( 67.14)	Acc@5  92.00 ( 90.49)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.9903e+00 (1.3097e+00)	Acc@1  59.00 ( 67.32)	Acc@5  86.00 ( 90.60)
 * Acc@1 67.360 Acc@5 90.560
### epoch[87] execution time: 28.362132787704468
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.218 ( 0.218)	Data  0.150 ( 0.150)	Loss 2.5903e-01 (2.5903e-01)	Acc@1  94.53 ( 94.53)	Acc@5  98.44 ( 98.44)
Epoch: [88][ 10/391]	Time  0.064 ( 0.080)	Data  0.001 ( 0.015)	Loss 3.0474e-01 (3.1432e-01)	Acc@1  92.97 ( 90.55)	Acc@5  98.44 ( 99.29)
Epoch: [88][ 20/391]	Time  0.069 ( 0.073)	Data  0.001 ( 0.008)	Loss 3.0117e-01 (3.1040e-01)	Acc@1  89.84 ( 90.40)	Acc@5 100.00 ( 99.29)
Epoch: [88][ 30/391]	Time  0.062 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.1531e-01 (3.1053e-01)	Acc@1  91.41 ( 90.57)	Acc@5  98.44 ( 99.29)
Epoch: [88][ 40/391]	Time  0.066 ( 0.069)	Data  0.001 ( 0.005)	Loss 3.8849e-01 (3.1611e-01)	Acc@1  87.50 ( 90.36)	Acc@5  99.22 ( 99.31)
Epoch: [88][ 50/391]	Time  0.067 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.7100e-01 (3.1812e-01)	Acc@1  92.19 ( 90.29)	Acc@5 100.00 ( 99.36)
Epoch: [88][ 60/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.1954e-01 (3.1389e-01)	Acc@1  94.53 ( 90.48)	Acc@5 100.00 ( 99.44)
Epoch: [88][ 70/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.2178e-01 (3.1605e-01)	Acc@1  93.75 ( 90.64)	Acc@5 100.00 ( 99.39)
Epoch: [88][ 80/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.2621e-01 (3.1726e-01)	Acc@1  89.84 ( 90.60)	Acc@5 100.00 ( 99.38)
Epoch: [88][ 90/391]	Time  0.064 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.0814e-01 (3.1525e-01)	Acc@1  89.06 ( 90.70)	Acc@5  99.22 ( 99.37)
Epoch: [88][100/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 4.0383e-01 (3.1371e-01)	Acc@1  88.28 ( 90.72)	Acc@5  99.22 ( 99.39)
Epoch: [88][110/391]	Time  0.065 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7211e-01 (3.1292e-01)	Acc@1  90.62 ( 90.73)	Acc@5 100.00 ( 99.40)
Epoch: [88][120/391]	Time  0.077 ( 0.067)	Data  0.001 ( 0.002)	Loss 2.1777e-01 (3.1048e-01)	Acc@1  95.31 ( 90.83)	Acc@5 100.00 ( 99.41)
Epoch: [88][130/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.5242e-01 (3.1105e-01)	Acc@1  92.19 ( 90.77)	Acc@5  99.22 ( 99.40)
Epoch: [88][140/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.7627e-01 (3.1215e-01)	Acc@1  89.84 ( 90.72)	Acc@5  97.66 ( 99.37)
Epoch: [88][150/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9977e-01 (3.1161e-01)	Acc@1  91.41 ( 90.76)	Acc@5 100.00 ( 99.37)
Epoch: [88][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5364e-01 (3.1270e-01)	Acc@1  86.72 ( 90.73)	Acc@5  99.22 ( 99.38)
Epoch: [88][170/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8301e-01 (3.1236e-01)	Acc@1  90.62 ( 90.73)	Acc@5 100.00 ( 99.38)
Epoch: [88][180/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6958e-01 (3.1416e-01)	Acc@1  94.53 ( 90.64)	Acc@5  99.22 ( 99.35)
Epoch: [88][190/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1444e-01 (3.1552e-01)	Acc@1  90.62 ( 90.64)	Acc@5 100.00 ( 99.34)
Epoch: [88][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1886e-01 (3.1520e-01)	Acc@1  89.84 ( 90.64)	Acc@5  99.22 ( 99.33)
Epoch: [88][210/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7603e-01 (3.1669e-01)	Acc@1  90.62 ( 90.58)	Acc@5 100.00 ( 99.34)
Epoch: [88][220/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1227e-01 (3.1525e-01)	Acc@1  90.62 ( 90.60)	Acc@5  99.22 ( 99.34)
Epoch: [88][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4830e-01 (3.1452e-01)	Acc@1  94.53 ( 90.63)	Acc@5 100.00 ( 99.34)
Epoch: [88][240/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.3209e-01 (3.1250e-01)	Acc@1  93.75 ( 90.69)	Acc@5 100.00 ( 99.35)
Epoch: [88][250/391]	Time  0.085 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3123e-01 (3.1235e-01)	Acc@1  91.41 ( 90.72)	Acc@5  99.22 ( 99.35)
Epoch: [88][260/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1825e-01 (3.1338e-01)	Acc@1  90.62 ( 90.69)	Acc@5  99.22 ( 99.34)
Epoch: [88][270/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1251e-01 (3.1260e-01)	Acc@1  90.62 ( 90.71)	Acc@5  99.22 ( 99.34)
Epoch: [88][280/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5302e-01 (3.1300e-01)	Acc@1  92.19 ( 90.71)	Acc@5  99.22 ( 99.34)
Epoch: [88][290/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6214e-01 (3.1318e-01)	Acc@1  91.41 ( 90.73)	Acc@5 100.00 ( 99.34)
Epoch: [88][300/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.8166e-01 (3.1287e-01)	Acc@1  91.41 ( 90.73)	Acc@5  99.22 ( 99.35)
Epoch: [88][310/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 4.6122e-01 (3.1325e-01)	Acc@1  84.38 ( 90.73)	Acc@5  98.44 ( 99.34)
Epoch: [88][320/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.5177e-01 (3.1384e-01)	Acc@1  89.84 ( 90.68)	Acc@5  98.44 ( 99.34)
Epoch: [88][330/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 1.9416e-01 (3.1334e-01)	Acc@1  96.88 ( 90.71)	Acc@5 100.00 ( 99.34)
Epoch: [88][340/391]	Time  0.062 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.9909e-01 (3.1383e-01)	Acc@1  89.84 ( 90.70)	Acc@5 100.00 ( 99.33)
Epoch: [88][350/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3562e-01 (3.1344e-01)	Acc@1  90.62 ( 90.72)	Acc@5 100.00 ( 99.33)
Epoch: [88][360/391]	Time  0.067 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.2753e-01 (3.1235e-01)	Acc@1  95.31 ( 90.75)	Acc@5 100.00 ( 99.35)
Epoch: [88][370/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7646e-01 (3.1203e-01)	Acc@1  92.97 ( 90.76)	Acc@5 100.00 ( 99.35)
Epoch: [88][380/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.3915e-01 (3.1244e-01)	Acc@1  87.50 ( 90.73)	Acc@5 100.00 ( 99.35)
Epoch: [88][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.5057e-01 (3.1249e-01)	Acc@1  88.75 ( 90.73)	Acc@5  98.75 ( 99.35)
## e[88] optimizer.zero_grad (sum) time: 0.40933918952941895
## e[88]       loss.backward (sum) time: 6.989962816238403
## e[88]      optimizer.step (sum) time: 3.506275177001953
## epoch[88] training(only) time: 25.691417455673218
# Switched to evaluate mode...
Test: [  0/100]	Time  0.159 ( 0.159)	Loss 1.2914e+00 (1.2914e+00)	Acc@1  71.00 ( 71.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.028 ( 0.040)	Loss 1.3496e+00 (1.3187e+00)	Acc@1  65.00 ( 68.18)	Acc@5  92.00 ( 89.36)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3873e+00 (1.2708e+00)	Acc@1  70.00 ( 68.43)	Acc@5  93.00 ( 90.71)
Test: [ 30/100]	Time  0.033 ( 0.031)	Loss 1.6613e+00 (1.3040e+00)	Acc@1  61.00 ( 67.61)	Acc@5  88.00 ( 90.35)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2215e+00 (1.2897e+00)	Acc@1  66.00 ( 67.34)	Acc@5  93.00 ( 90.88)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.3133e+00 (1.3126e+00)	Acc@1  72.00 ( 67.14)	Acc@5  90.00 ( 90.49)
Test: [ 60/100]	Time  0.027 ( 0.029)	Loss 1.3268e+00 (1.3004e+00)	Acc@1  64.00 ( 67.39)	Acc@5  91.00 ( 90.59)
Test: [ 70/100]	Time  0.028 ( 0.028)	Loss 1.1871e+00 (1.3075e+00)	Acc@1  66.00 ( 67.48)	Acc@5  94.00 ( 90.63)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4481e+00 (1.3168e+00)	Acc@1  65.00 ( 67.31)	Acc@5  91.00 ( 90.47)
Test: [ 90/100]	Time  0.026 ( 0.028)	Loss 1.9947e+00 (1.3085e+00)	Acc@1  57.00 ( 67.52)	Acc@5  84.00 ( 90.53)
 * Acc@1 67.490 Acc@5 90.550
### epoch[88] execution time: 28.542490482330322
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.222 ( 0.222)	Data  0.152 ( 0.152)	Loss 3.2729e-01 (3.2729e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.064 ( 0.079)	Data  0.001 ( 0.015)	Loss 2.9341e-01 (3.2492e-01)	Acc@1  88.28 ( 89.91)	Acc@5 100.00 ( 99.79)
Epoch: [89][ 20/391]	Time  0.065 ( 0.072)	Data  0.002 ( 0.008)	Loss 3.1075e-01 (3.1739e-01)	Acc@1  89.06 ( 90.36)	Acc@5 100.00 ( 99.59)
Epoch: [89][ 30/391]	Time  0.061 ( 0.070)	Data  0.001 ( 0.006)	Loss 3.7324e-01 (3.1224e-01)	Acc@1  89.06 ( 90.57)	Acc@5  99.22 ( 99.40)
Epoch: [89][ 40/391]	Time  0.063 ( 0.068)	Data  0.001 ( 0.005)	Loss 2.6957e-01 (3.1868e-01)	Acc@1  92.97 ( 90.45)	Acc@5 100.00 ( 99.28)
Epoch: [89][ 50/391]	Time  0.066 ( 0.068)	Data  0.001 ( 0.004)	Loss 2.6698e-01 (3.1742e-01)	Acc@1  93.75 ( 90.56)	Acc@5 100.00 ( 99.31)
Epoch: [89][ 60/391]	Time  0.066 ( 0.067)	Data  0.001 ( 0.004)	Loss 3.4164e-01 (3.1727e-01)	Acc@1  88.28 ( 90.65)	Acc@5  99.22 ( 99.35)
Epoch: [89][ 70/391]	Time  0.063 ( 0.067)	Data  0.001 ( 0.003)	Loss 3.6817e-01 (3.1985e-01)	Acc@1  89.06 ( 90.59)	Acc@5  98.44 ( 99.26)
Epoch: [89][ 80/391]	Time  0.067 ( 0.067)	Data  0.001 ( 0.003)	Loss 2.7187e-01 (3.1702e-01)	Acc@1  92.19 ( 90.60)	Acc@5 100.00 ( 99.27)
Epoch: [89][ 90/391]	Time  0.063 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.2635e-01 (3.1511e-01)	Acc@1  88.28 ( 90.67)	Acc@5  99.22 ( 99.29)
Epoch: [89][100/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.003)	Loss 2.4103e-01 (3.1214e-01)	Acc@1  91.41 ( 90.83)	Acc@5 100.00 ( 99.30)
Epoch: [89][110/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.003)	Loss 3.1470e-01 (3.1216e-01)	Acc@1  92.19 ( 90.88)	Acc@5  99.22 ( 99.30)
Epoch: [89][120/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.8558e-01 (3.1315e-01)	Acc@1  89.06 ( 90.73)	Acc@5  98.44 ( 99.32)
Epoch: [89][130/391]	Time  0.066 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.4488e-01 (3.1190e-01)	Acc@1  88.28 ( 90.76)	Acc@5 100.00 ( 99.30)
Epoch: [89][140/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.4526e-01 (3.1057e-01)	Acc@1  92.97 ( 90.77)	Acc@5 100.00 ( 99.31)
Epoch: [89][150/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1310e-01 (3.1050e-01)	Acc@1  92.19 ( 90.74)	Acc@5  99.22 ( 99.33)
Epoch: [89][160/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.7178e-01 (3.1211e-01)	Acc@1  94.53 ( 90.72)	Acc@5  99.22 ( 99.33)
Epoch: [89][170/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2923e-01 (3.1310e-01)	Acc@1  91.41 ( 90.64)	Acc@5 100.00 ( 99.33)
Epoch: [89][180/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0892e-01 (3.1305e-01)	Acc@1  90.62 ( 90.60)	Acc@5 100.00 ( 99.35)
Epoch: [89][190/391]	Time  0.071 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.0408e-01 (3.1396e-01)	Acc@1  89.06 ( 90.51)	Acc@5  98.44 ( 99.35)
Epoch: [89][200/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.1992e-01 (3.1414e-01)	Acc@1  92.19 ( 90.52)	Acc@5  99.22 ( 99.35)
Epoch: [89][210/391]	Time  0.065 ( 0.066)	Data  0.001 ( 0.002)	Loss 2.6749e-01 (3.1270e-01)	Acc@1  92.19 ( 90.59)	Acc@5  99.22 ( 99.36)
Epoch: [89][220/391]	Time  0.068 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2407e-01 (3.1322e-01)	Acc@1  87.50 ( 90.54)	Acc@5  99.22 ( 99.36)
Epoch: [89][230/391]	Time  0.064 ( 0.066)	Data  0.001 ( 0.002)	Loss 3.2020e-01 (3.1396e-01)	Acc@1  89.84 ( 90.50)	Acc@5  98.44 ( 99.36)
Epoch: [89][240/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5616e-01 (3.1441e-01)	Acc@1  91.41 ( 90.45)	Acc@5 100.00 ( 99.35)
Epoch: [89][250/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.6439e-01 (3.1480e-01)	Acc@1  87.50 ( 90.44)	Acc@5  98.44 ( 99.34)
Epoch: [89][260/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8870e-01 (3.1619e-01)	Acc@1  88.28 ( 90.37)	Acc@5  99.22 ( 99.34)
Epoch: [89][270/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.2742e-01 (3.1528e-01)	Acc@1  92.97 ( 90.40)	Acc@5 100.00 ( 99.34)
Epoch: [89][280/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1078e-01 (3.1556e-01)	Acc@1  89.84 ( 90.40)	Acc@5 100.00 ( 99.34)
Epoch: [89][290/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7525e-01 (3.1484e-01)	Acc@1  92.97 ( 90.41)	Acc@5  99.22 ( 99.34)
Epoch: [89][300/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.1447e-01 (3.1510e-01)	Acc@1  92.19 ( 90.37)	Acc@5 100.00 ( 99.35)
Epoch: [89][310/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7046e-01 (3.1535e-01)	Acc@1  93.75 ( 90.34)	Acc@5 100.00 ( 99.34)
Epoch: [89][320/391]	Time  0.064 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.6297e-01 (3.1513e-01)	Acc@1  92.97 ( 90.34)	Acc@5  99.22 ( 99.35)
Epoch: [89][330/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 4.3488e-01 (3.1598e-01)	Acc@1  88.28 ( 90.32)	Acc@5  98.44 ( 99.34)
Epoch: [89][340/391]	Time  0.067 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.8291e-01 (3.1612e-01)	Acc@1  89.06 ( 90.34)	Acc@5  97.66 ( 99.33)
Epoch: [89][350/391]	Time  0.068 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.3361e-01 (3.1572e-01)	Acc@1  92.19 ( 90.35)	Acc@5 100.00 ( 99.33)
Epoch: [89][360/391]	Time  0.065 ( 0.065)	Data  0.001 ( 0.002)	Loss 3.2984e-01 (3.1579e-01)	Acc@1  86.72 ( 90.35)	Acc@5 100.00 ( 99.34)
Epoch: [89][370/391]	Time  0.061 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.1921e-01 (3.1518e-01)	Acc@1  94.53 ( 90.36)	Acc@5 100.00 ( 99.34)
Epoch: [89][380/391]	Time  0.066 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.5738e-01 (3.1534e-01)	Acc@1  95.31 ( 90.36)	Acc@5 100.00 ( 99.35)
Epoch: [89][390/391]	Time  0.047 ( 0.065)	Data  0.001 ( 0.002)	Loss 2.7701e-01 (3.1520e-01)	Acc@1  90.00 ( 90.38)	Acc@5 100.00 ( 99.35)
## e[89] optimizer.zero_grad (sum) time: 0.4117727279663086
## e[89]       loss.backward (sum) time: 6.973273992538452
## e[89]      optimizer.step (sum) time: 3.4198975563049316
## epoch[89] training(only) time: 25.529785871505737
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3153e+00 (1.3153e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.025 ( 0.039)	Loss 1.3393e+00 (1.3342e+00)	Acc@1  69.00 ( 66.64)	Acc@5  91.00 ( 89.55)
Test: [ 20/100]	Time  0.025 ( 0.033)	Loss 1.3820e+00 (1.2814e+00)	Acc@1  67.00 ( 67.43)	Acc@5  93.00 ( 90.71)
Test: [ 30/100]	Time  0.027 ( 0.031)	Loss 1.7193e+00 (1.3146e+00)	Acc@1  61.00 ( 67.13)	Acc@5  85.00 ( 90.23)
Test: [ 40/100]	Time  0.025 ( 0.030)	Loss 1.2029e+00 (1.2994e+00)	Acc@1  67.00 ( 66.98)	Acc@5  92.00 ( 90.73)
Test: [ 50/100]	Time  0.028 ( 0.029)	Loss 1.3399e+00 (1.3222e+00)	Acc@1  71.00 ( 66.73)	Acc@5  89.00 ( 90.47)
Test: [ 60/100]	Time  0.028 ( 0.028)	Loss 1.3712e+00 (1.3116e+00)	Acc@1  65.00 ( 67.02)	Acc@5  92.00 ( 90.56)
Test: [ 70/100]	Time  0.025 ( 0.028)	Loss 1.1864e+00 (1.3196e+00)	Acc@1  69.00 ( 67.15)	Acc@5  94.00 ( 90.62)
Test: [ 80/100]	Time  0.025 ( 0.028)	Loss 1.4258e+00 (1.3288e+00)	Acc@1  65.00 ( 67.01)	Acc@5  91.00 ( 90.42)
Test: [ 90/100]	Time  0.028 ( 0.028)	Loss 1.9926e+00 (1.3202e+00)	Acc@1  58.00 ( 67.23)	Acc@5  84.00 ( 90.46)
 * Acc@1 67.230 Acc@5 90.420
### epoch[89] execution time: 28.358706951141357
### Training complete:
#### total training(only) time: 2309.6598324775696
##### Total run time: 2570.1945104599
