# Model: inception
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.inception
<function inception at 0x7f53a9862f28>
# model requested: 'inception'
# printing out the model
InceptionV3(
  (Conv2d_1a_3x3): BasicConv2d(
    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_2a_3x3): BasicConv2d(
    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_2b_3x3): BasicConv2d(
    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_3b_1x1): BasicConv2d(
    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Conv2d_4a_3x3): BasicConv2d(
    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
  )
  (Mixed_5b): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_5c): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_5d): InceptionA(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch5x5): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6a): InceptionB(
    (branch3x3): BasicConv2d(
      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (Mixed_6b): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6c): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6d): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_6e): InceptionC(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7stack): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_7a): InceptionD(
    (branch3x3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branch7x7): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): BasicConv2d(
        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (branchpool): AvgPool2d(kernel_size=3, stride=2, padding=0)
  )
  (Mixed_7b): InceptionE(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_1): BasicConv2d(
      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_2): BasicConv2d(
      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (Mixed_7c): InceptionE(
    (branch1x1): BasicConv2d(
      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_1): BasicConv2d(
      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3_2b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_1): BasicConv2d(
      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_2): BasicConv2d(
      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3a): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch3x3stack_3b): BasicConv2d(
      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)
      (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (branch_pool): Sequential(
      (0): AvgPool2d(kernel_size=3, stride=1, padding=1)
      (1): BasicConv2d(
        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout2d(p=0.5, inplace=False)
  (linear): Linear(in_features=2048, out_features=100, bias=True)
)
# model is low precision
# Model: inception
# Dataset: cifarcentum
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  5.186 ( 5.186)	Data  0.107 ( 0.107)	Loss 4.7188e+00 (4.7188e+00)	Acc@1   0.00 (  0.00)	Acc@5   2.34 (  2.34)
Epoch: [0][ 10/391]	Time  0.156 ( 0.616)	Data  0.001 ( 0.011)	Loss 5.4258e+00 (5.7290e+00)	Acc@1   0.00 (  0.85)	Acc@5   3.91 (  4.69)
Epoch: [0][ 20/391]	Time  0.157 ( 0.398)	Data  0.001 ( 0.006)	Loss 4.9648e+00 (5.3916e+00)	Acc@1   2.34 (  1.23)	Acc@5   9.38 (  5.95)
Epoch: [0][ 30/391]	Time  0.161 ( 0.320)	Data  0.003 ( 0.005)	Loss 4.7578e+00 (5.2127e+00)	Acc@1   2.34 (  1.41)	Acc@5   6.25 (  5.90)
Epoch: [0][ 40/391]	Time  0.156 ( 0.281)	Data  0.001 ( 0.004)	Loss 4.6250e+00 (5.0982e+00)	Acc@1   0.78 (  1.35)	Acc@5   3.91 (  5.95)
Epoch: [0][ 50/391]	Time  0.156 ( 0.257)	Data  0.001 ( 0.003)	Loss 4.6562e+00 (5.0312e+00)	Acc@1   0.00 (  1.26)	Acc@5   3.12 (  6.10)
Epoch: [0][ 60/391]	Time  0.157 ( 0.240)	Data  0.001 ( 0.003)	Loss 4.5898e+00 (4.9683e+00)	Acc@1   3.91 (  1.32)	Acc@5   9.38 (  6.28)
Epoch: [0][ 70/391]	Time  0.156 ( 0.229)	Data  0.001 ( 0.003)	Loss 4.5820e+00 (4.9193e+00)	Acc@1   2.34 (  1.33)	Acc@5   5.47 (  6.40)
Epoch: [0][ 80/391]	Time  0.157 ( 0.220)	Data  0.001 ( 0.002)	Loss 4.6758e+00 (4.8821e+00)	Acc@1   1.56 (  1.41)	Acc@5   9.38 (  6.51)
Epoch: [0][ 90/391]	Time  0.161 ( 0.213)	Data  0.001 ( 0.002)	Loss 4.5586e+00 (4.8534e+00)	Acc@1   2.34 (  1.40)	Acc@5   9.38 (  6.56)
Epoch: [0][100/391]	Time  0.160 ( 0.208)	Data  0.001 ( 0.002)	Loss 4.5352e+00 (4.8246e+00)	Acc@1   4.69 (  1.48)	Acc@5  10.94 (  6.70)
Epoch: [0][110/391]	Time  0.156 ( 0.203)	Data  0.001 ( 0.002)	Loss 4.5039e+00 (4.8025e+00)	Acc@1   2.34 (  1.49)	Acc@5  10.16 (  6.83)
Epoch: [0][120/391]	Time  0.159 ( 0.200)	Data  0.001 ( 0.002)	Loss 4.5234e+00 (4.7817e+00)	Acc@1   2.34 (  1.54)	Acc@5   8.59 (  7.08)
Epoch: [0][130/391]	Time  0.157 ( 0.197)	Data  0.001 ( 0.002)	Loss 4.5859e+00 (4.7633e+00)	Acc@1   1.56 (  1.54)	Acc@5   5.47 (  7.19)
Epoch: [0][140/391]	Time  0.156 ( 0.194)	Data  0.001 ( 0.002)	Loss 4.4766e+00 (4.7474e+00)	Acc@1   3.12 (  1.63)	Acc@5  11.72 (  7.36)
Epoch: [0][150/391]	Time  0.158 ( 0.192)	Data  0.001 ( 0.002)	Loss 4.4648e+00 (4.7311e+00)	Acc@1   0.78 (  1.65)	Acc@5   8.59 (  7.54)
Epoch: [0][160/391]	Time  0.158 ( 0.190)	Data  0.001 ( 0.002)	Loss 4.5156e+00 (4.7148e+00)	Acc@1   1.56 (  1.65)	Acc@5  11.72 (  7.68)
Epoch: [0][170/391]	Time  0.156 ( 0.188)	Data  0.001 ( 0.002)	Loss 4.4102e+00 (4.6994e+00)	Acc@1   1.56 (  1.69)	Acc@5  10.16 (  7.85)
Epoch: [0][180/391]	Time  0.158 ( 0.186)	Data  0.001 ( 0.002)	Loss 4.4531e+00 (4.6845e+00)	Acc@1   3.12 (  1.78)	Acc@5  10.94 (  8.07)
Epoch: [0][190/391]	Time  0.159 ( 0.185)	Data  0.001 ( 0.002)	Loss 4.3633e+00 (4.6703e+00)	Acc@1   3.91 (  1.85)	Acc@5  14.84 (  8.35)
Epoch: [0][200/391]	Time  0.158 ( 0.183)	Data  0.001 ( 0.002)	Loss 4.3906e+00 (4.6566e+00)	Acc@1   2.34 (  1.92)	Acc@5  16.41 (  8.61)
Epoch: [0][210/391]	Time  0.157 ( 0.182)	Data  0.001 ( 0.002)	Loss 4.3633e+00 (4.6450e+00)	Acc@1   3.91 (  1.97)	Acc@5  12.50 (  8.75)
Epoch: [0][220/391]	Time  0.158 ( 0.181)	Data  0.001 ( 0.002)	Loss 4.3789e+00 (4.6345e+00)	Acc@1   5.47 (  2.03)	Acc@5  14.84 (  8.92)
Epoch: [0][230/391]	Time  0.158 ( 0.180)	Data  0.001 ( 0.001)	Loss 4.4766e+00 (4.6243e+00)	Acc@1   1.56 (  2.05)	Acc@5  10.94 (  9.09)
Epoch: [0][240/391]	Time  0.158 ( 0.179)	Data  0.001 ( 0.001)	Loss 4.3828e+00 (4.6142e+00)	Acc@1   3.12 (  2.06)	Acc@5   8.59 (  9.21)
Epoch: [0][250/391]	Time  0.160 ( 0.179)	Data  0.001 ( 0.001)	Loss 4.3398e+00 (4.6042e+00)	Acc@1   1.56 (  2.08)	Acc@5  14.06 (  9.40)
Epoch: [0][260/391]	Time  0.159 ( 0.178)	Data  0.001 ( 0.001)	Loss 4.3750e+00 (4.5948e+00)	Acc@1   0.78 (  2.10)	Acc@5  14.84 (  9.57)
Epoch: [0][270/391]	Time  0.158 ( 0.177)	Data  0.001 ( 0.001)	Loss 4.3477e+00 (4.5851e+00)	Acc@1   3.91 (  2.10)	Acc@5  12.50 (  9.72)
Epoch: [0][280/391]	Time  0.157 ( 0.176)	Data  0.001 ( 0.001)	Loss 4.3047e+00 (4.5753e+00)	Acc@1   4.69 (  2.15)	Acc@5  14.84 (  9.92)
Epoch: [0][290/391]	Time  0.157 ( 0.176)	Data  0.001 ( 0.001)	Loss 4.2539e+00 (4.5656e+00)	Acc@1   5.47 (  2.19)	Acc@5  18.75 ( 10.05)
Epoch: [0][300/391]	Time  0.158 ( 0.175)	Data  0.001 ( 0.001)	Loss 4.4062e+00 (4.5586e+00)	Acc@1   3.91 (  2.21)	Acc@5  11.72 ( 10.11)
Epoch: [0][310/391]	Time  0.157 ( 0.175)	Data  0.001 ( 0.001)	Loss 4.3477e+00 (4.5514e+00)	Acc@1   1.56 (  2.27)	Acc@5  12.50 ( 10.25)
Epoch: [0][320/391]	Time  0.158 ( 0.174)	Data  0.001 ( 0.001)	Loss 4.3203e+00 (4.5446e+00)	Acc@1   3.12 (  2.30)	Acc@5  19.53 ( 10.38)
Epoch: [0][330/391]	Time  0.158 ( 0.174)	Data  0.001 ( 0.001)	Loss 4.4570e+00 (4.5372e+00)	Acc@1   1.56 (  2.33)	Acc@5  10.94 ( 10.53)
Epoch: [0][340/391]	Time  0.158 ( 0.173)	Data  0.001 ( 0.001)	Loss 4.2461e+00 (4.5313e+00)	Acc@1   4.69 (  2.38)	Acc@5  22.66 ( 10.66)
Epoch: [0][350/391]	Time  0.157 ( 0.173)	Data  0.001 ( 0.001)	Loss 4.3867e+00 (4.5243e+00)	Acc@1   4.69 (  2.42)	Acc@5  12.50 ( 10.81)
Epoch: [0][360/391]	Time  0.160 ( 0.173)	Data  0.001 ( 0.001)	Loss 4.2148e+00 (4.5174e+00)	Acc@1   7.03 (  2.45)	Acc@5  21.88 ( 10.96)
Epoch: [0][370/391]	Time  0.161 ( 0.172)	Data  0.001 ( 0.001)	Loss 4.2070e+00 (4.5105e+00)	Acc@1   0.78 (  2.48)	Acc@5  21.88 ( 11.11)
Epoch: [0][380/391]	Time  0.159 ( 0.172)	Data  0.001 ( 0.001)	Loss 4.2188e+00 (4.5040e+00)	Acc@1   2.34 (  2.54)	Acc@5  11.72 ( 11.25)
Epoch: [0][390/391]	Time  1.934 ( 0.176)	Data  0.001 ( 0.001)	Loss 4.4375e+00 (4.4987e+00)	Acc@1   2.50 (  2.59)	Acc@5  11.25 ( 11.35)
## e[0] optimizer.zero_grad (sum) time: 0.5549647808074951
## e[0]       loss.backward (sum) time: 15.051141738891602
## e[0]      optimizer.step (sum) time: 25.15637230873108
## epoch[0] training(only) time: 68.88895964622498
# Switched to evaluate mode...
Test: [  0/100]	Time  0.737 ( 0.737)	Loss 4.1328e+00 (4.1328e+00)	Acc@1   4.00 (  4.00)	Acc@5  22.00 ( 22.00)
Test: [ 10/100]	Time  0.045 ( 0.113)	Loss 4.2305e+00 (4.2383e+00)	Acc@1   3.00 (  2.91)	Acc@5  11.00 ( 16.27)
Test: [ 20/100]	Time  0.045 ( 0.081)	Loss 4.1914e+00 (4.2459e+00)	Acc@1   4.00 (  3.52)	Acc@5  16.00 ( 16.62)
Test: [ 30/100]	Time  0.055 ( 0.070)	Loss 4.3125e+00 (4.2414e+00)	Acc@1   4.00 (  3.58)	Acc@5  17.00 ( 16.29)
Test: [ 40/100]	Time  0.046 ( 0.064)	Loss 4.2578e+00 (4.2327e+00)	Acc@1   2.00 (  3.78)	Acc@5  22.00 ( 16.71)
Test: [ 50/100]	Time  0.045 ( 0.061)	Loss 4.1992e+00 (4.2297e+00)	Acc@1   4.00 (  3.98)	Acc@5  22.00 ( 17.25)
Test: [ 60/100]	Time  0.045 ( 0.058)	Loss 4.2266e+00 (4.2248e+00)	Acc@1   6.00 (  4.13)	Acc@5  21.00 ( 17.08)
Test: [ 70/100]	Time  0.045 ( 0.057)	Loss 4.2734e+00 (4.2269e+00)	Acc@1   4.00 (  4.11)	Acc@5  14.00 ( 16.90)
Test: [ 80/100]	Time  0.045 ( 0.055)	Loss 4.3672e+00 (4.2340e+00)	Acc@1   2.00 (  4.05)	Acc@5  16.00 ( 16.79)
Test: [ 90/100]	Time  0.047 ( 0.054)	Loss 4.1250e+00 (4.2299e+00)	Acc@1   5.00 (  4.14)	Acc@5  16.00 ( 16.87)
 * Acc@1 4.170 Acc@5 16.670
### epoch[0] execution time: 74.34760689735413
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.346 ( 0.346)	Data  0.154 ( 0.154)	Loss 4.2344e+00 (4.2344e+00)	Acc@1   3.12 (  3.12)	Acc@5  19.53 ( 19.53)
Epoch: [1][ 10/391]	Time  0.158 ( 0.176)	Data  0.001 ( 0.015)	Loss 4.1094e+00 (4.2450e+00)	Acc@1   5.47 (  4.19)	Acc@5  21.09 ( 18.82)
Epoch: [1][ 20/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.008)	Loss 4.2969e+00 (4.2580e+00)	Acc@1   3.12 (  3.57)	Acc@5  18.75 ( 17.93)
Epoch: [1][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 4.2227e+00 (4.2437e+00)	Acc@1   2.34 (  3.96)	Acc@5  18.75 ( 17.62)
Epoch: [1][ 40/391]	Time  0.157 ( 0.164)	Data  0.001 ( 0.005)	Loss 4.2461e+00 (4.2380e+00)	Acc@1   2.34 (  4.04)	Acc@5  19.53 ( 17.64)
Epoch: [1][ 50/391]	Time  0.158 ( 0.163)	Data  0.001 ( 0.004)	Loss 4.2695e+00 (4.2347e+00)	Acc@1   3.12 (  4.03)	Acc@5  14.06 ( 17.85)
Epoch: [1][ 60/391]	Time  0.157 ( 0.163)	Data  0.001 ( 0.004)	Loss 4.2227e+00 (4.2278e+00)	Acc@1   2.34 (  4.20)	Acc@5  12.50 ( 17.62)
Epoch: [1][ 70/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.003)	Loss 4.2695e+00 (4.2277e+00)	Acc@1   1.56 (  4.17)	Acc@5  17.19 ( 17.65)
Epoch: [1][ 80/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 4.3398e+00 (4.2253e+00)	Acc@1   4.69 (  4.15)	Acc@5  19.53 ( 17.93)
Epoch: [1][ 90/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.003)	Loss 4.2422e+00 (4.2239e+00)	Acc@1   7.03 (  4.16)	Acc@5  18.75 ( 18.17)
Epoch: [1][100/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.003)	Loss 4.2656e+00 (4.2201e+00)	Acc@1   3.91 (  4.12)	Acc@5  16.41 ( 18.23)
Epoch: [1][110/391]	Time  0.169 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.2344e+00 (4.2172e+00)	Acc@1   5.47 (  4.16)	Acc@5  14.84 ( 18.27)
Epoch: [1][120/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.1719e+00 (4.2138e+00)	Acc@1   3.12 (  4.15)	Acc@5  19.53 ( 18.32)
Epoch: [1][130/391]	Time  0.163 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.1328e+00 (4.2103e+00)	Acc@1   5.47 (  4.23)	Acc@5  17.19 ( 18.43)
Epoch: [1][140/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.0898e+00 (4.2079e+00)	Acc@1   3.91 (  4.28)	Acc@5  19.53 ( 18.51)
Epoch: [1][150/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.1641e+00 (4.2068e+00)	Acc@1   7.03 (  4.33)	Acc@5  19.53 ( 18.48)
Epoch: [1][160/391]	Time  0.174 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.2773e+00 (4.2036e+00)	Acc@1   3.91 (  4.33)	Acc@5  15.62 ( 18.51)
Epoch: [1][170/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.1328e+00 (4.2009e+00)	Acc@1   7.81 (  4.44)	Acc@5  21.88 ( 18.59)
Epoch: [1][180/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.0859e+00 (4.1979e+00)	Acc@1   7.81 (  4.47)	Acc@5  26.56 ( 18.70)
Epoch: [1][190/391]	Time  0.163 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.0273e+00 (4.1955e+00)	Acc@1   3.91 (  4.47)	Acc@5  24.22 ( 18.77)
Epoch: [1][200/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.1172e+00 (4.1940e+00)	Acc@1   6.25 (  4.50)	Acc@5  20.31 ( 18.82)
Epoch: [1][210/391]	Time  0.170 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.1680e+00 (4.1912e+00)	Acc@1   4.69 (  4.54)	Acc@5  20.31 ( 18.87)
Epoch: [1][220/391]	Time  0.160 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.2969e+00 (4.1884e+00)	Acc@1   5.47 (  4.55)	Acc@5  16.41 ( 18.97)
Epoch: [1][230/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.1602e+00 (4.1877e+00)	Acc@1   4.69 (  4.52)	Acc@5  16.41 ( 18.96)
Epoch: [1][240/391]	Time  0.159 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.1250e+00 (4.1832e+00)	Acc@1   3.91 (  4.58)	Acc@5  19.53 ( 19.10)
Epoch: [1][250/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.2031e+00 (4.1819e+00)	Acc@1   2.34 (  4.56)	Acc@5  17.19 ( 19.18)
Epoch: [1][260/391]	Time  0.170 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.0156e+00 (4.1782e+00)	Acc@1   5.47 (  4.59)	Acc@5  24.22 ( 19.36)
Epoch: [1][270/391]	Time  0.159 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.0898e+00 (4.1768e+00)	Acc@1   9.38 (  4.66)	Acc@5  24.22 ( 19.43)
Epoch: [1][280/391]	Time  0.159 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.1445e+00 (4.1749e+00)	Acc@1   5.47 (  4.67)	Acc@5  22.66 ( 19.50)
Epoch: [1][290/391]	Time  0.159 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.1289e+00 (4.1732e+00)	Acc@1   2.34 (  4.69)	Acc@5  16.41 ( 19.48)
Epoch: [1][300/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.1133e+00 (4.1696e+00)	Acc@1  10.94 (  4.73)	Acc@5  18.75 ( 19.63)
Epoch: [1][310/391]	Time  0.169 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.0586e+00 (4.1672e+00)	Acc@1   6.25 (  4.77)	Acc@5  23.44 ( 19.69)
Epoch: [1][320/391]	Time  0.160 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.2109e+00 (4.1654e+00)	Acc@1   4.69 (  4.78)	Acc@5  20.31 ( 19.81)
Epoch: [1][330/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.0859e+00 (4.1637e+00)	Acc@1   5.47 (  4.80)	Acc@5  17.97 ( 19.85)
Epoch: [1][340/391]	Time  0.176 ( 0.160)	Data  0.001 ( 0.001)	Loss 4.0000e+00 (4.1611e+00)	Acc@1   6.25 (  4.82)	Acc@5  26.56 ( 19.96)
Epoch: [1][350/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.001)	Loss 4.2734e+00 (4.1591e+00)	Acc@1   5.47 (  4.86)	Acc@5  21.09 ( 20.00)
Epoch: [1][360/391]	Time  0.168 ( 0.160)	Data  0.001 ( 0.001)	Loss 4.0820e+00 (4.1565e+00)	Acc@1   2.34 (  4.90)	Acc@5  19.53 ( 20.09)
Epoch: [1][370/391]	Time  0.159 ( 0.160)	Data  0.001 ( 0.001)	Loss 3.9258e+00 (4.1544e+00)	Acc@1   7.03 (  4.94)	Acc@5  32.03 ( 20.18)
Epoch: [1][380/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.001)	Loss 4.0859e+00 (4.1534e+00)	Acc@1   4.69 (  4.95)	Acc@5  25.78 ( 20.23)
Epoch: [1][390/391]	Time  0.115 ( 0.160)	Data  0.001 ( 0.001)	Loss 3.9746e+00 (4.1519e+00)	Acc@1  12.50 (  5.00)	Acc@5  25.00 ( 20.28)
## e[1] optimizer.zero_grad (sum) time: 0.5813651084899902
## e[1]       loss.backward (sum) time: 12.24734377861023
## e[1]      optimizer.step (sum) time: 25.402199745178223
## epoch[1] training(only) time: 62.67408561706543
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 3.9473e+00 (3.9473e+00)	Acc@1  10.00 ( 10.00)	Acc@5  30.00 ( 30.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 4.1641e+00 (4.0218e+00)	Acc@1   3.00 (  7.36)	Acc@5  19.00 ( 25.45)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 3.9219e+00 (4.0256e+00)	Acc@1   9.00 (  7.29)	Acc@5  30.00 ( 25.52)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 4.1055e+00 (4.0177e+00)	Acc@1   7.00 (  7.42)	Acc@5  21.00 ( 26.00)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 4.0742e+00 (4.0235e+00)	Acc@1   7.00 (  6.90)	Acc@5  20.00 ( 25.17)
Test: [ 50/100]	Time  0.045 ( 0.049)	Loss 4.0039e+00 (4.0203e+00)	Acc@1   6.00 (  6.92)	Acc@5  29.00 ( 25.10)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 4.0664e+00 (4.0225e+00)	Acc@1   5.00 (  6.97)	Acc@5  26.00 ( 24.97)
Test: [ 70/100]	Time  0.046 ( 0.048)	Loss 4.1094e+00 (4.0227e+00)	Acc@1   6.00 (  6.89)	Acc@5  22.00 ( 24.65)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 4.1250e+00 (4.0280e+00)	Acc@1   3.00 (  6.99)	Acc@5  19.00 ( 24.77)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 3.9668e+00 (4.0271e+00)	Acc@1   7.00 (  7.04)	Acc@5  24.00 ( 24.71)
 * Acc@1 6.980 Acc@5 24.580
### epoch[1] execution time: 67.53512334823608
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.313 ( 0.313)	Data  0.151 ( 0.151)	Loss 3.9492e+00 (3.9492e+00)	Acc@1   4.69 (  4.69)	Acc@5  28.91 ( 28.91)
Epoch: [2][ 10/391]	Time  0.158 ( 0.174)	Data  0.001 ( 0.015)	Loss 4.0703e+00 (4.0176e+00)	Acc@1   7.03 (  6.61)	Acc@5  20.31 ( 23.86)
Epoch: [2][ 20/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.008)	Loss 4.0039e+00 (4.0529e+00)	Acc@1   6.25 (  5.88)	Acc@5  24.22 ( 23.21)
Epoch: [2][ 30/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.006)	Loss 4.0938e+00 (4.0550e+00)	Acc@1   5.47 (  6.22)	Acc@5  25.78 ( 23.71)
Epoch: [2][ 40/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.005)	Loss 4.1289e+00 (4.0569e+00)	Acc@1   5.47 (  6.19)	Acc@5  19.53 ( 23.78)
Epoch: [2][ 50/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.004)	Loss 4.1016e+00 (4.0665e+00)	Acc@1   7.03 (  6.10)	Acc@5  24.22 ( 23.44)
Epoch: [2][ 60/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.004)	Loss 4.1367e+00 (4.0727e+00)	Acc@1   5.47 (  5.90)	Acc@5  19.53 ( 23.13)
Epoch: [2][ 70/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.003)	Loss 4.2422e+00 (4.0665e+00)	Acc@1   3.91 (  5.96)	Acc@5  16.41 ( 23.39)
Epoch: [2][ 80/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.003)	Loss 3.9961e+00 (4.0645e+00)	Acc@1   5.47 (  5.99)	Acc@5  32.03 ( 23.64)
Epoch: [2][ 90/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.003)	Loss 4.1953e+00 (4.0687e+00)	Acc@1   7.03 (  5.98)	Acc@5  18.75 ( 23.51)
Epoch: [2][100/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.003)	Loss 4.2109e+00 (4.0645e+00)	Acc@1   6.25 (  6.15)	Acc@5  21.09 ( 23.65)
Epoch: [2][110/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.0508e+00 (4.0635e+00)	Acc@1   6.25 (  6.11)	Acc@5  24.22 ( 23.64)
Epoch: [2][120/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.9336e+00 (4.0595e+00)	Acc@1   5.47 (  6.13)	Acc@5  19.53 ( 23.71)
Epoch: [2][130/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.0430e+00 (4.0618e+00)	Acc@1   7.81 (  6.26)	Acc@5  20.31 ( 23.65)
Epoch: [2][140/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.0273e+00 (4.0585e+00)	Acc@1  10.16 (  6.28)	Acc@5  25.78 ( 23.76)
Epoch: [2][150/391]	Time  0.162 ( 0.161)	Data  0.001 ( 0.002)	Loss 4.1328e+00 (4.0550e+00)	Acc@1   9.38 (  6.39)	Acc@5  25.00 ( 23.99)
Epoch: [2][160/391]	Time  0.162 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.9785e+00 (4.0503e+00)	Acc@1  10.16 (  6.48)	Acc@5  24.22 ( 24.04)
Epoch: [2][170/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.9473e+00 (4.0464e+00)	Acc@1   7.03 (  6.53)	Acc@5  24.22 ( 24.09)
Epoch: [2][180/391]	Time  0.162 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.9160e+00 (4.0446e+00)	Acc@1  10.16 (  6.58)	Acc@5  27.34 ( 24.08)
Epoch: [2][190/391]	Time  0.162 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.9766e+00 (4.0411e+00)	Acc@1  12.50 (  6.59)	Acc@5  28.91 ( 24.14)
Epoch: [2][200/391]	Time  0.160 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.0586e+00 (4.0370e+00)	Acc@1  10.16 (  6.69)	Acc@5  28.12 ( 24.42)
Epoch: [2][210/391]	Time  0.159 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.9707e+00 (4.0321e+00)	Acc@1   7.81 (  6.74)	Acc@5  23.44 ( 24.67)
Epoch: [2][220/391]	Time  0.162 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.9062e+00 (4.0285e+00)	Acc@1   8.59 (  6.74)	Acc@5  25.78 ( 24.70)
Epoch: [2][230/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.9219e+00 (4.0271e+00)	Acc@1  11.72 (  6.78)	Acc@5  30.47 ( 24.76)
Epoch: [2][240/391]	Time  0.161 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.0508e+00 (4.0252e+00)	Acc@1   5.47 (  6.84)	Acc@5  25.00 ( 24.82)
Epoch: [2][250/391]	Time  0.160 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.8359e+00 (4.0228e+00)	Acc@1  16.41 (  6.93)	Acc@5  31.25 ( 24.90)
Epoch: [2][260/391]	Time  0.160 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.1094e+00 (4.0217e+00)	Acc@1   6.25 (  6.99)	Acc@5  24.22 ( 24.99)
Epoch: [2][270/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.9629e+00 (4.0185e+00)	Acc@1   9.38 (  7.03)	Acc@5  28.91 ( 25.09)
Epoch: [2][280/391]	Time  0.161 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.9414e+00 (4.0164e+00)	Acc@1  10.16 (  7.07)	Acc@5  26.56 ( 25.19)
Epoch: [2][290/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.0234e+00 (4.0134e+00)	Acc@1   4.69 (  7.13)	Acc@5  25.00 ( 25.26)
Epoch: [2][300/391]	Time  0.159 ( 0.160)	Data  0.001 ( 0.002)	Loss 4.0039e+00 (4.0116e+00)	Acc@1   6.25 (  7.18)	Acc@5  29.69 ( 25.32)
Epoch: [2][310/391]	Time  0.160 ( 0.160)	Data  0.001 ( 0.002)	Loss 3.6797e+00 (4.0082e+00)	Acc@1  15.62 (  7.24)	Acc@5  37.50 ( 25.43)
Epoch: [2][320/391]	Time  0.160 ( 0.160)	Data  0.001 ( 0.001)	Loss 4.0508e+00 (4.0050e+00)	Acc@1   5.47 (  7.30)	Acc@5  25.00 ( 25.52)
Epoch: [2][330/391]	Time  0.159 ( 0.160)	Data  0.001 ( 0.001)	Loss 3.9023e+00 (4.0030e+00)	Acc@1   8.59 (  7.34)	Acc@5  25.78 ( 25.62)
Epoch: [2][340/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.001)	Loss 4.0078e+00 (4.0010e+00)	Acc@1   8.59 (  7.38)	Acc@5  26.56 ( 25.67)
Epoch: [2][350/391]	Time  0.158 ( 0.160)	Data  0.001 ( 0.001)	Loss 4.0625e+00 (3.9987e+00)	Acc@1   9.38 (  7.45)	Acc@5  23.44 ( 25.74)
Epoch: [2][360/391]	Time  0.160 ( 0.160)	Data  0.001 ( 0.001)	Loss 4.1797e+00 (3.9965e+00)	Acc@1   7.03 (  7.46)	Acc@5  21.88 ( 25.81)
Epoch: [2][370/391]	Time  0.159 ( 0.160)	Data  0.001 ( 0.001)	Loss 4.0391e+00 (3.9935e+00)	Acc@1   7.81 (  7.49)	Acc@5  28.12 ( 25.91)
Epoch: [2][380/391]	Time  0.161 ( 0.160)	Data  0.001 ( 0.001)	Loss 3.8984e+00 (3.9914e+00)	Acc@1   7.03 (  7.51)	Acc@5  35.16 ( 25.98)
Epoch: [2][390/391]	Time  0.116 ( 0.160)	Data  0.001 ( 0.001)	Loss 3.8262e+00 (3.9903e+00)	Acc@1  15.00 (  7.52)	Acc@5  32.50 ( 25.98)
## e[2] optimizer.zero_grad (sum) time: 0.568964958190918
## e[2]       loss.backward (sum) time: 12.279006958007812
## e[2]      optimizer.step (sum) time: 25.444351196289062
## epoch[2] training(only) time: 62.63912558555603
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 3.9980e+00 (3.9980e+00)	Acc@1   7.00 (  7.00)	Acc@5  26.00 ( 26.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 4.0156e+00 (3.9996e+00)	Acc@1   9.00 (  9.00)	Acc@5  27.00 ( 27.36)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 3.8301e+00 (3.9898e+00)	Acc@1   9.00 (  8.57)	Acc@5  31.00 ( 28.00)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 4.1055e+00 (3.9758e+00)	Acc@1   6.00 (  8.58)	Acc@5  23.00 ( 28.10)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 4.1992e+00 (3.9747e+00)	Acc@1   7.00 (  8.56)	Acc@5  23.00 ( 28.17)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 3.9492e+00 (3.9665e+00)	Acc@1  11.00 (  8.37)	Acc@5  33.00 ( 28.49)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 3.8809e+00 (3.9577e+00)	Acc@1   9.00 (  8.49)	Acc@5  37.00 ( 28.51)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.9316e+00 (3.9617e+00)	Acc@1   8.00 (  8.34)	Acc@5  29.00 ( 28.25)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 4.0664e+00 (3.9674e+00)	Acc@1   5.00 (  8.16)	Acc@5  28.00 ( 28.28)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 3.9863e+00 (3.9646e+00)	Acc@1   5.00 (  8.14)	Acc@5  25.00 ( 28.36)
 * Acc@1 8.230 Acc@5 28.420
### epoch[2] execution time: 67.54236078262329
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.344 ( 0.344)	Data  0.181 ( 0.181)	Loss 3.7676e+00 (3.7676e+00)	Acc@1  11.72 ( 11.72)	Acc@5  36.72 ( 36.72)
Epoch: [3][ 10/391]	Time  0.158 ( 0.178)	Data  0.001 ( 0.017)	Loss 3.5820e+00 (3.8498e+00)	Acc@1  12.50 (  9.30)	Acc@5  37.50 ( 29.40)
Epoch: [3][ 20/391]	Time  0.159 ( 0.170)	Data  0.001 ( 0.010)	Loss 3.8359e+00 (3.8792e+00)	Acc@1   5.47 (  8.82)	Acc@5  26.56 ( 28.65)
Epoch: [3][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.007)	Loss 4.0078e+00 (3.8856e+00)	Acc@1   7.03 (  9.17)	Acc@5  27.34 ( 29.21)
Epoch: [3][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.7422e+00 (3.8768e+00)	Acc@1  10.16 (  9.38)	Acc@5  32.03 ( 29.50)
Epoch: [3][ 50/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.005)	Loss 3.9297e+00 (3.8834e+00)	Acc@1  10.16 (  9.30)	Acc@5  32.81 ( 29.63)
Epoch: [3][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.0352e+00 (3.8797e+00)	Acc@1   3.91 (  9.37)	Acc@5  26.56 ( 30.02)
Epoch: [3][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.004)	Loss 3.9219e+00 (3.8787e+00)	Acc@1   9.38 (  9.38)	Acc@5  28.91 ( 30.00)
Epoch: [3][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.7676e+00 (3.8829e+00)	Acc@1   7.81 (  9.32)	Acc@5  35.16 ( 30.03)
Epoch: [3][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.7812e+00 (3.8742e+00)	Acc@1   7.81 (  9.35)	Acc@5  34.38 ( 30.37)
Epoch: [3][100/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.7793e+00 (3.8748e+00)	Acc@1   8.59 (  9.34)	Acc@5  33.59 ( 30.24)
Epoch: [3][110/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.8223e+00 (3.8731e+00)	Acc@1   8.59 (  9.37)	Acc@5  32.81 ( 30.36)
Epoch: [3][120/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.5566e+00 (3.8687e+00)	Acc@1  13.28 (  9.43)	Acc@5  35.16 ( 30.40)
Epoch: [3][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9453e+00 (3.8661e+00)	Acc@1  13.28 (  9.49)	Acc@5  28.91 ( 30.61)
Epoch: [3][140/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7402e+00 (3.8650e+00)	Acc@1  12.50 (  9.56)	Acc@5  32.81 ( 30.57)
Epoch: [3][150/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0156e+00 (3.8630e+00)	Acc@1   5.47 (  9.60)	Acc@5  17.97 ( 30.61)
Epoch: [3][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5859e+00 (3.8587e+00)	Acc@1  11.72 (  9.65)	Acc@5  36.72 ( 30.66)
Epoch: [3][170/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7422e+00 (3.8542e+00)	Acc@1  10.94 (  9.73)	Acc@5  33.59 ( 30.83)
Epoch: [3][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9570e+00 (3.8555e+00)	Acc@1  10.16 (  9.68)	Acc@5  32.81 ( 30.83)
Epoch: [3][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8359e+00 (3.8542e+00)	Acc@1  14.06 (  9.73)	Acc@5  28.91 ( 30.88)
Epoch: [3][200/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9395e+00 (3.8531e+00)	Acc@1   7.03 (  9.74)	Acc@5  25.00 ( 30.96)
Epoch: [3][210/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6797e+00 (3.8510e+00)	Acc@1  16.41 (  9.80)	Acc@5  38.28 ( 31.03)
Epoch: [3][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9238e+00 (3.8491e+00)	Acc@1  10.94 (  9.83)	Acc@5  28.12 ( 31.12)
Epoch: [3][230/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.8203e+00 (3.8484e+00)	Acc@1  14.06 (  9.81)	Acc@5  32.81 ( 31.19)
Epoch: [3][240/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.7148e+00 (3.8485e+00)	Acc@1  10.16 (  9.80)	Acc@5  36.72 ( 31.18)
Epoch: [3][250/391]	Time  0.176 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6973e+00 (3.8467e+00)	Acc@1  11.72 (  9.84)	Acc@5  34.38 ( 31.20)
Epoch: [3][260/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.8691e+00 (3.8456e+00)	Acc@1   6.25 (  9.84)	Acc@5  28.91 ( 31.29)
Epoch: [3][270/391]	Time  0.162 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.9473e+00 (3.8441e+00)	Acc@1  10.16 (  9.79)	Acc@5  28.91 ( 31.31)
Epoch: [3][280/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6035e+00 (3.8402e+00)	Acc@1  14.06 (  9.82)	Acc@5  39.84 ( 31.41)
Epoch: [3][290/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.7188e+00 (3.8394e+00)	Acc@1  11.72 (  9.84)	Acc@5  31.25 ( 31.45)
Epoch: [3][300/391]	Time  0.170 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6543e+00 (3.8393e+00)	Acc@1  14.06 (  9.82)	Acc@5  35.16 ( 31.46)
Epoch: [3][310/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.7949e+00 (3.8370e+00)	Acc@1  12.50 (  9.85)	Acc@5  33.59 ( 31.49)
Epoch: [3][320/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.8867e+00 (3.8351e+00)	Acc@1  10.16 (  9.88)	Acc@5  28.91 ( 31.57)
Epoch: [3][330/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.7695e+00 (3.8337e+00)	Acc@1   8.59 (  9.87)	Acc@5  35.94 ( 31.60)
Epoch: [3][340/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.7773e+00 (3.8332e+00)	Acc@1  12.50 (  9.91)	Acc@5  32.03 ( 31.63)
Epoch: [3][350/391]	Time  0.169 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.8164e+00 (3.8300e+00)	Acc@1   7.81 (  9.93)	Acc@5  35.16 ( 31.72)
Epoch: [3][360/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.8848e+00 (3.8275e+00)	Acc@1  10.16 (  9.97)	Acc@5  31.25 ( 31.79)
Epoch: [3][370/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.9355e+00 (3.8272e+00)	Acc@1   8.59 ( 10.02)	Acc@5  27.34 ( 31.80)
Epoch: [3][380/391]	Time  0.171 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.5625e+00 (3.8244e+00)	Acc@1  14.84 ( 10.09)	Acc@5  45.31 ( 31.88)
Epoch: [3][390/391]	Time  0.116 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.6953e+00 (3.8224e+00)	Acc@1   7.50 ( 10.12)	Acc@5  37.50 ( 31.93)
## e[3] optimizer.zero_grad (sum) time: 0.5886070728302002
## e[3]       loss.backward (sum) time: 12.313308715820312
## e[3]      optimizer.step (sum) time: 25.49720025062561
## epoch[3] training(only) time: 63.09261465072632
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 3.7031e+00 (3.7031e+00)	Acc@1  13.00 ( 13.00)	Acc@5  39.00 ( 39.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 3.7090e+00 (3.6642e+00)	Acc@1  13.00 ( 13.18)	Acc@5  33.00 ( 37.18)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 3.4766e+00 (3.6559e+00)	Acc@1  11.00 ( 13.33)	Acc@5  41.00 ( 37.43)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.8105e+00 (3.6524e+00)	Acc@1  14.00 ( 13.97)	Acc@5  38.00 ( 37.81)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.7051e+00 (3.6417e+00)	Acc@1  10.00 ( 13.85)	Acc@5  32.00 ( 37.85)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.5703e+00 (3.6345e+00)	Acc@1  25.00 ( 13.94)	Acc@5  37.00 ( 38.12)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 3.5938e+00 (3.6329e+00)	Acc@1  14.00 ( 14.08)	Acc@5  37.00 ( 37.93)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.7383e+00 (3.6343e+00)	Acc@1  11.00 ( 13.80)	Acc@5  37.00 ( 37.70)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.8164e+00 (3.6422e+00)	Acc@1  12.00 ( 13.69)	Acc@5  34.00 ( 37.41)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 3.6035e+00 (3.6364e+00)	Acc@1  13.00 ( 13.75)	Acc@5  35.00 ( 37.79)
 * Acc@1 13.700 Acc@5 37.700
### epoch[3] execution time: 68.0067663192749
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.349 ( 0.349)	Data  0.186 ( 0.186)	Loss 3.7656e+00 (3.7656e+00)	Acc@1  14.84 ( 14.84)	Acc@5  33.59 ( 33.59)
Epoch: [4][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.018)	Loss 3.7793e+00 (3.7862e+00)	Acc@1  10.16 ( 11.01)	Acc@5  32.03 ( 32.03)
Epoch: [4][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.010)	Loss 3.7051e+00 (3.7399e+00)	Acc@1  12.50 ( 11.20)	Acc@5  37.50 ( 34.00)
Epoch: [4][ 30/391]	Time  0.159 ( 0.167)	Data  0.001 ( 0.007)	Loss 3.6758e+00 (3.7416e+00)	Acc@1  14.06 ( 11.09)	Acc@5  34.38 ( 34.15)
Epoch: [4][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.006)	Loss 3.7266e+00 (3.7433e+00)	Acc@1  11.72 ( 11.13)	Acc@5  35.16 ( 34.26)
Epoch: [4][ 50/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.005)	Loss 3.5137e+00 (3.7241e+00)	Acc@1  10.94 ( 11.60)	Acc@5  48.44 ( 35.03)
Epoch: [4][ 60/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.004)	Loss 3.7090e+00 (3.7236e+00)	Acc@1  13.28 ( 11.71)	Acc@5  33.59 ( 34.87)
Epoch: [4][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.004)	Loss 3.6270e+00 (3.7213e+00)	Acc@1  14.06 ( 11.84)	Acc@5  39.84 ( 34.83)
Epoch: [4][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.6680e+00 (3.7116e+00)	Acc@1  15.62 ( 12.13)	Acc@5  32.03 ( 35.16)
Epoch: [4][ 90/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.8086e+00 (3.7130e+00)	Acc@1   7.03 ( 12.08)	Acc@5  32.03 ( 35.04)
Epoch: [4][100/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.5664e+00 (3.7128e+00)	Acc@1  12.50 ( 12.07)	Acc@5  41.41 ( 35.06)
Epoch: [4][110/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.5898e+00 (3.7141e+00)	Acc@1  14.84 ( 12.17)	Acc@5  40.62 ( 34.92)
Epoch: [4][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.7188e+00 (3.7169e+00)	Acc@1  14.06 ( 12.11)	Acc@5  35.16 ( 34.76)
Epoch: [4][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0469e+00 (3.7171e+00)	Acc@1   8.59 ( 12.11)	Acc@5  23.44 ( 34.71)
Epoch: [4][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7539e+00 (3.7177e+00)	Acc@1  10.16 ( 12.07)	Acc@5  33.59 ( 34.71)
Epoch: [4][150/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4902e+00 (3.7127e+00)	Acc@1  16.41 ( 12.13)	Acc@5  37.50 ( 34.81)
Epoch: [4][160/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6445e+00 (3.7097e+00)	Acc@1  11.72 ( 12.24)	Acc@5  35.16 ( 34.93)
Epoch: [4][170/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4707e+00 (3.7081e+00)	Acc@1  14.84 ( 12.20)	Acc@5  39.84 ( 34.96)
Epoch: [4][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6172e+00 (3.7069e+00)	Acc@1  17.97 ( 12.19)	Acc@5  39.84 ( 35.13)
Epoch: [4][190/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.5293e+00 (3.7026e+00)	Acc@1  10.16 ( 12.25)	Acc@5  39.06 ( 35.16)
Epoch: [4][200/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6445e+00 (3.7017e+00)	Acc@1  10.94 ( 12.33)	Acc@5  39.06 ( 35.19)
Epoch: [4][210/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.7676e+00 (3.7021e+00)	Acc@1  14.06 ( 12.32)	Acc@5  36.72 ( 35.28)
Epoch: [4][220/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6074e+00 (3.7005e+00)	Acc@1  15.62 ( 12.35)	Acc@5  35.16 ( 35.28)
Epoch: [4][230/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6465e+00 (3.6967e+00)	Acc@1  12.50 ( 12.43)	Acc@5  35.94 ( 35.37)
Epoch: [4][240/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6914e+00 (3.6937e+00)	Acc@1  14.06 ( 12.51)	Acc@5  37.50 ( 35.49)
Epoch: [4][250/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.4824e+00 (3.6893e+00)	Acc@1  15.62 ( 12.58)	Acc@5  39.84 ( 35.64)
Epoch: [4][260/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6816e+00 (3.6880e+00)	Acc@1  11.72 ( 12.60)	Acc@5  36.72 ( 35.71)
Epoch: [4][270/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6934e+00 (3.6843e+00)	Acc@1  11.72 ( 12.61)	Acc@5  30.47 ( 35.74)
Epoch: [4][280/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.3848e+00 (3.6814e+00)	Acc@1  18.75 ( 12.67)	Acc@5  44.53 ( 35.90)
Epoch: [4][290/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.4902e+00 (3.6786e+00)	Acc@1  18.75 ( 12.72)	Acc@5  40.62 ( 35.98)
Epoch: [4][300/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6191e+00 (3.6769e+00)	Acc@1  13.28 ( 12.77)	Acc@5  37.50 ( 36.06)
Epoch: [4][310/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.5859e+00 (3.6740e+00)	Acc@1  14.06 ( 12.83)	Acc@5  40.62 ( 36.15)
Epoch: [4][320/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.5059e+00 (3.6728e+00)	Acc@1  19.53 ( 12.85)	Acc@5  40.62 ( 36.17)
Epoch: [4][330/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.5039e+00 (3.6691e+00)	Acc@1  17.97 ( 12.92)	Acc@5  42.19 ( 36.30)
Epoch: [4][340/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.7051e+00 (3.6691e+00)	Acc@1  14.84 ( 12.94)	Acc@5  35.16 ( 36.29)
Epoch: [4][350/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6211e+00 (3.6675e+00)	Acc@1  10.94 ( 13.00)	Acc@5  33.59 ( 36.30)
Epoch: [4][360/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.5234e+00 (3.6648e+00)	Acc@1   9.38 ( 13.02)	Acc@5  38.28 ( 36.37)
Epoch: [4][370/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.5781e+00 (3.6622e+00)	Acc@1  13.28 ( 13.10)	Acc@5  35.16 ( 36.44)
Epoch: [4][380/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.9121e+00 (3.6620e+00)	Acc@1   7.03 ( 13.13)	Acc@5  28.12 ( 36.43)
Epoch: [4][390/391]	Time  0.116 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.8145e+00 (3.6604e+00)	Acc@1   8.75 ( 13.16)	Acc@5  30.00 ( 36.51)
## e[4] optimizer.zero_grad (sum) time: 0.5780661106109619
## e[4]       loss.backward (sum) time: 12.337940216064453
## e[4]      optimizer.step (sum) time: 25.502745389938354
## epoch[4] training(only) time: 62.96204710006714
# Switched to evaluate mode...
Test: [  0/100]	Time  0.172 ( 0.172)	Loss 3.7266e+00 (3.7266e+00)	Acc@1  13.00 ( 13.00)	Acc@5  33.00 ( 33.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 3.6777e+00 (3.5678e+00)	Acc@1  14.00 ( 15.55)	Acc@5  36.00 ( 40.27)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 3.4316e+00 (3.5459e+00)	Acc@1  17.00 ( 15.14)	Acc@5  40.00 ( 40.62)
Test: [ 30/100]	Time  0.048 ( 0.051)	Loss 3.6348e+00 (3.5402e+00)	Acc@1  13.00 ( 15.32)	Acc@5  37.00 ( 41.29)
Test: [ 40/100]	Time  0.048 ( 0.050)	Loss 3.5156e+00 (3.5282e+00)	Acc@1  14.00 ( 15.39)	Acc@5  45.00 ( 41.56)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.5664e+00 (3.5331e+00)	Acc@1  21.00 ( 15.69)	Acc@5  42.00 ( 41.51)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 3.2637e+00 (3.5292e+00)	Acc@1  21.00 ( 15.49)	Acc@5  46.00 ( 41.34)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.6680e+00 (3.5345e+00)	Acc@1  12.00 ( 15.28)	Acc@5  44.00 ( 41.11)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.7734e+00 (3.5369e+00)	Acc@1  11.00 ( 15.17)	Acc@5  37.00 ( 40.88)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 3.5156e+00 (3.5293e+00)	Acc@1  12.00 ( 15.24)	Acc@5  37.00 ( 41.14)
 * Acc@1 15.190 Acc@5 41.210
### epoch[4] execution time: 67.90638852119446
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.317 ( 0.317)	Data  0.152 ( 0.152)	Loss 3.4707e+00 (3.4707e+00)	Acc@1   9.38 (  9.38)	Acc@5  42.97 ( 42.97)
Epoch: [5][ 10/391]	Time  0.160 ( 0.174)	Data  0.001 ( 0.015)	Loss 3.5957e+00 (3.5778e+00)	Acc@1  13.28 ( 14.99)	Acc@5  33.59 ( 38.14)
Epoch: [5][ 20/391]	Time  0.159 ( 0.167)	Data  0.001 ( 0.008)	Loss 3.4141e+00 (3.5789e+00)	Acc@1  17.19 ( 15.10)	Acc@5  43.75 ( 38.91)
Epoch: [5][ 30/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.006)	Loss 3.5703e+00 (3.5640e+00)	Acc@1  15.62 ( 15.40)	Acc@5  42.97 ( 39.82)
Epoch: [5][ 40/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.005)	Loss 3.4277e+00 (3.5559e+00)	Acc@1  18.75 ( 15.53)	Acc@5  45.31 ( 40.09)
Epoch: [5][ 50/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.004)	Loss 3.7988e+00 (3.5549e+00)	Acc@1  14.84 ( 15.38)	Acc@5  36.72 ( 40.10)
Epoch: [5][ 60/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.6660e+00 (3.5530e+00)	Acc@1  12.50 ( 15.32)	Acc@5  35.16 ( 40.05)
Epoch: [5][ 70/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.4590e+00 (3.5542e+00)	Acc@1  15.62 ( 15.21)	Acc@5  41.41 ( 40.03)
Epoch: [5][ 80/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.4609e+00 (3.5479e+00)	Acc@1  17.97 ( 15.11)	Acc@5  39.06 ( 40.16)
Epoch: [5][ 90/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.3555e+00 (3.5471e+00)	Acc@1  16.41 ( 15.16)	Acc@5  42.19 ( 40.18)
Epoch: [5][100/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.7168e+00 (3.5489e+00)	Acc@1  10.16 ( 15.09)	Acc@5  35.94 ( 40.08)
Epoch: [5][110/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6133e+00 (3.5469e+00)	Acc@1  14.84 ( 15.13)	Acc@5  36.72 ( 40.10)
Epoch: [5][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5156e+00 (3.5440e+00)	Acc@1  21.09 ( 15.15)	Acc@5  35.94 ( 40.12)
Epoch: [5][130/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4785e+00 (3.5406e+00)	Acc@1  16.41 ( 15.21)	Acc@5  35.94 ( 40.17)
Epoch: [5][140/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2988e+00 (3.5361e+00)	Acc@1  23.44 ( 15.29)	Acc@5  46.09 ( 40.38)
Epoch: [5][150/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6016e+00 (3.5344e+00)	Acc@1  11.72 ( 15.22)	Acc@5  42.19 ( 40.44)
Epoch: [5][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5586e+00 (3.5323e+00)	Acc@1  14.06 ( 15.30)	Acc@5  43.75 ( 40.54)
Epoch: [5][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5078e+00 (3.5273e+00)	Acc@1  13.28 ( 15.34)	Acc@5  43.75 ( 40.63)
Epoch: [5][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5195e+00 (3.5236e+00)	Acc@1  13.28 ( 15.40)	Acc@5  42.19 ( 40.70)
Epoch: [5][190/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5684e+00 (3.5217e+00)	Acc@1  16.41 ( 15.49)	Acc@5  38.28 ( 40.69)
Epoch: [5][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5605e+00 (3.5242e+00)	Acc@1  17.19 ( 15.45)	Acc@5  44.53 ( 40.64)
Epoch: [5][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4727e+00 (3.5202e+00)	Acc@1  20.31 ( 15.47)	Acc@5  39.06 ( 40.70)
Epoch: [5][220/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.5586e+00 (3.5219e+00)	Acc@1  17.19 ( 15.46)	Acc@5  42.97 ( 40.72)
Epoch: [5][230/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.4082e+00 (3.5219e+00)	Acc@1  15.62 ( 15.47)	Acc@5  42.19 ( 40.75)
Epoch: [5][240/391]	Time  0.167 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.4941e+00 (3.5209e+00)	Acc@1  18.75 ( 15.54)	Acc@5  41.41 ( 40.79)
Epoch: [5][250/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.6465e+00 (3.5206e+00)	Acc@1  14.06 ( 15.53)	Acc@5  38.28 ( 40.79)
Epoch: [5][260/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.4062e+00 (3.5192e+00)	Acc@1  14.84 ( 15.54)	Acc@5  42.97 ( 40.82)
Epoch: [5][270/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.4941e+00 (3.5159e+00)	Acc@1  11.72 ( 15.56)	Acc@5  42.97 ( 40.97)
Epoch: [5][280/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.5371e+00 (3.5151e+00)	Acc@1  13.28 ( 15.56)	Acc@5  42.19 ( 40.95)
Epoch: [5][290/391]	Time  0.181 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.3887e+00 (3.5142e+00)	Acc@1  15.62 ( 15.55)	Acc@5  42.19 ( 41.00)
Epoch: [5][300/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.7500e+00 (3.5144e+00)	Acc@1  10.16 ( 15.57)	Acc@5  36.72 ( 41.00)
Epoch: [5][310/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.4160e+00 (3.5106e+00)	Acc@1  16.41 ( 15.61)	Acc@5  41.41 ( 41.12)
Epoch: [5][320/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.2168e+00 (3.5085e+00)	Acc@1  14.06 ( 15.60)	Acc@5  53.12 ( 41.18)
Epoch: [5][330/391]	Time  0.163 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.4102e+00 (3.5063e+00)	Acc@1  14.84 ( 15.66)	Acc@5  41.41 ( 41.19)
Epoch: [5][340/391]	Time  0.170 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.4023e+00 (3.5065e+00)	Acc@1  17.97 ( 15.62)	Acc@5  36.72 ( 41.17)
Epoch: [5][350/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.6406e+00 (3.5049e+00)	Acc@1  12.50 ( 15.66)	Acc@5  33.59 ( 41.25)
Epoch: [5][360/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.2539e+00 (3.5029e+00)	Acc@1  17.97 ( 15.68)	Acc@5  51.56 ( 41.32)
Epoch: [5][370/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.2539e+00 (3.5005e+00)	Acc@1  20.31 ( 15.74)	Acc@5  52.34 ( 41.43)
Epoch: [5][380/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.3809e+00 (3.4974e+00)	Acc@1  14.06 ( 15.78)	Acc@5  35.16 ( 41.51)
Epoch: [5][390/391]	Time  0.124 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.2891e+00 (3.4956e+00)	Acc@1  20.00 ( 15.81)	Acc@5  48.75 ( 41.53)
## e[5] optimizer.zero_grad (sum) time: 0.577601432800293
## e[5]       loss.backward (sum) time: 12.313815832138062
## e[5]      optimizer.step (sum) time: 25.53658175468445
## epoch[5] training(only) time: 63.070958614349365
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 3.3047e+00 (3.3047e+00)	Acc@1  15.00 ( 15.00)	Acc@5  50.00 ( 50.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 3.3652e+00 (3.3102e+00)	Acc@1  15.00 ( 18.55)	Acc@5  43.00 ( 46.00)
Test: [ 20/100]	Time  0.056 ( 0.053)	Loss 2.9902e+00 (3.2768e+00)	Acc@1  19.00 ( 19.10)	Acc@5  57.00 ( 47.38)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 3.4414e+00 (3.2855e+00)	Acc@1  20.00 ( 19.32)	Acc@5  39.00 ( 47.71)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 3.4199e+00 (3.2788e+00)	Acc@1  21.00 ( 19.41)	Acc@5  43.00 ( 47.29)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 3.3184e+00 (3.2742e+00)	Acc@1  22.00 ( 19.75)	Acc@5  49.00 ( 47.61)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 3.1113e+00 (3.2719e+00)	Acc@1  23.00 ( 19.69)	Acc@5  50.00 ( 47.44)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.5742e+00 (3.2761e+00)	Acc@1  17.00 ( 19.63)	Acc@5  39.00 ( 47.08)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 3.4180e+00 (3.2830e+00)	Acc@1  17.00 ( 19.43)	Acc@5  46.00 ( 47.09)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 3.2637e+00 (3.2740e+00)	Acc@1  19.00 ( 19.77)	Acc@5  51.00 ( 47.66)
 * Acc@1 19.670 Acc@5 47.640
### epoch[5] execution time: 67.94720029830933
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.321 ( 0.321)	Data  0.151 ( 0.151)	Loss 3.3926e+00 (3.3926e+00)	Acc@1  17.19 ( 17.19)	Acc@5  49.22 ( 49.22)
Epoch: [6][ 10/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.015)	Loss 3.4004e+00 (3.3610e+00)	Acc@1  18.75 ( 19.03)	Acc@5  39.84 ( 46.59)
Epoch: [6][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 3.3457e+00 (3.3730e+00)	Acc@1  14.06 ( 17.93)	Acc@5  50.00 ( 45.65)
Epoch: [6][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.6016e+00 (3.3768e+00)	Acc@1  14.06 ( 17.92)	Acc@5  42.19 ( 45.11)
Epoch: [6][ 40/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.5156e+00 (3.3921e+00)	Acc@1   9.38 ( 17.70)	Acc@5  39.84 ( 44.65)
Epoch: [6][ 50/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.4922e+00 (3.3937e+00)	Acc@1  15.62 ( 17.68)	Acc@5  43.75 ( 44.52)
Epoch: [6][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.3867e+00 (3.3903e+00)	Acc@1  16.41 ( 17.65)	Acc@5  48.44 ( 44.54)
Epoch: [6][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.1191e+00 (3.3845e+00)	Acc@1  20.31 ( 17.66)	Acc@5  51.56 ( 44.62)
Epoch: [6][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.4512e+00 (3.3799e+00)	Acc@1  12.50 ( 17.68)	Acc@5  40.62 ( 44.87)
Epoch: [6][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.3574e+00 (3.3805e+00)	Acc@1  17.97 ( 17.80)	Acc@5  42.97 ( 44.73)
Epoch: [6][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0469e+00 (3.3717e+00)	Acc@1  21.88 ( 18.01)	Acc@5  53.91 ( 44.97)
Epoch: [6][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3926e+00 (3.3645e+00)	Acc@1  13.28 ( 17.95)	Acc@5  47.66 ( 45.00)
Epoch: [6][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4746e+00 (3.3639e+00)	Acc@1  18.75 ( 17.94)	Acc@5  42.97 ( 44.96)
Epoch: [6][130/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3477e+00 (3.3611e+00)	Acc@1  17.97 ( 17.97)	Acc@5  44.53 ( 44.95)
Epoch: [6][140/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4258e+00 (3.3613e+00)	Acc@1  25.00 ( 18.10)	Acc@5  42.97 ( 45.01)
Epoch: [6][150/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2539e+00 (3.3555e+00)	Acc@1  19.53 ( 18.35)	Acc@5  46.09 ( 45.11)
Epoch: [6][160/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2656e+00 (3.3516e+00)	Acc@1  18.75 ( 18.49)	Acc@5  44.53 ( 45.22)
Epoch: [6][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2441e+00 (3.3518e+00)	Acc@1  21.88 ( 18.53)	Acc@5  47.66 ( 45.28)
Epoch: [6][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2422e+00 (3.3547e+00)	Acc@1  22.66 ( 18.48)	Acc@5  52.34 ( 45.22)
Epoch: [6][190/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1875e+00 (3.3528e+00)	Acc@1  24.22 ( 18.58)	Acc@5  50.00 ( 45.36)
Epoch: [6][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4570e+00 (3.3539e+00)	Acc@1  14.84 ( 18.53)	Acc@5  47.66 ( 45.43)
Epoch: [6][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1836e+00 (3.3491e+00)	Acc@1  25.78 ( 18.65)	Acc@5  50.78 ( 45.53)
Epoch: [6][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1660e+00 (3.3481e+00)	Acc@1  17.19 ( 18.64)	Acc@5  55.47 ( 45.66)
Epoch: [6][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2852e+00 (3.3455e+00)	Acc@1  23.44 ( 18.67)	Acc@5  46.09 ( 45.72)
Epoch: [6][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2871e+00 (3.3418e+00)	Acc@1  21.09 ( 18.77)	Acc@5  46.09 ( 45.85)
Epoch: [6][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4453e+00 (3.3381e+00)	Acc@1  17.19 ( 18.91)	Acc@5  42.19 ( 45.94)
Epoch: [6][260/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4824e+00 (3.3380e+00)	Acc@1  20.31 ( 18.88)	Acc@5  46.88 ( 45.97)
Epoch: [6][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2910e+00 (3.3341e+00)	Acc@1  16.41 ( 18.99)	Acc@5  45.31 ( 46.06)
Epoch: [6][280/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2949e+00 (3.3296e+00)	Acc@1  21.09 ( 19.05)	Acc@5  46.88 ( 46.14)
Epoch: [6][290/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3828e+00 (3.3259e+00)	Acc@1  14.84 ( 19.09)	Acc@5  46.88 ( 46.20)
Epoch: [6][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2969e+00 (3.3223e+00)	Acc@1  22.66 ( 19.19)	Acc@5  45.31 ( 46.32)
Epoch: [6][310/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2715e+00 (3.3192e+00)	Acc@1  20.31 ( 19.22)	Acc@5  46.09 ( 46.42)
Epoch: [6][320/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2695e+00 (3.3186e+00)	Acc@1  22.66 ( 19.25)	Acc@5  48.44 ( 46.47)
Epoch: [6][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1406e+00 (3.3152e+00)	Acc@1  18.75 ( 19.37)	Acc@5  46.88 ( 46.54)
Epoch: [6][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.2754e+00 (3.3143e+00)	Acc@1  18.75 ( 19.35)	Acc@5  46.09 ( 46.56)
Epoch: [6][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.2090e+00 (3.3114e+00)	Acc@1  21.09 ( 19.40)	Acc@5  51.56 ( 46.68)
Epoch: [6][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.1387e+00 (3.3089e+00)	Acc@1  22.66 ( 19.41)	Acc@5  51.56 ( 46.75)
Epoch: [6][370/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.2500e+00 (3.3053e+00)	Acc@1  20.31 ( 19.44)	Acc@5  49.22 ( 46.86)
Epoch: [6][380/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.1523e+00 (3.3025e+00)	Acc@1  21.09 ( 19.48)	Acc@5  53.12 ( 46.98)
Epoch: [6][390/391]	Time  0.117 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.2363e+00 (3.3011e+00)	Acc@1  23.75 ( 19.52)	Acc@5  38.75 ( 47.05)
## e[6] optimizer.zero_grad (sum) time: 0.5844151973724365
## e[6]       loss.backward (sum) time: 12.434397459030151
## e[6]      optimizer.step (sum) time: 25.422338247299194
## epoch[6] training(only) time: 63.19893550872803
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 3.1387e+00 (3.1387e+00)	Acc@1  23.00 ( 23.00)	Acc@5  53.00 ( 53.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 3.2598e+00 (3.1864e+00)	Acc@1  19.00 ( 22.18)	Acc@5  55.00 ( 53.45)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 2.8262e+00 (3.1272e+00)	Acc@1  24.00 ( 22.48)	Acc@5  58.00 ( 53.90)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 3.3516e+00 (3.1215e+00)	Acc@1  16.00 ( 22.55)	Acc@5  41.00 ( 53.55)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 3.0605e+00 (3.1050e+00)	Acc@1  25.00 ( 22.88)	Acc@5  54.00 ( 53.98)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 3.1973e+00 (3.1073e+00)	Acc@1  24.00 ( 23.25)	Acc@5  51.00 ( 54.04)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 3.0781e+00 (3.1091e+00)	Acc@1  18.00 ( 23.31)	Acc@5  53.00 ( 54.10)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.3750e+00 (3.1114e+00)	Acc@1  19.00 ( 23.37)	Acc@5  48.00 ( 54.07)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 3.1328e+00 (3.1169e+00)	Acc@1  28.00 ( 23.26)	Acc@5  52.00 ( 53.69)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 3.0996e+00 (3.1135e+00)	Acc@1  19.00 ( 23.32)	Acc@5  50.00 ( 53.80)
 * Acc@1 23.200 Acc@5 53.680
### epoch[6] execution time: 68.12019395828247
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.317 ( 0.317)	Data  0.146 ( 0.146)	Loss 2.9434e+00 (2.9434e+00)	Acc@1  23.44 ( 23.44)	Acc@5  53.91 ( 53.91)
Epoch: [7][ 10/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.014)	Loss 3.1914e+00 (3.1344e+00)	Acc@1  24.22 ( 22.94)	Acc@5  49.22 ( 52.27)
Epoch: [7][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.3262e+00 (3.1686e+00)	Acc@1  21.88 ( 21.91)	Acc@5  47.66 ( 51.23)
Epoch: [7][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.2266e+00 (3.1899e+00)	Acc@1  21.09 ( 21.70)	Acc@5  51.56 ( 49.90)
Epoch: [7][ 40/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.1484e+00 (3.1751e+00)	Acc@1  18.75 ( 21.70)	Acc@5  53.12 ( 50.21)
Epoch: [7][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.2129e+00 (3.1697e+00)	Acc@1  19.53 ( 21.78)	Acc@5  47.66 ( 50.58)
Epoch: [7][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.2539e+00 (3.1698e+00)	Acc@1  20.31 ( 21.93)	Acc@5  51.56 ( 50.56)
Epoch: [7][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.0938e+00 (3.1651e+00)	Acc@1  17.97 ( 21.88)	Acc@5  55.47 ( 50.69)
Epoch: [7][ 80/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.2227e+00 (3.1715e+00)	Acc@1  24.22 ( 21.80)	Acc@5  46.88 ( 50.42)
Epoch: [7][ 90/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 3.2441e+00 (3.1753e+00)	Acc@1  18.75 ( 21.81)	Acc@5  45.31 ( 50.34)
Epoch: [7][100/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0273e+00 (3.1719e+00)	Acc@1  24.22 ( 21.82)	Acc@5  49.22 ( 50.25)
Epoch: [7][110/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0020e+00 (3.1615e+00)	Acc@1  24.22 ( 21.88)	Acc@5  51.56 ( 50.60)
Epoch: [7][120/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3691e+00 (3.1648e+00)	Acc@1  14.06 ( 21.80)	Acc@5  44.53 ( 50.48)
Epoch: [7][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3086e+00 (3.1662e+00)	Acc@1  21.09 ( 21.83)	Acc@5  48.44 ( 50.43)
Epoch: [7][140/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2480e+00 (3.1657e+00)	Acc@1  20.31 ( 21.85)	Acc@5  51.56 ( 50.55)
Epoch: [7][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0234e+00 (3.1601e+00)	Acc@1  19.53 ( 21.90)	Acc@5  51.56 ( 50.67)
Epoch: [7][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9531e+00 (3.1567e+00)	Acc@1  28.91 ( 22.01)	Acc@5  52.34 ( 50.67)
Epoch: [7][170/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9863e+00 (3.1511e+00)	Acc@1  24.22 ( 22.11)	Acc@5  57.81 ( 50.81)
Epoch: [7][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7051e+00 (3.1432e+00)	Acc@1  32.03 ( 22.17)	Acc@5  60.16 ( 51.05)
Epoch: [7][190/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0039e+00 (3.1384e+00)	Acc@1  26.56 ( 22.26)	Acc@5  57.81 ( 51.26)
Epoch: [7][200/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.9844e+00 (3.1391e+00)	Acc@1  25.00 ( 22.29)	Acc@5  53.91 ( 51.19)
Epoch: [7][210/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.1641e+00 (3.1358e+00)	Acc@1  22.66 ( 22.37)	Acc@5  48.44 ( 51.21)
Epoch: [7][220/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.1016e+00 (3.1395e+00)	Acc@1  24.22 ( 22.33)	Acc@5  53.12 ( 51.13)
Epoch: [7][230/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0996e+00 (3.1378e+00)	Acc@1  27.34 ( 22.31)	Acc@5  50.78 ( 51.23)
Epoch: [7][240/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0410e+00 (3.1342e+00)	Acc@1  26.56 ( 22.42)	Acc@5  52.34 ( 51.28)
Epoch: [7][250/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.2559e+00 (3.1304e+00)	Acc@1  15.62 ( 22.48)	Acc@5  55.47 ( 51.41)
Epoch: [7][260/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0410e+00 (3.1295e+00)	Acc@1  28.12 ( 22.51)	Acc@5  53.12 ( 51.46)
Epoch: [7][270/391]	Time  0.163 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.9902e+00 (3.1243e+00)	Acc@1  21.88 ( 22.62)	Acc@5  53.91 ( 51.65)
Epoch: [7][280/391]	Time  0.166 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0176e+00 (3.1204e+00)	Acc@1  22.66 ( 22.69)	Acc@5  47.66 ( 51.74)
Epoch: [7][290/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0977e+00 (3.1183e+00)	Acc@1  20.31 ( 22.71)	Acc@5  53.91 ( 51.81)
Epoch: [7][300/391]	Time  0.163 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.8887e+00 (3.1153e+00)	Acc@1  24.22 ( 22.74)	Acc@5  55.47 ( 51.88)
Epoch: [7][310/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.1250e+00 (3.1132e+00)	Acc@1  22.66 ( 22.80)	Acc@5  49.22 ( 51.93)
Epoch: [7][320/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.9805e+00 (3.1109e+00)	Acc@1  33.59 ( 22.85)	Acc@5  60.16 ( 51.96)
Epoch: [7][330/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.1875e+00 (3.1058e+00)	Acc@1  21.88 ( 22.93)	Acc@5  53.12 ( 52.08)
Epoch: [7][340/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.0000e+00 (3.1034e+00)	Acc@1  25.78 ( 22.99)	Acc@5  54.69 ( 52.16)
Epoch: [7][350/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.1328e+00 (3.0994e+00)	Acc@1  21.88 ( 23.05)	Acc@5  53.91 ( 52.28)
Epoch: [7][360/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.0449e+00 (3.0976e+00)	Acc@1  24.22 ( 23.06)	Acc@5  51.56 ( 52.32)
Epoch: [7][370/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.8770e+00 (3.0940e+00)	Acc@1  28.12 ( 23.15)	Acc@5  60.16 ( 52.42)
Epoch: [7][380/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.2109e+00 (3.0920e+00)	Acc@1  19.53 ( 23.16)	Acc@5  49.22 ( 52.51)
Epoch: [7][390/391]	Time  0.113 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.9531e+00 (3.0911e+00)	Acc@1  21.25 ( 23.19)	Acc@5  58.75 ( 52.56)
## e[7] optimizer.zero_grad (sum) time: 0.5787558555603027
## e[7]       loss.backward (sum) time: 12.378821849822998
## e[7]      optimizer.step (sum) time: 25.461713790893555
## epoch[7] training(only) time: 63.01139450073242
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 2.8984e+00 (2.8984e+00)	Acc@1  28.00 ( 28.00)	Acc@5  59.00 ( 59.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.9297e+00 (2.8931e+00)	Acc@1  24.00 ( 28.55)	Acc@5  55.00 ( 59.27)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 2.6211e+00 (2.8620e+00)	Acc@1  29.00 ( 27.76)	Acc@5  68.00 ( 59.71)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 3.0312e+00 (2.8449e+00)	Acc@1  25.00 ( 28.45)	Acc@5  52.00 ( 59.84)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 2.6699e+00 (2.8313e+00)	Acc@1  32.00 ( 28.71)	Acc@5  62.00 ( 59.76)
Test: [ 50/100]	Time  0.056 ( 0.050)	Loss 2.9277e+00 (2.8344e+00)	Acc@1  39.00 ( 29.08)	Acc@5  59.00 ( 59.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.8496e+00 (2.8333e+00)	Acc@1  28.00 ( 29.10)	Acc@5  60.00 ( 59.57)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.9238e+00 (2.8413e+00)	Acc@1  22.00 ( 28.86)	Acc@5  60.00 ( 59.46)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 2.9375e+00 (2.8547e+00)	Acc@1  31.00 ( 28.35)	Acc@5  55.00 ( 59.12)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.6836e+00 (2.8474e+00)	Acc@1  30.00 ( 28.49)	Acc@5  67.00 ( 59.23)
 * Acc@1 28.640 Acc@5 59.260
### epoch[7] execution time: 67.89936518669128
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.318 ( 0.318)	Data  0.153 ( 0.153)	Loss 2.8086e+00 (2.8086e+00)	Acc@1  28.12 ( 28.12)	Acc@5  60.16 ( 60.16)
Epoch: [8][ 10/391]	Time  0.172 ( 0.175)	Data  0.001 ( 0.015)	Loss 2.9590e+00 (2.9361e+00)	Acc@1  22.66 ( 24.29)	Acc@5  58.59 ( 56.11)
Epoch: [8][ 20/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.008)	Loss 2.8789e+00 (2.9385e+00)	Acc@1  30.47 ( 25.67)	Acc@5  60.94 ( 56.73)
Epoch: [8][ 30/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.006)	Loss 2.9082e+00 (2.9534e+00)	Acc@1  28.91 ( 25.28)	Acc@5  59.38 ( 56.40)
Epoch: [8][ 40/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.005)	Loss 2.5996e+00 (2.9588e+00)	Acc@1  33.59 ( 25.29)	Acc@5  64.06 ( 56.33)
Epoch: [8][ 50/391]	Time  0.158 ( 0.163)	Data  0.001 ( 0.004)	Loss 2.8926e+00 (2.9439e+00)	Acc@1  26.56 ( 25.75)	Acc@5  59.38 ( 56.57)
Epoch: [8][ 60/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.004)	Loss 3.0586e+00 (2.9565e+00)	Acc@1  25.00 ( 25.50)	Acc@5  50.00 ( 56.39)
Epoch: [8][ 70/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 2.8672e+00 (2.9578e+00)	Acc@1  27.34 ( 25.30)	Acc@5  60.16 ( 56.44)
Epoch: [8][ 80/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.003)	Loss 2.6172e+00 (2.9631e+00)	Acc@1  28.91 ( 25.29)	Acc@5  64.84 ( 56.35)
Epoch: [8][ 90/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.003)	Loss 2.9766e+00 (2.9549e+00)	Acc@1  25.00 ( 25.53)	Acc@5  54.69 ( 56.51)
Epoch: [8][100/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 2.9141e+00 (2.9503e+00)	Acc@1  26.56 ( 25.60)	Acc@5  51.56 ( 56.64)
Epoch: [8][110/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1211e+00 (2.9477e+00)	Acc@1  26.56 ( 25.60)	Acc@5  57.03 ( 56.64)
Epoch: [8][120/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9570e+00 (2.9443e+00)	Acc@1  26.56 ( 25.73)	Acc@5  57.03 ( 56.64)
Epoch: [8][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0156e+00 (2.9432e+00)	Acc@1  28.12 ( 25.92)	Acc@5  54.69 ( 56.66)
Epoch: [8][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7363e+00 (2.9401e+00)	Acc@1  32.03 ( 25.92)	Acc@5  60.94 ( 56.67)
Epoch: [8][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7949e+00 (2.9367e+00)	Acc@1  28.91 ( 25.99)	Acc@5  64.84 ( 56.76)
Epoch: [8][160/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.7637e+00 (2.9322e+00)	Acc@1  29.69 ( 26.08)	Acc@5  64.06 ( 56.93)
Epoch: [8][170/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0391e+00 (2.9289e+00)	Acc@1  27.34 ( 26.17)	Acc@5  47.66 ( 56.97)
Epoch: [8][180/391]	Time  0.164 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.8047e+00 (2.9244e+00)	Acc@1  31.25 ( 26.34)	Acc@5  63.28 ( 57.13)
Epoch: [8][190/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.7031e+00 (2.9192e+00)	Acc@1  30.47 ( 26.44)	Acc@5  58.59 ( 57.20)
Epoch: [8][200/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.7402e+00 (2.9180e+00)	Acc@1  32.81 ( 26.48)	Acc@5  57.81 ( 57.24)
Epoch: [8][210/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.8789e+00 (2.9150e+00)	Acc@1  25.78 ( 26.55)	Acc@5  57.03 ( 57.31)
Epoch: [8][220/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.7422e+00 (2.9115e+00)	Acc@1  32.81 ( 26.57)	Acc@5  64.84 ( 57.42)
Epoch: [8][230/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.9688e+00 (2.9114e+00)	Acc@1  32.03 ( 26.71)	Acc@5  53.12 ( 57.40)
Epoch: [8][240/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0469e+00 (2.9076e+00)	Acc@1  28.12 ( 26.82)	Acc@5  57.81 ( 57.49)
Epoch: [8][250/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0000e+00 (2.9077e+00)	Acc@1  28.91 ( 26.88)	Acc@5  55.47 ( 57.49)
Epoch: [8][260/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.8418e+00 (2.9045e+00)	Acc@1  27.34 ( 26.95)	Acc@5  60.16 ( 57.57)
Epoch: [8][270/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.7598e+00 (2.9004e+00)	Acc@1  28.12 ( 27.02)	Acc@5  56.25 ( 57.65)
Epoch: [8][280/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.9941e+00 (2.8984e+00)	Acc@1  24.22 ( 27.10)	Acc@5  61.72 ( 57.72)
Epoch: [8][290/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.9805e+00 (2.8973e+00)	Acc@1  27.34 ( 27.11)	Acc@5  51.56 ( 57.76)
Epoch: [8][300/391]	Time  0.167 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.7832e+00 (2.8968e+00)	Acc@1  25.00 ( 27.10)	Acc@5  59.38 ( 57.77)
Epoch: [8][310/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 3.0840e+00 (2.8971e+00)	Acc@1  25.00 ( 27.11)	Acc@5  57.81 ( 57.77)
Epoch: [8][320/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.7539e+00 (2.8956e+00)	Acc@1  30.47 ( 27.11)	Acc@5  60.16 ( 57.77)
Epoch: [8][330/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.9590e+00 (2.8938e+00)	Acc@1  28.12 ( 27.19)	Acc@5  57.03 ( 57.79)
Epoch: [8][340/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.6699e+00 (2.8902e+00)	Acc@1  34.38 ( 27.27)	Acc@5  64.06 ( 57.88)
Epoch: [8][350/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.7910e+00 (2.8879e+00)	Acc@1  32.81 ( 27.30)	Acc@5  60.16 ( 57.94)
Epoch: [8][360/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.7070e+00 (2.8854e+00)	Acc@1  32.03 ( 27.37)	Acc@5  58.59 ( 58.00)
Epoch: [8][370/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.7793e+00 (2.8841e+00)	Acc@1  29.69 ( 27.39)	Acc@5  60.94 ( 58.03)
Epoch: [8][380/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.8086e+00 (2.8803e+00)	Acc@1  31.25 ( 27.50)	Acc@5  58.59 ( 58.11)
Epoch: [8][390/391]	Time  0.113 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.9277e+00 (2.8787e+00)	Acc@1  22.50 ( 27.53)	Acc@5  58.75 ( 58.15)
## e[8] optimizer.zero_grad (sum) time: 0.5811786651611328
## e[8]       loss.backward (sum) time: 12.377851009368896
## e[8]      optimizer.step (sum) time: 25.45576500892639
## epoch[8] training(only) time: 63.01179099082947
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 2.8945e+00 (2.8945e+00)	Acc@1  25.00 ( 25.00)	Acc@5  57.00 ( 57.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.7969e+00 (2.7749e+00)	Acc@1  27.00 ( 29.91)	Acc@5  59.00 ( 61.36)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 2.6875e+00 (2.7447e+00)	Acc@1  27.00 ( 29.90)	Acc@5  63.00 ( 61.67)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.8926e+00 (2.7518e+00)	Acc@1  28.00 ( 30.19)	Acc@5  49.00 ( 61.03)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 2.7520e+00 (2.7427e+00)	Acc@1  37.00 ( 30.80)	Acc@5  59.00 ( 60.90)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 2.9316e+00 (2.7556e+00)	Acc@1  31.00 ( 30.67)	Acc@5  56.00 ( 61.00)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.6172e+00 (2.7465e+00)	Acc@1  31.00 ( 30.89)	Acc@5  71.00 ( 61.26)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.9219e+00 (2.7487e+00)	Acc@1  24.00 ( 30.79)	Acc@5  56.00 ( 61.07)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 3.1074e+00 (2.7634e+00)	Acc@1  26.00 ( 30.49)	Acc@5  49.00 ( 60.75)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 2.6816e+00 (2.7574e+00)	Acc@1  29.00 ( 30.64)	Acc@5  62.00 ( 60.90)
 * Acc@1 30.860 Acc@5 60.980
### epoch[8] execution time: 67.97093963623047
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.310 ( 0.310)	Data  0.132 ( 0.132)	Loss 2.8242e+00 (2.8242e+00)	Acc@1  31.25 ( 31.25)	Acc@5  61.72 ( 61.72)
Epoch: [9][ 10/391]	Time  0.160 ( 0.174)	Data  0.001 ( 0.013)	Loss 2.8555e+00 (2.7710e+00)	Acc@1  28.12 ( 28.27)	Acc@5  60.94 ( 61.22)
Epoch: [9][ 20/391]	Time  0.159 ( 0.167)	Data  0.001 ( 0.007)	Loss 2.7012e+00 (2.7595e+00)	Acc@1  32.81 ( 29.99)	Acc@5  59.38 ( 60.23)
Epoch: [9][ 30/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.8027e+00 (2.7732e+00)	Acc@1  25.78 ( 30.09)	Acc@5  60.94 ( 60.26)
Epoch: [9][ 40/391]	Time  0.164 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.9336e+00 (2.7623e+00)	Acc@1  26.56 ( 29.90)	Acc@5  55.47 ( 60.63)
Epoch: [9][ 50/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.7207e+00 (2.7538e+00)	Acc@1  29.69 ( 30.12)	Acc@5  60.94 ( 61.03)
Epoch: [9][ 60/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.8320e+00 (2.7575e+00)	Acc@1  31.25 ( 29.99)	Acc@5  60.16 ( 61.09)
Epoch: [9][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.7148e+00 (2.7443e+00)	Acc@1  36.72 ( 30.19)	Acc@5  62.50 ( 61.29)
Epoch: [9][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.9453e+00 (2.7555e+00)	Acc@1  27.34 ( 30.08)	Acc@5  55.47 ( 60.87)
Epoch: [9][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5547e+00 (2.7607e+00)	Acc@1  32.03 ( 29.94)	Acc@5  64.84 ( 60.72)
Epoch: [9][100/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9961e+00 (2.7676e+00)	Acc@1  19.53 ( 29.73)	Acc@5  56.25 ( 60.75)
Epoch: [9][110/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0293e+00 (2.7670e+00)	Acc@1  22.66 ( 29.70)	Acc@5  51.56 ( 60.84)
Epoch: [9][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6914e+00 (2.7621e+00)	Acc@1  32.03 ( 29.74)	Acc@5  57.81 ( 60.90)
Epoch: [9][130/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7207e+00 (2.7639e+00)	Acc@1  23.44 ( 29.68)	Acc@5  62.50 ( 60.84)
Epoch: [9][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0859e+00 (2.7665e+00)	Acc@1  24.22 ( 29.64)	Acc@5  51.56 ( 60.83)
Epoch: [9][150/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5898e+00 (2.7654e+00)	Acc@1  31.25 ( 29.68)	Acc@5  64.84 ( 60.99)
Epoch: [9][160/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7441e+00 (2.7633e+00)	Acc@1  31.25 ( 29.64)	Acc@5  62.50 ( 60.98)
Epoch: [9][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5820e+00 (2.7562e+00)	Acc@1  32.03 ( 29.80)	Acc@5  64.84 ( 61.09)
Epoch: [9][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4434e+00 (2.7475e+00)	Acc@1  34.38 ( 30.08)	Acc@5  64.84 ( 61.28)
Epoch: [9][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5996e+00 (2.7436e+00)	Acc@1  30.47 ( 30.17)	Acc@5  65.62 ( 61.30)
Epoch: [9][200/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8867e+00 (2.7426e+00)	Acc@1  25.00 ( 30.17)	Acc@5  52.34 ( 61.33)
Epoch: [9][210/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6504e+00 (2.7398e+00)	Acc@1  37.50 ( 30.31)	Acc@5  64.84 ( 61.39)
Epoch: [9][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6191e+00 (2.7328e+00)	Acc@1  32.81 ( 30.46)	Acc@5  63.28 ( 61.55)
Epoch: [9][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7793e+00 (2.7295e+00)	Acc@1  28.91 ( 30.53)	Acc@5  57.81 ( 61.66)
Epoch: [9][240/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9688e+00 (2.7270e+00)	Acc@1  23.44 ( 30.56)	Acc@5  60.94 ( 61.78)
Epoch: [9][250/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6289e+00 (2.7264e+00)	Acc@1  32.03 ( 30.58)	Acc@5  61.72 ( 61.79)
Epoch: [9][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7324e+00 (2.7264e+00)	Acc@1  33.59 ( 30.62)	Acc@5  59.38 ( 61.76)
Epoch: [9][270/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7891e+00 (2.7271e+00)	Acc@1  27.34 ( 30.61)	Acc@5  55.47 ( 61.68)
Epoch: [9][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.6699e+00 (2.7269e+00)	Acc@1  29.69 ( 30.61)	Acc@5  62.50 ( 61.72)
Epoch: [9][290/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5742e+00 (2.7213e+00)	Acc@1  32.81 ( 30.73)	Acc@5  64.06 ( 61.88)
Epoch: [9][300/391]	Time  0.164 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.6289e+00 (2.7196e+00)	Acc@1  29.69 ( 30.73)	Acc@5  64.06 ( 61.95)
Epoch: [9][310/391]	Time  0.163 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.4551e+00 (2.7181e+00)	Acc@1  35.16 ( 30.79)	Acc@5  68.75 ( 61.98)
Epoch: [9][320/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.4551e+00 (2.7142e+00)	Acc@1  38.28 ( 30.92)	Acc@5  67.97 ( 62.05)
Epoch: [9][330/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.6484e+00 (2.7133e+00)	Acc@1  31.25 ( 30.92)	Acc@5  67.19 ( 62.07)
Epoch: [9][340/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.2734e+00 (2.7104e+00)	Acc@1  39.84 ( 31.03)	Acc@5  72.66 ( 62.13)
Epoch: [9][350/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.6797e+00 (2.7088e+00)	Acc@1  28.91 ( 31.04)	Acc@5  62.50 ( 62.16)
Epoch: [9][360/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.6406e+00 (2.7054e+00)	Acc@1  34.38 ( 31.08)	Acc@5  64.06 ( 62.26)
Epoch: [9][370/391]	Time  0.172 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.7773e+00 (2.7042e+00)	Acc@1  31.25 ( 31.09)	Acc@5  63.28 ( 62.33)
Epoch: [9][380/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.5996e+00 (2.7041e+00)	Acc@1  37.50 ( 31.11)	Acc@5  67.97 ( 62.36)
Epoch: [9][390/391]	Time  0.114 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.8574e+00 (2.7023e+00)	Acc@1  33.75 ( 31.16)	Acc@5  58.75 ( 62.39)
## e[9] optimizer.zero_grad (sum) time: 0.5860500335693359
## e[9]       loss.backward (sum) time: 12.419227361679077
## e[9]      optimizer.step (sum) time: 25.44525718688965
## epoch[9] training(only) time: 63.120168924331665
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.8086e+00 (2.8086e+00)	Acc@1  33.00 ( 33.00)	Acc@5  64.00 ( 64.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.9590e+00 (2.8498e+00)	Acc@1  27.00 ( 31.73)	Acc@5  59.00 ( 62.45)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 2.7090e+00 (2.7911e+00)	Acc@1  31.00 ( 31.38)	Acc@5  68.00 ( 62.57)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.8066e+00 (2.7865e+00)	Acc@1  30.00 ( 31.19)	Acc@5  64.00 ( 62.84)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 2.7617e+00 (2.7834e+00)	Acc@1  34.00 ( 31.51)	Acc@5  62.00 ( 62.54)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.8574e+00 (2.7882e+00)	Acc@1  28.00 ( 31.47)	Acc@5  57.00 ( 62.20)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.7539e+00 (2.7790e+00)	Acc@1  30.00 ( 31.39)	Acc@5  66.00 ( 62.61)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 3.0723e+00 (2.7819e+00)	Acc@1  25.00 ( 31.30)	Acc@5  55.00 ( 62.48)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 3.0547e+00 (2.7973e+00)	Acc@1  24.00 ( 30.90)	Acc@5  58.00 ( 62.02)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.8164e+00 (2.7894e+00)	Acc@1  34.00 ( 31.14)	Acc@5  63.00 ( 62.25)
 * Acc@1 31.200 Acc@5 62.290
### epoch[9] execution time: 68.01568460464478
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.329 ( 0.329)	Data  0.154 ( 0.154)	Loss 2.7402e+00 (2.7402e+00)	Acc@1  29.69 ( 29.69)	Acc@5  61.72 ( 61.72)
Epoch: [10][ 10/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.015)	Loss 2.4453e+00 (2.5675e+00)	Acc@1  35.16 ( 33.95)	Acc@5  71.09 ( 65.34)
Epoch: [10][ 20/391]	Time  0.158 ( 0.168)	Data  0.001 ( 0.008)	Loss 2.5371e+00 (2.5466e+00)	Acc@1  39.06 ( 35.19)	Acc@5  67.19 ( 66.41)
Epoch: [10][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.7051e+00 (2.5188e+00)	Acc@1  35.16 ( 34.90)	Acc@5  62.50 ( 67.29)
Epoch: [10][ 40/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.005)	Loss 2.4531e+00 (2.5292e+00)	Acc@1  39.06 ( 34.91)	Acc@5  63.28 ( 66.65)
Epoch: [10][ 50/391]	Time  0.170 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.2402e+00 (2.5366e+00)	Acc@1  38.28 ( 34.44)	Acc@5  78.12 ( 66.56)
Epoch: [10][ 60/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.5859e+00 (2.5520e+00)	Acc@1  33.59 ( 34.16)	Acc@5  65.62 ( 66.20)
Epoch: [10][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3887e+00 (2.5524e+00)	Acc@1  35.94 ( 33.93)	Acc@5  64.84 ( 66.09)
Epoch: [10][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.8027e+00 (2.5643e+00)	Acc@1  26.56 ( 33.86)	Acc@5  62.50 ( 65.86)
Epoch: [10][ 90/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 2.4570e+00 (2.5651e+00)	Acc@1  37.50 ( 34.00)	Acc@5  71.88 ( 65.81)
Epoch: [10][100/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4766e+00 (2.5639e+00)	Acc@1  40.62 ( 34.07)	Acc@5  67.97 ( 65.83)
Epoch: [10][110/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6406e+00 (2.5617e+00)	Acc@1  32.81 ( 34.11)	Acc@5  62.50 ( 65.84)
Epoch: [10][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5820e+00 (2.5629e+00)	Acc@1  35.16 ( 34.12)	Acc@5  64.84 ( 65.88)
Epoch: [10][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4941e+00 (2.5621e+00)	Acc@1  35.16 ( 34.16)	Acc@5  69.53 ( 65.96)
Epoch: [10][140/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3066e+00 (2.5603e+00)	Acc@1  42.19 ( 34.26)	Acc@5  70.31 ( 65.84)
Epoch: [10][150/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4473e+00 (2.5608e+00)	Acc@1  35.16 ( 34.22)	Acc@5  69.53 ( 65.90)
Epoch: [10][160/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8125e+00 (2.5583e+00)	Acc@1  28.91 ( 34.26)	Acc@5  60.16 ( 66.01)
Epoch: [10][170/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4668e+00 (2.5593e+00)	Acc@1  32.03 ( 34.16)	Acc@5  69.53 ( 66.03)
Epoch: [10][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6680e+00 (2.5606e+00)	Acc@1  34.38 ( 34.06)	Acc@5  66.41 ( 66.04)
Epoch: [10][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5293e+00 (2.5642e+00)	Acc@1  34.38 ( 34.03)	Acc@5  63.28 ( 65.94)
Epoch: [10][200/391]	Time  0.175 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4844e+00 (2.5654e+00)	Acc@1  33.59 ( 34.04)	Acc@5  67.97 ( 65.91)
Epoch: [10][210/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5215e+00 (2.5643e+00)	Acc@1  38.28 ( 34.06)	Acc@5  64.06 ( 65.98)
Epoch: [10][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2637e+00 (2.5650e+00)	Acc@1  36.72 ( 34.00)	Acc@5  68.75 ( 65.95)
Epoch: [10][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4062e+00 (2.5599e+00)	Acc@1  35.16 ( 34.09)	Acc@5  71.09 ( 66.08)
Epoch: [10][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3809e+00 (2.5563e+00)	Acc@1  35.94 ( 34.18)	Acc@5  67.97 ( 66.16)
Epoch: [10][250/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6484e+00 (2.5526e+00)	Acc@1  32.03 ( 34.23)	Acc@5  64.06 ( 66.26)
Epoch: [10][260/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5781e+00 (2.5494e+00)	Acc@1  35.16 ( 34.34)	Acc@5  63.28 ( 66.32)
Epoch: [10][270/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5625e+00 (2.5483e+00)	Acc@1  34.38 ( 34.39)	Acc@5  64.84 ( 66.35)
Epoch: [10][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6406e+00 (2.5447e+00)	Acc@1  30.47 ( 34.48)	Acc@5  63.28 ( 66.48)
Epoch: [10][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5723e+00 (2.5439e+00)	Acc@1  35.16 ( 34.47)	Acc@5  69.53 ( 66.46)
Epoch: [10][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6035e+00 (2.5417e+00)	Acc@1  31.25 ( 34.48)	Acc@5  62.50 ( 66.47)
Epoch: [10][310/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.4531e+00 (2.5387e+00)	Acc@1  37.50 ( 34.58)	Acc@5  64.84 ( 66.49)
Epoch: [10][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1582e+00 (2.5363e+00)	Acc@1  44.53 ( 34.70)	Acc@5  74.22 ( 66.49)
Epoch: [10][330/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3457e+00 (2.5336e+00)	Acc@1  39.84 ( 34.76)	Acc@5  67.97 ( 66.54)
Epoch: [10][340/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.4883e+00 (2.5332e+00)	Acc@1  35.16 ( 34.76)	Acc@5  67.97 ( 66.54)
Epoch: [10][350/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.5547e+00 (2.5321e+00)	Acc@1  39.84 ( 34.76)	Acc@5  62.50 ( 66.58)
Epoch: [10][360/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.2559e+00 (2.5319e+00)	Acc@1  38.28 ( 34.78)	Acc@5  70.31 ( 66.57)
Epoch: [10][370/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.2715e+00 (2.5289e+00)	Acc@1  36.72 ( 34.81)	Acc@5  72.66 ( 66.64)
Epoch: [10][380/391]	Time  0.172 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3984e+00 (2.5279e+00)	Acc@1  39.06 ( 34.87)	Acc@5  71.88 ( 66.65)
Epoch: [10][390/391]	Time  0.114 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.5000e+00 (2.5261e+00)	Acc@1  38.75 ( 34.90)	Acc@5  66.25 ( 66.67)
## e[10] optimizer.zero_grad (sum) time: 0.5867712497711182
## e[10]       loss.backward (sum) time: 12.45228886604309
## e[10]      optimizer.step (sum) time: 25.423476696014404
## epoch[10] training(only) time: 63.13526678085327
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 2.3496e+00 (2.3496e+00)	Acc@1  46.00 ( 46.00)	Acc@5  73.00 ( 73.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.4473e+00 (2.4561e+00)	Acc@1  35.00 ( 38.27)	Acc@5  70.00 ( 70.18)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 2.2422e+00 (2.4081e+00)	Acc@1  40.00 ( 38.67)	Acc@5  75.00 ( 70.10)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.3379e+00 (2.4115e+00)	Acc@1  33.00 ( 37.94)	Acc@5  67.00 ( 69.55)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 2.4121e+00 (2.3956e+00)	Acc@1  33.00 ( 38.15)	Acc@5  67.00 ( 69.54)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 2.9629e+00 (2.4216e+00)	Acc@1  43.00 ( 38.02)	Acc@5  73.00 ( 69.16)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.4395e+00 (2.4198e+00)	Acc@1  38.00 ( 37.82)	Acc@5  68.00 ( 69.15)
Test: [ 70/100]	Time  0.046 ( 0.048)	Loss 2.5703e+00 (2.4143e+00)	Acc@1  36.00 ( 37.97)	Acc@5  67.00 ( 69.23)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 2.3945e+00 (2.4190e+00)	Acc@1  37.00 ( 37.74)	Acc@5  65.00 ( 68.96)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.1484e+00 (2.4091e+00)	Acc@1  47.00 ( 37.92)	Acc@5  80.00 ( 69.34)
 * Acc@1 37.950 Acc@5 69.170
### epoch[10] execution time: 68.03199172019958
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.313 ( 0.313)	Data  0.144 ( 0.144)	Loss 2.3965e+00 (2.3965e+00)	Acc@1  36.72 ( 36.72)	Acc@5  69.53 ( 69.53)
Epoch: [11][ 10/391]	Time  0.159 ( 0.174)	Data  0.001 ( 0.014)	Loss 2.6191e+00 (2.4730e+00)	Acc@1  34.38 ( 36.36)	Acc@5  62.50 ( 67.68)
Epoch: [11][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 2.3965e+00 (2.4102e+00)	Acc@1  39.84 ( 37.20)	Acc@5  68.75 ( 69.12)
Epoch: [11][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.4609e+00 (2.4115e+00)	Acc@1  39.84 ( 37.02)	Acc@5  68.75 ( 69.15)
Epoch: [11][ 40/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.3848e+00 (2.4046e+00)	Acc@1  39.06 ( 37.39)	Acc@5  68.75 ( 69.40)
Epoch: [11][ 50/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.004)	Loss 2.3457e+00 (2.4002e+00)	Acc@1  37.50 ( 37.29)	Acc@5  71.09 ( 69.47)
Epoch: [11][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.5078e+00 (2.4056e+00)	Acc@1  31.25 ( 36.97)	Acc@5  65.62 ( 69.31)
Epoch: [11][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3477e+00 (2.4044e+00)	Acc@1  43.75 ( 37.13)	Acc@5  67.19 ( 69.36)
Epoch: [11][ 80/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.003)	Loss 2.6934e+00 (2.4212e+00)	Acc@1  32.03 ( 36.68)	Acc@5  62.50 ( 69.01)
Epoch: [11][ 90/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4414e+00 (2.4229e+00)	Acc@1  38.28 ( 36.70)	Acc@5  71.88 ( 69.08)
Epoch: [11][100/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8633e+00 (2.4215e+00)	Acc@1  25.78 ( 36.77)	Acc@5  55.47 ( 68.96)
Epoch: [11][110/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6621e+00 (2.4125e+00)	Acc@1  35.94 ( 37.08)	Acc@5  67.97 ( 69.21)
Epoch: [11][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3984e+00 (2.4122e+00)	Acc@1  36.72 ( 37.02)	Acc@5  66.41 ( 69.31)
Epoch: [11][130/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7422e+00 (2.4132e+00)	Acc@1  32.81 ( 37.05)	Acc@5  61.72 ( 69.30)
Epoch: [11][140/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4180e+00 (2.4066e+00)	Acc@1  34.38 ( 37.21)	Acc@5  70.31 ( 69.40)
Epoch: [11][150/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9453e+00 (2.4097e+00)	Acc@1  43.75 ( 37.18)	Acc@5  75.78 ( 69.34)
Epoch: [11][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0195e+00 (2.4082e+00)	Acc@1  50.00 ( 37.20)	Acc@5  78.91 ( 69.39)
Epoch: [11][170/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6270e+00 (2.4112e+00)	Acc@1  32.03 ( 37.15)	Acc@5  64.84 ( 69.34)
Epoch: [11][180/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3711e+00 (2.4072e+00)	Acc@1  37.50 ( 37.22)	Acc@5  72.66 ( 69.43)
Epoch: [11][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2676e+00 (2.4046e+00)	Acc@1  35.16 ( 37.18)	Acc@5  72.66 ( 69.44)
Epoch: [11][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5176e+00 (2.4049e+00)	Acc@1  33.59 ( 37.17)	Acc@5  64.06 ( 69.40)
Epoch: [11][210/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.1934e+00 (2.4061e+00)	Acc@1  41.41 ( 37.17)	Acc@5  74.22 ( 69.42)
Epoch: [11][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4043e+00 (2.4049e+00)	Acc@1  35.94 ( 37.17)	Acc@5  70.31 ( 69.44)
Epoch: [11][230/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.6094e+00 (2.4023e+00)	Acc@1  39.06 ( 37.27)	Acc@5  64.06 ( 69.46)
Epoch: [11][240/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 2.4844e+00 (2.4028e+00)	Acc@1  37.50 ( 37.26)	Acc@5  67.19 ( 69.41)
Epoch: [11][250/391]	Time  0.169 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3906e+00 (2.4038e+00)	Acc@1  32.03 ( 37.21)	Acc@5  66.41 ( 69.48)
Epoch: [11][260/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.7227e+00 (2.4057e+00)	Acc@1  32.03 ( 37.19)	Acc@5  63.28 ( 69.44)
Epoch: [11][270/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3926e+00 (2.4038e+00)	Acc@1  41.41 ( 37.21)	Acc@5  71.09 ( 69.47)
Epoch: [11][280/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3164e+00 (2.4043e+00)	Acc@1  38.28 ( 37.16)	Acc@5  69.53 ( 69.45)
Epoch: [11][290/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.1660e+00 (2.4018e+00)	Acc@1  40.62 ( 37.24)	Acc@5  76.56 ( 69.46)
Epoch: [11][300/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.4805e+00 (2.4027e+00)	Acc@1  33.59 ( 37.25)	Acc@5  71.09 ( 69.49)
Epoch: [11][310/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.4004e+00 (2.3992e+00)	Acc@1  32.81 ( 37.32)	Acc@5  67.97 ( 69.57)
Epoch: [11][320/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.2363e+00 (2.3966e+00)	Acc@1  40.62 ( 37.35)	Acc@5  68.75 ( 69.58)
Epoch: [11][330/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3887e+00 (2.3947e+00)	Acc@1  39.06 ( 37.40)	Acc@5  70.31 ( 69.61)
Epoch: [11][340/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.2188e+00 (2.3924e+00)	Acc@1  42.97 ( 37.47)	Acc@5  72.66 ( 69.67)
Epoch: [11][350/391]	Time  0.163 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3691e+00 (2.3878e+00)	Acc@1  35.16 ( 37.56)	Acc@5  72.66 ( 69.77)
Epoch: [11][360/391]	Time  0.162 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3223e+00 (2.3860e+00)	Acc@1  42.19 ( 37.60)	Acc@5  71.09 ( 69.84)
Epoch: [11][370/391]	Time  0.165 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.9941e+00 (2.3827e+00)	Acc@1  42.97 ( 37.70)	Acc@5  75.78 ( 69.89)
Epoch: [11][380/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.0625e+00 (2.3797e+00)	Acc@1  46.88 ( 37.79)	Acc@5  74.22 ( 69.93)
Epoch: [11][390/391]	Time  0.113 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.5762e+00 (2.3793e+00)	Acc@1  33.75 ( 37.81)	Acc@5  62.50 ( 69.93)
## e[11] optimizer.zero_grad (sum) time: 0.5720212459564209
## e[11]       loss.backward (sum) time: 12.419442176818848
## e[11]      optimizer.step (sum) time: 25.472337007522583
## epoch[11] training(only) time: 63.115415811538696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.3730e+00 (2.3730e+00)	Acc@1  44.00 ( 44.00)	Acc@5  73.00 ( 73.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 2.3281e+00 (2.2842e+00)	Acc@1  33.00 ( 42.18)	Acc@5  70.00 ( 72.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 2.0352e+00 (2.2194e+00)	Acc@1  43.00 ( 42.48)	Acc@5  76.00 ( 73.90)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.1738e+00 (2.2202e+00)	Acc@1  46.00 ( 41.87)	Acc@5  74.00 ( 73.23)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 2.1875e+00 (2.2108e+00)	Acc@1  44.00 ( 42.37)	Acc@5  74.00 ( 73.29)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.2383e+00 (2.2300e+00)	Acc@1  41.00 ( 42.24)	Acc@5  69.00 ( 72.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.1035e+00 (2.2243e+00)	Acc@1  45.00 ( 42.23)	Acc@5  77.00 ( 72.89)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.3984e+00 (2.2189e+00)	Acc@1  37.00 ( 42.23)	Acc@5  68.00 ( 73.18)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.3027e+00 (2.2249e+00)	Acc@1  36.00 ( 41.90)	Acc@5  69.00 ( 72.89)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.1367e+00 (2.2132e+00)	Acc@1  47.00 ( 42.15)	Acc@5  77.00 ( 73.24)
 * Acc@1 42.170 Acc@5 73.210
### epoch[11] execution time: 68.03943467140198
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.308 ( 0.308)	Data  0.145 ( 0.145)	Loss 2.1133e+00 (2.1133e+00)	Acc@1  39.84 ( 39.84)	Acc@5  75.78 ( 75.78)
Epoch: [12][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 2.3203e+00 (2.2873e+00)	Acc@1  43.75 ( 40.27)	Acc@5  70.31 ( 71.38)
Epoch: [12][ 20/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.008)	Loss 2.2207e+00 (2.3041e+00)	Acc@1  43.75 ( 39.77)	Acc@5  78.12 ( 72.10)
Epoch: [12][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.2598e+00 (2.2764e+00)	Acc@1  39.06 ( 40.52)	Acc@5  71.88 ( 72.35)
Epoch: [12][ 40/391]	Time  0.174 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.2441e+00 (2.2801e+00)	Acc@1  38.28 ( 40.09)	Acc@5  77.34 ( 72.68)
Epoch: [12][ 50/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.1895e+00 (2.2652e+00)	Acc@1  41.41 ( 40.58)	Acc@5  71.09 ( 72.96)
Epoch: [12][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.3594e+00 (2.2505e+00)	Acc@1  38.28 ( 40.75)	Acc@5  68.75 ( 73.04)
Epoch: [12][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3477e+00 (2.2626e+00)	Acc@1  35.16 ( 40.51)	Acc@5  64.84 ( 72.67)
Epoch: [12][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3711e+00 (2.2552e+00)	Acc@1  36.72 ( 40.51)	Acc@5  70.31 ( 72.79)
Epoch: [12][ 90/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3926e+00 (2.2600e+00)	Acc@1  39.84 ( 40.20)	Acc@5  67.97 ( 72.78)
Epoch: [12][100/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3652e+00 (2.2605e+00)	Acc@1  34.38 ( 40.23)	Acc@5  73.44 ( 72.59)
Epoch: [12][110/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2598e+00 (2.2607e+00)	Acc@1  45.31 ( 40.29)	Acc@5  72.66 ( 72.49)
Epoch: [12][120/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0117e+00 (2.2642e+00)	Acc@1  40.62 ( 40.16)	Acc@5  76.56 ( 72.49)
Epoch: [12][130/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1973e+00 (2.2656e+00)	Acc@1  39.84 ( 40.13)	Acc@5  80.47 ( 72.46)
Epoch: [12][140/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0664e+00 (2.2683e+00)	Acc@1  44.53 ( 40.03)	Acc@5  75.78 ( 72.24)
Epoch: [12][150/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4648e+00 (2.2695e+00)	Acc@1  35.16 ( 40.01)	Acc@5  68.75 ( 72.24)
Epoch: [12][160/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1875e+00 (2.2749e+00)	Acc@1  45.31 ( 39.92)	Acc@5  71.88 ( 72.13)
Epoch: [12][170/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4258e+00 (2.2782e+00)	Acc@1  37.50 ( 39.81)	Acc@5  67.19 ( 72.07)
Epoch: [12][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4238e+00 (2.2773e+00)	Acc@1  41.41 ( 39.89)	Acc@5  67.97 ( 72.05)
Epoch: [12][190/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0332e+00 (2.2780e+00)	Acc@1  48.44 ( 39.92)	Acc@5  78.12 ( 72.04)
Epoch: [12][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3418e+00 (2.2772e+00)	Acc@1  40.62 ( 40.00)	Acc@5  67.19 ( 72.00)
Epoch: [12][210/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2168e+00 (2.2771e+00)	Acc@1  39.06 ( 39.96)	Acc@5  74.22 ( 72.07)
Epoch: [12][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1699e+00 (2.2780e+00)	Acc@1  35.94 ( 39.87)	Acc@5  73.44 ( 72.04)
Epoch: [12][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4102e+00 (2.2797e+00)	Acc@1  34.38 ( 39.84)	Acc@5  71.09 ( 71.97)
Epoch: [12][240/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2832e+00 (2.2790e+00)	Acc@1  37.50 ( 39.83)	Acc@5  75.00 ( 72.00)
Epoch: [12][250/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5996e+00 (2.2791e+00)	Acc@1  31.25 ( 39.83)	Acc@5  64.06 ( 71.97)
Epoch: [12][260/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3535e+00 (2.2817e+00)	Acc@1  41.41 ( 39.80)	Acc@5  74.22 ( 71.95)
Epoch: [12][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5508e+00 (2.2832e+00)	Acc@1  36.72 ( 39.76)	Acc@5  64.06 ( 71.93)
Epoch: [12][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1426e+00 (2.2784e+00)	Acc@1  42.19 ( 39.91)	Acc@5  72.66 ( 72.01)
Epoch: [12][290/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3652e+00 (2.2774e+00)	Acc@1  35.16 ( 39.94)	Acc@5  75.00 ( 72.03)
Epoch: [12][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2461e+00 (2.2739e+00)	Acc@1  44.53 ( 40.10)	Acc@5  75.78 ( 72.13)
Epoch: [12][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2676e+00 (2.2717e+00)	Acc@1  36.72 ( 40.11)	Acc@5  67.19 ( 72.18)
Epoch: [12][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4609e+00 (2.2715e+00)	Acc@1  35.94 ( 40.10)	Acc@5  66.41 ( 72.18)
Epoch: [12][330/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0938e+00 (2.2688e+00)	Acc@1  44.53 ( 40.15)	Acc@5  75.00 ( 72.23)
Epoch: [12][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2363e+00 (2.2667e+00)	Acc@1  43.75 ( 40.20)	Acc@5  71.09 ( 72.29)
Epoch: [12][350/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3125e+00 (2.2629e+00)	Acc@1  39.06 ( 40.31)	Acc@5  71.88 ( 72.37)
Epoch: [12][360/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.1250e+00 (2.2640e+00)	Acc@1  48.44 ( 40.29)	Acc@5  76.56 ( 72.36)
Epoch: [12][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1035e+00 (2.2644e+00)	Acc@1  45.31 ( 40.31)	Acc@5  76.56 ( 72.33)
Epoch: [12][380/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.2832e+00 (2.2630e+00)	Acc@1  39.06 ( 40.34)	Acc@5  71.88 ( 72.37)
Epoch: [12][390/391]	Time  0.117 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.4551e+00 (2.2616e+00)	Acc@1  36.25 ( 40.37)	Acc@5  71.25 ( 72.41)
## e[12] optimizer.zero_grad (sum) time: 0.5731267929077148
## e[12]       loss.backward (sum) time: 12.507798910140991
## e[12]      optimizer.step (sum) time: 25.4311203956604
## epoch[12] training(only) time: 63.16619420051575
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 2.1152e+00 (2.1152e+00)	Acc@1  45.00 ( 45.00)	Acc@5  74.00 ( 74.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 2.2129e+00 (2.1336e+00)	Acc@1  34.00 ( 44.36)	Acc@5  76.00 ( 74.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.8594e+00 (2.1050e+00)	Acc@1  42.00 ( 44.71)	Acc@5  81.00 ( 74.86)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.0156e+00 (2.1234e+00)	Acc@1  42.00 ( 44.26)	Acc@5  76.00 ( 74.42)
Test: [ 40/100]	Time  0.057 ( 0.051)	Loss 1.9541e+00 (2.1126e+00)	Acc@1  51.00 ( 44.10)	Acc@5  77.00 ( 74.95)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.1445e+00 (2.1328e+00)	Acc@1  43.00 ( 43.59)	Acc@5  70.00 ( 74.59)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.0566e+00 (2.1334e+00)	Acc@1  48.00 ( 43.64)	Acc@5  77.00 ( 74.51)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 2.3105e+00 (2.1263e+00)	Acc@1  41.00 ( 43.94)	Acc@5  68.00 ( 74.59)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.2402e+00 (2.1318e+00)	Acc@1  42.00 ( 44.06)	Acc@5  69.00 ( 74.44)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.0820e+00 (2.1198e+00)	Acc@1  50.00 ( 44.30)	Acc@5  77.00 ( 74.78)
 * Acc@1 44.230 Acc@5 74.840
### epoch[12] execution time: 68.06958961486816
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.317 ( 0.317)	Data  0.145 ( 0.145)	Loss 2.0742e+00 (2.0742e+00)	Acc@1  42.97 ( 42.97)	Acc@5  76.56 ( 76.56)
Epoch: [13][ 10/391]	Time  0.162 ( 0.175)	Data  0.001 ( 0.014)	Loss 2.0586e+00 (2.1206e+00)	Acc@1  47.66 ( 44.03)	Acc@5  77.34 ( 74.72)
Epoch: [13][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.0820e+00 (2.0978e+00)	Acc@1  42.19 ( 44.12)	Acc@5  73.44 ( 75.26)
Epoch: [13][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.9404e+00 (2.1079e+00)	Acc@1  42.97 ( 43.70)	Acc@5  82.81 ( 75.55)
Epoch: [13][ 40/391]	Time  0.164 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.0098e+00 (2.1064e+00)	Acc@1  50.00 ( 43.60)	Acc@5  79.69 ( 75.95)
Epoch: [13][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.0996e+00 (2.1041e+00)	Acc@1  42.19 ( 43.63)	Acc@5  75.78 ( 75.90)
Epoch: [13][ 60/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.2324e+00 (2.1116e+00)	Acc@1  45.31 ( 43.65)	Acc@5  74.22 ( 75.65)
Epoch: [13][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.2871e+00 (2.1086e+00)	Acc@1  40.62 ( 43.86)	Acc@5  71.88 ( 75.65)
Epoch: [13][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.1543e+00 (2.1168e+00)	Acc@1  44.53 ( 43.60)	Acc@5  74.22 ( 75.46)
Epoch: [13][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.1719e+00 (2.1210e+00)	Acc@1  42.97 ( 43.53)	Acc@5  71.09 ( 75.42)
Epoch: [13][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2500e+00 (2.1291e+00)	Acc@1  41.41 ( 43.25)	Acc@5  71.88 ( 75.15)
Epoch: [13][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1348e+00 (2.1356e+00)	Acc@1  41.41 ( 43.17)	Acc@5  77.34 ( 75.08)
Epoch: [13][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2676e+00 (2.1363e+00)	Acc@1  37.50 ( 43.11)	Acc@5  74.22 ( 75.10)
Epoch: [13][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3379e+00 (2.1380e+00)	Acc@1  38.28 ( 43.11)	Acc@5  76.56 ( 75.15)
Epoch: [13][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2539e+00 (2.1425e+00)	Acc@1  33.59 ( 42.98)	Acc@5  71.88 ( 75.01)
Epoch: [13][150/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3105e+00 (2.1495e+00)	Acc@1  37.50 ( 42.84)	Acc@5  73.44 ( 74.82)
Epoch: [13][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1113e+00 (2.1497e+00)	Acc@1  42.97 ( 42.82)	Acc@5  75.00 ( 74.81)
Epoch: [13][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5117e+00 (2.1538e+00)	Acc@1  35.94 ( 42.69)	Acc@5  67.19 ( 74.72)
Epoch: [13][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0176e+00 (2.1543e+00)	Acc@1  42.19 ( 42.73)	Acc@5  80.47 ( 74.67)
Epoch: [13][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1797e+00 (2.1492e+00)	Acc@1  39.84 ( 42.84)	Acc@5  75.78 ( 74.75)
Epoch: [13][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0879e+00 (2.1499e+00)	Acc@1  39.06 ( 42.75)	Acc@5  78.91 ( 74.78)
Epoch: [13][210/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8887e+00 (2.1505e+00)	Acc@1  50.78 ( 42.74)	Acc@5  79.69 ( 74.77)
Epoch: [13][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8965e+00 (2.1481e+00)	Acc@1  50.00 ( 42.77)	Acc@5  82.81 ( 74.83)
Epoch: [13][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1426e+00 (2.1492e+00)	Acc@1  46.09 ( 42.81)	Acc@5  77.34 ( 74.78)
Epoch: [13][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3789e+00 (2.1496e+00)	Acc@1  37.50 ( 42.79)	Acc@5  63.28 ( 74.76)
Epoch: [13][250/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4180e+00 (2.1508e+00)	Acc@1  32.03 ( 42.79)	Acc@5  73.44 ( 74.76)
Epoch: [13][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0176e+00 (2.1480e+00)	Acc@1  46.88 ( 42.82)	Acc@5  82.03 ( 74.85)
Epoch: [13][270/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3848e+00 (2.1496e+00)	Acc@1  39.84 ( 42.78)	Acc@5  65.62 ( 74.77)
Epoch: [13][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7305e+00 (2.1482e+00)	Acc@1  49.22 ( 42.79)	Acc@5  82.03 ( 74.80)
Epoch: [13][290/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0957e+00 (2.1435e+00)	Acc@1  45.31 ( 42.88)	Acc@5  75.78 ( 74.91)
Epoch: [13][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9854e+00 (2.1396e+00)	Acc@1  43.75 ( 42.98)	Acc@5  77.34 ( 74.95)
Epoch: [13][310/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1016e+00 (2.1389e+00)	Acc@1  36.72 ( 43.01)	Acc@5  73.44 ( 74.96)
Epoch: [13][320/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3594e+00 (2.1364e+00)	Acc@1  40.62 ( 43.02)	Acc@5  75.00 ( 75.00)
Epoch: [13][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2988e+00 (2.1355e+00)	Acc@1  37.50 ( 43.07)	Acc@5  71.88 ( 75.01)
Epoch: [13][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1816e+00 (2.1362e+00)	Acc@1  44.53 ( 43.08)	Acc@5  72.66 ( 75.00)
Epoch: [13][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8984e+00 (2.1337e+00)	Acc@1  50.78 ( 43.13)	Acc@5  78.91 ( 75.05)
Epoch: [13][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0918e+00 (2.1326e+00)	Acc@1  42.19 ( 43.18)	Acc@5  76.56 ( 75.08)
Epoch: [13][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0391e+00 (2.1321e+00)	Acc@1  39.84 ( 43.17)	Acc@5  74.22 ( 75.11)
Epoch: [13][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1172e+00 (2.1315e+00)	Acc@1  42.97 ( 43.20)	Acc@5  73.44 ( 75.12)
Epoch: [13][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2617e+00 (2.1329e+00)	Acc@1  37.50 ( 43.19)	Acc@5  71.25 ( 75.10)
## e[13] optimizer.zero_grad (sum) time: 0.558955192565918
## e[13]       loss.backward (sum) time: 12.537462949752808
## e[13]      optimizer.step (sum) time: 25.420817136764526
## epoch[13] training(only) time: 63.250669717788696
# Switched to evaluate mode...
Test: [  0/100]	Time  0.185 ( 0.185)	Loss 2.1230e+00 (2.1230e+00)	Acc@1  45.00 ( 45.00)	Acc@5  71.00 ( 71.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 2.3418e+00 (2.2676e+00)	Acc@1  38.00 ( 45.00)	Acc@5  75.00 ( 74.45)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.8115e+00 (2.1736e+00)	Acc@1  51.00 ( 46.05)	Acc@5  80.00 ( 75.52)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 2.1035e+00 (2.2014e+00)	Acc@1  43.00 ( 45.00)	Acc@5  76.00 ( 75.00)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.8984e+00 (2.2015e+00)	Acc@1  48.00 ( 44.44)	Acc@5  78.00 ( 74.93)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.3164e+00 (2.2371e+00)	Acc@1  44.00 ( 44.31)	Acc@5  67.00 ( 74.65)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 2.0879e+00 (2.2323e+00)	Acc@1  45.00 ( 44.38)	Acc@5  78.00 ( 74.72)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.1543e+00 (2.2222e+00)	Acc@1  42.00 ( 44.30)	Acc@5  73.00 ( 74.80)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.4297e+00 (2.2312e+00)	Acc@1  41.00 ( 43.98)	Acc@5  69.00 ( 74.54)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 2.0977e+00 (2.2131e+00)	Acc@1  50.00 ( 44.27)	Acc@5  74.00 ( 74.91)
 * Acc@1 44.350 Acc@5 75.030
### epoch[13] execution time: 68.1860408782959
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.325 ( 0.325)	Data  0.157 ( 0.157)	Loss 1.8320e+00 (1.8320e+00)	Acc@1  45.31 ( 45.31)	Acc@5  85.16 ( 85.16)
Epoch: [14][ 10/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.015)	Loss 1.7354e+00 (2.0526e+00)	Acc@1  49.22 ( 45.10)	Acc@5  81.25 ( 76.28)
Epoch: [14][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.7744e+00 (2.0200e+00)	Acc@1  50.78 ( 46.69)	Acc@5  80.47 ( 76.49)
Epoch: [14][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.9668e+00 (1.9890e+00)	Acc@1  50.78 ( 46.85)	Acc@5  78.12 ( 77.39)
Epoch: [14][ 40/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.9082e+00 (2.0103e+00)	Acc@1  50.00 ( 46.23)	Acc@5  78.91 ( 77.27)
Epoch: [14][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.2031e+00 (2.0072e+00)	Acc@1  40.62 ( 46.00)	Acc@5  75.78 ( 77.05)
Epoch: [14][ 60/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9727e+00 (2.0066e+00)	Acc@1  47.66 ( 45.93)	Acc@5  78.91 ( 77.18)
Epoch: [14][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1367e+00 (2.0076e+00)	Acc@1  40.62 ( 45.95)	Acc@5  78.12 ( 77.23)
Epoch: [14][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8408e+00 (2.0150e+00)	Acc@1  51.56 ( 45.86)	Acc@5  79.69 ( 76.94)
Epoch: [14][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9785e+00 (2.0079e+00)	Acc@1  50.78 ( 46.07)	Acc@5  77.34 ( 77.21)
Epoch: [14][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0664e+00 (2.0146e+00)	Acc@1  43.75 ( 45.85)	Acc@5  80.47 ( 77.18)
Epoch: [14][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2383e+00 (2.0226e+00)	Acc@1  42.19 ( 45.69)	Acc@5  71.88 ( 77.06)
Epoch: [14][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1230e+00 (2.0171e+00)	Acc@1  48.44 ( 45.83)	Acc@5  73.44 ( 77.14)
Epoch: [14][130/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6855e+00 (2.0179e+00)	Acc@1  56.25 ( 45.82)	Acc@5  82.81 ( 77.11)
Epoch: [14][140/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1211e+00 (2.0216e+00)	Acc@1  43.75 ( 45.71)	Acc@5  75.78 ( 77.09)
Epoch: [14][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9385e+00 (2.0224e+00)	Acc@1  49.22 ( 45.73)	Acc@5  75.78 ( 77.01)
Epoch: [14][160/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0254e+00 (2.0207e+00)	Acc@1  40.62 ( 45.69)	Acc@5  75.00 ( 77.02)
Epoch: [14][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1816e+00 (2.0266e+00)	Acc@1  37.50 ( 45.49)	Acc@5  74.22 ( 76.96)
Epoch: [14][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9082e+00 (2.0249e+00)	Acc@1  48.44 ( 45.50)	Acc@5  81.25 ( 77.08)
Epoch: [14][190/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1309e+00 (2.0229e+00)	Acc@1  46.88 ( 45.55)	Acc@5  76.56 ( 77.09)
Epoch: [14][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1914e+00 (2.0256e+00)	Acc@1  38.28 ( 45.51)	Acc@5  74.22 ( 77.06)
Epoch: [14][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8232e+00 (2.0217e+00)	Acc@1  42.97 ( 45.58)	Acc@5  82.81 ( 77.11)
Epoch: [14][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9395e+00 (2.0241e+00)	Acc@1  42.19 ( 45.42)	Acc@5  79.69 ( 77.08)
Epoch: [14][230/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1074e+00 (2.0263e+00)	Acc@1  43.75 ( 45.35)	Acc@5  79.69 ( 77.10)
Epoch: [14][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9873e+00 (2.0264e+00)	Acc@1  45.31 ( 45.36)	Acc@5  78.91 ( 77.10)
Epoch: [14][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7598e+00 (2.0261e+00)	Acc@1  50.78 ( 45.37)	Acc@5  87.50 ( 77.08)
Epoch: [14][260/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8506e+00 (2.0257e+00)	Acc@1  46.09 ( 45.40)	Acc@5  82.03 ( 77.10)
Epoch: [14][270/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1797e+00 (2.0284e+00)	Acc@1  43.75 ( 45.29)	Acc@5  72.66 ( 77.02)
Epoch: [14][280/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0684e+00 (2.0253e+00)	Acc@1  40.62 ( 45.37)	Acc@5  74.22 ( 77.04)
Epoch: [14][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9453e+00 (2.0252e+00)	Acc@1  47.66 ( 45.35)	Acc@5  80.47 ( 77.04)
Epoch: [14][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0195e+00 (2.0281e+00)	Acc@1  53.12 ( 45.33)	Acc@5  78.12 ( 77.01)
Epoch: [14][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2910e+00 (2.0269e+00)	Acc@1  42.19 ( 45.36)	Acc@5  67.97 ( 77.04)
Epoch: [14][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 2.0996e+00 (2.0258e+00)	Acc@1  41.41 ( 45.44)	Acc@5  77.34 ( 77.07)
Epoch: [14][330/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2285e+00 (2.0269e+00)	Acc@1  40.62 ( 45.43)	Acc@5  74.22 ( 77.08)
Epoch: [14][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0176e+00 (2.0257e+00)	Acc@1  41.41 ( 45.47)	Acc@5  77.34 ( 77.10)
Epoch: [14][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8252e+00 (2.0271e+00)	Acc@1  50.78 ( 45.46)	Acc@5  80.47 ( 77.06)
Epoch: [14][360/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9258e+00 (2.0264e+00)	Acc@1  46.88 ( 45.44)	Acc@5  77.34 ( 77.10)
Epoch: [14][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7471e+00 (2.0249e+00)	Acc@1  57.03 ( 45.49)	Acc@5  84.38 ( 77.09)
Epoch: [14][380/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9414e+00 (2.0275e+00)	Acc@1  44.53 ( 45.42)	Acc@5  76.56 ( 77.02)
Epoch: [14][390/391]	Time  0.120 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0332e+00 (2.0308e+00)	Acc@1  45.00 ( 45.33)	Acc@5  78.75 ( 76.98)
## e[14] optimizer.zero_grad (sum) time: 0.5600326061248779
## e[14]       loss.backward (sum) time: 12.523597240447998
## e[14]      optimizer.step (sum) time: 25.440531253814697
## epoch[14] training(only) time: 63.502546310424805
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 2.0684e+00 (2.0684e+00)	Acc@1  49.00 ( 49.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 2.1016e+00 (2.1166e+00)	Acc@1  38.00 ( 45.45)	Acc@5  79.00 ( 76.36)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.7744e+00 (2.0630e+00)	Acc@1  51.00 ( 46.95)	Acc@5  77.00 ( 76.86)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 2.1836e+00 (2.0702e+00)	Acc@1  38.00 ( 46.16)	Acc@5  73.00 ( 76.55)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.8613e+00 (2.0548e+00)	Acc@1  52.00 ( 46.54)	Acc@5  81.00 ( 76.66)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 2.0137e+00 (2.0681e+00)	Acc@1  49.00 ( 46.08)	Acc@5  74.00 ( 76.08)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8604e+00 (2.0639e+00)	Acc@1  50.00 ( 46.20)	Acc@5  85.00 ( 76.31)
Test: [ 70/100]	Time  0.057 ( 0.049)	Loss 2.2637e+00 (2.0624e+00)	Acc@1  42.00 ( 45.96)	Acc@5  66.00 ( 76.31)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 2.2168e+00 (2.0678e+00)	Acc@1  39.00 ( 45.68)	Acc@5  72.00 ( 76.02)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 2.1387e+00 (2.0627e+00)	Acc@1  47.00 ( 45.90)	Acc@5  76.00 ( 76.22)
 * Acc@1 45.790 Acc@5 76.140
### epoch[14] execution time: 68.40942096710205
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.321 ( 0.321)	Data  0.153 ( 0.153)	Loss 2.2109e+00 (2.2109e+00)	Acc@1  46.09 ( 46.09)	Acc@5  71.09 ( 71.09)
Epoch: [15][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.8662e+00 (1.8866e+00)	Acc@1  47.66 ( 49.01)	Acc@5  78.91 ( 79.62)
Epoch: [15][ 20/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.008)	Loss 2.0215e+00 (1.9027e+00)	Acc@1  42.19 ( 48.47)	Acc@5  81.25 ( 79.20)
Epoch: [15][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.2129e+00 (1.9217e+00)	Acc@1  37.50 ( 47.91)	Acc@5  75.78 ( 79.06)
Epoch: [15][ 40/391]	Time  0.168 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.0039e+00 (1.9177e+00)	Acc@1  43.75 ( 47.94)	Acc@5  77.34 ( 78.77)
Epoch: [15][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.0566e+00 (1.9297e+00)	Acc@1  44.53 ( 47.64)	Acc@5  71.09 ( 78.40)
Epoch: [15][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9600e+00 (1.9275e+00)	Acc@1  42.97 ( 47.59)	Acc@5  82.03 ( 78.47)
Epoch: [15][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7578e+00 (1.9299e+00)	Acc@1  51.56 ( 47.45)	Acc@5  85.16 ( 78.55)
Epoch: [15][ 80/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.8281e+00 (1.9362e+00)	Acc@1  47.66 ( 47.35)	Acc@5  83.59 ( 78.46)
Epoch: [15][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.9277e+00 (1.9307e+00)	Acc@1  46.09 ( 47.56)	Acc@5  80.47 ( 78.69)
Epoch: [15][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1387e+00 (1.9494e+00)	Acc@1  42.19 ( 47.13)	Acc@5  76.56 ( 78.45)
Epoch: [15][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1387e+00 (1.9553e+00)	Acc@1  45.31 ( 47.10)	Acc@5  74.22 ( 78.27)
Epoch: [15][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7930e+00 (1.9562e+00)	Acc@1  48.44 ( 47.09)	Acc@5  81.25 ( 78.25)
Epoch: [15][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9385e+00 (1.9550e+00)	Acc@1  46.09 ( 47.07)	Acc@5  78.91 ( 78.20)
Epoch: [15][140/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8564e+00 (1.9509e+00)	Acc@1  53.12 ( 47.11)	Acc@5  80.47 ( 78.24)
Epoch: [15][150/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9121e+00 (1.9522e+00)	Acc@1  47.66 ( 47.15)	Acc@5  77.34 ( 78.20)
Epoch: [15][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0469e+00 (1.9553e+00)	Acc@1  43.75 ( 47.10)	Acc@5  78.91 ( 78.15)
Epoch: [15][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8789e+00 (1.9554e+00)	Acc@1  52.34 ( 47.11)	Acc@5  74.22 ( 78.16)
Epoch: [15][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0781e+00 (1.9551e+00)	Acc@1  42.97 ( 47.16)	Acc@5  73.44 ( 78.13)
Epoch: [15][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8027e+00 (1.9547e+00)	Acc@1  47.66 ( 47.13)	Acc@5  78.91 ( 78.12)
Epoch: [15][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1855e+00 (1.9572e+00)	Acc@1  42.97 ( 47.00)	Acc@5  73.44 ( 78.12)
Epoch: [15][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2031e+00 (1.9578e+00)	Acc@1  43.75 ( 46.99)	Acc@5  73.44 ( 78.13)
Epoch: [15][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9707e+00 (1.9571e+00)	Acc@1  40.62 ( 46.98)	Acc@5  75.00 ( 78.11)
Epoch: [15][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8105e+00 (1.9539e+00)	Acc@1  46.88 ( 47.07)	Acc@5  80.47 ( 78.23)
Epoch: [15][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0234e+00 (1.9553e+00)	Acc@1  47.66 ( 47.02)	Acc@5  78.12 ( 78.21)
Epoch: [15][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9365e+00 (1.9554e+00)	Acc@1  45.31 ( 47.01)	Acc@5  79.69 ( 78.23)
Epoch: [15][260/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2500e+00 (1.9566e+00)	Acc@1  37.50 ( 46.97)	Acc@5  71.88 ( 78.17)
Epoch: [15][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9658e+00 (1.9566e+00)	Acc@1  40.62 ( 47.00)	Acc@5  76.56 ( 78.14)
Epoch: [15][280/391]	Time  0.163 ( 0.162)	Data  0.002 ( 0.002)	Loss 1.8164e+00 (1.9553e+00)	Acc@1  50.78 ( 47.04)	Acc@5  78.91 ( 78.14)
Epoch: [15][290/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9521e+00 (1.9539e+00)	Acc@1  49.22 ( 47.08)	Acc@5  77.34 ( 78.15)
Epoch: [15][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9580e+00 (1.9533e+00)	Acc@1  46.88 ( 47.08)	Acc@5  76.56 ( 78.14)
Epoch: [15][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7920e+00 (1.9510e+00)	Acc@1  53.12 ( 47.14)	Acc@5  82.03 ( 78.20)
Epoch: [15][320/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8252e+00 (1.9491e+00)	Acc@1  50.78 ( 47.22)	Acc@5  79.69 ( 78.22)
Epoch: [15][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7646e+00 (1.9500e+00)	Acc@1  55.47 ( 47.22)	Acc@5  82.03 ( 78.21)
Epoch: [15][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8525e+00 (1.9496e+00)	Acc@1  45.31 ( 47.26)	Acc@5  82.81 ( 78.24)
Epoch: [15][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9697e+00 (1.9496e+00)	Acc@1  44.53 ( 47.29)	Acc@5  78.12 ( 78.25)
Epoch: [15][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7051e+00 (1.9491e+00)	Acc@1  53.12 ( 47.30)	Acc@5  82.03 ( 78.25)
Epoch: [15][370/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1035e+00 (1.9503e+00)	Acc@1  39.06 ( 47.25)	Acc@5  71.09 ( 78.22)
Epoch: [15][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0781e+00 (1.9495e+00)	Acc@1  46.09 ( 47.22)	Acc@5  72.66 ( 78.24)
Epoch: [15][390/391]	Time  0.121 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9297e+00 (1.9489e+00)	Acc@1  47.50 ( 47.23)	Acc@5  77.50 ( 78.27)
## e[15] optimizer.zero_grad (sum) time: 0.5535290241241455
## e[15]       loss.backward (sum) time: 12.53911542892456
## e[15]      optimizer.step (sum) time: 25.466013431549072
## epoch[15] training(only) time: 63.37647032737732
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.8867e+00 (1.8867e+00)	Acc@1  57.00 ( 57.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.9775e+00 (1.9623e+00)	Acc@1  43.00 ( 49.27)	Acc@5  80.00 ( 77.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.5039e+00 (1.8889e+00)	Acc@1  57.00 ( 49.90)	Acc@5  86.00 ( 79.43)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.9482e+00 (1.9168e+00)	Acc@1  45.00 ( 48.94)	Acc@5  79.00 ( 78.61)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.7686e+00 (1.9098e+00)	Acc@1  52.00 ( 49.20)	Acc@5  78.00 ( 78.83)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.9121e+00 (1.9247e+00)	Acc@1  54.00 ( 49.22)	Acc@5  75.00 ( 78.51)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.9824e+00 (1.9226e+00)	Acc@1  48.00 ( 48.98)	Acc@5  82.00 ( 78.69)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.0820e+00 (1.9271e+00)	Acc@1  50.00 ( 48.93)	Acc@5  72.00 ( 78.52)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.1230e+00 (1.9335e+00)	Acc@1  44.00 ( 48.70)	Acc@5  72.00 ( 78.28)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.8604e+00 (1.9239e+00)	Acc@1  49.00 ( 48.80)	Acc@5  80.00 ( 78.46)
 * Acc@1 48.820 Acc@5 78.610
### epoch[15] execution time: 68.28278613090515
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.324 ( 0.324)	Data  0.157 ( 0.157)	Loss 1.8262e+00 (1.8262e+00)	Acc@1  48.44 ( 48.44)	Acc@5  78.12 ( 78.12)
Epoch: [16][ 10/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.7578e+00 (1.8232e+00)	Acc@1  50.78 ( 50.71)	Acc@5  84.38 ( 81.18)
Epoch: [16][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.9336e+00 (1.8512e+00)	Acc@1  46.88 ( 50.19)	Acc@5  78.12 ( 80.73)
Epoch: [16][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.6875e+00 (1.8460e+00)	Acc@1  51.56 ( 50.10)	Acc@5  85.16 ( 80.67)
Epoch: [16][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.6621e+00 (1.8366e+00)	Acc@1  57.03 ( 50.71)	Acc@5  81.25 ( 80.83)
Epoch: [16][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.6875e+00 (1.8197e+00)	Acc@1  53.12 ( 50.66)	Acc@5  82.81 ( 81.17)
Epoch: [16][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8760e+00 (1.8187e+00)	Acc@1  49.22 ( 50.60)	Acc@5  78.12 ( 80.85)
Epoch: [16][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7529e+00 (1.8301e+00)	Acc@1  53.12 ( 50.45)	Acc@5  80.47 ( 80.57)
Epoch: [16][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.0098e+00 (1.8235e+00)	Acc@1  45.31 ( 50.62)	Acc@5  71.88 ( 80.61)
Epoch: [16][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.0332e+00 (1.8214e+00)	Acc@1  46.09 ( 50.52)	Acc@5  73.44 ( 80.64)
Epoch: [16][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6953e+00 (1.8282e+00)	Acc@1  51.56 ( 50.27)	Acc@5  83.59 ( 80.55)
Epoch: [16][110/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0078e+00 (1.8283e+00)	Acc@1  44.53 ( 50.25)	Acc@5  77.34 ( 80.62)
Epoch: [16][120/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8984e+00 (1.8343e+00)	Acc@1  50.78 ( 50.18)	Acc@5  78.91 ( 80.44)
Epoch: [16][130/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6572e+00 (1.8358e+00)	Acc@1  51.56 ( 50.17)	Acc@5  85.16 ( 80.40)
Epoch: [16][140/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8350e+00 (1.8374e+00)	Acc@1  50.00 ( 50.16)	Acc@5  79.69 ( 80.36)
Epoch: [16][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7021e+00 (1.8419e+00)	Acc@1  54.69 ( 50.14)	Acc@5  84.38 ( 80.25)
Epoch: [16][160/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8711e+00 (1.8381e+00)	Acc@1  45.31 ( 50.16)	Acc@5  78.91 ( 80.34)
Epoch: [16][170/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8037e+00 (1.8377e+00)	Acc@1  53.91 ( 50.21)	Acc@5  79.69 ( 80.35)
Epoch: [16][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3301e+00 (1.8401e+00)	Acc@1  39.84 ( 50.12)	Acc@5  73.44 ( 80.30)
Epoch: [16][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9170e+00 (1.8399e+00)	Acc@1  50.78 ( 50.13)	Acc@5  77.34 ( 80.34)
Epoch: [16][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8721e+00 (1.8410e+00)	Acc@1  50.00 ( 50.05)	Acc@5  82.03 ( 80.34)
Epoch: [16][210/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2734e+00 (1.8445e+00)	Acc@1  46.09 ( 49.93)	Acc@5  69.53 ( 80.24)
Epoch: [16][220/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7539e+00 (1.8423e+00)	Acc@1  48.44 ( 49.92)	Acc@5  85.94 ( 80.35)
Epoch: [16][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5869e+00 (1.8399e+00)	Acc@1  54.69 ( 49.98)	Acc@5  85.94 ( 80.42)
Epoch: [16][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8623e+00 (1.8414e+00)	Acc@1  48.44 ( 49.87)	Acc@5  82.81 ( 80.46)
Epoch: [16][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8105e+00 (1.8426e+00)	Acc@1  41.41 ( 49.81)	Acc@5  84.38 ( 80.42)
Epoch: [16][260/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6025e+00 (1.8428e+00)	Acc@1  55.47 ( 49.81)	Acc@5  84.38 ( 80.39)
Epoch: [16][270/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8252e+00 (1.8464e+00)	Acc@1  53.12 ( 49.76)	Acc@5  78.91 ( 80.35)
Epoch: [16][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9414e+00 (1.8472e+00)	Acc@1  42.97 ( 49.72)	Acc@5  82.81 ( 80.32)
Epoch: [16][290/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8818e+00 (1.8488e+00)	Acc@1  45.31 ( 49.72)	Acc@5  80.47 ( 80.26)
Epoch: [16][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8398e+00 (1.8473e+00)	Acc@1  49.22 ( 49.78)	Acc@5  79.69 ( 80.32)
Epoch: [16][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7100e+00 (1.8466e+00)	Acc@1  52.34 ( 49.80)	Acc@5  84.38 ( 80.31)
Epoch: [16][320/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9609e+00 (1.8494e+00)	Acc@1  53.91 ( 49.77)	Acc@5  75.78 ( 80.19)
Epoch: [16][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0273e+00 (1.8484e+00)	Acc@1  50.78 ( 49.78)	Acc@5  71.09 ( 80.20)
Epoch: [16][340/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6055e+00 (1.8470e+00)	Acc@1  55.47 ( 49.83)	Acc@5  85.94 ( 80.24)
Epoch: [16][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8535e+00 (1.8481e+00)	Acc@1  47.66 ( 49.78)	Acc@5  78.91 ( 80.25)
Epoch: [16][360/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0742e+00 (1.8486e+00)	Acc@1  42.97 ( 49.79)	Acc@5  78.91 ( 80.27)
Epoch: [16][370/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8398e+00 (1.8485e+00)	Acc@1  49.22 ( 49.79)	Acc@5  78.12 ( 80.25)
Epoch: [16][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5459e+00 (1.8468e+00)	Acc@1  59.38 ( 49.83)	Acc@5  83.59 ( 80.28)
Epoch: [16][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8018e+00 (1.8466e+00)	Acc@1  52.50 ( 49.82)	Acc@5  76.25 ( 80.28)
## e[16] optimizer.zero_grad (sum) time: 0.5542490482330322
## e[16]       loss.backward (sum) time: 12.571289777755737
## e[16]      optimizer.step (sum) time: 25.42264223098755
## epoch[16] training(only) time: 63.24974870681763
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.8750e+00 (1.8750e+00)	Acc@1  56.00 ( 56.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.9180e+00 (1.9078e+00)	Acc@1  41.00 ( 51.18)	Acc@5  79.00 ( 79.55)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.5908e+00 (1.8435e+00)	Acc@1  56.00 ( 51.57)	Acc@5  84.00 ( 80.29)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.8271e+00 (1.8627e+00)	Acc@1  43.00 ( 50.29)	Acc@5  84.00 ( 79.71)
Test: [ 40/100]	Time  0.048 ( 0.050)	Loss 1.8633e+00 (1.8690e+00)	Acc@1  51.00 ( 49.73)	Acc@5  81.00 ( 79.44)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 1.9512e+00 (1.8820e+00)	Acc@1  49.00 ( 49.55)	Acc@5  75.00 ( 79.04)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.8799e+00 (1.8698e+00)	Acc@1  48.00 ( 49.97)	Acc@5  80.00 ( 79.20)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.0312e+00 (1.8711e+00)	Acc@1  44.00 ( 49.99)	Acc@5  77.00 ( 79.24)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 2.0195e+00 (1.8758e+00)	Acc@1  49.00 ( 49.85)	Acc@5  76.00 ( 79.28)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.7998e+00 (1.8640e+00)	Acc@1  50.00 ( 50.02)	Acc@5  82.00 ( 79.38)
 * Acc@1 50.140 Acc@5 79.270
### epoch[16] execution time: 68.13723254203796
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.362 ( 0.362)	Data  0.181 ( 0.181)	Loss 1.6602e+00 (1.6602e+00)	Acc@1  56.25 ( 56.25)	Acc@5  86.72 ( 86.72)
Epoch: [17][ 10/391]	Time  0.159 ( 0.178)	Data  0.001 ( 0.017)	Loss 1.7842e+00 (1.7175e+00)	Acc@1  51.56 ( 52.77)	Acc@5  82.81 ( 83.45)
Epoch: [17][ 20/391]	Time  0.159 ( 0.171)	Data  0.001 ( 0.009)	Loss 1.7549e+00 (1.7248e+00)	Acc@1  51.56 ( 52.57)	Acc@5  86.72 ( 82.96)
Epoch: [17][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.007)	Loss 1.7393e+00 (1.7090e+00)	Acc@1  50.78 ( 52.75)	Acc@5  82.03 ( 83.24)
Epoch: [17][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.9268e+00 (1.7281e+00)	Acc@1  51.56 ( 52.46)	Acc@5  76.56 ( 82.70)
Epoch: [17][ 50/391]	Time  0.168 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.8955e+00 (1.7399e+00)	Acc@1  47.66 ( 52.37)	Acc@5  81.25 ( 82.35)
Epoch: [17][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4766e+00 (1.7519e+00)	Acc@1  51.56 ( 51.99)	Acc@5  91.41 ( 82.42)
Epoch: [17][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7705e+00 (1.7433e+00)	Acc@1  52.34 ( 52.09)	Acc@5  85.94 ( 82.57)
Epoch: [17][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.5430e+00 (1.7401e+00)	Acc@1  57.81 ( 52.17)	Acc@5  85.16 ( 82.50)
Epoch: [17][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.9609e+00 (1.7474e+00)	Acc@1  50.00 ( 52.11)	Acc@5  80.47 ( 82.37)
Epoch: [17][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6562e+00 (1.7460e+00)	Acc@1  56.25 ( 52.25)	Acc@5  78.91 ( 82.26)
Epoch: [17][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.8330e+00 (1.7472e+00)	Acc@1  48.44 ( 52.11)	Acc@5  82.81 ( 82.31)
Epoch: [17][120/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8438e+00 (1.7486e+00)	Acc@1  55.47 ( 52.28)	Acc@5  78.91 ( 82.12)
Epoch: [17][130/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6660e+00 (1.7486e+00)	Acc@1  50.00 ( 52.22)	Acc@5  83.59 ( 82.09)
Epoch: [17][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0137e+00 (1.7527e+00)	Acc@1  43.75 ( 52.14)	Acc@5  75.78 ( 81.95)
Epoch: [17][150/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6260e+00 (1.7471e+00)	Acc@1  53.91 ( 52.28)	Acc@5  88.28 ( 82.06)
Epoch: [17][160/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8389e+00 (1.7555e+00)	Acc@1  50.78 ( 52.14)	Acc@5  81.25 ( 81.93)
Epoch: [17][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6143e+00 (1.7560e+00)	Acc@1  56.25 ( 52.15)	Acc@5  82.03 ( 81.96)
Epoch: [17][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6836e+00 (1.7590e+00)	Acc@1  53.12 ( 52.05)	Acc@5  83.59 ( 81.91)
Epoch: [17][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9912e+00 (1.7632e+00)	Acc@1  45.31 ( 51.93)	Acc@5  75.00 ( 81.83)
Epoch: [17][200/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8525e+00 (1.7679e+00)	Acc@1  49.22 ( 51.83)	Acc@5  81.25 ( 81.73)
Epoch: [17][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3086e+00 (1.7658e+00)	Acc@1  61.72 ( 51.87)	Acc@5  89.06 ( 81.75)
Epoch: [17][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7012e+00 (1.7658e+00)	Acc@1  50.78 ( 51.93)	Acc@5  84.38 ( 81.72)
Epoch: [17][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9746e+00 (1.7667e+00)	Acc@1  41.41 ( 51.87)	Acc@5  75.78 ( 81.66)
Epoch: [17][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8135e+00 (1.7668e+00)	Acc@1  46.09 ( 51.79)	Acc@5  80.47 ( 81.66)
Epoch: [17][250/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6465e+00 (1.7673e+00)	Acc@1  52.34 ( 51.81)	Acc@5  85.16 ( 81.70)
Epoch: [17][260/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6318e+00 (1.7662e+00)	Acc@1  57.81 ( 51.83)	Acc@5  82.03 ( 81.72)
Epoch: [17][270/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0605e+00 (1.7694e+00)	Acc@1  47.66 ( 51.78)	Acc@5  75.78 ( 81.67)
Epoch: [17][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7256e+00 (1.7688e+00)	Acc@1  50.78 ( 51.77)	Acc@5  84.38 ( 81.66)
Epoch: [17][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7764e+00 (1.7686e+00)	Acc@1  50.00 ( 51.77)	Acc@5  85.16 ( 81.68)
Epoch: [17][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7705e+00 (1.7687e+00)	Acc@1  52.34 ( 51.79)	Acc@5  80.47 ( 81.66)
Epoch: [17][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1074e+00 (1.7688e+00)	Acc@1  42.97 ( 51.72)	Acc@5  75.00 ( 81.67)
Epoch: [17][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8818e+00 (1.7691e+00)	Acc@1  50.00 ( 51.68)	Acc@5  78.91 ( 81.70)
Epoch: [17][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9648e+00 (1.7694e+00)	Acc@1  49.22 ( 51.69)	Acc@5  78.12 ( 81.70)
Epoch: [17][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6240e+00 (1.7693e+00)	Acc@1  50.00 ( 51.72)	Acc@5  82.03 ( 81.69)
Epoch: [17][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6738e+00 (1.7687e+00)	Acc@1  53.91 ( 51.68)	Acc@5  81.25 ( 81.72)
Epoch: [17][360/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6396e+00 (1.7671e+00)	Acc@1  57.81 ( 51.78)	Acc@5  85.94 ( 81.75)
Epoch: [17][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7354e+00 (1.7654e+00)	Acc@1  47.66 ( 51.83)	Acc@5  82.81 ( 81.77)
Epoch: [17][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0508e+00 (1.7665e+00)	Acc@1  44.53 ( 51.76)	Acc@5  77.34 ( 81.76)
Epoch: [17][390/391]	Time  0.116 ( 0.161)	Data  0.001 ( 0.001)	Loss 2.3711e+00 (1.7632e+00)	Acc@1  33.75 ( 51.84)	Acc@5  68.75 ( 81.81)
## e[17] optimizer.zero_grad (sum) time: 0.5574562549591064
## e[17]       loss.backward (sum) time: 12.499422788619995
## e[17]      optimizer.step (sum) time: 25.4525728225708
## epoch[17] training(only) time: 63.22643041610718
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.9238e+00 (1.9238e+00)	Acc@1  48.00 ( 48.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.057 ( 0.059)	Loss 1.8418e+00 (1.8398e+00)	Acc@1  51.00 ( 51.82)	Acc@5  85.00 ( 80.00)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.5576e+00 (1.8193e+00)	Acc@1  59.00 ( 51.52)	Acc@5  85.00 ( 80.57)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 2.0059e+00 (1.8592e+00)	Acc@1  48.00 ( 50.68)	Acc@5  80.00 ( 80.06)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.7842e+00 (1.8468e+00)	Acc@1  53.00 ( 50.90)	Acc@5  79.00 ( 80.34)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6523e+00 (1.8575e+00)	Acc@1  54.00 ( 50.51)	Acc@5  74.00 ( 79.84)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.9014e+00 (1.8502e+00)	Acc@1  49.00 ( 50.41)	Acc@5  79.00 ( 79.90)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.1191e+00 (1.8521e+00)	Acc@1  47.00 ( 50.51)	Acc@5  72.00 ( 79.77)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.8516e+00 (1.8585e+00)	Acc@1  50.00 ( 50.53)	Acc@5  82.00 ( 79.68)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7910e+00 (1.8500e+00)	Acc@1  54.00 ( 50.95)	Acc@5  83.00 ( 79.76)
 * Acc@1 50.980 Acc@5 79.770
### epoch[17] execution time: 68.16666531562805
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.333 ( 0.333)	Data  0.159 ( 0.159)	Loss 1.5986e+00 (1.5986e+00)	Acc@1  54.69 ( 54.69)	Acc@5  84.38 ( 84.38)
Epoch: [18][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.7695e+00 (1.6996e+00)	Acc@1  53.12 ( 53.27)	Acc@5  82.81 ( 82.95)
Epoch: [18][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.009)	Loss 1.7451e+00 (1.6535e+00)	Acc@1  58.59 ( 54.65)	Acc@5  82.81 ( 83.48)
Epoch: [18][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.4746e+00 (1.6575e+00)	Acc@1  57.03 ( 54.31)	Acc@5  85.16 ( 83.85)
Epoch: [18][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.5889e+00 (1.6259e+00)	Acc@1  60.16 ( 55.22)	Acc@5  84.38 ( 84.34)
Epoch: [18][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.7002e+00 (1.6298e+00)	Acc@1  53.12 ( 54.84)	Acc@5  82.03 ( 84.13)
Epoch: [18][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.5215e+00 (1.6409e+00)	Acc@1  59.38 ( 54.66)	Acc@5  85.94 ( 83.86)
Epoch: [18][ 70/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.8096e+00 (1.6523e+00)	Acc@1  57.81 ( 54.40)	Acc@5  81.25 ( 83.66)
Epoch: [18][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.9424e+00 (1.6613e+00)	Acc@1  46.09 ( 54.30)	Acc@5  79.69 ( 83.49)
Epoch: [18][ 90/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6465e+00 (1.6605e+00)	Acc@1  55.47 ( 54.33)	Acc@5  81.25 ( 83.49)
Epoch: [18][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7158e+00 (1.6593e+00)	Acc@1  52.34 ( 54.52)	Acc@5  83.59 ( 83.50)
Epoch: [18][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6699e+00 (1.6555e+00)	Acc@1  54.69 ( 54.55)	Acc@5  82.81 ( 83.64)
Epoch: [18][120/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6006e+00 (1.6564e+00)	Acc@1  57.03 ( 54.61)	Acc@5  89.06 ( 83.65)
Epoch: [18][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6104e+00 (1.6527e+00)	Acc@1  53.12 ( 54.71)	Acc@5  87.50 ( 83.77)
Epoch: [18][140/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7812e+00 (1.6594e+00)	Acc@1  50.00 ( 54.49)	Acc@5  79.69 ( 83.64)
Epoch: [18][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5928e+00 (1.6664e+00)	Acc@1  56.25 ( 54.25)	Acc@5  85.94 ( 83.60)
Epoch: [18][160/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7695e+00 (1.6700e+00)	Acc@1  49.22 ( 54.21)	Acc@5  83.59 ( 83.52)
Epoch: [18][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6250e+00 (1.6704e+00)	Acc@1  53.91 ( 54.20)	Acc@5  85.16 ( 83.47)
Epoch: [18][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9033e+00 (1.6746e+00)	Acc@1  53.91 ( 54.16)	Acc@5  81.25 ( 83.49)
Epoch: [18][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7588e+00 (1.6730e+00)	Acc@1  48.44 ( 54.18)	Acc@5  82.03 ( 83.54)
Epoch: [18][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6904e+00 (1.6790e+00)	Acc@1  53.12 ( 54.00)	Acc@5  82.81 ( 83.45)
Epoch: [18][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7822e+00 (1.6761e+00)	Acc@1  48.44 ( 54.03)	Acc@5  75.78 ( 83.48)
Epoch: [18][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7441e+00 (1.6752e+00)	Acc@1  55.47 ( 54.01)	Acc@5  79.69 ( 83.48)
Epoch: [18][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5312e+00 (1.6748e+00)	Acc@1  58.59 ( 53.96)	Acc@5  84.38 ( 83.50)
Epoch: [18][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8369e+00 (1.6760e+00)	Acc@1  48.44 ( 53.93)	Acc@5  82.81 ( 83.50)
Epoch: [18][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8916e+00 (1.6772e+00)	Acc@1  50.78 ( 53.89)	Acc@5  74.22 ( 83.40)
Epoch: [18][260/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5752e+00 (1.6783e+00)	Acc@1  54.69 ( 53.81)	Acc@5  85.94 ( 83.34)
Epoch: [18][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6816e+00 (1.6830e+00)	Acc@1  53.91 ( 53.69)	Acc@5  82.81 ( 83.28)
Epoch: [18][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7197e+00 (1.6830e+00)	Acc@1  52.34 ( 53.71)	Acc@5  82.03 ( 83.26)
Epoch: [18][290/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7812e+00 (1.6820e+00)	Acc@1  50.78 ( 53.70)	Acc@5  79.69 ( 83.28)
Epoch: [18][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5596e+00 (1.6825e+00)	Acc@1  57.81 ( 53.68)	Acc@5  87.50 ( 83.26)
Epoch: [18][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6016e+00 (1.6836e+00)	Acc@1  55.47 ( 53.65)	Acc@5  85.16 ( 83.26)
Epoch: [18][320/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0469e+00 (1.6847e+00)	Acc@1  47.66 ( 53.58)	Acc@5  69.53 ( 83.24)
Epoch: [18][330/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8184e+00 (1.6848e+00)	Acc@1  50.00 ( 53.57)	Acc@5  82.03 ( 83.23)
Epoch: [18][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9404e+00 (1.6888e+00)	Acc@1  48.44 ( 53.49)	Acc@5  79.69 ( 83.14)
Epoch: [18][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5371e+00 (1.6901e+00)	Acc@1  53.91 ( 53.42)	Acc@5  85.94 ( 83.14)
Epoch: [18][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9014e+00 (1.6924e+00)	Acc@1  49.22 ( 53.41)	Acc@5  81.25 ( 83.08)
Epoch: [18][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8193e+00 (1.6920e+00)	Acc@1  56.25 ( 53.45)	Acc@5  78.12 ( 83.08)
Epoch: [18][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6465e+00 (1.6939e+00)	Acc@1  53.91 ( 53.41)	Acc@5  84.38 ( 83.06)
Epoch: [18][390/391]	Time  0.114 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.8398e+00 (1.6913e+00)	Acc@1  45.00 ( 53.48)	Acc@5  86.25 ( 83.09)
## e[18] optimizer.zero_grad (sum) time: 0.5468616485595703
## e[18]       loss.backward (sum) time: 12.511165618896484
## e[18]      optimizer.step (sum) time: 25.445446968078613
## epoch[18] training(only) time: 63.24718904495239
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.7412e+00 (1.7412e+00)	Acc@1  58.00 ( 58.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.7295e+00 (1.7188e+00)	Acc@1  44.00 ( 55.82)	Acc@5  86.00 ( 82.64)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.3848e+00 (1.6744e+00)	Acc@1  62.00 ( 56.29)	Acc@5  87.00 ( 83.19)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 1.5967e+00 (1.7033e+00)	Acc@1  53.00 ( 55.29)	Acc@5  85.00 ( 82.39)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.7471e+00 (1.7020e+00)	Acc@1  56.00 ( 54.66)	Acc@5  80.00 ( 82.22)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.8447e+00 (1.7245e+00)	Acc@1  55.00 ( 54.35)	Acc@5  74.00 ( 81.67)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.6943e+00 (1.7054e+00)	Acc@1  56.00 ( 54.30)	Acc@5  83.00 ( 81.93)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.8047e+00 (1.7155e+00)	Acc@1  53.00 ( 54.08)	Acc@5  81.00 ( 81.93)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.7705e+00 (1.7196e+00)	Acc@1  49.00 ( 53.57)	Acc@5  86.00 ( 82.09)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.6895e+00 (1.7101e+00)	Acc@1  53.00 ( 53.68)	Acc@5  80.00 ( 82.31)
 * Acc@1 53.610 Acc@5 82.270
### epoch[18] execution time: 68.1690411567688
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.328 ( 0.328)	Data  0.162 ( 0.162)	Loss 1.3203e+00 (1.3203e+00)	Acc@1  63.28 ( 63.28)	Acc@5  89.84 ( 89.84)
Epoch: [19][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.6553e+00 (1.5457e+00)	Acc@1  50.00 ( 55.33)	Acc@5  81.25 ( 85.65)
Epoch: [19][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.009)	Loss 1.7832e+00 (1.5688e+00)	Acc@1  50.00 ( 55.32)	Acc@5  79.69 ( 85.45)
Epoch: [19][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.4180e+00 (1.5741e+00)	Acc@1  64.06 ( 55.37)	Acc@5  85.16 ( 85.43)
Epoch: [19][ 40/391]	Time  0.167 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.4082e+00 (1.5621e+00)	Acc@1  61.72 ( 55.93)	Acc@5  87.50 ( 85.50)
Epoch: [19][ 50/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3193e+00 (1.5594e+00)	Acc@1  67.19 ( 56.20)	Acc@5  89.84 ( 85.60)
Epoch: [19][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.004)	Loss 1.6367e+00 (1.5716e+00)	Acc@1  53.12 ( 55.88)	Acc@5  83.59 ( 85.36)
Epoch: [19][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4658e+00 (1.5629e+00)	Acc@1  63.28 ( 56.16)	Acc@5  89.84 ( 85.53)
Epoch: [19][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.5605e+00 (1.5700e+00)	Acc@1  52.34 ( 55.79)	Acc@5  86.72 ( 85.35)
Epoch: [19][ 90/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3936e+00 (1.5841e+00)	Acc@1  60.16 ( 55.72)	Acc@5  88.28 ( 85.21)
Epoch: [19][100/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.003)	Loss 1.5352e+00 (1.5883e+00)	Acc@1  55.47 ( 55.55)	Acc@5  86.72 ( 85.11)
Epoch: [19][110/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6260e+00 (1.5926e+00)	Acc@1  50.00 ( 55.44)	Acc@5  80.47 ( 84.90)
Epoch: [19][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6787e+00 (1.6017e+00)	Acc@1  53.12 ( 55.26)	Acc@5  84.38 ( 84.78)
Epoch: [19][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4619e+00 (1.6028e+00)	Acc@1  58.59 ( 55.28)	Acc@5  86.72 ( 84.71)
Epoch: [19][140/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6611e+00 (1.6042e+00)	Acc@1  57.03 ( 55.31)	Acc@5  82.81 ( 84.66)
Epoch: [19][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5020e+00 (1.6072e+00)	Acc@1  58.59 ( 55.17)	Acc@5  89.84 ( 84.62)
Epoch: [19][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4580e+00 (1.6076e+00)	Acc@1  59.38 ( 55.20)	Acc@5  85.94 ( 84.62)
Epoch: [19][170/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0098e+00 (1.6108e+00)	Acc@1  45.31 ( 55.17)	Acc@5  78.12 ( 84.58)
Epoch: [19][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7695e+00 (1.6103e+00)	Acc@1  50.00 ( 55.19)	Acc@5  80.47 ( 84.52)
Epoch: [19][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4785e+00 (1.6105e+00)	Acc@1  57.03 ( 55.24)	Acc@5  88.28 ( 84.51)
Epoch: [19][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7256e+00 (1.6086e+00)	Acc@1  52.34 ( 55.24)	Acc@5  84.38 ( 84.53)
Epoch: [19][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4258e+00 (1.6118e+00)	Acc@1  56.25 ( 55.11)	Acc@5  91.41 ( 84.51)
Epoch: [19][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5381e+00 (1.6114e+00)	Acc@1  54.69 ( 55.12)	Acc@5  86.72 ( 84.54)
Epoch: [19][230/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8438e+00 (1.6156e+00)	Acc@1  50.78 ( 55.09)	Acc@5  76.56 ( 84.46)
Epoch: [19][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4785e+00 (1.6150e+00)	Acc@1  57.03 ( 55.13)	Acc@5  89.06 ( 84.43)
Epoch: [19][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3975e+00 (1.6164e+00)	Acc@1  61.72 ( 55.15)	Acc@5  89.84 ( 84.40)
Epoch: [19][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6348e+00 (1.6162e+00)	Acc@1  50.78 ( 55.17)	Acc@5  84.38 ( 84.39)
Epoch: [19][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6289e+00 (1.6143e+00)	Acc@1  53.91 ( 55.24)	Acc@5  82.03 ( 84.42)
Epoch: [19][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7314e+00 (1.6124e+00)	Acc@1  55.47 ( 55.29)	Acc@5  82.81 ( 84.47)
Epoch: [19][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5576e+00 (1.6114e+00)	Acc@1  53.91 ( 55.24)	Acc@5  82.81 ( 84.47)
Epoch: [19][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5771e+00 (1.6108e+00)	Acc@1  61.72 ( 55.22)	Acc@5  82.81 ( 84.46)
Epoch: [19][310/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6484e+00 (1.6130e+00)	Acc@1  55.47 ( 55.18)	Acc@5  83.59 ( 84.41)
Epoch: [19][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4111e+00 (1.6125e+00)	Acc@1  58.59 ( 55.18)	Acc@5  85.94 ( 84.38)
Epoch: [19][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4824e+00 (1.6133e+00)	Acc@1  53.12 ( 55.14)	Acc@5  84.38 ( 84.34)
Epoch: [19][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6787e+00 (1.6128e+00)	Acc@1  50.78 ( 55.16)	Acc@5  84.38 ( 84.37)
Epoch: [19][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7842e+00 (1.6150e+00)	Acc@1  50.78 ( 55.09)	Acc@5  81.25 ( 84.36)
Epoch: [19][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5391e+00 (1.6158e+00)	Acc@1  60.16 ( 55.10)	Acc@5  85.16 ( 84.32)
Epoch: [19][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7236e+00 (1.6170e+00)	Acc@1  52.34 ( 55.10)	Acc@5  82.03 ( 84.30)
Epoch: [19][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6514e+00 (1.6168e+00)	Acc@1  60.16 ( 55.10)	Acc@5  85.94 ( 84.32)
Epoch: [19][390/391]	Time  0.113 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.5234e+00 (1.6166e+00)	Acc@1  50.00 ( 55.07)	Acc@5  87.50 ( 84.35)
## e[19] optimizer.zero_grad (sum) time: 0.5527224540710449
## e[19]       loss.backward (sum) time: 12.55490493774414
## e[19]      optimizer.step (sum) time: 25.410407781600952
## epoch[19] training(only) time: 63.17282223701477
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 2.1152e+00 (2.1152e+00)	Acc@1  50.00 ( 50.00)	Acc@5  77.00 ( 77.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.9727e+00 (1.9118e+00)	Acc@1  44.00 ( 52.09)	Acc@5  80.00 ( 79.36)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.5684e+00 (1.8774e+00)	Acc@1  58.00 ( 52.19)	Acc@5  82.00 ( 79.86)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.9277e+00 (1.8957e+00)	Acc@1  44.00 ( 51.35)	Acc@5  82.00 ( 79.55)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.9980e+00 (1.8959e+00)	Acc@1  51.00 ( 51.39)	Acc@5  79.00 ( 80.12)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.7627e+00 (1.9047e+00)	Acc@1  56.00 ( 51.02)	Acc@5  79.00 ( 79.63)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.8223e+00 (1.8996e+00)	Acc@1  52.00 ( 50.79)	Acc@5  79.00 ( 79.66)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.1445e+00 (1.9003e+00)	Acc@1  47.00 ( 50.70)	Acc@5  77.00 ( 79.69)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 2.1523e+00 (1.9078e+00)	Acc@1  48.00 ( 50.54)	Acc@5  76.00 ( 79.31)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.6631e+00 (1.8975e+00)	Acc@1  54.00 ( 50.75)	Acc@5  80.00 ( 79.45)
 * Acc@1 50.710 Acc@5 79.530
### epoch[19] execution time: 68.15925669670105
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.328 ( 0.328)	Data  0.151 ( 0.151)	Loss 1.4492e+00 (1.4492e+00)	Acc@1  61.72 ( 61.72)	Acc@5  85.16 ( 85.16)
Epoch: [20][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.5488e+00 (1.4426e+00)	Acc@1  52.34 ( 59.09)	Acc@5  87.50 ( 87.71)
Epoch: [20][ 20/391]	Time  0.172 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.4648e+00 (1.4355e+00)	Acc@1  54.69 ( 58.89)	Acc@5  88.28 ( 87.72)
Epoch: [20][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.7607e+00 (1.4733e+00)	Acc@1  50.00 ( 58.32)	Acc@5  82.03 ( 87.27)
Epoch: [20][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.4111e+00 (1.4738e+00)	Acc@1  66.41 ( 58.75)	Acc@5  89.06 ( 87.35)
Epoch: [20][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4990e+00 (1.4920e+00)	Acc@1  59.38 ( 58.35)	Acc@5  85.94 ( 86.60)
Epoch: [20][ 60/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4346e+00 (1.4843e+00)	Acc@1  60.16 ( 58.40)	Acc@5  87.50 ( 86.94)
Epoch: [20][ 70/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2627e+00 (1.4757e+00)	Acc@1  60.94 ( 58.56)	Acc@5  90.62 ( 87.10)
Epoch: [20][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2588e+00 (1.4725e+00)	Acc@1  64.06 ( 58.83)	Acc@5  87.50 ( 87.06)
Epoch: [20][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6875e+00 (1.4805e+00)	Acc@1  54.69 ( 58.70)	Acc@5  85.94 ( 86.96)
Epoch: [20][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3721e+00 (1.4852e+00)	Acc@1  63.28 ( 58.59)	Acc@5  89.84 ( 86.79)
Epoch: [20][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5557e+00 (1.4933e+00)	Acc@1  62.50 ( 58.40)	Acc@5  82.03 ( 86.57)
Epoch: [20][120/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7295e+00 (1.4985e+00)	Acc@1  55.47 ( 58.32)	Acc@5  79.69 ( 86.49)
Epoch: [20][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5137e+00 (1.5052e+00)	Acc@1  60.16 ( 58.12)	Acc@5  85.16 ( 86.44)
Epoch: [20][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4180e+00 (1.5119e+00)	Acc@1  63.28 ( 57.91)	Acc@5  86.72 ( 86.26)
Epoch: [20][150/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4971e+00 (1.5180e+00)	Acc@1  60.16 ( 57.83)	Acc@5  87.50 ( 86.14)
Epoch: [20][160/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5332e+00 (1.5209e+00)	Acc@1  59.38 ( 57.84)	Acc@5  85.94 ( 86.08)
Epoch: [20][170/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6631e+00 (1.5238e+00)	Acc@1  48.44 ( 57.68)	Acc@5  85.94 ( 86.09)
Epoch: [20][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6172e+00 (1.5242e+00)	Acc@1  52.34 ( 57.65)	Acc@5  81.25 ( 86.09)
Epoch: [20][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5586e+00 (1.5218e+00)	Acc@1  52.34 ( 57.65)	Acc@5  87.50 ( 86.11)
Epoch: [20][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6953e+00 (1.5229e+00)	Acc@1  53.12 ( 57.60)	Acc@5  82.81 ( 86.07)
Epoch: [20][210/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7529e+00 (1.5252e+00)	Acc@1  51.56 ( 57.49)	Acc@5  81.25 ( 86.01)
Epoch: [20][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4551e+00 (1.5259e+00)	Acc@1  60.16 ( 57.41)	Acc@5  85.16 ( 86.03)
Epoch: [20][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4492e+00 (1.5258e+00)	Acc@1  57.81 ( 57.43)	Acc@5  91.41 ( 86.05)
Epoch: [20][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6963e+00 (1.5261e+00)	Acc@1  52.34 ( 57.43)	Acc@5  80.47 ( 86.01)
Epoch: [20][250/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5488e+00 (1.5295e+00)	Acc@1  55.47 ( 57.36)	Acc@5  84.38 ( 85.94)
Epoch: [20][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5791e+00 (1.5283e+00)	Acc@1  56.25 ( 57.39)	Acc@5  81.25 ( 85.92)
Epoch: [20][270/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6885e+00 (1.5306e+00)	Acc@1  53.12 ( 57.31)	Acc@5  85.94 ( 85.86)
Epoch: [20][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4521e+00 (1.5327e+00)	Acc@1  63.28 ( 57.23)	Acc@5  83.59 ( 85.81)
Epoch: [20][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4844e+00 (1.5326e+00)	Acc@1  61.72 ( 57.20)	Acc@5  82.81 ( 85.78)
Epoch: [20][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5674e+00 (1.5335e+00)	Acc@1  61.72 ( 57.19)	Acc@5  84.38 ( 85.77)
Epoch: [20][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4863e+00 (1.5338e+00)	Acc@1  56.25 ( 57.19)	Acc@5  85.94 ( 85.75)
Epoch: [20][320/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5117e+00 (1.5358e+00)	Acc@1  57.81 ( 57.11)	Acc@5  82.81 ( 85.71)
Epoch: [20][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4082e+00 (1.5361e+00)	Acc@1  64.06 ( 57.11)	Acc@5  86.72 ( 85.72)
Epoch: [20][340/391]	Time  0.175 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6934e+00 (1.5381e+00)	Acc@1  53.91 ( 57.08)	Acc@5  82.03 ( 85.64)
Epoch: [20][350/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6719e+00 (1.5376e+00)	Acc@1  55.47 ( 57.07)	Acc@5  82.81 ( 85.66)
Epoch: [20][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5215e+00 (1.5376e+00)	Acc@1  61.72 ( 57.09)	Acc@5  86.72 ( 85.67)
Epoch: [20][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6484e+00 (1.5401e+00)	Acc@1  57.03 ( 56.99)	Acc@5  81.25 ( 85.63)
Epoch: [20][380/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6318e+00 (1.5408e+00)	Acc@1  58.59 ( 57.00)	Acc@5  81.25 ( 85.61)
Epoch: [20][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5625e+00 (1.5401e+00)	Acc@1  57.50 ( 57.00)	Acc@5  82.50 ( 85.57)
## e[20] optimizer.zero_grad (sum) time: 0.561920166015625
## e[20]       loss.backward (sum) time: 12.540940523147583
## e[20]      optimizer.step (sum) time: 25.393375635147095
## epoch[20] training(only) time: 63.31271457672119
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.8184e+00 (1.8184e+00)	Acc@1  50.00 ( 50.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.046 ( 0.058)	Loss 1.8301e+00 (1.8050e+00)	Acc@1  50.00 ( 55.18)	Acc@5  84.00 ( 81.00)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.3057e+00 (1.7725e+00)	Acc@1  63.00 ( 54.71)	Acc@5  88.00 ( 82.10)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 1.5918e+00 (1.7826e+00)	Acc@1  48.00 ( 53.74)	Acc@5  87.00 ( 81.68)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.7021e+00 (1.7697e+00)	Acc@1  55.00 ( 53.80)	Acc@5  82.00 ( 82.10)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 1.7764e+00 (1.7761e+00)	Acc@1  57.00 ( 53.37)	Acc@5  82.00 ( 82.10)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.8936e+00 (1.7788e+00)	Acc@1  50.00 ( 53.08)	Acc@5  86.00 ( 82.39)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 2.0039e+00 (1.7835e+00)	Acc@1  51.00 ( 53.08)	Acc@5  79.00 ( 82.32)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 1.9229e+00 (1.7879e+00)	Acc@1  54.00 ( 52.94)	Acc@5  78.00 ( 82.27)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.7705e+00 (1.7804e+00)	Acc@1  59.00 ( 53.30)	Acc@5  82.00 ( 82.29)
 * Acc@1 53.360 Acc@5 82.210
### epoch[20] execution time: 68.24099373817444
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.351 ( 0.351)	Data  0.177 ( 0.177)	Loss 1.7207e+00 (1.7207e+00)	Acc@1  50.00 ( 50.00)	Acc@5  81.25 ( 81.25)
Epoch: [21][ 10/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.017)	Loss 1.5664e+00 (1.4935e+00)	Acc@1  57.03 ( 59.02)	Acc@5  85.94 ( 86.29)
Epoch: [21][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.009)	Loss 1.5400e+00 (1.4722e+00)	Acc@1  58.59 ( 59.00)	Acc@5  81.25 ( 86.12)
Epoch: [21][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.007)	Loss 1.5283e+00 (1.4591e+00)	Acc@1  52.34 ( 58.90)	Acc@5  85.16 ( 86.52)
Epoch: [21][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.5840e+00 (1.4625e+00)	Acc@1  59.38 ( 59.47)	Acc@5  85.16 ( 86.49)
Epoch: [21][ 50/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.4229e+00 (1.4606e+00)	Acc@1  60.94 ( 59.41)	Acc@5  85.16 ( 86.49)
Epoch: [21][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3428e+00 (1.4585e+00)	Acc@1  64.06 ( 59.41)	Acc@5  85.16 ( 86.53)
Epoch: [21][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.004)	Loss 1.3184e+00 (1.4538e+00)	Acc@1  61.72 ( 59.34)	Acc@5  89.06 ( 86.73)
Epoch: [21][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3193e+00 (1.4465e+00)	Acc@1  60.94 ( 59.38)	Acc@5  85.94 ( 86.84)
Epoch: [21][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3447e+00 (1.4442e+00)	Acc@1  64.84 ( 59.47)	Acc@5  86.72 ( 86.95)
Epoch: [21][100/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3613e+00 (1.4464e+00)	Acc@1  60.16 ( 59.49)	Acc@5  87.50 ( 86.96)
Epoch: [21][110/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4922e+00 (1.4467e+00)	Acc@1  53.91 ( 59.37)	Acc@5  85.94 ( 86.98)
Epoch: [21][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3584e+00 (1.4488e+00)	Acc@1  61.72 ( 59.45)	Acc@5  85.16 ( 86.91)
Epoch: [21][130/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8135e+00 (1.4560e+00)	Acc@1  53.91 ( 59.15)	Acc@5  80.47 ( 86.84)
Epoch: [21][140/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3760e+00 (1.4565e+00)	Acc@1  61.72 ( 59.27)	Acc@5  85.94 ( 86.74)
Epoch: [21][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4463e+00 (1.4582e+00)	Acc@1  59.38 ( 59.22)	Acc@5  87.50 ( 86.82)
Epoch: [21][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4932e+00 (1.4577e+00)	Acc@1  55.47 ( 59.17)	Acc@5  83.59 ( 86.80)
Epoch: [21][170/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7197e+00 (1.4639e+00)	Acc@1  55.47 ( 58.94)	Acc@5  82.03 ( 86.76)
Epoch: [21][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3506e+00 (1.4661e+00)	Acc@1  62.50 ( 58.89)	Acc@5  89.06 ( 86.72)
Epoch: [21][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4023e+00 (1.4683e+00)	Acc@1  62.50 ( 58.83)	Acc@5  86.72 ( 86.67)
Epoch: [21][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3438e+00 (1.4710e+00)	Acc@1  59.38 ( 58.73)	Acc@5  91.41 ( 86.68)
Epoch: [21][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4639e+00 (1.4735e+00)	Acc@1  53.91 ( 58.62)	Acc@5  88.28 ( 86.66)
Epoch: [21][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4639e+00 (1.4774e+00)	Acc@1  60.16 ( 58.57)	Acc@5  88.28 ( 86.60)
Epoch: [21][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4238e+00 (1.4789e+00)	Acc@1  58.59 ( 58.51)	Acc@5  89.84 ( 86.57)
Epoch: [21][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2949e+00 (1.4790e+00)	Acc@1  64.06 ( 58.51)	Acc@5  92.19 ( 86.51)
Epoch: [21][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2627e+00 (1.4787e+00)	Acc@1  60.16 ( 58.50)	Acc@5  92.19 ( 86.55)
Epoch: [21][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3857e+00 (1.4799e+00)	Acc@1  56.25 ( 58.41)	Acc@5  90.62 ( 86.55)
Epoch: [21][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4492e+00 (1.4787e+00)	Acc@1  57.03 ( 58.43)	Acc@5  87.50 ( 86.59)
Epoch: [21][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6230e+00 (1.4807e+00)	Acc@1  57.03 ( 58.41)	Acc@5  85.16 ( 86.55)
Epoch: [21][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6885e+00 (1.4816e+00)	Acc@1  56.25 ( 58.38)	Acc@5  85.94 ( 86.57)
Epoch: [21][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2002e+00 (1.4808e+00)	Acc@1  67.19 ( 58.40)	Acc@5  89.06 ( 86.54)
Epoch: [21][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6084e+00 (1.4815e+00)	Acc@1  55.47 ( 58.39)	Acc@5  82.03 ( 86.54)
Epoch: [21][320/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2900e+00 (1.4815e+00)	Acc@1  60.16 ( 58.34)	Acc@5  89.84 ( 86.53)
Epoch: [21][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6602e+00 (1.4840e+00)	Acc@1  55.47 ( 58.27)	Acc@5  82.81 ( 86.47)
Epoch: [21][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6240e+00 (1.4852e+00)	Acc@1  56.25 ( 58.25)	Acc@5  83.59 ( 86.43)
Epoch: [21][350/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3721e+00 (1.4853e+00)	Acc@1  62.50 ( 58.26)	Acc@5  87.50 ( 86.42)
Epoch: [21][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5391e+00 (1.4858e+00)	Acc@1  53.12 ( 58.21)	Acc@5  85.94 ( 86.44)
Epoch: [21][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0264e+00 (1.4860e+00)	Acc@1  71.09 ( 58.21)	Acc@5  95.31 ( 86.44)
Epoch: [21][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7393e+00 (1.4867e+00)	Acc@1  56.25 ( 58.20)	Acc@5  78.91 ( 86.41)
Epoch: [21][390/391]	Time  0.114 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.1572e+00 (1.4861e+00)	Acc@1  67.50 ( 58.23)	Acc@5  91.25 ( 86.43)
## e[21] optimizer.zero_grad (sum) time: 0.5614330768585205
## e[21]       loss.backward (sum) time: 12.543318033218384
## e[21]      optimizer.step (sum) time: 25.374322652816772
## epoch[21] training(only) time: 63.19598031044006
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 1.6572e+00 (1.6572e+00)	Acc@1  56.00 ( 56.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.7451e+00 (1.7594e+00)	Acc@1  50.00 ( 54.09)	Acc@5  85.00 ( 82.64)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.4023e+00 (1.7002e+00)	Acc@1  63.00 ( 55.19)	Acc@5  87.00 ( 83.48)
Test: [ 30/100]	Time  0.060 ( 0.051)	Loss 1.8213e+00 (1.7207e+00)	Acc@1  53.00 ( 54.52)	Acc@5  81.00 ( 82.71)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 1.6738e+00 (1.7165e+00)	Acc@1  57.00 ( 54.63)	Acc@5  83.00 ( 82.68)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.7656e+00 (1.7350e+00)	Acc@1  58.00 ( 54.47)	Acc@5  78.00 ( 82.31)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.6973e+00 (1.7228e+00)	Acc@1  53.00 ( 54.23)	Acc@5  84.00 ( 82.36)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 1.7754e+00 (1.7227e+00)	Acc@1  53.00 ( 54.28)	Acc@5  79.00 ( 82.28)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.9131e+00 (1.7317e+00)	Acc@1  56.00 ( 54.01)	Acc@5  80.00 ( 82.15)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.7617e+00 (1.7218e+00)	Acc@1  53.00 ( 54.31)	Acc@5  80.00 ( 82.31)
 * Acc@1 54.390 Acc@5 82.380
### epoch[21] execution time: 68.14021110534668
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.315 ( 0.315)	Data  0.143 ( 0.143)	Loss 1.3574e+00 (1.3574e+00)	Acc@1  60.94 ( 60.94)	Acc@5  85.94 ( 85.94)
Epoch: [22][ 10/391]	Time  0.160 ( 0.174)	Data  0.001 ( 0.014)	Loss 1.4414e+00 (1.3961e+00)	Acc@1  63.28 ( 60.87)	Acc@5  87.50 ( 88.35)
Epoch: [22][ 20/391]	Time  0.165 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.3252e+00 (1.4129e+00)	Acc@1  65.62 ( 60.16)	Acc@5  88.28 ( 88.13)
Epoch: [22][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.7861e+00 (1.4152e+00)	Acc@1  48.44 ( 60.08)	Acc@5  82.03 ( 87.98)
Epoch: [22][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.4160e+00 (1.3950e+00)	Acc@1  61.72 ( 60.71)	Acc@5  88.28 ( 88.19)
Epoch: [22][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3662e+00 (1.3784e+00)	Acc@1  56.25 ( 61.08)	Acc@5  89.06 ( 88.20)
Epoch: [22][ 60/391]	Time  0.175 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3799e+00 (1.3863e+00)	Acc@1  61.72 ( 60.94)	Acc@5  85.16 ( 87.97)
Epoch: [22][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3975e+00 (1.3712e+00)	Acc@1  58.59 ( 61.29)	Acc@5  88.28 ( 88.28)
Epoch: [22][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6055e+00 (1.3824e+00)	Acc@1  53.12 ( 60.91)	Acc@5  86.72 ( 88.15)
Epoch: [22][ 90/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.5586e+00 (1.3911e+00)	Acc@1  53.91 ( 60.64)	Acc@5  82.81 ( 87.92)
Epoch: [22][100/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5859e+00 (1.3946e+00)	Acc@1  57.81 ( 60.58)	Acc@5  85.16 ( 87.81)
Epoch: [22][110/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4453e+00 (1.4011e+00)	Acc@1  60.94 ( 60.53)	Acc@5  87.50 ( 87.68)
Epoch: [22][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4375e+00 (1.4025e+00)	Acc@1  57.81 ( 60.50)	Acc@5  89.06 ( 87.71)
Epoch: [22][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4150e+00 (1.4077e+00)	Acc@1  60.94 ( 60.31)	Acc@5  86.72 ( 87.58)
Epoch: [22][140/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4277e+00 (1.4073e+00)	Acc@1  58.59 ( 60.31)	Acc@5  88.28 ( 87.62)
Epoch: [22][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1787e+00 (1.4079e+00)	Acc@1  65.62 ( 60.24)	Acc@5  93.75 ( 87.63)
Epoch: [22][160/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7051e+00 (1.4086e+00)	Acc@1  51.56 ( 60.28)	Acc@5  86.72 ( 87.66)
Epoch: [22][170/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6494e+00 (1.4122e+00)	Acc@1  52.34 ( 60.12)	Acc@5  83.59 ( 87.61)
Epoch: [22][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3613e+00 (1.4096e+00)	Acc@1  63.28 ( 60.21)	Acc@5  89.06 ( 87.61)
Epoch: [22][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6973e+00 (1.4154e+00)	Acc@1  53.91 ( 60.09)	Acc@5  82.81 ( 87.48)
Epoch: [22][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3750e+00 (1.4183e+00)	Acc@1  61.72 ( 60.07)	Acc@5  89.84 ( 87.41)
Epoch: [22][210/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3877e+00 (1.4213e+00)	Acc@1  66.41 ( 60.01)	Acc@5  90.62 ( 87.37)
Epoch: [22][220/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6074e+00 (1.4215e+00)	Acc@1  57.81 ( 59.99)	Acc@5  82.03 ( 87.38)
Epoch: [22][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3154e+00 (1.4225e+00)	Acc@1  63.28 ( 60.02)	Acc@5  90.62 ( 87.35)
Epoch: [22][240/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2988e+00 (1.4213e+00)	Acc@1  62.50 ( 60.05)	Acc@5  90.62 ( 87.37)
Epoch: [22][250/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4150e+00 (1.4230e+00)	Acc@1  60.94 ( 60.03)	Acc@5  86.72 ( 87.36)
Epoch: [22][260/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5264e+00 (1.4234e+00)	Acc@1  53.91 ( 59.99)	Acc@5  87.50 ( 87.34)
Epoch: [22][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3330e+00 (1.4221e+00)	Acc@1  58.59 ( 59.98)	Acc@5  89.84 ( 87.37)
Epoch: [22][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7900e+00 (1.4246e+00)	Acc@1  51.56 ( 59.92)	Acc@5  82.81 ( 87.35)
Epoch: [22][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4580e+00 (1.4266e+00)	Acc@1  57.03 ( 59.90)	Acc@5  87.50 ( 87.29)
Epoch: [22][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5137e+00 (1.4262e+00)	Acc@1  56.25 ( 59.88)	Acc@5  84.38 ( 87.28)
Epoch: [22][310/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3740e+00 (1.4258e+00)	Acc@1  63.28 ( 59.94)	Acc@5  85.94 ( 87.28)
Epoch: [22][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5801e+00 (1.4261e+00)	Acc@1  58.59 ( 59.90)	Acc@5  87.50 ( 87.26)
Epoch: [22][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2256e+00 (1.4243e+00)	Acc@1  61.72 ( 59.89)	Acc@5  91.41 ( 87.30)
Epoch: [22][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7520e+00 (1.4244e+00)	Acc@1  51.56 ( 59.87)	Acc@5  82.81 ( 87.29)
Epoch: [22][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5830e+00 (1.4252e+00)	Acc@1  54.69 ( 59.86)	Acc@5  87.50 ( 87.30)
Epoch: [22][360/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3193e+00 (1.4257e+00)	Acc@1  59.38 ( 59.84)	Acc@5  89.06 ( 87.27)
Epoch: [22][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5303e+00 (1.4252e+00)	Acc@1  52.34 ( 59.86)	Acc@5  86.72 ( 87.28)
Epoch: [22][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4551e+00 (1.4259e+00)	Acc@1  58.59 ( 59.86)	Acc@5  85.94 ( 87.27)
Epoch: [22][390/391]	Time  0.113 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.2119e+00 (1.4262e+00)	Acc@1  62.50 ( 59.90)	Acc@5  93.75 ( 87.28)
## e[22] optimizer.zero_grad (sum) time: 0.5584475994110107
## e[22]       loss.backward (sum) time: 12.547126770019531
## e[22]      optimizer.step (sum) time: 25.37872815132141
## epoch[22] training(only) time: 63.149025201797485
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.4580e+00 (1.4580e+00)	Acc@1  63.00 ( 63.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.6787e+00 (1.6364e+00)	Acc@1  50.00 ( 55.73)	Acc@5  86.00 ( 83.64)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.3896e+00 (1.5995e+00)	Acc@1  65.00 ( 57.24)	Acc@5  85.00 ( 84.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4941e+00 (1.6096e+00)	Acc@1  54.00 ( 56.58)	Acc@5  87.00 ( 84.23)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.6016e+00 (1.5940e+00)	Acc@1  60.00 ( 56.71)	Acc@5  83.00 ( 84.37)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6104e+00 (1.6059e+00)	Acc@1  61.00 ( 56.82)	Acc@5  78.00 ( 83.90)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.6846e+00 (1.5934e+00)	Acc@1  58.00 ( 56.74)	Acc@5  81.00 ( 84.20)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4932e+00 (1.5918e+00)	Acc@1  63.00 ( 56.82)	Acc@5  87.00 ( 84.25)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.9014e+00 (1.6007e+00)	Acc@1  57.00 ( 56.51)	Acc@5  80.00 ( 84.22)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.5781e+00 (1.5915e+00)	Acc@1  61.00 ( 56.86)	Acc@5  82.00 ( 84.34)
 * Acc@1 56.860 Acc@5 84.470
### epoch[22] execution time: 68.05561423301697
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.325 ( 0.325)	Data  0.148 ( 0.148)	Loss 1.2373e+00 (1.2373e+00)	Acc@1  63.28 ( 63.28)	Acc@5  88.28 ( 88.28)
Epoch: [23][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.4121e+00 (1.2837e+00)	Acc@1  56.25 ( 62.00)	Acc@5  88.28 ( 90.27)
Epoch: [23][ 20/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.5811e+00 (1.3177e+00)	Acc@1  57.81 ( 61.87)	Acc@5  84.38 ( 89.66)
Epoch: [23][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.1963e+00 (1.3434e+00)	Acc@1  70.31 ( 61.87)	Acc@5  92.19 ( 89.26)
Epoch: [23][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.2344e+00 (1.3349e+00)	Acc@1  64.84 ( 62.35)	Acc@5  91.41 ( 89.23)
Epoch: [23][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4316e+00 (1.3454e+00)	Acc@1  59.38 ( 61.81)	Acc@5  88.28 ( 89.17)
Epoch: [23][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2666e+00 (1.3523e+00)	Acc@1  65.62 ( 61.72)	Acc@5  88.28 ( 89.02)
Epoch: [23][ 70/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1055e+00 (1.3455e+00)	Acc@1  71.88 ( 62.07)	Acc@5  92.97 ( 89.08)
Epoch: [23][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1602e+00 (1.3384e+00)	Acc@1  62.50 ( 62.21)	Acc@5  92.19 ( 89.09)
Epoch: [23][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3730e+00 (1.3341e+00)	Acc@1  59.38 ( 62.24)	Acc@5  88.28 ( 89.17)
Epoch: [23][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5479e+00 (1.3378e+00)	Acc@1  56.25 ( 62.22)	Acc@5  85.94 ( 89.04)
Epoch: [23][110/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3604e+00 (1.3383e+00)	Acc@1  61.72 ( 62.26)	Acc@5  89.84 ( 89.11)
Epoch: [23][120/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1953e+00 (1.3405e+00)	Acc@1  66.41 ( 62.22)	Acc@5  88.28 ( 88.99)
Epoch: [23][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3633e+00 (1.3397e+00)	Acc@1  58.59 ( 62.21)	Acc@5  88.28 ( 88.95)
Epoch: [23][140/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4580e+00 (1.3405e+00)	Acc@1  59.38 ( 62.17)	Acc@5  85.16 ( 88.88)
Epoch: [23][150/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0898e+00 (1.3360e+00)	Acc@1  67.19 ( 62.24)	Acc@5  90.62 ( 88.92)
Epoch: [23][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4668e+00 (1.3393e+00)	Acc@1  59.38 ( 62.14)	Acc@5  86.72 ( 88.81)
Epoch: [23][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2471e+00 (1.3421e+00)	Acc@1  65.62 ( 62.02)	Acc@5  89.06 ( 88.73)
Epoch: [23][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3418e+00 (1.3416e+00)	Acc@1  60.16 ( 62.02)	Acc@5  85.16 ( 88.68)
Epoch: [23][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2891e+00 (1.3421e+00)	Acc@1  67.19 ( 61.98)	Acc@5  89.06 ( 88.74)
Epoch: [23][200/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2695e+00 (1.3443e+00)	Acc@1  62.50 ( 61.85)	Acc@5  90.62 ( 88.72)
Epoch: [23][210/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7305e+00 (1.3496e+00)	Acc@1  50.78 ( 61.70)	Acc@5  82.03 ( 88.64)
Epoch: [23][220/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3730e+00 (1.3517e+00)	Acc@1  64.06 ( 61.65)	Acc@5  85.16 ( 88.57)
Epoch: [23][230/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5078e+00 (1.3526e+00)	Acc@1  56.25 ( 61.61)	Acc@5  89.06 ( 88.55)
Epoch: [23][240/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2930e+00 (1.3558e+00)	Acc@1  60.94 ( 61.49)	Acc@5  91.41 ( 88.52)
Epoch: [23][250/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6455e+00 (1.3581e+00)	Acc@1  52.34 ( 61.44)	Acc@5  86.72 ( 88.46)
Epoch: [23][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3115e+00 (1.3596e+00)	Acc@1  64.84 ( 61.43)	Acc@5  89.06 ( 88.41)
Epoch: [23][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3350e+00 (1.3603e+00)	Acc@1  61.72 ( 61.37)	Acc@5  88.28 ( 88.42)
Epoch: [23][280/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6504e+00 (1.3624e+00)	Acc@1  54.69 ( 61.28)	Acc@5  83.59 ( 88.37)
Epoch: [23][290/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2725e+00 (1.3619e+00)	Acc@1  61.72 ( 61.31)	Acc@5  89.84 ( 88.38)
Epoch: [23][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3691e+00 (1.3647e+00)	Acc@1  61.72 ( 61.23)	Acc@5  89.84 ( 88.34)
Epoch: [23][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3770e+00 (1.3664e+00)	Acc@1  56.25 ( 61.15)	Acc@5  89.06 ( 88.30)
Epoch: [23][320/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5762e+00 (1.3671e+00)	Acc@1  52.34 ( 61.13)	Acc@5  85.94 ( 88.29)
Epoch: [23][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5039e+00 (1.3680e+00)	Acc@1  57.81 ( 61.13)	Acc@5  86.72 ( 88.33)
Epoch: [23][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2227e+00 (1.3657e+00)	Acc@1  63.28 ( 61.18)	Acc@5  89.06 ( 88.32)
Epoch: [23][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5645e+00 (1.3680e+00)	Acc@1  57.03 ( 61.14)	Acc@5  86.72 ( 88.30)
Epoch: [23][360/391]	Time  0.179 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4824e+00 (1.3668e+00)	Acc@1  53.91 ( 61.16)	Acc@5  90.62 ( 88.32)
Epoch: [23][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1807e+00 (1.3675e+00)	Acc@1  67.97 ( 61.12)	Acc@5  91.41 ( 88.30)
Epoch: [23][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4551e+00 (1.3687e+00)	Acc@1  65.62 ( 61.14)	Acc@5  85.94 ( 88.27)
Epoch: [23][390/391]	Time  0.118 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.3994e+00 (1.3699e+00)	Acc@1  61.25 ( 61.12)	Acc@5  83.75 ( 88.27)
## e[23] optimizer.zero_grad (sum) time: 0.5638217926025391
## e[23]       loss.backward (sum) time: 12.572675943374634
## e[23]      optimizer.step (sum) time: 25.36154580116272
## epoch[23] training(only) time: 63.23516607284546
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.5010e+00 (1.5010e+00)	Acc@1  63.00 ( 63.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.5596e+00 (1.5847e+00)	Acc@1  59.00 ( 58.45)	Acc@5  87.00 ( 83.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.3789e+00 (1.5665e+00)	Acc@1  58.00 ( 57.86)	Acc@5  93.00 ( 85.00)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.6934e+00 (1.5898e+00)	Acc@1  51.00 ( 56.87)	Acc@5  82.00 ( 84.77)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.7266e+00 (1.5847e+00)	Acc@1  59.00 ( 57.02)	Acc@5  82.00 ( 84.80)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.6738e+00 (1.5877e+00)	Acc@1  58.00 ( 57.04)	Acc@5  80.00 ( 84.67)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.6445e+00 (1.5773e+00)	Acc@1  54.00 ( 57.07)	Acc@5  88.00 ( 84.82)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.6914e+00 (1.5840e+00)	Acc@1  50.00 ( 56.75)	Acc@5  86.00 ( 84.76)
Test: [ 80/100]	Time  0.057 ( 0.049)	Loss 1.7793e+00 (1.5940e+00)	Acc@1  53.00 ( 56.42)	Acc@5  82.00 ( 84.65)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.7627e+00 (1.5848e+00)	Acc@1  58.00 ( 56.82)	Acc@5  81.00 ( 84.76)
 * Acc@1 56.920 Acc@5 85.000
### epoch[23] execution time: 68.1336019039154
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.346 ( 0.346)	Data  0.170 ( 0.170)	Loss 1.4590e+00 (1.4590e+00)	Acc@1  57.03 ( 57.03)	Acc@5  87.50 ( 87.50)
Epoch: [24][ 10/391]	Time  0.160 ( 0.178)	Data  0.001 ( 0.016)	Loss 1.4268e+00 (1.3271e+00)	Acc@1  61.72 ( 62.07)	Acc@5  88.28 ( 88.42)
Epoch: [24][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.009)	Loss 1.1562e+00 (1.2758e+00)	Acc@1  67.97 ( 63.39)	Acc@5  92.19 ( 89.14)
Epoch: [24][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.1689e+00 (1.2913e+00)	Acc@1  65.62 ( 62.88)	Acc@5  90.62 ( 88.91)
Epoch: [24][ 40/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.2246e+00 (1.2774e+00)	Acc@1  67.97 ( 63.40)	Acc@5  89.06 ( 89.42)
Epoch: [24][ 50/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3789e+00 (1.2670e+00)	Acc@1  64.06 ( 63.91)	Acc@5  88.28 ( 89.57)
Epoch: [24][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3877e+00 (1.2695e+00)	Acc@1  57.81 ( 63.58)	Acc@5  86.72 ( 89.59)
Epoch: [24][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3164e+00 (1.2685e+00)	Acc@1  60.94 ( 63.72)	Acc@5  85.94 ( 89.57)
Epoch: [24][ 80/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1924e+00 (1.2657e+00)	Acc@1  61.72 ( 63.91)	Acc@5  90.62 ( 89.56)
Epoch: [24][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2871e+00 (1.2610e+00)	Acc@1  70.31 ( 64.17)	Acc@5  87.50 ( 89.59)
Epoch: [24][100/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2607e+00 (1.2616e+00)	Acc@1  59.38 ( 64.16)	Acc@5  89.84 ( 89.67)
Epoch: [24][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1904e+00 (1.2677e+00)	Acc@1  68.75 ( 63.98)	Acc@5  88.28 ( 89.48)
Epoch: [24][120/391]	Time  0.175 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4463e+00 (1.2747e+00)	Acc@1  58.59 ( 63.65)	Acc@5  88.28 ( 89.43)
Epoch: [24][130/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3564e+00 (1.2764e+00)	Acc@1  60.94 ( 63.60)	Acc@5  89.84 ( 89.43)
Epoch: [24][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1650e+00 (1.2800e+00)	Acc@1  67.19 ( 63.59)	Acc@5  89.84 ( 89.44)
Epoch: [24][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5107e+00 (1.2818e+00)	Acc@1  57.03 ( 63.53)	Acc@5  86.72 ( 89.48)
Epoch: [24][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2246e+00 (1.2849e+00)	Acc@1  66.41 ( 63.49)	Acc@5  89.84 ( 89.46)
Epoch: [24][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3037e+00 (1.2832e+00)	Acc@1  61.72 ( 63.56)	Acc@5  89.06 ( 89.56)
Epoch: [24][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2852e+00 (1.2861e+00)	Acc@1  62.50 ( 63.49)	Acc@5  89.06 ( 89.55)
Epoch: [24][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1611e+00 (1.2871e+00)	Acc@1  69.53 ( 63.47)	Acc@5  89.84 ( 89.57)
Epoch: [24][200/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2520e+00 (1.2896e+00)	Acc@1  61.72 ( 63.36)	Acc@5  89.06 ( 89.54)
Epoch: [24][210/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3008e+00 (1.2917e+00)	Acc@1  66.41 ( 63.29)	Acc@5  86.72 ( 89.52)
Epoch: [24][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4600e+00 (1.2944e+00)	Acc@1  64.06 ( 63.21)	Acc@5  83.59 ( 89.48)
Epoch: [24][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3623e+00 (1.2943e+00)	Acc@1  60.94 ( 63.16)	Acc@5  92.19 ( 89.49)
Epoch: [24][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4336e+00 (1.2948e+00)	Acc@1  58.59 ( 63.12)	Acc@5  87.50 ( 89.46)
Epoch: [24][250/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3008e+00 (1.2945e+00)	Acc@1  64.84 ( 63.13)	Acc@5  89.84 ( 89.45)
Epoch: [24][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3828e+00 (1.2944e+00)	Acc@1  67.97 ( 63.12)	Acc@5  87.50 ( 89.45)
Epoch: [24][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3848e+00 (1.2961e+00)	Acc@1  53.12 ( 63.04)	Acc@5  91.41 ( 89.43)
Epoch: [24][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1836e+00 (1.2975e+00)	Acc@1  67.97 ( 63.03)	Acc@5  90.62 ( 89.41)
Epoch: [24][290/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2412e+00 (1.2976e+00)	Acc@1  64.84 ( 63.03)	Acc@5  89.84 ( 89.39)
Epoch: [24][300/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4307e+00 (1.2973e+00)	Acc@1  59.38 ( 63.03)	Acc@5  87.50 ( 89.39)
Epoch: [24][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2539e+00 (1.2993e+00)	Acc@1  63.28 ( 62.95)	Acc@5  91.41 ( 89.35)
Epoch: [24][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3916e+00 (1.2979e+00)	Acc@1  60.16 ( 62.97)	Acc@5  85.16 ( 89.33)
Epoch: [24][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0938e+00 (1.2981e+00)	Acc@1  70.31 ( 62.94)	Acc@5  93.75 ( 89.33)
Epoch: [24][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3174e+00 (1.2999e+00)	Acc@1  57.81 ( 62.93)	Acc@5  89.06 ( 89.31)
Epoch: [24][350/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2725e+00 (1.2991e+00)	Acc@1  60.16 ( 62.95)	Acc@5  92.19 ( 89.32)
Epoch: [24][360/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2822e+00 (1.3008e+00)	Acc@1  67.97 ( 62.94)	Acc@5  89.06 ( 89.28)
Epoch: [24][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4258e+00 (1.3030e+00)	Acc@1  60.16 ( 62.87)	Acc@5  87.50 ( 89.27)
Epoch: [24][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4043e+00 (1.3042e+00)	Acc@1  60.16 ( 62.83)	Acc@5  86.72 ( 89.25)
Epoch: [24][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2803e+00 (1.3051e+00)	Acc@1  65.00 ( 62.82)	Acc@5  90.00 ( 89.24)
## e[24] optimizer.zero_grad (sum) time: 0.5563092231750488
## e[24]       loss.backward (sum) time: 12.539865255355835
## e[24]      optimizer.step (sum) time: 25.374544858932495
## epoch[24] training(only) time: 63.40223431587219
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 1.6357e+00 (1.6357e+00)	Acc@1  54.00 ( 54.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.8359e+00 (1.7025e+00)	Acc@1  49.00 ( 56.91)	Acc@5  89.00 ( 83.73)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.4580e+00 (1.6294e+00)	Acc@1  61.00 ( 57.86)	Acc@5  88.00 ( 84.33)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.6240e+00 (1.6507e+00)	Acc@1  49.00 ( 56.52)	Acc@5  87.00 ( 84.06)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.5459e+00 (1.6489e+00)	Acc@1  59.00 ( 56.27)	Acc@5  87.00 ( 84.02)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 1.5938e+00 (1.6666e+00)	Acc@1  61.00 ( 56.06)	Acc@5  80.00 ( 83.76)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.7090e+00 (1.6575e+00)	Acc@1  50.00 ( 56.25)	Acc@5  89.00 ( 83.95)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.6660e+00 (1.6455e+00)	Acc@1  55.00 ( 56.48)	Acc@5  80.00 ( 84.00)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6377e+00 (1.6414e+00)	Acc@1  57.00 ( 56.46)	Acc@5  85.00 ( 84.10)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.8105e+00 (1.6324e+00)	Acc@1  57.00 ( 56.63)	Acc@5  81.00 ( 84.30)
 * Acc@1 56.870 Acc@5 84.430
### epoch[24] execution time: 68.33746981620789
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.318 ( 0.318)	Data  0.144 ( 0.144)	Loss 1.1611e+00 (1.1611e+00)	Acc@1  66.41 ( 66.41)	Acc@5  90.62 ( 90.62)
Epoch: [25][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.2344e+00 (1.2098e+00)	Acc@1  63.28 ( 65.62)	Acc@5  92.19 ( 91.41)
Epoch: [25][ 20/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.2334e+00 (1.1981e+00)	Acc@1  62.50 ( 65.44)	Acc@5  91.41 ( 91.44)
Epoch: [25][ 30/391]	Time  0.169 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.2070e+00 (1.2024e+00)	Acc@1  61.72 ( 65.40)	Acc@5  95.31 ( 91.20)
Epoch: [25][ 40/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4316e+00 (1.2056e+00)	Acc@1  64.06 ( 65.34)	Acc@5  85.16 ( 90.91)
Epoch: [25][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.1650e+00 (1.2063e+00)	Acc@1  64.06 ( 65.43)	Acc@5  92.19 ( 90.95)
Epoch: [25][ 60/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.8047e-01 (1.2117e+00)	Acc@1  73.44 ( 65.52)	Acc@5  93.75 ( 90.74)
Epoch: [25][ 70/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0732e+00 (1.2105e+00)	Acc@1  64.84 ( 65.33)	Acc@5  92.97 ( 90.82)
Epoch: [25][ 80/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2041e+00 (1.2106e+00)	Acc@1  66.41 ( 65.43)	Acc@5  92.19 ( 90.83)
Epoch: [25][ 90/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3066e+00 (1.2035e+00)	Acc@1  62.50 ( 65.53)	Acc@5  86.72 ( 90.87)
Epoch: [25][100/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0322e+00 (1.2058e+00)	Acc@1  67.19 ( 65.35)	Acc@5  89.84 ( 90.90)
Epoch: [25][110/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1807e+00 (1.2083e+00)	Acc@1  68.75 ( 65.34)	Acc@5  90.62 ( 90.90)
Epoch: [25][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1875e+00 (1.2146e+00)	Acc@1  67.19 ( 65.17)	Acc@5  92.97 ( 90.81)
Epoch: [25][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1660e+00 (1.2170e+00)	Acc@1  68.75 ( 65.04)	Acc@5  89.84 ( 90.72)
Epoch: [25][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2031e+00 (1.2191e+00)	Acc@1  68.75 ( 65.02)	Acc@5  89.06 ( 90.62)
Epoch: [25][150/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1904e+00 (1.2219e+00)	Acc@1  63.28 ( 64.90)	Acc@5  92.19 ( 90.59)
Epoch: [25][160/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1172e+00 (1.2165e+00)	Acc@1  67.97 ( 65.07)	Acc@5  89.84 ( 90.62)
Epoch: [25][170/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2119e+00 (1.2197e+00)	Acc@1  64.84 ( 65.01)	Acc@5  90.62 ( 90.55)
Epoch: [25][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0127e+00 (1.2192e+00)	Acc@1  71.88 ( 65.03)	Acc@5  94.53 ( 90.53)
Epoch: [25][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1191e+00 (1.2224e+00)	Acc@1  67.19 ( 64.98)	Acc@5  94.53 ( 90.47)
Epoch: [25][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9951e-01 (1.2275e+00)	Acc@1  66.41 ( 64.90)	Acc@5  93.75 ( 90.42)
Epoch: [25][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1709e+00 (1.2294e+00)	Acc@1  66.41 ( 64.89)	Acc@5  91.41 ( 90.40)
Epoch: [25][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3271e+00 (1.2314e+00)	Acc@1  64.84 ( 64.74)	Acc@5  86.72 ( 90.41)
Epoch: [25][230/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 1.3154e+00 (1.2307e+00)	Acc@1  63.28 ( 64.75)	Acc@5  90.62 ( 90.41)
Epoch: [25][240/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 1.3457e+00 (1.2336e+00)	Acc@1  61.72 ( 64.61)	Acc@5  87.50 ( 90.34)
Epoch: [25][250/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.002)	Loss 1.2881e+00 (1.2397e+00)	Acc@1  65.62 ( 64.48)	Acc@5  88.28 ( 90.24)
Epoch: [25][260/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 1.2793e+00 (1.2396e+00)	Acc@1  62.50 ( 64.46)	Acc@5  89.06 ( 90.23)
Epoch: [25][270/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 9.5557e-01 (1.2399e+00)	Acc@1  74.22 ( 64.49)	Acc@5  91.41 ( 90.20)
Epoch: [25][280/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 1.4805e+00 (1.2445e+00)	Acc@1  54.69 ( 64.32)	Acc@5  89.06 ( 90.18)
Epoch: [25][290/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.002)	Loss 1.2646e+00 (1.2470e+00)	Acc@1  67.97 ( 64.25)	Acc@5  89.06 ( 90.13)
Epoch: [25][300/391]	Time  0.161 ( 0.161)	Data  0.001 ( 0.002)	Loss 1.1270e+00 (1.2485e+00)	Acc@1  71.88 ( 64.23)	Acc@5  89.84 ( 90.09)
Epoch: [25][310/391]	Time  0.158 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.2471e+00 (1.2478e+00)	Acc@1  63.28 ( 64.25)	Acc@5  89.84 ( 90.11)
Epoch: [25][320/391]	Time  0.162 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.3809e+00 (1.2482e+00)	Acc@1  61.72 ( 64.24)	Acc@5  87.50 ( 90.08)
Epoch: [25][330/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.3906e+00 (1.2499e+00)	Acc@1  57.81 ( 64.13)	Acc@5  92.19 ( 90.05)
Epoch: [25][340/391]	Time  0.159 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.3496e+00 (1.2497e+00)	Acc@1  63.28 ( 64.17)	Acc@5  89.84 ( 90.05)
Epoch: [25][350/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.1514e+00 (1.2497e+00)	Acc@1  67.19 ( 64.13)	Acc@5  91.41 ( 90.06)
Epoch: [25][360/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.3643e+00 (1.2506e+00)	Acc@1  64.84 ( 64.13)	Acc@5  86.72 ( 90.05)
Epoch: [25][370/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.2549e+00 (1.2528e+00)	Acc@1  60.16 ( 64.05)	Acc@5  92.19 ( 90.01)
Epoch: [25][380/391]	Time  0.160 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.0352e+00 (1.2526e+00)	Acc@1  70.31 ( 64.06)	Acc@5  92.19 ( 89.99)
Epoch: [25][390/391]	Time  0.116 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.4170e+00 (1.2533e+00)	Acc@1  60.00 ( 64.01)	Acc@5  86.25 ( 89.97)
## e[25] optimizer.zero_grad (sum) time: 0.5635719299316406
## e[25]       loss.backward (sum) time: 12.545544862747192
## e[25]      optimizer.step (sum) time: 25.384888887405396
## epoch[25] training(only) time: 63.10188865661621
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.5947e+00 (1.5947e+00)	Acc@1  61.00 ( 61.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.7236e+00 (1.6617e+00)	Acc@1  50.00 ( 56.82)	Acc@5  85.00 ( 84.09)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.3047e+00 (1.5689e+00)	Acc@1  63.00 ( 58.10)	Acc@5  88.00 ( 85.67)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5664e+00 (1.5904e+00)	Acc@1  51.00 ( 57.74)	Acc@5  88.00 ( 85.23)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.6533e+00 (1.5687e+00)	Acc@1  59.00 ( 57.80)	Acc@5  83.00 ( 85.63)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4951e+00 (1.5824e+00)	Acc@1  62.00 ( 57.71)	Acc@5  80.00 ( 85.37)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.5430e+00 (1.5707e+00)	Acc@1  58.00 ( 57.87)	Acc@5  87.00 ( 85.43)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.8369e+00 (1.5676e+00)	Acc@1  51.00 ( 57.76)	Acc@5  82.00 ( 85.37)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.7695e+00 (1.5676e+00)	Acc@1  55.00 ( 57.84)	Acc@5  82.00 ( 85.32)
Test: [ 90/100]	Time  0.056 ( 0.048)	Loss 1.7910e+00 (1.5581e+00)	Acc@1  53.00 ( 57.98)	Acc@5  82.00 ( 85.51)
 * Acc@1 58.260 Acc@5 85.430
### epoch[25] execution time: 68.00520491600037
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.342 ( 0.342)	Data  0.173 ( 0.173)	Loss 1.1113e+00 (1.1113e+00)	Acc@1  67.19 ( 67.19)	Acc@5  94.53 ( 94.53)
Epoch: [26][ 10/391]	Time  0.159 ( 0.178)	Data  0.001 ( 0.016)	Loss 1.2930e+00 (1.1111e+00)	Acc@1  64.84 ( 67.33)	Acc@5  89.06 ( 91.97)
Epoch: [26][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.009)	Loss 1.2383e+00 (1.1074e+00)	Acc@1  62.50 ( 66.78)	Acc@5  92.19 ( 92.49)
Epoch: [26][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.2559e+00 (1.1081e+00)	Acc@1  61.72 ( 66.76)	Acc@5  91.41 ( 92.52)
Epoch: [26][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 9.2529e-01 (1.1212e+00)	Acc@1  68.75 ( 66.35)	Acc@5  94.53 ( 92.34)
Epoch: [26][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.2383e+00 (1.1345e+00)	Acc@1  61.72 ( 66.08)	Acc@5  92.19 ( 92.13)
Epoch: [26][ 60/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.0742e+00 (1.1339e+00)	Acc@1  68.75 ( 66.42)	Acc@5  91.41 ( 92.15)
Epoch: [26][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1006e+00 (1.1466e+00)	Acc@1  66.41 ( 66.07)	Acc@5  90.62 ( 91.95)
Epoch: [26][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2666e+00 (1.1527e+00)	Acc@1  65.62 ( 65.89)	Acc@5  89.84 ( 91.75)
Epoch: [26][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3926e+00 (1.1590e+00)	Acc@1  63.28 ( 65.95)	Acc@5  89.84 ( 91.63)
Epoch: [26][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2812e+00 (1.1580e+00)	Acc@1  63.28 ( 66.04)	Acc@5  87.50 ( 91.52)
Epoch: [26][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0137e+00 (1.1652e+00)	Acc@1  64.84 ( 65.75)	Acc@5  96.09 ( 91.48)
Epoch: [26][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4521e+00 (1.1711e+00)	Acc@1  60.94 ( 65.81)	Acc@5  87.50 ( 91.32)
Epoch: [26][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3047e+00 (1.1739e+00)	Acc@1  65.62 ( 65.60)	Acc@5  90.62 ( 91.30)
Epoch: [26][140/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3047e+00 (1.1765e+00)	Acc@1  57.81 ( 65.44)	Acc@5  89.06 ( 91.26)
Epoch: [26][150/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4609e+00 (1.1798e+00)	Acc@1  58.59 ( 65.47)	Acc@5  90.62 ( 91.21)
Epoch: [26][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3760e+00 (1.1840e+00)	Acc@1  60.16 ( 65.34)	Acc@5  87.50 ( 91.16)
Epoch: [26][170/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2578e+00 (1.1885e+00)	Acc@1  62.50 ( 65.25)	Acc@5  94.53 ( 91.09)
Epoch: [26][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2715e+00 (1.1865e+00)	Acc@1  63.28 ( 65.36)	Acc@5  90.62 ( 91.08)
Epoch: [26][190/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0908e+00 (1.1834e+00)	Acc@1  68.75 ( 65.43)	Acc@5  90.62 ( 91.09)
Epoch: [26][200/391]	Time  0.179 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2490e+00 (1.1859e+00)	Acc@1  64.06 ( 65.31)	Acc@5  87.50 ( 91.03)
Epoch: [26][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2568e+00 (1.1886e+00)	Acc@1  65.62 ( 65.32)	Acc@5  89.06 ( 90.95)
Epoch: [26][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3525e+00 (1.1944e+00)	Acc@1  62.50 ( 65.23)	Acc@5  87.50 ( 90.88)
Epoch: [26][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3887e+00 (1.1974e+00)	Acc@1  58.59 ( 65.16)	Acc@5  87.50 ( 90.83)
Epoch: [26][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2881e+00 (1.1991e+00)	Acc@1  65.62 ( 65.13)	Acc@5  89.84 ( 90.79)
Epoch: [26][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0771e+00 (1.1986e+00)	Acc@1  67.97 ( 65.13)	Acc@5  91.41 ( 90.81)
Epoch: [26][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0869e+00 (1.2003e+00)	Acc@1  67.19 ( 65.11)	Acc@5  92.19 ( 90.76)
Epoch: [26][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0996e+00 (1.1984e+00)	Acc@1  66.41 ( 65.18)	Acc@5  92.97 ( 90.75)
Epoch: [26][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0752e+00 (1.2002e+00)	Acc@1  68.75 ( 65.11)	Acc@5  93.75 ( 90.76)
Epoch: [26][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2920e+00 (1.2042e+00)	Acc@1  67.97 ( 65.03)	Acc@5  89.84 ( 90.68)
Epoch: [26][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5059e+00 (1.2051e+00)	Acc@1  57.81 ( 64.96)	Acc@5  82.81 ( 90.66)
Epoch: [26][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2783e+00 (1.2042e+00)	Acc@1  63.28 ( 64.99)	Acc@5  88.28 ( 90.65)
Epoch: [26][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0205e+00 (1.2056e+00)	Acc@1  73.44 ( 64.98)	Acc@5  89.84 ( 90.62)
Epoch: [26][330/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4727e+00 (1.2049e+00)	Acc@1  60.94 ( 65.05)	Acc@5  87.50 ( 90.63)
Epoch: [26][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4990e+00 (1.2038e+00)	Acc@1  59.38 ( 65.10)	Acc@5  87.50 ( 90.63)
Epoch: [26][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0488e+00 (1.2031e+00)	Acc@1  68.75 ( 65.12)	Acc@5  89.06 ( 90.64)
Epoch: [26][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2031e+00 (1.2024e+00)	Acc@1  67.19 ( 65.12)	Acc@5  92.19 ( 90.66)
Epoch: [26][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4238e+00 (1.2042e+00)	Acc@1  56.25 ( 65.09)	Acc@5  85.94 ( 90.64)
Epoch: [26][380/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2178e+00 (1.2066e+00)	Acc@1  65.62 ( 65.04)	Acc@5  89.06 ( 90.63)
Epoch: [26][390/391]	Time  0.127 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2383e+00 (1.2068e+00)	Acc@1  65.00 ( 65.05)	Acc@5  86.25 ( 90.60)
## e[26] optimizer.zero_grad (sum) time: 0.562427282333374
## e[26]       loss.backward (sum) time: 12.593456029891968
## e[26]      optimizer.step (sum) time: 25.359187841415405
## epoch[26] training(only) time: 63.35763239860535
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.5537e+00 (1.5537e+00)	Acc@1  64.00 ( 64.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.046 ( 0.058)	Loss 1.5469e+00 (1.6734e+00)	Acc@1  58.00 ( 58.82)	Acc@5  86.00 ( 84.00)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.4395e+00 (1.6281e+00)	Acc@1  62.00 ( 58.29)	Acc@5  87.00 ( 85.24)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.9209e+00 (1.6557e+00)	Acc@1  54.00 ( 57.55)	Acc@5  81.00 ( 84.97)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.8281e+00 (1.6740e+00)	Acc@1  57.00 ( 57.00)	Acc@5  82.00 ( 84.90)
Test: [ 50/100]	Time  0.048 ( 0.049)	Loss 1.6855e+00 (1.6865e+00)	Acc@1  61.00 ( 57.08)	Acc@5  82.00 ( 84.73)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.7354e+00 (1.6753e+00)	Acc@1  53.00 ( 57.16)	Acc@5  82.00 ( 84.77)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.8496e+00 (1.6759e+00)	Acc@1  52.00 ( 57.10)	Acc@5  80.00 ( 84.72)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 1.7666e+00 (1.6751e+00)	Acc@1  54.00 ( 56.94)	Acc@5  84.00 ( 84.67)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.7031e+00 (1.6554e+00)	Acc@1  56.00 ( 57.35)	Acc@5  83.00 ( 84.78)
 * Acc@1 57.470 Acc@5 84.950
### epoch[26] execution time: 68.23696613311768
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.319 ( 0.319)	Data  0.145 ( 0.145)	Loss 1.0830e+00 (1.0830e+00)	Acc@1  67.97 ( 67.97)	Acc@5  91.41 ( 91.41)
Epoch: [27][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 9.4629e-01 (1.0886e+00)	Acc@1  72.66 ( 68.25)	Acc@5  93.75 ( 92.12)
Epoch: [27][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.0684e+00 (1.0708e+00)	Acc@1  64.84 ( 67.82)	Acc@5  92.19 ( 92.60)
Epoch: [27][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 9.2773e-01 (1.0745e+00)	Acc@1  68.75 ( 68.20)	Acc@5  94.53 ( 92.46)
Epoch: [27][ 40/391]	Time  0.168 ( 0.165)	Data  0.001 ( 0.005)	Loss 9.8096e-01 (1.0897e+00)	Acc@1  71.88 ( 67.82)	Acc@5  93.75 ( 92.26)
Epoch: [27][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.0684e+00 (1.0880e+00)	Acc@1  66.41 ( 67.85)	Acc@5  91.41 ( 92.37)
Epoch: [27][ 60/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.2266e+00 (1.0926e+00)	Acc@1  63.28 ( 67.74)	Acc@5  92.97 ( 92.32)
Epoch: [27][ 70/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 8.7402e-01 (1.0987e+00)	Acc@1  75.00 ( 67.73)	Acc@5  95.31 ( 92.15)
Epoch: [27][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0898e+00 (1.1063e+00)	Acc@1  71.09 ( 67.70)	Acc@5  92.19 ( 92.01)
Epoch: [27][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.8145e-01 (1.1047e+00)	Acc@1  69.53 ( 67.83)	Acc@5  94.53 ( 92.08)
Epoch: [27][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1631e+00 (1.1077e+00)	Acc@1  63.28 ( 67.73)	Acc@5  92.19 ( 92.08)
Epoch: [27][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2920e+00 (1.1151e+00)	Acc@1  62.50 ( 67.60)	Acc@5  88.28 ( 91.96)
Epoch: [27][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3369e+00 (1.1181e+00)	Acc@1  61.72 ( 67.47)	Acc@5  87.50 ( 91.85)
Epoch: [27][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0674e+00 (1.1227e+00)	Acc@1  75.00 ( 67.49)	Acc@5  94.53 ( 91.77)
Epoch: [27][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2773e+00 (1.1254e+00)	Acc@1  62.50 ( 67.27)	Acc@5  90.62 ( 91.75)
Epoch: [27][150/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1514e+00 (1.1292e+00)	Acc@1  63.28 ( 67.14)	Acc@5  92.97 ( 91.72)
Epoch: [27][160/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0791e+00 (1.1374e+00)	Acc@1  73.44 ( 67.02)	Acc@5  90.62 ( 91.60)
Epoch: [27][170/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1816e+00 (1.1365e+00)	Acc@1  68.75 ( 67.11)	Acc@5  87.50 ( 91.58)
Epoch: [27][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0234e+00 (1.1375e+00)	Acc@1  71.88 ( 67.08)	Acc@5  96.88 ( 91.58)
Epoch: [27][190/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0986e+00 (1.1391e+00)	Acc@1  64.06 ( 67.01)	Acc@5  93.75 ( 91.55)
Epoch: [27][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0947e+00 (1.1396e+00)	Acc@1  68.75 ( 66.95)	Acc@5  92.19 ( 91.57)
Epoch: [27][210/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2432e+00 (1.1432e+00)	Acc@1  66.41 ( 66.89)	Acc@5  91.41 ( 91.52)
Epoch: [27][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1865e+00 (1.1468e+00)	Acc@1  58.59 ( 66.73)	Acc@5  93.75 ( 91.47)
Epoch: [27][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.6338e-01 (1.1471e+00)	Acc@1  71.09 ( 66.72)	Acc@5  93.75 ( 91.45)
Epoch: [27][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0283e+00 (1.1488e+00)	Acc@1  75.00 ( 66.75)	Acc@5  89.06 ( 91.40)
Epoch: [27][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1152e+00 (1.1490e+00)	Acc@1  67.97 ( 66.68)	Acc@5  89.06 ( 91.43)
Epoch: [27][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.7998e-01 (1.1493e+00)	Acc@1  71.88 ( 66.67)	Acc@5  94.53 ( 91.43)
Epoch: [27][270/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1396e+00 (1.1514e+00)	Acc@1  65.62 ( 66.60)	Acc@5  92.19 ( 91.38)
Epoch: [27][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1016e+00 (1.1526e+00)	Acc@1  64.06 ( 66.55)	Acc@5  96.09 ( 91.39)
Epoch: [27][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2285e+00 (1.1569e+00)	Acc@1  60.94 ( 66.48)	Acc@5  92.19 ( 91.33)
Epoch: [27][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3818e+00 (1.1554e+00)	Acc@1  60.94 ( 66.51)	Acc@5  89.84 ( 91.40)
Epoch: [27][310/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3740e+00 (1.1590e+00)	Acc@1  63.28 ( 66.43)	Acc@5  85.16 ( 91.36)
Epoch: [27][320/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2871e+00 (1.1592e+00)	Acc@1  64.06 ( 66.39)	Acc@5  89.06 ( 91.35)
Epoch: [27][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1553e+00 (1.1598e+00)	Acc@1  64.84 ( 66.40)	Acc@5  91.41 ( 91.33)
Epoch: [27][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.2109e+00 (1.1614e+00)	Acc@1  66.41 ( 66.36)	Acc@5  92.19 ( 91.28)
Epoch: [27][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2354e+00 (1.1627e+00)	Acc@1  62.50 ( 66.32)	Acc@5  90.62 ( 91.25)
Epoch: [27][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.7363e-01 (1.1627e+00)	Acc@1  71.88 ( 66.31)	Acc@5  96.09 ( 91.24)
Epoch: [27][370/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1631e+00 (1.1649e+00)	Acc@1  69.53 ( 66.24)	Acc@5  89.84 ( 91.21)
Epoch: [27][380/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.1631e+00 (1.1627e+00)	Acc@1  67.19 ( 66.30)	Acc@5  91.41 ( 91.22)
Epoch: [27][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1201e+00 (1.1637e+00)	Acc@1  61.25 ( 66.26)	Acc@5  95.00 ( 91.20)
## e[27] optimizer.zero_grad (sum) time: 0.5633218288421631
## e[27]       loss.backward (sum) time: 12.586502075195312
## e[27]      optimizer.step (sum) time: 25.359667539596558
## epoch[27] training(only) time: 63.55323648452759
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.2705e+00 (1.2705e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.046 ( 0.058)	Loss 1.5303e+00 (1.4537e+00)	Acc@1  62.00 ( 62.55)	Acc@5  88.00 ( 86.00)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.1641e+00 (1.4315e+00)	Acc@1  66.00 ( 61.71)	Acc@5  93.00 ( 86.86)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.6533e+00 (1.4691e+00)	Acc@1  52.00 ( 60.97)	Acc@5  88.00 ( 86.45)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.4648e+00 (1.4742e+00)	Acc@1  63.00 ( 60.46)	Acc@5  88.00 ( 86.73)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 1.3711e+00 (1.4784e+00)	Acc@1  63.00 ( 60.51)	Acc@5  85.00 ( 86.47)
Test: [ 60/100]	Time  0.057 ( 0.049)	Loss 1.4336e+00 (1.4828e+00)	Acc@1  59.00 ( 60.20)	Acc@5  85.00 ( 86.08)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.5342e+00 (1.4878e+00)	Acc@1  57.00 ( 60.24)	Acc@5  83.00 ( 85.96)
Test: [ 80/100]	Time  0.047 ( 0.048)	Loss 1.7090e+00 (1.4934e+00)	Acc@1  56.00 ( 60.14)	Acc@5  81.00 ( 85.90)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.7832e+00 (1.4849e+00)	Acc@1  54.00 ( 60.27)	Acc@5  83.00 ( 86.07)
 * Acc@1 60.420 Acc@5 86.160
### epoch[27] execution time: 68.4421718120575
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.339 ( 0.339)	Data  0.174 ( 0.174)	Loss 9.7021e-01 (9.7021e-01)	Acc@1  75.00 ( 75.00)	Acc@5  92.19 ( 92.19)
Epoch: [28][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.017)	Loss 1.1836e+00 (9.9960e-01)	Acc@1  70.31 ( 72.73)	Acc@5  87.50 ( 92.83)
Epoch: [28][ 20/391]	Time  0.159 ( 0.170)	Data  0.001 ( 0.009)	Loss 1.0449e+00 (1.0101e+00)	Acc@1  69.53 ( 71.32)	Acc@5  88.28 ( 92.93)
Epoch: [28][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.0654e+00 (9.9701e-01)	Acc@1  66.41 ( 71.52)	Acc@5  94.53 ( 93.17)
Epoch: [28][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 9.4727e-01 (1.0026e+00)	Acc@1  72.66 ( 71.23)	Acc@5  92.97 ( 93.14)
Epoch: [28][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.0654e+00 (1.0083e+00)	Acc@1  66.41 ( 70.89)	Acc@5  92.97 ( 93.06)
Epoch: [28][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.0059e+00 (1.0206e+00)	Acc@1  69.53 ( 70.36)	Acc@5  96.09 ( 93.10)
Epoch: [28][ 70/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2549e+00 (1.0223e+00)	Acc@1  65.62 ( 70.17)	Acc@5  87.50 ( 93.10)
Epoch: [28][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1611e+00 (1.0304e+00)	Acc@1  63.28 ( 70.02)	Acc@5  92.97 ( 92.91)
Epoch: [28][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2324e+00 (1.0394e+00)	Acc@1  60.94 ( 69.63)	Acc@5  91.41 ( 92.82)
Epoch: [28][100/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.003)	Loss 1.1719e+00 (1.0482e+00)	Acc@1  69.53 ( 69.48)	Acc@5  89.06 ( 92.62)
Epoch: [28][110/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0400e+00 (1.0516e+00)	Acc@1  67.97 ( 69.36)	Acc@5  92.97 ( 92.57)
Epoch: [28][120/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2412e+00 (1.0590e+00)	Acc@1  67.19 ( 69.19)	Acc@5  87.50 ( 92.42)
Epoch: [28][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.8926e-01 (1.0643e+00)	Acc@1  71.09 ( 69.10)	Acc@5  93.75 ( 92.33)
Epoch: [28][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0469e+00 (1.0660e+00)	Acc@1  71.88 ( 68.93)	Acc@5  91.41 ( 92.37)
Epoch: [28][150/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0596e+00 (1.0674e+00)	Acc@1  69.53 ( 68.83)	Acc@5  92.97 ( 92.36)
Epoch: [28][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1934e+00 (1.0689e+00)	Acc@1  65.62 ( 68.73)	Acc@5  90.62 ( 92.30)
Epoch: [28][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.7217e-01 (1.0704e+00)	Acc@1  72.66 ( 68.64)	Acc@5  89.84 ( 92.28)
Epoch: [28][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0283e+00 (1.0728e+00)	Acc@1  68.75 ( 68.53)	Acc@5  94.53 ( 92.23)
Epoch: [28][190/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0576e+00 (1.0726e+00)	Acc@1  69.53 ( 68.64)	Acc@5  92.19 ( 92.24)
Epoch: [28][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.3555e-01 (1.0751e+00)	Acc@1  73.44 ( 68.57)	Acc@5  93.75 ( 92.17)
Epoch: [28][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.6865e-01 (1.0762e+00)	Acc@1  71.09 ( 68.49)	Acc@5  93.75 ( 92.15)
Epoch: [28][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.6865e-01 (1.0797e+00)	Acc@1  73.44 ( 68.43)	Acc@5  95.31 ( 92.11)
Epoch: [28][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1338e+00 (1.0812e+00)	Acc@1  66.41 ( 68.43)	Acc@5  91.41 ( 92.09)
Epoch: [28][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3223e+00 (1.0839e+00)	Acc@1  63.28 ( 68.31)	Acc@5  91.41 ( 92.08)
Epoch: [28][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2061e+00 (1.0859e+00)	Acc@1  64.84 ( 68.27)	Acc@5  90.62 ( 92.10)
Epoch: [28][260/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1523e+00 (1.0867e+00)	Acc@1  66.41 ( 68.24)	Acc@5  89.84 ( 92.05)
Epoch: [28][270/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0449e+00 (1.0929e+00)	Acc@1  69.53 ( 68.07)	Acc@5  94.53 ( 91.98)
Epoch: [28][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2305e+00 (1.0971e+00)	Acc@1  64.06 ( 67.98)	Acc@5  86.72 ( 91.90)
Epoch: [28][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1426e+00 (1.1003e+00)	Acc@1  68.75 ( 67.90)	Acc@5  89.06 ( 91.86)
Epoch: [28][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.9121e-01 (1.1014e+00)	Acc@1  72.66 ( 67.90)	Acc@5  93.75 ( 91.83)
Epoch: [28][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1650e+00 (1.1046e+00)	Acc@1  66.41 ( 67.78)	Acc@5  88.28 ( 91.80)
Epoch: [28][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0049e+00 (1.1049e+00)	Acc@1  70.31 ( 67.76)	Acc@5  95.31 ( 91.81)
Epoch: [28][330/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2266e+00 (1.1066e+00)	Acc@1  67.97 ( 67.70)	Acc@5  86.72 ( 91.80)
Epoch: [28][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.7705e-01 (1.1061e+00)	Acc@1  70.31 ( 67.71)	Acc@5  93.75 ( 91.80)
Epoch: [28][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1016e+00 (1.1084e+00)	Acc@1  73.44 ( 67.67)	Acc@5  90.62 ( 91.78)
Epoch: [28][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0479e+00 (1.1080e+00)	Acc@1  67.97 ( 67.69)	Acc@5  93.75 ( 91.78)
Epoch: [28][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2139e+00 (1.1099e+00)	Acc@1  69.53 ( 67.64)	Acc@5  87.50 ( 91.74)
Epoch: [28][380/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2246e+00 (1.1119e+00)	Acc@1  61.72 ( 67.59)	Acc@5  88.28 ( 91.73)
Epoch: [28][390/391]	Time  0.114 ( 0.161)	Data  0.001 ( 0.001)	Loss 1.1895e+00 (1.1127e+00)	Acc@1  60.00 ( 67.58)	Acc@5  95.00 ( 91.71)
## e[28] optimizer.zero_grad (sum) time: 0.5682754516601562
## e[28]       loss.backward (sum) time: 12.514925479888916
## e[28]      optimizer.step (sum) time: 25.408610582351685
## epoch[28] training(only) time: 63.18205809593201
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.4297e+00 (1.4297e+00)	Acc@1  64.00 ( 64.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.5166e+00 (1.4935e+00)	Acc@1  55.00 ( 62.55)	Acc@5  89.00 ( 86.64)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.2744e+00 (1.4699e+00)	Acc@1  67.00 ( 62.29)	Acc@5  90.00 ( 87.90)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.6650e+00 (1.5107e+00)	Acc@1  53.00 ( 61.00)	Acc@5  84.00 ( 86.94)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.5811e+00 (1.5163e+00)	Acc@1  62.00 ( 60.66)	Acc@5  86.00 ( 87.17)
Test: [ 50/100]	Time  0.047 ( 0.049)	Loss 1.5557e+00 (1.5272e+00)	Acc@1  64.00 ( 60.84)	Acc@5  83.00 ( 86.67)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.4072e+00 (1.5173e+00)	Acc@1  63.00 ( 60.79)	Acc@5  91.00 ( 86.70)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.7363e+00 (1.5139e+00)	Acc@1  54.00 ( 60.70)	Acc@5  86.00 ( 86.59)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 1.6787e+00 (1.5216e+00)	Acc@1  59.00 ( 60.43)	Acc@5  83.00 ( 86.44)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.7627e+00 (1.5131e+00)	Acc@1  48.00 ( 60.52)	Acc@5  85.00 ( 86.62)
 * Acc@1 60.560 Acc@5 86.620
### epoch[28] execution time: 68.08415985107422
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.331 ( 0.331)	Data  0.163 ( 0.163)	Loss 1.0361e+00 (1.0361e+00)	Acc@1  70.31 ( 70.31)	Acc@5  93.75 ( 93.75)
Epoch: [29][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.016)	Loss 9.7461e-01 (9.4371e-01)	Acc@1  71.88 ( 72.37)	Acc@5  93.75 ( 93.82)
Epoch: [29][ 20/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.009)	Loss 7.7637e-01 (9.4101e-01)	Acc@1  78.91 ( 72.14)	Acc@5  94.53 ( 93.86)
Epoch: [29][ 30/391]	Time  0.169 ( 0.167)	Data  0.001 ( 0.006)	Loss 9.6436e-01 (9.5023e-01)	Acc@1  67.97 ( 71.80)	Acc@5  95.31 ( 94.10)
Epoch: [29][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.0898e+00 (9.7381e-01)	Acc@1  69.53 ( 71.17)	Acc@5  94.53 ( 93.90)
Epoch: [29][ 50/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.3604e-01 (9.8323e-01)	Acc@1  73.44 ( 71.22)	Acc@5  92.97 ( 93.64)
Epoch: [29][ 60/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.0537e+00 (9.9563e-01)	Acc@1  72.66 ( 70.84)	Acc@5  92.97 ( 93.44)
Epoch: [29][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2100e+00 (9.9591e-01)	Acc@1  64.84 ( 70.96)	Acc@5  92.97 ( 93.51)
Epoch: [29][ 80/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2070e+00 (1.0053e+00)	Acc@1  65.62 ( 70.73)	Acc@5  93.75 ( 93.46)
Epoch: [29][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.8779e-01 (1.0107e+00)	Acc@1  70.31 ( 70.54)	Acc@5  92.97 ( 93.39)
Epoch: [29][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0107e+00 (1.0057e+00)	Acc@1  68.75 ( 70.64)	Acc@5  92.97 ( 93.46)
Epoch: [29][110/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1758e+00 (1.0084e+00)	Acc@1  64.84 ( 70.47)	Acc@5  90.62 ( 93.33)
Epoch: [29][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1338e+00 (1.0090e+00)	Acc@1  71.88 ( 70.43)	Acc@5  89.84 ( 93.34)
Epoch: [29][130/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3701e-01 (1.0067e+00)	Acc@1  77.34 ( 70.51)	Acc@5  94.53 ( 93.42)
Epoch: [29][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4482e-01 (1.0144e+00)	Acc@1  74.22 ( 70.29)	Acc@5  94.53 ( 93.38)
Epoch: [29][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1348e+00 (1.0212e+00)	Acc@1  70.31 ( 70.05)	Acc@5  90.62 ( 93.25)
Epoch: [29][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6826e-01 (1.0245e+00)	Acc@1  69.53 ( 69.97)	Acc@5  93.75 ( 93.23)
Epoch: [29][170/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0000e+00 (1.0268e+00)	Acc@1  69.53 ( 69.93)	Acc@5  92.19 ( 93.17)
Epoch: [29][180/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9512e-01 (1.0293e+00)	Acc@1  74.22 ( 69.93)	Acc@5  93.75 ( 93.13)
Epoch: [29][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1631e+00 (1.0334e+00)	Acc@1  64.84 ( 69.79)	Acc@5  92.19 ( 93.08)
Epoch: [29][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0332e+00 (1.0350e+00)	Acc@1  75.00 ( 69.75)	Acc@5  93.75 ( 93.07)
Epoch: [29][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0186e+00 (1.0387e+00)	Acc@1  71.88 ( 69.63)	Acc@5  92.97 ( 92.99)
Epoch: [29][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2900e+00 (1.0432e+00)	Acc@1  64.84 ( 69.51)	Acc@5  89.06 ( 92.93)
Epoch: [29][230/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0762e+00 (1.0451e+00)	Acc@1  67.19 ( 69.46)	Acc@5  92.19 ( 92.88)
Epoch: [29][240/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.9502e-01 (1.0451e+00)	Acc@1  68.75 ( 69.46)	Acc@5  97.66 ( 92.89)
Epoch: [29][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2109e+00 (1.0455e+00)	Acc@1  65.62 ( 69.39)	Acc@5  92.19 ( 92.88)
Epoch: [29][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2139e+00 (1.0468e+00)	Acc@1  65.62 ( 69.35)	Acc@5  92.19 ( 92.89)
Epoch: [29][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2041e+00 (1.0476e+00)	Acc@1  67.19 ( 69.34)	Acc@5  92.19 ( 92.86)
Epoch: [29][280/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0078e+00 (1.0488e+00)	Acc@1  70.31 ( 69.30)	Acc@5  92.97 ( 92.85)
Epoch: [29][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.7314e-01 (1.0520e+00)	Acc@1  71.09 ( 69.26)	Acc@5  93.75 ( 92.80)
Epoch: [29][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2559e+00 (1.0536e+00)	Acc@1  64.84 ( 69.22)	Acc@5  89.06 ( 92.77)
Epoch: [29][310/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.5547e-01 (1.0530e+00)	Acc@1  69.53 ( 69.18)	Acc@5  96.09 ( 92.79)
Epoch: [29][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2324e+00 (1.0568e+00)	Acc@1  67.97 ( 69.11)	Acc@5  89.06 ( 92.75)
Epoch: [29][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2305e+00 (1.0559e+00)	Acc@1  64.84 ( 69.16)	Acc@5  92.97 ( 92.80)
Epoch: [29][340/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.1113e-01 (1.0576e+00)	Acc@1  71.09 ( 69.12)	Acc@5  96.88 ( 92.77)
Epoch: [29][350/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1924e+00 (1.0594e+00)	Acc@1  63.28 ( 69.05)	Acc@5  91.41 ( 92.77)
Epoch: [29][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.1748e-01 (1.0630e+00)	Acc@1  77.34 ( 68.99)	Acc@5  95.31 ( 92.71)
Epoch: [29][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1533e+00 (1.0651e+00)	Acc@1  67.19 ( 68.94)	Acc@5  92.19 ( 92.69)
Epoch: [29][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1309e+00 (1.0676e+00)	Acc@1  66.41 ( 68.88)	Acc@5  91.41 ( 92.66)
Epoch: [29][390/391]	Time  0.113 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3291e+00 (1.0685e+00)	Acc@1  62.50 ( 68.87)	Acc@5  90.00 ( 92.66)
## e[29] optimizer.zero_grad (sum) time: 0.5707647800445557
## e[29]       loss.backward (sum) time: 12.5583975315094
## e[29]      optimizer.step (sum) time: 25.386586666107178
## epoch[29] training(only) time: 63.519185304641724
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.3682e+00 (1.3682e+00)	Acc@1  59.00 ( 59.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.7051e+00 (1.6291e+00)	Acc@1  52.00 ( 57.27)	Acc@5  89.00 ( 85.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.2158e+00 (1.5783e+00)	Acc@1  64.00 ( 58.33)	Acc@5  92.00 ( 86.29)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.6338e+00 (1.6007e+00)	Acc@1  53.00 ( 57.87)	Acc@5  86.00 ( 85.74)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 1.4648e+00 (1.5782e+00)	Acc@1  63.00 ( 58.46)	Acc@5  82.00 ( 85.83)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.4629e+00 (1.5861e+00)	Acc@1  60.00 ( 58.37)	Acc@5  86.00 ( 85.63)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.4785e+00 (1.5735e+00)	Acc@1  63.00 ( 58.79)	Acc@5  83.00 ( 85.54)
Test: [ 70/100]	Time  0.049 ( 0.049)	Loss 1.6562e+00 (1.5684e+00)	Acc@1  53.00 ( 58.92)	Acc@5  82.00 ( 85.45)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5908e+00 (1.5750e+00)	Acc@1  59.00 ( 58.93)	Acc@5  85.00 ( 85.30)
Test: [ 90/100]	Time  0.060 ( 0.049)	Loss 1.5586e+00 (1.5618e+00)	Acc@1  59.00 ( 59.22)	Acc@5  83.00 ( 85.48)
 * Acc@1 59.300 Acc@5 85.560
### epoch[29] execution time: 68.46874809265137
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.315 ( 0.315)	Data  0.140 ( 0.140)	Loss 1.0703e+00 (1.0703e+00)	Acc@1  72.66 ( 72.66)	Acc@5  89.84 ( 89.84)
Epoch: [30][ 10/391]	Time  0.160 ( 0.174)	Data  0.001 ( 0.014)	Loss 7.3096e-01 (9.2911e-01)	Acc@1  75.78 ( 73.08)	Acc@5  96.88 ( 94.60)
Epoch: [30][ 20/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.008)	Loss 8.5498e-01 (9.3706e-01)	Acc@1  77.34 ( 72.69)	Acc@5  96.09 ( 94.46)
Epoch: [30][ 30/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 8.8525e-01 (9.2232e-01)	Acc@1  75.00 ( 73.06)	Acc@5  96.88 ( 94.25)
Epoch: [30][ 40/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 7.6074e-01 (9.0263e-01)	Acc@1  73.44 ( 73.57)	Acc@5  96.09 ( 94.40)
Epoch: [30][ 50/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.004)	Loss 6.4990e-01 (8.9567e-01)	Acc@1  78.12 ( 73.56)	Acc@5  97.66 ( 94.58)
Epoch: [30][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.9189e-01 (8.7946e-01)	Acc@1  82.03 ( 74.12)	Acc@5  94.53 ( 94.74)
Epoch: [30][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.4824e-01 (8.7629e-01)	Acc@1  69.53 ( 74.09)	Acc@5  96.09 ( 94.78)
Epoch: [30][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.4775e-01 (8.7756e-01)	Acc@1  72.66 ( 73.94)	Acc@5  94.53 ( 94.75)
Epoch: [30][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 7.8809e-01 (8.6644e-01)	Acc@1  75.78 ( 74.40)	Acc@5  96.88 ( 94.81)
Epoch: [30][100/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8555e-01 (8.5468e-01)	Acc@1  80.47 ( 74.69)	Acc@5  96.88 ( 95.00)
Epoch: [30][110/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.1484e-01 (8.4974e-01)	Acc@1  82.81 ( 74.80)	Acc@5  95.31 ( 95.07)
Epoch: [30][120/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4160e-01 (8.4459e-01)	Acc@1  84.38 ( 74.98)	Acc@5  96.88 ( 95.14)
Epoch: [30][130/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.1934e-01 (8.4002e-01)	Acc@1  76.56 ( 75.14)	Acc@5  95.31 ( 95.20)
Epoch: [30][140/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.3291e-01 (8.3503e-01)	Acc@1  76.56 ( 75.30)	Acc@5  96.09 ( 95.28)
Epoch: [30][150/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.9844e-01 (8.2770e-01)	Acc@1  75.78 ( 75.49)	Acc@5  91.41 ( 95.35)
Epoch: [30][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.2080e-01 (8.2194e-01)	Acc@1  75.78 ( 75.63)	Acc@5  96.09 ( 95.44)
Epoch: [30][170/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.1494e-01 (8.2146e-01)	Acc@1  74.22 ( 75.62)	Acc@5  95.31 ( 95.40)
Epoch: [30][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.6572e-01 (8.1919e-01)	Acc@1  73.44 ( 75.71)	Acc@5  96.09 ( 95.43)
Epoch: [30][190/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.3984e-01 (8.1446e-01)	Acc@1  71.09 ( 75.79)	Acc@5  97.66 ( 95.49)
Epoch: [30][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.5820e-01 (8.0912e-01)	Acc@1  78.91 ( 76.00)	Acc@5 100.00 ( 95.54)
Epoch: [30][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8701e-01 (8.0553e-01)	Acc@1  81.25 ( 76.13)	Acc@5  96.88 ( 95.59)
Epoch: [30][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6943e-01 (8.0252e-01)	Acc@1  82.03 ( 76.23)	Acc@5  98.44 ( 95.62)
Epoch: [30][230/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.2002e-01 (8.0019e-01)	Acc@1  82.03 ( 76.27)	Acc@5  97.66 ( 95.62)
Epoch: [30][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.1699e-01 (7.9898e-01)	Acc@1  72.66 ( 76.29)	Acc@5  95.31 ( 95.63)
Epoch: [30][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.7764e-01 (7.9515e-01)	Acc@1  83.59 ( 76.42)	Acc@5  98.44 ( 95.68)
Epoch: [30][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.0068e-01 (7.9173e-01)	Acc@1  77.34 ( 76.50)	Acc@5  93.75 ( 95.68)
Epoch: [30][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.7871e-01 (7.9012e-01)	Acc@1  81.25 ( 76.58)	Acc@5  97.66 ( 95.70)
Epoch: [30][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2939e-01 (7.8601e-01)	Acc@1  78.12 ( 76.71)	Acc@5  96.88 ( 95.74)
Epoch: [30][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.3096e-01 (7.8393e-01)	Acc@1  80.47 ( 76.77)	Acc@5  97.66 ( 95.76)
Epoch: [30][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.2275e-01 (7.8245e-01)	Acc@1  75.78 ( 76.82)	Acc@5  94.53 ( 95.76)
Epoch: [30][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.5332e-01 (7.8039e-01)	Acc@1  81.25 ( 76.85)	Acc@5  96.09 ( 95.78)
Epoch: [30][320/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.0303e-01 (7.7843e-01)	Acc@1  82.81 ( 76.95)	Acc@5  96.88 ( 95.79)
Epoch: [30][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.2061e-01 (7.7719e-01)	Acc@1  86.72 ( 77.02)	Acc@5  97.66 ( 95.80)
Epoch: [30][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.0811e-01 (7.7565e-01)	Acc@1  79.69 ( 77.11)	Acc@5  93.75 ( 95.82)
Epoch: [30][350/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.8066e-01 (7.7393e-01)	Acc@1  78.91 ( 77.15)	Acc@5  95.31 ( 95.83)
Epoch: [30][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.4512e-01 (7.7028e-01)	Acc@1  75.78 ( 77.27)	Acc@5  95.31 ( 95.85)
Epoch: [30][370/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.6602e-01 (7.6833e-01)	Acc@1  75.00 ( 77.30)	Acc@5 100.00 ( 95.89)
Epoch: [30][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.0723e-01 (7.6706e-01)	Acc@1  75.00 ( 77.31)	Acc@5  92.19 ( 95.89)
Epoch: [30][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.8896e-01 (7.6549e-01)	Acc@1  80.00 ( 77.35)	Acc@5  98.75 ( 95.92)
## e[30] optimizer.zero_grad (sum) time: 0.5535945892333984
## e[30]       loss.backward (sum) time: 12.568103313446045
## e[30]      optimizer.step (sum) time: 25.387319564819336
## epoch[30] training(only) time: 63.25123357772827
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 1.0322e+00 (1.0322e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.1289e+00 (1.1489e+00)	Acc@1  70.00 ( 68.82)	Acc@5  92.00 ( 90.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 9.7021e-01 (1.1097e+00)	Acc@1  74.00 ( 70.05)	Acc@5  94.00 ( 91.33)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.2354e+00 (1.1346e+00)	Acc@1  60.00 ( 69.00)	Acc@5  93.00 ( 91.03)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0674e+00 (1.1271e+00)	Acc@1  71.00 ( 69.12)	Acc@5  88.00 ( 91.39)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.1240e+00 (1.1389e+00)	Acc@1  67.00 ( 68.86)	Acc@5  90.00 ( 91.02)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.0684e+00 (1.1252e+00)	Acc@1  67.00 ( 68.82)	Acc@5  93.00 ( 91.16)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.1729e+00 (1.1232e+00)	Acc@1  65.00 ( 68.99)	Acc@5  92.00 ( 91.03)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3027e+00 (1.1238e+00)	Acc@1  67.00 ( 69.10)	Acc@5  89.00 ( 91.06)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.2500e+00 (1.1146e+00)	Acc@1  61.00 ( 69.01)	Acc@5  90.00 ( 91.25)
 * Acc@1 69.040 Acc@5 91.310
### epoch[30] execution time: 68.18172574043274
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.329 ( 0.329)	Data  0.148 ( 0.148)	Loss 5.6787e-01 (5.6787e-01)	Acc@1  85.16 ( 85.16)	Acc@5  95.31 ( 95.31)
Epoch: [31][ 10/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.014)	Loss 6.3867e-01 (7.2186e-01)	Acc@1  82.81 ( 78.41)	Acc@5  98.44 ( 95.95)
Epoch: [31][ 20/391]	Time  0.168 ( 0.169)	Data  0.001 ( 0.008)	Loss 6.7285e-01 (6.9701e-01)	Acc@1  81.25 ( 79.17)	Acc@5  96.88 ( 96.50)
Epoch: [31][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 5.7715e-01 (6.9278e-01)	Acc@1  81.25 ( 79.69)	Acc@5  99.22 ( 96.57)
Epoch: [31][ 40/391]	Time  0.164 ( 0.165)	Data  0.001 ( 0.005)	Loss 6.4160e-01 (6.7694e-01)	Acc@1  84.38 ( 80.22)	Acc@5  94.53 ( 96.72)
Epoch: [31][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.3174e-01 (6.7773e-01)	Acc@1  85.16 ( 80.27)	Acc@5  96.88 ( 96.69)
Epoch: [31][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 5.8252e-01 (6.7807e-01)	Acc@1  84.38 ( 80.24)	Acc@5  96.88 ( 96.64)
Epoch: [31][ 70/391]	Time  0.168 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.4170e-01 (6.8156e-01)	Acc@1  75.00 ( 79.93)	Acc@5  96.88 ( 96.69)
Epoch: [31][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.4648e-01 (6.7489e-01)	Acc@1  82.81 ( 80.14)	Acc@5  94.53 ( 96.66)
Epoch: [31][ 90/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.5576e-01 (6.7582e-01)	Acc@1  79.69 ( 80.07)	Acc@5  97.66 ( 96.69)
Epoch: [31][100/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.7334e-01 (6.7041e-01)	Acc@1  79.69 ( 80.21)	Acc@5  96.88 ( 96.76)
Epoch: [31][110/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.3818e-01 (6.6878e-01)	Acc@1  82.03 ( 80.24)	Acc@5  96.09 ( 96.73)
Epoch: [31][120/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.4121e-01 (6.6757e-01)	Acc@1  79.69 ( 80.33)	Acc@5  96.09 ( 96.77)
Epoch: [31][130/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8232e-01 (6.6851e-01)	Acc@1  72.66 ( 80.27)	Acc@5  97.66 ( 96.78)
Epoch: [31][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3750e-01 (6.6708e-01)	Acc@1  88.28 ( 80.26)	Acc@5  99.22 ( 96.77)
Epoch: [31][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.8896e-01 (6.6403e-01)	Acc@1  81.25 ( 80.40)	Acc@5  93.75 ( 96.79)
Epoch: [31][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.6895e-01 (6.6386e-01)	Acc@1  76.56 ( 80.40)	Acc@5  97.66 ( 96.78)
Epoch: [31][170/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0713e-01 (6.6415e-01)	Acc@1  72.66 ( 80.35)	Acc@5  96.09 ( 96.78)
Epoch: [31][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9521e-01 (6.6140e-01)	Acc@1  80.47 ( 80.41)	Acc@5  98.44 ( 96.81)
Epoch: [31][190/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4395e-01 (6.5998e-01)	Acc@1  83.59 ( 80.51)	Acc@5  99.22 ( 96.81)
Epoch: [31][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.3818e-01 (6.6153e-01)	Acc@1  82.03 ( 80.46)	Acc@5  99.22 ( 96.79)
Epoch: [31][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.2754e-01 (6.6240e-01)	Acc@1  77.34 ( 80.40)	Acc@5  96.88 ( 96.76)
Epoch: [31][220/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9146e-01 (6.6311e-01)	Acc@1  83.59 ( 80.40)	Acc@5 100.00 ( 96.73)
Epoch: [31][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (6.6105e-01)	Acc@1  82.03 ( 80.47)	Acc@5 100.00 ( 96.74)
Epoch: [31][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.3389e-01 (6.6013e-01)	Acc@1  71.88 ( 80.48)	Acc@5  96.88 ( 96.75)
Epoch: [31][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4258e-01 (6.6309e-01)	Acc@1  78.91 ( 80.41)	Acc@5  97.66 ( 96.73)
Epoch: [31][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.5430e-01 (6.6237e-01)	Acc@1  81.25 ( 80.42)	Acc@5  99.22 ( 96.75)
Epoch: [31][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2646e-01 (6.6020e-01)	Acc@1  78.12 ( 80.49)	Acc@5  96.88 ( 96.78)
Epoch: [31][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6504e-01 (6.6046e-01)	Acc@1  78.91 ( 80.47)	Acc@5  96.09 ( 96.78)
Epoch: [31][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.2559e-01 (6.6126e-01)	Acc@1  82.03 ( 80.43)	Acc@5  95.31 ( 96.77)
Epoch: [31][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.7432e-01 (6.6046e-01)	Acc@1  78.12 ( 80.44)	Acc@5  97.66 ( 96.79)
Epoch: [31][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.7920e-01 (6.6051e-01)	Acc@1  78.91 ( 80.44)	Acc@5  96.09 ( 96.79)
Epoch: [31][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6201e-01 (6.5924e-01)	Acc@1  82.81 ( 80.49)	Acc@5  97.66 ( 96.80)
Epoch: [31][330/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6748e-01 (6.5993e-01)	Acc@1  83.59 ( 80.46)	Acc@5  97.66 ( 96.79)
Epoch: [31][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.2803e-01 (6.5975e-01)	Acc@1  79.69 ( 80.48)	Acc@5  93.75 ( 96.79)
Epoch: [31][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.6660e-01 (6.5997e-01)	Acc@1  76.56 ( 80.46)	Acc@5  94.53 ( 96.80)
Epoch: [31][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.5430e-01 (6.6048e-01)	Acc@1  76.56 ( 80.46)	Acc@5  97.66 ( 96.79)
Epoch: [31][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.0459e-01 (6.5980e-01)	Acc@1  78.91 ( 80.51)	Acc@5  97.66 ( 96.81)
Epoch: [31][380/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.1426e-01 (6.5855e-01)	Acc@1  84.38 ( 80.53)	Acc@5  96.09 ( 96.84)
Epoch: [31][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.9180e-01 (6.5749e-01)	Acc@1  82.50 ( 80.55)	Acc@5  97.50 ( 96.85)
## e[31] optimizer.zero_grad (sum) time: 0.5606577396392822
## e[31]       loss.backward (sum) time: 12.55511736869812
## e[31]      optimizer.step (sum) time: 25.436384439468384
## epoch[31] training(only) time: 63.3205029964447
# Switched to evaluate mode...
Test: [  0/100]	Time  0.205 ( 0.205)	Loss 1.0107e+00 (1.0107e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.1191e+00 (1.1301e+00)	Acc@1  69.00 ( 69.82)	Acc@5  92.00 ( 90.18)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 9.5801e-01 (1.0964e+00)	Acc@1  71.00 ( 70.05)	Acc@5  95.00 ( 91.29)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.2627e+00 (1.1245e+00)	Acc@1  60.00 ( 68.81)	Acc@5  92.00 ( 91.00)
Test: [ 40/100]	Time  0.049 ( 0.051)	Loss 1.0635e+00 (1.1144e+00)	Acc@1  68.00 ( 68.85)	Acc@5  90.00 ( 91.37)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.1016e+00 (1.1220e+00)	Acc@1  70.00 ( 68.75)	Acc@5  90.00 ( 91.18)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.0459e+00 (1.1063e+00)	Acc@1  69.00 ( 68.82)	Acc@5  89.00 ( 91.18)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.1709e+00 (1.1081e+00)	Acc@1  64.00 ( 68.83)	Acc@5  94.00 ( 91.04)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.3057e+00 (1.1082e+00)	Acc@1  65.00 ( 68.98)	Acc@5  88.00 ( 91.02)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.2373e+00 (1.0993e+00)	Acc@1  62.00 ( 69.02)	Acc@5  89.00 ( 91.16)
 * Acc@1 69.310 Acc@5 91.270
### epoch[31] execution time: 68.28909945487976
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.329 ( 0.329)	Data  0.155 ( 0.155)	Loss 5.7959e-01 (5.7959e-01)	Acc@1  85.94 ( 85.94)	Acc@5  96.88 ( 96.88)
Epoch: [32][ 10/391]	Time  0.159 ( 0.176)	Data  0.001 ( 0.015)	Loss 6.2744e-01 (6.0982e-01)	Acc@1  80.47 ( 81.68)	Acc@5  96.88 ( 97.16)
Epoch: [32][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 5.6104e-01 (6.0926e-01)	Acc@1  85.16 ( 81.81)	Acc@5  97.66 ( 96.95)
Epoch: [32][ 30/391]	Time  0.159 ( 0.167)	Data  0.001 ( 0.006)	Loss 6.7871e-01 (6.0841e-01)	Acc@1  77.34 ( 81.91)	Acc@5  96.09 ( 97.18)
Epoch: [32][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 7.1729e-01 (6.0615e-01)	Acc@1  80.47 ( 82.07)	Acc@5  95.31 ( 97.39)
Epoch: [32][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.6055e-01 (6.0658e-01)	Acc@1  81.25 ( 82.09)	Acc@5  96.88 ( 97.43)
Epoch: [32][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 8.1104e-01 (6.0730e-01)	Acc@1  74.22 ( 82.13)	Acc@5  95.31 ( 97.45)
Epoch: [32][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.8105e-01 (6.0563e-01)	Acc@1  81.25 ( 82.10)	Acc@5  98.44 ( 97.39)
Epoch: [32][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.2646e-01 (6.1094e-01)	Acc@1  80.47 ( 81.93)	Acc@5  96.09 ( 97.29)
Epoch: [32][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.0586e-01 (6.0627e-01)	Acc@1  86.72 ( 82.16)	Acc@5  98.44 ( 97.33)
Epoch: [32][100/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 7.3633e-01 (6.0932e-01)	Acc@1  74.22 ( 82.06)	Acc@5  96.88 ( 97.32)
Epoch: [32][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4932e-01 (6.0872e-01)	Acc@1  85.16 ( 82.13)	Acc@5 100.00 ( 97.36)
Epoch: [32][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2061e-01 (6.0807e-01)	Acc@1  78.91 ( 82.11)	Acc@5 100.00 ( 97.39)
Epoch: [32][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7373e-01 (6.1187e-01)	Acc@1  82.81 ( 81.95)	Acc@5  99.22 ( 97.38)
Epoch: [32][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.7236e-01 (6.1425e-01)	Acc@1  78.12 ( 81.91)	Acc@5  97.66 ( 97.33)
Epoch: [32][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.5386e-01 (6.1593e-01)	Acc@1  85.94 ( 81.93)	Acc@5  98.44 ( 97.32)
Epoch: [32][160/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.0020e-01 (6.1784e-01)	Acc@1  78.91 ( 81.84)	Acc@5  96.09 ( 97.31)
Epoch: [32][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5566e-01 (6.1862e-01)	Acc@1  83.59 ( 81.78)	Acc@5  99.22 ( 97.33)
Epoch: [32][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6309e-01 (6.1933e-01)	Acc@1  78.91 ( 81.78)	Acc@5  97.66 ( 97.34)
Epoch: [32][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0537e-01 (6.1833e-01)	Acc@1  86.72 ( 81.85)	Acc@5  96.09 ( 97.30)
Epoch: [32][200/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.0605e-01 (6.1859e-01)	Acc@1  78.91 ( 81.81)	Acc@5  96.88 ( 97.29)
Epoch: [32][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1514e-01 (6.1779e-01)	Acc@1  84.38 ( 81.82)	Acc@5  98.44 ( 97.31)
Epoch: [32][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.0156e-01 (6.1681e-01)	Acc@1  79.69 ( 81.82)	Acc@5 100.00 ( 97.31)
Epoch: [32][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8066e-01 (6.1730e-01)	Acc@1  82.03 ( 81.78)	Acc@5  96.88 ( 97.30)
Epoch: [32][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9927e-01 (6.1720e-01)	Acc@1  86.72 ( 81.77)	Acc@5  96.88 ( 97.30)
Epoch: [32][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6836e-01 (6.1696e-01)	Acc@1  83.59 ( 81.79)	Acc@5  99.22 ( 97.31)
Epoch: [32][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.8271e-01 (6.1875e-01)	Acc@1  78.12 ( 81.72)	Acc@5  96.09 ( 97.31)
Epoch: [32][270/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.6650e-01 (6.2001e-01)	Acc@1  76.56 ( 81.67)	Acc@5 100.00 ( 97.30)
Epoch: [32][280/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.3145e-01 (6.1963e-01)	Acc@1  77.34 ( 81.65)	Acc@5  97.66 ( 97.31)
Epoch: [32][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.2051e-01 (6.1900e-01)	Acc@1  85.16 ( 81.64)	Acc@5  96.09 ( 97.32)
Epoch: [32][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8843e-01 (6.1822e-01)	Acc@1  87.50 ( 81.69)	Acc@5  99.22 ( 97.34)
Epoch: [32][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.0205e-01 (6.1832e-01)	Acc@1  82.81 ( 81.68)	Acc@5  99.22 ( 97.32)
Epoch: [32][320/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.6299e-01 (6.1926e-01)	Acc@1  79.69 ( 81.63)	Acc@5  98.44 ( 97.33)
Epoch: [32][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.7529e-01 (6.1894e-01)	Acc@1  81.25 ( 81.65)	Acc@5  95.31 ( 97.31)
Epoch: [32][340/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.5439e-01 (6.1939e-01)	Acc@1  78.91 ( 81.68)	Acc@5  96.09 ( 97.29)
Epoch: [32][350/391]	Time  0.174 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.2930e-01 (6.1812e-01)	Acc@1  84.38 ( 81.73)	Acc@5  98.44 ( 97.29)
Epoch: [32][360/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.1143e-01 (6.1857e-01)	Acc@1  80.47 ( 81.72)	Acc@5  94.53 ( 97.28)
Epoch: [32][370/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.7324e-01 (6.1745e-01)	Acc@1  90.62 ( 81.77)	Acc@5  98.44 ( 97.30)
Epoch: [32][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.4785e-01 (6.1660e-01)	Acc@1  84.38 ( 81.78)	Acc@5  98.44 ( 97.31)
Epoch: [32][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.7676e-01 (6.1639e-01)	Acc@1  76.25 ( 81.79)	Acc@5  98.75 ( 97.31)
## e[32] optimizer.zero_grad (sum) time: 0.5588293075561523
## e[32]       loss.backward (sum) time: 12.583129405975342
## e[32]      optimizer.step (sum) time: 25.42734169960022
## epoch[32] training(only) time: 63.37262201309204
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 9.7461e-01 (9.7461e-01)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.1436e+00 (1.1334e+00)	Acc@1  69.00 ( 70.18)	Acc@5  91.00 ( 89.82)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 9.2236e-01 (1.0963e+00)	Acc@1  72.00 ( 70.76)	Acc@5  95.00 ( 91.00)
Test: [ 30/100]	Time  0.057 ( 0.051)	Loss 1.2871e+00 (1.1257e+00)	Acc@1  56.00 ( 69.52)	Acc@5  90.00 ( 90.77)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0791e+00 (1.1168e+00)	Acc@1  68.00 ( 69.46)	Acc@5  91.00 ( 91.32)
Test: [ 50/100]	Time  0.049 ( 0.049)	Loss 1.1631e+00 (1.1234e+00)	Acc@1  69.00 ( 69.29)	Acc@5  89.00 ( 91.06)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.0762e+00 (1.1083e+00)	Acc@1  69.00 ( 69.38)	Acc@5  91.00 ( 91.30)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 1.1768e+00 (1.1084e+00)	Acc@1  68.00 ( 69.46)	Acc@5  93.00 ( 91.23)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.2734e+00 (1.1080e+00)	Acc@1  64.00 ( 69.53)	Acc@5  89.00 ( 91.21)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.2275e+00 (1.0976e+00)	Acc@1  65.00 ( 69.64)	Acc@5  90.00 ( 91.36)
 * Acc@1 69.860 Acc@5 91.450
### epoch[32] execution time: 68.37616753578186
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.344 ( 0.344)	Data  0.168 ( 0.168)	Loss 5.3223e-01 (5.3223e-01)	Acc@1  85.16 ( 85.16)	Acc@5  97.66 ( 97.66)
Epoch: [33][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.016)	Loss 6.0156e-01 (5.7386e-01)	Acc@1  85.16 ( 84.45)	Acc@5  96.88 ( 97.94)
Epoch: [33][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.009)	Loss 5.1758e-01 (5.5306e-01)	Acc@1  85.16 ( 84.64)	Acc@5  99.22 ( 98.10)
Epoch: [33][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.007)	Loss 4.9829e-01 (5.6284e-01)	Acc@1  88.28 ( 83.90)	Acc@5  98.44 ( 97.68)
Epoch: [33][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 5.3467e-01 (5.6875e-01)	Acc@1  86.72 ( 83.78)	Acc@5  98.44 ( 97.56)
Epoch: [33][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 6.1719e-01 (5.6571e-01)	Acc@1  80.47 ( 83.75)	Acc@5  96.88 ( 97.52)
Epoch: [33][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 7.9053e-01 (5.6878e-01)	Acc@1  80.47 ( 83.71)	Acc@5  93.75 ( 97.50)
Epoch: [33][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.8545e-01 (5.6339e-01)	Acc@1  84.38 ( 83.77)	Acc@5  96.09 ( 97.57)
Epoch: [33][ 80/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.3369e-01 (5.6525e-01)	Acc@1  84.38 ( 83.66)	Acc@5  99.22 ( 97.56)
Epoch: [33][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 7.0947e-01 (5.7339e-01)	Acc@1  82.03 ( 83.37)	Acc@5  96.09 ( 97.47)
Epoch: [33][100/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.8984e-01 (5.7679e-01)	Acc@1  82.03 ( 83.23)	Acc@5  99.22 ( 97.43)
Epoch: [33][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.003)	Loss 6.1670e-01 (5.7701e-01)	Acc@1  84.38 ( 83.26)	Acc@5  97.66 ( 97.39)
Epoch: [33][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6045e-01 (5.7486e-01)	Acc@1  86.72 ( 83.37)	Acc@5  99.22 ( 97.42)
Epoch: [33][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2441e-01 (5.7178e-01)	Acc@1  82.81 ( 83.46)	Acc@5  97.66 ( 97.47)
Epoch: [33][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (5.7259e-01)	Acc@1  83.59 ( 83.47)	Acc@5  97.66 ( 97.47)
Epoch: [33][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1367e-01 (5.7257e-01)	Acc@1  85.94 ( 83.46)	Acc@5  99.22 ( 97.50)
Epoch: [33][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6362e-01 (5.7361e-01)	Acc@1  84.38 ( 83.42)	Acc@5  98.44 ( 97.49)
Epoch: [33][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2686e-01 (5.7249e-01)	Acc@1  85.16 ( 83.47)	Acc@5  96.88 ( 97.51)
Epoch: [33][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6201e-01 (5.7382e-01)	Acc@1  85.16 ( 83.42)	Acc@5  95.31 ( 97.46)
Epoch: [33][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.1523e-01 (5.7316e-01)	Acc@1  82.03 ( 83.47)	Acc@5  97.66 ( 97.45)
Epoch: [33][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.7676e-01 (5.7717e-01)	Acc@1  75.78 ( 83.32)	Acc@5  98.44 ( 97.44)
Epoch: [33][210/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.3770e-01 (5.7733e-01)	Acc@1  78.12 ( 83.29)	Acc@5  99.22 ( 97.43)
Epoch: [33][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6997e-01 (5.7782e-01)	Acc@1  86.72 ( 83.27)	Acc@5  98.44 ( 97.43)
Epoch: [33][230/391]	Time  0.158 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.3535e-01 (5.7793e-01)	Acc@1  79.69 ( 83.27)	Acc@5  95.31 ( 97.43)
Epoch: [33][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (5.7683e-01)	Acc@1  85.16 ( 83.31)	Acc@5  95.31 ( 97.46)
Epoch: [33][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2354e-01 (5.7675e-01)	Acc@1  81.25 ( 83.26)	Acc@5  94.53 ( 97.47)
Epoch: [33][260/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.4297e-01 (5.7752e-01)	Acc@1  85.16 ( 83.25)	Acc@5  99.22 ( 97.46)
Epoch: [33][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.4492e-01 (5.7685e-01)	Acc@1  79.69 ( 83.27)	Acc@5  98.44 ( 97.45)
Epoch: [33][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.2441e-01 (5.7685e-01)	Acc@1  87.50 ( 83.30)	Acc@5  97.66 ( 97.44)
Epoch: [33][290/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.6709e-01 (5.7767e-01)	Acc@1  80.47 ( 83.26)	Acc@5  96.09 ( 97.42)
Epoch: [33][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2061e-01 (5.7713e-01)	Acc@1  82.03 ( 83.28)	Acc@5  97.66 ( 97.43)
Epoch: [33][310/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.9043e-01 (5.7793e-01)	Acc@1  74.22 ( 83.27)	Acc@5  96.09 ( 97.42)
Epoch: [33][320/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1465e-01 (5.7805e-01)	Acc@1  83.59 ( 83.26)	Acc@5  97.66 ( 97.43)
Epoch: [33][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7949e-01 (5.7790e-01)	Acc@1  88.28 ( 83.27)	Acc@5  98.44 ( 97.44)
Epoch: [33][340/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.4199e-01 (5.7840e-01)	Acc@1  83.59 ( 83.24)	Acc@5  98.44 ( 97.45)
Epoch: [33][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3555e-01 (5.7674e-01)	Acc@1  91.41 ( 83.29)	Acc@5  97.66 ( 97.46)
Epoch: [33][360/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.3271e-01 (5.7565e-01)	Acc@1  84.38 ( 83.33)	Acc@5  97.66 ( 97.46)
Epoch: [33][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.3096e-01 (5.7541e-01)	Acc@1  79.69 ( 83.33)	Acc@5  92.97 ( 97.46)
Epoch: [33][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.6152e-01 (5.7589e-01)	Acc@1  82.03 ( 83.30)	Acc@5  97.66 ( 97.45)
Epoch: [33][390/391]	Time  0.121 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.6455e-01 (5.7596e-01)	Acc@1  85.00 ( 83.32)	Acc@5  93.75 ( 97.46)
## e[33] optimizer.zero_grad (sum) time: 0.5577976703643799
## e[33]       loss.backward (sum) time: 12.58138632774353
## e[33]      optimizer.step (sum) time: 25.414801836013794
## epoch[33] training(only) time: 63.37552356719971
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 9.5557e-01 (9.5557e-01)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.1279e+00 (1.1301e+00)	Acc@1  70.00 ( 70.45)	Acc@5  92.00 ( 90.09)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 9.5166e-01 (1.0969e+00)	Acc@1  73.00 ( 71.33)	Acc@5  95.00 ( 91.29)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.2256e+00 (1.1216e+00)	Acc@1  60.00 ( 70.10)	Acc@5  91.00 ( 91.10)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0762e+00 (1.1096e+00)	Acc@1  65.00 ( 70.02)	Acc@5  89.00 ( 91.34)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 1.1455e+00 (1.1158e+00)	Acc@1  66.00 ( 69.84)	Acc@5  90.00 ( 91.20)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.0938e+00 (1.1036e+00)	Acc@1  69.00 ( 69.97)	Acc@5  90.00 ( 91.33)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.1270e+00 (1.1026e+00)	Acc@1  66.00 ( 70.08)	Acc@5  91.00 ( 91.23)
Test: [ 80/100]	Time  0.046 ( 0.048)	Loss 1.3271e+00 (1.1039e+00)	Acc@1  66.00 ( 70.12)	Acc@5  88.00 ( 91.28)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.2236e+00 (1.0943e+00)	Acc@1  65.00 ( 70.19)	Acc@5  91.00 ( 91.45)
 * Acc@1 70.210 Acc@5 91.500
### epoch[33] execution time: 68.27129864692688
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.327 ( 0.327)	Data  0.153 ( 0.153)	Loss 5.8105e-01 (5.8105e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.88 ( 96.88)
Epoch: [34][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 7.2412e-01 (5.2903e-01)	Acc@1  78.12 ( 84.16)	Acc@5  96.88 ( 97.87)
Epoch: [34][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.008)	Loss 5.8252e-01 (5.4864e-01)	Acc@1  83.59 ( 83.63)	Acc@5  96.09 ( 97.92)
Epoch: [34][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 6.2354e-01 (5.2998e-01)	Acc@1  84.38 ( 84.32)	Acc@5  96.09 ( 97.96)
Epoch: [34][ 40/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.005)	Loss 5.6152e-01 (5.4044e-01)	Acc@1  83.59 ( 83.97)	Acc@5  96.88 ( 97.81)
Epoch: [34][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 5.8789e-01 (5.4998e-01)	Acc@1  79.69 ( 83.66)	Acc@5  98.44 ( 97.78)
Epoch: [34][ 60/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.4443e-01 (5.4657e-01)	Acc@1  83.59 ( 83.80)	Acc@5  98.44 ( 97.82)
Epoch: [34][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.3525e-01 (5.4994e-01)	Acc@1  78.12 ( 83.59)	Acc@5  96.09 ( 97.80)
Epoch: [34][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.9961e-01 (5.4752e-01)	Acc@1  79.69 ( 83.74)	Acc@5  97.66 ( 97.85)
Epoch: [34][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.1260e-01 (5.4591e-01)	Acc@1  87.50 ( 83.83)	Acc@5 100.00 ( 97.84)
Epoch: [34][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4248e-01 (5.4622e-01)	Acc@1  83.59 ( 83.77)	Acc@5  97.66 ( 97.85)
Epoch: [34][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2090e-01 (5.4437e-01)	Acc@1  87.50 ( 83.88)	Acc@5  98.44 ( 97.84)
Epoch: [34][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9521e-01 (5.4303e-01)	Acc@1  83.59 ( 83.92)	Acc@5  97.66 ( 97.88)
Epoch: [34][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4736e-01 (5.4473e-01)	Acc@1  83.59 ( 83.91)	Acc@5  99.22 ( 97.86)
Epoch: [34][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6460e-01 (5.4410e-01)	Acc@1  88.28 ( 83.98)	Acc@5  97.66 ( 97.82)
Epoch: [34][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8853e-01 (5.4311e-01)	Acc@1  86.72 ( 84.03)	Acc@5  99.22 ( 97.82)
Epoch: [34][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4834e-01 (5.4375e-01)	Acc@1  82.81 ( 83.99)	Acc@5  98.44 ( 97.80)
Epoch: [34][170/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5811e-01 (5.4430e-01)	Acc@1  80.47 ( 83.97)	Acc@5  97.66 ( 97.78)
Epoch: [34][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5615e-01 (5.4470e-01)	Acc@1  84.38 ( 83.97)	Acc@5  96.88 ( 97.78)
Epoch: [34][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.3379e-01 (5.4740e-01)	Acc@1  82.81 ( 83.93)	Acc@5  98.44 ( 97.75)
Epoch: [34][200/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5669e-01 (5.4644e-01)	Acc@1  90.62 ( 83.97)	Acc@5  99.22 ( 97.74)
Epoch: [34][210/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5728e-01 (5.4690e-01)	Acc@1  87.50 ( 83.94)	Acc@5  98.44 ( 97.71)
Epoch: [34][220/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.0010e-01 (5.4633e-01)	Acc@1  79.69 ( 83.94)	Acc@5  98.44 ( 97.71)
Epoch: [34][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.0303e-01 (5.4474e-01)	Acc@1  81.25 ( 84.01)	Acc@5  99.22 ( 97.73)
Epoch: [34][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.7773e-01 (5.4593e-01)	Acc@1  78.12 ( 83.97)	Acc@5  95.31 ( 97.70)
Epoch: [34][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6655e-01 (5.4450e-01)	Acc@1  87.50 ( 83.99)	Acc@5  98.44 ( 97.72)
Epoch: [34][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6167e-01 (5.4283e-01)	Acc@1  82.81 ( 84.05)	Acc@5  98.44 ( 97.75)
Epoch: [34][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.3086e-01 (5.4340e-01)	Acc@1  78.91 ( 84.06)	Acc@5  94.53 ( 97.74)
Epoch: [34][280/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.3716e-01 (5.4266e-01)	Acc@1  90.62 ( 84.06)	Acc@5  98.44 ( 97.76)
Epoch: [34][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.8496e-01 (5.4422e-01)	Acc@1  85.16 ( 84.00)	Acc@5  98.44 ( 97.77)
Epoch: [34][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.5039e-01 (5.4455e-01)	Acc@1  78.91 ( 83.98)	Acc@5  99.22 ( 97.78)
Epoch: [34][310/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.2461e-01 (5.4672e-01)	Acc@1  72.66 ( 83.86)	Acc@5  96.09 ( 97.76)
Epoch: [34][320/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.2598e-01 (5.4652e-01)	Acc@1  80.47 ( 83.84)	Acc@5  96.09 ( 97.77)
Epoch: [34][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.8950e-01 (5.4689e-01)	Acc@1  86.72 ( 83.86)	Acc@5  96.88 ( 97.75)
Epoch: [34][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.5127e-01 (5.4823e-01)	Acc@1  84.38 ( 83.81)	Acc@5  95.31 ( 97.74)
Epoch: [34][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.3174e-01 (5.4789e-01)	Acc@1  83.59 ( 83.82)	Acc@5  97.66 ( 97.73)
Epoch: [34][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.3623e-01 (5.4835e-01)	Acc@1  84.38 ( 83.84)	Acc@5  96.09 ( 97.73)
Epoch: [34][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.2114e-01 (5.4752e-01)	Acc@1  88.28 ( 83.87)	Acc@5  99.22 ( 97.74)
Epoch: [34][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.6943e-01 (5.4838e-01)	Acc@1  80.47 ( 83.81)	Acc@5  94.53 ( 97.73)
Epoch: [34][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.0928e-01 (5.4709e-01)	Acc@1  85.00 ( 83.85)	Acc@5  97.50 ( 97.73)
## e[34] optimizer.zero_grad (sum) time: 0.5521023273468018
## e[34]       loss.backward (sum) time: 12.525323867797852
## e[34]      optimizer.step (sum) time: 25.453266620635986
## epoch[34] training(only) time: 63.328994274139404
# Switched to evaluate mode...
Test: [  0/100]	Time  0.192 ( 0.192)	Loss 9.9707e-01 (9.9707e-01)	Acc@1  71.00 ( 71.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.1680e+00 (1.1470e+00)	Acc@1  70.00 ( 69.91)	Acc@5  93.00 ( 90.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 9.5752e-01 (1.1130e+00)	Acc@1  74.00 ( 70.67)	Acc@5  94.00 ( 91.33)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.2793e+00 (1.1379e+00)	Acc@1  59.00 ( 69.65)	Acc@5  92.00 ( 91.19)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 1.0908e+00 (1.1235e+00)	Acc@1  69.00 ( 69.49)	Acc@5  89.00 ( 91.56)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.1309e+00 (1.1267e+00)	Acc@1  70.00 ( 69.43)	Acc@5  90.00 ( 91.45)
Test: [ 60/100]	Time  0.055 ( 0.049)	Loss 1.0947e+00 (1.1129e+00)	Acc@1  68.00 ( 69.51)	Acc@5  91.00 ( 91.54)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.1504e+00 (1.1133e+00)	Acc@1  66.00 ( 69.49)	Acc@5  92.00 ( 91.46)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.2881e+00 (1.1124e+00)	Acc@1  67.00 ( 69.56)	Acc@5  88.00 ( 91.44)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.2881e+00 (1.1022e+00)	Acc@1  63.00 ( 69.65)	Acc@5  90.00 ( 91.57)
 * Acc@1 69.870 Acc@5 91.600
### epoch[34] execution time: 68.23299980163574
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.328 ( 0.328)	Data  0.161 ( 0.161)	Loss 4.9536e-01 (4.9536e-01)	Acc@1  86.72 ( 86.72)	Acc@5  98.44 ( 98.44)
Epoch: [35][ 10/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.016)	Loss 5.0928e-01 (4.9838e-01)	Acc@1  82.81 ( 86.72)	Acc@5  97.66 ( 97.94)
Epoch: [35][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.009)	Loss 3.9917e-01 (4.8855e-01)	Acc@1  88.28 ( 86.50)	Acc@5  99.22 ( 98.25)
Epoch: [35][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 4.6899e-01 (4.9351e-01)	Acc@1  86.72 ( 86.16)	Acc@5  98.44 ( 98.16)
Epoch: [35][ 40/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.005)	Loss 6.2598e-01 (5.0766e-01)	Acc@1  83.59 ( 85.71)	Acc@5  96.88 ( 98.06)
Epoch: [35][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.9268e-01 (5.0969e-01)	Acc@1  82.03 ( 85.49)	Acc@5  99.22 ( 98.07)
Epoch: [35][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.8047e-01 (5.1274e-01)	Acc@1  86.72 ( 85.23)	Acc@5  98.44 ( 98.04)
Epoch: [35][ 70/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.6982e-01 (5.2177e-01)	Acc@1  85.16 ( 85.02)	Acc@5  96.09 ( 98.01)
Epoch: [35][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.4102e-01 (5.2204e-01)	Acc@1  86.72 ( 84.95)	Acc@5  95.31 ( 97.99)
Epoch: [35][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.3945e-01 (5.2846e-01)	Acc@1  87.50 ( 84.73)	Acc@5  99.22 ( 97.96)
Epoch: [35][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.1074e-01 (5.2965e-01)	Acc@1  88.28 ( 84.72)	Acc@5  96.88 ( 97.98)
Epoch: [35][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1758e-01 (5.3025e-01)	Acc@1  85.16 ( 84.65)	Acc@5  98.44 ( 97.99)
Epoch: [35][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3042e-01 (5.2648e-01)	Acc@1  85.94 ( 84.66)	Acc@5  98.44 ( 98.00)
Epoch: [35][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9805e-01 (5.2546e-01)	Acc@1  85.94 ( 84.65)	Acc@5  98.44 ( 98.00)
Epoch: [35][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9707e-01 (5.2538e-01)	Acc@1  85.16 ( 84.64)	Acc@5  97.66 ( 98.01)
Epoch: [35][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8691e-01 (5.2671e-01)	Acc@1  82.81 ( 84.58)	Acc@5  96.09 ( 97.95)
Epoch: [35][160/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.6348e-01 (5.2629e-01)	Acc@1  80.47 ( 84.56)	Acc@5  98.44 ( 97.94)
Epoch: [35][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9619e-01 (5.2785e-01)	Acc@1  81.25 ( 84.48)	Acc@5  96.88 ( 97.97)
Epoch: [35][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.7080e-01 (5.2827e-01)	Acc@1  82.03 ( 84.47)	Acc@5  97.66 ( 97.96)
Epoch: [35][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.8994e-01 (5.2867e-01)	Acc@1  77.34 ( 84.42)	Acc@5  99.22 ( 97.97)
Epoch: [35][200/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9326e-01 (5.2801e-01)	Acc@1  85.16 ( 84.42)	Acc@5  95.31 ( 97.96)
Epoch: [35][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3359e-01 (5.2824e-01)	Acc@1  87.50 ( 84.38)	Acc@5  99.22 ( 97.98)
Epoch: [35][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.8496e-01 (5.2610e-01)	Acc@1  79.69 ( 84.42)	Acc@5  99.22 ( 98.00)
Epoch: [35][230/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5923e-01 (5.2445e-01)	Acc@1  84.38 ( 84.47)	Acc@5  98.44 ( 98.03)
Epoch: [35][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9473e-01 (5.2512e-01)	Acc@1  81.25 ( 84.46)	Acc@5  96.88 ( 98.00)
Epoch: [35][250/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.7227e-01 (5.2695e-01)	Acc@1  82.81 ( 84.45)	Acc@5  96.88 ( 97.95)
Epoch: [35][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.1133e-01 (5.2691e-01)	Acc@1  80.47 ( 84.42)	Acc@5  97.66 ( 97.96)
Epoch: [35][270/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8135e-01 (5.2718e-01)	Acc@1  86.72 ( 84.43)	Acc@5  98.44 ( 97.96)
Epoch: [35][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3408e-01 (5.2640e-01)	Acc@1  87.50 ( 84.43)	Acc@5  99.22 ( 97.98)
Epoch: [35][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9570e-01 (5.2524e-01)	Acc@1  83.59 ( 84.46)	Acc@5  97.66 ( 98.01)
Epoch: [35][300/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8779e-01 (5.2599e-01)	Acc@1  84.38 ( 84.47)	Acc@5  96.88 ( 97.98)
Epoch: [35][310/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.4453e-01 (5.2483e-01)	Acc@1  82.03 ( 84.51)	Acc@5  96.09 ( 97.98)
Epoch: [35][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.8057e-01 (5.2590e-01)	Acc@1  81.25 ( 84.45)	Acc@5  98.44 ( 97.98)
Epoch: [35][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.7520e-01 (5.2628e-01)	Acc@1  87.50 ( 84.48)	Acc@5  98.44 ( 97.97)
Epoch: [35][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.1064e-01 (5.2526e-01)	Acc@1  86.72 ( 84.50)	Acc@5  99.22 ( 97.98)
Epoch: [35][350/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.9194e-01 (5.2430e-01)	Acc@1  85.94 ( 84.55)	Acc@5  99.22 ( 97.98)
Epoch: [35][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.6362e-01 (5.2347e-01)	Acc@1  88.28 ( 84.57)	Acc@5  98.44 ( 98.00)
Epoch: [35][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.3125e-01 (5.2441e-01)	Acc@1  83.59 ( 84.52)	Acc@5  97.66 ( 98.01)
Epoch: [35][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.8350e-01 (5.2440e-01)	Acc@1  85.16 ( 84.55)	Acc@5  97.66 ( 98.02)
Epoch: [35][390/391]	Time  0.121 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.4512e-01 (5.2508e-01)	Acc@1  76.25 ( 84.51)	Acc@5  97.50 ( 98.01)
## e[35] optimizer.zero_grad (sum) time: 0.558030366897583
## e[35]       loss.backward (sum) time: 12.582580327987671
## e[35]      optimizer.step (sum) time: 25.416750192642212
## epoch[35] training(only) time: 63.39069890975952
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.0010e+00 (1.0010e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 1.1650e+00 (1.1439e+00)	Acc@1  71.00 ( 70.45)	Acc@5  92.00 ( 90.64)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 9.8193e-01 (1.1087e+00)	Acc@1  73.00 ( 71.19)	Acc@5  94.00 ( 91.29)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.2598e+00 (1.1357e+00)	Acc@1  60.00 ( 70.03)	Acc@5  90.00 ( 91.32)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 9.8682e-01 (1.1200e+00)	Acc@1  71.00 ( 70.05)	Acc@5  90.00 ( 91.66)
Test: [ 50/100]	Time  0.052 ( 0.051)	Loss 1.1289e+00 (1.1250e+00)	Acc@1  70.00 ( 69.90)	Acc@5  91.00 ( 91.51)
Test: [ 60/100]	Time  0.046 ( 0.051)	Loss 1.1201e+00 (1.1111e+00)	Acc@1  68.00 ( 69.87)	Acc@5  89.00 ( 91.59)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.1846e+00 (1.1125e+00)	Acc@1  68.00 ( 69.97)	Acc@5  92.00 ( 91.44)
Test: [ 80/100]	Time  0.046 ( 0.050)	Loss 1.2881e+00 (1.1113e+00)	Acc@1  71.00 ( 69.98)	Acc@5  87.00 ( 91.41)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2236e+00 (1.1009e+00)	Acc@1  65.00 ( 69.93)	Acc@5  91.00 ( 91.52)
 * Acc@1 70.140 Acc@5 91.540
### epoch[35] execution time: 68.40367579460144
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.323 ( 0.323)	Data  0.154 ( 0.154)	Loss 3.8379e-01 (3.8379e-01)	Acc@1  88.28 ( 88.28)	Acc@5  99.22 ( 99.22)
Epoch: [36][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.015)	Loss 5.0000e-01 (4.7004e-01)	Acc@1  85.16 ( 86.01)	Acc@5  96.88 ( 98.30)
Epoch: [36][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 4.4971e-01 (4.9721e-01)	Acc@1  87.50 ( 85.23)	Acc@5  98.44 ( 97.88)
Epoch: [36][ 30/391]	Time  0.168 ( 0.166)	Data  0.001 ( 0.006)	Loss 6.3623e-01 (5.0784e-01)	Acc@1  79.69 ( 84.98)	Acc@5  97.66 ( 97.96)
Epoch: [36][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 5.7129e-01 (5.1181e-01)	Acc@1  83.59 ( 84.89)	Acc@5  99.22 ( 97.92)
Epoch: [36][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.3848e-01 (5.0295e-01)	Acc@1  86.72 ( 85.23)	Acc@5  99.22 ( 98.05)
Epoch: [36][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.2480e-01 (5.0102e-01)	Acc@1  87.50 ( 85.22)	Acc@5  97.66 ( 98.08)
Epoch: [36][ 70/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 5.0049e-01 (4.9930e-01)	Acc@1  83.59 ( 85.11)	Acc@5  98.44 ( 98.14)
Epoch: [36][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.3481e-01 (5.0209e-01)	Acc@1  88.28 ( 85.02)	Acc@5  97.66 ( 98.10)
Epoch: [36][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.6436e-01 (5.0075e-01)	Acc@1  85.94 ( 85.14)	Acc@5  99.22 ( 98.15)
Epoch: [36][100/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 6.2354e-01 (5.0611e-01)	Acc@1  78.12 ( 85.05)	Acc@5  96.09 ( 98.08)
Epoch: [36][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7534e-01 (5.0520e-01)	Acc@1  88.28 ( 85.06)	Acc@5  97.66 ( 98.08)
Epoch: [36][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9170e-01 (5.0285e-01)	Acc@1  82.81 ( 85.13)	Acc@5  99.22 ( 98.09)
Epoch: [36][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.0010e-01 (5.0371e-01)	Acc@1  82.03 ( 85.08)	Acc@5  98.44 ( 98.11)
Epoch: [36][140/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.9082e-01 (5.0507e-01)	Acc@1  82.03 ( 85.01)	Acc@5  98.44 ( 98.15)
Epoch: [36][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.0928e-01 (5.0541e-01)	Acc@1  85.16 ( 84.95)	Acc@5  98.44 ( 98.17)
Epoch: [36][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8926e-01 (5.0542e-01)	Acc@1  88.28 ( 85.01)	Acc@5  98.44 ( 98.17)
Epoch: [36][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3711e-01 (5.0519e-01)	Acc@1  87.50 ( 85.03)	Acc@5  95.31 ( 98.15)
Epoch: [36][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2725e-01 (5.0652e-01)	Acc@1  85.16 ( 84.99)	Acc@5  99.22 ( 98.14)
Epoch: [36][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1514e-01 (5.0561e-01)	Acc@1  85.94 ( 85.00)	Acc@5  97.66 ( 98.14)
Epoch: [36][200/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8252e-01 (5.0730e-01)	Acc@1  81.25 ( 84.94)	Acc@5  98.44 ( 98.13)
Epoch: [36][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1270e-01 (5.0735e-01)	Acc@1  87.50 ( 84.95)	Acc@5  97.66 ( 98.13)
Epoch: [36][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.2158e-01 (5.0779e-01)	Acc@1  82.81 ( 84.91)	Acc@5  94.53 ( 98.12)
Epoch: [36][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.2051e-01 (5.0680e-01)	Acc@1  85.16 ( 84.98)	Acc@5  98.44 ( 98.12)
Epoch: [36][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3862e-01 (5.0438e-01)	Acc@1  90.62 ( 85.10)	Acc@5  99.22 ( 98.13)
Epoch: [36][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9292e-01 (5.0324e-01)	Acc@1  85.94 ( 85.15)	Acc@5  97.66 ( 98.14)
Epoch: [36][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.8950e-01 (5.0264e-01)	Acc@1  82.81 ( 85.17)	Acc@5  96.88 ( 98.14)
Epoch: [36][270/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.3955e-01 (5.0271e-01)	Acc@1  83.59 ( 85.18)	Acc@5  98.44 ( 98.13)
Epoch: [36][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.1865e-01 (5.0419e-01)	Acc@1  84.38 ( 85.12)	Acc@5  96.09 ( 98.09)
Epoch: [36][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.3857e-01 (5.0432e-01)	Acc@1  87.50 ( 85.12)	Acc@5  99.22 ( 98.12)
Epoch: [36][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.2100e-01 (5.0445e-01)	Acc@1  84.38 ( 85.11)	Acc@5  97.66 ( 98.13)
Epoch: [36][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1870e-01 (5.0408e-01)	Acc@1  85.94 ( 85.09)	Acc@5  97.66 ( 98.11)
Epoch: [36][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7266e-01 (5.0372e-01)	Acc@1  85.16 ( 85.06)	Acc@5  96.88 ( 98.11)
Epoch: [36][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7852e-01 (5.0333e-01)	Acc@1  82.03 ( 85.05)	Acc@5  99.22 ( 98.12)
Epoch: [36][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6836e-01 (5.0343e-01)	Acc@1  85.94 ( 85.06)	Acc@5  96.09 ( 98.11)
Epoch: [36][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1416e-01 (5.0363e-01)	Acc@1  85.94 ( 85.04)	Acc@5  96.88 ( 98.12)
Epoch: [36][360/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.2881e-01 (5.0378e-01)	Acc@1  85.16 ( 85.00)	Acc@5  98.44 ( 98.14)
Epoch: [36][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.3125e-01 (5.0402e-01)	Acc@1  85.16 ( 84.99)	Acc@5  98.44 ( 98.13)
Epoch: [36][380/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.3232e-01 (5.0415e-01)	Acc@1  80.47 ( 84.99)	Acc@5  99.22 ( 98.14)
Epoch: [36][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.9043e-01 (5.0485e-01)	Acc@1  76.25 ( 84.99)	Acc@5  98.75 ( 98.13)
## e[36] optimizer.zero_grad (sum) time: 0.55592942237854
## e[36]       loss.backward (sum) time: 12.568077564239502
## e[36]      optimizer.step (sum) time: 25.43440055847168
## epoch[36] training(only) time: 63.51090216636658
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 9.9072e-01 (9.9072e-01)	Acc@1  71.00 ( 71.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.1875e+00 (1.1345e+00)	Acc@1  69.00 ( 70.09)	Acc@5  92.00 ( 90.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 9.7217e-01 (1.1122e+00)	Acc@1  72.00 ( 70.43)	Acc@5  95.00 ( 91.38)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.2314e+00 (1.1374e+00)	Acc@1  57.00 ( 69.65)	Acc@5  92.00 ( 91.13)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 9.6729e-01 (1.1246e+00)	Acc@1  74.00 ( 69.78)	Acc@5  91.00 ( 91.56)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.2002e+00 (1.1280e+00)	Acc@1  65.00 ( 69.80)	Acc@5  90.00 ( 91.41)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.1123e+00 (1.1126e+00)	Acc@1  69.00 ( 69.95)	Acc@5  90.00 ( 91.59)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.1680e+00 (1.1135e+00)	Acc@1  72.00 ( 69.94)	Acc@5  91.00 ( 91.44)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3438e+00 (1.1159e+00)	Acc@1  68.00 ( 69.93)	Acc@5  88.00 ( 91.43)
Test: [ 90/100]	Time  0.056 ( 0.049)	Loss 1.2529e+00 (1.1055e+00)	Acc@1  65.00 ( 70.02)	Acc@5  92.00 ( 91.55)
 * Acc@1 70.230 Acc@5 91.610
### epoch[36] execution time: 68.47382855415344
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.357 ( 0.357)	Data  0.180 ( 0.180)	Loss 4.0112e-01 (4.0112e-01)	Acc@1  90.62 ( 90.62)	Acc@5  98.44 ( 98.44)
Epoch: [37][ 10/391]	Time  0.161 ( 0.181)	Data  0.001 ( 0.017)	Loss 5.1562e-01 (4.6158e-01)	Acc@1  84.38 ( 86.58)	Acc@5  98.44 ( 98.37)
Epoch: [37][ 20/391]	Time  0.161 ( 0.171)	Data  0.001 ( 0.010)	Loss 6.4893e-01 (4.7191e-01)	Acc@1  79.69 ( 86.05)	Acc@5  98.44 ( 98.36)
Epoch: [37][ 30/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.007)	Loss 4.4165e-01 (4.6759e-01)	Acc@1  85.94 ( 86.19)	Acc@5 100.00 ( 98.36)
Epoch: [37][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 4.2896e-01 (4.7400e-01)	Acc@1  89.84 ( 86.07)	Acc@5 100.00 ( 98.29)
Epoch: [37][ 50/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.9414e-01 (4.7882e-01)	Acc@1  83.59 ( 85.89)	Acc@5  99.22 ( 98.24)
Epoch: [37][ 60/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.5605e-01 (4.7674e-01)	Acc@1  86.72 ( 85.91)	Acc@5  99.22 ( 98.27)
Epoch: [37][ 70/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.6729e-01 (4.7998e-01)	Acc@1  86.72 ( 85.86)	Acc@5  99.22 ( 98.27)
Epoch: [37][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.1504e-01 (4.7594e-01)	Acc@1  86.72 ( 85.92)	Acc@5 100.00 ( 98.36)
Epoch: [37][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.4019e-01 (4.7542e-01)	Acc@1  86.72 ( 85.96)	Acc@5  98.44 ( 98.34)
Epoch: [37][100/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.8418e-01 (4.7145e-01)	Acc@1  92.97 ( 86.21)	Acc@5 100.00 ( 98.39)
Epoch: [37][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.0039e-01 (4.7218e-01)	Acc@1  91.41 ( 86.13)	Acc@5  98.44 ( 98.39)
Epoch: [37][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.6948e-01 (4.7161e-01)	Acc@1  89.84 ( 86.15)	Acc@5  97.66 ( 98.40)
Epoch: [37][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4492e-01 (4.7042e-01)	Acc@1  81.25 ( 86.10)	Acc@5  98.44 ( 98.43)
Epoch: [37][140/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (4.7219e-01)	Acc@1  87.50 ( 86.06)	Acc@5  99.22 ( 98.45)
Epoch: [37][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.8252e-01 (4.7303e-01)	Acc@1  79.69 ( 86.07)	Acc@5  98.44 ( 98.43)
Epoch: [37][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1123e-01 (4.7435e-01)	Acc@1  85.16 ( 86.05)	Acc@5  97.66 ( 98.44)
Epoch: [37][170/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1284e-01 (4.7468e-01)	Acc@1  87.50 ( 86.03)	Acc@5 100.00 ( 98.44)
Epoch: [37][180/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1187e-01 (4.7400e-01)	Acc@1  89.06 ( 86.10)	Acc@5 100.00 ( 98.45)
Epoch: [37][190/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4604e-01 (4.7279e-01)	Acc@1  88.28 ( 86.18)	Acc@5  97.66 ( 98.45)
Epoch: [37][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8794e-01 (4.7344e-01)	Acc@1  89.84 ( 86.15)	Acc@5  99.22 ( 98.43)
Epoch: [37][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0186e-01 (4.7334e-01)	Acc@1  87.50 ( 86.14)	Acc@5  99.22 ( 98.43)
Epoch: [37][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0649e-01 (4.7285e-01)	Acc@1  88.28 ( 86.15)	Acc@5  98.44 ( 98.45)
Epoch: [37][230/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5605e-01 (4.7446e-01)	Acc@1  85.94 ( 86.11)	Acc@5  98.44 ( 98.42)
Epoch: [37][240/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5264e-01 (4.7392e-01)	Acc@1  85.94 ( 86.14)	Acc@5  98.44 ( 98.41)
Epoch: [37][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6543e-01 (4.7460e-01)	Acc@1  83.59 ( 86.14)	Acc@5  98.44 ( 98.39)
Epoch: [37][260/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2041e-01 (4.7523e-01)	Acc@1  85.94 ( 86.07)	Acc@5  98.44 ( 98.40)
Epoch: [37][270/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0928e-01 (4.7594e-01)	Acc@1  81.25 ( 86.03)	Acc@5  98.44 ( 98.39)
Epoch: [37][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9331e-01 (4.7546e-01)	Acc@1  89.84 ( 86.03)	Acc@5  99.22 ( 98.40)
Epoch: [37][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5264e-01 (4.7600e-01)	Acc@1  85.16 ( 86.05)	Acc@5  97.66 ( 98.40)
Epoch: [37][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.2305e-01 (4.7631e-01)	Acc@1  83.59 ( 86.03)	Acc@5  96.88 ( 98.40)
Epoch: [37][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8315e-01 (4.7649e-01)	Acc@1  81.25 ( 86.01)	Acc@5  98.44 ( 98.39)
Epoch: [37][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8159e-01 (4.7592e-01)	Acc@1  91.41 ( 86.01)	Acc@5  97.66 ( 98.39)
Epoch: [37][330/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9634e-01 (4.7486e-01)	Acc@1  85.16 ( 86.06)	Acc@5  99.22 ( 98.40)
Epoch: [37][340/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5312e-01 (4.7488e-01)	Acc@1  85.16 ( 86.05)	Acc@5  98.44 ( 98.39)
Epoch: [37][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7681e-01 (4.7547e-01)	Acc@1  85.16 ( 86.03)	Acc@5  96.88 ( 98.38)
Epoch: [37][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.9521e-01 (4.7577e-01)	Acc@1  83.59 ( 86.05)	Acc@5  98.44 ( 98.37)
Epoch: [37][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7656e-01 (4.7537e-01)	Acc@1  88.28 ( 86.07)	Acc@5  96.88 ( 98.38)
Epoch: [37][380/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0537e-01 (4.7643e-01)	Acc@1  85.16 ( 86.03)	Acc@5  97.66 ( 98.37)
Epoch: [37][390/391]	Time  0.130 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.4668e-01 (4.7725e-01)	Acc@1  73.75 ( 86.01)	Acc@5  96.25 ( 98.36)
## e[37] optimizer.zero_grad (sum) time: 0.5493581295013428
## e[37]       loss.backward (sum) time: 12.571513175964355
## e[37]      optimizer.step (sum) time: 25.422110080718994
## epoch[37] training(only) time: 63.26118040084839
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 9.9854e-01 (9.9854e-01)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.1562e+00 (1.1395e+00)	Acc@1  68.00 ( 70.91)	Acc@5  93.00 ( 90.18)
Test: [ 20/100]	Time  0.059 ( 0.055)	Loss 9.6191e-01 (1.1144e+00)	Acc@1  73.00 ( 71.67)	Acc@5  93.00 ( 91.14)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.3359e+00 (1.1457e+00)	Acc@1  57.00 ( 70.58)	Acc@5  91.00 ( 91.19)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0156e+00 (1.1314e+00)	Acc@1  76.00 ( 70.73)	Acc@5  91.00 ( 91.46)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.1572e+00 (1.1357e+00)	Acc@1  69.00 ( 70.61)	Acc@5  91.00 ( 91.29)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.1211e+00 (1.1189e+00)	Acc@1  66.00 ( 70.51)	Acc@5  89.00 ( 91.44)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.1201e+00 (1.1167e+00)	Acc@1  68.00 ( 70.52)	Acc@5  93.00 ( 91.39)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3506e+00 (1.1186e+00)	Acc@1  70.00 ( 70.46)	Acc@5  87.00 ( 91.36)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.2246e+00 (1.1091e+00)	Acc@1  66.00 ( 70.44)	Acc@5  92.00 ( 91.52)
 * Acc@1 70.550 Acc@5 91.560
### epoch[37] execution time: 68.23852872848511
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.327 ( 0.327)	Data  0.150 ( 0.150)	Loss 4.4360e-01 (4.4360e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 4.6777e-01 (4.8739e-01)	Acc@1  85.16 ( 85.30)	Acc@5  98.44 ( 98.37)
Epoch: [38][ 20/391]	Time  0.175 ( 0.169)	Data  0.001 ( 0.008)	Loss 5.2490e-01 (4.7260e-01)	Acc@1  83.59 ( 86.05)	Acc@5  98.44 ( 98.59)
Epoch: [38][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 4.9390e-01 (4.6395e-01)	Acc@1  86.72 ( 86.39)	Acc@5  97.66 ( 98.59)
Epoch: [38][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 4.0088e-01 (4.5734e-01)	Acc@1  89.06 ( 86.62)	Acc@5  97.66 ( 98.57)
Epoch: [38][ 50/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.004)	Loss 5.9619e-01 (4.6012e-01)	Acc@1  83.59 ( 86.47)	Acc@5  96.88 ( 98.56)
Epoch: [38][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.7681e-01 (4.5750e-01)	Acc@1  84.38 ( 86.50)	Acc@5  97.66 ( 98.59)
Epoch: [38][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.8867e-01 (4.5418e-01)	Acc@1  86.72 ( 86.66)	Acc@5 100.00 ( 98.65)
Epoch: [38][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.8545e-01 (4.5482e-01)	Acc@1  85.16 ( 86.63)	Acc@5  96.88 ( 98.66)
Epoch: [38][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.1982e-01 (4.5498e-01)	Acc@1  91.41 ( 86.75)	Acc@5  99.22 ( 98.63)
Epoch: [38][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9014e-01 (4.5470e-01)	Acc@1  89.06 ( 86.74)	Acc@5  99.22 ( 98.64)
Epoch: [38][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.4849e-01 (4.5564e-01)	Acc@1  86.72 ( 86.72)	Acc@5 100.00 ( 98.63)
Epoch: [38][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.9365e-01 (4.5792e-01)	Acc@1  89.06 ( 86.69)	Acc@5  96.88 ( 98.61)
Epoch: [38][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.5469e-01 (4.6194e-01)	Acc@1  83.59 ( 86.58)	Acc@5  97.66 ( 98.55)
Epoch: [38][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3823e-01 (4.6141e-01)	Acc@1  89.06 ( 86.59)	Acc@5  98.44 ( 98.54)
Epoch: [38][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8755e-01 (4.6143e-01)	Acc@1  85.94 ( 86.59)	Acc@5  97.66 ( 98.55)
Epoch: [38][160/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1724e-01 (4.6208e-01)	Acc@1  88.28 ( 86.57)	Acc@5  98.44 ( 98.55)
Epoch: [38][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.8154e-01 (4.6097e-01)	Acc@1  83.59 ( 86.59)	Acc@5  97.66 ( 98.57)
Epoch: [38][180/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.0791e-01 (4.6053e-01)	Acc@1  84.38 ( 86.58)	Acc@5  97.66 ( 98.59)
Epoch: [38][190/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1309e-01 (4.6005e-01)	Acc@1  90.62 ( 86.62)	Acc@5  99.22 ( 98.57)
Epoch: [38][200/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9502e-01 (4.5979e-01)	Acc@1  89.84 ( 86.63)	Acc@5  96.09 ( 98.56)
Epoch: [38][210/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6973e-01 (4.5913e-01)	Acc@1  85.94 ( 86.66)	Acc@5  98.44 ( 98.56)
Epoch: [38][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.3174e-01 (4.5820e-01)	Acc@1  81.25 ( 86.65)	Acc@5 100.00 ( 98.57)
Epoch: [38][230/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2334e-01 (4.5782e-01)	Acc@1  85.16 ( 86.65)	Acc@5  99.22 ( 98.56)
Epoch: [38][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6167e-01 (4.5791e-01)	Acc@1  86.72 ( 86.62)	Acc@5  99.22 ( 98.56)
Epoch: [38][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6987e-01 (4.5831e-01)	Acc@1  88.28 ( 86.67)	Acc@5  97.66 ( 98.53)
Epoch: [38][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.8398e-01 (4.5853e-01)	Acc@1  82.03 ( 86.65)	Acc@5  97.66 ( 98.52)
Epoch: [38][270/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7778e-01 (4.5854e-01)	Acc@1  84.38 ( 86.63)	Acc@5  98.44 ( 98.50)
Epoch: [38][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3701e-01 (4.5971e-01)	Acc@1  86.72 ( 86.56)	Acc@5  98.44 ( 98.47)
Epoch: [38][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.0586e-01 (4.5970e-01)	Acc@1  82.81 ( 86.54)	Acc@5  98.44 ( 98.48)
Epoch: [38][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.5225e-01 (4.5973e-01)	Acc@1  81.25 ( 86.54)	Acc@5  97.66 ( 98.48)
Epoch: [38][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.0781e-01 (4.5933e-01)	Acc@1  85.94 ( 86.54)	Acc@5  96.88 ( 98.48)
Epoch: [38][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.2334e-01 (4.6092e-01)	Acc@1  85.16 ( 86.45)	Acc@5  99.22 ( 98.47)
Epoch: [38][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.0161e-01 (4.6119e-01)	Acc@1  89.06 ( 86.41)	Acc@5  98.44 ( 98.45)
Epoch: [38][340/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.0195e-01 (4.6175e-01)	Acc@1  84.38 ( 86.40)	Acc@5  99.22 ( 98.45)
Epoch: [38][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.5386e-01 (4.6154e-01)	Acc@1  86.72 ( 86.42)	Acc@5 100.00 ( 98.46)
Epoch: [38][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.5312e-01 (4.6225e-01)	Acc@1  87.50 ( 86.39)	Acc@5  98.44 ( 98.47)
Epoch: [38][370/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.1670e-01 (4.6367e-01)	Acc@1  82.03 ( 86.35)	Acc@5  96.09 ( 98.47)
Epoch: [38][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.0879e-01 (4.6332e-01)	Acc@1  85.16 ( 86.35)	Acc@5  98.44 ( 98.47)
Epoch: [38][390/391]	Time  0.117 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.9502e-01 (4.6267e-01)	Acc@1  90.00 ( 86.36)	Acc@5  98.75 ( 98.47)
## e[38] optimizer.zero_grad (sum) time: 0.5466570854187012
## e[38]       loss.backward (sum) time: 12.616844177246094
## e[38]      optimizer.step (sum) time: 25.3739013671875
## epoch[38] training(only) time: 63.22266483306885
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 9.9365e-01 (9.9365e-01)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.1523e+00 (1.1611e+00)	Acc@1  69.00 ( 70.18)	Acc@5  91.00 ( 90.00)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 9.5117e-01 (1.1172e+00)	Acc@1  71.00 ( 71.10)	Acc@5  96.00 ( 91.29)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.2412e+00 (1.1407e+00)	Acc@1  64.00 ( 70.19)	Acc@5  90.00 ( 91.10)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0049e+00 (1.1263e+00)	Acc@1  72.00 ( 70.34)	Acc@5  90.00 ( 91.39)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.1533e+00 (1.1274e+00)	Acc@1  68.00 ( 70.37)	Acc@5  93.00 ( 91.41)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.1445e+00 (1.1142e+00)	Acc@1  67.00 ( 70.34)	Acc@5  88.00 ( 91.48)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.1689e+00 (1.1139e+00)	Acc@1  70.00 ( 70.31)	Acc@5  92.00 ( 91.39)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3867e+00 (1.1173e+00)	Acc@1  66.00 ( 70.27)	Acc@5  88.00 ( 91.40)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.2773e+00 (1.1079e+00)	Acc@1  66.00 ( 70.33)	Acc@5  91.00 ( 91.48)
 * Acc@1 70.510 Acc@5 91.510
### epoch[38] execution time: 68.16179847717285
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.323 ( 0.323)	Data  0.157 ( 0.157)	Loss 4.6240e-01 (4.6240e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [39][ 10/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.015)	Loss 5.2881e-01 (4.4063e-01)	Acc@1  85.94 ( 87.71)	Acc@5  96.88 ( 98.72)
Epoch: [39][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 5.0439e-01 (4.4267e-01)	Acc@1  88.28 ( 87.50)	Acc@5  98.44 ( 98.85)
Epoch: [39][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.1616e-01 (4.3648e-01)	Acc@1  91.41 ( 87.75)	Acc@5  98.44 ( 98.77)
Epoch: [39][ 40/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.005)	Loss 5.4248e-01 (4.3797e-01)	Acc@1  79.69 ( 87.75)	Acc@5  98.44 ( 98.70)
Epoch: [39][ 50/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.3042e-01 (4.3683e-01)	Acc@1  87.50 ( 87.73)	Acc@5  98.44 ( 98.73)
Epoch: [39][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.8501e-01 (4.3753e-01)	Acc@1  88.28 ( 87.60)	Acc@5  99.22 ( 98.72)
Epoch: [39][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.1611e-01 (4.3747e-01)	Acc@1  84.38 ( 87.57)	Acc@5  98.44 ( 98.77)
Epoch: [39][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.3604e-01 (4.4012e-01)	Acc@1  87.50 ( 87.44)	Acc@5  98.44 ( 98.76)
Epoch: [39][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.2539e-01 (4.4421e-01)	Acc@1  85.94 ( 87.28)	Acc@5  96.09 ( 98.71)
Epoch: [39][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.0356e-01 (4.4413e-01)	Acc@1  89.84 ( 87.41)	Acc@5  97.66 ( 98.62)
Epoch: [39][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2139e-01 (4.4331e-01)	Acc@1  86.72 ( 87.43)	Acc@5  99.22 ( 98.62)
Epoch: [39][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9126e-01 (4.4488e-01)	Acc@1  92.97 ( 87.38)	Acc@5  99.22 ( 98.60)
Epoch: [39][130/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5840e-01 (4.4302e-01)	Acc@1  88.28 ( 87.47)	Acc@5 100.00 ( 98.62)
Epoch: [39][140/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9307e-01 (4.4343e-01)	Acc@1  92.19 ( 87.45)	Acc@5  99.22 ( 98.62)
Epoch: [39][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1748e-01 (4.4338e-01)	Acc@1  86.72 ( 87.45)	Acc@5  98.44 ( 98.61)
Epoch: [39][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2627e-01 (4.4223e-01)	Acc@1  89.84 ( 87.46)	Acc@5 100.00 ( 98.63)
Epoch: [39][170/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0869e-01 (4.4252e-01)	Acc@1  89.06 ( 87.40)	Acc@5  97.66 ( 98.61)
Epoch: [39][180/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0698e-01 (4.4247e-01)	Acc@1  85.94 ( 87.36)	Acc@5 100.00 ( 98.60)
Epoch: [39][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (4.4224e-01)	Acc@1  86.72 ( 87.38)	Acc@5 100.00 ( 98.60)
Epoch: [39][200/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6533e-01 (4.4375e-01)	Acc@1  88.28 ( 87.27)	Acc@5  97.66 ( 98.59)
Epoch: [39][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0088e-01 (4.4222e-01)	Acc@1  89.06 ( 87.32)	Acc@5  99.22 ( 98.61)
Epoch: [39][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1982e-01 (4.4160e-01)	Acc@1  91.41 ( 87.32)	Acc@5  99.22 ( 98.61)
Epoch: [39][230/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4556e-01 (4.4174e-01)	Acc@1  87.50 ( 87.32)	Acc@5  99.22 ( 98.60)
Epoch: [39][240/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.8447e-01 (4.4292e-01)	Acc@1  82.03 ( 87.30)	Acc@5  97.66 ( 98.59)
Epoch: [39][250/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7095e-01 (4.4256e-01)	Acc@1  85.94 ( 87.32)	Acc@5  98.44 ( 98.59)
Epoch: [39][260/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6743e-01 (4.4145e-01)	Acc@1  89.84 ( 87.35)	Acc@5  97.66 ( 98.60)
Epoch: [39][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0293e-01 (4.4301e-01)	Acc@1  85.16 ( 87.28)	Acc@5  97.66 ( 98.59)
Epoch: [39][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3579e-01 (4.4306e-01)	Acc@1  85.94 ( 87.26)	Acc@5  99.22 ( 98.60)
Epoch: [39][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6021e-01 (4.4251e-01)	Acc@1  84.38 ( 87.28)	Acc@5  99.22 ( 98.61)
Epoch: [39][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6997e-01 (4.4096e-01)	Acc@1  89.06 ( 87.37)	Acc@5  98.44 ( 98.62)
Epoch: [39][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4604e-01 (4.3986e-01)	Acc@1  87.50 ( 87.37)	Acc@5  98.44 ( 98.63)
Epoch: [39][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1309e-01 (4.4033e-01)	Acc@1  89.84 ( 87.35)	Acc@5  99.22 ( 98.62)
Epoch: [39][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8477e-01 (4.4102e-01)	Acc@1  90.62 ( 87.35)	Acc@5  98.44 ( 98.62)
Epoch: [39][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9780e-01 (4.4266e-01)	Acc@1  87.50 ( 87.28)	Acc@5  98.44 ( 98.60)
Epoch: [39][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5767e-01 (4.4331e-01)	Acc@1  92.19 ( 87.25)	Acc@5  98.44 ( 98.60)
Epoch: [39][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.2285e-01 (4.4356e-01)	Acc@1  88.28 ( 87.24)	Acc@5  98.44 ( 98.61)
Epoch: [39][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.1260e-01 (4.4382e-01)	Acc@1  83.59 ( 87.19)	Acc@5 100.00 ( 98.61)
Epoch: [39][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.3188e-01 (4.4425e-01)	Acc@1  86.72 ( 87.18)	Acc@5  99.22 ( 98.62)
Epoch: [39][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.7617e-01 (4.4512e-01)	Acc@1  81.25 ( 87.13)	Acc@5  97.50 ( 98.62)
## e[39] optimizer.zero_grad (sum) time: 0.5509858131408691
## e[39]       loss.backward (sum) time: 12.69773817062378
## e[39]      optimizer.step (sum) time: 25.36126732826233
## epoch[39] training(only) time: 63.3094482421875
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.0039e+00 (1.0039e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.1611e+00 (1.1266e+00)	Acc@1  67.00 ( 70.73)	Acc@5  92.00 ( 89.82)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0254e+00 (1.1116e+00)	Acc@1  70.00 ( 71.62)	Acc@5  91.00 ( 90.57)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 1.3057e+00 (1.1431e+00)	Acc@1  58.00 ( 70.55)	Acc@5  91.00 ( 90.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.0020e+00 (1.1242e+00)	Acc@1  73.00 ( 70.66)	Acc@5  90.00 ( 91.34)
Test: [ 50/100]	Time  0.056 ( 0.050)	Loss 1.1582e+00 (1.1263e+00)	Acc@1  68.00 ( 70.47)	Acc@5  91.00 ( 91.20)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1318e+00 (1.1104e+00)	Acc@1  67.00 ( 70.49)	Acc@5  90.00 ( 91.30)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.1514e+00 (1.1095e+00)	Acc@1  68.00 ( 70.48)	Acc@5  93.00 ( 91.23)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.3447e+00 (1.1133e+00)	Acc@1  67.00 ( 70.36)	Acc@5  88.00 ( 91.25)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.2500e+00 (1.1034e+00)	Acc@1  65.00 ( 70.38)	Acc@5  92.00 ( 91.37)
 * Acc@1 70.530 Acc@5 91.440
### epoch[39] execution time: 68.25425386428833
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.318 ( 0.318)	Data  0.142 ( 0.142)	Loss 2.8516e-01 (2.8516e-01)	Acc@1  95.31 ( 95.31)	Acc@5  97.66 ( 97.66)
Epoch: [40][ 10/391]	Time  0.172 ( 0.176)	Data  0.001 ( 0.014)	Loss 3.1665e-01 (4.0978e-01)	Acc@1  90.62 ( 87.78)	Acc@5  99.22 ( 98.51)
Epoch: [40][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 3.9087e-01 (4.0383e-01)	Acc@1  87.50 ( 88.10)	Acc@5  99.22 ( 98.85)
Epoch: [40][ 30/391]	Time  0.159 ( 0.167)	Data  0.001 ( 0.005)	Loss 3.6646e-01 (4.0741e-01)	Acc@1  89.84 ( 87.78)	Acc@5  98.44 ( 98.79)
Epoch: [40][ 40/391]	Time  0.168 ( 0.166)	Data  0.001 ( 0.004)	Loss 3.5889e-01 (4.0005e-01)	Acc@1  89.06 ( 88.30)	Acc@5  99.22 ( 98.89)
Epoch: [40][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.0894e-01 (3.9833e-01)	Acc@1  85.16 ( 88.42)	Acc@5 100.00 ( 98.88)
Epoch: [40][ 60/391]	Time  0.171 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.6318e-01 (3.9881e-01)	Acc@1  94.53 ( 88.52)	Acc@5  99.22 ( 98.83)
Epoch: [40][ 70/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.4336e-01 (3.9819e-01)	Acc@1  85.94 ( 88.41)	Acc@5  99.22 ( 98.91)
Epoch: [40][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.4639e-01 (3.9855e-01)	Acc@1  80.47 ( 88.43)	Acc@5  98.44 ( 98.95)
Epoch: [40][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.5166e-01 (4.0095e-01)	Acc@1  85.94 ( 88.33)	Acc@5  97.66 ( 98.92)
Epoch: [40][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.7388e-01 (4.0395e-01)	Acc@1  84.38 ( 88.21)	Acc@5  98.44 ( 98.93)
Epoch: [40][110/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1821e-01 (4.0720e-01)	Acc@1  89.06 ( 88.20)	Acc@5  98.44 ( 98.89)
Epoch: [40][120/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.6582e-01 (4.0987e-01)	Acc@1  85.16 ( 88.06)	Acc@5  97.66 ( 98.86)
Epoch: [40][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9819e-01 (4.1144e-01)	Acc@1  90.62 ( 87.98)	Acc@5  99.22 ( 98.85)
Epoch: [40][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 5.1855e-01 (4.1418e-01)	Acc@1  81.25 ( 87.87)	Acc@5  97.66 ( 98.87)
Epoch: [40][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7451e-01 (4.1349e-01)	Acc@1  89.84 ( 87.94)	Acc@5 100.00 ( 98.89)
Epoch: [40][160/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0195e-01 (4.1520e-01)	Acc@1  78.91 ( 87.85)	Acc@5  97.66 ( 98.87)
Epoch: [40][170/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1772e-01 (4.1413e-01)	Acc@1  88.28 ( 87.88)	Acc@5 100.00 ( 98.89)
Epoch: [40][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4912e-01 (4.1379e-01)	Acc@1  89.06 ( 87.93)	Acc@5  99.22 ( 98.90)
Epoch: [40][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8550e-01 (4.1451e-01)	Acc@1  89.06 ( 87.93)	Acc@5  97.66 ( 98.89)
Epoch: [40][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0029e-01 (4.1470e-01)	Acc@1  91.41 ( 87.89)	Acc@5 100.00 ( 98.90)
Epoch: [40][210/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1675e-01 (4.1415e-01)	Acc@1  87.50 ( 87.90)	Acc@5  99.22 ( 98.90)
Epoch: [40][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8550e-01 (4.1404e-01)	Acc@1  92.19 ( 87.94)	Acc@5  99.22 ( 98.90)
Epoch: [40][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6279e-01 (4.1465e-01)	Acc@1  91.41 ( 87.94)	Acc@5 100.00 ( 98.88)
Epoch: [40][240/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6484e-01 (4.1615e-01)	Acc@1  85.94 ( 87.90)	Acc@5  97.66 ( 98.88)
Epoch: [40][250/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9902e-01 (4.1623e-01)	Acc@1  86.72 ( 87.89)	Acc@5  97.66 ( 98.86)
Epoch: [40][260/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (4.1568e-01)	Acc@1  84.38 ( 87.91)	Acc@5  98.44 ( 98.84)
Epoch: [40][270/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.7046e-01 (4.1637e-01)	Acc@1  85.16 ( 87.89)	Acc@5  98.44 ( 98.84)
Epoch: [40][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3384e-01 (4.1628e-01)	Acc@1  85.94 ( 87.89)	Acc@5  98.44 ( 98.84)
Epoch: [40][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9414e-01 (4.1723e-01)	Acc@1  84.38 ( 87.87)	Acc@5  98.44 ( 98.83)
Epoch: [40][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2065e-01 (4.1703e-01)	Acc@1  85.94 ( 87.91)	Acc@5  99.22 ( 98.83)
Epoch: [40][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.8926e-01 (4.1728e-01)	Acc@1  85.94 ( 87.90)	Acc@5  96.88 ( 98.83)
Epoch: [40][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.7671e-01 (4.1801e-01)	Acc@1  86.72 ( 87.84)	Acc@5 100.00 ( 98.83)
Epoch: [40][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.3984e-01 (4.1806e-01)	Acc@1  92.19 ( 87.84)	Acc@5 100.00 ( 98.83)
Epoch: [40][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.1714e-01 (4.1768e-01)	Acc@1  90.62 ( 87.84)	Acc@5 100.00 ( 98.84)
Epoch: [40][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.5776e-01 (4.1760e-01)	Acc@1  85.94 ( 87.84)	Acc@5  98.44 ( 98.84)
Epoch: [40][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.1162e-01 (4.1744e-01)	Acc@1  88.28 ( 87.85)	Acc@5  99.22 ( 98.84)
Epoch: [40][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.5898e-01 (4.1735e-01)	Acc@1  86.72 ( 87.86)	Acc@5  97.66 ( 98.84)
Epoch: [40][380/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.0146e-01 (4.1791e-01)	Acc@1  82.03 ( 87.82)	Acc@5  99.22 ( 98.84)
Epoch: [40][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.8169e-01 (4.1791e-01)	Acc@1  87.50 ( 87.81)	Acc@5 100.00 ( 98.83)
## e[40] optimizer.zero_grad (sum) time: 0.5418455600738525
## e[40]       loss.backward (sum) time: 12.556860208511353
## e[40]      optimizer.step (sum) time: 25.4493989944458
## epoch[40] training(only) time: 63.276376485824585
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.0332e+00 (1.0332e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.1309e+00 (1.1573e+00)	Acc@1  69.00 ( 70.36)	Acc@5  93.00 ( 90.00)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0088e+00 (1.1295e+00)	Acc@1  71.00 ( 71.19)	Acc@5  91.00 ( 91.05)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.2617e+00 (1.1616e+00)	Acc@1  60.00 ( 70.16)	Acc@5  91.00 ( 90.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0205e+00 (1.1395e+00)	Acc@1  73.00 ( 70.56)	Acc@5  91.00 ( 91.20)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.1357e+00 (1.1410e+00)	Acc@1  68.00 ( 70.45)	Acc@5  93.00 ( 91.24)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.1152e+00 (1.1251e+00)	Acc@1  68.00 ( 70.52)	Acc@5  90.00 ( 91.46)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2256e+00 (1.1256e+00)	Acc@1  66.00 ( 70.34)	Acc@5  91.00 ( 91.48)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3711e+00 (1.1249e+00)	Acc@1  68.00 ( 70.35)	Acc@5  88.00 ( 91.40)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.2012e+00 (1.1126e+00)	Acc@1  64.00 ( 70.38)	Acc@5  92.00 ( 91.53)
 * Acc@1 70.490 Acc@5 91.560
### epoch[40] execution time: 68.22800707817078
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.322 ( 0.322)	Data  0.149 ( 0.149)	Loss 4.2554e-01 (4.2554e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 4.0576e-01 (3.6834e-01)	Acc@1  86.72 ( 89.63)	Acc@5  98.44 ( 99.29)
Epoch: [41][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 4.3286e-01 (3.7753e-01)	Acc@1  86.72 ( 89.29)	Acc@5  99.22 ( 99.22)
Epoch: [41][ 30/391]	Time  0.164 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.6304e-01 (3.8647e-01)	Acc@1  91.41 ( 89.21)	Acc@5  99.22 ( 99.17)
Epoch: [41][ 40/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.6499e-01 (3.8793e-01)	Acc@1  90.62 ( 88.93)	Acc@5 100.00 ( 99.20)
Epoch: [41][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.5068e-01 (3.9227e-01)	Acc@1  86.72 ( 88.77)	Acc@5  99.22 ( 99.19)
Epoch: [41][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 5.0977e-01 (3.9136e-01)	Acc@1  81.25 ( 88.88)	Acc@5  98.44 ( 99.18)
Epoch: [41][ 70/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.2822e-01 (3.9497e-01)	Acc@1  86.72 ( 88.70)	Acc@5  97.66 ( 99.11)
Epoch: [41][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.4668e-01 (3.9485e-01)	Acc@1  93.75 ( 88.78)	Acc@5  98.44 ( 99.08)
Epoch: [41][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.1064e-01 (3.9484e-01)	Acc@1  89.06 ( 88.92)	Acc@5  99.22 ( 99.03)
Epoch: [41][100/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1172e-01 (3.9582e-01)	Acc@1  85.16 ( 88.82)	Acc@5  98.44 ( 99.00)
Epoch: [41][110/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3066e-01 (3.9750e-01)	Acc@1  85.94 ( 88.65)	Acc@5  99.22 ( 98.99)
Epoch: [41][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.9316e-01 (4.0095e-01)	Acc@1  85.94 ( 88.51)	Acc@5  96.88 ( 98.93)
Epoch: [41][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9233e-01 (4.0355e-01)	Acc@1  88.28 ( 88.42)	Acc@5  99.22 ( 98.89)
Epoch: [41][140/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1323e-01 (4.0380e-01)	Acc@1  89.84 ( 88.42)	Acc@5 100.00 ( 98.89)
Epoch: [41][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8721e-01 (4.0301e-01)	Acc@1  89.84 ( 88.50)	Acc@5  99.22 ( 98.89)
Epoch: [41][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4849e-01 (4.0451e-01)	Acc@1  88.28 ( 88.45)	Acc@5  96.88 ( 98.85)
Epoch: [41][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1699e-01 (4.0275e-01)	Acc@1  89.84 ( 88.54)	Acc@5  99.22 ( 98.85)
Epoch: [41][180/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2666e-01 (4.0139e-01)	Acc@1  89.06 ( 88.57)	Acc@5 100.00 ( 98.85)
Epoch: [41][190/391]	Time  0.175 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6484e-01 (4.0121e-01)	Acc@1  85.94 ( 88.53)	Acc@5  99.22 ( 98.88)
Epoch: [41][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9600e-01 (4.0299e-01)	Acc@1  89.06 ( 88.50)	Acc@5  99.22 ( 98.85)
Epoch: [41][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4961e-01 (4.0471e-01)	Acc@1  89.06 ( 88.42)	Acc@5  99.22 ( 98.84)
Epoch: [41][220/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8965e-01 (4.0510e-01)	Acc@1  89.06 ( 88.39)	Acc@5  98.44 ( 98.85)
Epoch: [41][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2798e-01 (4.0551e-01)	Acc@1  86.72 ( 88.36)	Acc@5  99.22 ( 98.85)
Epoch: [41][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0835e-01 (4.0381e-01)	Acc@1  92.97 ( 88.42)	Acc@5 100.00 ( 98.87)
Epoch: [41][250/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9868e-01 (4.0499e-01)	Acc@1  89.06 ( 88.37)	Acc@5  98.44 ( 98.85)
Epoch: [41][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5449e-01 (4.0569e-01)	Acc@1  89.06 ( 88.33)	Acc@5 100.00 ( 98.83)
Epoch: [41][270/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3496e-01 (4.0582e-01)	Acc@1  91.41 ( 88.29)	Acc@5 100.00 ( 98.84)
Epoch: [41][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8818e-01 (4.0591e-01)	Acc@1  90.62 ( 88.29)	Acc@5 100.00 ( 98.85)
Epoch: [41][290/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4204e-01 (4.0529e-01)	Acc@1  87.50 ( 88.30)	Acc@5  99.22 ( 98.85)
Epoch: [41][300/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7646e-01 (4.0544e-01)	Acc@1  91.41 ( 88.32)	Acc@5  99.22 ( 98.87)
Epoch: [41][310/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.2188e-01 (4.0449e-01)	Acc@1  90.62 ( 88.35)	Acc@5  98.44 ( 98.88)
Epoch: [41][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.2896e-01 (4.0361e-01)	Acc@1  86.72 ( 88.42)	Acc@5  99.22 ( 98.87)
Epoch: [41][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.0635e-01 (4.0365e-01)	Acc@1  84.38 ( 88.41)	Acc@5  99.22 ( 98.86)
Epoch: [41][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.1016e-01 (4.0402e-01)	Acc@1  88.28 ( 88.38)	Acc@5  99.22 ( 98.87)
Epoch: [41][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.2666e-01 (4.0419e-01)	Acc@1  90.62 ( 88.37)	Acc@5  99.22 ( 98.85)
Epoch: [41][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.2456e-01 (4.0438e-01)	Acc@1  88.28 ( 88.36)	Acc@5  99.22 ( 98.86)
Epoch: [41][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.0601e-01 (4.0466e-01)	Acc@1  88.28 ( 88.33)	Acc@5  97.66 ( 98.85)
Epoch: [41][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.2944e-01 (4.0515e-01)	Acc@1  86.72 ( 88.31)	Acc@5  98.44 ( 98.84)
Epoch: [41][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.4238e-01 (4.0571e-01)	Acc@1  85.00 ( 88.27)	Acc@5  98.75 ( 98.84)
## e[41] optimizer.zero_grad (sum) time: 0.5574345588684082
## e[41]       loss.backward (sum) time: 12.601602792739868
## e[41]      optimizer.step (sum) time: 25.429081916809082
## epoch[41] training(only) time: 63.45604753494263
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 1.0273e+00 (1.0273e+00)	Acc@1  75.00 ( 75.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 1.2305e+00 (1.1680e+00)	Acc@1  67.00 ( 70.00)	Acc@5  91.00 ( 90.27)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.0303e+00 (1.1340e+00)	Acc@1  70.00 ( 71.24)	Acc@5  93.00 ( 91.10)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.3252e+00 (1.1662e+00)	Acc@1  61.00 ( 70.32)	Acc@5  90.00 ( 90.87)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 9.8779e-01 (1.1464e+00)	Acc@1  70.00 ( 70.44)	Acc@5  90.00 ( 91.22)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.1719e+00 (1.1476e+00)	Acc@1  67.00 ( 70.33)	Acc@5  92.00 ( 91.14)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1201e+00 (1.1330e+00)	Acc@1  67.00 ( 70.18)	Acc@5  90.00 ( 91.33)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.1553e+00 (1.1323e+00)	Acc@1  66.00 ( 70.21)	Acc@5  95.00 ( 91.34)
Test: [ 80/100]	Time  0.057 ( 0.049)	Loss 1.3301e+00 (1.1325e+00)	Acc@1  71.00 ( 70.20)	Acc@5  89.00 ( 91.38)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2412e+00 (1.1201e+00)	Acc@1  65.00 ( 70.31)	Acc@5  91.00 ( 91.52)
 * Acc@1 70.610 Acc@5 91.570
### epoch[41] execution time: 68.39188933372498
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.327 ( 0.327)	Data  0.150 ( 0.150)	Loss 4.2627e-01 (4.2627e-01)	Acc@1  82.81 ( 82.81)	Acc@5  99.22 ( 99.22)
Epoch: [42][ 10/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.015)	Loss 5.4395e-01 (4.2711e-01)	Acc@1  84.38 ( 87.14)	Acc@5  97.66 ( 99.15)
Epoch: [42][ 20/391]	Time  0.162 ( 0.169)	Data  0.002 ( 0.008)	Loss 2.9932e-01 (4.0596e-01)	Acc@1  91.41 ( 88.02)	Acc@5 100.00 ( 99.03)
Epoch: [42][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 4.0649e-01 (3.9545e-01)	Acc@1  87.50 ( 88.63)	Acc@5  98.44 ( 98.89)
Epoch: [42][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.9111e-01 (3.9602e-01)	Acc@1  88.28 ( 88.62)	Acc@5  98.44 ( 98.84)
Epoch: [42][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.5376e-01 (3.8639e-01)	Acc@1  90.62 ( 88.83)	Acc@5  98.44 ( 98.91)
Epoch: [42][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.004)	Loss 5.0195e-01 (3.8484e-01)	Acc@1  84.38 ( 89.08)	Acc@5  96.88 ( 98.90)
Epoch: [42][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.8965e-01 (3.8222e-01)	Acc@1  90.62 ( 89.15)	Acc@5  98.44 ( 98.91)
Epoch: [42][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 4.2944e-01 (3.8754e-01)	Acc@1  85.16 ( 88.99)	Acc@5  99.22 ( 98.89)
Epoch: [42][ 90/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.003)	Loss 2.9858e-01 (3.8506e-01)	Acc@1  91.41 ( 89.05)	Acc@5  99.22 ( 98.89)
Epoch: [42][100/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.003)	Loss 4.2163e-01 (3.8464e-01)	Acc@1  89.84 ( 89.04)	Acc@5  98.44 ( 98.90)
Epoch: [42][110/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7622e-01 (3.8232e-01)	Acc@1  86.72 ( 89.09)	Acc@5  99.22 ( 98.92)
Epoch: [42][120/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0186e-01 (3.8152e-01)	Acc@1  89.06 ( 89.09)	Acc@5  98.44 ( 98.93)
Epoch: [42][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1909e-01 (3.7993e-01)	Acc@1  89.06 ( 89.10)	Acc@5 100.00 ( 98.96)
Epoch: [42][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4458e-01 (3.7928e-01)	Acc@1  85.16 ( 89.11)	Acc@5 100.00 ( 98.98)
Epoch: [42][150/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0234e-01 (3.8029e-01)	Acc@1  87.50 ( 89.05)	Acc@5  97.66 ( 98.97)
Epoch: [42][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7378e-01 (3.8121e-01)	Acc@1  88.28 ( 89.02)	Acc@5  98.44 ( 98.96)
Epoch: [42][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1455e-01 (3.8254e-01)	Acc@1  85.94 ( 89.00)	Acc@5  96.88 ( 98.93)
Epoch: [42][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1812e-01 (3.8243e-01)	Acc@1  90.62 ( 88.95)	Acc@5  98.44 ( 98.95)
Epoch: [42][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9419e-01 (3.8347e-01)	Acc@1  91.41 ( 88.91)	Acc@5  99.22 ( 98.94)
Epoch: [42][200/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5474e-01 (3.8311e-01)	Acc@1  91.41 ( 88.96)	Acc@5 100.00 ( 98.94)
Epoch: [42][210/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4668e-01 (3.8273e-01)	Acc@1  90.62 ( 88.98)	Acc@5 100.00 ( 98.97)
Epoch: [42][220/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.6362e-01 (3.8393e-01)	Acc@1  83.59 ( 88.91)	Acc@5  99.22 ( 98.97)
Epoch: [42][230/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.8057e-01 (3.8528e-01)	Acc@1  81.25 ( 88.86)	Acc@5  99.22 ( 98.98)
Epoch: [42][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8354e-01 (3.8589e-01)	Acc@1  89.84 ( 88.86)	Acc@5  99.22 ( 98.95)
Epoch: [42][250/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1626e-01 (3.8590e-01)	Acc@1  84.38 ( 88.84)	Acc@5  99.22 ( 98.95)
Epoch: [42][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9551e-01 (3.8536e-01)	Acc@1  87.50 ( 88.85)	Acc@5  98.44 ( 98.96)
Epoch: [42][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9810e-01 (3.8523e-01)	Acc@1  92.19 ( 88.83)	Acc@5 100.00 ( 98.96)
Epoch: [42][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5474e-01 (3.8604e-01)	Acc@1  89.06 ( 88.77)	Acc@5 100.00 ( 98.97)
Epoch: [42][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0835e-01 (3.8585e-01)	Acc@1  92.19 ( 88.78)	Acc@5 100.00 ( 98.97)
Epoch: [42][300/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3262e-01 (3.8752e-01)	Acc@1  88.28 ( 88.75)	Acc@5  97.66 ( 98.96)
Epoch: [42][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4717e-01 (3.8705e-01)	Acc@1  89.06 ( 88.78)	Acc@5  99.22 ( 98.97)
Epoch: [42][320/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3301e-01 (3.8708e-01)	Acc@1  90.62 ( 88.79)	Acc@5  99.22 ( 98.97)
Epoch: [42][330/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0234e-01 (3.8673e-01)	Acc@1  87.50 ( 88.80)	Acc@5  99.22 ( 98.96)
Epoch: [42][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2896e-01 (3.8713e-01)	Acc@1  86.72 ( 88.78)	Acc@5  99.22 ( 98.96)
Epoch: [42][350/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3286e-01 (3.8725e-01)	Acc@1  87.50 ( 88.82)	Acc@5  99.22 ( 98.97)
Epoch: [42][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.8857e-01 (3.8800e-01)	Acc@1  90.62 ( 88.79)	Acc@5 100.00 ( 98.96)
Epoch: [42][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 5.4785e-01 (3.8878e-01)	Acc@1  85.94 ( 88.78)	Acc@5  97.66 ( 98.94)
Epoch: [42][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.3813e-01 (3.8892e-01)	Acc@1  89.84 ( 88.76)	Acc@5  99.22 ( 98.94)
Epoch: [42][390/391]	Time  0.115 ( 0.161)	Data  0.001 ( 0.001)	Loss 3.5669e-01 (3.8994e-01)	Acc@1  93.75 ( 88.75)	Acc@5  98.75 ( 98.94)
## e[42] optimizer.zero_grad (sum) time: 0.5298879146575928
## e[42]       loss.backward (sum) time: 12.559417724609375
## e[42]      optimizer.step (sum) time: 25.44017767906189
## epoch[42] training(only) time: 63.16484832763672
# Switched to evaluate mode...
Test: [  0/100]	Time  0.179 ( 0.179)	Loss 1.0869e+00 (1.0869e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.2852e+00 (1.1776e+00)	Acc@1  67.00 ( 70.55)	Acc@5  91.00 ( 90.36)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.0840e+00 (1.1463e+00)	Acc@1  71.00 ( 71.14)	Acc@5  92.00 ( 91.19)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.3154e+00 (1.1799e+00)	Acc@1  58.00 ( 70.10)	Acc@5  91.00 ( 90.87)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 9.8779e-01 (1.1609e+00)	Acc@1  72.00 ( 70.12)	Acc@5  89.00 ( 91.34)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.1152e+00 (1.1561e+00)	Acc@1  68.00 ( 70.29)	Acc@5  93.00 ( 91.35)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1162e+00 (1.1416e+00)	Acc@1  66.00 ( 70.26)	Acc@5  91.00 ( 91.46)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.1670e+00 (1.1415e+00)	Acc@1  69.00 ( 70.21)	Acc@5  94.00 ( 91.56)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.4072e+00 (1.1428e+00)	Acc@1  68.00 ( 70.20)	Acc@5  88.00 ( 91.54)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3086e+00 (1.1322e+00)	Acc@1  65.00 ( 70.30)	Acc@5  90.00 ( 91.59)
 * Acc@1 70.520 Acc@5 91.580
### epoch[42] execution time: 68.1092119216919
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.332 ( 0.332)	Data  0.158 ( 0.158)	Loss 3.8135e-01 (3.8135e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [43][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 2.9883e-01 (3.3957e-01)	Acc@1  89.06 ( 90.70)	Acc@5  99.22 ( 99.43)
Epoch: [43][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.7734e-01 (3.5073e-01)	Acc@1  92.19 ( 90.55)	Acc@5 100.00 ( 99.18)
Epoch: [43][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.4082e-01 (3.4913e-01)	Acc@1  89.84 ( 90.37)	Acc@5 100.00 ( 99.32)
Epoch: [43][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.4121e-01 (3.5590e-01)	Acc@1  92.97 ( 90.07)	Acc@5 100.00 ( 99.22)
Epoch: [43][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.3091e-01 (3.6107e-01)	Acc@1  89.84 ( 90.03)	Acc@5  98.44 ( 99.19)
Epoch: [43][ 60/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 3.0103e-01 (3.6089e-01)	Acc@1  91.41 ( 89.95)	Acc@5 100.00 ( 99.14)
Epoch: [43][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.6240e-01 (3.5928e-01)	Acc@1  90.62 ( 90.00)	Acc@5  97.66 ( 99.14)
Epoch: [43][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.7466e-01 (3.5788e-01)	Acc@1  92.19 ( 90.05)	Acc@5 100.00 ( 99.14)
Epoch: [43][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.1567e-01 (3.5995e-01)	Acc@1  93.75 ( 90.01)	Acc@5  99.22 ( 99.14)
Epoch: [43][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.1641e-01 (3.6270e-01)	Acc@1  92.19 ( 89.98)	Acc@5  99.22 ( 99.12)
Epoch: [43][110/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4106e-01 (3.6401e-01)	Acc@1  89.84 ( 89.89)	Acc@5  97.66 ( 99.08)
Epoch: [43][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3523e-01 (3.6467e-01)	Acc@1  93.75 ( 89.86)	Acc@5 100.00 ( 99.10)
Epoch: [43][130/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9492e-01 (3.6322e-01)	Acc@1  92.19 ( 89.86)	Acc@5  98.44 ( 99.11)
Epoch: [43][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8086e-01 (3.6500e-01)	Acc@1  89.84 ( 89.77)	Acc@5  97.66 ( 99.09)
Epoch: [43][150/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9722e-01 (3.6674e-01)	Acc@1  90.62 ( 89.65)	Acc@5  96.88 ( 99.06)
Epoch: [43][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8589e-01 (3.6615e-01)	Acc@1  92.19 ( 89.59)	Acc@5 100.00 ( 99.08)
Epoch: [43][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.7134e-01 (3.6522e-01)	Acc@1  91.41 ( 89.66)	Acc@5  99.22 ( 99.08)
Epoch: [43][180/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9126e-01 (3.6653e-01)	Acc@1  91.41 ( 89.60)	Acc@5  99.22 ( 99.05)
Epoch: [43][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.3760e-01 (3.6766e-01)	Acc@1  83.59 ( 89.53)	Acc@5  96.09 ( 99.05)
Epoch: [43][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.1465e-01 (3.6798e-01)	Acc@1  84.38 ( 89.54)	Acc@5  98.44 ( 99.07)
Epoch: [43][210/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0884e-01 (3.6647e-01)	Acc@1  92.97 ( 89.60)	Acc@5  99.22 ( 99.08)
Epoch: [43][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8110e-01 (3.6601e-01)	Acc@1  88.28 ( 89.61)	Acc@5  98.44 ( 99.07)
Epoch: [43][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0518e-01 (3.6491e-01)	Acc@1  91.41 ( 89.67)	Acc@5  99.22 ( 99.07)
Epoch: [43][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6768e-01 (3.6459e-01)	Acc@1  90.62 ( 89.68)	Acc@5  98.44 ( 99.07)
Epoch: [43][250/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7915e-01 (3.6418e-01)	Acc@1  89.06 ( 89.69)	Acc@5 100.00 ( 99.06)
Epoch: [43][260/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5776e-01 (3.6559e-01)	Acc@1  85.94 ( 89.63)	Acc@5  98.44 ( 99.06)
Epoch: [43][270/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7573e-01 (3.6619e-01)	Acc@1  89.06 ( 89.60)	Acc@5  99.22 ( 99.06)
Epoch: [43][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2666e-01 (3.6623e-01)	Acc@1  92.97 ( 89.61)	Acc@5  99.22 ( 99.06)
Epoch: [43][290/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3872e-01 (3.6689e-01)	Acc@1  87.50 ( 89.58)	Acc@5  97.66 ( 99.06)
Epoch: [43][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8306e-01 (3.6739e-01)	Acc@1  88.28 ( 89.55)	Acc@5  99.22 ( 99.06)
Epoch: [43][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9062e-01 (3.6782e-01)	Acc@1  86.72 ( 89.52)	Acc@5  98.44 ( 99.05)
Epoch: [43][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5474e-01 (3.6851e-01)	Acc@1  88.28 ( 89.46)	Acc@5  98.44 ( 99.05)
Epoch: [43][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.4360e-01 (3.6864e-01)	Acc@1  87.50 ( 89.44)	Acc@5  99.22 ( 99.06)
Epoch: [43][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6060e-01 (3.6870e-01)	Acc@1  92.19 ( 89.40)	Acc@5  97.66 ( 99.07)
Epoch: [43][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 5.6055e-01 (3.6906e-01)	Acc@1  82.81 ( 89.40)	Acc@5  95.31 ( 99.05)
Epoch: [43][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7476e-01 (3.6967e-01)	Acc@1  89.06 ( 89.39)	Acc@5  99.22 ( 99.04)
Epoch: [43][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.8394e-01 (3.6987e-01)	Acc@1  93.75 ( 89.42)	Acc@5 100.00 ( 99.04)
Epoch: [43][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.3140e-01 (3.6992e-01)	Acc@1  88.28 ( 89.40)	Acc@5  98.44 ( 99.05)
Epoch: [43][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.3457e-01 (3.6935e-01)	Acc@1  86.25 ( 89.42)	Acc@5  97.50 ( 99.05)
## e[43] optimizer.zero_grad (sum) time: 0.5408070087432861
## e[43]       loss.backward (sum) time: 12.614842176437378
## e[43]      optimizer.step (sum) time: 25.43045449256897
## epoch[43] training(only) time: 63.31800961494446
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.0332e+00 (1.0332e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.2578e+00 (1.1658e+00)	Acc@1  67.00 ( 70.82)	Acc@5  91.00 ( 90.27)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.1191e+00 (1.1438e+00)	Acc@1  69.00 ( 71.14)	Acc@5  91.00 ( 90.71)
Test: [ 30/100]	Time  0.046 ( 0.053)	Loss 1.3828e+00 (1.1703e+00)	Acc@1  62.00 ( 70.32)	Acc@5  90.00 ( 90.71)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0312e+00 (1.1570e+00)	Acc@1  70.00 ( 70.22)	Acc@5  89.00 ( 91.07)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.1689e+00 (1.1586e+00)	Acc@1  69.00 ( 70.18)	Acc@5  91.00 ( 91.04)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.1191e+00 (1.1438e+00)	Acc@1  69.00 ( 70.18)	Acc@5  91.00 ( 91.05)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2334e+00 (1.1441e+00)	Acc@1  68.00 ( 70.14)	Acc@5  92.00 ( 91.04)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4180e+00 (1.1473e+00)	Acc@1  68.00 ( 70.11)	Acc@5  88.00 ( 90.99)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.3086e+00 (1.1352e+00)	Acc@1  68.00 ( 70.26)	Acc@5  88.00 ( 91.13)
 * Acc@1 70.430 Acc@5 91.150
### epoch[43] execution time: 68.30032873153687
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.344 ( 0.344)	Data  0.163 ( 0.163)	Loss 2.9761e-01 (2.9761e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.016)	Loss 3.4717e-01 (3.2038e-01)	Acc@1  90.62 ( 91.69)	Acc@5  98.44 ( 99.29)
Epoch: [44][ 20/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.009)	Loss 3.3252e-01 (3.2547e-01)	Acc@1  92.97 ( 91.44)	Acc@5  98.44 ( 99.29)
Epoch: [44][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 3.5327e-01 (3.2486e-01)	Acc@1  90.62 ( 91.33)	Acc@5  99.22 ( 99.40)
Epoch: [44][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.9639e-01 (3.2166e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.45)
Epoch: [44][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.2104e-01 (3.3179e-01)	Acc@1  90.62 ( 91.02)	Acc@5  99.22 ( 99.43)
Epoch: [44][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 4.3530e-01 (3.2898e-01)	Acc@1  88.28 ( 91.16)	Acc@5  97.66 ( 99.37)
Epoch: [44][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.1104e-01 (3.3060e-01)	Acc@1  90.62 ( 90.98)	Acc@5 100.00 ( 99.39)
Epoch: [44][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.6538e-01 (3.2967e-01)	Acc@1  92.19 ( 90.98)	Acc@5 100.00 ( 99.38)
Epoch: [44][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.2056e-01 (3.3200e-01)	Acc@1  91.41 ( 90.85)	Acc@5 100.00 ( 99.37)
Epoch: [44][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.7622e-01 (3.3396e-01)	Acc@1  89.84 ( 90.71)	Acc@5  99.22 ( 99.38)
Epoch: [44][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.4399e-01 (3.3492e-01)	Acc@1  92.19 ( 90.71)	Acc@5  98.44 ( 99.35)
Epoch: [44][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7563e-01 (3.3572e-01)	Acc@1  89.06 ( 90.61)	Acc@5  99.22 ( 99.36)
Epoch: [44][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8345e-01 (3.3672e-01)	Acc@1  92.97 ( 90.54)	Acc@5  99.22 ( 99.37)
Epoch: [44][140/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5220e-01 (3.3783e-01)	Acc@1  93.75 ( 90.54)	Acc@5 100.00 ( 99.36)
Epoch: [44][150/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2202e-01 (3.3867e-01)	Acc@1  92.19 ( 90.55)	Acc@5  99.22 ( 99.36)
Epoch: [44][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8296e-01 (3.3908e-01)	Acc@1  92.19 ( 90.53)	Acc@5  98.44 ( 99.37)
Epoch: [44][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0200e-01 (3.4018e-01)	Acc@1  92.19 ( 90.47)	Acc@5  99.22 ( 99.36)
Epoch: [44][180/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1055e-01 (3.4002e-01)	Acc@1  90.62 ( 90.45)	Acc@5  98.44 ( 99.34)
Epoch: [44][190/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7695e-01 (3.3960e-01)	Acc@1  88.28 ( 90.45)	Acc@5 100.00 ( 99.35)
Epoch: [44][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7671e-01 (3.4180e-01)	Acc@1  88.28 ( 90.35)	Acc@5  97.66 ( 99.34)
Epoch: [44][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0112e-01 (3.4289e-01)	Acc@1  86.72 ( 90.25)	Acc@5 100.00 ( 99.35)
Epoch: [44][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8623e-01 (3.4349e-01)	Acc@1  89.06 ( 90.26)	Acc@5  99.22 ( 99.34)
Epoch: [44][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7866e-01 (3.4317e-01)	Acc@1  89.06 ( 90.28)	Acc@5  99.22 ( 99.35)
Epoch: [44][240/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1519e-01 (3.4455e-01)	Acc@1  90.62 ( 90.26)	Acc@5  99.22 ( 99.33)
Epoch: [44][250/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4619e-01 (3.4382e-01)	Acc@1  91.41 ( 90.29)	Acc@5  97.66 ( 99.31)
Epoch: [44][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.1016e-01 (3.4399e-01)	Acc@1  89.84 ( 90.31)	Acc@5  99.22 ( 99.31)
Epoch: [44][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2837e-01 (3.4456e-01)	Acc@1  89.06 ( 90.28)	Acc@5 100.00 ( 99.31)
Epoch: [44][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3457e-01 (3.4527e-01)	Acc@1  85.16 ( 90.24)	Acc@5 100.00 ( 99.31)
Epoch: [44][290/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5386e-01 (3.4712e-01)	Acc@1  85.94 ( 90.16)	Acc@5  99.22 ( 99.29)
Epoch: [44][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7109e-01 (3.4733e-01)	Acc@1  89.84 ( 90.15)	Acc@5  97.66 ( 99.29)
Epoch: [44][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.5142e-01 (3.4810e-01)	Acc@1  84.38 ( 90.09)	Acc@5 100.00 ( 99.30)
Epoch: [44][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.4546e-01 (3.4876e-01)	Acc@1  89.06 ( 90.10)	Acc@5 100.00 ( 99.30)
Epoch: [44][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.8906e-01 (3.4782e-01)	Acc@1  91.41 ( 90.12)	Acc@5 100.00 ( 99.30)
Epoch: [44][340/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2534e-01 (3.4781e-01)	Acc@1  94.53 ( 90.11)	Acc@5 100.00 ( 99.29)
Epoch: [44][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.1382e-01 (3.4829e-01)	Acc@1  86.72 ( 90.11)	Acc@5  98.44 ( 99.28)
Epoch: [44][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.3447e-01 (3.4815e-01)	Acc@1  91.41 ( 90.11)	Acc@5  97.66 ( 99.28)
Epoch: [44][370/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.9209e-01 (3.4899e-01)	Acc@1  87.50 ( 90.07)	Acc@5 100.00 ( 99.28)
Epoch: [44][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.0737e-01 (3.4929e-01)	Acc@1  89.84 ( 90.03)	Acc@5  99.22 ( 99.28)
Epoch: [44][390/391]	Time  0.123 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.1572e-01 (3.5014e-01)	Acc@1  83.75 ( 90.02)	Acc@5  96.25 ( 99.28)
## e[44] optimizer.zero_grad (sum) time: 0.5497872829437256
## e[44]       loss.backward (sum) time: 12.552001953125
## e[44]      optimizer.step (sum) time: 25.465725421905518
## epoch[44] training(only) time: 63.35689353942871
# Switched to evaluate mode...
Test: [  0/100]	Time  0.181 ( 0.181)	Loss 1.0781e+00 (1.0781e+00)	Acc@1  70.00 ( 70.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 1.2666e+00 (1.1878e+00)	Acc@1  65.00 ( 71.00)	Acc@5  91.00 ( 89.82)
Test: [ 20/100]	Time  0.060 ( 0.055)	Loss 1.0312e+00 (1.1561e+00)	Acc@1  71.00 ( 71.14)	Acc@5  90.00 ( 90.71)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4189e+00 (1.1911e+00)	Acc@1  62.00 ( 70.35)	Acc@5  90.00 ( 90.77)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0420e+00 (1.1679e+00)	Acc@1  70.00 ( 70.39)	Acc@5  90.00 ( 91.17)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.1279e+00 (1.1682e+00)	Acc@1  73.00 ( 70.51)	Acc@5  92.00 ( 91.14)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1406e+00 (1.1532e+00)	Acc@1  68.00 ( 70.33)	Acc@5  90.00 ( 91.31)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2188e+00 (1.1537e+00)	Acc@1  65.00 ( 70.27)	Acc@5  93.00 ( 91.30)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4092e+00 (1.1580e+00)	Acc@1  70.00 ( 70.32)	Acc@5  87.00 ( 91.20)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2354e+00 (1.1434e+00)	Acc@1  66.00 ( 70.55)	Acc@5  92.00 ( 91.43)
 * Acc@1 70.750 Acc@5 91.480
### epoch[44] execution time: 68.29837226867676
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.338 ( 0.338)	Data  0.164 ( 0.164)	Loss 3.1274e-01 (3.1274e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.016)	Loss 3.6182e-01 (3.4371e-01)	Acc@1  89.84 ( 89.99)	Acc@5  99.22 ( 99.29)
Epoch: [45][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.009)	Loss 3.1836e-01 (3.3589e-01)	Acc@1  91.41 ( 90.44)	Acc@5 100.00 ( 99.37)
Epoch: [45][ 30/391]	Time  0.162 ( 0.167)	Data  0.001 ( 0.006)	Loss 3.1909e-01 (3.3411e-01)	Acc@1  91.41 ( 90.32)	Acc@5 100.00 ( 99.40)
Epoch: [45][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 3.2568e-01 (3.2705e-01)	Acc@1  89.06 ( 90.45)	Acc@5  99.22 ( 99.45)
Epoch: [45][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 4.0894e-01 (3.2917e-01)	Acc@1  86.72 ( 90.58)	Acc@5  99.22 ( 99.37)
Epoch: [45][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 3.3472e-01 (3.2943e-01)	Acc@1  90.62 ( 90.52)	Acc@5 100.00 ( 99.37)
Epoch: [45][ 70/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.5669e-01 (3.2774e-01)	Acc@1  88.28 ( 90.67)	Acc@5 100.00 ( 99.38)
Epoch: [45][ 80/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.9688e-01 (3.2926e-01)	Acc@1  92.97 ( 90.53)	Acc@5  99.22 ( 99.38)
Epoch: [45][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.5986e-01 (3.2822e-01)	Acc@1  88.28 ( 90.60)	Acc@5  99.22 ( 99.38)
Epoch: [45][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.9028e-01 (3.2949e-01)	Acc@1  90.62 ( 90.59)	Acc@5 100.00 ( 99.34)
Epoch: [45][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2554e-01 (3.3062e-01)	Acc@1  89.84 ( 90.51)	Acc@5  98.44 ( 99.33)
Epoch: [45][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3018e-01 (3.2993e-01)	Acc@1  85.94 ( 90.50)	Acc@5 100.00 ( 99.35)
Epoch: [45][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1650e-01 (3.3139e-01)	Acc@1  86.72 ( 90.55)	Acc@5 100.00 ( 99.36)
Epoch: [45][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.1724e-01 (3.3146e-01)	Acc@1  85.94 ( 90.56)	Acc@5 100.00 ( 99.34)
Epoch: [45][150/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2617e-01 (3.3066e-01)	Acc@1  89.84 ( 90.58)	Acc@5  99.22 ( 99.33)
Epoch: [45][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2983e-01 (3.3214e-01)	Acc@1  89.84 ( 90.56)	Acc@5  99.22 ( 99.32)
Epoch: [45][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3633e-01 (3.3153e-01)	Acc@1  92.97 ( 90.56)	Acc@5 100.00 ( 99.32)
Epoch: [45][180/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0796e-01 (3.3242e-01)	Acc@1  88.28 ( 90.49)	Acc@5 100.00 ( 99.33)
Epoch: [45][190/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.8062e-01 (3.3311e-01)	Acc@1  87.50 ( 90.46)	Acc@5  99.22 ( 99.33)
Epoch: [45][200/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9199e-01 (3.3307e-01)	Acc@1  91.41 ( 90.45)	Acc@5 100.00 ( 99.34)
Epoch: [45][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2617e-01 (3.3351e-01)	Acc@1  89.84 ( 90.45)	Acc@5  99.22 ( 99.34)
Epoch: [45][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5806e-01 (3.3452e-01)	Acc@1  94.53 ( 90.40)	Acc@5  99.22 ( 99.31)
Epoch: [45][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3335e-01 (3.3367e-01)	Acc@1  85.16 ( 90.41)	Acc@5  98.44 ( 99.32)
Epoch: [45][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6279e-01 (3.3303e-01)	Acc@1  89.84 ( 90.42)	Acc@5  99.22 ( 99.33)
Epoch: [45][250/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4106e-01 (3.3273e-01)	Acc@1  90.62 ( 90.47)	Acc@5 100.00 ( 99.32)
Epoch: [45][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4131e-01 (3.3327e-01)	Acc@1  92.19 ( 90.46)	Acc@5  99.22 ( 99.33)
Epoch: [45][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0234e-01 (3.3439e-01)	Acc@1  88.28 ( 90.41)	Acc@5  98.44 ( 99.31)
Epoch: [45][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0542e-01 (3.3513e-01)	Acc@1  91.41 ( 90.39)	Acc@5 100.00 ( 99.30)
Epoch: [45][290/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.2554e-01 (3.3585e-01)	Acc@1  86.72 ( 90.38)	Acc@5  98.44 ( 99.30)
Epoch: [45][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5693e-01 (3.3674e-01)	Acc@1  88.28 ( 90.32)	Acc@5 100.00 ( 99.29)
Epoch: [45][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9688e-01 (3.3730e-01)	Acc@1  88.28 ( 90.31)	Acc@5 100.00 ( 99.28)
Epoch: [45][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0186e-01 (3.3704e-01)	Acc@1  87.50 ( 90.30)	Acc@5  99.22 ( 99.28)
Epoch: [45][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6011e-01 (3.3778e-01)	Acc@1  89.06 ( 90.28)	Acc@5  98.44 ( 99.28)
Epoch: [45][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0688e-01 (3.3766e-01)	Acc@1  93.75 ( 90.29)	Acc@5 100.00 ( 99.29)
Epoch: [45][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6587e-01 (3.3762e-01)	Acc@1  96.09 ( 90.31)	Acc@5  99.22 ( 99.28)
Epoch: [45][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.3848e-01 (3.3849e-01)	Acc@1  85.16 ( 90.25)	Acc@5  99.22 ( 99.28)
Epoch: [45][370/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.5498e-01 (3.3839e-01)	Acc@1  89.84 ( 90.27)	Acc@5  99.22 ( 99.27)
Epoch: [45][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7002e-01 (3.3833e-01)	Acc@1  91.41 ( 90.27)	Acc@5 100.00 ( 99.28)
Epoch: [45][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.7329e-01 (3.3854e-01)	Acc@1  90.00 ( 90.29)	Acc@5 100.00 ( 99.28)
## e[45] optimizer.zero_grad (sum) time: 0.5475192070007324
## e[45]       loss.backward (sum) time: 12.526089668273926
## e[45]      optimizer.step (sum) time: 25.49773073196411
## epoch[45] training(only) time: 63.41156482696533
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.1357e+00 (1.1357e+00)	Acc@1  71.00 ( 71.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.2002e+00 (1.1681e+00)	Acc@1  68.00 ( 70.64)	Acc@5  90.00 ( 90.09)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0127e+00 (1.1409e+00)	Acc@1  73.00 ( 71.38)	Acc@5  94.00 ( 91.00)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4082e+00 (1.1721e+00)	Acc@1  61.00 ( 70.35)	Acc@5  91.00 ( 91.06)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 9.8828e-01 (1.1564e+00)	Acc@1  73.00 ( 70.56)	Acc@5  93.00 ( 91.49)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2529e+00 (1.1600e+00)	Acc@1  69.00 ( 70.69)	Acc@5  92.00 ( 91.24)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1455e+00 (1.1421e+00)	Acc@1  68.00 ( 70.72)	Acc@5  90.00 ( 91.41)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2021e+00 (1.1442e+00)	Acc@1  67.00 ( 70.55)	Acc@5  92.00 ( 91.35)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.3818e+00 (1.1487e+00)	Acc@1  69.00 ( 70.60)	Acc@5  87.00 ( 91.33)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2803e+00 (1.1361e+00)	Acc@1  66.00 ( 70.67)	Acc@5  91.00 ( 91.46)
 * Acc@1 70.870 Acc@5 91.520
### epoch[45] execution time: 68.36580777168274
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.331 ( 0.331)	Data  0.161 ( 0.161)	Loss 3.6963e-01 (3.6963e-01)	Acc@1  91.41 ( 91.41)	Acc@5  98.44 ( 98.44)
Epoch: [46][ 10/391]	Time  0.159 ( 0.176)	Data  0.001 ( 0.015)	Loss 3.8501e-01 (3.2582e-01)	Acc@1  89.06 ( 90.55)	Acc@5 100.00 ( 99.29)
Epoch: [46][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.009)	Loss 3.2324e-01 (3.3178e-01)	Acc@1  90.62 ( 90.66)	Acc@5  99.22 ( 99.37)
Epoch: [46][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.4109e-01 (3.2570e-01)	Acc@1  93.75 ( 90.75)	Acc@5 100.00 ( 99.50)
Epoch: [46][ 40/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.005)	Loss 3.1934e-01 (3.2084e-01)	Acc@1  90.62 ( 90.82)	Acc@5  98.44 ( 99.50)
Epoch: [46][ 50/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.004)	Loss 2.4890e-01 (3.1907e-01)	Acc@1  93.75 ( 90.95)	Acc@5  99.22 ( 99.51)
Epoch: [46][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.7686e-01 (3.1717e-01)	Acc@1  93.75 ( 91.11)	Acc@5  99.22 ( 99.47)
Epoch: [46][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 4.0405e-01 (3.1600e-01)	Acc@1  90.62 ( 91.26)	Acc@5  97.66 ( 99.45)
Epoch: [46][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.0127e-01 (3.1778e-01)	Acc@1  92.97 ( 91.16)	Acc@5 100.00 ( 99.45)
Epoch: [46][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.4351e-01 (3.1678e-01)	Acc@1  90.62 ( 91.16)	Acc@5  99.22 ( 99.46)
Epoch: [46][100/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.2861e-01 (3.1666e-01)	Acc@1  90.62 ( 91.19)	Acc@5  99.22 ( 99.44)
Epoch: [46][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7173e-01 (3.1650e-01)	Acc@1  94.53 ( 91.09)	Acc@5  99.22 ( 99.43)
Epoch: [46][120/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3618e-01 (3.1809e-01)	Acc@1  88.28 ( 90.99)	Acc@5  98.44 ( 99.41)
Epoch: [46][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.3774e-01 (3.1877e-01)	Acc@1  89.84 ( 90.99)	Acc@5  99.22 ( 99.43)
Epoch: [46][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4597e-01 (3.1819e-01)	Acc@1  94.53 ( 91.06)	Acc@5 100.00 ( 99.44)
Epoch: [46][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8979e-01 (3.1777e-01)	Acc@1  92.97 ( 91.10)	Acc@5 100.00 ( 99.45)
Epoch: [46][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.2139e-01 (3.1929e-01)	Acc@1  85.94 ( 91.08)	Acc@5  99.22 ( 99.46)
Epoch: [46][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0581e-01 (3.1872e-01)	Acc@1  94.53 ( 91.08)	Acc@5 100.00 ( 99.46)
Epoch: [46][180/391]	Time  0.158 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9141e-01 (3.1749e-01)	Acc@1  96.09 ( 91.13)	Acc@5 100.00 ( 99.46)
Epoch: [46][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.9062e-01 (3.1791e-01)	Acc@1  89.06 ( 91.13)	Acc@5  99.22 ( 99.46)
Epoch: [46][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 4.0649e-01 (3.1787e-01)	Acc@1  88.28 ( 91.15)	Acc@5 100.00 ( 99.46)
Epoch: [46][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4524e-01 (3.1768e-01)	Acc@1  95.31 ( 91.15)	Acc@5  99.22 ( 99.46)
Epoch: [46][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6367e-01 (3.1803e-01)	Acc@1  94.53 ( 91.17)	Acc@5  99.22 ( 99.44)
Epoch: [46][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9028e-01 (3.1826e-01)	Acc@1  91.41 ( 91.18)	Acc@5  99.22 ( 99.44)
Epoch: [46][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8149e-01 (3.1863e-01)	Acc@1  93.75 ( 91.18)	Acc@5  99.22 ( 99.43)
Epoch: [46][250/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3569e-01 (3.1933e-01)	Acc@1  88.28 ( 91.10)	Acc@5  99.22 ( 99.43)
Epoch: [46][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0615e-01 (3.2090e-01)	Acc@1  90.62 ( 91.02)	Acc@5 100.00 ( 99.44)
Epoch: [46][270/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7246e-01 (3.2099e-01)	Acc@1  92.19 ( 91.04)	Acc@5 100.00 ( 99.44)
Epoch: [46][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9883e-01 (3.2189e-01)	Acc@1  90.62 ( 91.01)	Acc@5  99.22 ( 99.42)
Epoch: [46][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5352e-01 (3.2220e-01)	Acc@1  89.06 ( 91.01)	Acc@5  99.22 ( 99.42)
Epoch: [46][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0249e-01 (3.2310e-01)	Acc@1  92.19 ( 90.94)	Acc@5 100.00 ( 99.42)
Epoch: [46][310/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6597e-01 (3.2333e-01)	Acc@1  91.41 ( 90.92)	Acc@5  98.44 ( 99.43)
Epoch: [46][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9502e-01 (3.2362e-01)	Acc@1  88.28 ( 90.87)	Acc@5 100.00 ( 99.43)
Epoch: [46][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7124e-01 (3.2355e-01)	Acc@1  92.19 ( 90.88)	Acc@5 100.00 ( 99.43)
Epoch: [46][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.8174e-01 (3.2357e-01)	Acc@1  92.19 ( 90.89)	Acc@5  99.22 ( 99.42)
Epoch: [46][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3999e-01 (3.2344e-01)	Acc@1  93.75 ( 90.89)	Acc@5 100.00 ( 99.42)
Epoch: [46][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.2056e-01 (3.2379e-01)	Acc@1  87.50 ( 90.88)	Acc@5  98.44 ( 99.41)
Epoch: [46][370/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.9980e-01 (3.2384e-01)	Acc@1  92.19 ( 90.86)	Acc@5  99.22 ( 99.42)
Epoch: [46][380/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.9478e-01 (3.2418e-01)	Acc@1  90.62 ( 90.85)	Acc@5  97.66 ( 99.42)
Epoch: [46][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.9810e-01 (3.2413e-01)	Acc@1  92.50 ( 90.85)	Acc@5  98.75 ( 99.41)
## e[46] optimizer.zero_grad (sum) time: 0.5418250560760498
## e[46]       loss.backward (sum) time: 12.548949241638184
## e[46]      optimizer.step (sum) time: 25.466963291168213
## epoch[46] training(only) time: 63.420337438583374
# Switched to evaluate mode...
Test: [  0/100]	Time  0.197 ( 0.197)	Loss 1.1006e+00 (1.1006e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.2451e+00 (1.1788e+00)	Acc@1  67.00 ( 70.73)	Acc@5  91.00 ( 90.36)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.0898e+00 (1.1505e+00)	Acc@1  73.00 ( 71.24)	Acc@5  91.00 ( 91.05)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5000e+00 (1.1895e+00)	Acc@1  63.00 ( 70.39)	Acc@5  88.00 ( 90.90)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0010e+00 (1.1759e+00)	Acc@1  69.00 ( 70.24)	Acc@5  91.00 ( 91.27)
Test: [ 50/100]	Time  0.056 ( 0.050)	Loss 1.1934e+00 (1.1779e+00)	Acc@1  68.00 ( 70.24)	Acc@5  92.00 ( 91.18)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1143e+00 (1.1633e+00)	Acc@1  70.00 ( 70.20)	Acc@5  92.00 ( 91.34)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2568e+00 (1.1642e+00)	Acc@1  68.00 ( 70.15)	Acc@5  92.00 ( 91.34)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4121e+00 (1.1637e+00)	Acc@1  68.00 ( 70.07)	Acc@5  86.00 ( 91.28)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3242e+00 (1.1511e+00)	Acc@1  71.00 ( 70.30)	Acc@5  90.00 ( 91.38)
 * Acc@1 70.470 Acc@5 91.390
### epoch[46] execution time: 68.37018918991089
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.326 ( 0.326)	Data  0.151 ( 0.151)	Loss 2.3987e-01 (2.3987e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.173 ( 0.176)	Data  0.001 ( 0.015)	Loss 2.6709e-01 (3.1568e-01)	Acc@1  91.41 ( 91.26)	Acc@5 100.00 ( 99.36)
Epoch: [47][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.0786e-01 (3.1338e-01)	Acc@1  89.84 ( 91.26)	Acc@5 100.00 ( 99.29)
Epoch: [47][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.4609e-01 (3.0831e-01)	Acc@1  94.53 ( 91.71)	Acc@5  99.22 ( 99.29)
Epoch: [47][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 3.3105e-01 (3.0408e-01)	Acc@1  89.84 ( 91.65)	Acc@5  99.22 ( 99.37)
Epoch: [47][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.6562e-01 (3.0475e-01)	Acc@1  94.53 ( 91.64)	Acc@5 100.00 ( 99.45)
Epoch: [47][ 60/391]	Time  0.168 ( 0.165)	Data  0.001 ( 0.003)	Loss 2.5439e-01 (3.0319e-01)	Acc@1  93.75 ( 91.53)	Acc@5 100.00 ( 99.49)
Epoch: [47][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4927e-01 (3.0329e-01)	Acc@1  94.53 ( 91.56)	Acc@5  98.44 ( 99.47)
Epoch: [47][ 80/391]	Time  0.173 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.1104e-01 (2.9977e-01)	Acc@1  92.19 ( 91.72)	Acc@5 100.00 ( 99.52)
Epoch: [47][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.3545e-01 (2.9975e-01)	Acc@1  91.41 ( 91.72)	Acc@5 100.00 ( 99.52)
Epoch: [47][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7588e-01 (3.0128e-01)	Acc@1  88.28 ( 91.56)	Acc@5 100.00 ( 99.55)
Epoch: [47][110/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.8330e-01 (3.0450e-01)	Acc@1  88.28 ( 91.43)	Acc@5  99.22 ( 99.53)
Epoch: [47][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2056e-01 (3.0510e-01)	Acc@1  90.62 ( 91.37)	Acc@5 100.00 ( 99.54)
Epoch: [47][130/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9834e-01 (3.0458e-01)	Acc@1  89.84 ( 91.44)	Acc@5 100.00 ( 99.51)
Epoch: [47][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5083e-01 (3.0341e-01)	Acc@1  89.84 ( 91.49)	Acc@5  98.44 ( 99.50)
Epoch: [47][150/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5659e-01 (3.0195e-01)	Acc@1  91.41 ( 91.53)	Acc@5  99.22 ( 99.51)
Epoch: [47][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5571e-01 (3.0473e-01)	Acc@1  92.19 ( 91.44)	Acc@5  98.44 ( 99.50)
Epoch: [47][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5864e-01 (3.0451e-01)	Acc@1  90.62 ( 91.42)	Acc@5  99.22 ( 99.50)
Epoch: [47][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.6597e-01 (3.0439e-01)	Acc@1  90.62 ( 91.44)	Acc@5  99.22 ( 99.49)
Epoch: [47][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9443e-01 (3.0447e-01)	Acc@1  90.62 ( 91.43)	Acc@5  99.22 ( 99.47)
Epoch: [47][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0103e-01 (3.0323e-01)	Acc@1  94.53 ( 91.53)	Acc@5 100.00 ( 99.49)
Epoch: [47][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3730e-01 (3.0402e-01)	Acc@1  92.19 ( 91.48)	Acc@5 100.00 ( 99.47)
Epoch: [47][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0015e-01 (3.0418e-01)	Acc@1  86.72 ( 91.47)	Acc@5 100.00 ( 99.48)
Epoch: [47][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8442e-01 (3.0388e-01)	Acc@1  92.19 ( 91.48)	Acc@5 100.00 ( 99.48)
Epoch: [47][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9209e-01 (3.0471e-01)	Acc@1  87.50 ( 91.42)	Acc@5 100.00 ( 99.48)
Epoch: [47][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.3105e-01 (3.0416e-01)	Acc@1  91.41 ( 91.44)	Acc@5  97.66 ( 99.47)
Epoch: [47][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4937e-01 (3.0419e-01)	Acc@1  88.28 ( 91.40)	Acc@5  98.44 ( 99.45)
Epoch: [47][270/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.7866e-01 (3.0452e-01)	Acc@1  88.28 ( 91.36)	Acc@5  98.44 ( 99.46)
Epoch: [47][280/391]	Time  0.175 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7490e-01 (3.0432e-01)	Acc@1  92.19 ( 91.37)	Acc@5 100.00 ( 99.46)
Epoch: [47][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.9526e-01 (3.0423e-01)	Acc@1  86.72 ( 91.35)	Acc@5 100.00 ( 99.47)
Epoch: [47][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2485e-01 (3.0462e-01)	Acc@1  96.88 ( 91.37)	Acc@5  99.22 ( 99.48)
Epoch: [47][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3105e-01 (3.0422e-01)	Acc@1  90.62 ( 91.37)	Acc@5 100.00 ( 99.48)
Epoch: [47][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9824e-01 (3.0367e-01)	Acc@1  96.88 ( 91.40)	Acc@5 100.00 ( 99.48)
Epoch: [47][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6514e-01 (3.0435e-01)	Acc@1  92.19 ( 91.39)	Acc@5  99.22 ( 99.47)
Epoch: [47][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6475e-01 (3.0618e-01)	Acc@1  91.41 ( 91.32)	Acc@5  96.88 ( 99.45)
Epoch: [47][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6743e-01 (3.0669e-01)	Acc@1  89.84 ( 91.30)	Acc@5 100.00 ( 99.45)
Epoch: [47][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7832e-01 (3.0653e-01)	Acc@1  94.53 ( 91.30)	Acc@5 100.00 ( 99.46)
Epoch: [47][370/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4741e-01 (3.0713e-01)	Acc@1  89.06 ( 91.28)	Acc@5  99.22 ( 99.46)
Epoch: [47][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9712e-01 (3.0646e-01)	Acc@1  89.84 ( 91.30)	Acc@5  99.22 ( 99.46)
Epoch: [47][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 4.1309e-01 (3.0651e-01)	Acc@1  82.50 ( 91.30)	Acc@5  98.75 ( 99.46)
## e[47] optimizer.zero_grad (sum) time: 0.5441768169403076
## e[47]       loss.backward (sum) time: 12.55378770828247
## e[47]      optimizer.step (sum) time: 25.479283809661865
## epoch[47] training(only) time: 63.44644808769226
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.1172e+00 (1.1172e+00)	Acc@1  70.00 ( 70.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.1748e+00 (1.2005e+00)	Acc@1  67.00 ( 70.45)	Acc@5  92.00 ( 90.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.0605e+00 (1.1661e+00)	Acc@1  70.00 ( 71.10)	Acc@5  91.00 ( 91.05)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4609e+00 (1.1977e+00)	Acc@1  61.00 ( 70.45)	Acc@5  90.00 ( 90.71)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 9.9023e-01 (1.1797e+00)	Acc@1  74.00 ( 70.54)	Acc@5  92.00 ( 91.07)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2422e+00 (1.1840e+00)	Acc@1  70.00 ( 70.43)	Acc@5  93.00 ( 91.06)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1768e+00 (1.1675e+00)	Acc@1  68.00 ( 70.31)	Acc@5  89.00 ( 91.26)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2695e+00 (1.1711e+00)	Acc@1  62.00 ( 70.13)	Acc@5  92.00 ( 91.28)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.3730e+00 (1.1731e+00)	Acc@1  70.00 ( 70.16)	Acc@5  86.00 ( 91.25)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3496e+00 (1.1577e+00)	Acc@1  68.00 ( 70.44)	Acc@5  92.00 ( 91.42)
 * Acc@1 70.600 Acc@5 91.440
### epoch[47] execution time: 68.38323450088501
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.325 ( 0.325)	Data  0.157 ( 0.157)	Loss 2.7759e-01 (2.7759e-01)	Acc@1  92.19 ( 92.19)	Acc@5  98.44 ( 98.44)
Epoch: [48][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.015)	Loss 3.0542e-01 (2.9238e-01)	Acc@1  92.97 ( 93.18)	Acc@5  99.22 ( 99.29)
Epoch: [48][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 3.4424e-01 (3.0153e-01)	Acc@1  89.06 ( 92.22)	Acc@5 100.00 ( 99.37)
Epoch: [48][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.8442e-01 (2.9039e-01)	Acc@1  91.41 ( 92.46)	Acc@5  99.22 ( 99.50)
Epoch: [48][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.1289e-01 (2.8266e-01)	Acc@1  96.09 ( 92.84)	Acc@5 100.00 ( 99.60)
Epoch: [48][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.2324e-01 (2.8127e-01)	Acc@1  89.84 ( 92.72)	Acc@5 100.00 ( 99.63)
Epoch: [48][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.7637e-01 (2.8392e-01)	Acc@1  92.97 ( 92.61)	Acc@5 100.00 ( 99.64)
Epoch: [48][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4512e-01 (2.8100e-01)	Acc@1  92.97 ( 92.62)	Acc@5 100.00 ( 99.67)
Epoch: [48][ 80/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.7856e-01 (2.8077e-01)	Acc@1  91.41 ( 92.53)	Acc@5 100.00 ( 99.68)
Epoch: [48][ 90/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.9492e-01 (2.8358e-01)	Acc@1  92.19 ( 92.48)	Acc@5 100.00 ( 99.63)
Epoch: [48][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.5449e-01 (2.8218e-01)	Acc@1  87.50 ( 92.47)	Acc@5 100.00 ( 99.64)
Epoch: [48][110/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6001e-01 (2.8078e-01)	Acc@1  93.75 ( 92.55)	Acc@5 100.00 ( 99.63)
Epoch: [48][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5938e-01 (2.8138e-01)	Acc@1  87.50 ( 92.47)	Acc@5 100.00 ( 99.64)
Epoch: [48][130/391]	Time  0.178 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9492e-01 (2.8236e-01)	Acc@1  92.19 ( 92.42)	Acc@5  98.44 ( 99.64)
Epoch: [48][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.8320e-01 (2.8244e-01)	Acc@1  94.53 ( 92.45)	Acc@5 100.00 ( 99.63)
Epoch: [48][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.9370e-01 (2.8251e-01)	Acc@1  91.41 ( 92.45)	Acc@5  99.22 ( 99.64)
Epoch: [48][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2202e-01 (2.8461e-01)	Acc@1  90.62 ( 92.34)	Acc@5 100.00 ( 99.64)
Epoch: [48][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4402e-01 (2.8382e-01)	Acc@1  94.53 ( 92.38)	Acc@5  99.22 ( 99.64)
Epoch: [48][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4448e-01 (2.8478e-01)	Acc@1  87.50 ( 92.35)	Acc@5 100.00 ( 99.64)
Epoch: [48][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.5327e-01 (2.8475e-01)	Acc@1  89.84 ( 92.31)	Acc@5  98.44 ( 99.64)
Epoch: [48][200/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4097e-01 (2.8499e-01)	Acc@1  94.53 ( 92.30)	Acc@5 100.00 ( 99.63)
Epoch: [48][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5928e-01 (2.8446e-01)	Acc@1  92.19 ( 92.31)	Acc@5  98.44 ( 99.63)
Epoch: [48][220/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0347e-01 (2.8477e-01)	Acc@1  90.62 ( 92.29)	Acc@5 100.00 ( 99.64)
Epoch: [48][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6758e-01 (2.8604e-01)	Acc@1  94.53 ( 92.22)	Acc@5  99.22 ( 99.62)
Epoch: [48][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5742e-01 (2.8612e-01)	Acc@1  89.06 ( 92.22)	Acc@5 100.00 ( 99.61)
Epoch: [48][250/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9199e-01 (2.8569e-01)	Acc@1  93.75 ( 92.25)	Acc@5 100.00 ( 99.60)
Epoch: [48][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8076e-01 (2.8540e-01)	Acc@1  93.75 ( 92.27)	Acc@5  99.22 ( 99.60)
Epoch: [48][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9126e-01 (2.8569e-01)	Acc@1  91.41 ( 92.24)	Acc@5 100.00 ( 99.60)
Epoch: [48][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7344e-01 (2.8543e-01)	Acc@1  91.41 ( 92.27)	Acc@5 100.00 ( 99.59)
Epoch: [48][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.5889e-01 (2.8691e-01)	Acc@1  89.84 ( 92.21)	Acc@5  99.22 ( 99.59)
Epoch: [48][300/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0615e-01 (2.8722e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.59)
Epoch: [48][310/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.6914e-01 (2.8765e-01)	Acc@1  85.94 ( 92.18)	Acc@5 100.00 ( 99.59)
Epoch: [48][320/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2546e-01 (2.8727e-01)	Acc@1  95.31 ( 92.19)	Acc@5 100.00 ( 99.59)
Epoch: [48][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2080e-01 (2.8768e-01)	Acc@1  91.41 ( 92.18)	Acc@5  99.22 ( 99.58)
Epoch: [48][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 4.0698e-01 (2.8888e-01)	Acc@1  90.62 ( 92.14)	Acc@5  96.88 ( 99.56)
Epoch: [48][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6782e-01 (2.8897e-01)	Acc@1  94.53 ( 92.14)	Acc@5  98.44 ( 99.56)
Epoch: [48][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0605e-01 (2.9016e-01)	Acc@1  96.09 ( 92.09)	Acc@5 100.00 ( 99.55)
Epoch: [48][370/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4058e-01 (2.9062e-01)	Acc@1  89.84 ( 92.05)	Acc@5  99.22 ( 99.55)
Epoch: [48][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3057e-01 (2.9109e-01)	Acc@1  87.50 ( 91.99)	Acc@5  99.22 ( 99.55)
Epoch: [48][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0664e-01 (2.9136e-01)	Acc@1  93.75 ( 91.99)	Acc@5 100.00 ( 99.55)
## e[48] optimizer.zero_grad (sum) time: 0.54595947265625
## e[48]       loss.backward (sum) time: 12.545932292938232
## e[48]      optimizer.step (sum) time: 25.47926664352417
## epoch[48] training(only) time: 63.39533877372742
# Switched to evaluate mode...
Test: [  0/100]	Time  0.186 ( 0.186)	Loss 1.0322e+00 (1.0322e+00)	Acc@1  70.00 ( 70.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.3027e+00 (1.2126e+00)	Acc@1  66.00 ( 70.09)	Acc@5  92.00 ( 89.91)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.1230e+00 (1.1812e+00)	Acc@1  72.00 ( 70.62)	Acc@5  90.00 ( 90.71)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 1.3984e+00 (1.2149e+00)	Acc@1  60.00 ( 69.77)	Acc@5  89.00 ( 90.55)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0127e+00 (1.1945e+00)	Acc@1  73.00 ( 70.02)	Acc@5  91.00 ( 90.85)
Test: [ 50/100]	Time  0.046 ( 0.049)	Loss 1.2266e+00 (1.1974e+00)	Acc@1  67.00 ( 70.02)	Acc@5  93.00 ( 90.86)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.1641e+00 (1.1817e+00)	Acc@1  69.00 ( 70.13)	Acc@5  91.00 ( 90.98)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2393e+00 (1.1802e+00)	Acc@1  66.00 ( 70.11)	Acc@5  93.00 ( 91.04)
Test: [ 80/100]	Time  0.056 ( 0.049)	Loss 1.5098e+00 (1.1814e+00)	Acc@1  68.00 ( 70.09)	Acc@5  90.00 ( 91.01)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.3525e+00 (1.1674e+00)	Acc@1  70.00 ( 70.34)	Acc@5  89.00 ( 91.16)
 * Acc@1 70.540 Acc@5 91.200
### epoch[48] execution time: 68.30298113822937
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.334 ( 0.334)	Data  0.148 ( 0.148)	Loss 1.9531e-01 (1.9531e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 2.6172e-01 (2.4749e-01)	Acc@1  93.75 ( 93.82)	Acc@5 100.00 ( 99.50)
Epoch: [49][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.9995e-01 (2.5723e-01)	Acc@1  95.31 ( 93.60)	Acc@5 100.00 ( 99.37)
Epoch: [49][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.8101e-01 (2.6383e-01)	Acc@1  92.97 ( 93.17)	Acc@5 100.00 ( 99.37)
Epoch: [49][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.004)	Loss 2.1204e-01 (2.6223e-01)	Acc@1  93.75 ( 93.20)	Acc@5 100.00 ( 99.45)
Epoch: [49][ 50/391]	Time  0.171 ( 0.165)	Data  0.001 ( 0.004)	Loss 3.3447e-01 (2.6510e-01)	Acc@1  89.84 ( 93.11)	Acc@5 100.00 ( 99.48)
Epoch: [49][ 60/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.0884e-01 (2.7318e-01)	Acc@1  91.41 ( 92.62)	Acc@5  98.44 ( 99.47)
Epoch: [49][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.3621e-01 (2.7335e-01)	Acc@1  93.75 ( 92.59)	Acc@5 100.00 ( 99.52)
Epoch: [49][ 80/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.4683e-01 (2.7481e-01)	Acc@1  92.97 ( 92.53)	Acc@5  99.22 ( 99.51)
Epoch: [49][ 90/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.7808e-01 (2.7352e-01)	Acc@1  95.31 ( 92.57)	Acc@5 100.00 ( 99.55)
Epoch: [49][100/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.2397e-01 (2.7373e-01)	Acc@1  90.62 ( 92.49)	Acc@5 100.00 ( 99.59)
Epoch: [49][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4888e-01 (2.7519e-01)	Acc@1  91.41 ( 92.43)	Acc@5  97.66 ( 99.56)
Epoch: [49][120/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4805e-01 (2.7312e-01)	Acc@1  91.41 ( 92.50)	Acc@5 100.00 ( 99.59)
Epoch: [49][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2693e-01 (2.7204e-01)	Acc@1  92.19 ( 92.56)	Acc@5 100.00 ( 99.61)
Epoch: [49][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7578e-01 (2.7222e-01)	Acc@1  96.88 ( 92.55)	Acc@5 100.00 ( 99.62)
Epoch: [49][150/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.4814e-01 (2.7317e-01)	Acc@1  86.72 ( 92.51)	Acc@5 100.00 ( 99.62)
Epoch: [49][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2949e-01 (2.7262e-01)	Acc@1  92.19 ( 92.55)	Acc@5 100.00 ( 99.63)
Epoch: [49][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3193e-01 (2.7286e-01)	Acc@1  93.75 ( 92.51)	Acc@5 100.00 ( 99.64)
Epoch: [49][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7856e-01 (2.7312e-01)	Acc@1  92.97 ( 92.51)	Acc@5 100.00 ( 99.65)
Epoch: [49][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6318e-01 (2.7361e-01)	Acc@1  94.53 ( 92.50)	Acc@5  97.66 ( 99.63)
Epoch: [49][200/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.3521e-01 (2.7526e-01)	Acc@1  91.41 ( 92.44)	Acc@5 100.00 ( 99.60)
Epoch: [49][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8052e-01 (2.7568e-01)	Acc@1  89.06 ( 92.44)	Acc@5  99.22 ( 99.59)
Epoch: [49][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0029e-01 (2.7561e-01)	Acc@1  91.41 ( 92.44)	Acc@5 100.00 ( 99.60)
Epoch: [49][230/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4890e-01 (2.7583e-01)	Acc@1  93.75 ( 92.44)	Acc@5 100.00 ( 99.60)
Epoch: [49][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.4692e-01 (2.7664e-01)	Acc@1  89.06 ( 92.37)	Acc@5 100.00 ( 99.60)
Epoch: [49][250/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3145e-01 (2.7620e-01)	Acc@1  92.97 ( 92.39)	Acc@5 100.00 ( 99.60)
Epoch: [49][260/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7856e-01 (2.7743e-01)	Acc@1  92.97 ( 92.39)	Acc@5  99.22 ( 99.59)
Epoch: [49][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4819e-01 (2.7739e-01)	Acc@1  95.31 ( 92.41)	Acc@5 100.00 ( 99.59)
Epoch: [49][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.6294e-01 (2.7723e-01)	Acc@1  92.97 ( 92.41)	Acc@5  99.22 ( 99.59)
Epoch: [49][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5635e-01 (2.7711e-01)	Acc@1  93.75 ( 92.41)	Acc@5 100.00 ( 99.59)
Epoch: [49][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.9932e-01 (2.7693e-01)	Acc@1  91.41 ( 92.40)	Acc@5 100.00 ( 99.60)
Epoch: [49][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.4658e-01 (2.7797e-01)	Acc@1  93.75 ( 92.36)	Acc@5 100.00 ( 99.59)
Epoch: [49][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3645e-01 (2.7767e-01)	Acc@1  93.75 ( 92.37)	Acc@5 100.00 ( 99.59)
Epoch: [49][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7368e-01 (2.7797e-01)	Acc@1  95.31 ( 92.38)	Acc@5  98.44 ( 99.59)
Epoch: [49][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.4131e-01 (2.7820e-01)	Acc@1  89.06 ( 92.34)	Acc@5 100.00 ( 99.59)
Epoch: [49][350/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.4448e-01 (2.7872e-01)	Acc@1  89.84 ( 92.32)	Acc@5 100.00 ( 99.59)
Epoch: [49][360/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.0884e-01 (2.7932e-01)	Acc@1  90.62 ( 92.29)	Acc@5 100.00 ( 99.59)
Epoch: [49][370/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1313e-01 (2.7917e-01)	Acc@1  96.09 ( 92.30)	Acc@5 100.00 ( 99.60)
Epoch: [49][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1912e-01 (2.7878e-01)	Acc@1  93.75 ( 92.31)	Acc@5 100.00 ( 99.60)
Epoch: [49][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.3154e-01 (2.7924e-01)	Acc@1  95.00 ( 92.30)	Acc@5  97.50 ( 99.59)
## e[49] optimizer.zero_grad (sum) time: 0.5470879077911377
## e[49]       loss.backward (sum) time: 12.568933248519897
## e[49]      optimizer.step (sum) time: 25.435128927230835
## epoch[49] training(only) time: 63.377171754837036
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.1113e+00 (1.1113e+00)	Acc@1  70.00 ( 70.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.2725e+00 (1.2239e+00)	Acc@1  67.00 ( 70.00)	Acc@5  92.00 ( 90.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.1221e+00 (1.1897e+00)	Acc@1  71.00 ( 70.76)	Acc@5  92.00 ( 91.00)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4824e+00 (1.2209e+00)	Acc@1  63.00 ( 69.94)	Acc@5  89.00 ( 91.06)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0195e+00 (1.2032e+00)	Acc@1  74.00 ( 69.83)	Acc@5  92.00 ( 91.39)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2139e+00 (1.2092e+00)	Acc@1  72.00 ( 69.90)	Acc@5  91.00 ( 91.27)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1338e+00 (1.1943e+00)	Acc@1  66.00 ( 69.93)	Acc@5  92.00 ( 91.31)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2305e+00 (1.1904e+00)	Acc@1  67.00 ( 69.90)	Acc@5  93.00 ( 91.34)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4570e+00 (1.1908e+00)	Acc@1  68.00 ( 69.90)	Acc@5  86.00 ( 91.30)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.4180e+00 (1.1760e+00)	Acc@1  65.00 ( 70.15)	Acc@5  90.00 ( 91.41)
 * Acc@1 70.440 Acc@5 91.480
### epoch[49] execution time: 68.31815719604492
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.314 ( 0.314)	Data  0.140 ( 0.140)	Loss 3.4277e-01 (3.4277e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [50][ 10/391]	Time  0.159 ( 0.174)	Data  0.001 ( 0.014)	Loss 1.9836e-01 (2.4412e-01)	Acc@1  94.53 ( 92.83)	Acc@5 100.00 ( 99.86)
Epoch: [50][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 2.8125e-01 (2.4052e-01)	Acc@1  91.41 ( 93.42)	Acc@5 100.00 ( 99.78)
Epoch: [50][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.9736e-01 (2.4692e-01)	Acc@1  92.97 ( 93.20)	Acc@5 100.00 ( 99.75)
Epoch: [50][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.8931e-01 (2.5123e-01)	Acc@1  91.41 ( 92.93)	Acc@5  99.22 ( 99.73)
Epoch: [50][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.6636e-01 (2.5290e-01)	Acc@1  92.19 ( 92.98)	Acc@5  98.44 ( 99.63)
Epoch: [50][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 3.0762e-01 (2.5563e-01)	Acc@1  90.62 ( 92.98)	Acc@5 100.00 ( 99.63)
Epoch: [50][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 3.2422e-01 (2.5555e-01)	Acc@1  89.06 ( 92.92)	Acc@5  99.22 ( 99.65)
Epoch: [50][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4951e-01 (2.5563e-01)	Acc@1  92.19 ( 92.90)	Acc@5 100.00 ( 99.65)
Epoch: [50][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4426e-01 (2.5507e-01)	Acc@1  94.53 ( 92.96)	Acc@5 100.00 ( 99.68)
Epoch: [50][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.7759e-01 (2.5534e-01)	Acc@1  93.75 ( 92.94)	Acc@5  99.22 ( 99.68)
Epoch: [50][110/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3169e-01 (2.5647e-01)	Acc@1  92.19 ( 92.85)	Acc@5 100.00 ( 99.68)
Epoch: [50][120/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9272e-01 (2.5788e-01)	Acc@1  93.75 ( 92.87)	Acc@5 100.00 ( 99.68)
Epoch: [50][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3682e-01 (2.5794e-01)	Acc@1  94.53 ( 92.84)	Acc@5 100.00 ( 99.67)
Epoch: [50][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7432e-01 (2.5695e-01)	Acc@1  93.75 ( 92.90)	Acc@5 100.00 ( 99.67)
Epoch: [50][150/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1448e-01 (2.5758e-01)	Acc@1  94.53 ( 92.91)	Acc@5 100.00 ( 99.67)
Epoch: [50][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8555e-01 (2.5726e-01)	Acc@1  92.97 ( 92.94)	Acc@5 100.00 ( 99.67)
Epoch: [50][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1045e-01 (2.5911e-01)	Acc@1  93.75 ( 92.84)	Acc@5 100.00 ( 99.66)
Epoch: [50][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3792e-01 (2.5864e-01)	Acc@1  93.75 ( 92.87)	Acc@5  99.22 ( 99.66)
Epoch: [50][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7441e-01 (2.5988e-01)	Acc@1  95.31 ( 92.87)	Acc@5 100.00 ( 99.66)
Epoch: [50][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9189e-01 (2.5937e-01)	Acc@1  96.09 ( 92.89)	Acc@5 100.00 ( 99.65)
Epoch: [50][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5879e-01 (2.5839e-01)	Acc@1  92.19 ( 92.95)	Acc@5 100.00 ( 99.66)
Epoch: [50][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3413e-01 (2.5817e-01)	Acc@1  94.53 ( 92.95)	Acc@5 100.00 ( 99.66)
Epoch: [50][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1030e-01 (2.5774e-01)	Acc@1  90.62 ( 92.97)	Acc@5  99.22 ( 99.66)
Epoch: [50][240/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.2251e-01 (2.5880e-01)	Acc@1  89.06 ( 92.90)	Acc@5 100.00 ( 99.66)
Epoch: [50][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8955e-01 (2.5887e-01)	Acc@1  92.19 ( 92.91)	Acc@5  99.22 ( 99.66)
Epoch: [50][260/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7222e-01 (2.5991e-01)	Acc@1  91.41 ( 92.86)	Acc@5 100.00 ( 99.65)
Epoch: [50][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4866e-01 (2.6010e-01)	Acc@1  92.97 ( 92.86)	Acc@5 100.00 ( 99.65)
Epoch: [50][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6660e-01 (2.6040e-01)	Acc@1  93.75 ( 92.86)	Acc@5 100.00 ( 99.65)
Epoch: [50][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3315e-01 (2.6066e-01)	Acc@1  92.97 ( 92.85)	Acc@5  99.22 ( 99.64)
Epoch: [50][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.0811e-01 (2.6128e-01)	Acc@1  91.41 ( 92.85)	Acc@5 100.00 ( 99.64)
Epoch: [50][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2693e-01 (2.6199e-01)	Acc@1  92.97 ( 92.82)	Acc@5 100.00 ( 99.64)
Epoch: [50][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2485e-01 (2.6228e-01)	Acc@1  94.53 ( 92.81)	Acc@5  99.22 ( 99.64)
Epoch: [50][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.6562e-01 (2.6284e-01)	Acc@1  91.41 ( 92.80)	Acc@5 100.00 ( 99.64)
Epoch: [50][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.4268e-01 (2.6349e-01)	Acc@1  92.19 ( 92.77)	Acc@5 100.00 ( 99.64)
Epoch: [50][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1228e-01 (2.6353e-01)	Acc@1  94.53 ( 92.77)	Acc@5 100.00 ( 99.64)
Epoch: [50][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3120e-01 (2.6376e-01)	Acc@1  95.31 ( 92.77)	Acc@5 100.00 ( 99.64)
Epoch: [50][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5732e-01 (2.6517e-01)	Acc@1  92.97 ( 92.70)	Acc@5  99.22 ( 99.63)
Epoch: [50][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5293e-01 (2.6535e-01)	Acc@1  93.75 ( 92.71)	Acc@5 100.00 ( 99.63)
Epoch: [50][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.1885e-01 (2.6601e-01)	Acc@1  87.50 ( 92.67)	Acc@5 100.00 ( 99.62)
## e[50] optimizer.zero_grad (sum) time: 0.5460162162780762
## e[50]       loss.backward (sum) time: 12.557421207427979
## e[50]      optimizer.step (sum) time: 25.467544317245483
## epoch[50] training(only) time: 63.263731718063354
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.1836e+00 (1.1836e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.2861e+00 (1.2341e+00)	Acc@1  68.00 ( 70.55)	Acc@5  90.00 ( 90.27)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.1250e+00 (1.1935e+00)	Acc@1  75.00 ( 71.29)	Acc@5  93.00 ( 90.86)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4316e+00 (1.2256e+00)	Acc@1  64.00 ( 70.45)	Acc@5  90.00 ( 90.81)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 9.9512e-01 (1.2102e+00)	Acc@1  72.00 ( 70.39)	Acc@5  92.00 ( 91.10)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.1719e+00 (1.2112e+00)	Acc@1  72.00 ( 70.49)	Acc@5  90.00 ( 90.92)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1201e+00 (1.1979e+00)	Acc@1  68.00 ( 70.49)	Acc@5  91.00 ( 91.18)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.1992e+00 (1.1950e+00)	Acc@1  65.00 ( 70.51)	Acc@5  94.00 ( 91.21)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.4375e+00 (1.1981e+00)	Acc@1  70.00 ( 70.40)	Acc@5  88.00 ( 91.17)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3418e+00 (1.1820e+00)	Acc@1  70.00 ( 70.65)	Acc@5  90.00 ( 91.30)
 * Acc@1 70.900 Acc@5 91.380
### epoch[50] execution time: 68.19356966018677
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.330 ( 0.330)	Data  0.159 ( 0.159)	Loss 2.0483e-01 (2.0483e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.015)	Loss 2.1692e-01 (2.5754e-01)	Acc@1  96.88 ( 93.39)	Acc@5  99.22 ( 99.50)
Epoch: [51][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.8955e-01 (2.6093e-01)	Acc@1  91.41 ( 92.97)	Acc@5  99.22 ( 99.52)
Epoch: [51][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.3315e-01 (2.4590e-01)	Acc@1  92.97 ( 93.62)	Acc@5 100.00 ( 99.65)
Epoch: [51][ 40/391]	Time  0.172 ( 0.165)	Data  0.001 ( 0.005)	Loss 2.7856e-01 (2.4774e-01)	Acc@1  92.19 ( 93.41)	Acc@5 100.00 ( 99.71)
Epoch: [51][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.4939e-01 (2.4589e-01)	Acc@1  92.97 ( 93.50)	Acc@5 100.00 ( 99.74)
Epoch: [51][ 60/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.4243e-01 (2.4299e-01)	Acc@1  92.97 ( 93.66)	Acc@5 100.00 ( 99.74)
Epoch: [51][ 70/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4146e-01 (2.4129e-01)	Acc@1  94.53 ( 93.66)	Acc@5 100.00 ( 99.77)
Epoch: [51][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3340e-01 (2.4067e-01)	Acc@1  93.75 ( 93.71)	Acc@5 100.00 ( 99.79)
Epoch: [51][ 90/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.2412e-01 (2.4558e-01)	Acc@1  93.75 ( 93.57)	Acc@5 100.00 ( 99.75)
Epoch: [51][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4902e-01 (2.4339e-01)	Acc@1  92.97 ( 93.66)	Acc@5 100.00 ( 99.77)
Epoch: [51][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4365e-01 (2.4361e-01)	Acc@1  92.97 ( 93.66)	Acc@5  98.44 ( 99.73)
Epoch: [51][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0127e-01 (2.4452e-01)	Acc@1  91.41 ( 93.60)	Acc@5 100.00 ( 99.73)
Epoch: [51][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3828e-01 (2.4488e-01)	Acc@1  93.75 ( 93.55)	Acc@5 100.00 ( 99.73)
Epoch: [51][140/391]	Time  0.170 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8835e-01 (2.4445e-01)	Acc@1  96.88 ( 93.59)	Acc@5 100.00 ( 99.73)
Epoch: [51][150/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1460e-01 (2.4535e-01)	Acc@1  96.09 ( 93.58)	Acc@5 100.00 ( 99.73)
Epoch: [51][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3938e-01 (2.4622e-01)	Acc@1  95.31 ( 93.55)	Acc@5 100.00 ( 99.73)
Epoch: [51][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2852e-01 (2.4580e-01)	Acc@1  94.53 ( 93.60)	Acc@5 100.00 ( 99.72)
Epoch: [51][180/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7734e-01 (2.4581e-01)	Acc@1  94.53 ( 93.64)	Acc@5  98.44 ( 99.72)
Epoch: [51][190/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7808e-01 (2.4743e-01)	Acc@1  93.75 ( 93.59)	Acc@5 100.00 ( 99.71)
Epoch: [51][200/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2498e-01 (2.4758e-01)	Acc@1  94.53 ( 93.59)	Acc@5 100.00 ( 99.72)
Epoch: [51][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8979e-01 (2.4753e-01)	Acc@1  89.84 ( 93.58)	Acc@5 100.00 ( 99.72)
Epoch: [51][220/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7344e-01 (2.4746e-01)	Acc@1  95.31 ( 93.58)	Acc@5  99.22 ( 99.72)
Epoch: [51][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1375e-01 (2.4706e-01)	Acc@1  93.75 ( 93.59)	Acc@5 100.00 ( 99.72)
Epoch: [51][240/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5684e-01 (2.4718e-01)	Acc@1  93.75 ( 93.56)	Acc@5  99.22 ( 99.72)
Epoch: [51][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9543e-01 (2.4684e-01)	Acc@1  93.75 ( 93.57)	Acc@5 100.00 ( 99.72)
Epoch: [51][260/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0762e-01 (2.4699e-01)	Acc@1  91.41 ( 93.57)	Acc@5  98.44 ( 99.72)
Epoch: [51][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4341e-01 (2.4823e-01)	Acc@1  94.53 ( 93.48)	Acc@5  99.22 ( 99.72)
Epoch: [51][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8174e-01 (2.4833e-01)	Acc@1  93.75 ( 93.48)	Acc@5 100.00 ( 99.73)
Epoch: [51][290/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1899e-01 (2.4777e-01)	Acc@1  91.41 ( 93.49)	Acc@5 100.00 ( 99.73)
Epoch: [51][300/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.1396e-01 (2.4763e-01)	Acc@1  89.84 ( 93.51)	Acc@5  99.22 ( 99.73)
Epoch: [51][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3413e-01 (2.4813e-01)	Acc@1  93.75 ( 93.50)	Acc@5 100.00 ( 99.73)
Epoch: [51][320/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3303e-01 (2.4891e-01)	Acc@1  92.97 ( 93.46)	Acc@5 100.00 ( 99.73)
Epoch: [51][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.2739e-01 (2.4933e-01)	Acc@1  92.97 ( 93.45)	Acc@5  99.22 ( 99.73)
Epoch: [51][340/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7002e-01 (2.4942e-01)	Acc@1  91.41 ( 93.44)	Acc@5 100.00 ( 99.72)
Epoch: [51][350/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5586e-01 (2.4931e-01)	Acc@1  92.19 ( 93.45)	Acc@5  99.22 ( 99.72)
Epoch: [51][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.8345e-01 (2.4941e-01)	Acc@1  91.41 ( 93.46)	Acc@5 100.00 ( 99.72)
Epoch: [51][370/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.8857e-01 (2.4956e-01)	Acc@1  89.84 ( 93.45)	Acc@5 100.00 ( 99.72)
Epoch: [51][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1875e-01 (2.5049e-01)	Acc@1  96.09 ( 93.41)	Acc@5 100.00 ( 99.72)
Epoch: [51][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.3008e-01 (2.5132e-01)	Acc@1  92.50 ( 93.37)	Acc@5  98.75 ( 99.72)
## e[51] optimizer.zero_grad (sum) time: 0.5459489822387695
## e[51]       loss.backward (sum) time: 12.578681707382202
## e[51]      optimizer.step (sum) time: 25.472662448883057
## epoch[51] training(only) time: 63.428900480270386
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.0830e+00 (1.0830e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.1982e+00 (1.2101e+00)	Acc@1  68.00 ( 70.18)	Acc@5  90.00 ( 90.09)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.0996e+00 (1.1782e+00)	Acc@1  72.00 ( 71.00)	Acc@5  92.00 ( 91.05)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.4014e+00 (1.2136e+00)	Acc@1  62.00 ( 70.10)	Acc@5  89.00 ( 91.06)
Test: [ 40/100]	Time  0.057 ( 0.050)	Loss 9.9854e-01 (1.2003e+00)	Acc@1  71.00 ( 70.17)	Acc@5  92.00 ( 91.39)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.1641e+00 (1.2005e+00)	Acc@1  68.00 ( 70.08)	Acc@5  92.00 ( 91.29)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.2021e+00 (1.1887e+00)	Acc@1  68.00 ( 69.98)	Acc@5  90.00 ( 91.38)
Test: [ 70/100]	Time  0.050 ( 0.049)	Loss 1.2920e+00 (1.1877e+00)	Acc@1  68.00 ( 70.06)	Acc@5  94.00 ( 91.44)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.5088e+00 (1.1911e+00)	Acc@1  67.00 ( 69.94)	Acc@5  87.00 ( 91.35)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.2510e+00 (1.1775e+00)	Acc@1  70.00 ( 70.27)	Acc@5  91.00 ( 91.40)
 * Acc@1 70.460 Acc@5 91.460
### epoch[51] execution time: 68.33905386924744
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.321 ( 0.321)	Data  0.153 ( 0.153)	Loss 1.8188e-01 (1.8188e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.8542e-01 (2.2320e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.86)
Epoch: [52][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.4573e-01 (2.2224e-01)	Acc@1  95.31 ( 94.35)	Acc@5 100.00 ( 99.93)
Epoch: [52][ 30/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.5195e-01 (2.2523e-01)	Acc@1  93.75 ( 94.23)	Acc@5  99.22 ( 99.90)
Epoch: [52][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.7957e-01 (2.2977e-01)	Acc@1  95.31 ( 94.05)	Acc@5 100.00 ( 99.83)
Epoch: [52][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.6025e-01 (2.2876e-01)	Acc@1  92.97 ( 94.13)	Acc@5 100.00 ( 99.85)
Epoch: [52][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.8701e-01 (2.3117e-01)	Acc@1  96.09 ( 93.98)	Acc@5 100.00 ( 99.82)
Epoch: [52][ 70/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4121e-01 (2.3460e-01)	Acc@1  90.62 ( 93.75)	Acc@5 100.00 ( 99.80)
Epoch: [52][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4194e-01 (2.3386e-01)	Acc@1  94.53 ( 93.77)	Acc@5  99.22 ( 99.78)
Epoch: [52][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3962e-01 (2.3394e-01)	Acc@1  95.31 ( 93.77)	Acc@5 100.00 ( 99.79)
Epoch: [52][100/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.9604e-01 (2.3619e-01)	Acc@1  95.31 ( 93.71)	Acc@5  99.22 ( 99.78)
Epoch: [52][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5806e-01 (2.3652e-01)	Acc@1  92.19 ( 93.67)	Acc@5  99.22 ( 99.76)
Epoch: [52][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 3.0518e-01 (2.3695e-01)	Acc@1  94.53 ( 93.72)	Acc@5  98.44 ( 99.74)
Epoch: [52][130/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8750e-01 (2.3633e-01)	Acc@1  94.53 ( 93.74)	Acc@5 100.00 ( 99.73)
Epoch: [52][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8735e-01 (2.3640e-01)	Acc@1  92.19 ( 93.78)	Acc@5 100.00 ( 99.73)
Epoch: [52][150/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6196e-01 (2.3757e-01)	Acc@1  96.88 ( 93.76)	Acc@5  99.22 ( 99.73)
Epoch: [52][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6855e-01 (2.3805e-01)	Acc@1  93.75 ( 93.79)	Acc@5  99.22 ( 99.73)
Epoch: [52][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2778e-01 (2.3720e-01)	Acc@1  96.09 ( 93.83)	Acc@5 100.00 ( 99.73)
Epoch: [52][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6528e-01 (2.3635e-01)	Acc@1  97.66 ( 93.88)	Acc@5 100.00 ( 99.73)
Epoch: [52][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.1299e-01 (2.3798e-01)	Acc@1  89.84 ( 93.83)	Acc@5 100.00 ( 99.73)
Epoch: [52][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9189e-01 (2.3758e-01)	Acc@1  96.09 ( 93.82)	Acc@5 100.00 ( 99.74)
Epoch: [52][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5684e-01 (2.3851e-01)	Acc@1  92.97 ( 93.77)	Acc@5 100.00 ( 99.74)
Epoch: [52][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9897e-01 (2.3927e-01)	Acc@1  96.88 ( 93.73)	Acc@5 100.00 ( 99.74)
Epoch: [52][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2119e-01 (2.3868e-01)	Acc@1  93.75 ( 93.74)	Acc@5  99.22 ( 99.75)
Epoch: [52][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2742e-01 (2.3880e-01)	Acc@1  93.75 ( 93.74)	Acc@5 100.00 ( 99.76)
Epoch: [52][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2583e-01 (2.3885e-01)	Acc@1  96.09 ( 93.76)	Acc@5 100.00 ( 99.76)
Epoch: [52][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4365e-01 (2.3887e-01)	Acc@1  92.19 ( 93.74)	Acc@5 100.00 ( 99.76)
Epoch: [52][270/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.8418e-01 (2.3848e-01)	Acc@1  89.06 ( 93.74)	Acc@5 100.00 ( 99.76)
Epoch: [52][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6978e-01 (2.3913e-01)	Acc@1  92.19 ( 93.71)	Acc@5  99.22 ( 99.75)
Epoch: [52][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4121e-01 (2.3883e-01)	Acc@1  93.75 ( 93.71)	Acc@5 100.00 ( 99.76)
Epoch: [52][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1960e-01 (2.3883e-01)	Acc@1  93.75 ( 93.71)	Acc@5 100.00 ( 99.76)
Epoch: [52][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1655e-01 (2.3907e-01)	Acc@1  92.97 ( 93.70)	Acc@5 100.00 ( 99.77)
Epoch: [52][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7466e-01 (2.3971e-01)	Acc@1  94.53 ( 93.67)	Acc@5 100.00 ( 99.77)
Epoch: [52][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5439e-01 (2.3964e-01)	Acc@1  93.75 ( 93.67)	Acc@5  99.22 ( 99.77)
Epoch: [52][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2913e-01 (2.4047e-01)	Acc@1  93.75 ( 93.64)	Acc@5  99.22 ( 99.77)
Epoch: [52][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.5083e-01 (2.4172e-01)	Acc@1  88.28 ( 93.59)	Acc@5 100.00 ( 99.77)
Epoch: [52][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.4573e-01 (2.4210e-01)	Acc@1  92.97 ( 93.55)	Acc@5 100.00 ( 99.77)
Epoch: [52][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.9541e-01 (2.4264e-01)	Acc@1  89.06 ( 93.52)	Acc@5 100.00 ( 99.77)
Epoch: [52][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9690e-01 (2.4292e-01)	Acc@1  94.53 ( 93.50)	Acc@5 100.00 ( 99.76)
Epoch: [52][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.0884e-01 (2.4291e-01)	Acc@1  91.25 ( 93.51)	Acc@5 100.00 ( 99.76)
## e[52] optimizer.zero_grad (sum) time: 0.5388345718383789
## e[52]       loss.backward (sum) time: 12.568209409713745
## e[52]      optimizer.step (sum) time: 25.494222164154053
## epoch[52] training(only) time: 63.290876626968384
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 1.1768e+00 (1.1768e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.2969e+00 (1.2447e+00)	Acc@1  67.00 ( 70.09)	Acc@5  89.00 ( 89.45)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.1973e+00 (1.2152e+00)	Acc@1  70.00 ( 70.33)	Acc@5  91.00 ( 90.43)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4434e+00 (1.2429e+00)	Acc@1  61.00 ( 69.77)	Acc@5  90.00 ( 90.42)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 9.8828e-01 (1.2262e+00)	Acc@1  72.00 ( 69.78)	Acc@5  93.00 ( 90.85)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2129e+00 (1.2312e+00)	Acc@1  73.00 ( 69.71)	Acc@5  91.00 ( 90.75)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1943e+00 (1.2169e+00)	Acc@1  68.00 ( 69.84)	Acc@5  91.00 ( 90.87)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.3154e+00 (1.2199e+00)	Acc@1  66.00 ( 69.85)	Acc@5  92.00 ( 90.90)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5000e+00 (1.2186e+00)	Acc@1  70.00 ( 69.81)	Acc@5  86.00 ( 90.91)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3271e+00 (1.2011e+00)	Acc@1  70.00 ( 70.13)	Acc@5  91.00 ( 91.08)
 * Acc@1 70.290 Acc@5 91.100
### epoch[52] execution time: 68.25485563278198
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.331 ( 0.331)	Data  0.146 ( 0.146)	Loss 2.2156e-01 (2.2156e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [53][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 2.2473e-01 (2.1191e-01)	Acc@1  93.75 ( 94.67)	Acc@5 100.00 ( 99.72)
Epoch: [53][ 20/391]	Time  0.160 ( 0.170)	Data  0.001 ( 0.008)	Loss 1.8542e-01 (2.1083e-01)	Acc@1  96.09 ( 94.68)	Acc@5 100.00 ( 99.81)
Epoch: [53][ 30/391]	Time  0.159 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.2803e-01 (2.2477e-01)	Acc@1  92.97 ( 93.93)	Acc@5  99.22 ( 99.77)
Epoch: [53][ 40/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.004)	Loss 2.3572e-01 (2.2932e-01)	Acc@1  94.53 ( 93.77)	Acc@5  99.22 ( 99.75)
Epoch: [53][ 50/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.6003e-01 (2.2555e-01)	Acc@1  94.53 ( 94.03)	Acc@5 100.00 ( 99.77)
Epoch: [53][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1436e-01 (2.2633e-01)	Acc@1  96.09 ( 93.92)	Acc@5 100.00 ( 99.74)
Epoch: [53][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1338e-01 (2.2523e-01)	Acc@1  96.09 ( 94.04)	Acc@5 100.00 ( 99.78)
Epoch: [53][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.1704e-01 (2.2609e-01)	Acc@1  92.19 ( 93.97)	Acc@5 100.00 ( 99.78)
Epoch: [53][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4182e-01 (2.2441e-01)	Acc@1  93.75 ( 94.03)	Acc@5 100.00 ( 99.80)
Epoch: [53][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4353e-01 (2.2681e-01)	Acc@1  93.75 ( 93.99)	Acc@5 100.00 ( 99.79)
Epoch: [53][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7383e-01 (2.2737e-01)	Acc@1  93.75 ( 93.95)	Acc@5  98.44 ( 99.79)
Epoch: [53][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9312e-01 (2.2723e-01)	Acc@1  94.53 ( 93.92)	Acc@5 100.00 ( 99.79)
Epoch: [53][130/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1851e-01 (2.2669e-01)	Acc@1  96.09 ( 93.96)	Acc@5 100.00 ( 99.78)
Epoch: [53][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5881e-01 (2.2730e-01)	Acc@1  96.09 ( 93.96)	Acc@5 100.00 ( 99.77)
Epoch: [53][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3303e-01 (2.2910e-01)	Acc@1  93.75 ( 93.89)	Acc@5 100.00 ( 99.77)
Epoch: [53][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8298e-01 (2.2969e-01)	Acc@1  95.31 ( 93.90)	Acc@5  99.22 ( 99.76)
Epoch: [53][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3999e-01 (2.2985e-01)	Acc@1  92.97 ( 93.90)	Acc@5 100.00 ( 99.77)
Epoch: [53][180/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5732e-01 (2.2990e-01)	Acc@1  93.75 ( 93.93)	Acc@5  99.22 ( 99.77)
Epoch: [53][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0337e-01 (2.3088e-01)	Acc@1  92.97 ( 93.86)	Acc@5  99.22 ( 99.76)
Epoch: [53][200/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3523e-01 (2.3093e-01)	Acc@1  95.31 ( 93.89)	Acc@5 100.00 ( 99.76)
Epoch: [53][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3511e-01 (2.2983e-01)	Acc@1  93.75 ( 93.90)	Acc@5 100.00 ( 99.76)
Epoch: [53][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5977e-01 (2.3005e-01)	Acc@1  94.53 ( 93.94)	Acc@5  99.22 ( 99.76)
Epoch: [53][230/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8872e-01 (2.3077e-01)	Acc@1  95.31 ( 93.91)	Acc@5 100.00 ( 99.76)
Epoch: [53][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.9517e-01 (2.3083e-01)	Acc@1  89.06 ( 93.91)	Acc@5 100.00 ( 99.77)
Epoch: [53][250/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1960e-01 (2.3148e-01)	Acc@1  94.53 ( 93.91)	Acc@5 100.00 ( 99.77)
Epoch: [53][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1423e-01 (2.3110e-01)	Acc@1  95.31 ( 93.92)	Acc@5  99.22 ( 99.78)
Epoch: [53][270/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 3.0688e-01 (2.3113e-01)	Acc@1  93.75 ( 93.91)	Acc@5  98.44 ( 99.77)
Epoch: [53][280/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.6123e-01 (2.3125e-01)	Acc@1  90.62 ( 93.88)	Acc@5 100.00 ( 99.77)
Epoch: [53][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1924e-01 (2.3086e-01)	Acc@1  94.53 ( 93.89)	Acc@5 100.00 ( 99.77)
Epoch: [53][300/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7100e-01 (2.3092e-01)	Acc@1  92.19 ( 93.89)	Acc@5 100.00 ( 99.77)
Epoch: [53][310/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.5977e-01 (2.3162e-01)	Acc@1  92.97 ( 93.84)	Acc@5 100.00 ( 99.76)
Epoch: [53][320/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1826e-01 (2.3148e-01)	Acc@1  94.53 ( 93.84)	Acc@5 100.00 ( 99.77)
Epoch: [53][330/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3450e-01 (2.3114e-01)	Acc@1  93.75 ( 93.85)	Acc@5 100.00 ( 99.77)
Epoch: [53][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3013e-01 (2.3074e-01)	Acc@1  98.44 ( 93.87)	Acc@5 100.00 ( 99.78)
Epoch: [53][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8567e-01 (2.3155e-01)	Acc@1  93.75 ( 93.84)	Acc@5 100.00 ( 99.77)
Epoch: [53][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0837e-01 (2.3159e-01)	Acc@1  95.31 ( 93.85)	Acc@5 100.00 ( 99.76)
Epoch: [53][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3230e-01 (2.3186e-01)	Acc@1  95.31 ( 93.83)	Acc@5 100.00 ( 99.77)
Epoch: [53][380/391]	Time  0.174 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.2764e-01 (2.3254e-01)	Acc@1  90.62 ( 93.80)	Acc@5 100.00 ( 99.76)
Epoch: [53][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 3.5083e-01 (2.3283e-01)	Acc@1  92.50 ( 93.79)	Acc@5 100.00 ( 99.77)
## e[53] optimizer.zero_grad (sum) time: 0.5427680015563965
## e[53]       loss.backward (sum) time: 12.602623462677002
## e[53]      optimizer.step (sum) time: 25.466509342193604
## epoch[53] training(only) time: 63.45179986953735
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.1367e+00 (1.1367e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.3936e+00 (1.2769e+00)	Acc@1  65.00 ( 69.55)	Acc@5  92.00 ( 89.55)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.1953e+00 (1.2257e+00)	Acc@1  69.00 ( 70.19)	Acc@5  90.00 ( 90.76)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4277e+00 (1.2494e+00)	Acc@1  62.00 ( 69.58)	Acc@5  90.00 ( 90.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0186e+00 (1.2349e+00)	Acc@1  75.00 ( 69.56)	Acc@5  93.00 ( 91.00)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2549e+00 (1.2386e+00)	Acc@1  70.00 ( 69.51)	Acc@5  93.00 ( 90.96)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.2549e+00 (1.2254e+00)	Acc@1  66.00 ( 69.48)	Acc@5  92.00 ( 90.98)
Test: [ 70/100]	Time  0.056 ( 0.049)	Loss 1.2676e+00 (1.2247e+00)	Acc@1  66.00 ( 69.39)	Acc@5  92.00 ( 91.06)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5459e+00 (1.2294e+00)	Acc@1  70.00 ( 69.41)	Acc@5  88.00 ( 91.04)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.3486e+00 (1.2148e+00)	Acc@1  67.00 ( 69.80)	Acc@5  90.00 ( 91.15)
 * Acc@1 70.070 Acc@5 91.220
### epoch[53] execution time: 68.39313411712646
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.316 ( 0.316)	Data  0.143 ( 0.143)	Loss 2.0349e-01 (2.0349e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.6492e-01 (2.1758e-01)	Acc@1  96.88 ( 94.60)	Acc@5 100.00 ( 99.79)
Epoch: [54][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.7566e-01 (2.0801e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.85)
Epoch: [54][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.9067e-01 (2.1032e-01)	Acc@1  95.31 ( 94.66)	Acc@5 100.00 ( 99.85)
Epoch: [54][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.3523e-01 (2.1562e-01)	Acc@1  95.31 ( 94.44)	Acc@5  99.22 ( 99.79)
Epoch: [54][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.1362e-01 (2.2035e-01)	Acc@1  96.88 ( 94.32)	Acc@5  99.22 ( 99.74)
Epoch: [54][ 60/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1072e-01 (2.1346e-01)	Acc@1  99.22 ( 94.65)	Acc@5 100.00 ( 99.77)
Epoch: [54][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9617e-01 (2.1524e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.79)
Epoch: [54][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.1436e-01 (2.1540e-01)	Acc@1  94.53 ( 94.55)	Acc@5 100.00 ( 99.81)
Epoch: [54][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.0593e-01 (2.1577e-01)	Acc@1  93.75 ( 94.51)	Acc@5 100.00 ( 99.79)
Epoch: [54][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4353e-01 (2.1689e-01)	Acc@1  94.53 ( 94.49)	Acc@5 100.00 ( 99.77)
Epoch: [54][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.5903e-01 (2.1605e-01)	Acc@1  92.19 ( 94.47)	Acc@5 100.00 ( 99.78)
Epoch: [54][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0288e-01 (2.1581e-01)	Acc@1  94.53 ( 94.42)	Acc@5 100.00 ( 99.79)
Epoch: [54][130/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7285e-01 (2.1389e-01)	Acc@1  96.09 ( 94.49)	Acc@5 100.00 ( 99.79)
Epoch: [54][140/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4744e-01 (2.1396e-01)	Acc@1  92.97 ( 94.46)	Acc@5 100.00 ( 99.78)
Epoch: [54][150/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1069e-01 (2.1798e-01)	Acc@1  94.53 ( 94.32)	Acc@5  99.22 ( 99.75)
Epoch: [54][160/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9019e-01 (2.1697e-01)	Acc@1  96.88 ( 94.35)	Acc@5 100.00 ( 99.75)
Epoch: [54][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0911e-01 (2.1721e-01)	Acc@1  94.53 ( 94.33)	Acc@5 100.00 ( 99.75)
Epoch: [54][180/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3328e-01 (2.1729e-01)	Acc@1  96.88 ( 94.34)	Acc@5  99.22 ( 99.76)
Epoch: [54][190/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7346e-01 (2.1662e-01)	Acc@1  96.09 ( 94.36)	Acc@5 100.00 ( 99.77)
Epoch: [54][200/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7588e-01 (2.1648e-01)	Acc@1  89.84 ( 94.36)	Acc@5  99.22 ( 99.77)
Epoch: [54][210/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3059e-01 (2.1611e-01)	Acc@1  95.31 ( 94.39)	Acc@5 100.00 ( 99.76)
Epoch: [54][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3889e-01 (2.1706e-01)	Acc@1  95.31 ( 94.37)	Acc@5  99.22 ( 99.76)
Epoch: [54][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1582e-01 (2.1661e-01)	Acc@1  95.31 ( 94.40)	Acc@5 100.00 ( 99.76)
Epoch: [54][240/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1851e-01 (2.1635e-01)	Acc@1  94.53 ( 94.41)	Acc@5  99.22 ( 99.77)
Epoch: [54][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8921e-01 (2.1600e-01)	Acc@1  96.09 ( 94.40)	Acc@5 100.00 ( 99.77)
Epoch: [54][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7041e-01 (2.1709e-01)	Acc@1  96.88 ( 94.37)	Acc@5 100.00 ( 99.77)
Epoch: [54][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9250e-01 (2.1709e-01)	Acc@1  94.53 ( 94.36)	Acc@5 100.00 ( 99.78)
Epoch: [54][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8359e-01 (2.1725e-01)	Acc@1  93.75 ( 94.36)	Acc@5 100.00 ( 99.79)
Epoch: [54][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5269e-01 (2.1720e-01)	Acc@1  94.53 ( 94.35)	Acc@5  99.22 ( 99.79)
Epoch: [54][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7100e-01 (2.1782e-01)	Acc@1  92.97 ( 94.31)	Acc@5 100.00 ( 99.79)
Epoch: [54][310/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8884e-01 (2.1855e-01)	Acc@1  94.53 ( 94.29)	Acc@5 100.00 ( 99.79)
Epoch: [54][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7124e-01 (2.1879e-01)	Acc@1  93.75 ( 94.31)	Acc@5  99.22 ( 99.78)
Epoch: [54][330/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.6514e-01 (2.1915e-01)	Acc@1  90.62 ( 94.32)	Acc@5 100.00 ( 99.78)
Epoch: [54][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3450e-01 (2.1930e-01)	Acc@1  95.31 ( 94.31)	Acc@5 100.00 ( 99.78)
Epoch: [54][350/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2070e-01 (2.1923e-01)	Acc@1  95.31 ( 94.32)	Acc@5  99.22 ( 99.78)
Epoch: [54][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.4707e-01 (2.1973e-01)	Acc@1  91.41 ( 94.31)	Acc@5 100.00 ( 99.78)
Epoch: [54][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5269e-01 (2.1969e-01)	Acc@1  92.19 ( 94.31)	Acc@5 100.00 ( 99.79)
Epoch: [54][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.6953e-01 (2.2004e-01)	Acc@1  94.53 ( 94.30)	Acc@5  99.22 ( 99.79)
Epoch: [54][390/391]	Time  0.125 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5806e-01 (2.1998e-01)	Acc@1  90.00 ( 94.31)	Acc@5 100.00 ( 99.79)
## e[54] optimizer.zero_grad (sum) time: 0.5456039905548096
## e[54]       loss.backward (sum) time: 12.599645853042603
## e[54]      optimizer.step (sum) time: 25.444488048553467
## epoch[54] training(only) time: 63.37677884101868
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.1582e+00 (1.1582e+00)	Acc@1  75.00 ( 75.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.4141e+00 (1.2451e+00)	Acc@1  66.00 ( 71.18)	Acc@5  89.00 ( 90.18)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.0869e+00 (1.2131e+00)	Acc@1  69.00 ( 70.76)	Acc@5  94.00 ( 91.29)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 1.4580e+00 (1.2451e+00)	Acc@1  64.00 ( 70.06)	Acc@5  90.00 ( 90.97)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0244e+00 (1.2247e+00)	Acc@1  72.00 ( 70.20)	Acc@5  91.00 ( 91.29)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2969e+00 (1.2298e+00)	Acc@1  69.00 ( 70.08)	Acc@5  91.00 ( 91.20)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1963e+00 (1.2231e+00)	Acc@1  68.00 ( 70.05)	Acc@5  88.00 ( 91.15)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.2793e+00 (1.2208e+00)	Acc@1  66.00 ( 70.10)	Acc@5  93.00 ( 91.21)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5908e+00 (1.2266e+00)	Acc@1  69.00 ( 69.93)	Acc@5  87.00 ( 91.16)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.2686e+00 (1.2098e+00)	Acc@1  70.00 ( 70.30)	Acc@5  91.00 ( 91.31)
 * Acc@1 70.580 Acc@5 91.470
### epoch[54] execution time: 68.31518816947937
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.318 ( 0.318)	Data  0.141 ( 0.141)	Loss 1.6907e-01 (1.6907e-01)	Acc@1  97.66 ( 97.66)	Acc@5  99.22 ( 99.22)
Epoch: [55][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 2.4915e-01 (2.1505e-01)	Acc@1  93.75 ( 94.53)	Acc@5 100.00 ( 99.79)
Epoch: [55][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 2.2791e-01 (2.0600e-01)	Acc@1  92.97 ( 94.64)	Acc@5  99.22 ( 99.81)
Epoch: [55][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.5027e-01 (1.9812e-01)	Acc@1  96.09 ( 95.14)	Acc@5 100.00 ( 99.80)
Epoch: [55][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.8481e-01 (1.9701e-01)	Acc@1  96.88 ( 95.06)	Acc@5 100.00 ( 99.83)
Epoch: [55][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 2.5391e-01 (1.9508e-01)	Acc@1  94.53 ( 95.22)	Acc@5 100.00 ( 99.85)
Epoch: [55][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4902e-01 (1.9743e-01)	Acc@1  92.97 ( 95.15)	Acc@5  99.22 ( 99.85)
Epoch: [55][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.4670e-01 (1.9995e-01)	Acc@1  94.53 ( 95.13)	Acc@5 100.00 ( 99.85)
Epoch: [55][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.3218e-01 (2.0246e-01)	Acc@1  92.97 ( 95.02)	Acc@5 100.00 ( 99.83)
Epoch: [55][ 90/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9458e-01 (2.0360e-01)	Acc@1  93.75 ( 94.90)	Acc@5 100.00 ( 99.84)
Epoch: [55][100/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6943e-01 (2.0392e-01)	Acc@1  96.09 ( 94.83)	Acc@5 100.00 ( 99.85)
Epoch: [55][110/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6443e-01 (2.0476e-01)	Acc@1  95.31 ( 94.79)	Acc@5 100.00 ( 99.85)
Epoch: [55][120/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4414e-01 (2.0447e-01)	Acc@1  92.19 ( 94.80)	Acc@5  99.22 ( 99.83)
Epoch: [55][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3206e-01 (2.0421e-01)	Acc@1  92.19 ( 94.80)	Acc@5 100.00 ( 99.81)
Epoch: [55][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6833e-01 (2.0299e-01)	Acc@1  96.88 ( 94.84)	Acc@5 100.00 ( 99.82)
Epoch: [55][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7456e-01 (2.0293e-01)	Acc@1  96.09 ( 94.83)	Acc@5  99.22 ( 99.82)
Epoch: [55][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7148e-01 (2.0469e-01)	Acc@1  92.97 ( 94.74)	Acc@5 100.00 ( 99.82)
Epoch: [55][170/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5991e-01 (2.0451e-01)	Acc@1  95.31 ( 94.78)	Acc@5 100.00 ( 99.82)
Epoch: [55][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3792e-01 (2.0491e-01)	Acc@1  92.97 ( 94.80)	Acc@5 100.00 ( 99.82)
Epoch: [55][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5808e-01 (2.0438e-01)	Acc@1  95.31 ( 94.82)	Acc@5 100.00 ( 99.83)
Epoch: [55][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0618e-01 (2.0414e-01)	Acc@1  94.53 ( 94.82)	Acc@5  99.22 ( 99.83)
Epoch: [55][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0996e-01 (2.0482e-01)	Acc@1  92.19 ( 94.81)	Acc@5 100.00 ( 99.83)
Epoch: [55][220/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5637e-01 (2.0425e-01)	Acc@1  97.66 ( 94.85)	Acc@5 100.00 ( 99.84)
Epoch: [55][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2229e-01 (2.0479e-01)	Acc@1  93.75 ( 94.83)	Acc@5 100.00 ( 99.84)
Epoch: [55][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6541e-01 (2.0484e-01)	Acc@1  98.44 ( 94.84)	Acc@5 100.00 ( 99.84)
Epoch: [55][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3096e-01 (2.0543e-01)	Acc@1  94.53 ( 94.84)	Acc@5  99.22 ( 99.84)
Epoch: [55][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5830e-01 (2.0653e-01)	Acc@1  91.41 ( 94.76)	Acc@5 100.00 ( 99.84)
Epoch: [55][270/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2058e-01 (2.0672e-01)	Acc@1  93.75 ( 94.75)	Acc@5 100.00 ( 99.84)
Epoch: [55][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9727e-01 (2.0629e-01)	Acc@1  95.31 ( 94.76)	Acc@5 100.00 ( 99.84)
Epoch: [55][290/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5024e-01 (2.0567e-01)	Acc@1  93.75 ( 94.77)	Acc@5  99.22 ( 99.84)
Epoch: [55][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0435e-01 (2.0572e-01)	Acc@1  93.75 ( 94.77)	Acc@5 100.00 ( 99.84)
Epoch: [55][310/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7515e-01 (2.0551e-01)	Acc@1  90.62 ( 94.75)	Acc@5 100.00 ( 99.84)
Epoch: [55][320/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1130e-01 (2.0564e-01)	Acc@1  91.41 ( 94.73)	Acc@5 100.00 ( 99.84)
Epoch: [55][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6748e-01 (2.0595e-01)	Acc@1  95.31 ( 94.73)	Acc@5 100.00 ( 99.84)
Epoch: [55][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9409e-01 (2.0627e-01)	Acc@1  95.31 ( 94.70)	Acc@5 100.00 ( 99.84)
Epoch: [55][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3132e-01 (2.0628e-01)	Acc@1  92.97 ( 94.70)	Acc@5 100.00 ( 99.84)
Epoch: [55][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6797e-01 (2.0600e-01)	Acc@1  95.31 ( 94.72)	Acc@5 100.00 ( 99.84)
Epoch: [55][370/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.7539e-01 (2.0597e-01)	Acc@1  92.19 ( 94.72)	Acc@5 100.00 ( 99.84)
Epoch: [55][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1521e-01 (2.0619e-01)	Acc@1  94.53 ( 94.72)	Acc@5 100.00 ( 99.84)
Epoch: [55][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2449e-01 (2.0647e-01)	Acc@1  92.50 ( 94.71)	Acc@5 100.00 ( 99.83)
## e[55] optimizer.zero_grad (sum) time: 0.5358021259307861
## e[55]       loss.backward (sum) time: 12.590757608413696
## e[55]      optimizer.step (sum) time: 25.471365213394165
## epoch[55] training(only) time: 63.24195885658264
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.1152e+00 (1.1152e+00)	Acc@1  75.00 ( 75.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.2627e+00 (1.2435e+00)	Acc@1  64.00 ( 70.36)	Acc@5  93.00 ( 90.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.1377e+00 (1.2275e+00)	Acc@1  72.00 ( 70.76)	Acc@5  91.00 ( 90.52)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.4580e+00 (1.2617e+00)	Acc@1  64.00 ( 69.90)	Acc@5  90.00 ( 90.52)
Test: [ 40/100]	Time  0.048 ( 0.050)	Loss 1.0449e+00 (1.2457e+00)	Acc@1  70.00 ( 69.68)	Acc@5  91.00 ( 90.78)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2959e+00 (1.2503e+00)	Acc@1  72.00 ( 69.84)	Acc@5  90.00 ( 90.86)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.2178e+00 (1.2364e+00)	Acc@1  69.00 ( 69.93)	Acc@5  89.00 ( 90.92)
Test: [ 70/100]	Time  0.049 ( 0.049)	Loss 1.2266e+00 (1.2321e+00)	Acc@1  63.00 ( 69.99)	Acc@5  93.00 ( 90.92)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5537e+00 (1.2340e+00)	Acc@1  70.00 ( 70.02)	Acc@5  86.00 ( 90.88)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2969e+00 (1.2161e+00)	Acc@1  71.00 ( 70.36)	Acc@5  90.00 ( 91.09)
 * Acc@1 70.530 Acc@5 91.180
### epoch[55] execution time: 68.21740579605103
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.330 ( 0.330)	Data  0.154 ( 0.154)	Loss 1.6870e-01 (1.6870e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 2.2229e-01 (1.9978e-01)	Acc@1  93.75 ( 94.60)	Acc@5  99.22 ( 99.86)
Epoch: [56][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.7920e-01 (1.9597e-01)	Acc@1  97.66 ( 94.94)	Acc@5 100.00 ( 99.85)
Epoch: [56][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 2.2644e-01 (1.9323e-01)	Acc@1  94.53 ( 94.98)	Acc@5 100.00 ( 99.87)
Epoch: [56][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.0215e-01 (1.9058e-01)	Acc@1  96.88 ( 95.20)	Acc@5  99.22 ( 99.87)
Epoch: [56][ 50/391]	Time  0.172 ( 0.166)	Data  0.001 ( 0.004)	Loss 1.6687e-01 (1.9070e-01)	Acc@1  97.66 ( 95.31)	Acc@5 100.00 ( 99.88)
Epoch: [56][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.7175e-01 (1.9187e-01)	Acc@1  98.44 ( 95.35)	Acc@5 100.00 ( 99.86)
Epoch: [56][ 70/391]	Time  0.176 ( 0.165)	Data  0.001 ( 0.003)	Loss 2.0679e-01 (1.9266e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.88)
Epoch: [56][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.8970e-01 (1.9084e-01)	Acc@1  94.53 ( 95.31)	Acc@5 100.00 ( 99.88)
Epoch: [56][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9592e-01 (1.9082e-01)	Acc@1  95.31 ( 95.33)	Acc@5 100.00 ( 99.90)
Epoch: [56][100/391]	Time  0.170 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7065e-01 (1.9143e-01)	Acc@1  94.53 ( 95.26)	Acc@5 100.00 ( 99.88)
Epoch: [56][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.8237e-01 (1.9260e-01)	Acc@1  96.09 ( 95.24)	Acc@5 100.00 ( 99.87)
Epoch: [56][120/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.8286e-01 (1.9245e-01)	Acc@1  93.75 ( 95.24)	Acc@5 100.00 ( 99.87)
Epoch: [56][130/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.4756e-01 (1.9363e-01)	Acc@1  92.19 ( 95.19)	Acc@5 100.00 ( 99.86)
Epoch: [56][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8286e-01 (1.9338e-01)	Acc@1  94.53 ( 95.17)	Acc@5 100.00 ( 99.87)
Epoch: [56][150/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3074e-01 (1.9333e-01)	Acc@1  98.44 ( 95.19)	Acc@5 100.00 ( 99.88)
Epoch: [56][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3684e-01 (1.9366e-01)	Acc@1  96.88 ( 95.20)	Acc@5 100.00 ( 99.87)
Epoch: [56][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5918e-01 (1.9278e-01)	Acc@1  96.88 ( 95.23)	Acc@5 100.00 ( 99.87)
Epoch: [56][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5430e-01 (1.9148e-01)	Acc@1  96.88 ( 95.31)	Acc@5 100.00 ( 99.87)
Epoch: [56][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8640e-01 (1.9247e-01)	Acc@1  97.66 ( 95.28)	Acc@5 100.00 ( 99.86)
Epoch: [56][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.2107e-01 (1.9321e-01)	Acc@1  94.53 ( 95.26)	Acc@5  99.22 ( 99.86)
Epoch: [56][210/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9751e-01 (1.9319e-01)	Acc@1  96.09 ( 95.25)	Acc@5 100.00 ( 99.86)
Epoch: [56][220/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3291e-01 (1.9335e-01)	Acc@1  91.41 ( 95.23)	Acc@5 100.00 ( 99.87)
Epoch: [56][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3513e-01 (1.9296e-01)	Acc@1  96.88 ( 95.22)	Acc@5 100.00 ( 99.87)
Epoch: [56][240/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6760e-01 (1.9354e-01)	Acc@1  93.75 ( 95.20)	Acc@5 100.00 ( 99.87)
Epoch: [56][250/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.6758e-01 (1.9418e-01)	Acc@1  93.75 ( 95.18)	Acc@5 100.00 ( 99.87)
Epoch: [56][260/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7441e-01 (1.9496e-01)	Acc@1  90.62 ( 95.14)	Acc@5 100.00 ( 99.86)
Epoch: [56][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7810e-01 (1.9595e-01)	Acc@1  96.88 ( 95.13)	Acc@5 100.00 ( 99.86)
Epoch: [56][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8835e-01 (1.9655e-01)	Acc@1  96.09 ( 95.12)	Acc@5 100.00 ( 99.85)
Epoch: [56][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0630e-01 (1.9651e-01)	Acc@1  92.97 ( 95.11)	Acc@5 100.00 ( 99.86)
Epoch: [56][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9727e-01 (1.9602e-01)	Acc@1  94.53 ( 95.12)	Acc@5 100.00 ( 99.86)
Epoch: [56][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9312e-01 (1.9625e-01)	Acc@1  95.31 ( 95.11)	Acc@5 100.00 ( 99.85)
Epoch: [56][320/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2445e-01 (1.9648e-01)	Acc@1 100.00 ( 95.13)	Acc@5 100.00 ( 99.85)
Epoch: [56][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8359e-01 (1.9649e-01)	Acc@1  96.09 ( 95.13)	Acc@5 100.00 ( 99.85)
Epoch: [56][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8860e-01 (1.9666e-01)	Acc@1  95.31 ( 95.12)	Acc@5 100.00 ( 99.85)
Epoch: [56][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6711e-01 (1.9679e-01)	Acc@1  94.53 ( 95.12)	Acc@5 100.00 ( 99.85)
Epoch: [56][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1350e-01 (1.9748e-01)	Acc@1  94.53 ( 95.10)	Acc@5 100.00 ( 99.85)
Epoch: [56][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3730e-01 (1.9799e-01)	Acc@1  92.19 ( 95.08)	Acc@5 100.00 ( 99.85)
Epoch: [56][380/391]	Time  0.175 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8250e-01 (1.9841e-01)	Acc@1  97.66 ( 95.07)	Acc@5 100.00 ( 99.85)
Epoch: [56][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.5146e-01 (1.9881e-01)	Acc@1  93.75 ( 95.06)	Acc@5  98.75 ( 99.84)
## e[56] optimizer.zero_grad (sum) time: 0.5420877933502197
## e[56]       loss.backward (sum) time: 12.56807565689087
## e[56]      optimizer.step (sum) time: 25.472020626068115
## epoch[56] training(only) time: 63.483492612838745
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.2041e+00 (1.2041e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.059 ( 0.060)	Loss 1.3604e+00 (1.2662e+00)	Acc@1  67.00 ( 70.82)	Acc@5  90.00 ( 90.09)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.1865e+00 (1.2366e+00)	Acc@1  68.00 ( 70.81)	Acc@5  93.00 ( 91.00)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 1.3877e+00 (1.2672e+00)	Acc@1  65.00 ( 69.97)	Acc@5  90.00 ( 90.81)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.0039e+00 (1.2500e+00)	Acc@1  73.00 ( 70.00)	Acc@5  92.00 ( 91.15)
Test: [ 50/100]	Time  0.046 ( 0.051)	Loss 1.3232e+00 (1.2540e+00)	Acc@1  70.00 ( 69.96)	Acc@5  88.00 ( 91.02)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2217e+00 (1.2366e+00)	Acc@1  67.00 ( 70.11)	Acc@5  88.00 ( 91.10)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3545e+00 (1.2336e+00)	Acc@1  64.00 ( 70.28)	Acc@5  91.00 ( 91.10)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5137e+00 (1.2402e+00)	Acc@1  71.00 ( 70.27)	Acc@5  87.00 ( 91.09)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2900e+00 (1.2227e+00)	Acc@1  72.00 ( 70.55)	Acc@5  90.00 ( 91.24)
 * Acc@1 70.780 Acc@5 91.300
### epoch[56] execution time: 68.47411561012268
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.329 ( 0.329)	Data  0.150 ( 0.150)	Loss 2.5781e-01 (2.5781e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.22 ( 99.22)
Epoch: [57][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.4795e-01 (1.8637e-01)	Acc@1  96.09 ( 95.67)	Acc@5 100.00 ( 99.79)
Epoch: [57][ 20/391]	Time  0.165 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.8420e-01 (1.8266e-01)	Acc@1  95.31 ( 95.61)	Acc@5  99.22 ( 99.81)
Epoch: [57][ 30/391]	Time  0.165 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.0251e-01 (1.9181e-01)	Acc@1  93.75 ( 95.19)	Acc@5 100.00 ( 99.80)
Epoch: [57][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.6394e-01 (1.9031e-01)	Acc@1  95.31 ( 95.22)	Acc@5 100.00 ( 99.85)
Epoch: [57][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.1438e-01 (1.8584e-01)	Acc@1 100.00 ( 95.44)	Acc@5 100.00 ( 99.88)
Epoch: [57][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6882e-01 (1.8314e-01)	Acc@1  96.09 ( 95.53)	Acc@5 100.00 ( 99.90)
Epoch: [57][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2805e-01 (1.8227e-01)	Acc@1  98.44 ( 95.55)	Acc@5 100.00 ( 99.89)
Epoch: [57][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.1338e-01 (1.8241e-01)	Acc@1  96.09 ( 95.60)	Acc@5 100.00 ( 99.89)
Epoch: [57][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.2559e-01 (1.8341e-01)	Acc@1  95.31 ( 95.63)	Acc@5  99.22 ( 99.89)
Epoch: [57][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5454e-01 (1.8332e-01)	Acc@1  95.31 ( 95.58)	Acc@5 100.00 ( 99.90)
Epoch: [57][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7737e-01 (1.8515e-01)	Acc@1  96.09 ( 95.51)	Acc@5 100.00 ( 99.88)
Epoch: [57][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7615e-01 (1.8400e-01)	Acc@1  96.09 ( 95.54)	Acc@5  99.22 ( 99.88)
Epoch: [57][130/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0154e-01 (1.8316e-01)	Acc@1  95.31 ( 95.62)	Acc@5 100.00 ( 99.89)
Epoch: [57][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4587e-01 (1.8229e-01)	Acc@1  96.88 ( 95.60)	Acc@5 100.00 ( 99.89)
Epoch: [57][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8921e-01 (1.8351e-01)	Acc@1  94.53 ( 95.50)	Acc@5 100.00 ( 99.90)
Epoch: [57][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0642e-01 (1.8457e-01)	Acc@1  96.88 ( 95.48)	Acc@5 100.00 ( 99.90)
Epoch: [57][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5552e-01 (1.8373e-01)	Acc@1  98.44 ( 95.54)	Acc@5 100.00 ( 99.90)
Epoch: [57][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2314e-01 (1.8532e-01)	Acc@1  96.09 ( 95.47)	Acc@5 100.00 ( 99.89)
Epoch: [57][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1436e-01 (1.8535e-01)	Acc@1  93.75 ( 95.48)	Acc@5 100.00 ( 99.89)
Epoch: [57][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9409e-01 (1.8568e-01)	Acc@1  93.75 ( 95.47)	Acc@5 100.00 ( 99.90)
Epoch: [57][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7017e-01 (1.8519e-01)	Acc@1  94.53 ( 95.51)	Acc@5 100.00 ( 99.90)
Epoch: [57][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8298e-01 (1.8550e-01)	Acc@1  95.31 ( 95.51)	Acc@5 100.00 ( 99.89)
Epoch: [57][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0251e-01 (1.8608e-01)	Acc@1  95.31 ( 95.47)	Acc@5 100.00 ( 99.90)
Epoch: [57][240/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9043e-01 (1.8584e-01)	Acc@1  94.53 ( 95.49)	Acc@5 100.00 ( 99.90)
Epoch: [57][250/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4563e-01 (1.8666e-01)	Acc@1  97.66 ( 95.46)	Acc@5 100.00 ( 99.90)
Epoch: [57][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.7002e-01 (1.8683e-01)	Acc@1  92.19 ( 95.45)	Acc@5  99.22 ( 99.90)
Epoch: [57][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4417e-01 (1.8699e-01)	Acc@1  97.66 ( 95.46)	Acc@5 100.00 ( 99.90)
Epoch: [57][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7151e-01 (1.8680e-01)	Acc@1  95.31 ( 95.47)	Acc@5 100.00 ( 99.89)
Epoch: [57][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4243e-01 (1.8701e-01)	Acc@1  92.97 ( 95.47)	Acc@5  98.44 ( 99.88)
Epoch: [57][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6382e-01 (1.8711e-01)	Acc@1  97.66 ( 95.47)	Acc@5 100.00 ( 99.88)
Epoch: [57][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5564e-01 (1.8704e-01)	Acc@1  95.31 ( 95.48)	Acc@5 100.00 ( 99.89)
Epoch: [57][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0007e-01 (1.8753e-01)	Acc@1  92.19 ( 95.45)	Acc@5 100.00 ( 99.88)
Epoch: [57][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.4744e-01 (1.8787e-01)	Acc@1  91.41 ( 95.44)	Acc@5 100.00 ( 99.88)
Epoch: [57][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4246e-01 (1.8786e-01)	Acc@1  96.09 ( 95.44)	Acc@5 100.00 ( 99.88)
Epoch: [57][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.6343e-01 (1.8824e-01)	Acc@1  90.62 ( 95.43)	Acc@5  99.22 ( 99.88)
Epoch: [57][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7273e-01 (1.8882e-01)	Acc@1  97.66 ( 95.41)	Acc@5 100.00 ( 99.88)
Epoch: [57][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8494e-01 (1.8924e-01)	Acc@1  95.31 ( 95.40)	Acc@5 100.00 ( 99.88)
Epoch: [57][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0996e-01 (1.8902e-01)	Acc@1  92.97 ( 95.41)	Acc@5 100.00 ( 99.88)
Epoch: [57][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7017e-01 (1.8872e-01)	Acc@1  96.25 ( 95.42)	Acc@5 100.00 ( 99.88)
## e[57] optimizer.zero_grad (sum) time: 0.5413625240325928
## e[57]       loss.backward (sum) time: 12.608614921569824
## e[57]      optimizer.step (sum) time: 25.450717210769653
## epoch[57] training(only) time: 63.251983642578125
# Switched to evaluate mode...
Test: [  0/100]	Time  0.213 ( 0.213)	Loss 1.2451e+00 (1.2451e+00)	Acc@1  70.00 ( 70.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.064)	Loss 1.4150e+00 (1.2973e+00)	Acc@1  66.00 ( 70.45)	Acc@5  91.00 ( 89.55)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.2041e+00 (1.2602e+00)	Acc@1  67.00 ( 70.33)	Acc@5  92.00 ( 90.43)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.4570e+00 (1.2954e+00)	Acc@1  63.00 ( 69.90)	Acc@5  90.00 ( 90.32)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0166e+00 (1.2731e+00)	Acc@1  77.00 ( 70.05)	Acc@5  90.00 ( 90.76)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.3066e+00 (1.2725e+00)	Acc@1  67.00 ( 69.92)	Acc@5  92.00 ( 90.67)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.2559e+00 (1.2577e+00)	Acc@1  68.00 ( 70.03)	Acc@5  86.00 ( 90.77)
Test: [ 70/100]	Time  0.047 ( 0.050)	Loss 1.4023e+00 (1.2588e+00)	Acc@1  63.00 ( 70.04)	Acc@5  92.00 ( 90.86)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6875e+00 (1.2613e+00)	Acc@1  68.00 ( 70.05)	Acc@5  87.00 ( 90.89)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.3467e+00 (1.2428e+00)	Acc@1  68.00 ( 70.29)	Acc@5  89.00 ( 91.04)
 * Acc@1 70.610 Acc@5 91.160
### epoch[57] execution time: 68.2518265247345
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.330 ( 0.330)	Data  0.153 ( 0.153)	Loss 1.4478e-01 (1.4478e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.8750e-01 (1.7675e-01)	Acc@1  96.88 ( 95.67)	Acc@5  99.22 ( 99.93)
Epoch: [58][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.9189e-01 (1.7901e-01)	Acc@1  94.53 ( 95.61)	Acc@5 100.00 ( 99.89)
Epoch: [58][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.7712e-01 (1.7762e-01)	Acc@1  96.09 ( 95.61)	Acc@5 100.00 ( 99.90)
Epoch: [58][ 40/391]	Time  0.172 ( 0.166)	Data  0.001 ( 0.005)	Loss 2.4280e-01 (1.7679e-01)	Acc@1  91.41 ( 95.54)	Acc@5 100.00 ( 99.90)
Epoch: [58][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.7346e-01 (1.7258e-01)	Acc@1  96.09 ( 95.79)	Acc@5 100.00 ( 99.91)
Epoch: [58][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9165e-01 (1.7424e-01)	Acc@1  95.31 ( 95.75)	Acc@5 100.00 ( 99.88)
Epoch: [58][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6675e-01 (1.7331e-01)	Acc@1  96.88 ( 95.79)	Acc@5  99.22 ( 99.89)
Epoch: [58][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6248e-01 (1.7277e-01)	Acc@1  96.88 ( 95.76)	Acc@5 100.00 ( 99.90)
Epoch: [58][ 90/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.9385e-01 (1.7233e-01)	Acc@1  92.97 ( 95.85)	Acc@5 100.00 ( 99.89)
Epoch: [58][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6077e-01 (1.7218e-01)	Acc@1  97.66 ( 95.86)	Acc@5 100.00 ( 99.89)
Epoch: [58][110/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7993e-01 (1.7235e-01)	Acc@1  94.53 ( 95.81)	Acc@5 100.00 ( 99.90)
Epoch: [58][120/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0911e-01 (1.7187e-01)	Acc@1  95.31 ( 95.82)	Acc@5  99.22 ( 99.90)
Epoch: [58][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3560e-01 (1.7274e-01)	Acc@1  93.75 ( 95.80)	Acc@5  99.22 ( 99.89)
Epoch: [58][140/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4038e-01 (1.7235e-01)	Acc@1  96.88 ( 95.77)	Acc@5 100.00 ( 99.90)
Epoch: [58][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0203e-01 (1.7250e-01)	Acc@1  94.53 ( 95.78)	Acc@5 100.00 ( 99.91)
Epoch: [58][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3086e-01 (1.7280e-01)	Acc@1  97.66 ( 95.77)	Acc@5 100.00 ( 99.91)
Epoch: [58][170/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7163e-01 (1.7456e-01)	Acc@1  96.09 ( 95.72)	Acc@5 100.00 ( 99.91)
Epoch: [58][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4172e-01 (1.7462e-01)	Acc@1  96.88 ( 95.71)	Acc@5 100.00 ( 99.91)
Epoch: [58][190/391]	Time  0.173 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8896e-01 (1.7491e-01)	Acc@1  94.53 ( 95.68)	Acc@5  99.22 ( 99.91)
Epoch: [58][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6138e-01 (1.7622e-01)	Acc@1  96.09 ( 95.63)	Acc@5 100.00 ( 99.90)
Epoch: [58][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1460e-01 (1.7631e-01)	Acc@1  92.97 ( 95.63)	Acc@5 100.00 ( 99.90)
Epoch: [58][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7529e-01 (1.7607e-01)	Acc@1  96.09 ( 95.63)	Acc@5 100.00 ( 99.90)
Epoch: [58][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2937e-01 (1.7682e-01)	Acc@1  96.09 ( 95.61)	Acc@5 100.00 ( 99.91)
Epoch: [58][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5808e-01 (1.7605e-01)	Acc@1  97.66 ( 95.63)	Acc@5 100.00 ( 99.91)
Epoch: [58][250/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9946e-01 (1.7661e-01)	Acc@1  93.75 ( 95.64)	Acc@5 100.00 ( 99.91)
Epoch: [58][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9885e-01 (1.7714e-01)	Acc@1  95.31 ( 95.63)	Acc@5 100.00 ( 99.91)
Epoch: [58][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4197e-01 (1.7717e-01)	Acc@1  96.88 ( 95.63)	Acc@5 100.00 ( 99.91)
Epoch: [58][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2817e-01 (1.7777e-01)	Acc@1  96.88 ( 95.58)	Acc@5 100.00 ( 99.90)
Epoch: [58][290/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0642e-01 (1.7813e-01)	Acc@1  92.19 ( 95.54)	Acc@5 100.00 ( 99.90)
Epoch: [58][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6858e-01 (1.7838e-01)	Acc@1  96.09 ( 95.54)	Acc@5 100.00 ( 99.90)
Epoch: [58][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5833e-01 (1.7877e-01)	Acc@1  97.66 ( 95.54)	Acc@5 100.00 ( 99.90)
Epoch: [58][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7334e-01 (1.7858e-01)	Acc@1  94.53 ( 95.57)	Acc@5 100.00 ( 99.90)
Epoch: [58][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.2913e-01 (1.7931e-01)	Acc@1  93.75 ( 95.56)	Acc@5 100.00 ( 99.90)
Epoch: [58][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7932e-01 (1.7972e-01)	Acc@1  93.75 ( 95.55)	Acc@5 100.00 ( 99.89)
Epoch: [58][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1887e-01 (1.7972e-01)	Acc@1  92.97 ( 95.55)	Acc@5 100.00 ( 99.90)
Epoch: [58][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5894e-01 (1.8029e-01)	Acc@1  96.88 ( 95.53)	Acc@5 100.00 ( 99.89)
Epoch: [58][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6418e-01 (1.8035e-01)	Acc@1  96.09 ( 95.52)	Acc@5 100.00 ( 99.90)
Epoch: [58][380/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4087e-01 (1.8048e-01)	Acc@1  98.44 ( 95.52)	Acc@5 100.00 ( 99.90)
Epoch: [58][390/391]	Time  0.113 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6260e-01 (1.8012e-01)	Acc@1  93.75 ( 95.54)	Acc@5 100.00 ( 99.90)
## e[58] optimizer.zero_grad (sum) time: 0.5455949306488037
## e[58]       loss.backward (sum) time: 12.575823545455933
## e[58]      optimizer.step (sum) time: 25.487438917160034
## epoch[58] training(only) time: 63.40335130691528
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 1.1934e+00 (1.1934e+00)	Acc@1  72.00 ( 72.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.3506e+00 (1.2814e+00)	Acc@1  63.00 ( 70.36)	Acc@5  91.00 ( 90.09)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.2246e+00 (1.2479e+00)	Acc@1  69.00 ( 70.48)	Acc@5  93.00 ( 91.05)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.5820e+00 (1.2945e+00)	Acc@1  65.00 ( 69.84)	Acc@5  90.00 ( 90.74)
Test: [ 40/100]	Time  0.056 ( 0.051)	Loss 1.0322e+00 (1.2762e+00)	Acc@1  76.00 ( 70.05)	Acc@5  91.00 ( 90.98)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2588e+00 (1.2760e+00)	Acc@1  72.00 ( 70.08)	Acc@5  91.00 ( 90.90)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2363e+00 (1.2664e+00)	Acc@1  66.00 ( 69.98)	Acc@5  92.00 ( 90.98)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4082e+00 (1.2669e+00)	Acc@1  62.00 ( 69.89)	Acc@5  91.00 ( 91.08)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5801e+00 (1.2714e+00)	Acc@1  66.00 ( 69.68)	Acc@5  87.00 ( 91.07)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2539e+00 (1.2532e+00)	Acc@1  72.00 ( 70.02)	Acc@5  89.00 ( 91.16)
 * Acc@1 70.210 Acc@5 91.250
### epoch[58] execution time: 68.37904167175293
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.316 ( 0.316)	Data  0.148 ( 0.148)	Loss 1.6272e-01 (1.6272e-01)	Acc@1  98.44 ( 98.44)	Acc@5  99.22 ( 99.22)
Epoch: [59][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.2952e-01 (1.4782e-01)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.86)
Epoch: [59][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 8.7341e-02 (1.4771e-01)	Acc@1  98.44 ( 96.54)	Acc@5 100.00 ( 99.93)
Epoch: [59][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.6040e-01 (1.5259e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.92)
Epoch: [59][ 40/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.4233e-01 (1.4735e-01)	Acc@1  97.66 ( 96.63)	Acc@5  99.22 ( 99.92)
Epoch: [59][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.6309e-01 (1.4985e-01)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.92)
Epoch: [59][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3770e-01 (1.5258e-01)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.92)
Epoch: [59][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.2107e-01 (1.5851e-01)	Acc@1  92.19 ( 96.29)	Acc@5  99.22 ( 99.90)
Epoch: [59][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 2.1252e-01 (1.5893e-01)	Acc@1  93.75 ( 96.33)	Acc@5  99.22 ( 99.89)
Epoch: [59][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.5405e-01 (1.5880e-01)	Acc@1  96.88 ( 96.29)	Acc@5 100.00 ( 99.91)
Epoch: [59][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3037e-01 (1.5866e-01)	Acc@1  98.44 ( 96.34)	Acc@5 100.00 ( 99.91)
Epoch: [59][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5857e-01 (1.5881e-01)	Acc@1  97.66 ( 96.35)	Acc@5 100.00 ( 99.90)
Epoch: [59][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4929e-01 (1.5905e-01)	Acc@1  96.09 ( 96.31)	Acc@5 100.00 ( 99.90)
Epoch: [59][130/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5308e-01 (1.5858e-01)	Acc@1  95.31 ( 96.39)	Acc@5 100.00 ( 99.90)
Epoch: [59][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3254e-01 (1.5896e-01)	Acc@1  93.75 ( 96.35)	Acc@5  99.22 ( 99.90)
Epoch: [59][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7126e-01 (1.5998e-01)	Acc@1  96.88 ( 96.31)	Acc@5  99.22 ( 99.89)
Epoch: [59][160/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3550e-01 (1.6057e-01)	Acc@1  97.66 ( 96.29)	Acc@5 100.00 ( 99.90)
Epoch: [59][170/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6809e-01 (1.6149e-01)	Acc@1  95.31 ( 96.27)	Acc@5 100.00 ( 99.90)
Epoch: [59][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4404e-01 (1.6053e-01)	Acc@1  99.22 ( 96.33)	Acc@5 100.00 ( 99.90)
Epoch: [59][190/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9031e-01 (1.6184e-01)	Acc@1  95.31 ( 96.27)	Acc@5  99.22 ( 99.89)
Epoch: [59][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6272e-01 (1.6233e-01)	Acc@1  95.31 ( 96.27)	Acc@5 100.00 ( 99.89)
Epoch: [59][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5210e-01 (1.6268e-01)	Acc@1  96.88 ( 96.26)	Acc@5 100.00 ( 99.89)
Epoch: [59][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5906e-01 (1.6331e-01)	Acc@1  94.53 ( 96.20)	Acc@5 100.00 ( 99.89)
Epoch: [59][230/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8591e-01 (1.6411e-01)	Acc@1  94.53 ( 96.19)	Acc@5  99.22 ( 99.89)
Epoch: [59][240/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.7595e-02 (1.6465e-01)	Acc@1  98.44 ( 96.16)	Acc@5 100.00 ( 99.89)
Epoch: [59][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1301e-01 (1.6546e-01)	Acc@1  94.53 ( 96.12)	Acc@5 100.00 ( 99.89)
Epoch: [59][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7041e-01 (1.6561e-01)	Acc@1  96.09 ( 96.10)	Acc@5 100.00 ( 99.88)
Epoch: [59][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6541e-01 (1.6561e-01)	Acc@1  96.88 ( 96.10)	Acc@5 100.00 ( 99.88)
Epoch: [59][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3892e-01 (1.6562e-01)	Acc@1  97.66 ( 96.09)	Acc@5 100.00 ( 99.89)
Epoch: [59][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6699e-01 (1.6613e-01)	Acc@1  95.31 ( 96.06)	Acc@5 100.00 ( 99.89)
Epoch: [59][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4819e-01 (1.6657e-01)	Acc@1  94.53 ( 96.04)	Acc@5 100.00 ( 99.89)
Epoch: [59][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1802e-01 (1.6667e-01)	Acc@1  94.53 ( 96.04)	Acc@5  99.22 ( 99.89)
Epoch: [59][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5393e-01 (1.6742e-01)	Acc@1  96.09 ( 96.03)	Acc@5 100.00 ( 99.89)
Epoch: [59][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4697e-01 (1.6821e-01)	Acc@1  96.09 ( 95.98)	Acc@5 100.00 ( 99.89)
Epoch: [59][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0044e-01 (1.6864e-01)	Acc@1  95.31 ( 95.96)	Acc@5 100.00 ( 99.88)
Epoch: [59][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5271e-01 (1.6896e-01)	Acc@1  96.88 ( 95.93)	Acc@5 100.00 ( 99.88)
Epoch: [59][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.3035e-01 (1.6844e-01)	Acc@1  91.41 ( 95.96)	Acc@5 100.00 ( 99.88)
Epoch: [59][370/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8262e-01 (1.6841e-01)	Acc@1  96.09 ( 95.97)	Acc@5 100.00 ( 99.88)
Epoch: [59][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1371e-01 (1.6890e-01)	Acc@1  96.09 ( 95.95)	Acc@5 100.00 ( 99.88)
Epoch: [59][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4001e-01 (1.6909e-01)	Acc@1  97.50 ( 95.95)	Acc@5 100.00 ( 99.88)
## e[59] optimizer.zero_grad (sum) time: 0.5398504734039307
## e[59]       loss.backward (sum) time: 12.548088788986206
## e[59]      optimizer.step (sum) time: 25.522294521331787
## epoch[59] training(only) time: 63.57354807853699
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 1.1680e+00 (1.1680e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.3877e+00 (1.2856e+00)	Acc@1  64.00 ( 69.64)	Acc@5  91.00 ( 89.55)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.2188e+00 (1.2699e+00)	Acc@1  70.00 ( 69.81)	Acc@5  92.00 ( 90.48)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4355e+00 (1.2924e+00)	Acc@1  64.00 ( 69.74)	Acc@5  88.00 ( 90.26)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0186e+00 (1.2733e+00)	Acc@1  74.00 ( 69.71)	Acc@5  92.00 ( 90.66)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2578e+00 (1.2774e+00)	Acc@1  72.00 ( 69.88)	Acc@5  91.00 ( 90.59)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2246e+00 (1.2649e+00)	Acc@1  66.00 ( 69.92)	Acc@5  90.00 ( 90.69)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3535e+00 (1.2598e+00)	Acc@1  65.00 ( 69.94)	Acc@5  92.00 ( 90.76)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6445e+00 (1.2605e+00)	Acc@1  72.00 ( 70.01)	Acc@5  85.00 ( 90.83)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3330e+00 (1.2406e+00)	Acc@1  69.00 ( 70.30)	Acc@5  90.00 ( 90.98)
 * Acc@1 70.530 Acc@5 91.120
### epoch[59] execution time: 68.52766489982605
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.320 ( 0.320)	Data  0.143 ( 0.143)	Loss 1.0107e-01 (1.0107e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.5247e-01 (1.3795e-01)	Acc@1  95.31 ( 96.88)	Acc@5  99.22 ( 99.79)
Epoch: [60][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.2573e-01 (1.3976e-01)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.89)
Epoch: [60][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.3196e-01 (1.4335e-01)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.92)
Epoch: [60][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 2.1008e-01 (1.4565e-01)	Acc@1  92.97 ( 96.99)	Acc@5  99.22 ( 99.90)
Epoch: [60][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.7114e-01 (1.4784e-01)	Acc@1  94.53 ( 96.95)	Acc@5 100.00 ( 99.92)
Epoch: [60][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3586e-01 (1.5091e-01)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.92)
Epoch: [60][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.5356e-01 (1.5038e-01)	Acc@1  96.09 ( 96.85)	Acc@5 100.00 ( 99.93)
Epoch: [60][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0730e-01 (1.5015e-01)	Acc@1  98.44 ( 96.74)	Acc@5 100.00 ( 99.93)
Epoch: [60][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1243e-01 (1.4851e-01)	Acc@1  99.22 ( 96.81)	Acc@5 100.00 ( 99.93)
Epoch: [60][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7126e-01 (1.4930e-01)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.93)
Epoch: [60][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4111e-01 (1.4951e-01)	Acc@1  96.88 ( 96.71)	Acc@5 100.00 ( 99.94)
Epoch: [60][120/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5527e-01 (1.4996e-01)	Acc@1  95.31 ( 96.69)	Acc@5  99.22 ( 99.93)
Epoch: [60][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9861e-01 (1.4996e-01)	Acc@1  94.53 ( 96.73)	Acc@5 100.00 ( 99.92)
Epoch: [60][140/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7285e-01 (1.4955e-01)	Acc@1  95.31 ( 96.75)	Acc@5 100.00 ( 99.92)
Epoch: [60][150/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4548e-01 (1.5136e-01)	Acc@1  91.41 ( 96.67)	Acc@5 100.00 ( 99.93)
Epoch: [60][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2219e-01 (1.5115e-01)	Acc@1  97.66 ( 96.70)	Acc@5 100.00 ( 99.93)
Epoch: [60][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4636e-01 (1.5086e-01)	Acc@1  96.88 ( 96.69)	Acc@5 100.00 ( 99.92)
Epoch: [60][180/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4441e-01 (1.5144e-01)	Acc@1  96.88 ( 96.65)	Acc@5 100.00 ( 99.92)
Epoch: [60][190/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3416e-01 (1.5137e-01)	Acc@1  96.88 ( 96.63)	Acc@5 100.00 ( 99.92)
Epoch: [60][200/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6138e-01 (1.5215e-01)	Acc@1  92.97 ( 96.57)	Acc@5 100.00 ( 99.92)
Epoch: [60][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.4402e-01 (1.5285e-01)	Acc@1  93.75 ( 96.53)	Acc@5  98.44 ( 99.92)
Epoch: [60][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2201e-01 (1.5299e-01)	Acc@1  98.44 ( 96.51)	Acc@5 100.00 ( 99.92)
Epoch: [60][230/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6821e-01 (1.5299e-01)	Acc@1  96.88 ( 96.54)	Acc@5 100.00 ( 99.92)
Epoch: [60][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8311e-01 (1.5194e-01)	Acc@1  96.09 ( 96.57)	Acc@5 100.00 ( 99.92)
Epoch: [60][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3147e-01 (1.5182e-01)	Acc@1  99.22 ( 96.59)	Acc@5 100.00 ( 99.92)
Epoch: [60][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6968e-01 (1.5214e-01)	Acc@1  95.31 ( 96.58)	Acc@5 100.00 ( 99.92)
Epoch: [60][270/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6882e-01 (1.5178e-01)	Acc@1  98.44 ( 96.58)	Acc@5 100.00 ( 99.92)
Epoch: [60][280/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4783e-01 (1.5153e-01)	Acc@1  95.31 ( 96.59)	Acc@5 100.00 ( 99.92)
Epoch: [60][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5234e-01 (1.5125e-01)	Acc@1  96.88 ( 96.60)	Acc@5 100.00 ( 99.92)
Epoch: [60][300/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8323e-01 (1.5179e-01)	Acc@1  96.88 ( 96.57)	Acc@5  99.22 ( 99.92)
Epoch: [60][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1249e-01 (1.5161e-01)	Acc@1  96.88 ( 96.57)	Acc@5 100.00 ( 99.92)
Epoch: [60][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.5947e-02 (1.5183e-01)	Acc@1 100.00 ( 96.57)	Acc@5 100.00 ( 99.92)
Epoch: [60][330/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3892e-01 (1.5190e-01)	Acc@1  96.88 ( 96.57)	Acc@5 100.00 ( 99.91)
Epoch: [60][340/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3550e-01 (1.5161e-01)	Acc@1  97.66 ( 96.58)	Acc@5 100.00 ( 99.91)
Epoch: [60][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4453e-01 (1.5170e-01)	Acc@1  96.88 ( 96.57)	Acc@5 100.00 ( 99.92)
Epoch: [60][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4453e-01 (1.5151e-01)	Acc@1  96.09 ( 96.58)	Acc@5 100.00 ( 99.92)
Epoch: [60][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9263e-01 (1.5159e-01)	Acc@1  93.75 ( 96.58)	Acc@5 100.00 ( 99.92)
Epoch: [60][380/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1558e-01 (1.5175e-01)	Acc@1  95.31 ( 96.58)	Acc@5  99.22 ( 99.91)
Epoch: [60][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5442e-01 (1.5189e-01)	Acc@1  97.50 ( 96.58)	Acc@5 100.00 ( 99.91)
## e[60] optimizer.zero_grad (sum) time: 0.5404930114746094
## e[60]       loss.backward (sum) time: 12.57267141342163
## e[60]      optimizer.step (sum) time: 25.472317695617676
## epoch[60] training(only) time: 63.33346509933472
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.1953e+00 (1.1953e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.4277e+00 (1.2905e+00)	Acc@1  65.00 ( 69.73)	Acc@5  90.00 ( 90.00)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.2773e+00 (1.2683e+00)	Acc@1  70.00 ( 70.05)	Acc@5  92.00 ( 91.10)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4346e+00 (1.2927e+00)	Acc@1  64.00 ( 69.87)	Acc@5  87.00 ( 90.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0381e+00 (1.2747e+00)	Acc@1  74.00 ( 69.80)	Acc@5  91.00 ( 91.05)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2480e+00 (1.2761e+00)	Acc@1  73.00 ( 69.94)	Acc@5  92.00 ( 91.00)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.2207e+00 (1.2648e+00)	Acc@1  67.00 ( 69.93)	Acc@5  90.00 ( 91.08)
Test: [ 70/100]	Time  0.057 ( 0.049)	Loss 1.3438e+00 (1.2602e+00)	Acc@1  66.00 ( 70.04)	Acc@5  92.00 ( 91.14)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6416e+00 (1.2609e+00)	Acc@1  70.00 ( 70.09)	Acc@5  85.00 ( 91.12)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3320e+00 (1.2407e+00)	Acc@1  67.00 ( 70.38)	Acc@5  90.00 ( 91.20)
 * Acc@1 70.600 Acc@5 91.280
### epoch[60] execution time: 68.30027675628662
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.328 ( 0.328)	Data  0.159 ( 0.159)	Loss 1.0938e-01 (1.0938e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.7639e-01 (1.4382e-01)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 ( 99.93)
Epoch: [61][ 20/391]	Time  0.164 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.3928e-01 (1.4790e-01)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.89)
Epoch: [61][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.8140e-01 (1.5104e-01)	Acc@1  92.97 ( 96.52)	Acc@5 100.00 ( 99.90)
Epoch: [61][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.0535e-01 (1.4949e-01)	Acc@1  99.22 ( 96.63)	Acc@5 100.00 ( 99.90)
Epoch: [61][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.1963e-01 (1.4697e-01)	Acc@1  97.66 ( 96.81)	Acc@5 100.00 ( 99.91)
Epoch: [61][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.9312e-01 (1.4785e-01)	Acc@1  95.31 ( 96.76)	Acc@5 100.00 ( 99.92)
Epoch: [61][ 70/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3159e-01 (1.4517e-01)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.93)
Epoch: [61][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3220e-01 (1.4536e-01)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.93)
Epoch: [61][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6162e-01 (1.4330e-01)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.94)
Epoch: [61][100/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.8750e-01 (1.4298e-01)	Acc@1  93.75 ( 96.94)	Acc@5 100.00 ( 99.94)
Epoch: [61][110/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7566e-01 (1.4381e-01)	Acc@1  97.66 ( 96.87)	Acc@5 100.00 ( 99.94)
Epoch: [61][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5662e-01 (1.4484e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.94)
Epoch: [61][130/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3196e-01 (1.4432e-01)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.94)
Epoch: [61][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9365e-02 (1.4469e-01)	Acc@1  99.22 ( 96.88)	Acc@5 100.00 ( 99.94)
Epoch: [61][150/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8933e-01 (1.4559e-01)	Acc@1  95.31 ( 96.85)	Acc@5 100.00 ( 99.93)
Epoch: [61][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2009e-01 (1.4655e-01)	Acc@1  92.19 ( 96.78)	Acc@5 100.00 ( 99.93)
Epoch: [61][170/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0443e-01 (1.4644e-01)	Acc@1  98.44 ( 96.76)	Acc@5 100.00 ( 99.93)
Epoch: [61][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3464e-01 (1.4670e-01)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.93)
Epoch: [61][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6675e-01 (1.4703e-01)	Acc@1  95.31 ( 96.71)	Acc@5 100.00 ( 99.93)
Epoch: [61][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1615e-01 (1.4721e-01)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.93)
Epoch: [61][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4148e-01 (1.4713e-01)	Acc@1  95.31 ( 96.73)	Acc@5 100.00 ( 99.93)
Epoch: [61][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5552e-01 (1.4601e-01)	Acc@1  96.88 ( 96.79)	Acc@5  99.22 ( 99.93)
Epoch: [61][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3245e-01 (1.4646e-01)	Acc@1  96.88 ( 96.75)	Acc@5 100.00 ( 99.93)
Epoch: [61][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0669e-01 (1.4632e-01)	Acc@1  97.66 ( 96.74)	Acc@5 100.00 ( 99.94)
Epoch: [61][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0913e-01 (1.4579e-01)	Acc@1  98.44 ( 96.77)	Acc@5 100.00 ( 99.94)
Epoch: [61][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2622e-01 (1.4535e-01)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.94)
Epoch: [61][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3416e-01 (1.4529e-01)	Acc@1  96.88 ( 96.78)	Acc@5 100.00 ( 99.94)
Epoch: [61][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5552e-01 (1.4526e-01)	Acc@1  96.09 ( 96.77)	Acc@5 100.00 ( 99.94)
Epoch: [61][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1981e-01 (1.4452e-01)	Acc@1  97.66 ( 96.79)	Acc@5 100.00 ( 99.94)
Epoch: [61][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2695e-01 (1.4436e-01)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.95)
Epoch: [61][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4978e-01 (1.4468e-01)	Acc@1  98.44 ( 96.80)	Acc@5 100.00 ( 99.94)
Epoch: [61][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.6313e-02 (1.4446e-01)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.94)
Epoch: [61][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5552e-01 (1.4461e-01)	Acc@1  94.53 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [61][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0142e-01 (1.4459e-01)	Acc@1  95.31 ( 96.82)	Acc@5 100.00 ( 99.95)
Epoch: [61][350/391]	Time  0.178 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1021e-01 (1.4510e-01)	Acc@1  92.19 ( 96.79)	Acc@5  99.22 ( 99.94)
Epoch: [61][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0923e-01 (1.4549e-01)	Acc@1  93.75 ( 96.78)	Acc@5 100.00 ( 99.94)
Epoch: [61][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4062e-01 (1.4506e-01)	Acc@1  96.88 ( 96.80)	Acc@5 100.00 ( 99.94)
Epoch: [61][380/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2115e-01 (1.4478e-01)	Acc@1  98.44 ( 96.81)	Acc@5 100.00 ( 99.94)
Epoch: [61][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1667e-01 (1.4510e-01)	Acc@1  92.50 ( 96.78)	Acc@5 100.00 ( 99.94)
## e[61] optimizer.zero_grad (sum) time: 0.5333759784698486
## e[61]       loss.backward (sum) time: 12.55776572227478
## e[61]      optimizer.step (sum) time: 25.52030110359192
## epoch[61] training(only) time: 63.31206274032593
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1865e+00 (1.1865e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.3809e+00 (1.2846e+00)	Acc@1  67.00 ( 70.91)	Acc@5  90.00 ( 90.00)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.2539e+00 (1.2662e+00)	Acc@1  70.00 ( 71.00)	Acc@5  92.00 ( 91.00)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4268e+00 (1.2873e+00)	Acc@1  64.00 ( 70.35)	Acc@5  88.00 ( 90.71)
Test: [ 40/100]	Time  0.047 ( 0.050)	Loss 1.0352e+00 (1.2670e+00)	Acc@1  76.00 ( 70.44)	Acc@5  92.00 ( 91.05)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2207e+00 (1.2673e+00)	Acc@1  73.00 ( 70.53)	Acc@5  92.00 ( 90.86)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.2393e+00 (1.2578e+00)	Acc@1  67.00 ( 70.39)	Acc@5  91.00 ( 91.02)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 1.3789e+00 (1.2526e+00)	Acc@1  64.00 ( 70.46)	Acc@5  92.00 ( 91.07)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6338e+00 (1.2540e+00)	Acc@1  72.00 ( 70.46)	Acc@5  85.00 ( 90.99)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3301e+00 (1.2335e+00)	Acc@1  69.00 ( 70.78)	Acc@5  89.00 ( 91.13)
 * Acc@1 70.960 Acc@5 91.210
### epoch[61] execution time: 68.27094841003418
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.321 ( 0.321)	Data  0.147 ( 0.147)	Loss 9.7107e-02 (9.7107e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.162 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.5271e-01 (1.3837e-01)	Acc@1  95.31 ( 96.66)	Acc@5 100.00 ( 99.93)
Epoch: [62][ 20/391]	Time  0.159 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.2756e-01 (1.3982e-01)	Acc@1  96.88 ( 96.73)	Acc@5 100.00 ( 99.96)
Epoch: [62][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.8005e-01 (1.4228e-01)	Acc@1  94.53 ( 96.82)	Acc@5 100.00 ( 99.95)
Epoch: [62][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.3733e-01 (1.3955e-01)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.96)
Epoch: [62][ 50/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.7371e-01 (1.3930e-01)	Acc@1  95.31 ( 96.86)	Acc@5 100.00 ( 99.97)
Epoch: [62][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6272e-01 (1.3945e-01)	Acc@1  95.31 ( 96.87)	Acc@5 100.00 ( 99.97)
Epoch: [62][ 70/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6553e-01 (1.3764e-01)	Acc@1  95.31 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [62][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3794e-01 (1.3887e-01)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.98)
Epoch: [62][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1658e-01 (1.3868e-01)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [62][100/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3550e-01 (1.3854e-01)	Acc@1  96.09 ( 97.05)	Acc@5 100.00 ( 99.98)
Epoch: [62][110/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3464e-01 (1.4023e-01)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 ( 99.99)
Epoch: [62][120/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4380e-01 (1.4056e-01)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.98)
Epoch: [62][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6736e-01 (1.4230e-01)	Acc@1  96.09 ( 96.98)	Acc@5 100.00 ( 99.98)
Epoch: [62][140/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1206e-01 (1.4226e-01)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [62][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2256e-01 (1.4124e-01)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.97)
Epoch: [62][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5137e-01 (1.4121e-01)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.98)
Epoch: [62][170/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0272e-01 (1.4163e-01)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.98)
Epoch: [62][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8127e-01 (1.4195e-01)	Acc@1  95.31 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [62][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9580e-01 (1.4217e-01)	Acc@1  96.88 ( 96.96)	Acc@5 100.00 ( 99.98)
Epoch: [62][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8286e-01 (1.4174e-01)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.98)
Epoch: [62][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5002e-01 (1.4161e-01)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 ( 99.97)
Epoch: [62][220/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1802e-01 (1.4198e-01)	Acc@1  95.31 ( 96.98)	Acc@5  99.22 ( 99.97)
Epoch: [62][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7310e-01 (1.4225e-01)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.97)
Epoch: [62][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.0081e-01 (1.4253e-01)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.96)
Epoch: [62][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0931e-01 (1.4322e-01)	Acc@1  98.44 ( 96.91)	Acc@5 100.00 ( 99.97)
Epoch: [62][260/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.7534e-02 (1.4256e-01)	Acc@1 100.00 ( 96.96)	Acc@5 100.00 ( 99.96)
Epoch: [62][270/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3684e-01 (1.4237e-01)	Acc@1  94.53 ( 96.94)	Acc@5 100.00 ( 99.96)
Epoch: [62][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3232e-01 (1.4264e-01)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.96)
Epoch: [62][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.0261e-02 (1.4345e-01)	Acc@1  99.22 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [62][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5564e-01 (1.4342e-01)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [62][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4417e-01 (1.4354e-01)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.96)
Epoch: [62][320/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2854e-01 (1.4293e-01)	Acc@1  94.53 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [62][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.0056e-01 (1.4281e-01)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.96)
Epoch: [62][340/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0651e-01 (1.4288e-01)	Acc@1  96.88 ( 96.90)	Acc@5 100.00 ( 99.96)
Epoch: [62][350/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1371e-01 (1.4244e-01)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.96)
Epoch: [62][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5051e-01 (1.4221e-01)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.96)
Epoch: [62][370/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1340e-01 (1.4209e-01)	Acc@1  96.88 ( 96.92)	Acc@5 100.00 ( 99.96)
Epoch: [62][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7078e-01 (1.4225e-01)	Acc@1  95.31 ( 96.90)	Acc@5 100.00 ( 99.95)
Epoch: [62][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4795e-01 (1.4224e-01)	Acc@1  97.50 ( 96.91)	Acc@5 100.00 ( 99.95)
## e[62] optimizer.zero_grad (sum) time: 0.5381784439086914
## e[62]       loss.backward (sum) time: 12.59081482887268
## e[62]      optimizer.step (sum) time: 25.490566968917847
## epoch[62] training(only) time: 63.27484655380249
# Switched to evaluate mode...
Test: [  0/100]	Time  0.180 ( 0.180)	Loss 1.1943e+00 (1.1943e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.3809e+00 (1.2703e+00)	Acc@1  64.00 ( 70.27)	Acc@5  90.00 ( 90.00)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.2393e+00 (1.2496e+00)	Acc@1  70.00 ( 70.57)	Acc@5  92.00 ( 90.86)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4131e+00 (1.2754e+00)	Acc@1  66.00 ( 70.16)	Acc@5  87.00 ( 90.52)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0186e+00 (1.2556e+00)	Acc@1  76.00 ( 70.37)	Acc@5  91.00 ( 90.88)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.2217e+00 (1.2568e+00)	Acc@1  73.00 ( 70.49)	Acc@5  92.00 ( 90.78)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1816e+00 (1.2469e+00)	Acc@1  67.00 ( 70.52)	Acc@5  91.00 ( 90.90)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.3984e+00 (1.2449e+00)	Acc@1  64.00 ( 70.56)	Acc@5  92.00 ( 90.99)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6133e+00 (1.2465e+00)	Acc@1  71.00 ( 70.58)	Acc@5  86.00 ( 90.99)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.2969e+00 (1.2266e+00)	Acc@1  68.00 ( 70.89)	Acc@5  90.00 ( 91.10)
 * Acc@1 71.120 Acc@5 91.210
### epoch[62] execution time: 68.24518585205078
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.320 ( 0.320)	Data  0.140 ( 0.140)	Loss 1.2756e-01 (1.2756e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.161 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.4661e-01 (1.4986e-01)	Acc@1  97.66 ( 96.16)	Acc@5 100.00 ( 99.93)
Epoch: [63][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.3477e-01 (1.4109e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 ( 99.96)
Epoch: [63][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.0522e-01 (1.3792e-01)	Acc@1  98.44 ( 96.88)	Acc@5 100.00 ( 99.97)
Epoch: [63][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.0815e-01 (1.4099e-01)	Acc@1  96.88 ( 96.74)	Acc@5 100.00 ( 99.96)
Epoch: [63][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3953e-01 (1.4220e-01)	Acc@1  96.88 ( 96.68)	Acc@5 100.00 ( 99.97)
Epoch: [63][ 60/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3831e-01 (1.4141e-01)	Acc@1  96.88 ( 96.80)	Acc@5  99.22 ( 99.96)
Epoch: [63][ 70/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2512e-01 (1.4051e-01)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.97)
Epoch: [63][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4307e-01 (1.3897e-01)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.96)
Epoch: [63][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4294e-01 (1.3887e-01)	Acc@1  95.31 ( 96.94)	Acc@5 100.00 ( 99.96)
Epoch: [63][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3269e-01 (1.3938e-01)	Acc@1  96.88 ( 96.91)	Acc@5 100.00 ( 99.95)
Epoch: [63][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3110e-01 (1.4068e-01)	Acc@1  98.44 ( 96.90)	Acc@5 100.00 ( 99.95)
Epoch: [63][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7725e-01 (1.4050e-01)	Acc@1  96.88 ( 96.91)	Acc@5  99.22 ( 99.95)
Epoch: [63][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2561e-01 (1.4052e-01)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.95)
Epoch: [63][140/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0181e-01 (1.4127e-01)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.95)
Epoch: [63][150/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5015e-01 (1.4141e-01)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.95)
Epoch: [63][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2207e-01 (1.4090e-01)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.95)
Epoch: [63][170/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6113e-01 (1.4067e-01)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [63][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4270e-01 (1.4108e-01)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [63][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0797e-01 (1.4056e-01)	Acc@1  97.66 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [63][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1749e-01 (1.4050e-01)	Acc@1  99.22 ( 96.92)	Acc@5 100.00 ( 99.95)
Epoch: [63][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4978e-01 (1.4049e-01)	Acc@1  97.66 ( 96.92)	Acc@5 100.00 ( 99.95)
Epoch: [63][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7383e-01 (1.4050e-01)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.94)
Epoch: [63][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3452e-01 (1.4070e-01)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.95)
Epoch: [63][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4917e-01 (1.4045e-01)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [63][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8420e-01 (1.4020e-01)	Acc@1  92.19 ( 96.92)	Acc@5 100.00 ( 99.95)
Epoch: [63][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6748e-01 (1.3985e-01)	Acc@1  96.88 ( 96.93)	Acc@5 100.00 ( 99.95)
Epoch: [63][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3135e-01 (1.3975e-01)	Acc@1  98.44 ( 96.94)	Acc@5 100.00 ( 99.95)
Epoch: [63][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9470e-01 (1.4012e-01)	Acc@1  95.31 ( 96.93)	Acc@5  99.22 ( 99.95)
Epoch: [63][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1072e-01 (1.3983e-01)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.95)
Epoch: [63][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3538e-01 (1.3969e-01)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.94)
Epoch: [63][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4905e-01 (1.3931e-01)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.94)
Epoch: [63][320/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.6548e-02 (1.3896e-01)	Acc@1 100.00 ( 96.99)	Acc@5 100.00 ( 99.94)
Epoch: [63][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0284e-01 (1.3894e-01)	Acc@1  98.44 ( 97.00)	Acc@5 100.00 ( 99.95)
Epoch: [63][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 7.6538e-02 (1.3884e-01)	Acc@1 100.00 ( 97.01)	Acc@5 100.00 ( 99.94)
Epoch: [63][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0449e-01 (1.3882e-01)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.94)
Epoch: [63][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9055e-01 (1.3898e-01)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.94)
Epoch: [63][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4624e-01 (1.3908e-01)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.94)
Epoch: [63][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6895e-01 (1.3916e-01)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.94)
Epoch: [63][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3904e-01 (1.3922e-01)	Acc@1  96.25 ( 96.99)	Acc@5 100.00 ( 99.94)
## e[63] optimizer.zero_grad (sum) time: 0.5336697101593018
## e[63]       loss.backward (sum) time: 12.535525798797607
## e[63]      optimizer.step (sum) time: 25.505992889404297
## epoch[63] training(only) time: 63.305291175842285
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.1787e+00 (1.1787e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.060 ( 0.061)	Loss 1.3428e+00 (1.2663e+00)	Acc@1  66.00 ( 70.09)	Acc@5  90.00 ( 90.18)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.2441e+00 (1.2448e+00)	Acc@1  70.00 ( 70.57)	Acc@5  92.00 ( 91.05)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4131e+00 (1.2703e+00)	Acc@1  65.00 ( 70.32)	Acc@5  87.00 ( 90.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.0215e+00 (1.2522e+00)	Acc@1  75.00 ( 70.46)	Acc@5  92.00 ( 91.10)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2188e+00 (1.2535e+00)	Acc@1  74.00 ( 70.63)	Acc@5  91.00 ( 90.98)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1895e+00 (1.2427e+00)	Acc@1  66.00 ( 70.59)	Acc@5  89.00 ( 90.95)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3711e+00 (1.2397e+00)	Acc@1  64.00 ( 70.56)	Acc@5  93.00 ( 91.04)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6055e+00 (1.2399e+00)	Acc@1  70.00 ( 70.52)	Acc@5  86.00 ( 91.00)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3018e+00 (1.2199e+00)	Acc@1  68.00 ( 70.80)	Acc@5  90.00 ( 91.10)
 * Acc@1 70.990 Acc@5 91.200
### epoch[63] execution time: 68.24997401237488
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.325 ( 0.325)	Data  0.142 ( 0.142)	Loss 2.1313e-01 (2.1313e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.3867e-01 (1.5245e-01)	Acc@1  96.09 ( 96.38)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.4148e-01 (1.4309e-01)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.7297e-01 (1.3948e-01)	Acc@1  97.66 ( 97.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.2622e-01 (1.3914e-01)	Acc@1  96.88 ( 96.99)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.0626e-01 (1.3933e-01)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.9849e-01 (1.3980e-01)	Acc@1  94.53 ( 97.11)	Acc@5 100.00 ( 99.99)
Epoch: [64][ 70/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0248e-01 (1.3961e-01)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.99)
Epoch: [64][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2024e-01 (1.4005e-01)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.97)
Epoch: [64][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6003e-01 (1.3957e-01)	Acc@1  94.53 ( 97.12)	Acc@5  99.22 ( 99.97)
Epoch: [64][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1472e-01 (1.4052e-01)	Acc@1  93.75 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [64][110/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2573e-01 (1.4080e-01)	Acc@1  97.66 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [64][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7200e-01 (1.4020e-01)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 ( 99.97)
Epoch: [64][130/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0229e-01 (1.3986e-01)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.98)
Epoch: [64][140/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2128e-01 (1.4054e-01)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.96)
Epoch: [64][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9153e-01 (1.4079e-01)	Acc@1  92.97 ( 97.04)	Acc@5 100.00 ( 99.96)
Epoch: [64][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0730e-01 (1.4037e-01)	Acc@1  99.22 ( 97.05)	Acc@5 100.00 ( 99.96)
Epoch: [64][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7786e-01 (1.4053e-01)	Acc@1  96.09 ( 97.07)	Acc@5  99.22 ( 99.96)
Epoch: [64][180/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4551e-01 (1.4104e-01)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.96)
Epoch: [64][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0718e-01 (1.4093e-01)	Acc@1  97.66 ( 97.10)	Acc@5 100.00 ( 99.96)
Epoch: [64][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3416e-01 (1.4108e-01)	Acc@1  97.66 ( 97.09)	Acc@5 100.00 ( 99.96)
Epoch: [64][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2646e-01 (1.4065e-01)	Acc@1  96.88 ( 97.11)	Acc@5 100.00 ( 99.96)
Epoch: [64][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1523e-01 (1.4078e-01)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.96)
Epoch: [64][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0718e-01 (1.4086e-01)	Acc@1  98.44 ( 97.05)	Acc@5 100.00 ( 99.96)
Epoch: [64][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2146e-01 (1.4106e-01)	Acc@1  96.88 ( 97.03)	Acc@5 100.00 ( 99.95)
Epoch: [64][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1359e-01 (1.4128e-01)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.95)
Epoch: [64][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5845e-01 (1.4146e-01)	Acc@1  96.09 ( 97.01)	Acc@5 100.00 ( 99.95)
Epoch: [64][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9482e-01 (1.4138e-01)	Acc@1  92.97 ( 97.00)	Acc@5 100.00 ( 99.95)
Epoch: [64][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4246e-01 (1.4146e-01)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.95)
Epoch: [64][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3135e-01 (1.4126e-01)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.95)
Epoch: [64][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4490e-01 (1.4176e-01)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.95)
Epoch: [64][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0297e-01 (1.4163e-01)	Acc@1  98.44 ( 96.97)	Acc@5 100.00 ( 99.95)
Epoch: [64][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2634e-01 (1.4181e-01)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.95)
Epoch: [64][330/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5515e-01 (1.4173e-01)	Acc@1  97.66 ( 96.99)	Acc@5 100.00 ( 99.95)
Epoch: [64][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2659e-01 (1.4144e-01)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.95)
Epoch: [64][350/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0004e-01 (1.4083e-01)	Acc@1  98.44 ( 97.01)	Acc@5 100.00 ( 99.95)
Epoch: [64][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5332e-01 (1.4053e-01)	Acc@1  96.88 ( 97.02)	Acc@5 100.00 ( 99.95)
Epoch: [64][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6345e-01 (1.4055e-01)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.96)
Epoch: [64][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6675e-01 (1.4069e-01)	Acc@1  96.88 ( 97.01)	Acc@5 100.00 ( 99.95)
Epoch: [64][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6687e-01 (1.4086e-01)	Acc@1  95.00 ( 97.00)	Acc@5 100.00 ( 99.96)
## e[64] optimizer.zero_grad (sum) time: 0.5387630462646484
## e[64]       loss.backward (sum) time: 12.624043464660645
## e[64]      optimizer.step (sum) time: 25.44533348083496
## epoch[64] training(only) time: 63.28659415245056
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.1924e+00 (1.1924e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.3682e+00 (1.2610e+00)	Acc@1  66.00 ( 70.36)	Acc@5  90.00 ( 90.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.2207e+00 (1.2374e+00)	Acc@1  71.00 ( 70.90)	Acc@5  91.00 ( 91.19)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4395e+00 (1.2703e+00)	Acc@1  65.00 ( 70.48)	Acc@5  88.00 ( 90.77)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0283e+00 (1.2528e+00)	Acc@1  75.00 ( 70.51)	Acc@5  92.00 ( 91.07)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2285e+00 (1.2551e+00)	Acc@1  74.00 ( 70.65)	Acc@5  92.00 ( 90.98)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.1982e+00 (1.2455e+00)	Acc@1  66.00 ( 70.57)	Acc@5  90.00 ( 91.08)
Test: [ 70/100]	Time  0.048 ( 0.049)	Loss 1.3818e+00 (1.2432e+00)	Acc@1  64.00 ( 70.59)	Acc@5  93.00 ( 91.20)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6055e+00 (1.2445e+00)	Acc@1  71.00 ( 70.57)	Acc@5  86.00 ( 91.16)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3242e+00 (1.2247e+00)	Acc@1  69.00 ( 70.82)	Acc@5  90.00 ( 91.26)
 * Acc@1 71.030 Acc@5 91.330
### epoch[64] execution time: 68.23131322860718
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.310 ( 0.310)	Data  0.140 ( 0.140)	Loss 1.6785e-01 (1.6785e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.2073e-01 (1.4051e-01)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 ( 99.79)
Epoch: [65][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.5283e-01 (1.4757e-01)	Acc@1  94.53 ( 96.65)	Acc@5 100.00 ( 99.85)
Epoch: [65][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.0199e-01 (1.4354e-01)	Acc@1  99.22 ( 96.77)	Acc@5 100.00 ( 99.90)
Epoch: [65][ 40/391]	Time  0.168 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.2976e-01 (1.4315e-01)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.90)
Epoch: [65][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.1517e-01 (1.3724e-01)	Acc@1  99.22 ( 97.06)	Acc@5 100.00 ( 99.92)
Epoch: [65][ 60/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4526e-01 (1.3794e-01)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.94)
Epoch: [65][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.5198e-01 (1.3986e-01)	Acc@1  97.66 ( 96.99)	Acc@5  99.22 ( 99.93)
Epoch: [65][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4331e-01 (1.3883e-01)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.94)
Epoch: [65][ 90/391]	Time  0.175 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0138e-01 (1.3752e-01)	Acc@1  98.44 ( 97.03)	Acc@5 100.00 ( 99.95)
Epoch: [65][100/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.7830e-02 (1.3789e-01)	Acc@1  99.22 ( 97.04)	Acc@5 100.00 ( 99.95)
Epoch: [65][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4502e-01 (1.3836e-01)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.95)
Epoch: [65][120/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5759e-01 (1.3855e-01)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.95)
Epoch: [65][130/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0632e-01 (1.3780e-01)	Acc@1  99.22 ( 97.09)	Acc@5 100.00 ( 99.95)
Epoch: [65][140/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7310e-01 (1.3838e-01)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [65][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4087e-01 (1.3821e-01)	Acc@1  95.31 ( 97.09)	Acc@5 100.00 ( 99.95)
Epoch: [65][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.3977e-01)	Acc@1  96.88 ( 97.00)	Acc@5 100.00 ( 99.96)
Epoch: [65][170/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4648e-01 (1.4122e-01)	Acc@1  96.09 ( 96.97)	Acc@5 100.00 ( 99.95)
Epoch: [65][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2323e-01 (1.4187e-01)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.95)
Epoch: [65][190/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6382e-01 (1.4262e-01)	Acc@1  96.09 ( 96.93)	Acc@5 100.00 ( 99.94)
Epoch: [65][200/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0693e-01 (1.4281e-01)	Acc@1  99.22 ( 96.94)	Acc@5 100.00 ( 99.94)
Epoch: [65][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2756e-01 (1.4273e-01)	Acc@1  96.88 ( 96.94)	Acc@5 100.00 ( 99.93)
Epoch: [65][220/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2085e-01 (1.4261e-01)	Acc@1  98.44 ( 96.93)	Acc@5 100.00 ( 99.94)
Epoch: [65][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2561e-01 (1.4240e-01)	Acc@1  96.09 ( 96.95)	Acc@5 100.00 ( 99.94)
Epoch: [65][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8323e-01 (1.4269e-01)	Acc@1  96.88 ( 96.95)	Acc@5 100.00 ( 99.94)
Epoch: [65][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1310e-01 (1.4237e-01)	Acc@1  99.22 ( 96.98)	Acc@5 100.00 ( 99.94)
Epoch: [65][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2134e-01 (1.4218e-01)	Acc@1  97.66 ( 96.96)	Acc@5 100.00 ( 99.94)
Epoch: [65][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9670e-02 (1.4210e-01)	Acc@1  99.22 ( 96.97)	Acc@5 100.00 ( 99.95)
Epoch: [65][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2561e-01 (1.4185e-01)	Acc@1  98.44 ( 96.99)	Acc@5 100.00 ( 99.94)
Epoch: [65][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5588e-01 (1.4152e-01)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.95)
Epoch: [65][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4258e-01 (1.4209e-01)	Acc@1  97.66 ( 96.97)	Acc@5 100.00 ( 99.94)
Epoch: [65][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8762e-01 (1.4257e-01)	Acc@1  92.97 ( 96.96)	Acc@5 100.00 ( 99.94)
Epoch: [65][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5210e-01 (1.4227e-01)	Acc@1  98.44 ( 96.98)	Acc@5 100.00 ( 99.94)
Epoch: [65][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3098e-01 (1.4215e-01)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.95)
Epoch: [65][340/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5833e-01 (1.4142e-01)	Acc@1  96.09 ( 97.02)	Acc@5 100.00 ( 99.95)
Epoch: [65][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0425e-01 (1.4131e-01)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.94)
Epoch: [65][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3635e-01 (1.4118e-01)	Acc@1  96.09 ( 97.03)	Acc@5 100.00 ( 99.95)
Epoch: [65][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0181e-01 (1.4078e-01)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.95)
Epoch: [65][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1237e-01 (1.4076e-01)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.95)
Epoch: [65][390/391]	Time  0.124 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5222e-01 (1.4025e-01)	Acc@1  96.25 ( 97.06)	Acc@5 100.00 ( 99.95)
## e[65] optimizer.zero_grad (sum) time: 0.5419363975524902
## e[65]       loss.backward (sum) time: 12.641249895095825
## e[65]      optimizer.step (sum) time: 25.446417331695557
## epoch[65] training(only) time: 63.338982820510864
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1924e+00 (1.1924e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.3438e+00 (1.2676e+00)	Acc@1  65.00 ( 70.09)	Acc@5  91.00 ( 90.00)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.2422e+00 (1.2420e+00)	Acc@1  72.00 ( 70.76)	Acc@5  92.00 ( 91.05)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.4316e+00 (1.2702e+00)	Acc@1  64.00 ( 70.39)	Acc@5  88.00 ( 90.65)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0322e+00 (1.2520e+00)	Acc@1  74.00 ( 70.54)	Acc@5  91.00 ( 90.98)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2266e+00 (1.2542e+00)	Acc@1  73.00 ( 70.51)	Acc@5  93.00 ( 90.92)
Test: [ 60/100]	Time  0.056 ( 0.049)	Loss 1.1934e+00 (1.2446e+00)	Acc@1  67.00 ( 70.41)	Acc@5  91.00 ( 91.05)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.3955e+00 (1.2412e+00)	Acc@1  63.00 ( 70.52)	Acc@5  92.00 ( 91.14)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6279e+00 (1.2422e+00)	Acc@1  69.00 ( 70.49)	Acc@5  86.00 ( 91.09)
Test: [ 90/100]	Time  0.046 ( 0.048)	Loss 1.2979e+00 (1.2222e+00)	Acc@1  69.00 ( 70.82)	Acc@5  90.00 ( 91.19)
 * Acc@1 71.010 Acc@5 91.270
### epoch[65] execution time: 68.24902153015137
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.331 ( 0.331)	Data  0.146 ( 0.146)	Loss 1.3318e-01 (1.3318e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.162 ( 0.177)	Data  0.001 ( 0.014)	Loss 1.2006e-01 (1.3913e-01)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 ( 99.86)
Epoch: [66][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 2.0544e-01 (1.4241e-01)	Acc@1  92.19 ( 96.91)	Acc@5 100.00 ( 99.89)
Epoch: [66][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.2354e-01 (1.4374e-01)	Acc@1  97.66 ( 96.82)	Acc@5 100.00 ( 99.92)
Epoch: [66][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.1957e-01 (1.4302e-01)	Acc@1  97.66 ( 96.76)	Acc@5 100.00 ( 99.92)
Epoch: [66][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.1957e-01 (1.4071e-01)	Acc@1  97.66 ( 96.86)	Acc@5 100.00 ( 99.89)
Epoch: [66][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1566e-01 (1.3970e-01)	Acc@1  97.66 ( 96.98)	Acc@5 100.00 ( 99.91)
Epoch: [66][ 70/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2170e-01 (1.3843e-01)	Acc@1  99.22 ( 97.06)	Acc@5 100.00 ( 99.92)
Epoch: [66][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0938e-01 (1.3719e-01)	Acc@1  98.44 ( 97.15)	Acc@5 100.00 ( 99.92)
Epoch: [66][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0443e-01 (1.3773e-01)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.92)
Epoch: [66][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5405e-01 (1.3713e-01)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.92)
Epoch: [66][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5210e-01 (1.3674e-01)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.93)
Epoch: [66][120/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3928e-01 (1.3659e-01)	Acc@1  98.44 ( 97.09)	Acc@5 100.00 ( 99.93)
Epoch: [66][130/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4197e-01 (1.3664e-01)	Acc@1  97.66 ( 97.11)	Acc@5 100.00 ( 99.93)
Epoch: [66][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5283e-01 (1.3636e-01)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.94)
Epoch: [66][150/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2817e-01 (1.3625e-01)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.94)
Epoch: [66][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8225e-01 (1.3647e-01)	Acc@1  93.75 ( 97.17)	Acc@5 100.00 ( 99.95)
Epoch: [66][170/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3721e-01 (1.3592e-01)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.95)
Epoch: [66][180/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1902e-01 (1.3540e-01)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.95)
Epoch: [66][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1566e-01 (1.3461e-01)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.95)
Epoch: [66][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5173e-01 (1.3517e-01)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.95)
Epoch: [66][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2781e-01 (1.3446e-01)	Acc@1  96.88 ( 97.19)	Acc@5 100.00 ( 99.94)
Epoch: [66][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8506e-01 (1.3516e-01)	Acc@1  92.97 ( 97.15)	Acc@5 100.00 ( 99.94)
Epoch: [66][230/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2803e-01 (1.3568e-01)	Acc@1  92.19 ( 97.16)	Acc@5 100.00 ( 99.95)
Epoch: [66][240/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1859e-01 (1.3581e-01)	Acc@1  97.66 ( 97.15)	Acc@5 100.00 ( 99.95)
Epoch: [66][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.3597e-01)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.95)
Epoch: [66][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3916e-01 (1.3650e-01)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.95)
Epoch: [66][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5588e-01 (1.3651e-01)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.95)
Epoch: [66][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2109e-01 (1.3626e-01)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.95)
Epoch: [66][290/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1383e-01 (1.3572e-01)	Acc@1  99.22 ( 97.17)	Acc@5 100.00 ( 99.95)
Epoch: [66][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4429e-01 (1.3521e-01)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.96)
Epoch: [66][310/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7065e-01 (1.3559e-01)	Acc@1  94.53 ( 97.17)	Acc@5 100.00 ( 99.95)
Epoch: [66][320/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1005e-01 (1.3541e-01)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [66][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1133e-01 (1.3602e-01)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.95)
Epoch: [66][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4075e-01 (1.3621e-01)	Acc@1  96.09 ( 97.12)	Acc@5 100.00 ( 99.95)
Epoch: [66][350/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0449e-01 (1.3587e-01)	Acc@1  99.22 ( 97.12)	Acc@5 100.00 ( 99.96)
Epoch: [66][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9238e-01 (1.3605e-01)	Acc@1  94.53 ( 97.13)	Acc@5 100.00 ( 99.96)
Epoch: [66][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6211e-01 (1.3631e-01)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.96)
Epoch: [66][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4783e-01 (1.3613e-01)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.95)
Epoch: [66][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1145e-01 (1.3651e-01)	Acc@1  98.75 ( 97.12)	Acc@5 100.00 ( 99.96)
## e[66] optimizer.zero_grad (sum) time: 0.5371556282043457
## e[66]       loss.backward (sum) time: 12.592319965362549
## e[66]      optimizer.step (sum) time: 25.482282161712646
## epoch[66] training(only) time: 63.29519820213318
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.2021e+00 (1.2021e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 1.3574e+00 (1.2687e+00)	Acc@1  65.00 ( 70.45)	Acc@5  91.00 ( 90.45)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.2363e+00 (1.2437e+00)	Acc@1  70.00 ( 70.71)	Acc@5  92.00 ( 91.29)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.4590e+00 (1.2723e+00)	Acc@1  64.00 ( 70.35)	Acc@5  90.00 ( 90.97)
Test: [ 40/100]	Time  0.046 ( 0.052)	Loss 1.0283e+00 (1.2538e+00)	Acc@1  75.00 ( 70.34)	Acc@5  91.00 ( 91.24)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.2197e+00 (1.2569e+00)	Acc@1  74.00 ( 70.31)	Acc@5  92.00 ( 91.10)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2021e+00 (1.2460e+00)	Acc@1  66.00 ( 70.21)	Acc@5  91.00 ( 91.25)
Test: [ 70/100]	Time  0.046 ( 0.050)	Loss 1.3584e+00 (1.2424e+00)	Acc@1  63.00 ( 70.25)	Acc@5  92.00 ( 91.31)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.5898e+00 (1.2423e+00)	Acc@1  68.00 ( 70.36)	Acc@5  86.00 ( 91.21)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.3350e+00 (1.2227e+00)	Acc@1  69.00 ( 70.58)	Acc@5  90.00 ( 91.33)
 * Acc@1 70.810 Acc@5 91.420
### epoch[66] execution time: 68.29857110977173
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.321 ( 0.321)	Data  0.154 ( 0.154)	Loss 1.0797e-01 (1.0797e-01)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.6956e-01 (1.2655e-01)	Acc@1  94.53 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.159 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.5393e-01 (1.3417e-01)	Acc@1  96.09 ( 97.17)	Acc@5 100.00 ( 99.93)
Epoch: [67][ 30/391]	Time  0.173 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.2329e-01 (1.3316e-01)	Acc@1  96.88 ( 97.05)	Acc@5 100.00 ( 99.95)
Epoch: [67][ 40/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.2964e-01 (1.3384e-01)	Acc@1  96.88 ( 97.01)	Acc@5  99.22 ( 99.94)
Epoch: [67][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.4526e-01 (1.3634e-01)	Acc@1  96.09 ( 96.92)	Acc@5 100.00 ( 99.95)
Epoch: [67][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.7859e-01 (1.3640e-01)	Acc@1  95.31 ( 96.87)	Acc@5  99.22 ( 99.95)
Epoch: [67][ 70/391]	Time  0.164 ( 0.164)	Data  0.003 ( 0.003)	Loss 1.5356e-01 (1.3774e-01)	Acc@1  96.09 ( 96.83)	Acc@5 100.00 ( 99.96)
Epoch: [67][ 80/391]	Time  0.168 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3232e-01 (1.3665e-01)	Acc@1  95.31 ( 96.88)	Acc@5 100.00 ( 99.95)
Epoch: [67][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2262e-01 (1.3524e-01)	Acc@1  97.66 ( 96.94)	Acc@5 100.00 ( 99.95)
Epoch: [67][100/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1816e-01 (1.3482e-01)	Acc@1  97.66 ( 97.01)	Acc@5 100.00 ( 99.95)
Epoch: [67][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4185e-01 (1.3450e-01)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.96)
Epoch: [67][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 6.5979e-02 (1.3379e-01)	Acc@1  99.22 ( 97.09)	Acc@5 100.00 ( 99.96)
Epoch: [67][130/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0632e-01 (1.3329e-01)	Acc@1  99.22 ( 97.15)	Acc@5 100.00 ( 99.95)
Epoch: [67][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1981e-01 (1.3288e-01)	Acc@1  97.66 ( 97.17)	Acc@5 100.00 ( 99.96)
Epoch: [67][150/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5015e-01 (1.3365e-01)	Acc@1  94.53 ( 97.12)	Acc@5 100.00 ( 99.96)
Epoch: [67][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2976e-01 (1.3390e-01)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.95)
Epoch: [67][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.8999e-02 (1.3476e-01)	Acc@1  98.44 ( 97.04)	Acc@5 100.00 ( 99.95)
Epoch: [67][180/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1121e-01 (1.3405e-01)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 ( 99.95)
Epoch: [67][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1377e-01 (1.3329e-01)	Acc@1  98.44 ( 97.11)	Acc@5 100.00 ( 99.96)
Epoch: [67][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4832e-01 (1.3444e-01)	Acc@1  97.66 ( 97.06)	Acc@5 100.00 ( 99.95)
Epoch: [67][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1218e-01 (1.3447e-01)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][220/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4258e-01 (1.3445e-01)	Acc@1  96.88 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][230/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4404e-01 (1.3510e-01)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.95)
Epoch: [67][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7773e-01 (1.3580e-01)	Acc@1  95.31 ( 97.05)	Acc@5 100.00 ( 99.95)
Epoch: [67][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.1675e-02 (1.3558e-01)	Acc@1 100.00 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][260/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.8572e-02 (1.3583e-01)	Acc@1  98.44 ( 97.08)	Acc@5 100.00 ( 99.95)
Epoch: [67][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7881e-02 (1.3570e-01)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][280/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5125e-01 (1.3583e-01)	Acc@1  96.88 ( 97.06)	Acc@5 100.00 ( 99.95)
Epoch: [67][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3953e-01 (1.3520e-01)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3013e-01 (1.3520e-01)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5918e-01 (1.3565e-01)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8616e-01 (1.3582e-01)	Acc@1  96.09 ( 97.06)	Acc@5  99.22 ( 99.95)
Epoch: [67][330/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6394e-01 (1.3582e-01)	Acc@1  95.31 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9609e-02 (1.3595e-01)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.95)
Epoch: [67][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0321e-01 (1.3593e-01)	Acc@1  97.66 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0590e-01 (1.3569e-01)	Acc@1  98.44 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [67][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4819e-01 (1.3572e-01)	Acc@1  97.66 ( 97.08)	Acc@5 100.00 ( 99.95)
Epoch: [67][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3696e-01 (1.3573e-01)	Acc@1  99.22 ( 97.09)	Acc@5 100.00 ( 99.95)
Epoch: [67][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8262e-01 (1.3579e-01)	Acc@1  96.25 ( 97.10)	Acc@5 100.00 ( 99.95)
## e[67] optimizer.zero_grad (sum) time: 0.5389668941497803
## e[67]       loss.backward (sum) time: 12.582521438598633
## e[67]      optimizer.step (sum) time: 25.497323036193848
## epoch[67] training(only) time: 63.47793006896973
# Switched to evaluate mode...
Test: [  0/100]	Time  0.208 ( 0.208)	Loss 1.1982e+00 (1.1982e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.3604e+00 (1.2606e+00)	Acc@1  65.00 ( 70.18)	Acc@5  90.00 ( 90.64)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.2021e+00 (1.2321e+00)	Acc@1  69.00 ( 70.38)	Acc@5  92.00 ( 91.29)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4170e+00 (1.2642e+00)	Acc@1  65.00 ( 70.13)	Acc@5  87.00 ( 91.00)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.0264e+00 (1.2497e+00)	Acc@1  76.00 ( 70.24)	Acc@5  92.00 ( 91.32)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2471e+00 (1.2514e+00)	Acc@1  73.00 ( 70.35)	Acc@5  91.00 ( 91.22)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2031e+00 (1.2431e+00)	Acc@1  66.00 ( 70.33)	Acc@5  88.00 ( 91.18)
Test: [ 70/100]	Time  0.056 ( 0.049)	Loss 1.3779e+00 (1.2408e+00)	Acc@1  65.00 ( 70.42)	Acc@5  92.00 ( 91.24)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6201e+00 (1.2408e+00)	Acc@1  71.00 ( 70.42)	Acc@5  86.00 ( 91.25)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.2969e+00 (1.2213e+00)	Acc@1  71.00 ( 70.69)	Acc@5  90.00 ( 91.37)
 * Acc@1 70.910 Acc@5 91.450
### epoch[67] execution time: 68.4393265247345
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.316 ( 0.316)	Data  0.148 ( 0.148)	Loss 1.4856e-01 (1.4856e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.8555e-01 (1.4022e-01)	Acc@1  94.53 ( 96.66)	Acc@5  99.22 ( 99.93)
Epoch: [68][ 20/391]	Time  0.160 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.1707e-01 (1.4096e-01)	Acc@1  96.09 ( 96.69)	Acc@5 100.00 ( 99.93)
Epoch: [68][ 30/391]	Time  0.176 ( 0.167)	Data  0.001 ( 0.006)	Loss 2.0056e-01 (1.3741e-01)	Acc@1  92.19 ( 96.90)	Acc@5 100.00 ( 99.92)
Epoch: [68][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.4185e-01 (1.3712e-01)	Acc@1  98.44 ( 96.95)	Acc@5 100.00 ( 99.94)
Epoch: [68][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.9548e-02 (1.3563e-01)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.95)
Epoch: [68][ 60/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1267e-01 (1.3508e-01)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.96)
Epoch: [68][ 70/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3660e-01 (1.3547e-01)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.97)
Epoch: [68][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1200e-01 (1.3434e-01)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.97)
Epoch: [68][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3770e-01 (1.3448e-01)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [68][100/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.5215e-02 (1.3335e-01)	Acc@1  97.66 ( 97.22)	Acc@5 100.00 ( 99.98)
Epoch: [68][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0742e-01 (1.3378e-01)	Acc@1  99.22 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [68][120/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1884e-01 (1.3358e-01)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [68][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0081e-01 (1.3355e-01)	Acc@1  95.31 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [68][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1560e-01 (1.3406e-01)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [68][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2915e-01 (1.3355e-01)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.96)
Epoch: [68][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3586e-01 (1.3353e-01)	Acc@1  96.88 ( 97.27)	Acc@5  99.22 ( 99.95)
Epoch: [68][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8506e-01 (1.3399e-01)	Acc@1  95.31 ( 97.25)	Acc@5 100.00 ( 99.95)
Epoch: [68][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6541e-01 (1.3340e-01)	Acc@1  92.19 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [68][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2091e-01 (1.3294e-01)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [68][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0223e-01 (1.3198e-01)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [68][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5271e-01 (1.3171e-01)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [68][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1985e-01 (1.3224e-01)	Acc@1  93.75 ( 97.24)	Acc@5 100.00 ( 99.95)
Epoch: [68][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7065e-01 (1.3282e-01)	Acc@1  96.88 ( 97.22)	Acc@5 100.00 ( 99.96)
Epoch: [68][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2280e-01 (1.3382e-01)	Acc@1  96.88 ( 97.18)	Acc@5 100.00 ( 99.96)
Epoch: [68][250/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3733e-01 (1.3442e-01)	Acc@1  95.31 ( 97.16)	Acc@5 100.00 ( 99.96)
Epoch: [68][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3611e-01 (1.3501e-01)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.95)
Epoch: [68][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2622e-01 (1.3492e-01)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.95)
Epoch: [68][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3745e-01 (1.3478e-01)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.95)
Epoch: [68][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2103e-01 (1.3462e-01)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.95)
Epoch: [68][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9714e-01 (1.3510e-01)	Acc@1  92.97 ( 97.14)	Acc@5 100.00 ( 99.95)
Epoch: [68][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2866e-01 (1.3539e-01)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.95)
Epoch: [68][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0828e-01 (1.3552e-01)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.95)
Epoch: [68][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5088e-01 (1.3597e-01)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.94)
Epoch: [68][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1169e-01 (1.3577e-01)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.95)
Epoch: [68][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2091e-01 (1.3519e-01)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.95)
Epoch: [68][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0925e-01 (1.3501e-01)	Acc@1  96.88 ( 97.17)	Acc@5 100.00 ( 99.95)
Epoch: [68][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4453e-01 (1.3470e-01)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.95)
Epoch: [68][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2524e-01 (1.3472e-01)	Acc@1  98.44 ( 97.19)	Acc@5 100.00 ( 99.95)
Epoch: [68][390/391]	Time  0.122 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2964e-01 (1.3483e-01)	Acc@1  98.75 ( 97.19)	Acc@5 100.00 ( 99.95)
## e[68] optimizer.zero_grad (sum) time: 0.5405280590057373
## e[68]       loss.backward (sum) time: 12.579997062683105
## e[68]      optimizer.step (sum) time: 25.50318431854248
## epoch[68] training(only) time: 63.450968503952026
# Switched to evaluate mode...
Test: [  0/100]	Time  0.195 ( 0.195)	Loss 1.1895e+00 (1.1895e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.3369e+00 (1.2499e+00)	Acc@1  66.00 ( 70.00)	Acc@5  89.00 ( 90.45)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.2500e+00 (1.2303e+00)	Acc@1  69.00 ( 70.33)	Acc@5  92.00 ( 91.10)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4219e+00 (1.2631e+00)	Acc@1  66.00 ( 70.10)	Acc@5  89.00 ( 90.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.0098e+00 (1.2456e+00)	Acc@1  77.00 ( 70.34)	Acc@5  92.00 ( 91.10)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2197e+00 (1.2480e+00)	Acc@1  74.00 ( 70.51)	Acc@5  92.00 ( 90.98)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2119e+00 (1.2391e+00)	Acc@1  67.00 ( 70.51)	Acc@5  91.00 ( 91.08)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.3936e+00 (1.2386e+00)	Acc@1  64.00 ( 70.52)	Acc@5  92.00 ( 91.10)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.5977e+00 (1.2387e+00)	Acc@1  72.00 ( 70.59)	Acc@5  86.00 ( 91.14)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2852e+00 (1.2190e+00)	Acc@1  70.00 ( 70.86)	Acc@5  90.00 ( 91.22)
 * Acc@1 71.120 Acc@5 91.350
### epoch[68] execution time: 68.4052050113678
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.321 ( 0.321)	Data  0.153 ( 0.153)	Loss 1.0687e-01 (1.0687e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.159 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.5540e-01 (1.2852e-01)	Acc@1  95.31 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.171 ( 0.170)	Data  0.001 ( 0.008)	Loss 1.9666e-01 (1.3945e-01)	Acc@1  94.53 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.2231e-01 (1.3506e-01)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [69][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 8.2703e-02 (1.3498e-01)	Acc@1  99.22 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 50/391]	Time  0.177 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.3098e-01 (1.3488e-01)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.97)
Epoch: [69][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4404e-01 (1.3344e-01)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [69][ 70/391]	Time  0.171 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3708e-01 (1.3512e-01)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1456e-01 (1.3309e-01)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [69][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.1191e-01 (1.3423e-01)	Acc@1  93.75 ( 97.30)	Acc@5  99.22 ( 99.96)
Epoch: [69][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4905e-01 (1.3403e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [69][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4819e-01 (1.3413e-01)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [69][120/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3098e-01 (1.3333e-01)	Acc@1  96.88 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [69][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3940e-01 (1.3313e-01)	Acc@1  95.31 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [69][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3782e-01 (1.3291e-01)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.97)
Epoch: [69][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1938e-01 (1.3277e-01)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.97)
Epoch: [69][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1017e-01 (1.3238e-01)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.97)
Epoch: [69][170/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2549e-01 (1.3214e-01)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [69][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0425e-01 (1.3174e-01)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.97)
Epoch: [69][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4917e-01 (1.3135e-01)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [69][200/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2488e-01 (1.3162e-01)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.97)
Epoch: [69][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0931e-01 (1.3225e-01)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [69][220/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2671e-01 (1.3149e-01)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.97)
Epoch: [69][230/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6125e-01 (1.3236e-01)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [69][240/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3428e-01 (1.3250e-01)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [69][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1700e-01 (1.3235e-01)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [69][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4197e-01 (1.3274e-01)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [69][270/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5784e-01 (1.3300e-01)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.97)
Epoch: [69][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0889e-01 (1.3284e-01)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [69][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3110e-01 (1.3273e-01)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [69][300/391]	Time  0.174 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2415e-01 (1.3275e-01)	Acc@1  95.31 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [69][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1237e-01 (1.3268e-01)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [69][320/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4722e-01 (1.3251e-01)	Acc@1  96.88 ( 97.29)	Acc@5  99.22 ( 99.96)
Epoch: [69][330/391]	Time  0.177 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9922e-01 (1.3273e-01)	Acc@1  94.53 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [69][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1145e-01 (1.3277e-01)	Acc@1  99.22 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [69][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.5520e-02 (1.3265e-01)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [69][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3098e-01 (1.3269e-01)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [69][370/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7371e-01 (1.3260e-01)	Acc@1  96.09 ( 97.26)	Acc@5 100.00 ( 99.96)
Epoch: [69][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2378e-01 (1.3275e-01)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.97)
Epoch: [69][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.4194e-01 (1.3327e-01)	Acc@1  92.50 ( 97.25)	Acc@5 100.00 ( 99.96)
## e[69] optimizer.zero_grad (sum) time: 0.5432877540588379
## e[69]       loss.backward (sum) time: 12.61800241470337
## e[69]      optimizer.step (sum) time: 25.483641624450684
## epoch[69] training(only) time: 63.47116136550903
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1875e+00 (1.1875e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.3574e+00 (1.2599e+00)	Acc@1  66.00 ( 69.82)	Acc@5  90.00 ( 90.55)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.2178e+00 (1.2357e+00)	Acc@1  71.00 ( 70.71)	Acc@5  92.00 ( 91.10)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4053e+00 (1.2663e+00)	Acc@1  66.00 ( 70.42)	Acc@5  88.00 ( 90.71)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.0254e+00 (1.2484e+00)	Acc@1  76.00 ( 70.59)	Acc@5  92.00 ( 91.07)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2354e+00 (1.2500e+00)	Acc@1  74.00 ( 70.73)	Acc@5  92.00 ( 90.98)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.1973e+00 (1.2400e+00)	Acc@1  67.00 ( 70.67)	Acc@5  91.00 ( 91.11)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3721e+00 (1.2369e+00)	Acc@1  64.00 ( 70.70)	Acc@5  93.00 ( 91.21)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6084e+00 (1.2377e+00)	Acc@1  72.00 ( 70.73)	Acc@5  86.00 ( 91.19)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2939e+00 (1.2181e+00)	Acc@1  69.00 ( 70.91)	Acc@5  90.00 ( 91.34)
 * Acc@1 71.100 Acc@5 91.400
### epoch[69] execution time: 68.41872715950012
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.346 ( 0.346)	Data  0.179 ( 0.179)	Loss 1.8445e-01 (1.8445e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.017)	Loss 1.4343e-01 (1.4757e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.86)
Epoch: [70][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.010)	Loss 1.0938e-01 (1.4604e-01)	Acc@1  99.22 ( 96.65)	Acc@5 100.00 ( 99.85)
Epoch: [70][ 30/391]	Time  0.177 ( 0.168)	Data  0.001 ( 0.007)	Loss 8.3374e-02 (1.3924e-01)	Acc@1  99.22 ( 97.10)	Acc@5 100.00 ( 99.87)
Epoch: [70][ 40/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.6724e-01 (1.3818e-01)	Acc@1  94.53 ( 97.18)	Acc@5 100.00 ( 99.90)
Epoch: [70][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 9.7778e-02 (1.3786e-01)	Acc@1  99.22 ( 97.07)	Acc@5 100.00 ( 99.92)
Epoch: [70][ 60/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.5723e-01 (1.3954e-01)	Acc@1  95.31 ( 96.98)	Acc@5 100.00 ( 99.94)
Epoch: [70][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.5344e-01 (1.4140e-01)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.94)
Epoch: [70][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.5938e-02 (1.3822e-01)	Acc@1  99.22 ( 97.00)	Acc@5 100.00 ( 99.95)
Epoch: [70][ 90/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4417e-01 (1.3853e-01)	Acc@1  95.31 ( 97.01)	Acc@5 100.00 ( 99.94)
Epoch: [70][100/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.8103e-01 (1.3702e-01)	Acc@1  93.75 ( 97.10)	Acc@5 100.00 ( 99.94)
Epoch: [70][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3745e-01 (1.3742e-01)	Acc@1  96.09 ( 97.07)	Acc@5 100.00 ( 99.94)
Epoch: [70][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2152e-01 (1.3648e-01)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.94)
Epoch: [70][130/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3923e-02 (1.3546e-01)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.93)
Epoch: [70][140/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2500e-01 (1.3478e-01)	Acc@1  97.66 ( 97.19)	Acc@5 100.00 ( 99.93)
Epoch: [70][150/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5186e-01 (1.3491e-01)	Acc@1  96.09 ( 97.18)	Acc@5 100.00 ( 99.93)
Epoch: [70][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0105e-01 (1.3479e-01)	Acc@1  94.53 ( 97.16)	Acc@5 100.00 ( 99.94)
Epoch: [70][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2042e-01 (1.3459e-01)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.94)
Epoch: [70][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0675e-01 (1.3442e-01)	Acc@1  99.22 ( 97.17)	Acc@5 100.00 ( 99.94)
Epoch: [70][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8469e-01 (1.3525e-01)	Acc@1  92.97 ( 97.14)	Acc@5 100.00 ( 99.94)
Epoch: [70][200/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2476e-01 (1.3506e-01)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.94)
Epoch: [70][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4844e-01 (1.3523e-01)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.94)
Epoch: [70][220/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4355e-01 (1.3477e-01)	Acc@1  96.09 ( 97.17)	Acc@5  99.22 ( 99.94)
Epoch: [70][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2103e-01 (1.3487e-01)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.94)
Epoch: [70][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0571e-01 (1.3456e-01)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.94)
Epoch: [70][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0944e-01 (1.3522e-01)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 ( 99.93)
Epoch: [70][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3171e-01 (1.3570e-01)	Acc@1  95.31 ( 97.13)	Acc@5 100.00 ( 99.93)
Epoch: [70][270/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2079e-01 (1.3585e-01)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.93)
Epoch: [70][280/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7542e-01 (1.3616e-01)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.93)
Epoch: [70][290/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2292e-01 (1.3677e-01)	Acc@1  96.88 ( 97.10)	Acc@5 100.00 ( 99.93)
Epoch: [70][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.7473e-02 (1.3616e-01)	Acc@1  99.22 ( 97.11)	Acc@5 100.00 ( 99.94)
Epoch: [70][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3245e-01 (1.3563e-01)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.93)
Epoch: [70][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.7952e-02 (1.3538e-01)	Acc@1 100.00 ( 97.15)	Acc@5 100.00 ( 99.93)
Epoch: [70][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7346e-01 (1.3539e-01)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.94)
Epoch: [70][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1963e-01 (1.3534e-01)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.94)
Epoch: [70][350/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4136e-01 (1.3551e-01)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.94)
Epoch: [70][360/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4197e-01 (1.3549e-01)	Acc@1  96.88 ( 97.13)	Acc@5  99.22 ( 99.94)
Epoch: [70][370/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1777e-01 (1.3536e-01)	Acc@1  92.97 ( 97.15)	Acc@5  99.22 ( 99.94)
Epoch: [70][380/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4490e-01 (1.3523e-01)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.94)
Epoch: [70][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0321e-01 (1.3505e-01)	Acc@1  98.75 ( 97.16)	Acc@5 100.00 ( 99.94)
## e[70] optimizer.zero_grad (sum) time: 0.5403664112091064
## e[70]       loss.backward (sum) time: 12.60987639427185
## e[70]      optimizer.step (sum) time: 25.482637643814087
## epoch[70] training(only) time: 63.520668029785156
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.1768e+00 (1.1768e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.059 ( 0.061)	Loss 1.3535e+00 (1.2674e+00)	Acc@1  66.00 ( 70.09)	Acc@5  91.00 ( 90.27)
Test: [ 20/100]	Time  0.048 ( 0.055)	Loss 1.2129e+00 (1.2398e+00)	Acc@1  70.00 ( 70.52)	Acc@5  92.00 ( 91.19)
Test: [ 30/100]	Time  0.048 ( 0.053)	Loss 1.4082e+00 (1.2712e+00)	Acc@1  65.00 ( 70.19)	Acc@5  87.00 ( 90.74)
Test: [ 40/100]	Time  0.048 ( 0.052)	Loss 1.0215e+00 (1.2534e+00)	Acc@1  74.00 ( 70.29)	Acc@5  93.00 ( 91.05)
Test: [ 50/100]	Time  0.048 ( 0.051)	Loss 1.2285e+00 (1.2545e+00)	Acc@1  74.00 ( 70.37)	Acc@5  92.00 ( 90.94)
Test: [ 60/100]	Time  0.048 ( 0.051)	Loss 1.1904e+00 (1.2448e+00)	Acc@1  66.00 ( 70.30)	Acc@5  91.00 ( 91.08)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.3770e+00 (1.2417e+00)	Acc@1  64.00 ( 70.35)	Acc@5  92.00 ( 91.18)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.6045e+00 (1.2417e+00)	Acc@1  71.00 ( 70.36)	Acc@5  86.00 ( 91.16)
Test: [ 90/100]	Time  0.048 ( 0.050)	Loss 1.3174e+00 (1.2224e+00)	Acc@1  70.00 ( 70.64)	Acc@5  90.00 ( 91.30)
 * Acc@1 70.840 Acc@5 91.360
### epoch[70] execution time: 68.60879111289978
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.319 ( 0.319)	Data  0.150 ( 0.150)	Loss 1.3806e-01 (1.3806e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.1584e-01 (1.2528e-01)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.2079e-01 (1.2457e-01)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.96)
Epoch: [71][ 30/391]	Time  0.165 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.6370e-01 (1.2824e-01)	Acc@1  96.09 ( 97.71)	Acc@5 100.00 ( 99.95)
Epoch: [71][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.7786e-01 (1.3457e-01)	Acc@1  92.19 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [71][ 50/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.1187e-02 (1.3101e-01)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.4209e-01 (1.3092e-01)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 70/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.6187e-01 (1.3036e-01)	Acc@1  95.31 ( 97.54)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4404e-01 (1.3090e-01)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.97)
Epoch: [71][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3623e-01 (1.3163e-01)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [71][100/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.002)	Loss 9.7656e-02 (1.3115e-01)	Acc@1  99.22 ( 97.40)	Acc@5 100.00 ( 99.98)
Epoch: [71][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.3342e-01 (1.3037e-01)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [71][120/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.3928e-01 (1.3080e-01)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [71][130/391]	Time  0.178 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.1499e-01 (1.2982e-01)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [71][140/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.0468e-01 (1.2993e-01)	Acc@1  98.44 ( 97.52)	Acc@5 100.00 ( 99.95)
Epoch: [71][150/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.002)	Loss 9.4971e-02 (1.2960e-01)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.95)
Epoch: [71][160/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 7.9407e-02 (1.2918e-01)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [71][170/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.0303e-01 (1.3002e-01)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.95)
Epoch: [71][180/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.0370e-01 (1.3058e-01)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.94)
Epoch: [71][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2957e-02 (1.3088e-01)	Acc@1  99.22 ( 97.40)	Acc@5 100.00 ( 99.95)
Epoch: [71][200/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5686e-01 (1.3128e-01)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.94)
Epoch: [71][210/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4868e-01 (1.3126e-01)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.94)
Epoch: [71][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4563e-01 (1.3092e-01)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.94)
Epoch: [71][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4788e-02 (1.3112e-01)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.94)
Epoch: [71][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.6670e-02 (1.3134e-01)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.94)
Epoch: [71][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.9163e-02 (1.3173e-01)	Acc@1  99.22 ( 97.35)	Acc@5 100.00 ( 99.94)
Epoch: [71][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1389e-01 (1.3190e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.94)
Epoch: [71][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 9.2957e-02 (1.3140e-01)	Acc@1  99.22 ( 97.34)	Acc@5 100.00 ( 99.95)
Epoch: [71][280/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.2646e-01 (1.3168e-01)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.94)
Epoch: [71][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.8201e-01 (1.3200e-01)	Acc@1  94.53 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [71][300/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.4307e-01 (1.3206e-01)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [71][310/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.1066e-01 (1.3190e-01)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.95)
Epoch: [71][320/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.4087e-01 (1.3135e-01)	Acc@1  96.88 ( 97.35)	Acc@5  99.22 ( 99.95)
Epoch: [71][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.4465e-01 (1.3127e-01)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.95)
Epoch: [71][340/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 7.9468e-02 (1.3094e-01)	Acc@1  99.22 ( 97.36)	Acc@5 100.00 ( 99.95)
Epoch: [71][350/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.0663e-01 (1.3116e-01)	Acc@1  99.22 ( 97.36)	Acc@5 100.00 ( 99.95)
Epoch: [71][360/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.4685e-01 (1.3157e-01)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.95)
Epoch: [71][370/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.1462e-01 (1.3167e-01)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.95)
Epoch: [71][380/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.7029e-01 (1.3134e-01)	Acc@1  95.31 ( 97.36)	Acc@5 100.00 ( 99.95)
Epoch: [71][390/391]	Time  0.116 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.9141e-01 (1.3145e-01)	Acc@1  95.00 ( 97.36)	Acc@5 100.00 ( 99.95)
## e[71] optimizer.zero_grad (sum) time: 0.5569994449615479
## e[71]       loss.backward (sum) time: 12.604682207107544
## e[71]      optimizer.step (sum) time: 25.467679500579834
## epoch[71] training(only) time: 63.720452308654785
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1729e+00 (1.1729e+00)	Acc@1  74.00 ( 74.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.3262e+00 (1.2502e+00)	Acc@1  67.00 ( 71.18)	Acc@5  91.00 ( 90.55)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.2354e+00 (1.2287e+00)	Acc@1  71.00 ( 71.43)	Acc@5  92.00 ( 91.38)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4170e+00 (1.2606e+00)	Acc@1  66.00 ( 70.68)	Acc@5  88.00 ( 91.03)
Test: [ 40/100]	Time  0.050 ( 0.051)	Loss 9.9951e-01 (1.2413e+00)	Acc@1  76.00 ( 70.83)	Acc@5  92.00 ( 91.32)
Test: [ 50/100]	Time  0.049 ( 0.050)	Loss 1.1836e+00 (1.2425e+00)	Acc@1  74.00 ( 70.82)	Acc@5  92.00 ( 91.18)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2207e+00 (1.2353e+00)	Acc@1  67.00 ( 70.74)	Acc@5  92.00 ( 91.28)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3672e+00 (1.2330e+00)	Acc@1  64.00 ( 70.75)	Acc@5  93.00 ( 91.39)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6104e+00 (1.2332e+00)	Acc@1  71.00 ( 70.74)	Acc@5  86.00 ( 91.35)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3115e+00 (1.2142e+00)	Acc@1  70.00 ( 70.97)	Acc@5  90.00 ( 91.45)
 * Acc@1 71.150 Acc@5 91.520
### epoch[71] execution time: 68.69160628318787
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.328 ( 0.328)	Data  0.154 ( 0.154)	Loss 1.2189e-01 (1.2189e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.161 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.0870e-01 (1.1240e-01)	Acc@1  97.66 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.1493e-01 (1.1383e-01)	Acc@1  99.22 ( 98.14)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.164 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.2720e-01 (1.1650e-01)	Acc@1  96.88 ( 97.91)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.3977e-01 (1.1914e-01)	Acc@1  96.09 ( 97.77)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4197e-01 (1.2126e-01)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.98)
Epoch: [72][ 60/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0345e-01 (1.2288e-01)	Acc@1  97.66 ( 97.69)	Acc@5 100.00 ( 99.97)
Epoch: [72][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.7273e-01 (1.2493e-01)	Acc@1  93.75 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [72][ 80/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4038e-01 (1.2681e-01)	Acc@1  96.88 ( 97.55)	Acc@5 100.00 ( 99.98)
Epoch: [72][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1456e-01 (1.2694e-01)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.98)
Epoch: [72][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1450e-01 (1.2729e-01)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [72][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0602e-01 (1.2778e-01)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.99)
Epoch: [72][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.2800e-01)	Acc@1  94.53 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [72][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5540e-01 (1.2777e-01)	Acc@1  96.09 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [72][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3330e-01 (1.2799e-01)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [72][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0516e-01 (1.2740e-01)	Acc@1  99.22 ( 97.49)	Acc@5 100.00 ( 99.98)
Epoch: [72][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2366e-01 (1.2729e-01)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.98)
Epoch: [72][170/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2219e-01 (1.2799e-01)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.98)
Epoch: [72][180/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6553e-01 (1.2944e-01)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.98)
Epoch: [72][190/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4258e-01 (1.2998e-01)	Acc@1  96.09 ( 97.35)	Acc@5  99.22 ( 99.98)
Epoch: [72][200/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4612e-01 (1.3030e-01)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.97)
Epoch: [72][210/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9360e-01 (1.3055e-01)	Acc@1  96.09 ( 97.34)	Acc@5  99.22 ( 99.97)
Epoch: [72][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0620e-01 (1.3008e-01)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [72][230/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2067e-01 (1.2978e-01)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [72][240/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.3801e-02 (1.2941e-01)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 ( 99.97)
Epoch: [72][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2482e-01 (1.2910e-01)	Acc@1  96.88 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [72][260/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1401e-01 (1.2939e-01)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [72][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7944e-01 (1.2988e-01)	Acc@1  95.31 ( 97.37)	Acc@5 100.00 ( 99.97)
Epoch: [72][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2274e-01 (1.3009e-01)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [72][290/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6003e-01 (1.3006e-01)	Acc@1  96.09 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [72][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3159e-01 (1.3039e-01)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [72][310/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.2461e-01 (1.3111e-01)	Acc@1  92.97 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [72][320/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2433e-01 (1.3085e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [72][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4746e-01 (1.3094e-01)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.95)
Epoch: [72][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4026e-01 (1.3118e-01)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.95)
Epoch: [72][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6394e-01 (1.3143e-01)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [72][360/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1255e-01 (1.3143e-01)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [72][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5271e-01 (1.3159e-01)	Acc@1  97.66 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [72][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0950e-01 (1.3165e-01)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [72][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.3496e-02 (1.3145e-01)	Acc@1 100.00 ( 97.30)	Acc@5 100.00 ( 99.95)
## e[72] optimizer.zero_grad (sum) time: 0.5441446304321289
## e[72]       loss.backward (sum) time: 12.63580846786499
## e[72]      optimizer.step (sum) time: 25.464511394500732
## epoch[72] training(only) time: 63.376036167144775
# Switched to evaluate mode...
Test: [  0/100]	Time  0.189 ( 0.189)	Loss 1.1758e+00 (1.1758e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.3594e+00 (1.2526e+00)	Acc@1  65.00 ( 69.73)	Acc@5  90.00 ( 90.27)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.2109e+00 (1.2306e+00)	Acc@1  71.00 ( 70.33)	Acc@5  92.00 ( 91.10)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4248e+00 (1.2641e+00)	Acc@1  65.00 ( 70.00)	Acc@5  89.00 ( 90.84)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.0010e+00 (1.2464e+00)	Acc@1  76.00 ( 70.32)	Acc@5  92.00 ( 91.24)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2109e+00 (1.2475e+00)	Acc@1  74.00 ( 70.43)	Acc@5  92.00 ( 91.12)
Test: [ 60/100]	Time  0.057 ( 0.050)	Loss 1.2041e+00 (1.2376e+00)	Acc@1  67.00 ( 70.41)	Acc@5  91.00 ( 91.21)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3799e+00 (1.2361e+00)	Acc@1  64.00 ( 70.49)	Acc@5  93.00 ( 91.30)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6104e+00 (1.2367e+00)	Acc@1  72.00 ( 70.56)	Acc@5  86.00 ( 91.26)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.2793e+00 (1.2162e+00)	Acc@1  71.00 ( 70.84)	Acc@5  90.00 ( 91.36)
 * Acc@1 71.060 Acc@5 91.470
### epoch[72] execution time: 68.34335780143738
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.325 ( 0.325)	Data  0.154 ( 0.154)	Loss 1.1804e-01 (1.1804e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.2317e-01 (1.2251e-01)	Acc@1  97.66 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.1554e-01 (1.3717e-01)	Acc@1  97.66 ( 96.95)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.5820e-01 (1.3398e-01)	Acc@1  96.88 ( 97.08)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.3635e-01 (1.3378e-01)	Acc@1  97.66 ( 97.14)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.2140e-01 (1.3190e-01)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.98)
Epoch: [73][ 60/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.6797e-01 (1.3383e-01)	Acc@1  96.88 ( 97.16)	Acc@5 100.00 ( 99.97)
Epoch: [73][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4368e-01 (1.3276e-01)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.98)
Epoch: [73][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1908e-01 (1.3106e-01)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [73][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7334e-01 (1.3114e-01)	Acc@1  95.31 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [73][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7395e-01 (1.3140e-01)	Acc@1  95.31 ( 97.19)	Acc@5 100.00 ( 99.97)
Epoch: [73][110/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0175e-01 (1.3094e-01)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.97)
Epoch: [73][120/391]	Time  0.161 ( 0.163)	Data  0.002 ( 0.002)	Loss 1.7090e-01 (1.3186e-01)	Acc@1  95.31 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [73][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1694e-01 (1.3117e-01)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.97)
Epoch: [73][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5979e-01 (1.3118e-01)	Acc@1  95.31 ( 97.24)	Acc@5 100.00 ( 99.97)
Epoch: [73][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6492e-01 (1.3196e-01)	Acc@1  94.53 ( 97.20)	Acc@5  99.22 ( 99.96)
Epoch: [73][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0974e-01 (1.3132e-01)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [73][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3965e-01 (1.3104e-01)	Acc@1  96.09 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [73][180/391]	Time  0.176 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1926e-01 (1.3023e-01)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [73][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4600e-01 (1.2978e-01)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.97)
Epoch: [73][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2915e-01 (1.2967e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [73][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0309e-01 (1.2976e-01)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [73][220/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2341e-01 (1.2979e-01)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [73][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7810e-01 (1.2916e-01)	Acc@1  94.53 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [73][240/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1743e-01 (1.2961e-01)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [73][250/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.3242e-01 (1.3011e-01)	Acc@1  94.53 ( 97.34)	Acc@5  99.22 ( 99.96)
Epoch: [73][260/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0834e-01 (1.2992e-01)	Acc@1  99.22 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [73][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6895e-01 (1.3010e-01)	Acc@1  95.31 ( 97.35)	Acc@5  99.22 ( 99.95)
Epoch: [73][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3318e-01 (1.3016e-01)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 ( 99.95)
Epoch: [73][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4185e-01 (1.3039e-01)	Acc@1  96.88 ( 97.37)	Acc@5  99.22 ( 99.95)
Epoch: [73][300/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6504e-01 (1.3018e-01)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.95)
Epoch: [73][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4355e-01 (1.3019e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.95)
Epoch: [73][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2598e-01 (1.3040e-01)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.95)
Epoch: [73][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0529e-01 (1.3038e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.95)
Epoch: [73][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1432e-01 (1.3059e-01)	Acc@1  99.22 ( 97.36)	Acc@5 100.00 ( 99.95)
Epoch: [73][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3037e-01 (1.3018e-01)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.95)
Epoch: [73][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2256e-01 (1.3065e-01)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.95)
Epoch: [73][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1499e-01 (1.3089e-01)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 ( 99.95)
Epoch: [73][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2549e-01 (1.3112e-01)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.95)
Epoch: [73][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3171e-01 (1.3129e-01)	Acc@1  97.50 ( 97.31)	Acc@5 100.00 ( 99.95)
## e[73] optimizer.zero_grad (sum) time: 0.5381848812103271
## e[73]       loss.backward (sum) time: 12.612654447555542
## e[73]      optimizer.step (sum) time: 25.468653917312622
## epoch[73] training(only) time: 63.369582414627075
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.1787e+00 (1.1787e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.047 ( 0.059)	Loss 1.3486e+00 (1.2466e+00)	Acc@1  66.00 ( 70.27)	Acc@5  90.00 ( 90.36)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.2266e+00 (1.2261e+00)	Acc@1  67.00 ( 70.52)	Acc@5  92.00 ( 91.19)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4277e+00 (1.2598e+00)	Acc@1  65.00 ( 70.13)	Acc@5  89.00 ( 90.84)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0039e+00 (1.2411e+00)	Acc@1  77.00 ( 70.46)	Acc@5  92.00 ( 91.22)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2100e+00 (1.2437e+00)	Acc@1  74.00 ( 70.57)	Acc@5  94.00 ( 91.14)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1992e+00 (1.2340e+00)	Acc@1  68.00 ( 70.49)	Acc@5  91.00 ( 91.23)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4053e+00 (1.2314e+00)	Acc@1  64.00 ( 70.63)	Acc@5  91.00 ( 91.25)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.5869e+00 (1.2329e+00)	Acc@1  71.00 ( 70.65)	Acc@5  86.00 ( 91.21)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.2910e+00 (1.2128e+00)	Acc@1  71.00 ( 70.96)	Acc@5  90.00 ( 91.29)
 * Acc@1 71.170 Acc@5 91.380
### epoch[73] execution time: 68.32372045516968
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.319 ( 0.319)	Data  0.140 ( 0.140)	Loss 1.4355e-01 (1.4355e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.2408e-01 (1.2011e-01)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.5356e-01 (1.2520e-01)	Acc@1  96.88 ( 97.32)	Acc@5  99.22 ( 99.96)
Epoch: [74][ 30/391]	Time  0.171 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.1108e-01 (1.2738e-01)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.9946e-01 (1.2763e-01)	Acc@1  93.75 ( 97.43)	Acc@5 100.00 ( 99.98)
Epoch: [74][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.5234e-01 (1.2762e-01)	Acc@1  98.44 ( 97.52)	Acc@5 100.00 ( 99.98)
Epoch: [74][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1200e-01 (1.3076e-01)	Acc@1  96.88 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4893e-01 (1.3097e-01)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [74][ 80/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1322e-01 (1.3245e-01)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.97)
Epoch: [74][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2407e-02 (1.3111e-01)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [74][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4209e-01 (1.3288e-01)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.96)
Epoch: [74][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0126e-01 (1.3151e-01)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.96)
Epoch: [74][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2878e-01 (1.3094e-01)	Acc@1  96.09 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [74][130/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0492e-01 (1.3072e-01)	Acc@1  99.22 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [74][140/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.2102e-02 (1.3060e-01)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [74][150/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1920e-01 (1.2931e-01)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [74][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6406e-01 (1.2972e-01)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [74][170/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4172e-01 (1.2900e-01)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [74][180/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2427e-01 (1.2907e-01)	Acc@1  96.88 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [74][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0687e-01 (1.2934e-01)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [74][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5332e-01 (1.2992e-01)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.95)
Epoch: [74][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.9946e-01 (1.2982e-01)	Acc@1  95.31 ( 97.40)	Acc@5  99.22 ( 99.95)
Epoch: [74][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1884e-01 (1.2984e-01)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.95)
Epoch: [74][230/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7322e-01 (1.2981e-01)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [74][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5613e-01 (1.3064e-01)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [74][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2939e-01 (1.3088e-01)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [74][260/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1761e-01 (1.3123e-01)	Acc@1  98.44 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [74][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2097e-01 (1.3109e-01)	Acc@1  97.66 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [74][280/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5735e-01 (1.3094e-01)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [74][290/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.3750e-02 (1.3040e-01)	Acc@1 100.00 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [74][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9128e-01 (1.3062e-01)	Acc@1  94.53 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [74][310/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4465e-01 (1.3078e-01)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [74][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1938e-01 (1.3109e-01)	Acc@1  99.22 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [74][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3843e-01 (1.3100e-01)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [74][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3293e-01 (1.3141e-01)	Acc@1  99.22 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [74][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6089e-01 (1.3160e-01)	Acc@1  95.31 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [74][360/391]	Time  0.165 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.8145e-02 (1.3169e-01)	Acc@1  99.22 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [74][370/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1963e-01 (1.3202e-01)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [74][380/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5027e-01 (1.3181e-01)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [74][390/391]	Time  0.119 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8823e-01 (1.3193e-01)	Acc@1  96.25 ( 97.28)	Acc@5 100.00 ( 99.96)
## e[74] optimizer.zero_grad (sum) time: 0.5526926517486572
## e[74]       loss.backward (sum) time: 12.616791486740112
## e[74]      optimizer.step (sum) time: 25.471288204193115
## epoch[74] training(only) time: 63.362191677093506
# Switched to evaluate mode...
Test: [  0/100]	Time  0.188 ( 0.188)	Loss 1.1768e+00 (1.1768e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.3223e+00 (1.2520e+00)	Acc@1  65.00 ( 70.27)	Acc@5  90.00 ( 90.27)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.2344e+00 (1.2344e+00)	Acc@1  69.00 ( 70.38)	Acc@5  92.00 ( 90.90)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 1.4355e+00 (1.2668e+00)	Acc@1  66.00 ( 70.10)	Acc@5  90.00 ( 90.65)
Test: [ 40/100]	Time  0.048 ( 0.051)	Loss 9.9707e-01 (1.2486e+00)	Acc@1  78.00 ( 70.44)	Acc@5  93.00 ( 91.12)
Test: [ 50/100]	Time  0.047 ( 0.051)	Loss 1.2139e+00 (1.2500e+00)	Acc@1  74.00 ( 70.59)	Acc@5  92.00 ( 91.04)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.1982e+00 (1.2399e+00)	Acc@1  68.00 ( 70.61)	Acc@5  90.00 ( 91.07)
Test: [ 70/100]	Time  0.056 ( 0.050)	Loss 1.3857e+00 (1.2382e+00)	Acc@1  64.00 ( 70.61)	Acc@5  92.00 ( 91.14)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.6104e+00 (1.2387e+00)	Acc@1  70.00 ( 70.60)	Acc@5  86.00 ( 91.09)
Test: [ 90/100]	Time  0.047 ( 0.050)	Loss 1.2959e+00 (1.2185e+00)	Acc@1  70.00 ( 70.88)	Acc@5  90.00 ( 91.22)
 * Acc@1 71.090 Acc@5 91.310
### epoch[74] execution time: 68.39709281921387
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.324 ( 0.324)	Data  0.153 ( 0.153)	Loss 1.6382e-01 (1.6382e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.1206e-01 (1.4284e-01)	Acc@1  97.66 ( 96.73)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.162 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.7688e-01 (1.3818e-01)	Acc@1  95.31 ( 96.99)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.3538e-01 (1.3423e-01)	Acc@1  96.09 ( 97.10)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.2976e-01 (1.3631e-01)	Acc@1  97.66 ( 96.91)	Acc@5 100.00 ( 99.98)
Epoch: [75][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.4954e-01 (1.3753e-01)	Acc@1  96.88 ( 96.97)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 60/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6150e-01 (1.3624e-01)	Acc@1  96.09 ( 96.99)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6956e-01 (1.3505e-01)	Acc@1  92.97 ( 97.01)	Acc@5 100.00 ( 99.97)
Epoch: [75][ 80/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4526e-01 (1.3514e-01)	Acc@1  96.88 ( 97.04)	Acc@5 100.00 ( 99.96)
Epoch: [75][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.5276e-02 (1.3414e-01)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.97)
Epoch: [75][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.6113e-01 (1.3317e-01)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 ( 99.96)
Epoch: [75][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7285e-01 (1.3321e-01)	Acc@1  94.53 ( 97.14)	Acc@5 100.00 ( 99.96)
Epoch: [75][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8928e-02 (1.3216e-01)	Acc@1  98.44 ( 97.16)	Acc@5 100.00 ( 99.95)
Epoch: [75][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.1370e-02 (1.3081e-01)	Acc@1  97.66 ( 97.20)	Acc@5 100.00 ( 99.95)
Epoch: [75][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5295e-01 (1.3115e-01)	Acc@1  96.09 ( 97.20)	Acc@5 100.00 ( 99.96)
Epoch: [75][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6956e-01 (1.3061e-01)	Acc@1  96.09 ( 97.24)	Acc@5 100.00 ( 99.95)
Epoch: [75][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3123e-01 (1.3048e-01)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.96)
Epoch: [75][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3689e-02 (1.3035e-01)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [75][180/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6125e-01 (1.3030e-01)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [75][190/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5393e-01 (1.3006e-01)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [75][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0260e-01 (1.2944e-01)	Acc@1  99.22 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [75][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4343e-01 (1.2953e-01)	Acc@1  93.75 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [75][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3440e-01 (1.2961e-01)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [75][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3135e-01 (1.2983e-01)	Acc@1  97.66 ( 97.27)	Acc@5 100.00 ( 99.96)
Epoch: [75][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4636e-01 (1.2973e-01)	Acc@1  95.31 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [75][250/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6260e-01 (1.3090e-01)	Acc@1  96.88 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [75][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2244e-01 (1.3054e-01)	Acc@1  96.88 ( 97.25)	Acc@5 100.00 ( 99.96)
Epoch: [75][270/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0559e-01 (1.3064e-01)	Acc@1  98.44 ( 97.26)	Acc@5  99.22 ( 99.95)
Epoch: [75][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.6558e-02 (1.2970e-01)	Acc@1 100.00 ( 97.31)	Acc@5 100.00 ( 99.96)
Epoch: [75][290/391]	Time  0.179 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1249e-01 (1.2933e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [75][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2720e-01 (1.2892e-01)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [75][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8042e-01 (1.2912e-01)	Acc@1  96.09 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [75][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0968e-01 (1.2880e-01)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [75][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4478e-01 (1.2953e-01)	Acc@1  95.31 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [75][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8103e-01 (1.2949e-01)	Acc@1  94.53 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [75][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4099e-01 (1.2940e-01)	Acc@1  94.53 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [75][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6174e-01 (1.2988e-01)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [75][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1053e-01 (1.2946e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [75][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4368e-01 (1.2973e-01)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [75][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5442e-01 (1.2986e-01)	Acc@1  97.50 ( 97.35)	Acc@5 100.00 ( 99.96)
## e[75] optimizer.zero_grad (sum) time: 0.5516183376312256
## e[75]       loss.backward (sum) time: 12.658672094345093
## e[75]      optimizer.step (sum) time: 25.447930574417114
## epoch[75] training(only) time: 63.468005895614624
# Switched to evaluate mode...
Test: [  0/100]	Time  0.199 ( 0.199)	Loss 1.1865e+00 (1.1865e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.3506e+00 (1.2584e+00)	Acc@1  66.00 ( 70.55)	Acc@5  91.00 ( 90.45)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.2246e+00 (1.2332e+00)	Acc@1  71.00 ( 71.05)	Acc@5  92.00 ( 91.19)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4688e+00 (1.2701e+00)	Acc@1  64.00 ( 70.52)	Acc@5  88.00 ( 90.87)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0059e+00 (1.2519e+00)	Acc@1  77.00 ( 70.66)	Acc@5  92.00 ( 91.22)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2393e+00 (1.2535e+00)	Acc@1  74.00 ( 70.75)	Acc@5  91.00 ( 91.10)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.1973e+00 (1.2436e+00)	Acc@1  67.00 ( 70.59)	Acc@5  91.00 ( 91.16)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3662e+00 (1.2410e+00)	Acc@1  64.00 ( 70.66)	Acc@5  93.00 ( 91.30)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.6055e+00 (1.2411e+00)	Acc@1  71.00 ( 70.69)	Acc@5  86.00 ( 91.21)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.3096e+00 (1.2210e+00)	Acc@1  71.00 ( 70.89)	Acc@5  90.00 ( 91.33)
 * Acc@1 71.060 Acc@5 91.420
### epoch[75] execution time: 68.41081619262695
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.324 ( 0.324)	Data  0.147 ( 0.147)	Loss 1.2805e-01 (1.2805e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.160 ( 0.176)	Data  0.001 ( 0.014)	Loss 1.5283e-01 (1.3255e-01)	Acc@1  97.66 ( 97.23)	Acc@5 100.00 ( 99.93)
Epoch: [76][ 20/391]	Time  0.172 ( 0.169)	Data  0.001 ( 0.008)	Loss 9.4482e-02 (1.3524e-01)	Acc@1 100.00 ( 97.06)	Acc@5 100.00 ( 99.96)
Epoch: [76][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.1987e-01 (1.2962e-01)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [76][ 40/391]	Time  0.165 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.8616e-01 (1.3237e-01)	Acc@1  94.53 ( 97.14)	Acc@5  99.22 ( 99.96)
Epoch: [76][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 7.9041e-02 (1.3298e-01)	Acc@1 100.00 ( 97.10)	Acc@5 100.00 ( 99.95)
Epoch: [76][ 60/391]	Time  0.165 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3733e-01 (1.3185e-01)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.96)
Epoch: [76][ 70/391]	Time  0.172 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1060e-01 (1.3153e-01)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.97)
Epoch: [76][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.8713e-01 (1.3093e-01)	Acc@1  96.88 ( 97.24)	Acc@5 100.00 ( 99.97)
Epoch: [76][ 90/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7627e-01 (1.3108e-01)	Acc@1  94.53 ( 97.24)	Acc@5 100.00 ( 99.97)
Epoch: [76][100/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3135e-01 (1.3174e-01)	Acc@1  96.88 ( 97.20)	Acc@5 100.00 ( 99.98)
Epoch: [76][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1865e-01 (1.2990e-01)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [76][120/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2097e-01 (1.3029e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.97)
Epoch: [76][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1298e-01 (1.2989e-01)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.98)
Epoch: [76][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0120e-01 (1.2961e-01)	Acc@1  99.22 ( 97.36)	Acc@5 100.00 ( 99.97)
Epoch: [76][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6785e-01 (1.2986e-01)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.97)
Epoch: [76][160/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.7351e-02 (1.2984e-01)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.97)
Epoch: [76][170/391]	Time  0.169 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1359e-01 (1.3000e-01)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 ( 99.97)
Epoch: [76][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5332e-01 (1.3061e-01)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.97)
Epoch: [76][190/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1627e-01 (1.3134e-01)	Acc@1  98.44 ( 97.35)	Acc@5 100.00 ( 99.97)
Epoch: [76][200/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2976e-01 (1.3109e-01)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.97)
Epoch: [76][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2445e-01 (1.3142e-01)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [76][220/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.5825e-02 (1.3064e-01)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [76][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4343e-01 (1.3019e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [76][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1993e-01 (1.3006e-01)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [76][250/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0358e-01 (1.2991e-01)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [76][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1536e-01 (1.2968e-01)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [76][270/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1359e-01 (1.2931e-01)	Acc@1  98.44 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [76][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5796e-01 (1.2928e-01)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [76][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.9897e-01 (1.2983e-01)	Acc@1  93.75 ( 97.33)	Acc@5  99.22 ( 99.96)
Epoch: [76][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1517e-01 (1.3014e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [76][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2610e-01 (1.3013e-01)	Acc@1  97.66 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [76][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1420e-01 (1.2999e-01)	Acc@1  99.22 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [76][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4758e-01 (1.2970e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [76][340/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7859e-01 (1.2988e-01)	Acc@1  93.75 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [76][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3342e-01 (1.2949e-01)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [76][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2305e-01 (1.2957e-01)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [76][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3562e-01 (1.2974e-01)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [76][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2720e-01 (1.2930e-01)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [76][390/391]	Time  0.116 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3416e-01 (1.2927e-01)	Acc@1  96.25 ( 97.38)	Acc@5 100.00 ( 99.96)
## e[76] optimizer.zero_grad (sum) time: 0.5382885932922363
## e[76]       loss.backward (sum) time: 12.638265132904053
## e[76]      optimizer.step (sum) time: 25.47642183303833
## epoch[76] training(only) time: 63.36013674736023
# Switched to evaluate mode...
Test: [  0/100]	Time  0.217 ( 0.217)	Loss 1.1680e+00 (1.1680e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.3594e+00 (1.2603e+00)	Acc@1  66.00 ( 70.64)	Acc@5  91.00 ( 90.18)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.2207e+00 (1.2370e+00)	Acc@1  70.00 ( 71.24)	Acc@5  92.00 ( 91.05)
Test: [ 30/100]	Time  0.048 ( 0.052)	Loss 1.4277e+00 (1.2706e+00)	Acc@1  65.00 ( 70.65)	Acc@5  88.00 ( 90.68)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0156e+00 (1.2497e+00)	Acc@1  76.00 ( 70.68)	Acc@5  92.00 ( 91.07)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2305e+00 (1.2528e+00)	Acc@1  73.00 ( 70.71)	Acc@5  92.00 ( 91.00)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2041e+00 (1.2428e+00)	Acc@1  68.00 ( 70.64)	Acc@5  91.00 ( 91.13)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.3848e+00 (1.2408e+00)	Acc@1  64.00 ( 70.62)	Acc@5  92.00 ( 91.21)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5977e+00 (1.2414e+00)	Acc@1  71.00 ( 70.60)	Acc@5  86.00 ( 91.16)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2793e+00 (1.2205e+00)	Acc@1  71.00 ( 70.84)	Acc@5  90.00 ( 91.29)
 * Acc@1 71.030 Acc@5 91.340
### epoch[76] execution time: 68.33360362052917
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.317 ( 0.317)	Data  0.141 ( 0.141)	Loss 1.3501e-01 (1.3501e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 6.0577e-02 (1.1476e-01)	Acc@1 100.00 ( 98.08)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.165 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.2732e-01 (1.2363e-01)	Acc@1  96.09 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.4258e-01 (1.2769e-01)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.1798e-01 (1.2598e-01)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.3281e-01 (1.2872e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [77][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1182e-01 (1.2861e-01)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.96)
Epoch: [77][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 9.7290e-02 (1.2753e-01)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.94)
Epoch: [77][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3342e-01 (1.2726e-01)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.95)
Epoch: [77][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7517e-01 (1.2828e-01)	Acc@1  96.09 ( 97.42)	Acc@5 100.00 ( 99.95)
Epoch: [77][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7881e-02 (1.2809e-01)	Acc@1  99.22 ( 97.45)	Acc@5 100.00 ( 99.95)
Epoch: [77][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4148e-01 (1.2954e-01)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.96)
Epoch: [77][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3428e-01 (1.2991e-01)	Acc@1  96.88 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [77][130/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0706e-01 (1.3023e-01)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [77][140/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6130e-02 (1.3037e-01)	Acc@1  99.22 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [77][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5747e-01 (1.3128e-01)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [77][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0337e-01 (1.3249e-01)	Acc@1  95.31 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [77][170/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.3184e-01)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.97)
Epoch: [77][180/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4905e-01 (1.3201e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [77][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3440e-01 (1.3115e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.97)
Epoch: [77][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5320e-01 (1.3092e-01)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.97)
Epoch: [77][210/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1169e-01 (1.3011e-01)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [77][220/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3220e-01 (1.3014e-01)	Acc@1  96.09 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [77][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.1614e-02 (1.3006e-01)	Acc@1 100.00 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [77][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6321e-01 (1.3000e-01)	Acc@1  96.88 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [77][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4453e-01 (1.2983e-01)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [77][260/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4368e-01 (1.2997e-01)	Acc@1  95.31 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [77][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4294e-01 (1.2999e-01)	Acc@1  96.88 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [77][280/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1542e-01 (1.2935e-01)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [77][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4014e-01 (1.2925e-01)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.97)
Epoch: [77][300/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 6.9946e-02 (1.2864e-01)	Acc@1  99.22 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [77][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3269e-01 (1.2871e-01)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [77][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.8083e-02 (1.2849e-01)	Acc@1  99.22 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [77][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4490e-01 (1.2820e-01)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.97)
Epoch: [77][340/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4453e-01 (1.2835e-01)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [77][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3757e-01 (1.2838e-01)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [77][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1572e-01 (1.2823e-01)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [77][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2854e-01 (1.2826e-01)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [77][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1938e-01 (1.2830e-01)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [77][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.3010e-01 (1.2820e-01)	Acc@1  93.75 ( 97.39)	Acc@5 100.00 ( 99.97)
## e[77] optimizer.zero_grad (sum) time: 0.5432479381561279
## e[77]       loss.backward (sum) time: 12.626994848251343
## e[77]      optimizer.step (sum) time: 25.496791124343872
## epoch[77] training(only) time: 63.50935745239258
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.1807e+00 (1.1807e+00)	Acc@1  74.00 ( 74.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.3857e+00 (1.2673e+00)	Acc@1  66.00 ( 70.18)	Acc@5  91.00 ( 89.91)
Test: [ 20/100]	Time  0.048 ( 0.054)	Loss 1.2139e+00 (1.2434e+00)	Acc@1  70.00 ( 70.52)	Acc@5  92.00 ( 90.95)
Test: [ 30/100]	Time  0.057 ( 0.052)	Loss 1.4453e+00 (1.2756e+00)	Acc@1  65.00 ( 70.03)	Acc@5  89.00 ( 90.58)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0078e+00 (1.2586e+00)	Acc@1  78.00 ( 70.39)	Acc@5  93.00 ( 91.02)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2578e+00 (1.2597e+00)	Acc@1  74.00 ( 70.53)	Acc@5  92.00 ( 90.98)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.2188e+00 (1.2499e+00)	Acc@1  66.00 ( 70.44)	Acc@5  89.00 ( 90.98)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3770e+00 (1.2475e+00)	Acc@1  65.00 ( 70.49)	Acc@5  92.00 ( 91.07)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6309e+00 (1.2482e+00)	Acc@1  71.00 ( 70.51)	Acc@5  86.00 ( 91.04)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2734e+00 (1.2279e+00)	Acc@1  72.00 ( 70.75)	Acc@5  89.00 ( 91.14)
 * Acc@1 70.970 Acc@5 91.250
### epoch[77] execution time: 68.46389126777649
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.314 ( 0.314)	Data  0.142 ( 0.142)	Loss 1.0052e-01 (1.0052e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.161 ( 0.174)	Data  0.001 ( 0.014)	Loss 9.4788e-02 (1.2240e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.93)
Epoch: [78][ 20/391]	Time  0.162 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.3025e-01 (1.2612e-01)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.93)
Epoch: [78][ 30/391]	Time  0.174 ( 0.167)	Data  0.001 ( 0.005)	Loss 1.1023e-01 (1.2415e-01)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.95)
Epoch: [78][ 40/391]	Time  0.159 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.6443e-01 (1.2535e-01)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.94)
Epoch: [78][ 50/391]	Time  0.163 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.2396e-01 (1.2612e-01)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.94)
Epoch: [78][ 60/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3269e-01 (1.2561e-01)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.95)
Epoch: [78][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2164e-01 (1.2628e-01)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.94)
Epoch: [78][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0797e-01 (1.2522e-01)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.94)
Epoch: [78][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1664e-01 (1.2529e-01)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.94)
Epoch: [78][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0004e-01 (1.2551e-01)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.95)
Epoch: [78][110/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3330e-01 (1.2533e-01)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.95)
Epoch: [78][120/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4233e-01 (1.2606e-01)	Acc@1  98.44 ( 97.46)	Acc@5 100.00 ( 99.95)
Epoch: [78][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0669e-01 (1.2632e-01)	Acc@1  97.66 ( 97.44)	Acc@5  99.22 ( 99.94)
Epoch: [78][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1932e-01 (1.2563e-01)	Acc@1  99.22 ( 97.49)	Acc@5 100.00 ( 99.94)
Epoch: [78][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0376e-01 (1.2493e-01)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.94)
Epoch: [78][160/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2561e-01 (1.2556e-01)	Acc@1  99.22 ( 97.49)	Acc@5 100.00 ( 99.95)
Epoch: [78][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0077e-01 (1.2594e-01)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.94)
Epoch: [78][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2854e-01 (1.2601e-01)	Acc@1  97.66 ( 97.51)	Acc@5  99.22 ( 99.94)
Epoch: [78][190/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2091e-01 (1.2640e-01)	Acc@1  96.88 ( 97.49)	Acc@5 100.00 ( 99.94)
Epoch: [78][200/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4417e-01 (1.2630e-01)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 ( 99.94)
Epoch: [78][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2720e-01 (1.2635e-01)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 ( 99.94)
Epoch: [78][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1700e-01 (1.2686e-01)	Acc@1  96.88 ( 97.48)	Acc@5 100.00 ( 99.94)
Epoch: [78][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.4421e-02 (1.2661e-01)	Acc@1  97.66 ( 97.44)	Acc@5 100.00 ( 99.95)
Epoch: [78][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2793e-01 (1.2673e-01)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.94)
Epoch: [78][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6260e-01 (1.2659e-01)	Acc@1  96.09 ( 97.47)	Acc@5 100.00 ( 99.95)
Epoch: [78][260/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0236e-01 (1.2702e-01)	Acc@1 100.00 ( 97.45)	Acc@5 100.00 ( 99.95)
Epoch: [78][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3513e-01 (1.2718e-01)	Acc@1  98.44 ( 97.46)	Acc@5 100.00 ( 99.95)
Epoch: [78][280/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1652e-01 (1.2740e-01)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.95)
Epoch: [78][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5015e-01 (1.2761e-01)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.95)
Epoch: [78][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5344e-01 (1.2822e-01)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.95)
Epoch: [78][310/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.2021e-02 (1.2826e-01)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.94)
Epoch: [78][320/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0004e-01 (1.2846e-01)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 ( 99.94)
Epoch: [78][330/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8225e-01 (1.2859e-01)	Acc@1  93.75 ( 97.34)	Acc@5 100.00 ( 99.94)
Epoch: [78][340/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.2224e-02 (1.2868e-01)	Acc@1  99.22 ( 97.32)	Acc@5 100.00 ( 99.94)
Epoch: [78][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1133e-01 (1.2882e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.94)
Epoch: [78][360/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5198e-01 (1.2877e-01)	Acc@1  94.53 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [78][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4160e-01 (1.2912e-01)	Acc@1  96.09 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [78][380/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.7473e-02 (1.2900e-01)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.95)
Epoch: [78][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8799e-01 (1.2895e-01)	Acc@1  95.00 ( 97.32)	Acc@5 100.00 ( 99.95)
## e[78] optimizer.zero_grad (sum) time: 0.533423662185669
## e[78]       loss.backward (sum) time: 12.593576431274414
## e[78]      optimizer.step (sum) time: 25.51555371284485
## epoch[78] training(only) time: 63.316197633743286
# Switched to evaluate mode...
Test: [  0/100]	Time  0.183 ( 0.183)	Loss 1.1865e+00 (1.1865e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.047 ( 0.060)	Loss 1.3750e+00 (1.2649e+00)	Acc@1  65.00 ( 70.45)	Acc@5  91.00 ( 90.18)
Test: [ 20/100]	Time  0.046 ( 0.053)	Loss 1.2100e+00 (1.2354e+00)	Acc@1  68.00 ( 70.86)	Acc@5  92.00 ( 91.05)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.4531e+00 (1.2672e+00)	Acc@1  64.00 ( 70.39)	Acc@5  89.00 ( 90.81)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 1.0137e+00 (1.2514e+00)	Acc@1  77.00 ( 70.44)	Acc@5  92.00 ( 91.15)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2578e+00 (1.2549e+00)	Acc@1  74.00 ( 70.53)	Acc@5  92.00 ( 91.06)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.2129e+00 (1.2466e+00)	Acc@1  66.00 ( 70.36)	Acc@5  90.00 ( 91.15)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.3594e+00 (1.2431e+00)	Acc@1  64.00 ( 70.48)	Acc@5  92.00 ( 91.21)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6055e+00 (1.2437e+00)	Acc@1  72.00 ( 70.56)	Acc@5  86.00 ( 91.17)
Test: [ 90/100]	Time  0.047 ( 0.048)	Loss 1.3262e+00 (1.2227e+00)	Acc@1  69.00 ( 70.82)	Acc@5  90.00 ( 91.32)
 * Acc@1 71.060 Acc@5 91.390
### epoch[78] execution time: 68.22193336486816
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.334 ( 0.334)	Data  0.160 ( 0.160)	Loss 1.4099e-01 (1.4099e-01)	Acc@1  96.09 ( 96.09)	Acc@5  99.22 ( 99.22)
Epoch: [79][ 10/391]	Time  0.162 ( 0.178)	Data  0.001 ( 0.016)	Loss 1.2659e-01 (1.2836e-01)	Acc@1  96.09 ( 97.44)	Acc@5 100.00 ( 99.93)
Epoch: [79][ 20/391]	Time  0.159 ( 0.170)	Data  0.001 ( 0.009)	Loss 1.1847e-01 (1.2590e-01)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 30/391]	Time  0.165 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.3062e-01 (1.2419e-01)	Acc@1  98.44 ( 97.56)	Acc@5  99.22 ( 99.95)
Epoch: [79][ 40/391]	Time  0.168 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.6333e-01 (1.2644e-01)	Acc@1  96.09 ( 97.50)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 50/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.004)	Loss 1.1212e-01 (1.2607e-01)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.94)
Epoch: [79][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.2482e-01 (1.2360e-01)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.95)
Epoch: [79][ 70/391]	Time  0.164 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.4966e-01 (1.2153e-01)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.96)
Epoch: [79][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1877e-01 (1.2254e-01)	Acc@1  95.31 ( 97.59)	Acc@5 100.00 ( 99.95)
Epoch: [79][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 2.0642e-01 (1.2427e-01)	Acc@1  93.75 ( 97.54)	Acc@5 100.00 ( 99.95)
Epoch: [79][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5344e-01 (1.2419e-01)	Acc@1  95.31 ( 97.51)	Acc@5 100.00 ( 99.95)
Epoch: [79][110/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0901e-01 (1.2530e-01)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.96)
Epoch: [79][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1078e-01 (1.2625e-01)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.96)
Epoch: [79][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1438e-01 (1.2616e-01)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [79][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.2041e-02 (1.2569e-01)	Acc@1  99.22 ( 97.48)	Acc@5 100.00 ( 99.96)
Epoch: [79][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4661e-01 (1.2534e-01)	Acc@1  99.22 ( 97.52)	Acc@5  99.22 ( 99.96)
Epoch: [79][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4148e-01 (1.2553e-01)	Acc@1  96.09 ( 97.50)	Acc@5 100.00 ( 99.96)
Epoch: [79][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4502e-01 (1.2602e-01)	Acc@1  96.09 ( 97.47)	Acc@5 100.00 ( 99.96)
Epoch: [79][180/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5833e-01 (1.2682e-01)	Acc@1  96.88 ( 97.45)	Acc@5 100.00 ( 99.95)
Epoch: [79][190/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4543e-02 (1.2713e-01)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 ( 99.95)
Epoch: [79][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1981e-01 (1.2756e-01)	Acc@1  96.88 ( 97.43)	Acc@5 100.00 ( 99.95)
Epoch: [79][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2299e-01 (1.2788e-01)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.95)
Epoch: [79][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0638e-01 (1.2733e-01)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.95)
Epoch: [79][230/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4551e-01 (1.2757e-01)	Acc@1  95.31 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [79][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.7209e-02 (1.2755e-01)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [79][250/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0974e-01 (1.2782e-01)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [79][260/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0968e-01 (1.2796e-01)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.96)
Epoch: [79][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5186e-01 (1.2811e-01)	Acc@1  95.31 ( 97.43)	Acc@5 100.00 ( 99.96)
Epoch: [79][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1084e-01 (1.2809e-01)	Acc@1  99.22 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [79][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4587e-01 (1.2878e-01)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.96)
Epoch: [79][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5356e-01 (1.2926e-01)	Acc@1  96.88 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [79][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3684e-01 (1.2897e-01)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.96)
Epoch: [79][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2769e-01 (1.2919e-01)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [79][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0400e-01 (1.2911e-01)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [79][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 2.1082e-01 (1.2937e-01)	Acc@1  92.97 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [79][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2048e-01 (1.2948e-01)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [79][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6077e-01 (1.2978e-01)	Acc@1  96.09 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [79][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2646e-01 (1.2940e-01)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [79][380/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5112e-01 (1.2920e-01)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [79][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7456e-01 (1.2881e-01)	Acc@1  96.25 ( 97.40)	Acc@5 100.00 ( 99.96)
## e[79] optimizer.zero_grad (sum) time: 0.5422859191894531
## e[79]       loss.backward (sum) time: 12.625900506973267
## e[79]      optimizer.step (sum) time: 25.47169589996338
## epoch[79] training(only) time: 63.450318574905396
# Switched to evaluate mode...
Test: [  0/100]	Time  0.187 ( 0.187)	Loss 1.1768e+00 (1.1768e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.3467e+00 (1.2542e+00)	Acc@1  65.00 ( 70.36)	Acc@5  90.00 ( 89.82)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.1875e+00 (1.2289e+00)	Acc@1  71.00 ( 70.81)	Acc@5  92.00 ( 90.90)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4336e+00 (1.2627e+00)	Acc@1  64.00 ( 70.23)	Acc@5  89.00 ( 90.68)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 1.0039e+00 (1.2452e+00)	Acc@1  76.00 ( 70.37)	Acc@5  92.00 ( 91.12)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2480e+00 (1.2497e+00)	Acc@1  73.00 ( 70.39)	Acc@5  90.00 ( 91.06)
Test: [ 60/100]	Time  0.055 ( 0.050)	Loss 1.2051e+00 (1.2404e+00)	Acc@1  67.00 ( 70.39)	Acc@5  90.00 ( 91.15)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.3857e+00 (1.2381e+00)	Acc@1  64.00 ( 70.45)	Acc@5  93.00 ( 91.24)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5908e+00 (1.2381e+00)	Acc@1  71.00 ( 70.47)	Acc@5  86.00 ( 91.20)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.2695e+00 (1.2177e+00)	Acc@1  70.00 ( 70.74)	Acc@5  90.00 ( 91.34)
 * Acc@1 70.980 Acc@5 91.440
### epoch[79] execution time: 68.41092491149902
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.321 ( 0.321)	Data  0.146 ( 0.146)	Loss 1.0974e-01 (1.0974e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.014)	Loss 1.1279e-01 (1.3385e-01)	Acc@1  99.22 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.3293e-01 (1.3683e-01)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.1786e-01 (1.3236e-01)	Acc@1  98.44 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [80][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.3208e-01 (1.3163e-01)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.94)
Epoch: [80][ 50/391]	Time  0.159 ( 0.164)	Data  0.001 ( 0.004)	Loss 9.3201e-02 (1.3005e-01)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.94)
Epoch: [80][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0992e-01 (1.2765e-01)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [80][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6370e-01 (1.2798e-01)	Acc@1  94.53 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [80][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1029e-01 (1.2813e-01)	Acc@1  99.22 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [80][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4160e-01 (1.2808e-01)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.97)
Epoch: [80][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.8872e-01 (1.3031e-01)	Acc@1  93.75 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [80][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4246e-01 (1.3075e-01)	Acc@1  96.88 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [80][120/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6772e-01 (1.3105e-01)	Acc@1  96.09 ( 97.40)	Acc@5  99.22 ( 99.96)
Epoch: [80][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.0881e-02 (1.3045e-01)	Acc@1  99.22 ( 97.41)	Acc@5 100.00 ( 99.96)
Epoch: [80][140/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2323e-01 (1.3047e-01)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [80][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.9792e-02 (1.3033e-01)	Acc@1  99.22 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [80][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3391e-01 (1.3033e-01)	Acc@1 100.00 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [80][170/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3135e-01 (1.3029e-01)	Acc@1  96.09 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [80][180/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5784e-01 (1.3089e-01)	Acc@1  96.88 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [80][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3782e-01 (1.3049e-01)	Acc@1  99.22 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [80][200/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7896e-01 (1.3133e-01)	Acc@1  96.09 ( 97.35)	Acc@5 100.00 ( 99.96)
Epoch: [80][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2769e-01 (1.3146e-01)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [80][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0718e-01 (1.3141e-01)	Acc@1  99.22 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [80][230/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1145e-01 (1.3086e-01)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [80][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0907e-01 (1.3044e-01)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [80][250/391]	Time  0.171 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1536e-01 (1.3045e-01)	Acc@1 100.00 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [80][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2720e-01 (1.3030e-01)	Acc@1  98.44 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [80][270/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.1920e-01 (1.2992e-01)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [80][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2634e-01 (1.2993e-01)	Acc@1  96.09 ( 97.36)	Acc@5 100.00 ( 99.96)
Epoch: [80][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1157e-01 (1.2964e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [80][300/391]	Time  0.180 ( 0.163)	Data  0.001 ( 0.001)	Loss 9.5520e-02 (1.2917e-01)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [80][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7163e-01 (1.2905e-01)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.96)
Epoch: [80][320/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3232e-01 (1.2878e-01)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.96)
Epoch: [80][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0938e-01 (1.2874e-01)	Acc@1  97.66 ( 97.42)	Acc@5 100.00 ( 99.96)
Epoch: [80][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.0759e-02 (1.2845e-01)	Acc@1  99.22 ( 97.43)	Acc@5 100.00 ( 99.95)
Epoch: [80][350/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3416e-01 (1.2841e-01)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.95)
Epoch: [80][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1707e-01 (1.2810e-01)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.95)
Epoch: [80][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2225e-01 (1.2813e-01)	Acc@1  96.09 ( 97.42)	Acc@5 100.00 ( 99.95)
Epoch: [80][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 2.1509e-01 (1.2884e-01)	Acc@1  90.62 ( 97.39)	Acc@5 100.00 ( 99.95)
Epoch: [80][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4355e-01 (1.2897e-01)	Acc@1  98.75 ( 97.39)	Acc@5 100.00 ( 99.95)
## e[80] optimizer.zero_grad (sum) time: 0.5509850978851318
## e[80]       loss.backward (sum) time: 12.602162837982178
## e[80]      optimizer.step (sum) time: 25.508689641952515
## epoch[80] training(only) time: 63.51806139945984
# Switched to evaluate mode...
Test: [  0/100]	Time  0.191 ( 0.191)	Loss 1.2021e+00 (1.2021e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.048 ( 0.061)	Loss 1.3486e+00 (1.2552e+00)	Acc@1  66.00 ( 70.27)	Acc@5  91.00 ( 90.45)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.1982e+00 (1.2307e+00)	Acc@1  70.00 ( 70.86)	Acc@5  92.00 ( 91.24)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4365e+00 (1.2641e+00)	Acc@1  66.00 ( 70.19)	Acc@5  87.00 ( 90.74)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 9.9902e-01 (1.2465e+00)	Acc@1  76.00 ( 70.41)	Acc@5  92.00 ( 91.17)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2520e+00 (1.2489e+00)	Acc@1  72.00 ( 70.49)	Acc@5  92.00 ( 91.06)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.1992e+00 (1.2400e+00)	Acc@1  67.00 ( 70.41)	Acc@5  91.00 ( 91.16)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.4375e+00 (1.2393e+00)	Acc@1  64.00 ( 70.45)	Acc@5  91.00 ( 91.27)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.5938e+00 (1.2396e+00)	Acc@1  71.00 ( 70.49)	Acc@5  86.00 ( 91.23)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.2891e+00 (1.2192e+00)	Acc@1  70.00 ( 70.80)	Acc@5  90.00 ( 91.36)
 * Acc@1 71.040 Acc@5 91.450
### epoch[80] execution time: 68.47148275375366
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.316 ( 0.316)	Data  0.148 ( 0.148)	Loss 1.2732e-01 (1.2732e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.8274e-01 (1.3883e-01)	Acc@1  96.09 ( 97.30)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 7.8735e-02 (1.3315e-01)	Acc@1  99.22 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [81][ 30/391]	Time  0.172 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.1884e-01 (1.3506e-01)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [81][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.3330e-01 (1.3229e-01)	Acc@1  96.88 ( 97.28)	Acc@5  99.22 ( 99.94)
Epoch: [81][ 50/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.1664e-01 (1.3146e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.95)
Epoch: [81][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2256e-01 (1.3202e-01)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [81][ 70/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1804e-01 (1.3085e-01)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [81][ 80/391]	Time  0.176 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.6304e-02 (1.3065e-01)	Acc@1  99.22 ( 97.34)	Acc@5 100.00 ( 99.96)
Epoch: [81][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.5820e-01 (1.3112e-01)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.97)
Epoch: [81][100/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.7595e-02 (1.2966e-01)	Acc@1 100.00 ( 97.41)	Acc@5 100.00 ( 99.97)
Epoch: [81][110/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5051e-01 (1.2894e-01)	Acc@1  96.88 ( 97.44)	Acc@5  99.22 ( 99.96)
Epoch: [81][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3171e-01 (1.2908e-01)	Acc@1  99.22 ( 97.45)	Acc@5 100.00 ( 99.96)
Epoch: [81][130/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0437e-01 (1.2854e-01)	Acc@1  97.66 ( 97.45)	Acc@5  99.22 ( 99.96)
Epoch: [81][140/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.5825e-02 (1.2806e-01)	Acc@1  98.44 ( 97.45)	Acc@5 100.00 ( 99.96)
Epoch: [81][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4868e-01 (1.2867e-01)	Acc@1  96.09 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [81][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1774e-01 (1.2853e-01)	Acc@1  97.66 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [81][170/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1163e-01 (1.2913e-01)	Acc@1  97.66 ( 97.35)	Acc@5 100.00 ( 99.95)
Epoch: [81][180/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5356e-01 (1.2854e-01)	Acc@1  95.31 ( 97.41)	Acc@5 100.00 ( 99.95)
Epoch: [81][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2976e-01 (1.2877e-01)	Acc@1  98.44 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [81][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0632e-01 (1.2937e-01)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.96)
Epoch: [81][210/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4661e-01 (1.2974e-01)	Acc@1  97.66 ( 97.39)	Acc@5 100.00 ( 99.96)
Epoch: [81][220/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4185e-01 (1.3039e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.95)
Epoch: [81][230/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6089e-01 (1.2962e-01)	Acc@1  95.31 ( 97.40)	Acc@5 100.00 ( 99.96)
Epoch: [81][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1169e-01 (1.2900e-01)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.96)
Epoch: [81][250/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.8501e-02 (1.2922e-01)	Acc@1  99.22 ( 97.41)	Acc@5 100.00 ( 99.95)
Epoch: [81][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4624e-01 (1.2901e-01)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.96)
Epoch: [81][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1182e-01 (1.2880e-01)	Acc@1  98.44 ( 97.44)	Acc@5 100.00 ( 99.95)
Epoch: [81][280/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4868e-01 (1.2862e-01)	Acc@1  96.88 ( 97.43)	Acc@5 100.00 ( 99.95)
Epoch: [81][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5234e-01 (1.2867e-01)	Acc@1  96.09 ( 97.44)	Acc@5 100.00 ( 99.95)
Epoch: [81][300/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4807e-01 (1.2880e-01)	Acc@1  96.09 ( 97.45)	Acc@5 100.00 ( 99.95)
Epoch: [81][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 7.0923e-02 (1.2861e-01)	Acc@1  99.22 ( 97.45)	Acc@5 100.00 ( 99.95)
Epoch: [81][320/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7480e-01 (1.2886e-01)	Acc@1  96.09 ( 97.45)	Acc@5  99.22 ( 99.95)
Epoch: [81][330/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0211e-01 (1.2854e-01)	Acc@1 100.00 ( 97.48)	Acc@5 100.00 ( 99.95)
Epoch: [81][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2598e-01 (1.2823e-01)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.95)
Epoch: [81][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.9539e-02 (1.2797e-01)	Acc@1  99.22 ( 97.48)	Acc@5 100.00 ( 99.95)
Epoch: [81][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1432e-01 (1.2800e-01)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.95)
Epoch: [81][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1377e-01 (1.2791e-01)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.95)
Epoch: [81][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2189e-01 (1.2785e-01)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.95)
Epoch: [81][390/391]	Time  0.117 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.7834e-01 (1.2820e-01)	Acc@1  93.75 ( 97.46)	Acc@5 100.00 ( 99.95)
## e[81] optimizer.zero_grad (sum) time: 0.5394854545593262
## e[81]       loss.backward (sum) time: 12.604191064834595
## e[81]      optimizer.step (sum) time: 25.517826795578003
## epoch[81] training(only) time: 63.46992325782776
# Switched to evaluate mode...
Test: [  0/100]	Time  0.182 ( 0.182)	Loss 1.1836e+00 (1.1836e+00)	Acc@1  72.00 ( 72.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.052 ( 0.060)	Loss 1.3604e+00 (1.2549e+00)	Acc@1  65.00 ( 70.36)	Acc@5  91.00 ( 90.36)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.2148e+00 (1.2327e+00)	Acc@1  71.00 ( 71.14)	Acc@5  92.00 ( 91.14)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4336e+00 (1.2673e+00)	Acc@1  65.00 ( 70.61)	Acc@5  87.00 ( 90.68)
Test: [ 40/100]	Time  0.048 ( 0.050)	Loss 1.0146e+00 (1.2491e+00)	Acc@1  76.00 ( 70.61)	Acc@5  92.00 ( 91.05)
Test: [ 50/100]	Time  0.048 ( 0.050)	Loss 1.2471e+00 (1.2531e+00)	Acc@1  74.00 ( 70.67)	Acc@5  91.00 ( 90.94)
Test: [ 60/100]	Time  0.048 ( 0.050)	Loss 1.2119e+00 (1.2436e+00)	Acc@1  68.00 ( 70.62)	Acc@5  89.00 ( 91.05)
Test: [ 70/100]	Time  0.048 ( 0.050)	Loss 1.3926e+00 (1.2416e+00)	Acc@1  64.00 ( 70.68)	Acc@5  93.00 ( 91.18)
Test: [ 80/100]	Time  0.048 ( 0.050)	Loss 1.5898e+00 (1.2421e+00)	Acc@1  70.00 ( 70.68)	Acc@5  86.00 ( 91.15)
Test: [ 90/100]	Time  0.056 ( 0.050)	Loss 1.2998e+00 (1.2222e+00)	Acc@1  69.00 ( 70.89)	Acc@5  90.00 ( 91.32)
 * Acc@1 71.120 Acc@5 91.380
### epoch[81] execution time: 68.49542737007141
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.334 ( 0.334)	Data  0.158 ( 0.158)	Loss 1.9690e-01 (1.9690e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.6626e-01 (1.3240e-01)	Acc@1  97.66 ( 96.80)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.009)	Loss 1.3159e-01 (1.2658e-01)	Acc@1  95.31 ( 97.06)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 9.3628e-02 (1.2704e-01)	Acc@1  98.44 ( 97.25)	Acc@5 100.00 ( 99.97)
Epoch: [82][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.0797e-01 (1.2459e-01)	Acc@1  99.22 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [82][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.6858e-01 (1.2784e-01)	Acc@1  95.31 ( 97.35)	Acc@5 100.00 ( 99.97)
Epoch: [82][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.3611e-01 (1.2567e-01)	Acc@1  96.09 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [82][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2866e-01 (1.2560e-01)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [82][ 80/391]	Time  0.173 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.6357e-01 (1.2549e-01)	Acc@1  94.53 ( 97.41)	Acc@5 100.00 ( 99.97)
Epoch: [82][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1066e-01 (1.2568e-01)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [82][100/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4001e-01 (1.2609e-01)	Acc@1  95.31 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [82][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3867e-01 (1.2682e-01)	Acc@1  96.88 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [82][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7896e-01 (1.2627e-01)	Acc@1  92.19 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [82][130/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7639e-01 (1.2647e-01)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.98)
Epoch: [82][140/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7603e-01 (1.2613e-01)	Acc@1  96.09 ( 97.38)	Acc@5 100.00 ( 99.98)
Epoch: [82][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0444e-02 (1.2681e-01)	Acc@1 100.00 ( 97.38)	Acc@5 100.00 ( 99.97)
Epoch: [82][160/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2146e-01 (1.2713e-01)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [82][170/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.8938e-02 (1.2634e-01)	Acc@1  96.88 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [82][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5283e-01 (1.2632e-01)	Acc@1  96.88 ( 97.41)	Acc@5 100.00 ( 99.97)
Epoch: [82][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0602e-01 (1.2624e-01)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.97)
Epoch: [82][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0077e-01 (1.2572e-01)	Acc@1  99.22 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [82][210/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5320e-01 (1.2554e-01)	Acc@1  96.88 ( 97.44)	Acc@5  99.22 ( 99.97)
Epoch: [82][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.0149e-02 (1.2532e-01)	Acc@1  99.22 ( 97.45)	Acc@5 100.00 ( 99.96)
Epoch: [82][230/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0339e-01 (1.2573e-01)	Acc@1 100.00 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [82][240/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1829e-01 (1.2564e-01)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [82][250/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5857e-01 (1.2498e-01)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [82][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2964e-01 (1.2507e-01)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [82][270/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5771e-01 (1.2568e-01)	Acc@1  96.09 ( 97.44)	Acc@5 100.00 ( 99.97)
Epoch: [82][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4270e-01 (1.2571e-01)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [82][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3843e-01 (1.2631e-01)	Acc@1  96.09 ( 97.41)	Acc@5 100.00 ( 99.97)
Epoch: [82][300/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2952e-01 (1.2654e-01)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.97)
Epoch: [82][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4685e-01 (1.2670e-01)	Acc@1  95.31 ( 97.39)	Acc@5 100.00 ( 99.97)
Epoch: [82][320/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2445e-01 (1.2636e-01)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.97)
Epoch: [82][330/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.1981e-01 (1.2624e-01)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.97)
Epoch: [82][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.6479e-01 (1.2631e-01)	Acc@1  96.09 ( 97.42)	Acc@5 100.00 ( 99.97)
Epoch: [82][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.4534e-02 (1.2658e-01)	Acc@1  99.22 ( 97.41)	Acc@5 100.00 ( 99.97)
Epoch: [82][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.8755e-02 (1.2631e-01)	Acc@1 100.00 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [82][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.2834e-02 (1.2662e-01)	Acc@1  98.44 ( 97.43)	Acc@5 100.00 ( 99.97)
Epoch: [82][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.2825e-02 (1.2616e-01)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [82][390/391]	Time  0.136 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8359e-01 (1.2593e-01)	Acc@1  95.00 ( 97.46)	Acc@5 100.00 ( 99.97)
## e[82] optimizer.zero_grad (sum) time: 0.5518238544464111
## e[82]       loss.backward (sum) time: 12.685302019119263
## e[82]      optimizer.step (sum) time: 25.42846941947937
## epoch[82] training(only) time: 63.5594539642334
# Switched to evaluate mode...
Test: [  0/100]	Time  0.184 ( 0.184)	Loss 1.1621e+00 (1.1621e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.059)	Loss 1.3564e+00 (1.2509e+00)	Acc@1  65.00 ( 70.64)	Acc@5  91.00 ( 90.36)
Test: [ 20/100]	Time  0.060 ( 0.054)	Loss 1.2344e+00 (1.2317e+00)	Acc@1  67.00 ( 70.76)	Acc@5  92.00 ( 91.33)
Test: [ 30/100]	Time  0.046 ( 0.051)	Loss 1.4580e+00 (1.2664e+00)	Acc@1  67.00 ( 70.45)	Acc@5  89.00 ( 90.90)
Test: [ 40/100]	Time  0.046 ( 0.050)	Loss 9.9414e-01 (1.2499e+00)	Acc@1  74.00 ( 70.20)	Acc@5  92.00 ( 91.17)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2588e+00 (1.2530e+00)	Acc@1  74.00 ( 70.37)	Acc@5  91.00 ( 91.04)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.1973e+00 (1.2445e+00)	Acc@1  67.00 ( 70.31)	Acc@5  89.00 ( 91.10)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3711e+00 (1.2429e+00)	Acc@1  64.00 ( 70.35)	Acc@5  93.00 ( 91.23)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.5840e+00 (1.2433e+00)	Acc@1  69.00 ( 70.30)	Acc@5  86.00 ( 91.20)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.3193e+00 (1.2235e+00)	Acc@1  69.00 ( 70.53)	Acc@5  90.00 ( 91.30)
 * Acc@1 70.770 Acc@5 91.350
### epoch[82] execution time: 68.49975657463074
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.317 ( 0.317)	Data  0.149 ( 0.149)	Loss 1.3904e-01 (1.3904e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.015)	Loss 1.0541e-01 (1.1820e-01)	Acc@1  98.44 ( 97.94)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.172 ( 0.169)	Data  0.001 ( 0.008)	Loss 9.4299e-02 (1.1587e-01)	Acc@1  99.22 ( 98.07)	Acc@5 100.00 ( 99.96)
Epoch: [83][ 30/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.6199e-01 (1.2320e-01)	Acc@1  95.31 ( 97.86)	Acc@5 100.00 ( 99.97)
Epoch: [83][ 40/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.6992e-01 (1.2411e-01)	Acc@1  96.88 ( 97.85)	Acc@5 100.00 ( 99.98)
Epoch: [83][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.0986e-01 (1.2487e-01)	Acc@1  96.88 ( 97.76)	Acc@5 100.00 ( 99.97)
Epoch: [83][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.4038e-01 (1.2551e-01)	Acc@1  99.22 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [83][ 70/391]	Time  0.179 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3879e-01 (1.2490e-01)	Acc@1  95.31 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [83][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3330e-01 (1.2396e-01)	Acc@1  97.66 ( 97.74)	Acc@5 100.00 ( 99.98)
Epoch: [83][ 90/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.3416e-01 (1.2334e-01)	Acc@1  96.88 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [83][100/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.7341e-02 (1.2310e-01)	Acc@1 100.00 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [83][110/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1066e-01 (1.2379e-01)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.99)
Epoch: [83][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5649e-01 (1.2337e-01)	Acc@1  98.44 ( 97.75)	Acc@5 100.00 ( 99.98)
Epoch: [83][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1542e-01 (1.2263e-01)	Acc@1  97.66 ( 97.76)	Acc@5 100.00 ( 99.98)
Epoch: [83][140/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4604e-02 (1.2300e-01)	Acc@1  98.44 ( 97.76)	Acc@5 100.00 ( 99.98)
Epoch: [83][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.5449e-02 (1.2229e-01)	Acc@1  99.22 ( 97.78)	Acc@5 100.00 ( 99.97)
Epoch: [83][160/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5112e-01 (1.2228e-01)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.98)
Epoch: [83][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4319e-01 (1.2239e-01)	Acc@1  97.66 ( 97.78)	Acc@5 100.00 ( 99.98)
Epoch: [83][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.8684e-02 (1.2268e-01)	Acc@1  99.22 ( 97.74)	Acc@5 100.00 ( 99.98)
Epoch: [83][190/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9031e-01 (1.2360e-01)	Acc@1  96.88 ( 97.72)	Acc@5 100.00 ( 99.98)
Epoch: [83][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3330e-01 (1.2416e-01)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [83][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4648e-01 (1.2440e-01)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [83][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1548e-01 (1.2448e-01)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.98)
Epoch: [83][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.0933e-02 (1.2467e-01)	Acc@1 100.00 ( 97.64)	Acc@5 100.00 ( 99.98)
Epoch: [83][240/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0663e-01 (1.2458e-01)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.97)
Epoch: [83][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6394e-01 (1.2493e-01)	Acc@1  94.53 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [83][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 7.8247e-02 (1.2491e-01)	Acc@1 100.00 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [83][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3647e-01 (1.2497e-01)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [83][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4697e-01 (1.2476e-01)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.98)
Epoch: [83][290/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3318e-01 (1.2493e-01)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.98)
Epoch: [83][300/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.0801e-01 (1.2500e-01)	Acc@1  94.53 ( 97.59)	Acc@5 100.00 ( 99.98)
Epoch: [83][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7493e-01 (1.2539e-01)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.97)
Epoch: [83][320/391]	Time  0.177 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6162e-01 (1.2560e-01)	Acc@1  94.53 ( 97.58)	Acc@5  99.22 ( 99.97)
Epoch: [83][330/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2366e-01 (1.2557e-01)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [83][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5820e-01 (1.2557e-01)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [83][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5234e-01 (1.2570e-01)	Acc@1  96.09 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [83][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.7595e-02 (1.2557e-01)	Acc@1  99.22 ( 97.57)	Acc@5 100.00 ( 99.97)
Epoch: [83][370/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0205e-01 (1.2578e-01)	Acc@1  98.44 ( 97.56)	Acc@5 100.00 ( 99.97)
Epoch: [83][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.9304e-02 (1.2528e-01)	Acc@1  97.66 ( 97.58)	Acc@5 100.00 ( 99.97)
Epoch: [83][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0345e-01 (1.2526e-01)	Acc@1  97.50 ( 97.57)	Acc@5 100.00 ( 99.97)
## e[83] optimizer.zero_grad (sum) time: 0.540743350982666
## e[83]       loss.backward (sum) time: 12.677870273590088
## e[83]      optimizer.step (sum) time: 25.481494188308716
## epoch[83] training(only) time: 63.53241968154907
# Switched to evaluate mode...
Test: [  0/100]	Time  0.193 ( 0.193)	Loss 1.1709e+00 (1.1709e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.3691e+00 (1.2614e+00)	Acc@1  66.00 ( 69.91)	Acc@5  91.00 ( 90.27)
Test: [ 20/100]	Time  0.046 ( 0.054)	Loss 1.2178e+00 (1.2358e+00)	Acc@1  70.00 ( 70.81)	Acc@5  92.00 ( 91.19)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4287e+00 (1.2678e+00)	Acc@1  67.00 ( 70.45)	Acc@5  90.00 ( 90.94)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0127e+00 (1.2507e+00)	Acc@1  77.00 ( 70.51)	Acc@5  92.00 ( 91.24)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2070e+00 (1.2542e+00)	Acc@1  72.00 ( 70.43)	Acc@5  92.00 ( 91.12)
Test: [ 60/100]	Time  0.046 ( 0.049)	Loss 1.2461e+00 (1.2454e+00)	Acc@1  66.00 ( 70.39)	Acc@5  89.00 ( 91.20)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3750e+00 (1.2430e+00)	Acc@1  64.00 ( 70.46)	Acc@5  91.00 ( 91.25)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6055e+00 (1.2443e+00)	Acc@1  73.00 ( 70.49)	Acc@5  86.00 ( 91.19)
Test: [ 90/100]	Time  0.050 ( 0.049)	Loss 1.3184e+00 (1.2246e+00)	Acc@1  69.00 ( 70.80)	Acc@5  90.00 ( 91.33)
 * Acc@1 70.990 Acc@5 91.420
### epoch[83] execution time: 68.48737072944641
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.335 ( 0.335)	Data  0.152 ( 0.152)	Loss 1.4478e-01 (1.4478e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.6577e-01 (1.3595e-01)	Acc@1  96.88 ( 97.02)	Acc@5  99.22 ( 99.93)
Epoch: [84][ 20/391]	Time  0.161 ( 0.170)	Data  0.001 ( 0.008)	Loss 2.0801e-01 (1.3592e-01)	Acc@1  92.97 ( 97.02)	Acc@5 100.00 ( 99.96)
Epoch: [84][ 30/391]	Time  0.170 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.2537e-01 (1.3088e-01)	Acc@1  99.22 ( 97.20)	Acc@5 100.00 ( 99.97)
Epoch: [84][ 40/391]	Time  0.162 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.1987e-01 (1.2732e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 ( 99.96)
Epoch: [84][ 50/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.004)	Loss 1.0217e-01 (1.2406e-01)	Acc@1  97.66 ( 97.43)	Acc@5 100.00 ( 99.95)
Epoch: [84][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.1340e-01 (1.2319e-01)	Acc@1  99.22 ( 97.55)	Acc@5 100.00 ( 99.95)
Epoch: [84][ 70/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.003)	Loss 1.5247e-01 (1.2184e-01)	Acc@1  96.88 ( 97.71)	Acc@5 100.00 ( 99.94)
Epoch: [84][ 80/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1353e-01 (1.2383e-01)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.94)
Epoch: [84][ 90/391]	Time  0.163 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2622e-01 (1.2527e-01)	Acc@1  97.66 ( 97.57)	Acc@5 100.00 ( 99.94)
Epoch: [84][100/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2866e-01 (1.2554e-01)	Acc@1  96.88 ( 97.57)	Acc@5 100.00 ( 99.95)
Epoch: [84][110/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.002)	Loss 8.4778e-02 (1.2555e-01)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.95)
Epoch: [84][120/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1285e-01 (1.2554e-01)	Acc@1  98.44 ( 97.55)	Acc@5 100.00 ( 99.95)
Epoch: [84][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5125e-01 (1.2624e-01)	Acc@1  96.09 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [84][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3672e-01 (1.2619e-01)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [84][150/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6985e-02 (1.2524e-01)	Acc@1  98.44 ( 97.60)	Acc@5 100.00 ( 99.95)
Epoch: [84][160/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2415e-01 (1.2550e-01)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.96)
Epoch: [84][170/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4978e-01 (1.2477e-01)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [84][180/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0999e-01 (1.2473e-01)	Acc@1  98.44 ( 97.57)	Acc@5 100.00 ( 99.96)
Epoch: [84][190/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.9360e-01 (1.2495e-01)	Acc@1  95.31 ( 97.55)	Acc@5 100.00 ( 99.96)
Epoch: [84][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1975e-01 (1.2499e-01)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.96)
Epoch: [84][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0382e-01 (1.2562e-01)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [84][220/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0010e-01 (1.2563e-01)	Acc@1  98.44 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [84][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4673e-01 (1.2544e-01)	Acc@1  95.31 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [84][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 2.1155e-01 (1.2570e-01)	Acc@1  95.31 ( 97.50)	Acc@5  99.22 ( 99.96)
Epoch: [84][250/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2427e-01 (1.2538e-01)	Acc@1  96.09 ( 97.50)	Acc@5 100.00 ( 99.96)
Epoch: [84][260/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2708e-01 (1.2544e-01)	Acc@1  97.66 ( 97.50)	Acc@5 100.00 ( 99.96)
Epoch: [84][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1279e-01 (1.2544e-01)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.96)
Epoch: [84][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0773e-01 (1.2472e-01)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 ( 99.96)
Epoch: [84][290/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.9060e-02 (1.2499e-01)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 ( 99.96)
Epoch: [84][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.1614e-02 (1.2519e-01)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 ( 99.96)
Epoch: [84][310/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.4788e-02 (1.2516e-01)	Acc@1  99.22 ( 97.51)	Acc@5 100.00 ( 99.96)
Epoch: [84][320/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6650e-01 (1.2516e-01)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 ( 99.96)
Epoch: [84][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2103e-01 (1.2498e-01)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 ( 99.96)
Epoch: [84][340/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2390e-01 (1.2532e-01)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.96)
Epoch: [84][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4453e-01 (1.2498e-01)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.96)
Epoch: [84][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.4055e-02 (1.2478e-01)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.97)
Epoch: [84][370/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.5947e-02 (1.2443e-01)	Acc@1  98.44 ( 97.52)	Acc@5 100.00 ( 99.97)
Epoch: [84][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7310e-01 (1.2462e-01)	Acc@1  96.09 ( 97.51)	Acc@5  99.22 ( 99.97)
Epoch: [84][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1871e-01 (1.2466e-01)	Acc@1  97.50 ( 97.50)	Acc@5 100.00 ( 99.97)
## e[84] optimizer.zero_grad (sum) time: 0.5491249561309814
## e[84]       loss.backward (sum) time: 12.632062435150146
## e[84]      optimizer.step (sum) time: 25.507057905197144
## epoch[84] training(only) time: 63.5226263999939
# Switched to evaluate mode...
Test: [  0/100]	Time  0.209 ( 0.209)	Loss 1.1846e+00 (1.1846e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.3506e+00 (1.2574e+00)	Acc@1  64.00 ( 70.27)	Acc@5  91.00 ( 90.27)
Test: [ 20/100]	Time  0.046 ( 0.055)	Loss 1.2471e+00 (1.2397e+00)	Acc@1  69.00 ( 70.81)	Acc@5  92.00 ( 91.14)
Test: [ 30/100]	Time  0.056 ( 0.052)	Loss 1.4844e+00 (1.2733e+00)	Acc@1  65.00 ( 70.26)	Acc@5  89.00 ( 90.77)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 9.9609e-01 (1.2539e+00)	Acc@1  76.00 ( 70.46)	Acc@5  92.00 ( 91.12)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2471e+00 (1.2557e+00)	Acc@1  74.00 ( 70.51)	Acc@5  93.00 ( 91.02)
Test: [ 60/100]	Time  0.046 ( 0.050)	Loss 1.2178e+00 (1.2453e+00)	Acc@1  68.00 ( 70.49)	Acc@5  90.00 ( 91.13)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3828e+00 (1.2440e+00)	Acc@1  64.00 ( 70.52)	Acc@5  92.00 ( 91.17)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.6133e+00 (1.2445e+00)	Acc@1  69.00 ( 70.54)	Acc@5  86.00 ( 91.11)
Test: [ 90/100]	Time  0.047 ( 0.049)	Loss 1.2900e+00 (1.2239e+00)	Acc@1  69.00 ( 70.82)	Acc@5  91.00 ( 91.25)
 * Acc@1 71.050 Acc@5 91.350
### epoch[84] execution time: 68.48997974395752
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.321 ( 0.321)	Data  0.149 ( 0.149)	Loss 1.7004e-01 (1.7004e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.160 ( 0.175)	Data  0.001 ( 0.014)	Loss 1.3843e-01 (1.2957e-01)	Acc@1  97.66 ( 97.16)	Acc@5 100.00 ( 99.86)
Epoch: [85][ 20/391]	Time  0.161 ( 0.168)	Data  0.001 ( 0.008)	Loss 1.3562e-01 (1.3049e-01)	Acc@1  98.44 ( 97.17)	Acc@5 100.00 ( 99.93)
Epoch: [85][ 30/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.006)	Loss 8.1238e-02 (1.2544e-01)	Acc@1  98.44 ( 97.40)	Acc@5 100.00 ( 99.95)
Epoch: [85][ 40/391]	Time  0.162 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.2891e-01 (1.2581e-01)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.96)
Epoch: [85][ 50/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.5210e-01 (1.2922e-01)	Acc@1  95.31 ( 97.37)	Acc@5 100.00 ( 99.97)
Epoch: [85][ 60/391]	Time  0.171 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2732e-01 (1.3036e-01)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.97)
Epoch: [85][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2781e-01 (1.3003e-01)	Acc@1  96.09 ( 97.37)	Acc@5 100.00 ( 99.98)
Epoch: [85][ 80/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0278e-01 (1.2850e-01)	Acc@1  98.44 ( 97.35)	Acc@5 100.00 ( 99.98)
Epoch: [85][ 90/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0767e-01 (1.2713e-01)	Acc@1  98.44 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [85][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1066e-01 (1.2658e-01)	Acc@1  96.88 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [85][110/391]	Time  0.172 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1249e-01 (1.2632e-01)	Acc@1  99.22 ( 97.42)	Acc@5 100.00 ( 99.98)
Epoch: [85][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3257e-01 (1.2817e-01)	Acc@1  97.66 ( 97.32)	Acc@5  99.22 ( 99.97)
Epoch: [85][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1108e-01 (1.2845e-01)	Acc@1  97.66 ( 97.36)	Acc@5 100.00 ( 99.97)
Epoch: [85][140/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6907e-01 (1.2906e-01)	Acc@1  94.53 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [85][150/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0175e-01 (1.2867e-01)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.97)
Epoch: [85][160/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6113e-01 (1.2968e-01)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [85][170/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2115e-01 (1.2931e-01)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [85][180/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2622e-01 (1.2865e-01)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.97)
Epoch: [85][190/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4270e-01 (1.2947e-01)	Acc@1  97.66 ( 97.31)	Acc@5 100.00 ( 99.97)
Epoch: [85][200/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4331e-01 (1.2981e-01)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.97)
Epoch: [85][210/391]	Time  0.178 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4636e-01 (1.2939e-01)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.97)
Epoch: [85][220/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0522e-01 (1.2938e-01)	Acc@1  99.22 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [85][230/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3037e-01 (1.2926e-01)	Acc@1  96.09 ( 97.28)	Acc@5 100.00 ( 99.98)
Epoch: [85][240/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5979e-01 (1.2878e-01)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.98)
Epoch: [85][250/391]	Time  0.163 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7957e-01 (1.2881e-01)	Acc@1  98.44 ( 97.30)	Acc@5 100.00 ( 99.98)
Epoch: [85][260/391]	Time  0.173 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1615e-01 (1.2840e-01)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [85][270/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0980e-01 (1.2867e-01)	Acc@1  99.22 ( 97.30)	Acc@5 100.00 ( 99.98)
Epoch: [85][280/391]	Time  0.167 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1597e-01 (1.2848e-01)	Acc@1  98.44 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [85][290/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.9731e-02 (1.2827e-01)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [85][300/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1163e-01 (1.2821e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [85][310/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6284e-01 (1.2850e-01)	Acc@1  96.88 ( 97.30)	Acc@5  99.22 ( 99.97)
Epoch: [85][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1188e-01 (1.2828e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.98)
Epoch: [85][330/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3708e-01 (1.2815e-01)	Acc@1  96.88 ( 97.34)	Acc@5 100.00 ( 99.98)
Epoch: [85][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.7830e-02 (1.2794e-01)	Acc@1  98.44 ( 97.34)	Acc@5 100.00 ( 99.98)
Epoch: [85][350/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3428e-01 (1.2805e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [85][360/391]	Time  0.169 ( 0.162)	Data  0.001 ( 0.001)	Loss 6.4575e-02 (1.2783e-01)	Acc@1  99.22 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [85][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2048e-01 (1.2760e-01)	Acc@1  96.88 ( 97.35)	Acc@5 100.00 ( 99.98)
Epoch: [85][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5662e-01 (1.2786e-01)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.98)
Epoch: [85][390/391]	Time  0.114 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5381e-01 (1.2762e-01)	Acc@1  93.75 ( 97.34)	Acc@5 100.00 ( 99.98)
## e[85] optimizer.zero_grad (sum) time: 0.5362510681152344
## e[85]       loss.backward (sum) time: 12.666553974151611
## e[85]      optimizer.step (sum) time: 25.473541021347046
## epoch[85] training(only) time: 63.35176420211792
# Switched to evaluate mode...
Test: [  0/100]	Time  0.177 ( 0.177)	Loss 1.1875e+00 (1.1875e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.060)	Loss 1.3486e+00 (1.2583e+00)	Acc@1  65.00 ( 70.00)	Acc@5  91.00 ( 90.36)
Test: [ 20/100]	Time  0.047 ( 0.053)	Loss 1.2285e+00 (1.2325e+00)	Acc@1  69.00 ( 70.76)	Acc@5  92.00 ( 91.14)
Test: [ 30/100]	Time  0.047 ( 0.051)	Loss 1.4365e+00 (1.2687e+00)	Acc@1  64.00 ( 70.26)	Acc@5  88.00 ( 90.77)
Test: [ 40/100]	Time  0.048 ( 0.050)	Loss 1.0059e+00 (1.2490e+00)	Acc@1  76.00 ( 70.39)	Acc@5  92.00 ( 91.12)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2305e+00 (1.2508e+00)	Acc@1  75.00 ( 70.47)	Acc@5  92.00 ( 90.96)
Test: [ 60/100]	Time  0.047 ( 0.049)	Loss 1.2188e+00 (1.2413e+00)	Acc@1  67.00 ( 70.38)	Acc@5  90.00 ( 91.11)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3887e+00 (1.2386e+00)	Acc@1  64.00 ( 70.45)	Acc@5  92.00 ( 91.21)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.6104e+00 (1.2397e+00)	Acc@1  72.00 ( 70.48)	Acc@5  86.00 ( 91.20)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2900e+00 (1.2191e+00)	Acc@1  70.00 ( 70.75)	Acc@5  90.00 ( 91.42)
 * Acc@1 70.960 Acc@5 91.470
### epoch[85] execution time: 68.32322025299072
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.322 ( 0.322)	Data  0.150 ( 0.150)	Loss 1.7078e-01 (1.7078e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.015)	Loss 1.2939e-01 (1.1589e-01)	Acc@1  96.88 ( 98.15)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.5430e-01 (1.2180e-01)	Acc@1  96.09 ( 97.69)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.5002e-01 (1.2825e-01)	Acc@1  96.09 ( 97.30)	Acc@5 100.00 ( 99.95)
Epoch: [86][ 40/391]	Time  0.160 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.3660e-01 (1.2875e-01)	Acc@1  96.88 ( 97.29)	Acc@5 100.00 ( 99.94)
Epoch: [86][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.1249e-01 (1.2641e-01)	Acc@1  97.66 ( 97.40)	Acc@5 100.00 ( 99.94)
Epoch: [86][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.1865e-01 (1.2681e-01)	Acc@1  97.66 ( 97.41)	Acc@5 100.00 ( 99.92)
Epoch: [86][ 70/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0602e-01 (1.2502e-01)	Acc@1  99.22 ( 97.48)	Acc@5 100.00 ( 99.91)
Epoch: [86][ 80/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.003)	Loss 9.9060e-02 (1.2433e-01)	Acc@1  98.44 ( 97.50)	Acc@5 100.00 ( 99.92)
Epoch: [86][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.2683e-01 (1.2265e-01)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.93)
Epoch: [86][100/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4038e-01 (1.2217e-01)	Acc@1  96.88 ( 97.58)	Acc@5  99.22 ( 99.93)
Epoch: [86][110/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2793e-01 (1.2349e-01)	Acc@1  96.09 ( 97.53)	Acc@5 100.00 ( 99.94)
Epoch: [86][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2537e-01 (1.2392e-01)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 ( 99.94)
Epoch: [86][130/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.6313e-02 (1.2469e-01)	Acc@1  98.44 ( 97.47)	Acc@5 100.00 ( 99.94)
Epoch: [86][140/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0626e-01 (1.2416e-01)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 ( 99.94)
Epoch: [86][150/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2622e-01 (1.2445e-01)	Acc@1  99.22 ( 97.47)	Acc@5 100.00 ( 99.95)
Epoch: [86][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1163e-01 (1.2414e-01)	Acc@1  98.44 ( 97.47)	Acc@5 100.00 ( 99.95)
Epoch: [86][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2031e-02 (1.2335e-01)	Acc@1  99.22 ( 97.47)	Acc@5 100.00 ( 99.95)
Epoch: [86][180/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2244e-01 (1.2362e-01)	Acc@1  97.66 ( 97.47)	Acc@5 100.00 ( 99.95)
Epoch: [86][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.2370e-01)	Acc@1  94.53 ( 97.44)	Acc@5 100.00 ( 99.96)
Epoch: [86][200/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3862e-02 (1.2335e-01)	Acc@1 100.00 ( 97.48)	Acc@5 100.00 ( 99.96)
Epoch: [86][210/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3892e-01 (1.2341e-01)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.96)
Epoch: [86][220/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1896e-01 (1.2346e-01)	Acc@1  98.44 ( 97.48)	Acc@5 100.00 ( 99.96)
Epoch: [86][230/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.4045e-02 (1.2297e-01)	Acc@1  99.22 ( 97.50)	Acc@5 100.00 ( 99.96)
Epoch: [86][240/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.4788e-02 (1.2261e-01)	Acc@1  97.66 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [86][250/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2372e-01 (1.2270e-01)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [86][260/391]	Time  0.166 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.7769e-02 (1.2301e-01)	Acc@1 100.00 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [86][270/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1078e-01 (1.2343e-01)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [86][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2732e-01 (1.2348e-01)	Acc@1  96.88 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [86][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.6426e-02 (1.2362e-01)	Acc@1  97.66 ( 97.54)	Acc@5 100.00 ( 99.96)
Epoch: [86][300/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4392e-01 (1.2376e-01)	Acc@1  96.88 ( 97.52)	Acc@5  99.22 ( 99.96)
Epoch: [86][310/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2842e-01 (1.2366e-01)	Acc@1  97.66 ( 97.53)	Acc@5 100.00 ( 99.96)
Epoch: [86][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4258e-01 (1.2398e-01)	Acc@1  96.09 ( 97.51)	Acc@5 100.00 ( 99.96)
Epoch: [86][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1218e-01 (1.2445e-01)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.96)
Epoch: [86][340/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.8694e-02 (1.2429e-01)	Acc@1  97.66 ( 97.49)	Acc@5 100.00 ( 99.97)
Epoch: [86][350/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0150e-01 (1.2421e-01)	Acc@1  98.44 ( 97.49)	Acc@5 100.00 ( 99.96)
Epoch: [86][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2646e-01 (1.2396e-01)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [86][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.5815e-02 (1.2402e-01)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [86][380/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 8.3313e-02 (1.2398e-01)	Acc@1  99.22 ( 97.52)	Acc@5 100.00 ( 99.96)
Epoch: [86][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6528e-01 (1.2414e-01)	Acc@1  95.00 ( 97.53)	Acc@5 100.00 ( 99.96)
## e[86] optimizer.zero_grad (sum) time: 0.5450146198272705
## e[86]       loss.backward (sum) time: 12.666843891143799
## e[86]      optimizer.step (sum) time: 25.51058006286621
## epoch[86] training(only) time: 63.50020790100098
# Switched to evaluate mode...
Test: [  0/100]	Time  0.211 ( 0.211)	Loss 1.2012e+00 (1.2012e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.048 ( 0.062)	Loss 1.3809e+00 (1.2647e+00)	Acc@1  65.00 ( 70.09)	Acc@5  90.00 ( 90.36)
Test: [ 20/100]	Time  0.046 ( 0.056)	Loss 1.2500e+00 (1.2393e+00)	Acc@1  69.00 ( 70.62)	Acc@5  92.00 ( 91.14)
Test: [ 30/100]	Time  0.047 ( 0.053)	Loss 1.4346e+00 (1.2752e+00)	Acc@1  65.00 ( 70.06)	Acc@5  90.00 ( 90.84)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 1.0010e+00 (1.2543e+00)	Acc@1  77.00 ( 70.29)	Acc@5  92.00 ( 91.17)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2568e+00 (1.2577e+00)	Acc@1  73.00 ( 70.37)	Acc@5  92.00 ( 90.98)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2275e+00 (1.2495e+00)	Acc@1  67.00 ( 70.36)	Acc@5  89.00 ( 91.03)
Test: [ 70/100]	Time  0.049 ( 0.050)	Loss 1.3994e+00 (1.2475e+00)	Acc@1  64.00 ( 70.45)	Acc@5  93.00 ( 91.15)
Test: [ 80/100]	Time  0.055 ( 0.049)	Loss 1.5898e+00 (1.2485e+00)	Acc@1  71.00 ( 70.48)	Acc@5  86.00 ( 91.11)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3086e+00 (1.2278e+00)	Acc@1  69.00 ( 70.69)	Acc@5  90.00 ( 91.23)
 * Acc@1 70.930 Acc@5 91.280
### epoch[86] execution time: 68.48728275299072
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.318 ( 0.318)	Data  0.150 ( 0.150)	Loss 1.1603e-01 (1.1603e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.161 ( 0.176)	Data  0.001 ( 0.014)	Loss 6.4209e-02 (1.1711e-01)	Acc@1 100.00 ( 97.51)	Acc@5 100.00 ( 99.93)
Epoch: [87][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.0107e-01 (1.1992e-01)	Acc@1  99.22 ( 97.36)	Acc@5 100.00 ( 99.89)
Epoch: [87][ 30/391]	Time  0.159 ( 0.166)	Data  0.001 ( 0.006)	Loss 1.4514e-01 (1.2282e-01)	Acc@1  96.09 ( 97.35)	Acc@5 100.00 ( 99.90)
Epoch: [87][ 40/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.005)	Loss 1.6016e-01 (1.2278e-01)	Acc@1  96.88 ( 97.52)	Acc@5 100.00 ( 99.90)
Epoch: [87][ 50/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.1108e-01 (1.2155e-01)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.92)
Epoch: [87][ 60/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2415e-01 (1.2444e-01)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.91)
Epoch: [87][ 70/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.0004e-01 (1.2413e-01)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.91)
Epoch: [87][ 80/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.7993e-01 (1.2627e-01)	Acc@1  95.31 ( 97.61)	Acc@5 100.00 ( 99.92)
Epoch: [87][ 90/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.4856e-01 (1.2653e-01)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.91)
Epoch: [87][100/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.3811e-02 (1.2631e-01)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.92)
Epoch: [87][110/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5222e-01 (1.2724e-01)	Acc@1  98.44 ( 97.53)	Acc@5 100.00 ( 99.92)
Epoch: [87][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0425e-01 (1.2732e-01)	Acc@1  99.22 ( 97.53)	Acc@5 100.00 ( 99.93)
Epoch: [87][130/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2537e-01 (1.2733e-01)	Acc@1  98.44 ( 97.54)	Acc@5 100.00 ( 99.93)
Epoch: [87][140/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0205e-01 (1.2624e-01)	Acc@1  97.66 ( 97.57)	Acc@5 100.00 ( 99.93)
Epoch: [87][150/391]	Time  0.170 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.9783e-02 (1.2511e-01)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.93)
Epoch: [87][160/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3464e-01 (1.2431e-01)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.94)
Epoch: [87][170/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.8806e-02 (1.2375e-01)	Acc@1  99.22 ( 97.69)	Acc@5 100.00 ( 99.94)
Epoch: [87][180/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1725e-01 (1.2392e-01)	Acc@1  96.09 ( 97.67)	Acc@5 100.00 ( 99.94)
Epoch: [87][190/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1810e-01 (1.2424e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.94)
Epoch: [87][200/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5979e-01 (1.2403e-01)	Acc@1  95.31 ( 97.64)	Acc@5 100.00 ( 99.94)
Epoch: [87][210/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.8726e-01 (1.2438e-01)	Acc@1  95.31 ( 97.64)	Acc@5 100.00 ( 99.94)
Epoch: [87][220/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2708e-01 (1.2512e-01)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.94)
Epoch: [87][230/391]	Time  0.176 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2140e-01 (1.2563e-01)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.95)
Epoch: [87][240/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6370e-01 (1.2602e-01)	Acc@1  96.09 ( 97.57)	Acc@5 100.00 ( 99.95)
Epoch: [87][250/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2524e-01 (1.2561e-01)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [87][260/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4050e-01 (1.2578e-01)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [87][270/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.4534e-02 (1.2595e-01)	Acc@1  98.44 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [87][280/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 9.7046e-02 (1.2579e-01)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [87][290/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.0229e-01 (1.2534e-01)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.95)
Epoch: [87][300/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1951e-01 (1.2500e-01)	Acc@1  99.22 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [87][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8970e-01 (1.2546e-01)	Acc@1  94.53 ( 97.59)	Acc@5 100.00 ( 99.95)
Epoch: [87][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.8652e-01 (1.2562e-01)	Acc@1  92.97 ( 97.58)	Acc@5  99.22 ( 99.95)
Epoch: [87][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2305e-01 (1.2519e-01)	Acc@1  98.44 ( 97.59)	Acc@5 100.00 ( 99.95)
Epoch: [87][340/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5869e-01 (1.2482e-01)	Acc@1  96.88 ( 97.61)	Acc@5  99.22 ( 99.95)
Epoch: [87][350/391]	Time  0.172 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.2549e-01 (1.2479e-01)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [87][360/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.5820e-01 (1.2479e-01)	Acc@1  94.53 ( 97.62)	Acc@5 100.00 ( 99.95)
Epoch: [87][370/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.4941e-01 (1.2442e-01)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.95)
Epoch: [87][380/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1603e-01 (1.2452e-01)	Acc@1  97.66 ( 97.61)	Acc@5 100.00 ( 99.95)
Epoch: [87][390/391]	Time  0.118 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.3440e-01 (1.2482e-01)	Acc@1  96.25 ( 97.59)	Acc@5 100.00 ( 99.95)
## e[87] optimizer.zero_grad (sum) time: 0.540611982345581
## e[87]       loss.backward (sum) time: 12.612971067428589
## e[87]      optimizer.step (sum) time: 25.50646162033081
## epoch[87] training(only) time: 63.45484805107117
# Switched to evaluate mode...
Test: [  0/100]	Time  0.214 ( 0.214)	Loss 1.1826e+00 (1.1826e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.046 ( 0.062)	Loss 1.3750e+00 (1.2608e+00)	Acc@1  65.00 ( 70.45)	Acc@5  92.00 ( 90.45)
Test: [ 20/100]	Time  0.047 ( 0.055)	Loss 1.2373e+00 (1.2368e+00)	Acc@1  70.00 ( 71.00)	Acc@5  92.00 ( 91.29)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4385e+00 (1.2700e+00)	Acc@1  64.00 ( 70.39)	Acc@5  88.00 ( 90.94)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 9.9951e-01 (1.2520e+00)	Acc@1  77.00 ( 70.54)	Acc@5  92.00 ( 91.20)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2559e+00 (1.2545e+00)	Acc@1  73.00 ( 70.59)	Acc@5  92.00 ( 91.10)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2070e+00 (1.2452e+00)	Acc@1  67.00 ( 70.39)	Acc@5  89.00 ( 91.20)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.3887e+00 (1.2428e+00)	Acc@1  64.00 ( 70.51)	Acc@5  93.00 ( 91.30)
Test: [ 80/100]	Time  0.046 ( 0.049)	Loss 1.6016e+00 (1.2436e+00)	Acc@1  69.00 ( 70.54)	Acc@5  86.00 ( 91.25)
Test: [ 90/100]	Time  0.048 ( 0.049)	Loss 1.3027e+00 (1.2228e+00)	Acc@1  72.00 ( 70.82)	Acc@5  90.00 ( 91.40)
 * Acc@1 71.070 Acc@5 91.490
### epoch[87] execution time: 68.46190690994263
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.321 ( 0.321)	Data  0.152 ( 0.152)	Loss 1.1420e-01 (1.1420e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.159 ( 0.175)	Data  0.001 ( 0.015)	Loss 1.1707e-01 (1.2041e-01)	Acc@1  98.44 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.160 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.5869e-01 (1.1924e-01)	Acc@1  96.88 ( 97.84)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.160 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.2549e-01 (1.1648e-01)	Acc@1  97.66 ( 98.01)	Acc@5 100.00 ( 99.97)
Epoch: [88][ 40/391]	Time  0.161 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.4380e-01 (1.1658e-01)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.9934e-01 (1.1792e-01)	Acc@1  95.31 ( 97.99)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 60/391]	Time  0.160 ( 0.165)	Data  0.001 ( 0.004)	Loss 1.2610e-01 (1.2010e-01)	Acc@1  97.66 ( 97.93)	Acc@5 100.00 ( 99.97)
Epoch: [88][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5540e-01 (1.1987e-01)	Acc@1  95.31 ( 97.93)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 80/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.2915e-01 (1.1974e-01)	Acc@1  95.31 ( 97.88)	Acc@5 100.00 ( 99.98)
Epoch: [88][ 90/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0083e-01 (1.2096e-01)	Acc@1 100.00 ( 97.85)	Acc@5 100.00 ( 99.97)
Epoch: [88][100/391]	Time  0.160 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.0699e-01 (1.2117e-01)	Acc@1  98.44 ( 97.82)	Acc@5 100.00 ( 99.98)
Epoch: [88][110/391]	Time  0.162 ( 0.164)	Data  0.001 ( 0.002)	Loss 1.3489e-01 (1.2143e-01)	Acc@1  96.09 ( 97.79)	Acc@5 100.00 ( 99.98)
Epoch: [88][120/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7480e-01 (1.2209e-01)	Acc@1  94.53 ( 97.73)	Acc@5 100.00 ( 99.98)
Epoch: [88][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2012e-01 (1.2308e-01)	Acc@1  99.22 ( 97.67)	Acc@5 100.00 ( 99.98)
Epoch: [88][140/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4282e-01 (1.2269e-01)	Acc@1  96.88 ( 97.69)	Acc@5 100.00 ( 99.98)
Epoch: [88][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4685e-01 (1.2357e-01)	Acc@1  96.88 ( 97.65)	Acc@5 100.00 ( 99.98)
Epoch: [88][160/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2286e-01 (1.2332e-01)	Acc@1 100.00 ( 97.67)	Acc@5 100.00 ( 99.99)
Epoch: [88][170/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1200e-01 (1.2248e-01)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.99)
Epoch: [88][180/391]	Time  0.166 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.6028e-01 (1.2270e-01)	Acc@1  94.53 ( 97.63)	Acc@5 100.00 ( 99.99)
Epoch: [88][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1017e-01 (1.2265e-01)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.99)
Epoch: [88][200/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2244e-01 (1.2317e-01)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.98)
Epoch: [88][210/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1310e-01 (1.2372e-01)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.99)
Epoch: [88][220/391]	Time  0.163 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3538e-01 (1.2423e-01)	Acc@1  96.09 ( 97.53)	Acc@5 100.00 ( 99.99)
Epoch: [88][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3257e-01 (1.2407e-01)	Acc@1  98.44 ( 97.53)	Acc@5  98.44 ( 99.98)
Epoch: [88][240/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5295e-01 (1.2475e-01)	Acc@1  96.88 ( 97.50)	Acc@5 100.00 ( 99.98)
Epoch: [88][250/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3733e-01 (1.2502e-01)	Acc@1  93.75 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [88][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2354e-01 (1.2519e-01)	Acc@1  97.66 ( 97.45)	Acc@5 100.00 ( 99.98)
Epoch: [88][270/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1603e-01 (1.2498e-01)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [88][280/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4160e-01 (1.2510e-01)	Acc@1  96.88 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [88][290/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2018e-01 (1.2502e-01)	Acc@1  98.44 ( 97.47)	Acc@5 100.00 ( 99.97)
Epoch: [88][300/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4246e-01 (1.2574e-01)	Acc@1  95.31 ( 97.45)	Acc@5 100.00 ( 99.97)
Epoch: [88][310/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0211e-01 (1.2548e-01)	Acc@1  98.44 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [88][320/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.001)	Loss 9.1064e-02 (1.2540e-01)	Acc@1  99.22 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [88][330/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.2866e-01 (1.2487e-01)	Acc@1  96.09 ( 97.49)	Acc@5 100.00 ( 99.97)
Epoch: [88][340/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.0712e-01 (1.2524e-01)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [88][350/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.1377e-01 (1.2549e-01)	Acc@1  99.22 ( 97.46)	Acc@5 100.00 ( 99.97)
Epoch: [88][360/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.1194e-01 (1.2522e-01)	Acc@1  97.66 ( 97.48)	Acc@5 100.00 ( 99.97)
Epoch: [88][370/391]	Time  0.159 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.6077e-01 (1.2548e-01)	Acc@1  96.88 ( 97.47)	Acc@5  99.22 ( 99.97)
Epoch: [88][380/391]	Time  0.165 ( 0.163)	Data  0.001 ( 0.001)	Loss 1.0205e-01 (1.2539e-01)	Acc@1  96.88 ( 97.47)	Acc@5 100.00 ( 99.97)
Epoch: [88][390/391]	Time  0.115 ( 0.162)	Data  0.001 ( 0.001)	Loss 9.1431e-02 (1.2528e-01)	Acc@1 100.00 ( 97.47)	Acc@5 100.00 ( 99.97)
## e[88] optimizer.zero_grad (sum) time: 0.5430915355682373
## e[88]       loss.backward (sum) time: 12.706331253051758
## e[88]      optimizer.step (sum) time: 25.448808670043945
## epoch[88] training(only) time: 63.6043975353241
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.1846e+00 (1.1846e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.046 ( 0.061)	Loss 1.3389e+00 (1.2471e+00)	Acc@1  66.00 ( 69.82)	Acc@5  91.00 ( 90.18)
Test: [ 20/100]	Time  0.047 ( 0.054)	Loss 1.2100e+00 (1.2275e+00)	Acc@1  71.00 ( 70.62)	Acc@5  92.00 ( 91.00)
Test: [ 30/100]	Time  0.047 ( 0.052)	Loss 1.4404e+00 (1.2628e+00)	Acc@1  64.00 ( 70.16)	Acc@5  88.00 ( 90.65)
Test: [ 40/100]	Time  0.047 ( 0.051)	Loss 9.9316e-01 (1.2478e+00)	Acc@1  76.00 ( 70.24)	Acc@5  92.00 ( 91.07)
Test: [ 50/100]	Time  0.047 ( 0.050)	Loss 1.2119e+00 (1.2488e+00)	Acc@1  74.00 ( 70.22)	Acc@5  91.00 ( 90.90)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2314e+00 (1.2393e+00)	Acc@1  67.00 ( 70.18)	Acc@5  90.00 ( 91.11)
Test: [ 70/100]	Time  0.047 ( 0.049)	Loss 1.3994e+00 (1.2375e+00)	Acc@1  64.00 ( 70.30)	Acc@5  93.00 ( 91.23)
Test: [ 80/100]	Time  0.047 ( 0.049)	Loss 1.5791e+00 (1.2386e+00)	Acc@1  71.00 ( 70.36)	Acc@5  86.00 ( 91.16)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.3115e+00 (1.2187e+00)	Acc@1  69.00 ( 70.68)	Acc@5  90.00 ( 91.30)
 * Acc@1 70.870 Acc@5 91.380
### epoch[88] execution time: 68.56592893600464
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.329 ( 0.329)	Data  0.146 ( 0.146)	Loss 8.4595e-02 (8.4595e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.160 ( 0.177)	Data  0.001 ( 0.014)	Loss 1.6541e-01 (1.2806e-01)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.161 ( 0.169)	Data  0.001 ( 0.008)	Loss 1.6296e-01 (1.2998e-01)	Acc@1  96.88 ( 97.51)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.161 ( 0.167)	Data  0.001 ( 0.006)	Loss 1.7151e-01 (1.2761e-01)	Acc@1  96.09 ( 97.48)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.169 ( 0.166)	Data  0.001 ( 0.005)	Loss 1.1261e-01 (1.2710e-01)	Acc@1  98.44 ( 97.47)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 50/391]	Time  0.161 ( 0.165)	Data  0.001 ( 0.004)	Loss 9.6802e-02 (1.2664e-01)	Acc@1  97.66 ( 97.46)	Acc@5 100.00 ( 99.98)
Epoch: [89][ 60/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.004)	Loss 1.2537e-01 (1.2405e-01)	Acc@1  96.88 ( 97.62)	Acc@5 100.00 ( 99.99)
Epoch: [89][ 70/391]	Time  0.161 ( 0.164)	Data  0.001 ( 0.003)	Loss 1.5332e-01 (1.2497e-01)	Acc@1  96.09 ( 97.58)	Acc@5 100.00 ( 99.99)
Epoch: [89][ 80/391]	Time  0.176 ( 0.164)	Data  0.001 ( 0.003)	Loss 8.2642e-02 (1.2418e-01)	Acc@1  99.22 ( 97.63)	Acc@5 100.00 ( 99.96)
Epoch: [89][ 90/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.1792e-01 (1.2253e-01)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [89][100/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.003)	Loss 1.3281e-01 (1.2241e-01)	Acc@1  96.09 ( 97.71)	Acc@5 100.00 ( 99.95)
Epoch: [89][110/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.7273e-01 (1.2450e-01)	Acc@1  94.53 ( 97.63)	Acc@5 100.00 ( 99.94)
Epoch: [89][120/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5417e-01 (1.2469e-01)	Acc@1  96.88 ( 97.60)	Acc@5 100.00 ( 99.94)
Epoch: [89][130/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2891e-01 (1.2602e-01)	Acc@1  96.88 ( 97.56)	Acc@5 100.00 ( 99.95)
Epoch: [89][140/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.5869e-01 (1.2614e-01)	Acc@1  96.88 ( 97.58)	Acc@5 100.00 ( 99.95)
Epoch: [89][150/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2939e-01 (1.2549e-01)	Acc@1  97.66 ( 97.59)	Acc@5 100.00 ( 99.95)
Epoch: [89][160/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2469e-01 (1.2572e-01)	Acc@1  97.66 ( 97.60)	Acc@5 100.00 ( 99.96)
Epoch: [89][170/391]	Time  0.164 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.3867e-01 (1.2499e-01)	Acc@1  96.09 ( 97.63)	Acc@5 100.00 ( 99.96)
Epoch: [89][180/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2866e-01 (1.2480e-01)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.96)
Epoch: [89][190/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.3557e-02 (1.2450e-01)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [89][200/391]	Time  0.167 ( 0.163)	Data  0.001 ( 0.002)	Loss 8.2764e-02 (1.2414e-01)	Acc@1 100.00 ( 97.68)	Acc@5 100.00 ( 99.96)
Epoch: [89][210/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.0773e-01 (1.2444e-01)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.96)
Epoch: [89][220/391]	Time  0.162 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.2170e-01 (1.2492e-01)	Acc@1  99.22 ( 97.68)	Acc@5 100.00 ( 99.95)
Epoch: [89][230/391]	Time  0.161 ( 0.163)	Data  0.001 ( 0.002)	Loss 9.5215e-02 (1.2499e-01)	Acc@1  99.22 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [89][240/391]	Time  0.168 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.2475e-01)	Acc@1  96.09 ( 97.66)	Acc@5 100.00 ( 99.95)
Epoch: [89][250/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.1377e-01 (1.2429e-01)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.96)
Epoch: [89][260/391]	Time  0.160 ( 0.163)	Data  0.001 ( 0.002)	Loss 1.1908e-01 (1.2436e-01)	Acc@1  96.09 ( 97.65)	Acc@5 100.00 ( 99.96)
Epoch: [89][270/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.4429e-01 (1.2394e-01)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [89][280/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2152e-01 (1.2359e-01)	Acc@1  96.88 ( 97.66)	Acc@5 100.00 ( 99.96)
Epoch: [89][290/391]	Time  0.168 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3782e-01 (1.2357e-01)	Acc@1  97.66 ( 97.65)	Acc@5  99.22 ( 99.96)
Epoch: [89][300/391]	Time  0.159 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3953e-01 (1.2349e-01)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.96)
Epoch: [89][310/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.6882e-01 (1.2427e-01)	Acc@1  93.75 ( 97.61)	Acc@5  99.22 ( 99.96)
Epoch: [89][320/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.5601e-01 (1.2404e-01)	Acc@1  96.09 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [89][330/391]	Time  0.161 ( 0.162)	Data  0.001 ( 0.002)	Loss 8.9417e-02 (1.2412e-01)	Acc@1  98.44 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [89][340/391]	Time  0.171 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.7200e-01 (1.2426e-01)	Acc@1  95.31 ( 97.61)	Acc@5  99.22 ( 99.96)
Epoch: [89][350/391]	Time  0.164 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.0327e-01 (1.2397e-01)	Acc@1  98.44 ( 97.62)	Acc@5 100.00 ( 99.96)
Epoch: [89][360/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.3013e-01 (1.2399e-01)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.96)
Epoch: [89][370/391]	Time  0.160 ( 0.162)	Data  0.001 ( 0.002)	Loss 1.2183e-01 (1.2411e-01)	Acc@1  96.09 ( 97.61)	Acc@5 100.00 ( 99.96)
Epoch: [89][380/391]	Time  0.162 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.1621e-01 (1.2391e-01)	Acc@1  97.66 ( 97.63)	Acc@5 100.00 ( 99.96)
Epoch: [89][390/391]	Time  0.124 ( 0.162)	Data  0.001 ( 0.001)	Loss 1.6809e-01 (1.2401e-01)	Acc@1  95.00 ( 97.62)	Acc@5 100.00 ( 99.96)
## e[89] optimizer.zero_grad (sum) time: 0.5462267398834229
## e[89]       loss.backward (sum) time: 12.626327753067017
## e[89]      optimizer.step (sum) time: 25.482668161392212
## epoch[89] training(only) time: 63.480451822280884
# Switched to evaluate mode...
Test: [  0/100]	Time  0.190 ( 0.190)	Loss 1.1885e+00 (1.1885e+00)	Acc@1  72.00 ( 72.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.048 ( 0.060)	Loss 1.3428e+00 (1.2466e+00)	Acc@1  66.00 ( 69.91)	Acc@5  90.00 ( 90.45)
Test: [ 20/100]	Time  0.059 ( 0.055)	Loss 1.2090e+00 (1.2264e+00)	Acc@1  72.00 ( 70.71)	Acc@5  92.00 ( 91.19)
Test: [ 30/100]	Time  0.046 ( 0.052)	Loss 1.4365e+00 (1.2603e+00)	Acc@1  66.00 ( 70.23)	Acc@5  89.00 ( 90.87)
Test: [ 40/100]	Time  0.046 ( 0.051)	Loss 9.8926e-01 (1.2427e+00)	Acc@1  76.00 ( 70.29)	Acc@5  92.00 ( 91.20)
Test: [ 50/100]	Time  0.046 ( 0.050)	Loss 1.2588e+00 (1.2465e+00)	Acc@1  72.00 ( 70.41)	Acc@5  92.00 ( 91.08)
Test: [ 60/100]	Time  0.047 ( 0.050)	Loss 1.2227e+00 (1.2378e+00)	Acc@1  67.00 ( 70.34)	Acc@5  90.00 ( 91.11)
Test: [ 70/100]	Time  0.046 ( 0.049)	Loss 1.3984e+00 (1.2364e+00)	Acc@1  65.00 ( 70.44)	Acc@5  91.00 ( 91.18)
Test: [ 80/100]	Time  0.048 ( 0.049)	Loss 1.5879e+00 (1.2361e+00)	Acc@1  71.00 ( 70.46)	Acc@5  86.00 ( 91.12)
Test: [ 90/100]	Time  0.046 ( 0.049)	Loss 1.2949e+00 (1.2160e+00)	Acc@1  68.00 ( 70.68)	Acc@5  90.00 ( 91.27)
 * Acc@1 70.900 Acc@5 91.360
### epoch[89] execution time: 68.44970941543579
### Training complete:
#### total training(only) time: 5704.848787307739
##### Total run time: 6154.4531128406525
