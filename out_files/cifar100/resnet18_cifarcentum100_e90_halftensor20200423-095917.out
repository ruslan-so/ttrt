# Model: resnet18
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.resnet
<function resnet18 at 0x7f42799dff28>
# model requested: 'resnet18'
# printing out the model
ResNet(
  (conv1): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
  )
  (conv2_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv3_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv4_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (conv5_x): Sequential(
    (0): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (residual_function): Sequential(
        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (shortcut): Sequential()
    )
  )
  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=100, bias=True)
)
# model is low precision
# Model: resnet18
# Dataset: cifarcentum
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.472 ( 3.472)	Data  0.105 ( 0.105)	Loss 4.7383e+00 (4.7383e+00)	Acc@1   0.00 (  0.00)	Acc@5   3.12 (  3.12)
Epoch: [0][ 10/391]	Time  0.033 ( 0.347)	Data  0.001 ( 0.011)	Loss 5.0156e+00 (5.1108e+00)	Acc@1   1.56 (  1.78)	Acc@5   7.03 (  7.60)
Epoch: [0][ 20/391]	Time  0.036 ( 0.198)	Data  0.001 ( 0.006)	Loss 4.7461e+00 (5.0590e+00)	Acc@1   2.34 (  2.05)	Acc@5   7.81 (  8.04)
Epoch: [0][ 30/391]	Time  0.035 ( 0.146)	Data  0.001 ( 0.005)	Loss 4.9297e+00 (4.9627e+00)	Acc@1   6.25 (  2.19)	Acc@5  14.84 (  9.00)
Epoch: [0][ 40/391]	Time  0.035 ( 0.118)	Data  0.001 ( 0.004)	Loss 4.3906e+00 (4.8520e+00)	Acc@1   5.47 (  2.48)	Acc@5  15.62 ( 10.16)
Epoch: [0][ 50/391]	Time  0.035 ( 0.102)	Data  0.001 ( 0.004)	Loss 4.3750e+00 (4.7506e+00)	Acc@1   3.91 (  2.93)	Acc@5  14.06 ( 11.37)
Epoch: [0][ 60/391]	Time  0.033 ( 0.091)	Data  0.001 ( 0.003)	Loss 4.3828e+00 (4.6749e+00)	Acc@1   3.12 (  3.21)	Acc@5  14.06 ( 12.32)
Epoch: [0][ 70/391]	Time  0.033 ( 0.083)	Data  0.001 ( 0.003)	Loss 4.3008e+00 (4.6133e+00)	Acc@1   3.12 (  3.24)	Acc@5  21.88 ( 13.26)
Epoch: [0][ 80/391]	Time  0.033 ( 0.077)	Data  0.001 ( 0.003)	Loss 4.3242e+00 (4.5595e+00)	Acc@1   7.03 (  3.47)	Acc@5  21.09 ( 14.35)
Epoch: [0][ 90/391]	Time  0.034 ( 0.072)	Data  0.001 ( 0.003)	Loss 4.0547e+00 (4.5127e+00)	Acc@1   9.38 (  3.70)	Acc@5  26.56 ( 15.10)
Epoch: [0][100/391]	Time  0.030 ( 0.069)	Data  0.001 ( 0.003)	Loss 4.0742e+00 (4.4687e+00)	Acc@1   4.69 (  3.94)	Acc@5  24.22 ( 15.98)
Epoch: [0][110/391]	Time  0.032 ( 0.065)	Data  0.001 ( 0.003)	Loss 4.1211e+00 (4.4351e+00)	Acc@1   3.91 (  4.09)	Acc@5  22.66 ( 16.70)
Epoch: [0][120/391]	Time  0.035 ( 0.063)	Data  0.001 ( 0.003)	Loss 4.1250e+00 (4.4061e+00)	Acc@1   5.47 (  4.33)	Acc@5  27.34 ( 17.35)
Epoch: [0][130/391]	Time  0.034 ( 0.061)	Data  0.001 ( 0.002)	Loss 4.0664e+00 (4.3782e+00)	Acc@1   4.69 (  4.51)	Acc@5  25.00 ( 17.98)
Epoch: [0][140/391]	Time  0.034 ( 0.059)	Data  0.001 ( 0.002)	Loss 4.0117e+00 (4.3505e+00)	Acc@1   5.47 (  4.73)	Acc@5  24.22 ( 18.58)
Epoch: [0][150/391]	Time  0.037 ( 0.057)	Data  0.001 ( 0.002)	Loss 3.9824e+00 (4.3261e+00)	Acc@1  10.16 (  5.01)	Acc@5  24.22 ( 19.08)
Epoch: [0][160/391]	Time  0.036 ( 0.056)	Data  0.001 ( 0.002)	Loss 4.0547e+00 (4.3047e+00)	Acc@1   7.81 (  5.20)	Acc@5  25.78 ( 19.43)
Epoch: [0][170/391]	Time  0.033 ( 0.055)	Data  0.001 ( 0.002)	Loss 3.7891e+00 (4.2816e+00)	Acc@1   8.59 (  5.47)	Acc@5  38.28 ( 20.13)
Epoch: [0][180/391]	Time  0.034 ( 0.054)	Data  0.001 ( 0.002)	Loss 3.9082e+00 (4.2659e+00)	Acc@1  10.94 (  5.53)	Acc@5  34.38 ( 20.52)
Epoch: [0][190/391]	Time  0.035 ( 0.053)	Data  0.001 ( 0.002)	Loss 3.8613e+00 (4.2500e+00)	Acc@1   7.03 (  5.77)	Acc@5  28.12 ( 20.91)
Epoch: [0][200/391]	Time  0.033 ( 0.052)	Data  0.001 ( 0.002)	Loss 4.0625e+00 (4.2345e+00)	Acc@1   7.81 (  5.92)	Acc@5  23.44 ( 21.31)
Epoch: [0][210/391]	Time  0.035 ( 0.051)	Data  0.001 ( 0.002)	Loss 3.9043e+00 (4.2191e+00)	Acc@1   7.03 (  6.04)	Acc@5  28.91 ( 21.72)
Epoch: [0][220/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.002)	Loss 3.9316e+00 (4.2072e+00)	Acc@1   5.47 (  6.15)	Acc@5  28.12 ( 22.01)
Epoch: [0][230/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.002)	Loss 3.8184e+00 (4.1914e+00)	Acc@1   9.38 (  6.30)	Acc@5  32.81 ( 22.50)
Epoch: [0][240/391]	Time  0.035 ( 0.049)	Data  0.001 ( 0.002)	Loss 3.8848e+00 (4.1770e+00)	Acc@1  11.72 (  6.44)	Acc@5  28.91 ( 22.90)
Epoch: [0][250/391]	Time  0.036 ( 0.048)	Data  0.001 ( 0.002)	Loss 3.9082e+00 (4.1642e+00)	Acc@1   7.81 (  6.53)	Acc@5  31.25 ( 23.22)
Epoch: [0][260/391]	Time  0.033 ( 0.048)	Data  0.001 ( 0.002)	Loss 3.8750e+00 (4.1535e+00)	Acc@1  12.50 (  6.65)	Acc@5  26.56 ( 23.45)
Epoch: [0][270/391]	Time  0.034 ( 0.047)	Data  0.001 ( 0.002)	Loss 3.6484e+00 (4.1406e+00)	Acc@1  11.72 (  6.76)	Acc@5  35.94 ( 23.78)
Epoch: [0][280/391]	Time  0.036 ( 0.047)	Data  0.001 ( 0.002)	Loss 3.9258e+00 (4.1304e+00)	Acc@1  10.16 (  6.90)	Acc@5  28.12 ( 24.08)
Epoch: [0][290/391]	Time  0.034 ( 0.046)	Data  0.001 ( 0.002)	Loss 3.9902e+00 (4.1206e+00)	Acc@1   9.38 (  7.01)	Acc@5  32.03 ( 24.37)
Epoch: [0][300/391]	Time  0.032 ( 0.046)	Data  0.001 ( 0.002)	Loss 3.8867e+00 (4.1101e+00)	Acc@1  10.94 (  7.14)	Acc@5  30.47 ( 24.68)
Epoch: [0][310/391]	Time  0.034 ( 0.046)	Data  0.001 ( 0.002)	Loss 3.7461e+00 (4.0993e+00)	Acc@1  14.06 (  7.28)	Acc@5  39.06 ( 24.95)
Epoch: [0][320/391]	Time  0.033 ( 0.045)	Data  0.001 ( 0.002)	Loss 3.9375e+00 (4.0920e+00)	Acc@1   7.03 (  7.32)	Acc@5  30.47 ( 25.14)
Epoch: [0][330/391]	Time  0.036 ( 0.045)	Data  0.001 ( 0.002)	Loss 3.7871e+00 (4.0819e+00)	Acc@1  17.19 (  7.43)	Acc@5  38.28 ( 25.41)
Epoch: [0][340/391]	Time  0.035 ( 0.045)	Data  0.001 ( 0.002)	Loss 3.7969e+00 (4.0718e+00)	Acc@1   9.38 (  7.52)	Acc@5  30.47 ( 25.71)
Epoch: [0][350/391]	Time  0.033 ( 0.044)	Data  0.001 ( 0.002)	Loss 3.6738e+00 (4.0611e+00)	Acc@1  10.94 (  7.66)	Acc@5  33.59 ( 26.00)
Epoch: [0][360/391]	Time  0.035 ( 0.044)	Data  0.001 ( 0.002)	Loss 3.8184e+00 (4.0512e+00)	Acc@1  14.06 (  7.81)	Acc@5  34.38 ( 26.27)
Epoch: [0][370/391]	Time  0.039 ( 0.044)	Data  0.001 ( 0.002)	Loss 3.6680e+00 (4.0405e+00)	Acc@1   9.38 (  7.93)	Acc@5  35.16 ( 26.58)
Epoch: [0][380/391]	Time  0.034 ( 0.044)	Data  0.001 ( 0.002)	Loss 3.8379e+00 (4.0323e+00)	Acc@1   8.59 (  7.99)	Acc@5  23.44 ( 26.77)
Epoch: [0][390/391]	Time  0.317 ( 0.044)	Data  0.001 ( 0.002)	Loss 3.7930e+00 (4.0248e+00)	Acc@1   8.75 (  8.08)	Acc@5  32.50 ( 26.99)
## e[0] optimizer.zero_grad (sum) time: 0.17693042755126953
## e[0]       loss.backward (sum) time: 4.242982864379883
## e[0]      optimizer.step (sum) time: 1.2801342010498047
## epoch[0] training(only) time: 17.31391429901123
# Switched to evaluate mode...
Test: [  0/100]	Time  0.246 ( 0.246)	Loss 3.6992e+00 (3.6992e+00)	Acc@1  13.00 ( 13.00)	Acc@5  35.00 ( 35.00)
Test: [ 10/100]	Time  0.023 ( 0.040)	Loss 3.6777e+00 (3.6495e+00)	Acc@1  11.00 ( 10.73)	Acc@5  35.00 ( 38.00)
Test: [ 20/100]	Time  0.018 ( 0.029)	Loss 3.6016e+00 (3.6694e+00)	Acc@1  13.00 ( 11.19)	Acc@5  40.00 ( 37.05)
Test: [ 30/100]	Time  0.015 ( 0.025)	Loss 3.8848e+00 (3.6684e+00)	Acc@1  11.00 ( 11.45)	Acc@5  30.00 ( 36.35)
Test: [ 40/100]	Time  0.015 ( 0.023)	Loss 3.6543e+00 (3.6656e+00)	Acc@1  12.00 ( 11.41)	Acc@5  43.00 ( 36.66)
Test: [ 50/100]	Time  0.022 ( 0.022)	Loss 3.6094e+00 (3.6574e+00)	Acc@1  13.00 ( 11.69)	Acc@5  37.00 ( 36.90)
Test: [ 60/100]	Time  0.016 ( 0.021)	Loss 3.6465e+00 (3.6606e+00)	Acc@1  13.00 ( 11.66)	Acc@5  39.00 ( 36.85)
Test: [ 70/100]	Time  0.023 ( 0.021)	Loss 3.7754e+00 (3.6624e+00)	Acc@1  10.00 ( 11.61)	Acc@5  38.00 ( 36.80)
Test: [ 80/100]	Time  0.016 ( 0.020)	Loss 3.8379e+00 (3.6687e+00)	Acc@1   7.00 ( 11.68)	Acc@5  27.00 ( 36.33)
Test: [ 90/100]	Time  0.015 ( 0.020)	Loss 3.5430e+00 (3.6604e+00)	Acc@1  15.00 ( 11.80)	Acc@5  42.00 ( 36.66)
 * Acc@1 11.810 Acc@5 36.530
### epoch[0] execution time: 19.350289821624756
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.201 ( 0.201)	Data  0.150 ( 0.150)	Loss 3.6055e+00 (3.6055e+00)	Acc@1  10.94 ( 10.94)	Acc@5  35.16 ( 35.16)
Epoch: [1][ 10/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.015)	Loss 3.6602e+00 (3.6369e+00)	Acc@1  17.97 ( 13.49)	Acc@5  38.28 ( 36.72)
Epoch: [1][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 3.6758e+00 (3.6735e+00)	Acc@1  14.06 ( 12.65)	Acc@5  38.28 ( 36.68)
Epoch: [1][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.7090e+00 (3.6491e+00)	Acc@1  11.72 ( 12.85)	Acc@5  39.84 ( 37.37)
Epoch: [1][ 40/391]	Time  0.045 ( 0.039)	Data  0.001 ( 0.005)	Loss 3.7129e+00 (3.6485e+00)	Acc@1   9.38 ( 12.86)	Acc@5  33.59 ( 37.42)
Epoch: [1][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 3.4980e+00 (3.6331e+00)	Acc@1  13.28 ( 13.05)	Acc@5  38.28 ( 38.19)
Epoch: [1][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.8730e+00 (3.6265e+00)	Acc@1   9.38 ( 13.14)	Acc@5  34.38 ( 38.36)
Epoch: [1][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.5215e+00 (3.6180e+00)	Acc@1  17.19 ( 13.39)	Acc@5  37.50 ( 38.51)
Epoch: [1][ 80/391]	Time  0.041 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.6445e+00 (3.6129e+00)	Acc@1  10.16 ( 13.47)	Acc@5  39.06 ( 38.44)
Epoch: [1][ 90/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.6855e+00 (3.6054e+00)	Acc@1  13.28 ( 13.76)	Acc@5  38.28 ( 38.77)
Epoch: [1][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.5332e+00 (3.6021e+00)	Acc@1  11.72 ( 13.75)	Acc@5  42.97 ( 38.86)
Epoch: [1][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6973e+00 (3.5986e+00)	Acc@1  15.62 ( 13.99)	Acc@5  39.84 ( 38.99)
Epoch: [1][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4395e+00 (3.5921e+00)	Acc@1  16.41 ( 14.04)	Acc@5  45.31 ( 39.06)
Epoch: [1][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4590e+00 (3.5818e+00)	Acc@1  19.53 ( 14.28)	Acc@5  42.19 ( 39.30)
Epoch: [1][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.2871e+00 (3.5685e+00)	Acc@1  21.09 ( 14.48)	Acc@5  43.75 ( 39.71)
Epoch: [1][150/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6855e+00 (3.5667e+00)	Acc@1   9.38 ( 14.61)	Acc@5  33.59 ( 39.75)
Epoch: [1][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3711e+00 (3.5598e+00)	Acc@1  17.97 ( 14.66)	Acc@5  42.19 ( 39.99)
Epoch: [1][170/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3848e+00 (3.5532e+00)	Acc@1  14.84 ( 14.80)	Acc@5  50.00 ( 40.17)
Epoch: [1][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5273e+00 (3.5473e+00)	Acc@1  11.72 ( 14.86)	Acc@5  42.97 ( 40.31)
Epoch: [1][190/391]	Time  0.039 ( 0.036)	Data  0.004 ( 0.002)	Loss 3.2754e+00 (3.5393e+00)	Acc@1  22.66 ( 15.06)	Acc@5  50.00 ( 40.56)
Epoch: [1][200/391]	Time  0.036 ( 0.035)	Data  0.000 ( 0.002)	Loss 3.1992e+00 (3.5319e+00)	Acc@1  25.78 ( 15.21)	Acc@5  50.78 ( 40.74)
Epoch: [1][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2969e+00 (3.5277e+00)	Acc@1  20.31 ( 15.27)	Acc@5  48.44 ( 40.85)
Epoch: [1][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3887e+00 (3.5211e+00)	Acc@1  21.09 ( 15.39)	Acc@5  45.31 ( 41.05)
Epoch: [1][230/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4531e+00 (3.5167e+00)	Acc@1  16.41 ( 15.36)	Acc@5  40.62 ( 41.15)
Epoch: [1][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9883e+00 (3.5075e+00)	Acc@1  23.44 ( 15.50)	Acc@5  53.12 ( 41.40)
Epoch: [1][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2188e+00 (3.5002e+00)	Acc@1  21.09 ( 15.58)	Acc@5  47.66 ( 41.61)
Epoch: [1][260/391]	Time  0.043 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3105e+00 (3.4964e+00)	Acc@1  22.66 ( 15.68)	Acc@5  49.22 ( 41.71)
Epoch: [1][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3691e+00 (3.4897e+00)	Acc@1  23.44 ( 15.81)	Acc@5  45.31 ( 41.89)
Epoch: [1][280/391]	Time  0.031 ( 0.035)	Data  0.002 ( 0.002)	Loss 3.2734e+00 (3.4828e+00)	Acc@1  17.19 ( 15.93)	Acc@5  46.88 ( 42.10)
Epoch: [1][290/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4668e+00 (3.4785e+00)	Acc@1  14.84 ( 16.00)	Acc@5  41.41 ( 42.27)
Epoch: [1][300/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4570e+00 (3.4722e+00)	Acc@1  18.75 ( 16.09)	Acc@5  42.19 ( 42.48)
Epoch: [1][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4668e+00 (3.4637e+00)	Acc@1  14.06 ( 16.23)	Acc@5  49.22 ( 42.77)
Epoch: [1][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2188e+00 (3.4570e+00)	Acc@1  20.31 ( 16.31)	Acc@5  49.22 ( 42.97)
Epoch: [1][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2285e+00 (3.4497e+00)	Acc@1  18.75 ( 16.40)	Acc@5  46.09 ( 43.18)
Epoch: [1][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2715e+00 (3.4416e+00)	Acc@1  25.00 ( 16.57)	Acc@5  49.22 ( 43.43)
Epoch: [1][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2285e+00 (3.4354e+00)	Acc@1  18.75 ( 16.66)	Acc@5  47.66 ( 43.62)
Epoch: [1][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1699e+00 (3.4293e+00)	Acc@1  24.22 ( 16.74)	Acc@5  51.56 ( 43.74)
Epoch: [1][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2324e+00 (3.4248e+00)	Acc@1  19.53 ( 16.84)	Acc@5  50.00 ( 43.95)
Epoch: [1][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3320e+00 (3.4175e+00)	Acc@1  21.88 ( 17.01)	Acc@5  44.53 ( 44.19)
Epoch: [1][390/391]	Time  0.025 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0137e+00 (3.4104e+00)	Acc@1  28.75 ( 17.11)	Acc@5  60.00 ( 44.40)
## e[1] optimizer.zero_grad (sum) time: 0.17813515663146973
## e[1]       loss.backward (sum) time: 3.890737771987915
## e[1]      optimizer.step (sum) time: 1.2860066890716553
## epoch[1] training(only) time: 13.758076667785645
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 3.3574e+00 (3.3574e+00)	Acc@1  28.00 ( 28.00)	Acc@5  47.00 ( 47.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 3.3418e+00 (3.2408e+00)	Acc@1  15.00 ( 21.18)	Acc@5  52.00 ( 49.27)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 3.3457e+00 (3.2265e+00)	Acc@1  21.00 ( 21.52)	Acc@5  41.00 ( 50.19)
Test: [ 30/100]	Time  0.019 ( 0.022)	Loss 3.3418e+00 (3.1975e+00)	Acc@1  16.00 ( 21.61)	Acc@5  43.00 ( 50.42)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 3.3613e+00 (3.1942e+00)	Acc@1  19.00 ( 21.68)	Acc@5  49.00 ( 50.90)
Test: [ 50/100]	Time  0.024 ( 0.020)	Loss 3.0586e+00 (3.2089e+00)	Acc@1  23.00 ( 21.73)	Acc@5  48.00 ( 50.39)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 3.0332e+00 (3.2071e+00)	Acc@1  23.00 ( 21.52)	Acc@5  55.00 ( 50.26)
Test: [ 70/100]	Time  0.015 ( 0.020)	Loss 3.5293e+00 (3.2182e+00)	Acc@1  17.00 ( 21.37)	Acc@5  43.00 ( 50.07)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 3.4648e+00 (3.2244e+00)	Acc@1  18.00 ( 21.12)	Acc@5  43.00 ( 50.19)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 3.0469e+00 (3.2209e+00)	Acc@1  26.00 ( 21.21)	Acc@5  54.00 ( 50.38)
 * Acc@1 21.190 Acc@5 50.420
### epoch[1] execution time: 15.739834785461426
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.192 ( 0.192)	Data  0.150 ( 0.150)	Loss 2.9375e+00 (2.9375e+00)	Acc@1  28.12 ( 28.12)	Acc@5  53.91 ( 53.91)
Epoch: [2][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 3.2793e+00 (3.1527e+00)	Acc@1  21.88 ( 22.51)	Acc@5  46.88 ( 51.28)
Epoch: [2][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 3.1465e+00 (3.1163e+00)	Acc@1  22.66 ( 23.74)	Acc@5  51.56 ( 52.75)
Epoch: [2][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.3242e+00 (3.1084e+00)	Acc@1  16.41 ( 23.46)	Acc@5  49.22 ( 53.00)
Epoch: [2][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 2.9141e+00 (3.0988e+00)	Acc@1  32.81 ( 23.63)	Acc@5  57.81 ( 53.16)
Epoch: [2][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.2383e+00 (3.1129e+00)	Acc@1  19.53 ( 23.25)	Acc@5  53.12 ( 52.76)
Epoch: [2][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.3848e+00 (3.1143e+00)	Acc@1  21.88 ( 23.10)	Acc@5  44.53 ( 52.38)
Epoch: [2][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.8516e+00 (3.1135e+00)	Acc@1  22.66 ( 23.10)	Acc@5  61.72 ( 52.66)
Epoch: [2][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.9668e+00 (3.1025e+00)	Acc@1  21.88 ( 23.29)	Acc@5  56.25 ( 53.02)
Epoch: [2][ 90/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1953e+00 (3.0910e+00)	Acc@1  21.88 ( 23.51)	Acc@5  46.88 ( 53.04)
Epoch: [2][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.7754e+00 (3.0820e+00)	Acc@1  32.03 ( 23.58)	Acc@5  60.94 ( 53.21)
Epoch: [2][110/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1660e+00 (3.0772e+00)	Acc@1  22.66 ( 23.70)	Acc@5  50.78 ( 53.42)
Epoch: [2][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.0508e+00 (3.0722e+00)	Acc@1  24.22 ( 23.72)	Acc@5  54.69 ( 53.59)
Epoch: [2][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.0879e+00 (3.0674e+00)	Acc@1  20.31 ( 23.86)	Acc@5  57.81 ( 53.75)
Epoch: [2][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1113e+00 (3.0607e+00)	Acc@1  25.00 ( 24.02)	Acc@5  50.00 ( 53.99)
Epoch: [2][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.0938e+00 (3.0552e+00)	Acc@1  17.97 ( 24.13)	Acc@5  52.34 ( 54.14)
Epoch: [2][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.0469e+00 (3.0521e+00)	Acc@1  25.00 ( 24.28)	Acc@5  57.03 ( 54.28)
Epoch: [2][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1484e+00 (3.0452e+00)	Acc@1  21.88 ( 24.42)	Acc@5  53.12 ( 54.46)
Epoch: [2][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.9863e+00 (3.0457e+00)	Acc@1  18.75 ( 24.37)	Acc@5  53.91 ( 54.50)
Epoch: [2][190/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9980e+00 (3.0422e+00)	Acc@1  30.47 ( 24.42)	Acc@5  57.81 ( 54.62)
Epoch: [2][200/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2988e+00 (3.0388e+00)	Acc@1  17.19 ( 24.45)	Acc@5  46.88 ( 54.70)
Epoch: [2][210/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6816e+00 (3.0346e+00)	Acc@1  31.25 ( 24.48)	Acc@5  64.06 ( 54.85)
Epoch: [2][220/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9180e+00 (3.0282e+00)	Acc@1  25.78 ( 24.59)	Acc@5  59.38 ( 55.00)
Epoch: [2][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7695e+00 (3.0211e+00)	Acc@1  28.12 ( 24.66)	Acc@5  60.94 ( 55.20)
Epoch: [2][240/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6445e+00 (3.0130e+00)	Acc@1  28.91 ( 24.81)	Acc@5  64.84 ( 55.41)
Epoch: [2][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7383e+00 (3.0101e+00)	Acc@1  27.34 ( 24.92)	Acc@5  66.41 ( 55.47)
Epoch: [2][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9766e+00 (3.0040e+00)	Acc@1  26.56 ( 25.01)	Acc@5  60.94 ( 55.61)
Epoch: [2][270/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7520e+00 (2.9985e+00)	Acc@1  33.59 ( 25.14)	Acc@5  60.16 ( 55.74)
Epoch: [2][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0410e+00 (2.9956e+00)	Acc@1  21.09 ( 25.15)	Acc@5  51.56 ( 55.84)
Epoch: [2][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7871e+00 (2.9932e+00)	Acc@1  29.69 ( 25.19)	Acc@5  53.91 ( 55.90)
Epoch: [2][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9980e+00 (2.9877e+00)	Acc@1  26.56 ( 25.28)	Acc@5  56.25 ( 56.02)
Epoch: [2][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.8555e+00 (2.9840e+00)	Acc@1  25.78 ( 25.31)	Acc@5  56.25 ( 56.12)
Epoch: [2][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7754e+00 (2.9810e+00)	Acc@1  32.03 ( 25.37)	Acc@5  59.38 ( 56.17)
Epoch: [2][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6230e+00 (2.9762e+00)	Acc@1  33.59 ( 25.52)	Acc@5  64.84 ( 56.24)
Epoch: [2][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.8652e+00 (2.9719e+00)	Acc@1  32.81 ( 25.63)	Acc@5  60.16 ( 56.37)
Epoch: [2][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6914e+00 (2.9676e+00)	Acc@1  29.69 ( 25.70)	Acc@5  61.72 ( 56.47)
Epoch: [2][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.8516e+00 (2.9652e+00)	Acc@1  32.03 ( 25.79)	Acc@5  57.81 ( 56.51)
Epoch: [2][370/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.5684e+00 (2.9609e+00)	Acc@1  30.47 ( 25.85)	Acc@5  64.84 ( 56.59)
Epoch: [2][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9238e+00 (2.9550e+00)	Acc@1  26.56 ( 25.95)	Acc@5  57.81 ( 56.70)
Epoch: [2][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7637e+00 (2.9503e+00)	Acc@1  28.75 ( 25.98)	Acc@5  61.25 ( 56.82)
## e[2] optimizer.zero_grad (sum) time: 0.179579496383667
## e[2]       loss.backward (sum) time: 4.079457759857178
## e[2]      optimizer.step (sum) time: 1.229163646697998
## epoch[2] training(only) time: 13.803674221038818
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 3.0723e+00 (3.0723e+00)	Acc@1  27.00 ( 27.00)	Acc@5  54.00 ( 54.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 3.0879e+00 (2.9943e+00)	Acc@1  28.00 ( 27.27)	Acc@5  53.00 ( 56.18)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 2.8711e+00 (2.9852e+00)	Acc@1  25.00 ( 27.62)	Acc@5  54.00 ( 56.62)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 3.1387e+00 (2.9802e+00)	Acc@1  25.00 ( 27.13)	Acc@5  50.00 ( 56.97)
Test: [ 40/100]	Time  0.016 ( 0.020)	Loss 3.0312e+00 (2.9743e+00)	Acc@1  31.00 ( 27.49)	Acc@5  60.00 ( 57.37)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 2.9219e+00 (2.9977e+00)	Acc@1  28.00 ( 26.98)	Acc@5  55.00 ( 56.94)
Test: [ 60/100]	Time  0.023 ( 0.019)	Loss 2.7168e+00 (2.9941e+00)	Acc@1  32.00 ( 26.95)	Acc@5  54.00 ( 57.13)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 2.9746e+00 (3.0043e+00)	Acc@1  28.00 ( 26.69)	Acc@5  57.00 ( 56.77)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 3.4258e+00 (3.0097e+00)	Acc@1  21.00 ( 26.57)	Acc@5  47.00 ( 56.65)
Test: [ 90/100]	Time  0.023 ( 0.019)	Loss 2.8535e+00 (2.9960e+00)	Acc@1  27.00 ( 26.73)	Acc@5  62.00 ( 57.08)
 * Acc@1 26.760 Acc@5 57.170
### epoch[2] execution time: 15.739623785018921
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.191 ( 0.191)	Data  0.151 ( 0.151)	Loss 2.5781e+00 (2.5781e+00)	Acc@1  31.25 ( 31.25)	Acc@5  64.06 ( 64.06)
Epoch: [3][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 2.6328e+00 (2.7278e+00)	Acc@1  25.78 ( 30.54)	Acc@5  65.62 ( 64.06)
Epoch: [3][ 20/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.009)	Loss 2.4824e+00 (2.7359e+00)	Acc@1  39.06 ( 30.21)	Acc@5  67.97 ( 62.98)
Epoch: [3][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 2.7285e+00 (2.7264e+00)	Acc@1  28.91 ( 30.19)	Acc@5  67.97 ( 63.13)
Epoch: [3][ 40/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 2.6914e+00 (2.7020e+00)	Acc@1  31.25 ( 30.18)	Acc@5  65.62 ( 63.55)
Epoch: [3][ 50/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.005)	Loss 2.7715e+00 (2.6845e+00)	Acc@1  33.59 ( 30.82)	Acc@5  63.28 ( 63.73)
Epoch: [3][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.5332e+00 (2.6847e+00)	Acc@1  35.16 ( 30.88)	Acc@5  62.50 ( 63.42)
Epoch: [3][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.6895e+00 (2.6814e+00)	Acc@1  33.59 ( 31.16)	Acc@5  62.50 ( 63.25)
Epoch: [3][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.5352e+00 (2.6696e+00)	Acc@1  31.25 ( 31.41)	Acc@5  67.19 ( 63.53)
Epoch: [3][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.4453e+00 (2.6654e+00)	Acc@1  33.59 ( 31.41)	Acc@5  70.31 ( 63.50)
Epoch: [3][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.7969e+00 (2.6595e+00)	Acc@1  26.56 ( 31.51)	Acc@5  57.81 ( 63.63)
Epoch: [3][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.7207e+00 (2.6592e+00)	Acc@1  32.81 ( 31.49)	Acc@5  58.59 ( 63.78)
Epoch: [3][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.5605e+00 (2.6624e+00)	Acc@1  32.03 ( 31.51)	Acc@5  67.97 ( 63.86)
Epoch: [3][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.6152e+00 (2.6533e+00)	Acc@1  33.59 ( 31.77)	Acc@5  64.84 ( 64.13)
Epoch: [3][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.4902e+00 (2.6540e+00)	Acc@1  36.72 ( 31.68)	Acc@5  71.09 ( 64.07)
Epoch: [3][150/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.7559e+00 (2.6470e+00)	Acc@1  28.91 ( 31.78)	Acc@5  62.50 ( 64.28)
Epoch: [3][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.7637e+00 (2.6451e+00)	Acc@1  28.91 ( 31.82)	Acc@5  60.94 ( 64.34)
Epoch: [3][170/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.002)	Loss 2.4062e+00 (2.6423e+00)	Acc@1  40.62 ( 31.92)	Acc@5  68.75 ( 64.43)
Epoch: [3][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.5781e+00 (2.6393e+00)	Acc@1  36.72 ( 31.98)	Acc@5  68.75 ( 64.50)
Epoch: [3][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.5605e+00 (2.6354e+00)	Acc@1  34.38 ( 32.02)	Acc@5  65.62 ( 64.55)
Epoch: [3][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.5469e+00 (2.6298e+00)	Acc@1  33.59 ( 32.12)	Acc@5  60.94 ( 64.54)
Epoch: [3][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.4648e+00 (2.6240e+00)	Acc@1  32.81 ( 32.15)	Acc@5  67.19 ( 64.64)
Epoch: [3][220/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.6328e+00 (2.6245e+00)	Acc@1  32.03 ( 32.12)	Acc@5  61.72 ( 64.64)
Epoch: [3][230/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.4004e+00 (2.6213e+00)	Acc@1  39.84 ( 32.27)	Acc@5  69.53 ( 64.74)
Epoch: [3][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.4355e+00 (2.6183e+00)	Acc@1  37.50 ( 32.45)	Acc@5  67.97 ( 64.82)
Epoch: [3][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.4219e+00 (2.6132e+00)	Acc@1  33.59 ( 32.55)	Acc@5  71.88 ( 64.91)
Epoch: [3][260/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2695e+00 (2.6061e+00)	Acc@1  39.84 ( 32.65)	Acc@5  75.78 ( 65.09)
Epoch: [3][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6270e+00 (2.6024e+00)	Acc@1  32.81 ( 32.81)	Acc@5  64.06 ( 65.15)
Epoch: [3][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.5938e+00 (2.5976e+00)	Acc@1  32.03 ( 32.87)	Acc@5  65.62 ( 65.27)
Epoch: [3][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6797e+00 (2.5914e+00)	Acc@1  33.59 ( 32.98)	Acc@5  60.94 ( 65.42)
Epoch: [3][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3633e+00 (2.5862e+00)	Acc@1  35.16 ( 33.04)	Acc@5  75.78 ( 65.53)
Epoch: [3][310/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6836e+00 (2.5815e+00)	Acc@1  29.69 ( 33.17)	Acc@5  65.62 ( 65.61)
Epoch: [3][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.4961e+00 (2.5777e+00)	Acc@1  33.59 ( 33.19)	Acc@5  69.53 ( 65.67)
Epoch: [3][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3008e+00 (2.5713e+00)	Acc@1  29.69 ( 33.31)	Acc@5  70.31 ( 65.85)
Epoch: [3][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3711e+00 (2.5654e+00)	Acc@1  35.94 ( 33.48)	Acc@5  72.66 ( 65.98)
Epoch: [3][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.4453e+00 (2.5631e+00)	Acc@1  32.03 ( 33.54)	Acc@5  68.75 ( 66.05)
Epoch: [3][360/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.4395e+00 (2.5583e+00)	Acc@1  34.38 ( 33.65)	Acc@5  69.53 ( 66.15)
Epoch: [3][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2715e+00 (2.5513e+00)	Acc@1  41.41 ( 33.78)	Acc@5  71.88 ( 66.28)
Epoch: [3][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3398e+00 (2.5481e+00)	Acc@1  33.59 ( 33.90)	Acc@5  69.53 ( 66.33)
Epoch: [3][390/391]	Time  0.026 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1016e+00 (2.5432e+00)	Acc@1  32.50 ( 33.98)	Acc@5  73.75 ( 66.43)
## e[3] optimizer.zero_grad (sum) time: 0.17997956275939941
## e[3]       loss.backward (sum) time: 3.987670660018921
## e[3]      optimizer.step (sum) time: 1.2630946636199951
## epoch[3] training(only) time: 13.823487281799316
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 2.7422e+00 (2.7422e+00)	Acc@1  35.00 ( 35.00)	Acc@5  63.00 ( 63.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 2.9219e+00 (2.7724e+00)	Acc@1  26.00 ( 32.55)	Acc@5  62.00 ( 64.27)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 2.6230e+00 (2.7406e+00)	Acc@1  32.00 ( 32.10)	Acc@5  59.00 ( 63.14)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 2.6660e+00 (2.7319e+00)	Acc@1  26.00 ( 32.26)	Acc@5  62.00 ( 63.68)
Test: [ 40/100]	Time  0.013 ( 0.021)	Loss 2.7480e+00 (2.7541e+00)	Acc@1  33.00 ( 32.00)	Acc@5  66.00 ( 63.20)
Test: [ 50/100]	Time  0.020 ( 0.021)	Loss 2.6133e+00 (2.7641e+00)	Acc@1  35.00 ( 31.96)	Acc@5  62.00 ( 63.18)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 2.8555e+00 (2.7628e+00)	Acc@1  32.00 ( 32.18)	Acc@5  65.00 ( 63.36)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 2.9277e+00 (2.7721e+00)	Acc@1  27.00 ( 32.15)	Acc@5  69.00 ( 63.35)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.7793e+00 (2.7761e+00)	Acc@1  25.00 ( 32.04)	Acc@5  64.00 ( 63.21)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 2.9961e+00 (2.7635e+00)	Acc@1  26.00 ( 32.18)	Acc@5  54.00 ( 63.44)
 * Acc@1 32.120 Acc@5 63.420
### epoch[3] execution time: 15.8213050365448
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.184 ( 0.184)	Data  0.144 ( 0.144)	Loss 2.2617e+00 (2.2617e+00)	Acc@1  42.19 ( 42.19)	Acc@5  73.44 ( 73.44)
Epoch: [4][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.015)	Loss 2.2090e+00 (2.3097e+00)	Acc@1  41.41 ( 39.35)	Acc@5  75.00 ( 71.31)
Epoch: [4][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.008)	Loss 2.0820e+00 (2.2959e+00)	Acc@1  43.75 ( 38.39)	Acc@5  74.22 ( 72.25)
Epoch: [4][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 2.0625e+00 (2.2810e+00)	Acc@1  42.97 ( 39.39)	Acc@5  78.91 ( 72.71)
Epoch: [4][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 2.1934e+00 (2.2725e+00)	Acc@1  42.19 ( 39.42)	Acc@5  73.44 ( 72.87)
Epoch: [4][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 2.2324e+00 (2.2758e+00)	Acc@1  37.50 ( 39.17)	Acc@5  69.53 ( 72.69)
Epoch: [4][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.2793e+00 (2.2933e+00)	Acc@1  35.16 ( 38.92)	Acc@5  74.22 ( 72.16)
Epoch: [4][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.1797e+00 (2.2877e+00)	Acc@1  40.62 ( 39.16)	Acc@5  70.31 ( 72.22)
Epoch: [4][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.0859e+00 (2.2700e+00)	Acc@1  46.88 ( 39.70)	Acc@5  75.78 ( 72.58)
Epoch: [4][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.1133e+00 (2.2653e+00)	Acc@1  43.75 ( 39.54)	Acc@5  74.22 ( 72.72)
Epoch: [4][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.0469e+00 (2.2610e+00)	Acc@1  46.88 ( 39.63)	Acc@5  76.56 ( 72.82)
Epoch: [4][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.4160e+00 (2.2573e+00)	Acc@1  28.12 ( 39.85)	Acc@5  67.19 ( 72.89)
Epoch: [4][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.3047e+00 (2.2567e+00)	Acc@1  42.97 ( 39.88)	Acc@5  75.00 ( 72.94)
Epoch: [4][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.3691e+00 (2.2539e+00)	Acc@1  39.06 ( 39.93)	Acc@5  68.75 ( 72.82)
Epoch: [4][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.4570e+00 (2.2530e+00)	Acc@1  31.25 ( 39.92)	Acc@5  73.44 ( 72.76)
Epoch: [4][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.2031e+00 (2.2569e+00)	Acc@1  35.16 ( 39.78)	Acc@5  71.09 ( 72.60)
Epoch: [4][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.2559e+00 (2.2521e+00)	Acc@1  38.28 ( 39.94)	Acc@5  73.44 ( 72.79)
Epoch: [4][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.3008e+00 (2.2495e+00)	Acc@1  39.84 ( 39.98)	Acc@5  71.88 ( 72.76)
Epoch: [4][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.0684e+00 (2.2492e+00)	Acc@1  42.19 ( 39.91)	Acc@5  75.00 ( 72.78)
Epoch: [4][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.3184e+00 (2.2450e+00)	Acc@1  39.84 ( 39.91)	Acc@5  71.09 ( 72.86)
Epoch: [4][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.1016e+00 (2.2412e+00)	Acc@1  39.84 ( 40.14)	Acc@5  77.34 ( 72.99)
Epoch: [4][210/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 2.2246e+00 (2.2371e+00)	Acc@1  41.41 ( 40.13)	Acc@5  71.88 ( 73.08)
Epoch: [4][220/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2363e+00 (2.2345e+00)	Acc@1  42.97 ( 40.17)	Acc@5  72.66 ( 73.11)
Epoch: [4][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.0195e+00 (2.2355e+00)	Acc@1  43.75 ( 40.14)	Acc@5  78.91 ( 73.12)
Epoch: [4][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3496e+00 (2.2379e+00)	Acc@1  34.38 ( 40.10)	Acc@5  70.31 ( 73.08)
Epoch: [4][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3438e+00 (2.2359e+00)	Acc@1  40.62 ( 40.21)	Acc@5  68.75 ( 73.06)
Epoch: [4][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1602e+00 (2.2285e+00)	Acc@1  45.31 ( 40.32)	Acc@5  72.66 ( 73.21)
Epoch: [4][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1211e+00 (2.2255e+00)	Acc@1  41.41 ( 40.41)	Acc@5  75.00 ( 73.27)
Epoch: [4][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1191e+00 (2.2205e+00)	Acc@1  46.09 ( 40.51)	Acc@5  76.56 ( 73.38)
Epoch: [4][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1406e+00 (2.2158e+00)	Acc@1  42.97 ( 40.57)	Acc@5  75.00 ( 73.50)
Epoch: [4][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.9482e+00 (2.2144e+00)	Acc@1  46.09 ( 40.59)	Acc@5  82.81 ( 73.48)
Epoch: [4][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.0918e+00 (2.2108e+00)	Acc@1  39.84 ( 40.64)	Acc@5  79.69 ( 73.58)
Epoch: [4][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.0020e+00 (2.2092e+00)	Acc@1  47.66 ( 40.69)	Acc@5  82.81 ( 73.64)
Epoch: [4][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2148e+00 (2.2073e+00)	Acc@1  40.62 ( 40.72)	Acc@5  73.44 ( 73.73)
Epoch: [4][340/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1504e+00 (2.2053e+00)	Acc@1  43.75 ( 40.79)	Acc@5  71.09 ( 73.73)
Epoch: [4][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.0254e+00 (2.2043e+00)	Acc@1  46.09 ( 40.84)	Acc@5  74.22 ( 73.71)
Epoch: [4][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.4531e+00 (2.2009e+00)	Acc@1  38.28 ( 40.95)	Acc@5  70.31 ( 73.75)
Epoch: [4][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1270e+00 (2.1968e+00)	Acc@1  41.41 ( 41.05)	Acc@5  70.31 ( 73.81)
Epoch: [4][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.0918e+00 (2.1956e+00)	Acc@1  42.97 ( 41.10)	Acc@5  74.22 ( 73.78)
Epoch: [4][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.5684e+00 (2.1927e+00)	Acc@1  38.75 ( 41.16)	Acc@5  66.25 ( 73.85)
## e[4] optimizer.zero_grad (sum) time: 0.17900657653808594
## e[4]       loss.backward (sum) time: 4.04242467880249
## e[4]      optimizer.step (sum) time: 1.2502272129058838
## epoch[4] training(only) time: 13.797492742538452
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 2.1367e+00 (2.1367e+00)	Acc@1  41.00 ( 41.00)	Acc@5  73.00 ( 73.00)
Test: [ 10/100]	Time  0.016 ( 0.029)	Loss 2.2637e+00 (2.1870e+00)	Acc@1  36.00 ( 42.73)	Acc@5  71.00 ( 73.91)
Test: [ 20/100]	Time  0.023 ( 0.023)	Loss 1.8623e+00 (2.1193e+00)	Acc@1  46.00 ( 43.67)	Acc@5  85.00 ( 74.90)
Test: [ 30/100]	Time  0.018 ( 0.021)	Loss 2.1582e+00 (2.1323e+00)	Acc@1  40.00 ( 43.29)	Acc@5  75.00 ( 74.74)
Test: [ 40/100]	Time  0.015 ( 0.020)	Loss 1.9678e+00 (2.1357e+00)	Acc@1  50.00 ( 43.39)	Acc@5  81.00 ( 74.98)
Test: [ 50/100]	Time  0.019 ( 0.020)	Loss 2.1289e+00 (2.1559e+00)	Acc@1  45.00 ( 43.27)	Acc@5  75.00 ( 74.49)
Test: [ 60/100]	Time  0.015 ( 0.019)	Loss 2.0527e+00 (2.1491e+00)	Acc@1  47.00 ( 43.39)	Acc@5  76.00 ( 74.43)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 2.2207e+00 (2.1533e+00)	Acc@1  40.00 ( 43.13)	Acc@5  77.00 ( 74.42)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 2.2773e+00 (2.1563e+00)	Acc@1  40.00 ( 43.04)	Acc@5  70.00 ( 74.56)
Test: [ 90/100]	Time  0.023 ( 0.018)	Loss 2.0645e+00 (2.1493e+00)	Acc@1  44.00 ( 43.16)	Acc@5  71.00 ( 74.77)
 * Acc@1 42.960 Acc@5 74.700
### epoch[4] execution time: 15.716142654418945
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.184 ( 0.184)	Data  0.137 ( 0.137)	Loss 2.0430e+00 (2.0430e+00)	Acc@1  44.53 ( 44.53)	Acc@5  74.22 ( 74.22)
Epoch: [5][ 10/391]	Time  0.035 ( 0.048)	Data  0.001 ( 0.014)	Loss 1.8691e+00 (1.9513e+00)	Acc@1  48.44 ( 46.88)	Acc@5  78.12 ( 79.19)
Epoch: [5][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 2.1328e+00 (1.9129e+00)	Acc@1  42.97 ( 47.40)	Acc@5  76.56 ( 79.80)
Epoch: [5][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.006)	Loss 2.1660e+00 (1.9085e+00)	Acc@1  43.75 ( 47.28)	Acc@5  70.31 ( 79.71)
Epoch: [5][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 2.2500e+00 (1.9387e+00)	Acc@1  42.19 ( 46.17)	Acc@5  72.66 ( 79.33)
Epoch: [5][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.6797e+00 (1.9578e+00)	Acc@1  55.47 ( 45.91)	Acc@5  84.38 ( 78.88)
Epoch: [5][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.1172e+00 (1.9582e+00)	Acc@1  42.97 ( 45.67)	Acc@5  79.69 ( 78.97)
Epoch: [5][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.0938e+00 (1.9645e+00)	Acc@1  40.62 ( 45.43)	Acc@5  75.78 ( 78.87)
Epoch: [5][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.9736e+00 (1.9702e+00)	Acc@1  41.41 ( 45.24)	Acc@5  78.91 ( 78.72)
Epoch: [5][ 90/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.8232e+00 (1.9751e+00)	Acc@1  42.97 ( 45.08)	Acc@5  82.03 ( 78.39)
Epoch: [5][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.8135e+00 (1.9636e+00)	Acc@1  49.22 ( 45.39)	Acc@5  81.25 ( 78.57)
Epoch: [5][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.9727e+00 (1.9619e+00)	Acc@1  49.22 ( 45.54)	Acc@5  78.12 ( 78.60)
Epoch: [5][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.8330e+00 (1.9652e+00)	Acc@1  47.66 ( 45.54)	Acc@5  82.81 ( 78.49)
Epoch: [5][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.0312e+00 (1.9639e+00)	Acc@1  44.53 ( 45.70)	Acc@5  77.34 ( 78.58)
Epoch: [5][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.3105e+00 (1.9601e+00)	Acc@1  42.19 ( 45.86)	Acc@5  72.66 ( 78.63)
Epoch: [5][150/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.7773e+00 (1.9623e+00)	Acc@1  47.66 ( 45.85)	Acc@5  83.59 ( 78.58)
Epoch: [5][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.7383e+00 (1.9646e+00)	Acc@1  50.00 ( 45.74)	Acc@5  84.38 ( 78.46)
Epoch: [5][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.6992e+00 (1.9628e+00)	Acc@1  52.34 ( 45.83)	Acc@5  80.47 ( 78.46)
Epoch: [5][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.0625e+00 (1.9604e+00)	Acc@1  44.53 ( 45.90)	Acc@5  77.34 ( 78.52)
Epoch: [5][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.6621e+00 (1.9577e+00)	Acc@1  57.03 ( 46.00)	Acc@5  83.59 ( 78.57)
Epoch: [5][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.6387e+00 (1.9525e+00)	Acc@1  55.47 ( 46.17)	Acc@5  84.38 ( 78.68)
Epoch: [5][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.8076e+00 (1.9511e+00)	Acc@1  50.00 ( 46.30)	Acc@5  82.81 ( 78.72)
Epoch: [5][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.0938e+00 (1.9500e+00)	Acc@1  42.97 ( 46.34)	Acc@5  78.12 ( 78.74)
Epoch: [5][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.7061e+00 (1.9436e+00)	Acc@1  53.91 ( 46.49)	Acc@5  80.47 ( 78.85)
Epoch: [5][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.9248e+00 (1.9422e+00)	Acc@1  46.09 ( 46.55)	Acc@5  82.03 ( 78.87)
Epoch: [5][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.8594e+00 (1.9398e+00)	Acc@1  48.44 ( 46.66)	Acc@5  77.34 ( 78.86)
Epoch: [5][260/391]	Time  0.039 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7178e+00 (1.9397e+00)	Acc@1  50.00 ( 46.63)	Acc@5  82.81 ( 78.80)
Epoch: [5][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.9883e+00 (1.9367e+00)	Acc@1  44.53 ( 46.74)	Acc@5  78.12 ( 78.86)
Epoch: [5][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7969e+00 (1.9353e+00)	Acc@1  43.75 ( 46.79)	Acc@5  84.38 ( 78.91)
Epoch: [5][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.8037e+00 (1.9333e+00)	Acc@1  49.22 ( 46.85)	Acc@5  78.12 ( 78.95)
Epoch: [5][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.8174e+00 (1.9351e+00)	Acc@1  50.00 ( 46.84)	Acc@5  82.81 ( 78.93)
Epoch: [5][310/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7305e+00 (1.9338e+00)	Acc@1  49.22 ( 46.89)	Acc@5  81.25 ( 78.93)
Epoch: [5][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.0000e+00 (1.9316e+00)	Acc@1  46.88 ( 46.97)	Acc@5  81.25 ( 78.99)
Epoch: [5][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6416e+00 (1.9289e+00)	Acc@1  54.69 ( 47.05)	Acc@5  85.94 ( 79.06)
Epoch: [5][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2402e+00 (1.9287e+00)	Acc@1  38.28 ( 47.07)	Acc@5  74.22 ( 79.06)
Epoch: [5][350/391]	Time  0.039 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.8936e+00 (1.9270e+00)	Acc@1  51.56 ( 47.09)	Acc@5  82.03 ( 79.10)
Epoch: [5][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.9072e+00 (1.9267e+00)	Acc@1  50.00 ( 47.11)	Acc@5  80.47 ( 79.12)
Epoch: [5][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.0820e+00 (1.9245e+00)	Acc@1  43.75 ( 47.16)	Acc@5  75.78 ( 79.14)
Epoch: [5][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.8740e+00 (1.9231e+00)	Acc@1  50.00 ( 47.19)	Acc@5  81.25 ( 79.18)
Epoch: [5][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7617e+00 (1.9190e+00)	Acc@1  55.00 ( 47.31)	Acc@5  83.75 ( 79.27)
## e[5] optimizer.zero_grad (sum) time: 0.17895221710205078
## e[5]       loss.backward (sum) time: 3.992760419845581
## e[5]      optimizer.step (sum) time: 1.254993200302124
## epoch[5] training(only) time: 13.88066029548645
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.8721e+00 (1.8721e+00)	Acc@1  50.00 ( 50.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.016 ( 0.029)	Loss 2.1094e+00 (2.0020e+00)	Acc@1  38.00 ( 47.18)	Acc@5  82.00 ( 78.09)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.8945e+00 (1.9492e+00)	Acc@1  48.00 ( 47.14)	Acc@5  81.00 ( 79.38)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 2.0332e+00 (1.9479e+00)	Acc@1  45.00 ( 47.10)	Acc@5  82.00 ( 79.58)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 2.0352e+00 (1.9524e+00)	Acc@1  46.00 ( 47.49)	Acc@5  85.00 ( 79.49)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.9697e+00 (1.9752e+00)	Acc@1  51.00 ( 47.16)	Acc@5  75.00 ( 78.84)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 1.8926e+00 (1.9678e+00)	Acc@1  46.00 ( 47.02)	Acc@5  79.00 ( 78.87)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 2.0781e+00 (1.9713e+00)	Acc@1  48.00 ( 46.99)	Acc@5  80.00 ( 78.85)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 2.1660e+00 (1.9762e+00)	Acc@1  49.00 ( 47.02)	Acc@5  68.00 ( 78.56)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 2.0352e+00 (1.9713e+00)	Acc@1  46.00 ( 47.32)	Acc@5  77.00 ( 78.62)
 * Acc@1 47.410 Acc@5 78.760
### epoch[5] execution time: 15.838346004486084
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.189 ( 0.189)	Data  0.148 ( 0.148)	Loss 1.6338e+00 (1.6338e+00)	Acc@1  53.12 ( 53.12)	Acc@5  85.94 ( 85.94)
Epoch: [6][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 1.8623e+00 (1.6657e+00)	Acc@1  45.31 ( 52.56)	Acc@5  79.69 ( 83.17)
Epoch: [6][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.008)	Loss 1.9102e+00 (1.7054e+00)	Acc@1  46.88 ( 51.75)	Acc@5  80.47 ( 83.15)
Epoch: [6][ 30/391]	Time  0.040 ( 0.039)	Data  0.001 ( 0.006)	Loss 2.1875e+00 (1.7531e+00)	Acc@1  42.19 ( 50.93)	Acc@5  80.47 ( 82.61)
Epoch: [6][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.9443e+00 (1.7598e+00)	Acc@1  49.22 ( 50.80)	Acc@5  75.78 ( 82.05)
Epoch: [6][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.6963e+00 (1.7616e+00)	Acc@1  59.38 ( 50.95)	Acc@5  83.59 ( 82.23)
Epoch: [6][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.5547e+00 (1.7467e+00)	Acc@1  57.81 ( 51.40)	Acc@5  81.25 ( 82.44)
Epoch: [6][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.7598e+00 (1.7521e+00)	Acc@1  47.66 ( 51.16)	Acc@5  84.38 ( 82.37)
Epoch: [6][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.5811e+00 (1.7357e+00)	Acc@1  54.69 ( 51.59)	Acc@5  81.25 ( 82.62)
Epoch: [6][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.7627e+00 (1.7282e+00)	Acc@1  48.44 ( 51.70)	Acc@5  82.03 ( 82.74)
Epoch: [6][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.6104e+00 (1.7270e+00)	Acc@1  54.69 ( 51.60)	Acc@5  83.59 ( 82.76)
Epoch: [6][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.6279e+00 (1.7287e+00)	Acc@1  57.03 ( 51.55)	Acc@5  81.25 ( 82.76)
Epoch: [6][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.7051e+00 (1.7294e+00)	Acc@1  54.69 ( 51.40)	Acc@5  84.38 ( 82.72)
Epoch: [6][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.7188e+00 (1.7272e+00)	Acc@1  52.34 ( 51.51)	Acc@5  84.38 ( 82.71)
Epoch: [6][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.7471e+00 (1.7223e+00)	Acc@1  50.00 ( 51.62)	Acc@5  81.25 ( 82.76)
Epoch: [6][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.7695e+00 (1.7196e+00)	Acc@1  53.12 ( 51.73)	Acc@5  85.16 ( 82.85)
Epoch: [6][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.8164e+00 (1.7293e+00)	Acc@1  52.34 ( 51.59)	Acc@5  79.69 ( 82.64)
Epoch: [6][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.5791e+00 (1.7339e+00)	Acc@1  50.00 ( 51.59)	Acc@5  83.59 ( 82.52)
Epoch: [6][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.6572e+00 (1.7327e+00)	Acc@1  48.44 ( 51.58)	Acc@5  80.47 ( 82.48)
Epoch: [6][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.7979e+00 (1.7305e+00)	Acc@1  43.75 ( 51.62)	Acc@5  81.25 ( 82.49)
Epoch: [6][200/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.5410e+00 (1.7320e+00)	Acc@1  54.69 ( 51.59)	Acc@5  87.50 ( 82.46)
Epoch: [6][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.8086e+00 (1.7297e+00)	Acc@1  53.12 ( 51.66)	Acc@5  78.91 ( 82.49)
Epoch: [6][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.9697e+00 (1.7295e+00)	Acc@1  46.09 ( 51.65)	Acc@5  81.25 ( 82.49)
Epoch: [6][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6055e+00 (1.7270e+00)	Acc@1  53.91 ( 51.70)	Acc@5  84.38 ( 82.51)
Epoch: [6][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6172e+00 (1.7317e+00)	Acc@1  52.34 ( 51.69)	Acc@5  88.28 ( 82.43)
Epoch: [6][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7490e+00 (1.7321e+00)	Acc@1  50.78 ( 51.75)	Acc@5  82.81 ( 82.43)
Epoch: [6][260/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7363e+00 (1.7309e+00)	Acc@1  50.00 ( 51.75)	Acc@5  84.38 ( 82.45)
Epoch: [6][270/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7295e+00 (1.7296e+00)	Acc@1  47.66 ( 51.73)	Acc@5  85.16 ( 82.49)
Epoch: [6][280/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7822e+00 (1.7303e+00)	Acc@1  50.78 ( 51.70)	Acc@5  78.91 ( 82.48)
Epoch: [6][290/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6572e+00 (1.7301e+00)	Acc@1  50.00 ( 51.68)	Acc@5  83.59 ( 82.51)
Epoch: [6][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6914e+00 (1.7292e+00)	Acc@1  57.81 ( 51.67)	Acc@5  85.16 ( 82.53)
Epoch: [6][310/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5303e+00 (1.7297e+00)	Acc@1  55.47 ( 51.65)	Acc@5  86.72 ( 82.55)
Epoch: [6][320/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7578e+00 (1.7294e+00)	Acc@1  51.56 ( 51.68)	Acc@5  82.03 ( 82.53)
Epoch: [6][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6621e+00 (1.7274e+00)	Acc@1  57.03 ( 51.71)	Acc@5  82.03 ( 82.58)
Epoch: [6][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.8115e+00 (1.7253e+00)	Acc@1  45.31 ( 51.76)	Acc@5  82.81 ( 82.60)
Epoch: [6][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5654e+00 (1.7242e+00)	Acc@1  60.16 ( 51.79)	Acc@5  83.59 ( 82.64)
Epoch: [6][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6807e+00 (1.7253e+00)	Acc@1  53.91 ( 51.81)	Acc@5  80.47 ( 82.61)
Epoch: [6][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6445e+00 (1.7263e+00)	Acc@1  46.09 ( 51.77)	Acc@5  84.38 ( 82.60)
Epoch: [6][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5869e+00 (1.7236e+00)	Acc@1  54.69 ( 51.80)	Acc@5  85.94 ( 82.68)
Epoch: [6][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1230e+00 (1.7236e+00)	Acc@1  43.75 ( 51.82)	Acc@5  75.00 ( 82.66)
## e[6] optimizer.zero_grad (sum) time: 0.1774609088897705
## e[6]       loss.backward (sum) time: 3.986595392227173
## e[6]      optimizer.step (sum) time: 1.2709698677062988
## epoch[6] training(only) time: 13.809284925460815
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.6592e+00 (1.6592e+00)	Acc@1  57.00 ( 57.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 1.7842e+00 (1.8043e+00)	Acc@1  51.00 ( 52.45)	Acc@5  84.00 ( 81.36)
Test: [ 20/100]	Time  0.024 ( 0.025)	Loss 1.5615e+00 (1.7825e+00)	Acc@1  58.00 ( 51.90)	Acc@5  84.00 ( 81.67)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.8232e+00 (1.7942e+00)	Acc@1  46.00 ( 51.23)	Acc@5  81.00 ( 81.42)
Test: [ 40/100]	Time  0.029 ( 0.021)	Loss 1.6689e+00 (1.7847e+00)	Acc@1  50.00 ( 51.27)	Acc@5  83.00 ( 81.51)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.6377e+00 (1.7961e+00)	Acc@1  52.00 ( 51.04)	Acc@5  80.00 ( 81.00)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.7373e+00 (1.7887e+00)	Acc@1  51.00 ( 51.31)	Acc@5  79.00 ( 81.20)
Test: [ 70/100]	Time  0.013 ( 0.020)	Loss 1.8027e+00 (1.7862e+00)	Acc@1  50.00 ( 51.38)	Acc@5  84.00 ( 81.39)
Test: [ 80/100]	Time  0.022 ( 0.020)	Loss 1.7314e+00 (1.7886e+00)	Acc@1  54.00 ( 51.28)	Acc@5  80.00 ( 81.30)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.8838e+00 (1.7726e+00)	Acc@1  54.00 ( 51.84)	Acc@5  79.00 ( 81.53)
 * Acc@1 51.940 Acc@5 81.640
### epoch[6] execution time: 15.791789770126343
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.196 ( 0.196)	Data  0.150 ( 0.150)	Loss 1.4004e+00 (1.4004e+00)	Acc@1  62.50 ( 62.50)	Acc@5  89.06 ( 89.06)
Epoch: [7][ 10/391]	Time  0.036 ( 0.049)	Data  0.001 ( 0.015)	Loss 1.5674e+00 (1.5783e+00)	Acc@1  55.47 ( 55.75)	Acc@5  86.72 ( 85.80)
Epoch: [7][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 1.4531e+00 (1.5481e+00)	Acc@1  60.16 ( 56.99)	Acc@5  86.72 ( 85.53)
Epoch: [7][ 30/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.006)	Loss 1.4219e+00 (1.5409e+00)	Acc@1  57.81 ( 56.55)	Acc@5  89.06 ( 85.76)
Epoch: [7][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.5713e+00 (1.5494e+00)	Acc@1  64.84 ( 56.57)	Acc@5  86.72 ( 85.79)
Epoch: [7][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.6289e+00 (1.5515e+00)	Acc@1  46.88 ( 56.43)	Acc@5  85.16 ( 85.60)
Epoch: [7][ 60/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.5518e+00 (1.5544e+00)	Acc@1  52.34 ( 56.30)	Acc@5  85.16 ( 85.58)
Epoch: [7][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.7490e+00 (1.5632e+00)	Acc@1  50.00 ( 56.05)	Acc@5  80.47 ( 85.24)
Epoch: [7][ 80/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.6445e+00 (1.5680e+00)	Acc@1  52.34 ( 55.91)	Acc@5  85.94 ( 85.22)
Epoch: [7][ 90/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.8408e+00 (1.5672e+00)	Acc@1  53.12 ( 55.94)	Acc@5  82.03 ( 85.29)
Epoch: [7][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.6680e+00 (1.5637e+00)	Acc@1  53.12 ( 55.98)	Acc@5  84.38 ( 85.34)
Epoch: [7][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.5059e+00 (1.5624e+00)	Acc@1  58.59 ( 55.99)	Acc@5  87.50 ( 85.35)
Epoch: [7][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.5381e+00 (1.5600e+00)	Acc@1  54.69 ( 56.07)	Acc@5  87.50 ( 85.32)
Epoch: [7][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.4883e+00 (1.5562e+00)	Acc@1  53.91 ( 56.14)	Acc@5  90.62 ( 85.35)
Epoch: [7][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.6133e+00 (1.5548e+00)	Acc@1  59.38 ( 56.20)	Acc@5  82.03 ( 85.35)
Epoch: [7][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.4365e+00 (1.5566e+00)	Acc@1  57.81 ( 56.10)	Acc@5  89.06 ( 85.32)
Epoch: [7][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.5713e+00 (1.5549e+00)	Acc@1  53.91 ( 56.09)	Acc@5  85.16 ( 85.35)
Epoch: [7][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.4814e+00 (1.5527e+00)	Acc@1  56.25 ( 56.04)	Acc@5  85.94 ( 85.43)
Epoch: [7][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.5205e+00 (1.5536e+00)	Acc@1  58.59 ( 56.02)	Acc@5  85.16 ( 85.45)
Epoch: [7][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.5098e+00 (1.5539e+00)	Acc@1  57.03 ( 56.02)	Acc@5  85.94 ( 85.41)
Epoch: [7][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.4131e+00 (1.5564e+00)	Acc@1  58.59 ( 55.94)	Acc@5  90.62 ( 85.40)
Epoch: [7][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2480e+00 (1.5552e+00)	Acc@1  61.72 ( 55.98)	Acc@5  91.41 ( 85.39)
Epoch: [7][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.4717e+00 (1.5579e+00)	Acc@1  60.16 ( 55.86)	Acc@5  83.59 ( 85.29)
Epoch: [7][230/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.4365e+00 (1.5579e+00)	Acc@1  60.16 ( 55.85)	Acc@5  85.94 ( 85.31)
Epoch: [7][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.6572e+00 (1.5581e+00)	Acc@1  53.91 ( 55.86)	Acc@5  81.25 ( 85.31)
Epoch: [7][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5273e+00 (1.5560e+00)	Acc@1  57.03 ( 55.94)	Acc@5  85.94 ( 85.35)
Epoch: [7][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7500e+00 (1.5550e+00)	Acc@1  46.09 ( 55.95)	Acc@5  84.38 ( 85.38)
Epoch: [7][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6406e+00 (1.5566e+00)	Acc@1  53.12 ( 55.97)	Acc@5  82.81 ( 85.36)
Epoch: [7][280/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6475e+00 (1.5563e+00)	Acc@1  52.34 ( 56.00)	Acc@5  79.69 ( 85.36)
Epoch: [7][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6289e+00 (1.5561e+00)	Acc@1  55.47 ( 55.93)	Acc@5  80.47 ( 85.36)
Epoch: [7][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5020e+00 (1.5572e+00)	Acc@1  57.03 ( 55.93)	Acc@5  89.06 ( 85.36)
Epoch: [7][310/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4512e+00 (1.5558e+00)	Acc@1  60.16 ( 55.96)	Acc@5  85.16 ( 85.36)
Epoch: [7][320/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5039e+00 (1.5550e+00)	Acc@1  52.34 ( 55.95)	Acc@5  84.38 ( 85.40)
Epoch: [7][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5947e+00 (1.5548e+00)	Acc@1  53.91 ( 55.94)	Acc@5  82.81 ( 85.38)
Epoch: [7][340/391]	Time  0.034 ( 0.035)	Data  0.000 ( 0.002)	Loss 1.4102e+00 (1.5542e+00)	Acc@1  60.94 ( 56.00)	Acc@5  85.16 ( 85.36)
Epoch: [7][350/391]	Time  0.035 ( 0.035)	Data  0.002 ( 0.002)	Loss 1.5732e+00 (1.5561e+00)	Acc@1  62.50 ( 55.97)	Acc@5  81.25 ( 85.34)
Epoch: [7][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1504e+00 (1.5556e+00)	Acc@1  64.84 ( 56.00)	Acc@5  93.75 ( 85.36)
Epoch: [7][370/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6289e+00 (1.5549e+00)	Acc@1  56.25 ( 56.05)	Acc@5  84.38 ( 85.38)
Epoch: [7][380/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4473e+00 (1.5554e+00)	Acc@1  56.25 ( 56.06)	Acc@5  85.16 ( 85.34)
Epoch: [7][390/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4814e+00 (1.5542e+00)	Acc@1  62.50 ( 56.09)	Acc@5  86.25 ( 85.35)
## e[7] optimizer.zero_grad (sum) time: 0.17830657958984375
## e[7]       loss.backward (sum) time: 3.8653340339660645
## e[7]      optimizer.step (sum) time: 1.2699177265167236
## epoch[7] training(only) time: 13.840583562850952
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.7910e+00 (1.7910e+00)	Acc@1  57.00 ( 57.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 1.6963e+00 (1.7964e+00)	Acc@1  49.00 ( 52.82)	Acc@5  83.00 ( 83.09)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.4873e+00 (1.7591e+00)	Acc@1  62.00 ( 53.14)	Acc@5  87.00 ( 83.33)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.7676e+00 (1.7563e+00)	Acc@1  49.00 ( 52.68)	Acc@5  86.00 ( 83.03)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 1.7842e+00 (1.7434e+00)	Acc@1  48.00 ( 52.37)	Acc@5  89.00 ( 83.12)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5830e+00 (1.7639e+00)	Acc@1  54.00 ( 52.04)	Acc@5  85.00 ( 82.92)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.8330e+00 (1.7528e+00)	Acc@1  51.00 ( 52.05)	Acc@5  81.00 ( 83.03)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.8682e+00 (1.7541e+00)	Acc@1  50.00 ( 51.92)	Acc@5  83.00 ( 82.93)
Test: [ 80/100]	Time  0.022 ( 0.019)	Loss 1.7461e+00 (1.7591e+00)	Acc@1  53.00 ( 51.88)	Acc@5  77.00 ( 82.63)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.9434e+00 (1.7472e+00)	Acc@1  49.00 ( 52.15)	Acc@5  88.00 ( 82.85)
 * Acc@1 52.230 Acc@5 82.900
### epoch[7] execution time: 15.819252729415894
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.185 ( 0.185)	Data  0.132 ( 0.132)	Loss 1.4326e+00 (1.4326e+00)	Acc@1  61.72 ( 61.72)	Acc@5  88.28 ( 88.28)
Epoch: [8][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.013)	Loss 1.5205e+00 (1.4152e+00)	Acc@1  53.12 ( 58.66)	Acc@5  88.28 ( 87.64)
Epoch: [8][ 20/391]	Time  0.033 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.2793e+00 (1.3925e+00)	Acc@1  65.62 ( 59.86)	Acc@5  91.41 ( 87.72)
Epoch: [8][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 1.7598e+00 (1.4152e+00)	Acc@1  46.88 ( 59.02)	Acc@5  84.38 ( 87.65)
Epoch: [8][ 40/391]	Time  0.030 ( 0.039)	Data  0.001 ( 0.005)	Loss 1.2510e+00 (1.4160e+00)	Acc@1  63.28 ( 59.05)	Acc@5  92.19 ( 87.54)
Epoch: [8][ 50/391]	Time  0.031 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.4893e+00 (1.4243e+00)	Acc@1  57.81 ( 59.01)	Acc@5  85.94 ( 87.32)
Epoch: [8][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.3613e+00 (1.4294e+00)	Acc@1  59.38 ( 58.85)	Acc@5  89.06 ( 87.38)
Epoch: [8][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.1787e+00 (1.4154e+00)	Acc@1  61.72 ( 59.24)	Acc@5  93.75 ( 87.60)
Epoch: [8][ 80/391]	Time  0.040 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.5420e+00 (1.4107e+00)	Acc@1  55.47 ( 59.31)	Acc@5  89.06 ( 87.77)
Epoch: [8][ 90/391]	Time  0.036 ( 0.036)	Data  0.003 ( 0.003)	Loss 1.2969e+00 (1.4173e+00)	Acc@1  64.84 ( 59.24)	Acc@5  89.06 ( 87.70)
Epoch: [8][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.2109e+00 (1.4149e+00)	Acc@1  64.06 ( 59.26)	Acc@5  89.84 ( 87.71)
Epoch: [8][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.7422e+00 (1.4184e+00)	Acc@1  54.69 ( 59.18)	Acc@5  82.81 ( 87.65)
Epoch: [8][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.4033e+00 (1.4130e+00)	Acc@1  58.59 ( 59.25)	Acc@5  89.84 ( 87.75)
Epoch: [8][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.3848e+00 (1.4177e+00)	Acc@1  62.50 ( 59.16)	Acc@5  87.50 ( 87.63)
Epoch: [8][140/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.4512e+00 (1.4160e+00)	Acc@1  58.59 ( 59.26)	Acc@5  88.28 ( 87.69)
Epoch: [8][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.5771e+00 (1.4166e+00)	Acc@1  61.72 ( 59.28)	Acc@5  83.59 ( 87.72)
Epoch: [8][160/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1641e+00 (1.4209e+00)	Acc@1  65.62 ( 59.21)	Acc@5  92.97 ( 87.62)
Epoch: [8][170/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4961e+00 (1.4229e+00)	Acc@1  56.25 ( 59.21)	Acc@5  86.72 ( 87.61)
Epoch: [8][180/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3154e+00 (1.4215e+00)	Acc@1  59.38 ( 59.21)	Acc@5  88.28 ( 87.63)
Epoch: [8][190/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3486e+00 (1.4173e+00)	Acc@1  57.81 ( 59.30)	Acc@5  93.75 ( 87.78)
Epoch: [8][200/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.8174e+00 (1.4229e+00)	Acc@1  50.78 ( 59.12)	Acc@5  82.03 ( 87.74)
Epoch: [8][210/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3672e+00 (1.4240e+00)	Acc@1  64.06 ( 59.12)	Acc@5  86.72 ( 87.69)
Epoch: [8][220/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 1.1768e+00 (1.4255e+00)	Acc@1  67.19 ( 59.10)	Acc@5  92.97 ( 87.70)
Epoch: [8][230/391]	Time  0.031 ( 0.035)	Data  0.000 ( 0.002)	Loss 1.7119e+00 (1.4254e+00)	Acc@1  55.47 ( 59.08)	Acc@5  82.81 ( 87.71)
Epoch: [8][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3555e+00 (1.4268e+00)	Acc@1  60.94 ( 59.02)	Acc@5  86.72 ( 87.67)
Epoch: [8][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2373e+00 (1.4268e+00)	Acc@1  67.97 ( 59.05)	Acc@5  90.62 ( 87.66)
Epoch: [8][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4834e+00 (1.4272e+00)	Acc@1  59.38 ( 59.01)	Acc@5  86.72 ( 87.63)
Epoch: [8][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2080e+00 (1.4257e+00)	Acc@1  63.28 ( 59.07)	Acc@5  92.97 ( 87.64)
Epoch: [8][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5332e+00 (1.4277e+00)	Acc@1  55.47 ( 59.01)	Acc@5  83.59 ( 87.58)
Epoch: [8][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1816e+00 (1.4270e+00)	Acc@1  65.62 ( 58.99)	Acc@5  95.31 ( 87.60)
Epoch: [8][300/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3477e+00 (1.4265e+00)	Acc@1  57.03 ( 59.00)	Acc@5  88.28 ( 87.63)
Epoch: [8][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4150e+00 (1.4280e+00)	Acc@1  60.94 ( 58.98)	Acc@5  87.50 ( 87.61)
Epoch: [8][320/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4902e+00 (1.4281e+00)	Acc@1  58.59 ( 58.96)	Acc@5  84.38 ( 87.64)
Epoch: [8][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6719e+00 (1.4266e+00)	Acc@1  53.91 ( 59.00)	Acc@5  78.91 ( 87.62)
Epoch: [8][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7402e+00 (1.4264e+00)	Acc@1  50.00 ( 58.97)	Acc@5  78.91 ( 87.60)
Epoch: [8][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4951e+00 (1.4284e+00)	Acc@1  59.38 ( 58.91)	Acc@5  88.28 ( 87.56)
Epoch: [8][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5127e+00 (1.4284e+00)	Acc@1  59.38 ( 58.97)	Acc@5  85.94 ( 87.54)
Epoch: [8][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6436e+00 (1.4275e+00)	Acc@1  53.91 ( 59.00)	Acc@5  83.59 ( 87.55)
Epoch: [8][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4082e+00 (1.4274e+00)	Acc@1  60.94 ( 59.00)	Acc@5  86.72 ( 87.57)
Epoch: [8][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6572e+00 (1.4285e+00)	Acc@1  51.25 ( 58.99)	Acc@5  85.00 ( 87.56)
## e[8] optimizer.zero_grad (sum) time: 0.1792609691619873
## e[8]       loss.backward (sum) time: 3.9611876010894775
## e[8]      optimizer.step (sum) time: 1.3009483814239502
## epoch[8] training(only) time: 13.753960132598877
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.6689e+00 (1.6689e+00)	Acc@1  56.00 ( 56.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 1.6270e+00 (1.6486e+00)	Acc@1  53.00 ( 55.36)	Acc@5  86.00 ( 83.45)
Test: [ 20/100]	Time  0.026 ( 0.024)	Loss 1.5254e+00 (1.6096e+00)	Acc@1  57.00 ( 55.76)	Acc@5  86.00 ( 84.05)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5820e+00 (1.6194e+00)	Acc@1  53.00 ( 54.90)	Acc@5  86.00 ( 84.13)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.6777e+00 (1.6097e+00)	Acc@1  52.00 ( 55.00)	Acc@5  82.00 ( 84.29)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.5400e+00 (1.6195e+00)	Acc@1  58.00 ( 54.57)	Acc@5  84.00 ( 84.06)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.6084e+00 (1.6122e+00)	Acc@1  52.00 ( 54.46)	Acc@5  85.00 ( 84.20)
Test: [ 70/100]	Time  0.020 ( 0.020)	Loss 1.5732e+00 (1.6054e+00)	Acc@1  56.00 ( 54.82)	Acc@5  87.00 ( 84.42)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.5693e+00 (1.6145e+00)	Acc@1  53.00 ( 54.47)	Acc@5  88.00 ( 84.16)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.9219e+00 (1.6062e+00)	Acc@1  53.00 ( 54.80)	Acc@5  84.00 ( 84.27)
 * Acc@1 54.740 Acc@5 84.230
### epoch[8] execution time: 15.720648288726807
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.184 ( 0.184)	Data  0.144 ( 0.144)	Loss 1.0938e+00 (1.0938e+00)	Acc@1  70.31 ( 70.31)	Acc@5  92.97 ( 92.97)
Epoch: [9][ 10/391]	Time  0.034 ( 0.048)	Data  0.001 ( 0.014)	Loss 1.2217e+00 (1.2878e+00)	Acc@1  63.28 ( 63.99)	Acc@5  92.97 ( 90.48)
Epoch: [9][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 1.1768e+00 (1.3003e+00)	Acc@1  67.97 ( 62.83)	Acc@5  89.84 ( 89.73)
Epoch: [9][ 30/391]	Time  0.038 ( 0.039)	Data  0.001 ( 0.006)	Loss 1.1748e+00 (1.2927e+00)	Acc@1  68.75 ( 63.16)	Acc@5  93.75 ( 89.64)
Epoch: [9][ 40/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.1807e+00 (1.2926e+00)	Acc@1  63.28 ( 62.84)	Acc@5  92.19 ( 89.67)
Epoch: [9][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.5752e+00 (1.3055e+00)	Acc@1  53.91 ( 62.45)	Acc@5  85.16 ( 89.51)
Epoch: [9][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.4141e+00 (1.3077e+00)	Acc@1  60.16 ( 62.33)	Acc@5  89.84 ( 89.43)
Epoch: [9][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.4648e+00 (1.2997e+00)	Acc@1  57.81 ( 62.38)	Acc@5  84.38 ( 89.63)
Epoch: [9][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.2646e+00 (1.3106e+00)	Acc@1  64.84 ( 62.25)	Acc@5  91.41 ( 89.35)
Epoch: [9][ 90/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.4756e+00 (1.3144e+00)	Acc@1  59.38 ( 62.23)	Acc@5  88.28 ( 89.27)
Epoch: [9][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1123e+00 (1.3005e+00)	Acc@1  66.41 ( 62.48)	Acc@5  93.75 ( 89.48)
Epoch: [9][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0420e+00 (1.3010e+00)	Acc@1  74.22 ( 62.51)	Acc@5  92.19 ( 89.37)
Epoch: [9][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.2949e+00 (1.3006e+00)	Acc@1  62.50 ( 62.42)	Acc@5  87.50 ( 89.33)
Epoch: [9][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.2441e+00 (1.3019e+00)	Acc@1  62.50 ( 62.35)	Acc@5  90.62 ( 89.29)
Epoch: [9][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0303e+00 (1.2968e+00)	Acc@1  68.75 ( 62.43)	Acc@5  92.19 ( 89.41)
Epoch: [9][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2490e+00 (1.2931e+00)	Acc@1  60.16 ( 62.53)	Acc@5  89.84 ( 89.42)
Epoch: [9][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0400e+00 (1.2947e+00)	Acc@1  68.75 ( 62.39)	Acc@5  92.19 ( 89.41)
Epoch: [9][170/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1484e+00 (1.2959e+00)	Acc@1  68.75 ( 62.36)	Acc@5  92.19 ( 89.36)
Epoch: [9][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.3779e+00 (1.2983e+00)	Acc@1  61.72 ( 62.38)	Acc@5  87.50 ( 89.33)
Epoch: [9][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.4561e+00 (1.2986e+00)	Acc@1  56.25 ( 62.30)	Acc@5  87.50 ( 89.38)
Epoch: [9][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.3018e+00 (1.3002e+00)	Acc@1  59.38 ( 62.24)	Acc@5  88.28 ( 89.34)
Epoch: [9][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2461e+00 (1.3027e+00)	Acc@1  63.28 ( 62.14)	Acc@5  92.97 ( 89.30)
Epoch: [9][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2637e+00 (1.3032e+00)	Acc@1  62.50 ( 62.09)	Acc@5  92.97 ( 89.33)
Epoch: [9][230/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1113e+00 (1.2995e+00)	Acc@1  67.19 ( 62.18)	Acc@5  92.19 ( 89.39)
Epoch: [9][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1396e+00 (1.3005e+00)	Acc@1  67.97 ( 62.14)	Acc@5  93.75 ( 89.39)
Epoch: [9][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4082e+00 (1.3024e+00)	Acc@1  60.16 ( 62.09)	Acc@5  87.50 ( 89.36)
Epoch: [9][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2520e+00 (1.2992e+00)	Acc@1  71.09 ( 62.21)	Acc@5  87.50 ( 89.39)
Epoch: [9][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4814e+00 (1.2990e+00)	Acc@1  51.56 ( 62.20)	Acc@5  87.50 ( 89.39)
Epoch: [9][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4326e+00 (1.3019e+00)	Acc@1  58.59 ( 62.15)	Acc@5  87.50 ( 89.34)
Epoch: [9][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2822e+00 (1.3014e+00)	Acc@1  61.72 ( 62.18)	Acc@5  91.41 ( 89.38)
Epoch: [9][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4463e+00 (1.3004e+00)	Acc@1  57.81 ( 62.16)	Acc@5  88.28 ( 89.39)
Epoch: [9][310/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4053e+00 (1.3026e+00)	Acc@1  55.47 ( 62.13)	Acc@5  85.94 ( 89.36)
Epoch: [9][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3369e+00 (1.3035e+00)	Acc@1  57.03 ( 62.07)	Acc@5  89.84 ( 89.33)
Epoch: [9][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6484e+00 (1.3055e+00)	Acc@1  56.25 ( 61.99)	Acc@5  83.59 ( 89.34)
Epoch: [9][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5879e+00 (1.3070e+00)	Acc@1  53.12 ( 62.00)	Acc@5  83.59 ( 89.30)
Epoch: [9][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3389e+00 (1.3078e+00)	Acc@1  64.06 ( 62.02)	Acc@5  89.06 ( 89.27)
Epoch: [9][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3086e+00 (1.3072e+00)	Acc@1  65.62 ( 62.02)	Acc@5  89.84 ( 89.30)
Epoch: [9][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2822e+00 (1.3083e+00)	Acc@1  60.16 ( 62.02)	Acc@5  90.62 ( 89.28)
Epoch: [9][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3223e+00 (1.3100e+00)	Acc@1  55.47 ( 61.94)	Acc@5  89.84 ( 89.26)
Epoch: [9][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5254e+00 (1.3102e+00)	Acc@1  57.50 ( 61.94)	Acc@5  86.25 ( 89.25)
## e[9] optimizer.zero_grad (sum) time: 0.17908692359924316
## e[9]       loss.backward (sum) time: 4.044804811477661
## e[9]      optimizer.step (sum) time: 1.2481334209442139
## epoch[9] training(only) time: 13.876041650772095
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.7021e+00 (1.7021e+00)	Acc@1  54.00 ( 54.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.015 ( 0.028)	Loss 1.5693e+00 (1.5866e+00)	Acc@1  55.00 ( 56.91)	Acc@5  90.00 ( 85.73)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.5146e+00 (1.5306e+00)	Acc@1  61.00 ( 57.76)	Acc@5  84.00 ( 86.48)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.7490e+00 (1.5323e+00)	Acc@1  51.00 ( 57.71)	Acc@5  83.00 ( 86.16)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.6045e+00 (1.5289e+00)	Acc@1  60.00 ( 57.76)	Acc@5  88.00 ( 86.29)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.3193e+00 (1.5495e+00)	Acc@1  59.00 ( 57.55)	Acc@5  87.00 ( 85.90)
Test: [ 60/100]	Time  0.019 ( 0.020)	Loss 1.5371e+00 (1.5349e+00)	Acc@1  60.00 ( 57.70)	Acc@5  83.00 ( 86.18)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 1.6875e+00 (1.5357e+00)	Acc@1  54.00 ( 57.66)	Acc@5  81.00 ( 86.07)
Test: [ 80/100]	Time  0.025 ( 0.019)	Loss 1.6025e+00 (1.5450e+00)	Acc@1  61.00 ( 57.70)	Acc@5  82.00 ( 85.78)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.9443e+00 (1.5406e+00)	Acc@1  50.00 ( 57.86)	Acc@5  82.00 ( 85.77)
 * Acc@1 57.930 Acc@5 85.970
### epoch[9] execution time: 15.849220037460327
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.192 ( 0.192)	Data  0.150 ( 0.150)	Loss 1.3262e+00 (1.3262e+00)	Acc@1  62.50 ( 62.50)	Acc@5  85.94 ( 85.94)
Epoch: [10][ 10/391]	Time  0.032 ( 0.049)	Data  0.001 ( 0.015)	Loss 1.0166e+00 (1.1689e+00)	Acc@1  72.66 ( 65.06)	Acc@5  93.75 ( 91.83)
Epoch: [10][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.009)	Loss 1.2100e+00 (1.1791e+00)	Acc@1  64.06 ( 65.62)	Acc@5  92.97 ( 91.26)
Epoch: [10][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 1.2871e+00 (1.1775e+00)	Acc@1  60.16 ( 64.97)	Acc@5  85.16 ( 91.18)
Epoch: [10][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.1885e+00 (1.1908e+00)	Acc@1  69.53 ( 65.17)	Acc@5  89.06 ( 90.87)
Epoch: [10][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.0010e+00 (1.1906e+00)	Acc@1  68.75 ( 65.10)	Acc@5  95.31 ( 90.75)
Epoch: [10][ 60/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.1328e+00 (1.1723e+00)	Acc@1  60.94 ( 65.71)	Acc@5  91.41 ( 90.89)
Epoch: [10][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.0586e+00 (1.1666e+00)	Acc@1  71.09 ( 65.89)	Acc@5  92.97 ( 91.09)
Epoch: [10][ 80/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0664e+00 (1.1706e+00)	Acc@1  69.53 ( 65.76)	Acc@5  91.41 ( 91.01)
Epoch: [10][ 90/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.4727e-01 (1.1715e+00)	Acc@1  69.53 ( 65.63)	Acc@5  94.53 ( 91.01)
Epoch: [10][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.9658e-01 (1.1658e+00)	Acc@1  71.88 ( 65.80)	Acc@5  90.62 ( 91.03)
Epoch: [10][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1162e+00 (1.1596e+00)	Acc@1  64.84 ( 66.02)	Acc@5  92.97 ( 91.16)
Epoch: [10][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.4492e+00 (1.1599e+00)	Acc@1  57.03 ( 66.01)	Acc@5  88.28 ( 91.19)
Epoch: [10][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1748e+00 (1.1636e+00)	Acc@1  64.06 ( 65.88)	Acc@5  91.41 ( 91.21)
Epoch: [10][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.2432e+00 (1.1690e+00)	Acc@1  64.06 ( 65.68)	Acc@5  90.62 ( 91.10)
Epoch: [10][150/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1123e+00 (1.1752e+00)	Acc@1  72.66 ( 65.47)	Acc@5  88.28 ( 90.96)
Epoch: [10][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1777e+00 (1.1765e+00)	Acc@1  66.41 ( 65.44)	Acc@5  91.41 ( 90.94)
Epoch: [10][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2148e+00 (1.1777e+00)	Acc@1  63.28 ( 65.43)	Acc@5  91.41 ( 90.93)
Epoch: [10][180/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1309e+00 (1.1792e+00)	Acc@1  66.41 ( 65.43)	Acc@5  92.97 ( 90.88)
Epoch: [10][190/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1504e+00 (1.1823e+00)	Acc@1  64.84 ( 65.35)	Acc@5  94.53 ( 90.88)
Epoch: [10][200/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2471e+00 (1.1849e+00)	Acc@1  64.84 ( 65.25)	Acc@5  92.19 ( 90.87)
Epoch: [10][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1963e+00 (1.1902e+00)	Acc@1  70.31 ( 65.13)	Acc@5  90.62 ( 90.80)
Epoch: [10][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1748e+00 (1.1918e+00)	Acc@1  64.84 ( 65.13)	Acc@5  92.97 ( 90.81)
Epoch: [10][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2236e+00 (1.1928e+00)	Acc@1  64.84 ( 65.09)	Acc@5  90.62 ( 90.80)
Epoch: [10][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0381e+00 (1.1909e+00)	Acc@1  71.09 ( 65.17)	Acc@5  89.06 ( 90.83)
Epoch: [10][250/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.5312e-01 (1.1914e+00)	Acc@1  71.09 ( 65.14)	Acc@5  92.19 ( 90.81)
Epoch: [10][260/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5986e+00 (1.1944e+00)	Acc@1  50.78 ( 65.03)	Acc@5  89.84 ( 90.77)
Epoch: [10][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2188e+00 (1.1978e+00)	Acc@1  60.94 ( 64.93)	Acc@5  92.19 ( 90.72)
Epoch: [10][280/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3760e+00 (1.1988e+00)	Acc@1  60.94 ( 64.90)	Acc@5  89.84 ( 90.73)
Epoch: [10][290/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1416e+00 (1.1988e+00)	Acc@1  67.97 ( 64.88)	Acc@5  93.75 ( 90.74)
Epoch: [10][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1016e+00 (1.2002e+00)	Acc@1  65.62 ( 64.82)	Acc@5  92.97 ( 90.72)
Epoch: [10][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0928e+00 (1.2005e+00)	Acc@1  67.19 ( 64.78)	Acc@5  92.19 ( 90.72)
Epoch: [10][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1523e+00 (1.2023e+00)	Acc@1  67.19 ( 64.76)	Acc@5  92.19 ( 90.68)
Epoch: [10][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2705e+00 (1.2040e+00)	Acc@1  62.50 ( 64.73)	Acc@5  89.84 ( 90.64)
Epoch: [10][340/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1914e+00 (1.2043e+00)	Acc@1  62.50 ( 64.74)	Acc@5  92.19 ( 90.65)
Epoch: [10][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2930e+00 (1.2082e+00)	Acc@1  60.94 ( 64.62)	Acc@5  92.19 ( 90.62)
Epoch: [10][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0879e+00 (1.2085e+00)	Acc@1  67.19 ( 64.60)	Acc@5  89.84 ( 90.61)
Epoch: [10][370/391]	Time  0.043 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3340e+00 (1.2101e+00)	Acc@1  58.59 ( 64.57)	Acc@5  90.62 ( 90.59)
Epoch: [10][380/391]	Time  0.031 ( 0.035)	Data  0.002 ( 0.002)	Loss 1.3037e+00 (1.2107e+00)	Acc@1  63.28 ( 64.51)	Acc@5  89.84 ( 90.60)
Epoch: [10][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2705e+00 (1.2136e+00)	Acc@1  61.25 ( 64.42)	Acc@5  91.25 ( 90.58)
## e[10] optimizer.zero_grad (sum) time: 0.17972016334533691
## e[10]       loss.backward (sum) time: 4.046259164810181
## e[10]      optimizer.step (sum) time: 1.2532751560211182
## epoch[10] training(only) time: 13.839849710464478
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.5430e+00 (1.5430e+00)	Acc@1  62.00 ( 62.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 1.6357e+00 (1.6044e+00)	Acc@1  60.00 ( 59.55)	Acc@5  85.00 ( 84.64)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.4473e+00 (1.5698e+00)	Acc@1  60.00 ( 58.48)	Acc@5  81.00 ( 85.05)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.8320e+00 (1.5883e+00)	Acc@1  51.00 ( 58.06)	Acc@5  83.00 ( 85.00)
Test: [ 40/100]	Time  0.017 ( 0.020)	Loss 1.5713e+00 (1.5707e+00)	Acc@1  57.00 ( 57.80)	Acc@5  82.00 ( 85.22)
Test: [ 50/100]	Time  0.021 ( 0.020)	Loss 1.4629e+00 (1.5754e+00)	Acc@1  60.00 ( 57.61)	Acc@5  88.00 ( 84.90)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.4980e+00 (1.5570e+00)	Acc@1  59.00 ( 58.02)	Acc@5  82.00 ( 85.31)
Test: [ 70/100]	Time  0.021 ( 0.020)	Loss 1.6396e+00 (1.5541e+00)	Acc@1  60.00 ( 58.17)	Acc@5  85.00 ( 85.39)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.7822e+00 (1.5653e+00)	Acc@1  51.00 ( 58.02)	Acc@5  81.00 ( 85.14)
Test: [ 90/100]	Time  0.019 ( 0.019)	Loss 1.8447e+00 (1.5587e+00)	Acc@1  55.00 ( 58.15)	Acc@5  84.00 ( 85.30)
 * Acc@1 58.290 Acc@5 85.420
### epoch[10] execution time: 15.844630479812622
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.188 ( 0.188)	Data  0.148 ( 0.148)	Loss 1.0723e+00 (1.0723e+00)	Acc@1  68.75 ( 68.75)	Acc@5  90.62 ( 90.62)
Epoch: [11][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 1.3047e+00 (1.1377e+00)	Acc@1  64.06 ( 67.68)	Acc@5  87.50 ( 91.48)
Epoch: [11][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 1.0840e+00 (1.1326e+00)	Acc@1  67.97 ( 68.01)	Acc@5  92.19 ( 91.52)
Epoch: [11][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 9.9365e-01 (1.1149e+00)	Acc@1  75.00 ( 67.87)	Acc@5  92.97 ( 91.83)
Epoch: [11][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 9.6729e-01 (1.0984e+00)	Acc@1  70.31 ( 68.08)	Acc@5  91.41 ( 92.04)
Epoch: [11][ 50/391]	Time  0.031 ( 0.038)	Data  0.001 ( 0.004)	Loss 9.8145e-01 (1.0886e+00)	Acc@1  71.09 ( 68.43)	Acc@5  95.31 ( 92.00)
Epoch: [11][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.1484e+00 (1.0831e+00)	Acc@1  65.62 ( 68.60)	Acc@5  92.19 ( 92.19)
Epoch: [11][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.2832e+00 (1.0866e+00)	Acc@1  60.16 ( 68.31)	Acc@5  88.28 ( 92.12)
Epoch: [11][ 80/391]	Time  0.029 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.0967e-01 (1.0862e+00)	Acc@1  75.00 ( 68.26)	Acc@5  94.53 ( 92.16)
Epoch: [11][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0547e+00 (1.0944e+00)	Acc@1  71.09 ( 67.78)	Acc@5  92.19 ( 92.14)
Epoch: [11][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.2275e+00 (1.0986e+00)	Acc@1  60.94 ( 67.59)	Acc@5  89.84 ( 92.11)
Epoch: [11][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.8096e-01 (1.0981e+00)	Acc@1  65.62 ( 67.59)	Acc@5  96.88 ( 92.17)
Epoch: [11][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.2266e+00 (1.0958e+00)	Acc@1  64.84 ( 67.58)	Acc@5  91.41 ( 92.25)
Epoch: [11][130/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1289e+00 (1.1022e+00)	Acc@1  67.97 ( 67.48)	Acc@5  90.62 ( 92.17)
Epoch: [11][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.3691e+00 (1.1092e+00)	Acc@1  60.16 ( 67.24)	Acc@5  88.28 ( 92.07)
Epoch: [11][150/391]	Time  0.042 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1982e+00 (1.1113e+00)	Acc@1  62.50 ( 67.22)	Acc@5  90.62 ( 92.01)
Epoch: [11][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0371e+00 (1.1123e+00)	Acc@1  66.41 ( 67.16)	Acc@5  96.88 ( 92.05)
Epoch: [11][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0479e+00 (1.1130e+00)	Acc@1  67.97 ( 67.26)	Acc@5  91.41 ( 92.05)
Epoch: [11][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0244e+00 (1.1151e+00)	Acc@1  64.84 ( 67.19)	Acc@5  92.97 ( 92.04)
Epoch: [11][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0352e+00 (1.1189e+00)	Acc@1  66.41 ( 67.08)	Acc@5  94.53 ( 92.00)
Epoch: [11][200/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2119e+00 (1.1229e+00)	Acc@1  65.62 ( 66.99)	Acc@5  90.62 ( 91.95)
Epoch: [11][210/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0420e+00 (1.1222e+00)	Acc@1  66.41 ( 67.02)	Acc@5  93.75 ( 91.90)
Epoch: [11][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.0527e-01 (1.1216e+00)	Acc@1  75.00 ( 67.04)	Acc@5  91.41 ( 91.90)
Epoch: [11][230/391]	Time  0.033 ( 0.035)	Data  0.004 ( 0.002)	Loss 1.1680e+00 (1.1241e+00)	Acc@1  67.19 ( 67.03)	Acc@5  89.84 ( 91.85)
Epoch: [11][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1309e+00 (1.1256e+00)	Acc@1  66.41 ( 66.95)	Acc@5  92.19 ( 91.82)
Epoch: [11][250/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0244e+00 (1.1256e+00)	Acc@1  70.31 ( 66.95)	Acc@5  92.19 ( 91.82)
Epoch: [11][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1875e+00 (1.1241e+00)	Acc@1  67.19 ( 67.01)	Acc@5  92.19 ( 91.84)
Epoch: [11][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1797e+00 (1.1250e+00)	Acc@1  61.72 ( 66.99)	Acc@5  89.06 ( 91.77)
Epoch: [11][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1631e+00 (1.1259e+00)	Acc@1  65.62 ( 66.98)	Acc@5  92.19 ( 91.75)
Epoch: [11][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1104e+00 (1.1267e+00)	Acc@1  66.41 ( 66.93)	Acc@5  92.97 ( 91.75)
Epoch: [11][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0586e+00 (1.1270e+00)	Acc@1  65.62 ( 66.91)	Acc@5  91.41 ( 91.75)
Epoch: [11][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1572e+00 (1.1288e+00)	Acc@1  71.88 ( 66.94)	Acc@5  92.97 ( 91.74)
Epoch: [11][320/391]	Time  0.037 ( 0.035)	Data  0.002 ( 0.002)	Loss 9.0381e-01 (1.1271e+00)	Acc@1  71.09 ( 66.95)	Acc@5  94.53 ( 91.77)
Epoch: [11][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0615e+00 (1.1262e+00)	Acc@1  67.19 ( 66.97)	Acc@5  91.41 ( 91.76)
Epoch: [11][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1084e+00 (1.1284e+00)	Acc@1  69.53 ( 66.94)	Acc@5  90.62 ( 91.75)
Epoch: [11][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2920e+00 (1.1311e+00)	Acc@1  60.16 ( 66.86)	Acc@5  89.84 ( 91.71)
Epoch: [11][360/391]	Time  0.034 ( 0.035)	Data  0.003 ( 0.002)	Loss 1.2070e+00 (1.1313e+00)	Acc@1  62.50 ( 66.83)	Acc@5  89.06 ( 91.72)
Epoch: [11][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3291e+00 (1.1327e+00)	Acc@1  60.94 ( 66.81)	Acc@5  86.72 ( 91.68)
Epoch: [11][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3926e+00 (1.1342e+00)	Acc@1  59.38 ( 66.79)	Acc@5  89.06 ( 91.66)
Epoch: [11][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0586e+00 (1.1344e+00)	Acc@1  71.25 ( 66.79)	Acc@5  91.25 ( 91.66)
## e[11] optimizer.zero_grad (sum) time: 0.17705583572387695
## e[11]       loss.backward (sum) time: 3.8553707599639893
## e[11]      optimizer.step (sum) time: 1.3017873764038086
## epoch[11] training(only) time: 13.822773218154907
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.6182e+00 (1.6182e+00)	Acc@1  61.00 ( 61.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.025 ( 0.028)	Loss 1.4512e+00 (1.5121e+00)	Acc@1  66.00 ( 61.18)	Acc@5  88.00 ( 86.00)
Test: [ 20/100]	Time  0.020 ( 0.024)	Loss 1.2656e+00 (1.4677e+00)	Acc@1  70.00 ( 61.67)	Acc@5  87.00 ( 87.24)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5967e+00 (1.4868e+00)	Acc@1  55.00 ( 60.48)	Acc@5  85.00 ( 86.74)
Test: [ 40/100]	Time  0.019 ( 0.021)	Loss 1.5361e+00 (1.4806e+00)	Acc@1  62.00 ( 60.15)	Acc@5  88.00 ( 86.93)
Test: [ 50/100]	Time  0.023 ( 0.020)	Loss 1.4805e+00 (1.4944e+00)	Acc@1  59.00 ( 60.00)	Acc@5  86.00 ( 86.35)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 1.5293e+00 (1.4832e+00)	Acc@1  56.00 ( 59.97)	Acc@5  86.00 ( 86.62)
Test: [ 70/100]	Time  0.018 ( 0.020)	Loss 1.4893e+00 (1.4835e+00)	Acc@1  61.00 ( 59.68)	Acc@5  84.00 ( 86.63)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.6592e+00 (1.4906e+00)	Acc@1  58.00 ( 59.49)	Acc@5  86.00 ( 86.51)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.7354e+00 (1.4831e+00)	Acc@1  58.00 ( 59.66)	Acc@5  85.00 ( 86.73)
 * Acc@1 59.550 Acc@5 86.810
### epoch[11] execution time: 15.79178500175476
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.183 ( 0.183)	Data  0.146 ( 0.146)	Loss 8.5303e-01 (8.5303e-01)	Acc@1  77.34 ( 77.34)	Acc@5  93.75 ( 93.75)
Epoch: [12][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 8.6670e-01 (1.0084e+00)	Acc@1  75.78 ( 71.52)	Acc@5  93.75 ( 92.12)
Epoch: [12][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 9.6631e-01 (1.0263e+00)	Acc@1  71.09 ( 70.57)	Acc@5  93.75 ( 92.75)
Epoch: [12][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 1.0117e+00 (1.0048e+00)	Acc@1  72.66 ( 71.04)	Acc@5  93.75 ( 92.77)
Epoch: [12][ 40/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.0127e+00 (1.0097e+00)	Acc@1  71.09 ( 70.81)	Acc@5  91.41 ( 92.74)
Epoch: [12][ 50/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.0361e+00 (1.0187e+00)	Acc@1  69.53 ( 70.22)	Acc@5  90.62 ( 92.89)
Epoch: [12][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.0000e+00 (1.0215e+00)	Acc@1  73.44 ( 70.15)	Acc@5  94.53 ( 92.93)
Epoch: [12][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.1602e+00 (1.0267e+00)	Acc@1  67.19 ( 69.99)	Acc@5  92.97 ( 92.97)
Epoch: [12][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.9805e-01 (1.0290e+00)	Acc@1  71.88 ( 69.95)	Acc@5  94.53 ( 93.00)
Epoch: [12][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.1211e+00 (1.0325e+00)	Acc@1  63.28 ( 69.90)	Acc@5  92.19 ( 92.85)
Epoch: [12][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.3350e+00 (1.0386e+00)	Acc@1  55.47 ( 69.49)	Acc@5  86.72 ( 92.78)
Epoch: [12][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0117e+00 (1.0357e+00)	Acc@1  69.53 ( 69.62)	Acc@5  92.97 ( 92.80)
Epoch: [12][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0781e+00 (1.0391e+00)	Acc@1  70.31 ( 69.31)	Acc@5  89.84 ( 92.79)
Epoch: [12][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0850e+00 (1.0376e+00)	Acc@1  67.97 ( 69.32)	Acc@5  90.62 ( 92.76)
Epoch: [12][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.6670e-01 (1.0407e+00)	Acc@1  73.44 ( 69.23)	Acc@5  93.75 ( 92.82)
Epoch: [12][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0635e+00 (1.0420e+00)	Acc@1  71.88 ( 69.15)	Acc@5  92.19 ( 92.84)
Epoch: [12][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0947e+00 (1.0410e+00)	Acc@1  66.41 ( 69.11)	Acc@5  90.62 ( 92.81)
Epoch: [12][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1475e+00 (1.0411e+00)	Acc@1  67.19 ( 69.03)	Acc@5  89.84 ( 92.81)
Epoch: [12][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0654e+00 (1.0426e+00)	Acc@1  65.62 ( 69.00)	Acc@5  94.53 ( 92.77)
Epoch: [12][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1816e+00 (1.0394e+00)	Acc@1  61.72 ( 69.08)	Acc@5  91.41 ( 92.83)
Epoch: [12][200/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2236e+00 (1.0389e+00)	Acc@1  61.72 ( 69.08)	Acc@5  90.62 ( 92.82)
Epoch: [12][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0361e+00 (1.0419e+00)	Acc@1  70.31 ( 68.98)	Acc@5  93.75 ( 92.76)
Epoch: [12][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1943e+00 (1.0452e+00)	Acc@1  65.62 ( 68.92)	Acc@5  89.84 ( 92.70)
Epoch: [12][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2510e+00 (1.0478e+00)	Acc@1  62.50 ( 68.84)	Acc@5  90.62 ( 92.64)
Epoch: [12][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0908e+00 (1.0476e+00)	Acc@1  71.88 ( 68.87)	Acc@5  92.97 ( 92.68)
Epoch: [12][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9307e-01 (1.0488e+00)	Acc@1  71.09 ( 68.86)	Acc@5  95.31 ( 92.67)
Epoch: [12][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1377e+00 (1.0492e+00)	Acc@1  66.41 ( 68.90)	Acc@5  91.41 ( 92.69)
Epoch: [12][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0322e-01 (1.0508e+00)	Acc@1  79.69 ( 68.88)	Acc@5  96.09 ( 92.66)
Epoch: [12][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1680e+00 (1.0516e+00)	Acc@1  65.62 ( 68.82)	Acc@5  92.19 ( 92.69)
Epoch: [12][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0371e+00 (1.0528e+00)	Acc@1  70.31 ( 68.80)	Acc@5  91.41 ( 92.67)
Epoch: [12][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2490e+00 (1.0530e+00)	Acc@1  65.62 ( 68.80)	Acc@5  88.28 ( 92.67)
Epoch: [12][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.3994e-01 (1.0528e+00)	Acc@1  72.66 ( 68.80)	Acc@5  93.75 ( 92.67)
Epoch: [12][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.4385e-01 (1.0530e+00)	Acc@1  72.66 ( 68.77)	Acc@5  95.31 ( 92.67)
Epoch: [12][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0098e+00 (1.0539e+00)	Acc@1  70.31 ( 68.74)	Acc@5  92.19 ( 92.66)
Epoch: [12][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0908e+00 (1.0539e+00)	Acc@1  67.19 ( 68.72)	Acc@5  92.97 ( 92.66)
Epoch: [12][350/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1621e+00 (1.0533e+00)	Acc@1  67.97 ( 68.70)	Acc@5  89.84 ( 92.67)
Epoch: [12][360/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0576e+00 (1.0540e+00)	Acc@1  69.53 ( 68.65)	Acc@5  92.19 ( 92.67)
Epoch: [12][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0527e+00 (1.0544e+00)	Acc@1  67.19 ( 68.63)	Acc@5  92.97 ( 92.66)
Epoch: [12][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0469e+00 (1.0547e+00)	Acc@1  71.09 ( 68.66)	Acc@5  92.97 ( 92.67)
Epoch: [12][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0186e+00 (1.0576e+00)	Acc@1  71.25 ( 68.61)	Acc@5  93.75 ( 92.62)
## e[12] optimizer.zero_grad (sum) time: 0.18088698387145996
## e[12]       loss.backward (sum) time: 4.021016359329224
## e[12]      optimizer.step (sum) time: 1.2399685382843018
## epoch[12] training(only) time: 13.867913007736206
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.4795e+00 (1.4795e+00)	Acc@1  60.00 ( 60.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 1.3662e+00 (1.4908e+00)	Acc@1  65.00 ( 62.64)	Acc@5  91.00 ( 86.91)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.3232e+00 (1.4589e+00)	Acc@1  60.00 ( 62.19)	Acc@5  90.00 ( 87.67)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.8564e+00 (1.4877e+00)	Acc@1  53.00 ( 61.03)	Acc@5  83.00 ( 86.68)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.4648e+00 (1.4777e+00)	Acc@1  62.00 ( 60.68)	Acc@5  86.00 ( 87.20)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.4629e+00 (1.4969e+00)	Acc@1  62.00 ( 60.14)	Acc@5  86.00 ( 86.76)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 1.5059e+00 (1.4799e+00)	Acc@1  56.00 ( 60.44)	Acc@5  90.00 ( 86.92)
Test: [ 70/100]	Time  0.014 ( 0.020)	Loss 1.5127e+00 (1.4829e+00)	Acc@1  55.00 ( 60.21)	Acc@5  87.00 ( 86.96)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.7988e+00 (1.4890e+00)	Acc@1  63.00 ( 60.33)	Acc@5  82.00 ( 86.88)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 1.5225e+00 (1.4788e+00)	Acc@1  62.00 ( 60.43)	Acc@5  82.00 ( 86.95)
 * Acc@1 60.430 Acc@5 86.970
### epoch[12] execution time: 15.83370304107666
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.194 ( 0.194)	Data  0.148 ( 0.148)	Loss 9.0039e-01 (9.0039e-01)	Acc@1  73.44 ( 73.44)	Acc@5  93.75 ( 93.75)
Epoch: [13][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 8.3691e-01 (9.2347e-01)	Acc@1  74.22 ( 71.80)	Acc@5  94.53 ( 94.82)
Epoch: [13][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.008)	Loss 1.0137e+00 (9.4317e-01)	Acc@1  67.97 ( 71.58)	Acc@5  92.19 ( 94.08)
Epoch: [13][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 9.0918e-01 (9.3813e-01)	Acc@1  72.66 ( 71.75)	Acc@5  95.31 ( 93.98)
Epoch: [13][ 40/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.005)	Loss 9.7461e-01 (9.3882e-01)	Acc@1  71.88 ( 71.32)	Acc@5  89.84 ( 94.00)
Epoch: [13][ 50/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.0176e+00 (9.2115e-01)	Acc@1  64.84 ( 72.14)	Acc@5  93.75 ( 94.15)
Epoch: [13][ 60/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 9.0430e-01 (9.1649e-01)	Acc@1  71.88 ( 72.27)	Acc@5  95.31 ( 94.42)
Epoch: [13][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 8.3691e-01 (9.1027e-01)	Acc@1  75.00 ( 72.44)	Acc@5  94.53 ( 94.59)
Epoch: [13][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.2139e-01 (9.1128e-01)	Acc@1  69.53 ( 72.26)	Acc@5  96.88 ( 94.59)
Epoch: [13][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.0127e+00 (9.2705e-01)	Acc@1  69.53 ( 71.87)	Acc@5  92.97 ( 94.37)
Epoch: [13][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.6631e-01 (9.3366e-01)	Acc@1  75.00 ( 71.75)	Acc@5  90.62 ( 94.31)
Epoch: [13][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0176e+00 (9.3992e-01)	Acc@1  67.19 ( 71.61)	Acc@5  92.19 ( 94.19)
Epoch: [13][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.6826e-01 (9.4603e-01)	Acc@1  72.66 ( 71.48)	Acc@5  90.62 ( 94.05)
Epoch: [13][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1562e+00 (9.4854e-01)	Acc@1  67.97 ( 71.45)	Acc@5  91.41 ( 93.99)
Epoch: [13][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1494e+00 (9.4819e-01)	Acc@1  64.06 ( 71.53)	Acc@5  92.97 ( 94.04)
Epoch: [13][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.8867e-01 (9.5388e-01)	Acc@1  72.66 ( 71.41)	Acc@5  94.53 ( 93.99)
Epoch: [13][160/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.3252e-01 (9.5636e-01)	Acc@1  75.78 ( 71.42)	Acc@5  95.31 ( 93.92)
Epoch: [13][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.8281e-01 (9.6095e-01)	Acc@1  78.91 ( 71.32)	Acc@5  92.19 ( 93.81)
Epoch: [13][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0967e+00 (9.6318e-01)	Acc@1  66.41 ( 71.17)	Acc@5  90.62 ( 93.80)
Epoch: [13][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0264e+00 (9.6395e-01)	Acc@1  67.19 ( 71.19)	Acc@5  92.97 ( 93.83)
Epoch: [13][200/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0137e+00 (9.6539e-01)	Acc@1  70.31 ( 71.20)	Acc@5  91.41 ( 93.79)
Epoch: [13][210/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.8145e-01 (9.7015e-01)	Acc@1  71.88 ( 71.01)	Acc@5  92.97 ( 93.74)
Epoch: [13][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7012e-01 (9.7191e-01)	Acc@1  70.31 ( 70.95)	Acc@5  96.09 ( 93.70)
Epoch: [13][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5781e-01 (9.7073e-01)	Acc@1  75.00 ( 71.02)	Acc@5  96.88 ( 93.71)
Epoch: [13][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.9121e-01 (9.7027e-01)	Acc@1  69.53 ( 70.98)	Acc@5  96.88 ( 93.72)
Epoch: [13][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.7510e-01 (9.7221e-01)	Acc@1  72.66 ( 70.93)	Acc@5  93.75 ( 93.70)
Epoch: [13][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2725e+00 (9.7251e-01)	Acc@1  59.38 ( 70.86)	Acc@5  90.62 ( 93.71)
Epoch: [13][270/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1348e+00 (9.7662e-01)	Acc@1  67.19 ( 70.74)	Acc@5  91.41 ( 93.65)
Epoch: [13][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2695e+00 (9.7971e-01)	Acc@1  63.28 ( 70.66)	Acc@5  92.19 ( 93.62)
Epoch: [13][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2461e+00 (9.8333e-01)	Acc@1  64.06 ( 70.56)	Acc@5  90.62 ( 93.60)
Epoch: [13][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0547e+00 (9.8379e-01)	Acc@1  68.75 ( 70.56)	Acc@5  89.84 ( 93.56)
Epoch: [13][310/391]	Time  0.041 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.9346e-01 (9.8381e-01)	Acc@1  75.78 ( 70.55)	Acc@5  94.53 ( 93.58)
Epoch: [13][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1807e+00 (9.8748e-01)	Acc@1  64.06 ( 70.45)	Acc@5  91.41 ( 93.52)
Epoch: [13][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0078e+00 (9.8765e-01)	Acc@1  71.88 ( 70.44)	Acc@5  94.53 ( 93.52)
Epoch: [13][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0840e+00 (9.8901e-01)	Acc@1  66.41 ( 70.46)	Acc@5  92.97 ( 93.49)
Epoch: [13][350/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1221e+00 (9.9154e-01)	Acc@1  67.97 ( 70.40)	Acc@5  89.06 ( 93.46)
Epoch: [13][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7744e-01 (9.9106e-01)	Acc@1  72.66 ( 70.42)	Acc@5  96.88 ( 93.49)
Epoch: [13][370/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1211e+00 (9.9172e-01)	Acc@1  71.88 ( 70.41)	Acc@5  90.62 ( 93.49)
Epoch: [13][380/391]	Time  0.043 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0869e+00 (9.9252e-01)	Acc@1  72.66 ( 70.43)	Acc@5  91.41 ( 93.48)
Epoch: [13][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.9072e-01 (9.9149e-01)	Acc@1  68.75 ( 70.41)	Acc@5  92.50 ( 93.50)
## e[13] optimizer.zero_grad (sum) time: 0.1790144443511963
## e[13]       loss.backward (sum) time: 4.06995964050293
## e[13]      optimizer.step (sum) time: 1.2342050075531006
## epoch[13] training(only) time: 13.822335958480835
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.3447e+00 (1.3447e+00)	Acc@1  64.00 ( 64.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.3252e+00 (1.3817e+00)	Acc@1  62.00 ( 63.27)	Acc@5  90.00 ( 88.00)
Test: [ 20/100]	Time  0.021 ( 0.024)	Loss 1.2490e+00 (1.3492e+00)	Acc@1  68.00 ( 63.62)	Acc@5  89.00 ( 88.62)
Test: [ 30/100]	Time  0.023 ( 0.022)	Loss 1.6094e+00 (1.3681e+00)	Acc@1  58.00 ( 62.94)	Acc@5  89.00 ( 88.32)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.4492e+00 (1.3564e+00)	Acc@1  62.00 ( 63.00)	Acc@5  88.00 ( 88.56)
Test: [ 50/100]	Time  0.015 ( 0.021)	Loss 1.2090e+00 (1.3783e+00)	Acc@1  68.00 ( 62.39)	Acc@5  90.00 ( 88.39)
Test: [ 60/100]	Time  0.019 ( 0.020)	Loss 1.4434e+00 (1.3677e+00)	Acc@1  58.00 ( 62.52)	Acc@5  89.00 ( 88.69)
Test: [ 70/100]	Time  0.014 ( 0.020)	Loss 1.3955e+00 (1.3712e+00)	Acc@1  63.00 ( 62.58)	Acc@5  86.00 ( 88.73)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.2725e+00 (1.3746e+00)	Acc@1  65.00 ( 62.64)	Acc@5  89.00 ( 88.49)
Test: [ 90/100]	Time  0.022 ( 0.019)	Loss 1.5635e+00 (1.3636e+00)	Acc@1  58.00 ( 62.99)	Acc@5  89.00 ( 88.65)
 * Acc@1 63.110 Acc@5 88.720
### epoch[13] execution time: 15.809602737426758
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.193 ( 0.193)	Data  0.151 ( 0.151)	Loss 8.2227e-01 (8.2227e-01)	Acc@1  74.22 ( 74.22)	Acc@5  92.97 ( 92.97)
Epoch: [14][ 10/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.015)	Loss 7.7783e-01 (8.6213e-01)	Acc@1  75.00 ( 72.73)	Acc@5  96.09 ( 94.96)
Epoch: [14][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.009)	Loss 1.0303e+00 (8.8523e-01)	Acc@1  68.75 ( 72.25)	Acc@5  95.31 ( 94.68)
Epoch: [14][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 6.5820e-01 (8.7604e-01)	Acc@1  77.34 ( 72.68)	Acc@5  97.66 ( 94.88)
Epoch: [14][ 40/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.005)	Loss 7.4707e-01 (8.6184e-01)	Acc@1  77.34 ( 73.30)	Acc@5  94.53 ( 94.99)
Epoch: [14][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 7.7930e-01 (8.6411e-01)	Acc@1  78.91 ( 73.53)	Acc@5  94.53 ( 94.78)
Epoch: [14][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.6953e-01 (8.6687e-01)	Acc@1  74.22 ( 73.35)	Acc@5  93.75 ( 94.86)
Epoch: [14][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.5811e-01 (8.6030e-01)	Acc@1  85.94 ( 73.71)	Acc@5  99.22 ( 94.95)
Epoch: [14][ 80/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1367e+00 (8.6680e-01)	Acc@1  65.62 ( 73.66)	Acc@5  92.97 ( 94.94)
Epoch: [14][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0020e+00 (8.6606e-01)	Acc@1  65.62 ( 73.69)	Acc@5  93.75 ( 94.94)
Epoch: [14][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.4287e-01 (8.6748e-01)	Acc@1  69.53 ( 73.56)	Acc@5  94.53 ( 94.99)
Epoch: [14][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.4570e-01 (8.7454e-01)	Acc@1  75.00 ( 73.35)	Acc@5  94.53 ( 94.90)
Epoch: [14][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.9502e-01 (8.7912e-01)	Acc@1  74.22 ( 73.27)	Acc@5  93.75 ( 94.85)
Epoch: [14][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.9541e-01 (8.7939e-01)	Acc@1  73.44 ( 73.31)	Acc@5  95.31 ( 94.82)
Epoch: [14][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1113e+00 (8.8548e-01)	Acc@1  70.31 ( 73.11)	Acc@5  92.19 ( 94.70)
Epoch: [14][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0596e+00 (8.9143e-01)	Acc@1  66.41 ( 72.85)	Acc@5  91.41 ( 94.60)
Epoch: [14][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.3643e-01 (8.9410e-01)	Acc@1  72.66 ( 72.74)	Acc@5  96.09 ( 94.55)
Epoch: [14][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.0869e-01 (8.9865e-01)	Acc@1  73.44 ( 72.60)	Acc@5  96.88 ( 94.53)
Epoch: [14][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5596e-01 (8.9956e-01)	Acc@1  75.00 ( 72.47)	Acc@5  92.97 ( 94.54)
Epoch: [14][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.5898e-01 (8.9912e-01)	Acc@1  68.75 ( 72.49)	Acc@5  94.53 ( 94.56)
Epoch: [14][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0127e+00 (8.9848e-01)	Acc@1  66.41 ( 72.52)	Acc@5  94.53 ( 94.62)
Epoch: [14][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1260e+00 (9.0002e-01)	Acc@1  64.06 ( 72.45)	Acc@5  92.19 ( 94.58)
Epoch: [14][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0840e+00 (8.9968e-01)	Acc@1  64.84 ( 72.47)	Acc@5  91.41 ( 94.61)
Epoch: [14][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0479e+00 (9.0137e-01)	Acc@1  72.66 ( 72.43)	Acc@5  92.19 ( 94.60)
Epoch: [14][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0449e+00 (9.0328e-01)	Acc@1  71.88 ( 72.41)	Acc@5  92.19 ( 94.57)
Epoch: [14][250/391]	Time  0.044 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0928e+00 (9.0545e-01)	Acc@1  64.84 ( 72.35)	Acc@5  92.19 ( 94.54)
Epoch: [14][260/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5645e-01 (9.0762e-01)	Acc@1  76.56 ( 72.25)	Acc@5  95.31 ( 94.52)
Epoch: [14][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4082e-01 (9.0637e-01)	Acc@1  77.34 ( 72.31)	Acc@5  95.31 ( 94.55)
Epoch: [14][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0400e+00 (9.0970e-01)	Acc@1  70.31 ( 72.27)	Acc@5  95.31 ( 94.49)
Epoch: [14][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7832e-01 (9.1099e-01)	Acc@1  76.56 ( 72.24)	Acc@5  96.09 ( 94.45)
Epoch: [14][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1758e+00 (9.1505e-01)	Acc@1  64.84 ( 72.11)	Acc@5  89.84 ( 94.42)
Epoch: [14][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1787e+00 (9.1691e-01)	Acc@1  64.06 ( 72.07)	Acc@5  92.97 ( 94.39)
Epoch: [14][320/391]	Time  0.047 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1230e+00 (9.1875e-01)	Acc@1  64.84 ( 72.00)	Acc@5  91.41 ( 94.36)
Epoch: [14][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4766e-01 (9.1863e-01)	Acc@1  73.44 ( 72.02)	Acc@5  96.09 ( 94.36)
Epoch: [14][340/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.8779e-01 (9.2021e-01)	Acc@1  69.53 ( 71.99)	Acc@5  96.09 ( 94.32)
Epoch: [14][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.2725e-01 (9.2109e-01)	Acc@1  73.44 ( 71.98)	Acc@5  94.53 ( 94.31)
Epoch: [14][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.3838e-01 (9.2297e-01)	Acc@1  75.00 ( 71.94)	Acc@5  95.31 ( 94.30)
Epoch: [14][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3398e+00 (9.2692e-01)	Acc@1  60.94 ( 71.86)	Acc@5  89.06 ( 94.26)
Epoch: [14][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0361e+00 (9.2900e-01)	Acc@1  67.19 ( 71.84)	Acc@5  93.75 ( 94.24)
Epoch: [14][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1152e+00 (9.2863e-01)	Acc@1  71.25 ( 71.84)	Acc@5  93.75 ( 94.26)
## e[14] optimizer.zero_grad (sum) time: 0.17806673049926758
## e[14]       loss.backward (sum) time: 4.067727565765381
## e[14]      optimizer.step (sum) time: 1.2682359218597412
## epoch[14] training(only) time: 13.79950499534607
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.4307e+00 (1.4307e+00)	Acc@1  61.00 ( 61.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.6768e+00 (1.5463e+00)	Acc@1  58.00 ( 59.73)	Acc@5  85.00 ( 86.09)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 1.3105e+00 (1.4855e+00)	Acc@1  64.00 ( 60.71)	Acc@5  90.00 ( 86.29)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.4668e+00 (1.4924e+00)	Acc@1  58.00 ( 60.68)	Acc@5  89.00 ( 86.52)
Test: [ 40/100]	Time  0.019 ( 0.021)	Loss 1.3779e+00 (1.4568e+00)	Acc@1  60.00 ( 60.95)	Acc@5  89.00 ( 87.41)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4883e+00 (1.4759e+00)	Acc@1  62.00 ( 60.80)	Acc@5  87.00 ( 86.90)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 1.4111e+00 (1.4660e+00)	Acc@1  56.00 ( 60.51)	Acc@5  89.00 ( 87.26)
Test: [ 70/100]	Time  0.015 ( 0.020)	Loss 1.4443e+00 (1.4669e+00)	Acc@1  62.00 ( 60.49)	Acc@5  87.00 ( 87.24)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.5596e+00 (1.4672e+00)	Acc@1  62.00 ( 60.33)	Acc@5  83.00 ( 87.26)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.8105e+00 (1.4651e+00)	Acc@1  57.00 ( 60.49)	Acc@5  83.00 ( 87.40)
 * Acc@1 60.650 Acc@5 87.490
### epoch[14] execution time: 15.784894227981567
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.192 ( 0.192)	Data  0.151 ( 0.151)	Loss 1.0400e+00 (1.0400e+00)	Acc@1  72.66 ( 72.66)	Acc@5  91.41 ( 91.41)
Epoch: [15][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.015)	Loss 7.2510e-01 (8.6337e-01)	Acc@1  77.34 ( 74.79)	Acc@5  97.66 ( 94.18)
Epoch: [15][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.009)	Loss 9.2139e-01 (8.7154e-01)	Acc@1  75.00 ( 74.11)	Acc@5  93.75 ( 94.75)
Epoch: [15][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 7.7637e-01 (8.4684e-01)	Acc@1  74.22 ( 74.80)	Acc@5  96.88 ( 95.11)
Epoch: [15][ 40/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.005)	Loss 8.6084e-01 (8.3512e-01)	Acc@1  71.88 ( 75.10)	Acc@5  95.31 ( 95.14)
Epoch: [15][ 50/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.005)	Loss 7.2754e-01 (8.2436e-01)	Acc@1  78.12 ( 75.15)	Acc@5  97.66 ( 95.33)
Epoch: [15][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.8750e-01 (8.2263e-01)	Acc@1  83.59 ( 75.15)	Acc@5  97.66 ( 95.48)
Epoch: [15][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.3730e-01 (8.1856e-01)	Acc@1  80.47 ( 75.26)	Acc@5  96.09 ( 95.51)
Epoch: [15][ 80/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.3096e-01 (8.1689e-01)	Acc@1  79.69 ( 75.26)	Acc@5  94.53 ( 95.44)
Epoch: [15][ 90/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.1846e-01 (8.2033e-01)	Acc@1  75.00 ( 75.29)	Acc@5  94.53 ( 95.50)
Epoch: [15][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.5303e-01 (8.2695e-01)	Acc@1  75.00 ( 75.08)	Acc@5  92.97 ( 95.37)
Epoch: [15][110/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.0771e-01 (8.3262e-01)	Acc@1  70.31 ( 74.89)	Acc@5  93.75 ( 95.29)
Epoch: [15][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.0420e-01 (8.3419e-01)	Acc@1  74.22 ( 74.85)	Acc@5  95.31 ( 95.27)
Epoch: [15][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.2617e-01 (8.3585e-01)	Acc@1  73.44 ( 74.67)	Acc@5  95.31 ( 95.35)
Epoch: [15][140/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.3594e-01 (8.3503e-01)	Acc@1  72.66 ( 74.61)	Acc@5  97.66 ( 95.36)
Epoch: [15][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.1992e-01 (8.3224e-01)	Acc@1  72.66 ( 74.78)	Acc@5  94.53 ( 95.33)
Epoch: [15][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.7832e-01 (8.3088e-01)	Acc@1  78.91 ( 74.86)	Acc@5  94.53 ( 95.31)
Epoch: [15][170/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.0420e-01 (8.3681e-01)	Acc@1  78.12 ( 74.70)	Acc@5  95.31 ( 95.27)
Epoch: [15][180/391]	Time  0.031 ( 0.036)	Data  0.002 ( 0.002)	Loss 1.0557e+00 (8.3866e-01)	Acc@1  67.97 ( 74.60)	Acc@5  96.09 ( 95.24)
Epoch: [15][190/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.9443e-01 (8.4201e-01)	Acc@1  74.22 ( 74.46)	Acc@5  94.53 ( 95.20)
Epoch: [15][200/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.0332e-01 (8.4255e-01)	Acc@1  71.09 ( 74.43)	Acc@5  95.31 ( 95.20)
Epoch: [15][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0469e+00 (8.4624e-01)	Acc@1  71.09 ( 74.30)	Acc@5  92.97 ( 95.15)
Epoch: [15][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1895e+00 (8.4966e-01)	Acc@1  71.09 ( 74.24)	Acc@5  90.62 ( 95.10)
Epoch: [15][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1094e-01 (8.5280e-01)	Acc@1  78.12 ( 74.18)	Acc@5  96.88 ( 95.06)
Epoch: [15][240/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5596e-01 (8.5336e-01)	Acc@1  76.56 ( 74.21)	Acc@5  94.53 ( 95.03)
Epoch: [15][250/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0215e+00 (8.5624e-01)	Acc@1  72.66 ( 74.10)	Acc@5  90.62 ( 95.00)
Epoch: [15][260/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1406e+00 (8.5798e-01)	Acc@1  65.62 ( 74.03)	Acc@5  90.62 ( 94.99)
Epoch: [15][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0137e+00 (8.5928e-01)	Acc@1  69.53 ( 74.02)	Acc@5  93.75 ( 94.99)
Epoch: [15][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.9707e-01 (8.5994e-01)	Acc@1  74.22 ( 74.02)	Acc@5  91.41 ( 94.98)
Epoch: [15][290/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4521e-01 (8.5978e-01)	Acc@1  75.78 ( 74.01)	Acc@5  97.66 ( 95.00)
Epoch: [15][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9795e-01 (8.6184e-01)	Acc@1  72.66 ( 73.93)	Acc@5  94.53 ( 94.98)
Epoch: [15][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0176e-01 (8.6045e-01)	Acc@1  75.00 ( 73.98)	Acc@5  95.31 ( 94.99)
Epoch: [15][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7158e-01 (8.6165e-01)	Acc@1  71.09 ( 73.94)	Acc@5  95.31 ( 95.00)
Epoch: [15][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.6035e-01 (8.6352e-01)	Acc@1  77.34 ( 73.88)	Acc@5  92.19 ( 94.96)
Epoch: [15][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4795e-01 (8.6302e-01)	Acc@1  80.47 ( 73.90)	Acc@5  97.66 ( 94.96)
Epoch: [15][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4326e-01 (8.6381e-01)	Acc@1  73.44 ( 73.88)	Acc@5  95.31 ( 94.97)
Epoch: [15][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5732e-01 (8.6431e-01)	Acc@1  77.34 ( 73.82)	Acc@5  96.88 ( 94.96)
Epoch: [15][370/391]	Time  0.038 ( 0.035)	Data  0.002 ( 0.002)	Loss 7.9541e-01 (8.6475e-01)	Acc@1  75.00 ( 73.82)	Acc@5  96.88 ( 94.98)
Epoch: [15][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.9463e-01 (8.6743e-01)	Acc@1  70.31 ( 73.74)	Acc@5  94.53 ( 94.95)
Epoch: [15][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1455e+00 (8.6885e-01)	Acc@1  60.00 ( 73.70)	Acc@5  93.75 ( 94.94)
## e[15] optimizer.zero_grad (sum) time: 0.1782364845275879
## e[15]       loss.backward (sum) time: 3.8198838233947754
## e[15]      optimizer.step (sum) time: 1.290635347366333
## epoch[15] training(only) time: 13.852833032608032
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.4141e+00 (1.4141e+00)	Acc@1  66.00 ( 66.00)	Acc@5  85.00 ( 85.00)
Test: [ 10/100]	Time  0.021 ( 0.030)	Loss 1.4893e+00 (1.4760e+00)	Acc@1  58.00 ( 62.82)	Acc@5  91.00 ( 86.55)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.2598e+00 (1.4540e+00)	Acc@1  69.00 ( 62.38)	Acc@5  90.00 ( 87.24)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5244e+00 (1.4560e+00)	Acc@1  58.00 ( 62.10)	Acc@5  87.00 ( 87.00)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.4844e+00 (1.4444e+00)	Acc@1  62.00 ( 62.20)	Acc@5  86.00 ( 87.49)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4609e+00 (1.4730e+00)	Acc@1  63.00 ( 61.33)	Acc@5  86.00 ( 87.08)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 1.4385e+00 (1.4505e+00)	Acc@1  62.00 ( 61.64)	Acc@5  90.00 ( 87.52)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.4717e+00 (1.4497e+00)	Acc@1  59.00 ( 61.61)	Acc@5  89.00 ( 87.66)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.4844e+00 (1.4527e+00)	Acc@1  60.00 ( 61.49)	Acc@5  85.00 ( 87.60)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.7256e+00 (1.4405e+00)	Acc@1  60.00 ( 61.91)	Acc@5  86.00 ( 87.80)
 * Acc@1 61.990 Acc@5 87.810
### epoch[15] execution time: 15.804906845092773
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.182 ( 0.182)	Data  0.140 ( 0.140)	Loss 7.0557e-01 (7.0557e-01)	Acc@1  76.56 ( 76.56)	Acc@5  96.88 ( 96.88)
Epoch: [16][ 10/391]	Time  0.034 ( 0.048)	Data  0.001 ( 0.014)	Loss 5.5225e-01 (7.7508e-01)	Acc@1  82.81 ( 75.92)	Acc@5 100.00 ( 96.09)
Epoch: [16][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.008)	Loss 7.1094e-01 (7.6367e-01)	Acc@1  75.78 ( 76.86)	Acc@5  97.66 ( 96.06)
Epoch: [16][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 7.4268e-01 (7.6200e-01)	Acc@1  78.12 ( 76.92)	Acc@5  96.88 ( 96.12)
Epoch: [16][ 40/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.005)	Loss 8.2227e-01 (7.6602e-01)	Acc@1  73.44 ( 76.60)	Acc@5  96.88 ( 96.19)
Epoch: [16][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 8.6865e-01 (7.4771e-01)	Acc@1  78.91 ( 77.41)	Acc@5  93.75 ( 96.31)
Epoch: [16][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.1777e-01 (7.4257e-01)	Acc@1  76.56 ( 77.39)	Acc@5  96.88 ( 96.31)
Epoch: [16][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.0381e-01 (7.4874e-01)	Acc@1  75.78 ( 77.26)	Acc@5  92.19 ( 96.18)
Epoch: [16][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.4951e-01 (7.5304e-01)	Acc@1  77.34 ( 77.17)	Acc@5  95.31 ( 96.09)
Epoch: [16][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.3389e-01 (7.5641e-01)	Acc@1  75.78 ( 76.95)	Acc@5  96.09 ( 96.08)
Epoch: [16][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0693e-01 (7.5820e-01)	Acc@1  80.47 ( 76.86)	Acc@5  96.88 ( 96.01)
Epoch: [16][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.3525e-01 (7.5332e-01)	Acc@1  80.47 ( 76.96)	Acc@5  97.66 ( 96.10)
Epoch: [16][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.2236e-01 (7.6280e-01)	Acc@1  71.88 ( 76.65)	Acc@5  93.75 ( 96.06)
Epoch: [16][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.0234e-01 (7.6657e-01)	Acc@1  73.44 ( 76.54)	Acc@5  93.75 ( 96.02)
Epoch: [16][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.4863e-01 (7.6695e-01)	Acc@1  69.53 ( 76.58)	Acc@5  95.31 ( 95.96)
Epoch: [16][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.7891e-01 (7.6836e-01)	Acc@1  75.00 ( 76.52)	Acc@5  94.53 ( 95.98)
Epoch: [16][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.2383e-01 (7.6970e-01)	Acc@1  71.88 ( 76.49)	Acc@5  94.53 ( 95.94)
Epoch: [16][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.4512e-01 (7.6877e-01)	Acc@1  75.00 ( 76.54)	Acc@5  98.44 ( 95.95)
Epoch: [16][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.7637e-01 (7.7034e-01)	Acc@1  75.00 ( 76.43)	Acc@5  98.44 ( 95.98)
Epoch: [16][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.1455e-01 (7.7343e-01)	Acc@1  73.44 ( 76.38)	Acc@5  92.97 ( 95.95)
Epoch: [16][200/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.002)	Loss 9.1846e-01 (7.7618e-01)	Acc@1  70.31 ( 76.33)	Acc@5  96.88 ( 95.90)
Epoch: [16][210/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5000e-01 (7.7764e-01)	Acc@1  73.44 ( 76.26)	Acc@5  96.88 ( 95.89)
Epoch: [16][220/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.3066e-01 (7.8177e-01)	Acc@1  70.31 ( 76.18)	Acc@5  94.53 ( 95.82)
Epoch: [16][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.3115e-01 (7.8455e-01)	Acc@1  70.31 ( 76.15)	Acc@5  94.53 ( 95.78)
Epoch: [16][240/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7793e-01 (7.8651e-01)	Acc@1  73.44 ( 76.12)	Acc@5  95.31 ( 95.72)
Epoch: [16][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9795e-01 (7.9002e-01)	Acc@1  78.12 ( 76.08)	Acc@5  94.53 ( 95.71)
Epoch: [16][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.9883e-01 (7.9504e-01)	Acc@1  71.88 ( 75.91)	Acc@5  96.09 ( 95.66)
Epoch: [16][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.8379e-01 (7.9902e-01)	Acc@1  72.66 ( 75.82)	Acc@5  95.31 ( 95.62)
Epoch: [16][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0029e-01 (8.0189e-01)	Acc@1  79.69 ( 75.72)	Acc@5  94.53 ( 95.61)
Epoch: [16][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.0625e-01 (8.0390e-01)	Acc@1  67.97 ( 75.67)	Acc@5  96.09 ( 95.57)
Epoch: [16][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.6523e-01 (8.0584e-01)	Acc@1  77.34 ( 75.63)	Acc@5  92.19 ( 95.56)
Epoch: [16][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0498e+00 (8.0747e-01)	Acc@1  71.88 ( 75.57)	Acc@5  88.28 ( 95.53)
Epoch: [16][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.3838e-01 (8.0896e-01)	Acc@1  74.22 ( 75.51)	Acc@5  93.75 ( 95.52)
Epoch: [16][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0430e+00 (8.1070e-01)	Acc@1  66.41 ( 75.42)	Acc@5  93.75 ( 95.50)
Epoch: [16][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.3164e-01 (8.1259e-01)	Acc@1  72.66 ( 75.35)	Acc@5  96.09 ( 95.48)
Epoch: [16][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.5410e-01 (8.1320e-01)	Acc@1  67.97 ( 75.30)	Acc@5  96.09 ( 95.47)
Epoch: [16][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.3486e-01 (8.1439e-01)	Acc@1  78.91 ( 75.27)	Acc@5  95.31 ( 95.45)
Epoch: [16][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.8574e-01 (8.1606e-01)	Acc@1  74.22 ( 75.22)	Acc@5  93.75 ( 95.44)
Epoch: [16][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7012e-01 (8.1791e-01)	Acc@1  70.31 ( 75.14)	Acc@5  96.88 ( 95.44)
Epoch: [16][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.2363e-01 (8.1837e-01)	Acc@1  81.25 ( 75.12)	Acc@5  96.25 ( 95.45)
## e[16] optimizer.zero_grad (sum) time: 0.17949151992797852
## e[16]       loss.backward (sum) time: 4.091619491577148
## e[16]      optimizer.step (sum) time: 1.248133897781372
## epoch[16] training(only) time: 13.848008871078491
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.2773e+00 (1.2773e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.017 ( 0.032)	Loss 1.2939e+00 (1.3892e+00)	Acc@1  65.00 ( 65.73)	Acc@5  89.00 ( 87.09)
Test: [ 20/100]	Time  0.015 ( 0.025)	Loss 1.4238e+00 (1.3503e+00)	Acc@1  59.00 ( 64.81)	Acc@5  91.00 ( 88.62)
Test: [ 30/100]	Time  0.024 ( 0.023)	Loss 1.5146e+00 (1.3740e+00)	Acc@1  57.00 ( 63.94)	Acc@5  91.00 ( 88.55)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.6328e+00 (1.3735e+00)	Acc@1  60.00 ( 63.85)	Acc@5  86.00 ( 88.80)
Test: [ 50/100]	Time  0.020 ( 0.021)	Loss 1.4932e+00 (1.3928e+00)	Acc@1  61.00 ( 63.16)	Acc@5  88.00 ( 88.47)
Test: [ 60/100]	Time  0.025 ( 0.020)	Loss 1.3604e+00 (1.3764e+00)	Acc@1  67.00 ( 63.30)	Acc@5  90.00 ( 88.79)
Test: [ 70/100]	Time  0.015 ( 0.020)	Loss 1.3779e+00 (1.3754e+00)	Acc@1  64.00 ( 63.24)	Acc@5  89.00 ( 89.03)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.4551e+00 (1.3826e+00)	Acc@1  62.00 ( 63.10)	Acc@5  87.00 ( 88.83)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.5625e+00 (1.3693e+00)	Acc@1  62.00 ( 63.40)	Acc@5  90.00 ( 89.15)
 * Acc@1 63.590 Acc@5 89.150
### epoch[16] execution time: 15.84661865234375
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.191 ( 0.191)	Data  0.151 ( 0.151)	Loss 6.6846e-01 (6.6846e-01)	Acc@1  78.91 ( 78.91)	Acc@5  97.66 ( 97.66)
Epoch: [17][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 5.1465e-01 (6.9829e-01)	Acc@1  85.94 ( 79.90)	Acc@5 100.00 ( 97.02)
Epoch: [17][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.009)	Loss 9.2041e-01 (6.9094e-01)	Acc@1  74.22 ( 79.72)	Acc@5  96.09 ( 96.80)
Epoch: [17][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 6.1377e-01 (6.9764e-01)	Acc@1  82.03 ( 79.11)	Acc@5  96.09 ( 96.65)
Epoch: [17][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 5.7617e-01 (7.0189e-01)	Acc@1  85.94 ( 78.72)	Acc@5  96.09 ( 96.65)
Epoch: [17][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 7.3730e-01 (7.0052e-01)	Acc@1  79.69 ( 78.55)	Acc@5  94.53 ( 96.69)
Epoch: [17][ 60/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 9.0869e-01 (6.9820e-01)	Acc@1  75.00 ( 78.48)	Acc@5  94.53 ( 96.85)
Epoch: [17][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.8364e-01 (7.0098e-01)	Acc@1  80.47 ( 78.32)	Acc@5 100.00 ( 96.84)
Epoch: [17][ 80/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.1436e-01 (6.9849e-01)	Acc@1  78.12 ( 78.37)	Acc@5  96.88 ( 96.88)
Epoch: [17][ 90/391]	Time  0.033 ( 0.037)	Data  0.000 ( 0.003)	Loss 6.7676e-01 (7.0399e-01)	Acc@1  75.78 ( 78.21)	Acc@5  98.44 ( 96.79)
Epoch: [17][100/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.0215e-01 (7.0290e-01)	Acc@1  77.34 ( 78.16)	Acc@5  99.22 ( 96.83)
Epoch: [17][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.9277e-01 (7.0345e-01)	Acc@1  82.81 ( 78.17)	Acc@5  98.44 ( 96.84)
Epoch: [17][120/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.9189e-01 (7.1117e-01)	Acc@1  80.47 ( 78.05)	Acc@5  96.88 ( 96.75)
Epoch: [17][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.1670e-01 (7.1475e-01)	Acc@1  82.03 ( 77.97)	Acc@5  98.44 ( 96.64)
Epoch: [17][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.1104e-01 (7.1912e-01)	Acc@1  75.00 ( 77.80)	Acc@5  96.09 ( 96.60)
Epoch: [17][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.6953e-01 (7.2246e-01)	Acc@1  75.00 ( 77.71)	Acc@5  96.09 ( 96.57)
Epoch: [17][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.4990e-01 (7.2228e-01)	Acc@1  78.12 ( 77.74)	Acc@5  98.44 ( 96.56)
Epoch: [17][170/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.6035e-01 (7.2597e-01)	Acc@1  75.00 ( 77.70)	Acc@5  95.31 ( 96.51)
Epoch: [17][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.3340e-01 (7.3009e-01)	Acc@1  77.34 ( 77.60)	Acc@5  98.44 ( 96.45)
Epoch: [17][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.8076e-01 (7.3108e-01)	Acc@1  77.34 ( 77.55)	Acc@5  96.09 ( 96.46)
Epoch: [17][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.2607e-01 (7.3383e-01)	Acc@1  79.69 ( 77.53)	Acc@5  94.53 ( 96.39)
Epoch: [17][210/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.9932e-01 (7.3640e-01)	Acc@1  78.12 ( 77.52)	Acc@5  93.75 ( 96.38)
Epoch: [17][220/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 8.5205e-01 (7.3759e-01)	Acc@1  74.22 ( 77.44)	Acc@5  94.53 ( 96.36)
Epoch: [17][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7441e-01 (7.3887e-01)	Acc@1  75.78 ( 77.43)	Acc@5  96.09 ( 96.34)
Epoch: [17][240/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9189e-01 (7.4173e-01)	Acc@1  82.03 ( 77.43)	Acc@5  98.44 ( 96.31)
Epoch: [17][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.1797e-01 (7.4345e-01)	Acc@1  74.22 ( 77.36)	Acc@5  96.09 ( 96.31)
Epoch: [17][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.0576e-01 (7.4578e-01)	Acc@1  68.75 ( 77.29)	Acc@5  95.31 ( 96.28)
Epoch: [17][270/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.6514e-01 (7.4703e-01)	Acc@1  73.44 ( 77.26)	Acc@5  96.09 ( 96.25)
Epoch: [17][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7988e-01 (7.4943e-01)	Acc@1  74.22 ( 77.17)	Acc@5  96.09 ( 96.24)
Epoch: [17][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7627e-01 (7.5290e-01)	Acc@1  78.91 ( 77.04)	Acc@5  97.66 ( 96.19)
Epoch: [17][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7158e-01 (7.5448e-01)	Acc@1  72.66 ( 76.96)	Acc@5  96.88 ( 96.20)
Epoch: [17][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.5508e-01 (7.5603e-01)	Acc@1  68.75 ( 76.92)	Acc@5  93.75 ( 96.19)
Epoch: [17][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.5801e-01 (7.5893e-01)	Acc@1  74.22 ( 76.85)	Acc@5  94.53 ( 96.16)
Epoch: [17][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5479e-01 (7.6083e-01)	Acc@1  76.56 ( 76.77)	Acc@5  97.66 ( 96.16)
Epoch: [17][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9727e-01 (7.6214e-01)	Acc@1  78.12 ( 76.74)	Acc@5  97.66 ( 96.14)
Epoch: [17][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5693e-01 (7.6315e-01)	Acc@1  75.00 ( 76.70)	Acc@5  95.31 ( 96.12)
Epoch: [17][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7100e-01 (7.6622e-01)	Acc@1  78.91 ( 76.61)	Acc@5  96.09 ( 96.09)
Epoch: [17][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.6963e-01 (7.6674e-01)	Acc@1  71.09 ( 76.57)	Acc@5  96.09 ( 96.08)
Epoch: [17][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.6611e-01 (7.6771e-01)	Acc@1  79.69 ( 76.57)	Acc@5  97.66 ( 96.07)
Epoch: [17][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8018e-01 (7.6923e-01)	Acc@1  81.25 ( 76.52)	Acc@5  97.50 ( 96.03)
## e[17] optimizer.zero_grad (sum) time: 0.18062686920166016
## e[17]       loss.backward (sum) time: 4.050639390945435
## e[17]      optimizer.step (sum) time: 1.2691049575805664
## epoch[17] training(only) time: 13.83433985710144
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.6357e+00 (1.6357e+00)	Acc@1  63.00 ( 63.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.016 ( 0.030)	Loss 1.4336e+00 (1.5613e+00)	Acc@1  63.00 ( 62.27)	Acc@5  87.00 ( 85.55)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.3926e+00 (1.4994e+00)	Acc@1  62.00 ( 61.76)	Acc@5  89.00 ( 87.38)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.7754e+00 (1.5216e+00)	Acc@1  56.00 ( 61.39)	Acc@5  83.00 ( 87.03)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.5996e+00 (1.5091e+00)	Acc@1  64.00 ( 61.59)	Acc@5  87.00 ( 87.56)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5918e+00 (1.5270e+00)	Acc@1  59.00 ( 61.43)	Acc@5  84.00 ( 87.41)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.8428e+00 (1.5199e+00)	Acc@1  55.00 ( 61.64)	Acc@5  88.00 ( 87.54)
Test: [ 70/100]	Time  0.018 ( 0.020)	Loss 1.5293e+00 (1.5195e+00)	Acc@1  62.00 ( 61.58)	Acc@5  87.00 ( 87.52)
Test: [ 80/100]	Time  0.017 ( 0.020)	Loss 1.4492e+00 (1.5247e+00)	Acc@1  62.00 ( 61.53)	Acc@5  83.00 ( 87.40)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.8145e+00 (1.5137e+00)	Acc@1  55.00 ( 61.67)	Acc@5  83.00 ( 87.47)
 * Acc@1 61.860 Acc@5 87.580
### epoch[17] execution time: 15.827869415283203
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.193 ( 0.193)	Data  0.151 ( 0.151)	Loss 7.8516e-01 (7.8516e-01)	Acc@1  75.78 ( 75.78)	Acc@5  96.88 ( 96.88)
Epoch: [18][ 10/391]	Time  0.035 ( 0.049)	Data  0.001 ( 0.015)	Loss 6.3135e-01 (6.8395e-01)	Acc@1  80.47 ( 79.19)	Acc@5  96.88 ( 96.95)
Epoch: [18][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 5.7666e-01 (6.9003e-01)	Acc@1  85.16 ( 78.72)	Acc@5  99.22 ( 96.91)
Epoch: [18][ 30/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.006)	Loss 6.1328e-01 (6.8116e-01)	Acc@1  81.25 ( 78.86)	Acc@5  96.09 ( 96.93)
Epoch: [18][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 6.4111e-01 (6.7947e-01)	Acc@1  80.47 ( 79.21)	Acc@5  99.22 ( 96.84)
Epoch: [18][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 7.2998e-01 (6.7724e-01)	Acc@1  78.12 ( 79.37)	Acc@5  96.88 ( 96.75)
Epoch: [18][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.9434e-01 (6.7928e-01)	Acc@1  78.91 ( 79.47)	Acc@5  97.66 ( 96.80)
Epoch: [18][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.0742e-01 (6.7667e-01)	Acc@1  82.81 ( 79.56)	Acc@5  97.66 ( 96.79)
Epoch: [18][ 80/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.0957e-01 (6.8635e-01)	Acc@1  78.91 ( 79.32)	Acc@5  94.53 ( 96.67)
Epoch: [18][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.6836e-01 (6.8232e-01)	Acc@1  82.03 ( 79.46)	Acc@5  98.44 ( 96.72)
Epoch: [18][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.7852e-01 (6.8842e-01)	Acc@1  71.09 ( 79.21)	Acc@5  94.53 ( 96.70)
Epoch: [18][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.7080e-01 (6.8534e-01)	Acc@1  78.91 ( 79.34)	Acc@5  97.66 ( 96.67)
Epoch: [18][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.1396e-01 (6.8885e-01)	Acc@1  77.34 ( 79.20)	Acc@5  95.31 ( 96.64)
Epoch: [18][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.6611e-01 (6.8978e-01)	Acc@1  75.00 ( 79.18)	Acc@5  97.66 ( 96.64)
Epoch: [18][140/391]	Time  0.046 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.5000e-01 (6.9072e-01)	Acc@1  78.12 ( 79.15)	Acc@5  95.31 ( 96.63)
Epoch: [18][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.1924e-01 (6.9039e-01)	Acc@1  76.56 ( 79.13)	Acc@5  96.88 ( 96.63)
Epoch: [18][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4062e-01 (6.9183e-01)	Acc@1  78.12 ( 79.11)	Acc@5  98.44 ( 96.65)
Epoch: [18][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.5732e-01 (6.9467e-01)	Acc@1  78.91 ( 79.09)	Acc@5  97.66 ( 96.66)
Epoch: [18][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5869e-01 (6.9404e-01)	Acc@1  79.69 ( 79.11)	Acc@5  97.66 ( 96.69)
Epoch: [18][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4453e-01 (6.9578e-01)	Acc@1  77.34 ( 78.96)	Acc@5  96.09 ( 96.67)
Epoch: [18][200/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4033e-01 (6.9756e-01)	Acc@1  75.00 ( 78.88)	Acc@5  93.75 ( 96.67)
Epoch: [18][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3223e-01 (6.9759e-01)	Acc@1  83.59 ( 78.89)	Acc@5  99.22 ( 96.69)
Epoch: [18][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7490e-01 (7.0203e-01)	Acc@1  71.88 ( 78.76)	Acc@5  98.44 ( 96.66)
Epoch: [18][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9209e-01 (7.0544e-01)	Acc@1  73.44 ( 78.63)	Acc@5  93.75 ( 96.62)
Epoch: [18][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2793e-01 (7.0703e-01)	Acc@1  78.12 ( 78.56)	Acc@5  96.88 ( 96.59)
Epoch: [18][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6992e-01 (7.0816e-01)	Acc@1  80.47 ( 78.49)	Acc@5  97.66 ( 96.59)
Epoch: [18][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.8525e-01 (7.0997e-01)	Acc@1  71.09 ( 78.41)	Acc@5  96.88 ( 96.57)
Epoch: [18][270/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.8564e-01 (7.1354e-01)	Acc@1  78.12 ( 78.31)	Acc@5  97.66 ( 96.56)
Epoch: [18][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7100e-01 (7.1425e-01)	Acc@1  78.91 ( 78.25)	Acc@5  95.31 ( 96.56)
Epoch: [18][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0127e+00 (7.1845e-01)	Acc@1  73.44 ( 78.12)	Acc@5  92.97 ( 96.54)
Epoch: [18][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0811e-01 (7.2181e-01)	Acc@1  74.22 ( 78.05)	Acc@5  97.66 ( 96.52)
Epoch: [18][310/391]	Time  0.036 ( 0.035)	Data  0.004 ( 0.002)	Loss 6.0400e-01 (7.2151e-01)	Acc@1  81.25 ( 78.08)	Acc@5  97.66 ( 96.51)
Epoch: [18][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9824e-01 (7.2208e-01)	Acc@1  76.56 ( 78.04)	Acc@5  97.66 ( 96.51)
Epoch: [18][330/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.3730e-01 (7.2410e-01)	Acc@1  78.12 ( 77.98)	Acc@5  96.09 ( 96.49)
Epoch: [18][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0781e+00 (7.2551e-01)	Acc@1  65.62 ( 77.95)	Acc@5  94.53 ( 96.47)
Epoch: [18][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.1650e-01 (7.2564e-01)	Acc@1  72.66 ( 77.93)	Acc@5  94.53 ( 96.46)
Epoch: [18][360/391]	Time  0.043 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.8320e-01 (7.2807e-01)	Acc@1  74.22 ( 77.85)	Acc@5  95.31 ( 96.43)
Epoch: [18][370/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7480e-01 (7.2725e-01)	Acc@1  78.12 ( 77.89)	Acc@5  97.66 ( 96.44)
Epoch: [18][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.3604e-01 (7.3005e-01)	Acc@1  68.75 ( 77.80)	Acc@5  94.53 ( 96.41)
Epoch: [18][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5059e-01 (7.3435e-01)	Acc@1  78.75 ( 77.68)	Acc@5  96.25 ( 96.38)
## e[18] optimizer.zero_grad (sum) time: 0.17892909049987793
## e[18]       loss.backward (sum) time: 3.932391405105591
## e[18]      optimizer.step (sum) time: 1.2970271110534668
## epoch[18] training(only) time: 13.757405519485474
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.4404e+00 (1.4404e+00)	Acc@1  62.00 ( 62.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.021 ( 0.028)	Loss 1.4951e+00 (1.4539e+00)	Acc@1  63.00 ( 62.27)	Acc@5  90.00 ( 87.73)
Test: [ 20/100]	Time  0.022 ( 0.024)	Loss 1.3467e+00 (1.4228e+00)	Acc@1  64.00 ( 63.62)	Acc@5  89.00 ( 88.67)
Test: [ 30/100]	Time  0.015 ( 0.021)	Loss 1.7744e+00 (1.4344e+00)	Acc@1  59.00 ( 63.26)	Acc@5  85.00 ( 88.29)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.4160e+00 (1.4224e+00)	Acc@1  63.00 ( 63.22)	Acc@5  88.00 ( 88.56)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.5439e+00 (1.4374e+00)	Acc@1  65.00 ( 63.14)	Acc@5  88.00 ( 88.18)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.4941e+00 (1.4268e+00)	Acc@1  64.00 ( 63.41)	Acc@5  88.00 ( 88.23)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.6270e+00 (1.4285e+00)	Acc@1  57.00 ( 63.46)	Acc@5  88.00 ( 88.30)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.4463e+00 (1.4270e+00)	Acc@1  66.00 ( 63.37)	Acc@5  88.00 ( 88.23)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.7051e+00 (1.4125e+00)	Acc@1  57.00 ( 63.60)	Acc@5  86.00 ( 88.43)
 * Acc@1 63.690 Acc@5 88.490
### epoch[18] execution time: 15.720116376876831
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.190 ( 0.190)	Data  0.146 ( 0.146)	Loss 8.0371e-01 (8.0371e-01)	Acc@1  76.56 ( 76.56)	Acc@5  95.31 ( 95.31)
Epoch: [19][ 10/391]	Time  0.036 ( 0.049)	Data  0.001 ( 0.015)	Loss 7.4707e-01 (6.7727e-01)	Acc@1  79.69 ( 79.19)	Acc@5  94.53 ( 96.45)
Epoch: [19][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 6.7529e-01 (6.5910e-01)	Acc@1  76.56 ( 79.28)	Acc@5  99.22 ( 97.17)
Epoch: [19][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 6.1084e-01 (6.3432e-01)	Acc@1  82.03 ( 79.89)	Acc@5  96.88 ( 97.45)
Epoch: [19][ 40/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.005)	Loss 6.4990e-01 (6.3124e-01)	Acc@1  79.69 ( 79.84)	Acc@5  98.44 ( 97.60)
Epoch: [19][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.6357e-01 (6.3318e-01)	Acc@1  79.69 ( 79.79)	Acc@5  98.44 ( 97.64)
Epoch: [19][ 60/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.5576e-01 (6.4005e-01)	Acc@1  79.69 ( 79.55)	Acc@5  97.66 ( 97.58)
Epoch: [19][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 8.1836e-01 (6.4471e-01)	Acc@1  75.78 ( 79.47)	Acc@5  92.19 ( 97.40)
Epoch: [19][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.5420e-01 (6.4280e-01)	Acc@1  83.59 ( 79.52)	Acc@5  97.66 ( 97.35)
Epoch: [19][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.1279e-01 (6.4468e-01)	Acc@1  82.03 ( 79.46)	Acc@5  97.66 ( 97.37)
Epoch: [19][100/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.4658e-01 (6.4743e-01)	Acc@1  75.78 ( 79.44)	Acc@5  97.66 ( 97.35)
Epoch: [19][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.9131e-01 (6.5043e-01)	Acc@1  80.47 ( 79.31)	Acc@5  96.88 ( 97.34)
Epoch: [19][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.1250e-01 (6.5827e-01)	Acc@1  75.00 ( 79.08)	Acc@5  96.88 ( 97.25)
Epoch: [19][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.2266e-01 (6.6307e-01)	Acc@1  77.34 ( 78.98)	Acc@5  96.09 ( 97.14)
Epoch: [19][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.3428e-01 (6.6265e-01)	Acc@1  79.69 ( 79.13)	Acc@5  95.31 ( 97.14)
Epoch: [19][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0244e-01 (6.6362e-01)	Acc@1  85.94 ( 79.16)	Acc@5  99.22 ( 97.12)
Epoch: [19][160/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.5195e-01 (6.6607e-01)	Acc@1  75.00 ( 79.09)	Acc@5  98.44 ( 97.10)
Epoch: [19][170/391]	Time  0.040 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.2803e-01 (6.6529e-01)	Acc@1  78.91 ( 79.12)	Acc@5  96.09 ( 97.09)
Epoch: [19][180/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.0605e-01 (6.6608e-01)	Acc@1  79.69 ( 79.12)	Acc@5  96.88 ( 97.09)
Epoch: [19][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5498e-01 (6.6920e-01)	Acc@1  76.56 ( 79.08)	Acc@5  95.31 ( 97.11)
Epoch: [19][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.2822e-01 (6.7061e-01)	Acc@1  75.78 ( 79.13)	Acc@5  94.53 ( 97.04)
Epoch: [19][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9912e-01 (6.7007e-01)	Acc@1  80.47 ( 79.14)	Acc@5  98.44 ( 97.02)
Epoch: [19][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.4717e-01 (6.7193e-01)	Acc@1  71.09 ( 79.05)	Acc@5  96.88 ( 97.01)
Epoch: [19][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.7432e-01 (6.7126e-01)	Acc@1  78.12 ( 79.08)	Acc@5  96.88 ( 97.01)
Epoch: [19][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.1299e-01 (6.7407e-01)	Acc@1  74.22 ( 79.03)	Acc@5  96.09 ( 96.97)
Epoch: [19][250/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.8154e-01 (6.7622e-01)	Acc@1  81.25 ( 78.94)	Acc@5  97.66 ( 96.97)
Epoch: [19][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.4316e-01 (6.7922e-01)	Acc@1  77.34 ( 78.89)	Acc@5  98.44 ( 96.96)
Epoch: [19][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.2764e-01 (6.8157e-01)	Acc@1  75.00 ( 78.85)	Acc@5  96.09 ( 96.94)
Epoch: [19][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5156e-01 (6.8357e-01)	Acc@1  74.22 ( 78.80)	Acc@5  96.09 ( 96.92)
Epoch: [19][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.0166e-01 (6.8507e-01)	Acc@1  76.56 ( 78.75)	Acc@5  97.66 ( 96.90)
Epoch: [19][300/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7192e-01 (6.8686e-01)	Acc@1  85.94 ( 78.72)	Acc@5  97.66 ( 96.89)
Epoch: [19][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1875e-01 (6.8878e-01)	Acc@1  76.56 ( 78.66)	Acc@5  97.66 ( 96.86)
Epoch: [19][320/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7109e-01 (6.9100e-01)	Acc@1  71.09 ( 78.60)	Acc@5  96.09 ( 96.84)
Epoch: [19][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.8320e-01 (6.9234e-01)	Acc@1  75.78 ( 78.55)	Acc@5  96.88 ( 96.84)
Epoch: [19][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7627e-01 (6.9358e-01)	Acc@1  82.81 ( 78.48)	Acc@5  96.09 ( 96.84)
Epoch: [19][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9287e-01 (6.9329e-01)	Acc@1  76.56 ( 78.44)	Acc@5  98.44 ( 96.86)
Epoch: [19][360/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9043e-01 (6.9457e-01)	Acc@1  81.25 ( 78.41)	Acc@5  96.09 ( 96.83)
Epoch: [19][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9375e-01 (6.9566e-01)	Acc@1  82.81 ( 78.37)	Acc@5  97.66 ( 96.82)
Epoch: [19][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0098e-01 (6.9681e-01)	Acc@1  84.38 ( 78.33)	Acc@5  99.22 ( 96.82)
Epoch: [19][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.9102e-01 (6.9823e-01)	Acc@1  77.50 ( 78.31)	Acc@5  95.00 ( 96.78)
## e[19] optimizer.zero_grad (sum) time: 0.1791670322418213
## e[19]       loss.backward (sum) time: 4.070358991622925
## e[19]      optimizer.step (sum) time: 1.2450196743011475
## epoch[19] training(only) time: 13.859461784362793
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.5723e+00 (1.5723e+00)	Acc@1  63.00 ( 63.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 2.0020e+00 (1.6816e+00)	Acc@1  61.00 ( 63.00)	Acc@5  87.00 ( 86.73)
Test: [ 20/100]	Time  0.019 ( 0.024)	Loss 1.5244e+00 (1.6409e+00)	Acc@1  61.00 ( 62.86)	Acc@5  89.00 ( 86.86)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 2.0371e+00 (1.6513e+00)	Acc@1  52.00 ( 62.06)	Acc@5  81.00 ( 86.39)
Test: [ 40/100]	Time  0.015 ( 0.020)	Loss 1.4160e+00 (1.6242e+00)	Acc@1  64.00 ( 61.90)	Acc@5  89.00 ( 87.05)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.5703e+00 (1.6232e+00)	Acc@1  70.00 ( 62.08)	Acc@5  87.00 ( 86.88)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 1.5088e+00 (1.6025e+00)	Acc@1  61.00 ( 61.98)	Acc@5  88.00 ( 87.21)
Test: [ 70/100]	Time  0.021 ( 0.020)	Loss 1.5771e+00 (1.6006e+00)	Acc@1  64.00 ( 61.92)	Acc@5  91.00 ( 87.24)
Test: [ 80/100]	Time  0.022 ( 0.019)	Loss 1.7051e+00 (1.5983e+00)	Acc@1  61.00 ( 62.01)	Acc@5  83.00 ( 87.16)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.8340e+00 (1.5811e+00)	Acc@1  58.00 ( 62.07)	Acc@5  88.00 ( 87.35)
 * Acc@1 62.130 Acc@5 87.430
### epoch[19] execution time: 15.815819025039673
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.189 ( 0.189)	Data  0.150 ( 0.150)	Loss 6.8555e-01 (6.8555e-01)	Acc@1  78.12 ( 78.12)	Acc@5  98.44 ( 98.44)
Epoch: [20][ 10/391]	Time  0.036 ( 0.050)	Data  0.001 ( 0.015)	Loss 5.6445e-01 (6.3488e-01)	Acc@1  81.25 ( 80.18)	Acc@5  98.44 ( 97.09)
Epoch: [20][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 5.8594e-01 (5.9159e-01)	Acc@1  81.25 ( 81.44)	Acc@5  97.66 ( 97.62)
Epoch: [20][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 5.8643e-01 (5.8482e-01)	Acc@1  80.47 ( 81.33)	Acc@5  98.44 ( 97.76)
Epoch: [20][ 40/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.005)	Loss 6.2354e-01 (5.9701e-01)	Acc@1  78.91 ( 81.17)	Acc@5  96.88 ( 97.62)
Epoch: [20][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.3184e-01 (6.0060e-01)	Acc@1  79.69 ( 81.07)	Acc@5  99.22 ( 97.56)
Epoch: [20][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.9570e-01 (5.9924e-01)	Acc@1  79.69 ( 81.08)	Acc@5  96.88 ( 97.64)
Epoch: [20][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.8018e-01 (5.9770e-01)	Acc@1  78.91 ( 81.32)	Acc@5  97.66 ( 97.69)
Epoch: [20][ 80/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.0459e-01 (6.0299e-01)	Acc@1  78.91 ( 81.23)	Acc@5  96.88 ( 97.60)
Epoch: [20][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.9341e-01 (6.0033e-01)	Acc@1  82.81 ( 81.15)	Acc@5  98.44 ( 97.59)
Epoch: [20][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8896e-01 (6.0458e-01)	Acc@1  75.78 ( 81.09)	Acc@5  96.88 ( 97.52)
Epoch: [20][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1953e-01 (5.9984e-01)	Acc@1  86.72 ( 81.26)	Acc@5  97.66 ( 97.54)
Epoch: [20][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.7764e-01 (6.0254e-01)	Acc@1  79.69 ( 81.17)	Acc@5  99.22 ( 97.55)
Epoch: [20][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.7275e-01 (6.0343e-01)	Acc@1  83.59 ( 81.15)	Acc@5  96.88 ( 97.51)
Epoch: [20][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.5000e-01 (6.0434e-01)	Acc@1  80.47 ( 81.13)	Acc@5  95.31 ( 97.48)
Epoch: [20][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5186e-01 (6.0819e-01)	Acc@1  76.56 ( 81.01)	Acc@5  97.66 ( 97.43)
Epoch: [20][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.1865e-01 (6.1097e-01)	Acc@1  85.16 ( 81.01)	Acc@5  97.66 ( 97.42)
Epoch: [20][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.1035e-01 (6.1171e-01)	Acc@1  84.38 ( 80.94)	Acc@5  96.09 ( 97.44)
Epoch: [20][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.7393e-01 (6.1572e-01)	Acc@1  73.44 ( 80.82)	Acc@5  98.44 ( 97.41)
Epoch: [20][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.7490e-01 (6.1799e-01)	Acc@1  69.53 ( 80.74)	Acc@5  98.44 ( 97.41)
Epoch: [20][200/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1484e-01 (6.1840e-01)	Acc@1  77.34 ( 80.69)	Acc@5  97.66 ( 97.42)
Epoch: [20][210/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3467e-01 (6.1841e-01)	Acc@1  85.16 ( 80.68)	Acc@5  97.66 ( 97.43)
Epoch: [20][220/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8066e-01 (6.2083e-01)	Acc@1  78.12 ( 80.59)	Acc@5  96.09 ( 97.42)
Epoch: [20][230/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2207e-01 (6.2271e-01)	Acc@1  77.34 ( 80.53)	Acc@5  97.66 ( 97.43)
Epoch: [20][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.0674e-01 (6.2473e-01)	Acc@1  71.09 ( 80.40)	Acc@5  96.88 ( 97.45)
Epoch: [20][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.6582e-01 (6.2993e-01)	Acc@1  71.09 ( 80.24)	Acc@5  94.53 ( 97.40)
Epoch: [20][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4863e-01 (6.3426e-01)	Acc@1  71.09 ( 80.12)	Acc@5  96.09 ( 97.35)
Epoch: [20][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9814e-01 (6.3677e-01)	Acc@1  82.81 ( 80.07)	Acc@5  98.44 ( 97.35)
Epoch: [20][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1094e-01 (6.4153e-01)	Acc@1  78.12 ( 79.92)	Acc@5  95.31 ( 97.32)
Epoch: [20][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5918e-01 (6.4323e-01)	Acc@1  78.12 ( 79.85)	Acc@5  97.66 ( 97.30)
Epoch: [20][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.3838e-01 (6.4600e-01)	Acc@1  77.34 ( 79.76)	Acc@5  95.31 ( 97.28)
Epoch: [20][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0010e-01 (6.4699e-01)	Acc@1  78.12 ( 79.75)	Acc@5  96.09 ( 97.26)
Epoch: [20][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5146e-01 (6.4857e-01)	Acc@1  76.56 ( 79.66)	Acc@5  96.09 ( 97.26)
Epoch: [20][330/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5635e-01 (6.5074e-01)	Acc@1  76.56 ( 79.63)	Acc@5  97.66 ( 97.24)
Epoch: [20][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.1299e-01 (6.5164e-01)	Acc@1  72.66 ( 79.60)	Acc@5  97.66 ( 97.23)
Epoch: [20][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9580e-01 (6.5282e-01)	Acc@1  80.47 ( 79.58)	Acc@5  95.31 ( 97.21)
Epoch: [20][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.4365e-01 (6.5386e-01)	Acc@1  78.91 ( 79.54)	Acc@5  96.88 ( 97.20)
Epoch: [20][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0117e+00 (6.5670e-01)	Acc@1  68.75 ( 79.46)	Acc@5  94.53 ( 97.18)
Epoch: [20][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.5117e-01 (6.6102e-01)	Acc@1  69.53 ( 79.32)	Acc@5  96.88 ( 97.16)
Epoch: [20][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4424e-01 (6.6168e-01)	Acc@1  78.75 ( 79.31)	Acc@5  92.50 ( 97.15)
## e[20] optimizer.zero_grad (sum) time: 0.18083691596984863
## e[20]       loss.backward (sum) time: 4.137673377990723
## e[20]      optimizer.step (sum) time: 1.2325716018676758
## epoch[20] training(only) time: 13.840377807617188
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.3291e+00 (1.3291e+00)	Acc@1  68.00 ( 68.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.021 ( 0.030)	Loss 1.4824e+00 (1.5537e+00)	Acc@1  62.00 ( 63.82)	Acc@5  89.00 ( 85.91)
Test: [ 20/100]	Time  0.025 ( 0.024)	Loss 1.5098e+00 (1.5255e+00)	Acc@1  64.00 ( 63.81)	Acc@5  88.00 ( 87.10)
Test: [ 30/100]	Time  0.019 ( 0.022)	Loss 2.0039e+00 (1.5394e+00)	Acc@1  53.00 ( 63.32)	Acc@5  84.00 ( 87.52)
Test: [ 40/100]	Time  0.025 ( 0.022)	Loss 1.2793e+00 (1.5094e+00)	Acc@1  66.00 ( 63.61)	Acc@5  91.00 ( 87.98)
Test: [ 50/100]	Time  0.018 ( 0.021)	Loss 1.6445e+00 (1.5175e+00)	Acc@1  60.00 ( 63.27)	Acc@5  86.00 ( 87.67)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.5576e+00 (1.5098e+00)	Acc@1  61.00 ( 63.28)	Acc@5  85.00 ( 87.84)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 1.5430e+00 (1.5104e+00)	Acc@1  62.00 ( 62.92)	Acc@5  89.00 ( 87.87)
Test: [ 80/100]	Time  0.015 ( 0.020)	Loss 1.6260e+00 (1.5185e+00)	Acc@1  62.00 ( 62.62)	Acc@5  81.00 ( 87.68)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 2.0547e+00 (1.5107e+00)	Acc@1  55.00 ( 62.67)	Acc@5  81.00 ( 87.68)
 * Acc@1 62.740 Acc@5 87.730
### epoch[20] execution time: 15.823706150054932
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.196 ( 0.196)	Data  0.154 ( 0.154)	Loss 7.5195e-01 (7.5195e-01)	Acc@1  78.12 ( 78.12)	Acc@5  95.31 ( 95.31)
Epoch: [21][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.015)	Loss 6.4502e-01 (5.8008e-01)	Acc@1  82.03 ( 82.10)	Acc@5  96.88 ( 98.01)
Epoch: [21][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 7.7344e-01 (5.9091e-01)	Acc@1  74.22 ( 81.88)	Acc@5  94.53 ( 97.81)
Epoch: [21][ 30/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.006)	Loss 5.9033e-01 (5.8105e-01)	Acc@1  82.81 ( 81.91)	Acc@5  98.44 ( 97.93)
Epoch: [21][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.2002e-01 (5.6235e-01)	Acc@1  84.38 ( 82.36)	Acc@5  99.22 ( 98.04)
Epoch: [21][ 50/391]	Time  0.028 ( 0.038)	Data  0.001 ( 0.005)	Loss 5.3564e-01 (5.6655e-01)	Acc@1  84.38 ( 82.40)	Acc@5  98.44 ( 98.05)
Epoch: [21][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.1680e-01 (5.7820e-01)	Acc@1  79.69 ( 82.02)	Acc@5  96.09 ( 97.85)
Epoch: [21][ 70/391]	Time  0.030 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.4482e-01 (5.7840e-01)	Acc@1  84.38 ( 82.02)	Acc@5  97.66 ( 97.83)
Epoch: [21][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.0977e-01 (5.7739e-01)	Acc@1  83.59 ( 81.99)	Acc@5  96.09 ( 97.80)
Epoch: [21][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.1240e-01 (5.7734e-01)	Acc@1  78.91 ( 82.12)	Acc@5  95.31 ( 97.82)
Epoch: [21][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.5430e-01 (5.7882e-01)	Acc@1  80.47 ( 82.11)	Acc@5  97.66 ( 97.80)
Epoch: [21][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0635e-01 (5.8002e-01)	Acc@1  83.59 ( 82.08)	Acc@5 100.00 ( 97.77)
Epoch: [21][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.3428e-01 (5.8000e-01)	Acc@1  80.47 ( 82.06)	Acc@5  97.66 ( 97.80)
Epoch: [21][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.3730e-01 (5.8186e-01)	Acc@1  78.12 ( 81.95)	Acc@5  95.31 ( 97.79)
Epoch: [21][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0156e-01 (5.8443e-01)	Acc@1  80.47 ( 81.85)	Acc@5  96.88 ( 97.76)
Epoch: [21][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1123e-01 (5.8525e-01)	Acc@1  85.16 ( 81.78)	Acc@5  96.88 ( 97.76)
Epoch: [21][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0938e-01 (5.8836e-01)	Acc@1  78.12 ( 81.68)	Acc@5  96.88 ( 97.73)
Epoch: [21][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.4023e-01 (5.9058e-01)	Acc@1  78.12 ( 81.57)	Acc@5  96.09 ( 97.72)
Epoch: [21][180/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.0264e-01 (5.9446e-01)	Acc@1  79.69 ( 81.42)	Acc@5  96.88 ( 97.70)
Epoch: [21][190/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2012e-01 (5.9983e-01)	Acc@1  82.03 ( 81.21)	Acc@5  95.31 ( 97.64)
Epoch: [21][200/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.2607e-01 (5.9971e-01)	Acc@1  77.34 ( 81.20)	Acc@5  95.31 ( 97.64)
Epoch: [21][210/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8057e-01 (6.0032e-01)	Acc@1  82.03 ( 81.19)	Acc@5  99.22 ( 97.65)
Epoch: [21][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1709e-01 (5.9952e-01)	Acc@1  86.72 ( 81.22)	Acc@5  96.09 ( 97.64)
Epoch: [21][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0029e-01 (6.0120e-01)	Acc@1  72.66 ( 81.14)	Acc@5  98.44 ( 97.63)
Epoch: [21][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1953e-01 (6.0350e-01)	Acc@1  82.03 ( 81.06)	Acc@5  98.44 ( 97.62)
Epoch: [21][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7090e-01 (6.0794e-01)	Acc@1  80.47 ( 80.94)	Acc@5  97.66 ( 97.58)
Epoch: [21][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.2461e-01 (6.1129e-01)	Acc@1  79.69 ( 80.85)	Acc@5  97.66 ( 97.55)
Epoch: [21][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5215e-01 (6.1235e-01)	Acc@1  85.16 ( 80.82)	Acc@5  98.44 ( 97.52)
Epoch: [21][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7578e-01 (6.1452e-01)	Acc@1  73.44 ( 80.72)	Acc@5  98.44 ( 97.52)
Epoch: [21][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1758e-01 (6.1923e-01)	Acc@1  83.59 ( 80.57)	Acc@5  97.66 ( 97.49)
Epoch: [21][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0986e-01 (6.2025e-01)	Acc@1  80.47 ( 80.56)	Acc@5  97.66 ( 97.46)
Epoch: [21][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1514e-01 (6.2061e-01)	Acc@1  82.03 ( 80.56)	Acc@5 100.00 ( 97.46)
Epoch: [21][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8838e-01 (6.2197e-01)	Acc@1  82.03 ( 80.50)	Acc@5  97.66 ( 97.46)
Epoch: [21][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5967e-01 (6.2381e-01)	Acc@1  79.69 ( 80.45)	Acc@5  96.88 ( 97.45)
Epoch: [21][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.4268e-01 (6.2579e-01)	Acc@1  80.47 ( 80.39)	Acc@5  96.88 ( 97.44)
Epoch: [21][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8506e-01 (6.2835e-01)	Acc@1  75.00 ( 80.32)	Acc@5  96.88 ( 97.41)
Epoch: [21][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2539e-01 (6.2958e-01)	Acc@1  84.38 ( 80.28)	Acc@5  96.88 ( 97.39)
Epoch: [21][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6650e-01 (6.3058e-01)	Acc@1  80.47 ( 80.26)	Acc@5  98.44 ( 97.39)
Epoch: [21][380/391]	Time  0.035 ( 0.035)	Data  0.000 ( 0.002)	Loss 8.7842e-01 (6.3226e-01)	Acc@1  75.00 ( 80.23)	Acc@5  95.31 ( 97.38)
Epoch: [21][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.2568e-01 (6.3395e-01)	Acc@1  71.25 ( 80.16)	Acc@5  96.25 ( 97.35)
## e[21] optimizer.zero_grad (sum) time: 0.1781904697418213
## e[21]       loss.backward (sum) time: 3.944371223449707
## e[21]      optimizer.step (sum) time: 1.279529333114624
## epoch[21] training(only) time: 13.769596576690674
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.3916e+00 (1.3916e+00)	Acc@1  68.00 ( 68.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.016 ( 0.030)	Loss 1.6064e+00 (1.5864e+00)	Acc@1  62.00 ( 62.00)	Acc@5  84.00 ( 86.45)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 1.1074e+00 (1.4924e+00)	Acc@1  76.00 ( 63.71)	Acc@5  94.00 ( 87.67)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.7607e+00 (1.4957e+00)	Acc@1  61.00 ( 63.65)	Acc@5  86.00 ( 87.61)
Test: [ 40/100]	Time  0.021 ( 0.021)	Loss 1.5098e+00 (1.4836e+00)	Acc@1  61.00 ( 63.85)	Acc@5  87.00 ( 87.85)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5547e+00 (1.4928e+00)	Acc@1  61.00 ( 63.49)	Acc@5  88.00 ( 87.78)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 1.5205e+00 (1.4902e+00)	Acc@1  65.00 ( 63.77)	Acc@5  89.00 ( 87.98)
Test: [ 70/100]	Time  0.021 ( 0.020)	Loss 1.5781e+00 (1.4908e+00)	Acc@1  64.00 ( 63.59)	Acc@5  91.00 ( 88.03)
Test: [ 80/100]	Time  0.015 ( 0.020)	Loss 1.4395e+00 (1.4855e+00)	Acc@1  65.00 ( 63.74)	Acc@5  88.00 ( 88.20)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.9746e+00 (1.4751e+00)	Acc@1  51.00 ( 63.79)	Acc@5  89.00 ( 88.45)
 * Acc@1 63.900 Acc@5 88.500
### epoch[21] execution time: 15.775181293487549
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.189 ( 0.189)	Data  0.147 ( 0.147)	Loss 4.4507e-01 (4.4507e-01)	Acc@1  89.06 ( 89.06)	Acc@5  99.22 ( 99.22)
Epoch: [22][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 6.2939e-01 (5.6559e-01)	Acc@1  75.78 ( 82.74)	Acc@5  99.22 ( 98.30)
Epoch: [22][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.009)	Loss 4.9683e-01 (5.5191e-01)	Acc@1  81.25 ( 82.81)	Acc@5  99.22 ( 98.10)
Epoch: [22][ 30/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.006)	Loss 5.7520e-01 (5.4166e-01)	Acc@1  82.81 ( 83.29)	Acc@5  98.44 ( 98.19)
Epoch: [22][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.4834e-01 (5.3561e-01)	Acc@1  78.12 ( 83.35)	Acc@5 100.00 ( 98.21)
Epoch: [22][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.1367e-01 (5.3195e-01)	Acc@1  85.94 ( 83.41)	Acc@5  98.44 ( 98.24)
Epoch: [22][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.9658e-01 (5.3765e-01)	Acc@1  85.16 ( 83.40)	Acc@5  98.44 ( 98.18)
Epoch: [22][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.7861e-01 (5.4222e-01)	Acc@1  81.25 ( 83.07)	Acc@5  97.66 ( 98.11)
Epoch: [22][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.8350e-01 (5.4470e-01)	Acc@1  82.03 ( 83.07)	Acc@5  97.66 ( 98.07)
Epoch: [22][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.6689e-01 (5.4484e-01)	Acc@1  78.91 ( 82.97)	Acc@5  98.44 ( 98.09)
Epoch: [22][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0405e-01 (5.4245e-01)	Acc@1  86.72 ( 83.05)	Acc@5  98.44 ( 98.07)
Epoch: [22][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.5723e-01 (5.4853e-01)	Acc@1  78.12 ( 82.84)	Acc@5  95.31 ( 97.99)
Epoch: [22][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.7480e-01 (5.4896e-01)	Acc@1  80.47 ( 82.84)	Acc@5  97.66 ( 98.01)
Epoch: [22][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.3564e-01 (5.5436e-01)	Acc@1  85.16 ( 82.66)	Acc@5  96.09 ( 98.00)
Epoch: [22][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.0068e-01 (5.5717e-01)	Acc@1  76.56 ( 82.49)	Acc@5  99.22 ( 97.98)
Epoch: [22][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.2363e-01 (5.5719e-01)	Acc@1  77.34 ( 82.48)	Acc@5  97.66 ( 97.97)
Epoch: [22][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.2354e-01 (5.6105e-01)	Acc@1  82.03 ( 82.41)	Acc@5  96.88 ( 97.95)
Epoch: [22][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.3135e-01 (5.6369e-01)	Acc@1  82.03 ( 82.32)	Acc@5  99.22 ( 97.95)
Epoch: [22][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8901e-01 (5.6269e-01)	Acc@1  85.16 ( 82.30)	Acc@5  99.22 ( 97.95)
Epoch: [22][190/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6348e-01 (5.6393e-01)	Acc@1  81.25 ( 82.27)	Acc@5  96.88 ( 97.95)
Epoch: [22][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4248e-01 (5.6339e-01)	Acc@1  81.25 ( 82.23)	Acc@5  99.22 ( 97.98)
Epoch: [22][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.5908e-01 (5.6346e-01)	Acc@1  84.38 ( 82.22)	Acc@5  97.66 ( 97.99)
Epoch: [22][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5527e-01 (5.6480e-01)	Acc@1  80.47 ( 82.18)	Acc@5  96.09 ( 97.96)
Epoch: [22][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.3389e-01 (5.6858e-01)	Acc@1  76.56 ( 82.04)	Acc@5  97.66 ( 97.97)
Epoch: [22][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5288e-01 (5.6982e-01)	Acc@1  85.16 ( 82.01)	Acc@5  99.22 ( 97.96)
Epoch: [22][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9580e-01 (5.7058e-01)	Acc@1  80.47 ( 82.01)	Acc@5  93.75 ( 97.93)
Epoch: [22][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6260e-01 (5.7347e-01)	Acc@1  78.91 ( 81.96)	Acc@5  97.66 ( 97.90)
Epoch: [22][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8359e-01 (5.7531e-01)	Acc@1  77.34 ( 81.91)	Acc@5  96.09 ( 97.90)
Epoch: [22][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.3545e-01 (5.7813e-01)	Acc@1  72.66 ( 81.81)	Acc@5  96.09 ( 97.86)
Epoch: [22][290/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7666e-01 (5.8031e-01)	Acc@1  82.81 ( 81.74)	Acc@5  97.66 ( 97.85)
Epoch: [22][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4053e-01 (5.8182e-01)	Acc@1  85.94 ( 81.68)	Acc@5  98.44 ( 97.83)
Epoch: [22][310/391]	Time  0.041 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5293e-01 (5.8609e-01)	Acc@1  75.78 ( 81.55)	Acc@5  96.88 ( 97.81)
Epoch: [22][320/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.1357e-01 (5.8948e-01)	Acc@1  74.22 ( 81.44)	Acc@5  94.53 ( 97.80)
Epoch: [22][330/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4453e-01 (5.9332e-01)	Acc@1  80.47 ( 81.31)	Acc@5  97.66 ( 97.76)
Epoch: [22][340/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1045e-01 (5.9568e-01)	Acc@1  75.00 ( 81.21)	Acc@5  95.31 ( 97.75)
Epoch: [22][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7827e-01 (5.9666e-01)	Acc@1  86.72 ( 81.16)	Acc@5  99.22 ( 97.74)
Epoch: [22][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.3779e-01 (5.9864e-01)	Acc@1  81.25 ( 81.10)	Acc@5  98.44 ( 97.72)
Epoch: [22][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4131e-01 (6.0113e-01)	Acc@1  71.88 ( 81.05)	Acc@5  96.09 ( 97.71)
Epoch: [22][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5283e-01 (6.0251e-01)	Acc@1  80.47 ( 81.02)	Acc@5  97.66 ( 97.69)
Epoch: [22][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1436e-01 (6.0345e-01)	Acc@1  78.75 ( 80.99)	Acc@5  95.00 ( 97.67)
## e[22] optimizer.zero_grad (sum) time: 0.17821121215820312
## e[22]       loss.backward (sum) time: 3.9747118949890137
## e[22]      optimizer.step (sum) time: 1.2766222953796387
## epoch[22] training(only) time: 13.821792364120483
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.5762e+00 (1.5762e+00)	Acc@1  62.00 ( 62.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 1.7510e+00 (1.4859e+00)	Acc@1  58.00 ( 64.27)	Acc@5  85.00 ( 86.73)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.3818e+00 (1.4202e+00)	Acc@1  68.00 ( 65.52)	Acc@5  90.00 ( 88.67)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 2.0293e+00 (1.4538e+00)	Acc@1  52.00 ( 65.16)	Acc@5  83.00 ( 87.77)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.4443e+00 (1.4272e+00)	Acc@1  65.00 ( 65.12)	Acc@5  93.00 ( 88.71)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.5283e+00 (1.4611e+00)	Acc@1  64.00 ( 64.57)	Acc@5  86.00 ( 88.14)
Test: [ 60/100]	Time  0.019 ( 0.020)	Loss 1.5215e+00 (1.4539e+00)	Acc@1  61.00 ( 64.41)	Acc@5  90.00 ( 88.39)
Test: [ 70/100]	Time  0.014 ( 0.020)	Loss 1.4902e+00 (1.4591e+00)	Acc@1  62.00 ( 64.13)	Acc@5  88.00 ( 88.44)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.5283e+00 (1.4619e+00)	Acc@1  62.00 ( 63.80)	Acc@5  89.00 ( 88.53)
Test: [ 90/100]	Time  0.022 ( 0.019)	Loss 1.9531e+00 (1.4575e+00)	Acc@1  55.00 ( 64.03)	Acc@5  84.00 ( 88.49)
 * Acc@1 64.100 Acc@5 88.530
### epoch[22] execution time: 15.824264526367188
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.195 ( 0.195)	Data  0.151 ( 0.151)	Loss 4.6069e-01 (4.6069e-01)	Acc@1  85.16 ( 85.16)	Acc@5  97.66 ( 97.66)
Epoch: [23][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 5.0342e-01 (4.9880e-01)	Acc@1  82.81 ( 85.16)	Acc@5  99.22 ( 98.44)
Epoch: [23][ 20/391]	Time  0.033 ( 0.043)	Data  0.001 ( 0.009)	Loss 4.0820e-01 (4.8233e-01)	Acc@1  85.94 ( 85.08)	Acc@5  99.22 ( 98.70)
Epoch: [23][ 30/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.006)	Loss 5.5273e-01 (4.8712e-01)	Acc@1  86.72 ( 84.90)	Acc@5  98.44 ( 98.56)
Epoch: [23][ 40/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.6362e-01 (4.8988e-01)	Acc@1  84.38 ( 84.58)	Acc@5  99.22 ( 98.63)
Epoch: [23][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.4404e-01 (5.0045e-01)	Acc@1  78.12 ( 84.15)	Acc@5  96.88 ( 98.62)
Epoch: [23][ 60/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.4346e-01 (4.9568e-01)	Acc@1  81.25 ( 84.31)	Acc@5  99.22 ( 98.63)
Epoch: [23][ 70/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.8066e-01 (5.0500e-01)	Acc@1  78.12 ( 83.96)	Acc@5  96.88 ( 98.48)
Epoch: [23][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.6641e-01 (5.0821e-01)	Acc@1  82.03 ( 83.83)	Acc@5  96.09 ( 98.43)
Epoch: [23][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.1357e-01 (5.1316e-01)	Acc@1  86.72 ( 83.67)	Acc@5 100.00 ( 98.39)
Epoch: [23][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8506e-01 (5.1859e-01)	Acc@1  75.78 ( 83.52)	Acc@5  98.44 ( 98.34)
Epoch: [23][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.7080e-01 (5.2321e-01)	Acc@1  81.25 ( 83.39)	Acc@5 100.00 ( 98.29)
Epoch: [23][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0010e-01 (5.2721e-01)	Acc@1  82.03 ( 83.30)	Acc@5  96.09 ( 98.21)
Epoch: [23][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5044e-01 (5.3049e-01)	Acc@1  83.59 ( 83.15)	Acc@5  98.44 ( 98.19)
Epoch: [23][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4688e-01 (5.2879e-01)	Acc@1  78.12 ( 83.19)	Acc@5  99.22 ( 98.20)
Epoch: [23][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6055e-01 (5.2753e-01)	Acc@1  82.03 ( 83.25)	Acc@5  97.66 ( 98.19)
Epoch: [23][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.9922e-01 (5.3029e-01)	Acc@1  74.22 ( 83.19)	Acc@5  97.66 ( 98.17)
Epoch: [23][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.3760e-01 (5.3197e-01)	Acc@1  83.59 ( 83.12)	Acc@5  95.31 ( 98.15)
Epoch: [23][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5449e-01 (5.3594e-01)	Acc@1  77.34 ( 83.02)	Acc@5  95.31 ( 98.10)
Epoch: [23][190/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9365e-01 (5.3524e-01)	Acc@1  80.47 ( 83.03)	Acc@5 100.00 ( 98.11)
Epoch: [23][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3799e-01 (5.3916e-01)	Acc@1  86.72 ( 82.95)	Acc@5 100.00 ( 98.09)
Epoch: [23][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.5127e-01 (5.4033e-01)	Acc@1  82.81 ( 82.93)	Acc@5  96.88 ( 98.07)
Epoch: [23][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.8350e-01 (5.4130e-01)	Acc@1  79.69 ( 82.93)	Acc@5  98.44 ( 98.05)
Epoch: [23][230/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4990e-01 (5.4258e-01)	Acc@1  76.56 ( 82.86)	Acc@5 100.00 ( 98.06)
Epoch: [23][240/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1562e-01 (5.4487e-01)	Acc@1  84.38 ( 82.81)	Acc@5  98.44 ( 98.04)
Epoch: [23][250/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0352e-01 (5.4848e-01)	Acc@1  78.91 ( 82.66)	Acc@5  99.22 ( 98.02)
Epoch: [23][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3320e-01 (5.5047e-01)	Acc@1  83.59 ( 82.65)	Acc@5  99.22 ( 98.01)
Epoch: [23][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6699e-01 (5.5288e-01)	Acc@1  79.69 ( 82.61)	Acc@5  96.09 ( 97.99)
Epoch: [23][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8594e-01 (5.5303e-01)	Acc@1  82.03 ( 82.64)	Acc@5  97.66 ( 97.99)
Epoch: [23][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3330e-01 (5.5529e-01)	Acc@1  80.47 ( 82.58)	Acc@5  97.66 ( 97.97)
Epoch: [23][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4541e-01 (5.5768e-01)	Acc@1  85.94 ( 82.53)	Acc@5  96.09 ( 97.95)
Epoch: [23][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3281e-01 (5.5931e-01)	Acc@1  78.91 ( 82.48)	Acc@5  97.66 ( 97.95)
Epoch: [23][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7041e-01 (5.6172e-01)	Acc@1  82.81 ( 82.42)	Acc@5  96.09 ( 97.94)
Epoch: [23][330/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3467e-01 (5.6399e-01)	Acc@1  83.59 ( 82.35)	Acc@5  98.44 ( 97.92)
Epoch: [23][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5127e-01 (5.6535e-01)	Acc@1  81.25 ( 82.32)	Acc@5  97.66 ( 97.90)
Epoch: [23][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5039e-01 (5.6646e-01)	Acc@1  78.91 ( 82.30)	Acc@5  96.09 ( 97.88)
Epoch: [23][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7534e-01 (5.6716e-01)	Acc@1  85.16 ( 82.30)	Acc@5  99.22 ( 97.84)
Epoch: [23][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6406e-01 (5.6925e-01)	Acc@1  83.59 ( 82.24)	Acc@5  96.88 ( 97.84)
Epoch: [23][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0391e-01 (5.7038e-01)	Acc@1  82.81 ( 82.23)	Acc@5  97.66 ( 97.83)
Epoch: [23][390/391]	Time  0.027 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6348e-01 (5.7326e-01)	Acc@1  86.25 ( 82.17)	Acc@5  96.25 ( 97.80)
## e[23] optimizer.zero_grad (sum) time: 0.1796102523803711
## e[23]       loss.backward (sum) time: 4.099212884902954
## e[23]      optimizer.step (sum) time: 1.2479770183563232
## epoch[23] training(only) time: 13.873904466629028
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.6338e+00 (1.6338e+00)	Acc@1  62.00 ( 62.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.6455e+00 (1.6183e+00)	Acc@1  58.00 ( 62.18)	Acc@5  88.00 ( 87.64)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.4395e+00 (1.5625e+00)	Acc@1  61.00 ( 63.24)	Acc@5  88.00 ( 87.90)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 2.0840e+00 (1.6077e+00)	Acc@1  55.00 ( 62.42)	Acc@5  83.00 ( 87.10)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.5342e+00 (1.5912e+00)	Acc@1  64.00 ( 62.66)	Acc@5  89.00 ( 87.61)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.8301e+00 (1.6120e+00)	Acc@1  58.00 ( 62.29)	Acc@5  85.00 ( 87.47)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.6660e+00 (1.5836e+00)	Acc@1  57.00 ( 62.80)	Acc@5  88.00 ( 87.82)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.6504e+00 (1.5729e+00)	Acc@1  65.00 ( 62.90)	Acc@5  85.00 ( 87.94)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.5205e+00 (1.5705e+00)	Acc@1  68.00 ( 62.80)	Acc@5  85.00 ( 87.93)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 2.0488e+00 (1.5618e+00)	Acc@1  58.00 ( 63.11)	Acc@5  84.00 ( 88.01)
 * Acc@1 63.250 Acc@5 88.000
### epoch[23] execution time: 15.84490156173706
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.179 ( 0.179)	Data  0.139 ( 0.139)	Loss 5.3320e-01 (5.3320e-01)	Acc@1  88.28 ( 88.28)	Acc@5  96.09 ( 96.09)
Epoch: [24][ 10/391]	Time  0.034 ( 0.048)	Data  0.001 ( 0.014)	Loss 5.0684e-01 (5.0519e-01)	Acc@1  81.25 ( 83.38)	Acc@5  98.44 ( 98.79)
Epoch: [24][ 20/391]	Time  0.034 ( 0.041)	Data  0.001 ( 0.008)	Loss 5.3760e-01 (4.8229e-01)	Acc@1  82.03 ( 84.67)	Acc@5  98.44 ( 98.70)
Epoch: [24][ 30/391]	Time  0.037 ( 0.039)	Data  0.002 ( 0.006)	Loss 4.3677e-01 (4.8082e-01)	Acc@1  89.84 ( 84.78)	Acc@5  98.44 ( 98.59)
Epoch: [24][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.9023e-01 (4.8579e-01)	Acc@1  82.81 ( 84.62)	Acc@5  97.66 ( 98.59)
Epoch: [24][ 50/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.6401e-01 (4.8927e-01)	Acc@1  89.06 ( 84.70)	Acc@5  99.22 ( 98.54)
Epoch: [24][ 60/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.2847e-01 (4.7902e-01)	Acc@1  85.94 ( 84.95)	Acc@5  97.66 ( 98.58)
Epoch: [24][ 70/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.2920e-01 (4.7572e-01)	Acc@1  85.94 ( 85.07)	Acc@5  99.22 ( 98.62)
Epoch: [24][ 80/391]	Time  0.030 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.1821e-01 (4.7587e-01)	Acc@1  86.72 ( 85.05)	Acc@5 100.00 ( 98.63)
Epoch: [24][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.7827e-01 (4.7175e-01)	Acc@1  82.03 ( 85.10)	Acc@5  98.44 ( 98.64)
Epoch: [24][100/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.4355e-01 (4.7913e-01)	Acc@1  80.47 ( 84.95)	Acc@5  96.88 ( 98.57)
Epoch: [24][110/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.5078e-01 (4.8063e-01)	Acc@1  82.03 ( 84.90)	Acc@5  98.44 ( 98.59)
Epoch: [24][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.7236e-01 (4.8892e-01)	Acc@1  78.91 ( 84.63)	Acc@5  95.31 ( 98.50)
Epoch: [24][130/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.9634e-01 (4.9150e-01)	Acc@1  82.03 ( 84.52)	Acc@5  98.44 ( 98.50)
Epoch: [24][140/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.003)	Loss 5.4541e-01 (4.9420e-01)	Acc@1  83.59 ( 84.46)	Acc@5  98.44 ( 98.49)
Epoch: [24][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3213e-01 (4.9312e-01)	Acc@1  85.94 ( 84.42)	Acc@5  99.22 ( 98.52)
Epoch: [24][160/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.003)	Loss 4.6167e-01 (4.9582e-01)	Acc@1  85.94 ( 84.36)	Acc@5  99.22 ( 98.50)
Epoch: [24][170/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.003)	Loss 3.9722e-01 (4.9457e-01)	Acc@1  86.72 ( 84.39)	Acc@5  99.22 ( 98.52)
Epoch: [24][180/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.003)	Loss 6.8945e-01 (4.9885e-01)	Acc@1  75.00 ( 84.22)	Acc@5  96.09 ( 98.47)
Epoch: [24][190/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4502e-01 (4.9879e-01)	Acc@1  77.34 ( 84.20)	Acc@5  98.44 ( 98.47)
Epoch: [24][200/391]	Time  0.036 ( 0.035)	Data  0.003 ( 0.002)	Loss 6.5283e-01 (4.9966e-01)	Acc@1  77.34 ( 84.12)	Acc@5  97.66 ( 98.46)
Epoch: [24][210/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9766e-01 (5.0230e-01)	Acc@1  83.59 ( 84.09)	Acc@5  97.66 ( 98.43)
Epoch: [24][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4995e-01 (5.0526e-01)	Acc@1  85.94 ( 83.98)	Acc@5  98.44 ( 98.43)
Epoch: [24][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6016e-01 (5.0643e-01)	Acc@1  79.69 ( 83.97)	Acc@5  96.09 ( 98.42)
Epoch: [24][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6499e-01 (5.0757e-01)	Acc@1  84.38 ( 83.90)	Acc@5  99.22 ( 98.41)
Epoch: [24][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7773e-01 (5.1209e-01)	Acc@1  80.47 ( 83.79)	Acc@5  96.88 ( 98.37)
Epoch: [24][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7227e-01 (5.1415e-01)	Acc@1  82.03 ( 83.72)	Acc@5  98.44 ( 98.34)
Epoch: [24][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4834e-01 (5.1740e-01)	Acc@1  83.59 ( 83.66)	Acc@5  96.88 ( 98.31)
Epoch: [24][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0586e-01 (5.2063e-01)	Acc@1  81.25 ( 83.58)	Acc@5  97.66 ( 98.30)
Epoch: [24][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9805e-01 (5.2109e-01)	Acc@1  84.38 ( 83.55)	Acc@5  99.22 ( 98.29)
Epoch: [24][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3184e-01 (5.2198e-01)	Acc@1  79.69 ( 83.50)	Acc@5  98.44 ( 98.27)
Epoch: [24][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4688e-01 (5.2390e-01)	Acc@1  81.25 ( 83.44)	Acc@5  96.88 ( 98.25)
Epoch: [24][320/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2637e-01 (5.2405e-01)	Acc@1  83.59 ( 83.42)	Acc@5  99.22 ( 98.24)
Epoch: [24][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7559e-01 (5.2590e-01)	Acc@1  82.81 ( 83.36)	Acc@5  97.66 ( 98.22)
Epoch: [24][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7695e-01 (5.2969e-01)	Acc@1  73.44 ( 83.23)	Acc@5  94.53 ( 98.20)
Epoch: [24][350/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6934e-01 (5.3116e-01)	Acc@1  79.69 ( 83.18)	Acc@5  96.88 ( 98.19)
Epoch: [24][360/391]	Time  0.039 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1523e-01 (5.3396e-01)	Acc@1  81.25 ( 83.13)	Acc@5  96.88 ( 98.17)
Epoch: [24][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1475e-01 (5.3485e-01)	Acc@1  81.25 ( 83.10)	Acc@5  99.22 ( 98.17)
Epoch: [24][380/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5859e-01 (5.3612e-01)	Acc@1  83.59 ( 83.05)	Acc@5  99.22 ( 98.17)
Epoch: [24][390/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8203e-01 (5.3794e-01)	Acc@1  78.75 ( 82.95)	Acc@5  97.50 ( 98.16)
## e[24] optimizer.zero_grad (sum) time: 0.17858338356018066
## e[24]       loss.backward (sum) time: 3.8357884883880615
## e[24]      optimizer.step (sum) time: 1.3276920318603516
## epoch[24] training(only) time: 13.758871078491211
# Switched to evaluate mode...
Test: [  0/100]	Time  0.134 ( 0.134)	Loss 1.5039e+00 (1.5039e+00)	Acc@1  67.00 ( 67.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 1.5352e+00 (1.5455e+00)	Acc@1  57.00 ( 65.18)	Acc@5  90.00 ( 87.45)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.4502e+00 (1.5793e+00)	Acc@1  66.00 ( 64.71)	Acc@5  88.00 ( 87.43)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.9980e+00 (1.6236e+00)	Acc@1  58.00 ( 64.32)	Acc@5  84.00 ( 87.19)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.5146e+00 (1.5951e+00)	Acc@1  62.00 ( 64.20)	Acc@5  88.00 ( 87.63)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.6836e+00 (1.6034e+00)	Acc@1  67.00 ( 64.18)	Acc@5  85.00 ( 87.53)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.8008e+00 (1.5762e+00)	Acc@1  62.00 ( 64.44)	Acc@5  84.00 ( 87.85)
Test: [ 70/100]	Time  0.020 ( 0.020)	Loss 1.6309e+00 (1.5663e+00)	Acc@1  66.00 ( 64.39)	Acc@5  88.00 ( 88.00)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.6973e+00 (1.5585e+00)	Acc@1  69.00 ( 64.57)	Acc@5  85.00 ( 87.96)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.7949e+00 (1.5374e+00)	Acc@1  63.00 ( 64.67)	Acc@5  86.00 ( 88.20)
 * Acc@1 64.600 Acc@5 88.280
### epoch[24] execution time: 15.723623752593994
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.185 ( 0.185)	Data  0.140 ( 0.140)	Loss 3.9526e-01 (3.9526e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [25][ 10/391]	Time  0.034 ( 0.048)	Data  0.001 ( 0.014)	Loss 5.5566e-01 (5.0881e-01)	Acc@1  82.03 ( 84.30)	Acc@5  99.22 ( 99.08)
Epoch: [25][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.008)	Loss 4.4678e-01 (4.8578e-01)	Acc@1  85.94 ( 84.64)	Acc@5  98.44 ( 98.85)
Epoch: [25][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.2871e-01 (4.7561e-01)	Acc@1  87.50 ( 85.06)	Acc@5  99.22 ( 98.77)
Epoch: [25][ 40/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.7129e-01 (4.7775e-01)	Acc@1  81.25 ( 85.10)	Acc@5  98.44 ( 98.65)
Epoch: [25][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.3652e-01 (4.6535e-01)	Acc@1  84.38 ( 85.62)	Acc@5  99.22 ( 98.73)
Epoch: [25][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.2256e-01 (4.6281e-01)	Acc@1  80.47 ( 85.55)	Acc@5  97.66 ( 98.76)
Epoch: [25][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.8525e-01 (4.5871e-01)	Acc@1  86.72 ( 85.70)	Acc@5  99.22 ( 98.83)
Epoch: [25][ 80/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.0244e-01 (4.6070e-01)	Acc@1  85.94 ( 85.57)	Acc@5  96.88 ( 98.80)
Epoch: [25][ 90/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3018e-01 (4.6287e-01)	Acc@1  88.28 ( 85.52)	Acc@5  99.22 ( 98.82)
Epoch: [25][100/391]	Time  0.039 ( 0.036)	Data  0.002 ( 0.003)	Loss 6.0254e-01 (4.6459e-01)	Acc@1  82.03 ( 85.48)	Acc@5  98.44 ( 98.81)
Epoch: [25][110/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0635e-01 (4.6707e-01)	Acc@1  84.38 ( 85.42)	Acc@5  98.44 ( 98.75)
Epoch: [25][120/391]	Time  0.036 ( 0.036)	Data  0.000 ( 0.003)	Loss 4.3433e-01 (4.7185e-01)	Acc@1  82.81 ( 85.11)	Acc@5 100.00 ( 98.73)
Epoch: [25][130/391]	Time  0.046 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9380e-01 (4.7110e-01)	Acc@1  85.16 ( 85.08)	Acc@5 100.00 ( 98.75)
Epoch: [25][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.0166e-01 (4.7532e-01)	Acc@1  78.91 ( 84.92)	Acc@5  96.88 ( 98.73)
Epoch: [25][150/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1602e-01 (4.7778e-01)	Acc@1  85.94 ( 84.79)	Acc@5  98.44 ( 98.68)
Epoch: [25][160/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5288e-01 (4.7777e-01)	Acc@1  83.59 ( 84.79)	Acc@5  98.44 ( 98.67)
Epoch: [25][170/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5483e-01 (4.7685e-01)	Acc@1  84.38 ( 84.80)	Acc@5  99.22 ( 98.68)
Epoch: [25][180/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7910e-01 (4.7982e-01)	Acc@1  83.59 ( 84.71)	Acc@5  96.09 ( 98.63)
Epoch: [25][190/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1562e-01 (4.8299e-01)	Acc@1  83.59 ( 84.63)	Acc@5  99.22 ( 98.61)
Epoch: [25][200/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3467e-01 (4.8474e-01)	Acc@1  83.59 ( 84.56)	Acc@5  99.22 ( 98.60)
Epoch: [25][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2051e-01 (4.8918e-01)	Acc@1  86.72 ( 84.47)	Acc@5 100.00 ( 98.59)
Epoch: [25][220/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5273e-01 (4.9229e-01)	Acc@1  82.03 ( 84.38)	Acc@5  99.22 ( 98.56)
Epoch: [25][230/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2100e-01 (4.9604e-01)	Acc@1  80.47 ( 84.25)	Acc@5  99.22 ( 98.55)
Epoch: [25][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.4219e-01 (4.9892e-01)	Acc@1  74.22 ( 84.10)	Acc@5  95.31 ( 98.52)
Epoch: [25][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.3682e-01 (5.0364e-01)	Acc@1  77.34 ( 83.95)	Acc@5  93.75 ( 98.47)
Epoch: [25][260/391]	Time  0.031 ( 0.035)	Data  0.002 ( 0.002)	Loss 4.9536e-01 (5.0601e-01)	Acc@1  82.81 ( 83.83)	Acc@5  97.66 ( 98.46)
Epoch: [25][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8252e-01 (5.0790e-01)	Acc@1  79.69 ( 83.76)	Acc@5  96.88 ( 98.44)
Epoch: [25][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8936e-01 (5.0992e-01)	Acc@1  82.81 ( 83.71)	Acc@5  99.22 ( 98.42)
Epoch: [25][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8535e-01 (5.1175e-01)	Acc@1  85.16 ( 83.69)	Acc@5  97.66 ( 98.41)
Epoch: [25][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1279e-01 (5.1568e-01)	Acc@1  82.81 ( 83.54)	Acc@5  98.44 ( 98.40)
Epoch: [25][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9814e-01 (5.1797e-01)	Acc@1  79.69 ( 83.45)	Acc@5  96.88 ( 98.37)
Epoch: [25][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9238e-01 (5.1917e-01)	Acc@1  75.78 ( 83.39)	Acc@5  96.88 ( 98.37)
Epoch: [25][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2256e-01 (5.2148e-01)	Acc@1  80.47 ( 83.36)	Acc@5  97.66 ( 98.34)
Epoch: [25][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1816e-01 (5.2345e-01)	Acc@1  79.69 ( 83.29)	Acc@5  96.09 ( 98.31)
Epoch: [25][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8887e-01 (5.2574e-01)	Acc@1  80.47 ( 83.20)	Acc@5  99.22 ( 98.31)
Epoch: [25][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6704e-01 (5.2683e-01)	Acc@1  84.38 ( 83.16)	Acc@5  99.22 ( 98.30)
Epoch: [25][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7100e-01 (5.2982e-01)	Acc@1  75.78 ( 83.11)	Acc@5  96.88 ( 98.27)
Epoch: [25][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8359e-01 (5.3126e-01)	Acc@1  82.81 ( 83.08)	Acc@5  97.66 ( 98.27)
Epoch: [25][390/391]	Time  0.025 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5234e-01 (5.3397e-01)	Acc@1  77.50 ( 82.99)	Acc@5  96.25 ( 98.24)
## e[25] optimizer.zero_grad (sum) time: 0.1766965389251709
## e[25]       loss.backward (sum) time: 3.886977195739746
## e[25]      optimizer.step (sum) time: 1.2643499374389648
## epoch[25] training(only) time: 13.83577299118042
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.7412e+00 (1.7412e+00)	Acc@1  61.00 ( 61.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 1.7646e+00 (1.6104e+00)	Acc@1  56.00 ( 62.18)	Acc@5  89.00 ( 87.91)
Test: [ 20/100]	Time  0.023 ( 0.024)	Loss 1.5381e+00 (1.6077e+00)	Acc@1  64.00 ( 62.52)	Acc@5  93.00 ( 87.90)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 2.2363e+00 (1.6429e+00)	Acc@1  59.00 ( 62.16)	Acc@5  84.00 ( 87.42)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.7354e+00 (1.6358e+00)	Acc@1  57.00 ( 61.63)	Acc@5  89.00 ( 87.83)
Test: [ 50/100]	Time  0.022 ( 0.021)	Loss 1.7598e+00 (1.6511e+00)	Acc@1  61.00 ( 61.43)	Acc@5  85.00 ( 87.43)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.5674e+00 (1.6311e+00)	Acc@1  60.00 ( 61.57)	Acc@5  88.00 ( 87.69)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 2.0566e+00 (1.6339e+00)	Acc@1  56.00 ( 61.42)	Acc@5  87.00 ( 87.63)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.9170e+00 (1.6496e+00)	Acc@1  58.00 ( 61.27)	Acc@5  83.00 ( 87.42)
Test: [ 90/100]	Time  0.019 ( 0.019)	Loss 1.9180e+00 (1.6390e+00)	Acc@1  56.00 ( 61.40)	Acc@5  83.00 ( 87.44)
 * Acc@1 61.510 Acc@5 87.530
### epoch[25] execution time: 15.802974224090576
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.187 ( 0.187)	Data  0.146 ( 0.146)	Loss 4.6069e-01 (4.6069e-01)	Acc@1  87.50 ( 87.50)	Acc@5  99.22 ( 99.22)
Epoch: [26][ 10/391]	Time  0.035 ( 0.049)	Data  0.001 ( 0.015)	Loss 4.2993e-01 (4.8422e-01)	Acc@1  85.16 ( 84.23)	Acc@5  99.22 ( 98.93)
Epoch: [26][ 20/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.009)	Loss 5.5957e-01 (4.8103e-01)	Acc@1  85.16 ( 84.60)	Acc@5  97.66 ( 98.59)
Epoch: [26][ 30/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.006)	Loss 3.9502e-01 (4.5970e-01)	Acc@1  89.84 ( 85.36)	Acc@5  99.22 ( 98.74)
Epoch: [26][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.8315e-01 (4.6258e-01)	Acc@1  85.16 ( 85.31)	Acc@5  98.44 ( 98.69)
Epoch: [26][ 50/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.005)	Loss 4.1699e-01 (4.5542e-01)	Acc@1  89.84 ( 85.51)	Acc@5  98.44 ( 98.81)
Epoch: [26][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.0986e-01 (4.5232e-01)	Acc@1  79.69 ( 85.66)	Acc@5  96.88 ( 98.81)
Epoch: [26][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.5923e-01 (4.5124e-01)	Acc@1  85.94 ( 85.89)	Acc@5  99.22 ( 98.81)
Epoch: [26][ 80/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7817e-01 (4.4772e-01)	Acc@1  89.84 ( 85.96)	Acc@5  99.22 ( 98.83)
Epoch: [26][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4399e-01 (4.4643e-01)	Acc@1  88.28 ( 85.97)	Acc@5 100.00 ( 98.83)
Epoch: [26][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0586e-01 (4.4899e-01)	Acc@1  82.03 ( 85.88)	Acc@5  99.22 ( 98.85)
Epoch: [26][110/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.8940e-01 (4.4998e-01)	Acc@1  87.50 ( 85.89)	Acc@5  98.44 ( 98.82)
Epoch: [26][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5483e-01 (4.5325e-01)	Acc@1  85.94 ( 85.73)	Acc@5  99.22 ( 98.79)
Epoch: [26][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.4316e-01 (4.5543e-01)	Acc@1  78.12 ( 85.62)	Acc@5  97.66 ( 98.77)
Epoch: [26][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0195e-01 (4.5729e-01)	Acc@1  82.81 ( 85.50)	Acc@5 100.00 ( 98.77)
Epoch: [26][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.7324e-01 (4.5846e-01)	Acc@1  82.81 ( 85.45)	Acc@5  98.44 ( 98.79)
Epoch: [26][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6924e-01 (4.6123e-01)	Acc@1  87.50 ( 85.38)	Acc@5  98.44 ( 98.77)
Epoch: [26][170/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.002)	Loss 5.3271e-01 (4.6265e-01)	Acc@1  82.81 ( 85.31)	Acc@5  98.44 ( 98.77)
Epoch: [26][180/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8242e-01 (4.6554e-01)	Acc@1  82.03 ( 85.18)	Acc@5 100.00 ( 98.78)
Epoch: [26][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4629e-01 (4.6921e-01)	Acc@1  86.72 ( 85.07)	Acc@5  96.88 ( 98.73)
Epoch: [26][200/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7188e-01 (4.7025e-01)	Acc@1  79.69 ( 84.99)	Acc@5  96.88 ( 98.72)
Epoch: [26][210/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8438e-01 (4.7219e-01)	Acc@1  88.28 ( 85.00)	Acc@5  97.66 ( 98.68)
Epoch: [26][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4604e-01 (4.7258e-01)	Acc@1  89.06 ( 84.98)	Acc@5  98.44 ( 98.66)
Epoch: [26][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7314e-01 (4.7232e-01)	Acc@1  85.16 ( 84.98)	Acc@5  97.66 ( 98.66)
Epoch: [26][240/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5664e-01 (4.7300e-01)	Acc@1  82.81 ( 85.00)	Acc@5  96.88 ( 98.65)
Epoch: [26][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1475e-01 (4.7294e-01)	Acc@1  83.59 ( 85.02)	Acc@5  96.88 ( 98.66)
Epoch: [26][260/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5615e-01 (4.7447e-01)	Acc@1  80.47 ( 84.98)	Acc@5  98.44 ( 98.66)
Epoch: [26][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1562e-01 (4.7496e-01)	Acc@1  84.38 ( 84.97)	Acc@5  98.44 ( 98.65)
Epoch: [26][280/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9570e-01 (4.7702e-01)	Acc@1  80.47 ( 84.92)	Acc@5  96.88 ( 98.63)
Epoch: [26][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2334e-01 (4.7817e-01)	Acc@1  84.38 ( 84.85)	Acc@5  99.22 ( 98.63)
Epoch: [26][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8379e-01 (4.8031e-01)	Acc@1  86.72 ( 84.79)	Acc@5  99.22 ( 98.63)
Epoch: [26][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9282e-01 (4.8163e-01)	Acc@1  85.16 ( 84.76)	Acc@5  98.44 ( 98.62)
Epoch: [26][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3516e-01 (4.8196e-01)	Acc@1  82.03 ( 84.72)	Acc@5  98.44 ( 98.61)
Epoch: [26][330/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1577e-01 (4.8299e-01)	Acc@1  85.16 ( 84.69)	Acc@5  98.44 ( 98.60)
Epoch: [26][340/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7461e-01 (4.8373e-01)	Acc@1  85.16 ( 84.69)	Acc@5  96.88 ( 98.57)
Epoch: [26][350/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6094e-01 (4.8459e-01)	Acc@1  86.72 ( 84.71)	Acc@5  99.22 ( 98.55)
Epoch: [26][360/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3662e-01 (4.8602e-01)	Acc@1  82.03 ( 84.65)	Acc@5  96.88 ( 98.54)
Epoch: [26][370/391]	Time  0.042 ( 0.035)	Data  0.002 ( 0.002)	Loss 5.6543e-01 (4.8987e-01)	Acc@1  83.59 ( 84.55)	Acc@5  96.88 ( 98.50)
Epoch: [26][380/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9570e-01 (4.9184e-01)	Acc@1  81.25 ( 84.49)	Acc@5  98.44 ( 98.49)
Epoch: [26][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2822e-01 (4.9291e-01)	Acc@1  87.50 ( 84.47)	Acc@5  98.75 ( 98.49)
## e[26] optimizer.zero_grad (sum) time: 0.17710471153259277
## e[26]       loss.backward (sum) time: 3.898803234100342
## e[26]      optimizer.step (sum) time: 1.272533893585205
## epoch[26] training(only) time: 13.891557931900024
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.4668e+00 (1.4668e+00)	Acc@1  70.00 ( 70.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.023 ( 0.030)	Loss 1.8271e+00 (1.7061e+00)	Acc@1  57.00 ( 63.09)	Acc@5  88.00 ( 87.00)
Test: [ 20/100]	Time  0.018 ( 0.025)	Loss 1.4629e+00 (1.6648e+00)	Acc@1  65.00 ( 63.38)	Acc@5  90.00 ( 87.62)
Test: [ 30/100]	Time  0.023 ( 0.023)	Loss 2.0215e+00 (1.7091e+00)	Acc@1  54.00 ( 62.06)	Acc@5  86.00 ( 87.19)
Test: [ 40/100]	Time  0.023 ( 0.021)	Loss 1.6709e+00 (1.6888e+00)	Acc@1  61.00 ( 62.51)	Acc@5  86.00 ( 87.63)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.6348e+00 (1.7134e+00)	Acc@1  67.00 ( 62.31)	Acc@5  89.00 ( 87.24)
Test: [ 60/100]	Time  0.021 ( 0.020)	Loss 1.9160e+00 (1.7007e+00)	Acc@1  61.00 ( 62.30)	Acc@5  89.00 ( 87.48)
Test: [ 70/100]	Time  0.013 ( 0.020)	Loss 1.8096e+00 (1.6976e+00)	Acc@1  60.00 ( 62.25)	Acc@5  87.00 ( 87.44)
Test: [ 80/100]	Time  0.020 ( 0.020)	Loss 1.7178e+00 (1.6958e+00)	Acc@1  64.00 ( 62.37)	Acc@5  85.00 ( 87.36)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.9697e+00 (1.6723e+00)	Acc@1  63.00 ( 62.71)	Acc@5  87.00 ( 87.54)
 * Acc@1 62.830 Acc@5 87.520
### epoch[26] execution time: 15.885934114456177
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.189 ( 0.189)	Data  0.149 ( 0.149)	Loss 5.9668e-01 (5.9668e-01)	Acc@1  78.91 ( 78.91)	Acc@5  97.66 ( 97.66)
Epoch: [27][ 10/391]	Time  0.036 ( 0.049)	Data  0.001 ( 0.015)	Loss 5.4004e-01 (5.3025e-01)	Acc@1  79.69 ( 82.74)	Acc@5  99.22 ( 98.22)
Epoch: [27][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.009)	Loss 3.6816e-01 (4.6261e-01)	Acc@1  90.62 ( 85.53)	Acc@5 100.00 ( 98.70)
Epoch: [27][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.7314e-01 (4.5526e-01)	Acc@1  82.03 ( 85.58)	Acc@5  99.22 ( 98.71)
Epoch: [27][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.3726e-01 (4.4164e-01)	Acc@1  85.16 ( 85.99)	Acc@5 100.00 ( 98.84)
Epoch: [27][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.5557e-01 (4.3958e-01)	Acc@1  84.38 ( 86.03)	Acc@5  97.66 ( 98.74)
Epoch: [27][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.6118e-01 (4.3169e-01)	Acc@1  83.59 ( 86.19)	Acc@5 100.00 ( 98.76)
Epoch: [27][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.8691e-01 (4.3377e-01)	Acc@1  86.72 ( 86.33)	Acc@5  98.44 ( 98.72)
Epoch: [27][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.9634e-01 (4.3148e-01)	Acc@1  88.28 ( 86.41)	Acc@5  99.22 ( 98.81)
Epoch: [27][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.8706e-01 (4.3240e-01)	Acc@1  86.72 ( 86.45)	Acc@5  99.22 ( 98.79)
Epoch: [27][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.8584e-01 (4.3185e-01)	Acc@1  84.38 ( 86.47)	Acc@5  98.44 ( 98.79)
Epoch: [27][110/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6411e-01 (4.3212e-01)	Acc@1  85.16 ( 86.43)	Acc@5  97.66 ( 98.75)
Epoch: [27][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.9097e-01 (4.3410e-01)	Acc@1  83.59 ( 86.32)	Acc@5  98.44 ( 98.75)
Epoch: [27][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1953e-01 (4.3387e-01)	Acc@1  84.38 ( 86.28)	Acc@5  98.44 ( 98.78)
Epoch: [27][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7988e-01 (4.3525e-01)	Acc@1  87.50 ( 86.23)	Acc@5 100.00 ( 98.76)
Epoch: [27][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9355e-01 (4.3820e-01)	Acc@1  89.06 ( 86.19)	Acc@5 100.00 ( 98.74)
Epoch: [27][160/391]	Time  0.038 ( 0.036)	Data  0.002 ( 0.002)	Loss 6.4062e-01 (4.4238e-01)	Acc@1  82.03 ( 86.04)	Acc@5  98.44 ( 98.73)
Epoch: [27][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0088e-01 (4.4762e-01)	Acc@1  85.94 ( 85.91)	Acc@5  99.22 ( 98.70)
Epoch: [27][180/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9814e-01 (4.4989e-01)	Acc@1  83.59 ( 85.84)	Acc@5  97.66 ( 98.70)
Epoch: [27][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1138e-01 (4.5156e-01)	Acc@1  87.50 ( 85.79)	Acc@5  97.66 ( 98.67)
Epoch: [27][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6533e-01 (4.5163e-01)	Acc@1  87.50 ( 85.82)	Acc@5  97.66 ( 98.66)
Epoch: [27][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9502e-01 (4.5148e-01)	Acc@1  87.50 ( 85.76)	Acc@5  98.44 ( 98.66)
Epoch: [27][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2017e-01 (4.5326e-01)	Acc@1  83.59 ( 85.72)	Acc@5  99.22 ( 98.65)
Epoch: [27][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.9990e-01 (4.5559e-01)	Acc@1  75.78 ( 85.64)	Acc@5  95.31 ( 98.62)
Epoch: [27][240/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.002)	Loss 4.0234e-01 (4.5579e-01)	Acc@1  86.72 ( 85.65)	Acc@5 100.00 ( 98.62)
Epoch: [27][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0186e-01 (4.5731e-01)	Acc@1  89.84 ( 85.60)	Acc@5  96.88 ( 98.61)
Epoch: [27][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9629e-01 (4.5953e-01)	Acc@1  75.78 ( 85.51)	Acc@5  96.88 ( 98.60)
Epoch: [27][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6338e-01 (4.6033e-01)	Acc@1  85.16 ( 85.49)	Acc@5  97.66 ( 98.59)
Epoch: [27][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5435e-01 (4.6085e-01)	Acc@1  87.50 ( 85.47)	Acc@5  98.44 ( 98.58)
Epoch: [27][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1040e-01 (4.6221e-01)	Acc@1  88.28 ( 85.46)	Acc@5  99.22 ( 98.58)
Epoch: [27][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0693e-01 (4.6328e-01)	Acc@1  84.38 ( 85.45)	Acc@5  96.88 ( 98.57)
Epoch: [27][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1172e-01 (4.6466e-01)	Acc@1  84.38 ( 85.41)	Acc@5  99.22 ( 98.56)
Epoch: [27][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3335e-01 (4.6670e-01)	Acc@1  89.06 ( 85.34)	Acc@5  99.22 ( 98.55)
Epoch: [27][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3823e-01 (4.6913e-01)	Acc@1  85.16 ( 85.21)	Acc@5 100.00 ( 98.54)
Epoch: [27][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (4.7086e-01)	Acc@1  84.38 ( 85.15)	Acc@5  99.22 ( 98.55)
Epoch: [27][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4556e-01 (4.7218e-01)	Acc@1  85.16 ( 85.11)	Acc@5  97.66 ( 98.55)
Epoch: [27][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0293e-01 (4.7437e-01)	Acc@1  82.03 ( 85.02)	Acc@5 100.00 ( 98.55)
Epoch: [27][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7583e-01 (4.7666e-01)	Acc@1  82.03 ( 84.93)	Acc@5 100.00 ( 98.53)
Epoch: [27][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7324e-01 (4.8001e-01)	Acc@1  77.34 ( 84.82)	Acc@5  99.22 ( 98.50)
Epoch: [27][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1182e-01 (4.8166e-01)	Acc@1  80.00 ( 84.77)	Acc@5  97.50 ( 98.49)
## e[27] optimizer.zero_grad (sum) time: 0.17873668670654297
## e[27]       loss.backward (sum) time: 4.090251922607422
## e[27]      optimizer.step (sum) time: 1.2482352256774902
## epoch[27] training(only) time: 13.87316632270813
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.5918e+00 (1.5918e+00)	Acc@1  65.00 ( 65.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.021 ( 0.031)	Loss 1.7510e+00 (1.6219e+00)	Acc@1  61.00 ( 64.36)	Acc@5  89.00 ( 87.27)
Test: [ 20/100]	Time  0.015 ( 0.025)	Loss 1.3281e+00 (1.5666e+00)	Acc@1  69.00 ( 65.05)	Acc@5  90.00 ( 88.05)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.8662e+00 (1.5942e+00)	Acc@1  58.00 ( 64.23)	Acc@5  88.00 ( 88.19)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.5117e+00 (1.5681e+00)	Acc@1  62.00 ( 64.56)	Acc@5  87.00 ( 88.93)
Test: [ 50/100]	Time  0.023 ( 0.021)	Loss 1.8242e+00 (1.5821e+00)	Acc@1  63.00 ( 64.31)	Acc@5  87.00 ( 88.73)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.3018e+00 (1.5675e+00)	Acc@1  71.00 ( 64.54)	Acc@5  91.00 ( 88.80)
Test: [ 70/100]	Time  0.020 ( 0.020)	Loss 1.7588e+00 (1.5557e+00)	Acc@1  62.00 ( 64.59)	Acc@5  85.00 ( 88.93)
Test: [ 80/100]	Time  0.025 ( 0.019)	Loss 1.6875e+00 (1.5624e+00)	Acc@1  63.00 ( 64.32)	Acc@5  87.00 ( 88.78)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.9365e+00 (1.5525e+00)	Acc@1  62.00 ( 64.30)	Acc@5  85.00 ( 88.80)
 * Acc@1 64.580 Acc@5 88.920
### epoch[27] execution time: 15.86692762374878
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.182 ( 0.182)	Data  0.139 ( 0.139)	Loss 4.5044e-01 (4.5044e-01)	Acc@1  85.94 ( 85.94)	Acc@5  98.44 ( 98.44)
Epoch: [28][ 10/391]	Time  0.034 ( 0.048)	Data  0.001 ( 0.014)	Loss 4.1577e-01 (4.3186e-01)	Acc@1  86.72 ( 85.30)	Acc@5 100.00 ( 99.29)
Epoch: [28][ 20/391]	Time  0.032 ( 0.041)	Data  0.001 ( 0.008)	Loss 4.1284e-01 (4.1434e-01)	Acc@1  87.50 ( 86.50)	Acc@5  99.22 ( 99.00)
Epoch: [28][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.006)	Loss 4.4556e-01 (4.1209e-01)	Acc@1  86.72 ( 86.84)	Acc@5  99.22 ( 98.92)
Epoch: [28][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.5923e-01 (4.0515e-01)	Acc@1  86.72 ( 87.18)	Acc@5  98.44 ( 98.95)
Epoch: [28][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.7915e-01 (4.0278e-01)	Acc@1  85.94 ( 87.15)	Acc@5  99.22 ( 99.02)
Epoch: [28][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.9502e-01 (4.0032e-01)	Acc@1  90.62 ( 87.22)	Acc@5  99.22 ( 98.98)
Epoch: [28][ 70/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.5669e-01 (4.0800e-01)	Acc@1  86.72 ( 87.05)	Acc@5 100.00 ( 98.98)
Epoch: [28][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.3618e-01 (4.1076e-01)	Acc@1  89.06 ( 86.96)	Acc@5 100.00 ( 98.95)
Epoch: [28][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3228e-01 (4.0763e-01)	Acc@1  89.06 ( 87.02)	Acc@5  99.22 ( 98.98)
Epoch: [28][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2393e-01 (4.1164e-01)	Acc@1  82.03 ( 86.90)	Acc@5  99.22 ( 98.96)
Epoch: [28][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1919e-01 (4.1075e-01)	Acc@1  86.72 ( 86.96)	Acc@5  99.22 ( 98.99)
Epoch: [28][120/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0332e-01 (4.1234e-01)	Acc@1  85.94 ( 86.84)	Acc@5  98.44 ( 98.99)
Epoch: [28][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.4922e-01 (4.1344e-01)	Acc@1  85.16 ( 86.86)	Acc@5 100.00 ( 98.97)
Epoch: [28][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9648e-01 (4.1576e-01)	Acc@1  83.59 ( 86.72)	Acc@5 100.00 ( 98.95)
Epoch: [28][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6338e-01 (4.1666e-01)	Acc@1  85.16 ( 86.65)	Acc@5  98.44 ( 98.94)
Epoch: [28][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9756e-01 (4.1862e-01)	Acc@1  85.16 ( 86.63)	Acc@5 100.00 ( 98.91)
Epoch: [28][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8193e-01 (4.2166e-01)	Acc@1  85.16 ( 86.55)	Acc@5 100.00 ( 98.94)
Epoch: [28][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9082e-01 (4.2227e-01)	Acc@1  82.81 ( 86.55)	Acc@5  96.09 ( 98.91)
Epoch: [28][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.8506e-01 (4.2626e-01)	Acc@1  78.12 ( 86.41)	Acc@5  95.31 ( 98.88)
Epoch: [28][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9941e-01 (4.2860e-01)	Acc@1  88.28 ( 86.33)	Acc@5  97.66 ( 98.87)
Epoch: [28][210/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2100e-01 (4.2927e-01)	Acc@1  82.81 ( 86.35)	Acc@5 100.00 ( 98.87)
Epoch: [28][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1221e-01 (4.3189e-01)	Acc@1  81.25 ( 86.28)	Acc@5  99.22 ( 98.87)
Epoch: [28][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3662e-01 (4.3241e-01)	Acc@1  82.81 ( 86.30)	Acc@5  97.66 ( 98.84)
Epoch: [28][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6689e-01 (4.3551e-01)	Acc@1  80.47 ( 86.14)	Acc@5  99.22 ( 98.85)
Epoch: [28][250/391]	Time  0.049 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1479e-01 (4.3585e-01)	Acc@1  84.38 ( 86.10)	Acc@5  98.44 ( 98.85)
Epoch: [28][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1807e-01 (4.3905e-01)	Acc@1  79.69 ( 85.96)	Acc@5  97.66 ( 98.84)
Epoch: [28][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9243e-01 (4.4164e-01)	Acc@1  88.28 ( 85.82)	Acc@5  98.44 ( 98.84)
Epoch: [28][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2305e-01 (4.4513e-01)	Acc@1  79.69 ( 85.71)	Acc@5  96.09 ( 98.82)
Epoch: [28][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8447e-01 (4.4801e-01)	Acc@1  81.25 ( 85.62)	Acc@5  99.22 ( 98.81)
Epoch: [28][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4580e-01 (4.4892e-01)	Acc@1  84.38 ( 85.57)	Acc@5  98.44 ( 98.81)
Epoch: [28][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7510e-01 (4.4996e-01)	Acc@1  85.16 ( 85.55)	Acc@5 100.00 ( 98.81)
Epoch: [28][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0771e-01 (4.5219e-01)	Acc@1  85.16 ( 85.48)	Acc@5 100.00 ( 98.80)
Epoch: [28][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1504e-01 (4.5313e-01)	Acc@1  85.94 ( 85.47)	Acc@5  99.22 ( 98.79)
Epoch: [28][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4785e-01 (4.5409e-01)	Acc@1  82.81 ( 85.43)	Acc@5  96.88 ( 98.78)
Epoch: [28][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7046e-01 (4.5542e-01)	Acc@1  85.16 ( 85.40)	Acc@5  96.88 ( 98.78)
Epoch: [28][360/391]	Time  0.035 ( 0.035)	Data  0.002 ( 0.002)	Loss 5.2051e-01 (4.5754e-01)	Acc@1  80.47 ( 85.32)	Acc@5  97.66 ( 98.78)
Epoch: [28][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3467e-01 (4.5951e-01)	Acc@1  85.94 ( 85.26)	Acc@5  99.22 ( 98.75)
Epoch: [28][380/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6152e-01 (4.6167e-01)	Acc@1  87.50 ( 85.20)	Acc@5  98.44 ( 98.75)
Epoch: [28][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.0996e-01 (4.6358e-01)	Acc@1  77.50 ( 85.15)	Acc@5  96.25 ( 98.73)
## e[28] optimizer.zero_grad (sum) time: 0.18018245697021484
## e[28]       loss.backward (sum) time: 4.111804246902466
## e[28]      optimizer.step (sum) time: 1.2535033226013184
## epoch[28] training(only) time: 13.83341121673584
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.7100e+00 (1.7100e+00)	Acc@1  67.00 ( 67.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.016 ( 0.031)	Loss 1.4990e+00 (1.5955e+00)	Acc@1  64.00 ( 65.27)	Acc@5  89.00 ( 87.09)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 1.7266e+00 (1.5572e+00)	Acc@1  63.00 ( 65.62)	Acc@5  89.00 ( 88.52)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 2.4023e+00 (1.6119e+00)	Acc@1  51.00 ( 63.90)	Acc@5  83.00 ( 88.10)
Test: [ 40/100]	Time  0.029 ( 0.021)	Loss 1.7246e+00 (1.5722e+00)	Acc@1  57.00 ( 64.02)	Acc@5  87.00 ( 88.54)
Test: [ 50/100]	Time  0.022 ( 0.021)	Loss 1.6006e+00 (1.5940e+00)	Acc@1  67.00 ( 63.86)	Acc@5  85.00 ( 88.06)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.5303e+00 (1.5805e+00)	Acc@1  64.00 ( 63.92)	Acc@5  90.00 ( 88.18)
Test: [ 70/100]	Time  0.022 ( 0.020)	Loss 1.8779e+00 (1.5744e+00)	Acc@1  58.00 ( 63.72)	Acc@5  88.00 ( 88.37)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.6416e+00 (1.5747e+00)	Acc@1  66.00 ( 63.72)	Acc@5  85.00 ( 88.26)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.7969e+00 (1.5705e+00)	Acc@1  61.00 ( 63.84)	Acc@5  83.00 ( 88.23)
 * Acc@1 64.090 Acc@5 88.300
### epoch[28] execution time: 15.799814939498901
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.192 ( 0.192)	Data  0.146 ( 0.146)	Loss 4.4629e-01 (4.4629e-01)	Acc@1  82.81 ( 82.81)	Acc@5 100.00 (100.00)
Epoch: [29][ 10/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.015)	Loss 4.9097e-01 (4.2993e-01)	Acc@1  85.16 ( 86.22)	Acc@5  98.44 ( 99.08)
Epoch: [29][ 20/391]	Time  0.039 ( 0.043)	Data  0.001 ( 0.008)	Loss 4.0674e-01 (4.0754e-01)	Acc@1  85.16 ( 86.42)	Acc@5  99.22 ( 99.14)
Epoch: [29][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.007)	Loss 3.7842e-01 (4.0144e-01)	Acc@1  87.50 ( 86.84)	Acc@5 100.00 ( 99.32)
Epoch: [29][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 3.7085e-01 (3.9514e-01)	Acc@1  88.28 ( 87.20)	Acc@5  99.22 ( 99.31)
Epoch: [29][ 50/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.005)	Loss 5.9814e-01 (3.9612e-01)	Acc@1  79.69 ( 87.16)	Acc@5  97.66 ( 99.22)
Epoch: [29][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.2236e-01 (4.0625e-01)	Acc@1  82.81 ( 86.76)	Acc@5 100.00 ( 99.23)
Epoch: [29][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.8379e-01 (4.0538e-01)	Acc@1  87.50 ( 86.86)	Acc@5 100.00 ( 99.17)
Epoch: [29][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.8501e-01 (4.0638e-01)	Acc@1  87.50 ( 86.85)	Acc@5  99.22 ( 99.20)
Epoch: [29][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.1675e-01 (4.0520e-01)	Acc@1  86.72 ( 86.83)	Acc@5  99.22 ( 99.18)
Epoch: [29][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0771e-01 (4.0744e-01)	Acc@1  85.94 ( 86.78)	Acc@5  99.22 ( 99.12)
Epoch: [29][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4497e-01 (4.0976e-01)	Acc@1  89.84 ( 86.78)	Acc@5  99.22 ( 99.11)
Epoch: [29][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.3857e-01 (4.1213e-01)	Acc@1  85.16 ( 86.71)	Acc@5  98.44 ( 99.08)
Epoch: [29][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7646e-01 (4.1428e-01)	Acc@1  86.72 ( 86.65)	Acc@5  99.22 ( 99.05)
Epoch: [29][140/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0000e-01 (4.1730e-01)	Acc@1  85.94 ( 86.54)	Acc@5  99.22 ( 99.04)
Epoch: [29][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7900e-01 (4.1876e-01)	Acc@1  82.81 ( 86.50)	Acc@5  98.44 ( 99.07)
Epoch: [29][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1514e-01 (4.2271e-01)	Acc@1  85.16 ( 86.38)	Acc@5  98.44 ( 99.06)
Epoch: [29][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2139e-01 (4.2479e-01)	Acc@1  90.62 ( 86.37)	Acc@5  97.66 ( 99.04)
Epoch: [29][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.8154e-01 (4.2815e-01)	Acc@1  80.47 ( 86.27)	Acc@5  99.22 ( 99.02)
Epoch: [29][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.8623e-01 (4.2982e-01)	Acc@1  86.72 ( 86.20)	Acc@5  99.22 ( 99.01)
Epoch: [29][200/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4980e-01 (4.2953e-01)	Acc@1  82.81 ( 86.23)	Acc@5  97.66 ( 98.98)
Epoch: [29][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9014e-01 (4.2995e-01)	Acc@1  88.28 ( 86.23)	Acc@5  98.44 ( 98.97)
Epoch: [29][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0283e-01 (4.3219e-01)	Acc@1  86.72 ( 86.17)	Acc@5  99.22 ( 98.95)
Epoch: [29][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7266e-01 (4.3360e-01)	Acc@1  85.16 ( 86.14)	Acc@5  98.44 ( 98.93)
Epoch: [29][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9575e-01 (4.3376e-01)	Acc@1  88.28 ( 86.13)	Acc@5  98.44 ( 98.91)
Epoch: [29][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3496e-01 (4.3416e-01)	Acc@1  87.50 ( 86.11)	Acc@5 100.00 ( 98.91)
Epoch: [29][260/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7646e-01 (4.3572e-01)	Acc@1  85.94 ( 86.05)	Acc@5  99.22 ( 98.91)
Epoch: [29][270/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9619e-01 (4.3850e-01)	Acc@1  77.34 ( 85.96)	Acc@5  97.66 ( 98.89)
Epoch: [29][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3433e-01 (4.3870e-01)	Acc@1  84.38 ( 85.94)	Acc@5  97.66 ( 98.89)
Epoch: [29][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.0935e-01 (4.4010e-01)	Acc@1  94.53 ( 85.94)	Acc@5  99.22 ( 98.88)
Epoch: [29][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2793e-01 (4.4198e-01)	Acc@1  78.12 ( 85.88)	Acc@5  97.66 ( 98.86)
Epoch: [29][310/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3213e-01 (4.4335e-01)	Acc@1  85.94 ( 85.83)	Acc@5  99.22 ( 98.85)
Epoch: [29][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8804e-01 (4.4426e-01)	Acc@1  84.38 ( 85.78)	Acc@5  97.66 ( 98.84)
Epoch: [29][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9912e-01 (4.4582e-01)	Acc@1  82.03 ( 85.75)	Acc@5  96.88 ( 98.82)
Epoch: [29][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2246e-01 (4.4670e-01)	Acc@1  81.25 ( 85.72)	Acc@5  98.44 ( 98.82)
Epoch: [29][350/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6143e-01 (4.4802e-01)	Acc@1  85.16 ( 85.68)	Acc@5  98.44 ( 98.80)
Epoch: [29][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6445e-01 (4.4951e-01)	Acc@1  85.94 ( 85.68)	Acc@5  96.88 ( 98.79)
Epoch: [29][370/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3516e-01 (4.5010e-01)	Acc@1  82.03 ( 85.67)	Acc@5  97.66 ( 98.77)
Epoch: [29][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9526e-01 (4.5100e-01)	Acc@1  89.06 ( 85.65)	Acc@5  99.22 ( 98.77)
Epoch: [29][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5337e-01 (4.5304e-01)	Acc@1  86.25 ( 85.58)	Acc@5 100.00 ( 98.77)
## e[29] optimizer.zero_grad (sum) time: 0.1799008846282959
## e[29]       loss.backward (sum) time: 4.014860391616821
## e[29]      optimizer.step (sum) time: 1.2655322551727295
## epoch[29] training(only) time: 13.895894527435303
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.4922e+00 (1.4922e+00)	Acc@1  65.00 ( 65.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.7354e+00 (1.5908e+00)	Acc@1  59.00 ( 63.55)	Acc@5  90.00 ( 88.45)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 1.4375e+00 (1.5359e+00)	Acc@1  69.00 ( 64.48)	Acc@5  89.00 ( 89.00)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.9023e+00 (1.5665e+00)	Acc@1  62.00 ( 63.94)	Acc@5  89.00 ( 88.42)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.5020e+00 (1.5472e+00)	Acc@1  65.00 ( 63.78)	Acc@5  92.00 ( 88.71)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.7959e+00 (1.5525e+00)	Acc@1  62.00 ( 63.67)	Acc@5  86.00 ( 88.37)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.4404e+00 (1.5407e+00)	Acc@1  62.00 ( 63.75)	Acc@5  90.00 ( 88.61)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.6953e+00 (1.5305e+00)	Acc@1  59.00 ( 63.87)	Acc@5  91.00 ( 88.61)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.6328e+00 (1.5379e+00)	Acc@1  62.00 ( 63.83)	Acc@5  85.00 ( 88.40)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.8633e+00 (1.5253e+00)	Acc@1  56.00 ( 64.08)	Acc@5  84.00 ( 88.56)
 * Acc@1 64.370 Acc@5 88.710
### epoch[29] execution time: 15.848238706588745
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.188 ( 0.188)	Data  0.146 ( 0.146)	Loss 4.0381e-01 (4.0381e-01)	Acc@1  85.16 ( 85.16)	Acc@5  99.22 ( 99.22)
Epoch: [30][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 5.2246e-01 (4.1837e-01)	Acc@1  85.16 ( 87.36)	Acc@5  98.44 ( 98.86)
Epoch: [30][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 2.4231e-01 (3.9006e-01)	Acc@1  92.97 ( 87.91)	Acc@5 100.00 ( 99.11)
Epoch: [30][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.0957e-01 (3.5334e-01)	Acc@1  89.84 ( 88.89)	Acc@5 100.00 ( 99.37)
Epoch: [30][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 2.9272e-01 (3.2642e-01)	Acc@1  92.19 ( 89.73)	Acc@5  99.22 ( 99.49)
Epoch: [30][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.0566e-01 (3.1925e-01)	Acc@1  89.06 ( 90.04)	Acc@5 100.00 ( 99.49)
Epoch: [30][ 60/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.8452e-01 (3.0977e-01)	Acc@1  88.28 ( 90.47)	Acc@5  99.22 ( 99.50)
Epoch: [30][ 70/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.4131e-01 (3.0199e-01)	Acc@1  89.84 ( 90.72)	Acc@5 100.00 ( 99.54)
Epoch: [30][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.1030e-01 (2.9578e-01)	Acc@1  88.28 ( 90.95)	Acc@5  99.22 ( 99.55)
Epoch: [30][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.8286e-01 (2.8722e-01)	Acc@1  94.53 ( 91.23)	Acc@5 100.00 ( 99.59)
Epoch: [30][100/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.2910e-01 (2.8399e-01)	Acc@1  90.62 ( 91.30)	Acc@5  99.22 ( 99.60)
Epoch: [30][110/391]	Time  0.046 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.2925e-01 (2.8256e-01)	Acc@1  92.19 ( 91.25)	Acc@5 100.00 ( 99.61)
Epoch: [30][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.2485e-01 (2.7650e-01)	Acc@1  93.75 ( 91.43)	Acc@5 100.00 ( 99.63)
Epoch: [30][130/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.1106e-01 (2.7193e-01)	Acc@1  92.97 ( 91.55)	Acc@5  99.22 ( 99.62)
Epoch: [30][140/391]	Time  0.042 ( 0.036)	Data  0.002 ( 0.003)	Loss 3.2300e-01 (2.7014e-01)	Acc@1  89.06 ( 91.67)	Acc@5  99.22 ( 99.60)
Epoch: [30][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.8188e-01 (2.6591e-01)	Acc@1  95.31 ( 91.82)	Acc@5 100.00 ( 99.61)
Epoch: [30][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.9297e-01 (2.6372e-01)	Acc@1  89.06 ( 91.87)	Acc@5 100.00 ( 99.60)
Epoch: [30][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.5806e-01 (2.6095e-01)	Acc@1  93.75 ( 92.00)	Acc@5 100.00 ( 99.61)
Epoch: [30][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.8809e-01 (2.5917e-01)	Acc@1  92.19 ( 92.11)	Acc@5  99.22 ( 99.62)
Epoch: [30][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.4438e-01 (2.5751e-01)	Acc@1  93.75 ( 92.20)	Acc@5 100.00 ( 99.61)
Epoch: [30][200/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.8677e-01 (2.5429e-01)	Acc@1  93.75 ( 92.29)	Acc@5 100.00 ( 99.63)
Epoch: [30][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.4146e-01 (2.5380e-01)	Acc@1  93.75 ( 92.34)	Acc@5  99.22 ( 99.61)
Epoch: [30][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.6978e-01 (2.5205e-01)	Acc@1  92.19 ( 92.41)	Acc@5  99.22 ( 99.61)
Epoch: [30][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1216e-01 (2.5016e-01)	Acc@1  92.97 ( 92.49)	Acc@5 100.00 ( 99.61)
Epoch: [30][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.5830e-01 (2.4770e-01)	Acc@1  89.84 ( 92.55)	Acc@5 100.00 ( 99.62)
Epoch: [30][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7354e-01 (2.4625e-01)	Acc@1  85.94 ( 92.61)	Acc@5  99.22 ( 99.63)
Epoch: [30][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7371e-01 (2.4425e-01)	Acc@1  94.53 ( 92.70)	Acc@5 100.00 ( 99.63)
Epoch: [30][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4014e-01 (2.4180e-01)	Acc@1  98.44 ( 92.79)	Acc@5 100.00 ( 99.64)
Epoch: [30][280/391]	Time  0.035 ( 0.035)	Data  0.000 ( 0.002)	Loss 1.8298e-01 (2.4013e-01)	Acc@1  96.09 ( 92.84)	Acc@5 100.00 ( 99.65)
Epoch: [30][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3303e-01 (2.3809e-01)	Acc@1  90.62 ( 92.90)	Acc@5 100.00 ( 99.66)
Epoch: [30][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2720e-01 (2.3639e-01)	Acc@1  98.44 ( 92.97)	Acc@5 100.00 ( 99.67)
Epoch: [30][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2266e-01 (2.3550e-01)	Acc@1  94.53 ( 93.02)	Acc@5 100.00 ( 99.67)
Epoch: [30][320/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6248e-01 (2.3378e-01)	Acc@1  94.53 ( 93.07)	Acc@5 100.00 ( 99.68)
Epoch: [30][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6296e-01 (2.3245e-01)	Acc@1  93.75 ( 93.11)	Acc@5 100.00 ( 99.68)
Epoch: [30][340/391]	Time  0.034 ( 0.035)	Data  0.000 ( 0.002)	Loss 1.6003e-01 (2.3162e-01)	Acc@1  95.31 ( 93.13)	Acc@5 100.00 ( 99.68)
Epoch: [30][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5393e-01 (2.3088e-01)	Acc@1  95.31 ( 93.16)	Acc@5 100.00 ( 99.69)
Epoch: [30][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7114e-01 (2.2967e-01)	Acc@1  96.09 ( 93.21)	Acc@5 100.00 ( 99.69)
Epoch: [30][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.8262e-01 (2.2808e-01)	Acc@1  96.09 ( 93.27)	Acc@5 100.00 ( 99.70)
Epoch: [30][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7139e-01 (2.2681e-01)	Acc@1  93.75 ( 93.30)	Acc@5 100.00 ( 99.71)
Epoch: [30][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.9702e-01 (2.2621e-01)	Acc@1  96.25 ( 93.31)	Acc@5 100.00 ( 99.71)
## e[30] optimizer.zero_grad (sum) time: 0.179762601852417
## e[30]       loss.backward (sum) time: 3.9817004203796387
## e[30]      optimizer.step (sum) time: 1.269913911819458
## epoch[30] training(only) time: 13.881162166595459
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.1719e+00 (1.1719e+00)	Acc@1  71.00 ( 71.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.019 ( 0.030)	Loss 1.2939e+00 (1.2120e+00)	Acc@1  66.00 ( 70.18)	Acc@5  92.00 ( 91.18)
Test: [ 20/100]	Time  0.025 ( 0.024)	Loss 1.1045e+00 (1.1451e+00)	Acc@1  78.00 ( 72.52)	Acc@5  92.00 ( 91.71)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.6143e+00 (1.1912e+00)	Acc@1  64.00 ( 71.26)	Acc@5  90.00 ( 91.42)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 1.1475e+00 (1.1705e+00)	Acc@1  71.00 ( 71.37)	Acc@5  94.00 ( 91.85)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.3926e+00 (1.1794e+00)	Acc@1  68.00 ( 71.24)	Acc@5  92.00 ( 91.59)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.1855e+00 (1.1603e+00)	Acc@1  71.00 ( 71.64)	Acc@5  91.00 ( 91.84)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3770e+00 (1.1534e+00)	Acc@1  67.00 ( 71.68)	Acc@5  91.00 ( 92.01)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.2656e+00 (1.1586e+00)	Acc@1  71.00 ( 71.69)	Acc@5  90.00 ( 91.88)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.5664e+00 (1.1481e+00)	Acc@1  65.00 ( 71.90)	Acc@5  92.00 ( 92.08)
 * Acc@1 72.050 Acc@5 92.150
### epoch[30] execution time: 15.855025291442871
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.186 ( 0.186)	Data  0.144 ( 0.144)	Loss 1.4038e-01 (1.4038e-01)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [31][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 1.4392e-01 (1.3595e-01)	Acc@1  96.09 ( 97.16)	Acc@5 100.00 (100.00)
Epoch: [31][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 1.0516e-01 (1.5055e-01)	Acc@1  98.44 ( 96.43)	Acc@5 100.00 ( 99.96)
Epoch: [31][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 1.6260e-01 (1.5576e-01)	Acc@1  95.31 ( 96.14)	Acc@5 100.00 ( 99.95)
Epoch: [31][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 2.0752e-01 (1.5573e-01)	Acc@1  92.97 ( 96.07)	Acc@5 100.00 ( 99.94)
Epoch: [31][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 2.0105e-01 (1.5635e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 ( 99.92)
Epoch: [31][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.0571e-01 (1.5729e-01)	Acc@1  98.44 ( 96.04)	Acc@5 100.00 ( 99.87)
Epoch: [31][ 70/391]	Time  0.041 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.4905e-01 (1.5675e-01)	Acc@1  94.53 ( 96.05)	Acc@5 100.00 ( 99.85)
Epoch: [31][ 80/391]	Time  0.039 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.0217e-01 (1.5696e-01)	Acc@1  97.66 ( 95.98)	Acc@5 100.00 ( 99.85)
Epoch: [31][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.5825e-02 (1.5857e-01)	Acc@1  97.66 ( 95.89)	Acc@5 100.00 ( 99.85)
Epoch: [31][100/391]	Time  0.044 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.3293e-01 (1.5772e-01)	Acc@1  96.09 ( 95.83)	Acc@5 100.00 ( 99.86)
Epoch: [31][110/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.9529e-02 (1.5798e-01)	Acc@1  99.22 ( 95.76)	Acc@5 100.00 ( 99.85)
Epoch: [31][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.7542e-01 (1.5774e-01)	Acc@1  95.31 ( 95.82)	Acc@5 100.00 ( 99.85)
Epoch: [31][130/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.6748e-01 (1.6021e-01)	Acc@1  97.66 ( 95.72)	Acc@5 100.00 ( 99.84)
Epoch: [31][140/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.9409e-01 (1.6111e-01)	Acc@1  94.53 ( 95.63)	Acc@5 100.00 ( 99.86)
Epoch: [31][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.9861e-01 (1.6165e-01)	Acc@1  92.97 ( 95.59)	Acc@5 100.00 ( 99.86)
Epoch: [31][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.5032e-02 (1.6117e-01)	Acc@1  97.66 ( 95.60)	Acc@5 100.00 ( 99.86)
Epoch: [31][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2854e-01 (1.6029e-01)	Acc@1  96.09 ( 95.64)	Acc@5 100.00 ( 99.86)
Epoch: [31][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.4673e-01 (1.6000e-01)	Acc@1  95.31 ( 95.63)	Acc@5 100.00 ( 99.87)
Epoch: [31][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.4429e-01 (1.6014e-01)	Acc@1  96.88 ( 95.59)	Acc@5 100.00 ( 99.87)
Epoch: [31][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2122e-01 (1.5957e-01)	Acc@1  96.09 ( 95.60)	Acc@5 100.00 ( 99.87)
Epoch: [31][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.3257e-01 (1.5916e-01)	Acc@1  96.88 ( 95.63)	Acc@5 100.00 ( 99.87)
Epoch: [31][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.7100e-01 (1.5979e-01)	Acc@1  91.41 ( 95.63)	Acc@5 100.00 ( 99.87)
Epoch: [31][230/391]	Time  0.039 ( 0.036)	Data  0.002 ( 0.002)	Loss 1.6211e-01 (1.5984e-01)	Acc@1  96.09 ( 95.63)	Acc@5 100.00 ( 99.87)
Epoch: [31][240/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.5210e-01 (1.5894e-01)	Acc@1  96.09 ( 95.69)	Acc@5 100.00 ( 99.87)
Epoch: [31][250/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2494e-01 (1.5855e-01)	Acc@1  97.66 ( 95.72)	Acc@5 100.00 ( 99.86)
Epoch: [31][260/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3733e-01 (1.5774e-01)	Acc@1  95.31 ( 95.75)	Acc@5 100.00 ( 99.87)
Epoch: [31][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7310e-01 (1.5747e-01)	Acc@1  94.53 ( 95.77)	Acc@5 100.00 ( 99.86)
Epoch: [31][280/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0498e-01 (1.5705e-01)	Acc@1  97.66 ( 95.78)	Acc@5 100.00 ( 99.87)
Epoch: [31][290/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4343e-01 (1.5712e-01)	Acc@1  94.53 ( 95.76)	Acc@5 100.00 ( 99.87)
Epoch: [31][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5564e-01 (1.5715e-01)	Acc@1  96.09 ( 95.74)	Acc@5 100.00 ( 99.87)
Epoch: [31][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.7847e-01 (1.5729e-01)	Acc@1  95.31 ( 95.75)	Acc@5 100.00 ( 99.87)
Epoch: [31][320/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2363e-01 (1.5732e-01)	Acc@1  94.53 ( 95.76)	Acc@5 100.00 ( 99.87)
Epoch: [31][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4514e-01 (1.5679e-01)	Acc@1  96.09 ( 95.78)	Acc@5 100.00 ( 99.87)
Epoch: [31][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6089e-01 (1.5733e-01)	Acc@1  95.31 ( 95.74)	Acc@5 100.00 ( 99.87)
Epoch: [31][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.0032e-01 (1.5799e-01)	Acc@1  94.53 ( 95.73)	Acc@5  99.22 ( 99.87)
Epoch: [31][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2791e-01 (1.5779e-01)	Acc@1  91.41 ( 95.72)	Acc@5 100.00 ( 99.87)
Epoch: [31][370/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3586e-01 (1.5786e-01)	Acc@1  97.66 ( 95.72)	Acc@5 100.00 ( 99.87)
Epoch: [31][380/391]	Time  0.039 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6589e-01 (1.5759e-01)	Acc@1  91.41 ( 95.73)	Acc@5 100.00 ( 99.87)
Epoch: [31][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4685e-01 (1.5744e-01)	Acc@1  96.25 ( 95.75)	Acc@5 100.00 ( 99.87)
## e[31] optimizer.zero_grad (sum) time: 0.17724061012268066
## e[31]       loss.backward (sum) time: 3.8613321781158447
## e[31]      optimizer.step (sum) time: 1.2961900234222412
## epoch[31] training(only) time: 13.871078729629517
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.1621e+00 (1.1621e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.020 ( 0.031)	Loss 1.3379e+00 (1.2009e+00)	Acc@1  68.00 ( 71.55)	Acc@5  90.00 ( 91.82)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.1035e+00 (1.1411e+00)	Acc@1  77.00 ( 72.95)	Acc@5  91.00 ( 92.24)
Test: [ 30/100]	Time  0.017 ( 0.023)	Loss 1.6055e+00 (1.1911e+00)	Acc@1  65.00 ( 71.58)	Acc@5  92.00 ( 91.77)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1611e+00 (1.1685e+00)	Acc@1  74.00 ( 71.76)	Acc@5  94.00 ( 92.17)
Test: [ 50/100]	Time  0.024 ( 0.021)	Loss 1.4150e+00 (1.1756e+00)	Acc@1  70.00 ( 71.57)	Acc@5  92.00 ( 92.04)
Test: [ 60/100]	Time  0.021 ( 0.020)	Loss 1.1748e+00 (1.1541e+00)	Acc@1  69.00 ( 71.74)	Acc@5  92.00 ( 92.31)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 1.3555e+00 (1.1461e+00)	Acc@1  69.00 ( 71.94)	Acc@5  91.00 ( 92.49)
Test: [ 80/100]	Time  0.015 ( 0.020)	Loss 1.2607e+00 (1.1498e+00)	Acc@1  72.00 ( 71.84)	Acc@5  90.00 ( 92.30)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.5537e+00 (1.1386e+00)	Acc@1  66.00 ( 72.04)	Acc@5  92.00 ( 92.48)
 * Acc@1 72.230 Acc@5 92.550
### epoch[31] execution time: 15.85250186920166
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.184 ( 0.184)	Data  0.141 ( 0.141)	Loss 1.4526e-01 (1.4526e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [32][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.014)	Loss 1.1218e-01 (1.3983e-01)	Acc@1  96.88 ( 96.02)	Acc@5 100.00 (100.00)
Epoch: [32][ 20/391]	Time  0.038 ( 0.042)	Data  0.001 ( 0.008)	Loss 1.3696e-01 (1.4254e-01)	Acc@1  96.88 ( 95.91)	Acc@5 100.00 ( 99.93)
Epoch: [32][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 9.3201e-02 (1.3927e-01)	Acc@1  99.22 ( 96.19)	Acc@5 100.00 ( 99.95)
Epoch: [32][ 40/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 1.2109e-01 (1.4128e-01)	Acc@1  98.44 ( 96.21)	Acc@5 100.00 ( 99.94)
Epoch: [32][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.0828e-01 (1.3777e-01)	Acc@1  95.31 ( 96.37)	Acc@5 100.00 ( 99.95)
Epoch: [32][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.2781e-01 (1.3540e-01)	Acc@1  97.66 ( 96.45)	Acc@5 100.00 ( 99.96)
Epoch: [32][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.5332e-01 (1.3739e-01)	Acc@1  96.09 ( 96.37)	Acc@5 100.00 ( 99.96)
Epoch: [32][ 80/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.003)	Loss 8.8928e-02 (1.3716e-01)	Acc@1 100.00 ( 96.45)	Acc@5 100.00 ( 99.96)
Epoch: [32][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.1301e-01 (1.3923e-01)	Acc@1  91.41 ( 96.34)	Acc@5 100.00 ( 99.94)
Epoch: [32][100/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.6162e-01 (1.3788e-01)	Acc@1  95.31 ( 96.40)	Acc@5  99.22 ( 99.94)
Epoch: [32][110/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.3684e-01 (1.3687e-01)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.93)
Epoch: [32][120/391]	Time  0.034 ( 0.036)	Data  0.000 ( 0.003)	Loss 1.9080e-01 (1.3672e-01)	Acc@1  93.75 ( 96.52)	Acc@5 100.00 ( 99.93)
Epoch: [32][130/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.5601e-01 (1.3837e-01)	Acc@1  93.75 ( 96.45)	Acc@5 100.00 ( 99.92)
Epoch: [32][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.3501e-01 (1.3778e-01)	Acc@1  97.66 ( 96.49)	Acc@5 100.00 ( 99.92)
Epoch: [32][150/391]	Time  0.036 ( 0.036)	Data  0.002 ( 0.003)	Loss 1.3171e-01 (1.3695e-01)	Acc@1  97.66 ( 96.56)	Acc@5  99.22 ( 99.92)
Epoch: [32][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.5430e-01 (1.3681e-01)	Acc@1  97.66 ( 96.60)	Acc@5 100.00 ( 99.92)
Epoch: [32][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1682e-01 (1.3711e-01)	Acc@1  96.88 ( 96.59)	Acc@5 100.00 ( 99.92)
Epoch: [32][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2451e-01 (1.3847e-01)	Acc@1  96.88 ( 96.55)	Acc@5 100.00 ( 99.92)
Epoch: [32][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1700e-01 (1.3919e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.92)
Epoch: [32][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5571e-02 (1.3801e-01)	Acc@1  98.44 ( 96.55)	Acc@5 100.00 ( 99.92)
Epoch: [32][210/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5271e-01 (1.3819e-01)	Acc@1  96.09 ( 96.52)	Acc@5 100.00 ( 99.91)
Epoch: [32][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0872e-02 (1.3785e-01)	Acc@1  98.44 ( 96.53)	Acc@5 100.00 ( 99.91)
Epoch: [32][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3647e-01 (1.3783e-01)	Acc@1  96.88 ( 96.53)	Acc@5 100.00 ( 99.91)
Epoch: [32][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2671e-01 (1.3745e-01)	Acc@1  95.31 ( 96.53)	Acc@5 100.00 ( 99.92)
Epoch: [32][250/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6272e-01 (1.3790e-01)	Acc@1  95.31 ( 96.52)	Acc@5 100.00 ( 99.92)
Epoch: [32][260/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1316e-01 (1.3860e-01)	Acc@1  99.22 ( 96.50)	Acc@5 100.00 ( 99.92)
Epoch: [32][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6870e-01 (1.3836e-01)	Acc@1  96.09 ( 96.50)	Acc@5 100.00 ( 99.92)
Epoch: [32][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4294e-01 (1.3917e-01)	Acc@1  96.09 ( 96.48)	Acc@5 100.00 ( 99.92)
Epoch: [32][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0205e-01 (1.3878e-01)	Acc@1  98.44 ( 96.49)	Acc@5 100.00 ( 99.92)
Epoch: [32][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6724e-01 (1.3879e-01)	Acc@1  95.31 ( 96.48)	Acc@5 100.00 ( 99.92)
Epoch: [32][310/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3599e-01 (1.3863e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.92)
Epoch: [32][320/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3867e-01 (1.3815e-01)	Acc@1  95.31 ( 96.49)	Acc@5 100.00 ( 99.92)
Epoch: [32][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4014e-01 (1.3841e-01)	Acc@1  96.88 ( 96.49)	Acc@5  99.22 ( 99.92)
Epoch: [32][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.3933e-02 (1.3777e-01)	Acc@1  97.66 ( 96.50)	Acc@5 100.00 ( 99.92)
Epoch: [32][350/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0907e-01 (1.3786e-01)	Acc@1  97.66 ( 96.50)	Acc@5  99.22 ( 99.92)
Epoch: [32][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3025e-01 (1.3779e-01)	Acc@1  96.88 ( 96.50)	Acc@5 100.00 ( 99.92)
Epoch: [32][370/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6321e-01 (1.3727e-01)	Acc@1  96.88 ( 96.52)	Acc@5  99.22 ( 99.92)
Epoch: [32][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3806e-01 (1.3727e-01)	Acc@1  96.09 ( 96.53)	Acc@5 100.00 ( 99.92)
Epoch: [32][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4722e-01 (1.3685e-01)	Acc@1  96.25 ( 96.55)	Acc@5 100.00 ( 99.92)
## e[32] optimizer.zero_grad (sum) time: 0.17862892150878906
## e[32]       loss.backward (sum) time: 3.958325147628784
## e[32]      optimizer.step (sum) time: 1.270873785018921
## epoch[32] training(only) time: 13.876297235488892
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.2236e+00 (1.2236e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.019 ( 0.029)	Loss 1.3154e+00 (1.1909e+00)	Acc@1  69.00 ( 72.27)	Acc@5  91.00 ( 91.82)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 1.1025e+00 (1.1309e+00)	Acc@1  77.00 ( 73.67)	Acc@5  93.00 ( 92.38)
Test: [ 30/100]	Time  0.022 ( 0.022)	Loss 1.5977e+00 (1.1772e+00)	Acc@1  65.00 ( 72.55)	Acc@5  89.00 ( 91.77)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.1455e+00 (1.1541e+00)	Acc@1  72.00 ( 72.63)	Acc@5  94.00 ( 92.29)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.3467e+00 (1.1587e+00)	Acc@1  71.00 ( 72.49)	Acc@5  92.00 ( 92.10)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.1631e+00 (1.1368e+00)	Acc@1  72.00 ( 72.67)	Acc@5  93.00 ( 92.38)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3662e+00 (1.1305e+00)	Acc@1  69.00 ( 72.69)	Acc@5  91.00 ( 92.51)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.2109e+00 (1.1357e+00)	Acc@1  72.00 ( 72.54)	Acc@5  92.00 ( 92.38)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.5146e+00 (1.1238e+00)	Acc@1  65.00 ( 72.80)	Acc@5  94.00 ( 92.58)
 * Acc@1 72.960 Acc@5 92.630
### epoch[32] execution time: 15.811980485916138
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.185 ( 0.185)	Data  0.141 ( 0.141)	Loss 1.2683e-01 (1.2683e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [33][ 10/391]	Time  0.033 ( 0.048)	Data  0.001 ( 0.014)	Loss 1.2793e-01 (1.2371e-01)	Acc@1  97.66 ( 97.37)	Acc@5 100.00 (100.00)
Epoch: [33][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 1.2286e-01 (1.2370e-01)	Acc@1  97.66 ( 97.02)	Acc@5 100.00 (100.00)
Epoch: [33][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 1.7920e-01 (1.2442e-01)	Acc@1  92.97 ( 96.98)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 40/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.1163e-01 (1.2392e-01)	Acc@1  97.66 ( 96.89)	Acc@5 100.00 ( 99.96)
Epoch: [33][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.2915e-01 (1.2193e-01)	Acc@1  95.31 ( 97.04)	Acc@5 100.00 ( 99.97)
Epoch: [33][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.4685e-01 (1.2363e-01)	Acc@1  96.09 ( 97.00)	Acc@5 100.00 ( 99.95)
Epoch: [33][ 70/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.1086e-02 (1.2319e-01)	Acc@1 100.00 ( 97.04)	Acc@5 100.00 ( 99.94)
Epoch: [33][ 80/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.2396e-01 (1.2154e-01)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.95)
Epoch: [33][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.4099e-01 (1.2140e-01)	Acc@1  96.09 ( 97.14)	Acc@5 100.00 ( 99.95)
Epoch: [33][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.6528e-01 (1.2048e-01)	Acc@1  94.53 ( 97.18)	Acc@5 100.00 ( 99.95)
Epoch: [33][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.5540e-01 (1.2087e-01)	Acc@1  95.31 ( 97.14)	Acc@5 100.00 ( 99.94)
Epoch: [33][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1017e-01 (1.2173e-01)	Acc@1  99.22 ( 97.17)	Acc@5 100.00 ( 99.94)
Epoch: [33][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.3208e-01 (1.2219e-01)	Acc@1  96.09 ( 97.11)	Acc@5 100.00 ( 99.94)
Epoch: [33][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0883e-01 (1.2126e-01)	Acc@1  99.22 ( 97.15)	Acc@5 100.00 ( 99.94)
Epoch: [33][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0870e-01 (1.2103e-01)	Acc@1  98.44 ( 97.14)	Acc@5 100.00 ( 99.95)
Epoch: [33][160/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.9487e-02 (1.2101e-01)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.95)
Epoch: [33][170/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3708e-01 (1.2006e-01)	Acc@1  96.09 ( 97.14)	Acc@5  99.22 ( 99.95)
Epoch: [33][180/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1011e-01 (1.2061e-01)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.95)
Epoch: [33][190/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.3669e-02 (1.2091e-01)	Acc@1  98.44 ( 97.10)	Acc@5 100.00 ( 99.95)
Epoch: [33][200/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1835e-01 (1.2093e-01)	Acc@1  96.09 ( 97.09)	Acc@5 100.00 ( 99.95)
Epoch: [33][210/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4221e-01 (1.2153e-01)	Acc@1  94.53 ( 97.07)	Acc@5 100.00 ( 99.94)
Epoch: [33][220/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4160e-01 (1.2086e-01)	Acc@1  96.88 ( 97.12)	Acc@5 100.00 ( 99.94)
Epoch: [33][230/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0797e-01 (1.2067e-01)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.94)
Epoch: [33][240/391]	Time  0.032 ( 0.035)	Data  0.002 ( 0.002)	Loss 7.7087e-02 (1.2036e-01)	Acc@1  99.22 ( 97.15)	Acc@5 100.00 ( 99.94)
Epoch: [33][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0260e-01 (1.2080e-01)	Acc@1  98.44 ( 97.14)	Acc@5  99.22 ( 99.94)
Epoch: [33][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2488e-01 (1.2077e-01)	Acc@1  96.88 ( 97.14)	Acc@5 100.00 ( 99.94)
Epoch: [33][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2256e-01 (1.2088e-01)	Acc@1  96.88 ( 97.13)	Acc@5 100.00 ( 99.94)
Epoch: [33][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3184e-01 (1.2051e-01)	Acc@1  96.09 ( 97.15)	Acc@5 100.00 ( 99.94)
Epoch: [33][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3831e-01 (1.2033e-01)	Acc@1  96.88 ( 97.15)	Acc@5 100.00 ( 99.94)
Epoch: [33][300/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 1.3757e-01 (1.2097e-01)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.94)
Epoch: [33][310/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0175e-01 (1.2128e-01)	Acc@1  98.44 ( 97.13)	Acc@5 100.00 ( 99.93)
Epoch: [33][320/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1841e-01 (1.2112e-01)	Acc@1  97.66 ( 97.14)	Acc@5  99.22 ( 99.93)
Epoch: [33][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.9426e-02 (1.2151e-01)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.93)
Epoch: [33][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1267e-01 (1.2122e-01)	Acc@1  97.66 ( 97.12)	Acc@5 100.00 ( 99.93)
Epoch: [33][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5491e-01 (1.2139e-01)	Acc@1  95.31 ( 97.11)	Acc@5 100.00 ( 99.94)
Epoch: [33][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.8806e-02 (1.2121e-01)	Acc@1  98.44 ( 97.12)	Acc@5 100.00 ( 99.94)
Epoch: [33][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1322e-01 (1.2084e-01)	Acc@1  95.31 ( 97.14)	Acc@5 100.00 ( 99.93)
Epoch: [33][380/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.8440e-02 (1.2115e-01)	Acc@1  97.66 ( 97.13)	Acc@5 100.00 ( 99.94)
Epoch: [33][390/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.8022e-02 (1.2114e-01)	Acc@1  97.50 ( 97.12)	Acc@5 100.00 ( 99.94)
## e[33] optimizer.zero_grad (sum) time: 0.18042206764221191
## e[33]       loss.backward (sum) time: 3.991412878036499
## e[33]      optimizer.step (sum) time: 1.2968437671661377
## epoch[33] training(only) time: 13.807757139205933
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.2168e+00 (1.2168e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.014 ( 0.030)	Loss 1.3252e+00 (1.1861e+00)	Acc@1  68.00 ( 72.55)	Acc@5  90.00 ( 91.91)
Test: [ 20/100]	Time  0.021 ( 0.025)	Loss 1.0918e+00 (1.1246e+00)	Acc@1  79.00 ( 74.00)	Acc@5  93.00 ( 92.43)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5723e+00 (1.1712e+00)	Acc@1  65.00 ( 72.77)	Acc@5  92.00 ( 91.97)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.1348e+00 (1.1516e+00)	Acc@1  72.00 ( 72.76)	Acc@5  94.00 ( 92.51)
Test: [ 50/100]	Time  0.018 ( 0.021)	Loss 1.3818e+00 (1.1579e+00)	Acc@1  70.00 ( 72.69)	Acc@5  92.00 ( 92.18)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 1.1973e+00 (1.1370e+00)	Acc@1  73.00 ( 72.85)	Acc@5  94.00 ( 92.49)
Test: [ 70/100]	Time  0.021 ( 0.020)	Loss 1.3682e+00 (1.1295e+00)	Acc@1  70.00 ( 73.04)	Acc@5  91.00 ( 92.63)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.2539e+00 (1.1323e+00)	Acc@1  72.00 ( 72.93)	Acc@5  89.00 ( 92.54)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5586e+00 (1.1219e+00)	Acc@1  62.00 ( 73.02)	Acc@5  91.00 ( 92.73)
 * Acc@1 73.180 Acc@5 92.780
### epoch[33] execution time: 15.801532506942749
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.195 ( 0.195)	Data  0.154 ( 0.154)	Loss 1.3086e-01 (1.3086e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [34][ 10/391]	Time  0.037 ( 0.050)	Data  0.001 ( 0.016)	Loss 1.6406e-01 (1.1972e-01)	Acc@1  94.53 ( 96.52)	Acc@5  99.22 ( 99.93)
Epoch: [34][ 20/391]	Time  0.030 ( 0.043)	Data  0.001 ( 0.009)	Loss 1.5576e-01 (1.2034e-01)	Acc@1  96.88 ( 96.91)	Acc@5  99.22 ( 99.89)
Epoch: [34][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.007)	Loss 9.7412e-02 (1.1419e-01)	Acc@1  99.22 ( 97.18)	Acc@5 100.00 ( 99.90)
Epoch: [34][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.006)	Loss 1.2164e-01 (1.1242e-01)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 50/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.0248e-01 (1.1213e-01)	Acc@1  98.44 ( 97.38)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.1298e-01 (1.1388e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.92)
Epoch: [34][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.1676e-01 (1.1443e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 8.8013e-02 (1.1428e-01)	Acc@1  98.44 ( 97.26)	Acc@5 100.00 ( 99.93)
Epoch: [34][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.7778e-02 (1.1446e-01)	Acc@1  98.44 ( 97.24)	Acc@5 100.00 ( 99.94)
Epoch: [34][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1841e-01 (1.1311e-01)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.94)
Epoch: [34][110/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.5002e-02 (1.1273e-01)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.94)
Epoch: [34][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0358e-01 (1.1240e-01)	Acc@1  96.88 ( 97.32)	Acc@5 100.00 ( 99.95)
Epoch: [34][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.8684e-02 (1.1266e-01)	Acc@1  99.22 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [34][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1560e-01 (1.1241e-01)	Acc@1  98.44 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [34][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0101e-01 (1.1247e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.96)
Epoch: [34][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0815e-01 (1.1305e-01)	Acc@1  97.66 ( 97.32)	Acc@5 100.00 ( 99.96)
Epoch: [34][170/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2201e-01 (1.1301e-01)	Acc@1  96.88 ( 97.30)	Acc@5 100.00 ( 99.96)
Epoch: [34][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1957e-01 (1.1319e-01)	Acc@1  96.09 ( 97.29)	Acc@5  99.22 ( 99.95)
Epoch: [34][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0156e-01 (1.1290e-01)	Acc@1  98.44 ( 97.29)	Acc@5 100.00 ( 99.96)
Epoch: [34][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.4575e-01 (1.1311e-01)	Acc@1  96.09 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [34][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5266e-02 (1.1312e-01)	Acc@1  96.88 ( 97.28)	Acc@5 100.00 ( 99.95)
Epoch: [34][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.3037e-01 (1.1382e-01)	Acc@1  96.88 ( 97.27)	Acc@5 100.00 ( 99.95)
Epoch: [34][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.6182e-02 (1.1376e-01)	Acc@1  97.66 ( 97.25)	Acc@5 100.00 ( 99.95)
Epoch: [34][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0352e-01 (1.1413e-01)	Acc@1  98.44 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [34][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2683e-01 (1.1466e-01)	Acc@1  96.88 ( 97.21)	Acc@5 100.00 ( 99.95)
Epoch: [34][260/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.4563e-01 (1.1454e-01)	Acc@1  96.09 ( 97.23)	Acc@5 100.00 ( 99.95)
Epoch: [34][270/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.0027e-02 (1.1418e-01)	Acc@1  99.22 ( 97.25)	Acc@5 100.00 ( 99.95)
Epoch: [34][280/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.3506e-02 (1.1355e-01)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.95)
Epoch: [34][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1194e-01 (1.1317e-01)	Acc@1  97.66 ( 97.26)	Acc@5 100.00 ( 99.95)
Epoch: [34][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3345e-02 (1.1238e-01)	Acc@1 100.00 ( 97.29)	Acc@5 100.00 ( 99.95)
Epoch: [34][310/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 1.2177e-01 (1.1228e-01)	Acc@1  96.88 ( 97.31)	Acc@5 100.00 ( 99.95)
Epoch: [34][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2927e-01 (1.1230e-01)	Acc@1  96.09 ( 97.31)	Acc@5 100.00 ( 99.95)
Epoch: [34][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3672e-01 (1.1206e-01)	Acc@1  96.09 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [34][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.9365e-02 (1.1225e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [34][350/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.6548e-02 (1.1195e-01)	Acc@1  96.88 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [34][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.6863e-02 (1.1197e-01)	Acc@1  99.22 ( 97.34)	Acc@5 100.00 ( 99.95)
Epoch: [34][370/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5820e-01 (1.1227e-01)	Acc@1  96.09 ( 97.34)	Acc@5 100.00 ( 99.95)
Epoch: [34][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3281e-01 (1.1257e-01)	Acc@1  97.66 ( 97.33)	Acc@5 100.00 ( 99.95)
Epoch: [34][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.0210e-02 (1.1245e-01)	Acc@1  98.75 ( 97.34)	Acc@5 100.00 ( 99.95)
## e[34] optimizer.zero_grad (sum) time: 0.17783880233764648
## e[34]       loss.backward (sum) time: 3.968080759048462
## e[34]      optimizer.step (sum) time: 1.2628533840179443
## epoch[34] training(only) time: 13.87862253189087
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2061e+00 (1.2061e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 1.3057e+00 (1.1994e+00)	Acc@1  71.00 ( 72.27)	Acc@5  92.00 ( 92.00)
Test: [ 20/100]	Time  0.015 ( 0.023)	Loss 1.0996e+00 (1.1387e+00)	Acc@1  75.00 ( 73.62)	Acc@5  93.00 ( 92.67)
Test: [ 30/100]	Time  0.024 ( 0.022)	Loss 1.5703e+00 (1.1817e+00)	Acc@1  67.00 ( 72.71)	Acc@5  92.00 ( 92.10)
Test: [ 40/100]	Time  0.023 ( 0.020)	Loss 1.1367e+00 (1.1586e+00)	Acc@1  71.00 ( 72.78)	Acc@5  94.00 ( 92.63)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.4033e+00 (1.1653e+00)	Acc@1  71.00 ( 72.73)	Acc@5  92.00 ( 92.35)
Test: [ 60/100]	Time  0.015 ( 0.019)	Loss 1.2197e+00 (1.1428e+00)	Acc@1  73.00 ( 72.89)	Acc@5  92.00 ( 92.59)
Test: [ 70/100]	Time  0.019 ( 0.019)	Loss 1.3604e+00 (1.1348e+00)	Acc@1  67.00 ( 72.97)	Acc@5  91.00 ( 92.66)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.2852e+00 (1.1395e+00)	Acc@1  71.00 ( 72.78)	Acc@5  90.00 ( 92.51)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.5498e+00 (1.1294e+00)	Acc@1  66.00 ( 72.96)	Acc@5  90.00 ( 92.67)
 * Acc@1 73.110 Acc@5 92.760
### epoch[34] execution time: 15.82240080833435
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.182 ( 0.182)	Data  0.143 ( 0.143)	Loss 7.8186e-02 (7.8186e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [35][ 10/391]	Time  0.034 ( 0.048)	Data  0.001 ( 0.014)	Loss 8.6548e-02 (8.9228e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [35][ 20/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.008)	Loss 8.1604e-02 (9.5288e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [35][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 1.0040e-01 (9.9007e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 (100.00)
Epoch: [35][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 8.7402e-02 (9.8691e-02)	Acc@1  97.66 ( 97.88)	Acc@5 100.00 (100.00)
Epoch: [35][ 50/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.0693e-01 (9.9429e-02)	Acc@1  97.66 ( 97.89)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 60/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.4026e-01 (9.9938e-02)	Acc@1  96.88 ( 97.82)	Acc@5 100.00 ( 99.99)
Epoch: [35][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 1.0962e-01 (1.0089e-01)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.6375e-02 (9.9711e-02)	Acc@1 100.00 ( 97.84)	Acc@5 100.00 ( 99.98)
Epoch: [35][ 90/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.9182e-02 (1.0043e-01)	Acc@1  95.31 ( 97.83)	Acc@5 100.00 ( 99.97)
Epoch: [35][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.3059e-02 (9.9665e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.97)
Epoch: [35][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.7412e-02 (1.0032e-01)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.96)
Epoch: [35][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.6915e-02 (9.9798e-02)	Acc@1 100.00 ( 97.88)	Acc@5 100.00 ( 99.96)
Epoch: [35][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.9153e-01 (1.0022e-01)	Acc@1  94.53 ( 97.84)	Acc@5 100.00 ( 99.96)
Epoch: [35][140/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.2896e-02 (1.0007e-01)	Acc@1  96.09 ( 97.84)	Acc@5 100.00 ( 99.97)
Epoch: [35][150/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.2397e-02 (1.0122e-01)	Acc@1  98.44 ( 97.79)	Acc@5 100.00 ( 99.96)
Epoch: [35][160/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.3635e-01 (1.0197e-01)	Acc@1  96.88 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [35][170/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.4055e-02 (1.0235e-01)	Acc@1  98.44 ( 97.72)	Acc@5 100.00 ( 99.96)
Epoch: [35][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.6313e-02 (1.0277e-01)	Acc@1  98.44 ( 97.74)	Acc@5 100.00 ( 99.96)
Epoch: [35][190/391]	Time  0.036 ( 0.036)	Data  0.002 ( 0.002)	Loss 1.1188e-01 (1.0300e-01)	Acc@1  96.09 ( 97.71)	Acc@5 100.00 ( 99.96)
Epoch: [35][200/391]	Time  0.043 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.6558e-02 (1.0275e-01)	Acc@1  98.44 ( 97.71)	Acc@5 100.00 ( 99.96)
Epoch: [35][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.7524e-02 (1.0306e-01)	Acc@1  98.44 ( 97.68)	Acc@5 100.00 ( 99.96)
Epoch: [35][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.3313e-02 (1.0244e-01)	Acc@1 100.00 ( 97.71)	Acc@5 100.00 ( 99.96)
Epoch: [35][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.1421e-02 (1.0236e-01)	Acc@1  99.22 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [35][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2976e-01 (1.0251e-01)	Acc@1  95.31 ( 97.71)	Acc@5 100.00 ( 99.97)
Epoch: [35][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2915e-01 (1.0276e-01)	Acc@1  97.66 ( 97.72)	Acc@5 100.00 ( 99.97)
Epoch: [35][260/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1719e-01 (1.0281e-01)	Acc@1  96.09 ( 97.69)	Acc@5 100.00 ( 99.97)
Epoch: [35][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0028e-01 (1.0277e-01)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.97)
Epoch: [35][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.4158e-02 (1.0248e-01)	Acc@1  99.22 ( 97.70)	Acc@5 100.00 ( 99.97)
Epoch: [35][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4819e-02 (1.0215e-01)	Acc@1  98.44 ( 97.70)	Acc@5 100.00 ( 99.97)
Epoch: [35][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.1736e-02 (1.0243e-01)	Acc@1  96.88 ( 97.68)	Acc@5 100.00 ( 99.97)
Epoch: [35][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0797e-01 (1.0264e-01)	Acc@1  98.44 ( 97.67)	Acc@5 100.00 ( 99.97)
Epoch: [35][320/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.6741e-02 (1.0307e-01)	Acc@1  97.66 ( 97.65)	Acc@5 100.00 ( 99.97)
Epoch: [35][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9702e-02 (1.0312e-01)	Acc@1  98.44 ( 97.64)	Acc@5 100.00 ( 99.97)
Epoch: [35][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3892e-01 (1.0308e-01)	Acc@1  95.31 ( 97.64)	Acc@5 100.00 ( 99.97)
Epoch: [35][350/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3147e-01 (1.0312e-01)	Acc@1  95.31 ( 97.65)	Acc@5 100.00 ( 99.97)
Epoch: [35][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5906e-01 (1.0372e-01)	Acc@1  96.09 ( 97.62)	Acc@5 100.00 ( 99.97)
Epoch: [35][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4779e-02 (1.0349e-01)	Acc@1  99.22 ( 97.63)	Acc@5 100.00 ( 99.97)
Epoch: [35][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4697e-02 (1.0334e-01)	Acc@1  98.44 ( 97.63)	Acc@5 100.00 ( 99.97)
Epoch: [35][390/391]	Time  0.025 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.6050e-02 (1.0334e-01)	Acc@1  97.50 ( 97.62)	Acc@5 100.00 ( 99.97)
## e[35] optimizer.zero_grad (sum) time: 0.18000292778015137
## e[35]       loss.backward (sum) time: 4.063874006271362
## e[35]      optimizer.step (sum) time: 1.2666900157928467
## epoch[35] training(only) time: 13.86816930770874
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.1787e+00 (1.1787e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.3047e+00 (1.1926e+00)	Acc@1  71.00 ( 73.27)	Acc@5  90.00 ( 91.45)
Test: [ 20/100]	Time  0.020 ( 0.024)	Loss 1.0938e+00 (1.1381e+00)	Acc@1  76.00 ( 73.90)	Acc@5  93.00 ( 92.52)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.5469e+00 (1.1803e+00)	Acc@1  66.00 ( 73.03)	Acc@5  92.00 ( 92.06)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1289e+00 (1.1552e+00)	Acc@1  70.00 ( 73.07)	Acc@5  95.00 ( 92.66)
Test: [ 50/100]	Time  0.021 ( 0.020)	Loss 1.4268e+00 (1.1625e+00)	Acc@1  70.00 ( 72.90)	Acc@5  92.00 ( 92.45)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.1904e+00 (1.1399e+00)	Acc@1  71.00 ( 73.08)	Acc@5  94.00 ( 92.69)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 1.3545e+00 (1.1312e+00)	Acc@1  68.00 ( 73.25)	Acc@5  92.00 ( 92.79)
Test: [ 80/100]	Time  0.015 ( 0.020)	Loss 1.2822e+00 (1.1338e+00)	Acc@1  71.00 ( 73.06)	Acc@5  90.00 ( 92.72)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.5371e+00 (1.1232e+00)	Acc@1  66.00 ( 73.22)	Acc@5  90.00 ( 92.85)
 * Acc@1 73.380 Acc@5 92.910
### epoch[35] execution time: 15.855278015136719
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.190 ( 0.190)	Data  0.150 ( 0.150)	Loss 1.3660e-01 (1.3660e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [36][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 8.4045e-02 (1.0678e-01)	Acc@1  99.22 ( 98.01)	Acc@5 100.00 ( 99.93)
Epoch: [36][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 8.6426e-02 (1.0064e-01)	Acc@1  99.22 ( 98.03)	Acc@5 100.00 ( 99.96)
Epoch: [36][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 7.3547e-02 (9.4742e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.97)
Epoch: [36][ 40/391]	Time  0.033 ( 0.039)	Data  0.001 ( 0.005)	Loss 6.9275e-02 (9.6674e-02)	Acc@1  99.22 ( 98.02)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 1.1096e-01 (9.8343e-02)	Acc@1  99.22 ( 97.96)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 9.4055e-02 (9.7763e-02)	Acc@1  97.66 ( 98.00)	Acc@5 100.00 ( 99.95)
Epoch: [36][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.9895e-02 (9.8292e-02)	Acc@1  99.22 ( 97.98)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.1700e-01 (9.9077e-02)	Acc@1  95.31 ( 97.83)	Acc@5 100.00 ( 99.94)
Epoch: [36][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0571e-01 (1.0016e-01)	Acc@1  97.66 ( 97.78)	Acc@5 100.00 ( 99.95)
Epoch: [36][100/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.8735e-02 (9.9010e-02)	Acc@1  99.22 ( 97.77)	Acc@5 100.00 ( 99.94)
Epoch: [36][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0712e-01 (9.8607e-02)	Acc@1  98.44 ( 97.77)	Acc@5 100.00 ( 99.94)
Epoch: [36][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.6589e-02 (9.7748e-02)	Acc@1  99.22 ( 97.80)	Acc@5 100.00 ( 99.95)
Epoch: [36][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0504e-01 (9.7771e-02)	Acc@1  98.44 ( 97.78)	Acc@5 100.00 ( 99.95)
Epoch: [36][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.2407e-02 (9.7057e-02)	Acc@1  99.22 ( 97.82)	Acc@5 100.00 ( 99.96)
Epoch: [36][150/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.4331e-02 (9.7052e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.96)
Epoch: [36][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.5002e-02 (9.7244e-02)	Acc@1  98.44 ( 97.82)	Acc@5 100.00 ( 99.96)
Epoch: [36][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.1492e-02 (9.7586e-02)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.96)
Epoch: [36][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.8197e-02 (9.6959e-02)	Acc@1  99.22 ( 97.85)	Acc@5 100.00 ( 99.97)
Epoch: [36][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.1665e-02 (9.7178e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.96)
Epoch: [36][200/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1218e-01 (9.6948e-02)	Acc@1  96.88 ( 97.85)	Acc@5 100.00 ( 99.96)
Epoch: [36][210/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2042e-01 (9.7329e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.96)
Epoch: [36][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0333e-01 (9.7504e-02)	Acc@1  96.88 ( 97.87)	Acc@5 100.00 ( 99.96)
Epoch: [36][230/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2612e-02 (9.7644e-02)	Acc@1 100.00 ( 97.87)	Acc@5 100.00 ( 99.96)
Epoch: [36][240/391]	Time  0.038 ( 0.035)	Data  0.002 ( 0.002)	Loss 9.1797e-02 (9.7531e-02)	Acc@1  98.44 ( 97.88)	Acc@5 100.00 ( 99.96)
Epoch: [36][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9661e-02 (9.7823e-02)	Acc@1  98.44 ( 97.86)	Acc@5 100.00 ( 99.96)
Epoch: [36][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.6548e-02 (9.8167e-02)	Acc@1  98.44 ( 97.85)	Acc@5 100.00 ( 99.96)
Epoch: [36][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2158e-01 (9.7662e-02)	Acc@1  96.88 ( 97.88)	Acc@5 100.00 ( 99.96)
Epoch: [36][280/391]	Time  0.046 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3306e-01 (9.8380e-02)	Acc@1  95.31 ( 97.83)	Acc@5 100.00 ( 99.96)
Epoch: [36][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.6863e-02 (9.8458e-02)	Acc@1  98.44 ( 97.83)	Acc@5 100.00 ( 99.96)
Epoch: [36][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0383e-02 (9.8514e-02)	Acc@1  98.44 ( 97.84)	Acc@5 100.00 ( 99.96)
Epoch: [36][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9172e-02 (9.8755e-02)	Acc@1  97.66 ( 97.82)	Acc@5 100.00 ( 99.96)
Epoch: [36][320/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.5154e-02 (9.8974e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.96)
Epoch: [36][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0236e-01 (9.8918e-02)	Acc@1  97.66 ( 97.80)	Acc@5 100.00 ( 99.96)
Epoch: [36][340/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3428e-01 (9.8636e-02)	Acc@1  96.88 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [36][350/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4246e-01 (9.8793e-02)	Acc@1  96.09 ( 97.81)	Acc@5 100.00 ( 99.96)
Epoch: [36][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.5344e-01 (9.9063e-02)	Acc@1  96.88 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [36][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5754e-02 (9.9045e-02)	Acc@1  99.22 ( 97.80)	Acc@5 100.00 ( 99.97)
Epoch: [36][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7036e-02 (9.8889e-02)	Acc@1  97.66 ( 97.81)	Acc@5 100.00 ( 99.97)
Epoch: [36][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1932e-01 (9.8645e-02)	Acc@1  97.50 ( 97.82)	Acc@5  98.75 ( 99.97)
## e[36] optimizer.zero_grad (sum) time: 0.17943954467773438
## e[36]       loss.backward (sum) time: 4.027999639511108
## e[36]      optimizer.step (sum) time: 1.2638697624206543
## epoch[36] training(only) time: 13.849790096282959
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.1982e+00 (1.1982e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.016 ( 0.029)	Loss 1.3027e+00 (1.1920e+00)	Acc@1  71.00 ( 73.27)	Acc@5  91.00 ( 91.91)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.0898e+00 (1.1327e+00)	Acc@1  78.00 ( 74.48)	Acc@5  93.00 ( 92.57)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5264e+00 (1.1758e+00)	Acc@1  67.00 ( 73.29)	Acc@5  92.00 ( 92.13)
Test: [ 40/100]	Time  0.023 ( 0.021)	Loss 1.1426e+00 (1.1525e+00)	Acc@1  72.00 ( 73.39)	Acc@5  93.00 ( 92.66)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4395e+00 (1.1597e+00)	Acc@1  70.00 ( 73.27)	Acc@5  92.00 ( 92.49)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.1777e+00 (1.1374e+00)	Acc@1  72.00 ( 73.48)	Acc@5  94.00 ( 92.70)
Test: [ 70/100]	Time  0.029 ( 0.020)	Loss 1.3877e+00 (1.1289e+00)	Acc@1  66.00 ( 73.51)	Acc@5  93.00 ( 92.82)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.2607e+00 (1.1314e+00)	Acc@1  73.00 ( 73.40)	Acc@5  91.00 ( 92.79)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5479e+00 (1.1216e+00)	Acc@1  67.00 ( 73.47)	Acc@5  91.00 ( 92.90)
 * Acc@1 73.670 Acc@5 92.920
### epoch[36] execution time: 15.821141958236694
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.191 ( 0.191)	Data  0.150 ( 0.150)	Loss 1.0541e-01 (1.0541e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [37][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 8.3801e-02 (9.2801e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.93)
Epoch: [37][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 8.7036e-02 (8.9167e-02)	Acc@1  98.44 ( 98.25)	Acc@5 100.00 ( 99.93)
Epoch: [37][ 30/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.006)	Loss 6.1676e-02 (9.0211e-02)	Acc@1 100.00 ( 98.16)	Acc@5 100.00 ( 99.95)
Epoch: [37][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 7.6599e-02 (9.2252e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.96)
Epoch: [37][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 7.3975e-02 (9.1140e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 8.3374e-02 (9.1328e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.7444e-02 (9.0572e-02)	Acc@1  98.44 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [37][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.2830e-01 (9.0008e-02)	Acc@1  97.66 ( 98.16)	Acc@5  99.22 ( 99.96)
Epoch: [37][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1401e-01 (9.1445e-02)	Acc@1  96.09 ( 98.13)	Acc@5 100.00 ( 99.96)
Epoch: [37][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.2144e-02 (9.1232e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.96)
Epoch: [37][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0638e-01 (9.0928e-02)	Acc@1  97.66 ( 98.11)	Acc@5  99.22 ( 99.96)
Epoch: [37][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.9641e-02 (9.0079e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.96)
Epoch: [37][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1646e-01 (8.9636e-02)	Acc@1  96.88 ( 98.22)	Acc@5 100.00 ( 99.96)
Epoch: [37][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.1035e-02 (8.9581e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 ( 99.97)
Epoch: [37][150/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0345e-01 (8.9283e-02)	Acc@1  96.09 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [37][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9326e-02 (8.8887e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [37][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.8604e-02 (8.8766e-02)	Acc@1  98.44 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [37][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.9731e-02 (8.9577e-02)	Acc@1  97.66 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [37][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.8572e-02 (8.9242e-02)	Acc@1  98.44 ( 98.15)	Acc@5 100.00 ( 99.98)
Epoch: [37][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.2397e-02 (8.9419e-02)	Acc@1  99.22 ( 98.13)	Acc@5 100.00 ( 99.98)
Epoch: [37][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.4290e-02 (9.0006e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [37][220/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.2773e-02 (9.0309e-02)	Acc@1  96.88 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [37][230/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.0190e-02 (9.0035e-02)	Acc@1  97.66 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [37][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1462e-02 (9.0041e-02)	Acc@1 100.00 ( 98.11)	Acc@5 100.00 ( 99.98)
Epoch: [37][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5939e-02 (9.0147e-02)	Acc@1  98.44 ( 98.10)	Acc@5 100.00 ( 99.98)
Epoch: [37][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2854e-01 (9.0966e-02)	Acc@1  96.09 ( 98.06)	Acc@5 100.00 ( 99.98)
Epoch: [37][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0688e-02 (9.1395e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 ( 99.97)
Epoch: [37][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.3201e-02 (9.1322e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 ( 99.96)
Epoch: [37][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.4482e-02 (9.1163e-02)	Acc@1  96.88 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [37][300/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.0210e-02 (9.1132e-02)	Acc@1  97.66 ( 98.06)	Acc@5 100.00 ( 99.97)
Epoch: [37][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1273e-01 (9.1084e-02)	Acc@1  97.66 ( 98.07)	Acc@5 100.00 ( 99.97)
Epoch: [37][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.8022e-02 (9.1237e-02)	Acc@1  97.66 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [37][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5510e-02 (9.1265e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [37][340/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.4563e-01 (9.1710e-02)	Acc@1  94.53 ( 98.03)	Acc@5 100.00 ( 99.97)
Epoch: [37][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.2693e-02 (9.1472e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 ( 99.97)
Epoch: [37][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0352e-01 (9.1484e-02)	Acc@1  96.09 ( 98.04)	Acc@5 100.00 ( 99.97)
Epoch: [37][370/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6223e-02 (9.1079e-02)	Acc@1  99.22 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [37][380/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.6172e-02 (9.1321e-02)	Acc@1 100.00 ( 98.05)	Acc@5 100.00 ( 99.97)
Epoch: [37][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.6174e-01 (9.1526e-02)	Acc@1  95.00 ( 98.03)	Acc@5 100.00 ( 99.97)
## e[37] optimizer.zero_grad (sum) time: 0.17976689338684082
## e[37]       loss.backward (sum) time: 4.050789833068848
## e[37]      optimizer.step (sum) time: 1.2680819034576416
## epoch[37] training(only) time: 13.884675025939941
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.1816e+00 (1.1816e+00)	Acc@1  71.00 ( 71.00)	Acc@5  94.00 ( 94.00)
Test: [ 10/100]	Time  0.022 ( 0.029)	Loss 1.3164e+00 (1.1851e+00)	Acc@1  73.00 ( 72.91)	Acc@5  91.00 ( 92.00)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.0811e+00 (1.1289e+00)	Acc@1  76.00 ( 74.10)	Acc@5  93.00 ( 92.48)
Test: [ 30/100]	Time  0.019 ( 0.022)	Loss 1.5361e+00 (1.1746e+00)	Acc@1  66.00 ( 73.06)	Acc@5  92.00 ( 92.06)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1338e+00 (1.1526e+00)	Acc@1  71.00 ( 73.07)	Acc@5  93.00 ( 92.59)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4297e+00 (1.1571e+00)	Acc@1  71.00 ( 73.06)	Acc@5  92.00 ( 92.41)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.2129e+00 (1.1356e+00)	Acc@1  72.00 ( 73.16)	Acc@5  95.00 ( 92.66)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 1.3604e+00 (1.1266e+00)	Acc@1  69.00 ( 73.37)	Acc@5  92.00 ( 92.73)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.2510e+00 (1.1275e+00)	Acc@1  73.00 ( 73.22)	Acc@5  90.00 ( 92.64)
Test: [ 90/100]	Time  0.019 ( 0.019)	Loss 1.5439e+00 (1.1171e+00)	Acc@1  68.00 ( 73.45)	Acc@5  90.00 ( 92.78)
 * Acc@1 73.630 Acc@5 92.840
### epoch[37] execution time: 15.86359691619873
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.192 ( 0.192)	Data  0.151 ( 0.151)	Loss 1.0657e-01 (1.0657e-01)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [38][ 10/391]	Time  0.044 ( 0.050)	Data  0.001 ( 0.015)	Loss 7.4524e-02 (9.5742e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.93)
Epoch: [38][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.009)	Loss 8.1726e-02 (9.5523e-02)	Acc@1  98.44 ( 97.99)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.007)	Loss 5.7526e-02 (9.4562e-02)	Acc@1  99.22 ( 97.83)	Acc@5 100.00 ( 99.95)
Epoch: [38][ 40/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 1.2610e-01 (9.2956e-02)	Acc@1  96.88 ( 97.92)	Acc@5 100.00 ( 99.96)
Epoch: [38][ 50/391]	Time  0.039 ( 0.038)	Data  0.004 ( 0.005)	Loss 1.1084e-01 (9.1321e-02)	Acc@1  96.88 ( 97.96)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 60/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.004)	Loss 1.2610e-01 (9.0079e-02)	Acc@1  95.31 ( 97.94)	Acc@5 100.00 ( 99.97)
Epoch: [38][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 9.4788e-02 (8.9416e-02)	Acc@1  98.44 ( 98.02)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.3528e-02 (8.8100e-02)	Acc@1  99.22 ( 98.09)	Acc@5 100.00 ( 99.98)
Epoch: [38][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.3965e-02 (8.9037e-02)	Acc@1  98.44 ( 98.06)	Acc@5 100.00 ( 99.97)
Epoch: [38][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.6538e-02 (8.8408e-02)	Acc@1  99.22 ( 98.04)	Acc@5 100.00 ( 99.98)
Epoch: [38][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.5613e-02 (8.7177e-02)	Acc@1  99.22 ( 98.08)	Acc@5 100.00 ( 99.97)
Epoch: [38][120/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.7261e-02 (8.7143e-02)	Acc@1  99.22 ( 98.06)	Acc@5 100.00 ( 99.97)
Epoch: [38][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0175e-01 (8.6403e-02)	Acc@1  97.66 ( 98.09)	Acc@5 100.00 ( 99.97)
Epoch: [38][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.8938e-02 (8.7230e-02)	Acc@1  98.44 ( 98.08)	Acc@5 100.00 ( 99.97)
Epoch: [38][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.6833e-02 (8.6554e-02)	Acc@1  99.22 ( 98.12)	Acc@5 100.00 ( 99.97)
Epoch: [38][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.6191e-02 (8.6880e-02)	Acc@1  98.44 ( 98.14)	Acc@5 100.00 ( 99.98)
Epoch: [38][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.3201e-02 (8.6216e-02)	Acc@1  97.66 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [38][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.1279e-02 (8.5302e-02)	Acc@1  99.22 ( 98.21)	Acc@5 100.00 ( 99.98)
Epoch: [38][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5388e-02 (8.5688e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.98)
Epoch: [38][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2805e-01 (8.6168e-02)	Acc@1  97.66 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [38][210/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.1411e-02 (8.6708e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [38][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.9111e-02 (8.6508e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.98)
Epoch: [38][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0687e-01 (8.6129e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [38][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.1167e-02 (8.6468e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.98)
Epoch: [38][250/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1591e-01 (8.6757e-02)	Acc@1  96.88 ( 98.16)	Acc@5 100.00 ( 99.98)
Epoch: [38][260/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6406e-02 (8.6616e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [38][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7708e-02 (8.6566e-02)	Acc@1  97.66 ( 98.19)	Acc@5 100.00 ( 99.97)
Epoch: [38][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4067e-02 (8.6118e-02)	Acc@1  99.22 ( 98.20)	Acc@5 100.00 ( 99.97)
Epoch: [38][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0870e-01 (8.6323e-02)	Acc@1  96.88 ( 98.19)	Acc@5 100.00 ( 99.98)
Epoch: [38][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0974e-01 (8.6997e-02)	Acc@1  95.31 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [38][310/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5502e-02 (8.6944e-02)	Acc@1 100.00 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [38][320/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0327e-01 (8.7209e-02)	Acc@1  96.88 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [38][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3253e-02 (8.7073e-02)	Acc@1  99.22 ( 98.17)	Acc@5 100.00 ( 99.97)
Epoch: [38][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3025e-01 (8.7201e-02)	Acc@1  95.31 ( 98.14)	Acc@5  99.22 ( 99.97)
Epoch: [38][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5349e-02 (8.7008e-02)	Acc@1  99.22 ( 98.16)	Acc@5 100.00 ( 99.97)
Epoch: [38][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9172e-02 (8.6860e-02)	Acc@1  99.22 ( 98.18)	Acc@5 100.00 ( 99.97)
Epoch: [38][370/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2878e-01 (8.7596e-02)	Acc@1  96.09 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [38][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6814e-02 (8.7544e-02)	Acc@1 100.00 ( 98.14)	Acc@5 100.00 ( 99.97)
Epoch: [38][390/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0028e-02 (8.7707e-02)	Acc@1  98.75 ( 98.13)	Acc@5 100.00 ( 99.97)
## e[38] optimizer.zero_grad (sum) time: 0.17955589294433594
## e[38]       loss.backward (sum) time: 4.005618333816528
## e[38]      optimizer.step (sum) time: 1.2437124252319336
## epoch[38] training(only) time: 13.881016731262207
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.1846e+00 (1.1846e+00)	Acc@1  71.00 ( 71.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.020 ( 0.029)	Loss 1.2861e+00 (1.1832e+00)	Acc@1  71.00 ( 72.73)	Acc@5  91.00 ( 92.09)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.0801e+00 (1.1319e+00)	Acc@1  76.00 ( 73.62)	Acc@5  93.00 ( 92.71)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.5625e+00 (1.1792e+00)	Acc@1  66.00 ( 72.68)	Acc@5  93.00 ( 92.39)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1514e+00 (1.1567e+00)	Acc@1  72.00 ( 72.88)	Acc@5  93.00 ( 92.88)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.4170e+00 (1.1645e+00)	Acc@1  70.00 ( 72.86)	Acc@5  91.00 ( 92.49)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.2188e+00 (1.1436e+00)	Acc@1  72.00 ( 72.97)	Acc@5  94.00 ( 92.72)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3594e+00 (1.1357e+00)	Acc@1  66.00 ( 73.07)	Acc@5  92.00 ( 92.82)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.2568e+00 (1.1371e+00)	Acc@1  73.00 ( 73.01)	Acc@5  90.00 ( 92.73)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 1.5332e+00 (1.1260e+00)	Acc@1  69.00 ( 73.18)	Acc@5  91.00 ( 92.84)
 * Acc@1 73.370 Acc@5 92.870
### epoch[38] execution time: 15.865447044372559
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.202 ( 0.202)	Data  0.161 ( 0.161)	Loss 9.0759e-02 (9.0759e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [39][ 10/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.016)	Loss 5.0507e-02 (7.8250e-02)	Acc@1  99.22 ( 98.22)	Acc@5 100.00 ( 99.93)
Epoch: [39][ 20/391]	Time  0.033 ( 0.043)	Data  0.001 ( 0.009)	Loss 6.4697e-02 (7.8731e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 30/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.007)	Loss 6.3904e-02 (8.2260e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.006)	Loss 1.0419e-01 (8.1495e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 50/391]	Time  0.043 ( 0.038)	Data  0.001 ( 0.005)	Loss 8.7097e-02 (7.9488e-02)	Acc@1  97.66 ( 98.56)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 60/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.4962e-02 (7.8957e-02)	Acc@1 100.00 ( 98.51)	Acc@5 100.00 ( 99.96)
Epoch: [39][ 70/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.1289e-02 (8.1836e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 80/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.1422e-02 (8.2538e-02)	Acc@1 100.00 ( 98.45)	Acc@5 100.00 ( 99.97)
Epoch: [39][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.0980e-01 (8.2314e-02)	Acc@1  96.88 ( 98.44)	Acc@5 100.00 ( 99.97)
Epoch: [39][100/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.8613e-02 (8.2287e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.97)
Epoch: [39][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.7087e-02 (8.2589e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.96)
Epoch: [39][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.6008e-02 (8.3194e-02)	Acc@1  98.44 ( 98.40)	Acc@5 100.00 ( 99.95)
Epoch: [39][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.2388e-02 (8.3257e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.96)
Epoch: [39][140/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.7698e-02 (8.3205e-02)	Acc@1  99.22 ( 98.36)	Acc@5 100.00 ( 99.96)
Epoch: [39][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8655e-02 (8.3711e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.96)
Epoch: [39][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3427e-02 (8.3406e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [39][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1707e-01 (8.2636e-02)	Acc@1  96.09 ( 98.36)	Acc@5 100.00 ( 99.96)
Epoch: [39][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0364e-01 (8.2660e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.97)
Epoch: [39][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.1370e-02 (8.2598e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.96)
Epoch: [39][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.0017e-02 (8.2815e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.96)
Epoch: [39][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.6182e-02 (8.2814e-02)	Acc@1  97.66 ( 98.33)	Acc@5 100.00 ( 99.96)
Epoch: [39][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.0984e-02 (8.2708e-02)	Acc@1  99.22 ( 98.33)	Acc@5 100.00 ( 99.96)
Epoch: [39][230/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1859e-01 (8.2778e-02)	Acc@1  96.88 ( 98.34)	Acc@5 100.00 ( 99.96)
Epoch: [39][240/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5441e-02 (8.2503e-02)	Acc@1  99.22 ( 98.34)	Acc@5 100.00 ( 99.96)
Epoch: [39][250/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.1838e-02 (8.2376e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [39][260/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.9712e-02 (8.2534e-02)	Acc@1  97.66 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [39][270/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.7290e-02 (8.2885e-02)	Acc@1  97.66 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [39][280/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.9111e-02 (8.3669e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [39][290/391]	Time  0.035 ( 0.035)	Data  0.002 ( 0.002)	Loss 9.9487e-02 (8.3269e-02)	Acc@1  98.44 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [39][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.6243e-02 (8.3617e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [39][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.6538e-02 (8.3585e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [39][320/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6652e-02 (8.3326e-02)	Acc@1 100.00 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [39][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9050e-02 (8.3412e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [39][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.2815e-02 (8.3112e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [39][350/391]	Time  0.041 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.8196e-02 (8.3328e-02)	Acc@1  96.88 ( 98.31)	Acc@5 100.00 ( 99.97)
Epoch: [39][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0138e-01 (8.3609e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [39][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0253e-02 (8.3793e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [39][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0571e-01 (8.3816e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [39][390/391]	Time  0.027 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1438e-01 (8.3867e-02)	Acc@1  96.25 ( 98.28)	Acc@5 100.00 ( 99.97)
## e[39] optimizer.zero_grad (sum) time: 0.17798161506652832
## e[39]       loss.backward (sum) time: 4.001450061798096
## e[39]      optimizer.step (sum) time: 1.2825264930725098
## epoch[39] training(only) time: 13.882329225540161
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.1914e+00 (1.1914e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.020 ( 0.031)	Loss 1.2969e+00 (1.1998e+00)	Acc@1  71.00 ( 73.09)	Acc@5  91.00 ( 91.82)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 1.1074e+00 (1.1437e+00)	Acc@1  78.00 ( 74.24)	Acc@5  93.00 ( 92.57)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.5850e+00 (1.1864e+00)	Acc@1  66.00 ( 73.13)	Acc@5  93.00 ( 92.35)
Test: [ 40/100]	Time  0.019 ( 0.021)	Loss 1.1123e+00 (1.1613e+00)	Acc@1  71.00 ( 73.20)	Acc@5  93.00 ( 92.80)
Test: [ 50/100]	Time  0.021 ( 0.021)	Loss 1.3916e+00 (1.1673e+00)	Acc@1  70.00 ( 73.02)	Acc@5  92.00 ( 92.53)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 1.1953e+00 (1.1466e+00)	Acc@1  71.00 ( 73.07)	Acc@5  92.00 ( 92.67)
Test: [ 70/100]	Time  0.015 ( 0.020)	Loss 1.3711e+00 (1.1395e+00)	Acc@1  66.00 ( 73.20)	Acc@5  92.00 ( 92.75)
Test: [ 80/100]	Time  0.015 ( 0.020)	Loss 1.2617e+00 (1.1412e+00)	Acc@1  73.00 ( 73.10)	Acc@5  90.00 ( 92.67)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.5605e+00 (1.1303e+00)	Acc@1  68.00 ( 73.32)	Acc@5  90.00 ( 92.81)
 * Acc@1 73.470 Acc@5 92.910
### epoch[39] execution time: 15.87659215927124
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.190 ( 0.190)	Data  0.149 ( 0.149)	Loss 6.9946e-02 (6.9946e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [40][ 10/391]	Time  0.038 ( 0.049)	Data  0.001 ( 0.015)	Loss 7.3669e-02 (7.1164e-02)	Acc@1  99.22 ( 98.51)	Acc@5 100.00 (100.00)
Epoch: [40][ 20/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.009)	Loss 3.8300e-02 (7.4056e-02)	Acc@1 100.00 ( 98.55)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 7.5500e-02 (7.7305e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 8.4167e-02 (7.8908e-02)	Acc@1  98.44 ( 98.30)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.9275e-02 (7.7640e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [40][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.6895e-02 (7.7729e-02)	Acc@1  99.22 ( 98.46)	Acc@5 100.00 ( 99.96)
Epoch: [40][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.0547e-02 (7.6858e-02)	Acc@1  99.22 ( 98.46)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.8308e-02 (7.7420e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.97)
Epoch: [40][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1322e-01 (7.8335e-02)	Acc@1  96.88 ( 98.39)	Acc@5 100.00 ( 99.97)
Epoch: [40][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.3914e-02 (7.8449e-02)	Acc@1  98.44 ( 98.34)	Acc@5 100.00 ( 99.98)
Epoch: [40][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0541e-01 (7.8852e-02)	Acc@1  97.66 ( 98.34)	Acc@5 100.00 ( 99.97)
Epoch: [40][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.2205e-02 (7.9869e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [40][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.4220e-02 (8.0244e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.96)
Epoch: [40][140/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.0312e-02 (8.0208e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.97)
Epoch: [40][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.5378e-02 (8.0064e-02)	Acc@1  97.66 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [40][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.9834e-02 (7.9719e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.97)
Epoch: [40][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.8867e-02 (7.9793e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 ( 99.96)
Epoch: [40][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.2512e-01 (8.0101e-02)	Acc@1  96.09 ( 98.33)	Acc@5 100.00 ( 99.96)
Epoch: [40][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.2285e-02 (7.9806e-02)	Acc@1  97.66 ( 98.35)	Acc@5 100.00 ( 99.96)
Epoch: [40][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9072e-02 (8.0204e-02)	Acc@1  99.22 ( 98.32)	Acc@5 100.00 ( 99.96)
Epoch: [40][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0022e-01 (8.0095e-02)	Acc@1  96.88 ( 98.33)	Acc@5 100.00 ( 99.96)
Epoch: [40][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.2468e-02 (8.0203e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.96)
Epoch: [40][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2256e-02 (8.0496e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.97)
Epoch: [40][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.4768e-02 (8.0640e-02)	Acc@1  96.88 ( 98.30)	Acc@5 100.00 ( 99.96)
Epoch: [40][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.2998e-02 (8.0851e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.96)
Epoch: [40][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.0435e-02 (8.0946e-02)	Acc@1  98.44 ( 98.28)	Acc@5 100.00 ( 99.96)
Epoch: [40][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.0312e-02 (8.0683e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 ( 99.97)
Epoch: [40][280/391]	Time  0.034 ( 0.035)	Data  0.000 ( 0.002)	Loss 8.3313e-02 (8.0784e-02)	Acc@1  96.88 ( 98.28)	Acc@5 100.00 ( 99.96)
Epoch: [40][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.8135e-02 (8.1431e-02)	Acc@1  96.88 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [40][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5603e-02 (8.1475e-02)	Acc@1 100.00 ( 98.27)	Acc@5 100.00 ( 99.97)
Epoch: [40][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6101e-02 (8.2038e-02)	Acc@1  98.44 ( 98.24)	Acc@5 100.00 ( 99.96)
Epoch: [40][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2927e-02 (8.1731e-02)	Acc@1 100.00 ( 98.26)	Acc@5 100.00 ( 99.97)
Epoch: [40][330/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.4360e-02 (8.2218e-02)	Acc@1  97.66 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [40][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6152e-02 (8.2222e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [40][350/391]	Time  0.044 ( 0.035)	Data  0.004 ( 0.002)	Loss 8.7830e-02 (8.2177e-02)	Acc@1  98.44 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [40][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.5642e-02 (8.2104e-02)	Acc@1  96.88 ( 98.23)	Acc@5 100.00 ( 99.97)
Epoch: [40][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.8857e-02 (8.1886e-02)	Acc@1  97.66 ( 98.24)	Acc@5 100.00 ( 99.97)
Epoch: [40][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1194e-01 (8.1641e-02)	Acc@1  96.88 ( 98.25)	Acc@5 100.00 ( 99.97)
Epoch: [40][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0736e-01 (8.1666e-02)	Acc@1  97.50 ( 98.24)	Acc@5  98.75 ( 99.96)
## e[40] optimizer.zero_grad (sum) time: 0.17909598350524902
## e[40]       loss.backward (sum) time: 4.150517225265503
## e[40]      optimizer.step (sum) time: 1.239983320236206
## epoch[40] training(only) time: 13.896076202392578
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.1592e+00 (1.1592e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.020 ( 0.031)	Loss 1.2588e+00 (1.1805e+00)	Acc@1  71.00 ( 72.36)	Acc@5  91.00 ( 91.36)
Test: [ 20/100]	Time  0.014 ( 0.025)	Loss 1.1143e+00 (1.1323e+00)	Acc@1  76.00 ( 73.57)	Acc@5  92.00 ( 92.24)
Test: [ 30/100]	Time  0.014 ( 0.023)	Loss 1.5645e+00 (1.1818e+00)	Acc@1  67.00 ( 72.74)	Acc@5  92.00 ( 92.06)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 1.1348e+00 (1.1581e+00)	Acc@1  70.00 ( 72.85)	Acc@5  93.00 ( 92.61)
Test: [ 50/100]	Time  0.015 ( 0.021)	Loss 1.4424e+00 (1.1666e+00)	Acc@1  70.00 ( 72.73)	Acc@5  92.00 ( 92.43)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.2441e+00 (1.1445e+00)	Acc@1  71.00 ( 72.93)	Acc@5  92.00 ( 92.64)
Test: [ 70/100]	Time  0.021 ( 0.020)	Loss 1.3838e+00 (1.1383e+00)	Acc@1  67.00 ( 73.04)	Acc@5  92.00 ( 92.73)
Test: [ 80/100]	Time  0.015 ( 0.020)	Loss 1.2783e+00 (1.1403e+00)	Acc@1  74.00 ( 73.02)	Acc@5  90.00 ( 92.62)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.5322e+00 (1.1282e+00)	Acc@1  68.00 ( 73.21)	Acc@5  91.00 ( 92.78)
 * Acc@1 73.470 Acc@5 92.820
### epoch[40] execution time: 15.88964557647705
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.182 ( 0.182)	Data  0.142 ( 0.142)	Loss 6.6772e-02 (6.6772e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [41][ 10/391]	Time  0.034 ( 0.048)	Data  0.001 ( 0.014)	Loss 1.0162e-01 (7.9920e-02)	Acc@1  96.88 ( 98.22)	Acc@5 100.00 (100.00)
Epoch: [41][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 1.0266e-01 (7.9431e-02)	Acc@1  97.66 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [41][ 30/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.006)	Loss 7.3730e-02 (7.9738e-02)	Acc@1  98.44 ( 98.29)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 40/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.005)	Loss 5.7800e-02 (7.7987e-02)	Acc@1  98.44 ( 98.32)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 8.2703e-02 (7.6345e-02)	Acc@1  98.44 ( 98.36)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 60/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.0303e-02 (7.6440e-02)	Acc@1  98.44 ( 98.37)	Acc@5 100.00 ( 99.97)
Epoch: [41][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.7688e-02 (7.7460e-02)	Acc@1  99.22 ( 98.35)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.7087e-02 (7.7473e-02)	Acc@1 100.00 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [41][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.3364e-02 (7.6359e-02)	Acc@1  98.44 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [41][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.4331e-02 (7.6882e-02)	Acc@1  98.44 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [41][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.7444e-02 (7.7024e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [41][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.5247e-02 (7.7181e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [41][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0962e-01 (7.6673e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [41][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.9824e-02 (7.7128e-02)	Acc@1  97.66 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [41][150/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.9255e-02 (7.6890e-02)	Acc@1 100.00 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [41][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7516e-02 (7.6713e-02)	Acc@1 100.00 ( 98.46)	Acc@5 100.00 ( 99.99)
Epoch: [41][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4250e-02 (7.6301e-02)	Acc@1  99.22 ( 98.49)	Acc@5 100.00 ( 99.99)
Epoch: [41][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.1797e-02 (7.6571e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 ( 99.99)
Epoch: [41][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.0393e-02 (7.6688e-02)	Acc@1  96.88 ( 98.45)	Acc@5 100.00 ( 99.99)
Epoch: [41][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5979e-02 (7.6852e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [41][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.9060e-02 (7.6866e-02)	Acc@1  97.66 ( 98.44)	Acc@5  99.22 ( 99.99)
Epoch: [41][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6610e-02 (7.6657e-02)	Acc@1 100.00 ( 98.47)	Acc@5 100.00 ( 99.98)
Epoch: [41][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.4585e-02 (7.6993e-02)	Acc@1  99.22 ( 98.45)	Acc@5 100.00 ( 99.98)
Epoch: [41][240/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6223e-02 (7.7430e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.98)
Epoch: [41][250/391]	Time  0.035 ( 0.036)	Data  0.002 ( 0.002)	Loss 6.4514e-02 (7.7297e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [41][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.2458e-02 (7.7340e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [41][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1422e-02 (7.7378e-02)	Acc@1  99.22 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [41][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4106e-02 (7.7413e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [41][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.4463e-02 (7.7088e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.99)
Epoch: [41][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.1299e-02 (7.7034e-02)	Acc@1  99.22 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [41][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2267e-02 (7.7163e-02)	Acc@1 100.00 ( 98.44)	Acc@5 100.00 ( 99.98)
Epoch: [41][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.9966e-02 (7.7251e-02)	Acc@1  97.66 ( 98.44)	Acc@5 100.00 ( 99.99)
Epoch: [41][330/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.3342e-01 (7.7517e-02)	Acc@1  95.31 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [41][340/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.2764e-02 (7.7421e-02)	Acc@1  98.44 ( 98.41)	Acc@5 100.00 ( 99.99)
Epoch: [41][350/391]	Time  0.038 ( 0.035)	Data  0.004 ( 0.002)	Loss 7.3120e-02 (7.7386e-02)	Acc@1  99.22 ( 98.42)	Acc@5 100.00 ( 99.98)
Epoch: [41][360/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1422e-02 (7.7343e-02)	Acc@1  99.22 ( 98.43)	Acc@5 100.00 ( 99.98)
Epoch: [41][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8309e-02 (7.7349e-02)	Acc@1 100.00 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [41][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0750e-02 (7.7374e-02)	Acc@1  98.44 ( 98.42)	Acc@5 100.00 ( 99.99)
Epoch: [41][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1438e-01 (7.7598e-02)	Acc@1  93.75 ( 98.40)	Acc@5 100.00 ( 99.99)
## e[41] optimizer.zero_grad (sum) time: 0.1787092685699463
## e[41]       loss.backward (sum) time: 4.0370213985443115
## e[41]      optimizer.step (sum) time: 1.244520902633667
## epoch[41] training(only) time: 13.904079675674438
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.1826e+00 (1.1826e+00)	Acc@1  70.00 ( 70.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.030)	Loss 1.2988e+00 (1.1887e+00)	Acc@1  72.00 ( 73.36)	Acc@5  91.00 ( 91.64)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.0947e+00 (1.1387e+00)	Acc@1  76.00 ( 74.19)	Acc@5  93.00 ( 92.38)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5742e+00 (1.1824e+00)	Acc@1  65.00 ( 73.16)	Acc@5  93.00 ( 92.23)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 1.1221e+00 (1.1582e+00)	Acc@1  71.00 ( 73.34)	Acc@5  94.00 ( 92.76)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4336e+00 (1.1645e+00)	Acc@1  70.00 ( 73.16)	Acc@5  92.00 ( 92.57)
Test: [ 60/100]	Time  0.025 ( 0.020)	Loss 1.2568e+00 (1.1457e+00)	Acc@1  70.00 ( 73.13)	Acc@5  93.00 ( 92.77)
Test: [ 70/100]	Time  0.020 ( 0.019)	Loss 1.3936e+00 (1.1382e+00)	Acc@1  68.00 ( 73.25)	Acc@5  92.00 ( 92.82)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.2412e+00 (1.1402e+00)	Acc@1  74.00 ( 73.16)	Acc@5  90.00 ( 92.68)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.5654e+00 (1.1283e+00)	Acc@1  68.00 ( 73.40)	Acc@5  92.00 ( 92.84)
 * Acc@1 73.600 Acc@5 92.920
### epoch[41] execution time: 15.859598636627197
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.183 ( 0.183)	Data  0.141 ( 0.141)	Loss 8.8135e-02 (8.8135e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [42][ 10/391]	Time  0.033 ( 0.048)	Data  0.001 ( 0.014)	Loss 7.9102e-02 (6.9189e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [42][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.008)	Loss 1.2250e-01 (7.2728e-02)	Acc@1  96.88 ( 98.47)	Acc@5 100.00 (100.00)
Epoch: [42][ 30/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.006)	Loss 5.5420e-02 (7.3484e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [42][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 1.1987e-01 (7.3788e-02)	Acc@1  96.88 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [42][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.5176e-02 (7.4176e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [42][ 60/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 7.2083e-02 (7.2732e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [42][ 70/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.3792e-02 (7.2782e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 (100.00)
Epoch: [42][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.5634e-02 (7.2982e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [42][ 90/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.5369e-02 (7.2352e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 (100.00)
Epoch: [42][100/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.0984e-02 (7.2023e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [42][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.6650e-02 (7.1925e-02)	Acc@1 100.00 ( 98.67)	Acc@5 100.00 (100.00)
Epoch: [42][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.1786e-01 (7.2514e-02)	Acc@1  96.88 ( 98.65)	Acc@5 100.00 (100.00)
Epoch: [42][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.0759e-02 (7.2773e-02)	Acc@1  96.88 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [42][140/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8909e-02 (7.2883e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [42][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0333e-02 (7.3139e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [42][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.0679e-02 (7.3146e-02)	Acc@1 100.00 ( 98.64)	Acc@5 100.00 (100.00)
Epoch: [42][170/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2155e-02 (7.3543e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 (100.00)
Epoch: [42][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.2458e-02 (7.3682e-02)	Acc@1  96.88 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [42][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.2195e-02 (7.3569e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [42][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5510e-02 (7.3905e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [42][210/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.5786e-02 (7.3405e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [42][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.2144e-02 (7.3541e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [42][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.1106e-02 (7.3534e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [42][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.7810e-02 (7.3233e-02)	Acc@1  99.22 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [42][250/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.7444e-02 (7.3497e-02)	Acc@1  97.66 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [42][260/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.4604e-02 (7.4020e-02)	Acc@1  97.66 ( 98.56)	Acc@5 100.00 ( 99.99)
Epoch: [42][270/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.5398e-02 (7.3672e-02)	Acc@1  98.44 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [42][280/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.0688e-02 (7.3722e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [42][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9336e-02 (7.3687e-02)	Acc@1  99.22 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [42][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2439e-02 (7.3647e-02)	Acc@1  99.22 ( 98.59)	Acc@5 100.00 ( 99.99)
Epoch: [42][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0114e-01 (7.3708e-02)	Acc@1  96.09 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [42][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4656e-02 (7.3462e-02)	Acc@1  96.09 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [42][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.9731e-02 (7.3655e-02)	Acc@1  99.22 ( 98.58)	Acc@5 100.00 ( 99.99)
Epoch: [42][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2755e-02 (7.3553e-02)	Acc@1 100.00 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [42][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.2815e-02 (7.3597e-02)	Acc@1  98.44 ( 98.60)	Acc@5 100.00 ( 99.99)
Epoch: [42][360/391]	Time  0.040 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1849e-02 (7.3289e-02)	Acc@1 100.00 ( 98.61)	Acc@5 100.00 ( 99.99)
Epoch: [42][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5317e-02 (7.3166e-02)	Acc@1  99.22 ( 98.62)	Acc@5 100.00 ( 99.99)
Epoch: [42][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5796e-02 (7.3085e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [42][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.3435e-02 (7.3666e-02)	Acc@1  97.50 ( 98.60)	Acc@5 100.00 ( 99.99)
## e[42] optimizer.zero_grad (sum) time: 0.17966651916503906
## e[42]       loss.backward (sum) time: 4.084493160247803
## e[42]      optimizer.step (sum) time: 1.2718565464019775
## epoch[42] training(only) time: 13.884459972381592
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.1787e+00 (1.1787e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.016 ( 0.029)	Loss 1.3047e+00 (1.1953e+00)	Acc@1  73.00 ( 73.45)	Acc@5  90.00 ( 91.18)
Test: [ 20/100]	Time  0.016 ( 0.023)	Loss 1.1084e+00 (1.1435e+00)	Acc@1  75.00 ( 74.24)	Acc@5  93.00 ( 92.05)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5684e+00 (1.1891e+00)	Acc@1  67.00 ( 73.10)	Acc@5  93.00 ( 91.84)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 1.1523e+00 (1.1649e+00)	Acc@1  71.00 ( 73.27)	Acc@5  93.00 ( 92.41)
Test: [ 50/100]	Time  0.024 ( 0.020)	Loss 1.4639e+00 (1.1747e+00)	Acc@1  70.00 ( 73.10)	Acc@5  91.00 ( 92.18)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 1.2695e+00 (1.1547e+00)	Acc@1  74.00 ( 73.31)	Acc@5  93.00 ( 92.41)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3682e+00 (1.1459e+00)	Acc@1  67.00 ( 73.41)	Acc@5  92.00 ( 92.54)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.2910e+00 (1.1471e+00)	Acc@1  74.00 ( 73.41)	Acc@5  90.00 ( 92.38)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.5986e+00 (1.1348e+00)	Acc@1  66.00 ( 73.57)	Acc@5  89.00 ( 92.55)
 * Acc@1 73.720 Acc@5 92.630
### epoch[42] execution time: 15.848191022872925
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.197 ( 0.197)	Data  0.156 ( 0.156)	Loss 9.0820e-02 (9.0820e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [43][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.016)	Loss 5.2917e-02 (7.3370e-02)	Acc@1  99.22 ( 98.30)	Acc@5 100.00 (100.00)
Epoch: [43][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.009)	Loss 5.8136e-02 (7.5655e-02)	Acc@1  99.22 ( 98.29)	Acc@5 100.00 (100.00)
Epoch: [43][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.007)	Loss 4.4342e-02 (7.2165e-02)	Acc@1  99.22 ( 98.39)	Acc@5 100.00 (100.00)
Epoch: [43][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 7.3425e-02 (7.1284e-02)	Acc@1  99.22 ( 98.46)	Acc@5 100.00 (100.00)
Epoch: [43][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 8.0627e-02 (7.2094e-02)	Acc@1  97.66 ( 98.50)	Acc@5 100.00 (100.00)
Epoch: [43][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 8.7158e-02 (7.1748e-02)	Acc@1  97.66 ( 98.48)	Acc@5 100.00 (100.00)
Epoch: [43][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.9082e-02 (6.9905e-02)	Acc@1  98.44 ( 98.58)	Acc@5 100.00 (100.00)
Epoch: [43][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.4861e-02 (6.9809e-02)	Acc@1 100.00 ( 98.63)	Acc@5 100.00 (100.00)
Epoch: [43][ 90/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.2124e-02 (7.0403e-02)	Acc@1 100.00 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [43][100/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.8542e-02 (7.0951e-02)	Acc@1  99.22 ( 98.63)	Acc@5 100.00 ( 99.99)
Epoch: [43][110/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4779e-02 (7.0214e-02)	Acc@1  99.22 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.0557e-02 (7.0197e-02)	Acc@1  99.22 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [43][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.4575e-02 (7.0206e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [43][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.2266e-02 (6.9860e-02)	Acc@1  98.44 ( 98.64)	Acc@5 100.00 ( 99.99)
Epoch: [43][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3997e-02 (6.9209e-02)	Acc@1 100.00 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.5806e-02 (6.9204e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0236e-01 (6.9600e-02)	Acc@1  98.44 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.0049e-02 (6.9305e-02)	Acc@1  99.22 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6082e-02 (6.9194e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [43][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.9487e-02 (6.9335e-02)	Acc@1  97.66 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.8491e-02 (6.9625e-02)	Acc@1  98.44 ( 98.65)	Acc@5 100.00 ( 99.99)
Epoch: [43][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.1880e-02 (6.9408e-02)	Acc@1 100.00 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.2205e-02 (6.9241e-02)	Acc@1  97.66 ( 98.66)	Acc@5 100.00 ( 99.99)
Epoch: [43][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.0150e-02 (6.9073e-02)	Acc@1 100.00 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][250/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5571e-02 (6.9152e-02)	Acc@1  97.66 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [43][260/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.7556e-02 (6.9311e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [43][270/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4718e-02 (6.9460e-02)	Acc@1  98.44 ( 98.70)	Acc@5 100.00 ( 99.99)
Epoch: [43][280/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2429e-02 (6.9144e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [43][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9469e-02 (6.9374e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [43][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.2642e-02 (6.9429e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [43][310/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6976e-02 (6.9585e-02)	Acc@1  99.22 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [43][320/391]	Time  0.033 ( 0.035)	Data  0.000 ( 0.002)	Loss 1.1328e-01 (6.9924e-02)	Acc@1  96.09 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [43][330/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7993e-02 (6.9806e-02)	Acc@1  99.22 ( 98.72)	Acc@5 100.00 ( 99.99)
Epoch: [43][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.9895e-02 (6.9724e-02)	Acc@1  98.44 ( 98.73)	Acc@5 100.00 ( 99.99)
Epoch: [43][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4900e-02 (7.0082e-02)	Acc@1  97.66 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [43][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7505e-02 (7.0029e-02)	Acc@1  99.22 ( 98.71)	Acc@5 100.00 ( 99.99)
Epoch: [43][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.9041e-02 (7.0320e-02)	Acc@1  97.66 ( 98.69)	Acc@5 100.00 ( 99.99)
Epoch: [43][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.5215e-02 (7.0578e-02)	Acc@1  96.88 ( 98.68)	Acc@5 100.00 ( 99.99)
Epoch: [43][390/391]	Time  0.026 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1536e-01 (7.0781e-02)	Acc@1  96.25 ( 98.67)	Acc@5 100.00 ( 99.99)
## e[43] optimizer.zero_grad (sum) time: 0.17958950996398926
## e[43]       loss.backward (sum) time: 4.053978443145752
## e[43]      optimizer.step (sum) time: 1.2623748779296875
## epoch[43] training(only) time: 13.914361715316772
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2158e+00 (1.2158e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.022 ( 0.031)	Loss 1.2939e+00 (1.1991e+00)	Acc@1  71.00 ( 73.36)	Acc@5  91.00 ( 91.55)
Test: [ 20/100]	Time  0.022 ( 0.024)	Loss 1.1006e+00 (1.1439e+00)	Acc@1  75.00 ( 74.10)	Acc@5  93.00 ( 92.43)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5947e+00 (1.1901e+00)	Acc@1  67.00 ( 73.23)	Acc@5  92.00 ( 92.23)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1318e+00 (1.1646e+00)	Acc@1  69.00 ( 73.41)	Acc@5  94.00 ( 92.78)
Test: [ 50/100]	Time  0.021 ( 0.021)	Loss 1.4707e+00 (1.1735e+00)	Acc@1  70.00 ( 73.24)	Acc@5  91.00 ( 92.45)
Test: [ 60/100]	Time  0.018 ( 0.020)	Loss 1.2842e+00 (1.1545e+00)	Acc@1  71.00 ( 73.30)	Acc@5  92.00 ( 92.62)
Test: [ 70/100]	Time  0.021 ( 0.020)	Loss 1.3857e+00 (1.1454e+00)	Acc@1  67.00 ( 73.37)	Acc@5  92.00 ( 92.76)
Test: [ 80/100]	Time  0.015 ( 0.020)	Loss 1.3037e+00 (1.1470e+00)	Acc@1  75.00 ( 73.35)	Acc@5  89.00 ( 92.62)
Test: [ 90/100]	Time  0.021 ( 0.019)	Loss 1.6045e+00 (1.1358e+00)	Acc@1  68.00 ( 73.56)	Acc@5  88.00 ( 92.73)
 * Acc@1 73.730 Acc@5 92.800
### epoch[43] execution time: 15.90771198272705
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.190 ( 0.190)	Data  0.149 ( 0.149)	Loss 4.0131e-02 (4.0131e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [44][ 10/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.015)	Loss 8.0078e-02 (6.2203e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.93)
Epoch: [44][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 6.8604e-02 (6.1021e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 ( 99.96)
Epoch: [44][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 9.4543e-02 (6.2877e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 ( 99.97)
Epoch: [44][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.3610e-02 (6.4059e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 9.1370e-02 (6.4891e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 ( 99.98)
Epoch: [44][ 60/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 8.2703e-02 (6.5095e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 70/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.4575e-02 (6.6571e-02)	Acc@1  98.44 ( 98.91)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 1.0284e-01 (6.6711e-02)	Acc@1  96.88 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [44][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.0425e-02 (6.6440e-02)	Acc@1  98.44 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [44][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.7046e-02 (6.6565e-02)	Acc@1  97.66 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [44][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.9224e-02 (6.7710e-02)	Acc@1  97.66 ( 98.82)	Acc@5 100.00 ( 99.99)
Epoch: [44][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.6843e-02 (6.7812e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [44][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.6528e-02 (6.7751e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [44][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.3721e-02 (6.7818e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [44][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8228e-02 (6.7522e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [44][160/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.6660e-02 (6.7159e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [44][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0712e-01 (6.7193e-02)	Acc@1  96.88 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.0017e-02 (6.7526e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [44][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.5500e-02 (6.7210e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [44][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5430e-02 (6.7209e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [44][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5632e-02 (6.7089e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [44][220/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.5012e-02 (6.7595e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][230/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4962e-02 (6.6988e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [44][240/391]	Time  0.043 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4199e-02 (6.7306e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [44][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.0801e-02 (6.6846e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][260/391]	Time  0.046 ( 0.035)	Data  0.003 ( 0.002)	Loss 4.2084e-02 (6.6519e-02)	Acc@1 100.00 ( 98.84)	Acc@5 100.00 (100.00)
Epoch: [44][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0364e-02 (6.7060e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.1787e-02 (6.7255e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [44][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7800e-02 (6.7194e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [44][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1594e-02 (6.7263e-02)	Acc@1  97.66 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [44][310/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7546e-02 (6.7084e-02)	Acc@1 100.00 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [44][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1716e-02 (6.7252e-02)	Acc@1  97.66 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [44][330/391]	Time  0.034 ( 0.035)	Data  0.000 ( 0.002)	Loss 6.3232e-02 (6.7535e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1655e-02 (6.7708e-02)	Acc@1  98.44 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][350/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6040e-02 (6.7716e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [44][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1908e-01 (6.7911e-02)	Acc@1  98.44 ( 98.77)	Acc@5  99.22 (100.00)
Epoch: [44][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4678e-02 (6.8197e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [44][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1594e-02 (6.8038e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 ( 99.99)
Epoch: [44][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2769e-01 (6.8284e-02)	Acc@1  93.75 ( 98.75)	Acc@5 100.00 ( 99.99)
## e[44] optimizer.zero_grad (sum) time: 0.17846345901489258
## e[44]       loss.backward (sum) time: 3.959714889526367
## e[44]      optimizer.step (sum) time: 1.276994228363037
## epoch[44] training(only) time: 13.841668605804443
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.1982e+00 (1.1982e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.018 ( 0.030)	Loss 1.2871e+00 (1.1914e+00)	Acc@1  73.00 ( 73.27)	Acc@5  91.00 ( 91.91)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 1.0869e+00 (1.1396e+00)	Acc@1  75.00 ( 73.62)	Acc@5  93.00 ( 92.57)
Test: [ 30/100]	Time  0.015 ( 0.021)	Loss 1.6045e+00 (1.1840e+00)	Acc@1  63.00 ( 72.81)	Acc@5  92.00 ( 92.13)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1484e+00 (1.1607e+00)	Acc@1  72.00 ( 72.95)	Acc@5  93.00 ( 92.73)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4678e+00 (1.1693e+00)	Acc@1  69.00 ( 72.82)	Acc@5  90.00 ( 92.49)
Test: [ 60/100]	Time  0.025 ( 0.020)	Loss 1.3027e+00 (1.1506e+00)	Acc@1  73.00 ( 72.97)	Acc@5  93.00 ( 92.67)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3701e+00 (1.1428e+00)	Acc@1  70.00 ( 73.07)	Acc@5  92.00 ( 92.72)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.3086e+00 (1.1450e+00)	Acc@1  75.00 ( 73.05)	Acc@5  90.00 ( 92.58)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5762e+00 (1.1341e+00)	Acc@1  67.00 ( 73.27)	Acc@5  89.00 ( 92.70)
 * Acc@1 73.390 Acc@5 92.810
### epoch[44] execution time: 15.808085203170776
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.195 ( 0.195)	Data  0.153 ( 0.153)	Loss 8.4229e-02 (8.4229e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [45][ 10/391]	Time  0.037 ( 0.049)	Data  0.001 ( 0.015)	Loss 3.7506e-02 (6.4545e-02)	Acc@1 100.00 ( 98.72)	Acc@5 100.00 (100.00)
Epoch: [45][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 6.4087e-02 (6.4535e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [45][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.007)	Loss 7.9102e-02 (6.7673e-02)	Acc@1  99.22 ( 98.69)	Acc@5 100.00 (100.00)
Epoch: [45][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 6.4880e-02 (6.5570e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [45][ 50/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.005)	Loss 7.5256e-02 (6.6349e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [45][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.6111e-02 (6.6049e-02)	Acc@1  98.44 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [45][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.7495e-02 (6.5205e-02)	Acc@1 100.00 ( 98.75)	Acc@5 100.00 (100.00)
Epoch: [45][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.5186e-02 (6.6020e-02)	Acc@1  99.22 ( 98.74)	Acc@5 100.00 (100.00)
Epoch: [45][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.8624e-02 (6.6555e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [45][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.8483e-02 (6.6618e-02)	Acc@1  99.22 ( 98.75)	Acc@5 100.00 ( 99.99)
Epoch: [45][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.8796e-02 (6.6370e-02)	Acc@1  99.22 ( 98.77)	Acc@5 100.00 ( 99.99)
Epoch: [45][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.9204e-02 (6.6303e-02)	Acc@1 100.00 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [45][130/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0547e-02 (6.6167e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [45][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.9744e-02 (6.6069e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [45][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.4331e-02 (6.6042e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 ( 99.99)
Epoch: [45][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.7698e-02 (6.5955e-02)	Acc@1  99.22 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.4229e-02 (6.5937e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][180/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.5642e-02 (6.5836e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [45][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6711e-02 (6.5495e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [45][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.5500e-02 (6.6094e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [45][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.0322e-02 (6.5835e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 (100.00)
Epoch: [45][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.0923e-02 (6.6017e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [45][230/391]	Time  0.036 ( 0.036)	Data  0.005 ( 0.002)	Loss 4.4189e-02 (6.6345e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [45][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.3872e-02 (6.6569e-02)	Acc@1  99.22 ( 98.76)	Acc@5 100.00 (100.00)
Epoch: [45][250/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1594e-02 (6.6479e-02)	Acc@1  97.66 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [45][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6936e-02 (6.6393e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.1169e-01 (6.6378e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][280/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7455e-02 (6.6366e-02)	Acc@1  99.22 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [45][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.3547e-02 (6.6399e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][300/391]	Time  0.036 ( 0.035)	Data  0.002 ( 0.002)	Loss 1.2598e-01 (6.6351e-02)	Acc@1  95.31 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [45][310/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3354e-02 (6.6397e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [45][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2805e-02 (6.6811e-02)	Acc@1 100.00 ( 98.77)	Acc@5 100.00 (100.00)
Epoch: [45][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9133e-02 (6.6541e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3049e-02 (6.6774e-02)	Acc@1 100.00 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][350/391]	Time  0.040 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9153e-02 (6.6914e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5928e-02 (6.6829e-02)	Acc@1  98.44 ( 98.79)	Acc@5 100.00 (100.00)
Epoch: [45][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.1614e-02 (6.6902e-02)	Acc@1  97.66 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5928e-02 (6.6952e-02)	Acc@1  98.44 ( 98.78)	Acc@5 100.00 (100.00)
Epoch: [45][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5439e-02 (6.7082e-02)	Acc@1  97.50 ( 98.78)	Acc@5 100.00 (100.00)
## e[45] optimizer.zero_grad (sum) time: 0.17860198020935059
## e[45]       loss.backward (sum) time: 4.063662052154541
## e[45]      optimizer.step (sum) time: 1.2579307556152344
## epoch[45] training(only) time: 13.841058015823364
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.2080e+00 (1.2080e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 1.3252e+00 (1.1948e+00)	Acc@1  72.00 ( 73.00)	Acc@5  91.00 ( 91.82)
Test: [ 20/100]	Time  0.017 ( 0.024)	Loss 1.1064e+00 (1.1412e+00)	Acc@1  75.00 ( 73.81)	Acc@5  93.00 ( 92.48)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5752e+00 (1.1873e+00)	Acc@1  66.00 ( 72.90)	Acc@5  93.00 ( 92.19)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1562e+00 (1.1620e+00)	Acc@1  72.00 ( 73.29)	Acc@5  94.00 ( 92.78)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.4697e+00 (1.1697e+00)	Acc@1  69.00 ( 73.12)	Acc@5  92.00 ( 92.51)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 1.2803e+00 (1.1500e+00)	Acc@1  73.00 ( 73.26)	Acc@5  93.00 ( 92.66)
Test: [ 70/100]	Time  0.020 ( 0.020)	Loss 1.3740e+00 (1.1422e+00)	Acc@1  70.00 ( 73.30)	Acc@5  92.00 ( 92.73)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.2959e+00 (1.1443e+00)	Acc@1  75.00 ( 73.22)	Acc@5  89.00 ( 92.70)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.6006e+00 (1.1330e+00)	Acc@1  67.00 ( 73.43)	Acc@5  93.00 ( 92.88)
 * Acc@1 73.570 Acc@5 92.950
### epoch[45] execution time: 15.802052974700928
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.186 ( 0.186)	Data  0.148 ( 0.148)	Loss 3.9185e-02 (3.9185e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [46][ 10/391]	Time  0.033 ( 0.048)	Data  0.001 ( 0.015)	Loss 7.4890e-02 (7.0099e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [46][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.008)	Loss 6.7444e-02 (7.0190e-02)	Acc@1  99.22 ( 98.81)	Acc@5 100.00 (100.00)
Epoch: [46][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 7.0068e-02 (7.1556e-02)	Acc@1  98.44 ( 98.82)	Acc@5 100.00 ( 99.97)
Epoch: [46][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 6.4453e-02 (6.8228e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.7037e-02 (6.7467e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.98)
Epoch: [46][ 60/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.2571e-02 (6.6989e-02)	Acc@1  98.44 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 70/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 9.2529e-02 (6.7042e-02)	Acc@1  96.88 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.4819e-02 (6.6378e-02)	Acc@1  99.22 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [46][ 90/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.8950e-02 (6.7082e-02)	Acc@1 100.00 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [46][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.5703e-02 (6.6783e-02)	Acc@1  97.66 ( 98.79)	Acc@5 100.00 ( 99.99)
Epoch: [46][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.9346e-02 (6.6551e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [46][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.9072e-02 (6.6511e-02)	Acc@1  98.44 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [46][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.2327e-02 (6.6695e-02)	Acc@1  99.22 ( 98.80)	Acc@5 100.00 ( 99.99)
Epoch: [46][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.1777e-02 (6.6011e-02)	Acc@1  98.44 ( 98.81)	Acc@5 100.00 ( 99.99)
Epoch: [46][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.4036e-02 (6.5126e-02)	Acc@1  97.66 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [46][160/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.3303e-02 (6.5448e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [46][170/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.3364e-02 (6.5488e-02)	Acc@1  99.22 ( 98.82)	Acc@5 100.00 (100.00)
Epoch: [46][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.3162e-02 (6.5247e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 (100.00)
Epoch: [46][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.3679e-02 (6.5116e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 (100.00)
Epoch: [46][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5349e-02 (6.4777e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 (100.00)
Epoch: [46][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.7585e-02 (6.4738e-02)	Acc@1  98.44 ( 98.87)	Acc@5 100.00 (100.00)
Epoch: [46][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.1420e-01 (6.5270e-02)	Acc@1  96.88 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [46][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9800e-02 (6.5017e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 (100.00)
Epoch: [46][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9429e-02 (6.4993e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [46][250/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1676e-02 (6.5163e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [46][260/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7098e-02 (6.5124e-02)	Acc@1  97.66 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [46][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4688e-02 (6.5146e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [46][280/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8319e-02 (6.4869e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [46][290/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3802e-02 (6.5044e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [46][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4199e-02 (6.5051e-02)	Acc@1 100.00 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [46][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6458e-02 (6.4954e-02)	Acc@1  98.44 ( 98.83)	Acc@5 100.00 ( 99.99)
Epoch: [46][320/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.0931e-01 (6.4996e-02)	Acc@1  98.44 ( 98.84)	Acc@5  99.22 ( 99.99)
Epoch: [46][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3497e-02 (6.4692e-02)	Acc@1  99.22 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [46][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2734e-02 (6.4793e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [46][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7770e-02 (6.4572e-02)	Acc@1  99.22 ( 98.86)	Acc@5 100.00 ( 99.99)
Epoch: [46][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2053e-02 (6.4527e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [46][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5500e-02 (6.4519e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [46][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8615e-02 (6.4280e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [46][390/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9185e-02 (6.4431e-02)	Acc@1 100.00 ( 98.87)	Acc@5 100.00 ( 99.99)
## e[46] optimizer.zero_grad (sum) time: 0.17924714088439941
## e[46]       loss.backward (sum) time: 3.9988317489624023
## e[46]      optimizer.step (sum) time: 1.2977657318115234
## epoch[46] training(only) time: 13.818695306777954
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.1885e+00 (1.1885e+00)	Acc@1  70.00 ( 70.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.2744e+00 (1.2117e+00)	Acc@1  72.00 ( 72.91)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.021 ( 0.024)	Loss 1.1143e+00 (1.1536e+00)	Acc@1  74.00 ( 73.81)	Acc@5  92.00 ( 92.33)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.5869e+00 (1.2032e+00)	Acc@1  65.00 ( 72.61)	Acc@5  93.00 ( 92.06)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1680e+00 (1.1782e+00)	Acc@1  70.00 ( 72.78)	Acc@5  94.00 ( 92.71)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.4863e+00 (1.1863e+00)	Acc@1  69.00 ( 72.84)	Acc@5  91.00 ( 92.43)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.2793e+00 (1.1667e+00)	Acc@1  73.00 ( 73.10)	Acc@5  92.00 ( 92.59)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3994e+00 (1.1583e+00)	Acc@1  68.00 ( 73.20)	Acc@5  92.00 ( 92.65)
Test: [ 80/100]	Time  0.022 ( 0.019)	Loss 1.2988e+00 (1.1595e+00)	Acc@1  74.00 ( 73.11)	Acc@5  90.00 ( 92.62)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5840e+00 (1.1482e+00)	Acc@1  68.00 ( 73.33)	Acc@5  92.00 ( 92.76)
 * Acc@1 73.520 Acc@5 92.820
### epoch[46] execution time: 15.77229928970337
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.189 ( 0.189)	Data  0.142 ( 0.142)	Loss 9.4543e-02 (9.4543e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [47][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.014)	Loss 9.4116e-02 (6.3182e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 ( 99.93)
Epoch: [47][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 5.4260e-02 (6.2714e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.96)
Epoch: [47][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.4861e-02 (6.1653e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.97)
Epoch: [47][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 8.9783e-02 (6.2650e-02)	Acc@1  97.66 ( 98.88)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.7170e-02 (6.1043e-02)	Acc@1  99.22 ( 98.93)	Acc@5 100.00 ( 99.98)
Epoch: [47][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.7603e-02 (6.1700e-02)	Acc@1 100.00 ( 98.89)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.2439e-02 (6.2930e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.0222e-02 (6.3210e-02)	Acc@1 100.00 ( 98.85)	Acc@5 100.00 ( 99.99)
Epoch: [47][ 90/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.5370e-02 (6.2671e-02)	Acc@1 100.00 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [47][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8746e-02 (6.2791e-02)	Acc@1  98.44 ( 98.86)	Acc@5 100.00 ( 99.98)
Epoch: [47][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8970e-02 (6.3169e-02)	Acc@1  99.22 ( 98.84)	Acc@5 100.00 ( 99.99)
Epoch: [47][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.8391e-02 (6.2459e-02)	Acc@1  99.22 ( 98.88)	Acc@5 100.00 ( 99.99)
Epoch: [47][130/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.5083e-02 (6.2556e-02)	Acc@1  97.66 ( 98.87)	Acc@5 100.00 ( 99.99)
Epoch: [47][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.6294e-02 (6.2238e-02)	Acc@1  97.66 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [47][150/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8248e-02 (6.2507e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [47][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4749e-02 (6.3135e-02)	Acc@1  99.22 ( 98.90)	Acc@5 100.00 ( 99.99)
Epoch: [47][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2521e-02 (6.2543e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8157e-02 (6.2535e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [47][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0822e-01 (6.2757e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [47][200/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.8563e-02 (6.2990e-02)	Acc@1  99.22 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [47][210/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6163e-02 (6.2896e-02)	Acc@1 100.00 ( 98.92)	Acc@5 100.00 ( 99.99)
Epoch: [47][220/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8615e-02 (6.2717e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [47][230/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4769e-02 (6.2678e-02)	Acc@1 100.00 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [47][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.7444e-02 (6.2330e-02)	Acc@1  98.44 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [47][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.6416e-02 (6.2469e-02)	Acc@1  99.22 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [47][260/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0101e-01 (6.2747e-02)	Acc@1  97.66 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [47][270/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7211e-02 (6.2851e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
Epoch: [47][280/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.5623e-02 (6.2801e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][290/391]	Time  0.042 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.4524e-02 (6.2568e-02)	Acc@1  98.44 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][300/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4636e-02 (6.2574e-02)	Acc@1 100.00 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8370e-02 (6.2405e-02)	Acc@1  99.22 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7576e-02 (6.2115e-02)	Acc@1  98.44 ( 98.95)	Acc@5 100.00 ( 99.99)
Epoch: [47][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2866e-02 (6.2078e-02)	Acc@1 100.00 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [47][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7913e-02 (6.1961e-02)	Acc@1 100.00 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [47][350/391]	Time  0.040 ( 0.035)	Data  0.005 ( 0.002)	Loss 6.6833e-02 (6.1968e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 ( 99.99)
Epoch: [47][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6326e-02 (6.1992e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [47][370/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0924e-02 (6.1907e-02)	Acc@1  99.22 ( 98.96)	Acc@5 100.00 ( 99.99)
Epoch: [47][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2256e-01 (6.2154e-02)	Acc@1  95.31 ( 98.94)	Acc@5 100.00 ( 99.99)
Epoch: [47][390/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2725e-02 (6.2380e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 ( 99.99)
## e[47] optimizer.zero_grad (sum) time: 0.1799619197845459
## e[47]       loss.backward (sum) time: 4.0950915813446045
## e[47]      optimizer.step (sum) time: 1.2575926780700684
## epoch[47] training(only) time: 13.913408994674683
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.2148e+00 (1.2148e+00)	Acc@1  73.00 ( 73.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.016 ( 0.030)	Loss 1.3145e+00 (1.2084e+00)	Acc@1  72.00 ( 73.36)	Acc@5  90.00 ( 91.36)
Test: [ 20/100]	Time  0.021 ( 0.024)	Loss 1.1143e+00 (1.1499e+00)	Acc@1  74.00 ( 74.29)	Acc@5  93.00 ( 92.05)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5713e+00 (1.1966e+00)	Acc@1  66.00 ( 73.42)	Acc@5  90.00 ( 91.55)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1699e+00 (1.1717e+00)	Acc@1  71.00 ( 73.56)	Acc@5  93.00 ( 92.27)
Test: [ 50/100]	Time  0.027 ( 0.020)	Loss 1.4561e+00 (1.1797e+00)	Acc@1  70.00 ( 73.37)	Acc@5  92.00 ( 92.18)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.3105e+00 (1.1601e+00)	Acc@1  71.00 ( 73.54)	Acc@5  92.00 ( 92.33)
Test: [ 70/100]	Time  0.018 ( 0.020)	Loss 1.3408e+00 (1.1502e+00)	Acc@1  71.00 ( 73.69)	Acc@5  92.00 ( 92.41)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.2930e+00 (1.1513e+00)	Acc@1  74.00 ( 73.62)	Acc@5  90.00 ( 92.42)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.6240e+00 (1.1405e+00)	Acc@1  66.00 ( 73.86)	Acc@5  92.00 ( 92.57)
 * Acc@1 74.030 Acc@5 92.710
### epoch[47] execution time: 15.888306140899658
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.196 ( 0.196)	Data  0.157 ( 0.157)	Loss 6.7322e-02 (6.7322e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [48][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.016)	Loss 4.4495e-02 (5.4760e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [48][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.009)	Loss 3.8055e-02 (5.2495e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [48][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.007)	Loss 4.7852e-02 (5.2317e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [48][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 1.0303e-01 (5.5280e-02)	Acc@1  96.88 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [48][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.3335e-02 (5.5299e-02)	Acc@1 100.00 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [48][ 60/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.004)	Loss 7.0312e-02 (5.5874e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.2693e-02 (5.7059e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [48][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.3293e-02 (5.7178e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.8899e-02 (5.8706e-02)	Acc@1  98.44 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [48][100/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.5624e-02 (5.9365e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [48][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6814e-02 (5.9232e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [48][120/391]	Time  0.045 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.9885e-02 (5.9110e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.9631e-02 (6.0061e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [48][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.8910e-02 (5.9556e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.1116e-02 (5.9315e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [48][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9520e-02 (5.9283e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [48][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.7952e-02 (5.9319e-02)	Acc@1  97.66 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.5449e-02 (5.9869e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.9722e-02 (5.9994e-02)	Acc@1  97.66 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4077e-02 (5.9777e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4281e-02 (5.9505e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [48][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6091e-02 (5.9199e-02)	Acc@1  99.22 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [48][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.4158e-02 (5.9039e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.2449e-02 (5.9009e-02)	Acc@1  97.66 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [48][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2338e-02 (5.9223e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [48][260/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4169e-02 (5.9331e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [48][270/391]	Time  0.032 ( 0.035)	Data  0.003 ( 0.002)	Loss 4.5410e-02 (5.9505e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [48][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6692e-02 (5.9354e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][290/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8492e-02 (5.9425e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6783e-02 (5.9364e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [48][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1270e-02 (5.9671e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [48][320/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4717e-02 (5.9877e-02)	Acc@1  97.66 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [48][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8176e-02 (6.0094e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [48][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8542e-02 (6.0161e-02)	Acc@1  99.22 ( 99.00)	Acc@5 100.00 (100.00)
Epoch: [48][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3253e-02 (6.0262e-02)	Acc@1  98.44 ( 98.99)	Acc@5 100.00 (100.00)
Epoch: [48][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7749e-02 (6.0374e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [48][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9021e-02 (6.0415e-02)	Acc@1  98.44 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [48][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1117e-02 (6.0617e-02)	Acc@1  99.22 ( 98.97)	Acc@5 100.00 (100.00)
Epoch: [48][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.5558e-02 (6.0546e-02)	Acc@1 100.00 ( 98.98)	Acc@5 100.00 (100.00)
## e[48] optimizer.zero_grad (sum) time: 0.17736482620239258
## e[48]       loss.backward (sum) time: 4.054812431335449
## e[48]      optimizer.step (sum) time: 1.2703397274017334
## epoch[48] training(only) time: 13.883191108703613
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2051e+00 (1.2051e+00)	Acc@1  70.00 ( 70.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.017 ( 0.030)	Loss 1.2646e+00 (1.1979e+00)	Acc@1  71.00 ( 72.82)	Acc@5  91.00 ( 91.73)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.0986e+00 (1.1444e+00)	Acc@1  75.00 ( 73.81)	Acc@5  93.00 ( 92.33)
Test: [ 30/100]	Time  0.026 ( 0.022)	Loss 1.5889e+00 (1.1924e+00)	Acc@1  64.00 ( 72.81)	Acc@5  92.00 ( 91.97)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1670e+00 (1.1691e+00)	Acc@1  72.00 ( 73.00)	Acc@5  93.00 ( 92.49)
Test: [ 50/100]	Time  0.023 ( 0.021)	Loss 1.4697e+00 (1.1766e+00)	Acc@1  69.00 ( 73.12)	Acc@5  91.00 ( 92.35)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 1.2969e+00 (1.1571e+00)	Acc@1  71.00 ( 73.30)	Acc@5  92.00 ( 92.51)
Test: [ 70/100]	Time  0.024 ( 0.020)	Loss 1.3770e+00 (1.1473e+00)	Acc@1  70.00 ( 73.48)	Acc@5  92.00 ( 92.62)
Test: [ 80/100]	Time  0.020 ( 0.020)	Loss 1.2842e+00 (1.1494e+00)	Acc@1  74.00 ( 73.32)	Acc@5  90.00 ( 92.62)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5908e+00 (1.1385e+00)	Acc@1  68.00 ( 73.56)	Acc@5  91.00 ( 92.77)
 * Acc@1 73.740 Acc@5 92.880
### epoch[48] execution time: 15.891436100006104
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.190 ( 0.190)	Data  0.150 ( 0.150)	Loss 4.1534e-02 (4.1534e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 4.7638e-02 (5.7745e-02)	Acc@1 100.00 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [49][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 4.8706e-02 (5.5793e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [49][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 5.9601e-02 (5.6067e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 7.0251e-02 (5.9885e-02)	Acc@1  96.88 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [49][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.4708e-02 (5.8876e-02)	Acc@1 100.00 ( 99.02)	Acc@5 100.00 (100.00)
Epoch: [49][ 60/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.3375e-02 (5.8789e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [49][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.0838e-02 (5.7505e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [49][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.1168e-02 (5.7388e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8970e-02 (5.8138e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [49][100/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.3975e-02 (5.8575e-02)	Acc@1  96.88 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [49][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.4238e-02 (5.8759e-02)	Acc@1  97.66 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [49][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.2317e-02 (5.8757e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [49][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6753e-02 (5.8742e-02)	Acc@1  99.22 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [49][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1107e-02 (5.8027e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [49][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3793e-02 (5.7383e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [49][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1025e-02 (5.7448e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [49][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.1697e-02 (5.6807e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [49][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.6172e-02 (5.6720e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [49][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9082e-02 (5.6627e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [49][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5491e-02 (5.6643e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [49][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3427e-02 (5.7099e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [49][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3793e-02 (5.7042e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [49][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.4890e-02 (5.7004e-02)	Acc@1  98.44 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [49][240/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9397e-02 (5.6984e-02)	Acc@1  96.88 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [49][250/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.5115e-02 (5.7326e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [49][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7089e-02 (5.7450e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [49][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2786e-02 (5.7452e-02)	Acc@1 100.00 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [49][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5989e-02 (5.7459e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [49][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8269e-02 (5.7419e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [49][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.6599e-02 (5.7489e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [49][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7861e-02 (5.7465e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [49][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.2937e-02 (5.7683e-02)	Acc@1  97.66 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [49][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5632e-02 (5.7939e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [49][340/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3192e-02 (5.7831e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8798e-02 (5.7940e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [49][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.0190e-02 (5.7970e-02)	Acc@1  98.44 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [49][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8929e-02 (5.8231e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [49][380/391]	Time  0.039 ( 0.035)	Data  0.002 ( 0.002)	Loss 5.3894e-02 (5.8343e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [49][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3436e-02 (5.8454e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
## e[49] optimizer.zero_grad (sum) time: 0.18048620223999023
## e[49]       loss.backward (sum) time: 4.060531139373779
## e[49]      optimizer.step (sum) time: 1.25785231590271
## epoch[49] training(only) time: 13.874078035354614
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.2129e+00 (1.2129e+00)	Acc@1  70.00 ( 70.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.024 ( 0.029)	Loss 1.3037e+00 (1.2075e+00)	Acc@1  72.00 ( 73.09)	Acc@5  91.00 ( 91.73)
Test: [ 20/100]	Time  0.015 ( 0.023)	Loss 1.1016e+00 (1.1519e+00)	Acc@1  76.00 ( 73.90)	Acc@5  93.00 ( 92.48)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5977e+00 (1.1961e+00)	Acc@1  65.00 ( 73.10)	Acc@5  91.00 ( 92.16)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1953e+00 (1.1718e+00)	Acc@1  69.00 ( 73.12)	Acc@5  93.00 ( 92.73)
Test: [ 50/100]	Time  0.023 ( 0.020)	Loss 1.4639e+00 (1.1795e+00)	Acc@1  70.00 ( 73.00)	Acc@5  91.00 ( 92.51)
Test: [ 60/100]	Time  0.018 ( 0.020)	Loss 1.3213e+00 (1.1612e+00)	Acc@1  73.00 ( 73.20)	Acc@5  92.00 ( 92.67)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3691e+00 (1.1521e+00)	Acc@1  71.00 ( 73.32)	Acc@5  91.00 ( 92.72)
Test: [ 80/100]	Time  0.014 ( 0.019)	Loss 1.3242e+00 (1.1538e+00)	Acc@1  75.00 ( 73.26)	Acc@5  90.00 ( 92.63)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.5957e+00 (1.1429e+00)	Acc@1  68.00 ( 73.57)	Acc@5  92.00 ( 92.78)
 * Acc@1 73.730 Acc@5 92.870
### epoch[49] execution time: 15.84040093421936
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.205 ( 0.205)	Data  0.159 ( 0.159)	Loss 3.7842e-02 (3.7842e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [50][ 10/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.016)	Loss 4.3945e-02 (4.9896e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [50][ 20/391]	Time  0.033 ( 0.043)	Data  0.001 ( 0.009)	Loss 5.8228e-02 (5.7050e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [50][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.007)	Loss 3.5156e-02 (5.5817e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [50][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 8.4595e-02 (5.8755e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.0100e-02 (5.7553e-02)	Acc@1  99.22 ( 99.13)	Acc@5 100.00 (100.00)
Epoch: [50][ 60/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.6183e-02 (5.9053e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [50][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.9062e-02 (5.8753e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [50][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.1860e-02 (5.8010e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [50][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.5581e-02 (5.9096e-02)	Acc@1  98.44 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [50][100/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.0852e-02 (5.8485e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][110/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1208e-02 (5.8700e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [50][120/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1544e-02 (5.8865e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.3059e-02 (5.8767e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [50][140/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.2103e-02 (5.9058e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [50][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7668e-02 (5.8854e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [50][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7791e-02 (5.8799e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 (100.00)
Epoch: [50][170/391]	Time  0.038 ( 0.036)	Data  0.005 ( 0.003)	Loss 5.6641e-02 (5.9127e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5959e-02 (5.8917e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 (100.00)
Epoch: [50][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5063e-02 (5.8621e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [50][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.7280e-02 (5.8577e-02)	Acc@1  97.66 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [50][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.8685e-02 (5.8381e-02)	Acc@1  99.22 ( 99.06)	Acc@5 100.00 (100.00)
Epoch: [50][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3762e-02 (5.8448e-02)	Acc@1 100.00 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [50][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.1005e-02 (5.8528e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [50][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6467e-02 (5.8466e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [50][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.0394e-02 (5.8367e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 (100.00)
Epoch: [50][260/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4565e-02 (5.8518e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0777e-02 (5.8526e-02)	Acc@1 100.00 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [50][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.0872e-02 (5.8506e-02)	Acc@1  99.22 ( 99.05)	Acc@5 100.00 ( 99.99)
Epoch: [50][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7983e-02 (5.8656e-02)	Acc@1  98.44 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2603e-02 (5.8617e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9021e-02 (5.9141e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [50][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3264e-02 (5.9123e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 ( 99.99)
Epoch: [50][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5563e-02 (5.8729e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3030e-02 (5.8593e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3640e-02 (5.8611e-02)	Acc@1 100.00 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][360/391]	Time  0.033 ( 0.035)	Data  0.000 ( 0.002)	Loss 6.1676e-02 (5.8701e-02)	Acc@1  99.22 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9703e-02 (5.8595e-02)	Acc@1  99.22 ( 99.04)	Acc@5 100.00 ( 99.99)
Epoch: [50][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8289e-02 (5.8597e-02)	Acc@1  97.66 ( 99.03)	Acc@5 100.00 ( 99.99)
Epoch: [50][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 9.2834e-02 (5.8672e-02)	Acc@1  97.50 ( 99.03)	Acc@5 100.00 ( 99.99)
## e[50] optimizer.zero_grad (sum) time: 0.17864489555358887
## e[50]       loss.backward (sum) time: 4.03175163269043
## e[50]      optimizer.step (sum) time: 1.2714474201202393
## epoch[50] training(only) time: 13.890239238739014
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.1709e+00 (1.1709e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 1.2656e+00 (1.1997e+00)	Acc@1  72.00 ( 73.18)	Acc@5  91.00 ( 91.27)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.0840e+00 (1.1498e+00)	Acc@1  75.00 ( 73.86)	Acc@5  93.00 ( 92.19)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5977e+00 (1.2008e+00)	Acc@1  66.00 ( 73.06)	Acc@5  93.00 ( 91.90)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1602e+00 (1.1755e+00)	Acc@1  72.00 ( 73.07)	Acc@5  94.00 ( 92.59)
Test: [ 50/100]	Time  0.028 ( 0.020)	Loss 1.4414e+00 (1.1864e+00)	Acc@1  69.00 ( 72.86)	Acc@5  91.00 ( 92.37)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.2979e+00 (1.1672e+00)	Acc@1  71.00 ( 73.20)	Acc@5  92.00 ( 92.52)
Test: [ 70/100]	Time  0.020 ( 0.020)	Loss 1.3867e+00 (1.1573e+00)	Acc@1  69.00 ( 73.35)	Acc@5  92.00 ( 92.59)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.3184e+00 (1.1583e+00)	Acc@1  74.00 ( 73.35)	Acc@5  90.00 ( 92.60)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5801e+00 (1.1469e+00)	Acc@1  68.00 ( 73.56)	Acc@5  90.00 ( 92.71)
 * Acc@1 73.710 Acc@5 92.780
### epoch[50] execution time: 15.878329753875732
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.188 ( 0.188)	Data  0.147 ( 0.147)	Loss 6.2744e-02 (6.2744e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [51][ 10/391]	Time  0.036 ( 0.049)	Data  0.001 ( 0.015)	Loss 7.3730e-02 (6.2930e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [51][ 20/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.009)	Loss 5.9357e-02 (5.6840e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [51][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.0426e-02 (5.4592e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [51][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 6.6650e-02 (5.3685e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [51][ 50/391]	Time  0.038 ( 0.038)	Data  0.001 ( 0.004)	Loss 8.4412e-02 (5.4726e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [51][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.3192e-02 (5.5438e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [51][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.3669e-02 (5.5181e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [51][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.0903e-02 (5.5550e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [51][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.7607e-02 (5.5431e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [51][100/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.4637e-02 (5.4683e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [51][110/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.4565e-02 (5.4186e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [51][120/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.4495e-02 (5.4038e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [51][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.3253e-02 (5.4102e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [51][140/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.1768e-02 (5.3981e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [51][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0364e-02 (5.4453e-02)	Acc@1  98.44 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [51][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.0132e-01 (5.5183e-02)	Acc@1  96.88 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [51][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3945e-02 (5.5150e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 (100.00)
Epoch: [51][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6835e-02 (5.5545e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0283e-02 (5.5145e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4220e-02 (5.5514e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8981e-02 (5.5962e-02)	Acc@1  98.44 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [51][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.8147e-02 (5.5766e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [51][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.8176e-02 (5.5827e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [51][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6406e-02 (5.6076e-02)	Acc@1  99.22 ( 99.07)	Acc@5 100.00 (100.00)
Epoch: [51][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3457e-02 (5.6113e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [51][260/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6427e-02 (5.5909e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][270/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7852e-02 (5.5903e-02)	Acc@1  99.22 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][280/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5044e-02 (5.5809e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][290/391]	Time  0.035 ( 0.036)	Data  0.002 ( 0.002)	Loss 3.8757e-02 (5.5398e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][300/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9377e-02 (5.5373e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][310/391]	Time  0.041 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6865e-02 (5.5245e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][320/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6316e-02 (5.5242e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [51][330/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.1157e-02 (5.5295e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][340/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.8044e-02 (5.5306e-02)	Acc@1  99.22 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [51][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5858e-02 (5.5277e-02)	Acc@1 100.00 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [51][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9459e-02 (5.5285e-02)	Acc@1 100.00 ( 99.10)	Acc@5 100.00 (100.00)
Epoch: [51][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.5571e-02 (5.5076e-02)	Acc@1  97.66 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2744e-02 (5.5215e-02)	Acc@1 100.00 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [51][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2305e-01 (5.5434e-02)	Acc@1  97.50 ( 99.10)	Acc@5  98.75 (100.00)
## e[51] optimizer.zero_grad (sum) time: 0.17887067794799805
## e[51]       loss.backward (sum) time: 4.060158014297485
## e[51]      optimizer.step (sum) time: 1.262618064880371
## epoch[51] training(only) time: 13.914460182189941
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.1660e+00 (1.1660e+00)	Acc@1  70.00 ( 70.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.016 ( 0.029)	Loss 1.2197e+00 (1.1931e+00)	Acc@1  73.00 ( 73.27)	Acc@5  91.00 ( 91.64)
Test: [ 20/100]	Time  0.018 ( 0.024)	Loss 1.1113e+00 (1.1474e+00)	Acc@1  75.00 ( 73.90)	Acc@5  93.00 ( 92.33)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5713e+00 (1.1937e+00)	Acc@1  70.00 ( 73.39)	Acc@5  93.00 ( 92.00)
Test: [ 40/100]	Time  0.017 ( 0.021)	Loss 1.1396e+00 (1.1696e+00)	Acc@1  70.00 ( 73.49)	Acc@5  93.00 ( 92.59)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.4609e+00 (1.1792e+00)	Acc@1  69.00 ( 73.24)	Acc@5  91.00 ( 92.37)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.2695e+00 (1.1608e+00)	Acc@1  71.00 ( 73.46)	Acc@5  92.00 ( 92.51)
Test: [ 70/100]	Time  0.018 ( 0.020)	Loss 1.3770e+00 (1.1515e+00)	Acc@1  68.00 ( 73.54)	Acc@5  93.00 ( 92.62)
Test: [ 80/100]	Time  0.022 ( 0.019)	Loss 1.3203e+00 (1.1539e+00)	Acc@1  74.00 ( 73.46)	Acc@5  89.00 ( 92.56)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.5479e+00 (1.1426e+00)	Acc@1  67.00 ( 73.62)	Acc@5  92.00 ( 92.71)
 * Acc@1 73.770 Acc@5 92.800
### epoch[51] execution time: 15.89771032333374
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.188 ( 0.188)	Data  0.146 ( 0.146)	Loss 8.3618e-02 (8.3618e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 4.5776e-02 (4.9624e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [52][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.009)	Loss 3.8666e-02 (5.0095e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [52][ 30/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.7974e-02 (5.1682e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [52][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 8.9050e-02 (5.3420e-02)	Acc@1  97.66 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [52][ 50/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.4026e-02 (5.3740e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][ 60/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.7781e-02 (5.2764e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 8.1482e-02 (5.2680e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [52][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.4565e-02 (5.3375e-02)	Acc@1  98.44 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [52][ 90/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.5308e-02 (5.3257e-02)	Acc@1  97.66 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [52][100/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.2336e-02 (5.3835e-02)	Acc@1  97.66 ( 99.17)	Acc@5 100.00 ( 99.99)
Epoch: [52][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.4464e-02 (5.3422e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [52][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.4697e-02 (5.3643e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [52][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.5969e-02 (5.4112e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 ( 99.99)
Epoch: [52][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3630e-02 (5.3651e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 ( 99.99)
Epoch: [52][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1077e-02 (5.3480e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [52][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3518e-02 (5.3358e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][170/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.0293e-02 (5.3194e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [52][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2979e-02 (5.3434e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [52][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5247e-02 (5.3502e-02)	Acc@1  99.22 ( 99.17)	Acc@5  99.22 ( 99.99)
Epoch: [52][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.9641e-02 (5.3862e-02)	Acc@1  98.44 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [52][210/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3610e-02 (5.3787e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [52][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4434e-02 (5.3940e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 ( 99.99)
Epoch: [52][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 9.4971e-02 (5.4667e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [52][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9530e-02 (5.4640e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [52][250/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.0547e-02 (5.4713e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [52][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7759e-02 (5.4928e-02)	Acc@1  96.88 ( 99.11)	Acc@5 100.00 ( 99.99)
Epoch: [52][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8666e-02 (5.4660e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [52][280/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3203e-02 (5.4489e-02)	Acc@1 100.00 ( 99.12)	Acc@5 100.00 ( 99.99)
Epoch: [52][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8716e-02 (5.4346e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [52][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5979e-02 (5.4478e-02)	Acc@1  98.44 ( 99.13)	Acc@5 100.00 ( 99.99)
Epoch: [52][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0018e-02 (5.4531e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 ( 99.99)
Epoch: [52][320/391]	Time  0.040 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2765e-02 (5.4458e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [52][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4026e-02 (5.4508e-02)	Acc@1  98.44 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [52][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9448e-02 (5.4303e-02)	Acc@1  99.22 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2572e-02 (5.4356e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1575e-02 (5.4213e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [52][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7993e-02 (5.4295e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][380/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2399e-02 (5.4300e-02)	Acc@1 100.00 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [52][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9814e-02 (5.4127e-02)	Acc@1 100.00 ( 99.16)	Acc@5 100.00 (100.00)
## e[52] optimizer.zero_grad (sum) time: 0.17875313758850098
## e[52]       loss.backward (sum) time: 4.096763849258423
## e[52]      optimizer.step (sum) time: 1.2410831451416016
## epoch[52] training(only) time: 13.886500358581543
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.1943e+00 (1.1943e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 1.2852e+00 (1.2039e+00)	Acc@1  73.00 ( 73.55)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.1006e+00 (1.1520e+00)	Acc@1  76.00 ( 74.14)	Acc@5  92.00 ( 92.29)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.5605e+00 (1.1975e+00)	Acc@1  67.00 ( 73.26)	Acc@5  93.00 ( 92.03)
Test: [ 40/100]	Time  0.022 ( 0.021)	Loss 1.1455e+00 (1.1737e+00)	Acc@1  71.00 ( 73.37)	Acc@5  93.00 ( 92.61)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.4443e+00 (1.1835e+00)	Acc@1  70.00 ( 73.04)	Acc@5  90.00 ( 92.41)
Test: [ 60/100]	Time  0.021 ( 0.020)	Loss 1.2783e+00 (1.1639e+00)	Acc@1  73.00 ( 73.23)	Acc@5  93.00 ( 92.56)
Test: [ 70/100]	Time  0.020 ( 0.020)	Loss 1.3486e+00 (1.1556e+00)	Acc@1  69.00 ( 73.28)	Acc@5  92.00 ( 92.61)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.3203e+00 (1.1568e+00)	Acc@1  74.00 ( 73.27)	Acc@5  89.00 ( 92.56)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.5938e+00 (1.1458e+00)	Acc@1  66.00 ( 73.41)	Acc@5  92.00 ( 92.70)
 * Acc@1 73.640 Acc@5 92.800
### epoch[52] execution time: 15.85465383529663
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.179 ( 0.179)	Data  0.140 ( 0.140)	Loss 7.3853e-02 (7.3853e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.014)	Loss 5.9265e-02 (5.3117e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [53][ 20/391]	Time  0.036 ( 0.043)	Data  0.001 ( 0.008)	Loss 6.2134e-02 (5.3269e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [53][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 8.0200e-02 (5.1419e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [53][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.3213e-02 (5.2060e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [53][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.2166e-02 (5.1664e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [53][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.6122e-02 (5.2744e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [53][ 70/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.8258e-02 (5.2205e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [53][ 80/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.1575e-02 (5.2024e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [53][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.0446e-02 (5.1233e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [53][100/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 8.3435e-02 (5.1630e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [53][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8848e-02 (5.1765e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [53][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.8809e-02 (5.2521e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [53][130/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.003)	Loss 3.7933e-02 (5.2193e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [53][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6661e-02 (5.2245e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [53][150/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.2285e-02 (5.2472e-02)	Acc@1  96.88 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [53][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6885e-02 (5.2495e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [53][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9052e-02 (5.2574e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [53][180/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.7698e-02 (5.3241e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [53][190/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.9946e-02 (5.3370e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.5969e-02 (5.3216e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6589e-02 (5.2979e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][220/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.0455e-02 (5.3236e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][230/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6000e-02 (5.3449e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.8544e-02 (5.3134e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][250/391]	Time  0.040 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.2042e-02 (5.3109e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][260/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6528e-02 (5.3345e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][270/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2877e-02 (5.3297e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [53][280/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1260e-02 (5.2981e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][290/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2368e-02 (5.3113e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][300/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.0944e-02 (5.2991e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][310/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.7678e-02 (5.3118e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][320/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7018e-02 (5.3027e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [53][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3396e-02 (5.2961e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2856e-02 (5.2912e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6997e-02 (5.2862e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [53][360/391]	Time  0.039 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.9346e-02 (5.3297e-02)	Acc@1  98.44 ( 99.19)	Acc@5  99.22 ( 99.99)
Epoch: [53][370/391]	Time  0.040 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6946e-02 (5.3198e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [53][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8044e-02 (5.3178e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
Epoch: [53][390/391]	Time  0.025 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3721e-02 (5.3324e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.99)
## e[53] optimizer.zero_grad (sum) time: 0.17747211456298828
## e[53]       loss.backward (sum) time: 3.94279146194458
## e[53]      optimizer.step (sum) time: 1.266378402709961
## epoch[53] training(only) time: 13.938075304031372
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2148e+00 (1.2148e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.016 ( 0.030)	Loss 1.2617e+00 (1.2068e+00)	Acc@1  72.00 ( 73.45)	Acc@5  91.00 ( 91.55)
Test: [ 20/100]	Time  0.014 ( 0.024)	Loss 1.0889e+00 (1.1494e+00)	Acc@1  75.00 ( 74.14)	Acc@5  92.00 ( 92.19)
Test: [ 30/100]	Time  0.023 ( 0.023)	Loss 1.5908e+00 (1.1958e+00)	Acc@1  65.00 ( 73.42)	Acc@5  93.00 ( 91.87)
Test: [ 40/100]	Time  0.021 ( 0.021)	Loss 1.1641e+00 (1.1721e+00)	Acc@1  70.00 ( 73.49)	Acc@5  93.00 ( 92.54)
Test: [ 50/100]	Time  0.022 ( 0.021)	Loss 1.4521e+00 (1.1826e+00)	Acc@1  71.00 ( 73.35)	Acc@5  91.00 ( 92.31)
Test: [ 60/100]	Time  0.019 ( 0.020)	Loss 1.3184e+00 (1.1648e+00)	Acc@1  72.00 ( 73.52)	Acc@5  93.00 ( 92.48)
Test: [ 70/100]	Time  0.014 ( 0.020)	Loss 1.3975e+00 (1.1576e+00)	Acc@1  68.00 ( 73.55)	Acc@5  92.00 ( 92.55)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.3164e+00 (1.1598e+00)	Acc@1  74.00 ( 73.46)	Acc@5  89.00 ( 92.46)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.5625e+00 (1.1485e+00)	Acc@1  68.00 ( 73.65)	Acc@5  90.00 ( 92.56)
 * Acc@1 73.850 Acc@5 92.660
### epoch[53] execution time: 15.91692566871643
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.193 ( 0.193)	Data  0.152 ( 0.152)	Loss 3.8422e-02 (3.8422e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 5.5664e-02 (4.5635e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [54][ 20/391]	Time  0.033 ( 0.043)	Data  0.001 ( 0.009)	Loss 6.6223e-02 (4.8026e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [54][ 30/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.007)	Loss 6.0791e-02 (4.8478e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [54][ 40/391]	Time  0.029 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.2114e-02 (4.9640e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 7.3181e-02 (5.0259e-02)	Acc@1  97.66 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [54][ 60/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.4576e-02 (5.0013e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [54][ 70/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.5461e-02 (4.9440e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [54][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.3762e-02 (4.9615e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [54][ 90/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.0121e-02 (5.0252e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][100/391]	Time  0.039 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.9988e-02 (5.0966e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [54][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.4524e-02 (5.1075e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.5034e-02 (5.1129e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [54][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.1777e-02 (5.0833e-02)	Acc@1  98.44 ( 99.26)	Acc@5  99.22 ( 99.99)
Epoch: [54][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2847e-02 (5.1040e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [54][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.8247e-02 (5.1124e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 ( 99.99)
Epoch: [54][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.7322e-02 (5.1122e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [54][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.0502e-02 (5.1163e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4983e-02 (5.0954e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [54][190/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9368e-02 (5.1291e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6213e-02 (5.1104e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2572e-02 (5.0985e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.5684e-02 (5.1206e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.0852e-02 (5.1086e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3722e-02 (5.0829e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][250/391]	Time  0.035 ( 0.036)	Data  0.000 ( 0.002)	Loss 4.4830e-02 (5.1016e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][260/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9988e-02 (5.0992e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][270/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.3070e-02 (5.1010e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [54][280/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3508e-02 (5.1281e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [54][290/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 8.4412e-02 (5.1739e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [54][300/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8950e-02 (5.1838e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [54][310/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.3131e-02 (5.1733e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [54][320/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6621e-02 (5.2032e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [54][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.3984e-02 (5.2391e-02)	Acc@1  96.88 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2867e-02 (5.2045e-02)	Acc@1 100.00 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [54][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8879e-02 (5.2054e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [54][360/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8645e-02 (5.2087e-02)	Acc@1  98.44 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [54][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3965e-02 (5.2221e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [54][380/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0873e-02 (5.2263e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [54][390/391]	Time  0.025 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.6843e-02 (5.2533e-02)	Acc@1  96.25 ( 99.18)	Acc@5 100.00 (100.00)
## e[54] optimizer.zero_grad (sum) time: 0.17984485626220703
## e[54]       loss.backward (sum) time: 4.1241984367370605
## e[54]      optimizer.step (sum) time: 1.2460074424743652
## epoch[54] training(only) time: 13.917838335037231
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.2100e+00 (1.2100e+00)	Acc@1  71.00 ( 71.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.016 ( 0.030)	Loss 1.2559e+00 (1.2001e+00)	Acc@1  74.00 ( 73.09)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.0928e+00 (1.1423e+00)	Acc@1  74.00 ( 73.95)	Acc@5  92.00 ( 92.19)
Test: [ 30/100]	Time  0.022 ( 0.022)	Loss 1.6377e+00 (1.1935e+00)	Acc@1  63.00 ( 73.06)	Acc@5  92.00 ( 91.90)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1436e+00 (1.1727e+00)	Acc@1  73.00 ( 73.17)	Acc@5  93.00 ( 92.54)
Test: [ 50/100]	Time  0.017 ( 0.021)	Loss 1.4775e+00 (1.1818e+00)	Acc@1  69.00 ( 72.96)	Acc@5  90.00 ( 92.31)
Test: [ 60/100]	Time  0.018 ( 0.020)	Loss 1.3105e+00 (1.1640e+00)	Acc@1  72.00 ( 73.21)	Acc@5  93.00 ( 92.46)
Test: [ 70/100]	Time  0.026 ( 0.020)	Loss 1.3701e+00 (1.1559e+00)	Acc@1  66.00 ( 73.25)	Acc@5  93.00 ( 92.51)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.2744e+00 (1.1564e+00)	Acc@1  74.00 ( 73.22)	Acc@5  90.00 ( 92.48)
Test: [ 90/100]	Time  0.037 ( 0.019)	Loss 1.6064e+00 (1.1456e+00)	Acc@1  67.00 ( 73.43)	Acc@5  90.00 ( 92.67)
 * Acc@1 73.650 Acc@5 92.770
### epoch[54] execution time: 15.909284830093384
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.191 ( 0.191)	Data  0.149 ( 0.149)	Loss 2.9907e-02 (2.9907e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 10/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.015)	Loss 4.8676e-02 (5.4940e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [55][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 4.1260e-02 (5.1041e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [55][ 30/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.0314e-02 (4.9877e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [55][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.0131e-02 (4.9102e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 8.2397e-02 (4.9000e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][ 60/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.3406e-02 (4.8749e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [55][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.5573e-02 (4.8846e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.8359e-02 (4.9114e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.5084e-02 (4.9077e-02)	Acc@1  98.44 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][100/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.4077e-02 (4.9469e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [55][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.3293e-02 (4.9632e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [55][120/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7079e-02 (4.9414e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][130/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3243e-02 (4.9631e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4199e-02 (4.9948e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [55][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6499e-02 (4.9870e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.1798e-02 (5.0091e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7201e-02 (5.0311e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4312e-02 (5.0194e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2002e-02 (4.9871e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7394e-02 (4.9901e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4941e-02 (4.9994e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [55][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0253e-02 (4.9975e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9438e-02 (5.0006e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][240/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.5267e-02 (5.0011e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][250/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1107e-02 (5.0037e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][260/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6793e-02 (4.9997e-02)	Acc@1  97.66 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4819e-02 (5.0190e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][280/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2521e-02 (5.0057e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [55][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8157e-02 (4.9918e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][300/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3783e-02 (4.9937e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6840e-02 (4.9760e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [55][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9519e-02 (5.0044e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7343e-02 (5.0083e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [55][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3599e-02 (5.0093e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][350/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7211e-02 (5.0223e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7709e-02 (5.0222e-02)	Acc@1  97.66 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [55][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8126e-02 (5.0337e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1890e-02 (5.0240e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [55][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 1.2988e-01 (5.0688e-02)	Acc@1  96.25 ( 99.25)	Acc@5 100.00 (100.00)
## e[55] optimizer.zero_grad (sum) time: 0.18018865585327148
## e[55]       loss.backward (sum) time: 4.068965911865234
## e[55]      optimizer.step (sum) time: 1.2667970657348633
## epoch[55] training(only) time: 13.890344619750977
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.1924e+00 (1.1924e+00)	Acc@1  70.00 ( 70.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.020 ( 0.029)	Loss 1.2773e+00 (1.2102e+00)	Acc@1  73.00 ( 74.09)	Acc@5  91.00 ( 91.27)
Test: [ 20/100]	Time  0.015 ( 0.023)	Loss 1.1025e+00 (1.1495e+00)	Acc@1  74.00 ( 74.67)	Acc@5  93.00 ( 92.29)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.5820e+00 (1.1955e+00)	Acc@1  67.00 ( 73.90)	Acc@5  89.00 ( 91.90)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 1.1680e+00 (1.1701e+00)	Acc@1  69.00 ( 73.85)	Acc@5  93.00 ( 92.56)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.5156e+00 (1.1802e+00)	Acc@1  68.00 ( 73.43)	Acc@5  90.00 ( 92.29)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 1.2891e+00 (1.1628e+00)	Acc@1  71.00 ( 73.57)	Acc@5  93.00 ( 92.48)
Test: [ 70/100]	Time  0.014 ( 0.020)	Loss 1.4160e+00 (1.1559e+00)	Acc@1  69.00 ( 73.54)	Acc@5  91.00 ( 92.52)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.3193e+00 (1.1582e+00)	Acc@1  73.00 ( 73.44)	Acc@5  90.00 ( 92.48)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5488e+00 (1.1472e+00)	Acc@1  68.00 ( 73.66)	Acc@5  92.00 ( 92.65)
 * Acc@1 73.820 Acc@5 92.760
### epoch[55] execution time: 15.871208429336548
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.184 ( 0.184)	Data  0.143 ( 0.143)	Loss 7.2205e-02 (7.2205e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 4.3152e-02 (5.1851e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.008)	Loss 4.4586e-02 (5.1506e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [56][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.4403e-02 (5.0047e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [56][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 6.6040e-02 (5.3180e-02)	Acc@1  98.44 ( 99.09)	Acc@5 100.00 (100.00)
Epoch: [56][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.3121e-02 (5.1610e-02)	Acc@1  99.22 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [56][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.4119e-02 (5.0305e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [56][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.4697e-02 (5.0547e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [56][ 80/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0171e-02 (5.0417e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [56][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.7158e-02 (5.0618e-02)	Acc@1  97.66 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [56][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5044e-02 (4.9992e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [56][110/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9215e-02 (4.9445e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [56][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6682e-02 (4.9238e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [56][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.2622e-02 (4.9866e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [56][140/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.7495e-02 (4.9677e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [56][150/391]	Time  0.047 ( 0.036)	Data  0.000 ( 0.002)	Loss 6.6467e-02 (4.9425e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [56][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6051e-02 (4.9353e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [56][170/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4138e-02 (4.9435e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [56][180/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6224e-02 (4.9611e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [56][190/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8838e-02 (4.9979e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [56][200/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1544e-02 (5.0152e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [56][210/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2562e-02 (5.0111e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [56][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.8369e-02 (5.0449e-02)	Acc@1  97.66 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [56][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5898e-02 (5.0234e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [56][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4209e-02 (5.0367e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [56][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1472e-02 (5.0471e-02)	Acc@1  98.44 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [56][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9642e-02 (5.0440e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [56][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8828e-02 (5.0408e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [56][280/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3243e-02 (5.0473e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [56][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5369e-02 (5.0514e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [56][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9235e-02 (5.0681e-02)	Acc@1  98.44 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6772e-02 (5.0854e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6967e-02 (5.0855e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3538e-02 (5.0838e-02)	Acc@1  97.66 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6661e-02 (5.0897e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [56][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1096e-02 (5.0967e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [56][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7603e-02 (5.0913e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6957e-02 (5.0873e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [56][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0720e-02 (5.1086e-02)	Acc@1  98.44 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [56][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3823e-02 (5.0922e-02)	Acc@1  98.75 ( 99.21)	Acc@5 100.00 (100.00)
## e[56] optimizer.zero_grad (sum) time: 0.1791703701019287
## e[56]       loss.backward (sum) time: 3.979647636413574
## e[56]      optimizer.step (sum) time: 1.2658185958862305
## epoch[56] training(only) time: 13.800104141235352
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2061e+00 (1.2061e+00)	Acc@1  71.00 ( 71.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 1.2432e+00 (1.2014e+00)	Acc@1  73.00 ( 73.55)	Acc@5  91.00 ( 91.55)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.0928e+00 (1.1435e+00)	Acc@1  74.00 ( 74.43)	Acc@5  93.00 ( 92.29)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5713e+00 (1.1905e+00)	Acc@1  63.00 ( 73.42)	Acc@5  92.00 ( 92.00)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 1.1660e+00 (1.1693e+00)	Acc@1  69.00 ( 73.46)	Acc@5  93.00 ( 92.59)
Test: [ 50/100]	Time  0.020 ( 0.020)	Loss 1.4805e+00 (1.1797e+00)	Acc@1  70.00 ( 73.29)	Acc@5  90.00 ( 92.27)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.2969e+00 (1.1625e+00)	Acc@1  72.00 ( 73.61)	Acc@5  92.00 ( 92.41)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3828e+00 (1.1551e+00)	Acc@1  67.00 ( 73.61)	Acc@5  93.00 ( 92.51)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.3115e+00 (1.1560e+00)	Acc@1  74.00 ( 73.51)	Acc@5  90.00 ( 92.47)
Test: [ 90/100]	Time  0.024 ( 0.019)	Loss 1.5762e+00 (1.1459e+00)	Acc@1  68.00 ( 73.69)	Acc@5  91.00 ( 92.66)
 * Acc@1 73.860 Acc@5 92.740
### epoch[56] execution time: 15.763965606689453
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.202 ( 0.202)	Data  0.160 ( 0.160)	Loss 5.1575e-02 (5.1575e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.033 ( 0.050)	Data  0.001 ( 0.016)	Loss 8.1238e-02 (5.0823e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [57][ 20/391]	Time  0.033 ( 0.043)	Data  0.001 ( 0.009)	Loss 3.4821e-02 (4.9244e-02)	Acc@1 100.00 ( 99.14)	Acc@5 100.00 (100.00)
Epoch: [57][ 30/391]	Time  0.037 ( 0.041)	Data  0.001 ( 0.007)	Loss 3.1982e-02 (4.9630e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [57][ 40/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 3.5797e-02 (4.8999e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [57][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 6.0028e-02 (4.9326e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][ 60/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.6406e-02 (4.9294e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [57][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.1656e-02 (4.8452e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.4636e-02 (4.8780e-02)	Acc@1  99.22 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [57][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.5553e-02 (4.8520e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [57][100/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.2021e-02 (4.8778e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [57][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.6305e-02 (4.9267e-02)	Acc@1  98.44 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [57][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.8196e-02 (4.9348e-02)	Acc@1  98.44 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [57][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.3547e-02 (4.8971e-02)	Acc@1  97.66 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [57][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.8483e-02 (4.8563e-02)	Acc@1 100.00 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [57][150/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2948e-02 (4.8617e-02)	Acc@1  99.22 ( 99.20)	Acc@5 100.00 (100.00)
Epoch: [57][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.6642e-02 (4.8417e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [57][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.2104e-02 (4.8327e-02)	Acc@1 100.00 ( 99.21)	Acc@5 100.00 (100.00)
Epoch: [57][180/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.9519e-02 (4.8285e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [57][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4026e-02 (4.8522e-02)	Acc@1  99.22 ( 99.23)	Acc@5 100.00 (100.00)
Epoch: [57][200/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.4973e-02 (4.8073e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [57][210/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9581e-02 (4.7957e-02)	Acc@1  97.66 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5063e-02 (4.7986e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4037e-02 (4.8019e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9204e-02 (4.7988e-02)	Acc@1  97.66 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [57][250/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9500e-02 (4.8105e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [57][260/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.4180e-02 (4.8310e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [57][270/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9764e-02 (4.8219e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [57][280/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2917e-02 (4.8363e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [57][290/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5065e-02 (4.8315e-02)	Acc@1 100.00 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [57][300/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.4485e-02 (4.8241e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [57][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5471e-02 (4.8184e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9438e-02 (4.7989e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0211e-02 (4.8149e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3051e-02 (4.8120e-02)	Acc@1 100.00 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8065e-02 (4.8361e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9479e-02 (4.8473e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][370/391]	Time  0.036 ( 0.035)	Data  0.002 ( 0.002)	Loss 5.8990e-02 (4.8438e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][380/391]	Time  0.043 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2664e-02 (4.8552e-02)	Acc@1  99.22 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [57][390/391]	Time  0.026 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6345e-02 (4.8293e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
## e[57] optimizer.zero_grad (sum) time: 0.18102741241455078
## e[57]       loss.backward (sum) time: 4.0531415939331055
## e[57]      optimizer.step (sum) time: 1.2397398948669434
## epoch[57] training(only) time: 13.934661626815796
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.1973e+00 (1.1973e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.021 ( 0.030)	Loss 1.2754e+00 (1.2027e+00)	Acc@1  72.00 ( 73.36)	Acc@5  91.00 ( 91.64)
Test: [ 20/100]	Time  0.019 ( 0.024)	Loss 1.0908e+00 (1.1452e+00)	Acc@1  75.00 ( 74.05)	Acc@5  93.00 ( 92.52)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5869e+00 (1.1935e+00)	Acc@1  65.00 ( 73.10)	Acc@5  92.00 ( 92.19)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1631e+00 (1.1718e+00)	Acc@1  71.00 ( 73.05)	Acc@5  93.00 ( 92.73)
Test: [ 50/100]	Time  0.018 ( 0.020)	Loss 1.5000e+00 (1.1836e+00)	Acc@1  70.00 ( 72.80)	Acc@5  90.00 ( 92.41)
Test: [ 60/100]	Time  0.014 ( 0.020)	Loss 1.2822e+00 (1.1647e+00)	Acc@1  72.00 ( 73.05)	Acc@5  92.00 ( 92.51)
Test: [ 70/100]	Time  0.014 ( 0.020)	Loss 1.4121e+00 (1.1574e+00)	Acc@1  69.00 ( 73.21)	Acc@5  92.00 ( 92.61)
Test: [ 80/100]	Time  0.018 ( 0.019)	Loss 1.3184e+00 (1.1586e+00)	Acc@1  73.00 ( 73.22)	Acc@5  90.00 ( 92.56)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.5850e+00 (1.1479e+00)	Acc@1  68.00 ( 73.46)	Acc@5  91.00 ( 92.74)
 * Acc@1 73.650 Acc@5 92.790
### epoch[57] execution time: 15.907684564590454
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.181 ( 0.181)	Data  0.137 ( 0.137)	Loss 4.4830e-02 (4.4830e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [58][ 10/391]	Time  0.034 ( 0.048)	Data  0.001 ( 0.014)	Loss 4.1962e-02 (4.4378e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [58][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.008)	Loss 5.9875e-02 (4.6926e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [58][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.006)	Loss 4.1229e-02 (4.8933e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [58][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 3.6438e-02 (4.7282e-02)	Acc@1  99.22 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.9490e-02 (4.7393e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [58][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.7689e-02 (4.7852e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [58][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.1605e-02 (4.8864e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [58][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.1655e-02 (4.8874e-02)	Acc@1  97.66 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [58][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.8137e-02 (4.9044e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][100/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.2018e-02 (4.8390e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [58][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2948e-02 (4.8492e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [58][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4535e-02 (4.8451e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7567e-02 (4.8341e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [58][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.8157e-02 (4.8027e-02)	Acc@1  98.44 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [58][150/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3488e-02 (4.7427e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [58][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6772e-02 (4.7421e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [58][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4281e-02 (4.7227e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.7678e-02 (4.7167e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.7881e-02 (4.7306e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2979e-02 (4.7207e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [58][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6509e-02 (4.7298e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6783e-02 (4.6986e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][230/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0720e-02 (4.7122e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][240/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9347e-02 (4.7232e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [58][250/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5105e-02 (4.7566e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [58][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4973e-02 (4.7396e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [58][270/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4760e-02 (4.7337e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2053e-02 (4.7442e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][290/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5675e-02 (4.7413e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [58][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7659e-02 (4.7383e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [58][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8187e-02 (4.7696e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [58][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0171e-02 (4.7852e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [58][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8849e-02 (4.7778e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [58][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2196e-02 (4.7660e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [58][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7363e-02 (4.7553e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [58][360/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5950e-02 (4.7411e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [58][370/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0303e-02 (4.7307e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5674e-02 (4.7225e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [58][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5439e-02 (4.7202e-02)	Acc@1  98.75 ( 99.36)	Acc@5 100.00 (100.00)
## e[58] optimizer.zero_grad (sum) time: 0.1798717975616455
## e[58]       loss.backward (sum) time: 4.0164268016815186
## e[58]      optimizer.step (sum) time: 1.2715072631835938
## epoch[58] training(only) time: 13.818488359451294
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.2109e+00 (1.2109e+00)	Acc@1  70.00 ( 70.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.2578e+00 (1.2108e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.82)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.1064e+00 (1.1497e+00)	Acc@1  75.00 ( 74.10)	Acc@5  93.00 ( 92.52)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.5957e+00 (1.1978e+00)	Acc@1  65.00 ( 73.16)	Acc@5  92.00 ( 92.23)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1445e+00 (1.1731e+00)	Acc@1  71.00 ( 73.12)	Acc@5  93.00 ( 92.76)
Test: [ 50/100]	Time  0.017 ( 0.021)	Loss 1.4756e+00 (1.1863e+00)	Acc@1  71.00 ( 72.96)	Acc@5  91.00 ( 92.49)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.3135e+00 (1.1684e+00)	Acc@1  69.00 ( 73.18)	Acc@5  91.00 ( 92.59)
Test: [ 70/100]	Time  0.015 ( 0.020)	Loss 1.4082e+00 (1.1622e+00)	Acc@1  70.00 ( 73.31)	Acc@5  92.00 ( 92.63)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.3320e+00 (1.1630e+00)	Acc@1  73.00 ( 73.31)	Acc@5  89.00 ( 92.53)
Test: [ 90/100]	Time  0.023 ( 0.019)	Loss 1.6211e+00 (1.1516e+00)	Acc@1  68.00 ( 73.57)	Acc@5  90.00 ( 92.68)
 * Acc@1 73.760 Acc@5 92.750
### epoch[58] execution time: 15.807903051376343
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.189 ( 0.189)	Data  0.148 ( 0.148)	Loss 1.0956e-01 (1.0956e-01)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [59][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 5.9448e-02 (5.4073e-02)	Acc@1  98.44 ( 98.93)	Acc@5 100.00 (100.00)
Epoch: [59][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.009)	Loss 2.9907e-02 (4.8338e-02)	Acc@1  99.22 ( 99.11)	Acc@5 100.00 (100.00)
Epoch: [59][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.0619e-02 (4.6974e-02)	Acc@1  99.22 ( 99.19)	Acc@5 100.00 (100.00)
Epoch: [59][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.0262e-02 (4.8814e-02)	Acc@1 100.00 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [59][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.6021e-02 (4.7022e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [59][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.2104e-02 (4.6231e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][ 70/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.5471e-02 (4.6649e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [59][ 80/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.7932e-02 (4.6459e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3762e-02 (4.6169e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [59][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1544e-02 (4.6237e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [59][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2297e-02 (4.6516e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0730e-02 (4.6887e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [59][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8960e-02 (4.7041e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [59][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.1423e-02 (4.6952e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0120e-02 (4.7108e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [59][160/391]	Time  0.034 ( 0.035)	Data  0.000 ( 0.003)	Loss 3.6682e-02 (4.6722e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [59][170/391]	Time  0.035 ( 0.035)	Data  0.002 ( 0.003)	Loss 3.4760e-02 (4.6737e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [59][180/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9886e-02 (4.6477e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [59][190/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7872e-02 (4.6709e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [59][200/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2227e-02 (4.6703e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [59][210/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4525e-02 (4.6795e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [59][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9282e-02 (4.6879e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [59][230/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8218e-02 (4.7233e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [59][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5603e-02 (4.7629e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][250/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3680e-02 (4.7444e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [59][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4189e-02 (4.7506e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [59][270/391]	Time  0.036 ( 0.035)	Data  0.002 ( 0.002)	Loss 6.9641e-02 (4.7691e-02)	Acc@1  97.66 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9082e-02 (4.7741e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][290/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4047e-02 (4.7697e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][300/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1616e-02 (4.7625e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6926e-02 (4.7648e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][320/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1403e-02 (4.7687e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][330/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9480e-02 (4.7628e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7079e-02 (4.7511e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [59][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8187e-02 (4.7607e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [59][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.6233e-02 (4.7790e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [59][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0283e-02 (4.7874e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [59][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.8503e-02 (4.7850e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [59][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0472e-02 (4.7759e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
## e[59] optimizer.zero_grad (sum) time: 0.1787550449371338
## e[59]       loss.backward (sum) time: 3.888248920440674
## e[59]      optimizer.step (sum) time: 1.289867639541626
## epoch[59] training(only) time: 13.843390703201294
# Switched to evaluate mode...
Test: [  0/100]	Time  0.153 ( 0.153)	Loss 1.1748e+00 (1.1748e+00)	Acc@1  72.00 ( 72.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.023 ( 0.031)	Loss 1.2783e+00 (1.1951e+00)	Acc@1  72.00 ( 73.00)	Acc@5  91.00 ( 91.36)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.1348e+00 (1.1430e+00)	Acc@1  74.00 ( 73.76)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5869e+00 (1.1906e+00)	Acc@1  66.00 ( 73.19)	Acc@5  93.00 ( 91.94)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1465e+00 (1.1690e+00)	Acc@1  71.00 ( 73.34)	Acc@5  94.00 ( 92.54)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.5059e+00 (1.1812e+00)	Acc@1  70.00 ( 73.22)	Acc@5  91.00 ( 92.33)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 1.2832e+00 (1.1640e+00)	Acc@1  72.00 ( 73.43)	Acc@5  92.00 ( 92.48)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3662e+00 (1.1559e+00)	Acc@1  69.00 ( 73.45)	Acc@5  92.00 ( 92.52)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.3154e+00 (1.1563e+00)	Acc@1  73.00 ( 73.47)	Acc@5  90.00 ( 92.51)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5977e+00 (1.1453e+00)	Acc@1  66.00 ( 73.64)	Acc@5  90.00 ( 92.65)
 * Acc@1 73.850 Acc@5 92.730
### epoch[59] execution time: 15.816094636917114
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.191 ( 0.191)	Data  0.145 ( 0.145)	Loss 3.9703e-02 (3.9703e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [60][ 10/391]	Time  0.035 ( 0.049)	Data  0.001 ( 0.015)	Loss 2.5436e-02 (4.5848e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 6.9214e-02 (4.6782e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [60][ 30/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.006)	Loss 2.4979e-02 (4.4172e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [60][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 5.8655e-02 (4.6018e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.4891e-02 (4.6637e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.6255e-02 (4.6871e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.8960e-02 (4.6973e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [60][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.6244e-02 (4.7630e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [60][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6814e-02 (4.7691e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [60][100/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8970e-02 (4.7358e-02)	Acc@1  98.44 ( 99.25)	Acc@5 100.00 (100.00)
Epoch: [60][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7140e-02 (4.6939e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 (100.00)
Epoch: [60][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1372e-02 (4.6423e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [60][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4393e-02 (4.6170e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [60][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.6489e-02 (4.6341e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [60][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.1057e-02 (4.6223e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [60][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1718e-02 (4.6367e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [60][170/391]	Time  0.041 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5654e-02 (4.6501e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [60][180/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6560e-02 (4.6338e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [60][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6783e-02 (4.6288e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [60][200/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.2043e-02 (4.6024e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][210/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5197e-02 (4.5626e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][220/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6793e-02 (4.5638e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [60][230/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1473e-02 (4.5943e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [60][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2358e-02 (4.6191e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [60][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3926e-02 (4.5929e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6499e-02 (4.5975e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8147e-02 (4.5905e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5044e-02 (4.5961e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [60][290/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5767e-02 (4.5770e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4077e-02 (4.5807e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3131e-02 (4.5738e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][320/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9937e-02 (4.5679e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][330/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9022e-02 (4.5607e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5563e-02 (4.5784e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [60][350/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7476e-02 (4.5708e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7603e-02 (4.5683e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9648e-02 (4.5712e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7739e-02 (4.5650e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [60][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4006e-02 (4.5544e-02)	Acc@1  98.75 ( 99.37)	Acc@5 100.00 (100.00)
## e[60] optimizer.zero_grad (sum) time: 0.1786208152770996
## e[60]       loss.backward (sum) time: 3.9126803874969482
## e[60]      optimizer.step (sum) time: 1.272026777267456
## epoch[60] training(only) time: 13.84640097618103
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.1699e+00 (1.1699e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 1.2451e+00 (1.1982e+00)	Acc@1  74.00 ( 73.00)	Acc@5  91.00 ( 91.55)
Test: [ 20/100]	Time  0.023 ( 0.024)	Loss 1.1123e+00 (1.1462e+00)	Acc@1  76.00 ( 74.00)	Acc@5  93.00 ( 92.29)
Test: [ 30/100]	Time  0.023 ( 0.022)	Loss 1.6006e+00 (1.1957e+00)	Acc@1  64.00 ( 73.19)	Acc@5  92.00 ( 91.97)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1562e+00 (1.1735e+00)	Acc@1  73.00 ( 73.32)	Acc@5  94.00 ( 92.56)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4902e+00 (1.1832e+00)	Acc@1  69.00 ( 73.18)	Acc@5  91.00 ( 92.39)
Test: [ 60/100]	Time  0.018 ( 0.020)	Loss 1.3242e+00 (1.1671e+00)	Acc@1  71.00 ( 73.39)	Acc@5  92.00 ( 92.51)
Test: [ 70/100]	Time  0.016 ( 0.020)	Loss 1.4004e+00 (1.1610e+00)	Acc@1  69.00 ( 73.45)	Acc@5  91.00 ( 92.54)
Test: [ 80/100]	Time  0.021 ( 0.019)	Loss 1.3164e+00 (1.1609e+00)	Acc@1  73.00 ( 73.43)	Acc@5  90.00 ( 92.53)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5869e+00 (1.1499e+00)	Acc@1  66.00 ( 73.63)	Acc@5  90.00 ( 92.71)
 * Acc@1 73.790 Acc@5 92.810
### epoch[60] execution time: 15.852542400360107
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.189 ( 0.189)	Data  0.145 ( 0.145)	Loss 2.9205e-02 (2.9205e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [61][ 10/391]	Time  0.036 ( 0.049)	Data  0.001 ( 0.015)	Loss 4.5990e-02 (3.9143e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [61][ 20/391]	Time  0.033 ( 0.043)	Data  0.001 ( 0.008)	Loss 3.6682e-02 (3.7633e-02)	Acc@1  99.22 ( 99.81)	Acc@5 100.00 (100.00)
Epoch: [61][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.5767e-02 (3.9233e-02)	Acc@1  99.22 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [61][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 3.0090e-02 (4.0072e-02)	Acc@1 100.00 ( 99.62)	Acc@5 100.00 (100.00)
Epoch: [61][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.1504e-02 (4.2508e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [61][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.5095e-02 (4.2220e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.5828e-02 (4.2030e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [61][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.8981e-02 (4.2851e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.8218e-02 (4.2851e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][100/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.5309e-02 (4.2703e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [61][110/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.4778e-02 (4.2924e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [61][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3915e-02 (4.3088e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [61][130/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4474e-02 (4.3144e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [61][140/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6407e-02 (4.2740e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [61][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3884e-02 (4.2279e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 ( 99.99)
Epoch: [61][160/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0344e-02 (4.2567e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [61][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6600e-02 (4.2807e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9856e-02 (4.2909e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.0823e-02 (4.2765e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5959e-02 (4.3082e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [61][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5156e-02 (4.2907e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2745e-02 (4.2850e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0863e-02 (4.2879e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1290e-02 (4.2873e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8899e-02 (4.3098e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1350e-02 (4.3269e-02)	Acc@1  96.88 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [61][270/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3081e-02 (4.3201e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6610e-02 (4.3203e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5990e-02 (4.3264e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][300/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0045e-02 (4.3242e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [61][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3132e-02 (4.3065e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [61][320/391]	Time  0.036 ( 0.035)	Data  0.003 ( 0.002)	Loss 4.1534e-02 (4.3116e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [61][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2593e-02 (4.3121e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [61][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4636e-02 (4.3349e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][350/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3051e-02 (4.3450e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8757e-02 (4.3404e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7924e-02 (4.3351e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [61][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7954e-02 (4.3484e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [61][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7576e-02 (4.3588e-02)	Acc@1  97.50 ( 99.40)	Acc@5 100.00 (100.00)
## e[61] optimizer.zero_grad (sum) time: 0.17927074432373047
## e[61]       loss.backward (sum) time: 4.018103361129761
## e[61]      optimizer.step (sum) time: 1.2704973220825195
## epoch[61] training(only) time: 13.86237359046936
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.1680e+00 (1.1680e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.016 ( 0.031)	Loss 1.2520e+00 (1.1961e+00)	Acc@1  73.00 ( 73.09)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.1465e+00 (1.1466e+00)	Acc@1  74.00 ( 73.90)	Acc@5  93.00 ( 92.33)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.6240e+00 (1.1964e+00)	Acc@1  65.00 ( 73.10)	Acc@5  90.00 ( 91.84)
Test: [ 40/100]	Time  0.026 ( 0.021)	Loss 1.1416e+00 (1.1726e+00)	Acc@1  72.00 ( 73.49)	Acc@5  94.00 ( 92.49)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.5166e+00 (1.1850e+00)	Acc@1  70.00 ( 73.39)	Acc@5  90.00 ( 92.29)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.2979e+00 (1.1679e+00)	Acc@1  70.00 ( 73.51)	Acc@5  92.00 ( 92.38)
Test: [ 70/100]	Time  0.023 ( 0.020)	Loss 1.3760e+00 (1.1600e+00)	Acc@1  68.00 ( 73.51)	Acc@5  92.00 ( 92.44)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.3564e+00 (1.1600e+00)	Acc@1  73.00 ( 73.53)	Acc@5  90.00 ( 92.46)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5869e+00 (1.1488e+00)	Acc@1  66.00 ( 73.69)	Acc@5  91.00 ( 92.64)
 * Acc@1 73.900 Acc@5 92.720
### epoch[61] execution time: 15.831898927688599
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.192 ( 0.192)	Data  0.138 ( 0.138)	Loss 4.3335e-02 (4.3335e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [62][ 10/391]	Time  0.031 ( 0.049)	Data  0.001 ( 0.014)	Loss 3.1204e-02 (4.0670e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.008)	Loss 3.7048e-02 (4.0978e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [62][ 30/391]	Time  0.038 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.5767e-02 (4.2040e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [62][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 2.9846e-02 (4.1897e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [62][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.5004e-02 (4.2756e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][ 60/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.5604e-02 (4.2725e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.9587e-02 (4.2453e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [62][ 80/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8838e-02 (4.3785e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [62][ 90/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8075e-02 (4.4268e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][100/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.2928e-02 (4.4519e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [62][110/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1351e-02 (4.3969e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4321e-02 (4.4302e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [62][130/391]	Time  0.035 ( 0.036)	Data  0.005 ( 0.003)	Loss 6.9946e-02 (4.4786e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [62][140/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9001e-02 (4.4786e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [62][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1382e-02 (4.4512e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [62][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.4026e-02 (4.5052e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [62][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.8857e-02 (4.5547e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [62][180/391]	Time  0.043 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.7191e-02 (4.5218e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [62][190/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3956e-02 (4.5051e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][200/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1260e-02 (4.4937e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][210/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2053e-02 (4.4889e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [62][220/391]	Time  0.031 ( 0.035)	Data  0.002 ( 0.002)	Loss 5.4504e-02 (4.4919e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [62][230/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5491e-02 (4.5085e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][240/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3009e-02 (4.5391e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1289e-02 (4.5459e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8960e-02 (4.5595e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][270/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2328e-02 (4.5673e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [62][280/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7383e-02 (4.5811e-02)	Acc@1  96.88 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][290/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9388e-02 (4.5728e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3264e-02 (4.5759e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [62][310/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4149e-02 (4.5593e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][320/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9612e-02 (4.5760e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [62][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0018e-02 (4.5698e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][340/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4993e-02 (4.5603e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7659e-02 (4.5575e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5492e-02 (4.5328e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][370/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5706e-02 (4.5211e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [62][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7842e-02 (4.5121e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [62][390/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0089e-02 (4.5085e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
## e[62] optimizer.zero_grad (sum) time: 0.17764830589294434
## e[62]       loss.backward (sum) time: 3.835895299911499
## e[62]      optimizer.step (sum) time: 1.3168864250183105
## epoch[62] training(only) time: 13.844294548034668
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.1934e+00 (1.1934e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 1.2783e+00 (1.2072e+00)	Acc@1  75.00 ( 73.27)	Acc@5  90.00 ( 91.00)
Test: [ 20/100]	Time  0.015 ( 0.023)	Loss 1.1221e+00 (1.1505e+00)	Acc@1  74.00 ( 73.86)	Acc@5  93.00 ( 92.05)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.5967e+00 (1.1978e+00)	Acc@1  65.00 ( 73.32)	Acc@5  89.00 ( 91.81)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1748e+00 (1.1746e+00)	Acc@1  71.00 ( 73.51)	Acc@5  94.00 ( 92.46)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4941e+00 (1.1867e+00)	Acc@1  70.00 ( 73.29)	Acc@5  91.00 ( 92.31)
Test: [ 60/100]	Time  0.021 ( 0.020)	Loss 1.3105e+00 (1.1703e+00)	Acc@1  72.00 ( 73.44)	Acc@5  92.00 ( 92.44)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 1.3838e+00 (1.1633e+00)	Acc@1  69.00 ( 73.46)	Acc@5  92.00 ( 92.46)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.3252e+00 (1.1630e+00)	Acc@1  74.00 ( 73.44)	Acc@5  90.00 ( 92.46)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.6006e+00 (1.1515e+00)	Acc@1  66.00 ( 73.65)	Acc@5  91.00 ( 92.64)
 * Acc@1 73.850 Acc@5 92.740
### epoch[62] execution time: 15.832036018371582
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.191 ( 0.191)	Data  0.138 ( 0.138)	Loss 5.3284e-02 (5.3284e-02)	Acc@1  97.66 ( 97.66)	Acc@5 100.00 (100.00)
Epoch: [63][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.014)	Loss 2.7649e-02 (4.7585e-02)	Acc@1 100.00 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [63][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 5.5573e-02 (4.4166e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [63][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.6997e-02 (4.5796e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.1687e-02 (4.4694e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [63][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.1443e-02 (4.3923e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.9673e-02 (4.4161e-02)	Acc@1  99.22 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [63][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.0232e-02 (4.4728e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [63][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.7953e-02 (4.4563e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6316e-02 (4.4442e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.8218e-02 (4.4417e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.8976e-02 (4.4397e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.3364e-02 (4.4473e-02)	Acc@1  96.88 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7791e-02 (4.4195e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [63][140/391]	Time  0.049 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.0322e-02 (4.4865e-02)	Acc@1  98.44 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [63][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.1554e-02 (4.4563e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [63][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9072e-02 (4.4795e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2908e-02 (4.5065e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [63][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5746e-02 (4.4919e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.1910e-02 (4.4955e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7476e-02 (4.5022e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.8214e-02 (4.4800e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [63][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.2217e-02 (4.4670e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [63][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0577e-02 (4.4520e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9459e-02 (4.4557e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6224e-02 (4.4529e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][260/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1860e-02 (4.4572e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [63][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9877e-02 (4.4307e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [63][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7180e-02 (4.4238e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [63][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7634e-02 (4.4301e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [63][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0090e-02 (4.4097e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [63][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.5787e-02 (4.4144e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [63][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0995e-02 (4.4099e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [63][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2795e-02 (4.4243e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [63][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9856e-02 (4.4369e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [63][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1738e-02 (4.4187e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [63][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0314e-02 (4.4146e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [63][370/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 4.3121e-02 (4.4157e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [63][380/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5308e-02 (4.4274e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [63][390/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5369e-02 (4.4456e-02)	Acc@1  98.75 ( 99.39)	Acc@5 100.00 (100.00)
## e[63] optimizer.zero_grad (sum) time: 0.1785130500793457
## e[63]       loss.backward (sum) time: 4.020002603530884
## e[63]      optimizer.step (sum) time: 1.276416540145874
## epoch[63] training(only) time: 13.878546714782715
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.1709e+00 (1.1709e+00)	Acc@1  73.00 ( 73.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.020 ( 0.029)	Loss 1.3027e+00 (1.2111e+00)	Acc@1  72.00 ( 73.27)	Acc@5  90.00 ( 91.09)
Test: [ 20/100]	Time  0.016 ( 0.023)	Loss 1.1533e+00 (1.1586e+00)	Acc@1  74.00 ( 73.81)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.012 ( 0.021)	Loss 1.5957e+00 (1.2049e+00)	Acc@1  64.00 ( 73.16)	Acc@5  92.00 ( 91.94)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1514e+00 (1.1814e+00)	Acc@1  72.00 ( 73.44)	Acc@5  93.00 ( 92.51)
Test: [ 50/100]	Time  0.021 ( 0.020)	Loss 1.4922e+00 (1.1915e+00)	Acc@1  69.00 ( 73.24)	Acc@5  90.00 ( 92.27)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.3164e+00 (1.1737e+00)	Acc@1  70.00 ( 73.33)	Acc@5  93.00 ( 92.46)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 1.3848e+00 (1.1665e+00)	Acc@1  69.00 ( 73.39)	Acc@5  93.00 ( 92.56)
Test: [ 80/100]	Time  0.023 ( 0.019)	Loss 1.3213e+00 (1.1676e+00)	Acc@1  74.00 ( 73.41)	Acc@5  90.00 ( 92.58)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.5889e+00 (1.1557e+00)	Acc@1  65.00 ( 73.54)	Acc@5  92.00 ( 92.75)
 * Acc@1 73.750 Acc@5 92.840
### epoch[63] execution time: 15.834192514419556
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.188 ( 0.188)	Data  0.146 ( 0.146)	Loss 4.8279e-02 (4.8279e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 2.9358e-02 (4.0530e-02)	Acc@1 100.00 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [64][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.009)	Loss 2.9114e-02 (4.0940e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [64][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.6814e-02 (4.3141e-02)	Acc@1  98.44 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [64][ 40/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.5664e-02 (4.2681e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [64][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.0394e-02 (4.2224e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [64][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.9713e-02 (4.2297e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.4291e-02 (4.3030e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [64][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.6183e-02 (4.3030e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [64][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.4678e-02 (4.4073e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [64][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6509e-02 (4.4041e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 ( 99.99)
Epoch: [64][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.3680e-02 (4.3867e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [64][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1982e-02 (4.3824e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [64][130/391]	Time  0.037 ( 0.036)	Data  0.002 ( 0.003)	Loss 5.1270e-02 (4.3473e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [64][140/391]	Time  0.050 ( 0.036)	Data  0.002 ( 0.003)	Loss 3.7384e-02 (4.3403e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [64][150/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.3436e-02 (4.3332e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [64][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.9958e-02 (4.3066e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [64][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.3680e-02 (4.3030e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [64][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9591e-02 (4.3375e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [64][190/391]	Time  0.031 ( 0.036)	Data  0.002 ( 0.002)	Loss 3.2715e-02 (4.3233e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [64][200/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5247e-02 (4.3214e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [64][210/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9368e-02 (4.2967e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 ( 99.99)
Epoch: [64][220/391]	Time  0.044 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9377e-02 (4.3534e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [64][230/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5105e-02 (4.3431e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [64][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8340e-02 (4.3493e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [64][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9286e-02 (4.3595e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [64][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6488e-02 (4.3718e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [64][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4973e-02 (4.3765e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [64][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9866e-02 (4.3740e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [64][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.6660e-02 (4.3774e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [64][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2400e-02 (4.3940e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [64][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6122e-02 (4.3987e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [64][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1769e-02 (4.4116e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [64][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5176e-02 (4.4344e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [64][340/391]	Time  0.032 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.8854e-02 (4.4250e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [64][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8818e-02 (4.4088e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [64][360/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0527e-02 (4.3914e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3152e-02 (4.3941e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9327e-02 (4.3840e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [64][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5511e-02 (4.3876e-02)	Acc@1  98.75 ( 99.40)	Acc@5 100.00 (100.00)
## e[64] optimizer.zero_grad (sum) time: 0.17774200439453125
## e[64]       loss.backward (sum) time: 3.8709139823913574
## e[64]      optimizer.step (sum) time: 1.2862420082092285
## epoch[64] training(only) time: 13.849079370498657
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.1816e+00 (1.1816e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.2822e+00 (1.2036e+00)	Acc@1  74.00 ( 73.00)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.024 ( 0.024)	Loss 1.1221e+00 (1.1499e+00)	Acc@1  75.00 ( 73.81)	Acc@5  93.00 ( 92.33)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.5918e+00 (1.1980e+00)	Acc@1  67.00 ( 73.35)	Acc@5  91.00 ( 91.94)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1504e+00 (1.1732e+00)	Acc@1  72.00 ( 73.59)	Acc@5  95.00 ( 92.51)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4990e+00 (1.1835e+00)	Acc@1  68.00 ( 73.35)	Acc@5  90.00 ( 92.37)
Test: [ 60/100]	Time  0.025 ( 0.020)	Loss 1.2949e+00 (1.1665e+00)	Acc@1  72.00 ( 73.51)	Acc@5  92.00 ( 92.49)
Test: [ 70/100]	Time  0.019 ( 0.019)	Loss 1.4131e+00 (1.1588e+00)	Acc@1  71.00 ( 73.56)	Acc@5  91.00 ( 92.56)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.3389e+00 (1.1600e+00)	Acc@1  73.00 ( 73.57)	Acc@5  90.00 ( 92.54)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.6113e+00 (1.1486e+00)	Acc@1  66.00 ( 73.70)	Acc@5  91.00 ( 92.71)
 * Acc@1 73.880 Acc@5 92.810
### epoch[64] execution time: 15.825644731521606
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.175 ( 0.175)	Data  0.129 ( 0.129)	Loss 4.0436e-02 (4.0436e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.033 ( 0.048)	Data  0.001 ( 0.013)	Loss 4.8279e-02 (4.4187e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [65][ 20/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.007)	Loss 2.6855e-02 (4.3097e-02)	Acc@1 100.00 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [65][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 6.6895e-02 (4.4076e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [65][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.1412e-02 (4.4674e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [65][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.4148e-02 (4.5235e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [65][ 60/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.6538e-02 (4.4953e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.5878e-02 (4.5366e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [65][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.3711e-02 (4.5461e-02)	Acc@1  97.66 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [65][ 90/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.5095e-02 (4.4654e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1962e-02 (4.4353e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [65][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0070e-02 (4.4288e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [65][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.4189e-02 (4.4223e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [65][130/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.4765e-02 (4.4148e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][140/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5654e-02 (4.4234e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4819e-02 (4.4347e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.4170e-02 (4.3953e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][170/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7241e-02 (4.4458e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][180/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9825e-02 (4.4314e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][190/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0985e-02 (4.4194e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][200/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0802e-02 (4.4107e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][210/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6713e-02 (4.3968e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [65][220/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9072e-02 (4.4320e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][230/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9326e-02 (4.4333e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5664e-02 (4.4533e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][250/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3375e-02 (4.4478e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][260/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2622e-02 (4.4432e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][270/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8136e-02 (4.4376e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [65][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2552e-02 (4.4233e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5908e-02 (4.4233e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (4.4222e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][310/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.5195e-02 (4.4291e-02)	Acc@1  96.88 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4698e-02 (4.4162e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3416e-02 (4.4234e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [65][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2124e-02 (4.4328e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3081e-02 (4.4333e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5645e-02 (4.4314e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [65][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1870e-02 (4.4537e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [65][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7771e-02 (4.4558e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [65][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9164e-02 (4.4494e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
## e[65] optimizer.zero_grad (sum) time: 0.1771233081817627
## e[65]       loss.backward (sum) time: 3.9499547481536865
## e[65]      optimizer.step (sum) time: 1.2927708625793457
## epoch[65] training(only) time: 13.818633794784546
# Switched to evaluate mode...
Test: [  0/100]	Time  0.151 ( 0.151)	Loss 1.1895e+00 (1.1895e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.018 ( 0.029)	Loss 1.2871e+00 (1.2057e+00)	Acc@1  72.00 ( 72.82)	Acc@5  90.00 ( 91.18)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.1504e+00 (1.1516e+00)	Acc@1  75.00 ( 73.76)	Acc@5  93.00 ( 92.00)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.5869e+00 (1.1968e+00)	Acc@1  66.00 ( 73.35)	Acc@5  89.00 ( 91.68)
Test: [ 40/100]	Time  0.022 ( 0.021)	Loss 1.1602e+00 (1.1720e+00)	Acc@1  72.00 ( 73.51)	Acc@5  93.00 ( 92.32)
Test: [ 50/100]	Time  0.020 ( 0.021)	Loss 1.4805e+00 (1.1815e+00)	Acc@1  70.00 ( 73.33)	Acc@5  91.00 ( 92.16)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.3086e+00 (1.1646e+00)	Acc@1  71.00 ( 73.61)	Acc@5  92.00 ( 92.36)
Test: [ 70/100]	Time  0.029 ( 0.020)	Loss 1.3887e+00 (1.1575e+00)	Acc@1  69.00 ( 73.69)	Acc@5  92.00 ( 92.41)
Test: [ 80/100]	Time  0.022 ( 0.019)	Loss 1.3418e+00 (1.1580e+00)	Acc@1  73.00 ( 73.63)	Acc@5  90.00 ( 92.41)
Test: [ 90/100]	Time  0.019 ( 0.019)	Loss 1.6152e+00 (1.1463e+00)	Acc@1  66.00 ( 73.79)	Acc@5  90.00 ( 92.58)
 * Acc@1 73.990 Acc@5 92.690
### epoch[65] execution time: 15.812235116958618
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.191 ( 0.191)	Data  0.150 ( 0.150)	Loss 7.6599e-02 (7.6599e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 4.0833e-02 (4.1400e-02)	Acc@1 100.00 ( 99.64)	Acc@5 100.00 (100.00)
Epoch: [66][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 3.5309e-02 (4.4584e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [66][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 2.8961e-02 (4.5927e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [66][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.8004e-02 (4.3773e-02)	Acc@1  99.22 ( 99.56)	Acc@5 100.00 (100.00)
Epoch: [66][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 2.1317e-02 (4.4725e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [66][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.8788e-02 (4.5063e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [66][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.7190e-02 (4.5729e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [66][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.8076e-02 (4.5971e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [66][ 90/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.5786e-02 (4.5899e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [66][100/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.1909e-02 (4.5842e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [66][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.5430e-02 (4.5469e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [66][120/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2358e-02 (4.5624e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [66][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0436e-02 (4.5877e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [66][140/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.8910e-02 (4.5685e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [66][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.0984e-02 (4.6076e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [66][160/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.8513e-02 (4.5794e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [66][170/391]	Time  0.041 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.1788e-02 (4.5643e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [66][180/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.5450e-02 (4.5768e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [66][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2847e-02 (4.5793e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [66][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5492e-02 (4.5586e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [66][210/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9032e-02 (4.5269e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [66][220/391]	Time  0.038 ( 0.036)	Data  0.002 ( 0.002)	Loss 2.8854e-02 (4.5117e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [66][230/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2155e-02 (4.5192e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [66][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3010e-02 (4.5078e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [66][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9866e-02 (4.5095e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [66][260/391]	Time  0.041 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8136e-02 (4.4961e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [66][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0659e-02 (4.5050e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [66][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6641e-02 (4.5362e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [66][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9449e-02 (4.5153e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [66][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7402e-02 (4.5443e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [66][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5370e-02 (4.5271e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [66][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9998e-02 (4.5406e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [66][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1595e-02 (4.5499e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [66][340/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7140e-02 (4.5386e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [66][350/391]	Time  0.041 ( 0.035)	Data  0.004 ( 0.002)	Loss 3.1921e-02 (4.5462e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [66][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0894e-02 (4.5469e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [66][370/391]	Time  0.035 ( 0.035)	Data  0.002 ( 0.002)	Loss 5.9052e-02 (4.5323e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [66][380/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5969e-02 (4.5456e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [66][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3497e-02 (4.5476e-02)	Acc@1  98.75 ( 99.39)	Acc@5 100.00 (100.00)
## e[66] optimizer.zero_grad (sum) time: 0.17959022521972656
## e[66]       loss.backward (sum) time: 4.022204399108887
## e[66]      optimizer.step (sum) time: 1.2627019882202148
## epoch[66] training(only) time: 13.831111431121826
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.1562e+00 (1.1562e+00)	Acc@1  73.00 ( 73.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 1.2793e+00 (1.2023e+00)	Acc@1  73.00 ( 73.36)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.1279e+00 (1.1513e+00)	Acc@1  76.00 ( 74.05)	Acc@5  93.00 ( 92.19)
Test: [ 30/100]	Time  0.027 ( 0.022)	Loss 1.5977e+00 (1.2010e+00)	Acc@1  67.00 ( 73.35)	Acc@5  92.00 ( 91.90)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1621e+00 (1.1772e+00)	Acc@1  71.00 ( 73.44)	Acc@5  93.00 ( 92.49)
Test: [ 50/100]	Time  0.019 ( 0.020)	Loss 1.4912e+00 (1.1880e+00)	Acc@1  69.00 ( 73.22)	Acc@5  90.00 ( 92.27)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 1.2988e+00 (1.1707e+00)	Acc@1  72.00 ( 73.46)	Acc@5  93.00 ( 92.39)
Test: [ 70/100]	Time  0.016 ( 0.019)	Loss 1.4141e+00 (1.1640e+00)	Acc@1  69.00 ( 73.56)	Acc@5  92.00 ( 92.46)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.3242e+00 (1.1645e+00)	Acc@1  73.00 ( 73.48)	Acc@5  90.00 ( 92.44)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.6191e+00 (1.1531e+00)	Acc@1  67.00 ( 73.64)	Acc@5  90.00 ( 92.59)
 * Acc@1 73.830 Acc@5 92.670
### epoch[66] execution time: 15.806503057479858
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.195 ( 0.195)	Data  0.152 ( 0.152)	Loss 8.2703e-02 (8.2703e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.038 ( 0.050)	Data  0.001 ( 0.015)	Loss 6.8787e-02 (5.5213e-02)	Acc@1  97.66 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [67][ 20/391]	Time  0.031 ( 0.042)	Data  0.001 ( 0.009)	Loss 2.6932e-02 (4.8727e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [67][ 30/391]	Time  0.044 ( 0.040)	Data  0.001 ( 0.007)	Loss 3.7598e-02 (4.7555e-02)	Acc@1  99.22 ( 99.17)	Acc@5 100.00 (100.00)
Epoch: [67][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 6.2317e-02 (4.7672e-02)	Acc@1  99.22 ( 99.16)	Acc@5 100.00 (100.00)
Epoch: [67][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 3.0716e-02 (4.7539e-02)	Acc@1 100.00 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [67][ 60/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.9276e-02 (4.7058e-02)	Acc@1  99.22 ( 99.24)	Acc@5 100.00 (100.00)
Epoch: [67][ 70/391]	Time  0.032 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.7079e-02 (4.6366e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [67][ 80/391]	Time  0.036 ( 0.037)	Data  0.002 ( 0.004)	Loss 3.8849e-02 (4.6147e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [67][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.8534e-02 (4.5824e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [67][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6478e-02 (4.5228e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [67][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.8910e-02 (4.4895e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7577e-02 (4.4937e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [67][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.9957e-02 (4.4309e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0985e-02 (4.4168e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [67][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0009e-02 (4.4184e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [67][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6194e-02 (4.4032e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [67][170/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.003)	Loss 3.2898e-02 (4.4009e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [67][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.8641e-02 (4.3774e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [67][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7109e-02 (4.4045e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.3375e-02 (4.4469e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [67][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.8391e-02 (4.4258e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [67][220/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7170e-02 (4.4585e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [67][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6194e-02 (4.4399e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [67][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3650e-02 (4.4258e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3966e-02 (4.4102e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][260/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1218e-02 (4.4056e-02)	Acc@1  97.66 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [67][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3020e-02 (4.3988e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1758e-02 (4.4103e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [67][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9500e-02 (4.4229e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [67][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3335e-02 (4.4297e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [67][310/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5461e-02 (4.4553e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5166e-02 (4.4706e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [67][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9246e-02 (4.4675e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][340/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6896e-02 (4.4606e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [67][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6478e-02 (4.4544e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [67][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3314e-02 (4.4600e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7281e-02 (4.4641e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [67][380/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6917e-02 (4.4601e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [67][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.3669e-02 (4.4586e-02)	Acc@1  98.75 ( 99.38)	Acc@5 100.00 (100.00)
## e[67] optimizer.zero_grad (sum) time: 0.17873549461364746
## e[67]       loss.backward (sum) time: 4.0138044357299805
## e[67]      optimizer.step (sum) time: 1.2478246688842773
## epoch[67] training(only) time: 13.863607168197632
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.2070e+00 (1.2070e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.014 ( 0.029)	Loss 1.2607e+00 (1.2060e+00)	Acc@1  74.00 ( 73.09)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.015 ( 0.023)	Loss 1.1201e+00 (1.1500e+00)	Acc@1  74.00 ( 73.71)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5986e+00 (1.2004e+00)	Acc@1  66.00 ( 73.13)	Acc@5  90.00 ( 91.84)
Test: [ 40/100]	Time  0.015 ( 0.020)	Loss 1.1650e+00 (1.1782e+00)	Acc@1  71.00 ( 73.29)	Acc@5  94.00 ( 92.49)
Test: [ 50/100]	Time  0.022 ( 0.020)	Loss 1.4922e+00 (1.1878e+00)	Acc@1  70.00 ( 73.06)	Acc@5  91.00 ( 92.35)
Test: [ 60/100]	Time  0.022 ( 0.020)	Loss 1.3193e+00 (1.1701e+00)	Acc@1  72.00 ( 73.30)	Acc@5  92.00 ( 92.51)
Test: [ 70/100]	Time  0.021 ( 0.019)	Loss 1.3770e+00 (1.1627e+00)	Acc@1  70.00 ( 73.38)	Acc@5  91.00 ( 92.52)
Test: [ 80/100]	Time  0.022 ( 0.019)	Loss 1.3350e+00 (1.1640e+00)	Acc@1  72.00 ( 73.36)	Acc@5  90.00 ( 92.43)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.6133e+00 (1.1524e+00)	Acc@1  66.00 ( 73.56)	Acc@5  91.00 ( 92.57)
 * Acc@1 73.790 Acc@5 92.680
### epoch[67] execution time: 15.824345588684082
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.186 ( 0.186)	Data  0.145 ( 0.145)	Loss 3.5858e-02 (3.5858e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [68][ 10/391]	Time  0.030 ( 0.048)	Data  0.001 ( 0.015)	Loss 3.2379e-02 (3.9135e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [68][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 3.2471e-02 (3.7381e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [68][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.8340e-02 (4.0054e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [68][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.1016e-02 (4.3014e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [68][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.7292e-02 (4.3102e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [68][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.4576e-02 (4.2309e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [68][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.4189e-02 (4.2896e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [68][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.0577e-02 (4.2805e-02)	Acc@1  96.09 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [68][ 90/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7445e-02 (4.3177e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.0457e-02 (4.3156e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [68][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0079e-02 (4.3997e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [68][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.3925e-02 (4.3938e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.4556e-02 (4.4304e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][140/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4321e-02 (4.4398e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [68][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.7618e-02 (4.4118e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [68][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5074e-02 (4.4485e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [68][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1647e-02 (4.4083e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [68][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.0507e-02 (4.4205e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [68][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9917e-02 (4.4364e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [68][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4565e-02 (4.4492e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [68][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9469e-02 (4.4959e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [68][220/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9561e-02 (4.5021e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 ( 99.99)
Epoch: [68][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.9153e-02 (4.5092e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [68][240/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1504e-02 (4.5031e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [68][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2572e-02 (4.4988e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [68][260/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9846e-02 (4.5117e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [68][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8269e-02 (4.4978e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 ( 99.99)
Epoch: [68][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9185e-02 (4.4953e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [68][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4454e-02 (4.4949e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [68][300/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1208e-02 (4.5073e-02)	Acc@1  98.44 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [68][310/391]	Time  0.046 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6600e-02 (4.4982e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 ( 99.99)
Epoch: [68][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8116e-02 (4.4753e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [68][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0304e-02 (4.4675e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7505e-02 (4.4749e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8269e-02 (4.4851e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][360/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3041e-02 (4.4830e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2079e-02 (4.4721e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9266e-02 (4.4655e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [68][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7026e-02 (4.4900e-02)	Acc@1  97.50 ( 99.36)	Acc@5 100.00 (100.00)
## e[68] optimizer.zero_grad (sum) time: 0.17925381660461426
## e[68]       loss.backward (sum) time: 4.061396837234497
## e[68]      optimizer.step (sum) time: 1.2507507801055908
## epoch[68] training(only) time: 13.893308162689209
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.1738e+00 (1.1738e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.2676e+00 (1.1968e+00)	Acc@1  72.00 ( 73.36)	Acc@5  91.00 ( 91.36)
Test: [ 20/100]	Time  0.023 ( 0.024)	Loss 1.1221e+00 (1.1499e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.020 ( 0.023)	Loss 1.5889e+00 (1.1991e+00)	Acc@1  65.00 ( 73.39)	Acc@5  93.00 ( 91.94)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1562e+00 (1.1779e+00)	Acc@1  71.00 ( 73.41)	Acc@5  93.00 ( 92.44)
Test: [ 50/100]	Time  0.022 ( 0.021)	Loss 1.4961e+00 (1.1877e+00)	Acc@1  70.00 ( 73.20)	Acc@5  92.00 ( 92.25)
Test: [ 60/100]	Time  0.019 ( 0.020)	Loss 1.2871e+00 (1.1703e+00)	Acc@1  70.00 ( 73.44)	Acc@5  93.00 ( 92.39)
Test: [ 70/100]	Time  0.020 ( 0.020)	Loss 1.3926e+00 (1.1632e+00)	Acc@1  66.00 ( 73.45)	Acc@5  92.00 ( 92.44)
Test: [ 80/100]	Time  0.022 ( 0.020)	Loss 1.3115e+00 (1.1638e+00)	Acc@1  73.00 ( 73.43)	Acc@5  90.00 ( 92.43)
Test: [ 90/100]	Time  0.017 ( 0.019)	Loss 1.6162e+00 (1.1528e+00)	Acc@1  65.00 ( 73.63)	Acc@5  91.00 ( 92.59)
 * Acc@1 73.760 Acc@5 92.720
### epoch[68] execution time: 15.903739213943481
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.196 ( 0.196)	Data  0.155 ( 0.155)	Loss 2.5421e-02 (2.5421e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.016)	Loss 4.0344e-02 (4.2960e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [69][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 3.4149e-02 (4.2566e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.007)	Loss 5.4352e-02 (4.5657e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [69][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.3365e-02 (4.4680e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.0527e-02 (4.4045e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [69][ 60/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.6133e-02 (4.4824e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [69][ 70/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.0385e-02 (4.6310e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.6772e-02 (4.6900e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.7281e-02 (4.6798e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [69][100/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.1870e-02 (4.6253e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][110/391]	Time  0.038 ( 0.037)	Data  0.003 ( 0.003)	Loss 7.6294e-02 (4.6028e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][120/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3549e-02 (4.6233e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2155e-02 (4.6462e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][140/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6967e-02 (4.6481e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [69][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 9.1309e-02 (4.6781e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 ( 99.99)
Epoch: [69][160/391]	Time  0.030 ( 0.036)	Data  0.000 ( 0.003)	Loss 3.4454e-02 (4.6875e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][170/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2155e-02 (4.6955e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.0626e-01 (4.7406e-02)	Acc@1  95.31 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [69][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4626e-02 (4.7423e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][200/391]	Time  0.040 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9642e-02 (4.7290e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2277e-02 (4.7148e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [69][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.1544e-02 (4.6784e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [69][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.3590e-02 (4.6468e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [69][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7018e-02 (4.6176e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [69][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6366e-02 (4.6047e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][260/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6041e-02 (4.5801e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7964e-02 (4.5836e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2561e-02 (4.5643e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [69][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6844e-02 (4.5449e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [69][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9368e-02 (4.5377e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2786e-02 (4.5257e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9490e-02 (4.5167e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1819e-02 (4.4998e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8208e-02 (4.4901e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6906e-02 (4.4808e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3945e-02 (4.4780e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [69][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.8427e-02 (4.4886e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][380/391]	Time  0.039 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2110e-02 (4.4842e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [69][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.7097e-02 (4.4867e-02)	Acc@1  98.75 ( 99.42)	Acc@5 100.00 (100.00)
## e[69] optimizer.zero_grad (sum) time: 0.17740750312805176
## e[69]       loss.backward (sum) time: 3.8170628547668457
## e[69]      optimizer.step (sum) time: 1.3159897327423096
## epoch[69] training(only) time: 13.86290979385376
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.1650e+00 (1.1650e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.016 ( 0.030)	Loss 1.2744e+00 (1.2021e+00)	Acc@1  72.00 ( 72.82)	Acc@5  91.00 ( 91.27)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.1592e+00 (1.1519e+00)	Acc@1  75.00 ( 73.95)	Acc@5  92.00 ( 92.24)
Test: [ 30/100]	Time  0.019 ( 0.023)	Loss 1.5918e+00 (1.2009e+00)	Acc@1  65.00 ( 73.45)	Acc@5  93.00 ( 91.87)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1543e+00 (1.1777e+00)	Acc@1  72.00 ( 73.56)	Acc@5  93.00 ( 92.39)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.4775e+00 (1.1877e+00)	Acc@1  70.00 ( 73.37)	Acc@5  90.00 ( 92.12)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 1.2803e+00 (1.1702e+00)	Acc@1  71.00 ( 73.62)	Acc@5  91.00 ( 92.25)
Test: [ 70/100]	Time  0.018 ( 0.020)	Loss 1.3916e+00 (1.1626e+00)	Acc@1  66.00 ( 73.54)	Acc@5  92.00 ( 92.30)
Test: [ 80/100]	Time  0.024 ( 0.020)	Loss 1.3262e+00 (1.1626e+00)	Acc@1  73.00 ( 73.54)	Acc@5  90.00 ( 92.31)
Test: [ 90/100]	Time  0.022 ( 0.019)	Loss 1.6201e+00 (1.1514e+00)	Acc@1  67.00 ( 73.75)	Acc@5  90.00 ( 92.49)
 * Acc@1 73.950 Acc@5 92.600
### epoch[69] execution time: 15.844619989395142
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.190 ( 0.190)	Data  0.147 ( 0.147)	Loss 2.7359e-02 (2.7359e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [70][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.015)	Loss 3.2745e-02 (3.9595e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [70][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 5.8746e-02 (4.1038e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [70][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 2.6855e-02 (4.3244e-02)	Acc@1 100.00 ( 99.60)	Acc@5 100.00 (100.00)
Epoch: [70][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.6631e-02 (4.4005e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [70][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.2236e-02 (4.4412e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [70][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.9251e-02 (4.3566e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.3875e-02 (4.2803e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.4475e-02 (4.2990e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1718e-02 (4.3452e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [70][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6021e-02 (4.3743e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][110/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.0060e-02 (4.4107e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7302e-02 (4.4160e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0741e-02 (4.4031e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [70][140/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.3162e-02 (4.4101e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [70][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6967e-02 (4.3768e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 ( 99.99)
Epoch: [70][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.5491e-02 (4.3842e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][170/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1082e-02 (4.3468e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][180/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8615e-02 (4.3380e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.4607e-02 (4.3133e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [70][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.5817e-02 (4.3129e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4880e-02 (4.3020e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [70][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4688e-02 (4.2905e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [70][230/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2694e-02 (4.2891e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [70][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3112e-02 (4.2557e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [70][250/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8157e-02 (4.2623e-02)	Acc@1 100.00 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [70][260/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5370e-02 (4.2684e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [70][270/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4758e-02 (4.3189e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [70][280/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2135e-02 (4.3266e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][290/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9001e-02 (4.3149e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][300/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7150e-02 (4.3294e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][310/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0222e-02 (4.3341e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][320/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.5439e-02 (4.3341e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7882e-02 (4.3353e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1107e-02 (4.3621e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][350/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0039e-02 (4.3678e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][360/391]	Time  0.039 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9922e-02 (4.3688e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7354e-02 (4.3736e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [70][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3782e-02 (4.3753e-02)	Acc@1  97.66 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [70][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4880e-02 (4.3746e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
## e[70] optimizer.zero_grad (sum) time: 0.17890667915344238
## e[70]       loss.backward (sum) time: 3.9663376808166504
## e[70]      optimizer.step (sum) time: 1.250368356704712
## epoch[70] training(only) time: 13.913030624389648
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.1748e+00 (1.1748e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.014 ( 0.029)	Loss 1.2617e+00 (1.1966e+00)	Acc@1  71.00 ( 73.00)	Acc@5  91.00 ( 91.55)
Test: [ 20/100]	Time  0.015 ( 0.023)	Loss 1.1240e+00 (1.1464e+00)	Acc@1  74.00 ( 74.05)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.020 ( 0.022)	Loss 1.5977e+00 (1.1959e+00)	Acc@1  64.00 ( 73.26)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1611e+00 (1.1737e+00)	Acc@1  71.00 ( 73.37)	Acc@5  93.00 ( 92.46)
Test: [ 50/100]	Time  0.014 ( 0.021)	Loss 1.4912e+00 (1.1835e+00)	Acc@1  69.00 ( 73.18)	Acc@5  91.00 ( 92.24)
Test: [ 60/100]	Time  0.023 ( 0.020)	Loss 1.2988e+00 (1.1662e+00)	Acc@1  71.00 ( 73.36)	Acc@5  93.00 ( 92.39)
Test: [ 70/100]	Time  0.015 ( 0.020)	Loss 1.4053e+00 (1.1598e+00)	Acc@1  69.00 ( 73.51)	Acc@5  92.00 ( 92.42)
Test: [ 80/100]	Time  0.015 ( 0.020)	Loss 1.2920e+00 (1.1606e+00)	Acc@1  73.00 ( 73.49)	Acc@5  90.00 ( 92.33)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.6318e+00 (1.1496e+00)	Acc@1  66.00 ( 73.62)	Acc@5  89.00 ( 92.51)
 * Acc@1 73.810 Acc@5 92.610
### epoch[70] execution time: 15.911720514297485
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.181 ( 0.181)	Data  0.138 ( 0.138)	Loss 5.1483e-02 (5.1483e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [71][ 10/391]	Time  0.033 ( 0.048)	Data  0.001 ( 0.014)	Loss 5.3528e-02 (4.6922e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [71][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 6.5552e-02 (4.7451e-02)	Acc@1  97.66 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [71][ 30/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.006)	Loss 3.3020e-02 (4.5207e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [71][ 40/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.1107e-02 (4.4874e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [71][ 50/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.004)	Loss 5.0415e-02 (4.5774e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [71][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.6346e-02 (4.5065e-02)	Acc@1 100.00 ( 99.58)	Acc@5 100.00 (100.00)
Epoch: [71][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.4768e-02 (4.4911e-02)	Acc@1  97.66 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [71][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.9092e-02 (4.5406e-02)	Acc@1  97.66 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [71][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2084e-02 (4.5429e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [71][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.9601e-02 (4.5614e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6926e-02 (4.5037e-02)	Acc@1  99.22 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [71][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0516e-02 (4.5333e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6356e-02 (4.4564e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.0079e-02 (4.4753e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6509e-02 (4.4599e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [71][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3417e-02 (4.4713e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [71][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3884e-02 (4.4381e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.9816e-02 (4.4129e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [71][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.2227e-02 (4.4039e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5563e-02 (4.4173e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][210/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2469e-02 (4.4093e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [71][220/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7089e-02 (4.4208e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2765e-02 (4.4144e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [71][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2185e-02 (4.4432e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [71][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9652e-02 (4.4632e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1910e-02 (4.4590e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][270/391]	Time  0.033 ( 0.035)	Data  0.000 ( 0.002)	Loss 3.6011e-02 (4.4450e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0304e-02 (4.4441e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2797e-02 (4.4310e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.1106e-02 (4.4497e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [71][310/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9907e-02 (4.4516e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [71][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7628e-02 (4.4267e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4657e-02 (4.4332e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0995e-02 (4.4208e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8909e-02 (4.4339e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3152e-02 (4.4290e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4393e-02 (4.4285e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [71][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9866e-02 (4.4430e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [71][390/391]	Time  0.025 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7354e-02 (4.4584e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
## e[71] optimizer.zero_grad (sum) time: 0.17970871925354004
## e[71]       loss.backward (sum) time: 4.125518798828125
## e[71]      optimizer.step (sum) time: 1.2442023754119873
## epoch[71] training(only) time: 13.840425968170166
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2090e+00 (1.2090e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.031)	Loss 1.2881e+00 (1.2161e+00)	Acc@1  74.00 ( 73.55)	Acc@5  90.00 ( 91.27)
Test: [ 20/100]	Time  0.020 ( 0.025)	Loss 1.1338e+00 (1.1553e+00)	Acc@1  75.00 ( 74.24)	Acc@5  93.00 ( 92.14)
Test: [ 30/100]	Time  0.023 ( 0.023)	Loss 1.6152e+00 (1.2027e+00)	Acc@1  66.00 ( 73.52)	Acc@5  89.00 ( 91.71)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1611e+00 (1.1800e+00)	Acc@1  72.00 ( 73.66)	Acc@5  93.00 ( 92.34)
Test: [ 50/100]	Time  0.018 ( 0.021)	Loss 1.4971e+00 (1.1911e+00)	Acc@1  70.00 ( 73.35)	Acc@5  91.00 ( 92.24)
Test: [ 60/100]	Time  0.024 ( 0.020)	Loss 1.3154e+00 (1.1738e+00)	Acc@1  73.00 ( 73.52)	Acc@5  92.00 ( 92.36)
Test: [ 70/100]	Time  0.014 ( 0.020)	Loss 1.4111e+00 (1.1669e+00)	Acc@1  70.00 ( 73.55)	Acc@5  92.00 ( 92.44)
Test: [ 80/100]	Time  0.020 ( 0.020)	Loss 1.3379e+00 (1.1671e+00)	Acc@1  73.00 ( 73.52)	Acc@5  90.00 ( 92.41)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.6201e+00 (1.1551e+00)	Acc@1  67.00 ( 73.70)	Acc@5  91.00 ( 92.58)
 * Acc@1 73.910 Acc@5 92.650
### epoch[71] execution time: 15.842329263687134
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.189 ( 0.189)	Data  0.148 ( 0.148)	Loss 2.4002e-02 (2.4002e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 3.8574e-02 (4.2589e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [72][ 20/391]	Time  0.037 ( 0.043)	Data  0.001 ( 0.009)	Loss 4.6295e-02 (4.4160e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [72][ 30/391]	Time  0.036 ( 0.041)	Data  0.001 ( 0.007)	Loss 3.4363e-02 (4.2234e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.0039e-02 (4.2451e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [72][ 50/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.0995e-02 (4.3222e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [72][ 60/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.7566e-02 (4.4883e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [72][ 70/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.4819e-02 (4.4894e-02)	Acc@1  97.66 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [72][ 80/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.4078e-02 (4.5884e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [72][ 90/391]	Time  0.033 ( 0.037)	Data  0.000 ( 0.003)	Loss 3.5614e-02 (4.5833e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [72][100/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.8177e-02 (4.5158e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [72][110/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.2440e-02 (4.5517e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [72][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7913e-02 (4.4824e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [72][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.5156e-02 (4.4768e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [72][140/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.3483e-02 (4.4581e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [72][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3600e-02 (4.4130e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [72][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6255e-02 (4.3867e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [72][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.0396e-02 (4.3988e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9276e-02 (4.3787e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0710e-02 (4.3763e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [72][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3417e-02 (4.3574e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [72][210/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1036e-02 (4.3951e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [72][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7577e-02 (4.3863e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [72][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.7465e-02 (4.4177e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [72][240/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4270e-02 (4.4286e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [72][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9398e-02 (4.4279e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 ( 99.99)
Epoch: [72][260/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.2805e-02 (4.4310e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [72][270/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6650e-02 (4.4425e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [72][280/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0781e-02 (4.4502e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [72][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1798e-02 (4.4416e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [72][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7817e-02 (4.4373e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 ( 99.99)
Epoch: [72][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6946e-02 (4.4582e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 ( 99.99)
Epoch: [72][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0974e-02 (4.4558e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [72][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9988e-02 (4.4779e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [72][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.5742e-02 (4.4681e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [72][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5278e-02 (4.4713e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [72][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3925e-02 (4.4526e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [72][370/391]	Time  0.042 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7384e-02 (4.4519e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [72][380/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3722e-02 (4.4417e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [72][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8676e-02 (4.4300e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
## e[72] optimizer.zero_grad (sum) time: 0.17816543579101562
## e[72]       loss.backward (sum) time: 3.919447898864746
## e[72]      optimizer.step (sum) time: 1.2864186763763428
## epoch[72] training(only) time: 13.885377645492554
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.1982e+00 (1.1982e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.017 ( 0.029)	Loss 1.2734e+00 (1.2008e+00)	Acc@1  74.00 ( 73.00)	Acc@5  91.00 ( 91.82)
Test: [ 20/100]	Time  0.016 ( 0.023)	Loss 1.1318e+00 (1.1525e+00)	Acc@1  75.00 ( 73.76)	Acc@5  92.00 ( 92.24)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5938e+00 (1.2049e+00)	Acc@1  64.00 ( 73.03)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1777e+00 (1.1823e+00)	Acc@1  71.00 ( 73.07)	Acc@5  94.00 ( 92.54)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.4922e+00 (1.1922e+00)	Acc@1  71.00 ( 72.98)	Acc@5  91.00 ( 92.39)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.3184e+00 (1.1732e+00)	Acc@1  70.00 ( 73.31)	Acc@5  92.00 ( 92.54)
Test: [ 70/100]	Time  0.018 ( 0.019)	Loss 1.3896e+00 (1.1659e+00)	Acc@1  69.00 ( 73.44)	Acc@5  92.00 ( 92.59)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.3184e+00 (1.1670e+00)	Acc@1  73.00 ( 73.37)	Acc@5  90.00 ( 92.53)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.6201e+00 (1.1554e+00)	Acc@1  66.00 ( 73.51)	Acc@5  91.00 ( 92.70)
 * Acc@1 73.770 Acc@5 92.800
### epoch[72] execution time: 15.83610463142395
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.190 ( 0.190)	Data  0.148 ( 0.148)	Loss 4.0222e-02 (4.0222e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [73][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 3.8879e-02 (3.8444e-02)	Acc@1 100.00 ( 99.93)	Acc@5 100.00 (100.00)
Epoch: [73][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.009)	Loss 5.3436e-02 (4.4307e-02)	Acc@1  98.44 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [73][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 5.6671e-02 (4.5164e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [73][ 40/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.2755e-02 (4.4474e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [73][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.005)	Loss 5.7556e-02 (4.5173e-02)	Acc@1  98.44 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [73][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.5319e-02 (4.4758e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [73][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 7.5073e-02 (4.6494e-02)	Acc@1  99.22 ( 99.28)	Acc@5  99.22 ( 99.99)
Epoch: [73][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.5441e-02 (4.6021e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [73][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.9764e-02 (4.6195e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 ( 99.99)
Epoch: [73][100/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.6661e-02 (4.6554e-02)	Acc@1  99.22 ( 99.27)	Acc@5 100.00 ( 99.99)
Epoch: [73][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1052e-02 (4.6919e-02)	Acc@1 100.00 ( 99.26)	Acc@5 100.00 ( 99.99)
Epoch: [73][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1504e-02 (4.6209e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 ( 99.99)
Epoch: [73][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7272e-02 (4.6271e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 ( 99.99)
Epoch: [73][140/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3569e-02 (4.6128e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [73][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.9683e-02 (4.6011e-02)	Acc@1  98.44 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [73][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6530e-02 (4.5575e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [73][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.8727e-02 (4.5546e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [73][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6833e-02 (4.5416e-02)	Acc@1  97.66 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [73][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5187e-02 (4.5306e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [73][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3813e-02 (4.5135e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [73][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.3925e-02 (4.5307e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [73][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6702e-02 (4.5420e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [73][230/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3955e-02 (4.5565e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [73][240/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2877e-02 (4.5501e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [73][250/391]	Time  0.042 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4403e-02 (4.5691e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [73][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1718e-02 (4.5494e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [73][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1086e-02 (4.5446e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [73][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3793e-02 (4.5381e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [73][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3569e-02 (4.5356e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [73][300/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0319e-02 (4.5106e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [73][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7668e-02 (4.5139e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [73][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5654e-02 (4.5034e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [73][330/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7281e-02 (4.5117e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [73][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0201e-02 (4.5149e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [73][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6285e-02 (4.5049e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [73][360/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.4704e-02 (4.4910e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [73][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5980e-02 (4.4726e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [73][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8645e-02 (4.4642e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [73][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1300e-02 (4.4622e-02)	Acc@1  98.75 ( 99.37)	Acc@5 100.00 (100.00)
## e[73] optimizer.zero_grad (sum) time: 0.17829656600952148
## e[73]       loss.backward (sum) time: 3.9270060062408447
## e[73]      optimizer.step (sum) time: 1.2754673957824707
## epoch[73] training(only) time: 13.869118452072144
# Switched to evaluate mode...
Test: [  0/100]	Time  0.158 ( 0.158)	Loss 1.1924e+00 (1.1924e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.017 ( 0.031)	Loss 1.2324e+00 (1.1946e+00)	Acc@1  72.00 ( 72.64)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.1240e+00 (1.1452e+00)	Acc@1  75.00 ( 74.10)	Acc@5  92.00 ( 92.14)
Test: [ 30/100]	Time  0.017 ( 0.023)	Loss 1.5918e+00 (1.1939e+00)	Acc@1  64.00 ( 73.42)	Acc@5  91.00 ( 91.84)
Test: [ 40/100]	Time  0.029 ( 0.022)	Loss 1.1758e+00 (1.1728e+00)	Acc@1  70.00 ( 73.54)	Acc@5  93.00 ( 92.51)
Test: [ 50/100]	Time  0.017 ( 0.021)	Loss 1.4941e+00 (1.1840e+00)	Acc@1  69.00 ( 73.31)	Acc@5  91.00 ( 92.31)
Test: [ 60/100]	Time  0.023 ( 0.020)	Loss 1.2969e+00 (1.1670e+00)	Acc@1  71.00 ( 73.46)	Acc@5  91.00 ( 92.43)
Test: [ 70/100]	Time  0.020 ( 0.020)	Loss 1.4150e+00 (1.1607e+00)	Acc@1  69.00 ( 73.52)	Acc@5  92.00 ( 92.49)
Test: [ 80/100]	Time  0.023 ( 0.020)	Loss 1.3105e+00 (1.1603e+00)	Acc@1  73.00 ( 73.51)	Acc@5  90.00 ( 92.41)
Test: [ 90/100]	Time  0.022 ( 0.020)	Loss 1.6494e+00 (1.1502e+00)	Acc@1  66.00 ( 73.69)	Acc@5  91.00 ( 92.57)
 * Acc@1 73.840 Acc@5 92.640
### epoch[73] execution time: 15.890905380249023
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.191 ( 0.191)	Data  0.148 ( 0.148)	Loss 3.4821e-02 (3.4821e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [74][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 4.8492e-02 (4.6531e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [74][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 3.4241e-02 (4.5787e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [74][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.9703e-02 (4.5609e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [74][ 40/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.005)	Loss 3.7384e-02 (4.3806e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [74][ 50/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.6478e-02 (4.3539e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [74][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.4216e-02 (4.4939e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [74][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.3864e-02 (4.4471e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [74][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.5391e-02 (4.4226e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [74][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.7638e-02 (4.4014e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [74][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1138e-02 (4.3590e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [74][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.9129e-02 (4.3445e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [74][120/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.9469e-02 (4.3259e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [74][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.6467e-02 (4.3327e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [74][140/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5471e-02 (4.3391e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [74][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.2776e-02 (4.3577e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [74][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2155e-02 (4.3942e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [74][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1260e-02 (4.4125e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [74][180/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9622e-02 (4.3986e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [74][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5074e-02 (4.4392e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [74][200/391]	Time  0.034 ( 0.036)	Data  0.002 ( 0.002)	Loss 3.1616e-02 (4.4271e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [74][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5095e-02 (4.4279e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [74][220/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (4.4248e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [74][230/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3477e-02 (4.4444e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [74][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1525e-02 (4.4496e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [74][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3295e-02 (4.4469e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [74][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3518e-02 (4.4610e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [74][270/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2450e-02 (4.4547e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [74][280/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5370e-02 (4.4733e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [74][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7130e-02 (4.4668e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [74][300/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3711e-02 (4.4569e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [74][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2440e-02 (4.4328e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [74][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4403e-02 (4.4575e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [74][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9774e-02 (4.4625e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [74][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2990e-02 (4.4664e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [74][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.3669e-02 (4.4670e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [74][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3640e-02 (4.4711e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [74][370/391]	Time  0.044 ( 0.035)	Data  0.006 ( 0.002)	Loss 6.3416e-02 (4.4789e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [74][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3843e-02 (4.5091e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [74][390/391]	Time  0.026 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2551e-02 (4.5229e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
## e[74] optimizer.zero_grad (sum) time: 0.17714166641235352
## e[74]       loss.backward (sum) time: 3.878932237625122
## e[74]      optimizer.step (sum) time: 1.29060697555542
## epoch[74] training(only) time: 13.829561948776245
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.2061e+00 (1.2061e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.022 ( 0.029)	Loss 1.2715e+00 (1.2105e+00)	Acc@1  74.00 ( 73.36)	Acc@5  90.00 ( 91.09)
Test: [ 20/100]	Time  0.015 ( 0.023)	Loss 1.1387e+00 (1.1528e+00)	Acc@1  74.00 ( 74.10)	Acc@5  93.00 ( 92.19)
Test: [ 30/100]	Time  0.026 ( 0.021)	Loss 1.6250e+00 (1.2003e+00)	Acc@1  65.00 ( 73.35)	Acc@5  90.00 ( 91.90)
Test: [ 40/100]	Time  0.023 ( 0.021)	Loss 1.1660e+00 (1.1785e+00)	Acc@1  72.00 ( 73.39)	Acc@5  94.00 ( 92.61)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5000e+00 (1.1897e+00)	Acc@1  70.00 ( 73.24)	Acc@5  91.00 ( 92.39)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.2969e+00 (1.1713e+00)	Acc@1  72.00 ( 73.43)	Acc@5  92.00 ( 92.51)
Test: [ 70/100]	Time  0.017 ( 0.019)	Loss 1.3818e+00 (1.1639e+00)	Acc@1  68.00 ( 73.46)	Acc@5  92.00 ( 92.51)
Test: [ 80/100]	Time  0.024 ( 0.019)	Loss 1.3418e+00 (1.1645e+00)	Acc@1  73.00 ( 73.44)	Acc@5  90.00 ( 92.49)
Test: [ 90/100]	Time  0.018 ( 0.019)	Loss 1.6260e+00 (1.1535e+00)	Acc@1  67.00 ( 73.71)	Acc@5  90.00 ( 92.66)
 * Acc@1 73.890 Acc@5 92.740
### epoch[74] execution time: 15.794318437576294
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.195 ( 0.195)	Data  0.153 ( 0.153)	Loss 4.2053e-02 (4.2053e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 4.2358e-02 (4.5074e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [75][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 4.2206e-02 (4.6337e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [75][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.007)	Loss 3.0884e-02 (4.6373e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [75][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 3.2440e-02 (4.5945e-02)	Acc@1 100.00 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [75][ 50/391]	Time  0.036 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.5410e-02 (4.5065e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [75][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.2593e-02 (4.4485e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [75][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.7333e-02 (4.4913e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [75][ 80/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.3295e-02 (4.4548e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [75][ 90/391]	Time  0.035 ( 0.037)	Data  0.002 ( 0.003)	Loss 3.6194e-02 (4.5159e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [75][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1229e-02 (4.4793e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [75][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3447e-02 (4.4765e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [75][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6987e-02 (4.4700e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [75][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.9875e-02 (4.5508e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [75][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1052e-02 (4.4953e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [75][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.4994e-02 (4.4733e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [75][160/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.8798e-02 (4.4713e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6387e-02 (4.4473e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][180/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0466e-02 (4.4336e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6346e-02 (4.4023e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8248e-02 (4.4298e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][210/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7231e-02 (4.4307e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9948e-02 (4.4204e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6702e-02 (4.4209e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.7542e-02 (4.4044e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][250/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.5284e-02 (4.3721e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][260/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8340e-02 (4.3696e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][270/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.4973e-02 (4.3640e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][280/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5553e-02 (4.3711e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [75][290/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.9816e-02 (4.3469e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][300/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7109e-02 (4.3404e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [75][310/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7638e-02 (4.3602e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [75][320/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.9755e-02 (4.3977e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0018e-02 (4.4082e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0568e-02 (4.4153e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3732e-02 (4.4240e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][360/391]	Time  0.040 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8879e-02 (4.4211e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][370/391]	Time  0.040 ( 0.035)	Data  0.004 ( 0.002)	Loss 3.0365e-02 (4.4160e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [75][380/391]	Time  0.036 ( 0.035)	Data  0.002 ( 0.002)	Loss 3.1067e-02 (4.4205e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [75][390/391]	Time  0.029 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4534e-02 (4.4364e-02)	Acc@1  98.75 ( 99.46)	Acc@5 100.00 (100.00)
## e[75] optimizer.zero_grad (sum) time: 0.17824935913085938
## e[75]       loss.backward (sum) time: 4.005438804626465
## e[75]      optimizer.step (sum) time: 1.2635595798492432
## epoch[75] training(only) time: 13.930664300918579
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.1611e+00 (1.1611e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.015 ( 0.030)	Loss 1.2598e+00 (1.2020e+00)	Acc@1  71.00 ( 72.82)	Acc@5  90.00 ( 91.27)
Test: [ 20/100]	Time  0.019 ( 0.024)	Loss 1.1553e+00 (1.1541e+00)	Acc@1  74.00 ( 73.67)	Acc@5  92.00 ( 92.19)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.5820e+00 (1.1991e+00)	Acc@1  66.00 ( 73.16)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.018 ( 0.021)	Loss 1.1445e+00 (1.1745e+00)	Acc@1  72.00 ( 73.46)	Acc@5  93.00 ( 92.46)
Test: [ 50/100]	Time  0.022 ( 0.020)	Loss 1.5176e+00 (1.1853e+00)	Acc@1  70.00 ( 73.20)	Acc@5  91.00 ( 92.29)
Test: [ 60/100]	Time  0.019 ( 0.020)	Loss 1.2871e+00 (1.1681e+00)	Acc@1  70.00 ( 73.43)	Acc@5  92.00 ( 92.41)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3828e+00 (1.1605e+00)	Acc@1  68.00 ( 73.51)	Acc@5  92.00 ( 92.52)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.3115e+00 (1.1617e+00)	Acc@1  73.00 ( 73.47)	Acc@5  89.00 ( 92.52)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.6094e+00 (1.1508e+00)	Acc@1  66.00 ( 73.64)	Acc@5  90.00 ( 92.67)
 * Acc@1 73.840 Acc@5 92.750
### epoch[75] execution time: 15.879092693328857
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.191 ( 0.191)	Data  0.149 ( 0.149)	Loss 4.4189e-02 (4.4189e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 3.8147e-02 (3.8119e-02)	Acc@1  99.22 ( 99.57)	Acc@5 100.00 (100.00)
Epoch: [76][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 3.5889e-02 (4.2889e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [76][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 5.0476e-02 (4.2906e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.1290e-02 (4.4085e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [76][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.1016e-02 (4.3643e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [76][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.1311e-02 (4.3079e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [76][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.5441e-02 (4.2508e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][ 80/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.8534e-02 (4.2729e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.7903e-02 (4.2881e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [76][100/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6631e-02 (4.3184e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.3955e-02 (4.3310e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [76][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.0685e-02 (4.3095e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [76][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.6840e-02 (4.3255e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [76][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.7343e-02 (4.3648e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.7878e-02 (4.3517e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.8472e-02 (4.3228e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [76][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7689e-02 (4.3373e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [76][180/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9856e-02 (4.3604e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [76][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7241e-02 (4.3769e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [76][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4922e-02 (4.3723e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [76][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5614e-02 (4.3682e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [76][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6143e-02 (4.3964e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9001e-02 (4.4114e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][240/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1199e-02 (4.3950e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [76][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7333e-02 (4.4007e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][260/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.8946e-02 (4.4138e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][270/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7262e-02 (4.4041e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [76][280/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3152e-02 (4.3939e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [76][290/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5837e-02 (4.3880e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [76][300/391]	Time  0.035 ( 0.035)	Data  0.002 ( 0.002)	Loss 5.0812e-02 (4.3625e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [76][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7537e-02 (4.3780e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3600e-02 (4.3787e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][330/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2236e-02 (4.3678e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7455e-02 (4.3584e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [76][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2816e-02 (4.3772e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [76][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5685e-02 (4.3841e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [76][370/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7974e-02 (4.3889e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [76][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.1371e-02 (4.3922e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [76][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3243e-02 (4.3970e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
## e[76] optimizer.zero_grad (sum) time: 0.17954540252685547
## e[76]       loss.backward (sum) time: 4.028532028198242
## e[76]      optimizer.step (sum) time: 1.2651503086090088
## epoch[76] training(only) time: 13.90360426902771
# Switched to evaluate mode...
Test: [  0/100]	Time  0.154 ( 0.154)	Loss 1.2148e+00 (1.2148e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.014 ( 0.031)	Loss 1.2803e+00 (1.2116e+00)	Acc@1  71.00 ( 72.36)	Acc@5  91.00 ( 91.36)
Test: [ 20/100]	Time  0.020 ( 0.025)	Loss 1.1455e+00 (1.1565e+00)	Acc@1  74.00 ( 73.38)	Acc@5  92.00 ( 92.29)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.5957e+00 (1.2037e+00)	Acc@1  66.00 ( 72.90)	Acc@5  91.00 ( 92.03)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1787e+00 (1.1808e+00)	Acc@1  73.00 ( 73.12)	Acc@5  93.00 ( 92.49)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.4766e+00 (1.1914e+00)	Acc@1  70.00 ( 73.02)	Acc@5  91.00 ( 92.31)
Test: [ 60/100]	Time  0.020 ( 0.020)	Loss 1.3232e+00 (1.1732e+00)	Acc@1  73.00 ( 73.30)	Acc@5  92.00 ( 92.44)
Test: [ 70/100]	Time  0.019 ( 0.020)	Loss 1.3984e+00 (1.1656e+00)	Acc@1  69.00 ( 73.37)	Acc@5  92.00 ( 92.55)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.3242e+00 (1.1663e+00)	Acc@1  74.00 ( 73.35)	Acc@5  89.00 ( 92.52)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.6084e+00 (1.1542e+00)	Acc@1  65.00 ( 73.57)	Acc@5  92.00 ( 92.73)
 * Acc@1 73.740 Acc@5 92.840
### epoch[76] execution time: 15.870548009872437
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.192 ( 0.192)	Data  0.151 ( 0.151)	Loss 3.2654e-02 (3.2654e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [77][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 4.4586e-02 (4.7258e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [77][ 20/391]	Time  0.033 ( 0.042)	Data  0.001 ( 0.008)	Loss 4.9164e-02 (4.3937e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [77][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.4790e-02 (4.4425e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [77][ 40/391]	Time  0.034 ( 0.038)	Data  0.002 ( 0.005)	Loss 3.6865e-02 (4.3234e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [77][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.9185e-02 (4.4070e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [77][ 60/391]	Time  0.038 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.5339e-02 (4.4119e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [77][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.3813e-02 (4.4335e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [77][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.1616e-02 (4.4969e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [77][ 90/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.9093e-02 (4.5471e-02)	Acc@1  99.22 ( 99.26)	Acc@5 100.00 (100.00)
Epoch: [77][100/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.7079e-02 (4.5307e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [77][110/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.4006e-02 (4.4833e-02)	Acc@1  99.22 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [77][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6926e-02 (4.4874e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [77][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1494e-02 (4.4574e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [77][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2856e-02 (4.5060e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [77][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.9831e-02 (4.4663e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [77][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8359e-02 (4.4778e-02)	Acc@1  97.66 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [77][170/391]	Time  0.035 ( 0.036)	Data  0.003 ( 0.002)	Loss 4.1901e-02 (4.4833e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [77][180/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7913e-02 (4.4922e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [77][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6957e-02 (4.5012e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [77][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.8330e-02 (4.5152e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [77][210/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.4551e-02 (4.4726e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [77][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2633e-02 (4.5083e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [77][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9998e-02 (4.4999e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [77][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.9907e-02 (4.4809e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [77][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.2654e-02 (4.4645e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [77][260/391]	Time  0.028 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1748e-02 (4.4522e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [77][270/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.8519e-02 (4.4362e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [77][280/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.1371e-02 (4.4404e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [77][290/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4199e-02 (4.4223e-02)	Acc@1  98.44 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [77][300/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5797e-02 (4.4181e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [77][310/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3182e-02 (4.3976e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3802e-02 (4.4057e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9327e-02 (4.3956e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2684e-02 (4.4111e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [77][350/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 4.3976e-02 (4.4137e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][360/391]	Time  0.039 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5857e-02 (4.4048e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9194e-02 (4.4104e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [77][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1250e-02 (4.4063e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [77][390/391]	Time  0.025 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7026e-02 (4.4172e-02)	Acc@1  98.75 ( 99.42)	Acc@5 100.00 (100.00)
## e[77] optimizer.zero_grad (sum) time: 0.17977404594421387
## e[77]       loss.backward (sum) time: 4.082256317138672
## e[77]      optimizer.step (sum) time: 1.2511401176452637
## epoch[77] training(only) time: 13.915987968444824
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.1963e+00 (1.1963e+00)	Acc@1  70.00 ( 70.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.021 ( 0.030)	Loss 1.2627e+00 (1.2050e+00)	Acc@1  74.00 ( 73.18)	Acc@5  91.00 ( 91.55)
Test: [ 20/100]	Time  0.020 ( 0.024)	Loss 1.1123e+00 (1.1534e+00)	Acc@1  74.00 ( 74.00)	Acc@5  93.00 ( 92.33)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.6143e+00 (1.2015e+00)	Acc@1  63.00 ( 73.23)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.021 ( 0.021)	Loss 1.1953e+00 (1.1792e+00)	Acc@1  69.00 ( 73.29)	Acc@5  94.00 ( 92.56)
Test: [ 50/100]	Time  0.023 ( 0.021)	Loss 1.4912e+00 (1.1887e+00)	Acc@1  72.00 ( 73.16)	Acc@5  91.00 ( 92.31)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.3418e+00 (1.1711e+00)	Acc@1  70.00 ( 73.34)	Acc@5  92.00 ( 92.46)
Test: [ 70/100]	Time  0.014 ( 0.020)	Loss 1.4189e+00 (1.1660e+00)	Acc@1  68.00 ( 73.37)	Acc@5  92.00 ( 92.46)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3184e+00 (1.1662e+00)	Acc@1  73.00 ( 73.33)	Acc@5  90.00 ( 92.44)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.5850e+00 (1.1539e+00)	Acc@1  67.00 ( 73.56)	Acc@5  91.00 ( 92.65)
 * Acc@1 73.750 Acc@5 92.760
### epoch[77] execution time: 15.916255712509155
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.190 ( 0.190)	Data  0.148 ( 0.148)	Loss 3.4271e-02 (3.4271e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 3.1403e-02 (3.9364e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [78][ 20/391]	Time  0.035 ( 0.042)	Data  0.001 ( 0.009)	Loss 3.9948e-02 (3.9662e-02)	Acc@1  99.22 ( 99.78)	Acc@5 100.00 (100.00)
Epoch: [78][ 30/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.2603e-02 (4.2904e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [78][ 40/391]	Time  0.036 ( 0.039)	Data  0.001 ( 0.005)	Loss 3.9520e-02 (4.2524e-02)	Acc@1 100.00 ( 99.70)	Acc@5 100.00 (100.00)
Epoch: [78][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.0192e-02 (4.4011e-02)	Acc@1  99.22 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [78][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.8696e-02 (4.4142e-02)	Acc@1 100.00 ( 99.59)	Acc@5 100.00 (100.00)
Epoch: [78][ 70/391]	Time  0.031 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.0161e-02 (4.4115e-02)	Acc@1  99.22 ( 99.55)	Acc@5 100.00 (100.00)
Epoch: [78][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.8685e-02 (4.4968e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [78][ 90/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.5797e-02 (4.5076e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [78][100/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9429e-02 (4.5016e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2216e-02 (4.5035e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [78][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.5436e-02 (4.4739e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [78][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.5450e-02 (4.5156e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [78][140/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6438e-02 (4.5096e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [78][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1788e-02 (4.4842e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [78][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.5919e-02 (4.4758e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [78][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.5919e-02 (4.5279e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [78][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6509e-02 (4.5038e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.4979e-02 (4.4818e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [78][200/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.8258e-02 (4.4789e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][210/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0323e-02 (4.4899e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [78][220/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3049e-02 (4.4763e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [78][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1361e-02 (4.4941e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [78][240/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6407e-02 (4.5077e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][250/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5939e-02 (4.4999e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][260/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1403e-02 (4.4949e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4810e-02 (4.4927e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3417e-02 (4.4857e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5187e-02 (4.5170e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [78][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8492e-02 (4.5077e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3843e-02 (4.5193e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][320/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1586e-02 (4.5189e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6844e-02 (4.5232e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2043e-02 (4.5109e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2623e-02 (4.5169e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3875e-02 (4.5113e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2633e-02 (4.5128e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.8763e-02 (4.5053e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [78][390/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6591e-02 (4.4924e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
## e[78] optimizer.zero_grad (sum) time: 0.17908525466918945
## e[78]       loss.backward (sum) time: 3.9371163845062256
## e[78]      optimizer.step (sum) time: 1.2999956607818604
## epoch[78] training(only) time: 13.833722352981567
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.1875e+00 (1.1875e+00)	Acc@1  71.00 ( 71.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.015 ( 0.028)	Loss 1.2734e+00 (1.2015e+00)	Acc@1  73.00 ( 73.64)	Acc@5  91.00 ( 91.64)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.1094e+00 (1.1488e+00)	Acc@1  74.00 ( 74.14)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.021 ( 0.022)	Loss 1.5986e+00 (1.2015e+00)	Acc@1  65.00 ( 73.42)	Acc@5  93.00 ( 91.87)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1748e+00 (1.1795e+00)	Acc@1  71.00 ( 73.51)	Acc@5  94.00 ( 92.49)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.4863e+00 (1.1888e+00)	Acc@1  71.00 ( 73.33)	Acc@5  90.00 ( 92.27)
Test: [ 60/100]	Time  0.021 ( 0.020)	Loss 1.2959e+00 (1.1704e+00)	Acc@1  71.00 ( 73.61)	Acc@5  93.00 ( 92.46)
Test: [ 70/100]	Time  0.014 ( 0.020)	Loss 1.4072e+00 (1.1638e+00)	Acc@1  68.00 ( 73.66)	Acc@5  92.00 ( 92.49)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.3301e+00 (1.1649e+00)	Acc@1  72.00 ( 73.59)	Acc@5  90.00 ( 92.46)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.6191e+00 (1.1533e+00)	Acc@1  66.00 ( 73.75)	Acc@5  91.00 ( 92.64)
 * Acc@1 73.930 Acc@5 92.730
### epoch[78] execution time: 15.829920768737793
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.190 ( 0.190)	Data  0.149 ( 0.149)	Loss 6.0455e-02 (6.0455e-02)	Acc@1  98.44 ( 98.44)	Acc@5 100.00 (100.00)
Epoch: [79][ 10/391]	Time  0.034 ( 0.050)	Data  0.001 ( 0.015)	Loss 4.4678e-02 (4.5029e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 3.7445e-02 (4.2039e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [79][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.5156e-02 (4.3097e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [79][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.4169e-02 (4.5501e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [79][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.1951e-02 (4.6017e-02)	Acc@1  97.66 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [79][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 6.5430e-02 (4.5829e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [79][ 70/391]	Time  0.034 ( 0.037)	Data  0.002 ( 0.004)	Loss 4.5197e-02 (4.4791e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][ 80/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.9204e-02 (4.5507e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][ 90/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5959e-02 (4.5715e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][100/391]	Time  0.030 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1788e-02 (4.5965e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [79][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.8401e-02 (4.6425e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [79][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.5370e-02 (4.5908e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][130/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6204e-02 (4.5807e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2267e-02 (4.5675e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.5187e-02 (4.5671e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][160/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4128e-02 (4.5071e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.2430e-02 (4.4800e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [79][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1891e-02 (4.4506e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.6223e-02 (4.4417e-02)	Acc@1  98.44 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [79][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1586e-02 (4.4131e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [79][210/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3875e-02 (4.3918e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3447e-02 (4.3757e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5065e-02 (4.3762e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][240/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.3010e-02 (4.3496e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3152e-02 (4.3805e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][260/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1727e-02 (4.3879e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4138e-02 (4.3861e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][280/391]	Time  0.033 ( 0.035)	Data  0.000 ( 0.002)	Loss 3.2654e-02 (4.3866e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4729e-02 (4.3735e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6133e-02 (4.3732e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][310/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6589e-02 (4.3752e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8553e-02 (4.3812e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][330/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9174e-02 (4.3735e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2155e-02 (4.3786e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3020e-02 (4.3781e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [79][360/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.6814e-02 (4.3874e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.4595e-02 (4.4245e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [79][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3365e-02 (4.4152e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [79][390/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7760e-02 (4.4190e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
## e[79] optimizer.zero_grad (sum) time: 0.1792600154876709
## e[79]       loss.backward (sum) time: 3.9866273403167725
## e[79]      optimizer.step (sum) time: 1.2725639343261719
## epoch[79] training(only) time: 13.902795791625977
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.1660e+00 (1.1660e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.2686e+00 (1.2008e+00)	Acc@1  73.00 ( 73.09)	Acc@5  91.00 ( 91.18)
Test: [ 20/100]	Time  0.021 ( 0.024)	Loss 1.1494e+00 (1.1488e+00)	Acc@1  74.00 ( 73.95)	Acc@5  92.00 ( 92.24)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.6016e+00 (1.1964e+00)	Acc@1  65.00 ( 73.42)	Acc@5  91.00 ( 91.90)
Test: [ 40/100]	Time  0.014 ( 0.021)	Loss 1.1582e+00 (1.1747e+00)	Acc@1  72.00 ( 73.63)	Acc@5  93.00 ( 92.46)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.4746e+00 (1.1846e+00)	Acc@1  70.00 ( 73.31)	Acc@5  91.00 ( 92.20)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.2959e+00 (1.1673e+00)	Acc@1  71.00 ( 73.52)	Acc@5  92.00 ( 92.36)
Test: [ 70/100]	Time  0.014 ( 0.019)	Loss 1.3730e+00 (1.1597e+00)	Acc@1  69.00 ( 73.61)	Acc@5  92.00 ( 92.41)
Test: [ 80/100]	Time  0.017 ( 0.019)	Loss 1.3135e+00 (1.1596e+00)	Acc@1  73.00 ( 73.60)	Acc@5  90.00 ( 92.42)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.6094e+00 (1.1488e+00)	Acc@1  66.00 ( 73.77)	Acc@5  90.00 ( 92.56)
 * Acc@1 73.950 Acc@5 92.660
### epoch[79] execution time: 15.902201175689697
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.185 ( 0.185)	Data  0.142 ( 0.142)	Loss 3.2288e-02 (3.2288e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 5.5817e-02 (4.5209e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [80][ 20/391]	Time  0.036 ( 0.042)	Data  0.001 ( 0.008)	Loss 3.3173e-02 (4.6483e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [80][ 30/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.006)	Loss 7.0984e-02 (4.5018e-02)	Acc@1  97.66 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.3976e-02 (4.4994e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [80][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 2.4796e-02 (4.4005e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [80][ 60/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.5898e-02 (4.4139e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [80][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.3833e-02 (4.4433e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [80][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.0930e-02 (4.3958e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [80][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.1718e-02 (4.4052e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [80][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4515e-02 (4.4323e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [80][110/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9825e-02 (4.3896e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [80][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.4887e-02 (4.4056e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [80][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7415e-02 (4.3846e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [80][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.6885e-02 (4.3955e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [80][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4708e-02 (4.3618e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [80][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.0354e-02 (4.3863e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [80][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9784e-02 (4.3880e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [80][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9805e-02 (4.4198e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [80][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.9642e-02 (4.4096e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [80][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.7267e-02 (4.4229e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [80][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3671e-02 (4.4087e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [80][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5105e-02 (4.3839e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [80][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.7664e-02 (4.3689e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [80][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9753e-02 (4.3650e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [80][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4800e-02 (4.3509e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [80][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3782e-02 (4.3754e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [80][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6367e-02 (4.3625e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [80][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3295e-02 (4.3690e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [80][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7026e-02 (4.3776e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [80][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9622e-02 (4.3921e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [80][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 8.1299e-02 (4.3830e-02)	Acc@1  98.44 ( 99.45)	Acc@5  99.22 (100.00)
Epoch: [80][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4525e-02 (4.4075e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [80][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2725e-02 (4.4194e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [80][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4302e-02 (4.4064e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [80][350/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5767e-02 (4.4103e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [80][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1504e-02 (4.3993e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [80][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0807e-02 (4.3890e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [80][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2339e-02 (4.3856e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [80][390/391]	Time  0.026 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5746e-02 (4.3879e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
## e[80] optimizer.zero_grad (sum) time: 0.17861032485961914
## e[80]       loss.backward (sum) time: 4.09306263923645
## e[80]      optimizer.step (sum) time: 1.2533910274505615
## epoch[80] training(only) time: 13.868618965148926
# Switched to evaluate mode...
Test: [  0/100]	Time  0.168 ( 0.168)	Loss 1.1924e+00 (1.1924e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.019 ( 0.031)	Loss 1.2422e+00 (1.1960e+00)	Acc@1  73.00 ( 73.64)	Acc@5  91.00 ( 91.55)
Test: [ 20/100]	Time  0.020 ( 0.025)	Loss 1.1074e+00 (1.1457e+00)	Acc@1  75.00 ( 74.29)	Acc@5  92.00 ( 92.38)
Test: [ 30/100]	Time  0.015 ( 0.023)	Loss 1.5684e+00 (1.1971e+00)	Acc@1  64.00 ( 73.42)	Acc@5  92.00 ( 92.03)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1670e+00 (1.1751e+00)	Acc@1  71.00 ( 73.44)	Acc@5  93.00 ( 92.61)
Test: [ 50/100]	Time  0.020 ( 0.021)	Loss 1.5264e+00 (1.1865e+00)	Acc@1  68.00 ( 73.25)	Acc@5  90.00 ( 92.47)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.3213e+00 (1.1696e+00)	Acc@1  71.00 ( 73.48)	Acc@5  92.00 ( 92.61)
Test: [ 70/100]	Time  0.022 ( 0.020)	Loss 1.4102e+00 (1.1633e+00)	Acc@1  70.00 ( 73.54)	Acc@5  92.00 ( 92.66)
Test: [ 80/100]	Time  0.020 ( 0.020)	Loss 1.3115e+00 (1.1643e+00)	Acc@1  73.00 ( 73.48)	Acc@5  90.00 ( 92.59)
Test: [ 90/100]	Time  0.020 ( 0.019)	Loss 1.6025e+00 (1.1530e+00)	Acc@1  66.00 ( 73.68)	Acc@5  90.00 ( 92.74)
 * Acc@1 73.850 Acc@5 92.810
### epoch[80] execution time: 15.87962031364441
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.194 ( 0.194)	Data  0.152 ( 0.152)	Loss 7.3364e-02 (7.3364e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.016)	Loss 4.3518e-02 (5.1034e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [81][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 2.2476e-02 (4.4055e-02)	Acc@1 100.00 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [81][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 6.0120e-02 (4.4123e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [81][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 2.9129e-02 (4.4724e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.005)	Loss 4.4922e-02 (4.4578e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [81][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.7740e-02 (4.3806e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [81][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.1412e-02 (4.3895e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [81][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 7.6477e-02 (4.3784e-02)	Acc@1  97.66 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4688e-02 (4.4024e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8441e-02 (4.3624e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.3354e-02 (4.3556e-02)	Acc@1  98.44 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [81][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.5084e-02 (4.3805e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2673e-02 (4.4062e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [81][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.8147e-02 (4.4140e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [81][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7811e-02 (4.4115e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [81][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7659e-02 (4.4215e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5706e-02 (4.4539e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6835e-02 (4.4459e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [81][190/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1779e-02 (4.4814e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4636e-02 (4.4757e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [81][210/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4647e-02 (4.4620e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0894e-02 (4.4555e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7415e-02 (4.4605e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.4826e-02 (4.4546e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][250/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7771e-02 (4.4543e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1931e-02 (4.4411e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [81][270/391]	Time  0.043 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8981e-02 (4.4628e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][280/391]	Time  0.042 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2542e-02 (4.4502e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9042e-02 (4.4504e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2297e-02 (4.4653e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [81][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6224e-02 (4.4903e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [81][320/391]	Time  0.038 ( 0.035)	Data  0.002 ( 0.002)	Loss 2.9846e-02 (4.4752e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [81][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6886e-02 (4.4674e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [81][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3295e-02 (4.4517e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [81][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5797e-02 (4.4430e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [81][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2043e-02 (4.4706e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [81][370/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.9590e-02 (4.4707e-02)	Acc@1  96.88 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [81][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8401e-02 (4.4738e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [81][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.9336e-02 (4.4662e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
## e[81] optimizer.zero_grad (sum) time: 0.17973828315734863
## e[81]       loss.backward (sum) time: 4.116925477981567
## e[81]      optimizer.step (sum) time: 1.243210792541504
## epoch[81] training(only) time: 13.830649137496948
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.1631e+00 (1.1631e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.015 ( 0.029)	Loss 1.2510e+00 (1.1926e+00)	Acc@1  74.00 ( 73.18)	Acc@5  91.00 ( 91.64)
Test: [ 20/100]	Time  0.015 ( 0.024)	Loss 1.1045e+00 (1.1406e+00)	Acc@1  75.00 ( 74.05)	Acc@5  93.00 ( 92.38)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.5791e+00 (1.1892e+00)	Acc@1  64.00 ( 73.35)	Acc@5  93.00 ( 92.10)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1611e+00 (1.1679e+00)	Acc@1  72.00 ( 73.44)	Acc@5  94.00 ( 92.61)
Test: [ 50/100]	Time  0.021 ( 0.020)	Loss 1.4736e+00 (1.1772e+00)	Acc@1  70.00 ( 73.27)	Acc@5  90.00 ( 92.39)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.2949e+00 (1.1605e+00)	Acc@1  71.00 ( 73.49)	Acc@5  93.00 ( 92.57)
Test: [ 70/100]	Time  0.026 ( 0.020)	Loss 1.3857e+00 (1.1541e+00)	Acc@1  67.00 ( 73.48)	Acc@5  91.00 ( 92.51)
Test: [ 80/100]	Time  0.020 ( 0.019)	Loss 1.3115e+00 (1.1540e+00)	Acc@1  73.00 ( 73.46)	Acc@5  90.00 ( 92.48)
Test: [ 90/100]	Time  0.019 ( 0.019)	Loss 1.6006e+00 (1.1424e+00)	Acc@1  65.00 ( 73.68)	Acc@5  90.00 ( 92.64)
 * Acc@1 73.830 Acc@5 92.750
### epoch[81] execution time: 15.835174560546875
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.184 ( 0.184)	Data  0.144 ( 0.144)	Loss 2.8503e-02 (2.8503e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.034 ( 0.049)	Data  0.001 ( 0.015)	Loss 3.3539e-02 (4.8404e-02)	Acc@1 100.00 ( 99.08)	Acc@5 100.00 (100.00)
Epoch: [82][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.008)	Loss 4.6265e-02 (4.9545e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [82][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.7964e-02 (4.9360e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [82][ 40/391]	Time  0.035 ( 0.039)	Data  0.001 ( 0.005)	Loss 3.6163e-02 (4.7747e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [82][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 2.9846e-02 (4.6484e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][ 60/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 4.6448e-02 (4.5474e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.0577e-02 (4.6485e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [82][ 80/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.1310e-02 (4.6751e-02)	Acc@1  98.44 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [82][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.7323e-02 (4.6237e-02)	Acc@1  99.22 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [82][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1168e-02 (4.6749e-02)	Acc@1 100.00 ( 99.30)	Acc@5 100.00 (100.00)
Epoch: [82][110/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.7847e-02 (4.6858e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 (100.00)
Epoch: [82][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4149e-02 (4.5891e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3752e-02 (4.6109e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [82][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.6535e-02 (4.5505e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2643e-02 (4.5365e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.5497e-02 (4.5115e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.2419e-02 (4.4948e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6743e-02 (4.5033e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3905e-02 (4.4838e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][200/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1158e-02 (4.4878e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.3040e-02 (4.4859e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3488e-02 (4.4980e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [82][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.4250e-02 (4.5111e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [82][240/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1351e-02 (4.5118e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [82][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.2612e-02 (4.5320e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [82][260/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7964e-02 (4.5093e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4861e-02 (4.5263e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [82][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.6718e-02 (4.5169e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.1469e-02 (4.5111e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [82][300/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3030e-02 (4.4970e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [82][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0161e-02 (4.4891e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [82][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0243e-02 (4.4895e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [82][330/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2888e-02 (4.4715e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [82][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0741e-02 (4.4579e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [82][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7800e-02 (4.4319e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [82][360/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.7627e-02 (4.4370e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [82][370/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2561e-02 (4.4580e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [82][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1647e-02 (4.4606e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [82][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5084e-02 (4.4653e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
## e[82] optimizer.zero_grad (sum) time: 0.179826021194458
## e[82]       loss.backward (sum) time: 4.070571184158325
## e[82]      optimizer.step (sum) time: 1.2517647743225098
## epoch[82] training(only) time: 13.882901668548584
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.1738e+00 (1.1738e+00)	Acc@1  71.00 ( 71.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.020 ( 0.030)	Loss 1.2754e+00 (1.2064e+00)	Acc@1  73.00 ( 73.73)	Acc@5  91.00 ( 91.64)
Test: [ 20/100]	Time  0.023 ( 0.024)	Loss 1.1172e+00 (1.1505e+00)	Acc@1  74.00 ( 74.29)	Acc@5  93.00 ( 92.33)
Test: [ 30/100]	Time  0.016 ( 0.022)	Loss 1.5947e+00 (1.1987e+00)	Acc@1  65.00 ( 73.52)	Acc@5  93.00 ( 92.10)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1514e+00 (1.1765e+00)	Acc@1  72.00 ( 73.56)	Acc@5  93.00 ( 92.61)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.4980e+00 (1.1865e+00)	Acc@1  70.00 ( 73.22)	Acc@5  91.00 ( 92.41)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.2949e+00 (1.1681e+00)	Acc@1  72.00 ( 73.48)	Acc@5  93.00 ( 92.56)
Test: [ 70/100]	Time  0.015 ( 0.020)	Loss 1.3984e+00 (1.1622e+00)	Acc@1  69.00 ( 73.55)	Acc@5  92.00 ( 92.56)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.3037e+00 (1.1625e+00)	Acc@1  72.00 ( 73.51)	Acc@5  90.00 ( 92.49)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.6348e+00 (1.1508e+00)	Acc@1  67.00 ( 73.74)	Acc@5  90.00 ( 92.65)
 * Acc@1 73.860 Acc@5 92.730
### epoch[82] execution time: 15.866838216781616
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.190 ( 0.190)	Data  0.148 ( 0.148)	Loss 3.7323e-02 (3.7323e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [83][ 10/391]	Time  0.047 ( 0.050)	Data  0.001 ( 0.015)	Loss 4.8798e-02 (4.5309e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [83][ 20/391]	Time  0.035 ( 0.043)	Data  0.001 ( 0.009)	Loss 3.0472e-02 (4.3068e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 8.3496e-02 (4.4385e-02)	Acc@1  96.09 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [83][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.1626e-02 (4.5806e-02)	Acc@1 100.00 ( 99.20)	Acc@5 100.00 ( 99.98)
Epoch: [83][ 50/391]	Time  0.035 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.1443e-02 (4.4660e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.98)
Epoch: [83][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.9215e-02 (4.4945e-02)	Acc@1  98.44 ( 99.28)	Acc@5 100.00 ( 99.99)
Epoch: [83][ 70/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.1270e-02 (4.5026e-02)	Acc@1 100.00 ( 99.28)	Acc@5 100.00 ( 99.99)
Epoch: [83][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.5624e-02 (4.4552e-02)	Acc@1 100.00 ( 99.32)	Acc@5 100.00 ( 99.99)
Epoch: [83][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 8.1848e-02 (4.4363e-02)	Acc@1  99.22 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [83][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.0106e-02 (4.4806e-02)	Acc@1 100.00 ( 99.31)	Acc@5 100.00 ( 99.99)
Epoch: [83][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1067e-02 (4.4199e-02)	Acc@1 100.00 ( 99.34)	Acc@5 100.00 ( 99.99)
Epoch: [83][120/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6234e-02 (4.4212e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [83][130/391]	Time  0.040 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7750e-02 (4.4645e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [83][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2633e-02 (4.4677e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [83][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.2469e-02 (4.4705e-02)	Acc@1  99.22 ( 99.35)	Acc@5 100.00 ( 99.99)
Epoch: [83][160/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0446e-02 (4.4826e-02)	Acc@1  98.44 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [83][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6844e-02 (4.4632e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [83][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.7527e-02 (4.4631e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.4169e-02 (4.4815e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][200/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.0802e-02 (4.4684e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][210/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6509e-02 (4.4363e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [83][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6509e-02 (4.4453e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.5502e-02 (4.4284e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.8157e-02 (4.4264e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][250/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.3392e-02 (4.4117e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1992e-02 (4.4040e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9124e-02 (4.4378e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4180e-02 (4.4230e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][290/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0771e-02 (4.4225e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][300/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6560e-02 (4.4257e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][310/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0924e-02 (4.4268e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1473e-02 (4.4278e-02)	Acc@1  98.44 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [83][330/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7455e-02 (4.4140e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7638e-02 (4.4084e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2542e-02 (4.4009e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9509e-02 (4.4171e-02)	Acc@1  98.44 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [83][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2856e-02 (4.4195e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [83][380/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0502e-02 (4.4257e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [83][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2368e-02 (4.4323e-02)	Acc@1  98.75 ( 99.36)	Acc@5 100.00 (100.00)
## e[83] optimizer.zero_grad (sum) time: 0.17974615097045898
## e[83]       loss.backward (sum) time: 4.0664849281311035
## e[83]      optimizer.step (sum) time: 1.233717441558838
## epoch[83] training(only) time: 13.899617671966553
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.1826e+00 (1.1826e+00)	Acc@1  70.00 ( 70.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.016 ( 0.029)	Loss 1.2861e+00 (1.2045e+00)	Acc@1  74.00 ( 72.82)	Acc@5  91.00 ( 91.27)
Test: [ 20/100]	Time  0.019 ( 0.023)	Loss 1.1309e+00 (1.1503e+00)	Acc@1  74.00 ( 73.76)	Acc@5  92.00 ( 92.24)
Test: [ 30/100]	Time  0.018 ( 0.021)	Loss 1.5947e+00 (1.1975e+00)	Acc@1  66.00 ( 73.32)	Acc@5  91.00 ( 92.03)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1689e+00 (1.1751e+00)	Acc@1  73.00 ( 73.51)	Acc@5  93.00 ( 92.56)
Test: [ 50/100]	Time  0.016 ( 0.020)	Loss 1.4941e+00 (1.1859e+00)	Acc@1  70.00 ( 73.20)	Acc@5  91.00 ( 92.39)
Test: [ 60/100]	Time  0.017 ( 0.020)	Loss 1.3008e+00 (1.1688e+00)	Acc@1  71.00 ( 73.44)	Acc@5  91.00 ( 92.51)
Test: [ 70/100]	Time  0.015 ( 0.020)	Loss 1.3652e+00 (1.1616e+00)	Acc@1  70.00 ( 73.54)	Acc@5  91.00 ( 92.52)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.2930e+00 (1.1614e+00)	Acc@1  73.00 ( 73.49)	Acc@5  90.00 ( 92.49)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.6094e+00 (1.1496e+00)	Acc@1  65.00 ( 73.67)	Acc@5  91.00 ( 92.66)
 * Acc@1 73.830 Acc@5 92.780
### epoch[83] execution time: 15.887208223342896
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.188 ( 0.188)	Data  0.142 ( 0.142)	Loss 3.0197e-02 (3.0197e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 5.0964e-02 (4.3138e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [84][ 20/391]	Time  0.031 ( 0.042)	Data  0.001 ( 0.008)	Loss 3.2776e-02 (4.0909e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [84][ 30/391]	Time  0.036 ( 0.040)	Data  0.001 ( 0.006)	Loss 6.0425e-02 (4.3956e-02)	Acc@1  99.22 ( 99.32)	Acc@5 100.00 (100.00)
Epoch: [84][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 5.1178e-02 (4.3333e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [84][ 50/391]	Time  0.042 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.0100e-02 (4.2806e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][ 60/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.004)	Loss 2.1118e-02 (4.2044e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [84][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.5450e-02 (4.3645e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 6.5674e-02 (4.3546e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [84][ 90/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.1433e-02 (4.3430e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][100/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9398e-02 (4.3895e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][110/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6957e-02 (4.3727e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4973e-02 (4.3847e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5776e-02 (4.3834e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 8.4534e-02 (4.4111e-02)	Acc@1  97.66 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][150/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.9892e-02 (4.3751e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [84][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.3304e-02 (4.4148e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][170/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.1697e-02 (4.4244e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][180/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.7424e-02 (4.4457e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6011e-02 (4.4244e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][200/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.7903e-02 (4.4003e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [84][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1052e-02 (4.4083e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][220/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2866e-02 (4.4242e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][230/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5065e-02 (4.4303e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [84][240/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.9692e-02 (4.4386e-02)	Acc@1  97.66 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][250/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2043e-02 (4.4332e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2501e-02 (4.4197e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][270/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1534e-02 (4.4187e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6244e-02 (4.4231e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [84][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.5613e-02 (4.4313e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6793e-02 (4.4433e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [84][310/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8361e-02 (4.4520e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [84][320/391]	Time  0.038 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (4.4342e-02)	Acc@1  99.22 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [84][330/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.7698e-02 (4.4573e-02)	Acc@1  97.66 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [84][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4210e-02 (4.4510e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [84][350/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6793e-02 (4.4628e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [84][360/391]	Time  0.051 ( 0.035)	Data  0.002 ( 0.002)	Loss 3.8544e-02 (4.4776e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [84][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.2675e-02 (4.4646e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [84][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6407e-02 (4.4647e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [84][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0201e-02 (4.4608e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
## e[84] optimizer.zero_grad (sum) time: 0.17955255508422852
## e[84]       loss.backward (sum) time: 4.044257402420044
## e[84]      optimizer.step (sum) time: 1.2519125938415527
## epoch[84] training(only) time: 13.869014978408813
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.1836e+00 (1.1836e+00)	Acc@1  70.00 ( 70.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.021 ( 0.031)	Loss 1.2627e+00 (1.2041e+00)	Acc@1  74.00 ( 73.45)	Acc@5  90.00 ( 91.27)
Test: [ 20/100]	Time  0.022 ( 0.024)	Loss 1.1025e+00 (1.1482e+00)	Acc@1  74.00 ( 74.05)	Acc@5  93.00 ( 92.14)
Test: [ 30/100]	Time  0.017 ( 0.022)	Loss 1.5869e+00 (1.1955e+00)	Acc@1  64.00 ( 73.35)	Acc@5  92.00 ( 92.03)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1670e+00 (1.1725e+00)	Acc@1  72.00 ( 73.49)	Acc@5  93.00 ( 92.56)
Test: [ 50/100]	Time  0.019 ( 0.021)	Loss 1.5312e+00 (1.1844e+00)	Acc@1  70.00 ( 73.25)	Acc@5  91.00 ( 92.35)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.3193e+00 (1.1674e+00)	Acc@1  72.00 ( 73.38)	Acc@5  93.00 ( 92.51)
Test: [ 70/100]	Time  0.023 ( 0.020)	Loss 1.4043e+00 (1.1612e+00)	Acc@1  71.00 ( 73.46)	Acc@5  91.00 ( 92.52)
Test: [ 80/100]	Time  0.022 ( 0.020)	Loss 1.3193e+00 (1.1624e+00)	Acc@1  72.00 ( 73.43)	Acc@5  90.00 ( 92.44)
Test: [ 90/100]	Time  0.014 ( 0.019)	Loss 1.6084e+00 (1.1503e+00)	Acc@1  66.00 ( 73.64)	Acc@5  92.00 ( 92.63)
 * Acc@1 73.840 Acc@5 92.730
### epoch[84] execution time: 15.865228652954102
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.193 ( 0.193)	Data  0.145 ( 0.145)	Loss 4.5288e-02 (4.5288e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.037 ( 0.050)	Data  0.001 ( 0.015)	Loss 7.1289e-02 (4.9749e-02)	Acc@1  97.66 ( 99.15)	Acc@5 100.00 (100.00)
Epoch: [85][ 20/391]	Time  0.034 ( 0.043)	Data  0.002 ( 0.009)	Loss 2.4048e-02 (4.3767e-02)	Acc@1 100.00 ( 99.33)	Acc@5 100.00 (100.00)
Epoch: [85][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.1840e-02 (4.5452e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [85][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.5654e-02 (4.3811e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][ 50/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.4668e-02 (4.4433e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][ 60/391]	Time  0.033 ( 0.038)	Data  0.001 ( 0.004)	Loss 2.8107e-02 (4.4221e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.1157e-02 (4.4240e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][ 80/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.6428e-02 (4.4448e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 9.4727e-02 (4.5351e-02)	Acc@1  96.09 ( 99.34)	Acc@5 100.00 (100.00)
Epoch: [85][100/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9734e-02 (4.5045e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [85][110/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.8807e-02 (4.5030e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [85][120/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.7750e-02 (4.4992e-02)	Acc@1 100.00 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [85][130/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.1504e-02 (4.5474e-02)	Acc@1 100.00 ( 99.35)	Acc@5 100.00 (100.00)
Epoch: [85][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4821e-02 (4.5276e-02)	Acc@1  99.22 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [85][150/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.1300e-02 (4.5713e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [85][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1860e-02 (4.5580e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [85][170/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6285e-02 (4.5350e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.0446e-02 (4.5324e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3630e-02 (4.5446e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [85][200/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.9739e-02 (4.5159e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.4241e-02 (4.4933e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [85][220/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3722e-02 (4.4745e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][230/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.7322e-02 (4.4622e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][240/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.9297e-02 (4.4638e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][250/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.8086e-02 (4.4628e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][260/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1504e-02 (4.4288e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [85][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8635e-02 (4.4300e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [85][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5023e-02 (4.4531e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][290/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.0100e-02 (4.4676e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][300/391]	Time  0.036 ( 0.035)	Data  0.002 ( 0.002)	Loss 4.2145e-02 (4.4667e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][310/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4688e-02 (4.4691e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][320/391]	Time  0.036 ( 0.035)	Data  0.002 ( 0.002)	Loss 4.1473e-02 (4.4441e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [85][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3610e-02 (4.4349e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [85][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8542e-02 (4.4494e-02)	Acc@1  97.66 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][350/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1260e-02 (4.4668e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][360/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3447e-02 (4.4627e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [85][370/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6122e-02 (4.4637e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][380/391]	Time  0.033 ( 0.035)	Data  0.000 ( 0.002)	Loss 7.5195e-02 (4.4680e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [85][390/391]	Time  0.028 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.6589e-02 (4.4750e-02)	Acc@1  98.75 ( 99.40)	Acc@5 100.00 (100.00)
## e[85] optimizer.zero_grad (sum) time: 0.17837119102478027
## e[85]       loss.backward (sum) time: 3.94968318939209
## e[85]      optimizer.step (sum) time: 1.283388376235962
## epoch[85] training(only) time: 13.853027820587158
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.1582e+00 (1.1582e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.021 ( 0.030)	Loss 1.2773e+00 (1.2017e+00)	Acc@1  74.00 ( 73.27)	Acc@5  91.00 ( 91.64)
Test: [ 20/100]	Time  0.016 ( 0.024)	Loss 1.1221e+00 (1.1526e+00)	Acc@1  76.00 ( 74.14)	Acc@5  93.00 ( 92.29)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5732e+00 (1.1992e+00)	Acc@1  64.00 ( 73.52)	Acc@5  93.00 ( 92.00)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1680e+00 (1.1763e+00)	Acc@1  71.00 ( 73.56)	Acc@5  94.00 ( 92.63)
Test: [ 50/100]	Time  0.017 ( 0.020)	Loss 1.5000e+00 (1.1866e+00)	Acc@1  70.00 ( 73.33)	Acc@5  90.00 ( 92.33)
Test: [ 60/100]	Time  0.016 ( 0.020)	Loss 1.3066e+00 (1.1691e+00)	Acc@1  71.00 ( 73.49)	Acc@5  92.00 ( 92.51)
Test: [ 70/100]	Time  0.021 ( 0.020)	Loss 1.4062e+00 (1.1622e+00)	Acc@1  69.00 ( 73.51)	Acc@5  91.00 ( 92.59)
Test: [ 80/100]	Time  0.016 ( 0.019)	Loss 1.3291e+00 (1.1625e+00)	Acc@1  73.00 ( 73.52)	Acc@5  90.00 ( 92.62)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5947e+00 (1.1506e+00)	Acc@1  67.00 ( 73.71)	Acc@5  92.00 ( 92.80)
 * Acc@1 73.910 Acc@5 92.880
### epoch[85] execution time: 15.826990127563477
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.179 ( 0.179)	Data  0.140 ( 0.140)	Loss 3.4515e-02 (3.4515e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [86][ 10/391]	Time  0.035 ( 0.048)	Data  0.001 ( 0.014)	Loss 3.8818e-02 (3.9370e-02)	Acc@1 100.00 ( 99.79)	Acc@5 100.00 (100.00)
Epoch: [86][ 20/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.008)	Loss 4.0649e-02 (4.1977e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [86][ 30/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.006)	Loss 4.0497e-02 (4.1616e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [86][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 5.6671e-02 (4.2997e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [86][ 50/391]	Time  0.031 ( 0.038)	Data  0.002 ( 0.004)	Loss 6.0089e-02 (4.4140e-02)	Acc@1  99.22 ( 99.37)	Acc@5 100.00 (100.00)
Epoch: [86][ 60/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 5.4596e-02 (4.4085e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [86][ 70/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.6956e-02 (4.4054e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [86][ 80/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 1.8997e-02 (4.3669e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [86][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5746e-02 (4.3397e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [86][100/391]	Time  0.038 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.5593e-02 (4.2934e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][110/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.2297e-02 (4.3077e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.4373e-02 (4.3344e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][130/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.2959e-02 (4.3339e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [86][140/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4108e-02 (4.3160e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][150/391]	Time  0.039 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.2917e-02 (4.3357e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][160/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4882e-02 (4.3231e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1443e-02 (4.2880e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.0385e-02 (4.2634e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][190/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5675e-02 (4.2660e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.5848e-02 (4.2617e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.3875e-02 (4.2526e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9500e-02 (4.2489e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][230/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.8054e-02 (4.2748e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][240/391]	Time  0.037 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3110e-02 (4.2863e-02)	Acc@1  97.66 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [86][250/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1952e-02 (4.2860e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [86][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.0964e-02 (4.3097e-02)	Acc@1  98.44 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [86][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9408e-02 (4.2836e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][280/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7302e-02 (4.2626e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][290/391]	Time  0.034 ( 0.035)	Data  0.002 ( 0.002)	Loss 4.6204e-02 (4.2633e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [86][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.7668e-02 (4.2940e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3091e-02 (4.3056e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [86][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.6458e-02 (4.3032e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5786e-02 (4.3118e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [86][340/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5349e-02 (4.3382e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [86][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3711e-02 (4.3659e-02)	Acc@1  98.44 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [86][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9988e-02 (4.3705e-02)	Acc@1  99.22 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [86][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6774e-02 (4.3717e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [86][380/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.1910e-02 (4.3667e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [86][390/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.3274e-02 (4.3770e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
## e[86] optimizer.zero_grad (sum) time: 0.17881035804748535
## e[86]       loss.backward (sum) time: 3.9637341499328613
## e[86]      optimizer.step (sum) time: 1.2578372955322266
## epoch[86] training(only) time: 13.907604694366455
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.1748e+00 (1.1748e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.019 ( 0.030)	Loss 1.2871e+00 (1.2025e+00)	Acc@1  73.00 ( 73.18)	Acc@5  90.00 ( 91.64)
Test: [ 20/100]	Time  0.022 ( 0.024)	Loss 1.1250e+00 (1.1461e+00)	Acc@1  75.00 ( 73.95)	Acc@5  93.00 ( 92.43)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5947e+00 (1.1920e+00)	Acc@1  65.00 ( 73.48)	Acc@5  91.00 ( 92.06)
Test: [ 40/100]	Time  0.016 ( 0.021)	Loss 1.1602e+00 (1.1678e+00)	Acc@1  72.00 ( 73.63)	Acc@5  94.00 ( 92.66)
Test: [ 50/100]	Time  0.014 ( 0.020)	Loss 1.4844e+00 (1.1786e+00)	Acc@1  70.00 ( 73.37)	Acc@5  91.00 ( 92.45)
Test: [ 60/100]	Time  0.021 ( 0.020)	Loss 1.3037e+00 (1.1618e+00)	Acc@1  71.00 ( 73.49)	Acc@5  92.00 ( 92.56)
Test: [ 70/100]	Time  0.024 ( 0.020)	Loss 1.3496e+00 (1.1551e+00)	Acc@1  72.00 ( 73.61)	Acc@5  92.00 ( 92.56)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.3242e+00 (1.1561e+00)	Acc@1  73.00 ( 73.58)	Acc@5  90.00 ( 92.48)
Test: [ 90/100]	Time  0.016 ( 0.019)	Loss 1.6211e+00 (1.1456e+00)	Acc@1  66.00 ( 73.76)	Acc@5  90.00 ( 92.64)
 * Acc@1 73.930 Acc@5 92.730
### epoch[86] execution time: 15.88536787033081
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.189 ( 0.189)	Data  0.147 ( 0.147)	Loss 2.6291e-02 (2.6291e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [87][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 5.3375e-02 (4.9318e-02)	Acc@1  99.22 ( 99.01)	Acc@5 100.00 (100.00)
Epoch: [87][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.009)	Loss 5.5847e-02 (4.6922e-02)	Acc@1  99.22 ( 99.18)	Acc@5 100.00 (100.00)
Epoch: [87][ 30/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.006)	Loss 3.0273e-02 (4.6495e-02)	Acc@1 100.00 ( 99.27)	Acc@5 100.00 (100.00)
Epoch: [87][ 40/391]	Time  0.037 ( 0.039)	Data  0.001 ( 0.005)	Loss 4.3640e-02 (4.6390e-02)	Acc@1  99.22 ( 99.29)	Acc@5 100.00 (100.00)
Epoch: [87][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 3.7537e-02 (4.4967e-02)	Acc@1 100.00 ( 99.36)	Acc@5 100.00 (100.00)
Epoch: [87][ 60/391]	Time  0.037 ( 0.038)	Data  0.001 ( 0.004)	Loss 3.5278e-02 (4.3755e-02)	Acc@1  99.22 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [87][ 70/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.0457e-02 (4.3720e-02)	Acc@1 100.00 ( 99.38)	Acc@5 100.00 (100.00)
Epoch: [87][ 80/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.1748e-02 (4.3851e-02)	Acc@1 100.00 ( 99.39)	Acc@5 100.00 (100.00)
Epoch: [87][ 90/391]	Time  0.035 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.3650e-02 (4.2933e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][100/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 3.7323e-02 (4.3134e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][110/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.0771e-02 (4.2704e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [87][120/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.4912e-02 (4.2781e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [87][130/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 7.7881e-02 (4.3350e-02)	Acc@1  97.66 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][140/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.6284e-02 (4.3423e-02)	Acc@1  98.44 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [87][150/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.0201e-02 (4.3371e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][160/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.0059e-02 (4.3616e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9622e-02 (4.3628e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][180/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6926e-02 (4.3723e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][190/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.7435e-02 (4.3477e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.9408e-02 (4.3634e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][210/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.7322e-02 (4.3983e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][220/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.8298e-02 (4.4104e-02)	Acc@1  98.44 ( 99.42)	Acc@5  99.22 (100.00)
Epoch: [87][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 6.4514e-02 (4.3985e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5339e-02 (4.3884e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][250/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.6814e-02 (4.4073e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][260/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2928e-02 (4.4114e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][270/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.6335e-02 (4.4023e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5349e-02 (4.4073e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [87][290/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9434e-02 (4.4118e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][300/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.2622e-02 (4.4298e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3600e-02 (4.4316e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][320/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5736e-02 (4.4387e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][330/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1555e-02 (4.4381e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][340/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3966e-02 (4.4269e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [87][350/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2734e-02 (4.4263e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.3295e-02 (4.4261e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.9465e-02 (4.4116e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.9805e-02 (4.4148e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [87][390/391]	Time  0.024 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.8665e-02 (4.4335e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
## e[87] optimizer.zero_grad (sum) time: 0.1808335781097412
## e[87]       loss.backward (sum) time: 4.0461366176605225
## e[87]      optimizer.step (sum) time: 1.2670862674713135
## epoch[87] training(only) time: 13.882329225540161
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.1670e+00 (1.1670e+00)	Acc@1  72.00 ( 72.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.021 ( 0.030)	Loss 1.3027e+00 (1.2092e+00)	Acc@1  74.00 ( 73.27)	Acc@5  91.00 ( 91.27)
Test: [ 20/100]	Time  0.020 ( 0.024)	Loss 1.1152e+00 (1.1581e+00)	Acc@1  75.00 ( 73.86)	Acc@5  93.00 ( 92.05)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.5898e+00 (1.2070e+00)	Acc@1  65.00 ( 73.19)	Acc@5  91.00 ( 91.74)
Test: [ 40/100]	Time  0.024 ( 0.021)	Loss 1.1836e+00 (1.1827e+00)	Acc@1  72.00 ( 73.34)	Acc@5  94.00 ( 92.46)
Test: [ 50/100]	Time  0.015 ( 0.020)	Loss 1.5137e+00 (1.1946e+00)	Acc@1  67.00 ( 73.14)	Acc@5  90.00 ( 92.29)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.2930e+00 (1.1752e+00)	Acc@1  72.00 ( 73.38)	Acc@5  92.00 ( 92.44)
Test: [ 70/100]	Time  0.017 ( 0.020)	Loss 1.3936e+00 (1.1671e+00)	Acc@1  68.00 ( 73.38)	Acc@5  92.00 ( 92.51)
Test: [ 80/100]	Time  0.018 ( 0.020)	Loss 1.3535e+00 (1.1682e+00)	Acc@1  72.00 ( 73.32)	Acc@5  90.00 ( 92.49)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5967e+00 (1.1570e+00)	Acc@1  65.00 ( 73.47)	Acc@5  91.00 ( 92.67)
 * Acc@1 73.640 Acc@5 92.770
### epoch[87] execution time: 15.86260461807251
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.188 ( 0.188)	Data  0.147 ( 0.147)	Loss 4.7699e-02 (4.7699e-02)	Acc@1  99.22 ( 99.22)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.033 ( 0.049)	Data  0.001 ( 0.015)	Loss 2.9266e-02 (4.2515e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][ 20/391]	Time  0.034 ( 0.042)	Data  0.001 ( 0.008)	Loss 3.1860e-02 (4.0972e-02)	Acc@1  99.22 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [88][ 30/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.006)	Loss 6.9153e-02 (4.3406e-02)	Acc@1  99.22 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][ 40/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 5.1239e-02 (4.2714e-02)	Acc@1  98.44 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [88][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.5227e-02 (4.2753e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [88][ 60/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.4210e-02 (4.3524e-02)	Acc@1 100.00 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [88][ 70/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.004)	Loss 6.6833e-02 (4.3571e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][ 80/391]	Time  0.034 ( 0.037)	Data  0.001 ( 0.003)	Loss 4.8187e-02 (4.3899e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [88][ 90/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6041e-02 (4.4129e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [88][100/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.4810e-02 (4.3829e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][110/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9978e-02 (4.3985e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [88][120/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.7546e-02 (4.4294e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][130/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.9154e-02 (4.4440e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][140/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.3605e-02 (4.4292e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [88][150/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 6.8848e-02 (4.4376e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][160/391]	Time  0.029 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5034e-02 (4.4678e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [88][170/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 5.9723e-02 (4.4921e-02)	Acc@1 100.00 ( 99.40)	Acc@5 100.00 (100.00)
Epoch: [88][180/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 1.5152e-02 (4.4683e-02)	Acc@1 100.00 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [88][190/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6255e-02 (4.4846e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][200/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.5828e-02 (4.4586e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][210/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.4302e-02 (4.4386e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][220/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.0121e-02 (4.4223e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [88][230/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.1586e-02 (4.4023e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][240/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6774e-02 (4.4007e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [88][250/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7892e-02 (4.3818e-02)	Acc@1  99.22 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [88][260/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.8208e-02 (4.4186e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][270/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2623e-02 (4.4292e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][280/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2104e-02 (4.4291e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][290/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4403e-02 (4.4187e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [88][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.2776e-02 (4.4396e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][310/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 7.0618e-02 (4.4245e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][320/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.7506e-02 (4.4210e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][330/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8553e-02 (4.4205e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.7129e-02 (4.4230e-02)	Acc@1 100.00 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [88][350/391]	Time  0.036 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.3232e-02 (4.4487e-02)	Acc@1  98.44 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.5624e-02 (4.4556e-02)	Acc@1  99.22 ( 99.41)	Acc@5 100.00 (100.00)
Epoch: [88][370/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 2.7344e-02 (4.4301e-02)	Acc@1 100.00 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][380/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.8798e-02 (4.4302e-02)	Acc@1  99.22 ( 99.42)	Acc@5 100.00 (100.00)
Epoch: [88][390/391]	Time  0.027 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.5725e-02 (4.4191e-02)	Acc@1  98.75 ( 99.42)	Acc@5 100.00 (100.00)
## e[88] optimizer.zero_grad (sum) time: 0.1786806583404541
## e[88]       loss.backward (sum) time: 4.089832067489624
## e[88]      optimizer.step (sum) time: 1.2546184062957764
## epoch[88] training(only) time: 13.853402853012085
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 1.1885e+00 (1.1885e+00)	Acc@1  71.00 ( 71.00)	Acc@5  92.00 ( 92.00)
Test: [ 10/100]	Time  0.020 ( 0.031)	Loss 1.2822e+00 (1.2057e+00)	Acc@1  72.00 ( 73.45)	Acc@5  91.00 ( 91.55)
Test: [ 20/100]	Time  0.023 ( 0.024)	Loss 1.1182e+00 (1.1481e+00)	Acc@1  75.00 ( 74.10)	Acc@5  93.00 ( 92.24)
Test: [ 30/100]	Time  0.015 ( 0.022)	Loss 1.5830e+00 (1.1956e+00)	Acc@1  66.00 ( 73.61)	Acc@5  91.00 ( 91.87)
Test: [ 40/100]	Time  0.020 ( 0.021)	Loss 1.1885e+00 (1.1751e+00)	Acc@1  71.00 ( 73.66)	Acc@5  93.00 ( 92.44)
Test: [ 50/100]	Time  0.017 ( 0.021)	Loss 1.4932e+00 (1.1837e+00)	Acc@1  69.00 ( 73.31)	Acc@5  90.00 ( 92.29)
Test: [ 60/100]	Time  0.018 ( 0.020)	Loss 1.3203e+00 (1.1672e+00)	Acc@1  72.00 ( 73.46)	Acc@5  93.00 ( 92.46)
Test: [ 70/100]	Time  0.018 ( 0.020)	Loss 1.3916e+00 (1.1600e+00)	Acc@1  72.00 ( 73.55)	Acc@5  92.00 ( 92.51)
Test: [ 80/100]	Time  0.015 ( 0.019)	Loss 1.3047e+00 (1.1605e+00)	Acc@1  73.00 ( 73.51)	Acc@5  89.00 ( 92.52)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.5713e+00 (1.1484e+00)	Acc@1  67.00 ( 73.73)	Acc@5  91.00 ( 92.70)
 * Acc@1 73.850 Acc@5 92.780
### epoch[88] execution time: 15.832489490509033
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.195 ( 0.195)	Data  0.155 ( 0.155)	Loss 3.1708e-02 (3.1708e-02)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.035 ( 0.050)	Data  0.001 ( 0.016)	Loss 6.7993e-02 (4.4572e-02)	Acc@1 100.00 ( 99.72)	Acc@5 100.00 (100.00)
Epoch: [89][ 20/391]	Time  0.034 ( 0.043)	Data  0.001 ( 0.009)	Loss 4.9713e-02 (4.4679e-02)	Acc@1  98.44 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][ 30/391]	Time  0.037 ( 0.040)	Data  0.001 ( 0.007)	Loss 5.2643e-02 (4.3683e-02)	Acc@1  98.44 ( 99.52)	Acc@5 100.00 (100.00)
Epoch: [89][ 40/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.005)	Loss 3.3356e-02 (4.1604e-02)	Acc@1 100.00 ( 99.54)	Acc@5 100.00 (100.00)
Epoch: [89][ 50/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.005)	Loss 5.2490e-02 (4.2563e-02)	Acc@1  99.22 ( 99.53)	Acc@5 100.00 (100.00)
Epoch: [89][ 60/391]	Time  0.034 ( 0.038)	Data  0.001 ( 0.004)	Loss 4.5532e-02 (4.2285e-02)	Acc@1  99.22 ( 99.51)	Acc@5 100.00 (100.00)
Epoch: [89][ 70/391]	Time  0.037 ( 0.037)	Data  0.001 ( 0.004)	Loss 3.1342e-02 (4.2315e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][ 80/391]	Time  0.036 ( 0.037)	Data  0.001 ( 0.003)	Loss 5.0079e-02 (4.2592e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][ 90/391]	Time  0.033 ( 0.037)	Data  0.001 ( 0.003)	Loss 2.5848e-02 (4.2489e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][100/391]	Time  0.032 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.6835e-02 (4.2434e-02)	Acc@1  98.44 ( 99.50)	Acc@5 100.00 (100.00)
Epoch: [89][110/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.3966e-02 (4.2441e-02)	Acc@1 100.00 ( 99.49)	Acc@5 100.00 (100.00)
Epoch: [89][120/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.003)	Loss 3.1952e-02 (4.2685e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][130/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 2.5482e-02 (4.2864e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][140/391]	Time  0.037 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.9143e-02 (4.3022e-02)	Acc@1  99.22 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [89][150/391]	Time  0.036 ( 0.036)	Data  0.001 ( 0.003)	Loss 5.9601e-02 (4.3366e-02)	Acc@1  98.44 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][160/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.6600e-02 (4.3252e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][170/391]	Time  0.036 ( 0.036)	Data  0.000 ( 0.003)	Loss 3.8300e-02 (4.3137e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][180/391]	Time  0.035 ( 0.036)	Data  0.001 ( 0.003)	Loss 4.3213e-02 (4.3162e-02)	Acc@1 100.00 ( 99.47)	Acc@5 100.00 (100.00)
Epoch: [89][190/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.5370e-02 (4.2959e-02)	Acc@1 100.00 ( 99.48)	Acc@5 100.00 (100.00)
Epoch: [89][200/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.4912e-02 (4.3096e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][210/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.6957e-02 (4.3127e-02)	Acc@1  99.22 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][220/391]	Time  0.033 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.5192e-02 (4.2949e-02)	Acc@1 100.00 ( 99.46)	Acc@5 100.00 (100.00)
Epoch: [89][230/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 3.1799e-02 (4.3240e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][240/391]	Time  0.034 ( 0.036)	Data  0.001 ( 0.002)	Loss 7.4219e-02 (4.3371e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][250/391]	Time  0.031 ( 0.036)	Data  0.001 ( 0.002)	Loss 2.7954e-02 (4.3189e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][260/391]	Time  0.031 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.6072e-02 (4.3168e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][270/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 3.9001e-02 (4.3146e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][280/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3162e-02 (4.3033e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][290/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4952e-02 (4.3290e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][300/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4535e-02 (4.3273e-02)	Acc@1  98.44 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][310/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.3772e-02 (4.3395e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][320/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2612e-02 (4.3369e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][330/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.4220e-02 (4.3542e-02)	Acc@1  99.22 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][340/391]	Time  0.034 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1504e-02 (4.3530e-02)	Acc@1 100.00 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][350/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.4148e-02 (4.3455e-02)	Acc@1  99.22 ( 99.45)	Acc@5 100.00 (100.00)
Epoch: [89][360/391]	Time  0.033 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.4871e-02 (4.3478e-02)	Acc@1  98.44 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][370/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2725e-02 (4.3572e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
Epoch: [89][380/391]	Time  0.035 ( 0.035)	Data  0.001 ( 0.002)	Loss 5.2643e-02 (4.3657e-02)	Acc@1  99.22 ( 99.43)	Acc@5 100.00 (100.00)
Epoch: [89][390/391]	Time  0.025 ( 0.035)	Data  0.001 ( 0.002)	Loss 6.0516e-02 (4.3689e-02)	Acc@1 100.00 ( 99.44)	Acc@5 100.00 (100.00)
## e[89] optimizer.zero_grad (sum) time: 0.17938446998596191
## e[89]       loss.backward (sum) time: 4.039079904556274
## e[89]      optimizer.step (sum) time: 1.2782313823699951
## epoch[89] training(only) time: 13.863876342773438
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.1855e+00 (1.1855e+00)	Acc@1  72.00 ( 72.00)	Acc@5  93.00 ( 93.00)
Test: [ 10/100]	Time  0.019 ( 0.029)	Loss 1.2861e+00 (1.1979e+00)	Acc@1  73.00 ( 73.09)	Acc@5  91.00 ( 91.45)
Test: [ 20/100]	Time  0.023 ( 0.024)	Loss 1.1172e+00 (1.1460e+00)	Acc@1  75.00 ( 73.86)	Acc@5  92.00 ( 92.19)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 1.5762e+00 (1.1945e+00)	Acc@1  66.00 ( 73.19)	Acc@5  92.00 ( 91.87)
Test: [ 40/100]	Time  0.015 ( 0.021)	Loss 1.1699e+00 (1.1720e+00)	Acc@1  71.00 ( 73.39)	Acc@5  95.00 ( 92.49)
Test: [ 50/100]	Time  0.024 ( 0.020)	Loss 1.4990e+00 (1.1831e+00)	Acc@1  70.00 ( 73.18)	Acc@5  90.00 ( 92.31)
Test: [ 60/100]	Time  0.015 ( 0.020)	Loss 1.3076e+00 (1.1650e+00)	Acc@1  71.00 ( 73.46)	Acc@5  92.00 ( 92.49)
Test: [ 70/100]	Time  0.015 ( 0.019)	Loss 1.3682e+00 (1.1579e+00)	Acc@1  70.00 ( 73.59)	Acc@5  92.00 ( 92.54)
Test: [ 80/100]	Time  0.019 ( 0.019)	Loss 1.2939e+00 (1.1583e+00)	Acc@1  73.00 ( 73.51)	Acc@5  90.00 ( 92.56)
Test: [ 90/100]	Time  0.015 ( 0.019)	Loss 1.6240e+00 (1.1469e+00)	Acc@1  67.00 ( 73.67)	Acc@5  90.00 ( 92.67)
 * Acc@1 73.870 Acc@5 92.780
### epoch[89] execution time: 15.816989660263062
### Training complete:
#### total training(only) time: 1250.7276239395142
##### Total run time: 1432.692579984665
