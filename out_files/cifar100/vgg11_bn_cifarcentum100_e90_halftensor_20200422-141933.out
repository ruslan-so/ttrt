# Model: vgg11_bn
# Dataset: cifarcentum
# Batch size: 128
# Freezeout: False
# Low precision: True
# Epochs: 90
# Initial learning rate: 0.1
# module_name: models_cifarcentum.vgg
<function vgg11_bn at 0x7f959aa2d0d0>
# model requested: 'vgg11_bn'
# printing out the model
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(inplace=True)
    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU(inplace=True)
    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (27): ReLU(inplace=True)
    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (classifier): Sequential(
    (0): Linear(in_features=512, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=100, bias=True)
  )
)
# model is low precision
# Model: vgg11_bn
# Dataset: cifarcentum
# Freezeout: False
# Low precision: True
# Initial learning rate: 0.1
# Criterion: CrossEntropyLoss()
# Optimizer: SGD
#	lr 0.1
#	momentum 0.9
#	dampening 0
#	weight_decay 0.0001
#	nesterov False
CIFAR100 loading and normalization
==> Preparing data..
# CIFAR100 / low precision
Files already downloaded and verified
Files already downloaded and verified
#--> Training data: <--#
#-->>
EPOCH 0
# current learning rate: 0.1
# Switched to train mode...
Epoch: [0][  0/391]	Time  3.243 ( 3.243)	Data  0.104 ( 0.104)	Loss 4.6211e+00 (4.6211e+00)	Acc@1   2.34 (  2.34)	Acc@5   7.03 (  7.03)
Epoch: [0][ 10/391]	Time  0.021 ( 0.317)	Data  0.001 ( 0.011)	Loss 4.6133e+00 (4.7354e+00)	Acc@1   3.12 (  1.92)	Acc@5   7.81 (  5.40)
Epoch: [0][ 20/391]	Time  0.021 ( 0.177)	Data  0.001 ( 0.007)	Loss 4.6289e+00 (4.6789e+00)	Acc@1   2.34 (  1.79)	Acc@5   7.81 (  6.10)
Epoch: [0][ 30/391]	Time  0.019 ( 0.128)	Data  0.002 ( 0.005)	Loss 4.6406e+00 (4.6536e+00)	Acc@1   0.00 (  1.66)	Acc@5   7.81 (  6.12)
Epoch: [0][ 40/391]	Time  0.023 ( 0.103)	Data  0.001 ( 0.004)	Loss 4.5781e+00 (4.6329e+00)	Acc@1   1.56 (  1.75)	Acc@5   7.03 (  6.50)
Epoch: [0][ 50/391]	Time  0.020 ( 0.088)	Data  0.001 ( 0.004)	Loss 4.4648e+00 (4.6158e+00)	Acc@1   1.56 (  1.82)	Acc@5   7.81 (  6.74)
Epoch: [0][ 60/391]	Time  0.026 ( 0.078)	Data  0.001 ( 0.004)	Loss 4.4805e+00 (4.6023e+00)	Acc@1   0.78 (  1.88)	Acc@5   8.59 (  7.06)
Epoch: [0][ 70/391]	Time  0.020 ( 0.070)	Data  0.001 ( 0.003)	Loss 4.5391e+00 (4.5874e+00)	Acc@1   3.12 (  1.86)	Acc@5   3.91 (  7.49)
Epoch: [0][ 80/391]	Time  0.029 ( 0.065)	Data  0.000 ( 0.003)	Loss 4.4102e+00 (4.5696e+00)	Acc@1   0.78 (  1.81)	Acc@5   9.38 (  7.75)
Epoch: [0][ 90/391]	Time  0.021 ( 0.060)	Data  0.000 ( 0.003)	Loss 4.4336e+00 (4.5559e+00)	Acc@1   2.34 (  1.78)	Acc@5  10.16 (  7.92)
Epoch: [0][100/391]	Time  0.030 ( 0.057)	Data  0.003 ( 0.003)	Loss 4.3945e+00 (4.5427e+00)	Acc@1   3.91 (  1.85)	Acc@5  13.28 (  8.15)
Epoch: [0][110/391]	Time  0.021 ( 0.054)	Data  0.001 ( 0.003)	Loss 4.3320e+00 (4.5298e+00)	Acc@1   2.34 (  1.91)	Acc@5  16.41 (  8.47)
Epoch: [0][120/391]	Time  0.021 ( 0.051)	Data  0.002 ( 0.003)	Loss 4.4492e+00 (4.5191e+00)	Acc@1   1.56 (  1.92)	Acc@5   8.59 (  8.63)
Epoch: [0][130/391]	Time  0.021 ( 0.049)	Data  0.001 ( 0.003)	Loss 4.3203e+00 (4.5084e+00)	Acc@1   2.34 (  1.95)	Acc@5  14.06 (  8.84)
Epoch: [0][140/391]	Time  0.021 ( 0.048)	Data  0.000 ( 0.003)	Loss 4.4375e+00 (4.4973e+00)	Acc@1   1.56 (  2.06)	Acc@5   6.25 (  9.12)
Epoch: [0][150/391]	Time  0.023 ( 0.046)	Data  0.000 ( 0.003)	Loss 4.2500e+00 (4.4899e+00)	Acc@1   3.12 (  2.05)	Acc@5  14.06 (  9.26)
Epoch: [0][160/391]	Time  0.022 ( 0.045)	Data  0.002 ( 0.003)	Loss 4.4141e+00 (4.4806e+00)	Acc@1   1.56 (  2.09)	Acc@5  10.94 (  9.49)
Epoch: [0][170/391]	Time  0.020 ( 0.044)	Data  0.001 ( 0.003)	Loss 4.3398e+00 (4.4716e+00)	Acc@1   0.78 (  2.15)	Acc@5  10.94 (  9.69)
Epoch: [0][180/391]	Time  0.021 ( 0.043)	Data  0.001 ( 0.003)	Loss 4.3398e+00 (4.4655e+00)	Acc@1   1.56 (  2.14)	Acc@5  15.62 (  9.79)
Epoch: [0][190/391]	Time  0.037 ( 0.042)	Data  0.001 ( 0.003)	Loss 4.2539e+00 (4.4576e+00)	Acc@1   3.12 (  2.17)	Acc@5  17.19 (  9.99)
Epoch: [0][200/391]	Time  0.038 ( 0.041)	Data  0.001 ( 0.002)	Loss 4.2969e+00 (4.4512e+00)	Acc@1   2.34 (  2.21)	Acc@5  11.72 ( 10.18)
Epoch: [0][210/391]	Time  0.034 ( 0.040)	Data  0.005 ( 0.002)	Loss 4.3555e+00 (4.4457e+00)	Acc@1   1.56 (  2.22)	Acc@5   9.38 ( 10.30)
Epoch: [0][220/391]	Time  0.021 ( 0.040)	Data  0.001 ( 0.002)	Loss 4.2305e+00 (4.4390e+00)	Acc@1   6.25 (  2.28)	Acc@5  14.84 ( 10.47)
Epoch: [0][230/391]	Time  0.020 ( 0.039)	Data  0.001 ( 0.002)	Loss 4.2695e+00 (4.4324e+00)	Acc@1   3.91 (  2.32)	Acc@5  18.75 ( 10.65)
Epoch: [0][240/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.3320e+00 (4.4269e+00)	Acc@1   4.69 (  2.38)	Acc@5  14.84 ( 10.85)
Epoch: [0][250/391]	Time  0.020 ( 0.038)	Data  0.001 ( 0.002)	Loss 4.2578e+00 (4.4223e+00)	Acc@1   3.12 (  2.41)	Acc@5  13.28 ( 11.01)
Epoch: [0][260/391]	Time  0.024 ( 0.037)	Data  0.001 ( 0.002)	Loss 4.2344e+00 (4.4149e+00)	Acc@1   0.78 (  2.45)	Acc@5  15.62 ( 11.18)
Epoch: [0][270/391]	Time  0.019 ( 0.037)	Data  0.001 ( 0.002)	Loss 4.3594e+00 (4.4095e+00)	Acc@1   3.12 (  2.47)	Acc@5  13.28 ( 11.35)
Epoch: [0][280/391]	Time  0.021 ( 0.036)	Data  0.001 ( 0.002)	Loss 4.1797e+00 (4.4047e+00)	Acc@1   3.12 (  2.51)	Acc@5  14.84 ( 11.47)
Epoch: [0][290/391]	Time  0.031 ( 0.036)	Data  0.000 ( 0.002)	Loss 4.3984e+00 (4.3990e+00)	Acc@1   3.91 (  2.56)	Acc@5  13.28 ( 11.63)
Epoch: [0][300/391]	Time  0.021 ( 0.036)	Data  0.002 ( 0.002)	Loss 4.2773e+00 (4.3962e+00)	Acc@1   3.91 (  2.57)	Acc@5  21.09 ( 11.69)
Epoch: [0][310/391]	Time  0.030 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1797e+00 (4.3893e+00)	Acc@1   3.91 (  2.61)	Acc@5  17.19 ( 11.86)
Epoch: [0][320/391]	Time  0.020 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.1133e+00 (4.3844e+00)	Acc@1   6.25 (  2.65)	Acc@5  20.31 ( 11.98)
Epoch: [0][330/391]	Time  0.020 ( 0.035)	Data  0.001 ( 0.002)	Loss 4.2383e+00 (4.3797e+00)	Acc@1   4.69 (  2.66)	Acc@5  14.06 ( 12.07)
Epoch: [0][340/391]	Time  0.021 ( 0.034)	Data  0.000 ( 0.002)	Loss 4.2461e+00 (4.3750e+00)	Acc@1   1.56 (  2.69)	Acc@5  16.41 ( 12.17)
Epoch: [0][350/391]	Time  0.032 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.2656e+00 (4.3712e+00)	Acc@1   2.34 (  2.72)	Acc@5  14.84 ( 12.28)
Epoch: [0][360/391]	Time  0.022 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0078e+00 (4.3661e+00)	Acc@1   4.69 (  2.75)	Acc@5  25.78 ( 12.44)
Epoch: [0][370/391]	Time  0.024 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0664e+00 (4.3612e+00)	Acc@1   3.91 (  2.80)	Acc@5  20.31 ( 12.58)
Epoch: [0][380/391]	Time  0.021 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.1445e+00 (4.3567e+00)	Acc@1   2.34 (  2.82)	Acc@5  17.97 ( 12.71)
Epoch: [0][390/391]	Time  0.310 ( 0.034)	Data  0.001 ( 0.002)	Loss 4.0703e+00 (4.3536e+00)	Acc@1   2.50 (  2.82)	Acc@5  16.25 ( 12.76)
## e[0] optimizer.zero_grad (sum) time: 0.10515546798706055
## e[0]       loss.backward (sum) time: 2.708028554916382
## e[0]      optimizer.step (sum) time: 0.9422588348388672
## epoch[0] training(only) time: 13.34705662727356
# Switched to evaluate mode...
Test: [  0/100]	Time  0.256 ( 0.256)	Loss 4.0625e+00 (4.0625e+00)	Acc@1   6.00 (  6.00)	Acc@5  26.00 ( 26.00)
Test: [ 10/100]	Time  0.010 ( 0.036)	Loss 4.2031e+00 (4.1179e+00)	Acc@1   4.00 (  5.00)	Acc@5  21.00 ( 21.00)
Test: [ 20/100]	Time  0.019 ( 0.026)	Loss 4.0430e+00 (4.1009e+00)	Acc@1   7.00 (  5.00)	Acc@5  19.00 ( 20.62)
Test: [ 30/100]	Time  0.014 ( 0.022)	Loss 4.1055e+00 (4.1008e+00)	Acc@1   6.00 (  4.84)	Acc@5  21.00 ( 20.29)
Test: [ 40/100]	Time  0.021 ( 0.020)	Loss 4.1289e+00 (4.0985e+00)	Acc@1   2.00 (  4.73)	Acc@5  19.00 ( 19.83)
Test: [ 50/100]	Time  0.012 ( 0.019)	Loss 4.0234e+00 (4.0934e+00)	Acc@1   7.00 (  4.51)	Acc@5  27.00 ( 20.27)
Test: [ 60/100]	Time  0.022 ( 0.018)	Loss 4.0312e+00 (4.0941e+00)	Acc@1   2.00 (  4.44)	Acc@5  27.00 ( 20.28)
Test: [ 70/100]	Time  0.018 ( 0.018)	Loss 4.0586e+00 (4.0958e+00)	Acc@1   3.00 (  4.28)	Acc@5  19.00 ( 20.13)
Test: [ 80/100]	Time  0.010 ( 0.017)	Loss 4.1523e+00 (4.0998e+00)	Acc@1   4.00 (  4.21)	Acc@5  16.00 ( 20.17)
Test: [ 90/100]	Time  0.030 ( 0.017)	Loss 4.0859e+00 (4.0954e+00)	Acc@1   1.00 (  4.23)	Acc@5  17.00 ( 20.13)
 * Acc@1 4.170 Acc@5 20.110
### epoch[0] execution time: 15.081033945083618
EPOCH 1
# current learning rate: 0.1
# Switched to train mode...
Epoch: [1][  0/391]	Time  0.193 ( 0.193)	Data  0.158 ( 0.158)	Loss 4.2617e+00 (4.2617e+00)	Acc@1   5.47 (  5.47)	Acc@5  17.97 ( 17.97)
Epoch: [1][ 10/391]	Time  0.033 ( 0.040)	Data  0.001 ( 0.016)	Loss 4.1250e+00 (4.1186e+00)	Acc@1   3.91 (  5.26)	Acc@5  21.09 ( 21.16)
Epoch: [1][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 4.2539e+00 (4.1527e+00)	Acc@1   4.69 (  4.61)	Acc@5  18.75 ( 19.49)
Epoch: [1][ 30/391]	Time  0.021 ( 0.030)	Data  0.002 ( 0.007)	Loss 4.2188e+00 (4.1559e+00)	Acc@1   3.12 (  4.49)	Acc@5  14.84 ( 19.05)
Epoch: [1][ 40/391]	Time  0.019 ( 0.029)	Data  0.002 ( 0.006)	Loss 4.2461e+00 (4.1508e+00)	Acc@1   2.34 (  4.55)	Acc@5  17.19 ( 19.28)
Epoch: [1][ 50/391]	Time  0.020 ( 0.028)	Data  0.000 ( 0.005)	Loss 4.3164e+00 (4.1561e+00)	Acc@1   3.12 (  4.46)	Acc@5  11.72 ( 19.15)
Epoch: [1][ 60/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.2461e+00 (4.1541e+00)	Acc@1   3.12 (  4.41)	Acc@5  19.53 ( 19.22)
Epoch: [1][ 70/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.1758e+00 (4.1450e+00)	Acc@1   4.69 (  4.54)	Acc@5  15.62 ( 19.39)
Epoch: [1][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.1914e+00 (4.1377e+00)	Acc@1   2.34 (  4.49)	Acc@5  16.41 ( 19.65)
Epoch: [1][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.9238e+00 (4.1328e+00)	Acc@1   7.81 (  4.55)	Acc@5  23.44 ( 19.93)
Epoch: [1][100/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.1641e+00 (4.1350e+00)	Acc@1   1.56 (  4.64)	Acc@5  22.66 ( 19.90)
Epoch: [1][110/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.0039e+00 (4.1326e+00)	Acc@1   5.47 (  4.65)	Acc@5  23.44 ( 20.08)
Epoch: [1][120/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.0664e+00 (4.1295e+00)	Acc@1   6.25 (  4.76)	Acc@5  19.53 ( 20.11)
Epoch: [1][130/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9160e+00 (4.1228e+00)	Acc@1   6.25 (  4.83)	Acc@5  22.66 ( 20.30)
Epoch: [1][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1133e+00 (4.1223e+00)	Acc@1   8.59 (  4.86)	Acc@5  24.22 ( 20.33)
Epoch: [1][150/391]	Time  0.044 ( 0.026)	Data  0.005 ( 0.003)	Loss 3.9941e+00 (4.1153e+00)	Acc@1   3.91 (  4.90)	Acc@5  24.22 ( 20.65)
Epoch: [1][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1797e+00 (4.1129e+00)	Acc@1   4.69 (  4.91)	Acc@5  16.41 ( 20.68)
Epoch: [1][170/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9277e+00 (4.1091e+00)	Acc@1   7.03 (  4.88)	Acc@5  21.88 ( 20.68)
Epoch: [1][180/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.9570e+00 (4.1037e+00)	Acc@1   8.59 (  4.97)	Acc@5  26.56 ( 20.90)
Epoch: [1][190/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9922e+00 (4.1023e+00)	Acc@1   5.47 (  4.96)	Acc@5  25.00 ( 20.89)
Epoch: [1][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9980e+00 (4.0985e+00)	Acc@1   6.25 (  5.03)	Acc@5  21.09 ( 20.99)
Epoch: [1][210/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0586e+00 (4.0951e+00)	Acc@1   4.69 (  5.07)	Acc@5  24.22 ( 21.08)
Epoch: [1][220/391]	Time  0.038 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.9668e+00 (4.0899e+00)	Acc@1   3.12 (  5.10)	Acc@5  17.97 ( 21.24)
Epoch: [1][230/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1953e+00 (4.0879e+00)	Acc@1   4.69 (  5.14)	Acc@5  21.09 ( 21.28)
Epoch: [1][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2617e+00 (4.0853e+00)	Acc@1   7.81 (  5.19)	Acc@5  19.53 ( 21.38)
Epoch: [1][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0078e+00 (4.0826e+00)	Acc@1   4.69 (  5.23)	Acc@5  19.53 ( 21.41)
Epoch: [1][260/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9668e+00 (4.0768e+00)	Acc@1  10.94 (  5.28)	Acc@5  29.69 ( 21.60)
Epoch: [1][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0391e+00 (4.0743e+00)	Acc@1   5.47 (  5.31)	Acc@5  21.09 ( 21.66)
Epoch: [1][280/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2656e+00 (4.0730e+00)	Acc@1   3.91 (  5.31)	Acc@5  17.19 ( 21.65)
Epoch: [1][290/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.8555e+00 (4.0718e+00)	Acc@1   5.47 (  5.33)	Acc@5  32.03 ( 21.75)
Epoch: [1][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.0820e+00 (4.0685e+00)	Acc@1   5.47 (  5.40)	Acc@5  28.12 ( 21.90)
Epoch: [1][310/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.9570e+00 (4.0672e+00)	Acc@1   6.25 (  5.42)	Acc@5  28.91 ( 21.99)
Epoch: [1][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9629e+00 (4.0661e+00)	Acc@1   6.25 (  5.43)	Acc@5  25.00 ( 22.04)
Epoch: [1][330/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9043e+00 (4.0624e+00)	Acc@1   9.38 (  5.51)	Acc@5  23.44 ( 22.15)
Epoch: [1][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9883e+00 (4.0598e+00)	Acc@1   3.12 (  5.54)	Acc@5  28.12 ( 22.26)
Epoch: [1][350/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9102e+00 (4.0582e+00)	Acc@1  10.16 (  5.55)	Acc@5  25.78 ( 22.32)
Epoch: [1][360/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9082e+00 (4.0560e+00)	Acc@1   7.03 (  5.55)	Acc@5  27.34 ( 22.39)
Epoch: [1][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9141e+00 (4.0531e+00)	Acc@1  11.72 (  5.61)	Acc@5  27.34 ( 22.51)
Epoch: [1][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9961e+00 (4.0505e+00)	Acc@1   5.47 (  5.67)	Acc@5  23.44 ( 22.61)
Epoch: [1][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.8359e+00 (4.0475e+00)	Acc@1   7.50 (  5.71)	Acc@5  26.25 ( 22.69)
## e[1] optimizer.zero_grad (sum) time: 0.10679793357849121
## e[1]       loss.backward (sum) time: 2.252718448638916
## e[1]      optimizer.step (sum) time: 0.9202234745025635
## epoch[1] training(only) time: 10.138484477996826
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 3.8281e+00 (3.8281e+00)	Acc@1   7.00 (  7.00)	Acc@5  34.00 ( 34.00)
Test: [ 10/100]	Time  0.016 ( 0.025)	Loss 3.9082e+00 (3.8246e+00)	Acc@1   9.00 (  7.82)	Acc@5  32.00 ( 30.73)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 3.8105e+00 (3.8534e+00)	Acc@1  11.00 (  7.67)	Acc@5  31.00 ( 29.71)
Test: [ 30/100]	Time  0.014 ( 0.017)	Loss 3.8730e+00 (3.8346e+00)	Acc@1   8.00 (  7.77)	Acc@5  28.00 ( 30.39)
Test: [ 40/100]	Time  0.010 ( 0.016)	Loss 4.0391e+00 (3.8428e+00)	Acc@1   5.00 (  7.59)	Acc@5  25.00 ( 30.07)
Test: [ 50/100]	Time  0.020 ( 0.016)	Loss 3.8633e+00 (3.8423e+00)	Acc@1   7.00 (  7.22)	Acc@5  27.00 ( 30.14)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 3.8359e+00 (3.8465e+00)	Acc@1   7.00 (  7.31)	Acc@5  28.00 ( 29.85)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 3.8945e+00 (3.8483e+00)	Acc@1  11.00 (  7.37)	Acc@5  31.00 ( 29.90)
Test: [ 80/100]	Time  0.025 ( 0.015)	Loss 3.9180e+00 (3.8526e+00)	Acc@1   7.00 (  7.37)	Acc@5  23.00 ( 29.72)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 3.5488e+00 (3.8453e+00)	Acc@1   9.00 (  7.48)	Acc@5  39.00 ( 29.97)
 * Acc@1 7.600 Acc@5 29.910
### epoch[1] execution time: 11.720458984375
EPOCH 2
# current learning rate: 0.1
# Switched to train mode...
Epoch: [2][  0/391]	Time  0.171 ( 0.171)	Data  0.142 ( 0.142)	Loss 3.9043e+00 (3.9043e+00)	Acc@1   7.81 (  7.81)	Acc@5  29.69 ( 29.69)
Epoch: [2][ 10/391]	Time  0.031 ( 0.039)	Data  0.001 ( 0.015)	Loss 3.8340e+00 (3.8764e+00)	Acc@1   5.47 (  7.24)	Acc@5  28.12 ( 28.20)
Epoch: [2][ 20/391]	Time  0.032 ( 0.033)	Data  0.001 ( 0.008)	Loss 3.9766e+00 (3.8850e+00)	Acc@1   5.47 (  7.14)	Acc@5  25.78 ( 27.79)
Epoch: [2][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.7891e+00 (3.8970e+00)	Acc@1   7.03 (  7.18)	Acc@5  25.00 ( 27.47)
Epoch: [2][ 40/391]	Time  0.021 ( 0.028)	Data  0.002 ( 0.005)	Loss 3.7832e+00 (3.9012e+00)	Acc@1  11.72 (  7.28)	Acc@5  32.81 ( 27.31)
Epoch: [2][ 50/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.9160e+00 (3.9038e+00)	Acc@1   7.03 (  7.18)	Acc@5  28.12 ( 27.51)
Epoch: [2][ 60/391]	Time  0.036 ( 0.028)	Data  0.004 ( 0.004)	Loss 3.7812e+00 (3.8991e+00)	Acc@1   8.59 (  7.33)	Acc@5  30.47 ( 27.73)
Epoch: [2][ 70/391]	Time  0.020 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.9883e+00 (3.8983e+00)	Acc@1   7.81 (  7.19)	Acc@5  28.91 ( 27.71)
Epoch: [2][ 80/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.9805e+00 (3.9029e+00)	Acc@1   7.03 (  7.23)	Acc@5  22.66 ( 27.53)
Epoch: [2][ 90/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.8066e+00 (3.8962e+00)	Acc@1  10.94 (  7.37)	Acc@5  32.03 ( 27.78)
Epoch: [2][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.8945e+00 (3.8958e+00)	Acc@1   9.38 (  7.40)	Acc@5  25.00 ( 27.71)
Epoch: [2][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7266e+00 (3.8913e+00)	Acc@1   6.25 (  7.44)	Acc@5  34.38 ( 27.86)
Epoch: [2][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7305e+00 (3.8851e+00)	Acc@1   9.38 (  7.56)	Acc@5  30.47 ( 28.14)
Epoch: [2][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8516e+00 (3.8861e+00)	Acc@1   8.59 (  7.55)	Acc@5  30.47 ( 28.22)
Epoch: [2][140/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.7402e+00 (3.8824e+00)	Acc@1   7.03 (  7.62)	Acc@5  33.59 ( 28.36)
Epoch: [2][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0430e+00 (3.8825e+00)	Acc@1   8.59 (  7.72)	Acc@5  28.12 ( 28.44)
Epoch: [2][160/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7676e+00 (3.8807e+00)	Acc@1   7.81 (  7.80)	Acc@5  33.59 ( 28.60)
Epoch: [2][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8965e+00 (3.8786e+00)	Acc@1   7.03 (  7.82)	Acc@5  25.78 ( 28.67)
Epoch: [2][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8086e+00 (3.8763e+00)	Acc@1  10.16 (  7.90)	Acc@5  28.91 ( 28.71)
Epoch: [2][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.9355e+00 (3.8709e+00)	Acc@1   9.38 (  7.98)	Acc@5  30.47 ( 28.98)
Epoch: [2][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8789e+00 (3.8676e+00)	Acc@1  10.16 (  8.08)	Acc@5  26.56 ( 29.05)
Epoch: [2][210/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.7793e+00 (3.8646e+00)	Acc@1  10.16 (  8.06)	Acc@5  28.91 ( 29.08)
Epoch: [2][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7559e+00 (3.8621e+00)	Acc@1   7.03 (  8.09)	Acc@5  35.94 ( 29.25)
Epoch: [2][230/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.8672e+00 (3.8627e+00)	Acc@1   6.25 (  8.13)	Acc@5  30.47 ( 29.30)
Epoch: [2][240/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7402e+00 (3.8607e+00)	Acc@1   9.38 (  8.13)	Acc@5  34.38 ( 29.33)
Epoch: [2][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7070e+00 (3.8541e+00)	Acc@1   9.38 (  8.22)	Acc@5  30.47 ( 29.48)
Epoch: [2][260/391]	Time  0.039 ( 0.026)	Data  0.008 ( 0.002)	Loss 3.9551e+00 (3.8555e+00)	Acc@1   8.59 (  8.25)	Acc@5  26.56 ( 29.51)
Epoch: [2][270/391]	Time  0.037 ( 0.026)	Data  0.008 ( 0.002)	Loss 3.8457e+00 (3.8545e+00)	Acc@1   7.03 (  8.28)	Acc@5  32.03 ( 29.53)
Epoch: [2][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6719e+00 (3.8511e+00)	Acc@1  12.50 (  8.33)	Acc@5  31.25 ( 29.63)
Epoch: [2][290/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.002)	Loss 3.7305e+00 (3.8483e+00)	Acc@1   9.38 (  8.36)	Acc@5  34.38 ( 29.72)
Epoch: [2][300/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7051e+00 (3.8453e+00)	Acc@1  13.28 (  8.40)	Acc@5  38.28 ( 29.82)
Epoch: [2][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6387e+00 (3.8410e+00)	Acc@1  10.94 (  8.45)	Acc@5  33.59 ( 29.98)
Epoch: [2][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6543e+00 (3.8375e+00)	Acc@1   8.59 (  8.51)	Acc@5  28.91 ( 30.11)
Epoch: [2][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.8086e+00 (3.8336e+00)	Acc@1  10.94 (  8.56)	Acc@5  34.38 ( 30.22)
Epoch: [2][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5723e+00 (3.8291e+00)	Acc@1  12.50 (  8.59)	Acc@5  27.34 ( 30.32)
Epoch: [2][350/391]	Time  0.037 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.6758e+00 (3.8260e+00)	Acc@1  10.16 (  8.62)	Acc@5  33.59 ( 30.40)
Epoch: [2][360/391]	Time  0.036 ( 0.026)	Data  0.005 ( 0.002)	Loss 3.6895e+00 (3.8250e+00)	Acc@1   7.03 (  8.62)	Acc@5  35.16 ( 30.44)
Epoch: [2][370/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.8262e+00 (3.8234e+00)	Acc@1  11.72 (  8.64)	Acc@5  35.94 ( 30.52)
Epoch: [2][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6543e+00 (3.8205e+00)	Acc@1   8.59 (  8.70)	Acc@5  35.94 ( 30.58)
Epoch: [2][390/391]	Time  0.019 ( 0.025)	Data  0.001 ( 0.002)	Loss 3.5195e+00 (3.8163e+00)	Acc@1   8.75 (  8.77)	Acc@5  40.00 ( 30.74)
## e[2] optimizer.zero_grad (sum) time: 0.10470080375671387
## e[2]       loss.backward (sum) time: 2.260326862335205
## e[2]      optimizer.step (sum) time: 0.9390382766723633
## epoch[2] training(only) time: 10.064813613891602
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 3.6152e+00 (3.6152e+00)	Acc@1  10.00 ( 10.00)	Acc@5  38.00 ( 38.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 3.6699e+00 (3.6083e+00)	Acc@1  10.00 ( 12.18)	Acc@5  36.00 ( 36.55)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 3.6797e+00 (3.6076e+00)	Acc@1  10.00 ( 12.05)	Acc@5  37.00 ( 37.10)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 3.6973e+00 (3.6031e+00)	Acc@1  12.00 ( 12.23)	Acc@5  30.00 ( 37.71)
Test: [ 40/100]	Time  0.019 ( 0.017)	Loss 3.7363e+00 (3.6052e+00)	Acc@1  14.00 ( 12.12)	Acc@5  37.00 ( 37.63)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 3.6465e+00 (3.5964e+00)	Acc@1  16.00 ( 12.37)	Acc@5  35.00 ( 37.86)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 3.5410e+00 (3.5906e+00)	Acc@1  12.00 ( 12.23)	Acc@5  40.00 ( 38.23)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 3.7461e+00 (3.5930e+00)	Acc@1   9.00 ( 12.24)	Acc@5  43.00 ( 38.20)
Test: [ 80/100]	Time  0.024 ( 0.015)	Loss 3.7090e+00 (3.5961e+00)	Acc@1   8.00 ( 12.23)	Acc@5  35.00 ( 38.20)
Test: [ 90/100]	Time  0.021 ( 0.015)	Loss 3.5488e+00 (3.5931e+00)	Acc@1  10.00 ( 12.08)	Acc@5  44.00 ( 38.29)
 * Acc@1 12.160 Acc@5 38.250
### epoch[2] execution time: 11.668520450592041
EPOCH 3
# current learning rate: 0.1
# Switched to train mode...
Epoch: [3][  0/391]	Time  0.181 ( 0.181)	Data  0.147 ( 0.147)	Loss 3.5566e+00 (3.5566e+00)	Acc@1  13.28 ( 13.28)	Acc@5  41.41 ( 41.41)
Epoch: [3][ 10/391]	Time  0.022 ( 0.039)	Data  0.000 ( 0.015)	Loss 3.6523e+00 (3.6183e+00)	Acc@1  12.50 ( 12.43)	Acc@5  37.50 ( 38.07)
Epoch: [3][ 20/391]	Time  0.022 ( 0.032)	Data  0.004 ( 0.009)	Loss 3.7363e+00 (3.6392e+00)	Acc@1   7.81 ( 11.50)	Acc@5  35.16 ( 37.20)
Epoch: [3][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 3.5664e+00 (3.6488e+00)	Acc@1  12.50 ( 11.19)	Acc@5  39.84 ( 36.27)
Epoch: [3][ 40/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.7012e+00 (3.6523e+00)	Acc@1   8.59 ( 11.01)	Acc@5  32.03 ( 36.51)
Epoch: [3][ 50/391]	Time  0.041 ( 0.028)	Data  0.004 ( 0.005)	Loss 3.7012e+00 (3.6443e+00)	Acc@1  14.84 ( 11.15)	Acc@5  35.16 ( 36.29)
Epoch: [3][ 60/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.7422e+00 (3.6425e+00)	Acc@1  10.16 ( 11.21)	Acc@5  34.38 ( 36.45)
Epoch: [3][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.7070e+00 (3.6429e+00)	Acc@1   8.59 ( 11.07)	Acc@5  33.59 ( 36.49)
Epoch: [3][ 80/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.6660e+00 (3.6359e+00)	Acc@1  16.41 ( 11.28)	Acc@5  36.72 ( 36.57)
Epoch: [3][ 90/391]	Time  0.020 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.6230e+00 (3.6324e+00)	Acc@1   4.69 ( 11.23)	Acc@5  34.38 ( 36.73)
Epoch: [3][100/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.9082e+00 (3.6325e+00)	Acc@1   7.81 ( 11.21)	Acc@5  28.91 ( 36.71)
Epoch: [3][110/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3789e+00 (3.6256e+00)	Acc@1  17.97 ( 11.29)	Acc@5  46.09 ( 36.96)
Epoch: [3][120/391]	Time  0.047 ( 0.026)	Data  0.006 ( 0.003)	Loss 3.5410e+00 (3.6201e+00)	Acc@1  13.28 ( 11.31)	Acc@5  42.19 ( 37.19)
Epoch: [3][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5703e+00 (3.6212e+00)	Acc@1  17.97 ( 11.21)	Acc@5  48.44 ( 37.29)
Epoch: [3][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9629e+00 (3.6198e+00)	Acc@1   7.81 ( 11.24)	Acc@5  25.78 ( 37.26)
Epoch: [3][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7461e+00 (3.6143e+00)	Acc@1  10.94 ( 11.37)	Acc@5  35.16 ( 37.44)
Epoch: [3][160/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6387e+00 (3.6132e+00)	Acc@1  10.16 ( 11.41)	Acc@5  32.03 ( 37.44)
Epoch: [3][170/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4395e+00 (3.6079e+00)	Acc@1  12.50 ( 11.44)	Acc@5  43.75 ( 37.61)
Epoch: [3][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3672e+00 (3.6039e+00)	Acc@1  15.62 ( 11.51)	Acc@5  45.31 ( 37.81)
Epoch: [3][190/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4316e+00 (3.5973e+00)	Acc@1  15.62 ( 11.64)	Acc@5  43.75 ( 37.96)
Epoch: [3][200/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5898e+00 (3.5992e+00)	Acc@1  14.06 ( 11.64)	Acc@5  36.72 ( 37.93)
Epoch: [3][210/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6094e+00 (3.5983e+00)	Acc@1  15.62 ( 11.73)	Acc@5  36.72 ( 37.99)
Epoch: [3][220/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.4219e+00 (3.5947e+00)	Acc@1  18.75 ( 11.87)	Acc@5  42.19 ( 38.13)
Epoch: [3][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3301e+00 (3.5874e+00)	Acc@1  14.84 ( 11.99)	Acc@5  43.75 ( 38.35)
Epoch: [3][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3711e+00 (3.5829e+00)	Acc@1  16.41 ( 12.07)	Acc@5  45.31 ( 38.51)
Epoch: [3][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5059e+00 (3.5815e+00)	Acc@1  11.72 ( 12.10)	Acc@5  39.84 ( 38.59)
Epoch: [3][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5840e+00 (3.5791e+00)	Acc@1  19.53 ( 12.18)	Acc@5  43.75 ( 38.63)
Epoch: [3][270/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.2441e+00 (3.5762e+00)	Acc@1  16.41 ( 12.25)	Acc@5  49.22 ( 38.71)
Epoch: [3][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1660e+00 (3.5718e+00)	Acc@1  16.41 ( 12.35)	Acc@5  49.22 ( 38.83)
Epoch: [3][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6309e+00 (3.5669e+00)	Acc@1  10.16 ( 12.44)	Acc@5  36.72 ( 38.92)
Epoch: [3][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.2656e+00 (3.5602e+00)	Acc@1  18.75 ( 12.54)	Acc@5  50.00 ( 39.12)
Epoch: [3][310/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.5547e+00 (3.5577e+00)	Acc@1   8.59 ( 12.55)	Acc@5  40.62 ( 39.17)
Epoch: [3][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6387e+00 (3.5561e+00)	Acc@1   8.59 ( 12.56)	Acc@5  39.84 ( 39.22)
Epoch: [3][330/391]	Time  0.042 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5430e+00 (3.5535e+00)	Acc@1  11.72 ( 12.60)	Acc@5  36.72 ( 39.31)
Epoch: [3][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3828e+00 (3.5493e+00)	Acc@1  14.84 ( 12.68)	Acc@5  40.62 ( 39.42)
Epoch: [3][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6621e+00 (3.5467e+00)	Acc@1  11.72 ( 12.75)	Acc@5  39.84 ( 39.47)
Epoch: [3][360/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.3184e+00 (3.5437e+00)	Acc@1  18.75 ( 12.80)	Acc@5  43.75 ( 39.56)
Epoch: [3][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5586e+00 (3.5404e+00)	Acc@1  13.28 ( 12.86)	Acc@5  40.62 ( 39.65)
Epoch: [3][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5703e+00 (3.5375e+00)	Acc@1  13.28 ( 12.90)	Acc@5  38.28 ( 39.74)
Epoch: [3][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3555e+00 (3.5334e+00)	Acc@1  13.75 ( 12.97)	Acc@5  48.75 ( 39.86)
## e[3] optimizer.zero_grad (sum) time: 0.10653162002563477
## e[3]       loss.backward (sum) time: 2.2385096549987793
## e[3]      optimizer.step (sum) time: 0.9433207511901855
## epoch[3] training(only) time: 10.106114625930786
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 3.5293e+00 (3.5293e+00)	Acc@1  21.00 ( 21.00)	Acc@5  38.00 ( 38.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 3.3418e+00 (3.3786e+00)	Acc@1  17.00 ( 16.73)	Acc@5  46.00 ( 42.73)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 3.2598e+00 (3.3642e+00)	Acc@1  11.00 ( 16.05)	Acc@5  53.00 ( 44.00)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 3.5430e+00 (3.3532e+00)	Acc@1  16.00 ( 16.16)	Acc@5  41.00 ( 45.42)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 3.3223e+00 (3.3557e+00)	Acc@1  22.00 ( 16.51)	Acc@5  50.00 ( 45.27)
Test: [ 50/100]	Time  0.014 ( 0.017)	Loss 3.1836e+00 (3.3628e+00)	Acc@1  23.00 ( 16.22)	Acc@5  49.00 ( 44.96)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 3.4102e+00 (3.3658e+00)	Acc@1  16.00 ( 16.08)	Acc@5  40.00 ( 44.79)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 3.5703e+00 (3.3723e+00)	Acc@1  13.00 ( 15.86)	Acc@5  39.00 ( 44.45)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 3.4395e+00 (3.3763e+00)	Acc@1  22.00 ( 15.91)	Acc@5  44.00 ( 44.40)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 3.1094e+00 (3.3764e+00)	Acc@1  21.00 ( 15.96)	Acc@5  53.00 ( 44.60)
 * Acc@1 15.900 Acc@5 44.580
### epoch[3] execution time: 11.699813842773438
EPOCH 4
# current learning rate: 0.1
# Switched to train mode...
Epoch: [4][  0/391]	Time  0.187 ( 0.187)	Data  0.159 ( 0.159)	Loss 3.4609e+00 (3.4609e+00)	Acc@1  16.41 ( 16.41)	Acc@5  43.75 ( 43.75)
Epoch: [4][ 10/391]	Time  0.022 ( 0.039)	Data  0.002 ( 0.016)	Loss 3.4648e+00 (3.3748e+00)	Acc@1  14.84 ( 14.70)	Acc@5  39.84 ( 44.53)
Epoch: [4][ 20/391]	Time  0.021 ( 0.033)	Data  0.001 ( 0.010)	Loss 3.6211e+00 (3.4257e+00)	Acc@1  10.94 ( 14.14)	Acc@5  38.28 ( 44.27)
Epoch: [4][ 30/391]	Time  0.037 ( 0.031)	Data  0.005 ( 0.007)	Loss 3.2832e+00 (3.3789e+00)	Acc@1  16.41 ( 14.54)	Acc@5  47.66 ( 45.41)
Epoch: [4][ 40/391]	Time  0.029 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.4102e+00 (3.3780e+00)	Acc@1  11.72 ( 14.58)	Acc@5  46.09 ( 45.35)
Epoch: [4][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.3652e+00 (3.3635e+00)	Acc@1  14.84 ( 14.92)	Acc@5  42.97 ( 45.83)
Epoch: [4][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.5098e+00 (3.3617e+00)	Acc@1  14.06 ( 14.77)	Acc@5  39.84 ( 45.63)
Epoch: [4][ 70/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.4414e+00 (3.3497e+00)	Acc@1  17.97 ( 15.18)	Acc@5  46.88 ( 45.93)
Epoch: [4][ 80/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.3828e+00 (3.3446e+00)	Acc@1  15.62 ( 15.28)	Acc@5  45.31 ( 46.03)
Epoch: [4][ 90/391]	Time  0.028 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.0996e+00 (3.3356e+00)	Acc@1  23.44 ( 15.71)	Acc@5  53.12 ( 46.29)
Epoch: [4][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.0977e+00 (3.3345e+00)	Acc@1  25.00 ( 15.90)	Acc@5  52.34 ( 46.20)
Epoch: [4][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.3926e+00 (3.3376e+00)	Acc@1  14.84 ( 16.02)	Acc@5  46.88 ( 46.08)
Epoch: [4][120/391]	Time  0.022 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.3398e+00 (3.3419e+00)	Acc@1  20.31 ( 16.06)	Acc@5  50.78 ( 46.03)
Epoch: [4][130/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.2930e+00 (3.3501e+00)	Acc@1  17.19 ( 15.91)	Acc@5  50.00 ( 45.81)
Epoch: [4][140/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3457e+00 (3.3502e+00)	Acc@1  17.97 ( 15.94)	Acc@5  44.53 ( 45.79)
Epoch: [4][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1680e+00 (3.3430e+00)	Acc@1  17.97 ( 16.09)	Acc@5  48.44 ( 46.02)
Epoch: [4][160/391]	Time  0.025 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.1523e+00 (3.3385e+00)	Acc@1  17.19 ( 16.14)	Acc@5  50.00 ( 46.08)
Epoch: [4][170/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5391e+00 (3.3306e+00)	Acc@1  10.16 ( 16.27)	Acc@5  46.09 ( 46.28)
Epoch: [4][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3340e+00 (3.3266e+00)	Acc@1  19.53 ( 16.37)	Acc@5  44.53 ( 46.41)
Epoch: [4][190/391]	Time  0.020 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.4219e+00 (3.3201e+00)	Acc@1  13.28 ( 16.51)	Acc@5  48.44 ( 46.64)
Epoch: [4][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2988e+00 (3.3187e+00)	Acc@1  17.97 ( 16.55)	Acc@5  47.66 ( 46.68)
Epoch: [4][210/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3965e+00 (3.3184e+00)	Acc@1  14.84 ( 16.65)	Acc@5  46.09 ( 46.67)
Epoch: [4][220/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.3203e+00 (3.3152e+00)	Acc@1  15.62 ( 16.77)	Acc@5  46.88 ( 46.72)
Epoch: [4][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1230e+00 (3.3118e+00)	Acc@1  25.00 ( 16.86)	Acc@5  51.56 ( 46.80)
Epoch: [4][240/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.3477e+00 (3.3070e+00)	Acc@1  16.41 ( 16.94)	Acc@5  49.22 ( 46.99)
Epoch: [4][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1836e+00 (3.3039e+00)	Acc@1  18.75 ( 17.03)	Acc@5  51.56 ( 47.04)
Epoch: [4][260/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3184e+00 (3.3006e+00)	Acc@1  14.84 ( 17.08)	Acc@5  48.44 ( 47.15)
Epoch: [4][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2910e+00 (3.2993e+00)	Acc@1  15.62 ( 17.15)	Acc@5  50.00 ( 47.17)
Epoch: [4][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2656e+00 (3.2962e+00)	Acc@1  17.19 ( 17.21)	Acc@5  46.88 ( 47.26)
Epoch: [4][290/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9863e+00 (3.2907e+00)	Acc@1  25.78 ( 17.30)	Acc@5  56.25 ( 47.39)
Epoch: [4][300/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.3477e+00 (3.2903e+00)	Acc@1  19.53 ( 17.37)	Acc@5  46.09 ( 47.40)
Epoch: [4][310/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.3789e+00 (3.2893e+00)	Acc@1  14.06 ( 17.40)	Acc@5  44.53 ( 47.45)
Epoch: [4][320/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.2012e+00 (3.2853e+00)	Acc@1  18.75 ( 17.51)	Acc@5  50.78 ( 47.57)
Epoch: [4][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8594e+00 (3.2803e+00)	Acc@1  26.56 ( 17.61)	Acc@5  63.28 ( 47.71)
Epoch: [4][340/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0820e+00 (3.2762e+00)	Acc@1  17.97 ( 17.66)	Acc@5  57.03 ( 47.84)
Epoch: [4][350/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.0254e+00 (3.2708e+00)	Acc@1  25.00 ( 17.77)	Acc@5  49.22 ( 47.97)
Epoch: [4][360/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.1191e+00 (3.2668e+00)	Acc@1  19.53 ( 17.90)	Acc@5  50.78 ( 48.08)
Epoch: [4][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1797e+00 (3.2631e+00)	Acc@1  14.84 ( 17.97)	Acc@5  52.34 ( 48.17)
Epoch: [4][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0215e+00 (3.2602e+00)	Acc@1  20.31 ( 18.03)	Acc@5  52.34 ( 48.25)
Epoch: [4][390/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3359e+00 (3.2585e+00)	Acc@1  16.25 ( 18.09)	Acc@5  43.75 ( 48.28)
## e[4] optimizer.zero_grad (sum) time: 0.10273551940917969
## e[4]       loss.backward (sum) time: 2.223017930984497
## e[4]      optimizer.step (sum) time: 0.9124834537506104
## epoch[4] training(only) time: 10.113500356674194
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 3.1328e+00 (3.1328e+00)	Acc@1  23.00 ( 23.00)	Acc@5  58.00 ( 58.00)
Test: [ 10/100]	Time  0.020 ( 0.026)	Loss 3.1211e+00 (3.0772e+00)	Acc@1  21.00 ( 20.45)	Acc@5  49.00 ( 53.82)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 3.0312e+00 (3.0990e+00)	Acc@1  21.00 ( 20.76)	Acc@5  56.00 ( 53.05)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 3.3066e+00 (3.0801e+00)	Acc@1  16.00 ( 20.97)	Acc@5  48.00 ( 53.61)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 3.0762e+00 (3.0812e+00)	Acc@1  20.00 ( 20.90)	Acc@5  57.00 ( 53.20)
Test: [ 50/100]	Time  0.020 ( 0.016)	Loss 3.0488e+00 (3.0811e+00)	Acc@1  20.00 ( 20.96)	Acc@5  52.00 ( 53.35)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 3.0918e+00 (3.0852e+00)	Acc@1  22.00 ( 20.75)	Acc@5  57.00 ( 53.07)
Test: [ 70/100]	Time  0.020 ( 0.016)	Loss 3.4336e+00 (3.0929e+00)	Acc@1  15.00 ( 20.89)	Acc@5  44.00 ( 52.96)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 3.0137e+00 (3.0924e+00)	Acc@1  22.00 ( 21.05)	Acc@5  56.00 ( 52.95)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 3.0566e+00 (3.0849e+00)	Acc@1  29.00 ( 21.23)	Acc@5  51.00 ( 53.03)
 * Acc@1 21.190 Acc@5 53.100
### epoch[4] execution time: 11.706827640533447
EPOCH 5
# current learning rate: 0.1
# Switched to train mode...
Epoch: [5][  0/391]	Time  0.187 ( 0.187)	Data  0.150 ( 0.150)	Loss 3.1855e+00 (3.1855e+00)	Acc@1  17.19 ( 17.19)	Acc@5  53.12 ( 53.12)
Epoch: [5][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 3.0586e+00 (3.0845e+00)	Acc@1  21.09 ( 20.24)	Acc@5  51.56 ( 53.41)
Epoch: [5][ 20/391]	Time  0.037 ( 0.033)	Data  0.001 ( 0.009)	Loss 3.1504e+00 (3.0767e+00)	Acc@1  22.66 ( 20.46)	Acc@5  51.56 ( 53.46)
Epoch: [5][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 3.0000e+00 (3.0817e+00)	Acc@1  28.12 ( 20.87)	Acc@5  55.47 ( 53.50)
Epoch: [5][ 40/391]	Time  0.025 ( 0.029)	Data  0.002 ( 0.005)	Loss 3.2461e+00 (3.0997e+00)	Acc@1  16.41 ( 20.69)	Acc@5  48.44 ( 53.11)
Epoch: [5][ 50/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.1445e+00 (3.0892e+00)	Acc@1  20.31 ( 21.02)	Acc@5  52.34 ( 53.06)
Epoch: [5][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8926e+00 (3.0864e+00)	Acc@1  23.44 ( 21.25)	Acc@5  56.25 ( 53.23)
Epoch: [5][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.9922e+00 (3.0862e+00)	Acc@1  25.78 ( 21.28)	Acc@5  49.22 ( 53.17)
Epoch: [5][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.2617e+00 (3.0819e+00)	Acc@1  16.41 ( 21.39)	Acc@5  52.34 ( 53.42)
Epoch: [5][ 90/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.0508e+00 (3.0759e+00)	Acc@1  15.62 ( 21.43)	Acc@5  52.34 ( 53.50)
Epoch: [5][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1367e+00 (3.0821e+00)	Acc@1  17.19 ( 21.23)	Acc@5  53.91 ( 53.36)
Epoch: [5][110/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9258e+00 (3.0797e+00)	Acc@1  26.56 ( 21.30)	Acc@5  57.81 ( 53.43)
Epoch: [5][120/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1348e+00 (3.0736e+00)	Acc@1  20.31 ( 21.38)	Acc@5  50.78 ( 53.73)
Epoch: [5][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0000e+00 (3.0731e+00)	Acc@1  22.66 ( 21.37)	Acc@5  59.38 ( 53.80)
Epoch: [5][140/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.1367e+00 (3.0689e+00)	Acc@1  22.66 ( 21.46)	Acc@5  48.44 ( 54.01)
Epoch: [5][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1016e+00 (3.0671e+00)	Acc@1  20.31 ( 21.50)	Acc@5  53.12 ( 54.04)
Epoch: [5][160/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9805e+00 (3.0645e+00)	Acc@1  22.66 ( 21.57)	Acc@5  59.38 ( 54.12)
Epoch: [5][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9883e+00 (3.0638e+00)	Acc@1  20.31 ( 21.58)	Acc@5  52.34 ( 54.09)
Epoch: [5][180/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7363e+00 (3.0603e+00)	Acc@1  27.34 ( 21.62)	Acc@5  60.94 ( 54.25)
Epoch: [5][190/391]	Time  0.041 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.0527e+00 (3.0582e+00)	Acc@1  21.09 ( 21.59)	Acc@5  53.12 ( 54.26)
Epoch: [5][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0293e+00 (3.0556e+00)	Acc@1  19.53 ( 21.62)	Acc@5  56.25 ( 54.32)
Epoch: [5][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1777e+00 (3.0534e+00)	Acc@1  18.75 ( 21.68)	Acc@5  47.66 ( 54.25)
Epoch: [5][220/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2031e+00 (3.0509e+00)	Acc@1  20.31 ( 21.72)	Acc@5  55.47 ( 54.37)
Epoch: [5][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0234e+00 (3.0468e+00)	Acc@1  22.66 ( 21.81)	Acc@5  54.69 ( 54.45)
Epoch: [5][240/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.1250e+00 (3.0463e+00)	Acc@1  16.41 ( 21.86)	Acc@5  52.34 ( 54.53)
Epoch: [5][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8027e+00 (3.0428e+00)	Acc@1  25.00 ( 21.90)	Acc@5  60.16 ( 54.63)
Epoch: [5][260/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.1680e+00 (3.0407e+00)	Acc@1  20.31 ( 21.98)	Acc@5  52.34 ( 54.69)
Epoch: [5][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8145e+00 (3.0393e+00)	Acc@1  26.56 ( 22.01)	Acc@5  61.72 ( 54.74)
Epoch: [5][280/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.0059e+00 (3.0365e+00)	Acc@1  22.66 ( 22.08)	Acc@5  55.47 ( 54.83)
Epoch: [5][290/391]	Time  0.030 ( 0.026)	Data  0.004 ( 0.002)	Loss 2.5801e+00 (3.0319e+00)	Acc@1  31.25 ( 22.23)	Acc@5  64.84 ( 54.95)
Epoch: [5][300/391]	Time  0.044 ( 0.026)	Data  0.005 ( 0.002)	Loss 2.9531e+00 (3.0291e+00)	Acc@1  21.88 ( 22.28)	Acc@5  54.69 ( 55.01)
Epoch: [5][310/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.0781e+00 (3.0250e+00)	Acc@1  24.22 ( 22.34)	Acc@5  53.12 ( 55.09)
Epoch: [5][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9219e+00 (3.0220e+00)	Acc@1  27.34 ( 22.45)	Acc@5  56.25 ( 55.15)
Epoch: [5][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0605e+00 (3.0199e+00)	Acc@1  25.00 ( 22.52)	Acc@5  56.25 ( 55.19)
Epoch: [5][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7949e+00 (3.0185e+00)	Acc@1  25.78 ( 22.50)	Acc@5  61.72 ( 55.25)
Epoch: [5][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9121e+00 (3.0165e+00)	Acc@1  24.22 ( 22.58)	Acc@5  56.25 ( 55.30)
Epoch: [5][360/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.0254e+00 (3.0163e+00)	Acc@1  21.09 ( 22.60)	Acc@5  53.12 ( 55.28)
Epoch: [5][370/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8555e+00 (3.0128e+00)	Acc@1  26.56 ( 22.68)	Acc@5  61.72 ( 55.37)
Epoch: [5][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9219e+00 (3.0089e+00)	Acc@1  28.91 ( 22.76)	Acc@5  55.47 ( 55.46)
Epoch: [5][390/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.2168e+00 (3.0063e+00)	Acc@1  17.50 ( 22.81)	Acc@5  52.50 ( 55.55)
## e[5] optimizer.zero_grad (sum) time: 0.10588622093200684
## e[5]       loss.backward (sum) time: 2.3026247024536133
## e[5]      optimizer.step (sum) time: 0.9350941181182861
## epoch[5] training(only) time: 10.089945793151855
# Switched to evaluate mode...
Test: [  0/100]	Time  0.129 ( 0.129)	Loss 2.9473e+00 (2.9473e+00)	Acc@1  28.00 ( 28.00)	Acc@5  56.00 ( 56.00)
Test: [ 10/100]	Time  0.023 ( 0.026)	Loss 2.9863e+00 (2.8604e+00)	Acc@1  25.00 ( 27.09)	Acc@5  55.00 ( 58.00)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 2.8125e+00 (2.8549e+00)	Acc@1  28.00 ( 26.90)	Acc@5  60.00 ( 58.81)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 2.9590e+00 (2.8460e+00)	Acc@1  29.00 ( 26.55)	Acc@5  57.00 ( 59.00)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 2.9336e+00 (2.8358e+00)	Acc@1  26.00 ( 27.05)	Acc@5  56.00 ( 59.15)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 2.6211e+00 (2.8395e+00)	Acc@1  35.00 ( 27.00)	Acc@5  65.00 ( 58.86)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 2.6211e+00 (2.8339e+00)	Acc@1  32.00 ( 27.07)	Acc@5  72.00 ( 58.82)
Test: [ 70/100]	Time  0.026 ( 0.016)	Loss 3.0977e+00 (2.8456e+00)	Acc@1  24.00 ( 26.77)	Acc@5  51.00 ( 58.34)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 2.8359e+00 (2.8530e+00)	Acc@1  29.00 ( 26.53)	Acc@5  61.00 ( 58.31)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 2.8262e+00 (2.8524e+00)	Acc@1  33.00 ( 26.77)	Acc@5  59.00 ( 58.35)
 * Acc@1 26.880 Acc@5 58.300
### epoch[5] execution time: 11.712738513946533
EPOCH 6
# current learning rate: 0.1
# Switched to train mode...
Epoch: [6][  0/391]	Time  0.188 ( 0.188)	Data  0.156 ( 0.156)	Loss 2.8027e+00 (2.8027e+00)	Acc@1  25.78 ( 25.78)	Acc@5  60.16 ( 60.16)
Epoch: [6][ 10/391]	Time  0.032 ( 0.040)	Data  0.002 ( 0.016)	Loss 2.7227e+00 (2.8297e+00)	Acc@1  28.12 ( 27.34)	Acc@5  66.41 ( 61.08)
Epoch: [6][ 20/391]	Time  0.021 ( 0.033)	Data  0.002 ( 0.009)	Loss 2.8359e+00 (2.8375e+00)	Acc@1  18.75 ( 26.30)	Acc@5  60.16 ( 61.01)
Epoch: [6][ 30/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.007)	Loss 2.7441e+00 (2.8275e+00)	Acc@1  25.00 ( 26.31)	Acc@5  61.72 ( 60.56)
Epoch: [6][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.0586e+00 (2.8532e+00)	Acc@1  22.66 ( 25.51)	Acc@5  50.00 ( 59.64)
Epoch: [6][ 50/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.8906e+00 (2.8627e+00)	Acc@1  26.56 ( 25.61)	Acc@5  58.59 ( 59.57)
Epoch: [6][ 60/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.7930e+00 (2.8587e+00)	Acc@1  31.25 ( 25.70)	Acc@5  58.59 ( 59.67)
Epoch: [6][ 70/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.6680e+00 (2.8643e+00)	Acc@1  25.00 ( 25.65)	Acc@5  71.09 ( 59.56)
Epoch: [6][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.0059e+00 (2.8705e+00)	Acc@1  22.66 ( 25.62)	Acc@5  50.78 ( 59.37)
Epoch: [6][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.0215e+00 (2.8666e+00)	Acc@1  20.31 ( 25.70)	Acc@5  55.47 ( 59.47)
Epoch: [6][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8340e+00 (2.8637e+00)	Acc@1  24.22 ( 25.71)	Acc@5  57.81 ( 59.68)
Epoch: [6][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7422e+00 (2.8594e+00)	Acc@1  28.91 ( 25.83)	Acc@5  59.38 ( 59.76)
Epoch: [6][120/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.8145e+00 (2.8549e+00)	Acc@1  24.22 ( 25.87)	Acc@5  58.59 ( 59.84)
Epoch: [6][130/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9648e+00 (2.8569e+00)	Acc@1  27.34 ( 25.94)	Acc@5  53.12 ( 59.85)
Epoch: [6][140/391]	Time  0.040 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.0547e+00 (2.8636e+00)	Acc@1  20.31 ( 25.89)	Acc@5  55.47 ( 59.64)
Epoch: [6][150/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8086e+00 (2.8599e+00)	Acc@1  28.91 ( 25.97)	Acc@5  60.94 ( 59.79)
Epoch: [6][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8867e+00 (2.8553e+00)	Acc@1  26.56 ( 26.04)	Acc@5  57.81 ( 59.96)
Epoch: [6][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9707e+00 (2.8604e+00)	Acc@1  23.44 ( 25.89)	Acc@5  55.47 ( 59.82)
Epoch: [6][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5703e+00 (2.8553e+00)	Acc@1  26.56 ( 25.96)	Acc@5  71.09 ( 59.94)
Epoch: [6][190/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9727e+00 (2.8558e+00)	Acc@1  23.44 ( 25.97)	Acc@5  57.81 ( 59.91)
Epoch: [6][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9766e+00 (2.8523e+00)	Acc@1  21.09 ( 26.00)	Acc@5  56.25 ( 60.03)
Epoch: [6][210/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8164e+00 (2.8530e+00)	Acc@1  23.44 ( 26.08)	Acc@5  56.25 ( 60.01)
Epoch: [6][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9512e+00 (2.8506e+00)	Acc@1  26.56 ( 26.12)	Acc@5  57.81 ( 60.01)
Epoch: [6][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9453e+00 (2.8503e+00)	Acc@1  25.78 ( 26.05)	Acc@5  53.91 ( 60.01)
Epoch: [6][240/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6992e+00 (2.8477e+00)	Acc@1  30.47 ( 26.13)	Acc@5  65.62 ( 60.09)
Epoch: [6][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8457e+00 (2.8506e+00)	Acc@1  27.34 ( 26.10)	Acc@5  59.38 ( 60.06)
Epoch: [6][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8516e+00 (2.8464e+00)	Acc@1  25.78 ( 26.21)	Acc@5  59.38 ( 60.11)
Epoch: [6][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5508e+00 (2.8397e+00)	Acc@1  32.03 ( 26.35)	Acc@5  67.19 ( 60.22)
Epoch: [6][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7480e+00 (2.8351e+00)	Acc@1  26.56 ( 26.43)	Acc@5  62.50 ( 60.32)
Epoch: [6][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4492e+00 (2.8330e+00)	Acc@1  32.81 ( 26.46)	Acc@5  71.88 ( 60.42)
Epoch: [6][300/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.8965e+00 (2.8347e+00)	Acc@1  22.66 ( 26.43)	Acc@5  57.81 ( 60.36)
Epoch: [6][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8496e+00 (2.8308e+00)	Acc@1  31.25 ( 26.52)	Acc@5  60.94 ( 60.45)
Epoch: [6][320/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.5215e+00 (2.8271e+00)	Acc@1  35.94 ( 26.64)	Acc@5  67.19 ( 60.54)
Epoch: [6][330/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.3965e+00 (2.8248e+00)	Acc@1  31.25 ( 26.69)	Acc@5  72.66 ( 60.59)
Epoch: [6][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7988e+00 (2.8249e+00)	Acc@1  28.12 ( 26.68)	Acc@5  65.62 ( 60.63)
Epoch: [6][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7988e+00 (2.8226e+00)	Acc@1  29.69 ( 26.76)	Acc@5  59.38 ( 60.73)
Epoch: [6][360/391]	Time  0.038 ( 0.026)	Data  0.006 ( 0.003)	Loss 2.7520e+00 (2.8172e+00)	Acc@1  23.44 ( 26.86)	Acc@5  62.50 ( 60.83)
Epoch: [6][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8711e+00 (2.8173e+00)	Acc@1  28.91 ( 26.91)	Acc@5  58.59 ( 60.81)
Epoch: [6][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6660e+00 (2.8173e+00)	Acc@1  29.69 ( 26.94)	Acc@5  66.41 ( 60.80)
Epoch: [6][390/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.8320e+00 (2.8151e+00)	Acc@1  27.50 ( 26.97)	Acc@5  58.75 ( 60.84)
## e[6] optimizer.zero_grad (sum) time: 0.10528922080993652
## e[6]       loss.backward (sum) time: 2.2737231254577637
## e[6]      optimizer.step (sum) time: 0.9300320148468018
## epoch[6] training(only) time: 10.148801803588867
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 2.6465e+00 (2.6465e+00)	Acc@1  30.00 ( 30.00)	Acc@5  61.00 ( 61.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 2.9961e+00 (2.8223e+00)	Acc@1  16.00 ( 27.27)	Acc@5  54.00 ( 60.82)
Test: [ 20/100]	Time  0.017 ( 0.020)	Loss 2.6035e+00 (2.8442e+00)	Acc@1  32.00 ( 27.19)	Acc@5  67.00 ( 60.33)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.9883e+00 (2.8531e+00)	Acc@1  30.00 ( 27.16)	Acc@5  55.00 ( 59.71)
Test: [ 40/100]	Time  0.017 ( 0.017)	Loss 2.8340e+00 (2.8598e+00)	Acc@1  32.00 ( 27.41)	Acc@5  55.00 ( 59.46)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 2.7070e+00 (2.8676e+00)	Acc@1  36.00 ( 27.75)	Acc@5  63.00 ( 59.25)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 3.1270e+00 (2.8610e+00)	Acc@1  25.00 ( 28.00)	Acc@5  57.00 ( 59.41)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 3.0176e+00 (2.8615e+00)	Acc@1  18.00 ( 27.85)	Acc@5  56.00 ( 59.56)
Test: [ 80/100]	Time  0.015 ( 0.016)	Loss 2.9492e+00 (2.8710e+00)	Acc@1  32.00 ( 27.68)	Acc@5  52.00 ( 59.22)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 2.7344e+00 (2.8585e+00)	Acc@1  31.00 ( 28.05)	Acc@5  59.00 ( 59.40)
 * Acc@1 27.950 Acc@5 59.360
### epoch[6] execution time: 11.754621982574463
EPOCH 7
# current learning rate: 0.1
# Switched to train mode...
Epoch: [7][  0/391]	Time  0.172 ( 0.172)	Data  0.144 ( 0.144)	Loss 2.5801e+00 (2.5801e+00)	Acc@1  34.38 ( 34.38)	Acc@5  65.62 ( 65.62)
Epoch: [7][ 10/391]	Time  0.032 ( 0.038)	Data  0.001 ( 0.015)	Loss 2.8340e+00 (2.7459e+00)	Acc@1  25.00 ( 28.34)	Acc@5  60.16 ( 62.57)
Epoch: [7][ 20/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.009)	Loss 2.8125e+00 (2.7456e+00)	Acc@1  28.91 ( 28.98)	Acc@5  63.28 ( 62.65)
Epoch: [7][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.4805e+00 (2.7307e+00)	Acc@1  32.81 ( 29.41)	Acc@5  67.97 ( 63.10)
Epoch: [7][ 40/391]	Time  0.037 ( 0.028)	Data  0.010 ( 0.005)	Loss 2.4746e+00 (2.6966e+00)	Acc@1  38.28 ( 30.16)	Acc@5  65.62 ( 63.74)
Epoch: [7][ 50/391]	Time  0.021 ( 0.028)	Data  0.002 ( 0.005)	Loss 2.8184e+00 (2.6778e+00)	Acc@1  27.34 ( 30.35)	Acc@5  64.84 ( 64.41)
Epoch: [7][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5703e+00 (2.6782e+00)	Acc@1  34.38 ( 30.38)	Acc@5  70.31 ( 64.36)
Epoch: [7][ 70/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5664e+00 (2.6917e+00)	Acc@1  27.34 ( 29.97)	Acc@5  72.66 ( 64.16)
Epoch: [7][ 80/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7305e+00 (2.6890e+00)	Acc@1  27.34 ( 29.88)	Acc@5  64.84 ( 64.18)
Epoch: [7][ 90/391]	Time  0.033 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.6836e+00 (2.6892e+00)	Acc@1  28.12 ( 29.82)	Acc@5  61.72 ( 64.04)
Epoch: [7][100/391]	Time  0.037 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.7344e+00 (2.6900e+00)	Acc@1  30.47 ( 29.84)	Acc@5  62.50 ( 63.97)
Epoch: [7][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6035e+00 (2.6901e+00)	Acc@1  32.81 ( 30.07)	Acc@5  69.53 ( 63.98)
Epoch: [7][120/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7441e+00 (2.6852e+00)	Acc@1  25.78 ( 30.09)	Acc@5  64.06 ( 64.19)
Epoch: [7][130/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.8652e+00 (2.6783e+00)	Acc@1  30.47 ( 30.16)	Acc@5  61.72 ( 64.28)
Epoch: [7][140/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.5508e+00 (2.6720e+00)	Acc@1  29.69 ( 30.21)	Acc@5  67.19 ( 64.52)
Epoch: [7][150/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4609e+00 (2.6669e+00)	Acc@1  33.59 ( 30.33)	Acc@5  69.53 ( 64.59)
Epoch: [7][160/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5918e+00 (2.6587e+00)	Acc@1  32.03 ( 30.40)	Acc@5  68.75 ( 64.75)
Epoch: [7][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7012e+00 (2.6600e+00)	Acc@1  30.47 ( 30.41)	Acc@5  65.62 ( 64.71)
Epoch: [7][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5625e+00 (2.6607e+00)	Acc@1  32.03 ( 30.46)	Acc@5  71.88 ( 64.73)
Epoch: [7][190/391]	Time  0.043 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.7988e+00 (2.6615e+00)	Acc@1  28.91 ( 30.51)	Acc@5  59.38 ( 64.57)
Epoch: [7][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9023e+00 (2.6618e+00)	Acc@1  30.47 ( 30.50)	Acc@5  58.59 ( 64.54)
Epoch: [7][210/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8203e+00 (2.6560e+00)	Acc@1  29.69 ( 30.60)	Acc@5  60.16 ( 64.63)
Epoch: [7][220/391]	Time  0.020 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.8652e+00 (2.6600e+00)	Acc@1  28.91 ( 30.52)	Acc@5  59.38 ( 64.62)
Epoch: [7][230/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.3066e+00 (2.6574e+00)	Acc@1  41.41 ( 30.60)	Acc@5  75.00 ( 64.74)
Epoch: [7][240/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5195e+00 (2.6522e+00)	Acc@1  34.38 ( 30.69)	Acc@5  66.41 ( 64.90)
Epoch: [7][250/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5840e+00 (2.6501e+00)	Acc@1  28.91 ( 30.73)	Acc@5  70.31 ( 64.91)
Epoch: [7][260/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5723e+00 (2.6488e+00)	Acc@1  32.81 ( 30.77)	Acc@5  61.72 ( 64.97)
Epoch: [7][270/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6387e+00 (2.6479e+00)	Acc@1  30.47 ( 30.78)	Acc@5  67.97 ( 64.99)
Epoch: [7][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5020e+00 (2.6451e+00)	Acc@1  32.03 ( 30.85)	Acc@5  67.97 ( 65.08)
Epoch: [7][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6934e+00 (2.6441e+00)	Acc@1  24.22 ( 30.90)	Acc@5  63.28 ( 65.06)
Epoch: [7][300/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.5117e+00 (2.6428e+00)	Acc@1  32.03 ( 30.93)	Acc@5  67.97 ( 65.11)
Epoch: [7][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6055e+00 (2.6391e+00)	Acc@1  27.34 ( 31.01)	Acc@5  67.19 ( 65.16)
Epoch: [7][320/391]	Time  0.038 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.6855e+00 (2.6418e+00)	Acc@1  24.22 ( 30.99)	Acc@5  60.16 ( 65.08)
Epoch: [7][330/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.7266e+00 (2.6422e+00)	Acc@1  25.78 ( 30.95)	Acc@5  64.06 ( 65.04)
Epoch: [7][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5938e+00 (2.6428e+00)	Acc@1  30.47 ( 30.94)	Acc@5  63.28 ( 64.99)
Epoch: [7][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3574e+00 (2.6397e+00)	Acc@1  39.84 ( 31.02)	Acc@5  72.66 ( 65.08)
Epoch: [7][360/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.5625e+00 (2.6381e+00)	Acc@1  28.12 ( 31.03)	Acc@5  67.97 ( 65.16)
Epoch: [7][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4629e+00 (2.6367e+00)	Acc@1  25.00 ( 31.03)	Acc@5  69.53 ( 65.20)
Epoch: [7][380/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6133e+00 (2.6374e+00)	Acc@1  33.59 ( 31.03)	Acc@5  64.84 ( 65.17)
Epoch: [7][390/391]	Time  0.018 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.5977e+00 (2.6344e+00)	Acc@1  30.00 ( 31.05)	Acc@5  67.50 ( 65.27)
## e[7] optimizer.zero_grad (sum) time: 0.10492920875549316
## e[7]       loss.backward (sum) time: 2.282010793685913
## e[7]      optimizer.step (sum) time: 0.9364986419677734
## epoch[7] training(only) time: 10.072130918502808
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 2.4902e+00 (2.4902e+00)	Acc@1  38.00 ( 38.00)	Acc@5  67.00 ( 67.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 2.7090e+00 (2.5328e+00)	Acc@1  31.00 ( 36.27)	Acc@5  63.00 ( 67.18)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 2.2617e+00 (2.5274e+00)	Acc@1  37.00 ( 35.33)	Acc@5  73.00 ( 67.24)
Test: [ 30/100]	Time  0.014 ( 0.018)	Loss 2.5293e+00 (2.5192e+00)	Acc@1  31.00 ( 34.97)	Acc@5  69.00 ( 66.97)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 2.6387e+00 (2.5237e+00)	Acc@1  29.00 ( 34.27)	Acc@5  60.00 ( 66.59)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 2.3965e+00 (2.5376e+00)	Acc@1  34.00 ( 34.08)	Acc@5  67.00 ( 66.43)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 2.4570e+00 (2.5195e+00)	Acc@1  38.00 ( 34.30)	Acc@5  70.00 ( 66.87)
Test: [ 70/100]	Time  0.018 ( 0.016)	Loss 2.6582e+00 (2.5212e+00)	Acc@1  37.00 ( 34.34)	Acc@5  67.00 ( 66.79)
Test: [ 80/100]	Time  0.018 ( 0.015)	Loss 2.8691e+00 (2.5351e+00)	Acc@1  31.00 ( 33.98)	Acc@5  57.00 ( 66.63)
Test: [ 90/100]	Time  0.017 ( 0.015)	Loss 2.3730e+00 (2.5292e+00)	Acc@1  40.00 ( 34.33)	Acc@5  66.00 ( 66.73)
 * Acc@1 34.340 Acc@5 66.660
### epoch[7] execution time: 11.656732082366943
EPOCH 8
# current learning rate: 0.1
# Switched to train mode...
Epoch: [8][  0/391]	Time  0.183 ( 0.183)	Data  0.148 ( 0.148)	Loss 2.4355e+00 (2.4355e+00)	Acc@1  40.62 ( 40.62)	Acc@5  66.41 ( 66.41)
Epoch: [8][ 10/391]	Time  0.021 ( 0.037)	Data  0.002 ( 0.015)	Loss 2.4902e+00 (2.5272e+00)	Acc@1  34.38 ( 32.88)	Acc@5  68.75 ( 67.40)
Epoch: [8][ 20/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.009)	Loss 2.4414e+00 (2.5037e+00)	Acc@1  32.03 ( 33.22)	Acc@5  68.75 ( 68.49)
Epoch: [8][ 30/391]	Time  0.021 ( 0.030)	Data  0.002 ( 0.007)	Loss 2.3574e+00 (2.5113e+00)	Acc@1  33.59 ( 33.22)	Acc@5  71.88 ( 68.04)
Epoch: [8][ 40/391]	Time  0.024 ( 0.029)	Data  0.002 ( 0.006)	Loss 2.5430e+00 (2.5055e+00)	Acc@1  30.47 ( 33.31)	Acc@5  67.97 ( 68.04)
Epoch: [8][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.5820e+00 (2.5088e+00)	Acc@1  32.81 ( 33.27)	Acc@5  60.16 ( 67.66)
Epoch: [8][ 60/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.5801e+00 (2.5174e+00)	Acc@1  35.94 ( 33.43)	Acc@5  64.06 ( 67.58)
Epoch: [8][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4141e+00 (2.5142e+00)	Acc@1  32.03 ( 33.42)	Acc@5  67.97 ( 67.50)
Epoch: [8][ 80/391]	Time  0.039 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.2207e+00 (2.5068e+00)	Acc@1  45.31 ( 33.65)	Acc@5  76.56 ( 67.77)
Epoch: [8][ 90/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.4297e+00 (2.5147e+00)	Acc@1  37.50 ( 33.54)	Acc@5  67.97 ( 67.67)
Epoch: [8][100/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.004)	Loss 2.7246e+00 (2.5131e+00)	Acc@1  27.34 ( 33.60)	Acc@5  66.41 ( 67.87)
Epoch: [8][110/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5352e+00 (2.5069e+00)	Acc@1  38.28 ( 33.83)	Acc@5  73.44 ( 68.10)
Epoch: [8][120/391]	Time  0.041 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.3613e+00 (2.5097e+00)	Acc@1  34.38 ( 33.77)	Acc@5  71.88 ( 68.00)
Epoch: [8][130/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4961e+00 (2.5037e+00)	Acc@1  34.38 ( 33.81)	Acc@5  71.09 ( 68.27)
Epoch: [8][140/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.7109e+00 (2.5097e+00)	Acc@1  33.59 ( 33.70)	Acc@5  65.62 ( 68.08)
Epoch: [8][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4727e+00 (2.5041e+00)	Acc@1  35.16 ( 33.88)	Acc@5  67.97 ( 68.20)
Epoch: [8][160/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1191e+00 (2.4989e+00)	Acc@1  39.84 ( 34.03)	Acc@5  75.00 ( 68.32)
Epoch: [8][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3535e+00 (2.4994e+00)	Acc@1  36.72 ( 33.99)	Acc@5  71.88 ( 68.33)
Epoch: [8][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6426e+00 (2.5048e+00)	Acc@1  28.91 ( 33.88)	Acc@5  63.28 ( 68.28)
Epoch: [8][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.4453e+00 (2.5036e+00)	Acc@1  35.16 ( 33.89)	Acc@5  64.84 ( 68.32)
Epoch: [8][200/391]	Time  0.031 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.2656e+00 (2.5034e+00)	Acc@1  35.94 ( 33.96)	Acc@5  73.44 ( 68.28)
Epoch: [8][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6094e+00 (2.5058e+00)	Acc@1  32.81 ( 33.88)	Acc@5  65.62 ( 68.24)
Epoch: [8][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6523e+00 (2.5057e+00)	Acc@1  28.91 ( 33.88)	Acc@5  64.84 ( 68.18)
Epoch: [8][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4609e+00 (2.5039e+00)	Acc@1  31.25 ( 33.84)	Acc@5  71.09 ( 68.26)
Epoch: [8][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3906e+00 (2.5034e+00)	Acc@1  39.06 ( 33.88)	Acc@5  71.88 ( 68.30)
Epoch: [8][250/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5215e+00 (2.5028e+00)	Acc@1  32.03 ( 33.92)	Acc@5  64.06 ( 68.31)
Epoch: [8][260/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4570e+00 (2.5009e+00)	Acc@1  35.16 ( 34.00)	Acc@5  69.53 ( 68.37)
Epoch: [8][270/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5840e+00 (2.5028e+00)	Acc@1  30.47 ( 33.94)	Acc@5  66.41 ( 68.33)
Epoch: [8][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3965e+00 (2.5007e+00)	Acc@1  39.84 ( 34.01)	Acc@5  68.75 ( 68.38)
Epoch: [8][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4414e+00 (2.4966e+00)	Acc@1  33.59 ( 34.10)	Acc@5  62.50 ( 68.43)
Epoch: [8][300/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.4922e+00 (2.4973e+00)	Acc@1  34.38 ( 34.08)	Acc@5  68.75 ( 68.36)
Epoch: [8][310/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.5215e+00 (2.4944e+00)	Acc@1  30.47 ( 34.17)	Acc@5  67.19 ( 68.46)
Epoch: [8][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5273e+00 (2.4912e+00)	Acc@1  38.28 ( 34.26)	Acc@5  60.94 ( 68.48)
Epoch: [8][330/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3848e+00 (2.4898e+00)	Acc@1  31.25 ( 34.27)	Acc@5  75.00 ( 68.49)
Epoch: [8][340/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.5527e+00 (2.4881e+00)	Acc@1  37.50 ( 34.33)	Acc@5  61.72 ( 68.51)
Epoch: [8][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4414e+00 (2.4864e+00)	Acc@1  35.16 ( 34.36)	Acc@5  68.75 ( 68.60)
Epoch: [8][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4648e+00 (2.4829e+00)	Acc@1  35.16 ( 34.45)	Acc@5  70.31 ( 68.70)
Epoch: [8][370/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5879e+00 (2.4845e+00)	Acc@1  29.69 ( 34.47)	Acc@5  64.84 ( 68.66)
Epoch: [8][380/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3418e+00 (2.4848e+00)	Acc@1  34.38 ( 34.50)	Acc@5  71.88 ( 68.66)
Epoch: [8][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7148e+00 (2.4854e+00)	Acc@1  27.50 ( 34.48)	Acc@5  62.50 ( 68.64)
## e[8] optimizer.zero_grad (sum) time: 0.10535740852355957
## e[8]       loss.backward (sum) time: 2.270146131515503
## e[8]      optimizer.step (sum) time: 0.9290738105773926
## epoch[8] training(only) time: 10.072341918945312
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 2.2070e+00 (2.2070e+00)	Acc@1  37.00 ( 37.00)	Acc@5  72.00 ( 72.00)
Test: [ 10/100]	Time  0.015 ( 0.026)	Loss 2.4121e+00 (2.3002e+00)	Acc@1  36.00 ( 37.91)	Acc@5  65.00 ( 70.82)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 2.2266e+00 (2.3106e+00)	Acc@1  42.00 ( 38.38)	Acc@5  74.00 ( 71.10)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.3496e+00 (2.3113e+00)	Acc@1  43.00 ( 38.48)	Acc@5  71.00 ( 71.39)
Test: [ 40/100]	Time  0.022 ( 0.017)	Loss 2.2852e+00 (2.3077e+00)	Acc@1  38.00 ( 38.46)	Acc@5  74.00 ( 71.32)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 2.2734e+00 (2.3236e+00)	Acc@1  39.00 ( 38.45)	Acc@5  69.00 ( 70.86)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 2.4082e+00 (2.3177e+00)	Acc@1  37.00 ( 38.36)	Acc@5  73.00 ( 70.97)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 2.4824e+00 (2.3224e+00)	Acc@1  33.00 ( 38.30)	Acc@5  68.00 ( 71.10)
Test: [ 80/100]	Time  0.013 ( 0.016)	Loss 2.5527e+00 (2.3422e+00)	Acc@1  31.00 ( 37.91)	Acc@5  67.00 ( 70.77)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.3711e+00 (2.3391e+00)	Acc@1  43.00 ( 38.08)	Acc@5  69.00 ( 71.03)
 * Acc@1 38.130 Acc@5 71.050
### epoch[8] execution time: 11.69716215133667
EPOCH 9
# current learning rate: 0.1
# Switched to train mode...
Epoch: [9][  0/391]	Time  0.184 ( 0.184)	Data  0.155 ( 0.155)	Loss 2.4023e+00 (2.4023e+00)	Acc@1  36.72 ( 36.72)	Acc@5  68.75 ( 68.75)
Epoch: [9][ 10/391]	Time  0.021 ( 0.040)	Data  0.000 ( 0.016)	Loss 2.3125e+00 (2.3248e+00)	Acc@1  41.41 ( 37.22)	Acc@5  75.00 ( 71.09)
Epoch: [9][ 20/391]	Time  0.029 ( 0.033)	Data  0.003 ( 0.009)	Loss 2.2695e+00 (2.3306e+00)	Acc@1  40.62 ( 37.17)	Acc@5  71.88 ( 71.17)
Epoch: [9][ 30/391]	Time  0.030 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.4492e+00 (2.3724e+00)	Acc@1  37.50 ( 36.97)	Acc@5  67.97 ( 70.67)
Epoch: [9][ 40/391]	Time  0.043 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.3711e+00 (2.3527e+00)	Acc@1  38.28 ( 37.58)	Acc@5  69.53 ( 70.98)
Epoch: [9][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.3379e+00 (2.3607e+00)	Acc@1  32.81 ( 37.21)	Acc@5  74.22 ( 70.96)
Epoch: [9][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.6484e+00 (2.3604e+00)	Acc@1  33.59 ( 37.09)	Acc@5  64.84 ( 71.13)
Epoch: [9][ 70/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5273e+00 (2.3604e+00)	Acc@1  35.94 ( 37.13)	Acc@5  64.84 ( 70.97)
Epoch: [9][ 80/391]	Time  0.040 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.4707e+00 (2.3685e+00)	Acc@1  36.72 ( 37.15)	Acc@5  71.09 ( 70.88)
Epoch: [9][ 90/391]	Time  0.034 ( 0.027)	Data  0.003 ( 0.004)	Loss 2.2461e+00 (2.3593e+00)	Acc@1  40.62 ( 37.27)	Acc@5  71.88 ( 71.21)
Epoch: [9][100/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.2656e+00 (2.3603e+00)	Acc@1  40.62 ( 37.31)	Acc@5  75.78 ( 71.23)
Epoch: [9][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 2.4375e+00 (2.3552e+00)	Acc@1  39.06 ( 37.38)	Acc@5  67.97 ( 71.37)
Epoch: [9][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5488e+00 (2.3498e+00)	Acc@1  37.50 ( 37.49)	Acc@5  66.41 ( 71.42)
Epoch: [9][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4531e+00 (2.3490e+00)	Acc@1  39.84 ( 37.49)	Acc@5  71.09 ( 71.36)
Epoch: [9][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2910e+00 (2.3491e+00)	Acc@1  39.06 ( 37.52)	Acc@5  68.75 ( 71.25)
Epoch: [9][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4824e+00 (2.3489e+00)	Acc@1  35.94 ( 37.45)	Acc@5  67.97 ( 71.27)
Epoch: [9][160/391]	Time  0.032 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.4199e+00 (2.3451e+00)	Acc@1  40.62 ( 37.52)	Acc@5  65.62 ( 71.36)
Epoch: [9][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4062e+00 (2.3467e+00)	Acc@1  37.50 ( 37.50)	Acc@5  71.09 ( 71.37)
Epoch: [9][180/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.2168e+00 (2.3469e+00)	Acc@1  42.19 ( 37.52)	Acc@5  71.88 ( 71.35)
Epoch: [9][190/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1895e+00 (2.3474e+00)	Acc@1  41.41 ( 37.49)	Acc@5  76.56 ( 71.32)
Epoch: [9][200/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2305e+00 (2.3476e+00)	Acc@1  39.06 ( 37.52)	Acc@5  73.44 ( 71.25)
Epoch: [9][210/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1445e+00 (2.3455e+00)	Acc@1  39.84 ( 37.53)	Acc@5  78.91 ( 71.28)
Epoch: [9][220/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5977e+00 (2.3479e+00)	Acc@1  35.16 ( 37.48)	Acc@5  60.16 ( 71.20)
Epoch: [9][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7090e+00 (2.3484e+00)	Acc@1  33.59 ( 37.54)	Acc@5  61.72 ( 71.14)
Epoch: [9][240/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4961e+00 (2.3498e+00)	Acc@1  32.81 ( 37.48)	Acc@5  65.62 ( 71.11)
Epoch: [9][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3867e+00 (2.3466e+00)	Acc@1  35.16 ( 37.53)	Acc@5  74.22 ( 71.21)
Epoch: [9][260/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2637e+00 (2.3470e+00)	Acc@1  37.50 ( 37.48)	Acc@5  78.12 ( 71.24)
Epoch: [9][270/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2402e+00 (2.3501e+00)	Acc@1  38.28 ( 37.40)	Acc@5  73.44 ( 71.22)
Epoch: [9][280/391]	Time  0.045 ( 0.026)	Data  0.011 ( 0.003)	Loss 2.3125e+00 (2.3500e+00)	Acc@1  36.72 ( 37.42)	Acc@5  71.88 ( 71.22)
Epoch: [9][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4434e+00 (2.3471e+00)	Acc@1  32.03 ( 37.51)	Acc@5  75.00 ( 71.26)
Epoch: [9][300/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3359e+00 (2.3473e+00)	Acc@1  40.62 ( 37.57)	Acc@5  67.97 ( 71.23)
Epoch: [9][310/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1992e+00 (2.3433e+00)	Acc@1  46.88 ( 37.69)	Acc@5  73.44 ( 71.34)
Epoch: [9][320/391]	Time  0.038 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.5293e+00 (2.3443e+00)	Acc@1  34.38 ( 37.66)	Acc@5  67.19 ( 71.34)
Epoch: [9][330/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3164e+00 (2.3444e+00)	Acc@1  39.06 ( 37.71)	Acc@5  69.53 ( 71.31)
Epoch: [9][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1621e+00 (2.3418e+00)	Acc@1  42.19 ( 37.81)	Acc@5  75.78 ( 71.37)
Epoch: [9][350/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.1934e+00 (2.3390e+00)	Acc@1  38.28 ( 37.86)	Acc@5  75.78 ( 71.42)
Epoch: [9][360/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2754e+00 (2.3370e+00)	Acc@1  38.28 ( 37.89)	Acc@5  72.66 ( 71.47)
Epoch: [9][370/391]	Time  0.038 ( 0.026)	Data  0.004 ( 0.002)	Loss 2.3320e+00 (2.3388e+00)	Acc@1  39.06 ( 37.86)	Acc@5  71.09 ( 71.44)
Epoch: [9][380/391]	Time  0.040 ( 0.026)	Data  0.004 ( 0.002)	Loss 2.2715e+00 (2.3386e+00)	Acc@1  35.16 ( 37.86)	Acc@5  75.00 ( 71.46)
Epoch: [9][390/391]	Time  0.016 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.5605e+00 (2.3376e+00)	Acc@1  31.25 ( 37.92)	Acc@5  65.00 ( 71.50)
## e[9] optimizer.zero_grad (sum) time: 0.10641622543334961
## e[9]       loss.backward (sum) time: 2.2840330600738525
## e[9]      optimizer.step (sum) time: 0.9375240802764893
## epoch[9] training(only) time: 10.068552017211914
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 2.2480e+00 (2.2480e+00)	Acc@1  40.00 ( 40.00)	Acc@5  73.00 ( 73.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 2.3906e+00 (2.2752e+00)	Acc@1  34.00 ( 41.27)	Acc@5  71.00 ( 72.45)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 2.1074e+00 (2.2475e+00)	Acc@1  42.00 ( 40.43)	Acc@5  74.00 ( 73.10)
Test: [ 30/100]	Time  0.015 ( 0.018)	Loss 2.4805e+00 (2.2308e+00)	Acc@1  39.00 ( 40.52)	Acc@5  69.00 ( 73.74)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 2.2109e+00 (2.2281e+00)	Acc@1  43.00 ( 40.39)	Acc@5  73.00 ( 74.00)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 2.0273e+00 (2.2417e+00)	Acc@1  46.00 ( 40.08)	Acc@5  74.00 ( 73.27)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.9932e+00 (2.2252e+00)	Acc@1  44.00 ( 40.20)	Acc@5  80.00 ( 73.56)
Test: [ 70/100]	Time  0.029 ( 0.015)	Loss 2.4668e+00 (2.2312e+00)	Acc@1  34.00 ( 40.18)	Acc@5  71.00 ( 73.52)
Test: [ 80/100]	Time  0.022 ( 0.015)	Loss 2.3379e+00 (2.2395e+00)	Acc@1  43.00 ( 39.96)	Acc@5  71.00 ( 73.27)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.2930e+00 (2.2357e+00)	Acc@1  41.00 ( 40.13)	Acc@5  71.00 ( 73.30)
 * Acc@1 40.130 Acc@5 73.150
### epoch[9] execution time: 11.661950826644897
EPOCH 10
# current learning rate: 0.1
# Switched to train mode...
Epoch: [10][  0/391]	Time  0.177 ( 0.177)	Data  0.149 ( 0.149)	Loss 2.1406e+00 (2.1406e+00)	Acc@1  42.97 ( 42.97)	Acc@5  74.22 ( 74.22)
Epoch: [10][ 10/391]	Time  0.030 ( 0.039)	Data  0.000 ( 0.015)	Loss 2.2207e+00 (2.2841e+00)	Acc@1  39.84 ( 39.28)	Acc@5  73.44 ( 72.66)
Epoch: [10][ 20/391]	Time  0.022 ( 0.033)	Data  0.001 ( 0.009)	Loss 2.3418e+00 (2.2380e+00)	Acc@1  35.16 ( 39.66)	Acc@5  66.41 ( 74.22)
Epoch: [10][ 30/391]	Time  0.037 ( 0.030)	Data  0.002 ( 0.006)	Loss 2.4141e+00 (2.2531e+00)	Acc@1  35.94 ( 39.72)	Acc@5  67.19 ( 73.54)
Epoch: [10][ 40/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.5078e+00 (2.2482e+00)	Acc@1  31.25 ( 40.03)	Acc@5  71.09 ( 73.65)
Epoch: [10][ 50/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.0859e+00 (2.2465e+00)	Acc@1  46.09 ( 40.03)	Acc@5  75.78 ( 73.54)
Epoch: [10][ 60/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.9893e+00 (2.2354e+00)	Acc@1  42.19 ( 40.19)	Acc@5  78.91 ( 73.83)
Epoch: [10][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.2207e+00 (2.2377e+00)	Acc@1  41.41 ( 40.04)	Acc@5  74.22 ( 73.81)
Epoch: [10][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1309e+00 (2.2405e+00)	Acc@1  46.09 ( 40.29)	Acc@5  75.00 ( 73.74)
Epoch: [10][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.2539e+00 (2.2391e+00)	Acc@1  37.50 ( 40.14)	Acc@5  74.22 ( 73.70)
Epoch: [10][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4570e+00 (2.2393e+00)	Acc@1  30.47 ( 40.07)	Acc@5  71.09 ( 73.78)
Epoch: [10][110/391]	Time  0.046 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.0254e+00 (2.2466e+00)	Acc@1  42.97 ( 39.86)	Acc@5  78.12 ( 73.59)
Epoch: [10][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0566e+00 (2.2411e+00)	Acc@1  44.53 ( 39.95)	Acc@5  73.44 ( 73.66)
Epoch: [10][130/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.4688e+00 (2.2417e+00)	Acc@1  35.94 ( 40.07)	Acc@5  71.09 ( 73.73)
Epoch: [10][140/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.1738e+00 (2.2416e+00)	Acc@1  46.09 ( 40.08)	Acc@5  74.22 ( 73.71)
Epoch: [10][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1230e+00 (2.2420e+00)	Acc@1  40.62 ( 40.06)	Acc@5  74.22 ( 73.71)
Epoch: [10][160/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3281e+00 (2.2474e+00)	Acc@1  35.94 ( 39.89)	Acc@5  67.19 ( 73.55)
Epoch: [10][170/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2891e+00 (2.2509e+00)	Acc@1  34.38 ( 39.77)	Acc@5  74.22 ( 73.52)
Epoch: [10][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1523e+00 (2.2486e+00)	Acc@1  39.06 ( 39.87)	Acc@5  76.56 ( 73.56)
Epoch: [10][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2422e+00 (2.2455e+00)	Acc@1  43.75 ( 39.93)	Acc@5  74.22 ( 73.63)
Epoch: [10][200/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0820e+00 (2.2430e+00)	Acc@1  46.88 ( 39.96)	Acc@5  72.66 ( 73.55)
Epoch: [10][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5820e+00 (2.2446e+00)	Acc@1  34.38 ( 39.96)	Acc@5  65.62 ( 73.59)
Epoch: [10][220/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0508e+00 (2.2383e+00)	Acc@1  47.66 ( 40.14)	Acc@5  78.12 ( 73.72)
Epoch: [10][230/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1680e+00 (2.2358e+00)	Acc@1  42.19 ( 40.25)	Acc@5  75.00 ( 73.78)
Epoch: [10][240/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2559e+00 (2.2318e+00)	Acc@1  40.62 ( 40.34)	Acc@5  73.44 ( 73.85)
Epoch: [10][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0898e+00 (2.2319e+00)	Acc@1  45.31 ( 40.41)	Acc@5  72.66 ( 73.81)
Epoch: [10][260/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3477e+00 (2.2337e+00)	Acc@1  32.03 ( 40.37)	Acc@5  70.31 ( 73.74)
Epoch: [10][270/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.4453e+00 (2.2364e+00)	Acc@1  38.28 ( 40.33)	Acc@5  71.09 ( 73.69)
Epoch: [10][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9746e+00 (2.2309e+00)	Acc@1  46.09 ( 40.49)	Acc@5  78.12 ( 73.78)
Epoch: [10][290/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3301e+00 (2.2320e+00)	Acc@1  35.94 ( 40.41)	Acc@5  73.44 ( 73.74)
Epoch: [10][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3438e+00 (2.2304e+00)	Acc@1  43.75 ( 40.47)	Acc@5  71.88 ( 73.80)
Epoch: [10][310/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2344e+00 (2.2314e+00)	Acc@1  41.41 ( 40.45)	Acc@5  75.78 ( 73.79)
Epoch: [10][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5391e+00 (2.2351e+00)	Acc@1  34.38 ( 40.36)	Acc@5  66.41 ( 73.74)
Epoch: [10][330/391]	Time  0.043 ( 0.026)	Data  0.005 ( 0.002)	Loss 2.0898e+00 (2.2345e+00)	Acc@1  44.53 ( 40.35)	Acc@5  76.56 ( 73.79)
Epoch: [10][340/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.2266e+00 (2.2332e+00)	Acc@1  39.06 ( 40.36)	Acc@5  71.88 ( 73.78)
Epoch: [10][350/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4062e+00 (2.2317e+00)	Acc@1  30.47 ( 40.41)	Acc@5  74.22 ( 73.84)
Epoch: [10][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0312e+00 (2.2323e+00)	Acc@1  44.53 ( 40.43)	Acc@5  73.44 ( 73.78)
Epoch: [10][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4062e+00 (2.2327e+00)	Acc@1  38.28 ( 40.42)	Acc@5  68.75 ( 73.79)
Epoch: [10][380/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5586e+00 (2.2336e+00)	Acc@1  29.69 ( 40.44)	Acc@5  67.19 ( 73.76)
Epoch: [10][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3496e+00 (2.2353e+00)	Acc@1  37.50 ( 40.39)	Acc@5  68.75 ( 73.72)
## e[10] optimizer.zero_grad (sum) time: 0.1058034896850586
## e[10]       loss.backward (sum) time: 2.2276763916015625
## e[10]      optimizer.step (sum) time: 0.9154081344604492
## epoch[10] training(only) time: 10.139841794967651
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 2.1562e+00 (2.1562e+00)	Acc@1  46.00 ( 46.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 2.1270e+00 (2.1196e+00)	Acc@1  45.00 ( 44.91)	Acc@5  77.00 ( 76.27)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.7646e+00 (2.0917e+00)	Acc@1  54.00 ( 45.05)	Acc@5  83.00 ( 76.24)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.9277e+00 (2.0791e+00)	Acc@1  43.00 ( 44.68)	Acc@5  82.00 ( 76.35)
Test: [ 40/100]	Time  0.024 ( 0.017)	Loss 2.0430e+00 (2.0793e+00)	Acc@1  45.00 ( 44.34)	Acc@5  77.00 ( 76.46)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 2.0098e+00 (2.1050e+00)	Acc@1  50.00 ( 43.94)	Acc@5  73.00 ( 75.69)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 2.1914e+00 (2.0977e+00)	Acc@1  43.00 ( 44.18)	Acc@5  77.00 ( 75.95)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 2.2090e+00 (2.0992e+00)	Acc@1  44.00 ( 44.01)	Acc@5  72.00 ( 75.76)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 2.2715e+00 (2.1137e+00)	Acc@1  39.00 ( 43.78)	Acc@5  70.00 ( 75.49)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.2344e+00 (2.1097e+00)	Acc@1  42.00 ( 43.91)	Acc@5  72.00 ( 75.59)
 * Acc@1 43.930 Acc@5 75.650
### epoch[10] execution time: 11.74293828010559
EPOCH 11
# current learning rate: 0.1
# Switched to train mode...
Epoch: [11][  0/391]	Time  0.168 ( 0.168)	Data  0.140 ( 0.140)	Loss 2.1465e+00 (2.1465e+00)	Acc@1  35.94 ( 35.94)	Acc@5  76.56 ( 76.56)
Epoch: [11][ 10/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.014)	Loss 2.2090e+00 (2.1866e+00)	Acc@1  40.62 ( 41.62)	Acc@5  73.44 ( 74.50)
Epoch: [11][ 20/391]	Time  0.020 ( 0.031)	Data  0.001 ( 0.008)	Loss 1.9512e+00 (2.1567e+00)	Acc@1  49.22 ( 42.08)	Acc@5  82.03 ( 75.19)
Epoch: [11][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.2559e+00 (2.1457e+00)	Acc@1  42.19 ( 42.62)	Acc@5  74.22 ( 75.25)
Epoch: [11][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.1230e+00 (2.1384e+00)	Acc@1  48.44 ( 42.76)	Acc@5  77.34 ( 75.36)
Epoch: [11][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.2168e+00 (2.1381e+00)	Acc@1  37.50 ( 42.80)	Acc@5  70.31 ( 75.55)
Epoch: [11][ 60/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.9570e+00 (2.1322e+00)	Acc@1  47.66 ( 42.93)	Acc@5  78.12 ( 75.72)
Epoch: [11][ 70/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0430e+00 (2.1286e+00)	Acc@1  42.19 ( 43.12)	Acc@5  78.12 ( 75.87)
Epoch: [11][ 80/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0449e+00 (2.1444e+00)	Acc@1  46.09 ( 43.09)	Acc@5  78.91 ( 75.57)
Epoch: [11][ 90/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2949e+00 (2.1428e+00)	Acc@1  39.84 ( 42.99)	Acc@5  72.66 ( 75.64)
Epoch: [11][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0195e+00 (2.1416e+00)	Acc@1  39.84 ( 42.81)	Acc@5  81.25 ( 75.74)
Epoch: [11][110/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.1074e+00 (2.1349e+00)	Acc@1  39.84 ( 42.84)	Acc@5  77.34 ( 75.89)
Epoch: [11][120/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.0938e+00 (2.1411e+00)	Acc@1  40.62 ( 42.70)	Acc@5  77.34 ( 75.65)
Epoch: [11][130/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2871e+00 (2.1435e+00)	Acc@1  39.84 ( 42.69)	Acc@5  76.56 ( 75.61)
Epoch: [11][140/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9248e+00 (2.1362e+00)	Acc@1  50.78 ( 42.92)	Acc@5  77.34 ( 75.71)
Epoch: [11][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2773e+00 (2.1388e+00)	Acc@1  37.50 ( 42.84)	Acc@5  75.78 ( 75.66)
Epoch: [11][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0625e+00 (2.1450e+00)	Acc@1  50.78 ( 42.74)	Acc@5  78.12 ( 75.60)
Epoch: [11][170/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0664e+00 (2.1450e+00)	Acc@1  43.75 ( 42.75)	Acc@5  78.12 ( 75.58)
Epoch: [11][180/391]	Time  0.036 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.1133e+00 (2.1444e+00)	Acc@1  40.62 ( 42.73)	Acc@5  72.66 ( 75.59)
Epoch: [11][190/391]	Time  0.030 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.3086e+00 (2.1431e+00)	Acc@1  38.28 ( 42.73)	Acc@5  68.75 ( 75.61)
Epoch: [11][200/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9863e+00 (2.1429e+00)	Acc@1  46.88 ( 42.74)	Acc@5  81.25 ( 75.62)
Epoch: [11][210/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.3379e+00 (2.1422e+00)	Acc@1  36.72 ( 42.75)	Acc@5  69.53 ( 75.63)
Epoch: [11][220/391]	Time  0.034 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.1523e+00 (2.1412e+00)	Acc@1  46.88 ( 42.75)	Acc@5  74.22 ( 75.64)
Epoch: [11][230/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1016e+00 (2.1465e+00)	Acc@1  44.53 ( 42.69)	Acc@5  75.00 ( 75.55)
Epoch: [11][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4023e+00 (2.1468e+00)	Acc@1  35.94 ( 42.71)	Acc@5  65.62 ( 75.48)
Epoch: [11][250/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.8975e+00 (2.1446e+00)	Acc@1  49.22 ( 42.76)	Acc@5  80.47 ( 75.55)
Epoch: [11][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2324e+00 (2.1453e+00)	Acc@1  46.09 ( 42.72)	Acc@5  75.78 ( 75.52)
Epoch: [11][270/391]	Time  0.031 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.1094e+00 (2.1436e+00)	Acc@1  42.19 ( 42.75)	Acc@5  80.47 ( 75.57)
Epoch: [11][280/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.1172e+00 (2.1445e+00)	Acc@1  45.31 ( 42.70)	Acc@5  78.12 ( 75.57)
Epoch: [11][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1562e+00 (2.1469e+00)	Acc@1  42.97 ( 42.69)	Acc@5  75.78 ( 75.51)
Epoch: [11][300/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0234e+00 (2.1467e+00)	Acc@1  46.88 ( 42.70)	Acc@5  78.12 ( 75.51)
Epoch: [11][310/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9561e+00 (2.1445e+00)	Acc@1  46.09 ( 42.78)	Acc@5  81.25 ( 75.55)
Epoch: [11][320/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0195e+00 (2.1451e+00)	Acc@1  44.53 ( 42.71)	Acc@5  80.47 ( 75.55)
Epoch: [11][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9736e+00 (2.1450e+00)	Acc@1  46.88 ( 42.68)	Acc@5  78.91 ( 75.52)
Epoch: [11][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1211e+00 (2.1454e+00)	Acc@1  43.75 ( 42.70)	Acc@5  80.47 ( 75.53)
Epoch: [11][350/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2207e+00 (2.1454e+00)	Acc@1  39.06 ( 42.70)	Acc@5  73.44 ( 75.57)
Epoch: [11][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1055e+00 (2.1433e+00)	Acc@1  43.75 ( 42.77)	Acc@5  78.12 ( 75.63)
Epoch: [11][370/391]	Time  0.037 ( 0.026)	Data  0.007 ( 0.002)	Loss 2.4785e+00 (2.1430e+00)	Acc@1  35.16 ( 42.77)	Acc@5  64.06 ( 75.62)
Epoch: [11][380/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.2129e+00 (2.1411e+00)	Acc@1  42.19 ( 42.84)	Acc@5  75.78 ( 75.66)
Epoch: [11][390/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3320e+00 (2.1386e+00)	Acc@1  46.25 ( 42.92)	Acc@5  73.75 ( 75.73)
## e[11] optimizer.zero_grad (sum) time: 0.10605812072753906
## e[11]       loss.backward (sum) time: 2.2653515338897705
## e[11]      optimizer.step (sum) time: 0.9118010997772217
## epoch[11] training(only) time: 10.117062330245972
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.9678e+00 (1.9678e+00)	Acc@1  49.00 ( 49.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 2.0723e+00 (2.0724e+00)	Acc@1  47.00 ( 45.55)	Acc@5  78.00 ( 76.36)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.8711e+00 (2.0562e+00)	Acc@1  55.00 ( 45.76)	Acc@5  75.00 ( 75.81)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 2.0176e+00 (2.0634e+00)	Acc@1  47.00 ( 45.45)	Acc@5  80.00 ( 75.94)
Test: [ 40/100]	Time  0.009 ( 0.017)	Loss 1.9766e+00 (2.0617e+00)	Acc@1  43.00 ( 45.07)	Acc@5  75.00 ( 76.20)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 2.0176e+00 (2.0772e+00)	Acc@1  51.00 ( 44.59)	Acc@5  74.00 ( 75.71)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 2.0059e+00 (2.0672e+00)	Acc@1  46.00 ( 44.70)	Acc@5  81.00 ( 75.95)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 2.1484e+00 (2.0660e+00)	Acc@1  42.00 ( 44.85)	Acc@5  74.00 ( 76.00)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 2.2246e+00 (2.0722e+00)	Acc@1  46.00 ( 44.40)	Acc@5  72.00 ( 76.15)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 2.1875e+00 (2.0665e+00)	Acc@1  47.00 ( 44.67)	Acc@5  77.00 ( 76.36)
 * Acc@1 44.820 Acc@5 76.460
### epoch[11] execution time: 11.68854308128357
EPOCH 12
# current learning rate: 0.1
# Switched to train mode...
Epoch: [12][  0/391]	Time  0.168 ( 0.168)	Data  0.141 ( 0.141)	Loss 1.8223e+00 (1.8223e+00)	Acc@1  50.78 ( 50.78)	Acc@5  79.69 ( 79.69)
Epoch: [12][ 10/391]	Time  0.029 ( 0.039)	Data  0.000 ( 0.014)	Loss 1.8154e+00 (1.9767e+00)	Acc@1  48.44 ( 46.45)	Acc@5  82.03 ( 79.55)
Epoch: [12][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 2.0996e+00 (2.0286e+00)	Acc@1  42.97 ( 45.09)	Acc@5  76.56 ( 78.35)
Epoch: [12][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.9688e+00 (2.0563e+00)	Acc@1  45.31 ( 44.78)	Acc@5  84.38 ( 77.70)
Epoch: [12][ 40/391]	Time  0.036 ( 0.029)	Data  0.000 ( 0.005)	Loss 1.9834e+00 (2.0523e+00)	Acc@1  46.88 ( 45.12)	Acc@5  72.66 ( 77.36)
Epoch: [12][ 50/391]	Time  0.034 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.0742e+00 (2.0675e+00)	Acc@1  43.75 ( 44.59)	Acc@5  75.78 ( 76.96)
Epoch: [12][ 60/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.9609e+00 (2.0647e+00)	Acc@1  46.88 ( 44.57)	Acc@5  78.12 ( 76.97)
Epoch: [12][ 70/391]	Time  0.028 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.8076e+00 (2.0466e+00)	Acc@1  45.31 ( 44.61)	Acc@5  81.25 ( 77.18)
Epoch: [12][ 80/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.9736e+00 (2.0470e+00)	Acc@1  48.44 ( 44.74)	Acc@5  75.00 ( 77.15)
Epoch: [12][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1270e+00 (2.0421e+00)	Acc@1  42.97 ( 44.81)	Acc@5  77.34 ( 77.22)
Epoch: [12][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2188e+00 (2.0481e+00)	Acc@1  43.75 ( 44.91)	Acc@5  75.78 ( 77.03)
Epoch: [12][110/391]	Time  0.039 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.0469e+00 (2.0438e+00)	Acc@1  50.00 ( 45.03)	Acc@5  82.81 ( 77.16)
Epoch: [12][120/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.3164e+00 (2.0487e+00)	Acc@1  40.62 ( 45.08)	Acc@5  71.88 ( 77.11)
Epoch: [12][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0293e+00 (2.0469e+00)	Acc@1  48.44 ( 45.12)	Acc@5  74.22 ( 77.14)
Epoch: [12][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1289e+00 (2.0468e+00)	Acc@1  42.19 ( 45.03)	Acc@5  75.78 ( 77.19)
Epoch: [12][150/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9736e+00 (2.0513e+00)	Acc@1  45.31 ( 44.98)	Acc@5  78.12 ( 77.05)
Epoch: [12][160/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1973e+00 (2.0522e+00)	Acc@1  36.72 ( 44.91)	Acc@5  73.44 ( 77.03)
Epoch: [12][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0234e+00 (2.0470e+00)	Acc@1  40.62 ( 44.95)	Acc@5  80.47 ( 77.17)
Epoch: [12][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3047e+00 (2.0434e+00)	Acc@1  38.28 ( 45.17)	Acc@5  75.00 ( 77.20)
Epoch: [12][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2344e+00 (2.0464e+00)	Acc@1  43.75 ( 45.17)	Acc@5  75.00 ( 77.16)
Epoch: [12][200/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9941e+00 (2.0432e+00)	Acc@1  45.31 ( 45.25)	Acc@5  79.69 ( 77.22)
Epoch: [12][210/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1250e+00 (2.0425e+00)	Acc@1  47.66 ( 45.32)	Acc@5  75.78 ( 77.19)
Epoch: [12][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1309e+00 (2.0429e+00)	Acc@1  46.09 ( 45.33)	Acc@5  77.34 ( 77.16)
Epoch: [12][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0469e+00 (2.0433e+00)	Acc@1  43.75 ( 45.32)	Acc@5  78.12 ( 77.16)
Epoch: [12][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2324e+00 (2.0456e+00)	Acc@1  36.72 ( 45.30)	Acc@5  75.78 ( 77.10)
Epoch: [12][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0293e+00 (2.0469e+00)	Acc@1  42.97 ( 45.28)	Acc@5  78.91 ( 77.09)
Epoch: [12][260/391]	Time  0.038 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.0117e+00 (2.0491e+00)	Acc@1  42.19 ( 45.22)	Acc@5  78.91 ( 77.06)
Epoch: [12][270/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0137e+00 (2.0499e+00)	Acc@1  46.88 ( 45.24)	Acc@5  80.47 ( 77.06)
Epoch: [12][280/391]	Time  0.043 ( 0.026)	Data  0.010 ( 0.002)	Loss 2.0996e+00 (2.0491e+00)	Acc@1  44.53 ( 45.27)	Acc@5  75.00 ( 77.12)
Epoch: [12][290/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.1660e+00 (2.0494e+00)	Acc@1  37.50 ( 45.29)	Acc@5  75.00 ( 77.10)
Epoch: [12][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9023e+00 (2.0484e+00)	Acc@1  50.00 ( 45.34)	Acc@5  81.25 ( 77.09)
Epoch: [12][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1602e+00 (2.0449e+00)	Acc@1  36.72 ( 45.39)	Acc@5  77.34 ( 77.15)
Epoch: [12][320/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0078e+00 (2.0457e+00)	Acc@1  44.53 ( 45.36)	Acc@5  78.12 ( 77.16)
Epoch: [12][330/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1719e+00 (2.0465e+00)	Acc@1  40.62 ( 45.37)	Acc@5  75.78 ( 77.12)
Epoch: [12][340/391]	Time  0.022 ( 0.025)	Data  0.004 ( 0.002)	Loss 2.0547e+00 (2.0468e+00)	Acc@1  46.09 ( 45.36)	Acc@5  77.34 ( 77.17)
Epoch: [12][350/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.0410e+00 (2.0463e+00)	Acc@1  42.97 ( 45.29)	Acc@5  77.34 ( 77.18)
Epoch: [12][360/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8887e+00 (2.0433e+00)	Acc@1  50.78 ( 45.36)	Acc@5  78.12 ( 77.23)
Epoch: [12][370/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.1348e+00 (2.0453e+00)	Acc@1  43.75 ( 45.34)	Acc@5  75.00 ( 77.18)
Epoch: [12][380/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.1387e+00 (2.0439e+00)	Acc@1  46.88 ( 45.35)	Acc@5  76.56 ( 77.20)
Epoch: [12][390/391]	Time  0.017 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.9639e+00 (2.0416e+00)	Acc@1  47.50 ( 45.38)	Acc@5  77.50 ( 77.25)
## e[12] optimizer.zero_grad (sum) time: 0.1048579216003418
## e[12]       loss.backward (sum) time: 2.281053066253662
## e[12]      optimizer.step (sum) time: 0.9276165962219238
## epoch[12] training(only) time: 10.086603164672852
# Switched to evaluate mode...
Test: [  0/100]	Time  0.155 ( 0.155)	Loss 2.0527e+00 (2.0527e+00)	Acc@1  49.00 ( 49.00)	Acc@5  78.00 ( 78.00)
Test: [ 10/100]	Time  0.021 ( 0.028)	Loss 1.9629e+00 (2.0906e+00)	Acc@1  41.00 ( 45.91)	Acc@5  81.00 ( 76.36)
Test: [ 20/100]	Time  0.016 ( 0.021)	Loss 1.8213e+00 (2.1169e+00)	Acc@1  51.00 ( 45.10)	Acc@5  77.00 ( 75.81)
Test: [ 30/100]	Time  0.016 ( 0.019)	Loss 1.8965e+00 (2.0797e+00)	Acc@1  51.00 ( 45.42)	Acc@5  76.00 ( 76.45)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 2.0762e+00 (2.0733e+00)	Acc@1  46.00 ( 45.39)	Acc@5  77.00 ( 76.71)
Test: [ 50/100]	Time  0.025 ( 0.017)	Loss 1.9775e+00 (2.0921e+00)	Acc@1  47.00 ( 45.08)	Acc@5  73.00 ( 76.27)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 2.0254e+00 (2.0746e+00)	Acc@1  48.00 ( 45.20)	Acc@5  77.00 ( 76.51)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 2.1055e+00 (2.0774e+00)	Acc@1  44.00 ( 44.97)	Acc@5  72.00 ( 76.41)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 2.2969e+00 (2.0923e+00)	Acc@1  40.00 ( 44.64)	Acc@5  74.00 ( 76.30)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 2.2461e+00 (2.0842e+00)	Acc@1  40.00 ( 44.76)	Acc@5  75.00 ( 76.44)
 * Acc@1 44.760 Acc@5 76.410
### epoch[12] execution time: 11.715422868728638
EPOCH 13
# current learning rate: 0.1
# Switched to train mode...
Epoch: [13][  0/391]	Time  0.171 ( 0.171)	Data  0.143 ( 0.143)	Loss 2.0547e+00 (2.0547e+00)	Acc@1  48.44 ( 48.44)	Acc@5  77.34 ( 77.34)
Epoch: [13][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.015)	Loss 1.8418e+00 (1.9314e+00)	Acc@1  47.66 ( 47.80)	Acc@5  82.03 ( 79.26)
Epoch: [13][ 20/391]	Time  0.038 ( 0.032)	Data  0.001 ( 0.008)	Loss 2.0488e+00 (1.9535e+00)	Acc@1  48.44 ( 46.76)	Acc@5  76.56 ( 78.98)
Epoch: [13][ 30/391]	Time  0.019 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.1523e+00 (1.9279e+00)	Acc@1  48.44 ( 47.81)	Acc@5  75.78 ( 79.03)
Epoch: [13][ 40/391]	Time  0.029 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.1621e+00 (1.9458e+00)	Acc@1  42.19 ( 47.48)	Acc@5  76.56 ( 78.83)
Epoch: [13][ 50/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.0137e+00 (1.9472e+00)	Acc@1  50.00 ( 47.59)	Acc@5  72.66 ( 78.71)
Epoch: [13][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0195e+00 (1.9469e+00)	Acc@1  43.75 ( 47.76)	Acc@5  81.25 ( 78.82)
Epoch: [13][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0508e+00 (1.9567e+00)	Acc@1  41.41 ( 47.47)	Acc@5  80.47 ( 78.82)
Epoch: [13][ 80/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0254e+00 (1.9650e+00)	Acc@1  49.22 ( 47.29)	Acc@5  77.34 ( 78.73)
Epoch: [13][ 90/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7441e+00 (1.9623e+00)	Acc@1  50.00 ( 47.39)	Acc@5  78.91 ( 78.72)
Epoch: [13][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6006e+00 (1.9547e+00)	Acc@1  59.38 ( 47.44)	Acc@5  84.38 ( 78.90)
Epoch: [13][110/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.1719e+00 (1.9548e+00)	Acc@1  46.09 ( 47.33)	Acc@5  75.78 ( 79.01)
Epoch: [13][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1641e+00 (1.9600e+00)	Acc@1  42.19 ( 47.17)	Acc@5  78.12 ( 78.93)
Epoch: [13][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1133e+00 (1.9649e+00)	Acc@1  43.75 ( 46.99)	Acc@5  75.78 ( 78.78)
Epoch: [13][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8594e+00 (1.9706e+00)	Acc@1  50.00 ( 46.79)	Acc@5  75.78 ( 78.60)
Epoch: [13][150/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0000e+00 (1.9659e+00)	Acc@1  48.44 ( 46.91)	Acc@5  75.00 ( 78.62)
Epoch: [13][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7480e+00 (1.9659e+00)	Acc@1  51.56 ( 46.91)	Acc@5  82.81 ( 78.65)
Epoch: [13][170/391]	Time  0.020 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.1465e+00 (1.9708e+00)	Acc@1  42.97 ( 46.83)	Acc@5  75.78 ( 78.60)
Epoch: [13][180/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8379e+00 (1.9699e+00)	Acc@1  49.22 ( 46.78)	Acc@5  85.16 ( 78.68)
Epoch: [13][190/391]	Time  0.035 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.8359e+00 (1.9709e+00)	Acc@1  46.88 ( 46.77)	Acc@5  79.69 ( 78.66)
Epoch: [13][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9980e+00 (1.9730e+00)	Acc@1  45.31 ( 46.82)	Acc@5  78.12 ( 78.65)
Epoch: [13][210/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0469e+00 (1.9792e+00)	Acc@1  50.78 ( 46.79)	Acc@5  75.00 ( 78.49)
Epoch: [13][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9707e+00 (1.9792e+00)	Acc@1  46.88 ( 46.81)	Acc@5  78.91 ( 78.52)
Epoch: [13][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1719e+00 (1.9793e+00)	Acc@1  42.97 ( 46.75)	Acc@5  77.34 ( 78.56)
Epoch: [13][240/391]	Time  0.035 ( 0.026)	Data  0.006 ( 0.002)	Loss 2.2148e+00 (1.9802e+00)	Acc@1  40.62 ( 46.74)	Acc@5  77.34 ( 78.59)
Epoch: [13][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1699e+00 (1.9834e+00)	Acc@1  42.97 ( 46.64)	Acc@5  78.12 ( 78.51)
Epoch: [13][260/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.8867e+00 (1.9802e+00)	Acc@1  48.44 ( 46.73)	Acc@5  79.69 ( 78.58)
Epoch: [13][270/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.7764e+00 (1.9761e+00)	Acc@1  50.00 ( 46.82)	Acc@5  81.25 ( 78.62)
Epoch: [13][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9131e+00 (1.9741e+00)	Acc@1  50.00 ( 46.89)	Acc@5  79.69 ( 78.68)
Epoch: [13][290/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.2539e+00 (1.9763e+00)	Acc@1  43.75 ( 46.88)	Acc@5  74.22 ( 78.65)
Epoch: [13][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1250e+00 (1.9753e+00)	Acc@1  44.53 ( 46.93)	Acc@5  75.78 ( 78.70)
Epoch: [13][310/391]	Time  0.022 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.6924e+00 (1.9741e+00)	Acc@1  53.91 ( 46.98)	Acc@5  84.38 ( 78.70)
Epoch: [13][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7578e+00 (1.9749e+00)	Acc@1  58.59 ( 46.95)	Acc@5  81.25 ( 78.70)
Epoch: [13][330/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.8828e+00 (1.9743e+00)	Acc@1  53.91 ( 46.96)	Acc@5  80.47 ( 78.69)
Epoch: [13][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8457e+00 (1.9741e+00)	Acc@1  49.22 ( 46.98)	Acc@5  80.47 ( 78.69)
Epoch: [13][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0391e+00 (1.9719e+00)	Acc@1  46.09 ( 47.02)	Acc@5  78.91 ( 78.71)
Epoch: [13][360/391]	Time  0.039 ( 0.026)	Data  0.006 ( 0.002)	Loss 1.8477e+00 (1.9711e+00)	Acc@1  50.00 ( 47.08)	Acc@5  80.47 ( 78.70)
Epoch: [13][370/391]	Time  0.030 ( 0.026)	Data  0.009 ( 0.002)	Loss 2.1328e+00 (1.9694e+00)	Acc@1  44.53 ( 47.15)	Acc@5  71.09 ( 78.73)
Epoch: [13][380/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.2070e+00 (1.9694e+00)	Acc@1  42.97 ( 47.15)	Acc@5  74.22 ( 78.70)
Epoch: [13][390/391]	Time  0.020 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.1348e+00 (1.9689e+00)	Acc@1  45.00 ( 47.14)	Acc@5  80.00 ( 78.72)
## e[13] optimizer.zero_grad (sum) time: 0.10474896430969238
## e[13]       loss.backward (sum) time: 2.2910943031311035
## e[13]      optimizer.step (sum) time: 0.9344680309295654
## epoch[13] training(only) time: 10.078765392303467
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.9277e+00 (1.9277e+00)	Acc@1  50.00 ( 50.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 2.1230e+00 (2.0521e+00)	Acc@1  42.00 ( 47.27)	Acc@5  75.00 ( 75.45)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 2.0137e+00 (2.0678e+00)	Acc@1  48.00 ( 46.33)	Acc@5  74.00 ( 75.38)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.1836e+00 (2.0692e+00)	Acc@1  48.00 ( 46.16)	Acc@5  76.00 ( 75.81)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.9521e+00 (2.0712e+00)	Acc@1  45.00 ( 45.76)	Acc@5  80.00 ( 76.12)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 2.1289e+00 (2.0965e+00)	Acc@1  38.00 ( 45.00)	Acc@5  73.00 ( 75.43)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 2.1348e+00 (2.0887e+00)	Acc@1  40.00 ( 44.97)	Acc@5  78.00 ( 75.41)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 2.2695e+00 (2.0967e+00)	Acc@1  37.00 ( 44.87)	Acc@5  74.00 ( 75.30)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 2.3223e+00 (2.1089e+00)	Acc@1  45.00 ( 44.78)	Acc@5  71.00 ( 75.19)
Test: [ 90/100]	Time  0.017 ( 0.015)	Loss 2.1699e+00 (2.0952e+00)	Acc@1  46.00 ( 45.24)	Acc@5  71.00 ( 75.35)
 * Acc@1 45.160 Acc@5 75.380
### epoch[13] execution time: 11.684023141860962
EPOCH 14
# current learning rate: 0.1
# Switched to train mode...
Epoch: [14][  0/391]	Time  0.173 ( 0.173)	Data  0.146 ( 0.146)	Loss 2.0879e+00 (2.0879e+00)	Acc@1  42.19 ( 42.19)	Acc@5  76.56 ( 76.56)
Epoch: [14][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.9072e+00 (1.8933e+00)	Acc@1  53.91 ( 49.57)	Acc@5  78.91 ( 79.12)
Epoch: [14][ 20/391]	Time  0.034 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.8535e+00 (1.8930e+00)	Acc@1  50.78 ( 48.85)	Acc@5  76.56 ( 78.98)
Epoch: [14][ 30/391]	Time  0.022 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.9785e+00 (1.9103e+00)	Acc@1  47.66 ( 48.66)	Acc@5  84.38 ( 79.41)
Epoch: [14][ 40/391]	Time  0.021 ( 0.028)	Data  0.002 ( 0.005)	Loss 2.0684e+00 (1.9042e+00)	Acc@1  42.97 ( 48.80)	Acc@5  74.22 ( 79.55)
Epoch: [14][ 50/391]	Time  0.024 ( 0.028)	Data  0.002 ( 0.005)	Loss 2.0762e+00 (1.8951e+00)	Acc@1  46.09 ( 49.08)	Acc@5  76.56 ( 79.84)
Epoch: [14][ 60/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6768e+00 (1.8966e+00)	Acc@1  56.25 ( 49.15)	Acc@5  86.72 ( 79.92)
Epoch: [14][ 70/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9072e+00 (1.8881e+00)	Acc@1  46.88 ( 49.25)	Acc@5  77.34 ( 80.00)
Epoch: [14][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1172e+00 (1.8816e+00)	Acc@1  46.88 ( 49.30)	Acc@5  73.44 ( 80.19)
Epoch: [14][ 90/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6230e+00 (1.8797e+00)	Acc@1  54.69 ( 49.39)	Acc@5  84.38 ( 80.20)
Epoch: [14][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9121e+00 (1.8867e+00)	Acc@1  45.31 ( 49.22)	Acc@5  82.81 ( 80.03)
Epoch: [14][110/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7158e+00 (1.8897e+00)	Acc@1  56.25 ( 49.18)	Acc@5  82.81 ( 79.98)
Epoch: [14][120/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.8750e+00 (1.8941e+00)	Acc@1  47.66 ( 48.98)	Acc@5  78.91 ( 79.96)
Epoch: [14][130/391]	Time  0.038 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.0430e+00 (1.8940e+00)	Acc@1  46.88 ( 48.99)	Acc@5  77.34 ( 79.94)
Epoch: [14][140/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8105e+00 (1.8934e+00)	Acc@1  50.78 ( 48.91)	Acc@5  82.81 ( 79.93)
Epoch: [14][150/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8262e+00 (1.8962e+00)	Acc@1  52.34 ( 48.83)	Acc@5  82.03 ( 79.92)
Epoch: [14][160/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9746e+00 (1.9025e+00)	Acc@1  49.22 ( 48.73)	Acc@5  80.47 ( 79.89)
Epoch: [14][170/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.9111e+00 (1.8990e+00)	Acc@1  44.53 ( 48.86)	Acc@5  78.91 ( 79.96)
Epoch: [14][180/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4736e+00 (1.8965e+00)	Acc@1  60.16 ( 48.92)	Acc@5  89.06 ( 80.06)
Epoch: [14][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.8613e+00 (1.8958e+00)	Acc@1  51.56 ( 48.94)	Acc@5  75.78 ( 80.03)
Epoch: [14][200/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.8496e+00 (1.8966e+00)	Acc@1  48.44 ( 48.95)	Acc@5  80.47 ( 80.03)
Epoch: [14][210/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9707e+00 (1.8933e+00)	Acc@1  47.66 ( 49.06)	Acc@5  77.34 ( 80.07)
Epoch: [14][220/391]	Time  0.042 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.9121e+00 (1.8957e+00)	Acc@1  51.56 ( 48.96)	Acc@5  77.34 ( 80.04)
Epoch: [14][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7119e+00 (1.8995e+00)	Acc@1  55.47 ( 48.84)	Acc@5  82.03 ( 79.94)
Epoch: [14][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1523e+00 (1.8993e+00)	Acc@1  47.66 ( 48.90)	Acc@5  75.78 ( 79.93)
Epoch: [14][250/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9580e+00 (1.8995e+00)	Acc@1  47.66 ( 48.87)	Acc@5  77.34 ( 79.91)
Epoch: [14][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9727e+00 (1.9030e+00)	Acc@1  47.66 ( 48.77)	Acc@5  78.12 ( 79.84)
Epoch: [14][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3379e+00 (1.9031e+00)	Acc@1  42.19 ( 48.80)	Acc@5  75.00 ( 79.88)
Epoch: [14][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9521e+00 (1.9035e+00)	Acc@1  51.56 ( 48.80)	Acc@5  78.12 ( 79.85)
Epoch: [14][290/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8975e+00 (1.9010e+00)	Acc@1  48.44 ( 48.83)	Acc@5  78.12 ( 79.81)
Epoch: [14][300/391]	Time  0.037 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.8965e+00 (1.9006e+00)	Acc@1  43.75 ( 48.84)	Acc@5  81.25 ( 79.83)
Epoch: [14][310/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.8877e+00 (1.9025e+00)	Acc@1  47.66 ( 48.82)	Acc@5  82.81 ( 79.77)
Epoch: [14][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8701e+00 (1.9032e+00)	Acc@1  51.56 ( 48.79)	Acc@5  78.12 ( 79.74)
Epoch: [14][330/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9043e+00 (1.9013e+00)	Acc@1  53.91 ( 48.84)	Acc@5  77.34 ( 79.77)
Epoch: [14][340/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7012e+00 (1.9027e+00)	Acc@1  53.91 ( 48.80)	Acc@5  86.72 ( 79.75)
Epoch: [14][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8271e+00 (1.9045e+00)	Acc@1  51.56 ( 48.76)	Acc@5  81.25 ( 79.70)
Epoch: [14][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0430e+00 (1.9053e+00)	Acc@1  49.22 ( 48.79)	Acc@5  75.00 ( 79.71)
Epoch: [14][370/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8818e+00 (1.9051e+00)	Acc@1  49.22 ( 48.81)	Acc@5  78.12 ( 79.72)
Epoch: [14][380/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.8584e+00 (1.9050e+00)	Acc@1  48.44 ( 48.83)	Acc@5  78.91 ( 79.74)
Epoch: [14][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1094e+00 (1.9052e+00)	Acc@1  47.50 ( 48.77)	Acc@5  80.00 ( 79.75)
## e[14] optimizer.zero_grad (sum) time: 0.10411572456359863
## e[14]       loss.backward (sum) time: 2.2666523456573486
## e[14]      optimizer.step (sum) time: 0.9183523654937744
## epoch[14] training(only) time: 10.088700294494629
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.7324e+00 (1.7324e+00)	Acc@1  46.00 ( 46.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 2.2344e+00 (2.0195e+00)	Acc@1  44.00 ( 47.73)	Acc@5  73.00 ( 77.64)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 1.9199e+00 (2.0381e+00)	Acc@1  45.00 ( 47.14)	Acc@5  78.00 ( 77.52)
Test: [ 30/100]	Time  0.023 ( 0.018)	Loss 1.8408e+00 (2.0213e+00)	Acc@1  51.00 ( 47.16)	Acc@5  81.00 ( 77.77)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 2.0723e+00 (2.0267e+00)	Acc@1  45.00 ( 46.66)	Acc@5  78.00 ( 77.80)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 2.0508e+00 (2.0380e+00)	Acc@1  47.00 ( 46.71)	Acc@5  70.00 ( 77.16)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 2.1543e+00 (2.0322e+00)	Acc@1  47.00 ( 46.41)	Acc@5  78.00 ( 77.20)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 2.2480e+00 (2.0433e+00)	Acc@1  44.00 ( 46.28)	Acc@5  76.00 ( 77.08)
Test: [ 80/100]	Time  0.009 ( 0.015)	Loss 2.0840e+00 (2.0568e+00)	Acc@1  49.00 ( 46.19)	Acc@5  71.00 ( 76.83)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.8818e+00 (2.0466e+00)	Acc@1  47.00 ( 46.43)	Acc@5  78.00 ( 77.02)
 * Acc@1 46.520 Acc@5 77.120
### epoch[14] execution time: 11.704929828643799
EPOCH 15
# current learning rate: 0.1
# Switched to train mode...
Epoch: [15][  0/391]	Time  0.182 ( 0.182)	Data  0.155 ( 0.155)	Loss 1.8818e+00 (1.8818e+00)	Acc@1  46.88 ( 46.88)	Acc@5  82.81 ( 82.81)
Epoch: [15][ 10/391]	Time  0.022 ( 0.038)	Data  0.001 ( 0.015)	Loss 1.9727e+00 (1.8088e+00)	Acc@1  52.34 ( 49.86)	Acc@5  80.47 ( 81.53)
Epoch: [15][ 20/391]	Time  0.041 ( 0.032)	Data  0.008 ( 0.009)	Loss 1.7334e+00 (1.7931e+00)	Acc@1  49.22 ( 50.82)	Acc@5  85.16 ( 81.92)
Epoch: [15][ 30/391]	Time  0.034 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.9131e+00 (1.8129e+00)	Acc@1  44.53 ( 50.30)	Acc@5  79.69 ( 81.85)
Epoch: [15][ 40/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.8262e+00 (1.8032e+00)	Acc@1  53.91 ( 50.53)	Acc@5  80.47 ( 82.01)
Epoch: [15][ 50/391]	Time  0.020 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.8252e+00 (1.8022e+00)	Acc@1  50.00 ( 50.84)	Acc@5  82.03 ( 82.00)
Epoch: [15][ 60/391]	Time  0.029 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.5850e+00 (1.8019e+00)	Acc@1  56.25 ( 50.68)	Acc@5  83.59 ( 81.86)
Epoch: [15][ 70/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.5596e+00 (1.8012e+00)	Acc@1  58.59 ( 50.85)	Acc@5  85.94 ( 81.79)
Epoch: [15][ 80/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5078e+00 (1.7988e+00)	Acc@1  57.03 ( 50.89)	Acc@5  84.38 ( 81.72)
Epoch: [15][ 90/391]	Time  0.033 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.7529e+00 (1.8030e+00)	Acc@1  53.91 ( 50.90)	Acc@5  81.25 ( 81.73)
Epoch: [15][100/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7021e+00 (1.8028e+00)	Acc@1  55.47 ( 50.80)	Acc@5  83.59 ( 81.74)
Epoch: [15][110/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9854e+00 (1.8143e+00)	Acc@1  46.88 ( 50.53)	Acc@5  78.91 ( 81.63)
Epoch: [15][120/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7363e+00 (1.8182e+00)	Acc@1  50.78 ( 50.50)	Acc@5  79.69 ( 81.48)
Epoch: [15][130/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0234e+00 (1.8172e+00)	Acc@1  46.09 ( 50.44)	Acc@5  77.34 ( 81.58)
Epoch: [15][140/391]	Time  0.037 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.7236e+00 (1.8126e+00)	Acc@1  53.91 ( 50.53)	Acc@5  82.81 ( 81.67)
Epoch: [15][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6797e+00 (1.8143e+00)	Acc@1  53.91 ( 50.59)	Acc@5  82.81 ( 81.57)
Epoch: [15][160/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9033e+00 (1.8191e+00)	Acc@1  52.34 ( 50.52)	Acc@5  78.91 ( 81.42)
Epoch: [15][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6309e+00 (1.8161e+00)	Acc@1  60.16 ( 50.58)	Acc@5  84.38 ( 81.46)
Epoch: [15][180/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6729e+00 (1.8170e+00)	Acc@1  57.81 ( 50.57)	Acc@5  80.47 ( 81.41)
Epoch: [15][190/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9492e+00 (1.8142e+00)	Acc@1  50.78 ( 50.58)	Acc@5  78.91 ( 81.46)
Epoch: [15][200/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8623e+00 (1.8146e+00)	Acc@1  47.66 ( 50.56)	Acc@5  80.47 ( 81.45)
Epoch: [15][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8369e+00 (1.8172e+00)	Acc@1  51.56 ( 50.56)	Acc@5  79.69 ( 81.37)
Epoch: [15][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0801e+00 (1.8196e+00)	Acc@1  38.28 ( 50.47)	Acc@5  78.91 ( 81.28)
Epoch: [15][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5430e+00 (1.8212e+00)	Acc@1  59.38 ( 50.41)	Acc@5  84.38 ( 81.26)
Epoch: [15][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7412e+00 (1.8210e+00)	Acc@1  44.53 ( 50.39)	Acc@5  82.81 ( 81.30)
Epoch: [15][250/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.6270e+00 (1.8192e+00)	Acc@1  55.47 ( 50.42)	Acc@5  83.59 ( 81.35)
Epoch: [15][260/391]	Time  0.038 ( 0.026)	Data  0.006 ( 0.003)	Loss 1.6709e+00 (1.8190e+00)	Acc@1  53.91 ( 50.42)	Acc@5  85.94 ( 81.37)
Epoch: [15][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8369e+00 (1.8212e+00)	Acc@1  52.34 ( 50.35)	Acc@5  79.69 ( 81.32)
Epoch: [15][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0176e+00 (1.8237e+00)	Acc@1  43.75 ( 50.30)	Acc@5  83.59 ( 81.30)
Epoch: [15][290/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8740e+00 (1.8241e+00)	Acc@1  46.88 ( 50.35)	Acc@5  79.69 ( 81.26)
Epoch: [15][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9414e+00 (1.8274e+00)	Acc@1  50.00 ( 50.28)	Acc@5  74.22 ( 81.19)
Epoch: [15][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0488e+00 (1.8301e+00)	Acc@1  47.66 ( 50.23)	Acc@5  76.56 ( 81.16)
Epoch: [15][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8838e+00 (1.8309e+00)	Acc@1  51.56 ( 50.18)	Acc@5  78.12 ( 81.14)
Epoch: [15][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7695e+00 (1.8289e+00)	Acc@1  53.91 ( 50.24)	Acc@5  82.03 ( 81.13)
Epoch: [15][340/391]	Time  0.041 ( 0.026)	Data  0.009 ( 0.002)	Loss 1.6484e+00 (1.8284e+00)	Acc@1  52.34 ( 50.24)	Acc@5  85.16 ( 81.18)
Epoch: [15][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7197e+00 (1.8275e+00)	Acc@1  52.34 ( 50.27)	Acc@5  82.81 ( 81.19)
Epoch: [15][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9980e+00 (1.8289e+00)	Acc@1  50.00 ( 50.25)	Acc@5  74.22 ( 81.17)
Epoch: [15][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7402e+00 (1.8294e+00)	Acc@1  53.91 ( 50.24)	Acc@5  80.47 ( 81.16)
Epoch: [15][380/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7197e+00 (1.8314e+00)	Acc@1  54.69 ( 50.22)	Acc@5  85.94 ( 81.12)
Epoch: [15][390/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.6553e+00 (1.8308e+00)	Acc@1  52.50 ( 50.22)	Acc@5  80.00 ( 81.13)
## e[15] optimizer.zero_grad (sum) time: 0.10542654991149902
## e[15]       loss.backward (sum) time: 2.273740291595459
## e[15]      optimizer.step (sum) time: 0.930769681930542
## epoch[15] training(only) time: 10.072798252105713
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 1.8350e+00 (1.8350e+00)	Acc@1  50.00 ( 50.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.014 ( 0.025)	Loss 2.4375e+00 (2.0457e+00)	Acc@1  33.00 ( 47.00)	Acc@5  76.00 ( 77.09)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.9238e+00 (2.0168e+00)	Acc@1  50.00 ( 46.10)	Acc@5  76.00 ( 77.33)
Test: [ 30/100]	Time  0.014 ( 0.018)	Loss 1.8359e+00 (1.9930e+00)	Acc@1  49.00 ( 46.61)	Acc@5  79.00 ( 77.29)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.8545e+00 (1.9775e+00)	Acc@1  51.00 ( 46.90)	Acc@5  81.00 ( 77.76)
Test: [ 50/100]	Time  0.021 ( 0.016)	Loss 1.8496e+00 (1.9864e+00)	Acc@1  53.00 ( 46.80)	Acc@5  77.00 ( 77.55)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.9268e+00 (1.9796e+00)	Acc@1  48.00 ( 46.95)	Acc@5  79.00 ( 77.84)
Test: [ 70/100]	Time  0.009 ( 0.015)	Loss 2.2168e+00 (1.9831e+00)	Acc@1  45.00 ( 47.06)	Acc@5  76.00 ( 77.82)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.8828e+00 (1.9825e+00)	Acc@1  50.00 ( 47.12)	Acc@5  76.00 ( 77.94)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.1660e+00 (1.9696e+00)	Acc@1  44.00 ( 47.49)	Acc@5  77.00 ( 78.18)
 * Acc@1 47.630 Acc@5 78.220
### epoch[15] execution time: 11.639490604400635
EPOCH 16
# current learning rate: 0.1
# Switched to train mode...
Epoch: [16][  0/391]	Time  0.177 ( 0.177)	Data  0.149 ( 0.149)	Loss 1.8623e+00 (1.8623e+00)	Acc@1  52.34 ( 52.34)	Acc@5  85.16 ( 85.16)
Epoch: [16][ 10/391]	Time  0.033 ( 0.038)	Data  0.005 ( 0.015)	Loss 1.7998e+00 (1.7290e+00)	Acc@1  53.12 ( 51.85)	Acc@5  79.69 ( 84.16)
Epoch: [16][ 20/391]	Time  0.038 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.6367e+00 (1.7576e+00)	Acc@1  54.69 ( 51.00)	Acc@5  82.03 ( 83.59)
Epoch: [16][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.5605e+00 (1.7413e+00)	Acc@1  57.81 ( 51.92)	Acc@5  85.94 ( 83.62)
Epoch: [16][ 40/391]	Time  0.021 ( 0.029)	Data  0.000 ( 0.006)	Loss 1.8086e+00 (1.7365e+00)	Acc@1  52.34 ( 52.19)	Acc@5  85.16 ( 83.73)
Epoch: [16][ 50/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.5645e+00 (1.7423e+00)	Acc@1  60.16 ( 52.22)	Acc@5  88.28 ( 83.59)
Epoch: [16][ 60/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.8555e+00 (1.7447e+00)	Acc@1  51.56 ( 52.36)	Acc@5  78.91 ( 83.49)
Epoch: [16][ 70/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8584e+00 (1.7487e+00)	Acc@1  52.34 ( 52.38)	Acc@5  83.59 ( 83.41)
Epoch: [16][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8555e+00 (1.7422e+00)	Acc@1  52.34 ( 52.33)	Acc@5  79.69 ( 83.30)
Epoch: [16][ 90/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.6934e+00 (1.7389e+00)	Acc@1  52.34 ( 52.61)	Acc@5  82.81 ( 83.30)
Epoch: [16][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7568e+00 (1.7459e+00)	Acc@1  53.12 ( 52.29)	Acc@5  81.25 ( 83.14)
Epoch: [16][110/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0332e+00 (1.7506e+00)	Acc@1  49.22 ( 52.17)	Acc@5  74.22 ( 82.94)
Epoch: [16][120/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.8652e+00 (1.7497e+00)	Acc@1  52.34 ( 52.09)	Acc@5  82.03 ( 82.97)
Epoch: [16][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7568e+00 (1.7478e+00)	Acc@1  55.47 ( 52.14)	Acc@5  81.25 ( 82.96)
Epoch: [16][140/391]	Time  0.039 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.7051e+00 (1.7485e+00)	Acc@1  49.22 ( 52.18)	Acc@5  85.94 ( 82.89)
Epoch: [16][150/391]	Time  0.035 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.8223e+00 (1.7528e+00)	Acc@1  43.75 ( 52.14)	Acc@5  82.03 ( 82.85)
Epoch: [16][160/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7227e+00 (1.7598e+00)	Acc@1  50.78 ( 52.00)	Acc@5  84.38 ( 82.74)
Epoch: [16][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9648e+00 (1.7598e+00)	Acc@1  44.53 ( 52.06)	Acc@5  83.59 ( 82.70)
Epoch: [16][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7412e+00 (1.7673e+00)	Acc@1  56.25 ( 51.88)	Acc@5  82.03 ( 82.52)
Epoch: [16][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6270e+00 (1.7690e+00)	Acc@1  57.03 ( 51.84)	Acc@5  85.16 ( 82.43)
Epoch: [16][200/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7148e+00 (1.7682e+00)	Acc@1  50.00 ( 51.89)	Acc@5  78.12 ( 82.42)
Epoch: [16][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5889e+00 (1.7698e+00)	Acc@1  54.69 ( 51.83)	Acc@5  82.03 ( 82.41)
Epoch: [16][220/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6953e+00 (1.7680e+00)	Acc@1  54.69 ( 51.89)	Acc@5  81.25 ( 82.40)
Epoch: [16][230/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9336e+00 (1.7685e+00)	Acc@1  52.34 ( 51.89)	Acc@5  78.91 ( 82.34)
Epoch: [16][240/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7568e+00 (1.7710e+00)	Acc@1  53.91 ( 51.82)	Acc@5  78.12 ( 82.20)
Epoch: [16][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7979e+00 (1.7731e+00)	Acc@1  46.88 ( 51.74)	Acc@5  77.34 ( 82.17)
Epoch: [16][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5420e+00 (1.7738e+00)	Acc@1  55.47 ( 51.69)	Acc@5  85.94 ( 82.14)
Epoch: [16][270/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.4229e+00 (1.7701e+00)	Acc@1  58.59 ( 51.79)	Acc@5  85.94 ( 82.24)
Epoch: [16][280/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.6699e+00 (1.7691e+00)	Acc@1  53.91 ( 51.78)	Acc@5  81.25 ( 82.27)
Epoch: [16][290/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5928e+00 (1.7690e+00)	Acc@1  58.59 ( 51.83)	Acc@5  86.72 ( 82.26)
Epoch: [16][300/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6660e+00 (1.7692e+00)	Acc@1  53.12 ( 51.79)	Acc@5  82.81 ( 82.29)
Epoch: [16][310/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.6094e+00 (1.7717e+00)	Acc@1  57.03 ( 51.70)	Acc@5  85.94 ( 82.23)
Epoch: [16][320/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6641e+00 (1.7728e+00)	Acc@1  54.69 ( 51.69)	Acc@5  82.03 ( 82.18)
Epoch: [16][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6064e+00 (1.7752e+00)	Acc@1  52.34 ( 51.65)	Acc@5  86.72 ( 82.13)
Epoch: [16][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9141e+00 (1.7775e+00)	Acc@1  51.56 ( 51.62)	Acc@5  78.91 ( 82.10)
Epoch: [16][350/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6182e+00 (1.7771e+00)	Acc@1  54.69 ( 51.64)	Acc@5  88.28 ( 82.09)
Epoch: [16][360/391]	Time  0.022 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.6025e+00 (1.7748e+00)	Acc@1  53.12 ( 51.69)	Acc@5  83.59 ( 82.12)
Epoch: [16][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9629e+00 (1.7783e+00)	Acc@1  45.31 ( 51.63)	Acc@5  78.12 ( 82.08)
Epoch: [16][380/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8213e+00 (1.7801e+00)	Acc@1  47.66 ( 51.60)	Acc@5  84.38 ( 82.04)
Epoch: [16][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6865e+00 (1.7789e+00)	Acc@1  52.50 ( 51.62)	Acc@5  83.75 ( 82.07)
## e[16] optimizer.zero_grad (sum) time: 0.10438346862792969
## e[16]       loss.backward (sum) time: 2.2952821254730225
## e[16]      optimizer.step (sum) time: 0.9304890632629395
## epoch[16] training(only) time: 10.09569263458252
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 2.0449e+00 (2.0449e+00)	Acc@1  48.00 ( 48.00)	Acc@5  79.00 ( 79.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 2.1465e+00 (2.2288e+00)	Acc@1  44.00 ( 46.45)	Acc@5  77.00 ( 73.64)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 2.1543e+00 (2.2187e+00)	Acc@1  44.00 ( 45.57)	Acc@5  75.00 ( 74.57)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.8252e+00 (2.2065e+00)	Acc@1  50.00 ( 45.61)	Acc@5  84.00 ( 74.52)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.9141e+00 (2.1754e+00)	Acc@1  53.00 ( 45.54)	Acc@5  81.00 ( 74.95)
Test: [ 50/100]	Time  0.022 ( 0.016)	Loss 2.1270e+00 (2.1662e+00)	Acc@1  51.00 ( 45.69)	Acc@5  70.00 ( 74.73)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 2.0234e+00 (2.1484e+00)	Acc@1  51.00 ( 45.92)	Acc@5  76.00 ( 75.15)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 2.1914e+00 (2.1471e+00)	Acc@1  46.00 ( 45.99)	Acc@5  77.00 ( 75.39)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 2.2363e+00 (2.1671e+00)	Acc@1  45.00 ( 45.59)	Acc@5  75.00 ( 75.11)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 2.3574e+00 (2.1655e+00)	Acc@1  45.00 ( 45.69)	Acc@5  75.00 ( 75.24)
 * Acc@1 45.730 Acc@5 75.280
### epoch[16] execution time: 11.676182985305786
EPOCH 17
# current learning rate: 0.1
# Switched to train mode...
Epoch: [17][  0/391]	Time  0.176 ( 0.176)	Data  0.148 ( 0.148)	Loss 1.9863e+00 (1.9863e+00)	Acc@1  47.66 ( 47.66)	Acc@5  78.12 ( 78.12)
Epoch: [17][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.7578e+00 (1.7624e+00)	Acc@1  57.03 ( 52.20)	Acc@5  83.59 ( 82.03)
Epoch: [17][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.9482e+00 (1.7606e+00)	Acc@1  45.31 ( 52.12)	Acc@5  81.25 ( 82.81)
Epoch: [17][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.5713e+00 (1.7037e+00)	Acc@1  55.47 ( 53.30)	Acc@5  86.72 ( 83.59)
Epoch: [17][ 40/391]	Time  0.027 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.7988e+00 (1.7155e+00)	Acc@1  48.44 ( 52.74)	Acc@5  78.12 ( 83.23)
Epoch: [17][ 50/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.6328e+00 (1.7145e+00)	Acc@1  57.03 ( 53.22)	Acc@5  82.81 ( 83.10)
Epoch: [17][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5430e+00 (1.7182e+00)	Acc@1  58.59 ( 53.15)	Acc@5  84.38 ( 83.00)
Epoch: [17][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4502e+00 (1.7075e+00)	Acc@1  60.94 ( 53.59)	Acc@5  88.28 ( 83.14)
Epoch: [17][ 80/391]	Time  0.029 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.9512e+00 (1.7128e+00)	Acc@1  51.56 ( 53.40)	Acc@5  78.91 ( 82.94)
Epoch: [17][ 90/391]	Time  0.036 ( 0.026)	Data  0.005 ( 0.004)	Loss 1.6963e+00 (1.7143e+00)	Acc@1  50.78 ( 53.37)	Acc@5  81.25 ( 82.95)
Epoch: [17][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6895e+00 (1.7140e+00)	Acc@1  52.34 ( 53.36)	Acc@5  82.03 ( 82.95)
Epoch: [17][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9805e+00 (1.7089e+00)	Acc@1  49.22 ( 53.53)	Acc@5  78.12 ( 83.02)
Epoch: [17][120/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5791e+00 (1.7017e+00)	Acc@1  57.03 ( 53.53)	Acc@5  80.47 ( 83.08)
Epoch: [17][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7031e+00 (1.7064e+00)	Acc@1  56.25 ( 53.49)	Acc@5  83.59 ( 82.94)
Epoch: [17][140/391]	Time  0.026 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.8682e+00 (1.7085e+00)	Acc@1  50.78 ( 53.44)	Acc@5  81.25 ( 82.86)
Epoch: [17][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5654e+00 (1.7020e+00)	Acc@1  54.69 ( 53.47)	Acc@5  85.94 ( 82.98)
Epoch: [17][160/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9326e+00 (1.7085e+00)	Acc@1  46.09 ( 53.39)	Acc@5  82.03 ( 82.82)
Epoch: [17][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7539e+00 (1.7057e+00)	Acc@1  46.09 ( 53.46)	Acc@5  81.25 ( 82.86)
Epoch: [17][180/391]	Time  0.037 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6650e+00 (1.7032e+00)	Acc@1  53.91 ( 53.40)	Acc@5  78.91 ( 82.92)
Epoch: [17][190/391]	Time  0.030 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6934e+00 (1.7072e+00)	Acc@1  53.91 ( 53.27)	Acc@5  83.59 ( 82.87)
Epoch: [17][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7031e+00 (1.7085e+00)	Acc@1  50.78 ( 53.21)	Acc@5  85.16 ( 82.86)
Epoch: [17][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8633e+00 (1.7124e+00)	Acc@1  53.91 ( 53.19)	Acc@5  79.69 ( 82.81)
Epoch: [17][220/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.8965e+00 (1.7128e+00)	Acc@1  53.91 ( 53.22)	Acc@5  82.03 ( 82.83)
Epoch: [17][230/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7178e+00 (1.7149e+00)	Acc@1  48.44 ( 53.11)	Acc@5  82.03 ( 82.84)
Epoch: [17][240/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5713e+00 (1.7166e+00)	Acc@1  57.03 ( 53.06)	Acc@5  82.81 ( 82.81)
Epoch: [17][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9668e+00 (1.7130e+00)	Acc@1  46.88 ( 53.10)	Acc@5  82.03 ( 82.88)
Epoch: [17][260/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9404e+00 (1.7141e+00)	Acc@1  50.00 ( 53.13)	Acc@5  80.47 ( 82.88)
Epoch: [17][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6162e+00 (1.7148e+00)	Acc@1  55.47 ( 53.12)	Acc@5  85.16 ( 82.90)
Epoch: [17][280/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6299e+00 (1.7193e+00)	Acc@1  56.25 ( 53.03)	Acc@5  82.81 ( 82.86)
Epoch: [17][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4502e+00 (1.7189e+00)	Acc@1  59.38 ( 53.04)	Acc@5  83.59 ( 82.84)
Epoch: [17][300/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9912e+00 (1.7194e+00)	Acc@1  44.53 ( 53.00)	Acc@5  80.47 ( 82.85)
Epoch: [17][310/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6885e+00 (1.7200e+00)	Acc@1  50.00 ( 52.96)	Acc@5  84.38 ( 82.84)
Epoch: [17][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9492e+00 (1.7251e+00)	Acc@1  46.09 ( 52.85)	Acc@5  75.78 ( 82.75)
Epoch: [17][330/391]	Time  0.032 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.7373e+00 (1.7251e+00)	Acc@1  54.69 ( 52.87)	Acc@5  78.91 ( 82.75)
Epoch: [17][340/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9844e+00 (1.7244e+00)	Acc@1  48.44 ( 52.88)	Acc@5  82.81 ( 82.78)
Epoch: [17][350/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.6982e+00 (1.7252e+00)	Acc@1  53.12 ( 52.84)	Acc@5  81.25 ( 82.77)
Epoch: [17][360/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.0273e+00 (1.7253e+00)	Acc@1  44.53 ( 52.78)	Acc@5  78.91 ( 82.78)
Epoch: [17][370/391]	Time  0.021 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.6396e+00 (1.7270e+00)	Acc@1  55.47 ( 52.71)	Acc@5  82.81 ( 82.81)
Epoch: [17][380/391]	Time  0.020 ( 0.025)	Data  0.000 ( 0.002)	Loss 1.9482e+00 (1.7272e+00)	Acc@1  48.44 ( 52.75)	Acc@5  73.44 ( 82.78)
Epoch: [17][390/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5283e+00 (1.7273e+00)	Acc@1  62.50 ( 52.77)	Acc@5  86.25 ( 82.81)
## e[17] optimizer.zero_grad (sum) time: 0.10416626930236816
## e[17]       loss.backward (sum) time: 2.2959320545196533
## e[17]      optimizer.step (sum) time: 0.9298088550567627
## epoch[17] training(only) time: 10.04656982421875
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 2.0840e+00 (2.0840e+00)	Acc@1  49.00 ( 49.00)	Acc@5  76.00 ( 76.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.8887e+00 (1.8745e+00)	Acc@1  49.00 ( 51.55)	Acc@5  81.00 ( 79.00)
Test: [ 20/100]	Time  0.013 ( 0.021)	Loss 1.8408e+00 (1.8836e+00)	Acc@1  50.00 ( 50.24)	Acc@5  79.00 ( 78.90)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.7207e+00 (1.8650e+00)	Acc@1  49.00 ( 50.39)	Acc@5  85.00 ( 79.29)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.7188e+00 (1.8511e+00)	Acc@1  56.00 ( 50.56)	Acc@5  84.00 ( 79.76)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.9658e+00 (1.8730e+00)	Acc@1  52.00 ( 49.94)	Acc@5  74.00 ( 79.33)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.9805e+00 (1.8667e+00)	Acc@1  44.00 ( 49.79)	Acc@5  81.00 ( 79.72)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.9502e+00 (1.8768e+00)	Acc@1  52.00 ( 49.63)	Acc@5  79.00 ( 79.77)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 1.8896e+00 (1.8869e+00)	Acc@1  50.00 ( 49.72)	Acc@5  77.00 ( 79.54)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 2.0586e+00 (1.8782e+00)	Acc@1  45.00 ( 49.97)	Acc@5  75.00 ( 79.63)
 * Acc@1 49.960 Acc@5 79.650
### epoch[17] execution time: 11.636524677276611
EPOCH 18
# current learning rate: 0.1
# Switched to train mode...
Epoch: [18][  0/391]	Time  0.166 ( 0.166)	Data  0.141 ( 0.141)	Loss 1.5303e+00 (1.5303e+00)	Acc@1  57.81 ( 57.81)	Acc@5  85.94 ( 85.94)
Epoch: [18][ 10/391]	Time  0.033 ( 0.039)	Data  0.005 ( 0.015)	Loss 1.3340e+00 (1.5187e+00)	Acc@1  62.50 ( 58.45)	Acc@5  89.06 ( 86.72)
Epoch: [18][ 20/391]	Time  0.039 ( 0.031)	Data  0.002 ( 0.009)	Loss 1.7266e+00 (1.5935e+00)	Acc@1  53.12 ( 56.85)	Acc@5  82.03 ( 84.71)
Epoch: [18][ 30/391]	Time  0.021 ( 0.029)	Data  0.000 ( 0.007)	Loss 1.5078e+00 (1.6282e+00)	Acc@1  60.94 ( 55.62)	Acc@5  82.81 ( 84.53)
Epoch: [18][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.006)	Loss 1.6621e+00 (1.6255e+00)	Acc@1  57.81 ( 55.54)	Acc@5  83.59 ( 84.36)
Epoch: [18][ 50/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.3984e+00 (1.6305e+00)	Acc@1  60.16 ( 55.74)	Acc@5  87.50 ( 84.28)
Epoch: [18][ 60/391]	Time  0.039 ( 0.027)	Data  0.003 ( 0.005)	Loss 1.5879e+00 (1.6370e+00)	Acc@1  57.03 ( 55.67)	Acc@5  85.94 ( 84.21)
Epoch: [18][ 70/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9941e+00 (1.6540e+00)	Acc@1  44.53 ( 55.11)	Acc@5  76.56 ( 83.81)
Epoch: [18][ 80/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.4570e+00 (1.6540e+00)	Acc@1  63.28 ( 55.02)	Acc@5  89.84 ( 83.92)
Epoch: [18][ 90/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6846e+00 (1.6619e+00)	Acc@1  52.34 ( 54.66)	Acc@5  83.59 ( 83.84)
Epoch: [18][100/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.6309e+00 (1.6546e+00)	Acc@1  52.34 ( 54.69)	Acc@5  87.50 ( 84.04)
Epoch: [18][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8740e+00 (1.6604e+00)	Acc@1  51.56 ( 54.54)	Acc@5  83.59 ( 83.88)
Epoch: [18][120/391]	Time  0.049 ( 0.026)	Data  0.010 ( 0.003)	Loss 1.6396e+00 (1.6619e+00)	Acc@1  57.81 ( 54.42)	Acc@5  83.59 ( 83.99)
Epoch: [18][130/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4541e+00 (1.6570e+00)	Acc@1  59.38 ( 54.55)	Acc@5  89.06 ( 84.05)
Epoch: [18][140/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6670e+00 (1.6572e+00)	Acc@1  53.91 ( 54.61)	Acc@5  81.25 ( 84.03)
Epoch: [18][150/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6719e+00 (1.6611e+00)	Acc@1  52.34 ( 54.57)	Acc@5  83.59 ( 83.97)
Epoch: [18][160/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.5811e+00 (1.6627e+00)	Acc@1  57.03 ( 54.59)	Acc@5  85.94 ( 83.94)
Epoch: [18][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7549e+00 (1.6665e+00)	Acc@1  56.25 ( 54.53)	Acc@5  81.25 ( 83.94)
Epoch: [18][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6709e+00 (1.6692e+00)	Acc@1  58.59 ( 54.54)	Acc@5  82.03 ( 83.84)
Epoch: [18][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3447e+00 (1.6675e+00)	Acc@1  68.75 ( 54.62)	Acc@5  88.28 ( 83.84)
Epoch: [18][200/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4434e+00 (1.6693e+00)	Acc@1  60.94 ( 54.55)	Acc@5  89.84 ( 83.80)
Epoch: [18][210/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6318e+00 (1.6674e+00)	Acc@1  59.38 ( 54.64)	Acc@5  78.91 ( 83.78)
Epoch: [18][220/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.5654e+00 (1.6680e+00)	Acc@1  59.38 ( 54.66)	Acc@5  84.38 ( 83.79)
Epoch: [18][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8271e+00 (1.6716e+00)	Acc@1  52.34 ( 54.56)	Acc@5  78.91 ( 83.75)
Epoch: [18][240/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4551e+00 (1.6692e+00)	Acc@1  57.03 ( 54.61)	Acc@5  85.94 ( 83.74)
Epoch: [18][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6436e+00 (1.6702e+00)	Acc@1  57.03 ( 54.53)	Acc@5  84.38 ( 83.77)
Epoch: [18][260/391]	Time  0.028 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.4707e+00 (1.6672e+00)	Acc@1  59.38 ( 54.60)	Acc@5  86.72 ( 83.82)
Epoch: [18][270/391]	Time  0.039 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.6982e+00 (1.6641e+00)	Acc@1  53.12 ( 54.68)	Acc@5  81.25 ( 83.82)
Epoch: [18][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6572e+00 (1.6652e+00)	Acc@1  51.56 ( 54.66)	Acc@5  85.94 ( 83.77)
Epoch: [18][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5830e+00 (1.6667e+00)	Acc@1  55.47 ( 54.62)	Acc@5  88.28 ( 83.75)
Epoch: [18][300/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5986e+00 (1.6683e+00)	Acc@1  59.38 ( 54.55)	Acc@5  81.25 ( 83.74)
Epoch: [18][310/391]	Time  0.039 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.6572e+00 (1.6685e+00)	Acc@1  56.25 ( 54.49)	Acc@5  79.69 ( 83.75)
Epoch: [18][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6787e+00 (1.6687e+00)	Acc@1  56.25 ( 54.54)	Acc@5  82.03 ( 83.74)
Epoch: [18][330/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.8359e+00 (1.6702e+00)	Acc@1  50.00 ( 54.52)	Acc@5  80.47 ( 83.72)
Epoch: [18][340/391]	Time  0.038 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.6582e+00 (1.6717e+00)	Acc@1  57.81 ( 54.51)	Acc@5  85.16 ( 83.66)
Epoch: [18][350/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.8271e+00 (1.6751e+00)	Acc@1  53.12 ( 54.46)	Acc@5  80.47 ( 83.61)
Epoch: [18][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6270e+00 (1.6772e+00)	Acc@1  57.03 ( 54.39)	Acc@5  85.94 ( 83.57)
Epoch: [18][370/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9521e+00 (1.6789e+00)	Acc@1  50.00 ( 54.41)	Acc@5  77.34 ( 83.52)
Epoch: [18][380/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4990e+00 (1.6804e+00)	Acc@1  56.25 ( 54.37)	Acc@5  88.28 ( 83.49)
Epoch: [18][390/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.5967e+00 (1.6819e+00)	Acc@1  55.00 ( 54.31)	Acc@5  90.00 ( 83.48)
## e[18] optimizer.zero_grad (sum) time: 0.10555148124694824
## e[18]       loss.backward (sum) time: 2.2847483158111572
## e[18]      optimizer.step (sum) time: 0.9330835342407227
## epoch[18] training(only) time: 10.048595190048218
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.6094e+00 (1.6094e+00)	Acc@1  54.00 ( 54.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.013 ( 0.025)	Loss 1.8877e+00 (1.7196e+00)	Acc@1  46.00 ( 53.64)	Acc@5  87.00 ( 82.64)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.6143e+00 (1.7373e+00)	Acc@1  58.00 ( 53.71)	Acc@5  81.00 ( 81.81)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.7715e+00 (1.7436e+00)	Acc@1  54.00 ( 53.45)	Acc@5  82.00 ( 81.84)
Test: [ 40/100]	Time  0.018 ( 0.017)	Loss 1.7627e+00 (1.7615e+00)	Acc@1  53.00 ( 53.00)	Acc@5  83.00 ( 81.54)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.7422e+00 (1.7814e+00)	Acc@1  56.00 ( 52.39)	Acc@5  80.00 ( 81.22)
Test: [ 60/100]	Time  0.026 ( 0.016)	Loss 1.6084e+00 (1.7756e+00)	Acc@1  59.00 ( 52.34)	Acc@5  82.00 ( 81.28)
Test: [ 70/100]	Time  0.020 ( 0.015)	Loss 1.9600e+00 (1.7770e+00)	Acc@1  44.00 ( 52.25)	Acc@5  77.00 ( 81.23)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.7236e+00 (1.7828e+00)	Acc@1  58.00 ( 52.12)	Acc@5  78.00 ( 81.11)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.8379e+00 (1.7707e+00)	Acc@1  52.00 ( 52.65)	Acc@5  80.00 ( 81.49)
 * Acc@1 52.840 Acc@5 81.550
### epoch[18] execution time: 11.635281085968018
EPOCH 19
# current learning rate: 0.1
# Switched to train mode...
Epoch: [19][  0/391]	Time  0.182 ( 0.182)	Data  0.154 ( 0.154)	Loss 1.6689e+00 (1.6689e+00)	Acc@1  54.69 ( 54.69)	Acc@5  84.38 ( 84.38)
Epoch: [19][ 10/391]	Time  0.021 ( 0.039)	Data  0.002 ( 0.016)	Loss 1.6709e+00 (1.6439e+00)	Acc@1  57.81 ( 56.32)	Acc@5  83.59 ( 83.59)
Epoch: [19][ 20/391]	Time  0.021 ( 0.033)	Data  0.001 ( 0.009)	Loss 1.7139e+00 (1.5945e+00)	Acc@1  51.56 ( 56.51)	Acc@5  82.03 ( 84.86)
Epoch: [19][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.6113e+00 (1.6138e+00)	Acc@1  52.34 ( 56.00)	Acc@5  86.72 ( 84.75)
Epoch: [19][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.4258e+00 (1.5904e+00)	Acc@1  63.28 ( 56.61)	Acc@5  89.84 ( 85.04)
Epoch: [19][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.5869e+00 (1.5972e+00)	Acc@1  58.59 ( 56.53)	Acc@5  83.59 ( 84.82)
Epoch: [19][ 60/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.8281e+00 (1.6015e+00)	Acc@1  51.56 ( 56.21)	Acc@5  80.47 ( 84.53)
Epoch: [19][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8672e+00 (1.6206e+00)	Acc@1  51.56 ( 55.80)	Acc@5  84.38 ( 84.22)
Epoch: [19][ 80/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9023e+00 (1.6291e+00)	Acc@1  46.09 ( 55.26)	Acc@5  82.03 ( 84.21)
Epoch: [19][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7939e+00 (1.6269e+00)	Acc@1  51.56 ( 55.35)	Acc@5  81.25 ( 84.23)
Epoch: [19][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8105e+00 (1.6248e+00)	Acc@1  52.34 ( 55.34)	Acc@5  82.81 ( 84.29)
Epoch: [19][110/391]	Time  0.019 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.7119e+00 (1.6254e+00)	Acc@1  58.59 ( 55.48)	Acc@5  83.59 ( 84.23)
Epoch: [19][120/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5879e+00 (1.6221e+00)	Acc@1  62.50 ( 55.64)	Acc@5  84.38 ( 84.32)
Epoch: [19][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9375e+00 (1.6277e+00)	Acc@1  49.22 ( 55.41)	Acc@5  79.69 ( 84.32)
Epoch: [19][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4277e+00 (1.6269e+00)	Acc@1  60.94 ( 55.46)	Acc@5  87.50 ( 84.39)
Epoch: [19][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7627e+00 (1.6338e+00)	Acc@1  57.03 ( 55.28)	Acc@5  78.91 ( 84.28)
Epoch: [19][160/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6387e+00 (1.6351e+00)	Acc@1  57.03 ( 55.30)	Acc@5  85.16 ( 84.31)
Epoch: [19][170/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6318e+00 (1.6342e+00)	Acc@1  54.69 ( 55.44)	Acc@5  85.16 ( 84.33)
Epoch: [19][180/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5713e+00 (1.6350e+00)	Acc@1  50.00 ( 55.37)	Acc@5  89.84 ( 84.28)
Epoch: [19][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7539e+00 (1.6361e+00)	Acc@1  53.91 ( 55.40)	Acc@5  79.69 ( 84.20)
Epoch: [19][200/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3584e+00 (1.6338e+00)	Acc@1  58.59 ( 55.41)	Acc@5  87.50 ( 84.25)
Epoch: [19][210/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.0820e+00 (1.6358e+00)	Acc@1  46.09 ( 55.29)	Acc@5  78.12 ( 84.24)
Epoch: [19][220/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7373e+00 (1.6410e+00)	Acc@1  53.91 ( 55.23)	Acc@5  83.59 ( 84.15)
Epoch: [19][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6221e+00 (1.6451e+00)	Acc@1  55.47 ( 55.11)	Acc@5  85.16 ( 84.12)
Epoch: [19][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6914e+00 (1.6454e+00)	Acc@1  57.81 ( 55.15)	Acc@5  82.03 ( 84.11)
Epoch: [19][250/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6885e+00 (1.6425e+00)	Acc@1  51.56 ( 55.22)	Acc@5  85.16 ( 84.19)
Epoch: [19][260/391]	Time  0.038 ( 0.026)	Data  0.015 ( 0.003)	Loss 1.5967e+00 (1.6423e+00)	Acc@1  57.03 ( 55.26)	Acc@5  80.47 ( 84.14)
Epoch: [19][270/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6377e+00 (1.6421e+00)	Acc@1  53.12 ( 55.25)	Acc@5  84.38 ( 84.15)
Epoch: [19][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8799e+00 (1.6414e+00)	Acc@1  45.31 ( 55.27)	Acc@5  80.47 ( 84.17)
Epoch: [19][290/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5254e+00 (1.6432e+00)	Acc@1  54.69 ( 55.20)	Acc@5  85.94 ( 84.15)
Epoch: [19][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3779e+00 (1.6416e+00)	Acc@1  58.59 ( 55.19)	Acc@5  87.50 ( 84.20)
Epoch: [19][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3535e+00 (1.6418e+00)	Acc@1  64.84 ( 55.21)	Acc@5  84.38 ( 84.16)
Epoch: [19][320/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7881e+00 (1.6432e+00)	Acc@1  46.09 ( 55.17)	Acc@5  82.03 ( 84.13)
Epoch: [19][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7646e+00 (1.6415e+00)	Acc@1  50.78 ( 55.17)	Acc@5  81.25 ( 84.16)
Epoch: [19][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8086e+00 (1.6411e+00)	Acc@1  52.34 ( 55.18)	Acc@5  81.25 ( 84.17)
Epoch: [19][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4932e+00 (1.6395e+00)	Acc@1  61.72 ( 55.19)	Acc@5  85.94 ( 84.22)
Epoch: [19][360/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4268e+00 (1.6400e+00)	Acc@1  58.59 ( 55.18)	Acc@5  85.16 ( 84.19)
Epoch: [19][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8184e+00 (1.6405e+00)	Acc@1  53.12 ( 55.16)	Acc@5  78.12 ( 84.19)
Epoch: [19][380/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.7383e+00 (1.6409e+00)	Acc@1  53.91 ( 55.15)	Acc@5  82.81 ( 84.18)
Epoch: [19][390/391]	Time  0.021 ( 0.025)	Data  0.001 ( 0.002)	Loss 1.6357e+00 (1.6409e+00)	Acc@1  56.25 ( 55.14)	Acc@5  78.75 ( 84.16)
## e[19] optimizer.zero_grad (sum) time: 0.10441780090332031
## e[19]       loss.backward (sum) time: 2.2723920345306396
## e[19]      optimizer.step (sum) time: 0.922459602355957
## epoch[19] training(only) time: 10.068949222564697
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.6436e+00 (1.6436e+00)	Acc@1  58.00 ( 58.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 2.0586e+00 (1.8272e+00)	Acc@1  45.00 ( 51.45)	Acc@5  80.00 ( 81.18)
Test: [ 20/100]	Time  0.018 ( 0.020)	Loss 1.8896e+00 (1.8383e+00)	Acc@1  54.00 ( 51.71)	Acc@5  79.00 ( 80.71)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.7773e+00 (1.8003e+00)	Acc@1  51.00 ( 52.42)	Acc@5  83.00 ( 80.61)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 2.0918e+00 (1.7994e+00)	Acc@1  49.00 ( 52.54)	Acc@5  78.00 ( 80.83)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.8125e+00 (1.8216e+00)	Acc@1  59.00 ( 52.49)	Acc@5  76.00 ( 80.37)
Test: [ 60/100]	Time  0.019 ( 0.016)	Loss 1.6006e+00 (1.8163e+00)	Acc@1  61.00 ( 52.33)	Acc@5  82.00 ( 80.56)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.8877e+00 (1.8250e+00)	Acc@1  52.00 ( 52.38)	Acc@5  81.00 ( 80.42)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 1.9072e+00 (1.8389e+00)	Acc@1  56.00 ( 52.12)	Acc@5  76.00 ( 80.33)
Test: [ 90/100]	Time  0.019 ( 0.015)	Loss 1.8203e+00 (1.8340e+00)	Acc@1  52.00 ( 52.26)	Acc@5  86.00 ( 80.52)
 * Acc@1 52.220 Acc@5 80.550
### epoch[19] execution time: 11.669791221618652
EPOCH 20
# current learning rate: 0.1
# Switched to train mode...
Epoch: [20][  0/391]	Time  0.188 ( 0.188)	Data  0.157 ( 0.157)	Loss 1.6172e+00 (1.6172e+00)	Acc@1  53.91 ( 53.91)	Acc@5  82.03 ( 82.03)
Epoch: [20][ 10/391]	Time  0.021 ( 0.039)	Data  0.002 ( 0.016)	Loss 1.3926e+00 (1.6015e+00)	Acc@1  57.81 ( 55.97)	Acc@5  85.16 ( 84.09)
Epoch: [20][ 20/391]	Time  0.021 ( 0.032)	Data  0.002 ( 0.009)	Loss 1.4834e+00 (1.5701e+00)	Acc@1  60.94 ( 56.55)	Acc@5  87.50 ( 85.08)
Epoch: [20][ 30/391]	Time  0.032 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.4014e+00 (1.5657e+00)	Acc@1  60.94 ( 57.11)	Acc@5  82.03 ( 85.13)
Epoch: [20][ 40/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.3828e+00 (1.5706e+00)	Acc@1  60.16 ( 57.22)	Acc@5  87.50 ( 84.89)
Epoch: [20][ 50/391]	Time  0.033 ( 0.029)	Data  0.000 ( 0.005)	Loss 1.6992e+00 (1.5644e+00)	Acc@1  47.66 ( 57.00)	Acc@5  85.16 ( 85.08)
Epoch: [20][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.5381e+00 (1.5618e+00)	Acc@1  57.81 ( 56.97)	Acc@5  86.72 ( 85.07)
Epoch: [20][ 70/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.4600e+00 (1.5605e+00)	Acc@1  64.84 ( 56.98)	Acc@5  84.38 ( 84.99)
Epoch: [20][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5156e+00 (1.5584e+00)	Acc@1  60.16 ( 57.04)	Acc@5  86.72 ( 85.05)
Epoch: [20][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6289e+00 (1.5541e+00)	Acc@1  55.47 ( 57.07)	Acc@5  85.16 ( 85.29)
Epoch: [20][100/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.6836e+00 (1.5600e+00)	Acc@1  52.34 ( 56.89)	Acc@5  83.59 ( 85.14)
Epoch: [20][110/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6270e+00 (1.5593e+00)	Acc@1  48.44 ( 56.74)	Acc@5  85.16 ( 85.21)
Epoch: [20][120/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.3643e+00 (1.5527e+00)	Acc@1  61.72 ( 56.97)	Acc@5  88.28 ( 85.35)
Epoch: [20][130/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6445e+00 (1.5497e+00)	Acc@1  57.03 ( 57.09)	Acc@5  86.72 ( 85.47)
Epoch: [20][140/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5996e+00 (1.5550e+00)	Acc@1  54.69 ( 56.93)	Acc@5  84.38 ( 85.40)
Epoch: [20][150/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5488e+00 (1.5584e+00)	Acc@1  58.59 ( 56.86)	Acc@5  84.38 ( 85.37)
Epoch: [20][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7344e+00 (1.5583e+00)	Acc@1  53.91 ( 56.84)	Acc@5  85.16 ( 85.35)
Epoch: [20][170/391]	Time  0.047 ( 0.026)	Data  0.006 ( 0.003)	Loss 1.2275e+00 (1.5622e+00)	Acc@1  68.75 ( 56.76)	Acc@5  89.06 ( 85.31)
Epoch: [20][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4639e+00 (1.5595e+00)	Acc@1  58.59 ( 56.86)	Acc@5  85.94 ( 85.39)
Epoch: [20][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4658e+00 (1.5625e+00)	Acc@1  57.81 ( 56.81)	Acc@5  86.72 ( 85.32)
Epoch: [20][200/391]	Time  0.041 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.6650e+00 (1.5656e+00)	Acc@1  53.91 ( 56.72)	Acc@5  82.03 ( 85.30)
Epoch: [20][210/391]	Time  0.040 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.7930e+00 (1.5700e+00)	Acc@1  53.91 ( 56.66)	Acc@5  87.50 ( 85.24)
Epoch: [20][220/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3145e+00 (1.5675e+00)	Acc@1  61.72 ( 56.75)	Acc@5  89.06 ( 85.25)
Epoch: [20][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7793e+00 (1.5660e+00)	Acc@1  52.34 ( 56.85)	Acc@5  83.59 ( 85.25)
Epoch: [20][240/391]	Time  0.037 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.7451e+00 (1.5666e+00)	Acc@1  53.91 ( 56.88)	Acc@5  80.47 ( 85.20)
Epoch: [20][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5811e+00 (1.5671e+00)	Acc@1  56.25 ( 56.87)	Acc@5  82.81 ( 85.16)
Epoch: [20][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7334e+00 (1.5702e+00)	Acc@1  50.00 ( 56.78)	Acc@5  82.81 ( 85.15)
Epoch: [20][270/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5146e+00 (1.5701e+00)	Acc@1  64.84 ( 56.79)	Acc@5  82.03 ( 85.15)
Epoch: [20][280/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5439e+00 (1.5737e+00)	Acc@1  53.91 ( 56.69)	Acc@5  85.94 ( 85.09)
Epoch: [20][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4238e+00 (1.5743e+00)	Acc@1  57.03 ( 56.65)	Acc@5  82.81 ( 85.09)
Epoch: [20][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7676e+00 (1.5770e+00)	Acc@1  50.00 ( 56.58)	Acc@5  84.38 ( 85.06)
Epoch: [20][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5518e+00 (1.5784e+00)	Acc@1  56.25 ( 56.51)	Acc@5  85.94 ( 85.06)
Epoch: [20][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5645e+00 (1.5795e+00)	Acc@1  56.25 ( 56.54)	Acc@5  89.06 ( 85.06)
Epoch: [20][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6270e+00 (1.5805e+00)	Acc@1  58.59 ( 56.53)	Acc@5  87.50 ( 85.03)
Epoch: [20][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6299e+00 (1.5797e+00)	Acc@1  59.38 ( 56.54)	Acc@5  85.16 ( 85.06)
Epoch: [20][350/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5615e+00 (1.5809e+00)	Acc@1  56.25 ( 56.51)	Acc@5  85.94 ( 85.04)
Epoch: [20][360/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7090e+00 (1.5827e+00)	Acc@1  50.78 ( 56.43)	Acc@5  85.94 ( 85.05)
Epoch: [20][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5908e+00 (1.5829e+00)	Acc@1  54.69 ( 56.43)	Acc@5  82.81 ( 85.01)
Epoch: [20][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4629e+00 (1.5841e+00)	Acc@1  56.25 ( 56.44)	Acc@5  88.28 ( 84.98)
Epoch: [20][390/391]	Time  0.018 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.5137e+00 (1.5850e+00)	Acc@1  55.00 ( 56.41)	Acc@5  87.50 ( 85.00)
## e[20] optimizer.zero_grad (sum) time: 0.10830831527709961
## e[20]       loss.backward (sum) time: 2.255932569503784
## e[20]      optimizer.step (sum) time: 0.9164659976959229
## epoch[20] training(only) time: 10.152407169342041
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.8213e+00 (1.8213e+00)	Acc@1  51.00 ( 51.00)	Acc@5  80.00 ( 80.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.9902e+00 (1.9350e+00)	Acc@1  49.00 ( 51.64)	Acc@5  82.00 ( 79.45)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.6289e+00 (1.9251e+00)	Acc@1  57.00 ( 51.05)	Acc@5  83.00 ( 78.76)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 2.0312e+00 (1.9243e+00)	Acc@1  50.00 ( 50.81)	Acc@5  77.00 ( 78.77)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.5859e+00 (1.9188e+00)	Acc@1  57.00 ( 50.83)	Acc@5  84.00 ( 79.05)
Test: [ 50/100]	Time  0.013 ( 0.017)	Loss 1.8076e+00 (1.9236e+00)	Acc@1  54.00 ( 50.65)	Acc@5  75.00 ( 78.67)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.9395e+00 (1.9205e+00)	Acc@1  48.00 ( 50.43)	Acc@5  77.00 ( 78.84)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.8633e+00 (1.9309e+00)	Acc@1  56.00 ( 50.28)	Acc@5  80.00 ( 78.66)
Test: [ 80/100]	Time  0.009 ( 0.016)	Loss 1.8516e+00 (1.9347e+00)	Acc@1  56.00 ( 50.04)	Acc@5  80.00 ( 78.69)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.9756e+00 (1.9243e+00)	Acc@1  51.00 ( 50.43)	Acc@5  81.00 ( 78.92)
 * Acc@1 50.380 Acc@5 78.870
### epoch[20] execution time: 11.7997887134552
EPOCH 21
# current learning rate: 0.1
# Switched to train mode...
Epoch: [21][  0/391]	Time  0.176 ( 0.176)	Data  0.150 ( 0.150)	Loss 1.6260e+00 (1.6260e+00)	Acc@1  52.34 ( 52.34)	Acc@5  83.59 ( 83.59)
Epoch: [21][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.6172e+00 (1.5326e+00)	Acc@1  53.91 ( 56.89)	Acc@5  87.50 ( 86.36)
Epoch: [21][ 20/391]	Time  0.024 ( 0.033)	Data  0.001 ( 0.009)	Loss 1.0459e+00 (1.5279e+00)	Acc@1  73.44 ( 56.96)	Acc@5  89.84 ( 85.83)
Epoch: [21][ 30/391]	Time  0.032 ( 0.030)	Data  0.003 ( 0.007)	Loss 1.6309e+00 (1.4997e+00)	Acc@1  55.47 ( 58.24)	Acc@5  85.16 ( 86.04)
Epoch: [21][ 40/391]	Time  0.028 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.4062e+00 (1.5005e+00)	Acc@1  61.72 ( 58.38)	Acc@5  83.59 ( 85.99)
Epoch: [21][ 50/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.3359e+00 (1.5078e+00)	Acc@1  60.16 ( 58.29)	Acc@5  86.72 ( 86.08)
Epoch: [21][ 60/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.4414e+00 (1.5156e+00)	Acc@1  56.25 ( 58.16)	Acc@5  86.72 ( 86.16)
Epoch: [21][ 70/391]	Time  0.036 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.7461e+00 (1.5220e+00)	Acc@1  54.69 ( 58.01)	Acc@5  85.16 ( 86.10)
Epoch: [21][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5742e+00 (1.5181e+00)	Acc@1  53.12 ( 57.97)	Acc@5  86.72 ( 86.14)
Epoch: [21][ 90/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6191e+00 (1.5142e+00)	Acc@1  54.69 ( 57.99)	Acc@5  85.16 ( 86.30)
Epoch: [21][100/391]	Time  0.037 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.2852e+00 (1.5130e+00)	Acc@1  61.72 ( 58.03)	Acc@5  88.28 ( 86.25)
Epoch: [21][110/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.5605e+00 (1.5143e+00)	Acc@1  57.03 ( 58.04)	Acc@5  81.25 ( 86.12)
Epoch: [21][120/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3740e+00 (1.5198e+00)	Acc@1  65.62 ( 58.04)	Acc@5  88.28 ( 86.05)
Epoch: [21][130/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6250e+00 (1.5257e+00)	Acc@1  57.03 ( 57.90)	Acc@5  84.38 ( 85.96)
Epoch: [21][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4346e+00 (1.5283e+00)	Acc@1  60.16 ( 57.85)	Acc@5  89.84 ( 85.96)
Epoch: [21][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6396e+00 (1.5273e+00)	Acc@1  53.12 ( 57.83)	Acc@5  82.81 ( 85.92)
Epoch: [21][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6914e+00 (1.5308e+00)	Acc@1  50.78 ( 57.79)	Acc@5  82.03 ( 85.87)
Epoch: [21][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4785e+00 (1.5255e+00)	Acc@1  55.47 ( 57.83)	Acc@5  89.84 ( 86.01)
Epoch: [21][180/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4160e+00 (1.5240e+00)	Acc@1  60.16 ( 57.87)	Acc@5  86.72 ( 86.05)
Epoch: [21][190/391]	Time  0.046 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6094e+00 (1.5261e+00)	Acc@1  56.25 ( 57.93)	Acc@5  79.69 ( 85.98)
Epoch: [21][200/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6240e+00 (1.5277e+00)	Acc@1  53.12 ( 57.85)	Acc@5  84.38 ( 85.96)
Epoch: [21][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7939e+00 (1.5302e+00)	Acc@1  54.69 ( 57.79)	Acc@5  78.91 ( 85.92)
Epoch: [21][220/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3867e+00 (1.5320e+00)	Acc@1  60.16 ( 57.78)	Acc@5  89.06 ( 85.87)
Epoch: [21][230/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7061e+00 (1.5346e+00)	Acc@1  50.78 ( 57.71)	Acc@5  85.94 ( 85.84)
Epoch: [21][240/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5674e+00 (1.5369e+00)	Acc@1  55.47 ( 57.72)	Acc@5  84.38 ( 85.73)
Epoch: [21][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3467e+00 (1.5372e+00)	Acc@1  61.72 ( 57.70)	Acc@5  90.62 ( 85.76)
Epoch: [21][260/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6680e+00 (1.5410e+00)	Acc@1  53.12 ( 57.62)	Acc@5  82.81 ( 85.67)
Epoch: [21][270/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4541e+00 (1.5430e+00)	Acc@1  60.94 ( 57.60)	Acc@5  83.59 ( 85.63)
Epoch: [21][280/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6592e+00 (1.5419e+00)	Acc@1  56.25 ( 57.61)	Acc@5  84.38 ( 85.66)
Epoch: [21][290/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.5117e+00 (1.5401e+00)	Acc@1  61.72 ( 57.71)	Acc@5  84.38 ( 85.64)
Epoch: [21][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6973e+00 (1.5414e+00)	Acc@1  54.69 ( 57.72)	Acc@5  85.94 ( 85.62)
Epoch: [21][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4336e+00 (1.5428e+00)	Acc@1  59.38 ( 57.72)	Acc@5  89.84 ( 85.61)
Epoch: [21][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7227e+00 (1.5451e+00)	Acc@1  54.69 ( 57.67)	Acc@5  83.59 ( 85.60)
Epoch: [21][330/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7188e+00 (1.5462e+00)	Acc@1  50.78 ( 57.65)	Acc@5  84.38 ( 85.59)
Epoch: [21][340/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.7949e+00 (1.5459e+00)	Acc@1  52.34 ( 57.61)	Acc@5  80.47 ( 85.62)
Epoch: [21][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4727e+00 (1.5477e+00)	Acc@1  56.25 ( 57.52)	Acc@5  87.50 ( 85.57)
Epoch: [21][360/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.6074e+00 (1.5481e+00)	Acc@1  50.78 ( 57.52)	Acc@5  82.81 ( 85.56)
Epoch: [21][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6436e+00 (1.5468e+00)	Acc@1  57.03 ( 57.57)	Acc@5  82.03 ( 85.59)
Epoch: [21][380/391]	Time  0.040 ( 0.026)	Data  0.014 ( 0.002)	Loss 1.5566e+00 (1.5492e+00)	Acc@1  53.91 ( 57.52)	Acc@5  83.59 ( 85.53)
Epoch: [21][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7461e+00 (1.5528e+00)	Acc@1  48.75 ( 57.42)	Acc@5  82.50 ( 85.50)
## e[21] optimizer.zero_grad (sum) time: 0.1053624153137207
## e[21]       loss.backward (sum) time: 2.2518491744995117
## e[21]      optimizer.step (sum) time: 0.9247164726257324
## epoch[21] training(only) time: 10.109283208847046
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 2.0410e+00 (2.0410e+00)	Acc@1  54.00 ( 54.00)	Acc@5  75.00 ( 75.00)
Test: [ 10/100]	Time  0.020 ( 0.026)	Loss 2.0684e+00 (1.9002e+00)	Acc@1  46.00 ( 53.00)	Acc@5  80.00 ( 78.36)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.9150e+00 (1.8864e+00)	Acc@1  55.00 ( 52.90)	Acc@5  81.00 ( 79.81)
Test: [ 30/100]	Time  0.015 ( 0.018)	Loss 2.0449e+00 (1.9004e+00)	Acc@1  46.00 ( 52.39)	Acc@5  78.00 ( 79.45)
Test: [ 40/100]	Time  0.024 ( 0.017)	Loss 1.8799e+00 (1.8871e+00)	Acc@1  47.00 ( 52.05)	Acc@5  85.00 ( 80.02)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.8281e+00 (1.9001e+00)	Acc@1  53.00 ( 51.67)	Acc@5  82.00 ( 79.84)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.6699e+00 (1.8841e+00)	Acc@1  51.00 ( 51.75)	Acc@5  85.00 ( 80.02)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 2.0586e+00 (1.8909e+00)	Acc@1  48.00 ( 51.77)	Acc@5  75.00 ( 80.04)
Test: [ 80/100]	Time  0.020 ( 0.015)	Loss 2.0078e+00 (1.8999e+00)	Acc@1  55.00 ( 51.52)	Acc@5  75.00 ( 79.89)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.0078e+00 (1.8971e+00)	Acc@1  50.00 ( 51.84)	Acc@5  75.00 ( 79.97)
 * Acc@1 51.910 Acc@5 79.840
### epoch[21] execution time: 11.71727728843689
EPOCH 22
# current learning rate: 0.1
# Switched to train mode...
Epoch: [22][  0/391]	Time  0.179 ( 0.179)	Data  0.154 ( 0.154)	Loss 1.2861e+00 (1.2861e+00)	Acc@1  62.50 ( 62.50)	Acc@5  91.41 ( 91.41)
Epoch: [22][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.016)	Loss 1.4697e+00 (1.4928e+00)	Acc@1  58.59 ( 60.30)	Acc@5  85.16 ( 86.01)
Epoch: [22][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.4482e+00 (1.4548e+00)	Acc@1  55.47 ( 59.78)	Acc@5  87.50 ( 86.94)
Epoch: [22][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.2900e+00 (1.4475e+00)	Acc@1  67.19 ( 60.08)	Acc@5  90.62 ( 87.25)
Epoch: [22][ 40/391]	Time  0.022 ( 0.029)	Data  0.002 ( 0.006)	Loss 1.5693e+00 (1.4637e+00)	Acc@1  54.69 ( 59.66)	Acc@5  85.94 ( 86.95)
Epoch: [22][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.4814e+00 (1.4788e+00)	Acc@1  58.59 ( 59.08)	Acc@5  86.72 ( 86.92)
Epoch: [22][ 60/391]	Time  0.038 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.6982e+00 (1.4800e+00)	Acc@1  57.81 ( 59.21)	Acc@5  82.81 ( 86.63)
Epoch: [22][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3691e+00 (1.4780e+00)	Acc@1  60.94 ( 59.35)	Acc@5  90.62 ( 86.72)
Epoch: [22][ 80/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8213e+00 (1.4833e+00)	Acc@1  53.12 ( 59.34)	Acc@5  83.59 ( 86.63)
Epoch: [22][ 90/391]	Time  0.028 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.7539e+00 (1.4923e+00)	Acc@1  57.03 ( 59.07)	Acc@5  84.38 ( 86.57)
Epoch: [22][100/391]	Time  0.029 ( 0.027)	Data  0.005 ( 0.003)	Loss 1.8096e+00 (1.4998e+00)	Acc@1  47.66 ( 58.79)	Acc@5  83.59 ( 86.52)
Epoch: [22][110/391]	Time  0.041 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.4902e+00 (1.5075e+00)	Acc@1  64.06 ( 58.58)	Acc@5  89.06 ( 86.31)
Epoch: [22][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5410e+00 (1.5022e+00)	Acc@1  58.59 ( 58.76)	Acc@5  87.50 ( 86.46)
Epoch: [22][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4180e+00 (1.5030e+00)	Acc@1  57.03 ( 58.78)	Acc@5  89.06 ( 86.53)
Epoch: [22][140/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.4600e+00 (1.5034e+00)	Acc@1  59.38 ( 58.74)	Acc@5  92.97 ( 86.54)
Epoch: [22][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7188e+00 (1.5088e+00)	Acc@1  50.78 ( 58.63)	Acc@5  89.06 ( 86.53)
Epoch: [22][160/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.7129e+00 (1.5113e+00)	Acc@1  50.78 ( 58.58)	Acc@5  83.59 ( 86.49)
Epoch: [22][170/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3799e+00 (1.5092e+00)	Acc@1  57.81 ( 58.56)	Acc@5  84.38 ( 86.47)
Epoch: [22][180/391]	Time  0.033 ( 0.026)	Data  0.016 ( 0.003)	Loss 1.3877e+00 (1.5064e+00)	Acc@1  62.50 ( 58.62)	Acc@5  90.62 ( 86.53)
Epoch: [22][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6797e+00 (1.5066e+00)	Acc@1  50.78 ( 58.57)	Acc@5  83.59 ( 86.53)
Epoch: [22][200/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2676e+00 (1.5066e+00)	Acc@1  65.62 ( 58.62)	Acc@5  88.28 ( 86.47)
Epoch: [22][210/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5732e+00 (1.5106e+00)	Acc@1  60.16 ( 58.48)	Acc@5  87.50 ( 86.40)
Epoch: [22][220/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6055e+00 (1.5140e+00)	Acc@1  55.47 ( 58.40)	Acc@5  87.50 ( 86.34)
Epoch: [22][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5586e+00 (1.5098e+00)	Acc@1  59.38 ( 58.47)	Acc@5  86.72 ( 86.41)
Epoch: [22][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2861e+00 (1.5092e+00)	Acc@1  61.72 ( 58.47)	Acc@5  88.28 ( 86.41)
Epoch: [22][250/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.4482e+00 (1.5153e+00)	Acc@1  60.94 ( 58.32)	Acc@5  91.41 ( 86.30)
Epoch: [22][260/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6660e+00 (1.5137e+00)	Acc@1  52.34 ( 58.37)	Acc@5  82.81 ( 86.31)
Epoch: [22][270/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4316e+00 (1.5142e+00)	Acc@1  64.06 ( 58.41)	Acc@5  85.16 ( 86.25)
Epoch: [22][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5801e+00 (1.5161e+00)	Acc@1  56.25 ( 58.35)	Acc@5  84.38 ( 86.25)
Epoch: [22][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4531e+00 (1.5145e+00)	Acc@1  58.59 ( 58.37)	Acc@5  84.38 ( 86.27)
Epoch: [22][300/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3604e+00 (1.5170e+00)	Acc@1  59.38 ( 58.30)	Acc@5  89.84 ( 86.24)
Epoch: [22][310/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.5322e+00 (1.5210e+00)	Acc@1  59.38 ( 58.18)	Acc@5  86.72 ( 86.14)
Epoch: [22][320/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6064e+00 (1.5218e+00)	Acc@1  53.91 ( 58.17)	Acc@5  82.81 ( 86.13)
Epoch: [22][330/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5762e+00 (1.5240e+00)	Acc@1  53.12 ( 58.13)	Acc@5  85.16 ( 86.11)
Epoch: [22][340/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.4473e+00 (1.5273e+00)	Acc@1  57.81 ( 58.07)	Acc@5  89.06 ( 86.05)
Epoch: [22][350/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.5566e+00 (1.5266e+00)	Acc@1  56.25 ( 58.08)	Acc@5  89.06 ( 86.06)
Epoch: [22][360/391]	Time  0.037 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.5527e+00 (1.5269e+00)	Acc@1  57.03 ( 58.09)	Acc@5  88.28 ( 86.04)
Epoch: [22][370/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5166e+00 (1.5236e+00)	Acc@1  60.94 ( 58.19)	Acc@5  84.38 ( 86.06)
Epoch: [22][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4375e+00 (1.5213e+00)	Acc@1  63.28 ( 58.28)	Acc@5  89.06 ( 86.10)
Epoch: [22][390/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.5400e+00 (1.5230e+00)	Acc@1  57.50 ( 58.21)	Acc@5  80.00 ( 86.07)
## e[22] optimizer.zero_grad (sum) time: 0.1067354679107666
## e[22]       loss.backward (sum) time: 2.2129271030426025
## e[22]      optimizer.step (sum) time: 0.9214117527008057
## epoch[22] training(only) time: 10.142543077468872
# Switched to evaluate mode...
Test: [  0/100]	Time  0.136 ( 0.136)	Loss 1.5010e+00 (1.5010e+00)	Acc@1  64.00 ( 64.00)	Acc@5  84.00 ( 84.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.8652e+00 (1.7837e+00)	Acc@1  47.00 ( 54.09)	Acc@5  81.00 ( 80.55)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 1.7471e+00 (1.7996e+00)	Acc@1  55.00 ( 53.29)	Acc@5  78.00 ( 80.90)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.8086e+00 (1.8103e+00)	Acc@1  49.00 ( 52.61)	Acc@5  83.00 ( 81.23)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.9990e+00 (1.8095e+00)	Acc@1  49.00 ( 52.61)	Acc@5  80.00 ( 81.15)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.9951e+00 (1.8252e+00)	Acc@1  52.00 ( 52.39)	Acc@5  77.00 ( 80.82)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 1.8418e+00 (1.8128e+00)	Acc@1  53.00 ( 52.62)	Acc@5  82.00 ( 81.03)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.8408e+00 (1.8116e+00)	Acc@1  45.00 ( 52.72)	Acc@5  79.00 ( 81.27)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.8789e+00 (1.8155e+00)	Acc@1  56.00 ( 52.65)	Acc@5  77.00 ( 81.05)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 1.9707e+00 (1.8104e+00)	Acc@1  50.00 ( 52.79)	Acc@5  83.00 ( 81.12)
 * Acc@1 53.020 Acc@5 81.120
### epoch[22] execution time: 11.744670152664185
EPOCH 23
# current learning rate: 0.1
# Switched to train mode...
Epoch: [23][  0/391]	Time  0.179 ( 0.179)	Data  0.148 ( 0.148)	Loss 1.6885e+00 (1.6885e+00)	Acc@1  57.03 ( 57.03)	Acc@5  82.03 ( 82.03)
Epoch: [23][ 10/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.015)	Loss 1.3955e+00 (1.4459e+00)	Acc@1  59.38 ( 60.01)	Acc@5  85.16 ( 86.72)
Epoch: [23][ 20/391]	Time  0.021 ( 0.033)	Data  0.001 ( 0.009)	Loss 1.4521e+00 (1.4494e+00)	Acc@1  63.28 ( 59.97)	Acc@5  87.50 ( 86.79)
Epoch: [23][ 30/391]	Time  0.038 ( 0.030)	Data  0.002 ( 0.007)	Loss 1.2500e+00 (1.4349e+00)	Acc@1  67.19 ( 60.86)	Acc@5  90.62 ( 87.05)
Epoch: [23][ 40/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.1641e+00 (1.4278e+00)	Acc@1  67.19 ( 60.71)	Acc@5  92.97 ( 87.21)
Epoch: [23][ 50/391]	Time  0.027 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.5049e+00 (1.4319e+00)	Acc@1  63.28 ( 60.39)	Acc@5  85.94 ( 87.21)
Epoch: [23][ 60/391]	Time  0.026 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.7324e+00 (1.4501e+00)	Acc@1  50.78 ( 60.14)	Acc@5  83.59 ( 86.90)
Epoch: [23][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3555e+00 (1.4425e+00)	Acc@1  57.81 ( 60.26)	Acc@5  92.19 ( 87.24)
Epoch: [23][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4512e+00 (1.4486e+00)	Acc@1  61.72 ( 60.20)	Acc@5  83.59 ( 87.13)
Epoch: [23][ 90/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.6523e+00 (1.4546e+00)	Acc@1  57.03 ( 60.00)	Acc@5  82.81 ( 87.05)
Epoch: [23][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3447e+00 (1.4576e+00)	Acc@1  59.38 ( 60.00)	Acc@5  92.97 ( 87.01)
Epoch: [23][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3096e+00 (1.4549e+00)	Acc@1  60.94 ( 60.12)	Acc@5  87.50 ( 87.01)
Epoch: [23][120/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6553e+00 (1.4631e+00)	Acc@1  52.34 ( 59.79)	Acc@5  85.16 ( 86.92)
Epoch: [23][130/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8896e+00 (1.4644e+00)	Acc@1  50.78 ( 59.74)	Acc@5  82.03 ( 86.92)
Epoch: [23][140/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3916e+00 (1.4684e+00)	Acc@1  59.38 ( 59.69)	Acc@5  86.72 ( 86.90)
Epoch: [23][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5459e+00 (1.4698e+00)	Acc@1  57.03 ( 59.76)	Acc@5  83.59 ( 86.94)
Epoch: [23][160/391]	Time  0.034 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.4551e+00 (1.4682e+00)	Acc@1  58.59 ( 59.67)	Acc@5  86.72 ( 86.95)
Epoch: [23][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4053e+00 (1.4710e+00)	Acc@1  60.94 ( 59.64)	Acc@5  85.16 ( 86.90)
Epoch: [23][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3057e+00 (1.4747e+00)	Acc@1  64.84 ( 59.57)	Acc@5  89.84 ( 86.84)
Epoch: [23][190/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5752e+00 (1.4791e+00)	Acc@1  55.47 ( 59.47)	Acc@5  82.81 ( 86.73)
Epoch: [23][200/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.4629e+00 (1.4821e+00)	Acc@1  62.50 ( 59.39)	Acc@5  86.72 ( 86.67)
Epoch: [23][210/391]	Time  0.018 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.4248e+00 (1.4835e+00)	Acc@1  64.06 ( 59.39)	Acc@5  85.94 ( 86.67)
Epoch: [23][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4785e+00 (1.4825e+00)	Acc@1  60.16 ( 59.44)	Acc@5  86.72 ( 86.68)
Epoch: [23][230/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6172e+00 (1.4832e+00)	Acc@1  57.03 ( 59.42)	Acc@5  81.25 ( 86.65)
Epoch: [23][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2383e+00 (1.4832e+00)	Acc@1  66.41 ( 59.45)	Acc@5  90.62 ( 86.64)
Epoch: [23][250/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5889e+00 (1.4829e+00)	Acc@1  52.34 ( 59.40)	Acc@5  88.28 ( 86.68)
Epoch: [23][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4111e+00 (1.4836e+00)	Acc@1  58.59 ( 59.39)	Acc@5  85.94 ( 86.69)
Epoch: [23][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4746e+00 (1.4853e+00)	Acc@1  55.47 ( 59.30)	Acc@5  89.84 ( 86.70)
Epoch: [23][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5439e+00 (1.4876e+00)	Acc@1  57.81 ( 59.31)	Acc@5  85.94 ( 86.64)
Epoch: [23][290/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.4824e+00 (1.4880e+00)	Acc@1  59.38 ( 59.32)	Acc@5  85.94 ( 86.64)
Epoch: [23][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6611e+00 (1.4875e+00)	Acc@1  57.03 ( 59.31)	Acc@5  85.94 ( 86.66)
Epoch: [23][310/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5020e+00 (1.4860e+00)	Acc@1  60.16 ( 59.36)	Acc@5  80.47 ( 86.67)
Epoch: [23][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2998e+00 (1.4867e+00)	Acc@1  60.16 ( 59.32)	Acc@5  85.16 ( 86.64)
Epoch: [23][330/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3721e+00 (1.4864e+00)	Acc@1  61.72 ( 59.32)	Acc@5  89.84 ( 86.62)
Epoch: [23][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4150e+00 (1.4867e+00)	Acc@1  55.47 ( 59.32)	Acc@5  87.50 ( 86.59)
Epoch: [23][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4727e+00 (1.4860e+00)	Acc@1  55.47 ( 59.32)	Acc@5  85.16 ( 86.59)
Epoch: [23][360/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5332e+00 (1.4874e+00)	Acc@1  58.59 ( 59.27)	Acc@5  87.50 ( 86.55)
Epoch: [23][370/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.3838e+00 (1.4901e+00)	Acc@1  60.94 ( 59.20)	Acc@5  86.72 ( 86.51)
Epoch: [23][380/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8418e+00 (1.4927e+00)	Acc@1  50.00 ( 59.16)	Acc@5  77.34 ( 86.47)
Epoch: [23][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7539e+00 (1.4965e+00)	Acc@1  53.75 ( 59.09)	Acc@5  77.50 ( 86.42)
## e[23] optimizer.zero_grad (sum) time: 0.10491442680358887
## e[23]       loss.backward (sum) time: 2.228457450866699
## e[23]      optimizer.step (sum) time: 0.9169628620147705
## epoch[23] training(only) time: 10.112255811691284
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.6367e+00 (1.6367e+00)	Acc@1  53.00 ( 53.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.024 ( 0.025)	Loss 1.8965e+00 (1.6839e+00)	Acc@1  50.00 ( 56.36)	Acc@5  81.00 ( 81.45)
Test: [ 20/100]	Time  0.013 ( 0.019)	Loss 1.6729e+00 (1.6928e+00)	Acc@1  59.00 ( 55.38)	Acc@5  84.00 ( 82.00)
Test: [ 30/100]	Time  0.027 ( 0.018)	Loss 1.9912e+00 (1.7135e+00)	Acc@1  49.00 ( 55.10)	Acc@5  75.00 ( 81.42)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4814e+00 (1.7058e+00)	Acc@1  62.00 ( 55.17)	Acc@5  85.00 ( 81.73)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.7334e+00 (1.7132e+00)	Acc@1  63.00 ( 54.94)	Acc@5  80.00 ( 81.59)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6553e+00 (1.7080e+00)	Acc@1  60.00 ( 55.07)	Acc@5  86.00 ( 81.82)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.7822e+00 (1.7111e+00)	Acc@1  54.00 ( 54.99)	Acc@5  82.00 ( 81.96)
Test: [ 80/100]	Time  0.020 ( 0.015)	Loss 1.8398e+00 (1.7114e+00)	Acc@1  53.00 ( 55.00)	Acc@5  78.00 ( 82.02)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.8818e+00 (1.7015e+00)	Acc@1  45.00 ( 55.10)	Acc@5  78.00 ( 82.12)
 * Acc@1 55.070 Acc@5 82.050
### epoch[23] execution time: 11.693651914596558
EPOCH 24
# current learning rate: 0.1
# Switched to train mode...
Epoch: [24][  0/391]	Time  0.181 ( 0.181)	Data  0.153 ( 0.153)	Loss 1.3916e+00 (1.3916e+00)	Acc@1  59.38 ( 59.38)	Acc@5  88.28 ( 88.28)
Epoch: [24][ 10/391]	Time  0.028 ( 0.040)	Data  0.001 ( 0.015)	Loss 1.2949e+00 (1.3590e+00)	Acc@1  64.84 ( 61.86)	Acc@5  89.06 ( 88.85)
Epoch: [24][ 20/391]	Time  0.042 ( 0.032)	Data  0.008 ( 0.009)	Loss 1.3975e+00 (1.3780e+00)	Acc@1  60.16 ( 61.72)	Acc@5  85.94 ( 87.83)
Epoch: [24][ 30/391]	Time  0.028 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.5820e+00 (1.3745e+00)	Acc@1  54.69 ( 61.97)	Acc@5  85.16 ( 87.75)
Epoch: [24][ 40/391]	Time  0.024 ( 0.029)	Data  0.003 ( 0.006)	Loss 1.2734e+00 (1.3729e+00)	Acc@1  63.28 ( 61.89)	Acc@5  87.50 ( 87.75)
Epoch: [24][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.6475e+00 (1.3865e+00)	Acc@1  53.12 ( 61.41)	Acc@5  84.38 ( 87.39)
Epoch: [24][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.4551e+00 (1.3997e+00)	Acc@1  57.81 ( 61.18)	Acc@5  85.94 ( 87.33)
Epoch: [24][ 70/391]	Time  0.024 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.3184e+00 (1.4008e+00)	Acc@1  61.72 ( 61.16)	Acc@5  90.62 ( 87.43)
Epoch: [24][ 80/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.2178e+00 (1.4073e+00)	Acc@1  64.06 ( 60.92)	Acc@5  92.19 ( 87.45)
Epoch: [24][ 90/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2930e+00 (1.3982e+00)	Acc@1  64.06 ( 61.32)	Acc@5  89.06 ( 87.57)
Epoch: [24][100/391]	Time  0.038 ( 0.027)	Data  0.003 ( 0.003)	Loss 1.4170e+00 (1.3958e+00)	Acc@1  62.50 ( 61.45)	Acc@5  85.16 ( 87.52)
Epoch: [24][110/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8271e+00 (1.3999e+00)	Acc@1  53.91 ( 61.47)	Acc@5  85.16 ( 87.49)
Epoch: [24][120/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3945e+00 (1.4072e+00)	Acc@1  64.06 ( 61.32)	Acc@5  87.50 ( 87.49)
Epoch: [24][130/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.1025e+00 (1.4072e+00)	Acc@1  68.75 ( 61.35)	Acc@5  92.19 ( 87.46)
Epoch: [24][140/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6270e+00 (1.4096e+00)	Acc@1  58.59 ( 61.36)	Acc@5  85.16 ( 87.39)
Epoch: [24][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4785e+00 (1.4121e+00)	Acc@1  60.16 ( 61.23)	Acc@5  86.72 ( 87.40)
Epoch: [24][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4727e+00 (1.4123e+00)	Acc@1  57.81 ( 61.21)	Acc@5  87.50 ( 87.48)
Epoch: [24][170/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.5342e+00 (1.4103e+00)	Acc@1  57.81 ( 61.23)	Acc@5  84.38 ( 87.47)
Epoch: [24][180/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5381e+00 (1.4125e+00)	Acc@1  59.38 ( 61.13)	Acc@5  82.03 ( 87.44)
Epoch: [24][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3721e+00 (1.4169e+00)	Acc@1  63.28 ( 60.99)	Acc@5  89.84 ( 87.37)
Epoch: [24][200/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3076e+00 (1.4170e+00)	Acc@1  61.72 ( 60.93)	Acc@5  92.19 ( 87.38)
Epoch: [24][210/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5205e+00 (1.4161e+00)	Acc@1  55.47 ( 60.93)	Acc@5  90.62 ( 87.40)
Epoch: [24][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0000e+00 (1.4199e+00)	Acc@1  51.56 ( 60.85)	Acc@5  76.56 ( 87.34)
Epoch: [24][230/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4092e+00 (1.4221e+00)	Acc@1  60.94 ( 60.85)	Acc@5  91.41 ( 87.31)
Epoch: [24][240/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5176e+00 (1.4233e+00)	Acc@1  51.56 ( 60.76)	Acc@5  86.72 ( 87.31)
Epoch: [24][250/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5537e+00 (1.4258e+00)	Acc@1  54.69 ( 60.76)	Acc@5  89.06 ( 87.30)
Epoch: [24][260/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.2646e+00 (1.4263e+00)	Acc@1  67.97 ( 60.78)	Acc@5  92.19 ( 87.30)
Epoch: [24][270/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5049e+00 (1.4294e+00)	Acc@1  57.03 ( 60.69)	Acc@5  85.94 ( 87.21)
Epoch: [24][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4658e+00 (1.4292e+00)	Acc@1  57.81 ( 60.71)	Acc@5  85.94 ( 87.23)
Epoch: [24][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4590e+00 (1.4293e+00)	Acc@1  63.28 ( 60.71)	Acc@5  85.94 ( 87.22)
Epoch: [24][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7695e+00 (1.4317e+00)	Acc@1  54.69 ( 60.68)	Acc@5  83.59 ( 87.19)
Epoch: [24][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6533e+00 (1.4346e+00)	Acc@1  50.00 ( 60.60)	Acc@5  85.94 ( 87.12)
Epoch: [24][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4111e+00 (1.4376e+00)	Acc@1  59.38 ( 60.53)	Acc@5  89.84 ( 87.08)
Epoch: [24][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2852e+00 (1.4384e+00)	Acc@1  64.84 ( 60.52)	Acc@5  86.72 ( 87.07)
Epoch: [24][340/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.2979e+00 (1.4376e+00)	Acc@1  62.50 ( 60.52)	Acc@5  89.06 ( 87.10)
Epoch: [24][350/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2783e+00 (1.4387e+00)	Acc@1  62.50 ( 60.48)	Acc@5  89.06 ( 87.13)
Epoch: [24][360/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6084e+00 (1.4393e+00)	Acc@1  56.25 ( 60.40)	Acc@5  89.06 ( 87.17)
Epoch: [24][370/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.2939e+00 (1.4398e+00)	Acc@1  62.50 ( 60.34)	Acc@5  87.50 ( 87.13)
Epoch: [24][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4629e+00 (1.4390e+00)	Acc@1  56.25 ( 60.31)	Acc@5  89.84 ( 87.15)
Epoch: [24][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2246e+00 (1.4394e+00)	Acc@1  58.75 ( 60.32)	Acc@5  95.00 ( 87.14)
## e[24] optimizer.zero_grad (sum) time: 0.1035623550415039
## e[24]       loss.backward (sum) time: 2.237762928009033
## e[24]      optimizer.step (sum) time: 0.9119501113891602
## epoch[24] training(only) time: 10.12670636177063
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.4668e+00 (1.4668e+00)	Acc@1  62.00 ( 62.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 1.7881e+00 (1.7639e+00)	Acc@1  51.00 ( 54.82)	Acc@5  84.00 ( 81.00)
Test: [ 20/100]	Time  0.018 ( 0.021)	Loss 1.5850e+00 (1.7379e+00)	Acc@1  57.00 ( 54.62)	Acc@5  85.00 ( 81.86)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.5771e+00 (1.7421e+00)	Acc@1  56.00 ( 54.48)	Acc@5  84.00 ( 81.90)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4590e+00 (1.7126e+00)	Acc@1  60.00 ( 55.24)	Acc@5  87.00 ( 82.51)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.6875e+00 (1.7281e+00)	Acc@1  53.00 ( 54.47)	Acc@5  84.00 ( 82.43)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.8037e+00 (1.7108e+00)	Acc@1  53.00 ( 54.84)	Acc@5  82.00 ( 82.82)
Test: [ 70/100]	Time  0.020 ( 0.016)	Loss 1.6094e+00 (1.7173e+00)	Acc@1  55.00 ( 54.72)	Acc@5  83.00 ( 82.77)
Test: [ 80/100]	Time  0.012 ( 0.016)	Loss 1.6904e+00 (1.7220e+00)	Acc@1  60.00 ( 54.86)	Acc@5  81.00 ( 82.69)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9541e+00 (1.7080e+00)	Acc@1  51.00 ( 55.23)	Acc@5  80.00 ( 82.93)
 * Acc@1 55.190 Acc@5 82.860
### epoch[24] execution time: 11.73363184928894
EPOCH 25
# current learning rate: 0.1
# Switched to train mode...
Epoch: [25][  0/391]	Time  0.178 ( 0.178)	Data  0.149 ( 0.149)	Loss 1.4824e+00 (1.4824e+00)	Acc@1  58.59 ( 58.59)	Acc@5  89.06 ( 89.06)
Epoch: [25][ 10/391]	Time  0.034 ( 0.039)	Data  0.002 ( 0.015)	Loss 1.4844e+00 (1.3946e+00)	Acc@1  61.72 ( 60.87)	Acc@5  86.72 ( 88.14)
Epoch: [25][ 20/391]	Time  0.045 ( 0.032)	Data  0.011 ( 0.009)	Loss 1.4521e+00 (1.4053e+00)	Acc@1  57.03 ( 60.45)	Acc@5  89.06 ( 88.02)
Epoch: [25][ 30/391]	Time  0.021 ( 0.029)	Data  0.002 ( 0.007)	Loss 9.9609e-01 (1.3767e+00)	Acc@1  71.88 ( 61.67)	Acc@5  92.19 ( 88.21)
Epoch: [25][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.1436e+00 (1.3641e+00)	Acc@1  64.84 ( 62.06)	Acc@5  92.97 ( 88.17)
Epoch: [25][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.4121e+00 (1.3702e+00)	Acc@1  62.50 ( 62.01)	Acc@5  89.06 ( 88.40)
Epoch: [25][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.2881e+00 (1.3621e+00)	Acc@1  61.72 ( 62.15)	Acc@5  92.97 ( 88.55)
Epoch: [25][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2344e+00 (1.3708e+00)	Acc@1  63.28 ( 61.97)	Acc@5  90.62 ( 88.34)
Epoch: [25][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5039e+00 (1.3792e+00)	Acc@1  61.72 ( 61.77)	Acc@5  88.28 ( 88.30)
Epoch: [25][ 90/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.2588e+00 (1.3822e+00)	Acc@1  62.50 ( 61.68)	Acc@5  90.62 ( 88.20)
Epoch: [25][100/391]	Time  0.050 ( 0.027)	Data  0.014 ( 0.004)	Loss 1.5703e+00 (1.3907e+00)	Acc@1  58.59 ( 61.45)	Acc@5  82.03 ( 87.98)
Epoch: [25][110/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.0811e+00 (1.3815e+00)	Acc@1  69.53 ( 61.74)	Acc@5  89.84 ( 87.99)
Epoch: [25][120/391]	Time  0.037 ( 0.026)	Data  0.007 ( 0.003)	Loss 1.3232e+00 (1.3806e+00)	Acc@1  57.03 ( 61.67)	Acc@5  91.41 ( 87.95)
Epoch: [25][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3936e+00 (1.3813e+00)	Acc@1  58.59 ( 61.68)	Acc@5  86.72 ( 87.92)
Epoch: [25][140/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3701e+00 (1.3805e+00)	Acc@1  67.19 ( 61.72)	Acc@5  86.72 ( 87.93)
Epoch: [25][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5576e+00 (1.3846e+00)	Acc@1  59.38 ( 61.66)	Acc@5  82.81 ( 87.81)
Epoch: [25][160/391]	Time  0.020 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.5342e+00 (1.3885e+00)	Acc@1  57.03 ( 61.54)	Acc@5  88.28 ( 87.77)
Epoch: [25][170/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4990e+00 (1.3941e+00)	Acc@1  61.72 ( 61.50)	Acc@5  85.16 ( 87.64)
Epoch: [25][180/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5830e+00 (1.3942e+00)	Acc@1  57.81 ( 61.51)	Acc@5  83.59 ( 87.60)
Epoch: [25][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2871e+00 (1.4007e+00)	Acc@1  60.16 ( 61.30)	Acc@5  88.28 ( 87.57)
Epoch: [25][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3174e+00 (1.4042e+00)	Acc@1  60.94 ( 61.24)	Acc@5  85.94 ( 87.58)
Epoch: [25][210/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3418e+00 (1.4055e+00)	Acc@1  60.94 ( 61.16)	Acc@5  91.41 ( 87.58)
Epoch: [25][220/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.5303e+00 (1.4043e+00)	Acc@1  60.16 ( 61.21)	Acc@5  86.72 ( 87.59)
Epoch: [25][230/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3145e+00 (1.4040e+00)	Acc@1  57.03 ( 61.14)	Acc@5  86.72 ( 87.59)
Epoch: [25][240/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.2666e+00 (1.4062e+00)	Acc@1  61.72 ( 61.15)	Acc@5  93.75 ( 87.55)
Epoch: [25][250/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1123e+00 (1.4049e+00)	Acc@1  64.06 ( 61.18)	Acc@5  94.53 ( 87.58)
Epoch: [25][260/391]	Time  0.035 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.3525e+00 (1.4049e+00)	Acc@1  59.38 ( 61.19)	Acc@5  91.41 ( 87.57)
Epoch: [25][270/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5850e+00 (1.4028e+00)	Acc@1  56.25 ( 61.19)	Acc@5  85.16 ( 87.62)
Epoch: [25][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4473e+00 (1.4014e+00)	Acc@1  59.38 ( 61.18)	Acc@5  85.16 ( 87.65)
Epoch: [25][290/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.7383e+00 (1.4055e+00)	Acc@1  56.25 ( 61.13)	Acc@5  82.81 ( 87.56)
Epoch: [25][300/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.4727e+00 (1.4079e+00)	Acc@1  61.72 ( 61.07)	Acc@5  86.72 ( 87.53)
Epoch: [25][310/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.5996e+00 (1.4070e+00)	Acc@1  56.25 ( 61.07)	Acc@5  85.16 ( 87.58)
Epoch: [25][320/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.4385e+00 (1.4102e+00)	Acc@1  62.50 ( 60.96)	Acc@5  85.94 ( 87.54)
Epoch: [25][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4141e+00 (1.4133e+00)	Acc@1  59.38 ( 60.92)	Acc@5  89.06 ( 87.51)
Epoch: [25][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4775e+00 (1.4141e+00)	Acc@1  61.72 ( 60.91)	Acc@5  83.59 ( 87.49)
Epoch: [25][350/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.3223e+00 (1.4161e+00)	Acc@1  66.41 ( 60.88)	Acc@5  88.28 ( 87.42)
Epoch: [25][360/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2305e+00 (1.4179e+00)	Acc@1  64.84 ( 60.83)	Acc@5  89.06 ( 87.39)
Epoch: [25][370/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7471e+00 (1.4200e+00)	Acc@1  54.69 ( 60.80)	Acc@5  84.38 ( 87.37)
Epoch: [25][380/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.4961e+00 (1.4212e+00)	Acc@1  58.59 ( 60.77)	Acc@5  86.72 ( 87.36)
Epoch: [25][390/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5186e+00 (1.4225e+00)	Acc@1  56.25 ( 60.72)	Acc@5  86.25 ( 87.32)
## e[25] optimizer.zero_grad (sum) time: 0.1050715446472168
## e[25]       loss.backward (sum) time: 2.2720882892608643
## e[25]      optimizer.step (sum) time: 0.9208228588104248
## epoch[25] training(only) time: 10.113868236541748
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.6279e+00 (1.6279e+00)	Acc@1  56.00 ( 56.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 1.8174e+00 (1.7706e+00)	Acc@1  50.00 ( 53.55)	Acc@5  84.00 ( 81.91)
Test: [ 20/100]	Time  0.009 ( 0.020)	Loss 1.6533e+00 (1.7450e+00)	Acc@1  59.00 ( 54.81)	Acc@5  82.00 ( 81.57)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.7646e+00 (1.7515e+00)	Acc@1  51.00 ( 54.06)	Acc@5  79.00 ( 82.13)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.7246e+00 (1.7245e+00)	Acc@1  56.00 ( 54.29)	Acc@5  87.00 ( 82.78)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.7891e+00 (1.7435e+00)	Acc@1  61.00 ( 54.16)	Acc@5  79.00 ( 82.43)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.7646e+00 (1.7309e+00)	Acc@1  55.00 ( 54.31)	Acc@5  83.00 ( 82.75)
Test: [ 70/100]	Time  0.009 ( 0.016)	Loss 1.7715e+00 (1.7317e+00)	Acc@1  58.00 ( 54.41)	Acc@5  76.00 ( 82.63)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.8867e+00 (1.7401e+00)	Acc@1  55.00 ( 54.21)	Acc@5  79.00 ( 82.46)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 1.8828e+00 (1.7333e+00)	Acc@1  50.00 ( 54.29)	Acc@5  81.00 ( 82.62)
 * Acc@1 54.350 Acc@5 82.590
### epoch[25] execution time: 11.728209733963013
EPOCH 26
# current learning rate: 0.1
# Switched to train mode...
Epoch: [26][  0/391]	Time  0.180 ( 0.180)	Data  0.150 ( 0.150)	Loss 1.4072e+00 (1.4072e+00)	Acc@1  60.16 ( 60.16)	Acc@5  89.06 ( 89.06)
Epoch: [26][ 10/391]	Time  0.023 ( 0.040)	Data  0.001 ( 0.015)	Loss 1.3086e+00 (1.2868e+00)	Acc@1  65.62 ( 64.28)	Acc@5  90.62 ( 89.42)
Epoch: [26][ 20/391]	Time  0.020 ( 0.033)	Data  0.001 ( 0.009)	Loss 1.2432e+00 (1.3121e+00)	Acc@1  65.62 ( 63.36)	Acc@5  90.62 ( 89.21)
Epoch: [26][ 30/391]	Time  0.022 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.6543e+00 (1.3131e+00)	Acc@1  54.69 ( 63.13)	Acc@5  87.50 ( 89.52)
Epoch: [26][ 40/391]	Time  0.027 ( 0.029)	Data  0.003 ( 0.006)	Loss 1.7676e+00 (1.3614e+00)	Acc@1  54.69 ( 62.20)	Acc@5  82.81 ( 88.95)
Epoch: [26][ 50/391]	Time  0.036 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.3096e+00 (1.3680e+00)	Acc@1  61.72 ( 61.90)	Acc@5  88.28 ( 88.45)
Epoch: [26][ 60/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.1943e+00 (1.3574e+00)	Acc@1  66.41 ( 62.38)	Acc@5  90.62 ( 88.58)
Epoch: [26][ 70/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3770e+00 (1.3556e+00)	Acc@1  59.38 ( 62.31)	Acc@5  89.84 ( 88.73)
Epoch: [26][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3896e+00 (1.3538e+00)	Acc@1  63.28 ( 62.46)	Acc@5  91.41 ( 88.70)
Epoch: [26][ 90/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.3525e+00 (1.3572e+00)	Acc@1  68.75 ( 62.41)	Acc@5  85.94 ( 88.48)
Epoch: [26][100/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5078e+00 (1.3706e+00)	Acc@1  63.28 ( 61.91)	Acc@5  88.28 ( 88.28)
Epoch: [26][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3467e+00 (1.3751e+00)	Acc@1  65.62 ( 61.88)	Acc@5  88.28 ( 88.22)
Epoch: [26][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1885e+00 (1.3787e+00)	Acc@1  67.97 ( 61.82)	Acc@5  89.84 ( 88.15)
Epoch: [26][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4473e+00 (1.3787e+00)	Acc@1  60.94 ( 61.86)	Acc@5  86.72 ( 88.21)
Epoch: [26][140/391]	Time  0.035 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.4121e+00 (1.3822e+00)	Acc@1  60.16 ( 61.74)	Acc@5  89.06 ( 88.16)
Epoch: [26][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5850e+00 (1.3879e+00)	Acc@1  55.47 ( 61.63)	Acc@5  80.47 ( 88.01)
Epoch: [26][160/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4932e+00 (1.3877e+00)	Acc@1  57.03 ( 61.74)	Acc@5  89.06 ( 88.02)
Epoch: [26][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1680e+00 (1.3869e+00)	Acc@1  64.06 ( 61.75)	Acc@5  89.84 ( 88.04)
Epoch: [26][180/391]	Time  0.031 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.4746e+00 (1.3853e+00)	Acc@1  58.59 ( 61.82)	Acc@5  89.84 ( 88.06)
Epoch: [26][190/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6533e+00 (1.3828e+00)	Acc@1  56.25 ( 61.89)	Acc@5  85.16 ( 88.10)
Epoch: [26][200/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5664e+00 (1.3828e+00)	Acc@1  58.59 ( 61.90)	Acc@5  85.94 ( 88.13)
Epoch: [26][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4805e+00 (1.3851e+00)	Acc@1  56.25 ( 61.78)	Acc@5  85.94 ( 88.11)
Epoch: [26][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3750e+00 (1.3859e+00)	Acc@1  61.72 ( 61.72)	Acc@5  83.59 ( 88.08)
Epoch: [26][230/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4932e+00 (1.3894e+00)	Acc@1  59.38 ( 61.64)	Acc@5  85.94 ( 88.06)
Epoch: [26][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5234e+00 (1.3890e+00)	Acc@1  59.38 ( 61.65)	Acc@5  84.38 ( 88.01)
Epoch: [26][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4922e+00 (1.3911e+00)	Acc@1  59.38 ( 61.62)	Acc@5  85.94 ( 87.97)
Epoch: [26][260/391]	Time  0.038 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.3389e+00 (1.3921e+00)	Acc@1  67.19 ( 61.63)	Acc@5  90.62 ( 87.94)
Epoch: [26][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2100e+00 (1.3937e+00)	Acc@1  66.41 ( 61.59)	Acc@5  91.41 ( 87.89)
Epoch: [26][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2705e+00 (1.3972e+00)	Acc@1  65.62 ( 61.56)	Acc@5  89.06 ( 87.84)
Epoch: [26][290/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.7637e+00 (1.3976e+00)	Acc@1  57.81 ( 61.63)	Acc@5  83.59 ( 87.83)
Epoch: [26][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4072e+00 (1.3972e+00)	Acc@1  64.06 ( 61.69)	Acc@5  88.28 ( 87.85)
Epoch: [26][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4004e+00 (1.3979e+00)	Acc@1  61.72 ( 61.65)	Acc@5  86.72 ( 87.85)
Epoch: [26][320/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.4619e+00 (1.3975e+00)	Acc@1  59.38 ( 61.64)	Acc@5  85.16 ( 87.84)
Epoch: [26][330/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.3477e+00 (1.3960e+00)	Acc@1  63.28 ( 61.70)	Acc@5  85.16 ( 87.84)
Epoch: [26][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1836e+00 (1.3974e+00)	Acc@1  67.97 ( 61.69)	Acc@5  90.62 ( 87.81)
Epoch: [26][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5742e+00 (1.3983e+00)	Acc@1  53.12 ( 61.69)	Acc@5  89.84 ( 87.81)
Epoch: [26][360/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4062e+00 (1.3992e+00)	Acc@1  61.72 ( 61.68)	Acc@5  86.72 ( 87.79)
Epoch: [26][370/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.4609e+00 (1.4015e+00)	Acc@1  57.03 ( 61.63)	Acc@5  86.72 ( 87.77)
Epoch: [26][380/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6973e+00 (1.4028e+00)	Acc@1  55.47 ( 61.60)	Acc@5  81.25 ( 87.74)
Epoch: [26][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5771e+00 (1.4045e+00)	Acc@1  56.25 ( 61.52)	Acc@5  86.25 ( 87.69)
## e[26] optimizer.zero_grad (sum) time: 0.10388469696044922
## e[26]       loss.backward (sum) time: 2.2852463722229004
## e[26]      optimizer.step (sum) time: 0.9212019443511963
## epoch[26] training(only) time: 10.116161346435547
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.4053e+00 (1.4053e+00)	Acc@1  61.00 ( 61.00)	Acc@5  83.00 ( 83.00)
Test: [ 10/100]	Time  0.009 ( 0.026)	Loss 1.6289e+00 (1.7085e+00)	Acc@1  55.00 ( 56.45)	Acc@5  89.00 ( 81.55)
Test: [ 20/100]	Time  0.019 ( 0.020)	Loss 1.4121e+00 (1.7012e+00)	Acc@1  60.00 ( 56.48)	Acc@5  82.00 ( 82.14)
Test: [ 30/100]	Time  0.020 ( 0.018)	Loss 1.4570e+00 (1.6975e+00)	Acc@1  60.00 ( 56.00)	Acc@5  89.00 ( 82.77)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.6885e+00 (1.6884e+00)	Acc@1  55.00 ( 55.88)	Acc@5  81.00 ( 83.15)
Test: [ 50/100]	Time  0.013 ( 0.017)	Loss 1.7500e+00 (1.6960e+00)	Acc@1  61.00 ( 55.75)	Acc@5  78.00 ( 82.80)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 1.8594e+00 (1.6998e+00)	Acc@1  52.00 ( 55.56)	Acc@5  83.00 ( 82.85)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.5664e+00 (1.7061e+00)	Acc@1  57.00 ( 55.63)	Acc@5  82.00 ( 82.77)
Test: [ 80/100]	Time  0.027 ( 0.016)	Loss 1.8086e+00 (1.7079e+00)	Acc@1  52.00 ( 55.52)	Acc@5  80.00 ( 82.72)
Test: [ 90/100]	Time  0.010 ( 0.016)	Loss 1.7197e+00 (1.7021e+00)	Acc@1  53.00 ( 55.86)	Acc@5  87.00 ( 82.90)
 * Acc@1 55.950 Acc@5 82.830
### epoch[26] execution time: 11.72583556175232
EPOCH 27
# current learning rate: 0.1
# Switched to train mode...
Epoch: [27][  0/391]	Time  0.186 ( 0.186)	Data  0.161 ( 0.161)	Loss 1.1172e+00 (1.1172e+00)	Acc@1  65.62 ( 65.62)	Acc@5  91.41 ( 91.41)
Epoch: [27][ 10/391]	Time  0.020 ( 0.040)	Data  0.001 ( 0.016)	Loss 1.3975e+00 (1.2825e+00)	Acc@1  60.16 ( 64.42)	Acc@5  90.62 ( 89.20)
Epoch: [27][ 20/391]	Time  0.037 ( 0.033)	Data  0.002 ( 0.009)	Loss 1.4346e+00 (1.2908e+00)	Acc@1  58.59 ( 63.65)	Acc@5  89.06 ( 89.40)
Epoch: [27][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.2480e+00 (1.3060e+00)	Acc@1  64.06 ( 63.53)	Acc@5  92.19 ( 89.21)
Epoch: [27][ 40/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.2734e+00 (1.3103e+00)	Acc@1  59.38 ( 63.24)	Acc@5  90.62 ( 89.27)
Epoch: [27][ 50/391]	Time  0.039 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.4131e+00 (1.3101e+00)	Acc@1  62.50 ( 63.07)	Acc@5  90.62 ( 89.22)
Epoch: [27][ 60/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.1934e+00 (1.3091e+00)	Acc@1  63.28 ( 62.76)	Acc@5  89.84 ( 89.25)
Epoch: [27][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3232e+00 (1.3155e+00)	Acc@1  60.16 ( 62.74)	Acc@5  89.06 ( 89.00)
Epoch: [27][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2305e+00 (1.3100e+00)	Acc@1  65.62 ( 63.01)	Acc@5  89.06 ( 88.92)
Epoch: [27][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3945e+00 (1.3129e+00)	Acc@1  64.84 ( 63.12)	Acc@5  85.94 ( 88.78)
Epoch: [27][100/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3223e+00 (1.3108e+00)	Acc@1  61.72 ( 63.17)	Acc@5  85.94 ( 88.82)
Epoch: [27][110/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4326e+00 (1.3172e+00)	Acc@1  59.38 ( 63.12)	Acc@5  88.28 ( 88.76)
Epoch: [27][120/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.3369e+00 (1.3191e+00)	Acc@1  59.38 ( 63.17)	Acc@5  87.50 ( 88.71)
Epoch: [27][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4814e+00 (1.3196e+00)	Acc@1  66.41 ( 63.16)	Acc@5  87.50 ( 88.74)
Epoch: [27][140/391]	Time  0.020 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.3389e+00 (1.3174e+00)	Acc@1  56.25 ( 63.23)	Acc@5  89.84 ( 88.79)
Epoch: [27][150/391]	Time  0.037 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.2383e+00 (1.3235e+00)	Acc@1  62.50 ( 63.21)	Acc@5  87.50 ( 88.72)
Epoch: [27][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2119e+00 (1.3206e+00)	Acc@1  64.84 ( 63.27)	Acc@5  89.84 ( 88.82)
Epoch: [27][170/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.2168e+00 (1.3228e+00)	Acc@1  66.41 ( 63.16)	Acc@5  89.84 ( 88.78)
Epoch: [27][180/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2803e+00 (1.3189e+00)	Acc@1  63.28 ( 63.29)	Acc@5  89.84 ( 88.79)
Epoch: [27][190/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3809e+00 (1.3205e+00)	Acc@1  63.28 ( 63.27)	Acc@5  89.06 ( 88.82)
Epoch: [27][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4004e+00 (1.3231e+00)	Acc@1  60.94 ( 63.18)	Acc@5  87.50 ( 88.81)
Epoch: [27][210/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.3623e+00 (1.3238e+00)	Acc@1  58.59 ( 63.17)	Acc@5  89.06 ( 88.84)
Epoch: [27][220/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3066e+00 (1.3237e+00)	Acc@1  64.84 ( 63.13)	Acc@5  89.06 ( 88.88)
Epoch: [27][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4043e+00 (1.3272e+00)	Acc@1  61.72 ( 63.03)	Acc@5  89.06 ( 88.84)
Epoch: [27][240/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3525e+00 (1.3317e+00)	Acc@1  62.50 ( 62.92)	Acc@5  90.62 ( 88.76)
Epoch: [27][250/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.4707e+00 (1.3356e+00)	Acc@1  57.03 ( 62.85)	Acc@5  86.72 ( 88.68)
Epoch: [27][260/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7617e+00 (1.3365e+00)	Acc@1  47.66 ( 62.82)	Acc@5  85.16 ( 88.69)
Epoch: [27][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5400e+00 (1.3430e+00)	Acc@1  62.50 ( 62.64)	Acc@5  85.16 ( 88.63)
Epoch: [27][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6328e+00 (1.3436e+00)	Acc@1  57.03 ( 62.63)	Acc@5  82.03 ( 88.60)
Epoch: [27][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5576e+00 (1.3463e+00)	Acc@1  61.72 ( 62.56)	Acc@5  84.38 ( 88.55)
Epoch: [27][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3857e+00 (1.3480e+00)	Acc@1  60.16 ( 62.49)	Acc@5  88.28 ( 88.49)
Epoch: [27][310/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4990e+00 (1.3486e+00)	Acc@1  60.94 ( 62.50)	Acc@5  86.72 ( 88.47)
Epoch: [27][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5439e+00 (1.3494e+00)	Acc@1  60.16 ( 62.50)	Acc@5  83.59 ( 88.44)
Epoch: [27][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2627e+00 (1.3509e+00)	Acc@1  64.84 ( 62.42)	Acc@5  89.06 ( 88.47)
Epoch: [27][340/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4092e+00 (1.3531e+00)	Acc@1  61.72 ( 62.37)	Acc@5  85.94 ( 88.43)
Epoch: [27][350/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.3066e+00 (1.3530e+00)	Acc@1  61.72 ( 62.34)	Acc@5  89.06 ( 88.44)
Epoch: [27][360/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.3018e+00 (1.3552e+00)	Acc@1  67.97 ( 62.31)	Acc@5  86.72 ( 88.40)
Epoch: [27][370/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.5908e+00 (1.3554e+00)	Acc@1  57.03 ( 62.30)	Acc@5  86.72 ( 88.39)
Epoch: [27][380/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.3105e+00 (1.3566e+00)	Acc@1  57.81 ( 62.26)	Acc@5  88.28 ( 88.35)
Epoch: [27][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6924e+00 (1.3588e+00)	Acc@1  53.75 ( 62.23)	Acc@5  85.00 ( 88.31)
## e[27] optimizer.zero_grad (sum) time: 0.10538959503173828
## e[27]       loss.backward (sum) time: 2.252164602279663
## e[27]      optimizer.step (sum) time: 0.9128365516662598
## epoch[27] training(only) time: 10.155030250549316
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.7734e+00 (1.7734e+00)	Acc@1  57.00 ( 57.00)	Acc@5  82.00 ( 82.00)
Test: [ 10/100]	Time  0.011 ( 0.026)	Loss 1.9238e+00 (1.9520e+00)	Acc@1  55.00 ( 53.55)	Acc@5  82.00 ( 79.91)
Test: [ 20/100]	Time  0.026 ( 0.020)	Loss 2.0195e+00 (1.9419e+00)	Acc@1  48.00 ( 53.10)	Acc@5  77.00 ( 80.67)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.9414e+00 (1.9335e+00)	Acc@1  51.00 ( 52.58)	Acc@5  84.00 ( 80.84)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 2.0098e+00 (1.9093e+00)	Acc@1  54.00 ( 53.02)	Acc@5  84.00 ( 81.15)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 2.0215e+00 (1.9349e+00)	Acc@1  51.00 ( 52.51)	Acc@5  77.00 ( 80.29)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 1.8096e+00 (1.9257e+00)	Acc@1  52.00 ( 52.51)	Acc@5  80.00 ( 80.33)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.8379e+00 (1.9311e+00)	Acc@1  54.00 ( 52.54)	Acc@5  83.00 ( 80.25)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.9717e+00 (1.9357e+00)	Acc@1  53.00 ( 52.53)	Acc@5  75.00 ( 80.07)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 2.4395e+00 (1.9260e+00)	Acc@1  43.00 ( 52.38)	Acc@5  71.00 ( 80.16)
 * Acc@1 52.390 Acc@5 80.100
### epoch[27] execution time: 11.742329835891724
EPOCH 28
# current learning rate: 0.1
# Switched to train mode...
Epoch: [28][  0/391]	Time  0.167 ( 0.167)	Data  0.139 ( 0.139)	Loss 1.4531e+00 (1.4531e+00)	Acc@1  62.50 ( 62.50)	Acc@5  86.72 ( 86.72)
Epoch: [28][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.014)	Loss 1.3643e+00 (1.2349e+00)	Acc@1  58.59 ( 64.35)	Acc@5  89.84 ( 90.70)
Epoch: [28][ 20/391]	Time  0.039 ( 0.032)	Data  0.000 ( 0.009)	Loss 1.2695e+00 (1.2892e+00)	Acc@1  66.41 ( 63.50)	Acc@5  91.41 ( 89.81)
Epoch: [28][ 30/391]	Time  0.026 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.3467e+00 (1.3162e+00)	Acc@1  57.03 ( 62.78)	Acc@5  89.06 ( 89.42)
Epoch: [28][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.3291e+00 (1.3202e+00)	Acc@1  57.03 ( 62.75)	Acc@5  92.19 ( 89.37)
Epoch: [28][ 50/391]	Time  0.018 ( 0.028)	Data  0.000 ( 0.005)	Loss 1.0830e+00 (1.2996e+00)	Acc@1  72.66 ( 63.30)	Acc@5  92.97 ( 89.61)
Epoch: [28][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5078e+00 (1.3081e+00)	Acc@1  60.16 ( 63.24)	Acc@5  86.72 ( 89.49)
Epoch: [28][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3203e+00 (1.3128e+00)	Acc@1  62.50 ( 63.13)	Acc@5  91.41 ( 89.46)
Epoch: [28][ 80/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2432e+00 (1.3105e+00)	Acc@1  68.75 ( 63.49)	Acc@5  90.62 ( 89.40)
Epoch: [28][ 90/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5371e+00 (1.3155e+00)	Acc@1  60.94 ( 63.51)	Acc@5  85.94 ( 89.30)
Epoch: [28][100/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1250e+00 (1.3161e+00)	Acc@1  67.19 ( 63.61)	Acc@5  93.75 ( 89.25)
Epoch: [28][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3936e+00 (1.3126e+00)	Acc@1  60.16 ( 63.72)	Acc@5  85.94 ( 89.22)
Epoch: [28][120/391]	Time  0.026 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.3701e+00 (1.3095e+00)	Acc@1  59.38 ( 63.78)	Acc@5  88.28 ( 89.27)
Epoch: [28][130/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.3037e+00 (1.3067e+00)	Acc@1  67.19 ( 63.83)	Acc@5  89.06 ( 89.31)
Epoch: [28][140/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.1455e+00 (1.3064e+00)	Acc@1  64.84 ( 63.79)	Acc@5  92.97 ( 89.30)
Epoch: [28][150/391]	Time  0.035 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.5615e+00 (1.3017e+00)	Acc@1  55.47 ( 63.86)	Acc@5  85.16 ( 89.30)
Epoch: [28][160/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5771e+00 (1.3032e+00)	Acc@1  58.59 ( 63.83)	Acc@5  88.28 ( 89.30)
Epoch: [28][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5088e+00 (1.3063e+00)	Acc@1  55.47 ( 63.69)	Acc@5  85.94 ( 89.20)
Epoch: [28][180/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1748e+00 (1.3090e+00)	Acc@1  65.62 ( 63.67)	Acc@5  91.41 ( 89.18)
Epoch: [28][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3965e+00 (1.3127e+00)	Acc@1  61.72 ( 63.63)	Acc@5  86.72 ( 89.07)
Epoch: [28][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3135e+00 (1.3109e+00)	Acc@1  64.84 ( 63.65)	Acc@5  89.06 ( 89.08)
Epoch: [28][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3545e+00 (1.3138e+00)	Acc@1  64.06 ( 63.62)	Acc@5  85.94 ( 89.00)
Epoch: [28][220/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.1973e+00 (1.3159e+00)	Acc@1  68.75 ( 63.59)	Acc@5  92.19 ( 88.96)
Epoch: [28][230/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0928e+00 (1.3170e+00)	Acc@1  68.75 ( 63.59)	Acc@5  94.53 ( 88.95)
Epoch: [28][240/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.3857e+00 (1.3151e+00)	Acc@1  62.50 ( 63.64)	Acc@5  88.28 ( 88.99)
Epoch: [28][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.7754e-01 (1.3172e+00)	Acc@1  68.75 ( 63.57)	Acc@5  92.97 ( 88.98)
Epoch: [28][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3184e+00 (1.3188e+00)	Acc@1  63.28 ( 63.55)	Acc@5  86.72 ( 88.94)
Epoch: [28][270/391]	Time  0.025 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.4678e+00 (1.3220e+00)	Acc@1  57.03 ( 63.43)	Acc@5  86.72 ( 88.87)
Epoch: [28][280/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3711e+00 (1.3236e+00)	Acc@1  62.50 ( 63.40)	Acc@5  88.28 ( 88.85)
Epoch: [28][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2969e+00 (1.3263e+00)	Acc@1  64.84 ( 63.33)	Acc@5  89.84 ( 88.80)
Epoch: [28][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4551e+00 (1.3273e+00)	Acc@1  59.38 ( 63.31)	Acc@5  85.94 ( 88.78)
Epoch: [28][310/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3193e+00 (1.3288e+00)	Acc@1  61.72 ( 63.30)	Acc@5  86.72 ( 88.76)
Epoch: [28][320/391]	Time  0.038 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.3301e+00 (1.3319e+00)	Acc@1  60.94 ( 63.20)	Acc@5  90.62 ( 88.75)
Epoch: [28][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2070e+00 (1.3311e+00)	Acc@1  66.41 ( 63.18)	Acc@5  90.62 ( 88.77)
Epoch: [28][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4746e+00 (1.3334e+00)	Acc@1  58.59 ( 63.13)	Acc@5  85.94 ( 88.76)
Epoch: [28][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3838e+00 (1.3335e+00)	Acc@1  60.94 ( 63.15)	Acc@5  87.50 ( 88.76)
Epoch: [28][360/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4990e+00 (1.3376e+00)	Acc@1  62.50 ( 63.06)	Acc@5  83.59 ( 88.71)
Epoch: [28][370/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3311e+00 (1.3409e+00)	Acc@1  62.50 ( 63.00)	Acc@5  89.84 ( 88.63)
Epoch: [28][380/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.3018e+00 (1.3412e+00)	Acc@1  62.50 ( 62.98)	Acc@5  91.41 ( 88.62)
Epoch: [28][390/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3838e+00 (1.3413e+00)	Acc@1  61.25 ( 63.00)	Acc@5  87.50 ( 88.62)
## e[28] optimizer.zero_grad (sum) time: 0.10434484481811523
## e[28]       loss.backward (sum) time: 2.2780253887176514
## e[28]      optimizer.step (sum) time: 0.9248702526092529
## epoch[28] training(only) time: 10.070281982421875
# Switched to evaluate mode...
Test: [  0/100]	Time  0.135 ( 0.135)	Loss 1.6406e+00 (1.6406e+00)	Acc@1  58.00 ( 58.00)	Acc@5  81.00 ( 81.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.8613e+00 (1.7319e+00)	Acc@1  48.00 ( 56.27)	Acc@5  85.00 ( 81.73)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.6973e+00 (1.7223e+00)	Acc@1  62.00 ( 56.10)	Acc@5  82.00 ( 81.86)
Test: [ 30/100]	Time  0.018 ( 0.018)	Loss 1.7168e+00 (1.7147e+00)	Acc@1  49.00 ( 55.94)	Acc@5  85.00 ( 82.10)
Test: [ 40/100]	Time  0.011 ( 0.016)	Loss 1.4746e+00 (1.7035e+00)	Acc@1  57.00 ( 55.78)	Acc@5  87.00 ( 82.12)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.6562e+00 (1.7155e+00)	Acc@1  59.00 ( 56.12)	Acc@5  81.00 ( 81.63)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.7861e+00 (1.7051e+00)	Acc@1  53.00 ( 56.05)	Acc@5  86.00 ( 81.72)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 1.7197e+00 (1.7132e+00)	Acc@1  59.00 ( 55.79)	Acc@5  85.00 ( 81.89)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.6260e+00 (1.7226e+00)	Acc@1  62.00 ( 55.69)	Acc@5  80.00 ( 81.86)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.5879e+00 (1.7078e+00)	Acc@1  60.00 ( 55.97)	Acc@5  85.00 ( 82.20)
 * Acc@1 55.910 Acc@5 82.200
### epoch[28] execution time: 11.634087324142456
EPOCH 29
# current learning rate: 0.1
# Switched to train mode...
Epoch: [29][  0/391]	Time  0.176 ( 0.176)	Data  0.148 ( 0.148)	Loss 1.2314e+00 (1.2314e+00)	Acc@1  64.84 ( 64.84)	Acc@5  89.06 ( 89.06)
Epoch: [29][ 10/391]	Time  0.024 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.1982e+00 (1.3262e+00)	Acc@1  64.06 ( 63.35)	Acc@5  91.41 ( 88.00)
Epoch: [29][ 20/391]	Time  0.020 ( 0.032)	Data  0.002 ( 0.009)	Loss 1.3340e+00 (1.3179e+00)	Acc@1  66.41 ( 63.84)	Acc@5  86.72 ( 88.06)
Epoch: [29][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.0850e+00 (1.2714e+00)	Acc@1  67.19 ( 64.36)	Acc@5  91.41 ( 88.94)
Epoch: [29][ 40/391]	Time  0.021 ( 0.029)	Data  0.000 ( 0.005)	Loss 1.4072e+00 (1.2487e+00)	Acc@1  62.50 ( 65.00)	Acc@5  88.28 ( 89.23)
Epoch: [29][ 50/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.3145e+00 (1.2480e+00)	Acc@1  63.28 ( 65.13)	Acc@5  89.84 ( 89.45)
Epoch: [29][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4131e+00 (1.2553e+00)	Acc@1  65.62 ( 65.04)	Acc@5  87.50 ( 89.42)
Epoch: [29][ 70/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2744e+00 (1.2505e+00)	Acc@1  68.75 ( 65.02)	Acc@5  90.62 ( 89.48)
Epoch: [29][ 80/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2227e+00 (1.2571e+00)	Acc@1  69.53 ( 65.04)	Acc@5  87.50 ( 89.44)
Epoch: [29][ 90/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4824e+00 (1.2590e+00)	Acc@1  60.94 ( 65.05)	Acc@5  84.38 ( 89.47)
Epoch: [29][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.1309e+00 (1.2564e+00)	Acc@1  72.66 ( 65.25)	Acc@5  91.41 ( 89.50)
Epoch: [29][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3662e+00 (1.2573e+00)	Acc@1  65.62 ( 65.29)	Acc@5  89.84 ( 89.54)
Epoch: [29][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2686e+00 (1.2578e+00)	Acc@1  69.53 ( 65.32)	Acc@5  89.06 ( 89.56)
Epoch: [29][130/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.2500e+00 (1.2628e+00)	Acc@1  61.72 ( 65.19)	Acc@5  89.84 ( 89.58)
Epoch: [29][140/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.003)	Loss 9.6045e-01 (1.2631e+00)	Acc@1  71.09 ( 65.12)	Acc@5  95.31 ( 89.59)
Epoch: [29][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2783e+00 (1.2673e+00)	Acc@1  60.94 ( 65.02)	Acc@5  92.97 ( 89.58)
Epoch: [29][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4814e+00 (1.2680e+00)	Acc@1  53.91 ( 64.97)	Acc@5  87.50 ( 89.57)
Epoch: [29][170/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2891e+00 (1.2716e+00)	Acc@1  67.19 ( 64.88)	Acc@5  87.50 ( 89.51)
Epoch: [29][180/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3252e+00 (1.2719e+00)	Acc@1  62.50 ( 64.86)	Acc@5  85.94 ( 89.51)
Epoch: [29][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2744e+00 (1.2726e+00)	Acc@1  62.50 ( 64.80)	Acc@5  90.62 ( 89.52)
Epoch: [29][200/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3457e+00 (1.2807e+00)	Acc@1  63.28 ( 64.68)	Acc@5  88.28 ( 89.43)
Epoch: [29][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8203e+00 (1.2866e+00)	Acc@1  52.34 ( 64.51)	Acc@5  79.69 ( 89.33)
Epoch: [29][220/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3818e+00 (1.2881e+00)	Acc@1  60.94 ( 64.45)	Acc@5  89.84 ( 89.31)
Epoch: [29][230/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2275e+00 (1.2893e+00)	Acc@1  67.19 ( 64.38)	Acc@5  88.28 ( 89.31)
Epoch: [29][240/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.1406e+00 (1.2883e+00)	Acc@1  65.62 ( 64.37)	Acc@5  93.75 ( 89.33)
Epoch: [29][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3301e+00 (1.2889e+00)	Acc@1  64.84 ( 64.33)	Acc@5  86.72 ( 89.35)
Epoch: [29][260/391]	Time  0.034 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.6270e+00 (1.2909e+00)	Acc@1  54.69 ( 64.28)	Acc@5  87.50 ( 89.30)
Epoch: [29][270/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4805e+00 (1.2943e+00)	Acc@1  64.84 ( 64.23)	Acc@5  87.50 ( 89.24)
Epoch: [29][280/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1738e+00 (1.2973e+00)	Acc@1  69.53 ( 64.11)	Acc@5  91.41 ( 89.22)
Epoch: [29][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5850e+00 (1.3010e+00)	Acc@1  59.38 ( 64.05)	Acc@5  83.59 ( 89.15)
Epoch: [29][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2637e+00 (1.3045e+00)	Acc@1  63.28 ( 63.95)	Acc@5  90.62 ( 89.12)
Epoch: [29][310/391]	Time  0.038 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.1562e+00 (1.3048e+00)	Acc@1  67.19 ( 63.94)	Acc@5  91.41 ( 89.14)
Epoch: [29][320/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3330e+00 (1.3073e+00)	Acc@1  62.50 ( 63.87)	Acc@5  89.84 ( 89.10)
Epoch: [29][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6885e+00 (1.3083e+00)	Acc@1  57.81 ( 63.87)	Acc@5  83.59 ( 89.08)
Epoch: [29][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4316e+00 (1.3101e+00)	Acc@1  65.62 ( 63.84)	Acc@5  86.72 ( 89.06)
Epoch: [29][350/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5342e+00 (1.3127e+00)	Acc@1  55.47 ( 63.77)	Acc@5  80.47 ( 89.01)
Epoch: [29][360/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2598e+00 (1.3130e+00)	Acc@1  61.72 ( 63.77)	Acc@5  89.84 ( 88.98)
Epoch: [29][370/391]	Time  0.034 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.2197e+00 (1.3141e+00)	Acc@1  61.72 ( 63.72)	Acc@5  88.28 ( 88.98)
Epoch: [29][380/391]	Time  0.035 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.1357e+00 (1.3147e+00)	Acc@1  66.41 ( 63.67)	Acc@5  90.62 ( 89.00)
Epoch: [29][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5742e+00 (1.3159e+00)	Acc@1  63.75 ( 63.66)	Acc@5  83.75 ( 88.96)
## e[29] optimizer.zero_grad (sum) time: 0.1049051284790039
## e[29]       loss.backward (sum) time: 2.2460575103759766
## e[29]      optimizer.step (sum) time: 0.9136815071105957
## epoch[29] training(only) time: 10.104594469070435
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.6514e+00 (1.6514e+00)	Acc@1  56.00 ( 56.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.6631e+00 (1.7421e+00)	Acc@1  54.00 ( 55.73)	Acc@5  86.00 ( 82.27)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.7588e+00 (1.7647e+00)	Acc@1  60.00 ( 55.76)	Acc@5  79.00 ( 81.43)
Test: [ 30/100]	Time  0.019 ( 0.018)	Loss 1.7715e+00 (1.7642e+00)	Acc@1  55.00 ( 55.00)	Acc@5  79.00 ( 81.06)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 1.5010e+00 (1.7499e+00)	Acc@1  55.00 ( 54.90)	Acc@5  88.00 ( 81.68)
Test: [ 50/100]	Time  0.011 ( 0.016)	Loss 1.8350e+00 (1.7654e+00)	Acc@1  59.00 ( 54.86)	Acc@5  81.00 ( 81.65)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.6787e+00 (1.7597e+00)	Acc@1  57.00 ( 55.07)	Acc@5  83.00 ( 81.72)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 1.6807e+00 (1.7611e+00)	Acc@1  56.00 ( 55.30)	Acc@5  80.00 ( 81.70)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.5801e+00 (1.7725e+00)	Acc@1  62.00 ( 55.19)	Acc@5  80.00 ( 81.63)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.7969e+00 (1.7636e+00)	Acc@1  53.00 ( 55.30)	Acc@5  84.00 ( 81.75)
 * Acc@1 55.180 Acc@5 81.740
### epoch[29] execution time: 11.683283805847168
EPOCH 30
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [30][  0/391]	Time  0.185 ( 0.185)	Data  0.153 ( 0.153)	Loss 1.1719e+00 (1.1719e+00)	Acc@1  69.53 ( 69.53)	Acc@5  91.41 ( 91.41)
Epoch: [30][ 10/391]	Time  0.022 ( 0.040)	Data  0.001 ( 0.016)	Loss 1.1973e+00 (1.2211e+00)	Acc@1  63.28 ( 65.91)	Acc@5  91.41 ( 90.06)
Epoch: [30][ 20/391]	Time  0.020 ( 0.033)	Data  0.001 ( 0.009)	Loss 1.1094e+00 (1.1719e+00)	Acc@1  67.97 ( 67.00)	Acc@5  92.97 ( 90.51)
Epoch: [30][ 30/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.2324e+00 (1.1352e+00)	Acc@1  66.41 ( 68.35)	Acc@5  89.06 ( 90.83)
Epoch: [30][ 40/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.1240e+00 (1.1157e+00)	Acc@1  60.94 ( 68.88)	Acc@5  93.75 ( 91.10)
Epoch: [30][ 50/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 9.2285e-01 (1.0971e+00)	Acc@1  75.00 ( 69.59)	Acc@5  92.97 ( 91.22)
Epoch: [30][ 60/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.004)	Loss 9.7900e-01 (1.0819e+00)	Acc@1  67.97 ( 69.94)	Acc@5  96.88 ( 91.37)
Epoch: [30][ 70/391]	Time  0.033 ( 0.027)	Data  0.005 ( 0.004)	Loss 9.4482e-01 (1.0642e+00)	Acc@1  69.53 ( 70.00)	Acc@5  89.84 ( 91.73)
Epoch: [30][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0166e+00 (1.0585e+00)	Acc@1  70.31 ( 70.03)	Acc@5  96.09 ( 91.94)
Epoch: [30][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.6191e-01 (1.0466e+00)	Acc@1  74.22 ( 70.10)	Acc@5  92.19 ( 92.12)
Epoch: [30][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0010e+00 (1.0374e+00)	Acc@1  73.44 ( 70.33)	Acc@5  90.62 ( 92.30)
Epoch: [30][110/391]	Time  0.027 ( 0.027)	Data  0.003 ( 0.003)	Loss 1.0654e+00 (1.0386e+00)	Acc@1  67.19 ( 70.33)	Acc@5  92.19 ( 92.27)
Epoch: [30][120/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.2559e-01 (1.0308e+00)	Acc@1  79.69 ( 70.51)	Acc@5  96.09 ( 92.37)
Epoch: [30][130/391]	Time  0.025 ( 0.026)	Data  0.004 ( 0.003)	Loss 7.4561e-01 (1.0262e+00)	Acc@1  80.47 ( 70.63)	Acc@5  93.75 ( 92.43)
Epoch: [30][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.8926e-01 (1.0208e+00)	Acc@1  70.31 ( 70.78)	Acc@5  90.62 ( 92.46)
Epoch: [30][150/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.1113e+00 (1.0163e+00)	Acc@1  67.19 ( 70.87)	Acc@5  91.41 ( 92.54)
Epoch: [30][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0625e+00 (1.0113e+00)	Acc@1  72.66 ( 71.05)	Acc@5  90.62 ( 92.62)
Epoch: [30][170/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.7744e-01 (1.0054e+00)	Acc@1  71.88 ( 71.18)	Acc@5  92.97 ( 92.64)
Epoch: [30][180/391]	Time  0.039 ( 0.026)	Data  0.004 ( 0.003)	Loss 7.9053e-01 (1.0040e+00)	Acc@1  77.34 ( 71.17)	Acc@5  96.09 ( 92.64)
Epoch: [30][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0205e+00 (9.9697e-01)	Acc@1  71.09 ( 71.36)	Acc@5  93.75 ( 92.73)
Epoch: [30][200/391]	Time  0.038 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.0752e+00 (9.9408e-01)	Acc@1  67.97 ( 71.42)	Acc@5  93.75 ( 92.75)
Epoch: [30][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.6875e-01 (9.8941e-01)	Acc@1  76.56 ( 71.62)	Acc@5  92.97 ( 92.80)
Epoch: [30][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.6865e-01 (9.8407e-01)	Acc@1  73.44 ( 71.78)	Acc@5  92.19 ( 92.87)
Epoch: [30][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2529e-01 (9.7862e-01)	Acc@1  72.66 ( 71.93)	Acc@5  89.84 ( 92.95)
Epoch: [30][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.3838e-01 (9.7582e-01)	Acc@1  75.78 ( 71.97)	Acc@5  95.31 ( 92.98)
Epoch: [30][250/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1982e-01 (9.7270e-01)	Acc@1  78.12 ( 72.05)	Acc@5  94.53 ( 93.01)
Epoch: [30][260/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.0869e+00 (9.6852e-01)	Acc@1  64.06 ( 72.12)	Acc@5  94.53 ( 93.08)
Epoch: [30][270/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 9.8047e-01 (9.6623e-01)	Acc@1  74.22 ( 72.19)	Acc@5  91.41 ( 93.12)
Epoch: [30][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.3311e-01 (9.6204e-01)	Acc@1  71.88 ( 72.31)	Acc@5  92.97 ( 93.16)
Epoch: [30][290/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.7012e-01 (9.5913e-01)	Acc@1  77.34 ( 72.39)	Acc@5  95.31 ( 93.22)
Epoch: [30][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.5068e-01 (9.5659e-01)	Acc@1  78.91 ( 72.47)	Acc@5  88.28 ( 93.22)
Epoch: [30][310/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 8.2422e-01 (9.5472e-01)	Acc@1  74.22 ( 72.49)	Acc@5  94.53 ( 93.26)
Epoch: [30][320/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.5537e-01 (9.5074e-01)	Acc@1  76.56 ( 72.59)	Acc@5  94.53 ( 93.30)
Epoch: [30][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0303e+00 (9.4802e-01)	Acc@1  64.06 ( 72.65)	Acc@5  96.09 ( 93.33)
Epoch: [30][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.0664e-01 (9.4698e-01)	Acc@1  79.69 ( 72.70)	Acc@5  96.09 ( 93.33)
Epoch: [30][350/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.6074e-01 (9.4325e-01)	Acc@1  77.34 ( 72.80)	Acc@5  92.97 ( 93.37)
Epoch: [30][360/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.002)	Loss 7.7002e-01 (9.4266e-01)	Acc@1  74.22 ( 72.83)	Acc@5  94.53 ( 93.36)
Epoch: [30][370/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.002)	Loss 8.5742e-01 (9.4092e-01)	Acc@1  76.56 ( 72.85)	Acc@5  90.62 ( 93.38)
Epoch: [30][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.7598e-01 (9.3942e-01)	Acc@1  73.44 ( 72.90)	Acc@5  93.75 ( 93.40)
Epoch: [30][390/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.6504e-01 (9.3681e-01)	Acc@1  81.25 ( 72.93)	Acc@5  96.25 ( 93.43)
## e[30] optimizer.zero_grad (sum) time: 0.10576224327087402
## e[30]       loss.backward (sum) time: 2.29597806930542
## e[30]      optimizer.step (sum) time: 0.9284756183624268
## epoch[30] training(only) time: 10.138004779815674
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.2314e+00 (1.2314e+00)	Acc@1  69.00 ( 69.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.016 ( 0.025)	Loss 1.2812e+00 (1.3214e+00)	Acc@1  68.00 ( 66.18)	Acc@5  93.00 ( 87.27)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.3242e+00 (1.3304e+00)	Acc@1  66.00 ( 65.05)	Acc@5  88.00 ( 87.29)
Test: [ 30/100]	Time  0.018 ( 0.018)	Loss 1.1719e+00 (1.3292e+00)	Acc@1  64.00 ( 64.42)	Acc@5  90.00 ( 87.94)
Test: [ 40/100]	Time  0.018 ( 0.017)	Loss 1.2129e+00 (1.3240e+00)	Acc@1  65.00 ( 64.90)	Acc@5  91.00 ( 88.17)
Test: [ 50/100]	Time  0.018 ( 0.017)	Loss 1.3857e+00 (1.3347e+00)	Acc@1  64.00 ( 64.75)	Acc@5  85.00 ( 88.02)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 1.3584e+00 (1.3273e+00)	Acc@1  64.00 ( 64.79)	Acc@5  90.00 ( 88.08)
Test: [ 70/100]	Time  0.021 ( 0.016)	Loss 1.3535e+00 (1.3283e+00)	Acc@1  68.00 ( 64.90)	Acc@5  87.00 ( 88.06)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.3613e+00 (1.3333e+00)	Acc@1  66.00 ( 64.79)	Acc@5  81.00 ( 87.85)
Test: [ 90/100]	Time  0.017 ( 0.015)	Loss 1.4619e+00 (1.3250e+00)	Acc@1  57.00 ( 64.81)	Acc@5  91.00 ( 87.98)
 * Acc@1 64.800 Acc@5 88.020
### epoch[30] execution time: 11.740095853805542
EPOCH 31
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [31][  0/391]	Time  0.182 ( 0.182)	Data  0.153 ( 0.153)	Loss 1.0215e+00 (1.0215e+00)	Acc@1  69.53 ( 69.53)	Acc@5  92.19 ( 92.19)
Epoch: [31][ 10/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.016)	Loss 7.9346e-01 (8.2564e-01)	Acc@1  78.91 ( 76.42)	Acc@5  96.88 ( 95.10)
Epoch: [31][ 20/391]	Time  0.021 ( 0.031)	Data  0.001 ( 0.009)	Loss 8.4717e-01 (8.2329e-01)	Acc@1  75.78 ( 75.78)	Acc@5  93.75 ( 94.72)
Epoch: [31][ 30/391]	Time  0.025 ( 0.030)	Data  0.004 ( 0.007)	Loss 7.6025e-01 (8.1675e-01)	Acc@1  79.69 ( 75.73)	Acc@5  93.75 ( 94.68)
Epoch: [31][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.006)	Loss 8.0176e-01 (8.2274e-01)	Acc@1  76.56 ( 75.36)	Acc@5  95.31 ( 94.68)
Epoch: [31][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.4902e-01 (8.2688e-01)	Acc@1  72.66 ( 75.20)	Acc@5  97.66 ( 94.73)
Epoch: [31][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 8.1641e-01 (8.2633e-01)	Acc@1  76.56 ( 75.44)	Acc@5  92.97 ( 94.67)
Epoch: [31][ 70/391]	Time  0.026 ( 0.027)	Data  0.006 ( 0.004)	Loss 6.8018e-01 (8.1987e-01)	Acc@1  81.25 ( 75.77)	Acc@5  94.53 ( 94.69)
Epoch: [31][ 80/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0010e+00 (8.2921e-01)	Acc@1  73.44 ( 75.75)	Acc@5  93.75 ( 94.47)
Epoch: [31][ 90/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.6221e-01 (8.2709e-01)	Acc@1  71.09 ( 75.77)	Acc@5  96.09 ( 94.48)
Epoch: [31][100/391]	Time  0.038 ( 0.027)	Data  0.000 ( 0.003)	Loss 9.1406e-01 (8.2856e-01)	Acc@1  67.97 ( 75.66)	Acc@5  95.31 ( 94.47)
Epoch: [31][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.8818e-01 (8.2598e-01)	Acc@1  72.66 ( 75.70)	Acc@5  90.62 ( 94.51)
Epoch: [31][120/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.8242e-01 (8.2453e-01)	Acc@1  68.75 ( 75.71)	Acc@5  94.53 ( 94.55)
Epoch: [31][130/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.2285e-01 (8.2626e-01)	Acc@1  71.88 ( 75.63)	Acc@5  95.31 ( 94.63)
Epoch: [31][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.3398e-01 (8.2338e-01)	Acc@1  76.56 ( 75.69)	Acc@5  93.75 ( 94.70)
Epoch: [31][150/391]	Time  0.039 ( 0.026)	Data  0.004 ( 0.003)	Loss 7.3535e-01 (8.2291e-01)	Acc@1  79.69 ( 75.75)	Acc@5  96.09 ( 94.69)
Epoch: [31][160/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.7119e-01 (8.2592e-01)	Acc@1  73.44 ( 75.68)	Acc@5  92.97 ( 94.64)
Epoch: [31][170/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.003)	Loss 8.9990e-01 (8.2642e-01)	Acc@1  74.22 ( 75.66)	Acc@5  94.53 ( 94.67)
Epoch: [31][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8164e-01 (8.2386e-01)	Acc@1  75.00 ( 75.70)	Acc@5  96.09 ( 94.70)
Epoch: [31][190/391]	Time  0.036 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.3037e-01 (8.2553e-01)	Acc@1  81.25 ( 75.63)	Acc@5  96.88 ( 94.67)
Epoch: [31][200/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.3350e-01 (8.2779e-01)	Acc@1  75.00 ( 75.61)	Acc@5  93.75 ( 94.64)
Epoch: [31][210/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9346e-01 (8.2908e-01)	Acc@1  75.78 ( 75.57)	Acc@5  96.09 ( 94.63)
Epoch: [31][220/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.4717e-01 (8.3028e-01)	Acc@1  76.56 ( 75.54)	Acc@5  96.88 ( 94.65)
Epoch: [31][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4365e-01 (8.3137e-01)	Acc@1  78.12 ( 75.49)	Acc@5  98.44 ( 94.65)
Epoch: [31][240/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 9.1748e-01 (8.3067e-01)	Acc@1  73.44 ( 75.53)	Acc@5  93.75 ( 94.65)
Epoch: [31][250/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.1494e-01 (8.2982e-01)	Acc@1  76.56 ( 75.59)	Acc@5  96.09 ( 94.65)
Epoch: [31][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.4717e-01 (8.2646e-01)	Acc@1  71.88 ( 75.69)	Acc@5  97.66 ( 94.68)
Epoch: [31][270/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2373e-01 (8.2541e-01)	Acc@1  78.91 ( 75.73)	Acc@5  93.75 ( 94.69)
Epoch: [31][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 9.3164e-01 (8.2497e-01)	Acc@1  75.00 ( 75.80)	Acc@5  92.97 ( 94.69)
Epoch: [31][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.0322e-01 (8.2631e-01)	Acc@1  76.56 ( 75.77)	Acc@5  92.19 ( 94.67)
Epoch: [31][300/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.0176e-01 (8.2555e-01)	Acc@1  78.91 ( 75.78)	Acc@5  94.53 ( 94.69)
Epoch: [31][310/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.8320e-01 (8.2371e-01)	Acc@1  77.34 ( 75.81)	Acc@5  95.31 ( 94.68)
Epoch: [31][320/391]	Time  0.037 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.5430e-01 (8.2406e-01)	Acc@1  81.25 ( 75.80)	Acc@5  95.31 ( 94.67)
Epoch: [31][330/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.2900e-01 (8.2266e-01)	Acc@1  77.34 ( 75.80)	Acc@5  94.53 ( 94.67)
Epoch: [31][340/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.6719e-01 (8.2188e-01)	Acc@1  76.56 ( 75.83)	Acc@5  93.75 ( 94.70)
Epoch: [31][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.1377e-01 (8.2314e-01)	Acc@1  81.25 ( 75.81)	Acc@5  97.66 ( 94.68)
Epoch: [31][360/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.002)	Loss 7.0508e-01 (8.2222e-01)	Acc@1  82.03 ( 75.85)	Acc@5  97.66 ( 94.68)
Epoch: [31][370/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 9.1797e-01 (8.2294e-01)	Acc@1  74.22 ( 75.82)	Acc@5  94.53 ( 94.67)
Epoch: [31][380/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.002)	Loss 7.4805e-01 (8.2118e-01)	Acc@1  77.34 ( 75.87)	Acc@5  93.75 ( 94.70)
Epoch: [31][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.9844e-01 (8.2103e-01)	Acc@1  76.25 ( 75.89)	Acc@5  91.25 ( 94.68)
## e[31] optimizer.zero_grad (sum) time: 0.10454082489013672
## e[31]       loss.backward (sum) time: 2.2792932987213135
## e[31]      optimizer.step (sum) time: 0.9303712844848633
## epoch[31] training(only) time: 10.081280708312988
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.1572e+00 (1.1572e+00)	Acc@1  71.00 ( 71.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.012 ( 0.026)	Loss 1.2480e+00 (1.3058e+00)	Acc@1  66.00 ( 66.64)	Acc@5  93.00 ( 87.82)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.2705e+00 (1.2947e+00)	Acc@1  69.00 ( 65.86)	Acc@5  90.00 ( 88.48)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.1582e+00 (1.3052e+00)	Acc@1  65.00 ( 65.03)	Acc@5  93.00 ( 88.74)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.2559e+00 (1.3002e+00)	Acc@1  66.00 ( 65.39)	Acc@5  92.00 ( 88.98)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.3945e+00 (1.3137e+00)	Acc@1  67.00 ( 65.29)	Acc@5  85.00 ( 88.78)
Test: [ 60/100]	Time  0.024 ( 0.016)	Loss 1.3789e+00 (1.3015e+00)	Acc@1  64.00 ( 65.43)	Acc@5  89.00 ( 88.87)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 1.3135e+00 (1.3010e+00)	Acc@1  67.00 ( 65.39)	Acc@5  87.00 ( 88.76)
Test: [ 80/100]	Time  0.019 ( 0.016)	Loss 1.3564e+00 (1.3077e+00)	Acc@1  68.00 ( 65.21)	Acc@5  84.00 ( 88.58)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 1.5117e+00 (1.3003e+00)	Acc@1  58.00 ( 65.33)	Acc@5  90.00 ( 88.68)
 * Acc@1 65.360 Acc@5 88.600
### epoch[31] execution time: 11.687698602676392
EPOCH 32
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [32][  0/391]	Time  0.178 ( 0.178)	Data  0.150 ( 0.150)	Loss 7.1631e-01 (7.1631e-01)	Acc@1  77.34 ( 77.34)	Acc@5  96.09 ( 96.09)
Epoch: [32][ 10/391]	Time  0.032 ( 0.040)	Data  0.001 ( 0.016)	Loss 9.2627e-01 (7.6203e-01)	Acc@1  76.56 ( 77.34)	Acc@5  95.31 ( 94.89)
Epoch: [32][ 20/391]	Time  0.021 ( 0.033)	Data  0.001 ( 0.009)	Loss 6.8750e-01 (7.6751e-01)	Acc@1  81.25 ( 77.57)	Acc@5  95.31 ( 95.39)
Epoch: [32][ 30/391]	Time  0.020 ( 0.030)	Data  0.002 ( 0.007)	Loss 8.7646e-01 (7.5376e-01)	Acc@1  76.56 ( 78.00)	Acc@5  90.62 ( 95.39)
Epoch: [32][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 9.7168e-01 (7.5509e-01)	Acc@1  72.66 ( 77.93)	Acc@5  91.41 ( 95.45)
Epoch: [32][ 50/391]	Time  0.020 ( 0.028)	Data  0.000 ( 0.005)	Loss 8.7695e-01 (7.6974e-01)	Acc@1  73.44 ( 77.41)	Acc@5  92.97 ( 95.28)
Epoch: [32][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.1875e-01 (7.6527e-01)	Acc@1  77.34 ( 77.52)	Acc@5  95.31 ( 95.29)
Epoch: [32][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.1670e-01 (7.5794e-01)	Acc@1  80.47 ( 77.51)	Acc@5  98.44 ( 95.38)
Epoch: [32][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.2412e-01 (7.5941e-01)	Acc@1  82.03 ( 77.42)	Acc@5  96.88 ( 95.36)
Epoch: [32][ 90/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.2959e-01 (7.6612e-01)	Acc@1  75.78 ( 77.27)	Acc@5  96.88 ( 95.34)
Epoch: [32][100/391]	Time  0.041 ( 0.027)	Data  0.008 ( 0.004)	Loss 9.9707e-01 (7.7100e-01)	Acc@1  72.66 ( 77.14)	Acc@5  92.97 ( 95.24)
Epoch: [32][110/391]	Time  0.018 ( 0.027)	Data  0.001 ( 0.003)	Loss 8.0664e-01 (7.6963e-01)	Acc@1  74.22 ( 77.13)	Acc@5  96.09 ( 95.34)
Epoch: [32][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3477e-01 (7.6099e-01)	Acc@1  78.91 ( 77.29)	Acc@5  97.66 ( 95.42)
Epoch: [32][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4854e-01 (7.6393e-01)	Acc@1  77.34 ( 77.18)	Acc@5  93.75 ( 95.42)
Epoch: [32][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.6035e-01 (7.6221e-01)	Acc@1  71.09 ( 77.23)	Acc@5  94.53 ( 95.44)
Epoch: [32][150/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1006e-01 (7.6834e-01)	Acc@1  80.47 ( 77.11)	Acc@5  92.19 ( 95.30)
Epoch: [32][160/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2500e-01 (7.6594e-01)	Acc@1  81.25 ( 77.18)	Acc@5  97.66 ( 95.31)
Epoch: [32][170/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.6953e-01 (7.6524e-01)	Acc@1  78.12 ( 77.21)	Acc@5  95.31 ( 95.30)
Epoch: [32][180/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9570e-01 (7.6263e-01)	Acc@1  82.81 ( 77.33)	Acc@5  96.09 ( 95.31)
Epoch: [32][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9199e-01 (7.6191e-01)	Acc@1  74.22 ( 77.29)	Acc@5  93.75 ( 95.33)
Epoch: [32][200/391]	Time  0.037 ( 0.026)	Data  0.000 ( 0.003)	Loss 8.6328e-01 (7.6156e-01)	Acc@1  74.22 ( 77.32)	Acc@5  95.31 ( 95.33)
Epoch: [32][210/391]	Time  0.026 ( 0.026)	Data  0.005 ( 0.003)	Loss 7.5684e-01 (7.6164e-01)	Acc@1  77.34 ( 77.27)	Acc@5  94.53 ( 95.35)
Epoch: [32][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.0420e-01 (7.6022e-01)	Acc@1  71.88 ( 77.34)	Acc@5  96.09 ( 95.34)
Epoch: [32][230/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.2910e-01 (7.5855e-01)	Acc@1  74.22 ( 77.35)	Acc@5  96.09 ( 95.39)
Epoch: [32][240/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.8369e-01 (7.5857e-01)	Acc@1  75.00 ( 77.34)	Acc@5  94.53 ( 95.37)
Epoch: [32][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3535e-01 (7.5568e-01)	Acc@1  82.03 ( 77.46)	Acc@5  96.09 ( 95.40)
Epoch: [32][260/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.003)	Loss 7.5293e-01 (7.5456e-01)	Acc@1  76.56 ( 77.47)	Acc@5  95.31 ( 95.44)
Epoch: [32][270/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.9814e-01 (7.5515e-01)	Acc@1  81.25 ( 77.46)	Acc@5  99.22 ( 95.42)
Epoch: [32][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7871e-01 (7.5494e-01)	Acc@1  77.34 ( 77.49)	Acc@5  96.88 ( 95.43)
Epoch: [32][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.8955e-01 (7.5525e-01)	Acc@1  77.34 ( 77.46)	Acc@5  94.53 ( 95.41)
Epoch: [32][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.2520e-01 (7.5580e-01)	Acc@1  74.22 ( 77.47)	Acc@5  92.97 ( 95.37)
Epoch: [32][310/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.4512e-01 (7.5425e-01)	Acc@1  73.44 ( 77.51)	Acc@5  95.31 ( 95.38)
Epoch: [32][320/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.6230e-01 (7.5324e-01)	Acc@1  74.22 ( 77.56)	Acc@5  93.75 ( 95.37)
Epoch: [32][330/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0518e+00 (7.5337e-01)	Acc@1  68.75 ( 77.56)	Acc@5  90.62 ( 95.34)
Epoch: [32][340/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.2207e-01 (7.5189e-01)	Acc@1  77.34 ( 77.59)	Acc@5  95.31 ( 95.35)
Epoch: [32][350/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 7.0898e-01 (7.5176e-01)	Acc@1  80.47 ( 77.62)	Acc@5  95.31 ( 95.38)
Epoch: [32][360/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.7109e-01 (7.5333e-01)	Acc@1  77.34 ( 77.61)	Acc@5  93.75 ( 95.36)
Epoch: [32][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3828e-01 (7.5393e-01)	Acc@1  75.78 ( 77.60)	Acc@5  96.88 ( 95.35)
Epoch: [32][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.8027e-01 (7.5477e-01)	Acc@1  75.78 ( 77.60)	Acc@5  96.09 ( 95.34)
Epoch: [32][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.9932e-01 (7.5455e-01)	Acc@1  76.25 ( 77.61)	Acc@5  98.75 ( 95.34)
## e[32] optimizer.zero_grad (sum) time: 0.10408329963684082
## e[32]       loss.backward (sum) time: 2.257511854171753
## e[32]      optimizer.step (sum) time: 0.9302902221679688
## epoch[32] training(only) time: 10.088338851928711
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.1670e+00 (1.1670e+00)	Acc@1  73.00 ( 73.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.015 ( 0.026)	Loss 1.2451e+00 (1.2888e+00)	Acc@1  66.00 ( 67.18)	Acc@5  93.00 ( 87.73)
Test: [ 20/100]	Time  0.021 ( 0.020)	Loss 1.2920e+00 (1.2885e+00)	Acc@1  66.00 ( 66.00)	Acc@5  90.00 ( 88.00)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.2578e+00 (1.3051e+00)	Acc@1  61.00 ( 65.00)	Acc@5  91.00 ( 88.39)
Test: [ 40/100]	Time  0.019 ( 0.017)	Loss 1.2568e+00 (1.3024e+00)	Acc@1  66.00 ( 65.27)	Acc@5  90.00 ( 88.54)
Test: [ 50/100]	Time  0.009 ( 0.017)	Loss 1.3652e+00 (1.3200e+00)	Acc@1  67.00 ( 65.20)	Acc@5  86.00 ( 88.43)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.4062e+00 (1.3106e+00)	Acc@1  65.00 ( 65.26)	Acc@5  90.00 ( 88.56)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.3008e+00 (1.3103e+00)	Acc@1  71.00 ( 65.46)	Acc@5  89.00 ( 88.59)
Test: [ 80/100]	Time  0.030 ( 0.016)	Loss 1.4102e+00 (1.3141e+00)	Acc@1  67.00 ( 65.36)	Acc@5  83.00 ( 88.48)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.5283e+00 (1.3078e+00)	Acc@1  60.00 ( 65.44)	Acc@5  91.00 ( 88.60)
 * Acc@1 65.410 Acc@5 88.570
### epoch[32] execution time: 11.704124450683594
EPOCH 33
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [33][  0/391]	Time  0.180 ( 0.180)	Data  0.144 ( 0.144)	Loss 5.5078e-01 (5.5078e-01)	Acc@1  82.81 ( 82.81)	Acc@5  97.66 ( 97.66)
Epoch: [33][ 10/391]	Time  0.034 ( 0.039)	Data  0.001 ( 0.015)	Loss 7.0801e-01 (7.4019e-01)	Acc@1  80.47 ( 77.70)	Acc@5  94.53 ( 95.24)
Epoch: [33][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 6.7383e-01 (7.3816e-01)	Acc@1  80.47 ( 77.86)	Acc@5  97.66 ( 95.31)
Epoch: [33][ 30/391]	Time  0.025 ( 0.030)	Data  0.000 ( 0.006)	Loss 7.9150e-01 (7.3156e-01)	Acc@1  78.12 ( 78.07)	Acc@5  95.31 ( 95.41)
Epoch: [33][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 7.7539e-01 (7.3125e-01)	Acc@1  78.91 ( 78.41)	Acc@5  94.53 ( 95.48)
Epoch: [33][ 50/391]	Time  0.020 ( 0.028)	Data  0.002 ( 0.005)	Loss 8.1641e-01 (7.1628e-01)	Acc@1  76.56 ( 78.80)	Acc@5  93.75 ( 95.68)
Epoch: [33][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.0068e-01 (7.1155e-01)	Acc@1  77.34 ( 78.79)	Acc@5  97.66 ( 95.88)
Epoch: [33][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.4990e-01 (7.2032e-01)	Acc@1  82.03 ( 78.51)	Acc@5  96.09 ( 95.69)
Epoch: [33][ 80/391]	Time  0.031 ( 0.027)	Data  0.000 ( 0.004)	Loss 8.9697e-01 (7.2291e-01)	Acc@1  77.34 ( 78.59)	Acc@5  92.19 ( 95.61)
Epoch: [33][ 90/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.2256e-01 (7.1998e-01)	Acc@1  82.03 ( 78.68)	Acc@5  96.09 ( 95.60)
Epoch: [33][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 7.4707e-01 (7.2247e-01)	Acc@1  75.78 ( 78.61)	Acc@5  97.66 ( 95.66)
Epoch: [33][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5234e-01 (7.2508e-01)	Acc@1  79.69 ( 78.58)	Acc@5  96.09 ( 95.61)
Epoch: [33][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3047e-01 (7.2640e-01)	Acc@1  78.12 ( 78.52)	Acc@5  95.31 ( 95.58)
Epoch: [33][130/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.8076e-01 (7.2819e-01)	Acc@1  75.78 ( 78.46)	Acc@5  93.75 ( 95.55)
Epoch: [33][140/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1973e-01 (7.2565e-01)	Acc@1  75.00 ( 78.49)	Acc@5  97.66 ( 95.60)
Epoch: [33][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5830e-01 (7.2303e-01)	Acc@1  78.12 ( 78.51)	Acc@5  94.53 ( 95.62)
Epoch: [33][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.4072e-01 (7.2526e-01)	Acc@1  78.91 ( 78.46)	Acc@5  97.66 ( 95.60)
Epoch: [33][170/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.003)	Loss 6.9629e-01 (7.2872e-01)	Acc@1  77.34 ( 78.31)	Acc@5  95.31 ( 95.57)
Epoch: [33][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.6260e-01 (7.3024e-01)	Acc@1  83.59 ( 78.29)	Acc@5  96.88 ( 95.59)
Epoch: [33][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0312e-01 (7.2780e-01)	Acc@1  81.25 ( 78.40)	Acc@5  94.53 ( 95.61)
Epoch: [33][200/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.4814e-01 (7.3082e-01)	Acc@1  75.00 ( 78.32)	Acc@5  92.19 ( 95.50)
Epoch: [33][210/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9043e-01 (7.2996e-01)	Acc@1  75.78 ( 78.35)	Acc@5  97.66 ( 95.50)
Epoch: [33][220/391]	Time  0.039 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.8896e-01 (7.2888e-01)	Acc@1  78.12 ( 78.40)	Acc@5  97.66 ( 95.55)
Epoch: [33][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7920e-01 (7.2752e-01)	Acc@1  82.03 ( 78.42)	Acc@5  96.88 ( 95.56)
Epoch: [33][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.7686e-01 (7.2780e-01)	Acc@1  73.44 ( 78.45)	Acc@5  96.88 ( 95.56)
Epoch: [33][250/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.002)	Loss 6.8604e-01 (7.2810e-01)	Acc@1  78.91 ( 78.46)	Acc@5  95.31 ( 95.52)
Epoch: [33][260/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 6.4746e-01 (7.2884e-01)	Acc@1  83.59 ( 78.46)	Acc@5  97.66 ( 95.54)
Epoch: [33][270/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.0938e-01 (7.2698e-01)	Acc@1  79.69 ( 78.49)	Acc@5  96.09 ( 95.53)
Epoch: [33][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.4707e-01 (7.2590e-01)	Acc@1  74.22 ( 78.51)	Acc@5  95.31 ( 95.56)
Epoch: [33][290/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.002)	Loss 7.4512e-01 (7.2671e-01)	Acc@1  78.12 ( 78.46)	Acc@5  95.31 ( 95.56)
Epoch: [33][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2896e-01 (7.2402e-01)	Acc@1  89.06 ( 78.54)	Acc@5  99.22 ( 95.58)
Epoch: [33][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.9580e-01 (7.2525e-01)	Acc@1  80.47 ( 78.50)	Acc@5  94.53 ( 95.57)
Epoch: [33][320/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.8945e-01 (7.2530e-01)	Acc@1  73.44 ( 78.50)	Acc@5  96.88 ( 95.58)
Epoch: [33][330/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.002)	Loss 8.0029e-01 (7.2561e-01)	Acc@1  78.91 ( 78.51)	Acc@5  95.31 ( 95.59)
Epoch: [33][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.4619e-01 (7.2706e-01)	Acc@1  76.56 ( 78.48)	Acc@5  94.53 ( 95.59)
Epoch: [33][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.8145e-01 (7.2676e-01)	Acc@1  72.66 ( 78.49)	Acc@5  93.75 ( 95.60)
Epoch: [33][360/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.2207e-01 (7.2726e-01)	Acc@1  81.25 ( 78.50)	Acc@5  94.53 ( 95.56)
Epoch: [33][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.3574e-01 (7.2500e-01)	Acc@1  84.38 ( 78.56)	Acc@5  94.53 ( 95.58)
Epoch: [33][380/391]	Time  0.023 ( 0.026)	Data  0.005 ( 0.002)	Loss 8.9795e-01 (7.2503e-01)	Acc@1  74.22 ( 78.54)	Acc@5  93.75 ( 95.58)
Epoch: [33][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.2217e-01 (7.2600e-01)	Acc@1  82.50 ( 78.53)	Acc@5  96.25 ( 95.57)
## e[33] optimizer.zero_grad (sum) time: 0.1051638126373291
## e[33]       loss.backward (sum) time: 2.2843997478485107
## e[33]      optimizer.step (sum) time: 0.931516170501709
## epoch[33] training(only) time: 10.097136974334717
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.1367e+00 (1.1367e+00)	Acc@1  72.00 ( 72.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.013 ( 0.025)	Loss 1.2617e+00 (1.3220e+00)	Acc@1  70.00 ( 67.36)	Acc@5  93.00 ( 87.91)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.3809e+00 (1.3087e+00)	Acc@1  66.00 ( 66.62)	Acc@5  89.00 ( 88.24)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.2764e+00 (1.3257e+00)	Acc@1  63.00 ( 65.71)	Acc@5  91.00 ( 88.26)
Test: [ 40/100]	Time  0.009 ( 0.017)	Loss 1.2666e+00 (1.3206e+00)	Acc@1  65.00 ( 65.78)	Acc@5  91.00 ( 88.56)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.4189e+00 (1.3395e+00)	Acc@1  66.00 ( 65.43)	Acc@5  85.00 ( 88.43)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 1.3926e+00 (1.3255e+00)	Acc@1  64.00 ( 65.59)	Acc@5  90.00 ( 88.59)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 1.2900e+00 (1.3230e+00)	Acc@1  71.00 ( 65.80)	Acc@5  89.00 ( 88.62)
Test: [ 80/100]	Time  0.020 ( 0.016)	Loss 1.3721e+00 (1.3273e+00)	Acc@1  66.00 ( 65.68)	Acc@5  86.00 ( 88.47)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 1.6836e+00 (1.3211e+00)	Acc@1  57.00 ( 65.86)	Acc@5  90.00 ( 88.67)
 * Acc@1 65.790 Acc@5 88.650
### epoch[33] execution time: 11.724248886108398
EPOCH 34
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [34][  0/391]	Time  0.182 ( 0.182)	Data  0.156 ( 0.156)	Loss 4.9658e-01 (4.9658e-01)	Acc@1  83.59 ( 83.59)	Acc@5  99.22 ( 99.22)
Epoch: [34][ 10/391]	Time  0.023 ( 0.040)	Data  0.000 ( 0.016)	Loss 7.7686e-01 (6.7760e-01)	Acc@1  78.91 ( 79.26)	Acc@5  93.75 ( 96.52)
Epoch: [34][ 20/391]	Time  0.034 ( 0.033)	Data  0.001 ( 0.009)	Loss 6.1963e-01 (6.3996e-01)	Acc@1  82.03 ( 80.36)	Acc@5  96.88 ( 96.65)
Epoch: [34][ 30/391]	Time  0.032 ( 0.030)	Data  0.003 ( 0.007)	Loss 7.6953e-01 (6.4763e-01)	Acc@1  77.34 ( 80.22)	Acc@5  95.31 ( 96.57)
Epoch: [34][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 6.5967e-01 (6.5555e-01)	Acc@1  79.69 ( 80.09)	Acc@5  96.09 ( 96.53)
Epoch: [34][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.8691e-01 (6.6494e-01)	Acc@1  81.25 ( 79.81)	Acc@5  97.66 ( 96.46)
Epoch: [34][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 7.4658e-01 (6.7091e-01)	Acc@1  77.34 ( 79.61)	Acc@5  92.19 ( 96.31)
Epoch: [34][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.0508e+00 (6.8409e-01)	Acc@1  66.41 ( 79.21)	Acc@5  94.53 ( 96.10)
Epoch: [34][ 80/391]	Time  0.039 ( 0.027)	Data  0.004 ( 0.004)	Loss 7.9199e-01 (6.8345e-01)	Acc@1  77.34 ( 79.21)	Acc@5  94.53 ( 96.10)
Epoch: [34][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.2109e-01 (6.8331e-01)	Acc@1  82.81 ( 79.32)	Acc@5  95.31 ( 96.14)
Epoch: [34][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8975e-01 (6.8516e-01)	Acc@1  84.38 ( 79.29)	Acc@5  97.66 ( 96.16)
Epoch: [34][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0527e-01 (6.8192e-01)	Acc@1  87.50 ( 79.38)	Acc@5 100.00 ( 96.17)
Epoch: [34][120/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.2119e-01 (6.8301e-01)	Acc@1  78.91 ( 79.36)	Acc@5  94.53 ( 96.14)
Epoch: [34][130/391]	Time  0.036 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.9873e-01 (6.8546e-01)	Acc@1  80.47 ( 79.25)	Acc@5  96.09 ( 96.10)
Epoch: [34][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6465e-01 (6.8374e-01)	Acc@1  75.78 ( 79.35)	Acc@5  96.09 ( 96.10)
Epoch: [34][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1270e-01 (6.8067e-01)	Acc@1  82.81 ( 79.55)	Acc@5  99.22 ( 96.12)
Epoch: [34][160/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.4502e-01 (6.8032e-01)	Acc@1  79.69 ( 79.51)	Acc@5  97.66 ( 96.14)
Epoch: [34][170/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0654e-01 (6.8251e-01)	Acc@1  82.03 ( 79.56)	Acc@5  97.66 ( 96.12)
Epoch: [34][180/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.5098e-01 (6.8207e-01)	Acc@1  76.56 ( 79.57)	Acc@5  96.88 ( 96.12)
Epoch: [34][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2402e-01 (6.8053e-01)	Acc@1  82.03 ( 79.57)	Acc@5  93.75 ( 96.11)
Epoch: [34][200/391]	Time  0.044 ( 0.026)	Data  0.005 ( 0.003)	Loss 8.1982e-01 (6.8335e-01)	Acc@1  75.78 ( 79.47)	Acc@5  94.53 ( 96.14)
Epoch: [34][210/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8066e-01 (6.8534e-01)	Acc@1  76.56 ( 79.37)	Acc@5  97.66 ( 96.16)
Epoch: [34][220/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.7520e-01 (6.8736e-01)	Acc@1  83.59 ( 79.37)	Acc@5  97.66 ( 96.16)
Epoch: [34][230/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.3027e-01 (6.8557e-01)	Acc@1  83.59 ( 79.41)	Acc@5  97.66 ( 96.16)
Epoch: [34][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6855e-01 (6.8564e-01)	Acc@1  78.12 ( 79.42)	Acc@5  94.53 ( 96.14)
Epoch: [34][250/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2197e-01 (6.8652e-01)	Acc@1  85.94 ( 79.39)	Acc@5  99.22 ( 96.14)
Epoch: [34][260/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.002)	Loss 7.5195e-01 (6.8502e-01)	Acc@1  78.91 ( 79.45)	Acc@5  93.75 ( 96.14)
Epoch: [34][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.9141e-01 (6.8521e-01)	Acc@1  78.12 ( 79.48)	Acc@5  96.88 ( 96.12)
Epoch: [34][280/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.7578e-01 (6.8451e-01)	Acc@1  78.12 ( 79.50)	Acc@5  96.88 ( 96.12)
Epoch: [34][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.9863e-01 (6.8236e-01)	Acc@1  79.69 ( 79.57)	Acc@5  96.88 ( 96.14)
Epoch: [34][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.8076e-01 (6.8219e-01)	Acc@1  77.34 ( 79.53)	Acc@5  95.31 ( 96.11)
Epoch: [34][310/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 7.6562e-01 (6.8410e-01)	Acc@1  76.56 ( 79.47)	Acc@5 100.00 ( 96.13)
Epoch: [34][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.3691e-01 (6.8522e-01)	Acc@1  76.56 ( 79.44)	Acc@5  92.97 ( 96.09)
Epoch: [34][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.3721e-01 (6.8625e-01)	Acc@1  84.38 ( 79.43)	Acc@5  96.88 ( 96.07)
Epoch: [34][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.7490e-01 (6.8753e-01)	Acc@1  76.56 ( 79.43)	Acc@5  95.31 ( 96.06)
Epoch: [34][350/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 6.2061e-01 (6.8812e-01)	Acc@1  82.03 ( 79.40)	Acc@5  96.88 ( 96.06)
Epoch: [34][360/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.002)	Loss 8.1982e-01 (6.8934e-01)	Acc@1  74.22 ( 79.38)	Acc@5  94.53 ( 96.05)
Epoch: [34][370/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.3027e-01 (6.8888e-01)	Acc@1  82.03 ( 79.36)	Acc@5  96.88 ( 96.05)
Epoch: [34][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.1006e-01 (6.9050e-01)	Acc@1  78.12 ( 79.33)	Acc@5  92.19 ( 96.02)
Epoch: [34][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.4062e-01 (6.8972e-01)	Acc@1  80.00 ( 79.33)	Acc@5  98.75 ( 96.03)
## e[34] optimizer.zero_grad (sum) time: 0.10515904426574707
## e[34]       loss.backward (sum) time: 2.2948756217956543
## e[34]      optimizer.step (sum) time: 0.9284534454345703
## epoch[34] training(only) time: 10.10835886001587
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 1.1475e+00 (1.1475e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.016 ( 0.025)	Loss 1.2412e+00 (1.3118e+00)	Acc@1  66.00 ( 67.18)	Acc@5  94.00 ( 88.18)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.4014e+00 (1.2981e+00)	Acc@1  65.00 ( 66.71)	Acc@5  89.00 ( 88.33)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.2598e+00 (1.3171e+00)	Acc@1  65.00 ( 65.65)	Acc@5  90.00 ( 88.39)
Test: [ 40/100]	Time  0.032 ( 0.017)	Loss 1.2598e+00 (1.3040e+00)	Acc@1  63.00 ( 65.80)	Acc@5  92.00 ( 88.68)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.4131e+00 (1.3188e+00)	Acc@1  66.00 ( 65.61)	Acc@5  85.00 ( 88.47)
Test: [ 60/100]	Time  0.026 ( 0.016)	Loss 1.3789e+00 (1.3075e+00)	Acc@1  63.00 ( 65.74)	Acc@5  89.00 ( 88.57)
Test: [ 70/100]	Time  0.015 ( 0.016)	Loss 1.2705e+00 (1.3097e+00)	Acc@1  71.00 ( 65.99)	Acc@5  89.00 ( 88.54)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.4209e+00 (1.3139e+00)	Acc@1  65.00 ( 65.80)	Acc@5  83.00 ( 88.41)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.5938e+00 (1.3053e+00)	Acc@1  62.00 ( 66.00)	Acc@5  92.00 ( 88.67)
 * Acc@1 66.020 Acc@5 88.690
### epoch[34] execution time: 11.687489748001099
EPOCH 35
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [35][  0/391]	Time  0.183 ( 0.183)	Data  0.151 ( 0.151)	Loss 7.5830e-01 (7.5830e-01)	Acc@1  78.12 ( 78.12)	Acc@5  92.97 ( 92.97)
Epoch: [35][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 5.2832e-01 (6.8968e-01)	Acc@1  82.81 ( 79.40)	Acc@5  97.66 ( 95.88)
Epoch: [35][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 6.7041e-01 (7.0545e-01)	Acc@1  77.34 ( 78.61)	Acc@5  98.44 ( 95.83)
Epoch: [35][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 6.1377e-01 (6.7466e-01)	Acc@1  82.81 ( 79.79)	Acc@5  96.88 ( 96.04)
Epoch: [35][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.005)	Loss 8.0176e-01 (6.6340e-01)	Acc@1  78.12 ( 80.05)	Acc@5  93.75 ( 96.15)
Epoch: [35][ 50/391]	Time  0.027 ( 0.028)	Data  0.003 ( 0.005)	Loss 6.1572e-01 (6.6245e-01)	Acc@1  80.47 ( 79.93)	Acc@5  96.09 ( 96.25)
Epoch: [35][ 60/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.9102e-01 (6.6217e-01)	Acc@1  78.12 ( 80.11)	Acc@5  94.53 ( 96.27)
Epoch: [35][ 70/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.9492e-01 (6.5824e-01)	Acc@1  70.31 ( 80.02)	Acc@5  95.31 ( 96.40)
Epoch: [35][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.1992e-01 (6.6207e-01)	Acc@1  75.78 ( 80.08)	Acc@5  94.53 ( 96.35)
Epoch: [35][ 90/391]	Time  0.036 ( 0.027)	Data  0.004 ( 0.004)	Loss 7.4609e-01 (6.6464e-01)	Acc@1  76.56 ( 79.95)	Acc@5  92.97 ( 96.33)
Epoch: [35][100/391]	Time  0.024 ( 0.027)	Data  0.000 ( 0.003)	Loss 6.1328e-01 (6.6309e-01)	Acc@1  82.03 ( 79.95)	Acc@5  97.66 ( 96.40)
Epoch: [35][110/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8066e-01 (6.6288e-01)	Acc@1  81.25 ( 80.00)	Acc@5  96.88 ( 96.36)
Epoch: [35][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9570e-01 (6.6282e-01)	Acc@1  81.25 ( 79.98)	Acc@5  97.66 ( 96.35)
Epoch: [35][130/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8594e-01 (6.6394e-01)	Acc@1  79.69 ( 79.94)	Acc@5  97.66 ( 96.33)
Epoch: [35][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7285e-01 (6.6378e-01)	Acc@1  75.00 ( 79.91)	Acc@5  97.66 ( 96.38)
Epoch: [35][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0508e-01 (6.6318e-01)	Acc@1  81.25 ( 79.94)	Acc@5  96.88 ( 96.35)
Epoch: [35][160/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.7109e-01 (6.6327e-01)	Acc@1  76.56 ( 79.88)	Acc@5  94.53 ( 96.37)
Epoch: [35][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1133e-01 (6.6284e-01)	Acc@1  81.25 ( 79.87)	Acc@5  96.88 ( 96.35)
Epoch: [35][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1377e-01 (6.6565e-01)	Acc@1  83.59 ( 79.87)	Acc@5  96.09 ( 96.33)
Epoch: [35][190/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.003)	Loss 6.3037e-01 (6.6724e-01)	Acc@1  76.56 ( 79.74)	Acc@5  96.09 ( 96.27)
Epoch: [35][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5430e-01 (6.6728e-01)	Acc@1  77.34 ( 79.75)	Acc@5  97.66 ( 96.26)
Epoch: [35][210/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.8560e-01 (6.6815e-01)	Acc@1  87.50 ( 79.72)	Acc@5  97.66 ( 96.25)
Epoch: [35][220/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7432e-01 (6.6847e-01)	Acc@1  78.12 ( 79.71)	Acc@5  96.09 ( 96.26)
Epoch: [35][230/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7129e-01 (6.6835e-01)	Acc@1  79.69 ( 79.70)	Acc@5  99.22 ( 96.24)
Epoch: [35][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0254e-01 (6.6786e-01)	Acc@1  82.03 ( 79.70)	Acc@5  96.88 ( 96.26)
Epoch: [35][250/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.0205e-01 (6.6688e-01)	Acc@1  83.59 ( 79.74)	Acc@5  96.09 ( 96.25)
Epoch: [35][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.3965e-01 (6.6689e-01)	Acc@1  80.47 ( 79.81)	Acc@5  97.66 ( 96.25)
Epoch: [35][270/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.4658e-01 (6.6754e-01)	Acc@1  77.34 ( 79.79)	Acc@5  95.31 ( 96.25)
Epoch: [35][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.8252e-01 (6.6849e-01)	Acc@1  80.47 ( 79.77)	Acc@5  96.09 ( 96.23)
Epoch: [35][290/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.0479e-01 (6.7022e-01)	Acc@1  72.66 ( 79.74)	Acc@5  94.53 ( 96.19)
Epoch: [35][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.4199e-01 (6.6928e-01)	Acc@1  83.59 ( 79.74)	Acc@5  98.44 ( 96.20)
Epoch: [35][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.5186e-01 (6.6831e-01)	Acc@1  77.34 ( 79.75)	Acc@5  98.44 ( 96.21)
Epoch: [35][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.5957e-01 (6.6831e-01)	Acc@1  82.81 ( 79.76)	Acc@5  97.66 ( 96.21)
Epoch: [35][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.4355e-01 (6.6782e-01)	Acc@1  82.03 ( 79.82)	Acc@5  97.66 ( 96.22)
Epoch: [35][340/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.5957e-01 (6.6815e-01)	Acc@1  85.94 ( 79.82)	Acc@5  97.66 ( 96.22)
Epoch: [35][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.2021e-01 (6.6819e-01)	Acc@1  75.78 ( 79.86)	Acc@5  95.31 ( 96.22)
Epoch: [35][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.5840e-01 (6.6843e-01)	Acc@1  78.12 ( 79.86)	Acc@5  95.31 ( 96.23)
Epoch: [35][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.8994e-01 (6.6786e-01)	Acc@1  79.69 ( 79.85)	Acc@5  93.75 ( 96.22)
Epoch: [35][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.8672e-01 (6.6793e-01)	Acc@1  74.22 ( 79.83)	Acc@5  92.19 ( 96.21)
Epoch: [35][390/391]	Time  0.019 ( 0.025)	Data  0.000 ( 0.002)	Loss 7.9395e-01 (6.6771e-01)	Acc@1  71.25 ( 79.86)	Acc@5  95.00 ( 96.20)
## e[35] optimizer.zero_grad (sum) time: 0.10462546348571777
## e[35]       loss.backward (sum) time: 2.305016279220581
## e[35]      optimizer.step (sum) time: 0.9420316219329834
## epoch[35] training(only) time: 10.06507158279419
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.1396e+00 (1.1396e+00)	Acc@1  74.00 ( 74.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 1.1211e+00 (1.3006e+00)	Acc@1  67.00 ( 67.45)	Acc@5  93.00 ( 88.18)
Test: [ 20/100]	Time  0.012 ( 0.019)	Loss 1.3799e+00 (1.3021e+00)	Acc@1  69.00 ( 67.19)	Acc@5  90.00 ( 88.24)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.2471e+00 (1.3164e+00)	Acc@1  67.00 ( 66.26)	Acc@5  89.00 ( 88.52)
Test: [ 40/100]	Time  0.018 ( 0.017)	Loss 1.2236e+00 (1.3048e+00)	Acc@1  63.00 ( 66.37)	Acc@5  92.00 ( 88.85)
Test: [ 50/100]	Time  0.018 ( 0.016)	Loss 1.4082e+00 (1.3198e+00)	Acc@1  63.00 ( 66.12)	Acc@5  86.00 ( 88.84)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.4443e+00 (1.3102e+00)	Acc@1  61.00 ( 65.92)	Acc@5  90.00 ( 88.95)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.3105e+00 (1.3128e+00)	Acc@1  71.00 ( 65.97)	Acc@5  89.00 ( 88.90)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.4092e+00 (1.3171e+00)	Acc@1  66.00 ( 65.95)	Acc@5  83.00 ( 88.79)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 1.6572e+00 (1.3121e+00)	Acc@1  59.00 ( 66.09)	Acc@5  89.00 ( 88.92)
 * Acc@1 66.100 Acc@5 88.900
### epoch[35] execution time: 11.664401292800903
EPOCH 36
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [36][  0/391]	Time  0.181 ( 0.181)	Data  0.152 ( 0.152)	Loss 6.8408e-01 (6.8408e-01)	Acc@1  76.56 ( 76.56)	Acc@5  97.66 ( 97.66)
Epoch: [36][ 10/391]	Time  0.021 ( 0.039)	Data  0.002 ( 0.015)	Loss 7.2168e-01 (6.3479e-01)	Acc@1  78.12 ( 80.18)	Acc@5  96.09 ( 97.09)
Epoch: [36][ 20/391]	Time  0.021 ( 0.032)	Data  0.002 ( 0.009)	Loss 6.4014e-01 (6.3010e-01)	Acc@1  78.12 ( 80.43)	Acc@5  98.44 ( 96.84)
Epoch: [36][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.007)	Loss 7.3193e-01 (6.2053e-01)	Acc@1  76.56 ( 80.92)	Acc@5  99.22 ( 97.03)
Epoch: [36][ 40/391]	Time  0.035 ( 0.028)	Data  0.002 ( 0.006)	Loss 5.6006e-01 (6.1331e-01)	Acc@1  83.59 ( 81.36)	Acc@5  97.66 ( 96.97)
Epoch: [36][ 50/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.7764e-01 (6.2641e-01)	Acc@1  82.03 ( 81.20)	Acc@5  98.44 ( 96.75)
Epoch: [36][ 60/391]	Time  0.042 ( 0.028)	Data  0.002 ( 0.004)	Loss 5.2197e-01 (6.1924e-01)	Acc@1  83.59 ( 81.45)	Acc@5  96.88 ( 96.79)
Epoch: [36][ 70/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 8.2324e-01 (6.2581e-01)	Acc@1  77.34 ( 81.31)	Acc@5  89.84 ( 96.63)
Epoch: [36][ 80/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.6260e-01 (6.2878e-01)	Acc@1  81.25 ( 81.29)	Acc@5  96.88 ( 96.62)
Epoch: [36][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.2295e-01 (6.2789e-01)	Acc@1  82.03 ( 81.32)	Acc@5 100.00 ( 96.67)
Epoch: [36][100/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.003)	Loss 5.8496e-01 (6.2746e-01)	Acc@1  82.81 ( 81.35)	Acc@5  96.88 ( 96.61)
Epoch: [36][110/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.6455e-01 (6.3212e-01)	Acc@1  79.69 ( 81.11)	Acc@5  96.09 ( 96.59)
Epoch: [36][120/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9473e-01 (6.3589e-01)	Acc@1  81.25 ( 80.99)	Acc@5  99.22 ( 96.58)
Epoch: [36][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9424e-01 (6.3577e-01)	Acc@1  82.03 ( 80.91)	Acc@5  96.88 ( 96.54)
Epoch: [36][140/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3574e-01 (6.3724e-01)	Acc@1  81.25 ( 80.85)	Acc@5  94.53 ( 96.50)
Epoch: [36][150/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.6758e-01 (6.3906e-01)	Acc@1  81.25 ( 80.82)	Acc@5  94.53 ( 96.48)
Epoch: [36][160/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2988e-01 (6.3754e-01)	Acc@1  80.47 ( 80.88)	Acc@5  96.88 ( 96.50)
Epoch: [36][170/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.9775e-01 (6.3834e-01)	Acc@1  79.69 ( 80.85)	Acc@5  95.31 ( 96.52)
Epoch: [36][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0146e-01 (6.3854e-01)	Acc@1  86.72 ( 80.82)	Acc@5  96.09 ( 96.51)
Epoch: [36][190/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 7.7295e-01 (6.3910e-01)	Acc@1  77.34 ( 80.87)	Acc@5  95.31 ( 96.48)
Epoch: [36][200/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 8.0420e-01 (6.3938e-01)	Acc@1  75.00 ( 80.81)	Acc@5  96.88 ( 96.50)
Epoch: [36][210/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4160e-01 (6.3972e-01)	Acc@1  81.25 ( 80.88)	Acc@5  96.09 ( 96.47)
Epoch: [36][220/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4941e-01 (6.3667e-01)	Acc@1  80.47 ( 80.92)	Acc@5  94.53 ( 96.52)
Epoch: [36][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7471e-01 (6.3507e-01)	Acc@1  83.59 ( 80.95)	Acc@5  96.09 ( 96.55)
Epoch: [36][240/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.3418e-01 (6.3609e-01)	Acc@1  81.25 ( 80.87)	Acc@5  99.22 ( 96.54)
Epoch: [36][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0605e-01 (6.3648e-01)	Acc@1  78.91 ( 80.83)	Acc@5  96.09 ( 96.56)
Epoch: [36][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0742e-01 (6.3813e-01)	Acc@1  78.91 ( 80.77)	Acc@5  95.31 ( 96.53)
Epoch: [36][270/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7178e-01 (6.3878e-01)	Acc@1  78.12 ( 80.72)	Acc@5  96.88 ( 96.56)
Epoch: [36][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.6025e-01 (6.3963e-01)	Acc@1  75.00 ( 80.64)	Acc@5  96.88 ( 96.57)
Epoch: [36][290/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3193e-01 (6.4082e-01)	Acc@1  78.12 ( 80.62)	Acc@5  96.09 ( 96.55)
Epoch: [36][300/391]	Time  0.039 ( 0.026)	Data  0.006 ( 0.002)	Loss 6.7627e-01 (6.4233e-01)	Acc@1  80.47 ( 80.60)	Acc@5  94.53 ( 96.52)
Epoch: [36][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.0498e-01 (6.4250e-01)	Acc@1  82.03 ( 80.64)	Acc@5  96.09 ( 96.50)
Epoch: [36][320/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.8760e-01 (6.4462e-01)	Acc@1  76.56 ( 80.56)	Acc@5  95.31 ( 96.47)
Epoch: [36][330/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.7324e-01 (6.4600e-01)	Acc@1  83.59 ( 80.52)	Acc@5  96.88 ( 96.45)
Epoch: [36][340/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 5.9424e-01 (6.4714e-01)	Acc@1  86.72 ( 80.50)	Acc@5  96.09 ( 96.44)
Epoch: [36][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.5410e-01 (6.4882e-01)	Acc@1  71.88 ( 80.46)	Acc@5  93.75 ( 96.41)
Epoch: [36][360/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.0605e-01 (6.4814e-01)	Acc@1  74.22 ( 80.46)	Acc@5  96.88 ( 96.44)
Epoch: [36][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.4609e-01 (6.4788e-01)	Acc@1  75.78 ( 80.45)	Acc@5  92.97 ( 96.44)
Epoch: [36][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.3271e-01 (6.4522e-01)	Acc@1  84.38 ( 80.52)	Acc@5  98.44 ( 96.46)
Epoch: [36][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 8.1445e-01 (6.4679e-01)	Acc@1  77.50 ( 80.50)	Acc@5  96.25 ( 96.43)
## e[36] optimizer.zero_grad (sum) time: 0.1059560775756836
## e[36]       loss.backward (sum) time: 2.291883707046509
## e[36]      optimizer.step (sum) time: 0.9350686073303223
## epoch[36] training(only) time: 10.100967645645142
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.1602e+00 (1.1602e+00)	Acc@1  73.00 ( 73.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.011 ( 0.026)	Loss 1.1768e+00 (1.3058e+00)	Acc@1  69.00 ( 68.09)	Acc@5  93.00 ( 87.64)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.4316e+00 (1.2878e+00)	Acc@1  68.00 ( 66.90)	Acc@5  87.00 ( 88.14)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.3018e+00 (1.3106e+00)	Acc@1  66.00 ( 66.00)	Acc@5  91.00 ( 88.19)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.3115e+00 (1.3024e+00)	Acc@1  63.00 ( 66.27)	Acc@5  92.00 ( 88.80)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.4482e+00 (1.3236e+00)	Acc@1  66.00 ( 65.94)	Acc@5  85.00 ( 88.63)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.4287e+00 (1.3127e+00)	Acc@1  64.00 ( 66.11)	Acc@5  90.00 ( 88.74)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 1.2939e+00 (1.3150e+00)	Acc@1  70.00 ( 66.13)	Acc@5  88.00 ( 88.70)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.5234e+00 (1.3223e+00)	Acc@1  64.00 ( 65.98)	Acc@5  82.00 ( 88.59)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.6299e+00 (1.3143e+00)	Acc@1  62.00 ( 66.10)	Acc@5  87.00 ( 88.70)
 * Acc@1 66.130 Acc@5 88.740
### epoch[36] execution time: 11.687758922576904
EPOCH 37
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [37][  0/391]	Time  0.183 ( 0.183)	Data  0.154 ( 0.154)	Loss 5.4492e-01 (5.4492e-01)	Acc@1  82.03 ( 82.03)	Acc@5  97.66 ( 97.66)
Epoch: [37][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.016)	Loss 5.9229e-01 (6.0214e-01)	Acc@1  80.47 ( 82.03)	Acc@5  96.88 ( 96.88)
Epoch: [37][ 20/391]	Time  0.027 ( 0.033)	Data  0.000 ( 0.009)	Loss 7.8857e-01 (6.0469e-01)	Acc@1  77.34 ( 82.14)	Acc@5  93.75 ( 96.47)
Epoch: [37][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 5.7324e-01 (6.1775e-01)	Acc@1  80.47 ( 81.28)	Acc@5  98.44 ( 96.67)
Epoch: [37][ 40/391]	Time  0.029 ( 0.029)	Data  0.001 ( 0.006)	Loss 6.9922e-01 (6.1986e-01)	Acc@1  79.69 ( 81.14)	Acc@5  95.31 ( 96.65)
Epoch: [37][ 50/391]	Time  0.032 ( 0.028)	Data  0.000 ( 0.005)	Loss 7.2412e-01 (6.3135e-01)	Acc@1  76.56 ( 80.71)	Acc@5  95.31 ( 96.46)
Epoch: [37][ 60/391]	Time  0.037 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.3477e-01 (6.3086e-01)	Acc@1  78.91 ( 80.71)	Acc@5  96.88 ( 96.49)
Epoch: [37][ 70/391]	Time  0.022 ( 0.027)	Data  0.004 ( 0.004)	Loss 5.6348e-01 (6.1836e-01)	Acc@1  81.25 ( 81.15)	Acc@5  96.88 ( 96.61)
Epoch: [37][ 80/391]	Time  0.029 ( 0.027)	Data  0.000 ( 0.004)	Loss 6.4355e-01 (6.1739e-01)	Acc@1  78.91 ( 81.14)	Acc@5  95.31 ( 96.60)
Epoch: [37][ 90/391]	Time  0.035 ( 0.027)	Data  0.002 ( 0.004)	Loss 7.1191e-01 (6.1539e-01)	Acc@1  74.22 ( 81.17)	Acc@5  96.88 ( 96.62)
Epoch: [37][100/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 7.2412e-01 (6.1272e-01)	Acc@1  74.22 ( 81.20)	Acc@5  97.66 ( 96.74)
Epoch: [37][110/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.8340e-01 (6.1099e-01)	Acc@1  84.38 ( 81.29)	Acc@5  99.22 ( 96.77)
Epoch: [37][120/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.003)	Loss 7.0117e-01 (6.1177e-01)	Acc@1  78.12 ( 81.34)	Acc@5  95.31 ( 96.74)
Epoch: [37][130/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0000e-01 (6.1086e-01)	Acc@1  82.03 ( 81.42)	Acc@5  97.66 ( 96.69)
Epoch: [37][140/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.8887e-01 (6.1154e-01)	Acc@1  85.16 ( 81.41)	Acc@5  95.31 ( 96.69)
Epoch: [37][150/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5273e-01 (6.1474e-01)	Acc@1  81.25 ( 81.28)	Acc@5  97.66 ( 96.67)
Epoch: [37][160/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 5.5518e-01 (6.1419e-01)	Acc@1  82.03 ( 81.34)	Acc@5  98.44 ( 96.66)
Epoch: [37][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4639e-01 (6.1400e-01)	Acc@1  81.25 ( 81.36)	Acc@5  96.09 ( 96.66)
Epoch: [37][180/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1387e-01 (6.1420e-01)	Acc@1  79.69 ( 81.36)	Acc@5  94.53 ( 96.64)
Epoch: [37][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.2070e-01 (6.1399e-01)	Acc@1  75.00 ( 81.34)	Acc@5  96.88 ( 96.65)
Epoch: [37][200/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1299e-01 (6.1610e-01)	Acc@1  79.69 ( 81.33)	Acc@5  91.41 ( 96.61)
Epoch: [37][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4014e-01 (6.1514e-01)	Acc@1  81.25 ( 81.29)	Acc@5  95.31 ( 96.63)
Epoch: [37][220/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0488e-01 (6.1741e-01)	Acc@1  83.59 ( 81.26)	Acc@5  97.66 ( 96.59)
Epoch: [37][230/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9473e-01 (6.1431e-01)	Acc@1  82.03 ( 81.35)	Acc@5  97.66 ( 96.63)
Epoch: [37][240/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.9365e-01 (6.1330e-01)	Acc@1  82.81 ( 81.38)	Acc@5  96.09 ( 96.65)
Epoch: [37][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1025e-01 (6.1431e-01)	Acc@1  84.38 ( 81.36)	Acc@5  97.66 ( 96.61)
Epoch: [37][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 8.1738e-01 (6.1477e-01)	Acc@1  73.44 ( 81.37)	Acc@5  90.62 ( 96.58)
Epoch: [37][270/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.7871e-01 (6.1556e-01)	Acc@1  78.12 ( 81.35)	Acc@5  97.66 ( 96.56)
Epoch: [37][280/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.9390e-01 (6.1681e-01)	Acc@1  81.25 ( 81.32)	Acc@5  99.22 ( 96.55)
Epoch: [37][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.7373e-01 (6.1656e-01)	Acc@1  82.81 ( 81.28)	Acc@5  96.88 ( 96.58)
Epoch: [37][300/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 6.2598e-01 (6.1681e-01)	Acc@1  82.03 ( 81.26)	Acc@5  96.88 ( 96.59)
Epoch: [37][310/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.002)	Loss 6.3428e-01 (6.1595e-01)	Acc@1  78.12 ( 81.27)	Acc@5  93.75 ( 96.58)
Epoch: [37][320/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.9980e-01 (6.1736e-01)	Acc@1  80.47 ( 81.23)	Acc@5  95.31 ( 96.57)
Epoch: [37][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.9766e-01 (6.1998e-01)	Acc@1  78.91 ( 81.17)	Acc@5  97.66 ( 96.55)
Epoch: [37][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.0840e-01 (6.2290e-01)	Acc@1  80.47 ( 81.09)	Acc@5  96.09 ( 96.53)
Epoch: [37][350/391]	Time  0.041 ( 0.026)	Data  0.004 ( 0.002)	Loss 7.5293e-01 (6.2240e-01)	Acc@1  75.78 ( 81.11)	Acc@5  96.09 ( 96.54)
Epoch: [37][360/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.0244e-01 (6.2247e-01)	Acc@1  85.16 ( 81.12)	Acc@5  97.66 ( 96.55)
Epoch: [37][370/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.9766e-01 (6.2163e-01)	Acc@1  83.59 ( 81.15)	Acc@5  94.53 ( 96.56)
Epoch: [37][380/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.9102e-01 (6.2185e-01)	Acc@1  78.91 ( 81.14)	Acc@5  95.31 ( 96.56)
Epoch: [37][390/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.002)	Loss 9.5703e-01 (6.2252e-01)	Acc@1  71.25 ( 81.10)	Acc@5  87.50 ( 96.55)
## e[37] optimizer.zero_grad (sum) time: 0.1056675910949707
## e[37]       loss.backward (sum) time: 2.262579917907715
## e[37]      optimizer.step (sum) time: 0.9152798652648926
## epoch[37] training(only) time: 10.137991189956665
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.1299e+00 (1.1299e+00)	Acc@1  73.00 ( 73.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.009 ( 0.023)	Loss 1.2188e+00 (1.3112e+00)	Acc@1  67.00 ( 68.09)	Acc@5  93.00 ( 88.27)
Test: [ 20/100]	Time  0.020 ( 0.020)	Loss 1.3291e+00 (1.3036e+00)	Acc@1  68.00 ( 67.14)	Acc@5  89.00 ( 88.62)
Test: [ 30/100]	Time  0.020 ( 0.018)	Loss 1.2734e+00 (1.3234e+00)	Acc@1  64.00 ( 66.19)	Acc@5  89.00 ( 88.55)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 1.3057e+00 (1.3196e+00)	Acc@1  64.00 ( 66.24)	Acc@5  91.00 ( 88.68)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.4443e+00 (1.3354e+00)	Acc@1  68.00 ( 66.24)	Acc@5  85.00 ( 88.65)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.4717e+00 (1.3216e+00)	Acc@1  62.00 ( 66.44)	Acc@5  91.00 ( 88.87)
Test: [ 70/100]	Time  0.022 ( 0.016)	Loss 1.3135e+00 (1.3232e+00)	Acc@1  71.00 ( 66.51)	Acc@5  87.00 ( 88.85)
Test: [ 80/100]	Time  0.015 ( 0.016)	Loss 1.4531e+00 (1.3267e+00)	Acc@1  66.00 ( 66.43)	Acc@5  86.00 ( 88.84)
Test: [ 90/100]	Time  0.011 ( 0.015)	Loss 1.6660e+00 (1.3205e+00)	Acc@1  62.00 ( 66.58)	Acc@5  91.00 ( 88.98)
 * Acc@1 66.650 Acc@5 88.930
### epoch[37] execution time: 11.741248607635498
EPOCH 38
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [38][  0/391]	Time  0.172 ( 0.172)	Data  0.147 ( 0.147)	Loss 5.6982e-01 (5.6982e-01)	Acc@1  81.25 ( 81.25)	Acc@5  96.88 ( 96.88)
Epoch: [38][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.015)	Loss 5.7275e-01 (5.8802e-01)	Acc@1  82.81 ( 81.96)	Acc@5  96.09 ( 97.09)
Epoch: [38][ 20/391]	Time  0.043 ( 0.032)	Data  0.001 ( 0.009)	Loss 6.7969e-01 (5.8638e-01)	Acc@1  82.03 ( 81.99)	Acc@5  98.44 ( 97.21)
Epoch: [38][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 6.8408e-01 (5.8914e-01)	Acc@1  78.12 ( 82.03)	Acc@5  96.88 ( 97.08)
Epoch: [38][ 40/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.4492e-01 (5.9479e-01)	Acc@1  83.59 ( 82.18)	Acc@5  97.66 ( 96.76)
Epoch: [38][ 50/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.3428e-01 (5.8521e-01)	Acc@1  80.47 ( 82.35)	Acc@5  98.44 ( 96.88)
Epoch: [38][ 60/391]	Time  0.039 ( 0.028)	Data  0.005 ( 0.004)	Loss 5.5713e-01 (5.8773e-01)	Acc@1  85.94 ( 82.43)	Acc@5  96.09 ( 96.90)
Epoch: [38][ 70/391]	Time  0.028 ( 0.027)	Data  0.000 ( 0.004)	Loss 7.0215e-01 (5.9418e-01)	Acc@1  78.91 ( 82.21)	Acc@5  95.31 ( 96.83)
Epoch: [38][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.4443e-01 (6.0161e-01)	Acc@1  84.38 ( 82.03)	Acc@5  95.31 ( 96.76)
Epoch: [38][ 90/391]	Time  0.024 ( 0.026)	Data  0.006 ( 0.004)	Loss 5.9863e-01 (5.9862e-01)	Acc@1  82.81 ( 82.02)	Acc@5  98.44 ( 96.85)
Epoch: [38][100/391]	Time  0.025 ( 0.027)	Data  0.005 ( 0.003)	Loss 7.2461e-01 (6.0143e-01)	Acc@1  79.69 ( 81.94)	Acc@5  95.31 ( 96.87)
Epoch: [38][110/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.2500e-01 (5.9957e-01)	Acc@1  81.25 ( 82.02)	Acc@5  96.09 ( 96.85)
Epoch: [38][120/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.003)	Loss 7.6318e-01 (5.9590e-01)	Acc@1  79.69 ( 82.14)	Acc@5  93.75 ( 96.84)
Epoch: [38][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.3467e-01 (5.9567e-01)	Acc@1  85.94 ( 82.10)	Acc@5  97.66 ( 96.88)
Epoch: [38][140/391]	Time  0.036 ( 0.026)	Data  0.013 ( 0.003)	Loss 6.2646e-01 (5.9917e-01)	Acc@1  81.25 ( 81.98)	Acc@5  97.66 ( 96.88)
Epoch: [38][150/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9375e-01 (6.0007e-01)	Acc@1  81.25 ( 81.96)	Acc@5  97.66 ( 96.85)
Epoch: [38][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.2637e-01 (5.9666e-01)	Acc@1  83.59 ( 82.02)	Acc@5  98.44 ( 96.88)
Epoch: [38][170/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1953e-01 (5.9621e-01)	Acc@1  80.47 ( 82.06)	Acc@5  98.44 ( 96.88)
Epoch: [38][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0049e-01 (5.9696e-01)	Acc@1  84.38 ( 82.06)	Acc@5 100.00 ( 96.84)
Epoch: [38][190/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8413e-01 (5.9629e-01)	Acc@1  85.16 ( 82.13)	Acc@5  98.44 ( 96.85)
Epoch: [38][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8789e-01 (5.9753e-01)	Acc@1  80.47 ( 82.10)	Acc@5  99.22 ( 96.81)
Epoch: [38][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5127e-01 (5.9670e-01)	Acc@1  83.59 ( 82.11)	Acc@5  97.66 ( 96.81)
Epoch: [38][220/391]	Time  0.030 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.5137e-01 (5.9686e-01)	Acc@1  82.03 ( 82.11)	Acc@5  96.09 ( 96.79)
Epoch: [38][230/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.8838e-01 (5.9875e-01)	Acc@1  84.38 ( 82.07)	Acc@5  94.53 ( 96.75)
Epoch: [38][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5186e-01 (5.9841e-01)	Acc@1  78.91 ( 82.07)	Acc@5  98.44 ( 96.76)
Epoch: [38][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7168e-01 (5.9810e-01)	Acc@1  81.25 ( 82.06)	Acc@5  98.44 ( 96.79)
Epoch: [38][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9131e-01 (5.9857e-01)	Acc@1  79.69 ( 82.02)	Acc@5  98.44 ( 96.78)
Epoch: [38][270/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0156e-01 (6.0007e-01)	Acc@1  81.25 ( 81.96)	Acc@5  95.31 ( 96.75)
Epoch: [38][280/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5283e-01 (6.0014e-01)	Acc@1  78.12 ( 81.93)	Acc@5  98.44 ( 96.76)
Epoch: [38][290/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.9873e-01 (6.0010e-01)	Acc@1  79.69 ( 81.88)	Acc@5  96.88 ( 96.78)
Epoch: [38][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.4492e-01 (6.0178e-01)	Acc@1  82.81 ( 81.81)	Acc@5  96.09 ( 96.76)
Epoch: [38][310/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.6553e-01 (6.0159e-01)	Acc@1  81.25 ( 81.80)	Acc@5  97.66 ( 96.76)
Epoch: [38][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.6836e-01 (6.0052e-01)	Acc@1  84.38 ( 81.84)	Acc@5  96.09 ( 96.78)
Epoch: [38][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.3965e-01 (6.0074e-01)	Acc@1  79.69 ( 81.81)	Acc@5  96.09 ( 96.78)
Epoch: [38][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.6104e-01 (6.0107e-01)	Acc@1  82.81 ( 81.76)	Acc@5  97.66 ( 96.80)
Epoch: [38][350/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 5.5176e-01 (6.0035e-01)	Acc@1  82.81 ( 81.77)	Acc@5  96.88 ( 96.82)
Epoch: [38][360/391]	Time  0.034 ( 0.026)	Data  0.004 ( 0.002)	Loss 4.6509e-01 (5.9946e-01)	Acc@1  87.50 ( 81.78)	Acc@5  99.22 ( 96.85)
Epoch: [38][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.2803e-01 (5.9932e-01)	Acc@1  82.03 ( 81.81)	Acc@5  94.53 ( 96.84)
Epoch: [38][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.3525e-01 (5.9854e-01)	Acc@1  81.25 ( 81.83)	Acc@5  95.31 ( 96.83)
Epoch: [38][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3633e-01 (5.9938e-01)	Acc@1  80.00 ( 81.80)	Acc@5  95.00 ( 96.83)
## e[38] optimizer.zero_grad (sum) time: 0.10543465614318848
## e[38]       loss.backward (sum) time: 2.28505277633667
## e[38]      optimizer.step (sum) time: 0.9384372234344482
## epoch[38] training(only) time: 10.09816312789917
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.0928e+00 (1.0928e+00)	Acc@1  72.00 ( 72.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.016 ( 0.025)	Loss 1.1494e+00 (1.3231e+00)	Acc@1  68.00 ( 67.91)	Acc@5  93.00 ( 88.27)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.4150e+00 (1.3094e+00)	Acc@1  68.00 ( 67.14)	Acc@5  87.00 ( 88.67)
Test: [ 30/100]	Time  0.021 ( 0.018)	Loss 1.2354e+00 (1.3244e+00)	Acc@1  64.00 ( 66.16)	Acc@5  90.00 ( 88.68)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.2500e+00 (1.3208e+00)	Acc@1  67.00 ( 66.29)	Acc@5  92.00 ( 89.05)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.4316e+00 (1.3383e+00)	Acc@1  68.00 ( 66.12)	Acc@5  86.00 ( 88.86)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.4121e+00 (1.3279e+00)	Acc@1  64.00 ( 66.20)	Acc@5  89.00 ( 89.07)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 1.3838e+00 (1.3290e+00)	Acc@1  69.00 ( 66.21)	Acc@5  86.00 ( 89.04)
Test: [ 80/100]	Time  0.009 ( 0.015)	Loss 1.4785e+00 (1.3387e+00)	Acc@1  67.00 ( 66.11)	Acc@5  83.00 ( 88.93)
Test: [ 90/100]	Time  0.027 ( 0.015)	Loss 1.6553e+00 (1.3333e+00)	Acc@1  57.00 ( 66.12)	Acc@5  89.00 ( 89.01)
 * Acc@1 66.180 Acc@5 88.890
### epoch[38] execution time: 11.707117795944214
EPOCH 39
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [39][  0/391]	Time  0.173 ( 0.173)	Data  0.145 ( 0.145)	Loss 6.6650e-01 (6.6650e-01)	Acc@1  80.47 ( 80.47)	Acc@5  97.66 ( 97.66)
Epoch: [39][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.015)	Loss 6.2256e-01 (5.7677e-01)	Acc@1  82.03 ( 82.74)	Acc@5  96.88 ( 96.88)
Epoch: [39][ 20/391]	Time  0.029 ( 0.032)	Data  0.001 ( 0.009)	Loss 6.1865e-01 (5.8731e-01)	Acc@1  84.38 ( 82.44)	Acc@5  92.97 ( 96.73)
Epoch: [39][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.006)	Loss 6.2207e-01 (5.8476e-01)	Acc@1  84.38 ( 82.01)	Acc@5  94.53 ( 96.85)
Epoch: [39][ 40/391]	Time  0.024 ( 0.028)	Data  0.003 ( 0.005)	Loss 5.0391e-01 (5.6627e-01)	Acc@1  83.59 ( 82.60)	Acc@5  97.66 ( 97.12)
Epoch: [39][ 50/391]	Time  0.022 ( 0.028)	Data  0.004 ( 0.005)	Loss 5.8936e-01 (5.6189e-01)	Acc@1  82.03 ( 82.83)	Acc@5  98.44 ( 97.15)
Epoch: [39][ 60/391]	Time  0.036 ( 0.027)	Data  0.000 ( 0.004)	Loss 5.0439e-01 (5.6092e-01)	Acc@1  82.03 ( 82.98)	Acc@5  97.66 ( 97.07)
Epoch: [39][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.5225e-01 (5.6494e-01)	Acc@1  85.16 ( 82.86)	Acc@5  98.44 ( 96.97)
Epoch: [39][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.6553e-01 (5.7221e-01)	Acc@1  83.59 ( 82.56)	Acc@5  96.09 ( 96.88)
Epoch: [39][ 90/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.2163e-01 (5.7297e-01)	Acc@1  84.38 ( 82.49)	Acc@5 100.00 ( 96.91)
Epoch: [39][100/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.003)	Loss 6.5820e-01 (5.7136e-01)	Acc@1  79.69 ( 82.61)	Acc@5  92.97 ( 96.88)
Epoch: [39][110/391]	Time  0.036 ( 0.026)	Data  0.006 ( 0.003)	Loss 4.5093e-01 (5.7069e-01)	Acc@1  87.50 ( 82.65)	Acc@5  98.44 ( 96.83)
Epoch: [39][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1318e-01 (5.6814e-01)	Acc@1  86.72 ( 82.79)	Acc@5  96.09 ( 96.84)
Epoch: [39][130/391]	Time  0.026 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.8496e-01 (5.6701e-01)	Acc@1  80.47 ( 82.77)	Acc@5  98.44 ( 96.88)
Epoch: [39][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7520e-01 (5.6751e-01)	Acc@1  86.72 ( 82.76)	Acc@5  97.66 ( 96.88)
Epoch: [39][150/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.1279e-01 (5.7042e-01)	Acc@1  76.56 ( 82.58)	Acc@5  98.44 ( 96.90)
Epoch: [39][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0654e-01 (5.7060e-01)	Acc@1  78.12 ( 82.61)	Acc@5  93.75 ( 96.92)
Epoch: [39][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5381e-01 (5.7149e-01)	Acc@1  78.91 ( 82.54)	Acc@5  97.66 ( 96.93)
Epoch: [39][180/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1128e-01 (5.7078e-01)	Acc@1  89.06 ( 82.54)	Acc@5  99.22 ( 96.96)
Epoch: [39][190/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.5078e-01 (5.7164e-01)	Acc@1  83.59 ( 82.53)	Acc@5  96.88 ( 96.95)
Epoch: [39][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4160e-01 (5.7491e-01)	Acc@1  83.59 ( 82.44)	Acc@5  96.09 ( 96.94)
Epoch: [39][210/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 6.0889e-01 (5.7411e-01)	Acc@1  81.25 ( 82.43)	Acc@5  97.66 ( 96.95)
Epoch: [39][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5273e-01 (5.7506e-01)	Acc@1  86.72 ( 82.40)	Acc@5  96.88 ( 96.95)
Epoch: [39][230/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.8994e-01 (5.7469e-01)	Acc@1  81.25 ( 82.37)	Acc@5  96.09 ( 96.98)
Epoch: [39][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6934e-01 (5.7452e-01)	Acc@1  83.59 ( 82.36)	Acc@5  96.88 ( 96.99)
Epoch: [39][250/391]	Time  0.048 ( 0.026)	Data  0.006 ( 0.003)	Loss 5.7666e-01 (5.7580e-01)	Acc@1  85.16 ( 82.36)	Acc@5  97.66 ( 96.97)
Epoch: [39][260/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.0381e-01 (5.7622e-01)	Acc@1  86.72 ( 82.37)	Acc@5 100.00 ( 96.97)
Epoch: [39][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0059e-01 (5.7680e-01)	Acc@1  83.59 ( 82.40)	Acc@5  97.66 ( 96.99)
Epoch: [39][280/391]	Time  0.037 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.3091e-01 (5.7694e-01)	Acc@1  86.72 ( 82.41)	Acc@5  98.44 ( 96.96)
Epoch: [39][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5420e-01 (5.7597e-01)	Acc@1  85.16 ( 82.47)	Acc@5  96.88 ( 96.96)
Epoch: [39][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9023e-01 (5.7518e-01)	Acc@1  84.38 ( 82.48)	Acc@5  98.44 ( 96.99)
Epoch: [39][310/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.4844e-01 (5.7605e-01)	Acc@1  77.34 ( 82.45)	Acc@5  96.88 ( 97.00)
Epoch: [39][320/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.3379e-01 (5.7594e-01)	Acc@1  78.91 ( 82.41)	Acc@5  95.31 ( 97.00)
Epoch: [39][330/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6445e-01 (5.7763e-01)	Acc@1  82.81 ( 82.35)	Acc@5  96.88 ( 97.00)
Epoch: [39][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1328e-01 (5.7697e-01)	Acc@1  80.47 ( 82.36)	Acc@5  97.66 ( 96.99)
Epoch: [39][350/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.2588e-01 (5.7622e-01)	Acc@1  82.03 ( 82.39)	Acc@5  96.09 ( 96.98)
Epoch: [39][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7715e-01 (5.7743e-01)	Acc@1  82.81 ( 82.35)	Acc@5  95.31 ( 96.97)
Epoch: [39][370/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.8164e-01 (5.7768e-01)	Acc@1  76.56 ( 82.36)	Acc@5  94.53 ( 96.97)
Epoch: [39][380/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.2344e-01 (5.7733e-01)	Acc@1  83.59 ( 82.35)	Acc@5  98.44 ( 96.98)
Epoch: [39][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9102e-01 (5.7836e-01)	Acc@1  78.75 ( 82.33)	Acc@5  96.25 ( 96.98)
## e[39] optimizer.zero_grad (sum) time: 0.10587596893310547
## e[39]       loss.backward (sum) time: 2.2847089767456055
## e[39]      optimizer.step (sum) time: 0.9342484474182129
## epoch[39] training(only) time: 10.075944900512695
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.1035e+00 (1.1035e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.009 ( 0.027)	Loss 1.2236e+00 (1.3141e+00)	Acc@1  70.00 ( 68.73)	Acc@5  93.00 ( 88.18)
Test: [ 20/100]	Time  0.029 ( 0.021)	Loss 1.4072e+00 (1.3004e+00)	Acc@1  66.00 ( 67.95)	Acc@5  89.00 ( 88.43)
Test: [ 30/100]	Time  0.010 ( 0.019)	Loss 1.2627e+00 (1.3156e+00)	Acc@1  64.00 ( 66.77)	Acc@5  91.00 ( 88.65)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.3193e+00 (1.3166e+00)	Acc@1  66.00 ( 67.12)	Acc@5  89.00 ( 88.90)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 1.4678e+00 (1.3366e+00)	Acc@1  67.00 ( 66.69)	Acc@5  84.00 ( 88.65)
Test: [ 60/100]	Time  0.017 ( 0.016)	Loss 1.5234e+00 (1.3276e+00)	Acc@1  62.00 ( 66.62)	Acc@5  89.00 ( 88.89)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 1.3594e+00 (1.3289e+00)	Acc@1  66.00 ( 66.69)	Acc@5  85.00 ( 88.86)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 1.5361e+00 (1.3363e+00)	Acc@1  65.00 ( 66.59)	Acc@5  87.00 ( 88.88)
Test: [ 90/100]	Time  0.016 ( 0.016)	Loss 1.7041e+00 (1.3311e+00)	Acc@1  59.00 ( 66.63)	Acc@5  89.00 ( 89.02)
 * Acc@1 66.660 Acc@5 88.960
### epoch[39] execution time: 11.699354887008667
EPOCH 40
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [40][  0/391]	Time  0.170 ( 0.170)	Data  0.141 ( 0.141)	Loss 5.5957e-01 (5.5957e-01)	Acc@1  82.81 ( 82.81)	Acc@5  95.31 ( 95.31)
Epoch: [40][ 10/391]	Time  0.023 ( 0.038)	Data  0.001 ( 0.014)	Loss 5.1123e-01 (5.4117e-01)	Acc@1  85.94 ( 83.38)	Acc@5  97.66 ( 97.09)
Epoch: [40][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 4.4214e-01 (5.3338e-01)	Acc@1  86.72 ( 83.48)	Acc@5  96.88 ( 97.25)
Epoch: [40][ 30/391]	Time  0.024 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.8770e-01 (5.2885e-01)	Acc@1  87.50 ( 83.59)	Acc@5  98.44 ( 97.30)
Epoch: [40][ 40/391]	Time  0.023 ( 0.029)	Data  0.002 ( 0.005)	Loss 5.1807e-01 (5.4455e-01)	Acc@1  83.59 ( 83.40)	Acc@5  97.66 ( 97.24)
Epoch: [40][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.6582e-01 (5.4526e-01)	Acc@1  86.72 ( 83.23)	Acc@5  96.88 ( 97.24)
Epoch: [40][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 7.3633e-01 (5.4756e-01)	Acc@1  80.47 ( 83.24)	Acc@5  93.75 ( 97.20)
Epoch: [40][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.3564e-01 (5.4320e-01)	Acc@1  86.72 ( 83.41)	Acc@5  95.31 ( 97.13)
Epoch: [40][ 80/391]	Time  0.026 ( 0.027)	Data  0.004 ( 0.004)	Loss 5.0586e-01 (5.4171e-01)	Acc@1  82.81 ( 83.34)	Acc@5  96.88 ( 97.15)
Epoch: [40][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.3613e-01 (5.4247e-01)	Acc@1  84.38 ( 83.33)	Acc@5  96.88 ( 97.18)
Epoch: [40][100/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.3857e-01 (5.4068e-01)	Acc@1  78.91 ( 83.43)	Acc@5  97.66 ( 97.23)
Epoch: [40][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.3857e-01 (5.4440e-01)	Acc@1  82.81 ( 83.31)	Acc@5  97.66 ( 97.21)
Epoch: [40][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0557e-01 (5.4733e-01)	Acc@1  80.47 ( 83.21)	Acc@5  96.09 ( 97.24)
Epoch: [40][130/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5337e-01 (5.4767e-01)	Acc@1  86.72 ( 83.13)	Acc@5  98.44 ( 97.24)
Epoch: [40][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5566e-01 (5.5040e-01)	Acc@1  82.81 ( 83.11)	Acc@5  98.44 ( 97.25)
Epoch: [40][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4800e-01 (5.4950e-01)	Acc@1  87.50 ( 83.12)	Acc@5  98.44 ( 97.27)
Epoch: [40][160/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.6699e-01 (5.5257e-01)	Acc@1  82.03 ( 83.08)	Acc@5  94.53 ( 97.23)
Epoch: [40][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7676e-01 (5.5232e-01)	Acc@1  82.03 ( 83.09)	Acc@5  96.09 ( 97.25)
Epoch: [40][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2896e-01 (5.5320e-01)	Acc@1  85.16 ( 83.10)	Acc@5 100.00 ( 97.28)
Epoch: [40][190/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0361e-01 (5.5390e-01)	Acc@1  81.25 ( 83.12)	Acc@5  95.31 ( 97.25)
Epoch: [40][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9561e-01 (5.5125e-01)	Acc@1  85.16 ( 83.20)	Acc@5  96.88 ( 97.25)
Epoch: [40][210/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8364e-01 (5.5069e-01)	Acc@1  84.38 ( 83.21)	Acc@5  96.88 ( 97.25)
Epoch: [40][220/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7725e-01 (5.5137e-01)	Acc@1  78.12 ( 83.13)	Acc@5  94.53 ( 97.25)
Epoch: [40][230/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.3369e-01 (5.5297e-01)	Acc@1  84.38 ( 83.09)	Acc@5  97.66 ( 97.24)
Epoch: [40][240/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.1465e-01 (5.5261e-01)	Acc@1  86.72 ( 83.11)	Acc@5  96.09 ( 97.25)
Epoch: [40][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.8213e-01 (5.5276e-01)	Acc@1  81.25 ( 83.10)	Acc@5  96.88 ( 97.24)
Epoch: [40][260/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.6362e-01 (5.5225e-01)	Acc@1  85.16 ( 83.10)	Acc@5  99.22 ( 97.27)
Epoch: [40][270/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.5166e-01 (5.5207e-01)	Acc@1  88.28 ( 83.14)	Acc@5  98.44 ( 97.27)
Epoch: [40][280/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.002)	Loss 5.3613e-01 (5.5404e-01)	Acc@1  82.81 ( 83.10)	Acc@5  98.44 ( 97.25)
Epoch: [40][290/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.002)	Loss 5.0684e-01 (5.5507e-01)	Acc@1  84.38 ( 83.06)	Acc@5  99.22 ( 97.24)
Epoch: [40][300/391]	Time  0.038 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.2354e-01 (5.5523e-01)	Acc@1  80.47 ( 83.04)	Acc@5  96.09 ( 97.23)
Epoch: [40][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.1660e-01 (5.5548e-01)	Acc@1  82.81 ( 83.02)	Acc@5  96.88 ( 97.22)
Epoch: [40][320/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.3906e-01 (5.5602e-01)	Acc@1  82.03 ( 82.97)	Acc@5  97.66 ( 97.22)
Epoch: [40][330/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.002)	Loss 6.0449e-01 (5.5590e-01)	Acc@1  79.69 ( 82.97)	Acc@5  97.66 ( 97.21)
Epoch: [40][340/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.3164e-01 (5.5641e-01)	Acc@1  86.72 ( 82.95)	Acc@5  97.66 ( 97.19)
Epoch: [40][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.6396e-01 (5.5673e-01)	Acc@1  82.81 ( 82.93)	Acc@5  96.88 ( 97.17)
Epoch: [40][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.9814e-01 (5.5857e-01)	Acc@1  82.03 ( 82.87)	Acc@5  98.44 ( 97.18)
Epoch: [40][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.4746e-01 (5.6034e-01)	Acc@1  83.59 ( 82.82)	Acc@5  95.31 ( 97.16)
Epoch: [40][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.6602e-01 (5.6098e-01)	Acc@1  81.25 ( 82.79)	Acc@5  96.88 ( 97.17)
Epoch: [40][390/391]	Time  0.018 ( 0.025)	Data  0.000 ( 0.002)	Loss 4.5361e-01 (5.6086e-01)	Acc@1  85.00 ( 82.78)	Acc@5  98.75 ( 97.18)
## e[40] optimizer.zero_grad (sum) time: 0.1055445671081543
## e[40]       loss.backward (sum) time: 2.2835845947265625
## e[40]      optimizer.step (sum) time: 0.9313950538635254
## epoch[40] training(only) time: 10.062215328216553
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.1270e+00 (1.1270e+00)	Acc@1  75.00 ( 75.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 1.2871e+00 (1.3505e+00)	Acc@1  68.00 ( 68.45)	Acc@5  94.00 ( 88.64)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.4287e+00 (1.3277e+00)	Acc@1  73.00 ( 67.71)	Acc@5  88.00 ( 88.86)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.3096e+00 (1.3469e+00)	Acc@1  66.00 ( 66.52)	Acc@5  89.00 ( 88.58)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.3447e+00 (1.3462e+00)	Acc@1  66.00 ( 66.49)	Acc@5  92.00 ( 88.88)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.3750e+00 (1.3625e+00)	Acc@1  67.00 ( 65.98)	Acc@5  86.00 ( 88.78)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.5303e+00 (1.3449e+00)	Acc@1  61.00 ( 66.36)	Acc@5  88.00 ( 88.95)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.2988e+00 (1.3467e+00)	Acc@1  67.00 ( 66.41)	Acc@5  89.00 ( 88.86)
Test: [ 80/100]	Time  0.009 ( 0.015)	Loss 1.5459e+00 (1.3520e+00)	Acc@1  67.00 ( 66.37)	Acc@5  84.00 ( 88.80)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.5664e+00 (1.3451e+00)	Acc@1  63.00 ( 66.47)	Acc@5  89.00 ( 88.89)
 * Acc@1 66.440 Acc@5 88.800
### epoch[40] execution time: 11.662036418914795
EPOCH 41
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [41][  0/391]	Time  0.178 ( 0.178)	Data  0.153 ( 0.153)	Loss 6.2061e-01 (6.2061e-01)	Acc@1  83.59 ( 83.59)	Acc@5  96.88 ( 96.88)
Epoch: [41][ 10/391]	Time  0.028 ( 0.039)	Data  0.001 ( 0.016)	Loss 5.2881e-01 (5.2413e-01)	Acc@1  85.16 ( 84.59)	Acc@5  95.31 ( 97.51)
Epoch: [41][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 4.8169e-01 (5.0763e-01)	Acc@1  89.84 ( 85.01)	Acc@5  96.88 ( 97.25)
Epoch: [41][ 30/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.007)	Loss 4.3677e-01 (5.0682e-01)	Acc@1  88.28 ( 84.65)	Acc@5  97.66 ( 97.35)
Epoch: [41][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 4.9585e-01 (5.1212e-01)	Acc@1  84.38 ( 84.60)	Acc@5  97.66 ( 97.24)
Epoch: [41][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 5.3271e-01 (5.1126e-01)	Acc@1  82.03 ( 84.51)	Acc@5  97.66 ( 97.30)
Epoch: [41][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.2090e-01 (5.1347e-01)	Acc@1  89.06 ( 84.35)	Acc@5  99.22 ( 97.34)
Epoch: [41][ 70/391]	Time  0.036 ( 0.027)	Data  0.004 ( 0.004)	Loss 4.4873e-01 (5.1154e-01)	Acc@1  87.50 ( 84.30)	Acc@5  99.22 ( 97.43)
Epoch: [41][ 80/391]	Time  0.020 ( 0.027)	Data  0.000 ( 0.004)	Loss 4.0942e-01 (5.1062e-01)	Acc@1  85.94 ( 84.32)	Acc@5  99.22 ( 97.46)
Epoch: [41][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.4150e-01 (5.0695e-01)	Acc@1  82.81 ( 84.48)	Acc@5  96.88 ( 97.51)
Epoch: [41][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2690e-01 (5.0965e-01)	Acc@1  92.19 ( 84.47)	Acc@5  98.44 ( 97.48)
Epoch: [41][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5337e-01 (5.1299e-01)	Acc@1  86.72 ( 84.39)	Acc@5  96.88 ( 97.46)
Epoch: [41][120/391]	Time  0.039 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.6050e-01 (5.1133e-01)	Acc@1  92.19 ( 84.38)	Acc@5  98.44 ( 97.49)
Epoch: [41][130/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.4248e-01 (5.1136e-01)	Acc@1  82.03 ( 84.32)	Acc@5  98.44 ( 97.53)
Epoch: [41][140/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3428e-01 (5.1353e-01)	Acc@1  84.38 ( 84.30)	Acc@5  97.66 ( 97.54)
Epoch: [41][150/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9336e-01 (5.1335e-01)	Acc@1  73.44 ( 84.20)	Acc@5  98.44 ( 97.58)
Epoch: [41][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8706e-01 (5.1527e-01)	Acc@1  82.03 ( 84.07)	Acc@5  98.44 ( 97.56)
Epoch: [41][170/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0845e-01 (5.2023e-01)	Acc@1  87.50 ( 83.95)	Acc@5  99.22 ( 97.52)
Epoch: [41][180/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8999e-01 (5.2112e-01)	Acc@1  82.81 ( 83.90)	Acc@5  99.22 ( 97.51)
Epoch: [41][190/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.5078e-01 (5.2325e-01)	Acc@1  85.94 ( 83.90)	Acc@5  96.09 ( 97.51)
Epoch: [41][200/391]	Time  0.037 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.4404e-01 (5.2457e-01)	Acc@1  78.12 ( 83.83)	Acc@5  98.44 ( 97.52)
Epoch: [41][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9619e-01 (5.2499e-01)	Acc@1  80.47 ( 83.80)	Acc@5  95.31 ( 97.49)
Epoch: [41][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.3779e-01 (5.2795e-01)	Acc@1  79.69 ( 83.75)	Acc@5  96.09 ( 97.47)
Epoch: [41][230/391]	Time  0.030 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.5420e-01 (5.2919e-01)	Acc@1  84.38 ( 83.74)	Acc@5  96.88 ( 97.45)
Epoch: [41][240/391]	Time  0.026 ( 0.026)	Data  0.005 ( 0.003)	Loss 5.7275e-01 (5.2973e-01)	Acc@1  81.25 ( 83.73)	Acc@5  96.88 ( 97.45)
Epoch: [41][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9829e-01 (5.3260e-01)	Acc@1  82.81 ( 83.62)	Acc@5  99.22 ( 97.41)
Epoch: [41][260/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 7.6367e-01 (5.3403e-01)	Acc@1  80.47 ( 83.63)	Acc@5  93.75 ( 97.40)
Epoch: [41][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.0068e-01 (5.3429e-01)	Acc@1  78.12 ( 83.61)	Acc@5  94.53 ( 97.38)
Epoch: [41][280/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4541e-01 (5.3319e-01)	Acc@1  85.94 ( 83.65)	Acc@5  98.44 ( 97.38)
Epoch: [41][290/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0059e-01 (5.3539e-01)	Acc@1  78.91 ( 83.59)	Acc@5  96.88 ( 97.38)
Epoch: [41][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.6504e-01 (5.3625e-01)	Acc@1  82.03 ( 83.57)	Acc@5  96.09 ( 97.36)
Epoch: [41][310/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 6.1670e-01 (5.3850e-01)	Acc@1  83.59 ( 83.51)	Acc@5  98.44 ( 97.34)
Epoch: [41][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5430e-01 (5.3809e-01)	Acc@1  84.38 ( 83.51)	Acc@5  95.31 ( 97.35)
Epoch: [41][330/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.5371e-01 (5.3780e-01)	Acc@1  82.81 ( 83.53)	Acc@5  97.66 ( 97.36)
Epoch: [41][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.4355e-01 (5.3847e-01)	Acc@1  78.12 ( 83.46)	Acc@5  96.09 ( 97.36)
Epoch: [41][350/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.5908e-01 (5.3932e-01)	Acc@1  81.25 ( 83.45)	Acc@5  96.88 ( 97.34)
Epoch: [41][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.5635e-01 (5.4032e-01)	Acc@1  80.47 ( 83.43)	Acc@5  96.09 ( 97.31)
Epoch: [41][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.3242e-01 (5.4127e-01)	Acc@1  76.56 ( 83.41)	Acc@5  95.31 ( 97.30)
Epoch: [41][380/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.7715e-01 (5.4076e-01)	Acc@1  78.91 ( 83.43)	Acc@5  99.22 ( 97.31)
Epoch: [41][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.9727e-01 (5.4199e-01)	Acc@1  77.50 ( 83.40)	Acc@5  96.25 ( 97.30)
## e[41] optimizer.zero_grad (sum) time: 0.10436201095581055
## e[41]       loss.backward (sum) time: 2.3003525733947754
## e[41]      optimizer.step (sum) time: 0.921438455581665
## epoch[41] training(only) time: 10.099766731262207
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 1.1016e+00 (1.1016e+00)	Acc@1  75.00 ( 75.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.2031e+00 (1.3392e+00)	Acc@1  69.00 ( 68.09)	Acc@5  92.00 ( 88.18)
Test: [ 20/100]	Time  0.019 ( 0.020)	Loss 1.4209e+00 (1.3227e+00)	Acc@1  67.00 ( 67.38)	Acc@5  88.00 ( 88.33)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 1.2891e+00 (1.3351e+00)	Acc@1  65.00 ( 66.48)	Acc@5  90.00 ( 88.55)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.3389e+00 (1.3407e+00)	Acc@1  65.00 ( 66.56)	Acc@5  92.00 ( 88.80)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.4561e+00 (1.3556e+00)	Acc@1  68.00 ( 66.39)	Acc@5  85.00 ( 88.78)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.5625e+00 (1.3424e+00)	Acc@1  61.00 ( 66.52)	Acc@5  89.00 ( 88.92)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 1.3398e+00 (1.3482e+00)	Acc@1  65.00 ( 66.72)	Acc@5  88.00 ( 88.82)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.5000e+00 (1.3540e+00)	Acc@1  66.00 ( 66.54)	Acc@5  81.00 ( 88.74)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.7275e+00 (1.3478e+00)	Acc@1  58.00 ( 66.63)	Acc@5  87.00 ( 88.90)
 * Acc@1 66.720 Acc@5 88.870
### epoch[41] execution time: 11.666231393814087
EPOCH 42
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [42][  0/391]	Time  0.178 ( 0.178)	Data  0.145 ( 0.145)	Loss 4.8340e-01 (4.8340e-01)	Acc@1  85.94 ( 85.94)	Acc@5  96.88 ( 96.88)
Epoch: [42][ 10/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.015)	Loss 4.9463e-01 (4.9458e-01)	Acc@1  81.25 ( 84.16)	Acc@5  98.44 ( 97.59)
Epoch: [42][ 20/391]	Time  0.021 ( 0.033)	Data  0.001 ( 0.008)	Loss 4.4824e-01 (4.7510e-01)	Acc@1  85.16 ( 84.64)	Acc@5  98.44 ( 97.73)
Epoch: [42][ 30/391]	Time  0.022 ( 0.030)	Data  0.001 ( 0.006)	Loss 6.3916e-01 (4.8845e-01)	Acc@1  85.16 ( 84.40)	Acc@5  96.09 ( 97.66)
Epoch: [42][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.5596e-01 (4.9521e-01)	Acc@1  89.06 ( 84.43)	Acc@5 100.00 ( 97.71)
Epoch: [42][ 50/391]	Time  0.018 ( 0.028)	Data  0.000 ( 0.005)	Loss 5.4443e-01 (5.0719e-01)	Acc@1  84.38 ( 84.27)	Acc@5  98.44 ( 97.63)
Epoch: [42][ 60/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.2041e-01 (5.0881e-01)	Acc@1  84.38 ( 84.27)	Acc@5  99.22 ( 97.62)
Epoch: [42][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.2939e-01 (5.1156e-01)	Acc@1  79.69 ( 84.14)	Acc@5  96.88 ( 97.58)
Epoch: [42][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.1416e-01 (5.1349e-01)	Acc@1  83.59 ( 84.15)	Acc@5  96.88 ( 97.54)
Epoch: [42][ 90/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.8828e-01 (5.1158e-01)	Acc@1  83.59 ( 84.28)	Acc@5  98.44 ( 97.54)
Epoch: [42][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.9927e-01 (5.1385e-01)	Acc@1  87.50 ( 84.21)	Acc@5  97.66 ( 97.53)
Epoch: [42][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.0977e-01 (5.1445e-01)	Acc@1  79.69 ( 84.17)	Acc@5  97.66 ( 97.52)
Epoch: [42][120/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.5874e-01 (5.1117e-01)	Acc@1  89.84 ( 84.27)	Acc@5  96.09 ( 97.55)
Epoch: [42][130/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3623e-01 (5.1118e-01)	Acc@1  81.25 ( 84.23)	Acc@5  98.44 ( 97.60)
Epoch: [42][140/391]	Time  0.020 ( 0.026)	Data  0.003 ( 0.003)	Loss 6.6553e-01 (5.1419e-01)	Acc@1  81.25 ( 84.21)	Acc@5  93.75 ( 97.53)
Epoch: [42][150/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9229e-01 (5.1689e-01)	Acc@1  80.47 ( 84.15)	Acc@5  97.66 ( 97.51)
Epoch: [42][160/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.3564e-01 (5.2081e-01)	Acc@1  83.59 ( 84.03)	Acc@5  96.88 ( 97.44)
Epoch: [42][170/391]	Time  0.043 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4067e-01 (5.2000e-01)	Acc@1  87.50 ( 84.06)	Acc@5  96.88 ( 97.45)
Epoch: [42][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4795e-01 (5.2094e-01)	Acc@1  83.59 ( 84.02)	Acc@5  95.31 ( 97.44)
Epoch: [42][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6152e-01 (5.1954e-01)	Acc@1  83.59 ( 84.02)	Acc@5  96.09 ( 97.46)
Epoch: [42][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7422e-01 (5.2132e-01)	Acc@1  79.69 ( 83.96)	Acc@5  98.44 ( 97.47)
Epoch: [42][210/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7236e-01 (5.2216e-01)	Acc@1  80.47 ( 83.97)	Acc@5  97.66 ( 97.45)
Epoch: [42][220/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8120e-01 (5.2201e-01)	Acc@1  85.16 ( 83.99)	Acc@5  99.22 ( 97.45)
Epoch: [42][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6152e-01 (5.2420e-01)	Acc@1  85.94 ( 83.94)	Acc@5  96.88 ( 97.43)
Epoch: [42][240/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9365e-01 (5.2444e-01)	Acc@1  85.16 ( 83.96)	Acc@5  98.44 ( 97.43)
Epoch: [42][250/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.1338e-01 (5.2428e-01)	Acc@1  73.44 ( 83.95)	Acc@5  94.53 ( 97.42)
Epoch: [42][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.5322e-01 (5.2286e-01)	Acc@1  85.16 ( 84.01)	Acc@5  96.88 ( 97.44)
Epoch: [42][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.4248e-01 (5.2560e-01)	Acc@1  82.03 ( 83.91)	Acc@5  99.22 ( 97.44)
Epoch: [42][280/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.7168e-01 (5.2496e-01)	Acc@1  85.16 ( 83.93)	Acc@5  97.66 ( 97.44)
Epoch: [42][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.8999e-01 (5.2439e-01)	Acc@1  87.50 ( 83.99)	Acc@5  96.88 ( 97.43)
Epoch: [42][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.6406e-01 (5.2610e-01)	Acc@1  82.03 ( 83.94)	Acc@5  96.88 ( 97.43)
Epoch: [42][310/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.4932e-01 (5.2629e-01)	Acc@1  82.03 ( 83.95)	Acc@5  98.44 ( 97.43)
Epoch: [42][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.6602e-01 (5.2662e-01)	Acc@1  83.59 ( 83.96)	Acc@5  96.88 ( 97.43)
Epoch: [42][330/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.8652e-01 (5.2656e-01)	Acc@1  71.88 ( 83.92)	Acc@5  98.44 ( 97.44)
Epoch: [42][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.3613e-01 (5.2834e-01)	Acc@1  89.06 ( 83.89)	Acc@5  95.31 ( 97.40)
Epoch: [42][350/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.0781e-01 (5.2792e-01)	Acc@1  83.59 ( 83.89)	Acc@5  97.66 ( 97.40)
Epoch: [42][360/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.9316e-01 (5.2685e-01)	Acc@1  83.59 ( 83.94)	Acc@5  96.88 ( 97.39)
Epoch: [42][370/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.5874e-01 (5.2699e-01)	Acc@1  82.81 ( 83.90)	Acc@5  99.22 ( 97.40)
Epoch: [42][380/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.6655e-01 (5.2698e-01)	Acc@1  86.72 ( 83.87)	Acc@5  96.09 ( 97.42)
Epoch: [42][390/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.0400e-01 (5.2606e-01)	Acc@1  78.75 ( 83.90)	Acc@5  97.50 ( 97.43)
## e[42] optimizer.zero_grad (sum) time: 0.10588502883911133
## e[42]       loss.backward (sum) time: 2.27123761177063
## e[42]      optimizer.step (sum) time: 0.9282350540161133
## epoch[42] training(only) time: 10.10918927192688
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.0957e+00 (1.0957e+00)	Acc@1  74.00 ( 74.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.020 ( 0.025)	Loss 1.2930e+00 (1.3542e+00)	Acc@1  65.00 ( 68.36)	Acc@5  93.00 ( 88.64)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.4150e+00 (1.3275e+00)	Acc@1  68.00 ( 67.90)	Acc@5  88.00 ( 88.81)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.2588e+00 (1.3329e+00)	Acc@1  64.00 ( 66.81)	Acc@5  91.00 ( 88.90)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 1.3359e+00 (1.3364e+00)	Acc@1  65.00 ( 66.93)	Acc@5  92.00 ( 89.20)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 1.4941e+00 (1.3610e+00)	Acc@1  68.00 ( 66.69)	Acc@5  85.00 ( 88.88)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.5576e+00 (1.3477e+00)	Acc@1  60.00 ( 66.64)	Acc@5  90.00 ( 89.10)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.3398e+00 (1.3490e+00)	Acc@1  69.00 ( 66.72)	Acc@5  90.00 ( 89.04)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.5352e+00 (1.3570e+00)	Acc@1  66.00 ( 66.58)	Acc@5  82.00 ( 88.86)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.6367e+00 (1.3508e+00)	Acc@1  62.00 ( 66.65)	Acc@5  89.00 ( 89.02)
 * Acc@1 66.660 Acc@5 88.960
### epoch[42] execution time: 11.706435680389404
EPOCH 43
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [43][  0/391]	Time  0.177 ( 0.177)	Data  0.150 ( 0.150)	Loss 4.3140e-01 (4.3140e-01)	Acc@1  82.81 ( 82.81)	Acc@5  98.44 ( 98.44)
Epoch: [43][ 10/391]	Time  0.028 ( 0.039)	Data  0.001 ( 0.015)	Loss 6.2402e-01 (5.0990e-01)	Acc@1  78.91 ( 83.59)	Acc@5  99.22 ( 97.80)
Epoch: [43][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 5.4346e-01 (5.0349e-01)	Acc@1  84.38 ( 83.97)	Acc@5  94.53 ( 97.81)
Epoch: [43][ 30/391]	Time  0.040 ( 0.030)	Data  0.002 ( 0.007)	Loss 3.1812e-01 (5.0996e-01)	Acc@1  94.53 ( 83.95)	Acc@5 100.00 ( 97.81)
Epoch: [43][ 40/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.005)	Loss 5.4932e-01 (5.1124e-01)	Acc@1  85.16 ( 84.18)	Acc@5  96.09 ( 97.58)
Epoch: [43][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 6.0156e-01 (5.0672e-01)	Acc@1  77.34 ( 84.13)	Acc@5  97.66 ( 97.56)
Epoch: [43][ 60/391]	Time  0.019 ( 0.027)	Data  0.000 ( 0.004)	Loss 4.6753e-01 (5.1031e-01)	Acc@1  86.72 ( 84.16)	Acc@5  99.22 ( 97.66)
Epoch: [43][ 70/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.2451e-01 (5.1799e-01)	Acc@1  78.91 ( 83.84)	Acc@5  97.66 ( 97.57)
Epoch: [43][ 80/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.9683e-01 (5.1247e-01)	Acc@1  85.16 ( 83.95)	Acc@5  96.88 ( 97.60)
Epoch: [43][ 90/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.0586e-01 (5.1164e-01)	Acc@1  81.25 ( 83.98)	Acc@5  98.44 ( 97.65)
Epoch: [43][100/391]	Time  0.034 ( 0.027)	Data  0.004 ( 0.003)	Loss 4.5312e-01 (5.1184e-01)	Acc@1  85.16 ( 84.03)	Acc@5  96.88 ( 97.57)
Epoch: [43][110/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3677e-01 (5.0906e-01)	Acc@1  85.16 ( 83.97)	Acc@5  98.44 ( 97.65)
Epoch: [43][120/391]	Time  0.022 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.3555e-01 (5.0572e-01)	Acc@1  85.94 ( 84.09)	Acc@5  99.22 ( 97.68)
Epoch: [43][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5825e-01 (5.0428e-01)	Acc@1  88.28 ( 84.06)	Acc@5  97.66 ( 97.70)
Epoch: [43][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8193e-01 (5.0289e-01)	Acc@1  83.59 ( 84.11)	Acc@5  98.44 ( 97.73)
Epoch: [43][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4932e-01 (5.0060e-01)	Acc@1  81.25 ( 84.21)	Acc@5  97.66 ( 97.76)
Epoch: [43][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9131e-01 (5.0066e-01)	Acc@1  82.81 ( 84.19)	Acc@5  95.31 ( 97.76)
Epoch: [43][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.7725e-01 (5.0228e-01)	Acc@1  78.12 ( 84.14)	Acc@5  96.09 ( 97.77)
Epoch: [43][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2646e-01 (5.0110e-01)	Acc@1  82.03 ( 84.17)	Acc@5  94.53 ( 97.77)
Epoch: [43][190/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0552e-01 (5.0113e-01)	Acc@1  84.38 ( 84.24)	Acc@5 100.00 ( 97.74)
Epoch: [43][200/391]	Time  0.037 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.4116e-01 (5.0217e-01)	Acc@1  89.06 ( 84.24)	Acc@5  97.66 ( 97.73)
Epoch: [43][210/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.3433e-01 (5.0241e-01)	Acc@1  89.06 ( 84.24)	Acc@5  97.66 ( 97.68)
Epoch: [43][220/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.4922e-01 (5.0163e-01)	Acc@1  85.94 ( 84.30)	Acc@5  97.66 ( 97.68)
Epoch: [43][230/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.003)	Loss 3.1055e-01 (5.0229e-01)	Acc@1  89.84 ( 84.30)	Acc@5  99.22 ( 97.67)
Epoch: [43][240/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.3027e-01 (5.0350e-01)	Acc@1  82.81 ( 84.26)	Acc@5  96.88 ( 97.65)
Epoch: [43][250/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.2393e-01 (5.0282e-01)	Acc@1  83.59 ( 84.28)	Acc@5  96.88 ( 97.65)
Epoch: [43][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.6460e-01 (5.0317e-01)	Acc@1  86.72 ( 84.29)	Acc@5  99.22 ( 97.65)
Epoch: [43][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.0498e-01 (5.0383e-01)	Acc@1  82.03 ( 84.27)	Acc@5  97.66 ( 97.65)
Epoch: [43][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.9912e-01 (5.0466e-01)	Acc@1  79.69 ( 84.27)	Acc@5  98.44 ( 97.65)
Epoch: [43][290/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.3613e-01 (5.0664e-01)	Acc@1  86.72 ( 84.22)	Acc@5  96.09 ( 97.63)
Epoch: [43][300/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.4199e-01 (5.0674e-01)	Acc@1  87.50 ( 84.21)	Acc@5  96.88 ( 97.64)
Epoch: [43][310/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.4922e-01 (5.0633e-01)	Acc@1  85.94 ( 84.23)	Acc@5  98.44 ( 97.65)
Epoch: [43][320/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7280e-01 (5.0562e-01)	Acc@1  88.28 ( 84.26)	Acc@5  97.66 ( 97.65)
Epoch: [43][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 7.1338e-01 (5.0678e-01)	Acc@1  78.12 ( 84.21)	Acc@5  96.09 ( 97.64)
Epoch: [43][340/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.5850e-01 (5.0560e-01)	Acc@1  87.50 ( 84.26)	Acc@5  99.22 ( 97.66)
Epoch: [43][350/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 6.1084e-01 (5.0640e-01)	Acc@1  82.81 ( 84.23)	Acc@5  95.31 ( 97.65)
Epoch: [43][360/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.7412e-01 (5.0665e-01)	Acc@1  84.38 ( 84.24)	Acc@5  99.22 ( 97.64)
Epoch: [43][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.1221e-01 (5.0504e-01)	Acc@1  82.03 ( 84.28)	Acc@5  99.22 ( 97.67)
Epoch: [43][380/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2700e-01 (5.0313e-01)	Acc@1  86.72 ( 84.35)	Acc@5  99.22 ( 97.68)
Epoch: [43][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.3135e-01 (5.0355e-01)	Acc@1  80.00 ( 84.33)	Acc@5  98.75 ( 97.68)
## e[43] optimizer.zero_grad (sum) time: 0.10764908790588379
## e[43]       loss.backward (sum) time: 2.3056375980377197
## e[43]      optimizer.step (sum) time: 0.9339492321014404
## epoch[43] training(only) time: 10.065991878509521
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.1426e+00 (1.1426e+00)	Acc@1  71.00 ( 71.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.2246e+00 (1.3569e+00)	Acc@1  69.00 ( 67.36)	Acc@5  91.00 ( 87.91)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 1.3555e+00 (1.3494e+00)	Acc@1  65.00 ( 66.62)	Acc@5  91.00 ( 88.57)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.3174e+00 (1.3646e+00)	Acc@1  61.00 ( 65.65)	Acc@5  89.00 ( 88.61)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 1.3789e+00 (1.3615e+00)	Acc@1  66.00 ( 65.80)	Acc@5  92.00 ( 88.93)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5166e+00 (1.3826e+00)	Acc@1  66.00 ( 65.55)	Acc@5  87.00 ( 88.71)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.5127e+00 (1.3729e+00)	Acc@1  62.00 ( 65.85)	Acc@5  90.00 ( 88.85)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.3906e+00 (1.3747e+00)	Acc@1  66.00 ( 66.04)	Acc@5  90.00 ( 88.77)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.4902e+00 (1.3805e+00)	Acc@1  66.00 ( 66.01)	Acc@5  84.00 ( 88.64)
Test: [ 90/100]	Time  0.020 ( 0.015)	Loss 1.7822e+00 (1.3764e+00)	Acc@1  57.00 ( 66.11)	Acc@5  87.00 ( 88.70)
 * Acc@1 66.210 Acc@5 88.600
### epoch[43] execution time: 11.663944721221924
EPOCH 44
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [44][  0/391]	Time  0.176 ( 0.176)	Data  0.150 ( 0.150)	Loss 3.6206e-01 (3.6206e-01)	Acc@1  88.28 ( 88.28)	Acc@5  98.44 ( 98.44)
Epoch: [44][ 10/391]	Time  0.022 ( 0.038)	Data  0.001 ( 0.015)	Loss 3.9990e-01 (4.4249e-01)	Acc@1  88.28 ( 85.09)	Acc@5  98.44 ( 98.51)
Epoch: [44][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.010)	Loss 4.9780e-01 (4.6117e-01)	Acc@1  82.81 ( 85.38)	Acc@5  98.44 ( 98.14)
Epoch: [44][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 4.5630e-01 (4.7491e-01)	Acc@1  84.38 ( 84.95)	Acc@5  97.66 ( 97.98)
Epoch: [44][ 40/391]	Time  0.022 ( 0.029)	Data  0.001 ( 0.006)	Loss 5.3369e-01 (4.6628e-01)	Acc@1  82.81 ( 85.27)	Acc@5  96.88 ( 97.94)
Epoch: [44][ 50/391]	Time  0.021 ( 0.028)	Data  0.000 ( 0.005)	Loss 6.1035e-01 (4.7708e-01)	Acc@1  80.47 ( 85.05)	Acc@5  95.31 ( 97.89)
Epoch: [44][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.1113e-01 (4.7550e-01)	Acc@1  89.84 ( 85.31)	Acc@5 100.00 ( 97.85)
Epoch: [44][ 70/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.8169e-01 (4.7824e-01)	Acc@1  85.94 ( 85.23)	Acc@5  97.66 ( 97.82)
Epoch: [44][ 80/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 6.7383e-01 (4.7847e-01)	Acc@1  81.25 ( 85.21)	Acc@5  96.09 ( 97.82)
Epoch: [44][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.9033e-01 (4.7951e-01)	Acc@1  78.91 ( 85.14)	Acc@5  96.09 ( 97.83)
Epoch: [44][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5024e-01 (4.8047e-01)	Acc@1  91.41 ( 85.13)	Acc@5 100.00 ( 97.87)
Epoch: [44][110/391]	Time  0.036 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.4155e-01 (4.7666e-01)	Acc@1  87.50 ( 85.22)	Acc@5  98.44 ( 97.91)
Epoch: [44][120/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3750e-01 (4.7864e-01)	Acc@1  88.28 ( 85.13)	Acc@5  98.44 ( 97.90)
Epoch: [44][130/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.1025e-01 (4.8306e-01)	Acc@1  85.16 ( 85.10)	Acc@5  96.88 ( 97.84)
Epoch: [44][140/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8193e-01 (4.8162e-01)	Acc@1  86.72 ( 85.17)	Acc@5  98.44 ( 97.85)
Epoch: [44][150/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.3213e-01 (4.7962e-01)	Acc@1  88.28 ( 85.24)	Acc@5  98.44 ( 97.90)
Epoch: [44][160/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.9243e-01 (4.8052e-01)	Acc@1  81.25 ( 85.21)	Acc@5  97.66 ( 97.86)
Epoch: [44][170/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.0649e-01 (4.7945e-01)	Acc@1  87.50 ( 85.24)	Acc@5 100.00 ( 97.87)
Epoch: [44][180/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.4346e-01 (4.7939e-01)	Acc@1  84.38 ( 85.25)	Acc@5  96.88 ( 97.89)
Epoch: [44][190/391]	Time  0.028 ( 0.026)	Data  0.004 ( 0.003)	Loss 5.0293e-01 (4.7935e-01)	Acc@1  85.94 ( 85.24)	Acc@5  98.44 ( 97.91)
Epoch: [44][200/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0039e-01 (4.8119e-01)	Acc@1  87.50 ( 85.17)	Acc@5  98.44 ( 97.88)
Epoch: [44][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0234e-01 (4.8191e-01)	Acc@1  88.28 ( 85.18)	Acc@5  96.88 ( 97.86)
Epoch: [44][220/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8696e-01 (4.8128e-01)	Acc@1  89.06 ( 85.14)	Acc@5  99.22 ( 97.88)
Epoch: [44][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6924e-01 (4.8091e-01)	Acc@1  82.81 ( 85.12)	Acc@5  98.44 ( 97.90)
Epoch: [44][240/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.4004e-01 (4.8297e-01)	Acc@1  83.59 ( 85.05)	Acc@5  97.66 ( 97.90)
Epoch: [44][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8486e-01 (4.8474e-01)	Acc@1  85.94 ( 84.98)	Acc@5  98.44 ( 97.88)
Epoch: [44][260/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1113e-01 (4.8530e-01)	Acc@1  85.16 ( 84.94)	Acc@5  99.22 ( 97.89)
Epoch: [44][270/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.4941e-01 (4.8476e-01)	Acc@1  78.12 ( 84.99)	Acc@5  96.88 ( 97.88)
Epoch: [44][280/391]	Time  0.042 ( 0.026)	Data  0.005 ( 0.003)	Loss 5.0977e-01 (4.8483e-01)	Acc@1  84.38 ( 84.99)	Acc@5  96.09 ( 97.87)
Epoch: [44][290/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.5361e-01 (4.8517e-01)	Acc@1  84.38 ( 84.95)	Acc@5  98.44 ( 97.88)
Epoch: [44][300/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.5674e-01 (4.8563e-01)	Acc@1  83.59 ( 84.96)	Acc@5  95.31 ( 97.87)
Epoch: [44][310/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.0889e-01 (4.8508e-01)	Acc@1  82.81 ( 84.99)	Acc@5  98.44 ( 97.88)
Epoch: [44][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.8184e-01 (4.8543e-01)	Acc@1  86.72 ( 84.97)	Acc@5  99.22 ( 97.89)
Epoch: [44][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.4639e-01 (4.8606e-01)	Acc@1  81.25 ( 84.92)	Acc@5  97.66 ( 97.88)
Epoch: [44][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4985e-01 (4.8720e-01)	Acc@1  88.28 ( 84.90)	Acc@5  98.44 ( 97.87)
Epoch: [44][350/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.8770e-01 (4.8722e-01)	Acc@1  91.41 ( 84.90)	Acc@5  97.66 ( 97.86)
Epoch: [44][360/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.0479e-01 (4.8719e-01)	Acc@1  86.72 ( 84.90)	Acc@5  99.22 ( 97.87)
Epoch: [44][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.3750e-01 (4.8758e-01)	Acc@1  88.28 ( 84.90)	Acc@5  98.44 ( 97.86)
Epoch: [44][380/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.9819e-01 (4.8755e-01)	Acc@1  85.94 ( 84.88)	Acc@5  99.22 ( 97.85)
Epoch: [44][390/391]	Time  0.018 ( 0.026)	Data  0.004 ( 0.002)	Loss 6.2549e-01 (4.8791e-01)	Acc@1  75.00 ( 84.87)	Acc@5 100.00 ( 97.84)
## e[44] optimizer.zero_grad (sum) time: 0.10521674156188965
## e[44]       loss.backward (sum) time: 2.1759259700775146
## e[44]      optimizer.step (sum) time: 0.9232544898986816
## epoch[44] training(only) time: 10.173315525054932
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.1084e+00 (1.1084e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.014 ( 0.025)	Loss 1.3730e+00 (1.3896e+00)	Acc@1  63.00 ( 67.82)	Acc@5  92.00 ( 88.45)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.5117e+00 (1.3664e+00)	Acc@1  67.00 ( 67.43)	Acc@5  88.00 ( 88.52)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.3428e+00 (1.3808e+00)	Acc@1  65.00 ( 66.52)	Acc@5  85.00 ( 88.35)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 1.3818e+00 (1.3764e+00)	Acc@1  66.00 ( 66.66)	Acc@5  90.00 ( 88.66)
Test: [ 50/100]	Time  0.027 ( 0.016)	Loss 1.4629e+00 (1.3915e+00)	Acc@1  65.00 ( 66.24)	Acc@5  88.00 ( 88.63)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.5117e+00 (1.3746e+00)	Acc@1  62.00 ( 66.34)	Acc@5  90.00 ( 88.85)
Test: [ 70/100]	Time  0.026 ( 0.016)	Loss 1.3457e+00 (1.3744e+00)	Acc@1  68.00 ( 66.52)	Acc@5  88.00 ( 88.85)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.5361e+00 (1.3817e+00)	Acc@1  65.00 ( 66.40)	Acc@5  85.00 ( 88.73)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.7373e+00 (1.3761e+00)	Acc@1  62.00 ( 66.44)	Acc@5  89.00 ( 88.85)
 * Acc@1 66.440 Acc@5 88.840
### epoch[44] execution time: 11.764904737472534
EPOCH 45
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [45][  0/391]	Time  0.176 ( 0.176)	Data  0.152 ( 0.152)	Loss 4.9438e-01 (4.9438e-01)	Acc@1  88.28 ( 88.28)	Acc@5  96.09 ( 96.09)
Epoch: [45][ 10/391]	Time  0.020 ( 0.039)	Data  0.001 ( 0.016)	Loss 4.5850e-01 (4.2567e-01)	Acc@1  85.94 ( 86.08)	Acc@5  97.66 ( 98.44)
Epoch: [45][ 20/391]	Time  0.023 ( 0.032)	Data  0.001 ( 0.009)	Loss 4.5117e-01 (4.6787e-01)	Acc@1  82.81 ( 85.19)	Acc@5  99.22 ( 98.33)
Epoch: [45][ 30/391]	Time  0.027 ( 0.029)	Data  0.001 ( 0.007)	Loss 4.2920e-01 (4.7215e-01)	Acc@1  84.38 ( 85.18)	Acc@5  98.44 ( 98.16)
Epoch: [45][ 40/391]	Time  0.035 ( 0.029)	Data  0.001 ( 0.006)	Loss 6.6748e-01 (4.6829e-01)	Acc@1  78.91 ( 85.33)	Acc@5  98.44 ( 98.19)
Epoch: [45][ 50/391]	Time  0.020 ( 0.028)	Data  0.000 ( 0.005)	Loss 4.4043e-01 (4.6152e-01)	Acc@1  85.94 ( 85.43)	Acc@5  98.44 ( 98.13)
Epoch: [45][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.2578e-01 (4.6895e-01)	Acc@1  86.72 ( 85.30)	Acc@5  98.44 ( 98.05)
Epoch: [45][ 70/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.004)	Loss 4.8828e-01 (4.7007e-01)	Acc@1  84.38 ( 85.28)	Acc@5  96.88 ( 97.99)
Epoch: [45][ 80/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 5.7178e-01 (4.7019e-01)	Acc@1  82.03 ( 85.38)	Acc@5  96.88 ( 97.96)
Epoch: [45][ 90/391]	Time  0.039 ( 0.027)	Data  0.003 ( 0.004)	Loss 4.5020e-01 (4.7069e-01)	Acc@1  84.38 ( 85.36)	Acc@5  97.66 ( 97.95)
Epoch: [45][100/391]	Time  0.044 ( 0.027)	Data  0.005 ( 0.004)	Loss 3.7183e-01 (4.7206e-01)	Acc@1  88.28 ( 85.24)	Acc@5  99.22 ( 97.93)
Epoch: [45][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6753e-01 (4.7063e-01)	Acc@1  85.94 ( 85.31)	Acc@5  96.09 ( 97.96)
Epoch: [45][120/391]	Time  0.040 ( 0.026)	Data  0.003 ( 0.003)	Loss 6.3818e-01 (4.7007e-01)	Acc@1  79.69 ( 85.31)	Acc@5  96.09 ( 97.98)
Epoch: [45][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3765e-01 (4.6720e-01)	Acc@1  90.62 ( 85.42)	Acc@5  99.22 ( 97.97)
Epoch: [45][140/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.6987e-01 (4.6829e-01)	Acc@1  85.94 ( 85.33)	Acc@5  99.22 ( 97.97)
Epoch: [45][150/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4033e-01 (4.6738e-01)	Acc@1  90.62 ( 85.34)	Acc@5  97.66 ( 98.01)
Epoch: [45][160/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9858e-01 (4.6481e-01)	Acc@1  92.19 ( 85.47)	Acc@5  97.66 ( 98.03)
Epoch: [45][170/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.8643e-01 (4.6382e-01)	Acc@1  78.12 ( 85.50)	Acc@5  96.09 ( 98.04)
Epoch: [45][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3677e-01 (4.6388e-01)	Acc@1  86.72 ( 85.49)	Acc@5  99.22 ( 98.02)
Epoch: [45][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.0244e-01 (4.6657e-01)	Acc@1  85.94 ( 85.45)	Acc@5  96.88 ( 97.98)
Epoch: [45][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0527e-01 (4.6679e-01)	Acc@1  89.84 ( 85.43)	Acc@5  98.44 ( 97.96)
Epoch: [45][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9561e-01 (4.6841e-01)	Acc@1  84.38 ( 85.39)	Acc@5  99.22 ( 97.94)
Epoch: [45][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.1133e-01 (4.6694e-01)	Acc@1  83.59 ( 85.39)	Acc@5  95.31 ( 97.95)
Epoch: [45][230/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0991e-01 (4.6733e-01)	Acc@1  84.38 ( 85.37)	Acc@5  98.44 ( 97.96)
Epoch: [45][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9258e-01 (4.6715e-01)	Acc@1  89.06 ( 85.36)	Acc@5  98.44 ( 97.96)
Epoch: [45][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.5566e-01 (4.6760e-01)	Acc@1  87.50 ( 85.43)	Acc@5  96.09 ( 97.95)
Epoch: [45][260/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.8267e-01 (4.6745e-01)	Acc@1  88.28 ( 85.44)	Acc@5  97.66 ( 97.96)
Epoch: [45][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6680e-01 (4.6691e-01)	Acc@1  85.16 ( 85.45)	Acc@5  97.66 ( 97.97)
Epoch: [45][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (4.6724e-01)	Acc@1  89.06 ( 85.46)	Acc@5  97.66 ( 97.97)
Epoch: [45][290/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.002)	Loss 4.4067e-01 (4.6670e-01)	Acc@1  87.50 ( 85.48)	Acc@5  99.22 ( 97.98)
Epoch: [45][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 6.2451e-01 (4.6693e-01)	Acc@1  80.47 ( 85.49)	Acc@5  97.66 ( 97.99)
Epoch: [45][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.0439e-01 (4.6908e-01)	Acc@1  87.50 ( 85.42)	Acc@5  98.44 ( 97.99)
Epoch: [45][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.9277e-01 (4.6877e-01)	Acc@1  84.38 ( 85.41)	Acc@5  96.88 ( 98.00)
Epoch: [45][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.7168e-01 (4.6874e-01)	Acc@1  86.72 ( 85.41)	Acc@5  97.66 ( 97.99)
Epoch: [45][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6426e-01 (4.6887e-01)	Acc@1  89.06 ( 85.41)	Acc@5  99.22 ( 98.00)
Epoch: [45][350/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.1309e-01 (4.6894e-01)	Acc@1  85.16 ( 85.42)	Acc@5 100.00 ( 98.00)
Epoch: [45][360/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 5.1318e-01 (4.6996e-01)	Acc@1  83.59 ( 85.38)	Acc@5  98.44 ( 98.00)
Epoch: [45][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.7241e-01 (4.6987e-01)	Acc@1  84.38 ( 85.38)	Acc@5  98.44 ( 98.00)
Epoch: [45][380/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.1758e-01 (4.6982e-01)	Acc@1  87.50 ( 85.38)	Acc@5  98.44 ( 98.00)
Epoch: [45][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7964e-01 (4.7017e-01)	Acc@1  87.50 ( 85.39)	Acc@5  98.75 ( 97.98)
## e[45] optimizer.zero_grad (sum) time: 0.10588932037353516
## e[45]       loss.backward (sum) time: 2.2772789001464844
## e[45]      optimizer.step (sum) time: 0.9250054359436035
## epoch[45] training(only) time: 10.093878984451294
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.1445e+00 (1.1445e+00)	Acc@1  73.00 ( 73.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.3887e+00 (1.3937e+00)	Acc@1  61.00 ( 66.18)	Acc@5  91.00 ( 88.36)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.4463e+00 (1.3758e+00)	Acc@1  65.00 ( 66.00)	Acc@5  85.00 ( 88.29)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.4443e+00 (1.3879e+00)	Acc@1  61.00 ( 65.23)	Acc@5  86.00 ( 88.10)
Test: [ 40/100]	Time  0.010 ( 0.016)	Loss 1.4092e+00 (1.3860e+00)	Acc@1  66.00 ( 65.90)	Acc@5  90.00 ( 88.51)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.4424e+00 (1.4050e+00)	Acc@1  68.00 ( 65.67)	Acc@5  86.00 ( 88.47)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.6064e+00 (1.3942e+00)	Acc@1  63.00 ( 65.82)	Acc@5  88.00 ( 88.72)
Test: [ 70/100]	Time  0.019 ( 0.016)	Loss 1.3857e+00 (1.3952e+00)	Acc@1  70.00 ( 66.04)	Acc@5  90.00 ( 88.79)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.5049e+00 (1.4012e+00)	Acc@1  64.00 ( 65.84)	Acc@5  84.00 ( 88.62)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.7031e+00 (1.3926e+00)	Acc@1  60.00 ( 65.92)	Acc@5  89.00 ( 88.79)
 * Acc@1 66.030 Acc@5 88.790
### epoch[45] execution time: 11.669828176498413
EPOCH 46
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [46][  0/391]	Time  0.180 ( 0.180)	Data  0.150 ( 0.150)	Loss 5.1758e-01 (5.1758e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 97.66)
Epoch: [46][ 10/391]	Time  0.030 ( 0.039)	Data  0.002 ( 0.015)	Loss 3.3447e-01 (4.5541e-01)	Acc@1  92.19 ( 86.01)	Acc@5  98.44 ( 97.80)
Epoch: [46][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 4.7266e-01 (4.3587e-01)	Acc@1  81.25 ( 86.16)	Acc@5  98.44 ( 97.99)
Epoch: [46][ 30/391]	Time  0.021 ( 0.030)	Data  0.000 ( 0.006)	Loss 3.0933e-01 (4.5445e-01)	Acc@1  88.28 ( 85.61)	Acc@5 100.00 ( 98.01)
Epoch: [46][ 40/391]	Time  0.021 ( 0.029)	Data  0.000 ( 0.005)	Loss 5.2441e-01 (4.5856e-01)	Acc@1  85.94 ( 85.80)	Acc@5  96.88 ( 97.94)
Epoch: [46][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.8926e-01 (4.5753e-01)	Acc@1  84.38 ( 85.77)	Acc@5  98.44 ( 98.05)
Epoch: [46][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 5.5127e-01 (4.6301e-01)	Acc@1  81.25 ( 85.60)	Acc@5  96.88 ( 97.96)
Epoch: [46][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.3809e-01 (4.6467e-01)	Acc@1  82.81 ( 85.56)	Acc@5  98.44 ( 97.95)
Epoch: [46][ 80/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 5.1123e-01 (4.6767e-01)	Acc@1  82.81 ( 85.47)	Acc@5  98.44 ( 97.94)
Epoch: [46][ 90/391]	Time  0.039 ( 0.027)	Data  0.006 ( 0.003)	Loss 3.6450e-01 (4.6517e-01)	Acc@1  87.50 ( 85.46)	Acc@5  97.66 ( 97.96)
Epoch: [46][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7046e-01 (4.6294e-01)	Acc@1  83.59 ( 85.54)	Acc@5  96.88 ( 97.98)
Epoch: [46][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7632e-01 (4.6177e-01)	Acc@1  79.69 ( 85.45)	Acc@5  97.66 ( 98.03)
Epoch: [46][120/391]	Time  0.040 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1895e-01 (4.6192e-01)	Acc@1  86.72 ( 85.48)	Acc@5  97.66 ( 98.02)
Epoch: [46][130/391]	Time  0.047 ( 0.026)	Data  0.008 ( 0.003)	Loss 5.7812e-01 (4.6268e-01)	Acc@1  82.03 ( 85.44)	Acc@5  95.31 ( 98.00)
Epoch: [46][140/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7354e-01 (4.6526e-01)	Acc@1  85.94 ( 85.42)	Acc@5  99.22 ( 97.97)
Epoch: [46][150/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8008e-01 (4.6581e-01)	Acc@1  83.59 ( 85.41)	Acc@5  95.31 ( 97.96)
Epoch: [46][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0283e-01 (4.6386e-01)	Acc@1  87.50 ( 85.49)	Acc@5  97.66 ( 97.93)
Epoch: [46][170/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4189e-01 (4.6473e-01)	Acc@1  87.50 ( 85.49)	Acc@5  98.44 ( 97.93)
Epoch: [46][180/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 5.2881e-01 (4.6310e-01)	Acc@1  84.38 ( 85.51)	Acc@5  99.22 ( 97.95)
Epoch: [46][190/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.9619e-01 (4.6285e-01)	Acc@1  78.91 ( 85.50)	Acc@5  99.22 ( 97.95)
Epoch: [46][200/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1758e-01 (4.6133e-01)	Acc@1  83.59 ( 85.53)	Acc@5  96.09 ( 97.96)
Epoch: [46][210/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8887e-01 (4.6174e-01)	Acc@1  85.94 ( 85.54)	Acc@5  96.09 ( 97.95)
Epoch: [46][220/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8135e-01 (4.6297e-01)	Acc@1  85.94 ( 85.52)	Acc@5  99.22 ( 97.96)
Epoch: [46][230/391]	Time  0.034 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.3154e-01 (4.6092e-01)	Acc@1  90.62 ( 85.58)	Acc@5  98.44 ( 97.96)
Epoch: [46][240/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 7.1191e-01 (4.6088e-01)	Acc@1  79.69 ( 85.60)	Acc@5  96.88 ( 97.96)
Epoch: [46][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.9194e-01 (4.6079e-01)	Acc@1  84.38 ( 85.63)	Acc@5  96.09 ( 97.93)
Epoch: [46][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.5337e-01 (4.5927e-01)	Acc@1  86.72 ( 85.68)	Acc@5  98.44 ( 97.95)
Epoch: [46][270/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.8135e-01 (4.5700e-01)	Acc@1  84.38 ( 85.73)	Acc@5  99.22 ( 97.96)
Epoch: [46][280/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.8315e-01 (4.5839e-01)	Acc@1  86.72 ( 85.68)	Acc@5  96.09 ( 97.96)
Epoch: [46][290/391]	Time  0.046 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.8364e-01 (4.5965e-01)	Acc@1  85.94 ( 85.63)	Acc@5  98.44 ( 97.95)
Epoch: [46][300/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.7476e-01 (4.5956e-01)	Acc@1  86.72 ( 85.61)	Acc@5  99.22 ( 97.98)
Epoch: [46][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.4727e-01 (4.6059e-01)	Acc@1  88.28 ( 85.58)	Acc@5  96.09 ( 97.97)
Epoch: [46][320/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6938e-01 (4.6094e-01)	Acc@1  89.06 ( 85.58)	Acc@5  99.22 ( 97.96)
Epoch: [46][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.7617e-01 (4.6029e-01)	Acc@1  85.94 ( 85.60)	Acc@5  94.53 ( 97.97)
Epoch: [46][340/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.7290e-01 (4.5974e-01)	Acc@1  85.94 ( 85.60)	Acc@5  95.31 ( 97.98)
Epoch: [46][350/391]	Time  0.039 ( 0.026)	Data  0.008 ( 0.002)	Loss 4.6167e-01 (4.5993e-01)	Acc@1  85.94 ( 85.60)	Acc@5  98.44 ( 97.97)
Epoch: [46][360/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.4336e-01 (4.6078e-01)	Acc@1  84.38 ( 85.55)	Acc@5  98.44 ( 97.97)
Epoch: [46][370/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.6533e-01 (4.6091e-01)	Acc@1  89.84 ( 85.56)	Acc@5  97.66 ( 97.96)
Epoch: [46][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.3335e-01 (4.6074e-01)	Acc@1  85.16 ( 85.57)	Acc@5  97.66 ( 97.96)
Epoch: [46][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.1699e-01 (4.6035e-01)	Acc@1  86.25 ( 85.59)	Acc@5  97.50 ( 97.97)
## e[46] optimizer.zero_grad (sum) time: 0.10416793823242188
## e[46]       loss.backward (sum) time: 2.257197141647339
## e[46]      optimizer.step (sum) time: 0.9261379241943359
## epoch[46] training(only) time: 10.122878789901733
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.1709e+00 (1.1709e+00)	Acc@1  74.00 ( 74.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.019 ( 0.026)	Loss 1.3379e+00 (1.3744e+00)	Acc@1  65.00 ( 68.36)	Acc@5  90.00 ( 88.27)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 1.5869e+00 (1.3874e+00)	Acc@1  64.00 ( 67.00)	Acc@5  87.00 ( 88.29)
Test: [ 30/100]	Time  0.030 ( 0.018)	Loss 1.3721e+00 (1.3932e+00)	Acc@1  67.00 ( 66.10)	Acc@5  88.00 ( 88.23)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.3740e+00 (1.3957e+00)	Acc@1  66.00 ( 66.00)	Acc@5  93.00 ( 88.46)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.4717e+00 (1.4130e+00)	Acc@1  67.00 ( 65.98)	Acc@5  86.00 ( 88.27)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.5840e+00 (1.3991e+00)	Acc@1  63.00 ( 66.33)	Acc@5  90.00 ( 88.54)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 1.4258e+00 (1.4055e+00)	Acc@1  68.00 ( 66.38)	Acc@5  88.00 ( 88.52)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.5254e+00 (1.4123e+00)	Acc@1  66.00 ( 66.30)	Acc@5  86.00 ( 88.42)
Test: [ 90/100]	Time  0.027 ( 0.015)	Loss 1.8037e+00 (1.4066e+00)	Acc@1  58.00 ( 66.37)	Acc@5  87.00 ( 88.54)
 * Acc@1 66.360 Acc@5 88.490
### epoch[46] execution time: 11.71455454826355
EPOCH 47
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [47][  0/391]	Time  0.181 ( 0.181)	Data  0.151 ( 0.151)	Loss 5.1221e-01 (5.1221e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 97.66)
Epoch: [47][ 10/391]	Time  0.028 ( 0.039)	Data  0.001 ( 0.015)	Loss 5.1270e-01 (4.4385e-01)	Acc@1  85.94 ( 86.79)	Acc@5  96.09 ( 97.80)
Epoch: [47][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 4.3848e-01 (4.3773e-01)	Acc@1  87.50 ( 87.02)	Acc@5  96.88 ( 97.84)
Epoch: [47][ 30/391]	Time  0.022 ( 0.030)	Data  0.001 ( 0.007)	Loss 3.4717e-01 (4.3690e-01)	Acc@1  89.84 ( 86.79)	Acc@5  98.44 ( 97.96)
Epoch: [47][ 40/391]	Time  0.026 ( 0.029)	Data  0.001 ( 0.006)	Loss 5.3320e-01 (4.2808e-01)	Acc@1  84.38 ( 86.97)	Acc@5  96.88 ( 98.04)
Epoch: [47][ 50/391]	Time  0.031 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.3081e-01 (4.2589e-01)	Acc@1  91.41 ( 86.99)	Acc@5  99.22 ( 98.13)
Epoch: [47][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.1079e-01 (4.3116e-01)	Acc@1  92.19 ( 86.86)	Acc@5  97.66 ( 98.12)
Epoch: [47][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.1553e-01 (4.3156e-01)	Acc@1  89.06 ( 86.99)	Acc@5  97.66 ( 98.03)
Epoch: [47][ 80/391]	Time  0.023 ( 0.027)	Data  0.005 ( 0.004)	Loss 4.4385e-01 (4.3034e-01)	Acc@1  85.94 ( 87.00)	Acc@5  97.66 ( 98.11)
Epoch: [47][ 90/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 5.0635e-01 (4.3406e-01)	Acc@1  83.59 ( 86.92)	Acc@5  99.22 ( 98.05)
Epoch: [47][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.3516e-01 (4.3672e-01)	Acc@1  82.03 ( 86.86)	Acc@5  97.66 ( 98.05)
Epoch: [47][110/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1895e-01 (4.3508e-01)	Acc@1  88.28 ( 86.85)	Acc@5  98.44 ( 98.11)
Epoch: [47][120/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3604e-01 (4.2952e-01)	Acc@1  83.59 ( 86.86)	Acc@5  97.66 ( 98.19)
Epoch: [47][130/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.8389e-01 (4.3004e-01)	Acc@1  85.94 ( 86.76)	Acc@5  96.88 ( 98.20)
Epoch: [47][140/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3496e-01 (4.3094e-01)	Acc@1  88.28 ( 86.71)	Acc@5 100.00 ( 98.22)
Epoch: [47][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.8740e-01 (4.3450e-01)	Acc@1  78.91 ( 86.61)	Acc@5  96.88 ( 98.19)
Epoch: [47][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1958e-01 (4.3553e-01)	Acc@1  89.84 ( 86.57)	Acc@5  98.44 ( 98.19)
Epoch: [47][170/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 6.9629e-01 (4.3758e-01)	Acc@1  78.91 ( 86.54)	Acc@5  96.88 ( 98.17)
Epoch: [47][180/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3506e-01 (4.3807e-01)	Acc@1  87.50 ( 86.59)	Acc@5  99.22 ( 98.14)
Epoch: [47][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1641e-01 (4.3612e-01)	Acc@1  89.06 ( 86.58)	Acc@5  98.44 ( 98.17)
Epoch: [47][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2495e-01 (4.3585e-01)	Acc@1  91.41 ( 86.57)	Acc@5  99.22 ( 98.18)
Epoch: [47][210/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9634e-01 (4.3528e-01)	Acc@1  81.25 ( 86.54)	Acc@5  99.22 ( 98.20)
Epoch: [47][220/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.9580e-01 (4.3644e-01)	Acc@1  76.56 ( 86.51)	Acc@5  98.44 ( 98.19)
Epoch: [47][230/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.8560e-01 (4.3604e-01)	Acc@1  84.38 ( 86.51)	Acc@5  98.44 ( 98.19)
Epoch: [47][240/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8818e-01 (4.3527e-01)	Acc@1  89.06 ( 86.49)	Acc@5  97.66 ( 98.21)
Epoch: [47][250/391]	Time  0.045 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.0015e-01 (4.3502e-01)	Acc@1  90.62 ( 86.51)	Acc@5  97.66 ( 98.21)
Epoch: [47][260/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5889e-01 (4.3537e-01)	Acc@1  87.50 ( 86.46)	Acc@5  99.22 ( 98.22)
Epoch: [47][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.9487e-01 (4.3757e-01)	Acc@1  89.06 ( 86.40)	Acc@5  97.66 ( 98.20)
Epoch: [47][280/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.002)	Loss 6.1523e-01 (4.3900e-01)	Acc@1  81.25 ( 86.37)	Acc@5  96.88 ( 98.17)
Epoch: [47][290/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.1807e-01 (4.3657e-01)	Acc@1  83.59 ( 86.43)	Acc@5  97.66 ( 98.19)
Epoch: [47][300/391]	Time  0.025 ( 0.026)	Data  0.004 ( 0.002)	Loss 3.5938e-01 (4.3823e-01)	Acc@1  89.06 ( 86.39)	Acc@5  99.22 ( 98.18)
Epoch: [47][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7061e-01 (4.3798e-01)	Acc@1  87.50 ( 86.40)	Acc@5  97.66 ( 98.19)
Epoch: [47][320/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9590e-01 (4.3798e-01)	Acc@1  91.41 ( 86.38)	Acc@5  99.22 ( 98.19)
Epoch: [47][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1714e-01 (4.3795e-01)	Acc@1  86.72 ( 86.37)	Acc@5  99.22 ( 98.19)
Epoch: [47][340/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.7070e-01 (4.3911e-01)	Acc@1  83.59 ( 86.34)	Acc@5 100.00 ( 98.19)
Epoch: [47][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.3076e-01 (4.3897e-01)	Acc@1  80.47 ( 86.34)	Acc@5  98.44 ( 98.18)
Epoch: [47][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.9780e-01 (4.3948e-01)	Acc@1  80.47 ( 86.27)	Acc@5  98.44 ( 98.18)
Epoch: [47][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.5850e-01 (4.3937e-01)	Acc@1  84.38 ( 86.26)	Acc@5  98.44 ( 98.18)
Epoch: [47][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.2246e-01 (4.3994e-01)	Acc@1  81.25 ( 86.26)	Acc@5  98.44 ( 98.18)
Epoch: [47][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7305e-01 (4.4031e-01)	Acc@1  92.50 ( 86.25)	Acc@5  96.25 ( 98.17)
## e[47] optimizer.zero_grad (sum) time: 0.10318732261657715
## e[47]       loss.backward (sum) time: 2.2451000213623047
## e[47]      optimizer.step (sum) time: 0.9120903015136719
## epoch[47] training(only) time: 10.118512630462646
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.2227e+00 (1.2227e+00)	Acc@1  73.00 ( 73.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.3564e+00 (1.4317e+00)	Acc@1  66.00 ( 67.55)	Acc@5  93.00 ( 88.45)
Test: [ 20/100]	Time  0.021 ( 0.020)	Loss 1.5156e+00 (1.4138e+00)	Acc@1  70.00 ( 67.14)	Acc@5  91.00 ( 88.67)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.3857e+00 (1.4267e+00)	Acc@1  65.00 ( 66.26)	Acc@5  86.00 ( 88.77)
Test: [ 40/100]	Time  0.026 ( 0.017)	Loss 1.4092e+00 (1.4216e+00)	Acc@1  66.00 ( 66.41)	Acc@5  91.00 ( 88.93)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 1.6396e+00 (1.4418e+00)	Acc@1  65.00 ( 66.24)	Acc@5  86.00 ( 88.69)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 1.4980e+00 (1.4260e+00)	Acc@1  63.00 ( 66.31)	Acc@5  91.00 ( 88.80)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.4443e+00 (1.4283e+00)	Acc@1  68.00 ( 66.48)	Acc@5  87.00 ( 88.73)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.4424e+00 (1.4294e+00)	Acc@1  67.00 ( 66.44)	Acc@5  85.00 ( 88.64)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.8164e+00 (1.4231e+00)	Acc@1  62.00 ( 66.66)	Acc@5  88.00 ( 88.74)
 * Acc@1 66.630 Acc@5 88.700
### epoch[47] execution time: 11.717341661453247
EPOCH 48
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [48][  0/391]	Time  0.174 ( 0.174)	Data  0.144 ( 0.144)	Loss 4.5850e-01 (4.5850e-01)	Acc@1  85.94 ( 85.94)	Acc@5  97.66 ( 97.66)
Epoch: [48][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.015)	Loss 3.6621e-01 (3.8417e-01)	Acc@1  86.72 ( 87.93)	Acc@5  99.22 ( 98.51)
Epoch: [48][ 20/391]	Time  0.027 ( 0.032)	Data  0.000 ( 0.009)	Loss 3.8867e-01 (3.6943e-01)	Acc@1  88.28 ( 88.84)	Acc@5 100.00 ( 98.85)
Epoch: [48][ 30/391]	Time  0.021 ( 0.029)	Data  0.002 ( 0.006)	Loss 5.0977e-01 (3.9419e-01)	Acc@1  85.94 ( 88.13)	Acc@5  99.22 ( 98.56)
Epoch: [48][ 40/391]	Time  0.021 ( 0.028)	Data  0.000 ( 0.006)	Loss 4.3115e-01 (3.9942e-01)	Acc@1  85.16 ( 88.00)	Acc@5  98.44 ( 98.36)
Epoch: [48][ 50/391]	Time  0.036 ( 0.028)	Data  0.003 ( 0.005)	Loss 2.7759e-01 (3.9756e-01)	Acc@1  89.06 ( 87.76)	Acc@5  99.22 ( 98.39)
Epoch: [48][ 60/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.0723e-01 (4.0422e-01)	Acc@1  87.50 ( 87.44)	Acc@5  96.88 ( 98.41)
Epoch: [48][ 70/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 4.1528e-01 (4.1143e-01)	Acc@1  85.94 ( 87.20)	Acc@5  96.88 ( 98.33)
Epoch: [48][ 80/391]	Time  0.030 ( 0.027)	Data  0.000 ( 0.004)	Loss 4.6143e-01 (4.1341e-01)	Acc@1  81.25 ( 87.09)	Acc@5  96.88 ( 98.25)
Epoch: [48][ 90/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.8633e-01 (4.1751e-01)	Acc@1  82.03 ( 86.92)	Acc@5  96.88 ( 98.30)
Epoch: [48][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0264e-01 (4.1609e-01)	Acc@1  92.97 ( 87.05)	Acc@5 100.00 ( 98.28)
Epoch: [48][110/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.0649e-01 (4.1936e-01)	Acc@1  85.94 ( 86.96)	Acc@5  99.22 ( 98.28)
Epoch: [48][120/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.0503e-01 (4.1892e-01)	Acc@1  89.06 ( 87.03)	Acc@5  98.44 ( 98.27)
Epoch: [48][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.6934e-01 (4.2029e-01)	Acc@1  83.59 ( 87.06)	Acc@5  97.66 ( 98.26)
Epoch: [48][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9868e-01 (4.2241e-01)	Acc@1  83.59 ( 87.00)	Acc@5  98.44 ( 98.28)
Epoch: [48][150/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.0991e-01 (4.2045e-01)	Acc@1  82.81 ( 87.01)	Acc@5  98.44 ( 98.31)
Epoch: [48][160/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.4863e-01 (4.2187e-01)	Acc@1  87.50 ( 86.89)	Acc@5  99.22 ( 98.32)
Epoch: [48][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6035e-01 (4.1912e-01)	Acc@1  90.62 ( 86.98)	Acc@5  97.66 ( 98.34)
Epoch: [48][180/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6523e-01 (4.2054e-01)	Acc@1  87.50 ( 86.95)	Acc@5  99.22 ( 98.33)
Epoch: [48][190/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.7681e-01 (4.2096e-01)	Acc@1  84.38 ( 86.90)	Acc@5  97.66 ( 98.32)
Epoch: [48][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5278e-01 (4.2133e-01)	Acc@1  86.72 ( 86.91)	Acc@5  99.22 ( 98.29)
Epoch: [48][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4082e-01 (4.2309e-01)	Acc@1  89.06 ( 86.90)	Acc@5  99.22 ( 98.26)
Epoch: [48][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4280e-01 (4.2344e-01)	Acc@1  91.41 ( 86.88)	Acc@5 100.00 ( 98.26)
Epoch: [48][230/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.0503e-01 (4.2359e-01)	Acc@1  85.16 ( 86.86)	Acc@5  99.22 ( 98.25)
Epoch: [48][240/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.3613e-01 (4.2371e-01)	Acc@1  81.25 ( 86.78)	Acc@5  97.66 ( 98.25)
Epoch: [48][250/391]	Time  0.041 ( 0.026)	Data  0.005 ( 0.003)	Loss 4.5410e-01 (4.2277e-01)	Acc@1  85.94 ( 86.78)	Acc@5  97.66 ( 98.26)
Epoch: [48][260/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.2065e-01 (4.2423e-01)	Acc@1  88.28 ( 86.72)	Acc@5  97.66 ( 98.28)
Epoch: [48][270/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4287e-01 (4.2537e-01)	Acc@1  83.59 ( 86.72)	Acc@5  98.44 ( 98.27)
Epoch: [48][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5386e-01 (4.2491e-01)	Acc@1  85.16 ( 86.76)	Acc@5  99.22 ( 98.27)
Epoch: [48][290/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7778e-01 (4.2619e-01)	Acc@1  84.38 ( 86.72)	Acc@5  99.22 ( 98.26)
Epoch: [48][300/391]	Time  0.038 ( 0.026)	Data  0.003 ( 0.003)	Loss 4.6216e-01 (4.2668e-01)	Acc@1  86.72 ( 86.73)	Acc@5  97.66 ( 98.26)
Epoch: [48][310/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2764e-01 (4.2707e-01)	Acc@1  88.28 ( 86.71)	Acc@5  99.22 ( 98.26)
Epoch: [48][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0957e-01 (4.2695e-01)	Acc@1  90.62 ( 86.70)	Acc@5  99.22 ( 98.26)
Epoch: [48][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9771e-01 (4.2737e-01)	Acc@1  87.50 ( 86.66)	Acc@5  98.44 ( 98.25)
Epoch: [48][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.8643e-01 (4.2870e-01)	Acc@1  82.03 ( 86.63)	Acc@5  97.66 ( 98.25)
Epoch: [48][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9810e-01 (4.2906e-01)	Acc@1  92.97 ( 86.64)	Acc@5  99.22 ( 98.25)
Epoch: [48][360/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.0479e-01 (4.2900e-01)	Acc@1  86.72 ( 86.63)	Acc@5  99.22 ( 98.26)
Epoch: [48][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.5322e-01 (4.3102e-01)	Acc@1  83.59 ( 86.58)	Acc@5  96.88 ( 98.22)
Epoch: [48][380/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.0781e-01 (4.3155e-01)	Acc@1  84.38 ( 86.59)	Acc@5  97.66 ( 98.21)
Epoch: [48][390/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.6353e-01 (4.3091e-01)	Acc@1  88.75 ( 86.61)	Acc@5  96.25 ( 98.21)
## e[48] optimizer.zero_grad (sum) time: 0.10552668571472168
## e[48]       loss.backward (sum) time: 2.2965304851531982
## e[48]      optimizer.step (sum) time: 0.9266693592071533
## epoch[48] training(only) time: 10.09348464012146
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.2197e+00 (1.2197e+00)	Acc@1  69.00 ( 69.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.010 ( 0.024)	Loss 1.3643e+00 (1.4331e+00)	Acc@1  64.00 ( 66.73)	Acc@5  93.00 ( 88.73)
Test: [ 20/100]	Time  0.015 ( 0.020)	Loss 1.5508e+00 (1.4309e+00)	Acc@1  66.00 ( 66.38)	Acc@5  89.00 ( 88.38)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.4004e+00 (1.4372e+00)	Acc@1  63.00 ( 65.61)	Acc@5  85.00 ( 88.29)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 1.3730e+00 (1.4257e+00)	Acc@1  68.00 ( 65.93)	Acc@5  90.00 ( 88.61)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.5596e+00 (1.4521e+00)	Acc@1  65.00 ( 65.86)	Acc@5  88.00 ( 88.43)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.5469e+00 (1.4317e+00)	Acc@1  64.00 ( 66.20)	Acc@5  89.00 ( 88.82)
Test: [ 70/100]	Time  0.018 ( 0.015)	Loss 1.4365e+00 (1.4351e+00)	Acc@1  69.00 ( 66.51)	Acc@5  87.00 ( 88.73)
Test: [ 80/100]	Time  0.011 ( 0.015)	Loss 1.6289e+00 (1.4383e+00)	Acc@1  65.00 ( 66.46)	Acc@5  86.00 ( 88.56)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 1.7500e+00 (1.4281e+00)	Acc@1  58.00 ( 66.71)	Acc@5  88.00 ( 88.74)
 * Acc@1 66.740 Acc@5 88.800
### epoch[48] execution time: 11.670040845870972
EPOCH 49
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [49][  0/391]	Time  0.175 ( 0.175)	Data  0.145 ( 0.145)	Loss 2.9810e-01 (2.9810e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [49][ 10/391]	Time  0.026 ( 0.039)	Data  0.001 ( 0.015)	Loss 4.1968e-01 (4.0071e-01)	Acc@1  85.16 ( 87.14)	Acc@5 100.00 ( 98.65)
Epoch: [49][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.8477e-01 (3.8283e-01)	Acc@1  87.50 ( 87.50)	Acc@5  97.66 ( 98.74)
Epoch: [49][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 3.1934e-01 (3.7237e-01)	Acc@1  87.50 ( 87.90)	Acc@5  99.22 ( 98.74)
Epoch: [49][ 40/391]	Time  0.020 ( 0.028)	Data  0.002 ( 0.005)	Loss 2.6025e-01 (3.7613e-01)	Acc@1  91.41 ( 87.60)	Acc@5 100.00 ( 98.78)
Epoch: [49][ 50/391]	Time  0.030 ( 0.028)	Data  0.000 ( 0.005)	Loss 4.8340e-01 (3.9031e-01)	Acc@1  88.28 ( 87.38)	Acc@5  96.09 ( 98.59)
Epoch: [49][ 60/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.2178e-01 (3.9161e-01)	Acc@1  91.41 ( 87.47)	Acc@5 100.00 ( 98.58)
Epoch: [49][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.0601e-01 (3.9136e-01)	Acc@1  88.28 ( 87.47)	Acc@5  97.66 ( 98.54)
Epoch: [49][ 80/391]	Time  0.022 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.3667e-01 (3.9043e-01)	Acc@1  90.62 ( 87.54)	Acc@5  97.66 ( 98.50)
Epoch: [49][ 90/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.7925e-01 (3.9272e-01)	Acc@1  85.94 ( 87.63)	Acc@5  96.09 ( 98.44)
Epoch: [49][100/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.6118e-01 (3.9839e-01)	Acc@1  85.94 ( 87.45)	Acc@5  96.88 ( 98.38)
Epoch: [49][110/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 5.3564e-01 (3.9949e-01)	Acc@1  81.25 ( 87.37)	Acc@5  96.88 ( 98.39)
Epoch: [49][120/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.5913e-01 (4.0472e-01)	Acc@1  90.62 ( 87.29)	Acc@5  97.66 ( 98.35)
Epoch: [49][130/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.003)	Loss 5.5664e-01 (4.0681e-01)	Acc@1  82.03 ( 87.16)	Acc@5  97.66 ( 98.37)
Epoch: [49][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.2842e-01 (4.1052e-01)	Acc@1  85.94 ( 87.08)	Acc@5  94.53 ( 98.31)
Epoch: [49][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8818e-01 (4.1159e-01)	Acc@1  88.28 ( 87.06)	Acc@5  98.44 ( 98.32)
Epoch: [49][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5181e-01 (4.0956e-01)	Acc@1  89.84 ( 87.13)	Acc@5  99.22 ( 98.35)
Epoch: [49][170/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4644e-01 (4.0665e-01)	Acc@1  87.50 ( 87.18)	Acc@5  99.22 ( 98.40)
Epoch: [49][180/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2627e-01 (4.0808e-01)	Acc@1  86.72 ( 87.17)	Acc@5  97.66 ( 98.38)
Epoch: [49][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6411e-01 (4.0896e-01)	Acc@1  89.06 ( 87.20)	Acc@5  98.44 ( 98.38)
Epoch: [49][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2520e-01 (4.0680e-01)	Acc@1  90.62 ( 87.32)	Acc@5  96.88 ( 98.37)
Epoch: [49][210/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4937e-01 (4.0708e-01)	Acc@1  89.06 ( 87.33)	Acc@5  99.22 ( 98.37)
Epoch: [49][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4092e-01 (4.0662e-01)	Acc@1  85.16 ( 87.34)	Acc@5  99.22 ( 98.37)
Epoch: [49][230/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6929e-01 (4.0730e-01)	Acc@1  89.84 ( 87.32)	Acc@5 100.00 ( 98.36)
Epoch: [49][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0127e-01 (4.0578e-01)	Acc@1  90.62 ( 87.34)	Acc@5 100.00 ( 98.39)
Epoch: [49][250/391]	Time  0.039 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.7534e-01 (4.0661e-01)	Acc@1  85.94 ( 87.26)	Acc@5  98.44 ( 98.40)
Epoch: [49][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3408e-01 (4.0630e-01)	Acc@1  83.59 ( 87.27)	Acc@5 100.00 ( 98.41)
Epoch: [49][270/391]	Time  0.039 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.2139e-01 (4.0676e-01)	Acc@1  89.06 ( 87.27)	Acc@5  97.66 ( 98.40)
Epoch: [49][280/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.9561e-01 (4.0773e-01)	Acc@1  84.38 ( 87.23)	Acc@5  96.09 ( 98.38)
Epoch: [49][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.8477e-01 (4.0732e-01)	Acc@1  86.72 ( 87.26)	Acc@5  97.66 ( 98.37)
Epoch: [49][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2383e-01 (4.0781e-01)	Acc@1  84.38 ( 87.22)	Acc@5  98.44 ( 98.38)
Epoch: [49][310/391]	Time  0.048 ( 0.026)	Data  0.009 ( 0.002)	Loss 4.3848e-01 (4.0776e-01)	Acc@1  85.94 ( 87.24)	Acc@5  99.22 ( 98.38)
Epoch: [49][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.3564e-01 (4.0884e-01)	Acc@1  82.03 ( 87.18)	Acc@5  97.66 ( 98.38)
Epoch: [49][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.7803e-01 (4.0989e-01)	Acc@1  84.38 ( 87.13)	Acc@5  97.66 ( 98.37)
Epoch: [49][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9346e-01 (4.0969e-01)	Acc@1  89.06 ( 87.14)	Acc@5  99.22 ( 98.37)
Epoch: [49][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.6411e-01 (4.1024e-01)	Acc@1  85.94 ( 87.12)	Acc@5  96.09 ( 98.36)
Epoch: [49][360/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 5.5908e-01 (4.0994e-01)	Acc@1  82.03 ( 87.16)	Acc@5  96.09 ( 98.35)
Epoch: [49][370/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.0356e-01 (4.0892e-01)	Acc@1  90.62 ( 87.21)	Acc@5  99.22 ( 98.37)
Epoch: [49][380/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2383e-01 (4.0923e-01)	Acc@1  89.84 ( 87.21)	Acc@5  99.22 ( 98.37)
Epoch: [49][390/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.5176e-01 (4.1028e-01)	Acc@1  85.00 ( 87.17)	Acc@5  98.75 ( 98.37)
## e[49] optimizer.zero_grad (sum) time: 0.10461139678955078
## e[49]       loss.backward (sum) time: 2.2849338054656982
## e[49]      optimizer.step (sum) time: 0.9260802268981934
## epoch[49] training(only) time: 10.14363431930542
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.2666e+00 (1.2666e+00)	Acc@1  70.00 ( 70.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.019 ( 0.027)	Loss 1.3779e+00 (1.4609e+00)	Acc@1  61.00 ( 66.82)	Acc@5  92.00 ( 88.64)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.6143e+00 (1.4463e+00)	Acc@1  65.00 ( 66.90)	Acc@5  90.00 ( 88.57)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.3838e+00 (1.4399e+00)	Acc@1  67.00 ( 66.29)	Acc@5  86.00 ( 88.45)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.4961e+00 (1.4404e+00)	Acc@1  64.00 ( 66.24)	Acc@5  88.00 ( 88.44)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 1.5664e+00 (1.4605e+00)	Acc@1  66.00 ( 66.22)	Acc@5  86.00 ( 88.18)
Test: [ 60/100]	Time  0.018 ( 0.016)	Loss 1.5439e+00 (1.4454e+00)	Acc@1  65.00 ( 66.48)	Acc@5  90.00 ( 88.49)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 1.4004e+00 (1.4468e+00)	Acc@1  69.00 ( 66.58)	Acc@5  91.00 ( 88.58)
Test: [ 80/100]	Time  0.015 ( 0.016)	Loss 1.5840e+00 (1.4519e+00)	Acc@1  64.00 ( 66.51)	Acc@5  85.00 ( 88.42)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 1.7988e+00 (1.4396e+00)	Acc@1  59.00 ( 66.70)	Acc@5  88.00 ( 88.60)
 * Acc@1 66.580 Acc@5 88.610
### epoch[49] execution time: 11.753871202468872
EPOCH 50
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [50][  0/391]	Time  0.184 ( 0.184)	Data  0.148 ( 0.148)	Loss 3.7085e-01 (3.7085e-01)	Acc@1  89.84 ( 89.84)	Acc@5  98.44 ( 98.44)
Epoch: [50][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.016)	Loss 3.1714e-01 (3.8967e-01)	Acc@1  89.06 ( 88.00)	Acc@5 100.00 ( 98.58)
Epoch: [50][ 20/391]	Time  0.025 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.7817e-01 (3.9572e-01)	Acc@1  85.94 ( 87.69)	Acc@5  99.22 ( 98.29)
Epoch: [50][ 30/391]	Time  0.020 ( 0.030)	Data  0.001 ( 0.007)	Loss 4.5630e-01 (3.8974e-01)	Acc@1  85.16 ( 87.68)	Acc@5  98.44 ( 98.34)
Epoch: [50][ 40/391]	Time  0.030 ( 0.029)	Data  0.001 ( 0.006)	Loss 4.7266e-01 (3.9582e-01)	Acc@1  84.38 ( 87.56)	Acc@5  97.66 ( 98.30)
Epoch: [50][ 50/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.0796e-01 (4.0102e-01)	Acc@1  89.06 ( 87.33)	Acc@5  98.44 ( 98.24)
Epoch: [50][ 60/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.3408e-01 (3.9964e-01)	Acc@1  88.28 ( 87.46)	Acc@5 100.00 ( 98.26)
Epoch: [50][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5537e-01 (3.9997e-01)	Acc@1  94.53 ( 87.46)	Acc@5  99.22 ( 98.28)
Epoch: [50][ 80/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.9858e-01 (4.0158e-01)	Acc@1  92.19 ( 87.48)	Acc@5  99.22 ( 98.25)
Epoch: [50][ 90/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.2275e-01 (3.9716e-01)	Acc@1  91.41 ( 87.71)	Acc@5  99.22 ( 98.30)
Epoch: [50][100/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8159e-01 (3.9582e-01)	Acc@1  85.94 ( 87.71)	Acc@5  97.66 ( 98.31)
Epoch: [50][110/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.8804e-01 (4.0066e-01)	Acc@1  86.72 ( 87.61)	Acc@5  98.44 ( 98.31)
Epoch: [50][120/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.1675e-01 (4.0200e-01)	Acc@1  86.72 ( 87.58)	Acc@5  98.44 ( 98.29)
Epoch: [50][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5659e-01 (3.9879e-01)	Acc@1  91.41 ( 87.71)	Acc@5 100.00 ( 98.30)
Epoch: [50][140/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1465e-01 (4.0065e-01)	Acc@1  82.03 ( 87.65)	Acc@5  96.88 ( 98.29)
Epoch: [50][150/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9297e-01 (3.9851e-01)	Acc@1  89.84 ( 87.69)	Acc@5 100.00 ( 98.33)
Epoch: [50][160/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9502e-01 (3.9559e-01)	Acc@1  86.72 ( 87.75)	Acc@5  98.44 ( 98.36)
Epoch: [50][170/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2178e-01 (3.9431e-01)	Acc@1  90.62 ( 87.79)	Acc@5  98.44 ( 98.37)
Epoch: [50][180/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.8354e-01 (3.9657e-01)	Acc@1  85.16 ( 87.74)	Acc@5  98.44 ( 98.36)
Epoch: [50][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.9121e-01 (3.9469e-01)	Acc@1  83.59 ( 87.74)	Acc@5  98.44 ( 98.38)
Epoch: [50][200/391]	Time  0.047 ( 0.026)	Data  0.011 ( 0.003)	Loss 4.7290e-01 (3.9482e-01)	Acc@1  88.28 ( 87.76)	Acc@5  97.66 ( 98.37)
Epoch: [50][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6973e-01 (3.9559e-01)	Acc@1  88.28 ( 87.72)	Acc@5  96.88 ( 98.38)
Epoch: [50][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5229e-01 (3.9530e-01)	Acc@1  90.62 ( 87.75)	Acc@5  98.44 ( 98.40)
Epoch: [50][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6987e-01 (3.9527e-01)	Acc@1  87.50 ( 87.76)	Acc@5  98.44 ( 98.40)
Epoch: [50][240/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2642e-01 (3.9508e-01)	Acc@1  88.28 ( 87.74)	Acc@5  99.22 ( 98.40)
Epoch: [50][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.7031e-01 (3.9459e-01)	Acc@1  82.03 ( 87.75)	Acc@5  95.31 ( 98.39)
Epoch: [50][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6133e-01 (3.9430e-01)	Acc@1  85.16 ( 87.72)	Acc@5  99.22 ( 98.40)
Epoch: [50][270/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3164e-01 (3.9513e-01)	Acc@1  88.28 ( 87.72)	Acc@5  97.66 ( 98.39)
Epoch: [50][280/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.0527e-01 (3.9417e-01)	Acc@1  88.28 ( 87.74)	Acc@5  97.66 ( 98.39)
Epoch: [50][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.9561e-01 (3.9513e-01)	Acc@1  86.72 ( 87.70)	Acc@5  97.66 ( 98.39)
Epoch: [50][300/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.9453e-01 (3.9548e-01)	Acc@1  89.06 ( 87.67)	Acc@5  99.22 ( 98.40)
Epoch: [50][310/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.002)	Loss 6.0498e-01 (3.9560e-01)	Acc@1  85.94 ( 87.65)	Acc@5  95.31 ( 98.40)
Epoch: [50][320/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8882e-01 (3.9498e-01)	Acc@1  89.06 ( 87.66)	Acc@5 100.00 ( 98.41)
Epoch: [50][330/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.1382e-01 (3.9708e-01)	Acc@1  85.94 ( 87.59)	Acc@5  99.22 ( 98.40)
Epoch: [50][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.5825e-01 (3.9549e-01)	Acc@1  85.94 ( 87.63)	Acc@5 100.00 ( 98.42)
Epoch: [50][350/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.8745e-01 (3.9612e-01)	Acc@1  90.62 ( 87.62)	Acc@5  97.66 ( 98.41)
Epoch: [50][360/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9062e-01 (3.9758e-01)	Acc@1  84.38 ( 87.58)	Acc@5  98.44 ( 98.40)
Epoch: [50][370/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6182e-01 (3.9814e-01)	Acc@1  87.50 ( 87.54)	Acc@5  99.22 ( 98.39)
Epoch: [50][380/391]	Time  0.039 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.4961e-01 (3.9744e-01)	Acc@1  92.19 ( 87.58)	Acc@5  98.44 ( 98.41)
Epoch: [50][390/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.3774e-01 (3.9709e-01)	Acc@1  87.50 ( 87.61)	Acc@5  98.75 ( 98.42)
## e[50] optimizer.zero_grad (sum) time: 0.10527253150939941
## e[50]       loss.backward (sum) time: 2.2239344120025635
## e[50]      optimizer.step (sum) time: 0.9142732620239258
## epoch[50] training(only) time: 10.148002862930298
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.2266e+00 (1.2266e+00)	Acc@1  74.00 ( 74.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.020 ( 0.026)	Loss 1.4385e+00 (1.4829e+00)	Acc@1  61.00 ( 66.36)	Acc@5  91.00 ( 88.27)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.5400e+00 (1.4550e+00)	Acc@1  67.00 ( 66.38)	Acc@5  90.00 ( 88.52)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 1.5146e+00 (1.4689e+00)	Acc@1  60.00 ( 65.65)	Acc@5  84.00 ( 88.35)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.5674e+00 (1.4727e+00)	Acc@1  64.00 ( 65.76)	Acc@5  90.00 ( 88.32)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5117e+00 (1.4931e+00)	Acc@1  66.00 ( 65.43)	Acc@5  87.00 ( 88.35)
Test: [ 60/100]	Time  0.024 ( 0.016)	Loss 1.6543e+00 (1.4762e+00)	Acc@1  63.00 ( 65.77)	Acc@5  89.00 ( 88.54)
Test: [ 70/100]	Time  0.011 ( 0.015)	Loss 1.4434e+00 (1.4760e+00)	Acc@1  68.00 ( 65.97)	Acc@5  90.00 ( 88.69)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.5723e+00 (1.4811e+00)	Acc@1  62.00 ( 65.88)	Acc@5  86.00 ( 88.56)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.7607e+00 (1.4697e+00)	Acc@1  61.00 ( 66.20)	Acc@5  88.00 ( 88.65)
 * Acc@1 66.330 Acc@5 88.630
### epoch[50] execution time: 11.723920583724976
EPOCH 51
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [51][  0/391]	Time  0.181 ( 0.181)	Data  0.153 ( 0.153)	Loss 4.3018e-01 (4.3018e-01)	Acc@1  86.72 ( 86.72)	Acc@5  97.66 ( 97.66)
Epoch: [51][ 10/391]	Time  0.024 ( 0.039)	Data  0.001 ( 0.016)	Loss 3.8696e-01 (3.9077e-01)	Acc@1  85.94 ( 87.36)	Acc@5  99.22 ( 98.86)
Epoch: [51][ 20/391]	Time  0.027 ( 0.033)	Data  0.001 ( 0.009)	Loss 5.4102e-01 (3.7612e-01)	Acc@1  81.25 ( 87.76)	Acc@5  99.22 ( 98.88)
Epoch: [51][ 30/391]	Time  0.018 ( 0.030)	Data  0.000 ( 0.007)	Loss 3.3813e-01 (3.6577e-01)	Acc@1  86.72 ( 88.16)	Acc@5  99.22 ( 98.94)
Epoch: [51][ 40/391]	Time  0.037 ( 0.029)	Data  0.002 ( 0.005)	Loss 3.3447e-01 (3.6784e-01)	Acc@1  90.62 ( 88.19)	Acc@5  98.44 ( 98.82)
Epoch: [51][ 50/391]	Time  0.037 ( 0.028)	Data  0.005 ( 0.005)	Loss 3.8525e-01 (3.7259e-01)	Acc@1  86.72 ( 88.16)	Acc@5  99.22 ( 98.76)
Epoch: [51][ 60/391]	Time  0.035 ( 0.028)	Data  0.001 ( 0.004)	Loss 4.7046e-01 (3.8016e-01)	Acc@1  83.59 ( 87.87)	Acc@5  97.66 ( 98.67)
Epoch: [51][ 70/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.0151e-01 (3.7757e-01)	Acc@1  90.62 ( 87.89)	Acc@5 100.00 ( 98.66)
Epoch: [51][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8125e-01 (3.8295e-01)	Acc@1  90.62 ( 87.68)	Acc@5 100.00 ( 98.58)
Epoch: [51][ 90/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8442e-01 (3.7855e-01)	Acc@1  91.41 ( 87.95)	Acc@5  99.22 ( 98.59)
Epoch: [51][100/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6782e-01 (3.7823e-01)	Acc@1  90.62 ( 87.95)	Acc@5 100.00 ( 98.62)
Epoch: [51][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.2017e-01 (3.7983e-01)	Acc@1  88.28 ( 87.98)	Acc@5  98.44 ( 98.57)
Epoch: [51][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4727e-01 (3.7942e-01)	Acc@1  84.38 ( 87.98)	Acc@5 100.00 ( 98.60)
Epoch: [51][130/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.8491e-01 (3.7653e-01)	Acc@1  92.97 ( 88.05)	Acc@5  99.22 ( 98.62)
Epoch: [51][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5645e-01 (3.7510e-01)	Acc@1  89.84 ( 88.13)	Acc@5  96.88 ( 98.60)
Epoch: [51][150/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0820e-01 (3.7742e-01)	Acc@1  87.50 ( 88.05)	Acc@5  98.44 ( 98.58)
Epoch: [51][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1660e-01 (3.7872e-01)	Acc@1  81.25 ( 87.99)	Acc@5  99.22 ( 98.61)
Epoch: [51][170/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9331e-01 (3.7777e-01)	Acc@1  87.50 ( 88.02)	Acc@5  99.22 ( 98.61)
Epoch: [51][180/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1016e-01 (3.8076e-01)	Acc@1  88.28 ( 87.93)	Acc@5  99.22 ( 98.58)
Epoch: [51][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3301e-01 (3.8208e-01)	Acc@1  89.84 ( 87.91)	Acc@5  98.44 ( 98.58)
Epoch: [51][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2334e-01 (3.8311e-01)	Acc@1  83.59 ( 87.88)	Acc@5  99.22 ( 98.57)
Epoch: [51][210/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3716e-01 (3.8161e-01)	Acc@1  90.62 ( 87.94)	Acc@5  99.22 ( 98.59)
Epoch: [51][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8486e-01 (3.8239e-01)	Acc@1  81.25 ( 87.90)	Acc@5  97.66 ( 98.58)
Epoch: [51][230/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.0405e-01 (3.8028e-01)	Acc@1  88.28 ( 87.99)	Acc@5  99.22 ( 98.59)
Epoch: [51][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7158e-01 (3.7925e-01)	Acc@1  85.94 ( 88.00)	Acc@5  99.22 ( 98.60)
Epoch: [51][250/391]	Time  0.036 ( 0.026)	Data  0.005 ( 0.003)	Loss 3.8452e-01 (3.7994e-01)	Acc@1  86.72 ( 87.98)	Acc@5  98.44 ( 98.59)
Epoch: [51][260/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6460e-01 (3.7965e-01)	Acc@1  82.81 ( 87.96)	Acc@5  96.09 ( 98.59)
Epoch: [51][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1211e-01 (3.7839e-01)	Acc@1  82.03 ( 87.98)	Acc@5  98.44 ( 98.60)
Epoch: [51][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5562e-01 (3.7961e-01)	Acc@1  92.97 ( 87.96)	Acc@5  99.22 ( 98.59)
Epoch: [51][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9258e-01 (3.7962e-01)	Acc@1  82.81 ( 87.93)	Acc@5  99.22 ( 98.60)
Epoch: [51][300/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.8574e-01 (3.7962e-01)	Acc@1  87.50 ( 87.89)	Acc@5  98.44 ( 98.61)
Epoch: [51][310/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.7939e-01 (3.7995e-01)	Acc@1  85.94 ( 87.90)	Acc@5  98.44 ( 98.62)
Epoch: [51][320/391]	Time  0.039 ( 0.026)	Data  0.005 ( 0.002)	Loss 3.9893e-01 (3.8038e-01)	Acc@1  88.28 ( 87.91)	Acc@5 100.00 ( 98.62)
Epoch: [51][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3618e-01 (3.8202e-01)	Acc@1  89.84 ( 87.89)	Acc@5 100.00 ( 98.61)
Epoch: [51][340/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1777e-01 (3.8193e-01)	Acc@1  93.75 ( 87.90)	Acc@5 100.00 ( 98.60)
Epoch: [51][350/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8027e-01 (3.8102e-01)	Acc@1  91.41 ( 87.93)	Acc@5 100.00 ( 98.62)
Epoch: [51][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2188e-01 (3.8187e-01)	Acc@1  85.94 ( 87.93)	Acc@5  99.22 ( 98.61)
Epoch: [51][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.6729e-01 (3.8217e-01)	Acc@1  85.16 ( 87.93)	Acc@5  98.44 ( 98.60)
Epoch: [51][380/391]	Time  0.025 ( 0.026)	Data  0.004 ( 0.002)	Loss 3.8599e-01 (3.8238e-01)	Acc@1  85.94 ( 87.92)	Acc@5  97.66 ( 98.60)
Epoch: [51][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.4883e-01 (3.8279e-01)	Acc@1  86.25 ( 87.91)	Acc@5  98.75 ( 98.59)
## e[51] optimizer.zero_grad (sum) time: 0.10414242744445801
## e[51]       loss.backward (sum) time: 2.2663283348083496
## e[51]      optimizer.step (sum) time: 0.9129340648651123
## epoch[51] training(only) time: 10.158073902130127
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.2236e+00 (1.2236e+00)	Acc@1  74.00 ( 74.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.012 ( 0.026)	Loss 1.4707e+00 (1.4724e+00)	Acc@1  64.00 ( 67.36)	Acc@5  90.00 ( 88.36)
Test: [ 20/100]	Time  0.020 ( 0.020)	Loss 1.4473e+00 (1.4524e+00)	Acc@1  70.00 ( 66.90)	Acc@5  89.00 ( 88.48)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.3965e+00 (1.4594e+00)	Acc@1  65.00 ( 66.29)	Acc@5  89.00 ( 88.45)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 1.6025e+00 (1.4673e+00)	Acc@1  64.00 ( 66.17)	Acc@5  89.00 ( 88.54)
Test: [ 50/100]	Time  0.020 ( 0.016)	Loss 1.5498e+00 (1.4900e+00)	Acc@1  66.00 ( 65.82)	Acc@5  87.00 ( 88.25)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.5498e+00 (1.4711e+00)	Acc@1  66.00 ( 66.15)	Acc@5  90.00 ( 88.52)
Test: [ 70/100]	Time  0.014 ( 0.016)	Loss 1.5654e+00 (1.4782e+00)	Acc@1  65.00 ( 66.25)	Acc@5  88.00 ( 88.42)
Test: [ 80/100]	Time  0.020 ( 0.016)	Loss 1.6094e+00 (1.4831e+00)	Acc@1  64.00 ( 66.22)	Acc@5  85.00 ( 88.36)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.7861e+00 (1.4733e+00)	Acc@1  58.00 ( 66.34)	Acc@5  89.00 ( 88.58)
 * Acc@1 66.400 Acc@5 88.560
### epoch[51] execution time: 11.752108335494995
EPOCH 52
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [52][  0/391]	Time  0.168 ( 0.168)	Data  0.141 ( 0.141)	Loss 2.9126e-01 (2.9126e-01)	Acc@1  89.84 ( 89.84)	Acc@5 100.00 (100.00)
Epoch: [52][ 10/391]	Time  0.021 ( 0.039)	Data  0.002 ( 0.014)	Loss 2.3730e-01 (3.5793e-01)	Acc@1  91.41 ( 87.50)	Acc@5 100.00 ( 99.43)
Epoch: [52][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 3.1299e-01 (3.5546e-01)	Acc@1  91.41 ( 88.17)	Acc@5  99.22 ( 99.22)
Epoch: [52][ 30/391]	Time  0.022 ( 0.030)	Data  0.001 ( 0.006)	Loss 4.4580e-01 (3.5398e-01)	Acc@1  85.16 ( 88.36)	Acc@5 100.00 ( 99.19)
Epoch: [52][ 40/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.005)	Loss 4.9878e-01 (3.6112e-01)	Acc@1  85.16 ( 88.24)	Acc@5  96.88 ( 99.09)
Epoch: [52][ 50/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.5815e-01 (3.6263e-01)	Acc@1  85.16 ( 88.17)	Acc@5 100.00 ( 99.13)
Epoch: [52][ 60/391]	Time  0.027 ( 0.028)	Data  0.000 ( 0.004)	Loss 4.2676e-01 (3.6031e-01)	Acc@1  88.28 ( 88.40)	Acc@5  98.44 ( 99.07)
Epoch: [52][ 70/391]	Time  0.030 ( 0.027)	Data  0.003 ( 0.004)	Loss 6.0938e-01 (3.7143e-01)	Acc@1  85.16 ( 88.19)	Acc@5  95.31 ( 98.95)
Epoch: [52][ 80/391]	Time  0.029 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.5840e-01 (3.7262e-01)	Acc@1  88.28 ( 88.18)	Acc@5  98.44 ( 98.90)
Epoch: [52][ 90/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1089e-01 (3.6785e-01)	Acc@1  85.94 ( 88.26)	Acc@5  99.22 ( 98.95)
Epoch: [52][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1675e-01 (3.6752e-01)	Acc@1  83.59 ( 88.23)	Acc@5  99.22 ( 98.98)
Epoch: [52][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3472e-01 (3.6899e-01)	Acc@1  89.06 ( 88.21)	Acc@5  99.22 ( 98.92)
Epoch: [52][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7656e-01 (3.6965e-01)	Acc@1  82.81 ( 88.15)	Acc@5  98.44 ( 98.86)
Epoch: [52][130/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4448e-01 (3.6373e-01)	Acc@1  88.28 ( 88.34)	Acc@5 100.00 ( 98.90)
Epoch: [52][140/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4009e-01 (3.6520e-01)	Acc@1  88.28 ( 88.35)	Acc@5  98.44 ( 98.86)
Epoch: [52][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1201e-01 (3.6343e-01)	Acc@1  92.97 ( 88.41)	Acc@5  98.44 ( 98.85)
Epoch: [52][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3335e-01 (3.6338e-01)	Acc@1  85.94 ( 88.40)	Acc@5  99.22 ( 98.87)
Epoch: [52][170/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5864e-01 (3.6465e-01)	Acc@1  89.06 ( 88.35)	Acc@5  99.22 ( 98.87)
Epoch: [52][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9028e-01 (3.6501e-01)	Acc@1  92.97 ( 88.36)	Acc@5  98.44 ( 98.84)
Epoch: [52][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 6.3086e-01 (3.6774e-01)	Acc@1  83.59 ( 88.25)	Acc@5  96.09 ( 98.83)
Epoch: [52][200/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.8296e-01 (3.6821e-01)	Acc@1  89.84 ( 88.19)	Acc@5  97.66 ( 98.84)
Epoch: [52][210/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7964e-01 (3.6852e-01)	Acc@1  89.84 ( 88.17)	Acc@5  98.44 ( 98.85)
Epoch: [52][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1299e-01 (3.6955e-01)	Acc@1  89.84 ( 88.13)	Acc@5  99.22 ( 98.83)
Epoch: [52][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.1113e-01 (3.7112e-01)	Acc@1  89.06 ( 88.10)	Acc@5  96.88 ( 98.82)
Epoch: [52][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.5972e-01 (3.7227e-01)	Acc@1  88.28 ( 88.09)	Acc@5  98.44 ( 98.81)
Epoch: [52][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6099e-01 (3.7205e-01)	Acc@1  92.97 ( 88.09)	Acc@5 100.00 ( 98.81)
Epoch: [52][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.5703e-01 (3.7289e-01)	Acc@1  86.72 ( 88.10)	Acc@5  97.66 ( 98.80)
Epoch: [52][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9941e-01 (3.7347e-01)	Acc@1  89.84 ( 88.08)	Acc@5  97.66 ( 98.78)
Epoch: [52][280/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.6523e-01 (3.7610e-01)	Acc@1  88.28 ( 88.01)	Acc@5  98.44 ( 98.74)
Epoch: [52][290/391]	Time  0.034 ( 0.026)	Data  0.003 ( 0.002)	Loss 6.0303e-01 (3.7750e-01)	Acc@1  78.91 ( 87.99)	Acc@5  96.09 ( 98.73)
Epoch: [52][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.4092e-01 (3.7840e-01)	Acc@1  83.59 ( 87.93)	Acc@5  99.22 ( 98.73)
Epoch: [52][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8589e-01 (3.7796e-01)	Acc@1  91.41 ( 87.96)	Acc@5  99.22 ( 98.72)
Epoch: [52][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.1626e-01 (3.7830e-01)	Acc@1  85.16 ( 87.96)	Acc@5  96.09 ( 98.70)
Epoch: [52][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9917e-01 (3.7700e-01)	Acc@1  87.50 ( 88.00)	Acc@5  99.22 ( 98.71)
Epoch: [52][340/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4058e-01 (3.7713e-01)	Acc@1  90.62 ( 88.02)	Acc@5  98.44 ( 98.71)
Epoch: [52][350/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.6230e-01 (3.7640e-01)	Acc@1  90.62 ( 88.03)	Acc@5  97.66 ( 98.71)
Epoch: [52][360/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.6045e-01 (3.7631e-01)	Acc@1  85.94 ( 88.04)	Acc@5  96.88 ( 98.70)
Epoch: [52][370/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2456e-01 (3.7630e-01)	Acc@1  87.50 ( 88.04)	Acc@5  96.88 ( 98.70)
Epoch: [52][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4985e-01 (3.7617e-01)	Acc@1  89.84 ( 88.06)	Acc@5  99.22 ( 98.70)
Epoch: [52][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3667e-01 (3.7642e-01)	Acc@1  90.00 ( 88.03)	Acc@5  98.75 ( 98.71)
## e[52] optimizer.zero_grad (sum) time: 0.10457825660705566
## e[52]       loss.backward (sum) time: 2.2039201259613037
## e[52]      optimizer.step (sum) time: 0.9187154769897461
## epoch[52] training(only) time: 10.121224403381348
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.3740e+00 (1.3740e+00)	Acc@1  70.00 ( 70.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 1.4453e+00 (1.5229e+00)	Acc@1  67.00 ( 66.82)	Acc@5  89.00 ( 87.18)
Test: [ 20/100]	Time  0.029 ( 0.020)	Loss 1.6172e+00 (1.4779e+00)	Acc@1  68.00 ( 66.52)	Acc@5  89.00 ( 88.10)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.4199e+00 (1.4997e+00)	Acc@1  67.00 ( 65.94)	Acc@5  85.00 ( 87.81)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 1.5371e+00 (1.4895e+00)	Acc@1  67.00 ( 66.05)	Acc@5  89.00 ( 88.10)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.6504e+00 (1.5249e+00)	Acc@1  67.00 ( 65.80)	Acc@5  86.00 ( 87.71)
Test: [ 60/100]	Time  0.030 ( 0.016)	Loss 1.7461e+00 (1.5054e+00)	Acc@1  62.00 ( 65.97)	Acc@5  88.00 ( 88.03)
Test: [ 70/100]	Time  0.015 ( 0.016)	Loss 1.6406e+00 (1.5062e+00)	Acc@1  61.00 ( 66.04)	Acc@5  89.00 ( 88.15)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.5391e+00 (1.5074e+00)	Acc@1  64.00 ( 66.01)	Acc@5  83.00 ( 88.10)
Test: [ 90/100]	Time  0.020 ( 0.015)	Loss 1.7832e+00 (1.4965e+00)	Acc@1  61.00 ( 66.14)	Acc@5  87.00 ( 88.21)
 * Acc@1 66.120 Acc@5 88.170
### epoch[52] execution time: 11.717335224151611
EPOCH 53
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [53][  0/391]	Time  0.173 ( 0.173)	Data  0.148 ( 0.148)	Loss 3.3813e-01 (3.3813e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [53][ 10/391]	Time  0.024 ( 0.038)	Data  0.003 ( 0.016)	Loss 2.8223e-01 (3.5347e-01)	Acc@1  89.06 ( 88.35)	Acc@5  99.22 ( 98.65)
Epoch: [53][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.6426e-01 (3.5558e-01)	Acc@1  87.50 ( 88.36)	Acc@5  98.44 ( 98.70)
Epoch: [53][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 3.2788e-01 (3.3997e-01)	Acc@1  89.84 ( 88.58)	Acc@5  99.22 ( 98.89)
Epoch: [53][ 40/391]	Time  0.030 ( 0.029)	Data  0.000 ( 0.006)	Loss 4.4580e-01 (3.4430e-01)	Acc@1  85.94 ( 88.53)	Acc@5  99.22 ( 98.88)
Epoch: [53][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.2617e-01 (3.3830e-01)	Acc@1  89.84 ( 88.88)	Acc@5  98.44 ( 98.85)
Epoch: [53][ 60/391]	Time  0.020 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.4326e-01 (3.4098e-01)	Acc@1  92.19 ( 88.93)	Acc@5  97.66 ( 98.73)
Epoch: [53][ 70/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.2495e-01 (3.4644e-01)	Acc@1  89.84 ( 88.81)	Acc@5  98.44 ( 98.71)
Epoch: [53][ 80/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.004)	Loss 3.6279e-01 (3.4830e-01)	Acc@1  88.28 ( 88.76)	Acc@5  98.44 ( 98.71)
Epoch: [53][ 90/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.9722e-01 (3.4657e-01)	Acc@1  86.72 ( 88.82)	Acc@5  99.22 ( 98.75)
Epoch: [53][100/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.0566e-01 (3.4619e-01)	Acc@1  89.84 ( 88.92)	Acc@5  99.22 ( 98.72)
Epoch: [53][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0374e-01 (3.4642e-01)	Acc@1  95.31 ( 88.91)	Acc@5 100.00 ( 98.76)
Epoch: [53][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1812e-01 (3.4436e-01)	Acc@1  88.28 ( 89.00)	Acc@5  99.22 ( 98.78)
Epoch: [53][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.4360e-01 (3.4580e-01)	Acc@1  85.94 ( 89.04)	Acc@5  98.44 ( 98.78)
Epoch: [53][140/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0869e-01 (3.4555e-01)	Acc@1  88.28 ( 89.02)	Acc@5  96.88 ( 98.79)
Epoch: [53][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.8193e-01 (3.4762e-01)	Acc@1  83.59 ( 88.95)	Acc@5  98.44 ( 98.78)
Epoch: [53][160/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.2017e-01 (3.5005e-01)	Acc@1  84.38 ( 88.85)	Acc@5  97.66 ( 98.78)
Epoch: [53][170/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2871e-01 (3.5099e-01)	Acc@1  84.38 ( 88.86)	Acc@5  97.66 ( 98.78)
Epoch: [53][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7314e-01 (3.4992e-01)	Acc@1  84.38 ( 88.87)	Acc@5  99.22 ( 98.78)
Epoch: [53][190/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2505e-01 (3.4852e-01)	Acc@1  85.94 ( 88.90)	Acc@5  99.22 ( 98.81)
Epoch: [53][200/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1323e-01 (3.4769e-01)	Acc@1  88.28 ( 88.89)	Acc@5 100.00 ( 98.83)
Epoch: [53][210/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7744e-01 (3.4818e-01)	Acc@1  87.50 ( 88.92)	Acc@5  99.22 ( 98.80)
Epoch: [53][220/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.6816e-01 (3.4768e-01)	Acc@1  88.28 ( 88.90)	Acc@5  97.66 ( 98.78)
Epoch: [53][230/391]	Time  0.043 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9663e-01 (3.4990e-01)	Acc@1  90.62 ( 88.83)	Acc@5  98.44 ( 98.77)
Epoch: [53][240/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.7266e-01 (3.4897e-01)	Acc@1  85.16 ( 88.85)	Acc@5  97.66 ( 98.78)
Epoch: [53][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0713e-01 (3.4952e-01)	Acc@1  92.97 ( 88.84)	Acc@5  97.66 ( 98.77)
Epoch: [53][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1519e-01 (3.4938e-01)	Acc@1  89.84 ( 88.82)	Acc@5 100.00 ( 98.77)
Epoch: [53][270/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5229e-01 (3.5044e-01)	Acc@1  88.28 ( 88.79)	Acc@5 100.00 ( 98.76)
Epoch: [53][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0601e-01 (3.5090e-01)	Acc@1  88.28 ( 88.78)	Acc@5  97.66 ( 98.77)
Epoch: [53][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9575e-01 (3.5160e-01)	Acc@1  89.84 ( 88.76)	Acc@5  99.22 ( 98.75)
Epoch: [53][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3911e-01 (3.5198e-01)	Acc@1  89.06 ( 88.75)	Acc@5  98.44 ( 98.73)
Epoch: [53][310/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5571e-01 (3.5269e-01)	Acc@1  87.50 ( 88.71)	Acc@5  98.44 ( 98.72)
Epoch: [53][320/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.0879e-01 (3.5369e-01)	Acc@1  81.25 ( 88.68)	Acc@5  99.22 ( 98.74)
Epoch: [53][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3325e-01 (3.5506e-01)	Acc@1  87.50 ( 88.69)	Acc@5  99.22 ( 98.71)
Epoch: [53][340/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1079e-01 (3.5459e-01)	Acc@1  91.41 ( 88.69)	Acc@5  98.44 ( 98.72)
Epoch: [53][350/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.5596e-01 (3.5516e-01)	Acc@1  86.72 ( 88.68)	Acc@5  98.44 ( 98.72)
Epoch: [53][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3938e-01 (3.5561e-01)	Acc@1  92.19 ( 88.65)	Acc@5 100.00 ( 98.73)
Epoch: [53][370/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6328e-01 (3.5585e-01)	Acc@1  88.28 ( 88.65)	Acc@5  98.44 ( 98.73)
Epoch: [53][380/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4619e-01 (3.5635e-01)	Acc@1  86.72 ( 88.65)	Acc@5  99.22 ( 98.73)
Epoch: [53][390/391]	Time  0.018 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.0894e-01 (3.5590e-01)	Acc@1  83.75 ( 88.68)	Acc@5 100.00 ( 98.73)
## e[53] optimizer.zero_grad (sum) time: 0.10428977012634277
## e[53]       loss.backward (sum) time: 2.246068239212036
## e[53]      optimizer.step (sum) time: 0.9270238876342773
## epoch[53] training(only) time: 10.090349912643433
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.3555e+00 (1.3555e+00)	Acc@1  71.00 ( 71.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.4062e+00 (1.5138e+00)	Acc@1  65.00 ( 67.00)	Acc@5  89.00 ( 88.00)
Test: [ 20/100]	Time  0.021 ( 0.020)	Loss 1.5410e+00 (1.4964e+00)	Acc@1  71.00 ( 66.71)	Acc@5  90.00 ( 88.24)
Test: [ 30/100]	Time  0.019 ( 0.018)	Loss 1.4834e+00 (1.5054e+00)	Acc@1  63.00 ( 65.97)	Acc@5  85.00 ( 88.06)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 1.5869e+00 (1.5038e+00)	Acc@1  64.00 ( 65.98)	Acc@5  90.00 ( 88.29)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.6709e+00 (1.5299e+00)	Acc@1  66.00 ( 65.82)	Acc@5  87.00 ( 88.14)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.7461e+00 (1.5088e+00)	Acc@1  60.00 ( 66.11)	Acc@5  89.00 ( 88.33)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.6396e+00 (1.5102e+00)	Acc@1  64.00 ( 66.15)	Acc@5  88.00 ( 88.23)
Test: [ 80/100]	Time  0.015 ( 0.015)	Loss 1.5771e+00 (1.5126e+00)	Acc@1  64.00 ( 66.11)	Acc@5  86.00 ( 88.17)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9043e+00 (1.5020e+00)	Acc@1  62.00 ( 66.31)	Acc@5  87.00 ( 88.37)
 * Acc@1 66.500 Acc@5 88.450
### epoch[53] execution time: 11.710465669631958
EPOCH 54
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [54][  0/391]	Time  0.171 ( 0.171)	Data  0.143 ( 0.143)	Loss 3.8647e-01 (3.8647e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [54][ 10/391]	Time  0.022 ( 0.038)	Data  0.001 ( 0.014)	Loss 4.3481e-01 (3.1907e-01)	Acc@1  84.38 ( 90.06)	Acc@5  96.88 ( 98.65)
Epoch: [54][ 20/391]	Time  0.031 ( 0.032)	Data  0.001 ( 0.008)	Loss 4.1406e-01 (3.1125e-01)	Acc@1  90.62 ( 90.18)	Acc@5  96.09 ( 98.85)
Epoch: [54][ 30/391]	Time  0.032 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.0615e-01 (3.1842e-01)	Acc@1  92.19 ( 90.10)	Acc@5  98.44 ( 98.94)
Epoch: [54][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.2798e-01 (3.1327e-01)	Acc@1  89.84 ( 90.30)	Acc@5  98.44 ( 98.99)
Epoch: [54][ 50/391]	Time  0.030 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.3105e-01 (3.2488e-01)	Acc@1  88.28 ( 89.77)	Acc@5  99.22 ( 98.91)
Epoch: [54][ 60/391]	Time  0.037 ( 0.027)	Data  0.003 ( 0.004)	Loss 3.2129e-01 (3.3430e-01)	Acc@1  86.72 ( 89.41)	Acc@5  99.22 ( 98.90)
Epoch: [54][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5024e-01 (3.3628e-01)	Acc@1  92.19 ( 89.28)	Acc@5  99.22 ( 98.93)
Epoch: [54][ 80/391]	Time  0.025 ( 0.027)	Data  0.003 ( 0.004)	Loss 3.2397e-01 (3.3407e-01)	Acc@1  89.06 ( 89.29)	Acc@5  99.22 ( 98.97)
Epoch: [54][ 90/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.2495e-01 (3.3175e-01)	Acc@1  89.84 ( 89.40)	Acc@5 100.00 ( 98.96)
Epoch: [54][100/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6401e-01 (3.3235e-01)	Acc@1  87.50 ( 89.42)	Acc@5  99.22 ( 98.99)
Epoch: [54][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9673e-01 (3.3808e-01)	Acc@1  89.06 ( 89.24)	Acc@5  98.44 ( 98.97)
Epoch: [54][120/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0884e-01 (3.3969e-01)	Acc@1  87.50 ( 89.22)	Acc@5 100.00 ( 98.96)
Epoch: [54][130/391]	Time  0.036 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.5059e-01 (3.4249e-01)	Acc@1  91.41 ( 89.17)	Acc@5  99.22 ( 98.94)
Epoch: [54][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4595e-01 (3.4018e-01)	Acc@1  90.62 ( 89.24)	Acc@5  97.66 ( 98.96)
Epoch: [54][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5645e-01 (3.3887e-01)	Acc@1  90.62 ( 89.26)	Acc@5  98.44 ( 98.98)
Epoch: [54][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.5264e-01 (3.4027e-01)	Acc@1  82.81 ( 89.17)	Acc@5  99.22 ( 98.97)
Epoch: [54][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5879e-01 (3.4100e-01)	Acc@1  91.41 ( 89.15)	Acc@5 100.00 ( 98.98)
Epoch: [54][180/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.1406e-01 (3.4080e-01)	Acc@1  86.72 ( 89.17)	Acc@5  99.22 ( 98.98)
Epoch: [54][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0347e-01 (3.4274e-01)	Acc@1  89.06 ( 89.11)	Acc@5 100.00 ( 98.96)
Epoch: [54][200/391]	Time  0.034 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.2153e-01 (3.4368e-01)	Acc@1  89.06 ( 89.07)	Acc@5  99.22 ( 98.96)
Epoch: [54][210/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5635e-01 (3.4463e-01)	Acc@1  90.62 ( 89.08)	Acc@5 100.00 ( 98.95)
Epoch: [54][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3130e-01 (3.4500e-01)	Acc@1  89.06 ( 89.10)	Acc@5  99.22 ( 98.95)
Epoch: [54][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7256e-01 (3.4681e-01)	Acc@1  89.06 ( 89.05)	Acc@5  97.66 ( 98.91)
Epoch: [54][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8550e-01 (3.4703e-01)	Acc@1  88.28 ( 89.02)	Acc@5  99.22 ( 98.89)
Epoch: [54][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4719e-01 (3.4599e-01)	Acc@1  89.06 ( 89.03)	Acc@5 100.00 ( 98.91)
Epoch: [54][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7026e-01 (3.4582e-01)	Acc@1  89.84 ( 89.03)	Acc@5 100.00 ( 98.91)
Epoch: [54][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.5679e-01 (3.4590e-01)	Acc@1  87.50 ( 89.05)	Acc@5  97.66 ( 98.91)
Epoch: [54][280/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.0039e-01 (3.4511e-01)	Acc@1  89.06 ( 89.09)	Acc@5  97.66 ( 98.90)
Epoch: [54][290/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.002)	Loss 4.1455e-01 (3.4475e-01)	Acc@1  86.72 ( 89.12)	Acc@5  97.66 ( 98.91)
Epoch: [54][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6392e-01 (3.4427e-01)	Acc@1  91.41 ( 89.13)	Acc@5  99.22 ( 98.92)
Epoch: [54][310/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2358e-01 (3.4573e-01)	Acc@1  85.16 ( 89.09)	Acc@5  98.44 ( 98.91)
Epoch: [54][320/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.002)	Loss 3.4033e-01 (3.4497e-01)	Acc@1  89.06 ( 89.09)	Acc@5  98.44 ( 98.91)
Epoch: [54][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0420e-01 (3.4525e-01)	Acc@1  88.28 ( 89.11)	Acc@5 100.00 ( 98.90)
Epoch: [54][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6328e-01 (3.4530e-01)	Acc@1  89.84 ( 89.11)	Acc@5  98.44 ( 98.90)
Epoch: [54][350/391]	Time  0.046 ( 0.026)	Data  0.019 ( 0.002)	Loss 2.8833e-01 (3.4496e-01)	Acc@1  89.84 ( 89.13)	Acc@5 100.00 ( 98.89)
Epoch: [54][360/391]	Time  0.043 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.4790e-01 (3.4578e-01)	Acc@1  90.62 ( 89.11)	Acc@5  97.66 ( 98.87)
Epoch: [54][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4375e-01 (3.4566e-01)	Acc@1  86.72 ( 89.10)	Acc@5 100.00 ( 98.87)
Epoch: [54][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5938e-01 (3.4739e-01)	Acc@1  91.41 ( 89.07)	Acc@5  98.44 ( 98.85)
Epoch: [54][390/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9590e-01 (3.4919e-01)	Acc@1  87.50 ( 89.00)	Acc@5 100.00 ( 98.84)
## e[54] optimizer.zero_grad (sum) time: 0.10396409034729004
## e[54]       loss.backward (sum) time: 2.254558563232422
## e[54]      optimizer.step (sum) time: 0.9303805828094482
## epoch[54] training(only) time: 10.118448972702026
# Switched to evaluate mode...
Test: [  0/100]	Time  0.146 ( 0.146)	Loss 1.3877e+00 (1.3877e+00)	Acc@1  70.00 ( 70.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.012 ( 0.026)	Loss 1.4844e+00 (1.5384e+00)	Acc@1  66.00 ( 66.55)	Acc@5  92.00 ( 88.45)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.6533e+00 (1.5131e+00)	Acc@1  67.00 ( 66.19)	Acc@5  88.00 ( 88.48)
Test: [ 30/100]	Time  0.019 ( 0.018)	Loss 1.3906e+00 (1.5148e+00)	Acc@1  65.00 ( 65.55)	Acc@5  88.00 ( 88.61)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.5322e+00 (1.5131e+00)	Acc@1  64.00 ( 65.46)	Acc@5  90.00 ( 88.66)
Test: [ 50/100]	Time  0.015 ( 0.016)	Loss 1.5811e+00 (1.5454e+00)	Acc@1  68.00 ( 65.31)	Acc@5  88.00 ( 88.39)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.6221e+00 (1.5308e+00)	Acc@1  64.00 ( 65.69)	Acc@5  91.00 ( 88.64)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.5459e+00 (1.5323e+00)	Acc@1  67.00 ( 65.96)	Acc@5  88.00 ( 88.56)
Test: [ 80/100]	Time  0.018 ( 0.015)	Loss 1.6621e+00 (1.5355e+00)	Acc@1  63.00 ( 65.84)	Acc@5  88.00 ( 88.49)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.8623e+00 (1.5225e+00)	Acc@1  62.00 ( 66.11)	Acc@5  87.00 ( 88.68)
 * Acc@1 66.270 Acc@5 88.690
### epoch[54] execution time: 11.706708669662476
EPOCH 55
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [55][  0/391]	Time  0.180 ( 0.180)	Data  0.149 ( 0.149)	Loss 2.9102e-01 (2.9102e-01)	Acc@1  89.84 ( 89.84)	Acc@5  99.22 ( 99.22)
Epoch: [55][ 10/391]	Time  0.025 ( 0.040)	Data  0.001 ( 0.015)	Loss 3.1250e-01 (3.1642e-01)	Acc@1  89.06 ( 89.91)	Acc@5  99.22 ( 99.22)
Epoch: [55][ 20/391]	Time  0.021 ( 0.033)	Data  0.000 ( 0.009)	Loss 3.4375e-01 (3.1122e-01)	Acc@1  90.62 ( 90.44)	Acc@5  98.44 ( 99.18)
Epoch: [55][ 30/391]	Time  0.020 ( 0.030)	Data  0.000 ( 0.007)	Loss 3.9404e-01 (3.1313e-01)	Acc@1  84.38 ( 90.22)	Acc@5  98.44 ( 99.09)
Epoch: [55][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.005)	Loss 3.8916e-01 (3.1509e-01)	Acc@1  82.81 ( 90.03)	Acc@5  98.44 ( 99.12)
Epoch: [55][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.4182e-01 (3.1542e-01)	Acc@1  92.97 ( 90.00)	Acc@5 100.00 ( 99.17)
Epoch: [55][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.9639e-01 (3.1601e-01)	Acc@1  88.28 ( 89.91)	Acc@5 100.00 ( 99.22)
Epoch: [55][ 70/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.8999e-01 (3.2026e-01)	Acc@1  85.94 ( 89.80)	Acc@5  96.88 ( 99.19)
Epoch: [55][ 80/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 4.2383e-01 (3.2186e-01)	Acc@1  88.28 ( 89.80)	Acc@5  97.66 ( 99.15)
Epoch: [55][ 90/391]	Time  0.041 ( 0.027)	Data  0.000 ( 0.003)	Loss 3.3398e-01 (3.2103e-01)	Acc@1  89.84 ( 89.87)	Acc@5  96.88 ( 99.08)
Epoch: [55][100/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.003)	Loss 4.3579e-01 (3.2713e-01)	Acc@1  85.16 ( 89.65)	Acc@5  98.44 ( 99.05)
Epoch: [55][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.4277e-01 (3.2832e-01)	Acc@1  89.06 ( 89.53)	Acc@5  99.22 ( 99.08)
Epoch: [55][120/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.9077e-01 (3.2680e-01)	Acc@1  90.62 ( 89.60)	Acc@5  97.66 ( 99.07)
Epoch: [55][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6768e-01 (3.2331e-01)	Acc@1  89.06 ( 89.75)	Acc@5  99.22 ( 99.09)
Epoch: [55][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0811e-01 (3.2307e-01)	Acc@1  90.62 ( 89.78)	Acc@5  98.44 ( 99.09)
Epoch: [55][150/391]	Time  0.036 ( 0.026)	Data  0.006 ( 0.003)	Loss 1.4624e-01 (3.2306e-01)	Acc@1  95.31 ( 89.76)	Acc@5 100.00 ( 99.09)
Epoch: [55][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0054e-01 (3.2315e-01)	Acc@1  92.97 ( 89.75)	Acc@5  99.22 ( 99.09)
Epoch: [55][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3032e-01 (3.2412e-01)	Acc@1  89.06 ( 89.72)	Acc@5  98.44 ( 99.06)
Epoch: [55][180/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8892e-01 (3.2576e-01)	Acc@1  85.94 ( 89.65)	Acc@5  99.22 ( 99.04)
Epoch: [55][190/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.0127e-01 (3.2568e-01)	Acc@1  89.06 ( 89.62)	Acc@5  99.22 ( 99.05)
Epoch: [55][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7295e-01 (3.2598e-01)	Acc@1  92.97 ( 89.68)	Acc@5  99.22 ( 99.04)
Epoch: [55][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2595e-01 (3.2681e-01)	Acc@1  94.53 ( 89.64)	Acc@5 100.00 ( 99.04)
Epoch: [55][220/391]	Time  0.036 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.6050e-01 (3.2813e-01)	Acc@1  89.84 ( 89.63)	Acc@5 100.00 ( 99.02)
Epoch: [55][230/391]	Time  0.041 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0850e-01 (3.2873e-01)	Acc@1  92.97 ( 89.60)	Acc@5  99.22 ( 99.01)
Epoch: [55][240/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6499e-01 (3.2836e-01)	Acc@1  87.50 ( 89.59)	Acc@5  98.44 ( 99.00)
Epoch: [55][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4717e-01 (3.2771e-01)	Acc@1  89.06 ( 89.58)	Acc@5  99.22 ( 99.01)
Epoch: [55][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.3228e-01 (3.2844e-01)	Acc@1  88.28 ( 89.53)	Acc@5  99.22 ( 99.01)
Epoch: [55][270/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.1958e-01 (3.3008e-01)	Acc@1  91.41 ( 89.53)	Acc@5  98.44 ( 98.99)
Epoch: [55][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.3613e-01 (3.3077e-01)	Acc@1  84.38 ( 89.50)	Acc@5  96.88 ( 98.98)
Epoch: [55][290/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.3450e-01 (3.3153e-01)	Acc@1  92.97 ( 89.48)	Acc@5 100.00 ( 98.99)
Epoch: [55][300/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.7192e-01 (3.3225e-01)	Acc@1  83.59 ( 89.45)	Acc@5  96.88 ( 98.97)
Epoch: [55][310/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.2153e-01 (3.3175e-01)	Acc@1  91.41 ( 89.44)	Acc@5 100.00 ( 98.97)
Epoch: [55][320/391]	Time  0.034 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.1912e-01 (3.3066e-01)	Acc@1  92.97 ( 89.47)	Acc@5  99.22 ( 98.96)
Epoch: [55][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9380e-01 (3.3068e-01)	Acc@1  89.06 ( 89.47)	Acc@5  97.66 ( 98.96)
Epoch: [55][340/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.8110e-01 (3.3207e-01)	Acc@1  88.28 ( 89.41)	Acc@5  97.66 ( 98.95)
Epoch: [55][350/391]	Time  0.037 ( 0.026)	Data  0.005 ( 0.002)	Loss 3.8062e-01 (3.3271e-01)	Acc@1  85.94 ( 89.39)	Acc@5  98.44 ( 98.93)
Epoch: [55][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6450e-01 (3.3387e-01)	Acc@1  89.84 ( 89.34)	Acc@5  97.66 ( 98.92)
Epoch: [55][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0557e-01 (3.3304e-01)	Acc@1  93.75 ( 89.35)	Acc@5 100.00 ( 98.94)
Epoch: [55][380/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.6963e-01 (3.3357e-01)	Acc@1  87.50 ( 89.35)	Acc@5  99.22 ( 98.93)
Epoch: [55][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.7427e-01 (3.3335e-01)	Acc@1  86.25 ( 89.34)	Acc@5 100.00 ( 98.93)
## e[55] optimizer.zero_grad (sum) time: 0.10512232780456543
## e[55]       loss.backward (sum) time: 2.275937557220459
## e[55]      optimizer.step (sum) time: 0.9349548816680908
## epoch[55] training(only) time: 10.104809284210205
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.3652e+00 (1.3652e+00)	Acc@1  69.00 ( 69.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.4697e+00 (1.5328e+00)	Acc@1  68.00 ( 67.27)	Acc@5  91.00 ( 88.36)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 1.6211e+00 (1.5310e+00)	Acc@1  69.00 ( 66.57)	Acc@5  89.00 ( 88.43)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.5098e+00 (1.5364e+00)	Acc@1  63.00 ( 66.26)	Acc@5  88.00 ( 88.26)
Test: [ 40/100]	Time  0.014 ( 0.017)	Loss 1.6309e+00 (1.5319e+00)	Acc@1  66.00 ( 66.37)	Acc@5  91.00 ( 88.27)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.6504e+00 (1.5611e+00)	Acc@1  64.00 ( 65.94)	Acc@5  87.00 ( 87.98)
Test: [ 60/100]	Time  0.028 ( 0.016)	Loss 1.6504e+00 (1.5430e+00)	Acc@1  62.00 ( 65.90)	Acc@5  91.00 ( 88.36)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.5488e+00 (1.5525e+00)	Acc@1  70.00 ( 66.11)	Acc@5  89.00 ( 88.37)
Test: [ 80/100]	Time  0.009 ( 0.015)	Loss 1.4277e+00 (1.5528e+00)	Acc@1  67.00 ( 66.00)	Acc@5  86.00 ( 88.33)
Test: [ 90/100]	Time  0.021 ( 0.015)	Loss 1.9512e+00 (1.5455e+00)	Acc@1  59.00 ( 66.16)	Acc@5  84.00 ( 88.51)
 * Acc@1 66.230 Acc@5 88.500
### epoch[55] execution time: 11.696646213531494
EPOCH 56
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [56][  0/391]	Time  0.179 ( 0.179)	Data  0.151 ( 0.151)	Loss 3.0591e-01 (3.0591e-01)	Acc@1  87.50 ( 87.50)	Acc@5 100.00 (100.00)
Epoch: [56][ 10/391]	Time  0.023 ( 0.039)	Data  0.003 ( 0.016)	Loss 4.0820e-01 (3.3545e-01)	Acc@1  85.94 ( 88.99)	Acc@5  99.22 ( 99.15)
Epoch: [56][ 20/391]	Time  0.029 ( 0.032)	Data  0.002 ( 0.009)	Loss 4.2261e-01 (3.2835e-01)	Acc@1  85.94 ( 89.40)	Acc@5  98.44 ( 99.07)
Epoch: [56][ 30/391]	Time  0.028 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.1167e-01 (3.1006e-01)	Acc@1  93.75 ( 89.99)	Acc@5  99.22 ( 99.29)
Epoch: [56][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 3.8599e-01 (3.0832e-01)	Acc@1  88.28 ( 90.15)	Acc@5  98.44 ( 99.29)
Epoch: [56][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.2778e-01 (3.1148e-01)	Acc@1  92.19 ( 90.03)	Acc@5  99.22 ( 99.28)
Epoch: [56][ 60/391]	Time  0.022 ( 0.028)	Data  0.001 ( 0.005)	Loss 4.1113e-01 (3.1518e-01)	Acc@1  85.94 ( 89.97)	Acc@5  98.44 ( 99.23)
Epoch: [56][ 70/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.3594e-01 (3.1332e-01)	Acc@1  92.97 ( 90.03)	Acc@5  98.44 ( 99.22)
Epoch: [56][ 80/391]	Time  0.021 ( 0.027)	Data  0.003 ( 0.004)	Loss 3.5571e-01 (3.1227e-01)	Acc@1  86.72 ( 90.08)	Acc@5 100.00 ( 99.19)
Epoch: [56][ 90/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.5205e-01 (3.1279e-01)	Acc@1  87.50 ( 90.09)	Acc@5  99.22 ( 99.13)
Epoch: [56][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7124e-01 (3.1188e-01)	Acc@1  91.41 ( 90.14)	Acc@5 100.00 ( 99.15)
Epoch: [56][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7661e-01 (3.1088e-01)	Acc@1  90.62 ( 90.17)	Acc@5 100.00 ( 99.15)
Epoch: [56][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1616e-01 (3.1430e-01)	Acc@1  90.62 ( 90.08)	Acc@5  99.22 ( 99.12)
Epoch: [56][130/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.3572e-01 (3.1511e-01)	Acc@1  93.75 ( 90.10)	Acc@5  99.22 ( 99.12)
Epoch: [56][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0845e-01 (3.1571e-01)	Acc@1  88.28 ( 90.07)	Acc@5  99.22 ( 99.13)
Epoch: [56][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9819e-01 (3.1714e-01)	Acc@1  86.72 ( 90.04)	Acc@5  98.44 ( 99.12)
Epoch: [56][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9321e-01 (3.1769e-01)	Acc@1  90.62 ( 90.00)	Acc@5  99.22 ( 99.12)
Epoch: [56][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0200e-01 (3.1762e-01)	Acc@1  92.19 ( 90.00)	Acc@5  98.44 ( 99.11)
Epoch: [56][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9087e-01 (3.1667e-01)	Acc@1  86.72 ( 90.04)	Acc@5 100.00 ( 99.11)
Epoch: [56][190/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2886e-01 (3.1832e-01)	Acc@1  89.84 ( 90.04)	Acc@5  99.22 ( 99.11)
Epoch: [56][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9600e-01 (3.1924e-01)	Acc@1  86.72 ( 89.99)	Acc@5  97.66 ( 99.09)
Epoch: [56][210/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7539e-01 (3.1975e-01)	Acc@1  90.62 ( 89.97)	Acc@5  99.22 ( 99.09)
Epoch: [56][220/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.7466e-01 (3.2012e-01)	Acc@1  90.62 ( 89.94)	Acc@5  99.22 ( 99.10)
Epoch: [56][230/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.0430e-01 (3.1879e-01)	Acc@1  86.72 ( 89.95)	Acc@5  98.44 ( 99.11)
Epoch: [56][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2886e-01 (3.1980e-01)	Acc@1  88.28 ( 89.91)	Acc@5  97.66 ( 99.10)
Epoch: [56][250/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0813e-01 (3.1834e-01)	Acc@1  94.53 ( 89.92)	Acc@5 100.00 ( 99.11)
Epoch: [56][260/391]	Time  0.030 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.9932e-01 (3.1831e-01)	Acc@1  89.06 ( 89.90)	Acc@5 100.00 ( 99.11)
Epoch: [56][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3633e-01 (3.1840e-01)	Acc@1  92.97 ( 89.93)	Acc@5 100.00 ( 99.10)
Epoch: [56][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4976e-01 (3.1752e-01)	Acc@1  91.41 ( 89.95)	Acc@5  99.22 ( 99.10)
Epoch: [56][290/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6816e-01 (3.1883e-01)	Acc@1  88.28 ( 89.91)	Acc@5  98.44 ( 99.10)
Epoch: [56][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1982e-01 (3.2069e-01)	Acc@1  91.41 ( 89.91)	Acc@5  98.44 ( 99.07)
Epoch: [56][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.0928e-01 (3.2187e-01)	Acc@1  84.38 ( 89.88)	Acc@5  97.66 ( 99.06)
Epoch: [56][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 5.1709e-01 (3.2318e-01)	Acc@1  86.72 ( 89.82)	Acc@5  97.66 ( 99.06)
Epoch: [56][330/391]	Time  0.032 ( 0.026)	Data  0.004 ( 0.002)	Loss 3.0200e-01 (3.2472e-01)	Acc@1  87.50 ( 89.78)	Acc@5 100.00 ( 99.04)
Epoch: [56][340/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.1421e-01 (3.2465e-01)	Acc@1  89.06 ( 89.80)	Acc@5  98.44 ( 99.04)
Epoch: [56][350/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.8843e-01 (3.2372e-01)	Acc@1  89.06 ( 89.82)	Acc@5  98.44 ( 99.05)
Epoch: [56][360/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 4.3848e-01 (3.2345e-01)	Acc@1  91.41 ( 89.83)	Acc@5  98.44 ( 99.05)
Epoch: [56][370/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.3276e-01 (3.2412e-01)	Acc@1  87.50 ( 89.80)	Acc@5  99.22 ( 99.05)
Epoch: [56][380/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5962e-01 (3.2530e-01)	Acc@1  89.84 ( 89.77)	Acc@5  97.66 ( 99.04)
Epoch: [56][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4253e-01 (3.2567e-01)	Acc@1  88.75 ( 89.75)	Acc@5 100.00 ( 99.03)
## e[56] optimizer.zero_grad (sum) time: 0.10556364059448242
## e[56]       loss.backward (sum) time: 2.2445051670074463
## e[56]      optimizer.step (sum) time: 0.932347297668457
## epoch[56] training(only) time: 10.1005277633667
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.3320e+00 (1.3320e+00)	Acc@1  75.00 ( 75.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.016 ( 0.025)	Loss 1.5625e+00 (1.5855e+00)	Acc@1  62.00 ( 65.73)	Acc@5  90.00 ( 87.91)
Test: [ 20/100]	Time  0.013 ( 0.019)	Loss 1.5732e+00 (1.5713e+00)	Acc@1  64.00 ( 65.57)	Acc@5  91.00 ( 87.95)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.5732e+00 (1.5831e+00)	Acc@1  64.00 ( 65.29)	Acc@5  87.00 ( 87.77)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.5938e+00 (1.5875e+00)	Acc@1  64.00 ( 65.00)	Acc@5  92.00 ( 88.05)
Test: [ 50/100]	Time  0.013 ( 0.016)	Loss 1.5713e+00 (1.6105e+00)	Acc@1  67.00 ( 65.10)	Acc@5  86.00 ( 87.78)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6211e+00 (1.5861e+00)	Acc@1  61.00 ( 65.38)	Acc@5  91.00 ( 88.11)
Test: [ 70/100]	Time  0.012 ( 0.015)	Loss 1.6143e+00 (1.5809e+00)	Acc@1  64.00 ( 65.45)	Acc@5  89.00 ( 88.18)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.4814e+00 (1.5799e+00)	Acc@1  66.00 ( 65.49)	Acc@5  87.00 ( 88.05)
Test: [ 90/100]	Time  0.020 ( 0.015)	Loss 1.9854e+00 (1.5717e+00)	Acc@1  59.00 ( 65.67)	Acc@5  84.00 ( 88.21)
 * Acc@1 65.710 Acc@5 88.220
### epoch[56] execution time: 11.694862127304077
EPOCH 57
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [57][  0/391]	Time  0.173 ( 0.173)	Data  0.146 ( 0.146)	Loss 2.8931e-01 (2.8931e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [57][ 10/391]	Time  0.031 ( 0.039)	Data  0.005 ( 0.015)	Loss 2.2717e-01 (2.9508e-01)	Acc@1  92.97 ( 90.55)	Acc@5  99.22 ( 99.36)
Epoch: [57][ 20/391]	Time  0.025 ( 0.032)	Data  0.001 ( 0.009)	Loss 2.5879e-01 (2.9955e-01)	Acc@1  89.84 ( 90.22)	Acc@5 100.00 ( 99.33)
Epoch: [57][ 30/391]	Time  0.034 ( 0.030)	Data  0.002 ( 0.007)	Loss 3.4790e-01 (3.0701e-01)	Acc@1  86.72 ( 90.12)	Acc@5 100.00 ( 99.17)
Epoch: [57][ 40/391]	Time  0.030 ( 0.028)	Data  0.002 ( 0.005)	Loss 4.1528e-01 (3.1413e-01)	Acc@1  87.50 ( 89.88)	Acc@5  97.66 ( 99.12)
Epoch: [57][ 50/391]	Time  0.026 ( 0.028)	Data  0.002 ( 0.005)	Loss 2.4377e-01 (3.1163e-01)	Acc@1  89.06 ( 90.00)	Acc@5  99.22 ( 99.14)
Epoch: [57][ 60/391]	Time  0.029 ( 0.027)	Data  0.000 ( 0.004)	Loss 4.5630e-01 (3.1407e-01)	Acc@1  89.06 ( 89.97)	Acc@5  98.44 ( 99.13)
Epoch: [57][ 70/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.6499e-01 (3.0914e-01)	Acc@1  85.94 ( 90.13)	Acc@5  99.22 ( 99.14)
Epoch: [57][ 80/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7051e-01 (3.0510e-01)	Acc@1  89.84 ( 90.30)	Acc@5  99.22 ( 99.15)
Epoch: [57][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 4.1577e-01 (3.0215e-01)	Acc@1  85.16 ( 90.39)	Acc@5  97.66 ( 99.16)
Epoch: [57][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4365e-01 (3.0068e-01)	Acc@1  91.41 ( 90.46)	Acc@5  99.22 ( 99.15)
Epoch: [57][110/391]	Time  0.036 ( 0.027)	Data  0.004 ( 0.003)	Loss 2.5562e-01 (3.0131e-01)	Acc@1  90.62 ( 90.48)	Acc@5 100.00 ( 99.15)
Epoch: [57][120/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9468e-01 (3.0049e-01)	Acc@1  91.41 ( 90.48)	Acc@5 100.00 ( 99.15)
Epoch: [57][130/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.1616e-01 (3.0170e-01)	Acc@1  87.50 ( 90.43)	Acc@5  98.44 ( 99.14)
Epoch: [57][140/391]	Time  0.030 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.1592e-01 (2.9950e-01)	Acc@1  89.06 ( 90.44)	Acc@5 100.00 ( 99.17)
Epoch: [57][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5537e-01 (3.0000e-01)	Acc@1  89.84 ( 90.38)	Acc@5  98.44 ( 99.15)
Epoch: [57][160/391]	Time  0.032 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.9224e-01 (3.0220e-01)	Acc@1  91.41 ( 90.31)	Acc@5  99.22 ( 99.14)
Epoch: [57][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1509e-01 (3.0052e-01)	Acc@1  92.19 ( 90.37)	Acc@5 100.00 ( 99.16)
Epoch: [57][180/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 5.2539e-01 (3.0364e-01)	Acc@1  82.03 ( 90.27)	Acc@5  96.09 ( 99.15)
Epoch: [57][190/391]	Time  0.030 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.7979e-01 (3.0311e-01)	Acc@1  93.75 ( 90.31)	Acc@5  98.44 ( 99.15)
Epoch: [57][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7026e-01 (3.0256e-01)	Acc@1  88.28 ( 90.31)	Acc@5  99.22 ( 99.15)
Epoch: [57][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7246e-01 (3.0438e-01)	Acc@1  92.19 ( 90.26)	Acc@5  99.22 ( 99.11)
Epoch: [57][220/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.2278e-01 (3.0288e-01)	Acc@1  92.97 ( 90.27)	Acc@5 100.00 ( 99.12)
Epoch: [57][230/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5562e-01 (3.0340e-01)	Acc@1  89.84 ( 90.24)	Acc@5  99.22 ( 99.12)
Epoch: [57][240/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.3848e-01 (3.0475e-01)	Acc@1  84.38 ( 90.20)	Acc@5 100.00 ( 99.11)
Epoch: [57][250/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.8501e-01 (3.0591e-01)	Acc@1  87.50 ( 90.19)	Acc@5  98.44 ( 99.11)
Epoch: [57][260/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.8345e-01 (3.0561e-01)	Acc@1  89.84 ( 90.21)	Acc@5  99.22 ( 99.11)
Epoch: [57][270/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.1396e-01 (3.0551e-01)	Acc@1  92.19 ( 90.23)	Acc@5  97.66 ( 99.11)
Epoch: [57][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2212e-01 (3.0733e-01)	Acc@1  89.06 ( 90.16)	Acc@5 100.00 ( 99.11)
Epoch: [57][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9321e-01 (3.0641e-01)	Acc@1  89.84 ( 90.22)	Acc@5  99.22 ( 99.09)
Epoch: [57][300/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6304e-01 (3.0696e-01)	Acc@1  88.28 ( 90.21)	Acc@5  98.44 ( 99.08)
Epoch: [57][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9429e-01 (3.0723e-01)	Acc@1  88.28 ( 90.21)	Acc@5  98.44 ( 99.08)
Epoch: [57][320/391]	Time  0.041 ( 0.026)	Data  0.004 ( 0.003)	Loss 4.0576e-01 (3.0668e-01)	Acc@1  84.38 ( 90.23)	Acc@5  98.44 ( 99.08)
Epoch: [57][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4814e-01 (3.0734e-01)	Acc@1  89.84 ( 90.22)	Acc@5  97.66 ( 99.07)
Epoch: [57][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5439e-01 (3.0651e-01)	Acc@1  92.19 ( 90.27)	Acc@5  99.22 ( 99.07)
Epoch: [57][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4060e-01 (3.0566e-01)	Acc@1  89.06 ( 90.30)	Acc@5 100.00 ( 99.08)
Epoch: [57][360/391]	Time  0.036 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.8979e-01 (3.0535e-01)	Acc@1  89.06 ( 90.32)	Acc@5  99.22 ( 99.08)
Epoch: [57][370/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9639e-01 (3.0555e-01)	Acc@1  90.62 ( 90.32)	Acc@5  99.22 ( 99.08)
Epoch: [57][380/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.0942e-01 (3.0633e-01)	Acc@1  89.06 ( 90.31)	Acc@5  99.22 ( 99.07)
Epoch: [57][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.2651e-01 (3.0686e-01)	Acc@1  85.00 ( 90.28)	Acc@5  97.50 ( 99.06)
## e[57] optimizer.zero_grad (sum) time: 0.10594582557678223
## e[57]       loss.backward (sum) time: 2.2801706790924072
## e[57]      optimizer.step (sum) time: 0.9201900959014893
## epoch[57] training(only) time: 10.154157400131226
# Switched to evaluate mode...
Test: [  0/100]	Time  0.148 ( 0.148)	Loss 1.3193e+00 (1.3193e+00)	Acc@1  75.00 ( 75.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.009 ( 0.027)	Loss 1.4258e+00 (1.5944e+00)	Acc@1  66.00 ( 66.91)	Acc@5  90.00 ( 87.55)
Test: [ 20/100]	Time  0.016 ( 0.021)	Loss 1.5586e+00 (1.5808e+00)	Acc@1  66.00 ( 66.29)	Acc@5  90.00 ( 87.95)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.5098e+00 (1.5971e+00)	Acc@1  64.00 ( 65.90)	Acc@5  91.00 ( 88.00)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6299e+00 (1.5953e+00)	Acc@1  62.00 ( 65.83)	Acc@5  93.00 ( 88.17)
Test: [ 50/100]	Time  0.023 ( 0.016)	Loss 1.7012e+00 (1.6270e+00)	Acc@1  68.00 ( 65.53)	Acc@5  85.00 ( 87.86)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.6357e+00 (1.6055e+00)	Acc@1  63.00 ( 65.70)	Acc@5  91.00 ( 88.10)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.6250e+00 (1.6087e+00)	Acc@1  64.00 ( 65.87)	Acc@5  90.00 ( 88.06)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.5156e+00 (1.6083e+00)	Acc@1  66.00 ( 65.85)	Acc@5  87.00 ( 88.01)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9414e+00 (1.5935e+00)	Acc@1  58.00 ( 66.00)	Acc@5  87.00 ( 88.23)
 * Acc@1 66.090 Acc@5 88.300
### epoch[57] execution time: 11.73732614517212
EPOCH 58
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [58][  0/391]	Time  0.183 ( 0.183)	Data  0.158 ( 0.158)	Loss 3.9697e-01 (3.9697e-01)	Acc@1  87.50 ( 87.50)	Acc@5  98.44 ( 98.44)
Epoch: [58][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.016)	Loss 3.4692e-01 (3.0926e-01)	Acc@1  87.50 ( 89.35)	Acc@5 100.00 ( 99.01)
Epoch: [58][ 20/391]	Time  0.037 ( 0.032)	Data  0.001 ( 0.009)	Loss 2.7051e-01 (2.9381e-01)	Acc@1  92.19 ( 90.59)	Acc@5  99.22 ( 99.14)
Epoch: [58][ 30/391]	Time  0.032 ( 0.030)	Data  0.002 ( 0.007)	Loss 1.9360e-01 (3.0869e-01)	Acc@1  94.53 ( 90.30)	Acc@5  99.22 ( 99.07)
Epoch: [58][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.2083e-01 (2.9846e-01)	Acc@1  92.19 ( 90.45)	Acc@5 100.00 ( 99.16)
Epoch: [58][ 50/391]	Time  0.026 ( 0.028)	Data  0.003 ( 0.005)	Loss 2.8027e-01 (2.9732e-01)	Acc@1  92.19 ( 90.47)	Acc@5  99.22 ( 99.16)
Epoch: [58][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.8760e-01 (2.9592e-01)	Acc@1  90.62 ( 90.56)	Acc@5  99.22 ( 99.21)
Epoch: [58][ 70/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5171e-01 (3.0257e-01)	Acc@1  90.62 ( 90.45)	Acc@5  99.22 ( 99.09)
Epoch: [58][ 80/391]	Time  0.020 ( 0.027)	Data  0.000 ( 0.004)	Loss 4.2822e-01 (3.0299e-01)	Acc@1  85.16 ( 90.45)	Acc@5  98.44 ( 99.04)
Epoch: [58][ 90/391]	Time  0.019 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.7036e-01 (3.0452e-01)	Acc@1  88.28 ( 90.41)	Acc@5  98.44 ( 99.03)
Epoch: [58][100/391]	Time  0.041 ( 0.027)	Data  0.003 ( 0.004)	Loss 3.1128e-01 (3.0233e-01)	Acc@1  88.28 ( 90.47)	Acc@5  97.66 ( 99.07)
Epoch: [58][110/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.1030e-01 (3.0154e-01)	Acc@1  89.06 ( 90.44)	Acc@5 100.00 ( 99.09)
Epoch: [58][120/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9746e-01 (3.0440e-01)	Acc@1  88.28 ( 90.39)	Acc@5  97.66 ( 99.08)
Epoch: [58][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0566e-01 (3.0288e-01)	Acc@1  90.62 ( 90.38)	Acc@5  97.66 ( 99.10)
Epoch: [58][140/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.3018e-01 (3.0459e-01)	Acc@1  88.28 ( 90.39)	Acc@5  97.66 ( 99.10)
Epoch: [58][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8101e-01 (3.0682e-01)	Acc@1  91.41 ( 90.37)	Acc@5  99.22 ( 99.08)
Epoch: [58][160/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.3975e-01 (3.0632e-01)	Acc@1  92.97 ( 90.38)	Acc@5  99.22 ( 99.09)
Epoch: [58][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1421e-01 (3.0651e-01)	Acc@1  89.06 ( 90.35)	Acc@5  97.66 ( 99.09)
Epoch: [58][180/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9663e-01 (3.0629e-01)	Acc@1  88.28 ( 90.34)	Acc@5  99.22 ( 99.08)
Epoch: [58][190/391]	Time  0.040 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.1899e-01 (3.0387e-01)	Acc@1  92.19 ( 90.43)	Acc@5  99.22 ( 99.11)
Epoch: [58][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4561e-01 (3.0407e-01)	Acc@1  91.41 ( 90.39)	Acc@5 100.00 ( 99.12)
Epoch: [58][210/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5293e-01 (3.0518e-01)	Acc@1  91.41 ( 90.37)	Acc@5 100.00 ( 99.11)
Epoch: [58][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2129e-01 (3.0475e-01)	Acc@1  90.62 ( 90.39)	Acc@5 100.00 ( 99.10)
Epoch: [58][230/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1616e-01 (3.0562e-01)	Acc@1  89.06 ( 90.36)	Acc@5  99.22 ( 99.09)
Epoch: [58][240/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.4414e-01 (3.0563e-01)	Acc@1  89.84 ( 90.37)	Acc@5 100.00 ( 99.09)
Epoch: [58][250/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7280e-01 (3.0680e-01)	Acc@1  85.16 ( 90.30)	Acc@5  98.44 ( 99.09)
Epoch: [58][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.9746e-01 (3.0669e-01)	Acc@1  87.50 ( 90.32)	Acc@5  99.22 ( 99.10)
Epoch: [58][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4670e-01 (3.0661e-01)	Acc@1  96.09 ( 90.31)	Acc@5  98.44 ( 99.10)
Epoch: [58][280/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 5.1025e-01 (3.0769e-01)	Acc@1  84.38 ( 90.26)	Acc@5  96.88 ( 99.09)
Epoch: [58][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5415e-01 (3.0866e-01)	Acc@1  91.41 ( 90.26)	Acc@5  99.22 ( 99.08)
Epoch: [58][300/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3328e-01 (3.0844e-01)	Acc@1  93.75 ( 90.29)	Acc@5 100.00 ( 99.08)
Epoch: [58][310/391]	Time  0.037 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.0957e-01 (3.1006e-01)	Acc@1  91.41 ( 90.25)	Acc@5  99.22 ( 99.08)
Epoch: [58][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2803e-01 (3.1000e-01)	Acc@1  91.41 ( 90.25)	Acc@5 100.00 ( 99.07)
Epoch: [58][330/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.1616e-01 (3.0942e-01)	Acc@1  89.06 ( 90.24)	Acc@5  99.22 ( 99.07)
Epoch: [58][340/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2375e-01 (3.0836e-01)	Acc@1  92.97 ( 90.28)	Acc@5 100.00 ( 99.08)
Epoch: [58][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9409e-01 (3.0783e-01)	Acc@1  93.75 ( 90.30)	Acc@5 100.00 ( 99.09)
Epoch: [58][360/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9175e-01 (3.0874e-01)	Acc@1  89.06 ( 90.27)	Acc@5  98.44 ( 99.07)
Epoch: [58][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.6460e-01 (3.0870e-01)	Acc@1  85.16 ( 90.24)	Acc@5  96.88 ( 99.08)
Epoch: [58][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.9990e-01 (3.0724e-01)	Acc@1  89.06 ( 90.29)	Acc@5  99.22 ( 99.09)
Epoch: [58][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4155e-01 (3.0653e-01)	Acc@1  90.00 ( 90.29)	Acc@5  97.50 ( 99.10)
## e[58] optimizer.zero_grad (sum) time: 0.10633039474487305
## e[58]       loss.backward (sum) time: 2.248659372329712
## e[58]      optimizer.step (sum) time: 0.9233057498931885
## epoch[58] training(only) time: 10.123480081558228
# Switched to evaluate mode...
Test: [  0/100]	Time  0.150 ( 0.150)	Loss 1.1943e+00 (1.1943e+00)	Acc@1  74.00 ( 74.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 1.5430e+00 (1.5611e+00)	Acc@1  66.00 ( 68.09)	Acc@5  92.00 ( 88.64)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.6592e+00 (1.5568e+00)	Acc@1  64.00 ( 66.62)	Acc@5  89.00 ( 88.71)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.5303e+00 (1.5727e+00)	Acc@1  64.00 ( 65.97)	Acc@5  88.00 ( 88.42)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6816e+00 (1.5761e+00)	Acc@1  67.00 ( 66.07)	Acc@5  89.00 ( 88.44)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5791e+00 (1.5983e+00)	Acc@1  65.00 ( 65.73)	Acc@5  88.00 ( 88.16)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.7012e+00 (1.5770e+00)	Acc@1  63.00 ( 66.02)	Acc@5  90.00 ( 88.44)
Test: [ 70/100]	Time  0.012 ( 0.015)	Loss 1.6553e+00 (1.5851e+00)	Acc@1  67.00 ( 66.15)	Acc@5  86.00 ( 88.38)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.4658e+00 (1.5877e+00)	Acc@1  69.00 ( 66.06)	Acc@5  88.00 ( 88.33)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 1.7266e+00 (1.5705e+00)	Acc@1  68.00 ( 66.42)	Acc@5  87.00 ( 88.44)
 * Acc@1 66.370 Acc@5 88.400
### epoch[58] execution time: 11.687377452850342
EPOCH 59
# current learning rate: 0.010000000000000002
# Switched to train mode...
Epoch: [59][  0/391]	Time  0.181 ( 0.181)	Data  0.149 ( 0.149)	Loss 2.4500e-01 (2.4500e-01)	Acc@1  91.41 ( 91.41)	Acc@5  99.22 ( 99.22)
Epoch: [59][ 10/391]	Time  0.022 ( 0.038)	Data  0.001 ( 0.015)	Loss 2.8979e-01 (2.6134e-01)	Acc@1  91.41 ( 92.05)	Acc@5  99.22 ( 99.36)
Epoch: [59][ 20/391]	Time  0.036 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.6914e-01 (2.8532e-01)	Acc@1  88.28 ( 91.29)	Acc@5  97.66 ( 99.22)
Epoch: [59][ 30/391]	Time  0.024 ( 0.030)	Data  0.002 ( 0.007)	Loss 2.3474e-01 (2.8983e-01)	Acc@1  90.62 ( 91.18)	Acc@5  99.22 ( 99.22)
Epoch: [59][ 40/391]	Time  0.021 ( 0.029)	Data  0.000 ( 0.006)	Loss 2.6904e-01 (2.8445e-01)	Acc@1  89.84 ( 91.18)	Acc@5 100.00 ( 99.29)
Epoch: [59][ 50/391]	Time  0.036 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.4072e-01 (2.7879e-01)	Acc@1  93.75 ( 91.50)	Acc@5  99.22 ( 99.25)
Epoch: [59][ 60/391]	Time  0.038 ( 0.027)	Data  0.003 ( 0.004)	Loss 4.6680e-01 (2.8113e-01)	Acc@1  85.94 ( 91.34)	Acc@5  98.44 ( 99.19)
Epoch: [59][ 70/391]	Time  0.035 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.7490e-01 (2.7829e-01)	Acc@1  90.62 ( 91.37)	Acc@5 100.00 ( 99.26)
Epoch: [59][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5659e-01 (2.7964e-01)	Acc@1  90.62 ( 91.31)	Acc@5 100.00 ( 99.27)
Epoch: [59][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4548e-01 (2.7566e-01)	Acc@1  89.06 ( 91.37)	Acc@5 100.00 ( 99.25)
Epoch: [59][100/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1348e-01 (2.7633e-01)	Acc@1  87.50 ( 91.39)	Acc@5 100.00 ( 99.26)
Epoch: [59][110/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.5977e-01 (2.7747e-01)	Acc@1  92.97 ( 91.36)	Acc@5  98.44 ( 99.23)
Epoch: [59][120/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6931e-01 (2.7955e-01)	Acc@1  94.53 ( 91.25)	Acc@5  99.22 ( 99.24)
Epoch: [59][130/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3999e-01 (2.8085e-01)	Acc@1  91.41 ( 91.18)	Acc@5 100.00 ( 99.23)
Epoch: [59][140/391]	Time  0.045 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.5098e-01 (2.8212e-01)	Acc@1  91.41 ( 91.13)	Acc@5 100.00 ( 99.25)
Epoch: [59][150/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1641e-01 (2.8452e-01)	Acc@1  90.62 ( 91.13)	Acc@5  99.22 ( 99.25)
Epoch: [59][160/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5488e-01 (2.8631e-01)	Acc@1  91.41 ( 91.06)	Acc@5  99.22 ( 99.23)
Epoch: [59][170/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.5620e-01 (2.8575e-01)	Acc@1  88.28 ( 91.04)	Acc@5  99.22 ( 99.23)
Epoch: [59][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6377e-01 (2.8632e-01)	Acc@1  89.06 ( 91.01)	Acc@5  99.22 ( 99.25)
Epoch: [59][190/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.2446e-01 (2.8704e-01)	Acc@1  88.28 ( 90.98)	Acc@5  99.22 ( 99.25)
Epoch: [59][200/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 3.8892e-01 (2.8679e-01)	Acc@1  90.62 ( 90.96)	Acc@5  96.88 ( 99.23)
Epoch: [59][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.7939e-01 (2.8727e-01)	Acc@1  85.94 ( 90.92)	Acc@5  99.22 ( 99.23)
Epoch: [59][220/391]	Time  0.035 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.1128e-01 (2.8697e-01)	Acc@1  89.06 ( 90.95)	Acc@5 100.00 ( 99.25)
Epoch: [59][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5928e-01 (2.8673e-01)	Acc@1  91.41 ( 90.95)	Acc@5  99.22 ( 99.24)
Epoch: [59][240/391]	Time  0.039 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.7368e-01 (2.8622e-01)	Acc@1  92.19 ( 90.97)	Acc@5  99.22 ( 99.24)
Epoch: [59][250/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4692e-01 (2.8487e-01)	Acc@1  92.19 ( 91.01)	Acc@5  98.44 ( 99.25)
Epoch: [59][260/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5977e-01 (2.8518e-01)	Acc@1  88.28 ( 90.97)	Acc@5 100.00 ( 99.24)
Epoch: [59][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6538e-01 (2.8788e-01)	Acc@1  89.84 ( 90.88)	Acc@5 100.00 ( 99.22)
Epoch: [59][280/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4888e-01 (2.8840e-01)	Acc@1  86.72 ( 90.85)	Acc@5  98.44 ( 99.21)
Epoch: [59][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6221e-01 (2.8739e-01)	Acc@1  89.06 ( 90.89)	Acc@5 100.00 ( 99.23)
Epoch: [59][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5059e-01 (2.8700e-01)	Acc@1  89.06 ( 90.89)	Acc@5  98.44 ( 99.24)
Epoch: [59][310/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9370e-01 (2.8676e-01)	Acc@1  89.84 ( 90.88)	Acc@5  99.22 ( 99.25)
Epoch: [59][320/391]	Time  0.041 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.9067e-01 (2.8599e-01)	Acc@1  94.53 ( 90.89)	Acc@5 100.00 ( 99.25)
Epoch: [59][330/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.0142e-01 (2.8555e-01)	Acc@1  95.31 ( 90.92)	Acc@5  99.22 ( 99.25)
Epoch: [59][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.0186e-01 (2.8609e-01)	Acc@1  88.28 ( 90.90)	Acc@5  96.88 ( 99.23)
Epoch: [59][350/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.7114e-01 (2.8507e-01)	Acc@1  96.09 ( 90.92)	Acc@5 100.00 ( 99.24)
Epoch: [59][360/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1445e-01 (2.8573e-01)	Acc@1  86.72 ( 90.89)	Acc@5  99.22 ( 99.23)
Epoch: [59][370/391]	Time  0.038 ( 0.026)	Data  0.005 ( 0.002)	Loss 2.7588e-01 (2.8532e-01)	Acc@1  92.19 ( 90.91)	Acc@5  99.22 ( 99.24)
Epoch: [59][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6743e-01 (2.8581e-01)	Acc@1  88.28 ( 90.88)	Acc@5 100.00 ( 99.24)
Epoch: [59][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1445e-01 (2.8544e-01)	Acc@1  91.25 ( 90.87)	Acc@5  97.50 ( 99.24)
## e[59] optimizer.zero_grad (sum) time: 0.10480952262878418
## e[59]       loss.backward (sum) time: 2.250363826751709
## e[59]      optimizer.step (sum) time: 0.9318456649780273
## epoch[59] training(only) time: 10.121582269668579
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.2422e+00 (1.2422e+00)	Acc@1  77.00 ( 77.00)	Acc@5  91.00 ( 91.00)
Test: [ 10/100]	Time  0.020 ( 0.026)	Loss 1.4775e+00 (1.5880e+00)	Acc@1  67.00 ( 67.45)	Acc@5  92.00 ( 88.18)
Test: [ 20/100]	Time  0.016 ( 0.020)	Loss 1.6396e+00 (1.5985e+00)	Acc@1  66.00 ( 66.57)	Acc@5  90.00 ( 88.10)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 1.6602e+00 (1.6164e+00)	Acc@1  62.00 ( 66.23)	Acc@5  85.00 ( 87.68)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.6846e+00 (1.6075e+00)	Acc@1  65.00 ( 66.24)	Acc@5  90.00 ( 87.90)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6797e+00 (1.6320e+00)	Acc@1  65.00 ( 65.92)	Acc@5  87.00 ( 87.76)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 1.7559e+00 (1.6125e+00)	Acc@1  57.00 ( 66.16)	Acc@5  89.00 ( 87.93)
Test: [ 70/100]	Time  0.013 ( 0.016)	Loss 1.5723e+00 (1.6197e+00)	Acc@1  67.00 ( 66.04)	Acc@5  89.00 ( 87.87)
Test: [ 80/100]	Time  0.022 ( 0.015)	Loss 1.5674e+00 (1.6218e+00)	Acc@1  68.00 ( 65.95)	Acc@5  89.00 ( 87.86)
Test: [ 90/100]	Time  0.022 ( 0.015)	Loss 1.9541e+00 (1.6059e+00)	Acc@1  60.00 ( 66.18)	Acc@5  88.00 ( 88.07)
 * Acc@1 66.250 Acc@5 88.050
### epoch[59] execution time: 11.709245443344116
EPOCH 60
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [60][  0/391]	Time  0.182 ( 0.182)	Data  0.154 ( 0.154)	Loss 1.6809e-01 (1.6809e-01)	Acc@1  97.66 ( 97.66)	Acc@5  99.22 ( 99.22)
Epoch: [60][ 10/391]	Time  0.034 ( 0.040)	Data  0.001 ( 0.016)	Loss 1.9824e-01 (2.3551e-01)	Acc@1  93.75 ( 93.11)	Acc@5  99.22 ( 99.50)
Epoch: [60][ 20/391]	Time  0.022 ( 0.032)	Data  0.004 ( 0.009)	Loss 1.9458e-01 (2.4292e-01)	Acc@1  93.75 ( 92.41)	Acc@5 100.00 ( 99.44)
Epoch: [60][ 30/391]	Time  0.042 ( 0.030)	Data  0.010 ( 0.007)	Loss 1.9275e-01 (2.4724e-01)	Acc@1  92.97 ( 92.16)	Acc@5 100.00 ( 99.37)
Epoch: [60][ 40/391]	Time  0.029 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.0374e-01 (2.4897e-01)	Acc@1  94.53 ( 92.09)	Acc@5 100.00 ( 99.39)
Epoch: [60][ 50/391]	Time  0.028 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.3643e-01 (2.4659e-01)	Acc@1  89.06 ( 92.16)	Acc@5 100.00 ( 99.48)
Epoch: [60][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.7319e-01 (2.5182e-01)	Acc@1  92.97 ( 92.02)	Acc@5  98.44 ( 99.41)
Epoch: [60][ 70/391]	Time  0.026 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.6001e-01 (2.5674e-01)	Acc@1  90.62 ( 91.92)	Acc@5 100.00 ( 99.35)
Epoch: [60][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.6465e-01 (2.5467e-01)	Acc@1  92.97 ( 92.10)	Acc@5  98.44 ( 99.37)
Epoch: [60][ 90/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.3022e-01 (2.5776e-01)	Acc@1  92.97 ( 92.03)	Acc@5 100.00 ( 99.40)
Epoch: [60][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5513e-01 (2.5607e-01)	Acc@1  92.19 ( 92.08)	Acc@5 100.00 ( 99.44)
Epoch: [60][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5332e-01 (2.5649e-01)	Acc@1  95.31 ( 92.04)	Acc@5 100.00 ( 99.44)
Epoch: [60][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8835e-01 (2.5622e-01)	Acc@1  95.31 ( 92.01)	Acc@5 100.00 ( 99.43)
Epoch: [60][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9175e-01 (2.5710e-01)	Acc@1  92.97 ( 91.98)	Acc@5  98.44 ( 99.39)
Epoch: [60][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4292e-01 (2.5585e-01)	Acc@1  93.75 ( 92.02)	Acc@5  99.22 ( 99.38)
Epoch: [60][150/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7881e-01 (2.5395e-01)	Acc@1  87.50 ( 92.02)	Acc@5  99.22 ( 99.39)
Epoch: [60][160/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5693e-01 (2.5450e-01)	Acc@1  85.94 ( 91.99)	Acc@5 100.00 ( 99.40)
Epoch: [60][170/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.1396e-01 (2.5484e-01)	Acc@1  92.19 ( 91.99)	Acc@5  99.22 ( 99.39)
Epoch: [60][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6001e-01 (2.5707e-01)	Acc@1  89.84 ( 91.87)	Acc@5 100.00 ( 99.38)
Epoch: [60][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9590e-01 (2.5733e-01)	Acc@1  89.06 ( 91.86)	Acc@5  98.44 ( 99.37)
Epoch: [60][200/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.6377e-01 (2.5783e-01)	Acc@1  89.06 ( 91.81)	Acc@5  99.22 ( 99.38)
Epoch: [60][210/391]	Time  0.042 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9653e-01 (2.5713e-01)	Acc@1  94.53 ( 91.80)	Acc@5 100.00 ( 99.39)
Epoch: [60][220/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7578e-01 (2.5653e-01)	Acc@1  95.31 ( 91.83)	Acc@5 100.00 ( 99.38)
Epoch: [60][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7383e-01 (2.5777e-01)	Acc@1  92.97 ( 91.75)	Acc@5 100.00 ( 99.38)
Epoch: [60][240/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.4351e-01 (2.5705e-01)	Acc@1  85.94 ( 91.76)	Acc@5 100.00 ( 99.38)
Epoch: [60][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4473e-01 (2.5660e-01)	Acc@1  88.28 ( 91.76)	Acc@5  98.44 ( 99.38)
Epoch: [60][260/391]	Time  0.025 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.9983e-01 (2.5553e-01)	Acc@1  94.53 ( 91.80)	Acc@5  99.22 ( 99.39)
Epoch: [60][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5879e-01 (2.5490e-01)	Acc@1  91.41 ( 91.81)	Acc@5  98.44 ( 99.39)
Epoch: [60][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6538e-01 (2.5494e-01)	Acc@1  92.19 ( 91.80)	Acc@5  99.22 ( 99.39)
Epoch: [60][290/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.7969e-01 (2.5534e-01)	Acc@1  95.31 ( 91.80)	Acc@5  99.22 ( 99.39)
Epoch: [60][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2827e-01 (2.5700e-01)	Acc@1  94.53 ( 91.74)	Acc@5  99.22 ( 99.39)
Epoch: [60][310/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.3706e-01 (2.5633e-01)	Acc@1  90.62 ( 91.75)	Acc@5  99.22 ( 99.39)
Epoch: [60][320/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1934e-01 (2.5672e-01)	Acc@1  85.94 ( 91.74)	Acc@5  99.22 ( 99.38)
Epoch: [60][330/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9858e-01 (2.5600e-01)	Acc@1  92.97 ( 91.78)	Acc@5  97.66 ( 99.38)
Epoch: [60][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3425e-01 (2.5581e-01)	Acc@1  93.75 ( 91.80)	Acc@5  99.22 ( 99.37)
Epoch: [60][350/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8784e-01 (2.5531e-01)	Acc@1  89.84 ( 91.81)	Acc@5  99.22 ( 99.37)
Epoch: [60][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9067e-01 (2.5481e-01)	Acc@1  96.88 ( 91.83)	Acc@5 100.00 ( 99.37)
Epoch: [60][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6587e-01 (2.5506e-01)	Acc@1  92.19 ( 91.85)	Acc@5  99.22 ( 99.36)
Epoch: [60][380/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7100e-01 (2.5540e-01)	Acc@1  90.62 ( 91.84)	Acc@5 100.00 ( 99.35)
Epoch: [60][390/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.9934e-01 (2.5495e-01)	Acc@1  95.00 ( 91.86)	Acc@5  98.75 ( 99.36)
## e[60] optimizer.zero_grad (sum) time: 0.10550999641418457
## e[60]       loss.backward (sum) time: 2.305079460144043
## e[60]      optimizer.step (sum) time: 0.9290721416473389
## epoch[60] training(only) time: 10.107628345489502
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.2734e+00 (1.2734e+00)	Acc@1  75.00 ( 75.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.013 ( 0.026)	Loss 1.4229e+00 (1.5742e+00)	Acc@1  67.00 ( 67.55)	Acc@5  93.00 ( 87.82)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 1.6611e+00 (1.5748e+00)	Acc@1  65.00 ( 66.86)	Acc@5  90.00 ( 88.14)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.5537e+00 (1.5845e+00)	Acc@1  62.00 ( 66.35)	Acc@5  87.00 ( 87.90)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 1.6631e+00 (1.5778e+00)	Acc@1  65.00 ( 66.54)	Acc@5  91.00 ( 88.15)
Test: [ 50/100]	Time  0.020 ( 0.016)	Loss 1.6602e+00 (1.6092e+00)	Acc@1  67.00 ( 66.25)	Acc@5  89.00 ( 88.00)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.7373e+00 (1.5861e+00)	Acc@1  61.00 ( 66.57)	Acc@5  91.00 ( 88.33)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.5439e+00 (1.5929e+00)	Acc@1  68.00 ( 66.65)	Acc@5  91.00 ( 88.34)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.5400e+00 (1.5967e+00)	Acc@1  67.00 ( 66.57)	Acc@5  88.00 ( 88.27)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 1.9697e+00 (1.5825e+00)	Acc@1  57.00 ( 66.77)	Acc@5  87.00 ( 88.45)
 * Acc@1 66.860 Acc@5 88.410
### epoch[60] execution time: 11.683985710144043
EPOCH 61
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [61][  0/391]	Time  0.184 ( 0.184)	Data  0.158 ( 0.158)	Loss 2.9712e-01 (2.9712e-01)	Acc@1  89.06 ( 89.06)	Acc@5  98.44 ( 98.44)
Epoch: [61][ 10/391]	Time  0.042 ( 0.040)	Data  0.003 ( 0.016)	Loss 1.8860e-01 (2.2816e-01)	Acc@1  93.75 ( 92.26)	Acc@5  98.44 ( 99.43)
Epoch: [61][ 20/391]	Time  0.023 ( 0.033)	Data  0.001 ( 0.009)	Loss 1.6016e-01 (2.4663e-01)	Acc@1  95.31 ( 91.85)	Acc@5  99.22 ( 99.26)
Epoch: [61][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.4490e-01 (2.3282e-01)	Acc@1  92.97 ( 92.26)	Acc@5  99.22 ( 99.34)
Epoch: [61][ 40/391]	Time  0.026 ( 0.029)	Data  0.000 ( 0.006)	Loss 3.4497e-01 (2.3648e-01)	Acc@1  88.28 ( 92.19)	Acc@5 100.00 ( 99.35)
Epoch: [61][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.0935e-01 (2.3942e-01)	Acc@1  93.75 ( 92.00)	Acc@5 100.00 ( 99.37)
Epoch: [61][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.4302e-01 (2.4617e-01)	Acc@1  89.84 ( 91.88)	Acc@5  98.44 ( 99.30)
Epoch: [61][ 70/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5903e-01 (2.4492e-01)	Acc@1  90.62 ( 91.92)	Acc@5 100.00 ( 99.34)
Epoch: [61][ 80/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.8125e-01 (2.4284e-01)	Acc@1  89.84 ( 92.03)	Acc@5  99.22 ( 99.35)
Epoch: [61][ 90/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.2961e-01 (2.4374e-01)	Acc@1  92.97 ( 92.03)	Acc@5  98.44 ( 99.34)
Epoch: [61][100/391]	Time  0.038 ( 0.027)	Data  0.003 ( 0.003)	Loss 1.7712e-01 (2.4486e-01)	Acc@1  93.75 ( 91.97)	Acc@5  99.22 ( 99.31)
Epoch: [61][110/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.3623e-01 (2.4075e-01)	Acc@1  96.09 ( 92.07)	Acc@5 100.00 ( 99.36)
Epoch: [61][120/391]	Time  0.022 ( 0.027)	Data  0.004 ( 0.003)	Loss 1.5808e-01 (2.3933e-01)	Acc@1  96.88 ( 92.14)	Acc@5 100.00 ( 99.39)
Epoch: [61][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2251e-01 (2.3922e-01)	Acc@1  92.97 ( 92.24)	Acc@5  97.66 ( 99.37)
Epoch: [61][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2471e-01 (2.4273e-01)	Acc@1  91.41 ( 92.16)	Acc@5  96.88 ( 99.36)
Epoch: [61][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4475e-01 (2.4217e-01)	Acc@1  90.62 ( 92.16)	Acc@5  99.22 ( 99.36)
Epoch: [61][160/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.6953e-01 (2.4264e-01)	Acc@1  92.19 ( 92.16)	Acc@5  99.22 ( 99.35)
Epoch: [61][170/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8638e-01 (2.4358e-01)	Acc@1  90.62 ( 92.13)	Acc@5  99.22 ( 99.36)
Epoch: [61][180/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6978e-01 (2.4249e-01)	Acc@1  90.62 ( 92.17)	Acc@5  99.22 ( 99.37)
Epoch: [61][190/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7786e-01 (2.4323e-01)	Acc@1  96.88 ( 92.17)	Acc@5  99.22 ( 99.39)
Epoch: [61][200/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8884e-01 (2.4111e-01)	Acc@1  94.53 ( 92.27)	Acc@5 100.00 ( 99.40)
Epoch: [61][210/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2168e-01 (2.3944e-01)	Acc@1  92.19 ( 92.31)	Acc@5  99.22 ( 99.39)
Epoch: [61][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7539e-01 (2.3981e-01)	Acc@1  92.19 ( 92.30)	Acc@5  99.22 ( 99.40)
Epoch: [61][230/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 4.0918e-01 (2.3892e-01)	Acc@1  89.06 ( 92.35)	Acc@5  97.66 ( 99.41)
Epoch: [61][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4673e-01 (2.3968e-01)	Acc@1  97.66 ( 92.33)	Acc@5 100.00 ( 99.40)
Epoch: [61][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7954e-01 (2.3946e-01)	Acc@1  91.41 ( 92.38)	Acc@5  99.22 ( 99.41)
Epoch: [61][260/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4792e-01 (2.3992e-01)	Acc@1  91.41 ( 92.36)	Acc@5  99.22 ( 99.41)
Epoch: [61][270/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9177e-01 (2.4041e-01)	Acc@1  92.97 ( 92.33)	Acc@5 100.00 ( 99.42)
Epoch: [61][280/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.3560e-01 (2.4035e-01)	Acc@1  92.19 ( 92.35)	Acc@5  99.22 ( 99.42)
Epoch: [61][290/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.0056e-01 (2.4023e-01)	Acc@1  92.97 ( 92.34)	Acc@5 100.00 ( 99.42)
Epoch: [61][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4146e-01 (2.3983e-01)	Acc@1  95.31 ( 92.36)	Acc@5  98.44 ( 99.42)
Epoch: [61][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5635e-01 (2.4069e-01)	Acc@1  92.19 ( 92.35)	Acc@5  99.22 ( 99.42)
Epoch: [61][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5122e-01 (2.4082e-01)	Acc@1  93.75 ( 92.33)	Acc@5  98.44 ( 99.42)
Epoch: [61][330/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5220e-01 (2.4004e-01)	Acc@1  89.84 ( 92.35)	Acc@5 100.00 ( 99.43)
Epoch: [61][340/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.2021e-01 (2.4036e-01)	Acc@1  93.75 ( 92.34)	Acc@5  99.22 ( 99.42)
Epoch: [61][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3328e-01 (2.4072e-01)	Acc@1  92.97 ( 92.36)	Acc@5  99.22 ( 99.41)
Epoch: [61][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2375e-01 (2.4113e-01)	Acc@1  91.41 ( 92.33)	Acc@5 100.00 ( 99.41)
Epoch: [61][370/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.4814e-01 (2.4171e-01)	Acc@1  92.19 ( 92.32)	Acc@5  97.66 ( 99.40)
Epoch: [61][380/391]	Time  0.044 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.9248e-01 (2.4226e-01)	Acc@1  91.41 ( 92.31)	Acc@5  98.44 ( 99.39)
Epoch: [61][390/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.5547e-01 (2.4267e-01)	Acc@1  90.00 ( 92.30)	Acc@5  98.75 ( 99.39)
## e[61] optimizer.zero_grad (sum) time: 0.10496187210083008
## e[61]       loss.backward (sum) time: 2.263753890991211
## e[61]      optimizer.step (sum) time: 0.9217572212219238
## epoch[61] training(only) time: 10.129905462265015
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.3428e+00 (1.3428e+00)	Acc@1  74.00 ( 74.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.023 ( 0.026)	Loss 1.4502e+00 (1.5764e+00)	Acc@1  68.00 ( 67.55)	Acc@5  93.00 ( 88.00)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.6689e+00 (1.5692e+00)	Acc@1  64.00 ( 66.95)	Acc@5  91.00 ( 88.24)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.5352e+00 (1.5803e+00)	Acc@1  62.00 ( 66.39)	Acc@5  88.00 ( 88.13)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.6758e+00 (1.5754e+00)	Acc@1  66.00 ( 66.44)	Acc@5  91.00 ( 88.32)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.6777e+00 (1.6034e+00)	Acc@1  67.00 ( 66.35)	Acc@5  90.00 ( 88.14)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.6865e+00 (1.5785e+00)	Acc@1  63.00 ( 66.67)	Acc@5  90.00 ( 88.41)
Test: [ 70/100]	Time  0.015 ( 0.016)	Loss 1.5889e+00 (1.5863e+00)	Acc@1  69.00 ( 66.72)	Acc@5  90.00 ( 88.42)
Test: [ 80/100]	Time  0.012 ( 0.015)	Loss 1.5479e+00 (1.5890e+00)	Acc@1  66.00 ( 66.62)	Acc@5  88.00 ( 88.40)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.9824e+00 (1.5764e+00)	Acc@1  59.00 ( 66.82)	Acc@5  87.00 ( 88.57)
 * Acc@1 66.850 Acc@5 88.560
### epoch[61] execution time: 11.70347809791565
EPOCH 62
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [62][  0/391]	Time  0.174 ( 0.174)	Data  0.144 ( 0.144)	Loss 2.6367e-01 (2.6367e-01)	Acc@1  92.19 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [62][ 10/391]	Time  0.027 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.1597e-01 (2.2004e-01)	Acc@1  97.66 ( 93.04)	Acc@5 100.00 ( 99.57)
Epoch: [62][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 2.3901e-01 (2.2261e-01)	Acc@1  92.97 ( 92.78)	Acc@5 100.00 ( 99.74)
Epoch: [62][ 30/391]	Time  0.034 ( 0.030)	Data  0.003 ( 0.006)	Loss 1.9043e-01 (2.2817e-01)	Acc@1  93.75 ( 92.57)	Acc@5  99.22 ( 99.60)
Epoch: [62][ 40/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.7441e-01 (2.3115e-01)	Acc@1  92.19 ( 92.51)	Acc@5  99.22 ( 99.60)
Epoch: [62][ 50/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.005)	Loss 2.2424e-01 (2.2884e-01)	Acc@1  92.19 ( 92.45)	Acc@5 100.00 ( 99.66)
Epoch: [62][ 60/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7590e-01 (2.3140e-01)	Acc@1  95.31 ( 92.57)	Acc@5  99.22 ( 99.65)
Epoch: [62][ 70/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5977e-01 (2.3379e-01)	Acc@1  94.53 ( 92.56)	Acc@5  98.44 ( 99.64)
Epoch: [62][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.6807e-01 (2.3612e-01)	Acc@1  90.62 ( 92.42)	Acc@5  99.22 ( 99.61)
Epoch: [62][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7029e-01 (2.3537e-01)	Acc@1  96.09 ( 92.38)	Acc@5  99.22 ( 99.62)
Epoch: [62][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4731e-01 (2.3587e-01)	Acc@1  93.75 ( 92.37)	Acc@5  99.22 ( 99.57)
Epoch: [62][110/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8442e-01 (2.3538e-01)	Acc@1  89.84 ( 92.44)	Acc@5  98.44 ( 99.52)
Epoch: [62][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7285e-01 (2.3527e-01)	Acc@1  95.31 ( 92.45)	Acc@5 100.00 ( 99.50)
Epoch: [62][130/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4536e-01 (2.3710e-01)	Acc@1  92.97 ( 92.38)	Acc@5  99.22 ( 99.47)
Epoch: [62][140/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3694e-01 (2.3701e-01)	Acc@1  90.62 ( 92.36)	Acc@5 100.00 ( 99.47)
Epoch: [62][150/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9346e-01 (2.3764e-01)	Acc@1  90.62 ( 92.36)	Acc@5  97.66 ( 99.45)
Epoch: [62][160/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6980e-01 (2.3693e-01)	Acc@1  95.31 ( 92.37)	Acc@5  99.22 ( 99.45)
Epoch: [62][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8003e-01 (2.3763e-01)	Acc@1  92.97 ( 92.37)	Acc@5  99.22 ( 99.45)
Epoch: [62][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7368e-01 (2.3660e-01)	Acc@1  91.41 ( 92.36)	Acc@5  98.44 ( 99.45)
Epoch: [62][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4304e-01 (2.3598e-01)	Acc@1  92.97 ( 92.39)	Acc@5  99.22 ( 99.46)
Epoch: [62][200/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2610e-01 (2.3429e-01)	Acc@1  94.53 ( 92.45)	Acc@5 100.00 ( 99.46)
Epoch: [62][210/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.6685e-01 (2.3419e-01)	Acc@1  90.62 ( 92.41)	Acc@5  99.22 ( 99.46)
Epoch: [62][220/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8271e-01 (2.3516e-01)	Acc@1  87.50 ( 92.35)	Acc@5  99.22 ( 99.44)
Epoch: [62][230/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7393e-01 (2.3461e-01)	Acc@1  90.62 ( 92.37)	Acc@5  99.22 ( 99.44)
Epoch: [62][240/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4075e-01 (2.3423e-01)	Acc@1  95.31 ( 92.35)	Acc@5 100.00 ( 99.45)
Epoch: [62][250/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.1521e-01 (2.3385e-01)	Acc@1  92.19 ( 92.36)	Acc@5 100.00 ( 99.45)
Epoch: [62][260/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.5000e-01 (2.3441e-01)	Acc@1  89.06 ( 92.32)	Acc@5  99.22 ( 99.43)
Epoch: [62][270/391]	Time  0.035 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.3770e-01 (2.3427e-01)	Acc@1  96.09 ( 92.35)	Acc@5 100.00 ( 99.43)
Epoch: [62][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4084e-01 (2.3260e-01)	Acc@1  92.19 ( 92.42)	Acc@5  99.22 ( 99.44)
Epoch: [62][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5171e-01 (2.3325e-01)	Acc@1  92.97 ( 92.41)	Acc@5  99.22 ( 99.43)
Epoch: [62][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1104e-01 (2.3386e-01)	Acc@1  90.62 ( 92.41)	Acc@5  98.44 ( 99.42)
Epoch: [62][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2717e-01 (2.3397e-01)	Acc@1  92.19 ( 92.43)	Acc@5  99.22 ( 99.41)
Epoch: [62][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7905e-01 (2.3447e-01)	Acc@1  90.62 ( 92.42)	Acc@5 100.00 ( 99.42)
Epoch: [62][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5073e-01 (2.3442e-01)	Acc@1  92.97 ( 92.40)	Acc@5  99.22 ( 99.42)
Epoch: [62][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2646e-01 (2.3382e-01)	Acc@1  96.88 ( 92.46)	Acc@5  99.22 ( 99.41)
Epoch: [62][350/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.6660e-01 (2.3337e-01)	Acc@1  92.19 ( 92.48)	Acc@5 100.00 ( 99.42)
Epoch: [62][360/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.1680e-01 (2.3349e-01)	Acc@1  93.75 ( 92.49)	Acc@5 100.00 ( 99.42)
Epoch: [62][370/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4954e-01 (2.3312e-01)	Acc@1  96.09 ( 92.51)	Acc@5 100.00 ( 99.43)
Epoch: [62][380/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9321e-01 (2.3307e-01)	Acc@1  90.62 ( 92.52)	Acc@5  99.22 ( 99.43)
Epoch: [62][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3281e-01 (2.3326e-01)	Acc@1  96.25 ( 92.51)	Acc@5 100.00 ( 99.42)
## e[62] optimizer.zero_grad (sum) time: 0.10804271697998047
## e[62]       loss.backward (sum) time: 2.244563341140747
## e[62]      optimizer.step (sum) time: 0.9317538738250732
## epoch[62] training(only) time: 10.103808403015137
# Switched to evaluate mode...
Test: [  0/100]	Time  0.133 ( 0.133)	Loss 1.2871e+00 (1.2871e+00)	Acc@1  72.00 ( 72.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.012 ( 0.025)	Loss 1.4092e+00 (1.5519e+00)	Acc@1  67.00 ( 66.82)	Acc@5  92.00 ( 88.00)
Test: [ 20/100]	Time  0.012 ( 0.019)	Loss 1.6611e+00 (1.5566e+00)	Acc@1  65.00 ( 66.48)	Acc@5  89.00 ( 88.10)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.5352e+00 (1.5645e+00)	Acc@1  63.00 ( 66.13)	Acc@5  87.00 ( 88.00)
Test: [ 40/100]	Time  0.010 ( 0.016)	Loss 1.6689e+00 (1.5589e+00)	Acc@1  67.00 ( 66.56)	Acc@5  90.00 ( 88.22)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.6436e+00 (1.5855e+00)	Acc@1  67.00 ( 66.45)	Acc@5  90.00 ( 88.08)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.6875e+00 (1.5606e+00)	Acc@1  62.00 ( 66.67)	Acc@5  91.00 ( 88.36)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.4980e+00 (1.5673e+00)	Acc@1  68.00 ( 66.69)	Acc@5  89.00 ( 88.39)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.5010e+00 (1.5700e+00)	Acc@1  67.00 ( 66.63)	Acc@5  89.00 ( 88.42)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9521e+00 (1.5573e+00)	Acc@1  58.00 ( 66.81)	Acc@5  89.00 ( 88.60)
 * Acc@1 66.880 Acc@5 88.590
### epoch[62] execution time: 11.668261766433716
EPOCH 63
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [63][  0/391]	Time  0.173 ( 0.173)	Data  0.148 ( 0.148)	Loss 2.8467e-01 (2.8467e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [63][ 10/391]	Time  0.028 ( 0.039)	Data  0.001 ( 0.016)	Loss 2.5073e-01 (2.1643e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.57)
Epoch: [63][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.3232e-01 (2.1743e-01)	Acc@1  96.88 ( 92.97)	Acc@5 100.00 ( 99.63)
Epoch: [63][ 30/391]	Time  0.038 ( 0.030)	Data  0.005 ( 0.007)	Loss 4.6069e-01 (2.3507e-01)	Acc@1  85.94 ( 92.54)	Acc@5  98.44 ( 99.55)
Epoch: [63][ 40/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.8345e-01 (2.3080e-01)	Acc@1  89.84 ( 92.61)	Acc@5  98.44 ( 99.52)
Epoch: [63][ 50/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 3.2129e-01 (2.3926e-01)	Acc@1  89.06 ( 92.39)	Acc@5  99.22 ( 99.42)
Epoch: [63][ 60/391]	Time  0.024 ( 0.028)	Data  0.002 ( 0.004)	Loss 3.1494e-01 (2.3890e-01)	Acc@1  89.84 ( 92.44)	Acc@5  99.22 ( 99.39)
Epoch: [63][ 70/391]	Time  0.039 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.7026e-01 (2.3656e-01)	Acc@1  92.19 ( 92.50)	Acc@5  99.22 ( 99.43)
Epoch: [63][ 80/391]	Time  0.034 ( 0.027)	Data  0.005 ( 0.004)	Loss 3.0273e-01 (2.4013e-01)	Acc@1  92.97 ( 92.46)	Acc@5  99.22 ( 99.43)
Epoch: [63][ 90/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.0654e-01 (2.3975e-01)	Acc@1  92.97 ( 92.45)	Acc@5  99.22 ( 99.43)
Epoch: [63][100/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7053e-01 (2.3521e-01)	Acc@1  96.09 ( 92.56)	Acc@5  99.22 ( 99.46)
Epoch: [63][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3010e-01 (2.3411e-01)	Acc@1  89.84 ( 92.54)	Acc@5  99.22 ( 99.44)
Epoch: [63][120/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8030e-01 (2.3419e-01)	Acc@1  93.75 ( 92.52)	Acc@5 100.00 ( 99.46)
Epoch: [63][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2080e-01 (2.3580e-01)	Acc@1  90.62 ( 92.50)	Acc@5  98.44 ( 99.43)
Epoch: [63][140/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.4570e-01 (2.3661e-01)	Acc@1  88.28 ( 92.46)	Acc@5  99.22 ( 99.44)
Epoch: [63][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3779e-01 (2.3694e-01)	Acc@1  90.62 ( 92.39)	Acc@5 100.00 ( 99.46)
Epoch: [63][160/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8970e-01 (2.3484e-01)	Acc@1  92.97 ( 92.51)	Acc@5 100.00 ( 99.47)
Epoch: [63][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2998e-01 (2.3431e-01)	Acc@1  90.62 ( 92.53)	Acc@5  99.22 ( 99.48)
Epoch: [63][180/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.7563e-01 (2.3457e-01)	Acc@1  92.97 ( 92.59)	Acc@5 100.00 ( 99.48)
Epoch: [63][190/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9336e-01 (2.3303e-01)	Acc@1  94.53 ( 92.65)	Acc@5  98.44 ( 99.48)
Epoch: [63][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0752e-01 (2.3336e-01)	Acc@1  93.75 ( 92.58)	Acc@5  99.22 ( 99.49)
Epoch: [63][210/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.7712e-01 (2.3448e-01)	Acc@1  95.31 ( 92.57)	Acc@5  99.22 ( 99.47)
Epoch: [63][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6663e-01 (2.3321e-01)	Acc@1  94.53 ( 92.62)	Acc@5  99.22 ( 99.47)
Epoch: [63][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8140e-01 (2.3197e-01)	Acc@1  94.53 ( 92.64)	Acc@5 100.00 ( 99.48)
Epoch: [63][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5244e-01 (2.3200e-01)	Acc@1  91.41 ( 92.63)	Acc@5  99.22 ( 99.48)
Epoch: [63][250/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6733e-01 (2.3185e-01)	Acc@1  92.19 ( 92.60)	Acc@5 100.00 ( 99.49)
Epoch: [63][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3081e-01 (2.3195e-01)	Acc@1  89.06 ( 92.60)	Acc@5  97.66 ( 99.48)
Epoch: [63][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3328e-01 (2.3198e-01)	Acc@1  92.19 ( 92.61)	Acc@5 100.00 ( 99.48)
Epoch: [63][280/391]	Time  0.046 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.4341e-01 (2.3249e-01)	Acc@1  93.75 ( 92.59)	Acc@5 100.00 ( 99.48)
Epoch: [63][290/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.6392e-01 (2.3279e-01)	Acc@1  90.62 ( 92.57)	Acc@5  99.22 ( 99.48)
Epoch: [63][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6440e-01 (2.3315e-01)	Acc@1  92.19 ( 92.54)	Acc@5 100.00 ( 99.49)
Epoch: [63][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0493e-01 (2.3522e-01)	Acc@1  92.19 ( 92.49)	Acc@5  98.44 ( 99.47)
Epoch: [63][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5830e-01 (2.3538e-01)	Acc@1  90.62 ( 92.46)	Acc@5  99.22 ( 99.47)
Epoch: [63][330/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1423e-01 (2.3525e-01)	Acc@1  93.75 ( 92.46)	Acc@5 100.00 ( 99.47)
Epoch: [63][340/391]	Time  0.025 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.8140e-01 (2.3495e-01)	Acc@1  94.53 ( 92.47)	Acc@5 100.00 ( 99.47)
Epoch: [63][350/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2876e-01 (2.3533e-01)	Acc@1  90.62 ( 92.47)	Acc@5 100.00 ( 99.47)
Epoch: [63][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4197e-01 (2.3484e-01)	Acc@1  96.09 ( 92.49)	Acc@5 100.00 ( 99.47)
Epoch: [63][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4377e-01 (2.3506e-01)	Acc@1  90.62 ( 92.48)	Acc@5  99.22 ( 99.47)
Epoch: [63][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9980e-01 (2.3516e-01)	Acc@1  90.62 ( 92.46)	Acc@5  99.22 ( 99.47)
Epoch: [63][390/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0078e-01 (2.3503e-01)	Acc@1  90.00 ( 92.51)	Acc@5  98.75 ( 99.47)
## e[63] optimizer.zero_grad (sum) time: 0.10570955276489258
## e[63]       loss.backward (sum) time: 2.247722864151001
## e[63]      optimizer.step (sum) time: 0.9225027561187744
## epoch[63] training(only) time: 10.15282392501831
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 1.2910e+00 (1.2910e+00)	Acc@1  71.00 ( 71.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.015 ( 0.025)	Loss 1.4492e+00 (1.5747e+00)	Acc@1  67.00 ( 67.45)	Acc@5  93.00 ( 88.09)
Test: [ 20/100]	Time  0.014 ( 0.019)	Loss 1.6396e+00 (1.5747e+00)	Acc@1  66.00 ( 67.00)	Acc@5  91.00 ( 88.19)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.4590e+00 (1.5742e+00)	Acc@1  66.00 ( 66.87)	Acc@5  88.00 ( 88.13)
Test: [ 40/100]	Time  0.013 ( 0.016)	Loss 1.6758e+00 (1.5687e+00)	Acc@1  67.00 ( 67.10)	Acc@5  90.00 ( 88.34)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5869e+00 (1.5950e+00)	Acc@1  67.00 ( 66.88)	Acc@5  90.00 ( 88.12)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.7012e+00 (1.5702e+00)	Acc@1  62.00 ( 67.00)	Acc@5  93.00 ( 88.54)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.5508e+00 (1.5770e+00)	Acc@1  67.00 ( 67.06)	Acc@5  91.00 ( 88.61)
Test: [ 80/100]	Time  0.013 ( 0.015)	Loss 1.4717e+00 (1.5803e+00)	Acc@1  69.00 ( 67.05)	Acc@5  89.00 ( 88.54)
Test: [ 90/100]	Time  0.015 ( 0.015)	Loss 1.9160e+00 (1.5677e+00)	Acc@1  59.00 ( 67.18)	Acc@5  87.00 ( 88.68)
 * Acc@1 67.200 Acc@5 88.660
### epoch[63] execution time: 11.746715307235718
EPOCH 64
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [64][  0/391]	Time  0.178 ( 0.178)	Data  0.150 ( 0.150)	Loss 1.6150e-01 (1.6150e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [64][ 10/391]	Time  0.021 ( 0.037)	Data  0.002 ( 0.016)	Loss 2.4829e-01 (2.2842e-01)	Acc@1  90.62 ( 92.26)	Acc@5 100.00 ( 99.57)
Epoch: [64][ 20/391]	Time  0.028 ( 0.032)	Data  0.001 ( 0.009)	Loss 3.4131e-01 (2.3092e-01)	Acc@1  89.84 ( 92.30)	Acc@5 100.00 ( 99.70)
Epoch: [64][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.7002e-01 (2.3151e-01)	Acc@1  93.75 ( 92.36)	Acc@5  98.44 ( 99.62)
Epoch: [64][ 40/391]	Time  0.023 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.1545e-01 (2.2654e-01)	Acc@1  92.19 ( 92.63)	Acc@5  99.22 ( 99.62)
Epoch: [64][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.6343e-01 (2.3081e-01)	Acc@1  91.41 ( 92.40)	Acc@5  99.22 ( 99.51)
Epoch: [64][ 60/391]	Time  0.038 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.9666e-01 (2.2751e-01)	Acc@1  95.31 ( 92.69)	Acc@5  99.22 ( 99.47)
Epoch: [64][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.4644e-01 (2.3050e-01)	Acc@1  89.84 ( 92.65)	Acc@5  99.22 ( 99.49)
Epoch: [64][ 80/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.3157e-01 (2.3249e-01)	Acc@1  94.53 ( 92.57)	Acc@5  99.22 ( 99.47)
Epoch: [64][ 90/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5222e-01 (2.3343e-01)	Acc@1  94.53 ( 92.52)	Acc@5 100.00 ( 99.44)
Epoch: [64][100/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5537e-01 (2.3179e-01)	Acc@1  92.97 ( 92.58)	Acc@5  98.44 ( 99.47)
Epoch: [64][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4497e-01 (2.2935e-01)	Acc@1  89.84 ( 92.71)	Acc@5  99.22 ( 99.48)
Epoch: [64][120/391]	Time  0.038 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6370e-01 (2.2872e-01)	Acc@1  94.53 ( 92.78)	Acc@5 100.00 ( 99.47)
Epoch: [64][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9043e-01 (2.2916e-01)	Acc@1  92.19 ( 92.69)	Acc@5 100.00 ( 99.50)
Epoch: [64][140/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3838e-01 (2.2999e-01)	Acc@1  88.28 ( 92.68)	Acc@5  99.22 ( 99.50)
Epoch: [64][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9541e-01 (2.2995e-01)	Acc@1  90.62 ( 92.72)	Acc@5  99.22 ( 99.49)
Epoch: [64][160/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6392e-01 (2.3085e-01)	Acc@1  91.41 ( 92.68)	Acc@5  98.44 ( 99.47)
Epoch: [64][170/391]	Time  0.023 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.0007e-01 (2.2970e-01)	Acc@1  92.97 ( 92.69)	Acc@5  99.22 ( 99.47)
Epoch: [64][180/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.5806e-01 (2.2994e-01)	Acc@1  92.19 ( 92.76)	Acc@5  99.22 ( 99.46)
Epoch: [64][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2034e-01 (2.2981e-01)	Acc@1  95.31 ( 92.72)	Acc@5  98.44 ( 99.44)
Epoch: [64][200/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3398e-01 (2.3092e-01)	Acc@1  89.84 ( 92.69)	Acc@5  99.22 ( 99.44)
Epoch: [64][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3538e-01 (2.3049e-01)	Acc@1  96.09 ( 92.74)	Acc@5 100.00 ( 99.44)
Epoch: [64][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1445e-01 (2.3129e-01)	Acc@1  90.62 ( 92.71)	Acc@5  98.44 ( 99.43)
Epoch: [64][230/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.1143e-01 (2.3151e-01)	Acc@1  93.75 ( 92.70)	Acc@5 100.00 ( 99.42)
Epoch: [64][240/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0276e-01 (2.3143e-01)	Acc@1  93.75 ( 92.70)	Acc@5 100.00 ( 99.44)
Epoch: [64][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6001e-01 (2.3106e-01)	Acc@1  89.84 ( 92.73)	Acc@5  99.22 ( 99.42)
Epoch: [64][260/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0431e-01 (2.3069e-01)	Acc@1  96.88 ( 92.71)	Acc@5 100.00 ( 99.43)
Epoch: [64][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8027e-01 (2.3017e-01)	Acc@1  92.97 ( 92.75)	Acc@5  98.44 ( 99.44)
Epoch: [64][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8970e-01 (2.3074e-01)	Acc@1  94.53 ( 92.74)	Acc@5 100.00 ( 99.44)
Epoch: [64][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7651e-01 (2.3036e-01)	Acc@1  95.31 ( 92.76)	Acc@5  99.22 ( 99.44)
Epoch: [64][300/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6809e-01 (2.2998e-01)	Acc@1  94.53 ( 92.75)	Acc@5 100.00 ( 99.45)
Epoch: [64][310/391]	Time  0.038 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.0618e-01 (2.2876e-01)	Acc@1  92.97 ( 92.77)	Acc@5 100.00 ( 99.45)
Epoch: [64][320/391]	Time  0.034 ( 0.026)	Data  0.003 ( 0.002)	Loss 4.2603e-01 (2.3014e-01)	Acc@1  86.72 ( 92.74)	Acc@5  98.44 ( 99.45)
Epoch: [64][330/391]	Time  0.022 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.8750e-01 (2.2919e-01)	Acc@1  94.53 ( 92.78)	Acc@5 100.00 ( 99.44)
Epoch: [64][340/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4807e-01 (2.2951e-01)	Acc@1  95.31 ( 92.77)	Acc@5 100.00 ( 99.43)
Epoch: [64][350/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7395e-01 (2.2916e-01)	Acc@1  94.53 ( 92.78)	Acc@5 100.00 ( 99.44)
Epoch: [64][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4084e-01 (2.2962e-01)	Acc@1  92.19 ( 92.77)	Acc@5 100.00 ( 99.44)
Epoch: [64][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7773e-01 (2.2914e-01)	Acc@1  94.53 ( 92.78)	Acc@5  99.22 ( 99.44)
Epoch: [64][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0056e-01 (2.2850e-01)	Acc@1  92.19 ( 92.79)	Acc@5 100.00 ( 99.45)
Epoch: [64][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4622e-01 (2.2842e-01)	Acc@1  91.25 ( 92.80)	Acc@5 100.00 ( 99.45)
## e[64] optimizer.zero_grad (sum) time: 0.10567116737365723
## e[64]       loss.backward (sum) time: 2.2741193771362305
## e[64]      optimizer.step (sum) time: 0.9404621124267578
## epoch[64] training(only) time: 10.105595350265503
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.3076e+00 (1.3076e+00)	Acc@1  74.00 ( 74.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.4443e+00 (1.5627e+00)	Acc@1  64.00 ( 67.45)	Acc@5  92.00 ( 88.18)
Test: [ 20/100]	Time  0.021 ( 0.020)	Loss 1.6182e+00 (1.5659e+00)	Acc@1  67.00 ( 66.71)	Acc@5  91.00 ( 87.95)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.4854e+00 (1.5653e+00)	Acc@1  67.00 ( 66.61)	Acc@5  88.00 ( 88.13)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.6445e+00 (1.5546e+00)	Acc@1  67.00 ( 66.95)	Acc@5  90.00 ( 88.27)
Test: [ 50/100]	Time  0.023 ( 0.016)	Loss 1.5967e+00 (1.5808e+00)	Acc@1  67.00 ( 66.84)	Acc@5  90.00 ( 88.10)
Test: [ 60/100]	Time  0.023 ( 0.016)	Loss 1.7354e+00 (1.5572e+00)	Acc@1  62.00 ( 67.03)	Acc@5  93.00 ( 88.49)
Test: [ 70/100]	Time  0.009 ( 0.016)	Loss 1.5146e+00 (1.5638e+00)	Acc@1  68.00 ( 67.07)	Acc@5  89.00 ( 88.54)
Test: [ 80/100]	Time  0.016 ( 0.016)	Loss 1.4756e+00 (1.5647e+00)	Acc@1  67.00 ( 66.96)	Acc@5  88.00 ( 88.51)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.8701e+00 (1.5519e+00)	Acc@1  60.00 ( 67.15)	Acc@5  88.00 ( 88.65)
 * Acc@1 67.150 Acc@5 88.660
### epoch[64] execution time: 11.731441497802734
EPOCH 65
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [65][  0/391]	Time  0.176 ( 0.176)	Data  0.147 ( 0.147)	Loss 2.1057e-01 (2.1057e-01)	Acc@1  96.09 ( 96.09)	Acc@5 100.00 (100.00)
Epoch: [65][ 10/391]	Time  0.039 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.3586e-01 (2.3145e-01)	Acc@1  96.88 ( 93.54)	Acc@5 100.00 ( 99.36)
Epoch: [65][ 20/391]	Time  0.022 ( 0.033)	Data  0.000 ( 0.009)	Loss 2.6172e-01 (2.3001e-01)	Acc@1  89.84 ( 93.15)	Acc@5 100.00 ( 99.55)
Epoch: [65][ 30/391]	Time  0.031 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.5645e-01 (2.2780e-01)	Acc@1  89.06 ( 93.20)	Acc@5 100.00 ( 99.55)
Epoch: [65][ 40/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.8909e-01 (2.2683e-01)	Acc@1  93.75 ( 93.24)	Acc@5  99.22 ( 99.52)
Epoch: [65][ 50/391]	Time  0.038 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.8467e-01 (2.2796e-01)	Acc@1  91.41 ( 93.20)	Acc@5  98.44 ( 99.43)
Epoch: [65][ 60/391]	Time  0.043 ( 0.028)	Data  0.004 ( 0.004)	Loss 2.8174e-01 (2.2883e-01)	Acc@1  91.41 ( 93.19)	Acc@5  99.22 ( 99.46)
Epoch: [65][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4368e-01 (2.2723e-01)	Acc@1  95.31 ( 93.13)	Acc@5 100.00 ( 99.49)
Epoch: [65][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6370e-01 (2.2636e-01)	Acc@1  94.53 ( 93.24)	Acc@5 100.00 ( 99.46)
Epoch: [65][ 90/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3806e-01 (2.2331e-01)	Acc@1  96.88 ( 93.29)	Acc@5 100.00 ( 99.50)
Epoch: [65][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9053e-01 (2.2423e-01)	Acc@1  90.62 ( 93.25)	Acc@5  99.22 ( 99.50)
Epoch: [65][110/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.4707e-01 (2.2585e-01)	Acc@1  92.19 ( 93.16)	Acc@5  99.22 ( 99.47)
Epoch: [65][120/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5466e-01 (2.2614e-01)	Acc@1  96.88 ( 93.10)	Acc@5  99.22 ( 99.46)
Epoch: [65][130/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.7759e-01 (2.2775e-01)	Acc@1  92.19 ( 92.99)	Acc@5  99.22 ( 99.47)
Epoch: [65][140/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1167e-01 (2.3026e-01)	Acc@1  89.84 ( 92.88)	Acc@5 100.00 ( 99.45)
Epoch: [65][150/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1655e-01 (2.3176e-01)	Acc@1  91.41 ( 92.81)	Acc@5 100.00 ( 99.44)
Epoch: [65][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5830e-01 (2.2942e-01)	Acc@1  92.19 ( 92.87)	Acc@5  99.22 ( 99.47)
Epoch: [65][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2351e-01 (2.2859e-01)	Acc@1  94.53 ( 92.92)	Acc@5 100.00 ( 99.47)
Epoch: [65][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2656e-01 (2.2850e-01)	Acc@1  93.75 ( 92.90)	Acc@5 100.00 ( 99.48)
Epoch: [65][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4768e-01 (2.2807e-01)	Acc@1  90.62 ( 92.92)	Acc@5  99.22 ( 99.48)
Epoch: [65][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4133e-01 (2.2822e-01)	Acc@1  91.41 ( 92.93)	Acc@5  99.22 ( 99.46)
Epoch: [65][210/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4829e-01 (2.2764e-01)	Acc@1  93.75 ( 92.97)	Acc@5 100.00 ( 99.47)
Epoch: [65][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7358e-01 (2.2796e-01)	Acc@1  93.75 ( 92.95)	Acc@5 100.00 ( 99.47)
Epoch: [65][230/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.1714e-01 (2.2754e-01)	Acc@1  92.19 ( 92.95)	Acc@5  99.22 ( 99.47)
Epoch: [65][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6587e-01 (2.2868e-01)	Acc@1  92.19 ( 92.90)	Acc@5  99.22 ( 99.46)
Epoch: [65][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8638e-01 (2.2934e-01)	Acc@1  91.41 ( 92.85)	Acc@5  98.44 ( 99.45)
Epoch: [65][260/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.5283e-01 (2.2989e-01)	Acc@1  94.53 ( 92.84)	Acc@5 100.00 ( 99.45)
Epoch: [65][270/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.9897e-01 (2.2894e-01)	Acc@1  92.19 ( 92.84)	Acc@5 100.00 ( 99.45)
Epoch: [65][280/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.3682e-01 (2.2975e-01)	Acc@1  94.53 ( 92.83)	Acc@5  98.44 ( 99.44)
Epoch: [65][290/391]	Time  0.030 ( 0.026)	Data  0.005 ( 0.002)	Loss 2.4780e-01 (2.2989e-01)	Acc@1  92.19 ( 92.81)	Acc@5 100.00 ( 99.44)
Epoch: [65][300/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5425e-01 (2.2935e-01)	Acc@1  89.06 ( 92.86)	Acc@5  99.22 ( 99.44)
Epoch: [65][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3623e-01 (2.2895e-01)	Acc@1  98.44 ( 92.87)	Acc@5 100.00 ( 99.44)
Epoch: [65][320/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.4036e-01 (2.2986e-01)	Acc@1  91.41 ( 92.84)	Acc@5  99.22 ( 99.43)
Epoch: [65][330/391]	Time  0.024 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.1860e-01 (2.3034e-01)	Acc@1  91.41 ( 92.82)	Acc@5  98.44 ( 99.43)
Epoch: [65][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7246e-01 (2.3079e-01)	Acc@1  90.62 ( 92.80)	Acc@5  98.44 ( 99.43)
Epoch: [65][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5537e-01 (2.3084e-01)	Acc@1  94.53 ( 92.82)	Acc@5  99.22 ( 99.43)
Epoch: [65][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.3921e-01 (2.3115e-01)	Acc@1  87.50 ( 92.82)	Acc@5  96.88 ( 99.42)
Epoch: [65][370/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.9470e-01 (2.3066e-01)	Acc@1  95.31 ( 92.82)	Acc@5  99.22 ( 99.41)
Epoch: [65][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9202e-01 (2.3067e-01)	Acc@1  94.53 ( 92.79)	Acc@5 100.00 ( 99.42)
Epoch: [65][390/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6636e-01 (2.3061e-01)	Acc@1  92.50 ( 92.78)	Acc@5  98.75 ( 99.42)
## e[65] optimizer.zero_grad (sum) time: 0.10465383529663086
## e[65]       loss.backward (sum) time: 2.310011625289917
## e[65]      optimizer.step (sum) time: 0.9339234828948975
## epoch[65] training(only) time: 10.110126256942749
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.3262e+00 (1.3262e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.020 ( 0.026)	Loss 1.4590e+00 (1.5627e+00)	Acc@1  64.00 ( 67.91)	Acc@5  92.00 ( 88.36)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 1.6504e+00 (1.5625e+00)	Acc@1  67.00 ( 67.24)	Acc@5  90.00 ( 88.14)
Test: [ 30/100]	Time  0.018 ( 0.018)	Loss 1.4795e+00 (1.5625e+00)	Acc@1  66.00 ( 66.84)	Acc@5  87.00 ( 88.19)
Test: [ 40/100]	Time  0.016 ( 0.017)	Loss 1.6680e+00 (1.5565e+00)	Acc@1  67.00 ( 67.12)	Acc@5  90.00 ( 88.34)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5771e+00 (1.5791e+00)	Acc@1  67.00 ( 67.10)	Acc@5  89.00 ( 88.08)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 1.7344e+00 (1.5567e+00)	Acc@1  62.00 ( 67.20)	Acc@5  92.00 ( 88.51)
Test: [ 70/100]	Time  0.019 ( 0.016)	Loss 1.5342e+00 (1.5653e+00)	Acc@1  66.00 ( 67.25)	Acc@5  91.00 ( 88.56)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 1.4658e+00 (1.5669e+00)	Acc@1  69.00 ( 67.26)	Acc@5  88.00 ( 88.57)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 1.9062e+00 (1.5534e+00)	Acc@1  58.00 ( 67.45)	Acc@5  87.00 ( 88.73)
 * Acc@1 67.500 Acc@5 88.700
### epoch[65] execution time: 11.713056564331055
EPOCH 66
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [66][  0/391]	Time  0.177 ( 0.177)	Data  0.152 ( 0.152)	Loss 2.5366e-01 (2.5366e-01)	Acc@1  90.62 ( 90.62)	Acc@5 100.00 (100.00)
Epoch: [66][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.015)	Loss 1.6199e-01 (2.4129e-01)	Acc@1  93.75 ( 92.12)	Acc@5 100.00 ( 99.57)
Epoch: [66][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 2.9565e-01 (2.3016e-01)	Acc@1  89.84 ( 92.37)	Acc@5  99.22 ( 99.55)
Epoch: [66][ 30/391]	Time  0.021 ( 0.030)	Data  0.000 ( 0.007)	Loss 2.3157e-01 (2.2457e-01)	Acc@1  91.41 ( 92.72)	Acc@5 100.00 ( 99.47)
Epoch: [66][ 40/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.2156e-01 (2.3527e-01)	Acc@1  94.53 ( 92.40)	Acc@5 100.00 ( 99.43)
Epoch: [66][ 50/391]	Time  0.025 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.6709e-01 (2.3024e-01)	Acc@1  92.97 ( 92.63)	Acc@5  98.44 ( 99.48)
Epoch: [66][ 60/391]	Time  0.032 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0276e-01 (2.2972e-01)	Acc@1  91.41 ( 92.56)	Acc@5 100.00 ( 99.47)
Epoch: [66][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.1494e-01 (2.3207e-01)	Acc@1  90.62 ( 92.54)	Acc@5  99.22 ( 99.44)
Epoch: [66][ 80/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.5830e-01 (2.3222e-01)	Acc@1  89.84 ( 92.61)	Acc@5 100.00 ( 99.43)
Epoch: [66][ 90/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0825e-01 (2.2951e-01)	Acc@1  92.19 ( 92.67)	Acc@5 100.00 ( 99.44)
Epoch: [66][100/391]	Time  0.037 ( 0.027)	Data  0.003 ( 0.003)	Loss 2.0093e-01 (2.2866e-01)	Acc@1  93.75 ( 92.73)	Acc@5  99.22 ( 99.43)
Epoch: [66][110/391]	Time  0.026 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.5464e-01 (2.2426e-01)	Acc@1  92.19 ( 92.86)	Acc@5 100.00 ( 99.47)
Epoch: [66][120/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8604e-01 (2.2161e-01)	Acc@1  96.09 ( 92.97)	Acc@5  99.22 ( 99.45)
Epoch: [66][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3721e-01 (2.1942e-01)	Acc@1  95.31 ( 93.08)	Acc@5 100.00 ( 99.46)
Epoch: [66][140/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 4.0576e-01 (2.2159e-01)	Acc@1  87.50 ( 93.05)	Acc@5 100.00 ( 99.46)
Epoch: [66][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2144e-01 (2.1997e-01)	Acc@1  92.97 ( 93.07)	Acc@5  98.44 ( 99.45)
Epoch: [66][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6443e-01 (2.2166e-01)	Acc@1  95.31 ( 93.01)	Acc@5 100.00 ( 99.44)
Epoch: [66][170/391]	Time  0.039 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.7773e-01 (2.2107e-01)	Acc@1  93.75 ( 93.00)	Acc@5  99.22 ( 99.43)
Epoch: [66][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1152e-01 (2.2182e-01)	Acc@1  91.41 ( 92.98)	Acc@5  99.22 ( 99.42)
Epoch: [66][190/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7017e-01 (2.2420e-01)	Acc@1  93.75 ( 92.86)	Acc@5  98.44 ( 99.40)
Epoch: [66][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8662e-01 (2.2538e-01)	Acc@1  89.06 ( 92.83)	Acc@5  99.22 ( 99.40)
Epoch: [66][210/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.2620e-01 (2.2436e-01)	Acc@1  91.41 ( 92.85)	Acc@5  99.22 ( 99.41)
Epoch: [66][220/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.2046e-01 (2.2563e-01)	Acc@1  93.75 ( 92.81)	Acc@5 100.00 ( 99.42)
Epoch: [66][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9995e-01 (2.2519e-01)	Acc@1  95.31 ( 92.83)	Acc@5  99.22 ( 99.41)
Epoch: [66][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5659e-01 (2.2612e-01)	Acc@1  92.19 ( 92.78)	Acc@5 100.00 ( 99.44)
Epoch: [66][250/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6465e-01 (2.2563e-01)	Acc@1  89.84 ( 92.79)	Acc@5 100.00 ( 99.45)
Epoch: [66][260/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3169e-01 (2.2460e-01)	Acc@1  91.41 ( 92.80)	Acc@5 100.00 ( 99.46)
Epoch: [66][270/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5732e-01 (2.2380e-01)	Acc@1  94.53 ( 92.82)	Acc@5  99.22 ( 99.46)
Epoch: [66][280/391]	Time  0.042 ( 0.026)	Data  0.003 ( 0.002)	Loss 3.4302e-01 (2.2405e-01)	Acc@1  91.41 ( 92.83)	Acc@5  98.44 ( 99.46)
Epoch: [66][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.2349e-01 (2.2381e-01)	Acc@1  89.84 ( 92.86)	Acc@5  99.22 ( 99.46)
Epoch: [66][300/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0190e-01 (2.2282e-01)	Acc@1  92.19 ( 92.89)	Acc@5  99.22 ( 99.47)
Epoch: [66][310/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7065e-01 (2.2366e-01)	Acc@1  95.31 ( 92.87)	Acc@5 100.00 ( 99.47)
Epoch: [66][320/391]	Time  0.041 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9446e-01 (2.2293e-01)	Acc@1  92.19 ( 92.88)	Acc@5  99.22 ( 99.48)
Epoch: [66][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6797e-01 (2.2209e-01)	Acc@1  93.75 ( 92.93)	Acc@5 100.00 ( 99.48)
Epoch: [66][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3425e-01 (2.2282e-01)	Acc@1  92.97 ( 92.90)	Acc@5  98.44 ( 99.47)
Epoch: [66][350/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.3792e-01 (2.2331e-01)	Acc@1  94.53 ( 92.91)	Acc@5  99.22 ( 99.47)
Epoch: [66][360/391]	Time  0.030 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.7505e-01 (2.2319e-01)	Acc@1  93.75 ( 92.93)	Acc@5 100.00 ( 99.47)
Epoch: [66][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4121e-01 (2.2273e-01)	Acc@1  92.19 ( 92.93)	Acc@5 100.00 ( 99.48)
Epoch: [66][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8540e-01 (2.2328e-01)	Acc@1  89.06 ( 92.89)	Acc@5  97.66 ( 99.47)
Epoch: [66][390/391]	Time  0.016 ( 0.025)	Data  0.001 ( 0.002)	Loss 2.4390e-01 (2.2382e-01)	Acc@1  92.50 ( 92.87)	Acc@5  98.75 ( 99.47)
## e[66] optimizer.zero_grad (sum) time: 0.10503602027893066
## e[66]       loss.backward (sum) time: 2.2979745864868164
## e[66]      optimizer.step (sum) time: 0.9419596195220947
## epoch[66] training(only) time: 10.069085359573364
# Switched to evaluate mode...
Test: [  0/100]	Time  0.137 ( 0.137)	Loss 1.3027e+00 (1.3027e+00)	Acc@1  73.00 ( 73.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.011 ( 0.025)	Loss 1.4814e+00 (1.5760e+00)	Acc@1  65.00 ( 67.27)	Acc@5  93.00 ( 88.45)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.6455e+00 (1.5681e+00)	Acc@1  70.00 ( 66.76)	Acc@5  91.00 ( 88.10)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.4355e+00 (1.5670e+00)	Acc@1  68.00 ( 66.61)	Acc@5  88.00 ( 88.26)
Test: [ 40/100]	Time  0.012 ( 0.017)	Loss 1.6650e+00 (1.5614e+00)	Acc@1  67.00 ( 66.80)	Acc@5  90.00 ( 88.51)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6113e+00 (1.5904e+00)	Acc@1  67.00 ( 66.69)	Acc@5  90.00 ( 88.22)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.7266e+00 (1.5667e+00)	Acc@1  62.00 ( 66.85)	Acc@5  93.00 ( 88.57)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.5752e+00 (1.5738e+00)	Acc@1  68.00 ( 66.99)	Acc@5  89.00 ( 88.59)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.4561e+00 (1.5744e+00)	Acc@1  68.00 ( 66.94)	Acc@5  89.00 ( 88.59)
Test: [ 90/100]	Time  0.017 ( 0.015)	Loss 1.8701e+00 (1.5611e+00)	Acc@1  57.00 ( 67.11)	Acc@5  88.00 ( 88.80)
 * Acc@1 67.170 Acc@5 88.790
### epoch[66] execution time: 11.628155708312988
EPOCH 67
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [67][  0/391]	Time  0.181 ( 0.181)	Data  0.149 ( 0.149)	Loss 1.9019e-01 (1.9019e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [67][ 10/391]	Time  0.022 ( 0.038)	Data  0.000 ( 0.015)	Loss 1.1536e-01 (1.8288e-01)	Acc@1  98.44 ( 94.03)	Acc@5 100.00 ( 99.79)
Epoch: [67][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.010)	Loss 1.2573e-01 (2.1211e-01)	Acc@1  96.09 ( 93.38)	Acc@5 100.00 ( 99.44)
Epoch: [67][ 30/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.6101e-01 (2.0917e-01)	Acc@1  94.53 ( 93.45)	Acc@5  99.22 ( 99.40)
Epoch: [67][ 40/391]	Time  0.021 ( 0.028)	Data  0.002 ( 0.006)	Loss 2.6074e-01 (2.1701e-01)	Acc@1  93.75 ( 93.22)	Acc@5  98.44 ( 99.37)
Epoch: [67][ 50/391]	Time  0.027 ( 0.028)	Data  0.002 ( 0.005)	Loss 4.1382e-01 (2.1965e-01)	Acc@1  87.50 ( 93.18)	Acc@5  97.66 ( 99.37)
Epoch: [67][ 60/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.7993e-01 (2.2304e-01)	Acc@1  93.75 ( 92.98)	Acc@5 100.00 ( 99.39)
Epoch: [67][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4307e-01 (2.2157e-01)	Acc@1  96.09 ( 92.86)	Acc@5 100.00 ( 99.43)
Epoch: [67][ 80/391]	Time  0.034 ( 0.027)	Data  0.005 ( 0.004)	Loss 2.0593e-01 (2.1972e-01)	Acc@1  92.19 ( 92.98)	Acc@5 100.00 ( 99.46)
Epoch: [67][ 90/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 2.4304e-01 (2.2256e-01)	Acc@1  89.84 ( 92.92)	Acc@5  99.22 ( 99.42)
Epoch: [67][100/391]	Time  0.024 ( 0.027)	Data  0.003 ( 0.004)	Loss 1.6125e-01 (2.2614e-01)	Acc@1  94.53 ( 92.79)	Acc@5 100.00 ( 99.40)
Epoch: [67][110/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.0931e-01 (2.2437e-01)	Acc@1  97.66 ( 92.91)	Acc@5 100.00 ( 99.42)
Epoch: [67][120/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0762e-01 (2.2622e-01)	Acc@1  89.84 ( 92.87)	Acc@5  98.44 ( 99.37)
Epoch: [67][130/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0959e-01 (2.2537e-01)	Acc@1  92.19 ( 92.88)	Acc@5  99.22 ( 99.39)
Epoch: [67][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3035e-01 (2.2450e-01)	Acc@1  91.41 ( 92.87)	Acc@5 100.00 ( 99.40)
Epoch: [67][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6611e-01 (2.2445e-01)	Acc@1  92.19 ( 92.88)	Acc@5  99.22 ( 99.41)
Epoch: [67][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6611e-01 (2.2453e-01)	Acc@1  90.62 ( 92.88)	Acc@5  99.22 ( 99.41)
Epoch: [67][170/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5039e-01 (2.2351e-01)	Acc@1  95.31 ( 92.92)	Acc@5 100.00 ( 99.41)
Epoch: [67][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8076e-01 (2.2451e-01)	Acc@1  91.41 ( 92.91)	Acc@5 100.00 ( 99.40)
Epoch: [67][190/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5571e-01 (2.2397e-01)	Acc@1  87.50 ( 92.93)	Acc@5  99.22 ( 99.42)
Epoch: [67][200/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5125e-01 (2.2412e-01)	Acc@1  96.88 ( 92.91)	Acc@5 100.00 ( 99.43)
Epoch: [67][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.9053e-01 (2.2516e-01)	Acc@1  89.06 ( 92.88)	Acc@5  99.22 ( 99.43)
Epoch: [67][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2324e-01 (2.2427e-01)	Acc@1  89.06 ( 92.92)	Acc@5  99.22 ( 99.44)
Epoch: [67][230/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8945e-01 (2.2405e-01)	Acc@1  94.53 ( 92.95)	Acc@5 100.00 ( 99.44)
Epoch: [67][240/391]	Time  0.038 ( 0.026)	Data  0.006 ( 0.003)	Loss 2.0190e-01 (2.2451e-01)	Acc@1  93.75 ( 92.92)	Acc@5  99.22 ( 99.44)
Epoch: [67][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6028e-01 (2.2366e-01)	Acc@1  93.75 ( 92.95)	Acc@5 100.00 ( 99.46)
Epoch: [67][260/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.5781e-01 (2.2388e-01)	Acc@1  92.97 ( 92.92)	Acc@5 100.00 ( 99.46)
Epoch: [67][270/391]	Time  0.036 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.8091e-01 (2.2314e-01)	Acc@1  94.53 ( 92.95)	Acc@5  99.22 ( 99.47)
Epoch: [67][280/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.1216e-01 (2.2367e-01)	Acc@1  92.97 ( 92.92)	Acc@5 100.00 ( 99.46)
Epoch: [67][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2866e-01 (2.2353e-01)	Acc@1  96.09 ( 92.92)	Acc@5 100.00 ( 99.47)
Epoch: [67][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8311e-01 (2.2280e-01)	Acc@1  95.31 ( 92.95)	Acc@5  99.22 ( 99.47)
Epoch: [67][310/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6172e-01 (2.2222e-01)	Acc@1  91.41 ( 92.98)	Acc@5  99.22 ( 99.47)
Epoch: [67][320/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.9836e-01 (2.2186e-01)	Acc@1  92.97 ( 93.00)	Acc@5 100.00 ( 99.48)
Epoch: [67][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8164e-01 (2.2148e-01)	Acc@1  94.53 ( 93.02)	Acc@5  99.22 ( 99.48)
Epoch: [67][340/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6465e-01 (2.2170e-01)	Acc@1  92.97 ( 93.01)	Acc@5  99.22 ( 99.48)
Epoch: [67][350/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1761e-01 (2.2117e-01)	Acc@1  96.88 ( 93.04)	Acc@5 100.00 ( 99.47)
Epoch: [67][360/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4438e-01 (2.2079e-01)	Acc@1  92.19 ( 93.04)	Acc@5 100.00 ( 99.48)
Epoch: [67][370/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8931e-01 (2.2149e-01)	Acc@1  89.84 ( 93.02)	Acc@5  99.22 ( 99.48)
Epoch: [67][380/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2180e-01 (2.2140e-01)	Acc@1  92.97 ( 93.02)	Acc@5  98.44 ( 99.48)
Epoch: [67][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6221e-01 (2.2132e-01)	Acc@1  88.75 ( 93.02)	Acc@5 100.00 ( 99.48)
## e[67] optimizer.zero_grad (sum) time: 0.10545611381530762
## e[67]       loss.backward (sum) time: 2.267122507095337
## e[67]      optimizer.step (sum) time: 0.9409053325653076
## epoch[67] training(only) time: 10.125854730606079
# Switched to evaluate mode...
Test: [  0/100]	Time  0.147 ( 0.147)	Loss 1.3174e+00 (1.3174e+00)	Acc@1  73.00 ( 73.00)	Acc@5  90.00 ( 90.00)
Test: [ 10/100]	Time  0.015 ( 0.026)	Loss 1.4082e+00 (1.5678e+00)	Acc@1  66.00 ( 67.73)	Acc@5  93.00 ( 88.45)
Test: [ 20/100]	Time  0.019 ( 0.020)	Loss 1.6494e+00 (1.5666e+00)	Acc@1  67.00 ( 67.00)	Acc@5  91.00 ( 88.14)
Test: [ 30/100]	Time  0.013 ( 0.018)	Loss 1.4912e+00 (1.5690e+00)	Acc@1  64.00 ( 66.74)	Acc@5  87.00 ( 88.23)
Test: [ 40/100]	Time  0.010 ( 0.016)	Loss 1.6689e+00 (1.5635e+00)	Acc@1  67.00 ( 66.93)	Acc@5  90.00 ( 88.39)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.6074e+00 (1.5902e+00)	Acc@1  67.00 ( 66.78)	Acc@5  90.00 ( 88.22)
Test: [ 60/100]	Time  0.011 ( 0.016)	Loss 1.7598e+00 (1.5664e+00)	Acc@1  63.00 ( 66.95)	Acc@5  92.00 ( 88.57)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.5400e+00 (1.5738e+00)	Acc@1  68.00 ( 67.01)	Acc@5  91.00 ( 88.66)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.4980e+00 (1.5767e+00)	Acc@1  66.00 ( 66.98)	Acc@5  89.00 ( 88.60)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 1.9150e+00 (1.5637e+00)	Acc@1  58.00 ( 67.18)	Acc@5  88.00 ( 88.79)
 * Acc@1 67.250 Acc@5 88.770
### epoch[67] execution time: 11.696224689483643
EPOCH 68
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [68][  0/391]	Time  0.184 ( 0.184)	Data  0.156 ( 0.156)	Loss 1.8030e-01 (1.8030e-01)	Acc@1  94.53 ( 94.53)	Acc@5  99.22 ( 99.22)
Epoch: [68][ 10/391]	Time  0.022 ( 0.039)	Data  0.002 ( 0.016)	Loss 2.3499e-01 (2.1866e-01)	Acc@1  92.19 ( 93.04)	Acc@5 100.00 ( 99.64)
Epoch: [68][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.7151e-01 (2.2580e-01)	Acc@1  95.31 ( 92.63)	Acc@5  99.22 ( 99.52)
Epoch: [68][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.5464e-01 (2.3299e-01)	Acc@1  93.75 ( 92.34)	Acc@5  99.22 ( 99.57)
Epoch: [68][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.1155e-01 (2.3058e-01)	Acc@1  92.19 ( 92.59)	Acc@5 100.00 ( 99.49)
Epoch: [68][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.2253e-01 (2.2461e-01)	Acc@1  91.41 ( 92.89)	Acc@5 100.00 ( 99.54)
Epoch: [68][ 60/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.5269e-01 (2.2587e-01)	Acc@1  92.19 ( 92.78)	Acc@5  99.22 ( 99.54)
Epoch: [68][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.4790e-01 (2.2543e-01)	Acc@1  89.06 ( 92.84)	Acc@5  97.66 ( 99.48)
Epoch: [68][ 80/391]	Time  0.031 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.1689e-01 (2.2333e-01)	Acc@1  89.84 ( 92.90)	Acc@5  98.44 ( 99.48)
Epoch: [68][ 90/391]	Time  0.038 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.8433e-01 (2.2555e-01)	Acc@1  92.97 ( 92.87)	Acc@5 100.00 ( 99.45)
Epoch: [68][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7261e-01 (2.2480e-01)	Acc@1  92.97 ( 92.89)	Acc@5 100.00 ( 99.45)
Epoch: [68][110/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7859e-01 (2.2399e-01)	Acc@1  93.75 ( 92.91)	Acc@5  99.22 ( 99.46)
Epoch: [68][120/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0394e-01 (2.2053e-01)	Acc@1  96.88 ( 93.02)	Acc@5 100.00 ( 99.46)
Epoch: [68][130/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.6785e-01 (2.1878e-01)	Acc@1  95.31 ( 93.02)	Acc@5  99.22 ( 99.48)
Epoch: [68][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5620e-01 (2.1930e-01)	Acc@1  85.94 ( 93.04)	Acc@5  98.44 ( 99.46)
Epoch: [68][150/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0811e-01 (2.1966e-01)	Acc@1  90.62 ( 92.98)	Acc@5  99.22 ( 99.46)
Epoch: [68][160/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.0300e-01 (2.2155e-01)	Acc@1  93.75 ( 92.93)	Acc@5 100.00 ( 99.47)
Epoch: [68][170/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2188e-01 (2.2295e-01)	Acc@1  86.72 ( 92.90)	Acc@5 100.00 ( 99.47)
Epoch: [68][180/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3440e-01 (2.2195e-01)	Acc@1  95.31 ( 92.93)	Acc@5 100.00 ( 99.46)
Epoch: [68][190/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4585e-01 (2.2238e-01)	Acc@1  92.97 ( 92.90)	Acc@5  99.22 ( 99.48)
Epoch: [68][200/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.6025e-01 (2.2309e-01)	Acc@1  92.97 ( 92.86)	Acc@5  98.44 ( 99.47)
Epoch: [68][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4050e-01 (2.2229e-01)	Acc@1  96.09 ( 92.89)	Acc@5 100.00 ( 99.48)
Epoch: [68][220/391]	Time  0.037 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.1499e-01 (2.2206e-01)	Acc@1  96.88 ( 92.89)	Acc@5 100.00 ( 99.48)
Epoch: [68][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8198e-01 (2.2249e-01)	Acc@1  91.41 ( 92.87)	Acc@5  99.22 ( 99.49)
Epoch: [68][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5125e-01 (2.2183e-01)	Acc@1  96.09 ( 92.90)	Acc@5 100.00 ( 99.50)
Epoch: [68][250/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9348e-01 (2.2170e-01)	Acc@1  95.31 ( 92.92)	Acc@5 100.00 ( 99.50)
Epoch: [68][260/391]	Time  0.040 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.7637e-01 (2.2193e-01)	Acc@1  89.84 ( 92.87)	Acc@5  99.22 ( 99.51)
Epoch: [68][270/391]	Time  0.031 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.7334e-01 (2.2270e-01)	Acc@1  92.97 ( 92.86)	Acc@5 100.00 ( 99.50)
Epoch: [68][280/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8933e-01 (2.2271e-01)	Acc@1  93.75 ( 92.83)	Acc@5  99.22 ( 99.50)
Epoch: [68][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8198e-01 (2.2205e-01)	Acc@1  90.62 ( 92.86)	Acc@5  99.22 ( 99.50)
Epoch: [68][300/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4426e-01 (2.2200e-01)	Acc@1  94.53 ( 92.88)	Acc@5  98.44 ( 99.50)
Epoch: [68][310/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.6245e-01 (2.2261e-01)	Acc@1  91.41 ( 92.84)	Acc@5  99.22 ( 99.51)
Epoch: [68][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6333e-01 (2.2174e-01)	Acc@1  94.53 ( 92.87)	Acc@5  99.22 ( 99.52)
Epoch: [68][330/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7261e-01 (2.2139e-01)	Acc@1  96.09 ( 92.89)	Acc@5  99.22 ( 99.51)
Epoch: [68][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2622e-01 (2.2148e-01)	Acc@1  96.88 ( 92.91)	Acc@5 100.00 ( 99.51)
Epoch: [68][350/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3938e-01 (2.2122e-01)	Acc@1  91.41 ( 92.93)	Acc@5 100.00 ( 99.51)
Epoch: [68][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5830e-01 (2.2154e-01)	Acc@1  89.84 ( 92.91)	Acc@5  99.22 ( 99.51)
Epoch: [68][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4160e-01 (2.2102e-01)	Acc@1  95.31 ( 92.93)	Acc@5  99.22 ( 99.51)
Epoch: [68][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1484e-01 (2.2159e-01)	Acc@1  92.97 ( 92.92)	Acc@5  99.22 ( 99.51)
Epoch: [68][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5723e-01 (2.2152e-01)	Acc@1  95.00 ( 92.93)	Acc@5 100.00 ( 99.51)
## e[68] optimizer.zero_grad (sum) time: 0.10529637336730957
## e[68]       loss.backward (sum) time: 2.2725675106048584
## e[68]      optimizer.step (sum) time: 0.9191899299621582
## epoch[68] training(only) time: 10.163162469863892
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.2861e+00 (1.2861e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.4023e+00 (1.5536e+00)	Acc@1  65.00 ( 67.64)	Acc@5  92.00 ( 88.45)
Test: [ 20/100]	Time  0.011 ( 0.020)	Loss 1.6816e+00 (1.5598e+00)	Acc@1  66.00 ( 66.95)	Acc@5  91.00 ( 88.24)
Test: [ 30/100]	Time  0.014 ( 0.018)	Loss 1.4580e+00 (1.5554e+00)	Acc@1  66.00 ( 66.77)	Acc@5  88.00 ( 88.19)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6406e+00 (1.5495e+00)	Acc@1  66.00 ( 66.98)	Acc@5  90.00 ( 88.39)
Test: [ 50/100]	Time  0.018 ( 0.017)	Loss 1.6416e+00 (1.5786e+00)	Acc@1  67.00 ( 66.80)	Acc@5  90.00 ( 88.16)
Test: [ 60/100]	Time  0.020 ( 0.016)	Loss 1.7373e+00 (1.5553e+00)	Acc@1  61.00 ( 66.92)	Acc@5  92.00 ( 88.64)
Test: [ 70/100]	Time  0.019 ( 0.016)	Loss 1.5625e+00 (1.5631e+00)	Acc@1  66.00 ( 67.01)	Acc@5  91.00 ( 88.69)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.5176e+00 (1.5662e+00)	Acc@1  66.00 ( 66.90)	Acc@5  88.00 ( 88.65)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.9033e+00 (1.5531e+00)	Acc@1  59.00 ( 67.19)	Acc@5  87.00 ( 88.77)
 * Acc@1 67.220 Acc@5 88.750
### epoch[68] execution time: 11.756381273269653
EPOCH 69
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [69][  0/391]	Time  0.181 ( 0.181)	Data  0.149 ( 0.149)	Loss 2.8101e-01 (2.8101e-01)	Acc@1  88.28 ( 88.28)	Acc@5 100.00 (100.00)
Epoch: [69][ 10/391]	Time  0.022 ( 0.039)	Data  0.001 ( 0.016)	Loss 2.2412e-01 (2.3519e-01)	Acc@1  93.75 ( 92.19)	Acc@5  99.22 ( 99.22)
Epoch: [69][ 20/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.009)	Loss 2.2034e-01 (2.3009e-01)	Acc@1  91.41 ( 92.41)	Acc@5 100.00 ( 99.40)
Epoch: [69][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.1008e-01 (2.2692e-01)	Acc@1  94.53 ( 92.64)	Acc@5 100.00 ( 99.50)
Epoch: [69][ 40/391]	Time  0.040 ( 0.029)	Data  0.002 ( 0.006)	Loss 2.7051e-01 (2.2231e-01)	Acc@1  92.19 ( 92.84)	Acc@5  98.44 ( 99.47)
Epoch: [69][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.3047e-01 (2.2235e-01)	Acc@1  93.75 ( 92.91)	Acc@5  99.22 ( 99.45)
Epoch: [69][ 60/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6931e-01 (2.2064e-01)	Acc@1  94.53 ( 93.03)	Acc@5  99.22 ( 99.44)
Epoch: [69][ 70/391]	Time  0.040 ( 0.027)	Data  0.000 ( 0.004)	Loss 3.0835e-01 (2.2045e-01)	Acc@1  91.41 ( 93.09)	Acc@5 100.00 ( 99.41)
Epoch: [69][ 80/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4316e-01 (2.1561e-01)	Acc@1  92.97 ( 93.28)	Acc@5  99.22 ( 99.45)
Epoch: [69][ 90/391]	Time  0.027 ( 0.027)	Data  0.003 ( 0.004)	Loss 2.5708e-01 (2.1702e-01)	Acc@1  92.19 ( 93.23)	Acc@5 100.00 ( 99.45)
Epoch: [69][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9980e-01 (2.1621e-01)	Acc@1  90.62 ( 93.30)	Acc@5  99.22 ( 99.47)
Epoch: [69][110/391]	Time  0.026 ( 0.027)	Data  0.005 ( 0.003)	Loss 2.9321e-01 (2.1371e-01)	Acc@1  89.84 ( 93.33)	Acc@5 100.00 ( 99.49)
Epoch: [69][120/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.0396e-01 (2.1477e-01)	Acc@1  90.62 ( 93.27)	Acc@5  99.22 ( 99.50)
Epoch: [69][130/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.9907e-01 (2.1636e-01)	Acc@1  92.19 ( 93.20)	Acc@5  96.88 ( 99.46)
Epoch: [69][140/391]	Time  0.037 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.6235e-01 (2.1670e-01)	Acc@1  94.53 ( 93.14)	Acc@5 100.00 ( 99.45)
Epoch: [69][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9019e-01 (2.1761e-01)	Acc@1  92.97 ( 93.10)	Acc@5 100.00 ( 99.47)
Epoch: [69][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0691e-01 (2.1684e-01)	Acc@1  92.19 ( 93.16)	Acc@5 100.00 ( 99.47)
Epoch: [69][170/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.6880e-01 (2.1738e-01)	Acc@1  93.75 ( 93.12)	Acc@5  98.44 ( 99.47)
Epoch: [69][180/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6870e-01 (2.1719e-01)	Acc@1  93.75 ( 93.10)	Acc@5 100.00 ( 99.48)
Epoch: [69][190/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.2910e-01 (2.1787e-01)	Acc@1  91.41 ( 93.10)	Acc@5  99.22 ( 99.49)
Epoch: [69][200/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4329e-01 (2.1863e-01)	Acc@1  94.53 ( 93.10)	Acc@5  99.22 ( 99.49)
Epoch: [69][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4280e-01 (2.1883e-01)	Acc@1  92.97 ( 93.11)	Acc@5  99.22 ( 99.50)
Epoch: [69][220/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.003)	Loss 3.0640e-01 (2.1999e-01)	Acc@1  91.41 ( 93.12)	Acc@5  99.22 ( 99.49)
Epoch: [69][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2571e-01 (2.2131e-01)	Acc@1  92.97 ( 93.08)	Acc@5  99.22 ( 99.49)
Epoch: [69][240/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4622e-01 (2.2150e-01)	Acc@1  92.97 ( 93.10)	Acc@5 100.00 ( 99.47)
Epoch: [69][250/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8750e-01 (2.2077e-01)	Acc@1  93.75 ( 93.13)	Acc@5  99.22 ( 99.48)
Epoch: [69][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0703e-01 (2.1937e-01)	Acc@1  94.53 ( 93.18)	Acc@5 100.00 ( 99.49)
Epoch: [69][270/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4453e-01 (2.1920e-01)	Acc@1  96.88 ( 93.15)	Acc@5  99.22 ( 99.49)
Epoch: [69][280/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5161e-01 (2.1977e-01)	Acc@1  94.53 ( 93.17)	Acc@5 100.00 ( 99.48)
Epoch: [69][290/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.0679e-01 (2.2049e-01)	Acc@1  93.75 ( 93.15)	Acc@5  99.22 ( 99.48)
Epoch: [69][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1786e-01 (2.1989e-01)	Acc@1  96.88 ( 93.18)	Acc@5  99.22 ( 99.47)
Epoch: [69][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5210e-01 (2.2071e-01)	Acc@1  95.31 ( 93.15)	Acc@5  99.22 ( 99.47)
Epoch: [69][320/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2827e-01 (2.2058e-01)	Acc@1  92.97 ( 93.15)	Acc@5  99.22 ( 99.47)
Epoch: [69][330/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1387e-01 (2.2020e-01)	Acc@1  92.97 ( 93.17)	Acc@5 100.00 ( 99.48)
Epoch: [69][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7065e-01 (2.2091e-01)	Acc@1  93.75 ( 93.14)	Acc@5 100.00 ( 99.47)
Epoch: [69][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0044e-01 (2.2062e-01)	Acc@1  92.19 ( 93.17)	Acc@5 100.00 ( 99.48)
Epoch: [69][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0786e-01 (2.2046e-01)	Acc@1  89.06 ( 93.17)	Acc@5  99.22 ( 99.48)
Epoch: [69][370/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.7041e-01 (2.2207e-01)	Acc@1  94.53 ( 93.11)	Acc@5  99.22 ( 99.47)
Epoch: [69][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2952e-01 (2.2184e-01)	Acc@1  96.88 ( 93.10)	Acc@5 100.00 ( 99.47)
Epoch: [69][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7480e-01 (2.2136e-01)	Acc@1  96.25 ( 93.12)	Acc@5  98.75 ( 99.48)
## e[69] optimizer.zero_grad (sum) time: 0.10506010055541992
## e[69]       loss.backward (sum) time: 2.268004894256592
## e[69]      optimizer.step (sum) time: 0.9242074489593506
## epoch[69] training(only) time: 10.132959365844727
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.2734e+00 (1.2734e+00)	Acc@1  73.00 ( 73.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.4473e+00 (1.5651e+00)	Acc@1  66.00 ( 67.91)	Acc@5  93.00 ( 88.45)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.6719e+00 (1.5624e+00)	Acc@1  66.00 ( 67.29)	Acc@5  90.00 ( 88.10)
Test: [ 30/100]	Time  0.030 ( 0.018)	Loss 1.4893e+00 (1.5675e+00)	Acc@1  64.00 ( 66.84)	Acc@5  88.00 ( 88.10)
Test: [ 40/100]	Time  0.013 ( 0.017)	Loss 1.6641e+00 (1.5641e+00)	Acc@1  67.00 ( 67.02)	Acc@5  90.00 ( 88.37)
Test: [ 50/100]	Time  0.009 ( 0.016)	Loss 1.6357e+00 (1.5933e+00)	Acc@1  67.00 ( 66.78)	Acc@5  89.00 ( 88.12)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.7334e+00 (1.5684e+00)	Acc@1  62.00 ( 67.05)	Acc@5  92.00 ( 88.38)
Test: [ 70/100]	Time  0.016 ( 0.015)	Loss 1.5098e+00 (1.5739e+00)	Acc@1  67.00 ( 67.04)	Acc@5  89.00 ( 88.41)
Test: [ 80/100]	Time  0.018 ( 0.015)	Loss 1.4717e+00 (1.5757e+00)	Acc@1  66.00 ( 66.98)	Acc@5  89.00 ( 88.42)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.8770e+00 (1.5626e+00)	Acc@1  60.00 ( 67.20)	Acc@5  89.00 ( 88.64)
 * Acc@1 67.250 Acc@5 88.600
### epoch[69] execution time: 11.69599175453186
EPOCH 70
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [70][  0/391]	Time  0.167 ( 0.167)	Data  0.138 ( 0.138)	Loss 2.4329e-01 (2.4329e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [70][ 10/391]	Time  0.032 ( 0.039)	Data  0.001 ( 0.014)	Loss 1.7358e-01 (2.2138e-01)	Acc@1  94.53 ( 92.90)	Acc@5 100.00 ( 99.36)
Epoch: [70][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.008)	Loss 1.6968e-01 (2.1850e-01)	Acc@1  96.09 ( 92.97)	Acc@5 100.00 ( 99.52)
Epoch: [70][ 30/391]	Time  0.037 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.2646e-01 (2.1606e-01)	Acc@1  96.88 ( 93.15)	Acc@5 100.00 ( 99.50)
Epoch: [70][ 40/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.1692e-01 (2.0945e-01)	Acc@1  95.31 ( 93.45)	Acc@5  98.44 ( 99.47)
Epoch: [70][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.4158e-01 (2.0917e-01)	Acc@1  94.53 ( 93.55)	Acc@5  99.22 ( 99.45)
Epoch: [70][ 60/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.5320e-01 (2.0827e-01)	Acc@1  94.53 ( 93.52)	Acc@5 100.00 ( 99.46)
Epoch: [70][ 70/391]	Time  0.036 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3220e-01 (2.0505e-01)	Acc@1  95.31 ( 93.56)	Acc@5 100.00 ( 99.52)
Epoch: [70][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0032e-01 (2.0706e-01)	Acc@1  93.75 ( 93.49)	Acc@5  98.44 ( 99.52)
Epoch: [70][ 90/391]	Time  0.023 ( 0.027)	Data  0.003 ( 0.003)	Loss 2.6392e-01 (2.1044e-01)	Acc@1  90.62 ( 93.35)	Acc@5 100.00 ( 99.52)
Epoch: [70][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3425e-01 (2.1068e-01)	Acc@1  92.97 ( 93.39)	Acc@5  97.66 ( 99.50)
Epoch: [70][110/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6221e-01 (2.1236e-01)	Acc@1  90.62 ( 93.29)	Acc@5  99.22 ( 99.51)
Epoch: [70][120/391]	Time  0.031 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.4475e-01 (2.1111e-01)	Acc@1  92.19 ( 93.36)	Acc@5 100.00 ( 99.53)
Epoch: [70][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8188e-01 (2.1359e-01)	Acc@1  92.97 ( 93.27)	Acc@5 100.00 ( 99.51)
Epoch: [70][140/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1582e-01 (2.1420e-01)	Acc@1  94.53 ( 93.25)	Acc@5  99.22 ( 99.51)
Epoch: [70][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1680e-01 (2.1500e-01)	Acc@1  92.97 ( 93.23)	Acc@5  99.22 ( 99.51)
Epoch: [70][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0239e-01 (2.1544e-01)	Acc@1  94.53 ( 93.26)	Acc@5 100.00 ( 99.52)
Epoch: [70][170/391]	Time  0.036 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.1045e-01 (2.1544e-01)	Acc@1  92.97 ( 93.25)	Acc@5 100.00 ( 99.53)
Epoch: [70][180/391]	Time  0.025 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.9653e-01 (2.1555e-01)	Acc@1  93.75 ( 93.24)	Acc@5 100.00 ( 99.54)
Epoch: [70][190/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2546e-01 (2.1595e-01)	Acc@1  95.31 ( 93.25)	Acc@5  98.44 ( 99.54)
Epoch: [70][200/391]	Time  0.029 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1763e-01 (2.1694e-01)	Acc@1  89.06 ( 93.21)	Acc@5  99.22 ( 99.52)
Epoch: [70][210/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0032e-01 (2.1584e-01)	Acc@1  93.75 ( 93.23)	Acc@5  99.22 ( 99.52)
Epoch: [70][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0776e-01 (2.1479e-01)	Acc@1  92.19 ( 93.26)	Acc@5 100.00 ( 99.53)
Epoch: [70][230/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7100e-01 (2.1639e-01)	Acc@1  91.41 ( 93.20)	Acc@5  98.44 ( 99.51)
Epoch: [70][240/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9006e-01 (2.1571e-01)	Acc@1  93.75 ( 93.20)	Acc@5 100.00 ( 99.51)
Epoch: [70][250/391]	Time  0.029 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.7100e-01 (2.1553e-01)	Acc@1  92.97 ( 93.21)	Acc@5  98.44 ( 99.51)
Epoch: [70][260/391]	Time  0.028 ( 0.026)	Data  0.004 ( 0.002)	Loss 9.1919e-02 (2.1537e-01)	Acc@1  97.66 ( 93.23)	Acc@5 100.00 ( 99.50)
Epoch: [70][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4915e-01 (2.1686e-01)	Acc@1  93.75 ( 93.19)	Acc@5  98.44 ( 99.48)
Epoch: [70][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9092e-01 (2.1683e-01)	Acc@1  94.53 ( 93.17)	Acc@5 100.00 ( 99.49)
Epoch: [70][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2900e-01 (2.1682e-01)	Acc@1  92.97 ( 93.16)	Acc@5  99.22 ( 99.50)
Epoch: [70][300/391]	Time  0.050 ( 0.026)	Data  0.010 ( 0.002)	Loss 1.6016e-01 (2.1641e-01)	Acc@1  94.53 ( 93.17)	Acc@5 100.00 ( 99.51)
Epoch: [70][310/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8687e-01 (2.1730e-01)	Acc@1  90.62 ( 93.14)	Acc@5  98.44 ( 99.50)
Epoch: [70][320/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.5234e-01 (2.1688e-01)	Acc@1  96.09 ( 93.13)	Acc@5 100.00 ( 99.50)
Epoch: [70][330/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5854e-01 (2.1616e-01)	Acc@1  92.19 ( 93.14)	Acc@5 100.00 ( 99.52)
Epoch: [70][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9971e-01 (2.1570e-01)	Acc@1  92.97 ( 93.13)	Acc@5 100.00 ( 99.52)
Epoch: [70][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9275e-01 (2.1638e-01)	Acc@1  91.41 ( 93.10)	Acc@5 100.00 ( 99.51)
Epoch: [70][360/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5747e-01 (2.1575e-01)	Acc@1  95.31 ( 93.13)	Acc@5 100.00 ( 99.52)
Epoch: [70][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5869e-01 (2.1524e-01)	Acc@1  92.97 ( 93.16)	Acc@5  99.22 ( 99.51)
Epoch: [70][380/391]	Time  0.032 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.4585e-01 (2.1533e-01)	Acc@1  93.75 ( 93.16)	Acc@5 100.00 ( 99.52)
Epoch: [70][390/391]	Time  0.018 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.5596e-01 (2.1493e-01)	Acc@1  92.50 ( 93.19)	Acc@5 100.00 ( 99.52)
## e[70] optimizer.zero_grad (sum) time: 0.10585999488830566
## e[70]       loss.backward (sum) time: 2.269411563873291
## e[70]      optimizer.step (sum) time: 0.9305191040039062
## epoch[70] training(only) time: 10.116070032119751
# Switched to evaluate mode...
Test: [  0/100]	Time  0.137 ( 0.137)	Loss 1.2676e+00 (1.2676e+00)	Acc@1  73.00 ( 73.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.009 ( 0.025)	Loss 1.4551e+00 (1.5786e+00)	Acc@1  65.00 ( 67.45)	Acc@5  90.00 ( 87.91)
Test: [ 20/100]	Time  0.016 ( 0.021)	Loss 1.6357e+00 (1.5782e+00)	Acc@1  67.00 ( 66.95)	Acc@5  90.00 ( 87.81)
Test: [ 30/100]	Time  0.017 ( 0.018)	Loss 1.5039e+00 (1.5756e+00)	Acc@1  66.00 ( 66.71)	Acc@5  88.00 ( 88.00)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6650e+00 (1.5670e+00)	Acc@1  67.00 ( 67.00)	Acc@5  90.00 ( 88.22)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.5703e+00 (1.5919e+00)	Acc@1  67.00 ( 66.84)	Acc@5  89.00 ( 88.08)
Test: [ 60/100]	Time  0.018 ( 0.016)	Loss 1.7344e+00 (1.5663e+00)	Acc@1  61.00 ( 67.02)	Acc@5  91.00 ( 88.38)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.5293e+00 (1.5733e+00)	Acc@1  68.00 ( 67.13)	Acc@5  90.00 ( 88.42)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.4531e+00 (1.5741e+00)	Acc@1  68.00 ( 67.06)	Acc@5  89.00 ( 88.43)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 1.8506e+00 (1.5603e+00)	Acc@1  62.00 ( 67.27)	Acc@5  87.00 ( 88.62)
 * Acc@1 67.340 Acc@5 88.600
### epoch[70] execution time: 11.713929891586304
EPOCH 71
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [71][  0/391]	Time  0.171 ( 0.171)	Data  0.138 ( 0.138)	Loss 2.1021e-01 (2.1021e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [71][ 10/391]	Time  0.028 ( 0.038)	Data  0.001 ( 0.014)	Loss 1.6125e-01 (2.0004e-01)	Acc@1  94.53 ( 94.18)	Acc@5 100.00 ( 99.43)
Epoch: [71][ 20/391]	Time  0.020 ( 0.031)	Data  0.002 ( 0.008)	Loss 2.6099e-01 (2.1238e-01)	Acc@1  91.41 ( 93.49)	Acc@5  98.44 ( 99.29)
Epoch: [71][ 30/391]	Time  0.027 ( 0.030)	Data  0.001 ( 0.007)	Loss 3.0859e-01 (2.2087e-01)	Acc@1  89.84 ( 93.35)	Acc@5  98.44 ( 99.29)
Epoch: [71][ 40/391]	Time  0.027 ( 0.029)	Data  0.003 ( 0.006)	Loss 2.0728e-01 (2.1575e-01)	Acc@1  94.53 ( 93.46)	Acc@5 100.00 ( 99.37)
Epoch: [71][ 50/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.7957e-01 (2.1808e-01)	Acc@1  92.97 ( 93.35)	Acc@5 100.00 ( 99.40)
Epoch: [71][ 60/391]	Time  0.021 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.5454e-01 (2.1913e-01)	Acc@1  95.31 ( 93.22)	Acc@5 100.00 ( 99.42)
Epoch: [71][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.2278e-01 (2.2199e-01)	Acc@1  90.62 ( 92.94)	Acc@5  99.22 ( 99.39)
Epoch: [71][ 80/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6235e-01 (2.1885e-01)	Acc@1  93.75 ( 93.07)	Acc@5 100.00 ( 99.43)
Epoch: [71][ 90/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6125e-01 (2.1920e-01)	Acc@1  94.53 ( 93.01)	Acc@5 100.00 ( 99.45)
Epoch: [71][100/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3169e-01 (2.1989e-01)	Acc@1  92.19 ( 92.98)	Acc@5  99.22 ( 99.45)
Epoch: [71][110/391]	Time  0.043 ( 0.027)	Data  0.002 ( 0.003)	Loss 1.8872e-01 (2.1958e-01)	Acc@1  95.31 ( 93.00)	Acc@5 100.00 ( 99.45)
Epoch: [71][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7896e-01 (2.1864e-01)	Acc@1  94.53 ( 93.07)	Acc@5 100.00 ( 99.46)
Epoch: [71][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7920e-01 (2.1957e-01)	Acc@1  93.75 ( 92.98)	Acc@5 100.00 ( 99.48)
Epoch: [71][140/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1348e-01 (2.1911e-01)	Acc@1  90.62 ( 92.99)	Acc@5  98.44 ( 99.48)
Epoch: [71][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9153e-01 (2.1977e-01)	Acc@1  94.53 ( 92.97)	Acc@5  99.22 ( 99.48)
Epoch: [71][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5098e-01 (2.1902e-01)	Acc@1  92.97 ( 92.98)	Acc@5  99.22 ( 99.48)
Epoch: [71][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7017e-01 (2.1796e-01)	Acc@1  94.53 ( 93.05)	Acc@5  99.22 ( 99.49)
Epoch: [71][180/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5942e-01 (2.1762e-01)	Acc@1  93.75 ( 93.06)	Acc@5  99.22 ( 99.48)
Epoch: [71][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.6025e-01 (2.1834e-01)	Acc@1  93.75 ( 93.02)	Acc@5  97.66 ( 99.48)
Epoch: [71][200/391]	Time  0.034 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5439e-01 (2.1793e-01)	Acc@1  92.19 ( 93.03)	Acc@5 100.00 ( 99.49)
Epoch: [71][210/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9458e-01 (2.1653e-01)	Acc@1  92.19 ( 93.07)	Acc@5  99.22 ( 99.49)
Epoch: [71][220/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.6221e-01 (2.1692e-01)	Acc@1  92.19 ( 93.05)	Acc@5  99.22 ( 99.49)
Epoch: [71][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5342e-01 (2.1698e-01)	Acc@1  90.62 ( 93.02)	Acc@5 100.00 ( 99.50)
Epoch: [71][240/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6199e-01 (2.1842e-01)	Acc@1  94.53 ( 93.00)	Acc@5 100.00 ( 99.49)
Epoch: [71][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5317e-01 (2.1783e-01)	Acc@1  92.19 ( 93.00)	Acc@5  98.44 ( 99.50)
Epoch: [71][260/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.4390e-01 (2.1836e-01)	Acc@1  91.41 ( 93.00)	Acc@5  99.22 ( 99.49)
Epoch: [71][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4829e-01 (2.1912e-01)	Acc@1  92.19 ( 93.00)	Acc@5  99.22 ( 99.48)
Epoch: [71][280/391]	Time  0.042 ( 0.026)	Data  0.008 ( 0.003)	Loss 2.9883e-01 (2.2042e-01)	Acc@1  91.41 ( 92.98)	Acc@5 100.00 ( 99.48)
Epoch: [71][290/391]	Time  0.031 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.0581e-01 (2.2035e-01)	Acc@1  92.19 ( 93.01)	Acc@5 100.00 ( 99.48)
Epoch: [71][300/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.8323e-01 (2.2112e-01)	Acc@1  95.31 ( 92.99)	Acc@5 100.00 ( 99.49)
Epoch: [71][310/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2644e-01 (2.2062e-01)	Acc@1  92.97 ( 93.00)	Acc@5 100.00 ( 99.48)
Epoch: [71][320/391]	Time  0.031 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.8079e-01 (2.2008e-01)	Acc@1  93.75 ( 93.00)	Acc@5 100.00 ( 99.48)
Epoch: [71][330/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.002)	Loss 2.3901e-01 (2.1964e-01)	Acc@1  92.19 ( 93.02)	Acc@5 100.00 ( 99.49)
Epoch: [71][340/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7124e-01 (2.1941e-01)	Acc@1  92.97 ( 93.04)	Acc@5  98.44 ( 99.49)
Epoch: [71][350/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7078e-01 (2.2058e-01)	Acc@1  93.75 ( 92.97)	Acc@5 100.00 ( 99.48)
Epoch: [71][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9189e-01 (2.1951e-01)	Acc@1  94.53 ( 93.03)	Acc@5  98.44 ( 99.48)
Epoch: [71][370/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.002)	Loss 2.1448e-01 (2.1835e-01)	Acc@1  92.97 ( 93.05)	Acc@5  99.22 ( 99.49)
Epoch: [71][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8835e-01 (2.1835e-01)	Acc@1  93.75 ( 93.06)	Acc@5 100.00 ( 99.48)
Epoch: [71][390/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.1201e-01 (2.1804e-01)	Acc@1  91.25 ( 93.07)	Acc@5 100.00 ( 99.49)
## e[71] optimizer.zero_grad (sum) time: 0.10533022880554199
## e[71]       loss.backward (sum) time: 2.2985360622406006
## e[71]      optimizer.step (sum) time: 0.9258184432983398
## epoch[71] training(only) time: 10.109963178634644
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.2998e+00 (1.2998e+00)	Acc@1  73.00 ( 73.00)	Acc@5  86.00 ( 86.00)
Test: [ 10/100]	Time  0.016 ( 0.026)	Loss 1.4854e+00 (1.5802e+00)	Acc@1  65.00 ( 67.73)	Acc@5  92.00 ( 88.18)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.6533e+00 (1.5743e+00)	Acc@1  69.00 ( 67.38)	Acc@5  91.00 ( 87.90)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.5205e+00 (1.5782e+00)	Acc@1  65.00 ( 66.97)	Acc@5  88.00 ( 87.90)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6660e+00 (1.5749e+00)	Acc@1  67.00 ( 67.10)	Acc@5  91.00 ( 88.24)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6533e+00 (1.6011e+00)	Acc@1  67.00 ( 66.90)	Acc@5  90.00 ( 88.10)
Test: [ 60/100]	Time  0.012 ( 0.016)	Loss 1.7715e+00 (1.5774e+00)	Acc@1  62.00 ( 67.10)	Acc@5  91.00 ( 88.48)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.5605e+00 (1.5825e+00)	Acc@1  67.00 ( 67.20)	Acc@5  90.00 ( 88.54)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.4736e+00 (1.5834e+00)	Acc@1  65.00 ( 67.17)	Acc@5  89.00 ( 88.52)
Test: [ 90/100]	Time  0.019 ( 0.015)	Loss 1.9219e+00 (1.5700e+00)	Acc@1  59.00 ( 67.38)	Acc@5  88.00 ( 88.74)
 * Acc@1 67.390 Acc@5 88.740
### epoch[71] execution time: 11.712720394134521
EPOCH 72
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [72][  0/391]	Time  0.171 ( 0.171)	Data  0.143 ( 0.143)	Loss 1.8652e-01 (1.8652e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [72][ 10/391]	Time  0.022 ( 0.038)	Data  0.001 ( 0.015)	Loss 3.1592e-01 (2.2417e-01)	Acc@1  89.84 ( 92.97)	Acc@5 100.00 ( 99.50)
Epoch: [72][ 20/391]	Time  0.029 ( 0.032)	Data  0.001 ( 0.009)	Loss 2.0496e-01 (2.1953e-01)	Acc@1  92.97 ( 93.15)	Acc@5  99.22 ( 99.40)
Epoch: [72][ 30/391]	Time  0.028 ( 0.030)	Data  0.003 ( 0.007)	Loss 1.8579e-01 (2.1597e-01)	Acc@1  92.19 ( 93.32)	Acc@5 100.00 ( 99.45)
Epoch: [72][ 40/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.4561e-01 (2.1085e-01)	Acc@1  91.41 ( 93.50)	Acc@5 100.00 ( 99.49)
Epoch: [72][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.5854e-01 (2.1508e-01)	Acc@1  91.41 ( 93.32)	Acc@5  98.44 ( 99.49)
Epoch: [72][ 60/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 3.4033e-01 (2.2216e-01)	Acc@1  86.72 ( 93.16)	Acc@5 100.00 ( 99.41)
Epoch: [72][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.6025e-01 (2.2355e-01)	Acc@1  92.19 ( 93.01)	Acc@5 100.00 ( 99.42)
Epoch: [72][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5757e-01 (2.2302e-01)	Acc@1  90.62 ( 93.09)	Acc@5 100.00 ( 99.42)
Epoch: [72][ 90/391]	Time  0.038 ( 0.027)	Data  0.005 ( 0.004)	Loss 2.5024e-01 (2.2104e-01)	Acc@1  92.19 ( 93.19)	Acc@5 100.00 ( 99.42)
Epoch: [72][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 3.1299e-01 (2.1860e-01)	Acc@1  91.41 ( 93.34)	Acc@5  97.66 ( 99.43)
Epoch: [72][110/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0984e-01 (2.1643e-01)	Acc@1  94.53 ( 93.46)	Acc@5 100.00 ( 99.45)
Epoch: [72][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5488e-01 (2.1739e-01)	Acc@1  93.75 ( 93.44)	Acc@5 100.00 ( 99.44)
Epoch: [72][130/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1826e-01 (2.1616e-01)	Acc@1  92.97 ( 93.52)	Acc@5  99.22 ( 99.43)
Epoch: [72][140/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.4807e-01 (2.1397e-01)	Acc@1  96.88 ( 93.54)	Acc@5 100.00 ( 99.45)
Epoch: [72][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9226e-01 (2.1095e-01)	Acc@1  93.75 ( 93.63)	Acc@5 100.00 ( 99.47)
Epoch: [72][160/391]	Time  0.019 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1667e-01 (2.1112e-01)	Acc@1  92.97 ( 93.62)	Acc@5 100.00 ( 99.48)
Epoch: [72][170/391]	Time  0.035 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.6855e-01 (2.1081e-01)	Acc@1  91.41 ( 93.58)	Acc@5  99.22 ( 99.49)
Epoch: [72][180/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7954e-01 (2.1199e-01)	Acc@1  91.41 ( 93.52)	Acc@5 100.00 ( 99.50)
Epoch: [72][190/391]	Time  0.040 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.2253e-01 (2.1400e-01)	Acc@1  94.53 ( 93.45)	Acc@5 100.00 ( 99.49)
Epoch: [72][200/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.4426e-01 (2.1501e-01)	Acc@1  91.41 ( 93.42)	Acc@5 100.00 ( 99.48)
Epoch: [72][210/391]	Time  0.027 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5757e-01 (2.1379e-01)	Acc@1  91.41 ( 93.45)	Acc@5 100.00 ( 99.49)
Epoch: [72][220/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1787e-01 (2.1476e-01)	Acc@1  90.62 ( 93.42)	Acc@5  99.22 ( 99.47)
Epoch: [72][230/391]	Time  0.028 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.7979e-01 (2.1678e-01)	Acc@1  92.97 ( 93.38)	Acc@5 100.00 ( 99.46)
Epoch: [72][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5710e-01 (2.1751e-01)	Acc@1  95.31 ( 93.32)	Acc@5 100.00 ( 99.46)
Epoch: [72][250/391]	Time  0.036 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.3669e-01 (2.1733e-01)	Acc@1  93.75 ( 93.34)	Acc@5 100.00 ( 99.47)
Epoch: [72][260/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5125e-01 (2.1783e-01)	Acc@1  95.31 ( 93.32)	Acc@5 100.00 ( 99.47)
Epoch: [72][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1191e-01 (2.1772e-01)	Acc@1  90.62 ( 93.29)	Acc@5 100.00 ( 99.47)
Epoch: [72][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5684e-01 (2.1832e-01)	Acc@1  89.84 ( 93.25)	Acc@5 100.00 ( 99.47)
Epoch: [72][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2534e-01 (2.1858e-01)	Acc@1  92.97 ( 93.25)	Acc@5  98.44 ( 99.46)
Epoch: [72][300/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7295e-01 (2.1812e-01)	Acc@1  92.19 ( 93.26)	Acc@5  99.22 ( 99.46)
Epoch: [72][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4963e-01 (2.1896e-01)	Acc@1  89.06 ( 93.23)	Acc@5  99.22 ( 99.46)
Epoch: [72][320/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.2878e-01 (2.1870e-01)	Acc@1  96.88 ( 93.25)	Acc@5 100.00 ( 99.46)
Epoch: [72][330/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7246e-01 (2.1947e-01)	Acc@1  89.06 ( 93.20)	Acc@5 100.00 ( 99.46)
Epoch: [72][340/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4465e-01 (2.1870e-01)	Acc@1  95.31 ( 93.23)	Acc@5 100.00 ( 99.47)
Epoch: [72][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5259e-01 (2.1891e-01)	Acc@1  93.75 ( 93.22)	Acc@5 100.00 ( 99.47)
Epoch: [72][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 9.2529e-02 (2.1833e-01)	Acc@1  96.88 ( 93.24)	Acc@5 100.00 ( 99.47)
Epoch: [72][370/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3145e-01 (2.1767e-01)	Acc@1  92.97 ( 93.25)	Acc@5  99.22 ( 99.47)
Epoch: [72][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8689e-01 (2.1821e-01)	Acc@1  92.97 ( 93.24)	Acc@5 100.00 ( 99.47)
Epoch: [72][390/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3303e-01 (2.1699e-01)	Acc@1  93.75 ( 93.27)	Acc@5 100.00 ( 99.48)
## e[72] optimizer.zero_grad (sum) time: 0.1051950454711914
## e[72]       loss.backward (sum) time: 2.318193197250366
## e[72]      optimizer.step (sum) time: 0.9272675514221191
## epoch[72] training(only) time: 10.093634843826294
# Switched to evaluate mode...
Test: [  0/100]	Time  0.157 ( 0.157)	Loss 1.3135e+00 (1.3135e+00)	Acc@1  74.00 ( 74.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.009 ( 0.026)	Loss 1.4238e+00 (1.5696e+00)	Acc@1  66.00 ( 68.00)	Acc@5  91.00 ( 88.45)
Test: [ 20/100]	Time  0.010 ( 0.021)	Loss 1.6582e+00 (1.5722e+00)	Acc@1  66.00 ( 67.33)	Acc@5  91.00 ( 88.24)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.5010e+00 (1.5713e+00)	Acc@1  64.00 ( 67.13)	Acc@5  88.00 ( 88.16)
Test: [ 40/100]	Time  0.016 ( 0.018)	Loss 1.6641e+00 (1.5662e+00)	Acc@1  67.00 ( 67.27)	Acc@5  90.00 ( 88.37)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 1.6289e+00 (1.5937e+00)	Acc@1  67.00 ( 67.04)	Acc@5  89.00 ( 88.12)
Test: [ 60/100]	Time  0.014 ( 0.017)	Loss 1.7422e+00 (1.5712e+00)	Acc@1  62.00 ( 67.16)	Acc@5  91.00 ( 88.41)
Test: [ 70/100]	Time  0.015 ( 0.016)	Loss 1.5771e+00 (1.5784e+00)	Acc@1  68.00 ( 67.28)	Acc@5  89.00 ( 88.46)
Test: [ 80/100]	Time  0.016 ( 0.016)	Loss 1.4785e+00 (1.5803e+00)	Acc@1  67.00 ( 67.19)	Acc@5  89.00 ( 88.47)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9277e+00 (1.5676e+00)	Acc@1  59.00 ( 67.43)	Acc@5  87.00 ( 88.66)
 * Acc@1 67.490 Acc@5 88.660
### epoch[72] execution time: 11.708640336990356
EPOCH 73
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [73][  0/391]	Time  0.183 ( 0.183)	Data  0.148 ( 0.148)	Loss 2.0667e-01 (2.0667e-01)	Acc@1  93.75 ( 93.75)	Acc@5  99.22 ( 99.22)
Epoch: [73][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 1.6711e-01 (2.1983e-01)	Acc@1  92.97 ( 92.83)	Acc@5 100.00 ( 99.29)
Epoch: [73][ 20/391]	Time  0.036 ( 0.033)	Data  0.001 ( 0.009)	Loss 2.7856e-01 (2.1813e-01)	Acc@1  89.84 ( 92.97)	Acc@5  99.22 ( 99.37)
Epoch: [73][ 30/391]	Time  0.024 ( 0.031)	Data  0.001 ( 0.007)	Loss 1.6821e-01 (2.1836e-01)	Acc@1  94.53 ( 93.30)	Acc@5 100.00 ( 99.47)
Epoch: [73][ 40/391]	Time  0.042 ( 0.029)	Data  0.001 ( 0.005)	Loss 1.9543e-01 (2.1899e-01)	Acc@1  93.75 ( 93.33)	Acc@5 100.00 ( 99.49)
Epoch: [73][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.9565e-01 (2.1504e-01)	Acc@1  85.94 ( 93.43)	Acc@5 100.00 ( 99.51)
Epoch: [73][ 60/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.4514e-01 (2.1557e-01)	Acc@1  94.53 ( 93.30)	Acc@5 100.00 ( 99.55)
Epoch: [73][ 70/391]	Time  0.033 ( 0.027)	Data  0.004 ( 0.004)	Loss 2.1948e-01 (2.1658e-01)	Acc@1  93.75 ( 93.22)	Acc@5 100.00 ( 99.58)
Epoch: [73][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4453e-01 (2.1521e-01)	Acc@1  96.09 ( 93.25)	Acc@5  99.22 ( 99.61)
Epoch: [73][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3889e-01 (2.1665e-01)	Acc@1  92.97 ( 93.12)	Acc@5  99.22 ( 99.56)
Epoch: [73][100/391]	Time  0.023 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.1790e-01 (2.1616e-01)	Acc@1  92.97 ( 93.11)	Acc@5  99.22 ( 99.55)
Epoch: [73][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0410e-01 (2.1420e-01)	Acc@1  93.75 ( 93.19)	Acc@5 100.00 ( 99.56)
Epoch: [73][120/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.1350e-01 (2.1551e-01)	Acc@1  92.97 ( 93.20)	Acc@5 100.00 ( 99.53)
Epoch: [73][130/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6162e-01 (2.1495e-01)	Acc@1  96.09 ( 93.26)	Acc@5 100.00 ( 99.52)
Epoch: [73][140/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.6709e-01 (2.1331e-01)	Acc@1  89.84 ( 93.32)	Acc@5 100.00 ( 99.53)
Epoch: [73][150/391]	Time  0.026 ( 0.026)	Data  0.003 ( 0.003)	Loss 3.1104e-01 (2.1400e-01)	Acc@1  89.84 ( 93.29)	Acc@5  96.88 ( 99.51)
Epoch: [73][160/391]	Time  0.032 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.0654e-01 (2.1321e-01)	Acc@1  94.53 ( 93.31)	Acc@5 100.00 ( 99.52)
Epoch: [73][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2986e-01 (2.1365e-01)	Acc@1  94.53 ( 93.31)	Acc@5  99.22 ( 99.52)
Epoch: [73][180/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5146e-01 (2.1370e-01)	Acc@1  92.97 ( 93.30)	Acc@5  99.22 ( 99.53)
Epoch: [73][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8394e-01 (2.1208e-01)	Acc@1  92.19 ( 93.35)	Acc@5 100.00 ( 99.53)
Epoch: [73][200/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9019e-01 (2.1341e-01)	Acc@1  93.75 ( 93.33)	Acc@5 100.00 ( 99.52)
Epoch: [73][210/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2830e-01 (2.1368e-01)	Acc@1  96.88 ( 93.34)	Acc@5 100.00 ( 99.50)
Epoch: [73][220/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1627e-01 (2.1324e-01)	Acc@1  96.88 ( 93.34)	Acc@5 100.00 ( 99.49)
Epoch: [73][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2046e-01 (2.1178e-01)	Acc@1  92.19 ( 93.37)	Acc@5  99.22 ( 99.50)
Epoch: [73][240/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4463e-01 (2.1251e-01)	Acc@1  89.06 ( 93.35)	Acc@5 100.00 ( 99.50)
Epoch: [73][250/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.1055e-01 (2.1360e-01)	Acc@1  90.62 ( 93.33)	Acc@5  99.22 ( 99.49)
Epoch: [73][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3562e-01 (2.1362e-01)	Acc@1  96.88 ( 93.35)	Acc@5 100.00 ( 99.49)
Epoch: [73][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8689e-01 (2.1341e-01)	Acc@1  94.53 ( 93.35)	Acc@5  99.22 ( 99.49)
Epoch: [73][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4121e-01 (2.1361e-01)	Acc@1  91.41 ( 93.34)	Acc@5  99.22 ( 99.48)
Epoch: [73][290/391]	Time  0.024 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.9849e-01 (2.1385e-01)	Acc@1  93.75 ( 93.32)	Acc@5  98.44 ( 99.48)
Epoch: [73][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8127e-01 (2.1340e-01)	Acc@1  92.97 ( 93.33)	Acc@5  99.22 ( 99.47)
Epoch: [73][310/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3013e-01 (2.1371e-01)	Acc@1  95.31 ( 93.32)	Acc@5 100.00 ( 99.47)
Epoch: [73][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3120e-01 (2.1420e-01)	Acc@1  89.06 ( 93.29)	Acc@5 100.00 ( 99.47)
Epoch: [73][330/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8921e-01 (2.1373e-01)	Acc@1  96.09 ( 93.31)	Acc@5  99.22 ( 99.47)
Epoch: [73][340/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.6602e-01 (2.1364e-01)	Acc@1  93.75 ( 93.31)	Acc@5 100.00 ( 99.48)
Epoch: [73][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8003e-01 (2.1433e-01)	Acc@1  89.06 ( 93.29)	Acc@5  99.22 ( 99.48)
Epoch: [73][360/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3401e-01 (2.1416e-01)	Acc@1  92.19 ( 93.30)	Acc@5 100.00 ( 99.48)
Epoch: [73][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9800e-01 (2.1366e-01)	Acc@1  93.75 ( 93.33)	Acc@5 100.00 ( 99.48)
Epoch: [73][380/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.3831e-01 (2.1380e-01)	Acc@1  95.31 ( 93.34)	Acc@5 100.00 ( 99.48)
Epoch: [73][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0249e-01 (2.1387e-01)	Acc@1  91.25 ( 93.33)	Acc@5  97.50 ( 99.48)
## e[73] optimizer.zero_grad (sum) time: 0.10431718826293945
## e[73]       loss.backward (sum) time: 2.306798219680786
## e[73]      optimizer.step (sum) time: 0.9306938648223877
## epoch[73] training(only) time: 10.095330715179443
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.3164e+00 (1.3164e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.4473e+00 (1.5786e+00)	Acc@1  65.00 ( 67.45)	Acc@5  91.00 ( 88.45)
Test: [ 20/100]	Time  0.010 ( 0.019)	Loss 1.6963e+00 (1.5775e+00)	Acc@1  67.00 ( 67.14)	Acc@5  89.00 ( 88.38)
Test: [ 30/100]	Time  0.019 ( 0.017)	Loss 1.5205e+00 (1.5798e+00)	Acc@1  66.00 ( 66.81)	Acc@5  88.00 ( 88.39)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.6680e+00 (1.5730e+00)	Acc@1  67.00 ( 66.98)	Acc@5  90.00 ( 88.39)
Test: [ 50/100]	Time  0.012 ( 0.016)	Loss 1.6143e+00 (1.6018e+00)	Acc@1  67.00 ( 66.76)	Acc@5  90.00 ( 88.24)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.7422e+00 (1.5775e+00)	Acc@1  64.00 ( 66.98)	Acc@5  92.00 ( 88.57)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.5498e+00 (1.5836e+00)	Acc@1  68.00 ( 67.08)	Acc@5  89.00 ( 88.61)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.5000e+00 (1.5842e+00)	Acc@1  66.00 ( 66.98)	Acc@5  89.00 ( 88.64)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9229e+00 (1.5710e+00)	Acc@1  60.00 ( 67.24)	Acc@5  89.00 ( 88.81)
 * Acc@1 67.280 Acc@5 88.800
### epoch[73] execution time: 11.676198959350586
EPOCH 74
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [74][  0/391]	Time  0.178 ( 0.178)	Data  0.147 ( 0.147)	Loss 2.3389e-01 (2.3389e-01)	Acc@1  92.97 ( 92.97)	Acc@5  99.22 ( 99.22)
Epoch: [74][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 2.6636e-01 (2.1299e-01)	Acc@1  89.84 ( 93.18)	Acc@5 100.00 ( 99.57)
Epoch: [74][ 20/391]	Time  0.021 ( 0.032)	Data  0.002 ( 0.009)	Loss 2.0410e-01 (2.1088e-01)	Acc@1  95.31 ( 93.45)	Acc@5  99.22 ( 99.55)
Epoch: [74][ 30/391]	Time  0.036 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.4807e-01 (2.0760e-01)	Acc@1  95.31 ( 93.52)	Acc@5 100.00 ( 99.65)
Epoch: [74][ 40/391]	Time  0.025 ( 0.029)	Data  0.002 ( 0.005)	Loss 2.2705e-01 (2.0911e-01)	Acc@1  93.75 ( 93.33)	Acc@5  99.22 ( 99.68)
Epoch: [74][ 50/391]	Time  0.020 ( 0.028)	Data  0.002 ( 0.005)	Loss 1.8188e-01 (2.0504e-01)	Acc@1  93.75 ( 93.46)	Acc@5 100.00 ( 99.68)
Epoch: [74][ 60/391]	Time  0.021 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.4856e-01 (2.0892e-01)	Acc@1  95.31 ( 93.40)	Acc@5 100.00 ( 99.62)
Epoch: [74][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7859e-01 (2.0873e-01)	Acc@1  95.31 ( 93.53)	Acc@5 100.00 ( 99.60)
Epoch: [74][ 80/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9373e-01 (2.0526e-01)	Acc@1  92.19 ( 93.54)	Acc@5 100.00 ( 99.64)
Epoch: [74][ 90/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.5564e-01 (2.0612e-01)	Acc@1  96.88 ( 93.51)	Acc@5  99.22 ( 99.61)
Epoch: [74][100/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7139e-01 (2.0899e-01)	Acc@1  92.19 ( 93.40)	Acc@5 100.00 ( 99.57)
Epoch: [74][110/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2644e-01 (2.1167e-01)	Acc@1  94.53 ( 93.26)	Acc@5  99.22 ( 99.58)
Epoch: [74][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.6865e-01 (2.1363e-01)	Acc@1  86.72 ( 93.19)	Acc@5  99.22 ( 99.56)
Epoch: [74][130/391]	Time  0.022 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.6465e-01 (2.1381e-01)	Acc@1  91.41 ( 93.14)	Acc@5  98.44 ( 99.56)
Epoch: [74][140/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8262e-01 (2.1370e-01)	Acc@1  96.09 ( 93.16)	Acc@5  99.22 ( 99.52)
Epoch: [74][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0081e-01 (2.1428e-01)	Acc@1  92.19 ( 93.14)	Acc@5  99.22 ( 99.52)
Epoch: [74][160/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.6895e-01 (2.1422e-01)	Acc@1  94.53 ( 93.19)	Acc@5  99.22 ( 99.51)
Epoch: [74][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8408e-01 (2.1321e-01)	Acc@1  95.31 ( 93.23)	Acc@5  99.22 ( 99.51)
Epoch: [74][180/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5415e-01 (2.1331e-01)	Acc@1  90.62 ( 93.22)	Acc@5 100.00 ( 99.50)
Epoch: [74][190/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.4961e-01 (2.1306e-01)	Acc@1  90.62 ( 93.20)	Acc@5  99.22 ( 99.51)
Epoch: [74][200/391]	Time  0.039 ( 0.026)	Data  0.006 ( 0.003)	Loss 3.1299e-01 (2.1450e-01)	Acc@1  89.06 ( 93.13)	Acc@5 100.00 ( 99.51)
Epoch: [74][210/391]	Time  0.027 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.0386e-01 (2.1419e-01)	Acc@1  92.97 ( 93.12)	Acc@5 100.00 ( 99.52)
Epoch: [74][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3889e-01 (2.1477e-01)	Acc@1  91.41 ( 93.11)	Acc@5 100.00 ( 99.52)
Epoch: [74][230/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0178e-01 (2.1535e-01)	Acc@1  93.75 ( 93.05)	Acc@5  98.44 ( 99.50)
Epoch: [74][240/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.1993e-01 (2.1599e-01)	Acc@1  98.44 ( 93.05)	Acc@5 100.00 ( 99.48)
Epoch: [74][250/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4658e-01 (2.1556e-01)	Acc@1  91.41 ( 93.05)	Acc@5  99.22 ( 99.50)
Epoch: [74][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3511e-01 (2.1526e-01)	Acc@1  93.75 ( 93.06)	Acc@5  99.22 ( 99.50)
Epoch: [74][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0203e-01 (2.1502e-01)	Acc@1  92.19 ( 93.06)	Acc@5  99.22 ( 99.51)
Epoch: [74][280/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.8494e-01 (2.1503e-01)	Acc@1  92.97 ( 93.04)	Acc@5 100.00 ( 99.51)
Epoch: [74][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3049e-01 (2.1494e-01)	Acc@1  96.09 ( 93.06)	Acc@5 100.00 ( 99.51)
Epoch: [74][300/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.8711e-01 (2.1553e-01)	Acc@1  89.06 ( 93.04)	Acc@5  99.22 ( 99.51)
Epoch: [74][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0557e-01 (2.1637e-01)	Acc@1  93.75 ( 92.98)	Acc@5 100.00 ( 99.51)
Epoch: [74][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8054e-01 (2.1698e-01)	Acc@1  95.31 ( 92.97)	Acc@5  99.22 ( 99.51)
Epoch: [74][330/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.9836e-01 (2.1644e-01)	Acc@1  92.97 ( 92.98)	Acc@5 100.00 ( 99.51)
Epoch: [74][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5955e-01 (2.1593e-01)	Acc@1  95.31 ( 93.01)	Acc@5 100.00 ( 99.52)
Epoch: [74][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7871e-01 (2.1511e-01)	Acc@1  94.53 ( 93.06)	Acc@5 100.00 ( 99.53)
Epoch: [74][360/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.5220e-01 (2.1550e-01)	Acc@1  92.97 ( 93.07)	Acc@5  99.22 ( 99.52)
Epoch: [74][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9858e-01 (2.1615e-01)	Acc@1  92.97 ( 93.07)	Acc@5 100.00 ( 99.51)
Epoch: [74][380/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0691e-01 (2.1598e-01)	Acc@1  93.75 ( 93.08)	Acc@5  99.22 ( 99.51)
Epoch: [74][390/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7957e-01 (2.1593e-01)	Acc@1  96.25 ( 93.10)	Acc@5 100.00 ( 99.51)
## e[74] optimizer.zero_grad (sum) time: 0.1062631607055664
## e[74]       loss.backward (sum) time: 2.2924091815948486
## e[74]      optimizer.step (sum) time: 0.9269676208496094
## epoch[74] training(only) time: 10.133410692214966
# Switched to evaluate mode...
Test: [  0/100]	Time  0.145 ( 0.145)	Loss 1.3115e+00 (1.3115e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.012 ( 0.025)	Loss 1.4697e+00 (1.5677e+00)	Acc@1  63.00 ( 67.45)	Acc@5  91.00 ( 88.18)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 1.7051e+00 (1.5689e+00)	Acc@1  65.00 ( 67.24)	Acc@5  89.00 ( 88.00)
Test: [ 30/100]	Time  0.018 ( 0.018)	Loss 1.4883e+00 (1.5714e+00)	Acc@1  66.00 ( 66.97)	Acc@5  88.00 ( 87.97)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6758e+00 (1.5663e+00)	Acc@1  67.00 ( 67.02)	Acc@5  90.00 ( 88.12)
Test: [ 50/100]	Time  0.020 ( 0.016)	Loss 1.5889e+00 (1.5940e+00)	Acc@1  67.00 ( 66.80)	Acc@5  89.00 ( 87.94)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 1.7969e+00 (1.5693e+00)	Acc@1  61.00 ( 66.95)	Acc@5  93.00 ( 88.33)
Test: [ 70/100]	Time  0.018 ( 0.016)	Loss 1.5137e+00 (1.5755e+00)	Acc@1  69.00 ( 67.13)	Acc@5  90.00 ( 88.37)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.5264e+00 (1.5765e+00)	Acc@1  65.00 ( 67.04)	Acc@5  89.00 ( 88.37)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9209e+00 (1.5624e+00)	Acc@1  59.00 ( 67.31)	Acc@5  88.00 ( 88.56)
 * Acc@1 67.360 Acc@5 88.570
### epoch[74] execution time: 11.72324514389038
EPOCH 75
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [75][  0/391]	Time  0.170 ( 0.170)	Data  0.141 ( 0.141)	Loss 1.8311e-01 (1.8311e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [75][ 10/391]	Time  0.029 ( 0.038)	Data  0.002 ( 0.014)	Loss 2.3010e-01 (2.1507e-01)	Acc@1  91.41 ( 93.96)	Acc@5  99.22 ( 99.43)
Epoch: [75][ 20/391]	Time  0.022 ( 0.032)	Data  0.001 ( 0.008)	Loss 1.9214e-01 (2.0251e-01)	Acc@1  92.97 ( 94.01)	Acc@5 100.00 ( 99.63)
Epoch: [75][ 30/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.8149e-01 (2.0197e-01)	Acc@1  89.06 ( 93.88)	Acc@5  98.44 ( 99.65)
Epoch: [75][ 40/391]	Time  0.019 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.1578e-01 (2.0070e-01)	Acc@1  96.09 ( 93.88)	Acc@5  99.22 ( 99.62)
Epoch: [75][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.2937e-01 (2.0328e-01)	Acc@1  92.19 ( 93.72)	Acc@5 100.00 ( 99.62)
Epoch: [75][ 60/391]	Time  0.042 ( 0.027)	Data  0.003 ( 0.004)	Loss 2.7197e-01 (2.0808e-01)	Acc@1  92.97 ( 93.37)	Acc@5  99.22 ( 99.62)
Epoch: [75][ 70/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7456e-01 (2.0450e-01)	Acc@1  93.75 ( 93.47)	Acc@5  99.22 ( 99.61)
Epoch: [75][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.2351e-01 (2.0777e-01)	Acc@1  92.19 ( 93.48)	Acc@5  98.44 ( 99.54)
Epoch: [75][ 90/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8652e-01 (2.0633e-01)	Acc@1  95.31 ( 93.57)	Acc@5 100.00 ( 99.53)
Epoch: [75][100/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8323e-01 (2.0765e-01)	Acc@1  95.31 ( 93.53)	Acc@5  99.22 ( 99.54)
Epoch: [75][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8027e-01 (2.0931e-01)	Acc@1  93.75 ( 93.55)	Acc@5  98.44 ( 99.52)
Epoch: [75][120/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1836e-01 (2.0948e-01)	Acc@1  92.19 ( 93.51)	Acc@5  98.44 ( 99.51)
Epoch: [75][130/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5854e-01 (2.0974e-01)	Acc@1  92.97 ( 93.49)	Acc@5 100.00 ( 99.51)
Epoch: [75][140/391]	Time  0.025 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.9370e-01 (2.1104e-01)	Acc@1  86.72 ( 93.39)	Acc@5 100.00 ( 99.52)
Epoch: [75][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3608e-01 (2.0996e-01)	Acc@1  90.62 ( 93.42)	Acc@5 100.00 ( 99.52)
Epoch: [75][160/391]	Time  0.028 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1667e-01 (2.1048e-01)	Acc@1  93.75 ( 93.39)	Acc@5 100.00 ( 99.53)
Epoch: [75][170/391]	Time  0.022 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5415e-01 (2.1012e-01)	Acc@1  91.41 ( 93.41)	Acc@5 100.00 ( 99.54)
Epoch: [75][180/391]	Time  0.035 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.6638e-01 (2.1015e-01)	Acc@1  96.88 ( 93.41)	Acc@5  99.22 ( 99.55)
Epoch: [75][190/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4463e-01 (2.1192e-01)	Acc@1  91.41 ( 93.35)	Acc@5  99.22 ( 99.54)
Epoch: [75][200/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0789e-01 (2.1203e-01)	Acc@1  92.19 ( 93.30)	Acc@5 100.00 ( 99.56)
Epoch: [75][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5854e-01 (2.1250e-01)	Acc@1  90.62 ( 93.24)	Acc@5  99.22 ( 99.56)
Epoch: [75][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6248e-01 (2.1442e-01)	Acc@1  96.88 ( 93.19)	Acc@5  99.22 ( 99.54)
Epoch: [75][230/391]	Time  0.040 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.3206e-01 (2.1459e-01)	Acc@1  96.09 ( 93.23)	Acc@5  99.22 ( 99.53)
Epoch: [75][240/391]	Time  0.035 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.4829e-01 (2.1454e-01)	Acc@1  92.97 ( 93.22)	Acc@5  98.44 ( 99.53)
Epoch: [75][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7710e-01 (2.1477e-01)	Acc@1  92.19 ( 93.23)	Acc@5 100.00 ( 99.52)
Epoch: [75][260/391]	Time  0.029 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.0303e-01 (2.1364e-01)	Acc@1  96.88 ( 93.30)	Acc@5 100.00 ( 99.52)
Epoch: [75][270/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.0020e-01 (2.1337e-01)	Acc@1  92.97 ( 93.29)	Acc@5  99.22 ( 99.52)
Epoch: [75][280/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3499e-01 (2.1467e-01)	Acc@1  92.97 ( 93.25)	Acc@5  99.22 ( 99.51)
Epoch: [75][290/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9419e-01 (2.1474e-01)	Acc@1  89.84 ( 93.23)	Acc@5  99.22 ( 99.51)
Epoch: [75][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5586e-01 (2.1578e-01)	Acc@1  94.53 ( 93.19)	Acc@5  98.44 ( 99.50)
Epoch: [75][310/391]	Time  0.036 ( 0.026)	Data  0.005 ( 0.002)	Loss 2.8882e-01 (2.1620e-01)	Acc@1  91.41 ( 93.19)	Acc@5 100.00 ( 99.50)
Epoch: [75][320/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8970e-01 (2.1569e-01)	Acc@1  92.19 ( 93.19)	Acc@5 100.00 ( 99.51)
Epoch: [75][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1802e-01 (2.1639e-01)	Acc@1  93.75 ( 93.16)	Acc@5 100.00 ( 99.51)
Epoch: [75][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3328e-01 (2.1618e-01)	Acc@1  92.19 ( 93.17)	Acc@5 100.00 ( 99.52)
Epoch: [75][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7651e-01 (2.1633e-01)	Acc@1  95.31 ( 93.18)	Acc@5  99.22 ( 99.51)
Epoch: [75][360/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1372e-01 (2.1683e-01)	Acc@1  92.19 ( 93.18)	Acc@5  98.44 ( 99.50)
Epoch: [75][370/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5732e-01 (2.1633e-01)	Acc@1  92.97 ( 93.20)	Acc@5  99.22 ( 99.50)
Epoch: [75][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8457e-01 (2.1627e-01)	Acc@1  94.53 ( 93.19)	Acc@5 100.00 ( 99.50)
Epoch: [75][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4573e-01 (2.1592e-01)	Acc@1  90.00 ( 93.19)	Acc@5 100.00 ( 99.51)
## e[75] optimizer.zero_grad (sum) time: 0.10436058044433594
## e[75]       loss.backward (sum) time: 2.2899067401885986
## e[75]      optimizer.step (sum) time: 0.9109349250793457
## epoch[75] training(only) time: 10.11630654335022
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 1.2959e+00 (1.2959e+00)	Acc@1  74.00 ( 74.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.009 ( 0.025)	Loss 1.4395e+00 (1.5672e+00)	Acc@1  64.00 ( 68.00)	Acc@5  91.00 ( 88.55)
Test: [ 20/100]	Time  0.021 ( 0.020)	Loss 1.6719e+00 (1.5701e+00)	Acc@1  65.00 ( 67.33)	Acc@5  90.00 ( 88.29)
Test: [ 30/100]	Time  0.009 ( 0.017)	Loss 1.4639e+00 (1.5722e+00)	Acc@1  64.00 ( 66.94)	Acc@5  88.00 ( 88.13)
Test: [ 40/100]	Time  0.009 ( 0.017)	Loss 1.6992e+00 (1.5671e+00)	Acc@1  67.00 ( 67.07)	Acc@5  90.00 ( 88.39)
Test: [ 50/100]	Time  0.018 ( 0.017)	Loss 1.6211e+00 (1.5959e+00)	Acc@1  67.00 ( 66.84)	Acc@5  90.00 ( 88.18)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.7812e+00 (1.5705e+00)	Acc@1  62.00 ( 67.00)	Acc@5  92.00 ( 88.59)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.5342e+00 (1.5774e+00)	Acc@1  68.00 ( 67.15)	Acc@5  91.00 ( 88.62)
Test: [ 80/100]	Time  0.027 ( 0.016)	Loss 1.4961e+00 (1.5784e+00)	Acc@1  67.00 ( 67.10)	Acc@5  89.00 ( 88.63)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9141e+00 (1.5644e+00)	Acc@1  61.00 ( 67.35)	Acc@5  88.00 ( 88.82)
 * Acc@1 67.350 Acc@5 88.840
### epoch[75] execution time: 11.721669435501099
EPOCH 76
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [76][  0/391]	Time  0.180 ( 0.180)	Data  0.151 ( 0.151)	Loss 2.0300e-01 (2.0300e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [76][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.016)	Loss 2.2998e-01 (2.2453e-01)	Acc@1  92.97 ( 93.54)	Acc@5  99.22 ( 99.57)
Epoch: [76][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.010)	Loss 2.4536e-01 (2.1738e-01)	Acc@1  92.19 ( 93.53)	Acc@5  97.66 ( 99.52)
Epoch: [76][ 30/391]	Time  0.023 ( 0.030)	Data  0.002 ( 0.007)	Loss 2.4146e-01 (2.0858e-01)	Acc@1  90.62 ( 93.52)	Acc@5 100.00 ( 99.65)
Epoch: [76][ 40/391]	Time  0.022 ( 0.029)	Data  0.005 ( 0.006)	Loss 2.3596e-01 (2.0939e-01)	Acc@1  92.97 ( 93.37)	Acc@5  99.22 ( 99.60)
Epoch: [76][ 50/391]	Time  0.032 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.4038e-01 (2.0648e-01)	Acc@1  96.09 ( 93.57)	Acc@5 100.00 ( 99.60)
Epoch: [76][ 60/391]	Time  0.021 ( 0.028)	Data  0.002 ( 0.005)	Loss 2.4609e-01 (2.1438e-01)	Acc@1  92.97 ( 93.38)	Acc@5 100.00 ( 99.50)
Epoch: [76][ 70/391]	Time  0.036 ( 0.028)	Data  0.000 ( 0.004)	Loss 1.7773e-01 (2.1432e-01)	Acc@1  92.97 ( 93.31)	Acc@5 100.00 ( 99.53)
Epoch: [76][ 80/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8469e-01 (2.1656e-01)	Acc@1  95.31 ( 93.23)	Acc@5  99.22 ( 99.51)
Epoch: [76][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1448e-01 (2.1506e-01)	Acc@1  92.97 ( 93.24)	Acc@5  99.22 ( 99.53)
Epoch: [76][100/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4673e-01 (2.1627e-01)	Acc@1  96.09 ( 93.20)	Acc@5 100.00 ( 99.52)
Epoch: [76][110/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7554e-01 (2.1483e-01)	Acc@1  95.31 ( 93.26)	Acc@5  99.22 ( 99.53)
Epoch: [76][120/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9590e-01 (2.1543e-01)	Acc@1  89.84 ( 93.24)	Acc@5  99.22 ( 99.50)
Epoch: [76][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2549e-01 (2.1511e-01)	Acc@1  96.88 ( 93.29)	Acc@5  99.22 ( 99.51)
Epoch: [76][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7197e-01 (2.1661e-01)	Acc@1  91.41 ( 93.26)	Acc@5 100.00 ( 99.49)
Epoch: [76][150/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7246e-01 (2.1576e-01)	Acc@1  92.19 ( 93.29)	Acc@5 100.00 ( 99.51)
Epoch: [76][160/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0166e-01 (2.1506e-01)	Acc@1  92.97 ( 93.30)	Acc@5  99.22 ( 99.52)
Epoch: [76][170/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7395e-01 (2.1330e-01)	Acc@1  94.53 ( 93.38)	Acc@5  99.22 ( 99.53)
Epoch: [76][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9458e-01 (2.1207e-01)	Acc@1  96.09 ( 93.43)	Acc@5  99.22 ( 99.53)
Epoch: [76][190/391]	Time  0.028 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.7224e-01 (2.1045e-01)	Acc@1  94.53 ( 93.47)	Acc@5 100.00 ( 99.53)
Epoch: [76][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6223e-01 (2.0988e-01)	Acc@1  94.53 ( 93.47)	Acc@5 100.00 ( 99.54)
Epoch: [76][210/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.9348e-01 (2.1058e-01)	Acc@1  94.53 ( 93.46)	Acc@5 100.00 ( 99.53)
Epoch: [76][220/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0101e-01 (2.0977e-01)	Acc@1  97.66 ( 93.48)	Acc@5 100.00 ( 99.54)
Epoch: [76][230/391]	Time  0.026 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.5269e-01 (2.1095e-01)	Acc@1  90.62 ( 93.42)	Acc@5 100.00 ( 99.54)
Epoch: [76][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6931e-01 (2.1038e-01)	Acc@1  94.53 ( 93.41)	Acc@5 100.00 ( 99.55)
Epoch: [76][250/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.0435e-01 (2.0978e-01)	Acc@1  95.31 ( 93.43)	Acc@5 100.00 ( 99.55)
Epoch: [76][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2266e-01 (2.0974e-01)	Acc@1  92.97 ( 93.42)	Acc@5  99.22 ( 99.56)
Epoch: [76][270/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.8884e-01 (2.0956e-01)	Acc@1  94.53 ( 93.43)	Acc@5 100.00 ( 99.57)
Epoch: [76][280/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2766e-01 (2.0921e-01)	Acc@1  91.41 ( 93.43)	Acc@5  99.22 ( 99.57)
Epoch: [76][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4780e-01 (2.0924e-01)	Acc@1  92.97 ( 93.42)	Acc@5  98.44 ( 99.57)
Epoch: [76][300/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.2915e-01 (2.0999e-01)	Acc@1  96.09 ( 93.39)	Acc@5 100.00 ( 99.58)
Epoch: [76][310/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7124e-01 (2.1033e-01)	Acc@1  89.06 ( 93.38)	Acc@5  99.22 ( 99.57)
Epoch: [76][320/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.5100e-01 (2.0969e-01)	Acc@1  96.09 ( 93.40)	Acc@5 100.00 ( 99.56)
Epoch: [76][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3550e-01 (2.0894e-01)	Acc@1  95.31 ( 93.41)	Acc@5  99.22 ( 99.56)
Epoch: [76][340/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8555e-01 (2.0888e-01)	Acc@1  94.53 ( 93.42)	Acc@5  99.22 ( 99.56)
Epoch: [76][350/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7029e-01 (2.0899e-01)	Acc@1  94.53 ( 93.43)	Acc@5  99.22 ( 99.55)
Epoch: [76][360/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3855e-01 (2.0866e-01)	Acc@1  97.66 ( 93.46)	Acc@5 100.00 ( 99.55)
Epoch: [76][370/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7639e-01 (2.0886e-01)	Acc@1  92.97 ( 93.46)	Acc@5 100.00 ( 99.55)
Epoch: [76][380/391]	Time  0.030 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.4609e-01 (2.0879e-01)	Acc@1  91.41 ( 93.46)	Acc@5  99.22 ( 99.55)
Epoch: [76][390/391]	Time  0.016 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.0747e-01 (2.0924e-01)	Acc@1  90.00 ( 93.46)	Acc@5  93.75 ( 99.54)
## e[76] optimizer.zero_grad (sum) time: 0.10452413558959961
## e[76]       loss.backward (sum) time: 2.2769830226898193
## e[76]      optimizer.step (sum) time: 0.919147253036499
## epoch[76] training(only) time: 10.152292728424072
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.3115e+00 (1.3115e+00)	Acc@1  72.00 ( 72.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.4434e+00 (1.5716e+00)	Acc@1  64.00 ( 67.73)	Acc@5  91.00 ( 88.18)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 1.6816e+00 (1.5735e+00)	Acc@1  67.00 ( 67.00)	Acc@5  91.00 ( 87.95)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.4990e+00 (1.5775e+00)	Acc@1  65.00 ( 66.74)	Acc@5  88.00 ( 88.06)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6670e+00 (1.5720e+00)	Acc@1  67.00 ( 66.95)	Acc@5  90.00 ( 88.37)
Test: [ 50/100]	Time  0.016 ( 0.016)	Loss 1.6064e+00 (1.5998e+00)	Acc@1  67.00 ( 66.82)	Acc@5  89.00 ( 88.14)
Test: [ 60/100]	Time  0.010 ( 0.015)	Loss 1.7812e+00 (1.5754e+00)	Acc@1  60.00 ( 66.95)	Acc@5  92.00 ( 88.54)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 1.5664e+00 (1.5816e+00)	Acc@1  64.00 ( 67.15)	Acc@5  90.00 ( 88.63)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.5234e+00 (1.5829e+00)	Acc@1  66.00 ( 67.09)	Acc@5  89.00 ( 88.60)
Test: [ 90/100]	Time  0.009 ( 0.015)	Loss 1.9287e+00 (1.5693e+00)	Acc@1  60.00 ( 67.37)	Acc@5  88.00 ( 88.78)
 * Acc@1 67.380 Acc@5 88.790
### epoch[76] execution time: 11.726734399795532
EPOCH 77
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [77][  0/391]	Time  0.179 ( 0.179)	Data  0.148 ( 0.148)	Loss 1.7090e-01 (1.7090e-01)	Acc@1  96.09 ( 96.09)	Acc@5  98.44 ( 98.44)
Epoch: [77][ 10/391]	Time  0.027 ( 0.039)	Data  0.002 ( 0.015)	Loss 1.8066e-01 (2.1418e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 ( 99.22)
Epoch: [77][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.4600e-01 (2.0803e-01)	Acc@1  96.09 ( 93.71)	Acc@5 100.00 ( 99.29)
Epoch: [77][ 30/391]	Time  0.022 ( 0.030)	Data  0.004 ( 0.007)	Loss 2.2083e-01 (2.2053e-01)	Acc@1  92.97 ( 93.15)	Acc@5  99.22 ( 99.27)
Epoch: [77][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.1558e-01 (2.1802e-01)	Acc@1  91.41 ( 93.10)	Acc@5  99.22 ( 99.33)
Epoch: [77][ 50/391]	Time  0.033 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.1643e-01 (2.1609e-01)	Acc@1  92.97 ( 93.20)	Acc@5  99.22 ( 99.40)
Epoch: [77][ 60/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.8030e-01 (2.0737e-01)	Acc@1  92.97 ( 93.42)	Acc@5  99.22 ( 99.46)
Epoch: [77][ 70/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6797e-01 (2.0841e-01)	Acc@1  96.09 ( 93.32)	Acc@5 100.00 ( 99.48)
Epoch: [77][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7810e-01 (2.0941e-01)	Acc@1  96.09 ( 93.34)	Acc@5 100.00 ( 99.48)
Epoch: [77][ 90/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7297e-01 (2.0788e-01)	Acc@1  94.53 ( 93.33)	Acc@5 100.00 ( 99.53)
Epoch: [77][100/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.9883e-01 (2.0884e-01)	Acc@1  89.84 ( 93.20)	Acc@5  98.44 ( 99.50)
Epoch: [77][110/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4758e-01 (2.0834e-01)	Acc@1  95.31 ( 93.17)	Acc@5 100.00 ( 99.53)
Epoch: [77][120/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1920e-01 (2.0504e-01)	Acc@1  96.88 ( 93.26)	Acc@5 100.00 ( 99.55)
Epoch: [77][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0422e-01 (2.0657e-01)	Acc@1  92.97 ( 93.21)	Acc@5 100.00 ( 99.53)
Epoch: [77][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2888e-01 (2.0926e-01)	Acc@1  94.53 ( 93.20)	Acc@5  99.22 ( 99.49)
Epoch: [77][150/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.5952e-01 (2.1006e-01)	Acc@1  88.28 ( 93.17)	Acc@5  99.22 ( 99.49)
Epoch: [77][160/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7444e-01 (2.1176e-01)	Acc@1  94.53 ( 93.14)	Acc@5  99.22 ( 99.46)
Epoch: [77][170/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2439e-01 (2.0981e-01)	Acc@1  96.88 ( 93.22)	Acc@5  99.22 ( 99.47)
Epoch: [77][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5491e-01 (2.1035e-01)	Acc@1  94.53 ( 93.20)	Acc@5 100.00 ( 99.48)
Epoch: [77][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4280e-01 (2.1025e-01)	Acc@1  91.41 ( 93.19)	Acc@5  99.22 ( 99.48)
Epoch: [77][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0858e-01 (2.0799e-01)	Acc@1  96.88 ( 93.28)	Acc@5 100.00 ( 99.49)
Epoch: [77][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2693e-01 (2.0857e-01)	Acc@1  92.97 ( 93.28)	Acc@5 100.00 ( 99.48)
Epoch: [77][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5234e-01 (2.0827e-01)	Acc@1  96.09 ( 93.32)	Acc@5 100.00 ( 99.48)
Epoch: [77][230/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.0566e-01 (2.0870e-01)	Acc@1  92.97 ( 93.29)	Acc@5  98.44 ( 99.47)
Epoch: [77][240/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.4819e-01 (2.0835e-01)	Acc@1  95.31 ( 93.28)	Acc@5 100.00 ( 99.48)
Epoch: [77][250/391]	Time  0.029 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.3682e-01 (2.0767e-01)	Acc@1  92.97 ( 93.31)	Acc@5 100.00 ( 99.49)
Epoch: [77][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2140e-01 (2.0721e-01)	Acc@1  97.66 ( 93.35)	Acc@5 100.00 ( 99.49)
Epoch: [77][270/391]	Time  0.027 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.1951e-01 (2.0693e-01)	Acc@1  95.31 ( 93.37)	Acc@5 100.00 ( 99.49)
Epoch: [77][280/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0093e-01 (2.0761e-01)	Acc@1  91.41 ( 93.35)	Acc@5 100.00 ( 99.49)
Epoch: [77][290/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6636e-01 (2.0787e-01)	Acc@1  92.19 ( 93.34)	Acc@5  97.66 ( 99.48)
Epoch: [77][300/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9153e-01 (2.0864e-01)	Acc@1  94.53 ( 93.33)	Acc@5  99.22 ( 99.48)
Epoch: [77][310/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.4731e-01 (2.0794e-01)	Acc@1  92.19 ( 93.37)	Acc@5  99.22 ( 99.48)
Epoch: [77][320/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.9224e-01 (2.0801e-01)	Acc@1  87.50 ( 93.35)	Acc@5  99.22 ( 99.47)
Epoch: [77][330/391]	Time  0.037 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6895e-01 (2.0730e-01)	Acc@1  93.75 ( 93.37)	Acc@5  99.22 ( 99.47)
Epoch: [77][340/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3354e-01 (2.0760e-01)	Acc@1  95.31 ( 93.37)	Acc@5 100.00 ( 99.48)
Epoch: [77][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7566e-01 (2.0755e-01)	Acc@1  93.75 ( 93.39)	Acc@5 100.00 ( 99.49)
Epoch: [77][360/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.6489e-01 (2.0766e-01)	Acc@1  89.06 ( 93.38)	Acc@5 100.00 ( 99.49)
Epoch: [77][370/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.9910e-01 (2.0830e-01)	Acc@1  92.19 ( 93.36)	Acc@5 100.00 ( 99.49)
Epoch: [77][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5637e-01 (2.0771e-01)	Acc@1  93.75 ( 93.40)	Acc@5 100.00 ( 99.49)
Epoch: [77][390/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.5586e-01 (2.0732e-01)	Acc@1  90.00 ( 93.41)	Acc@5 100.00 ( 99.49)
## e[77] optimizer.zero_grad (sum) time: 0.10597515106201172
## e[77]       loss.backward (sum) time: 2.3031628131866455
## e[77]      optimizer.step (sum) time: 0.9260356426239014
## epoch[77] training(only) time: 10.139070272445679
# Switched to evaluate mode...
Test: [  0/100]	Time  0.143 ( 0.143)	Loss 1.3037e+00 (1.3037e+00)	Acc@1  72.00 ( 72.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.4727e+00 (1.5759e+00)	Acc@1  65.00 ( 67.64)	Acc@5  92.00 ( 88.18)
Test: [ 20/100]	Time  0.020 ( 0.020)	Loss 1.6904e+00 (1.5756e+00)	Acc@1  65.00 ( 67.05)	Acc@5  89.00 ( 87.90)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.5059e+00 (1.5740e+00)	Acc@1  65.00 ( 66.77)	Acc@5  88.00 ( 87.90)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6406e+00 (1.5693e+00)	Acc@1  67.00 ( 66.93)	Acc@5  90.00 ( 88.10)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6426e+00 (1.5973e+00)	Acc@1  67.00 ( 66.69)	Acc@5  89.00 ( 87.94)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.7666e+00 (1.5720e+00)	Acc@1  61.00 ( 66.97)	Acc@5  92.00 ( 88.34)
Test: [ 70/100]	Time  0.024 ( 0.016)	Loss 1.5107e+00 (1.5771e+00)	Acc@1  68.00 ( 67.13)	Acc@5  91.00 ( 88.42)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.4570e+00 (1.5783e+00)	Acc@1  68.00 ( 67.11)	Acc@5  89.00 ( 88.43)
Test: [ 90/100]	Time  0.023 ( 0.015)	Loss 1.9609e+00 (1.5662e+00)	Acc@1  61.00 ( 67.37)	Acc@5  88.00 ( 88.66)
 * Acc@1 67.360 Acc@5 88.660
### epoch[77] execution time: 11.74980878829956
EPOCH 78
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [78][  0/391]	Time  0.178 ( 0.178)	Data  0.149 ( 0.149)	Loss 1.3318e-01 (1.3318e-01)	Acc@1  94.53 ( 94.53)	Acc@5 100.00 (100.00)
Epoch: [78][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 2.5928e-01 (2.1203e-01)	Acc@1  92.19 ( 93.25)	Acc@5  98.44 ( 99.22)
Epoch: [78][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.8091e-01 (2.0907e-01)	Acc@1  94.53 ( 93.27)	Acc@5  99.22 ( 99.44)
Epoch: [78][ 30/391]	Time  0.027 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.4917e-01 (2.1006e-01)	Acc@1  95.31 ( 93.45)	Acc@5 100.00 ( 99.45)
Epoch: [78][ 40/391]	Time  0.025 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.5903e-01 (2.0670e-01)	Acc@1  92.19 ( 93.45)	Acc@5  99.22 ( 99.49)
Epoch: [78][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.4551e-01 (2.0557e-01)	Acc@1  95.31 ( 93.32)	Acc@5  99.22 ( 99.45)
Epoch: [78][ 60/391]	Time  0.024 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.6431e-01 (2.0747e-01)	Acc@1  95.31 ( 93.19)	Acc@5 100.00 ( 99.47)
Epoch: [78][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1670e-01 (2.0358e-01)	Acc@1  96.88 ( 93.42)	Acc@5 100.00 ( 99.47)
Epoch: [78][ 80/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.9006e-01 (2.0253e-01)	Acc@1  94.53 ( 93.46)	Acc@5  99.22 ( 99.50)
Epoch: [78][ 90/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.2339e-01 (2.0181e-01)	Acc@1  90.62 ( 93.50)	Acc@5  99.22 ( 99.48)
Epoch: [78][100/391]	Time  0.022 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0764e-01 (2.0126e-01)	Acc@1  94.53 ( 93.60)	Acc@5  99.22 ( 99.49)
Epoch: [78][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8384e-01 (2.0137e-01)	Acc@1  95.31 ( 93.62)	Acc@5 100.00 ( 99.49)
Epoch: [78][120/391]	Time  0.021 ( 0.026)	Data  0.003 ( 0.003)	Loss 8.8806e-02 (2.0362e-01)	Acc@1  96.88 ( 93.52)	Acc@5 100.00 ( 99.47)
Epoch: [78][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5391e-01 (2.0594e-01)	Acc@1  91.41 ( 93.44)	Acc@5 100.00 ( 99.48)
Epoch: [78][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2915e-01 (2.0633e-01)	Acc@1  96.88 ( 93.49)	Acc@5 100.00 ( 99.47)
Epoch: [78][150/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0215e-01 (2.0656e-01)	Acc@1  94.53 ( 93.50)	Acc@5 100.00 ( 99.48)
Epoch: [78][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4146e-01 (2.0769e-01)	Acc@1  90.62 ( 93.46)	Acc@5  99.22 ( 99.49)
Epoch: [78][170/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6528e-01 (2.0804e-01)	Acc@1  92.97 ( 93.48)	Acc@5 100.00 ( 99.48)
Epoch: [78][180/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7627e-01 (2.0604e-01)	Acc@1  92.19 ( 93.50)	Acc@5  99.22 ( 99.49)
Epoch: [78][190/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.3169e-01 (2.0706e-01)	Acc@1  92.97 ( 93.45)	Acc@5  98.44 ( 99.48)
Epoch: [78][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2656e-01 (2.0759e-01)	Acc@1  93.75 ( 93.44)	Acc@5  99.22 ( 99.48)
Epoch: [78][210/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3081e-01 (2.0769e-01)	Acc@1  89.06 ( 93.46)	Acc@5  99.22 ( 99.48)
Epoch: [78][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6760e-01 (2.0794e-01)	Acc@1  94.53 ( 93.41)	Acc@5  99.22 ( 99.48)
Epoch: [78][230/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4353e-01 (2.0885e-01)	Acc@1  91.41 ( 93.39)	Acc@5 100.00 ( 99.48)
Epoch: [78][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.6084e-01 (2.1030e-01)	Acc@1  87.50 ( 93.35)	Acc@5 100.00 ( 99.48)
Epoch: [78][250/391]	Time  0.023 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.2314e-01 (2.1087e-01)	Acc@1  92.19 ( 93.31)	Acc@5  99.22 ( 99.49)
Epoch: [78][260/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.0667e-01 (2.0981e-01)	Acc@1  92.97 ( 93.35)	Acc@5 100.00 ( 99.50)
Epoch: [78][270/391]	Time  0.023 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.5562e-01 (2.0921e-01)	Acc@1  90.62 ( 93.36)	Acc@5 100.00 ( 99.51)
Epoch: [78][280/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3633e-01 (2.0938e-01)	Acc@1  93.75 ( 93.36)	Acc@5 100.00 ( 99.51)
Epoch: [78][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5735e-01 (2.0858e-01)	Acc@1  96.88 ( 93.39)	Acc@5 100.00 ( 99.52)
Epoch: [78][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8027e-01 (2.0903e-01)	Acc@1  92.19 ( 93.40)	Acc@5  99.22 ( 99.52)
Epoch: [78][310/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9116e-01 (2.0855e-01)	Acc@1  94.53 ( 93.42)	Acc@5  99.22 ( 99.53)
Epoch: [78][320/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.1570e-01 (2.0829e-01)	Acc@1  92.19 ( 93.43)	Acc@5  99.22 ( 99.53)
Epoch: [78][330/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5879e-01 (2.0778e-01)	Acc@1  93.75 ( 93.45)	Acc@5  99.22 ( 99.53)
Epoch: [78][340/391]	Time  0.035 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7188e-01 (2.0771e-01)	Acc@1  94.53 ( 93.43)	Acc@5 100.00 ( 99.54)
Epoch: [78][350/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7505e-01 (2.0771e-01)	Acc@1  92.97 ( 93.43)	Acc@5 100.00 ( 99.54)
Epoch: [78][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0029e-01 (2.0855e-01)	Acc@1  90.62 ( 93.38)	Acc@5  98.44 ( 99.53)
Epoch: [78][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2620e-01 (2.0850e-01)	Acc@1  91.41 ( 93.38)	Acc@5  98.44 ( 99.53)
Epoch: [78][380/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.4355e-01 (2.0789e-01)	Acc@1  96.88 ( 93.40)	Acc@5 100.00 ( 99.53)
Epoch: [78][390/391]	Time  0.016 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1885e-01 (2.0802e-01)	Acc@1  90.00 ( 93.39)	Acc@5  98.75 ( 99.53)
## e[78] optimizer.zero_grad (sum) time: 0.10593366622924805
## e[78]       loss.backward (sum) time: 2.2686548233032227
## e[78]      optimizer.step (sum) time: 0.9268398284912109
## epoch[78] training(only) time: 10.125776290893555
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.3203e+00 (1.3203e+00)	Acc@1  72.00 ( 72.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.015 ( 0.025)	Loss 1.4756e+00 (1.5771e+00)	Acc@1  65.00 ( 67.64)	Acc@5  90.00 ( 88.27)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.7266e+00 (1.5768e+00)	Acc@1  66.00 ( 67.00)	Acc@5  89.00 ( 88.24)
Test: [ 30/100]	Time  0.011 ( 0.018)	Loss 1.5469e+00 (1.5798e+00)	Acc@1  65.00 ( 66.71)	Acc@5  88.00 ( 88.29)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6777e+00 (1.5757e+00)	Acc@1  67.00 ( 66.83)	Acc@5  90.00 ( 88.37)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6240e+00 (1.6019e+00)	Acc@1  67.00 ( 66.69)	Acc@5  89.00 ( 88.18)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.7480e+00 (1.5755e+00)	Acc@1  63.00 ( 66.95)	Acc@5  92.00 ( 88.57)
Test: [ 70/100]	Time  0.011 ( 0.016)	Loss 1.5273e+00 (1.5810e+00)	Acc@1  66.00 ( 67.06)	Acc@5  90.00 ( 88.66)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.4971e+00 (1.5816e+00)	Acc@1  67.00 ( 67.07)	Acc@5  89.00 ( 88.65)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 1.9688e+00 (1.5686e+00)	Acc@1  61.00 ( 67.34)	Acc@5  89.00 ( 88.85)
 * Acc@1 67.410 Acc@5 88.790
### epoch[78] execution time: 11.690422773361206
EPOCH 79
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [79][  0/391]	Time  0.178 ( 0.178)	Data  0.144 ( 0.144)	Loss 1.5430e-01 (1.5430e-01)	Acc@1  96.88 ( 96.88)	Acc@5  99.22 ( 99.22)
Epoch: [79][ 10/391]	Time  0.023 ( 0.039)	Data  0.001 ( 0.015)	Loss 2.3389e-01 (1.9843e-01)	Acc@1  87.50 ( 93.89)	Acc@5 100.00 ( 99.22)
Epoch: [79][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.7468e-01 (2.0641e-01)	Acc@1  94.53 ( 93.34)	Acc@5 100.00 ( 99.37)
Epoch: [79][ 30/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.007)	Loss 2.1143e-01 (2.0434e-01)	Acc@1  91.41 ( 93.27)	Acc@5 100.00 ( 99.52)
Epoch: [79][ 40/391]	Time  0.047 ( 0.028)	Data  0.014 ( 0.006)	Loss 2.2205e-01 (2.0918e-01)	Acc@1  91.41 ( 93.04)	Acc@5  99.22 ( 99.49)
Epoch: [79][ 50/391]	Time  0.023 ( 0.027)	Data  0.004 ( 0.005)	Loss 1.6589e-01 (2.0845e-01)	Acc@1  92.97 ( 93.12)	Acc@5 100.00 ( 99.49)
Epoch: [79][ 60/391]	Time  0.037 ( 0.027)	Data  0.001 ( 0.005)	Loss 1.5930e-01 (2.0462e-01)	Acc@1  92.19 ( 93.29)	Acc@5 100.00 ( 99.51)
Epoch: [79][ 70/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0264e-01 (2.0330e-01)	Acc@1  94.53 ( 93.39)	Acc@5 100.00 ( 99.56)
Epoch: [79][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7661e-01 (2.0582e-01)	Acc@1  91.41 ( 93.31)	Acc@5 100.00 ( 99.52)
Epoch: [79][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.1823e-01 (2.0703e-01)	Acc@1  96.88 ( 93.34)	Acc@5 100.00 ( 99.51)
Epoch: [79][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1948e-01 (2.0502e-01)	Acc@1  92.97 ( 93.43)	Acc@5 100.00 ( 99.55)
Epoch: [79][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0496e-01 (2.0627e-01)	Acc@1  92.19 ( 93.35)	Acc@5 100.00 ( 99.54)
Epoch: [79][120/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0911e-01 (2.0485e-01)	Acc@1  92.19 ( 93.39)	Acc@5 100.00 ( 99.55)
Epoch: [79][130/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6978e-01 (2.0430e-01)	Acc@1  90.62 ( 93.44)	Acc@5 100.00 ( 99.56)
Epoch: [79][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3889e-01 (2.0300e-01)	Acc@1  92.97 ( 93.52)	Acc@5  98.44 ( 99.55)
Epoch: [79][150/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6833e-01 (2.0391e-01)	Acc@1  96.09 ( 93.50)	Acc@5 100.00 ( 99.56)
Epoch: [79][160/391]	Time  0.043 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0557e-01 (2.0316e-01)	Acc@1  92.97 ( 93.51)	Acc@5 100.00 ( 99.55)
Epoch: [79][170/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7297e-01 (2.0321e-01)	Acc@1  94.53 ( 93.49)	Acc@5 100.00 ( 99.56)
Epoch: [79][180/391]	Time  0.032 ( 0.026)	Data  0.003 ( 0.003)	Loss 9.2346e-02 (2.0159e-01)	Acc@1  97.66 ( 93.55)	Acc@5 100.00 ( 99.57)
Epoch: [79][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.1348e-01 (2.0120e-01)	Acc@1  90.62 ( 93.59)	Acc@5  99.22 ( 99.58)
Epoch: [79][200/391]	Time  0.031 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7053e-01 (2.0183e-01)	Acc@1  96.88 ( 93.59)	Acc@5 100.00 ( 99.58)
Epoch: [79][210/391]	Time  0.028 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.2876e-01 (2.0154e-01)	Acc@1  91.41 ( 93.58)	Acc@5 100.00 ( 99.60)
Epoch: [79][220/391]	Time  0.037 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.6382e-01 (2.0149e-01)	Acc@1  96.88 ( 93.61)	Acc@5 100.00 ( 99.60)
Epoch: [79][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4148e-01 (2.0185e-01)	Acc@1  93.75 ( 93.61)	Acc@5  99.22 ( 99.58)
Epoch: [79][240/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1033e-01 (2.0119e-01)	Acc@1  89.06 ( 93.60)	Acc@5 100.00 ( 99.58)
Epoch: [79][250/391]	Time  0.033 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3977e-01 (2.0131e-01)	Acc@1  96.88 ( 93.60)	Acc@5 100.00 ( 99.57)
Epoch: [79][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.1829e-01 (2.0109e-01)	Acc@1  96.09 ( 93.62)	Acc@5 100.00 ( 99.57)
Epoch: [79][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4512e-01 (2.0153e-01)	Acc@1  91.41 ( 93.61)	Acc@5  99.22 ( 99.57)
Epoch: [79][280/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.1667e-01 (2.0333e-01)	Acc@1  93.75 ( 93.57)	Acc@5 100.00 ( 99.56)
Epoch: [79][290/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2250e-01 (2.0358e-01)	Acc@1  96.88 ( 93.59)	Acc@5 100.00 ( 99.56)
Epoch: [79][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5442e-01 (2.0247e-01)	Acc@1  96.09 ( 93.63)	Acc@5  99.22 ( 99.56)
Epoch: [79][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7273e-01 (2.0222e-01)	Acc@1  93.75 ( 93.63)	Acc@5 100.00 ( 99.56)
Epoch: [79][320/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0227e-01 (2.0173e-01)	Acc@1  92.19 ( 93.63)	Acc@5  99.22 ( 99.56)
Epoch: [79][330/391]	Time  0.037 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.1594e-01 (2.0222e-01)	Acc@1  89.84 ( 93.61)	Acc@5 100.00 ( 99.56)
Epoch: [79][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1021e-01 (2.0308e-01)	Acc@1  91.41 ( 93.57)	Acc@5 100.00 ( 99.56)
Epoch: [79][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0886e-01 (2.0259e-01)	Acc@1  92.97 ( 93.58)	Acc@5 100.00 ( 99.57)
Epoch: [79][360/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9666e-01 (2.0260e-01)	Acc@1  94.53 ( 93.57)	Acc@5  99.22 ( 99.57)
Epoch: [79][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7627e-01 (2.0223e-01)	Acc@1  95.31 ( 93.59)	Acc@5 100.00 ( 99.57)
Epoch: [79][380/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.4155e-01 (2.0299e-01)	Acc@1  90.62 ( 93.57)	Acc@5 100.00 ( 99.56)
Epoch: [79][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4194e-01 (2.0336e-01)	Acc@1  90.00 ( 93.56)	Acc@5 100.00 ( 99.56)
## e[79] optimizer.zero_grad (sum) time: 0.10493946075439453
## e[79]       loss.backward (sum) time: 2.278125047683716
## e[79]      optimizer.step (sum) time: 0.9319121837615967
## epoch[79] training(only) time: 10.09457802772522
# Switched to evaluate mode...
Test: [  0/100]	Time  0.142 ( 0.142)	Loss 1.3203e+00 (1.3203e+00)	Acc@1  71.00 ( 71.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.023 ( 0.026)	Loss 1.5205e+00 (1.5751e+00)	Acc@1  63.00 ( 67.45)	Acc@5  91.00 ( 88.27)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.7119e+00 (1.5728e+00)	Acc@1  67.00 ( 67.19)	Acc@5  90.00 ( 87.95)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.5098e+00 (1.5763e+00)	Acc@1  65.00 ( 67.03)	Acc@5  88.00 ( 87.97)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6396e+00 (1.5725e+00)	Acc@1  67.00 ( 67.10)	Acc@5  91.00 ( 88.20)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6064e+00 (1.6025e+00)	Acc@1  67.00 ( 66.86)	Acc@5  89.00 ( 87.96)
Test: [ 60/100]	Time  0.026 ( 0.016)	Loss 1.7910e+00 (1.5776e+00)	Acc@1  61.00 ( 66.97)	Acc@5  93.00 ( 88.43)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.5146e+00 (1.5808e+00)	Acc@1  68.00 ( 67.11)	Acc@5  90.00 ( 88.55)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 1.5029e+00 (1.5821e+00)	Acc@1  66.00 ( 67.00)	Acc@5  89.00 ( 88.53)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9727e+00 (1.5692e+00)	Acc@1  59.00 ( 67.31)	Acc@5  88.00 ( 88.75)
 * Acc@1 67.440 Acc@5 88.730
### epoch[79] execution time: 11.713869571685791
EPOCH 80
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [80][  0/391]	Time  0.177 ( 0.177)	Data  0.145 ( 0.145)	Loss 9.9121e-02 (9.9121e-02)	Acc@1  96.88 ( 96.88)	Acc@5 100.00 (100.00)
Epoch: [80][ 10/391]	Time  0.030 ( 0.039)	Data  0.005 ( 0.015)	Loss 1.5283e-01 (2.2017e-01)	Acc@1  96.09 ( 93.82)	Acc@5 100.00 ( 99.50)
Epoch: [80][ 20/391]	Time  0.036 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.0883e-01 (2.0820e-01)	Acc@1  95.31 ( 94.05)	Acc@5 100.00 ( 99.59)
Epoch: [80][ 30/391]	Time  0.019 ( 0.029)	Data  0.001 ( 0.007)	Loss 2.4048e-01 (2.0337e-01)	Acc@1  93.75 ( 93.83)	Acc@5  99.22 ( 99.65)
Epoch: [80][ 40/391]	Time  0.020 ( 0.029)	Data  0.000 ( 0.005)	Loss 2.5854e-01 (2.0468e-01)	Acc@1  91.41 ( 93.67)	Acc@5 100.00 ( 99.66)
Epoch: [80][ 50/391]	Time  0.021 ( 0.028)	Data  0.000 ( 0.005)	Loss 1.5356e-01 (2.0197e-01)	Acc@1  96.09 ( 93.77)	Acc@5  99.22 ( 99.68)
Epoch: [80][ 60/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.4502e-01 (2.0540e-01)	Acc@1  96.09 ( 93.66)	Acc@5 100.00 ( 99.62)
Epoch: [80][ 70/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.0581e-01 (2.0840e-01)	Acc@1  93.75 ( 93.44)	Acc@5 100.00 ( 99.63)
Epoch: [80][ 80/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.2122e-01 (2.0719e-01)	Acc@1  95.31 ( 93.44)	Acc@5 100.00 ( 99.59)
Epoch: [80][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9629e-01 (2.0748e-01)	Acc@1  93.75 ( 93.47)	Acc@5  99.22 ( 99.57)
Epoch: [80][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.5439e-01 (2.0665e-01)	Acc@1  90.62 ( 93.49)	Acc@5  99.22 ( 99.56)
Epoch: [80][110/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.0498e-01 (2.0492e-01)	Acc@1  96.88 ( 93.50)	Acc@5 100.00 ( 99.58)
Epoch: [80][120/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3757e-01 (2.0629e-01)	Acc@1  96.09 ( 93.46)	Acc@5 100.00 ( 99.59)
Epoch: [80][130/391]	Time  0.039 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.7896e-01 (2.0805e-01)	Acc@1  93.75 ( 93.45)	Acc@5 100.00 ( 99.56)
Epoch: [80][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6589e-01 (2.0753e-01)	Acc@1  95.31 ( 93.47)	Acc@5  98.44 ( 99.55)
Epoch: [80][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4746e-01 (2.0715e-01)	Acc@1  95.31 ( 93.51)	Acc@5  99.22 ( 99.54)
Epoch: [80][160/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0874e-01 (2.0843e-01)	Acc@1  93.75 ( 93.50)	Acc@5  99.22 ( 99.53)
Epoch: [80][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0312e-01 (2.0937e-01)	Acc@1  92.97 ( 93.48)	Acc@5  99.22 ( 99.53)
Epoch: [80][180/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.3071e-01 (2.1097e-01)	Acc@1  95.31 ( 93.42)	Acc@5 100.00 ( 99.53)
Epoch: [80][190/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.003)	Loss 7.9834e-02 (2.1043e-01)	Acc@1  98.44 ( 93.44)	Acc@5 100.00 ( 99.54)
Epoch: [80][200/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3535e-01 (2.1035e-01)	Acc@1  92.97 ( 93.44)	Acc@5  99.22 ( 99.54)
Epoch: [80][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0398e-01 (2.1084e-01)	Acc@1  92.97 ( 93.42)	Acc@5  99.22 ( 99.51)
Epoch: [80][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6199e-01 (2.1120e-01)	Acc@1  95.31 ( 93.37)	Acc@5  99.22 ( 99.51)
Epoch: [80][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0615e-01 (2.1109e-01)	Acc@1  90.62 ( 93.35)	Acc@5  98.44 ( 99.51)
Epoch: [80][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3684e-01 (2.1057e-01)	Acc@1  96.09 ( 93.36)	Acc@5 100.00 ( 99.50)
Epoch: [80][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6040e-01 (2.1008e-01)	Acc@1  92.19 ( 93.37)	Acc@5 100.00 ( 99.51)
Epoch: [80][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7808e-01 (2.0878e-01)	Acc@1  90.62 ( 93.40)	Acc@5  98.44 ( 99.52)
Epoch: [80][270/391]	Time  0.026 ( 0.026)	Data  0.005 ( 0.002)	Loss 1.1987e-01 (2.0798e-01)	Acc@1  96.88 ( 93.43)	Acc@5 100.00 ( 99.52)
Epoch: [80][280/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5576e-01 (2.0655e-01)	Acc@1  95.31 ( 93.47)	Acc@5 100.00 ( 99.53)
Epoch: [80][290/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6467e-01 (2.0681e-01)	Acc@1  96.09 ( 93.47)	Acc@5 100.00 ( 99.54)
Epoch: [80][300/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4075e-01 (2.0546e-01)	Acc@1  96.09 ( 93.52)	Acc@5 100.00 ( 99.55)
Epoch: [80][310/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6123e-01 (2.0509e-01)	Acc@1  92.19 ( 93.55)	Acc@5  99.22 ( 99.54)
Epoch: [80][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2144e-01 (2.0535e-01)	Acc@1  91.41 ( 93.56)	Acc@5 100.00 ( 99.54)
Epoch: [80][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6187e-01 (2.0489e-01)	Acc@1  93.75 ( 93.58)	Acc@5 100.00 ( 99.53)
Epoch: [80][340/391]	Time  0.036 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.2396e-01 (2.0452e-01)	Acc@1  96.09 ( 93.60)	Acc@5 100.00 ( 99.53)
Epoch: [80][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8921e-01 (2.0420e-01)	Acc@1  93.75 ( 93.61)	Acc@5  98.44 ( 99.53)
Epoch: [80][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9043e-01 (2.0413e-01)	Acc@1  92.97 ( 93.60)	Acc@5 100.00 ( 99.53)
Epoch: [80][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6467e-01 (2.0339e-01)	Acc@1  95.31 ( 93.63)	Acc@5 100.00 ( 99.53)
Epoch: [80][380/391]	Time  0.028 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.8335e-01 (2.0330e-01)	Acc@1  93.75 ( 93.62)	Acc@5 100.00 ( 99.54)
Epoch: [80][390/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.8184e-01 (2.0355e-01)	Acc@1  86.25 ( 93.62)	Acc@5 100.00 ( 99.54)
## e[80] optimizer.zero_grad (sum) time: 0.10551738739013672
## e[80]       loss.backward (sum) time: 2.244621515274048
## e[80]      optimizer.step (sum) time: 0.9179549217224121
## epoch[80] training(only) time: 10.167363166809082
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.3184e+00 (1.3184e+00)	Acc@1  74.00 ( 74.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.5146e+00 (1.5881e+00)	Acc@1  64.00 ( 67.91)	Acc@5  91.00 ( 88.18)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 1.7207e+00 (1.5862e+00)	Acc@1  66.00 ( 67.57)	Acc@5  92.00 ( 88.29)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.5303e+00 (1.5874e+00)	Acc@1  64.00 ( 67.19)	Acc@5  88.00 ( 88.16)
Test: [ 40/100]	Time  0.021 ( 0.017)	Loss 1.6787e+00 (1.5829e+00)	Acc@1  66.00 ( 67.12)	Acc@5  90.00 ( 88.24)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6250e+00 (1.6109e+00)	Acc@1  67.00 ( 66.82)	Acc@5  89.00 ( 88.06)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 1.7715e+00 (1.5862e+00)	Acc@1  63.00 ( 67.08)	Acc@5  92.00 ( 88.46)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 1.5840e+00 (1.5926e+00)	Acc@1  65.00 ( 67.13)	Acc@5  90.00 ( 88.51)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.5254e+00 (1.5939e+00)	Acc@1  67.00 ( 67.02)	Acc@5  88.00 ( 88.47)
Test: [ 90/100]	Time  0.014 ( 0.015)	Loss 1.9414e+00 (1.5801e+00)	Acc@1  59.00 ( 67.25)	Acc@5  89.00 ( 88.71)
 * Acc@1 67.330 Acc@5 88.690
### epoch[80] execution time: 11.773038625717163
EPOCH 81
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [81][  0/391]	Time  0.185 ( 0.185)	Data  0.153 ( 0.153)	Loss 1.6797e-01 (1.6797e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [81][ 10/391]	Time  0.035 ( 0.040)	Data  0.001 ( 0.015)	Loss 2.7368e-01 (2.2646e-01)	Acc@1  92.19 ( 92.90)	Acc@5  99.22 ( 99.43)
Epoch: [81][ 20/391]	Time  0.020 ( 0.033)	Data  0.001 ( 0.009)	Loss 1.8237e-01 (2.1902e-01)	Acc@1  95.31 ( 93.01)	Acc@5  99.22 ( 99.44)
Epoch: [81][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.1387e-01 (2.1766e-01)	Acc@1  91.41 ( 93.22)	Acc@5 100.00 ( 99.50)
Epoch: [81][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.005)	Loss 7.6599e-02 (2.0366e-01)	Acc@1  99.22 ( 93.73)	Acc@5 100.00 ( 99.58)
Epoch: [81][ 50/391]	Time  0.026 ( 0.029)	Data  0.000 ( 0.005)	Loss 1.5222e-01 (2.0423e-01)	Acc@1  95.31 ( 93.75)	Acc@5  99.22 ( 99.56)
Epoch: [81][ 60/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.004)	Loss 3.0957e-01 (2.0707e-01)	Acc@1  91.41 ( 93.72)	Acc@5  99.22 ( 99.53)
Epoch: [81][ 70/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.3586e-01 (2.0683e-01)	Acc@1  96.88 ( 93.69)	Acc@5 100.00 ( 99.55)
Epoch: [81][ 80/391]	Time  0.024 ( 0.027)	Data  0.004 ( 0.004)	Loss 1.4124e-01 (2.0491e-01)	Acc@1  96.88 ( 93.74)	Acc@5 100.00 ( 99.55)
Epoch: [81][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1350e-01 (2.0584e-01)	Acc@1  92.97 ( 93.66)	Acc@5 100.00 ( 99.54)
Epoch: [81][100/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9678e-01 (2.0641e-01)	Acc@1  93.75 ( 93.61)	Acc@5  99.22 ( 99.55)
Epoch: [81][110/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.8787e-01 (2.0451e-01)	Acc@1  93.75 ( 93.61)	Acc@5 100.00 ( 99.56)
Epoch: [81][120/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.3083e-01 (2.0533e-01)	Acc@1  92.97 ( 93.60)	Acc@5  99.22 ( 99.57)
Epoch: [81][130/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2400e-01 (2.0422e-01)	Acc@1  92.97 ( 93.67)	Acc@5 100.00 ( 99.58)
Epoch: [81][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7881e-01 (2.0641e-01)	Acc@1  92.97 ( 93.59)	Acc@5  99.22 ( 99.56)
Epoch: [81][150/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6904e-01 (2.0597e-01)	Acc@1  89.84 ( 93.55)	Acc@5  97.66 ( 99.54)
Epoch: [81][160/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6541e-01 (2.0535e-01)	Acc@1  93.75 ( 93.57)	Acc@5 100.00 ( 99.54)
Epoch: [81][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2571e-01 (2.0678e-01)	Acc@1  92.19 ( 93.57)	Acc@5 100.00 ( 99.54)
Epoch: [81][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9385e-01 (2.0656e-01)	Acc@1  94.53 ( 93.53)	Acc@5 100.00 ( 99.54)
Epoch: [81][190/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7651e-01 (2.0684e-01)	Acc@1  95.31 ( 93.53)	Acc@5 100.00 ( 99.54)
Epoch: [81][200/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.6980e-01 (2.0754e-01)	Acc@1  95.31 ( 93.54)	Acc@5 100.00 ( 99.53)
Epoch: [81][210/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9507e-01 (2.0755e-01)	Acc@1  95.31 ( 93.52)	Acc@5  99.22 ( 99.53)
Epoch: [81][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6650e-01 (2.0706e-01)	Acc@1  94.53 ( 93.52)	Acc@5 100.00 ( 99.54)
Epoch: [81][230/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8787e-01 (2.0716e-01)	Acc@1  95.31 ( 93.53)	Acc@5  99.22 ( 99.53)
Epoch: [81][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2512e-01 (2.0660e-01)	Acc@1  96.88 ( 93.55)	Acc@5 100.00 ( 99.53)
Epoch: [81][250/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7578e-01 (2.0663e-01)	Acc@1  95.31 ( 93.56)	Acc@5  99.22 ( 99.52)
Epoch: [81][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9214e-01 (2.0704e-01)	Acc@1  94.53 ( 93.55)	Acc@5  99.22 ( 99.51)
Epoch: [81][270/391]	Time  0.035 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.3328e-01 (2.0676e-01)	Acc@1  91.41 ( 93.53)	Acc@5  99.22 ( 99.51)
Epoch: [81][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.5083e-01 (2.0691e-01)	Acc@1  89.06 ( 93.53)	Acc@5  98.44 ( 99.51)
Epoch: [81][290/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2437e-01 (2.0746e-01)	Acc@1  92.19 ( 93.50)	Acc@5 100.00 ( 99.52)
Epoch: [81][300/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1792e-01 (2.0682e-01)	Acc@1  96.09 ( 93.51)	Acc@5 100.00 ( 99.51)
Epoch: [81][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2524e-01 (2.0667e-01)	Acc@1  97.66 ( 93.53)	Acc@5 100.00 ( 99.51)
Epoch: [81][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2241e-01 (2.0667e-01)	Acc@1  93.75 ( 93.52)	Acc@5  99.22 ( 99.52)
Epoch: [81][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5146e-01 (2.0751e-01)	Acc@1  92.97 ( 93.49)	Acc@5 100.00 ( 99.52)
Epoch: [81][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3108e-01 (2.0709e-01)	Acc@1  89.84 ( 93.51)	Acc@5  99.22 ( 99.52)
Epoch: [81][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5808e-01 (2.0647e-01)	Acc@1  94.53 ( 93.52)	Acc@5 100.00 ( 99.53)
Epoch: [81][360/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7371e-01 (2.0603e-01)	Acc@1  92.97 ( 93.51)	Acc@5  99.22 ( 99.54)
Epoch: [81][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7588e-01 (2.0616e-01)	Acc@1  89.84 ( 93.50)	Acc@5 100.00 ( 99.54)
Epoch: [81][380/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4990e-01 (2.0493e-01)	Acc@1  96.88 ( 93.55)	Acc@5 100.00 ( 99.54)
Epoch: [81][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.9639e-01 (2.0419e-01)	Acc@1  92.50 ( 93.58)	Acc@5  97.50 ( 99.54)
## e[81] optimizer.zero_grad (sum) time: 0.10516548156738281
## e[81]       loss.backward (sum) time: 2.207775831222534
## e[81]      optimizer.step (sum) time: 0.9290337562561035
## epoch[81] training(only) time: 10.111476182937622
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.2656e+00 (1.2656e+00)	Acc@1  73.00 ( 73.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.010 ( 0.025)	Loss 1.5068e+00 (1.5767e+00)	Acc@1  65.00 ( 67.73)	Acc@5  90.00 ( 88.27)
Test: [ 20/100]	Time  0.020 ( 0.020)	Loss 1.7217e+00 (1.5783e+00)	Acc@1  66.00 ( 67.38)	Acc@5  91.00 ( 88.00)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.4941e+00 (1.5785e+00)	Acc@1  66.00 ( 67.16)	Acc@5  88.00 ( 88.00)
Test: [ 40/100]	Time  0.020 ( 0.017)	Loss 1.6738e+00 (1.5734e+00)	Acc@1  67.00 ( 67.27)	Acc@5  90.00 ( 88.22)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.5908e+00 (1.6004e+00)	Acc@1  67.00 ( 67.10)	Acc@5  89.00 ( 88.02)
Test: [ 60/100]	Time  0.018 ( 0.016)	Loss 1.7900e+00 (1.5764e+00)	Acc@1  61.00 ( 67.21)	Acc@5  92.00 ( 88.46)
Test: [ 70/100]	Time  0.010 ( 0.015)	Loss 1.5469e+00 (1.5830e+00)	Acc@1  67.00 ( 67.34)	Acc@5  90.00 ( 88.55)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.4629e+00 (1.5843e+00)	Acc@1  66.00 ( 67.23)	Acc@5  89.00 ( 88.54)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9453e+00 (1.5702e+00)	Acc@1  61.00 ( 67.49)	Acc@5  88.00 ( 88.78)
 * Acc@1 67.540 Acc@5 88.730
### epoch[81] execution time: 11.73722219467163
EPOCH 82
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [82][  0/391]	Time  0.182 ( 0.182)	Data  0.149 ( 0.149)	Loss 1.7932e-01 (1.7932e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [82][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.016)	Loss 3.4302e-01 (2.2715e-01)	Acc@1  89.84 ( 92.68)	Acc@5  99.22 ( 99.29)
Epoch: [82][ 20/391]	Time  0.021 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.9775e-01 (2.0957e-01)	Acc@1  95.31 ( 93.34)	Acc@5 100.00 ( 99.48)
Epoch: [82][ 30/391]	Time  0.028 ( 0.030)	Data  0.003 ( 0.007)	Loss 1.8152e-01 (2.0159e-01)	Acc@1  95.31 ( 93.85)	Acc@5 100.00 ( 99.42)
Epoch: [82][ 40/391]	Time  0.034 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.4121e-01 (1.9953e-01)	Acc@1  92.19 ( 93.83)	Acc@5  99.22 ( 99.49)
Epoch: [82][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.7188e-01 (1.9266e-01)	Acc@1  96.09 ( 94.00)	Acc@5  99.22 ( 99.54)
Epoch: [82][ 60/391]	Time  0.021 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.7249e-01 (1.9237e-01)	Acc@1  94.53 ( 93.93)	Acc@5 100.00 ( 99.56)
Epoch: [82][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.3376e-01 (1.9400e-01)	Acc@1  93.75 ( 93.81)	Acc@5  99.22 ( 99.57)
Epoch: [82][ 80/391]	Time  0.028 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.5146e-01 (1.9655e-01)	Acc@1  90.62 ( 93.67)	Acc@5  99.22 ( 99.57)
Epoch: [82][ 90/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.8457e-01 (1.9468e-01)	Acc@1  94.53 ( 93.82)	Acc@5 100.00 ( 99.57)
Epoch: [82][100/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.004)	Loss 1.0272e-01 (1.9316e-01)	Acc@1  98.44 ( 93.85)	Acc@5 100.00 ( 99.58)
Epoch: [82][110/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.3145e-01 (1.9481e-01)	Acc@1  91.41 ( 93.75)	Acc@5 100.00 ( 99.60)
Epoch: [82][120/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0154e-01 (1.9533e-01)	Acc@1  94.53 ( 93.76)	Acc@5 100.00 ( 99.61)
Epoch: [82][130/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.6294e-01 (1.9529e-01)	Acc@1  93.75 ( 93.79)	Acc@5  98.44 ( 99.62)
Epoch: [82][140/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.9702e-01 (1.9777e-01)	Acc@1  93.75 ( 93.74)	Acc@5  99.22 ( 99.59)
Epoch: [82][150/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 3.2446e-01 (1.9909e-01)	Acc@1  89.84 ( 93.64)	Acc@5  99.22 ( 99.60)
Epoch: [82][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 4.2407e-01 (1.9792e-01)	Acc@1  86.72 ( 93.69)	Acc@5  97.66 ( 99.60)
Epoch: [82][170/391]	Time  0.024 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.5391e-01 (1.9811e-01)	Acc@1  91.41 ( 93.69)	Acc@5 100.00 ( 99.60)
Epoch: [82][180/391]	Time  0.033 ( 0.026)	Data  0.005 ( 0.003)	Loss 1.6943e-01 (1.9840e-01)	Acc@1  93.75 ( 93.72)	Acc@5 100.00 ( 99.60)
Epoch: [82][190/391]	Time  0.029 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.1204e-01 (1.9856e-01)	Acc@1  94.53 ( 93.71)	Acc@5 100.00 ( 99.61)
Epoch: [82][200/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.7371e-01 (1.9911e-01)	Acc@1  93.75 ( 93.70)	Acc@5 100.00 ( 99.61)
Epoch: [82][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0459e-01 (1.9936e-01)	Acc@1  92.19 ( 93.72)	Acc@5 100.00 ( 99.60)
Epoch: [82][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2695e-01 (1.9926e-01)	Acc@1  97.66 ( 93.76)	Acc@5 100.00 ( 99.60)
Epoch: [82][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6235e-01 (1.9897e-01)	Acc@1  95.31 ( 93.79)	Acc@5  99.22 ( 99.60)
Epoch: [82][240/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7310e-01 (1.9987e-01)	Acc@1  93.75 ( 93.73)	Acc@5 100.00 ( 99.59)
Epoch: [82][250/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3596e-01 (2.0077e-01)	Acc@1  91.41 ( 93.68)	Acc@5 100.00 ( 99.59)
Epoch: [82][260/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8921e-01 (2.0015e-01)	Acc@1  96.09 ( 93.71)	Acc@5  99.22 ( 99.59)
Epoch: [82][270/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9470e-01 (2.0013e-01)	Acc@1  91.41 ( 93.70)	Acc@5  99.22 ( 99.59)
Epoch: [82][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4023e-01 (2.0031e-01)	Acc@1  92.19 ( 93.70)	Acc@5 100.00 ( 99.58)
Epoch: [82][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2451e-01 (2.0022e-01)	Acc@1  97.66 ( 93.69)	Acc@5 100.00 ( 99.58)
Epoch: [82][300/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.3269e-01 (2.0037e-01)	Acc@1  95.31 ( 93.69)	Acc@5 100.00 ( 99.56)
Epoch: [82][310/391]	Time  0.036 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.0566e-01 (2.0065e-01)	Acc@1  88.28 ( 93.65)	Acc@5  97.66 ( 99.57)
Epoch: [82][320/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1631e-01 (2.0147e-01)	Acc@1  92.97 ( 93.62)	Acc@5 100.00 ( 99.57)
Epoch: [82][330/391]	Time  0.033 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.9885e-01 (2.0065e-01)	Acc@1  92.19 ( 93.63)	Acc@5  99.22 ( 99.57)
Epoch: [82][340/391]	Time  0.032 ( 0.026)	Data  0.005 ( 0.002)	Loss 2.0178e-01 (2.0102e-01)	Acc@1  96.09 ( 93.62)	Acc@5  99.22 ( 99.56)
Epoch: [82][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8396e-01 (2.0122e-01)	Acc@1  92.97 ( 93.62)	Acc@5 100.00 ( 99.56)
Epoch: [82][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5903e-01 (2.0128e-01)	Acc@1  90.62 ( 93.63)	Acc@5  99.22 ( 99.56)
Epoch: [82][370/391]	Time  0.036 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.6614e-01 (2.0013e-01)	Acc@1  96.09 ( 93.67)	Acc@5 100.00 ( 99.56)
Epoch: [82][380/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9324e-01 (2.0119e-01)	Acc@1  92.97 ( 93.64)	Acc@5 100.00 ( 99.56)
Epoch: [82][390/391]	Time  0.019 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.2561e-01 (2.0147e-01)	Acc@1  96.25 ( 93.64)	Acc@5 100.00 ( 99.55)
## e[82] optimizer.zero_grad (sum) time: 0.10456585884094238
## e[82]       loss.backward (sum) time: 2.302004337310791
## e[82]      optimizer.step (sum) time: 0.9329724311828613
## epoch[82] training(only) time: 10.076479196548462
# Switched to evaluate mode...
Test: [  0/100]	Time  0.138 ( 0.138)	Loss 1.3096e+00 (1.3096e+00)	Acc@1  72.00 ( 72.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.015 ( 0.025)	Loss 1.4971e+00 (1.5770e+00)	Acc@1  65.00 ( 68.09)	Acc@5  90.00 ( 87.73)
Test: [ 20/100]	Time  0.014 ( 0.020)	Loss 1.7148e+00 (1.5801e+00)	Acc@1  66.00 ( 67.62)	Acc@5  91.00 ( 88.14)
Test: [ 30/100]	Time  0.016 ( 0.018)	Loss 1.5176e+00 (1.5843e+00)	Acc@1  64.00 ( 67.23)	Acc@5  88.00 ( 88.13)
Test: [ 40/100]	Time  0.029 ( 0.017)	Loss 1.7021e+00 (1.5822e+00)	Acc@1  66.00 ( 67.20)	Acc@5  90.00 ( 88.20)
Test: [ 50/100]	Time  0.017 ( 0.016)	Loss 1.5850e+00 (1.6070e+00)	Acc@1  67.00 ( 66.96)	Acc@5  89.00 ( 88.02)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.7998e+00 (1.5832e+00)	Acc@1  61.00 ( 67.15)	Acc@5  91.00 ( 88.39)
Test: [ 70/100]	Time  0.013 ( 0.015)	Loss 1.5088e+00 (1.5894e+00)	Acc@1  67.00 ( 67.15)	Acc@5  91.00 ( 88.49)
Test: [ 80/100]	Time  0.014 ( 0.015)	Loss 1.5088e+00 (1.5904e+00)	Acc@1  66.00 ( 67.11)	Acc@5  89.00 ( 88.44)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 1.9502e+00 (1.5769e+00)	Acc@1  60.00 ( 67.40)	Acc@5  88.00 ( 88.64)
 * Acc@1 67.480 Acc@5 88.610
### epoch[82] execution time: 11.654481172561646
EPOCH 83
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [83][  0/391]	Time  0.172 ( 0.172)	Data  0.142 ( 0.142)	Loss 2.8564e-01 (2.8564e-01)	Acc@1  92.97 ( 92.97)	Acc@5  97.66 ( 97.66)
Epoch: [83][ 10/391]	Time  0.021 ( 0.038)	Data  0.001 ( 0.015)	Loss 1.8726e-01 (2.2571e-01)	Acc@1  94.53 ( 93.11)	Acc@5  99.22 ( 99.22)
Epoch: [83][ 20/391]	Time  0.030 ( 0.032)	Data  0.001 ( 0.008)	Loss 2.2375e-01 (2.0960e-01)	Acc@1  91.41 ( 93.34)	Acc@5  98.44 ( 99.40)
Epoch: [83][ 30/391]	Time  0.038 ( 0.030)	Data  0.001 ( 0.006)	Loss 2.8979e-01 (2.1086e-01)	Acc@1  91.41 ( 93.20)	Acc@5  98.44 ( 99.45)
Epoch: [83][ 40/391]	Time  0.022 ( 0.029)	Data  0.000 ( 0.005)	Loss 1.9141e-01 (2.1063e-01)	Acc@1  93.75 ( 93.29)	Acc@5 100.00 ( 99.45)
Epoch: [83][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.6270e-01 (2.1346e-01)	Acc@1  92.19 ( 93.11)	Acc@5  99.22 ( 99.45)
Epoch: [83][ 60/391]	Time  0.034 ( 0.027)	Data  0.004 ( 0.004)	Loss 2.6074e-01 (2.1562e-01)	Acc@1  92.19 ( 93.03)	Acc@5  98.44 ( 99.44)
Epoch: [83][ 70/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.2180e-01 (2.1196e-01)	Acc@1  92.97 ( 93.19)	Acc@5  99.22 ( 99.44)
Epoch: [83][ 80/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.2153e-01 (2.1101e-01)	Acc@1  90.62 ( 93.28)	Acc@5  99.22 ( 99.43)
Epoch: [83][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.3547e-01 (2.0787e-01)	Acc@1  92.97 ( 93.34)	Acc@5  99.22 ( 99.47)
Epoch: [83][100/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8188e-01 (2.0535e-01)	Acc@1  93.75 ( 93.46)	Acc@5 100.00 ( 99.50)
Epoch: [83][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6931e-01 (2.0352e-01)	Acc@1  93.75 ( 93.50)	Acc@5 100.00 ( 99.51)
Epoch: [83][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2485e-01 (2.0442e-01)	Acc@1  94.53 ( 93.50)	Acc@5  98.44 ( 99.52)
Epoch: [83][130/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2156e-01 (2.0378e-01)	Acc@1  93.75 ( 93.56)	Acc@5  99.22 ( 99.52)
Epoch: [83][140/391]	Time  0.043 ( 0.026)	Data  0.003 ( 0.003)	Loss 1.3550e-01 (2.0451e-01)	Acc@1  96.09 ( 93.51)	Acc@5 100.00 ( 99.53)
Epoch: [83][150/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8542e-01 (2.0362e-01)	Acc@1  95.31 ( 93.53)	Acc@5 100.00 ( 99.56)
Epoch: [83][160/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6016e-01 (2.0199e-01)	Acc@1  94.53 ( 93.58)	Acc@5 100.00 ( 99.56)
Epoch: [83][170/391]	Time  0.037 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6724e-01 (1.9992e-01)	Acc@1  95.31 ( 93.64)	Acc@5 100.00 ( 99.58)
Epoch: [83][180/391]	Time  0.026 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.6223e-01 (1.9964e-01)	Acc@1  94.53 ( 93.65)	Acc@5 100.00 ( 99.58)
Epoch: [83][190/391]	Time  0.039 ( 0.026)	Data  0.005 ( 0.003)	Loss 2.3438e-01 (1.9889e-01)	Acc@1  92.19 ( 93.64)	Acc@5  99.22 ( 99.59)
Epoch: [83][200/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.6541e-01 (1.9905e-01)	Acc@1  95.31 ( 93.63)	Acc@5 100.00 ( 99.58)
Epoch: [83][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7639e-01 (2.0057e-01)	Acc@1  95.31 ( 93.59)	Acc@5 100.00 ( 99.58)
Epoch: [83][220/391]	Time  0.038 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.0728e-01 (1.9996e-01)	Acc@1  92.19 ( 93.59)	Acc@5  99.22 ( 99.59)
Epoch: [83][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7566e-01 (1.9970e-01)	Acc@1  95.31 ( 93.61)	Acc@5 100.00 ( 99.58)
Epoch: [83][240/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7383e-01 (2.0004e-01)	Acc@1  92.97 ( 93.60)	Acc@5 100.00 ( 99.59)
Epoch: [83][250/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.7466e-01 (2.0024e-01)	Acc@1  89.84 ( 93.58)	Acc@5 100.00 ( 99.59)
Epoch: [83][260/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.5515e-01 (2.0063e-01)	Acc@1  95.31 ( 93.58)	Acc@5  99.22 ( 99.59)
Epoch: [83][270/391]	Time  0.041 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1096e-01 (1.9986e-01)	Acc@1  95.31 ( 93.60)	Acc@5  99.22 ( 99.58)
Epoch: [83][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9922e-01 (2.0034e-01)	Acc@1  95.31 ( 93.61)	Acc@5  99.22 ( 99.59)
Epoch: [83][290/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9971e-01 (2.0178e-01)	Acc@1  94.53 ( 93.59)	Acc@5  99.22 ( 99.57)
Epoch: [83][300/391]	Time  0.023 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.2095e-01 (2.0292e-01)	Acc@1  91.41 ( 93.56)	Acc@5 100.00 ( 99.57)
Epoch: [83][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7761e-01 (2.0310e-01)	Acc@1  96.09 ( 93.60)	Acc@5  99.22 ( 99.56)
Epoch: [83][320/391]	Time  0.031 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7163e-01 (2.0299e-01)	Acc@1  92.19 ( 93.58)	Acc@5 100.00 ( 99.57)
Epoch: [83][330/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8726e-01 (2.0273e-01)	Acc@1  94.53 ( 93.59)	Acc@5 100.00 ( 99.57)
Epoch: [83][340/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1021e-01 (2.0274e-01)	Acc@1  89.84 ( 93.59)	Acc@5 100.00 ( 99.57)
Epoch: [83][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8955e-01 (2.0283e-01)	Acc@1  91.41 ( 93.60)	Acc@5  99.22 ( 99.57)
Epoch: [83][360/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1957e-01 (2.0259e-01)	Acc@1  97.66 ( 93.60)	Acc@5 100.00 ( 99.57)
Epoch: [83][370/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8079e-01 (2.0310e-01)	Acc@1  94.53 ( 93.58)	Acc@5 100.00 ( 99.57)
Epoch: [83][380/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6562e-01 (2.0329e-01)	Acc@1  92.97 ( 93.58)	Acc@5  99.22 ( 99.56)
Epoch: [83][390/391]	Time  0.018 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.7109e-01 (2.0381e-01)	Acc@1  86.25 ( 93.56)	Acc@5  98.75 ( 99.56)
## e[83] optimizer.zero_grad (sum) time: 0.10500216484069824
## e[83]       loss.backward (sum) time: 2.3001723289489746
## e[83]      optimizer.step (sum) time: 0.9340033531188965
## epoch[83] training(only) time: 10.090160369873047
# Switched to evaluate mode...
Test: [  0/100]	Time  0.152 ( 0.152)	Loss 1.3232e+00 (1.3232e+00)	Acc@1  72.00 ( 72.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.017 ( 0.027)	Loss 1.4980e+00 (1.5897e+00)	Acc@1  63.00 ( 67.73)	Acc@5  90.00 ( 88.36)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.7139e+00 (1.5897e+00)	Acc@1  67.00 ( 67.19)	Acc@5  91.00 ( 88.14)
Test: [ 30/100]	Time  0.024 ( 0.018)	Loss 1.5068e+00 (1.5886e+00)	Acc@1  65.00 ( 67.06)	Acc@5  88.00 ( 88.03)
Test: [ 40/100]	Time  0.011 ( 0.017)	Loss 1.7139e+00 (1.5859e+00)	Acc@1  66.00 ( 67.15)	Acc@5  91.00 ( 88.32)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6260e+00 (1.6145e+00)	Acc@1  67.00 ( 66.90)	Acc@5  89.00 ( 88.16)
Test: [ 60/100]	Time  0.013 ( 0.016)	Loss 1.8027e+00 (1.5902e+00)	Acc@1  62.00 ( 67.10)	Acc@5  92.00 ( 88.51)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.5840e+00 (1.5981e+00)	Acc@1  66.00 ( 67.14)	Acc@5  90.00 ( 88.59)
Test: [ 80/100]	Time  0.016 ( 0.015)	Loss 1.5127e+00 (1.5997e+00)	Acc@1  67.00 ( 67.04)	Acc@5  89.00 ( 88.62)
Test: [ 90/100]	Time  0.010 ( 0.015)	Loss 1.9600e+00 (1.5863e+00)	Acc@1  61.00 ( 67.31)	Acc@5  88.00 ( 88.85)
 * Acc@1 67.380 Acc@5 88.820
### epoch[83] execution time: 11.672312259674072
EPOCH 84
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [84][  0/391]	Time  0.171 ( 0.171)	Data  0.146 ( 0.146)	Loss 1.5771e-01 (1.5771e-01)	Acc@1  93.75 ( 93.75)	Acc@5 100.00 (100.00)
Epoch: [84][ 10/391]	Time  0.022 ( 0.038)	Data  0.001 ( 0.015)	Loss 3.1714e-01 (2.1372e-01)	Acc@1  91.41 ( 93.68)	Acc@5 100.00 ( 99.50)
Epoch: [84][ 20/391]	Time  0.023 ( 0.032)	Data  0.002 ( 0.009)	Loss 3.1787e-01 (2.1668e-01)	Acc@1  91.41 ( 93.04)	Acc@5 100.00 ( 99.67)
Epoch: [84][ 30/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.007)	Loss 1.1420e-01 (2.0855e-01)	Acc@1  96.09 ( 93.30)	Acc@5 100.00 ( 99.60)
Epoch: [84][ 40/391]	Time  0.020 ( 0.028)	Data  0.002 ( 0.005)	Loss 3.1055e-01 (2.0056e-01)	Acc@1  89.84 ( 93.71)	Acc@5 100.00 ( 99.68)
Epoch: [84][ 50/391]	Time  0.021 ( 0.028)	Data  0.000 ( 0.005)	Loss 1.8262e-01 (1.9391e-01)	Acc@1  91.41 ( 93.83)	Acc@5 100.00 ( 99.69)
Epoch: [84][ 60/391]	Time  0.021 ( 0.027)	Data  0.000 ( 0.004)	Loss 8.8562e-02 (1.9274e-01)	Acc@1  97.66 ( 93.94)	Acc@5 100.00 ( 99.65)
Epoch: [84][ 70/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.3721e-01 (1.9080e-01)	Acc@1  95.31 ( 93.99)	Acc@5 100.00 ( 99.70)
Epoch: [84][ 80/391]	Time  0.034 ( 0.027)	Data  0.001 ( 0.004)	Loss 9.5703e-02 (1.9295e-01)	Acc@1  96.88 ( 93.92)	Acc@5 100.00 ( 99.66)
Epoch: [84][ 90/391]	Time  0.029 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.9324e-01 (1.9252e-01)	Acc@1  96.09 ( 93.89)	Acc@5  98.44 ( 99.66)
Epoch: [84][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2437e-01 (1.9355e-01)	Acc@1  92.19 ( 93.84)	Acc@5 100.00 ( 99.68)
Epoch: [84][110/391]	Time  0.032 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.9312e-01 (1.9431e-01)	Acc@1  91.41 ( 93.75)	Acc@5  99.22 ( 99.66)
Epoch: [84][120/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3367e-01 (1.9550e-01)	Acc@1  94.53 ( 93.74)	Acc@5 100.00 ( 99.67)
Epoch: [84][130/391]	Time  0.033 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.2388e-01 (1.9664e-01)	Acc@1  92.19 ( 93.64)	Acc@5  99.22 ( 99.67)
Epoch: [84][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0935e-01 (1.9737e-01)	Acc@1  92.19 ( 93.58)	Acc@5 100.00 ( 99.67)
Epoch: [84][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4575e-01 (1.9714e-01)	Acc@1  94.53 ( 93.60)	Acc@5 100.00 ( 99.65)
Epoch: [84][160/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3682e-01 (1.9786e-01)	Acc@1  94.53 ( 93.56)	Acc@5  99.22 ( 99.65)
Epoch: [84][170/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8481e-01 (1.9919e-01)	Acc@1  94.53 ( 93.53)	Acc@5 100.00 ( 99.63)
Epoch: [84][180/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7148e-01 (2.0044e-01)	Acc@1  92.19 ( 93.50)	Acc@5  99.22 ( 99.65)
Epoch: [84][190/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4243e-01 (2.0004e-01)	Acc@1  92.19 ( 93.53)	Acc@5  98.44 ( 99.64)
Epoch: [84][200/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8628e-01 (2.0055e-01)	Acc@1  91.41 ( 93.53)	Acc@5 100.00 ( 99.62)
Epoch: [84][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4893e-01 (2.0147e-01)	Acc@1  95.31 ( 93.54)	Acc@5 100.00 ( 99.63)
Epoch: [84][220/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.0455e-01 (2.0161e-01)	Acc@1  97.66 ( 93.57)	Acc@5 100.00 ( 99.63)
Epoch: [84][230/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3611e-01 (2.0042e-01)	Acc@1  95.31 ( 93.61)	Acc@5 100.00 ( 99.63)
Epoch: [84][240/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7236e-01 (2.0103e-01)	Acc@1  96.09 ( 93.60)	Acc@5 100.00 ( 99.63)
Epoch: [84][250/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.9751e-01 (2.0192e-01)	Acc@1  92.19 ( 93.58)	Acc@5 100.00 ( 99.64)
Epoch: [84][260/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.3635e-01 (2.0147e-01)	Acc@1  95.31 ( 93.60)	Acc@5 100.00 ( 99.63)
Epoch: [84][270/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 4.4946e-01 (2.0227e-01)	Acc@1  85.94 ( 93.58)	Acc@5  98.44 ( 99.63)
Epoch: [84][280/391]	Time  0.030 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6562e-01 (2.0212e-01)	Acc@1  91.41 ( 93.60)	Acc@5 100.00 ( 99.63)
Epoch: [84][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2360e-01 (2.0110e-01)	Acc@1  97.66 ( 93.65)	Acc@5 100.00 ( 99.63)
Epoch: [84][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5562e-01 (2.0135e-01)	Acc@1  89.06 ( 93.63)	Acc@5  99.22 ( 99.64)
Epoch: [84][310/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1082e-01 (2.0070e-01)	Acc@1  92.19 ( 93.66)	Acc@5 100.00 ( 99.63)
Epoch: [84][320/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.5181e-01 (2.0119e-01)	Acc@1  91.41 ( 93.64)	Acc@5  99.22 ( 99.63)
Epoch: [84][330/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.3611e-01 (2.0116e-01)	Acc@1  96.09 ( 93.65)	Acc@5 100.00 ( 99.63)
Epoch: [84][340/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9104e-01 (2.0179e-01)	Acc@1  93.75 ( 93.64)	Acc@5 100.00 ( 99.64)
Epoch: [84][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2195e-01 (2.0095e-01)	Acc@1  96.09 ( 93.68)	Acc@5 100.00 ( 99.63)
Epoch: [84][360/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0752e-01 (2.0128e-01)	Acc@1  92.97 ( 93.65)	Acc@5 100.00 ( 99.63)
Epoch: [84][370/391]	Time  0.040 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.8713e-01 (2.0116e-01)	Acc@1  92.97 ( 93.65)	Acc@5 100.00 ( 99.63)
Epoch: [84][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0227e-01 (2.0088e-01)	Acc@1  95.31 ( 93.68)	Acc@5  98.44 ( 99.62)
Epoch: [84][390/391]	Time  0.017 ( 0.026)	Data  0.000 ( 0.002)	Loss 3.0103e-01 (2.0096e-01)	Acc@1  87.50 ( 93.68)	Acc@5 100.00 ( 99.62)
## e[84] optimizer.zero_grad (sum) time: 0.1058957576751709
## e[84]       loss.backward (sum) time: 2.2238667011260986
## e[84]      optimizer.step (sum) time: 0.9300813674926758
## epoch[84] training(only) time: 10.110087394714355
# Switched to evaluate mode...
Test: [  0/100]	Time  0.139 ( 0.139)	Loss 1.3311e+00 (1.3311e+00)	Acc@1  73.00 ( 73.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.5068e+00 (1.5761e+00)	Acc@1  67.00 ( 67.55)	Acc@5  90.00 ( 87.91)
Test: [ 20/100]	Time  0.010 ( 0.020)	Loss 1.6826e+00 (1.5787e+00)	Acc@1  66.00 ( 67.05)	Acc@5  92.00 ( 88.05)
Test: [ 30/100]	Time  0.020 ( 0.018)	Loss 1.4912e+00 (1.5804e+00)	Acc@1  65.00 ( 66.81)	Acc@5  88.00 ( 88.06)
Test: [ 40/100]	Time  0.017 ( 0.018)	Loss 1.6973e+00 (1.5771e+00)	Acc@1  67.00 ( 66.95)	Acc@5  89.00 ( 88.29)
Test: [ 50/100]	Time  0.011 ( 0.017)	Loss 1.6260e+00 (1.6012e+00)	Acc@1  67.00 ( 66.78)	Acc@5  88.00 ( 88.12)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.7842e+00 (1.5782e+00)	Acc@1  62.00 ( 67.05)	Acc@5  91.00 ( 88.48)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 1.5498e+00 (1.5843e+00)	Acc@1  66.00 ( 67.08)	Acc@5  90.00 ( 88.54)
Test: [ 80/100]	Time  0.010 ( 0.016)	Loss 1.4678e+00 (1.5852e+00)	Acc@1  67.00 ( 67.02)	Acc@5  89.00 ( 88.53)
Test: [ 90/100]	Time  0.019 ( 0.016)	Loss 1.9404e+00 (1.5720e+00)	Acc@1  59.00 ( 67.30)	Acc@5  88.00 ( 88.76)
 * Acc@1 67.380 Acc@5 88.750
### epoch[84] execution time: 11.717400789260864
EPOCH 85
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [85][  0/391]	Time  0.171 ( 0.171)	Data  0.142 ( 0.142)	Loss 2.0203e-01 (2.0203e-01)	Acc@1  92.97 ( 92.97)	Acc@5 100.00 (100.00)
Epoch: [85][ 10/391]	Time  0.021 ( 0.039)	Data  0.001 ( 0.015)	Loss 2.9736e-01 (1.9727e-01)	Acc@1  89.06 ( 93.11)	Acc@5  99.22 ( 99.86)
Epoch: [85][ 20/391]	Time  0.028 ( 0.033)	Data  0.001 ( 0.008)	Loss 1.8506e-01 (2.1263e-01)	Acc@1  92.97 ( 92.60)	Acc@5 100.00 ( 99.59)
Epoch: [85][ 30/391]	Time  0.039 ( 0.030)	Data  0.001 ( 0.006)	Loss 3.1836e-01 (2.0982e-01)	Acc@1  91.41 ( 93.17)	Acc@5  99.22 ( 99.57)
Epoch: [85][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.4146e-01 (2.0761e-01)	Acc@1  92.19 ( 93.27)	Acc@5 100.00 ( 99.58)
Epoch: [85][ 50/391]	Time  0.023 ( 0.028)	Data  0.001 ( 0.005)	Loss 1.6992e-01 (2.0056e-01)	Acc@1  94.53 ( 93.52)	Acc@5  99.22 ( 99.63)
Epoch: [85][ 60/391]	Time  0.029 ( 0.028)	Data  0.001 ( 0.004)	Loss 1.7651e-01 (1.9907e-01)	Acc@1  92.19 ( 93.51)	Acc@5 100.00 ( 99.64)
Epoch: [85][ 70/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.1277e-01 (1.9565e-01)	Acc@1  94.53 ( 93.62)	Acc@5  99.22 ( 99.65)
Epoch: [85][ 80/391]	Time  0.019 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.4902e-01 (1.9364e-01)	Acc@1  92.97 ( 93.72)	Acc@5  98.44 ( 99.63)
Epoch: [85][ 90/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2327e-01 (1.9512e-01)	Acc@1  92.19 ( 93.65)	Acc@5  99.22 ( 99.60)
Epoch: [85][100/391]	Time  0.024 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2009e-01 (1.9560e-01)	Acc@1  91.41 ( 93.66)	Acc@5  99.22 ( 99.59)
Epoch: [85][110/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0825e-01 (1.9653e-01)	Acc@1  92.97 ( 93.63)	Acc@5 100.00 ( 99.61)
Epoch: [85][120/391]	Time  0.027 ( 0.027)	Data  0.002 ( 0.003)	Loss 2.2791e-01 (1.9655e-01)	Acc@1  92.97 ( 93.66)	Acc@5  99.22 ( 99.59)
Epoch: [85][130/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.7114e-01 (1.9700e-01)	Acc@1  92.97 ( 93.66)	Acc@5 100.00 ( 99.59)
Epoch: [85][140/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0959e-01 (1.9609e-01)	Acc@1  92.97 ( 93.68)	Acc@5  98.44 ( 99.58)
Epoch: [85][150/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.4722e-01 (1.9549e-01)	Acc@1  94.53 ( 93.69)	Acc@5 100.00 ( 99.57)
Epoch: [85][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8164e-01 (1.9649e-01)	Acc@1  94.53 ( 93.67)	Acc@5  99.22 ( 99.55)
Epoch: [85][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1423e-01 (1.9741e-01)	Acc@1  92.19 ( 93.60)	Acc@5 100.00 ( 99.56)
Epoch: [85][180/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9043e-01 (1.9678e-01)	Acc@1  93.75 ( 93.58)	Acc@5 100.00 ( 99.57)
Epoch: [85][190/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7651e-01 (1.9670e-01)	Acc@1  93.75 ( 93.55)	Acc@5 100.00 ( 99.58)
Epoch: [85][200/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7712e-01 (1.9666e-01)	Acc@1  96.88 ( 93.61)	Acc@5  99.22 ( 99.57)
Epoch: [85][210/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2473e-01 (1.9672e-01)	Acc@1  90.62 ( 93.64)	Acc@5 100.00 ( 99.56)
Epoch: [85][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6211e-01 (1.9761e-01)	Acc@1  94.53 ( 93.62)	Acc@5 100.00 ( 99.57)
Epoch: [85][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6538e-01 (1.9787e-01)	Acc@1  92.19 ( 93.61)	Acc@5  99.22 ( 99.56)
Epoch: [85][240/391]	Time  0.033 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.8457e-01 (1.9772e-01)	Acc@1  94.53 ( 93.63)	Acc@5  98.44 ( 99.56)
Epoch: [85][250/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.2720e-01 (1.9815e-01)	Acc@1  95.31 ( 93.61)	Acc@5 100.00 ( 99.56)
Epoch: [85][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8384e-01 (1.9801e-01)	Acc@1  90.62 ( 93.57)	Acc@5 100.00 ( 99.57)
Epoch: [85][270/391]	Time  0.037 ( 0.026)	Data  0.003 ( 0.002)	Loss 1.9141e-01 (1.9912e-01)	Acc@1  94.53 ( 93.54)	Acc@5 100.00 ( 99.56)
Epoch: [85][280/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0508e-01 (1.9760e-01)	Acc@1  93.75 ( 93.61)	Acc@5  99.22 ( 99.57)
Epoch: [85][290/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 3.0811e-01 (1.9703e-01)	Acc@1  93.75 ( 93.63)	Acc@5  99.22 ( 99.57)
Epoch: [85][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3779e-01 (1.9715e-01)	Acc@1  92.19 ( 93.63)	Acc@5 100.00 ( 99.56)
Epoch: [85][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4243e-01 (1.9695e-01)	Acc@1  90.62 ( 93.66)	Acc@5  99.22 ( 99.56)
Epoch: [85][320/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.1664e-01 (1.9745e-01)	Acc@1  96.09 ( 93.65)	Acc@5 100.00 ( 99.56)
Epoch: [85][330/391]	Time  0.022 ( 0.026)	Data  0.004 ( 0.002)	Loss 2.1106e-01 (1.9713e-01)	Acc@1  92.19 ( 93.68)	Acc@5  99.22 ( 99.56)
Epoch: [85][340/391]	Time  0.041 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.6733e-01 (1.9761e-01)	Acc@1  91.41 ( 93.67)	Acc@5 100.00 ( 99.56)
Epoch: [85][350/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9836e-01 (1.9886e-01)	Acc@1  96.09 ( 93.64)	Acc@5  99.22 ( 99.54)
Epoch: [85][360/391]	Time  0.035 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.1810e-01 (1.9914e-01)	Acc@1  96.88 ( 93.62)	Acc@5 100.00 ( 99.55)
Epoch: [85][370/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.0933e-01 (1.9985e-01)	Acc@1  89.06 ( 93.59)	Acc@5  99.22 ( 99.55)
Epoch: [85][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4795e-01 (1.9921e-01)	Acc@1  96.09 ( 93.61)	Acc@5 100.00 ( 99.55)
Epoch: [85][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4050e-01 (1.9930e-01)	Acc@1  93.75 ( 93.60)	Acc@5 100.00 ( 99.55)
## e[85] optimizer.zero_grad (sum) time: 0.10561347007751465
## e[85]       loss.backward (sum) time: 2.2524797916412354
## e[85]      optimizer.step (sum) time: 0.9263696670532227
## epoch[85] training(only) time: 10.12868046760559
# Switched to evaluate mode...
Test: [  0/100]	Time  0.144 ( 0.144)	Loss 1.2969e+00 (1.2969e+00)	Acc@1  73.00 ( 73.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.5107e+00 (1.5892e+00)	Acc@1  63.00 ( 67.18)	Acc@5  89.00 ( 87.82)
Test: [ 20/100]	Time  0.012 ( 0.020)	Loss 1.7305e+00 (1.5953e+00)	Acc@1  66.00 ( 66.71)	Acc@5  91.00 ( 87.81)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.5215e+00 (1.5913e+00)	Acc@1  65.00 ( 66.65)	Acc@5  88.00 ( 87.87)
Test: [ 40/100]	Time  0.028 ( 0.017)	Loss 1.7119e+00 (1.5877e+00)	Acc@1  67.00 ( 66.76)	Acc@5  91.00 ( 88.27)
Test: [ 50/100]	Time  0.010 ( 0.017)	Loss 1.6328e+00 (1.6142e+00)	Acc@1  67.00 ( 66.61)	Acc@5  89.00 ( 88.04)
Test: [ 60/100]	Time  0.009 ( 0.016)	Loss 1.7979e+00 (1.5898e+00)	Acc@1  61.00 ( 66.82)	Acc@5  92.00 ( 88.48)
Test: [ 70/100]	Time  0.024 ( 0.016)	Loss 1.5898e+00 (1.5958e+00)	Acc@1  65.00 ( 66.97)	Acc@5  91.00 ( 88.52)
Test: [ 80/100]	Time  0.011 ( 0.016)	Loss 1.5049e+00 (1.5964e+00)	Acc@1  67.00 ( 66.95)	Acc@5  88.00 ( 88.48)
Test: [ 90/100]	Time  0.012 ( 0.016)	Loss 1.9541e+00 (1.5829e+00)	Acc@1  60.00 ( 67.23)	Acc@5  88.00 ( 88.70)
 * Acc@1 67.280 Acc@5 88.710
### epoch[85] execution time: 11.758042812347412
EPOCH 86
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [86][  0/391]	Time  0.173 ( 0.173)	Data  0.148 ( 0.148)	Loss 2.0874e-01 (2.0874e-01)	Acc@1  94.53 ( 94.53)	Acc@5  98.44 ( 98.44)
Epoch: [86][ 10/391]	Time  0.025 ( 0.039)	Data  0.001 ( 0.015)	Loss 2.3206e-01 (2.0553e-01)	Acc@1  90.62 ( 93.47)	Acc@5 100.00 ( 99.72)
Epoch: [86][ 20/391]	Time  0.020 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.3538e-01 (1.8851e-01)	Acc@1  94.53 ( 94.08)	Acc@5 100.00 ( 99.63)
Epoch: [86][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.1460e-01 (2.0578e-01)	Acc@1  89.84 ( 93.47)	Acc@5 100.00 ( 99.52)
Epoch: [86][ 40/391]	Time  0.020 ( 0.029)	Data  0.001 ( 0.006)	Loss 1.0962e-01 (2.0276e-01)	Acc@1  97.66 ( 93.50)	Acc@5 100.00 ( 99.54)
Epoch: [86][ 50/391]	Time  0.023 ( 0.028)	Data  0.002 ( 0.005)	Loss 2.5903e-01 (2.0265e-01)	Acc@1  90.62 ( 93.50)	Acc@5  99.22 ( 99.53)
Epoch: [86][ 60/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.1875e-01 (1.9921e-01)	Acc@1  91.41 ( 93.58)	Acc@5 100.00 ( 99.58)
Epoch: [86][ 70/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.7834e-01 (2.0173e-01)	Acc@1  93.75 ( 93.55)	Acc@5 100.00 ( 99.50)
Epoch: [86][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 2.7539e-01 (2.0310e-01)	Acc@1  89.84 ( 93.52)	Acc@5  99.22 ( 99.51)
Epoch: [86][ 90/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.4744e-01 (2.0547e-01)	Acc@1  89.84 ( 93.43)	Acc@5 100.00 ( 99.52)
Epoch: [86][100/391]	Time  0.024 ( 0.027)	Data  0.002 ( 0.003)	Loss 7.0374e-02 (2.0399e-01)	Acc@1  98.44 ( 93.51)	Acc@5 100.00 ( 99.51)
Epoch: [86][110/391]	Time  0.033 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0691e-01 (2.0297e-01)	Acc@1  91.41 ( 93.50)	Acc@5 100.00 ( 99.50)
Epoch: [86][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5051e-01 (2.0279e-01)	Acc@1  95.31 ( 93.45)	Acc@5 100.00 ( 99.52)
Epoch: [86][130/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5369e-01 (2.0326e-01)	Acc@1  96.09 ( 93.45)	Acc@5  99.22 ( 99.52)
Epoch: [86][140/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3025e-01 (2.0142e-01)	Acc@1  96.88 ( 93.50)	Acc@5 100.00 ( 99.55)
Epoch: [86][150/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1082e-01 (2.0126e-01)	Acc@1  92.19 ( 93.50)	Acc@5  98.44 ( 99.53)
Epoch: [86][160/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.7661e-01 (2.0176e-01)	Acc@1  93.75 ( 93.53)	Acc@5  97.66 ( 99.52)
Epoch: [86][170/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 7.9529e-02 (1.9996e-01)	Acc@1  98.44 ( 93.61)	Acc@5 100.00 ( 99.53)
Epoch: [86][180/391]	Time  0.044 ( 0.026)	Data  0.009 ( 0.003)	Loss 2.7148e-01 (1.9971e-01)	Acc@1  90.62 ( 93.58)	Acc@5 100.00 ( 99.55)
Epoch: [86][190/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.9834e-01 (2.0106e-01)	Acc@1  91.41 ( 93.53)	Acc@5  98.44 ( 99.55)
Epoch: [86][200/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.0103e-01 (2.0151e-01)	Acc@1  90.62 ( 93.54)	Acc@5  98.44 ( 99.54)
Epoch: [86][210/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1753e-01 (2.0172e-01)	Acc@1  90.62 ( 93.52)	Acc@5 100.00 ( 99.54)
Epoch: [86][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1143e-01 (2.0204e-01)	Acc@1  93.75 ( 93.53)	Acc@5 100.00 ( 99.54)
Epoch: [86][230/391]	Time  0.048 ( 0.026)	Data  0.010 ( 0.003)	Loss 1.9910e-01 (2.0090e-01)	Acc@1  91.41 ( 93.55)	Acc@5 100.00 ( 99.56)
Epoch: [86][240/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7627e-01 (2.0164e-01)	Acc@1  92.19 ( 93.51)	Acc@5 100.00 ( 99.55)
Epoch: [86][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1106e-01 (2.0247e-01)	Acc@1  93.75 ( 93.50)	Acc@5  98.44 ( 99.54)
Epoch: [86][260/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5039e-01 (2.0319e-01)	Acc@1  98.44 ( 93.48)	Acc@5  99.22 ( 99.54)
Epoch: [86][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.2227e-01 (2.0390e-01)	Acc@1  91.41 ( 93.47)	Acc@5  98.44 ( 99.53)
Epoch: [86][280/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0398e-01 (2.0332e-01)	Acc@1  93.75 ( 93.51)	Acc@5 100.00 ( 99.54)
Epoch: [86][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4868e-01 (2.0325e-01)	Acc@1  95.31 ( 93.53)	Acc@5 100.00 ( 99.54)
Epoch: [86][300/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8591e-01 (2.0376e-01)	Acc@1  95.31 ( 93.51)	Acc@5  99.22 ( 99.54)
Epoch: [86][310/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.9507e-01 (2.0361e-01)	Acc@1  92.97 ( 93.49)	Acc@5  99.22 ( 99.54)
Epoch: [86][320/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4927e-01 (2.0429e-01)	Acc@1  90.62 ( 93.47)	Acc@5 100.00 ( 99.54)
Epoch: [86][330/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.4563e-01 (2.0353e-01)	Acc@1  93.75 ( 93.49)	Acc@5 100.00 ( 99.54)
Epoch: [86][340/391]	Time  0.043 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6553e-01 (2.0388e-01)	Acc@1  94.53 ( 93.48)	Acc@5 100.00 ( 99.55)
Epoch: [86][350/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8604e-01 (2.0397e-01)	Acc@1  92.97 ( 93.48)	Acc@5 100.00 ( 99.55)
Epoch: [86][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.0645e-01 (2.0330e-01)	Acc@1  97.66 ( 93.49)	Acc@5 100.00 ( 99.55)
Epoch: [86][370/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.1542e-01 (2.0274e-01)	Acc@1  95.31 ( 93.50)	Acc@5  99.22 ( 99.55)
Epoch: [86][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7004e-01 (2.0221e-01)	Acc@1  95.31 ( 93.52)	Acc@5  99.22 ( 99.56)
Epoch: [86][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.2839e-01 (2.0220e-01)	Acc@1  91.25 ( 93.52)	Acc@5 100.00 ( 99.56)
## e[86] optimizer.zero_grad (sum) time: 0.10544371604919434
## e[86]       loss.backward (sum) time: 2.310312509536743
## e[86]      optimizer.step (sum) time: 0.9267673492431641
## epoch[86] training(only) time: 10.107234716415405
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.3115e+00 (1.3115e+00)	Acc@1  73.00 ( 73.00)	Acc@5  89.00 ( 89.00)
Test: [ 10/100]	Time  0.010 ( 0.026)	Loss 1.5010e+00 (1.5837e+00)	Acc@1  64.00 ( 67.73)	Acc@5  90.00 ( 88.36)
Test: [ 20/100]	Time  0.016 ( 0.021)	Loss 1.7178e+00 (1.5849e+00)	Acc@1  67.00 ( 67.24)	Acc@5  90.00 ( 88.00)
Test: [ 30/100]	Time  0.012 ( 0.018)	Loss 1.5254e+00 (1.5845e+00)	Acc@1  66.00 ( 67.06)	Acc@5  88.00 ( 87.84)
Test: [ 40/100]	Time  0.015 ( 0.017)	Loss 1.6904e+00 (1.5814e+00)	Acc@1  67.00 ( 67.15)	Acc@5  90.00 ( 88.00)
Test: [ 50/100]	Time  0.024 ( 0.016)	Loss 1.6250e+00 (1.6115e+00)	Acc@1  67.00 ( 66.82)	Acc@5  89.00 ( 87.90)
Test: [ 60/100]	Time  0.010 ( 0.016)	Loss 1.7900e+00 (1.5865e+00)	Acc@1  62.00 ( 67.03)	Acc@5  92.00 ( 88.25)
Test: [ 70/100]	Time  0.010 ( 0.016)	Loss 1.5381e+00 (1.5916e+00)	Acc@1  66.00 ( 67.17)	Acc@5  91.00 ( 88.34)
Test: [ 80/100]	Time  0.009 ( 0.015)	Loss 1.5117e+00 (1.5938e+00)	Acc@1  66.00 ( 67.10)	Acc@5  89.00 ( 88.36)
Test: [ 90/100]	Time  0.012 ( 0.015)	Loss 1.9473e+00 (1.5797e+00)	Acc@1  60.00 ( 67.36)	Acc@5  88.00 ( 88.59)
 * Acc@1 67.470 Acc@5 88.550
### epoch[86] execution time: 11.694212198257446
EPOCH 87
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [87][  0/391]	Time  0.188 ( 0.188)	Data  0.160 ( 0.160)	Loss 1.3806e-01 (1.3806e-01)	Acc@1  95.31 ( 95.31)	Acc@5  99.22 ( 99.22)
Epoch: [87][ 10/391]	Time  0.029 ( 0.040)	Data  0.001 ( 0.016)	Loss 1.7993e-01 (1.8353e-01)	Acc@1  94.53 ( 93.96)	Acc@5 100.00 ( 99.72)
Epoch: [87][ 20/391]	Time  0.029 ( 0.033)	Data  0.001 ( 0.009)	Loss 1.9177e-01 (1.9476e-01)	Acc@1  95.31 ( 93.97)	Acc@5 100.00 ( 99.59)
Epoch: [87][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 1.3269e-01 (1.9347e-01)	Acc@1  96.09 ( 93.90)	Acc@5 100.00 ( 99.62)
Epoch: [87][ 40/391]	Time  0.021 ( 0.029)	Data  0.001 ( 0.006)	Loss 2.1460e-01 (1.9007e-01)	Acc@1  92.19 ( 94.00)	Acc@5 100.00 ( 99.66)
Epoch: [87][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.4561e-01 (1.9132e-01)	Acc@1  92.97 ( 94.03)	Acc@5  99.22 ( 99.62)
Epoch: [87][ 60/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.004)	Loss 2.9614e-01 (1.9325e-01)	Acc@1  88.28 ( 93.83)	Acc@5  98.44 ( 99.60)
Epoch: [87][ 70/391]	Time  0.031 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4722e-01 (1.9456e-01)	Acc@1  93.75 ( 93.87)	Acc@5 100.00 ( 99.59)
Epoch: [87][ 80/391]	Time  0.029 ( 0.027)	Data  0.000 ( 0.004)	Loss 1.9800e-01 (1.9428e-01)	Acc@1  91.41 ( 93.88)	Acc@5 100.00 ( 99.58)
Epoch: [87][ 90/391]	Time  0.025 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.4844e-01 (1.9436e-01)	Acc@1  96.88 ( 93.84)	Acc@5 100.00 ( 99.57)
Epoch: [87][100/391]	Time  0.033 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.6406e-01 (1.9284e-01)	Acc@1  95.31 ( 93.94)	Acc@5 100.00 ( 99.57)
Epoch: [87][110/391]	Time  0.030 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.2976e-01 (1.9277e-01)	Acc@1  95.31 ( 93.94)	Acc@5 100.00 ( 99.60)
Epoch: [87][120/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6882e-01 (1.9138e-01)	Acc@1  94.53 ( 94.00)	Acc@5  99.22 ( 99.59)
Epoch: [87][130/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3145e-01 (1.9097e-01)	Acc@1  92.97 ( 94.02)	Acc@5  98.44 ( 99.58)
Epoch: [87][140/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.5830e-01 (1.9102e-01)	Acc@1  92.97 ( 93.97)	Acc@5  99.22 ( 99.59)
Epoch: [87][150/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 3.3325e-01 (1.9116e-01)	Acc@1  89.06 ( 93.96)	Acc@5 100.00 ( 99.60)
Epoch: [87][160/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5967e-01 (1.9117e-01)	Acc@1  96.09 ( 93.96)	Acc@5  99.22 ( 99.60)
Epoch: [87][170/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3572e-01 (1.9193e-01)	Acc@1  92.19 ( 93.94)	Acc@5 100.00 ( 99.60)
Epoch: [87][180/391]	Time  0.022 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.7004e-01 (1.9175e-01)	Acc@1  95.31 ( 93.96)	Acc@5  99.22 ( 99.58)
Epoch: [87][190/391]	Time  0.034 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.3035e-01 (1.9218e-01)	Acc@1  92.97 ( 93.97)	Acc@5  99.22 ( 99.57)
Epoch: [87][200/391]	Time  0.029 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3364e-01 (1.9233e-01)	Acc@1  91.41 ( 93.96)	Acc@5  99.22 ( 99.57)
Epoch: [87][210/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2302e-01 (1.9256e-01)	Acc@1  91.41 ( 93.97)	Acc@5 100.00 ( 99.57)
Epoch: [87][220/391]	Time  0.032 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7847e-01 (1.9256e-01)	Acc@1  95.31 ( 93.97)	Acc@5 100.00 ( 99.57)
Epoch: [87][230/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2473e-01 (1.9380e-01)	Acc@1  92.19 ( 93.93)	Acc@5 100.00 ( 99.57)
Epoch: [87][240/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1899e-01 (1.9340e-01)	Acc@1  92.19 ( 93.93)	Acc@5 100.00 ( 99.58)
Epoch: [87][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8164e-01 (1.9399e-01)	Acc@1  94.53 ( 93.93)	Acc@5 100.00 ( 99.56)
Epoch: [87][260/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.7969e-01 (1.9452e-01)	Acc@1  96.88 ( 93.96)	Acc@5 100.00 ( 99.55)
Epoch: [87][270/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.3120e-01 (1.9510e-01)	Acc@1  92.19 ( 93.94)	Acc@5  99.22 ( 99.55)
Epoch: [87][280/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 2.7319e-01 (1.9558e-01)	Acc@1  92.97 ( 93.93)	Acc@5  99.22 ( 99.56)
Epoch: [87][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.1643e-01 (1.9651e-01)	Acc@1  92.97 ( 93.91)	Acc@5 100.00 ( 99.55)
Epoch: [87][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6797e-01 (1.9695e-01)	Acc@1  93.75 ( 93.88)	Acc@5 100.00 ( 99.54)
Epoch: [87][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6455e-01 (1.9702e-01)	Acc@1  94.53 ( 93.87)	Acc@5 100.00 ( 99.55)
Epoch: [87][320/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.8564e-01 (1.9804e-01)	Acc@1  90.62 ( 93.85)	Acc@5  99.22 ( 99.53)
Epoch: [87][330/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.4438e-01 (1.9821e-01)	Acc@1  90.62 ( 93.81)	Acc@5 100.00 ( 99.53)
Epoch: [87][340/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6382e-01 (1.9871e-01)	Acc@1  93.75 ( 93.80)	Acc@5 100.00 ( 99.53)
Epoch: [87][350/391]	Time  0.030 ( 0.026)	Data  0.009 ( 0.002)	Loss 2.5464e-01 (1.9874e-01)	Acc@1  92.97 ( 93.81)	Acc@5  98.44 ( 99.54)
Epoch: [87][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.6074e-01 (1.9903e-01)	Acc@1  88.28 ( 93.79)	Acc@5 100.00 ( 99.54)
Epoch: [87][370/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3425e-01 (1.9958e-01)	Acc@1  93.75 ( 93.78)	Acc@5 100.00 ( 99.54)
Epoch: [87][380/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5100e-01 (1.9925e-01)	Acc@1  96.88 ( 93.78)	Acc@5 100.00 ( 99.54)
Epoch: [87][390/391]	Time  0.017 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5952e-01 (1.9910e-01)	Acc@1  91.25 ( 93.80)	Acc@5 100.00 ( 99.54)
## e[87] optimizer.zero_grad (sum) time: 0.10584855079650879
## e[87]       loss.backward (sum) time: 2.271096706390381
## e[87]      optimizer.step (sum) time: 0.9300363063812256
## epoch[87] training(only) time: 10.17310357093811
# Switched to evaluate mode...
Test: [  0/100]	Time  0.141 ( 0.141)	Loss 1.3027e+00 (1.3027e+00)	Acc@1  74.00 ( 74.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.013 ( 0.025)	Loss 1.4688e+00 (1.5884e+00)	Acc@1  66.00 ( 67.45)	Acc@5  89.00 ( 87.64)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 1.7051e+00 (1.5906e+00)	Acc@1  65.00 ( 67.10)	Acc@5  90.00 ( 87.67)
Test: [ 30/100]	Time  0.010 ( 0.017)	Loss 1.5459e+00 (1.5923e+00)	Acc@1  64.00 ( 66.77)	Acc@5  88.00 ( 87.84)
Test: [ 40/100]	Time  0.030 ( 0.017)	Loss 1.7178e+00 (1.5880e+00)	Acc@1  67.00 ( 67.00)	Acc@5  90.00 ( 88.20)
Test: [ 50/100]	Time  0.010 ( 0.016)	Loss 1.6055e+00 (1.6143e+00)	Acc@1  67.00 ( 66.84)	Acc@5  89.00 ( 88.08)
Test: [ 60/100]	Time  0.016 ( 0.016)	Loss 1.7979e+00 (1.5888e+00)	Acc@1  62.00 ( 67.05)	Acc@5  91.00 ( 88.48)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 1.5635e+00 (1.5941e+00)	Acc@1  66.00 ( 67.15)	Acc@5  91.00 ( 88.51)
Test: [ 80/100]	Time  0.010 ( 0.015)	Loss 1.4844e+00 (1.5946e+00)	Acc@1  68.00 ( 67.11)	Acc@5  88.00 ( 88.51)
Test: [ 90/100]	Time  0.018 ( 0.015)	Loss 1.9629e+00 (1.5806e+00)	Acc@1  60.00 ( 67.40)	Acc@5  88.00 ( 88.75)
 * Acc@1 67.480 Acc@5 88.740
### epoch[87] execution time: 11.779700756072998
EPOCH 88
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [88][  0/391]	Time  0.176 ( 0.176)	Data  0.146 ( 0.146)	Loss 1.3672e-01 (1.3672e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [88][ 10/391]	Time  0.034 ( 0.039)	Data  0.002 ( 0.015)	Loss 1.4368e-01 (1.6002e-01)	Acc@1  95.31 ( 94.53)	Acc@5 100.00 ( 99.93)
Epoch: [88][ 20/391]	Time  0.024 ( 0.032)	Data  0.001 ( 0.009)	Loss 1.3306e-01 (1.8718e-01)	Acc@1  96.88 ( 93.97)	Acc@5 100.00 ( 99.63)
Epoch: [88][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.006)	Loss 1.5845e-01 (1.8427e-01)	Acc@1  94.53 ( 94.03)	Acc@5 100.00 ( 99.72)
Epoch: [88][ 40/391]	Time  0.024 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.4585e-01 (1.8518e-01)	Acc@1  90.62 ( 94.05)	Acc@5  98.44 ( 99.64)
Epoch: [88][ 50/391]	Time  0.021 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.1057e-01 (1.9299e-01)	Acc@1  93.75 ( 93.72)	Acc@5 100.00 ( 99.63)
Epoch: [88][ 60/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 3.0835e-01 (1.9563e-01)	Acc@1  89.84 ( 93.61)	Acc@5  99.22 ( 99.62)
Epoch: [88][ 70/391]	Time  0.020 ( 0.027)	Data  0.002 ( 0.004)	Loss 1.3013e-01 (1.9674e-01)	Acc@1  96.09 ( 93.57)	Acc@5 100.00 ( 99.61)
Epoch: [88][ 80/391]	Time  0.035 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.5271e-01 (1.9641e-01)	Acc@1  94.53 ( 93.66)	Acc@5 100.00 ( 99.62)
Epoch: [88][ 90/391]	Time  0.036 ( 0.027)	Data  0.005 ( 0.004)	Loss 1.4612e-01 (1.9604e-01)	Acc@1  96.09 ( 93.72)	Acc@5 100.00 ( 99.60)
Epoch: [88][100/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.4001e-01 (1.9725e-01)	Acc@1  95.31 ( 93.66)	Acc@5 100.00 ( 99.58)
Epoch: [88][110/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1277e-01 (1.9856e-01)	Acc@1  93.75 ( 93.71)	Acc@5  99.22 ( 99.60)
Epoch: [88][120/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4036e-01 (1.9704e-01)	Acc@1  91.41 ( 93.69)	Acc@5 100.00 ( 99.61)
Epoch: [88][130/391]	Time  0.028 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6528e-01 (1.9616e-01)	Acc@1  94.53 ( 93.72)	Acc@5 100.00 ( 99.62)
Epoch: [88][140/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.6074e-01 (1.9712e-01)	Acc@1  92.19 ( 93.71)	Acc@5 100.00 ( 99.61)
Epoch: [88][150/391]	Time  0.021 ( 0.026)	Data  0.004 ( 0.003)	Loss 2.9858e-01 (1.9807e-01)	Acc@1  91.41 ( 93.70)	Acc@5  97.66 ( 99.59)
Epoch: [88][160/391]	Time  0.018 ( 0.026)	Data  0.002 ( 0.003)	Loss 2.5488e-01 (1.9764e-01)	Acc@1  92.19 ( 93.77)	Acc@5 100.00 ( 99.59)
Epoch: [88][170/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.7930e-01 (1.9754e-01)	Acc@1  91.41 ( 93.76)	Acc@5  98.44 ( 99.57)
Epoch: [88][180/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.4854e-01 (1.9944e-01)	Acc@1  92.19 ( 93.70)	Acc@5  99.22 ( 99.55)
Epoch: [88][190/391]	Time  0.037 ( 0.026)	Data  0.004 ( 0.003)	Loss 1.7700e-01 (1.9978e-01)	Acc@1  96.09 ( 93.71)	Acc@5 100.00 ( 99.55)
Epoch: [88][200/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.0801e-01 (2.0030e-01)	Acc@1  92.97 ( 93.69)	Acc@5 100.00 ( 99.55)
Epoch: [88][210/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9812e-01 (2.0051e-01)	Acc@1  92.97 ( 93.71)	Acc@5  99.22 ( 99.54)
Epoch: [88][220/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8223e-01 (2.0082e-01)	Acc@1  89.84 ( 93.69)	Acc@5  99.22 ( 99.54)
Epoch: [88][230/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.2290e-01 (2.0097e-01)	Acc@1  90.62 ( 93.69)	Acc@5  99.22 ( 99.54)
Epoch: [88][240/391]	Time  0.022 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.4966e-01 (2.0167e-01)	Acc@1  95.31 ( 93.68)	Acc@5 100.00 ( 99.54)
Epoch: [88][250/391]	Time  0.039 ( 0.026)	Data  0.003 ( 0.003)	Loss 2.4207e-01 (2.0192e-01)	Acc@1  91.41 ( 93.65)	Acc@5  98.44 ( 99.54)
Epoch: [88][260/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.9312e-01 (2.0110e-01)	Acc@1  93.75 ( 93.68)	Acc@5  98.44 ( 99.55)
Epoch: [88][270/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.3975e-01 (2.0050e-01)	Acc@1  90.62 ( 93.68)	Acc@5 100.00 ( 99.56)
Epoch: [88][280/391]	Time  0.036 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8738e-01 (2.0044e-01)	Acc@1  93.75 ( 93.71)	Acc@5 100.00 ( 99.56)
Epoch: [88][290/391]	Time  0.023 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7041e-01 (2.0092e-01)	Acc@1  96.09 ( 93.69)	Acc@5 100.00 ( 99.56)
Epoch: [88][300/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5381e-01 (1.9981e-01)	Acc@1  96.09 ( 93.71)	Acc@5  99.22 ( 99.57)
Epoch: [88][310/391]	Time  0.032 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.6089e-01 (1.9920e-01)	Acc@1  95.31 ( 93.73)	Acc@5 100.00 ( 99.57)
Epoch: [88][320/391]	Time  0.037 ( 0.026)	Data  0.004 ( 0.002)	Loss 2.5488e-01 (1.9970e-01)	Acc@1  92.97 ( 93.71)	Acc@5  99.22 ( 99.58)
Epoch: [88][330/391]	Time  0.030 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.0508e-01 (1.9958e-01)	Acc@1  93.75 ( 93.71)	Acc@5 100.00 ( 99.58)
Epoch: [88][340/391]	Time  0.035 ( 0.026)	Data  0.002 ( 0.002)	Loss 1.4893e-01 (1.9894e-01)	Acc@1  96.09 ( 93.72)	Acc@5 100.00 ( 99.59)
Epoch: [88][350/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0544e-01 (1.9965e-01)	Acc@1  92.97 ( 93.71)	Acc@5  99.22 ( 99.57)
Epoch: [88][360/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5759e-01 (1.9891e-01)	Acc@1  96.09 ( 93.75)	Acc@5 100.00 ( 99.58)
Epoch: [88][370/391]	Time  0.039 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7615e-01 (2.0001e-01)	Acc@1  94.53 ( 93.71)	Acc@5 100.00 ( 99.57)
Epoch: [88][380/391]	Time  0.024 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4294e-01 (1.9975e-01)	Acc@1  95.31 ( 93.71)	Acc@5 100.00 ( 99.58)
Epoch: [88][390/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.6223e-01 (1.9891e-01)	Acc@1  93.75 ( 93.73)	Acc@5 100.00 ( 99.59)
## e[88] optimizer.zero_grad (sum) time: 0.10554766654968262
## e[88]       loss.backward (sum) time: 2.2737438678741455
## e[88]      optimizer.step (sum) time: 0.9204771518707275
## epoch[88] training(only) time: 10.101350784301758
# Switched to evaluate mode...
Test: [  0/100]	Time  0.140 ( 0.140)	Loss 1.3105e+00 (1.3105e+00)	Acc@1  72.00 ( 72.00)	Acc@5  87.00 ( 87.00)
Test: [ 10/100]	Time  0.019 ( 0.027)	Loss 1.4883e+00 (1.5837e+00)	Acc@1  64.00 ( 67.45)	Acc@5  90.00 ( 88.18)
Test: [ 20/100]	Time  0.013 ( 0.020)	Loss 1.7305e+00 (1.5917e+00)	Acc@1  65.00 ( 66.95)	Acc@5  91.00 ( 88.29)
Test: [ 30/100]	Time  0.010 ( 0.018)	Loss 1.5371e+00 (1.5927e+00)	Acc@1  63.00 ( 66.68)	Acc@5  88.00 ( 88.13)
Test: [ 40/100]	Time  0.019 ( 0.017)	Loss 1.6982e+00 (1.5877e+00)	Acc@1  66.00 ( 66.93)	Acc@5  90.00 ( 88.37)
Test: [ 50/100]	Time  0.012 ( 0.017)	Loss 1.6045e+00 (1.6127e+00)	Acc@1  67.00 ( 66.80)	Acc@5  89.00 ( 88.14)
Test: [ 60/100]	Time  0.015 ( 0.016)	Loss 1.7793e+00 (1.5878e+00)	Acc@1  63.00 ( 66.98)	Acc@5  92.00 ( 88.51)
Test: [ 70/100]	Time  0.016 ( 0.016)	Loss 1.5742e+00 (1.5940e+00)	Acc@1  66.00 ( 67.04)	Acc@5  91.00 ( 88.63)
Test: [ 80/100]	Time  0.015 ( 0.016)	Loss 1.4980e+00 (1.5960e+00)	Acc@1  66.00 ( 66.99)	Acc@5  89.00 ( 88.63)
Test: [ 90/100]	Time  0.013 ( 0.015)	Loss 1.9531e+00 (1.5828e+00)	Acc@1  60.00 ( 67.27)	Acc@5  89.00 ( 88.84)
 * Acc@1 67.350 Acc@5 88.800
### epoch[88] execution time: 11.71142578125
EPOCH 89
# current learning rate: 0.0010000000000000002
# Switched to train mode...
Epoch: [89][  0/391]	Time  0.180 ( 0.180)	Data  0.150 ( 0.150)	Loss 1.3635e-01 (1.3635e-01)	Acc@1  95.31 ( 95.31)	Acc@5 100.00 (100.00)
Epoch: [89][ 10/391]	Time  0.021 ( 0.039)	Data  0.000 ( 0.015)	Loss 1.1938e-01 (1.8293e-01)	Acc@1  96.88 ( 94.18)	Acc@5  99.22 ( 99.64)
Epoch: [89][ 20/391]	Time  0.031 ( 0.033)	Data  0.001 ( 0.009)	Loss 1.9421e-01 (1.8962e-01)	Acc@1  92.97 ( 93.94)	Acc@5 100.00 ( 99.67)
Epoch: [89][ 30/391]	Time  0.021 ( 0.030)	Data  0.001 ( 0.007)	Loss 2.0361e-01 (1.9448e-01)	Acc@1  93.75 ( 93.70)	Acc@5  99.22 ( 99.60)
Epoch: [89][ 40/391]	Time  0.034 ( 0.029)	Data  0.001 ( 0.005)	Loss 2.0618e-01 (1.9649e-01)	Acc@1  93.75 ( 93.71)	Acc@5  99.22 ( 99.47)
Epoch: [89][ 50/391]	Time  0.020 ( 0.028)	Data  0.001 ( 0.005)	Loss 2.5659e-01 (1.9431e-01)	Acc@1  91.41 ( 93.83)	Acc@5  99.22 ( 99.51)
Epoch: [89][ 60/391]	Time  0.028 ( 0.028)	Data  0.002 ( 0.004)	Loss 1.5198e-01 (1.9548e-01)	Acc@1  95.31 ( 93.74)	Acc@5  98.44 ( 99.45)
Epoch: [89][ 70/391]	Time  0.032 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.1069e-01 (1.9619e-01)	Acc@1  92.97 ( 93.71)	Acc@5 100.00 ( 99.50)
Epoch: [89][ 80/391]	Time  0.021 ( 0.027)	Data  0.001 ( 0.004)	Loss 1.6760e-01 (1.9554e-01)	Acc@1  94.53 ( 93.75)	Acc@5  99.22 ( 99.51)
Epoch: [89][ 90/391]	Time  0.038 ( 0.027)	Data  0.002 ( 0.004)	Loss 2.1790e-01 (1.9458e-01)	Acc@1  92.19 ( 93.69)	Acc@5 100.00 ( 99.54)
Epoch: [89][100/391]	Time  0.029 ( 0.027)	Data  0.000 ( 0.003)	Loss 1.6309e-01 (1.9642e-01)	Acc@1  94.53 ( 93.66)	Acc@5 100.00 ( 99.54)
Epoch: [89][110/391]	Time  0.020 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.0728e-01 (1.9581e-01)	Acc@1  94.53 ( 93.70)	Acc@5 100.00 ( 99.55)
Epoch: [89][120/391]	Time  0.027 ( 0.027)	Data  0.001 ( 0.003)	Loss 1.9763e-01 (1.9755e-01)	Acc@1  93.75 ( 93.65)	Acc@5 100.00 ( 99.55)
Epoch: [89][130/391]	Time  0.026 ( 0.027)	Data  0.000 ( 0.003)	Loss 2.2522e-01 (1.9915e-01)	Acc@1  92.19 ( 93.62)	Acc@5 100.00 ( 99.53)
Epoch: [89][140/391]	Time  0.023 ( 0.027)	Data  0.001 ( 0.003)	Loss 2.2205e-01 (1.9756e-01)	Acc@1  92.19 ( 93.68)	Acc@5 100.00 ( 99.54)
Epoch: [89][150/391]	Time  0.020 ( 0.026)	Data  0.002 ( 0.003)	Loss 1.8005e-01 (1.9818e-01)	Acc@1  96.09 ( 93.68)	Acc@5 100.00 ( 99.54)
Epoch: [89][160/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.6431e-01 (1.9930e-01)	Acc@1  96.09 ( 93.65)	Acc@5  99.22 ( 99.55)
Epoch: [89][170/391]	Time  0.021 ( 0.026)	Data  0.000 ( 0.003)	Loss 1.3416e-01 (1.9908e-01)	Acc@1  96.09 ( 93.66)	Acc@5 100.00 ( 99.56)
Epoch: [89][180/391]	Time  0.046 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.8906e-01 (1.9926e-01)	Acc@1  89.84 ( 93.66)	Acc@5 100.00 ( 99.55)
Epoch: [89][190/391]	Time  0.040 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.3611e-01 (1.9782e-01)	Acc@1  96.09 ( 93.71)	Acc@5  99.22 ( 99.56)
Epoch: [89][200/391]	Time  0.019 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1460e-01 (1.9721e-01)	Acc@1  91.41 ( 93.70)	Acc@5 100.00 ( 99.57)
Epoch: [89][210/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.8079e-01 (1.9730e-01)	Acc@1  92.97 ( 93.68)	Acc@5  99.22 ( 99.57)
Epoch: [89][220/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.003)	Loss 2.1130e-01 (1.9693e-01)	Acc@1  92.97 ( 93.71)	Acc@5 100.00 ( 99.57)
Epoch: [89][230/391]	Time  0.025 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.2573e-01 (1.9679e-01)	Acc@1  96.88 ( 93.73)	Acc@5 100.00 ( 99.58)
Epoch: [89][240/391]	Time  0.027 ( 0.026)	Data  0.001 ( 0.003)	Loss 1.5076e-01 (1.9626e-01)	Acc@1  95.31 ( 93.73)	Acc@5 100.00 ( 99.59)
Epoch: [89][250/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1323e-01 (1.9684e-01)	Acc@1  92.97 ( 93.73)	Acc@5  97.66 ( 99.58)
Epoch: [89][260/391]	Time  0.033 ( 0.026)	Data  0.000 ( 0.002)	Loss 2.1204e-01 (1.9696e-01)	Acc@1  92.19 ( 93.75)	Acc@5 100.00 ( 99.58)
Epoch: [89][270/391]	Time  0.038 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.0471e-01 (1.9685e-01)	Acc@1  94.53 ( 93.72)	Acc@5 100.00 ( 99.56)
Epoch: [89][280/391]	Time  0.021 ( 0.026)	Data  0.002 ( 0.002)	Loss 2.0496e-01 (1.9682e-01)	Acc@1  92.97 ( 93.75)	Acc@5  99.22 ( 99.56)
Epoch: [89][290/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.7676e-01 (1.9604e-01)	Acc@1  92.97 ( 93.78)	Acc@5 100.00 ( 99.57)
Epoch: [89][300/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8054e-01 (1.9565e-01)	Acc@1  93.75 ( 93.80)	Acc@5  99.22 ( 99.56)
Epoch: [89][310/391]	Time  0.020 ( 0.026)	Data  0.001 ( 0.002)	Loss 3.1055e-01 (1.9608e-01)	Acc@1  91.41 ( 93.78)	Acc@5  99.22 ( 99.56)
Epoch: [89][320/391]	Time  0.026 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.4441e-01 (1.9509e-01)	Acc@1  94.53 ( 93.81)	Acc@5 100.00 ( 99.57)
Epoch: [89][330/391]	Time  0.020 ( 0.026)	Data  0.000 ( 0.002)	Loss 1.4453e-01 (1.9442e-01)	Acc@1  95.31 ( 93.82)	Acc@5 100.00 ( 99.57)
Epoch: [89][340/391]	Time  0.034 ( 0.026)	Data  0.001 ( 0.002)	Loss 2.5122e-01 (1.9447e-01)	Acc@1  92.97 ( 93.81)	Acc@5  99.22 ( 99.57)
Epoch: [89][350/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.5076e-01 (1.9380e-01)	Acc@1  97.66 ( 93.83)	Acc@5  99.22 ( 99.57)
Epoch: [89][360/391]	Time  0.038 ( 0.026)	Data  0.003 ( 0.002)	Loss 2.3218e-01 (1.9459e-01)	Acc@1  90.62 ( 93.80)	Acc@5 100.00 ( 99.58)
Epoch: [89][370/391]	Time  0.021 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8921e-01 (1.9460e-01)	Acc@1  94.53 ( 93.79)	Acc@5  99.22 ( 99.57)
Epoch: [89][380/391]	Time  0.034 ( 0.026)	Data  0.004 ( 0.002)	Loss 1.7261e-01 (1.9437e-01)	Acc@1  95.31 ( 93.81)	Acc@5 100.00 ( 99.58)
Epoch: [89][390/391]	Time  0.018 ( 0.026)	Data  0.001 ( 0.002)	Loss 1.8579e-01 (1.9420e-01)	Acc@1  93.75 ( 93.81)	Acc@5  98.75 ( 99.57)
## e[89] optimizer.zero_grad (sum) time: 0.10627341270446777
## e[89]       loss.backward (sum) time: 2.2627170085906982
## e[89]      optimizer.step (sum) time: 0.9276809692382812
## epoch[89] training(only) time: 10.16518497467041
# Switched to evaluate mode...
Test: [  0/100]	Time  0.149 ( 0.149)	Loss 1.3096e+00 (1.3096e+00)	Acc@1  74.00 ( 74.00)	Acc@5  88.00 ( 88.00)
Test: [ 10/100]	Time  0.016 ( 0.026)	Loss 1.5010e+00 (1.5834e+00)	Acc@1  66.00 ( 67.73)	Acc@5  91.00 ( 88.36)
Test: [ 20/100]	Time  0.015 ( 0.021)	Loss 1.7266e+00 (1.5949e+00)	Acc@1  66.00 ( 67.00)	Acc@5  90.00 ( 87.81)
Test: [ 30/100]	Time  0.013 ( 0.019)	Loss 1.5020e+00 (1.5958e+00)	Acc@1  64.00 ( 66.87)	Acc@5  88.00 ( 87.77)
Test: [ 40/100]	Time  0.010 ( 0.017)	Loss 1.6738e+00 (1.5909e+00)	Acc@1  67.00 ( 66.95)	Acc@5  90.00 ( 88.02)
Test: [ 50/100]	Time  0.017 ( 0.017)	Loss 1.6113e+00 (1.6190e+00)	Acc@1  67.00 ( 66.78)	Acc@5  89.00 ( 87.84)
Test: [ 60/100]	Time  0.014 ( 0.016)	Loss 1.8076e+00 (1.5927e+00)	Acc@1  61.00 ( 66.98)	Acc@5  93.00 ( 88.30)
Test: [ 70/100]	Time  0.012 ( 0.016)	Loss 1.5176e+00 (1.5965e+00)	Acc@1  67.00 ( 66.99)	Acc@5  93.00 ( 88.46)
Test: [ 80/100]	Time  0.019 ( 0.016)	Loss 1.5000e+00 (1.5988e+00)	Acc@1  67.00 ( 66.95)	Acc@5  89.00 ( 88.48)
Test: [ 90/100]	Time  0.016 ( 0.015)	Loss 1.9971e+00 (1.5848e+00)	Acc@1  59.00 ( 67.24)	Acc@5  89.00 ( 88.73)
 * Acc@1 67.360 Acc@5 88.670
### epoch[89] execution time: 11.78150463104248
### Training complete:
#### total training(only) time: 913.1917395591736
##### Total run time: 1060.9658374786377
